# DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection

Zhiyuan Yan\({}^{1}\), Yong Zhang\({}^{2}\), Xinhang Yuan\({}^{1}\), Siwei Lyu\({}^{3}\), Baoyuan Wu\({}^{1}\)

\({}^{1}\)School of Data Science,

The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China

\({}^{2}\)Tencent AI Lab

\({}^{3}\)Department of Computer Science and Engineering,

University at Buffalo, State University of New York, USA

Corresponding author: Baoyuan Wu (wubaoyuan@cuhk.edu.cn)

###### Abstract

A critical yet frequently overlooked challenge in the field of deepfake detection is the lack of a standardized, unified, comprehensive benchmark. This issue leads to unfair performance comparisons and potentially misleading results. Specifically, there is a lack of uniformity in data processing pipelines, resulting in inconsistent data inputs for detection models. Additionally, there are noticeable differences in experimental settings, and evaluation strategies and metrics lack standardization. To fill this gap, we present the first comprehensive benchmark for deepfake detection, called _DeepfakeBench_, which offers three key contributions: 1) a unified data management system to ensure consistent input across all detectors, 2) an integrated framework for state-of-the-art methods implementation, and 3) standardized evaluation metrics and protocols to promote transparency and reproducibility. Featuring an extensible, modular-based codebase, _DeepfakeBench_ contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series of deepfake detection evaluation protocols and analysis tools, as well as comprehensive evaluations. Moreover, we provide new insights based on extensive analysis of these evaluations from various perspectives (\(e.g.\), data augmentations, backbones). We hope that our efforts could facilitate future research and foster innovation in this increasingly critical domain. All codes, evaluations, and analyses of our benchmark are publicly available at [https://github.com/SCLBD/DeepfakeBench](https://github.com/SCLBD/DeepfakeBench).

## 1 Introduction

Deepfake, widely recognized for its facial manipulation, has gained prominence as a technology capable of fabricating videos through the seamless superimposition of images. The surging popularity of deepfake technology in recent years can be attributed to its diverse applications, extending from entertainment and marketing to more complex usages. However, the proliferation of deepfake is not without risks. The same tools that enable creativity and innovation can be manipulated for malicious intent, undermining privacy, promoting misinformation, or eroding trust in digital media, \(etc\).

Responding to the risks posed by deepfake contents, numerous deepfake detection methods  have been developed to distinguish deepfake contents from real contents, which are generally categorized into three types: naive detector, spatial detector, and frequency detector. Despite rapid advancements in deepfake detection technologies, a significant challenge remains due to the lack of a standardized, unified, and comprehensive benchmark for a fair comparison among different detectors. This issue causes three major obstacles to the development of the deepfake detection field. **First**, there is a remarkable inconsistency in the training configurations and evaluation standards utilized in the field. This discrepancy inevitably leads to divergent outcomes, making a faircomparison difficult. **Second**, the source codes of many methods are not publicly released, which could be detrimental to the reproducibility and comparability of their reported results. **Third**, we find that the detection performance can be significantly influenced by several seemingly inconspicuous factors, \(e.g.\), the number of selected frames in a video. Since the settings of these factors are not uniform and their impacts are not thoroughly studied in most existing works, the reported results and corresponding claims may be biased or misleading.

To bridge this gap, we build the first comprehensive benchmark, called **DeepfakeBench**, offering a unified platform for deepfake detection. Our main contributions are threefold. **1) An extensible modular-based codebase:** Our codebase consists of three main modules. The data processing module provides a unified data management module to guarantee consistency across all detection inputs, such that alleviating the time-consuming data processing and evaluation. The training module provides a modular framework to implement state-of-the-art detection algorithms, facilitating direct comparisons among different detection algorithms. The evaluation and analysis module provides several widely adopted evaluation metrics and rich analysis tools to facilitate further evaluations and analysis. **2) Comprehensive evaluations:** We evaluate 15 state-of-the-art detectors with 9 deepfake datasets under a wide range of evaluation settings, providing a holistic performance evaluation of each detector. Moreover, we establish a unified evaluation protocol that enhances the transparency and reproducibility of performance evaluation. **3) Extensive analysis and new insights:** We provide extensive analysis from various perspectives, not only analyzing the effects of existing algorithms but also uncovering new insights to inspire new technologies. **In summary**, we believe _DeepfakeBench_ could constitute a substantial step towards calibrating the current progress in the deepfake detection field and promoting more innovative explorations in the future.

## 2 Related Work

Deepfake GenerationDeepfake technology, which generally centers on the artificial manipulation of facial imagery, has made considerable strides from its rudimentary roots. Starting in 2017, learning-based manipulation techniques have made significant advancements, with two prominent methods gaining considerable attention: Face-Swapping and Face-Reenactment. **1) Face-swapping** constitutes a significant category of deepfake generation. These techniques typically involve autoencoder-based manipulations, which are based on two autoencoders with a shared encoder and two different decoders. The autoencoder output is then blended with the rest of the image to create the forgery image. Notable face-swapping datasets of this approach include UADFV , FF-DF , CelebDF , DFD , DFDC , DeeperForensics-1.0 , and ForgeryNet . **2) Face-reenactment** is characterized by graphics-based manipulation techniques that modify source faces imitating the expressions of a different face. NeuralTextures  and Face2Face , utilized in FaceForensics++, stand out as standard face-reenactment methods. Face2Face uses key facial points to generate varied expressions, while NeuralTexture uses rendered images from a 3D face model to migrate expressions.

Deepfake DetectionCurrent deepfake detection can be broadly divided into three categories: naive detector, spatial detector, and frequency detector. **1) Naive detector** employs CNNs to directly distinguish deepfake content from authentic data. Numerous CNN-based binary classifiers have been proposed, \(e.g.\), MesoNet  and Xception . **2) Spatial detector** delves deeper into specific representation such as forgery region location , capsule network , disentanglement learning , image reconstruction , erasing technology , \(etc.\) Besides, some other

   Model Type & Declasses & Backbone & Repositories & Reference \\   Naive Detector & MesoNet  & DieselNet & [http://github.com/mural/MesoNet](http://github.com/mural/MesoNet) & Wips-2018 \\ Naive Detector & MesoNet/green1 & DieselNet & [http://github.com/mural/MesoNet](http://github.com/mural/MesoNet) & Wips-2018 \\ Naive Detector & CNS,Avg  & ResNet  & [http://github.com/mural/MesoNet](http://github.com/mural/MesoNet) & CVPR-2020 \\ Naive Detector & Effectively  & EfficientNet  & [http://github.com/mural/MesoNet](http://github.com/mural/MesoNet) & LCF-2019 \\ Naive Detector & Xception  & Xception  & [http://github.com/mural/MesoNet](http://github.com/mural/MesoNet) & LCF-2019 \\  Spatial Detector & CapsNet  & Diesel capsule  & [https://github.com/mural/mesoNet/Deep-forensic-20](https://github.com/mural/mesoNet/Deep-forensic-20) & KACS-2019 \\ Spatial Detector & DSP-FAW  & Xception  & [https://github.com/mural/MesoNet/Deep-forensic-20](https://github.com/mural/MesoNet/Deep-forensic-20) & KACS-2019 \\ Spatial Detector & Face-Avg  & HRK  & [https://github.com/mural/MesoNet/Deep-forensic-20](https://github.com/mural/MesoNet/Deep-forensic-20) & KACS-2019 \\ Spatial Detector & FPDI  & Xception  & erik.com.com.uk/m/projects/64.html & CVPR-2020 \\ Spatial Detector & CORE  & Xception  & [https://github.com/mural/OOSE](https://github.com/mural/OOSE) & CVPR-2022 \\ Spatial Detector & RICEC  & Diesel Novels & [https://github.com/mural/OOSE](https://github.com/mural/OOSE) & CVPR-2022 \\ Spatial Detector & UCF  & Xception  & Unpublished code, reproduced by us & KCV-2023 \\  Frequency Detector & FFN  & Xception  & Unpublished code, reproduced by us & ECCV-2020 \\ Frequency Detector & SPSI  & Xception  & Unpublished code, reproduced by us & CVPR-2021 \\ Frequency Detector & SRM  & Xception  & Unpublished code, reproduced by us & CVPR-2021 \\   

Table 1: _Summary of the compared deepfake detectors. For detectors without publicly available repositories, we undertake careful re-implementation, adhering to the instructions specified in the original papers._methods specifically focus on the detection of blending artifacts [22; 20; 3], generating forged images during training in a self-supervised manner to boost detector generalization. **3) Frequency detector** addresses this limitation by focusing on the frequency domain for forgery detection [13; 32; 26; 27]. SPSL  and SRM  are other examples of frequency detectors that utilize phase spectrum analysis and high-frequency noises, respectively. Qian _et al._ propose the use of learnable filters for adaptive mining of frequency forgery clues using frequency-aware image decomposition.

Related Deepfake Surveys and BenchmarksThe growing implications of deepfake technology have sparked extensive research, resulting in the establishment of several surveys and dataset benchmarks in the field. **1) Surveys** provide a detailed examination of various facets of deepfake technology. For instance, Westerlund _et al._ present a thorough analysis of deepfake, emphasizing its legal and ethical dimensions. Tolosana _et al._ furnish a comprehensive review of face manipulation techniques, including deepfake methods, along with approaches to detect such manipulations. **2) Benchmarks** in this field have emerged as essential tools to provide realistic forgery datasets. For instance, FaceForensics++ (FF++)  serves as a prominent benchmark, offering high-quality manipulated videos and a variety of forgery types. The Deepfake Detection Challenge Dataset (DFDC)  introduces a diverse range of actors across different scenarios.

While these benchmarking methodologies have made significant contributions, they specifically focus on their own datasets, without offering a standardized way to handle data across different datasets, which may lead to inconsistencies and obstacles to fair comparisons. Also, the lack of a unified framework in some benchmarks could lead to variations in training strategies, settings, and augmentations, which may result in discrepancies in the outcomes. Furthermore, the provision of comprehensive analytical tools is not always prominent, which might restrict the depth of analysis on the potential impacts of different factors. One notable work  aims to build a benchmark for evaluating various detectors under different datasets. Another recent work  introduces a benchmark centered around detecting GAN-generated images using continual learning. However, these two benchmarks still lack a modular, extensible, and comprehensive codebase that includes data preprocessing, unified settings, training modules, evaluations, and a series of analytical tools. _DeepfakeBench_, on the other hand, presents a concise but comprehensive benchmark. Its contributions are threefold: introducing a unified data management system for consistency, offering an integrated framework for implementing advanced methods, and analyzing the related factors with a series of analysis tools. Detailed comparisons between our _DeepfakeBench_ and  are shown in Tab.2.

## 3 Our Benchmark

### Datasets and Detectors

_Datasets_ Our benchmark currently incorporates a collection of 9 widely recognized and extensively used datasets in the realm of deepfake detection: FaceForensics++ (FF++) , CelebDF-v1 (CDFv1) , CelebDF-v2 (CDFv2) , DeepFakeDetection (DFD) , DeepFake Detection Challenge Preview (DFDC-P) , DeepFake Detection Challenge (DFDC) , UADFV , FaceShifter (Fsh) , and DeeperForensics-1.0 (DF-1.0) . Notably, FF++ contains 4 types of manipulation methods: Deepfakes (FF-DF) , Face2Face (FF-F2F) , FaceSwap (FF-FS) , NeuralTextures (FF-NT) . There are three versions of FF++ in terms of compression level, \(i.e.\)

   Feature / Paper & DeepfakeBench & Paper  \\    Scope of Deepfake \\ Number of Detectors \\ Number of Datasets \\ Code Open Source \\ Modular and Extensible Codebase \\ User-Friendly APIs \\ Customizable Preprocessing Module \\ Unified Training Framework \\ Rich Analysis Tools \\ Analysis of FLOPs \\ Evaluation Metrics \\  & 
 Face-swapping \\ 15 & 11 \\ 9 & 8 \\ \(\) & Not yet \\ \(\) & \(, lightly compressed (c23), and heavily compressed (c40). The detailed descriptions of each dataset are presented in the Sec. A.3 of the **Appendix**. Typically, FF++ is employed for model training, while the rest are frequently used as testing data. However, our benchmark allows users to select their combinations of training and testing data, thus encouraging custom experimentation.

It is notable that, although these datasets have been widely used in the community, they are not usually provided in a readily accessible and combined format. It often requires a substantial investment of time and effort in data sourcing, pre-processing (\(e.g.\), frame extraction, face cropping, and face alignment), and organization of the raw datasets, which are often organized in diverse structures. This considerable data preparation overhead often diverts researchers' attention away from the core tasks like methodology design and experimental evaluations. To tackle this challenge, our benchmark offers a collection of well-processed and systematically organized datasets, allowing researchers to devote more time to the core tasks. Additionally, our benchmark enriches some datasets (\(e.g.\), FF++  and DFD ), by including mask data (\(i.e.\), the forgery region) that is aligned with the respective facial images in these datasets. It could facilitate more comprehensive deepfake detection studies. **In summary**, our benchmark provides a unified, user-friendly, and diversified data resource for the deepfake detection community. It eliminates the cumbersome task of data preparation and allows researchers to concentrate more on innovating effective deepfake detection methods.

DetectorsOur benchmark has implemented a total of 15 established deepfake detection algorithms, as detailed in Tab. 1. The selection of these algorithms is guided by three criteria. **First**, we prioritize methods that hold a classic status (\(e.g.\), Xception), or those considered advanced, typically published in recent top-tier conferences or journals in computer vision or machine learning. **Second**, our benchmark classifies detectors into three categories: naive detectors, spatial detectors, and frequency detectors. Our primary emphasis is on image forgery detection, hence, temporal-based detectors have not yet been incorporated. Moreover, we have refrained from including traditional detectors (\(e.g.\), Headpose ) due to their limited scalability to large-scale datasets, making them less suitable for our benchmark's objectives. **Third**, we aim to include methods that are straightforward to implement and reproduce. We notice that several existing methods involve a series of steps, some of which are reliant on third-party algorithms or heuristic strategies. These methods usually have numerous hyper-parameters and are fraught with uncertainty, making their implementation and reproduction challenging. Therefore, these methods without open-source codes are intentionally excluded from our benchmark. However, it is important to note that there are also some non-open-source methods we employed that are derived from the code directly provided by their respective authors.

Figure 1: The general structure of the modular-based codebase of _DeepfakeBench_.

### Codebase

We have built an extensible modular-based codebase as the basis of _DeepfakeBench_. As shown in Fig. 1, it consists of three core modules, including _Data Processing Module_, _Training Module_, and _Evaluation and Analysis Module_.

Data Processing ModuleThe _Data Processing Module_ includes two pivotal sub-modules that automate the data processing sequence, namely the _Data Preprocessing_ and _Data Arrangement_ sub-modules. **1) Data preprocessing** sub-module presents a streamlined solution. First, Users are provided with a _YAML_ configuration file, enabling them to tailor the preprocessing steps to their specific requirements. Second, we furnish a unified preprocessing script, which includes frame extraction, face cropping, face alignment, mask cropping, and landmark generation. **2) Data arrangement** sub-module further augments the convenience of data management. This sub-module comprises a suite of _JSON_ files for each dataset. Users can execute a rearranged script to create a unified _JSON_ file for each dataset. This unified file provides access to the corresponding training, testing, and validation sets, along with other information such as the frames, landmarks, masks, \(etc\), related to each dataset.

Training ModuleThe _Training Module_ currently accommodates 15 detectors across three categories: naive detector, spatial detector, and frequency detector, all of which are shown in Tab. 1. **1) Naive detector** leverages various CNN architectures to directly detect forgeries without relying on additional manually designed features. **2) Spatial detector** builds upon the backbone of CNNs used in the Naive Detector and further explores manual-designed algorithms to detect deepfake. **3) Frequency detector** focuses on utilizing information from the frequency domain and extracting frequency artifacts for detection. Each detector implemented in our benchmark is managed in a streamlined and efficient way, with a _YAML_ config file created for each one. This allows users to easily set their desired parameters, \(e.g.\), batch size, learning rate, \(etc\). These detectors are trained on a unified trainer that records the metrics and losses during the training and evaluation process. Thus, the training and evaluation processes, logging, and visualization are handled automatically, eliminating the need for manual specification.

Evaluation and Analysis ModuleFor evaluation, we employ 4 widely used evaluation metrics: accuracy (ACC), the area under the ROC curve (AUC), average precision (AP), and equal error rate (EER) Besides, it is notable that there is an inconsistency in the usage of these evaluation metrics in the community, some are at the frame level, while others are at the video level, leading to unfair comparisons. Our benchmark currently adopts the frame level evaluation to build a fair basis for comparison among detectors. In addition to the evaluation values of these metrics, we also provide several visualizations to facilitate performance comparisons, \(e.g.\), the ROC-AUC curve, radar chart, and histogram. **For analysis**, we provide various visualization tools to gain deeper insights into the detectors' performance. For example, Grad-CAM  is used to highlight the potential forgery regions detected by the models, providing interpretability and assisting in understanding the underlying reasoning for the model's predictions. To explore the learned features and representations, we employ t-SNE visualization . Furthermore, we offer custom visualizations tailored to specific detectors. For example, for Face X-ray , we provide visualizations of the detection boundary of the face, as described in its original paper (see the top-right corner of Fig. 1).

## 4 Evaluations and Analysis

### Experimental Setup

In the data processing, face detection, face cropping, and alignment are performed using DLIB . The aligned faces are resized to \(256 256\) for both the training and testing. In the training module, we employ the Adam optimization algorithm with a learning rate of 0.0002. The batch size is fixed at 32 for all experiments. We sample 32 frames for each video for training and testing. We primarily leverage pre-trained backbones from ImageNet if feasible. Otherwise, we resort to initializing the remaining weights using a normal distribution. We also apply widely used data augmentations, _i.e._, image compression, horizontal flip, rotation, Gaussian blur, and random brightness contrast. In terms of evaluation, we compute the average value of the top-3 metrics (\(e.g.\), average top-3 AUC) as our evaluation metric. We also report other metrics (\(i.e.\), AP, EER, Precision, and Recall) in the Sec. A.3 of the **Appendix**. Further details of dataset configuration, algorithms implementation, and full training details can be seen in the Sec. A.1, Sec. A.2, and Sec. A.3 of the **Appendix**, respectively.

### Evaluations

In this section, we focus on performing two types of evaluations: **1) within-domain and cross-domain evaluation**, and **2) cross-manipulation evaluation**. The purpose of the within-domain evaluation is to assess the performance of the model within the same dataset, while cross-domain evaluation involves testing the model on different datasets. We also perform cross-manipulation evaluation to evaluate the model's performance on different forgeries under the same dataset.

Within-Domain and Cross-Domain EvaluationsIn this evaluation, we specifically train the model using FF++ (c23) as the default training dataset. Subsequently, we evaluate the model on a total of 14 different testing datasets, with 6 datasets for within-domain evaluation and 8 datasets for cross-domain evaluation. Tab. 3 provides an extensive evaluation of various detectors, divided into Naive, Spatial, and Frequency types, based on both within-domain and cross-domain tests. Regarding the results in Tab. 3, we observe that, for the within-domain evaluations, a majority of the detectors performed commendably, evidenced by high within-domain AUC. Remarkably, detectors such as UCF, Xception, EfficientB4, and F3Net registered significant average scores, specifically 95.37%, 94.50%, 93.89%, and 94.49% respectively. Furthermore, an unexpected revelation comes from the performance of Naive Detectors. Astonishingly, Naive Detectors (\(e.g.\), Xception and EfficientB4), which essentially rely on a straightforward CNN classifier, register high AUC values that are comparable to more sophisticated algorithms. This could potentially suggest that the performance leap from advanced state-of-the-art methods to Naive Detectors might not be as substantial as perceived, particularly in consistent settings (\(e.g.\), pre-training or data augmentation). In other words, the performance gap could be a product of these additional factors rather than the intrinsic superiority of the method. To delve deeper into this phenomenon, we will investigate the impact of data augmentation, backbone architecture, pre-training, and the number of training frames in the following section (see Sec. 4.3).

   } &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   Naive & Moses & Moneka & 0.087 & 0.5920 & 0.6771 & 0.6470 & 0.5946 & 0.5704 & 0.6097 & 0.7358 & 0.6091 & 0.9113 & 0.5481 & 0.5506 & 0.5994 & 0.5600 & 0.7150 & 0.6551 \\ Naive & Mershove & Moneka & 0.7538 & 0.7278 & 0.8542 & 0.8667 & 0.421 & 0.6871 & 0.7571 & 0.7366 & 0.6066 & 0.9233 & 0.6000 & 0.6252 & 0.7561 & 0.6438 & 0.6049 & 0.7364 \\ Naive & CNN-Aug & ResNet & 0.8493 & 0.7846 & 0.9044 & 0.8788 & 0.9028 & 0.7313 & 0.6499 & 0.7430 & 0.7027 & 0.7993 & 0.6644 & 0.6610 & 0.6170 & 0.5985 & 0.8379 & 0.7309 & 0.7020 \\ Naive & Xception & Xception & 0.6577 & 0.6261 & 0.5999 & 0.7978 & 0.6393 & 0.6395 & 0.4940 & 0.7744 & 0.7368 & 0.8341 & **0.8355** & 0.7077 & 0.7374 & 0.6249 & 0.5793 & 0.7715 & 2 \\ Naive & EfficientB4Cross-Manipulation EvaluationsWe also conduct a cross-manipulation evaluation to assess the model's performance on various manipulation forgeries within the same dataset (FF++ ). In this evaluation, only the forgery algorithm is altered. Other factors such as background and identity remain consistent across all the different forgeries. Fig. 2 compares the cross-manipulation detection performance of 10 detectors. Upon examining the figure, it becomes evident that the issue of generalization is prominent. While detectors such as CORE, EfficientB4, SPSL, SRM, and Xception exhibit excellent performance on the FF-DF test data when trained on FF-DF, their performance significantly deteriorates when faced with FF-FS forgeries. Furthermore, the "FT-NT" test data poses challenges for almost all detectors, as reflected by the diminished AUC values in this category throughout the heatmaps. In contrast, the "FT-DF" test data emerged as a comparatively facile challenge for the detectors. **In summary**, the varying nature of forgeries highlights a significant generalization gap. Models trained on specific forgeries often struggle to adapt to other unseen forgeries. This underscores the importance of training models to recognize generic forgery artifacts to better combat unseen forgery types.

### Analysis

Effect of Data AugmentationWe assess the influence of various augmentation techniques on the performance of forgery detectors in this section. Specifically, we investigate the impact of rotations, horizontal flips, image compression, isotropic scaling, color jitter, and Gaussian blur on two prototypical detectors: one from the spatial domain (Xception) and one from the frequency domain (SPSL). Fig. 3 compares the performance when training these detectors with all data augmentations (denoted as "w_All"), without any data augmentations ("wo_All"), and without a specific augmentation.

Our findings can be summarized into three main observations: **First**, in the case of within-domain evaluation (as seen in the FF++_c23 dataset), removing all augmentations appears to improve detector performance by approximately 2% for both Xception and SPSL, suggesting that most augmentations may have a negative impact within this context. **Second**, for evaluations involving compressed data (FF++_c40), certain augmentations such as Gaussian blur demonstrate effectiveness in both Xception and SPSL detectors, as they simulate the effects of compression on the data during training. **Third**, in the context of cross-domain evaluations (CelebDF-v2, DFD, and DFDCP), operations like compression and blur may significantly degrade the performance of SPSL in the DFD and DFDCP datasets, possibly due to their tendency to obscure high-frequency details. Similar negative effects of

Figure 4: _Visualization of the performance of 3 different backbones, ResNet, EfficientNet-B4, and Xception, across 4 different detectors, CORE, SPSL, UCF, and Face X-ray. The evaluation is conducted using the AUC metric, following the settings described in the previous section._

Figure 3: _Visualization of different augmentation methods. We apply two detectors, one in the spatial domain (Xception) and one in the frequency domain (SPSL), and then use 8 different augmentation strategies to measure the effect on 5 test datasets._

the blur operation are observed for Xception, likely as it diminishes the visibility of visual artifacts. These findings underscore the need for further exploration into identifying a universally beneficial augmentation that can be effectively utilized across a wide range of detectors in generalization scenarios, irrespective of their specific attributes or datasets.

Effect of Backbone ArchitectureWe here investigate the impact of different backbone architectures on the performance of forgery detection models. Specifically, we compare the performance of three popular backbones: Xception, EfficientNet-B4, and ResNet34. Each backbone is integrated into the detection model, and its performance is evaluated on both within-domain and cross-domain datasets (see Fig. 4). Our findings reveal that Xception and EfficientNet-B4 consistently outperform ResNet34, despite having a similar number of parameters. This indicates that the choice of backbone architecture plays a crucial role in detector performance, especially when evaluating the DeepfakeDetection dataset using CORE. **In summary**, these results highlight the critical role of carefully selecting a suitable backbone architecture in the design of deepfake detection models. Further research in this direction holds the potential for advancing the field in the future.

Additional In-depth Analysis towards the Effect of Backbone ArchitectureWhen analyzing the effect of backbone architecture, our analysis in Sec. 4.3 shows that Xception and EfficientNet-B4 work better than ResNet-34. Given the three architectures have similar numbers of parameters, we are curious about why there exists an obvious performance gap among the three architectures. Here, we dive deeper to explore the possible reasons.

After our preliminary investigation, we found that the reasons are related to two factors, namely architecture and models' scale. **First**, we identify a common module in EfficientNet and Xception that is not present in ResNet, namely the **depthwise separable convolution module**. We hypothesize that this module might be contributing to the performance advantage. To evaluate this, we insert this module into ResNet, replacing only the first convolutional layer. Experiments demonstrate significant improvements on many test datasets (as shown in Tab. 4). **Second**, upon closer scrutiny, additional factors that might exert an impact on the ultimate performance come to light. These encompass the number of layers within the model architecture as well as the number of parameters associated with it. Referring to Tab. 8 in the **Appendix**, it becomes evident that the parameter numbers remain comparable among the three models. Subsequently, a comprehensive exploration is conducted to assess the impact of layer numbers. This assessment involves a diverse range of ResNet variants, including ResNet 50 and ResNet 152. Results in Tab. 9 in our **Appendix** uncover that ResNet 50, characterized by a greater number of layers in comparison to ResNet 34, yields a substantial enhancement in performance. However, when confronted with a higher layer count, as exemplified by ResNet 152, the extent of improvement becomes restricted.

   Model & **FF++\_c23** & **FF++\_c40** & **CDF-v2** & **DFD** & **DFDCP** & **UADFV** & **Average** \\  ResNet & 0.8493 & 0.7846 & 0.7027 & 0.6464 & 0.6170 & 0.8739 & 0.7456 \\  ResNet-DSC & 0.8968 & 0.8048 & 0.7582 & 0.7006 & 0.6766 & 0.8895 & 0.7877 \\  Improvement (\%) & +5.60\% & +2.57\% & +7.90\% & +8.39\% & +9.64\% & +1.78\% & +5.64\% \\   

Table 4: _Ablation study regarding the effectiveness of the depthwise separable convolution module (DSC) for ResNet. The models are trained on FF++\_c23 and tested on other datasets. The metric is the frame-level AUC._

Figure 5: _Visualization of the effect of pre-trained weights on three different architectures. The evaluation is conducted using the AUC metric, following the settings described in the previous section._

Effect of Pre-training of the BackboneThis analysis focuses on the impact of pre-training on forgery detection models. Following the previous section, we analyze three typical architectures: Xception, EfficientNetB4, and ResNet34. Fig. 5 reveals that the pre-trained models can largely outperform their non-pre-trained counterparts, especially in the case of Xception (about 10% in DFDCP) and EfficientB4 (about 10% in DeepFakeDetection). This can be attributed to the ability of pre-trained models to capture and leverage meaningful low-level features. However, the benefits of pre-training are less pronounced for ResNet34, mainly due to its architectural design, which may not fully exploit the advantages offered by pre-trained weights. **Overall**, our findings underscore the importance of both architectural choices and the utilization of pre-trained weights in achieving optimal forgery detection performance.

Visualizing RepresentationsDeepfake detection can be considered a representation learning problem, where detectors learn representations through their backbones and employ various classification algorithms. It is crucial to assess whether the learned representations align with the expectations. To accomplish this, we utilize t-SNE  for analysis, which allows us to visualize the representation.

We examine t-SNE visualization from two perspectives. First, we assess whether the detectors can accurately differentiate between real and fake samples. This is achieved by assigning labels to the points in the t-SNE plot based on their corresponding ground truth. Second, we delve deeper into the fake category and investigate whether the models capture common features across different forgery types rather than being overfitted to specific forgeries. To conduct this analysis, we train and test each detector on the FF++ (c23) dataset and visualize the t-SNE representation using the test data. Also, we visualize all the samples with their corresponding labels, where the Deepfakes, Face2Face, FaceSwap, and NeuralTextures represent different forgery types in FF++. For visualization purposes, we randomly select 5000 samples, with an equal distribution of 2500 real and 2500 fake samples. Default parameters are used for t-SNE.

From the t-SNE results shown in Fig. 6, we observe that different detectors learn distinct feature representations in the visualized space. Notably, the results indicate that Meso4 struggles to differentiate between real and fake samples, as the two categories overlap and cannot be clearly distinguished.

## 5 Conclusions, Future Plans, and Societal Impacts

**Conclusions** We have developed _DeepfakeBench_, a groundbreaking and comprehensive framework, emphasizing the benefits of a modular architecture, including extensibility, maintainability, fairness,

Figure 6: t-SNE visualization for each detector. These detectors are trained and tested on FF++ (c23).

and analytical capability. We hope that _DeepfakeBench_ could contribute to the deepfake detection community in various ways. **First**, it provides a concise yet comprehensive platform that incorporates a tailored data processing pipeline, and accommodates a wide range of detectors, while also facilitating a fair and standardized comparison among various models. **Second**, it assists researchers in swiftly comparing their new methods with existing ones, thereby facilitating faster development and iterations. **Last**, the in-depth analysis and comprehensive evaluations performed through our benchmark have the potential to inspire novel research problems and drive future advancements in the field.

**Limitations and Future Plans** To date, _DeepfakeBench_ primarily focuses on providing algorithms and evaluations at the frame level. We will further enhance the benchmark by incorporating video-level detectors and evaluation metrics. This expansion will enable a more comprehensive assessment of forgery detection performance, considering the temporal dynamics and context within videos. Besides, we also plan to carry out more evaluations for detecting images directly produced by diffusion or GANs, using the existing benchmark. In the current version, we have provided the visualizations and analysis for GAN-generated and diffusion-generated data in the frequency domain (see Sec. A.4 in the Appendix). Furthermore, we aim to include a wider range of typical detectors and datasets to offer a more comprehensive platform for evaluating the performance of detectors. _DeepfakeBench_ will continue to evolve as a valuable resource for researchers, facilitating the development of advanced deepfake detection technologies.

**Societal Impact and Ethical Issue** The potential ethical issue lies in the risk that malicious actors might exploit _DeepfakeBench_ to refine deepfakes to evade detection. **1) Inherent challenge with benchmarking:**_DeepfakeBench_, like any benchmark created for positive intent, could inadvertently provide a blueprint for these actors due to its transparent nature. **2) Potential solutions and forward path:** As solutions, we are contemplating controlled access for users and are committed to the dynamic evolution of DeepfakeBench to ensure it remains robust against emerging threats.

## 6 Contents in Appendix

The Appendix accompanying this paper provides additional details. The Appendix is organized as follows: **1) Details of data processing** This section provides further elaboration on the data processing steps, including face detection, face cropping, alignment, and _etc_. **2) Details of algorithms implementation and visualizations** This section dives into the implementation details of the algorithms used in the study. It also includes additional visualizations to help readers gain a deeper understanding of the experimental results. **3) Training details and full experimental results**: This section presents comprehensive details of the training process, including additional evaluation metrics beyond those reported in the main paper. **4) Other analysis results**: This section conducts analysis on some parts that are not analyzed in detail in the main text, such as analyzing and visualizing the frequency domain analysis of images generated by GAN and diffusion, etc.

## 7 Acknowledgement

This work is supported by the National Natural Science Foundation of China under grant No. 62076213, Shenzhen Science and Technology Program under grant No. RCYX20210609103057050, No. ZDSYS2021102111415025, No. GXWD2020201231105722002-20200901175001001, and the Guangdong Provincial Key Laboratory of Big Data Computing, the Chinese University of Hong Kong, Shenzhen.