# Unveiling the Hidden Structure of Self-Attention

via Kernel Principal Component Analysis

 Rachel S.Y. Teo

Department of Mathematics

National University of Singapore

rachel.tsy@u.nus.edu

&Tan M. Nguyen

Department of Mathematics

National University of Singapore

tanmm@nus.edu.sg

###### Abstract

The remarkable success of transformers in sequence modeling tasks, spanning various applications in natural language processing and computer vision, is attributed to the critical role of self-attention. Similar to the development of most deep learning models, the construction of these attention mechanisms relies on heuristics and experience. In our work, we derive self-attention from kernel principal component analysis (kernel PCA) and show that self-attention projects its query vectors onto the principal component axes of its key matrix in a feature space. We then formulate the exact formula for the value matrix in self-attention, theoretically and empirically demonstrating that this value matrix captures the eigenvectors of the Gram matrix of the key vectors in self-attention. Leveraging our kernel PCA framework, we propose Attention with Robust Principal Components (RPC-Attention), a novel class of robust attention that is resilient to data contamination. We empirically demonstrate the advantages of RPC-Attention over softmax attention on the ImageNet-1K object classification, WikiText-103 language modeling, and ADE20K image segmentation task. The code is publicly available at https://github.com/rachtsy/KPCA_code.

## 1 Introduction

Transformers  have emerged as the preeminent model for tackling a myriad of challenging problems in natural language processing , computer vision , reinforcement learning , and other applications . The effectiveness of transformers is rooted in their ability to learn from unlabeled data and take advantage of pre-trained models for downstream tasks that involve diverse data modalities with limited supervision . At the core of the transformer's success lies the self-attention mechanism, which serves as the fundamental building block of a transformer model. This mechanism enables each token in a sequence to aggregate information from other tokens by computing a weighted average based on similarity scores between their feature representations. Facilitating dynamic interactions among tokens, this attention mechanism allows tokens to selectively attend to others, thereby attaining a contextual representation . The flexibility in capturing diverse syntactic and semantic relationships is an important factor contributing to the success of transformers .

**Self-Attention.** For a given input sequence \(:=[_{1},,_{N}]^{}^{N D_{x}}\) of \(N\) feature vectors, self-attention transforms \(\) into the output sequence \(\) in the following two steps:

_Step 1:_ The input sequence \(\) is projected into the query matrix \(\), the key matrix \(\), and the value matrix \(\) via three linear transformations

\[=_{Q}^{};=_{K}^{};= {W}_{V}^{},\] (1)where \(_{Q},_{K}^{D D_{x}}\), and \(_{V}^{D_{v} D_{x}}\) are the weight matrices. We denote \(:=[_{1},,_{N}]^{},:=[_{1},, _{N}]^{}\), and \(:=[_{1},,_{N}]^{}\), where the vectors \(_{i},_{i},_{i}\) for \(i=1,,N\) are the query, key, and value vectors, respectively.

_Step 2:_ The output sequence \(:=[_{1},,_{N}]^{}\) is then computed as follows

\[=^{}/:=,\] (2)

where the softmax function is applied to each row of the matrix \(^{}/\). The matrix \(:=^{}}{} ^{N N}\) and its component \(a_{ij}\) for \(i,\,j=1,,N\) are called the attention matrix and attention scores, respectively. For each query vector \(_{i}\) for \(i=1,,N\), an equivalent form of Eqn. (2) to compute the output vector \(_{i}\) is given by

\[_{i}=_{j=1}^{N}_{i}^{}_{j}/ _{j}.\] (3)

The self-attention computed by Eqn. (2) and (3) is called the scaled dot-product or softmax attention. In our paper, we call a transformer that uses this attention the softmax transformer. The structure that the attention matrix \(\) learns from training determines the ability of the self-attention to capture contextual representations for each token.

Despite their impressive achievements, the development of most attention layers rely on intuitions and heuristic approaches. The quest for a systematic and principled framework for studying and synthesizing attention layers has remained challenging.

**Contribution.** We study and analyze self-attention in transformers from the perspective of kernel principal component analysis (kernel PCA). In particular, we develop a novel connection between self-attention and kernel PCA, showing that _self-attention projects its query vectors \(_{i}\), \(i=1,,N\), onto principal component axes of the key matrix \(\) in a feature space_. We then inspect the structure of the value matrix \(\) of self-attention suggested by our kernel PCA framework, validating \(\)_captures the eigenvectors of the Gram matrix of the key vectors \(_{j}\),\(j=1,,N\)_. Using our framework, we then propose a new class of robust attention, namely the Attention with Robust Principal Components (RPC-Attention). Our contribution is three-fold.

1. We derive self-attention from kernel PCA, showing that the attention outputs are projections of the query vectors onto the principal components axes of the key matrix \(\) in a feature space.
2. We discover and validate that the value matrix \(\) in self-attention captures the eigenvectors of the Gram matrix of the key vectors \(_{j}\),\(j=1,,N\).
3. We develop the Attention with Robust Principal Components (RPC-Attention), a new attention mechanism that is resilient to data contamination, using our kernel PCA framework.

We empirically demonstrate the benefits of RPC-Attention on the ImageNet-1K object classification, ADE20K image segmentation, and large scale WikiText-103 language modeling tasks. We further illustrate the robustness of RPC-Attention through our evaluations on popular, standard robustness benchmarks, as well as various white and black box adversarial attacks on ImageNet-1K images, 15 different types of corruptions on the ADE20K dataset, and word swap attack on WikiText-103.

## 2 Principal Component Analysis of Attention

In this section, we will derive attention from kernel PCA. Suppose we are given a dataset \(M=\{_{1},,_{N}\}^{D}\). Here, \(_{1},,_{N}\) are attention keys in self-attention. As in kernel PCA, we first project these data points into a feature space using a feature map \(():=()/g()\), where \(()\) is a nonlinear transformation from \(^{D}\) to \(^{D^{}}\), and \(g()\) is a vector-scalar function that computes a scaling factor for \(()\). We center the projected data as follows:

\[}(_{j})=(_{j})-_{j^{ }=1}^{N}(_{j^{}}).\] (4)Letting \(\) be the covariance matrix of the centered data in feature space, its eigenvector expansion is

\[=_{j=1}^{N}}(_{j}) }(_{j})^{};_{d}=_{d} _{d},\ \ d=1,,D_{v}.\] (5)

Plugging \(\) into (5), we obtain

\[_{j=1}^{N}}(_{j})\{ {}(_{j})^{}_{d}\}=_{d}_{d}.\] (6)

Thus, provided that \(_{d}>0\), the eigenvector \(_{d}\) is given by a linear combination of the \(}(_{j})\) and with \(a_{dj}=}}(_{j})^{}_{d}\), can be written as

\[_{d}=_{j=1}^{N}a_{dj}}(_{j}).\] (7)

### Deriving Attention from Kernel PCA

In order to derive self-attention from kernel PCA, we consider the query vector \(_{i}\) in self-attention as a new data point. The projection \(_{i}\) of a new test point \(_{i}\) onto the principal components \(_{d}\) in Eqn. (7), for \(d=1,,D_{v}\), is given by

\[_{i}(d) =(_{i})^{}_{d}=_{j=1}^{N}a_{dj} (_{i})^{}}(_{j})=_{j=1}^{N }a_{dj}(_{i})^{}((_{j})- _{j^{}=1}^{N}(_{j^{}}))\] \[=_{j=1}^{N}_{i},_{j})}{g(_{i})} {a_{dj}}{g(_{j})}-_{j^{}=1}^{N}_{i}, _{j^{}})}{g(_{i})}_{j=1}^{N}}{g(_{j^{ }})}\] \[=_{j=1}^{N}_{i},_{j})}{g(_{i})} }{g(_{j})}-_{j^{}=1}^{N}}}{g(_{j})}=_{j=1}^{N}_{i},_{ j})}{g(_{i})}v_{dj},\] (8)

where the kernel \(k(,):=()^{}()\) and \(v_{dj}:=}{g(_{j})}-_{j^{}=1}^{N}}}{g(_{j})}\). We further let the self-attention's value vectors \(_{j}=[v_{1j},,v_{D_{v}j}]^{D_{v} 1}\), \(j=1,,N\), and rewrite the projection \(_{i}\) as \(_{i}=_{j=1}^{N}k(_{i},_{j})/g(_{i})_{j}\). Selecting \(g():=_{j=1}^{N}k(,_{j})\) and \(k(,)=(^{}/)\), we obtain a formula of an attention:

\[_{i}=_{j=1}^{N}_{i},_{j})}{_{j^ {}=1}^{N}k(_{i},_{j^{}})}_{j}=_{j=1}^{N} _{i}^{}_{j}/_{j}.\] (9)

**Recovering Self-Attention:** Eqn. (9) matches the exact formula of a self-attention as in Eqn. (3). Thus, we can view outputs \(_{i}\) of self-attention as projections of the query vectors \(_{i}\), \(i=1,,N\), onto \(D_{v}\) principal components in a feature space:

\[=[(_{1}),,(_{N}) ]^{}[_{1},,_{D_{v}}].\] (10)

**Computing the Value Vectors:** As derived above, the self-attention's value vectors \(_{j}\) are given by: \(_{j}=[v_{1j},,v_{D_{v}j}]^{D_{v} 1}\), \(j=1,,N\), where \(v_{dj}:=}{g(_{j})}-_{j^{}=1}^{N}}}{g(_{j})}\), \(d=1,,D_{v}\). Since \(g(_{j})\) can be calculated as \(g(_{j})=_{j^{}=1}^{N}k(_{j},_{j^{}})\), in order to compute \(_{j}\), we need to determine the coefficients \(a_{1j},,a_{D_{v}j}\) for \(j=1,,N\).

We define \(_{}(,):=}()^{} }()\) and the Gram matrix \(}_{}^{N N}\) with elements \(}_{}(i,j)=_{}(_{i}, _{j})\). Substituting the linear expansion in Eqn. (7) into (6), we attain

\[_{j=1}^{N}}(_{j})}(_{j})^{}_{j^{}=1}^{N}a_{dj^{}}}(_{j^{}})=_{d}_{j=1}^{N}a_{dj}}( _{j}).\]We multiply both sides of the above by \(}(_{})^{}\) to obtain \(}_{}^{2}_{d}=_{d}N}_{ }_{d}\), with the column vector \(_{d}=[a_{d1},,a_{dN}]^{}^{N 1}\). We compute \(_{d}\) by solving

\[}_{}_{d}=_{d}N_{d}.\] (11)

The calculation of \(}_{}\) is provided in Remark 1. We summarize our results in the following theorem.

**Theorem 1** (Softmax Attention as Principal Component Projections): _Given a set \(M\) of key vectors, \(M:=\{_{1},,_{N}\}^{D}\), a kernel \(k(,):=(^{}/)\), and a vector-scalar function \(g():=_{j=1}^{N}k(,_{j})\), self-attention performs kernel PCA and projects a query vector \(_{i}^{D}\) onto principal component axes of \(M\) in an infinite dimensional feature space \(\) as follows_

\[_{i}=_{j=1}^{N}_{i}^{}_{j}/ _{j}.\]

_The feature space \(\) is induced by the kernel \(k_{}(,):=,)}{g()g()}\), and the value vectors \(_{j}=[v_{1j},,v_{D_{v}j}]^{D_{v} 1}\), \(j=1,,N\), where \(v_{dj}:=}{g(_{j})}-_{j^{}=1}^{N}}}{g(_{j})}\), \(d=1,,D_{v}\). The column vectors \(_{d}=[a_{d1},,a_{dN}]^{}^{N 1}\) can be determined by solving the eigenvalue problem defined in Eqn. (11). This constraint on \(_{j}\) can be relaxed by letting the self-attention learn \(_{j}\) from data via a linear projection of the input \(_{j}\), i.e., \(_{j}=_{V}_{j}\) where \(_{V}\) is a learnable matrix._

**Remark 1** (Calculating the Gram Matrix \(}_{}\)): In the eigenvalue problem defined in Eqn. (11), the centered Gram matrix \(}_{}\) can be computed from the uncentered Gram matrix \(_{}\) with elements \(_{}(i,j)=k_{}(_{i},_{j})=(_{i})^{}(_{j})\). In particular, \(}_{}=_{}-_{N}_{ }-_{}_{N}+_{N}_{} _{N}\), where \(_{N}\) denotes the \(N N\) matrix in which every element takes the value \(1/N\) (See Appendix B). Here, the kernel \(k_{}(,):=,)}{g()g()}\).

**Remark 2** (Determining \(D_{v}\)): The feature space \(\) is infinite dimensional, so we can find infinitely many principal components. However, the number of nonzero eigenvalues of \(\) in Eqn. (5) cannot exceed \(N\), the number of data points, since \(\) has rank at most equal to \(N\). Notice that only principal components corresponding to nonzero eigenvalues are used for projections in kernel PCA. Thus, \(D_{v}\), the number of principal components used for projections as in Eqn. (10), must be less than or equal to \(N\), i.e., \(D_{v} N\).

**Remark 3** (Parameterization of the Value Matrix \(\)): Different parameterizations of the value matrix \(\) can result in different self-attention architectures. For instance, the projection \(_{i}\) of a query vector \(_{i}\) onto the principal components \(_{d}\), \(d=1,,D_{v}\), in Eqn. (8) can be rewritten as

\[_{i}(d)=_{j=1}^{N}_{i},_{j})}{g(_{i})} }{g(_{j})}-_{j^{}=1}^{N}_{j^{}})}{g(_{j})}}}{g(_{j^{ }})}.\]

Letting \(v_{dj}:=}{g(_{j})}\) and \(s_{jj^{}}=_{j^{}})}{g(_{j})}\), we obtain

\[_{i}(d)=_{j=1}^{N}_{i},_{j})}{g(_{i})} v_{dj}-_{j^{}=1}^{N}s_{jj^{}}v_{dj^{}} .\]

Following the same derivation as above, we can write the projection \(_{i}\) as an attention

\[_{i}=_{j=1}^{N}_{i}^{}_{j}/ _{j}-_{j^{}=1}^{N}s_{jj^{ }}_{j^{}}.\]

The matrix form of this new attention form is as follows:

\[=^{}/(-) ,\] (12)

where \(\) is an identity matrix, and \(\) is the matrix whose elements \((j,j^{})=s_{jj^{}}\). We name the self-attention defined by Eqn. (12) the Scaled Attention. Even though the softmax attention in (2) and the Scaled Attention in (12) are mathematically equivalent according to our kernel PCA framework, the training procedure might cause the self-attention models that are derived from different parameterizations to have different performances.

### Analysis on the Convergence of Self-Attention Layers to Kernel PCA

In this section, we discuss empirical justifications that after training, the value vectors \(_{j}\) parameterized as a \(1\)-layer linear network, i.e., \(_{j}=_{V}_{j}\), \(j=1,,N\), in self-attention converge to the values predicted by our kernel PCA framework in Theorem 1. In other words, we provide evidence that the self-attention layers in transformers try to learn their value vectors \(_{j}\) to perform the kernel PCA.

#### 2.2.1 Projection Error Minimization

PCA can be formulated based on projection error minimization as well. In particular, PCA minimizes the average projection cost defined as the mean squared distance between the original data points and their projections . Given our kernel PCA framework in Theorem 1, this implies that self-attention minimizes the following projection error

\[J_{}=_{i=1}^{N}\|(_{i})-_{d=1}^ {D_{v}}h_{di}_{d}\|^{2}=_{i=1}^{N}\|( {q}_{i})\|^{2}-\|_{i}\|^{2}\,.\] (13)

In the derivation above, we leverage the orthonormality of \(\{_{d}\}_{d=1}^{D_{v}}\) and \(h_{di}=(_{i})^{}_{d}\). Here, notice that we can compute \(\|(_{i})\|^{2}\) from \(_{i}\) and \(\{_{j}\}_{j=1}^{N}\) as \(\|(_{i})\|^{2}=(_{i}^{}_{i}/)/( _{j=1}^{N}(_{i}^{}_{j}/))^{2}\). In Fig. 1, we empirically show that a transformer model minimizes the projection loss \(J_{}\) during training. Here, we train a vision transformer , ViT-tiny model in particular, on the ImageNet-1K classification task and compute the average of \(J_{}\) across attention heads and layers. This result suggests that during training, the transformer learns to perform PCA at each self-attention layer by implicitly minimizing the projection loss \(J_{}\). Thus, the value vector \(_{j}\), \(j=1,,N\), in self-attention layers converge to the values specified in Theorem 1. We provide more details on the computation of \(J_{}\) in Appendix C.

#### 2.2.2 Learning Eigenvectors of \(}_{}\)in Eqn. (11)

In this section, we study empirical results that confirm Eqn. (11). In particular, we aim to verify that after training, the value matrix \(:=[_{1},,_{N}]^{}\) captures the eigenvectors \(_{d}\), \(d=1,,D_{v}\), of the Gram matrix \(}_{}\).

We first compute \(_{d}\) in terms of \(\). Recall from Eqn. (11) that \(_{d}=[a_{d1},a_{d2},,a_{dN}]^{}\). We denote the diagonal matrix \(:=(1/g(_{1}),,1/g(_{N}))\), the matrix \(:=[_{1},,_{D_{v}}]\), and rewrite the value vectors \(_{j}\), \(j=1,,N\), as follows:

\[_{j}=_{1}[j]}{g(_{j})}-_{j^{ }=1}^{N}_{1}[j^{}]}{g(_{j})},,_{D_{v}}[j]}{g(_{j})}-_{j^{}=1}^{N}_{ D_{v}}[j^{}]}{g(_{j})}.\]

The value matrix \(\) in self-attention is then given by

\[=-_{N}=(-_{N})^{-1}^{-1}\]

Thus, given the value matrix \(=_{V}^{}\) that the self-attention learns after training, the estimation \(}_{d}\) of \(_{d}\) can be computed as

\[}_{d}=(-_{N})^{-1}^{-1}[:,d].\]

We empirically verify that \(}_{}}_{d}}{N}_{d}}= =[_{1},,_{N}]\) where \(_{1}==_{N}=\), which confirms that \(}_{d}\) is an eigenvector of \(}_{}\). In particular, in Fig. 2(Left), we plot the average pairwise

Figure 1: Projection loss vs. training epochs of ViT-tiny model. The reconstruction loss is averaged over the batch, heads, and layers. The downward trend suggests that the model is implicitly minimizing this projection loss.

absolute differences of \(_{i}\) and \(_{j}\), \(i j\), \(i,j=1,,N\), for each principal component axis of \(}_{}\). The results are averaged over all attention heads and all layers in the 12-layer ViT-tiny model trained on ImageNet-1K. As can be seen in our figure, the absolute difference between any pair of \(_{i}\) and \(_{j}\) is almost 0 with a very small standard deviation. Similar results are observed at each layer when averaging over all attention heads in that layer. In Fig. 2(Right), we show the results for Layers 2, 5, 8, and 11 in the model. For comparison, we observe that the max, min, mean, and median of the absolute values of these \(D_{v}\) eigenvalues, averaged over all attention heads and layers, are 648.46, 4.65, 40.07, and 17.73, respectively, which are much greater than the values of \(|_{i}-_{j}|\). These results empirically justify that \(}_{}}_{d}}{N}_{d}}=\) const and the value matrix \(\) captures the eigenvectors of \(}_{}\) after the transformer model is trained, as suggested in Eqn. (11).

In order to prove that after training a transformer with the value vectors \(_{j}\) parameterized as \(_{V}_{j}\), \(j=1,,N\), using stochastic gradient descent, \(v_{dj}\) converges to \(}{g(_{j})}-_{j^{}=1}^{N}}}{g(_{j})}\) as stated in Theorem 1, it is sufficient to prove that after the training, the outputs \(_{i}\) of self-attention become projections of the query vectors \(_{i}\), \(i=1,,N\), onto \(D_{v}\) principal component axes in the feature space \(\), i.e., the eigenvectors \(_{1},,_{D_{v}}\) of the covariance matrix \(\). To theoretically prove this result for a multi-layer multi-head softmax transformer trained to explicitly minimize a particular loss, e.g., cross-entropy or L2 loss, using stochastic gradient descent is indeed challenging due to the highly nonconvex nature of the optimization problem and the nonlinear structure of the model. Our experimental results in Section 2.2.1 and 2.2.2 empirically justify this result and serve as guidance for the theoretical proof, which we leave for future work.

## 3 Robust Softmax Attention

In this section, we employ our kernel PCA framework in Section 2 to derive a new class of robust attention, namely, _Attention with Robust Principal Components_ (RPC-Attention). It is a well-known problem that both PCA and kernel PCA are sensitive to grossly corrupted data routinely encountered in modern applications [11; 49; 34; 19]. Since self-attention performs kernel PCA by projecting the query vectors \(_{i}\), \(i=1,,N\), onto principal components in a feature space as derived in Section 2.1, it is also vulnerable to data corruption and perturbation. Our RPC-Attention robustifies self-attention by solving a convex program known as Principal Component Pursuit (PCP) .

### Principal Component Pursuit

Given corrupted measurement matrix \(^{N D}\), both PCA and PCP aim to recover a low-rank matrix \(^{N D}\) from \(\). However, while PCA models the corruption by a small Gaussian noise term, PCP models the corruption by a matrix \(^{N D}\) that can have arbitrarily large magnitude with sparse supports. In particular, PCP solves the following convex optimization problem:

\[_{,}\|\|_{*}+\|\|_{1} +=,\]

Figure 2: Mean and standard deviation of the absolute differences of elements in the constant vector \(_{d}\), \(d=1,,D_{v}\). The mean should be \(0\) with small standard deviations when \(v_{dj}\) are close to the values predicted in Theorem 1. For comparison, we observe that the max, min, mean, and median of the absolute values of all the eigenvalues, averaged over all attention heads and layers, are 648.46, 4.65, 40.07, and 17.73, respectively, which are much greater than the values of \(|_{i}-_{j}|\).

where \(\|\|_{*}\) is the nuclear norm of \(\), i.e., the sum of the singular values of \(\), and \(\|\|_{1}=_{id}|S_{id}|\) is the \(_{1}\)-norm of \(\). From , under minimal assumptions on the rank and sparsity of \(\) and \(\), the PCP solution exactly recovers the low-rank component \(\) and the sparse component \(\). _Since PCP can recover the principal components of a data matrix even when a positive fraction of the measurements are arbitrarily corrupted, it is more robust than PCA._

### Attention with Robust Principal Components

In self-attention, following our kernel PCA framework in Section 2, the dataset \(M\) is given as \(M=\{_{1},,_{N}\}^{D}\) and \(_{1},,_{N}\) are key vectors. Thus, the key matrix \(:=[_{1},,_{N}]^{}^{N D}\) in self-attention can be set as the measurement matrix \(\) in PCP. Then, the PCP for self-attention can be formulated as

\[_{,}\|\|_{*}+\|\|_{1} +=.\] (14)

Following , we utilize the Alternating Direction Method of Multipliers (ADMM) algorithm introduced in [41; 91] to solve the convex program (14). The augmented Lagrangian of (14) is

\[(,,)=\|\|_{*}+\|\|_{1}+ ,--+/2\|--\|_{F}^{2}.\]

An ADMM solves the convex program (14) by iterating the following steps until convergence:  setting \(_{k+1}=_{}(_{k},,_{k})\),  setting \(_{k+1}=_{}(,_{k+1},_{k})\), and  updating the Lagrange multiplier matrix \(_{k+1}=_{k}+(-_{k+1}-_{k+1})\). We define \(_{}(x):=(x)(|x|-,0)\) as an element-wise shrinkage operator and \(_{}()=_{}()^{*}\) as a singular value thresholding operator with the singular value decomposition of \(=^{*}\). As proven in , we can rewrite steps  and  as

\[_{}(,,)=_{/}( -+^{-1});\;_{}(,, {Y})=_{}(--^{-1}).\]

\(_{}\) finds a low-rank approximation of \(--^{-1}\). Thus, we obtain an approximation of the above equation by replacing \(_{}\) by a low-rank approximation operator. Such an approximation takes a step towards the minimum of \((,,)\) when fixing \(\) and \(\). It has been empirically observed and theoretically proven that the output matrix \(\) of self-attention is low-rank, a phenomenon known as over-smoothing or rank collapse [71; 86; 21]. Therefore, we can replace \(_{}\) by a self-attention operator. The ADMM method applied to self-attention, which we name _Principal Attention Pursuit_ (PAP), is given by Algorithm 1. We define our RPC-Attention as

``` initialize:\(_{0}=_{0}=\); \(,>0\). while not converged do  compute \(_{k+1}=_{/}(-_{k}+^{-1}_{k})\);  compute \(_{k+1}=(-_{k+1}-^{-1}_{k},- {S}_{k+1}-^{-1}_{k})\);  compute \(_{k+1}=_{k}+(-_{k+1}-_{k+1})\); endwhile output:\(\). ```

**Algorithm 1** Principal Attention Pursuit (PAP)

**Definition 1** (Attention with Robust Principal Components): _An RPC-Attention performs the PAP in Algorithm 1 for \(n\) iterations with \(\) as a hyperparameter. For the key matrix \(^{N D}\), RPC-Attention sets \(=ND/4\|\|_{1}\) as suggested in , where \(\|\|_{1}=_{id}|K_{id}|\). The output matrix \(\) of RPC-Attention is set to be the low-rank output matrix \(\) from PAP._

## 4 Experimental Results

We aim to numerically show that: (i) RPC-Attention achieves competitive or even better accuracy than the baseline softmax attention on clean data, and (ii) the advantages of RPC-Attention are more prominent when there is a contamination of samples across different types of data and a variety of tasks. We also validate the performance of the Scaled Attention proposed in Remark 3.

Throughout our experiments, we compare the performance of our proposed models with the baseline softmax attention of the same configuration. All of our results are averaged over 5 runs with different seeds and run on 4 A100 GPU. Details on the models and training settings are provided in Appendix A and additional experimental results are provided in Appendix E. Primarily, we focus on a ViT-tiny model backbone , but included in the appendix are experiments on a larger model backbone, ViT-base, and a state of the art (SOTA) robust model, Fully Attentional Networks (FAN) .

### Vision Tasks: ImageNet-1K Object Classification

We implement PAP in Algorithm 1 in the symmetric softmax attention layers of a ViT-tiny model and compare it to the standard symmetric model as our baseline. We refer to our model as RPC-SymViT and the baseline model as SymViT. For RPC-SymViT, we study two settings. In the first setting, which we denote by RPC-SymViT (_n_iter/layer1), \(n=4,5,6\), to maintain the computational efficiency of the model, we apply \(n\) PAP iterations only at the first layer to recover a cleaner data matrix that is then sent to the subsequent layers in the model. In the second setting, which we denote by RPC-SymViT (_n_iter/all-layer), \(n=2\), we apply \(n\) PAP iterations at all layers. We note that an iterative scheme has the potential to have an increased computational load, hence, we provide a comparison on the number of flops per sample, run time per sample, memory and number of parameters between RPC-SymViT and the baseline in Appendix E.9, showing a comparable efficiency.

Robustness against Data Corruptions.To benchmark robustness to data corruptions, we use the standard datasets, ImageNet-C (IN-C) , ImageNet-A (IN-A), ImageNet-O (IN-O) , and ImageNet-R (IN-R) . We provide details on each dataset and the metrics for evaluation in Appendix A.1. The direction of increasing or decreasing values of these metrics signifying greater robustness are indicated in the Table 1 with an arrow, along with the results on each dataset.

Across all evaluations, RPC-SymViT outperforms the SymViT baseline, thereby justifying the advantages of our RPC-Attention. Particularly, RPC-SymViT with 6 iterations in the 1st layer achieves an improvement of over 1% in terms of accuracy on the clean ImageNet-1K validation set and almost 3 AUPR on ImageNet-O compared to the SymViT. This result is consistent with the intuition that a higher number of iterations executed on a consistent data matrix leads to cleaner data. The full details of all corruption types are presented in Appendix E.3.

    &  &  &  &  &  \\  & Top-1 \(\) & Top-5 \(\) & Top-1 \(\) & Top-1 \(\) & Top-1 \(\) & mCE \(\) & AUPR \(\) \\  SymViT (baseline) & 70.44 & 90.17 & 28.98 & 6.51 & 41.45 & 74.75 & 17.43 \\  RPC-SymViT (4iter/layer1) & 70.94 & 90.47 & 29.99 & 6.96 & 42.35 & 73.58 & 19.32 \\ RPC-SymViT (5iter/layer1) & 71.31 & 90.59 & **30.28** & 7.27 & 42.43 & 73.43 & **20.35** \\ RPC-SymViT (6iter/layer1) & **71.49** & **90.68** & 30.03 & 7.33 & **42.76** & **73.03** & 20.29 \\  RPC-SymViT (2iter/all-layer) & 70.59 & 90.15 & 29.23 & **7.55** & 41.64 & 74.52 & 19.18 \\   

Table 1: Top-1, Top-5 accuracy (%), mean corruption error (mCE), and area under the precision-recall curve (AUPR) of RPC-SymViT and SymViT on clean ImageNet-1K data and popular standard robustness benchmarks for image classification. RPC-SymViT (_n_iter/layer1) applies \(n\) PAP iterations only at the first layer. RPC-SymViT (_n_iter/all-layer) applies \(n\) PAP iterations at all layers. RPC-SymViT (_n_iter/all-layer) applies \(n\) PAP iterations at all layers.

    &  &  SymViT \\ (baseline) \\  } & RPC-SymViT & RPC-SymViT & RPC-SymViT & RPC-SymViT & RPC-SymViT \\  & & (4iter/layer1) & (5iter/layer1) & (6iter/layer1) & (2iter/all-layer) \\   & Top-1 \(\) & 4.98 & 5.15 & 5.11 & 5.20 & **6.12** \\  & Top-5 \(\) & 10.41 & 11.20 & 11.13 & 11.34 & **13.24** \\   & Top-1 \(\) & 23.38 & 26.62 & 26.75 & 27.22 & **29.20** \\  & Top-5 \(\) & 53.82 & 56.87 & 57.19 & 57.55 & **59.63** \\   & Top-1 \(\) & 47.94 & 48.13 & 49.29 & 48.75 & **51.01** \\  & Top-5 \(\) & 82.63 & 82.87 & 83.52 & 83.27 & **83.66** \\   & Top-1 \(\) & 67.91 & 68.48 & 68.60 & **68.99** & 67.60 \\  & Top-5 \(\) & 89.68 & 90.16 & 90.18 & **90.38** & 89.79 \\   & Top-1 \(\) & 48.44 & 50.00 & 50.08 & **50.36** & 48.77 \\  & Top-5 \(\) & 72.68 & 74.24 & 74.14 & **74.46** & 72.91 \\   & Top-1 \(\) & 67.81 & 68.37 & 68.72 & **68.81** & 68.09 \\  & Top-5 \(\) & 88.51 & 89.00 & 89.13 & **89.27** & 88.64 \\   & Top-1 \(\) & 23.09 & 24.56 & 24.68 & **24.74** & 23.51 \\  & Top-5 \(\) & 63.48 & 65.11 & 65.09 & **65.13** & 64.06 \\   

Table 2: Top-1/5 accuracy (%) on attacked ImageNet-1K data. RPC-SymViT (_n_iter/layer1) applies \(n\) PAP iterations at all layers.

**Robustness under Adversarial Attacks.** We report the top-1 and top-5 accuracy of the models on ImageNet-1K distorted by white box attacks, including PGD , FGSM , SLD , and CW  attacks. We further examine the models on the black-box attack SPSA , AutoAttack , and a noise-adding attack. In Table 2, we present the results of all attacks, where PGD and FGSM are reported for the maximum perturbation budget.

On all attacks, RPC-SymViT outperforms SymViT by a substantial margin, demonstrating the effectiveness of RPC-Attention. Notably, as the square attack in AutoAttack is also a score-based black box attack, in order to further verify our method, we include the results of RPC-SymViT (6iter/layer1) and the baseline SymViT on this attack in the Appendix E.4 in which our model also performs better. This result together with our model's considerable improvement over the baseline on the black-box SPSA attack justifies that RPC-Attention's robustness against adversarial attacks is not due to any form of gradient masking. In addition, we illustrate RPC-SymViT's robustness across increasing perturbations for PGD and FGSM in Fig. 3(left). Details on the evaluation of the models under all attacks are provided in Appendix A.2.

**ADE20K Image Segmentation.** We continue evaluating the benefits of our method by implementing RPC-Attention in a Segmenter model  and providing results on the ADE20K image segmentation task . Table 6 in Appendix A.3 shows that Segmenter with the RPC-SymViT backbone outperforms the Segmenter with the baseline SymViT backbone on both clean and corrupted data.

### Language Tasks: WikiText-103 Language Modeling

We assess our model on the large-scale WikiText-103 language modeling task . Using a standard transformer language model , with a symmetric attention layer (Symlm), we replace it with an RPC-Attention layer (RPC-Symlm). As with RPC-SymViT, we only implement RPC-Attention in the first four layers of Symlm and run PAP for 4 iterations to save on computational overhead. The results of the validation and test perplexity (PPL) summarized in Table 3 validate the advantage of our method, surpassing the baseline by at least 1 PPL for all datasets.

### Validating the Benefits of Scaled Attention

In this section, we provide an empirical comparison between the Scaled Attention in Remark 3 and the softmax attention. We train an asymmetric ViT-tiny model with two different versions of Scaled Attention. While the exact value of \(\) can be mathematically formulated as in Remark 3, it

Figure 3: **Left: Top-1 accuracy of RPC-SymViT vs. baseline SymViT evaluated on PGD/FGSM attacked ImageNet-1K validation set across increasing perturbation budgets. Right: Validation top-1 accuracy (%) and loss of Scaled Attention vs. the baseline asymmetric softmax attention in ViT for the first 50 training epochs.**

    &  &  \\  & Valid-ppl \(\) & Test-ppl \(\) & Valid-ppl \(\) & Test-ppl \(\) \\  Symlm & 38.37 & 36.36 & 42.90 & 44.32 \\  RPC-Symlm &  &  &  &  \\ (4iter/layer1-4) & & & & \\   

Table 3: Validation/test perplexity (PPL) on clean WikiText-103 and word swap attacked dataset. RPC-Symlm (_niter_/layer1_\(n\)-\(n_{2}\)) applies \(n\) iterations of PAP only in layers \(n_{1}\) to \(n_{2}\) of the model.

might lead to numerical errors that are difficult to handle. Therefore, in the first version of Scaled Attention, we let \(\) be a learned parameter matrix in each layer (Scaled Attention \(S\) in Fig. 3(right)). In the second version of Scaled Attention (Scaled Attention \(\) Asym in Fig. 3(right)), given that \(s_{jj^{}}=g(k_{j^{}})/g(k_{j})\), we rewrite \(\) as the product of a symmetric softmax attention matrix and the reciprocal of its transpose. The model learns this reciprocal by a learnable scalar \(\) and we let \(=_{Sym}\), where \(_{Sym}\) is the symmetric softmax attention. More details are in Appendix E.1.

Fig. 3(right) shows the top-1 validation accuracy and loss over 50 epochs when training ViT models equipped with Scaled Attention and softmax attention on the ImageNet-1K object classification task. The full training curve can also be found in Appendix E.1. The results suggest that both versions of Scaled Attention outperform the softmax attention. This provides further evidence that self-attention learns to approximate a kernel PCA since the Scaled Attention with a more explicit structure of the value matrix \(\) suggested in Theorem 1 obtains better performance than softmax attention.

## 5 Related Works

**Theoretical Perspectives of Attention Mechanisms.** The study of the attention mechanism in transformers through different theoretical frameworks has been expanding.  shows that attention can be analyzed as a kernel smoother over the inputs using an appropriate kernel score that is the similarities between them. [16; 87; 37; 55] reduce the quadratic complexity of transformers by linearizing the softmax kernel to improve the computational and memory efficiency. In addition, there are works interpreting transformers using the frameworks of ordinary/partial differential equations [50; 51; 44; 69; 27; 26] and from probabilistic viewpoints with Gaussian Mixture Models [52; 23; 75; 54; 92].  provides a new perspective by emphasizing the asymmetry of the softmax kernel and recovers the self-attention mechanism from an asymmetric Kernel Singular Value Decomposition (KSVD) using the duality of the optimization problem. Another related work views transformers from the perspective of Support Vector Machines [57; 76]. We discuss  and , as well as the approaches that use matrix decomposition and iterative algorithms in deep models, in Appendix F. Separate from these works, our kernel PCA perspective derives softmax attention as a projection of the query vectors in a feature space. Using our framework, we are able to predict the exact explicit form of the value matrix in self-attention, demonstrating that this matrix captures the eigenvectors of the Gram matrix of the key vectors in a feature space. Our work is the first to show this insight.

**Robustness of Transformers.** There have been many works studying the robustness of Vision Transformers (ViT)  against different types of attacks [7; 61; 73; 96]. Recent work that serves to address this include , whereby new training strategies and architectural adjustments are proposed to improve the robustness of ViT. In addition,  suggests employing a Mahalanobis distance metric to calculate attention weights, expanding the feature space along directions of high contextual relevance, thereby enhancing the model's robustness. Also,  adapts traditional robust kernel density estimation techniques to create new classes of transformers that are resilient to adversarial attacks and data contamination. [14; 10] integrate a Gaussian process into attention for out-of-distribution detection, and  develops equivariant neural functional networks for transformers. Our RPC-Attention is orthogonal to these methods.

## 6 Concluding Remarks

In this paper, we derive self-attention from kernel principal component analysis (kernel PCA) as a projection of the query vectors onto the principal component axes of the key matrix in a feature space. Using our kernel PCA framework, we derive a new class of robust attention, namely the Attention with Robust Principal Components (RPC-Attention), that is resilient to data contamination. A limitation of RPC-Attention is its derivation from an iterative algorithm that leads to its unrolling architecture, increasing the computational cost of the model. In our paper, we mitigate this by only replacing softmax attention with RPC-Attention in the first layer of the model and demonstrate that doing so is sufficiently effective for robustness. In addition, we provide a comparison of the efficiency of RPC-Attention with softmax attention in Appendix E.9 and find that we are comparable across all metrics at test time while only slightly less efficient during training. It is also interesting to extend our kernel PCA framework to explain multi-layer transformers. We leave these exciting directions as future work.