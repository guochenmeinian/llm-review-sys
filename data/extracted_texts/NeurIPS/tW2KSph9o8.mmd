# Ignorance is Bliss:

Robust Control via Information Gating

Manan Tomar

University of Alberta

&Riashat Islam

McGill University

&Matthew E. Taylor

University of Alberta

&Sergey Levine

University of California, Berkeley

&Philip Bachman

Microsoft Research Montreal

Correspondence to manan.tomar@gmail.com

###### Abstract

Informational parsimony provides a useful inductive bias for learning representations that achieve better generalization by being robust to noise and spurious correlations. We propose _information gating_ as a way to learn parsimonious representations that identify the minimal information required for a task. When gating information, we can learn to reveal as little information as possible so that a task remains solvable, or hide as little information as possible so that a task becomes unsolvable. We gate information using a differentiable parameterization of the signal-to-noise ratio, which can be applied to arbitrary values in a network, e.g., erasing pixels at the input layer or activations in some intermediate layer. When gating at the input layer, our models learn which visual cues matter for a given task. When gating intermediate layers, our models learn which activations are needed for subsequent stages of computation. We call our approach _InfoGating_. We apply InfoGating to various objectives such as multi-step forward and inverse dynamics models, Q-learning, and behavior cloning, highlighting how InfoGating can naturally help in discarding information not relevant for control. Results show that learning to identify and use minimal information can improve generalization in downstream tasks. Policies based on InfoGating are considerably more robust to irrelevant visual features, leading to improved pretraining and finetuning of RL models.

## 1 Introduction

Pretraining models on large, diverse datasets and transferring their representations to downstream applications is becoming common practice in deep learning. Such representations should capture useful information from available data while ignoring irrelevant features and noise . For instance, an object recognition model may achieve high accuracy on its training set while strongly relying on "spurious" correlations between background features and important objects. When transferred to

Figure 1: **InfoGating** at the input layer predicts a mask conditioned on the original input and then processes the masked input to compute representations that minimize a downstream loss. The mask is encouraged to remove as much of the original input as possible without hurting performance on the downstream task. The mask is applied by taking a convex combination of the input and Gaussian noise, weighted by the mask.

new data with the same objects but a different distribution of backgrounds, performance may suffer. Similarly, a policy for vision-based robot control [23; 10] may perform well in its original training environment, but fail catastrophically when transferred to a new setting  where all task-relevant features are the same but minor background features differ from training.

A promising principle for encouraging robustness to out-of-distribution observations is informational parsimony: learning features that contain minimal information  about the input while still containing enough information to solve the task(s) of interest. Such regularization is usually based on imposing an _information bottleneck_ (IB)  on the network that tries to minimize the amount of information flowing from the input, through the bottleneck, to the output prediction. Although IB approaches have been beneficial in some cases, adding an information bottleneck at the penultimate step of computation does little to prevent overfitting in the preceding steps of computation, which typically comprise the overwhelming bulk of the model. To that end, _we consider restricting the information used throughout a computation rather than just restricting the information that comes out of a computation_.

In this paper, we propose learning input-conditioned functions that _gate_ the flow of information through a model. For example, we can consider what happens when one inserts an IB near the beginning of a model's computation rather than near the end. In this case, we train the information gating functions to minimize information flow from the input into the rest of the model while still permitting the model to solve the task(s) of interest. We implement this gating using a differentiable parameterization of the signal-to-noise ratio. As the noise level goes up, the signal level goes down, and hence the model displays reduced dependencies on the input. Gating functions can be learnt in conjunction with any downstream loss corresponding to some task of interest. For example, the downstream loss could be a standard contrastive loss for self-supervised learning or an inverse dynamics loss when learning representations for reinforcement learning (RL). In both cases, we can learn gating functions that _reveal minimal information_ used to optimize the loss or learn adversarial gating functions that will _remove any information_ that can be used to optimize the loss. We primarily focus on InfoGating representations used for learning control policies in RL. A natural focus of the RL paradigm is on learning a mapping from observations to actions, thus discarding most information not useful for control. We bring forth such a notion of only capturing what the agent can affect through InfoGating. Using background distractors and multi-object interactions as a form of noise/irrelevant features, we see that InfoGating is able to remove almost all of the irrelevant information from the pixels, leading to better out-of-distribution generalization in the presence of noise and better in-distribution generalization in the presence of multiple task-irrelevant objects.

Our main contributions are as follows: **1.** A general purpose, practical framework for informational parsimony called InfoGating that can restrict information flow throughout a computation to learn robust representations. **2.** Qualitative analyses on the properties of the gating functions learned with InfoGating that show they are semantically meaningful and enhance intrepretability. **3.** Quantitative analyses of applying InfoGating in the context of various downstream objectives which show clear benefits in terms of improved generalization performance.

## 2 Related Work

**Information Bottleneck**. Much prior work in learning robust representations has stemmed from the idea of imposing an information bottleneck (IB) [38; 39]. Imposing an IB involves maximizing performance on the downstream task, while removing as much information about the input as possible from a network's internal representations. Typical approaches based on Variational IB  achieve this by learning stochastic representations that are constrained to be close to a standard Gaussian prior. Variants of dropout [36; 19] can have a similar effect (i.e., adding noise to representations used in the network) but without explicitly maximizing noise/minimizing information in the representations. While IB is typically applied to later stages of a network, InfoGating can be seen as enforcing an IB at the input itself, which is less explored in the IB literature. Like Variational IB methods, and unlike typical dropout-based methods, InfoGating learns how much noise can be added without hurting performance on the downstream task(s). Information Dropout  is perhaps the most similar prior work to InfoGating. Information Dropout imposes IBs at multiple points in a network's computation and the IBs are implemented using techniques based on variational dropout . A key distinction between InfoGating and Information Dropout is that InfoGating works post-hoc. With InfoGating a model can consider a computation it has already performed and predict ways in which it could produce the same result more parsimoniously.

**Masked Image Modelling**. There has been a lot of recent work in self-supervised learning where the input is masked and the model is tasked with reconstructing the missing bits [3; 15; 41]. Most of these works operate with a fixed, random masking scheme, instead of learning it directly through the downstream loss. The amount of information masked is also fixed _a priori_ whereas we seek to mask as much of the input as possible, while still retaining enough information to solve the task. Some related ideas include learning adversarial augmentations for producing new views of an input during contrastive learning , while also learning masks that essentially segment the input space .

**Representation Learning in Reinforcement Learning**. Prior work on learning representations for RL includes temporally contrastive learning [33; 27], one-step inverse models , learning through next state reconstruction , bisimulation metrics , and many more. Some of these objectives have been shown to be useful in learning robust representations , and we see our method as complementary to these approaches since InfoGating works with any downstream loss function, without needing knowledge of specific values like reward, next state, etc. For instance, bisimulation and task-informed abstractions  learn to compress the observation space using reward information, while recent work on learning Denoised MDPs  aims at regularizing next state reconstruction using a variational objective. On the other hand, InfoGating remains agnostic to choices of downstream objectives.

## 3 Background

This section describes the primary downstream losses we use with InfoGating in this paper, namely an InfoNCE based contrastive loss and a multi-step inverse dynamics loss.

**Mutual Information Estimation via InfoNCE**. Given \(\) and \(\) as two random variables, their mutual information can be defined as the decrease in uncertainty when observing \(\) given \(\), compared to just observing \(\): \(I(,)=H()-H()\), where \(H\) is the Shannon entropy. InfoNCE , based on Noise Contrastive Estimation , computes a lower bound on \(I(^{1},^{2})\), where \(^{1}\) and \(^{2}\) are two representations of the input \(\) produced by some encoder \(f()\). Specifically, the bound is optimized by discriminating "positive" and "negative" pairs:

\[_{}=_{Z^{-}}[^{1},\ ^{2})}}{e^{(^{1},\ ^{2})}+_{^{-} Z^{-}}e^{(^{1},\ ^{-})}}],\] (1)

where \(\) is a pairwise, scalar-valued function of the representations and \(Z^{-}\) is a batch of "negative samples". Typically, self-supervised contrastive learning uses data augmentation such as random cropping and color jittering to define two augmented views \((^{1},\ ^{2})\) of a given input as "positives" while views \(^{-}\) of other inputs are treated as "negatives". The InfoNCE objective then encourages the representations of positive views of \(\) to be close (i.e., \((^{1},\ ^{2})\) is high), while pushing apart the negatives (i.e., \((^{1},\ ^{-})\) is low) [4; 5].

**Multi-Step Inverse Dynamics Models**. Multi-step inverse dynamics models  predict the action(s) that took an agent from some observation \(_{t}\) to some future observation \(_{t+k}\), attempting to learn a useful representation of the observations. This resembles learning goal-conditioned policies through relabelling future observations as achieved goals. Although the trivial case of \(k=1\) (a one-step model) does not capture long term dependencies, recent work has shown that multi-step models are able to capture more information that may be useful for controlling the agent . Multi-step inverse models can be used to learn representations by simply predicting the actions \((_{t},...,_{t+k-1})\) conditioned on \(_{t}\) and \(_{t+k}\). Actions can be predicted using a standard max likelihood objective or with a contrastive reconstruction objective which maximizes the InfoNCE-based lower bound on \(I((_{t},_{t+k}),(_{t},...,_{t+k-1}))\). The learning objective in this case looks as follows:

\[=(_{t},_{t+k}),_ {t:t+k-1},\]

where \(_{t}\) and \(_{t+k}\) are representations computed from \(_{t}\) and \(_{t+k}\), and \(_{t:t+k-1}\) is a representation computed from \((_{t},...,_{t+k-1})\). The negative samples in this case come from randomly sampling action sequences of the appropriate length from the agent's collected experience. In the simplest case, the representation \(_{t:t+k-1}\) may depend only on \(_{t}\).

Information Gating

We present two approaches to InfoGating that we describe as _cooperative_ and _adversarial_. Cooperative InfoGating involves keeping as little information as possible to obtain good performance on the downstream task. Adversarial InfoGating involves removing as little information as possible to preclude good performance on the downstream task. We include both approaches in this paper since identifying _minimal sufficient information_ (cooperative) and identifying _any useful information_ (adversarial) may have different affects depending on the downstream loss and the application domain.

The InfoGating approach has two major components. The first is an encoder of the input \(\), \(f()\). The second is an information gating function, \(ig()\) (see Figure 1). In principle, we can gate information passing through any layer in the representation network that computes the encoding \(=f()\). The \(ig()\) function provides continuous-valued masks (values in \(\)) that describe where to erase information from the computation graph for \(f()\). The shape/size of the output of \(ig()\) will depend on where we want to gate information. For instance, if we wish to gate information in the input pixel space, \(ig(x)\) masks are the size of the image. In general, the goal of \(ig()\) is to erase as much information from \(f()\) as possible without hurting task performance.

### Cooperative InfoGating

We primarily focus on cooperative InfoGating in this paper and we consider two cooperative variants:

**InfoGating in Input Space**. We apply InfoGating to an input \(\) by taking a simple convex combination of \(\) and random Gaussian noise \(\). The combination weights are given by \(ig()\):

\[^{ig}=ig()+(1-ig()),\] (2)

where \(^{ig}\) denotes the info-gated input and \(\) denotes element-wise multiplication. An all-zero mask corresponds to complete noise (i.e., erasing all information from the input), while an all-one mask corresponds to keeping the original input. The function \(ig(x)\) is learnt using the same downstream objective that is used to learn \(f()\). We encourage masks to remove information from the input by minimizing their L1 norm. This tends to produce sparse masks due to properties of the L1 norm . The overall objective for learning with InfoGating is

\[_{ig,f}=_{}f(^{ig})+ \;||ig()||_{1},\] (3)

where \(_{}\) refers to any objective through which a useful representation \(\) can be learnt. Note that when doing InfoGating, we minimize the downstream loss for the masked input \(^{ig}\) (first term), instead of the original input \(\), while masking out as much of the input as possible through the L1 penalty (second term). The \(\) coefficient is a hyperparameter that controls how much of the input is masked. In principle, any loss function can be used in conjunction with InfoGating and we consider multiple downstream objectives: contrastive learning of dynamic, Q-learning, and behavior cloning. We describe the overall procedure in Algorithm 1, which assumes contrastive multi-step inverse dynamics modeling as the downstream objective.

**InfoGating in Feature Space**. As an alternative to directly gating the input pixels, we can also consider a variant where \(f()\) is masked instead of the input. Consider the following masking:

\[^{ig}=ig()+(1-ig()) ,=f().\] (4)

The training objective remains the same as in the pixel-level case, i.e., minimize the downstream loss \(_{}(^{ig})\) for the masked representation \(^{ig}\), while masking as much of \(\) as possible (Figure 2). This version is closely related to the deep variational information bottleneck (VIB), which minimizes the KL divergence between the (stochastic) representation \(\) and a prior distribution, typically chosen to be a standard Gaussian. Roughly, the values in \(ig()\) can be seen as specifying how many steps of forward diffusion to run in a Gaussian diffusion process initiated at \(\), where values near zero correspond

Figure 2: **Feature InfoGating**. gates representations produced by the encoder. The gating network and the encoder are jointly trained to minimize the downstream loss, while the gating network is also encouraged to mask as much of \(\) as possible.

[MISSING_PAGE_FAIL:5]

We test **1)** and **2)** on the offline visual D4RL domain  and **3)** on the Kitchen  manipulation domain. InfoGating can also be used in non-RL settings like self-supervised learning, as discussed in Appendix A. We now describe the details of each use of InfoGating and the corresponding experimental results. Hyper-parameters are listed in Appendix E.

### Inverse Dynamics Models

We use multi-step inverse models as our primary downstream loss due to its ability to recover the latent state effectively [17; 22]. Consider the contrastive loss based on InfoNCE as described in Eq. 1. To apply InfoGating, we mask both the current observation \(_{t}\) and the goal observation \(_{t+k}\) using the masks from \(ig(_{t})\) and \(ig(_{t+k})\) respectively. We then process both masked observations through the encoder to compute the corresponding representations \(_{t}^{ig}=f(_{t}^{ig})\) and \(_{t+k}^{ig}=f(_{t+k}^{ig})\). These are then optimized through a loss similar to Eq. 3:

\[=((_{t}^{ig},_{t+k}^{ig}),_{t})+(||ig(_{t})||_{1}+||ig(_{t+k})||_{1})\] (6)

Note that both current and future observation masks are penalized through the L1 term. We test the inverse model with InfoGating on datasets consisting of offline observation-action pairings. The datasets contain pixel-based observations with video distractors playing in the background . We first pretrain the representations with the InfoGating objective in Eq. 6 and then perform behavior cloning (BC) using a 2-layer MLP over the frozen representations. We have tried replacing BC-based evaluation with, e.g., Q-Learning-based evaluation, which leads to similar relative scores. We report scores for BC-based evaluation for easier reproducibility and reduced dependence on hyperparameters.

We work with three levels of distractor difficulty: _easy_, _medium_, and _hard_. These levels correspond to the amount of noise added to the observations. At evaluation time, a noise-free observation space is used, thus creating an out-of-distribution shift in the distractor. We use random cropping as a pre-processing step for the observations in all our experiments. We compare inverse dynamics with InfoGating to six baselines: control specific methods, i.e. 1) IQL , 2) DRIML , and 3) inverse dynamics without InfoGating (the standard formulation from Eq. 1), and regularization specific methods that work on top of inverse dynamics by adding 4) Dropout, 5) VIB bottleneck, 6) adversarial learning (RCAD)  and 6) random masks rather than learnable masks as in InfoGating. See Table 16 for results.

We observe that InfoGating leads to considerable performance gains over all baselines. Although adding random masking is better than no masking, learnable masks lead to much better performance.

 case & IQL & DRIML & Inv & Inv w/ Dropout & Inv w/ VIB & Inv w/ RCAD & Inv w/Rand & Inv w/ IG \\   \\  easy & 10.7 \(\) 8.0 & 0.9 \(\) 0.1 & 42.6 \(\) 36.3 & 11.79 \(\) 3.5 & 97.1 \(\) 17.9 & 25.0 \(\) 3.6 & 21.0 \(\) 15.6 & **176.2 \(\) 9.1** \\ medium & 29.2 \(\) 28.8 & 1.0 \(\) 0.6 & 29.5 \(\) 31.9 & 73.3 \(\) 12.3 & 38.9 \(\) 16.9 & 19.9 \(\) 24.8 & 7.2 \(\) 6.12 & **97.0 \(\) 5.7** \\ hard & 8.8 \(\) 1.9 & 13.8 \(\) 7.9 & 2 \(\) 0.6 & 5.9 \(\) 5.7 & 34.0 \(\) 19.4 & 1.4 \(\) 0.4 & 4.2 \(\) 0.7 & **44.8 \(\) 18.4** \\  overall & 16.2 & 5.2 & 24.7 & 30.3 & 56.6 & 15.4 & 10.8 & **106.0** \\   \\  easy & 42.6 \(\) 26.4 & 2.6 \(\) 0.2 & 45.6 \(\) 23.2 & 28.0 \(\) 23.6 & 113.5 \(\) 11.8 & 29.6 \(\) 24.5 & 42.0 \(\) 31.8 & **133.0 \(\) 12.8** \\ medium & 31.4 \(\) 24.7 & 86.6 \(\) 55.0 & 2 \(\) 0.8 & 39.9 \(\) 39.4 & 44.0 \(\) 22.8 & 4.0 \(\) 2.2 & 85.4 \(\) 13.2 & 92.0 \(\) 35.2 \\ hard & 15.2 \(\) 7.7 & 3.1 \(\) 1.5 & 10.7 \(\) 6.0 & 18.8 \(\) 15.2 & 6.1 \(\) 1.8 & 4.2 \(\) 1.6 & 5.3 \(\) 1.9 & 3.0 \(\) 14.3 \\  overall & 29.7 & 30.7 & 19.4 & 28.9 & 54.5 & 12.6 & 44.2 & **76.0** \\   \\  easy & 42.4 \(\) 31.4 & 12.9 \(\) 12.7 & 25.5 \(\) 45.7 \(\) 7.4 & 130.4 \(\) 32.0 & 51.8 \(\) 38.1 & 10.4 \(\) 9.9 & 158.0 \(\) 21.8 \\ medium & 39.6 \(\) 24.7 & 22.3 \(\) 13.6 & 14.2 \(\) 11.2 & 15.6 \(\) 8.1 & 52.2 \(\) 26.0 & 5.4 \(\) 3.9 & 24.8 \(\) 34.4 & **89.2 \(\) 7.0** \\ hard & 10.2 \(\) 4.0 & 5.2 \(\) 2.6 & 3.4 \(\) 4.8 & 16.2 \(\) 14.4 & 31.2 \(\) 38.7 & 9.1 \(\) 3.4 & 2.9 \(\) 1.2 & **38.8 \(\) 37.5** \\  overall & 30.7 & 13.4 & 14.3 & 25.8 & 71.2 & 22.1 & 12.7 & **95.3** \\  

Table 1: Multi-step Inverse Dynamics with InfoGating. Comparing performance in the presence of noisy distractors. We report returns achieved by a policy produced by behavior cloning on top of pretrained representations in _cheeath-run_. Extended results are provided in Appendix C. The “inv” model is our baseline with pretraining via multi-step inverse dynamics. The “w/ Rand” model adds random info gating during pretraining and the “w/ IG” model adds learned info gating during pretraining (this is our method). The easy/medium/hard settings add different levels of distractor noise. Results are for 3 seeds each, with mean and std. dev. reported.

Since random masking provides similar benefits to data augmentation, some performance improvement over the standard inverse model is expected. However, learning masks clearly does more than just data augmentation. Moreover, InfoGating in the input space also performs more robustly than methods that add regularization in the model weights (Dropout), in an intermediate layer (VIB) or at the output (adversarial samples). This result highlights how valuable removing information in the input space can be for certain applications (as can be visualized in Figure 1). Finally, these results also indicate that minimizing the downstream objective (in this case the inverse model loss is not always sufficient for learning robust representations. By incorporating the InfoGating objective, we add an inductive bias that encourages the model to focus on the most minimal, useful information without additional supervision. This inductive bias towards informational parsimony empirically produces more powerful representations.

### InfoGating for Stabilizing Q-Learning

Our prior experiments focused on pretraining representations and then learning regression-based policies, while keeping the representation fixed. In this section, we ask if the same conclusions hold if we train the representation end-to-end with a Q-Learning loss instead. We use the DrQ  loss as the downstream objective for this variant of InfoGating. Our InfoGating formulation for Q-Learning essentially amounts to doing standard DrQ training with observations that are masked by the gating network. The gating network learning is driven by the Q-Learning loss directly as follows:

\[_{ig,}=(Q_{}(_{t}^{ig},_{t})- (_{t},_{t})- Q_{}^{}(_{t+1},_{t+1}))^{2}+\ ||ig(_{t})||_{1},\] (7)

where \(Q_{}\) and \(Q_{}^{}\) denote the current and target Q networks respectively, while \((_{t},_{t})\) is the obtained reward. In Table 2, we observe that the base DrQ algorithm is quite prone to failure for all three distractor settings, while with InfoGating we see significant improvements in performance. This result shows how using minimal information in the input space can be beneficial in stabilizing TD-based critic training.

### Finetuning General Representations with Behavior Cloning

Recent work [29; 26] has investigated learning general representations from large datasets involving diverse object interactions and different varieties of tasks. A natural question arises when using such pretrained representations for downstream tasks -- how should the representation be fine-tuned so that only the relevant object and task features for the given task are used for learning the task's policy? InfoGating can be seen as a natural way to extract only information pertaining to the downstream task. Having tested InfoGating extensively on noisy environments, we now study whether there are similar benefits when there is no explicit noise present in the environments, but there are multiple objects/pixel components which could act as potential distractors. We choose five different tasks from the Kitchen  environment to test this hypothesis.

Specifically, given pretrained features, we test whether fine-tuning them using InfoGating is more powerful than fine-tuning with only a behavior cloning (BC) loss. We test two pretraining variations here: one corresponding to ImageNet features and the other corresponding to CLIP features. Both are 1) trained on ImageNet data, alongside language clippings for CLIP and 2) fine-tuned using a behavior cloning (BC) policy with a 2-layer MLP attached on top of the pre-trained encoding. Table 3 shows normalized success rates (each out of a maximum score of 100) for both BC and BC with InfoGating. We consistently see that InfoGating is able to mask out most pixel-level information except the robot gripper and the objects being manipulated (see Figure 3 for visualizations of the learnt masks), thus leading to strictly better success rates than the baseline BC policy.

## 6 Ablations

We study various design choices involved in InfoGating and include the most important ablations in this section. Further ablation results are provided in Appendix D.

 case & easy & medium & hard & overall \\  DrQ & 5.7 \(\) 2.3 & 29.8 \(\) 20.5 & 3.4 \(\) 0.8 & 12.9 \\ DrQ + IG & **63.4**\(\) **22.6** & **52.0**\(\) **6.2** & 5.3 \(\) 1.9 & **40.2** \\ 

Table 2: InfoGating for Q-Learning. We use DrQ as the base Q-Learning algorithm and add InfoGating to it. The experimental setup is the same as in Table 16. Results are for an expert policy level.

**InfoGating in the Feature Space**. We compare the feature InfoGating version with the Variational Information Bottleneck (VIB), while using the same backbone encoders for both methods (see Table 4). Both VIB and feature InfoGating lead to some performance improvement over the base architecture, but do not come close to the score for the pixel InfoGating version. This is potentially due to not removing background distractor information in the first layer itself. Interestingly, feature InfoGating almost matches the pixel InfoGating performance for the hard distractor case. Note that there exist variations such as change in body color, camera zoom, etc in the hard distractor case, that remain even after pixel-level InfoGating. We suspect that feature InfoGating is able to mask out such features but is hurt by the background distractor instead, hence falling to the same performance as the pixel-level variant.

**Adversarial InfoGating**. We test the adversarial InfoGating algorithm using a multi-step inverse model loss, just as we did for cooperative InfoGating. Figure 4 shows how the adversarial masks tend to hide the agent body. Some additional image content is also erased, since a precise silhouette of the agent's pose would be highly predictive of the agent's pose. To test how useful this kind of masking is, we simultaneously train a separate encoder over the reverse of the adversarial masks. If the adversarial process hides the robot pose successfully, then the reverse mask should contain maximal information about the agent and minimal information about the background. We indeed see that the adversarial InfoGating model performs similarly to the cooperative version, thus verifying that it is an equally viable approach for learning parsimonious representations (see Table 5).

**Effect of Mask Sparsity**. We test what range of \(\) values leads to improved performance (Figure 5). As is expected, when \(\) is too high, the entire input is masked and no useful information is captured.

 case & easy & medium & hard & overall \\  VIB & 97.7 \(\) 32.2 & 86.2 \(\) 24.8 & 11.0 \(\) 5.2 & 64.9 \\ feat. IG & 71.4 \(\) 44.5 & 76.7 \(\) 21.3 & **58.9**\(\) 28.3 & **69.0** \\ 

Table 4: **InfoGating in the Feature Space**. We compare the Variational Information Bottleneck with InfoGating in feature space. The experimental setup is the same as in Table 16. Results are for an expert policy level.

Figure 4: **Left**. Original distractor-based image. **Right**. Learnt adversarial mask over the original image.

Figure 3: **Visualizing Masks Produced by InfoGating**. Masks improve during training on the Kitchen-sdoor-open-v3 task (left to right corresponds to increasing training steps). InfoGating is able to learn semantically meaningful masks, even beyond the planar control domains such as the cheetah-run task.

  & ImageNet & CLIP \\  env & BC w/ IG & BC & BC w/ IG & BC \\  knob1-on & **17.6 \(\) 3.1** & 7.3 \(\) 4.2 & 13.3 \(\) 1.49 & 11.3 \(\) 0.94 \\ ldoor-open & 9 \(\) 5.3 & 5 \(\) 1.9 & 11.0 \(\) 4.8 & 4.6 \(\) 1.49 \\ light-on & 19.6 \(\) 7.7 & 11 \(\) 3.6 & 24.0 \(\) 12.8 & 11.0 \(\) 3.0 \\ sdoor-open & **61.6 \(\) 5.0** & 36.6 \(\) 16.6 & **61.6 \(\) 8.7** & 42.0 \(\) 5.9 \\ micro-open & 13.0 \(\) 7.8 & 4.3 \(\) 2.9 & 12.0 \(\) 5.6 & 5.5 \(\) 2.1 \\  overall & **24.1** & 12.8 & **24.3** & 14.8 \\ 

Table 3: **Fintuning with Behavior Cloning on Kitchen-v3**. We compare fine-tuning performance of BC with and without InfoGating. Representations are pre-trained either on ImageNet labels or through CLIP training. In both cases, InfoGating learns to remove irrelevant objects in the environment surroundings and leads to consistently higher performance.

Similarly, when \(\) is too low, none of the input is masked and we recover the performance of the base model, one that directly minimizes the downstream loss.

**Mixing Original and Masked Inputs to Encoder**. Since the masking network and the encoder process different kinds of input (unmasked and masked input, respectively), it is worth asking if the encoder can be better regularized by forcing it to be able to process both masked and unmasked images. We can also encourage the encoder to be robust to changes in the input/mask mapping learned by the masking network by occasionally swapping masks between images in a batch while training the encoder. We test both of these separately (Table 6) and observe that training the encoder on masked and unmasked inputs provides strong improvements over the base InfoGating model. This also has the added benefit of not using the masking network at evaluation time, and only using the encoder, which has now been implicitly trained to be invariant to the masked and unmasked inputs. Note that we use this mixed input InfoGating as the default for all our experiments.

## 7 Limitations and Future Work

Although InfoGating helps learn robust representations, it does not recover the same performance for all distractor levels as in the case when no distractors are present. Ideally, we should be able to learn masks that fully remove the background information and thus recover the distractor-free performance. This might be down to two reasons. First, the masks may leave room for noise information to escape around the edges of the object/agent they mask. Second, the encoder that processes the masked images should be robust to slight variations in the masking patterns between training and evaluation samples. In practice, using a UNet for more accurate masks and training the encoder on a mix of masked and unmasked inputs helped remedy these issues quite well, but there still remains room for improvement. This motivates the idea of efficiently InfoGating at multiple layers, without having separate masking networks for each InfoGating layer.

Furthermore, our initial exploration of InfoGating can be viewed as a step towards learning object-centric representations without any explicit notion of objects forced into the model architecture or learning objective. For example, in settings where an agent's actions may affect multiple objects, and we want to predict future observations conditioned on the agent's actions, the masks produced by InfoGating will need to reveal some information about (i.e., "look at") each object. The pursuit of informational parsimony should discourage masks from revealing more of each object than necessary.

 case & easy & medium & hard & overall \\  w/ mix input & 176.2 \(\) 9.1 & **97.0 \(\) 5.7** & **44.8 \(\) 18.4** & **106** \\ shuffle mask & 170.0 \(\) 31.6 & 54.6 \(\) 22.3 & 9.8 \(\) 10.2 & 78.1 \\ w/o mix input & 119.4 \(\) 15.6 & 18.4 \(\) 17.5 & 10.5 \(\) 7.4 & 49.4 \\ 

Table 6: **Mixing Masking and Unmasked Inputs**. Experiment setup follows Table 16. Default InfoGating version is marked in gray. Results are for an expert policy level.

Figure 5: **Scaling \(\) leads to different masks which reveal different amounts of information. For small \(\), the masked observation still contains distracting/non-salient information, thus hurting performance. Similarly, when masking is too aggressive, too much information is lost and performance goes down.**

 case & easy & medium & hard & overall \\  cooperative & 176.2 \(\) 9.1 & 97.0 \(\) 5.7 & 44.8 \(\) 18.4 & **105.9** \\ adversarial & **202.3 \(\) 2.64** & 84.8 \(\) 41.2 & 4.6 \(\) 1.7 & 96.8 \\ 

Table 5: **Adversarial InfoGating**. We compare differences in the representations learnt with Cooperative vs Adversarial InfoGating. The experimental setup is the same as in Table 16. Results are for an expert policy level.

Conclusion

We present InfoGating, a wide-ranging method to learn parsimonious representations that are robust to irrelevant features and noise. We describe two different approaches to InfoGating: cooperative and adversarial, while demonstrating that the gating can be learnt both at the input space or any intermediate feature space. We apply InfoGating to multiple downstream objectives including multi-step inverse dynamics models, Q-Learning, and behavior cloning. InfoGating produces semantically meaningful masks that improve interpretability, and leads to consistently better performing representations, in terms of both out-of-distribution (visual D4RL distractor noise) and in-distribution (irrelevant Kitchen objects) generalization.

## 9 Acknowledgments

The authors would like to thank Charlie Snell, Colin Li, and Dibya Ghosh for providing feedback on an earlier draft, and Katie Kang for suggesting the paper title. Part of this work has taken place in the Intelligent Robot Learning (IRL) Lab at the University of Alberta, which is supported in part by research grants from the Alberta Machine Intelligence Institute (Amii); a Canada CIFAR AI Chair, Amii; Compute Canada; Huawei; Mitacs; and NSERC.