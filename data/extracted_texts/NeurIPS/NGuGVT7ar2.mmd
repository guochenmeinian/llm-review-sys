# Enhancing LLM Reasoning via Vision-Augmented Prompting

Ziyang Xiao\({}^{1}\), Dongxiang Zhang\({}^{14}\)1, Xiongwei Han\({}^{2}\), Xiaojin Fu\({}^{2}\), Wing Yin Yu\({}^{2}\)

**Tao Zhong\({}^{2}\), Sai Wu\({}^{14}\), Yuan Wang\({}^{3}\), Jianwei Yin\({}^{1}\), Gang Chen\({}^{1}\)**

\({}^{1}\) Zhejiang University \({}^{2}\) Huawei Noah's Ark Lab

\({}^{3}\) School of Business, Singapore University of Social Sciences

\({}^{4}\) Hangzhou High-Tech Zone(Binjiang) Institute of Blockchain and Data Security

{xiaoziyang, zhangdongxiang, wusai, cg}@zju.edu.cn

{hanxiongwei, rocket.YuWingYin, zhongtao5}@huawei.com

fuxiaojin32@hotmail.com, Jessicawang36@gmail.com

zjuyjw@cs.zju.edu.cn

Corresponding author.

###### Abstract

Verbal and visual-spatial information processing are two critical subsystems that activate different brain regions and often collaborate together for cognitive reasoning. Despite the rapid advancement of LLM-based reasoning, the mainstream frameworks, such as Chain-of-Thought (CoT) and its variants, primarily focus on the verbal dimension, resulting in limitations in tackling reasoning problems with visual and spatial clues. To bridge the gap, we propose a novel dual-modality reasoning framework called Vision-Augmented Prompting (VAP). Upon receiving a textual problem description, VAP automatically synthesizes an image from the visual and spatial clues by utilizing external drawing tools. Subsequently, VAP formulates a chain of thought in both modalities and iteratively refines the synthesized image. Finally, a conclusive reasoning scheme based on self-alignment is proposed for final result generation. Extensive experiments are conducted across four versatile tasks, including solving geometry problems, Sudoku, time series prediction, and travelling salesman problem. The results validate the superiority of VAP over existing LLMs-based reasoning frameworks.

## 1 Introduction

The human cognitive system is characterized by the presence of two specialized subsystems within the working memory: the phonological loop, which processes verbal information, and the visual-spatial sketchpad, which processes visual and spatial information . Both of them play a crucial role in problem-solving by offering qualitatively distinct strategies for comprehending and manipulating information . Recently, with the rapid advancement of LLMs, there have emerged various reasoning frameworks, such as Chain of Thought (CoT) , Self-consistent CoT (CoT-SC) , and Tree of Thoughts (ToT) . Although these frameworks have shown impressive performance across a wide range of NLP tasks, they primarily focus on the verbal dimension with text-only representations, resulting in limitations in tasks that require visual and spatial interpretation (e.g., geometry problems or grid puzzles).

In this paper, we propose a novel dual-modality reasoning approach called **V**ision-**A**ugmented **P**rompting (VAP) that analogizes human cognition subsystems with the assistance of multimodal large language models (MLLMs). VAP takes textual problems as input and uses self-synthesizedimages as an additional information channel to enhance reasoning. As shown in Figure 1, we employ the state-of-the-art GPT-4V(ision)  to solve a geometry problem. In this case, the model accurately deduces the correct answer when given both image and text inputs. In contrast, it generates an ambiguous answer with purely textual information. This example is analogous to the process of human cognition where it is a common practice to use images to enhance comprehension when handling geometry problems .

Our proposed vision-augmented prompting comprises three steps. Initially, LLMs generates a high-level plan for the following steps, including selecting an appropriate drawing toolkit and creating an initial image. To automate the procedure, we leverage the API documentation of external tools as context of LLM to facilitate drawing tool selection and figure synthesis. In the second step, VAP iteratively performs reasoning on the image, updates it, and generates an accompanying textual thought in each iteration. This process results in a chain of thoughts in both image and text modalities, as illustrated in Figure 2. Lastly, the final image and the chain of textual thoughts are jointly fed to the MLLM to derive a conclusive answer. To enhance robustness, we introduce a technique called self-alignment, where the MLLM describes the image content first, and the image channel is discarded if the self-description fails to align with the initial high-level plan.

We conduct extensive experiments across four versatile tasks, among which VAP establishes new state-of-the-art performance compared to other training-free LLMs-based methods. These four reasoning tasks include: (1) Geometry Intersection Counting in the domain of geometry word problems (+5.0\(\%\) absolute accuracy gains); (2) Sudoku Puzzle as a logical reasoning task (\(+12.9\%\)); (3) Time Series Prediction as a numerical analysis task (\(+9.9\%\) in reducing mean absolute error); and (4) Travelling Salesman Problem as a classical NP-hard problem in the field of operations research (\(+1.8\%\) in reducing the optimal gap).

## 2 Related Work

### LLMs-based Reasoning

Improving the reasoning capabilities of LLMs such as GPT-4 , PaLM2 , and LLMA2  has been a hot topic in recent years. To achieve the goal, Chain-of-Thought (CoT)  breaks a reasoning task into a series of intermediate reasoning steps. Self-consistency  replaces the naive greedy decoding in CoT by sampling a diverse set of reasoning paths and selecting the most consistent answer. Tree of Thoughts (ToT)  and Graph of Thoughts (GoT )  further allow LLMs to explore and combine thoughts in a structured manner. Cumulative Reasoning (CR)  decomposes a reasoning task into smaller components, which are solved in a cumulative and iterative manner.

Figure 1: An example of solving the same Geometry Intersection Counting problem using different types of input. Outputs are derived from GPT-4; for brevity, only the conclusions are presented.

Figure 2: Compare with Standard Prompting and Chain of Thoughts Prompting(CoT).

Additionally, Chain-of-Experts  enhances reasoning capabilities through the collaboration of multiple LLMs.

### Multimodal Large Language Models

The emergence of Multi-modal Large Language Models (MLLMs) such as GPT-4V(ison)  has fostered a new research landscape. Although the architecture of GPT-4V is not publicly available, substantial progress has been made in the open-source domain [14; 15; 16; 17]. These works usually fine-tune traditional text-based LLMs to align with other modalities. Similar approaches are also adopted by multimodal chatbots  and multimodal universal task solvers [15; 19] for vision-related tasks [20; 21; 22].

### MLLMs-based Reasoning

According to a recent survey , reasoning based on MLLMs can be broadly categorized into LLM-aided visual reasoning and multi-modal Chain-of-Thought. LLM-aided visual reasoning focuses on solving traditional visual reasoning tasks with the assistance of LLMs. This includes tasks such as visual question answering , image segmentation , and video question answering . Some works in this category, like ViperGPT , VisProg , and LLava-Plus , also use MLLMs to call external tools for enhanced reasoning, while their task settings differ from ours. On the other hand, the multi-modal Chain-of-Thought extends traditional prompting techniques to the multi-modal context. MM-ReAct  extends ReAct  to support multimodal data. The Visual Chain of Thought (VCoT)  uses CoT with vision-language grounding to bridge the gap in multi-step temporal reasoning. Compared with our work, VCoT is designed to bridge the logical gaps within sequential data and facilitate temporal reasoning in tasks like visual storytelling and WikiHow summarization.

## 3 Methodology

Mainstream LLMs-based reasoning frameworks, such as CoT, ToT, and GoT, only consider the verbal domain \(\). To tackle more challenging problems with visual and spatial clues, we propose vision-augmented prompting (VAP) and extend the exploration space from a single domain \(\) to a dual-modality domain \(\), where \(\) represents the visual-spatial domain. It relies on the external image synthesis toolkit to automatically draw an image matching the text description of the input problem (Section 3.1). The core challenge lies in how to effectively navigate the joint space and maximize the success rate of problem solving, which will be presented in Section 3.2.

### External Image Synthesis Toolkit

While visual-spatial information can be beneficial for reasoning as demonstrated in Figure 1, LLMs lack the inherent ability to visualize concepts. Therefore, it is necessary to leverage external image synthesis toolkit. Vision-augmented prompting incorporates graphic rendering tools that rely on logical programming to render images, including Python Turtle  for graphical visualizations, and Matplotlib  for drawing analytical figures. In addition to these third-party programmable tools, we further integrate image generative models, such as DALL-E 3 , to produce images directly from textual prompts. Consequently, we define the set of our drawing tools as \(_{T}=\{\}\). These tools will be utilized by our method through API calls.

### Procedures of Vision-Augmented Prompting

We model the problem-solving process of VAP as an iterative reasoning process. With the aid of the image synthesis tools, VAP progressively updates the image according to the instruction provided by the language model. To maintain a coherent reasoning trajectory, a 'thought' is generated on each iteration. Subsequently, to derive the conclusive answer, the final image, the original problem, and the trajectory of iterative thoughts are sent to the MLLM to obtain the final answer. However, we find it challenging for LLMs to perform iterative multi-step drawing and reasoning directly due to the lack of a global view. To overcome this limitation, we introduce a planning step, where LLMs will create a high-level plan for subsequent steps. Furthermore, it is possible that the synthesized image could be disqualified and mislead the reasoning process. To alleviate the negative effect of such images, we introduce a technique named self-alignment to enforce MLLMs to literally describe the synthesized image and check whether its text description is aligned with the initial high-level plan. If disparity is detected, we discard the image and restart the reasoning process.

Step 1: PlanningAs demonstartd in Figure 3, our method takes a textual problem \(\) as input and firstly generates a high-level reasoning plan. In implementation, we first prompt the LLM to acquire a detailed description of the plan in natural language. The output of the plan consists of five key components: (1) an analysis of the problem's characteristics for visualization, along with the chosen drawing tool \(T\) from the set of available tools \(_{T}\); (2) A description of how to initialize the image using the chosen tool, accompanied by the corresponding API call instruction \(I_{0}\); (3) the prompt \(_{d}\) that drives the LLM to draw in each iterative step; (4) the prompt \(_{t}\) that drives the LLM to think step by step during the iterative drawing process; (5) the termination condition \(C\) for the iteration. To facilitate subsequent reasoning, we further transform the unstructured textual description of the planning into semi-structured JSON format. This step can be formalized as shown in Equation 1, where \(_{planning}\) denotes the language model involved in this step with prompt engineering. With the instruction \(I_{0}\), we can create the initial image \(g_{0}\) using the selected tool.

\[\{T,I_{0},_{d},_{t},C\}_{planning}( ,_{T}),\ g_{0} T(I_{0})\] (1)

Note that both plan generation and JSON format transformation are implemented using LLMs with prompt engineering. Details of the prompt templates can be found in Appendix A.1.1.

Step 2: Iterative reasoningIn the iterative reasoning step, the MLLM is specified by two prompts \(_{d}\) and \(_{t}\) generated in the previous step, along with the image synthesis tool \(_{T}\) and the assigned termination condition \(C\). We denote the MLLM in this step as \(_{\{_{d},_{t},_{T},C\}}\). In each iteration \(t\), the MLLM takes the original problem \(\), the partially-completed image \(g_{t}\) and trajectory of thoughts \(Z_{t}\) as input. The MLLM will then generate an instruction \(I_{t}\) containing API calls used to update the image \(g_{t}\). Regarding the update of the image, an accompanying 'thought' \(z_{t}\) that provides a textual interpretation of the current state is generated. The process is formally depicted in Equation 2, with details of the prompt template presented in Appendix A.1.2. Here, \(Z_{t}\) starts as an empty array and the new 'thought' \(z_{t}\) is appended to the trajectory at each iteration.

\[\{z_{t},I_{t}\}_{\{_{d},_{t}, _{T},C\}}(,g_{t},Z_{t}),\ Z_{t+1} Z_{t}\{z_ {t}\},\ g_{t+1} T(I_{t})\] (2)

Figure 3: Illustration of the workflow in VAP.

We observe that the MLLM occasionally failed to follow the instructions provided in the prompt (e.g., repeatedly drawing the same shape), causing disruption to the entire iterative process. To address this issue and improve the stability of reasoning process, we enrich the context of MLLM by appending the input and output from the last step, which serve as illustrative examples for the MLLM to better follow the instructions.

Step 3: Conclusive reasoningWhen the iterative reasoning terminates (i.e., condition \(C\) is met), we proceed to the final step of conclusive reasoning using the synthesized image of step 2, denoted by \(g_{n}\). Since we cannot guarantee \(g_{n}\) is a perfect output and a disqualified \(g_{n}\) may mislead the conclusive reasoning process, we introduce a technique named self-alignment to enforce MLLMs to literally describe the synthesized image and check whether its text description is aligned with the initial high-level plan. If disparity is detected, we discard the image and restart the reasoning process. If a qualified image cannot be obtained after a certain number of trials, VAP is degraded to simple input-output reasoning, without leveraging the visual channel.

As shown Equation 3, our conclusive reasoning takes \(g_{n}\), the trajectory of thoughts \(Z_{n}\), along with the original problem \(\) as input and leverage the prompt template presented in Appendix A.1.3 to derive the final answer \(\).

\[_{self-alignment}(,g_{n},Z_{n})\] (3)

## 4 Experiments

We evaluate VAP using four diversified and challenging tasks, including Geometry Intersection Count, Sudoku Puzzle, Time Series Prediction, and Travelling Salesman Problem. We also provide a typical input-output example for each task in Appendix A.2.

These tasks share three LLM-based reasoning baselines, including standard prompting, chain-of-thought (CoT) prompting, and CoT with self-consistency (CoT-SC) prompting .

For CoT prompting, the reasoning process involves multiple intermediate steps. In the Geometry Intersection Counting task, we define each intermediate step as calculating the number of intersections between a pair of shapes. In the Sudoku Puzzle task, each step involves considering a position that violates the Sudoku rules. For Time Series Prediction and Travelling Salesman Problem, it is challenging to define specific step content. Therefore, we append a 'think step by step' instruction to the standard prompt and include an additional step to extract the solution from the generated thoughts.

Additionally, for CoT-SC prompting, when the task output is discrete (i.e., Geometry Intersection Counting, Sudoku Puzzle, and Travelling Salesman Problem), we sample \(k\) answers and use the majority answer. When the task output is continuous (i.e., numeric value in the task of Time Series Prediction), we generate \(k\) predictions and calculate their average as the final forecast value. The default \(k\) is set to \(10\).

For fairness, we employ the 'GPT-4-vision-preview' as the underlying MLLM for these baselines and our VAP. The default temperature is set to \(0\). For methods that require sampling, such as SC, the temperature is set to \(0.7\).

### Task 1: Geometry Intersection Counting

Task DescriptionThe task determines the number of intersection points between a couple of geometric shapes described in natural language. For example, given the input "There is a line segment from \((-2.5,-1)\) to \((-0.5,-1)\) and a circle centered at \((-1.5,-1)\) with radius \(1\)," the correct output should be "\(2\)", as there are \(2\) intersection points between the circle and the line segment.

Task SetupWe randomly sample \(200\) problem instances from Geometry Intersection Counting task in the BIG-bench benchmark2. The performance metric used in this task is accuracy, which is defined as the percentage of problem instances in which the output number matches the ground truth.

Task-specific BaselinesWe introduce two additional baselines specifically developed for solving geometry problems. The first is Inter-GPS , a symbolic reasoner  for geometry problems. The second is G-LLaVA , a MLLM specialized trained on geometry problems. Since geometry figures are not available in the problem input, G-LLaVA only leverages the textual modality for reasoning.

ResultsAs shown in Table 1, Inter-GPS and G-LLaVA exhibit low accuracy, primarily because they are originally trained on dual-modality datasets and thus show limited effectiveness with text-only input. We also examine their performance when provided with an additional image input in Section 4.5, where the purpose is to validate the effectiveness of the synthesized images by our VAP algorithm. Among the general-purpose reasoning methods, standard prompting and CoT prompting yield relatively low accuracy (\(8.5\%\) and \(10.0\%\) respectively). The CoT-SC method, even with a sample size of \(20\), only marginally improves accuracy by \(1.5\%\). In contrast, VAP significantly outperforms these methods and achieves an accuracy of \(16.5\%\). Furthermore, in the break-down analysis with increasing number of shapes, as illustrated in Figure 4, VAP demonstrates clear superiority over other LLM-based methods in more complex scenarios involving four or more shapes. We can see that the accuracy of all baselines is almost close to \(0\) in these complex scenarios.

### Task 2: Sudoku Puzzle

Task DescriptionIn this task, an initial state of a Sudoku board is presented in natural language, with digits on filled cells and dots denoting empty cells. For example, in a \(9 9\) board, a row "\(\)\(6\)...." only contains digit \(6\) in the third column. The reasoning process iteratively provides the information of the next action, in the form of "x y digit", where 'x' and 'y' specify the cell's coordinates on the board, and 'digit' is the number to be placed in that cell.

Task SetupWe utilize the Sudoku puzzle generation program from the BIG-bench3 to create a dataset that includes \(150\) Sudoku puzzles. For performance evaluation, we employ correct rate and collision rate as two metrics. The correct rate measures the accuracy in solving the puzzles, and collision rate assesses the frequency at which the position in given command is already filled with numbers, which indicates a violation of Sudoku rules.

Task-specific BaselinesFor a more comprehensive evaluation, we also incorporate Tree of Thought (ToT)  as an additional baseline. The implementation of ToT in this task is not troublesome because it is straightforward to decompose the thought process into a tree-structured representation for board games.

ResultsIn Table 2, the standard prompting shows a low success rate of \(18.0\%\), with a high rate of rule violations. This can be attributed to Sudoku's demands for reasoning and rule comprehension, which pose a challenge for LLMs relying solely on prompt engineering. The CoT and CoT-SC methods show improved performance, as they can, to certain extent, enhance the coherence of LLM reasoning by providing a reference of previous steps. The ToT algorithm is the most effective among these baselines, achieving a \(22.6\%\) success rate, owing to its tree-structured thought process. Notably,

  
**Method** & **Accuracy** \\  Standard & \(8.5\%\) \\ CoT & \(10.0\%\) \\ CoT-SC\({}_{(k=5)}\) & \(11.0\%\) \\ CoT-SC\({}_{(k=10)}\) & \(11.5\%\) \\ CoT-SC\({}_{(k=20)}\) & \(11.5\%\) \\ Inter-GPS & \(6.0\%\) \\ G-LLaVA & \(14.0\%\) \\ VAP & **16.5\%** \\   

Table 1: Intersection Point Result.

VAP outperforms all other methods with a correct rate of \(35.5\%\). VAP also reduces the collision rate, implying that integrating the image modality is helpful for LLMs to understand game rules.

### Task 3: Time Series Prediction

Task DescriptionTo evaluate the numerical analysis capabilities, we regard the LLM as a time series predictor. In this task, a sequence of data points is provided, and the objective for the LLM is to predict the next \(n\) values in the series.

Task SetupThe dataset for this task is sourced from the Darts library , which includes a curated collection of \(8\) real univariate time series datasets. We configure the window size to \(100\) data points and predict the subsequent \(n=8\) values in the series. The performance is evaluated using the Mean Absolute Error (MAE) metric.

Task-specific BaselinesWe chose the LLMTime algorithm  as the most cutting-edge method for comparison, where a data rescaling strategy and a tokenization trick is introduced to enhance numerical precision. For instance, the sequence "8.05, 1, 35" will be tokenized as "8 0 5, 1 0 0, 3 5 0 0". We also adopt the simple moving average (SMA) as a traditional statistical baseline for time series forecasting.

ResultsAs shown in Figure 5, SMA is recognized as a traditionally effective method for time series prediction, surpassing most LLM-based methods with an MAE of \(611\). Among the LLM-based approaches, compared to standard prompting, CoT-SC approach significantly reduces the MAE from \(767\) to \(675\), justifying the effectiveness of integrating auto-regression and sample strategy. It is noteworthy that LLMTime, as a preprocessing technique for LLMs to handle sequential data, performs effectively. Its performance even marginally exceeds that of SMA. In the result, VAP outperforms the majority of LLM-based methods and is slightly inferior to LLMTime. However, considering that VAP is orthogonal to LLMTime, we introduce a new method that combines the same preprocessing technique with VAP. The results show that this combined version, VAP+LLMTime, outperforms all comparison algorithms.

### Task 4: Travelling Salesman Problem

Next, we test our method on travelling salesman problem (TSP), an NP-hard combinatorial optimization problem that poses challenges for neural network solvers . The input for TSP instances consists of a set of coordinates, and the output is a sequence of city indices representing the route for the salesman.

Task SetupWe use Euclidean TSP instances with \(10\) and \(20\) cities as our testset. For each city size, \(100\) instances are generated by a program, with coordinates uniformly distributed within a \(^{2}\) integer grid. The average path length and the gap from the optimal solution are used as metrics.

Task-specific BaselinesWe introduce four traditional TSP solvers as our task-specific baselines: Gurobi, an exact TSP solver using mixed integer programming; Nearest neighbour (NN) algorithm, which greedily selects the closest city; Fastest insertion (FI), an efficient insertion method; and a Random baseline that chooses paths randomly for performance benchmarking. Here, note that the output of LLM-based methods (i.e., Standard, CoT, CoT-SC, and VAP) might violate constraints of TSP (e.g., generating duplicate cities). In such cases, we randomly select an unvisited city to continue the solution.

ResultsAs shown in Table 3, the performance of standard prompting is comparable to the Nearest Neighbour algorithm when applied to TSP with \(10\) cities. CoT demonstrates very slight improvement over standard prompting. This might be attributed to the TSP's requirement for a heuristic and complex thought process, which is challenging for CoT to replicate. VAP outperforms all LLM-based algorithms, achieving a \(6.0\%\) gap for optimality, close to the Fastest Insertion's performance. It's worth noting that when the number of cities in a problem instance increases to \(20\), the performance of LLM-based algorithms significantly decreases as the search space of TSP grows exponentially with the problem size. Such a huge search space poses a significant challenge for LLMs not specifically tailored to solve TSP. Nonetheless, VAP's superiority over other LLM-based methods is still evident when the problem size increases.

### Effectiveness of the Synthesized Images

To demonstrate the quality of synthesized images, we first present several illustrative examples in Figure 6 and then provide numerical experiments in the following.

In the first experiment to evaluate the quality of synthesized images, we use the rate of integrity as the metric, which refers to whether the generated image contains all the elements described in the original problem. This is a relatively objective metric for human annotators to reach consensus. As shown in Table 4, the images synthesized by VAP demonstrate a high integrity rate across various tasks. Notably, for Time Series Prediction and TSP, the module achieves an impressive integrity rate of \(100\%\) and \(98.0\%\), respectively. We also evaluate the performance improvement of VAP when provided with the ground truth image. As shown in Table 4, the column of 'With ground truth image' refers to the performance boost rate. If the drawing errors had been corrected, we can observe notable potential for improvement. These results underscore the crucial role of accurate image rendering in enhancing the final output quality of the VAP.

In the next experiment, we provide the synthesized images by our VAP as additional input for the geometry solvers Inter-GPS and G-LLaVA involved in task of Geometry Intersection Counting. As shown in Table 5, these two approaches can greatly benefit from our synthesized images. Their remarkable accuracy boosting validates the effectiveness of the images generated by VAP.

   } &  &  \\   & & Gurobi & Random & NN & FI & Standard & CoT & CoT-SC & VAP \\   **LLM-1** \\ **CQ** \\  } & **Length** & \(\) & \(529.6\) & \(327.4\) & \(306.8\) & \(327.5\) & \(327.2\) & \(325.0\) & \(\) \\  & **Gap** & \(\%\) & \(79.7\%\) & \(11.1\%\) & \(4.1\%\) & \(11.1\%\) & \(11.0\%\) & \(10.4\%\) & \(\%\) \\   **LLM-1** \\ **CQ** \\  } & **Length** & \(\) & \(989.5\) & \(448.2\) & \(424.2\) & \(544.5\) & \(547.2\) & \(541.9\) & \(\) \\  & **Gap** & \(\%\) & \(170.7\%\) & \(16.8\%\) & \(10.6\%\) & \(41.9\%\) & \(42.6\%\) & \(41.2\%\) & \(\%\) \\   

Table 3: TSP task performance.

Figure 6: Examples of images synthesized by our VAP among the four tasks.

## 5 Ablation Study

In the ablation study, we evaluate the effectiveness of three steps, including high-level planning, iterative reasoning and self-alignment in conclusive reasoning. Note that removing the planning step refers to prompting the LLM to draw the image step-by-step without providing a plan in advance. Removing iterative reasoning refers to directly prompting the LLM to generate an entire image and then proceeding to the conclusive reasoning step without intermediate results.

As shown in Table 6, we report the core metric for four tasks: accuracy for Geometry Intersection Problems, correct rate for Sudoku puzzles, MSE for Time Series Prediction, and average tour length for the TSP with \(10\) cities. The results demonstrate that removing any of the key components causes a performance degradation, except for self-alignment in Time Series Prediction. This is because there is no error in the drawing module, as discussed in Subsection 4.5. Hence, in this scenario, self-alignment plays no effect. Interestingly, removing planning step causes the most significant performance drop for Geometry Intersection Problems and TSP. The reason is that planning is crucial for visualizing problems requiring precise spatial relationships. Furthermore, iterative reasoning proves to be an essential component, as its removal leads to considerable performance degradation across all tasks. This finding highlights the importance of our view that all problem-solving is a step-by-step process. Self-alignment plays an important role in the task of Sudoku. For example, the LLM will occasionally draw the incorrect coordinate range for the Sudoku board, resulting in a board with the wrong size. However, with self-alignment, the model first describes the board size in the figure and cross-checks this against the original plan, preventing such issues.

## 6 Limitations of VAP

Even though VAP has been shown to outperform other LLM-based approaches in a set of versatile tasks, there still exists a noticeable performance gap between VAP and task-specific approaches. However, these tailored approaches require nontrivial efforts for domain customization and is not able to support other types of reasoning tasks. In contrast, VAP is a lightweight and training-free framework. With negligible customization cost, VAP is general enough to handle a spectrum of complex reasoning tasks with visual and spatial clues.

The other limitation is the black-box functionality of VAP, even though the framework is inspired by the cooperation of two subsystems in different regions of human brain that cooperate with each other. In Appendix A.3, we provide two case studies as examples to provide certain insights and justify the effectiveness of VAP. It would be an interesting future research direction to explore the interpretability of VAP and its relationship with cognitive science.

    & **Geometry** & **Sudoku** & **Time Series** & **TSP\({}_{(N=10)}\)** \\  VAP & \(16.5\%\) & \(35.5\%\) & \(556\) & \(312.4\) \\ w/o planning & \(12.0\%\) & \(24.7\%\) & \(578\) & \(322.0\) \\ w/o iterative & \(14.5\%\) & \(19.9\%\) & \(582\) & \(321.3\) \\ w/o self-alignment & \(16.0\%\) & \(25.1\%\) & \(556\) & \(315.9\) \\   

Table 6: Ablation study results. Each column represents a task and the cell indicates the performance when removing a module in VAP.

  
**Task** & **Integrity** & **With ground truth image** \\  Geometry & \(83.5\%\) & \(5.8\%\) (\(17.0\% 18.0\%\)) \\ Sudoku & \(89.3\%\) & \(5.1\%\) (\(35.5\% 37.3\%\)) \\ Time Series & \(100.0\%\) & - \\ TSP & \(98.0\%\) & \(1.3\%\) (\(30.2\% 30.6\%\)) \\    
  
**Method** & **Accuracy** & **Accuracy\({}^{}\)** \\  Inter-GPS & \(6.0\%\) & \(\%\) \\ G-LLaVA & \(14.0\%\) & \(\%\) \\   

Table 4: Performance of Drawing Module.

  
**Task** & **Integrity** & **With ground truth image** \\  Geometry & \(83.5\%\) & \(5.8\%\) (\(17.0\% 18.0\%\)) \\ Sudoku & \(89.3\%\) & \(5.1\%\) (\(35.5\% 37.3\%\)) \\ Time Series & \(100.0\%\) & - \\ TSP & \(98.0\%\) & \(1.3\%\) (\(30.2\% 30.6\%\)) \\    
  
**Method** & **Accuracy** & **Accuracy\({}^{}\)** \\  Inter-GPS & \(6.0\%\) & \(\%\) \\ G-LLaVA & \(14.0\%\) & \(\%\) \\   

Table 5: Performance of dedicated geometry solvers with (denoted by \({}^{}\)) and without synthesized images.

Conclusion

In this paper, we introduced a novel reasoning framework called Vision-Augmented Prompting (VAP), designed to enhance the reasoning capabilities of large language models (LLMs) by emulating the human cognitive system's dual modality processing. VAP leverages the external image synthesis tools to generate visual representations that augment the textual input. The procedure of VAP follows a three-step algorithm: first, it automatically generates a high-level plan for reasoning; second, it engages in iterative drawing and reasoning based on the partially-completed image; and finally, original problem, all thoughts and generated image are jointly to derive a solution. To enhance robustness, we introduced a self-alignment technique, where the MLLM describes the image content, and the image channel is discarded if the self-description fails to align with the initial high-level plan. We conducted extensive experiments across four diverse reasoning tasks: intersection counting in Geometry Intersection Problems, Sudoku Puzzles, Time Series Prediction, and the Travelling Salesman Problem. The results demonstrated the effectiveness of VAP, establishing new state-of-the-art performance compared to other training-free LLM-based methods.