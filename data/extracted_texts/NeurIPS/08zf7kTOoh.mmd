# Anthony L. Caterini\({}^{\lx@sectionsign}\) J. Eric T. Taylor\({}^{\lx@sectionsign}\) Gabriel Loaiza-Ganem\({}^{\lx@sectionsign}\) {george, jesse, rasa, amy, brendan, valentin.v, zhaoyan, anthony, eric, gabriel}@layer6.ai

Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models

George Stein\({}^{*}\) Jesse C. Cresswell\({}^{}\) Rasa Hosseinzadeh\({}^{}\) Yi Sui\({}^{}\)

Brendan Leigh Ross\({}^{}\) Valentin Villecroze\({}^{}\) Zhaoyan Liu\({}^{}\)

**Anthony L. Caterini\({}^{@sectionsign}\) J. Eric T. Taylor\({}^{@sectionsign}\) Gabriel Loaiza-Ganem\({}^{@sectionsign}\) {george, jesse, rasa, amy, brendan, valentin.v, zhaoyan, anthony, eric, gabriel}@layer6.ai**

Layer 6 AI

###### Abstract

We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at [https://github.com/layer6ai-labs/dgm-eval](https://github.com/layer6ai-labs/dgm-eval).

## 1 Introduction

The capability of modern generative models to synthesize fake images that are seemingly indistinguishable from real samples has resulted in much public interest . While the evaluation of such models has a longstanding history , the unprecedented fidelity of modern synthetic images (e.g. ) raises the question of whether the current tools in use by researchers are sufficient to measure the extent to which these models have truly learned the ground truth distribution, and whether striving to achieve state-of-the-art performance on current metric leaderboards provides optimal targets to drive further algorithmic progress.

Evaluating a single generated image is straightforward, since humans can act as the "ground truth" for determining realism. Evaluating the quality of a model as a whole is much more difficult. Beyondquantifying to what extent images resemble those from the training set (_fidelity_), we must determine how well the generated samples span the full training distribution (_diversity_), and whether they are truly novel or are simply reproductions of training samples (_memorization_), as illustrated in Figure 1. An ideal generative model will synthesize high fidelity and diverse samples without memorizing the training set (the latter becoming a prominent concern for models trained on unlicensed data ). Researchers are well-practiced in ranking generative models by metrics such as the Frechet Inception distance (FID) , Inception score (IS) , and many others  which group fidelity and diversity into a single value without a clear tradeoff. Other popular diagnostic metrics separate sample quality from diversity such as precision/recall  and density/coverage . However, relating such metrics to human evaluation of image quality is not straightforward .

These metrics generally follow a two-step design: extract a lower-dimensional representation of each image, then calculate a notion of distance between true and generated samples in this space. The goal of the representation extractor, or encoder, is to embed images into a representation space that has a generalized perceptual relevance across the span of natural images. The implicit assumption in the de-facto use of the (pool3, 2048 dimensional) Inception-V3 network  trained for ImageNet1k  classification is that it provides such a space. Yet major concerns have been raised: it has been shown to be agnostic to features unrelated to the 1k classes of ImageNet , ImageNet classifiers in general are biased towards texture over shape , and many other criticisms .

Obvious choices for more universally applicable representation spaces are modern self-supervised learning (SSL) models  trained on large and diverse datasets, as they have proven to extract representations that excel at a number of generalized downstream tasks . While initial studies of a few self-supervised encoders reported that representations from these networks can produce more adequate rankings for generative adversarial networks (GANs)  on non-ImageNet domains , it is an open question as to which SSL methods and families provide the best perceptual representation space for evaluating natural images more generally. For example, SSL methods based on contrastive learning strategies utilizing strong augmentations tend to learn features that are more invariant to those augmentations . Thus, while the criteria for choosing an SSL encoder for classification tasks is straightforward - choose one that achieves strong linear classification accuracy - it is not clear that such a model will extract a general representation for generative evaluation rather than one that over-relies on object-based semantic information.

Understanding the interdependence of evaluation metrics, representation extractors, and their relation to human evaluation of generated images requires a large scale study of each component across a diverse set of datasets. Here we select 41 state-of-the-art generative models spanning diffusion models , GANs, variational autoencoders (VAEs) , normalizing flows , transformer-based models , and consistency models , and generate 4.1M images to provide such a study:

**Human evaluation** We designed and funded extensive human subject experiments to establish a robust baseline for generated image fidelity, and find that \((i)\) no current metric strongly correlates with human evaluators, and that \((ii)\) diffusion models significantly outperform GANs and all other generative techniques at producing images that are indistinguishable from training data.

**Self-supervised representations and evaluation metrics** We show that \((iii)\) the Frechet distance, kernel distance (KD) , precision, and density calculated with the Inception-V3 network do not correlate well with human evaluation. We then investigate the semantic information distilled from self-supervised methods spanning a wide variety of families, showing that \((iv)\) the perceptual qualities

Figure 1: An illustration of learned distributions and samples (orange, crosses) having different properties with respect to the true distribution and training set (blue, squares). Italicized text indicates metrics that purport to detect these properties.

of their representation spaces can strongly depend on training procedure and architecture, that \((v)\) supervised networks do not provide a perceptual space that generalizes well for image evaluation, and that \((vi)\) replacing Inception-V3 with DINOv2 ViT-L/14 _solves the discrepancy with human evaluators_ while the previously proposed SwAV and CLIP-B/32 replacements [72; 7] are sub-optimal.

**Diversity, rarity, and memorization** By leveraging the recently proposed Vendi  and rarity  scores, we show that \((vii)\) the discrepancy between human evaluators and FID is not due to models trading off fidelity for diversity, nor to human evaluators assessing rare images as fake. We see these results as evidence that human error rate is a sensible "ground truth" to align FD metrics with. Finally, we answer the question: are the best performing models according to our DINOv2-based metrics memorizing their training data? In doing so, we \((viii)\) find clear evidence of memorized samples across models, particularly on CIFAR10, and show that current memorization metrics and tests fail to capture this [70; 1].

**Summary** Our multifaceted investigation of generative evaluation shows that diffusion models are _unfairly punished by the Inception network_: they synthesize more realistic images as judged by humans and their diversity more closely resembles the training data, yet are consistently ranked worse than GANs on metrics computed with Inception-V3. While FID is already known to have shortcomings, we advocate for a complete replacement of Inception-V3 in all evaluation metrics of images, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models.

## 2 Datasets, metrics, and encoders

**Generated datasets** We investigate a wide range of generative models trained on a diverse set of image datasets (CIFAR10 , ImageNet1k , FFHQ , LSUN-Bedroom ) using a variety of generative techniques (diffusion, GAN, VAE, normalizing flow, Transformer-based, consistency). For each dataset we include current state-of-the-art models as ranked by FID, as well as models spanning different generative procedures. We include 13, 11, 9, and 8 models for the respective datasets listed above, for a total of 41 generated datasets. To decouple the effects of model architecture/training procedure from the training data used, we focus only on generative models that did not include any data external to the respective dataset during training. Models for ImageNet1k, FFHQ, and LSUN-bedroom were trained at a resolution of 256\(\)256. We chose to generate 100k images from each model, with an equal number of images per-class for class conditional models, and used the checkpoints and hyperparameters that achieved the lowest FID for each model; Appendix A details the full generation procedure. In total we assess each generative model across 17 metrics. We group the metrics by category here and provide full definitions in Appendix B.

**Metrics for ranking generative models** We include the well-known FID , which computes the Frechet distance (FD) between sets of 50k real and generated samples in the representation space of the Inception-V3 network. We study FD in several alternative representation spaces, and refer to these by the model used to extract the representation from each image, e.g. FDCLIP. We include alternatives to the FID such as the spatial FID (sFID) , FID\({}_{}\), IS , kernel Inception distance (KID) , and the feature likelihood score (FLS) .

**Metrics for diagnosing fidelity, diversity, rarity, or memorization** As proxies for sample fidelity we consider precision [95; 64] and density , while our human evaluation baseline, human error rate, provides a direct measurement of fidelity. To quantify sample diversity we consider recall [95; 64] and coverage , and for sample rarity we consider the rarity score . To study inter- vs. intra-class diversity for class-conditional image generation we utilize the Vendi score . While recall and coverage can also be determined per class, the small number of generated samples available for each class (e.g. 100 for each ImageNet class) results in difficulty constructing robust nearest-neighbour-based estimates. We also investigate a form of overfitting that we term _memorization_, in which models memorize individual images from their training data and emit them at generation time. We perform a direct check for pixel-wise memorization for each generative model for each of our datasets and report this value as the memorization ratio in Section 5.2. We include automated metrics which claim to isolate memorization from the effects of overfitting or mode collapse: the percentage of authentic samples , \(C_{T}\) score , and the percentage of overfit Gaussians from FLS .

**Representation spaces for generative evaluation** With the aim of finding a more general perceptual representation space across the span of natural images, we employ a number of alternative encoders beyond the standard Inception-V3  trained for supervised classification on ImageNet. First, as a more modern supervised benchmark we use a ConvNeXt-large architecture trained on ImageNet22k , a larger dataset with more classes. We also include a number of self-supervised feature extractors as alternatives to supervised learners. While such networks have proven to be useful for concurrent vision tasks including classification, object-detection, and segmentation [14; 119; 5], it remains an open question as to how the objective and augmentations used for training affect the representation space for generative evaluation. Thus we include seven self-supervised methods from a variety of families  - contrastive (SimCLRv2 ), self-distillation (DINOV2 ), canonical correlation analysis (SwAV ), masked image modelling (MAE  and data2vec ), and language-image (CLIP , using the OpenCLIP implementation  trained on DataComp-1B ). We also consider DreamSim , an ensemble of three self-supervised models (DINO , CLIP, and OpenCLIP) that have each been fine-tuned to better align with human perception of image similarity using a dataset of human similarity judgments over image pairs. We design experiments to qualitatively and quantitatively understand their respective feature spaces.

For CNN-based models we use the ResNet50  architecture while for vision transformers (ViT)  we use ViT-L/14 as we found it provides a good tradeoff between representation quality and computation cost. For both we select weights that were trained with the dataset and hyperparameters that achieved the highest linear classification on ImageNet. Full details are in Appendix B.4.

## 3 Human evaluation of generated data

The goal of our human subject experiments is to establish a large-scale, scientifically grounded baseline for image generation fidelity. Our experiments specifically target image realism, not the diversity of a model's learned distribution, nor whether samples are memorized, as realism is unequivocally a property for which humans can provide "ground truth". We find that the vast majority of models tested per dataset can be separated in terms of their realism with statistical significance, thus providing a clear ranking which can be compared to calculated metrics like FID. To our knowledge, this is the largest human subject experiment on the evaluation of generative models performed to date, with over 1000 paid participants, and 207k individual responses collected.

**Experimental design** We evaluated human perception of generated image realism for each of the 41 models across 4 datasets described in Section 2. Our experimental design was informed by experts and best practices in psychophysics, the study of the human perceptual system. It follows the design of experiments for the HYPE\({}_{}\) metric , with several modifications to increase the quality of collected data and expand the scope of models and datasets tested. Each trial is a two alternative forced choice task where a participant is shown either a generated image from one model, or an image from the training dataset, and must choose if it is real or fake. Models were evaluated based on _human error rate_, the fraction of images which were incorrectly classified. This is a simple metric with the intuition that models with better fidelity produce images that are more difficult to distinguish from real images. We take human perceptions of image realism as ground truth, noting the expansive efforts of the community to generate images that appear photo-realistic to humans, and will be used by humans. Full design details are provided in Appendix C.

Concurrent work has also targeted human perceptual benchmarks for image synthesis on a smaller subset of GAN and diffusion models  with 10 times fewer participants. While we are excited to see human benchmarks gaining popularity, we note a number of concerns with their methods and thus downstream conclusions. Specifically, observers had no training period nor knowledge of the training set, and were tasked to judge whether images were "photo-realistic". We believe this task contains much more ambiguity than our two alternative forced choice assessment, and introduces various response biases into participants' judgments. In their second user study, observers were asked to evaluate the relative realism of sets of images. This task is difficult to evaluate because participants can use a single image in the set to guide their decision for the entire set.

**Results and analysis** The main results of our experiments are shown in Figure 2. We plot the mean human error rate along with standard error for each model, and sort models on the \(x\)-axis by their FID (lower is better). If FID correlated well with human perception of fidelity, each plot would have a monotonically increasing trend, but this does not appear to hold. By inspection, the models with highest fidelity are almost always diffusion models, although GAN models often have lower FID. We find similar results for alternatives to the FID score in Appendix D.1.

To formally assess the relative performance of different model types, we separate participants' mean error rate into four one-way analyses of variance (ANOVA) - one per dataset - with model type (e.g. GAN, diffusion) as a between-subjects variable. We also performed planned comparisons between model types to probe omnibus effects. Analysis revealed effects of model type in all four ANOVAs, indicating significant differences between model types' effect on human error rate (all \(F\)'s > 12.09, all \(p\)'s < 0.001 ). Post hoc comparisons of mean error rate between model types using Bonferroni correction are shown in Table 1, where > indicates a significant difference, = indicates no significant difference, and model type is listed in order of descending mean error rate. Diffusion models were a clear standout for all datasets except FFHQ, where they performed on par with other model types (noting that only GANs appeared in more than one experiment on FFHQ). Coupling the results in Table 1 with the FID rankings in Figure 2, we conclude that current diffusion models produce the most realistic images according to human perception, but are downranked by FID.

## 4 Improved representation spaces for generative evaluation

### Qualitative examination of perceptual spaces

To qualitatively visualize what parts of an image the Frechet distance "perceives", we follow the gradient based visualization technique of , which focused on FID. Here we adapt and apply it to each of our CNN and ViT encoders. For CNN encoders our method is identical, while for ViTs we use the Grad-CAM variation introduced by . Experimental details can be found in Appendix D.2.1. Figure 3 shows two visualized samples from each high-resolution dataset. We find qualitative differences between CNN and ViT architectures - ViTs have a more global receptive field  - but also find starkly different characteristics between supervised and self-supervised models. In agreement with , regions deemed important by Inception are far from optimal for datasets outside of the ImageNet domain. For FFHQ the important features according to Inception are typically not part of the person's face, while for LSUN-Bedroom the focus is on a single object in the scene; features simply correspond with the Inception model's top-1 class prediction. We find that Inception does not perceive a holistic view of images even on its ImageNet training set, and see similar characteristics for ConvNeXt, indicating that such glaring issues are not mitigated by supervised training on more modern architectures, larger datasets, nor a larger numbers of classes.

Meanwhile SwAV and SimCLR often ignore important features, with SwAV exhibiting the strongest overlap with the classification networks. CLIP puts a large focus on the few main objects of an image - typically the main facial features (lips, eyes, etc.) or objects (beds) - which we presume is an outcome of the language-image pretext arising from image captions that do not describe texture or finer details of the image. DINOv2 usually focuses on the image structure as a whole while still identifying objects of importance. We argue that this is closer to the behaviour we would hope to

   Dataset &  \\  CIFAR10 & Diff. &  GAN &  VAE &  Flow \\ ImageNet & Diff. &  Transf. &  GAN & \\ LSUN & Diff. &  Transf. &  GAN &  Consistency \\ FFHQ & GAN &  Diff. &  Transf. &  VAE \\   

Table 1: Model types ranked by human error rate.

Figure 2: Human error rate on models ranked by FID. Data is displayed as the mean across participants with error bars showing the unbiased standard error.

have from an encoder meant to evaluate images, as it emphasizes the important objects while still being able to pick up on elements elsewhere. MAE and data2vec, both trained using masked image modelling, have a widespread focus on textures and shapes, with an often smaller importance for the semantic information related to the main object. We exclude DreamSim from this analysis as it is not straightforward to use Grad-CAM on its output, which is the concatenation of the output of multiple encoders. Appendix D.2.2 includes quantitative analyses, finding that masked models put more weight towards low-level image features rather than clustering classes by object semantics, while others (CLIP in particular) distill a more object-focused representation space - both in alignment with the qualitative analysis shown here.

In summary, we conclude that the representation spaces of self-supervised methods are more appropriate for generative evaluation than supervised approaches, and that self-distillation (DINov2) provides the best balance between focusing on important objects and holistic image structure.

### The (mis)alignment of evaluation metrics and human assessment

In conjunction with our human evaluation baseline, we evaluated the 17 ranking and diagnostic metrics outlined in Section 2 for each encoder and generated dataset. Figure 4 (top) shows the relation of human error rate with FD and precision. We investigate diversity further in the following section. We include the correlation of 7 common metrics and human error rate (bottom, best viewed while zoomed in). Note that across encoders, FD, FD\({}_{}\), and KD are very highly correlated, resulting in essentially the same model rankings. This suggests that all these metrics provide sensible ways of quantifying distances between probability distributions, provided a good encoder is chosen. Overall, we find that despite some sample complexity issues, the FD metric - when paired with an appropriate encoder - provides a strong way of evaluating generative models, see Appendix D.3 for details.

We find no strong correlation between human evaluation and any common metrics computed in the Inception representation space outside of the simplistic CIFAR10 dataset, showing that Inception fails to encode perceptually relevant features for the larger, more diverse, and complex datasets that are the testbeds driving advancement of generative model development. In agreement with the previous section we find relatively poor performance of the SwAV model, which was proposed as an alternative

Figure 3: Heatmaps visualizing what the Fréchet distance “perceives” for each encoder. The sign of the heatmap is given by the activations of the saliency layer that is visualized and does not reflect the sign of the gradient w.r.t. the FD – both bright yellow and deep blue can thus show an encoder’s focus. Additional examples are shown in Appendix D.2.1.

to the Inception model in  (in Appendix B.4.1 we also show poor alignment with a smaller CLIP-B/32 model investigated in a number of toy examples in ). CLIP VIT-L/14, DINOv2, and MAE display far greater alignment with the human experiment baseline, as does DreamSim, except on ImageNet - which we believe to be a particularly important dataset, as it is the most complex one being considered here, and it is thus used to train the most realistic models. Note that large precision values (which aims to quantify fidelity) do not consistently correspond to high human error rate, even for encoders whose FD strongly correlates with human evaluation: precision is thus likely measuring more than just fidelity, and FD should be preferred over it. We include analogous results for recall in Appendix D.1, showing that it does not only capture diversity.

We find that diffusion models are often driving the discrepancy in alignment with human evaluation for the Inception network: FID prefers GANs over diffusion while FD determined by self-supervised models trained on very large and diverse datasets does not. This discrepancy does not only occur on non-ImageNet benchmarks, as previous works have shown for GANs, but is also true on ImageNet.

## 5 Alternative explanations: diversity, rarity, and memorization

We have found that FID does not correlate with human error rate, and have shown that replacing the Inception-V3 network with an SSL encoder such as DINOv2-ViT-L/14 both recovers the correlation of FD with human error rate, and results in a metric which qualitatively focuses on the more relevant parts of images. While these are very promising characteristics for an evaluation metric, alternative explanations need to be ruled out before such a metric can be confidently adopted as a community standard. In this section we first verify that the lack of correlation between FID and human error rate is not due to models with large human error rates lacking diversity, nor to humans wrongly classifying rare real images as fake. This justifies our use of human error rate as "ground truth" for fidelity, and confirms that a lack of alignment with human error rate is a flaw of the encoder/metric pair. Then, we investigate whether the best performing generative models are memorizing their training data.

Figure 4: **Top**: Fréchet distance, precision, and human error rate for each generative model as measured by different encoders (columns) on different datasets (rows). Marker styles denote different generative techniques. Panels with a shaded background do not have strong (\(|r| 0.5\)) and significant (\(p 0.05\)) correlations between FD and human error rate. **Bottom**: Pearson correlation of metrics over the three high-resolution datasets.

### Diversity and rarity

**Diversity** FD-based metrics combine both fidelity and diversity into a single score, whereas human evaluation focuses only on the former. We must thus independently measure model diversity in order to confirm whether the lack of strong correlation between FID and human evaluation is due to the FID score being flawed as an evaluation metric which focuses on fidelity, or if the discrepancy can simply be explained by high fidelity models having worse diversity. To decide between these two alternatives we explore the extent to which \(_{}\) and FID align with diversity measures.

Our diversity analysis focuses on the Vendi score . We justify this choice in Appendix D.4 and verify that the Vendi score meaningfully quantifies diversity locally, but the same is not true globally. For example, Figure 6 displays samples from a DiT model on ImageNet with and without strong classifier-free guidance (cfg=4 and 1.5, respectively; we refer to the former model as DiT-guided), where it is evident that DiT-guided exhibits much lower per-class semantic diversity. Yet, Appendix D.4 shows that the overall Vendi score is higher for DiT-guided for almost all choices of encoders, while per-class Vendi scores are consistently lower for DiT-guided across encoders. These results justify the use of the per-class Vendi score as a sensible diversity metric; the overall Vendi score is mostly measuring inter-class diversity - which is not particularly meaningful for class-conditional models such as the ones commonly used on ImageNet - whereas the per-class scores focus on intra-class diversity consistently across encoders, and thus provide a more meaningful quantification of semantic diversity.

Equipped with the per-class Vendi scores, we evaluate the diversity of ImageNet models using the DINOv2 encoder in Figure 5 (left), where we can see that differences in diversity do not explain discrepancies between FID and human evaluations. For example, GigaGAN has diversity scores which are much farther away from those of the training data than most diffusion models, yet achieves a better FID (Figure 2). We see this as strong evidence of a limitation of the use of the Inception network to measure fidelity with FID. We perform the same analysis using the FD metric with the DINOv2 encoder in Figure 5 (right): this evaluation metric is not only much more correlated to human evaluators, but diversity also better explains the few discrepancies between the two, e.g. the lack of diversity in DiT-guided results in a worse \(_{}\) score than DiT despite having a better human error rate. Nonetheless, we highlight that the \(_{}\) score emphasizes fidelity more than it does diversity (e.g. the DiT-guided model still obtains a very strong score).

**Rarity** To ensure that participants are not confusing "unrealism" with "unlikeliness" and assessing rare images as fake - which would result in more diverse generative models ranking worse on human error rate - we investigated whether the human error rate on each real image (individual images were evaluated by an average of 13 humans) was correlated with the image's "rarity score" . Experiments are detailed in Appendix D.4, which show that human evaluators are _not_ confusing

Figure 5: **Left: Per-class Vendi scores of ImageNet models, in decreasing order of \(_{}\) score. Right: \(_{}\) on ImageNet, coloured by average per-class Vendi score (white corresponds to the train dataset).**

Figure 6: DiT-guided and DiT samples, labelled with per-class Vendi scores using DINOv2.

"unrealism" with "unlikeliness", and thus human error rate is a sensible ground truth for image fidelity. Combined with the diversity analysis above, this rules out diversity as an alternative explanation for the lack of alignment between human assessment and FID, and proves that alignment of FD metrics with human error rate is a desirable property for the studied generative models.

### Memorization

Recent works have shown that diffusion models are particularly prone to _memorization_ issues, in which models memorize individual images from their training data and emit them at generation time. Memorized samples may be near-pixel-wise identical, or semantically equivalent to their source object while differing in terms of pixel-wise identity, the latter termed _reconstructive memory_. Both predominantly occur either when the training set is small  or when there are a number of duplicate training samples . While it has been shown that diffusion and GAN models can memorize CIFAR10 samples  - a dataset that contains many duplicates  - to our knowledge it is currently unknown whether this occurs on larger, more diverse, and higher resolution datasets such as ImageNet, FFHQ, or LSUN-Bedroom, and whether this affects any of the metrics we report. We set out to investigate this here.

Our experiments (refer to Appendix D.6) suggest that the larger datasets (e.g. ImageNet) are not memorized by even the largest models we considered. This indicates that the models that are measured as superior in DINOv2 space are not capitalizing on memorization of the training data.

**Collecting memorized samples** We perform a direct check for pixel-wise memorization for each of the 100k images from each of our 41 generative models using the calibrated \(l_{2}\) distance proposed in . We find strong evidence on CIFAR10 that most generative models exhibit exact memory and showcase a set of memorized samples in Figure 7. We also report the memorization ratio of all models - the proportion of generated samples that match source samples in the training set - to illustrate degrees of exact memorization across different models. We found no conclusive evidence of pixel-wise memorization by any model on ImageNet, FFHQ, or LSUN-Bedroom, but found evidence of models exhibiting reconstructive memory on ImageNet and LSUN-Bedroom . We find that less than 0.5% of DiT-XL-2 images exhibit close reconstructive memory. Although reconstructive memory on complex datasets is not necessarily a major concern, we recommend monitoring the memorization ratio, especially as models become more powerful and pixel-wise memorization becomes more likely. We include examples in Figure 7 (left) and additional visualizations and details in Appendix D.6. We note that our study was not explicitly designed to detect a more "copy-paste" approach wherein models memorize aspects of different training images and combine them when sampling, and thus we cannot rule out all forms of memorization. We leave such investigations for future work.

**Evaluating memorization metrics** We evaluate the main memorization metrics in the literature in light of the memorized CIFAR10 samples we uncovered: the percentage of authentic samples (AuthPct) , the \(C_{T}\) score , and the percentage of overfit Gaussians in FLS (FLS-POG) . We first measure each metric's sensitivity to memorization in a controlled experiment, where we sample from the approximate posterior at different depths of a VDVAE's  hierarchical structure to generate a collection of synthetic datasets that serve as increasingly less faithful reconstructions of the training set. Appendix D.6.1 contains the full description and results, and establishes that AuthPct, the \(C_{T}\) score, and FLS-POG are sensitive to memorization in an ideal scenario where all samples become increasingly memorized. In Figure 7 (right) we investigate whether these automated metrics can differentiate models in practice, based on their measured CIFAR10 memorization ratios. Here we used the DINOv2 representation space, while results for other encoders are shown in Appendix D.6.2. For most encoders, the \(C_{T}\) score trends in the correct direction, whereas AuthPct and FLS-POG trend inconsistently and fail to differentiate between models with different numbers of memorized samples.

By swapping the training set with a _test set_ that the model cannot possibly memorize, we check whether memorization metrics are sensitive to confounding properties other than memorization, such as fidelity or mode collapse. This experiment is not possible for FLS-POG as it does not use the training set, and for the \(C_{T}\) score we split the test set into two as it is already used. The results for \(C_{T}\) and AuthPct on test data are depicted with low opacity in Figure 7 (right). Surprisingly, in both cases the results against the test set closely follow those against the training set, meaning that the \(C_{T}\) score and AuthPct are dominated by some property other than memorization. Based on our analysis in Appendix D.6.3, we postulate that these metrics focus more on mode shrinkage and image fidelity (see Figure 1), respectively. From this analysis, we note that modifying the \(C_{T}\) score by swapping the roles of the training and generated datasets makes it insensitive to mode collapse. We include this modification of the \(C_{T}\) score in Figure 7 (right).

In conclusion, we find that none of AuthPct, the \(C_{T}\) score, or FLS-POG is a reliable metric for memorization. FLS-POG correlates poorly with our estimates of the percentage of memorized samples, while the \(C_{T}\) score and AuthPct detect mode shrinking and image fidelity more than memorization. The reason behind these deficiencies is left to future work. Concurrent work  shows that in high dimensions moving the support of a distribution can drastically change precision and recall (measured using \(k\)-nearest neighbors), and the observed phenomenon here might have a similar cause. Our recommended modification to the \(C_{T}\) score improves on the \(C_{T}\) score and AuthPct, but still does not correlate well with the memorization ratio. Instead of using these metrics, we recommend researchers directly search for and collect memorized images using the calibrated \(l_{2}\)-distance as described above, even though it is labour-intensive and requires tuning.

## 6 Conclusions

We carried out the largest and most comprehensive assessment of generative model evaluation metrics to date. We found that currently prevalent metrics such as FID are not strongly predictive of human error rate, and that diffusion models achieve a higher human error rate than their GAN counterparts, yet often are ranked worse according to FID. Our multiple investigations show that this discrepancy is not caused by model diversity, nor by humans assessing rare but real images as fake. Together, these findings imply that the differences between FID and human assessment are unfairly punitive towards diffusion models (in terms of assessing fidelity). We also showed that these deficiencies can be mostly addressed by replacing the Inception encoder by DINov2 ViT-L/14. We include a table with the FDINOv2 score and many other metrics for all the models we considered in Appendix E, which we hope will be useful as an updated leaderboard of model performance. Finally, FD-based metrics are not designed to detect memorization, and we show that except on CIFAR10, the best performing models in terms of FDDINOv2 are not memorizing their training data. In doing so, we also found that while the calibrated \(l_{2}\) distance proposed in  is a reliable way to identify memorized samples, other metrics to detect memorization are not ideal. We thus advocate for work proposing new generative models to report the ratio of memorized samples using the calibrated \(l_{2}\) metric alongside metrics computed with DINov2-ViT-L/14. Because it requires tuning, finding more automated metrics that reliably detect memorization will be a productive avenue for future research.

Figure 7: **Left**: Generated samples on top with matched training samples on the bottom showcasing exact memorization on CIFAR10 and reconstructive memorization on ImageNet and LSUN-Bedroom. **Right**: CIFAR10 models plotted by memorization ratio vs. metric. Memorization metrics against the test set instead of the training set are shown with low opacity.