# Continual learning with the neural tangent ensemble

Ari S. Benjamin Christian Pehle Kyle Daruwalla

Cold Spring Harbor Laboratory

Cold Spring Harbor, NY 11724

{benjamini,pehle,daruwal}@cshl.edu

###### Abstract

A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions. This suggests that if a (single) neural network could be interpreted as an ensemble, one could design effective algorithms that learn without forgetting. To realize this possibility, we observe that a neural network classifier with N parameters can be interpreted as a weighted ensemble of N classifiers, and that in the lazy regime limit these classifiers are fixed throughout learning. We call these classifiers the _neural tangent experts_ and show they output valid probability distributions over the labels. We then derive the likelihood and posterior probability of each expert given past data. Surprisingly, the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights. Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time. These results offer a new interpretation of neural networks as Bayesian ensembles of experts, providing a principled framework for understanding and mitigating catastrophic forgetting in continual learning settings.

## 1 Introduction

Neural networks often forget previous knowledge when trained with gradient descent. In contrast, animals learn from sequential experiences, suggesting that true 'lifelong learners' use different strategies for learning .

One strategy to learns without forgetting is to update the posterior distribution over a set of fixed probabilistic models . This includes any fully Bayesian model, such as Bayesian linear regression. The fundamental reason why these algorithms do not forget information is because the posterior over models is invariant to data sequence. Given two permutations of the data, the posterior will be the same. This property of posteriors has inspired many strategies to reduce forgetting by approximating the posterior distribution over neural networks . However, these approximations introduce many new parameters and considerable memory overhead. In general, estimating the full posterior distribution over high-dimensional networks is prohibitive.

Here, we shift our perspective and instead interpret a _single_ neural network as an ensemble of many experts. This allows tracking a posterior (over experts, instead of networks) without introducing any memory overhead besides the network itself.

This motivates our main result, which we note is generally applicable outside of continual learning. More specifically, we show that **neural network classifiers perturbed by a small vector in parameter space can be described as a weighted ensemble of valid classifiers** outputting a probability distribution over labels. We call this the Neural Tangent Ensemble (NTE). Inspired by the Neural Tangent Kernel, this result depends on a first-order Taylor expansion around a seed point . As a consequence, it operates as an ensemble of _fixed_ classifiers in the NTK limit of infinite width.

In this framework, learning is framed as Bayesian posterior updating rather than optimization. These two approaches might be expected to be quite different, as a posterior update is multiplicative whereas gradient-based optimization is additive. Surprisingly, however, we find that the NTE posterior update rule is approximately stochastic gradient descent (SGD) on the network with batch size 1, thus shedding new light on the dynamics of neural network optimization.

Our primary contributions are:

* We introduce the Neural Tangent Ensemble (NTE), a novel formulation that interprets networks as ensembles of classifiers, with each parameter contributing one classifier.
* We derive the posterior update rule for the NTE for networks in the lazy regime, in which experts are fixed, and show that it is equivalent to single-example stochastic gradient descent (SGD) without momentum, projected to the probability simplex.
* This justifies the empirical finding that SGD with no momentum forgets much less than standard optimizer settings.
* We demonstrate that catastrophic forgetting in neural networks is associated with the transition from the lazy to the rich regime.

## 2 Motivation: Ensembles are natural continual learners

To demonstrate why Bayesian ensembles make good continual learners, consider a function \(f(x)\) that is an ensemble of many experts \(f_{i}(x)\) (Fig. 1). We will consider what it takes to modify this ensemble so that it performs well on two tasks \(\) and \(\).

A simple strategy for continual learning is to prune away experts. Let \(_{}\) be the set of functions that are good (and equally good) for task \(\). A good ensemble can be constructed by sampling from \(_{}\):

\[f_{}(x)=_{f_{i}_{}}f_{i}(x).\]

Given a subsequent task \(\), a new ensemble can be constructed on the fly by continuing to prune away the experts in \(f_{}(x)\) that perform poorly on task \(\). The remaining ensemble is still composed of experts from \(_{}\) (assuming that the set intersection is not zero).

In contrast to many continual learning strategies for neural networks, this does not require replay, task boundaries, or any additional memory dedicated to preserving old task performance.

### Belief updating generalizes set intersections

In real ensembles, experts do not perform equally well. This justifies weighing each expert in the ensemble with weights \(p_{i}\) which are chosen such that \(_{i}^{N}p_{i}=1\):

\[f_{}(x)=_{f_{i}}p_{i}f_{i}(x).\] (1)

This is particularly convenient if the experts encode the probability or belief about an event, \(f_{i}(x)=p(y|x,f_{i})\). In this case, one can weigh each function by its posterior probability given previous data:

\[p(y|x,)=_{f_{i}}p(f_{i}|)\ p(y|x,f_{i}).\] (2)

This is the optimal weighing strategy when the experts can be assumed to be independent .

It is useful to contrast the ensemble in Eq. 2 with linear regression using a feature map, \(f(x)=_{i}w_{i}_{i}(x)\), as might be observed in kernel regression. In an ensemble the weights \(w_{i}\) are strictly positive, whereas weights in regression may switch sign arbitrarily.

Figure 1: High-level intuition for model averaging and continual learning. Pruning the set of functions \(f_{i}\) to those good for task \(\), followed by further pruning for tasks \(\) and \(\), will result in a set of \(f_{i}\) still good on \(\).

### The posterior is invariant to data ordering

The property of Bayesian ensembles that motivates this paper is that the posterior probability of each expert is invariant to the order in which data in seen. This is because, like set intersections, single-task posteriors multiply to form the multi-task posterior:

\[p(f_{i}|) p(f_{i}|)p(f_{i}| ).\] (3)

This property is restated more formally in the following Lemma:

**Lemma 1**.: _Invariance to data ordering in Bayesian Ensembles. Let \(=f_{1},...,f_{N}\) be a set of fixed experts, \(=w_{1},...,w_{N}\) be their weights, and \(=D_{1},...,D_{T}\) be a sequence of datasets from \(T\) tasks. Then, for any permutation \(\) of the indices 1,..., T, \(p(f_{i}|)=p(f_{i}|D_{1},...,D_{T})=p(f_{i}|D_{(1)},...,D_{(T)})\)_

Proof.: By Bayes' rule, \(p(f_{i}|) p(f_{i})_{t=1}^{T}p(D_{t}|f_{i})\). The right-hand side is a product of terms, one for each dataset. Since multiplication is commutative, \(_{t=1}^{T}p(D_{t}|f_{i})=_{t=1}^{T}p(D_{(t)}|f_{i})\) for any permutation \(\). Therefore, \(p(f_{i}|D_{1},...,D_{T})=p(f_{i}|D_{(1)},...,D_{(T)})\). 

Thus, there is no catastrophic forgetting problem for models which are ensembles of fixed, independent probabilistic classifiers. This motivates assessing under what conditions neural networks approach this setting.

## 3 The Neural Tangent Ensemble

How might a neural network be described as an ensemble? One simple strategy would be to take the last network layer as a set of functions, and then to choose the output weights according to their relative performance. However, this is is an expensive strategy to construct a relatively small set of classifiers, and it does not specify how earlier weights might change.

Here, we employ a first-order Taylor expansion to show that neural networks are (approximately) large ensembles over \(N\) component functions, one for each edge in the network. We will examine a neural network \(p(y|x,W^{(t)})\) with parameters \(W^{(t)}\) whose output represents the probability or confidence of a label \(y\) given input \(x\). We can describe this output with a linearization around a very nearby _seed point_\(W^{(0)}\). Note that we use the notation \(W^{(0)}\) and \(W^{(t)}\) for consistency and in general \(W^{(0)}\) need not be the initialization or on the optimization trajectory at all.

\[p(y|x,W^{(t)}) p(y|x,W^{(0)})+_{i}^{N} w_{i})}{ w_{i}^{(0)}}\] (4)

At first glance it does not appear that this Taylor expansion is an ensemble. There seem to be no true classifiers: the gradients are not probabilities over classes \(y\), being neither nonnegative, bounded, nor normalized to 1 across the output labels. Nor are there true weights, as \( w_{i}\) is also not nonnegative. However, both of these criteria can be met with some rearrangements and with the assumption that the loss is sufficiently smooth with respect to its parameters. This leads to our main result:

**Theorem 2**.: _Suppose \(p(y|x,W^{(0)})\) is a neural network for which the log-likelihood is \(L-\)Lipschitz continuous in its parameters, i.e. all gradients of the loss are bounded by a constant \(L\). Let \(W^{(0)}\) then be perturbed by a \( W\) with \(\| W\|_{1}=z\). If the perturbation is sufficiently small (with \(zL<1\)), then **the network can be described as an ensemble of a set of N classifiers**\(\{p(y|x,f_{i})\}_{i}^{N}\), each with weight \(|}{z}\), plus higher-order contributions which vanish for small \(z\):_

\[p(y|x,W^{(t)})=_{i}^{N}|}{z}\ p(y|x,f_{i}) \,+\,(\| W\|_{2}^{2})\]_Each classifier \(p(y|x,f_{i})\), which we call the **neural tangent expert**, outputs a probability distribution over labels \(y\):_

\[p(y|x,f_{i})=p(y|x,W^{(0)})(1+z\,( w_{i})^{(0)}} p(y|x,W^{(0)}))\]

The proof is postponed to Appendix 8.1. Informally, it relies two simple rearrangements: splitting the weights into sign and magnitude \( w_{i}=| w_{i}|( w_{i})\), and bringing the zeroth order term inside the first-order sum. This results in a sum over a term which, surprisingly, sums to 1 over the output labels and is weighted by a term that sums to 1 over experts, meeting the criteria of an ensemble.

This simple reformulation invites a change in perspective about the role of each parameter in a deep neural network. Each parameter contributes a separate classifier. The distributed architecture and connected paths of the network matter, but they explicitly contribute through the gradients alone.

In the literature on ensembles, a common focus is to examine the _quality_ and _diversity_ of the experts separately. By the bias/variance decomposition, both aspects enter in the generalization error [38; 47]. Here, it is clear that all experts share a factor that is the overall quality of the center of the Taylor expansion, \(p(y|x,W^{(0)})\). What distinguishes experts from one another is the diversity of network gradients.

### Experts are fixed in the lazy regime

This paper is motivated by the fact that Bayesian ensembles of _fixed_ experts do not forget past data when learning by posterior updating. Under what conditions is the Neural Tangent Ensemble composed of fixed functions?

The answer to this question follows directly from the literature on the Neural Tangent Kernel (NTK) and lazy regime networks [19; 7]. If the network is in the 'lazy' regime, then the Jacobian of the network does not change during gradient descent learning and the linearization remains valid. This occurs in the limit of infinite width for MLPs for certain initializations . (Output scaling also controls laziness , and is a necessary when using softmax nonlinearities even in the infinite width .) As a consequence, the experts in the NTE interpretation are fixed functions in the lazy regime.

### Learning ensemble weights

If a network is secretly an ensemble, how should it learn from new data? The natural next step is to convert the NTE into a Bayesian ensemble. In a Bayesian ensemble, the weight of each function is its posterior probability given past data:

\[|}{z} p(f_{i}|)=|f_{i })\,p(f_{i})}{_{i}p(|f_{i})\,p(f_{i})}.\] (5)

This can be seen as the E step in a generalized EM algorithm . In the following section, we will describe how to calculate this posterior probability with an online learning algorithm. For the moment, we assume the experts are fixed functions (i.e. the network is lazy).

#### 3.2.1 The data likelihood

**Lemma 3**.: _For IID data \(=\{x_{k},y_{k}\}_{k=1}^{P}\), the likelihood of the data under an expert can be written in terms of a log-likelihood loss function \(_{k}^{(0)}=- p(y_{k}|x_{k},W^{(0)})\) of the network at initialization:_

\[p(|f_{i})=_{k}e^{-_{k}^{(0)}}(1-z\, ( w_{i})^{(0)}}_{k}^{(0)})\] (6)Proof.: Starting with the data likelihood,

\[p(|f_{i}) =_{k}p(y_{k}|x_{k},f_{i})\] (7) \[=_{k}(p(y_{k}|x_{k},W^{(0)})+z\, ( w_{i})^{(0)}}p(y_{k}|x_{k},W^ {(0)}))\] (8) \[=_{k}p(y_{k}|x_{k},W^{(0)})(1+z\,( w_{i})^{(0)}} p(y_{k}|x_{k},W^ {(0)}))\] (9)

Plugging in the definition of \(_{k}^{(0)}\) yields the above expression. 

#### 3.2.2 The posterior probability: renormalization

After the data likelihoods are computed for each neural tangent expert, they must be renormalized to obtain the posterior probabilities. In our case, we naturally have access to a very large number of tangent experts and their likelihoods. Indeed, if the width is indeed taken to infinity, this there are infinitely many neural tangent experts in a single network. We propose to use the distribution of likelihoods in the current network as a Monte Carlo estimate of the normalizing constant.

\[p(f_{i}|)=k}e^{-_{k}^{(0)}}(1-z\,( w_{i})^{(0)}}_{k}^{(0)} )p(f_{i})}{_{i}_{k}e^{-_{k}^{(0)}}(1-z\,( w_{i})^{(0)}}_{k}^{(0)} )p(f_{i})}\] (10)

This can be simplified by noting that each \(e^{-_{k}^{(0)}}\) term will cancel; the product \(_{k}e^{-_{k}^{(0)}}\) appears in every additive term in the denominator. Assuming a uniform prior \(p(f_{i})\), we then have:

\[p(f_{i}|)=(1-z\,( w_{i}) {}{ w_{i}^{(0)}}_{k}^{(0)})}{_{i}_{k}( 1-z\,( w_{i})^{(0)}}_{k}^ {(0)})}\] (11)

### The posterior update is (almost) stochastic gradient descent

We will now link this posterior expression with a neural network update rule. Recall that in Theorem 2, the normalized magnitude of each perturbation is interpreted as the posterior probability of the corresponding neural tangent expert.

\[|}{z} p(f_{i}|)\]

This means the parameters can act as a running cache of the posterior as new data is encountered. As in standard belief updating, this involves a likelihood update followed by renormalization. Surprisingly, this multiplicative belief update rule yields an update which is very close to SGD.

**Lemma 4**.: _For any network that is well-described as a first-order Taylor expansion around around \(W^{(0)}\) with perturbation \(\| W\|_{1}=z\), the posterior belief update given a new example is equivalent to single-example stochastic gradient descent under a cross-entropy loss objective, subject to the constraint that \(\| W\|_{1}=z\), and using a per-parameter learning rate of \(z| w_{i}|\)._

Proof.: The proof is a matter of writing out how the posterior changes with a single example. Multiplying by the likelihood of a new example, the unnormalized posterior updates as:

\[^{(t)}|}{z}=^{(t-1)}|}{z}(1- z\,( w_{i}^{(t-1)})^{(0)}} _{k}^{(0)})\] (12)This multiplicative update for the unnormalized weights can also be written an _additive_ rule. Multiplying by \(z\) and by \(( w_{i})\),

\[ w_{i}^{(t)}= w_{i}^{(t-1)}-z| w_{i}^{(t-1)}|^{(0)}}_{k}^{(0)}\] (13)

One can add the initial parameters to either side to yield a rule in the space of network parameters:

\[w_{i}^{(t)}=w_{i}^{(t-1)}-z| w_{i}^{(t-1)}|^{(0)}}_{k}^{(0)}\] (14)

This is true (single-example) stochastic gradient descent _projected in the L1 diamond_ with a learning rate \(z| w_{i}|\). Note that this does not allow averaging gradients across examples (a "batch size of 1" update) and that it uses the gradients at initialization (though see section 4.1).

To complete the update, the parameters should then be renormalized such that \(_{i}| w_{i}^{(t)}|=z\).

An alternative normalization scheme is to use a gradient projection algorithm. Adding a Langrage multiplier \(\) to Eq. 13 and solving for the \(\) that ensures \(_{i}| w_{i}|=z\) yields a update which keeps \(\| W\|_{1}=z\) even without renormalization:

\[w_{i}^{(t)}=w_{i}^{(t-1)}-z(| w_{i}^{(t-1)}|^{(0)}}_{k}^{(0)}-_{j}(| w_{ j}^{(t-1)}|^{(0)}}_{k}^{(0)}))\] (15)

Not only is the posterior update tractable, then, but it is sufficiently close to gradient descent that it can be interpreted in a standard optimization framework.

Although it may seem that our result would depend on the idiosyncratic likelihood function of the NTE, this result is nevertheless similar to previous algorithms that have been proposed as ways to weigh many experts. At high level, our result appears similar to the Multiplicative Weights algorithm described in . Another interpretation of this algorithm is as the _approximated exponential gradient descent with positive and negative weights_ algorithm from  but applied to the change in weights \( W\). There, it is derived by minimizing an arbitrary loss function under a constrained change in the relative entropy over ensemble weights to obtain the _exponentiated gradient descent algorithm_, which is then linearized with a Taylor expansion in the approximated version.

### Summary of the NTE theory

The Neural Tangent Ensemble is an interpretation of networks as ensembles of _neural tangent experts_. Updating the NTE of lazy networks as a Bayesian ensemble creates a perfect continual learner, in the sense that the multitask solution is guaranteed to be the same as the sequential task solution.

The posterior probability of each expert in the NTE is surprisingly tractable. Given a new example, the update rule is a simple additive rule in the space of network parameters which can be interpreted as projected gradient descent scaled by the change in parameters since initialization.

## 4 Networks away from the lazy regime

In real finite-width networks, gradients change throughout learning. Since each weight's corresponding neural tangent expert changes, there is no guarantee that weights at time \(t\) still reflect the cumulative likelihood of past data under that expert.

This phenomenon is clearly observed empirically by measuring how much experts change under the NTE update rule as a function of network width. In Fig. 2, we measure the average change in expert's Jacobian from initialization after training on MNIST as a function of network width with the NTE rule described above. Experts change less in wider networks than in narrow networks.

Another way this can be measured is by verifying that, in finite-width networks, the posterior update rule using the gradients around initialization does not lead to effective training. In Figure 3, we confirm that as the gradients lose correlation with the gradient at initialization, performance begins to rapidly degrade. This echoes the findings of  that linearized CNNs do not learn as effectively as their non-lazy counterparts. Thus, the NTE posterior update rule as written above is only effective when the Jacobian is truly static.

### Rich-regime networks are ensembles of adaptive experts

To ensure the NTE formulation remains valid, one can allow the seed point of the Taylor expansion (the 'initialization') to change throughout learning. This has an interesting interpretation. Namely, it allows us to view finite-width neural networks as **ensembles of changing neural tangent experts**.

**Lemma 5**.: _(informal) Let \(W^{(t)}\) be the parameters of a (finite-width) neural network. Choose a nearby **seed point**\(^{(t)}\) as \(W^{(t)}+\), with \(\) fixed and \(\|\|_{2}\) sufficiently small relative to the curvature such that the Jacobians of the log output probabilities of the perturbed and unperturbed networks are identical, \(J(^{(t)})=J(W^{(t)})\). The network can then be written as an ensemble of adaptive experts:_

\[p(y|x,f_{i}^{(t)})=p(y|x,^{(t)})(1+\|\|_{1}( _{i})^{(t)}} p(y|x,W^{(t)}))\]

_If \(\) is set as the uniform vector with values \(_{i}=\), the learning rate in the posterior update rule reduces to \(\|\|_{1}|_{i}|=\) and we recover stochastic gradient descent with mean-centered gradients and learning rate \(\):_

\[w_{i}^{(t+1)}=w_{i}^{(t)}-(^{(t)}} _{K}^{(t)}-_{j}(^{(t)}}_ {K}^{(t)}))\] (16)

Rich-regime learning is thus akin to a particle filter; each expert changes individually, but the prediction is the ensemble vote.

A interesting feature of this lemma is the equivalence between the rule that improves each expert (gradient descent on \(w\)) and the rule that decides how to weigh the experts in the ensemble (also gradient descent on \(w\)). This need not have been the case. As a result, one can perform belief updating assuming a fixed ensemble and end up improving each expert within it.

Figure 3: a) Gradients of an MLP at time \((t)\) rapidly lose correlation with the gradients at initialization. b) Training a network with the NTE posterior update rule fails when gradients diverge. Hyperparameters are reported in the Appendix.

Figure 2: The average squared difference between experts’ columns of the Jacobian measured at initialization and the end of training on MNIST with an 2-layer ReLU MLP and the NTE rule. Error bands indicate the standard deviation over 10 random seeds. As the width of the network increases, the average distance decreases, indicating the larger networks remain closer to the original linearization.

### The NTE rule with current gradients

Motivated by this result, we evaluated how well the NTE posterior update rule works when the gradients evaluated at initialization, \(^{(0)}}_{K}^{(0)},\) are replaced with the gradients of the current network \(^{(t)}}_{K}^{(t)}\). These converge in the infinite-width limit.

To obtain a practical algorithm, we additionally modify the NTE update rule with two hyperparameters that control the learning rate. First, noting that \(z\) in Eq. 14 acts as a learning rate, we replace it with a tunable parameter \(\). Secondly, we introduce a regularization parameter \(\) which keeps the network close to initialization as measured by the relative entropy of the change in parameters (see Appendix 8.2 for derivation). This constrains the amount of information contained in the weights .

Pseudocode for the resulting algorithm is in the Appendix 1. We also display the result of sweeps over \(\) and \(\) on the Permuted MNIST task in Fig. 7.

## 5 Predictions and results

Our findings suggest several ways to reduce forgetting in finite networks. First, networks closer to the lazy regime will better remember old tasks as long as the update rule is sufficiently similar to the NTE posterior update rule. Second, one should be able to reduce forgetting by ablating standard optimization methods like momentum and moving towards the NTE posterior update rule.

Below, we verify these predictions on the Permuted MNIST task with MLPs and on the task-incremental CIFAR100 with modern CNN architectures. In the Permuted MNIST task, an MLP with 10 output units is tasked with repeatedly classifying MNIST, but in each task the pixels are shuffled with a new static permutation. In task-incremental CIFAR100, a convolutional net with 100 output units sees only 10 classes each task. In the terminology of van de Ven et al, this is a task-incremental task, whereas Permuted MNIST is a domain-incremental task .

### Momentum causes forgetting

Momentum is not appropriate in a posterior update framework because it over-counts the likelihood of past data. Furthermore, it is a history-dependent factor. By contrast, posterior update rules are multiplicative and give identical results regardless of the order of data presentation.

Here, we report that _any_ amount of momentum with SGD is harmful for remembering past tasks. To our knowledge, this has not been noted by previous empirical studies on catastrophic forgetting [13; 36; 35; 2]. As can been seen in Fig. 4, increasing momentum monotonically increases forgetting a first task on Permuted MNIST. Similar trends exist for ResNet18 and ConvNeXtTiny on the CIFAR100 task (see Fig. 8) . Note that the momentum buffers were not reset between tasks; when they are reset, the momentum curve is nonmonotonic (see Fig. 9). Although momentum assists single-task performance, any amount of momentum will lead to forgetting previous knowledge.

Figure 4: Effect of momentum in SGD on the Permuted MNIST task for an MLP with 2 layers and 1,000 hidden units. (middle) Test accuracy on the first task at the end of training 5 sequential tasks. (right) Final test accuracy on the first task before seeing the other tasks. Error bars represent standard deviations over seeds. See Appendix for further parameters.

### Width improves remembering -- but only for certain optimizers

As networks grow wider and (slowly) approach their infinite-width limit, they should remember better if one uses the appropriate posterior update rule over the Neural Tangent Ensemble.

Previous literature confirms that this is generally the case. In , the authors report the benefits of scale are robust across architectures, tasks, and pretraining strategies, although they largely use SGD with momentum \(=0.9\). In , the authors report similar results and investigate other continual learning benchmark algorithms such as EWC (). Forgetting seems to be largely solved by scale.

The reason for this in our framework differs from the reason cited by both , which state that the gradients on different tasks will be more orthogonal in high dimensions, which reduces interference. Our interpretation is somewhat different and instead depends on the Jacobian of the network changes. We place no condition on gradient orthogonality between tasks. If the neural tangent experts are indeed fixed, the NTE update rule will find the multitask solution.

If this is the case, then wide networks should better remember only if the optimizer can be interpreted as a posterior update. In Fig. 5, we report that Adam's amnesia is not helped with increasing scale for the Permuted MNIST task. Although this could be for multiple reasons, we argue it stems from a divergence from a valid interpretation as a posterior update.

### The NTE posterior update rule using current gradients improves with scale

In Section 4.2, we introduced a modified version of NTE posterior update rule in which the Jacobian at initialization replaced with the current Jacobian. As networks get wider, this algorithm will converges to the proper update rule due to the fact that the network Jacobian does not change in the lazy regime. This predicts that this rule will improve with scale. To test this, we trained an ML on Permuted MNIST and ConvNeXtTiny on task-incremental CIFAR100 with this approximate rule. We find that both single-task and multitask learning are greatly improved with width (Fig. 5 and Fig. 10). We take this as empirical evidence that the proper NTE posterior update (with a static Jacobian) would work well in the infinite-width limit.

## 6 Related work

There is a long history of interpreting networks as ensembles. Networks with dropout, for example, allow this interpretation . This is also closely related to Mixture of Experts models in classic  and recent  work. These approaches explicitly encode the experts within the network, and unlike our work do not use a Taylor expansion to establish the ensemble experts.

The idea of a Bayesian ensemble _over_ networks is also well-studied. Such ensembles can either be assembled empirically through sampling , built via a Laplace approximation , or optimized . Bayesian posteriors are also common players in theoretical works using methods from statistical physics and PAC-Bayes . Some treatments of infinite-width limits study the ensemble of lazy learners . While similar in spirit, these methods study groups of many networks rather than view a single network as an ensemble.

Finally, there is related work that uses ideas from ensembles for continual learning. Many of these are in the category of methods that continually learn by training new modules for each task

Figure 5: Wider networks forget less, unless trained with Adam. See Alg. 1. All networks are 2-layer MLPs with ReLU nonlinearities trained on 5 Permuted MNIST tasks. Loss curves and further parameters in Appendix. Error bars represent standard deviations.

[49; 5; 42; 52; 39; 21]. Most directly related to this current work are papers that take a Bayesian approach and track statistics about the approximated posterior over networks [22; 10; 11; 28; 41; 37]. Many of these works in both categories require task boundaries. Furthermore, by introducing new modules or tracking statistics, these methods require additional memory to prevent forgetting.

### Moving in directions of low curvature to forget less

Our framework justifies the strategy of encouraging parameters to change mostly in directions of low curvature. Such regularization methods are already well-established and proven to reduce forgetting [24; 31; 41]. Although not directly equivalent, this is also similar to Elastic Weight Consolidation, which penalizes by the Fisher Information matrix (an expected second-order derivative of the _log_-likelihood, rather than the likelihood) . Another proximal method is Synaptic Intelligence, which penalizes parameter changes proportional to their integrated gradients along the path, which in the special case of diagonal, quadratic loss functions, is equivalent the Hessian . Thus, a second interpretation of why these methods work well (and improve with scale ) is that they ensure the tangent experts in the NTE do not change much while learning.

## 7 Discussion

Here, we described how networks in the lazy regime can be seen as ensembles of fixed classifiers. With this perspective, we proposed weighing each expert by its posterior probability to form a Bayesian ensemble, and derived the update rule. This strategy of learning by posterior updating has the benefit that the order of data presentation does not matter - sequential experience and interleaved experience lead to identical ensembles.

The posterior update rule to the Tangent Ensemble is surprisingly similar to SGD on the model weights. However, it is interesting to note that this update rule is suboptimal. Posterior probabilities are the optimal ensemble weights only when the experts are independent [47; 34; 38] and well-specified [33; 6]. This assumption is violated by the use of shared data, as well as the fact that neural network architectures introduce dependencies between gradients. Although this does not affect the equivalence between the interleaved and sequential task performance (i.e. forgetting), this will reduce the performance of networks trained with the NTE posterior update. This suggests avenues for improving SGD.

This suboptimality could be addressed in multiple ways. In the ensemble literature, there are many strategies to diversify the expert pool  such as repulsion . Different experts might also be trained on different data , and one might even take a boosting approach . It is very likely that these approaches would yield neural networks that outperform standard networks trained by updating the posterior distribution over tangent experts.

The ability in interpret single networks as ensembles opens many avenues for future research. These extend beyond continual learning; for example, one might be able to obtain a measure of uncertainty of the network output via the variance of the experts . We are hopeful that this insight will lead to deep learning systems that are better understood as their use expands within society.

## Code availability

The code for all figures in this paper were written in Jax and are available at https://github.com/ZadorLaboratory/NeuralTangentEnsemble.