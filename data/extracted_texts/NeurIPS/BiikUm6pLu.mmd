# Truncated Variance-Reduced Value Iteration

Yujia Jin

Stanford University

yujiajin@stanford.edu

&Ishani Karmarkar

Stanford University

ishanik@stanford.edu

&Aaron Sidford

Stanford University

sdford@stanford.edu

&Jiayi Wang

Stanford University

jyw@stanford.edu

###### Abstract

We provide faster randomized algorithms for computing an \(\)-optimal policy in a discounted Markov decision process with \(_{}\)-state-action pairs, bounded rewards, and discount factor \(\). We provide an \((_{}[(1-)^{-3}^{-2}+(1- )^{-2}])\)-time algorithm in the sampling setting, where the probability transition matrix is unknown but accessible through a generative model which can be queried in \((1)\)-time, and an \((s+_{}(1-)^{-2})\)-time algorithm in the offline setting where the probability transition matrix is known and \(s\)-sparse. These results improve upon the prior state-of-the-art which either ran in \((_{}[(1-)^{-3}^{-2}+(1- )^{-3}])\) time () in the sampling setting, \((s+_{}(1-)^{-3})\) time () in the offline setting, or time at least quadratic in the number of states using interior point methods for linear programming. We achieve our results by building upon prior stochastic variance-reduce value iteration methods . We provide a variant that carefully truncates the progress of its iterates to improve the variance of new variance-reduced sampling procedures that we introduce to implement the steps. Our method is essentially model-free and can be implemented in \((_{})\)-space when given generative model access. Consequently, our results take a step in closing the sample-complexity gap between model-free and model-based methods.

## 1 Introduction

Markov decision processes (MDPs) are a fundamental mathematical model for decision making under uncertainty. They play a central role in reinforcement learning and prominent problems in computational learning theory (see e.g., ). MDPs have been studied extensively for decades (), and there have been numerous algorithmic advances in efficiently optimizing them ().

In this paper, we consider the standard problem of _optimizing a discounted Markov Decision Process (DMDP)_\(=(,,,,)\). We consider the _tabular setting_ where there is a known finite set of _states_\(\) and at each state \(s\) there is a finite, non-empty, set of _actions_, \(_{s}\) for an agent to choose from; \(=\{(s,a):s,a_{s}\}\) denotes the full set of state action pairs and \(_{}:=||||\,.\) The agent proceeds in rounds \(t=0,1,2,.\) In each round \(t\), the agent is in state \(s_{t}\); chooses action \(a_{t}_{s_{t}}\), which yields a known reward \(_{t}=_{s_{t},a}\); and transitions to random state \(s_{t+1}\) sampled (independently) from a (potentially) unknown distribution \(_{a}(s_{t})^{}\) for round \(t+1\), where \(_{a}(s_{t})^{}\) is the \((s_{t},a)\)-th row of \(^{}\). The goal is to compute an _\(\)-optimal policy_, where a (deterministic) policy \(\), is a mapping from each state \(s\) to an action \((s)_{s}\) and is _\(\)-optimal_ if for every initial \(s_{0}\) the _expected discounted reward of \(\)_\([_{t 0}r_{t}^{t}]\) is atleast \(_{s_{0}}^{*}-\). Here, \(_{s_{0}}^{*}\) is the maximum expected discounted reward of any policy applied starting from initial state \(s_{0}\) and \(^{*}^{}\) is called the _optimal value_ of the MDP.

Excitingly, a line of work [15; 16; 2; 3; 17; 18] recently resolved the query complexity for solving DMDPs (up to polylogarithmic factors) in what we call the _sample setting_ where the transitions \(_{a}(s)\) are accessible only through a _generative model_ (). A _generative model_ is an oracle which when queried with any \(s\) and \(a_{s}\) returns a random \(s^{}\) sampled independently from \(_{a}(s)\). It was shown in  that for all \((0,(1-)^{-1}]\) there is an algorithm which computes an \(\)-optimal policy with probability \(1-\) using \((_{}(1-)^{-3}^{-2})\) queries where we use \(()\) to hide polylogarithmic factors in \(_{}\), \(^{-1}\), \((1-)^{-1}\), and \(^{-1}\). This result improved upon a prior result of  which achieved the same query complexity for \([0,(1-)^{-1/2}]\), of  which achieved this query complexity for \(\), and of  which achieved it for \([0,(||\,(1-))^{-1/2}]\). This query complexity is known to be optimal in the worst case (up to polylogarithmic factors) due to lower bounds of  (and extensions of ), which established that the optimal query complexity for finding \(\)-optimal policies with probability \(1-\) is \((_{}(1-)^{-3}^{-2}(_{}^{-1}))\).

Interestingly, recent state-of-the-art results [17; 18] (as well as ) are _model-based_: they query the oracle for every state-action pair, use the resulting samples to build an empirical model of the MDP, and then solve this empirical model. State-of-the-art computational complexities for the methods are then achieved by applying high-accuracy, algorithms for optimizing MDPs in what we call the _offline setting_, when the transition probabilities are known [2; 17].

Correspondingly, obtaining optimal query complexities for large \(\), e.g., \( 1\), comes with certain costs. This setting is of interest when the goal is to efficiently compute a coarse approximation of the optimal policy. Model-based methods use space \((_{}((1-)^{-3}^{-2},| |))\)-rather than the \((_{})\) memory used by _model-free_ methods (e.g., [2; 3; 21]), which run stochastic, low memory analogs of classic popular algorithms for solving DMDPs (e.g., value iteration). Moreover, although state-of-the-art model-based methods use \((_{}(1-)^{-3}^{-2})\)_samples,_ the state-of-the-art _runtime_ to compute the optimal policy is either \((_{}(1-)^{-3}\{1,^{-2} \})\) (using ) or has a larger larger polynomial dependence on \(_{}\) and \(||\) by using interior point methods (IPMs) for linear programming (see Section 1.1). Consequently, in the worst case, the _runtime cost_ per sample is more than polylogarithmic for \(\) sufficiently larger than \(1\), and it is natural to ask if this can be improved.

These costs are connected to the state-of-the-art runtimes for optimizing DMDPs in the offline setting. Ignoring IPMs (discussed in Section 1.1), the state-of-the-art runtime for optimizing a DMDP is \((()+_{}(1-)^{-3})\) due to  where \(()\) denotes the number of non-zero entries in \(\), i.e., the number of triplets \((s,s^{},a)\) where taking action \(a_{s}\) at state \(s\) has a non-zero probability of transitioning to \(s^{}\). This method is essentially model-free; it simply performs a variant of stochastic value iteration where passes on \(\) are used to reduce the variance of sampling and can be implemented in \(()\)-space given access to a generative model and the ability to multiply \(\) with vectors. The difficulty in further improving the runtimes in the sample setting and improving the performance of model-free methods seems connected to the difficulty in improving the additive \(_{}(1-)^{-3}\)-term in this runtime (see the discussion in Section 1.2.)

In this paper, we ask whether these complexities can be improved. _Is it possible to lower the memory requirements of near-optimal query algorithms for large \(\)? Can we improve the runtime for optimizing MDPs in the offline setting and can we improve the computational cost per sample in computing optimal policies in DMDPs?_ More broadly, _is it possible to close the sample-complexity gap between model-free and model-based methods for optimizing DMDPs?_

### Our results

In this paper, we show how to answer each of these motivating questions in the affirmative. We provide faster algorithms for optimizing DMDPs in both the sample and offline setting that are implementable in \((_{})\)-space provided suitable access to the input. In addition to computing \(\)-optimal policies, these methods also compute _\(\)-optimal values_: we call any \(^{}\) a _value vector_ and say that it is \(\)-optimal if \(\|-^{*}\|_{}\).

Here we present our main results on algorithms for solving DMDPs in sample setting and in the offline setting and compare to prior work. For simplicity of comparison, we defer any discussion and comparison of DMDP algorithms that use general IPMs for linear program to the end of this section. The state-of-the-art such IPM methods obtain improved running times but use \((||^{2})\) space and \((||^{2})\) time and use general-purpose linear system solvers. As such they are perhaps qualitatively different from the more combinatorial or dyanmic-programming based methods, e.g., value iteration and stochastic value iteration, more commonly discussed in this introduction.

In the sample setting, our main result is an algorithm that uses \((_{}[(1-)^{-3}^{-2}+(1-) ^{-2}])\) samples and time and \(O(_{})\)-space. It improves upon the prior, non-IPM, state-of-the-art which uses \((_{}[(1-)^{-3}^{-2}+(1- )^{-3}])\) time  and nearly matches the state-of-the-art sample complexity for all \(=O((1-)^{-1/2})\). See Table 2 for a more complete comparison.

**Theorem 1.1**.: _In the sample setting, there is an algorithm that uses \((_{}[(1-)^{-3}^{-2}+(1- )^{-2}])\) samples and time and \(O(_{})\) space, and computes an \(\)-optimal policy and \(\)-optimal values with probability \(1-\)._

Particularly excitingly, the algorithm in Theorem 1.1 runs in time nearly-linear in the number of samples whenever \(=O((1-)^{-1/2})\) and therefore, provided querying the oracle costs \((1)\), has a near-optimal runtime for such \(\)! Prior to this work such a near-optimal, non-IPM, runtime (for non-trivially small \(\)) was only known for \(=(1)\) (). Similarly, Theorem 1.1 shows that there are model-free algorithms (which for our purposes we define as an \((_{})\) space algorithm) which are nearly-sample optimal whenever \(=O((1-)^{-1/2})\). Previously this was only known for \(=(1)\). As discussed in prior-work (), this large \(\) regime is potentially of particular importance in large-scale learning settings, where one would like to _quickly_ compute a _coarse_ approximation of the optimal policy.

In the offline setting, our main result is an algorithm that uses \((()+_{}(1-)^{-2})\) time. It improves upon the prior, non-IPM, state-of-the-art which use \((()+_{}(1-)^{-3})\) time (). See Table 1 for a more complete comparison with prior work.

**Theorem 1.2**.: _In the offline setting, there is an algorithm that uses \((()+_{}(1-)^{-2})\) time, and computes an \(\)-optimal policy and \(\)-optimal values with probability \(1-\)._

The method of Theorem 1.2 runs in nearly-linear time when \((1-)^{-1}(()/_{})^{1/2}\), i.e., the discount factor is not too small relative to the average sparsity of rows of the transition matrix. Prior to this paper, such nearly-linear, non-IPM, runtimes (for non-trivially small \(\)) were only known for \((1-)^{-1}(()/_{})^{1/3}\) (). Thus, Theorem 1.2 expands the set of DMDPs which can be solved in nearly-linear time. The space usage and input access for this offline algorithm differs from the algorithm in Theorem 1.1 in that the algorithm in Theorem 1.2 assumes that access to the transition \(\) is provided as input and uses this to compute matrix-vector products with value vectors. The algorithm in Theorem 1.2 also requires access to samples from the generative model; if access to the generative model is not provided as input, then using the access to \(\), the algorithm can build a \((())\) data-structure so that queries to the generative model can be implemented in \((1)\) time (e.g., see discussion in ). Hence, if matrix-vector products and queries to the generative model can be implemented in \((_{})\)-space then so can the algorithm in Theorem 1.2.

  
**Algorithm** & **Runtime** & **Space** \\  Value Iteration  & \((()(1-)^{-1})\) & \((())\) \\  Empirical QVI  & \((()+_{}(1-)^{-3} ^{-2})\) & \((())\) \\  Randomized Primal-Dual Method  & \((()+E_{}(1-)^{-4} ^{-2})\) & \((_{})\) \\  High Precision Variance- & & \\ Reduced Value Iteration  & \((()+_{}(1-)^{-3})\) & \((_{})\) \\  Algorithm 4**This Paper** & \((()+_{}(1-)^{-2})\) & \((_{})\) \\   

Table 1: Running times to compute \(\)-optimal policies in the offline setting. In this table, \(E\) denotes an upper bound on the ergodicity of the MDP.

[MISSING_PAGE_FAIL:4]

Value iteration.Our approach stems from classic value-iteration method () for computing \(\)-optimal and its more modern \(Q\)-value and stochastic counterparts (). As the name suggests, value iteration proceeds in iterations \(t=0,1,\) computing _values_, \(^{(t)}^{S}\). Starting from initial \(^{(0)}^{S}\), in iteration \(t 1\), the value vector \(^{(t)}\) is computed as the result of applying the (Bellman) value operator \(:^{S}^{S}\), i.e.,

\[^{(t)}(^{(t-1)}()(s) :=_{a_{s}}(_{a}(s)+_{a}(s)^{}) s^{}\,.\] (1)

It is well-known that the value operator is \(\)-contractive and therefore, \(\|()-^{*}\|_{}\|-^{*}\|_{}\) for all \(v^{}\) (). If we initialize \(^{(0)}=\) then since \(\|^{*}\|_{}(1-)^{-1}\), we see that \(\|^{(t)}-^{*}\|_{}^{t}\|^{(0)}- ^{*}\|_{}^{t}(1-)^{-1}(1-)^{-1}(-t(1 -))\). Thus, \(^{(t)}\) are \(\)-optimal values for any \(t(1-)^{-1}(^{-1}(1-)^{-1})\). This yields an \((()(1-)^{-1})\) time algorithm in the offline setting.

Stochastic value iteration and variance reduction.To improve on the runtime of value iteration and apply it in the sample setting, a line of work implements _stochastic_ variants of value iteration (). Those methods take approximate value iteration steps where the _expected utilities_\(_{a}(s)^{}\) in (1) for each state-action pair are replaced by a _stochastic estimate_ of the expected utilities. In particular, note that \(_{a}(s)^{}=_{i_{a}(s)}\,_{i}\), i.e., the expected value of \(_{i}\) where \(i\) is drawn from the distribution given by \(_{a}(s)\). This is compatible in the sample setting, as computing \(_{i}\) for \(i\) drawn from \(_{a}(s)\) yields an unbiased estimate of \(_{a}(s)^{}\) with \(1\) query and \(O(1)\) time.

State-of-the-art model-free methods in the sample setting () and non-IPM runtimes in the offline setting () improve further by more carefully approximating the _expected utilities_\(_{a}(s)^{}\) of each state-action pair \((s,a)\). Broadly, given an arbitrary \(^{(0)}\) they first compute \(^{}\) that approximates \(^{(0)}\), i.e., \(_{a}(s)\) approximates \([^{(0)}]_{(s,a)}=_{a}(s)^{}^{(0)}\) for all \((s,a)\). In the offline setting, \(=^{(0)}\) can be computed directly in \(O(())\)-time. In the sample setting, \(^{(0)}\) can be approximated to sufficient accuracy using multiple queries for each state-action pair. Then, in each iteration \(t 1\) of the algorithm, fresh samples are taken to compute \(^{(t)}(^{(t-1)}-^{(0)})\) and perform the following update:

\[^{(t)}(s)_{a_{s}}(_{a}(s)+( {x}_{a}(s)+_{a}(s)^{(t)})s^{}\,.\] (2)

This approach is advantageous because sampling errors for estimating \((^{(t-1)}-^{(0)})\) depend on the magnitude of \(^{(t-1)}-^{(0)}\). After approximately computing \(\), the remaining task of computing \(^{(t)}(^{(t-1)}-^{(0)})\) so that \(+^{(t)}^{(t-1)}\) may be easier than the task of directly estimating \(^{(t)}\) (since \(^{(t-1)}-^{(0)}\) is smaller in magnitude than \(^{(t)}\) entrywise.) Due to similarities of this approach to variance-reduced optimization methods, e.g. (), this technique is called _variance reduction_.

The works , showed that if \(\) is computed sufficiently accurately and \(^{(0)}\) are \(\)-optimal values then applying (2) for \(t=((1-)^{-1})\) yields \(^{(t)}\) that is \(/2\)-optimal in just \((_{}(1-)^{-3})\) time and samples:  leverages this technique to compute \(\)-optimal values in the offline setting in \((()+_{}(1-)^{-3})\) time.  uses a similar approach to compute \(\)-optimal values in \((_{}[(1-)^{-3}^{-2}+(1- )^{-3})\) time and samples in the sample setting. A key difference in  and  is the accuracy to which they must approximate the initial utility \(^{(0)}\).

Recursive variance reduction.To improve upon the prior model-free approaches of  we improve how exactly the variance reduction is performed. We perform a similar scheme as in (2) and use essentially the same techniques as in  towards estimating \(\). Where we differ from prior work is in how we estimate the change in approximate utilities \(^{(t)}(^{(t-1)}-^{(0)})\). Rather than _directly_ sampling to estimate this difference we instead sample to estimate each individual \((^{(t-1)}-^{(t)})\) and maintain the sum. Concretely, for \(t 1\), we compute \(^{(t)}\) such that

\[^{(t)}(^{(t)}-^{(t-1)})\] (3)

so that these recursive approximations telescope. More precisely, setting \(^{(0)}=\), for \(t 1\), we set

\[^{(t)}^{(t-1)}+^{(t-1)}(^{( t-2)}-^{(0)})+(^{(t-1)}-^{(t-2)})=(^{(t-1)}-^{(0)}).\] (4)This difference is perhaps similar to how methods such as SARAH () differ from SVRG (). Consequently, we similarly call this approximation scheme _recursive variance reduction_. Interestingly, in constrast to the finite sum setting considered in , in our setting, recursive variance reduction for solving DMDPs ultimately leads to direct quantitative improvements on worst case complexity.

To analyze this recursive variance reduction method, we treat the error in \(^{(t)}(^{(t-1)}-^{(0)})\) as a martingale and analzye it using Freedman's inequality  (as stated in ). The hope in applying this approach is that by better bounding and reasoning about the changes in \(^{(t)}\), better bounds on the error of the sampling could be obtained by leveraging structural properties of the iterates.

_Unfortunately_, without further information about the change in \(^{(t)}\) or larger change to the analysis of variance reduced value iteration, in the worst case, the variance can be too large for this approach to work naively. Concretely, prior work () showed that it sufficed to maintain that \(\|^{(t+1)}-^{(t)}\|_{} O((1-))\). However, imagine that \(^{*}=\), \(^{(0)}=\), and in each iteration \(t\) one coordinate of \(^{(t)}-^{(t-1)}\) is \(()\). If \(||(1-)^{-1}\) and \(\|_{a}(s)\|_{}=O(1/||)\) for some \((s,a)\) then the variance of each sample used to estimate \(_{a}(s)^{}(^{(t)}-^{(t-1)})=(1/||)= ((1-))\). Applying Freedman's inequality, e.g., , and taking \(b\) samples for each \(O((1-)^{-1})\) iteration would yield, roughly, \(\|^{(t+1)}-(^{(t)}-^{(0)})\|_{}=O((1-)^{- 1}(1-)/)=O(1/)\). Consequently \(b=((1-)^{-2})\) and \(((1-)^{-3})\) samples would be needed in total, i.e., there is no improvement. Next, we will discuss how we circumvent this obstacle by _combining_ recursive variance reduction with a _second_ algorithm technique, which we call _truncation_.

**Truncated-value iteration.** The key insight to make our new recursive variance reduction scheme for value iteration yield faster runtimes is to modify the value iteration scheme itself. Recall that in the previous paragraph, we described that the case challenging case for recursive variance reduction occurs when, for example, in every iteration, a single coordinate of \(v\) changes by \(()\). We observe that there is a simple modification that one could make to value iteration to ensure that there is not such a large change between each iteration; simply _truncate_ the change in each iteration so that no coordinate of \(^{(t)}\) changes too much! To motivate our algorithm, consider the following _truncated_ variant of value iteration where

\[^{(t)}=(^{(t-1)}-(1-),(^{(t-1)}),^{(t-1)}+(1-))\] (5)

Where \(\) applies the median of the arguments entrywise. In other words, suppose we apply value iteration where we decrease or _truncate_ the change from \(^{(t-1)}\) to \(^{(t)}\) so that it is no more than \((1-)\) in absolute value in any coordinate. Then, provided that \(^{(t)}\) is \(\)-optimal, we can show that it is still the case that \(\|^{(t)}-^{*}\|_{}\|^{(t-1)}-^{*}\|_{}\). In other words, the worst-case progress of value iteration is unaffected! This follows immediently from the fact that \(\|^{(t)}-^{*}\|_{}\|^{(t-1)}-^{*}\|_{}\) in value iteration and the following simple technical lemma.

**Lemma 1.3**.: _For \(,,^{n}\) and \(,>0\), let \(:=\{-(1-),,+( 1-)\}\), where median is applied entrywise. Then, if \(\|-\|_{}\|-\|_{}\) and \(\|-\|_{}\), then \(\|-\|_{}\|-\|_{}\)._

Applying truncated value iteration, we know that \(\|^{(t)}-^{(t-1)}\|_{}(1-)\). In other words, the worst-case change in a coordinate has decreased by a factor of \((1-)\)! We show that this smaller movement bound does indeed decrease the variance in the martingale when using the aforementioned averaging scheme. We show this truncation scheme, when _combined_ with our recursive variance reduction scheme (4) for estimating \((^{(t)}-^{(0)})\), reduces the total samples required to estimate this and halve the error from \(((1-)^{-3})\) to just \(((1-)^{-2}\) per state-action pair.

**Our method.** Our algorithm applies stochastic truncated value iteration using sampling to estimate each \(^{(t)}(^{(t)}-^{(0)})\) as described. Some minor additional modifications are needed, however, to obtain our results. Perhaps the most substantial is our use of the _monotonicity technique_, as in prior work (). That is, we modify our method so that each \(^{(t)}\) is always an _underestimate_ of \(^{*}\) and the \(^{(t)}\)_increase_ monotonically as \(t\) increases. Thus, we only truncate the increase in the \(^{(t)}\) (since they do not decrease, and the median operation in (5) reduces to a minimum in Lemma 1.3).

Beyond simplifying this aspect of the algorithm, as in prior work, this monotonicity technique allows us to _simultaneously_ compute an \(\)-approximate policy as well as an \(\)-optimal value vector. We do this by tracking the actions associated with changed \(^{(t)}\) values, i.e., the \(\) in (2) in a variable \(^{(t)}\), which denotes the current estimated policy in iteration \(t\) of value iteration. Concretely, the monotonicity technique allows us to maintain the invariant that at each iteration \(t\), the current value estimate and policy estimate \(^{(t)}\), \(^{(t)}\) satisfy the relation \(^{(t)}[^{(t)}]\). Note that this ensures that the value of \(^{(t)}\) (denoted \(^{^{(t)}}\)) is _at least_\(^{(t)}\) because

\[^{(t)}[^{(t)}]^{2}[^{(t)}] ^{}[^{(t)}]=^{^{(t)}}\]

Thus, whenever \(^{(t)}\) is an \(\)-optimal value, \(^{(t)}\) is an _at least_\(\)-optimal policy.

By computing initial expected utilities \(=^{(0)}\) exactly, we obtain our offline results. By carefully estimating \(^{(0)}\) as in  we obtain our sampling results. Finally, building off of the analysis of  for deterministic or highly-mixing MDPs, we also show our method obtains even faster convergence guarantees under additional non-worst-case assumptions on the MDP structure.

### Notation and paper outline

General notation.Caligraphic upper case letters denote sets and operators, lowercase boldface letters denote vectors, and uppercase boldface letters (e.g., \(,\)) denote matrices. \(\) and \(\) denote the all-ones and all-zeros vectors, \([m]:=\{1,....,m\}\), and \(^{n}:=\{x^{n}: x x _{1}=1\}\) is the simplex. For \(^{}\), we use \(_{i}\) or \((i)\) for the \(i\)-th entry of vector \(\). For vectors \(^{}\), we use \(_{a}(s)\) to denote the \((s,a)\)-th entry of \(\), where \((s,a)\). We use \(},^{2},||^{n}\) for the element-wise square root, square, and absolute value of \(\) respectively and \(\{,\}\) and \(\{,,\}\) for element-wise maximum and median respectively. For \(,^{n}\), \(\) denotes that \((i)(i)\) for each \(i[n]\) (analogously for \(<,,>\).) We call \(^{n}\) an _\(\)-underestimate_ of \(^{n}\) if \(-\) for \( 0\) (see the discussion of monotonicity in Section 1.2 for motivation).

Dmp.As discussed, the objective in optimizing a DMDP is to find an \(\)-approximate policy \(\) and values. For a policy \(\), we use \(_{}():^{}^{}\) to denote the value operator associated with \(\), i.e., \(_{}()(s):=_{(s)}(s)+_{(s)}(s)^{} \) for all value vectors \(^{}\) and \(s\). We let \(^{}\) denote the unique value vector such that \(_{}(^{})=^{}\) and define its variance as \(_{^{}}:=^{}(^{})^{2}-(^{}^ {})^{2}\), where \(^{}^{}\) is the matrix such that \(_{s,s^{}}^{}=_{s,(s)}(s^{})\). The _optimal value vector_\(^{}^{}\) of the optimal policy \(^{}\) is the unique vector with \((^{})=^{}\), and \(^{}^{}:=^{^{ }}\).

Outline.Section 2 presents our offline setting results and Section 3 our sample setting results. Section 0.A discusses specialized settings where we can obtain even faster convergence guarantees. Omitted proofs are deferred to Appendix B.

## 2 Offline algorithm

In this section, we present our high-precision algorithm for finding an approximately optimal policy in the offline setting. We first define (Algorithm 1), which approximately computes products between \(^{S}\) and a value vector \(^{}\) using samples from a generative model. The following lemma states some immediate estimation bounds on () using linearity and the fact that \(^{}\).

``` Input: Value vector \(^{}\), sample size \(M\), and offset parameter \( 0\). for each \((s,a)\)do // In the sample setting, \(_{a}(s)\) is passed implicitly. \(_{a}(s)=(,_{a}(s),M,)\); return\(\) ```

**Algorithm 2**ApxUtility(\(,M,\))

**Lemma 2.1**.: _Let \(x=(,,M,0)\) for \(^{n}\), \(M_{>0}\), \(>0\), and \(^{}\). Then, \([x]=^{}\), \( x_{}\), and \([x] 1/M_{}^{2}\)._We can naturally apply \(\) to each state-action pair in \(\) as in the subroutine \(\) (Algorithm 2). If \(=(,M,)\), then \((s,a)\) is an estimate of the expected utility of taking action \(a_{s}\) from state \(s\) (as discussed in Section 1.2). When \(>0\), this estimate may potentially be shifted to increase the probability that \(\) underestimates the true changes in utilities; we leverage this in Section 3 (see also the discussion of monotonicity in Section 1.2). The terms arising in the definition of \(\) arise from applying Bernstein's inequality (Theorem B.2) to guarantee that \( x-\) with high probability.

The following algorithm \(\) (Algorithm 3) takes as input an initial value vector \(^{(0)}\) and policy \(^{(0)}\) such that \(^{(0)}\) is an \(\)-underestimate of \(^{}\) along with an approximate offset vector \(\), which is a \(\)-underestimate of \(^{(0)}\). It runs runs \(L=((1-)^{-1})\) iterations of approximate value iteration, making one call to \(\)(Algorithm 1) with a sample size of \(M=((1-)^{-1})\) in each iteration. The algorithm outputs \(^{L}\) which we show is an \(/2\)-underestimate of \(^{}\) (Corollary 2.5).

\(\) (Algorithm 3) is similar to variance reduced value iteration , in that each iteration, we draw \(M\) samples and use \(\) to maintain underestimates of \(_{a}(s)^{}(^{()}-^{(-1)})\) for each state-action pair \((s,a)\). However, there are two key distinctions between \(\)and variance-reduced value iteration  that enable our improvement. First, we use the recursive variance reduction technique, as described by (3) and (4), and second we apply truncation (Line 7), which essentially implements the truncation described in Lemma 1.3. Lemma 2.2 below illustrates how these two techniques can be combined to bound the necessary sample complexity for maintaining approximate transitions \(_{a}(s)^{}(^{(t)}-^{(0)})\) for a general sequence of \(_{}\)-bounded vectors \(\{^{(i)}\}_{i=1}^{T}\). The analysis leverages Freedman's Inequality  as stated in  and restated in Theorem B.1.

**Lemma 2.2**.: _Let \(T_{>0}\) and \(^{(0)},^{(1)},...,^{(T)}^{}\) such that \(\|^{(i)}-^{(i-1)}\|_{}\) for all \(i[T]\). Then, for any \(^{}\), \((0,1)\), and \(M 2^{8}T(2/)\) with probability \(1-\), \(|^{}(^{(t)}-^{(0)})-_{i[t]}_{j[M]} (^{(i)}-^{(i-1)},,1,0) 1/M|/8\) for all \(t[T]\)._

``` Input: Initial values \(^{(0)}^{}\), which is an \(\)-underestimate of \(^{}\). Input: Initial policy \(^{(0)}\) such that \(^{(0)}_{^{(0)}}(^{(0)})\). Input: Accuracy \([0,(1-)^{-1}]\) and failure probability \((0,1)\). Input: Offsets \(^{}\) ; // entrywise underestimate of \(^{(0)}\)
1 Initialize \(^{(1)}^{}\) and \(}^{(1)}^{}\) to \(\);
2\(L=(8)(1-)^{-1}\) and \(M= L 2^{8}(2_{}/)\) ;
3for each iteration \([L]\)do
4\(}=+(+}^{()})\);
5\(^{()}=^{(-1)}\) and \(^{()}=^{(-1)}\) ;
6for each state \(i\)do // Compute truncated value update (and associated action) \(}^{()}(i)=\{_{a_{i}}}_{i,a},^{(-1)}+(1-)\}\) and \(_{i}^{()}=*{argmax}_{a_{i}}}_{i,a}\); // Update value and policy if it improves if\(}^{()}(i)^{()}(i)\)then\(^{()}(i)=}^{()}(i)\) and \(_{i}^{()}=_{i}^{()}\) ; // Update for maintaining estimates of \((^{(l)}-^{0})\). \(^{()}=(^{()}-^{(-1)},M,0)\) and \(^{(+1)}=^{()}+^{()}\) ; // Shift estimates so that \(}^{(+1)}\) always underestimates \(_{a}(s)^{}^{()}\). \(}^{(+1)}=^{(+1)}-\);
7
8return\((^{(L)},^{(L)})\) ```

**Algorithm 3**\((^{(0)},^{(0)},,,)\)

While it is unclear how to significantly improve the constant of \(2^{8}=256\) appearing in Lemma 2.2 (and consequently Algorithm 3), we note that tightening these constants in the application of Freedman's inequality could be of practical interest. By applying Lemma 2.2 to the iterates \(^{()}\) in \(\), the following Corollary 2.3 shows that we can maintain additive \(O((1-))\)-underestimates of the transitions \(_{a}(s)^{}(^{()}-^{(0)})\) using only \((L)\) samples (as opposed to the \((L^{2})\) samples required in ) per state-action pair.

**Corollary 2.3**.: _In TVRVI (Algorithm 3), with probability \(1-\), in Lines 9, 10 and 2, for all \(s,a_{s}\), and \([L]\), we have \(|_{a}^{()}(s)-_{a}(s)^{}(^{(-1)}-^{(0 )})|(1-)/8\) and therefore \(}_{a}^{()}\) is a \((1-)/4\)-underestimate of \(_{a}(s)^{}(^{(-1)}-^{(0)})\)._

The following Lemma 2.4 shows that whenever the event in Corollary 2.3 holds, TVRVI (Algorithm 3) is approximately contractive and maintains monotonicity of the approximate values. By accumulating the error bounds in Lemma 2.4, we also obtain the following Corollary 2.5, which guarantees that TVRVI halves the error in the initial estimate \(^{(0)}\).

**Lemma 2.4**.: _Suppose that for some \(^{4}_{ 0}\), \(^{(0)}-^{(0)}\) and let \(_{^{}}^{}\) be defined as \(_{^{}}(s):=_{^{}(s)}(s)\) for each \(s\). Then, with probability \(1-\), at the end of every iteration \([L]\) (Line 3) in \((^{(0)},^{(0)},,,)\), the following hold for \(:=((1-)/4+_{^{}})\):_

\[^{(-1)} ^{()}_{^{()}}(^{()}),\] (6) \[0^{}-^{()} (^{}(^{}-^{(-1 )})+,(^{}-^{(-1)})).\] (7)

**Corollary 2.5**.: _Suppose that for some \( 0\) and \(^{}_{ 0}\), \(^{(0)}-^{(0)}\); \(^{(0)}\) is an \(\)-underestimate of \(^{}\); and \(^{(0)}_{^{(0)}}(^{(0)})\). Let \(_{^{}}^{}\) be defined as \(_{^{}}(s):=_{^{}(s)}(s)\) for each \(s\). Let \((^{(L)},^{(L)})=(^{(0)},^{(0)},,)\), and \(L,M\) be as in Line 2. Define \(:=((1-)/4+_{^{}})\). Then, with probability \(1-\), \(^{}-^{(L)}^{L}+(- ^{})^{-1}\), and \(^{(L)}_{^{(L)}}(^{(L)})\). In particular, if \(=\), then for \(L>(8)(1-)^{-1}\) we can reduce the error in \(^{(0)}\) by half: \(^{}-^{(L)}(^{}-^{(0)})/2\). Additionally, TVRVI is implementable with \((_{}ML)\) sample queries to the generative model and time and \(O(_{})\) space._

Theorem 1.2 now follows by recursively applying Corollary 2.5. OfflineTVRVI (Algorithm 4) provides the pseudocode for the algorithm guaranteed by Theorem 1.2.

``` Input: Target precision \(\) and failure probability \((0,1)\)
1\(K=_{2}(^{-1}(1-)^{-1})\), \(_{0}=\), \(_{0}\) is an arbitrary policy, and \(_{0}(1-)^{-1}\);
2for each iteration \(k[K]\)do
3\(_{k}=_{k-1}/2=2^{-k}(1-)^{-1}\);
4\(=_{k-1}\) ;
5\((_{k},_{k})=(_{k-1},_{k-1},,_{k-1}, 0,/K)\);
6return\((_{K},_{K})\) ```

**Algorithm 4**OfflineTVRVI\((,)\)

## 3 Sample setting algorithm

In this section, we show how to extend the analysis in the previous section in the sample setting, where we do not have explicit access to \(\). We follow a similar framework as in  to show that we can instead estimate the offsets \(\) in OfflineTVRVI by taking additional samples from the generative model. The pseudocode is shown in SampleTVRVI(Algorithm 5.) To analyze the algorithm, we first bound the error incurred when approximating the exact offsets \(\) in Line 4 of OfflineTVRVI (Algorithm 4) with approximate offsets \(}_{k-1}\) computed by sampling from the generative model. The proof leverages Hoeffding's and Bernstein's inequality, and follows a similar structure as the proof of Lemma 5.1 of .

**Lemma 3.1**.: _Consider \(^{}\). Let \(=(,m_{},)\), \(m(1/2^{-1})\), and \(=(m_{})^{-1}(1/2^{-1})\). Then, with probability \(1-\),_

\[-2_{^{*}}}+(2\| -^{*}\|_{}+18^{3/4}\|\|_{}) .\]

Finally, to obtain our main result Theorem 1.1, we utilize worst-case bounds on \(_{^{*}}\) from prior work  (see Lemma B.3, Lemma B.4) and inductively apply Lemma 3.1 and Corollary 2.5.

The constant of \(6500\) appearing in the initialization of \(N\) in Algorithm 5 arises due to technical reasons, from applying Bernstein's inequality, Hoeffding's inequality, union bound over all \(K\) outer loop iterations, and bounds on \(_{^{*}}\) from prior work  to prove Lemma 3.1. While it is unclear how to directly further tighten this constant, the proof of Lemma 3.1 shows that in the expression \(N=6500(1-)^{-3}(8_{}K^{-1})\) there is a natural trade-off between the leading constant (in this case \(6500\)) and the number of outer loop iterations \(K\). By increasing the number of outer-loop iterations \(K\) by constants, one can relax the error requirements of each iteration (i.e., decrease \(N\) by constants at the cost of increased logarithmic dependence on \(|S|,_{}\)). Although not the primary focus of our work, such trade-offs might be of practical importance.

## 4 Conclusion

We provided faster and more space-efficient algorithms for solving DMDPs. We showed how to apply truncation and recursive variance reduction to improve upon prior variance-reduced value iterations methods. Ultimately, these techniques reduced an additive \(((1-)^{-3})\) term in the time and sample complexity of prior variance-reduced value iteration methods to \(((1-)^{-2})\).

Natural open problems left by our work include exploring the practical implications of our techniques and exploring whether further runtime improvements are possible. For example, it may be of practical interest to explore whether there exist other analogs of truncation that do not need to limit the progress in individual steps of value iteration. Additionally, the question of whether the \(((1-)^{-2})\) term in our time and sample complexities can be further improved to \(((1-)^{-1})\) is a natural open problem; an affirmative answer to this question would yield the first near-optimal running times for solving a DMDP with a generative model for all \(\) and fully bridge the sample complexity gap between model-based and model-free methods. We hope this paper supports further studying these questions and establishing the optimal runtime for solving MDPs.