# SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning

Yifan Yang, Peiyao Xiao and Kaiyi Ji

Department of Computer Science and Engineering

University at Buffalo

Buffalo, NY 14260

{yyang99, peiyaoxi, kaiyiji}@buffalo.edu

###### Abstract

Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demonstrate the effectiveness of the proposed methods over existing FBO algorithms.

## 1 Introduction

Recent years have witnessed significant progress in a variety of emerging areas including meta-learning and fine-tuning , automated hyperparameter optimization , reinforcement learning , fair batch selection in machine learning , adversarial learning , AI-aware communication networks , fairness-aware federated learning , etc. These problems share a common nested optimization structure, and have inspired intensive study on the theory and algorithmic development of bilevel optimization. Prior efforts have been taken mainly on the single-machine scenario. However, in modern machine learning applications, data privacy has emerged as a critical concern in centralized training, and the data often exhibit an inherently distributed nature . This highlights the importance of recent research and attention on federated bilevel optimization, and has inspired many emerging applications including but not limited to federated meta-learning , hyperparameter tuning for federated learning , resource allocation over communication networks  and graph-aided federated learning , adversarial robustness on edge computing , etc. In general, the federated bilevel optimization problem takes the following mathematical formulation.

\[_{x^{p}}(x) =Fx,y^{*}(x):=_{i=1}^{n}p_{i}f_{i}(x,y^{*}(x))= _{i=1}^{n}p_{i}_{}f_{i}x,y^{*}(x);_{i} \] \[\;y^{*}(x) =*{arg\,min}_{y^{q}}G(x,y):=_{i=1}^ {n}p_{i}g_{i}(x,y)=_{i=1}^{n}p_{i}_{}g_{i}(x,y; _{i})\] (1)where \(n\) is the total number of clients, the outer- and inner-functions \(f_{i}(x,y)\) and \(g_{i}(x,y)\) for each client \(i\) take the expectation forms w.r.t. the random variables \(_{i}\) and \(_{i}\), and are jointly continuously differentiable. However, efficiently solving the federated problem in eq. (1) suffers from several main challenges posed by the federated hypergradient (i.e., \((x)\)) computation that contains the second-order global Hessian-inverse matrix, the lower- and upper-level data and system-level heterogeneity, and the nested optimization structure. To address these issues, [25; 65; 24; 22] proposed approximate implicit differentiation (AID)-based federated bilevel algorithms, which applied the idea of non-federated AID-based estimate in  to the federated setting, and involve two sub-loops for estimating the global lower-level solution \(y^{*}(x)\) and the Hessian-inverse-vector product, respectively.  then proposed AggITD by leveraging the idea of iterative differentiation, which improved the communication efficiency of AID-based approaches by synthesizing the lower-level optimization and the hypergradient computation into the same communication sub-loop. However, some limitations still remain in these approaches.

* First, the sub-loops, each with a large number of communication rounds, often compute products of a series of matrix-vector products, and hence can complicate the implementation and increase the communication cost.
* Second, the practical client sampling **without** replacement has not been studied in these methods due to challenges posed by the nested structure of AID- and ITD-based federated hypergradient estimators.
* Third, as observed in the single-level federated learning , in the presence of heterogeneous system capabilities such as diverse computing power and storage, clients can take a variable number of local updates or use different local optimizers, which may make these FBO algorithms converge to the stationary point of a different objective.

### Our Contributions

In this paper, we propose a communication-efficient federated bilevel method named SimFBO, which is simple to implement without sub-loops, flexible with a generalized server-side update, and resilient to system-level heterogeneity. Our specific contributions are summarized below.

**A simple and flexible implementation.** As illustrated in Figure 1, differently from AID- and ITD-based approaches that contain multiple sub-loops of communication rounds at each iteration, our proposed SimFBO is simpler with a single communication round per iteration, in which three variables \(y,x\) and \(v\) are updated simultaneously for optimizing the lower- and upper-level problems, and approximating the Hessian-inverse-vector product. SimFBO also includes a generalized server-side update on \(x,y,v\), which accommodates the client sampling without replacement, and allows for a flexible aggregation to improve the communication efficiency.

**Resilient server-side updates to system-level heterogeneity.** In the presence of heterogeneous local computation, we show that the naive server-side aggregation can lead to the convergence to a stationary point of a different objective. To this end, we propose System-level heterogeneity robust FBO (ShroFBO) building on a normalized version of the generalized server-side update with correction, which provably converges to a stationary point of the original objective.

Figure 1: Comparison of AID-based federated hypergradient estimation (FHE) in FedNest  (left), ITD-based FHE in AggITD  (middle) and our proposed SimFBO (right) at each iteration.

**Convergence analysis and improved complexity.** As shown in Table 1, our SimFBO and ShroFBO both achieve a sample complexity (i.e., the number of samples needed to reach an \(\)-accurate stationary point) of \((^{-2}P^{-1})\), which matches the best result obtained by FedMBO  but under a more practical client sampling without replacement. Moreover, SimFBO and ShroFBO both achieve the best communication complexity (i.e., the number of communication rounds to reach an \(\)-accurate stationary point) of \((^{-1})\), which improves those of other methods by an order of \(^{-1/2}\). Technically, we develop novel analysis in characterizing the client drifts by the three variables, bounding the per-iteration progress in the global \(y\) and \(v\) updates, and proving the smoothness and bounded variance in local \(v\) updates via induction, which may be of independent interest.

**Superior performance in practice.** In the experiments, the proposed SimFBO method significantly improves over existing strong federated bilevel baselines such as AggITD, FedNest and LFedNest in both the i.i.d. and non-i.i.d. settings. We also validate the better performance of ShroFBO in the presence of heterogeneous local computation due to the resilient server-side updates.

## 2 SimFBO: A Simple and Flexible Framework

### Preliminary: Federated Hypergradient Computation

The biggest challenge in FBO is to compute the federated hypergradient \((x)\) due to the implicit and complex dependence of \(y^{*}(x)\) on \(x\). Under suitable assumptions and using the implicit function theorem in , it has been shown that the \((x)\) takes the form of

\[(x)=_{i=1}^{n}p_{i}_{x}f_{i}(x,y^{*})-_{xy}^{2}G(x, y^{*})_{yy}^{2}G(x,y^{*})^{-1}_{i=1}^{n}p_{i}_{y}f_{ i}(x,y^{*})\]

which poses several computational challenges in the federated setting. First, the second term at the right-hand side contains three global components in a nonlinear manner, and hence the direct aggregation of local hypergradients given by

\[_{i=1}^{n}p_{i}_{x}f_{i}(x,y^{*})-_{xy}^{2}g_{i}(x,y^{* })_{yy}^{2}g_{i}(x,y^{*})^{-1}_{y}f_{i}(x,y^{*}) \]

is a biased estimation of \((x)\) due to the client drift. Second, it is infeasible to compute, store and communicate the second-order Hessian-inverse and Jacobian matrices due to the limited computing and communication resource. Although various AID- and ITD-based approaches have been proposed to address these challenges, they still suffer from several limitations (as we point out in the introduction) such as complicated implementation, high communication cost, lack of client sampling without replacement, and vulnerability to the system-level heterogeneity. To this end, we propose a simple, flexible and communication-efficient FBO framework named SimFBO in this section.

### Federated Hypergradient Surrogate

To estimate the federated hypergradient efficiently, we use the surrogate \(F(x,y,v)=_{x}F(x,y)-_{xy}^{2}G(x,y)v\), where \(v R^{d_{y}}\) is an auxiliary vector. Then, it suffices to find \(y\) and \(v\) as efficient

  
**Algorithm** &  System-level \\ heterogeneity \\  &  Partial \\ participation \\  &  Without \\ replacement \\  &  Linear \\ speedup \\  &  Samples \\ complexity \\  & 
 Communication \\ complexity \\  \\   FedNest  & ✗ & ✗ & ✗ & ✗ & \((^{-2})\) & \((^{-2})\) \\  FBO-AggITD  & ✗ & ✗ & ✗ & ✗ & \((^{-2})\) & \((^{-2})\) \\  FedBiO  & ✗ & ✗ & ✗ & ✓ & \((^{-2.5}n^{-1})\) & \((^{-1.5})\) \\  FedMBO  & ✗ & ✓ & ✗ & ✓ & \((^{-2}P^{-1})\) & \((^{-2})\) \\  SimFBO (this paper) & ✗ & ✓ & ✓ & ✓ & \((^{-2}P^{-1})\) & \((^{-1})\) \\  ShroFBO (this paper) & ✓ & ✓ & ✓ & ✓ & \((^{-2}P^{-1})\) & \((^{-1})\) \\   

Table 1: Comparison of different federated bilevel algorithms in the setting with heterogeneous data. We do not include the methods with momentum-based acceleration for a fair comparison. \(P n\) is the number of sampled clients in each communication round.

estimates of the solutions to the global lower-level problem and the global linear system (LS) \(^{2}_{yy}G(x,y)v=_{y}F(x,y)\) that is equivalent to solving following quadratic programming.

\[_{v}\;R(x,y,v) =v^{T}^{2}_{yy}G(x,y)v-v^{T}_{y}F(x,y)\] \[=_{i=1}^{n}p_{i}v^{T}^{2} _{yy}g_{i}(x,y)v-v^{T}_{y}f_{i}(x,y)}_{R_{i}(x,y,v)},\] (2)

where \(R_{i}(x,y,v)\) can be regarded as the loss function of client \(i\) for solving this global LS problem. Based on this surrogate, we next describe the proposed SimFBO framework.

### Simple Local and Server-side Aggregations and Updates

**Simple local update.** Differently from FedNest  and AggITD  that perform the lower-level optimization, the federated hypergradient estimation and the upper-level update alternatively in different communication sub-loops, our SimFBO conducts the simple updates on all these three procedures simultaneously in each communication round. In specific, each communication round \(t\) first selects a subset \(C^{(t)}\) of participating clients without replacement. Then, each active client \(i C^{(t)}\) updates three variables \(y,v,x\) at \(k^{th}\) local iteration **simultaneousously** as

\[y^{(t,k+1)}_{i}\\ v^{(t,k+1)}_{i}\\ x^{(t,k+1)}_{i}y^{(t,k)}_{i}\\ v^{(t,k)}_{i}\\ x^{(t,k)}_{i}-a^{(t,k)}_{i}_{y}_{y}g_{i} x^{(t,k)}_{i},y^{(t,k)}_{i};^{(t,k)}_{i}\\ _{v}_{v}R_{i}x^{(t,k)}_{i},y^{(t,k)}_{i},v^{(t,k)}_{i};^{ (t,k)}_{i}\\ _{x}f_{i}x^{(t,k)}_{i},y^{(t,k)}_{i},v^{(t,k)}_{i}; ^{(t,k)}_{i}\] (3)

where \(_{y}\), \(_{v}\), \(_{x}\) correspond to the local stepsizes, \(a^{(t,k)}_{i}\) is a client-specific coefficient to increase the flexibility of the framework, \(^{(t,k)}_{i}\), \(^{(t,k)}_{i}\), \(^{(t,k)}_{i}\) are independent samples, and the local hypergradient estimate takes the form of \(f_{i}(x,y,v;)=_{x}f_{i}(x,y;)-^{2}_{xy}g_{i}(x, y;)v_{i}\). The variables \(y,v\) and \(x\) in eq. (3), which optimize the lower-level problem, the LS problem and the upper-level problem, are updated with totally \(^{(t)}_{i}\) local steps. Note that the updates in eq. (3) also allow for parallel computation on \(x,v\) and \(y\) locally.

**Local and server-side aggregation.** After completing all local updates, the next step is to aggregate such local information on both the client and server sides. As shown in eq. (4), each participating client \(i C^{(t)}\) aggregates all the local gradients, and then communicate the aggregations \(q^{(t)}_{y,i},q^{(t)}_{v,i}\) and \(q^{(t)}_{x,i}\) to the server. Then, on the server side, such local information is further aggregated to be \(q^{(t)}_{y},q^{(t)}_{v}\) and \(q^{(t)}_{x}\), which will be used for a subsequent generalized server-side update.

\[q^{(t)}_{y} =_{i C^{(t)}}_{i}q^{(t)}_{y,i}=_{i C^ {(t)}}_{i}_{k=0}^{_{i}-1}a^{(t,k)}_{i}_{y}g_{i} x^{(t,k)}_{i},y^{(t,k)}_{i};^{(t,k)}_{i},\] \[q^{(t)}_{v} =_{i C^{(t)}}_{i}q^{(t)}_{v,i}=_{i C^ {(t)}}_{i}_{k=0}^{_{i}-1}a^{(t,k)}_{i}_{v}R_{i} x^{(t,k)}_{i},y^{(t,k)}_{i}v^{(t,k)}_{i};^{(t,k)}_{i},\] \[q^{(t)}_{x} =_{i C^{(t)}}_{i}q^{(t)}_{x,i}=_{i C^ {(t)}}_{i}^{_{i}-1}a^{(t,k)}_{i}f_{i}x^{(t,k)}_{i},y^{(t,k)}_{i},v^{(t,k)}_{i};^{(t,k)}_{i} }_{},\] (4)

where \(_{i}:=|}p_{i}\) is the effective weight of client \(i C^{(t)}\) among all participating clients such that \((_{i C^{(t)}}_{i})=1\). Note that in eq. (4), the local aggregation \(q^{(t)}_{y,i}\) (similarly for \(v\) and \(x\)) can be regarded as a linear combination of all local stochastic gradients, and hence covers a variety of local optimizers such as stochastic gradient descent, momentum-based gradient, variance reduction by choosing different coefficients \(a^{(t,k)}_{i}\) for \(i C^{(t)}\). This substantially enhances the flexibility of the proposed framework.

**Server-side updates.** Based on the aggregated gradients \(q_{y}^{(t)},q_{v}^{(t)}\) and \(q_{x}^{(t)}\), we then perform server-level gradient-based updates on variables \(x,v\) and \(y\) simultaneously as

\[y^{(t+1)}=y^{(t)}-_{y}q_{y}^{(t)},\;\;v^{(t+1)}=_{r}v^{( t)}-_{v}q_{v}^{(t)},\;\;x^{(t+1)}=x^{(t)}-_{x}q_{x}^{(t)},\] (5)

where \(_{y}\), \(_{v}\) and \(_{x}\) are server-side updating stepsizes for \(y\), \(v\), \(x\) and \(_{r}(v):=1,}v\) is a simple projection on a bounded ball with a radius of \(r\). There are a few remarks about the updates in eq. (5). First, in contrast to existing FBO algorithms such as , our introduced server-side updates leverage not only the client-side stepsizes \(_{y},_{v},_{x}\), but also the server-side stepsizes \(_{y},_{v}\) and \(_{x}\). This generalized two-learning-rate paradigm can provide more algorithmic and theoretical flexibility, and provides improved communication efficiency in practice and in theory. Second, the projection \(_{r}()\) serves as an important step to ensure the boundedness of variable \(v^{(t)}\), and hence guarantee the smoothness of the global LS problem and the boundedness of the estimation variance in \(v\) and \(x\) updates, both of which are crucial and necessary in the final convergence analysis. Note that we do not impose such projection on the local \(v_{i}^{(t,k)}\) variables because we can prove via induction that they are bounded given the boundedness of \(v^{(t)}\) (see Proposition 1).

### Resilient Server-side Updates against System-level Heterogeneity

**Limitations under system-level heterogeneity.** When clients have heterogeneous computing and storing capabilities (e.g., computer server v.s. phone in edge computing), an unequal number of local updates are often performed such that the global solution can be biased toward those of the clients with much more local steps or stronger optimizers. As observed in , this heterogeneity can deviate the iterates to minimize a different objective function. To explain this mismatch phenomenon, inspired by , we rewrite the server-side update on \(x\) (similarly for \(v\) and \(y\)) in eq. (4) as

\[q_{x}^{(t)}=_{i=1}^{n}p_{i}q_{i}^{(t,k)}=_{j=1}^{n }p_{j}\|a_{j}^{(t)}\|_{1}}_{^{(t)}}_{i=1}^{n}\|a_{i}^{(t)}\|_{1}}{_{j=1}^{n}p_{j}\|a_{j}^{(t)}\|_{1}}}_{w_{i}} ^{(t)}}{\|a_{i}^{(t)}\|_{1}}}_{h_{x,i}^{(t)}}.\] (6)

where \(a_{i}^{(t)}=[a_{i}^{(t,0)},...,a_{i}^{(t,_{i}^{(t)}-1)}]^{T}\) collects all local coefficients of client \(i\), and \(h_{x,i}^{(t)}\)**normalizes** the aggregated gradient \(q_{x,i}^{(t)}\) by \(1/\|a_{i}^{(t)}\|_{1}\) such that \(\|h_{x,i}^{(t)}\|\) does not grow with the increasing of \(_{i}^{(t)}\).

Although such normalization can help to mitigate the system-level heterogeneity, the effective weight \(w_{i}\) can deviate from the true weight \(p_{i}\) of the original objective in eq. (1), and the iterates converge to the stationary point of a different objective that replaces all \(p_{i}\) by \(w_{i}\) in eq. (1) (see Theorem 1).

**System-level heterogeneity robust FBO (ShroFBO).** To address this convergence issue, we then propose a new method named ShroFBO with stronger resilience to such heterogeneity. Motivated by the normalized reformulation in eq. (6), ShroFBO adopts a different server-side aggregation as

\[h_{y}^{(t)}=_{i C^{(t)}}_{i}h_{y,i}^{(t)}, h_{v}^{(t) }=_{i C^{(t)}}_{i}h_{v,i}^{(t)}, h_{x}^{(t)}=_{i  C^{(t)}}_{i}h_{x,i}^{(t)},\] (7)

where \(_{i}:=|}p_{i}\) and \(h_{y,i}^{(t)},h_{v,i}^{(t)},h_{x,i}^{(t)}\) are the normalized local aggregations defined in eq. (6). Accordingly, the server-side updates become

\[y^{(t+1)}=y^{(t)}-^{(t)}_{y}h_{y}^{(t)},\ \ v^{(t+1)}=_{ r}v^{(t)}-^{(t)}_{v}h_{v}^{(t)},\ \ x^{(t+1)}=x^{(t)}-^{(t)}_{x}h_{x}^{(t)}.\] (8)

Differently from eq. (6), we select the client weights to be \(_{i}\) to enforce the correct convergence to the stationary point of the original objective in eq. (1), as shown in Theorem 2 later.

## 3 Main Result

### Assumptions and Definitions

We make the following standard definitions and assumptions for the outer- and inner-level objective functions, as also adopted in stochastic bilevel optimization [26; 21; 30] and in federated bilevel optimization [65; 69; 24].

**Definition 1**.: _A mapping \(F\) is \(L\)-Lipschitz continuous if for \(\,z,\)\(\), \(\|F(z)-F(z^{})\| L\|z-z^{}\|\)._

Since the overall objective \((x)\) is nonconvex, the goal is expected to find an \(\)-accurate stationary point defined as follows.

**Definition 2**.: _We say \(z\) is an \(\)-accurate stationary point of the objective function \((x)\) if \(\|(z)\|^{2}\), where \(z\) is the output of an algorithm._

**Assumption 1**.: _For any \(x^{d_{x}}\), \(y^{d_{y}}\) and \(i\{1,2,...,n\}\), \(f_{i}(x,y)\) and \(g_{i}(x,y)\) are twice continuously differentiable, and \(g_{i}(x,y)\) is \(_{g}\)-strongly convex w.r.t. \(y\)._

The following assumption imposes the Lipschitz continuity conditions on the upper- and lower-level objective functions and their derivatives.

**Assumption 2**.: _Function \(f_{i}(x,y)\) is \(L_{f}\)-Lipschitz continuous; the gradients \( f_{i}(x,y)\) and \( g_{i}(x,y)\) are \(L_{1}\)-Lipschitz continuous; the second-order derivatives \(^{2}f_{i}(x,y)\) and \(^{2}g_{i}(x,y)\) are \(L_{2}\)-Lipschitz continuous; and the third-order derivatives \(^{3}g_{i}(x,y)\) is \(L_{3}\)-Lipschitz continuous for some constants \(L_{f},L_{1},L_{2},L_{3}>0\)._

The Lipschitz continuity of the third-order derivative is necessary here to ensure the smoothness of \(v^{*}(x)\), which guarantees the descent in the iterations of LS function (see Lemma 10), under our more challenging simultaneous and single-loop updating structure. Next, we assume the bounded variance conditions on the gradients and second-order derivatives.

**Assumption 3**.: _There exist constants \(_{f}^{2}\), \(_{g}^{2},_{gg}^{2}\) such that \(\| f_{i}(x,y)- f_{i}(x,y;)\|^{2} _{f}^{2}\), \(\| g_{i}(x,y)- g_{i}(x,y;)\|^{2} _{g}^{2}\) and \(\|^{2}g_{i}(x,y)-^{2}g_{i}(x,y;)\|^{2} _{gg}^{2}\)._

**Assumption 4**.: _For any \(x^{d_{x}}\), \(y^{d_{y}}\), there exist constants \(_{gh} 1\) and \(_{gh} 0\) such that_

\[_{i=1}^{n}w_{i}\|_{y}g_{i}(x,y)\|^{2}_{gh}^{2}\|_{i=1}^ {n}w_{i}_{y}g_{i}(x,y)\|^{2}+_{gh}^{2}.\]

_We have \(_{gh}=1\), and \(_{gh}=0\) when all \(g_{i}\)'s are identical._

This assumption of global heterogeneity uses \(_{gh}\) and \(_{gh}\) to measure the dissimilarity of \(_{y}g_{i}(x,y)\) for all \(i\).

### Convergence and Complexity Analysis

It can be seen from eq. (2) that the boundedness of \(v\) is necessary to guarantee the smoothness (w.r.t. \(x,y\)) and bounded variance in solving the local and global LS problems. Projecting the global \(v^{(t)}\) vector and the local \(v_{i}^{(t,k)},k 1\) vectors onto a bounded set can be a feasible solution, but in this case, the local aggregation \(q_{v,i}^{(t)}\) is no longer a linear combination of local gradients. This can complicate the implementation and analysis, and degrade the flexibility of the framework. Fortunately, we show via induction that the projection of the server-side vector \(v^{(t)}\) on a bounded set suffices to guarantee the boundedness of local vectors \(v_{i}^{t,k}\).

**Proposition 1** (Boundedness of Local \(v\)).: _Under Assumptions 1 and 2, for each iteration \(t\), client \(i\), and local iteration \(k=1,2,...,_{i}^{(t)}\), we have \(r_{i}:=\|v_{i}^{(t,k)}\|1+}{_{}}r,\) where the radius \(r=}{_{g}}\) and \(_{},_{}\) are chosen such that \(_{} a_{i}^{(t,k)}_{}.\)_

Next, we show an important proposition in characterizing the per-iteration progress of the global \(v^{(t)}\) updates in approximating the solution of a reweighted global LS problem. Let \(_{v}^{(t)}=\|v^{(t)}-^{*}(x^{(t)})\|^{2}\) denote the approximation error, where \(^{*}\) be the minimizer of \(_{i=1}^{n}w_{i}R_{i}(x,^{*},)\).

**Proposition 2**.: _Under the Assumption 1, 2 and 3, the iterates \(v^{(t)}\) in solving the global LS problem generated by Algorithm 1 satisfy_

\[\|v^{(t+1)}-^{*}(x^{(t+1)})\|^{2}- \|v^{(t)}-^{*}(x^{(t)})\|^{2}\] \[ (_{t}^{}-^{(t)}_{v}_{g}-_{t}^{ }^{(t)}_{v}_{g})\|v^{(t)}-^{*}(x^{ (t)})\|^{2}+(1+_{t}^{})(^{(t)}_{v})^{2} _{i C^{(t)}}_{i}h_{v,i}^{(t)}^{2}\] \[+(1+_{t}^{})^{(t)}_{v}^{2}}{ _{g}}y^{(t)}-^{*}(x^{(t)})^{2}+ ^{(t)}_{x}^{2}L_{v}^{2}+}{4} _{i C^{(t)}}_{i}h_{x,i}^{(t)} ^{2}\] \[+(1+_{t}^{})^{(t)}_{v}^{2}}{ _{g}}_{i=1}^{n}w_{i}_{k=0}^{_{i-1}}^{(t,k)}}{\|a_{i} ^{(t)}\|_{1}}x^{(t)}-x_{i}^{(t,k)}^{2}+ y^{(t)}-y_{i}^{(t,k)}^{2}\] \[+v^{(t)}-v_{i}^{(t,k)}^{2}+(^{(t)} _{x})^{2}}{_{t,1}^{}}_{i= 1}^{n}w_{i}_{x,i}^{(t)}^{2}.\]

_for all \(t\{0,1,...,T-1\}\), \(k\{0,1,...,_{i}^{(t)}-1\}\) and \(i\{1,2,...,n\}\), where \(_{i}:=|}w_{i}\)._

Similarly, we can provide a per-iteration process of \(y^{(t)}\) in approximating the solution \(^{*}\) of the reweighted lower-level global function \(_{i=1}^{n}w_{i}g_{i}(x,)\). Note that such characterizations do not exist in previous studies in single-level or minimax federated optimization with a single objective (e.g., ) because our analysis needs to handle three different lower-level, LS and upper-level objectives. As shown in Proposition 2, the bound involves the client drift term \(\|v^{(t)}-v_{i}^{(t,k)}\|^{2}\) (similarly for \(y,x\)), so the next step is to characterize this important quantity.

**Proposition 3**.: _Under Assumption 1 and 2, the local iterates client drift of \(v_{i}^{(t,k)}\) is bounded as_

\[_{i=1}^{n}w_{i}^{(t)}\|_{1}}_{k=1}^{_{i}-1}a_{i}^{ (t,k)}\|v_{i}^{(t,k)}-v^{(t)}\|^{2}_{v}^{2}_ {M1}^{2},\]

_for all \(t\{0,1,...,T-1\}\), \(k\{0,1,...,_{i}-1\}\) and \(i\{1,2,...,n\}\). We define \(:=_{i=1}^{n}_{i}/n\) and \(_{M1}^{2}:=_{}^{2}(_{f}^{2}+r_{}^{2}_{gg}^{2}) +_{}(L_{f}^{2}+r_{}^{2}L_{1}^{2})\)._

It can be seen from Proposition 3 that the bound on the client drift of the local updates on \(v\) is proportional to \(_{v}\) and \(\|a_{i}^{(t)}\|_{1}\). Since \(_{} a_{i}^{(t,k)}_{}\), \(\|a_{i}^{(t)}\|_{1}\) is proportional to the number \(_{i}^{(t)}\) of local steps. Thus, this client drift is controllable by choosing \(_{i}^{(t)}\) and the local stepsizes \(_{v}\) properly. Then, combining the results in the above Proposition 1, 2, 3, and under a proper Lyapunov function, we obtain the following theorem. Let \(P=|C^{(t)}|\) be the number of sampled clients.

**Theorem 1**.: _Define \((x)=(x,^{*})\) as the objective function by replacing \(p_{i}\) in eq. (1) with \(w_{i}\). Suppose Assumptions 1, 2 and 3 are satisfied. The iterates by SimFBO in Algorithm 1 satisfy_

\[_{t}(x^{(t)})^{2}= {(n-P)}{n}}{PT}}}_{ }+M_{2}T}}}_{}+}{T}}_{ },\] (9)

_where \(_{x}\), \(_{y}\), \(_{v}\), \(_{x}\), \(_{y}\), \(_{v}\) are set in eq. (38) and \(M_{1}\), \(M_{2}\), \(M_{3}\) are defined by eq. (40) in appendix. For the full client participation (i.e., \(P=n\)), the sample complexity is \(T=(n^{-1}^{-2})\), and the number of communication rounds is \(T=(^{-1})\). For partial client participation, the sample complexity is \(T=(P^{-1}^{-2})\), and the number of communication rounds is \(T=(P^{-1}^{-2})\)._

First, when set \(=(1)\), Theorem 1 shows that SimFBO converges to a stationary point of an objective function \((x)\) with a rate of \((}+)\), which, to the best of our knowledge, is the first linear speedup result under partial client participation without replacement. Note that without system-level heterogeneity, i.e., \(\|a_{1}^{(t)}\|=...=\|a_{n}^{(t)}\|\), \(w_{i}=\|a_{i}^{(t)}\|_{1}}{_{i=1}^{n}p_{j}\|a_{i}^{(t)}\|_{1}}= p_{i}\), and hence SimFBO converges to the stationary point of the original objective in eq. (1). However, in the presence of system-level heterogeneity, SimFBO may converge to the stationary point of a different objective. Second, when nearly full clients participate, the partial participation error is approximately zero. Then we can see that setting local update round \(\) to its upper-bound results in the best performance.

**Theorem 2**.: _Define \((x)=F(x,y^{*})\) as eq. (1). Suppose Assumptions 1, 2 and 3 are satisfied. The iterates generated by ShroFBO in Algorithm 1 satisfy_

\[_{t}(x^{(t)})^{2}= (n-P)}{n}}{PT}}+M_{ 2}T}}+}{T},\] (10)

_by setting the same server-side and local stepsizes and \(M_{1}\), \(M_{2}\) and \(M_{3}\) as in Theorem 1. For full client participation, the sample complexity is \(T=(n^{-1}^{-2})\), and the number of communication rounds is \(T=(^{-1})\). For partial client participation, the sample complexity is \(T=(P^{-1}^{-2})\), and the number of communication rounds is \(T=(P^{-1}^{-2})\)._

In Theorem 2, we show that even under the system-level heterogeneity, ShroFBO can converge to the original objective function with the same convergence rate as SimFBO. This justifies the design principle of robust server-side updates.

## 4 Related Work

**Bilevel optimization.** Bilevel optimization, first introduced by , has been studied for decades. A class of constraint-based bilevel methods was then proposed [20; 16; 59; 61], whose idea is to replace the lower-level problem by the optimality conditions. Gradient-based bilevel algorithms have attracted considerable attention due to the effectiveness in machine learning. Among them, AID-based approaches [8; 51; 38; 1] leveraged the implicit derivation of the hypergradient, which was then approximated via solving a linear system. ITD-based approaches [45; 12; 11; 56; 17] approximated the hypergradient based on automatic differentiation via the forward or backward mode. A group of stochastic bilevel approaches has been developed and analyzed recently based on Neumann series [5; 26; 1], recursive momentum [72; 23; 19] and variance reduction [72; 7], etc. For the lower-level problem with multiple solutions, several approaches were proposed based on the upper- and lower-level gradient aggregation [55; 43; 34], barrier types of regularization [41; 39], penalty-based formulations , primal-dual technique , and dynamic system-based methods .

**Federated (bilevel) learning.** Federated Learning was proposed to enable collaborative model training across multiple clients without compromising the confidentiality of individual data [32; 60; 49]. As one of the earliest methods of federated learning , FedAvg has inspired an increasing number of approaches to deal with different limitations such as slower convergence, high communication cost and undesired client drift by leveraging the techniques including proximal regularization , periodic variance reduction [48; 28], proximal splitting , adaptive gradients . Theoretically, the convergence of FedAvg and its variants has been analyzed in various settings with the homogeneous [63; 68; 64; 2] or heterogeneous datasets [37; 66; 48; 29].  analyzed the impact of thesystem-level heterogeneity such as heterogeneous local computing on the convergence.  further extended the analysis and the methods to the minimax problem setting.

Federated bilevel optimization has not been explored well except for a few attempts recently. For example, [14; 35] proposed momentum-based bilevel algorithms, and analyzed their convergence in the setting with homogeneous datasets. In the setting with non-i.i.d. datasets,  and  proposed FedNest and FedMBO via AID-based federated hypergradient estimation, and  proposed an ITD-based aggregated approach named Agg-ITD. Momentum-based techniques have been also used by [22; 36] to improve the sample complexity. Moreover, there are some studies that focus on other distributed scenarios, including decentralized bilevel optimization [6; 73; 44], asynchronous bilevel optimization over directed network , and distributed bilevel network utility maximization .

## 5 Experiments

In this section, we perform two hyper-representation experiments to compare the performance of our proposed SimFBO algorithm with FBO-AggITD , FedNest , and LFedNest , and validate the better performance of ShroFBO in the presence of heterogeneous local computation. We test the performance on MNIST and CIFAR datasets with MLP and CNN backbones. We follow the same experimental setup and problem formulation as in [65; 69]. The details of all experimental specifications can be found in Appendix A.1.

**Comparison to existing methods.** The comparison results are presented in Figure 2 and Figure 3. It can be seen that across different datasets and backbones, our proposed SimFBO consistently converges much faster than other comparison methods, while achieving a much higher training and test accuracy. We do not plot the curves of FedNest and LFedNest on CIFAR and CNN, because they are hard to converge under various hyperparameter configurations using their source codes.

**Performance under heterogeneous local computation.** We now test the performance in the setting where a total of \(10\) clients perform a variable number of local steps. This is to simulate the scenario where clients have heterogeneous computing capabilities and hence can perform an uneven number of local updates. In this experiment, we choose the number \(_{i}\) of the client \(i\)'s local steps from the set \(\{1,...,10\}\) uniform at random. As shown in Figure 4, the proposed ShroFBO method performs the best due to the better resilience to such client heterogeneity. We also compare the convergence rate of our proposed SimFBO, FedNest and FBO-AggITD w.r.t. running time. The results are provided in Figure 5. All the settings for different algorithms are the same as in Appendix A.2. It can be seen that the proposed SimFBO still converges fastest with a higher test accuracy in terms of running time.

Figure 3: Comparison of different methods: the test accuracy v.s. # of communication rounds.

Figure 2: Comparison among our SimFBO, FBO-AggITD , FedNest  and LFedNest . The left and middle ones plot the training accuracy v.s. # of communication rounds on i.i.d. MNIST datasets with MLP networks, and the right one plots the training accuracy v.s. # of rounds on i.i.d. CIFAR-10 datasets with a 7-layer CNN.

[MISSING_PAGE_FAIL:10]

*  M. Dagreou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. _arXiv preprint arXiv:2201.13409_, 2022.
*  J. Domke. Generic methods for optimization-based modeling. In _Artificial Intelligence and Statistics_, pages 318-326. PMLR, 2012.
*  A. Fallah, A. Mokhtari, and A. Ozdaglar. Personalized federated learning: A meta-learning approach. _arXiv preprint arXiv:2002.07948_, 2020.
*  M. Feurer and F. Hutter. Hyperparameter optimization. _Automated machine learning: Methods, systems, challenges_, pages 3-33, 2019.
*  C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
*  L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperparameter optimization. In _International Conference on Machine Learning_, pages 1165-1173. PMLR, 2017.
*  L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In _International Conference on Machine Learning_, pages 1568-1577. PMLR, 2018.
*  H. Gao. On the convergence of momentum-based algorithms for federated stochastic bilevel optimization problems. _arXiv preprint arXiv:2204.13299_, 2022.
*  S. Ghadimi and M. Wang. Approximation methods for bilevel programming. _arXiv preprint arXiv:1802.02246_, 2018.
*  S. Gould, B. Fernando, A. Cherian, P. Anderson, R. S. Cruz, and E. Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. _arXiv preprint arXiv:1607.05447_, 2016.
*  R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient computation. In _International Conference on Machine Learning_, pages 3748-3758. PMLR, 2020.
*  A. Griewank and A. Walther. _Evaluating derivatives: principles and techniques of algorithmic differentiation_. SIAM, 2008.
*  Z. Guo, Q. Hu, L. Zhang, and T. Yang. Randomized stochastic variance-reduced methods for multi-task stochastic bilevel optimization. _arXiv preprint arXiv:2105.02266_, 2021.
*  P. Hansen, B. Jaumard, and G. Savard. New branch-and-bound rules for linear bilevel programming. _SIAM Journal on scientific and Statistical Computing_, 13(5):1194-1217, 1992.
*  M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. _arXiv preprint arXiv:2007.05170_, 2020.
*  F. Huang. Fast adaptive federated bilevel optimization. _arXiv preprint arXiv:2211.01122_, 2022.
*  F. Huang and H. Huang. Biadam: Fast adaptive bilevel optimization methods. _arXiv preprint arXiv:2106.11396_, 2021.
*  M. Huang, D. Zhang, and K. Ji. Achieving linear speedup in non-iid federated bilevel learning. _arXiv preprint arXiv:2302.05412_, 2023.
*  Y. Huang, Q. Lin, N. Street, and S. Baek. Federated learning on adaptively weighted nodes by bilevel optimization. _arXiv preprint arXiv:2207.10751_, 2022.
*  K. Ji, J. Yang, and Y. Liang. Bilevel optimization: Convergence analysis and enhanced design. In _International conference on machine learning_, pages 4882-4892. PMLR, 2021.

*  K. Ji and L. Ying. Network utility maximization with unknown utility functions: A distributed, data-driven bilevel optimization approach. _arXiv preprint arXiv:2301.01801_, 2023.
*  S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
*  A. Khaled, K. Mishchenko, and P. Richtarik. First analysis of local gd on heterogeneous data. _arXiv preprint arXiv:1909.04715_, 2019.
*  P. Khanduri, S. Zeng, M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. _Advances in neural information processing systems_, 34:30271-30283, 2021.
*  V. Konda and J. Tsitsiklis. Actor-critic algorithms. _Advances in neural information processing systems_, 12, 1999.
*  J. Konecny, B. McMahan, and D. Ramage. Federated optimization: Distributed optimization beyond the datacenter. _arXiv preprint arXiv:1511.03575_, 2015.
*  Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
*  J. Li, B. Gu, and H. Huang. Improved bilevel model: Fast and optimal algorithm with theoretical guarantee. _arXiv preprint arXiv:2009.00690_, 2020.
*  J. Li, F. Huang, and H. Huang. Local stochastic bilevel optimization with momentum-based variance reduction. _arXiv preprint arXiv:2205.01608_, 2022.
*  J. Li, F. Huang, and H. Huang. Communication-efficient federated bilevel optimization with local and global lower level problems. _arXiv preprint arXiv:2302.06701_, 2023.
*  T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine learning and systems_, 2:429-450, 2020.
*  R. Liao, Y. Xiong, E. Fetaya, L. Zhang, K. Yoon, X. Pitkow, R. Urtasun, and R. Zemel. Reviving and improving recurrent back-propagation. In _International Conference on Machine Learning_, pages 3082-3091. PMLR, 2018.
*  B. Liu, M. Ye, S. Wright, P. Stone, and Q. Liu. Bome! bilevel optimization made easy: A simple first-order approach. _Advances in Neural Information Processing Systems_, 35:17248-17262, 2022.
*  R. Liu, J. Gao, J. Zhang, D. Meng, and Z. Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(12):10045-10067, 2021.
*  R. Liu, X. Liu, X. Yuan, S. Zeng, and J. Zhang. A value-function-based interior-point method for non-convex bi-level optimization. In _International Conference on Machine Learning_, pages 6882-6892. PMLR, 2021.
*  R. Liu, Y. Liu, S. Zeng, and J. Zhang. Towards gradient-based bilevel optimization with non-convex followers and beyond. _Advances in Neural Information Processing Systems_, 34:8662-8675, 2021.
*  R. Liu, P. Mu, X. Yuan, S. Zeng, and J. Zhang. A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton. In _International Conference on Machine Learning_, pages 6305-6315. PMLR, 2020.
*  S. Lu, X. Cui, M. S. Squillante, B. Kingsbury, and L. Horesh. Decentralized bilevel optimization for personalized client learning. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 5543-5547. IEEE, 2022.

*  D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through reversible learning. In _International conference on machine learning_, pages 2113-2122. PMLR, 2015.
*  P. Manoharan, R. Walia, C. Iwendi, T. A. Ahanger, S. Suganthi, M. Kamruzzaman, S. Bourouis, W. Alhakami, and M. Hamdi. Svm-based generative adverserial networks for federated learning and edge computing attack model and outpoising. _Expert Systems_, page e13072, 2022.
*  B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
*  A. Mitra, R. Jaafar, G. J. Pappas, and H. Hassani. Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients. _Advances in Neural Information Processing Systems_, 34:14606-14619, 2021.
*  M. Mohri, G. Sivek, and A. T. Suresh. Agnostic federated learning. In _International Conference on Machine Learning_, pages 4615-4625. PMLR, 2019.
*  R. Pathak and M. J. Wainwright. Fedsplit: An algorithmic framework for fast federated optimization. _Advances in neural information processing systems_, 33:7057-7066, 2020.
*  F. Pedregosa. Hyperparameter optimization with approximate gradient. In _International conference on machine learning_, pages 737-746. PMLR, 2016.
*  A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine. Meta-learning with implicit gradients. _Advances in neural information processing systems_, 32, 2019.
*  S. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Konecny, S. Kumar, and H. B. McMahan. Adaptive federated optimization. _arXiv preprint arXiv:2003.00295_, 2020.
*  Y. Roh, K. Lee, S. E. Whang, and C. Suh. Fairbatch: Batch selection for model fairness. In _International Conference on Learning Representations_, 2021.
*  S. Sabach and S. Shtern. A first order method for solving convex bilevel optimization problems. _SIAM Journal on Optimization_, 27(2):640-660, 2017.
*  A. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel optimization. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1723-1732. PMLR, 2019.
*  P. Sharma, R. Panda, and G. Joshi. Federated minimax optimization with client heterogeneity. _arXiv preprint arXiv:2302.04249_, 2023.
*  H. Shen and T. Chen. On penalty-based bilevel gradient descent method. _arXiv preprint arXiv:2302.05185_, 2023.
*  C. Shi, J. Lu, and G. Zhang. An extended kuhn-tucker approach for linear bilevel programming. _Applied Mathematics and Computation_, 162(1):51-63, 2005.
*  R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In _Proceedings of the 22nd ACM SIGSAC conference on computer and communications security_, pages 1310-1321, 2015.
*  A. Sinha, P. Malo, and K. Deb. A review on bilevel optimization: From classical to evolutionary approaches and applications. _IEEE Transactions on Evolutionary Computation_, 22(2):276-295, 2017.
*  D. Sow, K. Ji, Z. Guan, and Y. Liang. A constrained optimization approach to bilevel optimization with multiple inner minima. _arXiv preprint arXiv:2203.01123_, 2022.
*  S. U. Stich. Local sgd converges fast and communicates little. _arXiv preprint arXiv:1805.09767_, 2018.

*  S. U. Stich and S. P. Karimireddy. The error-feedback framework: Better rates for sgd with delayed gradients and compressed updates. _The Journal of Machine Learning Research_, 21(1):9613-9648, 2020.
*  D. A. Tarzanagh, M. Li, C. Thrampoulidis, and S. Oymak. Fednest: Federated bilevel, minimax, and compositional optimization. In _International Conference on Machine Learning_, pages 21146-21179. PMLR, 2022.
*  J. Wang and G. Joshi. Cooperative sgd: A unified framework for the design and analysis of local-update sgd algorithms. _The Journal of Machine Learning Research_, 22(1):9709-9758, 2021.
*  J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. _Advances in neural information processing systems_, 33:7611-7623, 2020.
*  Z. Wang, K. Balasubramanian, S. Ma, and M. Razaviyayn. Zeroth-order algorithms for nonconvex minimax problems with improved complexities. _arXiv preprint arXiv:2001.07819_, 2020.
*  P. Xiao and K. Ji. Communication-efficient federated hypergradient computation via aggregated iterative differentiation. _arXiv preprint arXiv:2302.04969_, 2023.
*  E. P. Xing, Q. Ho, P. Xie, and D. Wei. Strategies and principles of distributed machine learning on big data. _Engineering_, 2(2):179-195, 2016.
*  P. Xing, S. Lu, L. Wu, and H. Yu. Big-fed: Bilevel optimization enhanced graph-aided federated learning. _IEEE Transactions on Big Data_, 2022.
*  J. Yang, K. Ji, and Y. Liang. Provably faster algorithms for bilevel optimization. _Advances in Neural Information Processing Systems_, 34:13670-13682, 2021.
*  S. Yang, X. Zhang, and M. Wang. Decentralized gossip-based stochastic bilevel optimization over communication networks. _arXiv preprint arXiv:2206.10870_, 2022.
*  F. Yousefian. Bilevel distributed optimization in directed networks. In _2021 American Control Conference (ACC)_, pages 2230-2235. IEEE, 2021.
*  Y. Zeng, H. Chen, and K. Lee. Improving fairness via federated learning. _arXiv preprint arXiv:2110.15545_, 2021.
*  Y. Zhang, G. Zhang, P. Khanduri, M. Hong, S. Chang, and S. Liu. Revisiting and advancing fast adversarial training through the lens of bi-level optimization. In _International Conference on Machine Learning_, pages 26693-26712. PMLR, 2022.