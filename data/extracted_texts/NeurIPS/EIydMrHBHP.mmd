# PTADisc: A Cross-Course Dataset Supporting Personalized Learning in Cold-Start Scenarios

Liya Hu

Zhiang Dong

Zhejiang University

Jingyuan Chen

Guifeng Wang

Zhejiang Wang

Shanghai University

Zhou Zhao

Zhejiang University

###### Abstract

The focus of our work is on diagnostic tasks in personalized learning, such as cognitive diagnosis and knowledge tracing. The goal of these tasks is to assess students' latent proficiency on knowledge concepts through analyzing their historical learning records. However, existing research has been limited to single-course scenarios; cross-course studies have not been explored due to a lack of dataset. We address this issue by constructing **PTADisc**, a **D**iverse, **I**mmense, **S**tudent-centered dataset that emphasizes its sufficient **C**ross-course information for personalized learning. PTADisc includes \(74\) courses, \(1,530,100\) students, \(4,054\) concepts, \(225,615\) problems, and over \(680\) million student response logs. Based on PTADisc, we developed a model-agnostic **C**ross-**C**ourse **L**earner **M**odeling **F**ramework (**CCLMF**) which utilizes relationships between students' proficiency across courses to alleviate the difficulty of diagnosing student knowledge state in cold-start scenarios. CCLMF uses a meta network to generate personalized mapping functions between courses. The experimental results on PTADisc verify the effectiveness of CCLMF with an average improvement of \(4.2\)% on AUC. We also report the performance of baseline models for cognitive diagnosis and knowledge tracing over PTADisc, demonstrating that our dataset supports a wide scope of research in personalized learning. Additionally, PTADisc contains valuable programming logs and student-group information that are worth exploring in the future.

+
Footnote †: \(*\) Corresponding author.

## 1 Introduction

Personalized learning aims to provide students with customized learning services that align with their specific goals and abilities, which is facilitated by the vast amount of data accumulated on thriving online learning platforms. The focus of personalized learning is on diagnosing students' knowledge state and involves two key research tasks: (1) Cognitive Diagnosis (CD) [23; 5; 24; 3; 27; 8; 13], which assesses students' static latent proficiency on concepts using their learning records, and (2) Knowledge Tracing (KT) [21; 32; 20; 9; 30; 29; 16; 25; 15], which evaluates students' dynamic latent proficiency during various study phases based on their past sequential learning records. Based on the diagnostic results of students' knowledge states, several applications such as Computerized Adaptive Testing [1; 14] and Personalized Educational Planning  can be conducted, as illustrated in Figure 1(a).

To support the above research, several educational datasets are constructed [7; 12; 4; 31; 33; 22; 28]. However, existing datasets still face the following challenges: (1) Insufficient Data Coverage: Manyexisting datasets are limited in their application scope due to being built for specific tasks. For instance, KDD Cup 2010  lacks exercise content text, making it unsuitable for supporting content-based knowledge tracing. (2) Lack of Concept Annotation: Existing datasets reveal two problems including incomplete relation annotations between concepts and problems, and missing dependency relations among concepts, as seen in Table 5. (3) Poor Cross-course Information: Most existing datasets are constructed within a single course (_e.g._, ASSIST  in _Math_ and EdNet  in _English_). Although MOOCCubeX  consists of \(4,216\) courses, there exist no students with learning records in multiple courses. Cross-course studies are not supported by existing datasets.

In our work, we first address the above challenges by constructing a Diverse, Immense, Student-centered and Cross-course dataset licensed by Programming **T**eaching **A**ssistant (**PTA1**) platform, namely **PTADisc**. Specifically, PTADisc features in: (1) **D**iverse. As shown in Figure 1(c), it contains various information, comprising four key entities and relationships among them. The entities include course, student, problem, and concept. The relationship among entities is reflected in student behavior, concept-problem correlation and concept dependency. (2) Immense. It includes \(74\) courses, \(1,530,100\) students, \(4,504\) concepts, \(225,615\) problems, and over \(680\) million student exercising response logs (_i.e._, answer correctly or not). (3) **S**tudent-centered. The entire dataset is organized around student behaviors (_i.e._, response logs), providing valuable insight into personalized learning. (4) **C**ross-course. A subdataset with \(29,454\) students simultaneously taking \(5\) courses is extracted. This makes PTADisc **the first dataset to support cross-course analysis**.

Furthermore, we proposed a model-agnostic **C**ross-**C**ourse **L**earner **M**odeling **F**ramework (**CCLMF**) based on the cross-course subdataset of PTADisc. As shown in Figure 1(b), when a student enrolls in a new course and has few response logs, it's difficult to predict the student's latent proficiency on concepts and future performance. To address this cold-start problem, CCLMF leverages student latent proficiency relationships between courses to transfer knowledge from courses with sufficient response logs, thereby improving performance in the target course. The experimental results demonstrate the advancement of CCLMF in cold-start scenarios with an average improvement of \(4.2\)% on AUC. Our code and datasets are available at https://github.com/wahr0411/PTADisc.git.

The contributions of this paper include: (1) Construct PTADisc, a diverse, immense, student-centered and cross-course dataset, supporting various studies in personalized learning. (2) Construct a cross-course subdataset with a significant amount of students enrolled in multiple courses, making up for the lack of existing datasets that cannot support cross-course analysis. (3) Propose CCLMF, a model-agnostic Cross-Course Learner Modeling Framework which can improve the performance of diagnostic tasks in cold-start scenarios.

Figure 1: (a) Illustration of the personalized learning process. (b) Illustration of cold-start problem in personalized learning. (c) Overview of PTADisc.

Problem Definition

This study focuses on two tasks: cognitive diagnosis (CD) and knowledge tracing (KT). CD aims to analyze students' latent proficiency on concepts, assuming that they are in a stable learning state. Alternatively, KT focuses on dynamically assessing the knowledge proficiency of students as they progress through the learning process. Figure 2 visually represents the differences.

### Cognitive Diagnosis

Suppose there is a course with a total of \(N\) students, \(M\) problems, and \(K\) knowledge concepts, which can be denoted as \(=\{s_{1},s_{2},,s_{N}\}\), \(=\{p_{1},p_{2},,p_{M}\}\) and \(=\{c_{1},c_{2},,c_{K}\}\) respectively. The response logs of student \(s\) are denoted by \(_{s}\), a set of tuple \((p,r)\), with \(p\) and \(r\) indicating the score obtained by student \(s\) on problem \(p\). In addition, we have \(=\{Q_{ij}\}_{M K}\), with \(Q_{ij}=1\) if problem \(p_{i}\) is related to knowledge concept \(c_{j}\), and \(Q_{ij}=0\) in all other cases. The goal of CD is to assess student's level of proficiency on different knowledge concepts through the prediction process of student performance, given students' response logs \(\) and the Q-matrix \(\).

### Knowledge Tracing

The presentations of problems and knowledge concepts in KT are the same as in CD. For each student \(s\), their response logs are represented by \(_{s}=\{_{j}\}_{j=1}^{l}\), where \(_{j}\) denotes the \(j\)-th response log and \(l\) is the total number of logs. Each \(\) is denoted by a triplet \((p,r,t)\) where \(p\), \(r\) is the score and \(t\) is the timestamp of the student's response. The goal of KT is to predict students' proficiency levels of concepts at different study phases, given their response logs \(\) and the Q-matrix \(\).

## 3 Dataset

PTADisc is sourced from PTA, an online learning platform developed by Hangzhou PAT Education Technology Co., Ltd. PTA is an automatic program evaluation and open teaching assistance platform for universities and society. Given the close collaboration between PTA and universities, it is common for students to concurrently pursue a series of courses that align with their training program. Up to July 2023, PTA has attracted over \(1,000\) organizations, \(9,000\) teachers and \(3,900,000\) users and provides a problem bank of over \(290,000\) problems referenced by course problem sets and exams. The highlight of PTA is that it covers a significant amount of students enrolled in multiple courses. This feature perfectly meets the need to conduct cross-course research and mine student characteristics between courses.

### Privacy Protection

To prevent privacy disclosure, we have excluded personal and sensitive data such as student names and email addresses, retaining only the unique student IDs as individual identifiers, with anonymization employed. Additionally, we confirmed that the user-generated data was strictly authorized during the registration process, as specified in the _terms of service_ and _privacy statement_ of the PTA platform.

### Dataset Construction

#### 3.2.1 Raw Data Processing

First, we manually selected \(74\) courses. Then as shown in Figure 3, we applied privacy protection, data curation, data filtering

Figure 3: Construction pipeline of PTADisc.

Figure 2: Illustration of CD (a) and KT (b).

and data extraction to generate intermediate structured data. To eliminate redundancy, we created a unified problem bank that captured each problem's full score, content, and relevant concepts across all courses. Additionally, we filtered out problems without concept annotations or those with full scores of \(0\). For non-programming data, we selected only those students who had \(5\) or more response logs for each course. For programming data, we performed extraction to include relevant details such as codes submitted, programming language, as well as time and memory consumption.

#### 3.2.2 Structured Data

The data is organized by course, and we provide examples of course, problem, and response logs in Tables 1, 2 and 3, respectively. These structured data include the following content.

**Knowledge Concepts.** Knowledge concepts, also known as knowledge skills, are structured into a tree format that illustrates the hierarchical dependencies among concepts, as depicted in Figure 1(c). This hierarchical structure is manually annotated using the textbook catalog, and each relation is denoted as a tuples \((c_{i},c_{j})\) where \(c_{j}\) is the sub-concept of \(c_{i}\). Each problem in the dataset is associated with one or more leaf concepts within the tree, while the course name is the root node.

**Problems.** On PTA platform, registered teachers can create and publish problems containing content, title, problem difficulty, problem type, full score, and other specialized configurations. In general, PTADisc contains two broad categories of problems, which are non-programming and programming problems. All the problems are stored in a problem bank and assigned a unique problem ID.

**Student Groups.** Student groups can be regarded as classes. Each student group is given a student group ID and contains multiple students taking the same course.

**Problem Sets.** Problem sets are published by teachers as homework or quiz. All the problems in problem sets are selected from the problem bank. Once a problem is selected into a problem set, it is given a problem_set_problem ID (psp ID) and a specified full score. Each problem set has an opening and closing time, and students are only allowed to complete the problem set during this period.

**Student Behaviors.** Response logs are provided to represent student behaviors, including submission time, problem type, test score, psp ID, and judge status. It is important to note that PTADisc provides a variety of judge-related information for the programming problems, such as code, language, running time and memory consumption, which are well worth exploring in the future.

    &  &  &  &  &  &  &  \\  & & & Name & Parent ID & & Group ID & \\   &  & C\_1568 & Loop & C\_9088 & P\_3122 & G\_2144 & S\_e28d \\  & & C\_9472 & Function & C\_9088 & P\_3120 & G\_2144 & S\_369e \\  & & C\_2592 & Break & C\_1568 & P\_2600 & G\_5952 & S\_1c3a \\  & & C\_7488 & Continue & C\_1568 & P\_3143 & G\_5952 & S\_6758 \\   

Table 1: An example of a course in PTADisc.

    &  &  &  Reference Count \\ Count \\  } &  &  &  Pedimen Set \\ Problem ID \\  } &  &  &  &  \\  & & & & & & & & 2018/67 & 2018/76 \\  &  &  &  &  &  &  &  &  &  \\  & & & & & & & 010/8 & 23.59 \\    & & & & & & & 2018/73 & 2018/78 \\    & & & & & & & 11.50 & 23.59 \\   & C\_5696, &  &  &  &  &  &  &  &  /24 \\  & C\_6400 & & & & & & & 2021/326 & 2021/331 \\    & & & & & & & 16.00 & 23.59 \\   

Table 2: Examples of two problems in PTADisc.

    &  &  Student ID \\ ID \\  } &  SDUnit \\ Time \\  } &  SDUnit \\  } &  SDUnit \\  } &  Problem Set \\ Problem ID \\  } &  &  &  \\  True or false & Sub\_4736 & S\_9059 & 2018/12/28 & 0 & PSP\_4731 & NO\_ANSWER &  &  & \\  Multiple choice & Sub\_1520 & S\_9059 & 2018/12/28 & 2 & PSP\_4750 & ACCEPTED &  &  &  \\   & Sub\_5088 & S\_be716 & 2018/68 & 75 & PSP\_6944 &  PARTIAL\_ \\ ACCEPTED \\  &  \# _Code_ \\  & 
 time: 0.025, \\ memory: 3260416, \\ result: ACCEPTED, \\  \\   

Table 3: Examples of three response logs in PTADisc.

#### 3.2.3 Task-specific Datasets

**Cognitive Diagnosis Dataset.** Following the generation of structured data, we performed index mapping to create the CD datasets, which consist of a Q-matrix and students' response logs. The Q-matrix stores the relationships between problems and concepts. The response logs of each student were ratio of 70%/10%/20%, respectively.

**Knowledge Tracing Dataset.** The process for generating the KT dataset is similar to the CD dataset, except that each log in the KT dataset includes an additional submitting time.

**Cross-course Dataset.** We analyzed students taking multiple courses and identified the top five frequent course sets. Furthermore, we eliminated students with less than \(10\) response logs for any of the five courses and were left with a total of \(29,454\) students for the study, as presented in Table 4.

### Dataset Statistics

PTADisc includes \(74\) courses, \(1,530,100\) students, \(4,504\) concepts, \(225,615\) problems, and over \(680\) million student response logs. The response logs were partitioned according to problem type into programming and non-programming logs. Figure 4(b) presents the distribution of response logs and enrolled students for each course. Additional detailed statistics can be found in Appendix B.

### Dataset Characteristics

The comparison of PTADisc with other open-access educational datasets is shown in Table 5. We divide these datasets into three categories: (1) student-centered datasets including ASSIST093, Junyi4, KDD Cup 2010 , EdNet , which focus on student behaviors and are frequently used for diagnostic tasks in personalized learning; (2) knowledge-centered datasets including MOOCCubeX ; and (3) programming datasets including BePKT  and CodeNet . We illustrate the characteristics of PTADisc in four aspects.

**Diverse.** As illustrated in Table 5, PTADisc contains extensive concept-related information and detailed records of student behaviors. PTADisc offers coverage of concept dependencies which are not found in ASSIST, EdNet and CodeNet. Moreover, PTADisc is the only dataset that evaluates student responses using a scoring ratio system rather than binary values . Besides, PTADisc includes student group information, enabling group-level analysis.

**Immense.** As Table 5 illustrates, PTADisc is currently the largest educational dataset, featuring diverse data scales across multiple courses. This range of data scales presents researchers with a multitude of options for personalized learning studies.

  Course Name & Java & C++ & Python & DS2 & C \\  \#Students & 29,454 & 29,454 & 29,454 & 29,454 & 29,454 \\ \#Problems & 16,752 & 15,172 & 17,787 & 21,952 & 26,056 \\ \#Response Logs & 4,750,970 & 6,587,356 & 6,454,336 & 7,789,280 & 11,378,017 \\ \#Concepts & 773 & 547 & 685 & 767 & 847 \\  

Table 4: Cross-course datasets in PTADisc.

Figure 4: PTADisc statistics. (a) Correlation coefficients of student performance between five courses. (b) The distribution of response logs and enrolled students.

**Student-centered.** PTADisc is a student-centered dataset focusing on student behaviors. By validating problems and knowledge concepts through analysis of student response logs, the dataset improves consistency and maintainability, making it well-suited for diagnostic tasks.

**Cross-course.** PTADisc is the first dataset with cross-course information. For the cross-course subset in PTADisc, we further analyzed the correlation coefficient between these courses based on the performance of each student. As shown in Figure 4 (a), there is a positive correlation among the five courses, providing a statistical basis for cross-course learner modeling.

### Dataset Applications

PTADisc provides support for CD and KT tasks, as mentioned in Section 2. Section 5 presents the CD and KT baseline experimental results on several existing methods. Additionally, PTADisc can also support the following tasks in the field of AI for education: (1) Prerequisite discovery , which identifies the sequence in which concepts or topics should be learned, ensuring foundational concepts are understood before advanced ones. (2) Computerized adaptive testing , which aims to rapidly and accurately diagnose a candidate's level of knowledge mastery through personalized test items. (3) Educational recommendation , which provides appropriate learning suggestions to students based on their interactions with problems. (4) Cross-course research. PTADisc provides cross-course information, enabling the study of how students perform in different classes. We conducted a cross-course study addressing the cold-start issue in Section 4.

With various information provided, PTADisc also has the potential to support the following research directions: (1) It provides non-binary performance data. Most existing CD and KT methods tackle this as binary classification (wrong/right answer). Non-binary grades allow for regression-focused investigations. (2) It provides information on problem types, aiding research of CD and KT. (3) It provides information on problem difficulty, enabling to study how the level of difficulty of exercises relates to students' final learning outcomes. (4) It provides problem set specifics, like submission time, enabling modeling of students' learning habits based on when they submit their work. (5) It provides data on student groups, facilitating the assessment of teaching quality within classes and group-level educational analysis. (6) It provides information on programming exercises, including detailed code submissions and records, enabling in-depth research into programming-related studies.

## 4 CCLMF: Cross-course Learner Modeling Framework

By utilizing the cross-course subdataset of PTADisc, we can leverage the relationships between courses to mitigate the challenges associated with the cold-start problem. When a student begins a new course (_i.e._, target course) and has limited data, it can be challenging to diagnose his proficiency level through CD or KT. To address this problem, we present the **C**ross-**C**ourse **L**earner **M**odeling **F**ramework (**CCLMF**) inspired by cross-domain recommendation [18; 34], which utilizes auxiliary information from another course (_i.e._, source course) taken by the student and has enough data. By identifying connections between the student's proficiency in the target and source courses, CCLMF enhances the model's performance in low-data scenarios. CCLMF incorporates a meta-learner to predict network parameters, leveraging the power of meta-learning to improve performance in the target course by harnessing the knowledge acquired from source courses. Please note that CCLMF

   Field & Dataset Name & PTADisc & ASSIST & Juniy & KDD & EdNet & MOOCubeX & **B**pRT & **CodeNet** \\   & **Number of Logs** & **\#.816k** & 4.02e/e & 2.59e/ & 2.27e6 & 1.31k & 2.96k & 6.79e4 & 1.39e7 \\  & **Cross-course Info** & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & Concepts Annotates & ✓ & ✓ & ✓ & ✓ & ✓ & incomplete1  & ✓ & \(\) \\  & Concepts Dependencies & ✓ & \(\) & ✓ & \(\) & \(\) & ✓ & x & \(\) \\   & Programming Problems & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) & ✓ & ✓ \\  & Problem Type & ✓ & ✓ & ✓ & \(\) & \(\) & ✓ & \(\) & \(\) \\   & Detailed Programming Logs & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) & ✓ & ✓ \\  & Non-binary Response Results & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & Problem Set & ✓ & ✓ & \(\) & ✓ & ✓ & ✓ & \(\) & \(\) \\  & Student Group & ✓ & ✓ & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 5: Comparison between PTADisc and existing open datasets.

is a model-agnostic framework that can be applied to various CD or KT models. To explain it in a simpler way, we will illustrate CCLMF in the context of cognitive diagnosis.

### Problem Definition

We assumed that \(N\) students have enrolled in both the source and target courses, denoted as \(\). The source course has \(M^{s}\) problems and \(K^{s}\) knowledge concepts, denoted as \(^{s}\) and \(^{s}\). And the target course has \(M^{t}\) problems and \(K^{t}\) knowledge concepts, denoted as \(^{t}\) and \(^{t}\). The response logs in the source and target course are denoted as \(^{s}\) and \(^{t}\). For both courses, the Q-matrices are denoted as \(^{s}=\{Q^{s}_{ij}\}_{M^{s} K^{s}}\) and \(^{t}=\{Q^{t}_{ij}\}_{M^{t} K^{t}}\). The goal of the CCLMF is to accurately measure students' level of proficiency on various knowledge concepts in the target course, which incorporates student response logs and Q-matrices from both the target and source courses.

### CCLMF Architecture

We illustrate the architecture of CCLMF in Figure 5. CCLMF mainly consists of two stages: a pre-training stage and a meta stage. In the pre-training stage, a cognitive diagnosis model (CDM) such as NCD is trained using data from the source course, yielding a representation \(^{s}_{i}\) for student \(s_{i}\). In the meta stage, instead of directly mapping \(^{s}_{i}\) to student representation of the target course \(^{t}_{i}\), we applied the idea of meta-learning to learn a personalized cross-course transformation function \(F_{}\) for each student. Moreover, this meta learning procedure is task-oriented, with \(F_{}\) being trained by minimizing Equation (4) for specific tasks using data in the target course. After completing the meta stage, a personalized transformation function is obtained for each student. Thus, given any student \(s_{j}\) who is new to the target course, their proficiency representation can be determined by their transformation function as \(F_{_{j}}(^{s}_{j};_{_{j}})\).

Figure 5: CCLMF architecture consists of two stages. (a) In the **pre-training stage**, we train a CDM in the source course and get student \(i\)’s proficiency representation \(^{s}_{i}\). (b) In the **meta stage**, we utilize the meta network to generate the mapping function \(F_{_{i}}\) for each student \(i\), and the proficiency representation in the target course is obtained by \(^{t}_{i}=F_{_{i}}(^{s}_{i};_{_{i}})\).

**Pre-training stage.** CCLMF aims to leverage valuable insights gleaned from the vast data in the source course. In the pre-training stage, a CDM is trained in the source course, generating an informative student representation \(^{s}\). Specifically, our CCLMF can be employed on CDMs which characterize student proficiency as:

\[^{s}_{i}=H_{}(s_{i};_{h}),\] (1)

where \(_{h}\) denotes the parameters of \(H_{}\), and \(H_{}\) is a function abstracted from CDM which can calculate each student's latent proficiency representation \(^{s}_{i}\). The dimensionality of vector \(^{s}_{i}\) is \(d^{s}\) which is usually related to the number of knowledge concepts and the number of problems. The generated \(^{s}\) contains personalized and auxiliary information that can be utilized in the meta stage.

**Meta stage.** Due to individual differences, the relationships between student proficiency in the source and target courses can vary significantly from one student to another. Therefore we need to create personalized mapping functions for each student in order to retain students' individual characteristics. In the meta stage, we utilized a meta network to learn personalized mapping functions for each student. The meta network \(G()\) is formulated as:

\[_{_{i}}=G(^{s}_{i};_{g}),\] (2)

where \(_{g}\) is the parameters of \(G()\) and \(_{_{i}}\) is used as the parameters of the mapping function. The personalized mapping function \(F_{}(;_{})\) then produces personalized transformed student's representation in the target course as:

\[^{t}_{i}=F_{_{i}}(^{s}_{i};_{_{i}}).\] (3)

Instead of mapping-oriented optimization used by Man et al., we directly used the performance of diagnostic tasks as our optimization goal to train the meta network. This task-oriented training procedure advances in making full use of the ground truth values rather than approximate intermediate results. Therefore, the meta network \(G()\) as well as the mapping function \(F_{}(;_{})\) are trained together using data in the target course.

Given the ground truth value \(r\) from \(^{t}\) and the CC-CDM's final output \(_{i}\) which is generated based on \(^{t}_{i}\), the task-oriented loss can be formulated as:

\[loss_{}=-_{i}(r_{i}_{i}+(1-r_{i})_{i})}).\] (4)

**Inference.** The goal of CCLMF is to accurately measure students' level of proficiency in the target course. During inference, for any student \(s_{j}\) who is new to the target course, their level of proficiency can be determined by their personalized transformation function \(^{t}_{j}=F_{_{j}}(^{s}_{j};_{_{j}})\).

The whole procedure of CCLMF is summarized in Algorithm 1. Detailed implementation of CCLMF on NCD  can be found in Appendix C.

### Experiment Settings

We conducted CCLMF on a traditional cognitive diagnosis model MIRT  and a deep-learning-based model NCD , called CC-MIRT and CC-NCD respectively.

   Metrics & Model & no dropout & 10\% dropout & 20\% dropout & 30\% dropout & 40\% dropout & 50\% dropout \\   & MIRT & 0.6379 & 0.6412 & 0.6398 & 0.6399 & 0.6342 & 0.6363 \\  & CC-MIRT & 0.7059 (+0.0680) & 0.7025 (+0.0612) & 0.6998 (+0.0600) & 0.6998 (+0.0599) & 0.6957 (+0.0615) & 0.6918 (+0.0555) \\   & MIRT & 0.6832 & 0.7037 & 0.6869 & 0.6886 & 0.6889 & 0.6991 \\  & CC-MIRT & **0.7854 (+0.1022)** & 0.7834 (+0.0797) & 0.7826 (+0.0958) & 0.7833 (+0.0947) & 0.7822 (+0.0933) & 0.7797 (+0.0806) \\   & MIRT & 0.4092 & 0.4821 & 0.4884 & 0.4916 & 0.5013 & 0.4949 \\  & CC-MIRT & 0.3948 (-0.0954) & 0.3955 (+0.0885) & 0.3973 (+0.0911) & 0.3973 (+0.0943) & 0.3983 (+0.1029) & 0.3999 (+0.0950) \\   & NCD & 0.6885 & 0.6846 & 0.6797 & 0.6787 & 0.6736 & 0.6675 \\  & CC-NCD & 0.7106 (+0.0221) & 0.7061 (+0.0215) & 0.7028 (+0.0231) & 0.7007 (+0.0221) & 0.6953 (+0.0216) & 0.6895 (+0.0221) \\   & NCD & 0.7613 & 0.7662 & 0.7682 & 0.7606 & 0.7653 & 0.7640 \\  & CC-NCD & 0.7859 (+0.0246) & 0.7834 (+0.0172) & 0.7819 (+0.0137) & 0.7814 (+0.0208) & 0.7812 (+0.0177) & 0.7817 (+0.0177) \\   & NCD & 0.4109 & 0.4095 & 0.4081 & 0.4127 & 0.4121 & 0.4133 \\  & CC-NCD & 0.3966 (+0.0143) & 0.3973 (+0.0122) & 0.3981 (+0.0100) & 0.3991 (+0.0136) & 0.3998 (+0.0123) & 0.4009 (-0.0124) \\   

Table 6: CCLMF results on MIRT and NCD.

**Datasets.** We constructed the cross-course datasets based on the datasets shown in Table 4. As depicted in Figure 4(a), the correlation coefficient between students' performance in _Python Programming_ (Python) and _Java Programming_ (Java) is \(0.65\), indicating a relatively high correlation. This high correlation makes these two courses well-suited for addressing cold-start problems. _Python Programming_ was chosen as the source course. To simulate cold-start scenarios, \(332,568\) response logs (\(7\)% of each student's response logs) from _Java Programming_ were selected to form the target course. We reserved \(20\)% of response logs in the target course as the test set and then conducted experiments on sub-datasets with varying degrees of sparsity. Specifically, we randomly dropped \(10\)%, \(20\)%, \(30\)%, \(40\)%, and \(50\)% of the remaining data and split the resulting data into \(7\)/\(1\) as train/valid set. To reduce the influence of randomness, we repeated the dropout process \(10\) times for each dropout ratio, then reported the average results across the \(10\) sets of data.

**Settings and Metrics.** The meta network was implemented as a two-layer perceptron with input-size \(K^{s}\), output-size \(K^{s} K^{t}\) and hidden-size \(100\), where each linear layer was followed by a RELU activation function. The weights were initialized by Xavier . The batch size and learning rate are \(128\) and \(1 10^{-3}\). All experiments were executed on a Linux server with two GeForce RTX 3090s. The evaluation metrics include Area Under the ROC Curve (AUC) , Prediction Accuracy (ACC) and Root Mean Square Error (RMSE).

### Experimental Results and Analysis

Table 6 shows the superiority of CCLMF over baseline models. CCLMF has better performance on both NCD and MIRT models with different dropout ratios on all metrics. Specifically, CCLMF achieves an average improvement of \(4.2\)% on AUC, \(5.5\)% on Accuracy, and \(5.3\)% on RMSE.

To investigate the performance of our model over students of different sparsity levels, we show the performance with respect to the number of response logs a student has in Figure 6. Note that we did not re-train the model with different sets of students, instead we divided the test set into different groups by the number of logs per student. We observe that the performance improvement of CCLMF is more significant for students with fewer response logs, highlighting the advantage of our model in cold-start scenarios.

## 5 Experiments on CD and KT Tasks

To demonstrate PTDisc's support for CD and KT, we specifically selected four courses with varying response log scales, namely _Probability and Statistics_ (Probability), _Linux System_ (Linux), _Database Technology and Application_ (DB), and _Computational Thinking_ (Comp), as seen in Table 7, which were also marked in Figure 4(b) with vertical dashes.

### Baseline Models

For CD, we considered the following baseline methods: three traditional methods **DINA**[5; 26], **IRT**, and **MIRT**, deep-learning method **NCD** and graph based method **RCD**. For

  Course Name & Probability & Linux & DB & Comp \\  \#Students & 557 & 4,398 & 12,646 & 45,329 \\ \#Problems & 1,054 & 2,678 & 3,616 & 8,399 \\ \#Concepts & 247 & 284 & 325 & 477 \\ \#Response Logs & 46,106 & 391,434 & 2,363,206 & 6,504,414 \\ Logs per Student & 82.78 & 89.00 & 186.87 & 143.49 \\   

Table 7: Datasets for CD and KT.

Figure 6: AUC on NCD and CC-NCD w.r.t. student groups with varying degrees of sparsity.

KT, we considered the following baseline methods: deep sequential method **DKT **, memory augmented method **DKVMN **, attention-based methods **AKT ** and **SAKT **, graph based methods **GIKT ** and **SGKT **, and pre-training based method **PEBG **. Due to the intensive computing resources required by RCD, we did not conduct RCD on two courses with relatively large-scale data, which are DB and Comp. Additional information of these baseline models can be found in Appendix D.

### Experiment Settings and Metrics

For each baseline model, we employed the same parameter settings and optimization methods as described in their respective papers to ensure fairness in the comparison. The evaluation metrics used for CD included AUC, ACC, and RMSE, while for KT, the evaluation metrics were AUC and ACC, which are consistent with commonly used metrics in the literature.

### Experimental Results and Analysis

The results of the CD and KT tasks have yielded notable findings, as shown in Table 8. Firstly, we observed that the dataset size has a significant influence on model performance, with higher prediction accuracy observed for larger datasets. Secondly, we noted in Table 8 that the MIRT model performance was generally higher than that of the NCD model; this contrasts with the findings in Table 6. We attribute this discrepancy to the cross-course dataset's lower average number of student response logs (_i.e._ approximately \(10\) logs per student), compared to the larger dataset with over \(80\) logs per student, as shown in Table 7. These results suggest that the number of logs per student significantly influences model performance, and that the NCD model may outperform the traditional MIRT model specifically in cases where there are fewer student logs on average. Thirdly, PEBG achieved the best AUC performance across all datasets in the KT task, which demonstrates the effectiveness of employing pre-training techniques to capture the heterogeneous educational data.

## 6 Conclusion

**Outlook**: We describe concrete ongoing and future work based on PTADisc. Specifically,

* We will conduct research about adaptive learning and personalized educational planning, and incorporate them into a personalized learning system alongside the CCLMF model as shown in Figure 1(a). We will also analyze the group-level student learning behaviors.
* We will explore programming knowledge tracing based on PTADisc which contains a large amount of multi-round programming problem submission records and rich evaluation information.

**Limitation**: The original content text of the problems cannot be shared currently due to copyright constraints. We intend to disclose the content by extracting text features in the future.

**Conclusion**: PTADisc is a diverse, immense, student-centered and cross-course dataset that enables researchers to conduct previously infeasible cross-course studies. Based on PTADisc, we developed CCLMF to alleviate the difficulty of diagnosing student knowledge states in the cold-start scenario. Furthermore, we demonstrate the broad range of applications of our dataset by reporting on the performance of baseline models for CD and KT tasks over PTADisc.

   Dataset Name & Metric & DNA & IRT & MIRT & NCD & RCD & DKT & DKVMN & SAKT & AKT & GIKT & SGKT & PEBG \\   & AUC & 0.6569 & 0.7257 & 0.7324 & 0.7092 & **0.7485** & 0.6876 & 0.6815 & 0.6879 & 0.7123 & 0.7086 & 0.7079 & **0.7320** \\  & ACC & 0.6121 & 0.7071 & 0.7112 & 0.6894 & **0.7196** & **0.7025** & 0.6961 & 0.6721 & 0.6978 & 0.6838 & 0.6978 \\  & RMSE & 0.5141 & 0.4490 & 0.4730 & 0.4590 & **0.4346** & - & - & - & - & - & - \\   & AUC & 0.7577 & 0.8199 & 0.8168 & 0.8171 & **0.8318** & 0.7898 & 0.7856 & 0.7756 & 0.8074 & 0.8173 & 0.8156 & **0.8379** \\  & ACC & 0.7053 & 0.7802 & 0.7799 & 0.7755 & **0.7860** & 0.7726 & 0.7713 & 0.7665 & 0.7849 & 0.7791 & 0.7801 & **0.8040** \\  & RMSE & 0.4493 & 0.3900 & 0.4023 & 0.3934 & **0.3841** & - & - & - & - & - & - \\   & AUC & 0.7141 & 0.7901 & **0.8212** & 0.7901 & - & 0.7813 & 0.7635 & 0.7562 & 0.7956 & 0.8101 & 0.7996 & **0.8381** \\  & ACC & 0.7856 & 0.8322 & **0.8424** & 0.8299 & - & 0.8312 & 0.8281 & 0.8181 & **0.8383** & 0.8353 & 0.8299 & 0.8373 \\  & RMSE & 0.3934 & 0.3493 & **0.3407** & 0.3504 & - & - & - & - & - & - & - \\   & AUC & 0.7137 & 0.7819 & **0.8096** & 0.7734 & - & 0.7978 & 0.7811 & 0.7717 & 0.8091 & 0.8172 & **0.8281** \\  & ACC & 0.7303 & 0.7929 & **0.8018** & 0.7880 & - & 0.8276 & 0.8234 & 0.8012 & 0.8283 & **0.8337** & 0.8274 & 0.8194 \\   & RMSE & 0.4335 & 0.3808 & **0.3749** & 0.3849 & - & - & - & - & - & - & - \\   

Table 8: Baselines of existing CD and KT models on PTADisc.