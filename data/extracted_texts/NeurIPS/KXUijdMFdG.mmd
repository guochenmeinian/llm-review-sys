# Deep Homomorphism Networks

Takanori Maehara

Roku, Inc.

Cambridge, UK

tmaehara@roku.com &Hoang NT

University of Tokyo

Tokyo, Japan

hoangnt@g.ecc.u-tokyo.ac.jp

Authors are listed in alphabetical order.

###### Abstract

Many real-world graphs are large and have some characteristic subgraph patterns, such as triangles in social networks, cliques in web graphs, and cycles in molecular networks. Detecting such subgraph patterns is important in many applications; therefore, establishing graph neural networks (GNNs) that can detect such patterns and run fast on large graphs is demanding. In this study, we propose a new GNN layer, named _graph homomorphism layer_. It enumerates local subgraph patterns that match the predefined set of patterns \(^{}\), applies non-linear transformations to node features, and aggregates them along with the patterns. By stacking these layers, we obtain a deep GNN model called _deep homomorphism network (DHN)_. The expressive power of the DHN is completely characterised by the set of patterns generated from \(^{}\) by graph-theoretic operations; hence, it serves as a useful theoretical tool to analyse the expressive power of many GNN models. Furthermore, the model runs in the same time complexity as the graph homomorphisms, which is fast in many real-word graphs. Thus, it serves as a practical and lightweight model that solves difficult problems using domain knowledge.

## 1 Introduction

### Background

_Graph neural network (GNN)_ is a type of neural network that takes a graph as input. It has been applied to many problems in various domains, such as influence prediction in social networks , page ranking in web graphs , and chemical prediction in biological networks . See textbooks  for the basics of GNN.

The expressive power of GNNs is the central research topic in GNN . A recent interest in this topic is the detectability of subgraph patterns. Many graphs that appear in practice have typical subgraph patterns. For example, social networks have many triangles, which indicates the clustering structure of the society. Web graphs have many cliques that represent clusters of websites, such as link farms. Molecular networks have benzene structures. Since detecting these subgraph patterns is a common strategy in network science  and graph data mining , we expect that GNNs applied in these fields equip expressive power to detect such patterns. Furthermore, since the graphs in these applications are typically large, we also expect that the GNNs applied in these fields run fast.

Unfortunately, most of the existing GNN models do not meet these expectations. The commonly used GNNs, called _message-passing GNNs (MPGNNs)_, do not meet the expectation of expressive power, as they can only detect tree-shaped patterns . More complex GNNs can detect subgraph patterns, but typically do not meet either expectation: _Higher-order GNNs_ assign values to \(k\)-tuples of nodes instead of nodes . They have the same expressive power as the \(k\)-dimensionalWeisfeiler-Lehman (\(k\)-WL) test1, which detects subgraphs of treewidth at most \(k\); however, their complexity is typically \((n^{k})\), which is not applicable to large graphs. _Subgraph GNNs_ take a small subgraph for each node and apply a GNN to compute an embedding . Its expressive power depends on the choice of the subgraph selection policy and the base GNN, and the standard choice of the policy and the base GNN, it is strictly more expressive than the \(1\)-WL test but less expressive than the \(2\)-WL or \(3\)-WL tests [77; 25], which is often insufficient to capture the patterns of interest.

One promising direction is _explicit pattern detection_, which explicitly scans the patterns in the graph and uses that information. This approach has been studied and applied in practice for a long time before the GNN era [15; 52; 69; 26], and recent studies combine them with GNN [47; 56; 4; 9; 78; 49; 57]. This approach requires domain knowledge (or "subgraph feature engineering") of what patterns will be important, but often provides a more effective and efficient solution.

Amongst multiple notions of pattern enumeration, here we focus on _graph homomorphisms_, which is the adjacency-preserving mappings from a pattern to the target graph (see Section 2.2 for the definition). We focus on the following two theoretical GNN studies based on graph homomorphisms. The first is by NT and Maehara , who extended the homomorphism number to graphs with features and proposed using them as features of downstream models such as support vector machines. The limitation of this approach is that it is inefficient in achieving a higher expressive power -- Their approach specifies a set of patterns \(\) and computes the generalised homomorphism number for each \(P\). This detects all \(P\) (finite number of patterns) using \((||)\) time. On the other hand, MPGNNs such as GIN detect all \(T\), where \(\) is the set of trees (infinite set of patterns) without incurring a time complexity of \((||)\). The second is by Barcelo et al. , who proposed to add the precomputed rooted homomorphism numbers from the specified patterns \(^{}\) as node features of the graph and to apply MPGNN. The important finding is that such a simple approach boosts that the model detects all \(F^{}\) where where \(^{}\) is a set of graphs obtained by attaching a pattern \(P^{}\) to nodes of a tree (called \(^{}\)-trees) while keeping the time complexity of \(O(|^{}|)\) instead of \(O(|^{}|)\). This approach cannot capture the features of the patterns, which are important in many GNN applications, and the patterns it captures are limited.

Our goal is to establish a connection with the GNN architecture and homomorphisms by extending this line of studies. We observe that, according to the proof of , the method of  is inefficient in achieving a higher expressive power because it is not "deep" (see Remark 5.4). Therefore, our strategy to achieve more expressive GNNs is to deeply stack homomorphism-based layers.

### Our Contribution

In this study, we propose _generalised rooted graph homomorphism number_, which applies a non-linear transform to node features and then aggregates them along with graph homomorphisms (Section 3). We then propose _graph homomorphism layer_ that computes the generalised rooted homomorphism numbers with learnable non-linear transforms from a set of patterns \(^{}\) specified as a hyperparameter. We refer to a GNN that stacks this layer _deep homomorphism network_ (DHN or \(^{}\)-DHN to clarify the patterns). See Figure 1 for an illustration and Section 4 for the details of our model.

By construction, our layer is trained and evaluated in the same time complexity as the generalised rooted homomorphism numbers. Computing the homomorphism number is W-hard in general ; however, in many practical cases, such as bounded degree graphs and bounded degeneracy graphs, we can obtain faster algorithms by using the technique in graph homomorphisms (Section 4.2).

The expressive power of the model is analysed using a methodology similar to that in . Let \(^{}}\) be a set of graphs obtained by iteratively attaching \(P^{}^{}\) to the singleton (e.g., the set of trees is obtained from an edge by this construction). Then, we can show that the expressive power of \(^{}\)-DHN is characterised by the \(^{}\)-homomorphism distinguishability (Theorem 5.2). This characterisation is useful for establishing the expressive power hierarchy of the GNN models. In particular, we can discuss its relationship with \(k\)-GNN and subgraph GNNs using the underlying homomorphism patterns. Another important consequence of this theorem is that it reveals the advantage of stacking multiple GNN layers. Simply put, adding one layer corresponds to adding base patterns to each node in the current set of patterns. Hence, GNN can detect exponentially many patterns and linearly deeper patterns with respect to the number of GNN layers (Remark 5.4 in Section 5.1).

Essentially, \(^{}\)-DHN is a "deep" version of NT and Maehara 's method. Barcelo et al. 's method is a DHN that uses the \(^{}\)-homomorphism layer for the first layer and the MPGNN layer for the subsequent layers. This generalisation elucidates the relationship between the GNN architecture and the corresponding homomorphisms, thereby facilitating a better understanding of the expressive power hierarchy among different GNN architectures (Section 5.3).

The DHN model takes advantage of pattern enumeration and deep learning. Hence, we expect the model to solve difficult graph problems that require the capture of subgraph patterns at reasonable computational costs. We conducted experiments and observed that the DHN solved difficult benchmark problems (CSL, EXP, and SR25) with fewer parameters than the existing models. For real-world datasets, the proposed model showed promising results, but was still not competitive to the state-of-the-art models that involve a lot of engineering (see Section 6 for discussion).

## 2 Preliminaries

### Graphs

A _graph_\(G=(V(G),E(G))\) is a pair of _nodes_\(V(G)\) and _edges_\(E(G)\). We denote by \(e=(u,v)\) an _edge_ between \(u\) and \(v\)2 and \(N(u)=\{v:(u,v) E(G)\}\) the neighbours of \(u\). An _isomorphism from_\(G_{1}\)_to_\(G_{2}\) is a bijection \( V(G_{1}) V(G_{2})\) such that \((u_{1},v_{1}) E(G_{1})\) if and only if \(((u_{1}),(v_{1})) E(G_{2})\). Two graphs \(G_{1}\) and \(G_{2}\) are _isomorphic_ if there exists an isomorphism.

We fix a compact set \(^{d_{}}\) for the feature space. A _graph with features_ is a pair \((G,x)\) of a graph \(G\) and a collection \([x_{u}:u V(G)]^{V(G)}\) of node features. Two graphs with features, \((G_{1},x_{1})\) and \((G_{2},x_{2})\), are _isomorphic_ if there is an isomorphism \(\) from \(G_{1}\) to \(G_{2}\) such that \(x_{1,u_{1}}=x_{2,(u_{1})}\) for all \(u_{1} V(G_{1})\).

In this study, we mainly consider the node classification as it is a building block of all other GNN applications. We employ _rooted graph formulation_. A _rooted graph_\(G^{r}\) is a graph \(G=(V(G),E(G))\) with a distinguished node \(r V(G)\). We denote by \(G^{}\) if there is no need to specify the name of the root node. Two rooted graphs \(G_{1}^{r_{1}}\) and \(G_{2}^{r_{2}}\) are _isomorphic_ if there is an isomorphism \(\) from \(G_{1}\) to \(G_{2}\) such that \(r_{2}=(r_{1})\). The isomorphism of rooted graphs with features is defined similarly. A function \(f\) that takes a rooted graph with features \((G,x)\) and produces some quantity is said to be _equivariant_ if \(f((G_{1}^{},x_{1}))=f((G_{2}^{},x_{2}))\) if \((G_{1}^{},x_{1})\) and \((G_{2}^{},x_{2})\) are isomorphic. This study only considers equivariant functions because it is a natural and desirable property for the task (otherwise, the output depends on a synthetic ordering of nodes). Note that if we drop the equivariance, it is easy to construct arbitrary expressive models .

_Remark 2.1_ (Advantage of Rooted Graph Formulation).: Many existing studies formulate a node classification function as a function that takes a graph \(G\) as input and produces \(^{V(G) d}\) matrix as an output. Therefore, mathematically, its codomain is the disjoint union \(_{G}^{V(G) d}\) where \(\) is

Figure 1: Example Deep Homomorphism Network (DHN) built from two \(^{}\)-homomorphism layers: \(C_{3}^{}\) and \(C_{2}^{}\). By stacking different homomorphism patterns, DHN can detect new patterns without explicit specifications. This figure demonstrates that the “spoon” pattern can be detected by stacking \(C_{3}\) and \(C_{2}\) homomorphism layers.

the set of all graphs. Existing studies mitigated such a complex codomain by assuming that all \(G\) share the same node set, \(V(G)=\{1,,n\}\), but this creates a limitation on the number of nodes. The rooted graph formulation has no such issue as the domain is the set of rooted graphs and the codomain is \(^{d}\). Note that the statement for rooted graphs is easily converted to non-rooted graphs.

### Graph Homomorphism

A _graph homomorphism_ from a graph \(F\) to a graph \(G\) is a mapping \( V(F) V(G)\) such that \((i,j) E(F)\) implies \(((i),(j)) E(G)\); we refer to \(F\) as _pattern graph_ and \(G\) as _host graph_. As each homomorphism defines a subgraph of \(G\) as a homomorphism image \((F) G\), we can recognise that a homomorphism represents a \(F\)-pattern in \(G\).

We denote by \((F,G)\) the set of graph homomorphisms from \(F\) to \(G\) and \((F,G)\) by its cardinality, called _graph homomorphism number_. If we know \((F,G)\) for multiple \(F\), we can obtain a lot of information on the structure of \(G\). For example, \((C,G_{1})=(C,G_{2})\) for all cycles \(C\) means that \(G_{1}\) and \(G_{2}\) are cospectral and \((F,G_{1})=(F,G_{2})\) for all graphs \(F\) means that \(G_{1}\) and \(G_{2}\) are isomorphic . See Hell and Nesetril  for the basics of graph homomorphisms.

For rooted graphs \(F^{r}\) and \(G^{s}\), a _rooted graph homomorphism_3 is a homomorphism from \(F\) to \(G\) that maps \(r\) to \(s\). We denote by \((F^{r},G^{s})\) the set of rooted homomorphisms from \(F^{s}\) to \(G^{s}\).

### Weisfeiler-Lehman Test

The _(one-dimensional) Weisfeiler-Lehman test (WL test or \(1\)-WL test)_ is a procedure to identify whether given two graphs (with features) are non-isomorphic or potentially isomorphic . The WL-test calculates the "colour \({c_{u}}^{}\)" of nodes \(u\) using the following recursive procedure:

\[c_{u}^{(0)}=x_{u}, c_{u}^{(k+1)}=(c_{u}^{(k)},\{\!\!\{c_ {v}^{(k)}:v N(u)\}\!\!\}),\] (1)

where \(\{\!\!\{\!\{\!\{\!\{\!\}\}\}\!\! \}\!\!\}\!\!\}\) denotes the multiset. Here, each colour is a nested tuple of vectors and multisets; a practical implementation applies a hash function to them, but they are equivalent in theory. Let \(c^{(k)}(G)=\{\!\!\{c_{u}^{(k)}:u V(G)\}\!\!\}\!\!\}\) be the multiset of colours in the \(k\)-th step. If \(c^{(k)}(G_{1}) c^{(k)}(G_{2})\) for some \(k\), then \(G_{1}\) and \(G_{2}\) are not isomorphic. Dvorak  proved that two graphs \(G_{1}\) and \(G_{2}\) are indistinguishable by the WL-test if and only if \((T,G_{1})=(T,G_{2})\) for all trees \(T\).

### Graph Neural Networks

_Graph neural network (GNN)_ is a neural network that takes a graph as input. The most commonly used GNN is a _message-passing GNN_ (MPGNN), which computes the node values by

\[h_{u}^{(0)}=^{(0)}(x_{u}), h_{u}^{(k+1)}=^{(k+1)}(h_{u}^{(k )},^{(k)}(\{\!\!\{h_{v}^{(k)}:v N(u)\}\!\!\} )),\] (2)

where \(^{(k)}\) is a learnable function and \(^{(k)}\) is a (learnable) multi-set function, i.e., a permutation-invariant function for the arguments, for each \(k\). It is easy to see that the MPGNN defines equivariant functions. A typical implementation of MPGNN is _graph isomorphism network (GIN)_, which uses the summation for \(^{(k)}\).

Due to the similarity between the WL test (1) and the MPGNN (2), it can be proved that the expressive power of the MPGNN is identical to the WL test . As the WL-indistinguishability is equivalent to the homomorphism-indistinguishability from all trees, as mentioned above, we can conclude that MPGNN can only detect tree-shaped patterns.

## 3 Generalised Homomorphism Numbers for Rooted Graphs with Features

A _pattern graph with transformations_ is a pair \((F^{},)\) of a rooted graph \(F^{}\) and a collection of continuous functions \(=\{_{p}:p V(F^{})\}\) defined on the nodes of \(F^{}\), where each \(_{p}\) maps their inputs to \(^{d}\). The _generalised rooted homomorphism number_\(((F^{},),(G^{},x))\) from a pattern graph with transformations \((F^{},)\) to a rooted graph with features \((G^{},x)\) is then defined by

\[((F^{},),(G^{},x)):=_{(F^{},G^{ })}_{p V(F^{})}_{p}(x_{(p)}),\] (3)

where the product in the right-hand side is the element-wise product. Note that NT and Maehara 's generalised homomorphism is our special case, which uses the same transformation to all nodes.

A generalised homomorphism number maps a graph with features to a real vector (not necessarily a number). By definition, two isomorphic graphs with features have the same generalised homomorphism numbers for any pattern graph with transformations. Here, the converse also holds.

**Theorem 3.1**.: _Let \((G^{}_{1},x_{1})\) and \((G^{}_{2},x_{2})\) be rooted graphs with features. \((G^{}_{1},x_{1})\) and \((G^{}_{2},x_{2})\) are isomorphic if and only if \(((F^{},),(G^{}_{1},x_{1}))=((F^{},),(G^{ }_{2},x_{2}))\) for any pattern graphs with transformations \((F^{},)\)._

Proof Sketch.: We use the Lovasz theorem that any finite relational structure is determined from the number of homomorphisms . First, we recognise graphs with features as a relational structure consists of the adjacency relation and feature value relation. Then, we show that the number of homomorphisms as the relational structure (i.e., the number of mappings that preserve the edges and feature values) is computed using our generalised homomorphism by suitably choosing \(\). 

Let \(^{}\) be a set of pattern graphs with transformations. We say that two rooted graphs with features, \((G^{}_{1},x_{1})\) and \((G^{}_{2},x_{2})\), are \(^{}\)_-homomorphism indistinguishable_ if \(((F^{},),(G^{}_{1},x_{1}))=((F^{},),(G^{ }_{2},x_{2}))\) for all \((F^{},)^{}\); Theorem 3.1 states that \(^{}\)-homomorphism indistinguishability coincides with the isomorphism if \(^{}\) is the set of all pattern graphs with transformations. In general, homomorphism indistinguishability forms an equivalence relation.

_Remark 3.2_.: In graph homomorphism literature, _weighted homomorphism number_ is studied more frequently. It is essentially a generalised homomorphism number with linear transformations, and it cannot distinguish some non-isomorphic graphs [12; 70]. However, as shown in the above, our generalised homomorphism mitigates this issue by introducing the non-linearity of \(\).

## 4 Deep Homomorphism Networks

### Definition

Let \(^{}\) be a set of rooted graphs. A _graph homomorphism layer with respect to \(^{}\)_ is a GNN layer defined using the generalised homomorphism number as follows:

\[_{^{}}((G^{u},x);,\{_{^ {}}:P^{}^{}\})=(((P^{}, _{P^{}}),(G^{u},x)):P^{}^{}),\] (4)

where \((G^{u},x)\) is the input rooted graph with features, and \(\) and \(_{P^{},p}\) for all \(P^{}^{}\) and \(p V(P^{})\) are neural networks. We often omit neural network parameters and write it as \(_{^{}}((G^{u},x))\). The input dimensionality of \(\) is the sum of the output dimensionalities of \(_{P^{},u}\), and the input dimensionality of \(_{P^{},u}\) is the dimensionality of the input \(h\). The layer defines an equivariant function since the graph homomorphism numbers are equivariant functions.

_Deep homomorphism network (DHN)_ is a neural network obtained by "deeply" stacking the graph homomorphism layers as follows:

\[h^{(0)}=x, h^{(k+1)}=_{^{(k)}}((G^{ u},h^{(k)})).\] (5)

We denote by \((^{(1)},^{(2)},)\)-DHN if we want to emphasize the pattern sets, and we denote \(^{}\)-DHN for \((^{},^{},)\)-DHN. By definition, a DHN is an equivariant function. The number of parameters in DHN is proportional to the number of nodes in the pattern graphs.

**Example 4.1** (DHN generalises MPGNN).: Let \(^{}=\{,-\}\) be the patterns consisting of single-node and single-edge graphs. Here, we see that the \(^{}\)-DHN is a MPGNN.

We first consider the single-node graph \(\). There is the unique homomorphism from \(\) to \(G^{u}\) given by \(()=u\); hence,

\[((,\{_{,}\}),(G^{u},x))=_{,}(x_{u}).\] (6)We then consider the single-edge graph \(-\). As the set of homomorphisms from \(-\) to \(G^{u}\) corresponds to the set of edges incident to \(u\), we have

\[((-,\{_{-,},_{-,}\}), (G^{u},x))=_{v N(u)}_{-,}(x_{u})_{- ,}(x_{v}).\] (7)

By setting \(_{,}(x)=x\), \(_{-,}(x)=1\), and \(_{-,}(x)=x\) for some \(_{}\), we obtain the MPGNN:

\[_{^{}}((G^{u},x))=(x_{u},_{v N(u )}x_{v}).\] (8)

DHN generalises several existing models. We review such results in Sections 5.2.

### Computational Complexity

Evaluating a graph homomorphism layer with respect to \(^{}\) on \((G,x)\) takes the same time complexity as evaluating \(((P^{},),(G^{u},x))\) for all \(P^{}^{}\) and \(u V(G)\); therefore, its computational complexity is at least that of \((P,G)\) for some \(P^{}^{}\). We cannot expect a linear-time algorithm to evaluate this quantity without any assumption because computing \((P,G)\) is a \(W\)-hard problem parameterised by \(|V(P)|\). However, there are several cases that admit efficient algorithms for computing \((P,G)\). We see that these results can be generalised to our generalised homomorphism numbers as follows.

Case 1: \(P\) has a bounded treewidthTreewidth is a parameter that represents how far the graph is from being a tree; see  about treewidth. If \(P\) has a bounded treewidth, we can compute \((P,G)\) in \(O(n^{(P)+1})\) time using the dynamic programming algorithm . The algorithm is easily extended to generalised rooted graph homomorphism numbers; see Section A.1. Hence, we can evaluate the graph homomorphism number in polynomial time in this situation.

Case 2: \(G\) has a bounded degreeIn some examples, such as molecular networks, the host graph \(G\) has a small maximum degree. In this case, we can enumerate \((P^{},G^{u})\) in constant time by brute-force enumeration. Therefore, we can evaluate the graph homomorphism layer in linear time.

Case 3: \(G\) has a bounded degeneracy and \(P\) has a bounded DAG-treewidthA graph \(G\) has degeneracy at most \(k\) if there is an ordering of nodes \(u_{1},,u_{n}\) such that \(|\{j:u_{j} N(u_{i}),j i\}| k\) for all \(i=1,,n\). Many real-world graphs have small degeneracy . Hence, it is practically important to have algorithms that run fast on graphs of bounded degeneracy. Bressan  introduced _DAG-treewidth_, and proposed an algorithm for computing the homomorphisms number in \(O(n^{(P)})\) time using the dynamic programming algorithm. An important special case is that \(P\) has no induced cycles of length greater than five. In this case, the DAG treewidth is one  (the converse is also true); hence, we can evaluate the homomorphism numbers in linear time. To clarify the procedure, we put a linear-time algorithm for the quadrangle \(C_{4}\); see Section A.2 in Appendix.

## 5 Theoretical Analysis of \(^{}\)-DHN Model

### Expressive Power of \(^{}\)-DHN Model

In Example 4.1, we observed that a DHN with simple patterns \(\{,-\}\) contains a MPGNN. Here, we focus to the phenomenon that, although it aggregates local information along with such simple patterns, it has a great expressive power specified as the 1-WL test , which distinguishes all \(^{}\) homomorphism-distinguishable graphs, where \(^{}\) is the set of all trees . The goal of this section is to generalise this relation to arbitrary patterns.

Let \(F^{}\) and \(P^{r}\) be rooted graphs. The _rooted product_ of \(F^{}\) and \(P^{r}\) at \(p V(F^{})\) is the rooted graph obtained by attaching \(r\) at \(p\), i.e., \(F^{}*_{p}P^{r}:=F^{} P^{r}/\{p,r\}\); see Figure 2 for an example. Let \(^{}\) and \(^{}\) be sets of rooted graphs. We denote by \(^{}*^{}=\{F^{}*_{u}P^{}:u  V(F^{})\}\) the set of all rooted products. We denote by \(}=_{l=0,1,}(^{})^{ l}\) the set of all graphs obtained by the iterated rooted products, where \((^{})^{*0}=\{\}\) and \((^{})^{ l}=^{}*^{}\) (\(l\) times). We can easily verify the following example.

**Example 5.1**.: \(}\) is the set of all rooted trees \(^{}\).

Now Example 4.1 and Example 5.1 lead to the conjecture that the expressive power of \(^{}\)-DHN is characterised by the iterated rooted product \(^{}}\) of the pattern graph. We prove this as follows, which is the main theorem in this paper.

**Theorem 5.2**.: _Let \(^{}\) be a set of rooted graphs. For any two rooted graphs with features \((G_{1}^{},x_{1})\) and \((G_{2}^{},x_{2})\), the following are equivalent._

1. _For any_ \(^{}\)_-DHN_ \(h\)_, we have_ \(h(G_{1}^{},x_{1})=h(G_{2}^{},x_{2})\)_._
2. \((G_{1}^{},x_{1})\) _and_ \((G_{2}^{},x_{2})\) _are_ \(^{}}\)_-homomorphism indistinguishable._

The key lemma to prove this theorem is the following lemma, which decomposes the homomorphism from rooted product into the homomorphisms from the factors.

**Lemma 5.3** (Chain Rule).: _Let \(F^{}\) be a rooted graph obtained by taking the rooted product of \(P^{}\) and \(F_{p}^{}\) at node \(p\) for each \(p V(P^{})\). Then, for any \(\), there exists \(_{p}\) such that_

\[((F^{},),(G^{},x))=_{(P^{},G^{ })}_{p V(P)}((F_{p}^{},_{p}),(G^{(p)},x)).\] (9)

_for any rooted graph with features \((G^{},x)\)._

Proof Sketch of Theorem 5.2.: Instead of proving the equivalence between 1 and 2, we introduce a variant of WL-test, named \(^{}\)-_WL test_, and introduce the third statement: \((G_{1}^{},x_{1})\) and \((G_{2}^{},x_{2})\) are \(^{}\)-WL indistinguishable, and prove the equivalence of 1, 2, and 3. Here, \(3 2\) is clear from the definition of the \(^{}\)-WL test, which is similar to that of . \(1 2\) is straightforward by seeing that a generalised homomorphism from any \(F^{}^{}}\) is expressed by a DHN. To prove \(2 3\), we prove that the colour assigned by \(^{}\)-WL test is uniquely identified by evaluating suitably-chosen pattern graphs \(F^{}^{}}\) with transformations. This part is similar to  but we use the chain rule above and a basic results from multi-symmetric polynomials. 

_Remark 5.4_.: Establishing deeper GNN models is a central challenge in GNN community . Although deeper models do not necessarily perform well in _practice_, in _theory_, Theorem 5.2 and its proof clearly show the advantage of deeper GNNs in terms of the number of pattern graphs -- From the proof of Theorem 5.2, we see that \(l\)-layer DHN models can count homomorphisms from \(2^{O(l)}\) different patterns. In this sense, one could say that "the expressive power of a GNN grows exponentially in the number of layers."

### Relationship with Existing Models

In this section, we review the relationship between the proposed DHN and some existing models.

**Example 5.5** (DHN generalises NT and Maehara ).: Our first-motivated paper, NT and Maehara , proposed to compute (their version of) generalised homomorphism number and use it as a feature of downstream models for graph classification. By definition, our DHN can be seen as a multi-layer version of their approach.

**Example 5.6** (DHN generalises Barcelo et al. ).: Our second motivated paper, Barcelo et al. , proposed to append homomorphism numbers from arbitrary pattern \(^{}\) as node features. This can be seen as a DHN that uses an arbitrary pattern \(^{}\) in the first layer and the MPGNN pattern \(\{,-\}\) in the subsequent layers, i.e., it is the \((^{},\{,-\},\{,-\}, )\)-DHN. They showed that their model can detect graphs called \(^{}\)-patterns, which is obtained by attaching \(^{}\) to nodes of a tree. This follows from our theorem, as the \(^{}\)-patterns are exactly the graphs obtained by the rooted product to a tree and \(^{}\).

Figure 2: Rooted product of two graphs, the triangle and the edge, at \(p\).

**Example 5.7** (DHN generalises Paolino et al. ).: Recently, Paolino et al.  proposed a GNN that aggregates information over cycles. Their model is a DHN that uses the set of cycles \(^{}_{t}=\{C^{}_{1},,C^{}_{t}\}\) of lengths at most \(l\) as a pattern set, i.e., it is the \(^{}_{t}\)-DHN. They showed that their model can detect cactus graphs with a maximum cycle length of \(l\). This follows from our theorem since \(^{}_{t}}\) are the set of such cactus graphs.

**Example 5.8** (DHN generalises the most expressive subgraph GNNs).: For each layer, a subgraph GNN takes the \(l\)-hop neighbours and applies a GNN to compute the value of the root node . The most expressive GNN in this class uses the universal GNN on the subgraph. If the underlying graphs have a bounded degree, such a GNN is an instance of DHN -- Let \(^{}_{d,l}\) be the set of all rooted graphs of degree at most \(d\) and radius at most \(l\). Then, the \(^{}_{d,l}\) homomorphisms identify \(^{}_{d,l}\). Therefore, \(^{}_{d,l}\)-DHN has the same expressive power as the subgraph GNN with universal GNN if the underlying graphs have degree at most \(d\).

### Applications: Expressive Power Hierarchy

Theorem 5.2 is a powerful tool for comparing the expressive power of different GNN models. Let \(A\) and \(B\) be two GNN models. We say that \(A\) is _more expressive than \(B\)_ (denoted by \(A B\)) if \(h_{A}((G^{}_{1},x_{1}))=h_{A}((G^{}_{2},x_{2}))\) for all \(h_{A} A\) implies \(h_{B}((G^{}_{1},x_{1}))=h_{B}((G^{}_{2},x_{2}))\) for all \(h_{B} B\), and \(A\)_is strictly more expressive than \(B\)_ (denoted by \(A B\)) if the \(A B\) but \(B A\). To prove \(A B\), we typically show that \(A\) can implement \(B\), and find a pair of instances \((G^{}_{1},x_{1})\), \((G^{}_{2},x_{2})\), separating these classes. However, finding such a pair often requires nontrivial work.

Theorem 5.2 reduces the expressive power of \(^{}\)-DHN model to \(^{}}\)-homomorphism indistinguishability, and allows us to use lots of existing work established in graph theory literature . For example, after some preparation (Section D.6), we can easily prove the following hierarchy in a unified way. See also Figure D.6 in Appendix showing hierarchy of some models.

**Corollary 5.9**.: _Let \(^{}_{k}\) be a set of cycles of size at most \(k\) (where we identify the cycles of length one and two as a singleton and an edge, respectively), \(^{}_{k}\) be the set of cliques of size at most \(k\), and \(^{}_{k}\) be the set of connected graphs of size at most \(k\). Then, the following holds._

* \(^{}_{k}\)_-DHN model_ \(^{}_{k+1}\)_-DHN model,_ \(^{}_{k}\)_-DHN model_ \(^{}_{k+1}\)_-DHN model, and_ \(^{}_{k}\)_-DHN model_ \(^{}_{k+1}\)_-DHN model for all_ \(k 2\)_._
* \(^{}_{k}\)_-DHN model_ \(^{}_{k}\)_-DHN model for all_ \(k 4\)_, and_ \(^{}_{k}\)_-DHN model is incomparable with_ \(^{}_{k+1}\)_-DHN model for all_ \(k 3\)_._
* \(^{}_{k}\)_-DHN model_ \(^{}_{k}\)_-DHN model for all_ \(k 4\)_, and_ \(^{}_{k}\)_-DHN model is incomparable with_ \(^{}_{k+1}\)_-DHN model for_ \(k 3\)__
* \(^{}_{k}\)_-DHN model and_ \(^{}_{k}\)_-DHN model are incomparable for_ \(k 4\)_._

We can also prove the relations of expressive powers of existing architectures using our framework as follows. See Section B in the Appendix for a detailed discussion of the existing models.

**Corollary 5.10**.: _If \(\{,-\}^{}\) and \(^{}\) contains a graph with a cycle, then the \(^{}\)-DHN model is strictly more expressive than the MPGNN model._

**Corollary 5.11**.: _If the maximum treewidth of \(P^{}^{}\) is \(k\), then the \(^{}\)-DHN model is less expressive than the \(k\)-WL equivalent models such as \((k+1)\)-GNN and \((k+1)\)-IGN models._

**Corollary 5.12**.: _If the maximum chordless cycle length of \(P^{}^{}\) is finite, then \(^{}\)-DHN model is incomparable with \(2\)-WL equivalent models such as \(3\)-GNN and \(3\)-IGN models._

**Corollary 5.13**.: _The subgraph GNN model using the \(k\)-hop egograph selection policy and universal GNN as a base encoder is strictly more expressive than \(^{}_{k}\)-DHN model, and is incomparable with \(2\)-WL models such as \(3\)-GNN and \(3\)-IGN models._

_Remark 5.14_.: Recently,  provided homomorphism characterisation of GNN models based on \(k\)-WL-like tests for \(k 2\). As all of these models can capture arbitrary long cycles, they are not less expressive than any DHN model.

### Continuity and universality of \(^{}\)-Dhn

One of the desired properties of graph algorithms is the _continuity_. Graphs appear in network science applications are often very large and almost impossible to obtain the full structure. In a such case, we usually sample a smaller graph, conduct analysis, and expect the outcome approximates for the original graph . The continuity guarantees the validity of such a procedure so that the outcomes of the original graph and the sampled graph are close. Such property is referred to as the _size generalisability_ in GNN literature .

Different sampling procedure introduces different notion of continuity (i.e., topology) in the graph space . Here, we consider the _BFS sampling_, which randomly samples a node, performs \(k\)-hop breadth-first search (BFS), and select the subgraph induced by the nodes. The topology induced by the BFS sampling is called _Benjamini-Schramm topology_. We claim that \(^{}\)-DHN is continuous with respect to this topology.

Formally, we consider the set \(_{d}^{}\) of rooted graphs with features whose degrees are at most \(d\). Let \(_{d}^{}\) be the Cauchy completion of \(_{d}^{}\) with respect to the Benjamini-Schramm distance; see Appendix for the precise definition. Then, we can prove the following.

**Lemma 5.15**.: _For any finite \(^{}\), a \(^{}\)-DHN is a continuous function on \(_{d}^{}\) with respect to the Benjamini-Schramm topology._

This lemma has some applications. The first one is the universal approximation. We say that a function \(f\) is \(^{}\)_-homomorphism indistinguishable_ if \(f((G_{1}^{},x_{1}))=f((G_{2}^{},x_{2}))\) for any \(^{}\)-homomorphism indistinguishable \((G_{1}^{},x_{1})\) and \((G_{2}^{},x_{2})\). We can show that any \(^{}}\)-homomprhism indistinguishable function is arbitrary accurately approximated by the DHN model as follows, which guarantees the validity of using \(^{}\)-DHN model for tasks that \(^{}}\) substructure is relevant.

**Theorem 5.16**.: _For any integer \(d\) and a finite \(^{}\), the \(^{}\)-DHN model is dense in the set of all \(^{}}\)-indistinguishable continuous functions on \(_{d}^{}\)._

Another application is the comparison with existing GNN models as follows.

**Example 5.17** (DHN is incomparable with Zhang et al. ).: Recently, Zhang et al.  observed that many linear-time GNN models could not detect biconnectivity, and they proposed a new model that can detect biconnectivity. Their observation is true because the biconnectivity is not a continuous property in the Benjamini-Schramm topology, and most linear-time models, including DHN, are continuous in this topology. That is, for any continuous model, there are sufficiently close biconnected graph \(G_{1}\) and non-biconnected graph \(G_{2}\) such that the continuous model fails to detect their difference. Conversely, any model that can detect the biconnectivity must be non-continuous in the Benjamini-Schramm topology. Therefore, such models might not have size-generalisability, which is not suitable for large graph applications.

## 6 Experiments

Experimental SettingWe present the experimental results on the three most common synthetic benchmark datasets for GNN expressivity and two real-world graph classification datasets. The Circular Skip Links (CSL) dataset consists of 150 undirected regular graphs of degree four . EXP  and SR25  are datasets not distinguishable by 1-WL (EXP) and 3-WL (SR25). The ENZYMES  and PROTEINS  datasets represent the protein function prediction task formulated as the graph classification problem4 We set the same experimental setting as previous works , see the Appendix C for more details of these datasets. For our DHN, we use two sets of patterns as the building blocks. \(C_{i:j}=\{C_{i},,C_{j}\}\) denotes the sets of cycles of lengths \(i\) to \(j\). Similarly, \(K_{i:j}=\{K_{i},,K_{j}\}\) denotes the set of cliques of size \(i\) to \(j\). We use 3-layer MLPs for both \(\) and \(_{p}\) for the homomorphism layer (Eq. (4)). In Table 1, we present the models' configurations inside the single brackets. For example, DHN-\((C_{2}K_{3:5},C_{2}K_{3:5})\) means the model has two layers, and each layer consists of 4 kernels: \(C_{2},K_{3},K_{4}\), and \(K_{5}\). Note that \(K_{4}\) has the treewidth of four; hence, the DHN with \(K_{4}\) is incomparable with PPGN, I\({}^{2}\)-GNN, and N\({}^{2}\)-GNN.

ResultsOverall, we see that the performance of DHN depends on the choice of the pattern graphs. For a suitable choice (i.e., the last row), it can solve all the benchmark problems. CSL is easy andcan be solved with any model (except the MPNN, aka. GIN). EXP is not co-spectral; hence, we can detect the difference by using cycles; as shown in the table, using more cycles improves the performance. An important observation here is that stacking layer often boosts the expressive power of the DHN models -- the single-layer model DHN-\((C_{2:5})\) can only achieve 81% while adding one extra layer, DHN-\((C_{2:5},C_{2})\) achieves 99% accuracy. The same phenomenon is observed in other models except DHN-\((C_{2:4})\). SR25 is co-spectral; hence, adding cycles does not help solve the problem. Experimentally, we found that adding \(K_{3:5}\) solved the problem. Furthermore, stacking layers helped both in training convergence and achieving better results. In general, the DHN models have fewer parameters than the existing highly expressive GNNs because they are designed to capture a limited set of patterns, which leads to fast and low-memory training.5

We report the stratified 10-fold cross-validation accuracies for ENZYMES and PROTEINS datasets in Table 1. Our proposed models performed comparably to other much larger high-expressivity models on these real-world datasets. Although our results are still far from the reported state-of-the-art results (78% for ENZYMES and 84% for PROTEINS), we believe DHN has the potential to be improved beyond the theoretical context of this paper.

## 7 Conclusion

In this study, we developed a new GNN named deep homomorphism network (DHN). DHN is parameterised by a set of base patterns \(^{}\), which is typically specified by the domain knowledge and computational complexity. The expressive power of the model is completely characterised by the homomorphism numbers from any patterns generated from \(^{}\). Moreover, the model is evaluated efficiently in several cases, including the patterns having bounded treewidth, graphs having bounded degree, the patterns having bounded DAG-treewidth, and the graphs having bounded degeneracy.

LimitationThe DHN model is motivated by network science applications that involve large and sparse graphs. Therefore, it might not be suitable for other applications. More specifically, using DHN might not be competitive in the following situations: (1) when the graphs are small so that \(O(n^{k})\) time complexity of \(k\)-WL graph neural networks is acceptable. This is commonly seen in graph classification tasks. (2) when the graphs are dense so that pattern enumeration takes \((n^{k})\) time. Simple models such as MPGNN would be more suitable for such case.

Future WorkEssentially, our DHN is a "homomorphism extension" of the MPGNN model; therefore, it is fundamentally impossible to capture arbitrary long cycles. A promising future work is to establish the corresponding theory for the local \(k\)-GNN for \(k 2\), which allows us to capture arbitrary long cycles attached to small complex patterns which appear in biological networks. Such work will require combining our construction on top of the recently established homomorphism characterisation of local WLs .

    & \#params & CSL & EXP & SR25 & ENZYMES & PROTEINS \\  MPNN (4 layers)  & 27k & 0 & 0 & 0 & 54.6 \(\) 4.5 & 72.0 \(\) 4.0 \\ PPGN (4 layers)  & 96k & **100** & **100** & 0 & 58.2 \(\) 5.7 & 77.2 \(\) 3.7 \\ I\({}^{2}\)-GNN (4 layers)  & 143k & **100** & **100** & **100** & - & - \\ N\({}^{2}\)-GNN (4 layers)  & 355k & **100** & **100** & **100** & - & - \\  DHN–\((C_{2:4})\) & 5k & **100** & 50 & 0 & 64.3 \(\) 5.5 & 76.5 \(\) 3.0 \\ DHN–\((C_{2:5})\) & 7k & **100** & 81 & 0 & 63.7 \(\) 5.4 & 77.0 \(\) 3.2 \\ DHN–\((C_{2:10})\) & 27k & **100** & 98 & 0 & 58.0 \(\) 5.3 & 78.5 \(\) 2.5 \\ DHN–\((C_{2:3:5})\) & 7k & **100** & 50 & 53 & 63.3 \(\) 5.5 & 76.0 \(\) 2.7 \\  DHN–\((C_{2:4},C_{2})\) & 8k & **100** & 50 & 0 & 64.4 \(\) 5.9 & 77.1 \(\) 2.8 \\ DHN–\((C_{2:5},C_{2})\) & 11k & **100** & 99 & 0 & 62.0 \(\) 5.5 & 77.0 \(\) 2.5 \\ DHN–\((C_{2:5},C_{2:5})\) & 36k & **100** & 99 & 0 & 59.9 \(\) 5.2 & 76.7 \(\) 3.3 \\ DHN–\((C_{5:10},C_{2})\) & 27k & **100** & **100** & 0 & 63.5 \(\) 6.1 & 78.2 \(\) 3.3 \\ DHN–\((C_{2}K_{3:5},C_{2}K_{3:5})\) & 36k & **100** & **100** & **100** & 57.5 \(\) 6.6 & 77.4 \(\) 3.4 \\   

Table 1: Experimental results on synthetic and real-world datasets for GNN expressivity (Acc.%)