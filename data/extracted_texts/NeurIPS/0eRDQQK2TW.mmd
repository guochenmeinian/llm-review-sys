# A Finite-Particle Convergence Rate for

Stein Variational Gradient Descent

 Jiaxin Shi

Stanford University

Stanford, CA 94305

jiaxins@stanford.edu

&Lester Mackey

Microsoft Research New England

Cambridge, MA 02474

lmackey@microsoft.com

Part of this work was done at Microsoft Research New England.

###### Abstract

We provide the first finite-particle convergence rate for Stein variational gradient descent (SVGD), a popular algorithm for approximating a probability distribution with a collection of particles. Specifically, whenever the target distribution is sub-Gaussian with a Lipschitz score, SVGD with \(n\) particles and an appropriate step size sequence drives the kernel Stein discrepancy to zero at an order \(1/\) rate. We suspect that the dependence on \(n\) can be improved, and we hope that our explicit, non-asymptotic proof strategy will serve as a template for future refinements.

## 1 Introduction

Stein variational gradient descent [SVGD, 18] is an algorithm for approximating a target probability distribution \(P\) on \(^{d}\) with a collection of \(n\) particles. Given an initial particle approximation \(_{0}^{n}=_{i=1}^{n}_{x_{i}}\) with locations \(x_{i}^{d}\), SVGD (Algorithm 1) iteratively evolves the particle locations to provide a more faithful approximation of the target \(P\) by performing optimization in the space of probability measures. SVGD has demonstrated encouraging results for a wide variety of inferential tasks, including approximate inference , generative modeling , and reinforcement learning .

Despite the popularity of SVGD, relatively little is known about its approximation quality. A first analysis by Liu [17, Thm. 3.3] showed that _continuous SVGD_--that is, Algorithm 2 initialized with a continuous distribution \(_{0}^{}\) in place of the discrete particle approximation \(_{0}^{n}\)--converges to \(P\) in kernel Stein discrepancy [KSD, 4, 19, 9]. KSD convergence is also known to imply weak convergence  and Wasserstein convergence  under various conditions on the target \(P\) and the SVGD kernel \(k\). Follow-up work by Korba et al. , Salim et al. , Sun et al.  sharpened the result of Liu with path-independent constants, weaker smoothness conditions, and explicit rates of convergence. In addition, Duncan et al.  analyzed the continuous-time limit of continuous SVGD to provide conditions for exponential convergence. However, each of these analyses applies only to continuous SVGD and not to the finite-particle algorithm used in practice.

To bridge this gap, Liu [17, Thm. 3.2] showed that \(n\)-particle SVGD converges to continuous SVGD in bounded-Lipschitz distance but only under boundedness assumptions violated by most applications of SVGD. To provide a more broadly applicable proof of convergence, Gorham et al. [10, Thm. 7] showed that \(n\)-particle SVGD converges to continuous SVGD in \(1\)-Wasserstein distance under assumptions commonly satisfied in SVGD applications. However, both convergence results are asymptotic, providing neither explicit error bounds nor rates of convergence. Korba et al. [15, Prop. 7] explicitly bounded the expected squared Wasserstein distance between \(n\)-particle and continuous SVGD but only under the assumption of bounded \( p\), an assumption that rules out all stronglylog concave or dissipative distributions and all distributions for which the KSD is currently known to control weak convergence [9; 3; 12; 1]. In addition, Korba et al.  do not provide a unified bound for the convergence of \(n\)-particle SVGD to \(P\) and ultimately conclude that "the convergence rate for SVGD using [\(_{0}^{n}\)] remains an open problem." The same open problem was underscored in the later work of Salim et al. , who write "an important and difficult open problem in the analysis of SVGD is to characterize its complexity with a finite number of particles."

In this work, we derive the first unified convergence bound for finite-particle SVGD to its target. To achieve this, we first bound the \(1\)-Wasserstein discretization error between finite-particle and continuous SVGD under assumptions commonly satisfied in SVGD applications and compatible with KSD weak convergence control (see Theorem 1). We next bound KSD in terms of \(1\)-Wasserstein distance and SVGD moment growth to explicitly control KSD discretization error in Theorem 2. Finally, Theorem 3 combines our results with the established KSD analysis of continuous SVGD to arrive at an explicit KSD error bound for \(n\)-particle SVGD.

## 2 Notation and Assumptions

Throughout, we fix a nonnegative step size sequence \((_{s})_{s 0}\), a target distribution \(P\) in the set \(_{1}\) of probability measures on \(^{d}\) with integrable first moments, and a _reproducing kernel_\(k\)--a symmetric positive definite function mapping \(^{d}^{d}\) to \(\)--with reproducing kernel Hilbert space (RKHS) \(\) and product RKHS \(^{d}_{i=1}^{d}\). We will use the terms "kernel" and "reproducing kernel" interchangeably. For all \(,_{1}\), we let \((,)\) be the set of all _couplings_ of \(\) and \(\), i.e., joint probability distributions \(\) over \(_{1}_{1}\) with \(\) and \(\) as marginal distributions of the first and second variable respectively. We further let \(\) denote the independent coupling, the distribution of \((X,Z)\) when \(X\) and \(Z\) are drawn independently from \(\) and \(\) respectively. With this notation in place we define the \(1\)-Wasserstein distance between \(,_{1}\) as \(W_{1}(,)_{(,)}_{(X,Z) }[\|Z-X\|_{2}]\) and introduce the shorthand \(m_{,x^{*}}_{}[\|-x^{*}\|\|_{2}\) for each \(x^{*}^{d}\), \(m_{,P}_{(X,Z) P}[\|X-Z\|_{2}]\), and \(M_{,P}_{(X,Z) P}[\|X-Z\|_{2} ^{2}]\). We further define the Kullback-Leibler (KL) divergence as \((\|)_{}[()]\) when \(\) is absolutely continuous with respect to \(\) (denoted by \(\)) and as \(\) otherwise.

Our analysis will make use of the following assumptions on the SVGD kernel and target distribution.

**Assumption 1** (Lipschitz, mean-zero score function).: _The target distribution \(P_{1}\) has a differentiable density \(p\) with an \(L\)-Lipschitz score function \(s_{p} p\), i.e., \(\|s_{p}(x)-s_{p}(y)\|_{2} L\|x-y\|_{2}\) for all \(x,y^{d}\). Moreover, \(_{P}[s_{p}]=0\) and \(s_{p}(x^{*})=0\) for some \(x^{*}^{d}\)._

**Assumption 2** (Bounded kernel derivatives).: _The kernel \(k\) is twice differentiable and \(_{x,y^{d}}(|k(x,y)|,\|_{x}k(x,y)\|_{2},\| _{y}_{x}k(x,y)\|_{},\|_{x}^{2}k(x,y) \|_{})_{1}^{2}\) for \(_{1}>0\). Moreover, for all \(i,j\{1,2,,d\}\), \(_{x^{d}}_{y_{i}}_{y_{j}}_{x_{i}}_{ x_{j}}k(x,y)|_{y=x}_{2}^{2}\) for \(_{2}>0\)._

**Assumption 3** (Decaying kernel derivatives).: _The kernel \(k\) is differentiable and admits a \(\) such that, for all \(x,y^{d}\) satisfying \(\|x-y\|_{2} 1\),_

\[\|_{x}k(x,y)\|_{2}/\|x-y\|_{2}.\]Assumptions 1, 2, and 3 are both commonly invoked and commonly satisfied in the literature. For example, the Lipschitz score assumption is consistent with prior SVGD convergence analyses  and, by Gorham and Mackey [8, Prop. 1], the score \(s_{p}\) is mean-zero under the mild integrability condition \(_{X P}[\|s_{p}(X)\|_{2}]<\). The bounded and decaying derivative assumptions have also been made in prior analyses  and, as we detail in Appendix A, are satisfied by the kernels most commonly used in SVGD, like the Gaussian and inverse multiquadric (IMQ) kernels. Notably, in these cases, the bounds \(_{1}\) and \(_{2}\) are independent of the dimension \(d\).

To leverage the continuous SVGD convergence rates of Salim et al. , we additionally assume that the target \(P\) satisfies Talagrand's \(T_{1}\) inequality [25, Def. 22.1]. Remarkably, Villani [25, Thm. 22.10] showed that Assumption \(4\) is _equivalent_ to \(P\) being a sub-Gaussian distribution. Hence, this mild assumption holds for all strongly log concave \(P\)[23, Def. 2.9], all \(P\) satisfying the log Sobolev inequality [25, Thm. 22.17], and all _distantly dissipative_\(P\) for which KSD is known to control weak convergence [9, Def. 4].

**Assumption 4** (Talagrand's \(T_{1}\) inequality [25, Def. 22.1]).: _For \(P_{1}\), there exists \(>0\) such that, for all \(_{1}\),_

\[W_{1}(,P)(\|P)/}.\]

Finally we make use of the following notation specific to the SVGD algorithm.

**Definition 1** (Stein operator).: _For any differentiable vector-valued function \(g:^{d}^{d}\), the Langevin Stein operator  for \(P\) satisfying Assumption 1 is defined by_

\[(_{P}g)(x) s_{p}(x),g(x)+ g(x)  x^{d}.\]

**Definition 2** (Vector-valued Stein operator).: _For any differentiable function \(h:^{d}\), the vector-valued Langevin Stein operator  for \(P\) satisfying Assumption 1 is defined by_

\[(_{P}h)(x) s_{p}(x)h(x)+ h(x)  x^{d}.\]

**Definition 3** (SVGD transport map and pushforward).: _The SVGD transport map  for a target \(P\) satisfying Assumption 1, a kernel \(k\) satisfying Assumption 2, a step size \( 0\), and an approximating distribution \(_{1}\) takes the form_

\[T_{,}(x) x+_{X}[(_{P }k(,x))(X)] x^{d}.\]

_Moreover, the SVGD pushforward \(_{}()\) represents the distribution of \(T_{,}(X)\) when \(X\)._

**Definition 4** (Kernel Stein discrepancy).: _The Langevin kernel Stein discrepancy [KSD, 4, 19, 9] for \(P\) satisfying Assumption 1, \(k\) satisfying Assumption 2, and measures \(,_{1}\) is given by_

\[_{P}(,)_{\|g\|_{^{d} 1 }}_{}[_{P}g]-_{}[_{P}g].\]

Notably, the KSD so-defined is symmetric in its two arguments and satisfies the triangle inequality.

**Lemma 1** (KSD symmetry and triangle inequality).: _Under Definition 4, for all \(,,_{1}\),_

\[_{P}(,)=_{P}(,)_{P}(,)_{P}(,)+_{P}(,).\]

Proof.: Fix any \(,,_{1}\). For symmetry, we note that \(g^{d} f=-g^{d}\), so

\[_{P}(,)=_{\|g\|_{^{d} 1}} _{}[_{P}g]-_{}[_{P}g]=_{ \|f\|_{^{d} 1}}_{}[_{P}f]- _{}[_{P}f]=_{P}(,).\]

To establish the triangle inequality, we write

\[_{P}(,) =_{\|g\|_{^{d} 1}}_{}[ _{P}g]-_{}[_{P}g]+_{}[ _{P}g]-_{}[_{P}g]\] \[_{\|g\|_{^{d} 1}}(_{}[ _{P}g]-_{}[_{P}g])+_{\|h\|_{ ^{d} 1}}(_{}[_{P}h]-_{}[ _{P}h])\] \[_{P}(,)+_{P}(,).\]

## 3 Wasserstein Discretization Error of SVGD

Our first main result concerns the discretization error of SVGD and shows that \(n\)-particle SVGD remains close to its continuous SVGD limit whenever the step size sum \(b_{r-1}=_{s=0}^{r-1}_{s}\) is sufficiently small.

**Theorem 1** (Wasserstein discretization error of SVGD).: _Suppose Assumptions 1, 2, and 3 hold. For any \(_{0}^{n},_{0}^{}_{1}\), the outputs \(_{r}^{n}=(_{0}^{n},r)\) and \(_{r}^{}=(_{0}^{},r)\) of Algorithm 2 satisfy_

\[(_{r}^{n},_{r}^{})}{W_{1}(_{0}^{n},_ {0}^{})} b_{r-1}(A+B(Cb_{r-1}))\]

_for \(b_{r-1}_{s=0}^{r-1}_{s}\), \(A=(c_{1}+c_{2})(1+m_{P,x^{*}})\), \(B=c_{1}m_{_{0}^{},P}+c_{2}m_{_{0}^{},P}\), \(C=_{1}^{2}(3L+1)\), \(c_{1}=(_{1}^{2}L,_{1}^{2})\), and \(c_{2}=_{1}^{2}(L+1)+L(,_{1}^{2})\)._

We highlight that Theorem 1 applies to _any_\(_{1}\) initialization of SVGD: the initial particles supporting \(_{0}^{n}\) could, for example, be drawn i.i.d. from a convenient auxiliary distribution \(_{0}^{}\) or even generated deterministically from some quadrature rule. To marry this result with the continuous SVGD convergence bound of Section 5, we will ultimately require \(_{0}^{}\) to be a continuous distribution with finite \((_{0}^{}\|P)\). Hence, our primary desideratum for SVGD initialization is that \(_{0}^{n}\) have small Wasserstein distance to some \(_{0}^{}\) with \((_{0}^{}\|P)<\). Then, by Theorem 1, the SVGD discretization error \(W_{1}(_{r}^{n},_{r}^{})\) will remain small whenever the step size sum is not too large.

The proof of Theorem 1 in Section 6 relies on two lemmas. The first, due to Gorham et al. , shows that the one-step SVGD pushforward map \(_{}\) (Definition 3) is pseudo-Lipschitz with respect to the \(1\)-Wasserstein distance2 whenever the score function \( p\) and kernel \(k\) fulfill a commonly-satisfied pseudo-Lipschitz condition. Here, for any \(g:^{d}^{d}\), we define the Lipschitz constant \((g)_{x,z^{d}} g(x)-g(z) _{2}/ x-z_{2}\).

**Lemma 2** (Wasserstein pseudo-Lipschitzness of SVGD [10, Lem. 12]).: _For \(P\) satisfying Assumption 1, suppose that the following pseudo-Lipschitz bounds hold_

\[(s_{p}(x)k(x,)+_{x}k(x,))  c_{1}(1+ x-x^{*}_{2}),\] \[(s_{p}k(,z)+_{x}k(,z))  c_{2}(1+ z-x^{*}_{2}).\]

_for some constants \(c_{1},c_{2}\) and all \(x,z^{d}\). Then, for any \(,_{1}\),_

\[W_{1}(_{}(),_{}()) W_{1}(,)(1+ c _{,}),\]

_where \(_{}\) is the one-step SVGD pushforward (Definition 3) and \(c_{,}=c_{1}(1+m_{,x^{*}})+c_{2}(1+m_{,x^{*}})\)._

In Section 6, we will show that, under Assumptions 1, 2, and 3, the preconditions of Lemma 2 are fulfilled with \(c_{1}\) and \(c_{2}\) exactly as in Theorem 1. The second lemma, proved in Section 7, controls the growth of the first and second absolute moments under SVGD.

**Lemma 3** (SVGD moment growth).: _Suppose Assumptions 1 and 2 hold, and let \(C=_{1}^{2}(3L+1)\). Then the SVGD output \(_{r}\) of Algorithm 2 with \(b_{r-1}_{s=0}^{r-1}_{s}\) satisfies_

\[m_{_{r},x^{*}}-m_{P,x^{*}} m_{_{r},P}  m_{_{0},P}_{s=0}^{r-1}(1+_{s}C) m_{_{0},P}(Cb_{r-1}),\] \[M_{_{r},P}  M_{_{0},P}_{s=0}^{r-1}(1+_{s}C)^{2} M_{ _{0},P}(2Cb_{r-1}).\]

The key to the proof of Lemma 3 is that we show the norm of any SVGD update, i.e., \( T_{,}(x)-x_{2}\), is controlled by \(m_{,P}\), the first absolute moment of \(\) measured against \(P\). This is mainly due to the Lipschitzness of the score function \(s_{p}\) and our assumptions on the boundedness of the kernel and its derivatives. Then, we can use the result to control the growth of \(m_{_{r},P}\) across iterations since \(m_{_{r+1},P}=_{(X,Z)_{r} P} T_{_{r}, _{r}}(X)-Z_{2}_{2}\). The same strategy applies to the second absolute moment \(M_{,P}\). The proof of Theorem 1 then follows directly from Lemma 2 where we plug in the first moment bound of Lemma 3.

KSD Discretization Error of SVGD

Our next result translates the Wasserstein error bounds of Theorem 1 into KSD error bounds.

**Theorem 2** (KSD discretization error of SVGD).: _Suppose Assumptions 1, 2, and 3 hold. For any \(_{0}^{n},_{0}^{}_{1}\), the outputs of Algorithm 2, \(_{r}^{n}=(_{0}^{n},r)\) and \(_{r}^{}=(_{0}^{},r)\), satisfy_

\[_{P}(_{r}^{n},_{r}^{}) (_{1}L+_{2}d)w_{0,n}(b_{r-1}(A+B(Cb_{r-1} )))\] \[+_{1}d^{1/4}L^{},P}w_{0,n}}(b_{r -1}(2C+A+B(Cb_{r-1}))/2)\]

_for \(w_{0,n} W_{1}(_{0}^{n},_{0}^{})\) and \(A,B,C\) defined as in Theorem 1._

Our proof of Theorem 2 relies on the following lemma, proved in Section 8, that shows that the KSD is controlled by the \(1\)-Wasserstein distance.

**Lemma 4** (KSD-Wasserstein bound).: _Suppose Assumptions 1 and 2 hold. For any \(,_{1}\),_

\[_{P}(,)(_{1}L+_{2}d)W_{1}(,)+_{1} d^{1/4}LW_{1}(,)}.\]

Lemma 4 is proved in two steps. We first linearize \((_{P}g)(x)\) in the KSD definition through the Lipschitzness of \(s_{p}\) and the boundedness and Lipschitzness of RKHS functions. Then, we assign a 1-Wasserstein optimal coupling of \((,)\) to obtain the Wasserstein bound on the right.

Proof of Theorem 2The result follows directly from Lemma 4, the second moment bound of Lemma 3, and Theorem 1. 

## 5 A Finite-particle Convergence Rate for SVGD

To establish our main SVGD convergence result, we combine Theorems 1 and 2 with the following descent lemma for continuous SVGD error due to Salim et al.  which shows that continuous SVGD decreases the KL divergence to \(P\) and drives the KSD to \(P\) to zero.

**Lemma 5** (Continuous SVGD descent lemma [22, Thm. 3.2]).: _Suppose Assumptions 1, 2, and 4 hold, and consider the outputs \(_{r}^{}=(_{0}^{},r)\) and \(_{r+1}^{}=(_{0}^{},r+1)\) of Algorithm 2 with \(_{0}^{} P\). If \(_{0 s r}_{s} R_{,2}\) for some \(>1\) and_

\[R_{,p}\!\!\!(^{2}(L+^{2})},(-1)(1+Lm_{_{0}^{},x^{}}+2L {KL}(_{0}^{}\|P)}))p\{1,2\},\] (1)

_then_

\[(_{r+1}^{}\|P)-(_{r}^{}\|P)- _{r}1-^{2}(L+^{2})}{2}_{r} {KSD}_{P}(_{r}^{},P)^{2}.\] (2)

By summing the result (2) over \(r\{0,,t\}\), we obtain the following corollary.

**Corollary 1**.: _Under the assumptions and notation of Lemma 5, suppose \(_{0 r t}_{r} R_{,1}\) for some \(>1\), and let \(_{r})}{_{r=0}^{r}c(_{r})}\) for \(c()1-^{2}(L+^{2})}{2} \). Since \( c()<\), we have_

\[_{r=0}^{t}_{r}_{P}(_{r}^{},P)^{2}^{r}c(_{r})}(_{0}^{}\|P)^{r}_{r}}(_{0}^{}\|P).\]

Finally, we arrive at our main result that bounds the approximation error of \(n\)-particle SVGD in terms of the chosen step size sequence and the initial discretization error \(W_{1}(_{0}^{n},_{0}^{})\).

**Theorem 3** (KSD error of finite-particle SVGD).: _Suppose Assumptions 1, 2, 3, and 4 hold, fix any \(_{0}^{} P\) and \(_{0}^{n}_{1}\), and let \(w_{0,n} W_{1}(_{0}^{n},_{0}^{})\). If \(_{0 r<t}_{r}_{t} R_{,1}\)3 for some \(>1\) and \(R_{,1}\) defined in Lemma 5, then the Algorithm 2 outputs \(_{r}^{n}=(_{0}^{n},r)\) satisfy_

\[_{0 r t}_{P}(_{r}^{n},P)_{r=0}^{t}_{r} _{P}(_{r}^{n},P) a_{t-1}++b_{t-1}} (_{0}^{}\|P)},\] (3)

_for \(_{r}\) as defined in Lemma 5, \((A,B,C)\) as defined in Theorem 1, \(b_{t-1}_{r=0}^{t}_{r}\), and_

\[a_{t-1} (_{1}L+_{2}d)w_{0,n}(b_{t-1}(A+B(Cb_{ t-1})))\] (4) \[+_{1}d^{1/4}L^{},P}w_{0,n}}(b_{t- 1}(2C+A+B(Cb_{t-1}))/2).\]Proof.: By the triangle inequality (Lemma 1) and Theorem 2 we have

\[|_{P}(_{r}^{n},P)-_{P}(_{r}^{},P)|_ {P}(_{r}^{n},_{r}^{}) a_{r-1}\]

for each \(r\). Therefore

\[_{r=0}^{t}_{r}(_{P}(_{r}^{n},P)-a_{r-1})^{2}_{r=0}^ {t}_{r}_{P}(_{r}^{},P)^{2}+b_{ t-1}}(_{0}^{}\|P),\] (5)

where the last inequality follows from Corollary 1. Moreover, by Jensen's inequality,

\[_{r=0}^{t}_{r}(_{P}(_{r}^{n},P)-a_{r-1})^{2} _{r=0}^{t}_{r}_{P}(_{r}^{n},P)-_{r=0}^{t}_{r}a_{r-1 }^{2}.\] (6)

Combining (5) and (6), we have

\[_{r=0}^{t}_{r}_{P}(_{r}^{n},P)_{r=0}^{t}_{r}a_ {r-1}++b_{t-1}}(_{0}^{}\|P)}.\]

We finish the proof by noticing that \(_{r=0}^{t}_{r}a_{r-1}_{0 r t}a_{r-1}=a_{t-1}\). 

The following corollary, proved in Appendix B, provides an explicit SVGD convergence bound and rate by choosing the step size sum to balance the terms on the right-hand side of (3). In particular, Corollary 2 instantiates an explicit SVGD step size sequence that drives the kernel Stein discrepancy to zero at an order \(1/\) rate.

**Corollary 2** (A finite-particle convergence rate for SVGD).: _Instantiate the notation and assumptions of Theorem 3, let \((_{0,n},,,)\) be any upper bounds on \((w_{0,n},A,B,C)\) respectively, and define the growth functions_

\[(w)(e^{e}+)_{,}(x,y,)}((, -y)).\]

_If the step size sum \(b_{t-1}=_{r=0}^{t-1}_{r}=s_{n}^{}\) for_

\[s_{n}^{} _{,}_{0,n} _{0,n})},,_{1},_{,} _{0,n}\,(_{0,n}),+2,_{2} ,\] \[_{1} 1,_{,}_{0,n} _{0,n})},,1,\] \[_{2} 1,_{,}_{0,n} \,(_{0,n}),+2,1\]

_then_

\[_{0 r t}_{P}(_{r}^{n},P)\] \[(_{1}L+_{2}d)_{0,n}+_{1} d^{1/4}L^{},P}_{0,n}}+}(_{0}^{}\,\|P)}&s_{n}^{}=0\\ L+_{2}d)+ d^{1/4}L^{},P}} }{_{0,n})}}+(_{0}^{}\,\|P)} {R_{,1}+}((,n, \,(_{0,n})))}{(1,_{B},^{}(_{0,n},0,1))}--2))}}&\] (7) \[=O+_{0,n}} )}}.\] (8)

_If, in addition, \(_{0}^{n}=_{i=1}^{n}_{x_{i}}\) for \(x_{i}_{0}^{}\) with \(M_{_{0}^{}}_{_{0}^{}}\|\|_{2} ^{2}<\), then_

\[_{0,n}^{}}(n)^{[d-2]}}{\,n^{ 1/(2d)}} w_{0,n}\] (9)

_with probability at least \(1-c\) for a universal constant \(c>0\). Hence, with this choice of \(_{0,n}\),_

\[_{0 r t}_{P}(_{r}^{n},P)=O}\]

_with probability at least \(1-c\)._

Specifically, given any upper bounds \((_{0,n},,,)\) on the quantities \((w_{0,n},A,B,C)\) defined in Theorem 3, Corollary 2 specifies a recommended step size sum \(s_{n}^{}\) to achieve an order \(1/+_{0,n}})}\) rate of SVGD convergence in KSD. Several remarks are in order. First,the target step size sum \(s_{n}^{}\) is easily computed given \((_{0,n},,,)\). Second, we note that the target \(s_{n}^{}\) can equal \(0\) when the initial Wasserstein upper bound \(_{0,n}\) is insufficiently small since \(((,-y))=(}{B})=0\) for small arguments \(x\). In this case, the setting \(b_{t-1}=0\) amounts to not running SVGD at all or, equivalently, to setting all step sizes to \(0\).

Third, Corollary 2 also implies a time complexity for achieving an order \(1/+_{0,n}})}\) error rate. Recall that Theorem 3 assumes that \(_{0 r<t}_{r} R_{,1}\) for \(R_{,1}\) defined in (1). Hence, \(t^{} s_{n}^{}/R_{,1}\) rounds of SVGD are necessary to achieve the recommended setting \(_{r=0}^{t^{}-1}_{r}=s_{n}^{}\) while also satisfying the constraints of Theorem 3. Moreover, \(t^{}\) rounds are also sufficient if each step size is chosen equal to \(s_{n}^{}/t^{}\). In addition, since \(s_{n}^{}=O((e^{e}+_{0,n}}))\), Corollary 2 implies that SVGD can deliver \(_{0 r<t^{}}\). KSD\({}_{P}(_{r}^{n},P)\) in \(t^{}=O(1/^{2})\) rounds. Since the computational complexity of each SVGD round is dominated by \((n^{2})\) kernel gradient evaluations (i.e., evaluating \(_{x}k(x_{i},x_{j})\) for each pair of particles \((x_{i},x_{j})\)), the overall computational complexity to achieve the order \(1/+_{0,n}})}\) error rate is \(O(n^{2} s_{n}^{}/R_{,1})=O(n^{2}(e^{e}+_{0,n}}))\).

## 6 Proof of Theorem 1: Wasserstein discretization error of SVGD

In order to leverage Lemma 2, we first show that the pseudo-Lipschitzness conditions of Lemma 2 hold given our Assumptions 1 to 3. Recall that \(s_{p}\) is Lipschitz and \(x^{}\) satisfies \(s_{p}(x^{})=0\) by Assumption 1. Then, by the triangle inequality, the definition of \(\|\|_{}\), \(\|k(x,z)\|_{2}_{1}^{2}\) and \(\|_{z}_{x}k(x,z)\|_{}_{1}^{2}\) from Assumption 2, and Cauchy-Schwartz,

\[(s_{p}(x)k(x,)+_{x}k(x,))\] \[\|s_{p}(x)-s_{p}(x^{})\|_{2}\|_{z} k(x,z)\|_{2}+\|_{z}_{x}k(x,z)\|_{}\] \[=_{\|u\|_{2} 1}(\|s_{p}(x)-s_{p}(x^{ })\|_{2}\|_{z}k(x,z)^{}u\|)+\|_{z} _{x}k(x,z)\|_{}\] \[ L\|x-x^{}\|_{2}\|_{z}k(x,z)\|_{2}+ \|_{z}_{x}k(x,z)\|_{}\] \[(_{1}^{2}L,_{1}^{2})(1+\|x-x^{} \|_{2}).\]

Letting \(c_{1}=(_{1}^{2}L,_{1}^{2})\) and taking supremum over \(z\) proves the first pseudo-Lipschitzness condition. Similarly, we have

\[(s_{p}k(,z)+_{x}k(,z))\] \[_{x^{d}}(s_{p})|k(x,z)|+\| s_{p}(x)-s_{p}(x^{})\|_{2}\|_{x}k(x,z)\|_{2}+\| _{x}^{2}k(x,z)\|_{}\] \[_{1}^{2}L+_{x^{d}}L\|x-x^{}\|_{2} \|_{x}k(x,z)\|_{2}+_{1}^{2},\] (10)

where we used the Lipschitzness of \(s_{p}\) from Assumption 1 and \(|k(x,z)|_{1}^{2},\|_{x}^{2}k(x,z)\|_{ }_{1}^{2}\) from Assumption 2. Now we consider two cases separately: when \(\|x-z\|_{2} 1\) and \(\|x-z\|_{2}<1\).

* Case 1: \(\|x-z\|_{2} 1\). Recall that there exists \(>0\) such that \(\|_{x}k(x,z)\|_{2}/\|x-z\|_{2}\) by Assumption 3. Then, using this together with the triangle inequality, we have \[\|x-x^{}\|_{2}\|_{x}k(x,z)\|_{2} +\|z-x^{}\|_{2}}{\|x-z\|_ {2}}(1+\|z-x^{}\|_{2}).\] (11)
* Case 2: \(\|x-z\|_{2}<1\). Then, using \(\|_{x}k(x,z)\|_{2}_{1}^{2}\) from Assumption 2 and by the triangle inequality, we have \[\|x-x^{}\|_{2}\|_{x}k(x,z)\|_{2}_{1} ^{2}(\|x-z\|_{2}+\|z-x^{}\|_{2})<_{1}^{2}(1+ \|z-x^{}\|_{2}).\] (12) Combining (11) and (12) and using the triangle inequality, we get \[\|x-x^{}\|_{2}\|_{x}k(x,z)\|_{2}( ,_{1}^{2})(1+\|z-x^{}\|_{2}).\] (13) Plugging (13) back into (10), we can show the second pseudo-Lipschitzness condition holds for \(c_{2}=(_{1}^{2}(L+1)+L(,_{1}^{2}),L(, _{1}^{2}))=_{1}^{2}(L+1)+L(,_{1}^{2})\).

Now we have proved that both pseudo-Lipschitzness preconditions of Lemma 2 hold under our Assumptions 1 to 3. By repeated application of Lemma 2 and the inequality \((1+x) e^{x}\), we have

\[W_{1}(_{r+1}^{n},_{r+1}^{}) =W_{1}(_{_{r}}(_{r}^{n}),_{_{r}}(_{r} ^{}))(1+_{r}D_{r})W(_{r}^{n},_{r}^{})\] \[ W_{1}(_{0}^{n},_{0}^{})_{s=0}^{r}(1+ _{s}D_{s}) W_{1}(_{0}^{n},_{0}^{})(_{ s=0}^{r}_{s}D_{s})\] (14)

for \(D_{s}=c_{1}(1+m_{_{r}^{n},x^{*}})+c_{2}(1+m_{_{r}^{},x^{*}})\).

Using the result from Lemma 3, we have

\[D_{s+1} A+B(Cb_{s})\]

for \(A=(c_{1}+c_{2})(1+m_{P,x^{*}})\), \(B=c_{1}m_{_{0}^{},P}+c_{2}m_{_{0}^{},P}\), and \(C=_{1}^{2}(3L+d)\). Therefore

\[_{s=0}^{r}_{s}D_{s}_{0 s r}D_{s}_{s=0}^{r} _{s} b_{r}(A+B(Cb_{r-1})) b_{r}(A+B(Cb_{r})).\]

Plugging this back into (14) proves the result.

## 7 Proof of Lemma 3: SVGD moment growth

From Assumption 2 we know \(|k(y,x)|_{1}^{2}\) and \(_{y}^{2}k(y,x)_{op}_{1}^{2}\). The latter implies

\[\|_{y}k(y,x)-_{z}k(z,x)\|_{2}_{1}^{2}\|y-z\|_{2}.\]

Recall that \(s_{p}\) is Lipschitz and satisfies \(_{P}[s_{p}()]=0\) by Assumption 1. Let \(\) be any probability measure. Using the above results, Jensen's inequality and the fact that \(_{Z P}[(_{P}k(,x))(Z)]=0\), we have

\[\|T_{,}(x)-x\|_{2}\|_{X}[ (_{P}k(,x))(X)]\|_{2}\] \[=\|_{X}[(_{P}k(,x))(X)]- _{Z P}[(_{P}k(,x))(Z)]\|_{2}\] \[=\|_{(X,Z) P}[k(Z,x)(s_{p}(X)-s_ {p}(Z))+(k(X,x)-k(Z,x))(s_{p}(X)-_{P}[s_{p}()])\] \[+(_{X}k(X,x)-_{Z}k(Z,x))]\|_{2}\] \[_{(X,Z) P}[|k(Z,x)|\|_{Sp}(X )-s_{p}(Z)\|_{2}+(|k(X,x)|+|k(Z,x)|)\|s_{p}(X)-_{Y P}[s_{p}(Y)] \|_{2}\] \[+\|_{X}k(X,x)-_{Z}k(Z,x)\|_{2}]\] \[_{(X,Z) P}[_{1}^{2}(L+1 )\|X-Z\|_{2}]+ 2_{1}^{2}L_{(X,Y) P}[\|X-Y\|_{2}]\] \[=_{1}^{2}(3L+1)_{(X,Z) P}[ \|X-Z\|_{2}]\] \[= Cm_{,P}.\] (15)

The last step used the definitions \(m_{,P}_{(X,Z) P}[\|X-Z\|_{2}]\) and \(C=_{1}^{2}(3L+1)\). Then, applying the triangle inequality and (15), we have

\[m_{_{r+1},P} =_{(X,Z)_{r+1} P}[\|X-Z\|_{2}]= _{(X,Z)_{r} P}[\|T_{_{r},_{r}}(X)-Z\|_{2}]\] \[_{(X,Z)_{r} P}[\|T_{_{r},_ {r}}(X)-X\|_{2}+\|X-Z\|_{2}](1+_{r}C)m_{_{r},P},\] (16) \[M_{_{r+1},P} =_{(X,Z)_{r+1} P}[\|X-Z\|_{2}^{2}]=_{(X,Z)_{r} P}[\|T_{_{r},_{r}}(X)-Z\|_{2}^{2}]\] \[_{(X,Z)_{r} P}[\|T_{_{r},_ {r}}(X)-X\|_{2}^{2}+2\|T_{_{r},_{r}}(X)-X\|_{2}\|X-Z\|_{2}+\|X-Z\|_{2 }^{2}]\] \[(_{r}^{2}C^{2}+2_{r}C)m_{_{r},P}^{2}+M_{ _{r},P}(1+2_{r}C+_{r}^{2}C^{2})M_{_{r},P}\] \[=(1+_{r}C)^{2}M_{_{r},P},\] (17)

where the second last step used Jensen's inequality \(m_{_{r},P}^{2} M_{_{r},P}\). Then, we repeatedly apply (16) and (17) together with the triangle inequality and the bound \(1+x e^{x}\) to get

\[M_{_{r},P}  M_{_{0},P}_{s=0}^{r-1}(1+_{s}C)^{2} M_{ _{0},P}(2C_{s=0}^{r-1}_{s}) M_{_{0},P}(2Cb_{r-1})\] \[m_{_{r},x^{*}}-m_{P,x^{*}}  m_{_{r},P} m_{_{0},P}_{s=0}^{r-1}(1+_{s}C) m _{_{0},P}(Cb_{r-1}).\]

## 8 Proof of Lemma 4: KSD-Wasserstein bound

Our proof generalizes that of Gorham and Mackey [9, Lem. 18]. Consider any \(g^{d}\) satisfying \(\|g\|_{^{d}}^{2}_{i=1}^{d}\|g_{i}\|_{}^{2} 1\). From Assumption 2 we know

\[\|g(x)\|_{2}^{2}  k(x,x)_{i=1}^{d}\|g_{i}\|_{}^{2}_{1} ^{2},\] (18) \[\| g(x)\|_{op}^{2} \| g(x)\|_{F}^{2}=_{i=1}^{d}_{j=1}^{d }|_{x},g_{j}(x)|^{2}\|g\|_{^{d}}^{2}(_{y }_{x}k(x,y)|_{y=x})\] \[ d\|_{y}_{x}k(x,y)|_{y=x}\|_{} _{1}^{2}d,\] (19) \[\|( g(x))\|_{2}^{2} =_{i=1}^{d}_{j=1}^{d}_{x_{i}}_{x_{j}} g_{j}(x)^{2} d_{i=1}^{d}_{j=1}^{d}|_{x_{i}}_{x_{j}} g_{j}(x)|^{2}\] \[ d_{i=1}^{d}_{j=1}^{d}\|g_{j}\|_{}^{2}( _{y_{i}}_{y_{j}}_{x_{i}}_{x_{j}}k(x,y)|_{y=x}) _{2}^{2}d^{2},\]

Suppose \(X,Y,Z\) are distributed so that \((X,Y)\) is a \(1\)-Wasserstein optimal coupling of \((,)\) and \(Z\) is independent of \((X,Y)\). Since \(s_{p}\) is \(L\)-Lipschitz with \(_{P}[s_{p}]=0\) (Assumption 1), \(g\) is bounded (18), and \(g\) and \( g\) are Lipschitz (19), repeated use of Cauchy-Schwarz gives

\[_{}[_{P}g]-_{}[_ {P}g]\] \[=[ g(X)- g(Y)]+[ s _{p}(X)-s_{p}(Y),g(X)]+[ s_{p}(Y)-s_{p}(Z),g(X)-g(Y)]\] \[(_{2}d+_{1}L)W_{1}(,)+L[\|Y -Z\|_{2}(2_{1},_{1}\|X-Y\|_{2})].\]

Since our choice of \(g\) was arbitrary, the first advertised result now follows from the definition of KSD (Definition 4). The second claim then follows from Cauchy-Schwarz and the inequality \((a,b)^{2} ab\) for \(a,b 0\), since

\[[\|Y-Z\|_{2}(2_{1},_{1} \|X-Y\|_{2})] M_{,P}^{1/2}\,[(2_{1},_ {1}\|X-Y\|_{2})^{2}]^{1/2}\] \[}_{1}d^{1/4}[\|X-Y\| _{2}]^{1/2}=W_{1}(,)}_{1}d^{1/4}.\]

## 9 Conclusions and Limitations

In summary, we have proved the first unified convergence bound and rate for finite-particle SVGD. In particular, our results show that with a suitably chosen step size sequence, SVGD with \(n\)-particles drives the KSD to zero at an order \(1/\) rate. The assumptions we have made on the target and kernel are mild and strictly weaker than those used in prior work to establish KSD weak convergence control . However, we suspect that, with additional effort, the Lipschitz score assumption (Assumption 1) can be relaxed to accommodate pseudo-Lipschitz scores as in Erdogdu et al.  or weakly-smooth scores as in Sun et al. . A second limitation of this work is that the obtained rate of convergence is quite slow. However, we hope that this initial recipe for explicit, non-asymptotic convergence will serve as both a template and a catalyst for the field to develop refined upper and lower bounds for SVGD error. To this end, we leave the reader with several open challenges. First, can one establish a non-trivial minimax lower bound for the convergence of SVGD? Second, can one identify which types of target distributions lead to worst-case convergence behavior for SVGD? Finally, can one identify commonly met assumptions on the target distribution and kernel under which the guaranteed convergence rate of SVGD can be significantly improved? Promising follow-up work has already begun investigating speed-ups obtainable by focusing on the convergence of a finite set of moments  or by modifying the SVGD algorithm .