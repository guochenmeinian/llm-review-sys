# Automated Multi-level Preference for MLLMs

Mengxi Zhang\({}^{1,2}\), Wenhao Wu\({}^{3}\), Yu Lu\({}^{4}\), Yuxin Song\({}^{1}\), Kang Rong\({}^{1}\), Huanjin Yao\({}^{1,5}\)

**Jianbo Zhao\({}^{6}\), Fanglong Liu\({}^{1}\), Haocheng Feng\({}^{1}\), Jingdong Wang\({}^{1}\), Yifan Sun\({}^{1}\)**

\({}^{1}\)Baidu Inc. \({}^{2}\)Tianjin University \({}^{3}\)The University of Sydney

\({}^{4}\)University of Technology Sydney \({}^{5}\)Tsinghua University \({}^{6}\)Chinese Academy of Science

Corresponding author

###### Abstract

Current multimodal Large Language Models (MLLMs) suffer from "hallucination", occasionally generating responses that are not grounded in the input images. To tackle this challenge, one promising path is to utilize reinforcement learning from human feedback (RLHF), which steers MLLMs towards learning superior responses while avoiding inferior ones. We rethink the common practice of using binary preferences (_i.e._, superior, inferior), and find that adopting multi-level preferences (_e.g._, superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences. 2) It further integrates cross-level comparisons (beyond adjacent-level comparisons), thus providing a broader range of comparisons with hallucination examples. To verify our viewpoint, we present the Automated Multi-level Preference (**AMP**) framework for MLLMs. To facilitate this framework, we first develop an automated dataset generation pipeline that provides high-quality multi-level preference datasets without any human annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm to robustly conduct complex multi-level preference learning. Additionally, we propose a new hallucination benchmark, MRHal-Bench. Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method. Code is available at https://github.com/takomc/amp.

## 1 Introduction

Multimodal Large Language Models (MLLMs)  have achieved remarkable advancement in vision-language understanding tasks, _e.g._, vision question answering , image captioning , and human-machine conversation. Despite MLLMs achieving significant breakthroughs, they still suffer from hallucinations , referring to responses that are not accurately anchored to the context provided by images. This problem shrinks the performance of MLLMs and draws considerable research attention. To mitigate the hallucinations, some existing methods  adopt Reinforcement Learning from Human Feedback (RLHF) methods, which collect human/AI preferences and integrate them into the MLLMs optimization process via reinforcement learning.

Existing RLHF methods have demonstrated that comparing superior and inferior responses within a binary-level preference framework can improve the performance of optimized MLLMs. However, _Is a single comparison between superior and inferior responses sufficient for preference learning in MLLMs?_ Upon consideration, we find that a multi-level preference framework offers greater benefits for preference learning, primarily due to two main intuitive advantages. Firstly, reducing the gap between adjacent levels helps mitigate the challenge of distinguishing micro hallucinations in responses. As depicted in Fig. 1, in the baseline method (_i.e._, binary preference), significant differences exist between the superior response A and the inferior response C. By introducing an additional medium response and shifting the focus to multiple comparisons between adjacent levels("A>B", "B>C"), as highlighted by the red arrows in Fig. 1(b), we mitigate this issue. Interestingly, under certain conditions, the MLLM's performance with "A>C" comparison is even worse than that with "B>C", indicating that reducing the gap between adjacent levels is sometimes more effective than enhancing the quality of superior responses. Secondly, cross-level comparisons can further enhance performance. In the comparisons between adjacent levels, the only comparison utilizing the inferior response A is "A>B", which may lead the model to focus more on suppressing hallucinations in response B. To address this, we introduce the cross-level comparison "A>C" (the green arrow in Fig. 1(c)) to provide more negative examples, thereby helping the model suppress more possible hallucinations. By integrating these strategies, we evolve the conventional binary-level preference learning into a more sophisticated multi-level preference learning framework.

However, exploring multi-level preferences for MLLMs poses significant challenges: 1) Labeling multi-level preference datasets is expensive and laborious. While some methods  utilize human annotators to obtain preference labels, this approach is effective for binary datasets but falls short for multi-level preference datasets. Specifically, establishing a \(K\)-level preference dataset requires human annotators to make \(K(K-1)/2\) comparisons. For example, with \(K=5\), this results in 10 comparisons, significantly more than is required for binary datasets. On the other hand, datasets annotated by humans or AI often contain significant noise and bias. To investigate this, we collected preferences from both humans and GPT-4V  on a subset of ShareGPT4V , using three MLLMs to generate varied responses. Setting \(K\) to 3, we compared pairs of responses (A&B, B&C, A&C) through three comparisons. However, we observed a frequent contradictory pattern (A>B, B>C, C>A), with rates of approximately 14% and 11% in human and GPT-4V annotations, respectively, resulting in a low-quality multi-level preference dataset. 2) The optimal multi-level preference learning objective remains unclear. While multi-level preference is more beneficial for optimizing MLLMs, it introduces greater complexity than binary preference. Therefore, it requires an effective algorithm to fully utilize the knowledge embedded within multi-level preference datasets.

To overcome the challenges outlined above, we introduce innovative strategies at both the data and method levels: 1) At the data level, we propose two novel methods for generating initial multi-level preference datasets without human or AI annotators. Furthermore, we implement an auto-check mechanism to further refine these datasets by evaluating the scores and accuracy of the generated responses. 2) At the method level, we introduce the Multi-level Direct Preference Optimization (MDPO) algorithm, a derivative of the traditional Direct Preference Optimization (DPO) algorithm . The MDPO algorithm extends the capabilities of the DPO algorithm to facilitate multi-level preference optimization. Additionally, we incorporate a tailored penalty term into the MDPO learning objective to ensure robust multi-level preference learning. 3) Finally, we introduce a new evaluation benchmark, MRHal-Bench, which is the first designed specifically to evaluate hallucinations in multi-round dialogues. In summary, our contributions are as follows:

* Contrary to prior RLHF studies that focused solely on enhancing the quality of superior responses, our findings indicate that inferior responses can also play a crucial role in reducing hallucinations under the multi-level preference learning framework.

Figure 1: **Left:** Depicted are the input image, text prompt, and corresponding multi-level preference dataset. Contents highlighted in red signify hallucinations. Responses range from A to C, representing varying degrees of quality from superior to inferior. **Right:** Illustrating the strategy for leveraging inferior responses. (a) displays the conventional RLHF baseline, which adopts the binary-level preference. (b) To mitigate the gap between adjacent levels, we first split a single comparison into multiple comparisons by inserting extra medium responses. (c) Furthermore, we introduce the cross-level comparison to augment the dataset with more hallucination examples.

* To support effective multi-level preference learning, we develop two novel methods and an auto-check mechanism, enabling the creation of high-quality multi-level preference datasets without the need for human or AI annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm with a specifically crafted learning objective, allowing MLLMs to robustly learn from the multi-level preference dataset.
* Our extensive experiments across various hallucination benchmarks confirm the effectiveness of our framework. Additionally, we have introduced MRHal-Bench, the first benchmark specifically designed to evaluate hallucinations in multi-round dialogues.

## 2 Related Work

### Multimodal Large Language Models

Recently, the multimodal learning community has witnessed the great success of MLLMs [1; 2; 3; 4; 5; 6; 18], which employ a cross-modal alignment module to connect the visual encoder [19; 20; 21] and the language model [22; 23]. Typically, MLLMs undergo a standard training strategy involving two stages. First, to bridge the gap between visual and textual representations, the cross-modal alignment module is trained on a large-scale multimodal dataset [1; 24; 25], which endows the LLMs with visual-understanding ability. Then, MLLMs are further fine-tuned on specific visual instruction datasets [2; 16; 18; 26] to facilitate various downstream tasks [7; 8]. Despite the significant advancement, MLLMs still suffer from hallucinations, which decrease their performance on multiple tasks and attract increasing attention from researchers.

### Hallucinations in MLLMs

Hallucinations in MLLMs [9; 10] denote inconsistencies between the input image and the generated response. Unlike hallucinations in LLMs [27; 28], those observed in MLLMs are more complicated, which attracts more attention from researchers. Some methods [26; 29] focus on reducing hallucinations by constructing high-quality datasets, while others employ specialized mechanisms such as decoding strategies [30; 31], retrieval augmented generation , and chain-of-thought  to mitigate hallucinations. However, due to the inherent limitations of cross-entropy loss, these methods may provide insufficient guidance for modality alignment. Recently, reinforcement learning-based methods [11; 12; 13; 14; 34], leveraging techniques like DPO  and PPO , have emerged as promising direction. Yet, these methods rely on preference datasets annotated by humans or AI, which are costly and susceptible to noise. Besides, they follow the traditional binary-level preference framework, which is insufficient for preference learning of MLLMs. To address these problems, we propose a novel AMP framework, utilizing a human-free multi-level preference dataset and the MDPO algorithm to guide MLLMs.

## 3 Methods

In this section, we delve into the Automated Multi-level Preference (AMP) framework. Initially, we outline two strategies for constructing an initial multi-level preference dataset, aligning with two perspectives of the scaling law [36; 37]. Subsequently, we introduce the auto-check mechanism aimed at refining the initial dataset based on relevant metrics. Lastly, we introduce the Multi-level Direct Preference Optimization (MDPO) algorithm, featuring a novel and robust learning objective.

### Human-free Multi-level Preference Dataset Generation

The quality of the preference dataset significantly influences the refined model's performance. Constructing a high-quality initial preference dataset relies on two fundamental principles. Firstly, the ranking between superior and inferior responses should be correct in most cases. Secondly, the language style among different responses is expected to be consistent. Specifically, inconsistent language styles can introduce biases that mislead the MLLM, resulting in reward hacking and performance degradation [12; 36]. Considering these factors, we propose the _Multi-size Expert Generation (MEG)_ and _Incremental Generation (IG)_ strategies to build reliable preference datasets from the perspectives of model size and dataset size, respectively.

#### 3.1.1 Multi-size Expert Generation

Scaling laws suggest that the performance of the model improves as the model size increases. Thus, a logical strategy is to generate various responses using models of different sizes. For consistency in language style, it's preferable that these models stem from the same family. Specifically, we adpot LLaVA-based models, such as LLaVA-2B , LLaVA-7B, LLaVA-13B, and LLaVA-34B . Leveraging the standard response in the instruction tuning dataset , we procure up to 5 responses of differing quality.

#### 3.1.2 Incremental Generation

In _Multi-size Expert Generation_, our focus lies on employing models of various sizes, while _Incremental Generation_ involves training datasets of different sizes. In practice, we partition the entire fine-tuned dataset \(=\{;P;R\}\) into \(K-2\) equal parts for the \(K\)-rank preference dataset, where \(\), \(P\), and \(R\) symbolize the image, text prompt, and standard response, respectively. Then, we use subsets \(_{i}=[_{1},_{2},...,_{i}]\) to fine-tune the pre-trained MLLM \(\), yielding \(K-2\) fine-tuned MLLMs, where \(i[1,K-2]\). Hence, the \(K-2\) responses generated by fine-tuned MLLMs, along with the response generated by the pre-trained MLLM and the standard response constitute the \(K\)-rank preference dataset. The entire process is documented in Algorithm 1.

```
0: Image \(\), text prompt \(P\), and standard response \(R\) for fine-tuned dataset \(=\{;P;R\}\), annotated dataset \(=\{_{a};P_{a};R_{a}\}\), pre-trained MLLM \(\).
0:\(K\)-level preference dataset \(=\{_{a};P_{a};[R_{0},R_{1},...,R_{K-1}]\}\).
1: Split \(\) into \(K-2\) equal parts \([_{1},_{2},...,_{K-2}]\);
2:for (\(i=1\) to \(K-2\)) do
3: Train \(\) with subset \(_{i}=[_{1},_{2},...,_{i}]\) Get fine-tuned MLLM \(_{i}\);
4:\(R_{i}=_{i}(_{a},P_{a})\); {Generate response \(R_{i}\) via fine-tuned MLLM \(_{i}\)}
5:endfor
6:\(R_{0}\)=\(R_{a}\), \(R_{K-1}\)=\((_{a},P_{a})\);
7:return\(=\{_{a};P_{a};[R_{0},R_{1},...,R_{K-1}]\}\). ```

**Algorithm 1** The Pseudocode of Incremental Generation for \(K\)-rank Preference Dataset.

#### 3.1.3 Auto-check Mechanism

In the aforementioned process, we devised two strategies to establish the initial multi-level preference dataset. While the rankings in this dataset are generally accurate, occasional anomalies may lead to incorrect preferences. To enhance the ranking accuracy, we introduce the auto-check mechanism.

Figure 2: Pipeline for Constructing Human-free Multi-level Preference Dataset. We initiate the process with _Multi-size Expert Generation_ and _Incremental Generation_ to establish the initial dataset. Then, to enhance the quality of the initial preference dataset, we introduce the _Auto-check Mechanism_, which calculates both global and local metrics based on sentences and noun chunks, respectively.

First, we identify all nouns in the various responses, including terms like "motorhome", "street", _etc_. Note that certain nouns are deprecated (further details are provided in Appendix A.1). Next, we analyze the dependency relationships within the sentence to extend each noun into the longest possible noun chunks. For example, "a white motorhome, which is parked on a street" would be represented (denoted by pink color in Fig. 2).

After extracting all noun chunks, we send them into the noun chunk expert (_i.e_., the text encoder of CLIP ) to obtain text features \(F_{S}=\{f_{S_{1}},f_{S_{2}},...,f_{S_{}}\}\) and \(F_{G}=\{f_{G_{1}},f_{G_{2}},...,f_{G_{}}\}\), where \(\) and \(\) denote the number of noun chunks in standard and generated responses, respectively. We then calculate the similarity score as outlined in Eq. 1:

\[[m,n]=} f_{G_{n}}}{\|f_{S_{m}}\|\|f_{G_{n}} \|},[m]=max([m,:]),\] (1)

where \(^{}\) is the similarity matrix between standard and generated responses. \(^{}\) represents the similarity score of generated response \(F_{G}\). We further introduce the accuracy metric:

\[[m]=1&[m]>\\ 0&,=()/ ,\] (2)

where \(\) is the threshold, set to \(0.85\). Accuracy (\(\)) reflects the completeness of the credible components within the generated response.

While noun chunks represent the consistency at a local level, entire sentences represent global consistency, such as the relationships between multiple objects and the actions of objects, _etc_. To assess global consistency, we retrieve the sentences where each noun chunk is located as the global representation. The relevant metrics of sentences are the same as those of noun chunks.

Finally, we compute the final accuracy and scores by averaging the local and global metrics. Among the generated responses, the one with the highest accuracy is regarded as the best. In cases where multiple responses achieve equal accuracy, the one with the highest scores is considered superior.

### Multi-level Direct Preference Optimization (MDPO)

Reinforcement learning algorithms [11; 12; 13; 34] have demonstrated promising results in training MLLMs with human-preference datasets. Encouraged by the success of these pioneers, we delve deeper into the potential of multi-level preferences. In this section, we design the Multi-level Direct Preference Optimization (MDPO) algorithm, furnishing a novel and robust learning objective.

#### 3.2.1 Preliminary

Prevalent methods [11; 39; 40] leverage the Proximal Policy Optimization (PPO) algorithm to align with preference data. However, the performance of this approach highly depends on the extra reward model, which is sensitive to noises within the preference dataset. Besides, the last stage of PPO fine-tunes the actor and critic model with the online strategy, resulting in high computational costs and unstable procedures. To mitigate these issues, DPO  excludes the reward model by analytically expressing reward functions with optimal policy \(_{*}\) and initial policy \(_{}\). Denote \(x\) and \(y\) as the inputs and outputs of MLLMs, the reward function is converted into:

\[r(x,y)=(y|x)}{_{}(y|x)}+ Z(x),\] (3)

where \(Z()\) is the partition function, \(\) is a constant. Under the Bradley-Terry model, the policy objective becomes:

\[_{}(_{};_{ })&=-_{(x,y_{w},y_{l})} [(r(x,y_{w})-r(x,y_{l}))],\\ &=-_{(x,y_{w},y_{l})}[ ((y_{w}|x)}{_{}(y_{w}|x)}- (y_{l}|x)}{_{}(y_{l}|x)})], \] (4)

where \(()\) represents the Sigmoid function, and \(x\), \(y_{w}\), and \(y_{l}\) denote inputs, superior and inferior responses, respectively. In practice, \(_{}\) remains frozen during DPO training. Thus, only the policy model \(_{}\) is updated in the training process, ensuring efficiency and cost-effectiveness.

### Learning Objective of MDPO Algorithm

To facilitate the multi-level preference dataset, we revise the learning objective for \(K\)-rank with \(K(K-1)/2\) comparisons:

\[_{}(_{};_{})= -_{i=0}^{K-1}_{j=i+1}^{K-1}_{(x,y_{i },y_{j})},\] (5) \[= -_{i=0}^{K-1}_{j=i+1}^{K-1}_{(x,y_{i},y_{j}) }[((y_{i}|x)}{_{ }(y_{i}|x)}-(y_{j}|x)}{_{ }(y_{j}|x)})],\]

where the quality of response \(y_{i}\) is superior to \(y_{j}\).

During MDPO training, we observed that despite the loss decreasing normally, the optimized MLLM sometimes generates certain words or phrases repetitively. This occurs because the probability of the policy model producing both superior and inferior responses simultaneously decreases. While the probability of generating inferior responses declines more rapidly, the policy model's capability to generate superior responses also diminishes, leading to an overall deterioration in performance. To mitigate this risk, we introduce an additional penalty term, modifying Eq. 4 as follows:

\[_{}(_{};_{} )=-_{(x,y_{w},y_{0})}[( (y_{w} x)}{_{}( y_{w} x)}...-(y_{w} x )}{_{}(y_{w} x)}).\] (6) \[. 14.226378pt+(y_{w} x )}{_{}(y_{w} x)}].\]

With this penalty term, the probability of generating superior responses is explicitly improved. To minimize the impact of medium-quality responses, we apply the penalty term exclusively to the best response. Consequently, the learning objective of MDPO is formulated as:

\[_{}(_{};_{})=-[_{j=1 }^{K-1}_{(x,y_{0},y_{j})}+_{i=1}^ {K-1}_{j=i+1}^{K-1}_{(x,y_{i},y_{j})}].\] (7)

## 4 Experiments and Analysis

### Implementation Details

We adpot LLaVA-v1.5  as our base model for all experiments, which is built upon Vicuna [22; 23] and utilizes ViT-L/14  as the image encoder. Our training dataset contains 1k detailed captions from ShareGPT4V , 4k image-text pairs from , 4k human-annotated data from  and 2k multi-round dialogues annotated by us (the annotated process is detailed in Appendix A.2), forming a total of 11k training instances. For training MDPO, we employ the AdamW  optimizer for 4 epochs and apply a peak learning rate of \(5 10^{-5}\) with the cosine decay strategy. To enhance learning efficiency, we incorporate LoRA-based  fine-tuning, with a low-rank r set to 64 for both attention and feed-forward modules. All experiments are conducted with a batch size of 16 on 8 Nvidia A100 GPUs with 40G VRAM. Further implementation details of the Human-free Multi-level Preference Dataset generation are provided in Appendix A.3.

### Evaluation Benchmarks

To verify the effectiveness of our proposed AMP framework, we conduct comprehensive comparisons with various baselines across several benchmarks, including QA-based hallucination benchmark POPE , fine-grained hallucination benchmark MMHal-Bench , general benchmark LLaVA-Bench , and our newly developed multi-round dialogue hallucination benchmark MRHal-Bench. Specifically, POPE assesses the object existence hallucinations by prompting MLLMs to providebinary responses ('yes' or 'no'). MMHal-Bench is designed to quantify hallucinations with the assistance of GPT-4 . Different from the simple questions in conventional benchmarks, MMHal-Bench contains more general, open-ended, and fine-grained questions. LLaVA-Bench serves as a general benchmark for systematic comprehension, encompassing three categories: conversation, detailed description, and complex questions. In addition to these benchmarks, we introduce MRHal-Bench to evaluate hallucinations in multi-round dialogues, covering six aspects: attribute, description, existence, counting, reasoning, and spatial relation. For further details, please refer to Appendix A.4.

### Comparisons with Leading Methods

We compare our method with multiple MLLMs, including two types of state-of-the-art models: 1) General MLLMs. We include LLaVA , InstructBLIP , LLaVA-V1.5 , and Qwen-VL-Chat  as high-performing, open-sourced general models. These models are trained on extensive datasets and demonstrate promising results across various tasks. 2) RLHF-based MLLMs. Our comparisons also extend to RLHF models such as LLaVA-RLHF , RLHF-V , POVID , SILKIE , and FGAIF . Specifically, LLaVA-RLHF employs the PPO algorithm on 10k human-preference data and 72k factually augmented data for reward and policy models, respectively. RLHF-V utilizes 1.4k human-annotated, fine-grained preference data to optimize the policy model using the DPO algorithm. Both  and  apply the DPO algorithm to align MLLMs with GPT-4V preferences. POVID  generates hallucination examples through two strategies and also uses the DPO algorithm.

The quantitative results are shown in Table 1 and 2. We observe that our AMP surpasses all general MLLMs across all benchmarks, highlighting the benefits of further fine-tuning with the preference dataset. Besides, our method also achieves state-of-the-art performance among RLHF-based methods, which comes from two aspects. First, our MDPO algorithm facilitates multi-level preference learning, which enables the MLLM to discern semantic granularity among different responses. Second, the accurate ranking of our human-free preference dataset ensures reliable guidance for the MLLM, leading to more promising performance. We also provide some qualitative case studies in Fig 3. For more cases, please refer to Appendix A.5.

    &  &  &  \\   & Score\(\) & Hal.\(\) & Score (c/m)\(\) & Hal. (c/m)\(\) & Conv.\(\) & Detail\(\) & Comp.\(\) \\  LLaVA\({}_{ 13B}\) & 1.11 & 0.84 & 3.01 / 3.01 & 0.40 / 0.37 & 85.4 & 74.3 & 96.3 \\ InstructBLIP\({}_{ 7B}\) & 1.80 & 0.62 & 3.00 / 3.00 & 0.39 / 0.38 & 83.2 & 67.6 & 90.6 \\ LLaVA-v1.5\({}_{ 7B}\) & 2.01 & 0.61 & 3.38 / 3.39 & 0.32 / 0.29 & 80.2 & 75.9 & 89.2 \\ DeepSEEK-VL  & 2.22 & 0.56 & 3.54 / 3.53 & 0.29 / 0.25 & 74.4 & 76.5 & 78.2 \\ LLaVA-V1.6 \({}_{ 7B}\) & 2.30 & 0.59 & 3.80 / 3.78 & 0.27 / 0.26 & 82.3 & 85.3 & 96.9 \\ MiniCPM-V  & 2.34 & 0.50 & 3.31 / 3.31 & 0.39 / 0.34 & 80.8 & 75.6 & 89.2 \\ LLaVA-v1.5\({}_{ 13B}\) & 2.44 & 0.53 & 3.58 / 3.59 & 0.29 / 0.27 & 81.6 & 75.5 & 95.2 \\ Qwen-VL-Chat  & 2.70 & 0.46 & 3.71 / 3.68 & 0.27 / 0.21 & 81.9 & 77.1 & 92.3 \\ LLaVA-V1.6 \({}_{ 13B}\) & 3.04 & 0.43 & 3.73 / 3.79 & 0.30 / 0.25 & 89.2 & 90.3 & 98.3 \\   LLaVA-RLHF\({}_{ 7B}\) & 2.04 & 0.68 & 3.58 / 3.56 & 0.34 / 0.29 & 85.3 & 74.7 & 105.6 \\ LLaVA-RLHF\({}_{ 13B}\) & 2.53 & 0.57 & 3.26 / 3.27 & 0.45 / 0.38 & 93.8 & 74.3 & **111.4** \\ RLHF-V  & 2.66 & 0.52 & 2.54 / 2.60 & 0.52 / 0.56 & 93.1 & 75.3 & 91.6 \\ POVID  & 2.69 & – & 3.46 / 3.47 & 0.28 / 0.28 & 75.7 & 75.2 & 89.5 \\ SILKIE  & 3.02 & – & 3.71 / 3.70 & 0.30 / 0.29 & 86.3 & 76.4 & 95.3 \\ FGAIF  & 3.09 & 0.36 & 3.77 / 3.79 & 0.30 / 0.31 & **98.2** & **93.6** & 110.0 \\  AMP-MEG\({}_{ 7B}\) & 3.17 & 0.35 & 4.07 / 4.06 & 0.20 / 0.15 & 89.7 & 89.1 & 98.8 \\ AMP-MEG\({}_{ 13B}\) & **3.23** & **0.34** & **4.21 / 4.21** & **0.15 / 0.11** & 94.4 & 91.2 & 95.6 \\ AMP-IG\({}_{ 7B}\) & 3.12 & 0.41 & 4.02 / 4.04 & 0.22 / 0.13 & 90.2 & 85.9 & 99.8 \\ AMP-IG\({}_{ 13B}\) & 3.18 & 0.36 & 3.96 / 4.01 & 0.22 / 0.20 & 91.3 & 86.8 & 99.4 \\   

* Hal.: Hallucination rate, Conv.: Conversation, Detailed: Detailed Description, Comp.: Complex Question, c/m: cumulative/mean.

Table 1: Comparison of conventional MLLMs and RLHF-based MLLMs across MMHal-Bench, MRHal-Bench, and LLaVA-Bench. “MEG” represents training data generated via Multi-size Expert Generation, while “IG” indicates training data produced using Incremental Generation.

### Ablation Studies

**Impact of Preference Quantity.** We explore the effects of varying the number of preferences from 2 to 5, with detailed implementation found in Appendix A.6. As indicated in Table 3, a 4-level preference is identified as the optimal setting. We hypothesize that the diminished performance observed with a 5-level preference dataset may be due to increased hidden noise. Unless stated otherwise, all subsequent experiments are conducted on an MLLM using the Vicuna-7B, trained on the 4-level preference dataset produced through Multi-size Expert Generation.

    &  &  &  &  \\   & F1\(\) & Acc.\(\) & F1\(\) & Acc.\(\) & F1\(\) & Acc.\(\) & F1\(\) & Yes \\  LLAVA\({}_{13}\) & 74.4 & 67.2 & 78.2 & 73.6 & 78.8 & 73.7 & 77.1 & 73.7 \\ InstructBLIP\({}_{}\) & 70.4 & 65.2 & 80.2 & 79.7 & 89.3 & 88.6 & 80.0 & 59.0 \\ DeepSeek-VL  & 72.2 & 65.4 & 71.3 & 63.8 & 76.4 & 71.5 & 71.7 & 73.3 \\ Owen-VL-Chat  & 80.7 & 83.2 & 81.6 & 84.2 & 82.1 & 84.2 & 81.5 & 36.7 \\ MiniCPM-V  & 83.5 & 83.4 & 86.2 & 86.5 & 88.9 & 89.2 & 86.2 & 47.8 \\ LLAVA-V1.5\({}_{}\) & 84.5 & 85.5 & 86.0 & 87.1 & 87.2 & 88.0 & 85.9 & 42.2 \\ LLAVA-V1.5\({}_{}\) & 84.5 & 85.5 & 86.3 & 87.4 & 87.1 & 88.0 & 86.0 & 42.2 \\ LLAVA-V1.6\({}_{}\) & **85.2** & **86.4** & 86.4 & 87.6 & 87.6 & 88.5 & 86.4 & 41.5 \\ LLAVA-V1.6\({}_{}\) & **85.2** & **86.4** & 86.4 & 87.7 & 87.2 & 88.2 & 86.3 & 41.0 \\   LLAVA-RLHF\({}_{}\) & 79.5 & 80.7 & 81.8 & 83.3 & 83.3 & 84.8 & 81.5 & 41.8 \\ LLAVA-RLHF\({}_{}\) & 80.5 & 82.3 & 81.8 & 83.9 & 83.5 & 85.2 & 81.9 & 39.0 \\ RLHF-V  & 83.6 & 84.6 & 85.3 & 86.4 & 87.2 & 88.1 & 85.4 & 42.7 \\ POVID\({}^{*}\) & 84.0 & 84.7 & 85.8 & 86.8 & 87.7 & 88.5 & 85.8 & 43.6 \\ SILKIE  & 80.3 & 83.0 & 81.3 & 84.0 & 81.6 & 83.9 & 81.1 & 36.1 \\ FGAIF  & 79.9 & 79.6 & 83.7 & 84.0 & 86.7 & 87.0 & 83.4 & 48.3 \\  AMP-MEG\({}_{}\) & 83.4 & 83.1 & 87.7 & **88.2** & 89.6 & 89.9 & 86.9 & 48.0 \\ AMP-MEG\({}_{}\) & 83.4 & 82.8 & **88.0** & **88.2** & **90.3** & **90.4** & **87.2** & 49.8 \\ AMP-IG\({}_{}\) & 82.3 & 82.5 & 87.0 & 87.8 & 87.7 & 88.3 & 85.7 & 45.1 \\ AMP-IG\({}_{}\) & 83.0 & 82.7 & 86.0 & 86.3 & 89.6 & 90.0 & 86.2 & 46.3 \\   

Table 2: Comparisons on the POPE benchmark. \({}^{*}\) indicates evaluations using the official model.

Figure 3: Case studies including our AMP-MEG, LLAVA-V1.5 , and LLAVA-RLHF . Hallucinations, correct responses are highlighted in different colors. Please zoom in for the best view.

**Impact of Gap between Adjacent Levels.** The effectiveness of multi-level preference learning is partly attributed to reducing the gaps between adjacent levels. As shown in Table 4, we reduce the gap by "S>B \(\)S>A" and "A>C \(\)B>C", both of which result in performance enhancements.

**Impact of Including More Comparisons.** We further introduce more inferior responses, _i.e_., 'Response B' and 'Response C', by "S>A \(\) S>A & S>B" and "S>A & S>B \(\) S>A & A>B & B>C". The improvements depicted in Table 4 verify that inferior responses are also beneficial for preference learning. To provide more comparisons between the best response and hallucination examples, we devise cross-level comparisons based on settings "S>A & S>B" and "S>A & A>B & B>C". As illustrated in Table 4, this strategy brings extra performance improvement across multiple benchmarks, indicating the necessity of cross-level comparisons.

**Comparisons with AI-Annotated Preference.** Similar to reinforcement learning methods using AI feedback , we use GPT-4V  to directly rank the responses generated by MEG based on their visual faithfulness and helpfulness. Table 5 illustrates that training with our preference dataset yields more effective results compared to the AI-annotated preference dataset. This suggests that our human-free multi-level preference dataset contains less noise. Furthermore, the performance of the MLLM significantly decreases in the absence of our Auto-check mechanism, highlighting its crucial role in accurately refining the ranking of the multi-level preference dataset.

### Comparisons with other Rank-based Preference Alignment Approaches.

We make some empirical comparisons by replacing our MDPO with the learning objectives of  and . As reported in Table 6, our MDPO surpasses these two learning objectives on all hallucination benchmarks, \(e.g.\), 3.01 \(\) 3.17 on MMHal-Bench. The superiority of our MDPO comes from two aspects. First, our MDPO mitigates the challenge of distinguishing micro hallucinations in responses. Taking 3-level preference as an example, the comparisons of other methods are 'A>BC, B>C', while comparisons made by our AMP are 'A>B, A>C, B>C'. More specifically, our AMP splits 'A>BC' into 'A>B, A>C', which enables MLLMs to perceive the subtle differences between different responses. Second, our penalty term explicitly increases the probability of MLLMs generating good answers, ensuring the stability of the training process.

We also conduct some experiments about perturbation-based (PB) methods. Our implementation details are as follows. We randomly change the noun, adjective, preposition, and numeral. We obtain

    &  &  &  \\   & Score\(\) & Hal.\(\) & Score (c/m)\(\) & Hal.\(\) & Conv.\(\) & Detail\(\) & Comp.\(\) \\  SB & 2.50 & 0.50 & 3.56 / 3.57 & 0.28 / 0.28 & 79.7 & 71.5 & 80.3 \\ SA & 2.61 & 0.51 & 3.63 / 3.62 & 0.27 / 0.25 & 82.8 & 74.9 & 84.1 \\ SA \& AB & 2.68 & 0.43 & 3.69 / 3.71 & 0.29 / 0.22 & 83.0 & 79.6 & 87.0 \\  & 2.79 & 0.44 & 3.75 / 3.73 & 0.27 / 0.22 & 87.7 & 78.6 & 90.1 \\ SA \& AB \& BC & 2.85 & 0.40 & 3.86 / 3.90 & 0.24 / 0.17 & **90.2** & 81.3 & 92.4 \\  & **3.17** & **0.35** & **4.07** / **4.06** & **0.20** / **0.15** & 89.7 & **89.1** & **98.8** \\  AC & 2.33 & 0.57 & 3.37 / 3.37 & 0.34 / 0.34 & 75.7 & 70.4 & 80.1 \\ BC & 2.45 & 0.51 & 3.50 / 3.50 & 0.31 / 0.27 & 77.3 & 72.2 & 81.6 \\   

Table 4: Impact of the gap between adjacent levels and cross-level comparison. Preferences are ranked from most superior to most inferior in the following order: S, A, B, C.

    & Preference & MMHal-Bench &  &  \\  & Annotation & Score\(\) & Hal.\(\) & Score (c/m)\(\) & Hal.\(\) & Conv.\(\) & Detail\(\) & Comp.\(\) \\  AMP-MEG & AI & 2.87 & 0.44 & 3.87 / 3.88 & 0.25 / 0.21 & 92.3 & 79.6 & 89.9 \\ AMP-MEG & Auto Check & **3.17** & **0.35** & **4.07** / **4.06** & **0.20** / 0.15 & 89.7 & **89.1** & 98.8 \\ AMP-IG & Auto Check & 3.12 & 0.41 & 4.02 / 4.04 & 0.22 / **0.13** & **90.2** & 85.9 & **99.8** \\  AMP-MEG & Initial & 2.79 & 0.49 & 3.69 / 3.71 & 0.29 / 0.22 & 87.1 & 80.1 & 81.7 \\ AMP-IG & Initial & 2.80 & 0.48 & 3.75 / 3.77 & 0.26 / 0.21 & 89.2 & 78.3 & 81.9 \\   

Table 5: Ablations on the human-free multi-level preference dataset using different annotations, including AI (_i.e_., GPT-4V), Auto-check, and initial annotations from MEG and IG.

answers of varying quality by controlling the proportion of perturbations (10%, 30%, 50% based on 4-level preference setting). As shown in Table 6, perturbation-based methods is still inferior to our MDPO. We infer the hallucination pattern generated by random perturbation is different from the real MLLM and is thus not informative enough for preference learning.

### Evaluation of the Automated Multi-level Preference Dataset

We estimate the inconsistency rate of our AMP dataset to be 2.25% (through manual evaluation on 2000 random samples). The 2.25% inconsistency rate is significantly lower than the human (14.40% inconsistency) and GPT-4V (11.95% inconsistency) annotations.

Moreover, we conduct another experiment to validate the superiority of our AMP dataset. We mix the AMP and human/GPT-4V data for training the model. Fig. 4 shows that as the proportion of human/GPT-4V annotated data increases, the performance of MLLMs decreases accordingly.

## 5 Limitations

Our AMP framework offers more effective preference learning from the human-free multi-level preference dataset. However, several challenges remain: 1) The quality of standard responses limits the performance of the optimized MLLM. A portion of the standard responses in our dataset comes from superior responses generated by language models, potentially containing imperceptible hallucinations. Besides, the standard response is less helpful despite its high faithfulness, further restricting the performance. 2) Although our AMP successfully reduces hallucinations and promotes the truthfulness of MLLMs, the essence of preference learning is pushing the model to bias the preference dataset, causing a decrease in generalization ability. Therefore, finding a balance between preference learning and maintaining the capabilities of MLLMs is yet to be explored.

## 6 Conclusions

In this paper, we introduce the Automated Multi-level Preference (AMP) framework, achieving promising performance on several hallucination benchmarks, which benefits from the reduction of gaps in adjacent levels and the introduction of cross-level comparison. To enable the AMP framework, we propose a multi-level preference dataset generation pipeline, aiming to construct a high-quality preference dataset automatically. Furthermore, we design the Multi-level Direct Preference Optimization algorithm, which furnishes a novel learning objective to ensure robust and efficient preference learning. Lastly, we conduct the first hallucination benchmark in multi-round dialogues and devise the relevant metrics, which may stimulate future research.

    &  &  &  \\   & Score\(\) & Hal.\(\) & Score\(\) & Hal.\(\) & Conv.\(\) & Detail\(\) & Comp.\(\) \\  r.  & 2.96 & 0.41 & 3.85 / 3.82 & 0.26 / 0.23 & 84.1 & 81.7 & 88.2 \\ r.  & 3.01 & 0.38 & 3.95 / 3.91 & 0.24 / 0.19 & 86.2 & 84.3 & 91.9 \\ PB & 2.83 & 0.46 & 3.61 / 3.52 & 0.33 / 0.35 & 78.4 & 75.1 & 81.3 \\  MDPO & **3.17** & **0.35** & **4.07** / **4.06** & **0.20** / **0.15** & **89.7** & **89.1** & **98.8** \\   

Table 6: Performance on three hallucination benchmarks across other loss functions (#1, #2), MLLMs from different families (#3, DF), perturbation-based methods (#4, PB).

Figure 4: Performance on three hallucination benchmarks across different proportions of GPT-4V/human annotations.