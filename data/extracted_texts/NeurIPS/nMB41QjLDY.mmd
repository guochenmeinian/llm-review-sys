# Provably Efficient Algorithm for Nonstationary Low-Rank MDPs

Yuan Cheng

National University of Singapore

yuan.cheng@u.nus.edu

&Jing Yang

The Pennsylvania State University

yangjing@psu.edu

&Yingbin Liang

The Ohio State University

liang.889@osu.edu

###### Abstract

Reinforcement learning (RL) under changing environment models many real-world applications via nonstationary Markov Decision Processes (MDPs), and hence gains considerable interest. However, theoretical studies on nonstationary MDPs in the literature have mainly focused on tabular and linear (mixture) MDPs, which do not capture the nature of unknown representation in deep RL. In this paper, we make the first effort to investigate nonstationary RL under episodic low-rank MDPs, where both transition kernels and rewards may vary over time, and the low-rank model contains unknown representation in addition to the linear state embedding function. We first propose a parameter-dependent policy optimization algorithm called PORTAL, and further improve PORTAL to its parameter-free version of Ada-PORTAL, which is able to tune its hyper-parameters adaptively without any prior knowledge of nonstationarity. For both algorithms, we provide upper bounds on the average dynamic suboptimality gap, which show that as long as the nonstationarity is not significantly large, PORTAL and Ada-PORTAL are sample-efficient and can achieve arbitrarily small average dynamic suboptimality gap with polynomial sample complexity.

## 1 Introduction

Reinforcement learning (RL) has gained significant success in real-world applications such as board games of Go and chess (Silver et al., 2016, 2017, 2018), robotics (Levine et al., 2016; Gu et al., 2017), recommendation systems (Zhao et al., 2021) and autonomous driving (Bojarski et al., 2016; Ma et al., 2021). Most theoretical studies on RL have been focused on a stationary environment and evaluated the performance of an algorithm by comparing against only one best fixed policy (i.e., _static regret_). However, in practice, the environment is typically time-varying and _nonstationary_. As a result, the transition dynamics, rewards and consequently the optimal policy change over time.

There has been a line of research studies that investigated _nonstationary_ RL. Specifically, Gajane et al. (2018); Cheung et al. (2020); Mao et al. (2021) studied nonstationary tabular MDPs. To further overcome the curse of dimensionality, Fei et al. (2020); Zhou et al. (2020) proposed algorithms for nonstationary linear (mixture) MDPs and established upper bounds on the dynamic regret.

In this paper, we significantly advance this line of research by investigating nonstationary RL under _low-rank_ MDPs (Agarwal et al., 2020), where the transition kernel of each MDP admits a decomposition into a representation function and a state-embedding function that map to low dimensional spaces. Compared with linear MDPs where the representation is known, the low-rank MDP model contains unknown representation, and is hence much more powerful to capturerepresentation learning that occurs often in deep RL. Although there have been several recent studies on static low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022; Modi et al., 2021), nonstationary low-rank MDPs remain unexplored, and are the focus of this paper.

To investigate nonstationary low-rank MDPs, several challenges arise. (a) All previous studies of nonstationary MDPs took on-policy exploration, such a strategy will have difficulty in providing sufficiently accurate model (as well as representation) learning for nonstationary low-rank MDPs. (b) Under low-rank MDPs, since both representation and state-embedding function change over time, it is more challenging to use history data collected under previous transition kernels for current use.

The **main contribution** of this paper lies in addressing above challenges and designing a provably efficient algorithm for nonstationary low-rank MDPs. We summarize our contributions as follows.

* We propose a novel policy optimization algorithm with representation learning called PORTAL for nonstationary low-rank MDPs. PORTAL features new components, including off-policy exploration, data-transfer model learning, and target policy update with periodic restart.
* We theoretically characterize the average dynamic suboptimality gap (\(}\)) of PORTAL, where \(}\) serves as a new metric that captures the performance of target policies with respect to the best policies at each instance in the nonstationary MDPs under off-policy exploration. We further show that with prior knowledge on the degree of nonstationarity, PORTAL can select hyper-parameters that minimize \(}\). If the nonstationarity is not significantly large, PORTAL enjoys a diminishing \(}\) with respect to the number of iterations \(K\), indicating that PORTAL can achieve arbitrarily small \(}\) with polynomial sample complexity. Our analysis features a few new developments. (a) We provide a new MLE guarantee under nonstationary transition kernels that captures errors of using history data collected under different transition kernels for benefiting current model estimation. (b) We establish trajectory-wise uncertainty bound for estimation errors via a square-root \(_{}\)-norm of variation budgets. (c) We develop an error tracking technique via auxiliary anchor representation for convergence analysis.
* Finally, we improve PORTAL to a _parameter-free_ algorithm called Ada-PORTAL, which does not require prior knowledge on nonstationarity and is able to tune the hyper-parameters adaptively. We further characterize \(}\) of Ada-PORTAL as \((K^{-}(+1)^{})\), where \(\) captures the variation of the environment. Notably, based on PORTAL, we can also use the black-box method called MASTER in Wei and Luo (2021) to turn PORTAL into a parameter-free algorithm (called MASTER+PORTAL) with \(}\) of \((K^{-}^{})\). Clearly, Ada-PORTAL performs better than MASTER+PORTAL when nonstationarity is not significantly small, i.e. \((1)\).

To our best knowledge, this is the first study of nonstationary RL under low-rank MDPs.

## 2 Related Works

Various works have studied nonstationary RL under tabular and linear MDPs, most of which can be divided into two lines: policy optimization methods and value-based methods.

**Nonstationary RL: Policy Optimization Methods.** As a vast body of existing literature (Cai et al., 2020; Shani et al., 2020; Agarwal et al., 2020; Xu et al., 2021) has proposed policy optimization methods attaining computational efficiency and sample efficiency simultaneously in _stationary_ RL under various scenarios, only several papers investigated policy optimization algorithm in _nonstationary_ environment. Assuming time-varying rewards and time-invariant transition kernels, Fei et al. (2020) studied nonstationary RL under tabular MDPs. Zhong et al. (2021) assumed both transition kernels and rewards change over episodes and studied nonstationary linear mixture MDPs. These policy optimization methods all assumed prior knowledge on nonstationarity.

**Nonstationary RL: Value-based Methods.** Assuming that both transition kernels and rewards are time-varying, several works have studied nonstationary RL under tabular and linear MDPs, most of which adopted Upper Confidence Bound (UCB) based algrithms. Cheung et al. (2020) investigated tabular MDPs with infinite-horizon and proposed algorithms with both known variation budgets and unknown variation budgets. In addition, this work also proposed a Bandit-over-Reinforcement Learning (BORL) technique to deal with unknown variation budgets. Mao et al. (2021) proposed a model-free algorithm with sublinear dynamic regret bound. They then proved a lower bound for nonstationary tabular MDPs and showed that their regret bound is near min-max optimal. Touati& Vincent (2020); Zhou et al. (2020) considered nonstationary RL in linear MDPs and proposed algorithms achieving sublinear regret bounds with unknown variation budgets.

Besides these two lines of researches, Wei & Luo (2021) proposed a black-box method that turns a RL algorithm with optimal regret in a (near-)stationary environment into another algorithm that can work in a nonstationary environment with sublinear dynamic regret without prior knowledge on nonstationarity. In this paper, we show that our algorithm Ada-PORTAL outperforms such a type of black-box method (taking PORTAL as subroutine) if nonstationarity is not significantly small.

**Stationary RL under Low-rank MDPs.** Low-rank MDPs were first studied by Agarwal et al. (2020b) in a reward-free regime, and then Uehara et al. (2022) studied low-rank MDPs for both online and offline RL with known rewards. Cheng et al. (2023) studied reward-free RL under low-rank MDPs and improved the sample complexity of previous works. Modi et al. (2021) proposed a model-free algorithm MOFFLE under low-nonnegative-rank MDPs. Cheng et al. (2022); Agarwal et al. (2022) studied multitask representation learning under low-rank MDPs, and further showed the benefit of representation learning to downstream RL tasks.

## 3 Formulation

**Notations:** We use \([K]\) to denote set \(\{1,,K\}\) for any \(K\), use \(\|x\|_{2}\) to denote the \(_{2}\) norm of vector \(x\), use \(()\) to denote the probability simplex over set \(\), use \(()\) to denote uniform sampling over \(\), given \(||<\), and use \(()\) to denote the set of all possible density distributions over set \(\). Furthermore, for any symmetric positive definite matrix \(\), we let \(\|x\|_{}:= x}\). For distributions \(p_{1}\) and \(p_{2}\), we use \(D_{KL}(p_{1}()\|p_{2}())\) to denote the KL divergence between \(p_{1}\) and \(p_{2}\).

### Episodic MDPs and Low-rank Approximation

An episodic MDP is denoted by a tuple \(:=,,H,P:=\{P_{h}\}_{h=1}^{H},r:=\{r_{h }\}_{h=1}^{H}\), where \(\) is a possibly infinite state space, \(\) is a finite action space with cardinality \(A\), \(H\) is the time horizon of each episode, \(P_{h}(|,):( )\) denotes the transition kernel at each step \(h\), and \(r_{h}(,):\) denotes the deterministic reward function at each step \(h\). We further normalize the reward as \(_{h=1}^{H}r_{h} 1\). A policy \(=\{_{h}\}_{h[H]}\) is a set of mappings where \(_{h}:()\). For any \((s,a)\), \(_{h}(a|s)\) denotes the probability of selecting action \(a\) at state \(s\) at step \(h\). For any \((s,a)\), let \((s_{h},a_{h})(P,)\) denote that the state \(s_{h}\) is sampled by executing policy \(\) to step \(h\) under transition kernel \(P\) and then action \(a_{h}\) is sampled by \(_{h}(|s_{h})\).

Given any state \(s\), the value function for a policy \(\) at step \(h\) under an MDP \(\) is defined as the expected value of the accumulative rewards as: \(V_{h,P,r}^{}(s)=_{h^{}=h}^{H}_{(s_{h^{}},a_{h^{ }})(P,)}[r_{h^{}}(s_{h^{}},a_{h^{}})|s_{h}=s]\). Similarly, given any state-action pair \((s,a)\), the action-value function (\(Q\)-function) for a policy \(\) at step \(h\) under an MDP \(\) is defined as \(Q_{h,P,r}^{}(s,a)=r_{h}(s,a)+_{h^{}=h+1}^{H}_{(s_{h^{ }},a_{h^{}})(P,)}[r_{h^{}}(s_{h^{}},a_{h^{ }})|s_{h}=s,a_{h}=a]\). Denote \((P_{h}f)(s,a):=_{s^{} P_{h}(|s,a)}[f(s^{})]\) for any function \(f:\). Then we can write the action-value function as \(Q_{h,P,r}^{}(s,a)=r_{h}(s,a)+(P_{h}V_{h+1,P,r}^{})(s,a)\). For any \(k[K]\), without loss of generality, we assume the initial state \(s_{1}\) to be fixed and identical, and we use \(V_{P,r}^{}\) to denote \(V_{1,P,r}^{}(s_{1})\) for simplicity.

This paper focuses on low-rank MDPs (Jiang et al., 2017; Agarwal et al., 2020b) defined as follows.

**Definition 3.1** (Low-rank MDPs).: A transition kernel \(P_{h}^{*}:()\) admits a low-rank decomposition with dimension \(d\) if there exist a representation function \(_{h}^{*}:^{d}\) and a state-embedding function \(_{h}^{*}:^{d}\) such that

\[P_{h}^{*}(s^{}|s,a)=_{h}^{*}(s,a),_{h}^{*}(s^{} ), s,s^{},a.\]

Without loss of generality, we assume \(\|_{h}^{*}(s,a)\|_{2} 1\) for all \((s,a)\) and for any function \(g:\), \(\|f\|\,_{h}^{*}(s)g(s)ds\|_{2}\). An MDP is a low-rank MDP with dimension \(d\) if for any \(h[H]\), its transition kernel \(P_{h}^{*}\) admits a low-rank decomposition with dimension \(d\). Let \(^{*}=\{_{h}^{*}\}_{h[H]}\) and \(^{*}=\{_{h}^{*}\}_{h[H]}\) be the true representation and state-embedding functions.

### Nonstationary Transition Kernels with Adversarial Rewards

In this paper, we consider an episodic RL setting under changing environment, where both transition kernels and rewards vary over time and possibly in an adversarial fashion.

Specifically, suppose the RL system goes by _rounds_, where each round have a fixed number of episodes, and the transition kernel and the reward remain the same in each round, and can change adversarially across rounds. For each round, say round \(k\), we denote the MDP as \(^{k}=(,,H,P^{k}:=\{P^{,k}_{h}\}_{h=1}^{H},r^{k}:=\{r^{k}_{h}\}_{h=1}^{H})\), where \(P^{,k}\) and \(r^{k}\) denote the true transition kernel and the reward of round \(k\). Further, \(P^{,k}\) takes the low-rank decomposition as \(P^{,k}=^{,k},^{,k}\). Both the representation function \(^{,k}\) and the state embedding function \(^{,k}\) can change across rounds. Given the reward function \(r^{k}\), there always exists an optimal policy \(^{,k}\) that yields the optimal value function \(V^{^{,k}}_{P^{,k},r^{k}}=_{}V^{}_{P^{,k},r^{k}}\), abbreviated as \(V^{}_{P^{,k},r^{k}}\). Clearly, the optimal policy also changes across rounds.

We assume the agent interacts with the nonstationary environment (i.e., the time-varying MDPs) over \(K\) rounds in total without the knowledge of transition kernels \(\{P^{k,}\}_{k=1}^{K}\). At the beginning of each round \(k\), the environment changes to a possibly adversarial transition kernel unknown to the agent, picks a reward function \(r^{k}\), which is revealed to the agent only at the end of round \(k\), and outputs a fixed initial state \(s_{1}\) for the agent to start the exploration of the environment for each episode. The agent is allowed to interact with MDPs via a few episodes with one or multiple _exploration_ policies at her choice to take samples from the environment and then should output an target policy to be executed during the next round. Note that in our setting, the agent needs to decide exploration and target policies only based on the information in previous rounds, and hence exploration samples and the reward information of the current round help only towards future rounds.

### Learning Goal and Evaluation Metric

In our setting, the agent seeks to find the optimal policy at each round \(k\) (with only the information of previous rounds), where both transition kernels and rewards can change over rounds. Hence we define the following notion of _average dynamic suboptimality gap_ to measure the convergence of the target policy series to the optimal policy series.

**Definition 3.2** (Average Dynamic Suboptimality Gap).: For \(K\) rounds, and any policy set \(\{^{k}\}_{k[K]}\), the average dynamic suboptimality gap \((})\) of the value functions over \(K\) rounds is given as \(}(K)=_{k=1}^{K}[V^{}_{P^{,k},r^{k}}-V^{ ^{k}}_{P^{,k},r^{k}}]\). For any \(\), we say an algorithm is \(\)-average suboptimal, if it outputs a policy set \(\{^{k}\}_{k[K]}\) satisfying \(}(K)\).

\(}\) compares the agent's target policy to the optimal policy of each individual round in hindsight, which captures the dynamic nature of the environment. This is in stark contrast to the stationary setting where the comparison policy is a single fixed best policy over all rounds. This notion is similar to _dynamic regret_ used for nonstationary RL (Fei et al., 2020; Gajane et al., 2018), where the only difference is that \(}\) evaluates the performance of target policies rather than the exploration policies. Hence, given any target accuracy \( 0\), the agent is further interested in the statistical efficiency of the algorithm, i.e., using as few trajectories as possible to achieve \(\)-average suboptimal.

## 4 Policy Optimization Algorithm and Theoretical Guarantee

### Base Algorithm: PORTAL

We propose a novel algorithm called PORTAL (Algorithm 1), which features three main steps. Below we first summarize our main design ideas and then explain reasons behind these ideas as we further elaborate main steps of PORTAL.

**Summary of New Design Ideas:** PORTAL features the following main design ideas beyond previous studies on nonstationary RL under tabular and linear MDPs. (a) PORTAL features a specially designed _off-policy_ exploration which turns out to be beneficial for nonstationary low-rank /MDP models rather than the typical _on-policy_ exploration taken by previous studies of nonstationary tabular and linear MDP models. (b) PORTAL transfers history data collected under various different transition kernels for benefiting the estimation of the current model. (c) PORTAL updates targetpolicies with periodic restart. As a comparison, previous work using periodic restart (Fei et al., 2020) chooses the restart period \(\) based on a certain smooth visitation assumption. Here, we remove such an assumption and hence our choice of \(\) is applicable to more general model classes.

**Step 1. Off-Policy Exploration for Data Collection:** We take _off-policy_ exploration, which is beneficial for nonstationary low-rank MDPs than simply using the target policy for _on-policy_ exploration taken by the previous studies on nonstationary tabular or linear (mixture) MDPs (Zhong et al., 2021; Fei et al., 2020; Zhou et al., 2020). To further explain, we first note that under tabular or linear (mixture) MDPs studied in the previous work, a bonus term is introduced to the actual reward to serve as a _point-wise_ uncertainty level of the estimation error for each state-action pair at any step \(h\), so that for any step \(h\), \(_{h}^{k}\) is a good optimistic estimation for \(Q_{h,P^{*,k},r^{k}}^{}\). Hence it suffices to collect samples using the target policy. However, in low-rank MDPs, the bonus term \(_{h}^{k}\) cannot serve as a point-wise uncertainty measure. For step \(h 2\), \(_{h}^{k}\) is not a good optimistic estimation for the true value function if the agent only uses target policy to collect data (i.e., for on-policy exploration). Hence, more samples and a novel off-policy exploration are required for a good estimation under low-rank MDPs. Specifically, as line 5 in Algorithm 1, at the beginning of each round \(k\), for each step \(h[H]\), the agent explores the environment by executing the exploration policy \(^{k-1}\) to state \(}_{h-1}^{k,h}\) and then taking two uniformly chosen actions1, where \(^{k-1}\) is determined in Step 2 of the previous round.

```
1:Input: Rounds \(K\), hyper-parameters \(,W\), regularizer \(_{k,W}\), coefficient \(_{k,W}\), stepsize \(\) and models \(\{,\}\).
2:Initialization:\(_{0}(|s)\) to be uniform; \(}_{h}^{(0,0)}=\).
3:for episode \(k=1,,K\)do
4:for step \(h=1,,H\)do
5: Roll into \(_{h-1}^{(k,h)}\) using \(^{k-1}\), uniformly choose \(_{h-1}^{(k,h)},_{h}^{(k,h)}\), and enter into \(_{h}^{(k,h)}\), \(_{h+1}^{(k,h)}\).
6: Update datasets \[}_{h-1}^{(k,h,W)} =\{_{h-1}^{(i,h)},_{h-1}^{(i,h)}, _{h}^{(i,h)}\}_{i=1 k-W+1}^{k},\] \[}_{h}^{(k,h,W)} =\{_{h}^{(i,h)},_{h}^{(i,h)},_{h+ 1}^{(i,h)}\}_{i=1 k-W+1}^{k}.\]
7:endfor
8: Receive full information rewards \(r^{k}=\{r_{h}^{k}\}_{h[H]}\).
9: Estimate transition kernel and update the exploration policy \(^{k}\) for the next round via: \[^{2}(k,\{}_{h-1}^{(k,h,W)}\},\{ }_{h}^{(k,h,W)}\}).\]
10:for step \(h=1,,H\)do
11: Update \(_{h}^{k}=Q_{h,^{k},r^{k}}^{^{k}}\).
12:endfor
13:if\(k=1\)then
14: Set \(\{_{h}^{k}\}_{h[H]}\) as zero functions and \(\{_{h}^{k}\}_{h[H]}\) as uniform distributions on \(\).
15:endif
16:for step \(h=1,,H\)do
17: Update the target policy as in Equation (2).
18:endfor
19:endfor
20:Output:\(\{^{k}\}_{k=1}^{K}\). ```

**Algorithm 1** **PORTA** (Policy **O**ptimization with **R**epresen**TA**tion **L**earning under nonstationary MDPs)

**Step 2. Data-Transfer Model Learning and E\({}^{2}\)U:** In this step, we transfer history data collected under previous different transition kernels for benefiting the estimation of the current model. This is theoretically grounded by our result that the model estimation error can be decomposed into variation budgets plus a diminishing term as the estimation sample size increases, which justifies that the usage of data generated by mismatched distributions within a certain window is beneficial for model learning as long as variation budgets is mild. Then, the estimated model will further facilitate the selection of future exploration policies accurately.

Specifically, the agent selects desirable samples only from the latest \(W\) rounds following a _forgetting rule_(Garivier and Moulines, 2011). Since nonstationary low-rank MDPs (compared to tabular and linear MDPs) also have additional variations on representations over time, the choice of \(W\) needs to incorporate such additional information. Then the agent passes these selected samples to a subroutine \(^{2}\) (see Algorithm 2), in which the agent estimates the transition kernels via the maximum likelihood estimation (MLE). Next, the agent updates the empirical covariance matrix \(^{k,W}\) and exploration-driven bonus \(^{k}\) as in lines 4 and 5 in Algorithm 2. We then define a _truncated value function_ iteratively using the estimated transition kernel and the exploration-driven reward as follows:

\[^{}_{h,^{k},^{k}}(s_{h},a_{h})=\{ 1,^{k}_{h}(s_{h},a_{h})+^{k}_{h}^{}_{h+1,^{k}, ^{k}}(s_{h},a_{h})\},\] \[^{}_{h,^{k},^{k}}(s_{h})=_{} [^{}_{h,^{k},^{k}}(s_{h},a_{h})].\] (1)

Although the bonus term \(^{k}_{h}\) cannot serve as a _point-wise_ uncertainty measure, the truncated value function \(^{}_{^{k},^{k}}\) as the cumulative version of \(^{k}_{h}\) can serve as a _trajectory-wise_ uncertainty measure, which can be used to determine future exploration policies. Intuitively, for any policy \(\), the model estimation error satisfies \(_{(s,a)(P^{,k},)}[\|^{k}(|s,a)-P^{,k}( |s,a)\|_{TV}]^{}_{^{k},^{k}}+\), where the error term \(\) captures the variation of both transition kernels and representations over time. As a result, by selecting the policy that maximizes \(^{}_{^{k},^{k}}\) as the exploration policy as in line 7, the agent will explore the trajectories whose states and actions have not been estimated sufficiently well so far.

```
1:Input: round index \(k\), regularizer \(_{k,W}\) and coefficient \(_{k,W}\), datasets \(\{}^{(k,h)}_{h-1}\}\),\(\{}^{(k,h)}_{h}\}\) and models \(\{,\}\).
2:for step \(h=1,,H\)do
3: Learn the representation via MLE for step \(h\): \[^{k}_{h}=(^{k}_{h},^{k}_{h})=_{, }_{}^{(k,h)}_{h}}[(s_{ h},a_{h}),(s_{h+1})].\]
4: Compute the empirical covariance matrix as \[^{k,W}_{h}=_{}^{(k,h+1)}_{h}}^{k}_{h}(s_{ h},a_{h})^{k}_{h}(s_{h},a_{h})^{}+_{k,W}I\]
5: Define exploration-driven bonus \(^{k}_{h}(,)=\{_{k,W}\|^{k}_{h}(, )\|_{(^{k,W}_{h})^{-1}},1\}\).
6:endfor
7: Find exploration policy \(^{k}=_{}^{}_{^{k},^{k}}\), where \(^{}_{^{k},^{k}}\) is defined as in Equation (1).
8:Output: Model \(^{k}\) and exploration policy \(\{^{k}\}\). ```

**Algorithm 2****\(^{2}\)** (Model **E**stimation and **E**xploration Policy **U**pdate)

**Step 3: Target Policy Update with Periodic Restart:** The agent evaluates the target policy by computing the value function under the target policy and the estimated transition kernel. Then, due to the nonstationarity, the target policy is reset every \(\) rounds. Compared with the previous work using periodic restart (Fei et al., 2020), whose choice of \(\) is based on a certain smooth visitation assumption, we remove such an assumption and hence our choice of \(\) is applicable to more general model classes. Finally, the agent uses the estimated value function for target policy update for the next round \(k+1\) via online mirror descend. The update step is inspired by the previous works (Cai et al., 2020; Schulman et al., 2017). Specifically, for any given policy \(^{0}\) and MDP \(\), define the following function w.r.t. policy \(\) :

\[L^{,_{0}}()=V^{^{0}}_{P,r}+_{h=1}^{H}_{s_{h} (P,^{0})}[ Q^{^{0}}_{h,P,r},_{h}(|s_{h})- ^{0}_{h}(|s_{h})].\]\(L^{,_{0}}()\) can be regarded as a local linear approximation of \(V^{}_{P,r}\) at "point" \(^{0}\)(Schulman et al., 2017). Consider the following optimization problem:

\[^{k+1}=_{}L^{^{k},^{k}}()-_{h [H]}_{s_{h}(P^{*,h},^{k})}[D_{KL}(_{h}(|s_{h}) \|^{k}_{h}(|s_{h}))].\]

This can be regarded as a mirror descent step with KL divergence, where the KL divergence regularizes \(\) to be close to \(^{k}\). It further admits a closed-form solution: \(^{k+1}_{h}(|)^{k}_{h}(|)\{ Q ^{^{k}}_{h,P^{*,k},^{k}}(,)\}\). We use the estimated version \(^{k}_{h}\) to approximate \(Q^{^{k}}_{h,P^{*,k},^{k}}\) and get

\[^{k+1}_{h}(|)^{k}_{h}(|)\{ ^{k}_{h}(,)\}.\] (2)

### Technical Assumptions

Our analysis adopts the following standard assumptions on low-rank MDPs.

**Assumption 4.1**.: (Realizability). A learning agent can access to a model class \(\{(,)\}\) that contains the true model, namely, for any \(h[H],k[K]\), \(^{*,k}_{h},^{*,k}_{h}\).

While we assume cardinality of the model class to be finite for simplicity, extensions to infinite classes with bounded statistical complexity are not difficult (Sun et al., 2019).

**Assumption 4.2** (Bounded Density).: Any model induced by \(\) and \(\) has bounded density, i.e. \( P=,,,\), there exists a constant \(B 0\) such that \(_{(s,a,s^{})}P(s^{ }|s,a) B\).

**Assumption 4.3** (Reachability).: For each round \(k\) and step \(h\), the true transition kernel \(P^{,k}_{h}\) satisfies that for any \((s,a,s^{})\), \(P^{,k}_{h}(s^{}|s,a) p_{}\).

**Variation Budgets:** We next introduce several measures of nonstationarity of the environment: \(^{P}=_{k=1}^{K}_{h=1}^{H}_{(s,a) }\|P^{,k+1}_{h}(|s,a)-P^{,k}_{h}(|s,a)\|_{TV}\),\(^{}=_{k=1}^{K}_{h=1}^{H}_{(s,a) }\|P^{,k+1}_{h}(|s,a)-P^{,k}_{h}(|s,a)\|_{TV}^{1 /2}^{}=_{k=1}^{K}_{h=1}^{H}_{(s,a) }\|^{*,k+1}_{h}(s,a)-^{*,k}_{h}(s,a)\|_{2}\),\(^{}=_{k=1}^{K}_{h=1}^{H}_{s}\|^{ ,k}_{h}(|s)-^{*,k-1}_{h}(|s)\|_{TV}\). These notions are known as _variation budgets_ or _path lengths_ in the literature of online convex optimization (Besbes et al., 2015; Hazan, 2016; Hall and Willett, 2015) and nonstationary RL (Fei et al., 2020; Zhong et al., 2021; Zhou et al., 2020). The regret of nonstationary RL naturally depends on these notions that capture the variations of MDP models over time.

### Theoretical Guarantee

To present our theoretical result for PORTAL, we first discuss technical challenges in our analysis and the novel tools that we develop. Generally, large nonstationarity of environment can cause significant errors to MLE, empirical covariance and exploration-driven bonus design for low-rank models. Thus, different from static low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022), we devise several new techniques in our analysis to capture the errors caused by nonstationarity which we summarize below.

1. Characterizing nonstationary MLE guarantee. We provide a theoretical ground to support our design of leveraging history data collected under various different transition kernels in previous rounds for benefiting the estimation of the current model, which is somewhat surprising. Specifically, we establish an MLE guarantee of the model estimation error, which features a separation of variation budgets from a diminishing term as the estimation sample size \(W\) increases. Such a result justifies the usage of data generated by mismatched distributions within a certain window as long as the variation budgets is mild. Such a separation cannot be shown directly. Instead, we bridge the bound of model estimation error and the expected value of the ratio of transition kernels via Hellinger distance, and the latter can be decomposed into the variation budgets and a diminishing term as the estimation sample size increases.
2. Establishing trajectory-wise uncertainty for estimation error \(^{}_{P^{k},^{k}}\). To this end, straightforward combination of our nonstationary MLE guarantee with previous techniques on low-rank MDPs would yield a coefficient \(_{k,W}\) that depends on local variation budgets. Instead, we convert the 

[MISSING_PAGE_FAIL:8]

Specifically, Ada-PORTAL divides the entire \(K\) rounds into \( K/M\) blocks with equal length of \(M\) rounds. Then two sets \(_{W}\) and \(_{}\) are specified (see later part of this section), from which the window size \(W\) and the restart period \(\) for each block are drawn.

For each block \(i[]\), Ada-PORTAL treats each element of \(_{W}_{}\) as an arm and take it as a bandit problem to select the best arm for each block. In lines 4 and 5 of Algorithm 3, a master algorithm is run to update parameters and select the desired arm, i.e., the window size \(W_{i}\) and the restart period \(_{i}\). Here we choose EXP3-P (Bubeck & Cesa-Bianchi, 2012) as the master algorithm and discuss the details later. Then Algorithm 1 is called with input \(W_{i}\) and \(_{i}\) as a subroutine for the current block \(i\). At the end of each block, the total reward of the current block is computed by summing up all the empirical value functions of the target policy of each episode within the block, which is then used to update the parameters for the next block.

We next set the feasible sets and block length of Algorithm 1. Since optimal \(W\) and \(\) in PORTAL are chosen differently from previous works (Zhou et al., 2020; Cheung et al., 2019; Touati & Vincent, 2020) on nonstationary MDPs due to the low-rank structure, the feasible set here that covers the optimal choices of \(W\) and \(\) should also be set differently from those previous works using BORL.

\(M_{W}=d^{}H^{}K^{}\), \(M_{}=K^{}\), \(M=d^{}H^{}K^{}\). \(J_{W}=(M_{W})\), \(_{W}=\{M_{W}^{0}, M_{W}^{},,M_{W}\}\), \(J_{}=(M_{})\), \(_{}=\{M_{}^{0}, M_{}^{}, ,M_{}\}\), \(J=J_{W} J_{}\).

Then the parameters of EXP3-P are initialized as follows:

\[=0.95},=},=1.05},  q_{(k,l),1}=0,(k,l),\] (4)

where \(=\{(k,l):k\{0,1,,J_{W}\}\,,l\{0,1,,J_{}\}\}\). The parameter updating rule is as follows. For any \((k,l),i K/M\),

\[u_{(k,l),i}=(1-))}{_{(k,l)}( q_{(k,l),i})}+,\] (5)

where \(u_{(k,l),i}\) is a probability over \(\). From \(u_{(k,l),i}\), the agent samples a desired pair \((k_{i},l_{i})\) for each block \(i\), which corresponds to the index of feasible set \(_{W}_{}\) and is used to set \(W_{i}\) and \(_{i}\).

As a last step, \(R_{i}(W_{i},_{i})\) is rescaled to update \(q_{(k,l),i+1}\).

\[q_{(k,l),i+1}=q_{(k,l),i}+,l_{i})}R_{i}(W_{i},_ {i})/M}{u_{(k,l),i}}.\] (6)

We next establish a bound on \(}\) for Ada-PORTAL.

**Theorem 5.1**.: _Under the same conditions of Theorem 4.4, with probability at least \(1-\), the average dynamic suboptimality gap of Ada-PORTAL in Algorithm 3 is upper-bounded as \(}(K)(H^{}d^{}A^{} (A+d^{2})^{}K^{-}(^{}+^{}+1)^{ }+2HK^{-}(^{P}+^{}+1)^{})\)._

**Comparison with Wei & Luo (2021):** It is interesting to compare Ada-PORTAL with an alternative black-box type of approach to see its advantage. A black-box technique called MASTER was proposed in Wei & Luo (2021), which can also work with any base algorithm such as PORTAL to handle unknown variation budgets. Such a combined approach of MASTER+PORTAL turns out to have a worse \(}\) than our Ada-PORTAL in Algorithm 3. To see this, denote \(=^{}+^{}+^{}\). The \(}\) of MASTER+PORTAL is \((K^{-}^{})\). Then if variation budgets are not too small, i.e. \((1)\), this \(}\) is worse than Ada-PORTAL. See detailed discussion in Appendix C.2.

## 6 Conclusion

In the paper, we investigate nonstationary RL under low-rank MDPs. We first propose a notion of average dynamic suboptimality gap \(}\) to evaluate the performance of a series of policies in a nonstationary environment. Then we propose a sample-efficient policy optimization algorithm PORTAL and its parameter-free version Ada-PORTAL. We further provide upper bounds on \(}\) for both algorithms. As future work, it is interesting to investigate the impact of various constraints such as safety requirements in nonstationary RL under function approximations.

## 7 Acknowledgement

The work of Y. Liang was supported in part by the U.S. National Science Foundation under the grants RINGS-2148253, DMS-2134145, and ECCS-2113860. The work of J. Yang was supported by the U.S. National Science Foundation under the grants CNS-1956276 and CNS-2003131.