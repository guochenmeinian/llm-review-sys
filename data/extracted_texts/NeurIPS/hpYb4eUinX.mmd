# Boosting Verification of Deep Reinforcement Learning via Piece-wise Linear Decision Neural Networks

Jiaxu Tian \({}^{1}\), Dapeng Zhi \({}^{1}\), Si Liu \({}^{2}\), Peixin Wang* \({}^{3}\), Cheng Chen \({}^{1}\), Min Zhang* \({}^{1}\)

\({}^{1}\) Shanghai Key Laboratory of Trustworthy Computing, East China Normal University

\({}^{2}\) ETH Zurich

\({}^{3}\) University of Oxford

51215902@14@stu.ecnu.edu.cn, zhi.dapeng@163.com, si.liu@inf.ethz.ch,

peixin.wang@cs.ox.ac.uk, {chchen,zhangmin}@sei.ecnu.edu.cn

###### Abstract

Formally verifying deep reinforcement learning (DRL) systems suffers from both inaccurate verification results and limited scalability. The major obstacle lies in the large overestimation introduced inherently during training and then transforming the inexplicable decision-making models i.e., deep neural networks (DNNs), into easy-to-verify models. In this paper, we propose an inverse _transform-then-train_ approach, which first encodes a DNN into an equivalent set of efficiently and tightly verifiable linear control policies and then optimizes them via reinforcement learning. We accompany our inverse approach with a novel neural network model called _piece-wise linear decision neural networks_ (PLDNNs), which are compatible with most existing DRL training algorithms with comparable performance against conventional DNNs. Our extensive experiments show that, compared to DNN-based DRL systems, PLDNN-based systems can be more efficiently and tightly verified with up to 438 times speedup and a significant reduction in overestimation. In particular, even a complex 12-dimensional DRL system is efficiently verified with up to 7 times deeper computation steps.

## 1 Introduction

Deep neural networks (DNNs) have been exhibiting appealing advantages in decision-making and control for deep reinforcement learning (DRL) systems [1; 2; 3; 4]. Nonetheless, the complexity and inexplicability [5; 6] of DNNs render the formal verification of their hosting systems, quite often even themselves, inaccurate and unscalable. Most existing approaches [7; 8; 9; 10] over-approximate both embedded DNNs and non-linear environment dynamics to build verifiable models, which inevitably introduces _dual_ overestimation. In particular, DNN-specific overestimation is extremely unpredictable due to many factors such as the dimension of system states, the complexity of environment dynamics, and the size, weight, and activation function of a neural network. For example, the verification results may deviate significantly even if the DNNs of the same DRL system differ only in their weights (as we also observed; see Appendix A.4 ). Unsurprisingly, verifying high-dimensional DRL systems would only exacerbate the problems of large overestimation and limited scalability.

Common practice for formally verifying DRL systems is to _train and then transform_ the embedded DNNs into easy-to-verify models where, for any input set, output ranges can enclose the outputs of the over-approximated DNNs [7; 8; 9; 10]. Taylor models  are widely adopted due to their preservation of input-output dependencies and less overestimation (accumulated in multiple steps) than the range analysis approaches such as interval over-approximation [12; 13]. However, they are still prone to intractable overestimation as the accuracy of verification depends heavily on the weights of DNNswhose effects are difficult to quantify. Several other approaches attempt to extract approximated state-action policies, e.g., decision trees [14; 15], from DNNs via model compression  and distillation  techniques. However, no equivalence guarantee is established between DNNs and the extracted policies. Consequently, verification results are just probably approximately correct .

Inspired by recent advances [19; 20; 21; 22] in training near-optimal policies even with reduced training state space imposed by aggregated adjacent states, we propose a novel, inverse _transform-then-train_ approach: encoding a DNN into an equivalent set of easy-to-verify linear control policies and _then_ optimizing them by training the DNN using reinforcement learning. We accompany our inverse approach by devising a novel neural network model called _piece-wise linear decision neural networks_ (PLDNNs), which make linear decisions on each abstract state. Unlike conventional DNNs which build a state-action relation for each actual state, a PLDNN defines a linear relationship, called _Linear Control Unit_ (LCU), between actions and actual states associated with the same abstract state. To this end, a PLDNN is essentially a set of LCUs for all abstract states. In contrast to DNNs, LCUs are more explainable and verifiable without any over-approximation. Moreover, PLDNNs are _compatible_ with most existing DRL training algorithms as both share the same input and output layers for the same control task.

We extensively assess PLDNN, along with the state-of-the-art tools, with respect to both _performance_ (in terms of cumulative rewards and system robustness) and _verifiability_ (in terms of overestimation and time cost for the reachability analysis of trained systems) on a collection of benchmarks, including a 12-dimensional control task. Our experimental results show that, compared to the DNN-based systems, the PLDNN-based systems can be verified more precisely, with significantly less overestimation, and more efficiently, with up to 438 times speedup, while achieving comparable performance. Moreover, compared to the state-of-the-art tools, the complex 12-dimensional control task can be trained and verified with up to 7 times deeper computation steps, along with notable tightness improvement.

**Main Contributions.** Overall, we provide: (i) a novel _inverse_ approach for boosting the formal verification of DRL systems by learning efficiently and directly (without over-approximation) verifiable piece-wise linear policies with comparable performance; (ii) a novel neural network model to realize the learned piece-wise linear policies, which is compatible with most existing DRL algorithms; and (iii) a prototype called LnCon, along with an extensive assessment which demonstrates its tightness in verification results, outperformance over the state-of-the-art tools (up to 438 times speedup), and scalability (up to a 12-dimensional control task).

## 2 Problem Formulation and Motivation

A DRL system is driven by a DNN-implemented controller \(\), which is trained for decision-making, and a physical model defined by the ordinary differential equations (ODEs) \((t)=f(s(t),a(t))\), with \(s\) the state variables and \(a\) a control action. In what follows, we omit the time variable \(t\). Typically, for DRL systems, continuous time is discretized, and we have \(a=(s)\) at state \(s\) and assume \(=0\) during a small time step e.g., \(\). At the time point \(k\), \(k\), the decision network receives the current state \(s_{k}\) and outputs an action \(a_{k}=(s_{k})\). The state variables then evolve according to the physical model during the time interval \([0,]\). The reachable state \(s_{k+1}\) at \(\) from \(s_{k}\) is \(s_{k+1}=s_{k}+_{0}^{}f(s,a_{k})dt\) which is called the successor state of \(s_{k}\). Note that the system evolves continuously from \(s_{k}\) to \(s_{k+1}\). The intermediate states can be computed by substituting the time elapses for \(\) in the above formula.

**Definition 1** (Path of DRL systems).: _Given a DRL system \(\) with an environment dynamics \(f\), decision network \(\), time step size \(\) and initial state set \(S_{0}\), a path of \(\) is a finite or infinite state sequence: \([s_{0},a_{0}]}{{}}[s_{1 },a_{1}]}{{}}[s_{2},a_{2 }]}{{}}[s_{3},a_{3}] }{{}}\) such that:_

1. \(s_{0} S_{0}\)_,_
2. \(s_{i+1}=s_{i}+_{0}^{}f(s,a_{i})dt\) _and_ \(a_{i}=(s_{i})\) _for_ \(i=0,1,2,\)_._

A DRL system is essentially a DNN-controlled hybrid system. Definition 2 gives a formal definition of regular hybrid systems. The state space of a hybrid system is the Cartesian product of a set \(L\) of discrete locations and state space of \(n\) real-valued variables \(V_{c}\). At each location \(l L\), the \(n\) continuous variables evolve continuously according to a dynamical law \(=f_{l}(v)\). When the guard condition on the transition between locations \((l_{1},l_{2}) T\) is triggered, the system moves to \(l_{2}\), and the continuous variables are reset by \(R\). There are two steps involved in the state transition from \((l_{i},v_{i})\) to its _successor state_\((l_{i+1},v_{i+1})\): first, from \((l_{i},v_{i})\) to its _time successor_\((l_{i},_{f_{l_{i}}}(v_{i},t_{i}))\), and then, to \((l_{i+1},v_{i+1})\) that is the _transition successor_ of \((l_{i},_{_{i}}(v_{i},t_{i}))\), where \(_{_{i}}\) is the solution of \(f_{l_{i}}\) with initial condition \(v(0)=v_{i}\), mapping the initial state \(v_{i}\) to the state \(_{_{i}}(v_{i},t)\) (i.e., the reachable state at time \(t\) from \(v_{i}\)). Accordingly, the state of a hybrid system can be changed in two ways : (i) by a time delay that changes only the value of continuous variables according to the dynamics of the current location defined in \(F\); and (ii) by a discrete and instantaneous transition that changes both location and continuous variables according to the rules in \(T\).

**Definition 2** (Hybrid Automata ).: _A hybrid automaton is an 8-tuple \(H= L,,,F,T,G,R,I_{0}\), where:_

* \(L\) _is a finite set of discrete locations;_
* _Var is a finite set of_ \(n\) _real-valued variables with state space_ \(V_{c}^{n}\)_;_
* \(Inv:L 2^{V_{c}}\) _is a function assigning to each location an invariant condition;_
* \(F:L(V_{c}^{n})\) _is a function associating each location_ \(l\) _to a continuous dynamics_ \(=f(v)\)_;_
* \(T L L\) _is a set of transitions between locations;_
* \(G:T 2^{V_{c}}\) _is a function assigning each transition_ \((l_{1},l_{2}) T\) _a guard condition_ \(G(l_{1},l_{2}) Inv(l_{1})\)_;_
* \(R:T 2^{V_{c}}\) _is a function assigning each transition_ \((l_{1},l_{2}) T\) _a reset_ \(R(l_{1},l_{2}) Inv(l_{2})\)_;_
* \(I_{0} L V_{c}\) _is an initial state set._

**Definition 3** (Path of Hybrid Automaton ).: _Let \(H= L,,,F,T,G,R,I_{0}\) be a hybrid automata. A path of \(H\) is a finite or infinite sequence of location and state value pairs, starting from an initial pair \((l_{0},v_{0}) I_{0}\), i.e., \((l_{0},v_{0})[]{_{0}}(l_{1},v_{1})[]{t_{1}} []{t_{1}}(l_{2},v_{2})[]{t_{2}}[]{t_{2}}( l_{3},v_{3})[]{t_{1}}\) such that:_

1. \( 0 t t_{i},_{_{i}}(v_{i},t) Inv(l_{i})\)__
2. \((l_{i},l_{i+1}) T_{_{i}}(v_{i},t) G(l_{i},l_{i+1})  v_{i+1} R(l_{i},l_{i+1})\)__

**Theorem 1** (Modeling DRL Systems as Hybrid Automata).: _A DRL system with an environment dynamics \(f\), decision network \(\), time step size \(\), and initial state set \(S_{0}\), state space \(S\), can be equivalently modeled as the following hybrid automaton:_

* _Var: state variable_ \(s\)_, action_ \(a\)_, and clock variable_ \(t_{c}\)__
* \(L\)_:_ \(\{l_{0}\}\)__
* \(Inv\)_:_ \(Inv(l_{0})=\{s S,t_{c}\}\)__
* \(F\)_:_ \(F(l_{0})=\{=f(s,a),=0,t_{c}=1\}\)__
* \(T\)_:_ \(\{(l_{0},l_{0})\}\)__
* \(G\)_:_ \(G(l_{0},l_{0})=\{t_{c}=\}\)__
* \(R\)_:_ \(R(l_{0},l_{0})=\{t_{c}=0,a=(s)\}\)__

Proof.: We first analyze the path of the modeled hybrid automaton \(H\). For an arbitrary initial state \((l_{0},[s_{0},0,])\), \(s_{0} S_{0}\), since \(t_{c}=\), the only transition with guard condition \(\{t_{c}=\}\) will be triggered and transition to \((l_{0},[s_{0},a_{0},0])\) where \(a_{0}=(s_{0})\). Then \((l_{0},[s_{0},a_{0},0])\) will move to its time successor \((l_{0},[s_{1},a_{0},])\) where \(s_{1}=s_{0}+_{0}^{}f(s,a_{0})dt\). Next, the transition with guard condition \(\{t_{c}=\}\) will be made and conduct the reset operation \(t_{c}=0,a=(s_{1})\) in \(R\) to obtain the transition successor \((l_{0},[s_{1},a_{1},0])\) in which \(a_{1}=(s_{1})\). Repeating the evolution to the time successor and the transition successor yields the following sequence:

\[(l_{0},[s_{0},0,])[]{0}(l_{0},[s_{0},a_{0},0])[] {}(l_{0},[s_{1},a_{1},0])[]{}(l_{0},[s_{2},a_{2},0]) []{}\]

According to Definition 1, there exists the same transition relation \(s_{i+1}=s_{i}+_{0}^{}f(s,a_{i})dt\) with \(a_{i}=(s_{i})\) in \(\) and the modeled hybrid automaton \(H\) at each time point \(i\). Thus given an arbitrary initial state \(s_{0}\), the value of state variable and action of \(\) and \(H\) remain the same at each \(\). Then with the same state-action pair and dynamics \(f\) at each \(\), \(\) and \(H\) also have the same state-action value during each time interval \([i,(i+1))\). 

Figure 1 depicts the hybrid automaton defined in Theorem 1. There exists only one location \(l_{0}\). The invariants in \(Inv\) claim that any state belongs to the state space \(S\) and the clock variable \(t_{c}\) is less than or equal to the time step size \(\). The flow in \(F\) defines the dynamics \(f\) of the system. The only transition is triggered when \(t_{c}=\), updating the action \(a\) and resetting \(t_{c}\) as defined in \(R\). The continuous change happens during the time interval \([i,(i+1)]\), with \(i\), and the discrete change of actions occurs at each \(\).

Unfortunately, the hybrid automaton of a DRL system cannot be verified by using existing hybrid automata model checkers such as Flow* , Ariadne , and CORA . The reason is that the action \(a\) in \(R\) depends on the uninterpretable DNN \(\) by \(a:=(s)\), and \(=f(s,a)\) can not be expressed in a known closed-form, which however is required by regular hybrid automata supported by these tools . Hence, almost all reachability-based verification methods for DRL systems such as Polar , Sherlock , and ReachNN  inevitably over-approximate \(\) using a Taylor model, at the cost of large overestimation and time overhead.

## 3 Piece-Wise Linear Control Policies

To bypass the crux of over-approximating DNNs, we devise a novel, alternative neural network model which essentially realizes a set of linear control policies. Our approach bases on the common assumption that there exists a near-optimal linear control policy for every small region of the entire state space [29; 30; 21; 31]. Our objective is then to discretize the state space \(S\) of a DRL system and to train a linear control policy for each discretized region. Specifically, given an \(n\)-dimensional DRL system with \(m\)-dimensional control input, we train a DNN which implements a linear control function \(a_{j}=b^{j}+c_{1}^{j}x_{1}+c_{2}^{j}x_{2}++c_{n}^{j}x_{n}\) for each control dimension \(1 j m\) and each discretized region.

### Abstracting State Space via Abstract Interpretation

Abstract Interpretation  is an effective technique for scaling up formal verification of complex systems or programs by reducing the system space while preserving the soundness of verification results. For instance, an infinite state space \([-2,0]\) can be abstracted to be an abstract state represented as \((-2,0,0,2)\), when all the states in \([-2,0]\) share a same property. In general, given a system state space \(S\), we denote \(S_{}\) as a finite set of abstract states (each abstract state represents a possibly infinite set of actual system states in \(S\)). Let \(:S S_{}\) be an abstraction function that maps each actual state \(s\) in \(S\) to the corresponding abstract state in \(S_{}\), and \(^{-1}:S_{} 2^{S}\) be the inverse concretization function such that \(^{-1}(s_{})=\{s|s S,(s)=s_{}\}\).

For state space abstraction, we choose the very primitive but effective abstraction approach which abstracts actual system states as intervals. It is known as _interval abstract domain_ and has been well studied for system  and program verification  and even the approximation of neural networks . Specifically, let \(L_{i}\) and \(U_{i}\) be the lower and upper bounds for the \(i\)-th dimension value of \(S\). We first define the abstraction granularity as an \(n\)-dimensional vector \(=(d_{1},d_{2},,d_{n})\). Then the \(i\)-th dimension will be divided evenly into \((U_{i}-L_{i})/d_{i}\) intervals which means each abstract state can be represented as a \(2n\)-dimensional vector \((l_{1},u_{1},,l_{n},u_{n})\).

**Definition 4** (Interval-Based Abstraction Function).: _Given an \(n\)-dimensional continuous state space \(S\) and an abstract state space \(S_{}\) which discretizes \(S\) based on abstraction granularity \(\), \(:S S_{}\) is called an interval-based abstraction function such that, for every actual state \(s=(x_{1},,x_{n}) S\) and abstract state \(s_{}=(l_{1},u_{1},,l_{n},u_{n}) S_{}\), we have \((s)=s_{}\) if and only if \(l_{i} x_{i}<u_{i}\) holds for each dimension \(1 i n\)._

**Example 1** (Running Example).: _Consider a 2-dimensional system in  with state space \([-2,2)[-2,2)\). The dynamics \(f\) is defined by following ordinary differential equations (ODE) i.e., \(x_{1}=x_{2}-x_{1}^{3}\) and \(x_{2}=a\). The sign \(a\) means the control action. The objective is to train a DNN for determining action \(a\) based on \((x_{1},x_{2})\) so that the agent can move from the initial region \(x_{1}[0.7,0.9]\) and \(x_{2}[0.7,0.9]\) to the goal region \(x_{1}[-0.3,0.1]\) and \(x_{2}[-0.35,0.05]\) as soon as possible._

Suppose that the abstraction granularity is \(=(2,2)\). The continuous state space \([-2,2)[-2,2)\) is then partitioned into four regions, corresponding to four abstract states represented by \(S_{}=\{(-2,0,-2,0),(-2,0,0,2),(0,2,-2,0),(0,2,0,2)\}\), respectively.

### Piece-Wise Linear Decision Neural Networks

We devise an alternative DNN model called _piece-wise linear decision neural networks_ (PLDNNs). Unlike conventional DNNs, a PLDNN contains an _abstraction layer_ between the input layer and the first hidden layer. The abstraction layer is used to convert an actual system state into its corresponding abstract state. Then the output of a PLDNN is the control action that is the _dot product_ result of state variables of the actual state and the linear coefficients determined by the corresponding abstract state.

Figure 1: Hybrid automaton for a DRL system.

Figure 2 exemplifies the architecture of the PLDNN \(\) for a two-dimensional DRL system. The decision-making of \(\) is based on a coefficient network \(_{c}\) that outputs the linear coefficients. The second layer of \(_{c}\) is the inserted abstraction layer which consists of the blue neurons and the red neurons.

The output layer of \(_{c}\) contains \(n+1\) neurons that output the \(n+1\) linear coefficients depicted as purple neurons. As for the weights setting between the input layer and the abstraction layer, the weights of the connections between the \(i\)-th neuron in the input layer and the \((2i-1)\)-th and \(2i\)-th neurons in the abstraction layer are set to \(1\) which are represented by blue lines and red lines, respectively. While the weights of other connections denoted by the black dashed lines are set to \(0\). Under this setting of weights, the inputs to both \((2i-1)\)-th and \(2i\)-th neurons in the abstraction layer are \(x_{i}\). Moreover, the activation function of the \((2i-1)\)-th neuron in the abstraction layer is set to \(_{u}\) with the responsibility of computing the upper bound \(u_{i}\), and that of the \(2i\)-th neuron is set to \(_{l}\) for calculating the lower bound \(l_{i}\). Specifically, for a continuous state space partitioned by the abstraction granularity \(=(d_{1},,d_{n})\), the activation functions for the \((2i-1)\)-th and \(2i\)-th neurons in the abstraction layer can be formulated as follows:

\[_{l}^{i}(x_{i})=L_{i}+-L_{i})}{d_{i}} d_{i} _{u}^{i}(x_{i})=L_{i}+-L_{i})}{d_{i}} d_{ i}+d_{i}\]

With the above activation functions, the abstraction layer can output the same abstract state \(s_{}=(l_{1},u_{1},,l_{n},u_{n})\) for \( s^{-1}(s_{})\). The abstract state \(s_{}\) is then propagated to the fully connected layers of \(_{c}\) to generate the linear control coefficients \((b,c_{1},c_{2})\) denoted by \(_{c}(s)\).

To obtain the final output of action \(a\), an additional dot product operation between \(_{c}(s)\) and \([1,s]\) is performed with the result of the operation as the control action \(a\) where \([,]\) is the concatenation operation. For multiple dimensional control action \(a=(a_{1}, a_{m})\), we only need to modify the output dimension of \(_{c}\) to \(m(n+1)\), such that each \(n+1\) neurons output the linear coefficients of one dimension of \(a\). More specifically, we can obtain \(a_{j}\) as follows:

\[a_{j}=(s)_{j}=_{c}(s)[(n+1)(j-1):(n+1)j][1,s],1 j m.\]

where \(vector[start:end]\) denotes the slicing operation that extracts the elements of \(vertor\) from index \(start\) up to but not including index \(end\).

With the additional abstraction layer that can output an identical vector into the fully connected layers of \(_{c}\) for \( s^{-1}(s_{})\), we can ensure that \(_{c}\) always produces the same coefficients for all actual states located in the same abstract state. Consequently, we can extract a piecewise linear decision function with this structure of \(\) on each abstract state.

### The Training Procedure

Training a PLDNN can be achieved by extending existing deterministic policy gradient algorithms such as Deep Deterministic Policy Gradient (DDPG)  and Twin Delayed Deep Deterministic Policy Gradient  since the modifications made stay inside neural networks and are invisible to the DRL algorithms. The pseudo code of the training procedure is given in Algorithm 1, where we take the DDPG algorithm as an example. The training procedure starts with defining the abstraction

Figure 2: The arch. of the PLDNN for Example 1.

function \(\) according to \(\) (Line 2), initializing PLDNN with an abstraction layer based on \(\) (Line 3), and, following , initializing the critic network and the two target networks (Line 4). The procedure then invokes the DDPG algorithm with the networks as arguments (Line 5) since the PLDNN has the same input and output as the actor network implemented by DNN. During this procedure, we freeze the parameters between the input and the abstraction layers of \(\). The parameters in the fully connected layers are trained based on backpropagation  and gradient descent optimization .

## 4 Equivalent Policy Extraction and Verification

After training, we can extract \(|S_{}|\) LCUs based on the learned coefficients of linear control policies for the abstract states in \(S_{}\). Specifically, we choose an arbitrary actual state \(s^{-1}(s_{})\) for each abstract state \(s_{}\) and feed it to a PLDNN to obtain the coefficients defined on the abstract state. For instance, we can feed \((-1,-1)\) to the PLDNN in Example 1 and obtain the coefficients \((-0.16610657,-1.7437580,-1.8227874)\) of the linear control policy for the region \([-2,0)[-2,0)\). Figure 3 shows the LCUs extracted from a trained PLDNN in Example 1. They are depicted by planes with different colors in Figure 3. These four planes denote the following linear control functions:

\[(x_{1},x_{2}) =-0.16610657-1.7437580x_{1}-1.8227874x_{2}, x_{1}[-2,0), x_{2}[-2,0)\] (LCU1) \[(x_{1},x_{2}) =-0.20400035-1.8006037x_{1}-1.8679885x_{2}, x_{1}[-2,0), x_{2}[0,2)\] (LCU2) \[(x_{1},x_{2}) =-0.27547930-1.8884722x_{1}-1.9342268x_{2}, x_{1}[0,2), x_{2}[-2,0)\] (LCU3) \[(x_{1},x_{2}) =-0.29549897-1.9022338x_{1}-1.9436346x_{2}, x_{1}[0,2), x_{2}[0,2)\] (LCU4)

The underlying \(x_{1} x_{2}\) plane in Figure 3 is the projection of the four LCUs. Under the control of these four LCUs, the agent can reach the goal region (orange box). The two sequences of purple boxes represent the range of reachable states from corresponding initial regions to the goal region.

With exacted LCUs from a PLDNN, we can build a verifiable hybrid automaton for the system by substituting equivalently the neural networks using corresponding LCUs. Theorem 2 formulates the hybrid automaton after a decision network \(\) is substituted by LCUs. The differences from Theorem 1 include the definitions of transitions \(T\), guard condition \(G\) and reset formula \(R\). For the PLDNN controlled systems, We use \(|S_{}|\) transitions each of which contains a guard condition and a reset formula to update the action \(a\) to \(_{c}(s)[1,s]\) at each \(\). Notice that, \(_{c}(s)\) is a determined vector since \( s^{-1}(s^{i}_{})\), the outputs of \(_{c}\) are the same according to the dedicated structure of \(_{c}\). Thus, the reset formula for \(a\) is simplified to an affine mapping.

**Theorem 2**.: _Given a DRL system with environment dynamics \(f\), PLDNN \(\), time step size \(\) and initial state set \(S_{0}\), it can be equivalently modeled as a hybrid automaton defined as follows:_

* _Var: state variable_ \(s\)_, action_ \(a\)_, clock variable_ \(t_{c}\)__ \(\)__\(I_{0}\)_:_ \(\{(l_{0},(s S_{0},a=0,t_{c}=))\}\)__
* \(L\)_:_ \(\{l_{0}\}\)__\(\)__\(Inv\)_:_ \(Inv(l_{0})=\{s S,t_{c}\}\)__\(F\)_:_ \(F(l_{0})=\{=f(s,a),=0,t_{c}=1\}\)__
* \(T\)_:_ \(\{(l_{0},l_{0}),,(l_{0},l_{0})\}\) _where_ \(|T|=|S_{}|\)__
* \(G\)_:_ \(G(T[i])=\{t_{c}=,s^{-1}(s^{i}_{})\}\) _where_ \(0 i<|T| s^{i}_{} S_{}\)__
* \(R\)_:_ \(R(T[i])=\{t_{c}=0,a=_{c}(s)[1,s]\}\) _where_ \(0 i<|T| s^{-1}(s^{i}_{})\)__

Proof.: We prove that the path of hybrid automaton \(H_{1}\) in Theorem 1 is the same as the corresponding hybrid automaton \(H_{2}\) in Theorem 2 for an arbitrary initial state \((l_{0},[s_{0},0,])\). The path of \(H_{2}\) can be described by the following sequence:

\[(l_{0},[s_{0},0,])}(l_{0},[s_{0}, a_{0},0])}(l_{0},[s_{1},a_{1},0]) }(l_{0},[s_{2},a_{2},0]) }\]

where \(s_{i+1}=s_{i}+_{0}^{}f(s,a_{i})dt\) and \(a_{i}=_{c}(s_{i})[1,s_{i}]\). With the dedicated structure of PLDNN \(\), we have \((s)=_{c}(s)[1,s]\). Thus, at each \(\), the discrete transitions of \(H_{1}\) and \(H_{2}\) change both location

Figure 3: The LCUs extracted from a trained PLDNN for Example 1.

and continuous variables in the same way. We can conclude that the paths of \(H_{1}\) and \(H_{2}\) are exactly the same. By Theorem 1, it is obvious that \(\), \(H_{1}\) and \(H_{2}\) produce the same state-action value for the same initial state. 

According to Theorem 2, we can build a hybrid automaton that is equivalent to the DRL system in Example 1. Assuming the trained system has four linear control units as shown in Formulas (LCU\({}_{1}\)-LCU\({}_{4}\)) and \(=0.2\), we construct the corresponding hybrid automaton as depicted in Figure 4. The four transitions in the automaton correspond to the four LCUs, respectively. The guard of each transition represents the condition of triggering the corresponding policy.

Thanks to the linearity of control policies in \(R\), a hybrid automaton built for a PLDNN-controlled system can be efficiently verified by state-of-the-art tools. For instance, Flow*  is a representative tool for the reachability analysis of hybrid systems. In this paper we are focused on the verification of both goal-reach and reach-avoid properties. The former means that given a set of initial states, a system must eventually reach the goal region from any initial state. The latter means that the system never enters unsafe regions within a specific time horizon. Both properties can be verified via reachability analysis.

## 5 Experimental Evaluations

We prototype our approach into a tool called LinCon, with DDPG as the backend DRL algorithm and Flow* as the verification engine. We extensively assess it, along with the state-of-the-art tools. Our goal is to show, for the PLDNN-based training, (i) the reduction in the number of partitions with comparable cumulative rewards, robustness, and time overhead with respect to conventional DNN-based training; (ii) its high verification performance including the tightness of over-approximation sets and the efficiency of verification; and (iii) its scalability for large-sized neural networks and systems with complex dynamics and high-dimensional state space.

**Experimental Setup.** All experiments were conducted on a workstation equipped with a 32-core AMD Ryzen Threadripper CPU @ 3.6GHz and 256GB RAM, running Ubuntu 22.04.

**Benchmarks.** We choose eight benchmarks, including six regular benchmarks from Verisig 2.0  (B1-B5 and Tora) and two complex benchmarks (CartPole with extreme complex dynamics from OpenAI Gym  and quadrotor (QUAD) with 12-dimensional state space and 3-dimensional action space from ). For fair comparisons, we use the same training configuration and guarantee that all trained systems reach the specified reward threshold. See Appendix A.1 for the detailed setting. For B1-B5 and Tora the environment dynamics are the same with . The dynamics of QUAD are the same with . We omit their detailed definition here. However, the dynamics of CartPole are originally represented as a discrete-time model in  by difference equations. In our experiment, we consider its dynamics as a continuous-time model and formalize it with the following ODEs:

\[}= x_{2},}=x_{4},\] \[}= (a+0.05x_{4}^{2}sin(x_{3}))/1.1-(0.05((9.8sin(x_{3})-cos(x_{3})((a +0.05x_{4}^{2}sin(x_{3}))/1.1))/\] \[(0.5(4.0/3.0-(0.1cos^{2}(x_{3})/1.1)))cos(x_{3}))/1.1,\] \[}= 9.8sin(x_{3})-cos(x_{3})((a+0.05x_{4}^{2}sin(x_{3}))/1.1)/0.5(4. 0/3.0-(0.1cos^{2}(x_{3})/1.1)).\]

### Performance Evaluation

We assess the performance of PLDNN, together with the conventional DNNs, in terms of cumulative reward, robustness, and training time under the same training configuration. We also measure the number of abstracted states required for training linear control policies and constant policies . Due to space limitations, we present the experimental results only for B1, B2, and two complex cases

Figure 4: The hybrid automaton of the DRL system with the trained piece-wise linear controllers in Figure 3.

(i.e., CartPole and QUAD). The associated conclusions (from Table 1 and Figure 5) also apply to the other four cases; see Appendix A.2 for the detailed experimental results.

**Cumulative Reward.** Figure 5 plots the system cumulative reward (the average of five trials) during the training process. The solid lines and shadows refer to the average reward and 95% confidence interval, respectively. Apparently, the trends of accumulative rewards by PLDNNs and DNNs are comparable. Despite slightly more time required for training with PLDNN due to its additional conversion from actual state to abstract state (see Table 1, "Training Time"), we can, however, obtain a significant advance in the efficiency and tightness of verification (see Section 5.2).

**Robustness.** We also evaluate the robustness of PLDNNs as training the linear controller on each partition may lead to discontinuity of control decisions in the boundaries of partitions. For a current state \(s=(x_{1},,x_{n})\), we add a Gaussian noise \(X_{1},...X_{n}\) to \(s\) and obtain a perturbed state \(s^{}=(x_{1}+X_{1},,x_{n}+X_{n})\) for calculating the control action, where \(X_{i}(_{i},_{i}^{2})\) with \(1 i n\) and \(_{i}=0\). For each benchmark, we train 10 different policies and evaluate their robustness under 100 different perturbation levels to obtain the average and 95% confidence interval of the cumulative reward. Figure 5 depicts the reward trend with the increasing perturbation level. As \(\) increases, the decline ratio of the system with PLDNNs is comparable to that with DNNs, which implies that both achieve similar robustness.

**Reduction in the Number of Partitions.** We measure the effect of reducing the number of partitions by utilizing a linear policy, instead of the constant action on each partition, which is adopted by the work . In both cases, we start training from a coarse-grained abstraction granularity (with only one partition) and gradually increase the number of partitions until the preset reward threshold can be reached by the trained PLDNN. As shown in Table 1, linear policies significantly reduce the number of partitions required for reaching the reward threshold, which benefits both the verification efficiency and accuracy as we will see in the next section.

### Verification Efficiency and Tightness

We evaluate the verification efficiency and tightness for the PLDNN-based and DNN-based DRL systems, respectively. Regarding tightness, we choose two state-of-the-art tools, i.e., Polar  and Verisig 2.0 , for the reachability analysis of DNN-controlled systems. We do not consider ReachNN* which both Polar and Verisig 2.0 have been demonstrated to outperform [9; 10]. For efficiency, we employ Flow* to perform reachability analysis, which is used by both Polar and Verisig 2.0 as the backend reachability analysis tool.

**Efficiency.** Table 2 presents the comparison results for the verification efficiency. For each regular case, we choose four different network configurations: two smaller networks (e.g., \(_{2 20}\)) from  and two larger networks (e.g., \(_{3 100}\)). Our approach \(\) can handle all 26 instances including the two complex instances, while Polar succeeds only in 20 cases. Verisig 2.0 is not applicable to

   & **Task** & **B1** & **B2** & **CartPole** & **QUAD** \\   Training & PLDNN & 14.3 & 7.9 & 428.2 & 871.1 \\ Time & DNN & 11.0 & 6.6 & 403.6 & 781.5 \\    & LCU & 1 & 4 & 16 & 1 \\  & Const. & 4 & 100 & 25\({}^{4}\) & 4\({}^{12}\) \\  

Table 1: Training time and number of partitions

Figure 5: Performance and robustness comparison between PLDNNs and DNNs. The number in the parentheses is the base of \(\), e.g., when the abscissa is 50 in B1, we have \(=50 0.0004=0.02\).

ReLU networks (marked by \(}^{c}\)) and succeeds only in 9 instances. Overall, LinCon outperforms both Polar (up to 47.7\(\) speedup) and Verisig 2.0 (up to 438.6\(\) speedup). In particular, LinCon achieves even up to 58.1\(\) speedup compared to Verisig 2.0 accelerated by 20-core parallelization. For our approach LinCon, the only time overhead for encoding networks stems from extracting LCUs from PLDNNs, which is polynomial and negligible (less than 0.05s). Consequently, LinCon can scale up to large-sized neural networks.

**Tightness.** We compare the tightness of the over-approximation sets computed by different approaches. Figure 6 plots the experimental results, along with the corresponding simulation trajectories. For B2 and Tora, LinCon has a significant tightness improvement over Polar: the range of the over-approximation sets (red boxes) calculated by Polar far outreaches the range of the reachable states obtained from the simulation. Verisig 2.0 terminates prematurely in Tora since the range of the action reaches \(10^{7}\) during the calculation which is too large to proceed. We defer to Appendix A.3 the experimental results for the remaining four cases where all three tools obtain similar tightness results.

**Discussion on CartPole and QUAD.** We discuss the verification results of the two complex cases, namely CartPole and QUAD. Both Polar and Verisig 2.0 fail to verify them. As shown in Figure 6, Polar and Verisig 2.0 abort after 20 steps due to the huge over-approximation error in CartPole. In contrast, such a complex policy trained with our approach can be efficiently and tightly verified. In particular, the trajectories diverge first and finally merge. The computed reachable states tightly over-approximate these trajectories, and the computation takes only 151 seconds. Regarding the 12-dimensional QUAD case, Polar and Verisig 2.0 time out (two hours) after only two steps, while LinCon produces a very tight set of reachable states in 1054 seconds even after 15 steps, which is 7 times deeper than Polar. To the best of our knowledge, this is the first time that QUAD can be formally verified more than ten steps under various decision networks. Note that the trained policies used in the comparison differ due to different decision networks, and thus agents may follow different paths to the goal region (see the simulated trajectories in Figure 6). Hence, for a fair comparison, we conduct more evaluations, from which we can draw the same conclusion as from Figure 6. The results are given in Appendix A.3.

   &  &  & }}\)**Con} &  &  \\   & & & **T Core** & **V.R.** & **I Core** & **Impr.** & **V.R.** & **I Core** & **Impr.** & **20 Cores** & **Impr.** & **V.R.** \\    &  & Tanh\({}_{<20}\) & 2.31 & ✓ & 17 & \(7.4\) & ✓ & 45 & \(19.5\) & 38 & \(16.5\) & ✓ \\  & & Tanh\({}_{<100}\) & 2.28 & ✓ & 125 & \(54.8\) & ✓ & 413 & \(181.1\) & 123 & \(53.9\) & ✓ \\   & & ReLU\({}_{<20}\) & 2.11 & ✓ & 3 & \(1.4\) & ✓ & — & — & — & — & — \\  & & ReLU\({}_{<1000}\) & 2.59 & ✓ & – & — & ✗ & — & — & — & — & ✗ \\   &  & Tanh\({}_{<20}\) & 0.57 & ✓ & 5 & \(8.8\) & ✓ & 5 & \(8.8\) & 4 & \(7.0\) & ✗ \\  & & Tanh\({}_{<100}\) & 0.56 & ✓ & — & — & ✗ & — & — & — & — & ✗ \\  & & ReLU\({}_{<20}\) & 0.64 & ✓ & 3 & \(4.7\) & ✓ & — & — & — & — & ✗ \\  & & ReLU\({}_{<100}\) & 0.60 & ✓ & — & — & ✗ & — & — & — & — & ✗ \\   &  & Tanh\({}_{<20}\) & 2.69 & ✓ & 18 & \(6.7\) & ✓ & 36 & \(13.4\) & 28 & \(10.4\) & ✓ \\  & & Tanh\({}_{<100}\) & 3.57 & ✓ & 91 & \(25.5\) & ✓ & 357 & \(100.0\) & 88 & \(24.6\) & ✓ \\   & & ReLU\({}_{<20}\) & 3.05 & ✓ & 8 & \(2.6\) & ✓ & — & — & — & — & — \\  & & ReLU\({}_{<100}\) & 2.92 & ✓ & 14 & \(4.8\) & ✓ & — & — & — & — & ✗ \\   &  & Tanh\({}_{<20}\) & 1.44 & ✓ & 5 & \(3.5\) & ✓ & 7 & \(4.9\) & 5 & \(3.5\) & ✓ \\  & & Tanh\({}_{<100}\) & 1.45 & ✓ & 27 & \(18.6\) & ✓ & 114 & \(78.6\) & 31 & \(21.4\) & ✓ \\   & & ReLU\({}_{<100}\) & 1.43 & ✓ & 2 & \(1.4\) & ✓ & — & — & — & — & ✗ \\  & & ReLU\({}_{<100}\) & 1.43 & ✓ & 5 & \(3.5\) & ✓ & — & — & — & — & ✗ \\   &  & Tanh\({}_{<100}\) & 3.24 & ✓ & 38 & \(11.7\) & ✓ & 157 & \(48.5\) & 44 & \(13.4\) & ✓ \\  & & Tanh\({}_{<200}\) & 3.29 & ✓ & 157 & \(47.7\) & ✓ & 1443 & \(438.6\) & 191 & \(58.1\) & ✓ \\   & & ReLU\({}_{<100}\) & 3.28 & ✓ & 7 & \(2.1\) & ✓ & — & — & — & — & ✗ \\  & & ReLU\({}_{<200}\) & 3.29 & ✓ & 49 & \(14.9\) & ✓ & — & — & — & — & ✗ \\   &  & Tanh\({}_{<20}\) & 1.57 & ✓ & 45 & \(28.7\) & ✓ & 69 & \(43.9\) & ✗ & ✗ & ✗ \\  & & Tanh\({}_{<100}\) & 1.75 & ✓ & — & — & — & — & — & — & ✗ \\   & & ReLU\({}_{<20}\) & 1.58 & ✓ & 30 & \(19.0\) & ✓ & — & — & — & — & ✗ \\   & & ReLU\({}_{<100}\) & 1.62 & ✓ & 53 & \(32.7\) & ✓ & — & — & — & — & ✗ \\   &  & Tanh\({}_{<4}\) & 151 & ✓ & — & — & — & — & — & — & ✗ \\   & & Tanh\({}_{<4}\) & 151 & ✓ & — & — & — & — & — & — & ✗ \\   & 12 & Tanh\({}_{<4}\) & 1054 & ✓ & — & — & — & — & — & — & ✗ \\  

Table 2: Verification results and time in seconds.

## 6 Related Work

**Policy Synthesis.** Several works adopt programmatic policies (e.g., decision trees and program controllers) which are more interpretable and amenable to formal verification than neural policies. Bastani et al.  construct a decision tree to represent a DNN policy based on imitation learning . Verma et al. [43; 44] follow a similar routine, distilling neural network policies into predefined program templates. Trivedi et al.  use a two-stage learning scheme to synthesize programmatic policies. Some efforts are dedicated to exploring the combination of training and verification. Zhu et al.  propose an inductive framework for synthesizing a deterministic policy program from neural policies. Wang et al.  learn programmatic controllers based on verification feedback to avoid safety violations. Our proposed PLDNN is essentially a DNN-based implementation of programmatic controllers, which could be integrated with these verification-guided synthesis approaches.

**Reachability Analysis.** Our work is also built atop the approaches for reachability analysis of neural-network-controlled systems. NNV  utilizes star set  to perform range analysis of decision networks. JuliaReach  uses zonotope propagation to cover the output of a decision network. Verisig [49; 9] models a decision network with differentiable activation functions (e.g., Tanh) as a hybrid system and analyzes its reachability for over-approximating the network. ReachNN* [28; 8] abstracts the input-output mapping of a decision network with a Bernstein polynomial, together with an error bound on the approximation. Sherlock  focuses on ReLU-based networks and computes tight Taylor models via rule generation. Polar  integrates the Taylor and Bernstein approximation techniques for building a Taylor model which over-approximates decision networks. All these efforts over-approximate the embedded DNNs, which limit their scalability and verification accuracy.

## 7 Conclusion and Future Work

We have presented PLDNN that seamlessly integrates DNN and programmatic controls via state abstraction for boosting the reachability analysis of DRL systems. Unlike traditional train-then-transform approaches, PLDNN accompanies a novel inverse training and verification method, in which a DNN is first transformed into an equivalent set of linear control policies and then trained to optimize them. Experimental results have shown that PLDNN-controlled systems can be more efficiently and tightly verified than DNN-based systems, with up to 438 times speedup and 7 times deeper computation steps for a 12-dimensional control task.

Our work sheds light on a promising direction towards developing dependable DRL systems: learning easy-to-verify and high-performance control policies via DRL and abstraction techniques. Given the encouraging results of linear control policies, our work would also stimulate a passion for substituting them with, e.g., polynomial control policies, for training and verifying more complex DRL systems.

Figure 6: Tightness comparison with respect to the DRL systems with larger decision networks (red box: over-approximation sets; green lines: simulation trajectories; blue box: goal region).