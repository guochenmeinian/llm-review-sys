# SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning

Dohyeok Lee\({}^{1}\), Seungyub Han\({}^{1,2}\), Taehyun Cho\({}^{1}\), Jungwoo Lee\({}^{1,}\)

\({}^{1}\)Seoul National University, \({}^{2}\)Hodo AI Labs

{dohyeoklee,seungyubhan,talium,junglee}@snu.ac.kr

Corresponding author

###### Abstract

Alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. In order to overcome overestimation bias, ensemble methods for Q-learning have been investigated to exploit the diversity of multiple Q-functions. Since network initialization has been the predominant approach to promote diversity in Q-functions, heuristically designed diversity injection methods have been studied in the literature. However, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. By introducing a novel regularization loss for _Q-ensemble independence_ based on random matrix theory, we propose _spiked Wishart Q-ensemble independence regularization_ (SPQR) for reinforcement learning. Specifically, we modify the intractable hypothesis testing criterion for the Q-ensemble independence into a tractable KL divergence between the spectral distribution of the Q-ensemble and the target Wigner's semicircle distribution. We implement SPQR in several online and offline ensemble Q-learning algorithms. In the experiments, SPQR outperforms the baseline algorithms in both online and offline RL benchmarks.

## 1 Introduction

Reinforcement learning (RL), especially _deep reinforcement learning_ (DRL) that contains high capacity function approximators such as deep neural networks, has shown considerable success in complex sequential decision making problems including robotics , video games , and strategy games . Despite these promising results, modern DRL algorithms consistently suffer from sample-inefficiency and overestimation bias.

Vanilla off-policy Q-learning faces the overestimation issue in both online and offline reinforcement learning. The overestimation problem is specifically worse for out-of-distribution (OOD) state-action pairs. In order to address the overestimation problem, many existing methods utilize ensemble Q-learning. Ensemble Q-learning introduces multiple Q-networks to compute a Bellman target value to benefit from the diversity of their Q-values. Double Q-learning  proposes clipped Q-learning as the first method to adopt an ensemble to DRL. Along with double Q-learning, Ensemble DQN , Maxmin Q-learning , _randomized ensembled double Q-learning_ (REDQ) , and SUNRISE  are proposed as efficient ensemble Q-learning algorithms on online RL tasks. Clipped SAC  is proposed for offline RL tasks to alleviate the overestimation bias of Q-values from OOD data. Despite the objective of ensemble Q-learning, which enables RL agents to access diverse Q-values for each state-action, existing ensemble Q-learning algorithms have only focused on the computing Bellman target over an ensemble. MED-RL  and EDAC  propose a diversifying loss for online and offline reinforcement learning, such as the Gini coefficient and the variance of the Q-value onOOD data. However, these methods have a limitation in that the proposed losses do not guarantee independence from a theoretical perspective, but diversify Q-functions from heuristics and empirical studies although they have an i.i.d assumption. Most ensemble Q-learning algorithms assume that bias follows a uniform and independent distribution, which may not be true when using deep neural network approximation and a shared Bellman target. Hence we need to reinvestigate the notion of _Q-ensemble independence_ more rigorously through a random matrix theory. The random matrix theory studies the statistical behavior of a random matrix and has a wide range of applications in diverse fields such as physics, engineering, economics, and machine learning as shown in [5; 25; 29].

In this paper, we propose _spiked Wishart Q-ensemble independence regularization_ (SPQR), a tractable Q-ensemble independence regularization method based on the random matrix theory and a spiked random model perspective. SPQR is a regularization method that theoretically guarantees to improve the independence of a Q-ensemble during training. SPQR penalizes the KL divergence between the eigenvalue distribution of the Q-ensemble and an ideal independent ensemble. If a Q-ensemble becomes closer to the ideal independent ensemble, it satisfies the independence assumption of ensemble Q-learning. To show how effective SPQR is, we combine SPQR with various algorithms and online/offline RL tasks. SPQR outperforms baseline algorithms in MuJoCo, D4RL Gym, Franka Kitchen, and Antmaze. To summarize, our contributions are as follows:

1. We propose a tractable loss function for Q-ensemble independence without any prior assumption of Q-function distribution by adopting random matrix theory and a spiked random model for the first time in DRL algorithms.
2. It is empirically shown that the universality and performance gain of SPQR for various algorithms and tasks. SPQR is applied to SAC-Ens, REDQ, SAC-Min, CQL, and EDAC and evaluated in various complex and sparse tasks in online and offline RL, MuJoCo, D4RL Gym, Franka Kitchen, and Antmaze. To ensure our reproducibility and fair comparisons, we provide our source code.1 
## 2 Related Works

Ensemble Reinforcement Learning.Ensemble DQN  is the first method to adopt ensemble Q-learning for DRL. Ensemble DQN aims to reduce overestimation bias by computing a Bellman target as an average Q-value in the ensemble. Maxmin Q-learning  computes a Bellman target as a minimum Q-value over the Q-ensemble and shows significant performance enhancement based on the Atari benchmark. REDQ  constructs sample efficient ensemble Q-learning algorithm based on Maxmin Q-learning and SAC with a probabilistic ensemble by selecting a subset over Q-ensemble. Since REDQ combines the _update-to-data_ (UTD) ratio, which is an orthogonal method with ensemble subset minimization, REDQ achieves state-of-the-art performance in the online MuJoCo environment. For offline reinforcement learning tasks, clipped Q-learning , denoted as SAC-Min is proposed, which computes the Bellman target as in Maxmin Q-learning. Diversification methods are proposed to improve the performance of ensemble Q-learning. MED-RL  optimizes the Gini coefficient to diversify an ensemble, and EDAC  optimizes the variance of the Q-value on OOD data to diversify an ensemble from the OOD data. SPQR proposes a Q-ensemble independence loss from a theoretical perspective, based on a spiked random model and random matrix theory to make ensemble Q-networks as independent as possible. In addition, _conservative Q-learning_ (CQL) is proposed by , which penalizes the Q-values of OOD actions and performs well on numerous offline datasets.

Spiked Random Model.Signal recovery for a generative model using a spiked random model is analyzed by . SPQR analyzes Q-ensemble independence for ensemble Q-learning in a spiked random model perspective. A universal data-driven optimal likelihood hypothesis test with linear spectral statistics (LSS) for a spiked random model as a central limit theorem for test statistics is given by [7; 18]. SPQR proposes a tractable universal data-driven test \(T()\) to regularize the Q-ensemble independence.

## 3 Backgrounds

In this section, we describe ensemble Q-learning algorithms and visualize an empirical study of their limitations. In order to overcome its limitations, we introduce the random matrix theory and a spiked random model to formulate the notion of Q-ensemble independence from a theoretical perspective.

We consider a _Markov decision process_ (MDP), defined by a tuple \((,,P,r,)\), where \(\) is a finite state space, \(\) is a finite action space, \(P:\) is the state-action transition probability, \(r:\) is the reward function, and \([0,1)\) is the discount factor. We define a state-action value function, called Q-function, \(Q_{}:\), represents an expectation of the discounted sum of rewards for the trajectory started from \(s,a\), given by \(_{s_{t},a_{t}}[_{t=0}^{}^{t}r(s_{t},a_{t})]\).

### Ensemble Q-learning

We first define bias \(e^{i}_{sa}=Q^{*}(s,a)-Q_{i}(s,a)\), where \(Q^{*}(s,a)\) is the optimal Q-value given for \((s,a)\) and \(Q_{i}(s,a)\) is the Q-value of the \(i\)-th networks in ensemble. Ensemble Q-learning assumes that the bias follows a zero mean, uniform, and independent distribution, i.e., \(e^{i}_{sa} Unif(-,)\) and is independent with respect to \(i\), although REDQ does not assume uniform distribution. Given \(N\) Q-functions in the ensemble, there are three typical methods to compute a Bellman target: Ensemble DQN, Maxmin Q-learning, and REDQ.

**Ensemble DQN.** Ensemble DQN computes the Bellman target as the ensemble average Q-value. The bias and variance of the bias monotonically decrease as the ensemble size \(N\) increases, and they become 0 as \(N\) goes to infinity. We implement the SAC-style of Ensemble DQN, denoted as SAC-Ens.

**Maxmin Q-learning.** The Bellman target of Maxmin is to select the minimum Q-value within an ensemble, which can control the level of overestimation with a finite number of Q-functions. A modified version called SAC-Min is introduced specifically for offline RL tasks.

**REDQ.** REDQ computes the Bellman target as the minimum Q-value in a subset of the ensemble to become a sample-efficient algorithm.

Detailed explanations for Bellman targets and the evaluation function of each algorithm, denoted as \(Ens_{tar}\) and \(Ens_{eval}\), are described in Appendix C.

To confirm the validity of uniform and independent assumptions of ensemble Q-learning, we plot Q-value distribution in Figure 1 and test whether each Q-distribution is uniform using the \(^{2}\) test. We train 50 Q-networks 3M timesteps by the SAC-Min algorithm in the D4RL hopper-random dataset and evaluate it by the hopper-full-replay dataset. We also perform the \(^{2}\) test for independence in Table 1. Out of 1000 batch data, only 35.8% can accept that the distribution is uniform and 30.4% can accept independence. Thus, \(e^{i}_{sa} Unif(-,)\) and i.i.d. assumption is empirically inaccurate and it is necessary to introduce the notion of Q-ensemble independence. More details on the \(^{2}\) test are given in Appendix E.

Figure 1: **Left 1–4:** Histogram visualization of Q-value distribution over networks in the ensemble. Each plot represents a Q-value histogram for one state-action data. The X-axis represents the Q-value of each Q-network and the Y-axis represents the number of Q-networks in the histogram bin. The red horizontal line represents a uniform distribution and the blue solid line represents kernel density estimation for a given histogram. **Rightmost:** Heatmap visualization of the Pearson correlation coefficient matrix between each Q-network in the ensemble. Detailed values and explanations are given in Appendix E.

As  also points out, the early collapse of Q-values, which means that the Q-values in the ensemble become almost identical, occurs during the training. We empirically visualize a strong correlation among Q-networks in Figure 1. Although the main advantage of ensemble Q-learning is based on diverse Q-functions, network initialization is the only process to maintain diversity over an ensemble since the Bellman target is identical across all networks without diversification and independence regularization methods.

### Random Matrix Theory

Random matrix theory studies the probabilistic properties of large matrices with random entries. Dyson  constructs some categories of the random matrix as a real symmetric matrix as _Gaussian orthogonal ensemble_ (GOE), complex hermitian matrix as _Gaussian unitary ensemble_ (GUE), and quaternion self-dual matrix as _Gaussian symplectic ensemble_ (GSE), called _Dyson's classification_. In this paper, we restrict our interest to matrices in GOE.

**Definition 3.1** (Gaussian Orthogonal Ensemble, ).: Let random matrix \(W R^{N N}\), its entities random variables \(w_{i}\), \(P[W]=P(w_{1},,w_{N^{2}})\) to be a joint probability density function of \(w_{i}\). Define the set of \(W\) as GOE when these properties hold:

1. \(w_{i}\) are independent with respect to \(i\).
2. \(W\) is a real symmetric matrix.
3. \(W\) is rotational invariance, i.e., \(P[W]dW=P[W^{}]dW^{}\) if \(W^{}=UWU^{-1}\), \(U\) is orthogonal.

GOE is the most natural way to model random matrices in various applications, including machine learning. In simplified terms, we can consider GOE as a Wigner matrix, where entries are independent, symmetric, and follow Gaussian distribution. One of the main areas in random matrix theory is how eigenvalues or singular values of a large random matrix are distributed. More specifically, we only consider limiting properties of _empirical spectral density_ (ESD), which is defined as, for random matrix \(W^{N N}\) and its eigenvalues \(_{i},i=1,,N\),

\[p_{W}()=_{i=1}^{N}(-_{i})\] (1)

The limits of the empirical spectral density of the GOE, called the _Wigner's semicircle law_, are given by . For GOE \(^{N N}\) with variance of entries \(^{2}\), \(_{1},,_{N}\) to be a eigenvalue of \(W=}\) and empirical spectral distribution \(p_{W}=_{j=1}^{N}(-_{j})\), then \(p_{W}\) converges to \(p_{sc}\) weakly almost surely, where

\[p_{sc}()=}-^{2}}_{[-2,2]}\] (2)

Figure 2: Eigenvalue plot for the spiked Wishart model with Wigner’s semicircle law. For effective visualization of the spiked model, we use a complex hermitian random matrix, GUE. Blue dots represent each data in a complex plane. The red line represents Wigner’s semicircle distribution. The blue line in the histogram represents kernel density estimation. **Left 1–2:** Perturbation power is \(=0\). Eigenvalue distribution follows Wigner’s semicircle law. **Right 3–4:** Perturbation power is \(=10^{-5}\). Eigenvalue distribution almost follows Wigner’s semicircle law except for the largest eigenvalue, interpreted as a spike.

We call \(p_{sc}\) as the Wigner's semicircle distribution. Wigner's semicircle law does not rely on the probability density of entries.

### Spiked Random Model

Random matrix theory models random data well, but in reality, most data have both informative signals and random components. A spiked random model is a theory about constructing a model of perturbation with a data matrix. The objective of this theory is to separate true signals from perturbated ones with random noise. True signals show _spiked_ phenomena that make the largest eigenvalue detach from the bulk of eigenvalues. A spiked model was first introduced by  as an application for principal component analysis. We restrict our analysis to a _spiked Wishart model_, the most natural methodology to model a general perturbation. A spiked Wishart model is defined as follows.

**Definition 3.2**.: Let \(Y^{N M}_{}\), \(u,v\) is unknown vector, \(Y\) is **spiked Wishart model** when

1. \(Y=}uv^{T}+W\), which is equivalent to sample covariance matrix \(=I_{N}+uu^{T}\)
2. \(u,v\) are i.i.d from zero mean priors \(u P_{u},v P_{v}\), support bounded in radius by \(K_{u},K_{v}\)
3. \(W_{0}\) is a matrix with i.i.d noise entries
4. \(M/N\)

We will assume the spiked Wishart model with \(N=M\), where we can ignore property 4. In Figure 2, we visualize how the spiked Wishart model works in both the data and spectral domain. For a detailed explanation, see Appendix A.

## 4 Q-ensemble Independence Regularization

In this section, we present a method of Q-ensemble independence regularization that is computationally feasible through the utilization of a spiked Wishart model. We bring the concept of ensemble Q-learning to the spiked Wishart model as \(Y_{}\) representing learned Q-value by Q-learning algorithm and \(W_{0}\) representing the perfectly independent Q-ensemble. If \(W\) dominates \(Y\), \(Y\) follows independent assumptions of ensemble Q-learning. If not, a collapse of Q-value and high correlation occurs, which cannot be benefited by the ensemble method. By integrating with ensemble Q-learning and Q-ensemble independence testing, we propose a tractable Q-ensemble independence loss with gain \(\), which is further described in the following section. To help understand the independence and diversity of Q-ensemble more clearly, we mentioned that \(\) can be simply considered as a loss gain (weight) for Q-ensemble independence regularization before providing a detailed explanation about SPQR and \(\). A higher \(\) represents highly-likely-to-independent.

### Diversity and Independence of Q-ensemble

To clarify our work, we distinguish between the notions of diversity and independence. Numerous ensemble Q-learning methods improve their performance by minimizing the overestimation bias and the variance of the Bellman target estimate. However, such works assume that the biases \(e_{sa}^{i}\) are zero-mean independent and identically distributed over \(i\), although early collapse, a high correlation between Q-ensemble, occurs due to the shared Bellman target. On the other hand, diversification methods such as EDAC develop an ensemble diversification objective to reduce the number of ensemble networks required. In particular, EDAC increases the variance of the ensemble to penalize OOD actions. In our work, we focus on the independence of a Q-ensemble to reduce the effect of correlated behavior resulting from a shared Bellman target. Roughly speaking, diversification can be interpreted as flattening the distribution of a Q-ensemble, while independence regularization can be interpreted as promoting

Figure 3: Mean of predicted Q-value of SPQR-SAC-Min with various \(\) on hopper-random dataset, averaged over 4 seeds.

an i.i.d. sampling procedure for a given distribution. We verify that our proposed regularization successfully exploits both the conservative behavior and the diversification of a Q-ensemble. The Q-ensemble independence regularization empirically shows a large performance gain by controlling the conservatism, as shown in Figure 3, similar to the performance improvement of the ensemble method caused by the conservatism. For further details, see Appendix E.

### Tractable Loss for Q-ensemble Independence

We construct a tractable testing criterion to determine whether a distribution is independent in Theorem 4.1. \(D_{KL}(p_{}(),p_{sc}())\) does not require any prior assumption about \(_{0}\) and \(_{}\) since in spectral domain, Wigner's semicircle law is universal for all random matrices. Proof of Theorem 4.1 is shown in Appendix B.

**Theorem 4.1**.: _Following definition 3.2, as \(N\) with probability at least \(1-\), following test \(T()\) is optimal._

\[T()=(D_{KL}(p_{}(),p_{sc}()) )=H_{0}:Y_{0}\\ H_{1}:Y_{}\]

We now describe a practical Q-ensemble independence loss for ensemble Q-learning using the spiked Wishart model and Theorem 4.1. Let \(Y\) and \(u,v\) for the spiked Wishart model denote a Q-value matrix and an unknown signal. By the test \(T()\), if \(D_{KL}(p_{}(),p_{sc}())<\) or is minimized, it is theoretically guaranteed that \(Y\) is a random matrix, i.e. a Q-ensemble is independently distributed.

In conclusion, we propose \(D_{KL}(p_{}(),p_{sc}())\) as a Q-ensemble independence regularization loss for ensemble Q-learning, without any prior assumption such as uniform distribution.

### Algorithm

As described in Section 3.1, ensemble Q-learning has crucial but inaccurate assumptions about independence. To address the issue of insufficient independence, we propose **SPQR**: spiked Wishart Q-ensemble independence regularization for reinforcement learning, which we describe in Algorithm 1. Using the random matrix theory and a spiked random model lens, SPQR minimizes \(D_{KL}(p_{}(),p_{sc}())\) as a form of regularization loss, denoted as SPQR loss, on the baseline ensemble Q-learning algorithms. \(D_{KL}(p_{}(),p_{sc}())\) penalizes for going too far from an independent Q-ensemble, which means promoting Q-ensemble to follow \(_{0}\). We visualize our concept in Figure 4. To compute the SPQR loss using \(p_{}()\), we need to construct a symmetric Q-matrix \(Y\) filled with Q-values. Then, we normalize the Q-matrix \(Y\) with a zero-mean and unit variance to compute the Wigner's semicircle distribution \(p_{sc}()\) with \(=1\). Finally, we compute eigenvalues with a numerically stable algorithm by constructing the ESD \(p_{}()\) and calculate the KL divergence between \(p_{}()\) and \(p_{sc}()\). The loss function \(_{SPQR}\) is computed as a expectation of \(D_{KL}(p_{}(),p_{sc}())\) over the batch data. A detailed explanation of implementations and algorithms is shown in Appendix C.

```
1:Input: Q-network \(l_{k}=\{Q_{_{i}}(s^{},a^{})\}_{i=1}^{N}\)
2:\(Y^{D D}\), \(D=-1}{2}\):
3:\(Y_{pq}l_{Dp+q},&p q\\ Y_{pq},&\)
4:\(Y(Y-_{Y})/_{Y}\)
5:\(\{_{i}\}_{i=1}^{N} Eigen(}Y)\)
6:\(p_{esd}()=_{i}(-_{i})\)
7:\(p_{wigner}()=-^{2}}{2}\)
8:\(_{SPQR}_{j}_{i}p_{esd}(_{i} )(_{i})}{p_{wigner}(_{i})}\)
9:Output: SPQR loss \(_{SPQR}\) ```

**Algorithm 2** SPQR Loss

## 5 Experimental Results

To reveal the validity and universality of SPQR from algorithms to tasks, we implement SPQR in various ensemble Q-learning algorithms and various online/offline RL tasks. We implement SPQR in SAC-Ens, REDQ, SAC-Min, and CQL, and evaluate them in online MuJoCo Gym tasks, offline D4RL Gym, Franka Kitchen, and Antmaze tasks. We denote the SPQR-implemented version of each ensemble algorithm as SPQR-\(*\), such as SPQR-SAC-Ens. All the following implementations use the same hyperparameter and experimental protocol with a baseline. For fair evaluation, the SPQR code is written on top of each official baseline code, REDQ1, EDAC2, and CQL3. Implementation of SPQR can be done by only **50** lines and training time only increases by **5%**. We empirically show that SPQR outperforms baseline ensemble Q-learning algorithms in both online and offline RL tasks. See the implementation details in Appendix D.

Is SPQR really affecting independence?We perform independence hypothesis testing and visualize the spectral distribution to investigate whether SPQR really affects the independence of the Q-ensemble. First, we analyze the performance gain of SPQR from a spike reduction perspective. We train a Q-ensemble using SAC-Min and SPQR-SAC-Min on the hopper-random dataset and test it on the hopper-full-replay dataset. Figure 6 and Table 1 show the evaluation results for the spike-reducing phenomenon of SPQR, which deviates less from Wigner's semicircle law. Compared to SPQR-SAC-Min with \(=0.1\), SAC-Min has significantly more spikes, in particular, 2434 spikes compared to 2182 spikes over 25600 data. For extreme regularization gain, \(=100\), the spike is drastically reduced to 698, **71%** less than the baseline algorithm. We also plot a spike histogram for another diversification method, EDAC. EDAC increases spikes to **54%** more than SAC-Min, which can be interpreted as SPQR being the orthogonal method with EDAC.

We also demonstrate the \(^{2}\) test with null hypothesis \(H_{0}:P(Q_{i},Q_{j})=P(Q_{i})P(Q_{j})\) and significant level \(=0.025\) for independence hypothesis testing on the same experimental setting above. Since SPQR promotes being "more" independent, we use the proportion of data that can accept an independent hypothesis as a measure of more or less independence. In Table 1, we report the proportion of trials over 1000 tests that accept the null hypothesis \(H_{0}\), meaning that the Q-ensemble is independent. These empirical studies can be interpreted as SPQR having Q-ensemble independence regularization power since the acceptance rate increases monotonically as \(\) increases. Orthogonality between SPQR and EDAC is also found from an independence testing perspective. However, reducing spikes and being more independent does not guarantee optimal performance since there is an optimal conservatism for each task. We can find the proper conservatism by controlling the independence regularization power, \(\). More details can be found in Appendix E.

Figure 4: Illustrative example of SPQR. Figure 5: Illustration for building Q-matrix.

[MISSING_PAGE_FAIL:8]

To investigate why SPQR performs better, we evaluate the bias of the Q-value computed by the Monte-Carlo algorithm. The average of the Q-bias decreases with SPQR, which can be interpreted as the relationship between the distribution of the ensemble becoming more independent. We also evaluate the ablation study for the UTD ratio. SPQR-SAC-Ens shows performance improvement and bias alleviation with low standard deviation as the UTD ratio increases although high UTD SAC causes large and unstable bias. In terms of diversity, we also evaluate the standard deviation with different \(\) for SPQR. As \(\) increases, the standard deviation for explored state-action also increases in all training timesteps. In summary, Q-ensemble independence is the key factor in improving the performance of SPQR. Further details and experimental results can be found in Appendix E.

Offline Reinforcement Learning.SAC-Min is formally constructed and evaluated in . We follow the reported value and evaluation protocol from the paper. We consider baseline _behavior cloning_ (BC), CQL, EDAC, and SAC-Min. The evaluation results of SPQR-SAC-Min compared with SAC-Min and other baselines are shown in Table 2. For most tasks, SPQR outperforms all baselines well. Specifically, for \(*\)-random dataset, SPQR improves SAC-Min by about **16%** when CQL and EDAC fail for \(*\)-random dataset. It implies that SPQR is robust for the quality of data. Since SPQR gives more conservatism, which shows identical behavior with increasing ensemble size \(N\), SPQR-SAC-Min needs a smaller number of ensemble networks to outperform SAC-Min. For the hopper-medium, SAC-Min uses 500 Q-networks and SPQR uses only 50 Q-networks, which is about **90%** efficient in terms of computational cost. It implies that the independence regularization of the Q-ensemble becomes computationally efficient.

We also implement SPQR to CQL(\(\)), denoted as SPQR-CQL-Min. Referring to the implementation details in the paper and the original baseline code of CQL, the original CQL uses a twin Q-function trick, which is equivalent to SAC-Min with \(N=2\). We modified CQL as CQL-Min to use \(N\)

  
**Gym** & **BC** & **CQL-Min** & **EDAC** & **SAC-Min** & **SPQR-SAC-Min** \\  walker2d-random & \(1.3 0.1\) & 7.0 & \(16.6 7.0\) & \(21.7 0.0\) & \(\) \\ walker2d-medium & \(70.9 11.0\) & 74.5 & \(92.5 0.8\) & \(87.9 0.2\) & \(\) \\ walker2d-export & \(108.7 0.2\) & **121.6** & \(115.1 1.9\) & \(116.7 0.4\) & \(115.2 1.3\) \\ walker2d-medium-expt & \(90.1 13.2\) & 98.7 & \(114.7 0.9\) & \(116.7 0.4\) & \(\) \\ walker2d-medium-replay & \(20.3 9.8\) & 32.6 & \(87.1 2.3\) & \(78.7 0.7\) & \(\) \\ walker2d-full-replay & \(68.8 17.7\) & 98.96 & \(99.8 0.7\) & \(94.6 0.5\) & \(\) \\ halfcheeta-random & \(2.2 0.0\) & **35.4** & \(28.4 1.0\) & \(28.0 0.9\) & \(33.5 2.5\) \\ halfcheeta-medium & \(43.2 0.6\) & 44.4 & \(65.9 0.6\) & \(67.5 1.2\) & \(\) \\ halfcheeta-expert & \(91.8 1.5\) & 104.8 & \(106.8 3.4\) & \(105.2 2.6\) & \(\) \\ halfcheeta-medium-expert & \(440.1 16.6\) & 62.4 & \(106.3 1.9\) & \(107.1 2.0\) & \(\) \\ halfcheeta-medium-replay & \(37.6 2.1\) & 46.2 & \(61.3 1.9\) & \(63.9 0.8\) & \(\) \\ halfcheeta-full-replay & \(62.9 0.8\) & _82.07_ & \(84.6 0.9\) & \(84.5 1.2\) & \(\) \\ hopper-random & \(3.7 0.6\) & 7.0 & \(25.3 1.04\) & \(31.3 0.0\) & \(\) \\ hopper-medium & \(54.1 3.8\) & 86.6 & **101.6 \(\)** & \(100.3 0.3\) & \(100.2 1.3\) \\ hopper-expert & \(107.7 9.7\) & 109.9 & \(110.1 0.1\) & \(110.3 0.3\) & \(\) \\ hopper-medium-expert & \(53.9 4.7\) & 111.0 & \(110.7 0.1\) & \(110.1 0.3\) & \(\) \\ hopper-medium-replay & \(16.6 4.8\) & 48.6 & \(101.0 0.5\) & \(101.8 0.5\) & \(\) \\ hopper-full-replay & \(19.9 12.9\) & _104.85_ & \(105.4 0.7\) & \(102.9 0.3\) & \(\) \\  Average & 49.9 & _70.9_ & **85.2** & **84.5** & **89.6** \\ 
**Franka Kitchen** & **BC** & **SAC-Min** & **BEAR** & **CQL-Min** & **SPQR-CQL-Min** \\  kitchen-complete & \(33.8\) & \(15.0\) & \(0.0\) & \(43.8\) & \(\) \\ kitchen-partial & \(33.8\) & \(0.0\) & \(13.1\) & \(49.8\) & \(\) \\ kitchen-mixed & \(47.5\) & \(2.5\) & \(47.2\) & \(51.0\) & \(\) \\  Average & 38.4 & 5.8 & 20.1 & 48.2 & **52.6** \\ 
**Antmaze** & **BC** & **SAC-Min** & **BEAR** & **CQL-Min** & **SPQR-CQL-Min** \\  umaze & 65.0 & 0.0 & 73.0 & 74.0 & \(\) \\ umaze-diverse & 55.0 & 0.0 & 61.0 & \(\) & \(80.0 0.0\) \\ medium-play & 0.0 & 0.0 & 0.0 & 61.2 & \(\) \\  Average & 40.0 & 0.0 & 44.7 & 73.1 & **84.4** \\   

Table 2: Normalized average returns of SPQR-SAC-Min and SPQR-CQL-Min on D4RL Gym, Franka Kitchen, and Antmaze tasks, averaged over 4 random seeds. Except for SPQR-\(*\), the evaluated values of baselines are reported refer to , and reproduced for \(*\)-full-replay tasks of CQL-Min. Reproduced values are indicated as _italic_ font.

Q-networks to compute the Bellman target. Hence, the original CQL can be called CQL-Min with \(N=2\). We evaluate SPQR-CQL-Min on D4RL Franka Kitchen and Antmaze, one of the most complex and sparse tasks in D4RL.

In Table 2, we evaluate SPQR-CQL-Min with other baseline methods. SPQR-CQL-Min outperforms all baseline algorithms, notably 2.6 times better than BEAR. For Franka Kitchen tasks, SPQR **consistently** improves CQL by **10%** for all three tasks, which can be interpreted as robustness over the quality of data. Surprisingly, SPQR achieves the **maximum score** on 3 different Antmaze tasks, which outperforms all existing algorithms. Further details and results are presented in Appendix D and E.

Can other diversification methods benefit from the SPQR?Since SPQR regularizes Q-ensemble independence, which is the orthogonal method with other diversification methods, we implement SPQR in EDAC to benefit from the combination of two orthogonal methods. We evaluate SPQR-EDAC for the tasks where EDAC outperforms the baseline algorithm SAC-Min. SPQR and SPQR-EDAC show large performance gains over SAC-Min, as shown in Table 3.

## 6 Conclusions

We propose SPQR, a Q-ensemble independence regularization framework for ensemble Q-learning by applying random matrix theory and a spiked random model. We theoretically propose a tractable loss for Q-ensemble independence. The proposed theoretical approach does not require any prior assumption on the Q-function distribution, such as uniform distribution or i.i.d. In addition, we present the difference between diversity and independence and empirically show the orthogonality of the diversification method and SPQR. Furthermore, SPQR serves as a conservatism control framework for ensemble Q-learning by effectively regularizing independence with various empirical evidence. We also empirically show that SPQR is computationally efficient and outperforms many existing ensemble Q-learning algorithms on various online and offline RL benchmark tasks.