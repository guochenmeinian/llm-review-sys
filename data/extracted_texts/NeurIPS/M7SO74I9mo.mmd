# The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers

Hussein Mozannar\({}^{12}\)1 Valerie Chen\({}^{3*}\) Mohammed Alsobay\({}^{1}\) Subhro Das\({}^{24}\) Sebastian Zhao\({}^{5}\) Dennis Wei\({}^{24}\) Manish Nagireddy\({}^{24}\) Prasanna Sattigeri\({}^{24}\) Ameet Talwalkar\({}^{3}\) David Sontag\({}^{12}\)

\({}^{1}\) Massachusetts Institute of Technology Lab MIT-IBM Watson AI

\({}^{3}\) Carnegie Mellon University IBM Research UC Berkeley

###### Abstract

Evaluation of large language models for code has primarily relied on static benchmarks, including HumanEval , or more recently using human preferences of LLM responses. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks or more preferred LLM responses translate to programmer productivity when coding with LLMs, including time spent coding. We introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional--a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better proxy signals. We open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.

## 1 Introduction

Coding benchmarks such as HumanEval  and MBPP  play a key role in evaluating the capabilities of large language models (LLMs) as programming becomes a valuable application through products such as GitHub Copilot  and ChatGPT . These benchmarks quantify LLM abilities by measuring how well a model can complete entire coding tasks. As LLMs are increasingly adopted as programmer assistants--providing chat responses or autocomplete suggestions, rather than full code generations--prior works have argued for bringing humans-in-the-loop to evaluate LLMs [31; 11]. A predominant human-centric approach collects human preference judgments of intermediate LLM outputs, whether between pairs of LLM responses (e.g., Chatbot Arena ) or, for coding in particular, using programmer acceptance rates of LLM suggestions (e.g., in productssuch as Github Copilot ). However, such evaluation may not capture the LLM's downstream impact on programmer productivity.

Evaluating the utility of LLMs on downstream productivity requires conducting user studies where programmers code with LLM assistance. While a set of small-scale user studies have been conducted to primarily build a qualitative understanding of how programmers use LLM assistance, they are typically restricted to evaluations on one model and one form of LLM support, primarily relying on commercial tools like Github Copilot or ChatGPT . To enable evaluations of a broader set of LLMs and lower the barrier to conducting these studies, we introduce an online evaluation platform, RealHumanEval (Figure 1). The platform consists of a code editor where programmers can solve coding tasks with two common forms of LLM assistance: programmers can either ask questions to the LLM through a chat window or receive code completion suggestions through an autocomplete system inside the editor. The interface also supports executing and testing code and logging telemetry which can be used to compute productivity metrics, including time to complete a task or number of tasks completed, and preference metrics, including average acceptance rates of suggestions and the likelihood of copying code from chat responses.

Using RealHumanEval, we conduct a user study with 213 participants to understand the effect of a model's benchmark performance and the form of LLM assistance on downstream productivity metrics. Each participant was assigned to one of seven conditions: a control condition with no LLM support, three conditions with autocomplete support from either CodeLlama-7b , CodeLlama-34b , or GPT-3.5-turbo-instruct, and finally three conditions where the editor is equipped with

Figure 1: We introduce RealHumanEval, an end-to-end online evaluation platform of LLM-assisted coding through autocomplete suggestions and chat support. The goal of RealHumanEval is to facilitate human-centric evaluation of code LLMs, simplifying the workflow for researchers to conduct user studies to measure the effect of LLM assistance on downstream human productivity and preferences. We selected 3 families of LLMs of varying sizes (GPT-3.5, CodeLlama-34b, CodeLlama-7b)use RealHumanEval to study whether static benchmark performance or programmer preference judgments are aligned with programmer productivity.

a chat window powered by the chat variants of the previous models. We deliberately select three model families with increasingly higher benchmark performance and consider model pairs within each family with similar benchmark performance to understand the effect of autocomplete versus chat assistance. Through the study, we collected a dataset of interactions on 771 coding total tasks, where 5204 autocomplete suggestions were shown and 775 chat messages were sent.

Overall, we find that improving a model's base performance on existing coding benchmarks leads to gains in human productivity, particularly in the time spent completing tasks. These trends were present across both chat and autocomplete interactions, validating the potential "generalizability" of benchmarks to more realistic contexts. However, we observe that gaps in benchmark versus human performance are not necessarily proportional, suggesting that further gains in benchmark performance do not necessarily translate into equivalent gains in human productivity. We also investigated whether human preference metrics, such as the average acceptance rate of suggestions and the likelihood of copying code from chat responses, are aligned with productivity metrics. While these preference metrics are readily available in real deployments of LLM systems compared to task completion time and thus can be attractive proxy metrics , we find that they are only correlated with programmer perceptions of LLM helpfulness but not necessarily with actual programmer performance. The dissimilar findings between benchmarking and human preference metrics highlight the importance of careful evaluation to disentangle which metrics are indicative of downstream performance.

In summary, our contributions are as follows: (1) an open-source platform RealHumanEval to encourage more human-centric evaluations of code LLMs, (2) an evaluation of 6 code LLMs of varying performance using RealHumanEval to provide insights into the alignment and discrepancies between benchmark performance and human preferences with downstream user productivity. Our findings emphasize the importance of studying how programmers interact with code LLMs through user studies to identify nuances in programmer-LLM interactions. Finally, (3) we release the dataset of interactions collected from this study to guide the development of better coding assistants.2

## 2 Related Work

_Coding Benchmarks._ Benchmarks are essential for tracking the progress of LLMs, and coding benchmarks are a key piece [1; 29; 57; 21]. Moreover, the coding ability of an LLM can be informative of its reasoning abilities ; thus, performance on coding benchmark is of broader interest. While HumanEval  and MBPP  are the most commonly used coding benchmarks, many extensions and further benchmarks have been proposed [34; 40; 59; 33; 23; 26; 56; 55], we highlight a few: EvalPlus extends HumanEval's test cases , MultiPL-E  to other languages, ReCode with robustness checks , HUMANEVALPACK  with code repair and explanation tasks, and buggy-HumanEval  with bugs in the reference code. Relatedly, the DS-1000  benchmark evaluates models' abilities on data science problems that require using external libraries. More involved evaluations include the multi-turn program evaluation benchmark  and SWE-bench , which requires the LLM to resolve GitHub issues. While existing benchmarks evaluate a diverse set of LLM behaviors across models, these benchmarks do not, however, include a programmer-in-the-loop, as there would be in a real-world setup. Our evaluation complements this existing line of work by conducting a user study, where programmers put these behaviors to the test in realistic scenarios.

_Preference Metrics._ Instead of relying solely on coding benchmarks' pass@k metrics, which consider only the functional correctness of LLM-generated code, recent work has advocated for incorporating human preferences, which may better reflect how LLM code could be useful to a programmer without necessarily being functionally correct . Preferences are generally collected after a single turn (e.g., after a single LLM response or suggestion) and thus can be collected at scale [5; 11] or even simulated with LLMs [18; 58]. Given that preferences are only a form of intermediate feedback, in this study, we evaluate whether human preferences provide a signal for downstream productivity gains when coding with LLMs.

_Programmer-LLM Interaction._ Prior work conducting user studies where programmers code with LLM assistance has primarily focused on two forms of LLM support, autocomplete suggestions  and chat dialogue . While these studies have made progress in understanding programmer-LLM interactions, all studies only consider one LLM--often Copilot or ChatGPT--and one form of LLM support--either autocomplete or chat, making it difficult to compare outcomes and metrics _across models_ and _across forms of support_. We contribute a web platform RealHumanEval to enable ease of human-centric evaluation of more models and forms of support (see Appendix A for a more in-depth comparison). Beyond applications of coding assistance, our study contributes to the broader literature studying human interactions with LLMs .

## 3 RealHumanEval

We introduce RealHumanEval, a web-based platform to conduct human-centric evaluation of LLMs for programming through the workflow shown in Figure 1. We created RealHumanEval to facilitate large-scale studies of programmers coding with LLMs, eliminating the need for participants to perform any additional installation of a bespoke IDE or study-specific extension or to have access to special hardware to serve study-specific models.

**Interface.** As shown in Figure 2, RealHumanEval incorporates many basic features of common code editors and the functionality of programming interview sites such as LeetCode. Given a coding task that consists of a natural language description, partial code (e.g., a function signature), and unit tests that evaluate the task, RealHumanEval allows the programmer to write code with assistance from an LLM to complete the task. The platform has a panel that displays the natural language description of a task, as shown in Figure 2(a), alongside partial code to solve the task. Participants then write their code for the task in the code editor and can test their code with a button that checks the code against test cases and runs their code directly. The editor displays any errors, if available, and whether the code passes the unit test. Once the programmer completes the task, a new task can be loaded into the

Figure 2: We introduce RealHumanEval, an online evaluation platform for LLM-assisted coding. The platform consists of (a) a customizable task description, (b) the code editor which shows autocomplete suggestions in grey, and (c) the chat assistant. Above the editor, users can check their task progress and the amount of time left, reset the editor, change the editor theme, and view study instructions. Below the editor, they can run and submit their code.

interface. For our user study, we only use a single code editor file, however, RealHumanEval can support multiple-file projects.

**Forms of LLM Assistance.** RealHumanEval supports two forms of LLM assistance: _autocomplete-based_ and _chat-based_. Examples of autocomplete and chat assistants include GitHub's Copilot , Replit's Ghostwriter , Amazon CodeWhisperer , and ChatGPT . In _autocomplete-based_ assistance, the programmer writes code in an editor, and the LLM displays a code suggestion inline, which is greyed out as shown in Figure 2(b). The LLM is assumed to be able to fill in code given a suffix and prefix. A suggestion, based on the current code body in the editor, appears whenever the programmer pauses typing for more than two seconds or when the programmer requests a suggestion by pressing a hotkey. The programmer can accept the suggestion by pressing the tab key or reject it by pressing escape or continuing to type.

In _chat-based_ assistance, the programmer writes code in an editor and has access to a side chat window where the programmer can ask questions and get responses from the LLM, as illustrated in Figure 2(c). The LLM is assumed to be a chat model. The programmer can copy and paste code from the LLM's responses into the editor. Currently, the interface supports any LLM invoked via an online API. Further information on the implementation of both forms of assistance is in Appendix B.

**Telemetry logging.** RealHumanEval logs all user behavior, including interactions with LLM support. For each autocomplete suggestion, we log the following tuple \(\{(P_{i},S_{i}),R_{i},A_{i}\}_{i=1}^{n}\) where \((P_{i},S_{i})\) is the prefix and suffix of the code based on cursor position at the time of suggestion \(i\), \(R_{i}\) is the LLM suggestion, and \(A_{i}\) is a binary variable indicating whether the suggestion was accepted. All the logs are stored in a dataset \(_{}\). For chat-assistance, we log for each user message the following tuple \(\{X_{i},M_{i},R_{i},C_{i}\}_{i=1}^{n}\) where \(X_{i}\) is the code at the time of message \(i\), \(M_{i}\) is the user message (including prior chat history), \(R_{i}\) is the response from the LLM for the message, and \(C_{i}\) is the number of times code was copied from the LLM's response. All the logs are stored in a dataset \(_{}\). Moreover, every \(15\) seconds, the interface saves the entire code the user has written.

**Metrics.** From the telemetry logs, RealHumanEval provides multiple metrics to analyze programmer behaviors: the _number of tasks completed_ (completion is measured by whether the submitted code passes a set of private test cases), _time to task success_ (measured in seconds), _acceptance rate_ (fraction of suggestions shown that are accepted, for autocomplete), and _number of chat code copies_ (counting when user copies code from LLM response, for chat) among other metrics.

## 4 Study Design

Using RealHumanEval, we conducted a user study to evaluate (1) the impact of LLM assistance on programmer performance as a function of the LLM's performance on static benchmarks and (2) whether human preference metrics correlate with programmer productivity metrics.

**Overview.** For the entire duration of the study, participants are randomly assigned either to a control group, where they experienced the no LLM condition, or to the LLM-assisted group, where they experienced the _autocomplete_ or _chat support_ condition. For autocomplete-based support, the window in Figure 2(c) is hidden. For chat-based support, no autocomplete suggestions are shown in Figure 2(b). Participants are only assigned to one condition to minimize context switching, given the relatively short duration of the study. The study was conducted asynchronously using the RealHumanEval platform; participants were told not to use any outside resources (e.g., Google), and cannot paste any text originating outside the app into the editor. Specific instructions are in Appendix B. The first problem was a simple task (i.e., compute the sum and product of a list) for participants to familiarize themselves with the interface. Participants are given 35 minutes to complete as many tasks as possible. If 10 minutes pass and the participant has not completed the task, a button appears to provide the option to skip the task.

**Tasks.** We designed 17 coding tasks for the platform that can be categorized into three categories: (a) _algorithmic problems_ from HumanEval (e.g., solve interview-style coding), (b) _data manipulation problems_ (e.g., wrangle input dataframe into desired output), and (c) _editing and augmenting code tasks_ (e.g., fill in provided code scaffold to achieve desired behavior). While the set of tasks does not evaluate all types of coding problems exhaustively, they do capture tasks of varying difficulty and solutions of varying length, as well as the use of different programming skills, leading to varying opportunities to benefit from LLM support. We chose 17 tasks to build diversity across tasks while being able to collect enough samples per task. We ensured that no LLM model considered in the study, in addition to GPT-4, could solve all tasks perfectly, so that programmers would not simply accept all LLM suggestions and that each task could be solved in under 20 minutes by an experienced programmer (validated through pilots with the authors and volunteer participants), to ensure that these were reasonable questions to consider for a user study. These 17 tasks are distributed into five sets, where each set consists of a different mix of task types in varying orders but shares the first two tasks. Each participant is randomly assigned to one of these sets. The LLMs are not aware of the task descriptions unless the programmer types them in the editor or chat window; this is to simulate the real world where the task description represents the programmer's hidden true intent. We provide examples of the coding tasks in Appendix C and in full in the supplementary materials.

**Conditions.** For the autocomplete conditions, we chose base LLM models that naturally generate next-word predictions, whereas the "chatty" variants of the base models are employed for the chat conditions. To evaluate the effect of LLM capabilities, we selected three types of models that demonstrate clear gaps in performance on existing benchmarks (as shown in Figure 8). In total, we selected 6 LLMs for our study: 4 from the Code Llama family  (CodeLlama-7b, CodeLlama-7b-instruct, CodeLlama-34b, CodeLlama-34b-instruct), along with two models from the GPT series  (GPT-3.5-turbo and GPT-3.5-turbo-instruct). To avoid confusion, we refer to the autocomplete conditions by the base name of the model: CodeLlama-7b, CodeLlama-34b and GPT-3.5 (refers to GPT-3.5-turbo-instruct); and the chat conditions by the base name of the model with chat: CodeLlama-7b (chat) (refers to CodeLlama-7b-instruct), CodeLlama-34b (chat) (refers to CodeLlama-34b-instruct) and GPT-3.5 (chat) (refers to GPT-3.5-turbo). Specific choices of parameters, system prompts, and other considerations are provided in Appendix D.

**Participants.** We recruited 229 total participants from university mailing lists and social media to capture a range of coding experiences. We verified that participants were above 18 years of age, resided in the United States, and correctly completed a simple Python screening question. Out of the 229 participants, we filtered out those who did not complete any task or did not write code for a period of 15 minutes during the study to arrive at 213 final participants. Of the 229 participants, 34% identify as Female. In terms of occupation, 79% are Undergraduate or Graduate Students studying computer science, 13% work in Software Development and 7% work in AI. While a majority of our participants were students, only 34% of participants had less than 2 years of professional programming experience. We ensured that participants were roughly equally distributed across experimental conditions based on programming experience. 11% had never used any form of AI for coding while 67% of participants use AI at least once a week for coding. Participants were provided with a $15 Amazon gift card as compensation. This study was approved by institutional IRB review.

**User study metrics.** To quantify the benefits of LLM assistance on the number of tasks completed and time to task success, we report the gap between each condition where some form of LLM assistance was provided and the control no LLM condition, which we denoted as \(\). For example, for time to task success, \(<0\) for LLM support indicates that participants took less time to complete tasks with the LLM. In addition to the quantitative metrics, we also ask post-study questions to obtain participants' subjective measures of their interactions with the LLM: we ask participants to rate the helpfulness of the LLM on a scale of \(\) and to describe how the LLM support provided (if any) was helpful and how it could be improved. We also measure two preference metrics, suggestion acceptance rate and percentage of chat code copies.

## 5 Results

We report results for data collected from 213 participants split across the seven conditions; since condition assignment is random, each condition has around 25 to 35 participants (except for No LLM,which has 39 participants). Participants completed a total of 771 coding tasks (mean of 3.6 tasks per person) on average in 334 seconds (std=238 seconds), were shown 5204 autocomplete suggestions (\(|_{}|\)), with an average 11.3% acceptance rate, and received 775 messages from the chat LLMs (\(|_{}|\)), with 29.6% of messages having at least one copy event. In the following analyses, we conduct ordinary least squares regressions with Benjamini-Hochberg correction and use a significance level of 0.05. A more in-depth analysis of both datasets and results is in Appendix E.

Providing LLM assistance reduces the amount of time spent coding.To measure the productivity gains of LLM assistance to programmers, we look at two metrics: the amount of time spent coding (in seconds) and the number of tasks completed. We first distill our observations for each metric by comparing performance for each model type (i.e., combining autocomplete and chat models) against the No LLM condition.3 As shown in Figure 3(a), we find that compared to the No LLM setting where participants spent an average of 400 seconds per task, both GPT-3.5 and CodeLlama-34b models reduce the amount of time spent per task by an average of 78 and 64 seconds respectively (\(p=0.04\) and \(p=0.12\)). In contrast, CodeLlama-7b models slightly increase the average time spent on a task by 10 seconds. However, we do not observe statistical differences across _any_ of the conditions in

Figure 3: We measure the effect of LLM support on user study performance on mean task duration in seconds (a,c) and number of tasks completed across model type (b,d). In (a) and (b), we compute \(\), the difference between each model type—aggregating conditions corresponding to the same model type, e.g., Codellama7b and Codellama7b (chat)—and the No LLM condition for each metric. In (c) and (d), we break down the same metrics for each of the seven conditions and mark the percentage improvement over the No LLM condition. We observe that better LLM support can improve task completion time, but not necessarily increase the number of tasks completed. Error bars denote _standard errors_—the standard deviation divided by the square root of the sample size (i.e., across participants), where each participant contributes a single data point.

the number of tasks completed, as shown in Figure 3(b), meaning no form of LLM support allowed programmers to solve _more_ problems than they otherwise would have on their own. We hypothesize that benefits in task completion were not observed because of the short duration of the user study (35 minutes) and the amount of time it takes to complete each task, though we do observe an increase in the number of tasks attempted.

We now consider how our observations using RealHumanEval implicate the broader code LLM evaluation landscape, specifically the use of (1) static benchmarks and (2) human preference metrics.

**(1) Are LLM performance on static benchmarks informative of user productivity with LLM assistance?** We find that improvements in model-specific evaluations on benchmarks also improve human performance on both productivity metrics in the user study (i.e., CodeLlama-7b models led to the least number of tasks completed, while GPT-3.5 models led to the most). Interestingly, this trend holds even when considering metrics with chat and autocomplete separately, in Figure 3(c-d). _However_, significant gaps in benchmark performance result in relatively indistinguishable differences in terms of human performance. For instance, CodeLlama-34b (chat) is 19% better over CodeLlama-7b (chat) models on HumanEval, and participants are 22.8% (95% CI [2.8, 38.7]) faster on average to complete a task with 34b vs 7b. Yet, GPT-3.5 (chat) model outperforms CodeLlama-34b (chat) by 85% on HumanEval, and yet participants equipped with GPT-3.5 (chat) models are only 8.3% (95% CI [-11.2, 24.6]) faster than those with CodeLlama-34b (chat). While we do not necessarily expect performance gaps to be consistent, this finding suggests that, after a certain point, additional gains on static benchmarks may not translate to practical utility.

**(2) Do human preferences align with productivity?** We also consider programmer preferences for the LLM assistant's suggestions on autocomplete and chat: the average suggestion acceptance rate and the average copies-per-response respectively. While both GPT-3.5 and CodeLlama-34b models reduced the amount of time spent coding over CodeLlama-7b, we do not find the same trends reflected in human preferences. As shown in Figure 4(a), we find that suggestions from CodeLlama-34b are less likely to be accepted at \(5\%\) compared to \(15\%\) and \(9\%\) for GPT-3.5 and CodeLlama-7b (\(p<0.001\) and \(p=0.19\)). The same ordering occurs for the percentage of chat messages copied (\(27\%\) versus \(35\%\) and \(29\%\), though not significant) in Figure 4(b). By analyzing the participants' qualitative responses, discussed in Section F, we identify potential factors that may have contributed to these preferences, including a perceived lack of context in CodeLlama-34b suggestions and a slight increase in latency in CodeLlama-34b (chat) responses. These results suggest that various external factors that might be difficult to anticipate a priori can easily affect human preferences even if they do not impact downstream productivity.

### Additional User Study Observations

Findings on the effect of the form of LLM support and task type further illustrate the importance of evaluation with humans in the loop.

Figure 4: Measuring participant preferences of different models by the amount of interaction with chat (a) or autocomplete systems (b), with standard error. We find that preference judgments align with the reported helpfulness of the LLM assistant post-study (c); however, these preferences do not necessarily align with their actual task performance.

**Chat support is perceived to be more helpful than autocomplete support.** Even though autocomplete and chat variants obtained similar performance on static benchmarks and participant performance in both conditions conditioned on a model type was relatively similar, we observe that chat models are rated by participants in the post-study questions as significantly more helpful than autocomplete models (\(p<0.001\)), as shown in Figure 4(c). Again, we observe that CodeLlama-34b models tend to be rated as less helpful (3.3 out of 10), than the other two models (4.19 and 5.09 out of 10 for CodeLlama-7b and GPT-3.5).

**The benefits of LLM assistance can vary by task type.** We also analyze the time spent on each task category, comparing when participants have access to LLM assistance versus the control condition. As shown in Figure 12, we find suggestive evidence that LLM assistance was particularly effective in reducing the time programmers needed to solve data manipulation tasks, by 28.35%, and slightly less so for problems that required editing and augmenting existing code, by 13.48%. In contrast, we found that LLMs were unhelpful on algorithmic problems, increasing the amount of time spent by 11.7%. A breakdown by individual task is in Appendix E.

## 6 Discussion

In this work, we introduce RealHumanEval, a human-centric evaluation platform for code LLMs, and conduct a user study using the platform to measure programmer productivity assisted by different LLMs. We believe RealHumanEval can be adopted to evaluate newly released LLM models in a more meaningful way and become a standard for evaluation. To enable this, our interface is designed to be easily repurposed for future user studies and evaluations by the community and extended to evaluate new ways of interacting with LLMs for programming.

**Recommendations for future work.** We summarize participant suggestions on how coding assistants could be improved (more detail in Appendix F). Participants overwhelmingly felt that LLMs struggled to infer the appropriate _context_ to provide the most useful support from the information available, highlighting the need for benchmarks that capture settings where LLMs need to infer intent from partial or fuzzy instructions. The suggestion also underscores the importance of evaluating LLMs with humans-in-the-loop; we recommend the community leverage and build on RealHumanEval to evaluate new LLMs' coding abilities. There are also opportunities to improve autocomplete and chat assistants to be better programming partners . For example, autocomplete systems might benefit from personalization of when participants would benefit from suggestions and dynamically adjusting the length, while chat-based systems could be improved to have better, more tailored dialogue experience and better integration with the editor. Toward these goals, we release the datasets of user interactions that can be leveraged as signals of user preferences and behavior patterns.

**Limitations.** Firstly, we acknowledge that a set of 17 coding tasks does not span the entire set of tasks a professional programmer might encounter in their work and may limit the generalizability of our evaluations of the 6 models. We encourage future work to leverage RealHumanEval to conduct further studies with a more extensive set of tasks. Second, the coding tasks we used are of short duration, while real-world programming tasks can take hours to months. This presents a trade-off in study design: short tasks allow us to evaluate with more participants and models in a shorter period but give us a less clear signal compared to longer-term tasks. Third, RealHumanEval does not fully replicate all functionality existing products such as GitHub Copilot may have so the study may underestimate exact productivity benefits. Such products are complex systems comprising more than a single LLM, where many details are hidden and thus not easily replicable. We release RealHumanEval to enable others to build more functionality in an open-source manner.

**Societal implications.** While our evaluations focused on productivity metrics, there are additional metrics of interest that may be important to measure when studying programmer interactions with LLM support. On the programmer side, further evaluations are needed to understand whether programmers appropriately rely on LLM support  and whether LLM support leads to potential de-skilling . Further, our metrics do not consider potential safety concerns, where LLMs may generate harmful or insecure code [42; 44].