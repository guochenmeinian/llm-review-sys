# The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning

Anya Sims

University of Oxford

anya.sims@stats.ox.ac.uk

&Cong Lu

University of Oxford

&Jakob N. Foerster

FLAIR, University of Oxford

&Yee Whye Teh

University of Oxford

###### Abstract

Offline reinforcement learning (RL) aims to train agents from pre-collected datasets. However, this comes with the added challenge of estimating the value of behaviors not covered in the dataset. Model-based methods offer a potential solution by training an approximate dynamics model, which then allows collection of additional synthetic data via rollouts in this model. The prevailing theory treats this approach as online RL in an approximate dynamics model, and any remaining performance gap is therefore understood as being due to dynamics model errors. In this paper, we analyze this assumption and investigate how popular algorithms perform as the learned dynamics model is improved. In contrast to both intuition and theory, _if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail_. This reveals a key oversight: The theoretical foundations assume sampling of full horizon rollouts in the learned dynamics model; however, in practice, the number of model-rollout steps is aggressively reduced to prevent accumulating errors. We show that this truncation of rollouts results in a set of edge-of-reach states at which we are effectively "bootstrapping from the void." This triggers pathological value overestimation and complete performance collapse. We term this the edge-of-reach problem. Based on this new insight, we fill important gaps in existing theory, and reveal how prior model-based methods are primarily addressing the edge-of-reach problem, rather than model-inaccuracy as claimed. Finally, we propose _Reach-Aware Value Learning_ (RAVL), a simple and robust method that directly addresses the edge-of-reach problem and hence - unlike existing methods - does not fail as the dynamics model is improved. Since world models will inevitably improve, we believe this is a key step towards future-proofing offline RL.1

## 1 Introduction

Standard online reinforcement learning (RL) requires collecting large amounts of on-policy data. This can be both costly and unsafe, and hence represents a significant barrier against applying RL in domains such as healthcare  or robotics , and also against scaling RL to more complex problems. Offline RL  aims to remove this need for online data collection by enabling agents to be trained on pre-collected datasets. One hope  is that it may facilitate advances in RL similar to those driven by the use of large pre-existing datasets in supervised learning .

The central challenge in offline RL is estimating the value of actions not present in the dataset, known as the _out-of-sample action_ problem .2 A naive approach results in extreme value overestimation due to bootstrapping using inaccurate values at out-of-sample state-actions . There have been many proposals to resolve this, with methods largely falling into one of two categories: model-free  or model-based .

Model-free methods typically address the out-of-sample action problem by applying a form of conservatism or constraint to avoid using out-of-sample actions in the Bellman update. In contrast, the solution proposed by model-based methods is to allow the collection of additional data at any previously out-of-sample actions. This is done by first training an approximate dynamics model on the offline dataset , and then allowing the agent to collect additional synthetic data in this model via \(k\)-step rollouts (see Algorithm 1). The prevailing understanding is that this can be viewed as online RL in an approximate dynamics model, with the instruction being to then simply "run any RL algorithm on \(\) until convergence", where \(\) is the learned dynamics model with some form of dynamics uncertainty penalty. Existing methods propose various forms of dynamics penalties (see Table 7), based on the assumption that the remaining performance gap compared to online RL is solely due to inaccuracies in the learned dynamics model.

This understanding naturally implies that improving the dynamics model should also improve performance. Surprisingly, we find that existing offline model-based methods completely fail if the learned dynamics model is replaced with the true, error-free dynamics model, while keeping everything else the same (see Figure 1). Under the true dynamics, the only difference to online RL is that in online RL, data is sampled as full-length episodes, while in offline model-based RL, data is instead sampled as \(k\)-step rollouts, starting from a state in the original offline dataset, with rollout length \(k\) limited to avoid accumulating model errors. Failure under the true model therefore highlights that truncating rollouts has critical and previously-overlooked consequences.

We find that this rollout truncation leads to a set of states which, under any policy, can only be reached in the final rollout step (see red in Figure 2). The existence of these _edge-of-reach_ states is problematic as it means Bellman updates (see Equation (1)) use target values that are never themselves trained. This is illustrated in Figure 2, and described in detail in Section 3. This effective "bootstrapping from the void" triggers a catastrophic breakdown in \(Q\)-learning. Concisely, this issue can be viewed as all actions from edge-of-reach states remaining out-of-reach over training. Hence, contrary to common understanding, the out-of-sample action problem central to model-free methods is not fully resolved by a model-based approach. In fact, in Section 3.4 we provide detailed analysis suggesting that this is the predominant source of issues on the standard D4RL benchmark. In Section 6.5 we consequently reexamine how existing methods work and find they are indirectly and unintentionally addressing this issue (rather than model errors as claimed).

Figure 1: _Existing offline model-based RL methods fail if the accuracy of the dynamics model is increased (with all else kept the same)._ Results shown are for MOPO , but note that this failure indicates the _failure of all existing uncertainty-based methods_ since each of their specific penalty terms disappear under the true dynamics as ‘uncertainty’ is zero. By contrast, our method is much more robust to changes in dynamics model. The \(x\)-axis shows linearly interpolating next states and rewards of the learned model with the true model (center\(\)right) and random model (center\(\)left), with results on the D4RL W2d-medexp benchmark (min/max over 4 seeds). The full set of results and experimental setup are provided in Table 1 and Appendix C.2 respectively.

[MISSING_PAGE_FAIL:3]

**Model-Based Offline RL.** Model-based methods  aim to solve the out-of-sample issue by allowing the agent to collect additional synthetic data in a learned dynamics model. They generally share the same base procedure as described in Algorithm 1. This involves first training an approximate dynamics model \(=(,,,,_{0},)\) on \(_{}\). Here, \((s^{}|s,a)\) and \((s,a)\) denote the learned transition and reward functions, commonly realized as a deep ensemble [5; 22]. Following this an agent is trained using an online RL algorithm (typically SAC ), for which data is sampled as \(k\)-step trajectories (termed rollouts) under the current policy, starting from states in the offline dataset \(_{}\). The base procedure of training a SAC agent with model rollouts does not work out of the box. Existing methods attribute this as due to dynamics model errors. Consequently, a broad class of methods propose augmenting the learned dynamics model \(\) with some form of dynamics uncertainty penalty, often based on variance over the ensemble dynamics model [2; 14; 29; 36; 24]. We present some explicit examples in Table 7. Crucially, all of these methods assume that, under the true error-free model, no intervention should be needed, and hence all the uncertainty penalties collapse to zero. In the following section, we show that this assumption leads to catastrophic failure.

## 3 The Edge-of-Reach Problem

In the following section, we formally introduce the _edge-of-reach_ problem. We begin by showing the empirical failure of SOTA offline model-based methods on the _true environment dynamics_. Next, we present our edge-of-reach hypothesis for this, including intuition, empirical evidence on the main D4RL benchmark , and theoretical proof of its effect on offline model-based training.

### Surprising Failure with the True Dynamics

As described in Sections 1 and 2, prior works view offline model-based RL as _online RL in an approximate dynamics model_. Based on this understanding they propose various forms of dynamics uncertainty penalties to address model errors (see Table 7). This approach is described simply as: _"two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; (b) learning a near-optimal policy in this P-MDP"_. This shared base procedure is shown in Algorithm 1, where _"P-MDP"_ refers to the learned dynamics model with a penalty added to address model errors.

This assumption of issues being due to dynamics model errors naturally leads to the belief that the ideal case would be to have a perfect error-free dynamics model. However, in Figure 1 and Table 1 we demonstrate that, _if the learned dynamics model is replaced with the true dynamics,

Figure 2: The previously unnoticed edge-of-reach problem. _Left_ illustrates the base procedure used in offline model-based RL, whereby synthetic data is sampled as \(k\)-step trajectories “rollouts” starting from a state in the original offline dataset. **Edge-of-reach** states are those that can be reached in \(k\)-steps, but which cannot (under any policy) be reached in less than \(k\)-steps. We depict the data collected with two rollouts, one ending in \(s_{k}=D\), and the other with \(s_{k}=C\). _Right_ then shows this data arranged into a dataset of transitions as used in \(Q\)-updates. State \(D\) is edge-of-reach and hence appears in the dataset as \(s^{}\)_but never as \(s\)_. Bellman updates therefore _bootstrap from \(D\), but never update the value at \(D\)_ (see Equation (1)). (For comparison consider state \(C\): \(C\) is also sampled at \(s_{k}\), but unlike \(D\) it is not edge-of-reach, and hence is also sampled at \(s_{i<k}\) meaning it _is_ updated and hence does not cause issues.)all dynamics-penalized methods completely fail on most environments.5_ Note that under the true dynamics, all existing dynamics penalty-based methods assume no intervention is needed since there are no model errors. As a result, their penalties all become zero (see Table 7) and they all collapse to exactly the same procedure. Therefore the results shown in Table 1 indicate the failure of _all_ existing dynamics-penalized methods. In the following section, we investigate why having perfectly accurate dynamics leads to failure.

### The Edge-Of-Reach Hypothesis (Illustrated in Figure 2)

Failure under the error-free dynamics reveals that "model errors" cannot explain all the problems in offline model-based RL. Instead, it highlights that there must be a second issue. On investigation we find this to be the "edge-of-reach problem." We begin with the main intuition: in offline model-based RL, synthetic data \(_{}\) is generated as short \(k\)-step rollouts starting from states in the original dataset \(_{}\) (see Algorithm 1). Crucially, **Figure 2 (_left_) illustrates how, under this procedure, there can exist some states which can be reached in the final rollout step, but which _cannot_ - under any policy - be reached earlier. These _edge-of-reach_ states triggers a breakdown in learning, since even with the ability to collect unlimited data, the agent is never able to reach these states 'in time' to try actions from them, and hence is free to 'believe that these edge-of-reach states are great.'

More concretely: **Figure 2 (_right_)** illustrates how being sampled only at the final rollout step means edge-of-reach states will appear in the resulting dataset \(_{}\) as _nextstates_\(s^{}\), but will never appear in the dataset as _states_\(s\). Crucially, in Equation (1), we see that values at states \(s\) are updated, while values at \(s^{}\) are used for the targets of these updates. Edge-of-reach states are therefore _used for targets, but are never themselves updated_, meaning that their values can be arbitrarily disseminated. Updates consequently propagate misestimation over the entire state-action space. Furthermore, the \(\) operation in the policy improvement step (see Section 2) _exploits any misestimation_ by picking out the most heavily overestimated values . The _misestimation_ is therefore turned into _overestimation_, resulting in the value explosion seen in Figure 3. Thus, contrary to common understanding, the out-of-sample action problem key in model-free offline RL can be seen to persist in model-based RL.

**Edge-of-Reach Hypothesis**: Limited horizon rollouts from a fixed dataset leads to 'edge-of-reach' states: states which are used as targets for Bellman-based updates, but which are never themselves updated. The resulting maximization over misestimated values in Bellman updates causes pathological value overestimation and a breakdown in \(Q\)-learning.

_When is this an issue in practice?_ Failure via this mode requires _(a)_ the existence of edge-of-reach states, and _(b)_ such edge-of-reach states to be sampled. The typical combination of \(k H\) along with a limited pool of starting states (\(s_{}\)) means the rollout distribution is unlikely to sufficiently cover the full state space \(\), thus making _(a)_ likely. Moreover, we observe pathological 'edge-of-reach seeking' behavior in which the agent appears to'seek out' edge-of-reach states due to this being the source of overestimation, thus making _(b)_ likely. This behaviour is discussed further in Section 4.

In Appendix A, we give a thorough and unified view of model-free and model-based offline RL, dividing the problem into independent conditions for states and actions and examining when we can expect the edge-of-reach problem to be significant.

### Definitions and Formalization

**Definition 1** (Edge-of-reach states).: _Consider a deterministic transition model \(T:\), rollout length \(k\), and some distribution over starting states \(_{0}\). For some policy \(:\), rollouts are then generated according to \(s_{0}_{0}()\), \(a_{t}(|s_{t})\) and \(s_{t+1} T(|s_{t},a_{t})\) for \(t=0,,k-1\), giving \((s_{0},a_{0},s_{1},,s_{k})\). Let us use \(_{t,}(s)\) to denote the marginal distributions over \(s_{t}\)._

_We define a state \(s S\)**edge-of-reach** with respect to (\(T\), \(k\), \(_{0}\)) if: for \(t=k\), \(\,\) s.t. \(_{t,}(s)>0\), but, for \(t=1,,k-1\) and \(\,\), \(_{t,}(s)=0\). In our case, \(_{0}\) is the distribution of states in \(_{}\)._

In Appendix B, we include an extension to stochastic transition models, proof of how errors can consequently propagate to all states, and a discussion of the practical implications.

### Empirical evidence on the D4RL benchmark

There are two potential issues in offline model-based RL: _(1)_ dynamics model errors and subsequent model exploitation, and _(2)_ the edge-of-reach problem and subsequent pathological value overestimation. We ask: _Which is the true source of the issues observed in practice?_ While _(1)_ is stated as the sole issue and hence the motivation in prior methods, the "failure" results presented in Figure 1 and Table 1 are strongly at odds with this explanation. Furthermore, in Table 8, we examine the model rewards sampled by the agent over training. For _(1)_ to explain the \(Q\)-values seen in Figure 3, the sampled rewards in Table 8 would need to be on the order of \(10^{8}\). Instead, however, they remain less than \(10\), and are not larger than the true rewards, meaning _(1)_ does not explain the observed value explosion (see Appendix D.2). By contrast, the edge-of-reach-induced pathological overestimation mechanism of _(2)_ exactly predicts this value explosion, with the observations very closely resembling those of the analogous model-free out-of-sample problem . Furthermore, _(2)_ is consistent with the "failure" observations in Table 1. Finally, we again highlight the discussion in Section 3.2 where we explain why the edge-of-reach problem can be expected to occur in practice.

## 4 Analysis with a Simple Environment

In the previous section, we presented the edge-of-reach hypothesis as an explanation of why existing methods fail under the true dynamics. In this section, we construct a simple environment to empirically confirm this hypothesis. We first reproduce the observation seen in Figures 1 and 3 and Table 1 of the base offline model-based procedure resulting in exploding \(Q\)-values and failure to learn despite using the true dynamics model. Next, we verify that edge-of-reach states are the source of this problem by showing that correcting value estimates only at these states is sufficient to resolve the issues.

### Setup

We isolate failure observed in Section 3.1 with the following setup: Reward is defined as in Figure 3(a), the transitions function is simply \(}=+^{2}\), and initial states (analogous to \(s_{}\)) are sampled from \(_{0}=U([-2,2]^{2})\) (the area shown in the navy blue box Figure 4).

In applying the offline model-based procedure (see Algorithm 1) to this (true) environment we have precisely the same setup as in Section 3.1, where again the _only difference compared to online RL is the use of truncated \(k\)-step rollouts_ with \(k=10\) (compared to full horizon \(H=30\)). This small change results in the existence of edge-of-reach states (those between the red and orange boxes).

### Observing Pathological Value Overestimation

Exactly as with the benchmark experiments in Section 3.1, the base model-based procedure fails despite using the true dynamics (see blue Figure 4, **Base**), and the \(Q\)-values grow unboundedly over training (compare Figure 3(f) and Figure 2(b)).

Looking at the rollouts sampled over training (see Figure 7) we see the following behavior:

_(Before 25 epochs)_ Performance initially increases. _(Between 25 and 160 epochs)_ Value misestimation takes over, and the policy begins to aim toward unobserved state-actions (since their values can be misestimated and hence overestimated). _(After 160 epochs)_ This _'edge-of-reach seeking' behavior_ compounds with each epoch, leading the agent to eventually reach edge-of-reach states. From this point onwards, the agent samples edge-of-reach states at which it never receives any corrective feedback. The consequent pathological value overestimation results in a complete collapse in performance. In Figure 3(b) we visualize the final policy and see that it completely ignores the reward function, aiming instead towards an arbitrary edge-of-reach state.

Figure 3: The base procedure results in poor performance _(left)_ with exponential increase in \(Q\)-values _(right)_ on the D4RL benchmark. _Approx \(Q^{*}\)_ indicates the \(Q\)-value for a normalized score of 100 (with \(=0.99\)). Results are shown for Walker2d-medexp (\(6\) seeds), but we note similar trends across other D4RL datasets.

### Verifying the Hypothesis Using Value Patching

Our hypothesis is that the source of this failure is value misestimation at edge-of-reach states. Our **Base-OraclePatch** experiments (see yellow Figure 4) verify this by showing that patching in the correct values solely at edge-of-reach states is sufficient to completely solve the problem. This is particularly compelling as in practice we only corrected values at 0.4% of states over training. In Section 5 we introduce our practical method RAVL, which Figures 4 green and 7 show has an extremely similar effect to that of the ideal but practically impossible Base-OraclePatch intervention.

## 5 RAVL: Reach-Aware Value Learning

As verified in Section 4.3, issues stem from value overestimation at edge-of-reach states. To resolve this, we therefore need to _(A)_ detect and _(B)_ prevent overestimation at edge-of-reach states.

_(A) Detecting edge-of-reach states:_ As illustrated in Figure 2 _(right)_, edge-of-reach states are states at which the \(Q\)-values are never updated, i.e., those that are out-of-distribution (OOD) with respect to the training distribution of the \(Q\)-function, \(s_{}\). A natural solution for OOD detection is measuring high variance over a deep ensemble . We can therefore detect edge-of-reach states using an ensemble of \(Q\)-functions. We demonstrate that this is effective in Figure 5.

_(B) Preventing overestimation at these states:_ Once we have detected edge-of-reach states, we may simply apply value pessimism methods from the offline model-free literature. Our choice of an ensemble for part _(A)_ conveniently allows us to minimize over an ensemble of \(Q\)-functions, which effectively adds value pessimism based on ensemble variance.

Our resulting proposal is Reach-Aware Value Learning (RAVL). Concretely, we take the standard offline model-based RL procedure (see Algorithm 1), and simply exchange the _dynamics_ pessimism penalty for _value_ pessimism using minimization over an ensemble of \(N\)\(Q\)-functions:

\[Q_{}^{n}(s,a) r+_{i=1,,N}Q_{}^{i}(s^{}, _{}(s^{}))n=1,,N\] (2)

We include EDAC's  ensemble diversity regularizer, and in Section 7 we discuss how the impact of value pessimism differs significantly in the model-based (RAVL) vs model-free (EDAC) settings.

## 6 Empirical Evaluation

In this section, we begin by analyzing RAVL on the simple environment from Section 4. Next, we look at the standard D4RL benchmark, first confirming that RAVL solves the failure seen with the true dynamics, before then demonstrating that RAVL achieves strong performance with the

Figure 4: Experiments on the simple environment, illustrating the edge-of-reach problem and potential solutions. **(a)** Reward function, **(b)** final (failed) policy with naive application of the base procedure (see Algorithm 1), **(c)** final (successful) policy with patching in oracle \(Q\)-values for edge-of-reach states, **(d)** final (successful) policy with RAVL, **(e)** returns evaluated over training, **(f)** mean \(Q\)-values evaluated over training.

learned dynamics. In Appendix F, we include additional results on the challenging pixel-based V-D4RL benchmark on which RAVL now represents a new state-of-the-art. Finally in Section 6.5 we reexamine prior model-based methods and explain why they may work despite not explicitly addressing the edge-of-reach problem. We provide full hyperparameters in Appendix C.2 and ablations showing that RAVL is stable over hyperparameter choice in Appendix G.2.

### Simple Environment

Testing on the simple environment from Section 4 (see green Figure 4), we observe that RAVL behaves the same as the theoretically optimal but practically impossible _Base-OraclePatch_ method. Moreover, in Figure 5, we see that the \(Q\)-value variance over the ensemble is significantly higher for edge-of-reach states, meaning RAVL is detecting and penalizing edge-of-reach states exactly as intended.

### D4RL with the True Dynamics

Next, we demonstrate that RAVL works without dynamics uncertainty and solves the 'failure' observed in Section 3.1. Table 1 shows results on the standard offline benchmark D4RL  MuJoCo  v2 datasets with the true (zero error, zero uncertainty) dynamics. We see that RAVL learns the near-optimal policies, while existing methods using the base model-based procedure (Algorithm 1) completely fail. In Section 6.5 we examine how this is because existing methods overlook the critical edge-of-reach problem and instead only accidentally address it using dynamics uncertainty metrics. In the absence of model uncertainty, these methods have no correction for edge-of-reach states and hence fail dramatically.

### D4RL with Learned Dynamics

Next, we show that RAVL also performs well with a learned dynamics model. Figure 3 shows RAVL successfully stabilizes the \(Q\)-value explosion of the base procedure, and Table 2 shows RAVL largely matches SOTA, while having _significantly lower runtime_ (see Section 6.4). RAVL gives much higher performance on the Halfcheetah mixed and medium datasets than its model-free counterpart EDAC, and in Section 6.5, we discuss why we would expect the effect of value pessimism in the model-based setting (RAVL) to inherently offer much more flexibility than in the model-free setting (EDAC).

    &  &  &  &  \\   & medium & 74.6\(\)1.2 & 72.2\(\)4.1 (**1\%**) & 78.7\(\)2.0 & 76.4\(\)3.4 \\  & mixed & 71.7\(\)1.2 & 72.2\(\)4.1 (**1\%**) & 74.9\(\)2.0 & 77.3\(\)2.2 \\  & medexp & 108.2\(\)2.5 & 84.9\(\)17.6 (**22\%**) & 102.1\(\)8.0 & 109.3\(\)1.3 \\   & medium & 106.6\(\)0.6 & **17.2\(\)11.1 (**184\%**) & 90.6\(\)11.9 & 101.6\(\)0.9 \\  & mixed & 103.9\(\)1.0 & 71.5\(\)32.2 (**,**31\%**) & 103.1\(\)1.1 & 101.1\(\)0.5 \\  & medexp & 112.6\(\)0.2 & **6.1\(\)6.8 (**,**96\%**) & 110.1\(\)1.8 & 107.2\(\)0.9 \\   & medium & 87.7\(\)1.1 & **7.2\(\)1.6 (**,**92\%**) & 86.3\(\)1.6 & 101.2\(\)2.3 \\  & mixed & 89.9\(\)1.5 & **7.9\(\)1.9 (**,**91\%**) & 83.0\(\)2.5 & 86.4\(\)3.0 \\   & medexp & 115.2\(\)0.7 & **7.7\(\)2.1 (**,**93\%**) & 115.5\(\)2.4 & 103.5\(\)1.2 \\   

Table 1: **True dynamics (zero error, zero uncertainty)** Existing model-based methods are presented as different approaches for dealing with dynamics model errors. Surprisingly, however, all existing methods fail in the absence of dynamics errors (when the learned **approximate** model is replaced by the **true** model). This reveals that existing methods are unintentionally using their dynamics uncertainty estimates to address the previously unnoticed edge-of-reach problem. By contrast, RAVL directly addresses the edge-of-reach problem and hence does not fail when dynamics uncertainty is zero. Experiments are on the D4RL MuJoCo v2 datasets. Statistical significance highlighted (6 seeds). _**Note that while labeled as ‘MOBILE’, the results with the true dynamics will be identical for any other dynamics penalty-based method since penalties under the true model are all zero (see Table 7). Hence these results indicate the failure of all existing dynamics uncertainty-based methods._

Figure 5: RAVL’s effective penalty of \(Q\)-ensemble variance on the environment in Section 4, showing that - as intended - edge-of-reach states have significantly higher penalty than within-reach states.

We additionally include results for the challenging _pixel-based_ **V-D4RL benchmark** for which _latent-space models_ are used (in Appendix F), and accompanying **ablation experiments** (in Appendix G.2). In this setting, RAVL represents a new SOTA, giving a performance boost of more than 20% for some environments. These results are particularly notable as the pixel-based setting means the base algorithm (DreamerV2) uses model rollouts in an imagined latent space (rather than the original state-action space as in MBPO). The results therefore give promising evidence that RAVL is able to generalize well to different representation spaces.

### Runtime Discussion

Vectorized ensembles can be scaled with extremely minimal effect on the runtime (see Table 10). This means that, _per epoch_, RAVL is approximately 13% faster than the SOTA (MOBILE, due to MOBILE needing multiple extra forward passes to compute its uncertainty penalty). Furthermore, _in total_, we find that RAVL reliably requires \(3\) fewer epochs to converge on all but the medexp datasets, meaning the total runtime is approximately 70% faster than SOTA (see Appendix G.1).

### How can prior methods work despite overlooking the edge-of-reach problem?

While ostensibly to address model errors, we find that existing dynamics penalties _accidentally_ address the edge-of-reach problem: In Figure 6 we see a positive correlation between the penalties used in dynamics uncertainty methods and RAVL's effective penalty of value ensemble variance. This may be expected, as dynamics uncertainty will naturally be higher further away from \(_{}\), which is also where edge-of-reach states are more likely to lie. In Section 3.4 we present evidence suggesting that the edge-of-reach problem is likely the dominant source of issues on the main D4RL benchmark, thus indicating that the dynamics uncertainty penalties of existing methods are likely indirectly addressing the edge-of-reach problem. In general, dynamics errors are a second orthogonal problem and RAVL can be easily combined with appropriate dynamics uncertainty penalization [24; 29; 36] for environments where this is a significant issue.

## 7 Related Work

**Model-Based Methods.** Existing offline model-based methods present dynamics model errors and consequent model exploitation as the sole source of issues. A broad class of methods therefore propose reward penalties based on the estimated level of model uncertainty [2; 14; 29; 36], typically using variance over a dynamics ensemble. Rigter et al.  aim to avoid model exploitation by setting up an adversarial two-player game between the policy and model. Finally, most related to our method, COMBO , penalizes value estimates for state-actions outside model rollouts. However, similarly to Yu et al. , COMBO is theoretically motivated by the assumption of infinite horizon model rollouts, which we show overlooks serious implications. Critically, in contrast to our approach, none of these methods address the edge-of-reach problem and thus they fail as environment

    &  & **CQL** & **EDAC** & **MOPO** & **COMBO** & **RAMBO** & **MOBILE** & **RAVL (Ours)** \\   & random & 2.2 & 31.3 & 28.4 & 38.5 & 38.8 & 39.5 & **39.3** & 34.4\(\)2.0 \\  & medium & 43.2 & 46.9 & 65.9 & 73.0 & 54.2 & 77.9 & 74.6 & **78.7\(\)2.0** \\  & mixed & 37.6 & 45.3 & 61.3 & 72.1 & 55.1 & 68.7 & 71.7 & **74.9\(\)2.0** \\  & medexp & 44.0 & 95.0 & **106.3** & 90.8 & 90.0 & 95.4 & **108.2** & **102.1\(\)8.0** \\   & random & 3.7 & 5.3 & 25.3 & 31.7 & 17.9 & 25.4 & **31.9** & 31.4\(\)0.1 \\  & medium & 54.1 & 61.9 & 101.6 & 62.8 & 97.2 & 87.0 & **106.6** & 90.6\(\)11.9 \\  & mixed & 16.6 & 86.3 & 101.0 & **103.5** & 89.5 & 99.5 & **103.9** & **103.1\(\)1.1** \\  & medexp & 53.9 & 96.9 & 110.7 & 81.6 & 111.1 & 88.2 & **112.6** & 110.1\(\)1.8 \\   & random & 1.3 & 5.4 & 16.6 & 7.4 & 7.0 & 0.0 & **17.9** & **17.1\(\)7.9** \\  & medium & 70.9 & 79.5 & **92.5** & 84.1 & 81.9 & 84.9 & 87.7 & 86.3\(\)1.6 \\   & mixed & 20.3 & 76.8 & 87.1 & 85.6 & 56.0 & 89.2 & **89.9** & 83.0\(\)2.5 \\   & medexp & 90.1 & 109.1 & **114.7** & 112.9 & 103.3 & 56.7 & **115.2** & **115.5\(\)2.4** \\   

Table 2: A comprehensive evaluation of RAVL over the standard D4RL MuJoCo benchmark. We show the mean and standard deviation of the final performance averaged over 6 seeds. Our simple approach largely matches the state-of-the-art without any explicit dynamics penalization and hence works even in the absence of model uncertainty (where dynamics uncertainty-based methods fail) (see Table 1).

models become more accurate. A related phenomenon of overestimation stemming from hallucinated states has been observed in online model-based RL.

**Offline Model-Free Methods.** Model-free methods can broadly be divided into two approaches to solving the out-of-sample action problem central to offline model-free RL (see Section 2): action constraint methods and value pessimism-based methods. Action constraint methods [8; 17; 18] aim to avoid using out-of-sample actions in the Bellman update by ensuring selected actions are close to the dataset behavior policy \(^{}\). By contrast, value pessimism-based methods aim to directly regularize the value function to produce low-value estimates for out-of-sample state-actions [1; 16; 19]. The _edge-of-reach_ problem is the model-based equivalent of the _out-of-sample_ action problem, and this unified understanding allows us to transfer ideas directly from model-free literature. RAVL is based on EDAC's  use of minimization over a \(Q\)-ensemble [9; 35], and applies this to model-based offline RL.

_What is the effect of value pessimism in the model-based vs model-free settings?_

EDAC  can be seen as RAVL's model-free counterpart, however, the impact of value pessimism in the model-free vs the model-based settings is notably different. Recall that, with the ensemble \(Q\)-function trained on \((s,a,r,s^{})_{}\), the state-actions that are penalized (due to being outside the training distribution) are any \((s^{},a^{})\) that are out-of-distribution with respect to the \((s,a)\)'s in the dataset \(_{}\). Recall also that updates use values at \((s^{},a^{})\) where \(a^{}_{}\) (see Equation (1)).

_In the model-free case (EDAC):_ The dataset is \(_{}=_{offline}\), meaning the actions \(a\) are effectively sampled from the dataset behavior policy (off-policy), whereas the actions \(a^{}\) are sampled on-policy. This means that EDAC penalizes any \((s^{},a^{})\) where \(a^{}\) differs significantly from the behavior policy.

_In the model-based case (RAVL):_ The dataset is \(_{}=_{rollouts}\), meaning now both \(a\) and \(a^{}\) are sampled on-policy. As a result, the only \((s^{},a^{})\) which will now be out-of-distribution with respect to \((s,a)\) (and hence penalized) are those where the state \(s^{}\) is out-of-distribution with respect to the states \(s\) in \(_{rollouts}\). This happens when \(s^{}\) is reachable only in the final step of rollouts, i.e. exactly when \(s^{}\) is "edge-of-reach" (as illustrated in Figure 2). Compared to EDAC, RAVL can therefore be viewed as "relaxing" the penalty and giving the agent _freedom to learn a policy that differs significantly from the dataset behavior policy._ This distinction is covered in detail in Appendix A.

## 8 Conclusion

This paper investigates how offline model-based methods perform as dynamics models become more accurate. As an interesting hypothetical extreme, we test existing methods with the true error-free dynamics. Surprisingly, we find that all existing methods fail. This reveals that using truncated rollout horizons (as per the shared base procedure) has critical and previously overlooked consequences stemming from the consequent existence of 'edge-of-reach' states. We show that existing methods are _indirectly_ and _accidentally_ addressing this edge-of-reach problem (rather than addressing model errors as stated), and hence explain why they fail catastrophically with the true dynamics.

This problem reveals close connections between model-based and model-free approaches and leads us to present a unified perspective for offline RL. Based on this, we propose RAVL, a simple and robust method that achieves strong performance across both proprioceptive and pixel-based benchmarks. Moreover, RAVL _directly_ addresses the edge-of-reach problem, meaning that - unlike existing methods - RAVL does not fail under the true environment model, and has the practical benefit of not requiring dynamics uncertainty estimates. Since improvements to dynamics models are inevitable, we believe that resolving the brittle and unanticipated failure of existing methods under dynamics model improvements is an important step towards 'future-proofing' offline RL.

## 9 Limitations

Since dynamics models for the main offline RL benchmarks are highly accurate, the edge-of-reach effects dominate, and RAVL is sufficient to stabilize model-based training effectively without any explicit dynamics uncertainty penalty. In general, however, edge-of-reach issues could be mixed with dynamics error, and understanding how to balance these two concerns would be useful future work. Further, we believe that studying the impact of the edge-of-reach effect in a wider setting could be an exciting direction, for example investigating its effect as an implicit exploration bias in online RL.