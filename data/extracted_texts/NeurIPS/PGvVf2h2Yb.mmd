# Adaptive Transductive Inference via Sequential Experimental Design with Contextual Retention

Tareq Si Salem

Huawei Technologies, Paris Research Center, France

tareq.si.salem@huawei.com

###### Abstract

This paper presents a three-stage framework for active learning, encompassing data collection, model retraining, and deployment phases. The framework's primary objective is to optimize data acquisition, data freshness, and model selection methodologies. To achieve this, we propose an online policy with performance guarantees, ensuring optimal performance in dynamic environments. Our approach integrates principles of sequential optimal experimental design and online learning. Empirical evaluations validate the efficacy of our proposed method in comparison to existing baselines.

## 1 Introduction

The development of Machine Learning (ML) models is intrinsically reliant on the availability of data. Data plays a critical role in various stages of the ML model lifecycle, including parameter optimization, evaluation of inferential capabilities, and potential refinements to the model architecture. However, the acquisition of suitable data frequently constitutes a significant bottleneck in the training pipeline. The process of obtaining relevant data or measurements can be both costly and time-intensive and is often subject to resource constraints. These constraints may manifest in diverse forms, including restrictions on sample size, temporal constraints, or computational limitations. The challenge is further compounded when labels for these datasets are unavailable and are costly to acquire (e.g., clinical trials , drug discovery ). In such scenarios, strategic decisions regarding data collection or experimental design become essential. The selection of the most informative samples for a given task is known as _optimal experimental design_ (OED)  within the field of statistics. The primary objective in OED is to maximize information gain about an unknown model within the confines of a limited budget. OED has long been an essential part of statistical modeling, from the design of clinical trials , medical imaging , materials science , biological process models , networked systems , bandits , and regression problems in general . For a comprehensive overview of OED methodologies and applications, the reader is directed to the following surveys .

In many real-world applications of ML, the performance of a model remain unknown until it is deployed and interacts with its operational environment. This inherent uncertainty necessitates an iterative approach to experimental design, where subsequent experiments are informed by the outcomes of previous ones. This adaptive methodology, known as sequential optimal experimental design (SOED), aims to effectively mitigate uncertainty by dynamically adjusting the experimental design based on accumulating knowledge. SOED presents a unique challenge in that the optimal design at any given stage depends on the anticipated sequence of future predictions. A common suboptimal approach is to employ a greedy strategy, utilizing historical data to determine the seemingly best experiment at each step without explicitly considering the long-term consequences of this decision . This myopic approach fails to account for the potential impact ofearly design choices on the overall optimization process. Recently, online learning has emerged as a promising framework for addressing SOED by incorporating feedback mechanisms into the design process [24; 26; 45]. By learning from past experiences and adapting its strategy accordingly, online learning offers a more sophisticated approach to sequential decision-making in experimental design. Furthermore, the deployment of machine learning models often encounters the challenge of concept drift, which refers to the dynamic nature of the relationship between input features and the target variable [20; 34; 43; 50; 36]. Significant disruptions, such as the COVID-19 pandemic, can precipitate substantial alterations in traffic patterns. These alterations subsequently introduce significant concept drift in traffic forecasting methodologies, thereby challenging the efficacy of predictive models [33; 35]. Consequently, this necessitates the development of training strategies that effectively tune data freshness and recency to maintain predictive accuracy. Traditional approaches, predominantly focused on variance minimization through experimental design, are insufficient in addressing the bias introduced by concept drift. Consequently, a crucial question arises:

_How can we effectively optimize data acquisition, data freshness, and model selection methodologies in dynamic environments characterized by concept drifts?_

To address this challenge, this work introduces a novel active learning framework. This framework operates in three distinct stages: data collection, model retraining, and deployment. In the initial data collection phase, a policy guides the selection of informative experiments from a predefined pool of potential candidates. This selection process aims to maximize the value of acquired labels, thereby enhancing learning efficiency. The policy can request labels for multiple experiments concurrently, subject to a constraint on the number of simultaneous queries. These labeled datasets are then stored in a local repository with a fixed capacity, utilizing an eviction strategy to manage storage limitations. Periodically, the accumulated labeled data is used to retrain a machine learning model. This model, updated with an appropriate selection of fresh dataset from the local repository, is then deployed to predict labels for new, incoming queries. Unlike traditional inductive learning approaches that focus solely on the initial pool of experiments, this framework adopts a transductive learning perspective [11; 48; 6; 51; 12]. This means that the policy's experimental design choices are optimized not only with respect to the initial pool but also in relation to the sequence of revealed queries. The policy receives feedback on its predictions in the form of noisy prediction errors, allowing it to adapt and refine its strategy over time. The ultimate goal of the policy is to minimize its regret, which quantifies the performance difference between the policy's predictions and those of an optimal policy possessing complete information about the underlying data generating process. This minimization of regret ensures that the active learning framework efficiently learns and adapts to the underlying phenomenon, even in the presence of noise and uncertainty.

The remainder of this paper is organized as follows. Section 2 presents the problem formulation. A theoretical analysis of the problem is conducted in Section 3. Finally, the effectiveness of the proposed approach is numerically demonstrated in Section 4.

## 2 Problem Formulation

### System Model

The overall system model is illustrated in Figure 1. A list of the notation employed throughout this paper can be found in the Appendix.

**Data Collection.** The policy has access to a pool of experiments \(^{d}\) to collect labels from a variety of experimental sources, such as sensors, surveys, and databases. A data retention policy is implemented, periodically purging datasets that exceed a predetermined age threshold \(\). This practice adheres to data privacy regulations (e.g., GDPR , CCPA/CPRA[1; 38]). At each time slot \(t\), the policy is allocated a fixed experimental budget of \(M\) experiments. The set of all feasible experimental designs is defined as

\[..}}\{^{}:\|\|_{1}=1\}..\]

For a given continuous design \(\{}}\}\), the learner allocates \(M_{}[0,M]\) experiments to type \(\). At time \(t\), the system acquires a dataset \(_{t}\) of \(M\) experiment-labels pairs in \(\), following a design \(_{t}\{}}\}\). A rolling window of \(+1\) datasets is maintained, discarding older ones. The labels \(y\) are related to the experiment \(\) according to a noisy linear model \(y=_{t}^{*}+n\), where \(n(0,^{2})\) is a Gaussian noise and \(_{t}^{*}\) is the _time-varying_ true model.

**Model Retraining.** To address the challenge of concept drift during the model updating phase, we introduce a data-freshness parameter, denoted as \(_{t}\{0,1,,\}\). This parameter dictates the recency of the data used for model training. Specifically, at each time slot \(t\), we employ a least-squares estimator (LSE), denoted as \(}_{t}\), to generate future predictions. The LSE model is trained exclusively on the \((_{t}+1)\) most recent data points available at time \(t\). The model is given by \(}_{t}=(^{}( _{t-_{t}:t}))^{-1}^{}( _{t-_{t}:t})\), where \(=(^{})_{}^{  d}\) is the experiments' matrix, and \(()=(_{(,y_{i})_{}}y_{i} )_{}^{}\) is the labels' vector.

**Model Deployment.** During deployment, the trained model \(}_{t}\) is used to predict labels for experiments \(_{t}^{d}\). User feedback in the form of prediction errors is collected to refine the model. Specifically, the squared error \(_{t}=(y_{t}-_{t})^{2}\) is provided, where \(_{t}=_{t}}_{t}\) is the predicted label and \(y_{t}=_{t}_{t}^{}+n\) is the label with noise \(n(0,^{2})\). This feedback signal informs the learner about the model's performance and guides future data collection to improve accuracy. Both the feedback signal \(_{t}\) and the corresponding query \(_{t}\) are used to guide future model selection.

### Policies and Performance Metric

In this section, we provide a formal description of the policy that governs the collection of data through experimental designs and the freshness of data used to retrain the most recent model.

**Prediction Error and Bias-Variance Tradeoff.** The accuracy of the model \(}_{t}\) for a query point \(_{t}\) with noisy label \(y_{t}=_{t}_{t}^{}+n(_{t} _{t}^{},^{2})\) is measured by its expected prediction error (EPE). This EPE depends on the experimental designs \((_{t-},,_{t})\) and a data-freshness parameter \(_{t}\) controlling the influence of past data. The EPE is defined as follows: \(f_{t}(_{t-_{t}},,_{t})[_{t}] =[(y_{t}-_{t})^{2}]\). In the following Proposition, we clearly delineate the contributions of experimental design selection and data-freshness selection, by decomposing the EPE on query \(_{t}\) into its variance and bias components.

**Proposition 1**.: _Under designs \(\{_{s}\}_{s=t-}^{t}_{}^{ +1}\), the EPE of the LSE on experiment \(_{t}\) at time \(t\) under data-freshness window size \(+1\) is_

\[f_{t}(_{t-},,_{t})=^{2}+_{t}^{} (}_{t})_{t}+(_{t}( [}_{t}]-_{t}^{}) )^{2},\] (1)

_where \([}_{t}]=^{-1}(_{t-_{t}:t })(_{}^{}_{s=t-}^{t} _{s,}_{s}^{})\) and \((}_{t})=}{M}^{-1}(_{t- _{t}:t})\)._

The proof is provided in Appendix B.1. The bias-variance trade-off is evident in the decomposition. The expected prediction error is divided into three components: (a) irreducible variance due to noise in the labels, (b) variance related to data-freshness and experimental designs, and (c) bias reflecting model drift. In the absence of significant drift, a larger data-freshness window is beneficial. However, under significant drift, a smaller window is preferable, though increasing variance. Minimizing variance through careful experimental design is always advantageous.

**Online Policies.** The role of a policy is to select appropriately experimental designs \(_{t}_{}\) and data-freshness parameter \(_{t}\) at every timeslot \(t\), and adapts its decisions upon seeing the query \(_{t}\) and feedback \(_{t}\). Formally, at timeslot \(t\), the system adapts its state according to a randomized policy \(_{t}:(_{} )^{t}_{} \), defined as \((_{t+1},_{t+1})=_{t}(\{_{s},_{s}, _{s},_{t}\}_{s=1}^{t})\).

**Performance Metric.** We compare the performance of the sequence of designs and data-freshness parameters w.r.t. the best design in hindsight and data-freshness window size after seeing all the queries in terms of the EPE. Formally,

\[_{T}(})[_{t= +1}^{T}f_{t}(_{t-_{t}},,_{t})-_{t=+1}^{T} f_{t}(_{t-_{t}^{}}^{},,_{t}^{})],\] (2)

Figure 1: System Modelwhere \(_{t}^{*}\) and \(_{t}^{*}\) for \(t[T]\) are the minimizers of the aggregate expected prediction error, and the expectation is taken with respect to the randomness in the environment and the policy \(}\).

If the regret \(_{T}(})\) is sublinear, then the policy asymptotically achieves on average the performance of a policy that selects the optimal experimental designs and data-freshness parameters in hindsight.

## 3 Theoretical Analysis

Our theoretical analysis reveals that the optimization of experimental design and data-freshness can be efficiently decoupled into two interrelated subproblems. We propose a policy utilizing Online Mirror Descent (OMD) [8; 44; 23] to simultaneously address these subproblems.

### Decoupling of Experimental Design and Data-freshness Decisions

We propose a decoupled approach to experimental design and data-freshness parameter selection. Firstly, we employ a variance reduction policy within the full-information online learning framework [23; 37] to determine the optimal experimental design. Secondly, we formulate a multi-armed bandit problem  to select the data-freshness parameters, thereby mitigating bias.

**Variance Reduction Policy.** The variance reduction policy selects a new design at time \(t\) according to a mapping \(_{t}^{v}\) that maps the past experimental designs \(\{_{s}\}_{s=1}^{t}_{}^{t}\), and past experiment queries \(\{_{s}\}_{s=1}^{t}^{t}\) to a new design \(_{t+1}\) given by \(_{t+1}=_{t}^{v}(\{_{s},_{s}\}_{s=1}^{t})\). The policy incurs costs in the form of the \(_{t}\)-optimal design objective in Definition 2, and has the following regret: \(_{T}^{v}(}^{v})_{t=1}^{T }v_{t}(_{t})-_{\{_{t}^{*}\}_{t=1}^{T}_{}^{T}}_{t=1}^{T}v_{t}(_{t}^{*})\). Note that the cost \(v_{t}\) are fully determined once \(_{t}\) is made available.

**Bias Reduction Policy.** The bias reduction policy operates on top of the variance reduction policy. In particular, at timeslot \(t\), the data-freshness parameter \(_{t+1}\) is the output of the mapping \(_{t}^{b}\) that maps the past data-freshness parameters \(\{_{s}\}_{s=1}^{t}^{t}\) and prediction error feedback \(\{_{s}\}_{s=1}^{t}^{t}\) according to the mapping \(_{t+1}=_{t}^{b}(\{_{s},_{s}\}_{s=1}^{t})\). Note that the coupling between the variance reduction policy and bias reduction policy is implicitly encoded in the prediction error \(_{t}\) as this error depends on both decisions. The policy incurs the prediction errors, and has the following regret: \(_{T}^{b}(}^{b})[_{t= +1}^{T}_{t}-_{\{_{t}^{*}\}_{t=1}^{T}^{T}}_{t= _{t}^{*}+1}^{T}f_{t}(_{t-_{t}^{*}},,_{t})]\). The setup of bias reduction policy corresponds to a non-stationary setup of the multi-armed bandit problem where \(\) is the set of the arms [31; 7].

**VBR Policy.** The variance and bias reduction (VBR) policy denoted by \(}^{v+b}\) is the policy that determines experimental designs \(_{t}\) according to the variance reduction policy \(}^{v}\) and the data-freshness parameter \(_{t}\) according to the bias reduction policy \(}^{b}\) for any \(t[T]\). Formally, at timeslot \(t\), the policy is given by the mapping \(_{t}^{v+b}:(_{} )^{t}_{}\) given by \(_{t}^{v+b}(_{t}^{v},_{t}^{b})\). The variance and bias reduction policy enjoys the following regret guarantee:

**Theorem 1**.: _Under Assumptions 1-3, let \(\{_{t}\}_{t=1}^{T}^{T}\) be the sequence of queries, \(\{_{t}^{*}\}_{t=1}^{T}_{}^{T}\) be the sequence of optimal experimental designs and \(\{_{t}^{*}\}_{t=1}^{T}^{T}\) is the sequence of data-freshness windows. The regret (2) of the variance and bias reduction policy \(}^{v+b}\) satisfies:_

\[_{T}(}^{v+b})=_{T}^{b}(} ^{b})+_{T}^{v}(}^{v})+(P_{T}^{v}+P_{ T}^{*,v}),\] (3)

_where \(P_{T}^{*,v}=_{t=1}^{T}_{t}^{*}-_{t+1}^{*}_{1}\) and \(P_{T}^{v}=_{t=1}^{T}_{t}-_{t+1}_{1}\) are the path-lengths of the OEDs and variance reduction policy._

The proof is provided in Appendix E. In the next section, we provide a specific instantiations of the variance reduction and bias reduction policies.

### Entropic-VBR Policy

We introduce the Entropic-VBR policy (Appendix C.4), which achieves sublinear regret. Our approach provides a unified treatment of both full-information and bandit settings. To overcome the challenges of a non-stationary environment and bias reduction in the bandit setting, we meticulously instantiate the OMD framework. This involves carefully constructing gradient estimates and selecting an appropriate mirror map to ensure simultaneous regret guarantees. Leveraging the results of Corollary 2, Theorem 4, and Theorem 1, we establish a comprehensive regret guarantee for the Entropic-VBR policy. Formally,

**Corollary 1**.: _Under Assumptions 1-3, let \(\{_{t}\}_{t=1}^{T}^{T}\) be the sequence of queries, \(\{_{t}^{}\}_{t=1}^{T}_{,}^{ T}\) be the sequence of optimal experimental designs and \(\{_{t}^{}\}_{t=1}^{T}_{}^{}\) is the sequence of comparator data-freshness windows, with path lengths \(P_{T}^{,v}\) and \(P_{T}^{,b}\), respectively. The Entropic-VBR Policy (Appendix C.4) configured with learning rates \(_{}=((1/)P_{T}^{,v}T^{-1})\) and \(_{}=((1/^{})P_{T}^{,b}T^{-1})\) and \(^{}=(T^{-1})\) achieves the following regret:_

\[_{T}(}^{})=( ^{,v}T}+^{,b}T}+P_{T}^{ ,v}).\] (4)

The sequences in \(_{,}\) are \((1+)\)-competitive w.r.t. sequences in \(_{}\) (Prop. 2 in the Appendix).

## 4 Numerical Experiments

**Experimental Setup.** To evaluate the performance of our proposed methodology, we constructed a synthetic experimental setting, as illustrated in Figure 2 (a). Full description is in Appendix F.

**Discussion.** Our evaluation in Figure 2 (b) shows that a uniform experimental design with the maximum freshness window is the least effective baseline (indexed as \(\)). Optimizing the data-freshness window improves performance (indexed as \(\)), and further gains are achieved by optimizing the experimental design (indexed as \(\)). Our proposed policy (indexed as \(\)) outperforms the baseline, demonstrating its ability to identify optimal sequences of designs and freshness windows. Figure 2 (c) represents how the policy adapts to evolving query distributions. The temporal evolution of the learned distribution over window sizes in Figure 3 (Appendix) reveals that the optimal window size under these conditions is not immediately apparent.

## 5 Conclusion

This work introduced a novel framework that explicitly accounts for the evolving relationship between data freshness and model performance, encompassing data collection, data freshness decisions, and model retraining within a limited-capacity cache. A rigorous theoretical analysis revealed the inherent variance-bias trade-off and motivated a decoupled approach to address this challenge. This approach involved leveraging OCO for variance reduction in experimental design and formulating a non-stationary MAB problem for bias mitigation through data freshness parameter selection.

As avenues for future work, extending the inference model to encompass non-linear relationships is of considerable interest. Reproducing kernel methods [3; 4] present a promising initial direction for leveraging the proposed framework, as they permit analogous derivations. Additionally, exploring more general noise models beyond the Gaussian noise considered herein would enhance the framework's applicability.

Figure 2: Subfig. (a): Query distributions, and true model drift (\(10^{3}\) initial iterations). Subfig. (b): Time-ayged prediction error for three intervals: \(0 tT\), \(T<tT\), and \(T<t T\). Subfig. (c): Initial and selected designs at \(t=T\), \(t=T\), and \(t=T\).