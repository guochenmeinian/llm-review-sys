# Variational Flow Matching for Graph Generation

Floor Eijkelboom

UvA-Bosch Delta Lab

University of Amsterdam

&Grigory Bartosh

AMLab

University of Amsterdam

Christian A. Naesseth

UvA-Bosch Delta Lab

University of Amsterdam

&Max Welling

UvA-Bosch Delta Lab

University of Amsterdam

&Jan-Willem van de Meent

UvA-Bosch Delta Lab

University of Amsterdam

###### Abstract

We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). Based on this formulation we develop CatFlow, a flow matching method for categorical data. CatFlow is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. The key observation in VFM is that we can parameterize the vector field of the flow in terms of a variational approximation of the posterior probability path, which is the distribution over possible end points of a trajectory. We show that this variational interpretation admits both the CatFlow objective and the original flow matching objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art.

## 1 Introduction

In recent years, the field of generative modeling has seen notable advancements. In image generation , the development and refinement of diffusion-based approaches -- specifically those using denoising score matching  -- have proven effective for generation at scale . However, while training can be done effectively, the constrained space of sampling probability paths in a diffusion requires tailored techniques to work . This is in contrast to more flexible approaches such as continuous normalizing flows (CNFs) , that are able to learn a more general set of probability paths than diffusion models , at the expense of being expensive to train as they require one to solve an ODE during each training step (see e.g. ).

Recently, Lipman and collaborators  proposed flow matching (FM), an efficient and simulation-free approach to training CNFs. Concretely, they use a per-sample interpolation to derive a simpler objective for learning the marginal vector field that generates a desired probability path in a CNF. This formulation provides equivalent gradients without explicit knowledge of the (generally intractable) marginal vector field. This work has been extended to different geometries  and various applications . Similar work has been proposed concurrently in .

This paper identifies a reformulation of flow matching that we refer to as variational flow matching (VFM). In flow matching, the vector field at any point can be understood as the expected continuation toward the data distribution. In VFM, we explicitly parameterize the learned vector field as an expectation relative to a variational distribution. The objective of VFM is then to minimize theKullback-Leibler (KL) divergence between the posterior probability path, i.e. the distribution over possible end points (continuations) at a particular point in the space, and the variational approximation.

We show that VFM recovers the original flow matching objective when the variational approximation is Gaussian and the conditional vector field is linear in the data. Under the assumption of linearity, a solution to the VFM problem is also exact whenever the variational approximation matches the _marginals_ of the posterior probability path, which means that we can employ a fully-factorized variational approximation without loss of generality.

While VFM provides a general formulation, our primary interest in this paper is its application to graph generation, where the data are categorical. This setting leads to a simple method that we refer to as CatFlow, in which the objective reduces to training a classifier over end points on a per-component basis. We apply CatFlow to a set of graph generation tasks, both for abstract graphs  and molecular generation [40; 19]. By all metrics, our results match or substantially exceed those obtained by existing methods.

## 2 Background

### Transport Framework for Generative Modeling and CNFs

Common generative modeling approaches such as normalizing flows [42; 38] and diffusion models [15; 54] parameterize a transformation \(\) from some initial tractable probability density \(p_{0}\) - typically a standard Gaussian distribution - to the target data density \(p_{1}\). In general, there is a trade-off between allowing \(\) to be expressive enough to model the complex transformation while ensuring that the determinant term is still tractable. One such transformation is a continuous normalizing flow (CNF).

Any time-dependent1 vector field \(v_{t}:^{D}^{D}\) gives rise to such a transformation - called a _flow_ - as such a field induces a time-dependent diffeomorphism \(_{t}:^{D}^{D}\) defined by the following ordinary differential equation (ODE):

\[_{t}(x)=v_{t}(_{t}(x))_{0}(x)=x.\] (1)

In CNFs, this vector field is learned using a neural network \(v_{t}^{}\). Through the change of variables formula \(p_{t}(x)\) can be evaluated (see appendix E) and hence one could try and optimize the empirical divergence between the resulting distribution \(p_{1}\) and target distribution. However, obtaining a gradient sample for the loss requires ones to solve the ODE induced during training, making this approach computationally expensive.

### Flow Matching

In flow matching , the aim is to regress the underlying vector field of a CNF directly on the interval \(t\). Flow matching leverages the fact that even though we do not have access to the _actual_ underlying vector field - which we denote as \(u_{t}\) - and probability path \(p_{t}\), one can construct a per-example formulation by defining _conditional flows_, i.e. the trajectories towards specific datapoints \(x_{1}\). Concretely, FM sets:

\[u_{t}(x)= u_{t}(x x_{1})(x x_{1})p_{}(x_{1} )}{p_{t}(x)}x_{1},\] (2)

where \(u_{t}(x x_{1})\) is the conditional trajectory. The most common way to define \(u_{t}(x x_{1})\) is as the straight line continuation from \(x\) to \(x_{1}\), implying one can obtain samples \(x p_{t}(x x_{1})\) simply by interpolating samples \(x_{0} p_{0}\) for some \(p_{0}\) and \(x_{1} p_{1}\),

\[u_{t}(x x_{1}):=-x}{1-t} x=tx_{1}+(1-t)x_{0}x p_{t}(x x_{1}).\] (3)

Crucially, the flow matching objective

\[_{}()=_{t,x p_{t}(x)}[ \|v_{t}^{}(x)-u_{t}(x)\|_{2}^{2}]\] (4)is equivalent in expectation (up to a constant) to the conditional flow matching objective

\[_{}()=_{t,x_{1} p_{}( x_{1}),x p_{t}(x|x_{1})}[\|v_{t}^{}(x)-u_{t}(x x_{1}) \|_{2}^{2}].\] (5)

An advantage of flow matching is that this conditional trajectory \(u_{t}(x x_{1})\) can be chosen to make the problem tractable. The authors show that diffusion models can be instantiated as flow matching with a specific conditional trajectory, but also show that assuming a simple, straight-line trajectory leads to more efficient training. Note that in contrast to likelihood-based training of CNFs, flow matching is simulation free, leading to a scalable approach to learning CNFs.

## 3 Variational Flow Matching for Graph Generation

We derive CatFlow through a novel, variational view on flow matching we call _Variational Flow Matching (VFM)_. The VFM framework relies on two insights. First, we can define the marginal vector field and its approximation in terms of an expectation with respect to a distribution over end points of the transformation. This implies that we can map a flow matching problem onto a variational counterpart. Second, under typical assumptions on the forward process, we can decompose the expected conditional vector field into components for individual variables which can be computed in terms of the _marginals_ of the distribution over end points of the conditional trajectories. This implies that, without loss of generality, we can solve a VFM problem using a fully-factorized variational approximation, providing a tractable approximate vector field. For categorical data, the corresponding vector field can be computed efficiently via direct summation. This results in a closed-form objective to train CNFs for categorical data, which we refer to as _CatFlow_. We develop the theory of VFM in section 3 and we relate VFM to flow matching and score-based diffusion in section 4.

### Flow Matching using Variational Inference

In any flow matching problem, the vector field in eq.2 can be expressed as an expectation

\[u_{t}(x)= u_{t}(x x_{1})p_{t}(x_{1} x)x_{1}=_ {p_{t}(x_{1}|x)}[u_{t}(x x_{1})],\] (6)

where \(p_{t}(x_{1} x)\) is the posterior probability path, the distribution over possible end points \(x_{1}\) of paths passing through \(x\) at time \(t\),

\[p_{t}(x_{1} x):=(x,x_{1})}{p_{t}(x)}, p_{t}(x,x_{1}):=p_{t}(x x_{1})\,p_{}(x_{1}).\] (7)

This makes intuitive sense: the velocity in point \(x\) is given by all the continuations from \(x\) to final points \(x_{1}\), weighted by how likely that final point is given that we are at \(x\). Note that to compute \(u_{t}(x)\), one has to evaluate a joint integral over \(D\) dimensions.

This observation leads us to propose a change in parameterization of the learned vector field. Rather than predicting the components of the vector field directly, we can define an approximate vector field in terms of an expectation with respect to a variational distribution \(q_{t}^{}\) with parameters \(\),

\[v_{t}^{}(x):= u_{t}(x x_{1})\,q_{t}^{}(x_{1} x)\, x_{1}.\] (8)

Clearly, in this construction \(v_{t}^{}(x)\) will be equal to \(u_{t}(x)\) when \(q_{t}^{}(x_{1} x)\) and \(p_{t}(x_{1} x)\) are identical. This implies that we can map a flow matching problem onto a variational inference problem.

Concretely, we can define a variational flow matching problem by minimizing the Kullback-Leibler (KL) divergence from \(p_{t}\) to \(q_{t}^{}\), which we can express as

\[_{t}[p_{t}(x)p_{t}(x_{1} x)\|p_{t}(x)q_ {t}^{}(x_{1} x)]=-_{t,x,x_{1}}[ q_{ t}^{}(x_{1} x)]+,\] (9)

where \(t(0,1)\) and \(x,x_{1} p_{t}(x,x_{1})\) (see appendix A.1 for derivations). This leads us to propose the _variational flow matching (VFM)_ objective

\[_{}()=-_{t,x,x_{1}}[ q_{t}^{ }(x_{1} x)].\] (10)

While this variational formulation of flow matching is promising, two potential drawbacks emerge in practical applications. First, although it is feasible to reformulate any flow matching problem as a variational inference task, doing so requires learning an approximation of a potentially complex, high-dimensional distribution \(p_{t}(x_{1} x)\) (including any correlations between the different \(x_{1}^{d}\)). Second, representing \(v_{t}^{}(x)\) as an expectation may pose intractability challenges. Interestingly, under typical assumptions about the conditional velocity field in flow matching, this objective simplifies significantly, making it no more computationally demanding than standard flow matching, a point we will further address in section 3.2.

### Mean-Field Variational Flow Matching

Decomposing the conditional vector field.At first glance, we do not seem to obtain much from this variational view due to the intractability of \(p_{t}(x_{1} x)\) and \(v_{t}^{}(x)\). Fortunately, we can simplify the objective and the calculation of the marginal vector field under the typical case where the conditional vector field \(u_{t}(x x_{1})\) is linear in \(x_{1}\), such as in straight line interpolations commonly used in flow matching. This leads to two simplifications. First, we notice that the expected value of a linear conditional velocity field simply equals the conditional velocity field towards the expectation/mean of the distribution we are considering, i.e.

\[u_{t}(x)=_{p_{t}(x_{1} x)}[u_{t}(x x_{1})]=u_{t} (x_{p_{t}}[x_{1} x])u_{t}(x x_{1})x_{1}.\] (11)

This means that as long as our variational distribution has the same mean as \(p_{t}(x_{1} x)\), we will learn the same underlying field. Second, we realize that the expected value of \(x_{1}^{d}\) only depends on \(p_{t}(x_{1}^{d} x)\), and thus as such as long as \(q_{t}^{}(x_{1} x)\) has the same marginal expectations \([x_{1}^{d} x]\), we will learn the same field as under \(p_{t}(x_{1} x)\). In other words, it suffices to learn a fully-factorized approximation \(q_{t}^{}(x_{1} x)\), there is no need to fully characterize the covariance of the posterior probability path. This allows us to reduce a high-dimensional variational problem to a series of low dimension problems.

Formally, the following holds:

**Theorem 1**.: Assume that the conditional vector field \(u_{t}(x x_{1})\) is linear in \(x_{1}\). Then, for any distribution \(r_{t}(x_{1} x)\) such that the marginal distributions coincide with those of \(p_{t}(x_{1} x)\), the corresponding expectations of \(u_{t}(x x_{1})\) are equal, i.e.

\[_{r_{t}(x_{1} x)}[u_{t}(x x_{1})]=_{p _{t}(x_{1} x)}[u_{t}(x x_{1})].\] (12)

We provide a proof in appendix A.3.

It follows directly from theorem 1 that _without loss of generality_ we can consider the considerably easier task of a fully-factorized approximation

\[q_{t}^{}(x_{1} x):=_{d=1}^{D}q_{t}^{}(x_{1}^{d} x).\] (13)

We refer to this special case as _mean-field variational flow matching_ (MF-VFM), and the VFM objective reduces to

\[_{}()=-_{t,x,x_{1}}[ q_{t}^{ }(x_{1} x)]=-_{t,x,x_{1}}[_{d=1}^{D} q_ {t}^{}(x_{1}^{d} x)].\] (14)

Even though we use a factorized model, it is worth emphasizing that _due to the linearity of the conditional field, a mean-field variational distribution can learn the solution exactly_.

Computing the marginal vector field.To calculate the vector field \(v_{t}^{}(x)\), we can simply substitute the factorized distribution \(q_{t}^{}(x_{1} x)\) into eq. (8). However, this still requires an evaluation of an expectation. Fortunately, leveraging the linearity condition significantly simplifies this computation, since as long as we have access to the first moment of one-dimensional distributions \(q_{t}^{}(x_{1}^{d} x)\), we can efficiently calculate \(v_{t}^{}(x)\) by simply considering the conditional field towards it. Note that for many distributions - e.g. Gaussian - learning its parameters is equivalent to learning its expected value. Note that the training procedure will differ for two distinct distributions - e.g. Gaussian versus Categorical - so the form of the distribution \(q_{t}^{}(x_{1} x)\) remains practically important, a flexibility provided through the variational view on flow matching.

If we now use the standard flow matching case of using a conditional vector field based on a linear interpolation, the approximate vector field can be expressed in terms of the first moment of the variational approximation:

\[v_{t}^{}(x)=_{q_{t}^{}(x_{1}|x)}[-x}{1-t} ]=-x}{1-t},_{1}:=_{q_{t}^{}( x_{1}|x)}[x_{1}].\] (15)

Note that this covers both the case of categorical data, which we focus on in this paper, and the case of continuous data, as considered in traditional flow matching methods. We provided the general algorithm for training and generation in appendix B.1.

At first glance, the linearity condition of the conditional vector field \(u_{t}(x x_{1})\) in theorem 1 might seem restricting. However, in most state-of-the-art generative modeling techniques, this condition is satisfied, e.g. diffusion-based models, such as flow matching [27; 2; 32], diffusion models [15; 54], and models that combine the injection of Gaussian noise with blurring [44; 16], among others [10; 51].

### CatFlow: Mean-Field Variational Flow Matching for Categorical Data

The CatFlow ObjectiveIn CatFlow, we directly apply the VFM framework to the categorical case. Let our parameterised variational distribution \(q_{t}^{}(x_{1}^{d} x)=(x_{1}^{d}_{t}^{d}(x))\), and let us denote the parameters of this categorical as

\[_{t}^{dk}(x):=q_{t}^{}(x_{1}^{d}=k x)=_{q_{t}^{}( x_{1}|x)}[[x_{1}^{d}=k]].\] (16)

Then, the \(d\)th component of the learned vector field is

\[v_{t}^{,d}(x):=_{k=1}^{K_{d}}_{t}^{dk}(x)[x_{1}^{ d}=k]-x}{1-t}.\] (17)

Intuitively, CatFlow learns a _distribution_ over the conditional trajectories to all corners of the probability simplices, rather than regressing towards an expected conditional trajectory.

In the categorical setting, the MF-VFM objective can be written out explicitly. Writing out the probability mass function of the categorical distribution, we see that

\[ q_{t}^{}(x_{1}^{d} x)=_{k=1}^{K^{d}}(_{t}^{dk}(x))^ {[x_{1}^{d}=k]}=_{d=1}^{D}[x_{1}^{d}=k]_{t}^{dk }(x).\] (18)

As such, we find that _CatFlow_ objective is given by a standard cross-entropy loss:

\[_{}()=-_{t,x,x_{1}}[_{d=1}^{ D}_{k=1}^{K^{d}}[x_{1}^{d}=k]_{t}^{dk}(x)].\] (19)

Note, however, that when actually computing \(v_{t}^{}\), this can be done efficiently, since

\[_{q_{t}^{}(x_{1}^{d}|x)}[u_{t}(x^{d} x_{1}^{d}) ]=_{q_{t}^{}(x_{1}^{d}|x)}[^{d}-x^{d}}{ 1-t}]=(x)-x^{d}}{1-t},\] (20)

Figure 1: Parameterization of the vector field in CatFlow. Given an interpolant \(x_{t}=tx_{1}+(1-t)x_{0}\), CatFlow predicts a categorical distribution \(q_{t}^{}(x_{1} x_{t})\) parameterized by a vector \(_{t}(x_{t})\). The resulting construction for the vector field \(v_{t}^{}(x_{t})=(_{t}(x_{t})-x_{t})/(1-t)\) ensures that trajectories converge to a point on the simplex at \(t=1\).

since \(^{d}(x):=_{q_{t}^{g}(x_{t}^{}|x)}[u_{t}^{d}(x_{t}^{} x)]\) and the other terms are not in the expectation. Note that this geometrically corresponds to learning the mapping to a point in the probability simplex, and then flowing towards that. This procedure is illustrated in fig. 1. Because of this, training CatFlow is no less efficient than flow matching. We provided the algorithm for training and generation for CatFlow and a Gaussian VFM objective in appendix B.2 and appendix B.3 respectively.

More precisely, training CatFlow offers two key benefits over standard flow matching. First, given that CatFlow predicts points in the probability simplex and parametrizes the velocities to point into it, an inductive bias is introduced ensuring all generative paths align with realistic trajectories, hence avoiding misaligned paths. Second, using a cross-entropy loss instead of a mean-squared error improves gradient behavior during training. Both aspects enhance learning dynamics and speed up convergence, which is evaluated in section 6.3. Lastly, CatFlow's ability to learn probability vectors - rather than directly choosing classes as is common in discrete approaches - allows the model to express uncertainty about variables at a specific time. This is especially useful in complex domains like molecular generation, where initial uncertainty about components decreases as more structure is established, leading to more precise predictions.

Permutation Equivariance.Graphs, defined by vertices and edges, lack a natural vertex order unlike other data types. This permutation invariance means any vertex labeling represents the same graph if the connections remain unchanged. Note that even though the following results apply to graphs, an unordered set of categorical variables can be described by a graph without edges. Under natural conditions - see appendix A - we ensure CatFlow adheres to this symmetry (see appendix A.4 for the proof).

**Theorem 2**.: CatFlow generates exchangeable distributions, i.e. CatFlow generates all graph permutations with equal probability.

## 4 Flow Matching and Score Matching: Bridging the Gap

In this section, we relate the VFM framework to existing generative modeling approaches. First, we show that VFM has standard flow matching as a special case when the variational approximation is Gaussian. This implies that VFM provides a more general approach to learning CNFs. Second, we show that through VFM, we are not only able to compute the target vector field, but also the score function as used in score-based diffusion. This has two primary theoretical implications: 1) VFM simultaneously learns deterministic and stochastic dynamics - as diffusion models rely on stochastic dynamics, and 2) VFM provides a variational bound on the model likelihood.

Relationship to Flow Matching.VFM admits FM as a special case, under certain assumptions on \(u_{t}(x x_{1})\), when the variational approximation is Gaussian. Formally, the following holds (see theorem 3 in appendix A.2 for the proof):

**Theorem 3**.: Assume the conditional vector field \(u_{t}(x x_{1})\) is linear in \(x_{1}\) and is of the form

\[u_{t}(x|x_{1})=A_{t}(x)x_{1}+b_{t}(x),\] (21)

where \(A_{t}(x):^{D}^{D}^{D}\) and \(b_{t}(x):^{D}^{D}\). Moreover, assume that \(A_{t}(x)\) is an invertible matrix and \(q_{t}^{}(x_{1} x)=(x_{1}_{t}^{}(x),_{ t}(x))\), where \(_{t}(x)=(A_{t}^{}(x)A_{t}(x))^{-1}\). Then, VFM reduces to flow matching.

Relationship to Score-Based Models.Flow matching  is inspired by score-based models  and shares strong connections with them. This leads us to two observations. The first is that it should be possible to define a variational parameterization of the score function in diffusion models that is analogous to the one in VFM. The second is that we can build on existing results that show that many diffusion model objectives, including the standard flow matching objective with a linear interpolant, can be expressed as special cases of a general weighted loss function . Here we simililarly define a bound on the log-likelihood in terms of a reweighted VFM objective.

In score-based models, the objective is to approximate the score function \(_{x} p_{t}(x)\) with a function \(s_{t}^{}(x)\). A connection to VFM becomes apparent by observing that the score function can also be expressed as an expectation with respect to \(p_{t}(x_{1} x)\) (see appendix A.5 for derivation):

\[_{x} p_{t}(x)= p_{t}(x_{1} x)_{x} p_{t}(x x_{1}) x_{1}=_{p_{t}(x_{1} x)}[_{x} p_{t}(x  x_{1})],\] (22)

where \(_{x} p_{t}(x x_{1})\) is the tractable conditional score function. Similarly, we can parameterize \(s_{t}^{}(x)\) in terms of an expectation with respect to a variational approximation \(q_{t}^{}(x_{1} x)\),

\[s_{t}^{}(x):= q_{t}^{}(x_{1} x)_{x} p_{t}(x x _{1})\,x_{1}.\] (23)

It is now clear that \(s_{t}^{}(x)=_{x} p_{t}(x)\) when \(q_{t}^{}(x_{1} x)=p_{t}(x_{1} x)\). This suggests that there exists a variational formulation of score-based models that is entirely analogous to VFM. Indeed, existing work on continuous diffusion for categorical data  defines a parameterization of the score function of this form (see section 5 for a more detailed discussion).

Following [54; 3], we can construct stochastic generative dynamics \(dx=_{t}^{}(x)dt+g_{t}dw\) to approximate the true dynamics \(dx=_{t}(x)dt+g_{t}dw\) (see details in appendix A.6), with

\[_{t}(x):=_{p_{t}(x_{1} x)}[u_{t}(x x_{1})+ ^{2}}{2}_{x} p_{t}(x x_{1})],_{t} ^{}(x):=v_{t}^{}(x)+^{2}}{2}s_{t}^{}(x).\] (24)

Here \(g_{t}:_{+}\) is a scalar function, and \(w\) is a standard Wiener process.

This connection has two important implications. First, it shows that learning a variational approximation \(q_{t}^{}(x_{1} x)\) can be used to define both deterministic and stochastic dynamics, whereas flow matching typically considers deterministic dynamics only (as flows are viewed through the lens of ODEs). Second, it enables us to show that a reweighted version of the VFM objective provides a bound on the log-likelihood of the model. This result, inspired by , provides another theoretical motivation for learning using the VFM objective.

**Theorem 4**.: Rewrite the Variational Flow Matching objective as follows:

\[_{}()=_{t,x}[^{}( t,x)]^{}(t,x)=-_{x_{1}} [ q_{t}^{}(x_{1} x)].\] (25)

Then, the following holds:

\[-_{x_{1}}[ q_{1}^{}(x_{1})]_{t, x}[_{t}(x)^{}(t,x)]+C,\] (26)

where \(_{t}(x)\) is a non-negative function and \(C\) is a constant.

We provide a proof in appendix A.7. We further note that that applying the same linearity condition that we discussed in section 3.2 to the conditional score function maintains all the same connections with score-based models.

## 5 Related Work

Diffusion Models for Discrete Data.Several approaches to diffusion models have been developed for graph generation. In , the authors define a Markov process that progressively edits graphs by adding or removing edges and altering node or edge categories and is trained using a graph transformer network, that reverses this process to predict the original graph structure from its noisy version. This approach breaks down the complex task of graph distribution learning into simpler node and edge classification tasks. Moreover,  proposes a score-based generative model for graph generation using a system of stochastic differential equations (SDEs). The model effectively captures the complex dependencies between graph nodes and edges by diffusing both node features and adjacency matrices through continuous-time processes. These are non-autoregressive graph generation approaches that perform on par with autoregressive ones, such as in [31; 26; 36]. Other non-autoregressive approaches worth mentioning are [28; 34; 33].

There is also work on diffusion-based models for discrete data in the form of text and other sequential data [4; 33; 61; 18]. Though not explicitly formulated in terms of a variational perspective, the work on continuous diffusion for categorical data  arrives at a an approach that is closely related to that of CatFlow. This work defines a diffusion in the embedding space of a transformer-based language model. It defines an approximation of the score function as an expected value of a conditional score function as in eq. (23), which leads to an expression for the learned score function in terms of a mean embedding, analogous to the one we obtain in eq. (15). Where this approach differs from CatFlow, other than in that it defines a flow in the embedding space of language models, is in how the objective is defined. The authors also minimize a cross-entropy loss, but employ a time-warped objective, similar to the general weighted objective proposed in , where the warping is optimized to ensure that the entropy of predictions decreases linearly when moving from noise to data in uniform time.

Flow-based methods for Discrete Data.Recently, two flow-based methods for discrete generative modeling have been proposed, which differ both in terms of technical approach and intended use case from the work that we present here.2

In , a Dirichlet flow framework for DNA sequence design is introduced, utilizing a transport problem defined over the probability simplex, similar to diffusion on simplices proposed in . This approach differs from CatFlow in that it represents the conditional probability path \(p_{t}(x x_{1})\) using a Dirichlet distribution. This implies that points \(x\) are constrained to the simplex, which is not the case for CatFlow. Dirichlet Flows have not been evaluated on graph generation, but we did carry out preliminary experiments based on the released source code. We compare to this approach in our experiments.

In , Discrete Flow Models (DFMs) are introduced. DFMs use Continuous-Time Markov Chains to enable flexible and dynamic sampling in multimodal generative modeling of both continuous and discrete data. Though sharing a goal, this approach differs significantly from CatFlow as in the end the resulting model does not learn a CNF, but rather generation through sequential sampling from a time-dependent categorical distribution. As in the case of Dirichlet flows, no evaluation on graph generation was performed.

The switch to the variational perspective is inspired by , showing significant improvement through viewing the dynamics as a classification task over end points. However, CatFlow is still a continuous model, and integrates - rather than iteratively samples - during generation.

## 6 Experiments

We evaluate CatFlow in three sets of experiments. First, we consider an abstract graph generation task proposed in , where the goal of this task is to evaluate if CatFlow is able to capture the topological properties of graphs. Second, we consider two common molecular benchmarks, QM9  and ZINC250k , consisting of small and (relatively) large molecules respectively. This task is chosen to see if CatFlow can learn semantic information in graph generation, such as molecular properties. Finally, we perform an ablation comparing CatFlow to standard flow matching, specifically in terms of generalization. The experimental setup and model choices are provided in appendix D.

Note that we treat graphs as purely categorical/discrete objects and do not consider 'geometric' graphs that are embedded in e.g. Euclidean space. Specifically, for some graph with \(K_{v}\), node classes and \(K_{e}\) edge classes, we process the graph as a fully-connected graph, where each node is treated as a

    &  &  \\   & Degree \(\) & Clustering \(\) & Orbit \(\) & Degree \(\) & Clustering \(\) & Orbit \(\) \\  GraphVAE  & 0.130 & 0.170 & 0.050 & 0.350 & 0.980 & 0.540 \\ GNF  & 0.030 & 0.100 & 0.001 & 0.200 & 0.200 & 0.110 \\ EDP-GNN  & 0.052 & 0.093 & 0.007 & 0.053 & 0.144 & 0.026 \\ GDSS  & 0.021 & **0.024** & **0.007** & 0.045 & **0.086** & **0.007** \\ 
**CatFlow** & **0.013** & **0.024** & 0.008 & **0.018** & **0.086** & **0.007** \\   

Table 1: Results abstract graph generation.

categorical variable of one of \(K_{v}\) classes and each edge of \(K_{e}+1\) classes, where the extra class corresponds with being absent.

### Abstract graph generation

We first evaluate CatFlow on an abstract graph generation task, including synthetic and real-world graphs. We consider 1) Ego-small (200 graphs), consisting of small ego graphs drawn from a larger Citeseer network dataset , 2) Community-small (100 graphs), consisting of randomly generated community graphs, 3) Enzymes (587 graphs), consisting of protein graphs representing tertiary structures of the enzymes from , and 4) Grid (100 graphs), consisting of 2D grid graphs. We follow the standard experimental setup popularized by  and hence report the maximum mean discrepancy (MMD) to compare the distributions of degree, clustering coefficient, and the number of occurrences of orbits with 4 nodes between generated graphs and a test set. Following , we also use the Gaussian Earth Mover's Distance kernel to compute the MMDs instead of the total variation.

The results of the Ego-small and Community-small tasks are summarized in table 1, and additional results (and error bars) are provided in appendix C. The results indicate that CatFlow is able to capture topological properties of graphs, and performs well on abstract graph generation.

### Molecular Generation: QM9 & ZINC250k

Molecular generation entails designing novel molecules with specific properties, a complex task hindered by the vast chemical space and long-range dependencies in molecular structures. We evaluate CatFlow on two popular molecular generation benchmarks: QM9 and ZINC250k [40; 19].

We follow the standard setup - e.g. as in [49; 33; 58; 20] - of kekulizing the molecules using RDKit  and removing the hydrogen atoms. We sample 10,000 molecules and evaluate them on validity, uniqueness, and Frechet ChemNet Distance (FCD) - evaluating the distance between data and generated molecules using the activations of ChemNet . Here, validity is computed without valency correction or edge resampling, hence following  rather than [49; 33], as is more reasonable due to the existence of formal charges in the data itself. We do not report novelty for QM9 and ZINC250k, as QM9 is an exhaustive list of all small molecules under some chemical constraint and all models obtain (close to) 100% novelty on ZINC250k.3

    &  &  \\   & Valid \(\) & Unique \(\) & FCD \(\) & Valid \(\) & Unique \(\) & FCD \(\) \\  MoFlow  & 91.36 & 98.65 & 4.467 & 63.11 & 99.99 & 20.931 \\ EDP-GNN  & 47.52 & 99.25 & 2.680 & 82.97 & 99.79 & 16.737 \\ GraphEBM  & 8.22 & 97.90 & 6.143 & 5.29 & 98.79 & 35.471 \\ GDSS  & 95.72 & 98.46 & 2.900 & 97.01 & 99.64 & 14.656 \\ Digress  & 99.00 & 96.20 & - & - & - & - \\ Flow Matching  & 94.10 & 98.20 & 5.155 & 94.01 & 96.68 & 18.764 \\ Dirichlet FM  & 99.10 & 98.15 & 0.888 & 97.52 & 99.20 & 14.222 \\ 
**CatFlow** & **99.81** & **99.95** & **0.441** & **99.21** & **100.00** & **13.211** \\   

Table 2: Results molecular generation.

Figure 2: CatFlow samples of QM9 (top) and ZINC250k (bottom).

The results are summarized in table 2 and samples from the model are shown in fig. 2. CatFlow obtains state-of-the-art results on both QM9 and ZINC250k, virtually obtaining perfect performance on both datasets. It is worth noting that CatFlow also converges faster than flow matching and is not computationally more expensive than any of the baselines either during training or generation.

### CatFlow Ablations

To understand the difference in performance between CatFlow and a standard flow matching formulation we perform ablations. we focus on generalization capabilities, and as such consider ablations that the number of parameters in the model and the amount of training data.

In fig. 3 we report a score, which is the percentage of generated molecules that is valid _and_ unique. CatFlow not only outperforms regular flow matching in the large model and full data setting, but is also significantly more robust to a decrease in model-size and data. Moreover, we observe significantly faster convergence (curves not shown). We hypothesize this is a consequence of the optimization procedure not exploring 'irrelevant' paths that do not point towards the probability simplex.

## 7 Conclusion

We have introduced a variational reformulation of flow matching. This formulation in turn informed the design of a simple flow matching method for categorical data, which achieves strong performance on graph generation tasks. Variational flow is very general and opens up several lines of inquiry. We see immediate opportunities to apply CatFlow to other discrete data types, including text, source code, and more broadly to the modeling of mixed discrete-continuous data modalities. Additionally, the connections to score-based models that we identify in this paper, suggest a path towards learning both deterministic and stochastic dynamics.

Limitations.While the VFM formulation that we identify in this paper has potential in terms of its generality, we have as yet only considered its application to the specific task of categorical graph generation. We leave other use cases of VFM to future work. A limitation of CatFlow, which is shared with related approaches to graph generation, is that reasoning about the set of possible edges has a cost that is quadratic in the number of nodes. As a result CatFlow does not scale well to e.g. large proteins of \(10^{4}\) or more atoms.

Ethics Statement.Graph generation in general, and molecular generation specifically, holds great promise for advancing drug discovery and personalized medicine. However, this technology also poses ethical concerns, such as the potential for misuse in creating harmful substances. In terms of technology readiness, this work is not yet at a level where we foresee direct benefits or risks.

Acknowledgements.This project was supported by the Bosch Center for Artificial Intelligence.

Figure 3: Ablation results. We compare standard flow matching and CatFlow. We visualize performance degradation in terms of a score, which is the percentage of molecules that is valid and unique, for a varying number of layers and percentage of the training data.