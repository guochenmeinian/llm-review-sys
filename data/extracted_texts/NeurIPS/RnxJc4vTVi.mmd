# SCaR: Refining Skill Chaining for Long-Horizon Robotic Manipulation via Dual Regularization

Zixuan Chen\({}^{1}\)  Ze Ji\({}^{2}\)  Jing Huo\({}^{1}\)1  Yang Gao\({}^{1}\)

\({}^{1}\)State Key Laboratory for Novel Software Technology, Nanjing University, China,

\({}^{2}\)School of Engineering, Cardiff University, UK,

chenzx@nju.edu.cn, jiz1@cardiff.ac.uk, huojing@nju.edu.cn, gaoy@nju.edu.cn

Corresponding author.

###### Abstract

Long-horizon robotic manipulation tasks typically involve a series of interrelated sub-tasks spanning multiple execution stages. Skill chaining offers a feasible solution for these tasks by pre-training the skills for each sub-task and linking them sequentially. However, imperfections in skill learning or disturbances during execution can lead to the accumulation of errors in skill chaining process, resulting in execution failures. In this paper, we investigate how to achieve stable and smooth skill chaining for long-horizon robotic manipulation tasks. Specifically, we propose a novel skill chaining framework called Skill Chaining via Dual **R**egularization (**SCaR**). This framework applies dual regularization to sub-task skill pre-training and fine-tuning, which not only enhances the _intra-skill dependencies_ within each sub-task skill but also reinforces the _inter-skill dependencies_ between sequential sub-task skills, thus ensuring smooth skill chaining and stable long-horizon execution. We evaluate the SCaR framework on two representative long-horizon robotic manipulation simulation benchmarks: IKEA furniture assembly and kitchen organization. Additionally, we conduct a simple real-world validation in tabletop robot pick-and-place tasks. The experimental results show that, with the support of SCaR, the robot achieves a higher success rate in long-horizon tasks compared to relevant baselines and demonstrates greater robustness to perturbations.

## 1 Introduction

Long-horizon robotic manipulation tasks are characterized by sequences of diverse and interdependent sub-tasks, which makes it crucial to maintain the stability of multi-stage sequential execution. For instance, in the robotic assembly of a stool (Fig. 1) involving two sub-tasks of leg installation, overall success is evaluated based on both the sequential installation success and factors affecting the assembly within environmental constraints. Although recent advances in deep reinforcement learning (RL) and imitation learning (IL) show promise in training robots for such complex tasks , managing long-horizon tasks with a scratch RL or IL policy remains challenging due to computational demands, extensive exploration, and intricate step dependencies . Skill chaining, which involves decomposing long-horizon tasks into smaller sub-tasks, pre-training skills for each, and executing them sequentially, offers a practical solution . However, as shown in Fig. 1(a)(b), such methods tend to fail when sub-task skills are insufficiently trained or unexpected states arise due to disturbances, especially when applied to high-degree-of-freedom robots performing contact-rich, long-horizon tasks. .

In this paper, we argue that the coordination and enhancing of dependencies within and between sub-task skills is necessary for stable and smooth skill chaining of long-horizon robotic manipulation .

For instance, as depicted in Fig. 1 (a)(b), the robot must consider following two points to ensure the overall task is accomplished: 1) ensuring the gripper consistently grasps and installs the stool leg stably within each sub-task skill range. and 2) ensuring the terminal state of previous skill aligns with the initial state of next skill for smooth skill chaining. We define the above two points as _intra-skill dependencies_ between sequential actions within each sub-task skill and _inter-skill dependencies_ between sequential sub-task skills, respectively. In this context, we propose a novel robotic skill chaining framework, **S**kill **C**haining via Dual **R**egularization (**SCaR**). This framework enhances the aforementioned dependencies alternately through dual regularization during sub-task skill learning and chaining, aiming to provide stability for the execution of long-horizon robotic manipulation.

Specifically, in the pre-training phase of each sub-task skill, we propose the _adaptive sub-task skill learning_ shceme, which employs a two-part policy learning objective that focuses on what sub-tasks the robot should perform (via RL) and how the robot should perform that task (via IL), and utilizes a novel adaptive equilibrium scheduling (AES) regularization to balance these two parts based on the robot's learning progress. This process aims to reinforce the _intra-skill dependencies_, ensuring a coherent sequence of actions in each sub-task skill. Subsequently, _bi-directional adversarial learning_ is introduced in the fine-tuning phase of SCaR for better chaining sequential sub-task skills. This mechanism uses bi-directional regularization to bring the terminal state of the current skill close to the initial state of its successor, and also to bring the initial state of the successor close to the terminal state of the current skill. This bi-directional alignment aims to reinforce robust _inter-skill dependencies_ between sequential skills. Through the two innovative designs described, SCaR ensures coordination between the _intra-skill_ and _inter-skill_ dependencies, provides dual constraints for skill learning and skill chaining, as described in Fig. 1 (c), leading to a smooth skill chaining from the inside **(within the sub-task skills)** to the outside **(between sub-task skills)**. Experimental results show that compared to scratch-training and skill chaining baselines, SCaR provides better task execution performance and stronger robustness to environmental perturbations in various long-horizon and contact-rich robotic manipulation simulation tasks. In addition, we conduct a simple validation in real-world tabletop robot pick-and-place tasks, and the results show that SCaR achieves a higher task success rate compared to previous skill-chaining methods.

The principal contributions of our work are delineated as follows: **1)** We propose a novel robotic skill chaining framework via dual regularization, SCaR, for smoothly executing long-horizon manipulation

Figure 1: Illustration of the problem setting and the motivation of SCaR, using the example of a stool assembly task with two sub-tasks. Best viewed when zoomed in.

tasks. **2)** We introduce an adaptive sub-task skill learning scheme that acts as a regularization to enhance _intra-skill dependencies_ between sequential actions within each sub-task skill. **3)** We develop a bi-directional adversarial learning mechanism that serves as a regularization for reinforcing _inter-skill dependencies_ between sequential sub-task skills. **4)** In all eight simulated long-horizon robotic manipulation tasks and simple real-world pick-and-place tasks, SCaR demonstrates significantly better performance than scratch-training and skill-chaining baselines. Video demonstrations are available at: https://sites.google.com/view/scar8297.

## 2 Related Work

### Long-horizon Robotic Manipulation

Training robots from scratch for complex, long-horizon tasks using reinforcement learning (RL) and imitation learning (IL) is challenging due to computational demands and distributional errors. Solutions involve decomposing tasks into reusable sub-tasks . Typically, such algorithms consist of a set of sub-policies that can be obtained through various methods, such as unsupervised exploration [19; 20; 21; 22; 23], learning from demonstrations [5; 6; 24; 25], and predefined measures [26; 27; 28; 29; 14]. Despite the merits of each of these approaches, they do not address well the challenges of long-horizon robot manipulation in environments that are object-rich, contact-rich, and characterized by multi-stage tasks [28; 29; 14]. Thus, even when pre-trained skills are provided, ensuring a smooth connection between manipulation policies remains a formidable challenge.

### Skill Chaining for Long-horizon Tasks

Previous skill chaining methods for long-horizon tasks mainly focus on updating each sub-task policy to encompass the terminal state of the previous policy [11; 14; 30], implementing option chains [11; 31; 32] to forge logical skill sequences, or utilizing modulated skills to facilitate smoother transitions [33; 34; 35; 36; 14; 16]. However, these methods, while effective, often lead to a broad range of skill start and end states, a challenge in complex robotic manipulation tasks. T-STAR  is closely related to our work, addressing this by regularizing the learning process with a discriminator to control the expansion of the terminal state space. However, it focuses only on uni-directional dependencies between skills and ignores intra-skill dependencies within sub-task skills under long-horizon goals. Sequential Dexterity  centers on dexterous hand manipulation, introducing an optimization process to backpropagate long-term rewards across a policy chain. However, its scope still primarily emphasizes strengthening the dependencies between sub-task skills. GSC  attempts to solve skill chaining by employing diffusion models. It trains and chains primitive skills (pick, place, push, pull) through a Transformer-based skill diffusion model. However, due to the use of Transformer-based techniques, GSC requires high computational resources and cannot scale well to task environments with object-rich and contact-rich conditions. Our method instead employs simple and intuitive dual regularization constraints based on the lightweight policy network. By coordinating the dependencies within and between skills, we achieve refinement within sub-task policies and bi-directional alignment between them. This allows for stable skill chaining while also being scalable to various long-horizon manipulation tasks.

## 3 Preliminaries

Among several related works on skill chaining, we consider a challenging yet practical problem setting that _deals with long-horizon manipulation tasks through a combination of reinforcement learning (RL) and imitation learning (IL)_. In each sub-task in the long-horizon task, we consider robotic agents acting within a finite-horizon Markov Decision Process \((,,,r,,d_{},T)\), where \(\) is the state space, \(\) is the action space, \((s^{}|s,a)\) is the transition function, \(r(s,a,s^{})\) is the reward function, \(\) is the discount factor, \(d_{}\) is the initial state distribution, and \(T\) is the episode horizon of sub-task. We define a policy \(:\) that maps states to actions and correspondingly moves the robotic agent to a new state according to the transition probabilities. This sub-task policy is trained to maximize the expected sum of discounted rewards \(_{(s,a)}[_{t=1}^{T}^{t}r(s_{t},a_{t},s_{t+1})]\). We assume that each sub-task policy has an initial state set \(\) and a terminal state set \(\), where the initial set \(\) contains all the initial states that lead to the successful execution of the policy and the terminal state set \(\) contains all the final states of the successful execution. The environment 

[MISSING_PAGE_FAIL:4]

and attach it), and \(r_{i}^{ Pred}(s_{t},a_{t};)\) is the predicted reward by a least-square GAIL discriminator \(f_{}^{i}\), which is more stable than the standard GAIL objective using the sigmoid cross-entropy loss function. Therefore, the predicted reward is:

\[r_{i}^{ Pred}(s_{t},a_{t};)=[0,1-0.25[f_{}^{i}(s_{t},a_{t})- 1]^{2}].\] (2)

We adopt the training objective of the least-squares GAIL discriminator  with a gradient penalty term , This penalty term mitigates the instability of the training dynamics due to the interplay between the discriminator and the policy , as follows:

\[*{argmin}_{f_{}^{i}}_{(s)^{E}}[(f_{ }^{i}(s)-1)^{2}]+_{(s)_{}^{i}}[(f_{}^{i}(s)+ 1)^{2}]+}{2}_{(s)^{E}}[\|_{s} f_{}^{i}(s)\|^{2}],\] (3)

where \(^{ SP}\) is a manually-specified coefficient. The scales of \(r^{ Env}\) and \(r^{ Pred}\) in previous related works are set by fixed weights and linearly combined into the final reward function . This could lead to the agent rigidly imitating experts and curbing self-exploration, finding it difficult to adjust intra-skill dependencies and adapt to dynamic task perturbations. We propose a principle to counter this: _If the agent fails to imitate the expert's demonstration well, it should shift focus to self-learning from the environment. Conversely, effective imitation should continue, focusing on the expert to mitigate low sample efficiency in reinforcement learning._ Accordingly, we extend the automatic discount scheduling (ADS) solution  to our problem setting, and propose adaptive equilibrium scheduling (AES) to regularize the scales of \(r^{ Env}\) and \(r^{ Pred}\) in sub-task skill learning for adaptive scheduling the focus of reinforcement and imitation learning, as shown in Fig. 3.

Adaptive Equilibrium Scheduling (AES) RegularizationSpecifically, AES balances the scales of \(r^{ Env}\) and \(r^{ Pred}\) during the learning process of each skill through adaptive scheduling of \(_{ RL}\) and \(_{ IL}\), according to how well the agent imitates the expert's demonstration. To capture the agent's imitation progress, AES refers to the solution in ADS  and uses the imitation identifier \(\) to continuously monitor whether the agent is imitating the expert demonstration well enough.

At the beginning of training, the agent is assigned two initial balance factors \(_{ RL}=,_{ IL}=1-\), where base exponent \(\). We set \(=0.5\) in the experiments and the agent is assigned two identical balance factors \(_{ RL}=_{ IL}=0.5\)2, indicating that at the beginning of learning, the agent imitates the expert's behavior with the same weight as the behavior of environment exploration according to the task goal. As training progresses, the imitation progress recognizer \(\) is queried periodically to monitor the progress of the agent's imitation of the expert's behavior. \(\) receives the agent's collected trajectories and infers the agent's current imitation progress \(p[0,T)\), where \(p\) in an integer and \(T\) is the step of the entire episode.

The construction of \(\), with reference to ADS, first requires the construction of a sequence \((q_{1},,q_{T})\), where \(q_{i}=*{argmin}_{j}c(s_{i},s_{j}^{E})\) is the index of the nearest neighbor of \(s_{i}\) in \(^{E}\), \(c\) is the cosine similarity. The progress alignment between \(\) and \(_{j}^{E}\) is measured as the length of the longest increasing subsequence (LIS) in **Q**, denoted as \(LIS(,^{E})\). Specifically, the agent's imitation progress \(p\) is increased by 1 if the following inequality holds:

\[_{^{E}^{E}}LIS(_{1:p+1},_{1:p+1}^{E} )_{^{E},^{E}^{E}}LIS(_{1:p+1}^{E},_{1:p+1}^{E}),\] (4)

where \(^{E}^{E}\), the subscript \(1:p+1\) denotes the first \(p+1\) steps of the trajectory, and \(\) controls the strictness of the imitation progress monitoring. This suggests that the similarity of the agent trajectory to its best matching expert trajectory at time step \(p+1\) exceeds the minimal similarity criterion within the expert demonstration. See Appendix B for detailed explanation of AES.

After obtaining the current imitation progress \(p\) of the agent, AES then adopts a mapping function \(_{}(p)\) to schedule the two new balance discount factors \(_{ RL}\) and \(_{ IL}\). Straightforward idea of setting

Figure 3: AES regularization for sub-task skill learning.

\(_{}(p)\) is that **If \(p\) is larger and reaches a certain threshold, i.e., the agent is able to imitate the expert behavior well, then the more the agent tends to imitate the expert's behavior in subsequent training, and vice versa.** Therefore, we set the threshold as \(\). If \(p[0,)\), we propose \(_{}(p)=1-e^{(-)}\); if \(p[,T)\), we propose \(_{}(p)=e^{(-}{k})}\), where \(k\) is used to flatten the curve of the mapping function. Then \(_{}\) and \(_{}\) are scheduled to be :

\[_{}=^{_{}(p)},_{ }=1-^{_{}(p)}.&p[0,)\\ _{}=^{_{}(p)},_{}=1- ^{_{}(p)}.&p[,T)\] (5)

Consequently, the RL and IL components of sub-task skill learning can be adaptively scheduled and regularized through AES, effectively enhancing _intra-skill dependencies_ between sequential actions. The pseudo-code of adaptive sub-task skill learning is outlined in Algorithm 1 in Appendix A.1.

### Bi-directional Adversarial Learning for Skill Chaining

Executing pre-trained sub-task skills sequentially without considering inter-skill dependencies may lead to failure. To address this, we propose bi-directional adversarial learning to further refine and better integrate sequential sub-task skills. The pseudo-code of bi-directional adversarial learning is outlined in Algorithm 2 in Appendix A.2.

Bi-directional RegularizationIn contrast to previous uni-directional regularization schemes that only augment the initial state set \(_{i}\) or regularize the terminal state set \(_{i}\)[12; 15], we impose the _bi-directional constraints_ (\(_{1}\),\(_{2}\)) on inter-skill dependencies, facilitating smooth skill chaining, as shown in Fig 4. With the bi-directional constraint, we implement the bi-directional adversarial learning, centered on the joint training of a _bi-directional discriminator_, denoted by \(^{i}_{}\), which is adept at distinguishing between the terminal state set of the preceding policy and the initial state set of the subsequent policy. The bi-directional constraints \(_{1}\),\(_{2}\) are defined as Eq. 10:

\[&_{1}=_{s_{T}_{i+1}}[^{i}_{_{1}}(s_{T})-1]^{2}+ _{s_{T}_{i}}[^{i}_{_{1}}(s_{T})]^{2}\\ & _{2}=_{s_{T}_{i-1}}[^{i}_{_{2}}(s_{T}) -1]^{2}+_{s_{T}_{i}}[^{i}_{_{2}}(s_{T}) ]^{2}\] (6)

\(^{i}_{_{1}}\) and \(^{i}_{_{2}}\) are two separate networks, each used to minimize the adversarial learning process in two different directions, and the parameters of the two networks are averaged and combined into \(^{i}_{}\). In summary, \(^{i}_{}\) is trained for each policy to minimize the objective function3:\(_{i}()=_{1}+_{2}\). Guided by \(^{i}_{}\), the bi-directional adversarial learning not only steers the terminal state set of the current policy towards the initial state set of the subsequent policy, but also ensures alignment of the initial state set of the subsequent policy with the terminal state set of current policy. This dual alignment establishes a balanced mapping between the initial and terminal states of sequential skills to reinforce inter-skill dependencies, ensure consistency and stability in multi-stage tasks, and guarantee smooth transitions between sequential skills. Accordingly, the _bi-directional regularization_ can be added to the overall objective function of policy learning in the form of the following reward term: \(r^{}_{i}(s;)=_{s_{i}}^{i+1}(s)+ _{s_{i}}^{i-1}(s)\).

Overall Objective FunctionSo far, the objective function via dual regularization, i.e., AES regularization and bi-directional regularization, to pre-train, fine-tune and chain sub-task skills can be rewritten as a weighted sum of the individual reward terms:

\[r_{i}(s_{t},a_{t},s_{t+1};)=}r^{}_{i}(s_{t},a_{t},s_{t+1},g)+_{}r^{}_{i}(s_{t},a_{t}; )}_{}+}r^{}_{i} (s_{t+1};)}_{},\] (7)

Figure 4: Bi-directional regularization for sub-task skill chaining.

where \(_{}\) is the weighting factor of the bi-directional regularization. The objective function features AES regularization and bi-directional regularization to enhance intra- and inter-skill dependencies. It enables the agent to adaptively pre-train skills that can solve different sub-tasks well through environmental feedback and expert guidance, and further fine-tune them through the bi-directional discriminator to achieve dual alignment between sequential skills. At the same time, the fine-tuned sub-task skills help to collect terminal and initial states to refine the bi-directional discriminator. This iterative process ensures smooth long-horizon task skill chaining.

## 5 Experiments

### Experiment Setup

We conduct simulation experiments on six IKEA furniture assembly tasks and two kitchen organization tasks, and also perform long-horizon pick-and-place experiments on the real Sagittarius K1 robot. Please refer to the Appendix for more detailed simulation experiment setup (Appendix G), network architecture (Appendix H), training details (Appendix I), more quantitative (Appendix D) and qualitative results (Appendix E) of the simulation tasks, and the real-robot experiments (Appendix F).

**Furniture Assembly** We conduct experiments in six IKEA furniture assembly tasks in : _chair_agne_, _chair_bernhard_, _chair_ingolf_, _table_back_, _toy_table_, and _table_dockstra_.

1) _chair_agne_: Two stool legs need to be picked up and aligned with the cross notches on the stool back. 2) _chair_bernhard_: The two chair supports need to be taken and aligned with the slots at the bottom of the chair surface. 3) _chair_ingolf_: Two chair supports and front legs need to be attached to the chair seat, which must then be secured to the chair back while avoiding collision with each other. 4) _table_back_: The four table legs need to be picked up and aligned with the corners of the tabletop. 5) _toy_table_: The four table legs need to be picked up and aimed and inserted with the four notches on the table back. 6) _table_dockstra_: After supporting the two bases with table leg, the table top needs to be mounted while preventing collision. For each assembly task, we define the assembly of individual parts as sub-tasks. We collect 200 demonstrations per sub-task using a procedural assembly policy for imitation learning. Each demonstration consists of 150 steps.

Kitchen OrganizationWe use the Franka Kitchen tasks in D4RL  and collect 200 demonstrations per sub-task for imitation learning. Specifically, we refer to the kitchen task in  and further extend the task sequence: in the **Kitchen task**, the 7-DoF Franka Emika Panda arm needs to perform 4 sequential sub-tasks, namely _Turn on the microwave - Move the kettle - Turn on the stove - Turn on the light_. In the **Extended Kitchen task**, the robot needs to perform 5 sequential sub-tasks: _Turn on the microwave - Turn on the stove - Turn on the light - Slide the cabinet to the right - Open the cabinet_, in which the sub-tasks have a lower probability of switching and is more challenging.

**Baselines** We compare SCaR with the following two types of baselines:

Figure 5: Evaluation Performance of Sub-task Skill Learning. Best viewed zoomed.

Scratch Training: 1) **PPO** is a model-free RL algorithm  that utilizes environmental rewards to learn tasks from scratch. 2) **GAIL** is an adversarial imitation learning method to learn tasks from scratch, with a trained discriminator for distinguishing state-action distributions of experts and agents. 3) **Fixed-RL-IL** uses fixed-weight environmental rewards and GAIL rewards to train policies from scratch. 4) **SkiMo** is a model-based hierarchical RL approach that learns dynamic skill models for predicting outcomes in downstream tasks, which is used to test if modularly skill chaining method can surpass model-based scratch-training method on long-horizon tasks.

Skill Chaining: 1) **Policy Sequencing** focuses on sequentially expanding the initial sets in skill chaining. 2) **T-STAR** incorporates a discriminator to uni-directionally regularize the terminal states of sub-skills in a skill chaining. 3) **SCaR w/o Bi** reference to T-STAR during the fine-tuning phase, only uni-directional regularization of the terminal state set is performed to verify the validity of the proposed bi-directional regularization. 4) **SCaR w/o AES** fixes the scales of the two reward terms at 0.5 at all times to verify the effectiveness of the proposed AES regularization.

### Quantitative Results

Sub-task Skill Learning PerformanceFirst, we evaluate the proposed adaptive sub-task skill learning scheme in the sub-tasks of furniture assembly and kitchen organization. Specifically, we treat each sub-task as a separate task for policy learning and take the success rate of the trained policy tested in the reset sub-task as the criterion. All methods are trained in each sub-task with 5 random seeds, 150 million environment steps, and evaluated with the average success rate over 100 testing episodes. As shown in the Fig. 5, in _chair_ingolf_ and Extended Kitchen tasks, even with the increase of objects in the environment and the increase of unpredictable perturbations, our proposed adaptive skill learning learns good sub-task skills and consistently maintains a task success rate of more than 85% in all stages of the sub-task. In contrast, the PPO (only RL rewards), GAIL (only IL rewards), and Fixed-RL-IL (fixed RL and IL reward weights) baselines fail to maintain good sub-task success rates as the number of sub-task stages increases. This result well validates that our proposed adaptive weighted reward function based on AES regularization enhances _intra-skill dependencies_ for multi-stage sub-task learning and brings effectiveness and stability.

Long-horizon Execution PerformanceWe then demonstrate the performance of SCaR in performing 8 long-horizon tasks in IKEA furniture assembly and kitchen organization. Table 1 shows the mean and standard deviation for these 8 tasks across 200 testing episodes with 5 different seeds. The PPO and GAIL baselines show minimal success on tasks with 4 and 5 sub-tasks, indicating the difficulty of learning complex multi-stage tasks solely from reward signals or expert demonstrations. The fixed RL-IL baseline, although improved compared to PPO and GAIL, mostly completed only one sub-task, which highlights the limitations of using fixed RL and IL reward weights in long-horizon tasks. While SkiMo achieves better success rates than model-free methods by building dynamic skill models, its performance remains inconsistent on long-horizon tasks due to its scratch learning nature. The performance of these scratch baselines demonstrates the importance of effective staged sub-task learning for long-horizon tasks. The results in Table 1 further highlight the superiority of the SCaR framework. By reinforcing _intra- and inter-skill dependencies_, task success rates are considerably higher than previous skill chaining approaches such as Policy Sequencing and T-STAR, which primar

   &  &  \\   & _chair-bigg_ & _chair-bigg_ & _chair-bigg_ & _chair-bigg_ & _chair-bigg_ & _chair-bottom_ & **All** & **Kitchen** & **E-Kitchen** & **All** \\ 
**PPO Search RL** & 0.54.03 & 0.42.12 & 0.14.03 & 0.09.11 & 0.00.10 & 0.51.02 & 0.25.45 & 0.13.45 & 0.03.08 & 0.01.14 \\ GAIL (Scratch IL) & 0.31.03 & 0.23.04 & 0.00.00 & 0.00.10 & 0.00.10 & 0.00.10 & 0.121.00 & 0.001.00 & 0.00.00 & 0.00.10 \\
**Fixed-RL-IL** & 0.68.03 & 0.53.07 & 0.22.06 & 0.21.41 & 0.13.48 & 0.43.00 & 0.37.67 & 0.33.48 & 0.18.05 & 0.26.40 \\
**SkiMo** & 0.75 & 0.40.00 & 0.62.06 & 0.47.41 & 0.58 & 0.34.46 & 0.62 & 0.61.41 & 0.56.61 & 0.57.00 & 0.21.004 & 0.39.41 \\ 
**Policy Sequencing** & 0.89 & 0.88 & 0.40 & 0.77 & 0.41 & 0.63 & 0.45.18 & 0.61 & 0.41 & 0.70.43 & 0.53 & 0.41 & 0.36.008 & 0.44.409 \\
**T-STAR** & 0.92.02 & 0.90 & 0.40 & 0.89 & 0.90 & 0.71 & 0.51 & 0.77 & 0.70 & 0.50 & 0.68 & 0.13 & 0.48.00 & 0.88.11 \\
**SCaR w/o Bi** & 0.93 & 0.44 & 0.92 & -0.91 & -0.91 & 0.80 & 0.80 & 0.79 & 0.60 & 0.88 & 0.75 & 0.71.01 & 0.66.40 \\
**SCaR w/o AES** & 0.95.03 & **0.94.00** & 0.93.02 & 0.95.44 & 0.85.16 & 0.80.00 & 0.91.00 & 0.79.00 & 0.61 & 0.72 & 0.61.03 & 0.74.08 \\
**SCaR (Overs)** & **0.96.02** & **0.96.06** & **0.95.06** & **0.97.05** & **0.92.02** & **0.98.08** & **0.94.03** & **0.92.05** & **0.94.03** & **0.72.00** & **0.78.02** & **0.78.02** (18.66) \\  

Table 1: Long-horizon tasks execution performance (varies by sub-task completion progress): *tasks with 2 sub-tasks progress by 0.5 per sub-task, *tasks with 4 sub-tasks by 0.25, *tasks with 5 sub-tasks by 0.2, and table_dockstra with 3 sub-tasks by 0.3, where 0.9 indicates completion of all tasks. Best viewed zoomed.

ily address uni-directional inter-skill dependencies. Compared to T-STAR, SCaR increases average success rates by more than 12% on six furniture assembly tasks and 18% on two kitchen tasks.4.

### Robustness to Perturbations

Perturbation tests are conducted to evaluate the robustness of skill chaining for two furniture assembly tasks. As shown in the top figure of Table 2, for the _chair_bernhard_ task, the perturbation involves applying external joint torque to the robotic arm, moving the chair back before assembling the second support. For the _chair_ingolf_ task, the perturbation is applied by exerting external torque on the robotic arms, causing them to move slightly before mounting the assembled chair seat to the chair back. The results in Table 2 highlight the detrimental impact of environmental perturbations on the success rates of baseline methods during the execution of multiple sub-task skills. Methods like Policy Sequencing and T-STAR, which focus solely on inter-skill dependencies through uni-directional regularization, struggle to complete tasks after perturbations. In contrast, SCaR, demonstrates more robust performance even under unseen perturbations. These results further support the advantages of our proposed _dual regularization_ for stable skill chaining on long-horizon manipulation tasks.

### Ablations and Analysis

We perform ablation studies to explore the important factors that affect the performance of SCaR.

Modular AblationWe investigate how the adaptive sub-task skill learning and bi-directional adversarial learning impact skill chaining through SCaR w/o Bi and SCaR w/o AES. As shown in Table 1, without bi-directional regularization, SCaR w/o Bi experiences significant performance drops in tasks with more than two sub-tasks but still outperforms T-STAR. This is because SCaR w/o Bi maintains the adaptive scheduling of AES during sub-task skill learning, underscoring the importance of focusing on the _intra-skill dependencies_ between successive actions. Similarly, the absence of AES regularization reduces SCaR w/o AES's performance, though it still maintains stable outcomes. This underscores the importance of reinforcing _inter-skill dependencies_ on long-horizon tasks and reaffirms the contribution of bi-directional regularization. As shown in Table 2, SCaR w/o

  &  \\  \\  & & & & \\  &  \\  & **No Perturb** & **Perturb** & **No Perturb** & **Perturb** \\  & 0.82 \(\) 0.04 & 0.51 \(\) 0.04 & 0.77 \(\) 0.11 & 0.50 \(\) 0.04 \\  & 0.90 \(\) 0.04 & **0.60 \(\) 0.00** & 0.89 \(\) 0.04 & 0.59 \(\) 0.00 \\ SCaR w/o Bi & 0.92 \(\) 0.04 & 0.65 \(\) 0.11 & 0.91 \(\) 0.01 & 0.63 \(\) 1.00 \\ SCaR w/o AES & 0.94 \(\) 0.03 & 0.74 \(\) 0.03 & 0.93 \(\) 0.02 & 0.71 \(\) 1.00 \\ SCaR (**Ours**) & **0.96 \(\) 0.04** & **0.85 \(\) 0.01** & **0.95 \(\) 0.04** & **0.80 \(\) 0.03** \\  \\ 

Table 2: Comparison of the robustness of skill chaining in perturbed environments.

Figure 6: Ablation experiments.

Bi, though slightly more robust than T-STAR due to the presence of AES, still faces challenges in adapting to perturbations and maintaining stable skill chaining because of its uni-directional fine-tuning limitations. SCaR w/o AES manages to maintain a certain level of performance stability under perturbations, thanks to bi-directional regularization, which ensures the bi-directional alignment of initial and terminal states between skills. The results show that the pre-trained skills via AES exhibit enhanced _intra-skill dependencies_ within sub-tasks, and bi-directional regularization ensures stable long-horizon execution, even in the presence of perturbations, by reinforcing _inter-skill dependencies_.

Parametric AblationWe further investigate the impact of different scales of RL and IL reward terms, as well as the size of expert demonstration datasets. The effect of varying the base exponent \(\) on task success rates is tested across four tasks: _chair_agne_, _chair_ingolf_, _table_dockstra_, and _extend kitchen_. As depicted in Fig. 6(a), SCaR achieves the highest success rates in all four tasks when \(=0.5\), indicating a balance between RL and IL at the beginning of learning. When \(\) becomes smaller, emphasizing IL at the start, performance decreases more steeply. Conversely, as \(\) becomes larger, giving more weight to RL, performance also declines but at a slower rate. We also evaluate the impact of different sizes of expert datasets on three skill chaining methods: Policy Sequencing, T-STAR, and SCaR, specifically in the _chair_ingolf_ task. We vary the overall task expert data size from 80, 120, 200, 400, 600, to 800 demos. As shown in Fig. 6(b), the results indicate significant performance improvement when increasing the dataset size from 400 to 800 demos, while the improvement is less pronounced when going from 80 to 120 demos. This demonstrates the importance of the demo dataset size in the effectiveness of data-driven approaches like skill chaining.

## 6 Discussion

Limitation and future directionsThe primary limitation of our work is that the sub-task division for long-horizon tasks is predefined and does not incorporate visual or semantic processing of objects. Expanding our framework to handle longer-horizon visual manipulation tasks is a direction we aim to explore in future research. For example, integrating a more scalable architecture  and performing large-scale pre-training on extensive datasets [49; 50] are promising directions. Another avenue worth exploring is applying our framework to real-world robotic furniture assembly tasks, rather than only staged pick-and-place tasks. Constructing a deployment environment for real-world furniture assembly and ensuring the complete insertion of each furniture module presents significant challenges. We discuss additional limitations and potential solutions in further detail in Appendix J.

ConclusionIn this paper, we introduce SCaR, a novel skill chaining framework that ensures smooth and stable execution of long-horizon robotic manipulation tasks via dual regularization within and between sub-task skills. Extensive experiments demonstrate that the SCaR framework achieves better task success rates than the baseline methods in both simulated and real-robot manipulation tasks, while being robust against perturbations. We hope this work will inspire future research to further explore the potential of skill chaining for long-horizon robotic manipulation.

This work was supported in part by the Science and Technology Innovation 2030 New Generation Artificial Intelligence Major Project under Grant 2021ZD0113303; in part by the National Natural Science Foundation of China under Grant 62192783, Grant 62276128; in part by the Collaborative Innovation Center of Novel Software Technology and Industrialization; in part by the Fundamental Research Funds for the Central Universities under Grant 14380128; in part by the Postgraduate Research & Practice Innovation Program of Jiangsu Province under Grant KYCX24_0263.