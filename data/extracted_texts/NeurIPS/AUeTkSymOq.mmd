# (Step 2: The Chernoff Method)

Freya PAGE: First Optimal Time Complexity for Large-Scale Nonconvex Finite-Sum Optimization with Heterogeneous Asynchronous Computations

 Alexander Tyurin

KAUST, AIRI, Skoltech

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

Kaja Gruntkowska

KAUST 1

###### Abstract

In practical distributed systems, workers are typically not homogeneous, and due to differences in hardware configurations and network conditions, can have highly varying processing times. We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations. By being robust to "stragglers" and adaptively ignoring slow computations, Freya PAGE offers significantly improved _time complexity_ guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions. The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods. Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit. This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime, i.e., when \( n\), where \(n\) is # of workers, and \(m\) is # of data samples.

## 1 Introduction

In real-world distributed systems used for large-scale machine learning tasks, it is common to encounter device heterogeneity and variations in processing times among different computational units. These can stem from GPU computation delays, disparities in hardware configurations, network conditions, and other factors, resulting in different computational capabilities and speeds across devices (Chen et al., 2016; Tyurin and Richtarik, 2023). As a result, some clients may execute computations faster, while others experience delays or even fail to participate in the training altogether.

Due to the above reasons, we aim to address the challenges posed by device heterogeneity in the context of solving finite-sum nonconvex optimization problems of the form

\[_{x^{d}}\{f(x):=_{i=1}^{m}f_{i}(x) \},\] (1)

where \(f_{i}:^{d}\) can be viewed as the loss of a machine learning model \(x\) on the \(i^{}\) example in a training dataset with \(m\) samples. Our goal is to find an \(\)-stationary point, i.e., a (possibly random) point \(\) such that \([\| f()\|^{2}]\). We focus on the homogeneous distributed setup:

* there are \(n\)_workers/clients/devices_ able to work in parallel,* each worker has access to stochastic gradients \( f_{j}\), \(j[m]\),
* worker \(i\) calculates \( f_{j}()\) in less or equal to \(_{i}[0,]\) seconds for all \(i[n],j[m]\).

Without loss of generality, we assume that \(_{1}_{n}\). One can think of \(_{i}[0,]\) as an upper bound on the computation time rather than a fixed deterministic time. Looking ahead, iteration complexity can be established even if \(_{i}=\) for all \(i[n]\) (Theorem 4). We also provide results where the bounds \(\{_{i}^{k}\}\) are dynamic and change with every iteration \(k\) (Section 4.4). For simplicity of presentation, however, we assume that \(_{i}^{k}=_{i}\) for \(i[n],k 0\), unless explicitly stated otherwise.

### Assumptions

We adopt two weak assumptions, which are standard for the problem (1) (Fang et al., 2018).

**Assumption 1**.: _The function \(f\) is \(L_{-}\)-smooth and lower-bounded by \(f^{*}\)._

**Assumption 2**.: \(\,L_{+} 0\) _such that \(_{i=1}^{m}\| f_{i}(x)- f_{i}(y)\| ^{2} L_{+}^{2}\|x-y\|^{2}\, x,y^{d}\)._

We also consider Assumption 3. Note that this assumption does not restrict the class of considered functions \(\{f_{i}\}\). Indeed, if Assumption 2 holds with \(L_{+}\), then Assumption 3 holds with some \(L_{} L_{+}\). If one only wants to rely on Assumptions 1 and 2, it is sufficient to take \(L_{}=L_{+}\). However, Assumption 3 enables us to derive sharper rates, since \(L_{}\) can be small or even \(0\), even if \(L_{-}\) and \(L_{+}\) are large (Szlendak et al., 2021; Tyurin et al., 2023; Kovalev et al., 2022).

**Assumption 3** (Hessian variance (Szlendak et al., 2021)).: _There exists \(L_{} 0\) such that_

\[_{i=1}^{m}\| f_{i}(x)- f_{i}(y)-(  f(x)- f(y))\|^{2} L_{}^{2}\|x-y\|^{2}  x,y^{d}.\]

### Gradient oracle complexities

Iterative algorithms are traditionally evaluated based on their _gradient complexity_. Let us present a brief overview of existing theory. The classical result of Gradient Descent (GD) says that in the smooth nonconvex regime, the number of oracle calls needed to solve problem (1) is \((m^{-1})\) because GD converges in \((^{-1})\) iterations, and calculates the full gradient \( f=}{{m}}_{i=1}^{m} f_{i}\) in each iteration. This was improved to \((m+m^{}{{3}}}^{-1})\) by several _variance-reduced_ methods, including SVRG and SCSG (Allen-Zhu and Hazan, 2016; Reddi et al., 2016; Lei et al., 2017; Horvath and Richtarik, 2019). Since then, various other algorithms, such as SNVRG, SARAH, SPIDER, SpiderBoost, PAGE and their variants, have been developed (Fang et al., 2018; Wang et al., 2019; Nguyen et al., 2017; Li et al., 2021; Zhou et al., 2020; Horvath et al., 2022). These methods achieve a gradient complexity of \((m+^{-1})\), matching the lower bounds (Fang et al., 2018; Li et al., 2021).

That said, in practical scenarios, what often truly matters is the _time complexity_ rather than the _gradient complexity_(Tyurin and Richtarik, 2023). Although the latter metric serves as a natural benchmark for sequential methods, it seems ill-suited in the context of parallel methods.

### Some previous time complexities

Let us consider some examples to provide intuition about time complexities for problem (1).

GD **with \(1\) worker** (Hero GD).** In principle, each worker can solve the problem on their own. Hence, one approach would be to select the fastest client (assuming it is known) and delegate the task to them exclusively. A well-known result says that for \(L_{-}\)-smooth objective function \(f\) (Assumption 1), GD converges in \(^{0}L_{-}^{-1}\) iterations, where \(^{0}:=f(x^{0})-f^{*}\), and \(x^{0}\) is the starting point. Since at each iteration the method computes \(m\) gradients \( f_{i}()\), \(i[m]\), the time required to find an \(\)-stationary point is \(^{0}L_{-}^{-1} m_{1}\) seconds.

GD **with \(n\) workers and equal data allocation** (Soviet GD).** The above strategy leaves the remaining \(n-1\) workers idle, and thus potentially useful computing resources are wasted. A common approach is to instead divide the data into \(n\) equal parts and assign one such part to each worker, so that each has to compute \(}{{n}}\) gradients (assuming for simplicity that \(m\) is divisible by \(n\)). Since at each iteration the strategy needs to wait for the slowest worker, the total time is \(^{0}L_{-}^{-1}}}{{n}}\). Dependingon the relationship between \(_{1}\) and \(_{n}/n\), this could be more efficient or less efficient compared to Hero GD. This shows that the presence of stragglers can eliminate the potential speedup expected from parallelizing the training (Dutta et al., 2018).

SPIDER/PAGE with \(1\) worker or \(n\) workers and equal data allocation (Hero PAGE and Soviet PAGE).As mentioned in Section 1.2, SPIDER/PAGE can have better _gradient complexity_ guarantees than GD. Using the result of Li et al. (2021), the equal data allocation strategy with \(n\) workers leads to the time complexity of

\[T_{}:=(_{n}\{,1\}+ _{n}\{L_{-},L_{+}\}}{}\{}{n},1\})\] (2)

seconds. We refer to this method as Soviet PAGE. In practical regimes, when \(\) is small and \(L_{-} L_{}\), this complexity can be \(\) better than that of GD. Running PAGE on the fastest worker (which we will call Hero PAGE), we instead get the time complexity \(T_{}:=(_{1}m+{_{1}}^{^{0}}/ ).\)

Given these examples, the following question remains unanswered: what is the best possible time complexity in our setting? This paper aims to answer this question.

## 2 Contributions

We consider the finite-sum optimization problem (1) under weak assumptions and develop a new method, Freya PAGE. The method works with arbitrarily heterogeneous and asynchronous computations on the clients without making _any_ assumptions about the bounds on the processing times \(_{i}\). We show that the _time complexity_ of Freya PAGE is provably better than that of all previously proposed synchronous/asynchronous methods (Table 1). Moreover, we prove a lower bound that guarantees optimality of Freya PAGE in the large-scale regime (\( n\)). The algorithm leverages new computation strategies, ComputeGradient (Alg. 2) and ComputeBatchDifference (Alg. 3), which are generic

  
**Method** & **Werst-Case Time Complexity** & **Comment** \\  Hero OD (Souiet GD) & \(_{1}m}{}\) & \((_{n}}{})\) & Suboptimal \\  Hero PAGE (Souiet PAGE) & \(_{1}m+_{1}}{}\) & \((_{n}+_{n}}{}}{n})\) & Suboptimal \\   SYNTHESIS  & \(-\) & Limitations: \\  & \(-\) &  bounded gradient assumption, \\ calculates the full gradients1  \\ suboptimal.2  \\  Ramala SGD  & \(}{}_{j[n]}(( })^{-1}(}{}+j))\) &  \(^{2}\)-bounded variance assumption, \\ suboptimal when \(\) is small. \\  \\    Freya PAGE \\ (Theorems 7 and 8) \\  &  \(_{j[n]}((_{i=1}^{j}})^{-1} (m+j))\) \\ \(+}{}_{j[n]}((_{i=1 }^{j}})^{-1}(m+j))\) \\  & 
 Optimal in the large-scale regime, \\ i.e. \( n\) (see Section 5) \\  \\   Lower bound & \(_{j[n]}((_{i=1}^{j}})^{-1 }(m+j))\) & \(-\) \\ (Theorems 10) & \(+}{}_{j[n]}(( _{i=1}^{j}})^{-1}(m+j))\) & \(-\) \\   

Table 1: Comparison of the _worst-case time complexity_ guarantees of methods that work with asynchronous computations in the setup from Section 1 (up to smoothness constants). We assume that \(_{i}[0,]\) is the bound on the times required to calculate the stochastic gradient \( f_{j}\) by worker \(i\), \(_{1}_{n}\), and \(m n n\). Abbr: \(^{0}:=f(x^{0})-f^{*}\), \(m=\#\) of data samples, \(n=\#\) of workers, \(=\) error tolerance.

and can be used in any other asynchronous method. These strategies enable the development of our new SGD method (Freqx SGD); see Sections 6 and H. Experiments from Section A on synthetic optimization problems and practical logistic regression tasks support our theoretical results.

## 3 The Design of the New Algorithm

It is clear that to address the challenges arising in the setup under consideration and achieve optimality, a distributed algorithm has to adapt to and effectively utilize the heterogeneous nature of the underlying computational infrastructure. With this in mind, we now present a new algorithm, Freqx PAGE, that can efficiently coordinate and synchronize computations across the \(n\) devices, accommodating arbitrarily varying processing speeds, while mitigating the impact of slow devices or processing delays on the overall performance of the system.

Freqx PAGE is formalized in Algorithm 1. The update rule is just the regular PAGE update: at each iteration, with some (typically small) probability \(p\), the algorithm computes the full gradient \( f(x^{k+1})\), and otherwise, it samples a minibatch \(^{k}\) of size \(S\) and reuses the gradient estimator \(g^{k}\) from the previous iteration, updated by the cheaper-to-compute adjustment \(_{i S^{k}}( f_{i}(x^{k+1})- f_{i}(x^{k}))\).

```
1:Parameters: starting point \(x^{0}^{d}\), learning rate \(>0\), minibatch size \(S\), probability \(p(0,1]\), initialization \(g^{0}= f(x^{0})\) using ComputeGradient\((x^{0})\) (Alg. 2)
2:for\(k=0,1,,K-1\)do
3:\(x^{k+1}=x^{k}- g^{k}\)
4: Sample \(c^{k}(p)\)
5:if\(c^{k}=1\)then (with probability \(p\))
6:\( f(x^{k+1})=(x^{k+1})\) (Alg. 2)
7:\(g^{k+1}= f(x^{k+1})\) (with probability \(1-p\))
8:else (Alg. 3)
9:\(_{i S^{k}}( f_{i}(x^{k+1})- f_{i}(x^ {k}))=(S,x^{k+1},x^{k})\) (Alg. 3)
10:\(g^{k+1}=g^{k}+_{i^{k}}( f_{i}(x ^{k+1})- f_{i}(x^{k}))\)
11:endif
12:endfor (note): \(^{k}\) is a set of i.i.d. indices that are sampled from \([m]\), uniformly with replacement, \(|^{k}|=S\) ```

**Algorithm 1**Freqx PAGE

Within Algorithm 1, at each iteration we call one of two subroutines: ComputeGradient (Alg. 2, performing the low-probability step), and ComputeBatchDifference (Alg. 3, performing the high-probability step). Let us focus on ComputeGradient, designed to collect the full gradient: it takes a point \(x\) as input and returns \( f(x)=_{i=1}^{m} f_{i}(x)\). There exist many strategies for implementing this calculation, some of which were outlined in Section 1.3. The most naive one is to assign the task of calculating the whole gradient \( f\) to a single worker \(i\), resulting in a worst-case running time of \(m_{i}\) seconds for ComputeGradient. Another possible strategy is to distribute the functions \(\{f_{i}\}\) evenly among the workers; in this case, calculating \( f\) takes \(_{n}\{}{{n}},1\}\) seconds in the worst case.

Clearly, we could do better if we _have \(\{_{i}\}\) in advance_. Indeed, let us allocate to each worker \(j\) a number of functions \(\{f_{i}\}\) inversely proportional to \(_{j}\). This strategy is reasonable - the faster the worker, the more gradients it can compute. We can show that such a strategy finds \( f\) in

\[(_{j[n]}((_{i=1}^{j}} )^{-1}(m+j)))\] (3)

seconds in the worst case (see the proof of Theorem 2). This complexity is better than \(m_{1}\) and \(_{n}\{}{{n}},1\}\) (Theorem 31). However, this approach comes with two major limitations: i) it requires knowledge of the upper bounds \(\{_{i}\}\), ii) even if we have access to \(\{_{i}\}\), the computation environment can be adversarial: theoretically and practically, it is possible that at the beginning the first worker is the fastest and the last worker is the slowest, but after some time, their performancesswap. Consequently, the first worker might end up being assigned the largest batch, despite now having the lowest performance. Thus, this strategy is not robust to time-varying speeds.

New gradient computation strategy.The key innovation of this work lies in the introduction of new computation strategies: Algorithms 2 and 3. We start by examining Algorithm 2. It first broadcasts the input \(x^{d}\) to all workers. Then, for each worker, it samples \(j\)_uniformly_ from \([m]\) and asks it to calculate \( f_{j}(x)\) (with a non-zero probability, two workers can be assigned the same computation). Then, the algorithm enters the loop and waits for any worker to finish their calculations. Once this happens, it asks this worker to compute a stochastic gradient with a new index sampled _uniformly_ from the set \([m]\) of indices that have not yet been processed (again, it is possible to resample an index previously assigned to another worker). This continues until all indices \(i[m]\) have been processed and the full gradient \(_{i=1}^{m} f_{i}\) has been collected. Unlike the previous strategies, our Algorithm 2 does not use \(\{_{i}\}\), thus being _robust and adaptive to the changing compute times_. Furthermore, we can prove that its time complexity (almost) equals (3):

```
1:Input: point \(x^{d}\)
2:Init \(g=0^{d}\), set \(=\)
3:Broadcast \(x\) to all workers
4:For each worker \(i[n]\), sample \(j\) from \([m]\) uniformly and ask it to calculate \( f_{j}(x)\)
5:while\([m]\)do
6: Wait for \( f_{p}(x)\) from a worker
7:if\(p[m]\)then
8:\(g g+ f_{p}(x)\)
9: Update \(\{p\}\)
10:endif
11: Sample \(j\) from \([m]\) uniformly and ask
12:this worker to calculate \( f_{j}(x)\)
13:endwhile
14:Return \(g=_{i=1}^{m} f_{i}(x)\) ```

**Algorithm 2** ComputeGradient(\(x\), \(y\))

**Theorem 1**.: _The expected time needed by Algorithm 2 to calculate \(g=_{i=1}^{m} f_{i}\) is at most_

\[12_{j[n]}((_{i=1}^{j}})^{-1} (m+\{m,n\}(\{m,n\})+j))\] (4)

_seconds._

The result (4) (the proof of which can be found in Section C) is slightly worse than (3) due to the extra \(\{m,n\}(\{m,n\})\) term. This term arises because a worker may be assigned a gradient \( f_{j}(x)\) that was previously assigned to another worker (in Line 11 of Algorithm 2). Hence, with a _small_ (but non-zero) probability, two workers can perform the same calculations. However, typically the number of samples \(m\) is much larger than the number of workers \(n\). If we assume that \(m n n\), which is satisfied in many practical scenarios, then the time complexity (4) equals

\[(_{j[n]}((_{i=1}^{j}} )^{-1}(m+j)))\]

and the term \(\{m,n\}(\{m,n\})\) never dominates. Since this complexity is not worse than (3), our strategy behaves as if it knew \(\{_{i}\}\) in advance! To simplify formulas and avoid the logarithmic term, we use the following assumption throughout the main part of this paper:

**Assumption 4**.: \(m n n,\) _where \(m\) is the # of data samples and \(n\) is the # of workers._

We now proceed to discuss ComputeBatchDifference (Algorithm 3), designed to compute a minibatch of stochastic gradient differences. Both Algorithms 2 and 3 calculate sums. However the latter only waits until there are \(S\) samples in the sum, where some indices in the batch may not be unique. On the other hand, Algorithm 2 must ensure the collection of a full batch of \(m\)_unique_ stochastic gradients. As a result, Algorithm 3 offers better complexity results and, unlike ComputeGradient, does not suffer from suboptimal guarantees and logarithmic terms, as demonstrated in the theorem below.

**Theorem 2**.: _The time needed by Algorithm 3 to calculate \(g\) is at most_

\[4_{j[n]}((_{i=1}^{j}})^{-1}(S+j))\] (5)

_seconds._

Algorithm 1 uses _uniform sampling with replacement_, implemented in Algorithm 3. However, one can modify the two algorithms slightly to support virtually _any unbiased sampling_ (see Section E).

Note on asynchronicity.It is clear that to eliminate the need of waiting for very slow machines, some level of asynchronicity has to be injected into an algorithm for it to be efficient. Asynchronous SGD(Recht et al., 2011; Nguyen et al., 2018; Koloskova et al., 2022; Mishchenko et al., 2022) takes this concept to the extreme by never synchronizing and continually overwriting the updates. Consequently, the algorithm's time complexity is suboptimal. Conversely, imposing limitations on asynchronicity leads to optimal methods, both in our context (in the large-scale regime) and in the scenario examined by Tyurin and Richtarik (2023). Freya PAGE seamlessly combines synchrony and asynchrony, getting the best out of the two worlds.

## 4 Time Complexities and Convergence Rates

Formulas (3) and (4) will be used frequently throughout the paper. To lighten up the heavy notation, let us define the following mapping.

**Definition 3** (Equilibrium time).: A mapping \(t^{*}\::\:_{ 0}_{ 0}^{n}_{ 0}\) defined by

\[t^{*}(S,[_{i}]_{i=1}^{n}):=_{j[n]}((_{i=1}^{j} _{i}})^{-1}(S+j))[0,]\] (6)

is called the _equilibrium time_. We let \(t^{*}(S) t^{*}(S,[_{i}]_{i=1}^{n})\) when considering \(\{_{i}\}\) from Section 1.

Returning to the algorithm, we guarantee the following iteration complexity.

**Theorem 4** (Iteration complexity).: _Let Assumptions 1, 2 and 3 hold. Consider any minibatch size \(S:=\{1,2,\}\), any probability \(p(0,1]\), and let the stepsize be \(=(L_{-}+L_{}})^{-1}\). Then, after_

\[K K_{}:=}{}(L_{-}+L_{} })\] (7)

_iterations of Algorithm 1, we have \([\| f(^{K})\|^{2}]\), where \(^{K}\) is sampled uniformly at random from the iterates \(\{x^{0},,x^{K-1}\}\)._

Theorem 4 states that the iteration complexity is the same as in the optimal PAGE method (Li et al., 2021). Note that we can guarantee convergence even if the upper bounds \(\{_{i}\}\) are unknown or infinite (as long as there exists some worker that can complete computations within a finite time).

We now derive time complexity guarantees. With probability \(p\), the workers need to supply to the algorithm \(m\) stochastic gradients at each of the \(m\) data samples, which by Theorem 1 can be done in at most \(t^{*}(m)\) seconds (up to a log factor). Otherwise, they compute \(S\) differences of stochastic gradients, which by Theorem 2 takes at most \(t^{*}(S)\) seconds (up to a constant factor). The resulting time complexity is given in the theorem below.

**Theorem 5** (Time complexity with free parameters \(p\) and \(S\)).: _Consider the assumptions and the parameters from Theorem 4, plus Assumption 4. The expected time complexity of Algorithm 1 is at most_

\[& T(p,S,[_{i}]_{i=1}^{n}):=12 t^{*}(m,[_{i }]_{i=1}^{n})\\ &+}{}(L_{-}+L_{} })\{p t^{*}(m,[_{i}]_{i=1}^{n} )+(1-p) t^{*}(S,[_{i}]_{i=1}^{n})\}.\] (8)The first term comes from the preprocessing step, where the full gradient is calculated to obtain \(g^{0}= f(x^{0})\). Here, we use Assumption 4 that \(m n n\). The result (8) is valid even without this assumption, but at the cost of extra logarithmic factors.

### Optimal parameters \(S\) and \(p\)

The time complexity (8) depends on two free parameters, \(S\) and \(p(0,1]\). The result below (following from Theorems 13 and 14) determines their optimal choice.

**Theorem 6** (Main result).: _Consider the assumptions and parameters from Theorem 4, plus Assumption 4. Up to a constant factor, the time complexity (8) is at least \(T([_{i}]_{i=1}^{n}):=t^{*}(m,[_{i}]_{i=1}^{n})\)_

\[+}{}L_{-}t^{*}(m,[_{i}]_{i=1}^{n}),_{S}t^{*}(S,[_{i}]_{i=1}^{n})+L_ {}(m,[_{i}]_{i=1}^{n})t^{*}(S,[_{i}]_{i=1}^{n})}} {}]}_{F(S):=}},\] (9)

_and this lower bound is achieved with \(S^{*}=_{S}F(S)\) and \(p^{*}=p^{*}(S^{*})\), where_

\[p^{*}(S)=1,&L_{-}t^{*}(m,[_{i}]_{i=1}^{n}) L _{-}t^{*}(S,[_{i}]_{i=1}^{n})+L_{}(m,[_{i}]_{i=1}^ {n})t^{*}(S,[_{i}]_{i=1}^{n})}}{},\\ (S,[_{i}]_{i=1}^{n})}{t^{*}(m,[_{i}]_{i=1}^{n})},&.\]

Result (9) is the best possible time complexity that can be achieved with the Freya PAGE method. Unfortunately, the final time complexity has non-trivial structure, and the optimal parameters depend on \(\{_{i}\}\) in the general case. If we have access to all parameters and times \(\{_{i}\},\) then (9), \(S^{*},\) and \(p^{*}\) can be computed efficiently. Indeed, the main problem is to find \(_{S}F(S),\) which can be solved, for instance, by using the bisection method, because \(L_{-}t^{*}(S,[_{i}]_{i=1}^{n})\) is non-decreasing and \(L_{}(m,[_{i}]_{i=1}^{n})t^{*}(S,[_{i}]_{i=1}^{n})}/ \) is non-increasing in \(S.\)

### Optimal parameters \(S\) and \(p\) in the large-scale regime

Surprisingly, we can significantly simplify the choice of the optimal parameters \(S\) and \(p\) in the large-scale regime, when \( n\). This is a weak assumption, since typically the number of data samples is much larger than the number of workers.

**Theorem 7** (Main result in the large-scale regime).: _Consider the assumptions and parameters from Theorem 4, plus Assumption 4. Up to a constant factor and smoothness constants, if \( n,\) then the optimal choice of parameters in (8) is \(S^{*}=\) and \(p^{*}=}{{}}\). For this choice, the expected time complexity of Algorithm 1 is at most_

\[T(}{{}},,[_{i}]_{i=1}^{n})=12t^{*}(m,[_{ i}]_{i=1}^{n})+\{L_{-},L_{}\}}{}t^{*}( ,[_{i}]_{i=1}^{n})\] (10)

_seconds. The iteration complexity with \(S=\) and \(p=}{{}}\) is \(K_{}\{L_{-},L_{}\}}}{{}}\)._

We cannot guarantee that \(S=\) and \(p=}{{}}\) is the _optimal_ pair when \(<n\), but it is a valid choice for all \(m 1\). Note that (10) is true if \(m n n\), and it is true up to a log factor if \(m<n n\). In light of Theorem 15, we can further refine Theorem 7 if the ratio \(}}{{L}}\) is known:

**Theorem 8** (Main result in the large-scale regime using the ratio \(}}{{L}}\)).: _Consider the assumptions and parameters from Theorem 4, plus Assumption 4. The expected time complexity of Algorithm 1 is at most \((t^{*}(m,[_{i}]_{i=1}^{n})+L_{-}}}{{} } t^{*}(\{\{}}{{L_{-}}},1 \},m\},[_{i}]_{i=1}^{n}))\) seconds, where \(S=\{\{}}{{L_{-}}},1 \},m\}\) and \(p=}{{m}}\)._

For brevity reasons, we will continue working with the result from Theorem 7 in the main part of this paper. Note that the optimal parameters do not depend on \(\{_{i}\}\), and can be easily calculated since the number of functions \(m\) is known in advance. Hence, our method is _fully adaptive to changing and heterogeneous compute times of the workers_.

Even if the bounds are unknown and \(_{i}=\) for all \(i[n],\) our method converges after \(4^{0}\{L_{-},L_{}\}/\) iterations, and calculates the optimal number of stochastic gradients equal to \((m+^{0}\{L_{-},L_{}\}/)\).

### Discussion of the time complexity

Let us use Definition 3 and unpack the second term in the complexity (10), namely,

\[\{L_{-},L_{+}\}}{}_{j[n]}((_{ i=1}^{j}})^{-1}(+j)).\]

A somewhat similar expression involving \(_{j[n]}\) and harmonic means was obtained by Tyurin and Richtarik (2023); Tyurin et al. (2024) for minimizing expectation under the bounded variance assumption. The term \(^{0}\{L_{-},L_{}\}/\) is standard in optimization (Nesterov, 2018; Lan, 2020) and describes the difficulty of the problem (1). The term \(_{j[n]}((_{i=1}^{j}}{{_{i}}})^{-1}(+j))\) represents the average time of one iteration and has some nice properties. For instance, if the last worker is slow and \(_{n},\) then \(_{j[n]}()=_{j[n-1]}(),\) so the complexity effectively ignores it. Moreover, if \(j^{*}\) is an index that minimizes \(_{j[n]}(),\) then \(_{j[n]}((_{i=1}^{j}}{{_{i}}})^{-1}(+j))= ((_{i=1}^{j^{*}}}{{_{i}}})^{-1}(+j^{*})).\) The last formula, again, does not depend on the slowest workers \(\{j^{*}+1,,n\}\), which are automatically excluded from the time complexity expression. The same reasoning applies to the term \(t^{*}(m,[_{i}]_{i=1}^{n}).\) Let us now consider some extreme examples which are meant to shed some light on our time complexity result (10):

**Example 1**.: _[Equally Fast Workers] Suppose that the upper bounds on the processing times are equal, i.e., \(_{j}=\) for all \(j[n]\). Then_

\[T(}{{}},,[_{i}]_{i=1}^{n})=( \{,1\}+\{L_{-},L_{}\}} {}\{}{n},1\}).\]

The complexity in Example 1 matches that in (2), which makes sense since Soviet PAGE is a reasonable method when \(\{_{i}\}\) are equal. Note that the reduction happens without prior knowledge of \(\{_{i}\}\).

**Example 2**.: _[Infinitely Fast Worker] If \(_{1}=0\), then \(T(}{{}},,[_{i}]_{i=1}^{n})=0.\)_

**Example 3**.: _[Infinitely Slow Workers] If \(_{j}=\, j[n]\), then \(T(}{{}},,[_{i}]_{i=1}^{n})=.\)_

**Example 4**.: _[Extremely Slow Workers] Suppose that the times \(_{j}<\) are fixed \( j j_{B}\) and \(_{j} B\  j>j_{B}\) for some \(B\) large enough. Then \(T(}{{}},,[_{i}]_{i=1}^{n})=T(} {{}},,[_{i}]_{i=1}^{j_{B}}).\)_

Example 4 says that the workers whose processing time is too large are _ignored_, which supports the discussion preceding the examples.

### Dynamic bounds

It turns out that the guarantees from Theorem 7 can be generalized to the situation where the compute times \(\{_{i}\}\) are allowed to dynamically change throughout the iterations. Consider the \(k^{}\) iteration of Algorithm 1 and assume that worker \(i\) calculates one \( f_{j}\) in at most \(_{i}^{k}[0,]\) seconds \( i[n],j[m].\) Clearly, \(_{k-1}_{i}^{k}_{i}\  i[n]\) (where \(\{_{i}^{-1}\}\) are upper bounds from the preprocessing step), but \(_{i}^{k}\) can be arbitrarily smaller than \(_{i}\) (possibly \(_{i}^{k}=0\) and \(_{i}=\)).

**Theorem 9**.: _Consider the assumptions and parameters from Theorem 4, plus Assumption 4. Up to a constant factor, the expected time complexity of Algorithm 1 with \(S=\) and \(p=}{{}}\) is at most_

\[t^{*}(m,[_{_{-1,i}}^{-1}]_{i=1}^{n})+_{k=0}^{ 4^{0}\{L_{-},L_{ }\}/}t^{*}(,[_{_{k,i}}^{k}]_{i=1}^{n}),\] (11)

_where \(_{k,}\) is a permutation such that \(_{_{k,1}}^{k}_{_{k,n}}^{k}\) for all \(k-1.\) (This theorem follows from Theorem 22 with the chosen parameters)._

Hence, our algorithm is _adaptive to the dynamic compute times_\(\{_{i}^{k}\}.\) Let us consider an example with \(2\) workers. Assume that the first worker is stable: \(_{1}^{k}=\) for all \(k 0\), and the second worker is unstable: \(_{2}^{0}=\) is small in the first iteration, and \(_{1}^{2}\) in the second iteration. For explanation purposes, we ignore the preprocessing term \(t^{*}(m,),\) which is not a factor if \(\) is small. Then,

\[=t^{*}(,[_{_{0,1}}^{0},_{_{0,2}}^{0}])+t^ {*}(,[_{_{1,1}}^{1},_{_{1,2}}^{1}])+...=t^{*}(,[ ,])+t^{*}(,[])+...\]because \(t^{*}(,[_{_{1},_{1}}^{1},_{_{1}, _{2}}^{1}])=t^{*}(,[])\) when \(_{2}^{1}\). The time complexity in the second iteration depends on the first (stable) worker only, which is reasonable since \(_{2}^{1}\), and this happens _automatically_. At the same time, the first term \(t^{*}(,[,])\) depends on both workers, and this iteration will be faster because \(t^{*}(,[,]) t^{*}(,[])\).

### Comparison with previous strategies from Section 1.3

Our time complexities (9) and (10) are better than all known previous guarantees if \(m n n\). In particular, \(T(}{{}},,[_{i}]_{i=1}^{n}) T_{}\) (from (2)), because \(t^{*}(,[_{i}]_{i=1}^{n})}}{{n}}/{n}\) (Theorem 31). In fact, since \(_{_{n}}t^{*}(,[_{i}]_{i=1}^{n})=t^{*}( ,[_{i}]_{i=1}^{n-1})<\) and \(_{_{n}}=}}{{n}}_{}}{{n}}}=\), \(T_{}\) can be arbitrarily larger. We also improve on \(\) (see Remark 32).

### Comparison with existing asynchronous variance reduced methods

Several studies have explored asynchronous variance reduced algorithms. Essentially all of them are variants of the existing synchronous methods discussed in Section 1.2 and depend on the slowest worker in every iteration. There have been several attempts to combine variance reduction techniques with asynchronous computations. Perhaps the most relevant baseline is SYNTHESIS, an asynchronous variant of SPIDER (Fang et al., 2018) introduced by Liu et al. (2022). The obtained _gradient complexity_ matches that of SPIDER in terms of dependence on \(m\), but scales linearly with the bound on the time performance of the slowest worker, making it non-adaptive to slow computations. Moreover, in Line \(3\) of their Algorithm \(3\), the full gradient is calculated, assuming that it can be done for free. Lastly, the analysis assumes the gradients to be bounded.

## 5 Lower Bound

In previous sections, we showed that Freya PAGE converges in at most (9) or (10) seconds, providing better time complexity guarantees compared to all previous methods. The natural question is: how good are these time complexities, and can they be improved? In Section J, we formalize our setup and prove Theorems 29 and 30, which collectively yield the following lower bound.

**Theorem 10** (Less formal version of Theorems 29 and 30).: _Assume that \(0<_{1}_{n}\) and take any \(L_{+},^{0},>0\) and \(m\) such that \(<c_{1}L_{+}^{0}\). Then, for any (zero-respecting) algorithm, there exists a function \(f\) that satisfies \(f(0)-f^{*}^{0}\) and Assumption 2, such that it is impossible to find an \(\)-stationary point faster than in_

\[(t^{*}(m,[_{i}]_{i=1}^{n})+L_{+}}{}t^{*}(m,[_{i}]_{i=1}^{n}))\] (12)

_seconds using uniform sampling with replacement._

Comparing (10) and (12), we see that Freya PAGE is _optimal_ under Assumptions 1 and 2 in the large-scale regime (\( n\)). Indeed, without Assumption 3, we have \(\{L_{-},L_{}\}=\{L_{-},L_{+}\}=L_{+}\). Up to constant factor, (10) is less or equal to (12) since

\[t^{*}(,[_{i}]_{i=1}^{n}) =_{j[n]}((_{i=1}^{j}} )^{-1}(+j))=}_{j[n]}(( _{i=1}^{j}})^{-1}(m+j))\] \[ n}{}}_{j[n]} ((_{i=1}^{j}})^{-1}(m+j))=}t^{*}(m,[_{i}]_{i=1}^{n}).\]

This is _the first optimal method for the problem we consider_, and Theorem 10 gives _the first lower bound_.

## 6 Using the Developed Strategies in Other Methods

ComputeBatchDifference (Algorithm 3) is a generic subroutine and can be used in other methods. In Section H, we introduce Freya SGD, a simple algorithm with update rule \(x^{k+1}=x^{k}-(S,x^{k})\), where \(S\) is a batch size and ComputeBatch (Algorithm 4) is a minor modification of ComputeBatchDifference. Theorem 25 establishes that FreyaSGD converges in \((}{{}} t^{*}(}{{ }},[_{i}]_{i=1}^{n}))\) seconds (where we only keep the dependence on \(\) and \(\{_{i}\}\)). For small \(\), this complexity is worse than (10), but it can be better, for instance, in the interpolation regime (Schmidt and Roux, 2013; Ma et al., 2018). FreyaSGD resembles RennalaSGD(Tyurin and Richtarik, 2023), but unlike the latter, it is specialized to work with finite-sum problems (1) and does not require the \(^{2}-\)bounded variance assumption on stochastic gradients (which is not satisfied in our setting).