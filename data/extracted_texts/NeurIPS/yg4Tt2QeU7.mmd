# Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs

Zhao XU

AI Thrust

The Hong Kong University of Science and Technology (Guangzhou)

zxu674@connect.hkust-gz.edu.cn

Fan LIU

AI Thrust

The Hong Kong University of Science and Technology (Guangzhou)

fliu236@connect.hkust-gz.edu.cn

Hao LIU

AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)

Guangzhou HKUST Fok Ying Tung Research Institute

CSE, The Hong Kong University of Science and Technology

liuh@ust.hk

Corresponding author

###### Abstract

Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks. However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced **JailTrickBench** to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/JailTrickBench.

## 1 Introduction

Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks, yet they remain vulnerable to jailbreak attacks . These attacks manipulate

[MISSING_PAGE_FAIL:2]

level attacks [3; 4; 9; 10; 40; 41; 42; 43; 44; 45] leverage semantic manipulation and automated prompt-generation methods to craft adversarial prompts that exploit the model's vulnerabilities. For example, PAIR  constructs the attacker and target model through the iterative refinement of the original prompt by getting the feedback from the target model to iterative refine the adversarial prompt.

**Defenses Against Jailbreak Attacks.** To counteract jailbreak attacks, a variety of defense mechanisms [46; 15; 47; 48; 49; 27; 28; 50; 51] have been introduced recently. These defenses can be broadly categorized into system-level and model-level strategies. System-level defenses [15; 24; 47; 48; 52; 53; 54] implement external safety measures for both input and output. For example, SmoothLLM  generates multiple outputs from varied jailbreaking prompts and uses majority voting to select the safest response. For the model-level defense [26; 27; 28; 55; 56; 57; 58; 59; 60], the defense strategies involve directly modifying the LLM to reduce its susceptibility to malicious inputs. For example, safety training [61; 55; 62] involves incorporating safety-specific datasets during the tuning phases to strengthen the model against harmful instructions.

**Benchmark of Jailbreak Attack on LLMs.** Jailbreak attacks on LLMs exploit vulnerabilities to generate harmful, unethical, or unwanted content. Recent studies and benchmarks [18; 19; 20; 63; 64; 65; 66] have systematically evaluated the robustness of LLMs against these attacks, contributing significantly to the field. For instance, JailbreakBench  is a centralized benchmark designed to standardize jailbreak attack evaluations. It includes a repository of adversarial prompts, a standardized evaluation framework, and a performance leaderboard. Additionally, JailBreakV-28K  extends the evaluation to multimodal large language models [67; 68] (MLLMs) using a dataset of 28,000 test cases that combine text and image-based attacks, highlighting vulnerabilities in MLLMs . Although comprehensive benchmarks and diverse attack methodologies are crucial for understanding and mitigating jailbreak attack risks on LLMs, previous work [18; 19; 20; 63; 70] primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. In this work, we evaluate the impact of various attack settings on LLM performance and provide a baseline benchmark for jailbreak attacks, encouraging the adoption of a standardized evaluation framework.

## 3 Preliminaries

### Large Language Model

The LLMs [61; 71] are in a self-auto-regressive manner to predict sequences based on previously provided tokens. Assume \(_{1:n}\) is the tokens sequence, where each token \(x_{i}\) belongs to the vocabulary set \(\{1,,V\}\), with \(|V|\) being the vocabulary size. The goal of LLM is to predict the next token in the sequence, which can be represented as:

\[P_{_{}}(|_{1:n})=P_{_{}}(_{n+ i}|_{1:n+i-1}),\] (1)

where \(P_{_{}}(_{n+i}|_{1:n+i-1})\) is the probability of the next token \(_{n+i}\) given the preceding tokens \(_{1:n+i-1}\). The model \(_{}\) is parameterized by \(\), and \(\) represents the output sequence.

### Threat Model

**Objective of Jailbreak Attack.** The primary goal of a jailbreak attack is to craft adversarial prompts that trick the LLM into producing harmful output. The attack objective can be formulated as follows,

\[_{adv}(}_{1:n},})=- P_{_{ }}(}|}_{1:n}),\] (2)

where \(_{adv}(}_{1:n},})\) represents the adversarial loss. \(}_{1:n}\) is the adversarial prompt (_e.g._, How to make a bomb? ), and \(}\) is the target sequence (_e.g._, Sure, here are the steps to make the bomb!).

**Capability of Jailbreak Attack.** The capabilities of a jailbreak attack can be categorized based on the attacker's level of access to the model, including white-box [1; 7; 8; 72] and black-box attacks [3; 4; 9]. _White-Box Attack:_ In a white-box attack, the attacker has full access to the model's parameters and architecture. This allows the attacker to utilize gradient information to optimize adversarial prompts effectively. By leveraging this detailed knowledge, the attacker can iteratively adjust the promptsto maximize the adversarial loss, thereby increasing the likelihood of the model generating harmful output. _Black-Box Attack:_ In a black-box attack, the attacker cannot access the model's internal parameters or gradients. Instead, the attacker interacts with the model through queries and observes the responses. The attacker refines the adversarial prompts based on these responses to achieve the desired outcome. This process often involves crafting and modifying prompts iteratively to deceive the model into producing harmful output without direct knowledge of its internal model information.

### Jailbreak Defense

**Defending Against Jailbreak Attacks.** The goal of jailbreak defense is to prevent jailbreak attacks and ensure that the LLM generates safe responses (_e.g._, "Sorry, as a response AI, I must reject your request."). The jailbreak defense can be formulated as follows,

\[_{safe}(}_{1:n},})=- P_{_{ }}(A(}_{1:n}),C(})),\] (3)

where \(_{safe}(}_{1:n},})\) represents the safe loss function to improve the defense ability of the LLM itself. \(A(}_{1:n})\) and \(C(})\) are the input and output processing functions, respectively.

**System-level Defense.** At the system level, additional security measures are implemented to safeguard the model. This involves preprocessing inputs and filtering outputs to detect and block potential adversarial attempts [15; 47; 48; 25; 73; 74; 75]. For example, \(A(}_{1:n})\) is a random perturbation function that randomly inserts perturbations to mitigate malicious requests, and \(C(})\) is a filtering function that transforms malicious prompts into safe responses.

**Model-level Defense.** At the model level, the defense strategy is to enhance the robustness of the model itself to withstand adversarial attacks [26; 28; 76; 77; 78; 79]. This involves optimizing the model parameters \(\) to naturally improve its defensive capabilities.

## 4 Benchmarking of Jailbreak Attacks on LLMs

In this section, we first examine how often overlooked implementation details influence the performance of jailbreak attacks from the perspectives of the target model (model size, fine-tuning alignment, safety system prompt, and template) and the attack level (attacker ability, adversarial suffix length, attack budget, and attack intention). Subsequently, we evaluate the effectiveness of jailbreak attacks against various defense methods. An overview of benchmarking jailbreak attacks on LLMs is presented in Figure 1.

### Experiment Setup

**Datasets and Target Models.**_Datasets:_ To evaluate the effectiveness of jailbreak attacks, we use two widely recognized datasets: _AdvBench_ and _MaliciousInstruct_. _AdvBench_ contains 520 examples of harmful or toxic behaviors, including profanity, threatening behavior, misinformation, discrimination, and other illegal activities. _MaliciousInstruct_ includes ten different malicious intentions, covering a broader spectrum of harmful instructions. _Target Models:_ Following the in previous studies [1; 2; 3], we use Llama2-7B  and Vicuna-13B  as the base target models. To evaluate the impact of LLMs, we further evaluate more models including (Llama-2-13B and Llama-2-70B) and (Llama-3-8B Llama-3-70B). We provide more details of datasets and target models in Appendix A.1.

Figure 1: An overview of benchmarking jailbreak attacks on LLMs.

**Metrics.** Following the , we use the Attack Success Rate (ASR) to measure the vulnerability. The ASR is the percentage of instructions that are not rejected and are responded to appropriately. We calculate ASR using both prefix-based and GPT agent-based methods, denoted as ASR\({}_{}\) and ASR\({}_{}\), respectively. The prefix-based method  matches substrings against a set of rejection phrases, such as "Sorry, I cannot." If these phrases are absent, the attack is considered successful. The GPT agent-based method  uses a GPT-4 model to determine whether the LLM is jailbroken based on the input malicious instruction and the model's response. More details of these metrics can be found in Appendix A.2.

**Jailbreak Attack and Defense Methods.**_Attack Methods:_ To assess the vulnerability of LLMs, we employ the most widely used jailbreak attacks, including both token-level attacks (GCG , AutoDAN , AmpleGCG , and AdvPrompter ) and prompt-level attacks (PAIR , TAP , and GPTFuzz ). _Defense Methods:_ To enhance the defense capabilities of LLMs, we implement both system-level defense methods (Self-Reminder , RPO , and SmoothLLM ) and model-level defense methods (Adversarial Training , Unlearning , and Safety Training ). Details of the implementation settings for both jailbreak attacks and defenses are provided in Appendix A.3.

### Bag of Tricks: Key Factors of Implementation Details of Jailbreak Attack

#### 4.2.1 Target Model Level

**Model Size.** Different model sizes exhibit varying capabilities in handling complex tasks. Mazeika et al. provide a comprehensive study on various models defending against jailbreak attacks, revealing that some models have superior resilience. For example, Llama-7B offers better defense against jailbreaks than Llama-70B under the AutoDAN attack. In Figure 2, we evaluate the impact of model size on Llama, keeping other settings as default. As shown, the robustness of the models against jailbreak attacks appears to be independent of their size, as both smaller and larger models exhibit varying degrees of adversarial robustness under both token-level and prompt-level jailbreak attacks. The effect of model size is minimal.

**Fine-tuning Alignment.** Fine-tuning LLMs with a few adversarial examples can compromise their safety alignment . Given this, we further investigate the broader implications of fine-tuning, such as domain-specific fine-tuning and open-chat data fine-tuning, on the safety alignment of LLMs. We evaluate the model's robustness before and after fine-tuning under jailbreak attacks. As shown in Figure 3, fine-tuning significantly impacts the robustness of LLMs. For instance, the Vicuna-13B model, fine-tuned from LLaMA on user-shared conversations, exhibits a significant increase in ASR post fine-tuning, indicating reduced robustness. The impact of fine-tuning on the model's robustness is substantial.

**Safety System Prompt.** Xie et al  reveals that LLM safety can be enhanced by using system prompts with safety reminders  (_e.g._, "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content") that effectively sharpen the LLM's focus on secure responses. In Figure 4, we evaluate whether safety prompts affect jailbreak attack performance, and the results show that safety prompts significantly impact jailbreak performance.

**Template Type.** Cui et al  shows that switching between models and making small changes in prompt templates can lead to vastly different results. We further test whether using different templates affects jailbreak attack performance. As shown in Figure 5, we find that different templates significantly affect attack performance. The original form system significantly improves the robustness of LLMs. It is evident that the choice of template plays a crucial role in the model's vulnerability to adversarial attacks.

#### 4.2.2 Attacker Level

**Attacker Ability.** Chao et al.  propose that prompt-level jailbreaks utilize LLMs to iteratively refine adversarial prompts. However, different LLMs exhibit varying capabilities. For example, closed-source models (_e.g._, GPT-4, GPT-3.5 _etc._) often demonstrate strong abilities. GPTFuzzer uses GPT-3.5 to refine adversarial prompts, while PAIR uses Vicuna-13B as the attacker. In Figure 5(a),we employ GPT-4, GPT-3.5, Vicuna-13B, and Llama-2-7B to test how different attacker abilities affect jailbreak attack performance. we evaluate the performance of using Vicuna-13B, Llama-2-13B, GPT-3.5, and GPT-4 as attackers to iteratively refine adversarial prompts. The attacker's ability significantly impacts the attack performance, with stronger LLMs achieving better attack outcomes.

**Adversarial Suffix Length.** In the token-level jailbreak attack, GCG utilizes greedy updated optimization methods to optimize the adversarial suffix and append it to the original prompts. However, the adversarial suffix is initialized with the same length. In Figure 5(b), we vary the adversarial suffix length to verify its impact on making LLMs generate jailbroken responses. As the adversarial suffix length increases, the ASR significantly increases and then flattens out. This indicates that longer adversarial suffixes enhance the likelihood of generating jailbroken responses up to a certain point, beyond which the effect plateaus.

**Attack Budget.** When crafting the adversarial prompt in the iterative refinement process, token-level jailbreaks utilize hundreds of queries (_e.g._, GCG uses about 500 iterations to optimize the adversarial prompts), while prompt-level jailbreak attacks require only a few queries (_e.g._, PAIR uses about twenty queries). In Figure 7, we experiment with different attack budgets to verify whether this influences attack performance. As seen, for the token-level jailbreak, the ASR significantly increases with the attack budget. In contrast, for the prompt-level jailbreak attack, the effect of the attack budget is marginal.

**Attack Intention.** We collected 11 attack intentions, including Illegal Activity, Child Abuse Content, Hate/Harass/Violence, Malware, Physical Harm, Economic Harm, Fraud/Deception, Adult Content, Political Campaigning, Privacy Violation Activity, and Tailored Financial Advice from HEx-PHI . To test whether the attacker's intention leads to variations in outcomes, as demonstrated in Figure 8,

Figure 3: Effect of finetuning alignment on the robustness of LLMs. We assess the robustness of LLMs before and after fine-tuning by subjecting them to two distinct types of jailbreak attacks—AutoDAN and PAIR—across various configurations. Tulu, Vicuna, and WizardLM models represent the post-fine-tuning versions of Llama2 series. Additionally, we investigate the impact of fine-tuning alignment on Mistral-7B and Qwen1.5-14B, which are fine-tuned using the Tulu v2 SFT dataset, consisting of 326,154 samples. It was observed that fine-tuning significantly compromised the models’ safety alignment.

Figure 2: Effect of model size on jailbreak attack performance. We assess two types of jailbreak attacks: token-level (AutoDAN) and prompt-level (PAIR), across the Llama2, Llama3, and Qwen1.5 families with various model sizes. Our analysis indicates that attack robustness does not depend on model size.

[MISSING_PAGE_FAIL:7]

### Benchmark of Jailbreak Attack on Defense Methods

**Overview of Analysis.** Beyond above critical techniques, we further examine the effectiveness of jailbreak attacks on different defense methods. Table 3 and Table 4 present the overall experimental results based on the metric ASR\({}_{}\), while the results in Table 6 and Table 7 are based on ASR\({}_{}\) and are provided in Appendix A.2. The experiments assessed the effectiveness of various defense methods against jailbreak attacks using two datasets: AdvBench and MaliciousInstruct.

Figure 6: (a) Effect of attacker ability on the performance of jailbreak attacks. We utilize different models with various abilities (Vicuna-13B, Llama-2-13B, GPT-3.5, and GPT-4) as attackers to generate adversarial prompts. It can be observed that the stronger the LLM, the better its attack performance. (b) Effect of adversarial suffix length on the performance of jailbreak attacks. We vary the initialized adversarial suffix length, observing that as the adversarial suffix length increases, the ASR significantly increases and then flattens out. This indicates that longer adversarial suffixes enhance the likelihood of generating jailbroken responses up to a certain point, beyond which the effect plateaus.

Figure 7: Effect of attack budget on jailbreak attack performance. We vary the attacker’s queries to evaluate whether this would affect the jailbreak attack performance. As seen, for the token-level jailbreak, the ASR significantly increases with the attack budget. In contrast, for the prompt-level jailbreak attack, the effect of the attack budget is marginal.

The effectiveness of defense methods varied significantly across both datasets and models, with AppleCCG and AdvPrompter consistently showing strong defensive capabilities against jailbreak attacks. Conversely, the attack methods exhibited consistent effectiveness across both datasets and models, with AdvPrompter maintaining high attack success rates (ASRPrefix and ASRAgnt), indicating robust jailbreak capabilities against multiple defense strategies. Additionally, the susceptibility to jailbreak attacks differed between the models, with Vicuna-13B generally showing greater resilience compared to LLaMA-2-7B, as evidenced by the higher ASRPrefix and ASRAgnt values observed in LLaMA-2-7B, indicating its higher vulnerability to attacks.

## 5 Conclusion

In this work, we provide a comprehensive benchmarking of jailbreak attacks on LLMs, leading to several key insights and implications for the LLMs. Our empirical studies show that the performance of jailbreak attacks is significantly sensitive to key factors such as model size, fine-tuning, and the use of safety prompts. Additionally, the choice of template and the attacker's skill level also play critical roles in the success of these attacks. Thus, continuous benchmarking and standardized evaluation frameworks are essential for understanding and mitigating the risks of jailbreak attacks, thereby ensuring the reliability of LLMs.

Figure 8: Jailbreak attack performance w.r.t attack intention. We gathered data on eleven distinct attack intentions. We observed that variations in these intentions correspond to discernible disparities in the ASR. Our findings suggest that the specific nature of the attack intention exerts a substantial influence on the effectiveness of adversarial attacks.

Figure 9: Combination of tricks under metric ASRAgent and ASRPrefix.

## 6 Acknowledgments

This work was supported by the National Natural Science Foundation of China (Grant No.62102110, No.92370204), National Key R&D Program of China (Grant No.2023YFF0725004), Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality.