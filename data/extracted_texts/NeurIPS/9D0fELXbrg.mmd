# Scale-teaching: Robust Multi-scale Training for Time Series Classification with Noisy Labels

Zhen Liu

South China University of Technology

Guangzhou, China

cszhenliu@mail.scut.edu.cn

&Peitian Ma

South China University of Technology

Guangzhou, China

ma_scuter@163.com

&Dongliang Chen

South China University of Technology

Guangzhou, China

ytucd1@foxmail.com

&Wenbin Pei

Dalian University of Technology

Dalian, China

peiwenbin@dlut.edu.cn

&Qianli Ma

South China University of Technology

Guangzhou, China

qianlima@scut.edu.cn

Qianli Ma is the corresponding author.

###### Abstract

Deep Neural Networks (DNNs) have been criticized because they easily overfit noisy (incorrect) labels. To improve the robustness of DNNs, existing methods for image data regard samples with small training losses as correctly labeled data (small-loss criterion). Nevertheless, time series' discriminative patterns are easily distorted by external noises (i.e., frequency perturbations) during the recording process. This results in training losses of some time series samples that do not meet the small-loss criterion. Therefore, this paper proposes a deep learning paradigm called _Scale-teaching_ to cope with time series noisy labels. Specifically, we design a fine-to-coarse cross-scale fusion mechanism for learning discriminative patterns by utilizing time series at different scales to train multiple DNNs simultaneously. Meanwhile, each network is trained in a cross-teaching manner by using complementary information from different scales to select small-loss samples as clean labels. For unselected large-loss samples, we introduce multi-scale embedding graph learning via label propagation to correct their labels by using selected clean samples. Experiments on multiple benchmark time series datasets demonstrate the superiority of the proposed Scale-teaching paradigm over state-of-the-art methods in terms of effectiveness and robustness.

## 1 Introduction

Time series classification has recently received much attention in deep learning [1; 2]. Essentially, the success of Deep Neural Networks (DNNs) is driven by a large amount of well-labeled data. However, human errors  and sensor failures  produce noisy (incorrect) labels in time series datasets. For example, in electrocardiogram diagnosis , physicians with different experiences tend to make inconsistent category judgments. In recent studies [6; 7], DNNs have shown their powerful learning ability, which, however, makes it relatively easier to overfit noisy labels and inevitably degeneratethe robustness of models. Moreover, time series data has complex temporal dynamics that make it challenging to manually correct noisy labels .

To cope with noisy labels, existing studies on label-noise learning [9; 10] use the memory effect of DNNs to select samples with small losses for training. DNNs memorize the data with clean labels first, and then those with noisy labels in classification training (small-loss criterion) . It is worth noting that the small-loss criterion is not affected by the choice of training optimizations and network structures , and is widely utilized for label-noise learning in computer vision [13; 14]. However, the small loss criterion cannot always be applied to time series because the discriminative patterns of time series data are easily distorted by external noises [15; 16]. For example, in a smart grid, distortions may occur due to sampling frequency perturbations, imprecise sensors, or random differences in energy consumption . Such distortions can make it difficult for DNNs to learn the appropriate discriminant patterns of time series, resulting in large training losses for some clean labeled samples. In addition, the small-loss criterion only utilizes the data's label information and does not consider the inherent properties of time series features (i.e., multi-scale information).

Multi-scale properties are crucial in time series classification tasks. In recent years, multi-scale convolution , dynamic skip connections [18; 19] and adaptive convolution kernel size  have been utilized to learn discriminative patterns of time series. Furthermore, according to related studies [2; 20; 21], the selection of appropriate time scales for time series data can facilitate DNNs to learn class-characteristic patterns. With correct labels, the above studies indicate that the multi-scale properties of time series data can help DNNs learn appropriate discriminative patterns for mitigating the negative effects of time series recording noises. Nevertheless, it remains an open challenge as to how the multi-scale properties of time series can be used for label-noise learning.

To this end, we propose a deep learning paradigm, named Scale-teaching, for time-series classification with noisy labels. In particular, we design a fine-to-coarse cross-scale fusion mechanism for obtaining robust time series embeddings in the presence of noisy labels. We select four time series datasets from the UCR archive  to explain our motivation. As shown in Figure 1, in the single scale case (top row), the red and blue samples from the same category have large differences in certain local regions (the green rectangle in Figure 1). By downsampling the time series from fine to coarse, some local regions between the red and blue samples did become similar. Meanwhile, existing studies [12; 23] show that multiple DNNs with random initialization have classification divergence for noisy labeled samples, but are consistent for clean labeled samples. The above findings inspire us to utilize multiple DNNs to combine robust embeddings at different scales to deal with noisy labels. Nonetheless, the coarse scale discards many local regions in the fine scale (as in Figure 1 (c)), which may degenerate the classification performance. Hence, we propose the Scale-teaching paradigm, which can better preserve the local discriminative patterns of fine scale while dealing with distortions.

More specifically, the proposed Scale-teaching paradigm performs the cross-scale embedding fusion in the finer-to-coarser direction by utilizing time series at different scales to train multiple DNNs simultaneously. The cross-scale embedding fusion exploits complementary information from different scales to learn discriminative patterns. This enables the learned embeddings to be more robust to distortions and noisy labels. During training, clean labels are selected through cross-teaching on those networks with the learned embeddings. The small-loss samples in training are used as (clean) labeled data, and the unselected large-loss samples are used as (noisy) unlabeled data. Moreover,

Figure 1: Illustration of time series samples _from the same category_ at different time scales. Among all samples in the same category, red indicates the one with the largest variance, and blue indicates a few samples with the smallest variance.

multi-scale embedding graph learning is introduced to establish relationships between labeled and unlabeled samples for noisy label correction. Based on the multi-scale embedding graph, the label propagation theory  is employed to correct noisy labels. This drives the model to better fit time series category distribution. The contributions are summarized as follows:

* We propose a deep learning paradigm, called Scale-teaching, for time-series label-noise learning. In particular, a cross-scale fusion mechanism is designed to help the model select more reliable clean labels by exploiting complementary information from different scales.
* We further introduce multi-scale embedding graph learning for noisy label correction using the selected clean labels based on the label propagation theory. Unlike conventional image label-noise learning methods focused on sample loss levels, our approach uses well-learned multi-scale time series embeddings for noise label correction at sample feature levels.
* Extensive experiments on multiple benchmark time series datasets show that the proposed Scale-teaching paradigm achieves a state-of-the-art classification performance. In addition, multi-scale analyses and ablation studies indicate that the use of multi-scale information can effectively improve the robustness of Scale-teaching against noisy labels.

## 2 Related Work

Label-noise Learning.Existing label-noise learning studies focus mainly on image data . These studies can be broadly classified into three categories: (1) designing noise-robust objective functions [25; 26] or regularization strategies [27; 28]; (2) detecting and correcting noisy labels [13; 29; 30]; (3) transition-matrix-based [31; 32] and semi-supervised-based [14; 33] methods. In contrast to the methodologies in the first and third categories, approaches categorized under the second category have received considerable attention in recent years [7; 34]. Methods of the second category can be further divided into sample selection and label correction. The common methods of sample selection are the Co-teaching family [12; 13; 23] and FINE . Label correction [36; 37] attempts to correct noisy labels by either using prediction results of classifiers or pseudo-labeling techniques. Recently, SREA  utilizes pseudo-labels generated based on a clustering task to correct time-series noisy labels. Although the above methods can improve the robustness of DNNs, how the multi-scale properties of time series are exploited for label-noise learning has not been explored.

Multi-scale Time Series Modeling.In recent years, multi-scale properties have gradually gained attention in various time series downstream tasks [18; 38], such as time series classification, prediction, and anomaly detection . For example, Cui et al.  employ multiple convolutional network channels of different scales to learn temporal patterns that facilitate time series classification. Chen et al.  design a time-aware multi-scale RNN model for human action prediction. Wang et al.  introduce a multi-scale one-class RNN for time series anomaly detection. Also, recent studies [41; 42; 43; 44] indicate that multi-scale properties can effectively improve the performance of long-term time series prediction. Unlike prior work, we utilize multiple DNNs with identical architectures to separately capture discriminative temporal patterns across various scales. This enables us to acquire robust embeddings for handling noisy labels via a cross-scale fusion strategy.

Label Propagation.Label propagation (LP) is a graph-based inductive inference method [24; 45] that can propagate pseudo-labels to unlabeled graph nodes using labeled graph nodes. Since LP can utilize the feature information of data to obtain pseudo-labels of unlabeled samples, related works employ LP in few-shot learning  and semi-supervised learning [47; 48]. Generally speaking, DNNs have the powerful capability for feature extraction, and the learned embeddings tend to be similar within classes and different between classes. Each sample contains feature and label information. Intuitively, the embeddings of samples with noisy labels obtained by DNNs closely align with the true class distribution when the DNNs do not fit noisy labels in the early training stages. Naturally, we create a nearest-neighbor graph based on well-learned multi-scale time series embeddings at the feature level. Subsequently, we employ LP theory to correct the labels of unselected noisy samples using the labels of clean samples chosen by the DNNs. This approach leverages robust multi-scale embeddings to address the issue of noisy labels.

## 3 Proposed Approach

### Problem Definition

Given a noisy labeled time series dataset \(=\{(_{i},_{i})\}_{i=1}^{N}\), it contains \(N\) time series, where \(_{i} R^{L T}\), \(L\) denotes the number of variables, and \(T\) is the length of variable. \(}\{1,,C\}\) is the observed label of \(_{i}\) with \(\) probability of being a noisy label. Our goal is to enable the DNNs trained on the noisy labeled training dataset \(_{train}\) to correctly predict the ground-truth labels of the given time series in the test set. Specifically, the problem to be addressed in this paper consists of two steps. The first is to select clean labeled time series from \(_{train}\), and the second is to perform noisy label correction for time series in \(_{train}\) that have not been selected as clean labels.

### Model Architecture

The overall architecture of Scale-teaching is shown in Figure 2. While this figure illustrates Scale-teaching with input time series at three scales, it can be extended to models with more scales, exceeding three. We utilize a consistent structural encoder to learn embeddings for each input scale sequence. Each encoder undergoes training at two levels: embedding learning for clean sample selection at the feature level and label correction with the multi-scale embeddings. For embedding learning, we propose a cross-scale fusion (Section 3.3) mechanism from fine to coarse to obtain robust embeddings. This approach enables the selection of more dependable clean labels through the small-loss criterion. Specifically, embeddings (\(\)) encompass multi-scale information from fine, medium, and coarse scale sequences derived from the same time series. Regarding noisy label correction, we introduce multi-scale embedding graph learning (Section 3.4) based on label propagation, utilizing the selected clean samples to correct the labels of unselected large-loss samples.

### Cross-scale Fusion for Clean Label Selection

After downsampling the original time series at different scales, it eliminates some of the differences in local regions between samples of the same category (as in Figure 1). However, the downsampled sequences (i.e., coarse scale) discard many local regions of the original time series. This tends to degrade the model's classification performance if the downsampled sequences are used directly for classification (please refer to Table 2 in the Experiments section). Meanwhile, existing studies [12; 23] on label-noise learning show that DNNs with different random initializations have high consistency in classification results for clean labeled samples in the early training period, while there is disagreement in the classification of noisy labeled samples. Based on the above findings, we utilize multiple DNNs (or encoders) with different random initializations to learn embeddings of different downsampled scale sequences separately, and perform cross-scale fusion. On the one hand, we exploit complementary

Figure 2: The Scale-teaching paradigm’s general architecture comprises two core processes: (i) clean label selection and (ii) noisy label correction. In the clean label selection phase, networks A, B, and C engage in cross-scale fusion, moving from fine to coarse (A\(\)B, B\(\)C)). They employ clean labels acquired through cross-teaching (A\(\)B, B\(\)C, C\(\)A) to guide their respective classification training. In the noisy label correction phase, pseudo labels derived from multi-scale embeddings graph learning are employed as corrected labels for time series not selected as clean labeled samples.

information between adjacent scale embeddings to promote learned embeddings to be more robust for classification. On the other hand, we leverage the divergence in the classification of noisy labeled samples by different DNNs to mitigate the negative impact of noisy labels in training. In this way, we can utilize the cross-scale fusion embeddings for classification, thus better using the small loss criterion [11; 29] for clean label selection. Specifically, downsampling is employed to generate different scale sequences from the same time series. Given a time series \(_{i}=\{x_{1},x_{2},,x_{T}\}\), supposing the downsampling ratio is \(k\). Then, we only keep data points in \(_{i}\) as follows:

\[_{i}^{k}=\{x_{k*j}\},j=1,2,,,\] (1)

where \(k[1,T/2]\), and a larger \(k\) indicates that \(_{i}^{k}\) is coarser. As shown in Figure 2, time series with multiple downsampling intervals (i.e., \(k\) = 1, 2, 4) is treated as the input data for training. To better utilize the small-loss criterion for clean label selection, each time series sample performs cross-scale fusion from fine to coarse (i.e., A\(\)B, B\(\)C) in the embedding space, which is mathematically defined as:

\[v_{i}^{k}=f(r_{i}^{k}\|v_{i}^{k-t}\|(r_{i}^{k}-v_{i}^{k-t })\|(r_{i}^{k} v_{i}^{k-t})),\] (2)

where \(r_{i}^{k}\) represents the single-scale embedding acquired by learning \(_{i}^{k}\) through an encoder. Meanwhile, \(v_{i}^{k}\) (or \(v_{i}^{k-t}\)) denotes the embedding of the time series \(X_{i}^{k}\) (or \(X_{i}^{k-t}\)) after performing cross-scale fusion. Here, \(t\) denotes the interval between adjacent downsampling ratios, and \(\|\) signifies the concatenation of two vectors to form a new vector. Notably, when \(k=1\), we employ the single-scale for classification training, resulting in \(v_{i}^{k}=r_{i}^{k}\). By combining \((r_{i}^{k}-v_{i}^{k-t})\) and \((r_{i}^{k} v_{i}^{k-t})\) for vector concatenation, \(v_{i}^{k}\) can capture more nuanced discriminative information between \(r_{i}^{k}\) and \(v_{i}^{k-t}\) than that of simply concatenating \(r_{i}^{k}\) with \(v_{i}^{k-t}\). The function \(f()\) represents a two-layer nonlinear network mapping function for fusing information of \(r_{i}^{k}\) and \(v_{i}^{k-t}\). Additionally, \(v_{i}^{k}\) has the same dimension as \(r_{i}^{k}\) and serves as the input data for the multi-scale embedding graph learning process.

### Multi-scale Embedding Graph Learning for Noisy Label Correction

We now present the multi-scale embedding graph learning module for correcting noisy labels. This module incorporates selected clean labels using label propagation theory. The process consists of two stages: graph construction and noisy label correction.

**Graph Construction.** It is assumed that the set of cross-fusion embeddings obtained from a batch of time series is defined as \(V=\{v_{1}^{k},v_{2}^{k},,v_{M}^{k}\}\), where \(M\) is the batch size. Intuitively, samples close to each other in the feature space have a high probability of belonging to the same class. However, in label-noise learning, \(v_{i}^{k}\) obtained from the current iterative training of the model may have unstable information, resulting in large deviations in the information of the nearest-neighbor samples of \(v_{i}^{k}\). To address this issue, the proposed approach performs a momentum update  on \(v_{i}^{k}\) during training, which is defined as:

\[_{i}^{k}[e] =  v_{i}^{k}[e]+(1-)_{i}^{k}[e-1],\] (3)

where \(e\) is the current training epoch and \(\) denotes the momentum update parameter.

The multi-scale embeddings nearest-neighbor graph can be created by using Euclidean distance among different \(_{i}^{k}\). A common approach is the use of the Gaussian similarity function  to obtain the nearest-neighbor graph edge weight, which is defined as:

\[W_{ij} = (-d(_{i}^{k}}{}, _{j}^{k}}{})),\] (4)

where \(d()\) is the Euclidean distance function and \(\) is a fixed parameter. \(W R^{M M}\) is a symmetric adjacency matrix, and the element \(W_{ij}\) denotes the nearest-neighbor edge weight between the embedding \(v_{i}^{k}\) and \(v_{j}^{k}\) (note that larger values indicate closer proximity). Then, \(W\) is normalized based on the graph laplacians  to obtain \(Q=D^{-1/2}WD^{-1/2}\), where \(D=diag(W1_{n})\) is a diagonal matrix. Specifically, the \(K\) neighbors with the largest values in each row of \(Q\) are employed to create the nearest-neighbor graph. It is noteworthy that the embeddings in each mini-batch are utilized to generate the nearest-neighbor graph, thus obtaining \(Q\) within short computational time.

Noisy Label Correction.Specifically, small training loss samples acquired by DNNs in the early training period can be considered as clean samples, while samples with large training losses are considered as noisy ones [12; 14]. The above learning pattern of DNNs has been mathematically validated  (see Appendix A for details). Under this criterion, prior studies [12; 13; 23] have typically employed samples with small losses after a \(e_{warm}\) warm-up training as clean labels. Following , we extend the small-loss sample selection process to operate within each class, thereby enhancing the overall quality of the chosen clean labels. In our method, samples chosen with clean labels are considered labeled data, whereas unselected samples are treated as unlabeled data.

We utilize clean samples selected from time series at different scales in a cross-teaching manner (as in Figure 2). This could explore complementary information from different scale fusion embeddings to deal with noisy labels. It is supposed that there is a corresponding one-hot encoding matrix \(Y R^{M C}(Y_{ij}\{0,1\})\) for the cross-fusion embeddings \(V\). If \(y_{i}\) is identified as a clean label, we employ \(y_{i}\) to set \(Y_{i}\) as a one-hot encoded label. Otherwise, all the elements in \(Y_{i}\) are identified as zero. Through \(Y\), the pseudo-label of each node in the nearest-neighbor graph \(Q\) can be obtained in an iterative way based on the label propagation theory. The specific solution formula is defined as:

\[F_{t+1}= QF_{t}+(1-)Y,\] (5)

where \(F_{t} R^{M C}\) denotes the predicted pseudo-label of the \(t\)-th iteration and \((0,1)\) is a hyperparameter. Naturally, \(F_{t}\) has a closed-form solution  defined as follows:

\[=(I- Q)^{-1}Y,\] (6)

where \( R^{M C}\) is the final pseudo-labels and \(I\) denotes the identity matrix. Finally, the corrected label obtained for an unselected large-loss sample \(X_{i}\) is defined as:

\[y_{i}=_{c}_{i}^{c},\] (7)

However, \(\) is the estimated pseudo-labels, which inevitably contain some incorrect labels. To address this issue, two strategies are used to improve the quality of pseudo-labels in \(\). For the first strategy, the model continues training \(e_{update}\) epochs by using small-loss samples after \(e_{warm}\) epochs warm-up training to improve the robustness of the multi-scale embeddings. Then, the noisy label correction is performed after \((e_{warm}+e_{update})\) epoch. For the second strategy, a dynamic threshold \(_{e}(c)=(c)}{(_{e})}\) is utilized for each class  to select the pseudo-labels with a high confidence for noisy label correction, where \(_{e}(c)\) is the number of labeled samples contained in class \(c\) in the \(e\)-th epoch, and \(\) is a constant threshold.

Overall Training.Finally, each encoder utilizes the selected clean samples in combination with multi-scale embedding graph learning to perform noisy label correction for unselected large-loss samples. Combining the training data of the selected clean labels and those of corrected labels, the proposed Scale-teaching paradigm utilizes cross-entropy for time-series label-noise learning. Please refer to Algorithm 1 in the Appendix for the specific pseudo-code of Scale-teaching.

## 4 Experiments

### Experiment Setup

Datasets.We use three time series benchmarks (four individual large datasets [3; 52; 53], UCR 128 archive , and UEA 30 archive ) for experiments. Among the four individual large datasets, HAR  and UniMiB-SHAR  are human activity recognition scenarios; FD-A  is the mechanical fault diagnosis scenario; Sleep-EDF  belongs to the sleep stage classification scenario. The UCR archive  contains 128 univariate time series datasets from different real-world scenarios. The UEA archive  contains 30 multivariate time series datasets from real-world scenarios. For details on the above datasets, please refer to Appendix B. Since all the datasets in three time series benchmarks are correctly labeled, we utilize a label transformation matrix \(T\) to add noises to the original correct labels , where \(T_{ij}\) is the probability of label \(i\) being flipped to \(j\). We use three types of noisy labels for evaluations, namely Symmetric (Sym) noise, Asymmetric (Asym) noise, and Instance-dependent (Ins) noise. Symmetric (Asymmetric) noise randomly replaces a true label with other labels with an equal (unequal) probability. Instance noise  means that the noisy label is instance-dependent. Like [4; 12; 23], we use the test set with correct labels for evaluations.

**Baselines.** We select seven methods for comparative analyses, namely 1) Standard: direct training of the model using cross-entropy with all noisy labels; 2) Mixup ; 3) Co-teaching ; 4) FINE ; 5) SREA ; 6) SELC ; and 7) CULCU . Among them, Standard, Mixup, and Co-teaching are the benchmark methods for label-noise learning. FINE, SELC, and CULCU are the state-of-the-art methods that do not need to focus on data types, and SREA is the state-of-the-art method in time series domain. In addition, for fair comparisons, all the baselines and the proposed Scale-teaching paradigm use the same encoder and classifier. We focus on the ability of different label-noise learning paradigms to cope with time series noise labels, rather than the classification performance achieved by using fully correct labels. Hence, considering the trade-off between the running time and classification performance, we choose FCN  as the encoder of Scale-teaching. For more details of baselines, please refer to Appendix C.

**Implementation Details.** Based on the experience [19; 44] in time series modeling, we utilize three different sampling intervals 1, 2, 4 as the input muti-scale series data for Scale-teaching. We use Adam as the optimizer. The learning rate is set to 1e-3, the maximum batch size is set to 256, and the maximum epoch is set to 200. \(e_{warm}\) is set to 30 and \(e_{update}\) is set to 90. \(\) in Eq. 3 is set to 0.9, \(\) in Eq. 4 is set to 0.25, \(\) in Eq. 5 is set to 0.99, the largest neighbor \(K\) is set to 10, and \(\) is set to 0.99. In addition, following the parameter settings suggested in , we linearly decay the learning rate to zero from the \(80\)-th epoch to \(200\)-th epoch. For a comprehensive understanding of the hyperparameter selection and the implementation of the small-loss criterion applied to Scale-teaching, please consult Appendix C. To reduce random errors, we utilize the mean test classification accuracy of the last five epochs of the model on the test set as experimental results. All the experiments are independently conducted five times with five different seeds, and the average classification accuracy and rank are reported. Finally, we build our model using PyTorch 1.10 platform with 2 NVIDIA GeForce RTX 3090 GPUs. Our implementation of Scale-teaching is available at https://github.com/qianlima-lab/Scale-teaching.

### Main Results

We evaluate each time series benchmark using four noise ratios, Sym 20%, Sym 50%, Asym 40%, and Ins 40%. Due to space constraints, we only give the average ranking of all the methods on each benchmark in Table 1. Please refer to Appendix D for the specific test classification accuracies. Besides, for UCR 128 and UEA 30 archives, we use the Wilcoxon signed rank test (P-value)  to analyze the classification performance of baselines. As shown in Table 1, the proposed Scale-teaching paradigm achieves the best Avg Rank in all the cases. It is found that Mixup  and FINE  perform worse than the Standard method in most cases. For Mixup, the complex dynamic properties

of the original time series are destroyed probably due to the mixture of two different time series mechanisms. FINE uses embeddings of the input data to select clean labels. Although FINE achieves advanced classification performance for image data, it is difficult to be used directly for time series data because its discriminative patterns are easily distorted by external noises. SREA  has a good performance on the UEA 30 archive, while it performs poorly on the other benchmarks. Meanwhile, Co-teaching , SELC , and CULCU  are more robust against time series noisy labels in different cases, further indicating that the small-loss criterion is also applicable to time series.

### Multi-scale Analysis

To explain the multi-scale mechanism in the Scale-teaching paradigm, we add an ablation study based on Scale-teaching (w/o cross-scale fusion). We select the UCR 128 archive to analyze the classification results obtained by the fine, medium, and coarse scale classifiers. As shown in Figure 3, the classification results of different scale sequences have evident complementary information. Scale-teaching can effectively use complementary information between cross-scale to obtain more robust embeddings and clean labels. In response to the tendency of the coarse scale to ignore discriminative patterns in fine scale (please see Table 2), our proposed cross-scale fusion mechanism can effectively improve the classification performance of medium and coarse scales while retaining complementarity. Please refer to Appendix E for the specific classification results of Figure 3 and Table 2. In Appendix E, we also analyze the order and size of the downsampled input scale sequence for Scale-teaching.

Scale-teaching utilizes multi-scale embeddings to generate the nearest-neighbor graph, and uses clean labels selected for noisy label correction. To explore the distribution of different classes of embeddings, we employ t-SNE  for dimensionality reduction visualization. Specifically, we utilize the UniMiB-SHAR dataset containing Sym 20% noisy labels for visualization. As shown in Figure 4, we find that the embeddings learned by Scale-teaching are more discriminative across classes than the Standard and CULCU methods that use a single scale series for training. The above results suggest that Scale-teaching can effectively exploit the complementary information between different scales, prompting the learned embeddings to be more discriminative between classes. In addition, we choose the FD-A dataset for t-SNE visualization, and please refer to Appendix E.

    &  &  \\  Noise Ratio & Metric & Fine & Medium & Coarse & Fine & Medium & Coarse \\   & Avg Acc & 65.13 & 30.11 & 28.17 & 59.67 & 68.17 & **68.70** \\  & Avg Rank & 2.38 & 5.09 & 5.37 & 3.20 & 2.17 & **2.11** \\  & P-value & 1.89E-03 & 2.85E-37 & 2.07E-40 & 1.58E-09 & 3.74E-02 & - \\   & Avg Acc & 49.61 & 29.01 & 28.87 & 47.75 & 51.93 & **52.87** \\  & Avg Rank & 2.64 & 4.78 & 4.75 & 3.01 & 2.45 & **2.27** \\   & P-value & 1.94E-03 & 6.78E-25 & 1.59E-27 & 1.80E-07 & 2.80E-02 & - \\  

Table 2: The test classification accuracy (%) results of different scale classifiers on UCR 128 archive. The best results are **bold**, and the second best results are underlined. When P-value < 0.05, it indicates that the performance of Scale-teaching’s coarse scale classifier is significant than other classifiers.

Figure 3: Venn diagram of the average number of correctly classified samples for the different scale sequences of UCR 128 archive with Sym 20% noisy labels. The numbers in the figure indicate the complements and intersections of classification results at different scales.

### Small-loss Analysis

To analyze the application of the small-loss criterion to time series data, we visualize the change of loss values for ground-truth clean and noisy time series samples during training. Specifically, Figure 5 shows the change in loss values of the models trained by the Standard method and Scale-teaching on two UCR time series datasets. When the model is trained with the Standard method, differences can be found in the loss values of clean and noisy samples in the network early training, especially in Figure 5 (b). The Standard method makes the model gradually fit the noisy samples as the training proceeds, while Scale-teaching improves the ability of the model to handle noisy labels. To further prove its effectiveness, we selected two other UCR datasets for the loss value change analysis, which have the same pattern as Figure 5. Also, we report the HAR and UniMiB-SHAR dataset's loss value probability distributions of clean and noisy samples. For more details, please refer to Appendix F.

### Ablation Study

To verify the robustness of each module in Scale-teaching, the ablation experiments have been conducted in the HAR and UniMiB-SHAR datasets, and the results are shown in Table 3. Specifically, (1) **w/o cross-scale fusion**: the cross-scale embedding fusion from fine to coarse mechanism is ablated; (2) **only single scale**: only the original time series is used for training; (3) **w/o graph learning**: the multi-scale embedding graph learning module for noisy label correction is ablated; (4) **w/o moment**: the embedding momentum update mechanism (Eq. 3) is ablated; (5) **w/o dynamic threshold**: using a dynamic threshold to select high-quality propagation pseudo-labels is ablated.

As shown in Table 3, the cross-scale fusion strategy (w/o cross-scale fusion) and the clean labels cross-teaching mechanism (only single scale) can effectively improve the classification performance of Scale-teaching, especially on the UniMiB-SHAR dataset with a large number of classes. Meanwhile, in terms of label correction based on multi-scale embedding graph learning, the results of the corresponding ablation module show that improving the stability of embedding (w/o moment) and

Figure 4: t-SNE visualization of the learned embeddings on the UnimiB-SHAR dataset with Sym 20% noisy labels (values in parentheses are the test classification accuracies).

Figure 5: The change of loss values for clean and noisy time series samples under Aysm 40% noise labels. The solid line and shading indicate the mean and standard deviation loss values of all clean (or noisy) training samples within each epoch.

selecting high-quality pseudo-labels (w/o dynamic threshold) can effectively improve the performance of label correction based on graph learning.

Furthermore, we select the four individual large datasets without noisy labels for evaluation. As shown in Table 4, Scale-teaching's classification performance is still better than most baselines. It's worth mentioning that SREA  employs an unsupervised time series reconstruction loss as an auxiliary task, which reduces the model's classification performance without noisy labels. We also provide the corresponding test classification results for Tables 3 and 4 under the F1-score metric in Appendix G. Additionally, we find the running time of Scale-teaching, which is faster than FINE, SREA and CULCU for datasets with a larger number of samples or longer length of the sequence. We further analyze the classification performance of the proposed Scale-teaching paradigm and time series classification methods [15; 60] in Appendix G.

## 5 Conclusions

Limitations.The input scales of our proposed Scale-teaching paradigm can only select a fixed number of scales for training, and the running time will increase as the number of scales increases.

Conclusion.In this paper, we propose a deep learning paradigm for time-series classification with noisy labels called Scale-teaching. Experiments on the three time series benchmarks show that the Scale-teaching paradigm can utilize the multi-scale properties of time series to effectively handle noisy labels. Comprehensive analyses on multi-scale and ablation studies demonstrate the robustness of the Scale-teaching paradigm. In the future, we will explore the design of scale-adaptive time-series label-noise learning models.