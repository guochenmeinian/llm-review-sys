# Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules

Zhiyuan Liu\({}^{}\) Yaorui Shi\({}^{}\) An Zhang\({}^{}\) Enzhi Zhang\({}^{@sectionsign}\)

Kenji Kawaguchi\({}^{}\) Xiang Wang\({}^{}\) Tat-Seng Chua\({}^{}\)

\({}^{}\)National University of Singapore, \({}^{}\)University of Science and Technology of China

\({}^{@sectionsign}\)Hokkaido University

{acharkq,shiyaorui,xiangwang1223}@gmail.com,anzhang@u.nus.edu

enzhi.zhang.n6@elms.hokudai.ac.jp, {kenji,chuats}@comp.nus.edu.sg

###### Abstract

Masked graph modeling excels in the self-supervised representation learning of molecular graphs. Scrutinizing previous studies, we can reveal a common scheme consisting of three key components: (1) graph tokenizer, which breaks a molecular graph into smaller fragments (_i.e.,_ subgraphs) and converts them into tokens; (2) graph masking, which corrupts the graph with masks; (3) graph autoencoder, which first applies an encoder on the masked graph to generate the representations, and then employs a decoder on the representations to recover the tokens of the original graph. However, the previous MGM studies focus extensively on graph masking and encoder, while there is limited understanding of tokenizer and decoder. To bridge the gap, we first summarize popular molecule tokenizers at the granularity of node, edge, motif, and Graph Neural Networks (GNNs), and then examine their roles as the MGM's reconstruction targets. Further, we explore the potential of adopting an expressive decoder in MGM. Our results show that a subgraph-level tokenizer and a sufficiently expressive decoder with remask decoding have a large impact on the encoder's representation learning. Finally, we propose a novel MGM method **SimSGT**, featuring a **S**imple **G**NN-based **T**okenizer (**SGT**) and an effective decoding strategy. We empirically validate that our method outperforms the existing molecule self-supervised learning methods. Our codes and checkpoints are available at https://github.com/syr-cn/SimSGT.

## 1 Introduction

Molecular representation learning (MRL) [1; 2; 3] is a critical research area with numerous vital downstream applications, such as molecular property prediction , drug discovery [5; 6], and retrosynthesis [7; 8]. Given that molecules can be represented as graphs, graph self-supervised learning (SSL) is a natural fit for this problem. Among the various graph SSL techniques, Masked Graph Modeling (MGM) has recently garnered significant interest [9; 10; 11].

In this paper, we study MRL through MGM, aiming to pretrain a molecule encoder for subsequent fine-tuning in downstream applications. After looking at the masked modeling methods in graph [12; 9; 10], language [13; 14], and computer vision [15; 16], we summarize that MGM relies on three key components - graph tokenizer, graph masking, and graph autoencoder, as Figure 1 shows:

* **Graph tokenizer.** Given a graph \(g\), the graph tokenizer employs a graph fragmentation function [1; 17; 18; 2] to break \(g\) into smaller subgraphs, such as nodes and motifs. Then, thesefragments are mapped into fixed-length tokens to serve as the targets being reconstructed later. Clearly, the granularity of graph tokens determines the abstraction level of representations in masked modeling . This is especially relevant for molecules, whose properties are largely determined by patterns at the granularity of subgraphs . For example, the molecule shown in Figure 2 contains a benzene ring subgraph. Benzene ring confers the molecule aromaticity, making it more stable than saturated compounds that only have single bonds . Therefore, applying graph tokenizers that generate subgraph-level tokens might improve the downstream performances.
* **Graph masking.** Before feeding into the autoencoder, \(g\) is corrupted by adding random noise, typically through randomly masking nodes or dropping edges . Graph masking is crucial to prevent the autoencoder from merely copying the input, and guide the autoencoder to learn the relationships between co-occurring graph patterns.
* **Graph autoencoder.** Graph autoencoder consists of a graph encoder and a graph decoder . The graph encoder generates the corrupted graph's hidden representations, based on which the graph decoder attempts to recover the corrupted information. The encoder and the decoder are jointly optimized by minimizing the distance between the decoder's outputs and the reconstruction targets, _i.e.,_ the graph tokens induced by the graph tokenizer. Given the complex subgraph-level tokens as targets, an effective reconstruction might demand a sufficiently expressive graph decoder.

Although all three components mentioned above are crucial, previous MGM studies primarily focus on graph masking  and graph encoder , with less emphasis on the tokenizers and decoders. For example, while there exist extensive motif-based fragmentation functions for MRL , they have been overlooked as tokenizers for MGM. Moreover, many previous works  employ a linear or MLP decoder for graph reconstruction, leaving more expressive decoders largely unexplored.

In this work, we first summarize the various fragmentation functions as graph tokenizers, at the granularity of nodes, edges, motifs, and Graph Neural Networks (GNNs). Given this summary, we systematically evaluate their empirical performances for MGM. Our analysis shows that reconstructing subgraph-level tokens in MGM can improve over the node tokens. Moreover, we find that a sufficiently expressive decoder combined with remask decoding  could improve the encoder's representation quality. Notably, remask "decouples" the encoder and decoder, redirecting the encoder's focus away from molecule reconstruction and more towards MRL, leading to better downstream performances. In summary, we reveal that incorporating a subgraph-level tokenizer and a sufficiently expressive decoder with remask decoding gives rise to improved MGM performance.

Based on the findings above, we propose a novel pretraining framework - Masked Graph Modeling with a Simple GNN-based Tokenizer (**SimSGT**). SimSGT employs a **Simple GNN-**T**okenizer (**SGT**) that removes the nonlinear update function in each GNN layer. Surprisingly, we show that a single-layer SGT demonstrates competitive or better performances compared to other pretrained GNN-based and chemistry-inspired tokenizers. SimSGT adopts the GraphTrans  architecture for its encoder and a smaller GraphTrans for its decoder, in order to provide sufficient capacity for both the tasks of MRL and molecule reconstruction. Furthermore, we propose remask-v2 to decouple the encoder and decoder of the GraphTrans architecture. Finally, SimSGT is validated on downstream molecular property prediction and drug-target affinity tasks , surpassing the leading graph SSL methods (_e.g.,_ GraphMAE  and Mole-BERT ).

## 2 Preliminary

In this section, we begin with the introduction of MGM. Then, we provide a categorization of existing graph tokenizers. Finally, we discuss the architecture of graph autoencoders for MGM.

**Notations.** Let \(\) denote the space of graphs. A molecule can be represented as a graph \(g=(,)\), where \(\) is the set of nodes and \(\) is the set of edges. Each node \(i\) is associated with a node feature \(_{i}^{d_{0}}\) and each edge \((i,j)\) is associated with an edge feature \(_{ij}^{d_{1}}\). The graph \(g\)'s structure can also be represented by its adjacency matrix \(\{0,1\}^{||||}\), such that \(_{ij}=1\) if \((i,j)\) and \(_{ij}=0\) otherwise.

### Preliminary: Masked Graph Modeling

Here we illustrate MGM's three key steps: graph tokenizer, graph masking, and graph autoencoder.

**Graph tokenizer.** Given a graph \(g\), we leverage a graph tokenizer \(tok(g)=\{_{t}=m(t)^{d}|t f(g)\}\) to generate its graph tokens as the reconstruction targets. The tokenizer \(tok()\) is composed of a fragmentation function \(f\) that breaks \(g\) into a set of subgraphs \(f(g)=\{t=(_{t},_{t})|t g\}\), and a mapping function \(m(t)^{d}\) that transforms the subgraphs into fixed-length vectors. In this work, we allow \(f(g)\) to include overlapped subgraphs to enlarge the scope of graph tokenizers.

**Graph masking.** Further, we add noises to \(g\) by random node masking. Here we do not use edge dropping because Hou _et al._ empirically show that edge dropping easily leads to performance drop in downstream tasks. Specifically, node masking samples a random subset of nodes \(_{m}\) and replaces their features with a special token \(_{0}^{d_{0}}\). We denote the masked node feature by \(}_{i}\):

\[}_{i}=_{0},& i_{ m}\\ _{i},&.\] (1)

**Graph autoencoder.** The corrupted graph \(\) is then fed into a graph autoencoder for graph reconstruction. We defer the details of the graph autoencoder's architecture to Section 2.3. Let \(\{_{i}|i\}\) be the node-wise outputs of the graph autoencoder. We obtain subgraph \(t\)'s prediction \(}_{t}=(\{_{i}|i_{t}\})\) by mean pooling the representations of its nodes, if not especially noted. The graph autoencoder is trained by minimizing the distance between the predictions \(\{}_{t}|t f(g)\}\) and the targets \(tok(g)=\{_{t}|t f(g)\}\). The reconstruction loss is accumulated on tokens that include corrupted information \(\{t|t f(g),_{t}_{m}\}\):

\[L_{0}=_{t f(g),_{t}_{m} }(}_{t},_{t}),\] (2)

where \((,)\) is the loss function, dependent on the type of \(_{t}\). We use mean square error  for \((,)\) when \(_{t}\) is a continuous vector, and use cross-entropy  for \((,)\) when \(_{t}\) is a discrete value.

### Revisiting Molecule Tokenizers

Scrutinizing the current MRL methods, we summarize the molecule tokenizers into four distinct categories, as Table 1 shows. A detailed description of the first three categories is systematically provided here, while the Simple GNN-based Tokenizer is introduced in Section 3.

**Node, edge tokenizer [12; 11].** A graph's nodes and edges can be used as graph tokens directly:

\[tok_{}(g)=\{_{i}=_{i}|i\}, tok_ {}(g)=\{_{ij}=_{ij}|(i,j)\}.\] (3)

Figure 2(a) illustrates the use of atomic numbers of nodes and bond types of edges as graph tokens in a molecule. These have been widely applied in previous research [12; 11; 25; 24], largely due to their

  
**Tokenizers** & **Subgraph types** & **Tokens** & **Potential limitations** \\  Node, edge & Nodes and edges & Features of nodes and edges & Low-level feature \\ Motif & FGs, cycles, _etc._ & Motif types & Rely on expert knowledge \\ Pretrained GNN & Rooted subtrees & Frozen GNN representations & Extra pretraining for tokenizer \\ Simple GNN & Rooted subtrees & Frozen GNN representations & - \\   

Table 1: Summary of graph tokenizers.

simplicity. However, atomic numbers and bond types are low-level features. Reconstructing them may be suboptimal for downstream tasks that require a high-level understanding of graph semantics.

**Motif-based tokenizer.** Motifs are statistically significant subgraph patterns in graph structures. For molecules, functional groups (FGs) are motifs that are manually curated by experts based on the FGs' biochemical characteristics [30; 31]. For example, molecules that contain benzene rings exhibit the property of aromaticity. Considering that the manually curated FGs are limited and cannot fully cover all molecules, previous works [32; 1; 18] employ chemistry-inspired fragmentation functions for motif discovery. Here we summarize the commonly used fragmentation functions:

* **FGs**[18; 2]. An FG is a molecule's subgraph that exhibits consistent chemical behaviors across various compounds. In chemistry toolkits [30; 31], the substructural patterns of FGs are described by the SMARTS language . Let \(_{0}=\{s_{i}\}_{i=1}^{|_{0}|}\) be a set of SMARTS patterns for FGs, and let \(p_{s}(g)\) be the function returning the set of \(s\) FGs in \(g\). FG-based fragmentation works as: \[f_{}(g,_{0})=_{s_{0}}p_{s}(g),\] (4)
* **Cycles**[32; 1; 18]. Cycles in molecules are often extracted as motifs due to their potential chemical significance. Figure 2(b) depicts the process of fragmenting a five-node cycle as a motif. If two cycles overlapped more than two atoms, they can be merged because they constitute a bridged compound . Let \(C_{n}\) represent a cycle of \(n\) nodes. They can be written as: \[f_{}(g) =\{t|t=C_{|t|},t g\},\] (5) \[f_{}(g) =\{t_{i} t_{j}|t_{i},t_{j} f_{}(g),i j,|t_ {i} t_{j}|>2\},\] (6)
* **BRICS**[35; 1]. BRICS fragments a molecule at the potential cleavage sites, where chemical bonds can be broken under certain environmental conditions or catalysts. The key step of BRICS is to identify a molecule \(g\)'s potential cleavage sites, denoted by \((g)\). This is achieved by finding bonds with both sides matching one of the pre-defined "FG-like" substructural patterns \(_{1}\) in BRICS: \[(g)=\{(_{t}_{g-t})|t,g- t f_{}(g,_{1})\},\] (7) where \(g-t=g[_{t}]\) denotes deleting \(g\)'s nodes in \(t\) and the corresponding incident edges ; \((_{t}_{g-t})\) contains the bond that connects \(t\) and \(g-t\). Next, \(g\) is fragmented into the maximum subgraphs that contain no bonds in the cleavage sites \((g)\): \[f_{}(g)=\{t|(g)_{t}=,f_{}(t ) 2^{t}=\{t\},t g\}.\] (8)

Figure 3: Examples for the first three types of graph tokenizers and their induced subgraphs. (b) A motif-based tokenizer that applies the fragmentation functions of cycles and the remaining nodes. (c) A two-layer GIN-based tokenizer that extracts 2-hop rooted subtrees for every node in the graph.

Note that, the original BRICS includes more rules, such as a constraint on \(t\) and \(g-t\)'s combinations. We present only the key step here for simplicity.
* **Remaining nodes and edges**: Given another fragmentation function \(f_{0}\), the nodes and edges that are not included in any of \(f_{0}\)'s outputs are treated as individual subgraphs. This improves \(f_{0}\)'s coverage on the original graph. Figure 2(b) shows an example of fragmenting remaining nodes after \(f_{}\): nodes that are not in any cycles are treated as individual subgraphs.

To obtain more fine-grained subgraphs, previous works  usually combine several fragmentation functions together by unions (_e.g., \(f_{1}(g) f_{2}(g)\)_) or compositions (_e.g., \(\{f_{2}(t)|t f_{1}(g)\}\)_). Let \(f_{}\) be the final fragmentation function after combination. We break every molecule in the dataset by \(f_{}\) and gather a motif vocabulary \(\), which filters out infrequent motifs by a threshold. Then, given a new molecule \(g^{}\), we can generate its tokens by one-hot encoding its motifs \(f_{}(g^{})\):

\[tok_{}(g^{})=\{_{t}=(t,) |t f_{}(g^{})\}.\] (9)

**Pretrained GNN-based tokenizer .** Pretrained GNNs can serve as graph tokenizers. Take a \(k\)-layer Graph Isormophism Network (GIN)  as an example. Its node embedding summarizes the structural information of the node's \(k\)-hop rooted subtree, making it a subgraph-level graph token (Figure 2(c)). A GIN performs the fragmentation and the mapping simultaneously. It can be written as:

\[tok_{}(g)=\{_{i}=(_{i}^{(k)})|i \},\] (10) \[_{i}^{(k)}=^{(k)}(_{i}^{(k-1)},^{(k)}(\{_{j}^{k-1},j(i)\})),\] (11)

where \(()\) collects information from node \(i\)'s neighbors, based on which \(()\) updates \(i\)'s representation. \(()\) denotes stop-gradient, which stops the gradient flow to the tokenizer during MGM pretraining. In addition to GINs, other GNNs can also serve as tokenizers. To obtain meaningful graph tokens, we pretrain a GNN before employing it as a tokenizer . Once pretrained, this GNN is frozen and employed for the subsequent MGM pretraining. In Section 4, we evaluate the prevalent graph pretraining strategies for tokenizers. Given that GNN-based tokenizers provide node-wise tokens, we directly minimize the distance between the graph tokens and the autoencoder's outputs \(\{_{i}\}_{i=1}^{||}\) of the masked nodes \(_{m}\), _i.e.,_\(L_{0}=_{m}|}_{i_{m}}( }_{i}=_{i},_{i})\).

### Revisiting Graph Autoencoders

**Background.** Graph autoencoder consists of a graph encoder and a graph decoder. We pretrain them with the objective of graph reconstruction. Once well pre-trained, the encoder is saved for downstream tasks. MGM works  usually adopt expressive graph encoders like GINEs  and Graph Transformers . However, the exploration on expressive decoders has been limited. Many previous works  apply a linear or an MLP decoder, similar to BERT's design .

However, recent studies  have revealed the disparity between representation learning and reconstruction tasks. They show that a sufficiently expressive decoder could improve the encoder's representation quality. Delving into these studies, we identify two key elements to improve the representation quality: a sufficiently expressive decoder and remask decoding .

**Sufficiently expressive decoder.** Following , we devise smaller versions of the encoder's architecture as decoders. We adopt the GINE  and **G**raph**Trans** (denoted as **GTS**) as encoders. GTS stacks transformer layers on top of the GINE layers to improve the ability of modeling global interactions. Table 2 summarizes their different versions that we compare in this work.

**Remask decoding .** Remask controls the focus of the encoder and the decoder. Let \(\{_{i}|i\}\) be the encoder's node-wise hidden representations for the masked graph. Remask decoding masks the hidden representations of the masked nodes \(_{m}\) again by a special token \(_{1}^{d}\) before feeding them into the decoder. Formally, the remasked hidden representation \(}_{i}\) is as follows:

\[}_{i}=_{1},& i_{m}\\ _{i},&.\] (12)

   Model & GINE, dim 300 Transformer, dim 128 \\  Linear & - & - \\ GINE & 5 layer & - \\ GINE-Small & 3 layer & - \\ GTS & 5 layer & 4 layer \\ GTS-Small & 3 layer & 1 layer \\ GTS-Tiny & 1 layer & 1 layer \\   

Table 2: The compared GNN architectures for encoders and decoders.

Remarks the encoder's ability on predicting the corrupted information by removing the encoder's representation on the masked part. The encoder is enforced to generate effective representations for the unmasked part, to provide signals for the decoder for graph reconstruction.

## 3 Methodology

In this section, we present our method - Masked Graph Modeling with a Simple GNN-based Tokenizer (**SimSGT**) (Figure 4). Specifically, it applies the GTS  architecture for both its encoder and decoder. SimSGT features a **S**imple **G**NN-based **T**okenizer (**SGT**), and employs a new remask strategy to decouple the encoder and decoder of the GTS architecture.

**Simple GNN-based Tokenizer.** SGT simplifies existing aggregation-based GNNs  by removing the nonlinear update functions in GNN layers. It is inspired by studies showing that carefully designed graph operators can generate effective node representations [39; 40]. Formally, a \(k\)-layer SGT is:

\[tok_{}(g) =\{_{i}=([_{i}^{(1)},..., _{i}^{(k)}])|i\},\] (13) \[_{i}^{(0)} =(_{i})^{d},  i,\] (14) \[}^{(l)} =()^{(l-1)}^{| | d}, 1 l k,\] (15) \[^{(l)} =(}^{(l)}),\] (16)

where \(()\) is a linear layer that uses the weights of the encoder's node embedding function; \(_{i}^{(l)}\) is the \(i\)-th row of \(^{(l)}\); \(()\) is a standard Batch Normalization layer without the trainable scaling and shifting parameters ; and \(()\) is the graph operator that represents the original GNN's aggregation function. For example, GIN has \(()=+(1+)\) and GCN has \(()=}^{-1/2}}}^{-1/2}\), where \(}=+\) and \(}\) is the degree matrix of \(}\).

Note that, SGT does not have trainable weights, allowing its deployment without pretraining. Its tokenization ability relies on the graph operator \(()\) that summarizes each node's neighbor information. Additionally, we concatenate the outputs of every SGT layer to include multi-scale information. We show in experiments that SGT transforms the original GNN into an effective tokenizer.

**Graph autoencoder.** SimSGT employs the GTS architecture as the encoder, and a smaller version of GTS (_i.e.,_ GTS-Small in Table 2) as the decoder. This architecture follows the asymmetric encoder-decoder design in previous works [15; 38]. Further, we propose a new remask strategy, named **remask-v2**, to decouple SimSGT's encoder layers and decoder layers,

**Remask-v2.** Remask-v2 constrains the encoder's ability on predicting the corrupted information by dropping the masked nodes' \(_{m}\) representations before the Transformer layers (Figure 4). After the Transformer layers, we pad special mask tokens \(_{1}^{d}\) to make up the previously dropped nodes' hidden representations. Compared to the original remask, remask-v2 additionally prevents the Transformer layers from processing the masked nodes. It thereby avoids the gap of processing masked nodes in pretraining but not in fine-tuning .

Figure 4: Overview of the SimSGT’s framework.

## 4 Rethinking Masked Graph Modeling for Molecules

**Experimental setting.** In this section, we perform experiments to assess the roles of tokenizer and decoder in MGM for molecules. Our experiments follow the transfer learning setting in [12; 9]. We pretrain MGM models on 2 million molecules from ZINC15 , and evalute the pretrained models on eight classification datasets in MoleculeNet : BBBP, Tox21, ToxCast, Sider, ClinTox, MUV, HIV, and Bace. These downstream datasets are divided into train/valid/test sets by scaffold split to provide an out-of-distribution evaluation setting. We report the mean performances and standard deviations on the downstream datasets across ten random seeds. Throughout the experiments, we use random node masking of ratio \(0.35\). More detailed experimental settings can be found in Appendix D.

### Rethinking Decoder

We investigate the impact of an expressive decoder on MGM's performance. A single-layer GIN-based SGT tokenizer is utilized in these experiments. Table 2(a) summarizes the results.

**Finding 1. A sufficiently expressive decoder with remask decoding is crucial for MGM.** Table 2(a) shows that using a sufficiently expressive decoder with remask decoding can significantly improve downstream performance. This can be attributed to the disparity between molecule reconstruction and MRL tasks: the last several layers in the autoencoder will be specialized on molecule reconstruction, while losing some representation ability [15; 38]. When using a linear decoder, the last several layers in the encoder will be reconstruction-specialized, which can yield suboptimal results in fine-tuning.

A sufficiently expressive decoder is essential to account for the reconstruction specialization. As shown by Figure 4(a), employing an expressive decoder results in significantly lower reconstruction loss than a linear decoder. However, increasing the decoder's expressiveness without remask decoding cannot improve the downstream performance (Table 2(a)). This leads to our exploration on remask.

**Finding 2. Remask constrains the encoder's efforts on graph reconstruction for effective MRL.** Figure 4(b) shows that the linear probing accuracy of the masked atom types on an encoder pretrained with remask is significantly lower than the accuracy on an encoder pretrained without remask. This shows that remask makes the encoder spend less effort on predicting the corrupted information. Moreover, remask only slightly sacrifices the autoencoder's reconstruction ability, when paired with the GTS-Small decoder (Figure 4(a)). Combining the

   Encoder & Decoder & Avg. \\  GTS & GTS-Tiny & 74.7 \\ GTS & GTS-Small & **75.8** \\ GTS & GTS & 74.9 \\   

Table 4: Testing decoder’s size. Remask-v2 is used.

observation that remask improves downstream performances (Table 2(a)), it indicates that remask constrains the encoder's efforts on graph reconstruction, allowing it to focus on MRL.

In addition, remask-v2 outperforms remask-v1 with GTS architecture. This improvement can be attributed to remask-v2's ability to prevent the Transformer layers from processing the masked nodes, avoiding the gap of using masked nodes in pretraining but not in fine-tuning. Finally, we test decoder's sizes in Table 4. When the encoder is GTS, GTS-Small decoder provides the best performance.

### Rethinking Tokenizer

We investigate the impact of tokenizers on MGM's performance. In the following experiments, the graph autoencoder employs the GTS encoder and the GTS-Small decoder with remask-v2, given their superior performances in the previous section. The results are summarized in Table 2(b).

**Compared tokenizers.** We use the node tokenizer as the baseline. For motif-based tokenizers, we employ the leading fragmentation methods: MGSSL  and RelMole . For GNN-based tokenizers, we compare the prevalent pretraining strategies - GraphCL , GraphMAE , and VQ-VAE [10; 43] - to pretrain tokenizers on the 2 million molecules from ZINC15.

**Finding 3. Reconstructing subgraph-level tokens can give rise to MRL.** Table 2(b) shows that, given the appropriate setup, incorporating motif-based tokenizers or GNN-based tokenizers in MGM can provide better downstream performances than the node tokenizer. This observation underscores the importance of applying a subgraph-level tokenizer in MGM.

**Finding 4. Single-layer SGT outperforms or matches other tokenizers.** Table 2(b) shows that a single-layer SGT, applied to GIN, delivers comparable performance to a four-layer GIN pretrained by VQ-VAE, and surpasses other tokenizers. Further, Figure 6 shows that SGT can transform GCN and GraphSAGE into competitive tokenizers. We attribute SGTs' good performances to the effectiveness of their graph operators in extracting structural information. It has been shown that linear graph operators can effectively summarize structural patterns for node classification [39; 40].

**Finding 5. GNN-based tokenizers have achieved higher performances than motif-based tokenizers.** We hypothesize that this is due to GNN's ability of summarizing structural patterns. When using GNN's representations as reconstruction targets, the distance between targets reflects the similarity between their underlying subgraphs - a nuanced relationship that the one-hot encoding of motifs fails to capture. We leave the potential incorporation of GNNs into motif-based tokenizers for future works.

Finally, we show that although GNN-based tokenizers are agnostic to chemistry knowledge, incorporating them in MGM can improve the recognition of FGs. In Figure 7, we use a linear classifier to probe the encoder's mean pooling output to predict the FGs within the molecule. We use RDkit  to extract 85 types of FG. Details are in Appendix D. It can be seen that, incorporating a single-layer SGT in MGM improves the encoder's ability to identify FGs in comparison to the node tokenizer.

## 5 Comparison to the State-of-the-Art Methods

In this section, we compare SimSGT to the leading molecule SSL methods for molecular property prediction and a broader range of downstream tasks. For fair comparison, we report the performances of SimSGT and its variant that uses the GINE architecture. This variant employs a GINE as encoder and a GINE-Small as decoder (Table 2), and implements remask-v1 decoding.

**Molecular property prediction.** The molecular property prediction experiment follows the same transfer learning setting as Section 3. Table 5 presents the results. It can be observed that SimSGT outperforms all the baselines in average performances with both the GTS and GINE architectures. Notably, SimSGT with GTS establishes a new state-of-the-art of \(75.8\%\) ROC-AUC. It exceeds the second method by an absolute \(1.8\%\) points in average performance and achieves the best performance in five out of the eight molecular property prediction datasets. SimSGT with GINE outperforms the baselines by \(0.4\%\) points in average performance. These improvements demonstrate the effectiveness of our proposed tokenizer and decoder to improve MGM performance for molecules.

Figure 7: Linear probing encoder’s output for FGs.

[MISSING_PAGE_FAIL:9]

and standard deviations across 10 random seeds. The performances are reported in Table 7. We can observe that SimSGT consistently outperforms representative baselines of GraphCL, GraphMAE, and Mole-BERT.

**Computational cost.** We compare the wall-clock pretraining time for SimSGT and key baselines in Table 8. We can observe that: 1) SimSGT's pretraining time is on par with GraphMAE . This efficiency is largely attributed to the minimal computational overhead of our SGT tokenizer; 2) In comparison to Mole-BERT , the prior benchmark in molecule SSL, SimSGT is approximately three times faster. The computational demands of Mole-BERT can be attributed to its combined approach of MGM training and contrastive learning.

## 6 Conclusion and Future Works

In this work, we extensively investigate the roles of tokenizer and decoder in MGM for molecules. We compile and evaluate a comprehensive range of molecule fragmentation functions as molecule tokenizers. The results reveal that a subgraph-level tokenizer gives rise to MRL performance. Further, we show by empirical analysis that a sufficiently expressive decoder with remask decoding improves the molecule encoder's representation quality. In light of these findings, we introduce SimSGT, a novel MGM approach with subgraph-level tokens. SimSGT features a Simple GNN-based Tokenizer capable of transforming GNNs into effective graph tokenizers. It further adopts the GTS architecture for its encoder and decoder, and incorporates a new remask strategy. SimSGT exhibits substantial improvements over existing molecule SSL methods. For future works, the potential application of molecule tokenizers to joint molecule-text modeling , remains an interesting direction.