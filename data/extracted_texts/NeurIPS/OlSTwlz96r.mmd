# Federated Multi-Objective Learning

Haibo Yang

Dept. of Comput. & Info. Sci.

Rochester Institute of Technology

Rochester, NY 14623

hbycis@rit.edu

&Zhuqing Liu

Dept. of ECE

The Ohio State University

Columbus,OH 43210

liu.9384@osu.edu

&Jia Liu

Dept. of ECE

The Ohio State University

Columbus,OH 43210

liu@ece.osu.edu

&Chaosheng Dong

Amazon.com Inc.

Seattle, WA 98109

chaosd@amazon.com

&Michinari Momma

Amazon.com Inc.

Seattle, WA 98109

michi@amazon.com

###### Abstract

In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stochastic multi-gradient descent averaging (FSMGDA). Both algorithms allow local updates to significantly reduce communication costs, while achieving the _same_ convergence rates as those of their algorithmic counterparts in the single-objective federated learning. Our extensive experiments also corroborate the efficacy of our proposed FMOO algorithms.

## 1 Introduction

In recent years, multi-objective optimization (MOO) has emerged as a foundational problem underpinning many multi-agent multi-task learning applications, such as training neural networks for multiple tasks , hydrocarbon production optimization , recommendation system , tissue engineering , and learning-to-rank [5; 6; 7]. MOO aims at optimizing multiple objectives simultaneously, which can be mathematically cast as:

\[_{}():=[f_{1}(), ,f_{S}()],\] (1)

where \(^{d}\) is the model parameter, and \(f_{s}:^{d}\), \(s[S]\) is one of the objective functions. Compared to conventional single-objective optimization, one key difference in MOO is the coupling and potential conflicts between different objective functions. As a result, there may not exist a common \(\)-solution that minimizes all objective functions. Rather, the goal in MOO is to find a _Pareto stationary solution_ that is not improvable for all objectives without sacrificing some objectives. For example, in recommender system designs for e-commerce, the platform needs to consider differentcustomers with substantially conflicting shopping objectives (price, brand preferences, delivery speed, etc.). Therefore, the platform's best interest is often to find a Pareto-stationary solution, where one cannot deviate to favor one consumer group further without hurting any other group. MOO with conflicting objectives also has natural incarnations in many competitive game-theoretic problems, where the goal is to determine an equilibrium among the conflicting agents in the Pareto sense.

Since its inception dating back to the 1950s, MOO algorithm design has evolved into two major categories: gradient-free and gradient-based methods, with the latter garnering increasing attention in the learning community in recent years due to their better performances (see Section 2 for more detailed discussions). However, despite these advances, all existing algorithms in the current MOO literature remain limited to centralized settings (i.e., training data are aggregated and accessible to a centralized learning algorithm). Somewhat ironically, such centralized settings do _not_ satisfy the distributed nature and data privacy needs of many multi-agent multi-task learning applications, which motivates application of MOO in the first place. This gap between the existing MOO approaches and the rapidly growing importance of distributed MOO motivates us to make the first attempt to pursue a new **federated multi-objective learning** (FMOL) framework, with the aim to enable multiple clients to distributively solve MOO problems while keeping their computation and training data private.

So far, however, developing distributed optimization algorithms for FMOL with provable Pareto-stationary convergence remains uncharted territory. There are several key technical challenges that render FMOL far from being a straightforward extension of centralized MOO problems. First of all, due to the distributed nature of FMOL problems, one has to consider and model the _objective heterogeneity_ (i.e., different clients could have different sets of objective functions) that is unseen in centralized MOO. Moreover, with local and private datasets being a defining feature in FMOL, the impacts of _data heterogeneity_ (i.e., datasets are non-i.i.d. distributed across clients) also need to be mitigated in FMOL algorithm design. Last but not least, under the combined influence of objective and data heterogeneity, FMOL algorithms could be extremely sensitive to small perturbations in the determination of common descent direction among all objectives. This makes the FMOL algorithm design and the associated convergence analysis far more complicated than those of the centralized MOO. Toward this end, a fundamental question naturally arises:

_Under both objective and data heterogeneity in FMOL, is it possible to design effective and efficient algorithms with Pareto-stationary convergence guarantees?_

In this paper, we give an affirmative answer to the above question. Our key contribution is that we propose a new FMOL framework that captures both objective and data heterogeneity, based on which we develop two gradient-based algorithms with provable Pareto-stationary convergence rate guarantees. To our knowledge, our work is the first systematic attempt to bridge the gap between federated learning and MOO. Our main results and contributions are summarized as follows:

* [leftmargin=*,noitemsep,topsep=0pt,parsep=0pt,leftmargin=*]
* We formalize the first federated multi-objective learning (FMOL) framework that supports both _objective and data heterogeneity_ across clients, which significantly advances and generalizes the MOO formulation to the federated learning paradigm. As a result, our FMOL framework becomes a generic model that covers existing MOO models and various applications as special cases (see Section 3.2 for further details). This new FMOL framework lays the foundation to enable us to systematically develop FMOO algorithms with provable Pareto-stationary convergence guarantees.
* For the proposed FMOL framework, we first propose a federated multi-gradient descent averaging (FMGDA) algorithm based on the use of local full gradient evaluation at each client. Our analysis reveals that FMGDA achieves a linear \(((- T))\) and a sublinear \((1/T)\) Pareto-stationary convergence rates for \(\)-strongly convex and non-convex settings, respectively. Also, FMGDA employs a two-sided learning rates strategy to significantly lower communication costs (a key concern in the federated learning paradigm). It is worth pointing out that, in the single-machine special case where FMOL degenerates to a centralized MOO problem and FMGDA reduces to the traditional MGD method , our results improve the state-of-the-art analysis of MGD by eliminating the restrictive assumptions on the linear search of learning rate and extra sequence convergence. Thus, our results also advance the state of the art in general MOO theory.
* To alleviate the cost of full gradient evaluation in the large dataset regime, we further propose a federated stochastic multi-gradient descent averaging (FSMGDA) algorithm based on the use of stochastic gradient evaluations at each client. We show that FSMGDA achieves \(}(1/T)\) and \((1/)\) Pareto-stationary convergence rate for \(\)-strongly convex and non-convex settings, respectively. We establish our convergence proof by proposing a new (\(,\))-Lipschitz continuous stochastic gradient assumption (cf. Assumption 4), which relaxes the strong assumptions on first moment bound and Lipschitz continuity on common descent directions in . We note that this new (\(,\))-Lipschitz continuous stochastic gradient assumption can be viewed as a natural extension of the classical Lipschitz-continuous gradient assumption and could be of independent interest.

The rest of the paper is organized as follows. In Section 2, we review related works. In Section 3, we introduce our FMOL framework and two gradient-based algorithms (FMGDA and FSMGDA), which are followed by their convergence analyses in Section 4. We present the numerical results in Section 5 and conclude the work in Section 6. Due to space limitations, we relegate all proofs and some experiments to supplementary material.

## 2 Related work

In this section, we will provide an overview on algorithm designs for MOO and federated learning (FL), thereby placing our work in a comparative perspective to highlight our contributions and novelty.

**1) Multi-objective Optimization (MOO):** As mentioned in Section 1, since federated/distributed MOO has not been studied in the literature, all existing works we review below are centralized MOO algorithms. Roughly speaking, MOO algorithms can be grouped into two main categories. The first line of works are gradient-free methods (e.g., evolutionary MOO algorithms and Bayesian MOO algorithms [10; 11; 12; 13]). These methods are more suitable for small-scale problems but less practical for high-dimensional MOO models (e.g., deep neural networks). The second line of works focus on gradient-based approaches [14; 15; 8; 16; 9; 17], which are more practical for high-dimensional MOO problems. However, while having received increasing attention from the community in recent years, Pareto-stationary convergence analysis of these gradient-based MOO methods remains in its infancy.

Existing gradient-based MOO methods can be further categorized as i) multi-gradient descent (MGD) algorithms with full gradients and ii) stochastic multi-gradient descent (SMGD) algorithms. It has been shown in  that MGD methods achieve \((r^{T})\) for some \(r(0,1)\) and \((1/T)\) Pareto-stationary convergence rates for \(\)-strongly convex and non-convex functions, respectively. However, these results are established under the unconventional linear search of learning rate and sequence convergence assumptions, which are difficult to verify in practice. In comparison, FMGDA achieves a linear rate without needing such assumptions. For SMGD methods, the Pareto-stationary convergence analysis is further complicated by the stochastic gradient noise. Toward this end, an \((1/T)\) rate analysis for SMGD was provided in  based on rather strong assumptions on a first-moment bound and Lipschitz continuity of common descent direction. As a negative result, it was shown in  and  that the common descent direction needed in the SMGD method is likely to be a biased estimation, which may cause divergence issues.

In contrast, our FSMGDA achieves state-of-the-art \(}(1/T)\) and \((1/)\) convergence rates for strongly-convex and non-convex settings, respectively, under a much milder assumption on Lipschitz continuous stochastic gradients. For easy comparisons, we summarize our results and the existing works in Table 1. It is worth noting recent works [18; 19; 20] established faster convergence rates in

   &  &  \\   & Rate & Assumption\({}^{*}\) & Rate & Assumption\({}^{*}\) \\  MGD  & \((r^{T})\) * & Linear search \(\&\) & Linear search \(\&\) \\  & & sequence convergence & & sequence convergence \\  SMGD  & \((1/T)\) & First moment bound \(\&\) Lipschitz & Not provided & Not provided \\  FMMGDA & \(((- T))\)* & Not needed & \((1/T)\) & Not needed \\  FSMGDA & \(}(( T))\) & \((,)\)-Lipschitz continuous stochastic gradient & \((1/)\) & \((,)\)-Lipschitz continuous stochastic gradient \\   \({}^{\#}\) Notes on constants: \(\) is the strong convexity modulus; \(r\) is a constant depends on \(\), \(\), \(r(0,1)\).

\({}^{*}\) Assumption short-hands: “Linear search”: learning rate linear search ; “Sequence convergence”: \(\{_{t}\}\) converges to \(^{*}\); “First moment bound” (Asm. 5.2(b) ): \([\| f(,)- f()\|](a+b\|  f()\|)\);“Lipschitz continuity of \(\)” (Asm. 5.4 ): \(\|_{k}-_{t}\|\|[( f_{1}( _{k})- f_{1}(_{t}))^{T},,( f_{S}( _{k})- f_{S}(_{t}))^{T}]|\); “\((,)\)-Lipschitz continuous stochastic gradient”: see Asm. 4.

Table 1: Convergence rate results (shaded parts are our results) comparisons.

the centralized MOO setting by using acceleration techniques, such as momentum, regularization and bi-level formulation. However, due to different settings and focuses, these results are orthogonal to ours and thus not directly comparable. Also, since acceleration itself is a non-trivial topic and could be quite brittle if not done right, in this paper, we focus on the basic and more robust stochastic gradient approach in FMOL. But for a comprehensive comparison on assumptions and main results of accelerated centralized MOO, we refer readers to Appendix A for further details.

**Federated Learning (FL) :** Since the seminal work by , FL has emerged as a popular distributed learning paradigm. Traditional FL aims at solving single-objective minimization problems with a large number of clients with decentralized data. Recent FL algorithms enjoy both high communication efficiency and good generalization performance [21; 22; 23; 24; 25; 26]. Theoretically, many FL methods have the same convergence rates as their centralized counterparts under different FL settings [27; 28; 29; 30]. Recent works have also considered FL problems with more sophisticated problem structures, such as min-max learning [31; 32], reinforcement learning , multi-armed bandits , and bilevel and compositional optimization . Although not directly related, classic FL has been reformulated in the form of MOO, which allows the use of a MGD-type algorithm instead of vanilla local SGD to solve the standard FL problem. We will show later that this MOO reformulation is a special case of our FMOL framework. So far, despite a wide range of applications (see Section 3.2 for examples), there remains a lack of a general FL framework for MOO. This motivates us to bridge the gap by proposing a general FMOL framework and designing gradient-based methods with provable Pareto-stationary convergence rates.

## 3 Federated multi-objective learning

### Multi-objective optimization: A primer

As mentioned in Section 1, due to potential conflicts among the objective functions in MOO problem in (1), MOO problems adopt the the notion of Pareto optimality:

**Definition 1** ((Weak) Pareto Optimality).: _For any two solutions \(\) and \(\), we say \(\) dominates \(\) if and only if \(f_{s}() f_{s}(), s[S]\) and \(f_{s}()<f_{s}()\), \( s[S]\), A solution \(\) is Pareto optimal if it is not dominated by any other solution. One solution \(\) is weakly Pareto optimal if there does not exist a solution \(\) such that \(f_{s}()>f_{s}(), s[S]\)._

Similar to solving single-objective non-convex optimization problems, finding a Pareto-optimal solution in MOO is NP-Hard in general. As a result, it is often of practical interest to find a solution satisfying Pareto-stationarity (a necessary condition for Pareto optimality) stated as follows [14; 37]:

**Definition 2** (Pareto Stationarity).: _A solution \(\) is said to be Pareto stationary if there is no common descent direction \(^{d}\) such that \( f_{s}()^{}<0, s[S]\)._

Note that for strongly convex functions, Pareto stationary solutions are also Pareto optimal. Following Definition 2, gradient-based MOO algorithms typically search for a common descent direction \(^{d}\) such that \( f_{s}()^{} 0, s[S]\). If no such a common descent direction exists at \(\), then \(\) is a Pareto stationary solution. For example, MGD  searches for an optimal weight \(^{*}\) of gradients \(()\{ f_{s}(), s [S]\}\) by solving \(^{*}()=*{argmin}_{ C }\|^{}()\|^{2}\). Then, a common descent direction can be chosen as: \(=^{}()\). MGD performs the iterative update rule: \(-\) until a Pareto stationary point is reached, where \(\) is a learning rate. SMGD  also follows the same process except for replacing full gradients by stochastic gradients. For MGD and SMGD methods, it is shown in  and  show that if \(\|^{}()\|=0\) for some \( C\), where \(C\{^{S},_{s[S]}y_{s}=1\}\), then \(\) is a Pareto stationary solution. Thus, \(\|\|^{2}=\|^{}()\| ^{2}\) can be used as a metric to measure the convergence of non-convex MOO algorithms [8; 18; 19]. On the other hand, for more tractable strongly convex MOO problems, the optimality gap \(_{s[S]}_{s}[f_{s}()-f_{s}(^{*})]\) is typically used as the metric to measure the convergence of an algorithm , where \(^{*}\) denotes the Pareto optimal point. We summarize and compare different convergence metrics as well as assumptions in MOO, detailed in Appendix A.

### A general federated multi-objective learning framework

With the MOO preliminaries in Section 3.1, we now formalize our general federated multi-objective learning (FMOL) framework. For a system with \(M\) clients and \(S\) tasks (objectives), our FMOL framework can be written as:

\[_{} (^{}),\] (2) \[f_{1,1}&&f_{1,M}\\ &&\\ f_{S,1}&&f_{S,M}_{S M}, a_{1,1}&&a_{1,M}\\ &&\\ a_{S,1}&&a_{S,M}_{S M},\]

where matrix \(\) groups all potential objectives \(f_{s,i}()\) for each task \(s\) at each client \(i\), and \(\{0,1\}^{S M}\) is a _binary_ objective indicator matrix, with each element \(a_{s,i}=1\) if task \(s\) is of client \(i\)'s interest and \(a_{s,i}=0\) otherwise. For each task \(s[S]\), the global objective function \(f_{s}()\) is the average of local objectives over all related clients, i.e., \(f_{s}()|}_{i R_{s}}f_{s,i}()\), where \(R_{s}=\{i:a_{s,i}=1,i[M]\}\). Note that, for notation simplicity, here we use simple average in \(f_{s}()\), which corresponds to the balanced dataset setting. Our FMLO framework can be directly extended to imbalanced dataset settings by using weighted average proportional to dataset sizes of related clients. For a client \(i[M]\), its objectives of interest are \(\{f_{s,i}() a_{s,i}\!=\!1,s[S]\}\), which is a subset of \([S]\).

We note that FMOL generalizes MOO to the FL paradigm, which includes many existing MOO problems as special cases and corresponds to a wide range of applications.

* If each client has only one distinct objective, i.e., \(=_{M}\), \(S=M\), then \((^{})=[f_{1}(),,f_{S}( )]^{}\), where each objective \(f_{s}(),s[S]\) is optimized only by client \(s\). This special FMOL setting corresponds to the conventional multi-task learning and federated learning. Indeed,  and  formulated a multi-task learning problem as MOO and considered Pareto optimal solutions with various trade-offs.  also formulated FL as as distributed MOO problems. Other examples of this setting include bi-objective formulation of offline reinforcement learning  and decentralized MOO .
* If all clients share the same \(S\) objectives, i.e., \(\) is an all-one matrix, then \((^{})=[_{i[M]}f_{1,i} (),\,,_{i[M]}f_{S,i}()]^{}\). In this case, FMOL reduces to federated MOO problems with decentralized data that jointly optimizing fairness, privacy, and accuracy [41; 42; 43], as well as MOO with decentralized data under privacy constraints (e.g., machine reassignment among data centres  and engineering problems [45; 46; 47; 48]).
* If each client has a different subset of objectives (i.e., objective heterogeneity), FMLO allows distinct preferences at each client. For example, each customer group on a recommender system in e-commerce platforms might have different combinations of shopping preferences, such as product price, brand, delivery speed, etc.

### Federated Multi-Objective Learning Algorithms

Upon formalizing our FMOL framework, our next goal is to develop gradient-based algorithms for solving large-scale high-dimensional FMOL problems with _provable_ Pareto stationary convergence guarantees and low communication costs. To this end, we propose two FMOL algorithms, namely federated multiple gradient descent averaging (FMGDA) and federated stochastic multiple gradient descent averaging (FSMGDA) as shown in Algorithm 1. We summarize our key notation in Table 3 in Appendix to allow easy references for readers.

As shown in Algorithm 1, in each communication round \(t[T]\), each client synchronizes its local model with the current global model \(_{t}\) from the server (cf. Step 1). Then each client runs \(K\) local steps based on local data for all effective objectives (cf. Step 2) with two options: i) for FMGDA, each local step performs local full gradient descent, i.e., \(_{s,i}^{t,k+1}=_{s,i}^{t,k}-_{L} f_{s,i}(_{s,i}^{t,k}), s S_{i}\); ii) For FSMGDA, the local step performs stochastic gradient descent, i.e., \(_{s,i+1}^{t,k}=_{s,i}^{t,k}-_{L} f_{s,i}(_{s,i}^{t,k},_{i}^{t,k}), s S_{i}\), where \(_{i}^{t,k}\) denotes a random sample in local step \(k\) and round \(t\) at client \(i\). Upon finishing \(K\) local updates, each client returns the accumulated update \(_{s,i}^{t}\) for each effective objective to the server (cf. Step 3). Then, the server aggregates all returned \(\)-updates fromthe clients to obtain the overall updates \(_{s}^{t}\) for each objective \(s[S]\) (cf. Steps 4 and 5), which will be used in solving a convex quadratic optimization problem with linear constraints to obtain an approximate common descent direction \(_{t}\) (cf. Step 6). Lastly, the global model is updated following the direction \(_{t}\) with global learning rate \(_{t}\) (cf. Step 7).

Two remarks on Algorithm 1 are in order. First, we note that a two-sided learning rates strategy is used in Algorithm 1, which decouples the update schedules of local and global model parameters at clients and server, respectively. As shown in Section 4 later, this two-sided learning rates strategy enables better convergence rates by choosing appropriate learning rates. Second, to achieve low communication costs, Algorithm 1 leverages \(K\) local updates at each client and infrequent periodic communications between each client and the server. By adjusting the two-sided learning rates appropriately, the \(K\)-value can be made large to further reduce communication costs.

``` At Each Client \(i\):
1. Synchronize local models \(_{s,i}^{t,0}=_{t}, s S_{i}\).
2. Local updates: for all \(s S_{i}\), for \(k=1,,K\), (FMGDA): \(_{s,i}^{t,k}=_{s,i}^{t,k-1}-_{L} f_{s,i}( _{s,i}^{t,k-1})\), (FSMGDA): \(_{s,i}^{t,k}=_{s,i}^{t,k-1}-_{L} f_{s,i}( _{s,i}^{t,k-1},_{i}^{t,k})\).
3. Return accumulated updates to server \(\{_{s,i}^{t},s S_{i}\}\): (FMGDA): \(_{s,i}^{t}=_{k[K]} f_{s,i}(_{s,i}^{t,k})\), (FSMGDA): \(_{s,i}^{t}=_{k[K]} f_{s,i}(_{s,i}^{t,k},_{i}^ {t,k})\). At the Server:
4. Receive accumulated updates \(\{_{s,i}^{t}, s\!\!S_{i}, i\!\![M]\}\).
5. Compute \(_{s}^{t}=|}_{i R_{s}}_{s,i}^{t}, s [S]\), where \(R_{s}=\{i:a_{s,i}=1,i[M]\}\).
6. Compute \(_{t}^{*}^{S}\) by solving \[_{_{t} 0}\|_{s[S]} _{s}^{t}_{s}^{t}\|^{2},_{s [S]}_{s}^{t}=1.\] (3)
7. Let \(_{t}=_{s[S]}_{s}^{t,*}_{s}^{t}\) and update the global model as: \(_{t+1}=_{t}-_{t}_{t}\), with a global learning rate \(_{t}\). ```

**Algorithm 1** Federated (Stochastic) Multiple Gradient Descent Averaging (FMGDA/FSMGDA).

## 4 Pareto stationary convergence analysis

In this section, we analyze the Pareto stationary convergence performance for our FMGDA and FSMGDA algorithms in Sections 4.1 and 4.2, respectively, each of which include non-convex and strongly convex settings.

### Pareto stationary convergence of FMGDA

In what follows, we show FMGDA enjoys linear rate \(}((- T))\) for \(\)-strongly convex functions and sub-linear rate \(()\) for non-convex functions.

**1) FMGDA: The Non-convex Setting.** Before presenting our Pareto stationary convergence results for FMGDA, we first state several assumptions as follows:

**Assumption 1**.: _(L-Lipschitz continuous) There exists a constant \(L>0\) such that \(\| f_{s}()- f_{s}()\| L\|- \|,,^{d},s[S]\)._

**Assumption 2**.: _(Bounded Gradient) The gradient of each objective at any client is bounded, i.e., there exists a constant \(G>0\) such that \(\| f_{s,i}()\|^{2} G^{2}, s[S],i[M]\)._

With the assumptions above, we state the Pareto stationary convergence of FMGDA for non-convex FMOL as follows:

**Theorem 1** (FMGDA for Non-convex FMOL).: _Let \(_{t}=\). Under Assumptions 1 and 2, if at least one function \(f_{s},s[S]\) is bounded from below by \(f_{s}^{}\), then the sequence \(\{_{t}\}\) output by FMGDA satisfies: \(_{t[T]}\|}_{t}\|^{2}^{2}-f_{s}^{} )}{T}+\), where \(^{2}K^{2}L^{2}G^{2}(1+S^{2})}{}\)._

In non-convex functions, we use \(\|}_{t}\|^{2}\) as the metrics for FMOO, where \(}_{t}=_{t}^{T}((^{}))\) and \(_{t}\) is calculated by the quadratic programming problem 3 based on accumulated (stochastic) gradients \(_{t}\). We compare different metrics for MOO in Appendix A. The convergence bound in Theorem 1 contains two parts. The first part is an optimization error, which depends on the initial point and vanishes as \(T\) increases. The second part is due to local update steps \(K\) and data heterogeneity \(G\), which can be mitigated by carefully choosing the local learning rate \(_{L}\). Specifically, the following Pareto stationary convergence rate of FMGDA follows immediately from Theorem 1 with an appropriate choice of local learning rate \(_{L}\):

**Corollary 2**.: _With a constant global learning rate \(_{t}=\), \( t\), and a local learning rate \(_{L}=(1/)\), the Pareto stationary convergence rate of FMGDA is \((1/T)_{t[T]}\|}_{t}\|^{2}=(1/T)\)._

Several interesting insights of Theorem 1 and Corollary 2 are worth pointing out: **1)** We note that FMGDA achieves a Pareto stationary convergence rate \((1/T)\) for non-convex FMOL, which is the _same_ as the Pareto stationary rate of MGD for centralized MOO and the _same_ convergence rate of gradient descent (GD) for single objective problems. This is somewhat surprising because FMGDA needs to handle more complex objective and data heterogeneity under FMOL; **2)** The two-sided learning rates strategy decouples the operation of clients and server by utilizing different learning rate schedules, thus better controlling the errors from local updates due to data heterogeneity; **3)** Note that in the single-client special case, FMGDA degenerates to the basic MGD algorithm. Hence, Theorem 1 directly implies a Pareto stationary convergence bound for MGD by setting \(=0\) due to no local updates in centralized MOO. This convergence rate bound is consistent with that in . However, we note that our result is achieved _without_ using the linear search step for learning rate , which is much easier to implement in practice (especially for deep learning models); **4)** Our proof is based on standard assumptions in first-order optimization, while previous works require strong and unconventional assumptions. For example, a convergence of \(\)-sequence is assumed in .

**2) FMGDA: The Strongly Convex Setting.** Now, we consider the strongly convex setting for FMOL, which is more tractable but still of interest in many learning problems in practice. In the strongly convex setting, we have the following additional assumption:

**Assumption 3**.: _(\(\)-Strongly Convex Function) Each objective \(f_{s}(),s[S]\) is a \(\)-strongly convex function, i.e., \(f_{s}() f_{s}()+ f_{s}()(- )+\|-\|^{2}\) for some \(>0\)._

For more tractable strongly-convex FMOL problems, we show that FMGDA achieves a stronger Pareto stationary convergence performance as follows:

**Theorem 3** (FMGDA for \(\)-Strongly Convex FMOL).: _Let \(_{t}=\) such that \(\), \(\) and \(}\). Under Assumptions 1- 3, pick \(_{t}\) as the final output of the FMGDA algorithm with weights \(w_{t}=(1-)^{1-t}\). Then, it holds that \([_{Q}^{t}]\|_{0}-_{*}\|^{2}(- )+\), where \(_{Q}^{t}_{s[S]}_{s}^{t,*}[f_{s}( _{t})-f_{s}(_{*})]\) and \(=^{2}K^{2}L^{2}G^{2}S^{2}}{}+2_{L}^{2}K^{2}L^{2}G^ {2}\)._

Theorem 3 immediately implies following Pareto stationary convergence rate for FMGDA with a proper choice of local learning rate:

**Corollary 4**.: _If \(_{L}\) is chosen sufficiently small such that \(=((- T))\), then the Pareto stationary convergence rate of FMGDA is \([_{Q}^{t}]=((- T))\)._

Again, several interesting insights can be drawn from Theorem 3 and Corollary 4. First, for strongly convex FMOL, FMGDA achieves a linear convergence rate \(((- T))\), which again matches those of MGD for centralized MOO and GD for single-objective problems. Second, compared with the non-convex case, the convergence bounds suggest FMGDA could use a larger local learning rate for non-convex functions thanks to our two-sided learning rates design. A novel feature of FMGDA for strongly convex FMOL is the randomly chosen output \(x_{t}\) with weight \(w_{t}\) from the \(_{t}\)-trajectory, which is inspired by the classical work in stochastic gradient descent (SGD) . Note that, for implementation in practice, one does not need to store all \(_{t}\)-values. Instead, the algorithm can be implemented by using a random clock for stopping .

### Pareto stationary convergence of FSMGDA

While enjoying strong performances, FMGDA uses local full gradients at each client, which could be costly in the large dataset regime. Thus, it is of theoretical and practical importance to consider the stochastic version of FMGDA, i.e., federated stochastic multi-gradient descent averaging (FSMGDA).

**1) FSMGDA: The Non-convex Setting.** A fundamental challenge in analyzing the Pareto stationarity convergence of FSMGDA and other stochastic multi-gradient descent (SMGD) methods stems from bounding the error of the common descent direction estimation, which is affected by both \(_{t}^{*}\) (obtained by solving a quadratic programming problem) and the stochastic gradient variance. In fact, it is shown in  and  that the stochastic common descent direction in SMGD-type methods could be biased, leading to divergence issues. To address these challenges, in this paper, we propose to use a _new_ assumption on the stochastic gradients, which is stated as follows:

**Assumption 4** (\((,)\)-Lipschitz Continuous Stochastic Gradient).: _A function \(f\) has (\(,\))-Lipschitz continuous stochastic gradients if there exist two constants \(,>0\) such that, for any two independent training samples \(\) and \(^{{}^{}}\), \([\| f(,)- f(,^{{}^{}}) \|^{2}]\|-\|^{2}+^{2}\)._

In plain language, Assumption 4 says that the stochastic gradient estimation of an objective does not change too rapidly. We note that the (\(,\))-Lipschitz continuous stochastic gradient assumption is a natural extension of the classic \(L\)-Lipschitz continuous gradient assumption (cf. Assumption 1) and generalizes several assumptions of SMGD convergence analysis in previous works. We note that Assumption 1 is not necessarily too hard to satisfy in practice. For example, when the underlying distribution of training samples \(\) has a bounded support (typically a safe assumption for most applications in practice due to the finite representation limit of computing systems), suppose that Assumption 1 holds (also a common assumption in the optimization literature), then for any given \(\) and \(\), the left-hand-side of the inequality in Assumption 4 is bounded due to the L-smoothness in Assumption 1. In this case, there always exist a sufficiently large \(\) and a \(\) such that the right-hand-side of the inequality in Assumption 1 holds. Please see Appendix A for further details. In addition, we need the following assumptions for the stochastic gradients, which are commonly used in standard SGD-based analyses [49; 50; 51; 52].

**Assumption 5**.: _(Unbiased Stochastic Estimation) The stochastic gradient estimation is unbiased for each objective among clients, i.e., \([ f_{s,i}(,)]= f_{s,i}(), s [S],i[M]\)._

**Assumption 6**.: _(Bounded Stochastic Gradient) The stochastic gradients satisfy \([\| f_{s,i}(,)\|^{2}] D^{2}, s[S], i[M]\) for some constant \(D>0\)._

With the assumptions above, we now state the Pareto stationarity convergence of FSMGDA as follows:

**Theorem 5** (FSMGDA for Non-convex FMOL).: _Let \(_{t}=\). Under Assumptions 4-6, if an objective \(f_{s}\) is bounded from below by \(f_{s}^{}\), then the sequence \(\{_{t}\}\) output by FSMGDA satisfies: \(_{t[T]}\|}_{t}\|^{2}^{0}-f_{s}^{})}{^{T}}+\), where \(=(2S^{2}+4)(_{L}^{2}K^{2}D^{2}+^{2})\)._

Theorem 5 immediately implies an \((1/)\) convergence rate of FSMGDA for non-convex FMOL:

**Corollary 6**.: _With a constant global learning rate \(_{t}==(1/)\), \( t\) and a local learning rate \(_{L}=(1/T^{1/4})\), and if \(=()\), the Pareto stationarity convergence rate of FSMGDA is \(_{t[T]}\|}_{t}\|^{2}=(1/)\)._

**2) The Strongly Convex Setting:** For more tractable strongly convex FMOL problems, we can show that FSMGDA achieve stronger convergence results as follows:

**Theorem 7** (FSMGDA for \(\)-Strongly Convex FMOL).: _Let \(_{t}==()\). Under Assumptions 3, 5 and 6, pick \(_{t}\) as the final output of the FSMGDA algorithm with weight \(w_{t}=(1-)^{1-t}\). Then, it holds that: \([_{0}^{t}]\|_{0}-_{}\|^{2} (- T)+\), where \(_{Q}^{t}=_{s[S]}_{s}^{t,*}[f_{s}(_{t})-f_{ s}(_{})]\) and \(=S^{2}(_{L}^{2}K^{2}D^{2}+^{2})+D^{2}}{2}\)._

The following Pareto station convergence rate of FSMGDA follows immediately from Theorem 7:

**Corollary 8**.: _Choose \(_{L}=(})\) and \(=(T))}{ T})\). If \(=()\), then the Pareto stationary convergence rate of FSMGDA is \([_{Q}^{t}]}(1/T)\)._Corollary 8 says that, With proper learning rates, FSMGDA achieves \(}(1/T)\) Pareto stationary convergence rate (i.e., ignoring logarithmic factors) for strongly convex FMOL. Also, in the single-client special case with no local updates, FSMGDA reduces to the SMGD algorithm and \(= S^{2}^{2}+D^{2}}{2}\) in this case. Then, Theorem 7 implies an \(}()\) Pareto stationarity convergence rate for SMGD for strongly convex MOO problems, which is consistent with previous works . However, our convergence rate proof uses a more conventional \((,)\)-Lipschitz stochastic gradient assumption, rather than the unconventional assumptions on the first moment bound and Lipschitz continuity of common descent directions in .

## 5 Numerical results

In this section, we show the main numerical experiments of our FMGDA and FSMGDA algorithms in different datasets, while relegating the experimental settings and details to the appendix.

**1) Ablation Experiments on Two-Tasks FMOL: _1-a) Impacts of Batch Size on Convergence:_** First, we compare the convergence results in terms of the number of communication rounds using the "MultiMNIST" dataset  with two tasks (L and R) as objectives. We test our algorithms with four different cases with batch sizes being \(\). To reduce computational costs in this experiment, the dataset size for each client is limited to \(256\). Hence, the batch size \(256\) corresponds to FMGDA and all other batch sizes correspond to FSMGDA. As shown in Fig. 1(a), under non-i.i.d. data partition, both FMGDA and FSMGDA algorithms converge. Also, the convergence speed of the FSMGDA algorithm increases as the batch size gets larger. These results are consistent with our theoretical analyses as outlined in Theorems 1 and 5.

_1-b) Impacts of Local Update Steps on Convergence:_ Next, we evaluate our algorithms with different numbers of local update steps \(K\). As shown in Fig. 1(b) and Table 2, both algorithms converge faster as the number of the local steps \(K\) increases. This is because both algorithms effectively run more iterative updates as \(K\) gets large.

_1-c) Comparisons between FMOL and Centralized MOO:_ Since this work is the first that investigates FMOL, it is also interesting to empirically compare the differences between FMOL and centralized MOO methods. In Fig. 2(a), we compare the training loss of FMGDA and FSMGDA with those of the centralized MGD and SMGD methods after 100 communication rounds. For fair comparisons, the centralized MGD and SMGD methods use \(_{i}^{M}|S_{i}|\) batch-sizes and run \(K T\) iterations. Our results indicate that FMGDA and MGD produce similar results, while the performance of FSMGDA is slightly worse than that of SMGD due to FSMGDA's sensitivity to objective and data heterogeneity in stochastic settings. These numerical results confirm our theoretical convergence analysis.

   &  &  \\   & Task L & Task R & Task L & Task R \\  \(K=1\) & 82 & 84 & 96 & 82 \\  \(K=5\) & 18(4.6\(\)) & 20(4.2\(\)) & 24(4.0\(\)) & 20(4.1\(\)) \\  \(K=10\) & 10(8.2\(\)) & 9(9.3\(\)) & 13(7.4\(\)) & 10(8.2\(\)) \\  \(K=20\) & 5(16.4\(\)) & 5(16.8\(\)) & 6(16.0\(\)) & 5(16.4\(\)) \\  

Table 2: Communication rounds needed for \(10^{-2}\) loss.

Figure 1: Training loss convergence comparison.

**2) Experiments on Larger FMOL:** We further test our algorithms on FMOL problems of larger sizes. In this experiment, we use the River Flow dataset, which contains _eight_ tasks in this problem. To better visualize 8 different tasks, we illustrate the normalized loss in radar charts in Fig. 2(b). In this 8-task setting, we can again verify that more local steps \(K\) and a larger training batch size lead to faster convergence. In the appendix, we also verify the effectiveness of our FMGDA and FSMGDA algorithms in CelebA  (_40 tasks_), alongside with other hyperparmeter tuning results.

## 6 Conclusion and discussions

In this paper, we proposed the first general framework to extend multi-objective optimization to the federated learning paradigm, which considers both objective and data heterogeneity. We showed that, even under objective and data heterogeneity, both of our proposed algorithms enjoy the same Pareto stationary convergence rate as their centralized counterparts. In our future work, we will go beyond the limitation in the analysis of MOO that an extra assumption on the stochastic gradients (and \(\)). In this paper, we have proposed a weaker assumption (Assumption 4). We conjecture that using acceleration techniques, e.g., momentum, variance reduction, and regularization, could relax such assumption and achieve better convergence rate, which is a promising direction for future works. In addition, MOO in distributed learning gives rise to substantially expensive communication costs, which scales linearly with the number of clients and the number of objectives in each client. Developing communication-efficient MOO beyond typical gradient compression methods for distributed learning is also a promising direction for future works.