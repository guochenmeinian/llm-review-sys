# Understanding and Improving Ensemble Adversarial Defense

Yian Deng

Department of Computer Science

The University of Manchester

Manchester, UK, M13 9PL

yian.deng@manchester.ac.uk

&Tingting Mu

Department of Computer Science

The University of Manchester

Manchester, UK, M13 9PL

tingting.mu@manchester.ac.uk

###### Abstract

The strategy of ensemble has become popular in adversarial defense, which trains multiple base classifiers to defend against adversarial attacks in a cooperative manner. Despite the empirical success, theoretical explanations on why an ensemble of adversarially trained classifiers is more robust than single ones remain unclear. To fill in this gap, we develop a new error theory dedicated to understanding ensemble adversarial defense, demonstrating a provable 0-1 loss reduction on challenging sample sets in adversarial defense scenarios. Guided by this theory, we propose an effective approach to improve ensemble adversarial defense, named interactive global adversarial training (iGAT). The proposal includes (1) a probabilistic distributing rule that selectively allocates to different base classifiers adversarial examples that are globally challenging to the ensemble, and (2) a regularization term to rescue the severest weaknesses of the base classifiers. Being tested over various existing ensemble adversarial defense techniques, iGAT is capable of boosting their performance by up to \(17\%\) evaluated using CIFAR10 and CIFAR100 datasets under both white-box and black-box attacks.

## 1 Introduction

Many contemporary machine learning models, particularly the end-to-end ones based on deep neural networks, admit vulnerabilities to small perturbations in the input feature space. For instance, in computer vision applications, a minor change of image pixels computed by an algorithm can manipulate the classification results to produce undesired predictions, but these pixel changes can be imperceptible to human eyes. Such malicious perturbations are referred to as adversarial attacks. These can result in severe incidents, e.g., medical misdiagnose caused by unauthorized perturbations in medical imaging , and wrong actions taken by autonomous vehicles caused by crafted traffic images .

The capability of a machine learning model to defend adversarial attacks is referred to as adversarial robustness. A formal way to quantify such robustness is through an adversarial risk, which can be intuitively understood as the expectation of the worst-scenario loss computed within a local neighborhood region around a naturally sampled data example . It is formulated as below:

\[R_{adv}()=_{(,y)}[_{ ()}((( )),y)],\] (1)

where \(()\) denotes a local neighborhood region around the example \(\) sampled from a natural data distribution \(\). The neighbourhood is usually defined based on a selected norm, e.g., a region containing any \(\) satisfying \(\|-\|\) for a constant \(>0\). In general, \((,y)\) can be any loss function quantifying the difference between the predicted output \(\) and the ground-truth output \(y\). Alearning process that minimizes the adversarial risk is referred to as adversarially robust learning. In the classification context, a way to empirically approximate the adversarial robustness is through a classification error computed using a set of adversarial examples generated by applying adversarial attacks to the model . Here, an adversarial attack refers to an algorithm that usually builds on an optimization strategy, and it produces examples perturbed by a certain strength so that a machine learning model returns the most erroneous output for these examples [10; 24].

On the defense side, there is a rich amount of techniques proposed to improve the model robustness [10; 1]. However, a robust neural network enhanced by a defense technique would mostly result in a reduced accuracy for classifying the natural examples . A relation between the adversarial robustness and the standard accuracy has been formally proved by Tsipras et al. : _"Any classifier that attains at least \(1-\) standard accuracy on a dataset \(\) has a robust accuracy at most \( p/(1-p)\) against an \(L_{}\)-bounded adversary with \( 2\)."_, where \(\) is the \(L_{}\) bound indicating the attack strength, while \(\) is sufficiently large and \(p 0.5\) and they both are used for data generation. This statement presents a tradeoff between the adversarial robustness and natural accuracy. There has been a consistent effort invested to mitigate such tradeoff by developing defense techniques to improve robust accuracy without sacrificing much the natural accuracy.

Ensemble defense has recently arisen as a new state of the art [4; 25]. The core idea is to train strategically multiple base classifiers to defend the attack, with the underlying motivation of improving the statistical stability and cooperation between base models. Existing effort on ensemble defense has mostly been focused on demonstrating performance success for different algorithmic approaches. It is assumed that _training and combining multiple base models can defend better adversarial attacks as compared to training a single model_. Although being supported by empirical success, there is few research that provides rigorous understanding to why this is the case in general. Existing results on analyzing generalized error for ensemble estimators mostly compare the error of the ensemble and the averaged error of the base models, through, for instance, decomposition strategies that divide the error term into bias, variance, co-variance, noise and/or diversity terms [45; 7; 39]. It is not straightforward to extend such results to compare the error of an ensemble model and the error of a model without an ensemble structure.

To address this gap, we develop a new error theory (Theorem 4.1) dedicated to understanding ensemble adversarial defense. The main challenge in mitigating the tradeoff between the adversarial robustness and natural accuracy comes from the fact that adversarial defence techniques can reduce the classifier's capacity of handling weakly separable example pairs that are close to each other but from different classes. To analyse how ensemble helps address this particular challenge, we derive a provable error reduction by changing from using one neural network to an ensemble of two neural networks through either the average or max combiner of their prediction outputs.

Although ensemble defense can improve the overall adversarial robustness, its base models can still be fooled individually in some input subspaces, due to the nature of the collaborative design. Another contribution we make is the proposal of a simple but effective way to improve each base model by considering the adversarial examples generated by their ensemble accompanied by a regularization term that is designed to recuse the worst base model. We experiment with the proposed enhancement by applying it to improve four state-of-the-art ensemble adversarial defense techniques. Satisfactory performance improvement has been observed when being evaluated using CIFAR-10 and CIFAR-100 datasets  under both white-box and black-box attacks.

## 2 Related Work

**Adversarial Attack:** Typical attack techniques include white-box and black-box attacks. The white-box adversary has access to the model, e.g., the model parameters, gradients and formulation, etc, while the black-box adversary has limited knowledge of the model, e.g., knowing only the model output, and therefore is closer to the real-world attack setting . Representative white-box attacks include Deepfool  and the projected gradient descent (PGD) . The fast gradient sign method (FGSM)  is a simplified one-iteration version of PGD. The momentum iterative method (MIM)  improves the FGSM attack by introducing the momentum term. Another two commonly-used and cutting-edge white-box attacks are the Carlini-Wagner (CW) attack  that computes the perturbation by optimizing the perturbation scales, and the Jacobian-based saliency map attack (JSMA)  that perturbs only one pixel in each iteration. Representative black-box attacks include the square attack (SA) , the SignHunter attack  and the simple black-box attack (SimBA) , among which SA can generate stronger attacks but is more computationally expensive. A thorough survey on adversarial attacks is provided by Akhtar et al. . AutoAttack  encapsulates a selected set of strong white and black-box attacks and is considered as the state-of-the-art attack tool. In general, when evaluating the adversarial robustness of a deep learning model, it should be tested under both the white and black-box attacks.

**Adversarial Defense:** Adversarial training is the most straightforward and commonly used defense technique for improving adversarial robustness. It works by simply expanding the training data with additional adversarial examples. The initial idea was firstly adopted by Christian et al.  to train robust neural networks for classification, later on, Madry et al.  used adversarial examples generated by the PGD attack for improvement. Overall, this strategy is very adaptive and can be used to defend any attack, but it is on the expense of consuming more training examples. It has been empirically observed that adversarial training can reduce loss curvatures in both the input and model parameter spaces [17; 16], and this reduces the adversarial robustness gap between the training and testing data [52; 47]. These findings motivate the development of a series of regularization techniques for adversarial defense, by explicitly regularizing the curvature or other relevant geometric characteristics of the loss function. Many of these techniques can avoid augmenting the training set and are computationally cheaper. Typical regularization techniques that attempt to flatten the loss surface in the input space include the curvature regularization (CURE) , local linearity regularization (LLR) , input gradient regularization  and Lipschitz regularization . There are also techniques to flatten the loss in the model parameter space, such as TRADES , misclassification aware adversarial training (MART) , robust self-training (RST) [9; 33], adversarial weight perturbation (AWP) , and HAT , etc. Alternative to adversarial training and regularization, other defense strategies include pruning , pre-training , feature denoising , domain adaptation  and ensemble defense [25; 37], etc. Croce et al.  has provided a summary of recent advances.

**Ensemble Adversarial Defense:** Recently, ensemble has been actively used in adversarial defense, showing promising results. One core spirit behind ensemble is to encourage diversity between base models in order to achieve an improved ensemble prediction [45; 7; 39]. Therefore, the advances are mostly focused on designing effective ensemble diversity losses in training to improve adversarial robustness. For instance, the adaptive diversity promoting (ADP) method  uses Shannon entropy for uncertainty regularization and a geometric diversity for measuring the difference between the predictions made by base classifiers. The transferability reduced smooth (TRS) method  formulates the diversity term based on cosine similarities between the loss gradients of base models, meanwhile increases the model smoothness via an \(l_{2}\)-regularization of these gradients during training. The similar idea of exploiting loss gradients was proposed earlier in the gradient alignment loss (GAL) method . The conditional label dependency learning (CLDL) method  is a latest improvement over GAL and TRS, which measures the diversity using both the predictions and loss gradients of the base models. However, the ensemble nature of encouraging diversity can cause vulnerability for some base models over certain subsets of adversarial examples . In practice, this can limit the overall robustness of the ensemble when the other base models are not strong enough to correct the weak ones. To address this, the diversifying vulnerabilities for enhanced robust generation of ensembles (DVERGE)  proposes a vulnerability diversity to encourage each base model to be robust particularly to the other base models' weaknesses. The latest development for improving ensemble defense, known as synergy-of-experts (SoE) , follows a different research path. For each input, it adaptively selects a base model with the largest confidence to make the final prediction instead of combining all, for which the supporting algorithm and theory have been developed. Some surveys on ensemble adversarial attacks and defense can be found in He et al. , Lu et al. .

## 3 Notations and Preliminaries

Bold capital and lower-case letters, e.g., \(\) and \(\), denote matrices and vectors, respectively, while lower-case letters, e.g., \(x\), denote scalars. The \(i\)-th row and column of a matrix \(\) are denoted by \(_{i}\) and \(^{(i)}\), respectively, while \(x_{i,j}\) and \(x_{i}\) the elements of \(\) and \(\). A classification dataset \(D=\{(_{i},y_{i})\}_{i=1}^{n}\) includes \(n\) examples, which are referred to as _natural examples_, with \(_{i}^{d}\) (feature vector) and \(y_{i}[C]=\{1,2,,C\}\) (class label). We sometimes express the label of an example as \(y()\) or \(y_{}\). Storing \(_{i}\) as a row of \(^{n d}\) and \(y_{i}\) an element of \(^{n}\), we also denote this dataset by \(D=(,)\). The classifier \(:^{C}\) outputs class probabilitiesusually computed by a softmax function. Given the computed probability \(f_{c}\) for the \(c\)-th class, \(_{}()=_{[C]}f_{c}()\) predicts the class. For a neural network, we denote by \(^{(l)}\) the weight matrix connecting the \(l\)-th and the \((l-1)\)-th layers and by \(w^{(l)}_{i,j}\) its \(ij\)-th element. The \(L_{2}\)-norm \(\|\|_{2}\) is used to compute the vector length, while the \(L_{}\)-norm \(\|\|_{}\) to generate adversarial attacks. Concatenation of two sets is denoted by the symbol \(\).

We focus on classification by minimizing a classification loss \(((),y_{})\), and adapt it to \(((),)\) for the whole dataset. Also, we use \(_{CE}\) to emphasize the cross-entropy loss. The loss gradient is \(((),y_{})=(),y_{})}{}\). A cheap way to estimate the loss curvature is by finite difference approximation , e.g., the following curvature measure based on \(L_{2}\)-norm:

\[_{}(,)=(+),y_{})- ((),y_{})\|_{2}}{\|\|_{2 }},\] (2)

where \(^{d}\) is a perturbation. It measures how a surface bends at a point by different amounts in different directions. An adversarial example \(}=(,,A)\) is generated by attacking the classifier \(\) using an attack algorithm \(A\) on a natural example \(\). It is further adapted to \(}=(,,A)\) for the set of adversarial examples each generated from a natural example in \(\). The quantity \((,,A)\)\(=(,,A)-\) is referred to as the _adversarial perturbation_ of \(\), simplified to \(_{}=}-\). To control the perturbation strength, we restrict \(\|_{}\|_{}\) for some \(>0\), which results in the following adversarial example formulation, as

\[_{}(,,A)=(( {}(,,A),-),+ ),\] (3)

where both \((,)\) and \((,)\) are element-wise operators comparing their inputs.

## 4 An Error Theory for Adversarial Ensemble Defense

In adversarial ensemble defense, a widely accepted research hypothesis is that training and combining multiple base classifiers can improve adversarial defense as compared to training a single classifier. However, this hypothesis is mostly supported by empirical successes and there is a lack of formal theoretical justification. In this work, we seek theoretical evidence, proving that, when using multi-layer perceptrons (MLPs) for classification, classification error reduces when applying adversarial defence to the base MLPs of an ensemble as compared to a single MLP, under assumptions feasible in practice. The following theorem formalizes our main result.

**Theorem 4.1**.: _Suppose \(,^{0},^{1}:^{C}\) are \(C\)-class \(L\)-layer MLPs satisfying Assumption 4.2. Given a dataset \(D=\{(_{i},y_{i})\}_{i=1}^{n}\), construct an ambiguous pair set \(A(D)\) by Definition 4.3. Assume \(,^{0},^{1}\) are acceptable classifiers for \(A(D)\) by Assumption 4.4. Given a classifier \(:^{C}\) and a dataset \(D\), assess its classification error by 0-1 loss, as_

\[}_{0/1}(D,)=_{ D}1 [f_{y_{}}()<_{c y_{}}f_{c}()],\] (4)

_where \(1[]=1\) while \(1[]=0\). For an ensemble \(_{c}^{(0,1)}\) of two base MLPs \(^{0}\) and \(^{1}\) through either an average or a max combiner, i.e., \(_{c}^{(0,1)}=(^{0}+^{1})\) or \(_{c}^{(0,1)}=(^{0},^{1})\), it has a lower empirical 0-1 loss than a single MLP for classifying ambiguous examples, such as_

\[_{a A(D)}_{^{0},^{1}} [}_{0/1}(a,_{c}^{(0,1)})]< _{a A(D)}_{}[}_{0/1}(a,)].\] (5)

We prove the result for MLPs satisfying the following assumption.

**Assumption 4.2** (MLP Requirement).: Suppose a \(C\)-class \(L\)-layer MLP \(:^{d}^{C}\) expressed iteratively by

\[^{(0)}() =,\] (6) \[^{(l)}() =(^{(l)}^{(l-1)}()), l=1,2,...,L-1,\] (7) \[^{(L)}() =^{(L)}^{(L-1)}()=(),\] (8) \[() =(()),\] (9)where \(()\) is the activation function applied element-wise, the representation vector \(()^{C}\) returned by the \(L\)-th layer is fed into the prediction layer building upon the softmax function. Let \(w_{s_{l+1},s_{1}}^{(l)}\) denote the network weight connecting the \(s_{l}\)-th neuron in the \(l\)-th layer and the \(s_{l+1}\)-th neuron in the \((l+1)\)-th layer for \(l\{1,2,L\}\). Define a column vector \(^{(k)}\) with its \(i\)-th element computed from the neural network weights and activation derivatives, as \(p_{i}^{(k)}=_{s_{L}}^{(L-1)}()}{ x_{ k}}w_{i,s_{L}}^{(L)}\) for \(k=1,2, d\) and \(i=1,2, C\), also a matrix \(_{}=_{k=1}^{d}^{(k)}^{(k)^{T}}\) and its factorization \(_{}=_{}_{}^{T}\) with a full-rank factor matrix \(_{}\). For constants \(,B>0\), suppose the following holds for \(\):

1. Its cross-entropy loss curvature measured by Eq. (2) satisfies \(_{}(,)\).
2. The factor matrix satisfies \(\|_{}\|_{2} B_{0}\) and \(\|_{}^{}\|_{2} B\), where \(\|\|_{2}\) denotes the vector induced \(l_{2}\)-norm for matrix.

We explain the feasibility of the above MLP assumptions in the end of this section.

Although adversarial defense techniques can improve adversarial robustness, new challenges arise in classifying examples that are close to each other but from different classes, due to the flattened loss curvature for reducing the adversarial risk. We refer to a pair of such challenging examples as an _ambiguous pair_. Our strategy of proving improved performance for adversarial defense is to (1) firstly construct a challenging dataset \(A(D)\) comprising samples from these pairs, which is referred to as an _ambiguous pair set_, and then (2) prove error reduction over \(A(D)\). To start, we provide formal definitions for the ambiguous pair and set.

**Definition 4.3** (Ambiguous Pair).: Given a dataset \(D=\{(_{i},y_{i})\}_{i=1}^{n}\) where \(_{i}\) and \(y_{i}[C]\), an _ambiguous pair_ contains two examples \(a=((_{i},y_{i}),(_{j},y_{j}))\) satisfying \(y_{i} y_{j}\) and

\[\|_{i}-_{j}\|_{2}^{2}-)}},\] (10)

where \(J>2\) is an adjustable control variable, \(\), \(B\) and \(^{2}\) are constants associated with the MLP under Assumption 4.2. The _ambiguous pair set_\(A(D)\) contains all the ambiguous pairs existing in \(D\), for which \(J\) is adjusted such that \(A(D)\).

In Theorem 4.1, we are only interested in classifiers that do not fail too badly on \(A(D)\), e.g., having an accuracy level above \(42.5\%\). Comparing poorly performed classifiers is not very meaningful, also the studied situation is closer to practical setups where the starting classifiers for improvement are somewhat acceptable. Such a preference is formalized by the following assumption:

**Assumption 4.4** (**Acceptable Classifier)**.: Suppose an acceptable classifier \(:^{d}^{C}\) does not perform poorly on the ambiguous pair set \(A(D)\) associated with a control variable \(J\). This means that, for any pair \(a=((_{i},y_{i}),(_{j},y_{j})) A(D)\) and for any example \((_{i},y_{i})\) from the pair, the following holds:

1. With a probability \(p 42.5\%\), the classifier correctly classifies \((_{i},y_{i})\) by a sufficiently large predicted score, i.e., \(f_{y_{i}}(_{i}) 0.5+\), while wrongly classifies the other example \(_{j}\) to \(y_{i}\) by a less score, i.e., \(f_{y_{i}}(_{j}) 0.5+\).
2. When the classifier predicts \((_{i},y_{i})\) to class \(_{i}\), the predicted scores for the other classes excluding \(y_{i}\) are sufficiently small, i.e., \(f_{c}(_{i})_{i}}(_{i})}{C-1}\) for \(c y_{i},_{i}\).

Proof for Theorem 4.1 together with a toy illustration example is provided in supplementary material.

**Assumption Discussion.** Assumption 4.2 is feasible in practice. Reduced loss curvature is a natural result from adversarial defense, particularly for adversarial training and regularization based methods  as mentioned in Section 2. Regarding its second part determined by neural network weights and activation derivatives, common training practices like weight regularization and normalization help prevent from obtaining overly inflated elements in \(_{}\), and thus bound \(\|_{}\|_{2}\) and \(\|_{}^{}\|_{2}\). Following Definition 4.3, the ambiguous pair \(a=((_{i},y_{i}),(_{j},y_{j}))\) is constructed to let the classifier struggle with classifying the neighbouring example, e.g., \((_{j},y_{j})\), when it is able to classify successfully, e.g., \((_{i},y_{i})\). Consequently, the success of classifying \((_{i},y_{i})\) is mostly accompanied with a failure of classifying \((_{j},y_{j})\) into \(y_{i}\), and vice versa. In Assumption 4.4, for an acceptable classifier, the first part assumes its failure is fairly mild, while the second part assumes its struggle is between \(y_{i}\) and \(y_{j}\). As shown in our proof of Theorem 4.1, in order for Eq. (5) to hold, a polynomial inequality of the probability \(p\) needs to be solved, providing a sufficient condition on achieving a reduced ensemble risk, i.e., \(p 42.5\%\). Later, we conduct experiments to examine how well some assumptions are met by adversarially trained classifiers and report the results in supplementary material.

## 5 iGAT: Improving Ensemble Mechanism

Existing ensemble adversarial defense techniques mostly base their design on a framework of combining classification loss and diversity for training. The output of each base classifier contains the probabilities of an example belonging to the \(C\) classes. For an input example \(\), we denote its output from the \(i\)-th base classifier by \(^{i}()=[h_{1}^{i}()...,h_{C}^{i}()]\) for \(i[N]\), where \(N\) denotes the number of used base classifiers. Typical practice for combining base predictions includes the averaging, i.e., \(()=_{i=1}^{N}^{i}()\), or the max operation, i.e., \(h_{j}()=_{i[N]}(h_{j}^{i}())\). Without loss of generality, we denote the combiner by \(=(^{1},...,^{N})\). To train the base classifiers, we exemplify an ensemble loss function using one training example \((,y_{})\), as below

\[L_{E}(,y_{})=^{N}(^{i }(),y_{})}_{}+( ())+(^{1}(),^{2}(),,^{N}(),y_{})),\] (11)

where \(, 0\) are hyperparameters. An example choice for regularization is the Shannon entropy of the ensemble \(()\). Significant research effort has been invested to diversity design, for which it is optional whether to use the class information in diversity calculation. In the first section of supplementary material, we briefly explain four ensemble adversarial defense techniques highlighting their loss design strategies. These include ADP , CLDL , DVERGE  and SoE , and they are used later in Section 6 to test our proposed enhancing approach.

Despite the effort in diversity design that encourages better collaboration between base classifiers, it is unavoidable for some base classifiers to struggle with classifying examples from certain input subspaces. There are intersected subspaces that all the base classifiers are not good at classifying. To address this, we propose an _interactive global adversarial training_ (iGAT) approach. It seeks support from adversarial examples globally generated by the ensemble and distributes these examples to base classifiers with a probabilistic strategy empirically proven effective. Additionally, it introduces another regularization term to improve over the severest weakness of the base classifiers. Below we describe our proposal in detail.

### Distributing Global Adversarial Examples

We aim at improving adversarial robustness over intersected feature subspaces which are hard for all base classifiers to classify. These regions can be approximated by global adversarial examples generated by applying adversarial attacks to the ensemble, which are

\[(},})=(_{}( (^{1},...,^{N}),,A), ),\] (12)

where rows of \(}\) store the feature vectors of the generated adversarial examples. For instance, the FGSM attack can be used as \(A\). Instead of feeding the same full set of adversarial examples to train each base classifier, we distribute different examples to different base classifiers, to improve performance and to reduce training time. The generated examples are divided into \(N\) groups according to their predicted class probabilities. The \(i\)-th group \((}^{i},}^{i})\) is used to train the \(i\)-th base classifier, contributing to its classification loss.

Our core distributing strategy is to encourage each base classifier to keep improving over regions that they are relatively good at classifying. This design is motivated by our theoretical result. We have proved in Theory 4.1 an error reduction achieved by the ensemble of base MLPs that satisfy the acceptability Assumption 4.4. This assumption is partially examined by whether the classifier returns a sufficiently high prediction score for the correct class or low scores for most of the incorrect classes for some challenging examples. By keeping assigning each base classifier new challenging examples that they are relatively good at classifying, it encourages Assumption 4.4 to continue to hold. In Section 6.3, we perform ablation studies to compare our proposal with a few other distributing strategies, and the empirical results also verify our design. Driven by this strategy, we propose one hard and one soft distributing rule.

**Hard Distributing Rule:** Given a generated adversarial example \((},y)=((})_{k},_{k})\), the following rule determines which base classifier to assign it:

\[h_{y}^{i}(})>_{j i,j[N]}h_{y}^{j}( }),(},y)(}^{i},}^{i}).\] (13)

We refer to it as a hard distributing rule as it simply assigns examples in a deterministic way. The example is assigned to the base classifier that returns the highest predicted probability on its ground truth class.

**Soft Distributing Rule:** A hard assignment like the above can be sensitive to errors. Alternatively, we propose a soft distributing rule that utilizes the ranking of the base classifiers based on their prediction performance meanwhile introduces uncertainty. It builds upon rouleteste wheel selection , which is a commonly used genetic operator in genetic algorithms for selecting promising candidate solutions. Firstly, we rank in descending order the predicted probabilities \(\{h_{y}^{i}()\}_{i=1}^{N}\) by all the base classifiers for the ground truth class, and let \(r_{}(^{i})[N]\) denote the obtained ranking for the \(i\)-th base classifier. Then, we formulate a ranking-based score for each base classifier as

\[p_{i}=}(^{i})}}{_{i[N]}2^{ i-1}},\] (14)

and it satisfies \(_{i[N]}p_{i}=1\). A more top ranked base classifier has higher score. Next, according to \(\{p_{i}\}_{i=1}^{N}\), we apply roulette wheel selection and distribute the example to the selected base classifier. Specifically, the selection algorithm constructs \(N\) intervals \(\{[a_{i},b_{i}]\}_{i=1}^{N}\) where \(a_{1}=0\), \(b_{1}=p_{1}\), also \(a_{i}=b_{i-1}\) and \(b_{i}=a_{i}+p_{i}\) for \(i=2,3,,N\). After sampling a number \(q(0,1]\) following a uniform distribution \(q U(0,1)\), check which interval \(q\) belongs to. If \(a_{i}<q b_{i}\), then the example is used to train the \(i\)-th base classifier. This enables to assign examples based on ranking but in a probabilistic manner in order to be more robust to errors.

### Regularization Against Misclassification

We introduce another regularization term to address the severest weakness, by minimizing the probability score of the most incorrectly predicted class by the most erroneous base classifier. Given an input example \((,y_{})\), the proposed term is formulated as

\[L_{R}(,y_{})=-_{0/1}( (^{1}(),...,^{N}()),y_{ })(1-_{i=1}^{C}\;_{j=1}^{N}h_{i}^{j}()).\] (15)

Here, \(_{0/1}(,y)\{0,1\}\) is an error function, where if the input classifier \(\) can predict the correct label \(y\), it returns \(0\), otherwise \(1\). This design is also motivated by Assumption 4.4, to encourage a weak base classifier to perform less poorly on challenging examples so that its chance of satisfying the acceptability assumption can be increased.

### Enhanced Training and Implementation

The proposed enhancement approach iGAT, supported by (1) the global adversarial examples generated and distributed following Section 5.1 and (2) the regularization term proposed in Section 5.2, can be applied to any given ensemble adversarial defense method. We use \(L_{E}\) to denote the original ensemble loss as in Eq. (11), the enhanced loss for training the base classifiers become

\[_{\{^{i}\}_{i=1}^{N}}\ _{(,y_{}) (,)}[L_{E}(,y_{})]}_{ } +^{N}_{(,y_{ })(}^{i},^{i})}[_{CE}( ^{i}(),y_{})]}_{}\] (16) \[+_{(,y_{})( ,)(},)}[L_{R}( ,y_{})]}_{},\]

where \(, 0\) are hyper-parameters. In practice, the base classifiers are firstly trained using an existing ensemble adversarial defense technique of interest, i.e., setting \(==0\). If some pre-trained base classifiers are available, they can be directly used instead, and fine-tuned with the complete loss. In our implementation, we employ the PGD attack to generate adversarial training examples, as it is the most commonly used in existing literature and in practice.

## 6 Experiments and Results of iGAT

In the experiments, we compare with six state-of-the-art ensemble adversarial defense techniques including ADP , CLDL , DVERGE , SoE , GAL  and TRS . The CIFAR-10 and CIFAR-100 datasets are used for evaluation, both containing 50,000 training and 10,000 test images . Overall, ADP, CLDL, DVERGE and SoE appear to be the top performing methods, and we apply iGAT1 to enhance them. The enhanced, referred to as iGAT\({}_{}\), iGAT\({}_{}\), iGAT\({}_{}\) and iGAT\({}_{}\), are compared with their original versions, and additionally GAL  and TRS .

### Experiment Setting

We test against white-box attacks including PGD with \(20\) inner optimization iterations and CW with \(L_{}\) loss implemented by Wu et al. , and the black-box SignHunter (SH) attack  with \(500\) maximum loss queries. In accordance with Carmon et al. , the CW attack is applied on 1,000 equidistantly sampled testing examples. We also test against the strongest AutoAttack (AA) , which encapsulates variants of the PGD attack and the black-box square attack . All attack methods use the perturbation strength \(=8/255\).

For all the compared methods, an ensemble of \(N=8\) base classifiers with ResNet-20  backbone is experimented, for which results of both the average and max output combiners are reported. To implement the iGAT enhancement, the soft distributing rule from Eq. (14) is used. The two hyper-parameters are set as \(=0.25\) and \(=0.5\) for SoE, while \(=5\) and \(=10\) for ADP, CLDL and DVERGE, found by grid search. Here SOE uses a different parameter setting because its loss construction differs from the others, thus it requires a different scale of the parameter range for tuning \(\) and \(\). In practice, minor adjustments to hyper-parameters have little impact on the results. The iGAT training uses a batch size of \(512\), and multi-step leaning rates of \(\{0.01,0.002\}\) for CIFAR10 and \(\{0.1,0.02,0.004\}\) for CIFAR100. Implementation of existing methods uses either their pre-trained models or their source code for training that are publicly available. Each experimental run used one NVIDIA V100 GPU plus 8 CPU cores.

### Result Comparison and Analysis

We compare different defense approaches by reporting their classification accuracies computed using natural images and adversarial examples generated by different attack algorithms, and report the results in Table 1. The proposed enhancement has lifted the performance of ADP and DVERGE to a state-of-the-art level for CIFAR-10 under most of the examined attacks, including both the white-box and black-box ones. The enhanced DVERGE by iGAT has outperformed all the compared methods in most cases for CIFAR-100. In addition, we report in Table 2 the accuracy improvement obtained by iGAT for the studied ensemble defense algorithms, computed as their accuracy difference normalised by the accuracy of the original algorithm. It can be seen that iGAT has positively improved the baseline methods in almost all cases. In many cases, it has achieved an accuracy boost over \(10\%\).

Here are some further discussions on the performance. We observe that DVERGE and its iGAT enhancement perform proficiently on both CIFAR10 and CIFAR100, while ADP and its enhancement are less robust on CIFAR100. We attempt to explain this by delving into the algorithm nature of DVERGE and ADP. The ADP design encourages prediction disparities among the base models. As a result, each base model becomes proficient in classifying a subset of classes that the other base models may struggle with. However, a side effect of this is to discourage base models from becoming good at overlapping classes, which may become ineffective when having to handle a larger number of classes. The reasonably good improvement achieved by iGAT for ADP in Table 2 indicates that an addition of global adversarial examples is able to rescue such situation to a certain extent. On the other hand, in addition to encouraging adversarial diversity among the base models, DVERGE also aims at a stable classification so that each example is learned by multiple base models. This potentially makes it suitable for handling both large and small numbers of classes. Moreover, we also observe that the average combiner provides better performance than the max combiner in general.

The reason can be that an aggregated prediction from multiple well-trained base classifiers is more statistically stable.

### Ablation Studies

The key designs of iGAT include its distributing rule and the regularization term. We perform ablation studies to examine their effectiveness. Firstly, we compare the used soft distributing rule with three alternative distributing rules, including (1) a distributing rule opposite to the proposed, which allocates the adversarial examples to the base models that produce the lowest prediction score, (2) a random distributing rule by replacing Eq. (14) by a uniform distribution, and (3) the hard distributing rule in Eq. (13). Then, we compare with the setting of \(=0\) while keeping the others unchanged. This change removes the proposed regularization term. Results are reported in Table 3 using iGAT\({}_{}\) with the average combiner, evaluated by CIFAR-10 under the PGD attack. It can be seen that a change or removal of a key design results in obvious performance drop, which verifies the effectiveness of the design.

## 7 Conclusion, Limitation and Future Work

We investigate the challenging and crucial problem of defending against adversarial attacks in the input space of a neural network, with the goal of enhancing ensemble robustness against such attacks while without sacrificing much the natural accuracy. We have provided a formal justification of the advantage of ensemble adversarial defence and proposed an effective algorithmic improvement, bridging the gap between theoretical and practical studies. Specifically, we have proven a decrease in empirical 0-1 loss calculated on data samples challenging to classify, which is constructed to simulate the adversarial attack and defence scenario, under neural network assumptions that are feasible in practice. Also, we have proposed the iGAT approach, applicable to any ensemble adversarial defense technique for improvement. It is supported by (1) a probabilistic distributing rule for selectively allocating global adversarial examples to train base classifiers, and (2) a regularization penalty for addressing vulnerabilities across all base classifiers. We have conducted thorough evaluations and ablation studies using the CIFAR-10 and CIFAR-100 datasets, demonstrating effectiveness of the key designs of iGAT. Satisfactory performance improvements up to \(17\%\) have been achieved by iGAT.

However, there is limitation in our work. For instance, our theoretical result is developed for only two base MLPs. We are in progress of broadening the scope of Theorem 4.1 by further relaxing the neural network assumptions, researching model architectures beyond MLPs and beyond the average/max combiners, and more importantly generalizing the theory to more than two base classifiers. Additionally, we are keen to enrich our evaluations using large-scale datasets, e.g., ImageNet. So far, we focus on exploiting curvature information of the loss landscapes to understand adversarial robustness. In the future, it would be interesting to explore richer geometric information to improve the understanding. Despite the research success, a potential negative societal impact of our work is that it may prompt illegal attackers to develop new attack methods once they become aware of the underlying mechanism behind the ensemble cooperation.