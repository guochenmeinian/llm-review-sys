# DevBench: A multimodal developmental benchmark for language learning

Alvin W. M. Tan, Sunny Yu,

Bria Long, Wanjing Anya Ma, Tonya Murray,

**Rebecca D. Silverman, Jason D. Yeatman, Michael C. Frank**

Stanford University

{tanawm, syu03, bria, wanjingm, tonyamur,

rdsilver, jyeatman, mcfrank}@stanford.edu

###### Abstract

How (dis)similar are the learning trajectories of vision-language models and children? Recent modeling work has attempted to understand the gap between models' and humans' data efficiency by constructing models trained on less data, especially multimodal naturalistic data. However, such models are often evaluated on adult-level benchmarks, with limited breadth in language abilities tested, and without direct comparison to behavioral data. We introduce DevBench, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. We evaluate a set of vision-language models on these tasks, comparing models and humans on their response patterns, not their absolute performance. Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. We also examine the developmental trajectory of OpenCLIP over training, finding that greater training results in closer approximations to adult response patterns. DevBench thus provides a benchmark for comparing models to human language development. These comparisons highlight ways in which model and human language learning processes diverge, providing insight into entry points for improving language models.

## 1 Introduction

Humans are remarkably good language learners, acquiring language rapidly in the first few years of life without much explicit supervision. Recent machine learning models are also able to acquire some aspects of human language, although they require much larger quantities of language input than a typical human would receive . This "data gap" reflects differences in the learning processes and mechanisms employed by human language learners and language models; understanding the nature of these differences is pivotal for the joint goals of building better cognitive models of language learning and building more data-efficient language models.

Recent work has attempted to bridge this data gap by constructing models that are trained on less data, including on naturalistic data from children . However, these models have typically been evaluated using benchmarks that reflect adult human performance, whether explicitly (i.e., with comparison data collected from adult participants) or implicitly (i.e., high accuracy on some recognition task). Such evaluation methods are not appropriate to the goal of understanding whether developmentally realistic data leads to human-like learning. We would not expect child performance to be similar to adult performance on these tasks for various reasons related to both language competence (i.e., children's vocabulary knowledge) and to their co-developing cognitive abilities(e.g., working memory limitations). Instead, it is crucial to evaluate models on benchmarks that can indicate whether the language ability gained by machine learning models matches the language ability gained by children when exposed to similar developmental data.

Research in child language acquisition has observed that children learn different aspects of language at different rates, and the acquisition of different levels of representation also interact in complex and nonlinear ways [6; 7; 8; 9; 10]. Thus, in order to characterise models' language learning performance, we should evaluate multiple levels of linguistic representation, including the lexicon, syntax, and semantics - ideally how these correspond to children's development at different ages.

Additionally, to conduct methodologically rigorous cognitive evaluation of machine learning models, it is important to compare human and model performance directly (rather than assuming that humans will perform well), and to ensure that evaluation conditions are as similar as possible for models and humans [11; 12; 13]. Notably, many language evaluations with infants and toddlers are multimodal in nature, because this method permits the measurement of language comprehension without the additional working memory and motor control demands of language production. Young children can use nonverbal response methods such as looking or pointing, which can nonetheless reflect their language abilities. The incorporation of multimodality has an additional advantage of introducing grounding, which has been suggested as a possible solution to the data gap .

These observations provide a set of desiderata for a developmentally appropriate evaluation of language models:

1. Wide dynamic range of difficulty
2. Multiple levels of linguistic representations
3. Corresponding data from children
4. High similarity in evaluation method between models and humans

To create a benchmark that satisfies these desiderata, we introduce **DevBench**, a multimodal benchmark of language evaluation tasks. DevBench includes a suite of seven tasks measuring lexical, syntactic, and semantic ability, with human data from both children and adults. Notably, the primary evaluation metric reflects models' similarity to human response patterns, rather than absolute performance levels, allowing us to capture finer-grained details about human-model similarity.

We evaluate a set of vision-language models on DevBench, including not only state-of-the-art models but also smaller models and models trained on developmentally realistic data. We additionally investigate how DevBench performance varies over model training by evaluating intermediate checkpoints of OpenCLIP . Evaluation results suggest that current vision-language models exhibit variation in their human-likeness, suggesting further areas of research to develop language models that more closely approximate human language learning.

## 2 Related work

### Multimodal benchmarks

Since the advent of multimodal models, various benchmarks have been developed to evaluate their performance . A majority of these benchmarks primarily involve visual question answering [16; 17; 18; 19; 20; 21]; other tasks include image captioning [18; 22], prediction , and retrieval [17; 22].

Most tasks in these multimodal benchmarks probe models' perception, reasoning, and knowledge, evaluating models on domains including action prediction, counting, relational reasoning, or optical character recognition. Correct responses to these tests require the conjunction of many skills - notably, all reasoning tasks also require perception, and perception tasks in turn broadly require object knowledge. Furthermore, most multimodal benchmarks focus on models' visual understanding, and no existing benchmark focuses specifically on the linguistic abilities of multimodal models.

### Developmentally inspired evaluation

Some recent work in machine learning evaluation has used benchmarks inspired by developmental psychology and cognitive science. For example, the Large Language Model Response Score (LRS)  draws from key experiments in the child development literature to construct question answering tasks for large language models, though it does not involve comparison to data from children. In addition, most of its tasks were originally multimodal (with visual stimuli and verbal prompts), and it is not clear how the translation into a unimodal verbal task would affect human performance. In the visual domain, the Infant-Level Physical Reasoning Benchmark (InfLevel)  aimed to evaluate video models on physical reasoning, drawing from classic violation of expectation tasks in the infant cognition literature related to the principles of continuity, solidity, and gravity. However, this benchmark also did not have a direct comparison with human data. The Machine Word Learning (MEWL) benchmark  is a multimodal evaluation that builds upon hypothesised mechanisms of few-shot word learning in children, and requires the inference of the meaning of novel words from a set of visual scenes witherentially ambiguous labels. The benchmark contains data from adults, but their poor performance on some subsets of these trials (e.g., those requiring pragmatic implicature) suggests that children might find these tasks very difficult. More recently, the ModelVsBaby benchmark  has assessed out of distribution object recognition with corresponding data from 2-year-olds, providing a first step towards direct developmental model-human comparison.

Other work assessing developmental models has also developed ad-hoc evaluations. The most common method has been to design evaluation sets that resemble other machine learning model tasks (e.g., grammatical acceptability, image classification) while restricting the domain to a developmentally relevant domain (i.e., only including vocabulary familiar to the model, or choosing image categories that are child-relevant). This method has been applied both to language models (e.g., the Zorro benchmark on BabyBERTa models ), and to multimodal models (e.g., the Konkle Objects evaluation on CVCL models ). However, again in these cases there are typically no child data, and the assumption that children would perform well in such evaluations is not directly evaluated. We summarise the characteristics of a range of multimodal and developmental benchmarks in Table 1.

### Model learning trajectory analyses

A few studies have used developmental approaches to analyse model learning trajectories. Chang and Bergen  examined the age of acquisition of different words by measuring the change in mean surprisal of a word over training epochs, finding dissimilarities in the predictors of age of acquisition in models and children. Evanson et al.  examined the age of acquisition of different syntactic structures by measuring the point at which models began preferring the grammatical sentence in a grammatical acceptability task. We extend these approaches here by examining training trajectories across multiple levels of linguistic representation.

## 3 DevBench description

DevBench contains seven tasks across lexical, syntactic, and semantic domains. Each task is accompanied by item-level human data so that full human response distributions can be compared to model scores. The lexical tasks measure vocabulary knowledge, operationalised as the ability to correctly pick out the visual referent of a noun label. The syntactic tasks measure grammatical knowledge, operationalised as the ability to correctly pick out a scene containing the correct relations

    &  &  &  \\ Benchmark &  Multi- \\ modal \\  &  Zero- \\ shot \\  &  Nonverbal \\ response \\  & 
 Development \\ mental \\  & Lexicon & Syntax & Semantics & Children & Adult \\  LAMM  & ✓ & ✓ & & & & & & & \\ MultiBench  & ✓ & ✓ & & & & & & \\ GEM  & ✓ & ✓ & ✓ & & & & & \\ MMBench  & ✓ & ✓ & ✓ & & & & & \\ SEED-Bench  & ✓ & ✓ & ✓ & & ✓ & ✓ & & \\ MME  & ✓ & ✓ & ✓ & & ✓ & & ✓ & \\ Perception Test  & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & & \\ M3Esam  & ✓ & ✓ & & ✓ & & & & ✓ \\  LRS  & & ✓ & & ✓ & & & & & \\ InfLevel  & & ✓ & ✓ & ✓ & & & & & \\ Zorro  & & ✓ & ✓ & ✓ & & ✓ & & \\ MEWL  & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ ModelV8Baby  & ✓ & ✓ & ✓ & ✓ & ✓ & & ✓ & ✓ \\  DevBench & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Characteristics of multimodal and developmentally inspired benchmarks.

among its constituent members when given a sentential label. The semantic tasks measure conceptual (visual or linguistic) representation space via representational similarity. A schematic of the tasks and the ages of participants contributing data for each task is shown in Figure 1, and sample trials are shown in Figure 2. All code and data are available at github.com/alvinwmtan/dev-bench; all data are licensed under CC BY-NC-SA or more permissible licences, and have been anonymised to eliminate any personally identifiable information.

### Tasks

Lexicon: Looking-while-listening (LWL)In this task, children are presented with two visual images (one target and one distractor) as well as a verbal cue (e.g., "Look at the dog!") . Children's proportion looking time to each image is measured. We used data from three datasets to improve age coverage: 18-month-old data from Adams et al. , 24-month-old data from Frank et al. , and 30-month-old data from Donnelly and Kidd  (total \(N\) = 294). Some images in the original stimuli set were not shareable due to license restrictions; in these cases, we used replacement stimuli matched for visual and semantic properties.

Lexicon: Visual vocabulary task (VV)In this task, participants hear a word (e.g., "swordfish") and then see four visual images corresponding to the target and three distractor images that range in similarity to the target word (close, far, and distal) . Participants respond by choosing the image that they think matches the verbal cue. We used an unpublished dataset from Long et al.  containing responses from 3- to 12-year-olds as well as adults (total \(N\) = 1780).

Syntax: Test of Receptive Grammar (TROG)In this task, participants are presented with four visual images (one target and three distractors) and then hear a phrase or sentence cue (e.g., "The

Figure 1: Tasks in DevBench arranged by linguistic domain, along with the ages for which corresponding human data are available. A: Adult.

Figure 2: Sample trials for each task in DevBench.

brown horse chases the white dog") . In most trials, the distractors are constructed such that they would align with a label that contains the same words as the cue but in a different order (e.g., an image containing a brown dog chasing a white horse). Participants respond by choosing the image that they think matches the verbal cue. We used stimuli and unpublished data from Silverman and Yeatman , containing responses from children aged 11-12 (total \(N\) = 514).

Syntax: Winoground-NoTag (WG)This task contains trials with two images and two sentence labels, such that each image has one corresponding label, and the two labels only differ in word order . Human ratings were collected by asking adult crowdworkers whether one particular label matched one particular image, and an image-caption score was calculated as the proportion of affirmative responses (total \(N\) = 171). For comparability to the other tasks in this benchmark, we considered the image-caption scores for the two images and one sentence of each trial, converted to proportions. We included only trials labelled "NoTag" from , which reflect vanilla Winoground trials that rely more narrowly on syntax (as opposed to also requiring pragmatics or other language abilities).

Semantics: Free word association task (WAT)In this task, participants were presented with a cue word, and asked to name the first word association that came to mind. We use association frequencies as a proxy for similarity scores. We combined a child dataset from Entwisle  with a subset of the adult dataset from Nelson et al.  comprising all cue words found in the child dataset (total \(N\) > 7040), and thresholded the included responses to omit idiosyncratic responses.

Semantics: Visual object categorization (VOC)In this task, 4-, 10-, and 14-month-old infants saw pairs of images, and proportion looking times to each image was measured  (total \(N\) = 73). We used replacement stimuli for some images that were not shareable in the original stimuli set. Dissimilarities between images were calculated as the difference in proportion looking times to the two images.

Semantics: THINGS similarity ratingsIn this task, adult participants did a triplet odd-one-out task one triads constructed from the THINGS database [44; 45] (total \(N\) = 12340). These were used to generate a sparse positive similarity embedding , which we used to calculate pairwise similarities among images.

### Human-model comparison

Because we were interested in response patterns, we conducted human-model comparison by examining the (dis)similarities in human and model distributions in responses.

Our goal on the lexicon and syntax tasks was to compare the distribution of human choices across response options to model choices. We obtained image-text matching logits for each response option, and calculated the _softmax-optimised Kullback-Leibler divergence_ of human responses from model responses. This novel metric was operationalised as the minimum KL divergence between the human response probability distributions \(\) from model logits \(\), optimizing the softmax exponent \(\). We average this divergence across trials \(t\), and calculate each distribution across images \(i\):

\[D_{}^{*}(h m)=_{}_{t}D_{} (_{t}_{tt}}}{_{i}e^{ _{tt}}})\]

For WAT, we calculated the softmax-optimised KL divergence of human association probabilities from softmaxed model text embedding similarities, averaged over all trials. For the visual semantic tasks, we instead conducted human-model comparison by applying representational similarity analysis (RSA)  on human and model representational similarity matrices, which represents correlations in the representational geometries of humans and models. All comparisons were conducted within each age bin when applicable.

Our method applies to models from which image-text matching scores could be directly extracted (i.e., similarity models). More recent models have alternatively integrated visual and language inputs via conditional text generation (i.e., generation models). There is as yet limited consensus for the best method to obtain image-text matching scores for generation models; we conducted two exploratory evaluation methods relying on next-token prediction and on log-likelihood measurement. More details on the evaluation of generation models can be found in Appendix C.

### Baselines

We also constructed two baselines for the benchmark to demonstrate the dynamic range that is possible for each task. First, we calculated a human baseline for tasks on which participant-level data were available (LWL, VV, TROG, VOC). To estimate this baseline, we randomly split the participants into two groups and calculated the between-group softmax-optimised KL divergence or RSA similarity as appropriate, repeating for 1000 random splits. We used the median result as a point estimate of the human baseline, which serves as a positive baseline for our tasks. Note that this baseline is an underestimate of the true values, because the results are not corrected upwards for the attenuation due to splitting the data in half.

We also added a random baseline for all tasks, generated using a random initialisation of the OpenCLIP model. The random baseline serves as a negative baseline for our tasks.

## 4 Benchmark

We evaluated a diverse set of vision-language models on our benchmark, using an NVIDIA T4 GPU, an NVIDIA A40 GPU, or CPUs (depending on resource availability). A summary of model performance for each task (averaged across all ages) is shown in Table 2, along with model characteristics (number of parameters and size of training set). For lexicon and syntax tasks as well as WAT, we report \(D^{*}_{}\), for which _lower_ scores indicate greater human-likeness in response patterns. For visual semantic tasks (VOC, THINGS), we report RSA similarity, for which _higher_ scores indicate greater human-likeness in response patterns. Divergences are not comparable across datasets due to different features of each dataset. A description of all models and more details on the evaluation setup can be found in Appendix A.

Overall, CLIP-large and OpenCLIP-H performed relatively well on the lexicon and syntax tasks. CVCL, which was trained on a small set of annotated head-mounted camera from infants , was mostly dissimilar to humans in lexicon and syntax tasks, even on the looking-while-listening (LWL) task, which was administered to young children. However, CVCL outperformed other models on the visual object categorization task, suggesting that its visual representational space was more similar to infants'. SigLIP was also unusually poor-performing (especially given its general performance and accuracy), potentially suggesting that aspects of its training (e.g., its objective function) may have resulted in non-alignment with humans. We thus excluded it as an outlier from further analyses.

## 5 Analysis

To further understand the relationship between model and human responses, we conduct more fine-grained analyses, aiming to answer three specific research questions:

    & & &  &  &  \\  Model & \# params & \# images & LWL (\(\)) & VV (\(\)) & TROG (\(\)) & WG (\(\)) & WAT (\(\)) & VOC (\(\)) & THINGS (\(\)) \\  CLIP-base  & 149M & 400M & 0.014 & 0.205 & 0.732 & 0.256 & 0.495 & -0.081 & **0.397** \\ CLIP-large  & 428M & 400M & 0.013 & **0.179** & 0.692 & 0.256 & 0.495 & 0.005 & 0.246 \\ ViLT  & 87M & 4.1M & 0.009 & 0.326 & 0.682 & 0.252 & 0.495 & -0.053 & 0.127 \\ FLAVA  & 350M & 70M & 0.013 & 0.197 & 0.912 & 0.254 & 0.495 & -0.042 & 0.189 \\ BLIP  & 252M & 14M & 0.010 & 0.193 & **0.576** & 0.259 & 0.495 & -0.104 & 0.185 \\ Bridge-Power  & 333M & 4M & **0.008** & 0.265 & 0.584 & **0.250** & 0.495 & -0.095 & 0.345 \\ OpenCLIP-H  & 1.0B & 32B & 0.012 & 0.188 & 0.683 & 0.255 & **0.495** & 0.031 & 0.227 \\ SigLIP  & 800M & 9B & 0.067 & 0.612 & 0.888 & 0.258 & 0.495 & -0.028 & 0.192 \\ CVCL  & 26M & 600K & 0.060 & 0.740 & 0.911 & 0.258 & 0.495 & **0.138** & 0.175 \\  Human & & & 0.010 & 0.091 & 0.028 & & & 0.251 & \\ Random (OpenCLIP) & 1.0B & 0 & 0.087 & 0.740 & 0.908 & 0.258 & 0.495 & 0.246 & 0.054 \\   

Table 2: Model characteristics and performance across all tasks, demonstrating variation across models. Arrows indicate the direction of better performance (i.e., lower is better vs. higher is better). Bolded results indicate most human-like result on a task.

1. How does model-human similarity relate to other features of the models, namely their size (in terms of parameters), their training dataset size, and their overall accuracy on the tasks?
2. How does model-human similarity change over the course of model training, and in particular can we elucidate "developmental" trends in model training?
3. On which items are models and humans most dissimilar? On which items are they most similar?

### Model feature analysis

To understand the variation in human-likeness exhibited by different models, we considered a range of model features that may affect response patterns, namely the number of parameters of the model, the number of examples in its training set, and its accuracy on the task (for lexicon and syntax tasks, which have a "correct" answer). For each task and each feature, we calculated Pearson's correlations between feature values and model-human similarities; number of parameters and number of training images were log-transformed. Similarity-feature correlations are shown in Figure 3a.

Overall, we found that model-human similarity was most correlated with task accuracy, with consistently high correlations across all ages and tasks. Model size also correlates relatively well with model-human similarity for most tasks except for VOC, which is reasonable given that VOC was conducted on the youngest participants (aged 4-14 months). In contrast, the number of training examples exhibited the poorest correlations with model-human similarity, suggesting that dataset size may not be as informative about human-likeness as accuracy or model size.

To illustrate the relationship between accuracy and model-human similarity, we plot these values for the Visual Vocabulary (VV) task in Figure 3b, which is also the task for which we have the largest coverage across age groups. In the VV task, we found a strong correlation between accuracy and model-human similarity for all age bins. In addition, we examined whether we would see differences in the strength of this correlation in data from children of different ages. We found that worse-performing models tended to show response patterns that were more similar to those from younger children, whereas better-performing models showed response patterns more similar to those from older children and adults - captured by an interaction effect between accuracy and age in a linear mixed-effect model (\(b\) = -0.057, \(SE\) = 0.003, \(p\) <.001). These results suggest that children's lexical representations are more similar to those instantiated in lower-performing multimodal models early in development, and gradually become more similar to higher-performing multimodal models across middle childhood.

Figure 3: (a) Correlations between model–human similarity and task accuracy, log number of parameters (size), and log number of training images for each task (training), averaged across ages. Accuracy correlates the most strongly with model–human similarity, followed by size, then training. (b) Model–human dissimilarity as a function of task accuracy for each model on the Visual Vocabulary task. Higher performing models showed a closer correspondence to behavioral patterns from children and adults. A: Adult.

### Model training analysis

We next sought to understand whether human developmental trajectories were comparable to model training trajectories. We made use of OpenCLIP-H , an open model for which checkpoints were available on Hugging Face, allowing us to conduct "developmental" analyses. To do so, we sampled 32 checkpoints at approximately logarithmic intervals, and calculated model-human similarities for each task for each checkpoint. Training trajectory curves for three tasks (VV, TROG, and WG) are shown in Figure 4.

Overall, OpenCLIP increased in similarity to humans across training for these three tasks. Further, model performance on the Visual Vocabulary task also reflects a developmental trend: OpenCLIP exhibited greatest similarity to younger humans earlier in the training regime, and greatest similarity to older humans later in the training regime. This pattern matches the trend shown across models in Figure 3b - in both cases, better performing models show better correspondence to human data.

For brevity, we briefly describe the trajectory curves for OpenCLIP-H for the four remaining tasks; complete results are detailed in the SI. For LWL, we found that model-human similarity also improves with training, as expected. For the semantics tasks, the results were more complex, however. For WAT, model-human similarity largely remains constant over training, indicating that language representations do not significantly change. For VOC, the developmental trajectories were non-monotonic. For THINGS, model-human similarity _decreases_ with training, perhaps indicating divergence with human visual representational space. Together, these results suggest that multimodal model representations may be less applicable to the semantics tasks we selected, and future work is needed to understand differences between semantic representations in uni- and multimodal representations.

### Item-level analysis

Finally, we examined specific items to determine which items exhibited the greatest and least dissimilarity in response pattern between models and humans. For each model, we \(z\)-scored the \(D^{*}_{}\) values to control for inter-model variation in overall model-human similarity. We then averaged \(D^{*}_{}\) values across models for each trial, and extracted the top five items with the greatest mean divergence and the top five items with the least mean divergence for qualitative analysis. We illustrate this method for VV and WG, as shown in Table 3.

The most dissimilar items reveal certain features which may particularly drive model-human dissimilarity, particularly in comparison to the most similar items. For VV, some such features include polysemous targets (e.g., "horn" and "net"), targets which may have other labels (e.g., "hand plow" for "hoe", "pudding" for "flan"), or targets which could be labelled as one of the distractor categories (e.g., "lollipop" is a type of "candy"). For WG, several of the most dissimilar items have genuinely contentious captions - notably, the image for the caption "the dog is swimming and the person is standing" has the person hunched over rather than fully upright, the image for the caption "a person sits and a dog stands" has the dog mid-leap, and the image for the caption "green pants and blue top" has a top that could be described as grey. More broadly, across both VV and WG, humans appear to be better able to handle ambiguity and choose the most likely answer (perhaps through pragmatic reasoning ), whereas models are more likely to put less density on the true target.

Figure 4: Trajectories of model–human similarity for VV, TROG, and WG. OpenCLIP-H becomes more human-like over training, and recovers developmental trends for VV.

## 6 Discussion

In this work, we introduced DevBench, a multimodal benchmark for language learning consisting of seven tasks with corresponding data from both children and adults. Evaluating a set of vision-language models revealed variation across models in terms of model-human similarity across lexical, syntactic, and semantic domains. Furthermore, model-human similarity was strongly correlated with model accuracy, as well as model size to a lesser extent. Analysing OpenCLIP-H checkpoints across training also recovered developmental trends for some tasks, but not others.

More broadly, DevBench provides a method for models trained on developmentally realistic data to be evaluated using a method that is comparable to how children are evaluated. Recent work has seen a plethora of new unimodal or multimodal learning models, including some that are evaluated here (e.g., ), but to date no models trained on developmentally realistic data (e.g., head-mounted camera data) have been evaluated actual developmental performance. We believe that benchmarks like DevBench are essential for understanding the degree to which any given model can be used to approximate human learning.

Systematically comparing models and children's response patterns - rather than overall accuracy - is an essential piece of this puzzle. In doing so, our results already highlight ways in which model training trajectories are rather _unlike_ human development trajectories, pointing towards new avenues for future work. For example, models appear to be worse than humans at handling ambiguous inputs; ambiguity resolution is thus a potential avenue for future multimodal model development.

### Limitations and future work

The development of DevBench was constrained by currently available human data; notably, this means that some tasks have relatively few items (in comparison with many other machine learning benchmarks), and some tasks have relatively few human participants. These limitations may result in uncertain reliabilities for the obtained results. More data are currently being collected for some of the tasks in DevBench, which we anticipate will improve reliability, and simultaneously permit more expansive developmental analyses due to an extension of the included age ranges. However, we hope that this approach and standardized format will enable future researchers to contribute novel datasets to DevBench, which we anticipate may grow in the coming years. Indeed, DevBench

    \\ VV & horn (distractors: bone, chin, ladybug) \\  & hoe (distractors: peg, dustpan, beaker) \\  & flan (distractors: fuse, amplifier, turnstile) \\  & net (distractors: tee, domino, hydrant) \\  & lollipop (distractors: candy, doorbell, crumb) \\ WG & a person whispering into a dog’s ear / a dog whispering into a person’s ear \\  & there are more ladybugs than flowers / there are more flowers than ladybugs \\  & the dog is swimming and the person is standing / the dog is standing and the person is swimming \\  & blue pants and green top / green pants and blue top \\  & a person sits and a dog stands / a person stands and a dog sits \\   \\ VV & foam (distractors: float, quilt, asparagus) \\  & saddle (distractors: handle, figurine, broccoli) \\  & stump (distractors: log, bookshelf, showerhead) \\  & sorbent (distractors: palette, tamale, chive) \\  & typewriter (distractors: printer, sunglasses, drumstick) \\ WG & a horse getting wet / getting a horse wet \\  & a large living thing in front of a large non-living thing / \\  & a large non-living thing in front of a large living thing \\  & a deer’s nose is resting on a child’s hand / a child’s hand is resting on a deer’s nose \\  & clothing on lines / lines on clothing \\  & soft shoes are on a smooth floor / smooth shoes are on a soft floor \\   

Table 3: Top five most dissimilar items and top five most similar items between humans and all models for Visual Vocabulary and Winoground tasks.

also includes primarily tasks for English-speaking children and adults, due to dataset availability. This situation reflects inequalities in language acquisition research , and more data is needed to construct a multilingual version of DevBench, which will be more comprehensive and generalisable.

Additionally, model performance in our evaluation setup may be affected by the domain gap between models' training data and the stimuli used in our benchmark; for example, TROG uses cartoon depictions of events, which are dissimilar to the more photorealistic training data of CLIP. Thus, our evaluation results likely represent a lower bound on model-human similarity. Children as young as two years of age are able to learn from and generalise to pictographic depictions of objects [57; 58; 59], however, suggesting that generalisation across representations is an early-acquired skill.

In our analyses of the relationship between model-human similarity and model features, we could not hold model architectures constant due to the limited availability of relevant model checkpoints - thus we cannot make strong claims about precisely what aspects of models lead to better fit to human responses. Systematic evaluation of the roles of training data and model size thus remains an important research question for future work (see [60; 61] for related work on scaling).

Finally, we chose one linking hypothesis between model logits and human distributions, namely optimised KL divergence. This hypothesis assumes a perfect calibration between model logits and true uncertainties, which is only valid to some extent. Further research in model calibration  may help to mitigate these effects.

### Conclusion

We hope that DevBench can serve as an encouragement for machine learning researchers and cognitive scientists to develop vision-language models that not only perform well, but also can more closely approximate human learning. In particular, DevBench highlights the need for more fully open models with training checkpoints , enabling the study of training trajectories, as well as the need for more human-realistic training [2; 60] to better characterise model-human correspondences across developmental change. It may also be possible to adopt a developmental, multimodal approach to studying domains other than language, such as mathematical, logical, and social reasoning; more intentional data collection across a wide range of ages, tasks, and contexts will help to provide an increasingly comprehensive set of comparison data to better understand model learning trajectories (e.g., ). These research directions, among others, will help us to better understand the processes underlying human development, and how we might transfer humans' learning efficiencies onto machine learning models.