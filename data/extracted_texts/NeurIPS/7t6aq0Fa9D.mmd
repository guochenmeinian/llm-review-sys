# FASTopic: Pretrained Transformer is a Fast, Adaptive, Stable, and Transferable Topic Model

Xiaobao Wu\({}^{1}\)1  Thong Nguyen\({}^{2}\)  Delvin Ce Zhang\({}^{3}\)  William Yang Wang\({}^{4}\)  Anh Tuan Luu\({}^{1}\)1

\({}^{1}\)Nanyang Technological University

\({}^{3}\)The Pennsylvania State University

\({}^{4}\)University of California, Santa Barbara

xiaobao002@e.ntu.edu.sg e0998147@u.nus.edu delvin.ce.zhang@gmail.com william@cs.ucsb.edu anhtuan.luu@ntu.edu.sg

###### Abstract

Topic models have been evolving rapidly over the years, from conventional to recent neural models. However, existing topic models generally struggle with either effectiveness, efficiency, or stability, highly impeding their practical applications. In this paper, we propose FASTopic, a fast, adaptive, stable, and transferable topic model. FASTopic follows a new paradigm: Dual Semantic-relation Reconstruction (DSR). Instead of previous conventional, VAE-based, or clustering-based methods, DSR directly models the semantic relations among document embeddings from a pretrained Transformer and learnable topic and word embeddings. By reconstructing through these semantic relations, DSR discovers latent topics. This brings about a neat and efficient topic modeling framework. We further propose a novel Embedding Transport Plan (ETP) method. Rather than early straightforward approaches, ETP explicitly regularizes the semantic relations as optimal transport plans. This addresses the relation bias issue and thus leads to effective topic modeling. Extensive experiments on benchmark datasets demonstrate that our FASTopic shows superior effectiveness, efficiency, adaptivity, stability, and transferability, compared to state-of-the-art baselines across various scenarios. 1

Figure 1: (**a**): Running speed rank and overall performance rank on the experiments with 6 benchmark datasets, including topic quality, doc-topic distribution quality, downstream tasks, and transferability. (**b**): Running time under the WoS dataset with varying sizes. See complete results in Figure 6.

## 1 Introduction

Due to the unsupervised fashion and interpretability, topic models have derived a broad spectrum of applications [12; 14], such as content recommendation [39; 79], generation [17; 88], and trend analysis [18; 36]. Early conventional topic models follow probabilistic graphical models [10; 8] or non-negative matrix factorization [35; 58]. But they rely on laborious model-specific derivations and fall short on large-scale data . Due to this, recent neural topic models have attracted more attention [90; 75], including VAE-based [42; 43; 61] and clustering-based [59; 89; 24].

However, existing neural topic models lack either efficiency, effectiveness, or stability. First, VAE-based topic models, while effective, are limited by their low efficiency. They follow the VAE framework  and often incorporate extra modules like graph neural networks [87; 1] or external knowledge [66; 80], resulting in complicated modeling structures. Owing to this, they suffer from intensive time complexity, _e.g._, consuming hours to process a dataset of 10k documents . Second, clustering-based topic models [89; 24] excel in efficiency as they require no training, but they sacrifice effectiveness. They tend to yield repetitive topics other than desired diverse ones or infer inaccurate topic distributions of documents [73; 1]. What is worse, these neural topic models suffer from low performance stability. They are extremely sensitive to hyperparameters, especially when applied to various scenarios concerning data domains, vocabulary sizes, and document length . In consequence, these challenges hinder the applications of topic modeling in practice.

To tackle these challenges, we in this paper propose a **F**ast, **A**daptive, **S**table, and **T**ransferable topic model (**FASTopic**). Different from existing conventional, VAE-based, or clustering-based approaches, we introduce a new paradigm for topic modeling: **Dual Semantic-relation Reconstruction (DSR)** as illustrated in Figure 2. Instead of complicated neural networks, DSR only considers three parameters: document embeddings from a pretrained Transformer, and topic and word embeddings. DSR models the dual semantic relations between (1) document and topic embeddings, and (2) topic and word embeddings, and interprets them as distributions for topic modeling. By reconstruction with these relations, DSR discovers latent topics in a neat and efficient framework, avoiding the complicated structures in prior studies. To model these relations, we further propose the novel **Embedding Transport Plan (ETP)**. Rather than simple parameterized softmax [37; 21], ETP regularizes these relations as the optimal transport plans between document, topic, and word embeddings. This mitigates the relation bias issue and produces distinct topics and accurate topic distributions, enabling effective topic modeling. Following the DSR paradigm with ETP, FASTopic provides a solid solution to the challenges of current topic models. As reported in Figure 1, FASTopic shows both superior efficiency and effectiveness compared to state-of-the-art baselines. Additionally FASTopic shows high

Figure 2: Illustration of topic modeling paradigms. **(a)**: VAE-based topic modeling with an encoder and a decoder [91; 65; 73]. **(b)**: Clustering-based topic modeling by clustering document embeddings [2; 24]. **(c)**: **Dual Semantic-relation Reconstruction (DSR)**, modeling doc-topic distributions as the semantic relations between document () and topic embeddings (), and modeling topic-word distributions as the semantic relations between topic () and word embeddings (). Here we model these relations as the transport plans to alleviate the relation bias issue.

transferability, robust adaptivity and stability across various scenarios, delivering better performance without hyperparameter tuning. We conclude the main contributions of this paper as follows:

* We propose a novel topic model with a new dual semantic-relation reconstruction paradigm that models semantic relations among document, topic, and word embeddings, bringing about a neat and efficient topic modeling framework.
* We further propose a novel embedding transport plan method that regularizes the semantic relations as optimal transport plans, which avoids the relation bias issue and leads to effective topic modeling.
* We conduct extensive experiments and demonstrate that our model shows high effectiveness, efficiency, adaptivity, stability, and transferability compared to state-of-the-art baselines.

## 2 Related Work

Conventional Topic ModelsThese models have two types. The first type is probabilistic topic models [25; 7; 9; 8], _e.g.,_ LDA , using probabilistic graphical models with topics as latent variables and inferred by Gibbs sampling  or Variational Inference . The second type uses non-negative matrix factorization [29; 58]. These models have been extended to several scenarios like short texts [82; 67], multilingual , and dynamic topic modeling [9; 64]. But they require model-specific derivations for parameter inference and cannot well handle large-scale datasets.

VAE-based Neural Topic ModelsThese models follow the Variational AutoEncoder [VAE, 31; 55] framework and directly use gradient backpropagation to optimize parameters [42; 43; 61; 19; 83; 84; 81; 47; 68; 85; 84; 81; 46; 86; 87; 76; 77; 74]. Although some work [91; 65; 86] like ECRTM  also uses optimal transport, we highlight that our method differs from them in that: (**i**) While they still follow the traditional complicated VAE framework, our FASTopic leverages the neat and efficient dual semantic-relation reconstruction paradigm; (**ii**) While they use optimal transport only as alternative distance measures or regularization for VAE, our FASTopic leverages the novel embedding transport plan to model semantic relations. These differences not only bring about faster running speed, but also lead to higher topic modeling performance.

Clustering-based Neural Topic ModelsThey cluster pretrained word embeddings via clustering algorithms like KMeans to yield topics [59; 2; 89], but mostly cannot infer topic distributions of documents. BERTopic  clusters the document embeddings and approximates the topic distributions by comparing documents to each document cluster. Different from simple clustering in these studies, we focus on explicitly modeling the complex relations among the embeddings of documents, topics, and words, which enhances topic modeling performance.

Some recent studies leverage large language models and describe topics as conceptual descriptions , rather than the word distributions in LDA . They can reach higher interpretability, but we emphasize their two limitations: (i) They require more resources. They need to input each document as prompts to LLMs. This is time-consuming and computationally intensive, especially when handling large-scale datasets. (ii) They cannot produce precise distributions for topics and documents, which limits their applications in downstream tasks.

## 3 Methodology: FASTopic

In this section, we first recall the problem setting of topic modeling. Then we propose the new paradigm Dual Semantic-relation Reconstruction (DSR) and the novel Embedding Transport Plan (ETP) method. Finally we introduce our new **FASTopic**.

### Problem Setting and Notations

Consider a collection \(\{^{(1)},,^{(N)}\}\) with \(N\) documents and vocabulary size \(V\). Topic modeling targets to discover \(K\) latent topics from the collection. Following LDA , Topic\(\#k\) is defined as a distribution over all words, _i.e.,_ topic-word distribution, denoted as \(_{k}\!\!^{V}\). We have \(\!=\!(_{1},,_{K})\! \!\!\!\!^{V K}\) as the topic-word distribution matrix of all topics. Topic modeling also infers the topic distributions of a document (what topics a document contains), _i.e.,_ doc-topic distribution. We denote the doc-topic distribution of \(^{(i)}\) as \(^{(i)}\!\!\!\!_{K}\), with \(_{K}\) as a probability simplex.

### Dual Semantic-relation Reconstruction

In this section, we propose a new, neat, and efficient paradigm for topic modeling, **Dual Semantic-relation Reconstruction** (**DSR**). Figure 2 illustrates the differences between DSR and previous VAE-based and clustering-based methods.

Parameterizing Documents, Topics, and WordsAt the beginning, we parameterize documents, topics, and words as embeddings. Specifically, we embed documents into an \(H\)-dimensional semantic space via a pretrained Transformer \(f_{}\), _e.g.,_ BERT  or Sentence-BERT . Let \((_{1},,_{N}){}^{H N}\) denote all document embeddings, where \(_{i}{=}f_{}(^{(i)})\) refers to the embedding of \(i\)-th document. Then we randomly project all topics and words into the same semantic space as \(K\) topic embeddings \((_{1},,_{K}){}^{H K}\) and \(V\) word embeddings \((_{1},,_{V}){}^{H V}\). Notably we do _not_ use pretrained word embeddings like word2vec  or GloVe , because they may not belong to the same semantic space as document embeddings, which hinders correctly measuring their distance.

Reconstruction through Dual Semantic RelationsThen we model the dual semantic relations between the embeddings of (1) documents and topics, and (2) topics and words. We interpret these relations as doc-topic distributions and topic-word distributions respectively. To be specific, we model \(_{k}^{(i)}\), the probability of Topic#k given \(i\)-th document as the semantic relation between embeddings \(_{i}\) and \(_{k}\). Similarly, we model \(_{jk}\), the probability of \(j\)-th word given Topic#k as the semantic relation between embeddings \(_{k}\) and \(_{j}\). We detail how to model them later in Sec. 3.3. We learn these relations through reconstruction. As shown in Figure 2, we reconstruct document \(^{(i)}\) as \(^{(i)}\), where we transport the semantics from \(^{(i)}\) to each topic through \(^{(i)}\) and then from each topic to each word through \(\). Hence we formulate the objective for DSR as the reconstruction error:

\[_{}=-_{i=1}^{N}(^{(i)})^{} (^{(i)})\] (1)

where we transform \(^{(i)}\) into the Bag-of-Words following previous studies [42; 43; 61]. By minimizing this objective, we expect to push each topic embedding close to the embeddings of its semantically related documents and words; therefore we can learn informative semantic relations, _i.e.,_ meaningful topic-word distributions \(\) for latent topics and doc-topic distributions \(^{(i)}\) for documents.

The above DSR presents a neat and efficient topic modeling paradigm. Previous VAE-based methods incorporate complicated modeling structures with diverse objectives [5; 80; 73; 1]. Different from them, DSR solely includes the objective Eq. (1) that only involves the embeddings of documents, topics, and words. This sufficiently simplifies the topic modeling procedure and hence facilitates efficiency. Moreover, while prior clustering-based methods  depend on indirect approximations, DSR explicitly models topic-word distributions and doc-topic distributions, resulting in higher effectiveness (See experiments in Sec. 4).

Figure 3: **(a, b)**: Relation weights of topics to documents. **(c, d)**: t-SNE visualization  of document (), and topic () embeddings under 50 topics (\(K{=}50\)). While most topic embeddings gather together in Parameterized Softmax (a,c) as it causes biased relations, ETP (b,d) separates all topic embeddings with regularized relations, avoiding the bias issue.

### Embedding Transport Plan

In this section, we analyze how to model the semantic relations for topic modeling, and then propose a new solution **Embedding Transport Plan** (**ETP**).

How to Model Semantic Relations?We emphasize this is a _non-trivial_ problem. A common answer is the widely-used parameterized softmax function [28; 37; 21]:

\[_{k}^{(i)}=_{i}-_{k}\|^{2}/)}{ _{k^{}=1}^{K}(-\|_{i}-_{k^{}}\|^{2}/) },_{jk}=_{k}-_{j}\|^{2}/)}{ _{j^{}=1}^{V}(-\|_{k}-_{j^{}}\|^{2}/ )}\] (2)

where we measure the relation between embeddings as their Euclidean distance with hyperparameter \(\). Unfortunately, this straightforward way is ineffective, because it incurs the **relation bias issue**: most relations are minor as quantitatively illustrated in Figure 2(a). In consequence, most topic embeddings fail to cover informative and distinct semantics but gather together in the space as shown in Figures 2(c) and 2(a). This issue leads to repetitive topics and less accurate doc-topic distributions (See ablation studies in Sec. 4.7). Someone may guess this issue is because of the large topic number. We note that the relation bias issue still happens even under a small number of topics (See experimental supports in Table 9). Owing to these, we need alternatives to model the semantic relations.

Transport Plan from Documents to TopicsMotivated by the above analysis, we propose the new Embedding Transport Plan (ETP) to address the relation bias issue with effective regularization.

To regularize relations, we model them as the transport plan of a specifically defined optimal transport problem. In detail, we define two discrete measures \(_{1}\) and \(_{1}\) over document and topic embeddings: \(_{1}\)=\(_{i=1}^{N}_{_{i}}\) and \(_{1}\)=\(_{k=1}^{K}s_{k}_{_{k}}\), where \(_{x}\) denotes the Dirac unit mass on \(x\). We set the weight of each document embedding as \(1/N\) and the weight of each topic embedding as \(s_{k}\), where \(\)=\((s_{1},,s_{K})\) is a weight vector summing to 1. This later produces normalized doc-topic distributions. With these two, we formulate their entropic regularized optimal transport problem as

\[*{arg\,min}_{_{+}^{N K}}_ {}(_{1},_{1};_{1})\!\!=\!\!_{i=1}^{N}_{k =1}^{K}\!C_{ik}^{(1)}_{ik}\!+\!_{1}_{ik}(_{ik}\!\!-\! 1),_{K}\!\!=\!\!_{N}}{N}^{}_{N}\!\!=\!\!.\] (3)

The first term is the original optimal transport problem, and the second term is the entropic regularization with hyperparameter \(_{1}\) to make this problem tractable [13; 52]. This equation aims to find a transport plan \(\) that minimizes the total cost of transporting the weights of document embeddings to topic embeddings under the two conditions , where \(_{K}\) denotes a \(K\)-dimensional column vector of ones. Here \(_{ik}\) refers to the transport weight from \(_{i}\) to \(_{k}\). The transport cost between them is measured as Euclidean distance \(C_{ik}^{(1)}\!\!=\!\!\|_{i}-_{k}\|^{2}\), with \(^{(1)}\) as the transport cost matrix.

We can alleviate the relation bias issue with transport plan \(\) as the semantic relations. Eq. (3) constraints \(\) by two conditions. We set \(\!=\!*{softmax}(_{0})\), where \(_{0}\) is a learnable variable uniformly initialized as \(_{K}\). The uniform initialization and softmax function prevent excessively biased \(\). Therefore as illustrated in Figures 2(b) and 2(d), this avoids biased \(\) as it is constrained by \(\), which mitigates the relation bias issue (See experiment results in Sec. 4.7). Besides, this approach flexibly captures the varying weight of each topic within the document collection, since some topics may appear more frequently in the collection while others less, which aligns with the reality (See more interpretations in Appendix E).

Transport Plan from Topics to WordsWe further employ the transport plan between topic and word embeddings. We define two discrete measures over topic and word embeddings: \(_{2}\!=\!_{k=1}^{K}_{_{k}}\) and \(_{2}\!\!=\!\!_{j=1}^{V}u_{j}_{_{j}}\). Here we specify the weight of each topic embedding as \(1/K\) and the weight of each word embedding as \(u_{j}\), where \(\) is a weight vector and its sum is 1. This is to produce normalized topic-word distributions later. Following Eq. (3), we write the entropic regularized optimal transport problem between the two measures as

\[*{arg\,min}_{_{+}^{K V}}_ {}(_{2},_{2};_{2})\!\!=\!\!_{k=1}^{K}_{j =1}^{V}C_{kj}^{(2)}_{kj}\!+\!_{kj}(_{kj}\!\!-\!1), _{K}\!\!=\!\!_{K}}{K}^{}_{K}\!\!=\!\!.\] (4)

[MISSING_PAGE_FAIL:6]

 also solve the optimal transport problem, but their objectives involve complicated encoders and decoders inherent from VAE, which slows them down.

Moreover, FASTopic needs much fewer hyperparameters. It mainly has hyperparameters for Sinkhorn's algorithm \(_{1}\) and \(_{2}\) in Eq. (3) and (4). VAE-based models, like CombinedTM , ECRTM , and GINopic , require hyperparameters to set their encoders, decoders (dimensions, number of layers, and dropout), and prior distributions (Gaussian or Dirichlet). BERTopic needs hyperparameters to set its clustering and dimension reduction modules, like the number of neighbors and components of UMAP; the min cluster size, min samples, metrics of HDBSCAN.

### Inferring Doc-Topic distributions for New Documents

Finally we discuss how to infer doc-topic distributions for new documents. Considering a new document \(^{}\) and its document embedding \(^{}{=}f_{}(^{})\), we may directly follow the learning process in Sec. 3.3 and infer its doc-topic distribution \(^{}\) by computing the transport plan between \(^{}\) and learned topic embeddings \(\). Unfortunately, this way is unworkable. It transports the weight of one document to all topics; hence for any \(^{}\), the transport plan invariably becomes the learned topic weights \(\). Such trivial results are certainly unreasonable. To this end, we compute \(^{}\) as

\[^{}_{k}=}{_{k^{}=1}^{K}p_{k^{}}},  p_{k}=_{k}-^{}\|^{2}/ )}{_{i=1}^{N}(-\|_{k}-_{i}\|^{2}/)}\] (9)

with \(\) as a temperature hyperparameter. Here we model the relation between \(^{}\) and \(_{k}\) as the Euclidean distance and regularize it by the total relations between \(_{k}\) and all training documents to approximate the learned topic weight in Sec. 3.3. Then we compute \(^{}_{k}\) by normalizing over all topics. Since topic and word embeddings have been refined after training (Sec. 3.4), we can infer accurate doc-topic distributions for new documents in this way. See empirical results in Sec. 4.2 and 4.3.

## 4 Experiment

In this section we conduct comprehensive experiments to demonstrate that our FASTopic is fast, adaptive, stable, and transferable.

    &  &  &  \\  LDA-Mallet & 58.6 & 70.0 & 50.2 & 702.6 & 974.0 & 2083.0 \\ NMF & 87.0 & 81.4 & 74.8 & 268.6 & 399.4 & 939.0 \\ BERtopic & 34.2 & 35.0 & 55.6 & 66.8 & 114.2 \\ CombinedTM & 53.4 & 31.8 & 45.2 & 67.2 & 93.0 & 237.4 \\ GINopic & 41.7 & 334.2 & 309.2 & 905.8 & 664.8 & 1878.2 \\ ProGBN & 337.0 & 765.8 & 831.0 & 67.5 & 864.2 & 2180.2 \\ HyperMiner & 40.3 & 40.5 & 0.57 & 91.3 & 15.6 & 103.9 \\ ECRTM & 365.4 & 274.8 & 290.4 & 287.8 & 325.4 & 1270.0 \\ 
**FASTopic** & **10.6** & **12.5** & **12.4** & **18.3** & **60.3** & **50.5** \\   

Table 3: Running time (in seconds) on different datasets. The best is in **bold**.

Figure 4: (**Left**): Text classification results of Accuracy (Acc) and F1. (**Right**): Transferability results. We use topic models trained on Wikitext-103 to infer the doc-topic distributions of other datasets. The best is in **bold**.

    &  &  &  \\   &  **20NG** \\  } &  **NYT** \\  } &  **WoS** \\  } &  **NeurIPS** \\  } &  **ACL** \\  } &  **Wakit-103** \\  } \\  & & & & & \\  LDA-Mallet & 58.6 & 70.0 & 50.2 & 702.6 & 974.0 & 2083.0 \\ NMF & 87.0 & 81.4 & 74.8 & 268.6 & 399.4 & 939.0 \\ BERtopic & 34.2 & 35.2 & 35.0 & 55.6 & 66.8 & 114.2 \\ CombinedTM & 53.4 & 31.8 & 45.2 & 67.2 & 93.0 & 237.4 \\ GINopic & 41.7 & 334.2 & 309.2 & 905.8 & 664.8 & 1878.2 \\ ProGBN & 337.0 & 765.8 & 831.0 & 67.5 & 864.2 & 2180.2 \\ HyperMiner & 40.3 & 40.5 & 0.57 & 91.3 & 15.6 & 103.9 \\ ECRTM & 365.0 & 52.4 & 274.8 & 290.4 & 287.8 & 325.4 & 1270.0 \\ 
**FASTopic** & **10.6** & **12.5** & **12.4** & **18.3** & **60.3** & **50.5** \\   

Table 3: Running time (in seconds) on different datasets. The best is in **bold**.

### Experiment Setup

DatasetsWe adopt six benchmark datasets for experiments: **(i) 20NG** [20Newsgroup, 33] is one of the most commonly-used datasets, covering news articles with 20 labels. **(ii) NYT** includes news articles from the New York Times with 12 categories. **(iii) WoS** [Web Of Science, 32] contains published papers from the Web of Science website with 7 categories. **(iv) NeurIPS** is a dataset with papers published at the NeurIPS conference from 1987 to 2017. **(v) ACL**  contains research articles from the ACL anthology from 1970 to 2015. **(vi) Wikitext-103** includes Wikipedia articles. See more dataset details in Appendix B.

Evaluation MetricsAlthough topic modeling evaluation is still an open problem , we follow mainstream studies [19; 91; 73] and evaluate the topic quality and doc-topic distribution quality. For topic quality, we consider: **(i) Topic Coherence** measures the coherence between top words of discovered topics. We employ the widely-used coherence metric \(C_{V}\), which has been shown to outperform earlier NPMI, UCI, and UMass [46; 34; 56]. We use a widely-used large Wikipedia article collection as the external reference corpus to compute \(C_{V}\). **(ii) Topic Diversity** means the differences between discovered topics. We measure this with the Topic Diversity (TD) metric , which calculates the proportion of unique words in the topics. In terms of doc-topic distribution quality, we conduct document clustering, evaluated by Purity and NMI  following Zhao et al. . We do not evaluate the perplexity since our method does _not_ follow the VAE framework [42; 43], and the perplexity is incomparable across topic models as evidenced by early studies [90; 75].

Baseline ModelsWe consider the following baselines in three paradigms. For conventional topic models, we adopt **(i) LDA-Mallet**, a prominent method competitive to some neural models ; **(ii) NMF**, using non-negative matrix factorization. For clustering-based topic models, we have **(iii) BERTopic**, clustering document embeddings and discovering topics by TF-IDF. For VAE-based neural topic models, we include **(iv) CombinedTM**, combining contextual features and BoW as inputs; **(v) GINopic**, following CombinedTM but using graph isomorphism networks; **(vi) HyperMiner**, using hyperbolic embeddings to model topics; **(vii) ProGBN**, progressively generating documents of different levels with graph decoders; **(viii) ECRTM**,

    &  &  &  &  &  &  \\   & \(C_{V}\) & TD & \(C_{V}\) & TD & \(C_{V}\) & TD & \(C_{V}\) & TD & \(C_{V}\) & TD & \(C_{V}\) & TD \\  LDA-Mallet & \({}^{1}\)0.360 & 10.847 & 10.361 & 10.828 & 10.363 & 10.806 & 10.364 & 10.786 & 10.369 & 10.781 & 10.371 & 10.755 \\ NMF & \({}^{1}\)0.393 & 0.415 & 10.390 & 10.382 & 10.392 & 10.357 & 10.390 & 10.332 & 10.391 & 10.307 & 10.393 & 10.298 \\ BERTopic & \({}^{1}\)0.444 & 10.638 & 10.450 & 10.607 & 0.458 & 10.566 & 0.457 & 10.529 & 0.457 & 10.543 & 0.455 & 10.541 \\ CombinedTM & \({}^{1}\)0.388 & 10.482 & 10.385 & 10.473 & 10.388 & 10.455 & 10.387 & 10.428 & 10.390 & 10.425 & 10.389 & 10.443 \\ GINopic & \({}^{1}\)0.369 & 10.485 & 10.372 & 10.445 & 10.369 & 10.432 & 10.372 & 10.445 & 10.377 & 10.446 & 10.375 & 10.447 \\ ProGBN & \({}^{1}\)0.380 & 10.732 & 10.380 & 10.673 & 10.380 & 10.599 & 10.379 & 10.556 & 10.374 & 10.472 & 10.375 & 10.444 \\ HyperMiner & \({}^{1}\)0.356 & 10.586 & 10.530 & 10.595 & 10.351 & 10.568 & 10.352 & 10.547 & 10.349 & 10.518 & 10.355 & 10.490 \\ ECRTM & \({}^{1}\)**0.482** & 10.974 & 10.477 & 10.951 & 10.461 & 10.958 & 10.458 & 10.957 & 10.448 & 0.953 & 10.437 & 10.941 \\ 
**FASTTopic** & 0.465 & **0.998** & 0.464 & **0.993** & 0.460 & **0.984** & **0.459** & **0.972** & **0.458** & **0.959** & **0.456** & **0.949** \\   

Table 4: Topic quality results of \(C_{V}\) (topic coherence) and TD (topic diversity) under different topic numbers (\(K\)). The best is in **bold**.

    &  &  &  &  &  &  \\   & Purity & NMI & Purity & NMI & Purity & NMI & Purity & NMI & Purity & NMI & Purity & NMI \\  LDA-Mallet & \({}^{1}\)0.661 & \({}^{1}\)0.333 & \({}^{1}\)0.651 & \({}^{1}\)0.322 & \({}^{1}\)0.660 & \({}^{1}\)0.325 & \({}^{1}\)0.675 & \({}^{1}\)0.332 & \({}^{1}\)0.684 & \({}^{1}\)0.332 & \({}^{1}\)0.676 & \({}^{1}\)0.3a state-of-the-art method by regularizing embeddings with optimal transport. We fine-tune the hyperparameters of these baselines under different datasets and topic numbers.

### Effectiveness: Topic Quality and Doc-Topic Distribution Quality

We demonstrate the superior effectiveness of FASTopic compared to state-of-the-art baselines. Table 1 presents the topic quality results of topic coherence (\(C_{V}\)) and topic diversity (TD). We see that our FASTopic commonly surpasses all baselines with the highest performance across all datasets. Moreover, Table 2 reports the doc-topic distribution quality results concerning the Purity and NMI of document clustering. We observe that FASTopic reaches top performance as well. These results manifest that FASTopic produces high-quality topics and doc-topic distributions, showing better effectiveness. This also verifies the capability of our new DSR paradigm. See Appendix H for the examples of discovered topics.

### Effectiveness: Text Classification as Downstream Task

We consider text classification as a downstream task to evaluate topic models in an extrinsic manner. Following Wu et al. , we train SVM classifiers with the inferred doc-topic distributions as document features and then predict the class of each testing document. We measure this performance by Accuracy (Acc) and F1. Figure 4 reports that our FASTopic consistently outperforms baselines. We note that the improvements of FASTopic are statistically significant at 0.01 level. These results demonstrate that FASTopic can benefit more downstream classification tasks.

### Efficiency: Running Speed

We show the exceptionally fast running speed of FASTopic. Table 3 reports the running time of each model on each dataset. The running time indicates the duration from the completion of data loading to the finish of training. We see that our FASTopic consistently emerges as the fastest one by a large margin, statistically significant at 0.01 level. FASTopic completes running within 1 minute, while the longest takes 30 minutes. We notice that LDA-Mallet has increasing running time on the datasets with longer documents. For instance, it escalates from 50 seconds on 20NG to 2000 seconds on Wikitext-103. In contrast, FASTopic maintains its rapid performance regardless of document length. Figure 0(b) also evidences the fast speed of FASTopic in terms of varying dataset sizes. This is because FASTpoic adopts our neat and efficient DSR paradigm, which relieves from the complicated modeling structures in previous studies. See more running time analysis in Appendix G.

### Transferability

We verify the high transferability of FASTopic. In detail, we train a topic model on Wikitext-103, a general dataset with diverse topics, and then use it to infer the doc-topic distributions of documents in other datasets 20NG, NYT, and WoS. Following the previous setting, we use these doc-topic distributions as features to train SVM classifiers for text classification. This measures the transferability of a topic model from one data domain to another. Figure 4 shows that the transferability of FASTopic significantly outperforms baselines. The reason lies in that previous methods often rely on the Bag-of-Words [80; 20; 73]. Differently, FASTopic leverages richer representations, the pretrained document embeddings, and learns the doc-topic distributions through the effective ETP method, bringing about higher transferability.

### Adaptivity and Stability

We demonstrate the adaptivity and stability of FASTopic across various scenarios using the WoS dataset. First, Tables 4 and 5 summarize the performance under different topic numbers (\(K\) from 75 to 200). We observe that FASTopic generally remains top performance across these variations. Second, Tables 13 and 14 report the results under varying dataset sizes (\(N\) from 15k to 40k). These results show that FASTopic mostly reaches the best results. As aforementioned in Figure 0(b), FASTopic also has the fastest running speed. Third, we experiment with different vocabulary sizes (\(V\) from 20k to 50k) in Tables 15 and 16. Similarly, our FASTopic exhibits stable and high performance. We note that FASTopic uses the same hyperparameters in all these experiments (See Appendix D). The above results together highlight that our FASTopic can smoothly adapt to various scenarios with stable performance. This is a vital advantage of our model for practical applications.

### Ablation Study

We validate the necessity of our Embedding Transport Plan (ETP) method with ablation studies. Table 6 shows that using parameterized softmax rather than ETP (w/o ETP) to model semantic relations incurs degraded performance, concerning both topic and doc-topic distribution quality (See also the results on other datasets in Table 8). For instance, the \(C_{V}\) and TD decrease from 0.426, 0.983 to 0.368, 0.391; the Purity and NMI decrease from 0.577, 0.525 to 0.401, 0.452, indicating low-quality repetitive topics and less accurate doc-topic distributions. We observe similar results even with only 10 topics (\(K\)=10) in Table 9. This is because our ETP properly regularizes semantic relations, which addresses the relation bias issue. These results manifest the necessity of our ETP to reach effective topic modeling.

## 5 Model Usage

We have released our FASTopic as a Python package at PyPI 2. Users can easily install FASTopic through _pip_. Figure 5 shows a code example to use FASTopic on a dataset. After preprocessing the given dataset, it discovers top words and infers doc-topic distributions. With these simple APIs, users can smoothly handle their data for their various purposes. See our GitHub 3 for more tutorials and documentation of FASTopic.

## 6 Conclusion

In this paper, we propose FASTopic, a fast, adaptive, stable, and transferable topic model. Rather than traditional VAE-based or clustering-based approaches, FASTopic employs the new dual semantic-relation reconstruction paradigm to model latent topics with semantic relations and uses the new transport plan relation method to tackle the relation bias issue. Comprehensive experiments demonstrate the significantly superior performance of FASTopic in terms of effectiveness, efficiency, adaptivity, stability, and transferability. These advantages manifest the strong capability of FASTopic in practice, which benefits a wide range of real-world applications.

    &  &  &  \\   & \(C_{V}\) & TD & Purity & NMI & \(C_{V}\) & TD & Purity & NMI & \(C_{V}\) & TD & Purity & NMI \\  w/o ETP & \({}^{1}\)0.368 & \({}^{1}\)0.391 & \({}^{1}\)0.401 & \({}^{1}\)0.452 & \({}^{1}\)0.363 & \({}^{1}\)0.338 & \({}^{1}\)0.553 & \({}^{1}\)0.294 & \({}^{1}\)0.395 & \({}^{1}\)0.522 & \({}^{1}\)0.653 & \({}^{1}\)0.350 \\
**FASTopic** & 0.426 & 0.983 & 0.577 & 0.525 & 0.437 & 0.999 & 0.662 & 0.369 & 0.457 & 1.000 & 0.672 & 0.365 \\   

Table 6: Ablation study. w/o ETP means using parameterized softmax (Eq. (2)) to model semantic relations. See also Table 8 for results on other datasets.

Figure 5: A code example of using FASTopic. Install FASTopic via _pip_ and use its APIs to handle a dataset.