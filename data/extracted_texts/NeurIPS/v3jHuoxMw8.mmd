# Vision-Language Navigation with Energy-Based Policy

Rui Liu\({}^{1,2}\)  Wenguan Wang\({}^{2}\)  Yi Yang\({}^{1,2,}\)

\({}^{1}\)The State Key Lab of Brain-Machine Intelligence, Zhejiang University, Hangzhou, China

\({}^{2}\)College of Computer Science and Technology, Zhejiang University, Hangzhou, China

https://github.com/DefaultRui/VLN-ENP

Corresponding author: Yi Yang.

\({}^{2}\)These approaches require a slightly more interactive setting than traditional imitation learning, allowing the learner to query the expert at any navigation state during training. Previous studies  have adopted this setting, and this assume is feasible for many real-world imitation learning tasks .

###### Abstract

Vision-language navigation (VLN) requires an agent to execute actions following human instructions. Existing VLN models are optimized through expert demonstrations by supervised behavioural cloning or incorporating manual reward engineering. While straightforward, these efforts overlook the accumulation of errors in the Markov decision process, and struggle to match the distribution of the expert policy. Going beyond this, we propose an Energy-based Navigation Policy (Enp) to model the joint state-action distribution using an energy-based model. At each step, low energy values correspond to the state-action pairs that the expert is most likely to perform, and vice versa. Theoretically, the optimization objective is equivalent to minimizing the forward divergence between the occupancy measure of the expert and ours. Consequently, Enp learns to globally align with the expert policy by maximizing the likelihood of the actions and modeling the dynamics of the navigation states in a collaborative manner. With a variety of VLN architectures, Enp achieves promising performances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing VLN models.

## 1 Introduction

Vision-language navigation (VLN)  entails an embodied agent that executes a sequence of actions to traverse complex environments based on natural language instructions. VLN is typically considered as a Partially Observable Markov Decision Process (POMDP) , where future states are determined only by the current state and current action without explicit rewards. Therefore, the central challenge for VLN is to learn an effective navigation policy. In this context, existing neural agents  are naturally constructed as data-driven policy networks by imitation learning (IL)  and bootstrapping from expert demonstrations, _i.e._, ground-truth trajectories .

Most VLN models  utilize behavioural cloning (BC), a classic approach for IL, through supervised training. While conceptually simple, BC suffers seriously from quadratic accumulation errors  along the trajectory due to covariate shift, especially in partially observable settings. Several efforts introduce'student-forcing'  and 'pseudo interactive demonstrator' , which are essentially online versions of DAgger, to alleviate this distribution mismatch. DAgger assumes interactive access to expert policies (_i.e._, the shortest path from current state to the goal2) and reduces to linear expected errors . Some other studies  combine reinforcement learning (RL)  and IL algorithms to mitigate the limitations of BC. Though effective, it presents challenges in designing an optimal reward function  and demands careful manual reward engineering. Such reward functions may not be robust to changes in the environment dynamics andstruggle to generalize well across diverse scenes . In addition, recent research  has revealed that training transformer -based VLN models using RL would introduce instability.

From a probabilistic view, most VLN agents [1; 3; 4; 6] essentially train a discriminative action classifier [8; 3; 4], and model the action distribution conditioned on current navigation state \(P(a|s)\) at each time step (Fig. 1). This classifier is supervised by the cross-entropy loss to minimize 1-step deviation error of single-step decision along trajectories . However, the prediction of current action affects future states during the global execution of the learned policy, potentially leading to catastrophic failures [9; 12]. In other words, solely optimizing the conditional action probabilities (_i.e._, policy) remains unclear for the underlying reasons of the expert behaviour, yet fails to exploit complete information in the state-action distribution \(P(s,a)\) induced by the expert .

In light of this, we propose an Energy-based Navigation Policy (Enp) framework that addresses the limitations of current policy learning in VLN from an energy perspective (Fig. 1). Energy-based models (EBMs)  are flexible and capable of modeling expressive distributions, as they impose minimal restrictions on the tractability of the normalizing constant (partition function) . We reinterpret the VLN model as an energy-based model to learn the expert policy from the expert demonstrations. Specifically, Enp models the joint distribution of state-action pairs, _i.e._, \(P(s,a)\), by energy function, and assigns low energy values to the state-action pairs that the expert mostly performs. For optimization, this energy function is regarded as the unnormalized log-density of joint distribution, which can be decomposed into the standard cross-entropy of action prediction and marginal distribution of the states \(P(s)\). During training, Enp introduces persistent contrastive divergence [21; 22] to estimate the expectation of \(P(s)\), and samples by Stochastic Gradient Langevin Dynamics . In this way, Enp simulates the expert by maximizing the likelihood of the action and incorporates prior knowledge of VLN by optimizing state marginal distribution in a collaborative manner, thereby leveraging the benefits of both energy-based and discriminative approaches.

Theoretically, the training objective of Enp is to maximize the expected \(\)-likelihood function over the joint distribution \(P(s,a)\). This is equivalent to estimating the unnormalized probability density (_i.e._, energy) of the expert's occupancy measure [15; 24; 25]. Therefore, Enp performs prioritized optimization for entire trajectories rather than the single-time step decisions in BC, and achieves a global alignment with the expert policy. Furthermore, we realize that Enp is closely related to Inverse Reinforcement Learning (IRL) [7; 26; 15]. For the optimized objective, Enp shares similarities with IRL but minimizes the forward KL divergence [27; 28] between the occupancy measures.

For thorough examination (SS4), we explore Enp across several representative VLN architectures [3; 4; 6; 29]. Experimental results demonstrate that Enp outperforms the counterparts, _e.g._, \(\%\) SR and \(\%\) SPL gains over R2R , \(\%\) RGS on REVERIE , \(\%\) SR on R2R-CE , and \(\%\) NDTW on RxR-CE , respectively. Enp not only helps release the power of existing VLN models, but also evidences the merits of energy-based approaches in the challenging VLN decision-making.

## 2 Related Work

**Vision-Language Navigation (VLN).** Early VLN agents [1; 13; 3; 33] are built upon a recurrent neural policy network within a sequence-to-sequence framework [34; 35] to predict action distribution.

Figure 1: Comparison of behavioural cloning (BC) and Enp for VLN. Previous methods use BC to optimize the conditional action distribution directly. Enp models the joint state-action distribution through an energy-based model. The low energy values correspond to the state-action pairs that the expert is most likely to perform.

As compressing all history into a hidden vector might be sub-optimal for state tracking across extended trajectories, later attempts [36; 37; 6; 38] incorporate a memory buffer (_e.g._, topological graph) for long-term planning. Recent efforts [39; 5; 4] are devoted to encode complete episode history of navigation states and actions by transformer  and optimize the whole model in end-to-end training. Later, various strategies have been proposed to improve the generalization of policies in both seen and unseen environments [3; 8; 40; 41], such as progress estimation , instruction generation [33; 43; 44; 45; 46], backtracking , cross-modal matching [48; 49], self-supervised pre-training [50; 51; 52], environmental augmentation [3; 53; 54; 55], visual landmarks [56; 57], exploration [58; 59], map building [60; 61; 62; 63], waypoint prediction , and foundation models [65; 66].

**Policy Learning in VLN.** VLN can be viewed as a _POMDP_, providing several expert demonstrations sourced from ground-truth shortest-path trajectories [1; 30] or human demonstrations . Behavioural cloning (BC) [67; 68], a typical approach of imitation learning (IL) [7; 15], is widely used in current VLN agents [1; 33; 4; 6] with supervised policy learning. Nevertheless, it suffers from distribution shifts between training and testing [12; 69].  introduces'student-forcing'  training regime from sequence prediction to mitigate this limitation. Later agents [3; 8; 4; 5] combine both IL and model-free RL [14; 70] for policy learning, where the reward function is manually designed on the distance metric. Instead of directly mapping visual observations into actions or state-action values, [13; 71] investigate model-based RL [72; 73] for look-ahead planning, exploring the potential future states. As reward engineering requires careful manual tuning,  proposes to learn reward functions from the expert distribution directly. Although RL is an effective approach in principle, DUET  finds it difficult to train large-scale transformers with inaccurate and sparse RL rewards  in VLN. Existing studies [61; 62; 63] use an interactive demonstrator (similar to DAgger) to generate additional pseudo demonstrations and achieve promising performance.

In this paper, Enp learns to align with the expert policy by matching the joint state-action distribution for entire trajectories from the occupancy measure view. Furthermore, Enp reinterprets the optimization of policy into an energy-based model, representing an expressive probability distribution.

**Energy-based Model.** Energy-based models (EBMs)  are non-normalized probabilistic models that represent data using a Boltzmann distribution. In EBMs, each sample is associated with an energy value, where high-density regions correspond to lower energy . A range of MCMC-based [76; 21] and MCMC-free [77; 78] approaches can be adopted to estimate the gradient of the \(\)-likelihood. EBMs have been widely utilized in generation , perception , and policy learning [80; 81]. The most related work [25; 24; 22] will be discussed below.  employs EBM for distribution matching, strictly prohibiting interaction with the environment. Moreover, this approach is applied in an offline setting, specifically for healthcare.  considers the expert energy as the reward function and leverages this reward for reinforcement learning.  adopts parameterized energy functions to model the conditional action distribution, optimizing for actions that minimize the energy landscape.

In contrast, Enp devotes to learning the joint distribution of state-action pairs from the expert demonstrations online. An external marginal state memory is introduced to store the historical information for initializing the samples for next iterations. Enp demonstrates the potential of EBMs in VLN decision-making, and improves the existing VLN agents across various frameworks.

## 3 Method

In this section, we first formalize the VLN task and discuss the limitations of existing work from a probabilistic view (SS3.1). Then we introduce Energy-based Navigation Policy learning framework - Enp (SS3.2). Furthermore, Enp is compared with other imitation learning methods based on divergence minimization (SS3.3). Finally, we provide the implementation details (SS3.4).

### Problem Formulation

VLN is typically formulated as a POMDP [2; 51] with a finite horizon \(T\), defined as the tuple \(<\)\(,,,\), \(_{0}\)\(>\). \(\) is the navigation state space. \(\) is the action space. Note that we use panoramic action space , which encodes high-level behaviour. The transition distribution \((s_{t+1}|s_{t},a_{t})\) (Markov property) of the environment, the reward function \((s_{t},a_{t})\), and the initial state distribution \(_{0}\) are unknown in VLN and \(\) can only be queried through interaction with the environment.

In the standard VLN setting , a training dataset \(=\{(x,)\}\) consists of pairs of the language instructions \(x\) and corresponding expert trajectories \(\). At step \(t\), the agent acquires an egocentric observation \(O_{t}\), and then takes an action \(a_{t}\) from \(K_{t}\) candidates based on the instruction \(x\). The agent needs to learn a navigation policy \(^{*}(a|s)\) and predict the action distribution \(P(a_{t}|_{t})\).

Recent VLN agents employ Seq2Seq [8; 3] or transformer-based networks [4; 6] as a combination of a cross-modal state encoder \(_{_{s}}\) and a conditional action classifier \(_{_{a}}\) for action prediction. Hence they are usually built as a composition of \(_{_{s}}_{_{a}}\). There are different state representations in previous work [51; 4], here we define a more general one. The cross-modal state encoder extracts a joint state representation \(_{t}=_{_{s}}(,_{t},_{t}) ^{K_{t} D}\) by the visual embedding \(_{t}\), the instruction embedding \(\), and history embedding \(_{t}\) along the trajectory \(\). Concretely, \(_{t}\) comes from recurrent hidden units of Seq2Seq framework or an external memory module (_e.g._, topological graph ). Then, the action classifier maps the state representation \(_{t}\) into an action-related vector \(_{t}=\{_{t}[k]\}_{k=1}^{K_{t}}^{K_{t}}\), _i.e._, \(_{t}=_{_{s}}(_{t})\), and \(_{t}[k]\) is termed as the logit for \(k\)-th action candidate. Then the probability of \(a_{t,k}\) is computed by softmax, _i.e._, \(P(a_{t,k}|_{t})=_{t}[k])}{_{k=1}^{K}(_{t} [k])}\).

The cross-modal state encoder \(_{_{a}}\) and action classifier \(_{_{a}}\) usually employ a joint end-to-end training. Imitation learning (IL) aims to learn the policy from expert demonstrations \(\) without any reward signals. Behaviour cloning (BC)  is a straightforward approach of IL by maximizing likelihood estimation of \(^{*}(a|s)\), _i.e._, minimizing the cross-entropy loss in a supervised way:

\[*{arg\,min}_{}-[^{*}(a|s)] _{}=_{(x,)}_{t}-  P_{}(a_{t}|_{t}),\] (1)

where \(=_{s}_{a}\). BC suffers from the covariate shift problem for the crucial _i.i.d._ assumption, as the execution of \(a_{t}\) will affect the distribution of future state \(_{t+1}\). Inspired by DAgger, some agents [1; 6] query the shortest paths (by expert policy \(^{}\)) during navigation, explore more trajectories \(\{^{+}\}\) by interacting with the environments, and aggregate these trajectories as \(^{+}=\{(x,)\}\{(x,^{+})\}\). Then, the policy is updated by mixing iterative learning . However, these traditional approaches ignore the changes in distribution and train a policy that performs well under the distribution of states encountered by the expert. When the agent navigates in the unseen environment and receives different observations than expert demonstrations, it will lead to a compounding of errors, as they provide no way to understand the underlying reasons for the expert behaviour .

### Energy-based Expert Policy Learning

Instead of solely training the discriminative action classifier at each step, we want to learn a policy \(^{*}\) by exploiting the complete information in the state-action distribution from the expert policy \(^{}\). To facilitate later analysis, the occupancy measure \(^{}(s,a)\) of the policy \(\) is introduced . \(^{}(s,a)\) denotes the stationary distribution of state-action pairs \((s,a)\) when an agent navigates with policy \(\) in the environment, defined as:

\[^{}(s,a)(a|s)_{t}P(s_{t}=s|)= _{t}P(s_{t}=s,a_{t}=a),\] (2)

and \(^{}(s,a)\) has a one-to-one relationship with policy \(\): \(^{}=^{^{*}}=^{*}\). Therefore, we induce a policy \(^{*}\) with \(^{*}(s,a)\) that matches the expert \(^{}(s,a)\) based on the joint distribution of state-action pairs \(P(s,a)\). \(t\) is omitted for brevity. The matching loss \(_{}\) is designed as:

\[*{arg\,min}-_{(s,a)^{}(s,a)}[^{*}(s, a)]_{}=-_{(x,)} P_{ }(s,a).\] (3)

Then we reformulate it from an energy view, as energy-based models (EBMs) [19; 79] parameterize an unnormalized data density and can represent a more expressive probability distribution. Specifically,

Figure 2: Overview of Enp. At each step \(t\), the agent acquires a series of observations, and predicts the next step based on the instruction and navigation history. Enp optimizes the marginal state matching loss \(_{S}\) through SGLD sampling from Marginal State Memory (Eq. 9), and minimizes the cross-entropy loss \(_{}\) jointly (Eq. 1).

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

Experiment

We examine the efficacy of Enp for VLN in discrete environments (SS4.1), and more challenging continuous environments (SS4.2). Then we provide diagnostic analysis on core model design (SS4.3).

### VLN in Discrete Environments

**Datasets.** R2R  contains \(7,189\) shortest-path trajectories captured from \(90\) real-world building-scale scenes . It consists of \(22\)K step-by-step navigation instructions. REVERIE  contains \(21,702\) high-level instructions, which requires an agent to reach the correct location and localize a remote target object. All these datasets are devided into _train_, _val seen_, _val unseen_, and _test unseen_ splits, which mainly focus on the generalization capability in unseen environments. Both R2R  and REVERIE  are built upon Matterport3D Simulator .

**Evaluation Metrics.** As in previous work , Success Rate (SR), Trajectory Length (TL), Oracle Success Rate (OSR), Success rate weighted by Path Length (SPL), and Navigation Error (NE) are used for R2R. In addition, Remote Grounding Success rate (RGS) and Remote Grounding Success weighted by Path Length (RGSPL) are adopted for object grounding in REVERIE.

**Performance on R2R.** Table 1 demonstrates the numerical results of Enp with different frameworks on R2R. With Seq2Seq-style neural architectures, Enp provides \(\%\) SR and \(\%\) SPL gains over En-vDrop  on _val unseen_. Adopting Enp in transformer-based models leads to promising performance gains on _test unseen_, \(\%\) on SR with VLN\(\)BERT , and \(\%\) on SR with DUET . The performance is still lower than BEVBert  and BSG  due to the additional semantic maps. Moreover, we provide the average success rate across different path lengths in Appendix (Fig. 4).

**Performance on REVERIE.** Table 2 presents the results of Enp with different transformer-based frameworks on REVERIE. Enp surpasses all the counterparts on both _val unseen_ and _test unseen_ by large margins, _e.g._, \(.34\%\) on SR and \(.6\%\) on RGS over VLN\(\)BERT , \(.68\%\) on SR and \(.22\%\) on RGS over DUET , verifying the efficacy of Enp. Inspired by recent efforts , potential improvement on object grounding can be achieved by introducing CLIP features .

**Qualitative Results.** In Fig. 3 (a), we illustrate the qualitative comparisons of Enp against DUET  on R2R _val unseen_. Based on Enp, our agent yields more accurate predictions than DUET. It verifies that Enp leads to better decision-making when facing challenging scenes. In Fig. 3 (b), our agent may fail due to the serious occlusion, and introducing map prediction  may alleviate this problem.

    &  &  \\   & TL\(\) & OSR\(\) & SR\(\) & SPL\(\) & RL\(\) & OSR\(\) & SPL\(\) & ROS\(\) & ROS\(\) \\   Seq2Seq  (\(\)) & 11.07 & 8.07 & 4.20 & 2.84 & 2.16 & 1.63 & 10.89 & 6.88 & 3.99 & 3.09 & 2.00 & 1.58 \\ RCM  & 11.98 & 14.23 & 9.29 & 6.97 & 4.89 & 3.89 & 10.60 & 11.68 & 7.84 & 6.67 & 3.14 \\ INP  (\(\)) & 45.28 & 28.20 & 14.40 & 7.19 & 7.84 & 4.67 & 39.05 & 30.63 & 19.88 & 11.61 & 11.28 & 6.08 \\ Aubert  (\(\)) & 18.71 & 34.51 & 27.89 & 21.88 & 18.23 & 14.18 & 17.91 & 34.20 & 30.28 & 23.61 & 16.83 & 13.28 \\ HAMT  & 14.08 & 36.84 & 39.25 & 30.20 & 18.92 & 17.28 & 13.62 & 33.41 & 30.40 & 26.67 & 14.88 & 13.08 \\ HOP  (\(\)) & 16.46 & 36.24 & 31.78 & 26.11 & 18.85 & 15.73 & 16.38 & 33.06 & 30.17 & 24.34 & 17.69 & 14.34 \\ TDT  (\(\)) & - & 39.48 & 34.88 & 27.32 & 21.16 & 16.56 & - & 40.46 & 35.89 & 25.71 & 19.88 & 15.40 \\ LANA  & 23.18 & 52.97 & 48.31 & 33.86 & 23.27 & 12.77 & 18.83 & 57.20 & 51.72 & 36.45 & 23.95 & 22.85 \\ BSG  (\(\)) & 24.71 & 50.85 & 51.22 & 35.95 & 35.36 & 24.24 & 22.90 & 62.83 & 56.45 & 38.70 & 33.15 & 22.34 \\ BEVBert  (\(\)) & - & 56.40 & 51.78 & 36.37 & 34.71 & 24.44 & - & 57.26 & 52.81 & 36.41 & 32.06 & 2.09 \\ VER  (\(\)) & 23.03 & 61.09 & 55.98 & 39.66 & 33.71 & 23.70 & 24.74 & 62.22 & 56.82 & 38.76 & 33.88 & 23.19 \\  VLN-BERT  (\(\)) & 16.78 & 35.02 & 30.67 & 24.90 & 18.77 & 15.27 & 15.86 & 32.91 & 29.61 & 23.99 & 16.50 & 13.51 \\ Esp-VLN-BERT (\(\)) & **16.90** & **38.50** & **31.37** & **25.22** & **17.78** & **15.80** & **34.00** & **30.95** & **24.46** & **17.14** & **14.12** \\ DUET  (\(\)) & 22.11 & 50.07 & 46.98 & 33.73 & 32.15 & -23.03 & 21.30 & 56.91 & 52.51 & 36.06 & 31.88 & 22.06 \\ Esp-DUET & 25.76 & **54.70** & **48.90** & **37.78** & **34.74** & **23.39** & 22.70 & **59.38** & **53.19** & **36.26** & **33.10** & **22.14** \\  

Table 2: Quantitative comparison results on REVERIE  (§4.1).

    &  &  \\   & TL\(\) & OSR\(\) & SR\(\) & SPL\(\) & RS\(\) & RS\(\) & SPL\(\) & LL\(\) & OSR\(\) & SR\(\) & RGS\(\) \\   Seq2Seq  (\(\)) & 11.07 & 8.07 & 4.20 & 2.84 & 2.16 & 1.63 & 10.89 & 6.88 & 3.99 & 3.09 & 2.00 & 1.58 \\ RCM  (\(\)) & 11.98 & 14.23 & 9.29 & 6.97 & 4.89 & 3.89 & 10.60 & 11.68 & 7.84 & 6.67 & 3.67 & 3.14 \\ INP  (\(\)) & 45.28 & 28.20 & 14.40 & 7.19 & 7.84 & 4.67 & 39.05 & 30.63 & 19.88 & 11.61 & 11.28 & 6.08 \\ Aubert  (\(\)) & 18.71 & 34.51 & 27.89 & 21.88 & 18.23 & 14.18 & 17.91 & 34.20 & 30.28 & 23.61 & 16.83 & 13.28 \\ HAMT  (\(\)) & 14.08 & 36.84 & 39.25 & 30.20 & 18.92 & 17.28 & 13.62 & 33.31 & 30.40 & 26.67 & 14.88 & 13.08 \\ HOP  (\(\)) & 16.46

[MISSING_PAGE_FAIL:8]

### Diagnostic Experiment

**Joint Distribution Matching.** We first investigate the joint distribution matching loss \(_{}\) (Eq. 3) in Enp, which is factored as the discriminative action loss \(_{}\) (Eq. 1) and the marginal state distribution matching loss \(_{}\) (Eq. 7). Both of \(_{}\) and \(_{}\) are online optimized iteratively. A clear performance drop is observed in Table 5, _e.g._, SR from \(\) to \(72\%\) for DUET  on R2R, as only optimizing the conditional action distribution with \(_{}\) will degenerate into BC (or DAGger) with cross-entropy loss. This reveals the appealing effectiveness of the joint distribution matching strategy in Enp.

**Step Size and Guassian Noise in SGLD.** In Enp, SGLD  is used to draw samples from Boltzmann distribution (Eq. 9) and further optimize \(_{}\). Following recent EBM training, we relax the restriction on the relation between the step size \(\) and Gaussian noise \(\), _i.e._, \(()=\). Table 6 shows the results with different step sizes (when \(I=15\)), and changing \(\) in a small range has little effects on the performance. In addition, different Enp is insensitive to parameter changes. We find that \(\!=\!1.5\) and \((0,0.01)\) for the transition kernel of SGLD works well across a variety of datasets and frameworks in VLN.

**Number of SGLD Loop per Training Step.** For the SGLD sampler, we study the influence of the number of inner loop iterations per training step. In Table 7, we can find that \(I=15\) iterations is enough to sample the recurrent states for EnvDrop  and VLN\(\)BERT . Slightly more iterations are required for the models [6; 29] with additional topological graph memory due to the global action space. This can be mitigated by pre-storing the samples in the Marginal State Memory \(\) during pre-training. As these frameworks [6; 29] employ pre-training and finetuning paradigm for VLN, we design a different number of iterations, _i.e._, \(I_{}=20\) and \(I_{}=5\). It is worth noting that the state memory \(_{}\) can acquire the samples while performing single-step action prediction in the VLN pre-training. Thus, the SGLD iterations in finetuning based on \(_{}\)-\(_{}\) can be further reduced.

**Enp _vs._****AIRL.** In Table 8, we compare Enp against AIRL , which is a representative work of adversarial imitation learning. The objective function for Enp is equivalent to the forward KL divergence minimization, where \(\) is the gradient of the backward KL divergence. By modifying the original policy learning in VLN\(\)BERT  (a mixture of RL and IL) as an adversarial reward learning formulation, AIRL achieves the comparable performance. Enp yields better scores on R2R-CE with \(\) SR and \(\) SPL. Meanwhile, Enp avoids the potential influence from the adversarial training.

**Runtime Analysis.** The additional computational complexity of Enp is from the iterations of SGLD inner loop. Towards this, we make some specific designs, such as Marginal State Memory \(\) and pre-storing in pre-training. During inference, there is no additional computational consumption in executing the strategy compared to existing VLN agents.

## 5 Conclusion

In this paper, we propose Enp, an Energy-based Navigation Policy learning framework. By explicitly modeling the joint state-action distribution using an energy-based model, Enp shows promise to solve the intrinsic limitations of supervised behavioural cloning. Through maximizing the likelihood of the action prediction and modeling the dynamics of the navigation states jointly, Enp learns to minimize the divergence distance from the expert policy in a collaborative manner. Experimental results on gold-standard VLN datasets in both discrete (R2R and REVERIE) and continuous environments (R2R-CE and RxR-CE) demonstrate the superiority against existing methods. In the future, we will explore more tasks and frameworks with the energy-based policy.

    &  &  &  \\   & & SR1 & SR1 & SR1 & SR1 \\    & \(=1\), \(()=0.01\) & 64 & 57 & 45 & 39 \\  & \(=1.5\), \(()=0.01\) & **65** & **57** & **45** & **40** \\  & \(=2.\), \(()=0.01\) & 64 & 56 & 44 & 40 \\  & \(=1.5\), \(()=0.1\) & 63 & 55 & 41 & 38 \\  

Table 6: Ablation study of step size and noise (§4.3).

    &  &  &  \\   & & SR1 & SR1 & SR1 & SR1 \\    & \(I=5\) & 64 & 56 & 44 & 39 \\  & \(I=15\) & **65** & **57** & **45** & **40** \\  & \(I=20\) & 65 & 57 & 45 & 40 \\   & \(I_{}=10\), \(I_{}=5\) & 74 & 58 & 56 & 49 \\  & \(I_{}=20\), \(I_{}=5\) & 74 & 60 & 58 & 59 \\  & \(I_{}=20\), \(I_{}=10\) & 73 & 60 & 58 & 49 \\  

Table 7: Ablation study of SGLD iterations (§4.3).

    &  &  &  \\   & & SR1 & SR1 & SR1 \\   & \(=1.5\), \(()=0.01\) & 64 & 57 & 45 & 39 \\  & \(=1.5\), \(()=0.01\) & **65** & **57** & **45** & **40** \\  & \(=2.\), \(()=0.01\) & 64 & 56 & 44 & 40 \\  & \(=1.5\), \(()=0.1\) & 63 & 55 & 41 & 38 \\  

Table 7: Ablation study of SGLD iterations (§4.3).

    &  &  &  \\   & & SR1 & SR1 & SR1 \\   & \(=1.5\), \(()=0.01\) & 64 & 55 & 44 & 39 \\  & \(=1.5\), \(()=0.01\) & **65** & **57** & **45** & **40** \\   & \(=1.5\), \(()=0.1\) & 63 & 55 & 41 & 38 \\  

Table 5: Ablation study of objective loss (§4.3).

**Acknowledgment.** This work was supported by the National Science and Technology Major Project (No. 2023ZD0121300), the National Natural Science Foundation of China (No. 62372405), the Fundamental Research Funds for the Central Universities 226-2024-00058, the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University (No. HMHAI-202403), CIPSC-SMP-Zhipu Large Model Cross-Discisplinary Fund, and the Earth System Big Data Platform of the School of Earth Sciences, Zhejiang University.