# Knowledge-Empowered Dynamic Graph Network for Irregularly Sampled Medical Time Series

Yicheng Luo, Zhen Liu1, Linghao Wang, Junhao Zheng, Binquan Wu, Qianli Ma1

School of Computer Science and Engineering,

South China University of Technology, Guangzhou, China

{csluoyicheng2001, cszhenliu, cskyun_ng}@mail.scut.edu.cn,

{linghaowang6, junhaozheng47}@outlook.com, qianlima@scut.edu.cn

###### Abstract

Irregularly Sampled Medical Time Series (ISMTS) are commonly found in the healthcare domain, where different variables exhibit unique temporal patterns while interrelated. However, many existing methods fail to efficiently consider the differences and correlations among medical variables together, leading to inadequate capture of fine-grained features at the variable level in ISMTS. We propose Knowledge-Empowered Dynamic Graph Network (KEDGN), a graph neural network empowered by variables' textual medical knowledge, aiming to model variable-specific temporal dependencies and inter-variable dependencies in ISMTS. Specifically, we leverage a pre-trained language model to extract semantic representations for each variable from their textual descriptions of medical properties, forming an overall semantic view among variables from a medical perspective. Based on this, we allocate variable-specific parameter spaces to capture variable-specific temporal patterns and generate a complete variable graph to measure medical correlations among variables. Additionally, we employ a density-aware mechanism to dynamically adjust the variable graph at different timestamps, adapting to the time-varying correlations among variables in ISMTS. The variable-specific parameter spaces and dynamic graphs are injected into the graph convolutional recurrent network to capture intra-variable and inter-variable dependencies in ISMTS together. Experiment results on four healthcare datasets demonstrate that KEDGN significantly outperforms existing methods. Our code is available at [https://github.com/qianlima-lab/KEDGN](https://github.com/qianlima-lab/KEDGN).

## 1 Introduction

In the medical environment, the widely used Electronic Health Records (EHRs) have abundant typical Irregularly Sampled Medical Time Series (ISMTS) data . Each ISMTS typically comprises multiple medical variables for a patient, each with distinct medical properties, resulting in significant differences in the sampling patterns of each variable series. Additionally, due to the dynamic changes in a patient's condition, the sampling rate of variables varies over different periods, resulting in uneven sampling intervals .

Many existing methods for ISMTS primarily focus on addressing uneven sampling intervals and have proposed approaches such as Ordinary Differential Equations (ODEs)  and continuous-time embeddings , etc, which have already achieved significant success. Recent advancements in regularly sampled multivariate time series analysis  underscore the importance of capturing variable-specific temporal patterns. However, many existing methods for ISMTS have not adequately considered this aspect, having limited ability to explicitly distinguish multiple variable series withdifferent time patterns and thus lacking finer-grained capturing of variable-level features. Particularly in ISMTS, different variables have distinct medical properties, further intensifying the degree of differences among variables. In such cases, capturing differentiated variable patterns requires a deeper exploration of inherent differences among variables.

Despite the differentiated temporal patterns occurring among variables within ISMTS, they are not entirely independent but exhibit medical correlations. Due to the dynamic changes in the patient's condition, this correlation varies along with the sampling density of variables at different periods, as illustrated in Figure 1. Some existing work has introduced graph neural networks [9; 10] to model the time-varying correlations among variables in ISMTS. However, due to the lack of prior medical knowledge, these methods learn variable correlation graphs from misaligned and imbalanced observations in variables of ISMTS and rely solely on downstream tasks for graph optimization. Consequently, the variable graphs learned by these methods may face challenges in accurately reflecting the general medical correlations among variables, resulting in suboptimal performance.

To this end, we aim to explicitly consider the differences and correlations among variables in ISMTS, empowering the model to capture fine-grained features at the variable level. However, the sampling rates, sampling times, observation spans, and observation lengths of different variables vary in ISMTS, as shown in three subgraphs in Figure 2. This makes it tough and complex to infer these two aspects only from the time series modality of variables. We rethink the issue based on an intuitive observation as shown in Figure 2: Variables that exhibit (dis)similar temporal patterns frequently have (dis)similar medical properties in reality, which motivates us to consider the differences and correlations of variables from the perspective of domain knowledge directly. Recent work [11; 12] has successfully represented domain knowledge through textual modal information, enhancing model performance in medical imaging, which provides us with insights. Specifically, the medical properties of each variable can be described in natural language. Leveraging the powerful semantic understanding capabilities of the Pre-trained Language model (PLM), we can obtain semantic representations from the textual knowledge of each variable. This set of textual representations forms an overall view of variables from the perspective of medical knowledge, clearly showing inherent differences and correlations among variables--exactly what we need.

Based on the above analysis, we propose the Knowledge-Empowered Dynamic Graph Network (KEDGN), which utilizes textual semantic representations of variables obtained through PLM as guidance. On this basis, we 1) allocate unique parameter space for each variable to capture their specific temporal pattern, 2) generate a complete variable graph and introduce a density-aware mechanism to explicitly model time-varying correlations among variables in ISMTS. Finally, these

Figure 1: Illustration of three variables in an ISMTS sample. In the first 7 seconds (Box 1), a strong correlation between HR and NI-DiasABP is observed. As NIDiasABP becomes more sparse, the correlation between HR and NIDiasABP weakens between 7 and 50 seconds (Box 2), while the correlation between HR and DiasABP increases.

Figure 2: The time patterns (sampling rates, sampling times, observation spans, observation lengths, trends, etc.) of variables among different subgraphs exhibit significant differences, as they have distinct medical properties. Meanwhile, variables within the same subgraph share similar time patterns, and their medical properties are closely related. (More variable groups can be found in Figure 7).

two modules are integrated with a graph convolutional recurrent network to capture both temporal and inter-variable dependencies in ISMTS. Our contributions can be summarized in three aspects:

* We leverage variable-specific textual medical knowledge to empower the model to capture variable-specific temporal patterns in ISMTS distinctively.
* We introduce a density-aware mechanism based on the knowledge-empowered variable graph to model the time-varying inter-variable dependencies in ISMTS.
* Empirical results on four real-world medical datasets demonstrate that KEDGN outperforms state-of-the-art methods. Visualization analysis further illustrates the strong interpretability of our approach.

## 2 Related Work

Irregularly Sampled Multivariate Time Series ModelingExisting methods can be roughly categorized into interpolation-based and raw data-based approaches. The former, employing methods such as kernel-based approaches [13; 14], Gaussian process  or hourly aggregation , aims to obtain a set of regularly spaced observations. However, interpolation may result in the loss of useful information about the original sequences, such as missing patterns. The latter, raw data-based methods, directly learn from irregular time series. To adapt to uneven sampling intervals,  improves recurrent neural networks, [3; 4] introduce neural ordinary differential equations and [6; 5; 18; 19] adopt time embeddings.  converts ISMTS into line graph images and utilizes pretrained vision transformers for extracting features. These methods primarily focus on overall temporal dependencies, needing more consideration for fine-grained variable-level patterns and correlations. Despite recent research introducing attention [21; 22] or graph neural networks [9; 23] to account for variable correlations, these methods have limited performance due to the lack of prior knowledge and the use of shared parameter spaces among all variables.

Graph Neural Networks for Multivariate Time SeriesIn recent years, a series of studies have integrated GNN with various time series modeling frameworks to effectively capture both inter-variable and inter-temporal dependencies in MTS . These approaches have been widely applied in diverse domains, including transportation , healthcare , economics , demonstrating promising results in mainstream tasks such as prediction , classification , and imputation . Although recent work [31; 32; 33; 34] has proposed the idea of modeling variable relationships through learning dynamic graphs, most of these methods are primarily designed for regularly sampled MTS with synchronous observations, and further improvements are needed to adapt them for irregularly sampled time series.

Medical Knowledge Enhanced ModelsSeveral studies have utilized the rich domain knowledge in the medical field to enhance models. [12; 35] apply knowledge for computing additional features, while [36; 37] utilize knowledge to guide the final training loss, demonstrating the effectiveness of medical prior knowledge. However, existing methods commonly focus on visual language pretraining in medical scenes or medical report generation . How to effectively integrate domain knowledge to guide medical time series modeling remains a challenge.

## 3 Problem Definition

Given a dataset \(=\{(s_{i},\ y_{i}) i=1,,\ N\}\) containing \(N\) patient samples, the \(i\)-th sample consists of an irregular multivariate time series \(s_{i}\) and a label \(\ y_{i}\). For the dataset with a total variable count of \(V\) and a maximum sample observation length of \(T\), \(s_{i}\) can be denoted as a tuple: \(s_{i}=(t_{i},x_{i},m_{i})\), where \(t_{i}^{T}\) represents the observation timestamps, \(x_{i}^{V T}\) represents the multivariate time series observations, and any unobserved values or the missing parts of the time series shorter than the maximum sample observation length are filled with 0. The binary indicator \(m_{i}\) has the same size as \(x_{i}\), indicating which elements in the \(x_{i}\) are actually observed. We use 1 to represent observed values and 0 to represent missing values. In this paper, we focus on patient mortality and morbidity prediction, i.e., classification task, aiming to correctly predict class label \(\ _{i}\) given a sample \(s_{i}\).

## 4 The Proposed Model

### Variable Semantic Representations Extraction

First, we introduce how to extract semantic representation for each variable from medical knowledge. Let \(=\{v_{1},v_{2},,v_{V}\}\) be the set of variables, and a descriptive sentence of medical properties associated with the \(j^{th}\) variable can be denoted as:

\[P_{j}=w_{j,1},w_{j,2},...,w_{j,l_{j}}\ j=1,2,,\,V\}, \]

where \(l_{j}\) is the length of the \(j^{th}\) variable's sentence and \(w_{j,i}\) denotes the \(i^{th}\) word of the \(j^{th}\) sentence. We leverage a PLM to represent each text description \(P_{j}\) as a \(d\)-dimensional embedding. Considering the diverse types of PLMs with varying methods of utilization, we use the widely adopted encoder-based model BERT  as an illustration:

\[e_{j}=([],w_{j,1},w_{j,2},...,w_{j,l_{j}},[])^{(l_{j}+2) d}, \]

where [CLS] and [SEP] are special tokens indicating a sequence's beginning and end, respectively. BERT generates an embedding for each token of the input sequence. Since the embedding at the [CLS] position captures the semantic information of the entire input sequence, we take the hidden state at the [CLS] position as the overall semantic representation of each variable: \(E_{j}=e_{j}([])\). This yields a semantic representation matrix \(=[E_{1},E_{2},...,E_{V}]^{V d}\) and forms an overall view of variables from the perspective of medical knowledge.

### Variable-specific Parameter Learning

Since temporal patterns of ISMTS vary from variable to variable, simply using shared parameter space for all variables is insufficient to capture differentiated temporal dependencies. In this section, we adjust parameter space for different variables to adapt to differentiated temporal patterns based on the extracted variables' semantic representations. For any parameter matrix \(w^{I O}\) with input dimension \(I\) and output dimension \(O\), the total parameter space needed for \(V\) variables is \(^{V I O}\). Inspired by , we decompose \(\) into two matrices: a variable representation matrix \(^{V q}\) and a weight pool matrix \(^{q I O}\), where \(q\) is a hyperparameter for the intermediate dimension. Here, \(\) consists of \(q\)-dimensional query vectors for \(V\) variables used to distinguish differences among variables and obtain variable-specific parameters from the weight pool.

Figure 3: The model framework of KEDGN. We (1) utilize a PLM to extract semantic representations for each variable from textual medical properties (Section 4.1). Based on this, we (2) allocate variable-specific parameter space to capture variable-specific temporal patterns (Section 4.2), (3) generate dynamic variable graphs by combining knowledge-empowered graph with a density-aware mechanism to model time-varying correlations among variables (Section 4.3). (4) The above two modules are injected into graph convolutional recurrent network to model intra-variable and inter-variable dependencies in ISMTS simultaneously (Section 4.4).

We use a projection \(f():^{d}^{q}\) to variable semantic representations \(\) for obtaining query vectors rather than directly using \(\). On the one hand, the dimension of the query vector \(q\) directly determines the size of the weight pool, but the output dimension of PLM is often large (e.g., 768 in BERT), leading to a sharp increase in model complexity. On the other hand, there is a modality gap between textual embeddings and the temporal parameter space. The projection \(f()\) can achieve both feature reduction and modality transformation. In our implementation, we use a nonlinear projection with one additional hidden layer (and ReLU activation). Thus, the parameter space specific to variable \(i\) can be obtained using the following formula:

\[}=f(E_{i})^{I O}, \]

The approach we employ to generate variable-specific parameter space is general and not restricted to a specific model backbone because any model architecture is composed of multiple parameter matrices \(W\).

### Dynamic Variable Graph Generation

In this section, we introduce how to generate dynamic correlation graphs of variables for explicitly modeling the time-varying correlations among variables in ISMTS.

#### 4.3.1 Complete Variable Correlation Graph Learning

The misaligned and imbalanced observations of the variables in ISMTS make it difficult to learn the variable correlations from the time series. Therefore, we extract a static complete variable correlation graph based on the textual semantic representations of the variables directly from the perspective of the actual medical properties of variables. We apply another non-linear projection \(g():^{d}^{n}\) to the textual representations of variables \(\) to obtain \(n\)-dimensional node embeddings for each variable. Subsequently, we calculate the pairwise cosine similarity among the node embeddings of variables, resulting in a \(V V\) matrix of variable similarity. Finally, we use the softmax function to normalize the edge weights corresponding to each node, producing a normalized graph of variable correlations. The correlation weight between the \(i^{th}\) and \(j^{th}\) variables can be calculated as:

\[A_{ij}=\ () g(E_{j})}{\|g(E_{i})\|\|g (E_{j})\|}), \]

where \(\) represents vector dot product and \(\|\|\) represents the vector magnitude. The introduction of \(g()\) in this context not only performs feature reduction but also avoids using a completely fixed prior graph. It preserves the model's ability to adaptively optimize the graph structure based on different data distributions and downstream tasks. Thus, we obtain a knowledge-empowered complete graph with \(V\) nodes to measure the static correlation among variables in general medical cases, and its adjacency matrix is denoted as \(\).

#### 4.3.2 Dynamic Density-aware Adjustment Mechanism

Due to the varying subsets of variables observed at each timestamp in ISMTS, we use different subgraphs of \(\) to describe the variable correlations at different timestamps. Specifically, we use a mask matrix \(^{(t)}^{V V}\) to indicate the subgraph topology at timestamp \(t\):

\[^{(t)}_{ij}=1,&\\ 0,&, \]

Therefore, we can calculate the variable correlation subgraph \(^{(t)}\) at timestamp \(t\) through \(^{(t)}=^{(t)}\), where \(\) represents Hadamard product. Additionally, we introduce a density-aware mechanism to dynamically adjust edge weights of subgraphs in different timestamps to fit in the time-varying correlations among variables mentioned in Figure 1. Specifically, we estimate the sampling density of any observation point by considering the average time interval between each observation point and its preceding and succeeding observations. If there is no preceding/succeeding observation, we take the time interval of the succeeding/preceding observation as the density. If neither a preceding nor a succeeding observation exists, it indicates that this observation is the only one for the variable, and we take half of the maximum observation time span as the density. The formula for calculating the sampling density of the \(i\)-th observation of variable \(v\) at timestamp \(t\) is:

\[Z^{(t)}=Z_{i,v}=((t_{i,v}-t_{i-1,v})+(t_{i+1,v}-t_{i,v}))/2,$ and $t_{i-1,v}$ exist}\\ t_{i,v}-t_{i-1,v},$ does not exist}\\ t_{i+1,v}-t_{i,v},$ does not exist}\\ t_{max}/2,$ nor $t_{i-1,v}$ exists.} \]

Then we calculate the density scores for various variables at timestamp \(t\) through:

\[D^{(t)}=(Z^{(t)})^{V}, \]

where \(\) is an activation function and \(\) is a hyperparameter that controls the proportion. At timestamp \(t\), the edge weight between the \(i^{th}\) and \(j^{th}\) variables is adjusted as:

\[^{(t)}_{ij}=^{(t)}_{ij}(1-W_{ij}|D^{(t)}_{i}-D^{(t)}_{j}|), \]

where \(W^{V V}\) is a learnable parameter matrix. Thereby, we achieve the dynamic adjustment of the variable graph weights in response to changes in variable sampling density.

### Variable-specific Dynamic Graph Convolutional Recurrent Network

Under the empowerment of variable textual representation, we have obtained variable-specific parameters \(\ ^{V I}\) and dynamic variable graph \(^{T V V}\). In this section, we integrate these two modules into the graph convolutional neural network to handle ISMTS. GCRNN  is a backbone network that introduces graph convolutional operations on top of an RNN variant, Gated Recurrent Unit . This structure is simple, effective, and easy-to-adapt for ISMTS, as it enables variable-level parallel computation of asynchronous observations without explicit interpolation. Specifically, we allocate a unique hidden state for each variable, updating the state only at the observed timestamps to avoid imputation and preserve the individual sampling patterns of each variable. The graph convolution operation over a graph signal \(S^{V I}\) containing \(V\) nodes at timestamp \(t\) is defined as follows:

\[_{G^{(t)}}S(I_{V}+G^{(t)})^{T}S, \]

where \(I_{V}^{V V}\) is identity matrix, \(\) represents batch matrix multiplication. Here, we adopt \(1^{st}\)-order Chebyshev polynomial expansion approximation  for graph convolution. The updated formulas for variable states at timestamp \(t\) are:

\[r^{(t)}=(_{r}_{G^{(t)}}[X^{(t)}||H^{(t-1)}]+b_{ r}), \] \[u^{(t)}=(_{u}_{G^{(t)}}[X^{(t)}||H^{(t-1)}]+b_{ u}),\] (11) \[C^{(t)}=tanh(_{C}_{G^{(t)}}[X^{(t)}||(r^{(t)} H^ {(t-1)})]+b_{C}),\] (12) \[H^{(t)}_{i}=H^{(t-1)}_{i},\\ u^{(t)}_{i} H^{(t-1)}_{i}+(1-u^{(t)}_{i}) C^{(t)}_{i}, , \]

where \(||\) denotes the concatenate operation, \(H^{(t-1)}^{V h}\) is the variable states at the previous timestamp and \(X^{(t)}^{V k}\) denotes the input representation at current timestamp. We follow the structured input encoding method of , using multiple fully-connected mappings to encapsulate each observed value and its corresponding timestamp into a \(k\)-dimensional input representation (All 0 vectors for unobserved values) to indicate flexible observation time and adapt to the uneven intervals within variables. \(r^{(t)},u^{(t)}^{V h}\) are reset gate and updated gate, respectively. \(_{r},_{u},_{C}^{V(k+h) h}\) are variable-specific parameters obtained by respectively multiplying the query vectors matrix \(^{V q}\) with three weight matrices \(W_{r},W_{u},W_{C}^{q(k+h) h}\).

We calculate the sum of \(h\) channels for each variable's hidden state \(H_{i}\) at the last observed timestamp to get a \(V\)-dimensional vector \(C\). Additionally, we follow the approach used in  to incorporate the static features. Specifically, static features of each sample are mapped into a static vector \(S\) through a linear layer. Finally, \(C\) and \(S\) are concatenated to predict the final classification probabilities: \(=(W^{y}[C||S]+b^{y})\). The training objective is minimizing the cross-entropy loss between \(\) and \(y\). The pseudo-code for KEDGN is presented in Appendix A (Algorithm 1).

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

that the textual representation space of variables exhibits distinct clustering. Although there may be occasional outliers when using different text sources, such as BUN in Figure 3(a) and HCO3 in Figure 3(b), the overall clusters are generally consistent with the variable groups we divide based on time series patterns. However, the learnable variable embeddings in Figure 3(c), optimized by classification loss, tend to be distributed uniformly, which is difficult to effectively reflect the intrinsic differences among variables. More visualizations of other datasets and other PLMs can be found in Appendix F.8 and F.9, respectively. Based on this phenomenon, we infer that text descriptions and time series are both external manifestations of the inherent sense of variables; they just belong to different data modalities. These two forms of data for the same variable should exhibit relative consistency. Therefore, the relative distribution among variables extracted from text and time series should ideally be similar. Leveraging PLM allows for the straightforward and efficient extraction of this universal view from textual descriptions, which is equally applicable to describing the relative distribution of temporal patterns among variables. Thus, the effectiveness of PLM and the cross-modal relative consistency are the keys to guiding time series modeling based on textual information.

#### 5.4.2 Visualization of Variable Correlation Graph

We visualize the learned partial inter-variable correlation graphs on the MIMIC-III dataset in the form of heatmaps. In Figure 4(a), we depict the graph learned based on node embeddings mapped from textual representations, while in Figure 4(b), the graph is learned based on randomly initialized learnable node embeddings. We observe that the top-left and bottom-right corners of the heatmap in Figure 4(a) exhibit darker colors, indicating strong correlations consistent with medical domain knowledge. Specifically, the variables in the top-left corner, Heart Rate (HR) and Respiration Rate (RR), are commonly monitored together in clinical settings for assessing vital signs and respiratory system controls. The variables in the bottom-right corner, GCS-MR, GCS-T, and GCS-VR, collectively constitute the Glasgow Coma Scale (GCS), which is used to assess a patient's neurological status and level of consciousness . These correlations are not evident in the graph without textual representations. This once again validates our perspective that relying solely on downstream task optimization for adaptive learning in graphs in ISMTS is insufficient to reflect the actual medical correlations among variables and lacks interpretability. In contrast, textual representations can guide the model to accurately extract variable correlations aligned with domain knowledge to provide high interpretability.

#### 5.4.3 Visualization of Dynamic Density-aware Graph

In Figure 6, we visualize the dynamic density-aware graph for the sample in Figure 1. We present the time series of three variables and the corresponding correlation heatmaps learned by our model at

Figure 4: T-SNE visualization of partial variable representations on the P12 dataset.

Figure 5: Visualization of the learned correlation graph of variables on the MIMIC-III dataset.

timestamps \(4\), \(15\) and \(56\). From the time series, we observe that around \(t=4\), HR shows a strong correlation with NIDiasABP, while the correlation with DiaABP is masked as 0 since DiasABP has not been observed yet. Around \(t=15\), the correlation between HR and NIDiasABP decreases, while a relatively strong correlation with DiaSABP emerges. By \(t=56\), HR exhibits a strong correlation with both variables. This process is clearly reflected in the heatmaps: the color between HR and NIDiasABP transitions from dark to light from \(t=4\) to \(t=15\) and darkens again from \(t=15\) to \(t=56\). The color between HR and DiaSABP remains dark at \(t=15\) and \(t=56\). This demonstrates that our dynamic density-aware mechanism exactly reflects the time-varying correlations among variables in ISMTS.

## 6 Limitations

Although our proposed method effectively guides ISMTS modeling through the domain knowledge from text modality, it has some limitations. The backbone of our model is a recurrent-based architecture, which inherently has a sequential computation characteristic that can be a bottleneck in terms of runtime. Additionally, our method is specifically tailored for medical applications, and its performance may be limited in other irregular multivariate time series applications, such as human activity recognition, where variables lack domain knowledge and thus cannot generate high-quality text descriptions.

## 7 Conclusion

In this paper, we propose KEDGN for modeling ISMTS. The proposed method leverages a PLM to flexibly extract semantic representation for each variable from the textual medical knowledge. Based on these representations, we allocate the variable-specific parameter space to capture variable-specific temporal patterns and extract a complete variable graph as a measure of the variables' static medical correlations. Considering the time-varying variables correlations in ISMTS, we introduce a density-aware mechanism to dynamically adjust the subgraph across different periods. Our experimental results demonstrate that KEDGN outperforms existing methods in ISMTS classification tasks and provides high interpretability. Our future work will focus on investigating the applicability of KEDGN in a range of related tasks, such as interpolation, extrapolation, and regression.