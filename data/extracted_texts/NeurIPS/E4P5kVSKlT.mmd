# On the Asymptotic Learning Curves of Kernel Ridge Regression under Power-law Decay

Yicheng Li, Haobo Zhang

Center for Statistical Science, Department of Industrial Engineering

Tsinghua University, Beijing, China

{liyc22,zhang-hb21}@mails.tsinghua.edu.cn

Qian Lin

Center for Statistical Science, Department of Industrial Engineering

Tsinghua University, Beijing, China

qianlin@tsinghua.edu.cn

Qian Lin also affiliates with Beijing Academy of Artificial Intelligence, Beijing, China

###### Abstract

The widely observed 'benign overfitting phenomenon' in the neural network literature raises the challenge to the 'bias-variance trade-off' doctrine in the statistical learning theory. Since the generalization ability of the 'lazy trained' over-parametrized neural network can be well approximated by that of the neural tangent kernel regression, the curve of the excess risk (namely, the learning curve) of kernel ridge regression attracts increasing attention recently. However, most recent arguments on the learning curve are heuristic and are based on the 'Gaussian design' assumption. In this paper, under mild and more realistic assumptions, we rigorously provide a full characterization of the learning curve in the asymptotic sense under a power-law decay condition of the eigenvalues of the kernel and also the target function. The learning curve elaborates the effect and the interplay of the choice of the regularization parameter, the source condition and the noise. In particular, our results suggest that the 'benign overfitting phenomenon' exists in over-parametrized neural networks only when the noise level is small.

## 1 Introduction

Kernel methods, in particular kernel ridge regression (KRR), have been one of the most popular algorithms in machine learning. Its optimality under various settings has been an active topic since Caponnetto and De Vito (2007), Andreas Christmann (2008). The renaissance of kernel methods arising from the neural tangent kernel (NTK) theory (Jacot et al., 2018), which shows that over-parametrized neural networks can be well approximated by certain kernel regression with the corresponding NTK, has posed further challenges about the interplay of generalization, regularization and noise level. For example, it has been observed empirically that over-parametrized neural networks can fit any data perfectly but also generalize well (Zhang et al., 2017), which contradicts to our traditional belief of bias-variance trade-off (Vapnik, 1999).

The aforementioned 'benign overfitting phenomenon' that overfitted neural networks generalize well attracts lots of attention recently. Researchers provide various explanations to reconcile the contradiction between it and the bias-variance trade-off principle. For example, Belkin et al. (2019) proposed the 'double descent theory' to explain why large model can generalize well; some other works (e.g., Liang and Rakhlin (2020)) argued that kernel interpolating estimators can generalizewell in high dimensional settings. In contrast to the 'benign overfitting phenomenon', several other works (e.g., Rakhlin and Zhai (2018); Li et al. (2023a)) recently showed that kernel interpolation can not generalize in traditional fixed dimension setting. In order to understand the 'benign overfitting phenomenon', it would be of great interest to characterize the learning curve: the curve of the exact order of the generalization error of a certain algorithm (e.g., KRR) varying with respect to different choices of regularization parameters.

Recently, several works (e.g., Bordelon et al. (2020); Cui et al. (2021)) depicted the learning curve of KRR under the Gaussian design assumption that the eigenfunctions (see (5)) are i.i.d. Gaussian random functions. Though it is easy to figure out that the Gaussian design assumption can not be true in most scenarios, with some heuristic arguments, Cui et al. (2021) provide a description of the learning curves of KRR with respect to the regularization, source condition and noise levels. These works offered us some insights on the learning curve of KRR which strongly suggests that the learning curve should be U-shaped if the observations are noisy or monotone decreasing if the observations are noiseless.

In this paper, we consider the learning curves of KRR under the usual settings (without the Gaussian design assumption). Under mild assumptions, we rigorously prove the asymptotic rates of the excess risk, including both upper and lower bounds. These rates show the interplay of the eigenvalue decay of the kernel, the relative smoothness of the regression function, the noise and the choice of the regularization parameter. As a result, we obtain the traditional U-shaped learning curve for the noisy observation case and a monotone decreasing learning curve for the noiseless case, providing a full picture of the generalization of KRR in the asymptotic sense. Combined with the NTK theory, our results may also suggest that 'the benign overfitting phenomenon' may not exist if one trains a very wide neural network.

### Our contributions

The main contribution of this paper is that we remove the unrealistic Gaussian design assumption in previous non-rigorous works (Bordelon et al., 2020; Cui et al., 2021) and provide mathematically solid proof of the exact asymptotic rates of KRR with matching upper and lower bounds.

To be precise, let us introduce the quantities \(\), the regularization parameter in (1); \(\), the eigenvalue decay rate in (6), which characterizes the span of the underlying reproducing kernel Hilbert space (RKHS); and \(s\), the smoothness index in (12), describes the relative smoothness of the regression function with respect to the RKHS. Here we note that larger \(\) implies better regularity the RKHS and also larger \(s\) also implies better relative smoothness. Then, the asymptotic rates of the generalization error (excess risk) \(R()\) in the noisy case is roughly

\[R()=^{(s,2)}+^{2}^{- 1/}/n,&=(n^{-});\\ (^{2}),&=O(n^{-});\]

where \(n\) is the number of the samples and \(^{2}\) is the noise level. This result justifies the traditional U-shaped learning curve (see also Figure 1 on page 1) with respect to the regularization parameter.

For the technical part, we use the bias-variance decomposition and determine the exact rates of the both terms. Since the variance term was already considered in Li et al. (2023a), the main focus of this work is the bias term. Our technical contributions include:

* When the regularization parameter \(\) is not so small, that is, \(=(n^{-})\), we provide sharp estimates of the asymptotic orders (Lemma 4.1) of the bias term with both upper and lower bounds. Our result holds for both the well-specified case (\(s 1\)) and the mis-specified case (\(s(0,1)\)), which improves the upper bounds given in Zhang et al. (2023a).
* We further show an upper bound (Lemma A.12) of the bias term in the nearly interpolating case, i.e., \(=O(n^{-})\). The upper bound is tight and matches the information-theoretic lower bound provided in Proposition 4.4.
* Combining these results, we provide learning curves of KRR for both the noisy case (Theorem 3.2) and the noiseless case (Theorem 3.4). The results justify our traditional belief of the bias-variance trade-off principle.
* Our new techniques can also be generalized to other settings and might be of independent interest.

### Related works

The optimality of kernel ridge regression has been studied extensively (Caponnetto and De Vito, 2007; Steinwart et al., 2009; Fischer and Steinwart, 2020; Zhang et al., 2023a). Caponnetto and De Vito (2007) provided the classical optimality result of KRR in the well-specified case and the subsequent works further considered the mis-specified case. However, these works only provided an upper bound and the worst-case (minimax) lower bound, which are not sufficient for determining the precise learning curve. In order to answer the "benign overfitting" phenomenon (Bartlett et al., 2020; Liang and Rakhlin, 2020), several works (Rakhlin and Zhai, 2018; Buchholz, 2022; Beaglehole et al., 2022) tried to provide a lower bound for the kernel interpolation, which is a limiting case of KRR, but these works only focused on particular kernels and their techniques can hardly be generalized to provide a lower bound for KRR.

Another line of recent works considered the generalization performance of KRR under the Gaussian design assumption of the eigenfunctions (Bordelon et al., 2020; Jacot et al., 2020; Cui et al., 2021; Mallinar et al., 2022). In particular, the learning curves of KRR was described in Bordelon et al. (2020); Cui et al. (2021), but heuristic arguments are also made in addition to the unrealistic Gaussian design assumption. Though the heuristic arguments are inspirational, a rigorous proof is indispensable if one plans to perform further investigations. In this work, we provide the first rigorous proof for most scenarios of the smoothness \(s\), eigenvalue decay rate \(\), noise level \(^{2}\) and the regularization parameter \(\) based on the most common/realistic assumptions.

Recently, in order to show the so-called "saturation effect" in KRR, Li et al. (2023b) proved the exact asymptotic order of both the bias and the variance term when the regression function is very smooth and the regularization parameter \(\) is relatively large. Inspired by their analysis, Li et al. (2023a) showed the exact orders of the variance term. Our work further determines the orders of the bias term, completing the full learning curve or KRR.

KRR is also connected with Gaussian process regression (Kanagawa et al., 2018). Jin et al. (2021) claimed to establish the learning curves for Gaussian process regression and thus for KRR. However, as pointed out in Zhang et al. (2023b), there is a gap in their argument. Moreover, their results are also more restrictive than ours, see Section 3.3 for a comparison.

NotationsWe write \(L^{p}(,)\) for the Lebesgue space and sometimes abbreviate it as \(L^{p}\). We use asymptotic notations \(O(),\;o(),\;()\) and \(()\), and use \(()\) to suppress logarithm terms. We also write \(a_{n} b_{n}\) for \(a_{n}=(b_{n})\). We will also use the probability versions of the asymptotic notations such as \(O_{}()\). Moreover, to present the results more clearly, we denote \(a_{n}=O^{}(b_{n})\) if \(a_{n}=O(n^{p}b_{n})\) for any \(p>0\), \(a_{n}=^{}(b_{n})\) if \(a_{n}=(n^{-p}b_{n})\) for any \(p>0\), \(a_{n}=^{}(b_{n})\) if \(a_{n}=O^{}(b_{n})\), and \(a_{n}=^{}(b_{n})\); and we add a subscript \({}_{}\) for their probability versions.

## 2 Preliminaries

Let \(^{d}\) be compact and \(\) be a probability measure on \(\), whose marginal distribution on \(\) is denoted by \(\). Suppose that we are given \(n\) i.i.d. samples \((x_{1},y_{1}),,(x_{n},y_{n})\) from \(\). Let \(k\) be a continuous positive definite kernel \(k\) over \(\) and \(\) be the separable reproducing kernel Hilbert space (RKHS) associated with \(k\). Then, kernel ridge regression (KRR) obtains the regressor \(_{}\) via the following convex optimization problem

\[_{}=*{arg\,min}_{f}( _{i=1}^{n}(y_{i}-f(x_{i}))^{2}+\|f\|_{}^{2}),\] (1)

where \(>0\) is the regularization parameter. Let us denote \(X=(x_{1},,x_{n})\) and \(=(y_{1},,y_{n})^{T}\). A closed form of (1) can be provided by the representer theorem (Andreas Christmann, 2008):

\[_{}(x)=(x,X)((X,X)+n)^{-1}\] (2)

where \((x,X)=(k(x,x_{1}),,k(x,x_{n}))\) and \((X,X)=k(x_{i},x_{j})_{n n}\).

In terms of the generalization performance of \(_{}\), we consider the excess risk with respect to the squared loss

\[_{x}[_{}(x)-f_{}^{*}(x)]^{2}=\| _{}-f_{}^{*}\|_{L^{2}(,)}^{2},\] (3)

where \(f_{}^{*}(x)_{}[y x]\) is the conditional expectation and is also referred to as the regression function. We aim to provide asymptotic orders of (3) with respect to \(n\).

### The integral operator

We will introduce the integral operator, which is crucial for the analysis, as the previous works (Caponnetto and De Vito, 2007; Lin et al., 2018). Denote by \(\) the marginal probability measure of \(\) on \(\). Since \(k\) is continuous and \(\) is compact, let us assume \(_{x}k(x,x)^{2}\). Then, it is known (Andreas Christmann, 2008; Steinwart and Scovel, 2012) that we have the natural embedding \(S_{}: L^{2}\), which is a Hilbert-Schmidt operator with Hilbert-Schmidt norm \(\|S_{}\|_{}\). Let \(S_{}^{*}:L^{2}\) be the adjoint operator of \(S_{}\) and \(T=S_{}S_{}^{*}:L^{2} L^{2}\). Then, it is easy to show that \(T\) is an integral operator given by

\[(Tf)(x)=_{}k(x,y)f(y)(y),\] (4)

and it is self-adjoint, positive and trace-class (thus compact) with trace norm \(\|T\|_{1}^{2}\)(Caponnetto and De Vito, 2007; Steinwart and Scovel, 2012). Moreover, the spectral theorem of compact self-adjoint operators and Mercer's theorem (Steinwart and Scovel, 2012) yield the decompositions

\[T=_{i N}_{i},e_{i}_{L^{2}}e_{i},  k(x,y)=_{i N}_{i}e_{i}(x)e_{i}(y),\] (5)

where \(N\) is an index set, \(\{_{i}\}_{i N}\) is the set of positive eigenvalues of \(T\) in descending order, and \(e_{i}\) is the corresponding eigenfunction. Furthermore, \(\{e_{i}\}_{i N}\) forms an orthonormal basis of \(\,S_{}} L^{2}\) and \(\{_{i}^{1/2}e_{i}\}_{i N}\) forms an orthonormal basis of \(\,S_{}^{*}}\).

The eigenvalues \(_{i}\) actually characterize the span of the RKHS and the interplay between \(\) and \(\). Since we are interested in the infinite-dimensional case, we will assume \(N=\) and assume the following polynomial eigenvalue decay as in the literature (Caponnetto and De Vito, 2007; Fischer and Steinwart, 2020; Li et al., 2023), which is also referred to as the capacity condition or effective dimension condition. Larger \(\) implies better regularity of the functions in the RKHS.

**Assumption 1** (Eigenvalue decay).: There is some \(>1\) and constants \(c_{},C_{}>0\) such that

\[c_{}i^{-}_{i} C_{}i^{-}(i=1,2,),\] (6)

where \(_{i}\) is the eigenvalue of \(T\) defined in (5).

Such a polynomial decay is satisfied for the well-known Sobolev kernel (Fischer and Steinwart, 2020), Laplace kernel and, of most interest, neural tangent kernels for fully-connected multilayer neural networks (Bietti and Mairal, 2019; Bietti and Bach, 2020; Lai et al., 2023).

### The embedding index of an RKHS

We will consider the embedding index of an RKHS to sharpen our analysis. Let us first define the fractional power \(T^{*}:L^{2} L^{2}\) for \(s 0\) by

\[T^{*}(f)=_{i N}_{i}^{s} f,e_{i}_{L^{2}} e_{i}.\] (7)

Then, the interpolation space (Steinwart and Scovel, 2012; Fischer and Steinwart, 2020; Li et al., 2023)\([]^{*}\) is define by

\[[]^{s}=\,T^{s/2}=\{_{i N}a_{i}_{i}^{s /2}e_{i}\ \ _{i N}a_{i}^{2}<\} L^{2},\] (8)with the norm \(\|_{i N}a_{i}_{i}^{s/2}e_{i}\|_{[]^{s}}=( _{i N}a_{i}^{2})^{1/2}\). One may easily verify that \([]^{s}\) is also a separable Hilbert space with an orthonormal basis \(\{_{i}^{s/2}e_{i}\}_{i N}\). Moreover, it is clear that \([]^{0}=S_{}} L^{2}\) and \([]^{1}=S_{}}\). It can also be shown that if \(s_{1}>s_{2} 0\), the inclusions \([]^{s_{1}}[]^{s_{2}}\) are compact (Steinwart and Scovel, 2012).

Now, we say \(\) has an embedding property of order \((0,1]\) if \([]^{}\) can be continuously embedded into \(L^{}(,)\), that is, the operator norm

\[\||[]^{} L^{}(,)\|=M_{ }<.\] (9)

Moreover, Fischer and Steinwart (2020, Theorem 9) shows that

\[\|[]^{} L^{}(,)\|=\| _{}^{}\|_{L^{}}*{ess\,sup}_{ x,\ }_{i N}_{i}^{}e_{i}(x)^{2}.\] (10)

Therefore, since \(_{x}k(x,x)^{2}\), we know that (9) always holds for \(=1\). By the inclusion relation of interpolation spaces, it is clear that if \(\) has the embedding property of order \(\), then it has the embedding properties of order \(^{}\) for any \(^{}\). Consequently, we may introduce the following definition (Zhang et al., 2023b):

**Definition 2.1**.: The embedding index \(_{0}\) of an RKHS \(\) is defined by

\[_{0}=\{:\||[]^{} L^{ }(,)\|=M_{}<\}.\] (11)

It is shown in Fischer and Steinwart (2020, Lemma 10) that \(_{0}\) and we assume the equality holds as the following assumption.

**Assumption 2** (Embedding index).: The embedding index \(_{0}=1/\), where \(\) is the eigenvalue decay in (6).

Lots of the usual RKHSs satisfy this embedding index condition. It is shown in Steinwart et al. (2009) that Assumption 2 holds if the eigenfunctions are uniformly bounded, namely \(_{i N}\|e_{i}\|_{L^{}}<\). Moreover, Assumption 2 also holds for the Sobolev RKHSs, RKHSs associated with periodic translation invariant kernels and RKHSs associated with dot-product kernels on spheres, see Zhang et al. (2023a, Section 4).

## 3 Main Results

Before presenting our main results, we have to introduce a source condition on the regression function. Since we will establish both precise learning rates, we have to characterize the exact smoothness order of \(f_{}^{*}\) rather than merely assume \(f_{}^{*}\) belongs to some interpolation space \([]^{s}\).

**Assumption 3** (Source condition).: There are some \(s>0\) and a sequence \((a_{i})_{i 1}\) such that

\[f_{}^{*}=_{i=1}^{}a_{i}_{i}^{s/2}i^{-1/2}e_{i}\] (12)

and \(0<c|a_{i}| C\) for some constants \(c,C\).

**Remark 3.1**.: Assumption 3 is also considered in Cui et al. (2021, Eq. (8)) and a slightly weaker version of it is given in Jin et al. (2021, Assumption 5). We only consider this simple form since there is no essential difference in the proof to consider the weaker version. From the definition (8) we can see that Assumption 3 implies \(f_{}^{*}[]^{t}\) for any \(t<s\) but \(f_{}^{*}[]^{s}\).

### Noisy case

Let us first consider the noisy case with the following assumption:

**Assumption 4** (Noise).: We assume

\[_{(x,y)}[(y-f_{}^{*}(x))^{2}\ \ x ]=^{2}>0,\ x.\] (13)For technical reason, we have to further assume the kernel to be Holder-continuous, which is first in introduced in Li et al. (2023b). This assumption is satisfied for the Laplace kernel, Sobolev kernels and neural tangent kernels.

**Assumption 5**.: The kernel \(k\) is Holder-continuous, that is, there exists some \(p(0,1]\) and \(L>0\) such that

\[|k(x_{1},x_{2})-k(y_{1},y_{2})| L\|(x_{1},x_{2})-(y_{1},y_{2})\|_{^{d d}}^{p}, x_{1},x_{2},y_{1},y_{2}.\] (14)

**Theorem 3.2**.: _Under Assumptions 1-5, suppose \( n^{-}\) for \(>0\). Then,_

\[[\|_{}-f_{}^{}\|_{L^{2}}^{2} \,\,X]=_{}n^{-(s,2)}+^{2}n^{-(1-/)},&< \\ _{}^{}^{2},& ,\] (15)

_where \(_{}\) can be replaced with \(_{}\) for the first case if \(s 2\)._

**Remark 3.3**.: The two terms in the first case in Theorem 3.2 actually correspond to the bias and the variance term respectively. Balancing the two terms, we find the optimal regularization is \(_{}=+1}\) and the optimal rate is \(}{+1}\), where \(=(s,2)\), which recovers the classical optimal rate results (Caponnetto and De Vito, 2007). Moreover, while we treat \(^{2}\) as fixed for simplicity, we can also allow \(^{2}\) to vary with \(n\). Then, we can recover the results in Cui et al. (2021).

Figure 1: An illustration of the learning curves when choosing \(=n^{-}\). First row: The bias-variance plot and the error curves for the noisy and noiseless cases. Second row: Tow phase diagrams of the asymptotic rates of the excess risk with respect to parameter pairs \((,s)\) and \((,)\), where we set \(^{2}=n^{-}\) and \(=(s,2)\). In the “underfitting” (“overfitting”) region, bias (variance) is dominating. The “interpolating” region refers to the extreme cases of overfitting that the excess risk is lower bounded by a constant. For the first diagram we consider the case of constant noise. For the second diagram, the red vertical line shows the crossover of the noisy regime to the noiseless regime and an upper bound for the blank area on the upper-right corner is unknown yet.

### Noiseless case

**Theorem 3.4**.: _Under Assumptions 1-3, assume further that the noise is zero, i.e., \(y=f_{}^{*}(x)\). Then, we have:_

* _Suppose_ \( n^{-}\) _for_ \((0,)\)_, we have_ \[[\|_{}-f_{}^{*}\|_{L^{2}}^{2} X]=_{}n^{-(s,2)},\] (16) _where_ \(_{}\) _can be replaced with_ \(_{}\) _if_ \(s 2\)_._
* _Suppose_ \( n^{-}\) _for_ \(\) _and assume further that_ \(s>1\)_. Then,_ \[[\|_{}-f_{}^{*}\|_{L^{2}}^{2} X]=O_{}^{}n^{-(s,2)} .\] (17) _Moreover, we have the information-theoretical lower rate:_ \[_{\|f_{}^{*}\|_{|_{s}|} R} [\|_{}-f_{}^{*}\|_{L^{2}}^{2}X ]=(n^{-s}),\] (18) _where_ \(R>0\) _is a fixed constant._

**Remark 3.5**.: Theorem 3.4 shows that the generalization error of KRR in the noiseless case is monotone decreasing when \(\) increases and reaches the optimal rate \(n^{-}\) when \(\) if \(s 2\). Since the case \(\) corresponds to kernel interpolation, our result implies that kernel interpolation is optimal when there is no noise. In contrast, as shown in Theorem 3.2 (or Li et al. (2023a)), kernel interpolation can not generalize in the noisy case. For the case \(s>2\), the KRR method suffers from saturation and the resulting convergence rate is limited to \(n^{-2}\), while the possible lower rate is \(n^{-s}\).

### Discussion

Our results provide a full picture of the generalization of KRR, which is in accordance with our traditional belief of the bias-variance trade-off principle: the generalization error is a U-shaped curve with respect to the regularization parameter \(\) in the noisy case and is monotone decreasing in the noiseless case. See Figure 1 on page 1 for an illustration.

Our rates coincide with the upper rates in the traditional KRR literature (Caponnetto and De Vito, 2007; Fischer and Steinwart, 2020). Moreover, our results also recover the learning curves in Cui et al. (2021), but we do not need the strong assumption of Gaussian design eigenfunctions as in Cui et al. (2021), which may not be true in most cases. Our assumptions are mild and hold for a large class of kernels including the Sobolev kernels and the neural tangent kernels (NTK) on spheres.

Our results are based on the bias-variance decomposition and determining the rates for each term respectively. In the proof of Li et al. (2023b), they determined the rates of the variance term under the condition that \(<\) and that of the bias term when \(s 2\) and \(<1\). The subsequent work Li et al. (2023a) proved the rates of the variance term when \(<\) and provided a near constant lower bound for \(\). Considering the counterpart, our works further prove the rates of the bias term, which finally enables us to determine the complete learning curve of KRR.

The connection between KRR and Gaussian process regression also results in the connection between their learning curves. Jin et al. (2021) claimed to show learning curves for Gaussian process regression. However, regardless of the gap in their proof as pointed out in Zhang et al. (2023b), their results are more restrictive than ours. Considering a boundedness assumption of the eigenfunctions that \(\|e_{i}\|_{} C{}^{}\) for some \( 0\), they could only cover the regime of \(</(1+2)\). Moreover, to approach the \(=\) regime for the \((1)\) bound in the noisy case or the optimal rate in noiseless case, they have to require \(=0\), that is, the eigenfunctions are uniformly bounded, but it is not true for some kernels such as dot-product kernels on spheres (and thus for NTK) since in general spherical harmonics are not uniformly bounded. In contrast, our embedding index assumption still holds in this case.

Proof sketch

We first introduce the following sample versions of the auxiliary integral operators, which are commonly used in the related literature (Caponnetto and De Vito, 2007; Fischer and Steinwart, 2020; Li et al., 2023b). We define the sampling operator \(K_{x}:\) by \(K_{x}y=yk(x,)\), whose adjoint \(K_{x}^{*}:\) is given by \(K_{x}^{*}f=f(x)\). The sample covariance operator \(T_{X}:\) is defined by

\[T_{X}_{i=1}^{n}K_{x_{i}}K_{x_{i}}^{*},\] (19)

and the sample basis function is \(g_{Z}_{i=1}^{n}K_{x_{i}}y_{i}\). As shown in Caponnetto and De Vito (2007), the operator form of KRR writes

\[_{}=(T_{X}+)^{-1}g_{Z}.\] (20)

Let us further define

\[_{Z}(g_{Z}|X)=_{i=1}^{ n}K_{x_{i}}f_{}^{*}(x_{i}),\] (21)

and

\[_{}(_{}|X )=(T_{X}+)^{-1}_{Z}.\] (22)

Then, the traditional bias-variance decomposition (Li et al., 2023b; Zhang et al., 2023a) yields

\[(\|_{}-f_{}^{*}\|_{L^{2}}^{2} X)=^{2}()+(),\] (23)

where

\[^{2}()\|_{}-f_{}^{*} \|_{L^{2}}^{2},()}{n^{ 2}}_{i=1}^{n}\|(T_{X}+)^{-1}k(x_{i},)\|_{L^{2}}^{2}.\] (24)

### The noisy case

To prove the desired result, we have to establish the asymptotic orders of both \(^{2}()\) and \(()\). We first prove the asymptotic order of \(^{2}()\) as one of our technical contributions. As far as we know, we are the first to provide such a lower bound in (25).

**Lemma 4.1**.: _Under Assumptions 1,2,3, suppose \( n^{-}\) for \((0,)\). Then,_

\[^{2}()=_{}n^{- (s,2)},\] (25)

_where \(_{}\) can be replaced with \(_{}\) if \(s 2\)._

Proof sketch of Lemma 4.1.: Denote \(=(s,2)\). We first introduce the regularized regression function \(f_{} T(T+)^{-1}f_{}^{*}\) and triangle inequality implies

\[()=\|_{}-f_{}^{*} \|_{L^{2}}\|f_{}-f_{}^{*}\|_{L^{2}}-\| _{}-f_{}\|_{L^{2}}.\]

There is no randomness in the first term and we can use the expansion (12) and (5) to show that \(\|f_{}-f_{}^{*}\|_{L^{2}}=(n^{- {s}})\). Then, we have to prove the error term \(\|_{}-f_{}\|_{L^{2}}\) to be infinitesimal with respect to the main term, which is the main difficulty since it requires a refined analysis. Previous work only consider the case \(=\) (corresponding to the optimal regularization) and show an \(O(n^{-})\) bound rather than the \(o(n^{-})\) bound that we require. For the proof, we (1) apply the concentration techniques in Fischer and Steinwart (2020); (2) consider the \(L^{q}\)-embedding property in Zhang et al. (2023a) for the mis-specified case when \(s\) is small; (3) sharpen the estimation by exploiting the embedding property \(_{0}=1/\) and \(<\). For the detail, see Section 2.2 in the supplementary material.

The variance term has been analyzed in Li et al. (2023). We present the following proposition as a combination of Proposition 5.3 and Theorem 5.10 in Li et al. (2023).

**Proposition 4.2**.: Under Assumptions 1-5, suppose that \( n^{-}\). Then,

\[()=_{}^{} ^{2}n^{-(1-/)},&<;\\ _{}^{}^{2},& .\] (26)

### The noiseless case

For the noiseless case, the variance term vanishes in (23), and thus we only need to consider the bias term. Since we have already established the estimation for large \(\) in Lemma 4.1, we focus on the case of small \(\).

**Lemma 4.3**.: _Under Assumptions 1,2,3, assume further \(s>1\). Suppose \( n^{-}\) for \(\). Then,_

\[^{2}()=O_{}^{}(n^{-(s,2)}).\] (27)

Proof sketch of Lemma 12.: Intuitively, we hope to bound \(^{2}()\) with \(^{2}()\) for \(>\) such that concentration still works. However, we can not directly derive no monotone property of \(()\). Nevertheless, since \(f_{}^{*}\) when \(s>1\), the bias term can be written as

\[()=(T_{X}+)^{-1}f_{}^{*} _{L^{2}}=T^{}(T_{X}+)^{-1}f_{}^{*} _{}T^{}(T_{X}+)^{-1} _{()}f_{}^{*}_{}.\]

Then, by operator calculus we can show that

\[T^{s}[(T_{X}+)^{-1}]_{( )}T^{s}[(T_{X}+)^{- 1}]_{()}\]

reducing \(\) to \(\). Now, we can replace \(T_{X}\) with \(T\) using concentration results and derive the desired upper bound. 

The following proposition shows that the upper bound in Lemma A.12 matches the information-theoretical lower bound. The proof follows idea of the minimax principle (Micchelli and Wahba, 1979) and is deferred to the supplementary material.

**Proposition 4.4**.: Suppose Assumption 1 holds and \(s 1\). For any \(X=(x_{1},,x_{n})\), we have

\[_{\|f_{}^{*}\|_{||^{s}} R}^{2}()= n^{-s},\] (28)

where we note that here \(()\) is viewed as a function depending also on \(f_{}^{*}\) and \(X\).

## 5 Experiments

Lots of numerical experiments on both synthetic data and real data are done to study to learning curves of KRR (Li et al., 2023, Cui et al., 2021). In this section, we consider numerical experiments on a toy model to verify our theory.

Let us consider the kernel \(k(x,y)=(x,y)\) and \(x\). Then, the corresponding RKHS is (Wainwright, 2019)

\[=\{f:\ \ ff(0)=0,\ _{0}^{1}(f^{}(x))^{2}x<\}\]

and the eigenvalue decay rate \(=2\). Moreover, the eigensystem of \(k\) is known to be \(_{i}=^{-2}\) and \(e_{i}(x)= x\), which allows us to directly compute the smoothness of certain functions. For some \(f^{*}\), we generate data from the model \(y=f^{*}(x)+\) where \((0,0.05)\) and perform KRR with \(=cn^{-}\) for different \(\)'s with some fixed constant \(c\). Then, we numerically compute the variance, bias and excess risk by Simpson's formula with \(N n\) nodes. Repeating the experiment for \(n\) ranged in 1000 to 5000, we can estimate the convergence rate \(r\) by a logarithmic least-squares \(=r n+b\) on the values (variance, bias and excess risk). The results are collected in Table 1 on page 10. It can be seen that the resulting values basically match the theoretical values and we conclude that our theory is supported by the experiments. For more experiments and more details, we refer to the supplementary material.

## 6 Conclusion

In this paper, we prove rigorously the learning curves of KRR, showing the interplay of the eigenvalue decay of the kernel, the relative smoothness of the regression function, the noise and the choice of the regularization parameter. The results justify our traditional bias-variance trade-off principle and provide a full picture of the generalization performance of KRR. These results will help us better understand the generalization mystery of neural networks.

As for future works, we notice that for the nearly interpolating regime when \(\), there are still some missing parts due to technical limitations. We expect that further analysis will prove the exact orders of the variance term like that given in Mallinar et al. (2022) under the Gaussian design assumption. We also hypothesize that Lemma A.12 still holds in the mis-specified case (\(s<1\)).