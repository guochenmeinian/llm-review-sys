# Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration

Haitao Lin

Westlake University

linhaitao@westlake.edu.cn

&Yufei Huang

Westlake University

huangyufei@westlake.edu.cn

&Odin Zhang

Zhejiang University

haotianzhang@zju.edu.cn

&Lirong Wu

Westlake University

wilirong@westlake.edu.cn

&Siyuan Li

Westlake University

lisiyuan@westlake.edu.cn

&Zhiyuan Chen

Deep Potential

chenzhiyuan@dp.tech

&Stan Z. Li

Westlake University

stan.zq.li@westlake.edu.cn

Corresponding Author.

###### Abstract

In recent years, AI-assisted drug design methods have been proposed to generate molecules given the pockets' structures of target proteins. Most of them are _atom-level-based_ methods, which consider atoms as basic components and generate atom positions and types. In this way, however, it is hard to generate realistic fragments with complicated structures. To solve this, we propose D3FG, a _functional-group-based_ diffusion model for pocket-specific molecule generation and elaboration. D3FG decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. And the two kinds of components can together form complicated fragments that enhance ligand-protein interactions. To be specific, in the diffusion process, D3FG diffuses the data distribution of the positions, orientations, and types of the components into a prior distribution; In the generative process, the noise is gradually removed from the three variables by denoisers parameterized with designed equivariant graph neural networks. In the experiments, our method can generate molecules with more realistic 3D structures, competitive affinities toward the protein targets, and better drug properties. Besides, D3FG as a solution to a new task of molecule elaboration, could generate molecules with high affinities based on existing ligands and the hotspots of target proteins.

## 1 Introduction

The established notion in drug-target identification is that similar structures perform similar functions. This principle allows classical computer-aided drug design (CADD) to abstract protein-ligand interactions as pharmacophores, aligning similar functional groups and extracting pharmaceuticalinformation from these structures. Fitting suitable functional groups into the pharmacophores can enhance ligand-protein interactions, thus improving drug efficiency [1; 2].

Artificial intelligence has already achieved outstanding success in protein design [3; 4; 5; 6; 7; 8] and thus led to a new round of attention in drug design focused on AI-assisted drug design (AIDD). A number of methods of AI-assisted drug design on pocket-specific molecule generation are emerging [9; 10; 11; 12; 13; 14; 15], thanks to developments in deep generative models [16; 17; 18; 19; 20; 21] and graph neural networks (GNNs) [22; 23; 24; 25; 26; 27; 28; 29; 30; 31]. These methods usually focus on atom-level generation, which first generates atom types and positions and then assembles atoms into molecules that can bind to the protein pockets. Although significant progress has been made, they are still weak in two aspects. For one thing, it is hard for them to generate realistic functional groups that contribute pharmacological effects to the target as classical CADD methods are able to. It is shown that generating benzene rings is uncommon compared with the reference molecules, not to mention some large functional groups with complex structure constraints such as purine, indole, etc (Table. 1 in Sec. 5.2). For another, trade-offs between efficiency and sufficiency of protein context place them in a dilemma. For example, TargetDiff  employs sufficient protein context, which disassembles the amino acids into atoms, but leads to inefficiency due to a large node number (412.14 on average) in GNN's message passing. DiffSBDD  simplifies the representation of protein context, by only using \(_{}\)'s positions and residue types, resulting in a reduction of GNNs' node number (68.10 on average) but the insufficiency of context information.

To address the above issues, we establish a functional-group-based **diffusion** model (D3FG), including the following contributions: **1. Method Novelty.** We denote the molecules' functional groups and proteins' amino acids as the same level's fragments, in which the intra-relative positions of atoms are fixed like rigid bodies, and represent single atoms as linkers. The positions and orientation of local structures and the atom type variables are generated gradually through denoising processes. The fragment-linker designation leads the binding systems to heterogeneous graphs, and thereby, two schemes are proposed as solutions, which achieve competitive performance in terms of molecule structures, binding affinity, and drug properties, and sufficiency in encoding protein context by employing more features. **2. Dataset Establishment.** Though the CrossDocked2020  has been a widely-used dataset for evaluation methods' performance on the task, the analysis of the molecules' functional groups of it is missing. We deeply explore the details of the inter-relative positions and types of functional groups of the molecules and establish an extendable database of common functional groups. **3. New task.** Besides molecule generation, we propose molecule elaboration as another task that our model can fulfill. Fragment hotspot maps (FHM) [33; 34] are used to preprocess paired protein-molecule in CrossDocked2020 for the task. As a result, our model generates molecules with high binding affinity based on the reference.

## 2 Problem Statement

For a binding system composed of a protein-molecule pair (also called protein-ligand pair) as \(\), which contains \(N_{}\) amino acids of proteins and \(N_{}\) functional groups and \(N_{}\) single atoms of molecules, we represent the index set of the molecule's single atoms as \(_{}\), functional groups as \(_{}\), and the protein's amino acids as \(_{}\), where \(|_{}|=N_{}\), \(|_{}|=N_{}\) and \(|_{}|=N_{}\). Note that a molecule can be disassembled into functional groups and single atoms other than functional groups, which we also call linkers. For a protein, the amino acids can be represented by its type, \(_{}\) atom coordinate, and the orientation, denoted as \(s_{i}\{1,,20\}\), \(_{i}^{3}\), \(_{i}(3)\), where \(i_{}\). For a molecule, assuming there are \(M_{}\) and \(M_{}\) possible types in total functional groups and linker atoms respectively, the functional groups can be represented as the three elements if the inter-relative positions are fixed, as \(s_{j}\), \(_{j}\) and \(_{j}\), where \(s_{j}\{21,,21+M_{}\}\) is the type, \(_{j}^{3}\) is the predefined center atom's coordinate, and \(_{j}(3)\) can also be obtained in the same way as amino acids (See Sec. 5.1 and Appendix. D.) for \(j_{}\); And its linkers can be represented as \(s_{k}\), \(_{k}\) and \(_{k}\), with \(s_{k}\{22+M_{},,22+M_{}+M_{}\}\), \(k_{}\) and \(_{k}=\{1,1,1\}=\).

Therefore, \(=\{(s_{i},_{i},_{i})\}_{i=1}^{N_{}+N_{ }+N_{}}\) can be split into two sets as \(=\), where \(=\{(s_{i},_{i},_{i}):i_{}\}\) and \(=\{(s_{j},_{j},_{j}):j_{} _{}\}\). For protein-specific molecule generation, our goal is to establish a probabilistic model to learn the distribution of molecules conditioned on the target proteins, _i.e._\(p(|)\). In the following, we omit \(i_{}\) and \(j_{}_{}\) by default unless specified.

Related Work

3D molecule generation.Previous methods on molecule generation fucus on 1D-smiles-string [35; 36; 37; 38] or 2D-molecule-graph [39; 40; 41; 42; 43]. In recent years, more works concentrate on 3D molecule generation, thanks to fast development in equivariant graph neural networks [44; 45; 46] and generative models [16; 17; 18; 19; 20; 21]. Molecular conformation generation aims to generate 3D structures of molecules with stability, given 2D molecule graph structure, [47; 48; 49; 50; 51]. Further, De novo molecular generation attempts to generate both 2D chemical formulas and 3D structures from scratch [52; 53; 54].

Fragment-based drug design.Previously, works on fragment-based molecule generation are proposed. For example, JT-VAE  generates a tree-structured scaffold over chemical substructures and combines them into a 2D-molecule. PS-VAE  can automatically discover frequent principal subgraphs from the dataset, and assemble generated subgraphs as the final output molecule in 2D. Further, DeepFrag  predicts fragments conditioned on parents and the pockets, SQUID  generates molecules in a fragment level conditioned on molecule's shapes. FLAG auto-regressively generates fragments as motifs based on the protein structures in 3D.

Structure-based drug design.Success in 3D molecule generation and an increasing amount of available structural data of protein and molecules raises scientific interests in structure-based drug design (SBDD), which aims to generate both 2D molecule graphs and 3D structures conditioned on target protein structure as contextual constraints. For example, LiGAN  and 3DSBDD  are grid-based models which predict whether the grid points are occupied by specific atoms. By harnessing 3D equivariant graph neural networks, Pocket2Mol  and GraphBP  generate atoms auto-regressively and model the probability of the next atom's type as discrete categorical attribute and position as continuous geometry. FLAG is also fragment-based, but still generates motifs in an auto-regressive way. Recently, utilizing the diffusion denoising probability models [61; 62; 63; 64], a series of SBDD methods generate ligands conditioned on the target pockets at full atom levels [13; 14; 15].

## 4 Method

D3FG firstly decomposes molecules into two categories of components: functional groups and linkers, and them use the diffsion generative model to learn the type and geometry distributions of the components. In this section, we describe the D3FG by four parts: (i) the diffusion model as the generative framework, in which the three variables are generated; (ii) the denoiser parameterized by graph neural networks, satisfying certain symmetries so that the generative model is SE-(3) invariant; (iii) the sampling process in which the molecules are generated by the trained models; (iv) the further problems resulted from heterogenous graph with two solutions.

### Diffusion Models

Diffusion models construct two Markov processes to learn the data distributions. The first called the forward diffusion process adds noises gradually until the noisy data's distribution reaches the prior distribution; The other called the generative denoising process, gradually removes the noise from the data sampled from the prior distribution until they are recovered to the desired data distribution. Assume there are \(T\) steps in both processes, and we denote \(^{t}=\{(s^{t}_{j},^{t}_{j},^{t}_{j})\}\) as the \(t\)-th noisy state in the forward process, with \(^{T}(^{T})\), and \(^{0}=\), where the transition distribution is denoted as \(q(|)\); In the generative process, sample goes from \(T\) to 0, in which the transition distribution is denoted as \(p(|)\). Here we define the forward and generative processes of \(s^{t}_{j}\), \(^{t}_{j}\), and \(^{t}_{j}\).

Absorbing diffusion for functional group and linker types.Let \(^{t}_{j}\) as the one-hot encoding of the type of a single functional group or linker. The forward process followed by D3PM  randomly transits \(s^{t}_{j}\) into the absorbing state \(K\) (\(K=23+M_{}+M_{}\)) with

\[ q(s^{t}_{j}|s^{t-1}_{j})&=(^{t-1}_{j}^{t})\\ q(s^{t}_{j}|s^{0}_{j})&=( ^{0}_{j}}^{t})\] (1)where \(}^{t}=^{1}^{2}^{t}\) and \([^{t}]_{mn}=q(s^{t}_{j}=n|s^{t-1}_{j}=m)\) denotes diffusion transition probabilities, with

\[[^{t}]_{mn}=1& m=n=K\\ 1-^{t}_{ type}& m=n K\\ ^{t}_{ type}& m K,n=K.\] (2)

\(^{t}_{ type}\) monotonically increases from 0 to 1, means that when \(t=T\), all the type variables are absorbed into the \(K\)-th category. In the generative process, it first samples \(N_{ at}\) linkers and \(N_{ fg}\) functional groups whose types are all in the absorbing states, selects \((1-^{t}_{ type}) 100\%\) of them respectively, and transforms their types from the absorbing states to the predicted ones by

\[p(s^{t-1}_{j}|^{t},)=(F( ^{t},)[j]).\] (3)

where \(F\) is the type denoiser parameterized with a neural network, and \(F(,)[j]\) predicts the probability of the types for the \(j\)-th selected functional groups or linkers. With the effectiveness of BERT-style training , the denoiser directly predicts \(p(s^{0}_{j}|^{t},)\), leading to a training objective as

\[L^{t}_{ type}=_{^{t}}[_{j} p(s^{0}_{j}| ^{t},)].\] (4)

Gaussian diffusion for center atom positions.By defining the center atom in a functional group as shown in Appendix. D, and itself as the center in a linker, the Cartesian coordinate of center nodes \(_{j}\) represents its position. The forward transition distributions followed by DDPM  read

\[ q(^{t}_{j}|^{t-1}_{j})& =(^{t}_{j}|_{ pos}} ^{t-1}_{j},^{t}_{ pos});\\ q(^{t}_{j}|^{0}_{j})&=( ^{t}_{j}|^{t}_{ pos}}^{0}_{j},(1-^{0}_{ pos})),\] (5)

in which \(^{t}_{ pos}\) increases from \(0\) to \(1\), means that the noise levels are increasing and the data's coordinate signals fade out during the forward diffusion, with \(^{t}_{ pos}=1-^{t}_{ pos},^{t}_{ pos}=^{ 0}_{ pos}^{1}_{ pos}^{t}_{ pos}\), and finally \(^{T}_{j}(,)\). Note that Eq. (5) is equivalent to the Markov process of \(^{t}_{j}=^{t}_{ pos}}^{0}_{j}+^{0}_{ pos}}_{j}\), where \(_{j}(,)\). Rather than predicting the mean value of the reverse transition distribution in the generative process, the position denoiser \(G\) approximates the added noise \(_{j}\) with the reparameterization trick as

\[ p(^{t-1}_{j}|^{t},)& =(^{t-1}_{j}|_{ pos}(^ {t},),^{t}_{ pos});\\ _{ pos}(^{t},)&= _{ pos}}}(^{t}_{j}-_{ pos }}{^{t}_{ pos}}}G(^{t},)[j] ).\] (6)

Figure 1: An illustration of the workflows of D3FG of the two schemes.

The training objective is thus established in a score-based way, as

\[L^{t}_{}=_{^{t}}[_{j} G( ^{t},)[j]-_{j}_{2}^{2}].\] (7)

SO(3) diffusion for functional group orientations. By regarding the functional groups as rigid bodies, orientations together with the center atoms' positions determine all atoms' positions. Here we represent the orientation geometry as elements in \((3)\). Following , we use isotropic Gaussian distribution on SO(3)  to formulate the process, i.e. \(_{(3)}(|_{},_{})\), in which \(_{}\) and \(_{}\) are viewed as mean orientation and variance, in analogy with Gaussian distribution. The transition distribution for orientation matrices \(_{j}\) reads

\[q(_{j}^{t}|_{j}^{0})=_{(3)}(_{j}^ {t}|_{}(_{}^{t},_{j}^{0}),(1- _{}^{t})),\] (8)

\(_{}(_{}^{t},_{j}^{0})\) is the geodesic flow from \(\) to \(_{j}^{t}\) by the amount \(_{}^{t}\), as \(_{}(_{}^{t},_{j}^{0})= (_{}^{t}(_{j}^{0}))\), where \(()\) and \(()\) are exponential and logarithm map on the SO(3) manifold. As \(_{}^{t} 0\), \(_{}(_{}^{t},_{j}^{0})\). \(\{_{}^{t}\}_{t=0}^{T}\) is the predefined noise level schedule ranging from \(0\) to \(1\) as \(t\) increases, \(_{}^{t}=1-_{}^{t}\) and \(_{}^{t}=_{}^{0}_{ }^{1}_{}^{t}\).

In the generative process, an orientation denoiser \(H\) is used to predict the mean orientation in the isotropic Gaussian distribution, which reads

\[p(_{j}^{t-1}|^{t},)=_{(3)} (_{j}^{t-1}|H(^{t},)[j],_{} ^{t}).\] (9)

We use the same loss function as in  to minimize the expected discrepancy measured by the inner product between the data orientation matrices and the predicted ones, which reads

\[L^{t}_{}=_{^{t}}[_{j}(H( ^{t},)[j])^{}_{j}^{0}-_{F}^{ 2}].\] (10)

### Parametrization of Denoisers with Neural Networks

Amino acid context encoding.In order to decrease the computational complexity, we denote the protein context at amino-acid levels. Besides the amino acid types, \(_{}\) atom coordinate and the orientation, each atom's coordinates in the local system and three torsion angles including angles around '\(\)-\(_{}\)' bond, '\(_{}\)-\(\)' bond and '\(\)-\(\)' bond, are also used as intra-amino-acid features, which are concatenated and embedded by an MLP to create the intra-amino-acid embedding vector \(_{i}\). For inter-amino-acid features, the pair of amino acid types, sequential relationships (if the two amino acids are adjacent in the protein sequence), pairwise distances between \(_{}\) and inter-residue dihedrals are all embedded as inter-amino-acid embedding vector \(_{i,j}\) with \(i,j_{}\). Note that these embedding vectors are all translational and rotational invariant (Appendix. C).

Denoisers with equivariance.In the setting of generative models, the learned distribution \(p(|)\) should be equivariant to translation and rotation, such that \(p(_{g}()|_{g}())=p(| )\) for any \(g(3)\), where \(_{g}\) is the corresponding roto-translational transformations, and \(_{g}()\) means each atom in the molecule is rotated and translated by \(_{g}\). In the setting of diffusion models, the following proposition indicates the translational and rotational equivariance of each denoisers.

Proposition 1.Let \(p(^{T})\), \(p(^{T})\), and \(p(s^{T})\) be SE(3)-invariant distribution, and the transition distributions \(p(^{t-1}|^{t},)\) be SE(3)-equivariant, \(p(^{t-1}|^{t},)\) be T(3)-invariant and SO(3)-equivariant, and \(p(s^{t-1}|^{t},)\) be SE(3)-invariant, then the density \(p(|)\) modeled by the reverse Markov Chains in the generative process of diffusion models is SE(3)-equivariant.

According to the **Proposition 1**, while the denoisers for functional group and linker types are rototranslational invariant, the denoiser for positions should be roto-translational equivariant, and it for orientations should be translational invariant and rotational equivariant. Therefore, we employ our denoiser's network structures with IPA  by harnessing the expressivity of Transformer and roto-translational equivariance of LoCS . Denote the binding system as a graph, in which the nodes are composed of amino acids, functional groups, and linkers, and the edges are established with K-nearest neighbor. Let \(\{_{i}:i_{}_{} _{}\}\) be the node embedding which is SE(3)-invariant, \(\{_{i}:i_{}_{} _{}\}\) and \(\{_{i,j}:i,j_{}_{} _{}\}\) be the previously defined intra- and inter-amino acid embedding, with \(_{j}=\) and \(_{i,j}=\) if \(i_{}\) or \(j_{}\). The attention mechanism in IPA updates the embedding of node \(i\) as

\[_{i}^{}=_{j(i)}(_{q}_{j}^{})^{}(_{k}_{i}^{})+_{i,j} (_{v}_{j}^{})}{_{j(i)}(_{ q}_{j}^{})^{}(_{k}_{i}^{})+_{i,j} }\] (11)

where \(_{i}^{}=_{i}+_{i}\), \(_{i}^{}\) is the updated node embedding, \(_{q}\), \(_{k}\), \(_{v}\) are learnable parameters, and \((i)\) is neighborhood of node \(i\) obtained by the edges. Because \(_{i}\), \(_{i}\), and \(_{i,j}\) are all SE(3)-invariant, \(_{i}^{}\) is also invariant. Three heads parameterized with MLP are stacked after several layers of Transformers update the node embedding, denoted by \(_{F}(),_{G}()\), and \(_{H}()\) is used for updating \(s^{t-1}\), \(^{t-1}\) and \(^{t-1}\). The LoCS updates the parameters in Eq. (3), (6) and (9) by

\[ F(^{t},)[j]& =_{F}(_{j}^{})\\ G(^{t},)[j]&= _{G}(_{j}^{})_{j}^{t}\\ H(^{t},)[j]&= _{H}(_{j}^{})_{j}^{t}\] (12)

The output of \(G\) means predicting the coordinate deviations in the local coordinate systems and then projecting it into the global one; In \(H\), \(_{H}\) first predicts a vector in Lie group \((3)\) and the exponential map on SO(3) converts it into a rotation matrix. The updating process ensures the equivariance and the invariance of the transition distributions in **Proposition 1**, which is proved in  and . However, since \(_{i}^{t}=\) for any \(i_{}\), the equivariance of \(G\) on linkers will not be preserved, so we instead use EGNN  to get the output of \(G\) (Appendix. C).

### Sampling Process

In sampling, we first use two prior distributions as the empirical distributions \(_{}\) and \(_{}\) drawn from the training set to sample the linker \(N_{}\) and functional group number \(N_{}\). As \(p(^{T})\), \(p(^{T})\) and \(p(s^{T})\) should be SE(3)-invariant, \(p(s^{T})=_{(K)}(^{T})\), \(p(^{T})=(^{T}|,)\) and \(p(^{T})=_{(3)}(^{T})\) satisfy the conditions, where \(_{(})\) is the indicator function and \(_{(3)}()\) is the uniform distribution on \((3)\). And the iteratively generative process of \(p(s^{t-1}|^{t},)\), \(p(^{t-1}|^{t},)\) and \(p(^{t-1}|^{t},)\) are given in Eq. (3), (6) and (9). The detailed sampling algorithm is given in Algorithm. 1.

``` Input: Zero-centered protein \(\{_{i},_{i},_{i}\}_{i_{}}\), and graph denoiser \(F,G,H\), and node number sampler \(_{}\), \(_{}\).  Sample \(N_{}_{},N_{}_{}\), leading to the index set \(_{}\) and \(_{}\).  Sample initial states of functional groups, \(\{s_{j}^{T},_{j}^{T},_{j}^{T}\}_{j_{} _{}}\), where \(s_{j}^{t}=K\), \(_{j}^{T}(,)\), \(_{j}^{T}_{(3)}(^{T})\) for \(j_{}\) else \(_{j}^{T}=\). for\(t\) in \(T-1,T-2,,1\)do  Sample \(\{s_{j}^{t-1},_{j}^{t-1},_{j}^{t-1}\}_{j_{} _{}}\) as Eq. (3), (6) and (9) and update \(^{t-1}\). endfor Output:\(=\{s_{j}^{0},_{j}^{0},_{j}^{0}\}_{j_{ }_{}}\) ```

**Algorithm 1** Joint Generation for Molecules using D3FG

### Heterogeneous graph: Joint or Two-stage?

Amino acids and functional groups are both fragments composed of atoms in proteins and molecules, regarded as rigid bodies, while the linkers are single atoms, regarded as mass points. Therefore, in the binding graph, nodes are of different levels and connections are of different kinds, thus leading the graph to be heterogeneous. For this reason, we propose two generative schemes: joint generation scheme and two-stage generation scheme as shown in Figure. 1.

In joint scheme, we regard amino acids, functional groups, and linkers at the same level, and use one single neural network to predict the three variables and update them. In detail, this scheme directly models \(p(|)=p(\{s_{i},_{i},_{i}\}_{i_{ }_{}}|)\), where the parameterized transition distribution is \(p(\{s_{i}^{t-1},_{i}^{t-1},_{i}^{t-1}\}_{i_{}_{}}|^{t},)\). Note that \(_{i}^{t}=\) for any \(t\) if \(i_{}\).

In two-stage scheme, we regard amino acids and functional groups at the fragment level, and linkers at the atom level, and use two different neural networks to parameterize the transition distribution. In the first stage, the functional groups are generated, and then single atoms as linkers will be generated to connect the generated functional groups as a full molecule. The two-stage generative scheme is similar to CADD, which first determines the pharmacophores towards the target protein, fits functional groups with high activity into them, and then searches for the possible molecules with these functional groups. In specific, the generative process reads \(p(|)=p(\{s_{j},_{j}\}_{j_{}} |\{s_{i},_{i},_{i}\}_{i_{}},) (\{s_{i},_{i},_{i}\}_{i_{}}|)\). The transition distribution of the first stage \(p(\{s_{i}^{t-1},_{i}^{t-1},_{i}^{t-1}\}_{i_{}}|\{s_{i}^{t},_{i}^{t},_{i}^{t}\}_{i_{}}, )\) is parameterized by one neural network; In the second stage, the generated \(\{s_{i},_{i},_{i}\}_{i_{}}\) is used as context, so that the other neural network models \(p(\{s_{i}^{t-1},_{i}^{t-1}\}_{i_{}}|\{s_{i}^{t}, _{i}^{t}\}_{i_{}},\{s_{i},_{i},_{i} \}_{i_{}},)\).

## 5 Experiments

### Data Processing.

In the experiments, we use CrossDocked2020 for evaluation. In the prevailing works, they focus on generating molecules at the atom level, differing from our functional-group-based generation, so our first target is to divide the molecules into functional groups and linkers. We use EFGs to segment molecules. We select the 25 functional groups that appear most frequently with stable structures, which are partly shown in Figure. 2 For some functional groups, chirality exists in their structures, and we treat them as two functional groups. As a result, we finally build up a dataset as a corpus including 27 functional groups (two of the 25 have chirality) for Crossdocked2020, with their intra-structures fixed as rigid bodies, and assure that most molecules can be decomposed into the substructures in our corpus datasets. Details are given in Appendix. D. For linkers, we choose \(\{,,,,,,, ,,\}\) as representative heavy atoms. After the processing, we obtain \(M_{}=27\) and \(M_{}=10\) in our experiments. Besides, by the fragment-linker designation of a binding graph, the node number is reduced to 53.62 on average in GNN's message-passing.

### Pocket-Specific Molecule Generation

Dataset.The datasets for training and evaluation are split according to Pocket2Mol and TargetDiff. 22.5 million docked protein binding complexes with low RMSD ( < 1A) and sequence identity less than \(30\%\) are selected, leading to 100,000 pairs of pocket-ligand complexes, with 100 novel complexes as references for evaluation.

Setup.For performance comparison, our methods are compared with baselines including LiGAN, 3DSBDD, GraphBP, Pocket2Mol, DiffSBDD and TargetDiff. LiGAN as a 3D CNN-based method generates atoms on regular grids, with VAE as its generative model. 3DSBDD, GraphBP and Pocket2Mol are all GNN-based, generating atoms in an auto-regressive way. DiffSBDD and TargetDiff are two diffusion-based methods that generate molecules at the full atom level, with equivariant GNNs as the denoisers. The two schemes of generation lead to two variants of our method, written as D3FG(Joint) and D3FG(Stage). In some parts, we choose Pocket2Mol as the representative autoregressive methods, and DiffSBDD and TargetDiff as the benchmarks employing diffusion models, because these three baselines are the

Figure 2: Five of twenty-five functional groups with stable structures that occur most frequently in Crossdocked2020 and are used in D3FG.

latest works that show good empirical performance. We sample 100 valid molecules for each pocket for the baselines and D3FG, leading to 10,000 pairs of complexes. After the molecules are generated by the model, Openbabel is used to construct chemical bonds between atoms, and Universal Force Field (UFF) minimization  is used for refinement. The following evaluations are based on the samples. Figure. 4 gives an example of generated molecules by different methods.

Structure analysis.We first analyze the **functional groups** of generated molecules by different methods. We show the 'Ratio' and 'Freq' of the included functional groups partly, where 'Ratio' means how many specified functional groups are in each molecule on average, and 'Freq' means the statistical frequency of occurrence of the specified functional group in the generated molecules. Mean absolute error (MAE) as overall metrics is calculated according to reference 'Ratio' and generated 'Ratio'; Jensen-Shannon divergence (JSD) is calculated according to reference 'Freq' and generated 'Freq'. The smaller MAE and JSD are, the better performance the method achieves. Table. 1 shows the metrics of different methods' generated molecules, demonstrating D3FG's superiority in generating molecules with realistic drug structures since distributions of the functional groups in generated molecules are more similar to the real drug molecules'. Secondly, we analyze the **atom type**, **bond distance**, **bond angle**, **dihedral** and **atom type** distribution. JSDs are calculated between the distributions of bond distance for reference vs. generated molecules. '-', '=',':' represents single, double, and aromatic bonds, respectively. Besides, we report MAE on reference 'Ratio' vs. generated 'Ratio' and JSD on reference 'Freq' vs. generated 'Freq' based on atom type distribution. Table. 2, 3 and Figure. 3 gives details atom type analysis. For other geometries, please refer to Appendix. E. We found that D3FG(Stage) outperforms D3FG(Joint) in generating more realistic molecules, and has competitive performance with TargetDiff.

Binding affinity.We secondly make a comparison of Binding Affinity. Following 3DSBDD and L1GAN, we employed two evaluation metrics including Vina docking score  and Gnina docking score . Vina docking  as a classical docking tool, gives a lower score as Vina energy if the binding affinity of the molecule is better, while Gnina docking as a deep-learning-based docking tool, gives it a higher score. \(\)Affinity measures the percentage of generated molecules that have better predicted binding affinity than the corresponding reference molecules. Table. 4 shows that our D3FG

   } & } & } & } & } & } & } & } \\  C-C & 0.599 & 0.424 & 0.416 & 0.346 & 0.385 & 0.339 & **0.281** \\ C=C & 0.665 & 0.545 & 0.516 & 0.503 & 0.565 & 0.485 & **0.469** \\ C-N & 0.631 & 0.424 & 0.401 & **0.299** & 0.421 & 0.307 & 0.313 \\ C=N & 0.742 & 0.671 & 0.628 & 0.547 & 0.569 & 0.530 & **0.523** \\ C-O & 0.656 & 0.547 & 0.445 & 0.408 & 0.413 & 0.412 & **0.406** \\ C=O & 0.662 & 0.638 & 0.532 & **0.467** & 0.541 & 0.490 & 0.488 \\ c:c & 0.494 & 0.410 & 0.398 & **0.264** & 0.339 & 0.327 & 0.302 \\ c:n & 0.634 & 0.443 & 0.457 & **0.228** & 0.260 & 0.246 & 0.237 \\   

Table 2: Jensen-Shannon divergence between the distributions of bond distance for reference vs. generated molecules. The smaller, the better. Value in **bold** is the lowest.

   } & } & } & } & } &  &  &  \\  C-C & 0.599 & 0.424 & 0.416 & 0.346 & 0.385 & 0.339 & **0.281** \\ C=C & 0.665 & 0.545 & 0.516 & 0.503 & 0.565 & 0.485 & **0.469** \\ C-N & 0.631 & 0.424 & 0.401 & **0.299** & 0.421 & 0.307 & 0.313 \\ C=N & 0.742 & 0.671 & 0.628 & 0.547 & 0.569 & 0.530 & **0.523** \\ C-O & 0.656 & 0.547 & 0.445 & 0.408 & 0.413 & 0.412 & **0.406** \\ C=O & 0.662 & 0.638 & 0.532 & **0.467** & 0.541 & 0.490 & 0.488 \\ c:c & 0.494 & 0.410 & 0.398 & **0.264** & 0.339 & 0.327 & 0.302 \\ c:n & 0.634 & 0.443 & 0.457 & **0.228** & 0.260 & 0.246 & 0.237 \\   

Table 2: Jensen-Shannon divergence between the distributions of bond distance for reference vs. generated molecules. The smaller, the better. Value in **bold** is the lowest.

of the two-stage scheme achieves competitive affinity scores, comparable to TargetDiff and much better than D3FG of the joint scheme.

Drug property.Moreover, chemical properties are evaluated with RdKit, including QED  (quantitative estimation of drug-likeness), SA  (synthetic accessibility score), LogP  (the octanol-water partition coefficient, which should be between \(-0.4\) and \(5.6\) for good drug candidates), and Lipinski [83; 84] (number of rules the drug follow the Lipinski's rule of five). QED, SA, and Lipinski are three metrics with preferences for atom numbers, demonstrated in DiffBP. Table. 4 demonstrates that two variants of D3FG achieve overall best performance in these metrics.

    & Vina & Vina & Gina & Gina &  &  &  &  \\  & Score (\(\)) & Affinity (\(\)) & Score (\(\)) & & & & \\  Ref. (Test) & -7.06 & & 5.37 & - & 0.471 & 0.725 & 0.818 & 4.247 \\ LiGAN & -6.17 & 21.24\% & 4.29 & 21.68\% & 0.382 & 0.584 & -0.138 & 4.046 \\ GraphBP & -6.36 & 27.41\% & 4.52 & 26.54\% & 0.437 & 0.502 & 3.024 & 4.448 \\ JDSBDD & -6.12 & 20.73\% & 4.48 & 19.22\% & 0.426 & 0.625 & 0.266 & 4.735 \\ PockerZMol. & -6.92 & 45.86\% & 5.34 & 40.68\% & **0.543** & 0.746 & 1.584 & 4.904 \\ TargetDiff & **-7.11** & **49.52\%** & **5.41** & **42.40\%** & 0.474 & 0.581 & 1.402 & 4.487 \\ DiffSBDD & -6.37 & 31.32\% & 4.63 & 27.96\% & 0.494 & 0.343 & -0.157 & 4.895 \\ D3FG (Joint) & -6.89 & 37.32\% & 5.30 & 33.45\% & **0.507** & **0.832** & 2.796 & **4.943** \\ D3FG (Stage) & **-6.96** & **45.88\%** & **5.43** & **43.36\%** & 0.501 & **0.840** & 2.821 & **4.965** \\  D3FG (EHot) & -7.19 & 51.78\% & 5.51 & 56.53\% & 0.482 & 0.731 & 0.814 & 4.330 \\ D3FG (ECold) & -7.02 & 44.03\% & 5.16 & 32.69\% & 0.476 & 0.707 & 0.820 & 4.228 \\   

Table 4: Evaluation of Binding affinity and other chemical drug properties for baselines and variants of D3FG. \(\) means the smaller the value, the better the performance, and \(\) means the opposite. Values in **bold** are the top-2 best metrics.

Figure 4: Generated molecules by different methods on pocket 3o96_a_rec. The diffusion-based methods generated molecules more similar to the reference, appearing to be ‘vertical’.

Figure 3: Atom type distribution and metrics.

### Pocket-Specific Molecule Elaboration

Introduction.Molecule elaboration as a sub-task of molecule optimization, aims to elaborate a fragment of existing molecules amenable to chemical modification for improving binding affinity. To fulfill it, we here first select a functional group in a ligand that contributes to binding affinity and remove it to obtain the editable fragments. Then, we attempt to use D3FG to generate the new type of functional groups with its structures, for modifying the fragments and thus building up a new molecule with higher binding affinity to the target protein.

Dataset.The selection of a functional group for replacement is the first problem. Pharmacophoric information is extracted by calculating the fragment hotspot map (FHM) [33; 34] of the target protein. In specific, FHM describes regions of the binding pocket that are likely to make a positive contribution to binding affinity. Then, by placing the molecules into the pockets, we can thus obtain each functional group's hotspot scores, according to the binding complexes. The higher hotspot score a functional group reaches, the more contributions it makes to the binding affinity. Functional groups' hotspot scores of each ligand are calculated based on 100,000 pairs of pocket-ligand complexes in Crossdocked2020, and the selection of functional groups can be based on their scores. Finally, we established our new datasets for molecule elaboration based on FHM.

Binding affinity and drug property.We here consider two schemes for molecule elaboration, the first is to remove the functional groups with the highest scores, and elaborate the remaining fragments by replacing them with the functional group generated by D3FG. We write this as D3FG(EHot). The second D3FG(ECold), on the other hand, replaces the functional groups of the lowest scores). We report the D3FG's elaborated molecules by the two schemes in Table. 4. It shows that D3FG(EHot) tends to generate more molecules with higher affinity, while molecules elaborated by D3FG(ECold) show tiny differences in binding affinity from the raw references. Besides, on other chemical properties, the elaborated molecules are very close to the raw references, since the differences lie only in one single functional group, and the major molecular skeletons are almost identical.

## 6 Conclusion and Future Work

In this paper, a functional-group-based diffusion model called D3FG is proposed to generate molecules in 3D with protein structures as its context. Joint and two-stage generation schemes lead to two variants of D3FG. The molecules generated by the two-stage generation scheme show more realistic structures, competitive binding performance, and better drug properties. Besides, in molecule elaboration, D3FG can also generate molecules with good binding affinity. However, limitation still exists. First, the functional group datasets are still small, which will be enlarged in the future. Second, the binding affinities of generated molecules still remain to be improved, since other diffusion models even show better binding performance.