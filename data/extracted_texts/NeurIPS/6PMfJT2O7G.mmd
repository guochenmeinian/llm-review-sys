# On the Optimality of Dilated Entropy and Lower Bounds for Online Learning in Extensive-Form Games

Zhiyuan Fan

MIT

fanzy@mit.edu

&Christian Kroer

Columbia University

christian.kroer@columbia.edu

&Gabriele Farina

MIT

gfarina@mit.edu

###### Abstract

First-order methods (FOMs) are arguably the most scalable algorithms for equilibrium computation in large extensive-form games. To operationalize these methods, a distance-generating function, acting as a regularizer for the strategy space, must be chosen. The ratio between the strong convexity modulus and the diameter of the regularizer is a key parameter in the analysis of FOMs. A natural question is then: what is the _optimal_ distance-generating function for extensive-form decision spaces? In this paper, we make a number of contributions, ultimately establishing that the weight-one dilated entropy (DilEnt) distance-generating function is optimal up to logarithmic factors. The DilEnt regularizer is notable due to its iterate-equivalence with Kernelized OMWU (KOMWU)--the algorithm with state-of-the-art dependence on the game tree size in extensive-form games--when used in conjunction with the online mirror descent (OMD) algorithm. However, the standard analysis for OMD is unable to establish such a result; the only current analysis is by appealing to the iterate equivalence to KOMWU. We close this gap by introducing a pair of primal-dual _treeplex_ norms, which we contend form the natural analytic viewpoint for studying the strong convexity of DilEnt. Using these norm pairs, we recover the diameter-to-strong-convexity ratio that predicts the same performance as KOMWU. Along with a new regret lower bound for online learning in sequence-form strategy spaces, we show that this ratio is nearly optimal. Finally, we showcase our analytic techniques by refining the analysis of Clairvoyant OMD when paired with DilEnt, establishing an \((n|| T/T)\) approximation rate to coarse correlated equilibrium in \(n\)-player games, where \(||\) is the number of reduced normal-form strategies of the players, establishing the new state of the art.

## 1 Introduction

Extensive-form games (EFG) are a popular framework for modeling sequential games with imperfect information. The framework has been widely used to build superhuman AI agents in real-world imperfect information games . Several notions of equilibrium, including Nash equilibrium  in two-player zero-sum and coarse correlated equilibrium in general multiplayer EFGs, can be computed in polynomial time in the size of the game tree under the standard hypothesis of perfect recall . These polynomial-time algorithms, however, require running the ellipsoid method or polynomial algorithm for linear programming, both of which are impractical for large-scale games, due to the high memory usage and large per-iteration computational costs .

Instead, fast iterative methods based on convex first-order optimization methods (FOMs)  are commonly used to find an approximate equilibrium. These iterative methods define strategy update rules that each player can apply iteratively while training in self-play with other players, and that guarantee ergodic convergence to the set of equilibria in the long run. Three popular classes of such FOMs are employed in EFGs: methods based on online mirror descent (OMD) , methods based on the counterfactual regret minimization framework , and (in the context of two-player zero-sum games specifically) accelerated offline methods such as mirror prox  and the excessive gap technique  algorithm. In general, these methods are all proximal methods--that is, they perform a generalized notion of projected gradient descent step at each iteration. Some do this explicitly, including OMD and mirror prox, while others do it implicitly, including the counterfactual regret minimization algorithm, which runs proximal steps locally at each decision point .

In all methods mentioned above, except CFR,1 the constraint set for the proximal step (_i.e._, the set on which gradient steps must be projected onto) is the strategy polytope of the EFG. The proximal steps are parameterized by a choice of _distance-generating function (DGF)_ for the strategy polytope, which acts as a regularizer. The performance of FOMs is sensitive to the properties of the DGF. In particular, two qualities are often desired: (1) the ratio between the diameter of the feasible domain (as measured with the DGF) and the strong convexity modulus, with respect to a given norm, of the DGF must be as small as possible; and (2) projections with respect to the DGF onto the feasible set should take linear time in the dimension of the set.

In EFGs, the only DGF family that satisfies the second requirement is based on the framework of _dilated_ regularization introduced by Hoda et al. . Within this framework, Kroer et al.  gave the first explicit strong convexity bounds based on the dilation framework, specifically for the _dilated entropy DGF_. By combining optimistic regret minimizers for general convex sets with this DGF, one gets an algorithm that achieves a \(T^{-1}\) convergence rate for two-player zero-sum EFGs. Subsequent work by Farina et al.  introduced the _dilated global entropy_ DGF with an improved diameter-to-strong-convexity ratio. By plugging their DGF results into the generic OMD regret bound, one immediately achieves a regret bound of \((\|\|_{1}|})\). This was the state-of-the-art regret bound when introduced, in terms of dependence on game constants. Moreover, until now, it was the best bound known to be achievable through the direct application of OMD regret bounds combined with DGF results. However, Farina et al.  developed a, seemingly, different approach based on _kernelization_, which is a way to simulate, in linear time in the EFG size, the results of applying optimistic multiplicative weights (OMWU) on the normal-form reduction of an EFG. They call their algorithm _KOMWU_. KOMWU achieves a better, and now state-of-the-art, regret bound \((|})\) in online learning with full-information feedback. Based on their result, two open questions emerged: 1) is this the best possible regret bound that one can achieve? 2) is it possible to achieve such a bound directly using the standard OMD machinery, without resorting to this kernelization trick? Bai et al.  made highly interesting progress on the second question: they show that, in fact, the KOMWU algorithm is iterate equivalent to OMD, with the specific version of dilated entropy that uses weight one everywhere. However, their result only shows a state-of-the-art rate by equivalence to KOMWU, and it is still unknown whether this state-of-the-art rate is achieveable directly through results on DGF properties and the standard OMD regret bound. In this paper, we answer these two open questions, by answering the following question:

_What is the optimal DGF for FOMs in solving EFGs?_

We show that weight-one dilated entropy (DilEnt) is indeed the optimal DGF for FOMs in solving EFGs with full-information feedback, in terms of the diameter-to-strong-convexity ratio (\(||/\)), up to logarithmic factors. We note that the diameter-to-strong-convexity ratio of the regularizer is a key factor in the performance of FOMs. Intuitively, performance degrades as the diameter increases (since there is more "space" to search), and improves as the regularizer becomes more bowl-shaped (i.e., strongly convex). Consequently, a smaller diameter-to-strong-convexity ratio leads to better performance of the corresponding FOM. Our contributions can be summarized as follows:

* We introduce a pair of primal-dual treeplex norms for the extensive-form decision space. These norms establish an improved framework for analyzing FOMs in EFGs, leading to results with better dependence on the size of the game. Based on this framework, we derive a new state-of-the-artdiameter-to-strong-convexity ratio among all known DGFs for the EFG strategy spaces (see Table 1 for comparison). By combining this new results with the standard OMD regret bound, we establish a regret upper bound that aligns with the results achieved by KOMWU.
* By establishing a matching regret lower bound, we identify a minimum diameter-to-strong-convexity ratio for any regularizer. We find that the DilEnt regularizer achieves the optimal ratio up to logarithmic factors, making it a natural candidate for FOMs on EFGs.
* An advantage of our new results, as compared to showing a regret bound through the KOMWU equivalence, is that our DGF result can also be combined with other algorithmic setups. As an example of our results, we show that by equipping Clairvoyant OMD  with the DilEnt DGF, we enable convergence to a coarse correlated equilibrium at a rate of \((n|| T/T)\) in \(n\)-player EFGs. This improves upon the previous results of \((n||^{4}T/T)\) by Farina et al.  and \((n|||||_{1}^{2} T /T)\) by Farina et al. , establishing a new state-of-the-art rate.

## 2 Related Works

Equilibrium Computation in General-Sum Extensive-Form GamesThe first line work in the equilibrium computation on general-sum EFGs used linear program (LP) which can be solved efficiently [35; 24]. However, due to the large exponent of LP solvers, it is impractical to run such algorithms on large-scale games. The modern equilibrium computation using fast-iterative methods: Syrgkanis et al.  introduced the RVU property on the regret bound for a broad class of optimistic no-regret learning algorithms. With that property, they demonstrated that the individual regret of each player grows as \(T^{1/4}\) in general games, thus leading to a \(T^{-3/4}\) converge rate to the coarse correlated equilibrium (CCE). A near-optimal bound of order \(^{4}(T)\) was established by Daskalakis et al. , which implies a fast convergence rate of order \(}(1/T)\). Subsequent work by Farina et al.  generalized the result to a class of polyhedral games that includes EFG. Concurrently, Piliouras et al.  introduced the Clairvoyant MWU. Although the algorithm is not non-regret learning, a subset of the steps converge to CCE with a rate of \( T/T\). Farina et al.  showed that the algorithm is an instantiation of the conceptual proximal methods, which has been studied in the literature of FOMs [10; 33]. Using another technique, Farina et al.  also achieved this rate with worse game size dependence. Another class of fast iterative methods follows from counterfactual regret minimization (CFR) , which guarantees a regret bound of order \(\). Farina et al.  showed that running OMWU at each decision point achieves a \(T^{1/4}\) external regret, thus leading to a \(T^{-3/4}\) approximation rate to the CCE. Although CFR has a weaker guarantee on convergence rate, variants of the algorithm are widely used in practice due to their superior practical performance .

Regret Lower Bounds in Extensive-Form GamesSeveral works have studied lower bounds in EFGs across various settings. Koolen et al.  established a lower bound dependent on the number of orthogonal strategy profiles in the decision set for structured games, including EFGs, resulting in a bound of \((|})\). Syrgkanis et al.  demonstrated that in two-player zero-sum games, if one player uses MWU while the other best responds, the former must endure a regret of at least

   Regularizer & Norm pair & \(||/\) ratio & Max gradient norm \\  Dilated Entropy  & \(_{1}\) and \(_{}\) norms & \((2^{D}\|\|_{1}^{2}||)\) & \( 1\) \\ Dilated Gl. Entropy  & \(_{1}\) and \(_{}\) norms & \((\|\|_{1}^{2}||)\) & \( 1\) \\  DilEnt (**this paper**) & treeplex norms & \(||\) & \( 1\) \\   

Table 1: Comparison of the diameter-to-strong-convexity (\(||/\) ratio with prior results in DGFs for EFGs, where the “Norm pair” indicates the primal norm used in establishing the strong convexity, and its dual. “Max gradient norm” indicates the maximum norm—measured in the dual of the norm with respect to which each DGF is strongly convex—of any reward vector, or the gradient of utility function, that can be encountered during optimization, assuming that all payoffs at the terminal nodes of the EFG are in the range \(\). \(D\) denotes the depth of the tree, \(\|\|_{1}\) the tree size (see Section 3), \(||\) the maximum number of actions, and \(||\) the number of reduced normal-form strategies. We remark that \(||(\|\|_{1}||)\).

\(()\). Similarly, Chen and Peng  gave the same lower bound when both players use MWU. For equilibrium computation, Anagnostides et al.  analyzed the sparse-CCE in EFGs, showing that under certain assumptions, no polynomial-time algorithm can learn an \(\)-CCE with less than \(2^{_{2}^{1/2-o(1)}||}\) oracle accesses to the game for even constantly large \(>0\), where \(||\) is the number of nodes of the EFG. In the context of stochastic bandit, Bai et al. , Fiegel et al.  investigated online learning in EFGs with bandit feedback, establishing matched lower and upper bounds.

## 3 Preliminaries

General NotationWe use lowercase boldface letters, such as \(\), to denote vectors. Let \(\) represent the element-wise product of two vectors, and \(||\) the element-wise absolute value. For an index set \(\), denote by \([]^{}\) the entries of \(\) at indices in \(\), and by \(||\) the set cardinality. Let \( k\) be the set \(\{1,2,,k\}\) and \(\) the empty set. Denote the simplex over the set \(\) by \(^{}\). The logarithm of \(x\) to base \(2\) is denoted as \( x\). For non-negative sequences \(\{a_{n}\}\) and \(\{b_{n}\}\), \(a_{n}(b_{n})\) or \(b_{n}(a_{n})\) indicates the existence of a global constant \(C>0\) such that \(a_{n} Cb_{n}\) for all \(n>0\).

Extensive-Form GamesAn extensive-form game (EFG) is an \(n\)-player game with a sequential structure that can be represented using a tree. A detailed definition of EFG is available in Appendix A. Each node represents a game state where an agent (a.k.a. player) \(i n\) or the environment takes action. We use superscript \((i)\) to denote properties of player \(i\), but also omitting the superscript when context allows. Internal nodes branch into feasible actions. At these nodes, the designated player selects an action, advancing the game to the subsequent state according to the tree. The game concludes at a terminal node \(z\), where players receive a reward \([z]\). The goal of each player is to maximize their expected reward. We assume the reward for each player is bounded by \(1\) as follows:

**Assumption 3.1**.: The reward received by player \(i\) at any terminal node \(z\) satisfies \(^{(i)}[z]\).

Tree-Form Sequential Decision ProcessIn an EFG, an individual player \(i\)'s decision problem can be modeled by a tree-form sequential decision process (TFSDP). Let \(\) denote the set of decision points, where each point \(j\) corresponds to an information set in the EFG. At each decision point, the player is provided with a set of available actions \(_{j}\) and must select an action \(a_{j}\). After an action \(a\) is taken at decision point \(j\), the game either concludes before the player acts again or continues to a set of possible next decision points determined by actions of other players or by stochastic events. We denote the set of potential subsequent decision points as \(_{ja}\), which are reached immediately after action \(a\) at decision point \(j\). The tree structure guarantees non-overlapping successors, meaning \(_{ja}_{j^{}a^{}}=\) for any distinct pairs \(ja\) and \(j^{}a^{}\), where \(j j^{}\) or \(a a^{}\). This encapsulation of all past actions and outcomes at each decision point is known as _perfect recall_.

The point-action pair \(ja\), where action \(a\) is taken at decision point \(j\), is referred to as an _observation point_. This leads to a new state influenced by other agents and the environment. We denote the set of all point-action pairs as \(^{+}:=\{ja j,a_{j}\}\). Each decision point \(j\) has a parent \(p_{j}\), the last observation point on the path from the root of the decision process to \(j\). If no action precedes \(j\), \(p_{j}\) defaults to the special observation point \(\). We define \(:=^{+}\{\}\) as the set of observation points, each also called a _sequence_. The total set of points in the TFSDP, \(:=\), includes both decision points and sequences. We use \(h\) for unspecified point types. The TFSDP concludes at terminal observation points \(=\{:_{}=\}\), where reward \([]\) is observed. Under Assumption 3.1, it holds that \([]\).

Strategies and Transition KernelsA strategy profile for a player in a TFSDP is an assignment of probability distributions over actions \(_{j}\) at each decision point \(j\). As customary when using convex optimization techniques in EFGs, we represent strategies in the _sequence-form representation_. This representation stores a player's strategy as a vector whose entries represent the probability of _all_ of the player's actions on the _path_ from the root to the points. Since products of probabilities on paths are stored directly as variables, expected utilities are _multilinear_ in the sequence-form representation of the players' strategies. For symmetry reasons which will become apparent later, we slightly depart from the typical definition of the sequence form, by storing the product of a player's action probabilities on paths from the root to _all_ points in the tree--not only those that belong to the player. We call this representation the _extended sequence-form representation_. For an extended sequence-form strategy to be valid, probability conservation constraints must be satisfied at every point of the tree. Specifically, the set of all valid extended sequence-form strategies is given by

\[}:=\{ &[]=1\\ &[]=[j] ,j_{}\\ &[j]=_{a_{j}}[ja]  j\},\]

The distribution of observation outcomes at each observation point, or the transition kernel, is determined by the strategy played by other agents as well as the environment. It can viewed as an opponent who only acts at the observation points. This allows us to encode the transition kernel using a vector \(^{}\) similar to the sequence-form strategy. The entry corresponding to each point \(h\) represents the product of transition probabilities on the path from the root to the point. Formally, the transition kernel space is given by

\[}:=\{ &[]=1\\ &^{}:[]=_{j _{}}[j] \\ &[j]=[ja] j,a _{j}\},\]

We parameterize the vector spaces primarily over the terminals, since the reach of internal points can be uniquely determined by terminal reaches. We define the compressed extensive-form decision space \(:=\{[]}\}\) and the compressed transition kernel \(:=\{[]}\}\), representing the projection of the corresponding spaces onto the vector space generated by terminal observation points. The existing of one-to-one mapping guarantees that \(\) and \(\) are homogeneous to \(}\) and \(}\). For each non-terminal point \(h\), we denote by \([h]\) the value of \(}[h]\) in \(}\), corresponding to the compressed strategy profile \(\). When the agent adopts strategy profile \(\) while the transition kernel aligns with \(\), the reach probability of terminal point \(\) is given by \([][]\). The expected reward of the player can be computed from \(u(;)=,\), where \(:=\) is the reward vector, or the gradient of utility.

To assess the complexity of the game, we use several complexity measures for the extensive-form decision space. We define the _tree size_ and _leaf count_, denoted as \(\|\|_{1}\) and \(\|\|_{}\), as the maximum number of observation points and terminal observation points that can be reached among all pure strategy profiles, respectively. Formally, we write

\[\|\|_{1}:=_{}\|[]\|_{1 }=_{}_{}[], \|\|_{}:=_{}\|\|_ {1}=_{}_{}[ ],\]

where we implicitly extend the domain of \(\) to \(}\) when writing \([]\). We further define \(:=\{0,1\}^{}\) as the vertices in the extensive-form decision space. Each vertex refers to a pure strategy profile of the player, which reduced to a norm-form strategy. The number of reduced normal-form strategy is given by \(||\). We remark that both \(\|\|_{1}\) and \(||\) have been used in the literature (e.g., 20).

SubtreeAs we will often incorporate the recursive structure in TFSDP, we define a _subtree_ as the subgame starting from some internal point \(h\). For two points \(h,h^{}\), we write \(h^{} h\) if \(h^{}\) is reachable from \(h\) in TFSDP. Let \(_{h}=\{h^{}:h^{} h\}\) and \(_{h}=\{: h\}\) be the sets of points and terminals reachable from \(h\). We denote by \(_{h}\) and \(_{h}\) the projected spaces of \(\) and \(\) over \(^{_{h}}\), with restrictions \([h]=1\) and \([h]=1\), respectively. Formally, for any point \(h\), we define the compressed projected decision space as \(_{h}:=\{[_{h}], [h]=1\}\). From the definition of the compressed extensive-form decision space, this space can be seen as a projection of

\[}_{h}:=\{ &[h]=1\\ &^{_{h}}: &[]=[j] ,j_{}\\ &[j]=_{a_{j}}[ja]  j\}.\]

Similarly, we define the compressed projected transition kernel space as \(_{h}:=\{[_{h}], [h]=1\}\). It is important to note that this space exhibits a similar closed form to that of the compressed extensive-form decision space.

Proximal MethodsWe review the standard objects and notations that relate to proximal methods. For a given decision set \(\), the proximal method requires a distance generating function (DGF) \(:\) defined on the decision set. The algorithm is valid when the DGF is \(\)-strongly convex with respect to some norm \(\|\|\). The DGF induces a generalized notion of distance \(_{}:_{ 0}\), referred to as the _Bregman Divergence_, which is defined by

\[_{}(}\|):=(})-()-(),}-.\]

We define the _proximal operator_ with respect to the feasible space \(\) and the DGF \(\). Given a pivot point \(\) and a gradient vector \(^{}\), the proximal operator \(_{}(,)\) generalizes the notion of a gradient ascent step, and is defined as

\[_{}(,):=*{argmax}_{}}\{,}- _{}(}\|)\}.\]

For the extensive-form decision space \(\), the DGF is usually restricted to the dilated DGF  so that the proximal operator can be efficiently computed. Moreover, it is well known that the proximal operator is Lipschitz continuous: [e.g. 33, Lemma 2.1]

**Lemma 3.2**.: For any \(,^{}^{}\), it satisfies that \(\|_{}(,)-_{}(^{}, )\|^{-1}\|-^{}\|_{*}\).

## 4 Primal-Dual Treeplex Norms

We first introduce the treeplex \(_{1}\) norm \(\|\|_{,1}\) and the treeplex \(_{}\) norm \(\|\|_{,}\), which are a primal-dual norm pair defined over the vector space \(^{}\), with respect to a given TFSDP with the point set \(\). As we will show later, these norms enable a better framework for analyzing FOMs in EFGs. Specifically, in the analysis of OMD, we use the fact that the \(_{}\) norm for any feasible reward vector \(\) satisfies \(\|\|_{} 1\). Although treeplex \(_{}\) norm is a relaxation of the \(_{}\) norm, it can still preserve the same guarantee such that \(\|\|_{,} 1\). With the relaxation, we have that the treeplex \(_{1}\) norm generates a smaller distance compared to the \(_{1}\) norm, which allows us to provide a better strong convexity modulus for the regularizer, finally improving the induced regret upper bound.

Both treeplex norms are defined as the support functions with respect to the vector of element-wise absolute values. Specifically, the support function of treeplex \(_{1}\) norm is defined using the transition kernel space \(\), while the support function of treeplex \(_{}\) norm is defined using the extensive-form decision space \(\). Formally, for some vector \(^{}\), we denote

\[\|\|_{,1} :=_{}||, =_{}_{}| []|[],\] \[\|\|_{,} :=_{}||, =_{}_{}| []|[].\]

We remark that the treeplex \(_{}\) norm has been used by Zhang et al.  for analyzing low-degree swap regret minimization. When the EFG degenerates to an NFG (normal-form game), i.e., \(||=1\), the extensive-form decision space \(\) in the TFSDP becomes a simplex \(^{}\), and the transition kernel \(=\{\}\) only contains the all-one vector. It follows that the treeplex \(_{1}\) norm \(\|\|_{,1}\) and the treeplex \(_{}\) norm \(\|\|_{,}\) degenerate to the conventional \(_{1}\) norm and \(_{}\) norm for the vector space, respectively. The following lemma verifies that both treeplex \(_{1}\) norm and treeplex \(_{}\) norm are norms in the technical sense. The missing proofs in this section are provided in Appendix C.

**Lemma 4.1**.: The functions \(\|\|_{,1}\) and \(\|\|_{,}\) are norms defined on the space \(^{}\).

Thanks to the recursive structure of TFSDP, the maximization among \(\) or \(\) in the treeplex norms can be decomposed at each point \(h\). This decomposition allows us to compute both treeplex norms in a recursive manner.

**Lemma 4.2**.: Let \(^{_{h}}\) be a vector with respect to some point \(h\). The treeplex \(_{1}\) norm and the treeplex \(_{}\) norm of vector \(\) over \(_{h}\) can be computed recursively as follows.

* If \(h=\) is a terminal observation point, then: \[\|\|_{,1}:=|[]|,\|\|_{ _{},}:=|[]|.\]* If \(h=j\) is a decision point, then: \[\|\|_{_{j},1}:=_{a_{j}}\|[ _{ja}]\|_{_{ja},1},\|\|_{_{j}, }:=_{a_{j}}\|[_{ja}]\|_{_{ja},}.\]
* If \(h=\) is a non-terminal observation point, then: \[\|\|_{_{j},1}:=_{a_{j}}\|[ _{ja}]\|_{_{ja},1},\|\|_{_{j},}:=_{a_{j}}\|[_{ja}]\|_{_{ja},}.\]

Equipped with the recursive formula, we are able to show that the two treeplex norms with respect to the same TFSDP are a pair of primal-dual norms.

**Theorem 4.3**.: We have \(\|\|_{,1}=1\) for any strategy profile \(\), \(\|\|_{,}=1\) for any transition kernel \(\), and \(\|\|_{,} 1\) for any feasible reward vector \(\) under Assumption 3.1.

## 5 Metric Properties of the DilEnt Regularizer and Improved Regret Bounds

In this section, we study the strong convexity modulus of the weight-one dilated entropy (DilEnt) function with respect to the treeplex norms defined above. The DilEnt regularizer is an instantiation of the more general dilated DGFs framework . Specifically, a dilated DGF for an extensive-form decision space is constructed by taking a weighted sum over suitable _local_ regularizers \(_{j}\) for each \(j\), and is of the form

\[:_{j}_{j} _{j}^{}([p_{j}],\{[ja]\}_{a_{j}}),\]

where

\[_{j}^{}([p_{j}],\{[ja]\}_{a_{j }}):=0&[p_{j}]=0\\ [p_{j}]_{j}[ja]\}_{a_ {j}}}{[p_{j}]}&\]

and \(_{j}>0\) are flexible weight terms that can be chosen to ensure good properties. Note that we have implicitly extended the domain of \(\) to \(}\). Each local function \(_{j}:^{_{j}}\) is required to be continuously differentiable and strongly convex on the relative interior of the local probability simplex \(^{_{j}}\). They show that the proximal steps on the dilated DGF can be efficiently computed, provided that the proximal steps for each individual \(_{j}\) can be efficiently computed.

The DilEnt regularizer \(_{1}:\) is a specific instantiation of the dilated DGF with \(_{j}=1\) and each local regularizer \(d_{j}\) being the negative entropy function. It has been used as a specific instantiation for practical implementations [e.g. 28]. The function has the following closed form.

\[_{1}:_{j}_{a_{j}} [ja][ja]}{[p_{j}]}.\]

Prior to our work, weighted variants of the dilated entropy had been the only variants known to have concrete strong convexity bounds, all with weights that grew with the size of the decision space beneath a given decision point . These results used the standard \(_{1}\) norm as the corresponding norm for showing strong convexity. With the help of our new primal-dual treeplex norms, we can show that the DilEnt regularizer enjoys very strong properties on the extensive-form decision space. We inspect the following \(\)-weighted dilated entropy for \(\):

\[_{}:_{j}_{a _{j}}[ja][ja][ja]}{ [p_{j}]}.\]

By showing that the function is equivalent to the \(\)-weighted negative entropy on the terminal reach, we are able to prove \(_{}\) is \(1\)-strongly convex with respect to the \(\)-weighted \(_{1}\) norm. Since the difference \(_{1}-_{}\) is a summation of convex functions, it can be finally demonstrated that the DilEnt regularizer is \(1\)-strongly convex with respect to the treeplex \(_{1}\) norm.

**Lemma 5.1**.: The weight-one dilated entropy (DilEnt) is \(1\)-strongly convex within the extensive-form decision space \(\) with respect to the treeplex \(_{1}\) norm \(\|\|_{,1}\). Specifically, for any vector \(^{}\) and strategy profile \(\), we have that \(\|\|_{^{2}_{1}()}^{2}\|\|_{ ,1}^{2}\).

The complete proof is provided in Appendix D. Next, we inspect the diameter of the decision space measured by DilEnt. Using an induction statement, we show that \(||_{1}() 0\) holds for any strategy profile \(\). By choosing the initial strategy that minimizes the DilEnt regularizer, we upper bound the diameter of the sequence-form decision space with respect to the DilEnt regularizer.

**Lemma 5.2**.: Let \(_{1}:=*{argmin}_{}_{1}( )\) be the strategy profile minimize the DilEnt regularizer. The Bregman divergence generated by the DilEnt regularizer between \(_{1}\) and any \(_{*}\) can be upper bounded by \(_{_{1}}(_{*},_{1})||\).

By combining the above two lemmas, we have that the DilEnt regularizer achieves \(||/||\). Using this result, we can establish performance guarantees for FOMs with the DilEnt regularizer. We list these results in the following sections.

### Results on Online Mirror Descent

We first inspect online learning in TFSDP with full-information feedback. Consider the use of (Predictive) OMD . The pseudocode of the algorithm can be found in Algorithm 1 in Appendix B. The algorithm starts from \(}_{1}*{argmin}_{ }()\) and follows a straightforward structure in each episode \(t\): Take a proximal gradient step from \(}_{t}\) according to the prediction \(_{t}\) to get the policy \(_{t}\); Execute policy \(_{t}\); Take another proximal gradient step from \(}_{t}\) according to the observed reward vector \(_{t}\) to get \(}_{t+1}\). The algorithm takes some DGF \(\) to execute proximal steps:

\[_{t}_{}(_{t},}_{t}),}_{t+1}_{}(_{t},}_{t}).\]

The value of prediction \(_{t}\) depends on the specific variant used (e.g. \(_{t}_{t-1}\) in Optimistic OMD). For the non-predictive variant, we set \(_{t}\), and thus \(_{t}=}_{t}\). It is known that the algorithm has the following regret bound with respect to a given pair of primal-dual norms.

**Theorem 5.3** (Regret Bound for (Predictive) OMD, Rakhlin and Sridharan , Syrgkanis et al. ).: Let \(\|\|\) and \(\|\|_{*}\) be a pair of primal-dual norm defined on \(^{}\). Let \(\) be a DGF that is \(\)-strongly convex on \(\|\|\). Denote \(_{t}\) as the reward gradient received in episode \(t\). The cumulative regret of running (Predictive) OMD with DGF \(\) and learning rate \(\) can be upper bounded by

\[(T):=_{_{*}}_{t=1}^{T} _{*}-_{t},_{t}_{}(_{*},_{1})+_{t=1}^{T}\| _{t}-_{t}\|_{*}^{2}.\]

Consider using non-predictive OMD with the DilEnt regularizer \(_{1}\). The performance of the algorithm can be analyzed by selecting the treeplex \(_{1}\) norm and the treeplex \(_{}\) norm as the desired pair of primal-dual norms. Using the diameter-to-strong-convexity ratio of DilEnt, we can immediately get a regret upper bound that recovers the state-of-the-art result given by KOMWU .

**Theorem 5.4**.: Let \(\) be a regularizer for extensive-form decision space \(\) which is \(\)-strongly convex on \(\|\|_{,1}\) and has a diameter \(||:=_{x_{*}}_{}(x_{*},x_{1})\). Under Assumption 3.1, the cumulative regret of running OMD with regularizer \(\) and learning rate \(:=|/( T)}\) is upper bounded by

\[(T)|/}.\]

Moreover, if we use the DilEnt regularizer \(_{1}\) in proximal steps, the result can be specified as

\[(T)|}.\]

### Results on Clairvoyant Online Mirror Descent

Consider equilibrium computation in \(n\)-player EFGs. In this scenario, a group of agents aim to jointly learn the coarse correlated equilibrium (CCE) given only oracle access to the game (See Appendix A for detailed definition). We adopt Clairvoyant OMD to compute CCE, introduced by Piliouras et al.

, Farina et al. . The pseudocode of the algorithm can be found in Algorithm 2 in Appendix B. The algorithm can be viewed as a specialized form of predictive OMD, in each episode \(t[\![K]\!]\), an additional routines is introduced to compute the prediction vector \(_{t}\) for each player \(i\) (we omit the superscript). The prediction in the episode is calculated through \(L\) steps of fixed-point iteration starting from \(_{t,1}}_{t}\). In each step \(l[\![L]\!]\), each player \(i\) computes proximal step

\[_{t,l+1}_{}(_{t,l},}_{t}).\]

where we denote by \(_{t,l}\) the reward vector observed by the player joint policy is corresponding to \(_{t,l}\). Clairvoyant OMD finally sets the prediction \(_{t}\) to the iteration result \(_{t,L}\). In this case, the committed policy \(_{t}_{}(_{t},}_{t})\) in the OMD framework is equal to \(_{t,L}\) and the reward vector \(_{t}\) is \(_{t,L+1}\). We show the fixed-point iteration achieves linear convergence. Thus, the difference \(\|_{t,L+1}-_{t,L}\|_{,}\) can be made as arbitrarily small. The proof starts from the following inequality, establishing that the reward vector is Lipschitz continuous with respect to the joint strategy:

\[\|_{1}^{(i)}-_{2}^{(i)}\|_{^{(i)},} _{j=1}^{n}\|_{1}^{(j)}-_{2}^{(j)}\|_{^{(j)}, 1}.\]

We denote by \(_{1}^{(i)}\) the reward vector of player \(i\) when all the players align with joint policy \(\{_{1}^{(j)}\}_{j=1}^{n}\). Together with the fact that the proximal operator is Lipschitz (Lemma 3.2), we can show that the fixed-point iteration achieves a linear convergence rate when the learning rate \(\) is sufficiently small.

**Lemma 5.5**.: Under Assumption 3.1, when running COMD with DilEnt, the reward vector \(_{t,l}^{(i)}\) received by player \(i\) in any \((t,l)[\![K]\!][\![L]\!]\) satisfies \(\|_{t,l+1}^{(i)}-_{t,l}^{(i)}\|_{^{(i)},}  2(n)^{l-1}\).

Therefore, with a logarithmic number of iterations, the discrepancy between the reward vector and the prediction in the OMD framework can be made as small as \(\|_{t}-_{t}\|_{,}=\|_{t,L+1}- _{t,L}\|_{,} 1/K\). Substituting this result into Theorem 5.3 implies the average joint policy given by all \(_{t}\) among \(t[\![K]\!]\) episodes in Clairvoyant OMD only causes a constant regret. Using the standard online-to-batch conversion , we can demonstrate that Clairvoyant OMD finds an \(\)-CCE with only a near-linear number of oracle accesses to the game, establishing the new state of the art.

**Theorem 5.6**.: Under Assumption 3.1, if every player runs Clairvoyant OMD with DilEnt regularizer and learning rate \(=1/(2n)\) for \(K\) episodes. With \(L= K\) steps of inner iterations, the average joint policy \(_{K}\) is an \(\)-CCE for \((n||/K)\). This implies the algorithm converge to a CCE at rate \((n|| T/T)\) where \(T=KL\) is the number of oracle access to the game.

## 6 Lower Bounds for Regret Minimization in EFGs and Optimality of DilEnt

In this section, we show that the DilEnt regularizer has a nearly optimal diameter-to-strong-convexity ratio within the extensive-form decision space. To establish this, we prove a lower bound for online learning in TFSDP with full-information feedback. We show that every algorithm must suffer a regret lower bound that matches our regret upper bound in Theorem 5.4. The optimality of the DilEnt regularizer is demonstrated by contradiction: If there were a regularizer with a much better diameter-to-strong-convexity ratio, then the regret of running OMD with that regularizer would violate the established regret lower bound. We prove the lower bound by constructing a hard instance that is completely random. In this scenario, no online learning algorithm can benefit from historical data, while the cumulative reward of the optimal policy benefits from the anti-concentration properties of the maximum among random distributions.

**Theorem 6.1**.: Given a TFSDP with decision space \(\), there is an EFG satisfying Assumption 3.1 such that: when the other players are controlled by the adversary, any algorithm \(\) incurs an expected regret of at least \((\|_{}|_{0}|})\) for a given of episode number \(T\|\|_{}\), where \(|_{0}|:=_{j}|_{j}|\) is the size of the minimum action set.

We provide missing proofs in Appendix E. Comparing Theorem 6.1 with Theorem 5.4, we establish a lower bound for the diameter-to-strong-convexity ratio, \(||/(\|\|_{}|_{0}|)\) for any regularizer on the extensive-form decision space with respect to our new treeplex norms. Recall the diameter-to-strong-convexity ratio of the DilEnt regularizer is at most \(||/||\), derived from combining Lemma 5.1 and Lemma 5.2. We establish connections between these two quantities using the following lemma, implying that the ratio achieved by the DilEnt regularizer is nearly optimal.

**Lemma 6.2**.: Consider a TFSDP with a given point set \(\). Define \(||:=_{j}|_{j}|\) as the size of the largest action set. If there is no non-root observation point yields exactly one observation outcome, that is, \(|_{}| 2\) for any \(^{+}\), then it follows that \(||(\|\|_{}||)\). Without this structural condition, we have \(||(\|\|_{}| |)\) in general.

According to Lemma 6.2, if every each action set has the same number of actions \(|_{0}|=||\), and no non-root observation point yields only one outcome, we have \(||/||(\|\|_{} |_{0}|)\), implying the DilEnt regularizer achieves the optimal diameter-to-strong-convexity ratio up to constant factors in this scenario. If the action sets vary in size, it creates a gap logarithmic to the size of the maximal action set. If there is some observation point that yields only one observation, the gap inflates with another factor of logarithmic to the number of decision points. All in all, the DilEnt regularizer achieves the optimal diameter-to-strong-convexity ratio up to only logarithmic factors.

## 7 Conclusion, Limitations, and Open Questions

In this paper, we introduce a new primal-dual norm pair for studying the strong convexity properties of distance-generating functions for sequence-form strategy polytopes arising in extensive-form games. Quantifying these properties is a key component in the construction of efficient first-order optimization methods for equilibrium computation. Our techniques enable us to explain the strong theoretical performance of the DilEnt regularizer, for which no meaningful strong convexity bounds were previously known. In fact, we find that among all convex regularizers for extensive-form games, DilEnt is optimal up to logarithmic factors. To establish this result, we introduced a new regret lower bound for learning in extensive-form games, which is likely of independent relevance.

We remark that our lower bound only applies to extensive-form games with full-information feedback, a setting common in self-playing algorithms. Thus, the optimality of the DilEnt regularizer may not extend to scenarios with stochastic feedback. It would be interesting to study tight lower bounds for learning under other types of feedback. While Fiegel et al.  gave matching lower and upper bounds under trajectory bandit feedback, results for external sampling remain open to our knowledge.

Furthermore, we can only prove tight upper and lower bounds up to constant factors for the diameter-to-strong-convexity ratio in a specific family of TFSDPs. There still remains a logarithmic gap related to the total number of sequences in general. In principle, it is possible that this gap could be further reduced, yielding a different regularizer that offers logarithmic advantages over the DilEnt regularizer. Overcoming this technical hurdle and showing that the DilEnt regularizer indeed achieves the optimal rate remains an interesting direction of research. Notably, Fiegel et al.  were also only able to prove lower bounds in specific games under bandit feedback, alluding to the intrinsic hardness of proving lower bounds without slight restrictions to the game class.