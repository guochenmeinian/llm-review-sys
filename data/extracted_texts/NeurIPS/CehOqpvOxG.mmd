# Fair Kernel K-Means: from Single Kernel to Multiple Kernel

Peng Zhou

School of Computer Science and Technology

Anhui University

Hefei, 230601

zhoupeng@ahu.edu.cn

&Rongwen Li

School of Computer Science and Technology

Anhui University

Hefei, 230601

e22301284@stu.ahu.edu.cn

&Liang Du

School of Computer and Information Technology

Shanxi University

Taiyuan, 237016

duliang@sxu.edu.cn

Peng Zhou is the corresponding author. Peng Zhou and Rongwen Li are also with Anhui Provincial International Joint Research Center for Advanced Technology in Medical Imaging.

###### Abstract

Kernel k-means has been widely studied in machine learning. However, existing kernel k-means methods often ignore the _fairness_ issue, which may cause discrimination. To address this issue, in this paper, we propose a novel Fair Kernel K-Means (FKKM) framework. In this framework, we first propose a new fairness regularization term that can lead to a fair partition of data. The carefully designed fairness regularization term has a similar form to the kernel k-means which can be seamlessly integrated into the kernel k-means framework. Then, we extend this method to the multiple kernel setting, leading to a Fair Multiple Kernel K-Means (FMKKM) method. We also provide some theoretical analysis of the generalization error bound, and based on this bound we give a strategy to set the hyper-parameter, which makes the proposed methods easy to use. At last, we conduct extensive experiments on both the single kernel and multiple kernel settings to compare the proposed methods with state-of-the-art methods to demonstrate their effectiveness. Our code is available at https://github.com/rongwenli/NeurIPS24-FMKKM.

## 1 Introduction

Clustering is a fundamental unsupervised machine learning task. In clustering, kernel methods, such as Kernel K-Means (KKM), can effectively separate nonlinear data into different clusters. Therefore, KKM has been widely studied in both the single kernel setting and multiple kernel setting .

Notice that, in real-world applications, clustering is often used in some scenarios involving humans such as social networks  and crime analysis . In these scenarios, since the humans are involved, we should guarantee the _fairness_ of the clustering result, so that the clustering result will not cause discrimination to some specific groups. In the clustering task, we often consider the _group fairness_, where we have some pre-given groups that may suffer from the potential discrimination, called _protected groups_. Group fairness aims to partition data into some clusters and guarantee that no clusters contain a disproportionately small or large number of data in some specific protectedgroups . Although the above-mentioned kernel k-means and multiple kernel k-means methods show promising performance in the clustering task, none of them considers the fairness issue, and thus they may obtain some clustering results which cause discrimination to some groups.

To tackle this problem, in this paper, we propose a novel fair kernel k-means method and extend it from the single kernel setting to the multiple kernel setting. We follow a widely-used definition of fairness defined in , which is shown as Definition 1. By analyzing this definition, we carefully design a new fairness regularization term and prove that minimizing this term can lead to the optimal fairness defined in . Besides, we observe that our fairness regularization term has a similar form of the loss function of KKM, and thus can be naturally and seamlessly plugged into the KKM framework, yielding an extremely simple and elegant Fair Kernel K-Means (FKKM) framework. This framework is so concise that we do not even need to modify the loss of KKM but just adjust the input kernel to our proposed _fair kernel_. This framework can also be easily extended to the Multiple Kernel K-Means (MKKM) task, leading to Fair Multiple Kernel K-Means (FMKKM). We also provide some theoretical analysis of its generalization error bound. Furthermore, based on the generalization error bound, we provide a strategy to set the hyper-parameter in our framework, which makes the method easy to use. Extensive experiments on single kernel clustering and multiple kernel clustering tasks show the effectiveness of our framework w.r.t. both the clustering accuracy and fairness.

The main contributions of our paper are summarized as follows:

* We propose a novel fairness regularization term and prove that minimizing this term can reach the optimal fairness defined in .
* Our proposed regularization term has a similar form to the KKM, and thus can be seamlessly integrated into the KKM and MKKM framework. To the best of our knowledge, this is the first work for fair kernel k-means and fair multiple kernel k-means.
* We provide a strategy to set the hyper-parameter based on the theoretical analysis, which makes the methods easy to use.
* Extensive experiments in both single and multiple kernel clustering show the effectiveness and superiority of our proposed methods compared with the state-of-the-art methods.

## 2 Related Work and Preliminaries

In this paper, we use a bold uppercase letter (e.g. \(\)) and a bold lowercase letter (e.g. \(\)) to denote a matrix and a vector, respectively. Given a matrix \(\), we use \(M_{ij}\) to denote its \((i,j)\)-th element.

### Kernel K-means and Multiple Kernel K-means

Given a data matrix \(=[_{1},,_{n}]^{d n}\) with \(n\) instances and \(d\) features, let \(():^{d}\) represents a kernel mapping that maps \(\) into a Reproducing Kernel Hilbert Space (RKHS) \(\). The objective function of the kernel k-means with the sum-of-squares loss can be written as [39; 24]:

\[_{, Ind}\|()-^{T }\|_{}^{2},\] (1)

where \(()=[(_{1}),,(_{n})]\) and \(=[_{1},,_{c}]\) represents \(c\) clustering centroids in the RKHS \(\). \(\{0,1\}^{n c}\) is an indicator matrix, which is denoted as \(Ind\), and \(Y_{ij}=1\) if \(_{i}\) is assigned to the \(j\)-th cluster, and otherwise \(Y_{ij}=0\). Setting the derivative of Eq.(1) w.r.t. \(\) to zero, we can obtain the closed-form solution of \(\). Taking it back to Eq.(1), it can be rewritten as :

\[_{ Ind}()-((^{T})^{-}^{T} (^{T})^{-}),\] (2)

where \(=()^{T}()^{n n}\) is a kernel matrix with \(K_{ij}=(_{i})^{T}(_{j})\). For the convenience of optimization, we denote \(=(^{T})^{-}\). Since directly solving Eq.(2) is an NP-hard problem , previous works [26; 41; 21] substituted the constraints \( Ind\) with \(^{T}=\), leading to:

\[_{^{T}=}( (-^{T})).\] (3)

The optimal \(\) is formed by the \(c\) eigenvectors of \(\) corresponding to the \(c\) largest eigenvalues. After obtaining \(\), existing methods [54; 44; 35; 17] learn the final clustering results through some post-processing techniques such as k-means or spectral rotation on \(\).

Multiple kernel k-means aims to fuse multiple base kernels to a consensus one for kernel k-means. Previous works assume that the ideal consensus kernel matrix is a combination of base kernel matrices i.e., \(^{*}=_{p=1}^{m}_{p}^{2}^{(p)}\), where \(^{*}\) is the consensus kernel matrix, and \(^{(p)}\)s are base kernels [27; 28; 19]. \(_{p}\) is the weight of the \(p\)-th base kernel. Replacing \(\) in Eq.(3) with the consensus kernel \(^{*}\), we can obtain the objective function of MKKM:

\[_{,}(^{*}( -^{T})),\ \ s.t.\ ^{T}=,\ ^{T}=1,\ _{p} 0,\ ^{*}= _{p=1}^{m}_{p}^{2}^{(p)}.\] (4)

It can be solved by alternatively optimizing \(\) and \(\).

### Fair Clustering

Fair clustering considers the fairness in the clustering, which is an important problem in unsupervised machine learning. It was first introduced by Chierichetti et al., who proposed a fair decomposition method to avoid all members of a protected group being clustered into the same cluster . However, this method can only handle two protected groups. To tackle this problem, Bera et al. further proposed a concept of fairness applicable to multiple protected groups in , which is defined as:

**Definition 1**: _(Fairness)  Given a data matrix \(^{d n}\) with \(n\) instances and \(d\) features, it is partitioned into \(c\) disjoint clusters \(=\{_{1},,_{c}\}\). Given \(t\) disjoint protected groups \(_{1},_{2},,_{t}\), let \(_{i}=_{i}|}{n}\) and \(_{i}(k)=_{i}|}{|_{k}|}\) denote the proportion of group \(_{i}\) in the whole data and cluster \(_{k}\), respectively. The fairnesss of a cluster \(_{k}\) is defined as:_

\[fairness(_{k})=(}{_{i}(k)},(k)}{_{i}}),\  i\{1, t\}\] (5)

_The fairness of the whole clustering result \(\) is defined as:_

\[fairness()=_{k\{1, c\}}fairness(_{k})\] (6)

**Remark 1**: \(fairness()\)_, and the larger \(fairness()\) is, the fairer the clustering result is. A fair clustering result requires that the proportion of \(_{i}\) in each cluster, which is denoted as \(_{i}(k)\), should be close to the proportion of \(_{i}\) in the whole data, which is denoted as \(_{i}\). When all \(_{i}(k)=_{i}\), the \(fairness\) will achieve its maximum value 1, which means it is perfectly fair._

Based on Definition 1, many fair clustering methods have been proposed [4; 7; 1; 37]. For example, Ziko et al. proposed a variational fair clustering framework by integrating fairness term with a clustering objective ; Kleindessner et al. embedded fairness as a linear constraint into spectral clustering obtaining fair spectral clustering ; Ghadiri et al. introduced a fair k-means method that ensures all protected groups have equal cluster costs ; Li et al. proposed a deep fair clustering method . Wang et al. embedded this fairness into deep clustering by learning a differentiated and fair clustering allocation function ; Chhabra et al. provided a robust deep fair clustering method by considering the fairness attack .

## 3 Methodology

### Fairness Regularization Term

We first introduce our fairness regularization term. To control the fairness, according to Definition 1, we need to compute \(|_{k}_{i}|\) and \(|_{k}|\) in \(_{i}(k)\). To this end, we introduce two indicator matrices \(\{0,1\}^{n t}\) and \(\{0,1\}^{n c}\). \(\) is a protected group indicator matrix, where \(G_{ij}=1\) if the \(i\)-th instance belongs to the \(j\)-th protected group, and \(G_{ij}=0\) otherwise. \(\) is a cluster indicator matrix, where \(Y_{ij}=1\) if the \(i\)-th instance belongs to the \(j\)-th cluster, and \(Y_{ij}=0\) otherwise. It is easy to verify that

\[^{T}=|_{1}_{1}|&|_{2} _{1}|&&|_{c}_{1}|\\ |_{1}_{2}|&|_{2}_{2}|&&|_{c} _{2}|\\ &&&\\ |_{1}_{t}|&|_{2}_{t}|&&|_{c} _{t}|,\ and\ ^{T}=|_{1}|&0&&0\\ 0&|_{2}|&&0\\ &&&\\ 0&0&&|_{c}|\] (7)Notice that \(\) is a constant matrix because the protected groups are often pre-given, while \(\) is a variable that needs to learn for clustering. Based on Eq.(7), we define a fair regularization term \((^{T}^{T}(^{T})^{-1})\) and provide the following Theorem, which shows that minimizing this regularization term leads to the maximum of the fairness defined in Definition 1.

**Theorem 1**: _Given \(\) and \(\) defined as mentioned before, we can obtain the maximum of fairness by optimizing the following objective function:_

\[_{ Ind}(^{T}^{ T}(^{T})^{-1}).\] (8)

**Proof 1**: _We first have:_

\[(^{T}^{T}( ^{T})^{-1})=(( ^{T})^{-}^{T}^{T} (^{T})^{-})=\| ^{T}(^{T})^{-} \|_{F}^{2}.\]

_According to Eq.(7), we have_

\[^{T}(^{T})^{-}= _{1}|}{|}}&_{1}|}{|}}&& _{1}|}{|}}\\ _{1}|}{|}}& _{2}|}{|}}&&_{2}|}{ |}}\\ &&&\\ _{1}|}{|}}& _{1}|}{|}}&&_{t}|}{ |}}.\] (9)

_Therefore, minimizing Eq.(8) is equivalent to minimizing the following formula:_

\[\|^{T}(^{T})^{-}\|_{F}^{2}=_{i=1}^{t}_{k=1}^{c}_{i}|^{2}}{|_{k}|}.\] (10)

_According to Cauchy-Schwarz Inequality, we have:_

\[(_{k=1}^{c}_{i}|^{2}}{|_{k}|}) (_{k=1}^{c}|_{k}|)(_{k=1}^{c}|_{k} _{i}|)^{2}=|_{i}|^{2}_{k=1}^{c} _{i}|^{2}}{|_{k}|}_{i}|^ {2}}{n}.\] (11)

_Summing Eq.(11) w.r.t. \(i\), we have_

\[_{i=1}^{t}_{k=1}^{c}_{i}|^{2}}{|_{k}|} _{i=1}^{t}_{i}|^{2}}{n}.\] (12)

_The equation in Eq.(12) holds if and only if \(_{i}|}{|_{1}|}=_{i }|}{|_{2}|}==_{i}|}{|_{c}|}\) for any \(i\). It is easy to verify that \(_{i}|}{|_{1}|}== _{i}|}{|_{c}|}=_{i}|}{ |}}}{_{k=1}^{|_{k}|}}\). Notice that \(_{k}\) is a disjoint partition of all data, and thus we have \((_{1}_{i})(_{c}_{i})=_{i}\) and \((_{p}_{i})(_{q}_{i})=\) for any \(p,q\). Therefore, we have \(_{k}|_{k}_{i}|=|_{i}|\). Similarly, we have \(_{k}|_{k}|=n\). Taking them back to the condition holding, we have that the equation holds if and only if \(_{i}|}{|_{1}|}=_{i}|} {|_{2}|}==_{i}|}{|_{c}|}=_{i}|}{n}\)._

_Notice that \(_{i}|}{|_{k}|}=_{i}(k)\) and \(_{i}|}{n}=_{i}\). Therefore, when we minimize Eq.(8), we have \(_{i}(k)=_{i}\). According to Definition 1, it will lead to maximum fairness. This concludes the proof._

According to Theorem 1, we provide a simple yet effective fair regularization term Eq.(8), and can easily plug it into the KKM and MKKM framework.

### Fair Kernel K-means

Notice that the fairness regularization term \((^{T}^{T}(^{T})^{-1})\) has a similar form to KKM (i.e., Eq.(2)). Therefore, we can seamlessly integrate this term into the KKM framework, leading to a fair kernel k-means (FKKM):

\[_{ Ind}()-((^{T})^{-}^{T} (^{T})^{-} )+\,(^{T}^{T} (^{T})^{-1})\] \[ _{ Ind}(^{T}(-^{T})(^{T} )^{-1}),\] (13)where \(\) is a hyper-parameter to balance the trade-off between the clustering performance and the fairness. Larger \(\) will lead to a fairer clustering result. Of course, \(\) should not be too large, or it will dominate the loss function and the kernel k-means may not work. Comparing Eq.(13) with Eq.(2), we observe that if \(\) is small enough to make \(-^{T}\) positive semi-definite (p.s.d.), we can regard \(-^{T}\) as a new kernel matrix and Eq.(13) becomes a standard kernel k-means. In this case, we call \(-^{T}\) a _fair kernel_.

However, in practice, to make \(-^{T}\) be a valid kernel matrix, which means to make \(-^{T}\) p.s.d., we should set a very small \(\), which cannot guarantee the fairness. To address this issue, we find that we can add a large enough constant term \(()\) to Eq.(13), to obtain a valid fair kernel matrix. In more detail, we have:

\[(^{T}(- ^{T})(^{T})^{-1} )+()=(^{T}(+-^{T} )(^{T})^{-1}).\] (14)

It shows that optimizing Eq.(14) is always exactly equivalent to optimizing Eq.(13), no matter how we set \(\). With a large enough \(\), we can easily set an appropriate \(\) to make \(}=+-^{T}\) be p.s.d., and thus be a valid kernel matrix. We will discuss how to set \(\) and \(\) later.

In this way, we obtain an extremely simple yet elegant FKKM method. In this method, we do not even need to modify the loss of standard KKM. All we need is to modify the kernel by replacing \(\) to a fair kernel \(}=+-^{T}\). It means that we realize the fairness on the data level rather than the model level.

### Fair Multiple Kernel K-means

Eq.(14) can be naturally extended to a multiple kernel setting. Given a base kernel \(^{(p)}\), we first construct its fair kernel \(}^{(p)}=^{(p)}+- ^{T}\). Then similar to Eq.(4), we define the fair consensus kernel \(}^{*}=_{p=1}^{m}_{p}^{2}}^{(p)}\) and take it into Eq.(2) to obtain FMKKM:

\[_{,}( }^{*}(-(^{T})^{-1} ^{T}))\ \ s.t.\  Ind,\ ^{T}=1,\ _{p} 0, }^{*}=_{p=1}^{m}_{p}^{2}}^{(p)}.\] (15)

Notice that since our fairness regularization term \((^{T}^{T} (^{T})^{-1})\) requires that \(\) should be a discrete indicator matrix, our FKKM (i.e., Eq.(14)) and FMKKM (i.e., Eq.(15)) directly solve the discrete \(\) instead of the conventional two-step methods which learn an orthogonal embedding \(\) first and then obtain the discrete clustering result. As we know, in the two-step methods, the kernel k-means and the discretization post-processing are separated and when doing the discretization it cannot guarantee the clustering accuracy or fairness. Different from the two-step methods, we can directly learn the final clustering result \(\) by fully considering the clustering accuracy and fairness.

### Optimization

#### 3.4.1 Optimization of FKKM

When minimizing Eq.(14), we only need to solve one variable \(\). Notice that there is only one 1 in each row of \(\). Therefore, we can solve \(\) row by row. When solving the \(i\)-th row, we replace the \(i\)-th row with \([1,0,,0]\), \([0,1,0,,0]\),..., \([0,,0,1]\) respectively, and compute the values of the corresponding objective function to find the one which leads to the maximum. Then we set the \(i\)-th row as this row vector. Wang et al. propose an efficient method to compute these objective functions by reducing the computation redundancy .

#### 3.4.2 Optimization of FMKKM

In Eq.(15), there are two groups of variables, i.e., \(\) and \(\). We solve them by a block coordinate descent method, which optimizes one variable when fixing the other.

When fixing \(\) to solve \(\), we have the following subproblem w.r.t \(\):

\[_{ Ind}(^{T}} ^{*}(^{T})^{-1}),\] (16)

where \(}^{*}=_{p=1}^{m}_{p}^{2}}^{(p)}\). It is the same as the optimization of FKKM.

When fixing \(\) to solve \(\), we have following subproblem w.r.t \(\):

\[_{}_{p=1}^{m}_{p}^{2}h_{p},\ \ s.t._{p=1}^{m} _{p}=1,\ _{p} 0,\] (17)

where \(h_{p}=(}^{(p)}(- (^{T})^{-1}^{T}))\). According to Cauchy-Schwarz Inequality, the closed-form solution of \(_{p}\) is:

\[_{p}=^{-1}}{_{j=1}^{m}h_{j}^{-1}}.\] (18)

Appendix A shows the pseudo-codes of FKKM and FMKKM, respectively. When updating each row of \(\), the objective function of FKKM decreases and has a lower bound. Therefore, FKKM can always converge. Similarly, the convergence of FMKKM can also be guaranteed. Now, we analyze the time complexity. According to , optimizing the \(i\)-th row of \(\) has a time complexity of \(O(nc)\). FKKM has a time complexity of \(O(n^{2}c)\). Calculating \(\) has a time complexity of \(O(n)\). Therefore, FMKKM also has a time complexity of \(O(n^{2}c)\). According to , although the time complexity is square in the number of instances, it can be computed very efficiently in practice. Therefore, the time complexity of our method is comparable with the mainstream KKM and MKKM methods.

## 4 Theoretical Analysis

The generalization error bound of the k-means evaluates the expectation of distance between an unseen data and the clustering center it belongs to [30; 22; 21]. Since FKKM is a special case of FMKKM when \(m=1\), in this section, we derive the generalization error bound of our FMKKM. Before the derivation, we need the following two mild assumptions:

**Assumption 1**: _Each \(}^{(p)}=^{(p)}+- ^{T}\) is a valid kernel matrix, i.e., \(}^{(p)}\) is symmetric and p.s.d._

**Remark 2**: _This assumption is easy to satisfy. If \(}^{(p)}\) is not p.s.d., we can enlarge \(\) to make the assumption hold._

**Assumption 2**: _All \(^{(p)}\) are upper bounded. We denote \(b\) as the maximum of elements in all \(^{(p)}\)._

According to assumption 1, since all \(}^{(p)}\) are valid kernel matrices, \(}^{*}\) is also a valid kernel matrix. We define the corresponding kernel function of \(}^{*}\) as \(}^{*}(,)\), and its kernel mapping function is \(_{}(_{i})=[_{1}_{1}(_{i})^{T}, ,_{m}_{m}(_{i})^{T}]^{T}:^{d} \), where \(_{1}(_{i}),,_{m}(_{i})\) are the induced kernel mapping function of \(}^{(1)},,}^{(m)}\), respectively. Let \(=[_{1},,_{c}]\) denote the learned centroids matrix in the RKHS \(\), where \(_{i}\) is the center of the \(i\)-th cluster in \(\). FMKKM aims to minimize the error: \([_{\{_{1},,_{c}\}} \|_{}()-\|_{}^{2}],\) where \([_{1},,_{c}]\) are the standard orthonormal basis of \(^{c}\) space, i.e., \(_{i}\) is an all-zero vectors except that the \(i\)-th element is 1.

Then, we define a function class as our hypothesis space:

\[=\{f:_{\{_{1}, ,_{c}\}}\|_{}()- \|_{}^{2}\|^{T} =1,_{p} 0,_{k}\}.\] (19)

Similar to , we have the following Theorem to provide the generalization error bound:

**Theorem 2**: _Under Assumptions 1 and 2, given training data \(=[_{1},,_{n}]\), function class \(\) defined in Eq.(19), and any \( 0\), with probability at least \(1-\), the following inequality holds for all \(f\):_

\[[f()] _{i=1}^{n}f(_{i})+}{}[(1+c^{2})(b+)-(1+}{t})+c)}]\] \[+(4(b+)-2(1+) )},\] (20)

_where \(t\) and \(c\) are the number of protected groups and clusters, respectively._

**Proof 2**: _See Appendix B._

The first term in Eq.(20) is the empirical error. Notice that, we have \(_{i=1}^{n}f(_{i})=(}^{ *}(-(^{T})^{-1} ^{T}))\), which means our loss function is to minimize exactly this empirical error. However, in the two-step methods, which apply \(^{T}=\) where \(=(^{T})^{-}\) to replace \( Ind\), they only optimize a continual approximation of the empirical error.

Besides, the second and third terms represent the gap between the generalization and empirical errors. Intuitively, the gap is the smaller the better. To decrease the gap, we wish \(\) to be as small as possible. However, Assumption 1 prevents \(\) being too small because \(}^{(p)}\) should be p.s.d., or Theorem 2 will not hold anymore. Now we can derive the lower bound of \(\) according to Assumption 1. Suppose \(_{min}\) as the smallest eigenvalue of \(^{(1)},,^{(p)}\). Then, the smallest eigenvalue of \(^{(p)}+\) should be no smaller than \(_{min}+\). Notice that we have the following Lemma:

**Lemma 1**: _Given two real symmetric matrices \(\) and \(\) with the same size, where the smallest eigenvalue of \(\) is \(_{A}\) and the largest eigenvalue of \(\) is \(_{B}\). If \(_{A}_{B}\), then \(-\) is p.s.d._

**Proof 3**: _See Appendix C._

Denoting \(_{max}\) as the largest eigenvalue of \(^{T}\), it is easy to verify that \(_{max}=|_{max}|\), where \(_{max}\) is the protected group with the largest number of instances. According to Lemma 1, we have that if \(_{min}+-*|_{max}| 0\), \(}^{(p)}\) will be p.s.d. Therefore, \(\) has a lower bound \(*|_{max}|-_{min}\). In practice, \(_{min}\) is often very small and close to 0. To avoid the time consuming to compute the eigenvalues of the kernels, we can approximately set \(=*|_{max}|\).

Take \(=*|_{max}|\) back into the generalization error bound Eq.(20). We consider the gap between the generalization and empirical errors, i.e., the second and third terms:

\[ }{}[(1+c^{2})b+( |_{max}|-1+(|_{max}|t-1)}{t})+c _{max}|-1))(b+_{max}|t-1 }{t})}]\] \[+(4b+4(|_{max}|-1))}\] (21)

Notice that \(|_{max}|-1 0\) and \(|_{max}|t-1 0\), and thus we have that the gap decreases with \(\) decreases. It means that smaller \(\) leads to a lower gap. Therefore, \(\) is a trade-off between the clustering performance and fairness. Increasing \(\) may enlarge the error bound, but obtain a fairer result. Based on this theoretical analysis, we provide a strategy to set \(\) by observing a fairness metric, which can be computed without the ground truth. In more detail, we gradually enlarge \(\) from 0, set \(=*|_{max}|\), and observe the fairness metric. If it gets stable good fairness, we stop enlarging \(\) and set \(\) as the current value. This strategy does not need the ground truth, which is appropriate for unsupervised learning, and can obtain an as small as possible \(\) to achieve a good fairness result.

## 5 Experiments

### Data Sets and Experimental Setup

We conduct experiments on benchmark data sets which are widely used in fair clustering, including D&S , HAR , Jaffe , MNIST-USPS , Credit Card  and K1b . D&S is a human daily and sports activities data set including 8 participants. HAR is a human action recognition data set including 30 participants. In both D&S and HAR data sets, the data of each participant form a protected group. Jaffe is a face image data set. Following , the face images with the same expressions are put into a protected group. MNIST-USPS is an image data set containing images of handwritten digits from the subsets of MNIST and USPS data sets. Following , we randomly sample 2000 images from MNIST to form one protected group and randomly sample 1800 images from USPS to form the other protected group. Credit card is a data set that describes the customers' default payments and the data of males and females form two protected groups respectively. K1b is atext data set. Following , we randomly assign each text to a protected group with a Bernoulli distribution whose \(p=0.5\) to form two protected groups. The statistical information of these data sets is shown in Appendix D.

In the single kernel setting, we compare our FKKM with K-means , Kernel K-means (KKM) , Spectral Clustering (SC) , and three state-of-the-art fair clustering methods, including SpFC , VFC , and FFC . For the kernel methods (i.e., our FKKM and KKM), we use a Gaussian kernel with a bandwidth parameter fixing to \(*D\), where \(D\) is the average distance between samples. In the multiple kernel setting, we compare our FMKKM with 9 state-of-the-art MKKM methods, including ONKC , MKCSS , DPMKKM , LFLKA , EMKC , OSLR , ASLR , CSAMKC , FAMKKM . Detailed information of these compared methods is shown in Appendix E. Besides, for an ablation study, we also compare with the degeneration version of our method, which is without the fairness regularization term, denoted as FKKM-f (for single kernel version) and FMKKM-f (for multiple kernel version).

In the multiple kernel setting, following , we construct 12 kernels, including seven Gaussian kernels \((_{i},_{j})=(-\| _{i}-_{j}\|_{2}^{2}/2^{2})\) with \(=*D\), where \(s\) varies in the range of \(\{,,,1,2,4,8\}\) and \(D\) is the average distance between samples; four polynomial kernels \((_{i},_{j})=(a+_{i}^ {T}_{j})^{b}\) with \(a=\{0,1\}\) and \(b=\{2,4\}\); and a cosine kernel \((_{i},_{j})=(_{i}^{T} _{j})/(\|_{i}\|\|_{j}\|)\). Finally, all kernels have been normalized through \((_{i},_{j})/( _{i},_{i})(_{j},_ {j})}\) and then rescaled to \([0,1]\). We use Accuracy (ACC) and Normalized Mutual Information (NMI) to evaluate the clustering performance. Besides, we also use balance (Bal)  and Minimal Normalized Conditional Entropy (MNCE)  to evaluate fairness. Specifically, Bal is defined as

\[()=_{k}(^{}}{N_{k}^ {max}})[0,1],\] (22)

where \(N_{k}^{min}\) and \(N_{k}^{max}\) represent the number of instances in the smallest and the largest (in size) protected groups in cluster \(_{k}\), respectively. MNCE is defined as

\[=(-_{i}_{i}_{k}|} {|_{k}|}_{i}_{k}|}{|_{k}|})}{-_{i} _{i}|}{n}_{i}|}{n}}[0,1].\] (23)

All metrics are the larger the better. Based on previous analysis of hyper-parameter setting, we search \(\) as \(=1,2,,\) by observing the corresponding MNCE. When the MNCE gets stable, i.e., the change of MNCE is smaller than 0.005, we stop the searching and use the current \(\). For

   Data sets & K-means & KKM & SC & FairSC & VFC & FFC & FKKM-f & FKKM \\   & ACC & 0.555 & 0.552 & 0.558 & 0.433 & 0.539 & 0.521 & **0.648** & 0.636 \\  & NMI & 0.650 & 0.602 & 0.652 & 0.575 & 0.617 & 0.583 & **0.724** & 0.683 \\  & Bal & 0 & 0 & 0 & 0 & 0.186 & 0.100 & 0 & **0.559** \\  & MNCE & 0.156 & 0.531 & 0.023 & 0.023 & 0.923 & 0.712 & 0.477 & **0.991** \\   & ACC & 0.524 & 0.620 & 0.680 & 0.742 & 0.600 & 0.602 & 0.689 & **0.771** \\  & NMI & 0.596 & 0.609 & 0.618 & 0.703 & 0.654 & 0.490 & 0.625 & **0.710** \\  & Bal & 0 & 0 & 0 & 0 & 0.200 & 0.007 & 0 & **0.250** \\  & MNCE & 0.933 & 0.930 & 0.914 & 0 & 0.983 & 0.953 & 0.920 & **0.959** \\   & ACC & 0.363 & 0.396 & 0.406 & **0.458** & 0.360 & 0.437 & 0.403 & 0.432 \\  & NMI & 0.423 & 0.421 & **0.435** & 0.429 & 0.306 & 0.412 & 0.426 & 0.380 \\  & Bal & 0 & 0 & 0 & 0 & 0.142 & 0.217 & 0 & **0.847** \\  & MNCE & 0 & 0.003 & 0 & 0 & 0.544 & 0.684 & 0 & **0.997** \\   & ACC & 0.927 & 0.948 & 0.901 & 0.957 & 0.981 & 0.901 & 0.954 & **1** \\  & NMI & 0.914 & 0.922 & 0.889 & 0.943 & 0.969 & 0.918 & 0.930 & **1** \\  & Bal & 0 & 0 & 0 & 0 & 0.400 & 0.250 & 0 & **0.500** \\  & MNCE & 0.808 & 0.900 & 0.765 & 0.827 & 0.983 & 0.924 & 0.897 & **0.989** \\   & ACC & 0.362 & 0.381 & 0.311 & 0.351 & 0.381 & 0.364 & 0.400 & **0.404** \\  & NMI & 0.139 & 0.140 & 0.126 & 0.123 & 0.142 & 0.139 & 0.145 & **0.148** \\  & Bal & 0.510 & 0.550 & 0.567 & 0.603 & 0.586 & 0.550 & 0.536 & **0.624** \\  & MNCE & 0.953 & 0.961 & 0.967 & 0.973 & 0.970 & 0.969 & 0.956 & **0.985** \\   & ACC & 0.742 & 0.669 & 0.667 & **0.853** & 0.778 & 0.663 & 0.826 & 0.809 \\  & NMI & 0.589 & 0.537 & 0.536 & 0.666 & 0.553 & 0.503 & 0.628 & 0.591 \\   & Bal & 0.666 & 0.775 & 0.763 & 0.667 & 0.794 & 0.773 & 0.703 & **0.800** \\   & MNCE & 0.971 & 0.989 & 0.987 & 0.971 & 0.990 & 0.989 & 0.978 & **0.991** \\   

Table 1: Comparison results on the single kernel setting. The best and second best results are denoted in **bold** and underlined, respectively.

other comparison methods, we follow their recommended parameter configurations and search methodologies. All experiments are conducted on the 12th Gen Interl(R) Core(TM) i7-12700 with 32 GB RAM. All experiments are repeated 10 times and the average results are reported.

### Experimental Results

Table 1 shows the comparison results in the single kernel setting, where the best and second best results are denoted in bold and underlined, respectively. It can be seen that FKKM exhibits better fairness compared to K-means, KKM, SC, our ablation version (i.e., FKKM-f), and even the fair clustering methods, indicating the effectiveness of our fairness regularization term. When comparing w.r.t. clustering performance (i.e., ACC and NMI), FKKM still often achieves the best or the second-best results.

Table 2 presents the comparison results in the multiple kernel setting. FMKKM easily achieves the best fairness, due to the effectiveness of our fairness regularization term. Moreover, FMKKM often achieves better or at least comparable ACC and NMI. Notice that our method just simply modifies the original MKKM and can achieve competitive clustering performance, demonstrating that our method is simple yet effective.

Figure 1 shows the visualization results. It shows the number of instances of each protected group \(_{j}\) in each cluster \(_{i}\) in the D&S data set obtained by FMKKM-f and FMKKM, respectively. As shown in Figure 1(a), in FMKKM-f, without the fairness regularization term, the numbers of data of each protected group in each cluster have a great difference, which means the result is unfair. Figure 1 (b) shows that the distribution of protected group in each cluster is more balanced, which means the result obtained by FMKKM is much fairer than FMKKM-f. It demonstrates the effectiveness of our fair regularization term.

### Efficiency Results

The convergence curves of our methods are shown in Appendix F. The results show that our methods often converge very fast. We also conduct experiments to compare the running time of our methods with other compared methods. Our method is faster than or at least comparable with other methods on many data sets. The detailed results are shown in Appendix F.

### Parameter Study

Figure 2 shows the effects of \(\) of FKKM and FMKKM on MNIST-USPS and Credit Cards data sets. The other results are similar. The red points denote the \(\) selected by our strategy. We can see that

   Data sets &  & MCSS & DPMRKM & LFL+A & EMRC & OSLR & ASLR & CSMKC & FAMKKM & FMKKM-f & FMKKM \\   & ACC & 0.526 & 0.646 & 0.692 & 0.695 & 0.732 & 0.717 & 0.574 & 0.668 & 0.705 & 0.697 & **0.791** \\  & NMI & 0.557 & 0.670 & 0.622 & 0.625 & 0.656 & 0.549 & 0.558 & 0.642 & 0.655 & **0.752** \\  & Bal & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & **0.263** \\  & MNC & 0.933 & 0.920 & 0.905 & 0.914 & 0.939 & 0.917 & 0.520 & 0.928 & 0.923 & 0.888 & **0.990** \\   & ACC & 0.397 & 0.457 & 0.391 & 0.412 & 0.415 & 0.406 & 0.436 & 0.398 & 0.445 & 0.412 & **0.495** \\  & NMI & 0.400 & 0.442 & 0.359 & 0.407 & 0.406 & 0.446 & 0.449 & 0.382 & 0.402 & 0.416 & **0.454** \\  & Bal & 0 & 0 & 0 & 0 & 0 & 0 & 0.024 & 0 & 0 & **0.808** \\  & MNNC & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.161 & 0 & 0 & **0.993** \\   & ACC & 0.840 & 0.956 & 0.939 & 0.911 & 0.967 & 0.934 & 0.921 & 0.948 & 0.985 & 0.939 & **0.995** \\  & NMI & 0.848 & 0.958 & 0.924 & 0.887 & 0.964 & 0.903 & 0.936 & 0.925 & 0.971 & 0.914 & **0.991** \\  & Bal & 0 & 0.200 & 0 & 0 & 0 & 0 & 0 & 0.250 & 0 & **0.500** \\  & MNC & 0.542 & 0.880 & 0.826 & 0.964 & 0.923 & 0.686 & 0.917 & 0.970 & 0.895 & **0.989** \\   & ACC & **0.402** & 0.333 & 0.63 & 0.360 & 0.337 & 0.370 & 0.321 & 0.327 & 0.355 & 0.378 & 0.375 \\  & NMI & 0.141 & 0.139 & 0.126 & 0.135 & 0.119 & 0.138 & 0.103 & 0.091 & 0.123 & **0.148** & 0.147 \\  & Bal & 0.547 & 0.558 & 0.523 & 0.590 & 0.557 & 0.599 & 0.587 & 0.497 & 0.571 & 0.559 & **0.641** \\  & MNNC & 0.960 & 0.964 & 0.950 & 0.975 & 0.963 & 0.977 & 0.973 & 0.938 & 0.969 & 0.964 & **0.989** \\   & ACC & 0.692 & 0.688 & 0.723 & 0.687 & 0.601 & 0.623 & **0.850** & 0.749 & 0.745 & 0.826 & 0.828 \\  & NMI & 0.435 & 0.535 & 0.286 & 0.545 & 0.436 & 0.523 & **0.652** & 0.554 & 0.581 & 0.632 & 0.601 \\   & Bal & 0.428 & 0.794 & 0.545 & 0.818 & 0.881 & 0.834 & 0.714 & 0.892 & 0.849 & 0.757 & **0.935** \\   & MNNC & 0.881 & 0.991 & 0.937 & 0.993 & 0.997 & 0.994 & 0.980 & 0.998 & 0.995 & 0.986 & **1** \\  

Table 2: Comparison results on the multiple kernel setting. The best and second best results are denoted in **bold** and underlined, respectively.

with the increase of \(\), the fairness grows and the clustering performance may decrease, which is consistent with our previous discussion. We can often achieve a good trade-off between fairness and performance at the red point, which shows the effectiveness of our hyper-parameter setting strategy.

## 6 Conclusion

In this paper, we focused on the fairness issue in KKM and MKKM. We carefully designed a novel fairness regularization term, which can be seamlessly plugged into the KKM and MKKM framework. Equipped with this fairness regularization term, we proposed a novel FKKM and FMKKM method. We also provided a hyper-parameter setting strategy based on the theoretical analysis to make the methods easy to use. Extensive experiments demonstrated the effectiveness and superiority of our proposed FKKM and FMKKM methods.

Although the proposed methods achieve promising performance on fairness, they still have some limitations. For example, in our methods, the protected groups must be pre-given or decided by humans. An interesting question is how to automatically decide the protected groups without human intervention. In the future, we will focus on this problem.