# Using Noise to Infer Aspects of Simplicity

Without Learning

 Zachery Boner\({}^{1}\) Harry Chen\({}^{1}\) Lesia Semenova\({}^{2}\) Ronald Parr\({}^{1}\) Cynthia Rudin\({}^{1}\)

\({}^{1}\)Department of Computer Science, Duke University, \({}^{2}\)Microsoft Research

zachery.boner@duke.edu, harry.chen084@duke.edu, lsemenova@microsoft.com,

ronald.parr@duke.edu, cynthia.rudin@duke.edu

These authors contributed equally to this work. The names are listed in alphabetical order.

###### Abstract

Noise in data significantly influences decision-making in the data science process. In fact, it has been shown that noise in data generation processes leads practitioners to find simpler models. However, an open question still remains: what is the degree of model simplification we can expect under different noise levels? In this work, we address this question by investigating the relationship between the amount of noise and model simplicity across various hypothesis spaces, focusing on decision trees and linear models. We formally show that noise acts as an implicit regularizer for several different noise models. Furthermore, we prove that Rashomon sets (sets of near-optimal models) constructed with noisy data tend to contain simpler models than corresponding Rashomon sets with non-noisy data. Additionally, we show that noise expands the set of "good" features and consequently enlarges the set of models that use at least one good feature. Our work offers theoretical guarantees and practical insights for practitioners and policymakers on whether simple-yet-accurate machine learning models are likely to exist, based on knowledge of noise levels in the data generation process.

## 1 Introduction

Machine learning (ML) is being used more and more for high-stakes decisions, and there is a need for clear policy guidance. Simple models have advantages: they are much easier to troubleshoot and use. However, there is a concern that they are not as accurate as more complex black box models, which makes it challenging to provide guidance for policy makers to recommend simple models.

We believe we have barely scratched the surface of understanding simplicity in machine learning. Recent work suggests we consider the _Rashomon Effect_Breiman (2001), which is the phenomenon that datasets often admit many different good models. Semenova et al. (2022) shows that when there are a lot of good models, some of them are probably simple, meaning that there is no accuracy-simplicity trade-off. But we do not know in advance _how_ simple these models can get. For some applications, very sparse additive models or decision trees perform as well as the best black box models; these datasets do not benefit from complex models at all. When does that happen? How extreme on the simplicity scale do we expect these models to go?

A key insight into this question comes from Semenova et al. (2023), who showed that the simplicity we can expect seems to be related to the amount of outcome noise in the data generation process, which we denote informally as "\(\)." When \(\) is larger, we can get simpler models with performance comparable to the best models. However, while Semenova et al. (2023) showed that noise levels are important, they did not provide a quantitative relationship between noise and simplicity.

In this work, we ask a question that allows us to better understand the connections among noise in the data generation processes, model simplicity, and the Rashomon Effect. The question is asfollows: Given the noise level \(\), how much simpler can our ML models get as compared to the non-noisy case while still maintaining similar generalization performance? Semenova et al. (2023) proves only that the hypothesis space could be simplified in the presence of noise, but does not discuss how much.

To answer this question, we formally prove for two common hypothesis spaces that _noise is an implicit regularizer_, thus leading to simpler models. We quantify how much regularization is added to models as a precise function of the amount of noise, \(\), for several types of noise and regularization. Table 1 summarizes the main results of this paper. For various combinations of hypothesis spaces, losses, and types of noise, we show that if we have regularization \(\) and noise \(\), the optimization problem on noisy data is equivalent to optimizing over the cleaner data with stronger regularization (described in the rightmost column). This means that if we have noisy data, we get a simpler model than if the data were cleaner and we had performed the same optimization.

Since the Rashomon Effect seems to be an important mechanism to understand simplicity, we also study how noise affects it. The _Rashomon set_ is the set of models that have comparable performance to the best models within a class. In this work, we show that, in the presence of noise, the Rashomon set tends to consist of simpler models than in the non-noisy setting. This means that if a user is looking for a model in the Rashomon set that obeys specific constraints (e.g., fairness or monotonicity), these models will be simpler (and this task is likely to be easier) in the presence of noise.

We also study how noise changes the relationship between features and outcomes in an unregularized setting. Specifically, we show that, for decision trees, the number of "good features" (having high AUC relative to other features) increases with noise, and the set of models that use at least one good feature grows larger. Since most models in the Rashomon set use at least one good feature, the Rashomon set of the unregularized hypothesis space might also increase in size with increased noise. This attempts to shed more light on the results that previous work (Semenova et al., 2023) only observed empirically.

We confirm our results empirically and provide practical guidance for the datasets from domains of criminal justice and lending, where we expect outcome noise due to the random nature of the data generation process. We hope that our results are the initial steps that will help machine learning practitioners, and possibly policymakers, to reason about the simplicity of models they can expect to encounter for many high-stakes decision domains.

## 2 Related Work

There are several bodies of related literature.

**Rashomon sets.** The Rashomon set - the set of all near-optimal models - has been studied primarily in the context of its usefulness for solving downstream problems. Examples include developing stable measures of variable importance (Donnelly et al., 2023; Dong and Rudin, 2020; Fisher et al., 2019; Smith et al., 2020), quantifying predictive multiplicity (Marx et al., 2020; Hsu and Calmon, 2022; Watson-Daniels et al., 2023), and understanding fairness (Aivodji et al., 2021; Coston et al., 2021; Shamsabadi et al., 2022). There are also algorithms for computing complete or approximate Rashomon sets for a variety of hypothesis spaces (Mata et al., 2022; Xin et al., 2022; Zhong et al., 2023). The most related prior work to this paper is the work of Semenova et al. (2022) and Semenova et al. (2023), which together demonstrate the existence of large Rashomon sets, and therefore simple models, when there is a significant amount of randomness in the data generation process. In

|p{113.8pt}|p{113.8pt}|p{113.8pt}|}   & Complex hypothesis space/model & Loss & Noise, \(\) & Effective regularization \\ 
1 & Any model optimized on regularized 0-1 loss (e.g. sparse decision trees with leaf penalty, rule lists with length penalty, scoring systems with sparsity penalty) with regularization penalty \(\) & Misclassification error & Random noise & Model optimized with regularization penalty \(\) \\ 
2 & Linear models & Exponential loss & Additive attribute noise & Linear models that minimize logarithm of exponential loss with \(_{2}\) regularization, where \(^{2}\) is the regularization parameter \\  

Table 1: Summary of paper contributions and answers to the key question.

comparison to prior work, ours is the first to provide a quantitative relationship between noise levels and quantities related to simplicity such as regularization and the contents of the Rashomon set.

**Policy and interpretable ML.** With new regulations including "right to explanation," users can request an explanation if an automated decision has been made about them. However, such explanations are often post-hoc and may be misleading (Rudin et al., 2022; Rudin, 2019; Han et al., 2022; Adebayo et al., 2018), contradictory (Krishna et al., 2022), incomplete (Rudin, 2019), or failing in adversarial contexts (Bordt et al., 2022). Interpretable models do not have these problems, and empirically, interpretable models in high-stakes decision domains tend to be as accurate as black-box models; this has been shown in lending (e.g., Chen et al., 2022), criminal justice (e.g., Angelino et al., 2017), and healthcare (e.g., Zhu et al., 2023). However, policy makers still permit black boxes for high-stakes domains, possibly based on accuracy-simplicity trade-off concerns. Thus, more evidence about when this trade-off does and does not exist will be helpful.

**Noise and regularization.** The influence of noise on regularization has been studied for the hypothesis space of neural networks, though no prior work is directly relevant to our aims. Bishop (1995) showed that injecting a small amount of random attribute noise into the training data for a neural network was equivalent in the infinite data limit to a form of Tikhonov regularization on the magnitude of weights. Dhifallah and Lu (2021) extended these results to arbitrary noise for random feature models, which are a restricted class of neural networks. These papers supplement work designing loss functions robust to noisy data for training neural networks (Wang et al., 2019; Ma et al., 2020; Jin et al., 2021; Zhou et al., 2023) and greedily-grown decision trees (Wilton and Ye, 2024). Here, we study the effect of noise on the regularization of sparse models based on 0-1 loss, such as sparse decision trees (Lin et al., 2020), and on linear models trained under exponential loss.

**Noise and SGD.** There has also been recent work focused on analyzing the behavior of the stochastic gradient descent (SGD) algorithm in the presence of artificially injected noise during the training process. More specifically, HaoChen et al. (2021) found that applying label noise at each step of SGD allows the ground truth function of a data distribution to be approximated arbitrarily well, while Gaussian parameter noise may instead lead to poor generalization. Blanc et al. (2020) showed that SGD with label noise acts as an implicit regularizer for models with training error. Damian et al. (2021) and Vivien et al. (2022) generalize this result by showing that SGD implicitly optimizes a regularized objective function under various regimes. Our work instead focuses on noisy data generation processes, independent of the algorithm used to optimize the objective.

## 3 Definitions and Notation

Consider a dataset \(S=\{z_{i}=(x_{i},y_{i})\}_{i=1}^{n}\), where each \(z_{i}=\) is drawn i.i.d. from an unknown true distribution \(\). Here, \(^{n p}\) is the input space, and \(\{-1,1\}^{n}\) is the output space. Let \(\) be a hypothesis space, where \(f\) is a model mapping inputs to outputs, \(f:\). Define \(:^{+}\) as a loss function. For the misclassification error or 0-1 loss, we have \((f(x),y)=_{[f(x) y]}\). The true risk \(L_{}(f)\) is the expected loss over the true distribution \(\), given by \(L_{}(f)=_{z}[(f(x),y)]\), and the empirical risk \(_{S}(f)\) is the average loss on the dataset \(S\) drawn from \(\), calculated as \(_{S}(f)=_{i=1}^{n}(f(x_{i}),y_{i})\). We denote by \(R(f)\) an arbitrary regularization term with regularization parameter \(^{+}\). Regularization induces simplicity in this work; for example, \(R()\) can represent the number of leaves in a decision tree, the length of a rule list, or \(_{0}\), \(_{1}\), or \(_{2}\) norms. We are interested in learning a model \(f_{}^{*}\) that minimizes the true objective \(Obj_{}(f)\) that combines risk and regularization:

\[Obj_{}(f)=L_{}(f)+ R(f),\] (1)

where \(f_{}^{*}_{f}Obj_{}(f)\). Since this model depends on an unknown distribution \(\), we estimate it using the empirical risk minimizer \(_{S}\), defined as: \(_{S}_{f}_{S}(f),\) where \(_{S}(f)=_{S}(f)+ R(f)\).

Following Fisher et al. (2019); Semenova et al. (2022, 2023); Xin et al. (2022), we define the _true Rashomon set_\(R_{_{}}(,)\) to be

\[R_{_{}}(,):=\{f:Obj_{ }(f) Obj_{}(f_{}^{*})+\},\] (2)

that is, if \(L_{}(f)+ R(f) L_{}(f_{}^{*})+ R (f_{}^{*})+\), the model \(f\) is included in the Rashomon set. \( 0\) is the additive Rashomon parameter defined by the user. Similarly, the _empirical_Rashomon set \(_{_{S}}(,)\)_, contains models within \(\) of the regularized empirical risk minimizer: \(_{_{S}}(,):=\{f:_{ S}(f)_{S}(f)\}\). Past work has shown that the true and empirical Rashomon sets may be similar (Semenova et al., 2022; Donnelly et al., 2023). We will omit "true" or "empirical" and use only "Rashomon set" when it is not significant over which distribution the set is computed.

We will model noise in the labels of data with a uniform label noise model, where each label is flipped independently with the fixed probability \((0,)\). To sample data with random label noise from the distribution \(\), we sample \(z=(x,y)\), then with probability \(\) we change the label of \(y\). We denote the noisy version of this data distribution as \(_{}\). By this definition, for \(x,y\), with probability of \(y=1|x\) denoted as \(p_{y}\), when sampling from \(_{}\), we have \(p_{y}(1-2)+\)(Semenova et al., 2023). For a finite dataset, we denote \(S_{}\) to be a dataset sampled according to distribution \(_{}\). Let \(_{}^{n}\) be the distribution of datasets \(S_{}\) under this noise model. As shorthand notation, define \(_{S_{}}\) to mean \(_{S_{}_{}^{n}}\). We assume that in practice we receive noisy data \(S_{}\) and not cleaner data \(S\). (Here, \(S\) does not have the uniform random label noise, but it is not necessarily clean in other ways.)

In this work, we measure how noise impacts the simplicity of the best model in the hypothesis space as well as the models in the Rashomon set. First, in Section 4, we consider random label noise and 0-1 loss, and then examine additive attribute noise for the exponential loss in Section 6. For both of these cases, we show that with more noise, we can expect simpler models.

## 4 Random Label Noise and Regularized 0-1 Loss

Noisy labels are common in real-world datasets, especially in high-stakes decision domains. There are many sources of this noise, including subjective judgments, typographical and clerical errors, and systematic biases. Next, we show that when there is noise in the labels, the regularization of the optimal model is implicitly stronger.

### Noise Increases Regularization

We study the effect of random label noise on the optimal models for 0-1 misclassification loss. We first show that optimizing over the noisy data distribution is equivalent to optimizing over the cleaner data distribution with stronger regularization. Formally:

**Theorem 1** (Regularized 0-1 loss under random label noise).: _Consider true data distribution \(\), and uniform label noise with noise parameter \((0,1/2)\). Let \(_{}\) denote the noisy version of \(\). Consider 0-1 loss \(L\) and let \(R:\) be a regularization function with \(^{+}\) a regularization parameter. Formally,_

\[_{f}L_{_{}}(f)+ R(f)=_{f }L_{}(f)+R(f).\]

_Similarly, given a dataset \(S\) sampled according to \(\), and \(S_{}\) the noisy version of \(S\), \(_{f}_{S_{}}_{S_{}}(f)+ R (f)=_{f}_{S}(f)+R(f)\)._

Theorem 1 applies to any model class and any regularization function over the models in the model class. This includes model classes such as sparse decision trees, which regularize the number of leaves (Lin et al., 2020), rule lists, which regularize the number of rules (Angelino et al., 2017), and scoring systems, which regularize the \(_{0}\)-norm of the parameter vector (Ustun and Rudin, 2016). We prove Theorem 1 in Appendix A. This result proves the first row of Table 1, as each of these hypothesis spaces optimize 0-1 loss with a hypothesis-space-specific regularization function.

A consequence of Theorem 1, intuitively, is that the optimal model with a higher regularization penalty should be simpler and fit the original data less precisely. Formally,

**Theorem 2** (Optimal model simplifies under random label noise).: _Under the same assumptions as in Theorem 1, let \(f_{}^{*}\) be the optimal model in \(\) over distribution \(\) and let \(f_{_{}}^{*}\) be the optimal model in \(\) over \(_{}\). Then either \(R(f_{_{}}^{*})=R(f_{}^{*})\) and \(L_{}(f_{_{}}^{*})=L_{}(f_{}^{*})\) (same complexity model) or_

\[R(f_{_{}}^{*})<R(f_{}^{*})L_{}(f_{ _{}}^{*})>L_{}(f_{}^{*}).\]

_An identical result applies for finite data when \(f_{S_{}}^{*}\) is optimized over the loss function \(_{S_{}}_{S_{}}(f)\)._

A proof of Theorem 2 is in Appendix B. Corollary 10 in Appendix B gives a bound for how much simpler the noisy optimal model will be, based on its performance on the cleaner training data.

### Noise Simplifies the Rashomon Set

In the previous section, we considered optimizing for one model, namely the best-performing model in the class. However, the Rashomon set of models provides a lot of benefits, including deeper insights into the data, flexibility in model selection by choosing a more fair, robust model or model that obeys better domain constraints, and quantification of prediction uncertainty over this set of well-performing models (Rudin et al., 2024). Therefore, we further study how label noise influences the complexity of the models in the true Rashomon set. More specifically, we show that the models in the true Rashomon set arising from a noisy distribution cannot be more complex than those in the Rashomon set for the corresponding cleaner distribution.

Consider two true Rashomon sets \(R_{_{}}(,)\) and \(R_{_{_{}}}(,)\) over cleaner data distribution \(\) and noisier data distribution \(_{}\). We may partition these two sets of models into three disjoint sets: \(_{both}=\{f:f R_{_{}}( ,) R_{_{_{}}}(, )\}\) (the set of models in both the cleaner and noisier Rashomon sets), \(_{out}=\{f:f R_{_{}}( ,) R_{_{_{}}}(,)\}\) (the set of models in the Rashomon set over the cleaner data distribution, but not the noisy one), and \(_{in}=\{f:f R_{_{_{}}} (,) R_{_{}}(, )\}\) (the set of models in the Rashomon set over the noisy data distribution, but not the cleaner one).

We are interested in studying the relationship between the complexity of models in Rashomon sets over cleaner and noisy data. In this direction, we show that under mild assumptions, any models that are in the Rashomon set over the noisier data distribution, but not the cleaner one (\(_{in}\)), will be simpler than the optimal model over the cleaner data. Since models in \(_{both}\) have the same complexity in both Rashomon sets, and models in \(_{out}\) tend to be complex (see Theorem 11 in Appendix C), this result shows that the Rashomon set over noisy data will tend to contain lower complexity models than the Rashomon set over cleaner data. Formally,

**Theorem 3** (Models that enter the noisier true Rashomon set are simple).: _Consider true data distribution \(\), 0-1 loss function, regularization \(R()\) and regularization parameter \(\). Consider also uniform label noise, where each label is flipped independently with probability \((0,)\). Let \(_{}\) be the noisier data distribution. If \(Obj_{_{}}(f^{*}_{})>Obj_{_{}}(f^{*}_ {_{}})+2\), i.e., the optimal model over the cleaner data distribution \(\) is not in the Rashomon set of the noisy distribution with Rashomon parameter \(2\), then every model from \(_{in}\) in the noisier true Rashomon set \(R_{_{_{}}}(,)\) is simpler than \(f^{*}_{}\):_

\[_{in}:R()<R(f^{*}_{}).\]

_More specifically, \(R()<R(f^{*}_{})-(-)\), where \(=Obj_{_{}}(f^{*}_{})-Obj_{_{} }(f^{*}_{_{}})\). Note that \(->0\). An identical result applies for finite data when models are optimized over \(_{S_{}}_{S_{}}(f)\)._

The proof of Theorem 3 is in Appendix C. Note that we showed in the previous section that an optimal model over the noisier data distribution tends to be simpler than an optimal model of the cleaner data distribution, \(R(f^{*}_{_{}})<R(f^{*}_{})\). Therefore, we believe the assumption that \(f^{*}_{}\) is not in the noisier true Rashomon set with the Rashomon parameter \(2\) is plausible in practical noisy settings. For smaller amounts of noise, the cleaner and noisier Rashomon sets may be similar enough to violate the assumption in Theorem 3; in this case, we expect the two Rashomon sets to overlap a lot, leading to larger \(_{both}\) and similar model complexity between the Rashomon sets.

When the data is noisy, practitioners can expect to find simple-and-accurate models within the Rashomon sets for regularized 0-1 loss. We experimentally support our results in Sections 4.1 and 4.2 for empirical datasets and the expected empirical Rashomon set in Section 7 and Appendix I.

## 5 Unregularized Decision Trees and the Set of Grounded Models

The Rashomon ratio measures the size of the Rashomon set relative to the size of the hypothesis space (Semenova et al., 2022, 2023; Rudin et al., 2022). For regularized 0-1 loss, we demonstrated that random label noise is equivalent to an increase in the regularization parameter. A larger regularization parameter penalizes more complicated models and effectively shrinks the hypothesis space, which tends to increase the Rashomon ratio (Semenova et al., 2023). In turn, larger Rashomon ratios correspond to a higher probability of obtaining a desired (e.g., interpretable or simpler) model and correlate with the existence of simpler-yet-accurate models (Semenova et al., 2022).

However, what happens if there is no regularization in the first place and the hypothesis space does not change in size (for example, consider fully-grown decision trees without penalties on the number of leaves)? In this scenario, in the presence of noise, we show that practitioners can still expect to find simple models within the Rashomon set. However, the reason is not due to noise affecting regularization, but rather because noise makes the Rashomon set increase in size. We give theoretical evidence for this claim in Section 5.2, where we show that the _set of grounded models_ (as defined in Section 5.2) grows with noise. This set usually contains the Rashomon set (see Appendix I.3). We begin with an observation about features in our dataset: noise distorts signal in high-quality features faster than in lower-quality features. This leads to an increase in the size of the _set of (relatively) good features_, which we define formally next.

### The Set of Good Features Increases under Noise

For a dataset \(S=X Y\), let \(=\{g_{j}\}_{j=1}^{p}\) denote the set of features, where each \(g_{j}=\{x_{,j}\}\) is the \(j^{th}\) column of the feature matrix \(X\). Note that \(X\) can be continuous or binary. For every feature \(g\), we can evaluate its quality based on how close it is to the label vector \(Y\) according to a similarity function \(_{S}(g)=(g,Y)\). Different metrics can be used as \(()\), including area under the receiver operating characteristic (ROC) curve (AUC), normalized Hamming similarity (one minus the normalized Hamming distance, which is the normalized count of different element values between two binary vectors) if \(g_{j}\) are binary (\(g_{j}\{-1,1\}\)), and correlation if both the labels and the feature are continuous. Given \(_{S}(g)\), we define a set of good features as follows:

**Definition 4** (Set of good features).: _Assume we are given a dataset \(S=X Y\), set of features \(=\{g_{j}\}_{j=1}^{p}=\{x_{,j}\}_{j=1}^{p}\), a feature quality metric \(_{S}(g)\) and a parameter \(\). Let \(:=*{arg\,max}_{g}_{S}(g)\). Then we define the set of good features \(G_{_{S}}(,)\) to be_

\[G_{_{S}}(,):=\{g:_{S}(g) _{S}()-\}.\]

We can think of the set of good features similarly to the Rashomon set, where the former contains all relatively good features based on the quality metric, and the Rashomon set contains all relatively good models (combinations of features) with respect to risk.

Intuitively, we expect that datasets originating from less noisy data generation processes will have higher quality features. For example, if there exists a feature with a very high AUC, then the accuracy of learned models utilizing this feature will also be high. In the presence of label noise, we can precisely calculate how the quality of each feature changes for specific cases, including unnormalized AUC (as demonstrated in Theorem 5 below, proven in Appendix D) and normalized Hamming similarity with binary features (direct consequence of proof in Theorem 1). For a balanced dataset \(S\), where the number of positive and negative samples are the same and equal to \(n/2\), we define the unnormalized AUC as \(_{S}(g)=}{4}AUC_{S}(g)\) (AUC between \(g\) and the label on \(S\)).

**Theorem 5** (Unnormalized AUC for continuous features increases with label noise).: _Consider a balanced dataset \(S=X Y\), i.e. \((y=1)=(y=-1)\). Let \(g=x_{,j}\) be a continuous feature with distinct values \(g^{1}<<g^{n}\). Let \(_{S}(g)\) denote the unnormalized AUC value of \(g\) on the labels \(Y\). Consider uniform label noise, where each label is flipped independently with probability \(<\). Let \(S_{}\) be a noisier dataset. Then for every feature \(g\):_

\[_{S_{}}[_{S_{}}(g)]=(1-2)_ {S}(g)+C(,n),\]

_where \(C(,n)=()(+-1)\) is constant for a given \(\) and \(n\)._

An important corollary directly follows from Theorem 5, which states that under noise, good features with higher AUC lose signal faster than features with lower AUC.

**Corollary 6**.: _Under the same amount of uniform random label noise \(\), the expected unnormalized AUC of features with higher initial value decreases faster than the expected unnormalized AUC of features with lower initial value. For two features \(g_{1},g_{2}\), if \(_{S}(g_{1})<_{S}(g_{2})\), then_

\[_{S}(g_{1})-_{S_{}}_{S_{}}(g_{1 })<_{S}(g_{2})-_{S_{}}_{S_{}}(g_ {2}).\]

The different rate of change of features with different values of AUC also means that under noise, the set of good features increases. Since the quality metric \(_{_{S_{_{1}}}}(g)\) decreases with noise for a given feature \(g\), this implies that in the noisier dataset there are more features with equivalently weak signals as compared to a cleaner dataset.

**Corollary 7**.: _Consider a dataset \(S=X Y\). Let \(\{_{S}(g_{j})\}_{j=1}^{p}\) be in decreasing order and spaced by distances at most \(\), meaning that \(0_{S}(g_{j})-_{S}(g_{j+1})\) for each \(j=1,,p-1\). Assume that we apply uniform label noise with flip probabilities \(_{1}\) and \(_{2}\) to \(S\) to obtain \(S_{_{1}}\) and \(S_{_{2}}\), and that \(|G_{_{S_{_{1}}}_{S_{_{1}}}}(, )|<p\). If \(_{2}(_{1}):=(1-)}{ +(1-2_{1})})\) noting that \((_{1})>_{1}\), then the size of the set of features which are good in expectation is strictly larger with more noise,_

\[|G_{_{S_{_{1}}}_{S_{_{1}}}}(, )|<|G_{_{S_{_{2}}}_{S_{_{2}}}}(,)|.\]

The proof of Corollary 7 is in Appendix E and more experimental results are in Appendix I.3. Note that Corollaries 6 and 7 apply as well to the case of normalized Hamming similarity.

Interestingly, Corollary 7 provides one possible explanation for the existence of large Rashomon sets. If there are more features that can explain the labels approximately-equally-well, then multiple good models could be composed of these features, as we will discuss next.

### The Fraction of Grounded Models Increases for Unregularized Decision Trees

Consider a dataset with binary features and a hypothesis space \(_{d}\) of fully grown decision trees of depth \(d\). For example, a fully grown tree of depth \(2\) has three (internal) nodes (root and two child nodes) and four leaves (two leaves for each child node). Let the _set of grounded models_ (set of models that use good features), \(_{set_{S}}(_{d})\), consist of models that (1) use at least one feature from the set of good features \(G_{_{S}}(,)\), (2) use labels determined by a majority vote of data in the leaves. Let \(_{ratios}(_{d})\) be the fraction of such models in the hypothesis space, meaning that, similar to the Rashomon ratio, \(_{ratios}(_{d})=_{set_{S}}(_{d})|}{|_{d}|}\). Often, the set of grounded models contains most of the Rashomon set, because the trees in the Rashomon set usually rely on at least one feature with a strong relationship with the label (measured by AUC; see Appendix I.3).

Next, we formally show that the fraction of grounded models increases with more random label noise. We again use the set of good features discussed above, with features ordered by AUC values.

**Theorem 8** (Fraction of grounded models increases when the set of good features increases).: _For a dataset \(S=X Y\) with binary feature matrix \(X\{-1,1\}^{n p}\), consider a hypothesis space \(_{d}\) of fully grown trees of depth \(d\). Consider uniform random label noise with noise parameter \(\). Let \(_{_{S_{}}}(_{d})\) denote the set of grounded models, based on the set of good features \(G_{_{S_{}}_{S_{}}}(,)\). Under the assumptions of Corollary 7 on the set of good features, the fraction of grounded models increases with uniform random label noise. More formally, for \(_{2}(_{1})\),_

\[|_{_{S_{_{1}}}}(_{d})|<| _{_{S_{_{2}}}}(_{d})|_{_{_{S_{_{1}}}}}(_{d})< _{_{_{S_{_{2}}}}}(_{d}).\]

The proof of Theorem 8 is in Appendix F. The key observation made in our proof of Theorem 8 is that the set of grounded models increases in size as the set of good features grows. Since the set of grounded models typically contains the Rashomon set, this tends to increase the Rashomon ratio in unregularized hypothesis spaces. As a reminder, larger Rashomon ratios are associated with the existence of simpler-yet-accurate models. We expect the results of this section to hold for other hypothesis spaces as well, e.g., tree ensembles. So far we have considered 0-1 loss with random label noise. Unsurprisingly, we can expect simpler, more regularized, models for other losses and noise models as well. In particular, we next demonstrate that additive attribute noise acts as an implicit regularizer for the hypothesis space of linear models optimized for exponential loss.

## 6 Additive Attribute Noise and the Exponential Loss

It has been known since the 1990s that additive attribute noise to a dataset in the setting of ridge regression acts as an implicit regularizer (Bishop, 1995; Semenova et al., 2023). More specifically, adding noise \(_{i}(,^{2}I)\), where \(\) is a zero vector and \(I\) is identity matrix, to every sample \(x_{i}\) (and thus creating a new sample \(x^{}_{i}=x_{i}+\)) implicitly increases the \(_{2}\) regularization parameter from \(C\) to \(C+^{2}\). Semenova et al. (2023) have also shown that using additive attribute noise increases the Rashomon ratio for ridge regression. However, this work does not address other continuous losses for classification. In this section, we prove that for exponential loss and binaryclassification, additive attribute noise similarly functions as implicit \(_{2}\) regularization on the logarithm of the loss for an otherwise unregularized setting. We give an explicit characterization of the regularization parameter based on the variance of the noise added.

**Theorem 9** (Exponential loss under additive attribute noise).: _Consider the dataset \(S\) and a hypothesis space \(\) of linear models, \(=\{f=^{T}x,^{p}\}\). For a given model \(f\), consider the exponential loss \(L_{S}(f)=_{i=1}^{n}e^{-y_{i}^{T}x_{i}}\). Let \(_{i}\), such that \(_{i}(,^{2}I)\) (\(>0\), \(I\) is identity matrix), be i.i.d. noise vectors added to every sample: \(x^{}_{i}=x_{i}+_{i}\). If \(_{}\,L_{S_{()}}(f)\) is the expected exponential loss under additive Gaussian noise, then_

\[_{}\,L_{S_{()}}(f)=L_{S}(f) e^{}{2}\|\|_{2}^{2}},\]

_where for simplicity we denote \(_{_{1},,_{n}(,^{ 2}I)}\) as \(_{}\)._

A proof of Theorem 9 is in Appendix G. Immediately from Theorem 9, we have that

\[*{argmin}_{f}_{}\,L_{S_{ }}(f)=*{argmin}_{f}( L_{S}(f)+ {^{2}}{2}\|\|_{2}^{2}).\]

In other words, additive noise introduces \(_{2}\) regularization on the logarithm of the exponential loss. Furthermore, the \(_{2}\) regularization parameter is explicitly given as \(}{2}\). This shows that additive attribute noise encourages linear models to become simpler when there is more noise present.

## 7 Experimental Results

We now present experimental results supporting the results in Section 4 for uniform label noise and 0-1 loss and Section 6 for additive attribute noise and exponential loss. We give evidence for our finding that noise acts as an implicit regularizer and that the optimal model optimized over data with injected noise is simpler than the optimal model without additional noise. We focus our experiments in this section on criminal recidivism and financial datasets to emphasize the applicability of our work to high-stakes domains with human data. Please see Appendix I for additional experiments.

### Sparse Decision Trees, 0-1 Loss, Random Label Noise.

We used the GOSDT-guesses algorithm by McTavish et al. (2022) to optimize sparse decision trees over varying amounts of label noise (between 0.0 and 0.3). In order to correctly simulate the results in Theorem 1, the experiment estimates the size of a _model_ optimized over the _expectation_ over

Figure 1: (a), (b): For the hypothesis space of sparse decision trees and 0-1 loss, the number of leaves in optimal models for several datasets decreases with increased label noise. The solid lines depict the observed number of leaves in an optimal model over noisy data. The dashed lines depict the number of leaves of the optimal model over the cleaner data with regularization \(\) (see Theorem 1). (c): For the hypothesis space of linear models and exponential loss, the sum of the squares of the weights corresponding to the continuous features decreases as additive noise with standard deviation \(\) is applied to the dataset.

noise draws of the loss (i.e., \(R(f^{*}_{S_{}})\), where \(f^{*}_{S_{}}*{arg\,min}_{f}*{E}_{S _{}}_{S_{}}+ R(f)\)). This is different from taking the _expectation_ of the _size_ of models optimized over data with only a single noise draw (\(*{E}_{S_{}}R(f^{*})\), where \(f^{*}*{arg\,min}_{f}_{S_{}}(f)+  R(f)\)). To approximate optimization over expected noise draws, we concatenated \(250\) noise draws into a single dataset upon which to optimize a decision tree. The full experimental design is presented in Appendix I. The results of this experiment are shown in Figure 1(a)-(b). These results demonstrate simplification in accordance with Theorem 2 and the alignment between models trained on cleaner data with varying regularization and models trained on noisy data with consistent regularization (Theorem 1).

An interesting observation from these experiments is that as we increase the label noise parameter, the generalization gap between the accuracy on the cleaner train and test set tends to shrink and the test accuracy remains very stable (see Figures 2, 6, 7). This is what we would expect given Theorems 1 and 2, since optimizing over the expectation of noise increases the effective regularization in the cleaner problem.

### Linear Models, Exponential Loss, Additive Attribute Noise

To show that additive attribute noise has a regularizing effect on linear models under the exponential loss, we computed the optimal models on datasets under different noise levels and compared their complexities, measured by the norm of the weights. For each dataset, we sampled \(100\) independent noise draws for \(10\) different noise levels with \([0.05,0.5]\). For each noise draw, we computed the optimal linear model on the noisy dataset using gradient descent. In Figure 1(c), we observe that, as the noise level \(\) increases, the complexity of the optimal model rapidly decreases. This corroborates the regularizing effect of noise demonstrated in Theorem 9.

### Empirical Evidence that Rashomon Sets Over Noisier Data Contain Simpler Models

In Figure 3, we show empirical evidence that the complexity of the Rashomon set of sparse decision trees tends to decrease with the injection of label noise. Moreover, the more noise added to the dataset, the simpler the models in the Rashomon set become. We provide detailed descriptions of the experiments in this section in Appendix I.

## 8 Limitations and Future Work

One limitation of our work is that the theoretical results in Section 4 apply to models optimized over the expected loss over the distribution of possible noise draws with label-flip probability \(\). For larger sample sizes, we expect optimizing over the expectation of noise draws to behave similarly to optimizing over a single noise draw. However, for smaller sample sizes, the optimal model optimized over a single noise draw may deviate from the optimal model over the expectation of noise draws.

Figure 2: Experimental results for Section 4 on recidivism datasets. In accuracy plots (left), blue is accuracy on cleaner test data, orange is accuracy on cleaner train data, and green is accuracy on noisy train data. In complexity plots, blue corresponds to leaves in optimal model over noisy data, orange to optimal model over cleaner data with higher regularization as in Figure 1. Green is the leaf upper bound from Corollary 10. Lambda is regularization parameter optimized via 5-fold CV.

One possible future direction is to bound the expected complexity of a model optimized over a single noise draw in terms of sample size and the probability of flipping labels.

A natural extension of our work is to adapt our results to other losses, like hinge loss and logistic loss. These results are not immediate, because in loss functions more complicated than 0-1 loss, the error considers distances to the decision boundary. Focusing on specific hypothesis spaces rather than on particular choices of loss function (e.g., rule lists optimized on logistic loss, or GAMs optimized on exponential loss) can also help to produce more specific and possibly tighter results similar to those as in Section 6.

We can also try to generalize to other noise models (i.e. random flipping noise in the inputs, or constrained non-uniform noise in the outputs). Dropping the assumption of uniform random label noise needs other techniques as opposed to those that are used in Theorem 1, as the optimization problems are no longer equivalent. If there is non-uniform noise _at least_\(\), it can be decomposed into the non-uniform distribution plus \(\) uniform label noise, and our results in Theorem 1 can be applied. However, such decomposition is not always realistic as, for example, it is very likely that some features are not noisy.

## 9 Practical Guidance for High-Stakes Decision Domains

The fallacy of the premise of the movie Minority Report is that it is possible to predict with perfect accuracy whether someone will commit a crime in the future. In reality, predictions of recidivism are made \(\)2 or 3 years in advance, giving time for a multitude of random interactions in the world to take place. This unpredictability leads, as we showed in this paper, to inherent regularization, and provably simpler models than if noise were not present. Rather than assuming that increased algorithmic sophistication in the future can potentially lead to improved accuracy using the same types of data, it is more realistic to assume that the distributions of data in the future are generally similar to those in the present, and that our best ML methods already reach an approximate performance maximum for these types of data. This latter view clears the way for policy-makers to regulate the use of simpler models. Already the use of black box models has led to individuals being denied freedom based on typographical errors (Wexler, 2017, 2018; Rudin et al., 2020) and patients being deceived about the value of expensive medical treatment options (Afnan et al., 2021).

Our findings underscore the critical importance of using simpler models for datasets affected by noise, thereby prioritizing model interpretability and transparency. We believe that this understanding can empower policymakers to advocate for the use of simple, interpretable models, ensuring the trustworthy, accessible, and equitable deployment of AI systems in high-stakes decision domains.

Figure 3: A visual demonstration of the simplifying effect of noise on Rashomon sets. This shows a bar chart of the discrete probability distribution of the number of leaves among models in the Rashomon set. Results are shown aggregated in (a) for 23 real-world and synthetic datasets, and in (b), just real-world data, for 9 recidivism and finance datasets.