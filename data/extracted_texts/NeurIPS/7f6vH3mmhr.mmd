# Multi-Agent Learning with Heterogeneous Linear Contextual Bandits

Anh Do

Johns Hopkins University

ado8@jhu.edu

&Thanh Nguyen-Tang

Johns Hopkins University

nguyent@cs.jhu.edu

&Raman Arora

Johns Hopkins University

arora@cs.jhu.edu

###### Abstract

As trained intelligent systems become increasingly pervasive, multi-agent learning has emerged as a popular framework for studying complex interactions between autonomous agents. Yet, a formal understanding of how and when learners in heterogeneous environments benefit from sharing their respective experiences is still in its infancy. In this paper, we seek answers to these questions in the context of linear contextual bandits. We present a novel distributed learning algorithm based on the upper confidence bound (UCB) algorithm, which we refer to as H-LinUCB, wherein agents cooperatively minimize the group regret under the coordination of a central server. In the setting where the level of heterogeneity or dissimilarity across the environments is known to the agents, we show that H-LinUCB is provably optimal in regimes where the tasks are highly similar or highly dissimilar.

## 1 Introduction

Heterogeneous multi-agent systems enable agents to work together and coordinate their actions to solve complex problems. These systems are inherently scalable, as they can distribute the computational load across multiple agents. This scalability allows the system to handle large and sophisticated tasks beyond the capabilities of a single agent. Despite the potential of multi-agent systems, it poses the following fundamental challenges.

* **Statistical challenge.** Each agent's reward distribution may vary, meaning that different agents receive different rewards for the same action. This heterogeneity in reward distributions introduces complexity and makes coordination among agents more difficult. Furthermore, Wang et al. (2021) point out that an ineffective use of shared data could lead to a significant negative impact on the overall performance. In particular, sharing experiences amongst agents may hinder the system's performance if the tasks are too dissimilar (Rosenstein, 2005; Brunskill and Li, 2013).
* **Computational complexity.** Coordinating the decisions of numerous agents and performing complex computations pose challenges in terms of computational resources, time constraints, and algorithmic scalability.
* **Communication cost.** Efficient communication is also a fundamental challenge for a large-scale multi-agent system. As the number of agents in a system increases, the complexity of interactions between agents grows exponentially. Managing the interaction of numerous agents and making decisions in a timely manner becomes increasingly difficult.

Several works address these challenges for heterogeneous multi-agent systems, including federated linear bandits (Huang et al., 2021; Li and Wang, 2022), clustering bandits (Gentile et al., 2014; Ghosh et al., 2023), multi-task linear bandits (Hu et al., 2021; Yang et al., 2021). However, these works either rely on some special structure of the parameter (Li and Wang, 2022), e.g., a low-rank structure (Huet al., 2021) or make different assumptions, e.g., stochastic contexts, finite decision set (Huang et al., 2021; Ghosh et al., 2023), etc.

In this work, we provide a general notion of heterogeneous multi-agent linear contextual bandits (\(\)-MALCB) and give analytical results under different regimes of heterogeneity. Specifically, we study a model that consists of \(M\) agents. Each agent \(i[M]\) plays a \(d\)-dimensional linear contextual bandit, parametrized by \(_{i}\) (see Section 3 for more details), for \(T\) rounds. We capture the heterogeneity by a dissimilarity parameter \(>0\) such that \(\|_{i}-_{j}\|_{2}\), for all \(i,j[M]\). Notably, we do not assume any special structure on the linear parameter; and we allow the decision set to be infinite and possibly chosen adversarially.

Motivating application.Consider a personalized recommendation system for online advertisements (Li et al., 2010; Bouneffouf et al., 2020; Ghosh et al., 2023). Here, the platform needs to be adaptive to user preferences and maximize total user clicks based on user-click feedback. Each ad can be represented as a context vector, encoding information such as publisher, topic, ad content, etc. The inner product of the ad's context vector and user preference represents the alignment. A higher inner product value indicates greater relevance of the ad. Furthermore, the recommended ads must be personalized to accommodate user preference differences. One naive approach would involve solving a separate linear contextual bandit problem for each user. However, we can pose the following question: Can we enhance the system's performance by pooling data from other users? If so, to what extent of user heterogeneity can we achieve that?

Contributions.We make the following contributions in this paper.

* First, we formulate the heterogeneous multi-agent linear contextual bandits as \(\)-MALCB problem, building on the classic notion of heterogeneity in multiarmed bandits (MABs) Wang et al. (2021). Our notion of heterogeneity is natural and captures many settings in real-world applications.
* Second, when the level of dissimilarity is known, we propose a distributed algorithm, namely H-LinUCB which achieves a regret of \(}(d+\{ dMT,dM\})\) under the coordination of a central server. We discuss in detail how to handle the dissimilarity and introduce a criterion for stopping collaboration when the level of dissimilarity is high. We show that under the regime of low dissimilarity, we can still achieve a regret of \(}(d)\), which is the same regret rate as if \(M\) agents collaborate to solve the _same_ task. In this regime, H-LinUCB outperforms independent learners improving by a factor of \(\), from \(}(dM)\) to \(}(d)\). This is significant when we have a large number of agents.
* Third, we complement the upper bound with a lower bound of \((d+\{ MT,dM\})\). This suggests that our theoretical guarantees are tight in settings where tasks are highly similar or highly dissimilar.
* Finally, we validate our theoretical results with numerical simulations on synthetic data. When the level of dissimilarity is small, H-LinUCB outperforms independent learning. When the level of dissimilarity is high, our simulation shows that blindly using shared data can lead to linear regret, emphasizing the importance of the criterion we propose for when to stop the collaboration.

## 2 Related work

The classic linear bandits have a rich literature in both theory and application; see, for example, (Abbasi-Yadkori et al., 2011; Li et al., 2010; Chu et al., 2011; Chatterji et al., 2020; Rusmevichientong and Tsitsiklis, 2010; Bouneffouf et al., 2020; Mahadik et al., 2020), to name a few. With a surge in distributed computing, multi-agent systems have shown their potential and gained more attention in recent years. A large body of works dedicated to studying the homogeneous setting of multiple collaborating agents solve a global linear bandits problem (Wang et al., 2020; Dubey and Pentland, 2020; Moradipari et al., 2022; Mitra et al., 2022; Martinez-Rubio et al., 2019; Chawla et al., 2022).

The problem of multi-agent linear bandits in heterogeneous environments, on the other hand, has received limited attention. Soare et al. (2014) were amongst the first to study heterogeneous linear bandits; however, the focus in that work is not on group regret, and the authors only consider the setting where tasks are similar. More recently, Huang et al. (2021) proposed an algorithm with a novel multi-agent G-Optimal design. They assume that the heterogeneity comes from the contexts associated with each agent, but agents can still collaborate since they share the same arm parameters. Li and Wang (2022) consider an extension where they assume that each agent's parameter has two components - a shared global component and an individual local component. This formalization requires agents to work on their respective tasks (the local component) but still allows agents to collaborate on the common task (the global component). Wang et al. (2021) study heterogeneity of the Bernoulli MABs problem and provide guarantees for both cases when the level of heterogeneity is known and unknown.

A related line of work studies heterogeneous linear bandits through clustering (Gentile et al., 2014; Li et al., 2016, 2019; Korda et al., 2016; Ghosh et al., 2023). These works give a guarantee based on the clustering structure of the different linear bandit problems - agents belonging to the same cluster will likely achieve the highest collaboration gain. We do not make any assumption about the "clusterability" of different bandit problems we may encounter. A yet another approach focuses on multi-task linear bandits, wherein we solve multiple different but closely related linear bandits tasks (Yang et al., 2021, 2022; Hu et al., 2021; Cella et al., 2023; Du et al., 2023). In particular, these works rely on the assumption that all tasks share a common \(k\)-dimensional representation, where \(k d\). Then, pooling data from different bandits helps learn a good representation and reduces the statistical burden of learning by reducing the linear bandit problem in \(d\) dimensions to a \(k\)-dimensional setting. We do not consider multi-task learning here.

Our formulation of heterogeneous contextual linear bandits is similar to that of misspecified and corrupted bandits setting (Remark 3.1 for more details) (Ghosh et al., 2017; Lattimore and Csaba, 2020; Takemura et al., 2021; Foster et al., 2020; He et al., 2022). It is then natural to ask if we can apply the techniques from that part of the literature to deal with the dissimilarity between different bandits in a heterogeneous setting. However, there is a fundamental difference in how the two problems manifest themselves. While misspecification may be typically unavoidable in many settings, in a heterogeneous bandit setting, an agent can always choose to rely solely on its own data if it finds that the data from other agents are too dissimilar.

## 3 Preliminaries

Multi-Agent Linear Contextual Bandits.We consider a multi-agent learning setting with \(M\) agents. At each round \(t[T]\), each agent \(m[M]\) picks an action (context)1\(x_{m,t}_{m,t}\), where \(_{m,t}^{d}\) is a given decision set. The agent \(m\) receives reward \(y_{m,t}=x_{m,t}^{}_{m}+_{m,t}\), where \(_{m}^{d}\) is an unknown but fixed parameter and \(_{m,t}\) is sub-Gaussian noise. Let \(_{t}\) denote the filtration, i.e., the \(\)-algebra, induced by \((\{x_{m,k}\}_{m[M],k t+1},\{_{m,k}\}_{m[M],k t})\).

Regret.Our goal is to design algorithms for multi-agent linear contextual bandits that achieve a small group regret defined as

\[(M,T)=_{t=1}^{T}_{m=1}^{M}(_{x_{m,t }} x,_{m}- x_{m,t},_{m}).\]

Assumption 3.1.: Without loss of generality, we assume that,

1. **Bounded parameters**: \(\|_{m}\|_{2} 1,\|x\|_{2} 1, x_{m,t},m[M],t [T]\).
2. **Sub-Gaussian noise**: \(_{m,t}\) is conditionally zero-mean 1-sub-Gaussian random variable with respect to \(_{t-1}\).

We note that the assumptions above are standard in linear bandits literature (Abbasi-Yadkori et al., 2011; Hu et al., 2021; Huang et al., 2021). Further, it is straightforward to let \(\|_{m}\| B\), for some constant \(B\), by appropriately scaling the rewards. We make no additional assumptions on the context. The decision set could be infinite, and given to each agent possibly adversarially.

**Definition 3.1**.: _(\(\)-MALCB Problem) A multi-agent linear contextual bandits problem is said to be an \(\)-MALCB problem instance, if for any two agents \(i,j[M]\), \(\|_{i}-_{j}\|_{2}\), for an \( 0\). We call \(\) the dissimilarity parameter._

**Definition 3.2**.: _(Homogeneous setting) A multi-agent linear contextual bandits problem is homogeneous, if it is an \(\)-MALCB with \(=0\), i.e., \(_{i}=_{j}\), for all \(i,j[M]\)._

Given the bound on the parameters, we have that \(\|_{i}-_{j}\|_{2} 2\) for any \(i,j[M]\). Therefore, it suffices to only consider the case where \(\).

**Remark 3.1**.: (Misspecified structure) Under the Assumption 3.1, for any two agents \(i,j\) we have that \(_{i}^{}x-_{j}^{}x_{i}^{}x+\). Then, \([y_{j,x}]=_{i}^{}x+(x)\), for \((x)[-,]\). This represents a misspecified structure wherein agent \(i\) receives the reward \(y_{j,x}\) from agent \(j i\).

**Remark 3.2**.: (Recover the \(\)-MPMAB of Wang et al. (2021)). We note that the \(\)-MPMAB is a special case of \(\)-MALCB. Define the mean reward of \(K\) arms for agent \(m\) as \(_{m}=[_{1}^{m},,_{K}^{m}]\). Then, the reward for arm \(k\) at round \(t\) is \(y_{t,k}^{m}=_{i}^{}e_{k}+_{t}\). The decision set \(=\{e_{1},,e_{K}\}\) are the standard basis vectors. This is a fixed set of arms, given to all agents at each round. The dissimilarity parameter \(\) is defined as: \(\|_{i}-_{j}\|_{}\) for all \(i,j[M]\).

Nonetheless, the results in Wang et al. (2021) are not directly comparable to ours since the dissimilarity parameter \(\) hides inside the size of the set of _subpar_ arms \(|_{}|\).2 Furthermore, Wang et al. (2021) give guarantees in a full-communication setting, in which each agent has full access to the past data of all other agents at every round.

**Remark 3.3**.: There are other formulations that also capture the heterogeneity in multi-agent linear bandits. Huang et al. (2021) consider a multi-agent linear bandits setting with a fixed size decision set, containing \(K\) actions, \(\{_{a}\}_{a=1}^{K}\), which is unknown to the agents. Each agent \(i\) is associated with \(K\) different contexts \(\{x_{i,a}\}_{a=1}^{K}\). At reach round, each agent \(i\) picks an action \(a[K]\), and receives reward \(r_{i,a}=x_{i,a}^{}_{a}+_{i,a}\). Since \(x_{i,a}\) can vary for different agents, this captures the heterogeneity across agents. It also allows for collaboration across agents since they share the same decision set.

Li and Wang (2022) assume that each agent parameter \(\) has a special structure that consists of a shared global component and a unique local component. The reward of agent \(i\) can be given as,

\[r_{i,x}=[^{(g)}\\ ^{(i)}]^{}[^{(g)} \\ ^{(l)}]\]

The notion of \(\)-MALCB is in the worst-case sense. One can imagine an \(\)-MALCB instance of \(M\) agents, such that \(M-1\) agents have identical linear parameter, i.e. \(_{i}=_{j}, i,j[M-1]\), and the parameter of the last agent \(\|_{M}-_{M-1}\|_{2}=\). With this type of instance, for a large \(M\), we can simply use DisLinUCB of Wang et al. (2021) and achieve nearly optimal regret. Clustering bandits framework could also be used for this \(\)-MALCB instance since it presents a strong cluster structure. Even though our formulation takes a pessimistic approach but it can handle the case that bandits are largely unclustered.

Our goal is to design a system such that its performance is no worse than running \(M\) independent bandit algorithms for each user (zero collaboration). The system should also be adaptive to the heterogeneity in the problem instance, i.e., it should automatically leverage any structure in the problem parameters \(\{_{m}\}_{m=1}^{M}\) to collaboratively solve all bandit problems at a "faster" rate. To benchmark the performance of such a system, we consider the following baseline.

Independent Learners (Ind-OFUL).We establish a baseline algorithm in which each agent independently runs an optimal linear contextual bandits algorithm (OFUL, (Abbasi-Yadkori et al., 2011)) without any communication. Each agent incurs \(}(d)\) regret, and \(}(dM)\) group regret.

Notation.We denote the weighted norm of vector \(x\) w.r.t. matrix \(A\) (Mahalanobis norm) as \(\|x\|_{A}=Ax}\). We write \(A B\) iff \(A-B\) is a positive semi-definite matrix. We use \(}()\) to hide the polylogarithmic factors in standard Big O notation.

Main Results

In this section, we present H-LinUCB, a UCB-style algorithm for \(\)-MALCB problem, and give guarantees for the case when the dissimilarity \(\) is known to the agents. In Section 4.2, we present a lower bound and discuss the implication of our results in different regimes of dissimilarity. We defer the detailed proof to the Appendix.

```
0: Dissimilarity parameter \(\), number of agents \(M\), number of rounds \(T\), regularization parameter \(\), dimension \(d\), confidence parameters \(_{t}, t[T]\), weight threshold parameter \(\), collaboration budget \(\), synchronization threshold \(D\)
1:\(t_{syn} 1\)\(\)\(t_{syn}\) is the index of the last synchronized round
2:\(V_{syn} 0,b_{syn} 0 V_{syn},b_{syn}\) store the relevant statistics of all agents after synchronization
3:\(V_{epoch,m} 0,b_{epoch,m} 0, m[M]\)\(\)\(V_{epoch,m},b_{epoch,m}\) store the relevant statistics of agent \(m\) in the current epoch
4:for\(t=1,,T\)do
5:for Agent \(m=1,,M\)do
6:if\(t=\)then
7:\(V_{syn} 0,b_{syn} 0\)
8:\(V_{epoch,m} 0,b_{epoch,m} 0\)
9:endif
10:\(V_{m,t} I+V_{syn}+V_{epoch,m}\)
11:\(_{m,t} V_{m,t}^{-1}(b_{syn}+b_{epoch,m})\)
12: Construct the confidence ellipsoid \(_{m,t}=\{^{d}:\|_{m,t}- \|_{V_{m,t}}_{t}\}\)
13:\((x_{m,t},_{m,t})=*{arg\,max}_{(x,) _{m,t}_{m,t}},x\)
14: Play \(x_{m,t}\) and get reward \(y_{m,t}\)
15:\(w_{m,t}[t<](1,/\|x_{m,t}\|_ {V_{m,t}^{-1}})+[t]\)
16:\(V_{epoch,m} V_{epoch,m}+w_{m,t}x_{m,t}x_{m,t}^{}\)
17:\(b_{epoch,m} b_{epoch,m}+w_{m,t}x_{m,t}y_{m,t}\)
18:if\([(V_{m,t}+w_{m,t}x_{m,t}x_{m,t}x_{m,t}^{})/( I+V_ {syn})](t-t_{syn}) D\) and \(t<\)then
19: Send a synchronization signal to the server to start a communication round
20:endif
21:if A communication round is started then
22: Agent \(i\) sends \(V_{epoch,i},b_{epoch,i}\) to the server, \( i[M]\)
23: Server computes \(V_{syn} V_{syn}+V_{epoch,i},b_{syn} b_{syn}+b_{epoch,i},  i[M]\)
24: Server sends \(V_{syn},b_{syn}\) back to all agents
25:\(V_{epoch,i} 0;b_{epoch,i} 0; i[M]\)\(\) Reset \(V_{epoch,i},b_{epoch,i}\) for the new epoch
26:\(t_{syn} t\)
27:endif
28:endfor
29:endfor ```

**Algorithm 1** H-LinUCB

### H-LinUCB Algorithm

H-LinUCB is a distributed UCB-style algorithm (see Algorithm 1 for pseudocode), in which agents work cooperatively under the coordination of a central server.

H-LinUCB has two learning phases: the _collaboration_ phase (for rounds \(t\{1,,-1\}\)) and the _independent learning_ phase (for rounds \(t\{,,T\}\)), where \( T\) is the collaboration budget. Intuitively, our two-phase learning framework ensures that the agents stop collaboration after \(\) rounds lest they incur a linear regret in bandit environments with large dissimilarity. Naturally, then, the parameter \(\) should depend on the dissimilarity parameter, \(\). We give an optimal choice of \(\) in Theorem 4.1.

At each round \(t<\) (the collaboration phase), each agent's data is weighted to adapt to the dissimilarity across different agents (Line 15). Then, each agent uses the weighted data to construct its Confidence Ellipsoid (Line 12) and makes a decision following the optimism principle (Line 13). When a certain condition is met (Line 18), data is pooled and synchronized across the agents. Starting from round \(\), all collaboration ceases and each agent enters the independent learning mode and runs an independent copy of the OFUL algorithm (Abbasi-Yadkori et al., 2011) locally for the last \(T-+1\) rounds.

We note that H-LinUCB builds upon DisLinUCB of Wang et al. (2020, Protocol 8) with the following modifications:

* We scale each agent's data using the weight \((1,/\|x_{m,t}\|_{V_{m,t}^{-1}})\), which we adopt from He et al. (2022), to handle the dissimilarity across different agents (Line 15).
* We only allow collaboration until round \(\) (Line 18). The value of \(\) depends on the dissimilarity parameter, which we assume is given.
* We reset the variables \(V_{syn},b_{syn},V_{epoch,m},b_{epoch,m}\) at round \(\) (Lines 6-9), where each agent switches to the independent learning mode. Here, \(epoch\) refers to the time period between two consecutive synchronization rounds.

Each agent uses all of the data available to them at each round to construct the _Confidence Ellipsoid_\(_{m,t}\) using the result in Lemma 4.1. Given the confidence ellipsoid, the agent chooses the action optimistically: \((x_{m,t},_{m,t})=_{(x,)_{m,t} _{m,t}},x\). During the collaboration phase, if the variation in the volume of the ellipsoid exceeds a certain synchronization threshold, \(D\), it triggers a synchronization condition (Lines 18-20). Subsequently, the central server commences the synchronization procedure to update \(V_{syn},b_{syn}\) across all participating agents (Lines 21-27. The optimal value of \(D\) depends on the number of agents \(M\), dimension \(d\), and the collaboration budget \(\).

The weight \((1,/\|x_{m,t}\|_{V_{m,t}^{-1}})\) is a truncation of the inverse bonus, where \(>0\) is a threshold parameter that shall be optimized later. When \(x_{m,t}\) is not explored much, we have a large exploration bonus \(\|x_{m,t}\|_{V_{m,t}^{-1}}\) (low confidence). Hence, the algorithm will put a small weight on it to avoid a large regret due to stochastic noise and misspecification. When \(\|x_{m,t}\|_{V_{m,t}^{-1}}\) is small (high confidence), H-LinUCB puts a large weight on it, and it can be as large as one (He et al., 2022).3 We note that using this weighting could have a significant negative impact on the performance if we are not careful. Several recent studies show that we can incur a regret of \(}(d+T)\) and \(}(d+ dT)\) for misspecified and corrupted linear bandits, respectively (Lattimore and Csaba, 2020, Takemura et al., 2021, Foster et al., 2020, He et al., 2022).4 However, a direct application of an algorithm designed for misspecified/corrupted linear bandits to our setting can lead to linear regret when \(=(1)\). This is significantly worse as compared to naive independent learning, which always achieves sub-linear \(}(dM)\) regret.

We emphasize that the condition \((t<)\) in Line 18 is crucial to avoid linear regret for H-LinUCB in the regime of large \(\). For example, for \(=(1)\), \(=T\), Algorithm 1 incurs \(}(dMT)\) regret, which is linear in term of \(MT\). Furthermore, Theorem 4.3 indicates that there exists an instance of \(\)-MALCB such that any algorithm incurs at least \((dM)\) regret. Then, each agent playing OFUL independently would be enough to achieve an optimal \(}(dM)\) regret. This suggests that we get a tighter upper bound if we cease collaboration; we discuss the stopping criterion and the choice of \(\) in Theorem 4.2.

Communication protocol.We use a star-shaped communication network where \(M\) agents can interact with a central server (Wang et al., 2020, Dubey and Pentland, 2020). Each agent communicates with the server by uploading and downloading its data but does not communicate directly with each other. The communication will be triggered only if any agent has enough new data since the last synchronization round. Finally, we assume no latency, or error in the communication between the central server and agents.

**Remark 4.1**.: Wang et al. (2020) show that DisLinUCB can rely on old data and produce nearly optimal policy without much communication, only incurring logarithmic factors in the final regret. H-LinUCB has the same communication cost of \((M^{1.5}d^{3})\), as DisLinUCB, which does not depend on the horizon \(T\).

When an agent uses data from other agents, due to the dissimilarity, we need to adjust our confidence bound to make sure the true linear parameter \(_{m}\) lies in the defined ellipsoid with high probability. The next result shows how to construct such a _Confidence Ellipsoid_.

**Lemma 4.1**.: _(Confidence Ellipsoid). With probability at least \(1-M_{1}-M_{2}\), for each agent \(m[M]\), \(_{m}\) lies in the confidence set,_

\[_{m,t}=\{^{d}:\|_{m,t}- \|_{V_{m,t}}_{t}\},\]

_where_

\[_{t}=+ Mt+})},t<,\\ +})}, t.\]

Note that the result above provides two separate confidence bounds for \(_{m}\), one for the period before round \(\) and the other one for after round \(\). Before round \(\), agent \(m\) will use all the data from other agents to construct \(_{m,t}\). The proof follows by first bounding \(\|_{m,t}-_{m}\|_{V_{m,t}()}\) as

\[\|_{m,t}-_{m}\|_{V_{m,t}()}}{}}+}{_{k=1}^{t_{s}}w_{i,k}x_{i,k}x_{i,k}^{}( _{i}-_{m})\|_{V_{m,t}^{-1}()}}}+}{}}.\]

Here, \(I_{1}\) is a bounded regularization term, \(I_{3}\) is a noise term that can be bounded by self-normalization lemma (Lemma C.4). Finally, the dissimilarity term \(I_{2}\) is bounded from above by \( Mt\) using the definition of dissimilarity \(\|_{i}-_{m}\|\), and applying a similar argument as He et al. (2022) and the choice of the weight \(w_{m,t}=(1,/\|x_{m,t}\|_{V_{m,t}^{-1}})\). For the phase after round \(\), we use the same argument as Abbasi-Yadkori et al. (2011) to construct the confidence bound.

Next, we present the group regret upper bound of Algorithm 1 up to round \(\).

**Theorem 4.1**.: _Given Assumption 3.1, \(T 1\), any \( T\), and \(_{1}>0\), setting \(=1\) and \(=}{ M}\) in the upper bound of \(_{t}\) on the confidence interval according to Lemma 4.1\( t[T]\), and setting the synchronization threshold \(D=(M)/(dM)\), we have that with probability at least \(1-M_{1}\), the group regret of Algorithm 1 up to round \(\), is bounded as_

\[(M,) 20(d_{}^{2}+  dM_{}^{1.5}),\]

_where \(_{t}=(})\)._

Theorem 4.1 shows that Algorithm 1 incurs \(}(d)\) regret in the first term (which is the same order as a single agent playing for \(M\) rounds) plus a penalty of using the data from other agents in the order of \(}( dM)\). The regret of \(}(d)\) is unavoidable for any regime of \(\), and this rate is known to be optimal in the case of homogeneous multi-agent. Since we use the technique from He et al. (2022) to handle the dissimilarity, the corruption amount of each round is \(\), and a total corruption of \( dM\) when the central server allows to collaborate up to round \(\). To the best of our knowledge, we are not aware of any UCB-based algorithm for misspecified linear bandits in the setting of infinite arms. We expect that employing a misspecified linear bandits algorithm would achieve a regret of \(}(d+MT)\), which is tighter by a factor of \(\). It is worth noting that the CW-OFUL algorithm in He et al. (2022) is designed for handling corruption, whether it can achieve \(}(d+T)\) in a misspecified setting remains an open question.

We now present our main result giving an upper bound on the group regret of H-LinUCB.

**Theorem 4.2**.: _Given Assumption 3.1, \(T 1\), let \(=1,=}{ dM},_{1}=_{2}= T^{2}}\) in the upper bound of \(_{t}\) on the confidence interval (see Lemma 4.1) \( t[T]\). Let \(=(},T)\), and let the synchronization threshold \(D=(M)/(dM)\). Then, the expected group regret of Algorithm 1 is bounded as_

\[[(M,T)] 320(d+2 \{ dMT,dM\})^{2}(MT).\]

Here, \(\) is the maximum round that the central server allows communication. After that, all agents switch to independent learning. By choosing \(=(},T)\), agents fully cooperate in the regime \([0,}]\) if \(T}\), and gradually reduce \(\) as \(\) increases from \(}\) to \(+\). This is important for avoiding a linear regret since when \(\) is large, most of the regret comes from the \( dMT\) term and dominates the \(d\) term. The condition on Line 6 also discards all of the synchronized data. In the extreme case, when \(>1\), there is no collaboration happening due to the condition in Line 18 failing at every round. In other words, H-LinUCB behaves like Ind-OFUL.

In the other extreme case, when \(=0\), all agents solve identical linear bandits. The weight in Line 15 always evaluates to its minimum value of \(1\) for all \(t[T]\) since \(=}{ MT}+\). We have \(t<^{2}\) for all rounds. Therefore, the reset condition in Line 6 is never triggered and H-LinUCB behaves exactly like DisLinUCB, achieving a regret of \(}(d)\), which is optimal up to some logarithmic factors.

Theorem 4.2 suggests that the upper bound of H-LinUCB is tighter than Ind-OFUL for all \(\).

**Remark 4.2**.: Ghosh et al. (2023) also propose a personalized algorithm (PMLB) for the heterogeneous multi-agent linear bandits; however, our problem setting is fundamentally different than that of Ghosh et al. (2023). They consider a finite action set and impose a strong distributional assumption on how contexts are generated, i.e., the stochastic context \(x_{i,t}\) for each action \(i\) and at each round \(t\) is zero-mean and forms a positive-definite covariance matrix. In stark contrast, we consider the adversarial setting where the context set is _adversarially_ generated at each round (and thus, the associated action set can be infinite and arbitrary). This renders the algorithm and guarantees of Ghosh et al. (2023) inapplicable in our setting and, thus, requires a completely different treatment. Those assumptions of Ghosh et al. (2023) are crucial for them to obtain the \(}(T^{1/4})\) bound (for \(=0\)). We note that this bound is not information-theoretically possible in our adversarial setting; the minimax lower bound in such settings is \(()\) (Theorem 4.3).

### Lower bound

In this section, we present a lower bound result for the \(\)-MALCB problem. We denote \(_{,}(M,T)\) as a regret of algorithm \(\) on a problem instance \(\) of \(M\) agents run for \(T\) rounds.

**Theorem 4.3**.: _Let \(()\) denote the class of \(\)-MALCB problem instances that satisfy the Assumption 3.1. Then for any \(d,M,T^{+}\) with \( T,}{48} T\), \( 0\), we have the following,_

\[_{}_{()}_{ ,}(M,T)=(d+\{ MT,dM\}).\]

The first term is a straightforward observation that solving an \(\)-MALCB is at least as hard as solving a single linear bandits for \(MT\) rounds, or \(M\) agents solving identical bandits for \(T\) rounds. The second term suggests that we pay an additional regret of \( MT\) for a _small_\([0,}]\), and \((dM)\) for a _large_\(}\). We note that \((dM)\) is also the lower bound of Ind-OFUL when each agent incurs a regret of at least \((d)\). We believe that the analysis of the lower bound could be tightened by using the arguments from misspecified bandits literature, achieving a lower bound of \((d+\{MT,dM\})\).

The lower bound suggests that our upper bound is tight up to logarithmic factors in the following extreme regimes, (i) \([0,}]\), where \((M,T)=(d)\); (ii) \([},+]\), where \((M,T)=(dM)\). In regime (i), all agents solve tasks that are similar to one another, yielding the highest collaborative gain. In regime (ii), tasks are highly dissimilar, H-LinUCB turns off the collaboration and lets agents solve their own tasks individually.

Finally, in the regime that \((},})\), our results illustrate the interpolation between two extremes. In this regime, our upper bound presents a gap of \(d\) in the dissimilarity term \( dMT\) compared to \( MT\) in the lower bound.

The key idea in the proof of Theorem 4.3 is based on an information-theoretic lower bound of Lemma C.1, wherein we extend the result from (single-agent) linear bandits to _heterogeneous_ multi-agent linear bandits. Here, we give the lower bound result without any constraints on the communication, hence, this is also the lower bound of the H-LinUCB algorithm.

## 5 Numerical Simulations

In this section, we provide some numerical simulations to support our theory. Our goal is to address the following question: how does H-LinUCB perform in three different regimes of dissimilarity: (i) \([0,}]\), (ii) \((},})\), (iii) \([},+]\)?

We compare the performance of H-LinUCB with that of the following two algorithms: (a) Independent Learers (Ind-OFUL), wherein each agent independently runs OFUL algorithm of Abbasi-Yadkori et al. (2011), and there is no communication between agents (zero collaboration), and (b) DisLinUCB, for which we use the implementation of Wang et al. (2020) without any modification.

Simulation setup.We generate the \(\)-MALCB problem for \(M=60,d=30,T=10000\) via the following procedure. We first choose a value of \(\) in each of the three dissimilarity regimes. Then we create the linear parameters \(\{_{m}\}_{m=1}^{M}\) as follows. Let \(u,\{v_{m}\}_{m=1}^{M}\) be random vectors with unit norm. We set \(_{m}=c u+v_{m}\), where \(c\) is a constant in the range \([0,1-]\). This guarantees \(\|_{m}\| 1\) and \(\|_{i}-_{j}\|\) for any two agents \(i,j\). At each round, for each agent, we create a new decision set with a size of \(50\), each action is random and normalized to 1. The random noise is sampled from the standard normal distribution, \((0,1)\). We run each experiment 10 times, then report the

Figure 1: Simulation on synthetic data with \(M=60,d=30,T=10000\).

group regret averaged over the runs and the confidence intervals in Figure 1. Our code is available here: https://github.com/anhddo/hlinUCB.

Results and discussions.In regime (i), where the level of dissimilarity is small, plots (a) and (b) show that H-LinUCB retains a regret comparable with DisLinUCB.

In regime (ii), plots (c) and (d) illustrate the interpolation between the two extreme regimes.

In regime (iii), plots (e) and (f), DisLinUCB incurs linear regret, H-LinUCB has the same rate with Ind-OFUL. This illustrates that collaboration brings no benefit when the dissimilarity is high.

## 6 Conclusions

In this paper, we studied the _heterogeneous_ multi-agent linear contextual bandit problem. We formulated the problem under the notion of \(\)-MALCB, and provided the upper and lower bounds when \(\) is known. We showed that our results are provably optimal in the regime where tasks are highly similar or highly dissimilar. Finally, we validated our theoretical results with numerical simulations on synthetic data.

A natural avenue for future work would be to close the gap in the regime \((},})\). Another research direction pertains to designing an adaptive algorithm when \(\) is unknown. Such an algorithm would be practical and flexible enough to apply to a wide range of heterogeneous multi-agent bandit problems. We are also interested in extending this work to a more challenging setting such as Reinforcement Learning.