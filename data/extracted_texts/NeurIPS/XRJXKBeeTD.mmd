# Fine-Tuning is Fine, if Calibrated

Zheda Mai\({}^{1}\), Arpita Chowdhury\({}^{1}\), Ping Zhang\({}^{1}\), Cheng-Hao Tu\({}^{1}\), Hong-You Chen\({}^{1}\),

**Vardaan Pahuja\({}^{1}\)**, **Tanya Berger-Wolf\({}^{1}\)**, **Song Gao\({}^{2}\)**, **Charles Stewart\({}^{3}\)**, **Yu Su\({}^{1}\)**, **Wei-Lun Chao\({}^{1}\)**

\({}^{1}\)The Ohio State University, \({}^{2}\)University of Wisconsin Madison, \({}^{3}\) Rensselaer Polytechnic Institute.

Equal contributions.

###### Abstract

Fine-tuning is arguably the most straightforward way to tailor a pre-trained model (_e.g._, a foundation model) to downstream applications, but it also comes with the risk of losing valuable knowledge the model had learned in pre-training. For example, fine-tuning a pre-trained classifier capable of recognizing a large number of classes to master a subset of classes at hand is shown to drastically degrade the model's accuracy in the other classes it had previously learned. As such, it is hard to further use the fine-tuned model when it encounters classes beyond the fine-tuning data. In this paper, we systematically dissect the issue, aiming to answer the fundamental question, _"What has been damaged in the fine-tuned model?"_ To our surprise, we find that the fine-tuned model neither forgets the relationship among the other classes nor degrades the features to recognize these classes. Instead, the fine-tuned model often produces more discriminative features for these other classes, even if they were missing during fine-tuning! What really hurts the accuracy is the discrepant logit scales between the fine-tuning classes and the other classes, implying that a simple post-processing calibration would bring back the pre-trained model's capability and at the same time unveil the feature improvement over all classes. We conduct an extensive empirical study to demonstrate the robustness of our findings and provide preliminary explanations underlying them, suggesting new directions for future theoretical analysis. Our code is available at https://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated.

## 1 Introduction

Pre-trained models (_e.g._, foundation models) have become an indispensable component in modern AI development . Building upon neural networks with millions if not billions of parameters and trained with gigantic amounts of data, these models have led to groundbreaking results in various domains [30; 32; 39] and shown several emerging capabilities not observed priorly [21; 27; 2].

Yet, to obtain superior downstream performance, fine-tuning is still often needed. Typically, fine-tuning optimizes the model's performance on the available downstream data. Taking image classification as an example, end-users typically fine-tune the pre-trained classifier to maximize the accuracy of a certain set of classes at hand, no matter whether they know up front that it is the complete set of classes or not. As a result, it is hard to further apply the model to some other classes, _even if the model had learned about those classes in pre-training._ (Please see Figure 1 for an illustration.)

A recent work by Tu _et al._ systematically evidenced such a problem. They fine-tuned a pre-trained classifier with a subset of classes it had learned (_i.e._, _fine-tuning_ classes) and found this led to a drastic accuracy drop in the other classes (_i.e._, _absent_ classes). Tu _et al._ viewed this as an instance of catastrophic forgetting [12; 24; 7] and suggested two ways to address it: (1) identifying the model updating direction that can improve both the fine-tuning and absent classes; (2) preserving class relationships. They presented a strong baseline, combining a new stochastic gradient descent (SGD)strategy, feature rank regularization , distillation , and frozen classifiers , achieving decent results on preserving absent class accuracy while improving fine-tuning class accuracy.

We build upon  to further analyze the problem of _fine-tuning a pre-trained model_. Specifically, we want to understand _which part of the pre-trained classifier or what knowledge it had learned_ is damaged after fine-tuning with a subset of classes.

We first analyze the feature extractor of the fine-tuned model. _If the fine-tuned feature extractor forgets the absent classes, the extracted features would exhibit poor discrimination among these classes or confuse them with the fine-tuning classes._ We apply the Nearest Class Mean (NCM) classifier  to assess the feature quality after fine-tuning, using the hold-out data for calculating class means. Surprisingly, the fine-tuned feature extractor does not degrade but often improves for the absent classes! The resulting features could better separate absent-class data and raise the NCM classification accuracy over all the classes. This finding suggests that fine-tuning with merely a subset of classes can already _holistically_ improve the feature extractor for the downstream domain.

To search for the root cause of accuracy drops, we analyze the fine-tuned classifier as a whole but _decompose its prediction rule into two parts_: a binary classifier separating fine-tuning and absent classes, followed by the multi-class classifier dedicated to each set of classes. We find that the fine-tuned model can distinguish among the absent classes very well, implying that the absent-class relationship in the fully connected layer is preserved. _The only component that is damaged but ends up failing the whole model is the binary classifier._ Concretely, the biased logits towards fine-tuning classes make most absent-class examples misclassified as fine-tuning classes.

_The above findings are encouraging and have profound implications!_ They imply that a simple _post-processing calibration_ can potentially address the fine-tuned model's inferior accuracy on the absent classes, bringing back the pre-trained model's capability while unveiling the improved feature quality over all classes. Indeed, by adding a calibration bias factor to all the absent classes' logits , the fine-tuned model can successfully reclaim the absent class accuracy and obtain decent overall improvement in the downstream domain (Figure 2). The resulting performance even beats the strong baseline  in many of the benchmarks, including ImageNet and its variants , Office-Home , and VTAB , _without complicated training and hyperparameter setting_.

The "unexpected benign behaviors" of fine-tuning with a subset of classes raise several interesting follow-up questions. For example, are there any specific setups in our experiment contributing to the benign behaviors? If not, what are the explanations underlying these benign behaviors?

We consider different splits of fine-tuning and absent classes in terms of their distributions and numbers. We find that the benign behaviors are robust to the number of fine-tuning classes. Even if the fine-tuning and absent classes are semantically distinct (_e.g._, animals as fine-tuning and vehicles as absent classes), the absent class accuracy after fine-tuning stays quite close to that of the pre-trained model without suffering catastrophic forgetting. We further investigate different optimizers for fine-tuning. When the SGD optimizer is used, the benign behaviors are robust to hyperparameters such as learning rates and weight decay. When more advanced, adaptive optimizers like Adam  are applied, we observe noticeable degradation with improperly selected hyperparameters. Still, with smaller enough learning rates and weight decay, the benign behaviors show up and hold.

We conduct further analyses to understand the benign behaviors. Specifically, we investigate how fine-tuning with a subset of classes impacts the other (_i.e_., absent) class features. Our derivation shows that after each SGD step, the feature update of an absent-class example is governed by the gradients w.r.t. its most similar fine-tuning class examples. Suppose the gradients w.r.t. fine-tuning class data could effectively capture the downstream-specific information, our derivation offers a preliminary explanation of why fine-tuning with a subset of classes could improve the other, absent class features in the downstream domain. Please refer to section 6 for details and other analyses regarding the class relationship, frozen classifiers, etc.

**Remark.** We systematically dissect the damage caused by fine-tuning a pre-trained classifier with a subset of classes at hand. Our insights suggest that a simple post-processing calibration is sufficient to mitigate the damage, reclaiming the pre-training model's capability while holistically improving the accuracy in the downstream domain. Our study also opens up several interesting questions worth further exploration. For example, it is intriguing that end-to-end fine-tuning of the whole model without additional regularization and intermediate supervision only degrades a part of the model.

We note that NCM and post-processing calibration are well-known machine learning techniques and we certainly do not claim them as our novelties. Instead, we use them because we find their respective properties and application scenarios suitable for our problem. We use NCM to assess feature qualities because its performance solely depends on features, not the last fully connected layer. We identify post-processing calibration as a promising solution because we find that _the only major damage in the fine-tuned model is the biased logits towards fine-tuning classes_. In other words, if the fine-tuned model also suffers from feature degradation or class relationship forgetting (to our surprise, it does not), calibration alone is unlikely sufficient to address the problem.

## 2 Related Work

**Fine-tuning.** The basic methods are linear probing and full fine-tuning . Parameter-efficient fine-tuning (PEFT) [60; 35; 50] has attracted increasing attention lately (mainly for Transformers ), aiming to update only a fraction of pre-trained parameters on top of linear probing. _We focus on full fine-tuning_ because it is model-agnostic and arguably still the most widely used method. That said, we expect the insights and implications from our study not to be limited to full fine-tuning and potentially transferable to other fine-tuning methods as well.

**Risk in fine-tuning.** When the downstream data is scarce, fine-tuning is prone to over-fitting and needs certain forms of regularization . Even with sufficient data to represent the downstream task, fine-tuning may risk losing valuable knowledge the pre-trained model had learned. For example,  showed that fine-tuning with data from a specific domain (_e.g_., ImageNet-1K real images) degrades the pre-trained model's generalizability to other domains (_e.g_., ImageNet-Sketch/Rendition). On an orthogonal dimension,  showed that fine-tuning a pre-trained classifier with a subset of classes it had learned led to a huge accuracy drop in the other classes. Similar phenomenons have been observed in [66; 40]. Along with these findings come several proposed solutions.  showed that a weight interpolation between the pre-trained and fine-tuned models reclaims the pre-trained model's generalizability.  investigated many approaches, including weight interpolation, but found they are insufficient to preserve the accuracy in the other classes. They presented a novel SGD strategy that helps identify the gradient direction benefiting both fine-tuning and other classes.  developed dedicated solutions to CLIP-based vision-language models to preserve accuracy for absent classes.

_We 1) consider the fine-tuning scenario studied in  and 2) use the standard neural network classifier architecture with a fully connected layer on top._ Our focus is not to propose a brand-new solution and compete with existing ones, but to understand the underlying cause of the accuracy drop.

**Continual learning and catastrophic forgetting.**_Fine-tuning a pre-trained model with a subset of classes it had learned_ is related to continual learning  but has several differences. Class-incremental learning [33; 36] aims to expand the model's label space. In contrast, we suppose the pre-trained model had learned a wide range of classes; fine-tuning is meant to tailor it to a downstream domain (_e.g_., a different image style), not to learn extra categories. Compared to domain-incremental learning [33; 31], we do not ask the fine-tuned model to retain its performance in the pre-training domain. That said, the accuracy drop observed in our study and continual learning can all be considered certain forms of catastrophic forgetting [37; 12]. Indeed, a recent survey by Wang _et al_.  argued that forgetting is a common issue in various research fields, including foundation models, meta-learning,test-time adaptation, and generative models, among others. We thus expect our findings to serve as a valuable reference for addressing the forgetting issue resulting from fine-tuning.

**Post-processing calibration.** Training with class-imbalanced data is known to produce biased logits towards major classes . Such an issue not only appears in long-tailed recognition [20; 18] but also few-shot learning [62; 45] and continual learning with replay buffers [48; 47; 34]. Post-processing calibration [3; 38; 58; 59] is a widely applicable method to address this issue, which adjusts model confidence during inference. Popular methods include normalizing classifier norms [20; 17] and adjusting logits according to class sizes [61; 38; 22; 65]. Unlike the machine learning problems above, whether or not post-processing calibration can address the accuracy drop in our problem is not immediately clear. Without access to the absent class data, fine-tuning may have ruined the features or linear classifiers corresponding to these classes. If so, simply performing post-processing calibration cannot reclaim the accuracy of the absent classes. Our main contribution is therefore not merely the solution, but the systematic study that identifies post-processing calibration as an effective solution.

**Other paradigms.** There are several other machine learning paradigms related to the fine-tuning setting we study. We refer the readers to  for an in-depth discussion and comparison.

## 3 Background

**Problem definition.** We study the problem of fine-tuning a pre-trained classifier capable of recognizing \(C\) classes, using data from a subset of \(C^{*}\) classes at hand. The goal is to tailor the classifier to the downstream domain (_e.g_., a different image style).

More formally, let us denote by \(D_{}=\{(_{i},y_{i})\}_{i=1}^{N}\) the data set for fine-tuning, where \(\) is a **strict subset** of the pre-trained model's label space \(\), _i.e_., \(||=C^{*}<C=||\). Let us denote a neural network classifier by \(=*{arg\,max}_{c}_{c}^{}f_{}()\), where \(\) is an input sample, \(f_{}\) is the feature extractor parameterized by \(\), and \(=\{_{c}\}_{c=1}^{C}\) is the set of linear classifiers, a.k.a., the fully-connected (FC) layer. We call \(\{,\}\) the model parameters. The value \(_{c}^{}f_{}()\) is often referred to as the decision value or logit of class \(c\).

Without loss of generality, we define \(=\{1,,C\}\) and \(=\{1,,C^{*}\}\). That is, they share the first \(C^{*}\) classes. We call \(\) the fine-tuning classes and use \(=\{(C^{*}+1),,C\}\) to denote the absent classes during fine-tuning, where \(=\) and \(=\).

**Fine-tuning and its issue.** Full fine-tuning updates the pre-trained model \(\{_{},_{}\}\) by

\[\{_{},_{}\}= *{arg\,min}_{\{,\}}_{i=1}^{N} (_{i},y_{i};,),\{,\}\{_{ },_{}\},\]

where \(\) denotes the cross-entropy loss; \(\{_{},_{}\}\) denotes the fine-tuned model.

Since \(D_{}\) only covers a subset of classes \(\), the fine-tuned model \(\{_{},_{}\}\) was observed to degrade drastically in classifying data from the absent classes \(\). Figure 2 shows one example: the absent class accuracy in the y-axis drops from \( 23\%\) (\(\)) to only \( 3\%\) (marked \(\)) after fine-tuning, even though the fine-tuning class accuracy in the x-axis increases hugely by roughly \(60\%\).

**Terminology.** Tu _et al_. named the above setting Holistic Transfer (HT) . The core challenge is how to maintain and even improve the fine-tuned model's ability to recognize the \(C-C^{*}\) absent classes in the downstream domain. For ease of reference, we use the following terms interchangeably.

* fine-tuning classes & seen classes; absent classes & unseen classes;
* downstream domain & target domain; pre-training domain & source domain;
* fine-tuning & naive fine-tuning.

We emphasize that the "unseen" classes are indeed seen in pre-training but absent in fine-tuning.

## 4 A Systematic Study of Fine-Tuning (FT)

Seeing the ability to classify the absent classes "disappears" after fine-tuning, we are curious about

* how each component of the fine-tuned model \(\{_{},_{}\}\) contributes to it;* whether the ability is "forgotten" forever or "buried" temporarily by some other damaging factors emerging during fine-tuning.

To this end, we conduct a systematic analysis, aiming to dissect the degradation caused by fine-tuning.

### Experiment setup: datasets, models, and evaluation metrics

We focus on two of the largest datasets used in . We also consider the ImageNet Distribution Shift benchmark widely used in out-of-distribution (OOD) generalization .

1. **Office-Home**: a domain adaptation dataset with 65 classes and 4 domains. For each source-target pair, we pre-train a ResNet-50  using the source data and fine-tune it on 30 randomly selected classes in the target domain. We evaluate the model on all 65 classes in the target domain.
2. **VTAB**: a set of 19 visual recognition datasets. We follow  to use the \(8\) datasets provided with text labels and use CLIP (ViT-B/32)  as the pre-trained model. We extract the class name embedding to construct the FC layer and discard the CLIP text encoder afterward. We fine-tune the model on the randomly selected \(50\%\) of classes in each dataset and test it on all the classes.
3. **ImageNet-R ** and **ImageNet-S **: datasets for OOD detection and generalization . ImageNet-R  consists of \(200\) ImageNet classes  with renditions (paintings, cartoons, etc.). ImageNet-S  consists of 1K ImageNet classes with sketching. We use ResNet-50 (ViT-B/32  results in Appendix D) pre-trained on ImageNet-1K. We fine-tune them on randomly sampled 50% of classes (100/500 for ImageNet-R/S, respectively) and test them on all the classes.

By default, we use the cross-entropy loss and optimize the model using the SGD momentum optimizer.

**Evaluation metric.** Let \(D_{}=\{(_{i},y_{i})\}_{j=1}^{M}\) denote the downstream test set covering all the classes, we define the accuracy of classifying data belonging to classes \(\) into the label space \(\) by

\[_{/}=[y_{i} ][y_{i}=*{arg\,max}_{c} \,\,_{c}^{}_{}(_{i})]}{_{i}[y _{i}]}.\] (1)

For instance, \(_{/}\) is the accuracy of classifying fine-tuning class data into all \(C\) classes; \(_{/}\) is the accuracy of classifying absent class data into all \(C\) classes. We note that these are the two accuracies reported in  and depicted in Figure 2. In this paper, we further consider \(_{/}\) and \(_{/}\), corresponding to classifying each set of data into their respective label space.

### Is the fine-tuned feature extractor damaged?

We first investigate the feature extractor \(f_{}\), _i.e._, whether the fine-tuned extractor \(f_{_{}}\) forgets the discriminative ability to differentiate absent-class samples. We apply the NCM classifier , whose accuracy is solely governed by the feature quality, to examine the feature extractor. Given a test example \(\), the NCM classification rule is

\[=*{arg\,min}_{c}\| _{}}()}{\|f_{_{}}()\|_{2}}-_{c}\|_{2},\] (2)

which outputs the class \(\) whose feature mean \(_{}\) is the closest to \(f_{}()\). We hold out a subset of the downstream data to calculate the class mean, for both fine-tuning and absent classes.

Figure 3: **Accuracy gain after fine-tuning. We consider the neural network (NN) classifier with the FC layer (section 3) and the NCM classifier using only features (Equation 2). We show the average accuracy gain on fine-tuning classes (\(_{/}\)) and absent classes (\(_{/}\)). While the NN classifier suffers drops in \(_{/}\), the NCM classifier enjoys a consistent gain, suggesting the holistic improvement of features after fine-tuning.**

We compute the NCM accuracy (with \(=\)) on the fine-tuning class and absent class data using the fine-tuned (FT) feature extractor \(f_{_{}}\), _i.e._, \(_{/}(f_{_{}})\) and \(_{/}(f_{_{}})\). We also apply the pre-trained feature extractor \(f_{_{}}\) and obtain \(_{/}(f_{_{}})\) and \(_{/}(f_{_{}})\). We report the NCM accuracy gap between the two feature extractors in Figure 3. As one may expect, the FT extractor \(f_{_{}}\) improves the fine-tuning class accuracy. However, surprisingly, it also improves the absent class accuracy _without catastrophic forgetting_. This result sharply contrasts the accuracy obtained by the full neural network (NN) classifier (cf. section 3). As shown in Figure 3, the absent class accuracy by the NN classifier drops by \(15 40\%\) after fine-tuning. In short, we find that fine-tuning with a subset of classes can adapt the feature extractor holistically to the downstream domain to improve both the fine-tuning and absent classes, capturing the gradient direction of the domain shift .

**Remark.** We use NCM _only_ to assess feature qualities, not as the final classifier. After all, we do not have absent class data to compute the class means. This contrasts the use of NCM in continual or few-shot learning, where the prior- or few-shot class data are accessible (_e.g._, through replay buffers).

We emphasize that the feature improvement here is not as trivial as observed in conventional transfer or few-shot learning. **In these learning problems**, the pre-trained model typically had not learned about the absent classes; thus, there is no risk of forgetting them. Besides, the fine-tuning and absent classes are often treated as two separate tasks in evaluation; thus, it is unclear whether the fine-tuned feature extractor improves the overall classification accuracy. **In contrast, in our fine-tuning setting**, the pre-trained model had already learned about the absent classes. Fine-tuning thus risks forgetting them. However, as shown in Figure 3, fine-tuning improves absent class accuracy, in the context where the fine-tuning and absent classes are evaluated together in a single task.

### What is damaged in the fine-tuned neural network classifier?

The findings in subsection 4.2 eliminate the feature extractor \(_{}\) from the candidate root causes of the degraded FT model \(\{_{},_{}\}\). The drastic drop of absent class accuracy thus must come from the FC layer \(_{}\) or its alignment with the feature extractor. To analyze what goes wrong, we decompose the softmax probability induced by the neural network classifier as follows,

\[p(c|)= _{c}^{}f_{}())}{ _{c^{}}(_{c^{}}^{}f_{}())}=()}{_{c^{}( )}z_{c^{}}()}\] (3) \[= }z_{c^{}}()}{_ {c^{}}z_{c^{}}()+_{c^{}}z_{c^{}}()}()}{_{c^{} }z_{c^{}}()},\] (4)

where \(z_{c}()=(_{c}^{}f_{}())\). Let \(c\) be an absent class, the \(1^{}\) term stands for the predicted probability that \(\) belongs to the absent classes \(\), not the fine-tuning classes \(\); the \(2^{}\) term stands for the probability that within the absent classes \(\), \(x\) belongs to class \(c\). Correctly classifying an absent class example (\(,y\)), _i.e._, \(y=_{c}p(c|)\), thus requires 1) obtaining a high probability in the \(1^{}\) term and 2) correctly classifying the example among absent classes.

Building upon this insight, we analyze the accuracy of taking \(\) of the \(2^{}\) term to classify absent class examples. We note that this is exactly the \(_{/}\) defined in subsection 4.1. As shown in Figure 4, \(_{/}\) does not degrade but improves in the first few epochs of fine-tuning and stays stable afterward. This result implies two key messages. First, the FT model does not forget its ability to distinguish among absent classes. Second, the drastic accuracy drop of \(_{/}\) results from the \(1^{}\) term in Equation 4 -- the binary classifier separating fine-tuning and absent classes.

As shown in Figure 5, the predicted probability that _absent class examples belong to absent classes_ (_i.e._, the average of the \(1^{}\) term over absent class examples) reduces notably along with the fine-tuningepochs, suggesting the tendency to misclassify an absent class sample as fine-tuning classes. Indeed, as shown in Figure 6, for most of the absent class samples, the FT model ends up producing a lower value of \(_{c}_{c}^{}f_{}()\) than \(_{c}_{c}^{}f_{}()\), misclassifying them into fine-tuning classes. In short, _the main factor that damages the FT model's ability to correctly classify absent class examples is the biased logit values towards fine-tuning classes._

## 5 Post-Processing Calibration for the Rescue

The systematic study in section 4 highlights several key characteristics of the FT model \(\{_{},_{}\}\). First, the model retains and even improves the accuracy of classifying absent class samples when the label space is limited to absent classes (_i.e_., \(_{/}\)). Second, the model tends to assign much higher decision values \(_{c}^{}f_{}()\), a.k.a. logits, to the fine-tuning classes, ending up misclassifying most absent class samples into fine-tuning classes and thus hurting \(_{/}\).

_These characteristics suggest that a simple, post-processing calibration of the FT model's logits could potentially bring back the pre-trained model's ability to classify absent classes correctly._ To this end, we apply the calibration approach proposed in a different context of zero-shot learning , which adds a factor \(\) uniformly to the logits of all absent classes, leading to a new classification rule

\[=_{c}_{c}^{}f_{}()+ [c].\] (5)

This calibration factor lifts the logits of absent classes to a level comparable with those of fine-tuning classes. Suppose an absent class example is correctly classified among absent classes but misclassified into fine-tuning classes, adding a sufficiently large \(\) could reclaim the correct prediction.

We design two approaches to properly set \(\)_without_ accessing the absent class data. **Average logit gap (ALG)** estimates \(\) by the average logit gap between non-ground-truth fine-tuning classes and absent classes in the fine-tuning data \(D_{}\). **Pseudo cross-validation (PCV)** partitions \(D_{}\) into pseudo-fine-tuning and pseudo-absent classes and finds \(\) that can balance the pseudo-fine-tuning and pseudo-absent class accuracy. We also investigate a **"cheating" \(\)** based on the test data to estimate the upper-bound results. More details are in Appendix B.

**Performance.** Table 1 shows the results on ImageNet-{R, S}, VTAB, and Office-Home, averaged over datasets within each benchmark. Despite its simplicity, calibration effectively boosts the FT model's accuracy on the absent classes, achieving comparable accuracy to the SOTA method proposed in . On ImageNet-{R, S}, both calibration approaches outperform the SOTA by a notable margin.

**Compatibility of calibration.** Post-processing calibration is potentially applicable to any model. In Figure 7, we extend the calibration approach to the pre-trained model and the SOTA model  with varying \(\). Calibration can effectively balance \(_{/}\) and \(_{/}\) and trade one for the other. Interestingly, we find the curve of the FT model, in most cases, can well cover those of the other models. To characterize this, we follow 

   & } &  &  \\  Metrics (\%) & \(_{/}\) & \(_{/}\) & \(_{/}\) & \(_{/}\) & \(_{/}\) & \(_{/}\) & \(_{/}\) & \(_{/}\) \\  Pre-trained & 23.6 & 23.5 & 23.8 & 58.3 & 59.3 & 57.4 & 63.8 & 61.8 & 65.5 \\ Fine-tuning & 43.3 & 81.3 & 3.5 & 62.8 & 86.4 & 39.1 & 53.5 & 88.3 & 22.4 \\ Tu _et al_.  & 47.5 & 62.1 & 23.1 & 63.8 & 83.9 & 43.5 & 72.0 & 78.2 & 66.6 \\  Fine-tuning + \(_{}\) & 55.9 & 80.3 & 30.5 & 68.6 & 85.3 & 48.2 & 65.0 & 87.7 & 44.9 \\ Fine-tuning +\(_{}\) & 57.1 & 60.1 & 54.0 & 57.4 & 47.1 & 67.8 & 72.2 & 82.3 & 63.1 \\ Fine-tuning + \(\) & 60.8 & 73.6 & 47.6 & 69.3 & 75.6 & 62.8 & 72.7 & 79.1 & 66.9 \\  Oracle & 71.1 & 72.4 & 69.8 & 80.6 & 79.8 & 81.3 & 82.1 & 81.2 & 82.9 \\  

Table 1: Post-processing calibration can effectively bring back the pre-trained model’s capability in recognizing absent classes. Oracle is based on a classifier fine-tuned with both fine-tuning and absent class data.

   & } &  &  \\  Pre-trained & 0.083 & 0.444 & 0.499 \\  Tu _et al_.  & 0.317 & 0.553 & 0.618 \\  Fine-tuning & **0.439** & **0.586** & **0.632** \\  

Table 2: Results in AUSUC.

Figure 7: **Fine-tuning vs. Absent Class Accuracy Curve (\(_{/}\) at the x-axis; \(_{/}\) at the y-axis) by varying the factor \(\) for ImageNet-R.**

to calculate the Area Under the Seen-Unseen (((Fine-tuning / Absent)) Curve (AUSUC) to summarize the overall performance of each model across the entire spectrum of \(\). Table 2 reports the AUSUC. The FT model notably outperforms the other methods, showing its robustness in learning from a subset of classes.

**Remark.** Many post-processing calibration methods have been proposed (cf. section 2). Our study is not meant to compare them, but to show that calibration can effectively address the issue caused by fine-tuning. We also note that Tu _et al_.  has investigated many solutions, including weight interpolation , but found them less effective them their proposed SOTA method.

## 6 Ablation Study and Additional Analysis

**Data split and fine-tuning class size.** In the default setup of , fine-tuning classes and absent classes are uniformly randomly sampled and have each portion close to \(50\%\). In practice, however, end-users may have a smaller number of classes at hand or collect data from semantically or conceptually similar classes whose appearances are positively correlated. To explore such practical situations, we investigate how 1) a biased sampling such that the fine-tuning classes are conceptually or distributionally similar to each other than the absent classes and 2) a smaller size of fine-tuning classes would impact the performance of fine-tuning. Specifically, for Office-Home, classes that are similar in the pre-trained model's feature space are selected as fine-tuning classes, leaving the rest as absent ones. We also vary the number of fine-tuning classes. For ImageNet-S, we leverage the WordNet  hierarchy to select coherent groups, such as dog (118 classes) and mammal (218 classes) as the fine-tuning classes. Please see Appendix A for details.

Figure 8 illustrates the performance gain on AUSUC and NCM Acc\({}_{/}\) (from the pre-trained model to the FT model). Notably, the hierarchical split poses a greater challenge for fine-tuning in transferring domain knowledge from fine-tuning classes to absent classes, as evidenced by its relatively minor AUSU improvements compared to the random split. This difficulty is attributed to the inherent challenge of transferring features learned from dogs or animals to distinctly different classes like TVs and trucks. Furthermore, smaller fine-tuning class sizes present additional difficulties, leading to less pronounced improvements, especially in Office-Home. In summary, our analysis suggests that the benign behaviors of fine-tuning are robust in more practical and difficult splits but its performance requires further improvement when the fine-tuning class size is exceptionally small. Detailed experimental setup and additional results are available in Appendix D.

**Optimizer.** Prior work  predominantly used the SGD optimizer. To investigate optimizers' influence in fine-tuning, we scrutinize six popular optimizers including SGD with Momentum, Adam , AdaBelief , Adadelta , AdaGrad  and RMSprop. Our study includes 1) a variation across learning rates (LR), adjusted as multipliers of the default ones, _i.e_., \([10,1,0.1,0.01]\) LR, and 2) three distinct weight decays: \([0,5-4,5-3]\). The AUSUC, as illustrated in Figure 9, reveal the robustness of the benign behaviors to different hyperparameter settings of the SGD optimizer. Conversely, more advanced, adaptive optimizers show higher sensitivity when hyperparameters are not properly chosen. Nevertheless, under small enough learning rates (and weight decay), they perform similarly to SGD and notably improve the pre-trained model (whose AUSUC is around \(0.5\) in Office-Home).

**Why is absent class relationship preserved?** To elucidate the preservation of the absent class relationship, we analyze how classifier weights \(\) change within the fine-tuning and absent classes. We commence by calculating the L2 normalized weight changes between fine-tuning and the pre

Figure 8: The performance gain on AUSUC and NCM Acc\({}_{/}\) (from the pre-trained model to the FT model), under different data splits and fine-tuning class sizes. The hierarchical split of ImageNet-S contains dog (118 classes), mammal (218 classes), and animal (398 classes) as the fine-tuning classes.

trained model \(=_{}-_{}}{\|_{ }-_{}\|_{2}}\). Visualizing the cosine similarity of the change within fine-tuning and absent classes, _i.e._\(^{}(^{})^{}\) and \(^{}(^{})^{}\), allows us to assess the directional similarities in classifier updates. As depicted in Figure 10, there exists a pronounced dissimilarity in the update directions among fine-tuning classes since the gradients aim to separate fine-tuning classes during fine-tuning. Conversely, a notable similarity in the update directions is observed among absent classes. The absence of positive signals from absent class data results in nearly uniform updates of absent classifiers, thereby preserving the class relationships among absent classes throughout fine-tuning. More results for other datasets can be found in Appendix D.

**Absent Features and linear classifier Alignment in fine-tuning.** section 4 has demonstrated that the alignment between absent features and the linear classifier (_i.e._, the FC layer) remains intact during fine-tuning, as evidenced by the stable or improved \(_{/}\), despite the absence of absent class data. This indicates that fine-tuning retains, and even enhances, its capacity to differentiate among absent classes. To delve deeper into this phenomenon, we scrutinize the behavior of ground-truth class (GT) vs. the largest non-GT absent logits for absent test samples. As depicted in Figure 12, although both sets of logits exhibit a decline throughout the training process-- attributable to the lack of absent data during fine-tuning--the relative difference between them persists, underlining a consistent and stable alignment between the features and the linear classifier. Future work may delve into the theoretical understanding of this observed alignment between features and classifiers for absent classes.

**Why do absent class features improve after fine-tuning?** During fine-tuning, the pre-trained model is updated solely by gradients derived from the fine-tuning class data. Surprisingly, the fine-tuned feature extractor does not forget but improves its ability to differentiate the absent class data. Here, we explain it by considering a two-layer linear neural network \(=_{c}_{c}^{}()=_{c}_{c}^{ }\). Let us denote by \(D=\{(_{i},y_{i})\}_{i=1}^{N}\) a mini-batch during SGD, and denote by \(_{_{i}}\) the gradient w.r.t. \(_{i}=_{i}\) using the cross-entropy loss. The gradient w.r.t. the feature extractor \(\) is thus \(_{}=_{i}(_{_{i}})_{i}^{}\). Suppose we apply SGD with a learning rate \(\), the updated feature extractor is \(-_{i}(_{_{i}})_{ i}^{}\). Building upon this formula, we can further

Figure 10. Classifier update direction similarity within fine-tuning (left) and absent (right) classes for ImageNet-S. The update directions are highly similar within absent classes, thus preserving the inter-class relationships among absent classes.

Figure 9. Different optimizers and hyperparameters. We use the Office-Home and report the AUSUC. The AUSUC of the pre-trained model is \( 0.5\). Fine-tuning with SGD shows remarkable robustness to different learning rates and weight decay. Advanced optimizers necessitate more careful hyperparameter selection. Nevertheless, they perform similarly to SGD under small learning rates and weight decay.

derive how the feature \(=\) of an absent class data \(\) changes after the model update

\[(-_{i}(_{_{i}})_{i}^{})=-_{i}_{_{i}}( _{i}^{}).\] (6)

Namely, the update of \(\) is governed by its similar training examples -- those \(_{i}\) with high inner products \(_{i}^{}\) with \(^{2}\) -- and their corresponding feature gradients \(_{_{i}}\). Suppose the domain shift affects similar classes similarly and the gradients w.r.t. the fine-tuning class data and features could effectively overcome the domain shift, Equation 6 offers a preliminary explanation of why fine-tuning could improve the absent class features in the downstream domain.

To further illustrate this, we design a toy example with four classes, visually depicted by different colors in Figure 11. Blue and cyan denote the fine-tuning class data; red and magenta denote the absent class data. The dimensionality of \(\) and \(\) are set to be \(2\); the size of \(\) is thus \(2 2\). We deliberately set \(\) to be non-negative to simulate the output of a ReLU operation. We create the pre-training dataset and the fine-tuning dataset by performing local translations to the data. We then pre-train a two-layer multi-layer perceptron (MLP) on the pre-training data with four classes while keeping the first layer's weight (_i.e._, \(\)) frozen as an identify matrix to ease visualization (_i.e._, \(=\)). After pre-training, we then fine-tune the model on the downstream fine-tuning data with only two classes without freezing \(\). (Please find more details about the data creation and the model architecture in Appendix A.) As shown in Figure 11, after fine-tuning, the update of the absent class features (from \(\) to \(\)) follows the update of their closest fine-tuning class features, even though the absent class data is not involved in fine-tuning. Moreover, different classes stay quite distinguishable in the fine-tuned feature space, suggesting that fine-tuning with a subset of classes would not degrade but improve the feature quality.

**More analysis.** Due to the page limit, we leave more analyses including frozen classifiers and frozen backbones in fine-tuning, the investigation of biased logits toward fine-tuning classes, and absent class relationship analysis in Appendix C.

## 7 Conclusion

"What happens if one fine-tunes a pre-trained classifier with a subset of classes?" Prior work showed that while it improves the fine-tuning class accuracy in the downstream domain, it drastically degrades the model's ability to recognize the other classes the model had previously learned. Our systematic study, however, provides a different opinion. We found that fine-tuning does not degrade but often improves the model's ability to recognize the other classes, _if the classifiers' logits are well-calibrated._ We expect our study to serve as a valuable reference for practical fine-tuning of pre-trained models.

Figure 11: The toy example demonstrates that the features of absent class data are influenced by their similar fine-tuning class training data, resulting in absent class features moving in a similar direction as fine-tuning class features.