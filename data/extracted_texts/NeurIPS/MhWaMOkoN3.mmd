# Universality in Transfer Learning for Linear Models

Reza Ghane

Department of Electrical Engineering

California Institute of Technology

Pasadena, CA 91125

rghanekh@caltech.edu

&Danil Akhtiamov

Department of Computing + Mathematical Sciences

California Institute of Technology

Pasadena, CA 91125

dakhtiam@caltech.edu

&Babak Hassibi

Department of Electrical Engineering

Department of Computing + Mathematical Sciences

California Institute of Technology

Pasadena, CA 91125

hassibi@caltech.edu

Equal Contribution

###### Abstract

We study the problem of transfer learning and fine-tuning in linear models for both regression and binary classification. In particular, we consider the use of stochastic gradient descent (SGD) on a linear model initialized with pretrained weights and using a small training data set from the target distribution. In the asymptotic regime of large models, we provide an exact and rigorous analysis and relate the generalization errors (in regression) and classification errors (in binary classification) for the pretrained and fine-tuned models. In particular, we give conditions under which the fine-tuned model outperforms the pretrained one. An important aspect of our work is that all the results are "universal", in the sense that they depend only on the first and second order statistics of the target distribution. They thus extend well beyond the standard Gaussian assumptions commonly made in the literature. Furthermore, our universality results extend beyond standard SGD training to the test error of a classification task trained using ridge regression.

## 1 Introduction

Deep neural networks have revolutionized the way data processing and statistical inference are conducted. Despite their ground-breaking performance, these models often require a plethora of training samples, which can make the process of data acquisition expensive. Moreover, with the advent of increasingly complex deep networks and especially Large Language Models (LLMs), the process of training from scratch has also become prohibitively expensive. To alleviate the scarcity of prepared data and the high training costs, various strategies have been proposed. One such method is fine-tuning in which a network previously trained on a source dataset/task different from the target dataset/task is leveraged as the initialization point for training on the target dataset/task. In many practical applications, especially for networks containing billions of parameters, only a subset of weights are updated to adapt the model to the new task. A particularly attractive method involves fine-tuning only the last layer of the network. A foundational question would be: How effective is this procedure? Are there fundamental limits to how much one can achieve by utilization of a model pretrained on a different distribution?In this work, we would like to investigate this problem rigorously through the lens of linear regression and binary classification in the overparametrized regime, where the number of weights/parameters exceeds the number of data points. To do so, we analyze the performance of regressors/classifiers obtained through performing SGD initialized at a weight \(_{0}^{d}\) acquired through training on the source domain. It was shown in Gunasekar et al. (2018) and Azizan and Hassibi (2018) that gradient descent (GD) and stochastic gradient descent(SGD) iterations converge to the optimal solution \(^{d}\) of the following optimization problem:

\[_{}\|-_{0}\|_{2}^{2}\] (1)

\[s.t=\]

where \(_{0}^{d}\) is the initialization point for SGD, \(^{n d}\) is the design/data matrix which is comprised of independent rows reflecting the fact that datapoints (\(_{i},y_{i}\)) are sampled independently, and \(^{n}\) is the vector of labels. This property of SGD is known as "Implicit Regularization" and will serve as the basis for our analysis as the pretraining step is captured by the vector \(_{0}\). Understanding this "interpolating regime" is key to the theoretical analysis of machine learning models as most of the deep neural networks, on account of their highly overparametrized nature, operate in a setting where they are able to attain negligible training error. It is noteworthy that, even for linear models, characterizing the exact performance of fine-tuning approach has been somewhat limited and is inhibited furthermore by the Gaussianity assumption on \(\). One of our main contributions is to overcome this limitation. In fact, to showcase the ubiquity of our results, we establish a general universality theorem that applies to a large class of data distributions and extends beyond the context of Transfer Learning. To go into more detail, we say that Gaussian universality holds for a data distribution \(\) and a training algorithm \(T\) if the test error obtained by training on data sampled from \(\) using \(T\) is the same as the test error obtained by training on data sampled from the Gaussian distribution \((_{},_{})\) using \(T\), where \(_{}\) and \(_{}\) stand for the mean and the covariance matrix of \(\) respectively. In the context of binary classification, we establish universality of the classification error with respect to the distribution of each class. In light of the the latter result, the problem reduces to analyzing the case of the Gaussian design matrices. We allow classes with different non-scalar covariance matrices \(_{1}\) and \(_{2}\) and build on the results of Akhtiamov et al. (2024) to address the problem in this specific scenario.

### Related Works

Transfer Learning has been an active topic of research at least since the 1970's Bozinovski (2020). It is mainly applied in the situations where obtaining data from the true distribution, which we will refer as the _target distribution_, is costly but there is a cheap way to access data from a _source distribution_, which bears resemblance to the target distribution. The two most popular approaches to transfer learning consist of instance-based transfer learning and fine-tuning.

Instance-based transfer learning incorporates the source dataset, along with the target dataset, and trains the model on this amalgamated dataset. The key insight is that, provided that the source distribution is close enough to the target distibution and a suitable training scheme is chosen, the final performance enjoys an improvement. We refer the reader to the landmark workDai et al. (2007) as well as to the comprehensive overviews covering the empirical advances in this area Tan et al. (2018); Zhuang et al. (2020). For theoretical analysis, we would like to emphasize a recent work Jain et al. (2024) that characterized the scaling laws for ridge regression over the union of the source and target data in the high dimensional regime.

The present manuscript focuses on fine-tuning in which a model is first pretrained on the source distribution and then fine-tuned using the target data. Dar et al. (2021) analyzes transfer learning for linear regression assuming Gaussianity of the data. The results obtained in this paper, in particular, imply that the generalization error obtained in their work is universal, in the sense that they can be immediately extended to a much broader set of target distributions, which we elaborate on in much greater detail in the main body of the paper.

Gerace et al. (2022) consider the problem of binary classification via a two-layer neural network in a synthetic setting. The source data and the source labels are sampled according to \(=Relu()\) and \(y=Sign(^{T})\) respectively where \(\), \(\) and \(\) are i.i.d. Gaussian. The target data and labels are generated by perturbing \(\) and \(\) and then applying the same rules as for the source data. Gerace et al. (2022) trained the network on the source dataset and then proceeded by only fine-tuning the last layer on the target domain using the cross-entropy loss with an \(_{2}\) regularizer. The analysis was conducted using the Replica method. The authors of Gerace et al. (2022) compared their results with an equivalent random feature model and observed empirically a certain kind of Gaussian universality for real-world datasets, such as MNIST.

Extension of results obtained under Gaussianity assumptions to non-Gaussian distributions remains a pertinent research direction in which the idea of Gaussian universality plays a central role. Montanari and Nguyen (2017) proved the universality of the generalization error for the elastic net, assuming the design matrix \(\) is i.i.d subgaussian. Panahi and Hassibi (2017) generalized the result to the problem of regularized regression with a quadratic loss and a convex separable regularizer which is either \(f()=\|\|_{1}\) (LASSO) or strongly convex. Han and Shen (2023) generalized Panahi and Hassibi (2017)'s results to non-separable Lipschitz test functions and provided non-asymptotic bounds for the concentration of solutions. In the random geometry literature, Oymak and Tropp (2018) showed universality of the embedding dimension of randomized dimension reduction linear maps with i.i.d entries satisfying certain moment conditions. Abbasi et al. (2019) proved universality of the recovery threshold for minimizing strongly convex functions under linear measurements constraint, assuming the rows are iid and the norm of the mean is asymptotically negligible compared to the noise. Lahiry and Sur (2023), proved the universality of generalization error for ridge regression and LASSO when the rows are distributed iid with a specific block structure dependence per row, \(\) where \(\) has zero mean iid random subgaussian vectors per row and \(\) being diagonal.

Leveraging universality is not limited to the signal recovery literature. As an example, Hu and Lu (2022), Bosch et al. (2023), Schroder et al. (2023), have analyzed the performance of the random feature models by replacing nonlinear functions of Gaussians with the corresponding Gaussian distribution having the same mean and covariance in single-layer and multiple-layer scenarios, respectively, through a universality argument, referred to as the Gaussian equivalence principle. In a more general setting, under subgaussianity assumption on the data, Montanari and Saeed (2022) proved the universality of the training error for general loss and regularizer and test error when the regularizer is strongly convex and the loss is convex. Hastie et al. (2022) proved universality for the the minimal \(_{2}\)-norm linear interpolators for the data generated via a very specific rule \(=(^{})\), where \(\) is i.i.d zero mean and satisfies several other technical properties and \(\) is a deterministic PSD matrix. Dandi et al. (2024) considered the universality of mixture distributions in a similar setting to Montanari and Saeed (2022) and proved the universality of the free energy of the test error.

### Outline of the Paper

In Section 2.1, we formally define the optimization problem for which we aim to establish universality. Section 2.2 outlines our primary contributions. In Section 3, we present our main universality result, accompanied by several insightful remarks. Section 4 focuses on our findings related to fine-tuning in the contexts of linear regression and binary classification. In Section 5, we validate our theoretical results through empirical experiments. Finally, we conclude with a summary and discussion in Section 6.

## 2 Problem Formulation

### The Setting

We use bold letters for vectors and matrices. We denote the \(i\)'th largest singular value of matrix \(\) by \(s_{i}()\). We consider the proportional regime where \(d=(n)\) and \(>1\). We refer to Section A.1 for the other notations and definitions necessary for the rest of this exposition.

Given a training dataset \(\{(_{i},y_{i})\}_{i=1}^{n}\) and a model with a fixed architecture, the conventional approach of learning the weights for the model consists of choosing a loss function \(\) and finding \(w\) minimizing \(_{i=1}^{n}(_{i},_{i},y_{i})\). In this exposition we focus on linear models and loss functions of the form \((_{i},_{i},y_{i})=(y_{i}-_{i}^{T} _{i})\), where \(\) is differentiable and \((0)=0\). Azizan and Hassibi (2018) characterized behavior of a broad family of optimizers, called _stochastic mirror descent_ (SMD) algorithms, for linear models in the over-parametrized regime. For a strictly convex function \(g:^{d}\), the update rule of the SMD with a _mirror_\(g\) and a learning rate \(>0\) is defined as

\[ g(_{t})= g(_{t-1})-_{t} (_{t-1}), t 1,\] (2)where \(>0\) is the learning rate and \(_{t}\) is the loss function \(\) evaluated at a point chosen at random in the dataset corresponding to the \(t\)'th iteration. Due to strict convexity, \( g()\) defines an invertible transformation, which is why (2) is indeed a well-defined update rule. Note also that this includes SGD as a special case, which corresponds to taking \(g()=\|\|_{2}^{2}\). Azizan and Hassibi (2018) show that applying SMD initialized at \(_{0}\) to minimize \(_{i=1}^{n}(y_{i}-_{i}^{T})\) yields a weight vector defined by the following optimization problem:

\[_{}D_{g}(,_{0})\] \[s.t y_{i}=_{i}^{T}, 1 i n\]

We construct the random matrix \(^{n d}\) by setting its rows equal to \(_{i}\). Hence, stating in a more general fashion, we are interested in the analysis of the following optimization problem, with the main case of interest being when \(f\) is a quadratic:

\[_{}f()\] \[s.t=\]

As the objective is comprised of minimizing a strongly convex function \(f\) over a closed convex set, it has a unique minimizer. By using a Lagrange multiplier \(\), this convex optimization problem can be written in the unconstrained form:

\[():=_{>0}_{w}\| -\|_{2}^{2}+f()\] (3)

Considering the objective without the \(_{>0}\) yields the following regularized linear regression problem for which we will also establish a universality theorem:

\[_{}():=_{w}\|- \|_{2}^{2}+f()\] (4)

Note that (4) captures the case of explicit regularization, which is of highest importance whenever there is a high amount of noise or label corruption present. Furthermore, by deliberately choosing the regularizer \(f\), it is possible to obtain solutions that exhibit certain behavior, viz being sparse or compressed. In order to tackle optimizations such as 3, 4, we prove their equivalence to a problem with a suitable Gaussian design \(\) which is described in Definition 4 in Section A.1. Our approach for proving universality will be the Lindeberg approach Lindeberg (1922); Chatterjee (2006).

### Our Contributions

#### 2.2.1 Transfer Learning

**Linear Regression** We extend the results from Dar et al. (2021) by providing precise expressions for the generalization error in linear regression tasks. In addition, we demonstrate that the test error is always lower-bounded by a quantity that is attained only when the covariance matrix of the data is scalar. Furthermore, we identify specific conditions under which fine-tuning is successful.

**Binary Classification** We present the first precise characterization of the classification error for linear models trained using stochastic gradient descent (SGD) on data drawn from a general mixture distribution with arbitrary covariance matrices. Moreover, we delineate the regimes in which fine-tuning proves effective.

#### 2.2.2 Universality

**Proof of universality for implicit regularization.** In the context of SGD and, more generally, SMD (see 2) with a mirror \(f\) satisfying the assumptions of Theorem 1, we prove universality of the generalization error as well as of the value of the corresponding implicit regularization objective for a wide range of data distributions characterized by Assumptions 1. Note that this cannot be reduced to any known results on universality of constrained objectives as the latter assume that the constraints are deterministic, i.e the optimization variables belong to some deterministic set \(\) characterized by the constraints. While to analyze SMD one has to deal with constraints of the form \(=\) which represents a random polytope. To the best of our knowledge, the only other paper dealing with universality in the context of implicit regularization is Hastie et al. (2022). They study minimal \(_{2}\)-norm linear interpolators, but they work with a very specific (random feature) model for data distributions defined by \(=(^{})\), where \(\) is i.i.d. zero mean and \(\) is i.i.d. Gaussian. Our paper generalizes theirs in two non-trivial ways, as it allows for arbitrary smooth convex strongly convex mirrors \(f()\) as well as more general data distributions.

**Relaxing assumptions for explicit regularization.** To the best of the authors' knowledge, all previous results on universality assumed either that the data points \(^{d}\) have i.i.d. entries or that the rows are independent and \(\) is subgaussian. We relax these assumptions. That is, we show that, for the quadratic loss function and any strongly convex not necessarily differentiable regularizer universality holds as long as the rows are i.i.d., all moments of the \(\) up to the 6th satisfy \(_{}|(-)^{T}|^{q}  K\|_{2}^{q}}{d^{q/2}}\) for \(^{d}\) and \(Var(^{T}) 0\) for any \(\) of bounded operator norm.

**Extending universality to mixtures of distributions.** Since the main motivation behind this work is the study of generalization in classification tasks, we focus on data matrices sampled from mixture distributions. Note that most previous papers on universality, such as Montanari and Saeed (2022), do not apply to this setting directly. As an illustration to why results of Montanari and Saeed (2022) cannot just be applied to the mixture distributions directly, consider \(=(,)+(-,)\) corresponding to a mixture of two classes with antipodal means and the resulting classification error depends on \(\|\|\), while \(\) has mean \(\) and covariance \(\) meaning that the matching gaussian distribution \((0,)\) does not contain any information about the classes.

However, there is one prior paper Dandi et al. (2024) that studies universality specifically in the context of mixture distributions. It is worth mentioning that their definition of universality is different. Namely, they prove universality of the expectation with respect to the Gibbs distribution, which suffices to show universality of the train but not of the test error.

**Allowing for non-vanishing means**. In Definition 2, we assume only that \(_{}\|-\|_{2}^{2}=O(1)\), whereas in Abbasi et al. (2019), Montanari and Saeed (2022) and Dandi et al. (2024) they have \(\|_{2}^{2}}{_{}\|-\|_{ 2}^{2}} 0\), implying that the norms of the means are negligible compared to the norms of the data points on average. To our knowledge, our work is the first to tackle it.

## 3 Main Result

In this section, we present our main universality theorem and before doing so, we list the required technical assumptions. Consider the following convex optimization problem

\[_{}():=_{w}\|-\|_{2}^{2}+f()\] (5)

Denote the solution to the optimization problem 5 above by \(_{_{}()}\). Then we assume the following:

**Assumptions 1**.:
1. \(\) _is a regular block matrix and_ \(\) _is its matching Gaussian matrix as in Section_ A.1_._
2. _The regularizer_ \(f()\) _is regular also as Section_ A.1 _or there exists a sequence of regular_ \(f_{m}\)_'s converging uniformly to_ \(f\)_._
3. \(f()\) _is_ \(M\)_-strongly convex._
4. \(_{}\|\|_{2}^{2}=O(d)\) _with_ \(|y_{f}|^{q}=O(1)\) _for_ \(j[n]\) _and_ \(q\) _and is independent of_ \(\)_._
5. \(s_{}(^{T})=(1)\) _with high probability,_
6. \(Var(\|\|_{2}^{2}) 0\)__
7. \(\| f(_{_{}()})\|_{2} K_{f}\) _for a constant_ \(K_{f}\) _independent of_ \(\)_._

**Theorem 1**.: _If \(==_{ 0}_{}\), suppose parts (1) - (7) of Assumptions 1 hold and for \(=_{}\), only assume parts (1) - (4) of Assumptions 1. Then_

1. _(Universality of the training error)_ _For any_ \(L\)_-Lipschitz_ \(g:^{d}\)_, and every_ \(t_{1}>0,t_{2}>t_{1},c\) _the following holds:_ * _If_ \((|g(())-c|>t_{1}) 0\) _then_ \((|g(())-c|>t_{2}) 0\)_._* _Furthermore, if_ \(g\) _has bounded second derivative, then_ \[_{n}_{,}g(())- _{,}g(())=0\]
2. _(Universality of the solution) For a function_ \(\) _with_ \(^{2} g\) _if either_ \(\) _is regular (Definition_ 1_) or there is a sequence of regular functions converging uniformly to_ \(\)_,_ * _If for every_ \(t>0\)_,_ \((|()-c|>t) 0\) _then for_ \(\{,\}\)_, there exists_ \(c_{1}\) _such that for any_ \(t_{1}>0\) _we have_ \((|(_{()})-c_{1}|>t_{1}) 0\)_._ * _Furthermore, if_ \(\) _is bounded_ \[_{n}_{,}(_{ ()})-_{,}(_{( )})=0\]

To sum up, Theorem 1 says that the vector of weights \(_{}\) trained on data sampled from a non-gaussian distribution \(\) either via SMD with a mirror \(f\) or by minimizing the least squares objective with regularizer \(f\), shares many similar characteristics with the vector of weights \(_{}\) trained on data sampled from the matching GMM \(\) via the same optimization procedure under certain technical assumptions on \(\) and \(f\). By "similar characteristics", we mean that for any \(:^{d}\) with bounded Hessian, \((_{})=(_{})\) holds in the limit.

We would like to point out a few remarks.

**Remark 1**.: _Note that \(\) need not be convex, in fact any linear combination of regular \(\) is also regular. An immediate result is the universality of the empirical distribution of the coordinates of solutions._

**Remark 2**.: _The assumption \(\| f(_{_{}()})\|_{2}=O()\) in Theorem 1 is satisfied for any function \(f\) with a locally lipschitz gradient, i.e \(\| f()\|_{2} K(1+\|\|_{2})\). In particular it applies to \(f()=\|\|_{2}^{2}\)._

**Remark 3**.: _In both parts of Theorem 1, the regularizer is allowed to be an \(M\)-strongly convex uniform limit of differentiable convex functions, which implies that \(f()=t\|\|_{1}+M\|\|_{2}^{2},t>0,M>0,\) known as the elastic net, is also included in our results._

**Remark 4**.: _In Theorem 1, the optimal value of \(_{}\) is only achieved when \(\) and is not attained at any finite \(\). This means that proving the results for \(=\) in Theorem 1 requires additional ideas apart from the case \(=_{}\) in Theorem 1, as the latter makes use of the boundedness of \(\) extensively. To tackle this issue, we present a uniform convergence result in \(\) which might be of independent interest._

**Remark 5**.: _The condition \(s_{}(^{T})=(1)\) can be satisfied for a variety of random matrix models. If for each block of \(\), \(_{i}=}_{i}_{i}^{1}\) where \(}_{i}\) has iid entries and \(s_{}(_{i})=(1)\), then by the Bai-Yin's law Bai and Yin  the condition is satisfied. The second family is comprised of blocks with independent and identical rows where the norm of rows have exponential concentration. We prove this in the Appendix. One particular instance will be the distributions satsifying LCP from Definition 3._

**Remark 6**.: _The second assumption in Assumptions 1 is not too restrictive and is in particular satisfied for \(()=(^{T}>c)\). We have with high probability for \(\{_{},\}\)_

\[_{n}(^{T}_{( )}>c)-(^{T}_{()}>c)=0\]

_For \(\) independent of \(,\) but sampled from the same distribution as the rows of \(\), respectively. Note that by this argument we have reduced the problem of verifying CLT for \(^{T}_{()}\) to its Gaussian counterpart, \(^{T}_{()}\). Then, specifically for the applications presented in this paper, we provide a proof that \(^{T}_{()}\) satisfies CLT w.r.t. to randomness in \(\) with high probability in \(\). However, in general the latter CLT condition has to be verified on a case by case basis._

## 4 Applications: Transfer Learning

### Regression

We consider the classical problem of recovering the best linear regressor for the following linear model

\[=_{*}+\] (6)Here, \(_{*}\) is the ground truth, the rows of the data matrix \(\) are i.i.d with \(_{i}=\) and \(_{i}_{i}^{T}=:_{x}\), \(\) is centered and is independent of \(\) and satisfies \(_{}\|\|_{2}^{2}=^{2}n\). For a given \(}\), its generalization error is defined by:

\[e_{gen}(}):=_{x}(^{T}_{*}- ^{T}})^{2}=(}-_{*})^{T} _{x}(}-_{*})\]

Now in order to recover \(_{*}\) given observations \((,)\), we choose to optimize the least squares objective, \(_{}\|-\|_{2}^{2}\), and to do so, we leverage SGD. We would like to investigate how useful having a pretrained classifier \(_{0}\) can be for the recovery of \(_{*}\). As pointed out earlier, SGD initialized from \(=_{0}\), by its implicit regularization property, converges to the solution \(}\) of (1). We denote \(e_{a}:=e_{gen}(_{0})\); hence the name "a priori error", \(e_{a}\). We provide the assumptions necessary for our results on linear regression in Section A.2. In what follows we give a precise characterization of the posterior error, \(e_{p}\), of the model after training with SGD, in terms of \(e_{a}\) and the other parameters of the problem. In order to leverage Theorem 1 to establish the universality of the generalization error, it is sufficient to apply a change of variable \(^{}:=_{x}^{-1/2}\) and use \(()=\|\|_{2}^{2}\) as the test function.

**Theorem 2**.: _Under Assumptions 3 in Section A.2, the generalization error of the SGD solution initialized from \(_{0}\) converges in probability to_

\[e_{p}=^{2}+e_ {a}\] (7)

_With \(t=}dr\) where \(\) is found through \(=S_{p}(-)\) (12). And \(>1\) is the proportional constant, i.e for \(^{n d}\), \(\). Moreover, for any distribution \(p(r)\) we have_

\[e_{p}^{2}+e_{a}\] (8)

_The lower bound is attained if and only if \(p(r)=(r-r_{0})\) for some \(r_{0}>0\)_

The following remark is immediate:

**Remark 7**.: _Theorem 2 entails that, depending on the noise level present in the data, training could be even potentially harmful. Indeed, if \(^{2} e_{a}\), then \(e_{p}^{2}+e_{a}( 2}-) e_{a}\) for any \(>1\) and it is therefore more appropriate to use vector \(_{0}\) instead of performing fine-tuning. Moreover, if the covariance \(_{}\) is scalar, then the converse is true. Namely, if \(e_{a}^{2}\), then the best achievable error corresponds to \(_{*}=}}{}-}\) and equals \((2}-)\), which is less than \(e_{a}\). Therefore, transfer learning contributes to improving the test performance when the variance of the noise is not too high, but the model has to be fine-tuned on the correct amount of target data._

### Classification

#### 4.2.1 Problem setting

Consider a binary classification task and let \(\) stand for the data matrix and \(\) denote the vector of labels where each \(y_{i}= 1\) depending on what class the \(i\)-th point \(_{i}\) falls into. We denote \(_{}:=_{i}\) and \(_{}:=_{i}_{i}^{T}-_{} _{}^{T}\), for the mean and covariance of each datapoint \(_{i}\), respectively and \(\{1,2\}\) depending on the class of \(_{i}\). After learning a linear classifier \(\), we assign labels to new previously unseen points according to \(_{new}=sign(^{T}_{new})\). Without loss of generality we will assume that the first \(\) rows of \(\) are sampled from the first class and the remaining rows are sampled from the second as we can permute the rows otherwise. Modulo such permutation, it is straightforward to see that \(\) satisfies parts (1) - (3) of Definition 2. Since the topic of the present paper is fine-tuning, we assume that the following steps are performed:

1. Obtain a pre-trained classifier \(_{0}\)
2. Renormalize \(_{0}\) obtained during the previous step using the target data via \[_{}\|-_{0}\|^{2}\] This yields: \[=^{T}}{\|\|^{2}}\] (9)After finding \(\) take \(_{0}^{}:=_{0}\). This transform preserves the direction of \(_{0}\), while setting its squared magnitude to \(^{2}\|_{0}\|^{2}=_{0})^{2}\| _{0}\|^{2}}{\|_{0}\|^{4}}\), which depends only on the direction of \(_{0}\) but not on its magnitude anymore. We find applying this transform very meaningful, as it does not change the classification error for the source data but simplifies learning for the regression problem defined by the target data.
3. Learn the final classifier from the target data \(^{n d}\) and labels \(^{n}\) using SGD initialized at \(_{0}^{}\), obtained from the previous step.

We will use the assumptions in Section A.3 to define further details of the classification task we consider. Note that in practice the main difficulty of the classification task in the over-parametrized regime (\(n<d\)) arises due to the fact that \(_{1},_{2},_{1}\) and \(_{2}\) are not known and cannot be estimated reliably. Nevertheless, we would like to start with characterizing the optimal performance in the scenario where these are provided to us by an oracle. Lemma 1 in Section A.3 provides such a characterization under certain symmetry assumptions. In view of Lemma 1 it is natural to introduce the following assumption:

**Assumptions 2**.: _The initialization point \(_{0}\) satisfies_

\[_{0}=t_{*}_{*}+t_{}\] (10)

_where \(_{*}=(_{1}+_{2})^{-1}(_{1}-_{ 2})\) is the optimal classifier defined as in Lemma 1. \(\) is a noise vector with \(\|\|^{2}=1-r\) and for any deterministic matrix \(\) of bounded operator norm it holds that \(^{T}\) converges in probability to \(}()(1-r)}{d}\). Note that the smaller the ratio \(}}{t_{*}}\) is, the better performance \(_{0}\) has._

Indeed, 10 captures the closeness of the initilization point \(_{0}\) to the optimal classifier \(_{*}\). For example, in the isotropic case \(_{1}=_{2}=^{2}}\), if \(_{0}\) has the classification error \(_{a}\), this means that we should take \(}}{t_{*}}=}Q^{-1}(_{a})^{2}}-2}\). Finally, we would like to remark that, under notation and assumptions from Theorem 1, the following corollary is implied by Theorem 1 modulo Theorem 4 presented in the Appendix along with the explanation of the implication:

**Corollary 1** (Universality of the classification error for SGD and ridge regression objectives).: _If \(f()=\|-_{0}\|_{2}^{2}\), when \(\|_{0}\|_{2}^{2}=O(d)\) then for \(,\) independent of \(,\) but sampled from the same distribution as their rows, respectively, we have with high probability for \(\{_{},\}\)_

\[_{n}(^{T}_{( )}>0)-(^{T}_{()}>0)=0\]

#### 4.2.2 Analysis of the Classification Error

**Theorem 3**.: _Let \(_{0}\) be an initialization point that satisfies Assumption 2 and \(\) be a data matrix satisfying Assumptions 4 in Section A.3. Then the classification error of the SGD solution initialized at \(_{0}\) for \(\) defined by (9) and trained on \(,\) is given by \(e_{p}=Q(-}{}}})\) where \(\) and \(\) are determined in terms of a \(\) solving \(}{}_{_{1}+_{2}}(- }{})=1-\) whose expressions are provided in the Appendix (cf. Section I, equation 40) with \(>1\)._

The expressions from Theorem 3 can be simplified drastically in the case of scalar covariance matrices.

**Corollary 2**.: _Under the notation from Theorem 3, assume \(_{1}=_{2}=_{d}}{d}\) and define \(:=}\). Let \(e_{a}\) be the classification error of \(_{0}\) initialized according to 10. Then_

\[e_{p}(,)}Q((e_{a})^{2 }(2+-2)+)}{(e_{a})^{4}+Q^{-1}(e_{a})^{2}(2^{3}+^{2}- 2(-1)+)+2^{2})}})\] (11)

We arrive at the following conclusion summarizing the derivations above.

**Remark 8**.:
* _If_ \(=(1)\)_, then_ \(e_{p}(,)}}{{}}Q((Q^{-1 }(e_{a})+(e_{a})})})<e _{a}\) _and fine-tuning always succeeds. It is observed that for_ \(e_{a}>Q(1)\)_, the worse_ \(e_{a}\) _is, the better_ \(e_{p}\) _will be._
* _If_ \(=(1)\)_, then the fine-tuning step may or may not succeed depending on whether_ \(e_{p}(,)<e_{a}\) _or not. See Section_ 5.2 _for further (empirical) explorations on the usefulness of the fine-tuning of the pre-trained solution for this regime._
* _If_ \(=o(1)\)_, then the classification error of any linear classifier goes to_ \(\) _as_ \(n\) _since it is lower bounded by_ \(Q(})\) _according to Lemma_ 1_. Thus, any kind of learning will fail in this regime._

An interesting regime is where the number of samples is much lower than the number of parameters, which naturally rises in the context of fine-tuning large models and corresponds to letting \( 1\). For this regime, we have

\[e_{p} Q(Q^{-1}(e_{a})+(e_{a })}-3Q^{-1}(e_{a}))\]

We observe that if \(<1\), transfer learning always fails independent of value of \(e_{a}\) for \( 1\) and training would not help with improving performance. On the other hand, if \(e_{a}>Q(})>Q(})\), for large enough \(\), transfer learning always achieves an error less than \(e_{a}\).

## 5 Numerical experiments

### Regression

To corroborate our findings, we plotted the generalization error of the weight obtained through running SGD according to the Assumptions 3 in Section 4.1 with respect to \(=}\). To do so, we fixed \(d=1000\) and varied \(n\) across different values. We used CVXPY (Grant and Boyd (2014); Agrawal et al. (2018)) to solve (1) efficiently on a Laptop CPU. To verify the universality of our results, we initially constructed a centered random matrix \(^{}\) with i.i.d components according to the distributions \((0,1)\), \(Ber(0.5)\), and \(^{2}(1)\) and using a correlation matrix \(_{x}\) we defined \(:=^{}_{x}^{1/2}\). On the other hand, we generated \(_{x}\) according to the following three distributions: single level \(p(r):=(r-1)\), bilevel \(p(r):=0.3(r-1)+0.7(r-5)\) and uniform on the interval \(\). We specifically consider these cases as they are common in the literature and we use the parameter \(^{2}\) for the component-wise variance of \(\) in 6. Additionally, \(_{0}\) is chosen according to Assumptions 3 in such a manner that \(e_{a}=1\). In both Figures 1, 2 the blue line represents the prediction 7 made by Theorem 2, the red line depicts the lower bound 8. The markers showcase the performance of weights obtained under different distributions as described earlier. It can be seen that from Figures 1, 2, for the bilevel and uniform distributions, depending on the value of \(\), transfer learning might not be beneficial as discussed in Remark 7. In particular, in Figures 0(c) and 0(c), the generalization error is always lower bounded by \(e_{a}=1\) and only in \(\) can get close to 1. Finally, the single-level distribution on \(_{x}\) is always a lower bound for the generalization error of various distributions on \(_{x}\).

### Classification

Similar to the preceding subsection, we experimented with sampling the entries of \(\) independently from three different centered distributions: normal, Bernoulli, and \(^{2}\). We also sampled the means \(_{1}\) and \(_{2}\) from \((0,_{d})\) with a cross-correlation \(r=[_{1i}_{2i}]=0.9\) and added them to the corresponding rows of \(\). For Figures 3, we fixed \(=0.8,2,5\) respectively and plotted the classification error predicted by Corollary 2 as a solid red line, empirically observed classification errors for the normal, Bernoulli and \(^{2}\) entries as black squares, green circles and red triangles respectively. The blue lines depict the classification error at the initialization. It can be seen that there is a close match between the empirical errors between points from different distributions as well as with the theoretical prediction, thus validating both Theorem 1 and Theorem 3. Note that fixing \(\)in this setting corresponds to fixing \(^{2}\) as \(=}\). It is also worth mentioning that fine-tuning improves performance in the setting of Figure 2(c) but hurts it for Figure 2(a). Moreover, note that in Figure 2(b), although for smaller \(\) transfer learning hurts, past a certain \(\), training improves the performance. Also we observe that by increasing \(\) across the three plots, the classification task becomes easier and fine-tuning improves performance as supported by Remark 8.

## 6 Conclusion and future work

We presented a novel Gaussian universality result and used it to study the problem of transfer learning in linear models, for both regression and binary classification. In particular, we were able to precisely relate the performance of the pretrained model to that of the fine-tuned model trained via SGD and, as a result, identified situations where transfer learning helps and where it or does not. Possible future directions include investigating other problems where the universality result may be useful, extending the results to potential functions that are not necessarily convex nor separable, as well as exploring the implications of universality for objectives with explicit regularization.

Figure 1: Generalization error for the bilevel distribution on the covariance of the data

Figure 3: Classification error

Figure 2: Generalization error for the uniform distribution on the covariance of the data