# Approximation-Generalization Trade-offs under (Approximate) Group Equivariance

Mircea Petrache

UC Chile, Fac. de Matematicas, & Inst. de Ingenieria Matematica y Computacional, Av. Vicuna Mackenna 4860, Santiago, 6904441, Chile. mpetrache@mat.uc.cl.

Shubhendu Trivedi

shubhendu@csail.mit.edu.

###### Abstract

The explicit incorporation of task-specific inductive biases through symmetry has emerged as a general design precept in the development of high-performance machine learning models. For example, group equivariant neural networks have demonstrated impressive performance across various domains and applications such as protein and drug design. A prevalent intuition about such models is that the integration of relevant symmetry results in enhanced generalization. Moreover, it is posited that when the data and/or the model may only exhibit _approximate_ or _partial_ symmetry, the optimal or best-performing model is one where the model symmetry aligns with the data symmetry. In this paper, we conduct a formal unified investigation of these intuitions. To begin, we present general quantitative bounds that demonstrate how models capturing task-specific symmetries lead to improved generalization. In fact, our results do not require the transformations to be finite or even form a group and can work with partial or approximate equivariance. Utilizing this quantification, we examine the more general question of model mis-specification i.e. when the model symmetries don't align with the data symmetries. We establish, for a given symmetry group, a quantitative comparison between the approximate/partial equivariance of the model and that of the data distribution, precisely connecting model equivariance error and data equivariance error. Our result delineates conditions under which the model equivariance error is optimal, thereby yielding the best-performing model for the given task and data. Our results are the most general results of their type in the literature.

## 1 Introduction

It is a common intuition that machine learning models that explicitly incorporate task-specific symmetries tend to exhibit superior generalization. The rationale is simple: if aspects of the task remain unchanged or change predictably when subject to certain transformations, then, a model without knowledge of them would require additional training samples to learn to disregard them. While the general idea is by no means new in machine learning, it has experienced a revival in interest due to the remarkable successes of group equivariant convolutional neural networks (GCNNs) . Such networks hard-code equivariance to the action of an algebraic group on the inputs and have shown promise in a variety of domains , in particular in those where data is scarce, but exhibits clear symmetries. Further, the benefits of encoding symmetries in this manner extend beyond merely enhancing generalization. Indeed, innumerous applications in the physical sciences, neglecting to do so could lead to models producing unphysical outputs. However, the primary focus of most of the work in the area has been on estimating functions that are _strictly_ equivariant under a group action. It has often been noted that this may impose an overly stringent constraint as real-world data seldom exhibits perfect symmetry. The focus on strict equivariance is partly due to convenience: the well-developed machinery of group representation theory and noncommutative harmonic analysis can easily be marshalled to design models that are exactly equivariant. A general prescriptive theory for such models also exists . In a certain sense, the basic architectural considerations of such models are fully characterized.

Recent research has begun to explore models that enforce only partial or approximate equivariance . Some of these works suggest interpolating between models that are _exactly_ equivariant and those that are _fully flexible_, depending on the task and data. The motivation here is analogous to what we stated at the onset. If the data has a certain symmetry - whether exact, approximate or partial - and the model's symmetry does not match with it, its performance will suffer. We would expect that a model will perform best when its symmetry is correctly specified, that is, it aligns with the data symmetry.

Despite increased research interest in the area, the theoretical understanding of these intuitions remains unsatisfactory. Some recent work has started to study improved generalization for exactly equivariant/invariant models (see section 2). However, for the more general case when the data and/or model symmetries are only approximate or partial, a general treatment is completely missing. In this paper, we take a step towards addressing this gap. First, we show that a model enforcing task-specific invariance/equivariance exhibits better generalization. Our theoretical results subsume many existing results of a similar nature. Then we consider the question of model mis-specification under partial/approximate symmetries of the model and the data and prove a general result that formalizes and validates the intuition we articulated above.

We summarize below the main contributions of the paper:

* We present quantitative bounds, the most general to date, showing that machine learning models enforcing task-pertinent symmetries in an equivariant manner afford better generalization. In fact, our results do not require the set of transformations to be finite or even be a group. While our investigation was initially motivated by GCNNs, our results are not specific to them.
* Using the above quantification, we examine the more general setting when the data and/or model symmetries are _approximate_ or _partial_ and the model might be _misspecified_ relative to the data symmetry. We rigorously formulate the relationship between model equivariance error and data equivariance error, teasing out the precise conditions when the model equivariance error is optimal, that is, provides the best generalization for given data. To the best of our knowledge, this is the most general result of its type in the literature.

## 2 Related Work

As noted earlier, the use of symmetry to encode inductive biases is not a new idea in machine learning and is not particular to neural networks. One of the earliest explorations of this idea in the context of neural networks appears to be . Interestingly,  originated in response to the group invariance theorems in Minsky & Papert's _Perceptrons_, and was the first paper in what later became a research program carried out by Shawe-Taylor & Wood, building more general symmetries into neural networks , which also included PAC style analysis . While Shawe-Taylor & Wood came from a different perspective and already covered discrete groups , modern GCNNs go beyond their work . Besides, GCNNs  were originally proposed as generalizing the idea of the classical convolutional network , and seem to have been inspired by symmetry considerations from physics . Outside neural networks, invariant/equivariant methods have also been proposed in the context of support vector machines , general kernel methods , and polynomial-based feature spaces .

On the theoretical side, the assertion that invariances enhance generalization can be found in many works, going back to . It was argued by  that restricting a classifier to be invariant can not increase its VC dimension. Other examples include [3; 4; 51; 41]. The argument in these, particularly [51; 41] first characterizes the input space and then proceeds to demonstrate that certain data transformations shrink the input space for invariant models, simplifying the input and improving generalization. Some of our results generalize their results to the equivariant case and for more general transformations.

Taking a somewhat different tack,  showed a strict generalization benefit for equivariant linear models, showing that the generalization gap between a least squares model and its equivariant counterpart depends on the dimension of the space of anti-symmetric linear maps. This result was adapted to the kernel setting in .  studied sample complexity benefits of invariance in a kernel ridge regression setting under a geometric stability assumption, and briefly discussed going beyond group equivariance.  expands upon  by characterizing minimax optimal rates for the convergence of population risk in a similar setting when the data resides on certain manifolds.  characterizes the benefits of invariance in overparameterized linear models, working with invariant random features projected onto high-dimensional polynomials. Benefits of related notions like data augmentation and invariant averaging are formally shown in [33; 12]. Using a standard method from the PAC-Bayesian literature  and working on the intertwiner space,  provide a PAC-Bayesian bound for equivariant neural networks. Some improved generalization bounds for transformation-invariant functions are proposed in , using the notion of an induced covering number. A work somewhat closer to some of our contributions in this paper is , which gives PAC bounds for (exact) group equivariant models. However, we note that the proof of the main theorem in  has an error (see the Appendix for a discussion), although the error seems to be corrected in the author's dissertation . An analysis that somewhat overlaps , but goes beyond simply providing worst-case bounds was carried out by .

Finally, recent empirical work on approximate and partial equivariance includes [40; 59; 23; 56]. These make the case that enforcing strict equivariance, if misspecified, can harm performance, arguing for more flexible models that can learn the suitable degree of equivariance from data. However, there is no theoretical work that formalizes the underlying intuition.

## 3 Preliminaries

In this section, we introduce basic notation and formalize some key notions that will be crucial to ground the rest of our exposition.

### Learning with equivariant models

**Learning problem.** To begin, we first describe the general learning problem which we adapt to the equivariant case. Let's say we have an input space \(\) and an output space \(\), and assume that the pairs \(Z=(X,Y)\) are random variables having distribution \(\). Suppose we observe a sequence of \(n\) i.i.d pairs \(Z_{i}=(X_{i},Y_{i})\), and want to learn a function \(:\) that predicts \(Y\) given some \(X\). We will consider that \(\) belongs to a class of functions \(}\{:\}\) and that we work with a loss function \(:[0,)\). To fully specify the learning problem we also need a _loss class_, comprising of the functions \(f(x,y):=((x),y):[0,)\). Using these notions, we can define:

\[:=\{f(x,y):=((x),y):}\}, Pf:=[f(X,Y)], P_{n}f:=_{i=1 }^{n}f(X_{i},Y_{i}).\]

The quantities \(Pf\) and \(P_{n}f\) are known as the risk and the empirical risk associated with \(\), respectively. In practice we have access to \(P_{n}f\), and we estimate \(Pf\) by it. One of the chief quantities of interest in such a setup is the worst-case error of this approximation on a sample \(\{Z_{i}\}=\{Z_{i}\}_{i=1}^{n}\), that is, the generalization error:

\[(,\{Z_{i}\},):=_{f}( Pf-P_{n}f).\]

**Group actions.** We are interested in the setting where a group \(G\) acts by measurable bijective transformations over \(\) and \(\), which can also be seen as a \(G\)-action over \(Z\), denotedby \(g Z=(g X,g Y)\), where \(g G\). Such actions can also be applied to points \((x,(x))\) in the graph of functions \(\), transforming any \(}\) into \(g:x g^{-1}(g x)\). The set \(G:=\{g:\ g G\}\) forms the orbit of \(\) under the action of \(G\). Note that although we treat \(G\) as an abstract group throughout, we will sometimes abuse notation and use the same letter to also refer to the representations of \(G\) as transformations over the spaces in which \(X,Y,Z,,f\) live.

Equivariant models.We are now in a position to specify the learning setup with equivariant models. We say that \(}\) is equivariant, if \(G=\{\}\). Further, we say that the hypothesis class is \(G\)-equivariant if all \(}\) are equivariant. Note that if the \(G\)-action over \(Y\) is trivial, that is, if we have \(g Y=Y\) for all \(g G,y Y\)), then \(\) being equivariant reduces to requiring \((g x)=(x)\) for all \(g,x\). In such a case the model \(\) is said to be **invariant**. Also note that \((x)\) being equivariant corresponds to \(f(x,y)=((x),y)\) being invariant under some suitable product \(G\)-action over \(\).

Averaging over a set of transformations.Let \(G\) be a set of transformations of \(\). The \(G\)-averaged generalization error for arbitrary \(\) by taking averages \(_{g}\) with respect to the natural probability measure \(_{G}\)3 on \(G\) is as follows:

\[^{(G)}(,\{Z_{i}\},):=_{f}(_{g}_{Z}[f(g Z)]-_{i=1}^{n} _{g}f(g Z_{i})),\]

Note that the above equals \((,\{Z_{i}\},)\) if \(\) is composed of invariant functions.

Data augmentation/Orbit averaging.Following our discussion above, it might be instructive to consider the case of data augmentation. Note that a data distribution which is invariant under some \(G\)-action can be created from any \(\) by averaging over \(G\)-orbits: we take \(g\) to be a random variable uniformly distributed over a compact set of transformations \(G\). Then let \(^{(G)}:=_{G}g\). This is the distribution of the dataset obtained from \(\) under \(G\)-augmentation. Then we have by change of variables:

\[^{(G)}(,\{Z_{i}\},)=( ,\{_{g}[g Z_{i}]\},^{(G)}).\] (3.1)

Note that the notions of orbit and averaging used above do not rely on the property that \(G\) is a group in order to be well-defined.

### Partial and Approximate Equivariance

Since our analysis treats the general case where the model and/or data symmetries are not strict, we now state some basic attendant notions.

Error to equivariance function.We first need a measure to quantify how far off we are from perfect equivariance. To do so, we can define a function in terms of \(f(x,y)\) as:

\[: G[0,+),(f,g)= \|g f-f\|_{}.\]

The crucial property here is that \((f,g)=0\) iff \(g f=f\). Note that one could use other distances or discrepancies (e.g. like the \(_{2}\) norm) directly in terms of \(\), using which we can measure the error between \(g\) and \(\).

More general sets of transformations.We can use \(()\) to precisely specify the notions of partial and approximate equivariance that we work with. We consider the general case where the underlying transformations on \((X,Y)\) still have a common label set \(G\) and are assumed to be bijective over \(\) and \(\), but don't necessarily form a group. For \(>0\) and \(f\) we consider the \(\)-stabilizer

\[_{}(f):=\{g G:\ (f,g)\}.\]

It is fairly easy to see that this subset is automatically a group for \(=0\), but most often will not be when \(>0\). The latter corresponds to _partial symmetry_ and thus to the setting of **partial equivariance**. On the other hand, if we consider specific hypotheses such that for some \(>0\) we have \(_{}(f)=G\) for all \(f\), then we are working in the setting of **approximate equivariance**.

**Conditions on \(G\).** We conclude this section with a remark on the nature of \(G\) we consider in our paper. In our bounds, we first restrict to **finite**\(G\). However, this setting can be extended to **compact** groups which are doubling with respect to a metric that respects the composition operation. That is, a distance \(_{G}\) over \(G\), such that \(_{G}(gh,g^{}h)=d(g,g^{})\). The main examples we have in mind are compact Lie groups such as \(S^{1},SO(n),O(n)\), and their finite subgroups like \(/N\), as well as their quotients and products.

## 4 PAC bounds for \(G\)-equivariant hypotheses

In this section, we study the generalization properties of equivariant models. More specifically, we derive PAC bounds for both exact and approximately G-equivariant hypotheses. But first, we present some preliminaries related to PAC bounds in what follows.

**PAC learning bounds.**

We follow the standard approach (such as that outlined in ). We start with the following concentration bound, where \(\) is composed of functions with range \([-M/2,M/2]\):

\[[_{}|(P-P_{n})f|(_{Z} )+] 2(-n}{M}),\] (4.1)

where \(_{n}()\) denotes the Rademacher complexity, which is given by

\[(_{Z}):=_{}_{}_{i=1}^{n}_{i}f(Z_{i}),\]

and where \(Z_{1},,Z_{n}\) are \(n\) i.i.d. samples and \(=(_{1},,_{n})\) denotes the so-called Rademacher variable, which is a uniformly distributed random variable over \(\{-1,1\}^{n}\), independent of the \(Z_{i}\)'s. \((_{Z})\) can be bounded using a classical technique for bounding the expected suprema of random processes indexed by the elements of a metric space, variously called the Dudley entropy integral or the chaining bound. The result below was proven in [45, Lemma 3] (also see slightly stronger results in [58, Thm. 17] or [6, Thm. 1.1])

\[(_{Z}) 4_{>0}(+} _{}^{()}(,t,\|\|_{})}dt).\] (4.2)

Recall that for a metric space \((X,_{X})\), the **covering number**\((X,,_{X})\) is the smallest number of balls of radius \(\) required to cover \(X\). In (4.2) we use the cover numbers of \(\) in supremum distance, i.e. the distance between two functions \(f,g\), \(\|f-g\|_{}:=_{z}|f(z)-g(z)|)\). It is known that by rescaling the fundamental estimate due to Kolmogorov-Tikhomirov [54, eq. 238, with \(s=1\), and eq. 1], and under the mild assumption that \(\) is composed of \(1\)-Lipschitz4 functions on \(\) with values in an interval \([-M/2,M/2]\), for a centralizable5 metric space \(\), the following holds

\[(,2)_{2}(, ,\|\|_{})_{2}(+1)+ (,/2).\] (4.3)

Before we start stating our results, we need to define a few more notions:

**Doubling and Hausdorff dimensions.** If in a metric space \((X,_{X})\), every ball of radius \(R\) can be covered by at most \(\) balls of radius \(R/2\), the space has doubling dimension \((X)=_{2}\). The doubling dimension coincides with the usual notion of (Hausdorff) dimension \(X\), i.e. \(X=X\), in case \(X\) is a compact manifold with bounded injectivity radius, in particular it equals \(d\) if \(X^{D}\) is a regular \(d\)-dimensional submanifold or if \(D=d\) and \(X\) is a regular open subset such as a ball or a cube.

Discrete metric spaces.A metric space \((X,_{X})\) is **discrete** if it has strictly positive **minimum separation distance**\(_{X}:=_{x x^{}}_{X}(x,x^{})>0\). We then have

\[(X,)=(X,\{,_{X}\}),\] (4.4)

which is especially relevant for **finite groups**\(G\) endowed with the **word distance** (i.e. \(_{G}(g,g^{})\), which is the minimum number of generators required to express \(g^{-1}g^{}\)), for which \(_{G}=1\). Now, note that as a straightforward consequence of the definition, there exists a universal \(c>0\) such that:

\[(,)(() }{})^{()}.\] (4.5)

and by (4.3) we get the simplified bound (where implicit constants are universal):

\[(,,\|\|_{})(()}{})^{()}.\] (4.6)

By all the above considerations, we can bound the generalization error as follows:

**Proposition 4.1**.: _Assume that \(d=()>2\), and \(0<<1/2\), and let \(D:=()\). Then for any probability distribution \(\) of data over \(\), with notation as in the beginning of Section 4, the following holds with probability at least \(1-\):_

\[(,\{Z_{i}\},)d}{d-2} (}{n})^{1/d}+n^{-1/2}\|_{} (2/)},\]

_the implicit constant is independent of \(,n\); only depending on \(\) through the constants in (4.5)._

Due to space constraints, all our proofs are relegated to the Appendix. With the above background, we now first show generalization bounds under the more general notions of equivariance we have discussed.

### Generalization bounds improvement under partial or approximate equivariance

In this section, we prove generalization error bounds with the notations and definitions from Section 3.2. We consider the following sets of transformations:

\[_{}=_{}():=_{f }_{}(f).\]

We note that the strict equivariance case is recovered if, for \(=0\), we have \(_{0}=G\).

**Proposition 4.2**.: _Let \(_{}\) be as above, and assume that \(|_{}|>0\). let \(_{}^{0}\) be a measurable choice of \(_{}\)-orbit representatives for points in \(\), and let \(_{}^{0}:_{}^{0}\) be the measurable map that associates to each \(z\) its representative in \(_{}^{0}\). Let \(_{}^{0}:=\{f|_{_{}^{0}}:f\}\) and let \(_{}^{0}()\) represent the image measure of \(\). Then for each \(n\), if \(\{Z_{i}\}_{i=1}^{n}\) are i.i.d. samples with \(Z_{i}\) and \(Z_{,}^{0}:=_{}^{0} Z_{i}\), then we have_

\[(,\{Z_{i}\},) 2+ ^{(_{})}(,\{Z_{i}\},)=2+ (_{}^{0},\{Z_{i,}^{0}\},_{ }^{0}()).\]

The above result says that the generalization error of our setting of interest could roughly be obtained by working with a reduced set of only the orbit representatives for points in \(\). This is in line with prior work such as . However, note that our result is already more general and does not require that the set of transformations form a group. With this, we now need an understanding of the covering of spaces of representatives \(_{}^{0}\) and the effect of \(_{}\) on them. The answer is evident in case \(_{}=G\), that \(G\), \(^{0}\) are manifolds, and \(=^{0} G\). Since \(\) coincides with topological dimension, and we immediately have

\[d_{0}=d-(G).\]

Intuitively, the dimensionality of \(G\) can be understood as eliminating degrees of freedom from \(\), and it is this effect that improves generalization by \(n^{-1/(d-(G))}-n^{-1/d}\).

We now state a simplified form of Theorem A.3, which is sufficient for proceeding further. Our results generalize [51, Thm. 3]: we allow for non-product structure, possibly infinite sets of transformations that may not form a group, and we relax the distance deformation hypotheses for the action on \(\)).

**Corollary 4.3** (of Thm. A.3).: _With the same notation as above, assume that for \(_{}\) the transformations corresponding to the action of elements of \(_{}\) satisfy the following for some \(L>0\), and for a choice of a set of representatives \(_{}^{0}\) of representatives of \(_{}\)-orbits:_

1. _For all_ \(z_{0},z_{0}^{}_{}^{0}\) _and all_ \(g_{}\) _it holds that_ \((z_{0},z_{0}^{}) L\ (g z_{0},g z_{0}^{ })\)_._
2. _For all_ \(g,g^{}_{}\) _it holds that_ \(_{G}(g,g^{}) L\ (g_{}^{0},g^{ }_{}^{0})\)6_._

_Then for each \(>0\) there holds \((_{}^{0},)(, /2L)/(_{},)\)._

To be able to quantify the precise generalization benefit we need a bound on the quantity \((_{},)\). For doing so, we assume that \(_{}\) is a finite subset of a discrete group \(G\), or is a positive measure subset of a compact group \(G\). As before, let \(|G|\) and \(_{G}\) denote \( G\) and \((G)\) respectively for finite \(G\). Also, denote the Hausdorff measure and dimension by \((G)\) and \((G)\) for compact metric groups. Note that the minimum separation \(_{G}\) is zero for \((G)>0\). Then our covering bound can be expressed in the following umbrella statement:

\[(_{},) \{1,\ _{}|}{(\{,_{G}\})^{_{G}}} \}.\] (4.7)

In order to compare the above to the precise equivariance case, we later use the factor

\[():=_{}|}{|G|}(0,1],\]

which measures the richness of \(_{}\) compared to the whole ambient group \(G\), in terms of error \(\). The fact that \(()>0\) follows from our assumption that \(|_{}|>0\). Furthermore, \(()\) is always an increasing function of \(\), as follows from the definition of \(_{}\). With these considerations on coverings, we can state a general result quantifying the generalization benefit, where if we set \(=0\), we recover the case of exact group equivariance.

**Theorem 4.4**.: _Let \(>0\) be fixed. Assume that for a given ambient group \(G\) the almost stabilizers \(_{}\) satisfy assumption (4.7) and that the assumptions of Corollary 4.3 hold for some finite \(L=L_{}>0\). Then with the notations of Proposition 4.1 we have with probability \( 1-\)_

\[(,\{Z_{i}\},) n^{-1/2}\|_{}(2/)}+2+(E_{}),\]

_where_

\[(E_{}):=\{}d_{0}}{d_{0}-2} _{G}^{-d_{0}/2+1}()^{d}D^{d}}{|_{ }|n})^{1/2}&)^{d}D^{d}<|_{ }|n$ $_{G}^{d_{0}}$},\\ }d_{0}}{d_{0}-2}()^{d}D^{d}}{|_{}|n})^{1/d_{0}}&.\]

The interpretation of the above terms is direct: the term \(2\) is the effect of approximate equivariance, and the term \((E_{})\) includes the dependence on \(|_{}|\) and thus is relevant to the study of partial equivariance. In general the Lipschitz bound \(L_{}\) is increasing in \(\) as well, since \(_{}\) includes more elements of \(G\) as \(\) increases, and we can expect that actions of elements generating higher equivariance error in general have higher oscillations.

## 5 Approximation error bounds under approximate data equivariance

In the introduction, we stated that we aim to validate the intuition that that model is best whose symmetry aligns with that of the data. So far we have only given bounds on the generalization error. Note that our bounds hold for any data distribution: in particular, the bounds are not affected by whether the data distribution is \(G\)-equivariant or not. However,the fact that data may have fewer symmetries than the ones introduced in the model will have a controlled effect on worsening the approximation error, as described in this section.

However, controlling the approximation error has an additional actor in the fray to complicate matters: the distinguishing power of the loss function. Indeed, having a degenerate loss function with a large minimum set will not distinguish whether the model is fit to the data distribution or not. Thus, lowering the discrimination power of the loss has the same effect as increasing the function space for which we are testing approximation error.

Assuming at first that \(}\) is composed of \(G\)-equivariant functions, we compare its approximation error to the one of the set \(\) of all measurable functions \(m:\). Recall that \(f(Z)=((X),Y)\) is a positive function for all \(}\), thus, denoting by \(^{}:=\{F(x,y)=(m(x),y),m\}\), the approximation error gap can be defined and bounded as follows (where \(F^{*}^{}\) is a function realizing the first minimum in the second line below):

\[(,,) := (,)-(^{},):=_{f}[f(Z)]-_{F ^{}}[F(Z)]\] \[ _{F^{}}[_{g}[F(g  Z)]]-_{F^{}}[F(Z)][ _{g}[F^{*}(g Z)]]-[F^{*}(Z)]\] \[ _{F^{}}[_{g}[F(g  Z)]-F(Z)].\]

With applications to neural networks in mind, we will work with the following simplifying assumptions:

1. The loss function quantitatively detects whether the label is correct, i.e. we have \((y,y^{})=0\) if \(y=y^{}\), and \((y,y^{})(_{}(y,y^{}))\) for a strictly convex \(\) with \((0)=0\), where \(_{}\) is the distance on \(\). This assumption seems reasonable for use in applications, where objective functions with strict convexity properties are commonly used, as they help for the convergence of optimization algorithms.
2. The minimization across all measurable functions \(}=\) produces a zero-risk solution, i.e. \(_{F^{}}[F(Z)]=0\) is achieved by a measurable function \(y^{*}:\). By assumption 1, this implies that \(\) is concentrated on the graph of \(y^{*}\). This assumption corresponds to saying that the learning task is in principle deterministically solvable. Under this assumption, the task becomes to approximate to high accuracy the (unknown) solution \(y^{*}\).

With the above hypotheses we have

\[(,)=(, )_{F^{}}_{Z}[_{g}[ F(g Z)]].\]

As a slight simplification of Assumption 1 for ease of presentation, we will simply take \(=^{d}\) and \((y,y^{})=_{}(y,y^{})^{2}\) below. Note that Assumption 1 directly reduces to this case, as a strictly convex \((t)\) with \((0)=0\) as in Assumption 1 is automatically bounded below by a multiple of \(t^{2}\).

Now, with the aim to capture the mixed effects of approximate and partial equivariance, we introduce the following set of model classes. For \( 0,(0,1]\):

\[_{,}:=\{^{ }:\ ()=_{}()|}{|G|} \}.\]

In order to establish some intuition, note that \(_{}()\) is increasing in \(\) and as a consequence \(_{,}\) is also increasing in \(\). Directly from the definition one finds that \(_{0}()\) is necessarily a subgroup of \(G\), and thus we allow \(>0\), which allows more general sets of symmetries than just subgroups of \(G\). The most interesting parameter in the above definition of \(_{,}\) is \(\), which actually bounds from below the "amount of prescribed symmetries" for our models. In the fully equivariant case \(=0,=1\), we have the simple description \(_{0,1}=\{:(^{(G)})^{ }\}\), whose maximal element is \((^{(G)})^{}\). However in general \(_{,}\) will not have a unique maximal element for \(<1\): this is due to the fact that two different elements \(_{1}_{2}_{,}\) may have incomparable stabilizers, so that \(|_{}(_{1})_{}( _{2})|>0\) and \(|_{}(_{2})_{}( _{1})|>0\), and thus \(_{1}_{2}_{,}\). Our main result towards our approximation error bound is the following.

**Proposition 5.1**.: _Assume that \((,_{})\) is a compact metric space and consider \((y,y^{})=_{}(y,y^{})^{2}\). Fix \( 0\). Then for each \((0,1]\) there exists an element \(_{,}\) and a measurable selection \(_{}^{0}:_{}^{0}\) of representatives of orbits of \(\) under the action of \(_{}()\), such that if \(X_{}^{0},g\) are random variables obtained from \(X\) by imposing \(X_{}^{0}:=_{}^{0} X\) and \(X=g X_{}^{0}\), then we have_

\[(,)_{X_{}^{0}} _{y}_{g|X_{}^{0}}[(\{_{}(g y,\ y^{*}(g X_{}^{0}))-,\ 0\})^{2}].\]

In the above, as \(\) increases, we see a decrease in the approximation gap, since in this case model classes \(\) allow more freedom for approximating \(y^{*}\) more accurately. On the other hand, the effect of parameter \(\) is somewhat more subtle, and it has to do with the fact that as \(\) decreases, \(_{}()\) for \(_{,}\) can become smaller and the allowed oscillations of \(y^{*}(g X_{}^{0})\) are more and more controlled.

Now to bound the approximation error above using the quantities that we have been working with, we will use the following version of an **isodiametric inequality** on metric groups \(G\). As before we assume that \(G\) has Hausdorff dimension \((G)>0\) which coincides with the doubling dimension, or is discrete. By \(|X|\) we denote the Hausdorff measure of \(X G\) if \((G)>0\) or the cardinality of \(X\) if \(G\) is finite. Then there exists an isodiametric constant \(C_{G}\) depending only on \(G\) such that for all \(X G\) it holds that

\[(X) C_{G} ^{1/(G)}.\] (5.1)

The above bound can be obtained directly from the doubling property of \(G\), applied to a cover of \(X\) via a single ball of radius \((X)\), which can be refined to obtain better and better approximations of \(|X|\). We can now control the bound from Proposition 5.1 as follows:

**Theorem 5.2**.: _Assume that \((,_{})\) is a compact metric space and \((y,y^{})=_{}(y,y^{})^{2}\). Let \(G\) be a compact Lie group or a finite group, with distance \(_{G}\). Let \((G)\) be the doubling dimension of \(G\), and assume that for all \(g G\) such that the action of \(g\) over \(\) is defined, it holds that \((g x,g^{} x) L^{}_{G}(g,g^{ })\). Then, there exists a constant \(C_{G}\) depending only on \(G\) such that for all \((0,1]\) and \(>0\), there exists an element \(_{,}\) where for any data distribution \(\), \(y^{*}\) can be chosen Lipschitz with constant \((y^{*})\). We have_

\[(,)(\{C_{G}\ L^{ }\ (1+(y^{*}))^{1/(G)}-,\ 0\})^{2}.\]

We note that, given our proof strategy of Proposition 5.1, we conjecture that actually the above bound is sharp up to a constant factor: \(_{_{,}}(,)\) may have a lower bound comparable to the above upper bound. The goal of the above result is to formalize the property that the approximation error guarantees of \(\) will tend to grow as the amount of imposed symmetries grow, as measured by \(\).

## 6 Discussion: Imposing optimal equivariance

We have so far studied bounds on the generalization and the approximation errors, each of which take a very natural form in terms of properties of the space of orbit representatives (and thus the underlying \(G\)). One of them depends on the data distribution, while the other doesn't. Using them we can thus state a result that quantifies the performance error, or gives the generalization-approximation trade-off, of a model based on the data distribution (and their respective symmetries). Define the performance error of a model class \(_{,}\) over an i.i.d. sample \(\{Z_{i}\}_{i=1}^{n},Z_{i}\) as

\[(,\{Z_{i}\},):=( ,\{Z_{i}\},)+(,).\]

Combining Theorems 4.4 and 5.2, we get the following:

**Theorem 6.1**.: _Under the assumptions of Theorems 4.4 and 5.2, we have that, assuming all the function classes are restricted to functions with values in \([-M,M]\), then there existvalues \(C_{1},C_{2},C_{3}>\) depending only on \(d_{0},(2LD)^{d}/|G|,_{G}\) and \(C=C_{G}L^{}(1+(y^{*}))\) such that with probability at least \(1-\),_

\[(,\{Z_{i}\},) n^{-1/2}+2+(C^{1/d_{G}}-)_{+}^{2}+\{ []{ll}C_{1}}&n C_{3},\\ C_{2}}}&n\ <C_{3}..\]

Note that the above bounds are not informative as \( 0\) for the continuous group case, which corresponds to the second option in Theorem 6.1. This can be sourced back to the form of Assumption (4.7), in which the covering number on the left is always \( 1\), whereas in the continuous group case, the right-hand side may be arbitrarily small. Thus the result is only relevant for \(\) away from \(0\). If we fix \(=0\) and we optimize over \((0,1]\) in the above bound, we find a unique minimum \(^{*}\) for the error bound, specifying the optimum size of \(_{}\) for the corresponding case (see the Appendix for more details). This validates the intuition stated at the onset that for the best model its symmetries must align with that of the data. As an illustration, for \(C,C_{1},C_{2}=0.04\) and \(C_{3}=0.01\) with \(n=100000\), the error trade-off is shown on the figure on the right.

## 7 Concluding Remarks

In this paper, we have provided general quantitative bounds showing that machine learning models with task-pertinent symmetries improve generalization. Our results do not require the symmetries to be a group and can work with partial/approximate equivariance. We also presented a general result which confirms the prevalent intuition that if the symmetry of a model is mis-specified w.r.t to the data symmetry, its performance will suffer. We now indicate some important future research directions, that correspond to limitations of our work:

* (_Model-specific bounds_) While in Theorem 6.1 we obtain the existence of an optimal amount \(^{*}\) without hopes to be sharp in such generality, we may expect that if we restrict to specific tasks, the bounds can become much tighter, and if so, the value \(^{*}\) of optimal amount of symmetries can become interpretable and give insights into the structure of the learning tasks.
* (_Tighter bounds beyond classical PAC theory_) For the sake of making the basic principles at work more transparent, we based our main result on long-known classical results. However, tighter data-dependent or probabilistic bounds would be very useful for getting a sharper value for the optimal value of symmetries \(^{*}\) via a stronger version of Theorem 6.1.
* (_Beyond controlled doubling dimension_) Our focus here is on groups \(G\) of controlled doubling dimension. This includes compact and nilpotent Lie groups, and discrete groups of polynomial growth, but the doubling dimension is not controlled for notable important cases, such as for the permutation group \(S_{n}\) (cf. section 3 of ) or for the group \((/2)^{n}\). In order to cover these cases, it will be interesting to build analogues of our main result based on other group complexity bounds, beyond the doubling dimension case.