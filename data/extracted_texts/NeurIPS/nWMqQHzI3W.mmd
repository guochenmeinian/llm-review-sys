# SEEV: Synthesis with Efficient Exact Verification for ReLU Neural Barrier Functions

Hongchao Zhang

Electrical & Systems Engineering

Washington University in St. Louis

hongchao@wustl.edu

&Zhizhen Qin

Computer Science & Engineering

University of California, San Diego

zhizhenqin@ucsd.edu

&Sicun Gao

Computer Science & Engineering

University of California, San Diego

scungao@ucsd.edu

&Andrew Clark

Electrical & Systems Engineering

Washington University in St. Louis

andrewclark@wustl.edu

Equal contribution

###### Abstract

Neural Control Barrier Functions (NCBFs) have shown significant promise in enforcing safety constraints on nonlinear autonomous systems. State-of-the-art exact approaches to verifying safety of NCBF-based controllers exploit the piecewise-linear structure of ReLU neural networks, however, such approaches still rely on enumerating all of the activation regions of the network near the safety boundary, thus incurring high computation cost. In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV). Our framework consists of two components, namely (i) an NCBF synthesis algorithm that introduces a novel regularizer to reduce the number of activation regions at the safety boundary, and (ii) a verification algorithm that exploits tight over-approximations of the safety conditions to reduce the cost of verifying each piecewise-linear segment. Our simulations show that SEEV significantly improves verification efficiency while maintaining the CBF quality across various benchmark systems and neural network structures. Our code is available at https://github.com/HongchaoZhang-HZ/SEEV.

## 1 Introduction

Safety is a crucial property for autonomous systems that interact with humans and critical infrastructures in applications including medicine, energy, and robotics , which has motivated recent research into safe control . Control Barrier Functions (CBFs), which apply a constraint on the control input at each time in order to ensure that safety constraints are not violated, have attracted significant research attention due to their ease of implementation and compatibility with a variety of safety and performance criteria . Recently, CBFs that are defined by neural networks, denoted as Neural Control Barrier Functions (NCBFs), have been proposed to leverage the expressiveness of NNs for safe control of nonlinear systems . NCBFs have shown substantial promise in applications including robotic manipulation , navigation , and flight control .

A key challenge in NCBF-based control is safety verification, which amounts to ensuring that the constraints on the control can be satisfied throughout the state space under actuation limits. The NCBF safety verification problem effectively combines two problems that are known to be difficult, namely, input-output verification of neural networks (VNN)  and reachability verification of nonlinear systems. While sound and complete verifiers such as dReal can be applied toNCBFs, they typically can only handle systems of dimension three or small neural networks [21; 22]. In , exact conditions for safety verification of NCBFs with ReLU activation functions were proposed that leverage the piecewise-linearity of ReLU-NNs to reduce verification time compared to dReal for general activation functions. The exact conditions, however, still require checking correctness of the NCBF by solving a nonlinear optimization problem along each piecewise-linear segment. Hence, the NCBF verification problem remains intractable for high-dimensional systems.

In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV) for piecewise-linear NCBFs. The main insight of SEEV is that the computational bottleneck of NCBF verification is the inherent requirement of verifying each linear segment of the neural network. We mitigate this bottleneck by (i) developing a training procedure that reduces the number of segments that must be verified and (ii) constructing verification algorithms that efficiently enumerate the linear segments at the safety boundary and exploit easily-checked sufficient conditions to reduce computation time. Towards (i), we introduce a regularizer to the loss function that penalizes the dissimilarity of activation patterns along the CBF boundary. Towards (ii), we propose a breadth-first search algorithm for efficiently enumerating the boundary segments, as well as tight linear over-approximations of the nonlinear optimization problems for verifying each segment. Moreover, we integrate the synthesis and verification components by incorporating safety counterexamples returned by the safety verifier into the training dataset. Our simulation results demonstrate significant improvements in verification efficiency and reliability across a range of benchmark systems.

**Related Work:** Neural control barrier functions have been proposed to describe complex safety sets to remain inside and certify safety of a controlled system [24; 21; 25; 26] or synthesize control input based on NCBFs to ensure safety [8; 9; 10; 27]. However, the synthesized NCBF may not ensure safety. Safety verification of NCBFs is required. Sum-of-squares (SOS) optimization [28; 29; 30; 31; 32] has been widely used for polynomial barrier functions, however, they are not applicable due to the non-polynomial and potentially non-differentiable activation functions of NCBFs. VNN [33; 34; 35] and methods for ReLU neural networks [36; 37] are also not directly applicable to NCBF verification. Nonlinear programming approach  provides another route for exact verification but is computationally intensive and relies on VNN tools. To synthesize neural networks with verifiable guarantees, Counterexample Guided Inductive Synthesis (CEGIS) has been applied using SMT-based techniques [21; 38; 39; 40; 41]. Other verification-in-the-loop approaches utilize reachability analysis  and branch-and-bound neural network verification tools . However, existing works suffer from the difficulty of performing verification and generating counterexamples in a computationally efficient manner. Sampling-based approaches [41; 11] aim to prove safety using Lipschitz conditions, but they rely on dense sampling over the state space, which is computationally prohibitive. In this work, we present SEEV to integrate the synthesis and efficient verification by incorporating safety counterexamples from the exact verification.

**Organization** The remainder of the paper is organized as follows. Section 2 gives the system model and background on neural networks and the conditions of valid NCBFs. Section 3 presents the SEEV framework. Section 4 presents our efficient and exact verification. Section 5 contains simulation results. Section 6 concludes the paper.

## 2 Preliminaries

This section presents the system model, notations on neural networks, and exact conditions of safety.

### System Model

We consider a system with state \(x(t)^{n}\) and input \(u(t)^{m}\), with initial condition \(x(t_{0})=x_{0}\) where \(x_{0}\) lies in an initial set \(\). The continuous-time nonlinear control-affine system has the dynamics given by

\[(t)=f(x(t))+g(x(t))u(t)\] (1)

where \(f:^{n}^{n}\) and \(g:^{n}^{n m}\) are known continuous functions.

We consider the case that the system is required to remain inside a given set of states, i.e., \(x(t)\) for all time \(t t_{0}\). The set \(\), referred to as the _safe set_, is defined as \(=\{x:h(x) 0\}\) by some given continuous function \(h:^{n}\). The unsafe region is given by \(\).

### Neural Network Model and Notations

We let \(\) and \(\) denote the weight and bias of a neural network, and let \(\) be a parameter vector obtained by concatenating \(\) and \(\). We consider a \(\)-parameterized feedforward neural network \(b_{}:^{n}\) constructed as follows. The network consists of \(L\) layers, with each layer \(i\) consisting of \(M_{i}\) neurons. We let \((i,j)\{1,,L\}\{1,,M_{i}\}\) denote the \(j\)-th neuron at the \(i\)-th layer. We denote the pre-activation input as \(z_{j}^{(i)}\), piecewise linear activation function \(\) and the post-activation output as \(_{j}^{(i)}=(z_{j}^{(i)})\). Specifically, we assume that the NN has the Rectified Linear Unit (ReLU) activation function, defined by \((z)=z\) for \(z 0\) and \((z)=0\) for \(z<0\). We define the neuron \((i,j)\) as _active_ if \(z_{j}^{(i)}>0\), _inactive_ if \(z_{j}^{(i)}<0\) and _unstable_ if \(z_{j}^{(i)}=0\). Let \(=_{S}(x)=\{(i,j):z_{j}^{(i)} 0\}\{(i,j):i=1, ,L,j=1,,M_{i}\}\) denote the set of activated and unstable neurons, produced by state \(x\) and function \(_{S}\). Let \((x)=_{T}(x)=\{(i,j):z_{j}^{(i)}=0\}\) denote the set of unstable neurons produced by activation sets \(_{1},,_{r}\). The set of inactive neurons is given by \(^{c}\), i.e., the complement of \(\), and consists of neurons with negative pre-activation input. We define vectors \(_{ij}()^{n}\) and scalars \(_{ij}()\) such that \(z_{j}^{(i)}=_{ij}()^{T}x+_{ij}()\) in the Appendix A.1. The symmetric difference between two sets \(\) and \(\), denoted by \(\), is defined as \(=()( )\).

Finally, we define the terms _hyperplane_ and _hinge_. For any \(\{1,,L\}\{1,,M_{i}\}\), we define \(}():=\{x:(x)=\}\). The collection of \(}()\) for all \(\) is the set of _hyperplanes_ associated with the ReLU neural network. A hyperplane that intersects the set \(\{x:b_{}(x)=0\}\) is a _boundary hyperplane_. The intersection of hyperplanes \(}(_{1}),,}(_{r})\) is called a _hinge_. A hinge that intersects the set \(\{x:b_{}(x)=0\}\) is a _boundary hinge_.

### Guaranteeing Safety via Control Barrier Functions

Barrier certificates  ensure the safety of a feedback-controlled system under policy \((x)\) by identifying a CBF to represent the invariant safe set. The barrier certificate defines an inner safe region \(:=\{x:b(x) 0\}\) for some continuous function \(b\). The verifiable invariance of \(\) is obtained from the following result.

**Theorem 1** (Nagumo's Theorem , Section 4.2).: _A closed set \(\) is controlled positive invariant if, whenever \(x\), where \(\) denotes the boundary of \(\). we have_

\[(f(x)+g(x)u)_{}(x)\] (2)

_for some \(u\) where \(_{}(x)\) is the tangent cone to \(\) at \(x\)._

We denote a state \(_{ce}^{c}\) with \(_{ce}^{c}\) that violates (2) as a _safety counterexample_. In the case where \(b\) is continuously differentiable, (2) can be satisfied by selecting \(u\) to satisfy the condition \((f(x(t))+g(x(t))u(t))-(b(x(t)))\), where \(:\) is a strictly increasing function with \((0)=0\). When \(b\) is not continuously differentiable, as in a ReLU NCBFs, a modified condition is needed. Prior work  introduces exact conditions for safety verification of ReLU NCBFs, based on the following proposition. A collection of activation sets \(_{1},,_{}\) is complete if, for any \(^{}\{_{1},,_{}\}\), we have \(}(_{1})}( _{})}(^{})=\).

**Proposition 1**.: _Suppose the function ReLU neural network-defined function \(b\) satisfies the following conditions:_

1. _For all activation sets_ \(_{1},,_{r}\) _with_ \(\{_{1},,_{r}\}\) _complete and any_ \(x\) _satisfying_ \(b(x)=0\) _and_ \[x(_{l=1}^{r}}(_{l})),\] (3) _there exist_ \(l\{1,,r\}\) _and_ \(u\) _such that_ \[(}_{i-1}(_{l})W_{ij})^{T}(f(x)+g(x)u)  0(i,j)(_{1},,_ {r})_{l}\] (4) \[(}_{i-1}(_{l})W_{ij})^{T}(f(x)+g(x)u)  0(i,j)(_{1},,_ {r})_{l}\] (5) \[(_{l})^{T}(f(x)+g(x)u)  0\] (6)_._
2. _For all activation sets_ \(\)_, we have_ \[(}())=\] (7)

_If \(b(x(0)) 0\), then \(x(t)\) for all \(t 0\)._

Any feedback control law \(:\) that satisfies (4)-(6) is guaranteed to ensure safety and is referred to as an NCBF control policy. Given a nominal control policy \(_{nom}(x)\), safe actions can be derived from a ReLU NCBF as a safety filter[29; 9] by solving the following optimization problem proposed in [23; Lemma 2]:

\[_{(x),u}||u-_{nom}(x)||_{2}^{2} ()^{T}(f(x)+g(x)u)-(b(x)),u,(4)-(6)\] (8)

The solution to this optimization problem provides a control \(u\) that minimally deviates from the nominal control \(_{nom}(x)\) while satisfying NCBF constraints derived in Proposition 1 ensuring that \(\) is positive invariant and is contained in \(\). Based on Proposition 1, we can define different types of safety counterexamples. Correctness counterexamples, denoted by \(_{ce}^{c}\), refers to a state \(_{ce}^{c}()\). Hyperplane verification counterexamples refer to states \(_{ce}^{h}\) that violate (6). Hinge verification counterexamples are states \(x\) with \((x)\) that violate (4)-(6).

## 3 Synthesis

In this section, we present the framework to synthesize NCBF \(b_{}(x)\) to ensure the safety of the system (1). The synthesis framework aims to train an NCBF and construct an NCBF-based safe control policy. We first formulate the problem and present an overview of the framework in 3.1. Then we demonstrate the design of the loss function in 3.2.

### Overall Formulation

Our primary objective is to synthesize a ReLU Neural Control Barrier Function (ReLU-NCBF) for (1) and develop a safe control policy to ensure system safety.

**Problem 1**.: _Given a system (1), initial set \(\) and a safety set \(\), synthesize a ReLU NCBF \(b_{}(x)\) parameterized by \(\) such that the conditions in Proposition 1 are satisfied. Then construct a control policy to synthesize \(u_{t}\) such that the system remains positive invariant in \(:=\{x:b_{}(x) 0\}\)._

We propose SEEV to address this problem with the synthesis framework demonstrated in Fig. 1. The training dataset \(\) is initialized by uniform sampling over \(\). The training framework consists of two loops. The inner loop attempts to choose parameter \(\) for \(b_{}(x)\) to satisfy the safety condition by minimizing the loss function over training data \(\). The outer loop validates a given NCBF \(b_{}(x)\) by searching for safety counterexamples \(_{ce}\) and updates the training dataset as \(\{_{ce}\}\).

To train the parameters of the NCBF to satisfy the conditions of Proposition 1, we propose a loss function that penalizes the NCBF for violating constraints (i) and (ii) at a collection of sample points. The loss function is a weighted sum of three terms. The first term is the correctness loss penalizing state \(\) with \(b_{}(x) 0\). The second term is verification loss that penalizes states \(\) that \(\) such that (4)-(6) hold. The third term is a regularizer minimizing the number of hyperplanes and hinges along the boundary. However, minimizing the loss function is insufficient to ensure safety  because there may exist safety counterexamples outside of the training dataset. In order to guarantee safety, SEEV introduces an efficient exact verifier to certify whether \(b_{}(x)\) meets the safety conditions outlined in Proposition 1. The verifier either produces a proof of safety or generates a safety counterexample that can be added to the training dataset to improve the NCBF.

The integration of the verifier can improve safety by adding counterexamples to guide the training process, however, it may also introduce additional computation complexity. We propose a combined approach, leveraging two complementary methods to address this issue. First, the verification of SEEV introduces an efficient algorithm in Section 4 to mitigate the computational scalability challenges that arise as neural network depth and dimensionality increase. Second, SEEV introduces a regularizer to limit the number of boundary hyperplanes and hinges to be verified, addressing the complexity that arises as neural network size increases.

### Loss Function Design and NCBF Training

The goal of the inner loop is to choose parameters \(\) so that the conditions of Proposition 1 are satisfied for all \(\) and the computational cost of verifying safety is minimized. To achieve the latter objective, we observe (based on results presented in Table 2) that the computational complexity of verification grows with the cardinality of the collection of activation sets that intersect the safety boundary \(\). The collection is defined as \(:=\{_{i}:}( _{i})\}\). Hence, we formulate the following unconstrained optimization problem to search for \(\).

\[_{}_{}_{}() +_{f}_{f}()+_{c}_{c}()\] (9)

where \(_{}()\) regularizer to minimize \(||\), \(_{f}()\) is the loss penalizing the violations of constraint (4)-(6) ((i) of Proposition 1), \(_{c}()\) penalizes the violations of constraint (7) ((ii) of Proposition 1), and \(_{}\), \(_{f}\) and \(_{c}\) are non-negative coefficients defined as follows.

\(_{f}\) **Regularizer:** For each sample \(\) the safe control signal is calculated by

\[_{u,r}||u-_{nom}(x)||_{2}^{2} s.t.( _{l})^{T}(f()+g()u)+r 0\] (10)

where \(=\) for differentiable points and \(=\) defined as the subgradient at non-differentiable points. The regularizer \(_{f}\) enforces the satisfaction of the constraint by inserting a positive relaxation term \(r\) in the constraint and minimizing \(r\) with a large penalization in the objective function. We have the loss \(_{f}\) defined as \(_{f}=||u-_{nom}(x)||_{2}^{2}+r\). We use  to make this procedure differentiable, allowing us to employ the relaxation loss into the NCBF loss function design.

\(_{c}\) **Regularizer:**\(_{c}\) regularizer enforces the correctness of the NCBF. In particular, it enforces the \(b_{}(x)\) of safe samples \(x_{T}\) to be positive, and \(b_{}(x)\) of unsafe samples \(_{}\) to be negative. Define \(N_{}=|_{}|\) and \(N_{}=|_{}|\), and with a small positive tuning parameter \(>0\), the loss term \(_{c}\) can be defined as

\[_{c}=a_{1}}}_{x_{ }}[-b_{}(x)]_{+}+a_{2}}} _{x_{}}[+b_{}(x )]_{+}\] (11)

where \([]_{+}=max(,0)\). \(a_{1}\) and \(a_{2}\) are positive parameters controlling penalization strength of the violations of safe and unsafe samples.

\(_{}\) **Regularizer:** We propose a novel regularizer to limit the number of boundary hyperplanes and hinges by penalizing the dissimilarity, i.e., \((_{i})(_{j})\) of boundary activation sets \(()\). However, the dissimilarity measure of boundary activation sets is inherently nondifferentiable. To address this issue the regularizer introduces the generalized sigmoid function \(_{k}(z)=\) to compute the vector of smoothed activation defined as \(_{_{k}}(x):=[_{k}(z_{i,j}), i,j\{1,,L\} \{1,,M_{i}\}]\). The \(_{}\) regularizer conducts the following two steps to penalize dissimilarity.

In the first step, the regularizer identifies the training data in the boundary hyperplanes and hinges denoted as \(_{}\). The set is defined as \(_{}:=\{:} (),\}\). To further improve the efficiency, the regularizer approximates \(_{}\) with a range-based threshold \(\) on the output of the NCBF, i.e., \(|b_{}()|\).

The second step is to penalize the dissimilarity of \(\). To avoid the potential pitfalls of enforcing similarity across the entire boundary \(\), the regularizer employs an unsupervised learning approach to

Figure 1: SEEV: Synthesis with Efficient Exact Verifier for ReLU NCBFgroup the training data into \(N_{}\) clusters. We define the collection of the activation set in each cluster as \(_{i}\) and the collection in each cluster as \(_{_{i}}:=\{:_{ _{i}}}()\}\). The \(_{}\) is then defined as follows, with an inner sum over all pairs of \(_{_{i}}\) and an outer sum over all clusters.

\[_{}=}}_{_{i} }_{_{i}}|^{2}}_{_{i}, {x}_{j}_{_{i}}}\|_{_{k}}(x_{i})-_{ _{k}}(x_{j})\|_{2}^{2},\] (12)

## 4 Verification

In this section, we demonstrate the efficient exact verification of the given NCBF \(b_{}(x)\) to ensure the positive invariance of the set \(\) under the NCBF-based control policy. In what follows, we propose an efficient enumeration and progressive verification moving from sufficient to exact conditions. The overview of the proposed approach is as shown in Fig. 2. SEEV decomposes the NCBF into hyperplanes and hinges and verifies each component hierarchically, with novel tractable sufficient conditions verified first and the more complex exact conditions checked only if the sufficient conditions fail. Given an NCBF \(b_{}(x)\), the verification of SEEV returns (i) a Boolean variable that is 'true' if the conditions of Proposition 1 are satisfied and 'false' otherwise, and (ii) a safety counterexample \(\) that violates the conditions of Proposition 1 if the result is 'false'. Algorithm 1 presents an overview of the verification of SEEV. The algorithm consists of an enumeration stage to identify all boundary hyperplanes and hinges, and a verification stage to certify satisfaction of conditions in Proposition 1 for all hyperplanes and hinges.

```
1:Input:\(n\), \(_{},_{}\)
2:Output: Verification Boolean result \(r\), Categorized counterexample \(_{ce}\)
3:procedureEfficient Exact Verification(\(_{},_{}\))
4:\(_{0}(_{ },_{})\)\(\) Initial Activation Set Identification, Section 4.1
5:\((_{0})\)\(\) Activation Sets Enumeration, Section 4.1
6:\(r,_{ce}^{(e)}()\)\(\) Correctness Verification, Section 4.2
7:\(r,_{ce}^{(h)}()\)\(\) Hyperplane Verification, Section 4.2
8:\((,n)\)\(\) Hinges Enumeration, Section 4.1
9:\(r,_{ce}^{(g)}()\)\(\) Hinge Verification, Section 4.2
10:Return\(r\), \(_{ce}\) ```

**Algorithm 1** Efficient Exact Verification

### Enumeration of Hyperplanes and Hinges

The boundary enumeration identifies the boundary hyperplanes \(_{0}\). It initially identifies the initial boundary activation set \(_{0}\), enumerates all \(\) by NBFS starting from \(_{0}\), and finally enumerates all hinges consisting of the intersections of hyperplanes. The NBFS approach avoids over-approximation of the set of boundary hyperplanes that may be introduced by, e.g., interval propagation methods, and hence is particularly suited to deep neural networks.

In what follows, we assume that the unsafe region \(\) and initial safe set \(\) are connected, and use \(\) to refer to the connected component of the boundary of \(\) that separates \(\) and \(\). This

Figure 2: Overview of the Efficient Exact Verifier for ReLU NCBFs

assumption is without loss of generality since we can always repeat the following procedure for each connected component of \(\) and \(\).

**Initial Activation Set Identification:** First, we identify the initial boundary activation set \(_{0}\). Given \(_{U}\) and \(_{}\), define a line segment

\[=\{x^{n} x=(1-)_{ }+_{},\ \}\] (13)

The following lemma shows the initial boundary activation set \(_{0}\) can always be produced as \(_{0}=()\) for some \(\).

**Lemma 1**.: _Given two sample points \(_{U}\), such that \(b(_{U})<0\), and \(_{}\), such that \(b(_{U})>0\), let \(\) denote the line segment connecting these two points. Then, there exists a point \(\) with \(b_{}()=0\)._

Lemma 1 follows from the intermediate value theorem and continuity of \(b_{}(x)\). In order to search for \(_{0}\), we choose a sequence of \(N_{}\) points \(x_{1}^{0},,x_{N_{}}^{0}\). For each \(x_{i}^{0}\), we check to see if \(}((x_{i}^{0}))\) by solving boundary linear program \(((x_{i}^{0}))\) (14) in Appendix A.4

**Activation Sets Enumeration:** We next describe Neural Breadth-First Search (NBFS) for enumerating all activation sets along the zero-level set, given an initial set \(_{0}\). NBFS enumerates a collection of boundary hyperplanes denoted as \(\) by repeating the following two steps.

_Step 1:_ Given a set \(\), NBFS identifies a collection of neighbor activation sets denoted as \(}\) as follows.

For each \((i,j)\) with \(i=1,,L\) and \(j=1,,M_{i}\), construct \(^{}\) as \(^{}=(i,j),&(i,j) ^{}\\ (i,j),&(i,j)^{}\)

We check whether \(^{}\) by solving the linear program \((,(i,j))\) (15) in Appendix A.4.

If there exists such a state \(x\), then \(^{}\) is added to \(}\). To further improve efficiency, we employ the simplex algorithm to calculate the hypercube that overapproximates the boundary hyperplane, denoted as \(()}()\), and relax the last constraint of (14) to \(x()\).

_Step 2:_ For each \(^{}}\), NBFS determines if the activation region \(}(^{})\) is on the boundary (i.e., satisfies \(}(^{})\)) by checking the feasibility of \(BoundaryLP(^{})\). If there exists such a state \(x\), then \(^{}\) is added to \(\). This process continues in a breadth-first search manner until all such activation sets on the boundary have been enumerated. When this phase terminates, \(=\). Detailed procedure is described as Algorithm 3 in Appendix A.3.

**Hinge Enumeration:** The verifier of SEEV enumerates a collection \(\) of boundary hinges, where each boundary hinge \(\) is a subset \(_{1},,_{r}\) of \(\) with \(}(_{1})}(_{r })\).

Given \(_{i}\) and \(\), hinge enumeration filter the set of neighbor activation sets of \(\) defined as \(_{}(_{i}):=\{^{}:^{ }=1,^{}_{i}\}\). Then, hinge enumeration identifies hinges \(\) by solving linear program \((_{}^{(d)}(_{i}))\) (16) in Appendix A.4. If \( x\), hinge enumeration includes the hinge into the set \(\). The efficiency can be further improved by leveraging the sufficient condition verification proposed in Section 4.2. The following result describes the completeness guarantees of \(\) and \(\) enumerated in Line 5 and 8 of Algorithm 1.

**Proposition 2**.: _Let \(\) and \(\) denote the output of Algorithm 1. Then the boundary \(\) satisfies \(_{}}()\). Furthermore, if \(\) is complete and \((_{i=1}^{}}(_{i})) \{x:b(x)=0\}\), then \(\{_{1},,_{r}\}\)._

The proof is omitted due to the space limit. A detailed proof is provided in Appendix A.2

### Efficient Verification

The efficient verification component takes the sets of boundary hyperplanes \(\) and boundary hinges \(\) and checks that the conditions of Proposition 1 hold for each of them. As pointed out after Proposition 1, the problem of searching for safety counterexamples can be decomposed into searching for correctness, hyperplane, and hinge counterexamples. In order to maximize the efficiency of our approach, we first consider the least computationally burdensome verification task, namely, searching for correctness counterexamples. We then search for hyperplane counterexamples, followed by hinge counterexamples.

**Correctness Verification:** The correctness condition ((7)) can be verified for boundary hyperplane \(}()\) by solving the nonlinear program (17) in Appendix A.5. When \(h(x)\) is convex, (17) can be solved efficiently. Otherwise, dReal can be used to check satisfiability of (17) in a tractable runtime.

**Hyperplane Verification:** Hyperplane counterexamples can be identified by solving the optimization problem (18) in Appendix A.5. Solving (18) can be made more efficient when additional structures on the input set \(\) and dynamics \(f\) and \(g\) are present. Consider a case \(=\{D:||||_{} 1\}\). In this case, the problem reduces to the nonlinear program (19) in Appendix A.5. If \(f(x)\) and \(g(x)\) are linear in \(x\), then the problem boils down to a linear program. If bounds on the Lipschitz coefficients of \(f\) and \(g\) are available, then they can be used to derive approximations of (19).

If \(=^{m}\), then by (23, Corollary 1), the problem can be reduced to the nonlinear program (20) in Appendix A.5. If \(g(x)\) is a constant matrix \(G\), then safety is automatically guaranteed if \(()^{T}G 0\). If \(f(x)\) is linear in \(x\) as well, then (20) is a linear program.

**Hinge Verification:** The hinge \(=\{_{1},,_{r}\}\) can be certified by solving the nonlinear optimization problem (21) in Appendix A.5. In practice, simple heuristics are often sufficient to verify safety of hinges without resorting to solving (21). If \(()^{T}f(x)>0\) for all \(x}(_{1})} (_{r})\), then the control input \(u=\) suffices to ensure safety. Furthermore, if \(=^{m}\) and there exists \(i\{1,,m\}\) and \(s\{0,1\}\) such that \((((_{l})^{T}g(x))_{i})=s\) for all \(x}(_{1})}( _{r})\) and \(=1,,r\), then \(u\) can be chosen as \(u_{i}=Ks\) for some sufficiently large \(K>0\) to ensure that the conditions of Proposition 1.

**Safety Guarantee:** The safety guarantees of our proposed approach are summarized in Theorem 2.

**Theorem 2**.: _Given a NCBF \(b_{}(x)\), \(b_{}(x)\) is a valid NCBF if it passes the verification of Algorithm 1 using dReal to solve the optimization problems (17), (18), and (21)._

The proof is derived from completeness of the enumeration in Algorithm 1 and dReal. A detailed proof can be found in Appendix A.6

## 5 Experiments

In this section, we evaluate the proposed SEEV in regularizer efficacy and verification efficiency. We also demonstrated the improved performance with counter-example guidance in the synthesis framework, whose results are detailed in B.2. We experiment on four systems, namely Darboux, obstacle avoidance, hi-\(_{8}\), and spacecraft rendezvous. The experiments run on a workstation with an Intel i7-11700KF CPU, and an NVIDIA GeForce RTX 3080 GPU. We include experiment settings in Appendix B.1 and hyperparameters settings in Table 4 in Appendix B.3.

### Experiment Setup

**Darboux:** We consider the Darboux system , a nonlinear open-loop polynomial system with detailed settings presented in Appendix B.1

**Obstacle Avoidance (OA):** We evaluate our proposed method on a controlled system . We consider Unmanned Aerial Vehicles (UAVs) avoiding collision with a tree trunk. The system state consists of a 2-D position and aircraft yaw rate \(x:=[x_{1},x_{2},]^{T}\). The system is manipulated by the yaw rate input \(u\) with detailed settings presented in Appendix B.1.

**Spacecraft Rendezvous (SR):** We evaluate our approach on a spacecraft rendezvous problem from . The system state is \(x=[p_{x},p_{y},p_{z},v_{x},v_{y},v_{z}]^{T}\) and control input is \(u=[u_{x},u_{y},u_{z}]^{T}\) with with detailed settings presented in Appendix B.1.

**hi-\(_{8}\):** We evaluate our approach on an eight-dimensional system that first appeared in  to evaluate the scalability of the proposed method. Detailed settings can be found in Appendix B.1

### Regularizer Efficacy Evaluation

Table 2 and Figure 3 illustrates the impact of regularization on the CBF boundary's activation sets. Table 2 compares various configurations, where \(n\) denotes the input dimensions, \(L\) represents the number of layers, and \(M\) indicates the number of hidden units per layer. \(N_{o}\) and \(N_{r}\) are the number ofhyperplanes along the zero-level boundary of the CBF without and with regularization, respectively, with \(r\) indicating the regularization strength. \(C_{o}\) captures the CBF's coverage of the safe region, while \(_{()}=C_{()}/C_{o}\) represents the safety coverage ratio relative to the unregularized CBF. Notably, "N/A" entries indicate configurations where training a fully verifiable network was infeasible due to the excessive number of boundary hyperplanes, which leads the verification process to time out.

The results demonstrate that regularization effectively reduces the number of activation sets along the CBF boundary without compromising the coverage of the safe region. The efficiency is especially improved in cases with a greater number of hidden layers, where the unregularized model results in a significantly higher number of hyperplanes. For instance, in the SR case with \(n=6\), \(L=4\), and \(M=8\), the regularization reduces \(N_{r=50}\) to \(627\) from \(N_{o}=6218\), maintaining the same safety coverage (\(_{r=50}=1\)). See Appendix B.4 for hyperparameter sensitivity analysis.

Figure 3 illustrates the level sets of two CBFs trained for the SR case with \(n=6\), \(L=4\), and \(M=8\). These level sets are extracted from the first two dimensions with the rest set to zero. Each colored patch represents an activation pattern. The regularizer organizes the activation sets around the boundary, reducing unnecessary rapid changes and thereby enhancing the verification efficiency.

### Efficient Verification Evaluation

The results presented in Table 2 illustrate a significant improvement in verification efficiency for Neural Control Barrier Functions NCBFs using the proposed method. In the table \(t_{h}\) represents the time spent searching for hyperplanes containing the CBF boundary and verifying CBF sufficient conditions on these boundaries, and \(t_{g}\) represents the time spent in hinge enumeration and verification. The total time, \(T\), is the sum of \(t_{h}\) and \(t_{g}\). We compare our approach with three baselines, exact verification , SMT-based verification  with dReal and Z3. Baseline methods' run times are represented by Baseline , dReal, and Z3.

In the Darboux cases, our method achieves verification in 2.5 seconds and 3.3 seconds for \(M=256\) and \(M=512\) respectively, whereas baseline methods take substantially longer, with Baseline 

   Case & n & L & M & \(N_{o}\) & \(C_{o}\) & \(N_{r=1}\) & \(_{r=1}\) & \(N_{r=10}\) & \(_{r=10}\) & \(N_{r=50}\) & \(_{r=50}\) \\   & 3 & 2 & 8 & 26 & 89.46\% & 25 & 0.996 & 23.3 & 0.994 & **13.3** & 1.006 \\  & 3 & 2 & 16 & 116 & 83.74\% & 119 & 1.012 & 111 & 1.005 & **98** & 1.055 \\  & 3 & 4 & 8 & 40 & 91.94\% & 38 & 0.988 & 36 & 0.993 & **13** & 0.937 \\  & 3 & 4 & 16 & 156 & 87.81\% & 170 & 0.971 & 147 & 1.003 & **64** & 1.038 \\   & 6 & 2 & 8 & 2868 & 98.58\% & 2753 & 1 & 1559 & 1 & **418** & 1 \\  & 6 & 4 & 8 & 6371 & 98.64\% & 6218 & 1 & 3055 & 1 & **627** & 1 \\   & 6 & 2 & 16 & N/A & N/A & 204175 & N/A & 68783 & N/A & **13930** & N/A \\   

Table 1: Comparison of \(N\) the number of boundary hyperplanes and \(C\) coverage of the safe region \(\) of NCBF trained with and without boundary hyperplane regularizer denoted with subscripts \({}_{r}\) and \({}_{o}\).

Figure 3: Effects of boundary regularization (\(r\)) on activation sets along the boundary. The figures show the results from a neural network with 4 layers of 8 hidden units, applied to the Spacecraft case. The surface represents the first two dimensions with the last four dimensions fixed at 0. Increasing \(r\) results in more organized boundary activation sets.

taking 315 seconds and 631 seconds, and both dReal and Z3 taking more than 3 hours. Similarly, in the OA cases, our method's run times range from 0.39 seconds to 20.6 seconds, faster than the baseline methods. In the more higher dimensional systems high-\(_{}\) and SR, our method significantly outperforms Baseline . Specifically, in high-\(_{}\) our methods finishes within 22.4 seconds while Baseline , dReal and Z3 times out, due to the need to enumerate the 8-dimensional input space. For the SR case, SEEV's run time are 9.8 seconds and 60.1 seconds, beating Baseline  which takes 179 seconds and 298.7 seconds respectively. Neural barrier certificate based dReal and Z3 are able to directly applicable since they require an explicit expression of the controlled feedback system. However, the SR system is manipulated by an NCBF-based safe controller that is nontrivial to derive an explicit expression.

Note that hinge enumeration and certification may be time-consuming procedure, since they involve enumerating all combinations of hyperplanes. However, the results from Table 2 show that the certification can be completed on most hyperplanes with sufficient condition verification in Section 4.2, greatly improving the overall run time.

## 6 Conclusion

This paper considered the problem of synthesizing and verifying NCBFs with ReLU activation function in an efficient manner. Our approach is guided by the fact that the main contribution to the computational cost of verifying NCBFs is enumerating and verifying safety of each piecewise-linear activation region at the safety boundary. We proposed Synthesis with Efficient Exact Verification (SEEV), which co-designs the synthesis and verification components to enhance scalability of the verification process. We augment the NCBF synthesis with a regularizer that reduces the number of piecewise-linear segments at the boundary, and hence reduces the total workload of the verification. We then propose a verification approach that efficiently enumerates the linear segments at the boundary and exploits tractable sufficient conditions for safety.

**Limitations:** The method proposed in this paper mitigated the scalability issue. However, the synthesis and verification of NCBFs for higher-dimensional systems is challenging. Exact verification of non-ReLU NCBFs, which lack ReLU's simple piecewise linearity, remains an open problem.