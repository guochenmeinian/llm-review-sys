# SpecTr: Fast Speculative Decoding via Optimal Transport

Ziteng Sun

Google Research, New York

zitengsun@google.com

&Ananda Theertha Suresh

Google Research, New York

theertha@google.com

&Jae Hun Ro

Google Research, New York

jaero@google.com

&Ahmad Beirami

Google Research, New York

beirami@google.com

&Himanshu Jain

Google Research, New York

himj@google.com

&Felix Yu

Google Research, New York

felixyu@google.com

Equal contribution.

###### Abstract

Autoregressive sampling from large language models has led to state-of-the-art results in several natural language tasks. However, autoregressive sampling generates tokens one at a time making it slow, and even prohibitive in certain tasks. One way to speed up sampling is _speculative decoding_: use a small model to sample a _draft_ (block or sequence of tokens), and then score all tokens in the draft by the large language model in parallel. A subset of the tokens in the draft are accepted (and the rest rejected) based on a statistical method to guarantee that the final output follows the distribution of the large model. In this work, we provide a principled understanding of speculative decoding through the lens of optimal transport (OT) with _membership cost_. This framework can be viewed as an extension of the well-known _maximal-coupling_ problem. This new formulation enables us to generalize the speculative decoding method to allow for a set of \(k\) candidates at the token-level, which leads to an improved optimal membership cost. We show that the optimal draft selection algorithm (transport plan) can be computed via linear programming, whose best-known runtime is exponential in \(k\). We then propose a valid draft selection algorithm whose acceptance probability is \((1-1/e)\)-optimal multiplicatively. Moreover, it can be computed in time almost linear with size of domain of a single token. Using this new draft selection algorithm, we develop a new autoregressive sampling algorithm called _SpecTr_, which provides speedup in decoding while ensuring that there is no quality degradation in the decoded output. We experimentally demonstrate that for state-of-the-art large language models, the proposed approach achieves a wall clock speedup of 2.13X, a further 1.37X speedup over speculative decoding on standard benchmarks.

## 1 Introduction

Autoregressive language models have shown to achieve state-of-the-art results in several natural language tasks . During inference, given a context \(x^{t}\):=\(x(1),x(2),x(t)\), an autoregressive model \(_{b}\) generates successive tokens \(x(t+1),x(t+2),\) via temperature sampling , where the next token \(x(t+1)\) is drawn from the temperature-scaled distribution \(_{b}(|x^{t})\). If the temperature is zero, i.e., greedy decoding, the next token is determined by the maximum likelihood method i.e., \(x(t+1)=_{x}_{b}(x|x^{t})\), where \(\) is the domain of a single token also referred to as the vocabulary. The sampling approach can be further combined with other sampling primitives such as nucleus sampling  and top-\(k\) sampling .

All these approaches are autoregressive decoding2 methods, where tokens are generated serially one after another, which can be slow or even prohibitive in several applications . Hence, several techniques have been proposed to improve the speed of decoding. Before we proceed further, we first present some notations and a simplified computational model.

**Notations.** We use \(x^{i:j}\) to denote the sequence \(x(i),x(i+1),,x(j)\) and when \(i=1\), we simply use \(x^{j}=x^{1:j}\). \(x(i)\) denotes the \(i\)-th entry of \(x\). Subscripts are used to distinguish between different sequences, _e.g._, \(x^{1}_{i}\) and \(x^{2}_{i}\) denote two sequences of length \(t\). We use \([n]\) to denote the set \(\{1,,n\}\).

**A simplified computational model.**

* **Standard inference.** Given a context \(x^{t}\), with \(O(t^{2})\) computation and \(O(1)\) time, an autoregressive model \(_{b}\) can compute \(_{b}(y|x^{t})\), the (temperature-scaled) probability of all possible next tokens \(y\).
* **Parallelization along the time axis.** Given a context \(x^{t}\), with \(O(t^{2})\) computation and \(O(1)\) time, an autoregressive model \(_{b}\) can compute \(_{b}(y|x^{i})\), for all \(y\) and \(i\{1,2,,t\}\).
* **Parallelization along time and batch axis.** Let \(K\) be the maximum batch size that can be used during the inference of the autoregressive model. Given several contexts, \(x^{t}_{1},x^{t}_{2}, x^{t}_{K}\), with \(O(Kt^{2})\) computation and \(O(1)\) time, an autoregressive model \(_{b}\) can compute \(_{b}(y|x^{i}_{j})\), for all \(y\), \(i[t]\), and \(j[K]\).3

The above computation model shows that parallelizing along time and batch axes does not increase the computation time. It is a simplified characterization of the typical hardware, such as TPUs and GPUs, used in neural network inference. Previous approaches also assume similar computational model to devise faster decoding algorithms [19; 4]. In practice, there will be some overhead depending on hardware, implementation and resource utilization. In Appendix E, we experimentally verify that the theoretical gains are largely preserved for a large transformer model in practice. We also note that there are efficient transformer architectures, which reduces the computation cost from \(O(t^{2})\) to \(O(t t)\) (see  for a detailed survey). Such approaches are orthogonal to the focus of this paper, and they can be easily combined with our approach.

Broadly speaking, multiple previous approaches proposed to guess a few possible future tokens using an efficient model. They then compute several conditional probability distributions from the large model based on the guesses. Computing the distributions takes \(O(1)\) time due to parallelization along the time axis. The guessed tokens are then accepted or rejected based on a statistical method such that the accepted tokens are effectively samples from the large model. This guarantees that there is provably no degradation in the quality of the decoded output compared to that of the large model. When the guesses are plausible under the large model, multiple tokens will be accepted, leading to a larger gain in latency improvement. We will further characterize the acceptance probability as a function of the closeness of the distributions of large model and the small model. While this approach incurs the same computation cost as vanilla decoding (under the simplified computational model assumed in this paper), it can significantly improve decoding latency due to parallelization.

The goal of this work is to provide a principled understanding of the above approaches and discuss optimality conditions and algorithmic improvements. We start by providing a more formal overview of speculative decoding and related works.

## 2 Previous works and speculative decoding

Previous approaches make use of parallelization along the time axis to provide speedups. They first predict multiple tokens and validate if these multiple tokens can be generated by the model with the corresponding sampling or decoding scheme. For greedy decoding, multiple tokens can be predicted by a separate model , aggressive decoding , or retrieval augmented text . For sampling, recently [19; 4] proposed an algorithm called speculative decoding, and we provide an overview of this algorithm in the rest of the section. Suppose we have access to a computationally-inexpensive draft model \(_{s}\), which predicts the next token given the context, and the predictions of \(_{s}\) are close to that of \(_{b}\) for most contexts. Suppose we have obtained prefix \(x^{t}\). The next iteration of the speculative algorithm can be broken down into three steps (see Fig. 1 for an illustration).

1. **Draft construction.** The draft model is used to efficiently and "speculatively" sample \(L\) tokens, \((t+1),,(t+L)\). We keep the conditional probabilities on the next token \(_{s}(y x^{t},^{t+1:t+i})\) for each \(i<L\) and \( y\).
2. **Conditional probability computation.** After observing the samples, we compute the conditional distributions \(_{b}(y x^{t},^{t+1:t+i})\) for each \(i L\) and \( y\) in parallel (along time axis) in \(O(1)\) time.
3. **Draft selection.** Validate and select first \(L^{}\) of the \(L\) tokens and set \(x(t+i)=(t+i)\) for \(i L^{}\) given the draft sequence and the conditional probabilities from both models. Sample a token \((t+L^{}+1)\) from a _residual_ distribution as a _correction_ to the rejected token.4 
After this step, we use \(x_{1}^{t+L^{}+1}\) as the next context and sample the next few tokens using speculative decoding iteratively. For a complete statement of the algorithm, we refer the readers to . The crux of the above steps is draft selection, which given a draft sequence and the conditional probabilities from both models, selects a valid sequence such that the output has the same distribution as that of the large model. In speculative decoding, this is achieved via recursively applying a token-level maximal coupling algorithm, which is provided in Algorithm 1. Note that for the draft selection, Algorithm 1 is applied where \(p\) is the conditional distribution of the draft model \(_{s}( x^{t})\) and \(q\) is the conditional distribution of the large model \(_{b}( x^{t})\) (which may be further conditioned on the newly decoded tokens).

```
0: Distributions \(p,q\), Draft sample \(X p\).
1: Compute the residual distribution \(p^{}\) where \( x,p^{}(x)=}{1-_{x^ {}}\{p(x^{}),q(x^{})\}}\).
2: Sample \( U(0,1)\).
3:if\((1,)\)then
4: Return \(Y=X\). {Accept the draft token.}
5:endif
6:Return \(Y p^{}\). {Sample a corrected token from the residual distribution.} ```

**Algorithm 1** Token-level maximal coupling

Algorithm 1 returns a random variable \(Y\) which either is the accepted input \(X\) or a sample from the residual distribution \(p^{}\), which is defined in Step \(1\) of Algorithm 1. The algorithm is recursively applied as long as the draft tokens are accepted to select the first \(L^{} L\) tokens from the draft model. For the first rejected token, the sample \(Y\) from the residual distribution is used as a _correction_. Previous works showed that if \(X p\), then \(Y q\). In the case of the draft selection, this means that the output of the algorithm is distributed according to \(_{b}( x^{t})\), which is exactly the

Figure 1: One iteration of speculative decoding . Tokens in blue are decoded tokens from previous iterations, which are used as context for the current iteration. Tokens in red are drafts from the small model based on the context. The underlined tokens are the newly decoded tokens in the current iteration, where underlined red tokens represent tokens selected from the draft and underlined green token is selected from the residual distribution.

desired outcome. Furthermore

\[(Y=X)=_{x}(p(x),q(x))=1-d_{}(p,q),\]

where \(d_{}\) is the total variation distance between \(p\) and \(q\). The closer \(p\) and \(q\) are in \(d_{}\), the higher the chance of \((Y=X)\), and fewer the number of serial calls to the larger model. In the ideal case, if \(p=q\), then \((Y=X)=1\), i.e., the draft token is always accepted, and when used for speculative decoding we have \(L^{}=L\). Together with the extra sampled token5 from \(_{b}\), \(L+1\) tokens are obtained in one iteration. In such a case, based on our computational model (Section 1), assuming the decoding time of draft model is negligible, the speedup is \((L+1)\) times.

## 3 Our contributions

From a theoretical viewpoint, the speculative decoding algorithm raises multiple questions.

* What is the relationship between speculative decoding and the broader literature of sampling in statistics?
* Is speculative decoding optimal in an information-theoretic sense?
* Speculative decoding uses parallelization along time to speed up decoding; would it be possible to use parallelization along batch (number of drafts) to further improve decoding speed?

We provide answers to all the above questions in this work. We first relate the problem of speculative decoding to the broader and well-studied discrete optimal transport theory through a token-level coupling problem (Section 4). With this connection, it becomes clear that the token-level draft selection is the optimal solution for optimal transport with indicator cost function and also related to the problem of maximal coupling . Based on the connection to optimal transport, we show that one can further speed up the decoding by parallelizing along the batch axis by using multiple drafts from the draft model (Section 5).

More precisely, we formulate the token-level draft selection problem as a discrete optimal transport problem with membership cost, which is referred to as OTM. Discrete optimal transport can be solved with a linear program, but the number of variables is exponential in batch size, which can be prohibitive. To address this, we propose a valid transport plan that can be efficiently computed. Moreover, it achieves a \((1-1/e)\)-approximation of the optimal acceptance probability (Section 6).

With the theoretically motivated algorithms and guarantees, we circle back to speeding up decoding and propose a new algorithm called _SpecTr_ and theoretically show that it can be used to derive valid sequences from the large model with better speedups (Section 7). See Fig. 2 for an illustration of _SpecTr_. Compared to speculative decoding (Fig. 1), the main difference lies in the number of sampled drafts sampled from the small model and the selection algorithm that selects a valid sequence from multiple draft sequences. We remark here that the latter requires completely new statistical

Figure 2: One iteration of _SpecTr_. Tokens in blue are decoded tokens from previous iterations, which are used as context for the current iteration. Tokens in red are drafts from the small model based on the context. The underlined tokens are the newly decoded tokens in the current iteration, where underlined red tokens represent tokens selected from the draft and underlined green token is selected from the residual distribution. See Fig. 3 for a more detailed run of the draft selection step.

tools, and the connection between the token-level draft selection and OTM is critical for obtaining valid transport plans with good guarantees. We view this as one of the main contributions of the work. Similar to speculative decoding, there is provably no degradation in the quality of the decoded output compared that of the large model.

We then experimentally demonstrate the benefit of our approach on standard datasets (Section 8). More precisely, we show that for state-of-the-art large language models, _SpecTr_ achieves a wall clock speedup of 2.13X, a further 1.37X speedup over speculative decoding on standard benchmarks.

## 4 Token-level draft selection and optimal transport

In this section, we focus on the draft selection step of _SpecTr_. We start by considering the case when \(L=1\), which is a token-level draft selection problem. In particular, given context \(x^{t}\), let \(X_{1}, X_{k}\) be a collection of draft tokens sampled from the small model, _e.g.,_sampled _i.i.d._ from \(_{s}( x^{t})\). Note that by our assumption of the computation model, we could compute the following conditional probabilities from the large model in parallel ( along time and batch axes):

\[_{b}( x^{t}) i[k], _{b}( x^{t},X_{i}).\]

The goal of the draft selection algorithm \(f:^{k}\) is to output \(Y=f(X^{k})\), whose distribution follows \(_{b}( x^{t})\), and hence is a valid sample from the large model. Moreover, when \(Y\{X_{1},,X_{k}\}\), we could sample an extra token from \(_{b}( x^{t},Y)\) without calling \(_{b}\) since we have already computed the conditional probabilities \(_{b}( x^{t},Y)\). Hence we would like to maximize the probability that we accept one token from the set of drafts.

When \(L>1\), the drafts are sequences sampled from \(_{s}\), a sequence of token-level draft selection algorithms could be used along the time axis to select a valid sequence from the \(_{b}\). See an example in Fig. 3. The full details about the sequence-level selection algorithm is provided in Section 7.

The reminder of the section will be focused on the token-level draft selection problem. From the above discussion, there are the two main goals of the draft selection problem.

* **Validity.** The output token is always a valid token from the large model _i.e.,_ its distribution follows the conditional probability of the large model. This guarantees that there is no quality degradation compared to the large model.
* **Maximizing acceptance.** The higher the probability that we accept a draft token, the more serial computation we can save through parallelization, and hence better speedup.

Before proposing our framework to achieve the above goals, we would like to first discuss the technical challenge of draft selection with multiple draft tokens. One attempt is to sequentially apply the acceptance phase of Algorithm 1 (line 3 - 5) to each draft token \(X_{i}\) with \(p=_{s}( x^{t})\) and \(q=_{b}( x^{t})\). However, this approach would not guarantee that the final accepted token is from the desired distribution. To see this, consider the example of \(p=(1)\) and \(q=(1/2)\).6 Then we have \( i=1,,k\), \(X_{i}=1\) and each of them will be accepted with probability \(1/2\). After

Figure 3: An example run of the sequence-level draft selection in _SpecTr_ with \(L=4\) and \(4\) draft sequences. In the first step, there are \(4\) drafts tokens, and the token-level draft selection algorithm selects the word ‘_be_’ which appeared thrice. Note that all tokens following ‘_be_’ are valid draft tokens from the small model. In the second step, there are 3 drafts and the selection algorithm selects ‘_liked_’. The next token-level selection algorithm will have two drafts (‘_by_’ and ‘_for_’) and it selects ‘_by_’. Finally, there is only one draft following ‘_by_’, and the selection algorithm doesn’t select it and outputs ‘_three_’ as a correction. The process ends and a total of \(4\) tokens are generated.

applying Algorithm 1 to all \(X_{i}\)'s, the probability of getting a \(1\) will be at least \(1-1/2^{k}\) and hence the output distribution would not be \((1/2)\) for \(k>1\). Therefore the algorithm does not produce valid samples, which is a requirement of the draft selection problem.

In this work, we conduct a principled investigation of the draft selection problem, and show that these two main goals could be captured by the framework of optimal transport with a properly defined cost function. Next we define optimal transport formally and then connect it to draft selection with one draft. The generalization to multiple drafts is provided in Section 5.

Coupling and optimal transport.To simplify notations, we assume \(\) is a discrete domain.

**Definition 1** (Coupling).: For two probability distributions \(P\) over \(\) and \(Q\) over \(\), we say a joint distribution \(\) supported over \(\) is a coupling between \(P\) and \(Q\) if \( x,y,(x,y) 0\),

\[ y,\ \ _{x}(x,y)=Q(y), x,\ \ _{y}(x,y)=P(x).\]

We use \((P,Q)\) to denote the set of all possible couplings between \(P\) and \(Q\).

When it is clear from context, we will overload notation and refer to the probabilistic mapping \(f_{}:\) introduced by the conditional probability \((y x){:=}(x,y)/P(x)\) as a coupling, which is also referred to as a transport plan from \(P\) to \(Q\). In this paper, we will set \(P\) to be the distribution of the draft tokens and \(Q\) to be the target distribution of the output token. In this case, the \(f_{}\) is a valid draft selection algorithm. Formally, this is stated in the claim below.

**Claim 1**.: _For all \((P,Q)\), let \(f_{}\) be the probabilistic mapping defined above. If \(X P\), then \(f_{}(X) Q\)._

In this paper, we will design selection algorithms by finding valid couplings between the draft distribution and target distribution to guarantee validity of the output tokens.

**Definition 2** (Optimal Transport (OT) ).: For a cost function \(c:_{+}\), the _transportation cost_ of a coupling is defined as:

\[C()=_{X,Y}[c(X,Y)].\]

The _optimal transport plan_ is the coupling \((P,Q)\) that minimizes the transportation cost.

Speculative decoding with one draft token.With these definitions in place, we can see that with \(==\), the domain of the tokens and \(P=p,Q=q\), we recover the speculative decoding objective with one draft token using the cost function of _indicator cost_, which captures the resampling cost, defined below:

\[ x,\ y, c(x,y)=\{y x \}.\]

The transportation cost of the coupling will be \(C()=_{X,Y}[\{Y X\}]= _{X,Y}(Y X)\). This optimal transport cost is known to be

\[_{(p,q)}_{X,Y}(Y X)=_{x}( p(x),q(x)),\] (1)

which is achieved by the maximal coupling between \(p\) and \(q\) stated in Algorithm 1. And hence speculative sampling achieves the optimal cost with one draft token.

## 5 Optimal transport with multiple draft tokens

In this section, we generalize token-level selection to allow for multiple drafts. More formally, let \(=^{k}\) for some \(k_{+}\), which is the space of \(k\) draft tokens from \(\) and \(=\), which is the space of the final sampled token from the desired distribution. To characterize the resampling cost with multiple draft tokens, we use the cost function of _membership cost_, defined below:

\[ x^{k},\ y, c(x,y)=\{y S (x)\},\]

where \(S(x)=\{o ox\}\) denotes the set of distinct elements in \(x\). When \(k=1\), it recovers the indicator cost mentioned before. The transportation cost of the coupling is

\[C()=_{X,Y}[\{Y S(X)\} ]=_{X,Y}(Y S(X)).\] (2)We will also refer to the above cost \(C()\) as the _rejection probability_ due to its probabilistic interpretation. And similarly, \((){:=}1-C()=_{X,Y}(Y S(X))\) will be the _acceptance probability_.

From now on we will use membership cost as the default cost function and refer to the optimal transport solution as _optimal transport with membership cost_ (OTM). We use \(^{*}\) to denote the coupling that minimizes this cost \(^{*}=_{(P,Q)}C()\);7 and the cost \(C(^{*})\) is referred to as the _optimal transport cost_ between \(P\) and \(Q\). We use \((P,Q)=1-C(^{*})\) to denote the corresponding optimal acceptance probability.

Draft selection with _i.i.d._ draft tokens.In this paper, we will mainly focus on the case when the draft tokens are _i.i.d._ samples from a base distribution.8 Let \(p,q\) be supported over \(\) and the goal is to obtain one valid token from \(q\) given \(k\)_i.i.d._ samples from \(p\). For _SpecTr_ with context \(x^{t}\), we have \(p=_{s}( x^{t})\) and \(q=_{b}( x^{t})\). We set \(P=p^{ k}\), a product distribution whose marginals are all \(p\), and \(Q=q\). The OT problem we want to solve is the following:

\[ C()\ \ s.t.\ \ ({p^{ k}},q).\] (3)

We overload notation and denote the _optimal acceptance probability_ as \(_{k}(p,q){:=}({p^{ k}},q)=1-C(^{*})\). To better understand the quantity, we state a few properties about \(_{k}\).

**Lemma 1**.: _(Appendix A.2) The optimal acceptance probability satisfies the following properties._

* _Monotonicity. For any_ \(p,q\) _and_ \(k 1\)_,_ \(_{k}(p,q)_{k+1}(p,q)\)_._
* _Consistency. If_ \( x\)_,_ \(q(x)/p(x)\) _is bounded, we have_ \(_{k}_{k}(p,q)=1\)_. Else,_ \(_{k}_{k}(p,q)=_{x}\{p(x)>0 \}q(x)\)_._

The above properties demonstrate that for a large \(k\), the value of \(_{k}\) can become large. Hence increasing \(k\) could increase the acceptance probability, leading to further speedups. We now focus on computing the optimal transport plan and the optimal acceptance probability.

**OTM via Linear programming.** Optimal transport in discrete domain has been studied extensively , and it is shown that the optimal transport problem is equivalent to the following linear programming problem:

\[\ _{x^{k}}_{y}\!\!(x,y) \{y S(x)\} s.t.\ \ (P,Q).\] (4)

The linear program in (4) has \(||^{k+1}\) variables and \(||^{k}+||\) equality constraints (see Definition 1). Linear programming can be solved in time polynomial in the number of variables and constraints ,9 implying the following lemma.

**Lemma 2**.: _Given \(p,q\) over \(\), the solution to Eq. (3) can be computed in time \(O(||^{O(k)})\)._

We refer to the optimal coupling obtained above as OTM-\(k\) and denote it as \(^{ OTM-}k\). When \(k=1\), there is a closed form expression for the optimal acceptance cost (see Eq. (1)), whereas for larger values of \(k\), we are unaware of a general closed form expression. In Appendix A.1, we provide an information-theoretic upper (and lower) bound, which is tight up to a multiplicative constant of \(1-(1-1/k)^{k} 1-1/e\).

While solving OTM in Eq. (4) gives the plan with optimal acceptance probability, to the best of our knowledge, the best-known runtime will be exponential in \(k\), which can be prohibitive when either the vocabulary size \(||\) or the number of draft tokens \(k\) is large.10 In the next section, we will present a selection algorithm that can be efficiently computed and show that it achieves an acceptance probability of at least \((1-(1-1/k)^{k})_{k}(1-1/e)_{k}\).

Draft selection via \(k\)-sequential selection

In this section, we present a sequential selection algorithm (k-Seq), an approximate solution11 to the optimal transport problem in Eq. (3), which can be efficiently computed in time almost linear in \(||\) and logarithmic in \(k\). The algorithm is presented in Algorithm 2.

```
0: Distributions \(p,q\), samples \(X_{1},,X_{k}_{i.i.d.}p\). \([1,k]:\) division factor.
1: Let \(_{p,q}()=_{x}(p(x),q(x)/)\) and \(p_{}=1-(1-_{p,q}())^{k}\). Compute \(p^{}\) where \[ x,p^{}(x)=\}}}{_{p,q}()}}{1-p_{}}.\] (5)
2:for\(i=1,2,,k\)do
3: Sample \(_{i} U(0,1)\).
4:if\(_{i}(1,)}{^{p}(X_{i})})\)then
5: Return\(Y=X_{i}\). {Return the \(i\)th draft token.}
6:endif
7:endfor
8:Return\(Y p^{}\). {Sample a corrected token from the residual distribution.} ```

**Algorithm 2**\(k\)-sequential selection algorithm (k-Seq).

At a high-level, the algorithm goes over all \(k\) draft samples generated from \(p\) sequentially, and decides on whether to accept each \(X_{i}\) based on the ratio \(q(X_{i})/p(X_{i})\). The algorithm output the first accepted sample or result from a residual distribution \(p^{}\) if none of the samples is accepted. To guarantee that the the final returned token is a valid sample from \(q\), we choose an appropriate \([1,k]\) and accept \(X_{i}\) with probability \((1,q(X_{i})/( p(X_{i})))\) instead of \((1,q(X_{i})/(p(X_{i})))\) as in Algorithm 1. In Theorem 1, we show that with appropriately chosen \(\)'s, Algorithm 2 is indeed valid transportation plans from \(p^{ k}\) to \(q\). Moreover, to find the best transportation plan within the family, we only need to search over a single parameter \(\), which reduces the computation cost significantly. We also show that searching over this sub-family of couplings won't decrease the optimal acceptance probability by a multiplicative constant. The performance of Algorithm 2 is stated in Theorem 1.

**Theorem 1**.: _Let \(_{p,q}()=_{x}(p(x),)\) and \(^{*}\) be the solution to the identity below._

\[1-(1-_{p,q}())^{k}=_{p,q}().\] (6)

_When \(^{*}\), the coupling \(_{}^{}\) in Algorithm 2 is a valid transport plan from \(p^{ k}\) to \(q\). When \(=^{*}\), we have_

\[(_{^{*}}^{})(1-e^{-1})_{k}(p,q).\]

_Moreover, \(^{*}\) can be computed up to accuracy \(\) in time \(O(||((k-1)/))\)._

We provide the proof in Appendix C.1. In Appendix B, using a few canonical examples of distributions, we plot the acceptance probability of k-Seq and compare it with the optimal acceptance probability \(_{k}\). It can be shown that k-Seq could have a strictly worse acceptance probability compared to the OTM solution for certain cases while there also exist non-trivial cases where k-Seq achieves the optimal acceptance probability.

Concurrent and recent work of [20; 29] has proposed another efficient algorithm for the draft selection phase. To the best of our knowledge, there is no optimality guarantee proved for their proposed algorithm. In Appendix B.3, we present its acceptance probability empirically for the canonical case of Bernoulli distributions, and show that both our proposed algorithms (OTM and k-Seq) have a higher acceptance probability.

## 7 SpecTr: Application of OTM in autoregressive sampling

In this section, we describe how OTM can be used to speed up auto-regressive sampling, which we refer to as _SpecTr_ sampling. Similar to speculative decoding, each iteration of _SpecTr_ can be decomposed into three phases (Fig. 2):1. **Draft set construction.** Given current context \(x^{t}\), use the draft model sample a set of \(K\) draft sequences with length \(L\), denoted by \(S=\{z^{L}_{s}( x^{t})\}\). We keep the conditional probabilities \(_{s}(y x^{t},z^{i})\) for all \(y,i L\) and \(z^{L} S\).
2. **Conditional probability computation.** Compute the conditional probabilities on the next token for the large model \(_{b}(y x^{t},z^{i})\) for all \(y,i L\) and \(z^{L} S\) in parallel.
3. **Draft selection.** Select first \(L^{}\) of the \(L\) tokens and set \(x(t+i)=z(i)\) for \(i L^{}\) and some \(z S\) given the set of draft sequences and the conditional probabilities from both models. Sample a token from a _residual_ distribution as a correction to the rejected tokens.

The conditional probability computation step takes \(O(1)\) when \(|S|\) is not large based on our simplified computations model. We mainly focus on the draft set construction phase and draft selection phase.

```
0: Input sequence \(x^{t}\); draft sequence length: \(L\); draft sequences \(S=\{z_{i}^{L} i|S|\}\).
1: Compute a transport plan (using linear programming in Lemma 2 for an optimal solution or Algorithm 2 for a suboptimal solution) from \(_{s}( x^{t})^{|S|}\) to \(_{b}( x^{t})\), denoted by \(_{t}\).
2: Get the multi-set of next token-level drafts: \(S_{z}=\{z_{i}(1)\}_{i|S|}\) and compute \(Y=f_{_{t}}(S_{z})\).
3:if\(L=1\)then
4:if\(Y S_{z}\)then
5: Sample \(Y^{}_{b}((x^{t},Y))\).
6: Return\((x^{t},Y,Y^{})\). {Sample an extra token if the last token is accepted.}
7:else
8: Return\((x^{t},Y)\). {Stop and return the corrected token and previous accepted tokens.}
9:endif
10:endif
11: Let \(S_{}=\{z^{2:L} z Sz(1)=Y\}\) be the set that consists of sub-sequences of the candidates that agree with the selected next token.
12:if\(S_{}=\)then
13: Return\((x^{t},Y)\). {Stop and return the corrected token and previous accepted tokens.}
14:else
15: Return\(((x^{t},Y),L-1,S_{})\). {Keep the draft token and proceed to the next time step.}
16:endif ```

**Algorithm 3** Draft selection with multiple candidates (DraftSelection).

**Draft set with _i.i.d._ draft sequences.** Given context \(x^{t}\), a natural way to come up with a set of \(K\) drafts is to independently sample \(K\) draft sequences from \(_{s}( x^{t})\), _i.e.,_

\[z_{1}^{L},z_{2}^{L},,z_{K}^{L}_{i.i.d.}_{s}(,,  x^{t}).\] (7)

The draft set construction method in (7) can be generalized to a prefix-tree based algorithm. However, this generalized version did not perform better in our experiments. We include this construction in Appendix D for completeness.

**Draft selection with multiple candidates.** We present the sequence-level selection algorithm given a set of draft sequences in Algorithm 3. We assume the conditional probabilities on the next token are available given any prefix in the candidate set since they are computed in parallel in the second phase, and won't list them as inputs explicitly in Algorithm 3.

A sample run of the algorithm is presented in Fig. 3. The algorithm proceeds in a recursive fashion. Given prompt \(x^{t}\) and a candidate set \(S\) sampled from \(_{s}( x^{t})\), the algorithm first computes a token-level draft selection algorithm \(f_{}:^{|S|}\) which is a transport plan from \(_{s}( x^{t})^{|S|}\) to \(_{b}( x^{t})\). Then \(f_{}\) is applied to the set of first tokens of the draft sequences in \(S\) to obtained a valid token \(Y\) from \(_{b}( x^{t})\). If \(Y\) is not the last token (\(L 2\)), we filter out sequences in \(S\) whose first token is not \(Y\) and denote the remaining sequences as \(S_{}\) and feed it to the algorithm with context \((x^{t},Y)\) and draft length \(L-1\). This goes on until we have \(L=1\) or \(S_{}=\).

In this case when \(Y\) is the last token (_i.e.,_\(L=1\)) and \(Y S\), we have the choice to sample an additional token \(_{b}((x^{t},Y))\) since this conditional probability is already computed in the second phase. Due to the property of the token-level selection algorithms and the autoregressive structure of language models, it can be shown that \(Y\) is always a valid sample from \(_{b}( x^{t})\). Let \(L^{}\) be the number of decoded tokens in one iteration. Note that this is a random variable in the range \([1,L+1]\).

The formal quality guarantee is stated in Theorem 2. We present the proof in Appendix C.2.

**Theorem 2**.: _Assume all drafts in the set \(S\) are generated from the small model with input \(x^{t}\), or more precisely, \( z S,\)_

\[ i[1,L], z(i)_{s}( x^{t},z^{i-1}).\] (8)

_Let \((x^{t},Y^{})\) be the output of Algorithm 3 where \(\) is the length of the newly decoded tokens, then it satisfies that \(Y^{1:}\) is distributed according to \(_{b}(_{dct}} x^{t})\). More precisely, For any \(_{0}[1,L+1]\), and any \(_{0}\)-length, sequence \(o^{_{0}}=(o(1),,o(_{0}))^{_{0}}\), we have_

\[(Y^{_{0}}=o^{_{0}}|=_{0})=_{i=1}^{_{0}} _{b}(o(i) x^{t},o^{i-1}).\]

## 8 Experiments

We empirically evaluate _SpecTr_ and compare it with two methods: (1) the baseline auto-regressive decoding; and (2) speculative decoding with \(K=1\). Note that all three methods effectively generate samples from the same baseline large model, and hence the quality of the two speculative decoding methods is _provably_ neutral to that of the large model. Thus, we will only focus on measuring the speedup in our experiments. In the simplified computation model, we made the following assumptions: (1) Decoding time from small models is negligible compared to decoding from the small model; (2) Parallelization along the batch and time axis doesn't increase the time for a serial call to the large model. With these, the theoretical speedup compared to baseline decoding will be the average number of decoded tokens per serial call, which is called _block efficiency_, defined below

\[:=}{_{b}}.\]

However, in real deployment of the _SpecTr_ algorithm, the actual end-to-end (wall clock) speedup is further impacted by the following aspects. (1) The decoding time for \(_{s}\) might not be negligible; (2) Parallelization along the batch and time axis might increase the time for a single call to \(_{b}\); (3) Overhead due to the implementation of additional functionalities in _SpecTr_ such as the draft selection algorithm and switching between models. These factors will depend on how the algorithm is implemented and optimized. In our experiment, we consider both the block efficiency, and average wall clock speedup with our implementation of _SpecTr_.

We first present the performance of our algorithm and compare it to speculative decoding using state-of-the-art PALM-2 models with prompts from the one-billion language benchmark (LM1B) . In Appendix E, we use a pair of smaller transformer models to break down different affecting factors mentioned above.

In Table 1, we use PALM-2-Gecko and PALM-2-Bison as the small model and large model, respectively . The wall clock speedup is normalized by the wall clock latency of baseline autoregressive decoding. The time we log include all above mentioned aspects. In the considered parameter configurations, the wall clock speedup increases as \(K\) and \(L\) increases. As seen from the table, the actual wall clock speedup is smaller than the theoretical speedup of block efficiency, which is consistent with what we expected. Importantly, the benefit from _SpecTr_ outweighs these overheads. In particular, when \(L=8\) and \(K=8\), our proposed _SpecTr_ algorithm has a speedup of 2.13x, a further 1.37x increase compared to speculative decoding (\(K=1\)).

 Algorithm & \(K\) & \(L\) & Block efficiency & Relative wall clock speedup \\  & & & & (normalized by baseline) \\  Baseline & - & - & \(1.0\) & \(1.0\) \\  Speculative & 1 & 4 & \(2.4\) & 1.67 \\ _SpecTr_ & 8 & 4 & \(\) & \(\) \\  Speculative & 1 & 8 & \(2.9\) & 1.56 \\ _SpecTr_ & 8 & 8 & \(\) & \(\) \\ 

Table 1: Experimental results on the LM1B dataset with PALM-2-Gecko as the small model and PALM-2-Bison as the large model. Results are averaged over \(1000\) test prompts and \(3\) random seeds.

Acknowledgements

Authors thank Asaf Aharoni, Kwangjun Ahn, Badih Ghazi, Sanjiv Kumar, Teodor Marinov, Michael Riley, and NeurIPS reviewers for helpful comments and discussions.