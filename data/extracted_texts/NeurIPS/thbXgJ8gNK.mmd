# No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models

Jean Kaddour\({}^{1}\)1 Oscar Key\({}^{1}\)1 Piotr Nawrot\({}^{2}\) Pasquale Minervini\({}^{2}\) Matt J. Kusner\({}^{1}\)

\({}^{1}\)Centre for Artificial Intelligence, University College London

\({}^{2}\)School of Informatics, University of Edinburgh

{jean.kaddour.20, oscar.key.20, m.kusner}@ucl.ac.uk

{piotr.nawrot, p.minervini}@ed.ac.uk

Equal contribution, alphabetical order.

###### Abstract

The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: **dynamic architectures** (layer stacking, layer dropping), **batch selection** (selective backprop, RHO loss), and **efficient optimizers** (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call _reference system time_. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.

## 1 Introduction

Language models are advancing rapidly, surpassing human-level performances in some experimental setups . These improvements are primarily due to model, data, and training budget scaling . Training a single state-of-the-art language model requires hundreds of thousands of GPU hours, costs millions of dollars, and consumes as much energy as multiple average US family households per year .

To remedy this, there is a rapidly growing line of work on _efficient training_ algorithms, which modify the training procedure to save computation . Specifically, three distinct classes of such algorithms are **dynamic architectures** (layer stacking  and layer dropping ) which ignore some of the weights during training, **batch selection** (selective backprop  and RHO loss ) which skip irrelevant data, and **efficient optimizers** (Lion  and Sophia ) which claim to convergence faster than Adam(W) .

However, we find that the evaluation methodology is not standardized, and there are inconsistencies in the quantification of a "speed-up". Two general trends are: comparisons of (1) **intermediate performances** throughout training instead of final ones within a pre-defined training budget, and (2) **incomplete speedup metrics**, e.g., training progress as a function of the number of iterations (or epochs) instead of wall-clock computation times _despite unequal per-iteration costs_.

As pointed out by Dehghani et al. , Dahl et al. , this can be unfair to baselines, which were not tuned for efficiency within the same compute budget.

An example for (1) includes learning rate schedules, which can be arbitrarily stretched to improve the final performance while "sacrificing" the quality of intermediate checkpoints . This can make comparisons of intermediate performances unfair to baselines. For (2), additional regularization techniques can improve the per-iteration convergence at higher per-iteration costs , rendering a wall-clock time comparison more appropriate.

In this paper, we propose a simple evaluation protocol for comparing speedups of efficient training algorithms. We use this protocol to evaluate these algorithms for pre-training Transformer-based language models from scratch. We compare different training budgets (6, 12, and 24 hours), model architectures (BERT-Base  and T5-Base ), and (for batch selection algorithms) datasets (C4 , Wikipedia and BookCorpus , and MiniPile ). To account for variability in measured wall-clock time on different hardware and software configurations, we propose a simple measure that we call _reference system time_ (RST).

Our key findings are as follows:

* **Training loss** (layer stacking, layer dropping, Lion, Sophia): The only approach to consistently outperform the training loss of the fully-decayed learning rate baseline across budgets and models is layer stacking (Lion matches this performance for certain BERT training budgets). This improvement reduces as the budget increases to 24 hours.
* **Validation loss** (selective backprop, RHO loss): Across three training datasets, none of the batch selection methods outperform the validation loss of the baseline.
* **Downstream tasks**: For a 24-hour budget, none of the efficient training algorithms we evaluate improves the downstream performance of the baseline.
* Methods with lower per-iteration costs than the baseline (i.e., **dynamic architecture** methods: layer stacking, layer dropping) can slightly improve downstream performance for lower budgets (6 hours, 12 hours), but the improvement disappears with longer training.
* Methods with higher per-iteration costs (i.e., **batch selection** methods: selective backprop, RHO loss, and some **efficient optimizer** methods: Sophia) are significantly worse than the baseline in some downstream tasks (GLUE, SNI), for all budgets.
* If we ignore the additional per-iteration computations of the three above methods, the downstream performance is still matched by the baseline.

    &  &  \\   & 6h & 12h & 24h & 6h & 12h & 24h \\  Baseline & **77.1 \(\) 0.2** & 77.8 \(\) 0.2 & **78.3 \(\) 1.1** & **57.8 \(\) 0.9** & **58.5 \(\) 1.1** & **58.6 \(\) 1.2** \\
**Layer stacking** & **76.8 \(\) 0.8** & **78.4 \(\) 0.4** & **79.4 \(\) 0.2** & **58.6 \(\) 1.0** & 57.9 \(\) 0.7 & **58.7 \(\) 2.0** \\
**Layer dropping** & **76.8 \(\) 0.3** & 78.1 \(\) 0.2 & 78.6 \(\) 0.1 & **58.6 \(\) 0.8** & **58.5 \(\) 0.7** & **58.4 \(\) 0.8** \\ Selective backprop & 75.3 \(\) 0.6 & 76.6 \(\) 0.4 & 77.9 \(\) 0.3 & 57.4 \(\) 0.4 & **59.1 \(\) 0.9** & **58.5 \(\) 0.4** \\
**RHO loss** & 75.7 \(\) 0.1 & 76.5 \(\) 1.3 & 77.8 \(\) 0.2 & **57.6 \(\) 1.1** & 57.8 \(\) 0.6 & **58.7 \(\) 0.7** \\  Baseline (**BF16**) & **77.0 \(\) 0.3** & **77.8 \(\) 0.2** & **77.9 \(\) 0.3** & **57.6 \(\) 0.5** & **57.9 \(\) 0.5** & **57.9 \(\) 0.6** \\
**Lion (**BF16**)** & 62.0 \(\) 13.7 & 72.0 \(\) 0.5 & 71.4 \(\) 0.8 & **56.1 \(\) 2.5** & 57.5 \(\) 0.2 & **57.2 \(\) 2.3** \\
**Sophia (**BF16**)** & 73.9 \(\) 1.3 & 71.1 \(\) 4.2 & 72.3 \(\) 3.8 & **58.0 \(\) 0.6** & **57.8 \(\) 0.7** & **57.5 \(\) 0.7** \\   

Table 1: **Downstream performance, BERT. Results for efficient training methods on the GLUE and SuperGLUE dev sets for three budgets (6 hours, 12 hours, and 24 hours) after pre-training a carnamed BERT model . We report average validation of GLUE and SuperGLUE scores across all tasks (standard deviations for three seeds). We use mixed precision training with BF16 for the optimizer comparison as we found FP16 precision lead to numerical instabilities (Section 6).**

    &  \\   & 6h & 12h & 24h \\  Baseline & 34.2 \(\) 0.2 & 38.3 \(\) 0.4 & **39.5 \(\) 0.1** \\
**Layer stacking** & **38.9 \(\) 0.2** & **39.2 \(\) 0.1** & 38.9 \(\) 0.2 \\
**Layer dropping** & 31.5 \(\) 0.3 & 34.4 \(\) 0.8 & 38.0 \(\) 0.5 \\
**Lion** & 20.8 \(\) 1.1 & 30.7 \(\) 0.4 & 33.7 \(\) 0.0 \\
**Sophia** & 26.7 \(\) 0.8 & 31.5 \(\) 0.8 & 34.1 \(\) 1.1 \\   

Table 2: **Downstream performance, T5-Base . SNI  test ROUGE-L results (three seeds).**

## 2 Comparing Efficient Training Algorithms

What is the fairest way to compare efficient training algorithms? Ideally, each method should be given an identical compute budget, and methods that converge to better solutions than a baseline within this budget achieve a speed-up. Below, we discuss the issues of commonly used metrics to specify the compute budget, and propose an improved metric.

The Pitfalls of IterationsSpecifying a budget in number of training iterations is usually not suitable because the iteration time can vary between methods.

The Pitfalls of FLOPsWhile FLOPs count the number of elementary arithmetic operations, they do not account for parallelizability (e.g., RNNs vs Transformers) or hardware-related details that can affect runtime. For example, FLOP counting ignores memory access times (e.g., due to different layouts of the data in memory) and communication overheads, among other things [20; 8].

The Pitfalls of Wall-Clock TimeWCT can fluctuate even on the same hardware, for instance, due to the usage of non-deterministic operations 2, hidden background processes, or inconsequential configurations, such as the clock rate. Further, we would like a metric that allows researchers to run on shared compute clusters, where hardware configurations will vary.

Our Proposal: Reference System Time (RST)We propose a simple time measure that will allow us to standardize any timing result w.r.t. a reference hardware system (e.g., NVIDIA RTX 3090, CUDA runtime 11.8, PyTorch 2.0, etc.). We define the time elapsed on a reference system as _reference system time_ (RST). To convert the training run of an arbitrary device to RST, we first record the time per training iteration on the reference training system.3 Then, we compute the RST by multiplying the number of iterations run on the arbitrary device by the time per iteration on the reference system. This way, the time is grounded in practical time units but can be applied to any system.

### The Pitfall of Comparing Intermediate Performances

One difficulty with comparing efficient training methods stems from the importance of the learning rate schedule to model performance. Various works have demonstrated that deep learning models tend to reach their maximum performance only once the learning rate has been fully decayed [109; 76]. If the learning rate schedule is stretched, this can delay the number of iterations required for the model to reach a given performance threshold since the optimizer keeps "exploring" the local loss basin, wasting iterations oscillating [33; 42].

This leads to unfair comparisons when the learning rate schedule is not set fairly between methods; a non-obvious pitfall that has been discussed previously in the literature [50; 20], but which we still observed in several of the works we evaluated [30; 107; 39; 58]. Consider comparing a baseline B with an efficient training method X, where both methods are trained for the same number of iterations, and the learning rate is decayed in terms of the number of iterations. B and X reach a similar performance at the end of training, but because X completes the iterations in less WCT, a speed-up is declared.

However, this is not a fair comparison because the training budget was measured in iterations, but the performance of the models was evaluated at a point specified in WCT, specifically the time it took X to complete the budget. Thus, B is evaluated at an intermediate point during training when its learning rate schedule has not been fully decayed. If the learning rate schedule of B had been shortened so it finished at the same WCT as X, the two methods may have reached similar performances in the same WCT time. The same problem arises when the budget is specified using different units to the point of measurement, including the cases where FLOPs are used for the budget.

To avoid this issue, each method should be trained to a fixed budget with the performance measured at the end of this budget. Importantly, the learning rate and other schedules must be decayed as a function of the same unit used to measure the budget.

Experimental Setup

Given this computation measure, we can now evaluate efficient training algorithms under fixed computation budgets. We conduct experiments using two established and widely used Transformer language models: _BERT_ and _T5_. We choose a single-GPU setting following recent works [29; 36; 69] to facilitate reproducibility and access for compute-restricted researchers. However, in principle, our protocol can be run on any hardware and straightforwardly used in a distributed setup.

BERT: Encoder-only.We follow the setup and hyperparameters of Geiping & Goldstein  with minor modifications. We pre-train a BERT-base-like model with 16 layers instead of 12, which consists of 120M parameters, using a masked language modeling (MLM) objective. We use the AdamW optimizer  with hyperparameters \(_{1}=0.9\), \(_{2}=0.98\), \(=10^{-12}\), and weight decay of \(10^{-2}\). Further, we use a one-cycle learning rate schedule  with a peak learning rate of \(10^{-3}\) and gradient clipping of \(0.5\). The batch size is \(1536\) sequences, and the sequence length is \(128\). We fine-tune and evaluate the pre-trained BERT models using the GLUE  and SuperGLUE  benchmarks. For fine-tuning on GLUE, we use the hyper-parameters from Geiping & Goldstein . On SuperGLUE, we tune them using the validation accuracy of BoolQ . We found inconsistencies in the literature on how to aggregate the SuperGLUE scores; here, we average the following scores: for CB, we use the F1 score; for the rest (BoolQ, COPA, RTE, WiC, WSC), we use the accuracy.

T5: Encoder-Decoder.We pre-train a T5v1.1-Base [77; 82] model using the original span-corrupting MLM objective and SentencePiece  tokenizer. We follow Nawrot  and use the AdamW optimizer  with tensor-wise LR scaling by its root mean square (RMS), base learning rate \(0.02\), no weight decay, cosine schedule with final of \(10^{-5}\), gradient clipping of \(1.0\), and \(10^{4}\) warm up steps. We use a batch size of \(144\) examples, where each example consists of an input of \(512\) tokens and an output of \(114\) tokens. We evaluate our pre-trained T5 models on the Super-Natural-Instructions [SNI, 98] benchmark. To fine-tune the model on SNI, we strictly follow the hyperparameters of [69; 98]. For both pre-training and fine-tuning, we use TF32 precision.

Dataset.Unless specified otherwise, we use the C4  dataset for less than one epoch (i.e., without data repetitions) without sentence curriculum and de-duplication [53; 29]. In Section 5, we additionally use two other datasets.

Learning-rate schedule.We adjust the learning rate schedule based on the elapsed time conditional on a time budget (measured in RST), similar to [37; 29], who measure raw WCT.

Hyper-parameter search.For all considered methods, we tune their hyper-parameters based on the pre-training loss. We list all details about each method's hyper-parameters, our considered grid search ranges, and the best values we found in Appendix C.

Reference System.We record the RST on two separate systems for BERT and T5. For BERT, we choose a single NVIDIA RTX 3090, CUDA 11.7, PyTorch 1.13. For T5, we choose an NVIDIA A100, CUDA runtime 11.8, PyTorch 2.0.

## 4 Case Study 1: Dynamic Architectures

### Layer stacking

_Layer stacking_, as summarized in Algorithm 1, replicates a \(L\)-layer model into a \(2L\)-layer model by copying its parameters, effectively warm-starting the stacked model with parameters transferred from the smaller model. Thereby, it benefits from faster per-iteration times in the early training phases when using fewer layers. Gong et al.  attribute the success of this method to attention distributions of bottom layers being very similar to attention distributions of top layers, indicating that their functionalities are similar.

### Layer dropping

Layer dropping exploits the following observation: the layers of a network do not contribute equally to the loss reduction throughout training [35; 6; 106]. It does so by randomly choosing parts of the model to be skipped during each training step. Specifically, it replaces a subset of Transformer blocks with the identity function. As a result, it reduces the overall computational cost of the model for each training step because it skips these layers during the forward and backward passes.

To minimize the impact of layer dropping on the pretraining loss of the model, Zhang & He  employ a time and depth schedule that determines the probability of dropping each block. The time schedule begins with a zero probability of dropping each block. Then it increases this probability throughout the training process until it reaches a maximum of \((1-)\), where the hyperparameter \(=0.5\) as chosen by Zhang & He .

The depth schedule ensures that blocks located earlier in the model are dropped with a lower probability than those located later/deeper. An important hyperparameter of the depth schedule in layer dropping is \(_{f}\), which controls the rate at which the probability of dropping layers increases. A higher value of \(_{f}\) leads to a quicker increase in the probability. Zhang & He  set \(_{f}\) to 100 in their experiments. We refer the reader to the original work by Zhang & He  for more details.

### Results

Training losses.Figure 1 shows the pre-training losses for the baseline, layer stacking, and layer dropping on BERT and T5 when given a budget of 24 hours in RST. In both settings, layer stacking achieves the lowest loss within this budget. The gap between layer stacking and the baseline closes almost completely as the budget is increased to 24 hours. However, layer dropping is consistently worse than the baseline throughout training. In both models, the gap between layer dropping and the baseline is larger than between the baseline and layer stacking at the end of training. Further, layer dropping introduces additional fluctuations in the loss when training T5, compared to the baseline.

Downstream performance.Figure 2 shows the SuperGLUE downstream performance after using the baseline and all dynamic architecture methods (layer stacking and layer dropping) to pre-train

Figure 1: **Training losses, dynamic architecture methods** (layer stacking, and layer dropping). Results are shown for RST budgets of 6 hours (), 12 hours (), and 24 hours (), on C4.

BERT. We evaluate all methods using three different RST budgets (to get a sense of the Pareto-frontier of budget and performance ): 6 hours, 12 hours, and 24 hours. We notice that on specific SuperGLUE datasets, the efficiency methods have little or even detrimental effect on performance (CB, WiC, WSC, BoolQ). Across all datasets, COPA appears to have the largest gains from efficient training. Averaged across all datasets, efficient training methods have little effect. We report the exact numbers in Table 1 (right). On average, across all budgets, neither layer stacking and layer dropping significantly improve over the baseline.

We also compare the performance of dynamic architecture methods methods on the T5-base model evaluated on the SNI benchmark and report the results in Table 2. Here layer stacking significantly improves over the baseline for 6 and 12 hour budgets. However, given 24 hours for training, the final accuracy of layer stacking reduces back to the accuracy of the 6 hour model. Here the baseline significantly improves over all methods. Notably, for all budgets, layer dropping is consistently outperformed by the baseline.

## 5 Case Study 2: Batch Selection

Scaling up the size of web-crawled data for pre-training has been one of the major drivers to improve the capabilities of language models . A line of work has argued that training speed-ups emerge through the selection of _informative_ examples. They argue that these examples can be identified from certain statistics collected throughout training . Here, we focus on two such methods that directly alter the training procedure to steer gradient computations towards informative samples by subsampling examples from a _mega-batch_ to curate the mini-batch, called _batch selection_.

### Selective Backprop

Due to its simplicity, we first examine selective backprop , described in Algorithm 3. The high-level idea of selective backprop is to compute the backward pass only on the training examples with the highest loss. To construct such batches, it first computes the loss of every example in a

Figure 2: **BERT models evaluated on SuperGLUE. The black vertical error bars indicate the standard deviation over three seeds. The black horizontal line shows the baseline average performance. For clarity, the individual tasks are plotted against the left-hand axis, while the average accuracy is plotted against the right-hand axis.**

uniformly-sampled mega-batch. It then samples a high-loss subset of the mega-batch by ranking points based on their loss percentiles w.r.t. historical losses among recently-seen inputs.

### RHO Loss

Mindermann et al.  argue that solely prioritizing high training loss results in two types of examples that are unwanted: (i) mislabeled and ambiguous data, as commonly found in noisy, web-crawled data; and (ii) outliers, which are less likely to appear at test time. The authors propose down-weighting such data via a selection objective called _Reducible Holdout (RHO) loss_, shown in Algorithm 4.

```
1:Input: iterations \(T\), data \(\{(_{i},_{i})\}_{i=1}^{N}\), loss \(\), mini-batch size \(B_{m}\), mega-batch size \(B_{M}\), number of inputs to compute loss CDF \(R\), selectivity \(>0\), model \(\), loss history \(=()\)
2:for\(t 1\) to \(T\)do
3:\(_{m}\{\}\) {Initialize mini-batch.}
4:\(_{M}\{(_{i},_{i})\}_{i=1}^{N}\) {Select mega-batch.}
5:\(\{_{j}\}_{j=1}^{B_{M}}(_{M};)\) {Compute loss.}
6:for\(j 1\) to \(B_{M}\)do
7:\((_{j},)\) {Add to history}
8:\(p(_{j};_{\{1,,R\}})^{}\) {Selection prob.}
9:\(s(p)\) {Select or not.}
10:if\(s==1\)then
11:\(_{m}_{m}(_{i},_{j})\)
12:if\(|_{m}|==B_{m}\)then
13:\((,_ {m})\) {Backwards pass.}
14:\(_{m}\{\}\) ```

**Algorithm 4** RHO loss 

The authors acknowledge that their method comes with three overhead costs: (1) pre-training a proxy model using holdout data, (2) one extra forward pass over the entire training set to store the proxy model's training losses, and (3) additional forward passes for the batch selection. In our evaluations we never count the costs of (1) and (2) because, as Mindermann et al.  point out, these costs can be amortized over many training runs. For (1), we pre-train a model for 6 hours of RST on held-out data, which is enough to reach reasonable performances (Table 1).

Despite Mindermann et al.  motivating their method for a scenario where additional workers are available to perform batch selection, we wondered if it might still provide performance gains even if this is not the case. Mindermann et al.  do not implement or evaluate an algorithm where extra workers are used to select batches. Because of this, we evaluate RHO loss under two protocols: in the main results, we count the batch selection costs (3) against the training budget, while in Section 5.4, we provide a second set of results where we ignore the cost of batch selection.

### Results

We assume that the effects of selecting better training batches should be largely agnostic to whether we pre-train a BERT or T5 model. Hence, instead of training both architectures, we decide to pre-train only BERT models and instead vary the datasets and budgets as follows.

For the first set of experiments, we fix the budget to 12 hours and consider three different pre-training datasets: (i) C4 , consisting only of webpage text which, despite being regularly used for pre

    & **Val. Loss** & **GLUE** \\  Baseline & 2.21 & **77.79 \(\) 0.2** \\ Selective backprop & 2.23 & **77.92 \(\) 0.1** \\
**RHO loss** & **2.19** & 77.16 \(\) 0.4 \\   

Table 4: **Batch Selection For Free.** Results for a 12-hour RST budget on C4, removing all costs used to select batches.

    & **6h** & **12h** & **24h** \\  Baseline & **2.42 \(\) 0.00** & **2.20 \(\) 0.00** & **2.10 \(\) 0.01** \\ Selective backprop & 2.73 \(\) 0.18 & 2.44 \(\) 0.06 & 2.18 \(\) 0.02 \\   

Table 3: **Validation losses, batch selection methods** (selective backprop, and RHO loss). Results are shown for RST budgets of 6, 12, and 24 hours, on C4.

training, is known to have quality issues , (ii) Bookcorpus and Wikipedia , which contain polished, book(-like) text and MiniPile , a subset of the diverse Pile pre-training corpus , containing code, mathematics, books, webpages, and other scientific articles.

Validation loss.To rank the pre-training performances, we compare the **validation loss**, which is directly comparable as we use the same inputs for all methods (which is not the case for the training data). This is shown in Figure 3, which shows the validation loss every 3 hours throughout training. We find that both batch selection methods do not improve over the baseline.

Downstream performance.Next, we investigate downstream performances, fix the C4 corpus as the pre-training corpus, and vary the budgets (6, 12, and 24 hours). Figures 2 and 16 show the results on SuperGLUE and GLUE. We observe very small differences between the methods and the baseline. On average (Table 1) no batch selection method significantly outperforms the baseline.

### Ablation: Batch Selection For Free

We want to disentangle whether batch selection methods fail to improve over the baseline because their gains do not compensate for their computational overhead. In the previous section (Section 5.3) and the main results (Table 1), we accounted for this overhead. Therefore, in those experiments, selective backprop and RHO loss effectively update the model for fewer iterations than the baseline within the fixed budgets. Here, we re-run a small number of experiments where batch selection is free, and so we train these models with the same number of iterations as the baseline.

Specifically, we run an experiment for 12 hours using the C4 corpus and GLUE downstream tasks. For selective backprop, we choose \(=1\), which resulted in \(\)\(1.7\)x of the wall-clock time; while for RHO loss, we choose a mega-batch size that is \(10\)x larger (\(15360\)) than the mini-batch size (\(1536\)), following Mindermann et al. , which led to \(\)\(5.3\)x of the original pre-training time. Table 4 shows the results. We see that RHO loss reaches a slightly better final validation loss but performs worse on the GLUE downstream tasks than baseline and selective backprop, which does not improve over the baseline.

Figure 3: **Validation losses for different datasets. Results for batch selection methods (selective backprop and RHO loss) for a 12-hour RST budget.**

## 6 Case Study 3: Efficient Optimizers

Recently, two efficient optimizers were proposed to replace the ubiquitous Adam(W) optimizers [48; 62] for language models: Lion  and Sophia .

Lion  is an optimizer symbolically discovered in the vast program space of first-order optimization primitives. As such, it does not follow any theory-grounded principle that would justify its acceleration property a priori; however, Chen et al.  report empirical speed-ups over AdamW in many settings.

Sophia  is a scalable stochastic second-order optimizer primarily designed for and evaluated on language model pre-training. The authors claim that Sophia achieves a 2x speed-up compared with AdamW in the number of steps, total compute, and wall-clock time. The authors study two Hessian estimators, but as of this writing, only open-source the code for the empirically better one (Gauss-Newton-Bartlett), which we use here.

The baseline in this section simply refers to AdamW .

Mixed Precision Training with BF16 vs. FP16A common practice for training language models is to use mixed precision training . In initial experiments with BERT, we observed several numerical instabilities (NaN training losses) during hyper-parameter search after inserting Lion and Sophia into our training pipelines as drop-in replacements. Our default mixed-precision mode was FP16, and as noted by other practitioners of Sophia , BF16 can sometimes be more stable. Hence; for the optimizer comparisons with BERT, we report results in BF16; including the baseline (although we notice that the baseline's training curves are essentially identical across both modes). For the T5 model, we use the TF32 precision format.

RMS-Scaling for T5 Pre-trainingT5 pre-training [77; 82] typically employs the Adafactor optimizer , which relies on tensor-wise learning rate scaling determined by the tensor's root mean square (RMS). This scaling has been identified as critical for convergence during pre-training when using AdamW . Initial tests with Lion and Sophia without additional adjustments led to divergence for higher learning rates or suboptimal performance. This mirrored the behavior of AdamW without RMS scaling. To address this, we incorporated RMS scaling into Lion and Sophia for a subsequent round of experiments, which we outline in Appendix C.6.

### Results

In the case of BERT downstream performances (Table 1), we find that Lion and Sophia perform about the same as the baseline. Figure 6 shows the results in more detail. We note that baseline (AdamW) consistently ranks the highest and has a comparatively low standard deviation over random seeds.

Figure 4: BERT Validation loss when we do not count Sophia’s Hessian update steps.

Figure 5: **Training losses, efficient optimizer methods** (Lion, and Sophia). Results are shown for RST budgets of 6 hours (), 12 hours (), and 24 hours (), on C4.

Analogous to the batch selection for free ablation in Section 5.4, we also experiment with Sophia while not counting for Hessian update steps and running for the same number of iterations, as shown in Figure 4. Surprisingly, we still do not observe any speedup.

## 7 Conclusion

In this work, we closely examined efficient training algorithms which promised to deliver training speed-ups. First, we clarify that quantifying a training speed-up without specifying an explicit training budget can be misleading and that some previous works missed this. To normalize the wall clock time across different hardware, we introduce the reference system time measure. Then, we put three classes of algorithms to the test in various pre-training settings of BERT and T5 models. We found only a few settings where some of the considered algorithms improved over the baseline.