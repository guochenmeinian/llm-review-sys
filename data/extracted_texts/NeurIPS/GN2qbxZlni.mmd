# MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs

Zhongshen Zeng\({}^{1}\) Yinhong Liu\({}^{2}\) Yingjia Wan\({}^{2}\)

Jingyao Li\({}^{1}\) Pengguang Chen\({}^{1}\) Jianbo Dai\({}^{3}\) Yuxuan Yao\({}^{4}\)

Rongwu Xu\({}^{5}\) Zehan Qi\({}^{5}\) Wanru Zhao\({}^{2}\) Linling Shen\({}^{6}\)

Jianqiao Lu\({}^{7}\) Haochen Tan\({}^{4}\) Yukawa Chen\({}^{1}\) Hao Zhang\({}^{8}\)

Zhan Shi\({}^{6}\) Bailin Wang\({}^{9}\) Zhijiang Guo\({}^{2}\) Jiaya Jia\({}^{1}\)

\({}^{1}\)Chinese University of Hong Kong \({}^{2}\)University of Cambridge \({}^{3}\)University of Edinburgh

\({}^{4}\)City University of Hong Kong \({}^{5}\)Tsinghua University \({}^{6}\)University of Texas at Austin

\({}^{7}\)University of Hong Kong \({}^{8}\)Nanyang Technological University

\({}^{9}\)Massachusetts Institute of Technology

###### Abstract

Large language models (LLMs) have shown increasing capability in problem-solving and decision-making, largely based on the step-by-step chain-of-thought reasoning processes. However, evaluating these reasoning abilities has become increasingly challenging. Existing _outcome-based_ benchmarks are beginning to saturate, becoming less effective in tracking meaningful progress. To address this, we present a _process-based_ benchmark MR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. Our meta-reasoning paradigm is especially suited for system-2 slow thinking, mirroring the human cognitive process of carefully examining assumptions, conditions, calculations, and logic to identify mistakes. MR-Ben comprises 5,975 questions curated by human experts across a wide range of subjects, including physics, chemistry, logic, coding, and more. Through our designed metrics for assessing meta-reasoning on this benchmark, we identify interesting limitations and weaknesses of current LLMs (open-source and closed-source models). For example, with models like the o1 series from OpenAI demonstrating strong performance by effectively scrutinizing the solution space, many other state-of-the-art models fall significantly behind on MR-Ben, exposing potential shortcomings in their training strategies and inference methodologies1.

## 1 Introduction

Reasoning, the cognitive process of using evidence, arguments, and logic to reach conclusions, is crucial for problem-solving, decision-making, and critical thinking . With the rapid advancement of Large Language Models (LLMs), there is an increasing interest in exploring their reasoning capabilities . Consequently, evaluating reasoning in LLMs reliably becomes paramount. Current evaluation methodologies primarily focus on the final result , disregarding the intricacies of the reasoning process. While effective to some extent, such evaluation practices may conceal underlying issues like logical errors or unnecessary steps that compromise the accuracy and efficiency of reasoning .

Therefore, it is important to complement outcome-based evaluation with an intrinsic evaluation of the quality of the reasoning process. However, current benchmarks for evaluating LLMs' reasoning capabilities have certain limitations in terms of their scope and size. For instance, PRM800K  categorizes each reasoning step as positive, negative, or neutral. Similarly, BIG-Bench Mistake  focuses on identifying errors in step-level answers. We follow the same meta-reasoning paradigm as MR-GSM8K  and MR-Math , which go a step further by providing the error reason for the first negative step in the reasoning chain. However, these benchmarks are limited to a narrower task scope--MR-GSM8K and MR-Math focus solely on mathematical reasoning, while BIG-Bench Mistake mainly assesses logical reasoning. To ensure a comprehensive evaluation of reasoning abilities, it is crucial to identify reasoning errors and assess the LLMs' capacity to elucidate them across wider domains.

To bridge this gap, we construct a comprehensive benchmark MR-Ben comprising 6k questions covering a wide range of subjects, including natural sciences like math, biology, and physics, as well as coding and logic. One unique aspect of MR-Ben is its meta-reasoning paradigm, which involves challenging LLMs to reason about different forms of reasoning. In this paradigm, LLMs take on the role of a teacher, evaluating the reasoning process by assessing correctness, analyzing potential errors, and providing corrections, as depicted in Figure 1.

Our analysis of various LLMs [50; 51; 5; 33; 47] uncovers distinct limitations and previously unidentified weaknesses in their reasoning abilities. While many LLMs are capable of generating correct answers, they often struggle to identify errors within their reasoning processes and explain the underlying rationale. To excel under our meta-reasoning paradigm, models must meticulously scrutinize assumptions, conditions, calculations, and logical steps, even inferring step outcomes counterfactually. These requirements align with the characteristics of "System-2" slow thinking [35; 9], which we believe remains underdeveloped in most of the state-of-the-art models we evaluated.

We suspect that a key reason for this gap lies in current fine-tuning paradigms, which prioritize correct solutions and limit effective exploration of the broader solution space. Echoing this hypothesis, we observed that models like o1-preview , which reportedly incorporate effective search and disambiguation techniques across trajectories in the solution space, outperform other models by a large margin. Moreover, we found that leveraging high-quality and diverse synthetic data  significantly mitigates this issue, offering a promising path to enhance performance regardless of model size. Additionally, our results indicate that different LLMs excel in distinct reasoning paradigms, challenging the notion that domain-specific enhancements necessarily yield broad cognitive improvements. We hope that MR-Ben will guide researchers in comprehensively evaluating their models' capabilities and foster the development of more robust AI reasoning frameworks.

Our key contributions are summarized as follows:

* We introduced MR-Ben, which includes around 6k questions across a wide range of subjects, from natural sciences to coding and logic, and employs a unique meta-reasoning paradigm.
* We conduct an extensive analysis of various LLMs on MR-Ben, revealing various limitations and previously unidentified weaknesses in their reasoning abilities.
* We offer potential pathways for enhancing the reasoning abilities of LLMs and challenge the assumption that domain-specific enhancements necessarily lead to broad improvements.

## 2 Related Works

Reasoning BenchmarksEvaluating the reasoning capabilities of LLMs is crucial for understanding their potential and limitations. While existing benchmarks often assess reasoning by measuring performance on tasks that require reasoning, such as accuracy, they often focus on specific reasoning types like arithmetic, knowledge, logic, or algorithmic reasoning. Arithmetic reasoning, involving mathematical concepts and operations, has been explored in benchmarks ranging from elementary word problems [37; 4; 55; 16] to more complex and large-scale tasks [28; 48]. Knowledge reasoning, on the other hand, requires either internal (commonsense) or external knowledge, or a combination of both [14; 62; 22]. Logical reasoning benchmarks, encompassing deductive and inductive reasoning, use synthetic rule bases for the former [15; 61; 18] and specific observations for the latter to formulate general principles [78; 71]. Algorithmic reasoning often involves understanding the coding problem description and performing multi-step reasoning to solve it [17; 25]. Benchmarks like BBH  and MMLU  indirectly assess reasoning by evaluating performance on tasks that require it. However, these benchmarks primarily focus on final results, neglecting the analysis of potential errors in the reasoning process. Unlike prior efforts, MR-Ben goes beyond accuracy by assessing the ability to locate potential errors in the reasoning process and provide explanations and corrections. Moreover, MR-Ben covers different types of reasoning, offering a more comprehensive assessment.

Evaluation Beyond AccuracyMany recent studies have shifted their focus from using only the final result to evaluating the reasoning quality beyond accuracy. This shift has led to the development of two approaches: reference-free and reference-based evaluation. Reference-free methods aim to assess reasoning quality without relying on human-provided solutions. For example, ROSCOE  evaluates reasoning chains by quantifying reasoning errors such as redundancy and hallucination. Other approaches convert reasoning steps into structured forms, like subject-verb-object frames  or symbolic proofs , allowing for automated analysis. Reference-based methods depend on human-generated step-by-step solutions. For instance, PRM800K  offers solutions to MATH problems , categorizing each reasoning step as positive, negative, or neutral. Building on this, MR-GSM8K  and MR-Math  further provide the error reason behind the first negative step. MR-GSM8K focuses on elementary math problems, sampling questions from GSM8K . MR-Math samples a smaller set of 459 questions from MATH . Using the same annotation scheme, BIG-Bench Mistake  focuses on symbolic reasoning. It encompasses 2,186 instances from 5 tasks in BBH . Despite the progress made by these datasets, limitations in scope and size remain. To address this, we introduce MR-Ben, a benchmark consisting of 5,975 manually annotated instances covering a wide range of subjects, including natural sciences, coding, and logic. MR-Ben also features more challenging questions, spanning high school, graduate, and professional levels.

## 3 MR-Ben: Dataset Construction

### Dataset Structure

To comprehensively evaluate the reasoning capabilities of LLMs, MR-Ben employs a meta-reasoning paradigm. This paradigm casts LLMs in the role of a teacher, where they assess the reasoning process

Figure 1: Overview of the evaluation paradigm and representative examples in MR-Ben. Each data point encompasses three key elements: a question, a Chain-of-Thought (CoT) answer, and an error analysis. The CoT answer is generated by various LLMs. Human experts annotate the error analyses, which include error steps, reasons behind the error, and subsequent corrections. The three examples shown are selected to represent arithmetic, logical, and algorithmic reasoning types.

by evaluating its correctness, analyzing errors, and providing corrections. As shown in Figure 1, each data point within MR-Ben consists of three key elements: a question, a CoT answer, and an error analysis. The construction pipeline is shown in Figure 6 in Appendix-D.

QuestionThe questions in MR-Ben are designed to cover a diverse range of reasoning types and difficulty levels, spanning from high school to professional levels. To ensure this breadth, we curated questions from various subjects, including natural sciences (mathematics, biology, physics), coding, and logic. Specifically, we sampled questions from mathematics, physics, biology, chemistry, and medicine from MMLU , which comprehensively assesses LLMs across academic and professional domains. For logic questions, we draw from LogiQA , which encompasses a broad spectrum of logical reasoning types, including categorical, conditional, disjunctive, and conjunctive reasoning. Finally, we select coding problems from MHPP , which focuses on function-level code generation requiring advanced algorithmic reasoning. Questions in MMLU and LogiQA require a single-choice answer, while MHPP requires a snippet of code as the answer.

CoT AnswerWe queried GPT-3.5-Turbo-0125 , Claude2 , and Mistral-Medium  (as of February 2024) using a prompt template (provided in Figure-7 in Appendix-D) designed to elicit step-by-step solutions . For clarity, all LLMs were instructed to format their solutions with numbered steps, except for coding problems. To encourage diverse solutions, we set the temperature parameter to 1 during sampling. This empirical setting yielded satisfactory instruction following and desirable fine-grained reasoning errors, which annotators and evaluated models are expected to identify.

### Annotation Process

After acquiring the questions and their corresponding Chain-of-Thought (CoT) answers, we engage annotators to provide error analyses. The annotation process is divided into three stages.

Answer CorrectnessCoT answers that result in a final answer different from the ground truth are automatically flagged as incorrect. However, for cases where the final answer matches the ground truth, manual annotation is required. This is because there are instances where the reasoning process leading to the correct answer is flawed, as illustrated in the middle example of Figure 1. Therefore, annotators are tasked with meticulously examining the entire reasoning path to determine if the correct final answer is a direct result of the reasoning process.

Error StepThis stage is applicable for solutions with either an unmatched final output or a matched final output underpinned by flawed reasoning. Following the prior effort , each step in the reasoning process is categorized as positive, neutral, or negative. Positive and neutral steps represent stages where the correct final output remains attainable. Conversely, negative steps indicate a divergence from the path leading to the correct solution. Annotators are required to identify the first step in the reasoning process where the conditions, assumptions, or calculations are incorrect, making the correct final result unreachable for the subsequent reasoning steps.

Error Reason and CorrectionAnnotators are tasked with conducting an in-depth analysis of the reasoning that led to the identified error. As shown in Figure 1, annotators are required to provide the error reason and the corresponding correction to this reasoning step. This comprehensive approach ensures a thorough understanding and rectification of errors in the reasoning process.

### Data Statistics

Table 1 presents the statistics of MR-Ben. The benchmark exhibits a balanced distribution of correct and incorrect solutions, with an overall correct solution rate of 40.3%. Solutions, on average, involve 9.5 steps, and errors typically manifest around the fourth step (4.5). The questions and solutions are substantial, with average lengths of 85.6 and 308.8 words, respectively. The subject-wise analysis reveals that Math is the most challenging, with a correct solution rate of a mere 16.2%. This could be attributable to the intricacy of the arithmetic operations involved. Conversely, Biology emerges as the least daunting, with a high correct solution rate of 59.6%. Coding problems have the longest solutions, averaging 950.3 number of words. This underscores the complexity and the detailed procedural reasoning inherent in coding tasks. Similarly, Logic problems have the longest questions, averaging

154.8 words. This is in line with the need for elaborate descriptions in logical reasoning. The typical step at which the first error occurs is fairly consistent across most subjects, usually around the 3rd step out of a total of 5. However, Coding deviates from this trend. The first error tends to appear earlier, specifically around the 14th line out of a total of 32.5 lines. This suggests that the problem-solving process in Coding may have distinct dynamics compared to other subjects.

### Quality Control

AnnotatorsGiven the complexity of the questions, which span a range of subjects from high school to professional levels, we enlisted the services of an annotation company. This company meticulously recruited annotators, each holding a minimum of a bachelor's degree. Before their trial labeling, annotators are thoroughly trained and are required to review the annotation guidelines. We've included the guidelines for all subjects in Appendix H for reference. The selection of annotators is based on their performance on a balanced, small hold-out set of problems for each subject. In addition to the annotators, a team of 14 quality controllers diligently monitors the quality of the annotation weekly. As a final layer of assurance, we have 4 meta controllers who scrutinize the quality of the work.

Quality AssuranceEvery problem in MR-Ben undergoes a rigorous three-round quality assurance process to ensure its accuracy and clarity. Initially, each question is labeled by two different annotators. Any inconsistencies in the solution correctness or the first error step are identified and reviewed by a quality controller for arbitration. Following this, every annotated problem is subjected to a secondary review by annotators who were not involved in the initial labeling. This is to ensure that the annotations for different solutions to the same problem are consistent and coherent. In the final phase of the review, 10% of the problems are randomly sampled and reviewed by the meta controllers. Throughout the entire evaluation process, all annotated fields are meticulously examined in multiple rounds for their accuracy and clarity. Any incorrect annotations or those with disagreements are progressively filtered out and rectified, ensuring a high-quality dataset. This rigorous process allows us to maintain a high level of annotation quality.

Dataset Artifacts & BiasesTable 1 reveals a relatively balanced distribution of correct and incorrect solutions. However, an exception was observed in mathematical subjects, where the distribution tends to skew towards incorrect solutions. This skew could suggest an inherent complexity or ambiguity in mathematical problem statements. Our analysis of the first error step across all subjects indicated that errors predominantly occur in the initial stages (\(n 7\)) of problem-solving and are distributed relatively uniformly. This pattern was consistent across most subjects, with no significant skew towards later steps. More detailed discussions of biases are provided in the Appendix C.

## 4 Evaluation

For each question-solution pair annotated, the evaluated model are supposed to decide the correctness of the solution and report the first-error-step and error-reason if any. The solution-correctness and first-error-step is scored automatically based on the manual annotation result. Only when the evaluated model correctly identified the incorrect solution and first-error-step will its error-reason be further examined manually or automatically by models. Therefore in order to provide a unified and normalized score to reflect the overall competence of the evaluated model, we follow the work of  and apply a metric named **MR-Score**, which consist of three sub-metrics.

    & **Math** & **Medicine** & **Biology** & **Physics** & **Chemistry** & **Logic** & **Coding** & **Total** \\  Question-Solution Pairs & 918 & 828 & 1035 & 667 & 848 & 1441 & 238 & 5975 \\ Correct Solution Ratio & 16.2\% & 31.0\% & 59.6\% & 47.8\% & 45.0\% & 51.1\% & 31.1\% & 40.3\% \\ Avg Solution Steps & 6.8 & 5.3 & 5.1 & 5.7 & 5.6 & 5.3 & 32.5\% & 9.5 \\ Avg First Error Step & 3.1 & 3.0 & 2.7 & 3.1 & 3.0 & 2.8 & 14.0* & 4.5 \\ Avg Length of Questions & 44.3 & 88.7 & 56.3 & 66.6 & 48.1 & 154.8 & 140.1 & 85.6 \\ Avg Length of Solutions & 205.9 & 206.1 & 187.6 & 199.4 & 194.5 & 217.7 & 950.3 & 308.8 \\   

Table 1: Statistics of MR-Ben. The length of questions and solutions are measured inthe number of words. Notice that the steps for coding denote the number of lines of code. They are not directly comparable with other subjects.

The first one is the Matthews Correlation Coefficient (a.k.a MCC, 46) for the binary classification of solution-correctness.

\[MCC=} \]

where TP, TN, FP, FN stand for true positive, true negative, false positive and false negative. The MCC score ranges from -1 to +1 with -1 means total disagreement between prediction and observation, 0 indicates near random performance and +1 represents perfect prediction. In the context of this paper, we interpret negative values as no better than random guess and set 0 as cut-off threshold for normalization purpose.

The second metric is the ratio between numbers of solutions with correct first-error-step predicted and the total number of incorrect solutions.

\[ACC_{}=}}{N_{}} \]

The third metrics is likewise the ratio between number of solutions with correct first-error-step plus correct error-reason predicted and the total number of incorrect solutions.

\[ACC_{}=}}{N_{}} \]

**MR-Score** is then a weighted combination of three metrics, given by

\[MRScore=w_{1}*(0,MCC)+w_{2}*ACC_{}+w_{3}*ACC_{} \]

For the weights \(w_{1},w_{2}\) and \(w_{3}\), they are chosen based on our evaluation results to maximize the differentiation between different models. It is important to note that the Matthews Correlation Coefficient (MCC) and the accuracy of locating the first error step can be directly calculated by comparing the responses of the evaluated model with the ground truth annotations. However, assessing the accuracy of the error reason explained by the evaluated model presents more complexity. While consulting domain experts for annotations is a feasible approach, we instead utilized GPT-4-Turbo as a proxy to examine the error reasons, as detailed in Figure-11 in Appendix-D.

We operate under the assumption that while our benchmark presents a significant challenge for GPT-4 in evaluating complete solution correctness--identifying the first error step and explaining the error reason--it is comparatively easier for GPT-4 to assess whether the provided error reasons align with the ground truth. Specifically, in a hold-out set of sampled error reasons, there was a 92% agreement rate between the manual annotations by the authors and those generated by GPT-4. For more detailed evaluations on the robustness of MR-Score and its design thinking, please refer to our discussion in Appendix-B.

## 5 Experiments

### Experiment Setup

To evaluate the performance of different models on our new benchmark, we selected a diverse array of models based on size and source accessibility 2. This included smaller models like Geuma-2B, Phi-3, Qwen1.5-1.8B , as well as larger counterparts such as Llama3-70B , Deepseek-67B, and Qwen1.5-72B. We also compared open-source models (e.g. models from the Llama3 and Qwen1.5/Qwen2 series) against closed-source models from the GPT , Claude , Mistral , GLM , Yi , Moonshot , Doubao  families. Additionally, models from the Deepseek-Coder  series were included to assess the impact of coding-focused pretraining on reasoning performance.

Given the complexity of our benchmark, even larger open-source models like Llama3-70B-Instruct struggle to produce accurate evaluation results without the use of prompting methods, often achievingMR-Scores near zero. Consequently, we employed a step-wise chain-of-thought prompting technique similar to those described in [77; 64]. This approach guides models in systematically reasoning through solution traces before making final decisions, as detailed in Appendix-D.

Considering the complexity of the task, which includes question comprehension, reasoning through the provided solutions, and adhering to format constraints, few-shot demonstration setups are also explored to investigate if models can benefit from In-Context Learning (ICL) examples. Due to the context token limits, we report zero and one-shot results in the main result table (Table 2)3. The performance of additional few-shot configurations on a selection of models with various capabilities is further discussed in Section 6.1.

### Experiment Results

The MR-Ben benchmark presents a significant shift in the challenge for state-of-the-art large language models, transitioning from question-answering to the nuanced role of question-solution scoring. This section details our findings, emphasizing variations in model performances and their implications.

Overall PerformanceAmong the evaluated models, o1-preview consistently achieves the highest MR-Scores across all subjects, significantly outperforming most competitors from both open and

    &  &  &  &  &  &  &  &  \\   & \(k\)=0 & \(k\)=1 & \(k\)=0 & \(k\)=1 & \(k\)=0 & \(k\)=1 & \(k\)=0 & \(k\)=1 & \(k\)=0 & \(k\)=1 & \(k\)=0 & \(k\)=1 & \(k\)=0 & \(k\)=1 \\   \\  Claude3-Haiku & 5.7 & 5.8 & 3.3 & 3.5 & 3.1 & 3.1 & 6.5 & 6.4 & 2.0 & 2.0 & 1.2 & 1.2 & 9.0 & 0.0 & 4.4 & 3.1 \\ GPT-3.5-Turbo & 3.6 & 6.6 & 5.7 & 6.7 & 5.7 & 5.4 & 4.9 & 6.7 & 3.6 & 4.4 & 1.7 & 4.5 & 3.0 & 4.1 & 4.0 & 5.5 \\ Doubao-pro-4k & 8.4 & 13.5 & 10.0 & 11.7 & 12.3 & 15.5 & 10.6 & 17.5 & 5.9 & 10.0 & 4.5 & 5.5 & 9.8 & 7.4 & 8.8 & 11.6 \\ Mistral-Large & 22.2 & 28.0 & 26.7 & 25.4 & 24.3 & 28.2 & 24.0 & 27.0 & 15.9 & 19.3 & 14.7 & 17.1 & 21.1 & 21.4 & 21.3 & 23.8 \\ Yi-Large & 35.3 & 40.7 & 37.2 & 36.8 & 36.5 & 20.6 & 40.0 & 39.1 & 29.3 & 32.1 & 25.1 & 31.3 & 21.9 & 25.7 & 32.2 & 32.3 \\ Monoshot-v1-8k & 35.0 & 36.8 & 33.8 & 33.8 & 34.9 & 33.0 & 36.7 & 35.0 & 29.4 & 32.3 & 25.0 & 29.2 & 32.7 & 31.2 & 32.5 & 33.0 \\ GPT-40-mini & 37.7 & 38.9 & 38.5 & 37.4 & 44.4 & 40.4 & 39.2 & 37.0 & 33.9 & 25.1 & 23.6 & 17.7 & 41.6 & 34.9 & 37.0 & 33.1 \\ Zhipu-GLM-4 & 40.7 & 46.2 & 37.7 & 42.5 & 38.4 & 36.6 & 43.1 & 44.0 & 34.5 & 41.0 & 37.5 & 32.5 & 38.8 & 32.8 & 38.7 & 39.4 \\ GPT-4-Turbo & 44.7 & 47.3 & 42.8 & 45.2 & 44.3 & 45.4 & 44.0 & 46.0 & 38.8 & 38.4 & 34.1 & 33.6 & 53.6 & 57.3 & 43.2 & 44.7 \\ GPT-4o & 48.3 & 49.1 & 45.5 & 48.2 & 42.6 & 41.3 & 48.2 & 49.1 & 47.9 & 47.7 & 31.9 & 28.4 & 56.5 & 54.6 & 45.8 & 45.5 \\ o1-mini & 45.8 & 46.9 & 56.0 & 53.8 & 68.5 & 67.0 & 55.2 & 56.1 & 45.9 & 47.2 & 30.7 & 28.7 & 55.1 & 55.6 & 51.0 & 50.8 \\ o1-preview & 54.1 & 56.0 & 62.2 & 61.7 & 69.8 & 70.3 & 60.6 & 50.3 & 54.3 & 55.1 & 46.1 & 45.3 & 65.

closed-source communities. Notably, the open-sourced Owen2-72B and Deepseek-V2-236B models are performing exceptionally well, surpassing every other open-sourced model including Llama3 by a large margin. Their scores are even comparable to or greater than some of the most capable models from commercial companies, such as Mistral, Yi, and Moonshot AI. In the small language model category, the performance of Phi3-3.8B exceeds many of the mid-size models, including Deepseek-Coder-33B, whose size is around tenfold larger.

Performance across Model Size and Reasoning ParadigmTable 2 reveals a general trend where larger models tend to perform better, highlighting the correlation between model size and the efficacy in complex reasoning tasks. However, this relationship is not strictly linear, as demonstrated by models like Phi3-3.8B, which excel despite their smaller size. Since MR-Ben challenges the language models to reason about the reasoning in the solution space among a diverse range of domains, models like Phi-3 that are trained with effective data synthesis techniques and broader coverage of the solution space, intuitively achieve higher MR-Score. This suggests that while larger model sizes generally yield superior performances, techniques like knowledge distillation can also significantly boost reasoning performance. Similarly, although the size of the o1 model series remains undisclosed, these models reportedly employ mechanisms that scale computation efficiently through effective exploration, frequent retrospection, and meticulous reflection within the solution space. These characteristics align closely with the principles of "system-2" thinking, which emphasizes deliberate, reflective problem-solving. As a result, the o1 models demonstrate a more effective reasoning process, achieving significantly higher MR-Scores than other models by a large margin.

Performance across Reasoning TypesOur categorization into four reasoning types--knowledge, arithmetic, algorithmic, and logic--illustrates the unique challenges each model faces within these paradigms (Figure 3). Logic reasoning emerges as the most formidable due to the intricate logical operations required by questions from the LogiQA dataset. In stark contrast, o1-Preview and GPT-4-turbo demonstrate exceptional prowess in algorithmic reasoning, where their capabilities markedly surpass other models. Notably, models excel in different reasoning paradigms, reflecting their varied strengths and training backgrounds. For instance, despite Deepseek-Coder's specialized pre-training focused on coding tasks, it does not necessarily confer superior abilities in algorithmic reasoning, underscoring that targeted pretraining does not guarantee enhanced performance across all reasoning types. Comparing the performance of the Deepseek-Coder with that of the Phi-3 model, which excels despite its much smaller size, highlights the potential significance of high-quality synthetic data in achieving broad-based reasoning capabilities.

Sensitivity to Task Difficulty and Solution LengthAn examination across educational levels shows most models perform better at high school-level questions than college-level ones, indicating an intuitive level of sensitivity to the difficulty levels of the questions. Additionally, our analysis finds a minor negative correlation between the length of solution steps and MR-Scores, as detailed in Figure 4 and Figure 5.

Figure 2: Model performance across subjects Figure 3: Model performance on different reasoning paradigms

Summary:MR-Ben effectively differentiates model capabilities, often obscured in simpler settings. It not only identifies top performers but also underscores the influence of model size on outcomes, while demonstrating that techniques like knowledge distillation and test-time compute scaling, as seen with the Phi-3 and o1 models, can notably enhance smaller models' performance, challenging the dominance of larger models. The analysis further reveals that specialized training, such as in coding, does not guarantee superior algorithmic reasoning. This suggests the potential need for more balanced data approaches or improved data synthesis methods.

## 6 Further Analysis & Discussion

### Few Shot Prompting

As previously discussed and exemplified by our prompt template (Figure 10 in Appendix-D), our evaluation method is characterized by its high level of difficulty and complexity. In this experiment, we aimed to determine whether providing a few step-wise chain-of-thought (CoT) examples could improve model performance in terms of format adherence and reasoning quality. The results, as presented in Table 9 in Appendix, do not show a consistent pattern as the number of shots increases. While smaller language models like Gemma-2B exhibit performance improvements with additional shots, the performance of larger language models tends to fluctuate with an increasing number of shots. We hypothesize that for our complex tasks, the lengthy few-shot demonstrations may act more as a hindrance, providing distracting information rather than aiding in format adherence and reasoning. Our empirical findings suggest that a one-shot demonstration strikes the optimal balance between providing guidance and minimizing distraction. This supports our decision to focus on zero-shot versus one-shot comparisons in our primary experiments, as detailed in Table 2.

### Self Refine Prompting

As suggested by , large language models typically cannot perform self-correction without external ground truth feedback. To explore whether this phenomenon occurs in our benchmark, we adopted a similar setting by prompting the language model to verify its own answer across a three-round interaction sequence: query, examine, and refine. Our prompting template, detailed in Figure 8 in Appendix D, is minimalistic and designed solely to encourage the model to self-examine.

The results of this self-refinement process are recorded in Table 4. Notably, models smaller than Llama3-70B exhibit performance degradation with self-refinement, while larger models, such as GPT-4, show marginal benefits from the process. Conversely, from Llama3-8B to Llama3-70B, despite a significant portion of correct predictions shifting to incorrect ones, as previously reported by , our benchmark shows an increasing trend of incorrect predictions shifting to correct ones as model size increases. This shift results in the significant performance improvements observed in models like Llama3-70B.

To understand the disproportionate improvement observed in the 70B model, we analyzed performance breakdown at the task level. These results are visualized and discussed in Figure 9 of Appendix E. In short, we believe the lack of consistency does not necessarily indicate a more robust or advanced reasoning ability, despite the increase of the evaluation results.

### Solution Correctness Prior

To verify the influence of external ground truth signals, we sampled 100 incorrect solutions from each subject respectively as our test set. By observing the same set of language models under a zero-shot CoT setting, we aim to determine whether the knowledge of the solution's incorrectness enhances their ability to identify the first error step and the reason for the error.

The results in Table 3 illustrate that the benefits of knowing the solution correctness prior generally increase with the model's competence but begin to plateau at the level of sophisticated models like GPT-4. Specifically, the Gemma-2b model struggles significantly in our benchmark, showing nearly zero performance due to its limited ability to follow formats and comprehend complex tasks. Consequently, having the solution correctness prior does not improve its performance metrics. In contrast, models with moderate capabilities benefit substantially from this prior knowledge, which aids in accurately locating the first error step and elucidating the error reason. However, as model capabilities improve, the incremental benefits of this prior knowledge quickly diminish. For instance, GPT-4 shows only a marginal improvement in identifying the first error step and an almost negligible impact on error reason analysis when provided with the prior.

## 7 Conclusion

This paper highlights the importance of evaluating the reasoning capabilities of LLMs with process-oriented design and presents a comprehensive benchmark called MR-Ben that addresses the limitations of existing evaluation methodologies. MR-Ben consists of questions from a diverse range of subjects and incorporates a meta-reasoning paradigm, where LLMs act as teachers to evaluate the reasoning process. Our evaluation of a diverse suite of LLMs on MR-Ben reveals several key limitations and weaknesses. Many models struggle with identifying and correcting errors within reasoning chains, demonstrating difficulty in performing system-2 style thinking--such as scrutinizing assumptions, calculations, and intermediate steps. Furthermore, even state-of-the-art models often fail to maintain consistency across reasoning paradigms, exposing gaps in their generalization abilities. Additionally, our findings emphasize the importance of searching and reflecting on the solution space during inference. Models like the o1 series showcase the potential of scaling test-time computation, where frequent retrospection and iterative search through multiple solution paths significantly enhance reasoning performance. Nevertheless, improving LLMs' reasoning abilities on complex and nuanced tasks remains an open research question, and we encourage future work to develop upon MR-Ben.

## 8 Acknowledgement

This work was supported in part by the Research Grants Council under the Areas of Excellence scheme grant AoE/E-601/22-R.

  
**Model** & **0-shot CoT** & **Self-Refine** \\  Gemma-2B & 0.1 & 0.2 \\ Llama3-8B & 11.7 & 11.3 \\ Llama3-70B & 17.7 & 27.5 \\ GPT-4-Turbo & 43.2 & 45.5 \\   

Table 4: Comparison of prompting methods: MR-Scores achieved by zero-shot step-wise CoT and Self-Refine technique.

    &  &  \\   & w/o & with & w/o & with \\  Gemma-2B & 0.3 & 0.1 & 0.1 & 0.0 \\ Llama3-8B & 15.5 & 26.4 & 6.6 & 11.9 \\ Llama3-70B & 14.5 & 34.6 & 9.1 & 25.7 \\ GPT-4-Turbo & 40.9 & 41.6 & 37.9 & 38.0 \\   

Table 3: Comparison of average accuracy in identifying the first error step and the corresponding error reason, with and without prior knowledge of the solutions’ correctness.