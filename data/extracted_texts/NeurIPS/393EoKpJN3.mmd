# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

sis, ultimately deepening our comprehension of communication and sociality in non-human primates.

## 1 Introduction

Studying the behavior of non-human primates is essential for gaining evolutionary insights (Langergraber et al., 2012), conducting biomedical research (Schapiro et al., 2005), and improving animal welfare (Dawkins, 2003; Gonyou, 1994). Furthermore, given the close phylogenetic proximity between humans and non-human primates, it provides an ethically sound and effective avenue to probe the roots of human sociality (The Chimpanzee Sequencing and Analysis Consortium, 2005). Traditional field research typically requires researchers to enter wildlife conservation areas for extended durations, sometimes spanning multiple years. This involves habituating primate groups to human presence, capturing video footage, and laboriously manually coding these videos for subsequent statistical analysis (Hobaiter et al., 2017; Frohlich et al., 2020; Surbeck et al., 2017; Luncz et al., 2018; Sirianni et al., 2015). While video coding is heralded as the gold standard for distilling rich, nuanced behavioral patterns (Wiltshire et al., 2023), its practical utility hinges on the efficiency of the coding process. This not only demands researchers with specialized expertise but is also prone to attentional biases.

Recent strides in computer vision offer promise for the automated analyses of non-human primate behaviors, especially those of chimpanzees. Nevertheless, the scarcity of high-quality longitudinal datasets remains a bottleneck. Assembling chimpanzee behavioral data is a formidable endeavor, necessitating substantial resources and expertise. This process entails continuous video recording and meticulous manual annotation, with a keen emphasis on annotation accuracy and consistency. While some datasets (Marks et al., 2022; Bala et al., 2020) confine subjects to indoor enclosures, resulting in atypical and constrained environments, others resort to sourcing and labeling chimpanzee images online (Labuguen et al., 2021; Desai et al., 2022; Ng et al., 2022; Yao et al., 2023). Unfortunately, these often overlook the intricate social dynamics inherent to chimpanzee groups, hindering a comprehensive study of their social behaviors and social relationships.

Addressing the existing limitations, we introduce ChimpACT, a comprehensive longitudinal dataset tailored for the in-depth study of chimpanzee social behavior in a semi-naturalistic setting, replete with annotations of instance bounding boxes, body poses, and spatial-temporal action labels. A comparison with other datasets is provided in Tab. 1. ChimpACT encompasses footage of a specific chimpanzee group residing at Leipzig Zoo, Germany, with a particular focus on a juvenile male named Azibo (refer to Fig. 1). The data, gathered between 2015 and 2018, employs _focal sampling_

  &  &  &  &  \\  & & & detection, tracking, & ReID & & pose estimation & & action recognition & Source \\   & & ID \# & frame \# & box \# & track & frame \# & pose \# & track & dim. & class \# & label \# & \\  AP-10K & G & ✗ & ✗ & ✗ & ✗ & 10,015 & 13,028 & ✗ & 2D & ✗ & ✗ & I \\ (Vic et al., 2021) & G & ✗ & ✗ & ✗ & 10,015 & [\(<\)00] & & & & & & \\ AnimalKingdom & G & ✗ & ✗ & ✗ & 33,099 & 99,297 & ✗ & 2D & 140 & 30,100 & I \\ (Ng et al., 2022) & & & & & &  & ✗ & 10,876 & ✗ & 2D & ✗ & ✗ & I \\ OpenAgePose & P & ✗ & ✗ & ✗ & ✗ & 111,529 & [11,529 & ✗ & 2D & ✗ & ✗ & I \& Z \\ OpenMoneyChallenge & P & ✗ & ✗ & ✗ & 111,529 & [11,529 & ✗ & 33,192 & ✗ & & C \\
**OpenMoneyStudio** & & & & & & &  & ✓ & 3D & ✗ & ✗ & (6.7\(m^{2}\)) \\
**MacaquePose** & M & ✗ & ✗ & ✗ & 13,083 & 16,393 & ✗ & 2D & ✗ & ✗ & I \& Z \\
**Labuguen et al., 2021** & & & & & & &  & ✗ & ✗ & ✗ & 4 & \(\) & C \\ SIPC & M & 4 & 191 & 2,200 & ✓ & ✗ & ✗ & ✗ & ✗ & 4 & \(\) & C \\ (Marks et al., 2022) & & & & & & & & & & (15\(m^{2}\)) \\ CCR & & & & & & & & & & & & \\ (Bain et al., 2019) & C & 13 & 936,914 & 1,937,585 & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & W \\   **ChimpACT** \\ (Ours) \\  } & C & 23 & 160,500 & 56,324 & ✓ & 16,028 & 56,324 & ✓ & 2D & 23 & 64,289 & 
 CP \\ (4400\(m^{2}\)) \\  \\  

Table 1: **Comparison of ChimpACT with existing primate behavioral datasets. Square-bracketed numbers denote label counts for the chimpanzee category. \(\) denotes undocumented. For the “Species” row, G represents general, P for primates, M for macaque, and C for chimpanzee. In the “Source” row, I stands for Internet, Z for zoo, C for cage, W for wild, and CP for captive.**(Altmann, 1974). Born in April 2015, Azibo1 has been living in the group since birth, providing a unique perspective on the development of an individual within a chimpanzee group characterized by well-defined kin relationships. (also depicted in Fig. 2a). The footage covers the daily lives of over 20 chimpanzees in a group, aggregating to 163 video recordings, approximately 160,500 frames, and spanning around 2 hours.

Our annotations on ChimpACT are extensive, marking each individual's detection, tracking, identification, pose estimation, and spatiotemporal action detection. Sample frames with their corresponding annotations are illustrated in Fig. 1. Each chimpanzee's identity is confirmed by a seasoned behavioral researcher familiar with the Leipzig chimpanzees, ensuring data precision and trustworthiness. Crucially, we employ an ethogram (detailed in Fig. 2b) devised by the same expert for fine-grained action labels. To our knowledge, ChimpACT is the first to furnish ethogram annotations for the machine learning and computer vision community. This bespoke ethogram delineates behaviors into four categories: locomotion, object interaction, social interaction, and others, with each encompassing several detailed actions we diligently annotate.

While advancements in computer vision have notably addressed human-centric tasks, such as human pose estimation (Sun et al., 2019; Xiao et al., 2018), the dearth of chimpanzee datasets has curtailed progress on chimpanzee-specific challenges. Despite their genetic closeness to humans (The Chimpazee Sequencing and Analysis Consortium, 2005), deciphering chimpanzee behaviors is intricate due to their unique morphology, appearance, and keypoint articulation. Highlighting the importance of crafting sophisticated chimpanzee perception models, we evaluate prominent human perception methods on three tracks: (i) detection, tracking, and identification (ReID), (ii) pose estimation, and (iii) spatiotemporal action detection. Our findings underscore ChimpACT's potential as a platform for the community to pioneer advanced techniques for better perception of the chimpanzees and ultimately contribute to a deeper understanding of non-human primates.

## 2 Related work

Computer vision for animalsA myriad of datasets and benchmarks have emerged, harnessing computer vision techniques to advance animal research. For instance, 3D-ZeF20 (Pedersen et al., 2020) introduces 3D tracking of zebrafish to the MOT benchmarks. AnimalTrack (Zhang et al., 2023) emphasizes multi-animal tracking across a spectrum of species. AP-10K (Yu et al., 2021) and APT-36K (Yang et al., 2022) venture into animal pose estimation for diverse species. AnimalKingdom (Ng et al., 2022) extends its focus to fine-grained multi-label action recognition. Moreover, several studies have delved into multi-agent behavior understanding from a social interaction perspective (Sun et al., 2021, 2023). Distinctively, ChimpACT stands out as a holistic benchmark, encompassing three varied downstream tasks and boasting rich annotations of social interactions.

Human video datasetsIn contrast to animal-centric video datasets, a more substantial collection is tailored to human subjects, addressing diverse human-centric video understanding tasks. For instance, the MOT Challenge (Milan et al., 2016) is curated for multi-person tracking. Other benchmarks like COCO (Lin et al., 2014) and MPII (Andriluka et al., 2014) cater to human pose estimation. Meanwhile, datasets such as Kinetics (Kay et al., 2017), ActivityNet (Fabian Caba Heilbron and Niebles, 2015), and AVA (Gu et al., 2018) are dedicated to human action recognition. With ChimpACT, we encompass analogous tasks but introduce challenges specific to chimpanzee behavior.

Datasets on primate behavioral understandingMost existing primate datasets are tailored towards individual primate detection and pose estimation. These either stem from confined indoor settings (Bala et al., 2020; Marks et al., 2022) or are amassed and labeled from online sources (Labuguen et al., 2021; Desai et al., 2022; Ng et al., 2022; Yao et al., 2023). The former can induce atypical behavioral patterns, while the latter often omits longitudinal interactions, rendering them suboptimal for analyzing chimpanzee social dynamics. A notable exception is the CCR dataset (Bain et al., 2019), chronicling 13 chimpanzees in the Bossou forest over two years. Yet, it primarily focuses on individual detection and recognition, lacking behavioral annotations, which limits its efficacy for probing the social nuances of wild primates. Tab. 1 offers a comprehensive comparison. The narrow focus of most primate datasets on singular tasks restricts their breadth and adaptability to diverse research inquiries. Contrarily, ChimpACT presents a multifaceted approach, encompassing identities, kinship, detection labels, pose annotations, ethograms, and fine-grained action labels. This richness positions it as an indispensable tool for devising advanced chimpanzee behavior analysis methods and enriching the overarching comprehension of primate behavior.

Methods for primate behavioral analysisDeciphering primate behavior is instrumental in understanding their social dynamics and cognitive abilities. Behavioral analysis often encompasses subtasks like individual detection, tracking, and identification (Bain et al., 2019; Marks et al., 2022), pose estimation (Labuguen et al., 2021; Desai et al., 2022; Mathis et al., 2018; Wiltshire et al., 2023), and behavior recognition (Ng et al., 2022; Bain et al., 2021). While each task has specialized techniques, many are rooted in human behavioral research. Numerous algorithms exist for human tracking (Bewley et al., 2016; Pang et al., 2021), pose estimation (Sun et al., 2019; Xiao et al., 2018), and behavior recognition (Feichtenhofer et al., 2019). However, due to the dearth of primate datasets, primate behavioral analysis often repurposes algorithms designed for humans, including:

* **Detection, tracking, and ReID** identify individual primates in videos, often leveraging established object or human detection algorithms like Mask-RCNN (He et al., 2017). For instance, SIPEC (Marks et al., 2022) employs Mask-RCNN with a ResNet backbone (He et al., 2016) to track and segment macaque. Bain et al. (2019) utilize CNNs to crop and identify individual chimpanzees.
* **Pose estimation** discerns primate poses, frequently adapting human pose estimation methods like SimpleBaseline (Xiao et al., 2018). DeepLabCut (Mathis et al., 2018; Lauer et al., 2022), for instance, employs ResNet-50 with ImageNet pre-trained weights for 2D animal pose estimation. SIPEC (Marks et al., 2022) modifies SimpleBaseline for 2D macaque poses.
* **Behavior recognition** identifies primate actions and interactions. Contemporary methods (Bain et al., 2021; Bohnslav et al., 2021) often derive from human action recognition algorithms like SlowFast (Schindler and Steinhage, 2021). Notably, Bain et al. (2021) integrates audio cues for classifying two simple non-interactive behaviors: nut cracking and buttress drumming. In contrast, ChimpACT encompasses over 20 daily behaviors under an ethogram hierarchy, capturing both solitary actions and intricate social interactions.

In essence, primate behavioral analysis is a multifaceted endeavor, intertwining computer vision, machine learning, and primatology. The advent of ChimpACT marks a significant stride towards unraveling the intricate social tapestry of our primate kin.

## 3 ChimpACT

### Dataset description

ChimpACT comprises about 2-hour video footage of chimpanzees recorded at the Leipzig Zoo in Germany between 2015 and 2018. The videos focus on one male chimpanzee, Azibo, who was born in April 2015 to Swela and has lived with the A-chimpanzee group2 at the Leipzig Zoo ever since.

Figure 2: **(a) Kinship of the observed chimpanzee group. Rectangles and ellipses represent males and females, respectively, with arrows flowing from the parents to the child. Their vertical position relative to the time axis indicates the year of birth. (b) Ethogram with annotated behaviors.**

[MISSING_PAGE_EMPTY:5]

Detection, tracking, and ReIDThis task encompasses the detection and tracking of individual chimpanzees across video sequences, subsequently coupled with their re-identification. ChimpACT features over 23 distinct chimpanzee individuals, each identified by a primate expert familiar with the Leipzig A-group chimpanzees. Initially, annotators were instructed to delineate the bounding box of each chimpanzee, ensuring consistent box IDs for the same individual throughout a video clip. Subsequently, the expert matched these box IDs with the corresponding true names of the chimpanzees, resulting in the identification of 23 unique individuals. Additionally, every annotated bounding box is attached with a visibility attribute, indicating if the chimpanzee is fully visible, truncated, or occluded in a given frame. Such visibility annotations can support the reasoning of the chimpanzee behavior, potentially bolstering tracking robustness. Fig. 2(a) illustrates the occurrence frequency (on a _log_ scale) of each individual, revealing a long-tail distribution. This pattern aligns with the focal sampling strategy, where Azibo is the primary subject. Notably, Swela, Azibo's mother, also exhibits a high occurrence frequency, resonating with prior studies (Boesch, 1996).

Pose estimationPose estimation aims to predict the locations of the chimpanzee joints that have semantic meaning, such as the knee and shoulder, from an input image. There are four keypoints on the chimpanzee's face (_i.e._, two for the eyes, and one each for the upper and lower lips), for a total of 16 chimpanzee keypoints (refer to Sec. 3.3 and Fig. 4). Annotators are tasked with marking the 2D joint coordinates and the visibility status of each joint. We adopt the visibility protocol from the COCO 2D human keypoint annotations (Lin et al., 2014), where a value of 0 indicates a joint outside the image frame, 1 signifies an obscured joint within the image, and 2 designates a clearly visible joint. Such an annotation protocol affords reason about chimpanzee's orientation and action based on facial joint visibility. For instance, the chimpanzee might be eating something if the two lips are apart. Sample frames showcasing pose annotations are depicted in Fig. 1. Notably, ChimpACT holds the potential for future expansion to encompass pose tracking tasks, analogous to the PoseTrack (Andriluka et al., 2018) for humans.

Spatiotemporal action detectionSpatiotemporal action detection seeks to attribute one or multiple behavioral labels to each bounding box containing a chimpanzee, leveraging the spatiotemporal context within a video clip. Our ethogram, detailed in Fig. 1(b), delineates 23 nuanced subcategories of behaviors and guides the fine-grained annotations of chimpanzee behavior, such as "climbing"

   No. & Definition & No. & Definition \\ 
0 & Root of hip & 8 & Right eye \\
1 & Right knee & 9 & Left eye \\
2 & Right ankle & 10 & Right shoulder \\
3 & Left knee & 11 & Right elbow \\
4 & Left ankle & 12 & Right wrist \\
5 & Neck & 13 & Left shoulder \\
6 & Upper lip & 14 & Left elbow \\
7 & Lower lip & 15 & Left wrist \\   

Table 2: **Keypoint definitions for chimpanzee.**

Figure 3: **(a) Distribution (in log scale) of annotations for each individual. (b) Distribution (in log scale) of annotations for each behavior. Vector graphics; zoom for details.**

within the "locomotion" category. Notably, within the realm of social interactions, we meticulously differentiate between the action performer and receiver. For instance, the grooming behavior is bifurcated into "grooming" and "being groomed." Every chimpanzee in a frame has its subcategory behavior annotated. It is not uncommon for an individual to simultaneously exhibit multiple behaviors, exemplified by Swela's "carrying" and "moving" actions in Fig. 1. The distribution of these behavioral annotations, visualized in Fig. 2(b) on a _log_ scale, reveals a long-tail distribution, mirroring the authentic behavioral tendencies of chimpanzees in their natural habitats.

Fig. 4(a) showcases the distribution of the annotated behaviors, with social interactions constituting approximately 35% of the total annotations. Furthermore, Fig. 4(b) delineates the distribution of labeled social behaviors across distinct individuals, highlighting grooming, playing, and touching as predominant activities within the social dynamics of the group-living chimpanzees.

In essence, ChimpACT emerges as an invaluable resource for researchers spanning the domains of primatology, comparative psychology, computer vision, and machine learning. It furnishes a comprehensive and varied array of annotations, paving the way for in-depth analysis of multifaceted chimpanzee behaviors and catalyzing the development of advanced machine learning algorithms. The inherent long-tail distribution not only presents a formidable challenge for chimpanzee identification and behavior recognition but also beckons explorations into few-shot learning in future endeavors.

## 4 Experiments

To rigorously assess ChimpACT, we benchmark a suite of representative methods across the aforementioned three tracks: (i) detection, tracking, and ReID, (ii) pose estimation, and (iii) spatiotemporal action detection. Our computational framework leverages four NVIDIA GeForce RTX 3090 GPUs (24GB) for both training and evaluation across all tracks. In the subsequent sections, we delve into the implementation details, baseline methods, and evaluation metrics for each track.

### Detection, tracking, and ReID

SettingWe evaluate several prominent Multiple Object Tracking (MOT) algorithms on ChimpACT, including both classical methods such as SORT (Bewley et al., 2016), DeepSORT (Wojke et al., 2017), and Tracktor (Bergmann et al., 2019), as well as the state-of-the-art methods such as ByteTrack (Zhang et al., 2022), and OC-SORT (Cao et al., 2023). All implementations are based on the MMTracking (Contributors, 2020) codebase. For those methods supporting flexible detection backbones, we trial two typical detectors, including the two-stage detector Faster R-CNN (Ren et al., 2015) and the one-stage detector YOLOX (Ge et al., 2021). Each method undergoes training for 10 epochs, adhering to the official configurations, which encompass optimizer settings, batch size, data augmentation techniques, and pre-trained models. Given that the three classical methods lack inherent ReID

Figure 5: **(a) Distribution of the annotated behavior categories. (b) Distribution showcasing individuals alongside their respective social behaviors. Vector graphics; zoom for details.**

modules, we supplement with a dedicated ReID network built on ResNet-50 (He et al., 2016). The training curves of select methods (refer to Fig. A2a) affirm convergence within the training epochs.

We split the video clips in ChimpACT into 80% train, 10% validation, and 10% test. Both the train set and test set cover all the individuals. Models are trained on the training set, with performance metrics reported on the test set. We employ widely-accepted evaluation metrics, drawing from convention in human/object detection, tracking, and ReID (Bewley et al., 2016; Pang et al., 2021; Zhang et al., 2022). Specifically, we utilize (i) mean Average Precision (mAP) Lin et al. (2014) to gauge the detection accuracy, and (ii) the CLEAR metrics (Bermardin and Stiefelhagen, 2008) (MOTA, MOTP, FP, FN, IDs), IDF1 (Ristani et al., 2016), and HOTA (Luiten et al., 2021) to evaluate various facets of the tracking performance. It is worth noting that for FP, FN, and IDs, we report normalized values and denote these metrics as nFP, nFN, and nIDs, respectively.

ResultsTab. 3 summarizes these tracking algorithms' performances on the ChimpACT test set. We conducted three runs for each method and reported the average and variance of these metrics. Notably, the variance across multiple runs is minimal, underscoring the robust reproducibility of our benchmarking. A holistic view of the results reveals that QDTrack (Pang et al., 2021) emerges as the top performer. However, it does suffer from a higher count of identity switches compared to other methods. In terms of detection performance, the YOLOX algorithm (Ge et al., 2021) stands toe-to-toe with Faster R-CNN (Ren et al., 2015). A discernible trend is evident among contemporary tracking methods, which seem to excel in identity association capabilities over their classical counterparts. This is corroborated by marked improvements in tracking metrics like IDF1 and IDs. Such a trend intimates that the latest tracking methods might be adept at maintaining consistent object identities, a pivotal aspect when tracking and analyzing individual trajectories within chimpanzee cohorts.

While the results garnered by the array of tracking algorithms are commendable, they still lag behind the benchmarks set on human-centric datasets (Zhang et al., 2022; Pang et al., 2021; Cao et al., 2023). This disparity can be attributed to challenges like the low contrast and low color variation of the body fur of chimpanzees, compounded by intricate self-occlusions. Nonetheless, this very observation accentuates the significance of ChimpACT. It not only offers a challenging arena for tracking algorithms but also stands as an ideal platform for pioneering and refining tracking methods tailored for chimpanzees and other non-human primates.

### Pose estimation

SettingWe benchmark several state-of-the-art human pose estimation methods on ChimpACT, including CPM (Wei et al., 2016), SimpleBaseline (Xiao et al., 2018), HRNet (Sun et al., 2019), DarkPose (Zhang et al., 2020). Broadly, human pose estimation methods can be bifurcated into two primary paradigms: heatmap-based and regression-based. We harness the MMPose (Contributors, 2020) framework for implementing these methods. Please refer to Appx. C for more implementation details. All the models undergo training for 210 epochs, maintaining the official configurations for optimizers, batch sizes, and learning rates. To gauge any potential model overfitting, we present the validation curve on the AP metric in Fig. A2b, reassuringly suggesting an absence of overfitting.

   Method & Detector & ReID & HOTA \(\) & MOTA \(\) & MOTP \(\) & IDF1 \(\) & mAP \(\) & nFP \(\) & nFN \(\) & nIDs \(\) \\ 
**SORT** & Faster R-CNN (Bewley et al., 2016) & ResNet-50 & 42.6\(1.0\\ 39.8\) & 43.2\(1.3\\ 39.8\) & 20.3\(2.1\\ 32.13\) & 20.3\(3.7\\ 37.1\) & 71.4\(1.6\\ 16.1\) & 16.1\(3.3\\ 3.1\) & 37.8\(1.7\\ 2.8\) & 2.8\(0.5\\ 3.1\) \\ 
**DeepSORT** & Faster R-CNN (Wojke et al., 2017) & VGG & ResNet-50 & 40.2\(1.0\\ 40.2\) & 43.2\(1.2\\ 1.2\) & 20.3\(0.3\\ 3.8\) & 38.4\(1.9\\ 3.1\) & 71.4\(1.6\\ 16.1\) & 16.1\(3.3\\ 3.1\) & 37.8\(1.7\\ 2.9\) \\ 
**Trackor** & Faster R-CNN (Bergman et al., 2019) & Faster R-CNN & ResNet-50 & 49.5\(1.0\\ 5.7\) & 50.5\(1.1\\ 2.1\) & 22.6\(1.1\\ 55.6\) & 55.6\(1.2\\ 1.2\) & 70.7\(1.6\\ 13.8\) & 13.8\(0.5\\ 35.2\) & 35.2\(0.7\\ 0.5\) \\ 
**QDTrack** & & & & & & & & & & \\ (Pang et al., 2021) & Faster R-CNN & - & 50.3\(2.3\\ 3.2\) & 54.2\(1.4\\ 2.4\) & 22.2\(1.4\\ 2.14\) & 55.8\(1.3\\ 2.8\) & 77.8\(1.2\\ 2.0\) & 19.7\(1.3\\ 2.8\) & 24.6\(0.8\\ 1.4\) & 1.4\(0.2\\ 2.2\) \\ 
**ByteTrack** & Faster R-CNN (Zhang et al., 2022) & Faster R-CNN & - & 43.7\(2.0\\ 49.2\) & 36.9\(2.2\\ 2.0\) & 24.6\(0.3\\ 49.2\) & 48.8\(1.3\\ 2.0\) & 68.2\(1.1\\ 2.1\) & 27.7\(1.1\\ 3.1\) & 34.2\(1.0\\ 2.1\) & 1.2\(0.2\\ 2.0\) \\ 
**OC-SORT** & Faster R-CNN & - & 43.4\(1.0\\ 47.9\) & 38.2\(1.9\\ 42.4\) & 24.3\(0.2\\ 1.2\) & 48.7\(2.2\\ 2.8subarraysubarraysubarraysubarray} {subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray}\}&subarraysubarraysubarray \}&subarraysubarraysubarraysubarraysubarraysubarray \} {arrayarrayarrayarray} \\) & & 25.0\( &&&&\\\\ 2.0& & & & & & \\ (Cao et al., 2023) & YOLOX & - & 47.9\(1.4\\ 42.1 {subarraysubarraysubarraysubarraysubarraysubarraysubarray}&subarraysubarraysubarraysubarray \}{arrayarrayarrayarrayarrayarrayarrayarrayarray *) &&&&&\\ &&&& & & & \\ (Cao et al., 2023) & YOLOX & - & 63.2 & 78.0 & \(\) & 77.5 & \(\) & 2.7 & 19.0 & 0.3 \\   

Table 3: **Results of the detection, tracking, and ReID track on the ChimpACT test set.** The row highlighted in light blue is the performance reference on the human tracking dataset MOT-17 (Milan et al., 2016). \(-\) denotes not applicable. \(\) denotes unreported.

The train/test partitioning mirrors that of the first track. We use mAP with various thresholds, adhering to the conventions of human pose estimation (Lin et al., 2014). Additionally, we report the Percentage of Correctly estimated Keypoints (PCK) metric (Andriluka et al., 2014; Ng et al., 2022). PCK@\(\) quantifies the fraction of accurately predicted keypoints within a distance threshold defined as \( max(height,width)\), derived from the bounding box of the chimpanzee. This metric is widely recognized for its accuracy in body joint localization in both human and animal pose estimation.

ResultsTab. 4 consolidates these pose estimators' performances on the ChimpACT test set. Notably, the heatmap-based DarkPose (Zhang et al., 2020) with an HRNet (Sun et al., 2019) backbone emerges as the top-performing model. This trend aligns with observations in human pose estimation, where heatmap-centric methods (Wei et al., 2016; Xiao et al., 2018; Newell et al., 2016; Sun et al., 2019) predominantly lead the pack, attributed to their robustness against pose and appearance variations. However, the heatmap representation may be less accurate in scenarios where multiple joints are occluded or closely spaced, and it demands better computational and memory resources. Conversely, the newer regression-based methods (Li et al., 2021) are computationally leaner but tend to be more susceptible to overfitting and generally lag in performance.

These results underscore that the task of chimpanzee pose estimation is distinct and nuanced, and cannot be seamlessly addressed by merely repurposing human-centric pose estimation methods. We believe there are two primary reasons for this: (i) chimpanzees exhibit unique joint flexibility and a broader range of motion, and (ii) the visual texture and appearance of chimpanzee fur diverge significantly from human skin. These insights emphasize the need for chimpanzee specific pose estimation strategies.

### Spatiotemporal action detection

SettingWe benchmark four representative human action detection baselines on ChimpACT using the MMAction2 (Contributors, 2020c) codebase, including ARCN (Sun et al., 2018), LFB (Wu et al., 2019), and SlowFast with its variant SlowOnly (Feichtenhofer et al., 2019). All models undergo training for 20 epochs with a batch size of 32. Convergence is evident from the training curves

    & Backbone & PCK@0.05 & PCK@0.1 & AP & AP\({}^{50}\) & AP\({}^{75}\) & AP\({}^{M}\) & AP\({}^{L}\) & AR \\    } & SimpleBaseline & ResNet-50 & 25.3\(\)0.5 & 46.2\(\)0.5 & 8.6\(\)0.4 & 27.4\(\)1.3 & 3.9\(\)0.4 & 0.3\(\)0.1 & 12.5\(\)0.5 & 17.3\(\)0.7 \\  & (Xiao et al., 2018) & ResNet-101 & 26.2\(\)1.0 & 46.4\(\)1.1 & 8.7\(\)0.4 & 27.5\(\)0.6 & 4.2\(\)0.5 & 0.3\(\)0.0 & 12.9\(\)0.2 & 17.7\(\)0.4 \\  & (Xiao et al., 2018) & ResNet-152 & 26.3\(\)0.4 & 47.3\(\)0.8 & 9.3\(\)0.1 & 29.2\(\)1.1 & 4.7\(\)0.3 & 0.5\(\)0.0 & 13.4\(\)0.2 & 16.6\(\)0.0 \\   & & MobileNetV2 & 27.5\(\)1.4 & 48.1\(\)1.7 & 16.7\(\)0.8 & 43.1\(\)2.7 & 11.1\(\)0.8 & 2.0\(\)10.7 & 17.7\(\)0.8 & 19.5\(\)0.9 \\   &   } & ResNet-50 & 28.2\(\)1.7 & 47.1\(\)1.3 & 16.3\(\)2.5 & 41.2\(\)9.1 & 41.4\(\)1.4 & 1.3\(\)0.8 & 17.4\(\)2.8 & 20.0\(\)1.6 \\  & (Li et al., 2021) & ResNet-101 & 28.2\(\)3.5 & 46.5\(\)4.3 & 16.2\(\)2.6 & 41.1\(\)5.7 & 10.8\(\)2.4 & 2.1\(\)1.0 & 17.3\(\)2.8 & 20.1\(\)2.1 \\  & & ResNet-152 & 30.0\(\)1.3 & 48.4\(\)2.2 & 18.1\(\)2.8 & 43.0\(\)7.9 & 13.5\(\)0.6 & 1.4\(\)0.3 & 19.2\(\)3.2 & 22.3\(\)1.1 \\    } & CPM & 40.7\(\)0.2 & 60.4\(\)0.0 & 21.6\(\)0.1 & 51.0\(\)0.4 & 17.1\(\)0.1 & 9.5\(\)0.6 & 22.4\(\)0.1 & 25.4\(\)0.1 \\  & (Wei et al., 2016) & Hourglass-4 & 44.6\(\)0.5 & 60.8\(\)0.1 & 20.6\(\)0.3 & 48.9\(\)0.1 & 16.0\(\)0.4 & 4.6\(\)0.1 & 23.7\(\)0.6 & 28.2\(\)0.2 \\   &   } & MobileNetV2 & 39.8\(\)0.4 & 59.4\(\)0.4 & 19.4\(\)0.1 & 48.5\(\)0.6 & 14.3\(\)0.8 & 2.3\(\)0.1 & 20.6\(\)0.1 & 23.2\(\)0.1 \\  & & & & & & & & & \\   &   } & ResNet-50 & 43.3\(\)0.2 & 61.7\(\)1.2 & 22.1\(\)0.2 & 51.5\(\)0.4 & 17.7\(\)0.2 & 3.7\(\)0.4 & 23.4\(\)0.2 & 26.3\(\)0.1 \\  & & ResNet-101 & 42.8\(\)0.3 & 60.7\(\)0.2 & 21.7\(\)0.1 & 52.5\(\)0.4 & 16.7\(\)0.0 & 4.3\(\)0.2 & 23.0\(\)0.1 & 26.2\(\)0.2 \\  & (Xiao et al., 2018) & ResNet-152 & 43.9\(\)0.4 & 61.6\(\)0.1 & 22.7\(\)0.4 & 53.3\(\)0.6 & 18.3\(\)0.4 & 5.3\(\)0.5 & 23.9\(\)0.4 & 27.1\(\)0.1 \\    } & HRNet & HRNet-W32 & 48.6\(\)0.9 & 65.6\(\)0.6 & 25.9\(\)0.4 & 58.2\(\)0.8 & 22.1\(\)0.4 & 6.1\(\)0.4 & 27.0\(\)0.6 & 30.3\(\)0.5 \\  & (Sun et al., 2019) & HRNet-W48 & 47.3\(\)0.2 & 64.5\(\)0.2 & 25.1\(\)0.1 & 57.2\(\)0.6 & 21.0\(\)0.1 & 6.9\(\)0.9 & 26.2\(\)0.3 & 29.6\(\)0.1 \\   &   } & ResNet-50 & 43.7\(\)0.0 & 62.1\(\)0.6 & 22.8\(\)0.1 & 53.8\(\)0.8 & 18.8\(\)0.6 & 3.4\(\)0.2 & 24.1\(\)0.0 & 27.1\(\)0.1 \\  & & ResNet-101 & 43.1\(\)0.9 & 61.2\(\)1.4 & 22.1\(\)0.3 & 52.6\(\)0.6 & 17.6\(\)0.4 & 4.0 Fig. A2c. We maintain consistent optimizers and learning rates as in official implementations. Ground-truth bounding boxes for each chimpanzee are provided during both training and testing, as per Tang et al. (2020). Please refer to Appx. C for further details on ablative modules.

We adopt the same train-test split as previous tracks. Performance is gauged using mAP across 23 action classes, as per standard (Feichtenhofer et al., 2019; Tang et al., 2020). Additionally, we evaluate the mAP within the four behavioral types separately.

ResultsTab. 5 summarizes the action detection algorithms' performances on the ChimpACT test set. The overall mAP aligns with results on human action datasets, underscoring the feasibility of automated action detection for video coding and further analyses. Locomotion behaviors achieve a notably higher mAP, likely due to their solitary nature and distinct patterns. Conversely, Conversely, the "others" category registers the lowest mAP, attributed to its limited data--comprising just 0.14% of action instances across two fine-grained classes. This imbalance suggests the potential benefit of few-shot learning methods in the future. The results highlight both the promise and areas for improvement in the dataset, positioning it as a valuable platform for advancing spatiotemporal action detection algorithms. We anticipate that ChimpACT will further studies into the social dynamics of non-human primates in semi-naturalistic environments.

## 5 Conclusion

In this work, we introduced ChimpACT, a novel longitudinal video dataset capturing the intricate behaviors of group-living chimpanzees, focusing on the juvenile chimpanzee, Azibo. Our meticulous annotations and diverse social interactions within the dataset offer a unique view into the world of our closest evolutionary relatives. Through comprehensive experiments, we underscored the challenges and nuances of applying human-centric computer vision algorithms to the distinct behaviors and interactions of chimpanzees. The dataset's depth, combined with its long-tail distribution, not only emphasizes its significance but also paves the way for interdisciplinary research bridging primatology, comparative psychology, computer vision, and machine learning. By making this resource available, our aspiration is to catalyze advancements in video understanding, inspire the research community to craft specialized techniques for non-human primates and deepen our collective insights into their intricate social fabric and dynamics.

Limitation and future workChimpACT is based on captive chimpanzees living in a semi-natural environment, limiting the observable range of behaviors. Natural foraging, responses to predators, and intergroup encounters are absent. Focusing on Azibo overrepresents certain individuals and underrepresents others, limiting the assessment of the full social network. Nevertheless, we plan to contribute more data and labels to create a larger and more comprehensive chimpanzee dataset.

   Method & Frame sampling & Module & mAP & mAP\({}_{L}\) & mAP\({}_{O}\) & mAP\({}_{S}\) & mAP\({}_{o}\) \\  ACRN & \(8 8 1\) & & 24.4\({}_{ 0.5}\) & 58.7\({}_{ 0.7}\) & 33.8\({}_{ 1.7}\) & 14.7\({}_{ 0.4}\) & 0.0\({}_{ 0.0}\) \\ Sun et al. (2018) & \(4 16 1\) & & 23.9\({}_{ 1.3}\) & 57.8\({}_{ 0.4}\) & 35.0\({}_{ 0.4}\) & 13.8\({}_{ 1.6}\) & 0.0\({}_{ 0.0}\) \\ 
**LFB** & \(4 16 1\) & _w_: NL LFB & 22.0\({}_{ 0.9}\) & 50.1\({}_{ 0.8}\) & 32.3\({}_{ 0.9}\) & 13.5\({}_{ 1.6}\) & 0.6\({}_{ 0.1}\) \\ Wu et al. (2019) & \(4 16 1\) & _w_: Max LFB & 23.2\({}_{ 0.7}\) & 45.0\({}_{ 1.5}\) & 31.2\({}_{ 0.8}\) & 17.7\({}_{ 1.4}\) & 0.5\({}_{ 0.0}\) \\  & \(4 16 1\) & _w_: Avg LFB & 21.3\({}_{ 1.6}\) & 45.0\({}_{ 3.6}\) & 29.8\({}_{ 1.1}\) & 14.7\({}_{ 2.6}\) & 0.5\({}_{ 0.0}\) \\   & \(8 8 1\) & & 20.9\({}_{ 1.9}\) & 48.1\({}_{ 1.7}\) & 36.2\({}_{ 2.8}\) & 11.5\({}_{ 1.0}\) & 0.0\({}_{ 0.1}\) \\ SlowOnly & \(4 16 1\) & & 19.2\({}_{ 1.1}\) & 47.0\({}_{ 2.5}\) & 28.3\({}_{ 2.5}\) & 11.0\({}_{ 1.2}\) & 0.0\({}_{ 0.1}\) \\ Feichtenhofer et al. (2019) & \(8 8 1\) & _w_: Ctx & 22.3\({}_{ 1.9}\) & 52.3\({}_{ 3.2}\) & 31.2\({}_{ 1.3}\) & 13.8\({}_{ 2.4}\) & 0.1\({}_{ 0.1}\) \\  & \(4 16 1\) & _w_: Ctx & 21.4\({}_{ 0.9}\) & 47.6\({}_{ 2.0}\) & 33.0\({}_{ 1.2}\) & 13.2\({}_{ 2.2}\) & 0.2\({}_{ 0.1}\) \\   & \(8 8 1\) & & 21.9\({}_{ 1.0}\) & 53.0\({}_{ 0.7}\) & 30.6\({}_{ 2.2}\) & 12.9\({}_{ 1.2}\) & 0.0\({}_{ 0.1}\) \\ SlowFast & \(4 16 1\) & & 22.0\({}_{ 0.8}\) & 52.9\({}_{ 2.3}\) & 33.1\({}_{ 2.3}\) & 12.6\({}_{ 0.9}\) & 0.0\({}_{ 0.0}\) \\ Feichtenhofer et al. (2019) & \(8 8 1\) & _w_: Ctx & 24.3\({}_{ 0.6}\) & 56.8\({}_{ 1.6}\) & 31.5\({}_{ 2.0}\) & 15.6\({}_{ 0.8}\) & 0.1\({}_{ 0.1}\) \\  & \(4 16 1\) & _w_: Ctx & 24.1\({}_{ 0.9}\) & 56.6\({}_{ 2.0}\) & 34.7\({}_{ 2.7}\) & 14.6\({}_{ 0.4}\) & 0.1\({}_{ 0.1}\) \\  SlowFast & \(8 8 1\) & & 25.8 & – & – & – & – \\ Feichtenhofer et al. (2019) & \(8 8 1\) & & 25.8 & – & – & – & – \\   

Table 5: **Results of spatiotemporal action detection track on ChimpACT test set. The row highlighted in light blue is the performance reference on the human action dataset AVA (Gu et al., 2018). - denotes not applicable. \({}^{*}\)w. NL/Max/Avg LFB” denotes using non-local, max, or average LFB module. \({}^{*}\)w. Ctx” indicates using both the RoI feature and the global pooled feature for classification. \({}^{*}\)mAP, \({}^{*}\)mAP\({}_{L}\), \({}^{*}\)mAP\({}_{O}\), \({}^{*}\)mAP\({}_{S}\), and \({}^{*}\)mAP\({}_{o}\) represent the overall mAP and mAP for \(\)oconotion, \(\)bject interaction, \(\)ocial interaction, and \(\)thers.**