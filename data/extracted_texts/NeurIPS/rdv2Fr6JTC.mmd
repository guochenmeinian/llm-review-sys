# Precedence-Constrained Winter Value for Effective Graph Data Valuation

Hongliang Chi\({}^{1}\)  Wei Jin\({}^{2}\)  Charu Aggarwal\({}^{3}\)  Yao Ma\({}^{1}\)

\({}^{1}\)Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180

\({}^{2}\)Department of Computer Science, Emory University, Atlanta, GA 30322

\({}^{3}\)IBM T. J. Watson Research Center, Yorktown Heights, NY 10598

{chih3,may13}@rpi.edu, wei.jin@emoryy.edu, charu@us.ibm.com

###### Abstract

Data valuation is essential for quantifying data's worth, aiding in assessing data quality and determining fair compensation. While existing data valuation methods have proven effective in evaluating the value of Euclidean data, they face limitations when applied to the increasingly popular graph-structured data. Particularly, graph data valuation introduces unique challenges, primarily stemming from the intricate dependencies among nodes and the growth in value estimation costs. To address the challenging problem of graph data valuation, we put forth an innovative solution, **P**recedence-**C**onstrained **Winter** (PC-Winter) Value, to account for the complex graph structure. Furthermore, we develop a variety of strategies to address the computational challenges and enable efficient approximation of PC-Winter. Extensive experiments demonstrate the effectiveness of PC-Winter across diverse datasets and tasks.

## 1 Introduction

The abundance of training data has been a key driver of recent advancements in machine learning (ML) . As models and the requisite training data continue to expand in scale, data valuation has gained significant attention due to its ability to quantify the usefulness of data for ML tasks and determine fair compensation [28; 34]. Notable techniques in this field include Data Shapley  and its successors [20; 39; 29], which have gained prominence in assessing data value. Despite the promise of these methods, they are primarily designed for Euclidean data, where samples are often assumed to be independent and identically distributed (i.i.d.). Given the prevalence of graph-structured data in the real world [10; 31; 22], there arises a compelling need to perform data valuation for graphs. However, due to the interconnected nature of samples (nodes) on graphs, existing data valuation frameworks are not directly applicable to addressing the graph data valuation problem.

In particular, designing data valuation methods for graph-structured data faces several fundamental challenges: **Challenge I:** Graph machine learning algorithms such as Graph Neural Networks (GNNs) [19; 37; 41] often involve both labeled and unlabeled nodes in their model training process. Therefore, unlabeled nodes, despite their absence of explicit labels, also hold intrinsic value. Existing data valuation methods, which typically assess a data point's value based on its features and the associated label, do not readily accommodate the valuation of unlabeled nodes within graphs. **Challenge II:** Nodes in a graph contribute to model performance in an interdependent and complex way: (1) Unlabeled nodes, while not providing direct supervision, can contribute to model performance by potentially affecting multiple labeled nodes through message-passing. (2) Labeled nodes, on the otherhand, contribute by providing direct supervision signals for model training, and similarly to unlabeled nodes, they also contribute by affecting other labeled nodes through message-passing. **Challenge III:** Traditional data valuation methods are often computationally expensive due to repeated retraining of models . The challenge is magnified in the context of graph-structured data, where samples contribute to model performance in multifaceted manners. Additionally, the inherent message-passing mechanism in GNN models further amplifies the computational demands for model re-training.

In this work, we make _the first attempt_ to explore the challenging graph data valuation problem, to the best of our knowledge. In light of the aforementioned challenges, we propose the **P**recedence-**C**onstrained **Winter** (PC-Winter) Value, a pioneering approach designed to intricately unravel and analyze the contributions of nodes within graph structures, thereby offering a detailed perspective on the valuation of graph elements. Our key contributions are as follows:

* We formulate the graph data valuation problem as a unique cooperative game  with special coalition structures. Specifically, we decompose each node in the graph into several "players" within the game, each representing a distinct contribution to model performance. We then devise the PC-Winter to address the game, enabling the accurate valuation of all players. The PC-Winter values of these players can be conveniently combined to generate values for nodes and edges.
* To tackle the computational challenges of calculating PC-Winter values, we develop a set of strategies including _hierarchical truncation_ and _local propagation_. These strategies together enable an efficient approximation of PC-Winter values.
* Extensive experiments on various datasets and tasks, along with detailed ablation studies and parameter analyses, validate the effectiveness of PC-Winter and provide insights into its behavior.

## 2 Preliminary and Related Work

In this section, we delve into some fundamental concepts that are essential for developing our methodology. More extensive literature exploration can be found in Appendix A.

### Cooperative Game Theory

Cooperative game theory explores the dynamics where players, or decision-makers, can form alliances, known as coalitions, to achieve collectively beneficial outcomes [2; 7]. The critical components of such a game include a _player set_\(\) consisting of all players in the game and a _utility function_\(U()\), which quantifies the value or payoff that each coalition of players can attain. Shapley Value  is developed to fairly and efficiently distribute payoffs (values) among players.

**Shapley value.** The Shapley value \(_{i}(,U)\) for a player \(i\) can be defined on permutations of \(\) as follows.

\[_{i}(,U)=)|}_{()}[U(_{i}^{}\{i\})-U(_{i}^ {})]\] (1)

where \(()\) denotes the set of all possible permutations of \(\) with \(|()|\) denoting its cardinality, and \(_{i}^{}\) is predecessor set of \(i\), i.e, the set of players that appear before player \(i\) in a permutation \(\):

\[_{i}^{}=\{j(j)<(i)\}.\] (2)

The Shapley value considers each player's contribution to every possible coalition they could be a part of. Specifically, in Eq. (1), for each permutation \(\), the _marginal contribution_ of player \(i\) is calculated as the difference in the utility function \(U\) when player \(i\) is added to an existing coalition \(_{i}^{}\). The Shapley value \(_{i}(,U)\) for \(i\) is the average of these marginal contributions across all permutations in \(()\). The Shapley value has been widely applied in ML for various tasks such as data valuation [13; 17] and model explanation [24; 11]. In the context of graph ML, it has been primarily used for GNN explainability [8; 47; 25; 1]. A more detailed discussion on Shapley Value on graph ML can be found in Appendix A.3.

**Winter Value.** The Shapley value is to address cooperative games, where players collaborate freely and contribute on an equal footing. However, in many practical cases, cooperative games, exhibit a _Level Coalition Structure_[26; 36; 48], reflecting a hierarchical organization. For instance, consider a corporate Figure 1: Level setting where different tiers of management and staff contribute to a project in Coalition Structurevarying capacities and with differing degrees of decision-making authority. Players within such a game are hierarchically categorized into nested coalitions with several levels, as depicted in Figure 1. The outermost and largest ellipse represents the entire coalition and each of the smaller ellipse within the largest ellipse symbolizes a "sub-coaliation" at various hierarchical levels. Collaborations originate within the smallest sub-coalitions at the base level (illustrated by the innermost ellipses in Figure 1. These base units are then integrated into the next level, facilitating inter-coalition collaboration and enabling contributions to ascend to higher levels. This bottom-up flow of contributions continues, with each layer consolidating and passing on inputs to the next, culminating in a multi-leveled collaborative contribution to the final objective of the entire coalition. To accommodate such complex Level Coalition Structure, Winter value  was introduced. Winter value follows a similar permutation-based definition as Shapley Value (Eq. (1)) but with only a specific subset of permutations that respect the Level Coalition Structure. In these permutations, members of the same sub-coalition, regardless of the level, must appear in an unbroken sequence without interruptions. This ensures that the value attributed to each player is consistent with the level structure of the coalition. A formal definition of the Winter value can be found in Appendix B.

### Data Valuation and Data Shapley

Data valuation quantifies the contribution of data points for machine learning tasks. The seminal work  introduces Data Shapley, applying cooperative game theory to data valuation, where training samples are the _players_\(\), and the _utility function_\(U\) assesses a model's performance on subsets of these players using a validation set. With \(\) and \(U\), data values can be calculated with Eq. (1). However, Data Shapley and subsequent methods [13; 20; 39] primarily focus on i.i.d. data, overlooking potential coalitions or dependencies among data points.

### Graphs and Graph Neural Networks

Consider a graph \(=\{,\}\) where \(\) denotes the set of nodes and \(\) denotes the set of edges. Each node \(v_{i}\) carries a feature vector \(_{i}^{d}\), where \(d\) is the dimensionality of the feature space. Additionally, each node \(v_{i}\) is associated with a label \(y_{i}\) from a set of possible labels \(\). We assume that only a subset \(_{l}\) are with known labels.

GNNs [19; 37; 41] are prominent models for graph ML tasks. Specifically, from a local perspective for node \(v_{i}\), the \(k\)-th GNN layer generally performs a feature averaging process as \(_{i}^{(k)}=)}_{v_{j}(v_{ i})}_{j}^{(k-1)}\), where \(\) is the parameter matrix, \(deg(v_{i})\) and \((v_{i})\) denote the degree and neighbors of node \(v_{i}\), respectively. After a total of \(K\) layers, \(_{i}^{(K)}\) are utilized as the learned representation of \(v_{i}\). Such a feature aggregation process can be also described with a \(K\)-level _computation tree_ rooted on node \(v_{i}\).

**Definition 1** (Computation Tree).: _For a node \(v_{i}\), its \(K\)-level computation tree corresponding to a \(K\)-layer GNN model is denoted as \(_{i}^{K}\) with \(v_{i}\) as its root node. The first level of the tree consists of the immediate neighbors of \(v_{i}\), and each subsequent level is formed by the neighbors of nodes in the level directly above. This pattern of branching out continues, expanding through successive levels of neighboring nodes until the depth of the tree grows to \(K\)._

The feature aggregation process in a \(K\)-layer GNN can be regarded as a bottom-up feature propagation process in the computation tree, where nodes in the lowest level are associated with their initial features. Therefore, the final representation \(_{i}^{(K)}\) of a node \(v_{i}\) is affected by all nodes within its \(K\)-hop neighborhood, which is referred to as the _receptive field_ of node \(v_{i}\). The GNN model is trained using the (\(_{i}^{(K)}\), \(y_{i}\)) pairs, where each labeled node \(v_{i}\) in \(_{l}\) is represented by its final representation and corresponding label. _Thus, in addition to labeled nodes, those unlabeled nodes that are within the receptive field of labeled nodes also contribute to model performance_.

## 3 Methodology

In classic machine learning models designed for Euclidean data, such as images and texts, training samples are typically assumed as i.i.d. Thus, each labeled sample contributes to the model performance by directly providing supervision signals through the training objective. However, due to the interdependent nature of graph data, nodes in a graph contribute to GNN performance in a more complicated way, which poses unique challenges. Specifically, as discussed in Section 2.3, both labeled and unlabeled nodes are involved in the training stage through the feature aggregation. Next, we discuss how these nodes contribute to GNN performance.

**Observation 1**.: _Unlabeled nodes influence GNN performance by affecting the final representation of labeled nodes. On the other hand, labeled nodes can contribute to GNN performance in two ways: (1) they provide direct supervision signals to GNN with their labels, and (2) just like unlabeled nodes, they can impact the final representation of other labeled nodes through feature aggregation. Note that both labeled nodes and unlabeled nodes can affect the final representations of multiple labeled nodes, as long as they lie within the receptive field of these labeled nodes. Hence, a single node can make multifaceted and heterogeneous contributions to GNN performance by affecting multiple labeled nodes in various manners._

### The Graph Data Valuation Problem

Based on Observation 1, due to the heterogeneous and diverse effects of labeled and unlabeled nodes, it is necessary to perform fine-grained data valuation on graph data elements. In particular, we propose to decompose a node into distinct "duplicates" corresponding to their impact on different labeled nodes. We then aim to obtain values for all "duplicates" of these nodes. This could clearly express and separate how nodes impact GNN performance in various aspects. Following existing literature , we approach the graph data value problem through a cooperative game. Next, we introduce the _player set_ and the _utility function_ of this game. In general, we define the graph data valuation game based on \(K\)-layer GNN models.

**Definition 2** (Player Set).: _The player set \(\) in a graph data valuation game is defined as the union of nodes in the computation trees of labeled nodes. Duplication of nodes may occur within a single computation tree \(_{i}^{K}\) or across different labeled nodes' computation trees. In the graph data valuation game, these potential duplicates are treated as distinct players, uniquely identified by their paths to the corresponding labeled node. We define the player set \(\) as the set of all these distinct players across the computation trees of all labeled nodes in \(_{l}\)._

**Definition 3** (Utility Function).: _Given a subset \(\), we first generate a node-induced graph \(G_{in}()\) using their corresponding edges in the computation trees. Then, a GNN model \(\) is trained on the induced graph \(G_{in}()\). Its performance is evaluated on a held-out validation set to serve as the utility of \(\), calculated as \(U()=acc((G_{in}()))\), where \(acc\) measures the accuracy of the trained GNN model \((G_{in}())\) on a held-out validation set._

The goal of the graph data valuation problem is to assign a value to all players in \(\) with the help of the _utility function_\(U\). When calculated properly, these values are supposed to provide a detailed understanding of how players in \(\) contribute to the GNN performance in a fine-grained manner. Furthermore, these values can be flexibly combined to generate higher-level values for nodes and edges, which will be discussed in Section 3.5.

### Precedence-Constrained Winter Value

As discussed in Section 2.3, the final representations of a labeled node \(v_{i}\) come from the hierarchical collaboration of all players in the computation tree \(_{i}^{K}\). These labeled nodes with the updated representations then contribute to the GNN performance through the training objective. Such a contribution process forms a hierarchical collaboration between the players in \(\), which can be illustrated with a _contribution tree_\(\) as shown in Figure 1(a). In particular, the _contribution tree_\(\) is constructed by linking the root nodes of the computation trees of all labeled nodes with a dummy node representing the GNN training objective \(\). In Figure 1(a), for the ease of illustration, we set \(K=2\), include only \(2\) labeled nodes, i.e, \(v_{0},v_{1}\), and utilize \(w_{i}\), \(u_{i}\) to denote the nodes in the lower level. The subtree rooted at a labeled node \(v_{i}\) is the corresponding computation tree \(_{i}^{2}\). With this, we observe the following about the coalition structure of the graph data valuation game.

**Observation 2** (Level Coalition Structure).: _As shown in Figure 1(a), the players in \(\) hierarchically collaborate to contribute. At the bottom level, the players are naturally grouped by their parents. Specifically, players with a common parent such as \(u_{0},u_{1},u_{2}\) with their parent \(w_{0}\), establish a foundation sub-coalition. This sub-coalition is clearly depicted in Figure 1(b). Moving up the tree,these parent nodes, like \(w_{0}\), serve as "delegates" for their respective sub-coalitions, further engaging in collaborations with other sub-coalitions. This interaction forms higher-level sub-coalitions, such as the one between \(w_{0}\), \(w_{1}\), \(w_{2}\), and \(v_{0}\) in Figure 1(b), indicating inter-coalition cooperation. This ascending process of coalition formation continues until the root node \(\) is reached, which represents the objective of the entire coalition consisting of all players. The depicted hierarchical collaboration process aligns with the Level Coalition Structure discussed in Section 2.1._

While the contribution tree shares similarities with the Level Coalition Structure illustrated in Section 2.1, a pivotal distinction lies in the representation and function of "delegates" (highlighted in red in Figure 1(b)) within each coalition. In the traditional Level Coalition Structure, contributions within a sub-coalition are made collectively, with each player or lower-level sub- coalition participating on an equitable basis. In contrast, the contribution tree framework distinguishes itself by designating a "delegate" within each sub-coalition, a player that represents and advances the collective contributions, establishing a directed and tiered flow of influence, hence forming a Unilateral Dependency Structure.

**Observation 3** (Unilateral Dependency Structure).: _In the contribution tree framework, a player \(p\) contributes to the final objective through a hierarchical pathway facilitated by its ancestors (its "delegates" at different levels). Therefore, the collaboration between players in \(\) exhibits a Unilateral Dependency Structure, where a player \(p\)'s contribution is dependent on its ancestors._

According to these two observations, the players demonstrate unique coalition structures in the graph data valuation game. We aim to propose a permutation-based valuation framework similar to Eq. (1) to address the cooperative game with both Level Coalition Structure and Unilateral Dependency Structure. In particular, instead of utilizing all the permutations as in Eq. (1), only the _permissible permutations_ aligning with such coalition structures are included in the value calculations. As we described in Section 2.1, cooperative games with Level Coalition Structure have been addressed by the Winter value . Specifically, a permutation respecting the Level Coalition Structure must ensure that players in the same (sub-)coalition, regardless of its level, are grouped together without interruption from other players . In our scenario, any subtree of the contribution tree corresponds to a sub-coalition as demonstrated in Figure 2. Hence, we need to ensure that for any player \(p\), the player \(p\) and its descendants in the contribution tree should be grouped together in the permutation. For example, the players \(w_{0},u_{0},u_{1},u_{2}\) should present together as a group in the permutation with potentially different orders. On the other hand, to ensure the Unilateral Dependency Structure, a permutation must maintain a partial order. Specifically, for any player \(p\) in the permutation, its descendants must present in later positions in the permutation than \(p\). Otherwise, the descendants of \(p\) cannot make non-trivial contributions, resulting in \(0\) marginal contributions.

We formally define the _permissible permutations_ that align with both Level Coalition Structure and Unilateral Dependency Structure utilizing the following two constraints.

**Constraint 1** (Level Constraint).: _For any player \(p\), the set of its descendants in the contribution tree is denoted as \((p)\). Then, a permutation \(\) aligning with the Level Coalition Structure satisfies the following Level Constraint: \(|[i]-[j]||(p)|, i,j(p) p, p ,\) where \([i]\) denotes the positional rank of the \(i\) in \(\)._

**Constraint 2** (Precedence Constraint).: _A permutation \(\) aligning with the Unilateral Dependency Structure satisfies the following Precedence Constraint: \([p]<[i], i(p), p.\)_

We denote the set of _permissible permutations_ satisfying both _Level Constraint_ and _Precedence Constraint_ as \(\). Then, we define the **P**recedence-**C**onstrained **W**inter** (PC-Winter) value for a player \(p\) with the permutations in \(\) as follows.

\[_{p}(,U)=_{}(U( _{p}^{} p)-U(_{p}^{})),\] (3)

where \(U()\) is the utility function (see Definition 3), and \(_{p}^{}\) denotes the predecessor set of \(p\) in \(\) as defined in Eq. (2).

Figure 2: Graph Data Valuation Game Structure

### Permissible Permutations for PC-Winter

To calculate PC-Winter value, it is required to obtain all permissible permutations. A straightforward way is to enumerate all permutations and only retain the permissible permutations. However, such an approach is extremely computationally intensive and typically not feasible in reality. In this section, to address this challenge, we propose a novel method to directly generate these permutations by traversing the contribution tree with Depth-First Search (DFS). Specifically, each DFS traversal results in a _preordering_, which is a list of the nodes (players) in the order that they were visited by DFS. Such a _preordering_ naturally defines a permutation of \(\) by simply removing the dummy node in the contribution tree from the _preordering_. By iterating all possible DFS traversals of the contribution tree, we can obtain all permutations in \(\), which is demonstrated in the following theorems.

**Theorem 1** (Specificity).: _Given a contribution tree \(\) with a set of players \(\), any DFS traversal over the \(\) results in a permissible permutation of \(\) that satisfies both the Level Constraint and Precedence Constraint._

**Theorem 2** (Exhaustiveness).: _Given a contribution tree \(\) with a set of players \(\), any permissible permutation \(\) can be generated by a corresponding DFS traversal of \(\)._

The proofs for two theorems can be found in C. Theorem 1 demonstrates that DFS traversals _specifically_ generate _permissible permutations_. On the other hand, Theorem 2 ensures the _exhaustiveness_ of generation, which allows us to obtain all permutations in \(\) by DFS traversal. Together, these two theorems ensure us to _exactly_ generate the set of _permissible permutations_\(\).

Notably, the calculation of PC-Winter value involves two steps: 1) generating \(\) with DFS traversals; and 2) calculating the PC-Winter value according to Eq. (3). Nonetheless, it can be done in a streaming way while we perform the DFS traversals. Specifically, once we reach a player \(p\) in a DFS traversal, we can immediately calculate its marginal contribution. The PC-Winter values for all players are computed by averaging their marginal contributions from all possible DFS traversals.

### Efficient Approximation of PC-Winter

Calculating the PC-Winter value for players in \(\) is infeasible due to computational intensity, arising from: 1) The exponential growth in the number of permissible permutations with more players, rendering exhaustive enumeration intractable; 2) The necessity to re-train the GNN within the utility function for each permutation, a process repeated \(||\) times to account for every player's marginal contribution; and 3) The intensive computation involved in GNN re-training, requiring feature aggregation over the graph that increases in complexity with the graph's size. These challenges necessitate an efficient approximation method for PC-Winter valuation in practical applications. We propose three strategies to address these computational issues.

**Permutation Sampling.** Following Data Shapley , we adopt Monte Carlo (MC) sampling to randomly sample a subset of permissible permutations denoted as \(_{s}\). Then, we utilize \(_{s}\) to replace \(\) in Eq. (3) for approximating PC-Winter value.

**Hierarchical Truncation.** GNN models often demonstrate a phenomenon of _neighborhood saturation_, i.e, these models achieve satisfactory performance even when trained on a subgraph using only a small subset of neighbors, rather than the full neighborhood , indicating diminishing returns from additional neighbors beyond a certain point. This indicates that for a player \(p\) in a permissible permutation \(\) generated by DFS over the contribution tree, the marginal contributions of its late visited child players are insignificant. Thus, we propose hierarchical truncation for efficiently obtaining the marginal contributions by directly approximating insignificant values as \(0\). Specifically, during the DFS traversal, given a truncation ratio \(r\), we only compute actual marginal contributions for players in the first \(1-r\) portion of each node's child subtrees, approximating the marginal contributions of players in the remaining subtrees as \(0\). For example, in Figure 1(a), given a truncation ratio \(r=2/3\), when DFS reaches player \(v_{0}\), we only calculate marginal contributions for players in the subtree rooted at \(w_{0}\). Furthermore, in the subtree rooted at \(w_{0}\), due to the hierarchical truncation, only the marginal contribution of \(u_{0}\) is evaluated, those for node \(u_{1}\) and \(u_{2}\) are set to \(0\). This approach is further optimized by adjusting truncation ratios based on the tree level, accommodating varying contribution patterns across levels. In particular, we organize the pair of truncation ratio as \(r_{1}\)-\(r_{2}\)indicating we truncate \(r_{1}\) (or \(r_{2}\)) portion of subtrees (or child players) of \(v_{i}\) (or \(w_{i}\)). We show how the hierarchical truncation helps tremendously reduce the model re-training in Appendix D.

**Local Propagation.** To enhance scalability, we leverage SGC  in our utility function, which simplifies GNNs by aggregating node features before applying an MLP. According to the Level Constraint (Constraint 1), the players within the same computation tree are grouped together in the permutation. Therefore, the induced graph of any coalition \(_{p}^{}\) defined by a permissible permutation consists of a set of separated computation trees (or a partial computation tree corresponding to the last visited labeled node in \(_{p}^{}\)). A key observation is that the feature aggregation process for the labeled nodes can be done independently within their own computation trees. Hence, instead of performing the feature propagation for the entire induced graph, we propose to perform _local propagation_ only on necessary computation trees. In particular, the aggregated representation for a labeled node is fixed after we traverse its entire computation tree in DFS. Therefore, for evaluating a player \(p\)'s marginal contribution, only the partial computation tree of the last visited labeled node requires _local propagation_, minimizing feature propagation efforts.

The PC-Winter values for all players are approximated with these three strategies in a streaming manner. In particular, we randomly traverse the contribution tree with DFS for \(|_{s}|\) times. During each DFS traversal, the marginal contributions for all players in \(\) are efficiently obtained with the help of _hierarchical truncation_ and _local propagation_. The marginal contributions calculated through these \(|_{s}|\) DFS traversals are averaged to approximate the PC-Winter value for all players. In Appendix H.5, we provide a detailed complexity analysis of the PC-Winter algorithm.

### From PC-Winter to Node and Edge Values

The PC-Winter values for players in \(\) can be flexibly combined to obtain the values for elements in the original graph, which are illustrated in this section. Specifically, as discussed in Section 3.1, multiple "duplicates" of a node \(v\) in the original graph may potentially present in \(\). Thus, we could obtain _node value_ for the node \(v\) by summing the PC-Winter values of all its "duplicates" in \(\). On the other hand, each player (except for the rooted labeled players) in \(\) corresponds to an "edge" in the contribution tree as identified by the player and its parent. For instance, in Figure 1(a), the player \(u_{0}\) corresponds to "edge" connecting \(u_{0}\) and \(w_{0}\). Therefore, DFS traversals also generate permutations for these "edges". From this perspective, the marginal contribution for a player \(p\) calculated through a DFS traversal can be also regarded as the marginal contribution of its corresponding edge, if we treat this process as gradually adding "edges" to connecting the players in \(\). Hence, the PC-Winter values for players in \(\) can be regarded as PC-Winter values for their corresponding "edges" in the contribution tree. Multiple "duplicates" of an edge \(e\) in the original graph may be present in the contribution tree. Hence, similar to the _node values_, we define the _edge value_ for \(e\) by taking the summation of the PC-Winter value for all its "duplicates" in the contribution tree.

## 4 Experiment

**Datasets and Settings.** We assess the proposed approach on six real-world benchmark datasets: Cora, Citeseer, and Pubmed , Amazon-Photo, Amazon-Computer, and Coauther-Physics . The detailed statistics of datasets are summarized in Table 2 in Appendix G. Our experiments focus on the inductive node classification task. The detailed setup of the inductive setting can be found in Appendix G.1. To obtain the PC-Winter values, we run permutations in a streaming way as described in Section 3.4. This process terminates with a convergence criterion as detailed in Appendix G.4. PC-Winter typically terminates with a different number of permutations for different datasets. The other hyper-parameters are detailed in Appendix G.5.

### Dropping High-Value Nodes

In this section, we aim to evaluate the quality of data values produced by PC-Winter via dropping high-value nodes from the graph. Dropping high-value nodes is expected to significantly diminish performance, and thus the performance observed after removing high-value nodes serves as a strong indicator of the efficacy of graph data valuation. Notably, PC-Winter values values are calculated as described in Section 3.5.

To demonstrate the effectiveness of PC-Winter, we include Random value, Degree value, Leave-one-out (L00) value, and Data Shapley value as baselines. A more detailed description of these baselines is included in Appendix G.6. To conduct node-dropping experiments, nodes are ranked by their assessed values for each method and removed sequentially from the training graph \(_{tr}\). After each removal, we train a GNN model based on the remaining graph and evaluate its performance on the testing graph \(_{te}\). Performance changes are depicted through a curve that tracks the model's accuracy as nodes are progressively eliminated. Labeled nodes often contribute more significantly to model performance than unlabeled nodes because they directly offer supervision. Thereby, with accurately assigned node values, labeled nodes should be prioritized for removal over unlabeled nodes. We empirically validate this hypothesis in Figure 6, discussed in Appendix E. Specifically, in nearly all datasets, our observations reveal that the majority of labeled nodes are removed prior to the unlabeled nodes by both PC-Winter and Data Shapley. This leads to a plateau in the latter portion of the performance curves since a GNN model cannot be effectively trained with only unlabeled nodes. Consequently, this scenario significantly hampers the ability to assess the value of unlabeled nodes. Therefore, we propose to conduct separate assessments for the values of labeled and unlabeled nodes. Here, we only include the results for unlabeled nodes, while the results for labeled nodes are presented in Appendix F.

**Results and Analysis.** Figure 3 illustrates the performance comparison between PC-Winter and other baselines across various datasets. From Figure 3, we make the following observations. First, the removal of high-value unlabeled nodes identified by PC-Winter consistently results in the most significant decline in model performance across various datasets. This is particularly evident after removing a relatively small fraction (10%-20%) of the highest-value nodes. This trend underscores the importance of high-value nodes. Notably, in most datasets PC-Winter outperforms the best baseline method, Data Shapley, by a considerable margin, highlighting its effectiveness. Second, the decrease in performance caused by our method is not only substantial but also persistent throughout the node-dropping process, further validating the effectiveness of PC-Winter. Third, the performance curves of PC-Winter and Data Shapley eventually rebound towards the end. This rebound corresponds to the removal of unlabeled nodes that make negative contributions. Their removal aids in improving performance, ultimately reaching the MLP performance when all nodes are excluded. This upswing not only evidences the discernment of PC-Winter and Data Shapley in ascertaining node values but also showcases the particularly acute precision of PC-Winter. These insights collectively affirm the capability of PC-Winter in accurately assessing node values.

### Adding High-Value Edges

In this section, we explore the impact of adding high-value elements to a graph, providing an alternative perspective to validate the effectiveness of data valuation. Notably, adding high-value nodes to a graph typically involves the concurrent addition of edges, which complicates the addition process. Thus, we target the addition of high-value edges, providing a complementary perspective to our analysis. As described in Section 3.5, the flexibility of PC-Winter allows for obtaining edge values without a separate "reevaluation" process for edges.

Here, we keep all nodes in \(_{tr}\) and sequentially add edges according to the edge values in descending order, starting with the highest-valued ones. Similar to the node-dropping experiments, the effectiveness of the edge addition is shown through performance curves. We include Random value,

Figure 3: Dropping High-Value Nodes

Edge-Betweeness, Leave-one-out (L00) as baselines. Notably, here, Random and L00 specifically pertain to edges, and while we use the same terminology as in the prior section, they are distinct methods, which are detailed in Appendix G.6.

**Results and Analysis.** Figure 4 illustrates that the Random, L00, and Edge-Betweeness baselines achieve only linear performance improvements with the addition of more edges, failing to discern the most impactful ones for a sparse yet informative graph. In contrast, the inclusion of edges based on the PC-Winter value results in a steep performance climb, affirming the PC-Winter's efficacy in pinpointing key edges. Notably, the Cora dataset reaches full-graph performance using merely 8% of the edges selected by PC-Winter. Moreover, with just 10% of PC-Winter-selected edges, the accuracy climbs to 72.9%, outperforming the full graph's 71.3%, underscoring PC-Winter's capability to identify valuable edges. This trend is generally consistent across other datasets as well.

### Ablation Study, Parameter and Efficiency Analysis

In this section, we conduct an ablation study, parameter analysis, and efficiency analysis to gain deeper insights into PC-Winter using node-dropping experiments.

**Ablation Study.** We conduct an ablation study to understand how the two constraints in Section 3.2 affect the effectiveness of PC-Winter. We introduce two variants of PC-Winter by lifting one of the constraints for the permutations. In particular, we define PC-Winter-L using the permutations satisfying the Level Constraint. Similarly, PC-Winter-P is defined with permutations only satisfying Precedence Constraint. As shown in Figure 5, PC-Winter value outperforms the PC-Winter-L and PC-Winter-P on both datasets, which demonstrates that both constraints are crucial for PC-Winter. Additional results on other datasets are provided in Appendix H.1.

**Parameter Analysis.** We conduct parameter analyses to investigate the impact of permutation number and truncation ratios on PC-Winter's performance. The results reveal that PC-Winter achieves robust performance even with a significantly reduced number of permutations and high truncation ratios. Detailed findings are presented in Appendix H.2 and Appendix H.3, respectively.

**Efficiency Analysis.** We compare the efficiency of PC-Winter and Data Shapley. Analysis of converged permutation count and time per permutation across \(6\) datasets underscores PC-Winter's significantly higher efficiency. A comprehensive breakdown is available in Appendix H.4.

## 5 Conclusion

In this paper, we introduce PC-Winter, an innovative approach for effective graph data valuation. The method is specifically designed for graph-structured data and addresses the challenges posed by unlabeled elements and complex node dependencies within graphs. Furthermore, we introduce a set of strategies for reducing the computational cost, enabling efficient approximation of PC-Winter. Extensive experiments demonstrate the practicality and effectiveness of PC-Winter in various datasets and tasks. While PC-Winter demonstrates improved efficiency compared to Data Shapley, we acknowledge that further efficiency enhancements are crucial to fully unlock the potential of graph data valuation in real-world applications. Our work can be seen as a foundation for future research in this direction.

Figure 4: Adding the High-Value Edges

Figure 5: Ablation Study