# Learning Cooperative Trajectory Representations

for Motion Forecasting

 Hongzhi Ruan \({}^{1,2}\)  Haibao Yu \({}^{1,3}\)1

Workinging author

 Wenxian Yang \({}^{1}\)  Siqi Fan \({}^{1}\)  Zaiqing Nie \({}^{1}\)

\({}^{1}\) Institute for AI Industry Research (AIR), Tsinghua University

\({}^{2}\) University of Chinese Academy of Science \({}^{3}\) The University of Hong Kong

hongzhi.rynn@gmail.com, yuhaibao94@gmail.com, zaiqing@air.tsinghua.edu.cn

Corresponding authors.

###### Abstract

Motion forecasting is an essential task for autonomous driving, and utilizing information from infrastructure and other vehicles can enhance forecasting capabilities. Existing research mainly focuses on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while under-utilizing the motion and interaction context of traffic participants observed from cooperative devices. In this paper, we propose a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information. Specifically, we present V2X-Graph, a representative framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. V2X-Graph is evaluated on V2X-Seq in vehicle-to-infrastructure (V2I) scenarios. To further evaluate on vehicle-to-everything (V2X) scenario, we construct the first real-world V2X motion forecasting dataset V2X-Traj, which contains multiple autonomous vehicles and infrastructure in every scenario. Experimental results on both V2X-Seq and V2X-Traj show the advantage of our method. We hope both V2X-Graph and V2X-Traj will benefit the further development of cooperative motion forecasting. Find the project at https://github.com/AIR-THU/V2X-Graph.

## 1 Introduction

In recent years, autonomous driving has made significant progress. However, single-vehicle autonomous driving still faces substantial safety challenges due to its limited perception ability. Utilizing external information, such as data from other connected autonomous vehicles and infrastructure sensors through vehicle-to-everything (V2X), has shown great potential to enhance autonomous driving capabilities. In this paper, we focus on motion forecasting, a fundamental task for autonomous driving that has received significant attention in recent years . Specifically, considering currently practical communication conditions, we transmit perception results and input trajectories from the ego vehicle and external sources for cooperative motion forecasting.

Cooperative motion forecasting involves the ego vehicle aggregating its own data with data transmitted from other connected vehicles or infrastructure devices to predict future waypoints for each agent in traffic scenarios. To accommodate limited communication conditions, we consider data in the form of perception results. These perception results form historical trajectories of agents from respective views, termed cooperative trajectories. The autonomous vehicle utilizes these trajectories to enhance its motion forecasting capabilities. High-definition (HD) maps are also used in this task.

To leverage cooperative trajectories for improving motion forecasting performance, two critical issues must be addressed: (1) Observations of the agents from different views may different due to various sensor perspectives and configurations; (2) In the cooperative scenario, there are multiview observations of multi-agents, the redundant data need to be leveraged interpretably. Existing research mainly focuses on single-frame feature fusion to support real-world applications and enhance detection performance [50; 44; 46; 4; 42]. A recent method  attempts to complement perception over the historical horizon to improve forecasting performance. These methods are depicted in fig. 1(a). However, the above single-frame methods obtain the agent state at each frame individually, which may lead to a trade-off between the states observed from distinct views, and cannot utilize motion and interaction context sufficiently, thus failing to sufficiently model the historical behavior of agents. Instead, considering historical observations from each view holistically could address these shortcomings. Compared to previous approaches, this paper explores a novel forecasting-oriented trajectory feature fusion method, which aims to enhance the historical representation of agents, including their historical motion and surrounding interactions, for motion forecasting.

To address the challenges and effectively utilize the cooperative information, we propose V2X-Graph, a graph-based framework to achieve cooperative trajectory feature fusion for motion forecasting. Theoretically, V2X-Graph offers two advantages for enhancing motion forecasting performance. (1) Forecasting-oriented cooperative representation. For accurate motion forecasting, it is common practice to represent motion features from an agent's historical trajectory and interaction features from other agents [10; 25; 58; 19]. Instead of perception complement, our method is the first to consider cooperative perception information from the typical motion forecasting perspective, which independently represents motion and interaction representations of cooperative trajectories, customized relative spatial-temporal encodings are designed to support trajectories feature fusion of each agent over historical horizon. (2) Graph-guided heterogeneous feature fusion. To support multi-agent motion forecasting in a cooperative scenario, it is essential to interpretably integrate heterogeneous motion and interaction features for each specific agent from cooperative trajectories. Drawing inspiration from graph link prediction [12; 55], a classical task that analyzes the relationship of nodes in a graph, this may further support downstream applications like node feature propagation. To achieve end-to-end optimization in V2X-Graph, we constructed a graph that represents the cooperative scenario, an interpretable association is established to guide heterogeneous feature fusion based on agent identification across views. The framework is represented in fig. 1(b).

V2X-Graph is evaluated on V2X-Seq , which contains vehicle-to-infrastructure (V2I) cooperative scenarios. To evaluate its effectiveness in vehicle-to-vehicle (V2V) and further more cooperative devices scenarios, we construct the first public and real-world vehicle-to-everything (V2X) motion forecasting dataset V2X-Traj. This dataset is the first to include multiple autonomous vehicles and infrastructure in every scenario, broadening the research devoted to V2X motion forecasting task. Extensive experiments conducted on V2X-Seq and V2X-Traj show the advantages of V2X-Graph in utilizing additional cooperative information to enhance the motion forecasting capability.

Our contributions are four fold: (1) We propose a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information. (2) We design V2X-Graph, a representative framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. (3) We construct V2X-Traj, which is the first public and real-world dataset for V2X motion forecasting. It includes not only V2I but also V2V cooperation in every cooperative scenario. (4) Our approach achieves state-of-the-art on both V2X-Seq and V2X-Traj.

Figure 1: Scheme Comparison. (a) Existing methods utilize cooperative perception information at each frame individually then performs forecasting. (b) Our V2X-Graph considers this information from a typical forecasting perspective and employs interpretable trajectory feature fusion in an end-to-end manner, to enhance the historical representation of agents for cooperative motion forecasting.

Related Work

**Cooperative Autonomous Driving.** In recent years, more and more researchers pay attention to cooperative autonomous driving, which leverages additional information from infrastructure-side devices and other vehicles to achieve system-wide performance improvement. As several public cooperative perception datasets [45; 49; 15] have been released, most of them focus on cooperative perception. Different from the single-side object detection [56; 24; 8; 47; 48], cooperative detection methods aim to promote performance and transmission latency trade-offs to support real-world applications [46; 22; 44; 41; 50; 9; 4]. Some works also dive into cooperative segmentation task [38; 43]. However, as the downstream task of cooperative perception and directly influences the actions of autonomous vehicles, cooperative motion forecasting has not been well studied. A recent endeavor supplies historical observations with perception information from infrastructure devices to improve motion forecasting performance . Instead of perception completion then forecasting, this paper presents an end-to-end cooperative motion forecasting framework for cooperative trajectory feature fusion, to achieve comprehensive utilization of motion and interaction contexts from cooperative information. To further broaden the research into V2X cooperative motion forecasting, we construct the first real-world and public motion forecasting dataset for general V2X scenario, termed V2X-Traj, including not only V2I but also V2V cooperations in every scenario.

**Motion Forecasting.** Motion forecasting is an indispensable task in autonomous driving systems, which takes sequential perception results of agents and map elements into account to predict future trajectories of agents. Early works rasterize scenarios as images and deploy convolution neural networks to extract information [34; 21]. The research community turns to vectorize the representations of agents and maps for motion and interaction contexts . Some works consider pooling mechanism for feature fusion [1; 14; 10; 35]. Others utilize the convolution technique to extract local features [25; 52; 5]. Inspired by the effectiveness and widespread usage of the Transformer model , recent works adopted attention mechanism for learning representations in motion forecasting task [26; 39; 31; 58; 33; 19; 40]. Instead of sequential perception supplementation then forecasting, V2X-Graph explores novel trajectory feature fusion to comprehensively utilize information. It represents and decouples trajectory features into motion and interaction features for each view independently, and employs customized Transformer modules for aggregating interpretable features based on agent identification, which facilitates cooperative motion forecasting.

**GNNs for Motion Forecasting.** Graph neural network (GNN) [20; 37] is a common structure for motion forecasting. A graph consists of nodes and edges, with each node typically representing information related to an agent or a map element. While edges represent the relative information between pairs of nodes. The message-passing mechanism aggregates and updates node features from their neighbors. Previous methods adopted homogeneous GNN for unified but coarse scene representation [30; 10; 23; 57; 25; 13; 11]. While recent research introduced heterogeneous GNN [54; 16] to distinguish and further extracting features based on various settings of agents [32; 29; 18; 19]. Compared to previous approaches, V2X-Graph explores leveraging heterogeneous edge encodings and interpretable graph link prediction for trajectory-based motion and interaction features fusion.

## 3 Preliminary

Cooperative motion forecasting can play an important role for autonomous driving as it better reasons the future movements of surrounding agents. The ego vehicle receives sequential perception results from cooperative devices, including infrastructures and other vehicles, to enhance the capability of motion forecasting. The contextual information in the vector map is also taken into account.

**Problem Formulation.** The inputs of cooperative motion forecasting include multiple-source trajectories and vector maps. The cooperative motion forecasting scenario is represented as \(=\{,\}\), where \(\) and \(\) are described as follows. (1) Trajectory. In a typical V2X cooperation scenario, each cooperative device independently captures the historical status of agents as trajectories. The multi-source trajectories are denoted as \(=\{_{ego},_{other}\}\), here \(_{other}\) can include received multi-source cooperative trajectories such as \(_{inf}\) and \(_{vch}\) from the views of infrastructure and cooperative vehicles. The total number of trajectories is \(N_{t}=N_{ego}+N_{other}\). Trajectory information is summarized as \(^{N_{t} T C_{t}}\), where \(T\) is the historical horizon and \(C_{t}\) is the attributes of each trajectory to depict corresponding agent (\(e.g.\), tracking id, location, heading angle, detection bounding box and agent type). Specifically, the historical spatial status of each trajectory is formulated as \(\{_{i}^{t},_{i}^{t}\}_{t=1}^{T}\), where \(_{i}^{t}^{2}\) is the trajectory \(i\)'s location, and \(_{i}^{t}\) represents the heading theta vector at time step \(t\). (2) Vector Map. Vectorized representation  is usually adopted for representing the map elements in motion forecasting task, which leverages the sample points of the centerline within each lane and enables an efficient vectorized representation of spatial information. In this paper, we further consider vectorized lane segment, \(i.e.\), the vector between each two neighboring sample points. The set of vectorized lane segments is denoted as: \(^{N_{l} 2 C_{l}}\), where \(N_{l}\) is the number of lane segments and \(C_{l}\) is the attributes of each lane segment (\(e.g.\), location and road type). The start point and the end point of the lane segment are formulated as \(\{_{l}^{start},_{l}^{end}\}\), here \(l[1,2,...,N_{l}]\).

**Evaluation.** The output is \(\) future trajectories of the specified target agent in each scenario, the best one is chosen for evaluation, here \(=6\). Evaluation metrics are minADE, minFDE and MR, standard metrics for motion forecasting. _Lower number is better_.

**Challenges.** To enhance the capability of motion forecasting considering abundant cooperative trajectories, it is essential to: (1) effectively represent the cooperative scenario, (2) efficiently utilize valuable information from redundant cooperative trajectories.

## 4 Methodology

This section presents V2X-Graph, a graph-based framework designed to achieve interpretable trajectory feature fusion for cooperative motion forecasting. To represent the cooperative scenario, it constructs a graph with node and edge encodings. To enhance cooperative trajectory feature fusion, an interpretable graph consisting of three subgraphs is designed for the aggregation of heterogeneous motion and interaction features. The overall architecture is depicted in fig. 2.

### Scene Representation with Graph

In the graph that represents the cooperative scenario, trajectories from each view and their corresponding lane segments are independently encoded as nodes, while the relative spatial and temporal features between these nodes are encoded as edges.

**Graph Node Encodings.** We encode the node features from three perspectives: trajectory motion features, trajectory spatial-temporal features, and lane segment spatial features.

Compared to previous methods that leverage cooperative information at each frame individually, we encode differential information from each view then fuse it with correlation to mitigate the deviation caused by direct single-frame fusion. The embeddings of differential coordinates \(\{_{i}^{t}-_{i}^{t-1}\}_{t=1}^{T}\) of the trajectory are considered as the motion features at each timestep. The self-attention mechanism  is adopted to incorporate temporal dependency. Here, missing frames are padded with learnable tokens, and the attention mechanism is enforced to only attend to the preceding time steps.

For the purposes of trajectory identification for interpretable association and motion correlation measurement, we also encode spatial-temporal features for trajectories from each view. The spatial-temporal features are encoded by incorporating the temporal dependency between the ego-centric normalized coordinates of the trajectory. Missing frames are masked in the attention module.

Overall, node encodings of trajectory from each view are formulated as:

\[_{i}^{mot}=((_{i}^{T}(_{i}^{t}-_{i}^{t-1}))+^{t}),_{i}^{st}= {SelfAttn}((_{i}^{t})+^{t}),\] (1)

where \(^{t}\) signifies the learnable positional embedding at timestep \(t\), \(()\) is the multi-head self-attention module, and \(()\) represents a multi-layer perceptron.

To enhance the representation of trajectories for future intention reasoning, the feature of the vector map structure is incorporated. Specifically, the spatial feature of lane segments are represented by the relative coordinates (from the start point to the end point of each lane segment) in an agent-centric frame , and further encoded as nodes for feature aggregation. The formulation is:

\[_{l}^{map}=(_{i,l}^{T}(_{l}^{end}- _{l}^{start})),\] (2)

where \(_{i,l}^{T}\) is the relative heading vector between trajectory \(i\) with lane segment \(l\) in its current frame.

**Graph Edge Encodings.** For effective trajectory feature fusion, we design heterogeneous edge encodings, including spatial-temporal encoding and relative spatial encoding.

We use an attention module to aggregate the spatial-temporal encodings between each pair of cross-view trajectories. The spatial-temporal encoding captures the spatial-temporal correlations between two trajectories at each timestep, which facilitates motion feature fusion.

To capture the interaction features, we also introduce relative spatial encodings as edges.

Edge encodings can be formulated as follows:

\[_{i j}^{st}=([_{i}^{st}, _{j}^{st}]),_{i j}^{rs}=(_{i}^{ T}(_{i}-_{j})),\] (3)

where \(_{i j}^{rs}\) represents both agent-agent and agent-lane interaction relations. For agent-agent interaction, the coordinates of the current frame for trajectories are denoted by \(_{i}\) and \(_{j}\). While for agent-lane interaction, the encoding represents the feature between current coordinate \(_{i}\) of trajectory \(i\) and the starting point coordinates \(_{l}^{start}\) of the lane segment.

### Feature Fusion with Interpretable Graph

To achieve comprehensive historical representations of agents in a cooperative scenario, an interpretable graph is designed for multi-view trajectory feature fusion. Serving as a guidance for heterogeneous feature representations, the Interpretable Association component (IA) establishes explicit associations between trajectories of the same agent across views. The Motion Fusion subGraph (MFG) represents cooperative motion features by considering both explicit associations and implicit spatial-temporal encodings. The Agent-Lane subGraph (ALG) fuses the features from each view with lane segment features. The Cooperative Interaction subGraph (CIG) represents dense interaction representations by leveraging the spatial encodings between different agents from all views.

**Input:** Ego-view trajectories \(_{ego}\), Other-view trajectories \(_{other}\)

**Output:** Cross-views trajectories matching pesudo labels \(}\)

**for \(t=0\) to \(T\)do**

\(_{ego},_{other}\) Collect ego-view, other-view detection bounding boxes from \(_{ego}^{t},_{other}^{t}\) ;

Calculate bounding boxes IOU matrix \(_{t}^{|_{ego}||_{other}|}\) ;

\(^{t}\) Solving the optimal matching: Hungarian Algorithm(\(^{t}\));

Update greedy trajectory matching at \(t\): \(^{t}\);

\(}\) Solve error matching with length intersection threshold \(_{length}\) from \(\).

**Interpretable Association.** To achieve end-to-end optimization of heterogeneous feature fusion, we formulate the association process as a graph link prediction problem. An interpretable association component is introduced to establish interpretable associations of cross-view trajectories of agents, providing explicit guidance for the fusion of motion and interaction features. Additionally, we propose

Figure 2: V2X-Graph overview. Trajectories from the ego-view and other views, along with vector map information, are encoded as nodes and edges for graph construction to represent a cooperative scenario. The novel interpretable graph provides guidance for forecasting-oriented trajectory feature fusion, including motion and interaction features. In this figure, solid rectangles represent encodings of ego-view trajectories, hollow circles represent encodings of cooperative trajectories, distinguished by distinct colors. Specifically, within the same view, the use of the same color indicates interruptions caused by occlusion. Triangles represent encodings of lane segments. In trajectory feature fusion, grey arrow indicates an missing frame in motion case, a lane segment vector in interaction case.

a pseudo label generator, described in alg. 1, to collect trajectory matching labels for trajectories from the ego and another view over the historical horizon within the training set for knowledge distillation.

We denote an adjacency matrix for the two sets of trajectories \(=\{a_{i,j}|i N_{ego},j N_{other}\}\), where elements referring to associations are supervised by \(}\) which can be predicted as:

\[=_{}(^{st}_{i j}) \{0,1\}^{|N_{ego}||N_{other}|},\] (4)

here \(_{}\) refers to a MLP for binary classification to determine whether there exists an association between two trajectories, thereby instructing the perception information belonging to the same agent. The classification is based on relative spatial-temporal encodings of trajectories across views.

**Motion Fusion SubGraph.** To comprehensively represent the historical motion of agents, we employ the Motion Fusion subGraph (MFG) to aggregate cooperative motion feature representations from cross-view associated trajectories. MFG models cooperative motion representations by incorporating both interpretable associations and temporal-spatial correlations.

MFG is defined as: \(_{mfg}=(,)\) and \(_{mfg}=\), where the node set \(=\{v_{i}|i N_{t}\}\), and the edge set \(=\{e_{i,j}\}\) denotes the edges captured by \(_{mfg}\). Feature fusion and update process is:

\[^{(k+1)}_{i}=(^{(k)}_{i}+( ^{(k)}_{i},^{(k)}_{j}+^{st}_{i j})),\] (5)

where \(^{(k+1)}_{i}\) represents the updated feature of node \(^{(k)}_{i}\), \(()\) is a feed-forward network.

**Agent-Lane SubGraph.** We employ the Agent-Lane subGraph (ALG) to incorporate map information for cooperative motion forecasting. ALG is a bipartite graph that enables agent motion features to query relevant lane segment interaction features using relative spatial encodings. We consider all lane segments within the observation range of the current frame of agents from each view.

ALG is defined as: \(_{alg}=(,)\), where \(=\{v_{i},v_{l}|i N_{t},l N_{l}\}\) and \(=\{e_{i,l}\}\). The process of interaction feature aggregation and update from agents to lane segments can be formulated as:

\[^{(k+1)}_{i}=(^{(k)}_{i}+( ^{(k)}_{i},^{(k)}_{l}+^{rs}_{i l}+ _{l})),\] (6)

where \(_{l}\) represents learnable tokens of semantic attributes associated with the corresponding lane segment, such as turn direction and road type.

**Cooperative Interaction SubGraph.** The Cooperative Interaction subGraph (CIG) is employed for cooperative interaction features representation between trajectories of distinguished agents in all views. Incorporating both interpretable associations and relative spatial correlations, CIG models denser interaction representations in a cooperative scenario.

CIG is defined as: \(_{cig}=(,)\) and \(_{cig}=\), where \(=\{v_{i}|i N_{t}\}\), \(=\{e_{i,j}\}\) denotes the set of not associated cross-view trajectories, which is captured by the adjacency matrix \(_{cig}\), and all intra-view edges. The process of interaction feature fusion and update in CIG formulated as:

\[^{(k+1)}_{i}=(^{(k)}_{i}+( ^{(k)}_{i},^{(k)}_{j}+^{rs}_{i j}+ _{i,j})),\] (7)

where \(_{i,j}\) represents the learnable features of interaction attributes associated between two trajectories, such as relative headings at the current timestep. Note that CIG only considers the interaction among trajectories observed in the current frame.

### Multimodal Future Decoder

The future motion of traffic agents is inherently multi-modal. We parameterize the distribution of future trajectories as Laplacian Mixture Model (LMM) following . Every agent from each view is predicted and supervised during training phase for representations of trajectory feature fusion. While during inference phase, the decoder predicts the future trajectory of distinct agents in the cooperative scenario from the ego-view, guided by interpretable associations. Technically, we employ a MLP as a prediction head to aggregate all the intermediate features, which can be formulated as:

\[^{1:T}_{i}=([^{st}_{i},^ {mot}_{i},^{mfg}_{i},^{alg}_{i},^{cig}_{i}]),\] (8)

where \(^{1:T}_{i}\) includes \(\) Laplacian components \(_{1:}\) with multi-modal probability distributions \(p_{1:}\). The formulation for predicting the future coordinate distribution of agent \(i\) at time \(t\) is as:

\[P_{t}(o)=_{k=1}^{}p_{k}_{1:}(_{x},_{x},_{y},_{y},),\] (9)

the future positions are generated by the center of distributions. The distribution \((_{x},_{y})\) and corresponding probability \(p_{k}\) are generated by two MLPs separately.

### Training Losses

To achieve interpretable trajectory feature fusion, the framework is trained in an end-to-end manner with two components of optimization objectives. The first part is the cross-entropy loss to optimize the graph link prediction. The second part includes the regression loss and classification loss to optimize the motion forecasting. Please refer to appendix for more loss details.

## 5 Experiment

In this section, we evaluate the proposed V2X-Graph framework not only on V2I cooperative scenarios but also on V2V and broader V2X cooperative scenarios.

### Experimental Setup

**Dataset.** V2X-Graph is evaluated on both V2I and broader V2X scenarios. (1) **V2X-Seq**. A public large-scale and real-world V2I dataset. V2X-Seq consists of 51,146 V2I scenarios, and each scenario is 10 seconds long with the sample rate of 10 Hz. The task is to predict the motion of agents for the next 5 seconds, given the initial 5-second observation from both infrastructure and ego-view. (2) **V2X-Traj (Ours)**. To study the effectiveness of V2X-Graph in V2V and broader V2X scenarios, especially its ability to handle more than two views of trajectories, including both V2I and V2V cooperation, we construct the first real-world and public V2X cooperative motion forecasting dataset, termed V2X-Traj. It comprises 10,102 scenarios in challenging intersections. Each scenario includes two intelligent vehicles and an infrastructure perception device. The statistics and visualization of V2X-Traj are presented in fig. 3. Each scenario lasts for 8 seconds with a sample rate of 10 Hz. The 4-second observations from each view are used to predict the future motion in the next 4 seconds. We hope the V2X-Traj dataset can facilitate the development of cooperative motion forecasting for general V2X scenarios. More details are described in the Appendix.

**Implementation Details.** For scene graph representation, V2X-Graph employs a 4-layer temporal self-attention Transformer to encode motion features, a 2-layer temporal self-attention Transformer and a 2-layer self-attention module for relative temporal-spatial feature encoding. In the interpretable graph, there are 3 layers of MFG, 1 layer of ALG and 3 layers of CIG. The dimensions of the hidden feature is set as 128, and the number of heads in all multi-head attention blocks is 16. The lane segments corresponding to agents within a observation range of 50 meters are taken into consideration. For training, the initial learning rate is set to \(1 10^{-3}\) and is scheduled according to cosine annealing . The AdamW optimizer  is adopted with a weight decay of \(1 10^{-4}\). The model is trained for 64 epochs with batch size of 64 on a server with 8 NVIDIA RTX 4090s.

### Main Results

**Cooperative method comparison.** We compare our method with other cooperative methods on V2X-Seq. To the best of our knowledge, PP-VIC  is the only existing method of the same type. Typically, PP-VIC provides the ego vehicle with infra-side perception information in a frame-by-frame manner within the historical horizon. After supplementation, the perception information is provided to popular and competitive vanilla forecasting methods DenseTNT  and HiVT . For comparison, we also provide the output of PP-VIC to V2X-Graph in a similar way to the ego-view perception. As shown in table 1, perception completion enhances the downstream forecasting performance of HiVT and V2X-Graph. However, the trade-off in cross-view perception also leads to error propagation, resulting in performance degradation of DenseTNT. Instead, V2X-Graph enhances the historical representation of agents through trajectory feature fusion, leading to performance improvements, as evidenced by \(-0.07\) in minADE, \(-0.19\) in minFDE, and \(-5\%\) in MR.

Figure 3: V2X-Traj dataset. (a) Statistics of the total number and average length for the 8 classes of agents. (b) Visualizations. Orange boxes represent autonomous vehicles, blue elements denote other traffic participants and the green box denotes the target agent needs to be predicted.

**Graph-based methods comparison.** To reflect the unique advantages of V2X-Graph, we conduct evaluations on V2X-Traj and compared it with representative graph-based methods. Similar to V2X-Graph, cooperative trajectories are encoded as vanilla nodes of agents in each compared methods, for fair comparison. Experimental results in four typical settings are reported in table 2. As shown in the table, HDGT  achieves superior performance through precise heterogeneous design to represent relationship of agents, compared with DenseTNT , which employs a homogeneous graph to represent the scenario. V2X-Graph is also highly competitive in vehicle-only task, without sophisticated feature engineering and decoder design. However, our method outperforms compared methods by large margins in all cooperative settings and achieves the best performance in V2V&I cooperation, with \(-0.22\) in minADE, \(-0.43\) in minFDE, and \(-6\%\) in MR, illustrating the effectiveness of aggregation heterogeneous motion and interaction features to enhance cooperative forecasting.

### Ablation Study

To further illustrate the effectiveness of the method for trajectory feature fusion and the final result, we conduct ablation studies on the V2X-Traj validation set. Considering extensive ablation studies, experiments are conducted based on our small model with a hidden-size of 64.

**Effectiveness of Major Components.** Firstly, we alternately removing one of the components to illustrate the contribution of each component to the cooperative motion forecasting performance. As shown in table 3, the components within the interpretable graph separately represent the typical motion feature of historical states of agents and the interaction features with surroundings, demonstrating a marked performance enhancement of motion forecasting.

**Effectiveness of Cooperative Representations.** We further evaluate the effectiveness of the model in trajectory feature fusion by alternately masking the features of cooperative trajectories within each component. As demonstrated in table 4, each customized component benefits trajectory feature fusion and results in performance improvements to a certain degree. Specifically, the MFG effectively integrates the motion features of associated trajectories, leading to \(-0.08\) in minADE. Relatively, the ALG and CIG components fuse the interaction features of lane segments and cooperative trajectories. These component primarily enhance the performance of long-term intention prediction, as indicated \(-0.59\) in minFDE and \(-7\%\) in MR.

**Effectiveness of Interpretable Feature Fusion.** Moreover, we evaluate the effectiveness of the proposed interpretable graph in aggregating heterogeneous motion and interaction features within cooperative trajectories. In table 5, the first line presents the result of the vehicle-only setting, which has no cross-view motion and interaction features fusion. We simply employ fully connections in our graph to aggregate motion and interaction features, compared with the ego-setting, there is no obvious positive effect with a large amount of features fusion. The last line shows the result of interpretable

    &  &  &  \\   & Vehicle-only & PP-VIC & Vehicle-only & PP-VIC & Vehicle-only & PP-VIC & Feature Fusion \\  minADE & 1.71 & 1.84 & 1.28 & 1.12 & 1.16 & 1.12 & **1.05** \\ minFDE & 2.43 & 2.56 & 2.15 & 1.97 & 2.04 & 1.98 & **1.79** \\ MR & 0.27 & 0.28 & 0.31 & 0.30 & 0.30 & 0.30 & **0.25** \\   

Table 1: Cooperative method comparison on V2X-Seq.

    &  &  &  &  \\   & minADE & minFDE & MR & minADE & minFDE & MR & minADE & minFDE & MR & minADE & minFDE & MR \\  DenseTNT & 1.23 & 2.09 & 0.25 & 1.20 & 2.04 & 0.25 & 1.32 & 2.34 & 0.29 & 1.26 & 2.24 & 0.28 \\ HDGT & 0.91 & 1.48 & 0.14 & 0.94 & 1.57 & 0.17 & 0.94 & 1.59 & 0.16 & 0.94 & 1.56 & 0.17 \\ V2X-Graph & 0.90 & 1.56 & 0.17 & 0.77 & 1.26 & 0.12 & 0.80 & 1.30 & 0.13 & **0.72** & **1.13** & **0.11** \\   

Table 2: Graph-based methods comparison on V2X-Traj.

   MFG & ALG & CIG & minADE & minFDE & MR \\   & ✓ & ✓ & 1.03 & 2.04 & 0.25 \\ ✓ & ✓ & ✓ & 1.19 & 2.37 & 0.28 \\ ✓ & ✓ & ✓ & 1.08 & 2.17 & 0.27 \\ ✓ & ✓ & ✓ & **0.95** & **1.79** & **0.21** \\   

Table 4: Effect of cooperative representations.

    &  &  &  &  \\   & minADE & minFDE & MR & minADE & minFDE & MR & minADE & minFDE & MR & minADE & minFDE & MR \\  DenseTNT & 1.23 & 2.09 & 0.25 & 1.20 & 2.04 & 0.25 & 1.32 & 2.34 & 0.29 & 1.26 & 2.24 & 0.28 \\ HDGT & 0.91 & 1.48 & 0.14 & 0.94 & 1.57 & 0.17 & 0.94 & 1.59 & 0.16 & 0.94 & 1.56 & 0.17 \\ V2X-Graph & 0.90 & 1.56 & 0.17 & 0.77 & 1.26 & 0.12 & 0.80 & 1.30 & 0.13 & **0.72** & **1.13** & **0.11** \\   

Table 3: Effect of major components.

features fusion, it achieves improved performance with a low computational cost, demonstrating the effectiveness to interpretably aggregate heterogeneous cooperative features.

**Effectiveness of Pseudo Label Supervision.** To demonstrate the quality of pseudo labels and influence on the final motion forecasting performance, we disturb labels randomly for supervision and evaluate the motion forecasting results. As shown in fig. 4, as the proportion of disturbed pseudo labels increases, corresponding forecasting performance decreases, which illustrates the effectiveness of both interpretable feature fusion and pseudo label supervision. This also suggests that there is potential for further improvement in forecasting performance as the quality of labels increases.

## 6 Conclusion and Limitation

In this paper, we introduce a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information, and present V2X-Graph, a graph-based framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. Comparing to existing methods that rely on single-frame perception information cooperation, our approach enhances the historical representation of agents from lightweight cooperative trajectories, and achieve improved performance in the downstream task, namely cooperative motion forecasting. Moreover, we construct V2X-Traj, the first real-world and public V2X cooperative motion forecasting dataset, expanding the research from V2I to broader V2X motion forecasting task. Experiments on both two datasets demonstrate the effectiveness of our method.

**Limitation and future work.** Compared to the single-frame method, the proposed V2X-Graph explores trajectory feature fusion, which mitigates errors from single-frame perception completion and achieves better motion and interaction representation of agents. Despite these advantages, the performance still relies on the tracking quality from each view. Jointly optimizing the performance from perception to forecasting is significant to explore in the future.

Figure 4: Effectiveness of pseudo label supervision.

Figure 5: Qualitative results on V2X-Traj. There are several interesting cooperative scenarios at the challenging intersection, including speed-up, lane changing and turning. We visualize only the forecasting results of the target agent in each scenario for clarity. The ground-truth trajectories are shown in red, and the multimodal predicted trajectories are shown in green.

   Motion Fusion & Fusion Count in MFG & Interaction Fusion & Fusion Count in CIG & minADE & minFDE & MR \\  No Fusion & 0 & Ego & 986 & 1.06 & 2.09 & 0.27 \\ Full Fusion & 14,191 & Interpretable Fusion & 6,788 & 1.10 & 2.18 & 0.25 \\ Interpretable Fusion & 172 & Full Fusion & 6,982 & 1.08 & 2.32 & 0.27 \\ Full Fusion & 14,191 & Full Fusion & 6,982 & 1.04 & 2.09 & 0.24 \\ Interpretable Fusion & 175 & Interpretable Fusion & 6,836 & **0.95** & **1.79** & **0.21** \\   

Table 5: Effectiveness of feature fusion with interpretable graph. "Fusion Count" represents statistics average fusion counts of features per scenario. "Interpretable Fusion" indicates the aggregation of motion and interaction features through associations.