# Inductive biases of multi-task learning and finetuning: multiple regimes of feature reuse

Samuel Lippl

Center for Theoretical Neuroscience

Columbia University

New York, NY

samuel.lippl@columbia.edu

&Jack Lindsey

Anthropic

San Francisco, CA

jackwlindsey@gmail.com

Equal contributions.Work primarily conducted while at the Center for Theoretical Neuroscience, Columbia University.

###### Abstract

Neural networks are often trained on multiple tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In particular, it is common practice to pretrain neural networks on a large auxiliary task before finetuning on a downstream task with fewer samples. Despite the prevalence of this approach, the inductive biases that arise from learning multiple tasks are poorly characterized. In this work, we address this gap. We describe novel implicit regularization penalties associated with MTL and PT+FT in diagonal linear networks and single-hidden-layer ReLU networks. These penalties indicate that MTL and PT+FT induce the network to reuse features in different ways. 1) Both MTL and PT+FT exhibit biases towards feature reuse between tasks, and towards sparsity in the set of learned features. We show a "conservation law" that implies a direct tradeoff between these two biases. 2) PT+FT exhibits a novel "nested feature selection" regime, not described by either the "lazy" or "rich" regimes identified in prior work, which biases it to _rely on a sparse subset_ of the features learned during pretraining. This regime is much narrower for MTL. 3) PT+FT (but not MTL) in ReLU networks benefits from features that are correlated between the auxiliary and main task. We confirm these findings empirically with teacher-student models, and introduce a technique - weight rescaling following pretraining - that can elicit the nested feature selection regime. Finally, we validate our theory in deep neural networks trained on image classification. We find that weight rescaling improves performance when it causes models to display signatures of nested feature selection. Our results suggest that nested feature selection may be an important inductive bias for finetuning neural networks.

## 1 Introduction

Neural networks are often trained on multiple tasks, either simultaneously ("multi-task learning," henceforth MTL, see ) or sequentially ("pretraining" and subsequent "finetuning," henceforth PT+FT, see ). Empirically, models can transfer knowledge from auxiliary tasks to improve performance on tasks of interest. However, theoretical understanding of how auxiliary tasks influence learning and generalization is limited.

Auxiliary tasks are especially useful when there is less data available for the target task. Modern "foundation models," trained on data-rich general-purpose auxiliary tasks (like next-word prediction or image generation) before adaptation to downstream tasks, are a timely example of this use case .

Auxiliary tasks are also commonly used in reinforcement learning, where performance feedback can be scarce . Intuitively, auxiliary task learning biases the target task solution to use representations shaped by the auxiliary task. When the tasks share common structure, this influence may enable generalization from relatively few training samples for the task of interest. However, it can also have downsides, causing a model to inherit undesirable biases from auxiliary task learning [7; 8].

An influential strategy in the literature on the theory of _single-task_ learning has been to characterize the _implicit regularization_ conferred by the combination of network architecture and optimization algorithm [9; 10; 11; 12; 13]. Alternatively, some others characterize the effects of _explicit parameter regularization_ (e.g. an \(_{2}\)-penalty on weights) on the inductive bias of networks towards learning certain functions [14; 15; 16]. Compared to the single-task case, the inductive bias of MTL and PT+FT, whether obtained via explicit regularization or implicit regularization induced by optimization dynamics, is less well understood. Here we make progress on this question by studying inductive biases of MTL and PT+FT in two network architectures that have been extensively theoretically studied: diagonal linear networks, and densely connected networks with one hidden layer and a ReLU nonlinearity. We then demonstrate that our insights transfer to practically relevant scenarios by studying deep neural networks trained on image classification tasks.

Our specific contributions are as follows:

* We characterize regularization penalties associated with MTL or PT+FT in both diagonal linear networks and single hidden-layer ReLU networks (Section 3).
* We find that both MTL and PT+FT are biased towards solutions that reuse features between tasks, and that rely on a sparse set of features (Section 4.2).
* We then find that under suitable scalings, PT+FT exhibits a "nested feature selection" regime, distinct from previously characterized "rich" and "lazy" regimes, which biases finetuning to extract sparse subsets of the features learned during pretraining (Sections 4.3 and 4.4).
* We find that PT+FT in ReLU networks can benefit from correlated (not just identical) features between the main and auxiliary task, but only when coefficients of features in the main task weights are of comparable magnitude (Section 4.5).
* Finally, we study deep neural networks trained on natural image data (CIFAR-100 and ImageNet) (Section 5). Remarkably, we find that rescaling weights before finetuning improves accuracy in ResNets. Our analysis of the network representations suggests that this weight rescaling also results in the network relying on a low-dimensional subspace of its pretrained representation (i.e. exhibiting nested feature selection behavior). Intriguingly, Vision Transformers already exhibit signatures of nested feature selection without weight rescaling, and do not benefit from weight rescaling, suggesting that the nested feature selection regime is beneficial for finetuning performance.

## 2 Related work

A variety of studies have characterized implicit regularization effects in deep learning. These include biases toward low-frequency functions , stable minima in the loss landscape , low-rank solutions , and lower-order moments of the data distribution . Chizat & Bach  show that when using cross-entropy loss, shallow (single hidden layer) networks are biased to minimize the \(_{1}\) norm, an infinite-dimensional analogue of the \(_{1}\) norm over the space of possible hidden-layer features [see also 12; 14]. Other work has shown that implicit regularization for mean squared error loss in nonlinear networks cannot be exactly characterized as norm minimization , though \(_{1}\) norm minimization is a precise description under certain assumptions on the inputs .

Compared to the body of work on inductive biases of single-task learning, theoretical treatments of MTL and PT+FT are more scarce. Some prior studies have characterized benefits of multi-task learning with a shared representational layer in terms of bounds on sample efficiency [23; 24; 25]. Others have characterized the learning dynamics of deep linear networks trained from nonrandom initializations, which can be applied to understand finetuning dynamics [26; 27]. Similarly, insights on the implicit regularization of gradient descent in linear models has been applied to better understand forgetting and generalization in a continual learning setup [28; 29; 30; 31]. However, while these works demonstrate an effect of pretrained initializations on learned solutions, the linear models they study do not capture the notion of feature learning we are interested in. Finally, teacher-student setupshave been used to study the impact of task similarity on continual learning in deep neural networks [32; 33]. This methodology could also be applied to our setup (i.e. to investigate generalization on a finetuning task), and, similarly, our tools could be applied to continual learning setups.

A few empirical studies have compared the performance of MTL vs. PT+FT in language tasks, with mixed results depending on the task studied [34; 35]. Others have observed that PT+FT outperforms PT + "linear probing" (training only the readout layer and keeping the previous layers frozen after pretraining), implying that finetuning benefits from the ability to learn task-specific features [36; 37].

**Inductive biases of diagonal linear networks.** The theoretical component of our study relies heavily on a line of work [38; 39; 40; 41; 42] that studies the inductive bias of a simplified "diagonal linear network" model. Diagonal linear networks parameterize linear maps \(f:^{D}\) as

\[f_{}()=(),_{d}():=w_{+,d}^{(2)}w_{+,d}^{(1)}-w_{-,d}^{(2)}w_{-,d}^{(1)}\] (1)

where \(()^{D}\). These correspond to two-layer linear networks in which the first layer consists of one-to-one connections, with duplicate \(+\) and \(-\) pathways to avoid saddle point dynamics around \(=0\). Woodworth _et al._ showed that overparameterized diagonal linear networks trained with gradient descent on mean squared error loss find the zero-training-error solution that minimizes \(\|\|_{2}\), when trained from large initial weight magnitude (the "lazy" regime, equivalent to ridge regression). When trained from small initial weight magnitude, networks instead minimize \(\|\|_{1}\) (the "rich" regime). This bias is a linear analogue of feature learning/feature selection, as a model with an \(_{1}\) penalty tends to learn solutions that depend on a sparse set of input dimensions.

**Implicit vs. explicit regularization.** Theoretical work on the inductive biases conferred by different architectures has studied both the implicit regularization induced by gradient descent and explicit \(_{2}\)-regularization. Notably, in homogeneous networks trained with crossentropy loss, implicit and explicit regularization yield identical inductive biases in the limit of infinite training time and infinitesimal regularization [12; 43], but this does not hold in general . While [38; 40] are able to characterize the implicit regularization of gradient descent for diagonal linear networks, it is technically much more challenging to derive a similar result for multi-output diagonal linear networks, or for ReLU networks of any kind. In contrast, the impact of _explicit_ weight regularization has been characterized for both multi-output diagonal linear networks  and multi-output ReLU networks [44; 45; 46].

Our main technical contributions to this theoretical landscape are (1) spelling out the implications of existing results on implicit regularization in diagonal linear networks and (2) providing a novel characterization of the inductive bias induced by applying explicit parameter regularization to the finetuning of ReLU networks from arbitrary initialization. Our choice to study explicit parameter regularization for ReLU networks is made primarily for theoretical tractability; we view these results as a proxy for understanding the more theoretically complex problem of implicit regularization.

## 3 Implicit and explicit regularization penalties for MTL and PT+FT

### Theoretical setup

**Architectures.** First, we consider **diagonal linear networks** with hidden weights \(_{+},_{-}^{D}\) and \(O\{1,2\}\) output weights \(v_{+},v_{-}^{O D}\). (\(O\) is 1 or 2 depending on the training paradigm, see below.) The resulting network function is defined as

\[f_{w,v}()=(w,v),_{o}(w,v):=_{+,o} _{+}-_{-,o}_{-},(w,v)^{ O D}.\] (2)

Second, we consider **ReLU networks** with \(H\) hidden neurons, hidden weights \(w^{H D}\), and \(O\{1,2\}\) readout weights \(v^{O H}\). The network function is defined as

\[f_{w,v}()=_{h=1}^{H}_{h}(_{h},)_ {+},\] (3)

where \(()_{+}\) is the ReLU nonlinearity. Importantly, the ReLU nonlinearity is homogeneous and, as a result, the network function is invariant to rescaling the hidden weights by \(>0\) and the readout weights by \(\). It will therefore be useful to consider a re-parameterization in terms of the

\[m_{h}:=v_{h}\|_{h}\|_{2}_{h}:=_{h}/\|_{h}\|_{2}.\] (4)

Under this re-parameterization, an equivalent definition of the network function is given by

\[f_{m,}()=_{h=1}^{H}m_{h}(_{h}, )_{+}=f_{w,v}().\] (5)Training paradigms.We consider two datasets: an auxiliary task \(X^{aux}^{n_{aux} D},^{ux}^{n_{aux}}\) and a main task \(X^{main}^{n_{main} D},^{main}^{n_{main}}\).

First, we consider **multi-task learning** (MTL): simultaneous training on both the auxiliary and main task. In this case, we consider networks with \(O=2\) outputs. The first output corresponds to the auxiliary task and the second output to the main task. Accordingly, we denote \(^{aux}:=_{1}\) and \(^{main}:=_{2}\) in the diagonal linear network and \(^{ux}:=_{1}\) and \(^{main}:=_{2}\) in the ReLU network.

Second, we consider **pretraining** on the auxiliary task and subsequent **finetuning** on the main task (PT+FT). In this case, we consider networks with a single output (\(O=1\)), but re-initialize the readout weights before finetuning. Accordingly, we denote the parameters learned after pretraining by \(w^{aux}\) and \(v^{aux}\), and the parameters learned after finetuning by \(w^{main}\) and \(v^{main}\). We define \(^{main},^{aux}\) (for the diagonal network) and \(m^{main},^{main},m^{aux},^{aux}\) (for the ReLU network) analogously.

### Explicit regularization penalties in multi-task learning

To theoretically understand the inductive bias of diagonal linear and ReLU networks trained with MTL, we consider the effect of minimizing the \(_{2}\) parameter norm as an approximation of the implicit bias of training with gradient descent from small initialization. We argue that this is a reasonable heuristic. First, the analogous result holds in the single-output case for infinitesimally small initialization and two layers (though not for deeper networks, see ). Second, for cross-entropy loss it has been shown that gradient flow on all positively homogeneous networks (including diagonal linear networks and ReLU networks) converges to a KKT point of a max-margin/min-parameter-norm objective . Finally, explicit \(_{2}\) parameter norm regularization ("weight decay") is commonly used in practice, making its inductive bias important to understand in its own right as well.

We now derive the norms minimized by explicit regularization in MTL:

**Corollary 1**.: _For the multi-output **diagonal linear network** defined in Eq. 2, a solution \(^{*}\) with minimal parameter norm \(\|w\|_{2}^{2}+\|v\|_{2}^{2}\) subject to the constraint that it fits the training data (\(X^{main}^{main}=^{main},X^{aux}^{aux}=^{ aux}\)) also minimizes the following:_

\[^{*}=*{arg\,min}_{}(2_{d=1}^{D}^{aux})^{2}+(_{d}^{main})^{2}}) X^{main} ^{main}=^{main},\ X^{aux}^{aux}=^{aux}.\]

This norm is known as the group lasso  and denoted as \(\|\|_{1,2}\). For **ReLU networks**, by an argument analogous to the one above, parameter norm minimization translates to minimizing \(_{h=1}^{H}^{aux})^{2}+(m_{h}^{main})^{2}}\) (see Appendix A.1 and [44; 46]).

The norm \(\|\|_{1,2}\) is plotted in Fig. 1a. We analyze its impact in Section 4.

### Regularization penalties in finetuning

**Diagonal linear networks.** We now consider the behavior of PT+FT in overparameterized diagonal linear networks trained to minimize mean-squared error using gradient flow. We assume that the

Figure 1: Theoretically derived regularization penalties. **a**, Explicit regularization penalty associated with multi-task learning. **b**, Implicit regularization penalty associated with finetuning in diagonal linear networks. **c**, Explicit regularization penalty associated with finetuning in ReLU networks. This penalty also depends on the changes in feature direction over finetuning (measured by the correlation between the unit-normalized feature directions pre vs. post finetuning).

network is initialized prior to pre-training with infinitesimal weights, and that during pretraining, network weights are optimized to convergence on the training dataset \((X^{aux},^{aux})\) from the auxiliary task. After pretraining, the second-layer weights (\(v_{+}\) and \(v_{-}\)) are reinitialized with constant magnitude \(\). Further, to ensure that the network output pre-finetuning is zero (as in ), we set the values of corresponding positive and negative pathway weights to be equal to their sum following pretraining1. The network weights are further optimized to convergence on the main task dataset \((X^{main},^{main})\). The dynamics of the pretraining and finetuning steps can be derived as a corollary of :

**Corollary 2**.: _If the gradient flow solution \(^{aux}\) for the diagonal linear model in Eq. 1 during pretraining fits the auxiliary task training data with zero error (i.e. \(X^{aux}^{aux}=^{aux}\)), and following reinitialization of the second-layer weights and finetuning, the gradient flow solution \(^{*}\) after finetuning fits the main task data with zero training error (i.e. \(X^{main}^{main}=^{main}\)), then_

\[^{*}=*{arg\,min}_{^{main}}\|^{ main}\|_{Q} X=,\]

\[\|^{main}\|_{Q}:=_{d=1}^{D}(|_{d}^{aux}|+^{2} )q(^{main}}{|_{d}^{aux}|+^{2}}),  q(z)=2-}+z*{arcsinh}(z/2).\]

This corollary is proven in Appendix A.2. The norm is plotted in Fig. 0(b).

**ReLU networks.** We assume that after pretraining, the readout layer is re-initialized with arbitrary new weights \(^{H}\). We then characterize the solution to the finetuning task that minimizes the \(_{2}\) norm of weight changes from this initialization. This is similar to the explicit weight regularization considered in the previous section, except we now penalize weight changes from a particular initialization rather than the origin. We chose to consider this regularization penalty for two reasons. First, it is sometimes studied in the continual learning setting . Second, infinitesimal explicit regularization is equivalent to the implicit regularization induced by gradient descent in the case of shallow linear models . While this is not true more generally, it is a useful heuristic to motivate our theoretical analysis (which we then validate using our experiments in Section 4).

We show that this finetuning solution implicitly minimizes the following penalty:

**Proposition 3**.: _Consider a single-output ReLU network (see Eq. 3) with first-layer weights \(_{h}^{aux}^{D}\) after pretraining, and second-layer weights re-initialized to \(^{H}\). The solution to the finetuning task that minimizes the \(_{2}\) norm of changes in the weights, i.e. minimizes \(_{h=1}^{H}\|_{h}-_{h}^{aux}\|_{2}^{2}+(v_{h}-_{h})^{2}\), is equivalent to the solution that minimizes_

\[R(^{main},m^{main}|^{aux},m^{aux}):=_{h=1}^{H}r( _{h}^{main},m_{h}^{main}|_{h}^{aux},m_{h}^{aux}),\]

\[r(_{h}^{main},m_{h}^{main}|_{h}^{aux},m_{h}^{aux}):=( m_{h}^{main}/u^{*}-_{h})^{2}+(u^{*})^{2}+m_{h}^{aux}-2u^{*}^{ aux}}(_{h}^{main},_{h}^{aux}),\]

_where \(u^{*}\) is the unique positive real root of_

\[-(m_{h}^{main})^{2}+_{h}m_{h}^{main}u-m_{h}^{aux}(_{h}^{ main},_{h}^{aux})u^{3}+u^{4}=0.\] (6)

We prove the proposition in Appendix A.3. It implies that the regularization penalty associated with finetuning in the ReLU network only depends on the correlation \(_{h}:=_{h}^{main},_{h}^{aux}\) between the first-layer feature weights before and after finetuning, and the magnitudes of the weights of these features. We plot this penalty in Fig. 0(c).

## 4 Implications of the theory: multiple regimes of feature reuse

### Sample efficiency in teacher-student models

To validate these theoretical characterizations and illustrate their consequences, we now perform experiments in a teacher-student setup. In the diagonal linear network case, we consider a linear regression task defined by \(^{1000}\) with a sparse set of \(k\) non-zero entries. We sample two such vectors, corresponding to "auxiliary" and "main" tasks, varying the number of non-zero entries \(k_{aux}\) and \(k_{main}\), and the number of shared features (overlapping non-zero entries). We then uniformly sample input vectors \(^{1000}\) from the unit sphere, using the ground-truth weights to generate the target. We train on 1024 auxiliary samples and vary the number of main task samples.

In the ReLU network case, we consider a "teacher" ReLU network with a sparse number of units (i.e. a low-dimensional hidden layer) and different kinds of overlaps (e.g. shared, correlated, or orthogonal features) between the auxiliary and main task. We randomly sample input data \(^{15}\) from the unit sphere and use the teacher network to generate the target. We train on 1024 auxiliary samples and vary the number of main task samples. During finetuning, we randomly re-initialize the readout weights using a normal distribution with a variance of \(10^{-3}\).

### PT+FT and MTL benefit from sparse and shared features

**Feature sparsity.** To understand the impact of the derived penalties in the case of features either used or not used during pretraining, we consider their limit behavior for very large or very small pretrained features. First, we consider the limit \(|}{|_{d}^{aug}|+^{2}}\) (capturing the case of a feature not used during pretraining). In this limit, the MTL penalty converges to \(2|_{d}|\). Similarly, for finetuning in diagonal linear networks, the penalty converges to \(c|_{d}|\) where \(c((1/(|_{d}^{aux}|+^{2})))\) (per an analysis in ). For new features, both networks therefore have an \(_{1}\) norm minimization bias, suggesting that they tend to learn a sparse set of new features (just like in the single-task case).

To test this insight, we consider a teacher-student setup without any shared features between the auxiliary and main task, and vary the number of main task features. Indeed, we found that both MTL and PT+FT have a more rapidly decreasing generalization loss for fewer features (just like single-task learning (STL)) (Fig. 2a). We further confirmed that they learned a sparse set of weights (Fig. 7).

The same phenomenon holds true for the ReLU network penalties as well. We considered a teacher-student network with six auxiliary task features and one to six uncorrelated main task features. Again, both MTL and PT+FT have a lower generalization loss for fewer features (Fig. 2b).

**Feature sharing.** Next, we consider the opposite limit, i.e. large pretrained features: \(|}{|_{d}^{aug}|+|^{2}} 0\). In this limit, both the MTL and the PT+FT penalty for diagonal linear networks converges to \(^{aug}}{|_{d}^{aug}|}\)

Figure 2: PT+FT and MTL benefit from feature sparsity and reuse. **a,b**, Generalization loss for a) diagonal linear networks and b) ReLU networks trained on a) a linear model with distinct active dimensions and b) a teacher network with distinct units between auxiliary and main task (STL: single-task learning). MTL and PT+FT benefit from a sparser teacher on the main task. **c,d**, Generalization loss for c) diagonal linear networks and d) ReLU networks trained on a teacher model sharing all features between the auxiliary and main task. PT+FT and MTL both generalize better than STL. **e,f**, Generalization loss for e) diagonal linear networks and f) ReLU networks trained on a teacher model with overlapping features. Networks benefit from feature sharing _and_ can learn new features.

a weighted \(_{2}\) bias. This implies that the networks preferentially use large pretrained features. To test this, we consider a teacher-student setup with fully overlapping dimensions. Indeed, both MTL and PT+FT outperform STL in this case (Fig. 1(c)). Notably, they perform similarly to a network where we only finetune the second layer (PT+FT (Linear Probing, LP)), which exactly implements the weighted \(_{2}\) bias.

In the case of ReLU networks, we considered a teacher network with the same six features for the auxiliary and main task. Again, we found that MTL and PT+FT outperformed STL, though training a linear readout from a model with fixed features (either by using the hidden layer (PT+FT (LP)) or the neural tangent kernel (PT+FT (NTK))) generalized even better (Fig. 1(d)).

**Simultaneous sparsity and feature sharing.** Finally, our analysis above considered the limit behavior of each feature separately. This suggests that models should be able to 1) preferentially rely on pretrained features and 2) when necessary, learn a sparse set of new features. To test this insight, we consider partially overlapping teacher models. In diagonal linear networks, we consider teacher models with forty auxiliary and main task features, with twenty of thirty of those features overlapping (Fig. 1(e)). On the one hand, both PT+FT and MTL outperformed STL, indicating that they were able to benefit from the pretrained features. On the other hand, they also performed better than the PT+FT (LP) model which only finetuned the second layer (and therefore did not have a sparse inductive bias), indicating that they tended to learn a sparse set of new features. In ReLU networks, we consider teacher models with six auxiliary and main task features, varying their overlap. Again, we find that MTL and PT+FT outperformed both STL and PT+FT (LP), indicating that they benefitted from feature learning by implementing an inductive bias towards sparse and shared features.

**Differences between the MTL and PT+FT norms.** So far, we have highlighted several similarities between the MTL and PT+FT norms in diagonal linear networks: they tend towards the \(_{1}\) norm for small auxiliary features and a weighted \(_{2}\) norm for large auxiliary features. In the next section, we will highlight an important difference that arises in the intermediate regime. Here we briefly highlight a difference in the limit behavior for diagonal linear networks: the relative weights of the \(_{1}\)- and weighted \(_{2}\)-penalty are different between MTL and PT+FT. In particular, in the \(_{1}\) penalty limit, there is an extra factor of order \(((1/(|_{d}^{aux}|+^{2})))\) in the PT+FT penalty. Assuming small initializations, this factor tends to be larger than \(2\), the corresponding coefficient in the MTL penalty. Thus, PT+FT is more strongly biased toward reusing features from the auxiliary task compared to MTL. We are careful to note, however, that in the case of ReLU networks this effect is complicated by a qualitatively different phenomenon with effects in the reverse direction (see Section 4.5).

### A conservation law

We now turn our attention to the intermediate regime where the coefficients of a feature are of similar magnitude in the auxiliary and main tasks. We define two functions for a given penalty \(P(_{d}^{main},_{d}^{aux})\) (where \(_{d}^{main}\) is the main task feature coefficient and \(_{d}^{aux}\) is the auxiliary task feature coefficient): 1) the "\(\)-order," \(^{aux}}\), which measures, locally, how strongly \(P\) changes with increasing \(_{d}^{main}\) and 2) the "feature dependence" (FD), \(^{aux}}\), which measures, locally, how much \(P\) decreases for a larger auxiliary feature. In the previous section, we found that for \(_{d}^{aux} 0\), \(\)-order \( 1\) and FD \( 0\), i.e. the penalty becomes \(_{1}\)-like and does not depend on the magnitude of the auxiliary feature. In contrast, for \(_{d}^{aux} 1\), \(\)-order \( 2\) and FD \(-1\), i.e. the penalty becomes \(_{2}\)-like and depends inversely on the auxiliary task feature coefficient magnitude.

How are \(\)-order and FD related for intermediate values of \(_{d}^{aux}\)? Remarkably, we find an exact analytical relationship between these measures that holds for all penalties considered here:

**Proposition 4**.: _For the MTL and PT+FT penalties derived in both the diagonal linear and ReLU cases, \(\)-order \(+\)\(FD=1\)._

See Appendix A.4 for proof.

Thus, there is an exact tradeoff between the sparsity and feature dependence even for intermediate auxiliary task feature coefficient values: an increase in sparsity (i.e. a smaller \(\)-order) yields a corresponding decrease in feature dependence (i.e. FD becomes closer to zero).

### The nested feature selection regime

Proposition 4 predicts the existence of a novel "nested feature selection" regime: for intermediate magnitudes of the auxiliary features, the penalties should encourage both sparsity and feature dependence. To test this prediction in diagonal linear networks, we use a teacher-student setting in which all of the main task features are a subset of the auxiliary task features, i.e. \(k_{main} k_{aux}\), and the number of overlapping units is equal to \(k_{main}\). Solving this task most efficiently involves "nested feature selection," a bias towards feature reuse _and_ towards sparsity among the reused features. We plot \(\)-order and FD for \(_{d}^{main}=1\) and varying the auxiliary task feature coefficient (Fig. 3a) and find that for \(_{d}^{aux} 1\), both norms exhibit "lazy regime"-like behavior (\(\)-order of around 2, and FD near \(0\)). This predicts that neither MTL nor PT+FT networks should be able to benefit from nested sparsity task structure in this regime, which we confirm empirically: as \(k_{main}\) decreases, the networks' sample efficiency does not become substantially better (Fig. 3b).

However, for features with auxiliary task coefficients that are moderately smaller than their main task coefficients, Fig. 3a suggests a broad regime where the \(\)-order is closer to 1 (incentivizing sparsity), but feature dependence remains high. We can produce this behavior in these tasks by rescaling the weights of the network following pretraining by a factor less than \(1\). In line with the prediction of the theory, performing this manipulation enables PT+FT to leverage sparse structure _within_ auxiliary task features (Fig. 3c), even while retaining their ability to privilege features learned during pretraining above others (see Fig. 9). By contrast, this regime is much narrower for MTL (Fig. 3a).

For ReLU networks, the MTL penalty is the same as in diagonal linear networks. We plot the regularization penalty derived for PT+FT, conditioned on various correlations \(_{h}^{main},_{h}^{aux}\) between the post-pretraining and post-finetuning feature weights (Fig. 3d). For features that are fully aligned before and after pretraining, this penalty is again \(_{2}\)-like for \(m^{aux} 1\). However, in most cases, these features change direction at least a little bit, and we find that in that case the penalty is more \(_{1}\)-like while still remaining sufficiently feature-dependent. This suggests that a nested feature selection regime may arise in ReLU networks even when auxiliary task feature coefficients have comparable magnitude to main task feature coefficients. To test this insight, we considered a set of tasks in which the main task solution relies exclusively on a subset of auxiliary task features. We found that PT+FT (even without any weight rescaling) was able to benefit from the nested sparsity structure, but MTL was not (Fig. 3e).2 Performing weight rescaling in either direction following pretraining uncovers the

Figure 3: PT+FT (much moreso than MTL) exhibits a nested feature selection regime. **a-c**, Diagonal linear networks. **a**, \(\)-order/feature dependence plotted for \(_{d}^{main}=1\) and varying the auxiliary task feature coefficient. **b**, Generalization loss for models trained on a teacher with 40 active units during the auxiliary task and a subset of those units active during the main task. **c**, Generalization loss for PT+FT models whose weights are rescaled by the factor in the parentheses before finetuning. **d-f**, ReLU networks. **d**, \(\)-order/feature dependence plotted for the explicit finetuning and MTL penalties, for \(m=1\) and varying the auxiliary task feature coefficient. **e**, Generalization loss for models trained on a teacher network with six active units on the auxiliary task and a subset of those units on the main task. **f**, Generalization loss for PT+FT models whose weights are rescaled before finetuning.

initialization-insensitive (FD near \(0\)), sparsity-biased (\(\)-order near \(1\)) rich / feature-learning regime and the initialization-biased (FD near \(-1\)), no-sparsity-bias (\(\)-order near \(2\)) lazy learning regime (Fig. 3f). This suggests that for different architectures and different tasks, different rescaling values may be required to enter the nested feature selection regime.

### PT+FT, but not MTL, in ReLU networks benefits from correlated features

The regularization associated with PT+FT yields benefits even when main/auxiliary task directions are correlated but not identical (Fig. 1c). In contrast, MTL cannot softly share features as it encodes correlated features with entirely distinct units. To test this hypothesis, we conduct experiments in which the ground-truth auxiliary and main tasks rely on correlated but distinct features. Indeed, we find PT+FT outperforms STL in this case, whereas MTL only does so in the low-sample setting (Fig. 4a). Notably, if features are identical, MTL outperforms PT+FT in the low-sample setting (Fig. 2d). Thus, PT+FT (compared to MTL) trades off the flexibility to "softly" share features for reduced sample-efficiency when such flexibility is not needed.

The analysis in Fig. 3d would predict that ReLU networks can no longer benefit from correlated features if the magnitude of auxiliary task features is much higher than that of main task features. To test this, we varied the magnitude of the main task features and their correlation with auxiliary task features (Fig. 4b). We found that for lower magnitudes, PT+FT still improved performance for the case of identical but not correlated feature directions, confirming our prediction.

## 5 Weight rescaling in deep networks gives rise to nested feature selection

Our analysis has focused on shallow networks trained on synthetic tasks. To test the applicability of our insights, we conduct experiments with convolutional networks (ResNet-18, ) on a vision task (CIFAR-100, ), using classification of two image categories (randomly sampled for each training run) as the primary task and classification of the other 98 as the auxiliary task. As in our experiments above, MTL and PT+FT improve sample efficiency compared to single-task learning (Fig. 5a).

Our findings in Section 4.4 indicate that the nested feature selection bias of PT+FT can be exposed or masked by rescaling the network weights following pretraining. Such a bias may be beneficial when the main task depends on a small subset of features learned during pretraining, as may often be the case in practice. We experiment with rescaling in our CIFAR setup. We find that rescaling values less than \(1\) improve finetuning performance (Fig. 5b). These results suggest that rescaling network weights before finetuning may be practically useful. We corroborate this hypothesis with additional experiments using networks pre-trained on ImageNet  (see Fig. 10).

To facilitate comparison of the phenomenology in deep networks with our teacher-student experiments above, we propose a signature of nested feature selection that can be characterized without knowledge of the underlying feature space (since the correct feature basis to analyze is less clear in multi-hidden-layer networks). Specifically, we propose to measure (1) the _dimensionality_ of the network representation pre- and post-finetuning, and (2) the extent to which the representational structure post-finetuning is shared with / inherited from that of the network following pretraining prior to finetuning. We employ the commonly used _participation ratio_ (PR, ) as a measure of dimensionality, and the _effective number of shared dimensions_ (ENSD, ) as a soft measure of the number of aligned principal components between two representations. Intuitively, the PR and ENSD of network representations pre- and post-finetuning capture the key phenomena of the nested feature selection regime: we expect the dimensionality of network after finetuning to be

Figure 4: PT+FT, but not MTL, in ReLU networks benefits from correlated features. **a**, Generalization loss for main task features that are correlated (0.9 cosine similarity) with the auxiliary task features. PT+FT outperforms both MTL and STL. **b**, Generalization loss for main task features with varying correlation and magnitude (mag.). PT+FT only outperforms STL if the features are either identical in direction or identical in magnitude.

lower than after pretraining (\(PR(_{FT})<PR(_{PT})\)), and for nearly all of the representational dimensions expressed by the network post-finetuning to be inherited from the network state after pretraining (\(ENSD(_{PT},_{FT}) PR(_{FT})\)). We validate that this description holds in our nonlinear teacher-student experiments with networks in the nested feature selection regime (Fig. 11).

Remarkably, we find that the ResNet-18 exhibits the same phenomenology, but only for weights rescaled by a small value (Fig. 5c). This supports the hypothesis that the observed benefits of rescaling indeed arise from pushing the network into the nested feature selection regime.

Finally, we conduct the same experiment for Vision Transformers (ViT) . We confirm that PT+FT improves performance over single-task learning, though MTL offers no similar benefit in this case (Fig. 5d). We further find that rescaling before finetuning (both by larger and smaller values) decreases generalization performance (Fig. 5e). Notably, our representational analysis reveals that rescaling by a larger value yields a higher-dimensional subspace both before and after finetuning, whereas rescaling by a smaller value yields a lower-dimensional subspace, but pushes the effective number of shared dimensions down substantially (Fig. 5f). This indicates that a rescaling value of \(1\) may already give rise to the nested feature selection regime and rescaling by a smaller value pushes the ViT towards the pure feature learning regime. Taken together, our results suggest finetuning performance is best when networks operate in the nested feature selection regime, and weight rescaling can push networks into this regime when it does not arise naturally.

## 6 Conclusion

In this work we have provided a detailed characterization of the inductive biases associated with two common training strategies, MTL and PT+FT, in diagonal linear and ReLU networks. These biases incentivize both feature sharing and sparse task-specific feature learning. In the case of PT+FT, we characterized a novel _nested feature selection_ learning regime which encourages sparsity _within_ features inherited from pretraining. This insight motivates a simple technique for improving PT+FT performance by pushing networks into this regime, which shows promising empirical results.

There are several avenues for extending our theoretical work: for example, connecting our derived penalties for ReLU networks (which assumed explicit parameterization) to the implicit regularization induced by dynamics of gradient descent, and extending our theory to the case of cross-entropy loss. In addition, more work is needed to extend our theory to more complex tasks and larger models. For instance, we are interested in investigating how the weight magnitudes required to enter the nested feature regime depend on architecture and properties of the tasks -- we observed a difference between the rescaling values required for ResNets and Vision Transformers, which our shallow network theory is unable to speak to. Better understanding the conditions needed for nested feature selection could also inspire more sophisticated interventions than our weight rescaling trick. Finally, we considered a particularly simple multi-task setup, with identical formats between auxiliary and main tasks -- our theory could be extended to cases where the different tasks use different objectives. Nevertheless, our work already provides new and practical insights into multi-task learning and finetuning.

Figure 5: Experiments in deep neural networks trained on CIFAR-100: **a-c**, ResNet-18, **d-f**, ViT. **a,d**, Accuracy for MTL, PT+FT, and STL in a) ResNet-18 and d) ViT. **b,e** Accuracy for PT+FT with weight rescaling in b) ResNet-18 and **e)** ViT. **c,f** The participation ration of c) ResNet-18’s and f) ViT’s layers before and after finetuning (PR Pre and PR Post) as well as their ENSD.