# Learning Disentangled Representations for

Perceptual Point Cloud Quality Assessment via

Mutual Information Minimization

 Ziyu Shan\({}^{}\), Yujie Zhang\({}^{}\), Yipeng Liu, Yiling Xu\({}^{}\)

Cooperative Medianet Innovation Center, Shanghai Jiao Tong University

{shanziyu, yujie19981026, liuyipeng, yl.xu}@sjtu.edu.cn

Ziyu Shan and Yujie Zhang contribute equally to this work.Corresponding author

###### Abstract

No-Reference Point Cloud Quality Assessment (NR-PCQA) aims to objectively assess the human perceptual quality of point clouds without relying on pristine-quality point clouds for reference. It is becoming increasingly significant with the rapid advancement of immersive media applications such as virtual reality (VR) and augmented reality (AR). However, current NR-PCQA models attempt to indiscriminately learn point cloud content and distortion representations within a single network, overlooking their distinct contributions to quality information. To address this issue, we propose DisPA, a novel disentangled representation learning framework for NR-PCQA. The framework trains a dual-branch disentanglement network to minimize mutual information (MI) between representations of point cloud content and distortion. Specifically, to fully disentangle representations, the two branches adopt different philosophies: the content-aware encoder is pretrained by a masked auto-encoding strategy, which can allow the encoder to capture semantic information from rendered images of distorted point clouds; the distortion-aware encoder takes a mini-patch map as input, which forces the encoder to focus on low-level distortion patterns. Furthermore, we utilize an MI estimator to estimate the tight upper bound of the actual MI and further minimize it to achieve explicit representation disentanglement. Extensive experimental results demonstrate that DisPA outperforms state-of-the-art methods on multiple PCQA datasets.

## 1 Introduction

With recent advances in 3D capture devices, point clouds have become a prominent media format to represent 3D visual content in various immersive applications, such as autonomous driving and virtual reality [4; 43]. These extensive applications stem from the rich information provided by point clouds (_e.g._, geometric coordinates, color). Nevertheless, before reaching the user-client, point clouds inevitably undergo various distortions at multiple stages, including acquisition, compression, transmission and rendering, leading to undesired perceptual quality degradation. Accordingly, it is necessary to develop an effective metric that introduces human perception into the research of point cloud quality assessment (PCQA), especially in the common no-reference (NR) situation where pristine reference point clouds are unavailable.

In recent years, many deep learning-based NR-PCQA methods [21; 47; 51; 32; 3] have shown remarkable performance on multiple benchmarks, which can be applied directly to 3D point cloud data or 2D rendered images. Most of these methods [47; 22; 51; 3] tend to learn a unified representation for quality prediction, ignoring the fact that perceptual quality is determined by both point cloudcontent information and distortion pattern. Although some other models  alternately learn content and distortion representations through different training objectives, they are still based on a single-branch network and thus may lead to highly entangled features in the representation space.

From the perspective of visual rules, insufficient disentanglement between representations of point cloud content and distortion disobeys the perception mechanisms of the human vision system (HVS), further limiting performance improvement. In fact, many studies  highlight the distinct visual processing of high-level (_e.g._, semantics) and low-level information (_e.g._, distortions) in different areas of the brain. Concretely, the left and right hemispheres of the brain are specialized in processing high-level and low-level information, respectively. These findings suggest a relatively disentangled processing mechanism in our brain, challenging existing methods that seek to learn these conflicting representations using a single network indiscriminately.

The difficulty of disentangled feature learning is relatively great for NR-PCQA due to data imbalance. Specifically, although a wide range of distortion types and intensities in current PCQA datasets can enable the learning of robust low-level distortion representations, it is non-trivial to learn the representations of point cloud content that lies in a considerable high dimensional space because these PCQA datasets are extremely limited in terms of content (_e.g._, up to 104 contents in LS-PCQA ). This data limitation can lead to overfitting of NR-PCQA models regarding point cloud content, that is, when the content changes, the prediction score changes in the undesired manner, even with the same distortion pattern. As illustrated in Figure 1 (b) and (c), the NR-PCQA models PQA-Net  and GPA-Net  correctly predict the trend of quality degradation with increasing distortion intensity, but their predicted score spans deviate a lot from the ground truth in Figure 1 (a), where the content varies but the distortion pattern remains intact. Based on these observations, we expect a new disentangled representation learning framework that can obey the separate information processing mechanism of HVS, and alleviate the difficulty of content and distortion representation learning introduced by data imbalance.

In this paper, we propose a new **Dis**entangled representation learning framework tailored for NR-PCQA, named DisPA. Motivated by the HVS perception mechanism, DisPA employs a dual-branch structure to learn representations of point cloud content and distortion (called content-aware and distortion-aware branches). DisPA has three steps to achieve disentanglement: 1) To address the problem introduced by data imbalance, we pretrain a content-aware encoder based on masked autoencoding strategy. Specifically, in this pretraining process, the distorted point cloud is rendered into multi-view images whose patches will be partially masked. The partially masked images are then fed into the content-aware encoder to reconstruct the rendered images of the corresponding reference point cloud. 2) To facilitate learning of distortion-aware representations, we decompose the distorted multi-view images into a mini-patch map through grid mini-patch sampling , which can prominently present local distortions and forces the distortion-aware encoder to ignore the global content. 3) Inspired by the utilization of mutual information (MI) in disentangled representation learning , we propose an MI-based regularization to explicitly disentangle the latent representations. Compared to simple linear correlation coefficients (_e.g._, cosine similarity), mutual information can capture the nonlinear statistical dependence between representations . To achieve this, we utilize an MI estimator to estimate a tight upper bound of the MI and further minimize it to achieve straightforward disentanglement. We summarize the main contributions as follows:

Figure 1: Statistics of SJTU-PCQA (part)  and predicted quality scores of NR-PCQA models (PQA-Net  and GPA-Net ). Quality scores of different distortion types are in lines of different colors. Red circles are to highlight the score span of different contents with the same distortion.

* We propose a novel disentangled representation learning method for NR-PCQA called DisPA, which obeys the particular HVS perception mechanism. To the best of our knowledge, DisPA is the first framework to explore representation disentanglement in PCQA.
* We propose the key MI-based regularization that can explicitly disentangle the representations of point cloud content and distortion through MI minimization.
* We conduct comprehensive experiments on three datasets (SJTU-PCQA , WPC , LS-PCQA ), and achieve superior performance over the state-of-the-art methods on all of these datasets.

## 2 Related Work

No-Reference Point Cloud Quality Assessment.NR-PCQA aims to evaluate the perceptual quality of distorted point clouds without available references. According to the modalities, the NR-PCQA methods can be categorized into three types: projection-based, point-based and multi-modal methods. For the projection-based methods, various learning-based networks [21; 47; 52; 33; 34] adopt multi-view projection for feature extraction, while Zhang _et al._ integrates the projected images into a video to conveniently utilize video quality assessment methods to evaluate the perceptual quality. Xie _et al._ first computes four types of projected images (_i.e._, texture, normal, depth and roughness) and fuses their latent features using a graph-based network. For the point-based methods, Zhang _et al._ extracts carefully designed hand-crafted features, while Liu _et al._ transforms point clouds into voxels and utilizes 3D sparse convolution to learn the quality representations. Some 3D native methods [37; 32; 39] divide point clouds into local patches and utilize hierarchical networks structurally like PointNet++  to learn the representations. For the multi-modal methods, Zhang _et al._ utilizes individual 2D and 3D encoders to separately extract features, and fuse them using a symmetric attention module. Other works [38; 3; 22] leverage various cross-modal interaction mechanisms to enhance the fusion between 2D and 3D modalities. Compared to previous methods that learn quality representations indiscriminately, our work solves quality representation disentanglement from a more essential perspective of mutual information, which reveals the intrinsic correlations between point cloud content and distortion pattern.

Representation Learning for Image/Video Quality Assessment.As for image quality assessment (IQA), CONTRIQUE  learns distortion-related information on images with synthetic and realistic distortions based on contrastive learning. Re-IQA  trains two separate encoders to learn high-level content and low-level image quality features through an improved contrastive paradigm. QPT  also learns quality-aware representations through contrastive learning, where the patches from the same image are treated as positive samples, while the negative sample are categorized into content-wise and distortion-wise samples to contribute distinctly to the contrastive loss. QPTv2  is based on masked image modeling (MIM), which learns both quality-aware and aesthetics-aware representations through performing the MIM that considers degradation patterns.

As for VQA, CSPT  learns useful feature representation by using distorted video samples not only to formulate content-aware distorted instance contrasting but also to constitute an extra self-supervision signal for the distortion prediction task. DisCoVQA  models both temporal distortions and content-related temporal quality attention via transformer-based architectures. Ada-DQA  considers video distribution diversity and employ diverse pretrained models to benefit quality representation. DOVER  divides and conquers aesthetic-related and technical-related (distortion-related) perspectives in videos, introduces inductive biases for each perspective, including specific inputs, regularization strategies, and pretraining. However, there is no current work to utilize mutual information (MI) to achieve representation disentanglement, which has not been explored in IQA/VQA.

Mutual Information Estimation.Mutual information (MI) has been widely used as regularizers or objectives to constrain independence between variables [2; 7; 13; 14]. Hjelm _et al._ performs unsupervised representation learning by maximizing MI between the input and output of a deep neural network. Kim _et al._ learns disentangled representations by encouraging the distribution of representations to be factorial and hence independent across the dimensions. Moreover, MI minimization has been drawing increasing attention in disentangled representation learning [6; 15; 55]. Chen _et al._ introduces a contrastive log-ratio upper bound for mutual information estimation, and extends the estimator to a variational version for general scenarios when only samples of the joint distribution are obtainable. Dunion _et al._ minimizes the conditional mutual information between representations to improve generalization abilities under correlation shifts and enhances training performance in scenarios with correlated features. However, to our knowledge, there has been no previous work focusing on learning disentangled representations or exploring MI estimation for visual quality assessment.

## 3 Mutual Information Estimation and Minimization

Given the content-aware and distortion-aware representations \((,)\), our goal is to estimate the MI between \(\) and \(\) and further minimize it. In this section, we explain the mathematical background of how to leverage a neural network to estimate the MI between \(\) and \(\).

The MI between \(\) and \(\) can be defined as:

\[(;)& = p(,),)} {p()p()}dd\\ &=_{p(,)}[,)}{p()p()}]\] (1)

where \(p(,)\) is the joint distribution, \(p()\) and \(p()\) are the marginal distributions.

Unfortunately, the exact computation of MI between high-dimensional representations is actually intractable. Therefore, inspired by [6; 8; 15], we focus on estimating the MI upper bound and further minimize it. The tight upper bound of mutual information (MI) means an upper boundary that is always higher the actual value of MI. A tight upper bound means the bound is close to the actual value of MI and equal to MI under certain conditions. An MI upper bound estimator \(}(;)\) can be formulated as (proof in Appendix A):

\[}(;):=_{p(, )}[ p(|)]-_{p()}_{p( )}[ p(|)]\] (2)

Since the conditional distribution \(p(|)\) is unavailable in our case, we approximate it using a variational distribution \(q_{}(|)=_{}(,)\), where the conditional distribution is inferred by another light neural network \(_{}\) with parameters \(\). Then the variational form \(}_{}(;)\) can be formulated as (in a discretized form):

\[}_{}(;)=}_{i=1 }^{N}_{j=1}^{N}[ q_{}(_{i}|_{i} )- q_{}(_{j}|_{i})]\] (3)

where \(\{(_{i},_{i})\}_{i=1}^{N}\) is \(N\) samples pairs drawn from the joint distribution \(p(,)\). To make \(}_{}(;)\) a tight MI upper bound, \(_{}\) is trained to accurately approximate \(p(|)\) by minimizing the KL divergence between \(p(|)\) and \(q_{}(|)\):

\[_{}(p(|)\|q_{}( |))\] \[= _{}_{p(,)}[ p (|)]}_{}_{p(, )}[ q_{}(|)]}_{}\] (4)

Obviously, the first term in Equation 4 has no relation with \(\), thus Equation 4 equals minimization of the second term. Therefore, the can be a tight MI upper bound if we minimize the following negative log-likelihood of the inferred conditional distribution:

\[_{}=-_{i=1}^{N} q_{}(_{i} |_{i})=-_{i=1}^{N}_{}(_{ i},_{i})\] (5)

Now, given the representations \(\) and \(\), we can train the MI estimator \(}_{}(;)\) to predict the MI between \(\) and \(\) by minimizing \(_{}\). Afterwards, we minimize \(}_{}(;)\) for explicit disentanglement, detailed implementations will be explained in Section 4.4.

## 4 Proposed Framework

### Overall Architecture

The aforementioned analysis in Section 1 reveals that HVS processes high-level and low-level features in a relatively separate manner. To obey this mechanism, as illustrated in Figure 2 (a), the architecture of DisPA is divided into two branches to learn content-aware and distortion-aware representations, respectively. Given a distorted point cloud \(P\), we first render it into multi-view images \(I\). The multi-view images are fed into a frozen pretrained vision transformer (ViT) \(\) with parameters \(_{f}\) to generate the content-aware representation \(\). Next, the multi-view images are decomposed into a mini-patch map \(M\) through grid mini-patch sampling. The mini-patch map is encoded by the distortion-aware encoder \(\) (a Swin Transformer) ) with parameters \(_{g}\) to obtain representation \(\). After obtaining \(\) and \(\), we also use them to train the MI estimator \(\) and obtain the estimated MI \(}_{}(;)\) following the process in Section 3. Finally, we concatenate \(\) and \(\) (denoted as \([,]\)) and regress it by fully-connected layers \(\) with parameters \(_{h}\) to predict quality score \(\). The whole process can be described as follows:

\[ =([(I;_{f}),(M;_{g} )];_{h})\] (6a) \[}_{}(;) =((I;_{f}),(M;_{g}))\] (6b)

### Content-Aware Pretraining via Masked Autoencoding

As analyzed in Section 1, the learning difficulty of content representation is more intractable than distortion due to the limited dataset scale in terms of point cloud content. To address this problem, we pretrain the content-aware encoder \(\) via the proposed masked autoencoding strategy. As illustrated in Figure 2 (b), given a distorted point cloud \(P\) and its corresponding reference point cloud \(P_{}\), our goal is to render \(P\) and \(P_{}\) into multi-view images \(\{I^{(n)}^{H W 3}\}_{n=1}^{N_{v}}\), \(\{I^{(n)}_{}^{H W 3}\}_{n=1}^{N_{v}}\) and pretrain \(\) by using partially masked \(I^{(n)}\) to reconstruct \(I^{(n)}_{}\), where \(N_{v}\) is the number of views.

Multi-View Rendering.Instead of directly performing masked autoencoding in 3D space, we render point clouds into 2D images to achieve pixel-to-pixel correspondence between the rendered images of \(P\) and \(P_{}\), which facilitates the computation of pixel-wise reconstruction loss between the predicted patches and the ground truth patches. To perform the rendering, we translate \(P\) (or \(P_{}\)) to the origin and geometrically normalize it to the unit sphere to achieve a consistent spatial scale. Then, to fully capture the quality information of 3D point clouds, we apply random rotations before rendering \(P\), \(P_{}\) into \(\{I^{(n)}\}_{n=1}^{N_{v}}\) and \(\{I^{(n)}_{}\}_{n=1}^{N_{v}}\).

Patchifying and Masking.After obtaining the rendered image \(I^{(n)}\), we partition it into non-overlapping \(16 16\) patches following . Then we randomly sample a subset of patches and mask the remaining ones, where the masking ratio is relaxed to 50% instead of the high ratio in  (_e.g.,

Figure 2: Architecture of proposed DisPA (a). Our DisPA consists of two encoders \(\) and \(\) for learning content-aware and distortion-aware representations, and an MI estimator \(\). The content-aware encoder \(\) is pretrained using masked autoencoding (b). ”\(\)” denotes concatenation.

75% and even higher) because some point cloud samples in PCQA datasets exhibit severe distortions, necessitating more patches to extract effective content-aware information. In addition, the relatively low masking ratio can mitigate the influence of the background of \(I^{(n)}\).

#### Encoding and Reconstruction.

The unmasked patches of \(I^{(n)}\) are fed into the ViT \(\) for initial embedding and subsequent encoding to obtain the representation \(\). To compensate for the scarcity of point cloud content in PCQA datasets, we initialize the encoder using the parameters optimized on ImageNet-1K . Next, to reconstruct \(I^{(n)}_{}\), we feed \(\) into a decoder and reshape it into 2D pixels to generate the reconstructed \(^{(n)}_{}\). By reconstructing masked reference patches from unmasked distorted patches, the encoder \(\) is forced to focus more on semantic information than distortion patterns. The content-aware representation can be learned by the reconstruction loss \(_{}\):

\[_{}=_{n=1}^{N_{v}}\|^{(n)}_{}-I ^{(n)}_{}\|_{2}^{2}\] (7)

### Distortion-Aware Mini-patch Map Generation

To learn an effective distortion-aware representation \(\), we decompose the multi-view images \(\{I^{(n)}\}_{n=1}^{N_{v}}\) to a mini-patch map through grid mini-patch sampling, following [40; 42; 52]. As illustrated in Figure 3, the distortion pattern is well preserved and even more obvious on the mini-patch map while the content information is blurred. More concretely, for each multi-view image \(I^{(n)}\), we first split it into uniform \(L L\) grids, the set of grids \(G^{(n)}\) can be described as:

\[G^{(n)}=\{g^{(n)}_{0,0},,g^{(n)}_{i,j},,g^{(n)}_{L,L}\}, g^{ (n)}_{i,j}=I^{(n)}[:,:]\] (8)

where \(g^{(n)}_{i,j}^{ 3}\) denotes the grid in the \(i\)-th row and \(j\)-th column of \(I^{(n)}\). Then we sample the mini-patches from each \(g^{(n)}_{i,j}\) and splice all the selected mini-patches to get the mini-patch map \(M\). Note that blank mini-patches (_i.e._, image background) are ignored, and the map is ensured to \(M^{H W 3}\) by filling in the unemployed mini-patches. After the mini-patch map generation, we feed it into the distortion-aware encoder \(\) to generate the corresponding representation \(\).

### Disentangled Representation Learning

MI-based Regularization.After obtaining content-aware and distortion-aware representations \(\) and \(\), we further disentangle them by minimizing the MI upper bound in \(}_{}(;)\) in Equation 3. As revealed in Equation 4 and 5, the key to accurately estimate a tight \(}_{}(;)\) is to minimize the negative log-likelihood of the variational network \(Q_{}(,)\). Here we implement the \(_{}\) using MLPs, and model the variational distribution as an isotropic Gaussian parameterized by a mean value \(_{}=[_{}(x_{1}),...,_{}(x_{D})]\) and a diagonal covariance matrix \(=_{}^{2}\), where \(_{}=[_{}(x_{1}),...,_{}(x_{D})]\), \(D\) is the feature dimension of \(\) and \(\). Then the variational distribution can be inferred as:

\[q_{}(|)=_{}(,)= _{d=1}^{D}_{}^{2}\;(x_{d})}}\{ --_{}\;(x_{d}))^{2}}{2_{}^{2}\;(x_{d})}\}\] (9)

where \(_{}\) and \(_{}^{2}\) are obtained via the last two MLP layers. \(\) is optimized by minimizing \(_{}\) in Equation 5, the negative log-likelihood of \(_{}(,)\). It is noted that the parameters of \(\) are optimized independently with the main networks \(_{f}\) and \(_{g}\), seeing Algorithm 1.

Loss Function.After obtaining the \(}_{}(;)\), we incorporate it into our total training objective as a regularizer to disentangle the content-aware and distortion-aware representations, the total training

Figure 3: Illustration of mini-patch map generation.

loss function \(\) can be formulated as:

\[=_{b=1}^{B}(_{b}-q_{b})^{2}+_{1}_{}+_{2}_{}(;)\] (10)

where \(B\) is the batch size and \(_{}\) is a differential ranking loss following . The \(_{1}\) and \(_{2}\) are weighting factors to balance each loss term. To better recognize quality differences for the point clouds with close MOSs, the differential ranking loss \(_{}\) is used to model the ranking relationship between \(\) and \(q\):

\[_{rank}=}_{i=1}^{B} _{j=1}^{B}&(0,|q_{i}-q_{j}|-e(q_{i},q_{j}) (_{i}-_{j}))\,,\\ e(q_{i},q_{j})=\{1,q_{i} q _{j}\\ -1,q_{i}<q_{j}.\] (11)

Algorithm 1 summarizes the overall pipeline of the disentangled representation learning framework (one iteration), where \(N_{}\) is the steps of updating for variational networks per epoch, and \(\) is initialized with the pretrained parameters \(_{f}\) after masked autoencoding. The parameters of the main network \(_{g},_{h}\) and the variational networks \(\) are updated alternately.

```
0: A batch of rendered images \(\{I_{b}^{(n)}|N_{v}\}_{b=1}^{B}\); mini-patch map \(\{M_{b}\}_{b=1}^{B}\); networks \(,,\) with parameters \(^{}_{f},_{g},_{h}\); MI estimator \(\) with variational network \(_{}\); optimizer; \(_{1},_{2}\)
0: Updated parameters \(^{}_{g},^{}_{h}\) and \(^{}\)//Parameters \(^{}_{f}\) is frozen
1: Encode rendered images to generate content-aware representation \((\{I_{b}^{(n)}\});^{}_{f})\)
2: Encode mini-patch map to generate distortion-aware representation \((\{M_{b}\};_{g})\)
3:for\(m=1 N_{}\)do
4: Compute negative log-likelihood \(_{}_{b=1}^{B}_{}(, )\)
5: Update \(\) by minimizing \(_{}\)\(^{}\)optimizer (\(,_{}_{}\))
6: Compute the estimated MI \(_{}(;)}_{i=1}^{ B}_{j=1}^{B}[_{^{}}(_{i},_{i})- _{^{}}(_{i},_{j})]\)
7: Predict the quality scores \(_{b}([_{b},_{b}];_{h})\)
8: Compute the total loss \(_{b=1}^{B}(_{b}-q_{b})^{2}+_ {1}_{}+_{2}_{}(;)\)
9: Update the parameters \(\{^{}_{g},^{}_{h}\}\)optimizer(\(\{_{g},_{h}\},\{_{_{g}},_{ _{h}}\}\)) ```

**Algorithm 1** Disentangled Representation Learning Pipeline

## 5 Experiments

### Datasets and Evaluation Metrics

Datasets.Our experiments are performed on three popular PCQA datasets, including LS-PCQA , SJTU-PCQA , and WPC . The content-aware pretraining is based on LS-PCQA, which contains 24,024 distorted point clouds, and each reference point cloud is impaired with 33 types of distortions (_e.g._, V-PCC, G-PCC) under 7 levels. The disentangled representation learning is conducted on all three datasets separately using labeled data, where SJTU-PCQA includes 9 reference point clouds and 378 distorted samples impaired with 7 types of distortions (_e.g._, color noise, downsampling) under 6 levels, while WPC contains 20 reference point clouds and 740 distorted samples disturbed by 5 types of distortions (_e.g._, compression, gaussian noise).

Evaluation Metrics.Three widely adopted evaluation metrics are employed to quantify the level of agreement between predicted quality scores and ground truth (_i.e._, Mean Opinion Score, MOS): Spearman rank order correlation coefficient (SROCC), Pearson linear correlation coefficient (PLCC), and root mean square error (RMSE). To ensure consistency between the value ranges of the predicted scores and subjective values, nonlinear Logistic-4 regression is used to align their ranges.

[MISSING_PAGE_FAIL:8]

\(1.3\%\) on SJTU-PCQA. Note that CoPA has also been pretrained on LS-PCQA. Furthermore, DisPA significantly reduces RMSE by \(4.2\%\) compared to CoPA.

Statistical Analysis.We perform the statistical analysis on SJTU-PCQA in Figure 4 for DisPA. Compared with the statistics of PQA-Net and GPA-Net in Figure 1, our DisPA not only predicts quality scores more accurately, but also obviously predicts closer score spans when point cloud content varies, even when the distortion intensity is at the highest level. The statistical analysis demonstrates that the content-aware pretraining strategy can effectively address the problem of superior difficulty of learning representations for point cloud content caused by data imbalance.

Qualitative Evaluation.In Figure 5, we present examples of SJTU-PCQA and WPC with predicted scores of PQA-Net  and CoPA , where Figure 5 (a)(b), (e)(f) share the same content, (b)-(d), (f)-(h) share the the same distortion. Note that each score is predicted on the testing set of 5-fold validation. We can see that the predicted score of our DisPA is obviously closer to the ground truth (_i.e._, Mean Opinion Score, MOS) and dose not deviate from the MOS when content varies, which further validates the effectiveness of content-aware pretraining and representation disentanglement.

Cross-Dataset Validation.To test the generalization capability of NR-PCQA methods when encountering various data distribution, we perform cross-dataset on LS-PCQA , SJTU-PCQA  and WPC . In Table 2, we mainly train the compared models on the complete LS-PCQA and test the trained model on the complete SJTU-PCQA and WPC, and the result with minimal training loss is recorded. This procedure is repeated for mutual cross-dataset validation between SJTU-PCQA and WPC. From Table 2, we can see that the performance of the cross-dataset validation is relatively low due to the tremendous variation of data distribution. However, our method still present competitive performances, demonstrating the superior generalizability of DisPA.

Figure 4: Statistical Analysis of SJTU-PCQA (part) and predicted quality scores of DisPA.

Figure 5: Qualitative Evaluation of NR-PCQA methods (PQA-Net , CoPA  and DisPA) on SJTU-PCQA  and WPC . Figure (b)-(d) share the same distortion pattern (_i.e._, color noise), same for (f)-(h) (_i.e._, downsampling). ”GT” denotes ground truth.

### Ablation Study

We conduct ablation study of DisPA on SJTU-PCQA  in Table 3. From Table 3, we have following observations: 1) Seeing 1 and 2, the pretraining strategy effectively improves the performance of DisPA. 2) Seeing 1, 2 and 4, the philosophy of representation disentanglement brings significant improvements to our model, because using simple cosine similarity in 4 for disentanglement can achieve fair performance. However, using MI for disentanglement can better constrain the dependence between representations. 3) Seeing 1, 3 and 6, using single branch to infer quality scores causes poor performance, since PCQA is a combination judgement based on the interaction of distortion estimation and content recognition. 4) Seeing 1 and 7, the performance is close, demonstrating the robustness of our model using different training loss functions.

Furthermore, as shown in Figure 6, we conduct a t-SNE visualization to compare the representation embeddings of PQA-Net, GPA-Net, content-aware and distortion-aware branches of our DisPA on the testing set of SJTU-PCQA. PQA-Net and GPA-Net are selected for comparison because these two methods both use distortion type prediction to learn distortion-aware representations. The scattered points are color and shape marked according to distortion type and content. The distortion-aware features are visualized in 3rd sub-image, where we can see that the learned distortion-aware representation shows clear and separate clustering for different distortion types, indicating a strong correlation with degradations. The content-aware features present non-clustering for distortion types but a clear boundary to split the content.

## 6 Conclusion

In this paper, we propose a disentangled representation learning framework (DisPA) for No-Reference Point Cloud Quality Assessment (NR-PCQA) based on mutual information (MI) minimization. As for the MI minimization, we use a variational network to infer the upper bound of the MI and further minimize it to achieve explicit representation disentanglement. In addition, to tackle the nontrivial learning difficulty of content-aware representations, we propose a novel content-aware pretraining strategy to enable the encoder to capture effective semantic information from distorted point clouds. Furthermore, to learn effective distortion-aware representations, we decompose the rendered images into mini-patch maps, which can preserve original distortion pattern and make the encoder ignore the global content. We demonstrate the high performance of DisPA on three popular PCQA benchmarks and validate the generalizability compared with multiple NR-PCQA models.

Acknowledgments.This paper is supported in part by National Natural Science Foundation of China (62371290, U20A20185), the Fundamental Research Funds for the Central Universities of China, and 111 project (BP0719010).

   Train & Test & PQA-Net & GPA-Net & MM-PCQA & CoPA & **DisPA** & & & \\  LS & SJTU & 0.842 & 0.586 & 0.581 & 0.644 & **0.653** & & & \\ LS & WFC & 0.265 & 0.433 & 0.454 & **0.516** & 0.550 & & & \\ WFC & SJTU & 0.235 & 0.533 & 0.612 & 0.643 & **0.657** & & & \\ SJTU & WFC & 0.220 & 0.418 & 0.269 & 0.533 & **0.538** & & & \\   

Table 2: Cross-dataset validation on LS-PCQA , SJTU-PCQA  and WPC  (complete set). The best results (PLCC) are in **bold**, and the second results are underlined.

Figure 6: Visualization of t-SNE embeddings of representations of PQA-Net, GPA-Net, content-aware and distortion-aware branches of our DisPA.