# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

them. Similarly, when removing a plane from an image of an airport field (Figure 1, fourth row), a model might insert another plane due to the presence of other planes in the background. Alternatively, a model might introduce shoes on the floor (Figure 1, first row) when the user's intention was to fill the space with background elements. In this paper, to address this issue, we propose a novel approach called CLIPAway that leverages AlphaCLIP  embeddings to distinguish between foreground (the object to be removed) and background regions. Our method aims to identify an embedding for the inpainted region that focuses on the background while excluding the foreground content, thereby enhancing the quality and accuracy of the inpainting process.

Recent advancements in diffusion-based inpainting for object removal have yielded notable works such as InstInpaint , MagicBrush , InstructPix2Pix , and concurrently ObjectDrop . These methods introduce training datasets with varying approaches: Some generate targets through existing inpainting methods , resulting in imperfect targets, while others rely on synthetically generated pairs  that may contain annotation errors. Certain methods resort to costly manual annotations  or require extensive data collection setups where images of scenes before and after object removal are captured . Furthermore, these techniques typically involve either training a model from scratch or fine-tuning existing models specifically for the removal task. In contrast, our model, CLIPAway, distinguishes itself by its lack of dependency on a specialized training set. It offers a plug-and-play solution compatible with various diffusion-based inpainting methods, ensuring seamless object removal without the need for costly or complex data preparation.

Our contributions can be summarized as follows:

* We introduce CLIPAway, a method that utilizes AlphaCLIP embeddings to effectively differentiate between foreground and background regions for superior object removal.
* Our approach offers a simple plug-and-play solution that does not require specialized training datasets, making it adaptable to various diffusion-based inpainting methods.
* By focusing on background regions, CLIPAway significantly improves the quality and accuracy of inpainting results, avoiding the common issue of object hallucination.
* We provide comprehensive evaluations on a standard dataset, demonstrating consistent improvements over state-of-the-art methods.

Figure 1: **Diffusion-based inpainting methods often struggle with object removal tasks.** Instead of seamlessly filling the erased area with background elements, diffusion models may unintentionally replace the removed object with another or add irrelevant objects. This outcome diverges from the user’s intention, which is typically to restore the area with the background alone, without introducing new elements. Our method, CLIPAway, aims at amending this deficiency by precisely focusing on maintaining the integrity of the background, ensuring that the space is filled as intended by the user.

Related Work

Image inpainting involves replacing missing pixels in an image with new ones that blend seamlessly with the surrounding content. Historically, Generative Adversarial Networks (GANs) have dominated this field where they demonstrated significant success across various image domains [21; 17; 33; 12; 16; 32]. However, GAN-based models are typically trained separately for specific image domains, such as face inpainting using training datasets like FFHQ  or scenery inpainting using datasets like Places . This domain-specific training restricts their ability to generalize to diverse scenes, thereby limiting their versatility.

Recently, diffusion-based models have made strides, showing promising results [18; 24]. For instance, the Repair model  employs a pretrained unconditional diffusion model to perform image inpainting by conditioning the generation on the unerased parts of the image. Despite its effectiveness, Repair operates on the image space, thus computationally demanding and slow. Alternative approaches that work in the latent space, e.g. SD-Inpaint  and Blended Latent Diffusion , adapt the Stable Diffusion (SD) model by adding a mask channel to the latent inputs. These methods, however, often introduce new objects into the scene based on context rather than removing existing ones, conflicting with the user's intention of background restoration. Other diffusion-based methods, such as GLIDE  and SmartBrush , are designed to add objects rather than remove them.

There has been growing interest in instruction-based inpainting methods for object removal, such as Instruct-Pix2Pix  and Inst-Inpaint , which use prompts instead of masks for object removal. These methods require datasets specifically tailored for this task. For example, Instruct-Pix2Pix generates paired datasets using the GPT-3 language model  and the text-to-image Stable Diffusion model , incorporating prompt-to-prompt techniques . While capable of object removal, Instruct-Pix2Pix performs this task with limited precision, possibly due to the synthetic data's lack of diversity or inaccurate annotations. Inst-Inpaint, on the other hand, is trained with paired data where targets are inpainted images generated with GAN-based models. Hence, it inherits the artifacts of these GAN models.

Other works have relied on manual annotation efforts , or extensive data collection setups involving scenes captured with and without the object . However, the high cost of manual annotations limits the scale of these datasets. In contrast, our method, CLIPAway, sets itself apart by eliminating the need for specialized training sets. It offers a flexible, plug-and-play solution compatible with various diffusion-based inpainting methods, ensuring seamless object removal without the necessity for costly or complex data preparation.

## 3 Method

### Preliminaries

Our framework leverages pretrained diffusion models, particularly the latent diffusion model , chosen for its computational efficiency. This model includes an encoder (\(E\)) and a decoder (\(D\)). The encoder compresses images into a lower-dimensional latent space while the decoder reconstructs images from these latent codes. These components function similarly to a variational autoencoder and are trained separately from the diffusion process.

The diffusion process, as described by , operates on latent codes, denoted as \(z_{0}=E(x)\), where \(x\) is the input image. Noise is gradually added to \(z_{0}\) over a series of time steps \(t\) until, after \(T\) steps, \(z_{T}\) approximates a normal distribution with zero mean and an identity covariance matrix.

Diffusion models act as denoising autoencoders, trained to reverse the noise addition process. They aim to predict a denoised version of their input, \(z_{t}\), where \(z_{t}\) is a noisy version of \(z_{0}\). The objective function for this denoising task on the latent codes is defined as follows:

\[_{LDM}:=_{E(x), N(0,1),t}[\|- _{}(z_{t},t)\|]\] (1)

Here, \(t\) is sampled from the range \(1\) to \(T\), and \(_{}(z_{t},t)\) represents a neural network, specifically a UNet that predicts the noise added to \(z_{t}\), conditioned on the time step \(t\). We specifically employ models fine-tuned for inpainting tasks. These methods involve adding a single-channel mask, which is downsampled to fit the latent space, to the denoising UNet. The diffusion models are commonly trained using text-image pairs, where the text information is extracted from a frozen CLIP text encoder . This encoded text data is then integrated into the UNet via attention layers.

### CLIPAway

Our objective is to seamlessly remove objects while maintaining the integrity of the background. Unlike conventional inpainting methods that ignore the pixels from the erased area, our approach utilizes these pixels to guide the model on what not to fill in. This distinguishes our method significantly from others. To achieve this, we exploit the detailed pixel-level information available from both the regions to be erased and the unerased regions of the image. While popular Stable-Diffusion models typically rely solely on text conditioning, we explore conditioning the inpainting process on embeddings derived from image pixels.

Recent advancements have introduced additional control signals via adapters, addressing the limitations of text in fully expressing desired outcomes. In some cases, edge maps, poses, or reference images are necessary to effectively control the generation process [35; 36; 19]. The CLIP model utilized for text embedding is originally trained with a contrastive objective jointly with a CLIP image encoder. Adapters have demonstrated that rather than training an image encoder from scratch for reference image-based control, the existing CLIP image encoder can be utilized. UniControl  and T2IAdapter  extract features from the CLIP image encoder, map them to new features via a trainable network, and concatenate them with text features. These merged features are then fed into the UNet of the diffusion model to guide the image generation process. IP-Adapter  further shows that instead of merging image and text features in the cross-attention layer, the features can pass through a small trainable projection network, which are then fed into the UNet via a decoupled attention layer. Our implementation is based on IP-Adapter but can be used with others as well [36; 19].

Our goal is to achieve inpainting by focusing on the background. However, directly using the IP-Adapter with the input image proves ineffective. Using the entire image as the reference (prompt image), the method predictably fills the erased area with the original object (Figure 2, left pane). Conversely, erasing the input image results in black pixels in the masked area, leading to black artifacts in the filled regions (Figure 2, middle pane). Lastly, providing a text prompt as "background", also does not help in removing the object (Figure 2, right pane). Therefore, we need an embedding that solely focuses on the background. To address this, we explore AlphaCLIP , which achieves region focus without altering the original image by incorporating regions of interest through an additional alpha channel input. Although it is initialized with the CLIP  model, its training requires a substantial set of region-text paired data. By utilizing the Segment Anything Model (SAM)  and multimodal large model BLIP-2  for image captioning, millions of region-text pairs are generated. AlphaCLIP model is pretrained on a mixture of region-text pairs and image-text pairs. Their dataset has not been released; fortunately, our method does not require a specialized training set. Instead, we leverage existing models and techniques to achieve our inpainting objectives.

We train a Multi-Layer Perceptron (MLP) model to adapt the publicly released AlphaCLIP image encoder (CLIP-L/14) to the CLIP image encoder used in the IP-Adapter (OpenCLIP ViT-H/14). The MLP consists of six blocks, each containing a linear layer, layer normalization, and GELU activation. It begins with 768 features and outputs 1024 features, matching the output dimensions of the CLIP-L/14 encoder and the OpenCLIP ViT-H/14 encoder, respectively. For this training, we use

Figure 2: **Limitations of IP-Adapter  for Inpainting**. Direct use of the IP-Adapter with the input image as the image prompt is ineffective for inpainting, as it predictably fills the erased area with the original object. In addition, directly giving the prompt “_background_” is also problematic as the background can also contain instances of the images that we want to remove, resulting in a direct replacement of the foreground object. On the other hand, using an erased image as the prompt results in the generation of black artifacts.

the COCO image dataset with the alpha channel set to all 1s, corresponding to the full image rather than focusing on a specific region for AlphaCLIP. This setup aligns with AlphaCLIP training, where the authors occasionally set alpha channels to all 1s to indicate full images and sometimes to local regions. In our training, the target is the OpenCLIP embeddings for a given image. This allows us to train a projection layer so that AlphaCLIP outputs features that the rest of the IP-Adapter expects. This part is shown in Figure 3 (Training). We show that AlphaCLIP can be aligned with other CLIP Image encoders without a special dataset.

One of the promises of AlphaCLIP is its ability to focus on a specific region while maintaining contextual awareness. For example, given an image and mask pointing to the background, it may primarily focus on the background while still encoding the foreground, albeit with reduced emphasis. This behavior is illustrated in Figure 4, where we use image prompts as input images with alpha channels corresponding to the mask for foreground focus and the inverse of the mask for background focus. The results shown incorporate our projection layer, which bridges the AlphaCLIP and IP-Adapter. The first row displays the conditional image generations, while the second row shows the inpainting results. When the foreground is focused, the foreground object appears more prominent. Conversely, when the background is focused, the foreground object is present but receives less attention. Therefore, even when the background is focused, the inpaintings still include the object one aims to remove. To remove the foreground overall, we propose to subtract the foreground embedding from the background via projection.

Given two vectors \(}\) (background focused embedding) and \(}\) (foreground focused embedding), the final embedding \(}\) can be calculated as the equation below:

\[}=}-(}}}{\|}\|})}}{\|}\|}\] (2)

where \(}}\) is the dot product of \(}\) and \(}\), and \(\|}\|\) is the norm of \(}\). With this vector arithmetic, we find the final embedding that is orthogonal to the foreground embedding. After performing this subtraction, the embedding process predominantly focuses on the background, as illustrated in Figure 4 in our results. This tendency is evident in conditional image generation, where the resulting image predominantly exhibits the background style. Consequently, this translates into consistent background filling in the inpainting task for erased areas.

Figure 3: **The overall framework of CLIPaway.** Input images, comprising both foreground and background elements, are embedded via AlphaCLIP. These embedded images are then processed through an MLP trained to adapt features to the IPAdapter input space. Through vector arithmetic on the features, a background embedding without foreground influence is achieved. SDInpaint is depicted as if it is working on the image space for clarity; it works on the latent space.

The overall framework is depicted in Figure 3 (Inference). Input images containing both foreground and background elements are embedded via AlphaCLIP. These embedded images are then fed into the MLP, which we trained to adapt the features to the IP-Adapter input space. By performing vector arithmetic on the features, we achieve a background embedding without the influence of the foreground. Notably, this method does not necessitate an object removal dataset and can be readily utilized as a plug-and-play feature.

## 4 Experiments

### Baselines

We compare our method with state-of-the-art GAN-based and diffusion-based inpainting methods. The GAN based methods include ZITS++ , MAT , and LaMa  models, whereas diffusion based models include Blended Latent Diffusion , Unipaint , SD-Inpaint . We use the models released by the authors that achieve the best scores.

For GAN-based models, we use the models that are trained on the Places2 dataset. For the diffusion models, we provide an empty prompt. We also experiment with providing prompts as "_background_", but that does not change the results. In our experiments with Unipaint combined with CLIPAway, the masking mechanism proposed in Unipaint is applied to the projected embeddings of our network and then fed into the UNet with the help of IP-Adapter. To demonstrate the flexibility of our method across different CLIP and AlphaCLIP embedding spaces, we also present comparative analyses of our approach when the projection step is applied both before and after the MLP layer. This comparison

Figure 4: Starting with an input image and mask, we present our findings utilizing both foreground and background-focused embeddings. The images in the first row depict the conditional image generation outcomes of the stable-diffusion model without the inpainting task. These visuals offer insights into the focus of the embeddings. While both embeddings capture features from various parts of the image, the foreground embedding tends to emphasize the foreground, whereas the background embedding predominantly focuses on the background but still contains the foreground. Our approach successfully removes the foreground in the generated results, yielding pure background. This outcome is consistent with the image inpainting outputs, as demonstrated in the second row.

employs three distinct AlphaCLIP backbones--ViT-L/14, ViT-L/14@336px, and ViT-B/16--by training an MLP layer tailored to each evaluated backbone.

### Datasets

We evaluate the models on the COCO 2017 validation dataset , which provides us with the collection of images indoor and outdoor and instance level annotations. For each image in the validation set, we set the masks that correspond to object instances, excluding stuff categories. We dilate the masks with a kernel size of \(5\) as sometimes the pixels from an object remain in the image and result in artifacts for all models.

### Metrics

We report the Frechet Inception Distance (FID) , Kernel Inception Distance (KID)  and the CLIP Maximum Mean Discrepancy (CMMD) metrics  to assess the photorealism of the generated images by comparing the source image distribution with the inpainted image distributions. However, these metrics do not evaluate if the object is correctly removed.

To measure the accuracy of correct object removal, we use the CLIP metrics proposed by Inst-Inpaint , namely CLIP Distance and CLIP Accuracy. The goal of CLIP Distance is to evaluate how well the target object is removed. We extract the image regions indicated by bounding boxes from both the source and the inpainted images, then estimate the CLIP similarities  between these regions. We expect a larger distance if the object is correctly removed. For CLIP Accuracy, we utilize CLIP as a zero-shot classifier. We identify the most likely semantic label of the image region extracted from the source image using the object's bounding box, considering the object classes in our dataset. Next, we perform another prediction for the image region extracted from the inpainted image. We expect the class prediction to change after performing the object removal operation. If the predicted class based on the source image is not in the Top-1, Top-3, or Top-5 predictions of the inpainted image, it is considered a success. We report the percentage of successes.

We also conduct a user study with \(20\) participants on the first \(20\) samples of the validation set. We include ZITS++, Unipaint, SD-Inpaint, and CLIPAway to provide a range of the best-performing models. Participants are asked to evaluate whether the object is correctly removed and to assess the quality of the inpainting, and then select the best result among all choices. Details of the study are given in the Appendix.

### Results

We provide quantitative and qualitative comparisons of our method with state-of-the-art inpainting models in Table 1 and Figure 6, respectively. Our approach, when combined with various diffusion-based inpainting methods, demonstrates consistent improvements in both FID and CLIP metrics. Figure 5 illustrates that CLIPAway (with SD-Inpaint) achieves significantly better CLIP@3 and FID scores, positioning it in the top left corner, indicating more accurate and higher quality results compared to other methods. Additionally, our user study shows a strong preference for our approach over competing methods. Human subjects preferred our method 71.75% of the time, compared to 13.25% for ZITS++, 11.25% for SD-Inpaint, and 3.75% for Unipaint.

A few predictions from competing models are presented in Figure 6. GAN-based models, namely LaMa, MAT, and ZITS++, do not exhibit issues with object insertion. However, this problem is evident in the outputs of diffusion-based methods. This may be because diffusion models are more powerful and better at modeling data distribution, leading them to generate objects that more closely match the real distribution. Although GAN-based models avoid object insertion object insertion, they fail to

Figure 5: Comparison of CLIPAway with state-of-the-art methods based on image quality and inpainting accuracy.

generate realistic backgrounds. Our method, CLIPAway, is the only one that effectively removes objects and fills the regions realistically. For example, in the first row, GAN-based models fill the inpainted area in a blurry way, and diffusion models insert a person. In contrast, our method removes the person and fills the area seamlessly. In the third example, our approach is the only one that realistically fills the kitchen background. Similarly, in the eighth example, while GAN-based models extend the object in a blurry way and diffusion models add unrelated content, our method provides a sharp and realistic result.

Table 2 illustrates the adaptability of our approach across various CLIP and AlphaCLIP embedding spaces. Our method is not constrained to the CLIP embedding spaces employed in our initial experiments; with different backbones, the results consistently enhance the performance of the SD-Inpaint model across diverse CLIP embeddings. Additionally, our projection method is applicable beyond the OpenCLIP embedding space. Since AlphaCLIP's vision transformer is trained with objectives similar to those of the CLIP vision transformer, the resulting feature spaces are conceptually aligned. Consequently, projections can be effectively performed in the AlphaCLIP embedding space or in other CLIP embedding spaces with comparable properties. To validate this, we evaluated the projection method on the AlphaCLIP feature space (projection on AlphaCLIP space followed by MLP) using the same experimental setup.

Figure 7 shows our method integrated with various diffusion-based inpainting techniques, highlighting the significant performance enhancement our module offers. In Figure 8, we present qualitative comparisons between our method and instruction-based diffusion models. Since our method requires a mask and these models require prompts, this is not a direct one-to-one comparison. The visual results reveal that Instruct-Pix2Pix  struggles to accurately remove an object, and Inst-Inpaint  produces blurry inpainted images due to its training with GAN-generated targets. It is important to note that these competing methods rely on specialized datasets and specific model training for this task. In contrast, our method does not necessitate a special dataset and can seamlessly integrate with existing diffusion models.

  
**Models** & **FID \(\)** & **KID \(\)** & **CMMD \(\)** & **CLIP Dist.\(\)** & **CLIP@1 \(\)** & **CLIP@3 \(\)** & **CLIP@5 \(\)** \\   \\  ZITS++  & 67.72 & 0.0208 & 0.74 & 0.66 & 76.15 & 61.31 & 52.91 \\ MAT  & 63.39 & 0.0278 & 0.93 & 0.76 & 80.62 & 65.85 & 60.10 \\ LaMa  & 65.76 & 0.0195 & 0.81 & 0.66 & 78.34 & 64.42 & 56.85 \\ SD-Inpaint + LaMa & 51.33 & 0.0117 & 0.45 & 0.75 & 72.29 & 57.61 & 50.01 \\  Blended Diff.  & 72.24 & 0.0362 & 0.89 & 0.85 & 85.69 & 75.01 & 69.34 \\ 
**+ CLIPAway** & 61.66(+0.58) & 0.0194(+0.0168) & 0.78(+0.11) & 0.83(+0.02) & 87.28(+0.59) & 78.87(+0.386) & 73.20(+0.386) \\  Unipaint  & 77.58 & 0.0360 & 0.98 & 0.78 & 85.38 & 74.48 & 67.44 \\ 
**+ CLIPAway** & 62.18(+15.40) & 0.0199(+0.0161) & 0.79(+0.19) & 0.84(+0.06) & 88.26(+2.88) & 78.65(+0.17) & 73.05(+0.61) \\  SD-Inpaint  & 59.21 & 0.0145 & 0.54 & 0.75 & 70.45 & 57.14 & 49.88 \\ 
**+ CLIPAway** & 57.32(+18.89) & 0.0108(+0.0037) & 0.53(+0.01) & 0.81(+0.06) & 84.82(+14.37) & 74.42(+12.86) & 67.76(+17.88) \\   

Table 1: **Evaluation results. Improvements of CLIPAway over the base models are given in parenthesis for each metric.**

  
**Models** & **FID \(\)** & **CMMD \(\)** & **CLIP Dist.\(\)** & **CLIP@1 \(\)** & **CLIP@3 \(\)** & **CLIP@5 \(\)** \\  SD-Inpaint  & 59.21 & 0.54 & 0.75 & 70.45 & 57.14 & 49.88 \\   \\  SD-Inpaint + CLIPAway (VIT L/14)  & 57.32 & 0.53 & 0.81 & 84.82 & 74.42 & 67.76 \\ SD-Inpaint + CLIPAway (VIT L/14@336px)  & 54.93 & 0.48 & 0.80 & 82.36 & 71.68 & 63.28 \\ SD-Inpaint + CLIPAway (VIT R-B/16)  & 55.31 & 0.48 & 0.78 & 83.57 & 72.44 & 63.81 \\   \\  SD-Inpaint + CLIPAway (VIT L/14)  & 56.15 & 0.42 & 0.86 & 85.31 & 74.26 & 68.58 \\ SD-Inpaint + CLIPAway (VIT L/14@336px)  & 54.46 & 0.36 & 0.82 & 82.13 & 70.02 & 63.28 \\ SD-Inpaint + CLIPAway (ViT-B/16)  & 54.99 & 0.41 & 0.84 & 85.08 & 74.79 & 68.35 \\   

Table 2: **Evaluation results. SD-Inpaint and SD-Inpaint + CLIPAway across different CLIP embedding configurations, with projections evaluated before and after MLP.**

## 5 Conclusion and Broader Impacts

In this paper, we introduced CLIPAway, a novel approach for object removal in images using diffusion-based inpainting methods. Specifically, CLIPAway addresses the common issue of diffusion models hallucinating removed objects by focusing embeddings on the background. Our method leverages AlphaCLIP embeddings to effectively distinguish between foreground and background regions, focusing on background restoration to achieve seamless and realistic inpainting. By eliminating

Figure 6: Diffusion models often replace the removed object or insert new content instead of simply removing it. GAN-based models avoid adding new objects but struggle to generate realistic backgrounds. Our method, CLIPAway, effectively removes objects and fills the regions with realistic background content.

Figure 7: Qualitative results of diffusion-based models and our method combined with them.

the dependency on specialized training datasets, CLIPAway provides a flexible, plug-and-play solution compatible with various diffusion-based inpainting techniques. Our extensive experiments demonstrated that CLIPAway significantly improves the quality and accuracy of inpainting compared to state-of-the-art methods.

**Broader Impacts.** Our framework has the potential to image restoration, editing, and completion. However, this technology also brings important ethical considerations. One potential misuse is in the alteration or falsification of visual content, leading to the creation of misleading or deceptive images. We do not endorse such activities and emphasize the necessity of establishing safeguards to ensure the ethical use of this technology.

**Limitations.** Our method is demonstrated using latent-based diffusion models. A limitation is that, despite the diffusion occurring in the latent space, these models are still slower than GAN-based methods and do not run real-time. Another limitation of our model is the degradation in performance when the preferred resolution is not used. For instance, when utilizing the SD-Inpaint pipeline 1, the expected resolution for inference without quality loss is 512\(\)512. If a different resolution is provided, performance degrades. While this issue is inherent to latent-based diffusion models rather than specific to our approach, it remains a present limitation. Additionally, our model, while removing objects, does not remove their shadows if they are not included in the mask. This can be seen in Figure 1 in the first two examples on the right, where shadows are handled as patterns by the model instead of being removed.