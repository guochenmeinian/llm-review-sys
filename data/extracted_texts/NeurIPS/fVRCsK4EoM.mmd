# PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference

Kendong Liu1, Zhiyu Zhu1\({}^{}\)2, Chuanhao Li2\({}^{}\)3, Hui Liu3, Huanqiang Zeng4, Junhui Hou1

Equal ContributionCorresponding Author.

###### Abstract

In this paper, we make the first attempt to align diffusion models for image inpainting with human aesthetic standards via a reinforcement learning framework, significantly improving the quality and visual appeal of inpainted images. Specifically, instead of directly measuring the divergence with paired images, we train a reward model with the dataset we construct, consisting of nearly 51,000 images annotated with human preferences. Then, we adopt a reinforcement learning process to fine-tune the distribution of a pre-trained diffusion model for image inpainting in the direction of higher reward. Moreover, we theoretically deduce the upper bound on the error of the reward model, which illustrates the potential confidence of reward estimation throughout the reinforcement alignment process, thereby facilitating accurate regularization. Extensive experiments on inpainting comparison and downstream tasks, such as image extension and 3D reconstruction, demonstrate the effectiveness of our approach, showing significant improvements in the alignment of inpainted images with human preference compared with state-of-the-art methods. This research not only advances the field of image inpainting but also provides a framework for incorporating human preference into the iterative refinement of generative models based on modeling reward accuracy, with broad implications for the design of visually driven AI applications. Our code and dataset are publicly available at https://prefpaint.github.io.

## 1 Introduction

Image inpainting, the process of filling in missing or damaged parts of images, is a critical task in computer vision with applications ranging from photo restoration  to content creation . Traditional approaches have leveraged various techniques, from simple interpolation  to complex texture synthesis , to achieve visually plausible results. The recent advent of deep learning, particularly diffusion models, has revolutionized the field by enabling more coherent and contextually appropriate inpaintings . Despite these advancements, a significant gap remains between the technical success of these models and their alignment with human aesthetic preferences, which are inherently subjective and intricate. As shown in Fig. 1, the existing stable diffusion-based inpainting model tends to generate weird and discord reconstruction.

Human preference for visual content is influenced by complex and mutual factors, including but not limited to personal background, experiences, and the context in which the content is viewed . This makes the task of aligning inpainting models with human preference particularly challenging, asit requires the model not only to understand the content of the missing parts but also to predict and adapt to the diverse tastes of its users.

Inspired by recent advancements in the reinforced alignment of large pre-trained models [15; 16; 17; 18; 19], we propose to align diffusion models for image inpainting with human preference through reinforcement learning. Specifically, our approach is grounded in the hypothesis that by incorporating human feedback into the training loop, we can guide the model toward generating inpainted images that are not only technically proficient but also visually appealing to users. Technically, we formulate the boundary of each reward prediction in terms of the model's alignment with human aesthetic preferences, thereby leveraging the accuracy of the reward model to amplify the regularization strength on more reliable samples. Extensive experiments validate that the proposed method can consistently reconstruct visually pleasing inpainting results and greatly surpass state-of-the-art methods.

In summary, the main contributions of this paper lie in:

* we make the _first attempt_ to align diffusion models for image inpainting with human preferences by integrating human feedback through reinforcement learning;
* we theoretically deduce the accuracy bound of the reward model, modulating the refinement process of the diffusion model for robustly improving both efficacy and efficiency; and
* we construct a dataset containing 51,000 inpainted images annotated with human preferences.

## 2 Related Work

Reinforcement Learning & Model Alignment.Reinforcement learning [20; 21; 22] is a paradigm of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. The foundational theory of reinforcement learning is rooted in the concepts of Markov decision processes , which provide a mathematical structure

Figure 1: Visual comparisons of the results by the diffusion-based image inpainting model named “_Runway_”, and the aligned model through the proposed method.

for modeling decision-making in environments with stochastic dynamics and rewards [24; 25]. With the advent of deep learning, deep reinforcement learning [21; 26] has significantly expanded the capabilities and applications of traditional reinforcement learning. The integration of neural networks with reinforcement learning, exemplified by the Deep Q-Network [27; 28] algorithm, has enabled the handling of high-dimensional state spaces, which were previously intractable. Subsequent innovations, including policy gradient methods [29; 30] like Proximal Policy Optimization  and actor-critic frameworks [32; 33] like Soft Actor-Critic , have further enhanced the efficiency and stability of learning in complex environments.

The recent surge in popularity of large-scale models has significantly underscored the importance of reinforcement learning in contemporary AI research and application [15; 16]. As these models, including large language models [35; 36] and deep generative networks [37; 38; 39], become more prevalent, reinforcement learning is increasingly employed to fine-tune, control, and optimize their behaviors in complex, dynamic environments. This integration is particularly visible in areas such as natural language processing, where reinforcement learning techniques are used to improve the conversational abilities of chat-bots and virtual assistants, making them more adaptive and responsive to user needs. [17; 18; 19] Moreover, in the realm of content recommendation and personalization, reinforcement learning algorithms are instrumental in managing the balance between exploration of new content and exploitation of known user preferences, significantly enhancing the user experience. The growing intersection between large models and reinforcement learning not only pushes the boundaries of what's achievable in AI but also amplifies the need for sophisticated reinforcement learning techniques that can operate at scale, adapt in real-time, and make decisions under uncertainty, thereby marking a pivotal evolution in how intelligent systems are developed and deployed.

Diffusion Model.Diffusion models have recently emerged as a powerful class of generative models [38; 39; 40; 41; 42], demonstrating remarkable success in generating high-quality, coherent images [43; 44]. These models work by gradually transforming a distribution of random noise into a distribution of images, effectively 'diffusing' the noise into structured patterns [45; 46]. In the context of image inpainting, diffusion models offer a significant advantage by leveraging their generative capabilities to predict and fill in missing parts of images in a way that is contextually and visually coherent with the surrounding image content [47; 48].

Recent studies have showcased the potential of diffusion models in achieving state-of-the-art results in image inpainting tasks, outperforming previous generative models like Generative Adversarial Networks in terms of image quality and coherence [49; 50]. However, while these models excel in technical performance, there remains a gap in their ability to cater to diverse human aesthetic preferences. Most existing works focus on the objective quality of inpainting results, such as fidelity to the original image and coherence of the generated content, with less attention given to subjective satisfaction or preference alignment.

## 3 Proposed Method

Due to the random masking within the task of image inpainting, there may not be a definitive causal relationship between the known and inpainted contents, which manifests as a one-to-many issue. Consequently, stringent per-pixel regularization inherently results in unnatural reconstruction, which significantly diverges from samples conforming to human preference. To this end, we propose a reinforcement learning-based alignment process involving human preferences to fine-tune pre-trained diffusion models for image inpainting, aiming to improve the visual quality of inpainted images (Sec. 3.1). More importantly, we theoretically deduce the upper bound on the error of the reward model (Sec. 3.2). Based on this deduction, we formulate a reward trustiness-aware alignment process that is more efficient and effective (Sec. 3.3).

### Reinforced Training of Diffusion Models for Image Inpainting

Diffusion models [38; 40] iteratively refine a randomly sampled standard Gaussian distributed noise, resulting in a generated image. To adjust the distribution of diffusion models, we introduce human feedback rewards to measure and regularize the distribution of model sampling outputs. To be specific, instead of applying standard policy-gradient descent  that is hard to converge to high-quality models, inspired by classical methods, e.g., TRPO/PPO [31; 52], which introduces a model trust region to avoid potential model collapse during the training process, we achieve the reinforced

[MISSING_PAGE_FAIL:4]

where \(=^{}+\) and \(\|\|_{^{-1}}:=\|^{}^{-1}\|_{2}\). Moreover, based on _Theorem 2_ of , for \(>0\), we have \(1-\) probability to make following inequality stands

\[\|^{}\|_{^{-1}} B()^{1/2}()^{-1/2}}{ })},\] (8)

where \(()\) computes the determinant of the matrix, and \(B\) is a scalar. We have

\[|^{}}-^{}_{*}|\| \|_{^{-1}}( )^{1/2}()^{-1/2}}{})}+^{1 /2}\|_{*}\|_{2})}_{C_{bound}}.\] (9)

Due to \(C_{bound}\) being constant after the training of the reward model, we can conclude that

\[_{z p(z)}|^{}}-^{}_{*}|\|\|_{^{-1}},\] (10)

where \(\) represents the upper bound. Such a theoretical bound is also experimentally verified in Fig. 2.

### Reward Trustiness-Aware Alignment Process

According to Eq. (10), it can be known that the error of the reward model is bounded by \(\|\|_{^{-1}}\), there might be a relatively large error for those \(\|\|_{^{-1}}\) quite large. Thus, we further propose a weighted regularization strategy to amplify the penalty strength of those samples in the high-trust region. Specifically, we calculate the amplification factor as

\[=e^{-k\|\|_{^{-1}}+b},\] (11)

where the hyperparameters \(k=0.05\) and \(b=0.7\) are used for scaling the regularization strength. We then define the overall gradient of the reward trustiness-aware alignment process as

\[_{}^{}()=_{} (),\] (12)

where \(^{}()\) is the weighted reward loss for sample \(\). For the updating of the reference model \(^{}\), we adopt  to update the model in each optimization step. Correctly amplifying the scaling factor can both speed up the convergence speed and reconstruction effectiveness, as experimentally validated in Sec. 5.2.

## 4 Human Preference-Centric Dataset for Reward

We first randomly selected 6,000, 4,000, 6,000, and 1,000 images with diverse content from ADE20K [56; 57], ImageNet , KITTI , and Div2K [60; 61] datasets, respectively. We then applied the operations described below to generate prompt images (i.e., incomplete images), which were further fed into the diffusion model for image inpainting named _Rumway_, producing inpainted images. To mitigate the potential bias of the reward model on different rewards, we

Figure 3: Statistical characteristics of the dataset we constructed. **(a)** the score distribution of the images across different selected datasets; **(b)** the comparison between the distribution of the average score and score for details; **(c)** and **(d)** show the numbers of images with different mask ratios on the outpainting and warping splits, respectively.

repetitively generated from a given prompt image three distinct inpainted images. Consequently, we obtained 51,000 inpainted images in total, which were scored by human experts following the criteria described below.

**Generation of Incomplete Images.** We considered two distinctive image completion patterns: inpainting and outpainting. For inpainting, we simulated warping holes on images by changing the viewpoints, where we derived the depth of the scene using . Following past practice , we defined a camera sequence that forms a sampling grid with three columns for yaw and three rows for pitch. The resulting nine views feature yaw angles within a \( 0.3\) range (i.e., a total range of 35\({}^{}\)) and pitch angles within a \( 0.15\) range (i.e., a total range of 17\({}^{}\)). As the range of views increases, the task of inpainting becomes progressively more challenging. For outpainting, we randomly masked the boundary of the image through two types of random cropping methods: (1) square cropping, where the size of the prompt ranges from 15% to 25% of the image size (\(512 512\)) randomly; (2) rectangular cropping, where the height of the prompt matches the image size, while the width is randomly sampled between 35% and 40% of the image size. Each cropping method accounts for half of the outpainting prompts.

**Scoring Criteria.** In the scoring process, we employed three criteria, i.e., (**1**) _structural rationality_ representing the rationality of the overall structure, whether illogical objects and structures are generated; (**2**) _feeling of local texture_ showing whether strange textures are generated that do not conform to the characteristics of the object; and (**3**) _overall feeling_ indicating the impression capturing the overall feeling upon first glances at the image. The score value is in the range of 1 to 7, indicatingfrom the least to most favorable. The final score for reward model training is derived by averaging those three scores using the weights of \([0.15,0.15,0.7]\). We refer readers to the _Supplementary Material_ for more details about dataset process and labeling scheme.

**Statistical Characteristics of our Dataset.** As illustrated in Fig. 3 (a), the score distribution of images from different datasets is generally uniform. Meanwhile, from Fig. 3 (b), we can see that the details and overall score are independent of each other, showing the correctness of the scoring scheme and the necessity of each score. Finally, Figs. 3 (c) and (d) show the ratio of images with different mask sizes, where it can be seen that the outpainting has a more uniform distribution than warping based hole, which rely on depth map and may not have uniform distribution.

## 5 Experiments

**Evaluation Metrics.** We adopted seven metrics to measure the quality of inpainted images, i.e., the predicted reward value by our trained reward model, T2I reward , CLIP , BLIP , Aesthetic (Aes.) , Classification Accuracy (CA) , and Inception Score (IS) . Specifically, T2I reward, CLIP, BLIP, and Aes. directly measure the consistency between the semantics of inpainted images and the language summary of corresponding prompt images. While, CA and IS indicate the quality of the generative model. Due to the fact that our inpainting reward directly measures the reconstruction quality, we adopted it as the principle measurement of our experiment.

**Implementation Details.** We partitioned the dataset in Sec. 4 into training, validation and testing sets, containing 12,000, 3,000 and 2,000 prompts (with 36,000, 9,000 and 6,000 images), respectively. The reinforcement fine-tuning dataset contains the prompts from the original reward training dataset. We employed the pre-trained CLIP (ViT-B) checkpoint as the backbone of our reward model \(()\) with the final MLP channel equal to 256. We utilized a cosine schedule to adjust the learning rate. Notably, we achieved optimal preference accuracy by fixing 70% of the layers with a learning rate of \(1e-5\) and a batch size of 5. We trained the reward model with four

Figure 5: Visual comparisons of our approach and SOTA methods. The prompted images of \(5^{th}\) and \(7^{th}\) rows are generated by boundary cropping, while the remaining rows by warping. All images were generated with the same random seeds.

[MISSING_PAGE_FAIL:8]

discrete scores of annotated reward samples, while the latter directly makes a regression on the scores. As listed on the right side of Table 3, it can be seen that the regression-driven training generally outperforms the classification-driven one on reward accuracy. Moreover, the larger variances from the regression-driven training show the strong discriminative ability. Based on accuracy and boundary performance, we finally selected a fixed rate of 0.7 and MLP-256 with the regression-driven training manner as the configuration of our reward model.

**Parameterize amplification factor \(\).** To parameterize the amplification factor in Eq. (10), we investigated various functions to parameterize as shown in Table 4. The experimental results indicate that the exponential function provides the best regularization effect. In contrast, the linear function and static constant do not fully exploit the regularization effect of the reward upper boundary.

### Further analysis

**Reward errors distribution.** We make statistics of reward estimation errors, and the results are shown in Fig. 9. Although the proportion of very large error samples is not large, the incremental performance of our method lies in a more suitable choice of amplification function, as evidenced by Table 4.

**Necessity of Our dataset.** Although existing metrics such as BERT Score  provide a general measure of quality, we emphasize that our dataset is specifically tailored for the task of image inpainting, where human-labeled scores are both essential and more precise. To substantiate this claim, we conduct a comparison between the BERT score and our dataset's score, as illustrated in Fig. 10. The results reveal a significant divergence between human judgments and BERT's preferences, underscoring the necessity and superior accuracy of the proposed dataset for this specific task.

Figure 6: Results of image FOV enlargement by our method on two scenes (**a**) and (**b**), where the prompt region (the given image) is delineated by the central area between white dashed lines.

Figure 7: Novel view synthesis on KITTI dataset of 6 scenes from (**a**) to (**f**). For each scene, we give the ”Prompt”, which is warped from the ”Given View”, with the white regions referring to holes/missing regions. ”Result” is our in-painting result from ”Prompt”. Note that for the synthesized novel view, there is no ground-truth available.

### Applications of Image FOV Enlargement and Novel View Synthesis

We also applied our approach to two additional tasks: (1) image field of view (FOV) enlargement, where we iteratively extended the boundaries of a typical image strip in the horizontal direction to create a wider FOV image; and (2) novel view synthesis, where we warped a given image using the predicted depth image through the method in  to generate a novel viewpoint and subsequently applied diffusion models to fill the holes/missing regions of the Warped view. As depicted in Fig. 6 (a)-a oil painting and (b)-a realistic photography, our method yields natural and visually pleasing results that can be seamlessly integrated with the prompt regions. As illustrated in Figs. 7, our method can generate more reasonable novel views and a visually appealing reconstruction. We refer readers to the _Supplementary Material_ for more details about more application visual demonstrations.

## 6 Conclusion and Discussion

We have presented PrefPaint, an innovative scheme that leverages the principles of the linear bandit theorem to align diffusion models for image inpainting with human preferences iteratively. By integrating human feedback directly into the training loop, we have established a dynamic framework that not only respects the subjective nature of visual aesthetics but also continuously adapts to the evolving preferences of users. We conducted extensive experiments to demonstrate the necessity for alignment in the task of image inpainting and the significant superiority of PrefPaint both quantitatively and qualitatively. We believe that our method, along with the newly constructed dataset, has the potential to bring significant benefits to the development of visually driven AI applications.

PrefPaint aligns with a distribution that corresponds to the preferences of a specific group. However, it is important to recognize that individual preferences for image styles vary. Therefore, after achieving alignment with the general preferences of a group, it is advisable to develop personalized rewards and a corresponding reinforced alignment model to ensure complete alignment with the preferences of each user. Recently, the implementation of a reward-free alignment process, as discussed in , has gained popularity. Consequently, exploring reward model-free training methods for alignment in diffusion-based models represents a promising avenue for future research. Furthermore, there is an opportunity to explore the potential applications of our in-painting algorithm for additional 3D reconstruction tasks.

Figure 8: Visualization of various image in-painting results and associated rewards from our model. Our model effectively evaluates in-painting reconstructions based on human preference.

Figure 9: Reward error distri-Figure 10: Plot of GT reward by our Figure 11: WinRate heat-map buttons of the proposed reward dataset (x-axis) and the Bert Score of user study. The winrate at model. The distribution of re-(y-axis) on the validation set, where specific locations shows the reward error percentages is de-each point indicates a sample. The tio of superior top methods to picted on the y-axis to the right. darker indicated the larger error. those at the bottom.