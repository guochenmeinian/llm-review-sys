# Contextual Multinomial Logit Bandits

with General Value Functions

 Mengxiao Zhang

University of Iowa

mengxiao-zhang@uiowa.edu

&Haipeng Luo

University of Southern California

haipengl@usc.edu

###### Abstract

Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising. However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability. Motivated by this fact, in this work, we consider contextual MNL bandits with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual bandits. Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off. When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards.

## 1 Introduction

As assortment recommendation becomes ubiquitous in real-world applications such as online retailing and advertising, the multinomial (MNL) bandit model has attracted great interest in the past decade since it was proposed by Rusmevichientong et al. . It involves a learner and a customer interacting for \(T\) rounds. At each round, knowing the reward/profit for each of the \(N\) available items, the learner selects a subset/assortment of size at most \(K\) and recommend it to the customer, who then purchases one of these \(K\) items or none of them according to a multinomial logit model specified by the customer's valuation over the items. The goal of the learner is to learn these unknown valuations over time and select the assortments with high reward.

To better capture practical applications where there is rich contextual information about the items and customers, a sequence of recent works study a contextual MNL bandit model where the customer's valuation is determined by the context via an unknown (generalized) linear function [8; 21; 7; 19; 20; 23; 2]. However, there are no studies on general value functions, despite many recent breakthroughs for classic contextual multi-armed bandits using a general value function class with much stronger representation power that enables fruitful results in both theory and practice [1; 10; 27; 11; 25].

Contributions.Motivated by this gap, we propose a contextual MNL bandit model with a general value function class that contains the ground truth (a standard realizability assumption), and develop a suite of algorithms for different settings and with different computation-regret trade-off.

More specifically, in Section3, we first consider a stochastic setting where the context-reward pairs are i.i.d. samples of an unknown distribution. Following the work by Simchi-Levi and Xu  for contextual bandits, we reduce the problem to an easier offline log loss regression problem and propose two strategies using an offline regression oracle: one with simple and efficient uniform exploration, and another with more adaptive exploration (and hence improved regret) induced by a novel log-barrier regularized strategy. Our results rely on several new technical findings, including a fastrate regression result (Lemma3.1), a "reverse Lipschitzness" for the MNL model (Lemma3.3), and a certain "low-regret-high-dispersion" property of the log-barrier regularized strategy (Lemma3.6).

Next, in Section4, we switch to the more challenging adversarial setting where the context-reward pairs can be arbitrarily chosen. We start by following the idea of  for contextual bandits and reducing our problem to online log loss regression, and show that it suffices to find a strategy with a small Decision-Estimation Coefficient (DEC) . We then show that, somewhat surprisingly, the same log-barrier regularized strategy we developed for the stochastic setting leads to a small DEC, despite the fact that it is not the exact DEC minimizer (unlike its counterpart for contextual bandits ). We prove this by using the same aforementioned low-regret-high-dispersion property, which to our knowledge is a new way to bound DEC and reveals why log-barrier regularized strategies work in different settings and for different problems. Finally, we also extend the idea of Feel-Good Thompson Sampling  and propose a variant for our problem that leads to the best regret bounds in some cases, despite its lack of computational efficiency.

Throughout the paper, we use two running examples to illustrate the concrete regret bounds our different algorithms achieve: the finite class and the linear class. In particular, for the linear class, this leads to five new results, summarized in Table1 together with previous results. These results all have their own advantages and disadvantages, but we highlight the following:

* While all previous regret bounds depend on a problem-dependent constant \(\) that can be exponentially large in the norm of the weight vector \(B\), _none of our results depends on \(\)_. In fact, our best results (Corollary4.8) even has only logarithmic dependence on \(B\), a potential _doubly-exponential improvement_ compared to prior works.1 * The regret bounds of our two algorithms that make use of an online regression oracle are _dimension-free_, despite not having the optimal \(\)-dependence (Corollary4.4 and Corollary4.7).
* Our results are the first to handle completely adversarial context-reward pairs.2

  Context \(x_{t}\) \& reward \(r_{t}\) & Regret & Efficient? \\  ,r_{t})\)} & \(}((dBNK)^{}{{3}}}T^{}{{3}}})\) (Corollary3.5) & ✓ \\   & \(}(K^{2})\) (Corollary3.8) & ✗ \\  Adversarial \(x_{t}\), \(r_{t}\) & \(}(dK+d^{2}K^{4})\) & ✗ \\  \)} & \(}(d+d^{2}K^{2}^{4})\) & ✗ \\   & \(}(+^{4})\) & ✗ \\  Adversarial \(r_{t}\) & \(}(d+^{2})\) & ✓ \\   & \((\{,d/K\})\) & N/A \\  ,r_{t})\)} & \(((NKB)^{}{{3}}}T^{}{{6}}})\) (Corollary4.4) & ✓ \\   & \((K^{2}}{{4}}}})\) (Corollary4.7) & ✗ \\    & \(}(K^{2})\) (Corollary4.8) & ✗ \\  

Table 1: Comparisons of results for contextual MNL bandits with \(T\) rounds, \(N\) items, size-\(K\) assortments, and a \(d\)-dimensional linear value function class with norm bounded by \(B\). All previous results depend on a problem-dependent constant \(\) that is \((2B)\) in the worst case, while ours (in gray) do not. The notation \(}()\) hides logarithmic dependency on all parameters. In the last column, ✓ means polynomial runtime in all parameters; ✗ means polynomial only when \(K\) is a constant; and ✗ means not polynomial even for a small \(K\).

Related works.The (non-contextual) MNL model was initially studied in , followed by a line of improvements [3; 4; 6; 5; 22]. Specifically, Agrawal et al. [3; 5] introduced a UCB-type algorithm achieving \(}()\) regret and proved a lower bound of \(()\). Subsequently, Chen and Wang  enhanced the lower bound to \(()\), matching the upper bound up to log factors.

Cheung and Simchi-Levi  first extended MNL bandits to its contextual version and designed a Thompson sampling based algorithm. Follow-up works consider this problem under different settings, including stochastic context [7; 19; 20], adversarial context [21; 2], and uniform reward over items . However, as mentioned, all these works consider (generalized) linear value functions, and our work is the first to consider contextual MNL bandits under a general value function class.

Our work is also closely related to the recent trend of designing contextual bandits algorithms for a general function class. Due to space limit, we defer the discussion to Appendix A.

## 2 Notations and Preliminary

Notations.Throughout this paper, we denote the set \(\{1,2,,N\}\) for some positive integer \(N\) by \([N]\) and \(\{0,1,2,,N\}\) by \([N]_{0}\). For a vector \(u^{N}\), we use \(u_{i}\) to denote its \(i\)-th coordinate, and for a matrix \(W^{N M}\), we use \(W_{j}\) to denote its \(j\)-th column. For a set \(\), we denote by \(()\) the set of distributions over \(\), and by \(()\) the convex hull of \(\). Finally, for a distribution \(([N]_{0})\) and an outcome \(i[N]_{0}\), the corresponding log loss is \(_{}(,i)=-_{i}\).

We consider the following contextual MNL bandit problem that proceeds for \(T\) rounds. At each round \(t\), the learner receives a context \(x_{t}\) for some arbitrary context space \(\) and a reward vector \(r_{t}^{N}\) which specifies the reward of \(N\) items. Then, out of these \(N\) items, the learner needs to recommend a subset \(S_{t}\) to a customer, where \( 2^{[N]}\) is the collection of all subsets of \([N]\) with cardinality at least \(1\) and at most \(K\) for some \(K N\). Finally, the learner observes the customer purchase decision \(i_{t} S_{t}\{0\}\), where \(0\) denotes the no-purchase option, and receives reward \(r_{t,i_{t}}\), where for notational convenience we define \(r_{t,0}=0\) for all \(t\) (no reward if no purchase). The customer decision \(i_{t}\) is assumed to follow an MNL model:

\[[i_{t}=i S_{t},x_{t}]=^{}(x_{t}) }{1+_{j S_{t}}f_{j}^{}(x_{t})}&i S_{t},\\ }f_{j}^{}(x_{t})}&i=0,\\ 0&,\] (1)

where \(f^{}:^{N}\) is an unknown value function, specifying the costumer's value for each item under the given context. The MNL model above implicitly assumes a value of \(1\) for the no-purchase option, making it the most likely outcome. This is a standard assumption that holds in many realistic settings [5; 9; 18].

To simplify notation, we define \(:^{N}([N]_{0})\) such that \(_{i}(S,v) v_{i}[i S\{0\}]\) with the convention \(v_{0}=1\). The purchase decision \(i_{t}\) is thus sampled from the distribution \((S_{t},f^{}(x_{t}))\). In addition, given a reward vector \(r^{N}\) (again, with convention \(r_{0}=0\)), we further define the expected reward of choosing subset \(S\) under context \(x\) as

\[R(S,v,r)=_{i(S,v)}[r_{i}]=_{i S}_{i}(S, v)r_{i}=r_{i}v_{i}}{1+_{i S}v_{i}}.\]

The goal of the learner is then to minimize her regret, defined as the expected gap between her total reward and that of the optimal strategy with the knowledge of \(f^{}\):

\[_{}=[_{t=1}^{T}_{S }R(S,f^{}(x_{t}),r_{t})-_{t=1}^{T}R(S_{t},f^{}(x_{t} ),r_{t})].\]

To ensure that no-regret is possible, we make the following assumption, which is standard in the literature of contextual bandits.

**Assumption 1**: _The learner is given a function class \(=\{f:^{N}\}\) which contains \(f^{}\)._

Our hope is thus to design algorithms whose regret is sublinear in \(T\) and polynomial in \(N\) and some standard complexity measure of the function class \(\). So far, we have not specified how the context \(x_{t}\) and the reward \(x_{t}\) are chosen. In the next two sections, we will discuss both the easier stochastic case where \((x_{t},r_{t})\) is jointly drawn from some fixed and unknown distribution, and the harder adversarial case where \((x_{t},r_{t})\) can be arbitrarily chosen by an adversary.

## 3 Contextual MNL Bandits with Stochastic Contexts and Rewards

In this section, we consider contextual MNL bandits with stochastic contexts and rewards, where at each round \(t[T]\), \(x_{t}\) and \(r_{t}\) are jointly drawn from a fixed and unknown distribution \(\). Following the literature of contextual bandits, we aim to reduce the problem to an easier and better-studied offline regression problem and only access the function class \(\) through some offline regression oracle \(_{}\) takes as input a set of i.i.d. context-subset-purchase tuples and outputs a predictor from \(\) with low generalization error in terms of log loss, formally defined as follows.

**Assumption 2**: _Given \(n\) samples \(D=\{(x_{k},S_{k},i_{k})\}_{k=1}^{n}\) where each \((x_{k},S_{k},i_{k})[N]_{0}\) is an i.i.d. sample of some unknown distribution \(\) and the conditional distribution of \(i_{k}\) is \((S_{k},f^{}(x_{k}))\), with probability at least \(1-\) the offline regression oracle \(_{}\) outputs a function \(_{D}\) such that:_

\[_{(x,S,i)}[_{}((S,_{D}(x) ),i)-_{}((S,f^{}(x)),i)]_{}(n, ,),\] (2)

_for some function \(_{}(n,,)\) that is non-increasing in \(n\)._

Given the similarity between MNL and multi-class logistic regression, assuming such a log loss regression oracle is more than natural. Indeed, in the following lemma, we prove that for both the finite class and a certain linear function class, the empirical risk minimizer (ERM) not only satisfies this assumption, but also enjoys a fast \(1/n\) rate. The proof is based on the observation that our loss function \(_{}((S,f(x)),i)\), when seen as a function of \(f\), satisfies the so-called strong \(1\)-central condition [17, Definition 7], which might be of independent interest; see Appendix B.1 for details.

**Lemma 3.1**: _The ERM strategy \(_{D}=*{argmin}_{f}_{(x,S,i) D} _{}((S,f(x)),i)\) satisfies Assumption 2 for the following two cases:_

* _(Finite class)_ \(\) _is a finite class of functions with image_ \([,1]^{N}\) _for some_ \((0,1)\) _and_ \(_{}(n,,)=(|/}{n})\)_._
* _(Linear class)_ \(\{x^{d N}\|x_{i}\|_{2} 1,\  i [N]\}\)_,_ \(=\{f_{,i}(x)=e^{^{}x_{i}-B}\|\|_{2} B\}\)_, and_ \(_{}(n,,)=}{n}\)_, for some_ \(B>0\)_.\(3\) 
Due to space limit, we only use these two simple function classes as running examples throughout the paper, but we emphasize that our results can be applied to any class as long as regression is feasible. For additional examples, see Appendix D.

Given \(_{}\), we now outline a natural algorithm framework that proceeds in epochs with exponentially increasing length (see Algorithm 1): At the beginning of each epoch \(m\), the algorithm feeds all the context-subset-purchase tuples from the last epoch to the offline regression oracle \(_{}\) and obtains a value predictor \(f_{m}\). Then, it decides in some way using \(f_{m}\) a stochastic policy \(q_{m}\), which maps a context \(x\) and a reward vector \(r^{N}\) to a distribution over \(\). With such a policy in hand, for every round \(t\) within this epoch, the algorithm simply samples a subset \(S_{t}\) according to \(q_{m}(x_{t},r_{t})\) and recommend it to the customer.

We will specify two concrete stochastic policies \(q_{m}\) in the next two subsections. Before doing so, we highlight some key parts of the analysis that shed light on how to design a "good" \(q_{m}\). The first step is an adaptation of Simchi-Levi and Xu [25, Lemma 7], which quantifies the expected reward difference of any policy under the ground-truth value function \(f^{}\) versus the estimated value function \(f_{m}\). Specifically, for a deterministic policy \(:^{N}\) mapping from a context-reward pair to a subset, we define its true expected reward and its expected reward under \(f_{m}\) respectively as (overloading the notation \(R\)):

\[R()=_{(x,r)}[R((x,r),f^{}(x),r) ],\ \ R_{m}()=_{(x,r)}[R((x,r),f_{m}(x),r) ].\] (3)

Moreover, for any \(()\), define \(w()^{N}\) such that \(w_{i}()=_{S:i S}(S)\) is the probability of item \(i\) being selected under distribution \(\), and for any stochastic policy \(q\), further define a dispersion measure for a deterministic policy \(\) as \(V(q,)=_{(x,r)}[_{i(x,r)}(q(x,r))}]\) (the smaller \(V(q,)\) is, the more disperse the distribution induced by \(q\) is). Using the Lipschitzness (in \(v\)) of the reward function \(R(S,v,r)\) (Lemma B.1), we prove the following.

**Lemma 3.2**: _For any deterministic policy \(:^{N}\) and any epoch \(m 2\), we have_

\[|R_{m}()-R()|,)}_{(x,r) ,S q_{m-1}(x,r)}[_{i S}(f_{m,i}(x)-f_{i}^{ }(x))^{2}]}.\]

If the learner could observe the true value of each item in the selected subset (or its noisy version), then doing squared loss regression on these values would make the squared loss term in Lemma 3.2 small; this is essentially the case in the contextual bandit problem studied by Simchi-Levi and Xu . However, in our problem, only the purchase decisions are observed but not the true values that define the MNL model. Nevertheless, one of our key technical contributions is to show that the offline log-loss regression, which only relies on observing the purchase decisions, in fact also makes sure that the squared loss above is small.

**Lemma 3.3**: _For any \(S\) and \(v,v^{}^{N}\), we have_

\[}_{i S}(v_{i}-v_{i}^{})^{2}\|(S,v)- (S,v^{})\|_{2}^{2} 2_{i(S,v^{})}[_{ }((S,v),i)-_{}((S,v^{}),i)].\]

The first equality establishes certain "reverse Lipschitzness" of \(\) and is proven by providing a universal lower bound on the minimum singular value of its Jacobian matrix, which is new to our knowledge. It implies that if two value vectors induce a pair of close distributions, then they must be reasonably close as well. The second equality, proven using known facts, further states that to control the distance between two distributions, it suffices to control their log loss difference, which is exactly the job of the offline regression oracle.

Therefore, combining Lemma 3.2 and Lemma 3.3, we see that to design a good algorithm, it suffices to find a stochastic policy that "mostly" follows \(*{argmax}_{S}R(S,f_{m}(x_{t}),r_{t})\), the best decision according to the oracle's prediction, and at the same time ensures high dispersion for all \(\) such that the oracle's predicted reward for any policy is close to its true reward. The design of our two algorithms in the remaining of this section follows exactly this principle.

### A Simple and Efficient Algorithm via Uniform Exploration

As a warm-up, we first introduce a simple but efficient \(\)-greedy-type algorithm that ensures reasonable dispersion by uniformly exploring all the singleton sets. Specifically, at epoch \(m\), given the value predictor \(f_{m}\) from \(}\), \(q_{m}(x,r)()\) is defined as follows for some \(_{m}>0\):

\[q_{m}(S|x,r)=(1-_{m})[S=*{ argmax}_{S^{*}}R(S^{*},f_{m}(x),r)]+}{N} _{i=1}^{N}[S=\{i\}].\] (4)

In other words, with probability \(1-\), the learner picks the subset achieving the maximum reward based on the reward vector \(r\) and the predicted value \(f_{m}(x)\); with the remaining \(\) probability, the learner selects a uniformly random item \(i[N]\) and recommend only this item, which clearly ensures \(V(q_{m},)}\) for any \(\). Based on our previous analysis, it is straightforward to prove the following regret guarantee.

**Theorem 3.4**: _Under Assumption 1 and Assumption 2, Algorithm 1 with \(q_{m}\) defined in Eq. (4) and the optimal choice of \(_{m}\) ensures \(}}=_{m=1}^{_{2}T} (2^{m}(NK_{}(2^{m-1},1/T^{2},))^{})\)._

To better interpret this regret bound, we consider the finite class and the linear class discussed in Lemma 3.1. Combining it with Theorem 3.4, we immediately obtain the following corollary:

**Corollary 3.5**: _Under Assumption 1, Algorithm 1 with \(q_{m}\) defined in Eq. (4), the optimal choice of \(_{m}\), and ERM as \(}\) ensures \(}}=((NK(| |T))^{}T^{})\) for finite class and \(}}=((dBNK K)^{}T^{ }(BT) T)\) for linear class (see Lemma 3.1 for definitions)._

While these \(}(T^{}{{3}}})\) regret bounds are suboptimal, Theorem 3.4 provides the first computationally efficient algorithms for contextual MNL bandits with an offline regression oracle for a general function class. Indeed, computing \(*{argmax}_{S^{*}}R(S^{*},f_{m}(x),r)\) can be efficiently done in \((N^{2})\) time according to [24, Section 2.1]. Moreover, for the linear case, the ERM oracle can indeed be efficiently (and approximately) implemented because it is a convex optimization problem over a simple ball constraint. Importantly, previous regret bounds for the linear case all depend on a problem-dependent constant \(=_{\|\| B,S,i,t[T]}(S,f_{}(x_{t}))_{0}(S,f_{}(x_{t}))}\), which is \((2B)\) in the worst case [7; 20; 23], but ours only has polynomial dependence on \(B\).

### Better Exploration Leads to Better Regret

Next, we show that a more sophisticated construction of \(q_{m}\) in Algorithm 1 leads to better exploration and consequently improved regret bounds. Specifically, \(q_{m}\) is defined as (for some \(_{m}>0\)):

\[q_{m}(x,r)=*{argmax}_{()}_{S }[R(S,f_{m}(x),r)]-}{_{m}}_{i=1}^{ N}()}.\] (5)

The first term of the optimization objective above is the expected reward when one picks a subset according to \(\) and the value function is \(f_{m}\), while the second term is a certain log-barrier regularizer applied to \(\), penalizing it for putting too little mass on any single item. This specific form of regularization ensures that \(q_{m}\) enjoys a low-regret-high-dispersion guarantee, as shown below.

**Lemma 3.6**: _For any \(x\) and \(r^{N}\), the distribution \(q_{m}(x,r)\) defined in Eq. (5) satisfies:_

\[_{S^{*}}R(S^{*},f_{m}(x),r)-_{S q _{m}(x,r)}[R(S,f_{m}(x),r)]}{_{m}},\] (6) \[ S, _{i S}(q_{m}(x,r))} N+} {(K+1)^{4}}(_{S^{*}}R(S^{*},f_{m}(x),r)-R(S,f_{m}(x), r)).\] (7)

Eq. (6) states that following \(q_{m}(x,r)\) does not incur too much regret compared to the best subset predicted by the oracle, and Eq. (7) states that the dispersion of \(q_{m}(x,r)\) on any subset is controlledby how bad this subset is compared to the best one in terms of their predicted reward -- a good subset has a large dispersion while a bad one can have a smaller dispersion since we do not care about estimating its true reward very accurately. Such a refined dispersion guarantee intuitively provides a much more adaptive exploration scheme compared to uniform exploration.

This kind of low-regret-high-dispersion guarantees is in fact very similar to the ideas of Simchi-Levi and Xu  for contextual bandits (which itself is similar to an earlier work by Agarwal et al. ). While Simchi-Levi and Xu  were able to provide a closed-form strategy with such a guarantee for contextual bandits, we do not find a similar closed-form for MNL bandits and instead provide the strategy as the solution of an optimization problem Eq. (5). Unfortunately, we are not aware of an efficient way to solve Eq. (5) with polynomial time complexity, but one can clearly solve it in \((||)=(N^{K})\) time since it is a concave problem over \(()\). Thus, the algorithm is efficient when \(K\) is small, which we believe is the case for most real-world applications.

Combining Lemma 3.2 and Lemma 3.6, we prove the following regret guarantee, which improves the \(_{}^{1/3}\) term in Theorem 3.4 to \(_{}^{1/2}\) (proofs deferred to Appendix B).

**Theorem 3.7**: _Under Assumption 1 and Assumption 2, Algorithm 1 with \(q_{m}\) defined in Eq. (5) and the optimal choice of \(_{m}\) ensures \(_{}=(_{m=1}^{_{2}T }2^{m}K^{2}_{}(2^{m-1},1/T^{2},)})\)._

Similar to Section 3.1, we instantiate Theorem 3.7 using the following two concrete classes:

**Corollary 3.8**: _Under Assumption 1, Algorithm 1 with \(q_{m}\) defined in Eq. (5), the optimal choice of \(_{m}\), and ERM as \(_{}\) ensures \(_{}=(K^{2} (||T)})\) for the finite class and \(_{}=(K^{2})\) for the linear class (see Lemma 3.1 for definitions)._

The dependence on \(T\) in these \(()\) regret bounds is known to be optimal [6; 7]. Once again, in the linear case, we have no exponential dependence on \(B\), unlike previous results.

## 4 Contextual MNL Bandits with Adversarial Contexts and Rewards

In this section, we move on to consider the more challenging case where the context \(x_{t}\) and the reward vector \(r_{t}\) can both be arbitrarily chosen by an adversary. We propose two different approaches leading to three different algorithms, each with its own pros and cons.

### First Approach: Reduction to Online Regression

In the first approach, we follow a recent trend of studies that reduces contextual bandits to online regression and only accesses \(\) through an online regression oracle [10; 11; 15; 31; 29]. More specifically, we assume access to an online regression oracle \(_{}\) that follows the protocol below: at each round \(t[T]\), \(_{}\) outputs a value predictor \(f_{t}()\); then, it receives a context \(x_{t}\), a subset \(S_{t}\), and a purchase decision \(i_{t} S_{t}\{0\}\), all chosen arbitrarily, and suffers log loss \(_{}((S_{t},f_{t}(x_{t})),i_{t})\).4 The oracle is assumed to enjoy the following regret guarantee.

**Assumption 3**: _The predictions made by the online regression oracle \(_{}\) ensure:_

\[[_{t=1}^{T}_{}((S_{t},f_{t}(x_{t})),i_{t})- _{t=1}^{T}_{}((S_{t},f^{}(x_{t})),i_{t})] _{}(T,),\]

_for any \(f^{}\) and some regret bound \(_{}(T,)\) that is non-decreasing in \(T\)._

While most previous works on contextual bandits assume a squared loss online oracle, log loss is more than natural for our MNL model (it was also used by Foster and Krishnamurthy  to achieve first-order regret guarantees for contextual bandits). The following lemma shows that Assumption 3 again holds for the finite class and the linear class.

**Lemma 4.1**: _For the finite class and the linear class discussed in Lemma 3.1, the following concrete oracles satisfy Assumption 3:_

* _(Finite class) Hedge_ _[_16_]_ _with_ \(_{}(T,)=(|} )\)_;_
* _(Linear class) Online Gradient Descent_ _[_32_]_ _with_ \(_{}(T,)=(B)\)_._

Unfortunately, unlike the offline oracle, we are not able to provide a "fast rate" (that is, \(}(1)\) regret) for these two cases, because our loss function does not appear to satisfy the standard Vovk's mixability condition or any other sufficient conditions discussed in Van Erven et al. . This is in sharp contrast to the standard multi-class logistic loss , despite the similarity between these two models. We leave as an open problem whether fast rates exist for these two classes, which would have immediate consequences to our final MNL regret bounds below.

With this online regression oracle, a natural algorithm framework works as follows: at each round \(t\), the learner first obtains a value predictor \(f_{t}()\) from the regression oracle \(_{}\); then, upon seeing context \(x_{t}\) and reward vector \(r_{t}\), the learner decides in some way a distribution \(q_{t}()\) based on \(f_{t}(x_{t})\) and \(r_{t}\), and samples \(S_{t}\) from \(q_{t}\); finally, the learner observes the purchase decision \(i_{t}\) and feeds the tuple \((x_{t},S_{t},i_{t})\) to the oracle \(_{}\) (see Algorithm 2 in Appendix C). To shed light on how to design a good sampling distribution \(q_{t}\), we show a general lemma that holds for any \(q_{t}\).

**Lemma 4.2**: _Under Assumption 1 and Assumption 3, Algorithm 2 (with any \(q_{t}\)) ensures_

\[_{}[_{t=1}^{T}_{ }(q_{t};f_{t}(x_{t}),r_{t})]+2_{}(T,)\]

_for any \(>0\), where \(_{}(q;v,r)\) is the Decision-Estimation Coefficient (DEC) defined as_

\[_{v^{}^{N}}_{S^{}}\{R(S^{}, v^{},r)-_{S q}[R(S,v^{},r)]-_{S  q}[\|(S,v)-(S,v^{})\|_{2}^{2}]\}.\] (8)

Our DEC adopts the idea of Foster et al.  for general decision making problems: the term \(R(S^{},v^{},r)-_{S q}[R(S,v^{},r)]\) represents the instantaneous regret of strategy \(q\) against the best subset \(S^{}\) with respect to reward vector \(r\) and the worst-case value vector \(v^{}\), and the term \(_{S q}[\|(S,v)-(S,v^{})\|_{2}^{2}]\) is the expected squared distance between two distributions induced by \(v\) and \(v^{}\), which, in light of the second inequality of Lemma 3.3, lower bounds the instantaneous log loss regret of the online oracle. Therefore, a small DEC makes sure that the learner's MNL regret is somewhat close to the oracle's log loss regret \(_{}\), formally quantified by Lemma 4.2. With the goal of ensuring a small DEC, we again propose two strategies similar to Section 3.

Uniform Exploration.We start with a simple uniform exploration approach similar to Eq. (4):

\[q_{t}(S)=(1-)[S=*{argmax}_{S^{} }R(S^{},f_{t}(x_{t}),r_{t})]+ _{i=1}^{N}[S=\{i\}].\] (9)

where \(>0\) is a parameter specifying the probability of uniformly exploring the singleton sets. We prove the following results for this simple algorithm.

**Theorem 4.3**: _The strategy defined in Eq. (9) guarantees \(_{}(q_{t};f_{t}(x_{t}),r_{t})=(+)\). Consequently, under Assumption 1 and Assumption 3, Algorithm 2 with \(q_{t}\) calculated via Eq. (9) and the optimal choice of \(\) and \(\) ensures \(_{}=(NK_{}(T, ))^{}T^{}\)._

Combining this with Lemma 4.1, we immediately obtain the following corollary.

**Corollary 4.4**: _Under Assumption 1, Algorithm 2 with \(q_{t}\) defined in Eq. (9) and the optimal choice of \(\) and \(\) ensures \(_{}=(NK)^{}T^{}\) for the finite class (with Hedge as \(_{}\)) and \(_{}=(NKB)^{}T^{}\) for the linear class (with Online Gradient Descent as \(_{}\))._

While these regret bounds have a large dependence on \(T\), the advantage of this algorithm is its computational efficiency as discussed before.

Better Exploration.Can we improve the algorithm via a strategy with an even smaller DEC? In particular, what happens if we take the extreme and let \(q_{t}\) be the minimizer of \(_{}(q;f_{t}(x_{t}),r_{t})\)? Indeed, this is exactly the approach in several prior works that adopt the DEC framework , where the exact minimizer for DEC is characterized and shown to achieve a small DEC value.

On the other hand, for our problem, it appears quite difficult to analyze the exact DEC minimizer. Somewhat surprisingly, however, we show that the same construction in Eq.5 for the stochastic environment in fact also achieves a reasonably small DEC for the adversarial case:

**Theorem 4.5**: _The following distribution satisfies \(_{}(q_{t},f_{t}(x_{t}),r_{t})(}{})\):_

\[q_{t}=*{argmax}_{q()}_{S q} [R(S,f_{t}(x_{t}),r_{t})]-}{}_{i=1}^{N} (q)}.\] (10)

A couple of remarks are in order. First, while for some cases such as the contextual bandit problem studied by Foster et al. , this kind of log-barrier regularized strategies is known to be the exact DEC minimizer, one can verify that this is not the case for our DEC. Second, the fact that the same strategy works for both the stochastic and the adversarial environments is similar to the case for contextual bandits where the same inverse gap weighting strategy works for both cases , but to our knowledge, the connection between these two cases is unclear since their analysis is quite different. Finally, our proof (in AppendixC) in fact relies on the same low-regret-high-dispersion property of Lemma3.6, which is a new way to bound DEC as far as we know. More importantly, this to some extent demystifies the last two points: the reason that such log-barrier regularized strategies work regardless whether they are the exact minimizer or not and regardless whether the environment is stochastic or adversarial is all due to their inherent low-regret-high-dispersion property.

Combining Theorem4.5 with Lemma4.2, we obtain the following improved regret.

**Theorem 4.6**: _Under Assumption1 and Assumption3, Algorithm2 with \(q_{t}\) calculated via Eq.10 and the optimal choice of \(\) ensures \(_{}=K^{2}_{ }(T,)}\)._

**Corollary 4.7**: _Under Assumption1, Algorithm2 with \(q_{t}\) defined in Eq.10 and the optimal choice of \(\) ensures \(_{}=K^{2}} T^{}(||)^{}\) for the finite class (with Hedge as \(_{}\)) and \(_{}=K^{2}}} \) for the linear class (with Online Gradient Descent as \(_{}\))._

We remark that if the "fast rate" discussed after Lemma4.1 exists, we would have obtained the optimal \(\)-regret here. Despite having worse dependence on \(T\), however, our result for the linear case enjoys three advantages compared to prior work : 1) no exponential dependence on \(B\) (as in all our other results), 2) no dependence at all on the dimension \(d\), and 3) valid even when contexts and rewards are adversarial. We refer the reader to Table1 again for detailed comparisons.

### Second Approach: Feel-Good Thompson Sampling

The second approach we take is to extend the idea of the Feel-Good Thompson Sampling algorithm of Zhang  for contextual bandits. Due to space limit, we defer the algorithm and its analysis to AppendixE, and only state its regret bounds for the finite class and the linear class (a corollary of a more general regret bound in TheoremE.1).

**Corollary 4.8**: _Under Assumption1, Algorithm3 wensures \(_{}=K^{2} |}\) for the finite class and \(_{}=K^{2}\) for the linear class._

In terms of the dependence on \(T\), Algorithm3 achieves the best (and in fact optimal) regret bounds among all our results. For the linear case, it even has only logarithmic dependence on \(B\), a potential doubly-exponential improvement compared to prior works. The caveat is that there is no efficient way to implement the algorithm even for the linear case and even when \(K\) is a constant (unlike all our other algorithms). We leave the question of whether there exists a computationally efficient algorithm (even only for small \(K\)) with a \(\)-regret bound that has no exponential dependence on \(B\) as a key future direction.

Conclusion and Future Directions

In this work, we consider contextual MNL bandits with a general value function class under a realizability assumption. For both the stochastic and the adversarial settings, we propose a suite of algorithms with different computational-regret trade-off. Notably, none of our regret bounds suffers from the exponentially large dependence on some problem dependent constant in the case with linear value functions. One interesting future direction is to improve the \((K,N)\) dependence in our regret upper bounds, which seems to require new techniques.