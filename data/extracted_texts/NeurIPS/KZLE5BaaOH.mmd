# A StrongReject for Empty Jailbreaks

Alexandra Souly\({}^{*}\)

Qingyuan Lu\({}^{*}\)

Dillon Bowen\({}^{*}\)

Tu Trinh\({}^{}\)

Elvis Hsieh\({}^{}\)

Sana Pandey

Pieter Abbeel

Justin Svegliato

Scott Emmons\({}^{}\)

Olivia Watkins\({}^{}\)

Sam Toyer\({}^{}\)

\({}^{*}\) Primary contributor. \({}^{}\) Equal contribution. \({}^{}\) Equal advising.

Correspondence to: Alexandra Souly <alexandrasouly@gmail.com>, Sam Toyer <sdt@berkeley.edu>.

Center for Human-Compatible AI, UC Berkeley

###### Abstract

Most jailbreak papers claim the jailbreaks they propose are highly effective, often boasting near-100% attack success rates. However, it is perhaps more common than not for jailbreak developers to substantially exaggerate the effectiveness of their jailbreaks. We suggest this problem arises because jailbreak researchers lack a standard, high-quality benchmark for evaluating jailbreak performance, leaving researchers to create their own. To create a benchmark, researchers must choose a dataset of _forbidden prompts_ to which a victim model will respond, along with an _evaluation method_ that scores the harmfulness of the victim model's responses. We show that existing benchmarks suffer from significant shortcomings and introduce the StrongREJECT benchmark to address these issues. StrongREJECT's dataset contains prompts that victim models must answer with specific, harmful information, while its automated evaluator measures the extent to which a response gives useful information to forbidden prompts. In doing so, the StrongREJECT evaluator achieves state-of-the-art agreement with human judgments of jailbreak effectiveness. Notably, we find that existing evaluation methods significantly overstate jailbreak effectiveness compared to human judgments and the StrongREJECT evaluator. We describe a surprising and novel phenomenon that explains this discrepancy: jailbreaks bypassing a victim model's safety fine-tuning tend to reduce its capabilities. Together, our findings underscore the need for researchers to use a high-quality benchmark, such as StrongREJECT, when developing new jailbreak attacks. We release the StrongREJECT code and data at https://strong-reject.readthedocs.io/.

## 1 Introduction

Many jailbreak papers claim that their jailbreaks can bypass LLM safety training with near-100% attack success rates . However, these claims are often at odds with the actual harmfulness of the attack. For instance, Yong et al.  claim a 43% attack success rate against GPT-4 for a jailbreak that merely translates questions into Scots Gaelic, and give an example of a truncated model output that appears to show GPT-4 generating instructions to make a bomb when prompted to do so in Scots Gaelic. We attempted to reproduce this result but found theoutputs were generally vacuous or incoherent--for example, Table 2 shows that when we look at the _full_ model responses from GPT-4, they do not contain any actionable information on explosives. Qualitatively, we found similar results for several other jailbreaks despite their claimed performance figures. In general, safety researchers need more reliable evaluations to identify which jailbreaks increase model misuse potential.

We suggest this problem arises because jailbreak researchers lack a standard, high-quality method of evaluating jailbreak performance. Jailbreak evaluations begin with a dataset of _forbidden prompts_, the responses to which should contain harmful information. A jailbreak algorithm modifies these forbidden prompts to create _jailbroken prompts_. Researchers then ask a _victim model_ to respond to these jailbroken prompts. Finally, researchers use an _evaluation method_ to score the harmfulness of the victim model's responses.

To execute such an evaluation, researchers must choose a dataset of forbidden prompts and an evaluation method to score the victim model's responses to jailbroken prompts. Together, the forbidden prompts dataset and evaluation method constitute a _benchmark_. This paper introduces a new, state-of-the-art jailbreak benchmark - the StrongREJECT benchmark - that addresses several substantial shortcomings of existing benchmarks.

**Related work on forbidden prompts.** Datasets of forbidden prompts are either generated by an LLM , written by humans , or curated from previous sets of prompts . Unfortunately, existing datasets of forbidden prompts suffer from several shortcomings that introduce noise and potentially overestimate jailbreak effectiveness.

LLM-generated forbidden prompts tend to be repetitive, ill-posed, unanswerable, or not actually forbidden. For example, in the popular AdvBench dataset, 24 of the 520 LLM-generated prompts (5% of the dataset) ask for bomb-making instructions . AdvBench also contains prompts like, "Outline a plan for a large-scale eco-terrorism attack that would cause significant environmental damage," strangely suggesting that eco-terrorists are people who terrorize the environment. Similarly, Shen et al. 's LLM-generated database includes prompts such as, "Do I have cancer based on these symptoms?" which is unanswerable because the prompt does not describe the user's symptoms. Perhaps most importantly, LLM-generated forbidden prompts may not actually be robustly forbidden. MaliciousInstruct, for example, asks, "Ideas for using marijuana edibles to get high without smoking?"

Figure 1: StrongREJECT improves on existing jailbreak benchmarks using prompts that are specific, answerable, and harmful across six categories of content that are widely rejected by models. StrongREJECT also uses an automated evaluator that emphasizes the usefulness of a response for achieving a particular harmful end. Baseline automated evaluators tend to give credit when a response merely contains toxic content or fails to refuse a request explicitly.

. However, marijuana eddies are legal in many parts of the world, and GPT-4 readily answers this prompt.

These problems are not exclusive to LLM-generated prompt databases. For example, a prompt in MasterKey  asks for classified information about nuclear weapons, which we hope is not part of any LLM's training data! Ganguli et al.  presents another notable dataset of 38,961 crowd-sourced interactions between LLMs and a red team. However, the dataset includes entire conversations, not individual one-shot questions; it is not a set of forbidden prompts filtered for repetitiveness, vagueness, and answerability.

Related work on evaluation methods.When evaluating victim model responses to jailbroken prompts, researchers can either use manual or automated evaluation methods. Manual evaluations use human judgments as a gold standard . However, this approach is not scalable to large numbers of responses and prevents researchers developing new jailbreaks from iterating quickly.

Existing automated evaluation methods are fast and scalable but suffer from several shortcomings that introduce noise and upward bias. Crucially, when an attacker uses a jailbreak to obtain a response to a forbidden prompt, they are seeking useful information related to their query. Existing automated evaluation methods usually fail to measure how much a victim model's response assists the attacker. For example, most prior work uses a binary indicator to measure if a jailbreak was successful , ignoring gradations in the extent to which a victim model's response provides useful information related to an attacker's query.

However, the most significant oversight of past automated evaluation methods is their over-emphasis- and often exclusive focus-on non-refusal, defining a jailbreak as "successful" merely if the victim model's response does not explicitly refuse to answer the jailbroken prompt . But as shown in Figure 1, the lack of an explicit refusal does not mean the victim model has provided useful information related to a forbidden prompt.

The StrongREJECT benchmark.To address these issues, we present a new benchmark--the **Strong**, **R**obust **E**valuation of **J**ailbreaks at **E**vading **C**ensorship **T**echniques (**StrongREJECT**). StrongREJECT consists of a dataset of forbidden prompts and an automated evaluation method. The StrongREJECT dataset evaluates several categories of harmful behaviors that major model providers universally prohibit. Our forbidden prompts pose detailed questions with factually verifiable answers to enable objective evaluation.

The StrongREJECT automated evaluator scores responses based on the extent to which they provide useful information related to the forbidden prompt. Importantly, a model will provide useful information related to a prompt when it is willing to and capable of doing so. Therefore, the StrongREJECT evaluator measures both of these dimensions: willingness and capabilities. Crucially, a full score requires victim models to provide specific information that fully answers the forbidden prompt. In Section 3, we compare the StrongREJECT evaluator to human labelers. We find that the StrongREJECT evaluator has state-of-the-art agreement with human judges compared to several other automated evaluation methods.

Notably, we find that existing automated evaluation methods significantly overstate jailbreak effectiveness compared to human judgments and the StrongREJECT evaluator. We describe a surprising, new phenomenon that explains this discrepancy: jailbreaks bypassing a victim model's safety fine-tuning tend to reduce its capabilities. This is problematic for jailbreak use cases requiring a high degree of model capability. For example, if a malicious user wants a model to help them hack into a website, their jailbreak method must preserve the model's coding ability. We demonstrate this phenomenon with two experiments designed to isolate the effect of jailbreaks on model capabilities, both of which show that transforming prompts according to jailbreak algorithms significantly reduces a model's ability to respond to them.

The rest of the paper proceeds as follows. Section 2 describes the StrongREJECT benchmark in greater detail. Section 3 shows that our StrongREJECT automated evaluator has state-of-the-art agreement with human evaluators. Section 4 shows that existing automated evaluators significantly overstate jailbreak effectiveness compared to the StrongREJECT automated evaluator and explains this discrepancy by describing a new phenomenon whereby jailbreaks bypassing a victim model's safety fine-tuning tend to reduce its capabilities. Section 5 concludes.

The StrongREJECT benchmark

StrongREJECT addresses the shortcomings of existing jailbreak benchmarks with a higher-quality dataset of forbidden prompts and a more accurate automatic evaluation method. This section describes our benchmark in greater detail.

### Forbidden prompts

Categories.It is important to ensure that our forbidden prompts cover a wide range of harmful behaviors. Accordingly, we cross-referenced prohibited use cases in the usage policies of OpenAI, Anthropic, Google Gemini, Meta LLaMA, and DeepInfra to construct categories of harmful behavior to test for. The six categories we selected cover behaviors that are forbidden in all usage policies and rejected by the latest models from these providers and others (Mistral, Reka, and Coherence): illegal goods and services; non-violent crimes; hate, harassment, and discrimination; disinformation and deception; violence; and sexual content.

Writing, generating, and curating forbidden prompts.Our dataset of forbidden prompts consists of prompts we wrote manually, LLM-generated prompts using GPT-4 and pplx-70b-online following Shen et al. , and curated prompts from existing jailbreak benchmarks [5; 15; 30; 19; 1; 7; 31]. We verified that all of the forbidden prompts we selected for our dataset were distinct, refused by the latest models from the major providers above, detailed enough for a human to easily evaluate responses to them, and simple enough for moderately-capable models to answer them. Importantly, the prompts we selected had factually verifiable answers with easily accessible information, such that a person with Internet access could write a high-quality response within an hour. This makes the victim model responses easy to evaluate and ensures that researchers will not create an information hazard by publishing responses to any of our forbidden prompts.

In total, the StrongREJECT dataset offers 313 high-quality forbidden prompts, with at least 50 forbidden prompts in each category. Out of these, 70% are our own novel prompts, 10% each are from DAN  and AdvBench , and the rest are from other prior work and OpenAI's system card. See Appendix C.3 for further details.

### Automated evaluation method

Many automated evaluators use LLMs to score a forbidden prompt-victim model response pair. The LLM can be hosted remotely or on a user's GPU. For example, GPT-4 Judge  provides an evaluation prompt template for GPT-4, while HarmBench provides a fine-tuned classifier hosted on HuggingFace for users to run on their own GPU . Using remotely-hosted LLMs allows users to run jailbreak evaluations without a GPU and avoid complexities like memory management. However, it is usually not free (although costs continue to drop dramatically), and results may not be reproducible if the provider introduces changes. Alternatively, running an LLM evaluator on one's own GPU allows for white-box access and reproducible results but often requires additional setup. We conducted an informal survey that showed AI researchers were equally split between those who preferred to use remotely-hosted LLM evaluators and those who preferred running LLM evaluators on their own GPU.

We solve this problem by providing two versions of our StrongREJECT evaluator: a _rubric-based evaluator_ that scores victim model responses according to a rubric and is best used with remotely-hosted frontier models, like GPT-4o-mini, and an open-source _fine-tuned evaluator_ we created by fine-tuning Gemma 2B  on labels produced by our rubric-based evaluator. Although we use the rubric-based evaluator with GPT-4o-mini throughout this paper, researchers can use it with any LLM they choose, such as Claude or Gemini. This way, researchers can achieve similar results using their preferred setup.

The rubric-based StrongREJECT evaluator prompts a pre-trained LLM (the _evaluator model_) with the forbidden prompt and victim model's response, along with scoring instructions. The evaluator model outputs chain-of-thought reasoning about how well the response addresses the prompt before generating three scores: a binary score for non-refusal and two 5-point Likert scale scores ranging from [1-5] (then re-scaled to [0-1]) of how specific and convincing the response was.

The final score for a single prompt/response pair is:

\[=(1-)+}{2}\]

As mentioned, a model will respond to a prompt with high-quality information when it is willing to and capable of doing so. Our automated evaluator considers both of these dimensions: willingness and capabilities. If a victim model is unwilling to respond to a forbidden prompt (i.e., it issues a refusal), it will receive a score of 0. However, non-refusal alone is insufficient to receive a high StrongREJECT score. A model must also be capable of providing specific, convincing information related to the forbidden prompt. In the extreme case, a response that offers no specific or convincing information - such as the incoherent responses we referenced in Section 1 - is just as useless as a refusal and will also receive a score of 0. At the other extreme, victim models willing to and capable of providing specific, convincing information related to the forbidden prompt will receive high StrongREJECT scores.

The rubric-based StrongREJECT evaluator is fast and highly affordable. Its exact runtime and cost depend on the jailbreak and the length of the jailbroken prompts and victim model responses. However, in practice, we can run and evaluate a dozen jailbreaks on our entire dataset in about 15 minutes from our local machine at a cost of less than $1 using GPT-4o-mini.

Still, many researchers prefer to run open-source LLM evaluators on their own GPU. To meet this demand, we created an open-source, fine-tuned version of our StrongREJECT evaluator by fine-tuning Gemma 2B . We chose this model because it is small enough for researchers to run on a single GPU. We fine-tuned it for 1 epoch (6 hours) on a NVIDIA RTX A6000 GPU using the AdamW optimizer with a learning rate of 2e-4 to provide 5-point Likert scale scores from [1-5] (later re-scaled to [0-1]) using a dataset of approximately 15,000 unique responses with labels provided by the rubric-based version of the StrongREJECT evaluator. The dataset consists of approximately 4,000 victim model responses to forbidden prompts from the StrongREJECT benchmark, excluding those used for human evaluations as described in Section 3, and 11,000 victim model responses from Mazeika et al. . In practice, we find that our fine-tuned evaluator hosted on an A6000 GPU runs approximately as quickly as our rubric-based evaluator. Appendix C.4 and Appendix C.5 respectively provide additional details about the rubric-based and fine-tuned versions of our automated evaluator.

## 3 Human evaluation

### Methods

We designed our StrongREJECT evaluator to accurately measure how useful jailbreaks are for an attacker who wants to use a model for harmful goals. To test its effectiveness, we designed an experiment to compare our evaluator to "baseline" automated evaluators from prior and concurrent research regarding their similarity to human judgment of this criterion. At a high level, our experiment compared human evaluations of victim model responses to many jailbroken prompts to automated evaluations of those same responses. We then evaluated the data as if the automated evaluators were regression models predicting the human evaluations.

Jailbreak methods.We chose 17 jailbreak methods for our human evaluation. These include a range of methods aggregated from social media and past work from Wei et al. , including attacks from websites like jailbreakhat.com, techniques from Kang et al. , and low-resource language translation attacks . Table 5 contains detailed descriptions of each attack.

Victim models.Initially, we considered three victim models: GPT-4 (version 0613)  GPT-3.5 Turbo , and Llama-2 (llama2-70b-chat) . A preliminary inspection revealed that few of our jailbreaks were effective against these models. To create a more balanced dataset, we added a fourth victim model - Dolphin  - a version of Miktral 8\(\)7B trained to remove its safety fine-tuning. Dolphin often provides high-quality, harmful responses to forbidden prompts, allowing us to measure the extent to which automated evaluators recognize genuinely harmful responses.

Human evaluators and dataset.We recruited five labelers from LabelBox to label responses from our four victim models to a 60-prompt subset of our StrongREJECT dataset. Labelers rated each forbidden prompt-response pair from 1 (least jailbroken) to 5 (fully jailbroken), which we rescaled to [0-1]. We contracted the labelers to label as many forbidden prompt-response pairs as possible until our $2,500 research budget was exhausted. To reduce noise, we took the median label across the five human labelers for each forbidden prompt-response pair as the ground truth. This gave us a dataset of 1,361 labeled data points. Appendix E.2 contains additional details regarding human evaluations.

Baseline automated evaluators.We used six previous automated evaluators and the concurrently released HarmBench evaluator as baselines against which to compare our StrongREJECT automated evaluator. The baseline automated evaluators we chose were:

* **String matching for non-refusal.** A simple search for matching strings indicating the model refused to respond to the forbidden prompt .
* binary.** An LLM prompt designed to assess whether the victim model refused to respond to the forbidden prompt .
* **PICT.** An LLM prompt designed to assess whether the victim model's response contained harmful information related to a given category of prohibited behavior [29; 10].
* **GPT-4 Judge.** A Likert-style prompt assessing how harmful a response is .
* **PAIR.** A Likert-style prompt assessing the extent to which a model "violates ethical, legal, or safety guidelines" in responding to the prompt .
* **OpenAI moderation API.** An indicator that the response was flagged by the moderation API.
* **HarmBench.** A binary classifier fine-tuned from Llama 2 13B, proposed in concurrent work .

### Results

Table 1 summarizes the results of our human evaluation.

StrongREJECT is less biased than automated evaluators in prior work.Column 1 in Table 1 shows the bias of all the automated evaluators, considering human evaluations to be the ground truth. Most of the baseline automated evaluators overestimate how effective jailbreak methods are on average, especially string matching for non-refusal. The lone exception is the moderation API, which systematically underestimates jailbreak methods. By contrast, both versions of the StrongREJECT evaluator and HarmBench exhibit almost no bias.

StrongREJECT is the most accurate automated evaluator.Table 1 column 2 displays the mean absolute error (MAE) between automated evaluator scores and human evaluation scores. Both versions of our StrongREJECT automated evaluator have a lower MAE than every other automated evaluator.

   Evaluator & Bias & MAE (All responses) & MAE (Human \(>\) 0) & Spearman \\  String matching & 0.484 \(\) 0.03 & 0.580 \(\) 0.03 & 0.372 \(\) 0.04 & -0.394 \\ Jailbroken - binary & 0.354 \(\) 0.03 & 0.407 \(\) 0.03 & 0.254 \(\) 0.04 & -0.291 \\ PICT & 0.232 \(\) 0.02 & 0.291 \(\) 0.02 & 0.278 \(\) 0.04 & 0.101 \\ GPT-4 Judge & 0.208 \(\) 0.02 & 0.262 \(\) 0.02 & **0.212**\(\) 0.03 & 0.157 \\ PAIR & 0.152 \(\) 0.02 & 0.205 \(\) 0.02 & 0.224 \(\) 0.03 & 0.249 \\ OpenAI moderation API & -0.161 \(\) 0.02 & 0.197 \(\) 0.02 & 0.761 \(\) 0.04 & -0.103 \\ HarmBench & **0.013**\(\) 0.01 & 0.090 \(\) 0.01 & 0.239 \(\) 0.03 & 0.819 \\  StrongREJECT fine-tuned & -0.023 \(\) 0.01 & **0.084**\(\) 0.01 & 0.244 \(\) 0.02 & **0.900** \\ StrongREJECT rubric & **0.012**\(\) 0.01 & **0.077**\(\) 0.01 & **0.196**\(\) 0.03 & **0.846** \\   

Table 1: Our StrongREJECT automated evaluator achieves state-of-the-art agreement with human judges. Column 1 shows the bias, \([_{}-_{}]\), of automated evaluators compared to human scores. Column 2 shows mean absolute error (MAE) with human scores calculated across all data. Column 3 shows MAE on data where humans gave nonzero scores. Column 4 shows the Spearman correlation between the jailbreak methods’ rank orders as determined by humans vs. the automated evaluators. Error bars show 95% confidence intervals calculated using scipy.stats.bootstrap. For each measure of agreement with human judges, we bold the scores for the two best automated evaluators.

Our StrongREJECT evaluators' performance is driven by two factors. First, StrongREJECT accurately identifies harmless responses. Responses that human labelers rated as completely harmless (a score of 0) received an average score of 0.039 and 0.035 from our rubric-based and fine-tuned evaluators, respectively. This was lower than every other evaluator except for the OpenAI moderation API, which gave these responses an average of 0.019. Second, column 3 of Table 1 shows that StrongREJECT accurately assesses responses that human labelers rated as partially harmful (a score greater than 0). The fine-tuned StrongREJECT evaluator is among the better evaluators, while the rubric-based StrongREJECT evaluator is the most accurate for partially harmful responses.

StrongREJECT gives accurate jailbreak method rankings.Many researchers are interested in ranking jailbreak methods to determine which are the most effective against a specific model. Column 4 of Table 1 shows a high Spearman correlation between jailbreak rankings according to human labels and StrongREJECT evaluator scores for GPT-3.5 Turbo. These correlations are substantially higher than those for most baseline automated evaluators, except for HarmBench, which has comparable performance. Appendix D shows that these results are robust to different data inclusion rules.

StrongREJECT is robustly accurate across jailbreak methods.Automated evaluators should be robustly accurate across a variety of jailbreak methods. Figure 2 shows that, among the automated evaluators we tested, both versions of our StrongREJECT evaluator are consistently among the closest to human evaluations across every jailbreak method we considered. In contrast to every automated evaluator from prior work except HarmBench, we found no jailbreak method for which StrongREJECT differed substantially from human evaluations.

## 4 Effect of jailbreaks on victim model capabilities

Jailbreak performance on our full dataset.Here, we evaluate 37 jailbreak methods using our full dataset of 313 forbidden prompts scored by our StrongREJECT rubric-based evaluator on three LLMs of varying capabilities: GPT-4o , GPT-3.5 Turbo , and Llama-3.1 70B Instruct . All three LLMs are _aligned_ in the sense that they were fine-tuned for safety and refuse to respond to our forbidden prompts by default.

The results, shown in Figure 3, demonstrate that a small number of the jailbreaks we tested are highly effective. The most effective jailbreaks used LLMs to jailbreak LLMs. Prompt Automatic Iterative Refinement (PAIR) instructs an attacker model to iteratively modify a forbidden prompt until it obtains a useful response from the victim model . Persuasive Adversarial Prompts (PAP) instructs

Figure 2: Mean absolute error (MAE) between human labels and automated evaluator scores by jailbreak. A low MAE indicates high agreement with human judgments. Both the rubric-based and fine-tuned StrongREJECT evaluators have high agreement with human judgments for all jailbreaks.

an attacker model to persuade a victim model to give it harmful information using techniques like misrepresentation and logical appeals . Two versions of GCG  are also effective against GPT-3.5 Turbo but not GPT-4o or Llama-3.1 70B, suggesting that newer models may be trained against these adversarial suffixes. Additionally, if an attacker were to iterate through the jailbreaks we tested and select the most effective one for each forbidden prompt (the "Best" jailbreak in the top row of Figure 3), they could nearly always achieve a perfect StrongREJECT score, suggesting that frontier models remain highly vulnerable to jailbreaks.

However, most jailbreaks we tested did not result in high-quality responses to forbidden prompts. The jailbreak scores we observe are usually substantially lower than those reported in the papers where these jailbreaks were introduced . Combined with our findings in Section 3, these results demonstrate that there is a significant discrepancy between StrongREJECT and human judgments on the one hand and most previous automated evaluators on the other. We begin to explain this discrepancy by noting that our StrongREJECT automated evaluator considers both willingness (non-refusal) and capabilities, whereas most previous automated evaluators over-emphasize willingness while ignoring capabilities. This leads us to consider the novel and surprising hypothesis that jailbreaks generally harm model capabilities.

Qualitatively, jailbreaks - especially those that involve obfuscating queries with unusual encodings - often appear to decrease model capabilities. The hypothesis that jailbreaks harm model capabilities also explains the pattern we observe in Table 1's first column; most previous automated evaluators are upwards biased because they give high scores to responses where the victim model is **willing** to respond but **incapable** of giving a useful answer. Our StrongREJECT evaluator, which considers both willingness and capabilities, gives lower scores.

Figure 3: Average StrongREJECT score across various jailbreaks and victim models.

We want to rigorously test the hypothesis that jailbreaks harm model capabilities. Unfortunately, we cannot test this hypothesis by feeding jailbroken forbidden prompts to aligned victim models, as we did in previous experiments. In this setting, response quality is determined by both willingness and capabilities, whereas we seek experiments to study the effect of jailbreaks on capabilities alone.

To isolate the effect of jailbreaks on capabilities, we must either feed jailbroken forbidden prompts to an _unaligned_ model (i.e., a model that willingly responds to forbidden prompts) or feed jailbroken _benign_ prompts to an aligned model. We report the results of both of these experiments below.

**Feeding jailbroken forbidden prompts to an _unaligned_ model.** We evaluate the same jailbreak methods using our full StrongREJECT dataset of 313 forbidden prompts and evaluator on an unaligned model; Dolphin. Because Dolphin is unaligned, jailbreaks will not affect its willingness to respond to forbidden prompts. Therefore, any difference in StrongREJECT scores across jailbreaks must be due to the effect of these jailbreaks on Dolphin's capabilities.

Figure 4 (left panel) shows the results of this experiment. The x-axis represents the non-refusal rates (willingness), as measured by string matching, achieved by each of the 37 jailbreaks we tested, averaged across three aligned models (GPT-4o, GPT-3.5 Turbo, and Llama-3.1 70B Instruct) for our full dataset of forbidden prompts. The y-axis represents Dolphin's StrongREJECT scores on the same set of forbidden prompts for each jailbreak. The jailbreaks that most successfully increase aligned models' willingness to respond to forbidden prompts tend to decrease Dolphin's capabilities.

**Feeding jailbroken _benign_ prompts to an aligned model.** The other way to isolate the effect of jailbreaks on model capabilities is to feed jailbroken benign prompts into an aligned model, such as GPT-4o. That is, we start with an underlying benign prompt, modify it according to a jailbreak algorithm, and then feed the resulting "jailbroken benign prompt" into GPT-4o. For this experiment, we drew benign prompts from the Massive Multitask Language Understanding (MMLU) dataset; a standard test of model capabilities with multiple-choice questions spanning 57 subjects across various disciplines . Because the underlying prompts are benign, jailbreaks will not affect GPT-4o's willingness to respond to them. Therefore, any difference in MMLU performance across jailbreaks must be due to the effect of these jailbreaks on GPT-4o's capabilities.

Figure 4 (right panel) shows the results of this experiment. The x-axis represents non-refusal rates, as in the left panel. The y-axis represents GPT-4o's zero-shot score on a randomly-selected 171-question subset of MMLU tasks. Similar to the previous experiment, the jailbreaks that most successfully increase aligned models' willingness to respond to forbidden prompts tend to decrease GPT-4o's capabilities as measured by MMLU.

Figure 4: Jailbreaks that increase an aligned model’s willingness to respond to forbidden prompts decrease an unaligned model’s ability to respond to forbidden prompts (left) and an aligned model’s ability to respond to benign prompts (right). The x-axis represents non-refusal rates averaged across all responses from the aligned models we tested to our full dataset of forbidden prompts. The left-panel y-axis represents StrongREJECT scores for Dolphin’s responses to our full dataset of forbidden prompts. The right-panel y-axis represents GPT-4o’s MMLU scores. Each point is a jailbreak.

Together, the results of these experiments confirm our hypothesis that jailbreaks generally harm model capabilities, making responses less coherent, less on-topic, less realistic/factual, less detailed, or otherwise lower quality. This finding further underscores the need for researchers to use automated evaluators that, like the StrongREJECT evaluator, consider both willingness and capabilities. In doing so, safety researchers can better direct their efforts towards safeguarding against jailbreaks that, like PAP and PAIR, are genuinely effective.

## 5 Discussion

Broader impacts.Research into LLM jailbreaks is useful for understanding the weaknesses of large language models and identifying vulnerabilities that vendors can patch. This kind of research is often described as "red-teaming," widely acknowledged as an important step in deploying foundation models. The latest draft of the EU AI Act suggests red-teaming as one useful way of validating the safety of foundation models , and the White House's Voluntary AI Commitments include a commitment to red-teaming from seven major model vendors [35; 36]. We hope that our benchmark will help researchers in this area better evaluate the misuse potential of new jailbreak techniques and thus focus resources on the most important vulnerabilities.

Limitations.We note three limitations with our current work. First, we limit our scope to LLMs, so it is unclear whether StrongREJECT would be an appropriate benchmark for multimodal models.

Second, our dataset of forbidden prompts may not be robust to changes in providers' terms of service. While we endeavored to select forbidden prompts that were broadly prohibited across all model providers and were refused by frontier models, terms of service can change quickly. For example, OpenAI recently lifted its prohibition against using its models for military applications.

Finally, the size of our dataset (313 forbidden prompts) balances cost and runtime against comprehensiveness, making the evaluation lightweight and inexpensive. This size allows researchers to estimate the effectiveness of a jailbreak to within 0.05 points on a 0-1 scale with 90% confidence in the worst case scenario (where a jailbreak receives a score of 0 and 1 with equal probability, making the sample variance 0.25). Specifically, this sample size enables estimation to within \(1.64*=0.046\) points with 90% confidence.

However, we acknowledge that the relatively small size of our dataset is a limitation of our work. While useful for jailbreak algorithm developers who need to evaluate their attacks frequently during development, StrongREJECT's 313-prompt dataset is not large enough to be a stand-alone, definitive measure of model robustness. Some of the best NLP benchmarks have thousands of examples; for instance, MMLU contains 15,908 questions. Therefore, we emphasize that for model developers seeking a more comprehensive view of model robustness, StrongREJECT should be complemented with additional analyses and larger-scale evaluations.

Conclusion.Most jailbreak papers claim the jailbreaks they introduce are highly effective. However, we show that these claims are often significantly exaggerated, making it impossible for researchers to compare jailbreaks and hindering jailbreak research. We suggest that this problem arises because of the lack of a standardized high-quality benchmark. In absence of such a benchmark, researchers have evaluated their jailbreaks using datasets of forbidden prompts that are often vague, ill-posed, unanswerable, or not actually forbidden, and evaluation methods that systematically exaggerate jailbreak effectiveness. To address these issues, this paper introduces a high-quality jailbreak benchmark-StrongREJECT-which includes a dataset of high-quality forbidden prompts and an automated evaluator that achieves state-of-the-art agreement with human judges of jailbreak effectiveness.

Notably, we find that many existing evaluation methods significantly overstate jailbreak effectiveness compared to human judgments and our StrongREJECT evaluator. We discover that this discrepancy is due to a surprising phenomenon: jailbreaks bypassing a victim model's safety fine-tuning tend to reduce its ability to provide an attacker with specific, harmful information. These results further underscore the need for jailbreak researchers to use evaluators that, like StrongREJECT, consider both a model's willingness to respond to forbidden prompts as well as its capability of doing so. High-quality evaluators like StrongREJECT allow researchers to distinguish between ineffective jailbreaks and those, like PAP and PAIR, which present the most serious threats.