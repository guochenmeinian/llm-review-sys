# Language Generation in the Limit

Jon Kleinberg

Departments of Computer Science

and Information Science

Cornell University

Ithaca NY &Sendhil Mullainathan

Booth School of Business

University of Chicago

Chicago IL

###### Abstract

Although current large language models are complex, the most basic specifications of the underlying language generation problem itself are simple to state: given a finite set of training samples from an unknown language, produce valid new strings from the language that don't already appear in the training data. Here we ask what we can conclude about language generation using only this specification, without further assumptions. In particular, suppose that an adversary enumerates the strings of an unknown target language \(L\) that is known only to come from one of a possibly infinite list of candidates. A computational agent is trying to learn to generate from this language; we say that the agent _generates from \(L\) in the limit_ if after some finite point in the enumeration of \(L\), the agent is able to produce new elements that come exclusively from \(L\) and that have not yet been presented by the adversary. Our main result is that there is an agent that is able to generate in the limit for every countable list of candidate languages. This contrasts dramatically with negative results due to Gold and Angluin in a well-studied model of language learning where the goal is to identify an unknown language from samples; the difference between these results suggests that identifying a language is a fundamentally different problem than generating from it.

## 1 Introduction

The recent advances in large language models (LLMs) have been remarkable, sparking active lines of theoretical work into their performance. These investigations implicitly revolve around two fundamental questions: how do we formally reason about the effectiveness of LLMs; and within such a framework, what are the core ideas at a mathematical level that enable their performance?

Answers to these questions must begin by formalizing the specification for what a generative algorithm for language should be doing. Here, we propose starting from a very basic, assumption-free, statement for such a specification: there is an unknown target language \(L\), over time the algorithm sees a sequence of strings from \(L\), and eventually we would like the algorithm to generate new strings from \(L\) that it has not seen before.1

Viewed this way, it is also clear why it seems so remarkable for LLMs to be doing well at such a problem. The fully general statement of the problem feels unsolvable: if we know nothing about the unknown target language \(L\), then how can a generative algorithm reliably produce valid strings from \(L\) that it hasn't seen before?Language Learning in the Limit.In fact, there is a well-established formalism that allows us to phrase this point precisely: the classical model of language learning in the limit, formulated by Mark Gold in 1967 and fully characterized by Dana Angluin in 1980 . In this model, there is an unknown language \(K\) that is known only to be produced by one of a list of candidate representations \(R_{1},R_{2},R_{3},\), where \(L_{i}\) is the language produced by representation \(R_{i}\). We can think of this list of representations as the set of all possible context-free grammars, or the set of all possible finite automata, or the set of all Turing machines with a fixed space bound, or any other generative model that produces strings; in fact, the formal result is much more general than this, in that it is sufficient to suppose that the unknown language \(K\) simply comes from a countable list of candidate languages \(L_{1},L_{2},L_{3},\), and we can dispense with explicit representations altogether.2

In the Gold-Angluin model, an adversary enumerates the strings of \(K\) one by one, and the algorithm is required after each new string to guess a language \(L_{i}\) from the list such that \(L_{i}=K\). If there is some finite step \(t\) after which the algorithm's guess is always correct, then we say the algorithm has _identified \(K\) in the limit_. Gold proved that this is impossible in general, even for simple language families such as the regular languages (i.e. those produced by finite automata), and Angluin characterized precisely those families for which it is possible, further establishing how limited they are . Note, crucially, that in the Gold-Angluin model, the adversary enumerates strings in \(K\), but does not provide examples of strings that do not belong to \(K\), nor does it allow the algorithm to ask questions about a string's membership in \(K\); their point with this formalism was to focus on cases where an agent tries inferring a language purely from seeing a sufficient number of examples of strings that belong to the language. (In Section A.1, we provide a self-contained proof of the negative result for identification in the limit; while not strictly necessary for our results, we find it provides useful background and context for the problem.)

Our Results: Language Generation in the Limit.These negative results of Gold and Angluin feel intuitive -- how should we be able to identify a language from a finite sample when we are allowed to make essentially no assumptions about its structure? Because of this intuition, both via the Gold-Angluin model and for more informal reasons as well, the focus in language generation has naturally turned to distributional assumptions; one posits that large language models are so effective because they are able to exploit distributional probabilities of language, and from a finite set of samples they are able to estimate conditional probabilities of strings with increasing accuracy. In this way, the question moves from adversaries to probability distributions, and one seeks explanations for the effectiveness of LLMs through underlying probabilistic models.

In this paper, we offer a sharply different view: we show that in the Gold-Angluin model of adversarially produced examples, _language generation is always possible._ We will provide full details on the result and its proof beginning in the next section, but the key point at a high level is that even in an adversarial model with an unknown language \(K\), language generation is a fundamentally different task than language identification: where identification asks an algorithm to eventually name a language \(L_{i}=K\) after seeing a large enough finite sample \(S\) from \(K\), generation instead asks an algorithm to eventually output strings in \(K-S\) after seeing a large enough \(S\) from \(K\). Our main result is that this difference in specifications leads to dramatic differences in what is possible in the limit; whereas the Gold-Angluin results establish that identification in the limit is impossible except in highly constrained cases, we show that generation in the limit is possible for _every_ countable list of candidate languages.

General Connections to Language Modeling.Our approach emphasizing theoretical properties of language generation and worst-case guarantees, in the style of the Gold-Angluin model, is a source of limitations but also a source of generality; it is therefore important to discuss how we draw stylized insights about language modeling from our theoretical formalism. Clearly, methods to design large language models in practice make extensive use of the empirical distributional properties of language, as they should. Our results don't question this design methodology; when there are genuine empirical regularities in the training data, there is no reason not to use them. Rather, our results argue that if we are looking for the essential reasons why language generation is tractable, we do not fundamentally require any empirical regularities, or indeed any probabilistic assumptions at all; there is instead a formal sense in which language generation -- unlike broader learning tasks such as language identification -- is possible even against an adversary presenting positive training examples in a worst-case fashion. In some essential way, the generation problem is therefore different from these other learning tasks in crucial ways that more detailed formalisms may potentially obscure.

Despite the generality of the model, producing the generation algorithm that proves our main theorem makes use of subtle structural properties of the given list of candidate languages. Again, we defer any detailed description to the subsequent sections, but the idea at a high level is to maintain a sequence of "provisional languages" among the candidate languages that are consistent with the finite sample \(S\) from \(K\) seen so far, and to continually refine this sequence of provisional languages as the adversary adds more strings to the sample \(S\). Since the Gold-Angluin result says that the algorithm can never be sure which is the true language \(K\), there is a sense in which this refinement process essentially needs to continue indefinitely, and in general it leads the algorithm to generate from provisional languages that may be increasingly "thin" subsets of \(K\). This does not cause trouble for the specification of language generation, since it is acceptable to produce any unseen string from \(K\), but it does mean that while the algorithm is able to eventually produce an infinite sequence of unseen strings from \(K\), in general it might do so from a narrow part of \(K\).

This property of the solution in the presence of an adversary suggests interesting connections to the problem of generation in practice as well. In particular, any method for generation has to deal with an underlying _validity problem_ -- producing valid outputs -- and an underlying _breadth problem_ -- producing outputs that represent the full range of valid outputs in some reasonable way. The breadth problem is notoriously difficult, and it manifests itself in numerous ways in the methodology of machine learning and generative models. The approach that proves our main result helps illustrate the tension between validity and breadth even in settings with worst-case assumptions rather than probabilistic ones, and this tension shows up in both the early phases of our algorithm's execution and the later phases. In the early phases, before the algorithm has refined its provisional language sufficiently, it is generating too broadly and producing strings that are not part of the target language \(K\) -- an analogue at a high level of a kind of hallucination in which the generated strings belong to some consistent candidate language, but not to the actual target language [7; 8; 11]. In the later phases, on the other hand, the algorithm continuously shrinks its range of possible outputs so as to ensure that they will be contained within \(K\) -- sacrificing validity for breadth in a manner analogous to the issues that arise in the problem of mode collapse for generative models [3; 4]. Our model therefore suggests interesting questions about the fundamental trade-offs that may exist between validity and breadth even in settings without an underlying probabilistic model.

## 2 Formal Model and Results

We now provide a formal description of the model and the statement of our results. To begin with, we have a countable list of candidate languages \(=\{L_{1},L_{2},L_{3},\}\), where each \(L_{i}\) is a subset of some countable set \(U\). All we assume about the list of languages is that it is specified through a black box that can answer questions of the form "Is \(w L_{i}\)?" for any string \(w U\) and language \(L_{i}\). (If the reader finds it helpful for concreteness, they can consider the results that follow in the context of a specific list of languages \(\), such as the set of all context-free languages or the set of all regular languages; but everything we say applies to general collections of languages.) We will allow the collection \(}\) to contain repetitions, in that we may have \(L_{i}=L_{j}\) for different indices \(i\) and \(j\). We will assume that all the languages \(L_{i}\) are infinite; while the original Gold-Angluin framework did not require this, it becomes important in specifying the generation problem: if we require an algorithm to output unseen strings forever, then this is not possible from a finite language, where the algorithm would eventually run out of new strings to generate.

An adversary and an algorithm now play the following game. The adversary chooses a language \(K\) from \(\) without revealing it to the algorithm, and it begins enumerating the strings of \(K\) one by one over a sequence of steps \(t=1,2,3,\). The adversary can repeat strings in its enumeration, but crucially, for every string \(w K\), there must be at least one time step \(t\) in which \(w\) appears. Let \(S_{t}\) be the set of strings that the adversary has enumerated in steps 1 through \(t\).

Identification and Generation.In this framework, we can now specify both the Gold-Angluin problem of identification and the contrasting problem of generation that we study in this paper.

_Identification (from :_ In each step, the algorithm observes \(S_{t}\) and must output an index \(i\) (its guess for the true language \(K\)). The algorithm _identifies \(K\) in the limit_ if there is some \(t^{*}\) such that for all steps \(t t^{*}\), the algorithm's guess in step \(t\) is an index \(i\) for which \(L_{i}=K\).
* _Generation (from the present paper):_ In each step, the algorithm observes \(S_{t}\) and must output a string \(a_{t}\) (its guess for an unseen string in \(K\)). The algorithm _generates from \(K\) in the limit_ if there is some \(t^{*}\) such that for all steps \(t t^{*}\), the algorithm's guess \(a_{t}\) belongs to \(K-S_{t}\).

A key point in both problem formulations is that the algorithm is not told if its guesses are correct.

We know from the Gold-Angluin results that there is no algorithm that can achieve identification in the limit for an arbitrary countable collection \(\) of languages (or even for specific countable collections, like the set of all regular languages or the set of all context-free languages). Our main result is a dramatically different answer for language generation; it is possible for every countable collection:

**(2.1)** _There is an algorithm with the property that for any countable collection of languages \(=\{L_{1},L_{2},L_{3},\}\), and any enumeration of one of these languages \(K\), the algorithm generates from \(K\) in the limit._

A Result for Finite Collections.We prove a second result as well, focusing on the variant of the problem in which the collection of languages \(\) is finite. In this case, it follows from Angluin's characterization that every finite collection \(\) allows for identification in the limit. Given this, what more might we ask for? A natural question is whether there is a _uniform_ bound on the number of samples needed to ensure that the algorithm can correctly identify the true language \(K\): for any finite collection \(\), is there a bound \(t()\) and an algorithm with the property that after seeing any \(t()\) distinct strings from \(K\), the algorithm is guaranteed to report \(K\) as its guess for the true language?

It is easy to see that for the Gold-Angluin model of learning, this is not possible. For example, suppose that \(\) is the collection consisting of two languages \(L_{1}\) and \(L_{2}\): \(L_{1}\) consists of all possible strings, and \(L_{2}\) consists of all strings of even length. Suppose there were a bound \(t()\) and an algorithm that was guaranteed to guess correctly after seeing \(t()\) distinct samples. Then an adversary could present \(t()\) distinct strings of even length, and then ask the algorithm to guess whether the true language is \(L_{1}\) or \(L_{2}\): if the algorithm guesses \(L_{2}\) at this point, then the adversary could announce that the answer is \(L_{1}\), and conversely if the algorithm guesses \(L_{1}\). This does not prevent the algorithm from learning the true language in the limit (since the adversary must eventually output an odd-length string if the true answer is \(L_{1}\)); but there is no fixed bound \(t()\) by which this can be guaranteed.

However, for the problem of generation with a finite collection of candidate languages, we can provide this much stronger type of uniform bound, via an algorithm that generates correctly after seeing a finite sample whose size \(t()\) is specified at the outset. In fact, we can achieve more: after seeing this finite sample, the algorithm can generate an infinite sequence of unseen elements from the true language.

**(2.2)** _There is an algorithm with the property that for any finite collection of languages \(\), there is a number \(t()\), such that for any language \(K\) in \(\), and any sequence \(S\) of at least \(t()\) distinct elements from \(K\), the algorithm can produce an infinite sequence of distinct strings from \(K-S\)._

In subsequent work building on the present paper, Raman and Tewari  have recently proved further results about the type of _uniform generation_ that we consider in (2.2), when it is possible to put an a priori bound \(t()\) on the number of distinct strings an algorithm needs to see from \(K\) before it can be guaranteed to begin generating unseen strings from \(K\). They also consider variants of this definition, situating their analysis in the framework of classical learning theory.

Extensions and Generalizations.Following these two main results in our basic model of generation, we provide (in Section 7) the following generalization of the model. Specifically, a familiar issue from language generation applications is the role of the _prompt_: a user provides an input string \(p\), and a generation algorithm is then supposed to produce a "continuation" string \(c\) to come after the prompt, so that the concatenation of \(p\) and \(c\) is a valid utterance. We offer an extension that incorporates the notion of prompting, while maintaining the basic structure of the model, and we show how to formulate and prove a generalization of our first main result (2.1) in a setting where at each time step the adversary is allowed to specify a prompt that must be completed.

An Approach to Generation that Doesn't Work

The following section is important, because it describes an approach that is arguably the most natural non-trivial attempt at achieving language generation in the limit. It seems at first seems to solve the problem directly, but in fact it fails for a deep reason that is important to understand, since it motivates the more involved solution that follows.

The strategy is to move through the list of languages \(=\{L_{1},L_{2},L_{3},\}\) in order, treating each language \(L_{i}\) as a hypothesis for \(K\) until the sample \(S_{t}\) proves otherwise. That is, we start with \(L_{1}\), and we generate strings from \(L_{1}-S_{t}\) until we encounter (if ever) a step \(t\) in which \(S_{t} L_{1}\). At this point we know that \(L_{1}\) cannot be the true language \(K\), and so we continue the process from \(L_{2}\). The nice idea that underpins this strategy is that the true language \(K\) is equal to \(L_{z}\) for some index \(z\). (Since \(\) can contain repetitions, \(K\) might appear several times, but we can take \(L_{z}\) as the first appearance.) So if our process were to reach \(L_{z}\) at some step \(t^{*}\), it would never move on from \(L_{z}\), and so we would be generating from \(K-S_{t}\) for all \(t t^{*}\).

Unfortunately, there is a deep problem with this approach: there may be a language \(L_{i}\) with the property that \(L_{i}\) comes before \(L_{z}\) and \(L_{i}\) properly contains \(L_{z}\) (that is, \(i<z\), and \(L_{z} L_{i}\)). In this case, our procedure would stop at the first such \(L_{i}\) forever, since it would never encounter a string in \(S_{t}\) that didn't belong to \(L_{i}\). And when it generated from \(L_{i}-S_{t}\), there is no guarantee that it would choose strings from \(L_{z}\).

This problem is not easily avoided, since if this approach worked as written, it would also solve identification in the limit, which we know is impossible. So we need to extract some of the useful ideas from this failed approach -- in particular, the trick that \(K\) appears at some finite point in the list \(\), as the language \(L_{z}\) -- but add important further ideas as well. Specifically, if the algorithm is maintaining hypotheses for the true language \(K\) over time, it can provably never know whether its current hypothesis is correct; instead, it must be always moving further down the collection of languages, potentially considering languages that are not \(K\), but in such a way that it is eventually always generating from \(K-S_{t}\). This is what our proof beginning in the next section will have to accomplish.

## 4 Generation in the Limit via a Function

We prove our main result in two parts. We first give a method for language generation in the limit that is not concerned with the computational power required by the agent performing the generation. Thus, rather than an algorithm to generate the string, we ask whether we can construct a function \(f_{}\) based on the given language collection that maps a finite set of strings to a new string; this function takes the strings \(S_{t}\) seen so far and outputs a string \(f_{}(S_{t})\) intended to be in \(K-S_{t}\). We will prove the following:

**(4.1)**_For every countable collection of languages \(\), there is a function \(f_{}\) from finite subsets of \(U\) to elements of \(U\), such that for every enumeration of a language \(K\), there is a \(t^{*}\) such that for all \(t t^{*}\), we have \(f_{}(S_{t}) K-S_{t}\)._

Note that while this positive result is not concerned with the computational power required to evaluate \(f_{}\), it already contains the core contrast with language identification, which remains impossible even if we simply ask for a function \(f_{}\). In the next section, we will then go on to prove (2.1) by using an algorithm that only performs standard computational steps and membership queries of the form "\(w L_{i}\)?"

Minimal and critical languages.As before, we will suppose \(z\) is an index such that \(L_{z}=K\). We say that a language \(L_{i}\) is _consistent_ with the sample at step \(t\) if \(S_{t} L_{i}\). An important idea, which is implicit in our discussion of the failed approach at the end of Section 2, is that if \(L_{i} L_{j}\) are both consistent with \(S_{t}\), then it is safer for an algorithm to generate from \(L_{i}\) than from \(L_{j}\): if \(w L_{j}-S_{t}\) then we must also have \(w L_{i}-S_{t}\). This suggests that it would be useful to find consistent languages that are _minimal_ with respect to inclusion: we would say that \(L\) is _minimal_ if \(L\) is consistent with \(S_{t}\), and also \(L L^{}\) for every \(L^{}\) that is consistent with \(S_{t}\). Unfortunately, this is too much to ask for, since there exist instances of the problem for which there might not be any languages that are minimal with respect to inclusion. (In a finite collection of language there would need to be a minimal language, but it is easy to construct infinite collections without one.)Therefore, we define a related concept that only involves examining the inclusion of a given language with respect to a finite set of other languages. Specifically, we look for languages \(L_{n}\) that are consistent with \(S_{t}\) in a given step \(t\), such that \(L_{n}\) is a subset of every consistent language that _precedes_ it in the indexing of \(\). We will say that such a language is _critical_ at step \(t\). To define this formally, we first let \(_{n}\) denote the finite collection of languages \(\{L_{1},L_{2},,L_{n}\}\). We now have the following definition.

**(4.2)**_A language \(L_{n}\) is critical at step \(t\) if \(L_{n}\) is consistent with \(S_{t}\), and for every language \(L_{i}_{n}\) that is consistent with \(S_{t}\), we have \(L_{n} L_{i}\)._

Properties of critical languages.At any given step \(t\), there is at least one language consistent with \(S_{t}\), since the language \(L_{z}=K\) is always consistent with \(S_{t}\). It follows that there is also at least one critical language at any step \(t\): for any \(t\), the consistent language \(L_{i}\) with the lowest index \(i\) must be critical at step \(t\), as it is the only consistent language in \(_{i}\).

Note that there can be choices of \(t\) for which the language \(L_{z}=K\) is not critical at step \(t\). But a crucial fact is that \(L_{z}\) will eventually become critical at some step \(t\) and remain critical forever after that. For reasons of space, we provide complete proofs for this and all subsequent results in the Appendix, in this case in Section A.2.

**(4.3)**_There exists a time step \(t^{+}\) such that for all \(t t^{+}\), the language \(L_{z}\) is critical at step \(t\)._

There can be multiple critical languages at a given step \(t\); for example, if on the step \(t^{+}\) in (4.3) the first consistent language \(L_{i}\) is not equal to \(L_{z}\), then both \(L_{i}\) and \(L_{z}\) will be critical at step \(t^{+}\). Despite the potential multiplicity of critical languages, the collection of all critical languages at step \(t\) has a useful nested structure: \(L_{i}\) and \(L_{j}\) are both critical at step \(t\), with \(i<j\), then since \(L_{j}\) is contained in all consistent languages that precede it, in particular it is contained in \(L_{i}\). We therefore have:

**(4.4)**_Let \(i<j\), and suppose that \(L_{i}\) and \(L_{j}\) are both critical at step \(t\). Then \(L_{j} L_{i}\)._

A function for generation in the limit.At a given step \(t\), suppose that the critical languages are \(L_{n_{1}},L_{n_{2}},L_{n_{3}},\) where \(n_{1}<n_{2}<n_{3}<\). (This list of critical languages might be finite or infinite.) Then (4.4) tells us that this sequence is nested by inclusion: \(L_{n_{1}} L_{n_{2}} L_{n_{3}}\).

By (4.3) we know that the language \(L_{z}\) will eventually appear on this nested list from some step \(t^{+}\) onward, but even then we do not know which index \(n_{i}\) it corresponds to at any given step \(t t^{+}\). Indeed, to recall a point from earlier, the Gold-Angluin results for learning in the limit tell us that we can never know for sure which index corresponds to \(L_{z}\). But we now arrive at the crucial point, which is that beyond some finite index, all the critical languages are subsets of \(L_{z}\), so it is safe to generate from any of them.

Given this, we are prepared to construct our function \(f_{}\).

**(4.5)**\(f_{}(S_{t})\) _is defined as follows. We first identify all languages in \(_{t}\) that are critical at step \(t\). (If no such languages exist -- which can only happen if none of them are consistent with \(S_{t}\) -- we define \(f_{}(S_{t})\) arbitrarily.) Among these critical languages, let \(L_{n_{t}}\) be the one with the largest index \(n_{t} t\). We define \(f_{}(S_{t})\) to be the lowest-indexed element of \(L_{n_{t}}-S_{t}\)._

Finally, to establish our initial claim (4.1), it is sufficient to prove the following (in Section A.2):

**(4.6)**_For any language \(L_{z}\) and any enumeration of \(L_{z}\), there is a \(t^{*}\) such that for all \(t t^{*}\), we have \(f_{}(S_{t}) L_{z}-S_{t}\)._

As a final note, we observe that the current formulation of \(f_{}\) allows it to generate the same string more than once, provided that this string is in \(K-S_{t}\). However, it is not hard to modify \(f_{}\) so that it generates a different string each time, essentially just by defining it so that it generates the lowest-indexed element that it hasn't already generated.

The computational power required to produce \(f_{}\).Our plan was to construct \(f_{}\) without worrying about the computational power required to do so (and recalling that for comparison, in the corresponding problem of identification in the limit, no function \(f_{}\) achieving identification could exist regardless of the computational power required to produce it). Now that we've constructed an appropriate \(f_{}\), we can ask what was in fact required computationally.

In addition to standard computational steps and membership queries of the form "\(w L_{i}\)?", the definition of \(f_{}(S_{t})\) requires that we identify the critical languages in \(_{t}\). From the definition, we can do this provided we can answer a finite number of _subset queries_ of the form "\(L_{i} L_{j}\)?". So an algorithm augmented with the power to perform such subset queries can perform generation in the limit.

In the next section, we will show how to remove the necessity for subset queries, so that generation in the limit can be performed by an algorithm using only standard computational steps and membership queries.

## 5 Generation in the Limit via an Algorithm

We now prove (2.1) by giving an algorithm that generates in the limit for any countable collection of languages \(\), using only standard computational steps and membership queries of the form "\(w L_{i}\)?"

The set of possible strings \(U\) can be written as \(U=\{u_{1},u_{2},u_{3},\}\), and for simplicity we will also use the language of the positive integers to describe \(U\), treating \(u_{i}\) as the number \(i\). In an enumeration of the true language \(L_{z}=K\), let the sequence of strings that are enumerated step by step be denoted \(w_{1},w_{2},w_{3},\).

Extending definitions to finite subsets of languages \(L_{i}\).One important idea in designing the algorithm is to work with finite subsets of the languages in \(\), gradually expanding the size of these subsets. Thus, for a language \(L_{i}\) and a number \(m\), we will use \(L_{i}[m]\) to denote the finite set \(L_{i}\{1,2,3,,m\}\). We extend definition (4.2) of _critical languages_ from the previous section to handle finite subsets.

**(5.1)**_Let \(t\) and \(m\) be positive integers. A language \(L_{n}\) is \((t,m)\)-critical if \(L_{n}\) is consistent with \(S_{t}\), and for every language \(L_{i}_{n}\) such that \(L_{i}\) is consistent with \(S_{t}\), we have \(L_{n}[m] L_{i}[m]\)._

Since \(L_{n} L_{i}\) implies that \(L_{n}[m] L_{i}[m]\) for any \(m 1\), we have the following analogue of (4.3).

**(5.2)**_There exists a time step \(t^{+}\) such that for all \(t t^{+}\) and all \(m 1\), the language \(L_{z}\) is \((t,m)\)-critical._

The analogue of (4.4) also still holds with this definition, using the same proof.

**(5.3)**_Let \(i<j\) and suppose that \(L_{i}\) and \(L_{j}\) are both \((t,m)\)-critical. Then \(L_{j}[m] L_{i}[m]\)._

Finally, there is a basic monotonicity property of \((t,m)\)-criticality that is useful to write down explicitly; we give the proof in Appendix A.3.

**(5.4)**_Suppose that \(L_{n}\) is \((t,m)\)-critical, and \(m^{}<m\). Then \(L_{n}\) is \((t,m^{})\)-critical._

### An algorithm for generation in the limit

We now describe an algorithm for generation in the limit. As before, \(S_{t}=\{w_{1},w_{2},,w_{t}\}\) is the subset of \(K\) enumerated through step \(t\), treating the \(w_{i}\) as integers. We will consider the languages in \(_{t}\) in step \(t\), and maintain an auxiliary variable \(m_{t}\), roughly corresponding to how large a prefix \(L_{i}[m]\) we consider from each language \(L_{i}_{t}\).

At the start of step \(t\), we set \(m_{t}=(m_{t-1},w_{t})\); note that by induction this implies \(m_{t}_{t^{}<t}w_{t^{}}\). (Later we will increment \(m_{t}\), so we should think of it as a variable in the algorithm whose value can be modified.) We then determine which \(L_{i}_{t}\) are consistent with \(S_{t}\); note that by the definition of \(m_{t}\), it is sufficient to perform membership queries for only the finite set of elements in \(L_{i}[m_{t}]\) in order to do this. If there are no consistent languages in \(_{t}\), then we output a string arbitrarily.

Otherwise, there is at least one language consistent with \(S_{t}\), and so there is at least one \((t,m)\)-critical language for any choice of \(m\), since the first consistent language is \((t,m)\)-critical for all \(m\). Our goal is to imitate the plan from (4.5) and generate a new string from the highest-indexed critical language. But to do this, we have to find a new string, and this will in general require performing additional membership queries.

Generating a string.For any choice of \(m\), let \(n_{t}(m)\) be the maximum index of a \((t,m)\)-critical language from \(_{t}\); as noted above, \(n_{t}(m)\) is well-defined since we are in the case where at least one language in \(_{t}\) is consistent with \(S_{t}\), and so the first consistent language is \((t,m)\)-critical for all \(m\). We now search for a string to generate as follows.

We maintain a counter \(m\) that begins at \(m_{t}\) and gets incremented iteratively, with each iteration doing the following:

* Increment \(m\) by 1.
* Perform membership queries to determine \(L_{i}[m]\) for each \(L_{i}_{t}\). Note that since \(m m_{t}_{t^{} t}w_{t^{}}\), the determination of which languages in \(_{t}\) are consistent with \(S_{t}\) does not change when we do this.
* Determine which languages are \((t,m)\)-critical, and from this determine \(n_{t}(m)\). Note that this only requires consulting the results of membership queries already performed.
* If \(u_{m} L_{n_{t}(m)}\), then output \(u_{m}\) and define \(m_{t}=m\). Otherwise, continue to the next iteration.

Analyzing the algorithm.As written, it is not immediately clear that the algorithm's iterations in step \(t\) will come to an end with an output string \(u_{m}\), rather than running forever. But we can show the algorithm does terminate, via the following claim proved in Section A.3.

**(5.5)** _In step \(t\), the algorithm outputs a string after a finite sequence of operations._

Given that (5.5) establishes that the algorithm outputs a string in step \(t\), it is useful to record an additional property of the algorithm that follows directly from its construction.

**(5.6)** _In step \(t\), there is an \(m_{t}\) and an \(n_{t}\) such that the algorithm outputs a string from \(L_{n_{t}}[m_{t}]\), where \(L_{n_{t}}\) is the \((t,m_{t})\)-critical language with maximum index in \(_{t}\)._

Finally, we have the natural analogue of (4.6), proved in Section A.3, from which our main result (2.1) directly follows.

**(5.7)** _For any language \(L_{z}\) and any enumeration of \(L_{z}\), there is a \(t^{*}\) such that for all \(t t^{*}\), the algorithm generates a string in \(L_{z}-S_{t}\)._

Similar to the end of Section 4, it is straightforward to modify the algorithm so it generates strings without repetition.

## 6 Generation for Finite Collections of Languages

We now turn to our second main result, (2.2), which derives a stronger conclusion for finite collections of languages.

The proof of this result takes a different approach from what we used in the previous two sections; for the problem in this section, at a given step \(t\), the approach we follow is to take the finite sample \(S_{t}\) seen so far and ask whether there are any additional strings \(w S_{t}\) that are common to all the languages in \(\) that are consistent with \(S_{t}\). If so, then we can output such a string and be sure of satisfying the specification for generation.

The closure operation.We formalize this plan using the notion of a _closure_ operation, as follows. For a sequence of strings \(S_{t}\) from a language in \(\), we define the _closure_ of \(S_{t}\) in \(\), denoted \( S_{t}\), to be the intersection of all languages in \(\) that are consistent with \(S_{t}\). If there is a string in \( S_{t}-S_{t}\), then it is always safe for the algorithm to generate such a string; by definition, it must be an unseen string from the true language \(L_{z}\). The closure operation would not have been useful for us in the case of general infinite collections of languages \(\), since there are instances of the general problem where \( S_{t}=S_{t}\) and so the closure does not provide us with any proposals for new strings to generate. We give an example of such an instance in Appendix A.6.

However, in the case of finite collections \(\) such as we are considering in this section, the closure operation becomes a powerful tool: the crux of proving (2.2) is to show that once \(S_{t}\) is large enough, \( S_{t}\) must become infinite, so we will have an infinite set of options in \( S_{t}-S_{t}\) to use.

Proving the result for finite collections.We start by giving the basic idea behind the proof, and then the proof itself. Let us write the finite collection of candidate languages as \(=\{L_{1},L_{2},,L_{n}\}\), and suppose that after the adversary has enumerated a set \(S\) of strings, the languages consistent with \(S\) are \(L_{i_{1}},L_{i_{2}},,L_{i_{k}}\). Note that the true language \(K\) must be one of these \(k\) languages. Now, the closure \( S\) is equal to the mutual intersection \(L_{i_{1}} L_{i_{2}} L_{i_{k}}\), and there are two possibilities: either \( S\) is infinite, or it is finite. If it is infinite, then the algorithm can safely generate all of the strings in \( S-S\), and thus achieve the goal specified by (2.2). On the other hand, if \( S\) is finite, then it has size equal to some natural number \(m\); in this case, after the adversary enumerates at most \(m+1\) more distinct strings, the algorithm will learn that at least one of \(L_{i_{1}},L_{i_{2}},,L_{i_{k}}\) is no longer consistent. We will then have a set of at most \(k-1\) consistent languages, and we can iterate this argument at most \(k-2\) more times until (i) there is only a single consistent language, which must be \(K\), or (ii) more generally, the set of all consistent languages has a mutual intersection that is infinite, in which case the algorithm can safely generate from this infinite set.

This argument conveys the key point underlying the proof, that as the adversary enumerates strings from \(K\), it cannot prevent itself from reaching a point where the set of strings it has enumerated has an infinite closure. To turn this into an argument that produces a uniform bound on how many strings are needed before the closure must become infinite, we replace the iterative argument in the previous paragraph with one that is shorter and more direct. Specifically, consider all sub-collections of languages from \(\) (where we think of a sub-collection as any way of choosing some of the languages from \(\) but not others). Note that since \(\) is finite, there are only finitely many possible sub-collections of languages from \(\). For each sub-collection \(^{}\), the mutual intersection of the languages in \(^{}\) is either finite or infinite. Consider the sub-collections that have a finite mutual intersection, and let \(m^{*}\) be the maximum size of such a mutual intersection. Now, suppose the adversary produces a set \(S\) of \(m^{*}+1\) distinct strings from \(K\). If we consider the sub-collection of all languages in \(\) that are consistent with \(S\), its mutual intersection must contain \(S\) and therefore it has cardinality greater than \(m^{*}\). By the definition of \(m^{*}\), this means that its cardinality must be infinite. So the closure \( S\) is infinite, and therefore the algorithm can safely generate all the strings in \( S-S\).

We give a formal version of this argument in Section A.4, providing the complete proof of (2.2).

## 7 Extension: Prompted Generation in the Limit

As discussed in Section 2, a natural generalization is to ask whether we can preserve the basic structure of the model while adding a notion of _prompting_: the algorithm is provided with a _prompt_ string and it must complete it to a valid output. The overall set-up is as before: an adversary chooses a true language \(K\) from \(\), and it begins enumerating the strings of \(K\) in a sequence of steps.

A model of prompting.The new feature of the problem in our generalization is that in every step \(t\), the adversary provides the algorithm with two things: a string \(w_{t}\) from the true language \(K\), and a string \(p_{t}\) that serves as a _prompt_. (The adversary is allowed use the same prompt in multiple steps.) The algorithm in step \(t\) must then produce a string \(c_{t}\) with the goal that the concatenation of \(p_{t}\) and \(c_{t}\) is a string belonging to \(K-S_{t}\); that is, it should be an unseen string from \(K\). In what follows, we will use \(p_{t} c_{t}\) to denote the concatenation of \(p_{t}\) and \(c_{t}\).

We observe that it leads to an equivalent problem whether we ask the algorithm to output \(c_{t}\) so that \(p_{t} c_{t} K-S_{t}\), or whether we ask the algorithm to output the full concatenated string \(a_{t}=p_{t} c_{t}\). In this latter formulation, we can phrase the algorithm's task as follows: given a prompt \(p_{t}\), output a string \(a_{t}\) with the properties that \(p_{t}\) is a prefix of \(a_{t}\), and \(a_{t} K-S_{t}\). Because it makes the exposition slightly simpler, we will take this latter formulation as our default way of describing the problem, but the two formulations are equivalent.

To establish a positive result for prompted generation in the limit, we need to impose some type of restriction on the prompts the adversary can provide. For example, if the adversary were simply to provide an arbitrary string \(p_{t}\) and ask the algorithm if there exists a string \(c_{t}\) and a language \(L_{i}\) for which \(p_{t} c_{t} L_{i}\), this is not a problem that could be solved by an algorithm that must terminate and whose only access to \(\) comes in the form of membership queries of the form "Is \(w L_{i}\)?"

Here we describe a result based on the following restriction on the adversary's prompts: We say a prompt \(p\) is _robust_ if for all languages \(L_{i}\), there exist arbitrarily long strings \(c\) for which \(p c L_{i}\). In the present discussion we will consider adversaries that only provide robust prompts.

We say that the algorithm achieves _prompted generation from \(K\) in the limit_ if there is some \(t^{*}\) such that for all steps \(t t^{*}\), the algorithm's output \(a_{t}\) has the property that \(p_{t}\) is a prefix of \(a_{t}\) and \(a_{t} K-S_{t}\). We can prove the following.

**(7.1)** _There is an algorithm with the property that for any countable collection of languages \(=\{L_{1},L_{2},L_{3},\}\), and any enumeration of one of these languages \(K\) accompanied by a sequence of robust prompts, the algorithm achieves prompted generation from \(K\) in the limit._

We provide a proof of this theorem in Section A.5 in the Appendix. In the full version of the paper we also explore stronger statements that are based on giving the adversary more freedom in the prompts it is allowed to provide.

We close with two initial observations about (7.1). First, (7.1) is a strict generalization of our first main result (2.1), since if the adversary always provides the empty string as its prompt \(p_{t}\), then the problem of finding continuations \(c_{t}\) for which \(p_{t} c_{t} K-S_{t}\) is simply the problem of finding strings \(c_{t}\) in \(K-S_{t}\), as in the original definition of generation in the limit. Moreover, the empty string is a robust prompt, since each of the languages \(L_{i}\) is infinite, and so there are arbitrarily long continuation strings that belong to \(L_{i}\) when concatenated to the empty string.

Second, we observe that there is no requirement that the algorithm has ever seen a string beginning with the prefix \(p_{t}\) among the adversary's examples \(S_{t} K\) before the first step \(t\) in which the adversary provides \(p_{t}\). An important point to notice about (7.1) is that the algorithm can achieve prompted generation in the limit despite this challenge.

## 8 Concluding Remarks

Generating from a language based on observed samples is thus a fundamentally different, more tractable problem than identifying the language from observed samples. It is so tractable, in fact, that it can be accomplished provided only that we know the samples come from a language in a known countable collection of languages.

It is therefore interesting to ask what stylized conclusions we might draw from these general results about generation as a task, and its relation to other learning processes. In the case of finite collections of languages, the basic idea underlying the proof is that a large "core" to the language (the closure of the sample, in our terminology) emerges at a known time after a finite set of observations, and it is then enough to generate from this core even though there might always remain peripheral parts of the language -- disjoint from this core -- that we can never be sure about. In the case of infinite collections of languages, the task is even more complex, because there is never a known time at which a core to the language emerges. Instead, the algorithm may need to continually shrink the set it is using for generation; through this infinite process of shrinkage, the algorithm can be sure that beyond a certain point, it is always generating from the true language \(K\), even if it can not be sure when it has reached this point or what the true language is.

Thus, as noted earlier, the solutions we develop highlight interesting tensions between the problem of producing _valid_ strings that belong to the target language, and the problem of maintaining _breadth_ by not restricting to only a small subset of the target language. Our approaches achieve validity through a strategy that implicitly gives up on breadth, and it is interesting to ask if this is essentially necessary for any method that achieves language generation in the limit. In this way, our formalism may also help shed light on a particular broader impact of large language models, which is their implications for the homogenization of text style, in which writing becomes less varied; our results provide further indications that decreased breadth in text output may be a crucial feature of successful solutions to text generation.

This tension between validity and breadth, as it arises in our solution, also creates an interesting echo of the human process by which people acquire the vernacular within a new community : as with our solution in this abstract model, people encountering the dialect in a new community similarly pass through a sequence of conceptually distinct phases: an initial phase in which they are generating too adventurously and producing invalid utterances; then a phase where the utterances are approximately aligned with the scope of the language; and finally a phase in which the range of utterances they are willing to generate shrinks further over their lifetime, as they become increasingly conservative in what they consider valid. Again, it is interesting to consider whether this type of structure is inherent in any solution to the task of generation in the limit.

Acknowledgements.We thank Bobby Kleinberg, Lillian Lee, Marios Papachristou, and Kenny Peng for helpful discussions on these questions and on early drafts of this paper. The work has been supported in part by a Vannevar Bush Faculty Fellowship, a Simons Collaboration grant, a grant from the MacArthur Foundation, and the Center for Applied AI at the University of Chicago Booth School of Business.