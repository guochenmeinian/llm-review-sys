# Dynamic Rescaling for Training GNNs

**Nimrah Mustafa**

CISPA

66123 Saarbrucken, Germany

nimrah.mustafa@cispa.de &**Rebekka Burkholz**

CISPA

66123 Saarbrucken, Germany

burkholz@cispa.de

###### Abstract

Graph neural networks (GNNs) with a rescale invariance, such as GATs, can be re-parameterized during optimization through dynamic rescaling of network parameters and gradients while keeping the loss invariant. In this work, we explore dynamic rescaling as a tool to influence GNN training dynamics in two key ways: i) balancing the network with respect to various criteria, and ii) controlling the relative learning speeds of different layers. We gain novel insights, unique to GNNs, that reveal distinct training modes for different tasks. For heterophilic graphs, achieving balance based on relative gradients leads to faster training and better generalization. In contrast, homophilic graphs benefit from delaying the learning of later layers. Additionally, we show that training in balance supports larger learning rates, which can improve generalization. Moreover, controlling layer-wise training speeds is linked to grokking-like phenomena, which may be of independent interest.

Deep neural networks (DNNs) with positively homogeneous non-linear activation functions such as ReLU exhibit the rescale invariance property , i.e. scaling down incoming weights to a neuron and scaling up the outgoing weights from the neuron by the same factor does not alter the function represented by the network.

Under gradient flow that assumes an infinitesimally small learning rate, this rescale symmetry induces the conservation of a relationship between network parameters and gradients, determined by the initial state . More specifically, it is known that for traditional DNNs and convolutional neural networks (CNNs) with homogenous activation functions such as ReLUs, the difference between the squared \(L2\)-norms of incoming and outgoing parameters to a neuron stays constant (and is thus conserved). When this conserved quantity is (nearly) zero, the network is said to be in a balanced state. Training DNNs is generally considered well-conditioned by a balanced state of this conservation law.

In the context of Graph Neural Networks (GNNs), the first work presenting insights regarding norm balance derives the conservation law for GATs  and demonstrates how a balanced initialization enhances the trainability of GATs, particularly deeper networks . This balanced state that would hold under gradient flow could potentially be beneficial throughout training. However, in practice, factors that drive optimization such as finite learning rates, momentum, weight decay, and batch stochasticity break the rescale (and other) symmetries  and consequent conserved quantities, causing the network to topple out of balance due to which training conditions may deteriorate.

Motivated by the positive outcomes of training a model balanced at initialization, we utilize the identified rescale invariance of GATs  to further investigate the effects of maintaining this balance throughout training rather than only at initialization, by _dynamic rescaling_, i.e. scaling network parameters at the neuron level during training in a loss-invariant manner. To this end, we derive a general procedure to balance a GAT network, not necessarily only w.r.t. parameter weight norms, but any criterion that is a function of network parameters and gradients. More specifically, we propose a novel criterion based on relative gradients of network parameters and demonstrate that this criterion, or a combination of both criteria, often offers practical gains in terms of training speedup or better generalization.

The implications of the core concept of dynamic rescaling extends beyond training in balance. For example, it enables arbitrary control of the order in which network layers learn during training. This can be viewed as inducing an imbalanced state in the network that may be desirable in some cases . Based on the experimental exploration of our methodological ideas, we draw various novel insights into how dynamic rescaling can potentially be leveraged. Firstly, specifically regarding graph learning, we discover potential trends of optimal learning dynamics for homophily and heterophily. We observe that, in terms of training speed and generalization, training all network layers in balance tends to be more beneficial for heterophilic than homophilic graphs. On the contrary, homophilic graphs tend to benefit from more focused learning in the first (earlier) layers. Secondly, we hypothesize that larger learning rates that tend to increasingly disrupt network balance can be better supported by dynamic rebalancing of the network in improving generalization. Thirdly, we encounter other interesting phenomena similar to grokking  that we relate to our insight on layer learning order.

This work is also motivated by earlier studies on deep feedforward non-linear neural networks that exploit rescale invariance, using transformations respecting loss-invariant symmetry to teleport parameters to another point in the loss landscape with steeper gradients to improve optimization  and/or convergence rate guarantees . To the best of our knowledge, we are the first to conduct an initial exploration of these ideas on GNNs, where identifying and exploiting the rescale invariance is not as straightforward, due to their peculiar architectural design elements such as node-wise neighborhood aggregation that make identifying the corresponding rescale invariances challenging. Thus, this largely remains unexplored territory for GNNs and our understanding of the underlying gradient dynamics in GNNs lags in comparison to DNNs. Rather than proposing a one-size-fits-all solution or achieving state-of-the-art performance, this paper aims to contribute insights into GNN learning dynamics.

In summary, our contributions are as follows:

1. We prove that, given a GNN exhibits a rescale invariance (like GCNs or GATs), we can manipulate gradient norms while leaving the GNN and the loss invariant and thus influence the learning dynamics during optimization.
2. We derive the procedure to balance a GAT network w.r.t. any criterion that is a function of network parameters and gradients by dynamic rescaling.
3. We suggest a novel criterion for balancing based on relative gradients and find it to be promising for improving generalization and training speed in practical settings.
4. We explore our conceptual ideas empirically and find promising directions to utilize dynamic rescaling for more practical benefits, by training in balance or controlling order of learning among network layers.
5. We discuss novel insights regarding i) trends in training dynamics for homophilic and heterophilic graphs ii) larger learning rates and iii) interesting grokking-like phenomena.

## 1 Related work

There has recently been an increased interest in studying training dynamics and generalization . Approaches to address trainability issues in GNNs include initialization [20; 28; 13; 12], normalization [3; 5; 50; 48], skip-connections, regularization[34; 44], their combinations[25; 4], architectural variations, and insights from graph signal processing based on spectral properties . While our approach of dynamic rescaling allows us to use gradients in the network to control the rate at which GNN layers (and potentially neurons) learn non-linear transformations of their features, draws an interesting parallel by modulating message passing updates based on gradients to control the rate at which nodes learn. The closest work to ours is  which proposes using balanced norms at initialization. Our work differs in mainly three ways as we: i) derive how a balanced state can be achieved not only at initialization but also during training by dynamic rescaling, ii) propose a different criterion for balance based on relative gradients, and iii) present insights on training in and out of balance in light of input graph homophily and heterophily.

For traditional DNNs, there is a deeper understanding of loss invariant symmetries and their impact on gradient dynamics[8; 17; 40]. Several studies exploit rescale symmetry and corresponding conservation laws in (feed-forward and convolutional) neural networks in various ways to aid optimization [31; 26; 46], regularization , and compression.  introduce a set of nonlinear,data-dependent symmetries, relate conserved quantities to the convergence rate and sharpness of the optima, and provide insights into how initialization impacts convergence and generalizability. Gradient flow equations for neural networks have also been extended to account for realistic optimization elements such as finite learning rates, momentum, weight decay, etc. .

## 2 Dynamic rescaling

PreliminariesConsider a \(L\) layer GAT network \(f\) with positively homogeneous activation \(\) (i.e \((x)=x^{}(x)\)) and consequently, \((ax)=a(x)\) for positive scalars \(a\)) such as ReLU \((x)=\{x,0\}\) or LeakyReLU \((x)=\{x,0\}+-\{-x,0\}\). Then, as shown by , the parameters \(^{l}[i,:],^{l+1}[:,i]\), and \(^{l}[i]\) associated with a hidden unit \(i\) in the network layer \(l\), may be respectively scaled to \(}^{l}[i,:]=^{l}[i,:]\), \(}^{l+1}[:,i]=^{-1}^{l+1}[:,i]\) and \(}^{l}[i]=^{-1}^{l}[i]\) where \(>0\) such that \(f=\), i.e. the rescaling respects the network symmetry.

Note that \(^{l}[i,:]\) and \(^{l+1}[:,i]\) denote weights incoming to and outgoing from neuron \(i[d_{l}]\), respectively, where \(d_{l}\) is the width of layer \(l\). Given a network parameter \(\) and its gradient \(_{}\) w.r.t. the network loss \(\), the relative gradient \(\) of parameter \(\) is defined as:

\[=_{}/\ \ \ \  0\ \ \ \ =0\ \ \ \ =0.\] (1)

This rescale property is more powerful than it might appear at first, as it provides us the means to significantly influence the training dynamics. It suggests that we have a high number of degrees of freedom to pick a parameterization without changing the function of a GAT. Concretely, we can use any scaling factors \(_{i}^{(l)}>0\) that are associated with features in the middle layers and define another parameterization \(}^{l}[i,j]=_{i}^{(l)}/_{j}^{(l-1)}^ {l}[i,j]\) and \(}^{l}[i]=^{l}[i]/_{i}^{(l-1)}\) that will induce the same function. Yet, according to the following lemma, the gradients of the parameters depend on the rescale factors and can therefore be controlled correspondingly.

**Lemma 2.1** (Gradient scaling).: _Under the rescale invariance of GATs, if a parameter is scaled by \(=\), then its gradient is scaled as \(_{}=^{-1}_{}\)._

We defer the proof to the appendix A.1. It generally implies that we have the freedom to pick any positively homogeneous constants so that our resulting gradients \(_{}^{l}[i,j]}=_{j}^{(l-1)}/_ {i}^{(l)}_{^{l}[i,j]}\) and \(_{}^{l}[i]}=_{i}^{(l-1)}_{ ^{l}[i]}\) induce favorable learning dynamics. Considering the flexibility, in which gradient direction we can move during gradient descent, the choice likely has a significant influence on our learning success. We aim to exploit this fact in this work and discover conceptual insights into what criteria could constitute choices. Thus, we follow the following procedure.

Balancing criteria and procedureThe rescale invariance property allows us to rescale parameters to fulfill the desired criterion not only at initialization but also during training without changing the network output while potentially improving the training dynamics.

We could indeed choose a relatively general criterion \(g:^{}^{}\) that depends on our rescaled parameters and gradients and determines our choice of scaling factors. As long as we can solve

\[g(_{i}^{(l)}^{l}[i,:],_{i}^{(l)-1}^{l+ 1}[:,i],_{i}^{(l)-1}^{l}[i],_{i}^{(l)-1}_{ ^{l}[i,:]},_{i}^{(l)}_{^{l+1}[:,i ]},_{i}^{(l)}_{^{l}[i]})=0.\] (2)

, For unique nonzero scaling parameters, \(g\) can act as our guide during gradient descent. Importantly, not all reasonable criteria determine the scaling factors.

As has been derived recently for GATs , the rescale invariance also induces a conservation law that holds throughout training and is characterized by the fact that the scaling factors cancel out. In fact, any such law that remains invariant under specific parameter transformations is also linked to an invariance like the rescale invariance. according to Noether's theorem. Specifically, in the case of GATs, the following equations

\[ W^{l}[i,:],_{W^{l}[i,:]}- a^{l}[i], _{a^{l}[i]}- W^{l+1}[:,i],_{W^{l+1}[:,i ]}=0.\] (3)

hold regardless of the scaling factors. This law implies that a specific sum of L2-norms of the corresponding parameters stays conserved throughout gradient descent if the learning rates are sufficiently small.

Recently, it has been shown that balancing these squared parameter \(l2-\)norms at initialization such that \(\|^{l}[i,:]\|^{2}-\|^{l}[i]\|^{2}- \|^{l+1}[:i]\|^{2}=0\), \( i d_{l},l[L-1]\), induces good initial trainability in GATs . For larger learning rates, this balance might get disturbed during training. Yet, we could use our rescaling degrees of freedom to bring the parameters back in balance. To induce even better trainability, we propose another criterion to rescale a GAT network, which balances the norms of gradients relative to the corresponding parameters, i.e., \(=_{}/\). Intuitively, this should allow the parameters in different layers to move at similar speeds and ensure good trainability in all parts of the network.

\[\|^{l}[i,:]\|^{2}-\|^{l}[i] \|^{2}-\|^{l+1}[:i]\|^{2}=0.\] (4)

Yet, as this criterion is not naturally preserved during gradient descent or gradient flow like the weight norms, fulfilling the equation above requires frequent rescaling during training. Thus, based on Eq.(2), balancing a neuron \(i\) in layer \(l\) w.r.t. relative gradients requires fulfilling:

\[\|^{(l)-1}_{^{l}[i,:]}}{ _{i}^{(l)}^{l}[i,:]}\|^{2}-\|^{( l)}_{^{l+1}[:i]}}{_{i}^{(l)-1}^{l+1}[: i,i]}\|^{2}-\|^{(l)}_{^{l}[i]} }{_{i}^{(l)-1}^{l}[i]}\|^{2}=0.\] (5)

Balancing the entire network requires fulfilling Eq.(5) \( i[d_{l}],l[L-1]\). In this case, note that every weight \(^{l}[i,j]\) is eventually scaled by \(_{i}^{(l)}/_{j}^{(l)}\). Thus, balancing the entire network requires iterative rescaling until convergence of all rescaling factors to \(1\).

For each iteration \(t[1,T]\) and given \({_{i}^{(l)}}^{(0)}=1\), \( i[d_{l}],l[L-1]\), the scaling factor \({_{i}^{(l)}}^{(t)}\) is given by:

\[{_{i}^{(l)}}^{(t)} =(^{l}[i,:]^{(t-1)}\|^{2} }{\|^{l}[i]^{(t-1)}\|^{2}+\|^ {l+1}[:i]^{(t-1)}\|^{2}})^{}\ \ ;\ \] (6) \[^{(t)} =\{ &^{(l)}}^{(t)} ^{(t)}_{}}{_{i}^{(l)}}^{l}[i,:]^{(t-1)}\ ;\ t>0\\ &^{(l)}}^{(t)}_{}}{ _{i}^{(l)}}\{^{l+1}[:i]^{(t-1)}, ^{l}[i]^{(t-1)}\}\ ;\ t>0..\] (7)

Ideally, this process is repeated to convergence until \({_{i}^{(l)}}^{(T)}=1\), \( i[d_{l}],l[L-1]\). The number of required iterations depends on the frequency of rebalancing during training as well as the network parameters and gradients. In practice, we find that this is not a too computationally expensive process and a few iterations (\(<10\)) are sufficient to (mostly if not completely) balance the network. In terms of time complexity, this only affects the training time linearly depending on the frequency of rebalancing, which is a controllable hyperparameter.

ImplicationsDynamic rescaling opens up the opportunity to control training dynamics in several ways for which we lay out two key ideas as follows.

Firstly, dynamic rescaling can be used to train networks in balance w.r.t. certain criteria. We propose that balancing based on relative gradients may be one such good candidate. Our intuition is that balanced relative gradients allow all layers (and neurons) in the network a relatively equal opportunity to learn by propagating gradients to drive parameter change throughout the network, thereby enhancing trainability. We observe that this novel insight shows the promising potential of being translated into practical gains such as faster or better generalization on real-world data, particularly for heterophilic tasks.

Secondly, dynamic rescaling allows us to control of relative training speed at the level or neuron level. In principle, by rescaling layers, we can configure the relative order in which they learn arbitrarily, at any time during training. This is a direct consequence of the conservation law that parameters and their gradients in the network adhere to. The underlying insight is that by controlling the parameter weight or relative gradient norms, the gradients that drive parameter change can be influenced. As a result, a layer receiving relatively larger gradients (due to relatively scaled-downweight norms) thus learns'more' or 'faster' than other layers. While the possibilities are numerous, in this work, we limit our investigation to a simple but core case: allowing the network to (initially) focus learning on a specific layer by scaling down its weight norm at initialization. We observe that initially allowing more focused learning in the earlier layers of the network can improve the convergence time substantially while retaining or even improving the generalization for homophilic tasks.

Our findings suggest that the order in which layers learn influences convergence time and generalization. This building block may be leveraged to devise more sophisticated learning sequences such as training layers cyclically or in a task-dependent manner rather than in a predefined order. We elaborate on the current limitations of dynamic rescaling in the appendix.

While these concepts apply to any GNN, provided its rescale invariance has been identified, we focus our investigation on the GAT architecture in this work as they are a generalization of the more basic GCN architecture that is the building block of more complex GNNs. Furthermore, GAT serves as a strong basis for graph learning with an attention mechanism, in which there has been an increased interest recently [18; 9].

## 3 Experiments

We divide our exploration of the ideas discussed in SS2 into three parts: 1) practical gains on real-world data of training in balance by dynamic rescaling, 2) empirical insights into the layer-level order of learning, and 3) observation of a grokking-like phenomenon, which is related to 2). Hereafter, we use the notation \(_{W}\), and \(_{RG}\) to denote dynamic rescaling w.r.t. weight norms and relative gradients, respectively, every \(10^{th}\) epoch. \(_{C}\) denotes a combination of the two by rescaling w.r.t. weight norms every \(10^{th}\) epoch and w.r.t. relative gradients in all other epochs. A maximum of \(10\) iterations for the rebalancing procedure outlined in Eq. (6) and (7) were used. All experiments use the Adam optimizer and networks are randomly initialized with looks-linear orthogonal structure [36; 1] unless specified otherwise. Experiments were run on an NVIDIA RTX A6000 GPU with 50GB RAM. Our experimental code is available at https://github.com/RelationalML/Dynamic_Rescaling_GAT.

### Training in balance

We primarily study the effect of training GAT in a balanced state based on the relative gradients criterion (see Eq.(4)), by dynamic rescaling on five real-world heterophilic benchmark datasets .

We find that rebalancing the network w.r.t. relative gradient norms is more effective than the criterion based on parameter weight norms, as shown in Table 1. This aligns with observations on CNNs where rebalancing w.r.t. parameter norms was also not effective . Therefore, our insight on the impact of rebalancing w.r.t. relative gradients may also be of independent interest outside the context of GNNs.

In addition to improved generalization in most cases, dynamic rescaling may also provide the benefit of fewer training epochs to attain comparable or even slightly better generalization, as shown in Fig. 1. We make an interesting observation that a balanced state during training together with larger learning rates results in better generalization than when either of the two components is individually employed.

    & roman-empire & amazon-ratings & tolokers & questions & minesweeper \\  w/o DR & \(.4978.0209\) & \(.4545.0043\) & \(.6493.008\) & \(.5829.0172\) & \(.5057.0058\) \\ \(_{W}\) & \(.3307.0670\) & \(.\) & \(.6451.0135\) & \(.5791.0145\) & \(.5058.0051\) \\ \(_{RG}\) & \(.5422.0234\) & \(.4540.0029\) & \(.6637.0088\) & \(.\) & \(.5065.0076\) \\ \(_{C}\) & \(.\)* & \(.4526.0042\) & \(.\) & \(.5696.0132\) & \(.\) \\   

Table 1: Results of training a 5-layer GAT network with various dynamic rescaling (DR) settings. The mean \( 95\%\) CI test metric at the epoch of the best validation metric across \(10\) splits is reported using the best learning rate from \(\{0.01,0.001,0.005\}\). The evaluation metric is accuracy for roman-empire and amazon-ratings, and ROC AUC for the remaining three datasets.

This synergetic effect can be attributed to two factors. Firstly, several works report empirical evidence that larger learning rates are usually associated with flatter minima that have been linked to better generalization [49; 7; 19]. On the contrary, larger learning rates push the network out of its balanced state faster and more severely (see Fig. 2), which may impede tranability. However, this can now be addressed by rebalancing the network during training to facilitate trainability. Thus, balancing by dynamic rescaling supports higher learning rates in improving generalization. However, for a given task, only a narrow range of these 'large enough' learning rates can produce optimal results . We show in Fig. 3 that training in balance can further improve the performance for this range of larger learning rates.

We use gradient clipping in combination with dynamic rescaling to accommodate exploding or vanishing gradients as a result of training with a larger learning rate and any numerical instabilities that may arise due to direct manipulation of parameter weights and gradients. We defer the ablation study of gradient clipping to the appendix.

### Learning layers in order

As discussed in SS2, one potential opportunity with dynamic rescaling to improve training dynamics is to control the order in which layers (or even neurons) in the network learn. We investigate this experimentally on both synthetic and real-world data, gaining novel insights into potential trends of optimal training dynamics for different tasks based on their homophily and heterophily. We defer the description of the synthetic data generation to the appendix.

The training curves and generalization performance achieved on the synthetic task for various train settings are summarized in Fig. 4. For this task, allowing the network to initially concentrate learning on the first layer results in faster training, lower minimum test loss, and better generalization than training the network in the standard setting or with dynamic rescaling. Interestingly, as more initial focus is placed on each subsequent layer, the training slows down and generalization worsens, with \(l=5\) being (marginally) the lowest. Dynamic rescaling with a learning rate of \(0.001\), as used in all cases, is not as effective. However, using a larger learning rate of \(0.01\) allows the fastest training with the second-best generalization. We expand on this as we analyze the evolution of relative gradients for interesting cases in Fig. 8 in the appendix.

Note that this synthetic task, which benefits most from concentrated learning in the first layer, is designed to be homophilic and thus differs from the heterophilic tasks that benefit more from training in balance. Prompted by this observation, we next study the impact of ordered learning of layers for real-world homophilic and heterophilic graphs.

As shown in Fig. 5, we find that real-world graphs indeed exhibit different trends regarding generalization and convergence time when trained in balance (w.r.t. relative gradients) and when trained out of balance (by controlling learning order of layers) depending on their homophilic or heterophilic nature. Training layers in balance is generally more effective for heterophilic tasks whereas allowing learning

Figure 4: Performance of a five-layer GAT network on synthetic data under varying settings. Standard implies regular training with no constant or dynamic rescaling. \(L=l\) for \(l\) denotes scaling down the parameters of \(l\) by a constant (\(=0.002\)) at initialization followed by regular training. DR denotes dynamic rescaling to balance relative gradients during training with the specified learning rate (lr). Note that the train and test accuracy axis have been zoomed in for clarity and the initial (train or test) accuracy is lower than 0.9 (but rises sharply in the first few epochs). The best strategy (among considered cases) for this task is to (initially) focus the learning more on the first layer.

Figure 5: Impact of training layers of a two-layer GAT network in and out balance for different tasks. The tasks {amazon-ratings, questions, roman-empire, tolokers, minesweeper} are heterophilic and the remaining are homophilic. Homophilic tasks tend to perform better and converge much faster with learning concentrated in the first layer initially (lower weight norms imply larger relative gradients), whereas heterophilic tasks perform better when layers are trained in balance. Interestingly, even freezing the initial values of parameters in the second layer (i.e. only allowing the second layer to learn) does not significantly reduce the performance for homophilic tasks, even without an additional classifier layer. On the contrary, freezing the first layer results in a severe drop in performance for all tasks.

to focus on the first layer tends to benefit homophilic tasks, particularly in terms of convergence time. We discuss this interesting observation more broadly in light of GNN training dynamics.

Generally, the optimal performance achieved by a model is, to a large extent, dependent on how well the inductive bias of the model architecture aligns with the task and its underlying graph structure. For example, it is widely known that general GNNs, without specially-introduced architectural elements, such as GCN perform better on homophilic than on heterophilic tasks. Intuitively, we hypothesize that homophilic tasks rely more on the neighborhood aggregation functionality of GNNs rather than feature learning. In this case, an aggregation over a random transformation of similar features may still be sufficient for good generalization.

Our insight is in line with a recent analysis of training dynamics  which shows that the NTK that controls the evolution of the learned GNN function tends to align with the message passing matrix (i.e. the adjacency matrix in most cases). Furthermore, for homophilic graphs, the adjacency matrix also aligns well with the optimal kernel matrix that represents nodes with the same label. As a result, on homophilic graphs, the alignment of the underlying structure with the optimal kernel matrix allows parameter-free methods similar to label propagation to perform at par with GNNs. However, the generalization of GNNs on heterophilic tasks, where the graph structure does not align with the optimal kernel, is mostly adversely impacted by the NTK aligning with the adjacency matrix. In other words, the structural information in the graph is not very relevant for a node's label in heterophilic settings and thus the node relies more on learning in the feature space rather than neighborhood aggregation. This is also supported by results showing that embedding additional MLP layers in the network significantly improves the performance of basic GNNs such as GATs on these heterophilic tasks . Thus, we conclude that training in balance to potentially learn better feature transformations in all layers (and potentially neighbors farther away in deeper models) is more effective in heterophilic cases.

### Grokking-like phenomena

Grokking  is defined as a phenomenon where the validation loss reduces, long after a near-zero train loss has been achieved, towards perfect generalization. It has been observed primarily in the context of algorithmic and synthetic datasets . Yet, it is still regarded as a problem of fundamental interest, not only because the phenomenon appears to be puzzling at first,

Figure 6: Evolution of gradient norms in a five-layer GCN network trained on synthetic data and evaluated on two validation sets from the same input distribution as the train set (left and right) with initial learning focused on the second and third layers in (a) and (b), respectively. The plot design is similar to that described in Fig. 8 except that in this case, the heatmap represents gradient norms.

but also because it gives rise to nuanced insights into how neural networks learn, as it accentuates different training phases.

Grokking also touches upon our study in which order network layers learn best, as it has been attributed to network layers getting out of balance due to weight decay and weight norm decrease . More precisely,  argues that it is the result of delayed feature learning. According to their theory and observations for a 3-layer neural network, primarily the last layer learns first a linear combination of random features, as the parameter norms are scaled in such a way that it renders the first layer effectively untrainable, still reaching nearly zero training loss. With an increase of the last layer norm, however, feature learning by the first layer begins and starts to drive down the generalization error.

We conjecture that, depending on the learning task, grokking might be induced also by delayed learning of other layers, not only the last one. Our insights into rescaling based on relative gradients give us the means to test this hypothesis.

A synthetic node classification task is constructed, similarly as for the experiments in SS3.2, except with \(N=100\), \(p=0.01\), and using a GCN instead of GAT as the target network. Using \(50\) nodes as the train set, we train a five-layer GCN and evaluate it on the remaining nodes as two disjoint test sets each of size \(25\).

Under the condition that the network is allowed to focus learning on the second or third hidden layer by scaling the weight parameters of the respective layer by \(=0.02\) at initialization, the resulting training curves in Fig. 6 exhibit a trend similar to grokking.

While one test set eventually achieves zero validation loss, the other's loss is reduced only to a certain extent before increasing again to give rise to a U-shaped curve that is more common. The latter is also similar to the only other case of grokking case detected in GNNs , to the best of our knowledge. Note that, in the former case, learning focused on the middle (third) layer of the network achieves perfect generalization much faster than when learning is focused on the second layer. This indicates that layer-level control of learning may be leveraged to facilitate faster generalization in grokking scenarios.

Figure 7: Left: Layer-wise relative gradient norm (\(_{10}\) scale) and loss curves similar to Figure 3 in the paper. Right: Corresponding accuracy of the same run. Grokking-like phenomenon can be induced On a 5-layer GAT using real-world roman-empire dataset by placing initial training focus on layers 4 (top) and 5 (bottom) by scaling down initial parameter norms, followed by rebalancing w.r.t. relative gradients every 10 epochs staring only at epoch 1000. Note the sharp drop in validation/test loss immediately after rebalancing which also translates to more rapid improvement in test accuracy.

We also induce grokking-related behavior on a real-world dataset roman-empire (see Fig. 7) in two steps. Firstly, we allow only the last (or second to last) layer to learn which allows the training accuracy to increase continually while the test accuracy saturates or begins to drop. At this point, we rescale the network to bring all layers in balance w.r.t. relative gradients, following which, the test accuracy immediately begins to improve more rapidly accompanied by a drop in training accuracy. This can be interpreted as the network 'learning' more effectively when trained in balance rather than overfitting to the training data. While this is different from grokking where the training accuracy would generally not drop, it is independently an interesting observation on a real-world dataset.

Our key takeaway is that, as opposed to the general perception that grokking is induced by learning only the last layer, we observe a similar pattern by focusing learning on other (hidden) layers. Strikingly, grokking as a phenomenon can also support learning. While this possibility is a novel insight, we acknowledge that it is a potentially noisy phenomenon that materializes only in specific settings. We would like to highlight that understanding grokking is a separate area on its own and recent efforts are also limited to synthetic data . Our induction of a similar phenomenon in real-world data by influencing learning dynamics can be of independent interest to develop a deeper theoretical understanding of such observations. Further investigation for more conclusive insights related to grokking is an interesting direction for future work.

## 4 Discussion

We have proposed to exploit the rescale invariance of GNNs such as GATs to control their training dynamics for improved training speed and generalization. To that end, we propose to rescale the parameters in such a way that the function, which is defined by a GNN that is trained, remains unperturbed, while the gradients are rescaled. The partial control of the learning dynamics, which is thus available, offers us the opportunity to teleport in the parameter space.

While the guiding principle of this teleportation is flexible, we propose to balance relative gradient norms such that all layers of the GNN are equally involved in the learning process. Our experiments highlight the potential of this rescaling to support larger learning rates and to balance relative gradients in search of flatter optima generally associated with better generalization. To our knowledge, we are the first to tap into the potential that the rescaling flexibility of GNNs has to offer.

While the goal of balancing relative gradients is to involve all network layers equally in learning, we have also analyzed the other end of the spectrum and discussed how we can influence the order in which GNN layers are learned. In doing so, we find that while training in balance is effective for heterophilic graphs, homophilic graphs tend to benefit more from training layers out of balance, with initially more focus on learning in the first layers. This novel insight into trends regarding training modes for different tasks contributes to understanding GNN learning dynamics under homophily and heterophily.

Furthermore, the ordered learning of layers also has implications for grokking-like phenomena, which provide insights into the inner mechanics of learning and are therefore also of fundamental interest. With our novel set of experiments, we could show that grokking does not necessarily result from a delay in feature learning, in which primarily the last layer learns in the first training phase. We could also induce a similar phenomenon by giving the middle layer a headstart in learning. Interestingly, this case provides also an example of a scenario in which a grokking-like phenomenon improves the overall generalization performance of the resulting model. We conjecture that unbalanced learning can also act as regularization that fights over-fitting.

Thus, our explorations and insights serve as the first stepping stone to developing a more comprehensive theory or set of guidelines to leverage dynamic rescaling for training not only GNNs, but various other deep learning architectures, to improve training speed, generalization, and, potentially, robustness.