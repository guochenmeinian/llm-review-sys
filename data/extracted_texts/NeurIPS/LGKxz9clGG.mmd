# Handling Data Heterogeneity via Architectural Design

for Federated Visual Recognition

 Sara Pieri  Jose Renato Restom  Samuel Horvath  Hisham Cholakkal

Johamed Bin Zayed University of Artificial Intelligence (MBZUAI)

Equal contribution

###### Abstract

Federated Learning (FL) is a promising research paradigm that enables the collaborative training of machine learning models among various parties without the need for sensitive information exchange. Nonetheless, retaining data in individual clients introduces fundamental challenges to achieving performance on par with centrally trained models. Our study provides an extensive review of federated learning applied to visual recognition. It underscores the critical role of thoughtful architectural design choices in achieving optimal performance, a factor often neglected in the FL literature. Many existing FL solutions are tested on shallow or simple networks, which may not accurately reflect real-world applications. This practice restricts the transferability of research findings to large-scale visual recognition models. Through an in-depth analysis of diverse cutting-edge architectures such as convolutional neural networks, transformers, and MLP-mixers, we experimentally demonstrate that architectural choices can substantially enhance FL systems' performance, particularly when handling heterogeneous data. We study 19 visual recognition models from five different architectural families on four challenging FL datasets. We also re-investigate the inferior performance of convolution-based architectures in the FL setting and analyze the influence of normalization layers on the FL performance. Our findings emphasize the importance of architectural design for computer vision tasks in practical scenarios, effectively narrowing the performance gap between federated and centralized learning. Our source code is available at https://github.com/sarapieri/fed_het.git.

## 1 Introduction

The growing focus on data privacy and protection  has sparked significant research interest in federated learning (FL) as it provides an opportunity for collaborative machine learning in many domains, such as healthcare , mobile devices , internet of things (IoTs)  and autonomous driving . In FL, data is kept separate by individual clients to learn a global model on the server side in a decentralized way.

Despite the potential benefits of federated learning, the FL environments are highly non-trivial, as the local datasets of the individual clients may not accurately represent the global data distribution. As each client is a unique end user, the heterogeneity caused by uneven distributions of features, labels, or an unequal amount of data across clients poses a serious obstacle . Therefore, the setting is inherently challenging since FL methods must accommodate both statistical diverseness and exploit similarities in client data. Typically, the non-independent and identically distributed (non-IID) nature of the clients drastically affects the model's performance, resulting in lower accuracy and slower convergence than the counterparts trained with all the data collected in a single location.

To this end, the problem of client heterogeneity received significant attention from the optimization community [23; 38; 58; 29; 36; 1; 75; 50; 15].

In parallel to these developments in FL, visual recognition tasks have witnessed remarkable progress in recent years, primarily due to the advancements in deep learning models. These models have achieved state-of-the-art performance when trained on centralized datasets. However, when deployed in an FL setting, these image recognition architectures often exhibit performance degradation. Recent works have investigated the utilization of pretrained models to address the non-iid issue [4; 54]. Qu et al.  examined the robustness of four neural architectures across heterogeneous data, introducing the idea of tackling the problem from an architectural standpoint and encouraging the development of "federated-friendly" architectures. The study suggests that self-attention-based architectures are robust to distribution shifts compared to convolutional architectures, making them more adept in FL tasks. In this work, we perform a comprehensive study across 19 visual recognition models from five different architectural families on four challenging FL datasets. We also re-investigate the inferior performance of convolution-based architectures and analyze the influence of normalization layers on the model performance in non-IID settings.

**Contributions:** This work strives to offer architectural design insights that enable immediate performance improvements without the need for additional data, complex training strategies, or extensive hyperparameter tuning. The key contributions of this work are the following:

* We perform an exhaustive experimental analysis comparing 19 different state-of-the-art (SOTA) models from the five leading computer vision architectural families, including Convolutional Neural Networks (CNNs), Transformers, MLP-Mixers, Hybrids, and Metaformer-like architectures. Some of the architectural families we studied were never previously introduced in FL, demonstrating the feasibility of addressing data heterogeneity in FL from an architectural perspective. These architectures were tested across four prevalent CV federated datasets under the most common non-identicalness sources, such as feature distribution skew, label distribution skew, and domain shift in highly heterogeneous data. Our study, comprehensively illustrated in Figure 1, embodies a step toward more efficient, practical, and robust FL systems.
* Our experiments in highly heterogeneous settings reveal that convolution operations are not inherently sub-optimal in FL applications and Metaformer-like architectures generally exhibit greater robustness in non-IID settings.
* We illustrate that Batch Normalization (BN) adversely impacts performance in the heterogeneous federated learning framework. To counteract this, we empirically demonstrate that

Figure 1: Our study incorporates a wide range of datasets, each presenting varying data heterogeneity levels, including Fed-ISIC2019 and GLD-23K, which are inherently federated datasets due to their image acquisition and sensor mechanisms. Differently from prior literature, we extensively evaluate models from each visual architectural family. Additionally, we assess the effectiveness of four distinct optimization methods orthogonally to the architectural choice. Our findings empirically underscore the significant influence of architectural design choices in narrowing the disparity with centralized configurations and effectively handling data heterogeneity.

replacing BN with Layer Normalization is an effective solution to mitigate the performance drop. This study case underlines how architectural design choices can significantly influence FL performance.
* We conduct a study on complex networks with four different optimizers, establishing that the application of optimization methods does not yield substantial performance improvements in the context of complex architectures. We argue that altering the architecture, in practical scenarios, offers a more effective and simpler-to-implement choice.

## 2 Related Works

Federated learning (FL) aims to train models in a decentralized fashion, harnessing edge-device computational capabilities to safeguard the privacy of client data. In this setup, clients retain their data on the device, coordinating with a server to develop a global model. A distinct challenge in FL is data heterogeneity, as client data is often non-IID. The issue of addressing this heterogeneity has garnered significant focus from the optimization domain. To enhance the established aggregation method, FedAvg , various strategies have emerged. These encompass optimization improvements for federated averaging such as FedAVGM , FedProx , SCAFFOLD , and FedExP  and methods to manage data heterogeneity and imbalance like FedShuffle , FedLC , and MooN .

A complementary angle to explore is how the choice of models can elevate FL training outcomes. It has been observed that pretrained models can significantly counteract non-IID challenges . Some previous studies have highlighted the impact of Batch Normalization (BN) on performance drops under heterogeneous settings , and few works  suggested replacing BN with Group Normalization. Our work is related to Qu et al.'s examination  of neural architecture in visual recognition robustness across heterogeneous data splits. Our experiments compare 19 cutting-edge models from diverse computer vision architectural families against demanding heterogeneous datasets and different optimizers.

## 3 Optimization Methods for Federated Learning

In this section, we present a brief overview of the optimization techniques utilized in our study to evaluate their effectiveness in handling data heterogeneity when applied in conjunction with complex computer vision architectures.

**Federated Averaging (FedAVG) :** FedAVG was proposed by McMahan _et al._ (2017) as the standard aggregation method for FL. It offers a straightforward and effective approach where local models are trained on individual devices, and their parameters are averaged to update the global model. However, FedAVG faces challenges in scenarios involving non-IID and unbalanced data, which are prevalent in FL settings.

**FedAVG with Momentum (FedAVGM) ** To address some of the limitations of FedAVG, the addition of a momentum term has been explored. This technique enhances the standard FedAVG method by updating the global model at the server using a gradient-based server optimizer that operates on the mean of the client model updates. The inclusion of momentum facilitates efficient navigation through the complex optimization landscape and accelerates learning, particularly in the presence of sparse or noisy gradients.

**FedProx **: To handle the heterogeneity in data and device availability in FL, Li _et al._ proposed FedProx (Federated Proximal). This method introduces a proximal term into the local loss function, preventing local models from deviating significantly from the global model. The proximal term acts as a regularizer, penalizing large deviations from the current global model. This regularization approach improves overall performance and enhances robustness in FL scenarios.

**SCAFFOLD :** SCAFFOLD is a stochastic algorithm designed to enhance the performance of Federated Averaging (FedAVG) by introducing a controlled stochastic averaging step. This step reduces the variance of gradient estimates by correcting the local updates. SCAFFOLD employs control variates (variance reduction) to address the "client-drift" issue in local updates. Notably, SCAFFOLD requires fewer communication rounds, making it less susceptible to data heterogeneity or client sampling. Furthermore, the algorithm leverages data similarity among clients, leading to faster convergence.

## 4 Architectures for Computer Vision

In this section, we provide an overview of the architecture families studied in this work.

**Convolutional-based models:** We include Resnet  and EfficientNet  as the study of Qu _et al._, and extend it by adding a recently introduced architecture, ConvNeXt . EfficientNet and Resnet consist of a stack of convolutional layers, pooling layers, and batch normalization layers. ConvNext was conceived by modernizing ResNet-50 to pair with the performance of vision transformers. The key changes introduced at the component-level are: replacing the typical batch normalization with layer normalization, using inverted residual blocks and larger kernel sizes, and substituting the more common ReLU  with the GeLU  nonlinearity.

**Vision Transformers:** ViTs were introduced to the computer vision community by Dosovitskiy _et al._ after becoming the de-facto standard for natural language processing tasks. The Transformer architecture reconsiders the need for convolutions and replaces them with a self-attention mechanism. ViTs work by splitting the image into non-overlapping patches treated as tokens and re-weighting them based on a similarity measure between each token pair. The main differences with CNNs are the lack of inductive bias of the vanilla self-attention and the global receptive field capable of capturing long-distance dependencies as opposed to the local receptive field of convolutions. As representatives of this family of architectures, we include ViT , Swin , Swin V2 , and DeiT .

**MLP-Mixers:** MLP-Mixers were first proposed by Tolstikhin _et al._. It was the first architecture to show that although convolutions and self-attention are sufficient to achieve top performance, they are not necessary. The MLP-Mixers take inspiration from ViT's use of patches for input representation but fully use multi-layer perceptrons instead of the typical self-attention mechanism of Transformers. For our experiments, we make use of the original MLP-Mixer , the ResMLP variation , and the gMLP  which uses a special gating unit to enable cross-token interactions.

**Hybrids:** Many efforts have been made to incorporate features from CNNs into Transformers and vice versa. We group this set of mixed models under the same 'Hybrids' category. Namely ConvMixer , CoATNet , and MaxViT . ConvMixer combines a transformer-like patch design with convolution-based layers to mix patch embeddings' spatial and channel locations. CoATNet vertically stacks convolution layers and attention layers. Finally, MaxViT hierarchically stacks repeated blocks composed of multi-axis self-attention and convolutions.

**MetaFormers:** Although the success of transformer-based architectures was longly attributed to self-attention, recent works [73; 74] have started to identify the overall structure of the transformer block as the critical component in achieving competitive performances. Based on this hypothesis,

Figure 2: Model Accuracy Analysis across CIFAR-10 splits grouped by the architecture type. The presented results depict the accuracy performance of various models on distinct partitions of the CIFAR-10 dataset. Each split represents a unique partition, progressing from an IID scenario (Split-1) to a non-IID scenario (Split-3). The models under comparison exhibit similar parameters, FLOPS, and approximately matching ImageNet and centralized accuracies. Notably, Metaformer-like architectures, including Metaformers (e), Transformers (b), and MLP-Mixers (d), exhibit a comparatively smaller performance degradation on Split-2 and Split-3. The best-performing models are CA-Former and ConvFormer, which are Metaformers with advanced token mixers.

Metaformer is constructed from Transformers without specifying the design of the token mixers. If the token mixer is self-attention or spatial MLP, the architecture becomes a Transformer or an MLP-Mixer.

Although most of the Transformers and MLP-Mixers formally fall into the 'Metaformers' structure, we group in this family only the works published by the authors under this name. Within the networks selected, we have some working with simple operators in the token mixers, published with the explicit intent of pushing the limit of the Metaformer structure. Respectively, RandFormer, IdentityFormer , and PoolFormer  have a random matrix, an identity mapping, and a pooling operator as the token mixer. Moreover, ConvFormer  applies depthwise separable convolutions as the token mixer, and CAFormer  further combines it with vanilla self-attention in the deep stages. We also add to this category RIFormer , an architecture that optimizes the latency/accuracy trade-off by using re-parametrization  to remove the token mixer without significantly compromising the performance.

## 5 Experimental Setup

**Models:** This study aims to enrich the FL landscape by introducing a variety of cutting-edge architectures for computer vision. Within the FL framework, we conduct a comparative analysis of models that have similar parameters and FLOPS and roughly equivalent accuracy in centralized settings to the benchmark model, ResNet-50. Ensuring fairness, all the models under consideration fall within a similar parameter range (21-31M) and floating point operations per second (FLOPS) range (4-6G) 1. Each model exhibits a comparable centralized test accuracy on ImageNet-1K .

Unless explicitly stated, all models are pretrained on ImageNet-1K and subsequently finetuned on the specified dataset.

**Federated learning:** In an effort to ensure that the insights derived from this study remain independent of the training heuristics, we have standardized and simplified the training strategy across all architectures. Our approach involves performing FedAVG for a specific number of communication rounds to reach convergence: 100 rounds for both the CIFAR-10 and Fed-ISIC2019 datasets, 30 rounds for the CelebA dataset, and 200 rounds for the GLD-23K dataset. With the exception of the GLD-23K dataset, where we conduct five local steps, each round of local training encompasses one local epoch. The training regimen employs the SGD optimizer with an initial learning rate of 0.03, cosine decay, and a warm-up phase of 100 steps. We set the local training batch size at 32 and apply gradient clipping with a unitary norm to stabilize the training process.

The numerical values presented in this paper represent the average results derived from three repetitions of the same experiment, each with different initializations, to ensure the robustness and reproducibility of our findings. Regarding optimization, we conduct hyperparameter tuning for each model based on the set of values suggested by the respective authors in their original publications. The results reported in this paper reflect the optimal outcomes derived from the hyperparameter search. A more in-depth discussion is given in the supplementary material.

**Datasets:** We conduct experiments on four different datasets, CIFAR-10 , CelebA , Fed-ISIC2019 [68; 5; 7] and Google Landmarks Dataset v2 (GLD-23K) . Across the experiments, all the images are resized to a standard dimension of 224 x 224. The data heterogeneity of the different datasets used in our experiments is characterized in terms of Label Distribution Skew (LDS), Data Distribution Skew (DDS), and Feature Distribution Skew (FDS). The LDS is characterized by a heterogeneous number of samples per class for each client. In the DDS, the amount of data per client varies. Lastly, the FDS is defined by varying image features per class across clients due to changes in acquisition sensors, image domains, user preferences, geographical locations, etc. (see supplementary materials for more details).

The **CIFAR-10** dataset comprises 60,000 32x32 color images in 10 distinct classes representing common objects. Since CIFAR-10 is not inherently a federated dataset, we artificially simulate three federated data partitions across clients following the approach outlined in . The original test set is retained as the global test set, and 5,000 images are set aside from the training set for validation purposes, resulting in a revised training dataset of 45,000 images. We employ the Kolmogorov-Smirnov statistic (KS) to simulate one Independent and Identically Distributed (IID) data partition, namely Split-1 (KS=0), ensuring balanced labels per client, and two non-IID partitions, Split-2 (KS=0.5) and Split-3 (KS=1), with label distribution skew. In Split-2, each client has access to four classes and does not receive samples from the remaining six classes. In Split-3, each client strictly sees samples from two classes only. Therefore, this is the most challenging partition, featuring a pronounced class imbalance. Each data partition has five clients.

The **CelebA** dataset is a large-scale face attributes dataset featuring over 200,000 high-resolution celebrity images. Each image is annotated with 40 binary labels that denote the presence or absence of distinct facial attributes. In this study, we employ the federated version of the CelebA dataset proposed by the LEAF benchmark . In line with the methodology outlined in , we test the models on a binary classification task, determining whether a celebrity is smiling. The dataset is partitioned across 227 clients, each representing a specific celebrity and collectively accounting for 1,213 images. The dataset presents significant challenges due to the large-scale setting involving numerous clients. Each client has access to only a handful of samples, five per client on average, and in some instances, data from only one class is available.

The **Fed-ISIC2019** dataset contains skin lesion images collected from four hospitals. We adopt the partitioning strategy proposed by , which is based on the imaging acquisition systems utilized. Given that one hospital employed three distinct imaging technologies over time, the federated dataset is distributed across six clients, encompassing 23,247 images. The task involves classifying the images into eight melanoma classes with high label imbalance across clients. Additionally, the dataset exhibits quantity skew with disparity in the number of images per client. The largest client possesses more than half the data, whereas the smallest has only 439 samples.

The **Google Landmark-V2** dataset is a vast collection with over 5 million images of global landmarks. We utilize the federated GLD-23K partition, involving 233 smartphone users as clients contributing 23,080 images across 203 classes. These datasets exhibit a geographically influenced, long-tailed distribution, with some landmarks having many representations while others have very few, mirroring real-world disparities. Also, photographers' images present unique distributional characteristics tied to their habits and locations, resulting in large diversity and imbalance across clients.

## 6 Experimental Results

### Optimizers and Complex Networks in Federated Learning

An effective learning system typically requires both an optimized architecture and an effective optimization technique to shape the learning process and resultant model. The federated learning community has focused considerably on optimization techniques to address challenges related to non-IID data and the need to balance statistical diversity and leverage similarities in client data for central model training. However, most recent studies  rarely present results incorporating the latest architectures developed by the computer vision community. Many of these publications on vision tasks present their work on simple and shallow networks ranging from a few fully connected layers to the ResNet-50 model. This discrepancy highlights a significant chasm between the research focus of federated learning and state-of-the-art computer vision architectures.

    &  &  \\  Architecture & ResNet-50 & ConvNext-T & CAFformer-S18 & ResNet-50 & ConvExT-T & CAFformer-S18 \\  FedAVG  & \(59.9 1.87\) & \(93.5 0.36\) & \(97.1 0.35\) & \(67.6 1.59\) & \(78.6 0.29\) & \(83.1 0.49\) \\ FedAVGM  & \(80.2 0.49\) & \(93.4 0.48\) & \(97.0 0.14\) & \(66.2 2.16\) & \(79.0 0.52\) & \(83.8 0.37\) \\ FedProx  & \(79.0 0.41\) & \(93.4 0.16\) & \(97.1 0.25\) & \(68.0 1.93\) & \(79.4 1.13\) & \(83.0 0.79\) \\ SCAFFOLD  & \(78.1 0.06\) & \(94.3 0.30\) & \(96.9 0.21\) & \(62.2 0.26\) & \(76.0 1.24\) & \(77.1 0.61\) \\   

Table 1: Accuracy of ResNet-50, ConvNext-T, and CAFformer on CIFAR-10 and Fed-ISIC2019 across four different optimization methods. The average test accuracy of the model is displayed along with the standard deviation of three repetitions of the experiments.

To evaluate the compatibility of advanced architectures with federated optimization methods, we consider the widely used ResNet-50 model in our study and extend it to more recent computer vision architectures. We compare different models with the baseline ResNet-50, combined with diverse optimization techniques across four federated datasets. Alongside ResNet, we employ ConvNext, proposed to narrow the gap of convolutional with transformer architectures, and CAFormer, a recently introduced model in the Metaformer family. We consider four of the most popular optimizers in federated learning, namely FedAVG , FedAVGM (enhanced with momentum) , FedProx , and SCAFFOLD .

Results reported in Table 1 can be summarized as follows. First, using the standard FedAVG replacing ResNet-50 with the above computer vision architectures significantly increases accuracy across all datasets, especially with the CAFormer architecture, suggesting that architectural modification can significantly boost performance. Secondly, combining optimizers with complex models does not enhance results, with most of the performance maintaining a steady status, except for FedAVGM applied with ResNet on the CIFAR-10 dataset. In fact, for the Fed-ISIC2019 dataset, some of the optimizers proved to be detrimental to the performance of the network when compared to the simple FedAVG (e.g., SCAFFOLD for all architectures, and ResNet with FedAVGM and FedProx). The lack of optimization benefits across different datasets and architectures can be attributed to the complexity of modern deep learning architectures, such as Transformer-based models, which are significantly more complex than models typically used in FL research, leading to a more challenging optimization landscape. Applying these optimization techniques to the tough non-convex optimization problems encountered during the training of modern deep neural networks remains an unresolved issue. Most FL methods have demonstrated efficacy for certain types of non-convex problems under specific assumptions that might not be applicable to these complex models in practice [10; 30]. Our experiments reveal that the naive application of these techniques, combined with advanced models, often leads to negligible improvement.

This leaves us with the unresolved question of how we can optimize federated learning performance for practical applications, bridge the gap between advancements in the computer vision and FL communities, and address the issue of closing the performance gap with centralized settings. Inspired by our findings, we inspect the benefits of architectural design choices. Replacing the architecture is an easy-to-implement strategy, given the wide array of pretrained models available and their proven effectiveness on various vision tasks. Moreover, cutting-edge architectures often include design elements that inherently facilitate learning, such as superior feature extraction, incorporation of prior knowledge (inductive biases), and regularization that could mimic the effects of federated optimization techniques. In the following Section 6.2, we explore the possibility of leveraging the inherent strengths of different computer vision models to enhance performance and naturally bridge the non-IID gap in federated learning.

### Tackling the Heterogeneity Challenge with Architectural Design

We comprehensively compare network architectures from diverse network families, such as CNNs, Vision Transformers, Hybrids, MLP-Mixers, and Metaformers, with a particular focus on how each architecture family performs against each other. Tables 2 and 3 show our results on CIFAR, CelebA, Fed-ISIC2019, and GLD-23k datasets.

**Architectural Comparison on CIFAR-10:** On CIFAR-10, all models show performance degradation compared to the ideal centralized training, with most of the architectures displaying a significant reduction in accuracy as we progress to the non-IID splits (see Figure 2). Table 2 shows the performance of each model across all splits and the centralized version. The key observations from this study are summarized below.

_Convolutional Networks:_ Convolutional models exhibit the overall highest performance degradation among all families. ResNet  and EfficientNet  are found to be non-robust in heterogeneous settings, with a loss of 36 and 17.1 points, respectively, observed in Split-3. While ConvNeXt  shows improvement over its predecessors, it still falls short of achieving top results when compared to other model families.

_Transformers and Metaformers:_ Transformers consistently demonstrate robust performance, particularly in non-IID data partitions, exhibiting only a marginal average degradation of 1.7 points. Notably, ViT  achieves remarkable resilience, attaining an accuracy of 96.8% and experiencing a minimal drop of merely 0.8%, even in the most challenging split. These findings corroborate the earlier observations made by Qu _et al._ regarding the robustness of ViT models against data heterogeneity. However, Qu _et al._ attribute the superior performance of Transformers, compared to convolutional networks, to the robustness of self-attention in mitigating distribution shifts. In contrast, the emergence of the Metaformer family, employing various token-mixers including convolution operation, suggests that the remarkable proficiency exhibited by Transformers may be primarily attributed to the underlying architectural design of Metaformers, rather than the exclusive reliance on self-attention in models like ViT. Indeed, even Metaformers employing basic token mixers, such as RandFormer with a random token mixer, deliver remarkably satisfactory performances. On the challenging Split-3, CAFormer  garners the highest result, achieving an accuracy of 97.1% and delivering exemplary performance across all splits, followed by fully convolutional token mixer (without self-attention) based ConvFormer , which is also on par with ViTs.

_MLP-Mixers:_ As for MLP-Mixers, on average, their performance on the Split-3 is close to those of Transformers. The best is the gMLP  with 0.8% of degradation (second only to ViT) and an accuracy of 96.1%. Finally, Hybrid models achieve reasonable performance, but their results are far behind those of the other families. It is worth noticing that most models achieve higher accuracy in Split-1 compared to the centralized setting. We conjuncture that this is due to a regularizing effect caused by each client's access to the IID data partition (with uniform distribution over 10 classes).

**Architectural Comparison on CelebA, Fed-ISIC2019 and GLD-23k:** The results for CelebA, Fed-ISIC2019 and GLD-23k are reported in Table 3.

_CelebA_: The highest accuracies for CelebA are the ones of Swin  (89.0) and ConvFormer  (88.1), followed by MLP-Mixer  and CAFormer . Again, all Metaformer-like architectures (encompassing Transformers and MLP-Mixers) achieve the best results, far surpassing those of CNNs and Hybrid models, whose average performance is 84 and 74.9, respectively.

_GLD-23K:_ On the GLD-23K, we also observe that ViT remains among the highest-performing networks. However, Metaformers such as CaFormer and ConvFormer do not yield high performance as in previous datasets.

_Fed-ISIC-2019:_ Finally, on the Fed-ISIC-2019 dataset, which is naturally federated (due to the differences in the image acquisition systems across clients), we find CAFormer and ViT as the best

   Model & Central & Split-1 & Split-2 & Split-3 \\  ResNet-50  & \(95.9 0.21\) & \(96.8 0.17\) & \(92.7 0.25\) & \(59.9 1.87\) (\( 36.0\)) \\ EfficientNet-B5  & \(97.6 0.21\) & \(97.8 0.06\) & \(95.1 0.06\) & \(80.5 0.92\) (\( 17.1\)) \\ ConvNeXt-T  & \(97.2 0.15\) & \(97.6 0.06\) & \(96.6 0.21\) & \(93.5 0.36\) (\( 3.7\)) \\  Swin-T  & \(97.6 0.15\) & \(97.9 0.10\) & \(97.6 0.06\) & \(95.7 0.15\) (\( 1.9\)) \\ ViT-S  & \(97.6 0.06\) & \(98.5 0.10\) & \(98.2 0.12\) & \(96.8 0.15\) (\( 0.8\)) \\ SwinV2-T  & \(96.4 0.00\) & \(97.4 0.10\) & \(96.4 0.15\) & \(93.7 0.38\) (\( 2.7\)) \\ DeiT-S  & \(96.8 0.12\) & \(97.8 0.06\) & \(97.2 0.12\) & \(95.2 0.15\) (\( 1.6\)) \\  ConvFormer-S18  & \(97.9 0.12\) & \(98.4 0.12\) & \(98.0 0.12\) & \(96.8 0.35\) (\( 1.2\)) \\ CAFormer-S18  & \(98.1 0.18\) & \(98.5 0.06\) & \(\) & \(\) (\( 1.0\)) \\  RandFormer-S36  & \(95.7 0.12\) & \(95.8 0.10\) & \(94.6 0.06\) & \(91.3 0.35\) (\( 4.4\)) \\ IdentityFormer-S36  & \(95.3 0.26\) & \(95.6 0.12\) & \(94.6 0.12\) & \(90.8 0.45\) (\( 4.5\)) \\ PoolFormer-S36  & \(97.1 0.25\) & \(97.4 0.10\) & \(96.9 0.06\) & \(95.2 0.12\) (\( 1.9\)) \\ RifFormer-S36  & \(95.9 0.08\) & \(96.6 0.16\) & \(95.5 0.15\) & \(93.0 0.10\) (\( 2.9\)) \\  MLPMixer-S/16  & \(94.8 0.10\) & \(96.5 0.12\) & \(95.6 0.06\) & \(92.7 0.06\) (\( 2.1\)) \\ ResMLP-S24  & \(96.2 0.10\) & \(97.2 0.10\) & \(96.7 0.06\) & \(94.1 0.17\) (\( 2.1\)) \\ gMLP-S  & \(96.9 0.26\) & \(98.0 0.06\) & \(97.6 0.12\) & \(96.1 0.23\) (\( 0.8\)) \\  ConvMixer-768/32  & \(97.4 0.12\) & \(97.3 0.06\) & \(94.4 0.15\) & \(74.1 1.07\) (\( 23.3\)) \\ CoAtNet-0  & \(97.7 0.15\) & \(98.3 0.15\) & \(97.5 0.06\) & \(94.2 0.06\) (\( 3.4\)) \\ MaxViT-T  & \(98.0 0.10\) & \(\) & \(97.3 0.12\) & \(92.0 1.15\) (\( 6.0\)) \\   

Table 2: Performance of different types of models across all splits of CIFAR-10. The average test accuracy of the model is displayed along with the standard deviation of the experiments. Split-3 shows the degradation compared to the centralized version of the training.

performing models (83.1 and 82.3 accuracy). In fact, we can see a significantly superior performance from all models following the Metaformer structure, indistinctly of their use of convolutions and/or self-attention in their basic block structure.

In summary, our comprehensive experiments on 19 state-of-the-art architectures demonstrate that the choice of architecture and its components play a noteworthy role in tackling performance degradation caused by data heterogeneity. As a matter of fact, the experiments indicate that selecting an appropriate architecture design can have a greater impact on closing the gap with centralized training than the choice of the optimizer. For example, Table 1 shows that CAFormer with simple FedAVG outperforms all other models even when using more advanced optimization methods.

#### 6.2.1 How architectural design improves performances - Normalization Layer

Based on the results observed in the previous sections, we further investigate the performance reduction by delving into the contribution of specific architectural components. Our objective is to identify some key structural elements to achieve stronger performances in heterogeneous settings.

As shown in Table 3, models making use of Batch Normalization (like ResNet, EfficientNet, and ConvMixer) are consistently outperformed by models with Layer Norm on Split-3. This insight sheds light on why older convolutional (such as ResNet and EfficientNet) models have such high degradation. On the contrary, ConvNext, which uses Layer Norm, reaches satisfactory performance, contradicting the initial conclusion of  that convolutions are inherently problematic in the federated learning scenario under non-IID conditions. Moreover, we observe how ConvFormer, a model without Self-attention based blocks (based primarily on convolutions) but with a MetaFormer-like structure and Layer Norms, regularly reaches top performance even under the most challenging data partitions.

To further support our claims, we perform experiments with three different models, namely PoolFormer, CoAtNet, and ResNet. Each model trains with a variant using Batch Norm and another using Layer Norm. Lastly, we include a training scheme with FedBN  (a method developed specifically to deal with the performance degradation caused by the Batch Norm layer). Results (displayed in Table 4) show a trend indicating the superiority of Layer Norm over Batch Norm, even when using a tailored aggregation method for the latter (e.g., FedBN ).

   Model & Norm & Main Operation & CeleA & GLD-23K & Fed-ISIC2019 \\  ResNet-50  & BN & Conv & \(84.9 1.48\) & \(54.3 0.48\) & \(67.6 1.59\) \\ EfficientNet-B5  & BN & Conv & \(79.5 0.50\) & \(51.9 1.21\) & \(70.0 0.64\) \\ ConvNeXt-T  & LN & Conv & \(87.5 0.87\) & \(65.5 0.57\) & \(78.6 0.29\) \\  Swin-T  & LN & SA & \(\) & \(72.1 0.67\) & \(81.9 0.42\) \\ ViT-S  & LN & SA & \(87.3 0.53\) & \(\) & \(82.3 0.95\) \\ SwinV2-T  & LN & SA & \(86.8 0.62\) & \(74.4 0.02\) & \(81.7 0.50\) \\ DET-S  & LN & SA & \(87.4 0.60\) & \(69.1 1.24\) & \(82.3 0.59\) \\  ConvFormer-S18  & LN & Conv & \(88.1 0.42\) & \(53.8 1.11\) & \(81.1 1.54\) \\ CAFormer-S18  & LN & SA+Conv & \(87.5 0.49\) & \(57.8 0.54\) & \(\) \\  RandFormer-S36  & LN & Conv & \(83.9 0.35\) & \(56.3 0.10\) & \(77.3 0.23\) \\ IdentityFormer-S36  & LN & Conv & \(85.8 0.49\) & \(56.0 2.07\) & \(76.9 1.43\) \\ PoolFormer-S36  & LN & Conv+Pool & \(86.4 0.61\) & \(55.8 1.41\) & \(79.6 0.29\) \\ HiFOME-S36  & LN & Conv & \(87.2 0.37\) & \(69.4 0.20\) & \(81.9 1.37\) \\  MLPmixer-S/16  & LN & Conv & \(87.9 0.51\) & \(71.3 0.32\) & \(80.5 1.62\) \\ ResMLP-S24  & - & Conv & \(87.0 0.35\) & \(64.8 0.59\) & \(81.1 0.45\) \\ gMLP-S  & LN & Conv & \(86.6 0.31\) & \(67.4 0.16\) & \(79.9 1.60\) \\  ConvMixer-768/32 & BN & Conv & \(56.5 1.07\) & \(45.0 0.76\) & \(58.6 2.02\) \\ CoAtNet-0  & BN+LN & SA+Conv & \(82.2 1.66\) & \(70.4 0.25\) & \(66.0 3.40\) \\ MaxVIT-T  & LN & SA & \(86.1 0.72\) & \(71.7 0.90\) & \(70.1 1.27\) \\   

Table 3: Accuracy of the model families on CelebA, GLD-23K, and Fed-ISIC2019. For each model, we report the type of normalization layer, either LayerNorm or BatchNorm (LN/BN), and main operation employed (Convolution/Self-Attention). The mean accuracy of all repetitions is displayed along with the standard deviation.

Finally, as in , we measure and report the weight divergence of the models with respect to the centralized training. Figure 3 shows the increase of divergence for the models using BN to their LN counterparts, supporting the selection of Layer Norm over Batch Norm as an architectural choice for federated learning.

## 7 Limitations

Our research primarily explores visual recognition tasks within the federated learning domain, and we have not explored potential downstream tasks, such as object detection and segmentation. It's worth noting that visual recognition models often serve as backbones for extracting input image features in downstream tasks, thereby suggesting that our observed performance trends could likely extend to these downstream tasks. We plan to address this research gap in future studies.

Furthermore, our approach did not design a completely new model from scratch. Instead, we demonstrated our results utilizing pre-existing models and components. Our study also underscores the limitations of employing established optimizers (e.g., FedAVG, FedAVGM, FedProx, SCAFFOLD) along with modern network architectures. However, we abstain from plunging into the creation of innovative optimization techniques. Instead, we focus on evaluating diverse architectural families on many practical federated learning datasets and optimizers. As a result, our investigation leaves an intriguing avenue for future research, integrating our findings to develop dedicated architecture and optimizers tailored specifically for federated learning (FL) scenarios. By exploring this direction, we anticipate the potential to further enhance the performance and efficiency of FL systems.

## 8 Conclusion

This research expands the architectural domain in federated learning, emphasizing architectural design for computer vision applications under data heterogeneity. Our experiments assess various state-of-the-art models from five architectural families across four federated datasets, demonstrating the potential of addressing FL data heterogeneity architecturally. We show that Metaformer-like architectures exhibit superior robustness in non-IID settings. In contrast to existing studies, we observe that convolution operations are not necessarily a sub-optimal choice for architectures in FL applications and highlight the negative impact of Batch Normalization layer on the model performance. Our study also shows that architectural alterations offer an effective and more practical solution to handle data heterogeneity in non-IID settings.