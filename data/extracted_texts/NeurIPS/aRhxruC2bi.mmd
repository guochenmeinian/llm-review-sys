# Towards Open-Vocabulary Semantic Segmentation

Without Semantic Labels

 Heeseong Shin\({}^{1}\)   Chaehyun Kim\({}^{1}\)   Sunghwan Hong\({}^{2}\)   Seokju Cho\({}^{1}\)

**Anurag Arnab\({}^{1,3}\)   Paul Hongsuck Seo\({}^{,2}\)   Seungryong Kim\({}^{,1}\)\({}^{1}\)**

\({}^{1}\)KAIST  \({}^{2}\)Korea University  \({}^{3}\)Google Research

{hsshin98, kchyun, seokju.cho, seungryong.kim}@kaist.ac.kr\({}^{1}\)

{sung_hwan, phseo}@korea.ac.kr\({}^{2}\)   aarnab@google.com\({}^{3}\)

 Corresponding authors

###### Abstract

Large-scale vision-language models like CLIP have demonstrated impressive open-vocabulary capabilities for image-level tasks, excelling in recognizing _what_ objects are present. However, they struggle with pixel-level recognition tasks like semantic segmentation, which additionally require understanding _where_ the objects are located. In this work, we propose a novel method, **PixelCLIP**, to adapt the CLIP image encoder for pixel-level understanding by guiding the model on _where_, which is achieved using unlabeled images and masks generated from vision foundation models such as SAM and DINO. To address the challenges of leveraging masks without semantic labels, we devise an online clustering algorithm using learnable class names to acquire general semantic concepts. PixelCLIP shows significant performance improvements over CLIP and competitive results compared to caption-supervised methods in open-vocabulary semantic segmentation. Project page is available at https://cvlab-kaist.github.io/PixelCLIP

## 1 Introduction

Semantic segmentation is a fundamental task in computer vision where the goal is to identify class labels for each pixel within the given image. However, segmentation datasets often require extensive human effort to obtain densely-annotated semantic labels, limiting their scalability. In this regard, recent advances in large-scale pre-trained vision-language models, _e.g._ CLIP  and ALIGN ,

Figure 1: **Illustration of different approaches for open-vocabulary semantic segmentation. In contrast to existing methods utilizing (a) pixel-level semantic labels  or (b) image-level semantic labels , we leverage unlabeled masks as supervision, which can be freely generated from vision foundation models such as SAM  and DINO .**have facilitated open-vocabulary semantic segmentation [1; 3; 2; 17; 4; 6], which aims to generalize semantic segmentation into unbounded range of classes. Despite showing remarkable generalization capabilities, they still require pixel-level semantic labels for leveraging the image-level pre-trained vision-language models for semantic segmentation.

Recently, several studies [11; 12; 7; 8] have pioneered open-vocabulary semantic segmentation without densely-annotated semantic labels. These studies often utilize image-level semantic labels, such as image captions, to enhance the pre-trained vision-language models like CLIP for semantic segmentation. However, image captions typically provide information about _what_ is in the image, but without _where_ it is. Since CLIP is already effective in recognizing _what_ the objects are, this causes models to only implicitly learn object locations, leading to sub-optimal performance or requiring millions of image-caption pairs to compensate for this weak supervision [8; 7]. Instead, we focus on informing CLIP about _where_ objects are located to address the missing information.

In this study, we propose a novel approach to achieve open-vocabulary semantic segmentation _without_ leveraging semantic labels, but through guiding the pre-trained vision-language models, such as CLIP, on _where_ to look. We leverage recent vision foundation models (VFMs), such as DINO  and SAM , to partition images into fine-grained regions to indicate _where_ to look. Consequently, we explore methods to effectively leverage these masks for fine-tuning the image encoder of CLIP.

In contrast to existing works that leverage semantic labels [6; 19; 7], we do not have any captions or class names that can be fed to the text encoder of CLIP. To leverage its knowledge, we devise a method that employs prompt learning [20; 21] on the text encoder of CLIP to construct _learnable classes_. Setting the learnable classes as a centroid, we propose applying the online clustering algorithm [18; 22] along the given masks to gather them into semantically meaningful groups, as shown in Fig. 2. We keep these learnable classes global across the entire images, which guides the learnable classes to contain the general semantic concepts. Despite the absence of semantic labels, our method is able to jointly leverage the image encoder and text encoder of CLIP during training, successfully achieving dense open-vocabulary recognition.

Our framework, called PixelCLIP, achieves significant improvements to CLIP, on average of \(+16.2\) mIoU in open-vocabulary semantic segmentation. Moreover, despite not using any semantic labels, PixelCLIP shows competitive performance in comparison to image-level supervised methods using captions [7; 9; 10], demonstrating the effectiveness of unlabeled masks for. We further show the effectiveness of PixelCLIP for classifying masks from various open-vocabulary segmentation models, which can be simply done by replacing the CLIP within existing methods. We also provide extensive ablation studies to validate our choices, with a detailed analysis of our method.

We summarize our contribution as follows:

* We propose a novel formulation of learning from images _without_ semantic label for open-vocabulary semantic segmentation by leveraging masks generated from DINO and SAM to fine-tune vision-language models.
* We propose to globally cluster semantically similar masks by employing an online clustering algorithm, while learning class prompts for representing semantic clusters.
* We demonstrate significant gains in open-vocabulary semantic segmentation, even surpassing methods leveraging image-level semantic labels, and provide thorough ablation studies with analysis to validate our framework.

Figure 2: **Visualization of masks from vision foundation models. We visualize the masks generated by SAM  and by clustering image features from DINO . Although such models can freely generate fine-grained masks, the resulting masks can be too small or incomplete to have semantic meaning. To address this over-segmentation issue, we employ online clustering  of the masks into semantically meaningful groups defined globally for given images.**

Related Work

### Open-vocabulary semantic segmentation

Open-vocabulary semantic segmentation [2; 23] aims to label each pixel within an image into an unbounded range of classes. In this regard, recent works [1; 17; 2; 6; 24] aim to generalize to classes unseen during training through leveraging pre-trained vision-language models, such as CLIP . Despite their remarkable performance, they leverage per-pixel semantic labels during their training, which requires expensive cost to annotate. Instead, we focus on the weakly-supervised setup, where the goal is to zero-shot transfer to segmentation task _without_ densely-annotated class labels [11; 12; 25; 5; 26; 7; 27], utilizing image-level labels as supervision or even no labels at all.

In this regard, recent studies [11; 12; 25; 5] leverage image caption as supervision. GroupViT  and ViL-Seg  are pioneering works for identifying groups or clusters emerging from captions. Along with the advance of vision-language models, SegCLIP  and TCL  leverage pre-trained CLIP and learn additional decoder modules to learn dense vision-language alignment. PACL  learns additional embedding layers to enhance the patch-level alignment in vision-language models and SAM-CLIP  attempts to merge SAM  and CLIP  into a unified model by additionally leveraging unlabeled mask data from SAM. Apart from these approaches, we avoid employing _any_ semantic labels [26; 28], but leverage vision foundation models to obtain masks as a source for supervision to fine-tune the CLIP image encoder for achieving open-vocabulary semantic segmentation.

### Fine-tuning vision-language models for dense prediction

Recent large-scale pre-trained vision-language models have shown its effectiveness for jointly understanding images and language [29; 15; 16]. Notably, CLIP , trained with web-scale image-caption pairs, has been widely popularized for transferring its open-vocabulary recognition capabilities to various downstream tasks [26; 30; 31; 32]. However, despite its success in image-level tasks like image classification, CLIP tends to struggle in dense prediction tasks [17; 26; 6], such as object detection and semantic segmentation. This originates from CLIP being trained from image-level supervision being captions, hence exhibits bias towards the global image rather than fine-grained regions within the image . While non-learnable approaches, such as MaskCLIP  show improvements by slightly modifying the architecture, CLIP still shows limited capabilities in dense predictions in comparison to its global understanding.

To address this, OWL-ViT  directly fine-tunes pre-trained vision and text encoders to downstream open-vocabulary detection task, and CAT-Seg  introduces a cost aggregation scheme for fine-tuning the encoders of CLIP for semantic segmentation. Alternatively, ZegCLIP  and Xu et al.  implement prompt tuning [21; 20] for tuning the image and text encoders of CLIP. Instead of fine-tuning the full model, they learn prompt tokens that serve as a global prefix for the encoders of CLIP. While such methods show remarkable results from fine-tuning the encoders of CLIP for dense downstream tasks, they require densely annotated detection and segmentation data for training.

### Vision foundation models

With the advent of large-scale learning enabled by scalable vision backbone architectures [34; 35] and vast amounts of data, diverse vision foundation models are emerging in the field of computer vision. In this regard, self-supervised methods [36; 37; 38; 39] have demonstrated the effectiveness of its rich visual representations for various downstream tasks. Especially, DINO  exerted strengths in fine-grained semantic recognition [40; 41], making it highly effective for object detection and image segmentation. Moreover, DINO features have also been demonstrated for yielding fine-grained masks within the image through applying the \(k\)-means clustering with its features [42; 43; 44].

On the other hand, the segment anything model (SAM)  has demonstrated its capability for generating fine-grained, high-quality segmentation masks for any object in an image. Through its self-annotation pipeline, SAM has collected an unprecedented amount of mask annotation for achieving its capabilities. While we can freely leverage SAM to obtain detailed masks in any given image, we mainly utilize the pre-computed masks within the collected dataset, SA-1B. Both DINO and SAM, however, yield unlabeled masks without semantic labels as both models are also trained without semantic labels, presenting a challenge for leveraging their masks for achieving dense vision-language recognition.

## 3 Methodology

In this section, we first establish our problem formulation of learning dense vision-language alignment from images paired with masks, generated from vision foundation models. Next, we discuss the challenges of leveraging masks as supervision for fine-tuning the image encoder of CLIP and finally, present our methodology of semantic clustering of masks to address the challenges.

### Preliminaries

Given an input image \(I^{H W 3}\), open-vocabulary semantic segmentation [6; 7] aims to label each pixel within an image with classes given in free-form text. As a training signal, semantic labels offer a set of \(S\) textual descriptions for a semantic class \(T=\{T_{i}\}_{i=1}^{S}\) related to \(I\). This can be directly utilized with the CLIP text encoder \(_{L}()\) to obtain text features \(f_{T}=_{L}(T)^{S d}\), where \(d\) is the hidden dimension. Dense image features \(f_{I}=_{V}(I)^{h w d}\), where \(h w\) is the output feature resolution, are then extracted. We finally obtain dense image-text similarity map \(M_{IT}^{h w S}\):

\[M_{IT}(x,y,n)=(x,y) f_{T}(n)}{\|f_{I}(x,y)\|\|f_{T}(n)\|}.\] (1)

This can be interpreted as soft binary masks predicted from image and text features of CLIP, and be supervised with binary mask loss \(_{}\) in a pixel-level manner to fine-tune CLIP .

### Integrating masks into CLIP features

In this work, we do not have any access to \(T\), but are only given unlabeled masks \(M^{H W N}\), where \(N\) denotes the number of masks for the given image \(I\). Hence, we devise methods to predict masks by incorporating \(M\) into CLIP features. We aim to fine-tune the CLIP image encoder \(_{V}()\) through leveraging unlabeled masks \(M\) as supervision. Since \(M\) is generated from vision foundation models, _e.g._ DINO or SAM, this presents us with the challenge of not having any semantic labels.

In order to integrate masks into CLIP, a straightforward approach would be employing the masks \(M\) with the CLIP image feature map \(f_{I}\) to obtain per-mask CLIP features. While there could be various methods to extract regional CLIP features [26; 45; 5], we apply mask pooling over \(f_{I}\) to obtain mask

Figure 3: **Illustration of our overall framework. We provide illustration of PixelCLIP, utilizing unlabeled images and masks for fine-tuning the image encoder of CLIP, enabling open-vocabulary semantic segmentation. We note that the momentum image encoder and the mask decoder are only leveraged during training, and inference is only done with image and text encoders of CLIP.**pooled features \(f_{M}=(f_{I},M)^{N d}\). Consequently, we can leverage \(f_{M}\) to obtain image-_mask_ similarity map denoted \(M_{IM}^{h w N}\):

\[M_{IM}(x,y,n)=(x,y) f_{M}(n)}{\|f_{I}(x,y)\|\|f_{M}(n)\|}.\] (2)

This allows us to supervise the model with a binary mask loss \(_{}\) for fine-tuning CLIP with given image \(I\) and unlabeled masks \(M\). In practice, since \(M_{IM}\) has the same resolution as the feature map from the CLIP image encoder \(f_{I}\), we employ a light-weight decoder \(\) to mitigate the resolution gap between \(M_{IM}\) and \(M\), as shown in Fig. 3. This can be written as \(:^{h w}^{h^{} w^ {}}\), where \(h^{} w^{}\) is resolution for the upsampled mask. Therefore, the output of the model can be updated as \(=(M)\).

### Semantic clustering of masks

Upon using mask pooled CLIP image features \(f_{M}\) to predict \(M_{IM}\), however, we find the masks generated from DINO and SAM to often over-segment the image, resulting in too small or incomplete masks as seen in Fig. 2. This would require CLIP to forcefully discriminate regions that are semantically similar, impeding the training process.

In this regard, we propose to group semantically similar masks into _clusters_ and predict based on the clusters rather than individual masks. Moreover, we aim to define this cluster _globally_, which is shared across the entire training process rather than for each image or iteration. This would be analogous to constructing pixel-level semantic labels, where a fixed set of classes defined over the dataset is equivalent to each cluster. However, the difference is that there is no pre-defined set of classes that we can define the clusters with. While we could heuristically pre-define such classes, we describe our learnable method for globally clustering masks into semantically meaningful groups.

Online clustering via learnable class prompts.To globally cluster masks into semantic categories, we propose representing these clusters using CLIP _text_ features as centroids for clustering mask features. Given that the CLIP text encoder is trained with a broad understanding of natural language semantics, we expect these clusters to capture meaningful semantics by leveraging its comprehensive pre-trained knowledge. In this regard, we take a learnable approach, where each cluster is defined by class-specific learnable prompts fed into the CLIP text encoder. Unlike existing prompt learning methods, which typically focus on learning a task-specific prefix [20; 21; 3], we aim to learn prompt tokens that represent each class. For instance, in the sentence "A photo of an object", traditional prompting methods would learn the tokens for the "A photo of a" prefix, whereas our method focuses on learning the token for the "object."

Specifically, given the number of clusters \(k\), we can define prompt tokens as \(C^{k l d_{c}}\), where \(l\) is the token length of the prompt and \(d_{e}\) is the dimension of the token embeddings. From this, we can utilize the CLIP text encoder \(_{L}()\) to obtain a set of _class_ features \(f_{C}=_{L}(P^{*},C)^{k d}\) in the form of CLIP text features, where \(P^{*}\) is a fixed template for the CLIP text encoder, such as "A photo of a {} in the scene." While we could assign each mask \(f_{M}\) with \(f_{C}\) in a winner-takes-all manner, we desire the classes to encode general semantics across all images. Therefore, we assume that we can equally divide \(m\) masks within a minibatch [18; 14], into \(k\) clusters given a sufficient amount of masks.

Consequently, we aim to find an assignment \(Q^{k m}_{+}\) based on the image-text similarity between the mask pooled features \(f_{M}\) and the class text features, which can be defined as:

\[_{Q}(Q^{}F_{M}^{}f_{C})+ H(Q),  Q^{k m}_{+}, Q^{} _{k}=_{m}, Q_{m}= _{k},\] (3)

where \(F_{M}\) is the set of all \(m\) mask features \(f_{M}\) within the minibatch, and \(_{k}\) denotes the \(k\)-dimensional vector of ones. \(H\) is the entropy function, \(H(Q)=-_{ij}Q_{ij}Q_{ij}\) with \(\) as a hyperparameter. The solution \(Q\) from Eq. 3 is an assignment matrix defining which of the \(k\) clusters each \(m\) mask should belong to, hence \(\) determines the smoothness of this mapping Q by scaling the entropy regularization from \(H\). The equipartition constraint, \(Q^{}_{k}=_{m},Q_{m}=_{k}\) encourages the class features \(f_{C}\) to be selected at least \(m/k\) times on average, allowing to learn general concepts represented by the masks within the dataset. In practice, with the soft assignment relaxation ,can be solved as follows:

\[Q=(u)(^{}f_{C}}{})(v),\] (4)

where \(u^{k}\), \(v^{m}\) denote renormalization vectors, which can be efficiently computed by Sinkhorn-Knopp algorithm .

Finally, we can re-write the prediction of our model to be a cosine-similarity map between \(f_{I}\) and \(f_{C}\):

\[M_{IC}(x,y,i)=(x,y) f_{C}(i)}{\|f_{I}(x,y)\|\|f_{C}(i)\|},\] (5)

thereby predicting masks for \(f_{C}(i)\) being the \(i\)-th class feature from \(f_{C}\), which we have obtained from clustering mask pooled features \(f_{M}\). Accordingly, ground truth masks \(M\) are also clustered according to \(Q\) by converting it into hard assignment with the argmax operator [47; 22]. This can be written as \(^{k H W}\) where \(_{i}\) is the union of masks assigned into the cluster represented by \(i\)-th learned class \(f_{C}(i)\).

Momentum encoder for integrating mask features.Since we jointly optimize the CLIP image encoder \(_{V}()\) as well as the learnable class feature \(f_{C}\), we may experience instability during our training process, or forgetting of the pre-trained knowledge . To stabilize the training, we keep a momentum encoder [39; 38] for obtaining \(f_{M}\) as seen in Fig. 3. Therefore, we update \(f_{M}\) as \(f_{M}=(}_{V}(I),M)\), where \(}_{V}\) is the momentum encoder of the CLIP image encoder, updated with momentum \(\). This can be denoted as \(^{}_{V}^{}_{V}+(1-)_{V}\), where \(_{^{}},_{V}\) are model parameters of \(}_{V}\) and \(_{V}\), respectively.

## 4 Experiments

### Implementation details

For training, we employ per-pixel binary cross-entropy loss as \(_{}\) to jointly train all of the components . For all our experiments, we use a single text prompt ''A photo of {} in the scene'' for \(P^{*}\), including for our learnable class prompts while training and for inference, we apply prompt ensemble strategy  with 7 additional prompts originally curated from CLIP . We train our model on SA-1B  dataset, where we randomly sample 5% of the images. We train for 10000 iterations with a batch size of 48 for all experiments. For experiments using masks from DINO, we obtain masks with \(k\)-means clustering where we set \(k=16\). For experiments using masks from SAM, we use the unlabeled mask annotation in the SA-1B dataset. Without specification, we report results on ConvNeXt-B  backbone with mask annotation from SAM, which takes approximately 6 hours to train with 4 NVIDIA A6000 GPUs. We provide more details in the supplementary materials.

### Experimental setting

Following Cha et al. , we evaluate our model on zero-shot transfer to semantic segmentation on the validation sets of COCO-Stuff , ADE-20K , PASCAL-Context , PASCAL VOC , and CityScapes . For CLIP , we apply MaskCLIP  for ViT backbone for extracting image features, and remove the global pooling layer for OpenCLIP  with ConvNeXt  backbone. We note that we do not apply any post-processing to the predictions and for the compared methods. For the evaluation metric, we employ the mean Intersection over Union (mIoU).

### Results

Open-vocabulary semantic segmentation.We provide results for quantitative comparisons in Tab. 1. We first compare with CLIP, and demonstrate remarkable gains in all benchmarks, bringing in an average of +16.2 mIoU improvement. Since we do not have comparable baselines without leveraging semantic labels, we further provide a comparison with image-level supervised methods [7; 9; 10]. Surprisingly, PixelCLIP surpasses TCL  and SegCLIP  in all benchmarks while using only a fraction of the images _without_ semantic labels. Furthermore, we show competitive performance compared to SAM-CLIP, which uses not only 40 million image-level semantic labels, but also leverages the SA-1B dataset on a similar scale to our framework.

Zero-shot mask classification.We provide results for evaluating mask classification in Tab. 2. We consider ZegFormer  and FC-CLIP  as baselines since they first predict masks, then employ CLIP as a zero-shot mask classifier within their framework, and also provide results with ground-truth masks to simulate having oracle mask predictions. For all methods, we apply masked pooling to CLIP image feature map to classify masks. For ZegFormer  and FC-CLIP , reported results are only from the zero-shot prediction branch to solely alone our gains. We highlight that PixelCLIP can be readily applied to existing frameworks that leverage CLIP as a zero-shot mask classifier, and bring instantaneous improvements by simply replacing the model and weights of CLIP.

Qualitative results.We provide qualitative results for open-vocabulary semantic segmentation in Fig. 4 compared with results from CLIP, highlighting the dense open-vocabulary recognition capabilities of our framework. We further provide qualitative results in the supplementary materials.

### Ablation studies

In Tab. 3, we show ablation studies on open-vocabulary semantic segmentation to validate our design choices. We report results without prompt ensembling for ablations, and also report results from OpenCLIP  as a baseline.

Component analysis.In Tab. 3 (a), we provide results for ablating our key components. Notably, we observe that without global semantic clustering of masks, the framework collapses and loses the pre-trained knowledge of CLIP. This validates the challenge presented by leveraging unlabeled masks and demonstrates the crucial role of our proposed clustering approach. Moreover, we observe constant improvements over all datasets with our learnable class prompt, proving our approach of leveraging the text encoder of CLIP to define the clusters in the form of prompt learning. We also observe constant gains with the momentum encoder for extracting mask pooled features \(f_{M}\).

Number of clusters.In Tab. 3 (b), we compare the results of the variants of the proposed method by varying the number of clusters \(k\). We find that scaling \(k\) does not necessarily guarantee performance boosting, but it generally improves until \(k\) is set to 64 and tends to degrade as \(k\) grows. Considering that with an extremely large number for \(k\), we can assign each of the masks to individual clusters(_e.g._ 1 billion for SA-1B.) This scenario would virtually be identical to not having semantic clustering as

seen in Table. 3 (a), and progressively growing \(k\) would slowly converge to this scenario. We further provide analysis in 4.5, studying the different aspects from varying \(k\).

Length to represent learnable class prompts.Tab. 3 (c) compares the effects of varying the length of the learnable class prompt, \(l\). We find that \(l=1\) shows lower scores in comparison to other lengths. We can interpret this as only describing a class with a single word, whereas having multiple words would better describe the depicted class. However, for \(l=4\) and larger, we find that increasing \(l\) does not result in a gain of performance, hence, we adopt \(l=4\) as default.

Effects of learnable prompt token.Finally, we compare PixelCLIP to having a pre-defined set of classes instead of using learnable prompt tokens. Specifically, we use 171 classes from COCO-Stuff , and do not apply online clustering for assignment when utilizing classes from COCO-Stuff, as it already yields text features with semantic meanings. We find apparent improvements over all the datasets as shown in Tab. 3 (d). We speculate that since the classes defined in COCO-Stuff are heuristically chosen, it is hard to ideally encompass various semantics and concepts that may appear in images, hence restricting the perception of the model to the finite set of classes.

Figure 4: **Comparison between PixelCLIP and CLIP. We provide qualitative comparison on ADE-20K  dataset with PixelCLIP and CLIP. We demonstrate the dense visual recognition capabilities achieved from fine-tuning CLIP, whereas CLIP shows results with significant noise.**

Table 3: **Ablation studies. We show results on open-vocabulary semantic segmentation for validating our design choices. We also report results from OpenCLIP  as baseline in the results.**

### Analysis

Learnable class prompt.We further analyze the learned class prompt in Fig. 5 (a-b) with \(t\)-SNE visualization on the text features encoded from the learned class prompts, as well as text features obtained from class names of COCO-Stuff. Since we initialize the class prompt tokens as random tokens, we observe that they are in a skewed distribution in the initial state. However, the learned prompts show that they are well-dispersed among the text features from COCO-Stuff, indicating that the class prompts have well-learned diverse semantic concepts within the text features. We observe well-distributed features both for \(k=64\) and \(k=128\).

Since the learned prompts should act as implicit class names, we visualize the results from inference with learned class prompts in Fig. 5 (c-d). Although both \(k=64\) and \(k=128\) show similar performance when evaluated, we observe that the prompts have learned more fine-grained semantics for \(k=128\). We generally observe human parts to be well distinguished; this could come from the SA-1B dataset, as there are numerous images with fine-grained masks representing human parts as annotations.

Interpreting learned classes.Considering the learned class prompts represent semantic concepts, we further study the learned embeddings by mapping each class embeddings to class names in COCO-Stuff with the highest cosine-similarity score. Fig. 6 shows results when we first inference the image features with learned class prompts, then map the results with the closest COCO classes. We can observe that with \(k=128\), as the prompt learns more diverse semantics, we observe more accurately mapped classes. However, we still see predictions with large disparity to the actual ground truth. We leave a more in-depth analysis of the learned classes for future investigation.

## 5 Conclusion

In this paper, we introduced PixelCLIP, a framework for leveraging unlabeled images and masks for fine-tuning the pre-trained vision-language models for open-vocabulary semantic segmentation. To address the unique challenges posed by incorporating unlabeled masks generated by vision foundation models into our framework, we propose global semantic clustering of the masks, with learnable class prompts to represent each cluster. We demonstrated PixelCLIP to show remarkable improvements to CLIP and its applicability to existing methods, providing instantaneous improvements, as well as surpassing methods that leverage image-level semantic labels such as image captions.

Figure 5: **Visualization of learned class prompts. We visualize the text features from our learned class prompts, as well as text features from classnames of COCO-Stuff with \(t\)-SNE visualization in (a-b). We also visualize images inferenced with the learned class prompts in (c-d).**

Figure 6: **Visualization of interpreting learned text prompt. We provide visualization on results for predicting with learned class prompts, then mapping the results to classes in the dataset with the highest similarity to the prompt.**Acknowledgement.This research was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2019-II190075, RS-2024-00509279, RS-2020-II201819, RS-2024-00398115, Research on the reliability and coherence of outcomes produced by Generative AI) and the Culture, Sports, and Tourism R&D Program through the Korea Creative Content Agency grant funded by the Ministry of Culture, Sports and Tourism (RS-2024-00348469, RS-2023-00266509), and National Research Foundation of Korea (RS-2024-00346597).