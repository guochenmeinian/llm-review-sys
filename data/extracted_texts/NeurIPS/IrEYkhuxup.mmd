# Why Did This Model Forecast This Future? Information-Theoretic Saliency for Counterfactual Explanations of Probabilistic Regression Models

Why Did This Model Forecast This Future? Information-Theoretic Saliency for Counterfactual Explanations of Probabilistic Regression Models

Chirag Raman Alec Nonnemaker Amelia Villegas-Morcillo Hayley Hung Marco Loog

Delft University of Technology, Delft, The Netherlands

{c.a.raman, a.o.villegasmorcillo, h.hung, m.loog}@tudelft.nl

a.m.nonnemaker@student.tudelft.nl

###### Abstract

We propose a post hoc saliency-based explanation framework for counterfactual reasoning in probabilistic multivariate time-series forecasting (regression) settings. Building upon Miller's framework of explanations derived from research in multiple social science disciplines, we establish a conceptual link between counterfactual reasoning and saliency-based explanation techniques. To address the lack of a principled notion of saliency, we leverage a unifying definition of information-theoretic saliency grounded in preattentive human visual cognition and extend it to forecasting settings. Specifically, we obtain a closed-form expression for commonly used density functions to identify which observed timesteps appear salient to an underlying model in making its probabilistic forecasts. We empirically validate our framework in a principled manner using synthetic data to establish ground-truth saliency that is unavailable for real-world data. Finally, using real-world data and forecasting models, we demonstrate how our framework can assist domain experts in forming new data-driven hypotheses about the causal relationships between features in the wild.

## 1 Introduction

As we go about our daily lives, engaging in conversations, walking down the street, or driving a car, we rely on our ability to anticipate the future actions and states of those around us [1; 2]. However, the numerous unknowns, such as hidden thoughts and intentions, make our predictions of the future inherently uncertain . To reflect this uncertainty, several machine learning methods in such settings forecast a full distribution over plausible futures, rather than making a single point prediction [3; 4]. Identifying the factors that influence such a model's forecasts is particularly useful for domain experts seeking to understand the causal relationships guiding complex real-world behaviors, especially in situations where the future is uncertain. In this work, we introduce and address a novel research question toward counterfactual reasoning in multivariate probabilistic regression settings: how can we identify the observed timesteps that are salient for a model's probabilistic forecasts over a specific future window? Specifically, we introduce the first post hoc, model-agnostic, saliency-based explanation framework for _probabilistic_ time-series forecasting.

We begin with a fundamental observation about human social cognition: we are averse to uncertainty and strive to minimize it . Consider the scenario where a pedestrian is approaching you on the street. Initially, there is uncertainty about which direction each of you will take to avoid a collision. As one of you changes direction, the other observes and takes the opposite direction, ultimately avoiding a collision. Concretely, the thesis of this work is to formalize the following notion of saliency: the timestep that changes the uncertainty of a predicted future is salient toward predicting that future.

For instance, in the aforementioned scenario, we posit that the moment when one pedestrian changes direction is salient toward forecasting the future trajectories of the pedestrians.

Our notion of saliency is grounded in preattentive human cognition and related to the concept of surprisal or information associated with observations [5; 6]. Praattentive saliency captures what the brain subconsciously finds informative before conscious, or attentive, processing occurs. An unexpected or surprising observation is considered salient in this context. However, when applied to forecasting, the idea of surprisal or informativeness must be _linked to the future outcome_. Consequently, we propose that a timestep that alters an observer's certainty about the future is surprising, and therefore, salient. Crucially, our unifying 'bottom-up' perspective treats a forecasting model like a human observer, providing a principled definition of saliency that is not arbitrarily tied to task-specific error metrics. In contrast, the 'top-down' or task-specific notions of saliency common in post hoc explainable artificial intelligence (XAI) literature suffer from several drawbacks. Computed saliency maps may not measure the intended saliency, and even be independent of both the model and data generating process [7; 8; 9]. Moreover, what constitutes a _good_ explanation is subject to the biases, intuition, or the visual assessment of the human observer [7; 10]; a phenomenon we refer to as the _interpretation being in the eye of the beholder_. Finally, as Barredo Arrieta et al. [11, Sec. 5.3] note, "there is absolutely no consistency behind what is known as saliency maps, salient masks, heatmaps, neuron activations, attribution, and other approaches alike."

To the best of our knowledge, no existing work addresses the specific task of obtaining post hoc model-agnostic explanations for probabilistic forecasts. Existing XAI methods for time-series data have predominantly focused on sequence classification, as we discuss in Section 2 and Appendix A. For regression, instead of post hoc explainability, researchers have emphasized interpretability by design  or intrinsic interpretability , where interpretability stems from the simple structure of models or coefficients of predefined basis functions [13; 14]. Against this backdrop, we present the following key contributions:

* Conceptual Grounding: We establish the conceptual foundation for linking saliency-based explanations with counterfactual reasoning. We draw upon insights from Miller's  work on explanations in artificial intelligence, highlighting the contrastive nature of explanations (Section 3).
* Information-Theoretic Framework: We extend Loog's  framework of bottom-up preattentive saliency to the domain of probabilistic forecasting. Specifically, we introduce a novel expression of saliency based on the differential entropy of the predicted future distribution, providing a closed-form solution for commonly used density functions in the literature (Section 4).
* Empirical Validation: We empirically evaluate our framework using synthetic and real-world data. In the synthetic setting, we achieve full accuracy in retrieving salient timesteps with known ground truth saliency. In real-world scenarios without ground truth saliency, we demonstrate the utility of our framework in explaining forecasts of social nonverbal behavior and vehicle trajectories, showcasing its effectiveness in complex and dynamic contexts (Section 5).

## 2 Related Work

XAI Techniques for Time-Series Data.The taxonomy commonly used for explainability methods categorizes techniques based on three criteria: (i) intrinsic or post hoc, (ii) model-specific or model-agnostic, and (iii) local or global . In the context of time-series regression, existing techniques predominantly focus on non-probabilistic settings and fall into the category of intrinsic and model-specific approaches. These include: (i) incorporating inductive biases through internal basis functions  (also extended to the probabilistic setting ), (ii) utilizing self-attention mechanisms in the model , and (iii) adapting saliency maps from computer vision to measure the contribution of features to the final forecast [16; 17]. For a comprehensive review of XAI methods across domains and time-series tasks, please refer to Appendix A.

Saliency-Based Explanations and Drawbacks.Saliency maps gained popularity as post hoc explanation tools for image classification [16; 18]. However, the lack of consistency in defining saliency has led to diverse interpretations, including occlusion sensitivity, gradient-based attribution heatmaps, and neuron activations [11; 12]. Nevertheless, these maps are typically computed by perturbing different parts of the input and observing the resulting change in the prediction error or output class. Several issues arise with the current use of saliency maps as explanations: (i) the feature-level manipulations used for saliency maps may distort the sample in ways that deviate from the real-world data manifold and destroy semantics [7; 8; 9]; (ii) given the arbitrary definitions, evaluating saliency maps becomes challenging and is subject to observer biases [12, Sec.10.3.2], which can lead to maps appearing correct even when they are insensitive to the model and data ; (iii) for forecasting, Pan et al.'s  notion of saliency based on the error between the point prediction and ground truth future is arbitrary and relies on ground truths unavailable during testing; and (iv) the saliency map is explicitly retrained for a single observed-future sequence, failing to capture salient patterns across similar observed sequences that result in divergent but plausible futures .

Model-Agnostic Techniques.The SHAP framework, which integrates ideas from Shapley Values, LIME, LRP, and DeepLIFT, has gained popularity as a model-agnostic approach . However, adapting these techniques to time-series tasks poses several challenges. Firstly, the Shapley methods rely on functions with real-valued codomains, such as a regression function \(f_{x}\)[19, see Eq. 4, 8], while our focus is on probabilistic models that output the distribution \(p_{Y|X}\) instead of some \(y=f_{x}()\) to handle future uncertainty. Adapting these methods to deal with full predicted distributions is nontrivial. Similarly, gradient-based approaches compute gradients with respect to a single output instead of a full distribution. Secondly, these methods provide feature importance measures for a single output, whereas in time-series analysis, we are interested in identifying the importance of an observed timestep for an _entire future sequence_. That is, the joint consideration of the entire future sequence when computing input importance measures is challenging. As Pan et al.  note, in evaluating single-time predictions, these methods "ignore crucial temporal information and are insufficient for forecasting interpretation". Finally, similar to perturbation-based saliency methods, the sampling of features from arbitrary background samples in methods like Shapley/SHAP can lead to _Frankenstein Monster instances_[12, Sec. 9.5.3.3] that may not be valid samples on the data manifold. This undermines the semantics of the data, particularly in scenarios like motion trajectories, where randomly replacing features can result in physically impossible or glitchy motions.

## 3 Conceptual Grounding: Linking Saliency-Based Explanations to Counterfactual Reasoning

Given the challenges in XAI where speculations are often presented in the guise of explanations , we argue for grounding the concept of explanation within established frameworks of how humans define, generate, and present explanations. Turning to research in philosophy, psychology, and cognitive science, Miller  emphasized the importance of causality in explanatory questions. Drawing upon Pearl and Mackenzie's _Ladder of Causation_, he proposed the following categorization:

* Associative (_What?_): Reason about which unobserved events could have occurred given the observed events.
* Interventionist (_How?_): Simulate a change in the situation to see if the event still happens.
* Counterfactual (_Why?_): Simulate alternative causes to see whether the event still happens.

To apply Miller's framework in the context of forecasting, one needs to define the abstract notions of 'events' and 'causes'. Consider a model \(\) that predicts features over a future window \(_{}\) by observing features over a window \(_{}\). We assert that the intrinsic interpretability methods involving inductive biases [13; 14] and attention mechanisms , fall under associative reasoning. These methods assess the (unobserved) importance of features over \(_{}\) using model parameters or attention coefficients based on a single prediction from \(\) (the 'event') for a fixed \(_{}\) and single \(_{}\). In contrast, we posit that the perturbation-based saliency methods can support counterfactual reasoning. They perturb different parts of the input over \(_{}\) simulating alternative 'causes' from \(\)'s perspective, and observe the effect on an error metric (the 'event'). However, the current application of these methods encounters issues outlined in Section 2.

To address the aforementioned challenges, we employ a unifying information-theoretic concept of bottom-up saliency grounded in preattentive human cognition [5; 6] as discussed in Section 1. Concretely, we propose the following implication that links this saliency to counterfactual reasoning:

\[t_{}$'s information about}\\ I_{}_{} t .\] (1)Note that the antecedent (on the left of the implication) is a counterfactual statement. We formally express the implication using causal graphs  in Figure 1. The generic graph expresses relationships between the random variables prior to training the forecasting model \(M\). The exogenous variable \(_{M}\) captures the randomness in the training process and modeling choices, including the distribution family for representing the forecasts. The exogenous variable \(_{H}\) captures the randomness in the human observer's choice of observed and future windows to examine the model. Our central idea is to evaluate the information in the model's predicted distribution denoted by \(I_{}\). Specifically, we propose posing the following counterfactual question: _What information would \(\) have about the future over a fixed \(_{}\) if it observed the features over \(_{}\)?_ The modified graph for evaluating this question is in Figure 0(b). Once the model \(\) has been trained and the windows \(_{}\) and \(_{}\) have been chosen, the effect of the exogenous variables on the variable \(I_{}\) disappears. This allows us to evaluate the change in the information about the future in response to _different_ realizations of \(_{}\) and \(_{}\), facilitating counterfactual analysis. Note that we assume the modified graph is already available, as our focus is on the explanation phase. While the procedure starting from training the model in the generic graph implicitly follows Pearl's _abduct-action-prediction_ process [22, p. 207], estimating the distribution over the exogenous variables from the _abduction_ step is conceptually not applicable in this setting.

Note that these graphs are not meant to describe relationships between random variables in the data for a specific hypothesis, as is typical in causal inference literature: for instance, the effect of [rotating toward the speaker] on [turn changes] in conversations. Rather, they describe the _process of a human generating contrastive explanations_ for a given pretrained forecasting model \(M\)--irrespective of whether or not it is the true model--for some sequences in the data \((_{},_{})\). Further, the notion of counterfactuals, as used within the context of contrastive explanations, is also distinct from that in causal inference. As Miller [10, Sec. 2.3] points out, "it is important to note that this is not the same counterfactual that one refers to when determining causality. For causality, the counterfactuals are hypothetical 'noncauses'... whereas in contrastive explanation, counterfactuals are hypothetical outcomes." Miller's point is that _why_ explanations entail contrastive reasoning which involves comparing 'outcomes' in response to alternate 'causes'. In our work, this 'outcome' relates to the information in \(M\)'s predicted distribution, the what-if question being "would the information in \(M\)'s prediction for the window \(_{}\) change if it had observed features over a different (contrastive) \(_{}\)?". Contrast this to associative reasoning which uses features from a single \(_{}\) to generate the attribution map. A longer discussion is in Appendix E.

Considering the information in forecasts in implication 1 links counterfactual reasoning to a more principled notion of saliency than has been used in XAI literature. Note that the implication entails that for the antecedent to be true \(t\) must be salient. However, knowing the antecedent is false is not sufficient to conclude that \(t\) is not salient, i.e. there can be other notions of saliency that make \(t\) salient. However, for less speculative evaluation, it is crucial that we use a unifying notion of saliency that is not arbitrarily defined based on task-specific error metrics or model gradients . Praetentive saliency, as we formalize in Section 4.1, is based on what is informative for the brain _before_ conscious processing, making it more objective in nature.

Our framework addresses all the concerns associated with saliency-based approaches described in Section 2: (i) the counterfactuals in our framework are real observed features rather than random input perturbations, preserving the semantics of the real-world data; (ii) our use of information-theoretic preattentive saliency is principled and objective; (iii) our framework allows for saliency computation on unseen test data where the ground-truth future is unavailable, relying solely on the underlying model; and (iv) our approach considers the distribution over possible futures for a single input, capturing the structural predictive relationships between features across multiple samples. An additional advantage of our framework is that it does not require any training to compute the saliency and can be applied to any model that outputs a distribution over futures.

Figure 1: Causal Graphs for Explaining Forecasts

Methodology: Closed-Form Saliency for Probabilistic Forecasting

### Preliminary: Information Theoretic Preatentive Saliency

Loog  developed a general closed-form expression for saliency based on computational visual perception that unifies different definitions of saliency encountered in the literature. The framework was illustrated on images and employed a surprisal-based operational definition of bottom-up attention. In this framework, an image is represented by a feature mapping function \(\) that relates each location in the image to a set of features. The saliency of a location \(x\) is determined by the information or surprise associated with its corresponding feature vector \((x)\) compared to other feature vectors extracted from the same image. The saliency measure is defined as follows:

\[S(x)>S(x^{})- p_{}((x))>- p_{}((x^{})).\] (2)

Here, \(p_{}\) represents the probability density function over all feature vectors, while \(p_{X}\) captures any prior knowledge that influences the saliency of different image locations.

Contrary to approaches that determine saliency maps through an explicit data-driven density estimation [16; 18; 23; 24; 25], once the feature mapping \(\) is fixed, a closed-form expression for saliency can be obtained. The information content \(- p_{}\) can be obtained from \( p_{X}\) through a simple change of variables  from \(x\) to \((x)\). The saliency \(S(x)\) is then given by the expression:

\[- p_{}((x))=- p_{X}(x)+(J_{}^{t}(x)J_{ }(x)),\] (3)

where \(J_{}\) denotes the Jacobian matrix of \(\), and \(\_^{t}\) indicates matrix transposition. Since a monotonic transformation does not essentially alter the map, Loog  simplifies the saliency map definition to

\[S(x)(J_{}^{t}(x)J_{}(x)),\] (4)

This formulation of saliency offers several advantages. It provides a principled and objective measure that captures the informativeness of features for human perception. Moreover, the saliency computation is purely local to an image, making it independent of previously observed data.

### Defining \(\) in Terms of the Uncertainty over the Future Window \(}}\)

Let \(_{}\;\;[o1,o2,...,oT]\) represent a window of consecutively increasing observed timesteps, and \(_{}\;\;[f1,f2,...,fT]\) denote an unobserved future time window, where \(f1>oT\). Consider a set of \(n\) interacting agents, and let \([_{i}^{t};t_{}]_{i=1}^{n}\) and \([_{i}^{t};t_{}]_{i=1}^{n}\) represent their features over \(_{}\) and \(_{}\) respectively. Here, \(_{i}^{t}\) captures multimodal features from agent \(i\) at time \(t\). The forecasting task is to predict the density \(p_{Y|X}\). Given a model that outputs \(p_{Y|X}\), our task is to compute the saliency \(S(_{})\) of an observed \(_{}\) with respect to a fixed choice of \(_{}\).

To extend Loog's  framework to forecasting settings, we need to choose an appropriate \(\). We formalize the implication in Equation 1 and map \(_{}\) to the differential entropy of the model's predicted future distribution over \(_{}\). Specifically, we define \(:_{} h(Y|X=)\), where the conditional differential entropy of \(Y\) given \(\{X=\}\) is defined as

\[h(Y|X=)- p_{Y|X}(|) p_{Y|X}(|)d .\] (5)

Our framework is summarized in Algorithm 1. Consider that a domain expert selects a specific \(_{}\) corresponding to a high-order semantic behavior they wish to analyze. This could be a speaking-turn change [27; 28] an interaction termination [29; 30], or a synchronous behavior event . Given an underlying forecasting model \(M\) and look-back period before \(_{}\), we compute \(h(Y|X=)\) for different _observed multivariate features_\(\) corresponding to different locations of a sliding \(_{}\). The computed differential entropy values are then inserted into Equation 4 to obtain the saliency of different \(_{}\) locations towards the future over the chosen \(_{}\). In Appendix B we discuss other favorable properties of differential entropy that make it a suitable choice as \(\).

Explanation Using the Running Example.Within our running example from Section 1, \(_{}\) corresponds to the two pedestrians passing each other while avoiding collision. In this example, let us assume \(M\)'s training data contains examples of pedestrians passing others to both the left and the right. Consequently, for a \(_{}\) containing the pedestrians approaching each other in a straight line, the predicted distribution \(p_{Y|X}\) over \(_{}\) encapsulates both possibilities of each pedestrian passing to the left as well as the right of the other. So the entropy \(h(Y|X=)\) is high for this \(_{}\). Only once \(M\) is fed as input with the trajectories from the \(_{}\) containing the pedestrians choosing one of the two directions to pass, the predicted \(p_{Y|X}\) is certain in terms of the pedestrians continuing along the chosen direction. (Note that in this case, we assume \(M\) has been trained by maximizing likelihood over the dataset containing only these two direction changes for avoiding collision.) Consequently, the entropy \(h(Y|X=)\) drops only once this moment of the pedestrians committing to a direction is seen by the model and would be considered salient for our algorithm.

### Computing \(h(Y|X=)\)

Typically, the density \(p_{Y|X}\) is modeled as a multivariate Gaussian distribution . When the decoding of the future is non-autoregressive, the parameters of the distributions for all \(t_{}\) are estimated at once, and the differential entropy has a closed-form expression, given by (see Cover and Thomas [35, Theorem 8.4.1])

\[h(Y|X=)=h(_{d}(,))=[(2 e)^{d }()].\] (6)

A common choice is to set \(\) to be diagonal, i.e. the predicted distribution is factorized over agents and features. In this case, we can simply sum the \(\) of the individual variances to obtain the feature mapping \(\). Note that from Equation 6, for a multivariate Gaussian distribution, the differential entropy only depends on the covariance, or the _spread_ of the distribution, aligning with the notion of differential entropy as a measure of total uncertainty. (See [35, Tab. 17.1; 36] for closed-form expressions for a large number of commonly employed probability density functions.)

In cases where probabilistic autoregressive decoders are used , we do not have access to the full joint distribution \(p_{Y_{1},,Y_{T}|X}\) for the timesteps in \(_{}\). This is because inferring the density function \(p_{Y|X}\) often involves sampling: a specific sample \(}_{t}\) is taken from the predicted density at each \(t_{}\), and passed back as input to the decoder for estimating the density at timestep \(t+1\). Therefore, the density at \(t+1\) depends on the randomness introduced in sampling \(}_{t}\). Figure 2 illustrates the concept for two timesteps. Here, a single forecast would only output the shaded red distribution for \(Y_{2}\). In such cases, computing the joint entropy \(h(Y_{1},Y_{2})\) directly is challenging in the absence of the full joint distribution \(p_{Y_{1},Y_{2}}\).

To address this, we have two options. The simpler option is to redefine our feature-mapping as \(:_{}_{t_{}}h(Y_{t}|}_{<t},)\), i.e. we approximate the total uncertainty over the predicted sequence by summing the differential entropies of the individual densities estimated at

Figure 2: Illustrating predicted densities under greedy autoregressive decoding for two timesteps. For simplicity, we depict a joint Gaussian distribution and omit the conditioning on \(\) everywhere.

each timestep. Note that following the chain rule for differential entropy (see Cover and Thomas [35, Eq. 8.62]), the joint entropy can indeed be written as the sum of individual conditionals. However,

\[h(Y|X=)=_{t t_{}}h(Y_{t}|Y_{<t},)_{t t _{}}h(Y_{t}|}_{<t},).\] (7)

And yet, training autoregressive decoders by maximizing likelihood actually assumes the inequality in Equation 7 to be approximately equal (see [39, Sec. 2; 40, Eq. 5]). The approximation relies on the observation that, for autoregressive decoding, the parameters of the predicted distribution for \(Y_{t}\) are computed as a deterministic function of the decoder hidden state. That is, \(Y_{t}\) is conditionally independent of \(Y_{<t}\) given the hidden state of the decoder \(_{t}\) at timestep \(t\). The underlying assumption is that for a well-trained decoder, \(_{t}\) encodes all relevant information from other timesteps to infer the distribution of \(Y_{t}\). So at inference, despite being a function of the single sample \(}_{t-1}\), the predicted distribution conditioned on \(_{t}\) provides a reasonable estimate of the uncertainty in \(Y_{t}\). This assumption allows us to again obtain a closed-form expression for the saliency map when each \(Y_{t}\) is modeled using a density function with a known closed-form expression for differential entropy [35, Tab. 17.1; 36]. For the common choice of modeling \(Y_{t}\) using a Gaussian mixture , approximations that approach the true differential entropy can also be obtained efficiently  to directly compute the feature mapping \(\).

The second option is to estimate \(h(Y|X=)\) using sampling or other non-parametric approaches when analytical expressions or computationally efficient approximations are not available . These sampling-based methods provide approximations that converge to the true entropy, although they may be computationally more expensive than parametric methods. Overall, the choice of modeling the future density and the approach for estimating the differential entropy depends on the specific scenario and the available resources.

## 5 Experiments

The common evaluation of saliency-based explanations relies on qualitative visual assessment, which is subjective and prone to observer biases . Meanwhile, establishing a reliable ground truth for the salient relationship between the observed window \(_{}\) and the future window \(_{}\) is challenging in real-world data due to conflicting domain evidence on predictive relationships . Furthermore, fair validation of a _model agnostic, post hoc_ method requires evaluating it independently of imperfections in the underlying forecasting model. To address these challenges we conduct two types of empirical evaluation: one using synthetic data to establish ground truth predictive saliency and _validate the framework_, and another to _demonstrate empirical utility_ in real-world scenarios where perfect forecasts and ground truth saliency are unavailable.

No existing benchmarks or post hoc explanation frameworks exist for probabilistic time-series regression that meet the necessary requirements for a meaningful empirical comparison. Nevertheless, we provide results by adapting several explainability frameworks in our experiments. Specifically, we considered DeepSHAP and GradientSHAP , and IntegratedGradients and SmoothGrad . It is important to note that we do not imply that these are fair comparisons; they are not (see Section 2). However, the comparisons are meant to characterize results from popular tools that practitioners are likely to use in the absence of our proposed framework. Implementation details for the following experiments and additional results for the real-world scenarios are in Appendices C and D, respectively.

### Empirical Validation using Synthesized Ground Truth Saliency

Dataset.We simulate a group conversation that emulates real behavior patterns. Listeners typically focus on the speaker, while the speaker looks at different listeners . Additionally, head gestures and gaze patterns predict the next speaker . In our simplified simulation, the speaker rotates towards the center when speaking, and listeners nod to trigger a turn handover. We use real-valued quaternions to represent 3D head poses, commonly used for human motion and pose representation . Following the notation in Section 4.2, \(=[q_{w},q_{x},q_{y},q_{z},ss]\) where \(ss\) denotes binary speaking status. We simulate the turn changes to occur once clockwise and once anticlockwise. The ground truth salient timestep is when a listener initiates a head nod to trigger a turn handover, ensuring a certain future turn change. Figure 3 illustrates this mechanism. The code, dataset, and animated visualization are available in the Supplement.

Empirical Validation.To validate our framework in isolation, we assume a perfect forecasting model that predicts the true distribution over the possible future quaternion trajectories. The forecasting model focuses solely on low-level features and does not incorporate any high-order semantics of turn-taking. The saliency map generated by our framework, as shown in Figure 3(a), accurately identifies the ground truth salient timesteps at frames \(138\) and \(139\) where the head nod begins. The saliency decreases once the nod is already in motion, indicating that it does not provide additional information about the future. This empirically validates our framework.

Introducing a Real Forecasting Model.We evaluate our framework using a real underlying forecasting model trained on synthetic data. We employ a _Social Process_ model  for its ability to capture relative partner behavior and adapt to specific group dynamics. As shown in Figure 3(b), our framework identifies the true salient timesteps with higher saliency values. Conversely, the attributions provided by other explainability frameworks in Figure 3(c) for the predicted mean and

Figure 4: **Computing Saliency**. The top plots show the quaternion dimensions \(qx\) and \(qy\) for the listener that nods over \(_{}^{2}\) in Figure 3. The gray dotted line indicates the true salient timestep \(138\) when the head nod begins, making the future over timesteps \(183-228\) (\(_{}\)) certain. The rest of plots show the **(a)** entropy over future values of all participants (middle), and saliency map obtained using our framework (bottom), considering perfect forecasts; **(b)** entropy and saliency for the forecasts from a Social Process model; and **(c)** mean attributions across features per timestep from different explainability frameworks (DeepSHAP, GradientSHAP, IntegratedGradients, and SmoothGrad) for the predicted mean and std. of the same forecast from the Social Process model.

Figure 3: **Illustrating the synthetic conversation dynamics dataset.** Speakers are denoted in orange and listeners in green. For a fixed \(_{}\) we depict two preceding \(_{}\) windows. By construction, when observing a stable speaking turn over \(_{}^{1}\), two valid futures are possible over \(_{}\). These correspond to a turn handover to the immediate left or right of the current speaker. Over \(_{}^{2}\), when a listener nods to indicate the desire to take the floor, the future over \(_{}\) becomes certain, corresponding to the listener successfully taking over the speaking turn. Here \(_{}^{2}\) is consequently more salient than \(_{}^{1}\) towards forecasting the turn change over \(_{}\). (Best viewed as video, see Supplement.)

standard deviation of the same forecast fail to capture the salient predictive relationships in the data. This comparison underscores the effectiveness of our framework in capturing meaningful and interpretable saliency, even in conjunction with an imperfect forecasting model.

### Empirical Evaluation in Real-World Scenarios

#### 5.2.1 Group Leaving Behavior in the Wild

The study of group-leaving behavior has garnered interest in social psychology and the development of conversational agents . Recent approaches employ data-driven models to predict future non-verbal cues, capturing general predictive patterns in the data . In this study, we demonstrate how our framework can assist domain experts in hypothesizing about the causal relationships between behavioral patterns and group leaving. We leverage the publicly available _MatchNMingle_ dataset , which features natural interactions of \(92\) individuals during a cocktail party. We use an _Attentive Social Process_ model  to forecast continuous head pose, body pose, and binary speaking status.

Through our analysis (see Figure 4(a)), we find that the salient timesteps in the model's forecasts correspond to instances when a person about to leave directs their gaze away from the shared group space (_o-space_) by rotating their head. This observation leads to the following hypothesis:

* gazing away from the o-space of a conversing group is predictive of group leaving.

Figure 5: **(a) Analysis of the group leaving instance at \(12\):\(11\) on _Day 1, Cam 12_ in the _MatchNMingle_ dataset. _Row 1_: Video frames and overlaid arrows denoting the head orientation of participants. Orange indicates the person leaving the group over the \(_{}\). _Row 2_: Head orientation of the leaver plotted as 2D horizontal rotation. _Row 3_: Saliency map from running predictions from the Attentive Social Process model through our framework. The timesteps salient towards the modelâ€™s forecasts correspond to the leaver making sweeping gazes away from the group. _Rows 4-5_: mean DeepSHAP and GradientSHAP across features per timestep for the predicted mean and std. of the same forecast. (b) Analysis of the vehicle turn making instance on _Scene 3_ in the _nuScenes_ dataset. _Row 1_: Video frames showing the bus and surrounding cars from the camera. _Row 2_: Future predictions for the bus position (circled) from the Trajectron++ model (ground truth in white, predicted mean in black and variance in red). _Row 3_: Saliency map from running predictions through our framework. The timesteps salient correspond to the model being more certain that the bus will make a turn. _Rows 4-5_: mean DeepSHAP values across features per timestep for the predicted mean and std. of the same forecast. Best viewed as video (see Supplement).**

While this hypothesis aligns with established leave-taking patterns  and the sweeping gaze behavior associated with seeking new interaction partners , it requires further validation through subsequent studies and rigorous statistical testing with the involvement of domain experts. Nonetheless, our experiment demonstrates how the framework can unveil data-driven insights into patterns that, in other cases, may have been overlooked by humans but captured by the forecasting model. By contrast, we do not observe any discernible intuitive patterns in the features associated with the trends in DeepSHAP and GradientSHAP values for the predicted mean and standard deviation.

#### 5.2.2 Vehicle Trajectory Forecasting

The accurate forecasting of pedestrian and vehicle trajectories is crucial for safe and socially-aware autonomous navigation of vehicles . In this study, we utilize our framework to investigate vehicle dynamics in real driving scenarios. Specifically, we leverage the _nuScenes_ dataset, a multimodal dataset for autonomous driving , and the _Trajectron++_ forecasting model .

Figure 4(b) illustrates our analysis of vehicle dynamics at an intersection. Notably, our framework identifies a salient timestep for the Trajectron++ model precisely when it becomes more confident that the bus will make a turn instead of continuing straight. This coincides with the model's increased certainty that the point-of-view vehicle will decelerate as a new vehicle enters the scene from the left. Although there are no relevant domain-specific theories in this case to interpret this saliency, these identified patterns align with expected driving behavior. In contrast, the DeepSHAP values fail to capture the model's change in certainty about the bus making the turn instead of continuing straight. Moreover, we also do not identify any intuitive patterns in the predictions associated with the DeepSHAP trends. Thus, our framework serves as a valuable tool for sanity-checking model forecasts in real-world driving scenarios. It helps identify instances where the model's predictions align or misalign with established norms and expectations.

## 6 Conclusion

We have proposed a computational framework that provides counterfactual explanations of model forecasts based on a principled notion of bottom-up task-agnostic saliency. We derive a closed-form expression to compute this saliency for commonly used probability density functions to represent forecasts . To validate our framework, we conduct empirical experiments using a synthetic setup, enabling quantitative validation and mitigating observer biases associated with visual assessment of saliency maps. Additionally, we demonstrate the practical utility of our framework in two real-world scenarios involving the prediction of nonverbal social behavior and vehicle trajectories. By identifying salient timesteps towards a predicted future through counterfactual reasoning, our framework can support domain experts in formulating data-driven hypotheses regarding the predictive causal patterns involving the _features_ present at those salient timesteps. These hypotheses can then be tested through subsequent controlled experiments, establishing a human-in-the-loop Explainable AI (XAI) methodology. For a more comprehensive discussion, please refer to Appendix E.

## 7 Limitations and Potential Negative Societal Impact

While our framework provides a closed-form or efficient solution for most probability density functions, limitations arise when an analytic expression for differential entropy is unavailable. As discussed in Section 4.3, alternative approaches like sampling or nonparametric methods can be employed to approximate the entropy, albeit at an increased computational cost.

Our work here is an upstream methodological contribution. However, when applied downstream to human behavior or healthcare data, ethical considerations arise naturally. Here, care must be taken that such methods are not applied for gaining insights into behavior in a way that violates the privacy of people. Our framework enables domain experts to derive data-driven insights and hypotheses about predictive causal patterns. However, hypotheses should be rigorously tested, using controlled experiments and peer review, before being considered valid statements about human behavior. Collaboration among researchers, practitioners, and policymakers across disciplines is crucial to mitigate such societal risks and ensure ethical deployment of AI technologies.