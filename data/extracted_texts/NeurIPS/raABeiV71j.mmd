# Loki: Low-rank Keys for Efficient Sparse Attention

Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, Abhinav Bhatele

Department of Computer Science, University of Maryland

College Park, MD 20742

prajwal@umd.edu,bhatele@cs.umd.edu

###### Abstract

Inference on large language models (LLMs) can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in LLM inference contributes significantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximate self-attention by focusing on the dimensionality of _key_ vectors computed in the attention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose _Loki_, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to speed up the attention computation due to reduced data movement (load/store) and compute costs while maintaining the efficacy of the models better than other popular approximation methods.

## 1 Introduction

As large language models (LLMs) grow in size, deploying them for efficient inference presents substantial challenges, largely due to computation and memory access bottlenecks in the self-attention block , especially when handling long sequences. These challenges stem from the autoregressive nature of attention, which generates the output one token at a time. At each step, the entire preceding state, stored in the key-value (KV) cache, must be fetched from memory, which can sometimes exceed the size of the model parameters itself . This frequent KV-cache access from GPU DRAM to registers becomes costly, as it scales quadratically with the output sequence length. In addition, matrix multiplications in the attention layers also have a quadratic scaling cost with sequence length, compounding the overall computational burden.

Several strategies  have been proposed to address this challenge by reducing the computational complexity and/or memory demands associated with the self-attention mechanism. One promising category of approaches focuses on approximating attention, employing techniques such as quantization or using a subset of the tokens in the KV-cache  (sparse attention).

In contrast to other sparse attention approaches that either permanently prune tokens from the key-value cache  or impose a fixed sparsity pattern , our proposed method dynamically selects key tokens at each generation step based on approximate attention scores and avoids deletions. This approach is inspired by a critical observation: across a range of LLMs and datasets, key tensors consistently occupy a significantly lower-dimensional space than the full attention head dimension. For instance, in Figure 1 (left), we show that across various LLMs , 90% of the variance explained by PCA is captured at an effective key vector rank of around 80, despite the key tensor dimension being much larger (128).

ased on this observation, we introduce Loki, a sparse attention method that leverages the low-dimensional structure of key vectors to reduce data movement and computation costs without significantly impacting model quality. First, we apply PCA to keys generated from a calibration dataset, storing all principal components but using only the top \(d\) (25-50%) to compute approximate attention scores during inference. This dimensionality reduction, informed by our previous observation that key vectors have low effective rank, allows us to efficiently identify the top-\(k\) (12.5-25%) most relevant tokens using the approximate scores. For these selected keys, we then revert to the full dimensionality to compute the final attention scores, ensuring both efficiency and accuracy. Figure 1 (right) illustrates our approach.

Our theoretical complexity analysis demonstrates that Loki can provide significant speedups in the attention step. However, actually realizing these gains requires an efficient implementation of our method to minimize data movement in the additional operations introduced on top of the original self attention algorithm. Thus, we implement optimized sparse matrix multiplication kernels for Loki in Triton, leading to a speedup of up to 45% over the standard HuggingFace Transformer's  attention implementation (_vanilla_ attention) for Llama2-13B. For this setting, the average degradation in model accuracy (measured across 6 different benchmarks and 8 different models) is only 6.8%.

Our contributions can be summarized as follows:

* Detailed analysis showing the intrinsic low-dimensionality of keys in self-attention, its variation across layers for different models, and consistency across different datasets.
* Loki: a sparse attention method that exploits the aforementioned low dimensionality of keys to make the attention computation faster without sacrificing model quality.
* Optimized kernels for efficient implementation of Loki in PyTorch.
* Evaluation of Loki1 on multiple LLMs and downstream tasks, showing that it can achieve significant speedups with minimal degradation in model quality. 
## 2 Background and Related Work

The attention mechanism  is at the core of the transformer architecture. Consider a single attention query head with head dimension \(D\), processing an input token sequence of length \(S\). During auto-regressive generation, the output of the attention head is calculated as:

\[=^{}}{} \] (1)

where \(^{1 D}\) is the query, and \(^{S D}\) and \(^{S D}\) are the key and value caches respectively. Additionally, newer transformer models add Rotary Position Embeddings (RoPE)  to the keys and query, before computing the attention scores. Since every query attends to all past keys, the mechanism has a quadratic complexity \((S^{2})\) in number of input \(+\) generated tokens.

Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).

### Related Work

Numerous studies have explored the low-rank structures in transformers for various purposes. Linformer  demonstrated that the attention score matrix is low-rank and proposed alternative low-rank attention formulations during training for linear computational complexity. LoRA  showed that parameter updates to a transformer model during fine-tuning reside in a low-dimensional subspace. To the best of our knowledge, our work is the first to study the intrinsic low dimensionality of the attention keys themselves and demonstrate the generalizability of this low-dimensional structure across different models (for natural language data).

Sparse-transformers  was one of the first works to introduce a sparse-attention method employing strided sparsity patterns in the attention mechanism. Reformer  used locally-sensitive hashing to compute attention scores in a sparse manner. Performer  used positive orthogonal random features to approximate the attention mechanism. Unlike these methods, which require training or fine-tuning, our approach operates entirely post-training without any fine-tuning.

Another category of sparse attention methods employ token eviction policies to permanently delete tokens from the KV-cache based on some heuristic. StreamingLLM  uses initial tokens and a rolling KV-cache for processing infinite-length sequences. Zhang et al.  retain only "Heavy Hitters" tokens in the KV-cache based on accumulated attention scores. Scissorhands  prioritizes important tokens based on the "Persistence of Importance Hypothesis". Ge et al.  propose an adaptive eviction policy for each transformer layer. These methods are effective in reducing the memory and compute footprint of the attention but suffer from permanent loss of information leading to a non-trivial degradation in model quality. Our method does not involve any permanent loss of information with the trade-off of not reducing the memory footprint. Quantization-based approximate approaches [14; 23] are complementary to our work and can be applied in tandem.

SparQ Attention  is a recent work that inspires our approach. They use high-magnitude query dimensions and corresponding key dimensions for approximate attention scoring, followed by computing the full attention scores for the top-\(k\) keys. However, their method requires costly non-contiguous column indexing of the key vectors. Further, they store two copies of the past keys for efficiency, increasing memory use by 50%. In contrast, Loki avoids the extra memory and leverages the natural ordering of principal components, allowing for a more efficient slicing operation.

A concurrent work, InfiniGen , accelerates attention by pre-fetching top-\(k\) keys from CPU to GPU memory, using SVD-based low-rank approximation of the attention scores. While their low-rank approximation is similar to Loki, our work provides deeper analysis of the intrinsic low-rank structure of attention keys and focuses on speeding up attention computation without CPU offloading. Importantly, their results affirm the benefits of the low-dimensional nature of attention keys applied in other contexts.

## 3 Dimensionality Analysis of Attention Keys

As noted in Section 1, Loki, our proposed method for sparse self-attention, is based on the observation that key tensors consistently reside in a lower-dimensional space than the full attention head dimension suggests. Here, we present empirical evidence supporting this claim by performing PCA on the keys generated in several language models and datasets.

### Models and Datasets Used

To investigate the dimensionality of attention keys, we run 11 transformer-based models: Llama-2 7B/13B/70B , Llama-3 8B/70B , TinyLlama-1.1B , Pythia-6.9B , Mistral-7B , Mistral-8x7B/8x22B , and Phi3-Mini-4K  on three popular English language datasets: WikiText-2  (Validation Split), C4  (Custom Split), and BookCorpus  (Custom Split). Custom splits are used for datasets where the validation split is not available. We run perplexity evaluation on these datasets and save the generated attention keys, before and after the application of rotary embeddings , referred to as _pre-rotary_ and _post-rotary_ keys, respectively throughout the paper. We then perform PCA on all the keys generated for each layer and head individually.

The metric we use in our analysis is the rank at which \(v\)% of the variance is explained by the principal components. We calculate this metric for each layer and head of the models as follows:

\[Rank_{l,h}@v=\{d^{+}:_{j=1}^{d}_{l,h}^{j} v /100\}\] (2)

where, \(_{l,h}^{j}\) is the \(j^{th}\) normalized eigenvalue of the covariance matrix of the keys for layer, \(l\) and head, \(h\). We average this metric ranks across all heads of layer, \(l\) and refer to it as \(Rank_{l}@v\).

### Findings and Discussion

Figure 1 (left) shows the average \(Rank_{l}@90\) across all layers for models with full key dimensionality of 128. We can see that the average rank is significantly lower than the full dimensionality of the keys for all models. Diving deeper, we present a layer-wise analysis for a few models: Llama2-7B, Llama3-70B, Mixtral-8x7B, and Phi3-Mini-4K in Figure 2. The results for the other models are similar and can be found in Appendix A.1.

We observe that the dimensionality of the keys (both pre-rotary and post-rotary) is significantly lower than the full dimensionality of the keys across all calibration datasets. Furthermore, the \(Rank_{l}@90\) for a particular layer is consistent across datasets, for all combinations of models and datasets. This indicates that the lower-dimensional structure of the keys is consistent when calculated using different calibration datasets. Another trend we observe is that the initial layers of most models have a very low rank, as compared to the later layers, and this trend is particularly prominent for the pre-rotary keys. Lastly, we also observe that for most models, the average of \(Rank_{l}@90\) across all layers is lower for pre-rotary keys as compared to post-rotary keys, indicating that the rotary embeddings increase the dimensionality of the keys. Further analysis on the variation of the rank across different heads within a layer and across different layers within a model can be found in Appendix A.1.

These results indicate the existence of the following properties: (1) The keys produced by the attention layers of transformer models lie in a significantly lower-dimensional space. (2) The lower-dimensional structure of the keys is consistent across different calibration datasets. (3) Rotary embeddings increase the dimensionality of the keys for most models. We now use the first two properties to propose Loki, an efficient sparse-attention method.

## 4 Loki: Low-Dimensional Key Representations

We now describe our proposed algorithm for sparse attention - Loki. Loki leverages low dimensional projections of the keys in the KV-cache to efficiently and accurately select the top-\(k\) (12.5-25%) most relevant tokens for self attention. Before discussing our approach, let us first look at some theoretical properties of attention in the PCA-transformed space of the key tensors.

Figure 2: Rank at which 90% of the variance is explained for pre-rotary and post-rotary keys produced by each layer averaged across all heads (\(Rank_{l}@90\)) for different models. We observe that all models exhibit significantly low rank (full dimensionality is 128 or 96 represented by the black dashed line) consistently across all datasets.

### Properties of Attention in the PCA-transformed Space

We begin by proving two lemmas that provide the rationale for our approach to compute attention in the PCA-transformed space.

**Lemma 4.1**.: _Let \(D\) be the dimension of an attention head and \(^{D D}\) be the PCA projection matrix of key tensors calibrated offline on a dataset. Assuming we are generating the \(S^{th}\) token in the sequence, let \(_{S}^{1 D}\) be the query vector for the \(S^{th}\) token, \(_{ S}^{S D}\) be the key vectors, including the past \((S-1)\) keys and the current key. Then, the attention scores computed using the PCA-transformed query and keys are equivalent to the attention scores computed using the original query and keys._

Proof.: Let \(}_{S}=_{S}\) and \(}_{ S}=_{ S}\) be the PCA transformed query and key vectors. Focusing on the dot product term in the attention computation (Equation 1), we have:

\[_{S}_{ S}^{T} =_{S}(}_{ S}^{T})^{T} { [inverting the PCA transform]}\] \[=_{S}((^{T})^{T}}_{ S}^ {T})=(_{S})}_{ S}^{T}=}_{S}}_{ S}^{T}\]

It is important to note here that Lemma 4.1 holds for any orthogonal \(\). 

**Lemma 4.2**.: _Let \(}_{ S:d}^{S d}\) (\(d<D\)) be the reduced dimension key vectors obtained by projecting the key vectors onto the first \(d\) principal components of \(\). Then, the attention scores computed using \(}_{ S:d}\) are a good approximation of the the actual attention scores._

Proof.: Let \(_{:d}^{d D}\) be an orthogonal transformation that transforms the keys into the reduced dimension space as \(_{ S:,d}=_{ S}_{ d}\). Our objective is to minimize the following expression:

\[_{_{:d}}||_{S}_{ S}^{T}-_{S}( _{ S:,d}_{ d}^{T})^{T}||_{2}^{2}\] (3)

Using Cauchy-Schwarz inequality, we have:

\[||_{S}_{ S}^{T}-_{S}(_{ S:, d}_{ d}^{T})^{T}||_{2}^{2}||_{S}||_{2}^{2}|| _{ S}^{T}-(_{ S:,d}_{ d}^{T})^{T }||_{2}^{2}\] (4)

We change our objective to minimize the upper bound in the RHS instead of the original objective. We know that PCA minimizes the reconstruction error (2nd term in the RHS) among all the orthogonal transformations. Thus, it follows that the optimal value of \(_{ d}^{*}=_{:d}\), and \(_{ S:,d}^{*}=}_{ S:,d}\) 

Since we minimize an upper bound when proving Lemma 4.2, it is possible that some other transformation might give a better approximation to the dot product. Thus, in our experiments, we use PCA transforms computed on both the pre-rotary and post-rotary keys as candidate transformations.

Based on these lemmas and the inherent low-dimensional nature of key tensors in attention, we now introduce the workings of the Loki algorithm.

### PCA-based Top-K Algorithm

Loki implements a PCA-based Top-K Attention approach. Previous works have shown that attention scores for a query are highly concentrated on a small subset of keys [36; 30]. This observation has motivated several methods that compute attention using only the top-\(k\) most relevant keys. However, these previous works either compute the exact attention scores and then select the top-\(k\) keys  or compute non-exact scores but have significantly higher memory requirements . Loki alleviates these issues by computing approximate attention scores (for ranking the keys) in the reduced lower-dimensional space, without any significant increase in memory requirements. Algorithm 1 shows our Loki method. Line 5 of the algorithm computes the approximate attention scores using \(d\) principal dimensions of the query and key vectors. Lines 6-7 select the top-\(k\) keys based on the approximate attention scores. Line 8 computes the exact attention scores using the selected top-\(k\) keys, directly in the transformed space (Lemma 4.1).

**Compute and Memory Analysis:** For vanilla attention, the complexity of computing \(_{S}_{ S}^{T}\) is \((DS)\) and the complexity of multiplying the values with the attention scores is \((DS)\). For Loki, the complexity of calculating the approximate attention scores (Line 5) is \((dS)\). The complexity of selecting the top-\(k\) keys (Lines 6-7) is approximately \((Slog(S)+k)\) (sorting followed by selection). The complexity of calculating the exact attention scores and multiplying with the values (Line 8-9)is \((2Dk)\). Additionally, the complexity of projections into the PCA space (Line 3) is \((2D^{2})\). Assuming the complexity of selecting the top-\(k\) keys is small compared to the other operations, the overall complexity of the algorithm is \((dS+2Dk+2D^{2})\). Then, we have:

\[speedup=}=/2 +k_{f}}(D<<S)\] (5)

where, \(d_{f}=d/D\) and \(k_{f}=k/S\). The memory requirement of the KV-cache is the same as the original attention, with a small overhead of storing the PCA transformation matrix.

### Implementation in Triton

Performing Loki efficiently involves complex indexing operations within the KV-cache (lines 5 and 7 of Algorithm 1). Standard PyTorch operations create temporary, dense copies of the KV-cache data in memory, leading to slowdowns due to expensive memory access. To alleviate this issue, we develop optimized kernels in Triton  for the three matrix multiplication operations in Loki. Our kernels can directly access relevant subsets of the KV-cache (both feature and sequence dimensions) and perform computations within GPU registers. This eliminates the need for creating dense copies, significantly improving performance. Our approach builds on SparQ , which introduced similar kernels for top-\(k\) attention calculations. However, we identified and addressed inefficiencies in the SparQ kernels, which resulted in speedups of nearly \(2-3\) in certain scenarios. (see Appendix C).

## 5 Experimental Setup

We evaluate Loki on the basis of perplexity using the WikiText-2  dataset (test split), and on the basis of downstream task performance for short contexts using the LM-harness benchmark  and long contexts using LongBench . For the short-context evaluation, we choose the same tasks and associated metrics as the HuggingFace OpenLLM leaderboard . For the LongBench tasks, we evaluate on all the English language tasks.

We compare our method against three methods - full attention without any approximations, the exact TopK approach which computes the exact attention scores and then uses the top-\(k\) tokens to compute the final output, and H\({}_{2}\)O , a popular token-eviction method. For these comparisons, we show the results with a budget size of \(k_{f}\) = 0.25 and 0.125. For our method, we additionally use \(d_{f}\) = 0.25 and 0.125. This configuration of our represents a 2.6x theoretical speedup. Table 1 provides an overview of the methods compared and the associated budget terms. H\({}_{2}\)O's budget was split equally between the heavy hitter and recent tokens, as per the author's recommendations. For H\({}_{2}\)O, we were unable to run the GSM8K task as the the author's ML benchmarking code was too memory intensive to run for that task. For the aforementioned experiments, we generate PCA transforms using the WikiText-103 dataset. For the LongBench tasks, we compare our method with the full attention baseline as we were unable to run H\({}_{2}\)O due to memory constraints.

For the generalizability study, we compare the results of our method with PCA transforms from different calibration datasets: WikiText-103 , C4 , and BookCorpus . Additionally, we also benchmark our triton based implementation of Loki by running an attention microbenchmark on a Llama2-13B-like setup (same hidden size and number of heads) for various prompt and generation lengths, and demonstrate speedups over vanilla attention.

All experiments are run on NVIDIA A100 GPUs with 40 and 80 GB of memory on the Perlmutter  supercomputer. For larger models, we use AxoNN [27; 28] to shard the model across multiple GPUs.

## 6 Results

We now present the comparisons of Loki with full attention and other sparse attention methods, including a comparison of the computation times.

### Comparison with Full Attention

Let us begin our discussion with Figure 3, showing the perplexity (left) and short-context downstream task evaluation (right) results for Loki on different models. We focus on the Llama2-7B model, comparing pre-rotary (light green/purple) and post-rotary (dark green/purple) PCA transforms for different \(k_{f}\) and \(d_{f}\) values. For Llama2-7B, we see that the performance of both candidate transforms is similar. This trend is consistent across all the models except for Llama3-8B/70B and Mistral-7B, where the post-rotary PCA transform performs significantly worse than the pre-rotary one. For Llama3-8B, perplexity jumps from about 5 for the full attention to over 10, a significant decline not seen with the pre-rotary transform. Mistral-7B shows a similar pattern. This is a surprising observation since attention scores are calculated from post-rotary keys in the original attention mechanism. A possible explanation is that post-rotary PCA captures token distributions tied to specific positions in the calibration dataset, while pre-rotary PCA may generalize better by using less positional information. Nevertheless, at least one of the PCA transformations performs well for every model. For subsequent results, we only show the better-performing transformation for each model.

Figure 4 shows the performance of Loki on the LongBench tasks for the Llama2-7B-Chat model. We see that for all tasks, either one of the two candidate transforms performs similarly to full attention. For Summarization, Few Shot Learning, Synthetic, and Code Completion task categories, the best performing Loki configuration is at par or better than the full attention model. For the Single-Doc QA and Multi-Doc QA task categories, Loki performs slightly worse than the full attention model,

   Method & Budget & Dim. & Description & Speedup & Memory Savings \\  Exact Top-K & \(k_{f}\) & Full & \(k_{f}\) fraction of keys selected using exact attention scores & No & No \\ H\({}_{2}\)O & \(k_{f}\) & Full & \(k_{f}\) fraction of keys \& values selected using H\({}_{2}\)O policy & \(^{2}}\) & \(^{2}}\) \\ Loki & \(k_{f}\) & \(d_{f}\) & \(k_{f}\) fraction of keys \&values selected using attention & \((d_{f}/2)+k_{f}\) & No \\  & & & scores computed with \(d_{f}\) fraction of full dimensionality & & \\   

Table 1: Explanation of key-budget and dimensionality (Dim.) for different approaches, along with the expected speedup and memory savings.

Figure 3: Evaluation of Loki on perplexity (left plot) and short-context tasks (right plot) for different models. Task accuracy is an average across all short-context tasks mentioned in 5.

with the biggest drop in performance observed for HotpotQA of around 3%. Comparing different \((k_{f},d_{f})\) settings, we see that using \(k_{f}\) = 0.25 and \(d_{f}\) = 0.25 (green), is better than using \(k_{f}\) = 0.125 and \(d_{f}\) = 0.5 (purple) for all models and tasks (short-context and long-context). These two settings balance speed and performance well, with the first being superior for accuracy.

### Comparison with Other Sparse Attention Methods

Next, we compare the performance of Loki with other methods, using \(k_{f}\) = 0.25 for all methods and \(d_{f}\) = 0.25 for ours. Table 2 shows the perplexity results for Llama2-7B/13B, Llama3-8B, and Mistral-7B. Loki's perplexity drop is within 0.1 of full attention across all models, a threshold considered acceptable for attention mechanism approximations . In contrast, H\({}_{2}\)O's perplexity drop nears 0.2 for all models. Figure 5 confirms this trend on short-context evaluation. Loki performs similar to full attention for all models, except Llama3-8B, where the performance is notably worse, but still better than H\({}_{2}\)O. Importantly, on the challenging MMLU task, Loki degrades less than H\({}_{2}\)O.

It is important to note here that Loki is designed to be compatible with other sparse attention methods. For instance, token-eviction methods like H\({}_{2}\)O delete tokens to save KV-cache memory, whereas Loki reduces memory bandwidth by selecting the top-\(k\) tokens without deletion, making them orthogonal. A combined approach could involve using H\({}_{2}\)O to delete tokens, then applying Loki to select top-\(k\) tokens from the remaining cache. Similarly, Loki is theoretically orthogonal to quantization methods.

  Method & \(k_{f}\) & \(d_{f}\) & Speedup & Llama2-7B & Llama2-13B & Llama3-8B & Mistral-7B \\  Full Attention & - & - & No & 5.1101 & 4.5680 & 5.5696 & 4.9140 \\ Exact-TopK & 0.25 & - & No & 5.1809 & 4.5926 & 5.5716 & 4.9171 \\  H\({}_{2}\)O & 0.25 & - & Yes & 5.2810 & 4.7009 & 5.7056 & 5.0805 \\ Loki & 0.25 & 0.25 & Yes & **5.2017** & **4.6102** & **5.6648** & **4.9233** \\  

Table 2: Perplexity evaluation of Loki and other approaches for different models (lower is better).

Figure 4: Evaluation of Loki on LongBench tasks for the Llama2-7B-Chat model.

Figure 5: Downstream task performance for Loki and other approaches for different models (higher is better). GSM8K is excluded, as we were unable to run H\({}_{2}\)O for this task.

Comparing Loki with Exact-TopK, we find similar performance for Llama2-7B/13B and Mistral-7B. Exact-TopK represents the upper performance bound for Loki if it could perfectly select the top-\(k\) tokens. To understand why Loki works well, we examined the top-\(k\) agreement between Loki's reduced dimensional attention scores and exact attention scores. Figure 6 shows the Jaccard similarity between the top-\(k\) tokens selected by both methods across all layers and heads for Llama2-7B. For the settings: (\(k_{f}=0.25\), \(d_{f}=0.25\)) and (\(k_{f}=0.125\), \(d_{f}=0.5\)), evaluated in Figure 3, the Jaccard similarity is around 0.9, validating that the Loki is able to select the top-\(k\) tokens with high accuracy.

### Generalizability

We now turn our attention to the generalizability of the PCA transformations used in our method. Figure 6 (middle) shows the performance of Loki using PCA transformations derived from different calibration datasets (\(k_{f}=0.25,d_{f}=0.25\)). We see that the performance of Loki is consistent across different calibration datasets, indicating that the PCA transformations used in our method are generalizable. This is an important observation as it shows that the PCA keys can be generated using a variety of calibration datasets and still achieve good performance.

### Computational Efficiency

We now turn our attention to the computational efficiency of Loki. Analyzing Llama2-13B with Hugging Face Transformers exposed an interesting bottleneck (Figure 6, rightmost). Regardless of the attention type (vanilla or Loki), more than 80% of the time is consumed within the Hugging Face framework for appending key-value pairs of the latest token to the KV-cache. This shared bottleneck minimizes the overall performance improvement of our optimizations. We hypothesize that using a more advanced inference system like vLLM  could significantly reduce this append time, but leave that exploration for future work. To isolate the impact of our optimizations, the plots in Figure 7 focus solely on the attention computation time, excluding the KV-cache append time.

In the left plot of Figure 7, we see that Loki speeds up the total attention compute time (excluding KV-cache appends) compared to vanilla attention across various prompt and generation lengths. For a prompt length of 3072 and generation length of 512, Loki achieves nearly a 45% speedup, despite the fact that it incurs an extra matrix multiplication operation. The breakdowns also show that the top-\(k\) operation is nearly as expensive as the smaller matrix multiplications, which is a significant bottleneck. Replacing PyTorch's top-\(k\) with a custom kernel could improve this. For the shorter prompt length of 2048 we observe a speedup of around 40% (generation length = 512), slightly lower than the speedup at 3072. This trend is expected as larger prompts result in a bigger KV-cache, amplifying the impact of our optimizations.

Figure 7 (Right) shows the accuracy vs. attention time trade-off across various \(k_{f},d_{f}\) settings of Loki, with accuracy measured on LongBench and attention times from our microbenchmark. The previously evaluated settings, \(k_{f}=0.25,d_{f}=0.25\) and \(k_{f}=0.125,d_{f}=0.5\), provide a good balance between performance and accuracy, with \(k_{f}=0.25,d_{f}=0.25\) favoring accuracy slightly and \(k_{f}=0.125,d_{f}=0.5\) favoring performance.

Figure 6: Top-\(k\) agreement between Loki and Exact-TopK methods for Llama2-7B (left plot). Performance of Loki using transformations derived from different calibration datasets (middle plots). Benchmarking vanilla attention and Loki for Llama2-13B using huggingface transformers with cache append times (right plot, prompt length = 3072, generation length = 512).

## 7 Conclusion

In conclusion, we introduced Loki, an algorithm for efficient sparse attention that does not compromise the model quality while reducing the computational complexity of self attention. We made a crucial observation that key vectors in attention lie in a low-dimensional space, across different models and datasets. Leveraging this insight, Loki uses attention scores computed in a lower-dimensional space to rank and select the top-\(k\) most relevant tokens from the KV-cache. It then uses the full dimensionality only for the selected tokens to compute the final attention. Our theoretical analysis shows that Loki can provide significant speedups in the attention step. To implement this efficiently, we develop optimized kernels for the various sparse matrix multiplications in our approach. Our empirical evaluation shows that Loki performs better than popular approximation methods on a variety of models and tasks, with respect to preserving model quality. Finally, we show that Loki can provide speedups of up to 45% over the vanilla attention empirically, making it a promising approach to address the computational challenges in transformer inference.

**Limitations and Future Work:** Loki does not focus on reducing memory usage of the KV-cache currently. As mentioned previously in 6.2, it can potentially be combined with other sparse attention method for improved memory-performance-accuracy trade-offs. Another direction involves storing the KV-cache in CPU memory and transferring only the top-\(k\) keys and values to the GPU .

While Loki outperforms vanilla attention in our benchmarks, practical deployment would require integration with efficient attention kernels like FlashAttention . As seen in our compute benchmarking, the top-\(k\) selection operation could introduce a bottleneck towards achieving this. Investigating this bottleneck and integrating Loki with optimized attention kernels is left for future work.

Our finding of the keys' low intrinsic dimensionality suggests promising research directions. The variation of this dimensionality across heads and layers could further be explored. We briefly experimented with a variable \(d_{f}\) policy per layer (see Appendix B.2), but did not observe significant significant improvements. A more sophisticated policy could be explored in future work.