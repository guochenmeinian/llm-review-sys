# American Stories: A Large-Scale Structured Text

Dataset of Historical U.S. Newspapers

Melissa Dell\({}^{1,2^{*}}\), Jacob Carlson\({}^{1}\), Tom Bryan\({}^{1}\), Emily Silcock\({}^{1}\), Abhishek Arora\({}^{1}\), Zejiang Shen\({}^{3}\),

Luca D'Amico-Wong\({}^{1}\), Quan Le\({}^{4}\), Pablo Querubin\({}^{2,5}\), Leander Heldring\({}^{6}\)

\({}^{1}\)Harvard University; Cambridge, MA, USA.

\({}^{2}\)National Bureau of Economic Research; Cambridge, MA, USA.

\({}^{3}\)Massachusetts Institute of Technology; Cambridge, MA, USA.

\({}^{4}\)Princeton University; Princeton, NJ, USA.

\({}^{5}\)New York University; New York, NY, USA.

\({}^{6}\)Kellogg School of Management, Northwestern University, Evanston, IL, USA.

\({}^{*}\)Corresponding author: melissadell@fas.harvard.edu.

###### Abstract

Existing full text datasets of U.S. public domain newspapers do not recognize the often complex layouts of newspaper scans, and as a result the digitized content scrambles texts from articles, headlines, captions, advertisements, and other layout regions. OCR quality can also be low. This study develops a novel, deep learning pipeline for extracting full article texts from newspaper images and applies it to the nearly 20 million scans in Library of Congress's public domain Chronicling America collection. The pipeline includes layout detection, legibility classification, custom OCR, and association of article texts spanning multiple bounding boxes. To achieve high scalability, it is built with efficient architectures designed for mobile phones. The resulting American Stories dataset provides high quality data that could be used for pre-training a large language model to achieve better understanding of historical English and historical world knowledge. The dataset could also be added to the external database of a retrieval-augmented language model to make historical information - ranging from interpretations of political events to minutiae about the lives of people's ancestors - more widely accessible. Furthermore, structured article texts facilitate using transformer-based methods for popular social science applications like topic classification, detection of reproduced content, and news story clustering. Finally, American Stories provides a massive silver quality dataset for innovating multimodal layout analysis models and other multimodal applications.

## 1 Introduction

Historical local newspapers provide a massive repository of texts about American communities and their inhabitants that can elucidate topics ranging from semantic change to political polarization to the construction of national and cultural identities to the minutiae of the daily lives of people's ancestors. Given the enormous breadth and depth of content, historical newspapers have been widely studied, yet existing open source U.S. newspaper datasets have significant limitations that complicate the extent to which modern deep learning methods can leverage and liberate their content.

Library of Congress's Chronicling America project  is the primary public domain historical U.S. newspaper dataset. It consists of around 20 million historical newspaper scans and their corresponding digitized texts. Its content is concentrated before 1925, as this content has entered the publicdomain. Chronicling America does not recognize oftentimes complex newspaper layouts, and so digitized texts are provided at the page level, often scrambling headlines, articles, advertisements, captions, and other content regions together. Because a non-trivial share of the underlying scans are illegible, incoherent texts are prevalent, with illegibility varying across space and time. This complicates applying natural language processing (NLP) and statistical methods, and the data are not of sufficient quality to use for training a large language model to achieve a better understanding of historical English and historical world knowledge.

To address these limitations, we develop a pipeline for cheaply extracting high quality digitized article texts and layout regions from newspaper scans. First, layout detection predicts the coordinates and classes of content regions - _e.g._ articles, headlines, bylines, advertisements, pictures, etc. - using object detection methods . Then, an image classifier removes illegible text bounding boxes. We next digitize the text regions using a novel optical character recognition (OCR) architecture that yields highly scalable, accurate results within our constrained budget. The focus on cost effectiveness makes the pipeline accessible to others with limited budgets who would like to digitize massive historical document collections. Finally, we associate headline, byline, and article bounding boxes. We do not process foreign language newspapers, as off-the-shelf OCR tends to perform poorly on the diverse languages and scripts.

The resulting American Stories (**S**tructured **text on** reporting **in** every state) dataset contains 1.14B content regions. The dataset has extensive geographic coverage across all states and has content dating as far back as the 17th century, although the bulk of content comes from the early 20th century. Figure 1 shows the distribution of scans across years and states. The vast majority of American Stories is older than the 72 year rule that the U.S. government uses to release personal information (_e.g._, from the census) into the public domain.

We show that the pipeline produces accurate predictions. The resulting texts could be used for historical language model training, or added to an external database of a retrieval augmented language model to facilitate the study of topics ranging from international events to family history. The layouts and corresponding texts could provide a massive silver quality dataset for applications like multimodal layout analysis and classification. The American Stories dataset also yields significantly better performance on social science analyses than the Chronicling America OCR and allows analyses that would be impossible without structured article texts. For example, we cluster article embeddings to detect which stories (e.g., Pancho Villa Expedition, 1916) received the most coverage each year.

The rest of this study is organized as follows. Section 2 discussed related literature and Section 3 describes the American Stories dataset. Section 4 outlines the digitization pipeline, and Section 5 evaluates the quality of the outputs. Section 6 discusses applications and Section 7 considers limitations and recommended usage.

## 2 Related Literature

Public domain newspaper datasets exist for many countries, but typically pipelines are proprietary, as the norm is to outsource digitization to a private company. Commercial newspaper databases

Figure 1: Scans in the Chronicling America database across time and space.

likewise do not disclose their pipelines, resulting in a dearth of open-source methods. The most closely related work to American Stories is the open-source Newspaper Navigator dataset . The main, and crucial, difference between American Stories and  is that  does not detect bounding boxes of articles.  focuses on 7 classes of visual content: headlines, photographs, illustrations, maps, comics, editorial cartoons, and advertisements. Distinguishing articles enables legibility classification, application of custom OCR, and association of articles across bounding boxes. These tasks are at the core of our contribution, and enable the usefulness of our contribution for downstream applications. In addition, the OCR in  is limited to Chronicling America's OCR, which we show leads to a quality deterioration.

While we focus exclusively on texts where the entire newspaper is indisputably in the public domain (typically because it was published more than 95 years ago), it is worth noting that our pipeline could help address some of the copyright issues that have limited the public availability of historical newspapers. Outside of the nation's most widely circulated newspapers, it was rare for local papers to publish with a copyright notice or renew their copyright, required formalities until the latter half of the 20th century. Hence, the majority of local papers well into the 20th century are off-copyright. Yet these papers might sometimes print copyrighted content by third parties - _e.g._, frequently comics, rarely ads, and occasional runs of syndicated fiction . Individual news articles did not have their copyrights renewed, as copyrighting yesterday's news lacked commercial value. Detecting individual content regions - _e.g._, so that ads and comics could be cropped out and fictional texts removed with a classifier - is a prerequisite for removing content potentially under copyright. Some copyright experts  have suggested this as a way forward for making historical newspapers more accessible.

## 3 Dataset

Table 1 describes American Stories, totaling 1.14 billion content region bounding boxes. Headlines, images, bylines, and captions are OCR'ed if legible. The dataset contains 3,313 tokens per page on average, making the full dataset 65.6 billion tokens.

American Stories provides the classes and coordinates for all content regions. Using the provided metadata, it is straightforward for users to link the coordinates with the original scans, which can be downloaded through the Library of Congress's website. We do not OCR ads because they oftentimes have unusual fonts and complex layouts, including scene text and complex tables with pricing or schedule information, complicating OCR. The table class includes tabular article data, _e.g._ sporting rosters. We do not transcribe these because accurately detecting and harmonizing the diversity of table layouts is challenging with current technology. Newspaper headers are not OCR'ed because most of their information is contained in the metadata. Masteheads - which contain subscription information - and page numbers often used very small fonts and hence are disproportionately likely to be illegible; page numbers can be inferred from the metadata.

Each text region is classified as legible, illegible, or borderline, with examples of each category shown in Figure 2. Borderline forms the grey zone between clearly legible and clearly illegible texts and is OCR'ed. Users can remove it if they would like to limit to the highest quality texts.

Substantial shares of illegible content can degrade language model training and bias downstream applications. For example, it is common for social scientists to construct data based on the presence of keyword terms , assigning a positive outcome if the term is present and a zero outcome otherwise. Illegibility can bias downstream analyses if it is correlated with an underlying unobserved factor. Figure 3 shows that illegibility is correlated with space and time, and hence likely to be cor

    & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\  & Total & &  &  \\  & Boxes & Articles & Headlines & Captions & Bylines & Images & Ads & Tables & Masteheads \\   

Table 1: American Stories dataset statistics.

related with unobserved factors. Using our data, researchers can remove papers with high illegibility rates if desired and more realistically assess selection into the database.

American Stories has a Creative Commons CC-BY license, to encourage widespread use. The data are available on the Hugging Face Hub1. The raw files are in a json format, and the Hugging Face repo comes with a setup script that easily allows people to download both raw and parsed data to facilitate language modeling and computational social science applications. The supplementary materials and the Readme on the dataset repository provide a detailed usage guide.

## 4 Methods

**Overview:** The American Stories pipeline consists of four steps: layout/line detection, legibility classification, OCR, and article association. The pipeline is available on Github2.

The pipeline's modularity offers several advantages. Theoretically, localization (of layouts, lines, words, and characters) and recognition (of characters, akin to classification) may rely on different features of the image, suggesting modularity . Practically, there are vast differences in the number of labels required for training each component of the pipeline. Modularity also leads to architectural simplicity. Swapping in different encoders is straightforward, which makes the pipeline more customizable and future-proof. Off-the-shelf models performing a single pipeline task - like Segment Anything  or an OCR engine - would be straightforward to swap in.

Figure 3: Illegible articles in the Chronicling America database across time and space.

Figure 2: Examples of legibility classification, as predicted by our trained model.

The American Stories pipeline uses architectures designed for mobile phones - Yolo v8  and MobileNet v3  - because the accuracy hit relative to much larger models was very modest and deployment costs were over an order of magnitude lower. If budget is not a concern, it would be straightforward to swap in a vision transformer, (_e.g._, ) and a two stage object detection framework (_e.g._, ), variations that  examine quantitatively on Chronicling America.

The pipeline was run on Azure F-series CPU nodes. Training used an Nvidia A6000 GPU card. Details are described in the Supplementary Materials.

**Layout and Line Detection:** Layout region coordinates and classes are detected using Yolo v8 , with Figure 4 showing examples. We train the layout detection model on 2,202 labeled newspaper images, consisting of 48,874 layout objects, with an average of 22 layout objects per page. All annotations for the pipeline were created by undergraduate research assistants, with scans selected randomly and using active learning . To develop a general purpose model, we annotated scans of public domain and off-copyright newspapers from throughout the 19th and 20th centuries. Scans from Chronicling America comprise 13% of the training sample.

For content regions that are OCR'ed, we detect individual lines within each region using Yolo v8, as lines are the input to OCR. The model is trained on 4,000 synthetic and 373 annotated line crops. Yolo v8, like other object detection frameworks, takes square images as inputs. When the aspect ratios of content regions differ significantly from squares, downstream OCR performance tends to be adversely affected. Hence, for content regions with an aspect ratio greater than 2:1 (twice as tall as wide), we split the layout region (with overlap), run line detection over each separate box, and then run non-maximum suppression jointly over the resulting predictions. For the same reason, before sending lines to OCR, we split lines with an aspect ratio below 1:30 (thirty times wider than tall).

**Legibility Classification:** We classify headline, article, byline, and caption regions as legible, illegible, or borderline (see Figure 2) using an image classifier with a Mobilenet v3 backbone that is trained on 1,094 double-labeled content region crops.

**OCR:** We aimed to deploy a highly accurate digitization pipeline within a constrained cloud compute budget of $60,000 USD. As documented in detail in the supplementary materials, existing OCR solutions did not meet these requirements. Commercial solutions were far too costly, and TrOCR Base  - an open-source transformer sequence-to-sequence OCR that produced accurate results - was nearly fifty times more costly to deploy on cloud CPUs than our budget. More scaleable open-source OCR engines were noisy even when fine-tuned on newspaper annotations.

We met our accuracy and cost objectives with the EfficientOCR framework . EfficientOCR uses deep learning-based object detection methods to localize individual characters and/or words in an image. Character/word recognition is modeled as a character/word-level image retrieval problem, using a vision encoder contrastively trained on character/word crops, largely created through augmenting digital fonts. At inference time, character/word embeddings are decoded to text in parallel by retrieving their nearest neighbor from an offline index of exemplar character or word embeddings, created by rendering character/word images with a digital font. Distances are computed using cosine similarity with a Facebook Artificial Intelligence Similarly Search (FAISS) backend .

Figure 4: Variety of newspaper layouts with our layout detection pipeline outputs overlayed.

Switching between character and word recognition is important. There are many terms that would not appear in even a very large dictionary, as narrow newspaper columns imply that hyphenated words at the end of lines are common.3 The texts also have a long-tailed distribution of proper nouns and antiquated acronyms and words. Hence, at inference time, whenever a word crop is below a tuned cosine similarity threshold of 0.82 from its nearest neighbor in the offline word embedding index, we instead apply character-level EfficientOCR to individual character crops within the word. The supplementary materials provide a detailed description of training and deployment.

**Content Association**: We associate headlines together (if spanning multiple boxes), associate them with bylines (if present), and with the first article bounding box, using rule-based methods that exploit the position of article and byline bounding boxes relative to headlines (see the Supplementary Materials). Rule-based methods perform less well for articles spanning multiple columns or pages, and language understanding is required. However, we find on a labeled sample that only around 3.8% of articles span multiple article bounding boxes, and only 0.2% of articles span multiple pages, meaning there is little scope for gains from neural methods. (Articles spanning multiple columns and pages become more common after the Chronicling America period, as the price of paper falls and font size increases.) Since these cases are rare and our compute budget is limited, we do not associate multiple article bounding boxes together for this release, but may do so in the future, using the RoBERTa cross-encoder method developed in .

## 5 Pipeline Evaluation

**Measurement:** We use four carefully constructed datasets to evaluate the pipeline:

* **Full page scans:** Two student annotators labeled layout regions and hand-entered the texts for 10 full page scans, resolving all discrepancies by hand. The set consists of 597 content regions and 196,655 characters. It allows us to evaluate the end-to-end pipeline, which requires transcribed full-page scans since layout analysis is applied at the page level. We further use this sample for evaluating content association. It contains 214 headline-article bounding box pairs.
* **Transcribed day per decade sample:** To evaluate line detection and OCR on highly diverse content, we hand-transcribe a randomly selected sample of 50 lines for each decade between 1850-1920. During this period, printing and archival technology changes significantly. Examples of these textlines, with their accompanied EffOCR transcriptions, are shown in the supplemental materials as Table 1.
* **Transcriptions of randomly selected lines from Carlson et al. **: This sample is used to report comparisons to other object detection frameworks, backbones, and OCR engines. These comparisons are taken from , which develops EfficientOCR. This sample includes 64 textlines drawn randomly from random scans in the Chronicling America collection.
* **Legibility sample**: We evaluate legibility on a randomly selected (from legibility training data), double labeled sample of 100 bounding boxes, 50 headlines and 50 articles. Transcriptions are not included, as we cannot create character labels for illegible content.

We measure pipeline accuracy with the character error rate (CER), defined as the Levenshtein distance  between the end-to-end digitized content and the ground truth, divided by the length of the ground truth. OCR is similarly evaluated by the character error rate on ground truth layout and line annotations (and hence does not include transcriptions errors induced by errors in the layout predictions). We also examine non-word rate, as it does not require costly-to-create labeled data so can be evaluated on a much larger sample, though it is more difficult to interpret. To measure the quality of layout and line detection, we use mean average precision (mAP@50:95), as well as decomposing what share of the overall character error rate is due to layout and line detection errors. For full article association, we focus on confusability between legible and illegible scans. Finally, we evaluate content association using F1.

**Overall Pipeline Evaluation:** The end-to-end CER is 0.051 (Table 2), showing the pipeline's high overall accuracy. Some of the errors - _e.g._ confusing commas and periods are particularly prevalent - are straightforward to fix in post-processing. When we apply a lightweight spellchecker , the CER falls to 0.044. Spellchecking produces a slight increase in CER in headlines, likely due to a higher concentration of proper nouns.

Figure 5 plots the non-word rate at the scan level on a day-per-decade sample, where non-word rate measures the share of terms not in a lengthy dictionary with 82,765 terms . Even with a perfect OCR, the non-word rate could be appreciable, due to hyphenated words at the end of rows, acronyms, proper nouns, and antiquated terms. The American Stories distribution is concentrated well to the left of Chronicling America's distribution, underscoring the quality of our texts. Differences in the non-word rate between Chronicling America and American Stories are likely to reflect both differences in OCR quality and differences in filtering content based on legibility and content region type, as Chronicling America digitizes all the texts it can localize on the page.

**Layout and Line Detection Evaluation:** mAP, reported in Table 2, is high, particularly for the classes of central interest such as headlines (88.64) and articles (91.31). Confusing ads for articles - as they often look similar in this period - is relatively common. The ads that look like articles tend to OCR well and have natural language texts, so these errors are also tolerable. The overwhelming majority of content regions in the ten labeled scans are articles, headlines, and advertisements, as photographs - a rarity in this period - do not appear. The mAP for line detection is 86.20.

We also decompose the overall CER into errors due to OCR and errors due to line and layout detection. When the layout model fails to detect text regions, misclassifies them as another category such as ads that isn't OCR'ed, or when line detection misses or crops lines, this increases the CER. The CER due to layout and line detection errors is 0.012.

    & (1) & (2) & (3) & (4) \\  & Overall & Headlines & Articles & Ads \\  Mean Average Precision & 63.48 & 88.64 & 91.31 & 78.4 \\ Overall CER (Spellchecked) &.044 &.092 &.038 & - \\ Overall CER & **.051** &.089 &.049 & - \\ CER from OCR &.043 &.071 &.039 & - \\ CER from layout detection &.012 &.018 &.010 & - \\ Article Association F1 & 97.0 & - & - & - \\   

Table 2: Pipeline evaluation on ten labeled scans. CER is the character error rate, decomposed into errors from OCR and from layout detection. Article association F1 evaluates the association of headlines with each other and the first article bounding box. Spellchecking is applied after EffOCR, using .

Figure 5: Non-Word Rate Distributions of American Stories and Chronicling America.

**Legibility Classification Evaluation:** We want to avoid classifying legible texts as illegible and vice versa, with the borderline class existing to encompass the grey zone between these two categories. In a random sample of labeled texts - containing 81 legible texts - none of the legible texts are misclassified as illegible. Of 16 illegible texts, only one is misclassified as legible. To further evaluate this, we ran all content regions from a one-day-per-decade sample through OCR. The non-word rate is more than three times higher for illegibly classified content as compared to legibly classified content, and more than twice as high for illegibly classified content as for borderline content.

**OCR Evaluation:** Table 2 reports the CER from running OCR on ground truth layouts and lines. It is 0.043. The supplementary materials provide a much more detailed analysis of OCR quality. Evaluation on a day per decade sample shows that CER ranges from 8.9% (1850s) to 1.8% (1910s), with the bulk of content concentrated in the later period where scans are less challenging. The supplementary materials also document that EffOCR  best meets our accuracy and cost requirements, by comparing to a variety of open-source and proprietary OCR engines.

**Content Association Evaluation:** We achieve an F1 of 97 for associating headlines with articles on our labeled evaluation dataset. We associate all bylines correctly.

## 6 Applications

**Language Modeling**: American Stories provides a massive amount of text that can be used to continue pre-training large transformer language models, helping a language model to develop a better understanding of 19th and early 20th century English and greater world knowledge about the past. Moreover, a retrieval augmented language model (e.g. ), combined with longer context windows, could provide a valuable tool for retrieving and summarizing vast information, whether it be perspectives on paradigm-shifting historical events or the minutiae of the daily lives of people's ancestors. American Stories also provides extensive content for studying semantic change.

**Multimodal Classification**: The layouts and texts in American Stories comprise a rich multi-modal dataset, providing vast silver quality data that could be used for developing novel methods for multimodal layout analysis or multimodal classification - _e.g._, with image-caption pairs.

**Topic Classification:** Topic classification of texts in historical newspapers is a common social science application, with the literature overwhelmingly using keyword searches to measure topics . To evaluate how American Stories can facilitate topic classification, relative to the existing Chronicling America page-level OCR, we use neural and sparse methods to classify whether a randomly selected set of content is about politics, a frequently covered topic. We use a RoBERTa large  classifier, applied to articles in American Stories and chunks of Chronicling America (the page OCR is significantly longer than the context window). The training data were randomly sampled at the article level and contain 2418 articles. The development set contains 15 randomly selected full scans (498 articles) and the test set contains 62 randomly selected scans (1473 articles). We also consider keywords, using two different approaches for selecting these: keyword mining on the training set and asking ChatGPT, with careful prompting. All methods are described in the supplementary materials.

American Stories supports article level classification, whereas Chronicling America only supports page level classification. A page is a positive example in the ground truth if any of the articles are on topic, and article or chunk predictions can be aggregated to page level predictions using the same definition. Retrieval at the scan level tends to retrieve a lot of extraneous content, as typically only part of the page contains articles about politics.

On structured article texts, neural methods outperform keyword methods by a wide margin (F1 of 83.6 versus 58.1). All methods perform better at the page level, as there is a much lower chance of false negatives, with the best performance by a wide margin coming from applying neural methods to the American Stories corpus.

**Content Dissemination Networks:** Reproduced content is of considerable interest to social scientists , but detecting it can be challenging due to OCR noise and abridgement. We evaluate this task on a full-day sample of labeled front pages from March 1, 1916, a random day that consists of 113 reproduced articles (the median article is reproduced twice) as well as 1,994 singleton articles.

To detect reproduced content at the article level with American Stories, we deploy the pre-trained neural model from . They contrastively tuned a Sentence BERT model  on a large, hand-annotated sample of paired reproduced articles from a later period. At inference time, article representations are clustered using single linkage clustering to detect reproduced content. To make the American Stories result comparable with Chronicling America, we amalgamate the predictions to the page level. A page-pair is counted as positive if the pages have any article in common, making the task easier.

For detecting reproduced content with the Chronicling America full page scans, where we lack the article texts required for the neural method, we deploy the sparse methods from Viral Texts . Viral Texts was designed specifically for detecting reproduced texts in Chronicling America's noisy page-level OCR by looking for overlapping \(n\)-gram spans. We also apply Viral texts to American Stories at the article and page level. Table 3 reports the adjusted rand index (ARI) for these specifications. The Viral Texts method gives slightly better results on American Stories, but the real advantage of American Stories is that the article structure allows the use of a neural method, which dominates the sparse method by nearly 12 percentage points at the page level.

**News Story Clustering:** The structured nature of American Stories allows for further article-level clustering of content. As a demonstration of this, we show how articles can be grouped into news stories, with different articles that are part of the same unfolding news story clustering together. This prediction is not possible with the unstructured page content in Chronicling America.

To create these clusters, we fine tune a contrastive biencoder on modern news texts, scraped from allsides.com, a website which amalgamates different representations of the same news story from different news outlets. Full details of the data, model and training are given in the supplementary materials. We ran this model over all de-duplicated front page articles from 1885-1920. We read 20 random articles in the largest cluster for each year and named the story which the articles in the cluster refer to. Table 4 shows the largest news story cluster by year.

**Year** & **Biggest story** & **Year** & **Biggest story** \\
1885 & Death of General Grant & 1903 & Panama Canal Treaty \\
1886 & Southwest Railroad Strike & 1904 & Russo-Japanese War \\
1887 & Vatican supports Knights of Labor & 1905 & Russo-Japanese Peace Process \\
1888 & Rail strikes & 1906 & Hepburn Railroad Rate Bill \\
1889 & Samoan Crisis & 1907 & Mining accidents \\
1890 & 1893 World’s Fair planning & 1908 & Taft presidential victory \\
1891 & New Orleans Lynchings & 1909 & Race to the North Pole \\
1892 & Homestead Steel Strike & 1910 & Rail strikes \\
1893 & World’s Fair, Chicago & 1911 & Canadian Reciprocity Bill \\
1894 & Wilson-Gorman Tariff Act & 1912 & Republican National Convention (Taft v Roosevelt) \\
1895 & British occupation of Corinto, Nicaragua & 1913 & Underwood-Simmons Tariff Act \\
1896 & Bimetallism Movement & 1914 & World War I \\
1897 & Coal Miners’ Strike & 1915 & World War I \\
1898 & Cuban War of Independence & 1916 & Pancho Villa Expedition \\
1899 & Philippine-American War & 1917 & World War I \\
1900 & Anglo-Boer War & 1918 & World War I \\
1901 & U.S. Steel Recognition Strike & 1919 & Treaty of Versailles \\
1902 & Anthracite Coal Strike & 1920 & Rail strikes \\ 

Table 4: Largest news story in each year, 1885-1920.

    &  &  \\   & Neural &  &  &  \\ F1 & Mining & GPT &  &  \\ (1) & (2) & (3) & (4) & (5) \\  _Article Level_ & & & & \\ American Stories & **83.6** & 56.4 & 58.1 & **75.1** & 32.3 \\ _Page Level_ & & & & \\ American Stories & **96.0** & 82.8 & 79.6 & **86.2** & 74.6 \\ Chronicling America & 83.3 & 83.7 & 79.6 & - & 71.4 \\   

Table 3: Comparing American Stories to Library of Congress’s Chronicling America.

This application demonstrates one of the many ways that the structured article texts in American Stories can be used to unlock new ways of studying historical and social questions.

In addition to these tasks, image-caption pairs from layout analysis could be used for training and assessing image captioning models, visual question-answering models, cross-modal retrieval models, image retrieval models, and multi-modal understanding (e.g., representation learning) models. American Stories could also be leveraged to substantially lower the costs of creating new benchmark datasets for other common ML tasks, e.g., image and text classification, image retrieval, and named entity recognition.

## 7 Limitations and Recommended Usage

American Stories contains historical language, that reflects the semantics and cultural biases of the time. This is a distinguishing feature, that is core to many potential applications. We do not filter texts that use antiquated terms or that may be considered offensive, as this would invalidate the use of the dataset for studying semantic change and historical contexts. At the same time, this makes American Stories less suited for tasks that require texts that fully conform to current cultural standards or semantic norms. For these reasons, we recommend against the use of American Stories for training generative models. While the OCR is high quality, American Stories is also not well-suited to tasks requiring fully clean texts. Rather, American Stories can be used for a wide variety of applications, ranging from elucidating social science questions to training a historically-oriented language model to exploring world and family history. It also provides a modular pipeline that can be customized for other document collections and scaled cheaply, offering a blueprint for liberating large-scale historical text corpora.