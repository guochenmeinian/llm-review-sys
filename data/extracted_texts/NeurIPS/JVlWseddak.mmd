# EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding

Karttikeya Mangalam Raijmbek Akshkulakov

Jitendra Malik

UC Berkeley

{mangalam, raiymbek, malik}@eecs.berkeley.edu

egoschema.github.io

###### Abstract

We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over \(5000\) human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. For each question, EgoSchema requires the correct answer to be selected between five given options based on a three-minute-long video clip. While some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered. To remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks & datasets. Based on this metric, we find EgoSchema to have intrinsic temporal lengths over \(5.7\) longer than the second closest dataset and \(10\) to \(100\) longer than any other video understanding dataset. Further, our evaluation of several current state-of-the-art video and language models shows them to be severely lacking in long-term video understanding capabilities. Even models with several billions of parameters achieve QA accuracy less than 33% (random is 20%) on the EgoSchema multi-choice question answering task, while humans achieve about 76% accuracy. We posit that EgoSchema, with its long intrinsic temporal structures and diverse complexity, would serve as a valuable evaluation probe for developing effective long-term video understanding systems in the future. Data and Zero-shot model evaluation code are open-sourced under the Ego4D license at egoschema.github.io.

## 1 Introduction

We introduce EgoSchema, a diagnostic benchmark for assessing very long-form video-language understanding capabilities of modern multimodal systems. Understanding long natural videos requires a host of interconnected abilities such as action and scene understanding, perceiving and tracking object states, long-term visual memory, abstract reasoning, hierarchical information aggregation, and more. Shown in Fig. 1 is an exemplar of the curated EgoSchema dataset. Consider the visual cognitive faculties involved in answering the question: 'What is the overarching behavior of C and the man in the video?'. First, is the spatial recognition capabilities for disambiguating thereferred character 'C' (camera wearer) and 'the man' as well as the present objects such as 'cards', 'notebook', deck as so on. Next is short-term temporal recognition capabilities of understanding the atomic actions and movement of the characters such as 'playing', 'taking notes','shuffling' etc. Built upon these are the capabilities for visually understanding the mental states such 'distracted', 'attention' and social dynamics such as 'teaching','showing'. Next are medium-term actions such as 'organizing the deck' or 'keeping track'. Finally, long-term reasoning capabilities need to be employed for abstracting the 'overarching behavior' of the video from all the low-level signals to be able to rule out all the other wrong options and conclude option 3 to be correct. Note that even for humans, it is impossible to answer the illustrated questions with only the shown 9 uniformly sampled frames from the three-minute video (Fig. 1).

While there have been some prior attempts to formulate long-form video tasks , they broadly tend to fall into two failure modes. The first failure mode stems from the difficulty of capturing the explosive diversity of human behavior in narrow pre-defined label spaces that leading unduly narrow and oddly specific tasks, such as like ratio or relationship prediction . Hence, we propose to probe video systems capturing the rich complexity of long-form video with something just as rich and complex - natural language. However, natural language outputs are notoriously difficult to evaluate with popular metrics such as BLEU  and ROUGE  having well-known shortcomings . Hence, we propose to evaluate language understanding as a multiple-choice question-answering task, thereby using the well-defined benchmark metric of overall question-answering accuracy.

The second failure mode for a long-term video task is that the proposed task happens to actually be a short-term one - only disguised as a long-term task. To measure the intrinsic "long-term" nature

Figure 1: **The EgoSchema dataset** contains over 5000 very long-form video language understanding questions spanning over 250 hours of real, diverse, and high-quality egocentric video data. Each question requires choosing the correct answer out of five choices based on a _three minute_ long video clip. The questions are manually curated to require very long _temporal certificates_ (§3.2). EgoSchema median certificate length is about \(100\) seconds, which is \(5\) longer than the closest second dataset and \(10\) to \(100\) longer (Fig. 3) than any other video understanding dataset. State-of-the-Art video-language models consisting of billion of parameters achieve very low accuracy (< 33%) in Zero-shot evaluation (random is 20%) while humans achieve about 76%. ‘C’ refers to the camera wearer. Visualized clips are available at egoschema.github.io/explorer.

of a video understanding task, we propose the notion of temporal _certificate length_. Intuitively, certificate length (SS3.2) is the length of the video a human verifier needs to observe to be convinced of the veracity of the marked annotation. The idea of temporal certificates is not limited only to question-answering or vision-language tasks but is applicable to several video understanding tasks, including pure vision tasks such as action classification, detection, or even temporal action localization.

Based on the length of the temporal _certificate_, we propose the following temporal understanding taxonomy for video tasks: Datasets with certificate length in the order of \(1\) second are termed short video tasks. Next, we name datasets with certificate length in the order of \(10\) seconds as, long-form video tasks. Finally, datasets with certificate length in the order of \(100\) seconds are termed as, very long-form video tasks. Fig. 3 presents estimates of the certificate lengths for a variety of datasets plotted against the temporal length of the video clip. We observe that the temporal certificate length is quite weakly correlated with the length of the video clip. This is due to the intentional design choice in defining the certificate set, which decouples the task of searching or retrieving the relevant sub-clip from a bigger clip from the task of visually understanding the retrieved sub-clip. And in this manner, using temporal certificate length as a metric for measuring the intrinsic temporal hardness of a dataset, avoids the failure mode of formulating an implicitly short-term task disguised as a long-term one. Section 3.2 details precise operationalizations for estimating the temporal certificate sets.

In summary, our contributions are three-fold. _First_, we propose the notion of temporal certificates, a broadly applicable notion that measures the intrinsic temporal hardness of clips in a video understanding dataset. We estimate temporal certificate lengths for a broad variety of existing datasets and show that EgoSchema has a median temporal certificate of about \(100\) seconds, which is \(5\) longer than the dataset with the second longest certificate length , and \(25\) to \(100\) longer than all other existing video understanding datasets (with or without language). _Second_, building upon the notion of temporal certificates, we introduce EgoSchema, a diagnostic benchmark for assessing the very long-form video understanding capability of multimodal video-language systems. _Third_, we benchmark both state-of-the-art video-language systems and humans in Zero-shot settings on EgoSchema to find that even the most advanced current video-language understanding systems consisting of billion of parameters achieve very low accuracy in long-from multiple-choice question-answering (< 33%) while humans achieve about \(76\%\) accuracy in the unconstrained setting.

Figure 3: **Certificate Length across video datasets** for a broad spectrum of tasks such as action classification, detection, relationship classification, concept classification, video classification, and multiple choice question-answering. §4.1 details the precise operationalizations.

Figure 2: We introduce the notion of a temporal certificate set (top, §3.2), a tool to measure the intrinsic temporal length of a benchmark and show the EgoSchema certificate length distribution (bottom, §4.1) for randomly chosen \(100\) clips.

## 2 Related Works

**Video Question-Answering Datasets.** Visual Question-Answering  is a popular video-language task with several large internet-scale datasets for video-language pre-training such as Ego4D , HowTo100M  and HowToVQA69M . However, as the scope and size of pre-training datasets and models soar, it becomes critical to construct evaluations for assessing the model capabilities on various axes. Hence, many smaller datasets have been proposed for evaluating different aspects of video-language understanding such as compositional reasoning [21; 22], causal and common scene comprehension , instruction understanding [34; 57], video description ability , dynamic environments understanding , complex web video understanding , situated reasoning , spatiotemporal reasoning , social intelligence , dynamic neuro-symbolic reasoning , external knowledge-based reasoning  and many more [37; 61; 42; 10; 9; 13; 45; 56; 30; 31; 8; 64; 11; 32; 59; 66; 52; 24]. How2VQA69M  and iVQA  have leveraged HowTo100M  ASR text for generating questions. However, unlike Ego4D narrations that are used in EgoSchema, ASR text does not necessarily describe the visual elements in the scene. Hence, questions can suffer from biases where a key required information is visually absent. . Additionally, generated question-answers also have quite short certificate lengths (iVQA in Fig. 2) due to the local nature of the ASR text.

**Long-form Video Understanding Datasets** have been very sparsely explored in prior works.  posits a long-form video understanding benchmark but the proposed tasks are unduly narrow and specific, such as the 'like' ratio and view count prediction. Also,  average certificate length is about \(5.7\) smaller than EgoSchema.

 proposes a dataset for benchmarking efficient video inference consisting of frame-wise object mask annotations from Mask-RCNN  but without any long-term annotations.  introduces a dataset of about 111 hours of video sourced from Kinetics-400  for generic event boundary detection. While the task itself requires comprehensive understanding, the video clip length is only 10 seconds long, with temporal _certificates_ (SS3.2) being much shorter.  proposes a question-answering dataset based on long movie clips but due to the open-ended nature of questions, successful approaches tend to neglect the visual data and are biased purely with approaches using additional text such as story lines.  proposes MAD, a language grounding dataset with an average clip of \(110\) minutes. However, the length of the retrieved clip is quite short (average \(4.1\) seconds) thereby resulting in a temporal _certificate_ (SS3.2) only a few seconds long. Further, MAD  and several other movie-based datasets [27; 48; 54] do not release any video data because of copyright issues. In contrast, EgoSchema has an average certificate length of about \(100\) seconds. Further, EgoSchema

Figure 4: EgoSchema data pipeline. Stage I filters the suitable Ego4D RGB videos and narrations for question-answer generation (§3.1.1). Stage II uses narrations in a chained LLM prompting (§3.1.2) procedure to generate multiple \(\) triplets per three-minute video clip (§3.1.2). Stage III performs pre-filtering with rule-based and LLM-based logic (§3.1.3). Finally, Stage IV involves two rounds of human curation on filtered \(\) for selecting very long-form video-language understanding data (§3.1.4). The stage width ratios are indicative of the filter selection ratios.

will be publicly released under the Ego4D license, which allows direct public use of the video and text data for both research and commercial purposes.

## 3 Collecting EgoSchema

Collecting video and language datasets, even without a focus on very long-form video is quite challenging. Manually collecting, observing, and annotating videos with free-form language, in contrast to using images and pre-defined label categories, is both labor-intensive and time-consuming and thereby quite expensive. In addition to burgenoning cost, ensuring visual data diversity and minimizing visual and linguistic bias while ensuring high quality of marked annotations also contribute to the overall difficulty. All these factors get severely more challenging for long-form videos.

In this work, we propose a staged data collection pipeline (Fig. 4) utilizing existing large-scale but short-term video datasets, rule-based filtering procedures, and exciting new capabilities afforded by LLMs to significantly lighten the burden on human annotators. We use the proposed pipeline for curating EgoSchema, a high-quality and diverse very long-form video question-answering dataset. Associated datasheets  and data cards  for EgoSchema are provided in the _supplementary_.

### EgoSchema Pipeline

#### 3.1.1 Stage I: Raw Data Filtering

Ego4D  has over 3670 hours of RGB video spread consisting of over 3.85 million narration instances covering over 1,772 unique verbs (activities) and 4,336 unique nouns (objects) . The narrators are instructed to continuously pause and describe everything that the camera wearer ('C') does. This creates dense and precise narrations that accurately describe the visuals.

Naturally, the collected video has non-uniform length and narration density. Since we would like to standardize the clip length for evaluation and have sufficiently rich narrations to allow interesting question-answer pairs to form in later stages, we filter the data based on the length and narration density. We choose to filter for non-overlapping three-minute clips each with at least 30 human annotated narrations (each narration is a timestamped sentence) to build EgoSchema. Detailed statistic of the number of viable clips for different possible length and narration density choices is discussed in _supplementary_.

#### 3.1.2 Stage II: Question Answer Generation

The filtered narrations are processed with a capable LLM to generate \(N\) Question-Answer triplets (\(\)), each consisting of the question \(\), the correct answer \(\), and \(M\) wrong answers \(\), per clip. To achieve this, we experimented with several LLM inference call chaining procedures with trade-offs between quality and cost of generation that are briefly described next.

**One-shot** is the simplest prompting procedure to prompt for all \(N\) instances of \(\) in one inference call. This is the most cost-efficient option but we found the generations to be of significantly low quality. The generated \(\) often are very similar to each other and the generated \(\) have a very high false positive rate for the correct answers as well as a false negative rate for the wrong answers.

Figure 5: An abridged example of the generation and filtering prompts used in the EgoSchema data generation pipeline (§3). Full versions are provided in the _supplementary_.

**N-shot** is the next natural prompting procedure where we generate one \(\) per LLM inference call. This significantly improves the false positive and false negative rates but since the generated \(\) are independent and generated with the same prompt, they still tend to be very similar (comparable to one-shot), even at higher sampling temperatures. Further, the cost of generation also scales with \(N\).

**QAW-shot** generates each of the \(N\) questions \(\) in one inference call, followed by another inference call for generating \(N\) correct answer \(|\) and finally, \(N M\) wrong answers, \(|,\). Since each of the \(N\)\(\) is generated jointly, they can be forced to be distinct with appropriate prompting. Similarly, the generated \(\) and \(\) can also be made distinct. However, this requires _3 chained_ LLM inference calls, and generation failures in earlier calls cascade steeply.

**Q(AW)-shot** generates each of the \(N\) questions \(\) in one inference call, followed by a final inference call for generating all the \(N\) correct and \(N M\) incorrect answers in one go \(,|\). It enjoys the same uniqueness properties as QAW-shot while having just two chained calls, making it both \(~{}30\%\) cheaper and less prone to generation failure cascading. Further, between Q(AW)-shot and QAW-shot, we observe Q(AW)-shot to have a higher generated \(\) quality, perhaps since LLM can jointly model \(\) while generating \(\). We choose this to be our main method of choice for generating \(\).

**Prompt** for imputing narrations into the LLM has a tremendous effect on the quality of generated \(\). We experiment with several seed prompts for each of which we inspect the quality of the \(N\) generated \(\) for \(10\) clips. Based on this we iteratively improve the seed prompts manually in a zeroth order optimization fashion. In total, we experiment with a total of about \(85\) prompts in this fashion to arrive at our final EgoSchema prompts \(-_{}\) for generating \(N\) questions and \(_{}\) for generating all remaining options \(()|\). While we fix the \(_{}\) prompt, we use multiple \(_{}\) prompts so as to avoid any unintended bias in the options. Fig. 5 shows an abridged example of \(_{}\) and \(_{}\), full versions available in _supplementary_ material.

**Choice of LLM** is extremely crucial for obtaining interesting long-form \(\) and generating hard negatives for \(\). With weaker LLMs, the \(\) diversity across video clips remains narrow, and \(\) tends to be either obviously wrong or, too similar to \(\) and thus a false negative. While we experimented with both GPT-3  and ChatGPT  but only found good quality generated \(\) at a high enough rate with GPT-4 , Bard , and Claude . For details please see _supplementary_.

We generate \(N=3\) questions per three-minute clip as well as \(M=4\) wrong answers to every question in addition to the correct answer. We observe that larger \(N\) or \(M\) tends to generate similar questions and wrong answers putting unnecessary pressure on Stages III and IV for filtering.

#### 3.1.3 Stage III: Generated Question Answer Filtering

While Stage II produces several high-quality \(\), even the best LLM generations are prone to output format aberrations, hallucinations, and sometimes plain false outputs. Further, despite specific pinpointed prompts (Fig. 5), LLMs can fail to comply. Since, we want to ensure EgoSchema to be extremely high-quality and accurate, we set up several filtering rounds to ensure the correctness and high difficulty of questions.

**Rule-based filtering.** Keywords from the prompts such as 'long-term', 'narrations', 'timestamp' etc. can sometimes bleed into the generated \(\) which are then discarded. The output generations can also fail to parse according to a specified format and are also then discarded and the concerned \(\) is regenerated.

**LLM-based filtering.** While rule-based filtering weeds out logic errors, we would like to further enrich \(\) before employing human labor. For example, we aim to ensure EgoSchema requires grounded visual reasoning to solve, and hence questions should not be answerable _ungrounded_, without carefully observing the video. Hence, we develop a "blind" baseline.

**Blind filtering baseline** employs LLM to guess the correct answer based on the question, without having access to the video narrations conditioned on the shown filtering prompt (Fig. 5). All such ungrounded questions that can be answered blindly are filtered out. This also ensures that generated \(\) are indeed relevant and plausible answers to \(\), since otherwise, the LLM would be able to guess \(\) based only on the setting of \(\). Note that this is overly restrictive since it is possible that a question is guessed correctly through chance and is not necessarily ungrounded. However, we choose to optimize precision over recall since the amount of filtered \(\) is still large enough.

**No-\(\) baseline.** We also experimented with a No-\(\) baseline, where the LLM is prompted to guess the correct answer using the narrations but without the question \(\). This ensures that the wrong answers are relevant and plausible to the video clip. However, we found this baseline to have near random accuracy (\( 20\%\)), highlighting the efficacy of Stage II. Hence, we decided to not use this filter in the final pipeline. Additional details including the full prompt are in _supplementary_.

#### 3.1.4 Stage IV: Manual \(\) Curation

While LLM filtering ensures that the generated \(\) relates to the video content, it's also necessary to ensure the veracity and a long temporal certificate length for every generated \(\). This is achieved through a two-step manual curation process.

In the first round of curation, annotators are tasked with three primary responsibilities: **(A)** First, they verify that \(\) is well-formed and \(\) is indeed the correct answer to \(\). **(B)** Next, they confirm that all the \(M\) distractors, \(\), are indeed wrong answers to \(\). **(C)** Finally, they ensure that the temporal certificate length for answering \(\) is at least 30 seconds.

A \(\) is discarded if any of these three conditions are not met. This reduces the number of admissible questions by a factor of about \(4\) to \(5\) within the first round itself. Next is a second round of re-curation, to reinforce the conditions and guarantee data of the highest quality. We find that more than \(97\%\) of the questions that pass the first round also pass the second round, speaking to the efficacy of the curation process. A crucial aspect of ensuring that the question assesses very long-form video-language understanding capabilities is the notion of temporal certificate length (condition (C) above), which we describe next. The detailed procedures for onboarding and training the human annotators, as well as the instructions for the curation process are provided in the _supplementary_.

### Temporal Certificates

We define the temporal _certificate_ of a given video in a video understanding task to be the minimum set of _subclips_ of the video that are both _necessary_ and _sufficient_ to convince a human verifier that the marked annotation for that data (such as timestamps in temporal activity localization, class label in activity recognition or, the correct option in multiple-choice question-answering) is indeed correct, without having to watch the rest of the clip outside of the certificate set (Fig. 2). Naturally, we define certificate length to be the sum of the temporal lengths of the sub-clips present in the certificate set.

**Meta-rules.** Datasets often have implicit rules that apply uniformly across the entire dataset. We call these conventions meta-rules and allow the human verifier to be well aware of them. For example, in temporal action localization datasets , an implicit assumption is that the action to be localized in a contiguous sub-clip and hence can be uniquely determined by the start and end timestamps. Since this rule is valid for all data, we consider it to be a meta-rule.

A comprehensive understanding of _meta_-rules of a dataset is necessary for accurate estimation of the certificate set, and hence the certificate length. Otherwise, a spuriously long certificate might be necessary to ensure the veracity of the marked annotations. For example, consider the task of action classification on Kinetics-400. A valid meta-rule to be made available to the human verifier in this case is the mutual exclusivity of action classes i.e., each data point can belong only to one of the 400 classes present in Kinetics-400. Without this understanding, given, say a 10-second clip of a human skiing, the certificate set needs to necessarily encompass the entire 10 seconds since otherwise the human verifier might not be convinced that all of the other 399 actions are not occurring in the clip. However, with the knowledge of the label exclusivity meta-rule, the certificate length will be drastically reduced to just a fraction of a second since just observing the action of skiing in a few frames is sufficient for the human verifier to out-rule all other action classes.

**Certificate Conventions**. For small certificate lengths, it is difficult for humans to estimate the exact sub-clip timestamps to be included in the certificate set. Hence, we choose to have a minimum length of \(0.1\) second for a certificate. Further, in the case of two non-contiguous certificates, we collapse them into one if their closest ends are \(<5\) seconds apart. In cases where a fact needs to be verified at several places throughout the video, we let the annotator make a reasonable judgment for the length of the certificate to be included as long as it follows the above conditions.

Benchmarking EgoSchema

### Evaluating Certificate Lengths

Fig. 3 presents certificate lengths for a spectrum of tasks spread across \(15\) different datasets such as, action classification (Kinetics , Something-Something , UCF101 , HVU-Action ), detection (AVA ), relationship classification (LVU ), concept classification (HVU-Concept ), video classification (Youtube-8M ), Question-Answering (NextQA , AGQA , NextQA , IVQA , MSRVTT , ActivityNet-QA , EgoSchema). For EgoSchema we benchmark the certificate length for 5 hours of video data (\(100\)) chosen randomly. For each other dataset, we ensure that (A) each annotated label class (if applicable) has at least 1 data sample evaluated and, (B) at least two hours of human effort is applied. Fig. 2 shows the histogram of estimated EgoSchema temporal certificate lengths for the 100 clips.

Fig. 3 plots the certificate length against the actual clip length. We observe that EgoSchema has temporal certificate length \(5.7\) longer than the second longest certificate length dataset, and \(10\) to \(100\) longer than all other video understanding datasets.

### Evaluating Multiple-choice Question Answering on EgoSchema

In Table 6, We benchmark several state-of-the-art video-language models, with the intention of adding more models in the future, in a Zero-shot question-answering setting on EgoSchema. We evaluate each model in at least two settings. First is the conventional inference setting, where the model is assessed based on the same number of frames it was trained with. And second is a less challenging setting, where the model is tested on the maximum number of frames possible to execute inference with, using an 80G A100, without exceeding the GPU memory capacity. In both settings, frames are sampled uniformly from the input video clip.

**FrozenBiLM** adapts frozen multi-modal encoders trained on web-scale data for the task of question answering and achieves state-of-the-art zero-shot QA accuracy across \(8\) video question-answering datasets. We choose the How2QA FrozenBiM model under both \(10\) and \(90\) frames.

**VIOLET** a masked token modeling-based video language transformer that performs competitively on a variety of video-language tasks. We evaluate four of the best VIOLET models that are finetuned on different tasks for both \(5\) and \(75\) frames and choose the model with the best overall accuracy. More details are in _supplementary_.

**mPLUG-Owl** proposes a training strategy to add image & video modality to pretrained large language models. We adapt mPLUG to facilitate the multiple choice QA by prompting the model with each of the options individually in the format: 'Given question <question text>, is answer <answer text> correct?' along with the video frames. Then, we choose the option with the highest softmax score of the token 'Yes' in the output text. We observe accuracy to be non-monotonic in frame length, and report results in \(1\) to \(30\) frames in Table 6.

**InternVideo** proposes training video-language models jointly with masked video modeling and contrastive learning objectives. By default, InternVideo does not directly support multiple-choice video QA. We adapt the MSRVTT finetuned InternVideo model, which performs zero-shot

Figure 6: **Benchmarking Zero-shot QA on EgoSchema**

multiple-choice tasks, by incorporating the question with each answer choice in the format: 'Question: <question text>'s list <answer text>'. Then, we choose the option with the highest output score as the prediction. We report results spanning 10 to 90 input frames in Table 6. We observe that performance is monotonic with the number of frames but the gain saturates around just \(30\) frames.

**Human.** We also benchmark human performance on multiple-choice question answering task on EgoSchema in Table 7. _First_, are time pressure settings where the annotators are asked to choose the correct answer under one ('In <1 min') and three ('In <3 min') minutes. Humans can already achieve an impressive 67.0% accuracy, in under 1 minute! Interestingly, this only slightly increases (+1.0%) when allowed three minutes. We believe that this can inform about performance on EgoSchema in limited model inference capacities. We believe this could inform about the frame rate needed for long-form video understanding in future models. _Second_, we also benchmark human performance using only 1 fps video ('180 frames'). Surprisingly, we observe that just with 1 fps humans can achieve an impressive 67.2%. _Third_, we evaluate human performance in a restrictive setting where the annotator is forced to first watch the video without reading the text, and then answer the question without re-watching the video ('Video \(\) Text'). Curiously, this achieves better accuracy than the 'No constraint' setting where the annotators are asked to simply answer without any constraints (76.2% vs. 75.0%). A possible hypothesis is that watching the video without text allows the annotator to focus more closely on the video, thereby benefiting performance than the setting where the attention is somewhat divided between the text and video. We believe this will help us understand the performance trade-offs in the early vs. late fusion of video and text modalities for long-form video-language models. All accuracies are estimated over 5 hours of video.

## 5 Conclusion

We present EgoSchema, a novel diagnostic benchmark designed for assessing very long-form video-language understanding capabilities of modern multimodal models. We also introduce the notion of a temporal _certificate_ set, a probe that can be applied to a wide array of video tasks and benchmarks for understanding their intrinsic temporal lengths. We estimate temporal certificates of 15 varied datasets and demonstrate EgoSchema to exhibit temporal certificate length approximately \(5.7\) longer than the next longest dataset and \(25\) to \(100\) longer than all other video understanding datasets. We also benchmark several state-of-the-art models on EgoSchema and find their Zero-shot question-answering accuracy to be less than \(33\%\) while humans achieve 76%. We believe that EgoSchema will play a key role in the development and evaluation of future very long-form video-language models.

**Limitations.** EgoSchema RGB clips are sourced from Ego4D  and inherit Ego4D egocentric video biases. Further, the text is carefully curated for veracity, there are inevitable text data distribution biases that can occur in LLM-generated outputs due to biases present in web-scale LLM training data. Finally, human curation itself is far from perfect and while we perform two rounds of curation to minimize false positives, the collected EgoSchema is most likely to inevitably contain some small mislabelled or ill-formed question-answer sets. We plan to host a crowd-sourced errata board to minimize human curation error over time with the support of the open-source research community.