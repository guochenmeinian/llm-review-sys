# Laura Grigori 4, Aydin Buluc 2,1, James Demmel 1

Fast Exact Leverage Score Sampling from Khatri-Rao Products with Applications to Tensor Decomposition

Vivek Bharadwaj 1,2, Osman Asif Malik 2, Riley Murray 3,2,1,1Electrical Engineering and Computer Science Department, UC Berkeley

2Computational Research Division, Lawrence Berkeley National Lab

3International Computer Science Institute

4 Institute of Mathematics, EPFL & Lab for Simulation and Modelling, Paul Scherrer Institute

###### Abstract

We present a data structure to randomly sample rows from the Khatri-Rao product of several matrices according to the exact distribution of its leverage scores. Our proposed sampler draws each row in time logarithmic in the height of the Khatri-Rao product and quadratic in its column count, with persistent space overhead at most the size of the input matrices. As a result, it tractably draws samples even when the matrices forming the Khatri-Rao product have tens of millions of rows each. When used to sketch the linear least squares problems arising in CANDECOMP / PARAFAC tensor decomposition, our method achieves lower asymptotic complexity per solve than recent state-of-the-art methods. Experiments on billion-scale sparse tensors validate our claims, with our algorithm achieving higher accuracy than competing methods as the decomposition rank grows.

## 1 Introduction

The Khatri-Rao product (KRP, denoted by \(\)) is the column-wise Kronecker product of two matrices, and it appears in diverse applications across numerical analysis and machine learning . We examine overdetermined linear least squares problems of the form \(_{X}\|AX-B\|_{F}\), where the design matrix \(A=U_{1}... U_{N}\) is the Khatri-Rao product of matrices \(U_{j}^{I_{j} R}\). These problems appear prominently in signal processing , compressed sensing , inverse problems related to partial differential equations , and alternating least squares (ALS) CANDECOMP / PARAFAC (CP) tensor decomposition . In this work, we focus on the case where \(A\) has moderate column count (several hundred at most). Despite this, the problem remains formidable because the height of \(A\) is \(_{j=1}^{N}I_{j}\). For row counts \(I_{j}\) in the millions, it is intractable to even materialize \(A\) explicitly.

Several recently-proposed randomized sketching algorithms can approximately solve least squares problems with Khatri-Rao product design matrices . These methods apply a sketching operator \(S\) to the design and data matrices to solve the reduced least squares problem \(_{}\|SA-SB\|_{F}\), where \(S\) has far fewer rows than columns. For appropriately chosen \(S\), the residual of the downsampled system falls within a specified tolerance \(\) of the optimal residual with high probability \(1-\). In this work, we constrain \(S\) to be a _sampling matrix_ that selects and reweights a subset of rows from both \(A\) and \(B\). When the rows are selected according to the distribution of _statistical leverage scores_ on the design matrix \(A\), only \((R/())\) samples are required (subject to the assumptions at the end of section 2.1). The challenge, then, is to efficiently sample according to the leverage scores when \(A\) has Khatri-Rao structure.

We propose a leverage-score sampler for the Khatri-Rao product of matrices with tens of millions of rows each. After construction, our sampler draws each row in time quadratic in the column count, but logarithmic in the total row count of the Khatri-Rao product. Our core contribution is the following theorem.

**Theorem 1.1** (Efficient Khatri-Rao Product Leverage Sampling).: _Given \(U_{1},...,U_{N}\) with \(U_{j}^{I_{j} R}\), there exists a data structure satisfying the following:_

1. _The data structure has construction time_ \(O(_{j=1}^{N}I_{j}R^{2})\) _and requires additional storage space_ \(O(_{j=1}^{N}I_{j}R).\) _If a single entry in a matrix_ \(U_{j}\) _changes, it can be updated in time_ \(O(R(I_{j}/R))\)_. If the entire matrix_ \(U_{j}\) _changes, it can be updated in time_ \(O(I_{j}R^{2})\)_._
2. _The data structure produces_ \(J\) _samples from the Khatri-Rao product_ \(U_{1}... U_{N}\) _according to the exact leverage score distribution on its rows in time_ \[O(NR^{3}+J_{k=1}^{N}R^{2}(I_{k},R))\] _using_ \(O(R^{3})\) _scratch space. The structure can also draw samples from the Khatri-Rao product of any subset of_ \(U_{1},...,U_{N}\)_._

The efficient update property and ability to exclude one matrix are important in CP decomposition. When the inputs \(U_{1},...,U_{N}\) are sparse, an analogous data structure with \(O(R_{j=1}^{N}(U_{j}))\) construction time and \(O(_{j=1}^{N}(U_{j}))\) storage space exists with identical sampling time. Since the output factor matrices \(U_{1},...,U_{N}\) are typically dense, we defer the proof to Appendix A.8. Combined with error guarantees for leverage-score sampling, we achieve an algorithm for alternating least squares CP decomposition with asymptotic complexity lower than recent state-of-the-art methods (see Table 1).

Our method provides the most practical benefit on sparse input tensors, which may have dimension lengths in the tens of millions (unlike dense tensors that quickly incur intractable storage costs at large dimension lengths) . On the Amazon and Reddit tensors with billions of nonzero entries, our algorithm STS-CP can achieve 95% of the fit of non-randomized ALS between 1.5x and 2.5x faster than a high-performance implementation of the state-of-the-art CP-ARLS-LEV algorithm . Our algorithm is significantly more sample-efficient; on the Enron tensor, only \( 65,000\) samples per solve were required to achieve the 95% accuracy threshold above a rank of 50, which could not be achieved by CP-ARLS-LEV with even 54 times as many samples.

## 2 Preliminaries and Related Work

Notation.We use \([N]\) to denote the set \(\{1,...,N\}\) for a positive integer \(N\). We use \(\) notation to indicate the presence of multiplicative terms polylogarithmic in \(R\) and \((1/)\) in runtime complexities. For the complexities of our methods, these logarithmic factors are no more than \(O((R/))\). We

   Algorithm & Complexity per Iteration \\  CP-ALS  & \(N(N+I)I^{N-1}R\) \\ CP-ARLS-LEV  & \(N(R+I)R^{N}/()\) \\ TNS-CP  & \(N^{3}IR^{3}/()\) \\ Gaussian TNE  & \(N^{2}(N^{1.5}R^{3.5}/^{3}+IR^{2})/^{2}\) \\
**STS-CP (ours)** & \(N(NR^{3} I+IR^{2})/()\) \\   

Table 1: Asymptotic Complexity to decompose an \(N\)-dimensional \(I... I\) dense tensor via CP alternating least squares. For randomized algorithms, each approximate least-squares solution has residual within \((1+)\) of the optimal value with high probability \(1-\). Factors involving \( R\) and \((1/)\) are hidden (\(\) notation). See A.1 for details.

use Matlab notation \(A[i,:]\), \(A[:,i]\) to index rows, resp. columns, of matrices. For consistency, we use the convention that \(A[i,:]\) is a row vector. We use \(\) for standard matrix multiplication, \(\) as the elementwise product, \(\) to denote the Kronecker product, and \(\) for the Khatri-Rao product. See Appendix A.2 for a definition of each operation. Given matrices \(A^{m_{1} n}\), \(B^{m_{2} n}\), the \(j\)-th column of the Khatri-Rao product \(A B^{m_{1}m_{2} n}\) is the Kronecker product \(A[:,j] B[:,j]\).

We use angle brackets \(,...,\) to denote a **generalized inner product**. For identically-sized vectors / matrices, it returns the sum of all entries in their elementwise product. For \(A,B,C^{m n}\),

\[ A,B,C:=_{i=1,j=1}^{m,n}A[i,j]B[i,j]C [i,j].\]

Finally, \(M^{+}\) denotes the pseudoinverse of matrix \(M\).

### Sketched Linear Least Squares

A variety of random sketching operators \(S\) have been proposed to solve overdetermined least squares problems \(_{X}\|AX-B\|_{F}\) when \(A\) has no special structure . When \(A\) has Khatri-Rao product structure, prior work has focused on _sampling_ matrices , which have a single nonzero entry per row, operators composed of fast Fourier / trigonometric transforms , or Countsketch-type operators . For tensor decomposition, however, the matrix \(B\) may be sparse or implicitly specified as a black-box function. When \(B\) is sparse, Countsketch-type operators still require the algorithm to iterate over all nonzero values in \(B\). As Larsen and Kolda  note, operators similar to the FFT induce fill-in when applied to a sparse matrix \(B\), destroying the benefits of sketching. Similar difficulties arise when \(B\) is implicitly specified. This motivates our decision to focus on row sampling operators, which only touch a subset of entries from \(B\). Let \(_{1},...,_{J}\) be a selection of \(J\) indices for the rows of \(A^{I R}\), sampled i.i.d. according to a probability distribution \(q_{1},...,q_{I}\). The associated sampling matrix \(S^{J I}\) is specified by

\[S[j,i]=}},&_{j}=i\\ 0,&\]

where the weight of each nonzero entry corrects bias induced by sampling. When the probabilities \(q_{j}\) are proportional to the _leverage scores_ of the rows of \(A\), strong guarantees apply to the solution of the downsampled problem.

Leverage Score Sampling.The leverage scores of a matrix assign a measure of importance to each of its rows. The leverage score of row \(i\) from matrix \(A^{I R}\) is given by

\[_{i}=A[i,:](A^{}A)^{+}A[i,:]^{}\] (1)

for \(1 i I\). Leverage scores can be expressed equivalently as the squared row norms of the matrix \(Q\) in any reduced \(QR\) factorization of \(A\). The sum of all leverage scores is the rank of \(A\). Dividing the scores by their sum, we induce a probability distribution on the rows used to generate a sampling matrix \(S\). The next theorem has appeared in several works, and we take the form given by Malik et al. . For an appropriate sample count, it guarantees that the residual of the downsampled problem is close to the residual of the original problem.

**Theorem 2.1** (Guarantees for Leverage Score Sampling).: _Given \(A^{I R}\) and \(,(0,1)\), let \(S^{J I}\) be a leverage score sampling matrix for \(A\). Further define \(=*{arg\,min}_{X}\|SAX-SB\|_{F}\). If \(J R((R/),1/())\), then with probability at least \(1-\) it holds that_

\[\|A-B\|_{F}(1+)_{X}\|AX-B\| _{F}.\]

For the applications considered in this work, \(R\) ranges up to a few hundred. As \(\) and \(\) tend to 0 with fixed \(R\), \(1/()\) dominates \((R/)\). Hence, we assume that the minimum sample count \(J\) to achieve the guarantees of the theorem is \((R/())\).

### Prior Work

Khatri-Rao Product Leverage Score Sampling.Well-known sketching algorithms exist to quickly estimate the leverage scores of dense matrices . These algorithms are, however, intractable for \(A=U_{1}... U_{N}\) due to the height of the Khatri-Rao product. Cheng et al.  instead approximate each score as a product of leverage scores associated with each matrix \(U_{j}\). Larsen and Kolda  propose CP-ARLS-LEV, which uses a similar approximation and combines random sampling with a deterministic selection of high-probability indices. Both methods were presented in the context of CP decomposition. To sample from the Khatri-Rao product of \(N\) matrices, both require \(O(R^{N}/())\) samples to achieve the \((,)\) guarantee on the residual of each least squares solution. These methods are simple to implement and perform well when the Khatri-Rao product has column count up to 20-30. On the other hand, they suffer from high sample complexity as \(R\) and \(N\) increase. The TNS-CP algorithm by Malik et al.  samples from the exact leverage score distribution, thus requiring only \(O(R/())\) samples per least squares solve. Unfortunately, it requires time \(O(_{j=1}^{N}I_{j}R^{2})\) to draw each sample.

Comparison to Woodruff and Zandieh.The most comparable results to ours appear in work by Woodruff and Zandieh , who detail an algorithm for approximate ridge leverage-score sampling for the Khatri-Rao product in near input-sparsity time. Their work relies on a prior oblivious method by Ahle et al. , which sketches a Khatri-Rao product using a sequence of Countsketch / OSNAP operators arranged in a tree. Used in isolation to solve a linear least squares problem, the tree sketch construction time scales as \(O(_{j=1}^{N}(U_{j}))\) and requires an embedding dimension quadratic in \(R\) to achieve the \((,)\) solution-quality guarantee. Woodruff and Zandieh use a collection of these tree sketches, each with carefully-controlled approximation error, to design an algorithm with linear runtime dependence on the column count \(R\). On the other hand, the method exhibits \(O(N^{7})\) scaling in the number of matrices involved, has \(O(^{-4})\) scaling in terms of the desired accuracy, and relies on a sufficiently high ridge regularization parameter. Our data structure instead requires construction time quadratic in \(R\). In exchange, we use distinct methods to design an efficiently-updatable sampler with runtime linear in both \(N\) and \(^{-1}\). These properties are attractive when the column count \(R\) is below several thousand and when error as low as \( 10^{-3}\) is needed in the context of an iterative solver (see Figure 5). Moreover, the term \(O(R^{2}_{j=1}^{N}I_{j})\) in our construction complexity arises from symmetric rank-\(k\) updates, a highly-optimized BLAS3 kernel on modern CPU and GPU architectures. Appendix A.3 provides a more detailed comparison between the two approaches.

Kronecker Regression.Kronecker regression is a distinct (but closely related) problem to the one we consider. There, \(A=U_{1}... U_{N}\) and the matrices \(U_{i}\) have potentially distinct column counts \(R_{1},...,R_{N}\). While the product distribution of leverage scores from \(U_{1},...,U_{N}\) provides only an approximation to the leverage score distribution of the Khatri-Rao product [6; 15], it provides the _exact_ leverage distribution for the Kronecker product. Multiple works [7; 9] combine this property with other techniques, such as dynamically-updatable tree-sketches , to produce accurate and updatable Kronecker sketching methods. None of these results apply directly in our case due to the distinct properties of Kronecker and Khatri-Rao products.

## 3 An Efficient Khatri-Rao Leverage Sampler

Without loss of generality, we will prove part 2 of Theorem 1.1 for the case where \(A=U_{1}... U_{N}\); the case that excludes a single matrix follows by reindexing matrices \(U_{k}\). We further assume that \(A\) is a nonzero matrix, though it may be rank-deficient. Similar to prior sampling works [18; 29], our algorithm will draw one sample from the Khatri-Rao product by sampling a row from each of \(U_{1},U_{2},....\) in sequence and computing their Hadamard product, with the draw from \(U_{j}\) conditioned on prior draws from \(U_{1},...,U_{j-1}\).

Let us index each row of \(A\) by a tuple \((i_{1},...,i_{N})[I_{1}]...[I_{N}]\). Equation (1) gives

\[_{i_{1},...,i_{N}}=A[(i_{1},...,i_{N}),:](A^{}A)^{+}A [(i_{1},...,i_{N}),:]^{}.\] (2)

For \(1 k N\), define \(G_{k}:=U_{k}^{}U_{k}^{R R}\) and \(G:=(_{k=1}^{N}G_{k})^{R R}\); it is a well-known fact that \(G=A^{}A\). For a single row sample from \(A\), let \(_{1},...,_{N}\) be random variables for thedraws from multi-index set \([I_{1}]...[I_{N}]\) according to the leverage score distribution. Assume, for some \(k\), that we have already sampled an index from each of \([I_{1}]\,,...,[I_{k-1}]\), and that the first \(k-1\) random variables take values \(_{1}=s_{1},...,_{k-1}=s_{k-1}\). We abbreviate the latter condition as \(_{<k}=s_{<k}\). To sample from \(I_{k}\), we seek the distribution of \(_{k}\) conditioned on \(_{1},..._{k-1}\). Define \(h_{<k}\) as the transposed elementwise product1 of rows already sampled:

\[h_{<k}:=_{i=1}^{k-1}U_{i}[s_{i},:]^{}.\] (3)

Also define \(G_{>k}\) as

\[G_{>k}:=G^{+}\;\;_{i=k+1}^{N}G_{i}.\] (4)

Then the following theorem provides the conditional distribution of \(_{k}\).

**Theorem 3.1** (Malik 2022, , Adapted).: _For any \(s_{k}[I_{k}]\),_

\[p(_{k}=s_{k}_{<k}=s_{<k}) =C^{-1} h_{<k}h_{<k}^{},U_{k}[s_{k},:]^{ }U_{k}[s_{k},:],G_{>k}\] (5) \[:=q_{h_{<k},U_{k},G_{>k}}[s_{k}]\]

_where \(C= h_{<k}h_{<k}^{},U_{k}^{}U_{k},G_{>k}\) is nonzero._

We include the derivation of Theorem 3.1 from Equation (2) in Appendix A.4. Computing all entries of the probability vector \(q_{h_{<k},U_{k},G_{>k}}\) would cost \(O(I_{j}R^{2})\) per sample, too costly when \(U_{j}\) has millions of rows. It is likewise intractable (in preprocessing time and space complexity) to precompute probabilities for every possible conditional distribution on the rows of \(U_{j}\), since the conditioning random variable has \(_{k=1}^{j-1}I_{k}\) potential values. Our key innovation is a data structure to sample from a discrete distribution of the form \(q_{h_{<k},U_{k},G_{>k}}\)_without_ materializing all of its entries or incurring superlinear cost in either \(N\) or \(^{-1}\). We introduce this data structure in the next section and will apply it twice in succession to get the complexity in Theorem 1.1.

### Efficient Sampling from \(q_{h,U,Y}\)

We introduce a slight change of notation in this section to simplify the problem and generalize our sampling lemma. Let \(h^{R}\) be a vector and let \(Y^{R R}\) be a positive semidefinite (p.s.d.) matrix, respectively. Our task is to sample \(J\) rows from a matrix \(U^{I R}\) according to the distribution

\[q_{h,U,Y}[s]:=C^{-1} hh^{},U^{}[s,:]U[ s,:],Y\] (6)

provided the normalizing constant \(C= hh^{},U^{}U,Y\), is nonzero. We impose that all \(J\) rows are drawn with the same matrices \(Y\) and \(U\), but potentially distinct vectors \(h\). The following lemma establishes that an efficient sampler for this problem exists.

**Lemma 3.2** (Efficient Row Sampler).: _Given matrices \(U^{I R},Y^{R R}\) with \(Y\) p.s.d., there exists a data structure parameterized by positive integer \(F\) that satisfies the following:_

1. _The structure has construction time_ \(O(IR^{2})\) _and storage requirement_ \(O(R^{2} I/F)\)_. If_ \(I<F\)_, the storage requirement drops to_ \(O(1)\)_._
2. _After construction, the data structure can produce a sample according to the distribution_ \(q_{h,U,Y}\) _in time_ \(O(R^{2} I/F+FR^{2})\) _for any vector_ \(h\)_._
3. _If_ \(Y\) _is a rank-1 matrix, the time per sample drops to_ \(O(R^{2} I/F+FR)\)_._

This data structure relies on an adaptation of a classic binary-tree inversion sampling technique . Consider a partition of the interval \(\) into \(I\) bins, the \(i\)-th having width \(q_{h,U,Y}[i]\). We sample \(d[0,1]\) and return the index of the containing bin. We locate the bin index through a binary search terminated when at most \(F\) bins remain in the search space, which are then scanned in linear time. Here, \(F\) is a tuning parameter that we will use to control sampling complexity and space usage.

We can regard the binary search as a walk down a full, complete binary tree \(T_{I,F}\) with \([I/F]\) leaves, the nodes of which store contiguous, disjoint segments \(S(v)=\{S_{0}(v)..S_{1}(v)\}[I]\) of size at most \(F\). The segment of each internal node is the union of segments held by its children, and the root node holds \(\{1,...,I\}\). Suppose that the binary search reaches node \(v\) with left child \(L(v)\) and maintains the interval \([,][0,1]\) as the remaining search space to explore. Then the search branches left in the tree iff \(d<+_{i S(L(v))}q_{h,U,Y}[i].\)

This branching condition can be evaluated efficiently if appropriate information is stored at each node of the segment tree. Excluding the offset "low", the branching threshold takes the form

\[_{i S(v)}q_{h,U,Y}[i]=C^{-1} hh^{},_{i S(v )}U[i,]^{}U[i,],Y:=C^{-1} hh ^{},G^{v},Y.\] (7)

Here, we call each matrix \(G^{v}^{R R}\) a _partial Gram matrix_. In time \(O(IR^{2})\) and space \(O(R^{2} I/F)\), we can compute and cache \(G^{v}\) for each node of the tree to construct our data structure. Each subsequent binary search costs \(O(R^{2})\) time to evaluate Equation (7) at each of \( I/F\) internal nodes and \(O(FR^{2})\) to evaluate \(q_{h,U,Y}\) at the \(F\) indices held by each leaf, giving point 2 of the lemma. This cost at each leaf node reduces to \(O(FR)\) in case \(Y\) is rank-1, giving point 3. A complete proof of this lemma appears in Appendix A.5.

### Sampling from the Khatri-Rao Product

We face difficulties if we directly apply Lemma 3.2 to sample from the conditional distribution in Theorem 3.1. Because \(G_{>k}\) is not rank-1 in general, we must use point 2 of the lemma, where no selection of the parameter \(F\) allows us to simultaneously satisfy the space and runtime constraints of Theorem 1.1. Selecting \(F=R\) results in cost \(O(R^{3})\) per sample (violating the runtime requirement in point 2), whereas \(F=1\) results in a superlinear storage requirement \(O(IR^{2})\) (violating the space requirement in point 1, and becoming prohibitively expensive for \(I 10^{6}\)). To avoid these extremes, we break the sampling procedure into two stages. The first stage selects a 1-dimensional subspace spanned by an eigenvector of \(G_{>k}\), while the second samples according to Theorem 3.1 after projecting the relevant vectors onto the selected subspace. Lemma 3.2 can be used for _both_ stages, and the second stage benefits from point 3 to achieve better time and space complexity.

Below, we abbreviate \(q=q_{h_{<k},U_{k},G_{>k}}\) and \(h=h_{<k}\). When sampling from \(I_{k}\), observe that \(G_{>k}\) is the same for all samples. We compute a symmetric eigendecomposition \(G_{>k}=V V^{}\), where each column of \(V\) is an eigenvector of \(G_{>k}\) and \(=((_{u})_{u=1}^{R})\) contains the eigenvalues along the diagonal. This allows us to rewrite entries of \(q\) as

\[q[s]=C^{-1}_{u=1}^{R}_{u} hh^{},U_{k}[s, :]^{}U_{k}[s,:],V[:,u]V[:,u]^{ }.\] (8)

Define matrix \(W^{I_{k} R}\) elementwise by

\[W[t,u]:= hh^{},U_{k}[t,:]^{}U_{k}[t,:],V[:,u]V[:,u]^{}\]

and observe that all of its entries are nonnegative. Since \(_{u} 0\) for all \(u\) (\(G_{>k}\) is p.s.d.), we can write \(q\) as a mixture of probability distributions given by the normalized columns of \(W\):

\[q=_{u=1}^{R}w[u]},\]

where the vector \(w\) of nonnegative weights is given by \(w[u]=(C^{-1}_{u}\|W[:,u]\|_{1})\). Rewriting \(q\) in this form gives us the two stage sampling procedure: first sample a component \(u\) of the mixture according to the weight vector \(w\), then sample an index in \(\{1..I_{k}\}\) according to the probability vector

Figure 1: A segment tree \(T_{8,2}\) and probability distribution \(\{q_{1},...,q_{8}\}\) on \([1,...,8]\).

[MISSING_PAGE_FAIL:7]

in the Khatri-Rao product to match the ordering of rows in the matricized tensor (see Appendix A.9 for an explicit formula for the matricization). These problems are ideal candidates for randomized sketching [4; 12; 15], and applying the data structure in Theorem 1.1 gives us the **STS-CP** algorithm.

**Corollary 3.3** (Sts-Cp).: _Suppose \(\) is dense, and suppose we solve each least squares problem in ALS with a randomized sketching algorithm. A leverage score sampling approach as defined in section 2 guarantees that with \((R/())\) samples per solve, the residual of each sketched least squares problem is within \((1+)\) of the optimal residual with probability \((1-)\). The efficient sampler from Theorem 1.1 brings the complexity of ALS to_

\[(}{}_{j=1}^{N}( NR^{3} I_{j}+I_{j}R^{2}))\]

_where "\(\#\)it" is the number of ALS iterations, and with any term \( I_{j}\) replaced by \( R\) if \(I_{j}<R\)._

The proof appears in Appendix A.9 and combines Theorem 1.1 with Theorem 2.1. STS-CP also works for sparse tensors and likely provides a greater advantage here than the dense case, as sparse tensors tend to have much larger mode size . The complexity for sparse tensors depends heavily on the sparsity structure and is difficult to predict. Nevertheless, we expect a significant speedup based on prior works that use sketching to accelerate CP decomposition [6; 15].

## 4 Experiments

Experiments were conducted on CPU nodes of NERSC Perlmutter, an HPE Cray EX supercomputer, and our code is available at https://github.com/vbharadwaj-bk/fast_tensor_leverage.git. On tensor decomposition experiments, we compare our algorithms against the random and hybrid versions of CP-ARLS-LEV proposed by Larsen and Kolda . These algorithms outperform uniform sampling and row-norm-squared sampling, achieving excellent accuracy and runtime relative to exact ALS. In contrast to TNS-CP and the Gaussian tensor network embedding proposed by Ma and Solomonik (see Table 1), CP-ARLS-LEV is one of the few algorithms that can practically decompose sparse tensors with mode sizes in the millions. In the worst case, CP-ARLS-LEV requires \((R^{N-1}/())\) samples per solve for an \(N\)-dimensional tensor to achieve solution guarantees like those in Theorem 2.1, compared to \((R/())\) samples required by STS-CP. Appendices A.10, A.11, and A.13 provide configuration details and additional results.

### Runtime Benchmark

Figure 2 shows the time to construct our sampler and draw 50,000 samples from the Khatri-Rao product of i.i.d. Gaussian initialized factor matrices. We quantify the runtime impacts of varying \(N\), \(R\), and \(I\). The asymptotic behavior in Theorem 1.1 is reflected in our performance measurements, with the exception of the plot that varies \(R\). Here, construction becomes disproportionately cheaper than sampling due to cache-efficient BLAS3 calls during construction. Even when the full Khatri-Rao product has \( 3.78 10^{22}\) rows (for \(I=2^{25},N=3,R=32\)), we require only \(0.31\) seconds on average for sampling (top plot, rightmost points).

### Least Squares Accuracy Comparison

We now test our sampler on least squares problems of the form \(_{x}\|Ax-b\|\), where \(A=U_{1}... U_{N}\) with \(U_{j}^{I R}\) for all \(j\). We initialize all matrices \(U_{j}\) entrywise i.i.d. from a standard normal distribution and randomly multiply 1% of all entries by 10. We choose \(b\) as a Kronecker product \(c_{1}... c_{N}\), with each vector \(c_{j}^{I}\) also initialized entrywise from a Gaussian distribution. We assume this form for \(b\) to tractably compute the exact solution to the linear least squares problem and evaluate the accuracy of our randomized methods. We **do not** give our algorithms access to the Kronecker form of \(b\); they are only permitted on-demand, black-box access to its entries.

For each problem instance, define the distortion of our sampling matrix \(S\) with respect to the column space of \(A\) as

\[D(S,A)=\] (12)

where \(Q\) is an orthonormal basis for the column space of \(A\) and \((SQ)\) is the condition number of \(SQ\). A higher-quality sketch \(S\) exhibits lower distortion, which quantifies the preservation of distances from the column space of \(A\) to the column space of \(SA\). For details about computing \((SQ)\) efficiently when \(A\) is a Khatri-Rao product, see Appendix A.12. Next, define \(=_{}}{_{ }}-1\), where \(_{}\) is the residual of the randomized least squares algorithm. \(\) is nonnegative and (similar to its role in Theorem 2.1) quantifies the quality of the randomized algorithm's solution.

For varying \(N\) and \(R\), Figure 3 shows the average values of \(D\) and \(\) achieved by our algorithm against the leverage product approximation used by Larsen and Kolda . Our sampler exhibits nearly constant distortion \(D\) for fixed rank \(R\) and varying \(N\), and it achieves \( 10^{-2}\) even when \(N=9\). The product approximation increases both the distortion and residual error by at least an order of magnitude.

### Sparse Tensor Decomposition

We next apply STS-CP to decompose several large sparse tensors from the FROSTT collection  (see Appendix A.11 for more details on the experimental configuration). Our accuracy metric is the tensor fit. Letting \(}\) be our low-rank CP approximation, the fit with respect to ground-truth tensor \(\) is \((},)=1-\|}- \|_{F}/\|\|_{F}\).

Table 4 in Appendix A.13.1 compares the runtime per round of our algorithm against the Tensorly Python package  and Matlab Tensor Toolbox , with dramatic speedup over both. As Figure 4 shows, the fit achieved by CP-ARLS-LEV compared to STS-CP degrades as the rank increases forfixed sample count. By contrast, STS-CP improves the fit consistently, with a significant improvement at rank 125 over CP-ARLS-LEV. Timings for both algorithms are available in Appendix A.13.5. Figure 6 explains the higher fit achieved by our sampler on the Uber and Amazon tensors. In the first 10 rounds of ALS, we compute the exact solution to each least squares problem before updating the factor matrix with a randomized algorithm's solution. Figure 6 plots \(\) as ALS progresses for hybrid CP-ARLS-LEV and STS-CP. The latter consistently achieves lower residual per solve. We further observe that CP-ARLS-LEV exhibits an oscillating error pattern with period matching the number of modes \(N\).

To assess the tradeoff between sampling time and accuracy, we compare the fit as a function of ALS update time for STS-CP and random CP-ARLS-LEV in Figure 6 (time to compute the fit excluded). On the Reddit tensor with \(R=100\), we compared CP-ARLS-LEV with \(J=2^{16}\) against CP-ARLS-LEV with progressively larger sample count. Even with \(2^{18}\) samples per randomized least squares solve, CP-ARLS-LEV cannot achieve the maximum fit of STS-CP. Furthermore, STS-CP makes progress more quickly than CP-ARLS-LEV. See Appendix A.13.4 for similar plots for other datasets.

## 5 Discussion and Future Work

Our method for exact Khatri-Rao leverage score sampling enjoys strong theoretical guarantees and practical performance benefits. Especially for massive tensors such as Amazon and Reddit, our randomized algorithm's guarantees translate to faster progress to solution and higher final accuracies. The segment tree approach described here can be applied to sample from tensor networks besides the Khatri-Rao product. In particular, modifications to Lemma 3.2 permit efficient leverage sampling from a contraction of 3D tensor cores in ALS tensor train decomposition. We leave the generalization of our fast sampling technique as future work.