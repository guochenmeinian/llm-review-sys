# Awt: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation

Yuhan Zhu\({}^{1}\)   Yuyang Ji\({}^{1}\)   Zhiyu Zhao\({}^{1,2}\)   Gangshan Wu\({}^{1}\)   Limin Wang\({}^{1,2}\)

\({}^{1}\)State Key Laboratory for Novel Software Technology, Nanjing University

\({}^{2}\)Shanghai AI Laboratory

https://github.com/MCG-NJU/AWT

Corresponding author: lmwang@nju.edu.cn

###### Abstract

Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks. However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes. To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space. AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module. We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales.

## 1 Introduction

Recent advances in vision-language models (VLMs) , which undergo extensive pre-training on web-scale image-text pairs, have exhibited remarkable success in various classification tasks. VLMs are trained to associate images with relevant textual descriptions. In the standard protocol (Fig. 1(a)), raw images and class names are projected into a joint vision-language embedding space, where the class with the shortest distance to the image representation is selected as the prediction result.

However, directly using raw images and class names in testing has limitations . Visually, the broad scope of pre-training compels VLMs to analyze all image elements, lacking capability of focusing on specific interested regions. For instance, a model might miss critical facial features of a cat while unnecessarily focusing on irrelevant elements like "bench" and "grass" (Fig. 1(a)). Textually, since VLM pre-training associates visual elements with diverse and rich textual descriptions (_e.g._, colors and textures), merely using class names during test falls short of capturing the full spectrum of visual content. To enhance input effectiveness, the literature focuses on post-training prompts  (Fig. 1(b)) that provide contextual cues, thereby helping the model in prioritizing relevant features, such as cat's attributes. However, this approach often depends on the availability of training resources, which may not be always practical.

In this study, we are interested in enhancing inputs for better adaptation of VLMs **without** training prompts. We advocate for data augmentation as a simple yet effective strategy, as depicted in Fig. 1(c).

Techniques like random resized cropping and image flipping enrich the input with varied and multiscale perspectives, while detailed textual descriptions for each class provide richer visual narratives. Although manually crafting diverse descriptions for each class is expensive, employing Large Language Models (LLMs)  presents an efficient alternative.

Nonetheless, several challenges remain. First, the intra-modal importance of each augmented image and description needs assessment, as not all views contribute equally to class recognition--some may be irrelevant background elements or non-visual descriptors such as the cat's personality. Second, the inter-modal interaction requires consideration, as descriptions such as "dark face" or "light-colored body" might have direct semantic correlations with some image crops (Fig. 1(c)).

To tackle these challenges, we propose AWT, a novel framework that **augments** raw inputs into diverse views, **weights** view importance in each modality dynamically, and **transports** semantic correlations across modalities. Initially, AWT augments raw inputs via image transformations and LLMs. Subsequently, it weights the importance of each view on the fly based on its prediction entropy, as more confident predictions typically indicate higher accuracy . This method allows AWT to identify and prioritize significant views, and adjust the importance distribution dynamically according to the task-specific context (_e.g._, candidate class names). AWT then formulates the image-text distance calculation as an optimal transport problem , considering each augmented view as a quantity of sand. The importance assessed for each view determines the mass of its corresponding sand pile, and distances are calculated using cosine similarity. This formulation can effectively discover cross-modal correlations by solving the optimal transport problem--which minimizes the effort required to transport sand from one modality to another. Additionally, generating class descriptions from LLMs using a simple prompt like "Describe a {class}." often results in overly generic descriptions. Inspired by chain-of-thought approach , we introduce a two-step, dataset-aware prompting method. This approach encourages LLMs to produce class descriptions that are both diverse and dataset-relevant.

We implement AWT using the CLIP model  and evaluated its performance across 21 datasets covering four challenging tasks: zero-shot and few-shot image classification, out-of-distribution generalization, and zero-shot video action recognition. As shown in Fig. 1(d), AWT consistently surpasses the existing state-of-the-art methods in each setting. Our extensive analysis further examines AWT's flexibility with diverse architectures, its scalability with different model sizes, and its potential applicability to other VLMs.

## 2 Related Work

Vision-Language Models.Leveraging the extensive pre-training on web-scale text-image pairs, vision-language models (VLMs) such as CLIP  and ALIGN  excel in acquiring versatile

Figure 1: (a) Standard protocol directly calculates distances between raw images and class names in the joint V-L space. (b) Prompt-based methods enhance inputs with post-trained visual or textual prompts to provide the task-specific context. (c) Augment-based method enriches raw inputs with image transformations and class descriptions, requiring no additional training. Upon this, we propose AWT, which considers both intra-modal importance variations and cross-modal semantic correlations. (d) AWT is evaluated against SOTA methods across four tasks: zero-shot and few-shot image classification, out-of-distribution generalization, and zero-shot video action recognition.

representations that span multiple modalities. These models adeptly embed texts and images into a shared vision-language feature space, enabling the proximity of inputs with analogous semantics. The inherent flexibility of natural language allows VLMs to be effectively utilized across a wide range of open-set tasks including image classification [1; 9; 13], object detection [41; 42; 43], image generation [44; 45], video action recognition [46; 47; 48]. However, such general-purpose models often fail to focus on task-specific details, which can result in sub-optimal performance. This study aims to overcome this limitation by proposing a novel adaptation framework, namely AWT, for VLMs.

Adapt VLMs to downstream tasks.Direct adaptation of pre-trained VLMs to downstream tasks often results in suboptimal and unstable performance . To overcome this, the existing literature has primarily focused on the use of post-training to enrich task context. This includes strategies such as few-shot prompt learning [9; 11; 12; 29; 32], cross-dataset prompt generalization [25; 26; 27; 28; 29; 30; 31; 33], unsupervised prompt tuning [23; 24; 28], test-time prompt tuning [10; 49; 50; 51; 52; 53; 54], and adapter tuning [55; 56; 57; 58; 59]. Conversely, other approaches aim to augment inputs using various resources such as the WordNet relationship hierarchy , Large Language Models [61; 62; 63; 64], or Stable Diffusion models [65; 50; 66]. Nonetheless, these methods mainly enhance only one modality. In contrast, our study innovatively applies augmentation to both visual and textual modalities and addresses significant challenges in dual-modality augmentation scenarios.

Optimal Transport (OT).Optimal transport (OT), originating from the Monge problem  in the eighteenth century, serves as a metric for quantifying the distance between mathematical entities  while considering their intricate geometric structures . Historically rediscovered in various forms, OT first gained fame in computer vision under the name of earth mover's distances . The development of efficient approximate solvers  has recently propelled a resurgence in OT's popularity, broadening its utility across multiple domains, including object detection [70; 71], domain adaptation [72; 73; 74], generative modeling [75; 76; 77; 78], semantic correspondence , point clouds [80; 81; 82], prompt learning [83; 84; 85] and video understanding [86; 87]. Of particular relevance to our study are PLOT  and Wang et al. , which leverage OT for fine-grained prompt learning to enhance VLMs. Distinct from these two studies, our research diverges by eschewing the need for additional training resources, opting instead for an augmentation-based direction.

## 3 Methodology

### Preliminaries

Contrastive Language-Image Pre-training (CLIP).CLIP  integrates dual encoders--an image encoder \(f()\) and a text encoder \(g()\)--to map images and textual descriptions into a shared vision-language (V-L) embedding space. CLIP is designed to minimize the cosine distance between embeddings of semantically related image-text pairs. Thanks to the flexibility of natural language, CLIP enables direct application to classification tasks without the need for task-specific training. For instance, given an image \(X^{3 H W}\) and a set of candidate class names \(\{t_{i}\}_{i=1}^{C}\), where \(C\) denotes the class count. CLIP computes the embeddings \(^{d}\) for the image and \(\{_{i}\}_{i=1}^{C}^{C d}\) for all class names, where \(d\) is the feature dimension. Subsequently, the classification probability for image \(X\) being of class \(t_{i}\) can be formulated as:

\[p(t_{i} X)=_{i},)/ )}{_{j=1}^{C}((_{j},)/ )},\] (1)

where \(\) is a temperature parameter.

Optimal Transport (OT).Optimal transport (OT) theory, originating from the Monge problem , provides a framework for structural distance measurements. This theory conceptualizes scenarios such as relocating sand at a construction site with the goal of minimizing effort. Mathematically, the initial and target distributions of sands are modeled as discrete measures:

\[=_{i=1}^{N}_{i}_{x_{i}}=_{j=1}^{M}_{j}_{y_{j}},\] (2)

where \(_{x_{i}}\) denotes the Dirac with a concentrated mass \(_{i}\) centered at \(x_{i}\), and similarly for \(\). Here, \(N\) and \(M\) represent the number of source and target locations, respectively. The cost of transporting sands from any source location \(x_{i}\) to any target location \(y_{j}\) is given by the cost function \(c(x_{i},y_{j})\). To extend the application to broader and more intricate scenarios, _e.g._, cross-modal correlation, the Kantorovich relaxation  is employed. This relaxation introduces flexibility in the transport plan and ensures symmetric transport solutions. The transport plan \(_{+}^{N M}\), where element \(_{i,j}\) indicates the mass transported from \(x_{i}\) to \(y_{j}\), must satisfy the constraints:

\[(,)\;}}{{=}} \;\{_{+}^{N M}:_{M}= ^{}_{N}= \}.\] (3)

Kantorovich's formulation seeks to minimize the total transportation cost:

\[_{c}(,)\;}}{{=}}\; _{(,)}, \;}}{{=}}\;_{i,j} _{i,j}_{i,j},\] (4)

where \(_{i,j}=c(x_{i},y_{j})\) defines the cost matrix.

### Awt: Augment, Weight, then Transport

Pre-trained VLMs often underperform when adapted to new concepts due to insufficient information about new classes. Moreover, their extensive pre-training scope leads them to analyze all elements of an image, causing them to miss contextually important cues crucial for specific downstream applications. To overcome these limitations, we introduce a novel framework, termed AWT (Augment, Weight, then Transport), to enhance the adaptability of VLMs without additional training. The AWT framework, as depicted in Fig. 2, consists of three critical components: augmenting raw inputs to generate diverse and content-rich views, weighting the significance of these views within each modality, and transporting semantically correlated elements across modalities.

#### 3.2.1 Augment Raw Inputs

The augmentation process begins with an image \(X^{3 H W}\) and the class name set \(\{t_{i}\}_{i=1}^{C}\), aiming to transform these inputs into various views that offer different perspectives and details.

For visual augmentation, we apply standard data augmentation including random resized cropping and random flipping to produce a set of varied views \(\{X^{n}\}_{n=1}^{N+1}\). This set includes \(N\) augmented images alongside the original (denoted as the \(0\) index), enriching the input with diverse and multiscale perspectives. An illustrative example is shown in Fig. 2.

To enrich the textual modality, we utilize Large Language Models (LLMs) to generate class descriptions. Typical prompts like "Describe a {class}." often result in descriptions that are either vague--lacking in specific visual details--or contextually misaligned. For instance, in contexts

Figure 2: **Pipeline of AWT: Augment, Weight, then Transport. Given an image and candidate class names, we first augment each input into diverse views. These views are then fed into the CLIP model to obtain coarse predictions. To assess the importance of each view, we use prediction confidence as a proxy and introduce an entropy-based weighting mechanism. Next, we measure the distance between image-text view sets by solving an optimal transport (OT) problem. Finally, the resulting OT distance is used to represent the distance between the input image and each class name.**

such as classifying sketches, generic descriptions of a category may not correspond well with the sketch images. To address this, we adopt a two-step, dataset-aware prompt strategy. Initially, we prompt LLMs to generate multiple questions that probe different aspects of the category, which is crucial for eliciting detailed and varied descriptions. To ensure the queries are aligned with the visual content, we incorporate a dataset-level description into the initial prompts. Specifically, we start by asking LLMs to "Generate questions to classify images from a dataset, which {dataset description}.". Using the dataset-related questions generated from the first step, we proceed to the second step where these questions are combined with the specific class name to obtain tailored descriptions. The set of augmented views for each class \(t_{i}\) is denoted as \(\{t_{i}^{m}\}_{m=1}^{M+1}\), including an additional view formed by the raw class name. This method ensures both diversity in the descriptions and their relevance to the visual content. More details can be found in Appendix C.2.

#### 3.2.2 Weight Augmented Views

Following augmentation, it is essential to assess the significance of each augmented view, as not all views contribute equally to classification. Some views may be critical while others might be less informative or even noisy. To address this variation, we introduce an entropy-based weighting mechanism to quantify each view's importance. Our key insight is that the impact of a view on classification confidence--a metric often correlated with accuracy --can serve as a proxy for its importance. A view that significantly enhances classification confidence is considered more vital.

To assess the importance of \(n\)-th image view \(X^{n}\), we maintain a constant text context and compute the averaged embedding for each class as \(\{}_{i}=_{m=1}^{M+1}_{i}^{m}\}_{i=1}^{C}\), where \(_{i}^{m}\) is the CLIP embedding of \(t_{i}^{m}\). The classification probability \(p(t X^{n})\) is then calculated using the image embedding \(^{n}\) and text embeddings \(\{}_{i}\}_{i=1}^{C}\), following Eq. (1). Predictive confidence is then quantified using the entropy formula \(H_{n}(t)=-_{t}p(t X^{n}) p(t X^{n})\). Lower entropy indicates higher confidence, allowing us to evaluate view importance through the negative entropy as follows:

\[_{n}=(t)/_{v})}{_{j=1}^{N+1} (-H_{j}(t)/_{v})}, n=1,,N+1,\] (5)

where \(_{v}\) is a temperature parameter adjusting the distribution's sharpness.

Similarly, to determine the importance of \(m\)-th description for \(i\)-th class, _i.e._, \(t_{i}^{m}\), we calculate the classification probability \(p_{m}^{i}(t X^{0})\), with the image embedding \(^{0}\) and text embeddings \(\{_{i}^{m}\}\{}_{j}\}_{j=1,j i}^{C}\). The classification entropy is given by \(H_{m}^{i}(t)=-_{t}p_{m}^{i}(t X^{0}) p_{m}^{i}(t  X^{0})\). We then calculate the importance scores for all descriptions within the \(i\)-th class as follows:

\[_{m}^{i}=^{i}(t)/_{t})}{_{k=1 }^{M+1}(-H_{k}^{i}(t)/_{t})}, m=1,,M+1,\] (6)

where \(_{t}\) is the temperature parameter. This entropy-based weighting mechanism ensures the prioritization of the contextually significant views. By dynamically adjusting the importance based on the direct impact on classification confidence, the augmented view sets can be well-prepared for the optimal transport process.

#### 3.2.3 Transport Across Modalities

Our primary goal is to precisely measure the distance between an image and its candidate names. Through the augmentation, we have transformed each original image or class name into a set of augmented views. Typically, the distance between these sets is measured by simply averaging the embeddings within each set. However, such practice often fails to capture the dynamic correlation across modalities. Consider the scenario depicted in Fig. 2, where specific textual descriptions such as "has a dome-shaped top" might correlate directly with certain image crops. The conventional averaging strategy typically overlooks these intuitive and meaningful correlations.

To address this issue, we propose a novel approach by formulating distance measurement as an optimal transport (OT) problem, which facilitates richer interactions between modalities. We model each view within the V-L space as a mass located at its embedding position:

\[=_{n=1}^{N+1}_{n}_{^{n}}\{^{i}=_{m=1}^{M+1}_{m}^{i}_{_{i}^{m}} \}_{i=1}^{C}.\] (7)Here, the importance weight of each view, derived from Eqs. (5) and (6), determines the mass allocation. The transportation cost between any two points (_e.g._, an image and a textual description) is quantified using the cosine distance between their embeddings, \(=1-(,)\), which serves as an intuitive measure of semantic similarity. The goal of optimal transport is to minimize the total cost of transporting mass from visual modality into textual modality. Specifically, the distance between the image view set \(\{X^{n}\}_{n=1}^{N+1}\) and \(i\)-th class description set \(\{t_{i}^{m}\}_{m=1}^{M+1}\) is redefined as an OT problem between \(\) and \(_{i}\), as formulated in Eq. (4). We employ Sinkhorn's Algorithm  to efficiently approximate the solution, denoted as \(}\). Consequently, the classification probability can be expressed as:

\[p_{}(t_{i} X)=_{i}/ )}{_{j=1}^{C}(_{j}/)},\] (8)

where \(=_{i}_{j}}_{ij}(1-)_{ ij}\). By employing the optimal transport framework, we ensure that semantically related views receive more attention, enhancing the accuracy and relevance of the classification process.

## 4 Experiments

### Zero-shot Image Tasks

Datasets.For zero-shot image tasks, we consider image classification and out-of-distribution (OOD) generalization. Our study encompasses 18 datasets that span a wide array of recognition tasks: ImageNet , Caltech101  and Caltech256  for generic object recognition, Oxford-Pets , StanfordCars , OxfordFlowers , Food101 , FGVCAircraft , Birdsnap  and CUB  for fine-grained classification, SUN397  for scene recognition, DTD  for texture classification, EuroSAT  for satellite recognition, and UCF101  for action recognition. Besides, four ImageNet variant datasets are involved to assess the model's capability for OOD generalization: ImageNet-A , ImageNetV2 , ImageNet-R  and ImageNet-Sketch .

Competitive methods.We mainly compare three distinct categories of approaches: 1) Prompt learning methods: These involve additional data to post-train visual or textual prompts, including CoOp , CoCoOp , MaPLe , PLOT++ , POMP . 2) Test-time prompt tuning methods: These optimize prompts during inference, such as TPT , DiffTPT , and PromptAlign . 3) Augment-based method: These use LLMs or diffusion models to augment inputs, including CuPL , VisDesc , WaffleCLIP , and SuS-X-SD .

    & & & & & & & & & & & & & & & & & & \\   & & & & & & & & & & & & & & & & & & & \\  CLIP  & ✗ & 66.74 & 67.44 & 44.27 & 88.25 & 65.48 & 65.13 & 93.35 & 83.65 & 62.99 & 23.67 & 42.01 & 42.80 & 82.50 & 54.90 & 63.06 \\  CoOp  & ✓ & 71.51 & 68.71 & 41.92 & 89.14 & 64.51 & 66.55 & 93.30 & 85.30 & 64.15 & 18.47 & 46.39 & 41.43 & 82.91 & 53.18 & 63.42 \\ CoCoOp  & ✓ & 71.02 & 71.88 & 45.73 & 90.14 & 65.32 & 68.21 & 94.43 & 86.06 & 76.36 & 29.24 & 45.37 & 43.75 & 85.99 & 65.69 & 65.26 \\ MapLe  & ✓ & 70.72 & 72.23 & 46.90 & 94.09 & 65.57 & 68.69 & 93.53 & 86.20 & 67.01 & 24.74 & 48.06 & 44.06 & 85.58 & 57.18 & 65.75 \\ PLOT++  & ✓ & 72.48 & 69.10 & 38.24 & 90.49 & 61.20 & 68.94 & 91.32 & 86.07 & 61.59 & 24.84 & 49.90 & 36.37 & 84.30 & 48.58 & 63.11 \\ POMP  & ✓ & 70.16 & 72.72 & 44.44 & 89.85 & 66.70 & 68.44 & 94.56 & 82.68 & 67.27 & 45.77 & 52.65 & 43.94 & 86.76 & 59.22 & 66.10 \\ ProVP-Ref  & ✓ & 71.14 & 71.62 & 45.97 & 91.58 & 65.29 & 67.72 & 93.79 & 86.17 & 66.29 & 24.51 & 51.95 & – – – & – & – & 66.91 \\  TPT  & ✓ & 68.98 & 68.98 & 47.75 & 87.97 & 68.67 & 68.04 & 94.16 & 84.67 & 65.90 & 24.78 & 42.44 & 44.46 & 85.71 & 56.97 & 64.80 \\ DiffTPT  & ✓ & 70.30 & 70.10 & 47.00 & 88.22 & 67.01 & 68.22 & 92.49 & **87.33** & 67.46 & 56.40 & 43.13 & – – – & – & – & 6.991 \\ PromptAlign  & ✓ & 71.44 & 72.39 & 47.24 & 90.76 & 85.00 & 69.47 & 94.01 & 86.65 & 67.54 & 24.80 & 47.86 & 45.28 & 86.05 & 57.90 & 66.42 \\ Self-TPPs  & ✓ & 72.96 & 71.79 & 49.35 & 91.26 & 68.81 & 69.50 & 94.71 & 85.41 & 68.18 & 27.57 & 51.91 & – – – & – & – & 68.31 \\  CuPL  & ✗ & 66.23 & 71.30 & 44.56 & 89.13 & 65.29 & 62.98 & 58.61 & 65.99 & 24.09 & 47.84 & 41.24 & 36.30 & 56.28 & 51.64 \\ ViNoDesc  & ✗ & 68.55 & 70.85 & 44.98 & 88.85 & 64.08 & 67.12 & 94.60 & 85.05 & 67.99 & 24.30 & 54.84 & 43.64 & 87.16 & 56.59 & 65.61 \\ WaffleCLIP  & ✗ & 68.1 & 72.35 & 45.21 & 89.95 & 63.57 & 67.19 & 94.02 & 86.68 & 67.23 & 25.39 & 55.07 & 43.92 & 87.04 & 57.17 & 65.97 \\ Sis-S-X-SD  & ✗ & 69.88 & 73.81 & 54.55 & 50.57 & 66.13 & 66.59 & 93.36 & 86.08 & 67.73 & 28.68 & 57.49 & 45.53 & 87.45 & 57.11 & 67.54 \\  AVT & ✗ & **71.32** & **75.07** & **65.56** & **92.53** & **69.93** & **72.51** & **95.54** & 85.54 & **70.58** & **29.22** & **58.61** & **48.75** & **88.84** & **60.20** & **69.39** \\   

Table 1: **Zero-shot image classification. We report top-1 accuracy (%) for each dataset. The “Train” column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot.**Implementation details.We implemented the AWT framework using the CLIP-B/16 model . Image augmentations include random resized cropping and flipping, and class descriptions are generated via GPT-3.5 . We set the number of augmented images \(N\) and descriptions \(M\) to 50 each. Dataset-level descriptions are provided in Appendix C. For both visual and textual modalities, we configured the importance distribution temperatures at \(_{v}=1/2\) and \(_{t}=1/2\). The optimal transport problem is approximated using Sinkhorn's Algorithm with an \(\) of 0.1 . All experiments are conducted on one NVIDIA A100-SXM4-80GB GPU.

Results.In Tab. 1, we compare AWT with three categories of CLIP adaptation methods: prompt learning, test-time prompt tuning, and existing augmentation-based methods. Remarkably, without additional training, AWT outperforms all existing methods by a significant margin, achieving state-of-the-art performance on 13 out of 14 datasets and surpassing the previous best results by an average accuracy of 2.05%. Further, the out-of-distribution (OOD) generalization capabilities of AWT are detailed in Table 2. Leveraging dataset-aware prompting and a dynamic weighting approach that adjusts in real-time during testing, AWT effectively manages complex scenarios encountered in OOD. Consequently, AWT stands out by delivering the highest performance across all four OOD datasets, surpassing the previous arts by an average accuracy improvement of 3.62%.

### Zero-shot Video Tasks

Setup.Here, we focus on the zero-shot video action recognition task, using three representative datasets: UCF101 , HMDB51 , and Kinetics-600 . For UCF101 and HMDB51, we adopt two evaluation protocols: **1) EP1:** test model on all 101 UCF classes and 51 HMDB classes [47; 117], and report the top-1 accuracy. **2) EP2:** evaluate the model using three official splits and averaging the results of each split. The average top-1 accuracy and standard deviation are reported. For Kinetics-600, the top-1 accuracy and standard deviation are reported on three validation sets split by Chen and Huang .

Implementation details.To model temporal dynamics, we follow Open-VCILP  to use neighbor-frame attention and fine-tune CLIP on Kinetics-400 . Note that three test subsets of Kinetics-600 have a disjoint class set compared to Kinetics-400. All AWT configurations are the same for zero-shot image tasks except for the visual augmentation, in which we directly use the different sampled temporal and cropped video frames.

Results.In Tab. 3, we present a comparison of our AWT with existing CLIP-based zero-shot video action recognition methods. Although AWT was not originally tailored for video tasks, it sets new records in this domain, outperforming the recent state-of-the-art method, FROSTER, by 1.6% and 2.4% on HMDB51, and by 1.3% on Kinetics-600. These results suggest that our AWT framework could be effectively extended to video understanding tasks.

### Few-shot Image Tasks

Setup.We assessed the few-shot transfer capabilities of our method across 11 datasets: ImageNet , Caltech101 , OxfordPets , StanfordCars , OxfordFlowers , Food101 , FGVCAircraft , SUN397 , DTD , EuroSAT , and UCF101 . We trained our model using 1, 2, 4, 8, and 16 shots. Results are averaged over three runs.

Implementation details.All AWT configurations are the same as zero-shot image tasks. In this task, we introduced a multi-modal Adapter for efficient learning, inserted after each Multi-Head

  
**Method** & **[N-A**  & **N-V2**  & **N-R**  & **IN-K**  & _OOD_ \\  CLIP  & 47.74 & 60.75 & 73.98 & 46.13 & 57.15 \\ TPT  & 54.77 & 63.45 & 77.06 & 47.94 & 60.81 \\ DiffTPT  & 55.68 & 65.10 & 75.00 & 46.80 & 60.65 \\ Curl.  & 50.72 & 63.27 & 77.05 & 49.02 & 60.02 \\ ViDes  & 49.07 & 61.80 & 75.13 & 47.97 & 58.49 \\ WaffleCLIP  & 50.78 & 62.54 & 77.49 & 49.10 & 59.98 \\ AVT & **60.33** & **65.15** & **80.64** & **51.60** & **64.43** \\   

Table 2: **Out of distribution generalization.**

  
**Method** & **UCF101** & **INDIND31** & **K000** \\   & EP1 & EP2 & EP2 & EP2 \\  ActionCLIP  & 77.74 & 77.53\(\)0.48 & 48.21\(\)1.5 & 62.5\(\)1.2 \\ X-CLIP  & - & 72.0\(\)2.3 & - & 44.6\(\)5.2 & 65.2\(\)0.4 \\ Jet at  & - & 69.3\(\)4.2 & - & 4.4\(\)3.2 & 58.8\(\)0.7 \\ Textually  & 79.6 & 49.8 & - & 68.9\(\)1.0 \\ AHM  & 79.0 & 99.4\(\)1.0 & 49.5 & 50.3\(\)0.3 & 66.7\(\)0.5 \\ SF-Adapter  & 77.9 & 76.4\(\)0.7 & 50.3 & 51.1\(\)0.6 & 60.2\(\)1.8 \\ ViL-CLIP  & - & 75.0\(\)0.6 & - & 48.6\(\)0.6 & 67.4\(\)0.5 \\ ViL-CLIP  & - & 76.8\(\)0.7 & - & 51.3\(\)0.6 & 71.2\(\)1.0 \\ Adaptment  & 80.5 & 80.3\(\)1.0 & 50.5 & 51.0\(\)0.8 & 67.0\(\)0.4 \\ Open-VCILP  & 83.5 & 83.4\(\)1.2 & 53.2 & 39.1\(\)2.7 & 79.0\(\)0.8 \\ FROSTER  & 85.0 & 84.8\(\)1.1 & 54.5 & 54.8\(\)1.3 & 74.8\(\)0.9 \\  AWT & **85.4** & **85.2\(\)**0.7 & **56.1** & **57.2\(\)**0.9 & **76.1\(\)**0.6 \\   

Table 3: **Zero-shot video action recognition.**

[MISSING_PAGE_FAIL:8]

importance) weighting. By incorporating our entropy-based weighting method, which accurately assesses the importance of each view, we again achieve substantial performance gains.

Number of augmentation views.We present the study on the augmented view quantities for both visual and textual sides in Tabs. 3(b) and 3(c), respectively. The results clearly demonstrate that performance tends to increase with the number of views. Our findings suggest that about 50 views per modality are sufficient to achieve decent performance. The number of augmentation views is crucial for AWT's effectiveness. Given that AWT is augmentation-driven, this correlation is intuitive. However, increasing the augmented view quantities can also lead to higher computational costs during inference, we also include an efficiency-performance trade-off analysis in Fig. 9.

LLM prompting strategy.We evaluated the effectiveness of our LLM prompting strategy, detailed in Tab. 3(d). Our method is compared with two established approaches: VisDesc  and CuPL . VisDesc uses a uniform prompt template across different datasets, while CuPL employs a tailored, dataset-specific manual prompting strategy, enriching context for LLMs. We developed a refined two-step process that enhances context comprehension through dataset-level descriptions and increases diversity by utilizing chain-of-thought queries. Our strategy consistently outperforms the existing methods in both evaluated tasks.

Temperature in weighting.We evaluated our entropy-based weighting method by conducting an ablation study on the softmax function's temperature parameter (see Eqs. (5) and (6)). A higher temperature creates a more uniform importance distribution. The findings for both modalities are presented in Tabs. 3(e) and 3(f), respectively. Our results reveal that a very high temperature (_e.g._, 100) leads to suboptimal performance, likely due to insufficient emphasis on contextually significant views. Conversely, lowering the temperature enhances focus on these important views, improving performance. Empirically, a temperature of \(1/2\) has been identified as optimal for both modalities. For a clearer understanding of our weighting strategy, visualizations are provided in Appendix A.

### Versatility Study

Our AWT is applicable to any VLM using dual encoders to map images and text into a joint space with appropriate distance metrics (_e.g._. cosine similarity). Therefore, it is crucial to assess AWT's effectiveness across various scenarios. We conduct evaluations with both ResNet  and ViT  architectures, explore AWT's scalability from ViT-B/32 to ViT-L/14@336, and assess its generalizability across three VLMs: ALIGN , SigLIP , and EVA-CLIP . We conduct experiments across 18 image datasets and present the results in Fig. 4. Our findings reveal that AWT consistently achieves performance gains in all tested scenarios, highlighting its broad applicability.

### Failure Case Analysis

Although AWT has shown success across various datasets and tasks, we identified certain limitations when applying it to the CIFAR datasets. As detailed in Tab. 5, AWT resulted in performance declines of 0.91% and 0.17% on two CIFAR datasets, respectively. To investigate this issue, we analyzed the images produced by the transformations used in our study. We discovered that with low-resolution images, such as 32 x 32 pixels, the random resized crop operation tends to overly blur the images,

Figure 4: **Versatility analysis of AWT. Average top-1 accuracy (%) on 18 image datasets is reported.**

obscuring the objects within them, as illustrated in Fig. 5. To address this, we integrated a diffusion model, specifically DALL-E 2 , as a substitute for traditional data augmentations. Fig. 5 shows examples of enhanced image views generated by DALL-E 2, which produce sharper images and offer a variety of perspectives. By incorporating this advanced technique into the AWT framework, we have significantly improved its performance. The updated benchmark results, presented in Tab. 5, demonstrate that AWT now consistently achieves performance gains over baselines.

## 5 Conclusion

In this paper, we have introduced the AWT (Augment, Weight, then Transport) framework, designed to enhance the transferability of pre-trained vision-language models (VLMs). Rather than using raw images and class names directly, our approach enriches the inputs by augmenting them with diverse visual perspectives and detailed class descriptions. We further develop an entropy-based weighting strategy to dynamically prioritize these augmented views and employ optimal transport to measure the cross-modal distance in the structured visual-language space. The AWT framework not only boosts the zero-shot performance of VLMs without the need for additional training but also facilitates few-shot transfer learning via an integrated multimodal adapter module. Our evaluations across four challenging tasks demonstrate that AWT significantly outperforms existing state-of-the-art methods.

Acknowledgments.This work is supported by the National Key R\(\&\)D Program of China (No. 2022ZD0160900), the National Natural Science Foundation of China (No. 62076119), the Fundamental Research Funds for the Central Universities (No. 020214380119), the Nanjing University-China Mobile Communications Group Co., Ltd. Joint Institute, and the Collaborative Innovation Center of Novel Software Technology and Industrialization.

Figure 5: **Comparison of image augmentation techniques** on low-resolution images. We present images from the CIFAR-10/100 datasets, where each image is 32 \(\) 32 pixels. The comparison includes images generated by traditional image transformations and DALL-E 2.

   Method & Image augmentation & CIFAR10  & CIFAR100  \\  CLIP  & - & 90.16 (_baseline_) & 67.78 (_baseline_) \\ AWT & Random resized crop and flip & 89.25 (0.91) \(\) & 67.61 (0.17) \(\) \\ AWT & DALL-E 2  & 92.30 (2.14) \(\) & 68.69 (0.91) \(\) \\   

Table 5: **Failure case analysis.** We focus on low-resolution datasets CIFAR-10 and CIFAR-100 and evaluate the effectiveness of AWT when equipped with different image augmentation techniques.