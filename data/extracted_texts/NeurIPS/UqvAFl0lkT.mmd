# EReLELA: Exploration in Reinforcement Learning

via Emergent Language Abstractions

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Instruction-following from prompts in Natural Languages (NLs) is an important benchmark for Human-AI collaboration. Training Embodied AI agents for instruction-following with Reinforcement Learning (RL) poses a strong exploration challenge. Previous works have shown that NL-based state abstractions can help address the exploitation versus exploration trade-off in RL. However, NLs descriptions are not always readily available and are expensive to collect. We therefore propose to use the Emergent Communication paradigm, where artificial agents are free to learn an emergent language (EL) via referential games, to bridge this gap. ELs constitute cheap and readily-available abstractions, as they are the result of an unsupervised learning approach. In this paper, we investigate (i) how EL-based state abstractions compare to NL-based ones for RL in hard-exploration, procedurally-generated environments, and (ii) how properties of the referential games used to learn ELs impact the quality of the RL exploration and learning. Results indicate that the EL-guided agent, namely EReLELA, achieves similar performance as its NL-based counterparts without its limitations. Our work shows that Embodied RL agents can leverage unsupervised emergent abstractions to greatly improve their exploration skills in sparse reward settings, thus opening new research avenues between Embodied AI and Emergent Communication.

## 1 Introduction

Natural Languages (NLs) have some properties, such as compositionality and recursive syntax, that allow us to talk about infinite meanings while only using a finite number of words (or even letters, or phonemes...). In other words, it enables us to be as expressive as one might needs. However, it may be interesting sometimes to use language to abstract away from the details and only focus on the essence of a specific experience, or a specific sensory stimulus. Thus, even though NLs can sometimes be used with high expressiveness, they also can work as abstractions. For instance, using a unique utterance to refer to a lot of semantically-similar but (visually) different situations, such as the one presented in Figure 1 where the utterance 'one can see a purple key and a green ball' can refer to many of the first-person perspective of the embodied agent, irrespective of the actual perspective under which each object is seen.

Tam et al.  referred to that aspect as compacting/clustering a state/observation space, which is in effect segmenting it into a set of less-detailed but more-meaningful sub-spaces. We employ the term meaningful with respect the task that the embodied agent is possibly trained for. For instance, if the task consists of picking and placing objects, then it is meaningful for utterances to contain information about objects and places, but not so much to contain information about other agents in the environment, if any. In this paradigm, Tam et al.  and Mu et al.  provided some arguments towards the compacting/clustering assumption of NLs, as they used NLs oracle to build an abstractionover a 3D and 2D environments. They relied upon state-of-the-art exploration algorithms, such as Random Network Distillation (RND - Burda et al. ) and Never-Give-Up (NGU - Badia et al. ), which can be difficult to deploy.

Thus, in this work, we aim to simplify the process of using languages as abstractions and address the limitation of using NLs, as they are expensive to harvest and not necessarily the most meaningful abstraction for any given task. Indeed, instead of state-of-the-art exploration algorithms, we show that simpler count-based approaches combined with language abstraction can be leveraged for hard-exploration tasks. And, in order to remove the reliance on NLs, we look at the field of Emergent Communication (EC) [41; 7] which have shown that artificial languages, that we refer to as emergent languages (ELs), can emerge through unsupervised learning algorithms, such as Referential Games and variants , with structure and properties similar to NLs. Our experimental evidences show that ELs, acquired over an embodied agent's observations in an online fashion and in parallel of its training, can be leveraged for hard-exploration tasks. We investigate what are the properties of NLs and ELs in terms of their abstraction building abilities by proposing a novel metric entitled Compactness Ambiguity Metric (CAM). Measures show that ELs abstractions are aligned but not similar to NLs in terms of the abstractions they perform, as the Emergent Communication context successfully picks up on the meaningful features of the environment. Indeed, EReELEA's abstractions reflect colors in the _MultiRoom-N7-S4_ environment which only features coloured, unlocked doors, but no distracting objects, or shapes in the _KeyCorridor-S3-R2_ environment where it is important to pickup a relevant key, among other distracingly-shaped objects, and to open the locked door-shaped object.

We continue by reviewing EC and RL backgrounds and notations in Section 2. After detailing our method in Section 3, we present experimental results on procedurally-generated, hard-exploration task from the MiniGrid  benchmarks in Section 4. Finally, we discuss in Section 5 the results presented in light of some related works and highlight possible future works.

## 2 Background & Notation

We provide details on our Reinforcement Learning (RL) settings and count-based exploration methods in Section 2.1.Then, we review Emergent Communication in Section 2.2.

### Exploration vs Exploitation in Reinforcement Learning

An RL agent interacts with an environment in order to learn a mapping from states to actions that maximises its reward signal. Initially, both the reward signal and the dynamics of the environment, i.e. the impact that the agent actions may have on the environment, are unknown to the agent. It must explore the environment and gather information, but, all the while it is exploring, it cannot exploit the best strategy that it has found so far to maximise the currently-known reward signal. This dilemma is known as the Exploration-vs-Exploitation trade-off of RL.This dilemma is only the start of the rabbit hole, as it can even get worse. Indeed, in sparse reward environments, the reward signal is mainly zero most of the time. This context makes it very difficult for RL agents to learn anything, because RL algorithms derive feedback (i.e. gradients to update their parameters) from the reward signal that they observe from the environment.It is usually referred to as extrinsic, in order to differentiate it from an intrinsic reward signal. As the extrinsic reward is mostly zero, RL agents must exploit another signal to derive information about the currently-unknown environment. This other signal can be found in relation to the observation/state space, as RL agents can learn to seek novelty or surprise around the observation/state space and attempt to manipulate it efficiently by choosing relevant actions. Focusing on this novelty, RL agents can harvest an intrinsic reward signal, in the sense that RL agents are building it and giving it to themself. Note that this intrinsic reward signal is very different from the

Figure 1: Top-view visualization of a wall-free 3D environment with different objects (e.g. red and blue cubes, purple and green keys, and green ball) showing the trajectory (from blue to red dots) of a randomly-walking embodied agent, with first-person perspectives highlighted at relevant timesteps using colored cones - showing the agentâ€™s viewpoint direction when a new utterance is used to describe the first-person perspective using an oracle speaking in NL.

extrinsic reward signal, because it does not inform about the task that RL agents need to perform in the environment. Ideally, though, it provides a graded and dense signal that the RL agent can use to start learning anything about the environment. This is inspired by intrinsic motivation in psychology . Exploration driven by curiosity/novelty might be an important way for children to grow and learn. Here, we focus on novelty, but the intrinsic rewards could be correlated with e.g. impact , surprise  or familiarity of the state. The intrinsic reward signal is only a proxy for RL agents to start to make progress into learning about the environment and eventually, hopefully encounter some non-zero extrinsic reward signal along the way. It provides a denser reward signal that can guide RL agents into learning internal representations about the environment's dynamic so that, whenever some extrinsic reward are encountered along the way, then they can efficiently bind their previously-learned representations to those recently-encountered extrinsic rewards.

Formally, we study a single agent in a Markov Decision Process (MDP) defined by the tuple \((,,T,,)\), referring to, respectively, the set of states, the set of actions, the transition function \(T: P()\) which provides the probability distribution of the next state given a current state and action, the reward function \(: r\), and the discount factor \(\). The agent is modelled with a stochastic policy \(: P()\) from which actions are sampled at every time step of an episode of finite time horizon \(T\). The agent's goal is to learn a policy which maximises its discounted expected return at time \(t\), defined in equation 1. We further define \(=_{}^{}+_{} ^{}\) as the weighted sum of the extrinsic and intrinsic reward functions, respectively, \(^{},^{}\), with weights \(_{},_{int}\). Indeed, while the extrinsic reward is provided by the environment, we assume that for any tuple \((s_{t},a_{t},s_{t+1})\) we can compute an intrinsic reward.

Stanton and Clune  identifies two categories of exploration strategies, to wit _across-training_, where novelty of states, for instance, is evaluated in relation to all prior training RL episodes, and _intra-life_, where it is evaluated solely in relation of the current RL episode. And, historically, we can identify two types of intrinsic motivation exploration depending on how the intrinsic reward is computed, either relying on count-based or prediction-based methods. Prediction-based methods fit into the _across-training_ category and count-based methods can actually fit in both categories but they have mainly been instantiated in the literature as _across-training_ methods after extension of _intra-life_ core mechanisms. As our proposed architecture EReLELA fit into the category of count-based methods, we detail them further.In the context of an intrinsic reward signal correlated with surprise, then it is necessary to quantify how much of surprise each observation/state provides. Intuitively, we can count how many times a given observation/state has been encountered and derive from that count our intrinsic reward. The reward would guide the RL agent to prefer rarely visited/observed states compared to common states. This is referred to as the count-based exploration method. Count-based exploration method were originally only applicable to tabular RL where the state space is discrete and it is easy to compare states together. When dealing with continuous or high-dimensional state spaces, such method is not practical. Thus, Bellemare et al.  proposed (and extended in Ostrovski et al. ) a pseudo-count approach which was derived from increasingly more efficient density models, and they showed success in applying it to image-based exploration environments from Atari 2600 benchmark, such as _Montezuma's Revenge_, _Private Eye_, and _Venture_. We provide more relevant details in Appendix B.

Nevertheless, hard-exploration task involving procedurally-generated environments are notoriously difficult for count-based exploration methods. Indeed, when states are procedurally-generated, almost all states will be showing 'novel' features, most times irrespectively of whether it is relevant to the task or not. It will follow that their state (pseudo-)count will always be low and therefore the RL agent will get feedback towards reaching all of them indefinitely, but if every state is 'novel' then there is nothing to guide the agent in any specific direction that would entail to good exploration.

### Emergent Communication

Emergent Communication is at the interface of language grounding and language emergence. While language emergence raises the question of how to make artificial languages emerge, possibly with similar properties to NLs, such as compositionality [2; 45; 55; 24], language grounding is concerned with the ability to ground the meaning of (natural) language utterances into some sensory processes,e.g. the visual modality. On one hand, the compositionality of ELs has been shown to further the learnability of said languages [38; 57; 8; 45] and, on the other hand, the compositionality of NLs promises to increase the generalisation ability of the artificial agent that would be able to rely on them as a grounding signal, as it has been found to produce learned representations that generalise, when measured in terms of the data-efficiency of subsequent transfer and/or curriculum learning [27; 49; 50; 33]. Yet, emerging languages are far from being 'natural-like' protolanguages [40; 10; 11], and the questions of how to constraint them to a specific semantic or a specific syntax remain open problems. Nevertheless, some sufficient conditions can be found to further the emergence of compositional languages and generalising learned representations [40; 43; 17; 5; 24; 39; 12; 21].

The backbone of the field rests on games that emphasise the functionality of languages, namely, the ability to efficiently communicate and coordinate between agents. The first instance of such an environment is the _Signaling Game_ or _Referential Game (RG)_ by Lewis , where a speaker agent is asked to send a message to the listener agent, based on the _state/stimulus_ of the world that it observed. The listener agent then acts upon the observation of the message by choosing one of the _actions_ available to it in order to perform the 'best' _action_ given the observed _state_ depending on the notion of 'best' _action_ being defined by the interests common to both players. In RGs, typically, the listener action is to discriminate between a target stimulus, observed by the speaker and prompting its message generation, and some other distractor stimuli. Distractor stimuli are selected using a distractor sampling scheme, which has been shown to impact the resulting EL [42; 43]. The listener must discriminate correctly while relying solely on the speaker's message. The latter defined the discriminative variant, as opposed to the generative variant where the listener agent must reconstruct/generate the whole target stimulus (usually played with symbolic stimuli). Visual (discriminative) RGs have been shown to be well-suited for unsupervised representation learning, either by competing with state-of-the-art self-supervised learning approaches on downstream classification tasks , or because they have been found to further some forms of disentanglement [28; 35; 14; 46] in learned representations [65; 18]. Such properties can enable "better up-stream performance", greater sample-efficiency, and some form of (systematic) generalization [48; 26; 59]. Thus, this paper aims to investigate visual discriminative RGs as auxiliary tasks for RL agents.

## 3 Method

In this section, following the acknowledgement of a gap in terms of evaluating the abstractions that different languages perform over different state/observation space, we start by introducing in Section 3.1 our Compactness Ambiguity Metric (CAM) that attempts to fill in that gap.Then, in Section 3.2, we present the EReLELA architecture that leverages EL abstractions in an _intra-life_ count-based exploration scheme for RL agents.

### Compactness Ambiguity Metric

In order to measure qualities related to the kind of abstraction that a language performs over stimuli, we propose to rely on the temporal aspects of embodied agent's trajectories in a given environment. We build over the following intuition, represented in Figure 2: we consider two possible languages grounded into the first-person viewpoint of an embodied agent situated in a 3D environment populated with objects of different shapes and colors. On one hand, we have the Blue language, which is only concerned about blue objects and its utterances only describe that they are of color blue when they are, while, on the other hand, we have the Color language, which is describing the color of all visible objects. Inherently, those two languages expose different semantics about the world, and therefore they perform different abstractions. We aim to build a metric that captures how different the semantics they expose are. To do so, we propose to arrange their respective utterances when prompted with the very same agent's trajectories into different timespan-focused buckets towards building an histogram. These timespan-focused buckets reflect \((u)\) the number of consecutive timesteps \((t_{k})_{k[k_{},k_{}+(u)]}\) for which a specific utterance \(u\) would be uttered by a speaker of each language when prompted with the stimuli in those timesteps. We will refer to these are compactness counts. For instance the Blue language's utterance 'I see a blue object' at the beginning of the trajectory occupies twice as more consecutive timesteps as the same utterance coming from a Color language speaker (or, its compactness count in the Blue language is twice its compactness count in the Color language). Therefore, in the case of the Blue language, this utterance would increment the medium-length bucket, while it would increment the short-length bucket in the case of Color language histogram. It ensuesthat the histograms of timespan-focused buckets captures semantics exposed by each language, and we will therefore refer to the resulting histogram as the histogram of semantic-clustering timespans. As the toy example highlights, the histograms of semantic-clustering timespans will differ from one language to another depending on the semantics each language expose or, in other words, depending on the abstractions they perform. This is the first intuition on which the Compactness Ambiguity

Formally, we define \(\) as the set of all possible languages over vocabulary \(V\) with maximum sentence length \(L\), such that for any language \(l\) we denote \(_{l}: l\) as a speaker agent or oracle that maps any state/observation \(s\) to a caption or utterance \(u l\). Thus, we can now consider \(N\) buckets whose related timespans \((T_{i})_{i[1,N]}\) are sampled relative to the maximal length \(T\) of a trajectory in the given environment, and the histogram of semantic-clustering timespans that they induce.

Then, the other intuition on which the metric is built is made evident by considering the expressivity or, its inverse, the ambiguity, of a given language \(l\), defined as \(_{l}=}{\#}\) with \(\#\) the set cardinality operator. Dealing with stimuli being states/observations of a (randomly walking) embodied agent, gathered into a dataset \(\), the number of unique stimuli cannot be estimated reliably when dealing with complex, continuous stimuli. Thus, the best we can rely on is a measure of relative expressivity over a dataset, that we define as \(_{l}()=}{\#}=_{l}()}{||}\), with \(||\) being the size operator over collections (differing from sets in the sense that they allow duplicates). In those terms, the relative expressivity is maximised if and only if (i) \(\#=||\), and (ii) \(_{l}\) is a bijection over \(\). On the other hand, considering that a language \(l\) performs an abstraction over \(\) is tantamount to some stimuli \((s,s^{})^{2}\) sharing the same utterance \(u=_{l}(s)=_{l}(s^{})\), i.e. consisting of a hash collision, meaning that the mapping \(_{l}\) from \(\) to \(l\) would not be injective (and therefore not bijective). Incidentally, the relative expressivity \(_{l}()\) cannot be maximised, leading to the language \(l\) being ambiguous over \(\). In this consideration, we can see that the ambiguity of a language (over a given dataset) can be impacted by either the extent to which an abstraction is performed (meaning that most colliding states/observations are of consecutive timesteps) or the extent to which the dataset is redundant (meaning \(\#<<||\)). Therefore it is important that our proposed Compactness Ambiguity Metric is built to focus on sources of ambiguities that are the result of consecutive-timesteps states colliding, more than sources of ambiguities that are the result of redundancy in the given dataset.

\[ i[1,N],\ T_{i}=1+_{i} _{l}()\] (2) \[ i[1,N],\ T^{}_{i}=1+_{i} T\] (3) \[ i[1,N],\ CA()_{T_{i}}=_{u l}}^{>T_{i}}(u)}{\#_{}(u)}\] (4)

\(\), we define the buckets' related timespans in relation to the relative ambiguity \(_{l}()=_{l}()}=|}{\#_{l}()}\), as shown in equation 2 with \(_{i}\ s.t.\ (j,k),\ j<k_{j}<_{k}\), and \(\) being the ceiling operator. This is in lieu of defining them in relation to the maximal length \(T\) of an agent's trajectory in the environment, as shown in equation 3. More specifically, let us first acknowledge decomposition of relative ambiguity over two independent quantities, one for each of its sources being either abstraction or redundancy, such that \(_{l}=_{l}^{}+ _{l}^{}\). Then note that the relative ambiguity is equal to the mean number of consecutive timesteps, or compactness count, for which a given utterance would be used when the unique utterances are uniformly distributed over the dataset \(\). Thus, in the metric, we propose to absorb variations of relative ambiguity due to redundancy by changing the metric's bucket setup, from Equation 3 to Equation 2. Doing so, it is true that the metric's bucket setup will also vary when the abstraction-induced relative ambiguity varies, we remark that the metric would not build invariance to this source of relative ambiguity since it is taken into accounts when sorting out the different unique utterances into their relevant bucket, based

Figure 2: Toy example illustration of how different languages expose different semantics over the same observed trajectory of stimuli, and that the discrepancy in exposed semantics can be captured by an histogram of semantic-clustering timespans.

on the maximal number of consecutive timesteps in which they occur, as shown in equation 4 with \(_{}:l 2^{}\) is the compactness count function that associates each utterances \(u l\) to its related set of compactness counts over dataset \(\), i.e. the set that contains numbers of consecutive timesteps for which \(u l\) was uttered by \(_{l}\), each time it was uttered without being uttered in the previous timestep. For instance, if we consider \(u l\) such that \(_{l}^{-1}(u)=\{s_{t_{1}},s_{t_{1}+1},s_{t_{1}+2},s_{t_{2}}\}\), with \((t_{1},t_{2})[0,T]^{2}\) such that \(t_{2}>t_{1}+3\), then \(_{}(u)=\{3,1\}\) because \(u\) occurred \(2\) non-consecutive times over \(\) and those occurrences lasted for, respectively, \(3\) and \(1\) consecutive timesteps, i.e. for compactness counts of \(3\) and \(1\). The superscript \( T_{i}\) in \(_{}^{ T_{i}}\) implies filtering of the output set based on compactness counts being greater or equal to \(T_{i}\). We provide in appendix C an analysis of the sensitivity of our proposed metric, and in appendix E.1 experimental results that ascertain the internal validity of our proposed metric, we consider a 3D room environment of MiniWorld , filled with 5 different, randomly-placed objects, as shown in a top-view perspective in Figure 1.

### EReLELA Architecture

This section details the EReLELA architecture, which stands for Exploration in Reinforcement Learning via Emergent Language Abstractions. As a count-based exploration method, we present here its _intra-life_ core mechanism, where intrinsic reward signals are derived from novelty at the level of language utterances describing the current observation/state. It relies on a hashing-like function (cf. Appendix B), which takes the form of the speaker agent of a referential game (RG), to turn continuous and high-dimensional observations/states into discrete, variable-length sequences of tokens. EReLELA is built around an RL agent augmented with an unsupervised auxiliary task, a (discriminative, here, or generative) RG, following the UNREAL architecture from Jaderberg et al. , as shown in Figure 3.

We train the RG agents in a descriptive, discriminative RG with \(K=256\) distractors, every \(T_{RG}=32768\) gathered RL observations, on a dataset \(_{RG}\) consisting of the most recent \(|_{RG}|=8192\) observations, among which \(2048\) are held-out for validation/testing-purpose, over a maximum of \(N_{RG-epoch}=32\) epochs or until they reach a validation/testing RG accuracy greater than a given threshold \(acc_{RG-thresh}=90\%\). Our preliminary experiments in Appendices D.1 and D.2 show, respectively, that increasing the RG accuracy threshold \(acc_{RG-thresh}\) increases the sample-efficiency of the EL-guided RL agent, and that the number of distractors \(K\) is critical (even more so than the distractor sampling scheme - which we set to be uniform unless specified otherwise), and that it correlates positively with the performance of the RL agent. More specific details about the RG and its agents' architectures can be found in Appendices F and G and our open-source implementation1.

## 4 Experiments

**Agents** Our RL agent is optimized using the R2D2 algorithm from  with the Adam optimizer Kingma and Ba . Importantly, as it aims to maximise the weighted sum of the extrinsic and intrinsic reward functions following equation 1, throughout this paper, we use \(_{int}=0.1\) and \(_{ext}=10.0\) in order to make sure that the agent pursues the external goal once the exploration of the environment has highlighted it. Further details about the RL agent can be found in Appendix F. For our RG agents, we consider optimization using either the Impatient-Only or the LazImpa loss function from Rita et al. , but the latter is adapted to the context of a Straight-Through Gumbel-Softmax (STGS) communication channel [25; 21], as detailed in Appendix G.1, and we refer to it as STGS-LazImpa. Indeed, the LazImpa loss function has been shown to induce Zipf's Law of

Figure 3: EReLELA architecture consisting of a stimulus/observation encoder shared between an RL agent and the speaker and listener agents of a RG, framed as an unsupervised auxiliary task . The language utterances outputted by the RG speaker agent are used in a count-based exploration method to generate intrinsic rewards for the RL agent.

Abbreviation (ZLA) in the ELs. Thus, we can investigate in the following experiments how does **structural** similarity between NLs and ELs affect the kind of abstractions they perform, as well as the resulting RL agent. Further details about the RG in EReLELA can be found in Appendix G.

**Environments.** After having considered in our preliminary experiments (cf. Appendix E.4) the 2D environment _MultiRoom-N7-S4_, we propose below experiments in the more challenging _KeyCorridor-S3-R2_ environment from MiniGrid . Indeed, it involves complex object manipulations, such as (distractors) object pickup/drop and door unlocking, which requires first picking up the relevantly-colored key object.

**Natural Language Oracles.** Our implementation of a NL oracle is simply describing the visible objects in terms of their colour and shape attributes, from left to right on the agent's perspective, whilst also taking into account object occlusions. For instance, around the end of the trajectory presented in Figure 1, the green key would be occluded by the blue cube, therefore the NL oracle would provide the description 'blue cube red cube' alone. We also implement colour-specific and shape-specific language oracles, which consists of filtering out from the NL oracle's utterance the information that each of those language abstract away, i.e. removing any shape-related word in the case of the colour-specific language, and vice-versa.

**Hypotheses.** We seek to validate the following hypotheses. Firstly, we consider whether NL abstractions can help for hard-exploration in RL with a simple count-based approach (**H1**), and refer to the relevant agent using NL abstractions to compute intrinsic rewards as NLA. We carry on with the hypothesis that ELs can be used similarly (**H2**), and we investigate to what extent do ELs compare to NLs in terms of abstraction. We would expect ELs to perform more meaningful abstractions than NLs (**H3**), in the sense that their abstractions would be more aligned with the relevant features of a given environment.

**Evaluation.** We employ \(3\) random seeds for each agent. We evaluate (H1) and (H2) using both the success rate and the manipulation count, in the hard-exploration task of _KeyCorridor-S3-R2_. The manipulation count is a per-episode counter incremented each time an object is successfully picked up or dropped by the RL agent over the course of each episode. In order to evaluate both (H3.1) and (H3.2), we use the CAM to measure the kind of abstractions performed by ELs, and compare those measures with those of the oracles' languages that we previously studied. We report the CAM distances between ELs and the NL, Color language, and Shape language oracles, which is computed as an euclidean distance in \(^{6}\) by considering the \(N=6\) CAM scores for each timespans/thresholds as vectors in this space. As we remarked that an agent's skillfullness at the task would induce very different trajectories (e.g. in _MultiRoom-N7-S4_, staying in the first room and only ever seeing the first door, for an unskillfull agent, as opposed to visiting multiple rooms and observing multiple colored-doors, for a skillfull agent), we compute the oracle languages CAM scores on the exact same trajectories than used to compute each EL's CAM scores.

Figure 4: Success rate learning curve (left), computed as running averages over \(1024\) episodes each time (i.e. \(32\) in parallel, as there are \(32\) actors, over \(32\) running average steps), and barplot (right), along with per-episode manipulation count (middle) in _KeyCorridor-S3-R2_ from MiniGrid , for different agents: (i) the _Natural Language Abstraction_ agent (NLA) refers to using the NL oracle to compute intrinsic reward, (ii) the _STGS-LazImpa-\(_{1}\)-\(_{2}\) EReLELA_ agents with \(_{1}=5\) (agnostic only) or \(_{1}=10\) (shared and agnostic), and \(_{2}=1\), (iii) the _Impatient-Only EReLELA_ agents (shared and agnostic), and (iv) the _RANDOM_ agent referring to an ablated version of EReLELA without RG training.

### EReELLA learns Systematic Navigational & Manipulative Exploration Skills from Scratch

We present in Figure 4 both the success rate of the different agents (as line plot through learning -left-, or barplot at the end of learning -right-), and the per-episode manipulation count (middle). From the fact that both the NLA and EReELLA agent performance converges higher or close to \(80\%\) of success rate (except the \(\)-\(\)-10-1), we validate hypotheses (H1) and (H2), meaning that it is possible to learn systematic exploration skills from both NL or EL abstractions with a simple count-based exploration method, in 2D environments (cf. further evidence in Appendix D.1 with the _MultiRoom-S7-R4_ environment). This result puts into perspective the directions of previous literature designing complex exploration algorithms [9; 1].

The sample-efficiency is better for NLA than it is for most EL-based agents, except the Agnostic \(\)-\(\)-10-1 agent, possibly because of the fact that ELs are learned online in parallel of the RL training, as opposed to the case of NLA which makes use of a ready-to-use oracle. Concerning the most-sample-efficient Agnostic STGS-\(\)-10-1 agent, we interpret its success to be the result of benefiting from both a language structure ascribing to the ZLA and a performed abstraction that is more optimal than NL oracle's ones, because it is learned from the stimuli themselves.

Among the different Agnostic EReELLA agents, the final performance are not statistically-significantly distinguishable, meaning that learning systematic exploration skills with EReELLA can be done with some robustness to the anecdotical differences in qualities of the different ELs. On the other hand, the shared/non-agnostic EReELLA agents's performance are statistically-significantly distinguishable from each other and from their agnostic versions, achieving lower performance or even failing to learn anything in the case of the \(\)-\(\)-10-1 EReELLA agent. We interpret these results as being caused by some kind of interference between the RG training and the RL training, preventing any valuable representations from being learned in the shared observation encoder (cf. Figure 3), thus warranting the need for future works to investigate whether a synergy can be achieved.

Finally, acknowledging the RANDOM agent, which is the ablated version of EReELLA without RG training, enabling still a median performance around \(70\%\) of success rate, we recall the Random Network Distillation approach from Burda et al. , for they both share a randomly initialised networked from which feedback is harvested to guide an RL agent. Thus, even more so in a 2D environment, this ablated version is not to be confused with a lower-bound baseline but rather an interesting ablation that enables us to show the impact of the RG training, increasing the sample-efficiency and final performance of the resulting RL agent.

### EReELLA learns Meaningful Abstractions

Regarding hypothesis (H3), we show in Figure 5 the CAM distances between the different agent's ELs and the natural, colour-specific, and shape-specific languages. We recall that in the _KeyCorridor-S3-R2_ environment, the most important feature is object shape as the agent must pickup a key from

Figure 5: CAM distances to NL (left), Color language (middle), and Shape language (right), for ELs brought about in _KeyCorridor-S3-R2_ from MiniGrid , with different agents: (i) the _STGS-LazImpa-\(_{1}\)-\(_{2}\)_EReELLA_ agents with \(_{1}=5\) (agnostic only) or \(_{1}=10\) (shared and agnostic), and \(_{2}=1\), (ii) the _Impatient-Only EReELLA_ agents (shared and agnostic), and (iii) the _RANDOM_ agent referring to an ablated version of EReELLA without RG training.

all other distractor objects and then use it to unlock the locked door. Thus, as we observe that most ELs' abstractions are closer to the shape-specific language than the others, we conclude that EReELELA learns meaningful abstractions, thus validating hypothesis (H3) (cf. Appendix E.3 for further evidence in the context of _MultiRoom-N7-S4_). Further, we remark that the failing STGS-LazImpa-10-1 EReELELA agent is indeed failing because its EL's abstractions are not highlighting shape features. When considering the shared/non-agnostic agents only, we can see that they require many more RG training epochs, meaning that they reach the accuracy threshold less often than their agnostic counterparts. We take this as further evidence for our interpretation that there might be interference between the RL objective and the RG objective.

We note that abstractions from ELs brought about in the contexts of the _Agnostic STGS-LazImpa_ agents and the _Agnostic Impatient-Only_ agents are the closest to that of the shape-specific language ones, and their evolution throughout learning are similar. Yet, the _Agnostic STGS-LazImpa_ agents achieves statistically-significantly better sample-efficiency (cf. Figure 7). We interpret this as being caused by the ZLA structure of the ELs in the context of the _Agnostic STGS-LazImpa_ agents, thus showing that NL-like structure is impacting the kind of abstractions being performed in ways that are yet to be unveiled by future works.

**Limitations.** With regards to the external validity of EReELLA, we acknowledge that the current work only addresses a 2D environment and therefore, despite being procedurally-generated, it presents less challenges to count-based exploration methods than in the context of 3D procedurally-generated environments. Although we provide some results in Appendix E.3 showing that EReELLA is able to learn meaningful abstractions in a 3D environment, we leave it to future work to ascertain the external validity of EReELELA by testing it in a procedurally-generated 3D environment that pose purely-navigational or navigational and manipulative exploration challenges.

## 5 Discussion

We investigated the compacting/clustering hypothesis for ELs, questioning how do NLs and ELs compare in terms of the abstractions they perform over state/observation spaces. To answer this question, we proposed a novel metric entitled Compactness Ambiguity Metric (CAM), for which we analysed the sensitivity and performed internal validation.

We then leveraged this metric to show that ELs abstractions are more meaningful than NLs ones, as the Emergent Communication context successfully picks up on the meaningful features of the environment.

Then, we have proposed the **Exploration in Reinforcement Learning via Emergent Languages Abstractions (EReELLA)** agent, which leverages ELs abstractions to generate intrinsic motivation rewards for an RL agent to learn systematic exploration skills. Our experimental evidences showed the performance of EReELLA in procedurally-generated, hard-exploration 2D environments from MiniGrid .

Moreover, in the parallel optimization of the RG players, we evidenced how the STGS-LazImpa loss function, which induces EL to abide by ZLA like most NLs, impacts the kind of abstraction being performed compared to baseline Impatient-Only loss function, and yields better sample-efficiency for the RL agent training.

Future work ought to investigate different loss functions and distractor sampling schemes, especially if playing discriminative RGs like here, as we expect, for instance, that sampling distractors more contrastively, e.g. like in Choi et al. , may induce the emergence of more complete, and therefore more meaningful ELs. By complete, we mean that the ELs would still be abstracting away details but also capturing more information about the underlying structure of the stimuli space, e.g. capturing both colour- and shape-related information of visible objects. In this light, we would also expect generative RGs to propose a possibly different picture that is worth investigating.

While we leave it to subsequent work to investigate the external validity of EReELLA and whether it transfers similarly well to 3D environments, our results open the door to a new application of the principles of Emergent Communication and ELs towards influencing/shaping the learned representations and behaviours of Embodied AI agents trained with RL.