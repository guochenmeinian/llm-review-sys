# Riemannian stochastic optimization methods

avoid strict saddle points

 Ya-Ping Hsieh, Mohammad Reza Karimi, Andreas Krause

ETH Zurich

{yaping.hsieh, mkarimi}@inf.ethz.ch, krausea@ethz.ch

&Panayotis Mertikopoulos

Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG 38000 France

panayotis.mertikopoulos@imag.fr

###### Abstract

Many modern machine learning applications - from online principal component analysis to covariance matrix identification and dictionary learning - can be formulated as minimization problems on Riemannian manifolds, typically solved with a Riemannian stochastic gradient method (or some variant thereof). However, in many cases of interest, the resulting minimization problem is _not_ geodesically convex, so the convergence of the chosen solver to a desirable solution - i.e., a local minimizer - is by no means guaranteed. In this paper, we study precisely this question, that is, whether stochastic Riemannian optimization algorithms are guaranteed to avoid saddle points with probability 1. For generality, we study a family of retraction-based methods which, in addition to having a potentially much lower per-iteration cost relative to Riemannian gradient descent, include other widely used algorithms, such as natural policy gradient methods and mirror descent in ordinary convex spaces. In this general setting, we show that, under mild assumptions for the ambient manifold and the oracle providing gradient information, the policies under study avoid strict saddle points / submanifolds with probability 1, from any initial condition. This result provides an important sanity check for the use of gradient methods on manifolds as it shows that, almost always, the limit state of a stochastic Riemannian algorithm can only be a local minimizer.

## 1 Introduction

Modern machine learning systems have achieved remarkable success in the efficient optimization of highly non-convex functions using straightforward _Euclidean_ techniques like stochastic gradient descent. A widely accepted hypothesis to explain this phenomenon is that, when the learning system under study - e.g., a neural network - is sufficiently expressive, local minimizers are essentially as good as global ones ; by this token, a training algorithm can attain satisfactory performance by simply evading _saddle points_ of the model's loss surface.

This observation has sparked a far-reaching research thread examining the behavior of various algorithms around saddle points in non-convex functions. Informally, these studies aim to tackle two fundamental questions:

* When does a given scheme, like stochastic gradient descent, avoid saddle points?
* Can we _augment_ a given scheme so that it efficiently escapes saddle points?

Of the above questions, Q1 focuses on _explaining_ the empirical success of commonly used schemes, while the resolution of Q2 usually revolves around proposing new schemes with desirable escape guarantees. These complementary perspectives have been extensively studied over the past decade,leading to a fairly complete understanding of how and when a Euclidean (stochastic) algorithm escapes saddle points, cf. [6; 28; 29; 41; 42; 46; 47; 53; 54] and references therein.

In parallel to the above, the recent surge of interest in Riemannian optimization has prompted a closer examination of _Riemannian_ methods, thereby motivating an extension of \(_{1}\) and \(_{2}\) to a manifold setting - itself due to a wide range of breakthrough applications to machine learning and data science, from natural language processing, and signal processing to dictionary learning and robotics [48; 52; 64; 65]. As a result, there is an increasing demand for a comprehensive exploration of various spaces, such as the \(d\)-dimensional torus, Grassmannian or Stiefel manifolds, hyperbolic spaces, and many others.

Unfortunately, in a proper Riemannian setting, only \(_{2}\) has received sufficient scrutiny thus far. Recent works by Criscitiello & Boumal  and Sun et al.  have shown that standard Riemannian deterministic algorithms can be augmented via the injection of an infinitesimal amount of noise (proportional to the method's desired accuracy), to achieve comparable escape guarantees in terms of oracle complexity as the corresponding Euclidean methods . To the best of our knowledge, all existing results for \(_{1}\) concern _deterministic_ methods [28; 41; 42; 53] which are significantly limited in scope in large-scale machine learning applications because of their prohibitively high per-iteration cost.

Our results and techniques.In view of the above, our paper aims to provide a general answer to \(_{1}\) for a broad class of Riemannian stochastic optimization methods - including Riemannian stochastic gradient descent, its retraction-based and/or optimistic variants, etc. Concretely, we focus throughout on a flexible template of _Riemannian Robbins-Monro_ (RRM) schemes [34; 59] which, in addition to the specific algorithms of interest mentioned above, also includes a range of Euclidean methods that can be analyzed efficiently from a Riemannian viewpoint.

Informally, our main result may be stated as follows:

_Under any stochastic Riemannian Robbins-Monro method, the probability of converging to a strict saddle point (or a submanifold thereof) is zero._

This statement provides firm grounds for accepting the output of a stochastic Riemannian optimization method as valid, as it shows that saddle points are avoided with probability 1.1 In the context of stochastic methods, our result builds on a series of foundational results by Pemantle  and Brandiere & Duflo  who focused on _hyperbolic traps_ (isolated saddle points with invertible Hessian). These results were subsequently extended by Benaim & Hirsch  to a more general class of unstable _sets_, but this analysis remained grounded in a flat, Euclidean setting. The connecting tissue of our analysis with these works is the notion of an asymptotic pseudotrajectory (APT), which allows us to couple the long-run behavior of discrete-time RRM methods to that of an associated Riemannian gradient flow. [This discrete-to-continuous comparison is crucial for our analysis in order to apply center stable manifold techniques  to the RRM framework.] However, this comes at a significant cost, as establishing the APT property in a Riemannian setting is quite challenging. To achieve this, we employ a set of techniques recently developed by  which allow us to make this comparison precise and establish the desired avoidance result.

## 2 Background on Riemannian Manifolds

We begin with a brief overview of some basic definitions from Riemmanian geometry and optimization, solely intended to set notation and terminology; our presentation roughly follows the masterful account of Lee [43; 44], to which we refer the reader for a comprehensive introduction to the topic.

Le \(\) be a \(d\)-dimensional, geodesically complete Riemannian manifold. Throughout the sequel, the tangent space to \(\) at a point \(x\) will be denoted by \(_{x}\), and we will write \((t)_{(t)}\) for the velocity vector to a smooth curve \(\) at time \(t\). We will also write \(,_{x}\) for the metric at \(x\), \(\|\|_{x}\) for the associated norm, and \((,)\) for the induced distance function on \(\), the latter being defined via the minimization of the length functional \([]=\|(t)\|_{(t)}\ dt\).

Given a point \(x\) and a tangent vector \(z_{x}\), the (necessarily unique) geodesic emanating from \(x\) along \(z\) will be denoted by \(_{z}\), and we define the exponential map at \(x\) as \(_{x}(z)=_{z}(1)\) for all \(z_{x}\) (recall here that \(\) is assumed complete, so this map is well-defined for all \(x\) and all \(z_{x}\)). Whenever well-defined, the inverse of \(_{x}\) will be written as \(_{x}:_{x}\), with the understanding that the domain of \(_{x}\) is actually the largest neighborhood of \(x\) on which the restriction of \(_{x}\) is a (global) diffeomorphism; by definition, we have \(_{x}(_{x}(z))=z\) for all \(z\) for which the relevant quantities are well-defined. Finally, given a pair of points \(x,x^{}\) and a tangent vector \(z_{x}\), we will write \(_{x x^{}}(z)\) for the vector obtained by parallel transporting \(z\) along any minimizing geodesic connecting \(x\) and \(x^{}\).

In this general context, we will be interested in solving the Riemannian optimization problem

\[_{x}f(x)\] (Opt)

for some smooth _objective function_\(f\). We will also respectively write

\[v(x)-f(x) H(x)(f(x))\] (1)

for the _negative_ (_Riemannian_) _gradient_ and the (_Riemannian_) _Hessian_ of \(f\) at \(x\). Finally, in terms of solutions of (Opt), we will focus on the avoidance of _strict saddle points_ of \(f\), i.e., points \(\) for which

\[v()=0_{}(H())<0\] (2)

where \(_{}\) denotes the minimum eigenvalue of the tensor in question. We will also say that a smooth compact component (in the sense of manifolds) of critical points of \(f\) is a _strict saddle manifold_ if there exist constants \(c_{}>0\) such that all negative eigenvalues of \(H()\), \(\), are bounded from above by \(-c_{-}<0\), and any positive eigenvalues (if they exist) are bounded from below by \(c_{+}>0\).

To differentiate the above from the Euclidean setting, when \(\) is a real space equipped with the Euclidean metric, we will instead write \( f\) and \(^{2}f\) for the (ordinary) gradient and Hessian matrix of \(f\). In this case, as is customary, we will not distinguish between primal and dual vectors.

## 3 Core algorithmic framework

For generality, our avoidance analysis will be carried out in an abstract stochastic approximation framework which includes several popular Riemannian optimization algorithms - from ordinary Riemannian (stochastic) gradient descent, to its retraction-based variants, optimistic methods, etc. For concreteness, we start with the general template below, and we present a (nonehaustive!) series of representative examples right after.

### The Riemannian Robbins-Monro template.

The _Riemannian Robbins-Monro_ (RRM) framework that we will consider for solving (Opt) is an iterative family of methods which directly extends the seminal stochastic approximation scheme of Robbins & Monro  to a manifold setting by replacing vector addition with the Riemannian exponential. Roughly following , we will focus on the abstract update rule

\[X_{n+1}=_{X_{n}}(_{n}_{n})\] (RRM)

where

1. \(X_{n}\) denotes the state of the algorithm at each iteration \(n=1,2,\)
2. \(_{n}_{X_{n}}\) is a surrogate for the (negative) gradient \(v(X_{n})\) of \(f\) at \(X_{n}\) (defined in detail below).
3. \(_{n}>0\) is the method's step-size (discussed in detail in Section 4).

In the above, the defining element of (RRM) is the sequence of "surrogate gradients" \(_{n}\), \(n=1,2,\), so this will be our first object of interest. Formally, letting \(_{n}\) denote the history of \(X_{n}\) up to stage \(n\) (inclusive), we will write

\[_{n} v(X_{n})+U_{n}+b_{n}\] (3)

where we have defined

\[U_{n}_{n}-[_{n}\,|\,_{n}]  b_{n}[_{n}\,|\,_{n}]-v(X _{n}),\] (4)

as the _random error_ and the _offset_ of \(_{n}\) relative to \(v(X_{n})\) respectively. It will also be convenient to introduce the _total error_\(W_{n}=_{n}-v(X_{n})=U_{n}+b_{n}\), which captures both random and systematic fluctuations in \(_{n}\), and which measures the total deviation of \(_{n}\) from \(v(X_{n})\).

Two points are worth noting here: First, \(_{n}\) is _not_ adapted to \(_{n}\), so \(U_{n}\) is random relative to \(_{n}\); on the other hand, \(b_{n}\) is \(_{n}\)-measurable, so it is deterministic relative to \(_{n}\). This brings us to the second important point regarding \(_{n}\): given the systematic offset term \(b_{n}\) in \(_{n}\), the latter should _not be seen_ as the output of a gradient oracle for \(v(X_{n})\). In particular, \(b_{n}\) is intended to capture possible corrective terms, deviations from the exponential mapping, different algorithmic update structures (such as optimism), etc. We make this distinction precise below.

### Specific algorithms and examples.

In the series of examples that follow, we will assume that the optimizer can access \(f\) via a _stochastic first-order oracle_ (SFO) returning noisy gradients of \(f\) at the evaluation point. Formally, following Nesterov , an SFO is a black-box mechanism which, when queried at \(x\), returns a (negative) stochastic gradient of the form

\[V(x;)=v(x)+(x;)\] (SFO)

where the _seed_\(\) is a random variable taking values in some complete probability space \(\), and \((x;)\) is an umbrella error term capturing all sources of uncertainty in the model.

The archetypal example of an SFO occurs when \(f\) is itself a stochastic expectation of the form \(f(x)=}[F(x;)]\) for some random function \(F\) - the so-called _stochastic optimization_ framework. In this case, \(V\) is typically given by \(V(x;)=-_{x}F(x;)\), so, under standard assumptions for exchanging differentiation and expectation, we have \(}[V(x;)]=v(x)\). Extrapolating from this basic framework, our only assumption for the moment will be that \(}[(x;)]=0\); for a detailed discussion of the required assumptions for (SFO), see Section4.

In practice, (SFO) will be queried repeatedly at a sequence of states \(X_{n}\), \(n=1,2,\), with a different random seed \(_{n}\) drawn i.i.d. from \(\). In this manner, we obtain the following specific algorithms as special cases of (RRM):

**Algorithm 1** (Riemannian stochastic gradient descent).: Following Bonnabel , the _Riemannian stochastic gradient descent_ (RSGD) algorithm queries (SFO) at \(X_{n}\) and proceeds as

\[X_{n+1}=_{X_{n}}(_{n}V(X_{n};_{n})).\] (RSGD)

As such, (RSGD) can be seen as an RRM scheme with \(_{n}=V(X_{n};_{n})\) or, equivalently, \(U_{n}=(X_{n};_{n})\) and \(b_{n}=0\). \(\)

A key factor limiting the applicability of (RSGD) is that the exponential map \(_{X_{n}}()\) could be prohibitively expensive to compute in practice, even for relatively low-dimensional manifolds. On that account, a popular alternative to (RSGD) is to employ a _retraction map_, that is, a smooth mapping \(\) that agrees with the exponential map up to first order, namely

\[_{x}(0)=x_{t=0}_{x}(tz)=z(x,z).\] (Rtr)

With this machinery in hand, we obtain the following retraction-based variant of (RSGD):

**Algorithm 2** (Retraction-based stochastic gradient descent).: By replacing the exponential map in (RSGD) with a retraction, we obtain the _retraction-based stochastic gradient descent_ scheme

\[X_{n+1}=_{X_{n}}(_{n}V(X_{n};_{n})).\] (Rtr-SGD)

This algorithm does not seem immediately related to the RRM template - and, indeed, the whole point of introducing a retraction was to get rid of the exponential map in (RRM). The expressive power of (RRM) can be seen in the fact that, despite this apparent disconnect, (Rtr-SGD) can still be expressed as a special case of (RRM) in a fairly straightforward fashion.

To do so, define the "forward-backward" gradient mapping

\[_{n}}_{X_{n}}(_{X_{n}}( _{n}V(X_{n};_{n})))\] (5)

with the proviso that the Riemannian logarithm in (5) is well-defined (we discuss the conditions under which this holds later in the paper). Under this definition, (Rtr-SGD) can be recast as a special case of (RRM) by running the latter with the surrogate gradient sequence \(_{n}\) of Eq.5. To streamline our presentation, we defer the discussion about the inherent error \(W_{n}=_{n}-v(X_{n})\) to AppendixA.

As we mentioned before, retraction-based algorithms typically exhibit significantly lower per-iteration complexity compared to geodesic methods, resulting in their remarkable success in practical machine learning applications . In addition, as we show below, the use of a retraction mapping allows us to provide a unified perspective for several classical algorithms which, at first sight, might seem completely unrelated. An important example is provided by the (stochastic) _mirror descent_ (MD) family of algorithms :

**Algorithm 3** (Stochastic mirror descent).: Let \(\) be an open convex subset of \(^{M}\) and let \(h\) be a \(C^{2}\)-smooth, strongly convex _Legendre function_ on \(\), that is, \(\| h(x)\|\) whenever \(x()\) [cf. 60, Chap. 26]. Then, the _stochastic mirror descent_ (SMD) algorithm unfolds as

\[X_{n+1}=_{X_{n}}(_{n}V(X_{n};_{n}))\] (SMD)

where \(V(X_{n};_{n})\) is the output of an SFO query for \( f(X_{n})\) at \(X_{n}\), and \(^{M}\) is the so-called _prox-mapping_ associated to \(h\), viz.

\[_{x}(y)=_{x^{}}\{ h(x)+y, x^{}-h(x^{})\}x,\,y^{M}.\] (6)

where \(,\) stands for the ordinary Euclidean inner product in \(^{M}\).

Now, even though the notation in (SMD) is reminiscent of (Rtr-SGD), the definition (6) of \(\) does not bear any resemblance to a geodesic exponential or a retraction - and, indeed, its origins are starkly different. However, as we show below, \(\) can indeed be seen as a retraction relative to a specific Riemannian structure on \(\), the _Hessian Riemannian_ (HR) metric associated to \(h\).

To make this precise, the first step is to note that the basic recursive structure \(x^{+}=_{x}(y)\) of (SMD) can be rewritten as

\[x^{+}=_{x}(y)= h^{*}( h(x)+y)\] (7)

where \(h^{*}(y)=_{x}\{ y,x-h(x)\}\) denotes the convex conjugate of \(h\), and we have used Danskin's theorem  to write \(_{x}\{ y,x-h(x)\}= h^{*}(y)\). Then, if we endow \(\) with the Hessian Riemannian metric \(g(x)=^{2}h(x)\), the Riemannian gradient of \(f\) relative to \(g\) becomes \(f(x)=[^{2}h(x)]^{-1} f(x)\); more generally, given a cotangent (dual) vector \(y\) to \(\) at \(x\), the corresponding tangent (primal) vector will be \(z=g(x)^{-1}y=[^{2}h(x)]^{-1}y\). In view of this, by inverting the relation \(z=g(x)^{-1}y\), the abstract mirror descent recursion (7) can be rewritten as

\[x^{+}=_{x}(z)_{x}(g(x)z)\,.\] (8)

Now, to proceed, consider the curve

\[(t)=_{x}(tz)=_{x}(tg(x)z)= h^{*}( h(x )+tg(x)z),\] (9)

so, by definition, \((0)=x\). In addition, by a direct differentiation, we readily obtain

\[(0)=^{2}h^{*}( h(x))g(x)z=z\] (10)

where we used the standard identity \(^{2}h^{*}( h(x))=[^{2}h(x)]^{-1}\). This shows that the map \(_{x}(z)=_{x}(g(x)z)\) is, in fact, a _retraction_, so (SMD) is a special case of (Rtr-SGD) - and hence, of the general stochastic approximation template (RRM). \(\)

_Remark_.: Even though elements of the above ideas are implicit in previous works on mirror descent and Hessian Riemannian metrics , to the best of our knowledge, this is the first time that (SMD) is formalized as a retraction-based (Hessian) Riemannian scheme. \(\)

**Algorithm 4** (Riemannian optimistic gradient).: Moving forward, an important algorithm for solving online optimization problems and games is the so-called optimistic gradient method - originally pioneered by Popov  and subsequently popularized by Rakhlin & Sridharan . In the Euclidean case, this method introduces an interim, "optimistic" correction to gradient dynamics and updates as

\[ X_{n}^{+}&=X_{n}+_{n}V(X_{n-1}^ {+};_{n-1})\\ X_{n+1}&=X_{n}+_{n}V(X_{n}^{+};_{n}) \] (OG)

where, as usual, \(V\) is an SFO for the (negative) gradient \( f\) of \(f\). This idea can then be directly transported to a manifold setting , leading to the _Riemannian optimistic gradient_ method

\[ X_{n}^{+}&=_{X_{n}}(_{n}V(X_{n- 1}^{+};_{n-1})),\\ X_{n+1}&=_{X_{n}}(_{X_{n}^{+} X_{n}}( _{n}V(X_{n}^{+};_{n}))).\] (ROG)

Importantly, the recursion (ROG) may be seen as a special case of (RRM) by setting \(_{n}=(1/_{n})_{X_{n}^{+} X_{n}}(_{n}V(X_{n}^ {+};_{n}))\) or, equivalently \(U_{n}=_{X_{n}^{+} X_{n}}((X_{n}^{+};_{n}))\) and \(b_{n}=_{X_{n}^{+} X_{n}}(v(X_{n}^{+}))-v(X_{n})\). We defer the details of this calculation to the Appendix A. \(\)

**Algorithm 5** (Natural gradient descent).: Our last example concerns the influential _natural gradient descent_ (NGD) method of Amari , a stochastic optimization scheme for Euclidean spaces, but adapted to the local geometry defined by a strictly convex function \(h\). Specifically, NGD queries an SFO and proceeds as

\[X_{n+1}=X_{n}-_{n}(f(X_{n})+(X_{n}; _{n}))\] (NGD)

where \(f(x)[^{2}h(x)]^{-1} f(x)\) denotes the Riemannian gradient of \(f\) relative to Hessian Riemannian metric \(g(x)= h^{2}(x)\) on \(^{M}\). It is well known that (NGD) can be seen as a retraction-based Riemannian scheme , and may thus be integrated directly within the framework of (RRM); we defer the details to Appendix A. Importantly, (NGD) also includes the celebrated _natural policy gradient_ which plays an important role in reinforcement learning. \(\)

The above examples have been chosen to illustrate a range of different update mechanisms that can be integrated within the general algorithmic template provided by (RRM). Of course, it is not possible to be exhaustive but, for illustration purposes, we provide some more examples in Appendix A.

## 4 Analysis and results

We are now in a position to state and discuss our main result concerning the avoidance of saddle points under (RRM). For concreteness, we begin by discussing the technical assumptions that we will need in Section 4.1; subsequently, we proceed with the formal statement of our result and some direct applications thereof in Section 4.2.

### Technical assumptions.

Our technical assumptions concern the four main ingredients of (RRM), namely (\(i\)) the regularity of \(f\); (\(ii\)) the method's step-size sequence \(_{n}\); (\(iii\)) the statistics of the surrogate gradients \(_{n}\) entering (RRM); and (\(iv\)) the ambient manifold \(\). Specifically, we will require the following:

**Assumption 1** (Regularity of \(f\)).: The function \(f\) is \(C^{2}\) and \(v=-f\) is (geodesically) \(L\)_-Lipschitz_, i.e., for all \(x,x^{}\), we have

\[\|_{x x^{}}(v(x))-v(x^{})\|_{x^{}}=\|v(x)- _{x^{} x}(v(x^{}))\|_{x} L(x,x^{ }).\] (11)

**Assumption 2** (Step-size schedule).: The step-size sequence \(_{n}\) of (RRM) satisfies

\[_{n=1}^{}_{n}=_{n=1}^{} ^{1/_{n}}<(0,1).\] (12)

**Assumption 3** (Surrogate gradients).: The offset and random error components of \(_{n}\) satisfy

\[\|b_{n}\|_{_{n}} C_{n},\|U_{n}\|_{_{n}}, [\{ U_{n},z_{_{n}}\}_{+}\,|\, _{n}]\] (13)

for suitable constants \(C,,>0\) and for all \(z_{_{n}}\), \(\|z\|_{_{n}}=1\) (in the above, all conditions are to be interpreted in the almost sure sense and \([t]_{+}=\{0,z\}\) denotes the positive part of \(t\)).

**Assumption 4** (Injectivity radius).: The injectivity radius of \(\) is bounded from below by \(>0\).

Before proceeding, we discuss the implications and range of validity of each of the above assumptions. Since Assumption 1 is standard, we focus on the remaining three below:

**On Assumption 2.** The step-size conditions typically encountered in the analysis of Robbins-Monro schemes is the \(L^{2}-L^{1}\) ("_square-summabe-but-not-summable_") condition \(_{n}_{n}=\), \(_{n}_{n}^{2}<\), cf.  and references therein. This puts a hard threshold on the range of allowed step-size schedules at \((1/n^{1/2})\): any step-size that decays at least as slow as \(1/n^{1/2}\) cannot be used under the \(L^{2}-L^{1}\) assumption. By contrast, the step-size condition (12) is considerably more lax and can tolerate near-constant step-sizes of the form \(_{n} 1/( n)^{1+}\) for some \(>0\). This is enough to cover all decreasing step-size policies used in practice. [We also recall here that, in stochastic non-convex settings, trajectory convergence cannot be guaranteed in general without a vanishing step-size, cf.  and references therein.]On Assumption 3.Three remarks are in order for the noise and offset requirements (13). First, we should note that the condition \(b_{n}=(_{n})\) is, a priori, _implicit_, because it depends on the statistics of the feedback sequence \(_{n}\), and these may be difficult to estimate in general. However, in most practical applications, this quantity is under the _explicit_ control of the optimizer: in particular, as we show later in this section, this requirement is satisfied by all the specific algorithms of Section 3.2.

Likewise, the bounded noise requirement is satisfied in many practical cases of interest. For example, when the problem's objective function admits a finite-sum decomposition of the form \(f(x)=_{i=1}^{N}f_{i}(x)\) for an ensemble of empirical instances \(f_{i}\), \(i=1,,N\) (the standard framework for applications to data science and machine learning), \(U_{n}\) is typically generated by sampling a minibatch of \(f\), which in turn results in an error term of the form \(U_{n}=q(X_{n})\) where \(q(x):_{x}\) is bounded on all compact subsets of \(\). Therefore, \(\|U_{n}\|_{X_{n}}\|q(X_{n})\|_{X_{n}}<\) for some constant \(\) for any convergent algorithm \(\{X_{n}\}_{n}\).

Finally, the "uniform excitability" condition \([\{ U_{n},z_{X_{n}}\}_{+}\,|\,_{n}]\) is also standard in the avoidance literature , and it is substantially weaker than the _isotropic_ condition, which, roughly speaking, requires the noise to have the same \(L^{2}\) magnitude along all directions in space . Instead, (13) only posits that the noise \(U_{n}\) has a _non-zero_ component along each direction, and imposes no other restrictions on the statistical profile of the noise.

On Assumption 4.For our last assumption, recall first that the injectivity radius of \(\) at a point \(x\) is the largest radius for which \(_{x}\) is a diffeomorphism onto its image; the injectivity radius of \(\) is then taken to be the infimum over all such radii . In this regard, Assumption 4 simply serves to ensure that the exponential map is invertible at consecutive iterates of (RRM) so no local topological complications can arise. This assumption is automatically satisfied in closed manifolds (independent of curvature), as well as in non-positively curved manifolds - such as Cartan-Hadamard spaces and the like . This assumption (and its variants) is also standard in the literature, cf.  and references therein.

### Avoidance of saddle points.We are now in a position to state our main avoidance result:

**Theorem 1**.: _Let \(X_{n}\), \(n=1,2,\), be the sequence of states generated by (RRM), and let \(\) be a strict saddle manifold of \(f\). Then, under Assumptions 1-4, we have_

\[((,X_{n}) 0n )=0\] (14)

_where \((,X_{n})=_{x}(x,X_{n})\) denotes the (Riemannian) distance of \(X_{n}\) from \(\)._

Before discussing the proof of Theorem 1, it is worthwhile to compare our work with its closest antecedents. First, in regard to the general avoidance theory in Euclidean spaces , the statement is similar in scope (avoidance of unstable manifolds with probability 1), but the techniques and challenges involved are very different. The reason for this is simple: the additive, vector space structure of \(^{m}\) is ingrained at every step of the way in the Euclidean analysis of , and adapting the various constructions to a manifold setting can be a complicated affair. For an illustration of the technical difficulties involved, see the recent stochastic approximation analysis of .

By contrast, the recent results of  paint a complementary picture: they concern Riemannian problems but, at their core, they are deterministic results. More precisely, the noise in  is actually _injected_ in an otherwise deterministic gradient scheme to facilitate the escape from flat regions in the vicinity of a saddle point; other than that, the magnitude of the noise must be proportional to the solver's desired accuracy, and hence is typically extremely small. As a result, the analysis of  cannot be extended to bona fide stochastic schemes - like (RSGD) - which also explains why these results involve a constant step-size (as opposed to a decreasing step-size schedule, which is required to guarantee trajectory convergence in settings with persistent noise). In this regard, Theorem 1 simultaneously complements the stochastic analysis of  to Riemannian problems, and the Riemannian analysis of  to a stochastic setting.

Proof outline.To facilitate the reading of our proof, we provide below a detailed outline of the main steps and techniques involved therein, deferring the full proof to Appendix B. We begin with a high-level description of our proof strategy and then encode the main arguments in a series of steps right after.

For the purposes of illustration, suppose that \(\) is a subset of \(^{m}\). Then, given a tangent vector \(z_{x}\), we define the _geodesic offset_ (see Fig. 1) from \(x\) along \(z\) as

\[(x;z)=_{x}(z)-x-z\] (15)

i.e., as the difference between the geodesic emanating from \(x\) along \(z\) and its first-order approximation relative to \(x\) in the ambient space \(^{m}\) (with all differences expressed in the ordinary vector space structure of \(^{m}\)). The offset \((x;z)\) is readily checked to be second-order in \(z\) so, while the curve \(x+tz\) does not in general induce a retraction on the target manifold \(\) (in particular, the point \(x+tz\) may not even _belong_ to \(\)), the converse _is_ true: the map \(_{x}(z)\) is always a retraction on the ambient, Euclidean space \(^{m}\). In this way, the basic iteration (RRM) can be expressed as

\[X_{n+1}=_{X_{n}}(_{n}_{n})=X_{n}+_{n}_{n}+ (X_{n};_{n}_{n})\] (16)

leading to the fundamental question below:

_What is the maximum offset \(_{n}(X_{n};_{n}_{n})\) that can be tolerated by a Euclidean stochastic approximation algorithm to avoid saddle points?_

A key technical step in our work is to develop the means to control the offset term \(_{n}\) under (RRM) under a sufficiently broad class of assumptions that includes Algorithms 1-5. Crucially, this step is made possible thanks to the very recent - and technical - stochastic approximation work of . To help the reader navigate our proof strategy, we outline the main steps below, focusing for simplicity on the case of a single saddle point.

Step 1: From discrete to continuous time (and back).Let \(\) be a strict saddle point of \(f\). By the stable manifold theorem , the set of all initializations such that the Riemannian gradient flow

\[(t)=-f(x(t))\] (RGF)

converges to \(\) is of measure \(0\). Then, assuming for the moment that the geodesic offset error \(_{n}=(X_{n};_{n}_{n})\) in (16) is sufficiently small, the iterates of (RRM) can be seen as a noisy, approximate Euler discretization of (RGF); as such, it is reasonable to expect that the induced trajectories of (RRM) will never converge to \(\).

To make this intuition precise, our first step will be to show that the iterates of (RRM) comprise an _asymptotic pseudotrajectory_ of (RGF) in the sense of Benaim , i.e., they asymptotically track the orbits of (RGF) with arbitrary precision over windows of arbitrary length. To formalize this, define the "effective time" variable \(_{n}=_{k=1}^{n-1}_{k}\) and the associated _geodesic interpolation_\(X(t)\) of \(X_{n}\) as

\[X(t)=_{X_{n}}((t-_{n})_{n}), _{n+1})$, $n 1$}\] (GI)

so, by construction, \((a)\)\(X(_{n})=X_{n}\) for all \(n\); and \((b)\) each segment of \(X(t)\) is a geodesic. Then, letting \(_{+}\) denote the _flow_ of (RGF) - i.e., \(_{h}(x)\) is simply the position at time \(h 0\) of the solution orbit of (RGF) that starts at \(x\) - we will say that \(X(t)\) is an APT of (RGF) if, for all \(T>0\), we have

\[_{t}_{0 h T}(X(t+h),_{h}(X(t)))=0.\] (APT)

This requirement is non-trivial, and our first technical result is to guarantee precisely this:

**Theorem 2**.: _Suppose that Assumptions 2-4 hold. Then, with probability \(1\), the geodesic interpolation \(X(t)\) of the sequence of iterates \(X_{n}\), \(n=1,2,\), generated by (RRM) is an APT of (RGF)._

A version of Theorem 2 was very recently derived by  under a different set of assumptions: On the one hand,  imposes a much more restrictive step-size schedule for \(_{n}\) (square summability) but, on the other hand, it only posits that the noise increments \(U_{n}\) are bounded in \(L^{2}\) (as opposed to \(L^{}\) in our case). Our proof relies on the same construction of the Picard iteration map as , but otherwise diverges significantly in the probabilistic analysis required to establish (APT).

Step 2: From Riemannian to Euclidean schemes (and back).Albeit crucial, the APT property is decidedly not enough to guarantee avoidance: after all, the constant orbit \(X(t)=\) for all \(t 0\) is trivially an APT of (RGF) but, of course, it does not avoid \(\). To proceed, we will need to exploit the precise update structure of (RRM) in conjunction with the stable manifold theorem applied to (RGF).

In the Euclidean case, this is achieved by means of an intricate Lyapunov function argument, originally due to . Our second step is to devise a new geometric argument to reduce the analysis from an arbitrary _intrinsic_ manifold to an isometrically embedded submanifold of \(^{m}\). This step is carried out by a combination of the celebrated Nash embedding theorem and a (smooth) Tietze extension argument to rewrite (RRM) as a "corrected" Robbins-Monro scheme on \(^{m}\) that actually evolves on \(\). This construction also requires a "perturbation analysis" to ensure that certain subtle topological issues do not arise when we invoke the stable manifold theorem; we present the details in Appendix B.

Step 3: Controlling the geodesic offset.As we briefly described in the beginning of the proof overview, this Euclidean reframing of (RRM) introduces an intrinsic offset error \(_{n}=(X_{n};_{n}_{n})\), which is difficult to analyze in detail (the offset incurred by a retraction on \(\) is of similar order, so the exponential-retraction distinction is not important at this stage). Our crucial observation here is that, under our blanket assumptions, \(_{n}\) is small relative to \(_{n}\) and, in particular, \(_{n}=(_{n}^{2})\). Thanks to this bound, we are able to leverage a series of stochastic bounds - originally developed by Pemantle  - to show that the probability that these terms will have an adverse effect on exiting the center manifold of \(\) is zero (this is also where Assumption 3 comes in). We formalize this in Appendix B; Theorem 1 then follows by putting everything together.

_Remark_.: A concept similar to our geodesic offset \((x;z)\) has been explored in the _reverse direction_ by the very recent works  whose goal was to study of avoidance of _Euclidean_ subgradient methods as an inexact Riemannian gradient scheme. They further show that this inexact Riemannian gradient descent can avoid saddle points if uniform noise is injected. While their core idea bears some similarity to ours, it remains unclear how to apply the analysis in  to handle general RRM schemes, such as retraction-based and natural policy gradient methods.

### Applications.

As an illustration of the generality of Theorem 1, we now instantiate it to the range of specific algorithms discussed in Section 3.2. Since all these algorithms are run with gradient input generated by (SFO), applying Theorem 1 would require mapping the requirements of Assumption 3 to the primitives of (SFO). A convenient way to achieve this is by means of the proposition below:

**Proposition 1**.: _Suppose that Algorithms 1-5 are run with a gradient oracle \(V(x;)=v(x)+(x;)\) such that_

\[\|(x;)\|_{x}(x) {}[\{(x;),z_{x}\}_{+}] (x)\] (17)

_for all \(z_{x}\), \(\|z\|_{x}=1\), and for suitable functions \(,_{+}\) with \(\) bounded on bounded subsets of \(\) and \(_{x}(x)>0\). Then, under Assumptions 2 and 4, the conclusion of Theorem 1 holds, that is, Algorithms 1-5 avoid strict saddle manifolds of \(f\)._

The proof of Proposition 1 is deferred to Appendix B; we only note here that its proof mainly hinges on verifying the bias requirement \(\|b_{n}\|_{X_{n}}=(_{n})\) of (13) by means of (_i_) the boundedness of the error function \((x)\) on bounded subsets of \(\); and (_ii_) controlling the maximal deviation between a retraction and the exponential map for input vectors bounded by \(\).

## 5 Numerical Illustrations

In this section, we aim to demonstrate the practical applicability of the theoretical framework proposed in our paper by providing numerical illustrations. To do so, we utilize a 2-dimensional torus as the optimization landscape, where the complexity and multi-modal nature of the objective function can be easily visualized in Fig. 2.

Our objective function features three saddle points (in black) and one global minimizer (in red). We subject two RRM schemes, i.e., (RSGD) and (RSEG), to initialization in proximity to these saddle points. This strategic choice rigorously tests their ability to navigate and converge to the global optimum. In line with our theoretical predictions, both RRM methods avoid the saddle points and

Figure 1: The geodesic offset \((x;z)\).

eventually reach the desirable global minimum. This empirical confirmation reinforces the central message of our paper: stochastic Riemannian schemes only converge to local minimizers.

## 6 Conclusions and future work

In this paper, we addressed the question of when Riemannian stochastic algorithms can effectively evade saddle points, focusing on the broad category of Riemannian Robbins-Monro schemes. We introduced a novel framework for analyzing the avoidance of Riemannian saddle points within the RRM framework, which encompasses many commonly used Riemannian stochastic algorithms, including retraction-based algorithms. Our framework builds upon the notion of strict saddle points and provides a set of easily verifiable conditions that guarantee the avoidance of such traps.

Our work paves the way for several promising research directions in learning with Riemannian methods. One intriguing avenue for exploration is the investigation of whether Riemannian _zeroth-order_ methods, such as the Riemannian extension of the work by Kiefer & Wolfowitz , can effectively evade strict saddle points. We believe that combining the insights from the asymptotic pseudotrajectory theory with Euclidean analysis can shed light on this question and provide valuable insights into the behavior of these methods in the Riemannian setting.

Furthermore, an interesting direction for future research is the extension of the avoidance of _unstable limit cycles_ in Euclidean min-max optimization, as studied by Hsieh et al. , to the realm of Riemannian games. Investigating the avoidance of unstable limit cycles in this context has the potential to uncover novel phenomena specific to the manifold settings, leading to a deeper understanding on the intricate dynamics and strategies involved in Riemannian games.