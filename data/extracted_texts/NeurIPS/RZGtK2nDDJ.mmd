# Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder

Huiwon Jang\({}^{ A}\) Jihoon Tack\({}^{ A}\) Daewon Choi\({}^{ B}\) Jongheon Jeong\({}^{ A}\) Jinwoo Shin\({}^{ A}\)

\({}^{ A}\)Korea Advanced Institute of Science and Technology (KAIST) \({}^{ B}\)Korea University

{huiwoen0516, jihoontack}@kaist.ac.kr

Equal contributions

###### Abstract

Despite its practical importance across a wide range of modalities, recent advances in self-supervised learning (SSL) have been primarily focused on a few well-curated domains, e.g., vision and language, often relying on their domain-specific knowledge. For example, _Masked Auto-Encoder_ (MAE) has become one of the popular architectures in these domains, but less has explored its potential in other modalities. In this paper, we develop MAE as a unified, modality-agnostic SSL framework. In turn, we argue _meta-learning_ as a key to interpreting MAE as a modality-agnostic learner, and propose enhancements to MAE from the motivation to jointly improve its SSL across diverse modalities, coined _MetaMAE_ as a result. Our key idea is to view the mask reconstruction of MAE as a meta-learning task: masked tokens are predicted by adapting the Transformer meta-learner through the amortization of unmasked tokens. Based on this novel interpretation, we propose to integrate two advanced meta-learning techniques. First, we adapt the amortized latent of the Transformer encoder using gradient-based meta-learning to enhance the reconstruction. Then, we maximize the alignment between amortized and adapted latents through task contrastive learning which guides the Transformer encoder to better encode the task-specific knowledge. Our experiment demonstrates the superiority of MetaMAE in the modality-agnostic SSL benchmark (called DABS), significantly outperforming prior baselines. Code is available at https://github.com/alinlab/MetaMAE.

## 1 Introduction

Self-supervised learning (SSL), i.e., learning without human supervision, recently has demonstrated substantial success across fields including, computer vision , natural language processing (NLP) , and speech recognition . The efficacy of SSL is derived by extracting transferable knowledge from unlabeled datasets, a feature that manifests significant utility for various downstream tasks such as classification and segmentation. As a result, SSL has become an indispensable technique in real-world applications (for instance, industrial contexts like medical imaging ), not only improving the performance on new datasets but also reducing a significant amount of computations and costs, e.g., expert annotation poses significant costs . However, despite the importance of SSL in such fields, recent advancements have been predominantly focused on specific domains (e.g., images and NLP) where the majority of existing SSL frameworks on such domains require modality-specific knowledge, thereby constraining the applicability and scalability of previous works across new modalities.

To tackle this issue, we draw attention to the recent success of the Masked Auto-Encoder (MAE) framework , which eliminates the need for modality-specific inductive biases. Initially presented as a generative model , the MAE models the network to reconstruct the original inputsignal based on the randomly masked part of the signal. Recently, the integration of MAE with the Transformer architecture  has resulted in a powerful SSL framework for various domains, such as vision [33; 5; 10], NLP [18; 70], and tabular  datasets. For instance, it has been demonstrated that BERT , utilizing a Transformer encoder with a linear decoder, can effectively transfer to diverse tasks for the NLP domain. On the other hand, it has been suggested that utilizing a decoder of a deep Transformer architecture is essential for applying MAE within the vision domain , achieving remarkable performance . Building on these insights, we found that MAE is quite a promising direction for modality-agnostic SSL: our experiment demonstrates that using a deep Transformer decoder for MAE significantly outperforms previous modality-agnostic SSL frameworks (see Table 5). In this paper, we suggest to further exploit the benefits of MAE to build a unified SSL framework.

**Contribution.** We propose Meta-learned Masked Auto-Encoder (MetaMAE), a novel modality-agnostic SSL framework that leverages the power of meta-learning; see the overview in Figure 1. Our key idea is to interpret MAE as a meta-learning framework, thereby improving the generalization through the use of advanced meta-learning schemes. To be specific, we interpret the data reconstruction itself as a task, where the Transformer meta-learner is adapted through amortization of the support set (i.e., unmasked tokens) to predict the query set (i.e., masked tokens). Based on this interpretation, we propose a novel integration of two advanced meta-learning techniques to enhance MAE; namely the use of gradient-based meta-learning  and task contrastive learning [28; 60].

* _Latent Adaptation via Gradient-based Meta-learning_: We suggest adapting the amortized latent to better reconstruct the given support set (and the nearby tokens) through gradient-based meta-learning on the decoder . Then, the optimized latent is used to condition the decoder for the query prediction. This approach generally eases the task compared to the direct reconstruction, thereby streamlining and improving the task adaptation process.
* _Task Contrastive Learning_: To further leverage this optimized latent, we suggest utilizing task contrastive learning [28; 60]. Specifically, since both the optimized and predicted latents originate from the same task, we aim to maximize their similarity while minimizing their similarity with other tasks. This prompts the Transformer encoder to produce predictions closely aligned with the optimized latent, effectively guiding the Transformer to better encode the task knowledge.

We verify the efficacy of MetaMAE through extensive evaluations on multiple data modalities from modality-agnostic SSL benchmarks (i.e., DABS 1.0  and 2.0 ), including time-series, tabular, discrete token, multi-spectral images, speech, and multi-modal datasets. Overall, our experimental results demonstrate strong results, consistently and significantly outperforming previous modality-agnostic SSL methods in linear evaluation. For instance, measured with classification accuracy (%), MetaMAE improves the prior state-of-the-art results by 85.3 \(\) 89.3 on PAMAP2 , 53.6 \(\) 69.4 on Genomics , and 60.2 \(\) 79.8 on LibriSpeech  datasets. Moreover, we also demonstrate that MetaMAE significantly improves the linear evaluation performance on cross-domain datasets, indicating the improved transfer ability of MAE through meta-learning.

Figure 1: An overview of the proposed Meta-learned Masked Auto-Encoder (MetaMAE): we adapt the amortized latent \(_{}\) of the Transformer encoder \(f_{}\) using gradient-based meta-learning on the Transformer decoder \(g_{}\) to enhance the reconstruction, then maximize the alignment with optimized latent \(_{}^{*}\) to guide the Transformer encoder towards improved predictions via task contrastive learning.

Meta-Learning Modality-Agnostic Masked Auto-Encoder

In this section, we present _Meta-learned Masked Auto-Encoder (MetaMAE)_, a novel and effective modality-agnostic self-supervised learning (SSL) framework. Our key contribution is to tackle the modality-agnostic SSL problem with in-depth utilization of MAE, which was quite under-explored in the field. Based on our novel interpretation that views MAE as a meta-learning framework (in Section 2.1), we improve the transfer ability of MAE by suggesting enhanced modality-agnostic meta-learning techniques including latent optimization-based meta-learning and task contrastive learning (in Section 2.2). Our framework is visually depicted in Figure 1, and the pseudo-code is provided in Algorithm 1.

**Problem setup.** We first describe the problem setup of our interest, modality-agnostic SSL. This problem aims to learn a transferable representation from the unlabeled dataset without utilizing the modality-specific inductive biases. For a given unlabeled pretrain dataset \(_{}=\{_{i}\}_{i=1}^{N}\), where \(^{d}\) represents an input sampled from a certain data-generating distribution in an i.i.d manner, our objective is to train an encoder \(f_{}\) that can linearly separate the given labeled transfer dataset drawn from a similar or the same data-generating distribution.

### Rethinking Masked Auto-Encoder as a Meta-Learning Framework

Meta-learning  aims to extract and utilize the knowledge from the distribution of tasks to better solve a relevant task. This problem is typically approached by training a meta-learner that can transfer its knowledge to a task-specific model through adaptation, where the performance of the meta-learner is evaluated on the basis of how well each adapted model performs on the corresponding task. To learn such a meta-learner, a standard way is to use a set of _support set_ samples to adapt the task-specific model from the meta-learner and use another disjoint set of samples, called _query set_ samples to evaluate the adaptation performance [92; 82].

**Mask prediction as a modality-agnostic task.** MAE is an SSL technique that trains an autoencoder to reconstruct the original input signal with a randomly masked part of the signal. To implement such a technique, recent works utilize the Transformer architecture for the autoencoder design which is necessary for successful training. To use Transformer for MAE, the input data is broken down into non-overlapping units coined tokens (e.g., patches for images, and words for languages) where such tokens are divided into two disjoint sets (unmasked and masked) for MAE modeling, i.e., the Transformer autoencoder predicts the masked token using the unmasked tokens.

Our key insight is to interpret the signal reconstruction of MAE as a meta-learning task, where two disjoint unmasked and masked token sets are viewed as support and query sets to adapt and evaluate the Transformer meta-learner. To be specific, the Transformer encoder extracts the task knowledge through amortization of the support set, where this amortized latent adapts the Transformer decoder to predict the query set of the task. Formally, for a given data sample \(\), we first divide the signal into two disjoint sets, namely the support set \(_{}\) and the query set \(_{}\), by utilizing the tokenize operation \(()\{(m,}^{(m)})\}_{m=1 }^{M}=_{}_{}\). Then, for a given Transformer encoder \(f_{}\) and decoder \(g_{}\), MAE minimizes the discrepancy between the predicted token and the corresponding masked token (i.e., the query sample) as:

\[_{}(,;_{}) _{(q,}^{(q)})_{}}d}^{(q)},g_{}^{(q)}_{} \ \ \ \ _{}=f_{}(_{}),\] (1)

where \(d(,)\) is a discrepancy function: \(_{2}\) norm for continuous (e.g., time-series, speech) and cross-entropy for discrete (e.g., token) datasets, respectively. Based on this interpretation, we improve the transfer ability of MAE (for modality-agnostic SSL) through a novel integration of two effective modality-agnostic meta-learning techniques to MAE.

### MetaMAE: Improving Masked Auto-Encoder through Meta-Learning

We now describe our method, MetaMAE, which further improves the representation of MAE through a novel integration with advanced modality-agnostic meta-learning techniques. In a nutshell, MetaMAE operates by further optimizing the amortized latent of the Transformer encoder using gradient-based meta-learning. Then we maximize the alignment between the optimized and the amortized latents via contrastive learning, to guide the Transformer encoder to improve the generalization.

**Latent adaptation via gradient-based meta-learning.** To further improve the generalization of MAE, we suggest utilizing the gradient-based meta-learning (i.e., model-agnostic meta-learning; MAML ) on the amortized latent space . Specifically, we adapt the amortized latent of the support set to better reconstruct the support and the nearby tokens (of support tokens) by using the gradients of the decoder. Then, we utilize the optimized latent to condition the decoder to predict the query tokens. Here, our key idea is the use of nearby tokens when optimizing the latent, which turns out to be crucial for improved performance. Intuitively, optimizing such tokens induce an error correction on the latent, which eases the mask reconstruction (or prediction) task compared to the direct reconstruction, and thereby improves the task adaptation process .

```
1:Unlabeled pretrain dataset \(D_{}\), weight hyperparameter \(\), Nearby-\(\) ratio \(r\), batch size \(B\), learning rates \(\), \(\).
2:Initialize \(\), \(\), \(\) using the standard initialization scheme.
3:while not done do
4: Sample mini-batch \(=\{_{i}\}_{i=1}^{B}\) from \(D_{}\)
5:for\(i=1\) to \(B\)do\(\)Note: we use the batch computation.
6: Sample Support set \(_{_{i}}\) and Query set \(_{_{i}}\) from \(_{i}\)
7:\(_{_{i}}=f_{}(_{_{i}})\)\(\)Amortization through Transformer encoder.
8: Sample \((_{_{i}};r)\) where \(|(_{_{i}};r)|=r|_{_{i}}|\)\(\)Sample the Nearby-\(\) tokens.
9:\(_{_{i}}^{*}_{_{i}}- _{_{_{i}}}_{}(,; }_{_{i}})\).
10:\(}_{_{i}}=_{_{i}} (_{_{i}};r)\)\(\)Adapt the amortized latent.
11: Compute MAE reconstruction loss \(_{}^{i}\) with \(_{^{i}}^{*}\). \(\)Eq. (2)
12: Compute task contrastive loss \(_{}^{i}\) with \(\{_{^{i}}\}_{i=1}^{B}\) and \(\{_{^{i}}^{*}\}_{i=1}^{B}\)\(\)Eq. (4)
13: Compute MetaMAE loss \(_{}^{i}\) with \(_{}^{i}\), \(_{}^{i}\) and \(\)\(\)Eq. (5)
14:endfor
15:\(,,,,-_{i=1}^{B} _{}^{i}\)\(\)Update the entire networks.
16:endwhile ```

**Algorithm 1** MetaMAE: Meta-Learning Modality-Agnostic Masked Auto-Encoder

Concretely, for a given support set \(_{}\), we select the nearby tokens of support tokens from the query set \(_{}\), namely \((_{};r)_{}\), such that the cardinality is \(|(_{};r)|=r|_{}|\) with a ratio of \(r>0\). Then, we optimize the amortized latent \(_{}\) to better reconstruct the support and the nearby tokens \(}_{}_{}(_{};r)\) using the decoder gradient, then condition the meta-learner, i.e., the Transformer decoder \(g_{}\), to predict the query set \(_{}\) as follows:

\[_{}(,,)_{(q,}^{(q)})_{}}d}^{(q) },g_{}^{(q)}_{}^{*}\ \ _{ }^{*}=_{}-_{_{} }_{}(,;}_{})\] (2)

where \(>0\) is the step size for the adaptation. One can easily extend the latent optimization to obtain \(_{}^{*}\) with more than one gradient step where we found a single step adaptation is already quite effective yet showing computation efficiency compared to multiple iterations. Furthermore, we found that it is important to use the second-order gradients for the adaptation, i.e., backpropagation on the decoder adaptation gradient when optimizing the loss function, which enables the Transformer encoder to better amortize for the reconstruction task. Note that this gradient calculation on the decoder does not increase the computation too much, as using a smaller decoder size (compared to the encoder) is the key to the success of MAE .

**Task contrastive learning.** To further exploit the benefit of the gradient-based meta-learning, we suggest nudging the amortized latent to be as close as possible to the further optimized latent in Eq. (2). By doing so, the Transformer encoder is guided to better encode the reconstruction task knowledge as the optimized latent is further adapted with support and the nearby tokens. To effectively implement this concept, we utilize the idea of task contrastive learning . Specifically, as both the optimized and amortized latents originate from the same task, we maximize the latent similarity within the same task while minimizing the similarity with other task latents.

Formally, let \(_{}\) and \(_{}^{*}\) be the amortized latent and optimized latent of the given input \(\) from a mini-batch \(\), respectively. We then use a non-linear projection network \(h_{}\) and the average set pooling of latent tokens to obtain task-specific representation \(_{}=h_{}(_{}) \) and \(_{}^{*}=h_{}(_{}^{*})\) for the task contrastive learning. For a given set of task-specific representations \(=_{}\{_{}, _{}^{*}\}\), the task contrastive objective is defined as follows:

\[_{}(,,) l_{}(_{};_{ }^{*},\{_{}^{*}\})+l_{ }(_{}^{*};_{}, \{_{}\})\] (3) \[ l_{}(;^{+},\{ ^{-}\})-(,^{+})/}{(,^{+})/ +_{^{-}}(,^{-}) /}\] (4)

where \((,^{})^ {}/||\|^{}\|\) be the cosine similarity and \(>0\) is the temperature hyperparameter. From the perspective of contrastive representation learning, our task contrastive framework can be viewed as augmenting the positive pair. However, instead of using domain-specific inductive biases, we leverage gradient adaptation, thereby showing the possibilities of extending prior contrastive learning methods to modality-agnostic SSL frameworks.

**Overall meta-learning objective.** In the end, we derive a final training objective, \(_{}\): a meta-learning objective combining the latent adaptation Eq. (2) and the task contrastive learning Eq. (4). For a given hyper-parameter \(>0\), the meta-objective of MetaMAE becomes:

\[_{}(,,,)_{}(,,)+_{}( ,,)\] (5)

## 3 Experiments

In this section, we demonstrate the effectiveness of the proposed framework by measuring the linear-evaluation performance under various datasets across modalities. We first describe our experimental setup (Section 3.1), and then we present the main experimental results (Section 3.2). We provide ablation studies regarding MetaMAE (Section 3.3).

### Experimental Setup

We here briefly describe overall experimental setups. We provide further details of pretraining, evaluation, and hyperparameters in Appendix A.

**Datasets.** We select 8 sub-benchmarks from the DABS 2.0 benchmark , with categorizing the modalities for each sub-benchmark. We pretrain and transfer MetaMAE on the selected datasets:

* **Time-series** modality consists of datasets where the data is organized sequentially over time. In this paper, we use the PAMAP  dataset, which contains sensor signals from physical activity.
* **Tabular** modality refers to datasets where the data is structured in a table format, with rows (for instances) and columns (for attributes). We use the HIGGS  dataset from particle physics.
* **Multi-spectral (MS) Image** modality contains multi-channel 2D image datasets. We use the EuroSAT [34; 35] dataset, which consists of 13-channel satellite images.
* **Token** modality features datasets consisting of sequences of discrete units, similar to natural languages. We pretrain MetaMAE on both (a) the Genomics  dataset, subsequently transferring the learned model to the Genomics and Genomics-OOD datasets; and (b) the Pfam  dataset of proteins, followed by transfer learning to several tasks from the TAPE benchmarks , including Pfam, SCOP , Secondary Structure [43; 8], Stability , and Fluorescence .
* **Speech** modality includes 2D spectrograms of audio datasets. We pretrain MetaMAE on LibriSpeech , a large English audiobook corpus, and then transfer the model to datasets including LibriSpeech, Audio MNIST , Fluent Speech , Google Speech , and VoxCeleb1 .
* **RGB Image** modality comprises 3-channel 2D image datasets. We pretrain MetaMAE on (a) the ImageNet32  dataset, which is scaled to \(32 32\), and transfer the pretrained model to datasets including CIFAR-10 , CUB , VGG Flowers , DTD , Traffic Sign , and Aircraft ; and (b) the WaferMap  dataset.
* **Vision-Language** modality comprises a combination of 3-channel 2D image and sequences of English text descriptions. We pretrain MetaMAE on MSCOCO , and then transfer the model to mismatched-caption detection  and the Visual Question Answering (i.e., VQA) tasks .

Note that the transferred datasets can be in-domain (i.e., same dataset) or cross-domain (i.e., different dataset). The details of the benchmarks are described in Appendix C.

**Baselines.** For the main experiments, we compare MetaMAE's performance with existing modality-agnostic self-supervised learning methods suggested by DABS 1.0 , and 2.0 :

* \(e\)**-Mix** is a generalized version of \(i\)-Mix , designed to consistently apply methods across both discrete and continuous domains by applying the mixup strategy in the embedding space.
* **ShED** is a generalized version of ELECTRA . ShED constructs the pretext task, which involves predicting shuffled embeddings.
* **Capri** applys contrastive learning to the token level representation by randomly masking the token and treating different tokens as negative pairs.
* **MAE** aims to reconstruct the input. However, here, MAE employs a linear decoder for the continuous domain and no decoder for the discrete domain.

Additionally, we regard the randomly initialized encoder, referred to as the Baseline, as one of the baseline to check the effectiveness of self-supervised pretraining.

**Architectures.** Following [83; 85], we use 12 layers for the transformer encoder with the hidden size 256, and 8 attention heads. For the decoder, we fix the hidden size 128, and 4 attention heads. However, we choose an appropriate number of layers for the decoder to demonstrate the effect of the decoder for MAE. We also utilize different hyperparameters for each modality as other baselines, but we find that the hyperparameters can be shared across modalities (See Appendix B).

**Pretraining and transfer learning.** To evaluate our method, we pretrain each dataset \(100\)K iterations and \(100\) epochs transfer learning, overall experiments by following . We pretrain entire networks, i.e., encoder \(f_{}\), decoder \(g_{}\), and projection header \(h_{}\), but we utilize only the frozen encoder \(f_{}\) on transfer learning. When pretraining, the masking ratio can differ from the datasets. For the masking ratio hyper-parameter, we choose the best value among candidates suggested by the prior work .

### Main Experiments

**In-domain linear evaluation.** We evaluate the pretrained representation on each in-domain downstream classification task. We report the performance of a linear classifier trained on top of the frozen features. The results in Table 1 demonstrate that our proposed method, MetaMAE, achieves state-of-the-art performance across the entire dataset. For instance, we obtain 16% accuracy gain (53.6% \(\) 69.4%) on Genomics. Moreover, we note that MAE has achieved moderate performance compared to other self-supervised learning (SSL) methods on these benchmarks, but MetaMAE demonstrates the ability to enhance MAE and outperform other SSL approaches. For example, MetaMAE achieves the best performance on Pfam (44.7% \(\) 62.3%) and LibriSpeech (60.2% \(\) 79.8%) with significant improvement, here is where MAE reported in  (i.e., MAE with linear decoder) was not the best among baselines.

  
**Modality** & **Time-series** & **Tabular** & **MS Image** &  & **Speech** & **RGB Image** \\ 
**Dataset** & PAMAP2 & HIGGS & EuroSAT & Genom & Pfam & Libri & WaferMap \\    \\  Baseline & 69.8\({}^{}\) & 54.8\({}^{}\) & 62.3\({}^{}\) & 37.2\({}^{}\) & 30.1 & 17.1\({}^{*}\) & 77.7\({}^{}\) \\    \\  \(e\)-Mix & 80.1 & 65.7 & 87.4 & 40.5 & 31.3 & 60.2 & 92.6 \\ ShED & 85.2 & 68.0\({}^{}\) & 61.5\({}^{}\) & 33.6 & 54.7 & 34.8\({}^{*}\) & 92.4\({}^{}\) \\ Capri & - & - & 67.4\({}^{}\) & 23.5\({}^{}\) & 27.4 & 25.4 & 92.5\({}^{}\) \\ MAE & 85.3\({}^{}\) & 70.0\({}^{}\) & 86.3\({}^{}\) & 53.6 & 44.7 & 46.0 & 93.9\({}^{}\) \\
**MetaMAE** & **89.3** & **71.5** & **88.5** & **69.4** & **62.3** & **79.8** & **95.5** \\   

Table 1: In-domain linear evaluation performance across multiple modalities. We report F1-score (%) for WaferMap and the classification accuracy (%) for the rest. MS Image indicates the Multi-spectral image modality. \({}^{*}\), and \({}^{}\) denote the results from the DABS 1.0, and DABS 2.0 paper, respectively, where - of Capri results indicates that the pretraining loss divergence as described in .

**Cross-domain linear evaluation.** We evaluate our method on a diverse set of cross-domain downstream tasks including both classification and regression. We employ a linear classifier, or regressor trained on the frozen features as the in-domain setup. Table 2 shows that MetaMAE outperforms all the baselines across all the benchmarks consistently, except for one specific dataset. For example, we obtain 9% accuracy gain (80.4% \(\) 89.5%) on the linear classification performance of transfer setup from LibriSpeech to Audio MNIST. It is important to note that cross-domain downstream tasks, due to their wider range of variations for each domain, are typically more challenging to consistently excel in compared to in-domain tasks. This significant performance improvement demonstrates the applicability of MetaMAE in various cross-domain transfer learning scenarios across the modalities.

**Multi-modal dataset evaluation.** One important future direction for the modality-agnostic SSL research community is to bind all modalities under a singular model . Here, we believe MetaMAE can be quite a promising method to tackle this problem, e.g., managing multiple modalities on a single model supplemented by domain-specific embedding modules. To this end, we verify the possibility of MetaMAE for tackling unified multi-modal self-supervised learning. As shown in Table 3, MetaMAE outperforms other modality-agnostic SSL methods on the vision-language tasks where we believe this multi-modal learning ability can help when unifying the modalities for SSL.

### Ablation study

We perform an ablation study on six modalities: time-series (PAMAP2), tabular (HIGGS), speech (LibriSpeech), multi-spectral image (EuroSAT), and token (Pfam and Genomics). Throughout this section, we report the in-domain linear classification accuracy (%), unless otherwise specified.

**Component analysis.** In Table 4, we demonstrate the necessity of each component in MetaMAE by adding each component one by one: Deeper decoder \(g_{}\) with a Transformer architecture, latent optimization via gradient-based meta-learning, and the task contrastive loss \(_{}\). We first found

    & & &  \\ 
**Pretrain data** & **Transfer data** & Baseline & e-Mix & ShED & Capri & MAE & **MetaMAE** \\   & VQA & 53.4 & 57.6 & 53.1 & 52.9 & 54.2 & **69.7** \\  & Mismatched-caption & 49.8 & 50.1 & 50.6 & 49.6 & 49.3 & **70.5** \\   

Table 3: Linear classification accuracy (%) pretrained on a vision-language dataset, MSCOCO.

    & & &  \\ 
**Pretrain data** & **Transfer data** & Baseline & e-Mix & ShED & Capri & MAE & **MetaMAE** \\   & SCOP & 8.0 & 5.7 & 10.7 & 2.0 & 7.9 & **11.8** \\  & Secondary & 52.4 & 53.7 & **67.6** & 49.5 & 62.5 & 65.9 \\  & Stability & 0.31 & 0.39 & **0.53** & 0.26 & 0.40 & **0.53** \\  & Fluorescence & 0.04 & 0.20 & 0.27 & 0.06 & 0.06 & **0.31** \\   & Audio MNIST & 33.1\({}^{*}\) & 80.4\({}^{*}\) & 67.3\({}^{*}\) & 53.6 & 45.1 & **89.5** \\  & Fluent Loc & 62.1\({}^{*}\) & 60.9\({}^{*}\) & 60.2\({}^{*}\) & 59.8 & 61.7 & **66.7** \\  & Fluent Act & 26.2\({}^{*}\) & 29.9\({}^{*}\) & 30.5\({}^{*}\) & 28.3 & 26.8 & **38.4** \\  & Fluent Obj & 30.1\({}^{*}\) & 39.9\({}^{*}\) & 39.4\({}^{*}\) & 33.1 & 32.0 & **49.3** \\  & Google Speech & 4.9\({}^{*}\) & 19.2\({}^{*}\) & 20.7\({}^{*}\) & 13.7 & 9.5 & **46.8** \\  & VoxCeleb1 & 0.6\({}^{*}\) & 2.4\({}^{*}\) & 2.8\({}^{*}\) & 1.6 & 1.6 & **7.4** \\   & CIFAR-10 & 24.2\({}^{*}\) & 39.4\({}^{*}\) & 39.6\({}^{*}\) & 48.7 & 46.0 & **59.2** \\  & CUB & 1.6\({}^{*}\) & 3.9\({}^{*}\) & 3.0\({}^{*}\) & 3.7 & 3.1 & **6.3** \\   & VGG Flowers & 9.0\({}^{*}\) & 26.0\({}^{*}\) & 13.0\({}^{*}\) & 18.6 & 22.2 & **36.3** \\   & DTD & 7.4\({}^{*}\) & 8.8\({}^{*}\) & 18.4\({}^{*}\) & 14.7 & 14.2 & **20.9** \\   & Traffic Sign & 14.3\({}^{*}\) & 65.1\({}^{*}\) & 27.5\({}^{*}\) & 28.0 & 32.0 & **67.1** \\   & Aircraft & 2.7\({}^{*}\) & 10.2\({}^{*}\) & 5.6\({}^{*}\) & 6.4 & 5.9 & **16.4** \\   

Table 2: Cross-domain linear evaluation performance across multiple modalities. We report the Spearman correlation for Stability and Fluorescence datasets, and the classification accuracy (%) for the rest. \({}^{*}\) denote the results from the DABS 1.0 paper.

that incorporating a deep decoder is a critical component in our framework, enabling domain-agnostic capabilities similar to the success of MAE on the image domain . Thus, we here suggest that improving MAE for the domain-agnostic is quite a promising direction to explore.

In addition, Table 4 verifies the contribution of meta-learning schemes to the performance of MetaMAE. We found that the gradient-based latent optimization rule, which includes the utilization of Nearby-\(\), is more beneficial. We also confirm that task contrastive learning is a critical component in our framework like recent meta-learning frameworks . Note that this task contrastive learning scheme is exclusively applicable in gradient-based approaches, emphasizing the significance of the gradient-based latent optimization method for MetaMAE.

**Importance of decoder size for MAE.** To verify the effect of decoder size for MAE, we evaluate the linear evaluation accuracy on datasets where the original MAE (i.e., no decoder) performed worse than other baselines. As Table 5 shows, we found that MAE can achieve the best performance compared to baselines by choosing the proper decoder size, yet there is room for enhancement as shown in Table 4. This result demonstrates the superiority of MAE for tackling modality-agnostic SSL problems, where we believe the development of MAE would be an important direction to investigate. In this respect, we believe MetaMAE will serve as an important baseline in this field.

**Nearby supports.** We further analyze the effect of \(r\), i.e., the Nearby-\(\) ratio. We conduct the experiment with \(r\{0,0.1,0.5,1.0\}\). We note that \(r=0\) indicates the gradient updates without any help of queries (i.e., direct reconstruction of \(\)), and \(r=1\) denotes the gradient updates with the entire queries near the \(\). As shown in Table 6, this approach is found to be beneficial compared to the direct reconstruction, and the small ratio is suggested to be proper \(r\), e.g., \(r=0.1\) is the best. This is because it effectively bridges the gap between the latent representation and the latents of masked tokens, thereby enhancing the encoding of knowledge required for the reconstruction task.

**Computational efficiency.** MetaMAE might be perceived as compute-inefficient when incorporating MAE due to the computational demands of second-order gradients; however, our findings suggest otherwise. Although MetaMAE increases the total training time of MAE by approximately 1.4 times (with the one-step adaptation), we have observed that it is much faster to achieve the best performance of MAE: in Figure 2, we compare the accuracy under the same training wall-clock time with MAE, e.g., 1.9 times faster on PAMAP2 dataset.

   \(r\) ratio & PAMAP2 & HIGGS & Pfam \\ 
0 & 87.5 & 71.1 & 62.0 \\
0.1 & **89.3** & **71.5** & **62.3** \\
0.5 & 88.2 & 70.8 & 62.0 \\
1.0 & 84.2 & 70.1 & 62.1 \\   

Table 6: Effect of the nearby token (i.e., Nearby-\(\)) selection ratio \(r\) on the classification accuracy (%). We use three different datasets across modalities.

   Decoder & Gradient-based & Task contrast & PAMAP2 & Genomics & EuroSAT & LibriSpeech & HIGGS & Pfam \\  \(\) & \(\) & \(\) & 85.3 & 53.6 & 86.3 & 33.3 & 70.0 & 44.7 \\ \(\) & \(\) & \(\) & 86.5 & 65.2 & 87.4 & 64.1 & 70.5 & 61.3 \\ \(\) & \(\) & \(\) & 88.3 & **69.4** & 87.4 & 64.5 & 71.1 & 61.3 \\  \(\) & \(\) & \(\) & **89.3** & **69.4** & **88.5** & **79.8** & **71.5** & **62.3** \\   

Table 4: Ablation study on each component of MetaMAE, namely the use of the decoder, latent adaptation using gradient-based meta-learning (Gradient-based), and task contrastive learning (Task contrast). We report the classification accuracy (%) across six different modalities.

   decoder size & EuroSAT & Pfam & LibriSpeech \\  _prev. best_ & **87.4** & 54.7 & 60.2 \\ 
0 & 86.3 & 44.7 & 33.3 \\
2 & 86.7 & **61.4** & 68.1 \\
4 & **87.4** & 61.3 & 64.1 \\
6 & 86.7 & **61.4** & **74.1** \\   

Table 5: Effect of the decoder size of MAE on the classification accuracy (%). We use three different datasets across modalities.

Figure 2: Computation efficiency comparison of MAE and MetaMAE. We report the pretraining wall clock time.

Related work

**Self-supervised learning (SSL).** SSL, i.e., learning without human supervision, recently has demonstrated substantial success across fields including, computer vision , natural language processing (NLP) , and speech recognition , frequently show better transferability and generalization ability compared to conventional pretraining methods, e.g., training on labeled datasets . To learn such representation, SSL optimizes the loss on a pretext task that does not require any human labels. For instance, pioneer works for SSL proposed such tasks based on data reconstruction through auto-encoding  such as context prediction , and in-painting . Later on, multiple SSL works found that utilizing the domain-specific inductive biases can effectively learn representations in a self-supervised manner, including, colorization , solving jigsaw puzzles , counting the number of objects , and rotation prediction , to name a few. More recently, contrastive representation learning has garnered significant attention in SSL . This technique maximizes the similarities of similar (i.e., positive) pairs and minimizes the similarities of dissimilar (i.e., negative) pairs, rather than focusing on training an instance classifier. To generate such positive pairs, multiple works rely on domain-specific inductive biases such as data augmentations , i.e., different augmented views as a positive pair. In addition, recent advances have been made with the development of various architectural components: e.g., Siamese networks , self-distillation , asymmetric architectures , and utilization of Transformer architectures . Despite the success of these strategies, most existing SSL frameworks rely heavily on domain-specific inductive biases, which limits their applicability to new modalities.

**Modality-agnostic SSL.** Recently, several streams of work have emerged focusing on the development of more generalized SSL methods, specifically modality-agnostic SSL. For example, DACL  and \(i\)-Mix  utilize the idea of mixup  to propose domain-agnostic contrastive learning, and \(e\)-Mix  generalizes the concept of \(i\)-Mix to be embedding-level instead of input-level. Capri , as a variant of CPC , contrasts the predicted representations from randomly masked tokens.  develops generative models to learn data-dependent distortions for contrast. Instead of contrastive learning, ShED  (a generalized version of ELECTRA ) constructs the pretext task of replacing token detection with a masking strategy. DABS 2.0  proposes a method to generalize MAE  to be modality-agnostic. In their approach, however, decoders are not utilized for discrete domains like BERT , while only a linear decoder is employed for continuous domains. In this paper, we suggest an effective modality-agnostic latent optimization for learning representations by interpreting masked prediction for MAE  in a novel manner.

**Masked Auto-Encoder (MAE).** MAE , i.e., predicting the masked parts with a given unmasked parts, has been extended to multiple applications  across various domains . Among them, the recent combination of MAE with Transformer architecture  has shown promise in tackling SSL scenarios. For instance, BERT  utilized MAE for natural language processing (NLP) tasks, incorporating a linear layer into its architecture. Furthermore, multiple variants of MAE show impressive performance in various domains, by suggesting modality-agnostic SSL , architecture-agnostic SSL , multi-modal pretraining , and generative pretraining frameworks . In this paper, we focus on improving the most basic form of mask-modeling (i.e., MAE) for constructing a modality-agnostic SSL framework which remains under-explored, despite its potential significance, through the lens of meta-learning. It is worth noting that our interpretation of viewing MAE as a meta-learning framework can be applied to any other masked-modeling-based SSL frameworks where we believe combining our meta-learning regularization to such SSL methods would be an interesting direction to explore.

**Meta-learning.** Meta-learning , i.e., learning to learn by extracting common knowledge over a task distribution, has emerged as a popular paradigm for enabling systems to adapt to new tasks in a sample-efficient way. Under various applications across domains (e.g., computer vision , natural language processing , and robotics ), there have been significant efforts to design a variety of meta-learning schemes, including gradient-based  and amortization-based approaches  such as metric-based , and neural processes . Typically, recent works have combined gradient-based meta-learning (or iterative functional update) with amortization-based schemes to enhance adaptation performance . Furthermore, there have been varieties of amortization-based schemes (such as neural processes) that utilize the recent success of contrastive learning into meta-learning, i.e., task contrastive learning . In this paper, we interpret MAE as an amortization-based meta-learning, which is further enhanced via the benefit of model-agnosticism of gradient-based meta-learning and task contrastive learning.

Discussion and conclusion

In this paper, we tackle modality-agnostic self-supervised learning (SSL), an important problem of SSL that consists of multiple real-world applications. To this end, we explore the possibilities of the Masked Auto-Encoder (MAE) in tackling modality-agnostic SSL which is quite under-explored, despite its potential. We propose MetaMAE, a novel and effective SSL framework that enhances MAE with meta-learning. Our key idea is to interpret mask reconstruction task of MAE as a meta-learning task, which allows us to treat MAE as a meta-learning framework. Based on this novel interpretation, we suggest a unique integration with advanced modality-agnostic meta-learning methods to improve the generalization of MAE. Our experiments demonstrate that MetaMAE significantly improves the performance of modality-agnostic SSL approaches across a diverse range of modalities.

**Limitations and future work.** While MetaMAE becomes a state-of-the-art approach for modality-agnostic SSL problems, it still inherits a general limitation of the MAE, namely the modality-specific masking ratio, i.e., the masking ratio may differ across modalities. This is due to our shared design elements with MAE, which include masking, encoding, and decoding. Recent works propose design choices for the masking scheme [50; 94], including automation, where incorporating these ideas into MetaMAE would be an intriguing future research direction, potentially enhancing our approach to be an even more effective modality-agnostic SSL framework.

**Potential negative impacts.** SSL often requires a large computation and a large network capacity, therefore raising environmental concerns, e.g., carbon generation . As MetaMAE is built upon the SSL method (i.e., MAE), practitioners may need to consider some computation for successful training. To address this issue, efficient training methods [80; 40], distilling knowledge to a smaller network , or network sparsity schemes [31; 46] would be required to ameliorate such problems.