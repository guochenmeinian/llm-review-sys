# Testing Calibration in Nearly-Linear Time

Lunjia Hu

Harvard University

lunjia@alumni.stanford.edu &Arun Jambulapati

University of Michigan

jmblpati@gmail.com &Kevin Tian

University of Texas at Austin

kjtian@cs.utexas.edu &Chutong Yang

University of Texas at Austin

cyang98@utexas.edu

###### Abstract

In the recent literature on machine learning and decision making, _calibration_ has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by , which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of _calibration testing_ from samples where given \(n\) draws from a distribution \(\) on (predictions, binary outcomes), our goal is to distinguish between the cases where \(\) is perfectly calibrated or \(\)-far from calibration. We make the simple observation that the empirical smooth calibration linear program can be reformulated as an instance of minimum-cost flow on a highly-structured graph, and design an exact dynamic programming-based solver for it which runs in time \(O(n^{2}(n))\), and solves the calibration testing problem information-theoretically optimally in the same time. This improves upon state-of-the-art black-box linear program solvers requiring \((n^{})\) time, where \(>2\) is the exponent of matrix multiplication. We also develop algorithms for tolerant variants of our testing problem improving upon black-box linear program solvers, and give sample complexity lower bounds for alternative calibration measures to the one considered in this work. Finally, we present experiments showing the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale efficiently to accommodate large sample sizes.

## 1 Introduction

Probabilistic predictions are at the heart of modern data science. In domains as wide-ranging as forecasting (e.g. predicting the chance of rain from meteorological data ), medicine (e.g. assessing the likelihood of disease ), computer vision (e.g. assigning confidence values for categorizing images ), and more (e.g. speech recognition  and recommender systems ), prediction models have by now become essential components of the decision-making pipeline. Particularly in the context of critical, high-risk use cases, the interpretability of prediction models is therefore paramount in downstream applications. That is, how do we assign meaning to the predictions our model gives us, especially when the model is uncertain?

We focus on perhaps the most ubiquitous form of prediction modeling: binary predictions, represented as tuples \((v,y)\) in \(\{0,1\}\) (where the \(v\) coordinate is our prediction of the likelihood of an event, and the \(y\) coordinate is the observed outcome). We model prediction-outcome pairs in the binary prediction setting by a joint distribution \(\) over \(\{0,1\}\), fixed in the following discussion. Inthis context, _calibration_ of a predictor has emerged as a basic desideratum. A prediction-outcome distribution \(\) is said to be calibrated if

\[_{(v,y)}[y v=t]=tt. \]

That is, calibration asks that the outcome is \(1\) exactly \(60\%\) of the time, when the model returns a prediction \(v=0.6\). While calibration (or approximate variants thereof) is a relatively weak requirement on a meaningful predictor, as it can be achieved by simple models,1 it can still be significantly violated in practice. For example, interest in calibration in the machine learning community was spurred by , which observed that many modern deep learning models are far from calibrated. Moreover, variants of calibration have been shown to have strong postprocessing properties for fairness constraints and loss minimization , which has garnered renewed interest in calibration by the theoretical computer science and statistics communities.

The question of measuring the calibration of a distribution is subtle; even a calibrated distribution incurs measurement error due to sampling. For example, consider the _expected calibration error_, used in e.g.  as a ground-truth measure of calibration:

\[():=_{(v,y)}[| _{(v^{},y)}[y v^{}=v]-v |].\]

Unfortunately, the empirical \(\) is typically meaningless; if the marginal density of \(v\) is continuous, we will almost surely only observe a single sample with each \(v\) value. Further,  observed that \(\) is discontinuous in \(v\). In practice, binned variants of \(\) are often used as a proxy, where a range of \(v\) is lumped together in the conditioning event. However, hyperparameter choices (e.g. the number of bins) can significantly affect the quality of binned \(\) variants as a distance measure .2 Moreover, as we explore in this paper, binned calibration measures inherently suffer from larger sample complexity-to-accuracy tradeoffs, and are less faithful to ground truth calibration notions in experiments than the calibration measures we consider.

Recently,  undertook a systematic study of various measures of distance to calibration proposed in the literature. They proposed information-theoretic tractability in the _prediction-only access (PA) model_, where the calibration measure definition can only depend on the joint prediction-outcome distribution (rather than the features of training examples),3 as a desirable criterion for calibration measures. Correspondingly,  introduced Definition 1 as a ground-truth notion for measuring calibration in the PA model, which we also adopt in this work.4

**Definition 1** (Lower distance to calibration).: _Let \(\) be a distribution over \(\{0,1\}\). The lower distance to calibration (LDTC) of \(\), denoted \(}()\), is defined by_

\[}():=_{()} _{(u,v,y)}|u-v|,\]

_where \(()\) is all distributions \(\) over \((u,v,y)\{0,1\}\) satisfying the following._

* _The marginal distribution of_ \((v,y)\) _is_ \(\)_._
* _The marginal distribution_ \((u,y)\) _is perfectly calibrated, i.e._ \(_{}[y|u]=u\)_._

Definition 1 has various beneficial aspects: it is convex in \(v\), computable in the PA model, and (as shown by ) polynomially-related to various other calibration measures, including some which require feature access, e.g. the _distance to calibration_ (DTC, Eq. (1), ). Roughly, the DTC of a distribution is the tightest lower bound on the \(_{1}\) distance between \(v\) and any calibrated function of the features, _after taking features into account_. The LDTC is the analog of this feature-aware measure of calibration when limited to the PA model, so it does not depend on features. We focus on Definition 1 as our ground-truth measure in the remainder of the paper.

### Our results

We initiate the algorithmic study of the _calibration testing_ problem, defined as follows.

**Definition 2** (Calibration testing).: _Let \(\). We say algorithm \(\) solves the \(\)-calibration testing problem with \(n\) samples, if given \(n\) i.i.d. draws from a distribution \(\) over \(\{0,1\}\), \(\) returns either "yes" or "no" and satisfies the following with probability \(\).5_

* \(\) _returns "no" if_ \(}()\)_._
* \(\) _returns "yes" if_ \(}()=0\)_._

_In this case, we also call \(\) an \(\)-calibration tester._

To our knowledge, we are the first to formalize the calibration testing problem in Definition 2, which is natural from the perspective of _property testing_, an influential paradigm in statistical learning . In particular, there is an \(_{n}=(n^{-1/2})\) so that it is information-theoretically impossible to solve the \(_{n}\)-calibration testing problem from \(n\) samples (see Lemma 5), so a variant of Definition 2 with an exact distinguishing threshold between "calibrated/uncalibrated" is not tractable. Hence, Definition 2 only requires distinguishing distributions \(\) which are "clearly uncalibrated" (parameterized by a threshold \(\)) from those which are perfectly calibrated.

We note that a variant of Definition 2 where \(}\) is replaced by variants of the \(\) was recently proposed by . However, due to the aforementioned discontinuity and binning choice issues which plague the \(\),  posed as an explicit open question whether an alternative calibration metric makes for a more appropriate calibration testing problem, motivating our Definition 2. Indeed, Proposition 9 of  shows that without smoothness assumptions on the data distribution, it is impossible to solve the \(\) calibration testing problem from finite samples.6

Our first algorithmic contribution is a nearly-linear time algorithm for calibration testing.

**Theorem 1**.: _Let \(n\), and \(=(_{n})\), where \(_{n}=(n^{-1/2})\) is minimal such that it is information-theoretically possible to solve the \(_{n}\)-calibration testing problem (Definition 2) with \(n\) samples. There is an algorithm which solves the \(\)-calibration testing problem with \(n\) samples, running in time \(O(n^{2}(n))\)._

The lower bound on the acceptable range of \(_{n}\) in Theorem 1 is well-known, and recalled in Lemma 5 for completeness. Our main contribution is to prove the upper bound (i.e., achieving \(O(_{n})\)-calibration testing) in Theorem 1 by designing a new algorithm for computing \((}_{n})\), the _smooth calibration error_ (Definition 3), an alternative calibration measure, of an empirical distribution \(}_{n}\).

**Definition 3** (Smooth calibration error).: _Let \(W\) be the set of Lipschitz functions \(w:[-1,1]\). The smooth calibration error of distribution \(\) over \(\{0,1\}\), denoted \(()\), is_

\[()=_{w W}|_{(v,y)}[(y-v)w(v)]|.\]

It was shown in  that \(()\) is a constant-factor approximation to \(}()\) for all \(\) on \(\{0,1\}\) (see Lemma 4). Additionally, the empirical \(\) admits a representation as a linear program with an \(O(n) O(n)\)-sized constraint matrix encoding Lipschitz constraints.7 Thus,  proposed a simple procedure for estimating \(()\): draw \(n\) samples from \(\), and solve the associated linear program on the empirical distribution. While there have been significant recent runtime advances in the linear programming literature , \(\), \(^{+}21\)], all state-of-the-art black-box linear programming algorithms solve linear systems involving the constraint matrix, which takes \((n^{})\) time, where \(>2.371\) is the current exponent ofmatrix multiplication. Even under the best-possible assumption that \(=2\), the strategy of exactly solving a linear program represents an \((n^{2})\) quadratic runtime barrier for calibration testing.

We bypass this barrier by noting that the smCE linear program is highly-structured, and can be reformulated as minimum-cost flow on a planar graph. We believe this observation is already independently interesting, as it opens the door to using powerful software packages designed for efficiently solving flow problems to measure calibration in practice. Moreover, using recent theoretical breakthroughs in graph algorithms  as a black box, this observation readily implies an \(O(n(n))\)-time algorithm for solving the smooth calibration linear program.

However, these aforementioned algorithms are quite complicated, and implementations in practice are not available, leaving their relevance to empirical calibration testing unclear at the moment. Motivated by this, in Section 2 we develop a custom solver for the minimum-cost flow reformulation of empirical smooth calibration, based on dynamic programming. Our theoretical runtime improvement upon  is by at least a large polylogarithmic factor, and moreover our algorithm is simple enough to implement in practice, where it attains faster runtimes than general-purpose commercial solvers on moderate or large dataset sizes, evaluated in Section 3.

We further define a _tolerant_ variant of Definition 2 (see Definition 4), where we allow for error thresholds in both the "yes" and "no" cases; "yes" is the required answer when \(}()_{2}\), and "no" is required when \(}()_{1}\). Our algorithm in Theorem 1 continues to serve as an efficient tolerant calibration tester when \(_{1} 4_{2}\), formally stated in Theorem 3. This constant-factor loss comes from a similar loss in the relationship between smCE and dCE, see Lemma 4. We make the observation that a constant factor loss in the tolerant testing parameters is inherent following this strategy, via a lower bound in Lemma 13. Thus, even given infinite samples, computing the smooth calibration error cannot solve tolerant calibration testing all the way down to the information-theoretic threshold \(_{1}_{2}\). To develop an improved tolerant calibration tester, we directly show how to approximate the LDTC of an empirical distribution, our second main algorithmic contribution.

**Theorem 2** (Informal, see Theorem 4, Corollary 3).: _Let \(n\), and let \(_{1}-_{2}=(_{n})\), where \(_{n}=(n^{-1/2})\) is minimal such that it is information-theoretically possible to solve the \(_{n}\)-calibration testing problem (Definition 4) with \(n\) samples. There is an algorithm which solves the \((_{1},_{2})\)-tolerant calibration testing problem with \(n\) samples, running in time \(O(-_{2})^{2}})=O(n^{2}(n))\)._

While Theorem 2 is slower than Theorem 1, it directly approximates the LDTC, making it applicable to tolerant calibration testing. We mention that state-of-the-art black-box linear programming based solvers, while still applicable to (a discretizeation of) the empirical LDTC, require \((n^{2.5})\) time , even if \(=2\). This is because the constraint matrix for the \(\)-approximate empirical LDTC linear program has dimensions \(O() O(n)\), resulting in an \(=()\) overhead in the dimension of the decision variable. We prove Theorem 2 in Appendix C, where we use recent advances in minimax optimization  and a custom combinatorial rounding procedure to develop a faster algorithm, improving state-of-the-art linear programming runtimes by an \(()\) factor.

In Appendix D, we complement our algorithmic results with lower bounds (Theorems 5, 6) on the sample complexity required to solve variants of the testing problem in Definition 2, when dCE is replaced with different calibration measures. For several widely-used distances in the machine learning literature, including binned and convolved variants of ECE, we show that \((^{-2.5})\) samples are required to the associated \(\)-calibration testing problem. This demonstrates a statistical advantage of our focus on dCE as our ground-truth notion for calibration testing.

We corroborate our theoretical findings with experimental evidence on real and synthetic data in Section 3. First, on a simple Bernoulli example, we show that dCE and smCE testers are more reliable measures of calibration than a recently-proposed binned ECE variant. We then apply our smCE tester to postprocessed neural network predictions to test their calibration levels, validating against the findings in . Finally, we implement our method from Theorem 1 on our Bernoulli dataset, showing that it scales to high dimensions and runs faster than both a linear program solver from CVXPY for computing the empirical smCE, as well as a commercial minimum-cost flow solver from Gurobi Optimization (combined with our reformulation in Lemma 2).8

### Our techniques

Theorems 1 and 2 follow from designing custom algorithms for approximating empirical linear programs associated with the smCE and dCE of a sampled dataset \(}_{n}:=\{(v_{i},y_{i})\}_{i[n]}_{}\). In both cases, generalization bounds from  show it suffices to approximate the value of the empirical calibration measures to error \(=(n^{-1/2})\), though our solver in Theorem 1 will be exact.

We begin by explaining our strategy for estimating \((}_{n})\) (Definition 3). By definition, the smooth calibration error of \(}_{n}\) can be formulated as a linear program,

\[_{x[-1,1]^{n}}_{i[n]}x_{i}(v_{i}-y_{i}),|x_{i}-x_{j}||v_{i}-v_{j}|(i,j)[n][n]. \]

Here, \(x_{i}[-1,1]\) corresponds to the weight on \(v_{i}\), and there are \(2\) constraints on the decision variable \(x\), each of which corresponds to a Lipschitz constraint. We make the simple observation that every Lipschitz inequality constraint can be replaced by two constraints of the form \(x_{i}-x_{j}|v_{j}-v_{i}|\) (with \(i,j\) swapped). Moreover, the box constraints \(x[-1,1]^{n}\) can be handled by introducing a dummy variable \(x_{n+1}\) and writing \((x_{i}-x_{n+1},x_{n+1}-x_{i}) 1\), after penalizing \(x_{n+1}\) appropriately in the objective. Notably, this substitution makes every constraint the difference of two decision variables, which is enforceable using the edge-vertex incidence matrix of a graph. Finally, the triangle inequality implies that we only need to enforce Lipschitz constraints in (2) corresponding to adjacent \(i,j\). After making these simplifications, the result is the dual of a minimum-cost flow problem on a graph which is the union of a star and a path; this argument is carried out in Lemma 2.

Because of the sequential structure of the induced graph, we show in Appendix B.2 that a dynamic programming-based approach, which maintains the minimum-cost flow value after committing to the first \(i<n\) flow variables in the graph recursively, succeeds in computing the value (2). To implement each iteration of our dynamic program in polylogarithmic time, we rely on a generalization of the classical segment tree data structure that we develop in Appendix B.3; combining gives Theorem 1.

On the other hand, the linear program corresponding to the empirical dCE is more complex (with two types of constraints), and to our knowledge lacks the graphical structure to be compatible with the aforementioned approach. Moreover, it is not obvious how to use first-order methods, an alternative linear programming framework suitable when only approximate answers are needed, to solve this problem more quickly. This is because the empirical dCE linear program enforces hard constraints to a set that is difficult to project to under standard distance metrics. To develop our faster algorithm in Theorem 2, we instead follow an "augmented Lagrangian" method where we lift the constraints directly into the objective as a soft-constrained penalty term. To prove correctness of this lifting, we follow a line of results in combinatorial optimization . These works develop a "proof-by-rounding algorithm" framework to show that the hard-constrained and soft-constrained linear programs have equal values, summarized in Appendix C.1 (see Lemma 14).

To use this augmented Lagrangian framework, it remains to develop an appropriate rounding algorithm to the feasible polytope for the empirical dCE linear program, which enforces two types of constraints: marginal satisfaction of \((v,y)\), and calibration of \((u,y)\) (using notation from Definition 1). In Appendix C.3, we design a two-step rounding procedure, which first fixes the marginals on the \((v,y)\) coordinates, and then calibrates the \(u\) coordinates without affecting any \((v,y)\) marginal.

### Related work

The calibration performance of deep neural networks has been studied extensively in the literature (e.g. ). Measuring the calibration error in a meaningful way can be challenging, especially when the predictions are not naturally discretized (e.g. in neural networks). Recently,  addresses this challenge using the _distance to calibration_ as a central notion. They consider a calibration measure to be _consistent_ if it is polynomially-related to the distance to calibration. Consistent calibration measures include the smooth calibration error , Laplace kernel calibration error , interval calibration error , and convolved ECE .9On the algorithmic front, substantial observations were made by  on linear programming characterizations of calibration measures such as the LDTC and smooth calibration. While there have been significant advances on the runtime frontier of linear programming solvers, current runtimes for handling an \(n d\) linear program constraint matrix with \(n d\) remain \(((nd+d^{2.5},n^{}))\)[13, vdBLL\({}^{+}\)21, JSWZ21]. Our constraint matrix is roughly-square and highly-sparse, so it is plausible that e.g. the recent research on sparse linear system solvers  could apply to the relevant Newton's method subproblems and improve upon these rates. Moreover, while efficient estimation algorithms have been proposed by  for (surrogate) interval calibration error and by  for convolved ECE, these algorithms require suboptimal sample complexity for solving our testing task in Definition 2 (see Appendix D). To compute their respective distances to error \(\) from samples, these algorithms require \((^{-5})\) and \((^{-3})\) time. As comparison, under this parameterization Theorems 1 and 2 require \((^{-2})\) and \((^{-4})\) time, but can solve stronger testing problems with the same sample complexity, experimentally validated in Section 3.

Notation.Throughout, \(\) denotes a distribution over \(\{0,1\}\). When \(\) is clear from context, we let \(}_{n}=\{(v_{i},y_{i})\}_{i[n]}\) denote a dataset of \(n\) independent samples from \(\) and, in a slight abuse of notation, the distribution with probability \(\) for each \((v_{i},y_{i})\). We say \(\) is a _calibration measure_ if it takes distributions on \(\{0,1\}\) to the nonnegative reals \(_{ 0}\), so \(\) (Definition 1) and \(\) (Definition 3) are both calibration measures. We use \(\) and \(\) to hide polylogarithmic factors in the argument. We denote \([n]:=\{i i n\}\). We denote matrices in boldface throughout. For any \(^{m n}\), we refer to its \(i^{}\) row by \(_{i:}\) and its \(j^{}\) column by \(_{:j}\). For a set \(S\) identified with rows of a matrix \(\), we let \(_{s:}\) denote the row indexed by \(s S\), and use similar notation for columns. For a directed graph \(G=(V,E)\), we define its edge-vertex incidence matrix \(\{-1,0,1\}^{E V}\) which has a row corresponding to each \(e=(u,v) E\) with \(_{eu}=1\) and \(_{ev}=-1\). When \(G\) is undirected, we similarly define \(\{-1,0,1\}^{E V}\) with arbitrary edge orientations.

## 2 Smooth calibration

In this section, we overview our main result on approximating the smooth calibration of a distribution on \(\{0,1\}\), deferring some aspects of the proof to Appendix B. We first show that the linear program corresponding to the smooth calibration of an empirical distribution can be reformulated as an instance of minimum-cost flow on a highly-structured graph. We then explain our dynamic programming approach to solving this minimum-cost flow problem and state a runtime guarantee. Finally, we give our main result on near-linear time calibration testing, Theorem 1.

Throughout this section, we fix a dataset under consideration, \(}_{n}:=\{(v_{i},y_{i})\}_{i[n]}\{0,1\}\), and the corresponding empirical distribution (which, in an abuse of notation, we also denote \(}_{n}\)), i.e. we use \((v,y)}_{n}\) to mean that \((v,y)=(v_{i},y_{i})\) with probability \(\) for each \(i[n]\). We also assume without loss of generality that the \(\{v_{i}\}_{i[n]}\) are in sorted order, so \(0 v_{1} v_{n} 1\). Recalling Definition 3, the associated empirical smooth calibration linear program is

\[(}_{n}) :=_{x[-1,1]^{n}}b^{}x, \] \[|x_{i}-x_{j}|  v_{j}-v_{i}(i,j)[n][n]i<j,\] \[b_{i} :=(y_{i}-v_{i})i[n].\]

We first make a simplifying observation, which shows that it suffices to replace the Lipschitz constraints in (3) with only the Lipschitz constraints corresponding to adjacent indices \((i,j)\).

**Lemma 1**.: _If \(x,v^{n}\), where \(v\) has monotonically nondecreasing coordinates, and \(|x_{i}-x_{i+1}| v_{i+1}-v_{i}\) for all \(i[n-1]\), then \(|x_{i}-x_{j}| v_{j}-v_{i}\) for all \((i,j)[n][n]\) with \(i<j\)._

We now reformulate (3) as a (variant of a) minimum-cost flow problem.

**Lemma 2**.: _Consider an instance of (3). Let \(G=(V,E)\) be an undirected graph on \(n+1\) vertices labeled by \(V:=[n+1]\), and with \(2n-1\) directed edges \(E\) defined and with edge costs as follows._

* _For all_ \(i[n-1]\)_, there is an edge between vertices_ \((i,i+1)\) _with edge cost_ \(v_{i+1}-v_{i}\)* _For all_ \(i[n]\)_, there is an edge between vertices_ \((i,n+1)\) _with edge cost_ \(1\)_._

_Let \(c^{E}\) be the vector of all edge costs, let \(d^{n+1}\) be the demand vector which concatenates \(-b\) in (3) with a last coordinate set to \(_{i[n]}b_{i}\), and let \(\{-1,0,1\}^{E V}\) be the edge-vertex incidence matrix of \(G\). Then the problem_

\[_{f^{E}\\ ^{}f=d}c^{}|f|:=_{e E}c_{e}|f_{e}| \]

_has the same value as the empirical smooth calibration linear program (3)._

Proof.: By Lemma 1, solving (3) is equivalent to solving

\[_{x[-1,1]^{n}}-b^{}x,|x_{i}-x_{i+1}| v_{i+1}-v_{i }i[n-1], \]

We create a dummy variable \(x_{n+1}\), and rewrite (5) as

\[_{x^{n+1}}_{i[n]}-b_{i}(x_{i}-x_{n+1}), |x_{i}-x_{i+1}| v_{i+1}-v_{i}i[n-1], \] \[-1 x_{i}-x_{n+1} 1i[n].\]

Next, consider a directed graph \(=(V,)\) with \(4n-2\) edges which duplicate the undirected edges described in the lemma statement in both directions. Let \(}\{-1,0,1\}^{ V}\) be the edge-vertex incidence matrix of \(\), and let \(^{}\) be the edge cost vector so that both edges in \(\) corresponding to \(e E\) have the same cost \(c_{e}\). Then (6) is equivalent to the linear program \(_{x^{n+1}}d^{}x\) such that \(}x c\), where \(d\) is described as in the lemma statement. The dual of this linear program is

\[_{f^{}_{o}\\ }^{}f=d}^{}f, \]

a minimum-cost flow problem on \(\). In particular, based on the way we defined \(}\), we can check that \(}^{}f\) encodes the net flow at each vertex of \(\), which is set according to the demand vector \(d\) in the above optimization problem. Next, for each pair of directed edges \((e^{},e^{})\) in \(\) corresponding to some \(e E\), note that an optimal solution to (7) will only put nonzero flow on one of \(e^{}\) or \(e^{}\), else we can achieve a smaller cost by canceling out redundant flow. Therefore, we can collapse each pair of directed edges into a single undirected edge, where we allow the flow variable \(f\) to be negative but charge its magnitude \(|f|\) in cost, proving equivalence of (7) and (4) as claimed. 

We believe this observation in Lemma 2 is already interesting, as it lets us to use specialized graph algorithms to achieve faster runtimes in both theory and practice for solving (3). By using the special structure of the graph (the union of a star and path), we show in Appendix B that we can develop a more efficient custom algorithm for this problem. Specifically, we show how to replace the constrained problem (4) with an unconstrained problem on only the path edges, of the form

\[_{f^{n-1}}A(f):=|d_{1}+f_{1}|+|d_{n}-f_{n-1}|+_{i[n-2] }|f_{i}-f_{i+1}-d_{i+1}|+_{i[n-1]}c_{i}|f_{i}|. \]

We prove the following result in Appendix B.2.

**Proposition 1**.: _There is an algorithm which computes a minimizer \(f^{n-1}\) to \(A\) in (8), as well as the minimizing value \(A(f)\), in time \(O(n^{2}(n))\)._

Figure 1: Example graph \(G\) for \(n=5\) with \(n+1=6\) vertices and \(2n-1=9\) edges.

Our algorithm for establishing Proposition 1 is based on dynamic programming, and recursively represents partial solutions to \(A\) as a piecewise-linear function. We implement updates to this representation via a _segment tree_ data structure in polylogarithmic time, giving our overall solution.

**Corollary 1**.: _There is an algorithm which computes the value of (3) in time \(O(n^{2}(n))\)._

Proof.: This is immediate from Lemma 2, the equivalence between the constrained problem (4) and the unconstrained problem (8) established in Appendix B.2, and Proposition 1. 

We now describe how to build upon Corollary 1 to give an algorithm for proving Theorem 1, using a result from  which bounds how well the smooth calibration of an empirical distribution approximates the smooth calibration of the population.

**Lemma 3** (Corollary 9.9, ).: _For \((0,1)\), there is an \(n=O(})\) such that if \(}_{n}\) is the empirical distribution over \(n\) i.i.d. draws from \(\), with probability \(\), \(|()-(}_{n})|\)._

Further, we recall the smooth calibration error is constant-factor related to the LDTC.

**Lemma 4** (Theorem 7.3, ).: _For any distribution \(\) over \(\{0,1\}\), we have \(}()()  2}()\)._

Proof of Theorem 1.: We take \(n=O(})\) samples so Lemma 3 ensures \(|()-(}_{n})| {}{2}\) with probability \(\). We then compute \(=(}_{n})\) using Corollary 1, and return "yes" iff \(\), which distinguishes between the two cases in Definition 2 via Lemma 4. 

## 3 Experiments

In this section, we present experiments on synthetic data and CIFAR-100 supporting our argument that \(}\) and \(\) are reliable measures of calibration for use in defining a testing problem. We then evaluate our custom algorithms from Section 2 and Appendix B, showing promising results on their runtimes outperforming standard packages for linear programming and minimum-cost flow. The experiments in the first and third part of this section are run on a 2018 laptop with 2.2 GHz 6-Core Intel Core i7 processor. The experiments in the second part are run on a cluster using 2x AMD EPYC 7763 64-Core Processor and a single NVIDIA A100 PCIE 40GB.

Synthetic dataset.In our first experiment, we considered the ability of \(\)-d-testers (Definition 5) to detect the miscalibration of a synthetic dataset, for various levels of \(\{0.01,0.03,0.05,0.07,0.1\}\), and various choices of \(\{,},\}\).10 The synthetic dataset we used is \(n\) independent draws from \(\), where a draw \((v,y)\) first draws \(v_{}[0,1-^{}]\), and \(y(v+^{})\), for \(^{}:=0.01\).11 Note that \(}()=^{}=0.01\), by the proof in Lemma 13. In Table 1, where the columns index \(n\) (the number of samples), for each choice of \(\) we report the smallest value of \(\) such that a majority of \(100\) runs of an \(\)-\(\)-tester report "yes." For \(=\), we implemented our tester by running code in  to compute \(\) and thresholding at \(\). For \(\{,}\}\), we used the standard linear program solver from CVXPY  and again thresholded at \(\). We remark that the CVXPY solver, when run on the \(}\) linear program, fails to produce stable results for \(n>2^{9}\) due to the size of the constraint matrix. As seen from Table 1, both \(\) and \(}\) testers are more reliable estimators of the ground truth calibration error \(^{}\) than \(\).

  \(n\) & \(2^{6}+1\) & \(2^{7}+1\) & \(2^{8}+1\) & \(2^{9}+1\) & \(2^{10}+1\) & \(2^{11}+1\) \\  \(\) & \(0.07\) & \(0.05\) & \(0.03\) & \(0.03\) & \(0.01\) & \(0.01\) \\  \(}\) & \(0.03\) & \(0.01\) & \(0.01\) & & & \\  \(}\) & \(0.1\) & \(0.1\) & \(0.07\) & \(0.07\) & \(0.05\) & \(0In Figure 2, we plot the median error with error bars for each calibration measure, where the \(x\) axis denotes \(_{2}(n-1)\), and results are reported over \(100\) runs.

Postprocessed neural networks.In , which observed modern deep neural networks may be very miscalibrated, various strategies were proposed for postprocessing network predictions to calibrate them. We evaluate two of these strategies using our testing algorithms. We trained a DenseNet40 model  on the CIFAR-100 dataset , producing a distribution \(_{}\), where a draw \((v,y)_{}\) selects a random example from the test dataset, sets \(y\) to be its label, and \(v\) to be the prediction of the neural network. We also learned calibrating postprocessing functions \(f_{}\) and \(f_{}\) from the training dataset, the former via isotonic regression and the latter via temperature scaling. These induce (ideally, calibrated) distributions \(_{}\), \(_{}\), where a draw from \(_{}\) samples \((v,y)_{}\) and returns \((f_{}(v),y)\), and \(_{}\) is defined analogously. The neural network and postprocessing functions were all trained by adapting code from .

We computed the median smooth calibration error of \(20\) runs of the following experiment. In each run, for each \(\{_{},_{},_{}\}\), we drew \(256\) random examples from \(\), and computed the average smooth calibration error smCE of the empirical dataset using a linear program solver from CVXPY. We report our findings in Table 2. We also compared computing smCE using the CVXPY solver and a commercial minimum-cost flow solver from Gurobi Optimization  (on the objective from Lemma 2) in this setting. The absolute difference between outputs is smaller than \(10^{-5}\) in all cases, verifying that minimum-cost flow solvers accurately measure smooth calibration.

Qualitatively, our results (based on smCE) agree with findings in  (based on binned variants of ECE), in that temperature scaling appears to be the most effective postprocessing technique.

smCE tester.Finally, we evaluated the efficiency of our proposed approaches to computing the empirical smCE. Specifically, we measure the runtime of four solvers for computing (3): a linear program solver from CVXPY, a commercial minimum-cost flow solver from Gurobi Optimization, a naive implementation of our algorithm from Corollary 1 using Python, and a slightly-optimized implementation using the PyPy package . We use the same experimental setup as in Table 1, i.e. measuring calibration of a uniform predictor on a miscalibrated synthetic dataset, with \(^{*}=0.01\).12 In Table 3, we report the average runtimes for each trial (across \(10\) runs), varying the sample size. Again, the absolute difference between the outputs of all methods is negligible (\( 10^{-9}\) in all cases). As seen in Table 3, our custom algorithm (optimized with PyPy) outperforms standard packages from CVXPY and Gurobi Optimization starting from moderate sample sizes. We believe that Table 3 demonstrates that our new algorithms are a scalable, reliable way of testing calibration, and that these performance gains may be significantly improvable by further optimizing our code.

  \(\) & \(_{}\) & \(_{}\) & \(_{}\) \\  Empirical smCE & \(0.2269\) & \(0.2150\) & \(0.1542\) \\  

Table 2: Empirical smCE on postprocessed DenseNet40 predictions (median over \(20\) runs)

Figure 2: The \(25\%\) quantile, median, and \(75\%\) quantile (over \(100\) runs) for smCE, dCE and cECE respectively. The \(x\)-axis is for dataset with size \(2^{x}+1\).