# Accelerating Blockwise Parallel Language Models

with Draft Refinement

 Taehyeon Kim\({}^{1}\) Ananda Theertha Suresh\({}^{2}\) Kishore Papineni\({}^{2}\) Michael Riley\({}^{2}\)

Sanjiv Kumar\({}^{2}\) Adrian Benton\({}^{2}\)

\({}^{1}\)KAIST AI \({}^{2}\)Google Research

Work done while at Google Research as a student researcher. Corresponding authors: <kimtaehyeon610@gmail.com> and <adbenton@google.com>.

###### Abstract

Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et al.  as a method to improve inference speed of language models by simultaneously predicting multiple future tokens, termed _block drafts_, which are subsequently verified by the autoregressive model. This paper advances the understanding and improvement of block drafts in two ways. First, we analyze token distributions generated across multiple prediction heads. Second, leveraging these insights, we propose algorithms to improve BPD inference speed by refining the block drafts using task-independent \(n\)-gram and neural language models as lightweight rescorers. Experiments demonstrate that by refining block drafts of open-sourced Vicuna and Medusa LLMs, the mean accepted token length are increased by 5-25% relative. This results in over a 3x speedup in wall clock time compared to standard autoregressive decoding in open-source 7B and 13B LLMs.

## 1 Introduction

The landscape of natural language processing has been profoundly reshaped by recent advances in autoregressive language models . These models have shown remarkable proficiency across a range of text generation tasks, including applications like question answering  and summarization . However, a significant obstacle to their wider application is high inference latency, particularly for extremely deep models with hundreds of billions of parameters . This latency, intrinsic to decoding with autoregressive language models (LMs), imposes considerable computational burdens and limits real-time deployment.

In response to these challenges, the field has seen a shift towards decoding methods aimed at reducing the inference latency in large language models (LLMs). One promising development is the concept of blockwise parallel decoding (BPD) . Unlike autoregressive decoding, which generates one token at a time, blockwise parallel LMs are outfitted with a set of prediction heads that propose and verify a draft, a block of subsequent tokens, in parallel. While BPD offers one solution to accelerated text generation, it also poses a challenge in ensuring that the proposed drafts are fluent and natural.

BPD inference speed depends both on the time it takes to generate a block draft and verification of the draft's agreement with the original LM's output (referred to as _base LM_ from here on) (Figure 0(a)). Unlike standard autoregressive LMs that generate tokens sequentially -- ensuring consistency with all preceding tokens (e.g., 'Messi' following 'Lionel') -- BPD employs a non-autoregressive drafting strategy. Here, blockwise parallel LMs simultaneously predict multiple token drafts (e.g., 'Lionel' and 'Ronaldo'), each position produced independently. The primary challenge in BPD drafting is ensuring that these concurrently-generated tokens are consistent with each other. An effective block drafter should prefer coherent sequences, such as 'Lionel Messi' over less coherent combinations like'Lionel Ronaldo', which would be improbable under a reasonable LM. The focus of this paper is on improving the quality of block drafts without altering the underlying model parameters.

## 2 Our contributions

This paper first investigates properties of the drafts from blockwise parallel LMs across seven tasks. These analyses are based on modest, 1.5 billion (B) parameter LMs. Given our observations, we propose lattice rescoring algorithms to produce higher quality block drafts. Finally, we apply these lattice rescoring algorithms to improve the drafts from large (7B/13B parameter) open-source LLMs, reducing mean per-token latency relative to both standard BPD and Medusa decoding across tasks.

### Observations on block drafts

#### 2.1.1 Consecutive repetitions

All heads within a block make predictions independently in a blockwise parallel LM. Unsurprisingly, we observe that this leads to block drafts with significant token repetition across heads. Consecutive repetition is pervasive across tasks, ranging from 20% to 75% of all neighboring draft tokens, depending on the task (Section5.1).

#### 2.1.2 Confidence of different heads

We analyze the distribution of probabilities within each block head. Our empirical analysis reveals an interesting property of BPD: the block drafter tends to be more confident with initial tokens, and becomes progressively less confident for subsequent tokens. We find that the confidence of block heads correlates strongly with the quality of the block drafter (Section5.2).

#### 2.1.3 Oracle top-\(k\) block efficiency

In the standard BPD algorithm (**Algorithm 1**), the most likely token at each head is generated as the draft. As mentioned above, this is prone to two issues: (1) this sequence might contain unnatural, consecutive repetitions and (2) the model might not be confident of the prediction at some of the heads. We use block efficiency, the average number of draft tokens accepted during decoding, to measure the quality of a given drafter [28; 46]. We ask whether the block efficiency can be improved by considering the top-\(k\) most likely tokens at each head. To measure the potential benefit of considering top-\(k\) tokens, we define the block efficiency of the oracle path through this top-\(k\) lattice, _oracle top-\(k\) block efficiency_, and show that there is significant headroom for improvement across tasks (Section5.3).

### New block draft algorithms with lightweight rescoring

Based on these observations, we propose two algorithms to leverage the top-\(k\) predictions at each head and improve average latency for open-source LLMs (Figure1). We show that these algorithms can also reduce the average latency in Medusa decoding , a recent popular extension of BPD (Section7). Neither of these algorithms requires changes to the underlying blockwise parallel LMs.

#### 2.2.1 Local rescoring via neural LMs

Given the top-\(k\) predictions at each head, we refine the block draft by using a small neural, autoregressive LM to greedily rescore these local predictions (Section6.1). While the block prediction scores are produced independent of each other, neural rescoring should favor sequences that are fluent, encouraging coherence between the predictions at each head.

#### 2.2.2 Global rescoring via _n_-gram LMs with multi-drafts

If the blockwise parallel LM has \(h\) heads and we consider the top-\(k\) tokens from each head, then there are \(k^{h}\) candidate drafts of length \(h\) that

Figure 1: (a) Illustration of two tokens that are decoded by autoregressive decoding vs. two tokens drafted by BPD. (b) Outputs from our proposed algorithms, where the top-\(k\) token-level predictions are refined using local neural or global _n_-gram rescoring, which selects the \(p\) most probable sequences by dynamic programming, for batched verification.

can be formed. We propose to use an _n_-gram model to efficiently rescore _all_ paths, via dynamic programming, and generate the \(p\) most probable rescored paths as a batch of draft candidates. These \(p\) drafts can then be verified in parallel by the blockwise parallel LM (Section 6.2).

There are two critical distinctions between the proposed algorithms: the amount of context/expressive power available to each class of rescoring model, and fundamental limitations of decoding with each class. While neural rescoring models are potentially more expressive and can leverage unbounded context, _n_-gram LMs can be used to efficiently find the globally most likely rescored drafts from the exponentially-sized set of possible draft candidates. Detailed algorithms are given in Section 6.1.

## 3 Preliminaries

**Autoregressive decoding** Let \(_{}\) be an autoregressive LM parameterized by \(\). The objective is to generate an output sequence \(y_{ T}=(y_{1},,y_{T})\) conditioned on an input sequence \(\). \(z_{t}=_{}(|,y_{ t})\) is a vector of logits, \(z_{t}^{||}\), where \(\) is the vocabulary over tokens. Let \(z_{t}[y]\) denote the logit of symbol \(y\). These logits define a conditional probability distribution at each time step \(p_{}(y|,y_{ t})=[y]}}{_{y^{} }e^{z_{t}[y^{}]}}\), which by the chain rule yields \(p_{}(y_{ T}|)=_{t=1}^{T}p_{}(y_{t}|,y_{<t})\). Sequences are generated autoregressively, either through ancestral sampling from some form of the conditional next token distribution , or by a beam search through the space of possible sequences to return a probable sequence. For simplicity, in this paper we focus on greedy decoding, where at each step the next token is predicted as \(y_{t+1}=_{y}p_{}(y|,y_{ t})\). The goal of BPD is to predict the same tokens as the base model, albeit efficiently.

**Blockwise parallel decoding** Let \(_{}^{h}\) be a blockwise parallel LM with block size \(h\) and let \(z_{t}^{i}=_{,i}^{h}(|,y_{ t})\) be the vector of logits corresponding to the \(i^{}\) block given context \(,y_{ t}\). This model employs \(h\) distinct feedforward neural (FFN) layers, each with a single hidden layer, atop the base LM's final hidden layer. The output of each FFN is followed by a softmax layer over the vocabulary to predict each of the \(h\) subsequent tokens in the block. In our initial analyses, the parameters of the FFNs are learned jointly with the base LM during training, and the weights of all softmax layers are tied to the input embedding table. Similar to , the first head is the same as the base LM, i.e., \(z_{t}^{1}=_{,1}^{h}(|,y_{ t})= _{}(|,y_{ t})=z_{t}\) and the hope is that for subsequent heads \(i 2\), \(_{,i}^{h}(|,y_{ t}) _{}(|,y_{ t+i-1})\).

**Algorithm 1** describes the BPD greedy decoding procedure. We outline the algorithm below and refer readers to  for additional details.

1. **Predict:**\(_{}^{h}\) generates a draft of \(h\) token predictions \(_{t+1},_{t+2},,_{t+h}\), conditioned on the prompt, \(\), and existing generated text, \(y_{ t}\) (i.e., \(_{t+i}=_{y}z_{t}^{i}[y]\  i h\)). Since the first head is same as the base LM, \(_{t+1}\) is identical to \(y_{t+1}\), the output of the base LM with greedy decoding.
2. **Verify:** In order to verify the predicted drafts, the base LM greedily generates next-token logits \(\{_{t},,_{t+h}\}\) conditioned on the existing prefix and block draft i.e., \(_{t+i}=_{}(,y_{ t},_{t+1},_{t +2},,_{t+i})\) for \(i\{0,1,,h\}\). Verification amounts to check ing which block draft tokens match the autoregressive greedy decode from the base LM: \((*{arg\,max}_{y}_{t+i}[y])==_{t+i+1}\). Note that the verification of all positions can be performed in parallel under the assumption that the base LM is a decoder-only transformer.
3. **Accept:** Finally, the length of the longest contiguous prefix \(n\) where draft tokens match the base LM's greedy decode is identified. Since the first head is the same as the base LM, the first token \(_{t+1}\) is always accepted. After accepting the tokens, one _free token_ can be obtained as the conditional probability of the base LM based on accepted tokens have already been calculated. Thus, the decoded sequence is extended by \(n+1\) tokens and we iterate. Typically, not all \(h\) tokens are accepted, with some draft tokens discarded. As the block generation has minimal overhead compared to the base LM's forward pass, even modest gains in accepted prefix length justify the cost of block draft generation.

## 4 Analysis setup

We train a \( 1.5\) billion (B) parameter decoder-only transformer LM with 9 heads, and investigate the drafts produced by this modest blockwise parallel LM.2 The 1.5B model and all auxiliary LMs were pretrained on (English) C4  with the causal next token prediction objective tokenized with the GPT3 subword vocabulary . For the 1.5B blockwise parallel LM, all heads were trained jointly to predict the following \(h\) tokens at each iteration. During pretraining, we use batches of 2048 subword sequences, each 512 tokens in length, amounting to \( 200\)B input tokens in total. Model training/inference was run on TPUv3/TPUv4 , and implemented in Jax .

We evaluate the potential latency improvement of block drafts by _block efficiency_[28; 46]. In this context, block efficiency represents the theoretical speedup compared to standard greedy decoding. It is defined as the average number of tokens decoded per serial call to the blockwise parallel LM. The formula for block efficiency is given by \(B}{_{}^{h}}\).

In this definition, the total number of decoded tokens is the sum of the number of accepted tokens across decoding steps, not necessarily all \(h\) predicted tokens in each block. Only the tokens that pass the 'Verify' stage and align with the base LM's predictions are accepted and integrated into the final sequence. This ensures that generated text is identical to the base LM, while achieving speedup. The total number of serial calls to \(_{}^{h}\) is the number of times the model processes a block of tokens. A block efficiency of 1 means that one is achieving no speedup relative to standard decoding.

We investigate the drafts produced by this 1.5B blockwise parallel LM on LAMBADA  (language modeling), SQuAD V1  (extractive QA), along with five summarization tasks: XSUM , MultiNews , SAMSum , NewsRoom  and CNN/DailyMail . For each task other than language modeling, we finetune the blockwise parallel LM for that task.3

   Task & Dataset & Performance & Block Efficiency \\  LM & LAMBADA  & 7.88 & 3.12 \\  QA & SQuAD V1  & 57.60 & 2.08 \\  S-SUM & CNN/Daily  & 39.85 & 1.74 \\  & SAMSUM  & 37.66 & 1.27 \\   & MultiNews  & 23.08 & 1.10 \\ L-SUM & XSUM  & 52.15 & 1.13 \\  & NewsRoom  & 39.85 & 1.08 \\   

Table 1: Per-task test performance of each finetuned model and block efficiency over language modeling (LM), extractive question answering (QA), and both long and short summarization (L-Sum & S-Sum).

  
**LAMBADA** \\  it’s nothing more than a **faceless**, formless brown blob to me, but I take his **wed** for the **resemblance to our genetic makeup,... _(Skip)_/... \\ 
**SQuAD V1** \\  Question: Who was announced as the LEM contract in November 1962? contact: Wiesner kept up the pressure, even making the disagreement public _- (Skip)_/... _(Skip)_/... \\ Answer: **Grunman** \\ 
**XSUM** \\ 
**Summarizer:... _(Skip)_/... \\ 
**Summarizer:... _(Skip)_/.

Table 1 shows that block efficiency varies dramatically across task.4 Language modeling, most closely matching the pretraining objective, achieves the highest block efficiency followed by the context-constrained task of extractive question answering. Table 2 sketches how BPD acts on three examples from each class of tasks.

* **LM:** BPD excels at generating common multi-word expressions in a single step. For example, (no) 'thing more than', and (take) 'his word for the' are each drafted and accepted in a single step.
* **QA:** BPD also attains high block efficiency in extractive QA, where it correctly drafts multi-token entities copied from the input sequence. In SQuAD V1, it accurately completes the answer 'Grumman' from 'Gru' by adding'mman', highlighting its ability to process multiple tokens at once and quickly extend answers.
* **SUM:** BPD's effectiveness in SUM tasks varies by dataset. For formulaic summaries like CNN/DailyMail, it performs well, reflecting its alignment with LM and QA tasks. However, in narrative-driven datasets like SAMSum and XSUM, where concise summaries are required, the block efficiency of BPD is little better than standard decoding.

## 5 Exploration of block drafts

### Consecutive repetition

We observe that vanilla block drafts are prone to significant token repetition. This is due to the fact that each head's prediction is independent of the others, and is a limitation shared with non-autoregressive generation in general . Table 3 shows the proportion of consecutive tokens in block drafts that are identical to each other, along with the average maximum length of repeated sequences in block drafts across all decode time steps. We compare these statistics before and after rescoring with a 2-gram LM - a trivial rescer, but one that can encourage local consistency between consecutive draft tokens. Strings of repeated tokens are unnatural, and unlikely to be generated by a strong base language model. Rescoring the top-\(k\) lattice with even a simple language model eliminates a significant amount of repetition, reducing the percentage of consecutive repeated tokens from between **9.9%** to **24.5%**, depending on the task.

### Confidence across multiple heads

Intuitively, predicting the identity of the \(i^{}\) future token becomes harder as \(i\) increases. To better understand this phenomenon, we measure the confidence of the predictions by the entropy of the

    &  &  &  \\   & & Vanilla & 2-gram & Vanilla & 2-gram \\  LM & LAMBADA & 20.0 & **10.7** & 2.2 & **1.8** \\  QA & SQuAD V1 & 75.5 & **67.6** & 6.6 & **6.1** \\   & CNN/Daily & 46.4 & **21.9** & 3.8 & **2.5** \\  & SAMSUM & 29.9 & **20.0** & 3.1 & **2.5** \\   & MultiNex & 33.6 & **14.7** & 3.1 & **2.1** \\  & XSUM & 24.0 & **9.4** & 2.6 & **1.7** \\   & NewsRoom & 47.2 & **32.1** & 4.1 & **3.3** \\   

Table 3: Consecutive token repetition in block drafts before and after C4-trained 2-gram rescoring of the top-16 lattice. “_%_ Conse” is the percentage of consecutive identical draft tokens out of all pairs of consecutive tokens. “Max run” is the average maximum repeated subsequence length in tokens (upper bound of 9, the number of block draft heads). Higher values correspond to more egregious repetition in drafts.

Figure 2: (a) Entropy distributions across block draft heads on LAMBADA . The density plots illustrate the entropy distribution for each head in the model. (b) Correlation between block efficiency and \(h_{}\), the head until which the average entropy in a task increases nearly monotonically.

token-level probability distribution for each head. In Figure 1(a), we plot the normalized histogram of entropy of each head on the LAMBADA dataset. From the normalized histogram, it is clear that the entropy increases as we move from first head to the last head, which agrees with our intuition that token prediction becomes more difficult as \(i\) increases.

However, we observed that the head entropy does not increase monotonically for all tasks as a function of \(i\). Let \(}[i]\) be the average entropy of head \(i\) on a particular corpus, and let \(h_{}=_{k}\{k: i<k,}[i]}[i+1]\}\), be the index of the largest head such that the average entropy of each head increases monotonically to that point. We observed a strong correlation between \(h_{}\) and block efficiency (Figure 1(b)). Heads with lower entropy (indicating more confident predictions) intuitively contribute more to efficiency. A linear regression confirms this with an R-value of \(\). This analysis suggests that the entropies of block heads could be used as a proxy for block efficiency, and thus inference latency.

### Oracle top-\(k\) block efficiency

**Oracle efficiency** The concept of oracle block efficiency serves as a theoretical benchmark, illustrating the headroom available from improving the quality of the block draft. To compute oracle block efficiency, we consider the top-\(k\) most probable tokens at each head, and form a "sausage" lattice from these. This data structure is a weighted directed graph, which succinctly represents all possible drafts (and their score under the blockwise parallel LM) that could be formed from selecting one of \(k\) tokens from each of the \(h\) heads (Figure 3). In the automatic speech recognition and machine translation communities, it is known as a "confusion network" [26; 41].

Given the top-\(k\) lattice at each decoding step, we identify an _oracle path_ that represents the path through the lattice that maximizes the length of the accepted prefix. This exercise, as shown in Figure 4, gives us insight into how much headroom exists in improving block drafts.

**Potential headroom from oracle selection** Oracle drafting is not practical, but rather a reference point. Analyzing the gap between actual BPD performance and the oracle upper bound (Figure 5) helps us to understand the limitations of the original block drafts and potential areas for improvement. Additionally, exploring oracle efficiency as a function of the \(k\) in the top-\(k\) lattice, demonstrates how "close" the block draft was to producing a stronger draft.

## 6 Lattice rescoring with lightweight rescorers

Having explored the properties of block draft predictions, we propose two drafting algorithms to improve block efficiency through rescoring of the top-\(k\) lattice with lightweight auxiliary LMs. This section presents techniques for rescoring the top-\(k\) lattice along with empirical results.

Each of these algorithms is a modification of the block drafted in **Stage 1** in Algorithm 1. Instead of using the most likely token at each head as the prediction, we construct the top-\(k\) sausage lattice of likely drafts from each head, where the set of top-\(k\) tokens is denoted as \(S_{i}\) for head \(i\). This approach allows any token within \(S_{i}\) to be chosen for position \(i\), yielding a total possible combinations of: \(|S_{1}||S_{2}||S_{h}|=k^{h}\). 5

Figure 4: Illustration of the output through oracle selection. For a given top \(k\) tokens of 3, if we can choose the oracle path successfully, the block efficiency can be improved from 1 to 5.

Figure 3: An example of a top-5 sausage lattice on a NewsRoom example. Edge weights correspond to logits. Edges at each time step are ordered in descending weight and green, bolded edges correspond to candidates matching the greedy decode over the next nine tokens: _”... desktop computers with new Intel Corp processors that it... ”_. The initial node in this graph is state 0 and the final node is 9.

In this lattice, any path from the start to final state represents a viable draft. Two algorithms are proposed to select a small number of \(h\)-length drafts from this lattice, which are then passed to the verification step. The first algorithm employs neural autoregressive transformers (**Section 6.1**), while the second utilizes _n_-gram language models (**Section 6.2**).

### Local rescoring via neural models

A simple approach uses a small neural rescorer LM, interpolating between the logits of the rescorer LM and vanilla block draft logits with an interpolation weight (**Algorithm 2**). Recall that \(z_{i}^{j}\) is the vector of logits corresponding to the \(j^{}\) block. Let \(S_{j}\) denote the set of symbols with top-k values in the logits vector \(z_{t}^{j}\). The rescorer prediction for head \(j\) is given by:

\[z_{t}^{j}[S_{j}] z_{t}^{j}[S_{j}]+ r_{t+j}[S_{j}],\]

where \(\) is the weight placed on the rescorer's prediction and \(r_{t+j}\) are the corresponding logits predicted by the small neural rescoring model, when conditioned on the sequence \(y_{ t},_{t+1},,_{t+2},,_{t+j-1}\). We also set logits for symbols outside set \(S_{j}\) (\(S_{j}^{c}\)) to be negative infinity, which corresponds to zero probability. Note that we do not rescore the first head as it is the same as the base LM. We then run **Algorithm 1**, where instead of using logits directly from the BPD model, we use the rescored logits to generate the draft. We experiment with decoder-only transformers having 32, 61, and 94 million (M) weight parameters (**Appendix D**).

```
1:Blockwise parallel LM \(_{}^{h}\), top-\(k\) indices selection function Top-\(k()\), rescoring model \(_{_{r}}\), interpolation weight \(>0\).
2:\(z_{t}^{j}_{,i}^{h}(|x,y_{ t}), i  h\)
3:\(_{t+i}_{y V}z_{t}^{i}[y], i h\)
4:/# Local lattice rescoring */
5:for\(j 2,,h\) in parallel do
6:\(r_{t+j}_{_{r}}(|x,y_{ t},_{t+1},,_{t+j-1})\)
7:\(z_{t}^{j}[S_{j}] z_{t}^{j}[S_{j}]+ r_{t+j}[S_{j}]\)
8:\(z_{t}^{j}[S_{j}^{c}]-\)
9:endfor ```

**Algorithm 2**Local rescoring via neural models

### Global _n_-gram rescoring

We also evaluate the quality of drafts generated by rescoring with an _n_-gram LM. Recall that blockwise parallel LMs can be used to compute a lattice representing \(k^{h}\) possible sequences. We rescore all of these sequences except the first position token with an _n_-gram model, select the top \(p\) most likely sequences and pass them to the verification stage. When \(p=1\), we refer to this as _n-gram rescoring_ and when \(p>1\), we refer to this as _p-best n-gram BPD_.

While global rescoring typically yields better results compared to local rescoring, rescoring \(k^{h}\) sequences with a neural LM and selecting the most likely sequence would take time \(O(k^{h})\), which is computationally prohibitive in most cases. Hence, we take advantage of _n_-gram LMs, which are unique in that one can efficiently select the most likely rescored sequence in time poly\((k,h)\), using

Figure 5: Oracle block efficiency over the top-\(k\) lattice as a function \(k\). Each plot (a-f) represents a different task, demonstrating the relative improvement in block efficiency of the oracle draft with respect to the standard block draft as a function of the number of block draft heads used.

[MISSING_PAGE_FAIL:8]

accepted prefix length of the rescored draft vs. vanilla draft. Table 5 displays the win frequency across tasks along with the percentage of wins/losses attributed to introducing/eliminating repetition.

Note that in the tasks where rescoring improves block efficiency the most, NewsRoom and MultiNews, a high percentage of those repaired instances are driven by fixing erroneously repeated tokens. In fact, for MultiNews, 66.23% of block drafts are improved through repetition repair. We also evaluated the performance of rescoring with in-domain trained rescoring LMs, but found that they tended to perform no better than C4-trained LMs (Appendix E).

## 7 Lattice rescoring on open-source blockwise parallel LLMs

Medusa decoding  extends BPD by verifying a set of plausible candidates in parallel. Verification is performed efficiently through a tree-attention mechanism, requiring only a single forward pass. Other aspects not explicitly mentioned remain the same as described in Algorithm 1. While Medusa employs tree-attention during decoding to efficiently verify a subset of likely drafts, our approach focuses on rescoring these draft candidates, making them potentially complementary techniques. We explore this synergy by integrating neural rescoring method into Medusa decoding. In this section, we apply rescoring to large open-source LLMs, using Vicuna 7B-v1.3 and Vicuna 13B-v1.3 as base models. We report both block efficiency and speedup ratio achieved relative to standard autoregressive decoding using the SpecBench benchmark . To ensure rigorous verification, we expand our experiments to include a wider range of datasets. We use existing pretrained Medusa heads as the block drafter6. Although these base LMs were not trained jointly with the block drafter, this corresponds to the Medusa-1 configuration, which has been shown to result in comparable speedups to jointly trained Medusa models . For lattice neural rescoring, we set \(k\) to be the full vocabulary size, using 5 heads with the next-word-prediction LM head as one of the heads, following Algorithm 2. All timings were evaluated on a single NVIDIA A100 80GB GPU with batch size 1.

Figure 7 demonstrates the block efficiency and speedup ratio on MT-Bench , comparing greedy BPD and Medusa with and without local rescoring for Vicuna 13B models by setting the interpolation weight \(\) to 1.0. The same analysis on Vicuna 7B is described in the Figure 9, which is detailed in Appendix G. A key observation is that even after increasing model size from 7B to 13B, a relatively small neural model (68M) can effectively serve as the rescoring drafter, showcasing the robustness of our approach. The rescoring model used in these experiments is a decoder-only LM trained on the C4 and ShareGPT datasets7. Furthermore, we observe consistent performance improvements across both the original BPD and its extension, Medusa, further validating the efficacy of our local rescoring method. While the speedup gains might not always directly correlate with the increase in block efficiency, we consistently observe performance improvements across all categories. This difference suggests that block efficiency does not always translate into equivalent speedup, likely due to system-level factors. However, there remains potential for further acceleration through additional system-level optimizations.

Table 6 further presents speedup ratios across diverse datasets for Vicuna 7B and 13B models, respectively. We evaluate not only under greedy decoding (Temperature=0.0) but also under temperature sampling (Temperature=0.7, 1.0), employing typical acceptance for verification . Both BPD and Medusa, enhanced with our local rescoring, consistently yield speedup improvements across all

Figure 7: Block efficiency and speedup ratio relative to the standard autoregressive decoding on sub-categories of MT-Bench dataset  when greedily decoding with Vicuna 13B.

[MISSING_PAGE_FAIL:10]