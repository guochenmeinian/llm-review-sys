# Temporal Continual Learning with Prior Compensation for Human Motion Prediction

Jianwei Tang

Sun Yat-sen University

tangjw7@mail2.sysu.edu.cn &Jiangxin Sun

Sun Yat-sen University

sunjx5@mail2.sysu.edu.cn &Xiaotong Lin

Sun Yat-sen University

linxt29@mail2.sysu.edu.cn &Lifang Zhang

Dongguan University of Technology

2017028@dgut.edu.cn &Wei-Shi Zheng

Sun Yat-sen University

wszheng@ieee.org &Jian-Fang Hu

Sun Yat-sen University

hujf5@mail.sysu.edu.cn

Jian-Fang Hu is the corresponding author. Hu is also with Guangdong Province Key Laboratory of Information Security Technology, Guangzhou, China and Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China.

###### Abstract

Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at https://github.com/hyqlat/TCL.

## 1 Introduction

Human Motion Prediction (HMP) aims to predict future poses at varied temporal moments based on the observed motion sequences. The accurate prediction of human motion plays a vital role in many applications, such as autonomous driving, human-robot interaction, and security monitoring, enabling the anticipation and mitigation of risks. This task is challenging due to its requirement for predicting multiple moments, including short-term predictions for the "near-future" and long-term predictions for the "far-future".

Previous approaches address this task by autoregressively forecasting using recurrent neural networks (RNNs) and transformer architectures , or parallelly generating all frames with graph convolution networks (GCNs) . These methods employthe one-stage training strategy to directly train a model that can predict both the short-term prediction and the long-term prediction. However, the long-term motion prediction is more challenging since the future motion can vary greatly (i.e., the prediction space is large), which would increase the uncertainty and ambiguity of future prediction. As the prediction length increases, the fitting of the high-uncertainty long-term prediction will gradually dominate the learning process of the prediction model, which hinders the learning of short-term predictions and further limits the full potential of leveraging the prior knowledge learned from short-term inputs to facilitate long-term predictions.

This motivates us to exploit proper training strategies to better learn and utilize the prior knowledge. We further investigate this by conducting preliminary experiments on the following settings "short+long", "short only", and "short then short+long", which are illustrated in Figure 1. The results are summarized in Figure 2. We observe that "short then short+long" outperforms "short+long" on long-term prediction, which implies that the knowledge learned in short-term prediction can serve as a prior to facilitating the learning of "far-future" prediction. This is intuitive and thus we can use a progressive learning approach, where the model is trained to predict increasing numbers of frames over multiple training stages, e.g., starting with 5 frames in the first stage and 10 frames in the second stage, and so on. However, we also observe that "short then short+long" performs worse than "short only" by a considerable margin for short-term prediction, which demonstrates the joint learning of short-term and long-term prediction results in knowledge forgetting for short-term prediction.

To overcome these problems, we introduce the Prior Compensation Factor (PCF) into the multi-stage training method to obtain a sequential continuous learning framework, which is named Temporal Continual Learning (TCL). It is a multi-stage training framework that alleviates constraint posed by long-term prediction on short-term prediction and effectively utilize prior information from short-term prediction. Specifically, we divide the future sequence into segments and divide the training process

Figure 1: Illustration of three different prediction settings. “short+long” and “short only” represent optimizing prediction for the entire sequence and prediction only for the first 5 frames, respectively. “short then short+long” denotes training the entire sequence after pretraining for the first 5 frames.

Figure 2: Preliminary experiment results of three different prediction settings (lower values indicate better performance). (a) shows the short-term prediction results (predicting the 2-nd frame), while (b) illustrates the long-term prediction results(predicting the 25-th frame).

into multiple stages accordingly. We incrementally increase the number of prediction segments, allowing us to leverage the prior knowledge acquired from earlier stages for predicting the subsequent ones. Upon completion of each stage's training, the learned prior knowledge is saved in the model parameters. However, with the changing of optimization objective when switching stages, the prior knowledge gradually fades away. In order to overcome this forgetting problem of prior knowledge, we further introduce the PCF, which is designed as a learnable variable. Then, we derive a more reasonable optimization objective for this regression problem through theoretical derivation.

The proposed training framework is flexible and can be easily integrated with various HMP backbones or adapted to different datasets. To validate the effectiveness and flexibility of our framework, we conduct experiments on four popular HMP benchmark datasets by integrating TCL with several HMP backbones. Our contributions can be summarized as follows:1) We identify certain limitations in existing HMP models and propose a novel multi-stage training strategy called Temporal Continual Learning to obtain a more accurate motion prediction model. 2) We introduce a Prior Compensation Factor to tackle the forgetting problem of prior knowledge, which can be learned jointly with the prediction model parameters. 3) We obtain an easily optimized and more reasonable objective function through theoretical derivation.

## 2 Related Work

Autoregressive prediction approaches for HMP.Motivated by natural language processing, many researchers have adopted sequence-to-sequence models to exploit the temporal information of pose sequences in HMP, which includes RNNs, Long Short-Term Memory Networks (LSTMs)  and Transformer . For instance, ERD  combined LSTMs with an encoder-decoder to model the temporal aspect, while Jain _et al._ proposed Structural-RNN to capture spatiotemporal features of human motion. Martinez _et al._ applied a sequence-to-sequence architecture for modeling the human motion structure. Aksan _et al._ used Transformer to autoregressively predict future poses. Sun _et al._ designed a query-read process to retrieve some motion dynamics from the memory bank. Lucas _et al._ proposed a GPT-like autoregressive method to generate human poses. Tang _et al._ combined attention mechanism and LSTM to complete the human motion prediction task. However, autoregressive methods are difficult to train and suffer from error accumulation problem.

Parallel prediction approaches for HMP.Some researchers employed parallel prediction methods to address HMP problem [2; 6; 7; 23; 25; 30; 31; 28]. The works of [21; 22; 23] used GCN to encode feature or to decode it, which associates different joints' information. Mao _et al._ viewed a pose as a fully connected graph and used GCN to extract hidden information between any pair of joints. Martinez _et al._ devised a transformer-based network to predict human poses. Sofianos _et al._ proposed a method to extract spatiotemporal features using GCNs. And Ma _et al._ tried to achieve better prediction results using a progressive manner. Xu _et al._ used multi-level spatial-temporal anchors to make diverse predictions.

Continual learning.Although Deep Neural Networks (DNNs) have demonstrated impressive performance on specific tasks, their limitations in handling diverse tasks hinder their broader applicability. Therefore, some researchers introduced the concept of Continual Learning (CL) to DNNs to ensure that models retain the knowledge of previous tasks while learning new tasks. Kirkpatrick _et al._ proposed the Elastic Weight Consolidation (EWC) method to overcome the catastrophic forgetting problem and improve the performance of multi-task problems. Shin _et al._ introduced a method that addresses catastrophic forgetting in sequential learning scenarios by using a generative model to replay data from past tasks during the training of new tasks. It is important to note that the traditional CL approaches do not account for temporal correlation and are unable to leverage data from previous tasks.

## 3 Method

The problem of human motion prediction involves predicting future motion sequences by utilizing previously observed motion sequences. Formally, let \(_{1:T_{h}}=[_{1},_{2},,_{T_{h}} ]^{J D T_{h}}\) denotes the observed motion sequence of length \(T_{h}\) where \(_{i}\) indicates motion of time \(i\), and \(_{T_{h}+1:T_{h}+T_{p}}=[_{T_{h}+1},_{T_{h}+2}, ,_{T_{h}+T_{p}}]^{J D T_{p}}\) represents the motion sequence of length \(T_{p}\) that needs to be predicted. Note that \(J\) is the number of joints for each pose, and \(D\) is the dimension of coordinates. It can be regarded as a composite task consisting of multiple sequential prediction tasks, which involves predicting the future poses at varied moments conditioned on the motion sequences observed in the past.

To accomplish this multiple sequential prediction task, we first model the HMP problem as solving the following optimization problem:

\[^{*}=*{arg\,max}_{}P(_{T_{h}+1}, _{T_{h}+2},,_{T_{h}+T_{p}}|_{1},_ {2},,_{T_{h}};).\] (1)

Thus our target is to find the optimal model that maximizes Equation (1). To achieve this, we propose a framework called Temporal Continual Learning. Specifically, we partition the entire prediction interval into several smaller segments and perform multi-stage training. We claim that this enables the utilization of prior information from previous segments as knowledge for predicting the subsequent segments. Further, as the optimization objective changes in each training stage, we find that the prior knowledge, i.e., information learned in previous training stages, will be forgotten to a certain degree. To mitigate this problem, we introduce a Prior Compensation Factor and accordingly derive a more reasonable optimization objective at each stage.

### Multi-stage Training Process.

We initially decouple the future sequence into \(K\) segments with time boundaries \(T_{Z_{1}},T_{Z_{2}},,T_{Z_{K}}\), where \(T_{Z_{K}}=T_{h}+T_{p}\). And we denote the prediction of segment \(k\) as task \(Z_{k}\), which can be expressed as follows:

* Task \(Z_{1}:_{1:T_{h}}_{T_{h}+1:T_{Z_{1}}}\)
* Task \(Z_{2}:_{1:T_{h}}_{T_{Z_{1}}+1:T_{Z_{2}}}\)
* Task \(Z_{K}:_{1:T_{h}}_{T_{Z_{K-1}}+1:T_{Z_{K}}}\)

To be specific, the target of task \(Z_{1}\) is to predict \(_{T_{h}+1:T_{Z_{1}}}\) conditioned on \(_{1:T_{h}}\), and task \(Z_{k}\) aims to predict \(_{T_{Z_{k-1}}+1:T_{Z_{k}}}\) with \(_{1:T_{h}}\) as condition. Therefore, by leveraging bayesian formulation, optimization problem \(P(_{T_{h}+1},_{T_{h}+2},,_{T_{h}+T_{p}}| _{1},_{2},,_{T_{h}};)\) can be formulated as:

\[P(Z_{1}Z_{2} Z_{K};)=P(Z_{K}|Z_{1}Z_{2} Z_{K-1};)P(Z_{K-1}|Z_{1}Z_{2} Z_{K-2};) P(Z_{1};),\] (2)

where \(\) is model parameters to be learned. In the following, we denote "\(Z_{1}Z_{2} Z_{k}\)" as "\(Z_{1:k}\)". Our target is to maximize Equation (2) which means finding optimal model to accomplish all tasks.

For the purpose of transferring the prior knowledge in preceding tasks to their subsequent prediction task, we progressively increase the number of tasks in temporal order and train them successively. More precisely, our training is decomposed into \(K\) stages. In each stage \(S_{k}\), we leverage the optimal model parameters \(_{k-1}^{*}\) trained in the previous stage \(S_{k-1}\) to initialize the parameters \(\), and then update it based on the prediction tasks \(Z_{1},Z_{2},,Z_{k}\) (maximizing \(P(Z_{1:k};)\)). Since the model is trained to optimize the prediction tasks \(Z_{1:k-1}\) in training stage \(S_{k-1}\), the knowledge of tasks \(Z_{1:k-1}\) can be implicitly involved in its well-trained parameters \(_{k-1}^{*}\). Initializing \(_{k}\) as \(_{k-1}^{*}\) can exploit the prior knowledge learned in previous tasks to assist the prediction of the next task.

### Definition of Prior Compensation Factor.

With training different optimization objectives stage by stage, the prior knowledge provided by previous tasks can be effectively exploited to predict the subsequent task. However, the change of the optimization objective in different training stages could also bring about the knowledge forgetting problem. To mitigate the problem, we introduce \(_{Z_{1:k-1} Z_{k}}\) to estimate the extent of forgotten knowledge when utilizing prior knowledge from tasks \(Z_{1:k-1}\) to predict task \(Z_{k}\), which we refer to as the "Prior Compensation Factor".

\[_{Z_{1:k-1} Z_{k}}=P(Z_{k}|Z_{1:k-1};)-P(Z_{k}| _{1:k-1};).\] (3)

Here, \(_{1:k-1}\) is regarded as the prior knowledge that is reserved and can be still provided for predicting task \(Z_{k}\). So \(_{1:k-1}\) initially represents the prior knowledge reserved in \(_{k-1}^{*}\) in every stage \(S_{k}\) and would get somewhat corrupted gradually during training. \(P(Z_{k}|Z_{1:k-1};)\) indicates the most ideal case, where the current prediction task \(Z_{k}\) can fully leverage the prior information provided by previous prediction tasks \(Z_{1:k-1}\). Consequently, the loss of the prior knowledge is non-negative, implying that \(0_{Z_{1:k-1} Z_{k}} 1-P(Z_{k}|_{1:k-1};)\). Specifically, we can observe that \(=0\) when \(P(Z_{k}|Z_{1:k-1};)=P(Z_{k}|_{1:k-1};)\), which implies that all the prior knowledge of previous tasks is completely exploited although the \(\) changes. By substituting Equation (3) into Equation (2) and taking the negative logarithm, we can obtain:

\[- P(Z_{1:k};)=- P(Z_{1};)-_{i=2}^{k}(P( Z_{i}|_{1:i-1};)+_{Z_{1:i-1} Z_{i}}).\] (4)

Our objective is to minimize \(- P(Z_{1:k};)\) with respect to the model parameter \(\) and prior compensation factors \(\{_{Z_{1:i-1} Z_{i}},i=2,3,,k\}\).

### Optimization Objective

Optimizing Equation (4) directly is challenging due to the presence of the PCF \(\) and \(P(Z_{k}|_{1:k-1};)\) inside the logarithm. By applying Lemma 3.1 (details provided in the appendix), we can obtain an upper bound for Equation (4), which can be expressed as:

\[ UB&=- P(Z_{1};)+_{ i=2}^{k}((1-_{Z_{1:i-1} Z_{i}})(- P(Z_{i}|_{1:i-1};))\\ &+(1-_{Z_{1:i-1} Z_{i}})(1-_{Z_{1:i-1} Z_{ i}})+(1+_{Z_{1:i-1} Z_{i}})).\] (5)

Hence, we can turn to minimize the upper bound of Equation (4). It appears that in the optimization objective, \(_{Z_{1:i-1} Z_{i}}\) serves as a factor to control the weights of different tasks, which mitigates the loss of prior information and thus compensates for the lost prior knowledge. The Lemma 3.2 indicates that the largest difference between \(- P(Z_{1:k};)\) and the upper bound \(UB\) would not exceed \((}{{2}})*(k-1)\) when \(P(Z_{i}|_{1:i-1};)}{{2}},i\{2,3,,k\}\).

Lemma 3.1.For \(0 a 1-b\) and \(0<b 1\), the inequality \(-(a+b)(1-a)(- b)+(1-a)(1-a)+(1+a)\) holds. The equality holds if and only if \(a=0\).

Lemma 3.2.The absolute difference between the target objective (Equation (4)) and the upper bound (Equation (5)) is not larger than \((}{{2}})*(k-1)\) when \(P(Z_{i}|_{1:i-1};)}{{2}},i\{2,3,,k\}\). This bound is achieved when \(P(Z_{k}|_{1:k-1};)=}{{2}}\) and \(_{Z_{1:i-1} Z_{i}}=}{{2}},i\{2,3,,k\}\).

Due to the space limitation, we present the proofs of Lemma 3.1 and 3.2 in the appendix.

An intuitive explanation.Figure 3 illustrates the comparison between the term \(-(P(Z_{k}|_{1:k-1};)+_{Z_{1:k-1} Z_{k}})\) in the actual optimization objective, the term \(- P(Z_{k}|_{1:k-1};)\) in the naive optimization objective merely leveraging multi-stage training strategy, and the corresponding approximate term in our optimization objective. It can be observed that our approximation method has a smaller difference from the actual optimization objective compared to the naive method. This is important as a more reasonable objective function can improve the accuracy of the optimization process. When the prior compensation factor \(_{Z_{1:k-1} Z_{k}}\) is zero, which means \(P(Z_{k}|Z_{1:k-1};)=P(Z_{k}|_{1:k-1};)\), the prior prediction information from previous tasks \(Z_{1:k-1}\) is not lost. As the value of \(\) increases, the discrepancy between the actual objective \(P(Z_{k}|Z_{1:k-1};)\) and the objective of the naive method \(P(Z_{k}|_{1:k-1};)\) becomes more evident, indicating a greater degree of forgetting prior knowledge. In contrast, our approach narrows this gap by effectively mitigating the loss of prior information obtained from tasks \(Z_{1:k-1}\).

### Optimization Strategy

We train the model in a multi-stage manner, in which an initial stage and \(K-1\) TCL stages are involved. The initial stage \(S_{1}\) aims to forecast the motion in the foremost segment, while each TCL stage \(S_{k},k\{2,3,,K\}\) performs prediction for segments \(Z_{1:k}\) and simultaneously trains\(_{Z_{1:k-1} Z_{k}}\). Once the stage \(S_{k}\) is completely trained, we then estimate the factors \(_{Z_{1:k-1} Z_{k}}\) which would be used in the optimization of the following stages. This process is repeated until the final stage \(S_{K}\) is reached.

Learning of initial stage \(S_{1}\).Following the implementations of previous methods , we can train the initial stage \(Z_{1}\) with Mean Suqared Error (MSE) loss:

\[_{1}=_{i=T_{h}+1}^{T_{Z_{1}}}\|_{i}-}_{i}\|^{2}\] (6)

where \(_{i}\) and \(}_{i}\) represent the ground truth and predicted motion of the \(i\)-th frames respectively.

Temporal Continual Learning at stage \(S_{k}\).In stage \(S_{k}\) (\(k 2\)), we need to update the model parameters \(\) corresponding to tasks \(Z_{1:k}\) and the PCF \(_{Z_{1:k-1} Z_{k}}\). According to Equation (5), the loss function in this stage can be calculated as follows:

\[_{k}=&(1-_{Z_{1:k-1}  Z_{k}})_{i=T_{Z_{k-1}}+1}^{T_{Z_{k}}}\|_{i}-}_{i}\|^{2}+(1-_{Z_{1:k-1} Z_{k}})(1- _{Z_{1:k-1} Z_{k}})\\ &+(1+_{Z_{1:k-1} Z_{k}})+_{j=2}^{k-1}(1-_{Z_{1:j-1} Z_{j}})_{i=T_{Z_{j-1}}+1}^{T_{Z_{j}}}\| _{i}-}_{i}\|^{2}+_{1}\] (7)

where the parameters \(_{Z_{1} Z_{2}},,_{Z_{1:k-2} Z_{k-1}}\) are determined in the learning of previous stages. Once the model parameters for stage \(S_{k}\) are determined, we then calculate \(_{Z_{1:k-1} Z_{k}}\) as:

\[_{Z_{1:k-1} Z_{k}}=_{m=1}^{M}_{Z_{1: k-1} Z_{k}}^{m}\] (8)

where \(M\) represents number of samples and \(_{Z_{1:k-1} Z_{k}}^{m}\) is PCF estimated for the \(m\)-th sample.

We continue the TCL training process by predicting stage \(S_{k+1}\) and updating the model parameter \(\) as well as PCF \(_{Z_{1:k} Z_{k+1}}\). We repeat this TCL process until we reach the final stage \(S_{K}\). In practice, we require the backbone model to output an extra dimension and pass it through an MLP head to obtain \(\). The algorithm flow is summarized in Algorithm 1.

Figure 3: A toy example illustrating the optimization objectives. Our approximate optimization objective is closer to the actual objective compared to the naive optimization method.

## 4 Experiments

### Experimental Setup

We validate our framework on four benchmark datasets. Human3.6M is a large dataset that contains 3.6 million 3D human pose data. 15 types of actions performed by 7 actors(S1, S5, S6, S7, S8, S9 and S11) are included in this dataset. Each actor is represented by a skeleton of 32 joints. However, following the data preprocessing method proposed in [28; 31], we only use 22 joints. The global rotations and translations of poses are removed, and the frame rate is downsampled from 50 fps to 25 fps. For testing and validation, we use actors S5 and S11, while training is conducted on the remaining sections of the dataset. CMU-MoCap is a smaller dataset that has 8 different action categories. The global rotations and translations of the poses are also removed. Each pose contains 38 joints, but following the data preprocessing methods in [28; 31], we only use 25 joints. 3DPW is a challenging dataset that contains human motion data captured from both indoor and outdoor scenes. Poses in this dataset are represented in 3D space, with each pose containing 26 joints. However, only 23 of these joints are used, as the other three are redundant. The Archive of Motion Capture as Surface Shapes (AMASS) dataset gathers 18 existing mocap datasets. Following , we select 13 from those and take 8 for training, 4 for validation and 1 (BMLrub) as the test set. We consider forecasting the body joints only and discard those 4 static ones, leading to an 18-joint human pose.

Following the benchmark protocols, we use the Mean Per Joint Position Error (MPJPE) in millimeters (ms) as our evaluation metric for 3D coordinate errors and Euler Angle Error (EAE) for Euler angle representations. The performance is better if this metric is smaller.

Implementation Details.Following [28; 30; 39], we set the input length to 10 frames and the predictive output to 25 frames for Human3.6M, AMASS and CMU-Mocap datasets, respectively. For the 3DPW dataset, we predict 30 frames conditioned on the observation of the preceding 10 frames. We choose PGBIG as our backbone model by default. In order to learn the PCF, we add an extra dimension to the output of the backbone model and calculate PCF through an MLP network whose hidden dimension is set to 512. We partitioned the future sequences into three segments with lengths of 3, 9, and 13. The training process was conducted on an NVIDIA RTX 3090 GPU for 120 epochs, allocating 50, 90, and 120 epochs for each respective stage.

[MISSING_PAGE_FAIL:8]

the preservation of prior information, resulting in the forgetting of prior knowledge. Here, we utilize \(\) to mitigate the loss of prior information, thereby improving the overall training of the model.

### Ablation Studies

We conduct several ablation experiments to further verify the effectiveness of our proposed framework.

Evaluation on the number of tasks.As shown in Table 3, the model's performance improves as the number of tasks gets larger from 1 to 3, and it remains stable when the number of tasks becomes larger than 3.

Evaluation on different implementations.As Figure 6 shows, we compare the results of four different implementations. PGBIG is our baseline model. "w/o \(\)" means that we only divide the training process into several stages to train each task without using the PCF. "HC" represents using a hand-crafted coefficient that changes its values similar to our PCF at each epoch. Specifically, in stage \(S_{1}\), the value of \(\) is set to 1. In stage \(S_{2}\), \(\) is initially set to 0.1 and increased by 0.05 at each epoch until reaching 0.5, where it remains constant. The same pattern applies to stage \(S_{3}\). In each subfigure, we show the results of \(Z_{k}\) on the validation set which is obtained at stage \(S_{k}\).

Figure 4: Some visualization results on Human3.6M dataset.

Figure 5: \(\) at different stages.

  number of tasks & 1 & 2 & 3 & 5 & 8 \\  avg error & 66.95 & 66.02 & 65.00 & 65.05 & 65.03 \\  

Table 3: The average error of different numbers of tasks.

In stage \(S_{1}\), where no prior information is available, the results of "w/o \(\)", "HC", and "Ours" are identical, but superior to the baseline. This validates the effectiveness of decomposing multiple-moment predictions, as it alleviates the constraint of long-term predictions on short-term predictions and enhances the model's ability to learn short-term predictions. In the following stages, the performance of "w/o \(\)" is worse than "Ours", which indicates that the prior knowledge exploited by our framework benefits the prediction model training. Moreover, as the training period progressed, the performance gap becomes larger. However, the method without \(\) can still achieve better performance than "PGBIG", indicating that training model in a multi-stage manner can also exploit some useful prior information for prediction. We also note that the performance of "Ours" is much better than "HC", which conducts temporal continual learning with fixed and manually defined PCF. It demonstrates that joint training PCF and model parameters is beneficial.

The forgetting of prior knowledge.As shown in Table 4, introducing the prior compensation factor alleviates the performance degradation from stage \(S_{1}\) to stage \(S_{3}\) of task \(Z_{1}\)'s predictions. Specifically, without PCF, the prediction error of \(Z_{1}\) increases by 0.83, whereas with PCF, it only increases by 0.27. This result suggests that PCF can effectively alleviate the forgetting issue. As a result, \(Z_{1}\) can offer more comprehensive priors for \(Z_{2}\) and \(Z_{3}\) predictions, resulting in better prediction performance.

## 5 Conclusion

In this paper, we introduced the temporal continual learning framework for addressing the challenges in human motion prediction. Our framework addresses the constraint between long-term and short-term prediction, allowing for better utilization of prior knowledge from short-term prediction to enhance the performance of long-term prediction. Additionally, we introduced the prior compensation factor to mitigate the issue of forgetting information. Extensive experiments demonstrated our framework's effectiveness and flexibility.

Limitation.Our proposed training framework may slightly increase training time. However, the testing time remains unchanged compared to the backbone model.

Broader Impact.We believe our work has value for not only human motion prediction but also for more general prediction tasks and backbone models . This has benefits in various areas such as security monitoring, robotics, and autonomous driving.

Table 4: The average error of different tasks at the end of each stage. \(Z_{i}\) represents task \(i\).

Figure 6: Comparison of different approaches. PGBIG is a baseline model that is trained without multi-stage. “w/o \(\)” represents training multi-stage process without PCF. “HC” means using a hand-designed coefficient. “Ours” is a multi-stage training process with PCF.