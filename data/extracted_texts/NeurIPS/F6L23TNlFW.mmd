# Predicting Label Distribution from Ternary Labels

Yunan Lu, Xiuyi Jia

School of Computer Science and Engineering

Nanjing University of Science and Technology, Nanjing 210094, China

{luyn, jiaxy}@njust.edu.cn

Corresponding author

###### Abstract

Label distribution learning is a powerful learning paradigm to deal with label polysemy and has been widely applied in many practical tasks. A significant obstacle to the effective utilization of label distribution is the substantial expenses of accurate quantifying the label distributions. To tackle this challenge, label enhancement methods automatically infer label distributions from more easily accessible multi-label data based on binary annotations. However, the binary annotation of multi-label data requires experts to accurately assess whether each label can describe the instance, which may diminish the annotating efficiency and heighten the risk of erroneous annotation since the relationship between the label and the instance is unclear in many practical scenarios. Therefore, we propose to predict label distribution from ternary labels, allowing experts to annotate labels in a three-way annotation scheme. They can annotate the label as "0" indicating "uncertain relevant" if it is difficult to definitively determine whether the label can describe the instance, in addition to the binary annotation of "1" indicating "definitely relevant" and "\(-1\)" indicating "definitely irrelevant". Both the theoretical and methodological studies are conducted for the proposed learning paradigm. In the theoretical part, we conduct a quantitative comparison of approximation error between ternary and binary labels to elucidate the superiority of ternary labels over binary labels. In the methodological part, we propose a Categorical distribution with monotonicity and orderliness to model the mapping from label description degrees to ternary labels, which can serve as a loss function or as a probability distribution, allowing most existing label enhancement methods to be adapted to our task. Finally, we experimentally demonstrate the effectiveness of our proposal.

## 1 Introduction

LDL (Label Distribution Learning)  is an effective learning paradigm for addressing label polysemy (i.e., the cases where an instance can be described by multiple labels). Distinct from multi-label learning , which assign a binary-valued vector to each instance, LDL assigns each instance a real-valued vector, akin to descrete probability distributions, to represent the description degree of each label to the instance. Label distributions provide fine-grained information about label polysemy, and thus have been applied to many practical tasks, such as sentiment analysis , facial age estimation  and movie rating prediction .

A fundamental bottleneck hindering LDL is the difficulty in acquiring ground-truth label distributions, as the accurate quantification of these distributions can be prohibitively expensive. Therefore, LE (Label Enhancement)  is proposed to automatically infer label distributions from the more easily accessible multi-label data. Multi-label data is based on binary annotations (i.e., utilizing binary values \( 1\) to annotate each label) which demand that experts accurately identify whether each labelcan describe the instance. However, accurate identification is challenging in real-world tasks due to the prevalence of ambiguous instances and labels which provoke uncertain binary associations between the instances and the labels. For example, the facial image in Figure 1 can be definitely (negatively or positively) associated to "happy", "sad", and "surprise", whereas the association with "fear" is uncertain, since it is not clear whether the emotion comes from being stimulated by something frightening. If experts are coerced into providing a definitive annotation in uncertain cases, not only does it diminish the efficiency of annotating, but it also heightens the risk of erroneous annotation.

Therefore, we propose to predict label distribution from ternary labels. Inspired by the philosophies of three-way decision or three-world thinking , where decision-makers are provided with the flexibility to delay judgment when the available information is inadequate to support a determined decision, ternary labels take values from \(\{-1,0,1\}\), where "\( 1\)" indicates whether the label can describe the instance, and "\(0\)" denotes that the relationship between the label and the instance is uncertain. For the proposed learning paradigm, we conduct theoretical and methodological studies. In the theoretical part, we first quantify the errors of approximating the ground-truth label description vector 2 from ternary labels and binary labels, respectively. Further, we conduct a quantitative comparison of approximation error between ternary and binary labels to elucidate the superiority of ternary labels. In the methodological part, we propose CatRO distribution (Categorical distribution with Monotonicity and Orderliness) to model the mapping from label description degrees to ternary labels, which can serve as the probability distribution for generating ternary labels or as the loss function for measuring the inconsistency between the ternary labels and label distributions, allowing most existing LE methods to be adapted to our task. Specifically, we first analyze the rules governing the generation of ternary labels from label description degrees, and formalize them as the assumptions about the probabilistic monotonicity and orderliness of ternary labels. Further, we derive the probability mass function of CateMO to ensure probabilistic monotonicity and orderliness. Finally, we create two comparison algorithms and evaluate the prediction performance on three real-world datasets. Experimental results unequivocally affirm the superiority of ternary labels. Our contributions can be summarized as follows:

* We propose to predict label distribution from ternary labels, which not only enhances the annotation accuracy but also significantly reduces the annotating cost when contrasted with the traditional binary annotating methods.
* We rigorously analyze the error of approximating the ground-truth label description degrees by ternary and binary labels, respectively, which provides a quantitative elucidation of the superior performance of the ternary label.
* We propose the CateMO distribution specifically designed to capture the mapping from label description degrees to ternary labels, which is theoretically constructed to maintain the monotonicity and ordinality of the probabilities associated with ternary labels.

## 2 Related Work

To address the challenge of obtaining accurate ground-truth label distributions, LE (Label Enhancement)  is proposed as a method to automatically infer label distributions from more accessible

Figure 1: An annotation from JAFFE dataset .

multi-label data, including binary labels  and multi-label rankings [13; 15]. Existing works on LE can be broadly categorized into discriminative LE methods and generative LE methods. Discriminative LE directly treats the label distribution as conditional probabilities of the labels given feature observations, and subsequently mine additional information to estimate the conditional probabilities. Generative LE, on the other hand, focus on describing the generation process of observations in a principled manner and uncovering the underlying patterns of the observed data.

Current discriminative LE methods generally strive to optimize both \((,)\) and \(()\), where \((,)\) measures the inconsistency between the label description degrees \(\) and the more accessible labels \(\) (including binary labels and multi-label rankings), and \(()\) is the regularization terms based on various sources of information. Typically, \((,)\) is modeled using MSE (Mean Squared Error), and the information for \(()\) includes instance relationships and label correlations. For instance, several algorithms [5; 10; 20; 21; 23; 33; 34] assume that the instance manifolds based on features are similar to the instance manifolds based on label distributions. Additionally, several algorithms [11; 26; 29; 32] operate under the assumption that instances with similar feature vectors exhibit similarity in their label distributions. To incorporate label correlations, some algorithms [17; 25] regularize the label distributions using the Graph Laplacian operators of label correlation graphs. LEPNLR , on the other hand, enhances the labels by preserving the ranking relation of labels. Existing generative LE methods generally decompose the joint distribution of complete data as \(p(|)\), \(p(|)\), and \(p()\), where \(p(|)\) models the relationship between \(\) and \(\), and \(p(|)\) captures the relationships between \(\) and other observed variables. Current works primarily focus on modeling \(p(|)\) and \(p(|)\). For instance, LEVI [27; 28] and GLEMR  model \(p(|)\) as a Gaussian distribution and a ranking-preserved distribution, respectively. GLERB  and LEIC  model \(p(|)\) by incorporating instance relationships and label correlations into generation processes.

Furthermore, similar to ternary labels, the binary labels with missing values  are formally represented by \(\{0, 1\}\). However, it should be emphasised that they inherently differ in the following aspects. First, they differ in the origin of the labels with value of \(0\), i.e., the missing labels in the literature  and the uncertain label in ternary labels. The missing labels stem from the "absence", i.e., the association between the label and the corresponding instance is undocumented or unannotated rather than undeterminable. By contrast, the uncertain labels stem from the "uncertainty", i.e., it is difficult for experts to definitively determine whether the label is relevant to the corresponding instance. Second, they differ in the range of the underlying label description degree. The description degree of the missing labels to the corresponding instance can take any value in the interval \(\), since the missing labels may be relevant labels, irrelevant labels, or uncertain labels. By contrast, the description degree of the uncertain labels to the corresponding instance take values in a small sub-interval of \(\), which is much tigher than the range of the description degrees of missing labels.

## 3 Quantitative analysis

### Preliminary

Given an instance with feature vector of \(\), the label description vector of the instance is denoted by \(\) whose \(m\)-th element \(z_{m}\) indicates the description degree of the \(m\)-th label to the instance \(\). If an instance is annotated with binary labels, the label description vector will be degenerately expressed as a vector of binary values \(\{-1,1\}^{M}\) whose \(m\)-th element \(b_{m}\) denotes whether the \(m\)-th label can describe the instance or not. If an instance is annotated with ternary labels, the label description vector will be degenerately expressed as a vector of ternary values \(\{-1,0,1\}^{M}\) where \(s_{m}=1\) and \(s_{m}=-1\) denotes that the \(m\)-th label can describe and cannot describe the instance, respectively, and \(s_{m}=0\) denotes that the association between the \(m\)-th label and the instance \(\) is uncertain.

### Approximation error analysis

To quantify the advantages of the ternary label over the binary label, we leverage EAE (Expected Approximation Error) [13; 15] which measures the error of approximating the true label description degree by a more accessible label such as binary label, multi-label ranking, or ternary label. The definition of EAE is formalized as follows:

**Definition 1**: _Given a label with the true label description degree \(z\), if the label is annotated by a reduced label with the approximate label description degree \(}\), then the expected approximation error of the reduced label, i.e., \((},)\), is quantified as:_

\[(},)=_{z}_{z}}}(z-)^{2}z, V =_{z}z,=_{z}} . \]

According to Definition 1, it can be seen that EAE depends on \(\) and \(}\), so below we propose the following assumptions to determine \(,}\) and their relationships.

**Assumption 1**: _As stated in the introduction, a label in practical cases can be positive (i.e., the label can describe the instance), negative (i.e., the label cannot describe the instance), and uncertain (i.e., the association between the label and the instance is uncertain). If a label is positive, then the true label description degree \(z(,1]\). If the label is negative, then the true label description degree \(z[0,)\), where \([0,]\). If the label is uncertain, then the true label description degree \(z[,]\). That is, \(z_{s}\),_

\[_{s}=z:s=1 z(,1]s= -1 z[0,)s=0 z[,]}. \]

_If a label is annotated by a ternary label \(\), then we have the \(\)-based label description degree \(_{}\),_

\[_{}=:=1(,1]=-1[0,) =0[,]}, \]

_where \(0 1\) are the predefined parameters as approximations to \(\) and \(\), respectively, since \(\) and \(\) are unavailable from the annotation results._

_If a label is annotated by a binary label \(\), then we have the \(\)-based label description degree \(_{}\),_

\[_{}=:=-1[0, )=1[,1] }, \]

_where \([,]\) must hold, otherwise there exists some label description degree corresponding to "\(=1=-1\)" or "\(=-1=1\)", which is semantically contradictory._

**Assumption 2**: _Since a label can be positive, negative or uncertain in practical cases, we discuss the relationships between the true label and the annotation results in each of the three cases below._

_If a positive or negative label (i.e., \(s= 1\)) is annotated by a ternary value \(\) or a binary value \(\), then we have \(==s\)._

_If an uncertain label (i.e., \(s=0\)) is annotated by a ternary value \(\) or a binary value \(\), then we have \(=s\), \(p(=-1|s=0)=\), and \(p(=1|s=0)=1-\), where \(0 1\)._

Next, we give the analytical form of EAE of the binary label and the ternary label.

**Theorem 1**: _Given a label equiprobably being positive, negative, and uncertain, if the label is annotated by a ternary value, then we have the expected approximation error:_

\[_{,s}[(_{},_{s})]=(+)^{2}+(+)^{2}-(+)-(+)(+ )+(1--). \]

_If the label is annotated by a binary value, then we have the expected approximation error:_

\[_{,s}[(_{},_{s})]=( -}{9})+}{9 }+^{2}---}{3}--3}{18}, \]

_where \(=p(=-1|s=0)\). Suppose that \(()\), \(( 0 1)\), \([,]([,] 0 1)\), we have_

\[_{,s,,}[(_{}, _{s})]=36^{-1}(8^{2}+4-12+8^{2}-8+7), \] \[_{,s,,,,}[(_{},_{s})]=16^{-1}(2^{2}+2 -3+2^{2}-3+3).\]

\[_{,s,,}[(_{},_{s}) ]_{,s,,,,}[(_{ },_{s})]-^{2}+- ^{2}.\]The detailed proof of Theorem 1 can be found in Appendix. Figure 2 visualizes the results of Theorem 1. Specifically, Figure 2(a) shows the relationship between the predefined parameters \([,]\) and \(}_{_{,}[(_{}, _{s})]_{,s}[(_{}, _{s})]}\) which is defined as:

\[&}_{_{,}[( _{},_{s})]_{,s}[( _{},_{s})]}=|}_ {(,,,)}(_{,s}[ (_{},_{s})]_{,s}[( _{},_{s})]),\\ &=\{(,,,): r(1,10^{-3} ), r(,10^{-3}), r(1,10^{-3}), r(1,10^{-3})\},  \]

where \(()\) is an indicator function that outputs 1 if the internal condition is true, and 0 otherwise; \(r(u,v)\) outputs an increasing sequence \([0,v,2v,,u]\). Equation (8) measures the approximate proportion of cases where the ternary label is inferior to the binary label for all possible \([,,,]\) in \(\).

Figure 2(b) and Figure 2(c) show the distributions of the average EAE of the binary label and ternary label, respectively. Obviously, the ternary label outperforms the binary label in most cases. Specifically, the binary label shows superiority only in the extreme cases, i.e., \((,)\{(0,0),(0,0.1),(0.9,1),(1,1)\}\), but \(\) and \(\) basically never take these values in practical applications since both the binary and ternary labels exhibit very high approximation error in these cases. In addition, Theorem 1 also derives that \(_{,s,,}[(_{},_{s}) ]_{,s,,,,}[(_{ {b}},_{s})]\) if and only if \(-^{2}+- ^{2}\) and \(\), which is visualized as the overlapping area of the red and blue regions in Figure 3. It can be seen that the overlapping area is essentially consistent to the blue area in Figure 2(a). Therefore, the ternary label is superior to the binary label w.r.t. approximating the ground-truth label description degrees.

## 4 CateMO: Categorical distribution with monotonicity and orderliness

In order to learn label distributions from ternary labels, existing LE methods can be borrowed, the fundamental frameworks of which have been visualized in Figure 4 and illustrated in Section 2. It can be observed that both discriminative and generative LE methods necessitate modeling the relationship between the label description degrees \(\) and the more accessible labels \(\). Therefore,we propose CateMO (Categorical distribution with Monotonicity and Orderliness) to model the conditional probability of ternary label given the label description degree. CateMO can serve as \(p(|)\) in generative LE methods, and the negative likelihood function of CateMO can be used as \((,)\) in discriminative LE methods, so that most existing LE methods can be employed to address our task by replacing \((,)\) and \(p(|)\) with CateMO. In the following subsections, we first provide an intuitive discussion of the rules governing the generation of ternary labels and then formalize these rules as the assumptions about the probability monotonicity and orderliness of ternary labels. Furthermore, we derive the parametric mass function for CateMO, which is theoretically guaranteed to maintain the probability monotonicity and orderliness of ternary labels.

### Generation rules of ternary labels

On the one hand, we explore how the label description degree affects the probabilities of the label being positive, negative and uncertain. Obviously, a label is more likely to be positive if the description degree of the label is larger; a label is more likely to be negative if the description degree of the label is smaller. Besides, the uncertain label is used to encode situations where the expert is unsure whether the label can describe the instance, which arises from the fact that the probability of the label being positive is close to the probability of the label being negative. Therefore, we believe that a label is more likely to be uncertain if the probabilities of the label being positive and negative are closer. The above intuitions can be formalized as follows:

**Assumption 3** (Probability monotonicity of ternary labels): _Given any two real values \(v_{1}\) and \(v_{2}\) between \(0\) and \(1\), if \(v_{1}<v_{2}\), then \(p(s=1|z=v_{1})<p(s=1|z=v_{2})\) and \(p(s=-1|z=v_{1})>p(s=-1|z=v_{2})\). If \(0<p(s=1|z=v_{1})-p(s=-1|z=v_{1})<p(s=1|z=v_{2})-p(s=-1|z=v_{2})\) or \(0<p(s=-1|z=v_{1})-p(s=1|z=v_{1})<p(s=-1|z=v_{2})-p(s=1|z=v_{2})\), then \(p(s=0|z=v_{1})>p(s=0|z=v_{2})\)._

On the other hand, we explore how the label description degree affects the orderliness among the probabilities of the label being positive, negative and uncertain. Obviously, a label is most likely to be negative and least likely to be positive if the label description degree is sufficiently small; a label is most likely to be positive and least likely to be negative if the label description degree is sufficiently large; a label is most likely to be uncertain if the label description degree is moderate. We formalize the intuition as follows:

**Assumption 4** (Probability orderliness of ternary labels): _There exists two real values \(0 v_{1}<v_{2} 1\), \(p(s=1|z)<p(s=0|z)<p(s=-1|z)\) holds for any \(z<v_{1}\), \(p(s=-1|z)<p(s=0|z)<p(s=1|z)\) holds for any \(z>v_{2}\), \(\{p(s=1|z),p(s=-1|z)\}<p(s=0|z)\) holds for any \(v_{1}<z<v_{2}\)._

### Probability mass function of CateMO distribution

Since a ternary label can take three possible values, i.e., \(s\{0, 1\}\), we model ternary label by a categorical distribution, which can be formalized as follows:

\[p(s|z)=(s[(z),(z),(z)]), \]

Figure 4: Fundamental frameworks of existing LE methods. \(\) can be either binary labels or multi-label rankings. The learning target of discriminative LE methods can be decomposed as the inconsistency between \(\) and the label description degrees \(\), i.e., \((,)\), and the regularization term of \(\) based on other data, i.e., \(()\). The joint distribution of the generation process in generative LE methods can be decomposed as the conditional probability of \(\) given \(\), i.e., \(p(|)\), and the generative distributions of other observed variables, i.e., \(p(|)\).

where \((z)=p(s=-1|z)\), \((z)=p(s=0|z)\), and \((z)=p(s=1|z)\) represent the generation principles from the label description degree to the negative, uncertain, and positive labels, respectively. We name \((z)\), \((z)\), and \((z)\) as ternary generation functions. According to Assumption 3, we preliminarily assume the parametric form of ternary generation functions as follows:

\[(z)=e^{-z^{2}},(z) =e^{-(z-)^{2}},(z)= e^{-(z-1)^{2}}, \]

where \(Z=e^{-z^{2}}+e^{-(z-)^{2}}+e^{-(z-1)^{2}}\), \(>0\), \(>0\), \(>0\) and \(0<<1\) are parameters. Intuitively, the parameters \((,,)\) largely governs the precision of CateMO distribution (despite the fact that they are not exactly equal), which is similar to the reciprocal of the temperature coefficient in the softmax layer of a deep neural network. Hence, we refer to the parameters \(\), \(\), and \(\) as the precision of negative labels, positive labels, and uncertain labels, respectively. Figure 5 visualizes the shape of \(p(s|z)\) on different parameters, which can be found that some parameter configurations violate the above three assumptions. For example, Figure 5(a) violates Assumption 3, Figure 5(c) violates Assumption 3 and Assumption 4 at the same time. Therefore, we propose Theorem 2 to ensure that the ternary generation functions defined by Equation (10) satisfy the proposed assumptions about probability monotonicity and orderliness of ternary labels.

**Theorem 2**: _Given \(>0\), \(>0\), and \(>0\), the ternary genertaion functions satisfy Assumption 3 and Assumption 4 if the following conditions hold:_

\[&- --)^{-1}, =(2}+2})^{-1}( 2}- }+}),\\ &\{(+e^{})^{-1},((1+e^{})(1-))^{-1}\}< <\{(1-)^{-1},^{ -1}\}. \]

The details of the proof can be found in Appendix. Therefore, the probability mass function of CateMO distribution can be formalized as follows:

\[&(s=1 z)=Z^{-1}e^{-(z-1)^{2}},(s=0 z)=Z^{-1}e^{-(z-)^{2}}, \\ &(s=-1 z)=Z^{-1}e^{-z^{2}},Z= e^{-z^{2}}+e^{-(z-)^{2}}+e^{-(z-1)^{2}}, \\ &\ =(2}+2 })^{-1}(2}- }+}),---)^{-1},\\ &,,>0,\{ +^{}^{-1},((1 +e^{})(1-))^{-1}\}<< \{(1-)^{-1},^{-1}\}.  \]

Finally, to apply CateMO, we simply replace \((,)\) in Figure 4(a) with the negative log-likelihood of CateMO and replace \(p(|)\) in Figure 4(b) with CateMO, which can be formalized as follows:

\[(,)=-_{m=1}^{M}(s_{m} z_{m}),  p(|)=_{m=1}^{M}(s_{m} z_{m}). \]

Figure 5: \(p(s|z)\) on different parameters. The horizontal and vertical axes denote the label description degree \(z\) and the probability \(p(s|z)\) defined by Equation (9) and Equation (10), respectively.

Experiments

### Datasets and evaluation measure

Although there are many LDL datasets, they all lack ground-truth ternary label data. Therefore, we select three real-world LDL datasets (i.e., JAFFE , Painting , and Music ), and manually re-annotate them with both binary labels and the ternary labels. The details of these datasets can be found in Appendix. We use five common LDL metrics to evaluate the algorithm performance, which are Cheb (Chebyshev distance), KL (Kullback-Leibler divergence), Cosine (cosine coefficient) , and Rho (Spearman's rho coefficient) . The lower values of Cheb and KL indicate the better performance. The higher values of Cosine, Intersec, and Rho indicate the better performance. We use "+" and "+" to denote that the better performance is represented by the higher and lower values of a metric, respectively.

### Comparison methods and experimental procedures

Comparison methodsSince there is no LE method specifically for ternary labels, we design two approaches to construct the comparison algorithms. On the one hand, we design a data transformation method (which is abbreviated as DT method) to transform the dataset with ternary labels into an extended dataset with binary labels so that any existing LE algorithm can be applied to ternary labels. Specifically, for any instance with uncertain labels, we transform the instance into two instances which set all uncertain labels to positive and negative labels, respectively. For instance, the example \([,]\) will be transformed into \([,]\) and \([,[1,-1,-1]]\), respectively. On the other hand, we replace the loss term \((,)\) with \(\|(+1)/2-\|_{2}^{2}\), so that most existing LE methods can be used to enhance ternary labels. We abbreviate this method as MSE method. Besides, we select three recently proposed binary label enhancement algorithms: GL , LR , and MR . The hyperparameter settings follow their respective literature. We combine GL, LR and MR algorithms with DT and MSE methods in pairs to construct six comparison algorithms: GL-DT, GL-MSE, LR-DT, LR-MSE, MR-DT, and MR-MSE. In terms of our proposal, we replace the conditional distribution \(p(|)\) in MR with CateMO and replace \((,)\) in GL and LR with the negative log-likelihood function of CateMO, which constructs three algorithms: GL-CateMO, LR-CateMO, and MR-CateMO. We set the parameters \([,,]\) in CateMO as \(\).

Experimental proceduresWe aim to test the performance of label distribution prediction based on different LE algorithms. Specifically, we use different LE methods to recover training label distributions and use these recovered label distributions to train an LDL model, whose performance on test instances will be reported. LDL-LRR  is used as the LDL model in this paper, whose hyperparameters \(\) and \(\) are selected from \(\{10^{-6},10^{-5},,10^{-1}\}\) and \(\{10^{-3},10^{-2},,10^{2}\}\) as suggested . We randomly partition the whole dataset (70% for training and 30% for testing), and repeat the above process ten times and report the average and standard deviation of the results.

### Results and discussions

Table 1 shows the prediction performance of the comparison algorithms on three datasets. Each result is formatted as "mean\(\)std". In the first column of Table 1, "MR", "LR" and "GL" denote the existing binary LE algorithms, the suffix "-LL" denotes that these algorithms run on binary labels directly, and the suffixes "-DT", "-MSE" and "-CateMO" denote that these algorithms run on ternary labels by the DT method, MSE method and our proposed CateMO distribution, respectively. "Ground-Truth" denotes that LDL-LRR is trained directly on the ground-truth label distributions. In each area separated by dashed lines, **bold** and _italics_ denote the 1st and 2nd, respectively. The results of statistical significance test are shown in Appendix. It can be seen that the performance of our proposed CateMO is better than other three comparison algorithms in all cases, and is close to "Ground-Truth". To visualize the advantages of ternary labels over binary labels, we show the relationship between binary labels, ternary labels, and label distributions in terms of annotation time and prediction performance in Figure 6. The horizontal axis denotes the average time (in seconds) spent by an expert in annotating a label for an instance. The vertical axis denotes the average prediction performance calculated from Table 1. It can be seen that ternary labels is superior to binary labels in terms of both prediction performance and annotating cost.

   Method & Cheb (\(\)) & KL (\(\)) & Cosine (\(\)) & Intersec (\(\)) & Rho (\(\)) \\   \\  Ground-Truth & \(0.165 0.010\) & \(0.145 0.015\) & \(0.897 0.009\) & \(0.793 0.011\) & \(0.463 0.051\) \\ MR-CateMO & \(\) & \(\) & \(\) & \(\) & \(\) \\ MR-MSE & \(0.183 0.013\) & \(0.166 0.017\) & \(0.884 0.010\) & \(0.774 0.014\) & \(\) \\ MR-DT & \(\) & \(\) & \(\) & \(\) & \(0.407 0.033\) \\ MR-LL & \(0.180 0.010\) & \(0.171 0.014\) & \(0.883 0.009\) & \(0.774 0.008\) & \(0.383 0.028\) \\ LR-CateMO & \(\) & \(\) & \(\) & \(\) & \(\) \\ LR-MSE & \(\) & \(\) & \(\) & \(\) & \(0.422 0.045\) \\ LR-DT & \(\) & \(\) & \(\) & \(\) & \(0.424 0.044\) \\ LR-LL & \(0.201 0.015\) & \(0.320 0.145\) & \(0.847 0.023\) & \(0.738 0.027\) & \(0.320 0.072\) \\ GL-CateMO & \(\) & \(\) & \(\) & \(\) & \(\) \\ GL-MSE & \(\) & \(\) & \(\) & \(\) & \(\) \\ GL-DT & \(0.206 0.017\) & \(0.210 0.023\) & \(0.857 0.013\) & \(0.745 0.016\) & \(0.381 0.058\) \\ GL-LL & \(0.199 0.014\) & \(0.311 0.156\) & \(0.852 0.024\) & \(0.742 0.027\) & \(0.323 0.078\) \\   \\  Ground-Truth & \(0.252 0.009\) & \(0.535 0.017\) & \(0.737 0.007\) & \(0.605 0.009\) & \(0.316 0.038\) \\ MR-CateMO & \(\) & \(\) & \(\) & \(\) & \(\) \\ MR-MSE & \(\) & \(0.564 0.029\) & \(\) & \(\), \(\), and \(\)) of CateMO are pre-fixed. In fact, a more adaptive approach is to collaboratively learn these parameters and other model parameters. However, since the parameters of CateMO satisfy the conditions shown in Theorem 2, in which the parameters are interdependent, it may lead to difficulties in directly exploiting gradient descent optimization methods. Therefore, in future works, we will explore how to appropriately reduce the parameter space of CateMO so that the parameters \(\), \(\), and \(\) can be decoupled from each others.

ConclusionIn this paper, we propose to predict label distribution from ternary labels, which reduces both the annotation inaccuracy and cost when contrasted with the traditional binary annotating methods. In the theoretical part, we analyze the approximation error for both ternary and binary labels, which provides a quantitative elucidation of the superior performance of the ternary label. In the methodological part, we propose the CateMO distribution to model the mapping from label description degrees to ternary labels, which is theoretically constructed to maintain the monotonicity and ordinality of the probabilities associated with ternary labels. In the experimental part, extensive experiments demonstrate the effectiveness of our proposal.

## 7 Acknowledgments

This work was partially supported by the National Natural Science Foundation of China (62176123, 62476130), and the Natural Science Foundation of Jiangsu Province (BK20242045).

Figure 6: Cost-benefit analysis of different forms of labels. The horizontal and vertical axes denote the average annotating time (in seconds) and performance, respectively.

Figure 7: Recovery performance of GL-CateMO with varying precision parameters.