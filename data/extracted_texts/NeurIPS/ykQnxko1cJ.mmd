# CemiFace: Center-based Semi-hard Synthetic Face Generation for Face Recognition

Zhonglin Sun

Queen Mary University of London

zhonglin.sun@qmul.ac.uk

&Siyang Song

University of Exeter

ss2796@cam.ac.uk

&Ioannis Patras

Queen Mary University of London

i.patras@qmul.ac.uk

Corresponding Author

&Georgios Tzimiropoulos

Queen Mary University of London

g.tzimiropoulos@qmul.ac.uk

###### Abstract

Privacy issue is a main concern in developing face recognition techniques. Although synthetic face images can partially mitigate potential legal risks while maintaining effective face recognition (FR) performance, FR models trained by face images synthesized by existing generative approaches frequently suffer from performance degradation problems due to the insufficient discriminative quality of these synthesized samples. In this paper, we systematically investigate what contributes to solid face recognition model training, and reveal that face images with certain degree of similarities to their identity centers show great effectiveness in the performance of trained FR models. Inspired by this, we propose a novel diffusion-based approach (namely **C**enter-based **S**emi-hard Synthetic Face **G**eneration (**CemiFace**)) which produces facial samples with various levels of similarity to the subject center, thus allowing to generate face datasets containing effective discriminative samples for training face recognition. Experimental results show that with a modest degree of similarity, training on the generated dataset can produce competitive performance compared to previous generation methods. The code will be available at:https://github.com/szlbibibibiu/CemiFace

## 1 Introduction

Face Recognition (FR) has gained significant achievement in recent years owing to the combination of discriminative loss function [1; 2; 3; 4; 5], proprietary backbones [6; 7; 8; 9; 10; 11] and large-scale face datasets [12; 13; 14; 15]. For example, with a 4M training set, existing FR models can achieve over 99% accuracy on various academic datasets [3; 6; 2; 16]. However, in real-world industrial face recognition applications, collecting large-scale face datasets is not always available due to the related licence agreements, ethical issues and privacy policies .

To expand limited training samples in real-world scenarios, generative models [18; 19; 20; 21; 22; 23] are widely adopted owing to their ability to generate high-quality images. However, simply adopting face images produced by those generic generative models to train face recognition models is impractical as there is ambiguity about the identities of the produced images because they are derived from random noises, i.e., the identities of these generated face images cannot be obtained without a well-trained FR model . To address such issues, synthetic face dataset generation-based solutions [24; 25; 26; 27; 28] have been found to gain benefits in developing effective face recognition models. Existing synthetic face recognition (SFR) methods are frequently built upon recent advances of generative modelssuch as Style-GAN [25; 27], Diffusion methods [24; 27] and 3DMM rendering . For instance, a style-transferring diffusion model-based method namely Dcface  is proposed, which increases the diversity of existing face recognition datasets by generating additional discriminative face images with different styles (e.g. hair, overall lighting, which can be observed in visualization Section 4.3.2) for each subject. However, domain gap issues exists as the model is trained with paired face images belonging to the same identity, while those paired images are not available at the inference stage. It can only take samples belonging to different identities at the inference stage, which may negatively impact on the images synthesized at the inference stage. Furthermore, the definition of discriminative facial images remains unclear in this study.

To address the problems outlined above, firstly we explore the factors resulting in performance degradation for SFR and reveal that previous approaches fail to consider the properties of effective FR training-relationship/similarity between samples. Consequently, considering the facts: (a) semi-hard negative samples are crucial to train effective face recognition model for Triplet loss ; (b) samples close to the decision boundary contribute most to the training gradient ; (c) all face images belonging to the same subject can be represented by a hypersphere in the latent feature space  (i.e., can be measured by existing FR models, e.g., AdaFace ), whose distances (radius) to the identity center are negatively correlated to their similarities to the center. We hypothesize that face recognition performance is sensitive to the data with different levels of similarity to the identity center in the hypersphere, and experimentally reveal that the optimal performance is obtained with samples of mid-level similarity, which we term **center-based semi-hard samples**. Inspired by this crucial finding, we propose a novel diffusion-based synthetic face recognition approach (**CemiFace**) which generates **c**enter-based semi-hard face samples by regulating the similarity between the generated image and the inquiry image, through a similarity controlling factor condition. Figure 1 presents the overall hypothesis by showcasing samples with various similarities to the identity center. Comprehensive experiments are conducted to illustrate the effectiveness of our proposed CemiFace. Our method achieves promising performance in synthetic face recognition (SFR). The main contributions and novelties of this work are summarized as follows:

* We provide the first comprehensive analysis to illustrate how FR model performance is affected by different levels of similarity of samples, particularly center-based semi-hard samples.
* We propose a novel diffusion-based model CemiFace that can generate face images with various levels of similarity to the identity center, which can be further applied to generate infinite center-based semi-hard face images for SFR.
* We demonstrate our method can be extended to use as much as the data without label supervision for training which is an advantage over the previous method .
* Experiments show that our CemiFace surpasses other SFR methods with a large margin, reducing the GAP-to-Real error by half.

## 2 Related Works and Preliminary

**Synthetic Face Generation for FR:** With the emergency of generative models, synthesizing facial data for various facial tasks has become a critical issue, such as applications in Face Anti-sproofing  and Face Recognition [25; 26; 24; 28; 32; 33]. SynFace  aims to mix the real

Figure 1: Visualization of the samples with different similarities. Given an inquiry image, it can form a hypersphere based on similarity to the inquiry image, where samples with the same similarity share the same radius. Samples with similarities between 0 to 1 with an interval of 0.33 are shown. With our proposed CemiFace, each inquiry image finally forms a novel subject.

images with the DiscoFaceGAN-generated  samples. DigiFace  uses 3DMM for rendering facial images to construct the dataset. DCFace  takes diffusion models to adapt style from the style bank to the identity image and result in discriminative samples with diverse styles. IDiffFace  proposes the condition latent diffusion models  to the feature embedding and images are synthesized by pretrained decoder.

**Preliminary-DDPM:** Diffusion models [21; 22] are generative models which denoise an image from a random noise image. The training pipeline for diffusion models consists of a forward process wherein noise is progressively added to a given image and a denoising process to predict the estimated noise for effective denoising. A single forward process is formulated as Markov Gaussian diffusion with timestep \(t\):

\[q(_{t}|_{t-1})=(_{t};}_{t-1},_{t})\] (1)

Where \(()\) is adding noise function. When accumulating the time step over \(0-\), the final process is given as follows:

\[q(_{1:}|_{0})=_{t=1}^{}q( _{t}|_{t-1})\] (2)

Then the denoising process is conducted to predict the noise for the time step \(t\) using a model \(_{}\) ( typically a UNet ), the training loss is:

\[L_{}=E_{t,_{0},}||_{}(_{t}}_{0}+_{t}},t)-||_{ 2}^{2}]\] (3)

where \(_{t}\) is the pre-set forward process variances. Then notation \(_{t}\) is given as: \(_{t}=_{s=1}^{t}_{s}\) and \(_{t}=1-_{t}\). \(\) is a random Gaussian noise image \((0,1)\).

## 3 The proposed approach

In Section 3.1, we first investigate the relationship between sample similarity and their effectiveness in training FR models, presenting the finding that samples with certain similarities (i.e., center-based semi-hard samples) to their identity centers are more effective for training FR models on a real dataset and subsequently devise a toy experiment to validate it. Inspired by our findings, in Section 3.2 we propose a novel CemiFace, a conditional diffusion model that produces images with various levels of similarity to an inquiry image. Specifically, Section 3.2.1 introduces how we construct the similarity condition which is fed to diffusion model to guide the generation, and discusses the \(L_{SimMat}\) to require the generated sample to exhibit a certain similarity degree to the inquiry image. In Section 3.2.2, we then present how to use our diffusion model to generate a synthetic face dataset given a fixed similarity condition \(m\) and a set of inquiry images.

### The Relationship between Samples Similarity and Performance Degradation

**Performance Degradation for Synthetic Face Recognition:** Face recognition models trained on face images synthesised by existing generative models (e.g., style-transfering , 3DMM rendering  and latent diffusion expansion ) frequently suffer from performance degradation [24; 25; 26]. For example, with the same data volume, the model trained on the state-of-the-art synthetic dataset DCFace  produces 11.23% lower verification performance on CFP-FP testset than the model with the same architecture trained on the real dataset. A key reason for this issue is that these generative models only intuitively explore the properties of discriminative samples, but fail to consider the similarity levels among synthesized face images. However, previous studies [29; 2; 3] empirically reveal that _training effective FR models intrinsically relies on semi-hard negative samples in Triplet Loss  or samples close to the decision boundary [2; 3; 35]_.

**Hypothesis and Findings:** Since face images belonging to the same identity/class can be aggregated within a hypersphere , where the location of each face image is decided by its similarity to the identity center (the center of the hypersphere) (illustrated in Fig. 1). We treat all face images of each subject as an N-1 (N=512 in AdaFace ) dimensional sphere with its center representing the subject-level identity center. Then, the spheres of all subjects can be combined in an N-dimensional sphere, where each subject-level sphere is a cluster.

Based on this, we hypothesize that samples of mid-level similarities to the identities center play a dominant impact on the FR performance, as they exhibit discriminative style variations (e.g. age,pose). To validate the hypothesis, we conduct the first comprehensive investigation for the impact of different levels of similarity to the identity center on the FR performance. We first split face images in the CASIA-WebFace  into various levels of groups according to their similarities to their corresponding subject-level identity centers. Here, the identity center of each subject is obtained by the weight of the linear classification layer, trained using AdaFace . To avoid the impacts caused by different numbers of training samples, we assign around 100k face images to each group representing close similarities to their identity centers. This results in 5 distinct similarity groups. Table 1 reports the performance of model trained on each group and test on five standard face recognition evaluation datasets . Table 9 in _Supplementary material_ Section B.4 displays the similarity range of each group. We further validate the style variation in Visualization Sec 4.3.2. We also visualize randomly selected samples of each group in Figure 2. Results reveal that groups with middle-level similarities (0.76 and 0.70) produced similar but top-performing average accuracy. This indicates that **face images of a certain low similarity to their identity centers (which we refer to as center-based semi-hard samples) are essential for learning highly accurate face recognition models.** In contrast, the group whose images have the lowest similarity (i.e., 0.53) to their identity centers obtained the worst performance, which suggests that it is difficult to train an effective face recognition model with the most challenging samples (i.e., the samples are normally hard to be distinguished by human observation).

### Center-based Semi-hard Face Image Generator

Inspired by the above findings, this section proposes a novel conditional diffusion model, namely CemiFace, for synthesising effective center-based semi-hard face images given the inquiry image (identity center) \(\) and a pre-defined similarity controlling factor \(\), based on which a new discriminative synthetic dataset is obtained to train effective face recognition models.

Methodology overview:As illustrated in the left side of Fig 3, the training process starts with adding a noise \((0,1)\) with timestep \(t\) to the clean input image \(x\), resulting in \(x_{t}\). Meanwhile, similarity conditions \(m\) and identity condition \(}(x)\) are fed to the diffusion model by cross-attention as illustrated in the lower right part of Fig. 3. Consequently, the diffusion Unet \(_{}\) outputs the estimated noise \(^{}=_{}(x_{t},t,m,}(x))\) for denoising the image as a clean estimated image \(_{0}\). Based on the obtained estimated image \(_{0}\), original \(x\) and condition \(C_{att}\), the whole model is optimized by the combination of \(L_{MSE}\) and \(L_{SimMat}\), defined in the following Section 3.2.1 as well as the details of constructing condition.

At the inference stage (the upper right part of Fig. 3), random noise \(x_{t}=x_{}=(0,1)\) and the time step \(t=\) are first fed to a CD block. This results in an estimated noise \(^{}=_{}(x_{t},t,})\). Then, a denoise step is adopted to generate \(x_{t-1}\) from \(x_{t}\) for efficient interface speed. This process is repeatedly conducted on the obtained denoised latent images (\(x_{t-1},x_{t-2},,x_{0}\)) until \(t=0\), where \(x_{0}\) is treated as the final generated face image. Here, we assign the same identity label as \(x\) to all face images generated from the inquiry image \(x\). To ensure high inter-class variation, our inquiry images are filtered by a pretrained FR ( IR-101 trained on the WebFace4M  dataset by AdaFace.), which enforces the similarity between each pair of query images is lower than 0.3. The number of identities is fully decided by the number of inquiry face images. The pseudo-code for training and generation are given in _Supplementary Material_ Section A.3.

  Sim & LFW & CFP-FP & AgeDB & CALFW & CPLFW & AVG \\ 
0.85 & 98.43 & 85.67 & 89.43 & 91.08 & 82.78 & 89.48 \\
0.81 & 98.91 & 88.8 & 91.03 & 91.71 & 84.58 & 91.01 \\
0.76 & **98.94** & 90.92 & **91.5** & **91.7** & **85.85** & **91.78** \\
0.70 & 98.66 & **91.08** & 90.32 & 90.76 & **86.92** & 91.55 \\
0.53 & 94.63 & 82.12 & 77.63 & 80.11 & 77.3 & 82.36 \\  

Table 1: Accuracy of groups with different similarities. Sim means the average similarity to the identity center. AVG is the average accuracy on the 5 evaluation datasets

Figure 2: Samples with different similarity groups from CASIA-WebFace dataset. From left to right are samples with lower similarity to the identity center

#### 3.2.1 Training CemiFace

To facilitate our diffusion-based CemiFace can generate diverse center-based semi-hard face images, we propose a novel diffusion model training strategy. During training, a random Gaussian noise image \((0,1)\) is firstly added to a clean face image \(\) at the time step \(t\), before feeding it to the diffusion model to generate the noise face image \(}\):

\[}=}}}+}}\] (4)

Then, conditions are constructed based on the similarity controlling factor \(\), the identity condition \(}\) and time step \(t\) condition. Subsequently, the diffusion model outputs the estimated noise \(^{}=_{}(},t,},)\) for denoising the image.

**Constructing Similarity Controlling Condition:** To address the purposes of generating images at different scales of similarities, two conditions are injected into the diffusion process to guide the generation process. The first one is the identity condition \(}\) aiming to anchor the center of the generated facial images which can be formulated as:

\[}=E_{}()\] (5)

where \(E_{}\) is a pre-trained face recognition model (e.g., IResnet-50 pretrained from AdaFace ). \(}\) represents the feature embedding of the given image \(\). Then the most important part is similarity controlling condition \(}\) which maps the scalar similarity \(\) into feature embedding. This condition serves to regulate the similarity to the inquiry image, facilitating the generation of images spanning from the most challenging samples (\(\)=-1) to the most similar ones (\(\)=1).

\[}=F_{1}()\] (6)

Where \(F_{i}()\) is the linear projection layer. Then following DCFace  the two conditions are combined and projected as cross-attention conditions for sending to the DDPM process. AdaGN  is adopted to embed time step condition \(t\). \(()\) is the concatenation operation.

\[}=F_{2}((},}))\] (7)

Figure 3: Illustration of our proposed method. The left part is the training framework for learning images with various levels of similarity. Firstly noise is added to the clean facial image before it is processed by the diffusion model. Then similarity controlling condition \(\) ranging between [-1,1] with facial embedding is injected to guide the generation. Consequently, the model outputs the estimated noise, which is adopted to calculate the estimated image. We add similarity matching loss \(L_{}\) between the estimated image and the input image. For generation, we gradually denoise a noising image with time step scaling from \(\) to 0, conditions for identity and similarity are left fixed. The two diffusion models in the generation part mean the same diffusion model at two different time steps. The right bottom part is the details of using cross-attention to inject similarity condition and facial embedding into the diffusion modelsThe \(}\) is further processed by a cross-attention operation with the intermediate latent representation of diffusion UNet \(_{}\) learned from the input noisy image as:

\[CA(Q,K,V,K_{c},V_{c})=SoftMax(([K,K_{c}]W_{k})^{T}}{})W_{v}[V, V_{c}]\] (8)

where \(}\) is treated as the key \(K_{c}\) and value \(V_{c}\) (same as DCFace) to influence the generated face images. \(Q=K=V\) are the query, key and value, representing the latent feature of UNet \(_{}\).

**Training Loss:** To ensure the similarity between the generated face \(}\) and the corresponding inquiry image (identity center) \(\) adheres to the specified similarity factor \(\) as given in the following equation:

\[=(E_{}(),E_{}(}))\] (9)

where \(()\) denotes a similarity measurement (e.g., can be computed by Cosine Similarity or Euclidean Distance). Following DDPM [24; 21], an approximated clean sample \(}\) can be traced from \(_{t}\) at the time step \(t\) through the following formula:

\[}_{0}}=(_{t}-}}^{})/}}\] (10)

This gives a hint that the generated face image \(}\) can be controlled at the training phase by regularizing the estimated \(_{0}}\), which allows the gradient to be back-propagated to the diffusion model, e.g., controlling facial attributes  and styles . Inspired by this, we propose a novel similarity Matching loss \(L_{}\) aimed at disentangling the generated face image \(}\) to exhibit a certain similarity to the inquiry image, which is determined by the similarity controlling factor \(\). We employ the Time-step Dependent loss  with different time step t at Eq 13, specifically firstly an identity loss for recovering the identity of the original inquiry image x, which will be applied to produce original facial embedding when the time step \(t 0\):

\[L_{}=||1-(E_{}(),E_{ }(}_{0}))||_{2}\] (11)

Then, we require the estimated \(}_{0}\) to produce an feature embedding \(E_{}(}_{0})\) which matches the original \(\) with \(\) similarity as:

\[L_{}=||-(E_{}(),E_{ }(}_{0})||_{2}\] (12)

Consequently, the overall identity regularization loss at the time step \(t\) can be formulated as:

\[L_{}=(1-_{t})L_{}+_{t}L_{}\] (13)

where \(_{t}=}\) is the scaling weight for adjusting the similarity of the generated \(}_{0}\). At the time step \(t\)=0, the model outputs an image with the same identity as the original image \(\). When \(t\) scales from 0 to the maximum time step \(\), the generated face image gradually shifts far away from the \(\). When approaching \(\), the model will output the image with \(\) similarity to the original image. The proposed \(L_{}\) loss is inspired by the fact that facial images, with diverse styles but the same degree of similarity, are located at a circle of the hypersphere. This loss can regularize the model to learn this kind of pattern. Specifically, the similarity is guaranteed by our proposed loss, and the diversity is facilitated by the random noise \(\) of the diffusion models, which is validated in the Visualization Sec. 4.3.2. The overall training object is:

\[L=L_{}+ L_{}\] (14)

where \(\) is a hyperparameter for balance the training focuses on noise estimation or identity-related similarity regularization.

#### 3.2.2 Face Image Generation with Appropriate Similarity

Given a random noise \(\) and conditions(i.e. identity, similarity and time step), the well-trained model progressively denoises the noisy image \(\) with a varying time step \(t\) (from the maximum \(\) to 0) and a fixed similarity factor condition \(\) to generate a clean image with a specified similarity \(\) to the given inquiry image \(\), we adopt DDIM  for efficient interface speed.

We experimentally investigate the appropriate generation similarity \(\) for synthetic face recognition. Specifically, we first adopt fixed similarity factors to test the best similarity. We also explore mixing the similarity around the appropriate fixed \(\) (mixing semi-hard \(\)) and mixing appropriate fixed \(\) samples with easy samples (mixing easy \(\)).

Experiment

### Implementation Details

**Evaluation Metrics:** We examine the 1:1 verification accuracy trained on the dataset generated by our CemiFace on various famous testsets including LFW , CFP-FP , AgeDB-30 , CPLFW , CALFW  and their average verification accuracy **AVG**. Gap-to-Real is the gap to the results trained on CASIA-WebFace with CosFace loss.

**Details of CemiFace Training and Generation:** The condition **m** is appropriately adjusted during the training phase to facilitate better generalization across various similarities. Considering the overall cosine similarity ranges from -1 to 1, the model is enabled to discern differences in generated images under varying similarity controlling conditions when training. Specifically, in the mini-batch, we assign a randomly selected m from -1 to 1 with an interval of 0.02, allowing the model to generate corresponding images at different similarity scales. The synthetic face recognition datasets are generated in 3 volumes. Specifically in 0.5M data volume, we generate 50 images per subject and a total of 10k subjects; As for 1.0M, we keep 50 images per subject but with 20k subjects; For 1.2M, we add 5 images per subject with 40k subjects to the 1.0M settings. Oversampling method as used in DCFace is adopted which adds 5 repeated inquiry images to each subject. For more details including model, ablation studies and discussions please refer to _Supplementary material_ A.1, B and C

**Details of Training the Synthetic Dataset** As the training code of DCFace  and DigiFace  for training the SFR is not released. We opt for CosFace  with some regularizations to match the performance of DCFace . Specifically, the margin of Cosface is 0.4, weight decay is 5e-4, learning rate is 1e-1 and is decayed by 10 at the 26th and 34th epoch, totally the model is trained for 40 epochs. We add random resize & crop with the scale of [0.9, 1.0], Random Erasing with the scale of [0.02,0.1], and random flip. Brightness, contrast, saturation and hue are all set to be 0.1. The backbone opted for is IR-SE50 

### Ablation Studies

#### 4.2.1 Impact of Similarity m

**Appropriate m for Generation:** Herein we ablate how a scalar **m** influences the generation in terms of training performance. We adopt different **m** ranging from -1 to 1 with the interval of 0.1 to generate face groups of 10k identities with 10 samples per identity to match the data volume in the finding for CASIA-WebFace in Sec. 3.1. Figure 4 illustrates the accuracy curves when using those data for training face recognition (for detailed numerical results please refer to _Supplementary Material_ B.4.2). Similarity **m**=0 provides the best recognition performance **89.567** in terms of the AVG, then **m**=0.1 has the AVG of 89.368, with **m**=-0.1 obtains 87.708. It can be concluded the appropriate degree of similarity for generating discriminative samples is around 0 to 0.1, which is different from the similarity of CASIA-WebFace where the best recognition performance is obtained with the similarity of 0.7. This may be because the model for comparing similarity on CASIA-WebFace is pretrained on this dataset.

**Generation with Mixing m:** We conduct the experiment of including mixed **m** when generating the dataset, as is shown in the top part of Tab 2 and Tab 3. Specifically, we opt for mixing the generation **m** from -0.1 to 0.1, and from 0 to 0.1. We use training m varying from , and the generation interval is 0.02. The results show that mixing m with 0 to 0.1 when generating the data will bring worse performance compared to single m=0. However, mixing m with -0.1 to 0.1 obtains a similar performance compared to m=0. Additionally, progressively mixing m with easy and semi-hard

Figure 4: Accuracy of samples with different similarity varying from -1 to 1. The left figure is the specific performance on each evaluation dataset. The right figure is the average accuracy of our CemiFace

samples are provided in Tab 3, as observed, with more easy samples included in the training dataset, the FR performance reduced more prominent. We keep the generation m to be 0 for later discussion.

**Training with Various m:** The choice of various levels of \(\) during the training stage are ablated in the bottom part of Table 2 where 3 settings are considered when training CemiFace:(a) single \(\) with similarity of 0; (b) multiple discrete \(\) ranging from 0 to 1 with 50 steps; (c) similar to (b) but with a range of [-1,1] and interval 0.04. Then we synthesize the data with \(\)=0. As observed, setting (c) yields the best performance, indicating that with a broad range of similarity across -1 to 1, covering all the available probabilities, the CemiFace model can generalize well when adapted for generating highly discriminative samples. We also include experiments of changing the interval for (c) setting from 0.02 to 0.06 at the bottom of Tab 2, the result suggests that our approach is robust to the discrete interval but sensitive to the range of training \(\). We do not consider continuous similarity as the trained model collapses to generate the same image when given different similarities \(\).

#### 4.2.2 Ablation Study for Training and Inquiry Data

**Impact of Training Data:** Since our method does not require paired images for training the diffusion model, the limitation of using unlabelled data is alleviated. Consequently, we conduct experiments to see the impact of different training data. Specifically, we employ 3 datasets for training:(a) CASIA-WebFace as used in DCFace; (b) A challenging in-the-wild dataset Flickr with 1.2M images collected by us from Flickr website; (c) VGGFace2  which is a large-scale dataset containing 3.3M clean images. Training \(\) is set to vary within the range of [-1,1] while generation \(\) is kept as 0. We do not consider data from FFHQ  due to restrictions on being applied for face recognition.

We can see from Table 4 using VGGFace2 as the training set produces the best performance when training a model on it, indicating that training on a large-scale dataset will bring more advance in generating discriminative dataset. However, to conduct a fair comparison with previous methods, we adopt CASIA-WebFace for the following studies. Additionally, although Flickr contains much more challenging conditions such as blurred, cartoon, and occluded faces, it results in similar performance compared to DCFace , which proves the effectiveness of our proposed CemiFace.

**Impact of the Inquiry Data:** The choice of appropriate inquiry image \(\) which can be referred to as an initial point, is essential because we regard the generated group from the given \(\) to be an

   Method & Training Data & Inquiry Data & AVG \\    &  & 1-shot Web & **91.64** \\  & & DDPM & 91.49 \\  & & 1-shot Flickr & 88.97 \\   &  & 1-shot Web & **90.25** \\  & & DDPM & 90.19 \\  & & 1-shot Flickr & 88.65 \\   &  & 1-shot Web & **92.20** \\  & & DDPM & 92.01 \\  & & 1-shot Flickr & 90.586 \\   DCFace & CASIA & 1-shot Web & 89.8 \\  & & DDPM & 90.18 \\   

Table 4: Impact of Training and Inquiry Data. We also include results of training on DCFace for comparison

    &  &  \\  & & 0 & 0.0 & 9 & 1 \\    & ✓ & & & 91.64 \\  & ✓ & ✓ & & 90.36 \\  & ✓ & ✓ & ✓ & 90.12 \\  & ✓ & & ✓ & 89.57 \\   

Table 3: Ablation studies for **mixing \(\)** in generation stage with **easy and semi-hard samples**

    &  &  \\  & & 0 & 0.5 & 0.9 & 1 \\    & ✓ & & & 91.64 \\  & ✓ & ✓ & & 90.36 \\  & ✓ & ✓ & ✓ & 90.12 \\  & ✓ & & ✓ & 89.57 \\   

Table 3: Ablation studies for **mixing \(\)** in generation stage with **easy and semi-hard samples**

   Dataset & m & AVG \\    & -0.1 & 91.27 \\  & 0 & **91.64** \\  & 0.1 & 90.89 \\   & -0.1 & 89.96 \\  & 0 & **90.67** \\  & 0.1 & 90.38 \\   & -0.1 & 91.36 \\  & 0 & **91.47** \\   & 0.1 & 90.96 \\   

Table 5: Accuracy of the optimal \(\) on different inquiry sets.

independent identity group. DCFace employs a pre-trained DDPM model  trained on FFHQ to generate synthetic facial images. The style bank is sampled from a real-world dataset, e.g. CASIA-WebFace . Their process involves a combination of synthetic facial data and a real dataset. In contrast to DCFace, our method has fewer constraints when referring to the source data. The source data can be either synthetic or real, and we ablate the impact of using synthetic data and real data.

For taking synthetic data as the inquiry samples, we use the samples from DCFace to conduct a fair comparison, noted as DDPM. As for adopting real-data, we consider two options: (a) 1-shot data randomly sampled from WebFace-4m  which provides a clean dataset. (b) 1-shot Flickr, a challenging dataset filtered from the one collected in Sec. 4.2.2, with fewer licence restriction. If inquiry images with high similarity, they result in overlapped groups of synthetic images in hypersphere space. Therefore, we follow DCFace to filter out samples with a similarity higher than 0.3. We ablate the choice of the inquiry data source in Table 4, observing from changing the inquiry data, using 1-shot data of WebFace4M performs slightly better for our CemiFace. However, applying 1-shot WebFace4M to DCFace leads to a performance drop, as there are constraints for DCFace training and generation, e.g. frontal face and no glasses. Then using the challenging 1-shot Flickr as inquiry data brings worse results. This indicates that clean and real inquiry images are beneficial to generate discriminative datasets. Additionally, appropriate \(\) for each inquiry dataset with 0.5M volume is also around 0 which can be observed in Tab 5.

### Comparison with the State-of-Art methods

#### 4.3.1 Quantitative Results:

We compare our CemiFace with the previous methods to demonstrate its effectiveness. The models compared are SynFace , DigiFace , IDiff-Face  and DCFace  in both 0.5M, 1M and 1.2M image volumes. The loss for training the synthetic dataset is CosFace. For the CemiFace training set, we choose CASIA-WebFace to have a fair comparison with DCFace, training \(\) ranges from -1 to 1 with 50 discrete steps, and generation \(\) is 0. The results are available in Table 6. In 0.5 M protocol, our method exceeds the previous state-of-art method DCFace in terms of all the evaluation datasets where we achieve significant improvement on pose-sensitive dataset CFP-FP and CPLFW by 3.36 and 4.39 respectively. And in the average protocol, we get 92.30 while DCFace is 90.18. Our method still cannot exceed the model trained on the real dataset CASIA-WebFace, but we reduce the GAP-to-Real error from 4.08 to 1.96 (\(=51.96\%\) relative error) compared to DCFace. When it refers to the 1.0M and 1.2M settings, a similar phenomenon can be observed, our method surpasses DCFace on all protocols which reduces the Gap-to-Real by half, i.e. 1.59 and 1.36. In general, CemiFace behaves well on all verification accuracy and improves pose-related performance by a large margin.

   Method & Data Volume & LFW & CFP-FP & AgeDB & CALFW & CPLFW & AVG & GtR \\   CASIA-WebFace (AdaFace) &  & 99.42 & 96.56 & 94.08 & 93.32 & 89.73 & 94.62 & - \\ CASIA-WebFace (CosFace)\(\) & & 99.3 & 94.87 & 94.35 & 93.15 & 89.65 & 94.26 & 0 \\   SynFace &  & 91.93 & 75.03 & 61.63 & 74.73 & 70.43 & 74.75 & 19.51 \\ DigiFace & & 95.4 & 87.40 & 76.97 & 78.62 & 78.87 & 83.45 & 10.81 \\ IDiff-Face & & 98.00 & 85.47 & 86.43 & 90.65 & 80.45 & 88.20 & 6.06 \\ DCFace & & 98.55 & 85.33 & 89.70 & 91.60 & 82.62 & 89.56 & 4.70 \\ DCFace\(\) & & 98.33 & 87.7 & 90.01 & 91.61 & 83.26 & 90.18 & 4.08 \\ CemiFace, ours & & **99.03** & **91.06** & **91.33** & **92.42** & **87.65** & **92.30** & **1.96** \\   DCFace &  & 98.83 & 88.40 & 90.45 & 92.38 & 84.22 & 90.86 & 3.40 \\ DCFace\(\) & & 98.88 & 89.71 & 91.25 & 92.15 & 85.2 & 91.44 & 2.82 \\ CemiFace, ours & & **99.18** & **92.75** & **91.97** & **93.01** & **88.42** & **93.07** & **1.19** \\   DigiFace &  & 96.17 & 89.81 & 81.10 & 82.55 & 82.23 & 86.37 & 7.89 \\ DCFace & & 98.58 & 88.61 & 90.97 & 92.82 & 85.07 & 91.21 & 3.05 \\ DCFace\(\) & & 99.05 & 89.8 & 91.73 & 92.7 & 86.05 & 91.87 & 2.39 \\ CemiFace, ours & & **99.22** & **92.84** & **92.13** & **93.03** & **88.86** & **93.22** & **1.04** \\   

Table 6: Comparison with the previous methods. AVG is the average accuracy of the 5 evaluation datasets. GtR is the results compared to CASIA-WebFace with CosFace. Methods with \(\) are the results reproduced by our settings

#### 4.3.2 Qualitative Results

We visualize the generated results to compare with DCFace in Figure 5. Specifically, samples with different **m** scaling between [-1,1] with interval 0.2 are presented. For each row, we opt for the same noise to illustrate the variations across different similarities. We observe that when **m** is set to 1, the identity of the generated sample is very close to the inquiry image. When **m** is 0.4, gender and age change can be observed from the last two rows. With **m** scaling far away from the inquiry image, pose changes can be noticed for the first 3 rows. Another interesting phenomenon appears when similarity is -1.0 where the generated samples change significantly. Additionally, when the noise changes, the generated images exhibit different styles, aligning with our hypothesis in Sec. 3.2.1. Finally, with **m**=0, the group looks extremely different to the inquiry image, but can deliver highly accurate face recognition performance.

## 5 Conclusion

This paper proposes a novel method to generate a discriminative dataset for training effective face recognition models with reduced privacy concerns. We investigate the factors contributing to the effective face recognition model training and re-formulate the challenge of generating discriminative samples as synthesizing center-based semi-hard samples. A similarity controlling factor condition is adopted for generating semi-hard samples. Models trained on the generated dataset with center-based semi-hard samples produce accurate face recognition performance over the previous methods. A notable advantage of CemiFace is its independence from a labelled dataset for training. However, the limitations of CemiFace include relying on the pretrained identity network's performance for conducting similarity comparisons, being sensitive to the quality of the inquiry image and privacy issues arising as the pretrained model derives from a dataset without user consent.

Acknowledgments:This work was in part funded by UK Research and Innovation (UKRI) under the UK government's Horizon Europe funding guarantee [grant number 10093336] and funded by the European Union [under EC Horizon Europe grant agreement number 101135800 (RAIDO)].

Figure 5: Sample Visualization under different similarity. From left to right are inquiry images, images with m from 1 to -1 and samples generated by DCFace. Different rows in each inquiry group represent the results produced by different noises. The first column are the inquiry images. The yellow dashed box includes samples where we obtain the best accuracy. Pink dashed boxes are samples that vary vastly.