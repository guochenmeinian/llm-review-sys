# AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation

Anil Kag  Huseyin Coskun  Jierun Chen1  Junli Cao

**Willi Menapace**  Aliaksandr Siarohin  Sergey Tulyakov  Jian Ren

Snap Inc.

Project Page: https://snap-research.github.io/snap_image

###### Abstract

Neural network architecture design requires making many crucial decisions. The common desiderata is that similar decisions, with little modifications, can be reused in a variety of tasks and applications. To satisfy that, architectures must provide promising latency and performance trade-offs, support a variety of tasks, scale efficiently with respect to the amounts of data and compute, leverage available data from other tasks, and efficiently support various hardware. To this end, we introduce AsCAN--a hybrid architecture, combining both convolutional and transformer blocks. We revisit the key design principles of hybrid architectures and propose a simple and effective _asymmetric_ architecture, where the distribution of convolutional and transformer blocks is _asymmetric_, containing more convolutional blocks in the earlier stages, followed by more transformer blocks in later stages. AsCAN supports a variety of tasks: recognition, segmentation, class-conditional image generation, and features a superior trade-off between performance and latency. We then scale the same architecture to solve a large-scale text-to-image task and show state-of-the-art performance compared to the most recent public

Figure 1: Example images generated by our efficient text-to-image generation model based on an asymmetric architecture. It generates photo-realistic images while following long prompts.

and commercial models. Notably, even without any computation optimization for transformer blocks, our models still yield faster inference speed than existing works featuring efficient attention mechanisms, highlighting the advantages and the value of our approach.

## 1 Introduction

Convolutional neural networks (CNNs) and transformers have been deployed in a wide spectrum of real-world applications, addressing various computer vision tasks, _e.g._, image recognition [1; 2] and image generation [3; 4; 5; 6; 7; 8]. CNNs encode many desirable properties like translation equivariance facilitated through the convolutional operators . However, they lack the input-adaptive weighting and the global receptive field capabilities offered by transformers [10; 11]. By recognizing the potential benefits of combining these complementary strengths, research endeavors explore _hybrid architectures_ that integrate both convolutional and attention mechanisms [12; 13; 14; 15]. Recently, such architectures witness huge success when scaled up to train large-scale text-to-image (T2I) diffusion models [16; 17; 18; 19], enabling a vast range of visual applications, such as content editing [20; 21; 22; 23; 24; 25; 26] and video generation [27; 28; 29; 30].

One prominent research area for hybrid models involves the creation of building blocks that can effectively combine convolution and attention operators [31; 32]. While these efforts seek to use the strengths of both operators, their faster attention alternatives only approximate the global attention, leading to compromised model performance as lacking a global receptive field. Thus, they necessitate incorporating additional layers to compensate for the capacity reduction due to the attention approximation. On the other hand, minimal effort is directed toward optimizing the _entire_ hybrid architecture. This raises the question: _Is the current macro design of hybrid architecture optimal?_

In this work, we propose a _simple_ yet _effective_ hybrid architecture, wherein the number of convolution and transformer blocks is _asymmetric_ in different stages. Specifically, we adopt more convolutional blocks in the early stages, where the feature maps have relatively large spatial sizes, and more transformer blocks at the later stages. This design is verified across different tasks. For example, in Fig. 3, we demonstrate superior advantages of our model for the latency-performance trade-off on ImageNet-1K  classification task. Particularly, our model achieves even _faster speed_ than many works featuring efficient attention operations. Additionally, we scale up our architecture to train the large-scale T2I diffusion model for high-fidelity generation (Fig. 1, Fig. 4, and Tab. 3). Furthermore, considering the high training cost for the large-scale T2I diffusion models, we introduce a multi-stage training pipeline to improve the training efficiency. Overall, our contributions can be summarized as follows:

* We revisit the macro design principles of hybrid convolutional-transformer architectures and propose one with asymmetrically distributed convolutional and transformer blocks.
* For the image classification task, we perform extensive latency analysis on the ImageNet-1K dataset and show our models achieve superior throughput-performance trade-offs than existing works (see Fig. 3). Notably, we show that the model runtime can be significantly accelerated _even without any acceleration optimization on attention operations_. Additionally, we show our pre-trained models on ImageNet-1K can be applied to downstream tasks such as semantic segmentation.
* For the class-conditional generation on ImageNet-1K (\(256 256\)), our asymmetric UNet achieves similar performance as state-of-the-art models with half the compute resources (see Tab. 4).
* For the text-to-image generation task, we demonstrate that our network can be scaled up for the large-scale T2I generation with a better performance-latency trade-off than existing public models (as in Tab. 3). Additionally, we improve the training efficiency through a multi-stage training pipeline, where we first train the model on a small dataset, _i.e._, ImageNet-1K, for T2I generation, then fine-tune the model on a large-scale dataset.

## 2 Related Works

**Efficient Hybrid Architectures.** Over the past decade, convolutional neural networks (CNNs) [34; 35; 36] have achieved unprecedented performance in various computer vision tasks [37; 38]. Despite numerous attempts to improve CNNs [39; 40; 41; 42; 43], they still face limitations, particularly in terms of their local and restrictive receptive fields. On the other hand, the vision transformer (ViT)treats images as sequences of tokens , facilitating the computation of global dependencies between tokens to enhance the receptive field. However, ViTs come with quadratic computation complexity concerning input resolution. To address this, several studies have aimed to develop more efficient attention operations [45; 46; 47; 48; 49]. Recently, there has been a growing interest in exploring models that go beyond pure convolutional or transformer blocks, combining them within a single architecture to harness the spatial and translational priors from convolutions and the global receptive fields from the attention mechanism [11; 31; 2; 12; 32]. We revisit the design choices of hybrid architectures and propose a new design with a fast runtime while maintaining high performance. We reveal that optimizing the macro architecture directly allows us to use the original attention for a superior latency-performance trade-off compared to existing works. Most importantly, our designed networks can be applied to different domains, _e.g._, image recognition and image generation, with both superior latency-performance trade-offs.

**Text-to-Image Diffusion Models.** The development of text-to-image models, such as GAN-based models [50; 7], autoregressive models [51; 52], and diffusion models [3; 53], has enabled the generation of high-fidelity images by using textual descriptions. Among them, diffusion models demonstrate advantages in stable training processes and the scalability of large-scale neural networks [19; 54; 55; 16; 17; 13]. Recent studies explore the enhancements of text-to-image diffusion models through various directions, such as designing new network architectures [56; 57; 58; 59; 60; 61; 62; 63], improving inference efficiency during multi-steps sampling [64; 65; 66], and accelerating the training convergence [67; 68; 69; 70; 71]. Our designed network can be scaled up for the T2I generation when modified to a UNet architecture . The model achieves better performance-latency trade-offs than open-sourced models. Furthermore, we can reduce the training cost through the proposed two-stage training pipeline.

## 3 Methods

This section describes our network and training designs in detail. First, we justify our choices of utilizing the convolutional and transformer operations for the building blocks, which we then use to design the asymmetric architecture (Sec. 3.1). Second, we scale up the network architecture for training the text-to-image diffusion models (Sec. 3.3). We point out that it is easier to ablate the design of building blocks on the image classification task due to the lower resource requirements (training/evaluation compute) in comparison to image generation.

### Architecture Design: Asymmetric Convolution-Attention Networks (AsCAN)

Our hybrid architecture consists of a mix of convolutional and transformer blocks, which enables operating on different input resolutions seamlessly and allows us to be pareto efficient _w.r.t._ performance-compute trade-off compared to other architectures. Before delving into the exact architecture configuration, we discuss our choice of the building blocks. We use \(X^{H W C}\) to represent the input feature map \(X\) that has \(H W\) spatial dimensions along with \(C\) channels. We denote \(Y^{H^{{}^{}} W^{{}^{}} C^{{}^{}}}\) as the output of a building block (convolution or transformer) and \(\) symbol for the function composition operator. In the following, we present our design choices based on ImageNet-1K  classification task by varying different convolution and transformer blocks.

**Convolutional Block (C).** There are various choices for a convolutional block that can be used in our architecture, _e.g._, MBConv , FusedMBConv , and ConvNeXt . While MBConv block has been used in many networks [75; 31], the presence of depthwise convolutions results in low accelerator utilization for high-end GPUs . To better understand the latency-performance trade-off for various convolutional blocks, we experiment with the _same_ hybrid architecture yet with _different_ convolutional blocks. As in Tab. 1 (more experimental settings in Sec. 4.1), FusedMBConv has better latency on A100 and V100 GPUs than others, while maintaining high performance. Therefore, we adopt FusedMBConv (C) for the convolutional block and represent it as \(Y=X+(X)\), where \(\) is a full \(3 3\) convolution with \(4\) channel expansion followed by batch norm and GeLU non-linearity , \(\) is a squeeze-and-excite operator  with shrink ratio of \(0.25\), and \(\) is a \(1 1\) convolution to project to \(C\) channels.

**Transformer Block (T).** Similar to the convolution blocks, there are many choices for attention mechanism in transformers, _e.g._, blocks containing efficient attention mechanisms like multi-axial attention  and hierarchical attention . Tab. 2 shows that vanilla attention mechanism providesa better accuracy _vs_ throughput trade-off across different GPUs and batch sizes. Thus, we choose the vanilla attention in our transformer block (T). We express it with the following update equations:

\[Y=X+Y_{}+Y_{};\ \ Y_{}= _{}();\ \ Y_{}=_{}(),\] (1)

where \(=(X)\), \(\) denotes layer normalization, \(\) denotes the GeLU non-linearity , \(\) is the multi-headed self-attention function, \(_{}\) & \(_{}\) denote the linear projection to the QKV and MLP space, respectively, and \(\) denotes the projection operator to the same space as the input. Note that these update equations are inspired by recent works , where the feed-forward and the self-attention operators are arranged in parallel in order to get improved throughput with marginal reduction in performance.

**Design Choices.** Given the FusedMBConv (C) and Vanilla Transformer (T) blocks, we introduce the macro design for our hybrid architecture. For the image classification task, we follow the existing works  to utilize a four-stage architecture (excluding the convolutional Stem at the beginning and classifier components at the end). However, before finalizing our architecture, we still have a design question to answer, namely, _in which configuration should we arrange these building blocks?_ For instance, CoAtNet  chooses to stack convolutional blocks in the first two stages and transformer blocks in the remaining stages while MaxViT  stacks convolutional and transformer blocks alternatively throughout the entire network. A formal algorithm requires evaluating all possible C and T configurations, which is computationally expensive. Even neural architecture search leads to exponential search space. Thus, we follow a naive strategy that is based on the following principles:

* **C before** T. In any stage, we prefer convolutions followed by transformer blocks to capture the global dependence between the features aggregated by the convolutions, as they can capture scale and translation-aware information. Our ablations in Appendix Tab. 9 justify this design choice.
* **Fixed first stage.** As transformer blocks have quadratic computation complexity in terms of the sequence length, we prefer the first stage to contain only convolutional blocks to improve the inference latency.
* **Equal blocks in remaining stages.** For ease of analysis, we fix the number of blocks in the remaining stages, _i.e._, stages 2 to 4, to be four. Once we finalize the basic configuration, we can scale these stages similar to earlier works  to achieve larger models.
* **Asymmetric _vs_ Symmetric.** We refer to the architectures as symmetric whenever C and T blocks are distributed equally within a stage. For example, a configuration of CCCC-CCTTTT is

Figure 2: **Example AsCAN architectures for Image Classification & Text-to-Image Generation.****(a):** The architecture for the image classification and details of the convolutional (C) and transformer blocks (T). AsCAN includes Stem (consisting of convolutional layers) and four stages followed by pooling and classifier. **(b):** The UNet architecture for the image generation. The Down blocks (the first three blocks starting from _left_) have the reverted reflection as the Up blocks (the first three blocks starting from _right_). **(c):** The details for C and T used in UNet. For the T that performs the cross attention between latent image features and textural embedding, the \(Q\) matrix comes from the textural embedding. Note that, compared to image classification, the C and T blocks for image generation only adds extra components to incorporate the input time-step and textual embeddings.

symmetric since both C and T blocks are equal within a stage. In contrast, the configuration of CCCT-CCTT-CTTT is asymmetric since C and T blocks are not equal in stages 2 and 4.

**Final Architecture.** Given these design principles, we list various promising configurations for the building blocks (C & T), and analyze their inference throughput and top-1 accuracy. Tab. 2 provides these configurations along with performance and runtime on different GPUs. For a better comparison with existing works, we also add the configurations of CoAtNet and MaxViT for reference. From the results, we can draw the following conclusions:

* Compared to symmetric architecture design, asymmetric distribution of C & T blocks yield a better trade-off for throughput and accuracy, as compared by configurations C1-C5 _vs_ C6-C10.
* Higher number of transformer blocks in the early stages result in lower throughput, which can be observed by comparing the latency for the configurations of C2 _vs_ C10, and C8 _vs_ C9.
* While increasing the number of transformer blocks in the network improves the throughput, it does not result in improved accuracy, as demonstrated by C6 _vs_ C9.

Given above analysis, we prefer the C1 configuration for simplicity along with better accuracy _vs_ latency trade-off. Since transformer blocks capture global dependencies, we prefer at least some blocks in the early layers as well in conjunction with the convolutional blocks. Similarly, we prefer having few convolutional blocks in the later stages to capture the spatial, translation, or scale aware features. To be concrete, our final architecture includes a convolutional stem followed by four stages and the classifier head. In the first stage, we only keep convolutional blocks. In the second stage, we keep \(75\%\) convolutional and \(25\%\) transformer blocks. This trend is reversed in the final stage. For the third stage, we keep equal number of convolutional and transformer blocks. We visualize this configuration in Fig. 2 along with the diagrams representing the convolutional and transformer blocks.

**Remarks.** For simplicity, in this work, we focus on the configuration in which to combine the C & T blocks, and leverage vanilla quadratic attention and convolutional mechanisms. We can incorporate faster alternatives to quadratic attention to further boost the performance and latency trade-offs. We leave this exploration to future research.

### Discussion

While many works in the literature focus on improving the trade-off between performance and multiply-add operation counts (MACs), most of the time, MACs do not translate to throughput gains. It is primarily due to the following reasons:

* _Excessive use of operations that do not contribute to MACs._ Such tensor operators include reshape, permute, concatenate, stack, etc. While these operations do not increase MACs, they burden the accelerator with tensor rearrangement. The cost of such rearrangement grows with the size of the feature maps. Thus, whenever these operations occur frequently, the throughput gains drop significantly. For instance, MaxViT  uses axial attention that includes many permute operations for window/grid partitioning of the spatial features. Similarly, SMT  includes many concatenation and reshape operations in the SMT-Block. It reduces the throughput significantly even though their MACs are lower than AsCAN (see Appendix Tab. 7).
* _MACs do not account for non-linear accelerator behavior in batched inference._ Another issue is that MACs do not account for the non-linear behavior of the GPU accelerators in the presence of larger batch sizes. For instance, with small batch sizes (B=1), the GPU accelerator is not fully utilized. Thus, the benchmark at this batch size is not enough. Instead, one should benchmark at larger batch sizes to see consistency between architectures.
* _Lack of efficient CUDA operators for specialized building blocks._ Many architectures propose specialized and complex attention or convolution building blocks. While these blocks offer better MACs-vs-performance trade-offs, it is likely that their implementation relies on naive CUDA constructs and does not result in significant throughput gains. For instance, Bi-Former  computes attention between top-k close regions using a top-k sorting operation and performs many gather operations on the queries and keys. Similarly, RMT  computes the Manhattan distance between the tokens in the image. It includes two separate attention along the height and width of the image. This process invokes many small kernels along with reshape and permute operations. These specialized blocks would benefit from efficient CUDA kernels.
* _Using accelerator-friendly operators._ Depending on the hardware, some operators are better than others. Depth-wise separable convolutions reduce the MACs, yet they may not be efficient for particular hardware. Excessive use of depth-wise separable convolutions should be avoided in favor of the full convolutions wherever possible. For instance, MogaNet  extensively uses depth-wise convolutions with large kernel sizes and concatenation operations. These operators reduce the multiply-addition counts, which are not necessarily efficient on high-end GPU accelerators.

### Scaling Up Architecture for Image Generation

We further scale up our architecture for the image generation task, which requires more computation than the image recognition task. We train our network by utilizing the denoising diffusion probabilistic models (DDPM) [16; 17; 54] in the latent space . The latent diffusion model includes a variational autoencoder (VAE) [83; 84] that encodes the image \(\) into latent \(\) and a diffusion model \(}_{}()\) with parameters \(\). We utilize UNet  as the network for the diffusion model. For the T2I generation, we get the text embedding with Flan-T5-XXL encoder . We train the diffusion model following the noise prediction [16; 17]:

\[_{}_{t[1,T],(, })}\|-}_{}( _{t},})\|^{2},\] (2)

where \(t\) is the time step and \(T\) is the total number of steps, \(\) is the added Gaussian noise, and \(}\) is the condition signal. \(_{t}\) is a noisy latent obtained with a pre-defined variance schedule \(\{_{t}(0,1)\}_{t}\) with the following updates :

\[_{t}=_{t}}+_{t}} {};\ \ _{t}=1-_{t};\ \ _{t}=_{i=1}^{t}_{i}.\] (3)

In the following, we discuss our design for the diffusion model and the multi-stage training pipeline.

#### 3.3.1 Asymmetric UNet Architecture

We follow the existing literature [13; 8; 15; 86] to design the UNet architecture for the T2I diffusion model. Fig. 2 gives an overview of the overall architecture. It consists of three main stages, namely, Down blocks, Middle Block, and Up blocks. The Up blocks are the reverted reflection of the Down blocks. In addition, there are skip connections to add the features from Down blocks to Up blocks. We adopt the VAE from SDXL  to transform the image to the latent space and carry out the diffusion in this space. Since the text-to-image generation task requires additional inputs (_i.e._, time and text embeddings), we modify the C and T blocks to incorporate these conditions. Similar to the existing literature , we add the time embedding to the C blocks. Additionally, we add the text embedding to the T blocks through a cross-attention operation carried in parallel to the self-attention operation as shown in Fig. 2(c).

   } &  &  &  \\  & & & **A100** & **V100** & **Acc.** \\    & & **B-64** & **B-16** & **A-16** \\  MBConv & 29M & 3013 & 914 & 83.12\% \\ ConvNext & 35M & 3923 & 1104 & 82.81\% \\  FusedMDConv & 55M & 4295 & 1148 & 83.44\% \\    } &  &  &  \\  & & **A100** & **V100** & **Acc.** \\    & & **B-64** & **B-16** & **A-16** \\  Multi-Axial & 83M & 3541 & 630 & 83.59\% \\ Hierarchical & 74M & 3470 & 552 & 83.51\% \\  Vanilla & 55M & 4295 & 1148 & 83.43\% \\   

Table 1: **Analysis of the Configuration of Convolution and Transformer Blocks.** We analyze various convolutional and transformer blocks by training hybrid architectures with different options on the ImageNet-1K dataset. We provide the inference latency (as throughput) for different GPUs. Results show that FusedMDB-Conv and Vanilla Transformer blocks provide a better trade-off over accuracy and latency than others.

   } &  &  \\  & & **Params** & **A100** & **V100** & **Acc.** \\    & & **B-64** & **B-16** & **A-16** \\  Our & OC-COCOT-CTT-CTT (C1) & 55M & 3224 & 4295 & 1148 & 83.4\% \\  & OC-COCOT-CTT-CTT (C2) & 73M & 3217 & 1105 & 86.32\% \\  & \(\)-COCOT-CTT-TTT (C3) & 41M & 3384 & 4472 & 1242 & 82.9\% \\  & OC-COCOT-CTT-CTT (C4) & 50M & 3344 & 4111 & 182 & 83.1\% \\  & OC-COCOT-CCT-CCT-CG (C5) & 59M & 3135 & 4066 & 991 & 87.2\% \\  & OC-COCOT-CTT-TG (C6) & 51M & 3783 & 4998 & 1280 & 82.8\% \\  & OC-COCOT-CTT-TTT (C7) & 42M & 3564 & 9419 & 1286 & 82.4\% \\  & OC-COCOT-TTT-TT-TT (C8) & 34M & 3475 & 5311 & 1469 & 26.2\% \\  & OC-TT-TTT-TTWhile the UNet can take arbitrary resolution as input, we notice that adding positional embeddings in the self-attention operation reduces the artifacts such as duplicate patterns. For this purpose, we add RoPE  embeddings for encoding the position information in the T blocks. Further, we incorporate query-key normalization using the RMSNorm  for stable training in lower precision (bfloat16).

#### 3.3.2 Improved Multi-Stage Training Pipeline

Instead of training the T2I network from scratch, we first train the model on a small-scale dataset and then fine-tune the model on a much larger dataset. It effectively reduces the training cost on the large-scale scale dataset (see ablation in Appendix Sec. A.6, Fig 8). This strategy differs from existing works that perform multi-stage training using different architectures. For instance, PixArt-\(\) trains a class conditional image generation network and modifies it to fine-tune the network for text-to-image generation. Both our pre-training and fine-tuning tasks use the same architecture for text-to-image generation. We use the AdamW optimizer  with \(_{1}=0.9\) and \(_{2}=0.99\).

Specifically, in the first stage, we train our model using ImageNet-1K  for the text-to-image generation. Following Esser _et al._, we form the conditioned text prompt as "a photo of a <class name>", where class name is randomly chosen from the label of each image. The model is trained to generate an image with a resolution of \(256 256\). In the second stage, we fine-tune the model from the first stage on a much larger dataset. Here we train the model in four phases: First, we conduct training at the resolution of \(256 256\) for \(300\)K iterations with the batch size as \(16,384\) and \(4e-4\) as the learning rate. Second, we continue the training at the resolution of \(512 512\) for \(200\)K iterations with the batch size as \(6,144\) and \(1e-4\) as the learning rate. Third, we train the model for \(1024 1024\) for \(100\)K iterations with the batch size as \(1,536\) and \(5e-5\) as the learning rate. Finally, we perform the multi-aspect ratio training such that the model can synthesize images at various resolution . Additionally, we adjust the added noise at different resolution , _i.e._, \(_{T}\) in Eq. (3) is chosen as \(0.01\) for \(256 256\) and \(0.02\) for higher resolution (see ablations in Appendix Sec. A.6). We add an offset noise of \(0.05\) during multi-aspect ratio training similar to earlier works.

## 4 Experiments

In this section, we evaluate the architectures proposed in Sec. 3.1 on image recognition and generation tasks. We also apply this design to the semantic segmentation task in Appendix Sec. A.3. We highlight the important aspects of these experiments below and provide the experimental details in the appendix.

Figure 3: **Top-1 Accuracy _vs_ Inference Latency on ImageNet-1K Classification. We plot the latency measured as images inferred per second on a single V100 GPU (_Left_)/A100 GPU (_Right_) with batch-size \(16\) with \(224 224\) resolution. The plot compares state-of-the-art models (convolutional, transformer, hybrid architectures) against the proposed AsCAN architecture. The area of each circle is proportional to the model size. Our model consistently achieves better accuracy _vs_ latency trade-offs. While some models regress between two hardware (_e.g._, MaxViT-S vs SMT-B ), our model consistently achieves better accuracy _vs_ latency trade-offs. We report additional baselines along with multiply-add operations count and different batch sizes in Appendix Tab. 7.**

### Image Recognition

We scale the AsCAN architecture discussed in Sec. 3.1 to base and large variants following earlier works . We train these variants on the ImageNet-1K classification task. We provide the experimental details (dataset description, architectures, training hyper-parameters) in Appendix Sec. A.2. Fig. 3 plots the inference speed on a V100 GPU with batch size \(16\) (measured in images processed per second) and top-1 accuracy achieved on this task for various models. In addition, Appendix Tab. 7 shows the parameter count and inference speed on both V100 and A100 GPUs along with additional details such as floating-point multiply-add count (FLOPs/MACs) and inference speed across different batch sizes. Below, we highlight the salient observations from these experiments.

* _More than \(2\) higher throughput across accelerators._ Compared to the existing hybrid architectures such as FasterViT  and MaxViT , the proposed AsCAN family achieves similar or better top-1 accuracy with more than \(2\) higher throughput. This trend holds true for both A100 and V100 GPUs. For instance, on A100 with batch=\(16\), FasterViT-1 achieves \(83.2\%\) top-1 accuracy with throughput as \(1123\) images/s, while AsCAN-T has \(83.44\%\) with throughput as \(3224\) images/s.
* _Better throughput across different batch sizes._ AsCAN consistently achieves better throughput across batch sizes for both the accelerators compared to baselines.
* _Better storage and computational footprint._ AsCAN-family of architectures requires less number of parameters and float operations to achieve similar performance. For example, to achieve nearly \(85.2\%\) accuracy, MaxViT-L requires \(212\)M parameters and \(43.9\)G MACs whereas AsCAN-L requires \(173\)M parameters and \(30.7\)G MACs.
* We achieve better latency _vs._ accuracy trade-off than hybrid architectures with better alternatives to the attention mechanisms in transformer. For instance, we outperform newer architectures such as PVTv2, MOAT, EfficientViT, and Scale-Aware Modulation Transformers.

Further, we observe similar trends when we scale these models to pre-training on _ImageNet-21K_ dataset (see Appendix Sec. A.2.3) as well as semantic _segmentation_ task (Appendix A.3).

### Class Conditional Generation

We apply our asymmetric UNet architecture to learn class conditional image generation on the ImageNet-1K dataset with \(256 256\) resolution. We train a smaller variant of our asymmetric UNet architecture (as in Sec. 3.3.1) with nearly \(400\)M parameters and inject the class condition through cross-attention mechanism. We train this model using DDPM  (more details in Appendix Sec. A.7), and provide results in Tab. 4. As can be seen, we achieve Frechet Inception Distance (FID)  close to state-of-the-art models with nearly half the FLOPs (_e.g._, Ours _vs._ DiT-XL/2-G), and better FID than the existing work with similar computation (_e.g._, Ours _vs._ U-ViT-L/2).

### Text-to-Image Generation

Below, we evaluate our T2I asymmetric UNet architecture trained using the multi-stage training.

**GenEval Benchmark.** We evaluate our model on the GenEval  benchmark that studies various aspects of an image generation model. Tab. 3 shows that our model outperforms fast training pipelines such as PixArt-\(\) by a significant margin. It beats even larger models such as SDXL which consume significantly more training data and computational resources.

   Model & Overall &  Single \\ Object \\  &  Two \\ Objects \\  & Counting & Colors & Position & 
 Color \\ Attribution \\  \\  SDv1.5 & 0.43 & 0.97 & 0.38 & 0.38 & 0.76 & 0.04 & 0.06 \\ PixArt-\(\) & 0.48 & 0.98 & 0.50 & 0.44 & 0.80 & 0.08 & 0.07 \\ SDv2.1 & 0.50 & 0.98 & 0.51 & 0.44 & 0.85 & 0.07 & 0.17 \\ PixArt-\(\) & 0.53 & 0.99 & 0.65 & 0.46 & 0.82 & 0.12 & 0.12 \\ SDXL & 0.55 & 0.98 & 0.74 & 0.39 & 0.85 & 0.15 & 0.23 \\  Ours & 0.64 & 0.99 & 0.78 & 0.43 & 0.88 & 0.28 & 0.48 \\   

Table 3: **GenEval Scores.** We use this benchmark to compare T2I models on various aspects of generation, including counting, color attribution, position, etc. It clearly shows that our model achieves better overall score, by convincingly outperforming pipelines such as PixArt-\(\) and SDXL.

**Qualitative Comparison.** Fig. 4 shows a qualitative comparison between different methods. We compare highlight salient differences between the baselines and our generations. Baselines such as PixArt-\(\) and PixArt-\(\) tend to generate images with much less photo-realism and more often it is much more on the cartoonish side or it has grainy artifacts. Similarly, SDXL with refiner framework is unable to adapt to long text prompts due to limitations of the CLIP text-encoder. Hence, it misses many key features described in the prompts. In contrast, our model is able to follow the long prompts while adhering to the required semantics as well as photo-realism.

**Human Preference study.** We perform a user study to compare open-sourced models to evaluate their image-text alignment characteristics. We select \(1000\) prompts from our validation set and generate images from SDXL, PixArt-\(\), PixArt-\(\), and our multi-aspect ratio model. We show these results in the Fig. 5. It shows that our model convincingly outperforms both SDXL and PixArt-\(\) generations, while it has similar image-text alignment as PixArt-\(\) model. To further evaluate these models, we generate \(10\)K images from our validation set (described in Appendix A.5.2) and compute the FID  between the generated and original images. We show these scores in Tab. 5, which

Figure 4: **Qualitative Comparison against open source and commercial models. We compare our T21 model against generations from different baselines. We illustrate that many times existing models generate images with less photo-realism (either lot less details or more on the cartoonish side), specially for PixArt-\(\) and PixArt-\(\). Further, they frequently miss the fine-grained details explicitly asked in the prompts. We highlight these mistakes in red color in the input prompt. For instance, in the above generations (ordered A \(\) F from top to bottom row), baselines miss details such as, (A) lack of realism (B) light blue jeans, (C) white sunglasses, (D) black, orange, and white feathers, (E) grey scarf & back towards camera, and (F) gray knitted hat with dark blue-brown patterns.**clearly indicates that our generations align well with the real-world images. This is also evident from our earlier comparison on qualitative visualization in Fig. 4.

**Resource Efficiency.** We compare the resource requirements of our model against various baselines in Appendix Tab. 12. It shows that we achieve better inference latency compared to many existing models. Further, we consume considerably less compute to achieve much better performance than many existing baselines as illustrated by evaluations in the previous section.

## 5 Conclusion

In this work, we design hybrid architectures comprising convolutional and transformer blocks with applications to many computer vision tasks. We focus on developing a simple hybrid model with better throughput and performance trade-offs. Instead of designing efficient alternatives to the convolutional and transformer (mainly attention mechanism) blocks, we leverage existing vanilla attention along with the FusedMBConv block to design the new architecture, called AsCAN. Our main philosophy revolves around the uneven distribution of the convolutional and transformer blocks in the different stages of the network. We refer to this distribution as _asymmetric_, in the sense that it favors more convolutional blocks in the early stages with a mix of few transformer blocks, while it reverses this trend favoring more transformer blocks in the later stages with fewer convolutional blocks. We demonstrate the superiority of the proposed architecture through extensive evaluations across the image recognition task, class conditional generation, and text-to-image generation.

    &  &  \\  Model & FLOPs &  Throughput \\ A100 (B=64) \\ samples/sec \\  & FID & FLOPs & 
 Throughput \\ A100 (B=64) \\ samples/sec \\  & FID \\  ADM  & 110G & - & 10.60 & - & - & - \\ LDM  & 104G & 362 & 3.60 & - & - & - \\ U-ViT-LZ  & 77G & 498 & 3.40 & 340G & 86 & 4.67 \\ U-ViT-H/2  & 133G & 271 & 2.29 & 546G & 45 & 4.05 \\ DiT-XL/2-G  & 118G & 293 & 2.27 & 525G & 51 & 3.04 \\  Ours & 52G & 556 & 2.41 & 224G & 130 & 3.28 \\ Ours (sampled cfg ) & 52G & 556 & 2.23 & 224G & 130 & 3.15 \\   

Table 4: **ImageNet-1K Class Conditional Generation. We train a smaller variant of our T2I UNet architecture to perform class conditional generation on ImageNet-1K. We train this model at \(256 256\) and \(512 512\). Our asymmetric architecture achieves similar FID as the state-of-the-art models with less than half the floating point operations (_e.g._, Ours _vs._ DiT-XL/2-G), and better FID than the existing work with similar computation (_e.g._, Ours _vs._ U-ViT-L/2).**

Figure 5: **Image-Text Alignment Study. We perform user study for \(1000\) prompts and ask them to choose images with better image-text alignment. It shows that we outperform SDXL and PixArt-\(\). While our performance is on par with PixArt-\(\), Tab. 5 shows that we yield more realistic generations.**

   Model &  FID \\ Set-B-10K \\  & 
 FID \\ Set-A-10K \\  \\  PixArt-\(\) & 46.03 & 20.12 \\ PixArt-\(\) & 40.01 & 19.25 \\ SDXL & 35.86 & 16.49 \\  Ours & 15.45 & 10.88 \\   

Table 5: **Validation Data Evaluation. We use the validation set of captioned data for computing the FID scores for comparison between different models (see details in Appendix A.5.2)**