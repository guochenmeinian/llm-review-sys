# NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA

Marlon Tobaben\({}^{1}\) Mohamed Ali Souibgui\({}^{2}\) Ruben Tito\({}^{2}\) Khanh Nguyen\({}^{2}\)

Raouf Kerkouche\({}^{3}\) Kangsoo Jung\({}^{4}\) Joonas Jalko\({}^{1}\) Lei Kang\({}^{2}\) Andrey Barsky\({}^{2}\)

**Vincent Poulain d'Andecy\({}^{5}\) Aurelie JOSEPH\({}^{5}\) Aashiq Muhamed\({}^{5}\) Kevin Kuo\({}^{6}\) Virginia Smith\({}^{6}\) Yusuke Yamasaki\({}^{7}\) Takumi Fukami\({}^{7}\) Kenta Niwa\({}^{7}\) Ifian Tyou\({}^{7}\) Hiro Ishii\({}^{5}\) Rio Yokota\({}^{8}\) Ragul N\({}^{9}\) Rintu Kutum\({}^{9}\) Josep Llados\({}^{2}\) Ernest Valveny\({}^{2}\) Antti Honkela\({}^{1}\) Mario Fritz\({}^{3}\) Dimosthenis Karatzas\({}^{2}\)**

\({}^{1}\)University of Helsinki \({}^{2}\)Computer Vision Center, Universitat Autonoma de Barcelona

\({}^{3}\)CISPA Helmholtz Center for Information Security \({}^{4}\)INRIA \({}^{5}\)Yooz

\({}^{6}\)Carnegie Mellon University \({}^{7}\)NTT \({}^{8}\)Tokyo Institute of Technology \({}^{9}\)Asoka University

This analysis is jointly written by organizers and participants. See author contributions in Appendix A.1.

###### Abstract

The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) competition challenged the community to develop provably private and communication-efficient solutions in a federated setting for a real-life use case: invoice processing. The competition introduced a dataset of real invoice documents, along with associated questions and answers requiring information extraction and reasoning over the document images. Thereby, it brings together researchers and expertise from the document analysis, privacy, and federated learning communities. Participants fine-tuned a pre-trained, state-of-the-art Document Visual Question Answering model provided by the organizers for this new domain, mimicking a typical federated invoice processing setup. The base model is a multi-modal generative language model, and sensitive information could be exposed through either the visual or textual input modality. Participants proposed elegant solutions to reduce communication costs while maintaining a minimum utility threshold in track 1 and to protect all information from each document provider using differential privacy in track 2. The competition served as a new testbed for developing and testing private federated learning methods, simultaneously raising awareness about privacy within the document image analysis and recognition community. Ultimately, the competition analysis provides best practices and recommendations for successfully running privacy-focused federated learning challenges in the future.

## 1 Introduction

Automatic document image processing has become a highly active research field in recent years (Appalaraju et al., 2024; Lee et al., 2023; Tito et al., 2023), with invoices being one of the most frequently processed document types (Simsa et al., 2023). In a typical real-life invoicing scenario, business suppliers produce invoices for their services and send them to their customers. These documents contain sensitive information, such as consumer/purchaser identity, transaction details, purpose, date, phone numbers, amount paid, account information for payment, etc. The customers (document users) needto extract this information and take the corresponding actions (i.e. reject, or make a payment against the invoice). In automated pipelines, these documents would be sent to AI technology providers, typically offered in the form of cloud services2, which automatically extract all required information from the documents, and return it to the document users.

A generic approach to extract information from invoices is DocVQA (Mathew et al., 2020). The extraction is done by asking questions in a natural language form to get specific information as answers, using a deep learning model. However, training an accurate DocVQA model requires a considerable amount of data, that is rarely held by a single entity. One solution is to train this model collaboratively by aggregating and centralizing data from a set of clients that face the same problem. But, documents often cannot be freely exchanged due to the sensitive information they contain. Federated Learning (FL) is a learning paradigm that purports to solve this problem (McMahan et al., 2017b). Rather than exchanging privately-held data, participating entities (known as clients) train models on their data in a decentralized fashion, exchanging only the local model updates with a central server. However, even though FL is more private than the centralized approach, a significant amount of information can still be inferred from the updates shared during training, or from the parameters of the resulting trained model, whether by an adversarial server, client, or downstream user (Sikandar et al., 2023).

Differential Privacy (DP) (Dwork et al., 2016) is considered the gold standard in terms of privacy preservation and can be used to provide provable privacy guarantees. DP formally quantifies the maximum information leakage from the inclusion of any one individual record in a dataset. Deep learning models can be trained under DP by clipping parameter updates and adding noise to them (Rajkumar and Agarwal, 2012; Song et al., 2013; Abadi et al., 2016). However, this introduces a trade-off between privacy and utility. Stronger privacy guarantees require introducing more noise, which proportionately degrades model accuracy.

Another drawback of FL is the high communication cost (Kairouz et al., 2021). At each federated round, the global model is transmitted by the server to selected clients (downstream step) to be trained on their local data, and then the update of this model is sent by these selected entities back to the server (upstream step). For models with millions or even billions of parameters, this requires significant bandwidth, multiplied by the number of federated rounds required to reach model convergence.

In this paper, we present an analysis of the NeurIPS 2023 competition on privacy preserving FL DocVQA that we designed to expose the above challenges and invite the community to design novel creative solutions for this real-life use case. It brought together researchers and expertise from the document analysis, privacy, and FL communities. Additionally, it added a realistic use case for privacy and FL researchers as well as expanding the scope of document analysis to DP solutions.

## 2 Related Work

**Document Visual Question Answering (DocVQA)** DocVQA has been an evolving field during the last few years. This is due to the emerging datasets that address different document domains. For instance, industry documents (Mathew et al., 2020, 2021; Tito et al., 2021, 2023a), infographics (Mathew et al., 2022), multidomain (Landeghem et al., 2023a,b), open-ended questions (Tanaka et al., 2021), multilingual (Qi et al., 2022), multipage (Tito et al., 2023a) or collections of documents (Tito et al., 2021). However, these datasets are often small and highly domain-specific, which limits generalizability.

**Federated Learning (FL)** FL (Shokri and Shmatikov, 2015; McMahan et al., 2017) addresses this issue, and has seen practical use in both research and industrial applications (Li et al., 2020), particularly in domains where sensitive data is common, such as medicine (Dayan et al., 2021) and finance (Long et al., 2020). FL carries a trade-off between model utility, data privacy, and communication efficiency (Zhang et al., 2023), and requires specific consideration of client data heterogeneity,scalability, and fault tolerance. Much recent work in FL focuses on mitigating these problems, primarily through developments in aggregation algorithms (Moshawrab et al., 2023; Elkordy and Avestimehr, 2022; So et al., 2022; Nguyen et al., 2022), but also in parameter compression (Tang et al., 2019) and quantization (Xu et al., 2022).

**Privacy Attacks** While FL offers privacy advantages, it remains vulnerable to various attacks that jeopardize client dataset privacy. In the federated architecture, both the server and clients can potentially act as adversaries. Gradient updates in FL have the potential to disclose information about the training data, making them susceptible to "gradient inversion attacks" (Zhu et al., 2019; Zhao et al., 2020; Fu et al., 2022; Wainakh et al., 2022; Li et al., 2022; Geiping et al., 2020; Melis et al., 2019; Li et al., 2022d), which enable accurate data reconstruction. Moreover, adversaries can execute "membership inference attacks" (Nasr et al., 2019; Melis et al., 2019; Suri et al., 2022; Shokri et al., 2017; Choquette-Choo et al., 2021; Li and Zhang, 2021; Hu et al., 2022b) to infer the inclusion of specific data points in other participants' datasets, as well as "property inference attacks" (Melis et al., 2019) to deduce subgroup statistics despite secure aggregation (Kerkouche et al., 2023; Pejo and Biczok, 2023). FL inherently lacks protection against these threats, necessitating explicit mitigation strategies to safeguard client data from adversaries.

**Differential Privacy (DP)**\((,)\)-DP (Dwork et al., 2006) has a privacy budget consisting of \( 0\) and \(\), where smaller values correspond to a stronger privacy guarantee. Especially relevant to our setting is group-level DP, which preserves privacy leakage from the inclusion or exclusion of groups of datapoints (Galli et al., 2023; Marathe and Kanani, 2022), such as multiple records associated with a specific user. We refer to Dwork and Roth (2014) for a comprehensive intro to DP.

**High utility models under DP** Currently, many works improve the utility-privacy trade-off through transfer learning (Yosinski et al., 2014) assuming the availability of non-sensitive public data for pre-training and only utilizing DP to protect sensitive downstream data during fine-tuning. We would like to refer to Tramer et al. (2022a) for a discussion on the drawbacks of these assumptions. Transfer learning is highly effective for both language (Li et al., 2022c; Yu et al., 2022a) and vision tasks (Cattan et al., 2022; De et al., 2022; Kurakin et al., 2022; Tobaben et al., 2023). In particular, parameter-efficient fine-tuning (Houlsby et al., 2019) with adaptation methods such as LoRA (Hu et al., 2022a) have been demonstrated to yield improved utility-privacy trade-offs for DP, as have quantization (Youn et al., 2023) or compression of model updates (Kerkouche et al., 2021a,b; Miao et al., 2022). All these methods reduce the size of the updates, and thereby reduce the amount of noise addition required. The same strategies often yield competitive performance for FL.

## 3 General Competition Information

This section describes general information about the competition that is common to both tracks. These are the dataset, metrics, model, starter kit and the participation statistics.

### PFL-DocVQA Dataset

For this competition, we created PFL-DocVQA (Tito et al., 2023b), the first dataset for private federated DocVQA. The dataset is created using invoice document images gathered from the DocILE dataset (Simsa et al., 2023). For every image, we provide the OCR transcription and form a set of question/answer pairs. The competition's version of PFL-DocVQA contains a total of 336,842 question-answer pairs framed on 117,661 pages of 37,669 documents from 6,574 different invoice providers. PFL-DocVQA is designed to be used in two tasks, and so is divided into two subsets. For the first task of training and evaluating machine learning privacy-preserving solutions on DocVQA in a FL fashion, a base subset of PFL-DocVQA called the "BLUE" data is used. In the second task, membership inference attacks are designed to assess the privacy guarantees of the DocVQA models that were trained with the base data. These attacking approaches are to utilize a second subset called the "RED" data. In this competition, we focus on the first task, thus, we use only the "BLUE" data. For more details on the full PFL-DocVQA datasets, refer to Tito et al. (2023b).

PFL-DocVQA aims to train and evaluate DocVQA systems that protect sensitive document information. In our scenario, sensitive information encompasses all information originating from each invoice provider. Therefore, an effective model must prevent the disclosure of any details associated with these providers (such as provider names, emails, addresses, logos, etc.) across diverse federated clients. Following this, the base data used in this competition consists of a training set divided among \(N\) clients (we use \(N=10\)), a validation set and a test set. (See Figure A.1). The training set of each of the \(N\) clients contains invoices sampled from a different subset of providers, resulting in a highly non-i.i.d. distribution. In the validation and test sets, we include documents both from the providers that were seen during training, and from a set of providers that were not seen, to better evaluate the generalizability of the models.

### Evaluation Metrics

In the PFL-DocVQA Competition three main aspects are evaluated: The model's utility, the communication cost during training and the DP privacy budget spent through training the model.

**Utility** To evaluate the visual question answering performance of the participants' methods we use accuracy and ANLS (Average Normalized Levenshtein Similarity), a standard soft version of accuracy extensively used in most of the text-based VQA tasks . This metric is based on the normalized Levenshtein Distance  between the predicted answer and the ground truth, allowing us to assess the method's reasoning capabilities while smoothly penalizing OCR errors.

**Communication cost** We measure the efficiency of the communications as the total amount of information transmitted between the server and the clients in Gigabytes (GB) in both directions. The initial transmission of the pre-trained model to the clients is not included in the communication cost.

**Privacy** The methods of track 2 are required to comply with a DP privacy budget of no more than a pre-defined \(\{1,4,8\}\) at \(=10^{-5}\). We provided a script within the starter kit detailed in Section 3.4 to compute the required noise multiplier given the target (\(\), \(\)). Participants may need to adjust the script to their algorithms. Moreover, we required the participants to upload a theoretical privacy proof of their methods, which was manually reviewed by the competition organizers.

### Pre-trained Model

The participants were asked to implement their solutions starting from the same pre-trained model. The architecture chosen is Visual T5 (VT5), it is a multimodal generative network consisting of a simplified version of Hi-VT5 , which was originally proposed for multi-page DocVQA. VT5 exploits the image and text modalities, which is beneficial to perform the DocVQA task. However, this dual-modality approach also presents a more complex challenge: safeguarding private information across both modalities, compared to handling just one. Moreover, VT5 is a generative model based on the T5  language model. Language models can suffer hallucinations , leading to the potential leakage of private information.

The architecture VT5 consists of an encoder-decoder model based on T5. The input of the model is the question, the OCR tokens of the document (text and spatial information), and the encoded document image using the DiT  vision transformer model. These three inputs are concatenated and fed to the VT5 to output the answer following the autoregressive mechanism.

We also provide pre-trained weights for VT5. First, the language backbone T5 is initialized with the pre-trained weights on the C4 dataset , and the visual DiT with the pre-trained weights on the document classification task. After that, the full model is fine-tuned on the single-page DocVQA task, using the SP-DocVQA dataset  for 10 epochs.

### Starter Kit

The starter kit includes the pre-trained model checkpoint, the fine-tuning dataset, code for running the baselines and instructions on how to run and modify the code. The code itself is based on established libraries such as PyTorch (Paszke et al., 2019) and the FL framework Flower (Beutel et al., 2020). Besides the training code, the starter kit includes functions for computing the privacy parameters based on the hyperparameters and for logging the communication between server and clients. We tested the installation and execution of the baseline on various clusters across different institutions and provided support to participants if they encountered any difficulties. The starter kit is openly available: https://github.com/rubenpt91/PFL-DocVQA-Competition.

### Participation Statistics

Refer to Table 1 for the participation statistics. Our competition has gained interest across the communities and remains an open benchmark in the future: https://benchmarks.elsa-ai.eu/?ch=2&com=introduction. In Section 6.2 we discuss measures to lower the participation threshold.

## 4 Track 1: Communication Efficient Federated Learning

Track 1 focuses on training high utility models while reducing the communication cost in federated learning. We describe the task, the organizers' baseline and two submitted approaches (See Table 2).

### Task Formulation

The objective of track 1 is to reduce the communication used (# bytes), while achieving a comparable utility (ANLS) with the organizers' baseline. The baseline achieved a validation ANLS of 0.8676 and we define a comparable utility to the baseline as 0.8242 ANLS (5% w.r.t. the baseline). Any submission that achieves at least that ANLS is valid, thus the deciding factor for winning the competition is the communication efficiency, which is measured using a single metric. We opted for scoring using a single metric as the trade-off between utility and communication is not linear. Furthermore, in real world applications less communication efficiency will lead to higher monetary costs or longer training times that need to be considered in contrast to changes in model utility.

Participants are required to use the VT5 baseline model with the initial pre-trained weights and utilize only the PFL-DocVQA dataset for fine-tuning. Further the participants are not allowed to change the PFL-DocVQA data distribution. Additionally, participants are required to upload a log of the communication between the clients and the central party (# bytes) and the final model checkpoint.

The organizers evaluate the model utility on a secret test set and thus the model architecture needs to be the same as the initial baseline. While this makes some solutions such model distillation more challenging, the track is open to a wide range of possible solutions. Participants could, e.g., utilize parameter-efficient fine-tuning, compression of the FL updates, lower precision or better hyper-parameters to achieve higher communication efficiency while maintaining a comparable utility.

### Baseline Solution Track 1

The baseline solution for track 1 fine-tunes all parameters of the pre-trained model but the visual module. It essentially uses Federated Averaging (FedAvg) (McMahan et al., 2017). In each global round, the central server samples \(K=2\) clients out of all \(N=10\), and each of these clients computes

  
**Registrations to platform** & **Downloads** & **Countries** & **Submissions Track 1** & **Submissions Track 2** \\ 
382 & 494 & 21 & 13 & 6 \\   

Table 1: Participation Statistics as of May 31, 2024.

the weight updates locally across multiple local rounds. The central server aggregates the client updates and communicates the updated model to the sampled clients in the next round.

### Winner Track 1: Muhamed, Kuo, and Smith

We considered three orthogonal methods to reduce communication (LoRA, tuning FL hyperparameters, and quantization). The winning solution for Track 1 uses only LoRA (\(100\) reduction). Combining all methods can achieve a \(5200\) reduction. For complete details, see Appendix C.

**1. LoRA. Low-Rank Adaptation trains low-rank adapters while freezing the rest of the model (Hu et al., 2022). We use LoRA to reduce the number of trainable parameters to 3.4M (1.37% from 250M). Using 2 clients per round, we reach the target ANLS in 7 rounds (**0.38 GB** total communication).

**2. Tuning FL hyperparameters.** On top of **1. LoRA**, we sample 1 client per round (default: 2) and train for 16 local epochs (default: 1), which respectively reduces communication and improves utility. With these adjustments, we reach the target ANLS in 2 rounds (**55 MB** total communication).

**3. Quantization** is a lossy compression approach which we use to reduce the size of the communicated LoRA updates. We use NF4 (4-bit) quantization which reduces the message size by \( 8\) while achieving the target ANLS with the same configuration as **2. (7.7 MB** total communication).

### Runners-up Track 1: Niwa, Ishii, Yamasaki, Fukami, Tyou, and Yokota

We briefly present our methods and experimental results. For more detailed information can be found in Appendix D. We aimed to achieve faster convergence of training for local models with fewer communication rounds. To achieve this, we utilized Shampoo (Gupta et al., 2018), a second-order optimization method, in local update rules by multiplying the local preconditioning matrix to the local stochastic gradient. The update rules of our method, named _FedShampoo_, are outlined in Alg. 1 in Appendix D.1. Shampoo enables smooth local updates by geometrically rotating and scaling stochastic gradients. To reduce the memory footprint in computing large-scale preconditioning matrices, we approximated them by employing layer-wise block-diagonalization. Notably, the local preconditioning matrices (approximated by sub-matrices) were not transmitted to the central server, thus avoiding excess communication costs. Furthermore, we excluded the embedding layer from the optimization target, resulting in a reduction of approximately 26 % in communication per round compared to whole parameters3.

In Table 2, FedShampoo achieved the target ANLS score with 10.01 GB communication cost. Refer to Figure A.3 in Appendix D.1 for convergence curves using validation loss, ACC and ANLS. We submitted the model after only \(R=3\) communication rounds, surpassing the target ANLS score of \(0.8873\) and resulting in an approximately 30 % reduction of the communication cost compared with the baseline method (using solely AdamW-based optimizer). Furthermore, FedShampoo achieved higher ACC and ANLS scores compared with the baseline method after exceeding the ANLS target

 
**Rank** & **Team** & **Method** & **Communication \(\)** & **ANLS \(\)** \\ 
1 & Muhamed et al. (Section 4.3) & LoRA & 0.38 GB (-99.14\%) & 0.8566 (-3.45\%) \\
2 & Niwa et al. (Section 4.4) & FedShampoo & 10.01 GB (-77.37\%) & 0.8891 (+0.20\%) \\   & Organizers (Section 4.2) & Baseline & 44.65 GB & 0.8873 \\  

Table 2: Competition Winners Track 1 (Communication efficient federated learning)score (after 3 communication rounds). This provides as empirical evidence of FedShampoo's faster convergence, which benefits from applying the preconditioning matrix to the stochastic gradient. The detailed experimental configurations, such as hyperparameter tunings of learning rate and clipping threshold, are summarized in Appendix D.1.

## 5 Track 2: Differentially Private Federated Learning

Track 2 focuses on training as high utility models as possible while preserving all information from each document provider in the training set through DP. We describe the task, the organizer's baseline and two submitted approaches (See Table 3).

### Track 2 Task Formulation

The objective of track 2 is to achieve the best utility possible while protecting all information from each document provider in the training set, which could be exposed through textual (provider company name) or visual (logo, presentation) information. Participants are required to train under DP at different levels from medium DP (\(=1\)) to weak DP (\(=8\)) to mitigate the risk of provider information being leaked. Ultimately, the goal is to achieve the best utility while complying to the privacy budgets of \(\{1,4,8\}\) at \(=10^{-5}\). The definition of DP critically depends on the concept of adjacency of datasets. We seek to protect the privacy of providers and thus the typical document-level adjacency definition would be too weak, as there are many documents from the same provider and combining them could leak private information. Instead we use _provider-level add/remove adjacency_, where adjacent training datasets can be obtained by adding or removing all documents from one provider. Prior work denotes this as group-level DP (Marathe and Kanani, 2022; Galli et al., 2023).

Participants are required to follow the same rules regarding the pre-trained model and fine-tuning data as in track 1. Besides uploading the final model checkpoint solutions, they are required to submit a theoretical privacy proof and description. The requirement for a theoretical privacy proof in track 2 ensures that the solutions proposed by participants are rigorously validated for their adherence to differential privacy principles. This proof demonstrates that the final model maintains the privacy of all information from each document provider by offering a quantifiable measure of privacy loss. Additionally, a thorough description and code submission are necessary to facilitate reproducibility and allow for independent verification of the privacy claims, ensuring transparency and trustworthiness in the solutions provided.

### Baseline Solution Track 2

The baseline solution for track 2 utilizes DP stochastic optimization. The optimization of the model is done in multiple global rounds. In each round, the central server first samples a set of clients from all \(N=10\) clients. Each selected client runs a local instance of federated learning where each provider acts as the training data of a _virtual client_ within the real client. The client randomly selects providers, clips the per-provider updates and the adds an appropriate amount of noise so that the update aggregated by the server is differentially private with respect to all providers over all clients4 The privacy loss of the baseline follows the usual analysis of DP stochastic optimisation consisting of compositions of sub-sampled Gaussian mechanisms. The loss depends on the number of iterations \(T_{cl}\), sub-sampling rate \(q\) (both over clients and providers) and noise scale \(\)(Mironov et al., 2019; Balle et al., 2020). (See more details in Appendix A.4 and the privacy analysis in Appendix B).

### Winner Track 2: Ragul N and Kutum

Similar to the winning solution for track 1, our method also uses LoRA. We choose LoRA for the following two reasons: First, it significantly reduces the communication cost as shown in Section 4.3.

Second, empirical results have shown that differentially private adaptation of language models using parameter-efficient methods such as LoRA outperforms full fine-tuning in centralized settings (Yu et al., 2022b). These methods reduce the overall noise added by only updating a small proportion of the parameters in the model, thereby increasing the utility of the model. The communication efficiency of LoRA also allowed us to increase the number of FL rounds from 5 in the baseline method to 30 in our method without increasing communication costs. With these changes to the baseline, our method improved the ANLS by 10-11 percentage points across all privacy settings.

### Runners-up Track 2: Fukami, Yamasaki, Niwa, and Tyou

We briefly present our methods and experimental results. More detailed information can be found in Appendix D. It is well-known that applying DP to FedAVG with a relatively high privacy level often stagnates the model training process due to local parameter drift. This is mainly caused by i) noise addition in DP and ii) data heterogeneity among clients. To address these issues, we propose _DP-CLGECL_, which incorporates the DP's Gaussian mechanism into CLGECL Tyou et al. (2024). The update rules in DP-CLGECL are derived by solving a linearly constrained loss-sum minimization problem, resulting in robustness against local gradient drift due to data heterogeneity, and this would also be effective in addressing the drift issue due to DP's Gaussian mechanism. Note that the DP analysis of the private baseline detailed in Appendix B is applicable to our DP-CLGECL. More details about our methodologies are provided in Appendix D.2.

As indicated in Table 3, ANLS showed significant improvement with the use of our DP-CLGECL compared with the baseline method for each \(\). Associated experimental results, including convergence curves in Figure A.4 are summarized in Appendix D.2. After passing the competition deadline, we observed a negative impact of using AdamW optimizer in the baseline method. The norm of stochastic gradient, preconditioned by AdamW, often increased, and the gradient clipping used to ensure the pre-defined DP levels led to a loss of valuable information in model parameter training. To address this issue, we replaced AdamW with momentum in the local update of DP-CLGECL, resulting in further improved ANLS. Although more details can be found in Figure A.5, the ANLS was then \(0.5918\) for \(=1\) using DP-CLGECL with momentum.

## 6 Lessons Learnt and Recommendations for Future FL and DP Competitions

In this section we present lessons learnt from organizing this competition and discuss best practices that could be considered for organizing competitions in the future.

### Ensuring that the Track 2 Submissions Are DP

The track 2 of this competition required participants to provide a model checkpoint trained under DP. Additionally, we asked the participants to provide a privacy proof outlining how their method is formally differential private and requested the source code.

**Formal privacy proof** Asking for a privacy proof from the participants results in two things: (i) The organizers can check that a new proposed method is DP; and (ii) The participating team can reflect on ensuring that their method is actually DP. Insufficient formal analysis in prior work has lead to response papers (Carlini et al., 2021, 2022) that corrected the wrong analysis.

**Ensuring that the implementations are DP** While the privacy proof ensures that theoretically the submissions are DP, even small mistakes in the implementation of DP methods can invalidate

    &  &  &  \\  & & & **at \(=1\)** & **at \(=4\)** & **at \(=8\)** \\ 
1 & 
 Ragul N and Kutum (Section 5.3) \\  & LoRA & 0.5854 & 0.6121 & 0.6225 \\
2 & 
 Fukami et al. (Section 5.4) \\  & DP-CLGECL & 0.5724 & 0.6018 & 0.6033 \\
2 &  Organizers (Section 5.2) \\  &  Baseline \\  & 
 0.4832 \\  & 0.3024 & 0.5132 \\   

Table 3: Competition Winners Track 2 (Differential Private Federated Learning)or severely weaken the DP guarantees (Tramer et al., 2022b). Among these are the clipping of the updates, the correct noise addition and scaling as well as the subsampling. Thus, members of the organizing team have inspected the implementations of the best scoring methods but this is a manual process that does not scale to competitions with a large number of participants. The code reviews could be complemented with automatic tests that increase the chance of finding bugs in the implementation. Established DP libraries such as Opacas (Yousefpour et al., 2021) use unit tests but these tests are custom to the implementation that are testing and writing new tests requires much more manual labour than plain code reviews. Using only established implementations (e.g., like Opacus) for critical parts of the code would reduce the risk of bugs but also limit the possible solutions.

**Automation of the validation of DP methods and implementations** When scaling up the participant numbers of a competition, processes need to be automated. One example for that is our automatic utility evaluation on the secret test set. Automating the validation of DP methods and implementations is less straightforward: There are methods for auditing DP implementations (Jagielski et al., 2020; Nasr et al., 2023) but they are computationally expensive. Recent advancements have significantly reduced the cost of DP auditing (Steinke et al., 2023). One option would be auditing new submissions to assist in DP validation but it is unclear how computationally costly that would be. Auditing cannot conclusively prove something DP, so it should only be used to complement privacy proofs and code checks, not replace them.

### Lowering the Threshold for Participation

Referring to Table 1 one can see that the competition has received some interest. Also, it led to the data set being adopted in the privacy community (Wu et al., 2024) and increased the awareness in the document intelligence community (Biescas et al., 2024). Participants were required to be able to train a state-of-the-art Document Visual Question Answering model in a federated learning setting (under DP). The number of potential participants that have the required skill set is not as high as in other challenges. Thus it is important that the threshold for participation is as low as possible. We discuss measures that we took to lower the threshold for participation.

**Starting Kit** All solutions that are described in this analysis report utilized the provided starting kit to some extent. Based on the feedback from the participants, we think that the starting kit was crucial for them to participate. We can recommend to future organizers to test and document the starting kit extensively and include convenience functions (e.g., to compute communication cost or DP noise).

**Computational Cost** Simulating the FL setting and even just fine-tuning large pre-trained models requires a significant amount of compute. This is especially true under DP as the privacy/utility trade-off can be improved by training longer (Ponomareva et al., 2023) and using larger batch sizes (Raisa et al., 2024). We aimed to lower the threshold for participation by reducing the size of the client datasets and utilizing not the largest pre-trained model available. Still, executing the baselines with consumer hardware is hard if not impossible. One possible avenue for the future would be to open separate tracks for consumer hardware and provide cloud compute to teams that could otherwise not participate. The recent NeurIPS 2023 challenge on LLMs5 introduced some of these measures.