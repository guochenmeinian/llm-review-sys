# Improving Equivariant Model Training via Constraint Relaxation

Stefanos Pertigkiozoglou

University of Pennsylvania

pstefano@seas.upenn.edu

&Evangelos Chatzipantazis

University of Pennsylvania

vaghat@seas.upenn.edu

&Shubhendu Trivedi

Independent

shubhendu@csail.mit.edu

&Kostas Daniilidis

University of Pennsylvania

Archimedes, Athena RC

kostas@cis.upenn.edu

Equal Contribution

###### Abstract

Equivariant neural networks have been widely used in a variety of applications due to their ability to generalize well in tasks where the underlying data symmetries are known. Despite their successes, such networks can be difficult to optimize and require careful hyperparameter tuning to train successfully. In this work, we propose a novel framework for improving the optimization of such models by relaxing the hard equivariance constraint _during_ training: We relax the equivariance constraint of the network's intermediate layers by introducing an additional non-equivariant term that we progressively constrain until we arrive at an equivariant solution. By controlling the magnitude of the activation of the additional relaxation term, we allow the model to optimize over a larger hypothesis space containing approximate equivariant networks and converge back to an equivariant solution at the end of training. We provide experimental results on different state-of-the-art network architectures, demonstrating how this training framework can result in equivariant models with improved generalization performance. Our code is available at https://github.com/StefanosPert/Equivariant_Optimization_CR

## 1 Introduction

The explicit incorporation of task-specific symmetry in the design and implementation of effective and parameter-efficient neural network (NN) models has matured into a rational and attractive NN design meta-formalism in recent years--that of group equivariant convolutional neural networks (GCNNs) (Cohen and Welling, 2016; Ravanbakhsh et al., 2017; Esteves et al., 2018; Kondor and Trivedi, 2018; Cohen et al., 2019; Maron et al., 2019; Weiler and Cesa, 2019; Bekkers, 2020; Villar et al., 2021; Xu et al., 2022; Pearce-Crump, 2023). GCNNs involve using the machinery of group and representation theory to compose layers that are equivariant to transformations of the input. Such networks, with hard-coded symmetry, have proven to be successful across a wide variety of tasks, while often affording significant data efficiency. Such tasks/domains include: RNA structure (Townshend et al., 2021), protein structure (Baek et al., 2021; Jumper et al., 2021), molecule generation (Satorras et al., 2021), medical imaging (Winkels and Cohen, 2019), natural language processing (Gordon et al., 2020; Petrache and Trivedi, 2024), computer vision (Chatzipantazis et al., 2023), robotics (Zhu et al., 2022; Ordonez-Apraez et al., 2024), density functional theory (Gong et al., 2023), particle physics (Bogatskiy et al., 2020), lattice gauge theories (Boyda et al., 2021) amongst many others. GCNNs now have also matured enough to have a well-developed theory. This includes both prescriptive (orarchitectural) theory and descriptive analysis. In general, GCNNs particularly stand out in domains with data scarcity, or with a high degree of symmetry (Kufel et al., 2023; Boyda et al., 2021), or in the physical sciences where respecting explicit symmetries could be dictated by physical laws, violating which could lead to physically implausible predictions.

Despite the successes of group equivariant models, there are several outstanding challenges that don't yet have general satisfactory solutions. We discuss two that have attracted recent attention. The first challenge--the primary motivation of our paper--has to do with the common observation that equivariant networks can be difficult to train (Wang et al., 2024; Kondor et al., 2018; Liao and Smidt, 2023). The reasons for this general difficulty are not well-understood, but it seems to occur in part because the training dynamics of such networks can be notably different from non-equivariant architectures. For instance, if a GCNN operates entirely in Fourier space (Bogatskiy et al., 2020; Kondor et al., 2018; Xu et al., 2022), most of the usual intuition about training NN models does not apply. Further, depending on the level of equivariance error tolerance for a task, the internal layers could be computationally intensive, and involve e.g. higher-order tensor products. Notably, the above difficulty arises even when the model is correctly specified i.e. the model and the data encode the same symmetry. The second challenge with GCNNs, has to do with the downsides of working with exact equivariance when the data itself might have some (possibly) relaxed symmetry. This has recently led to a spurt of work on developing more flexible networks that can vary the amount of equivariance depending on the task (Finzi et al., 2021; Romero and Lohit, 2022; van der Ouderaa et al., 2022; Wang et al., 2022; Huang et al., 2023; Petrache and Trivedi, 2023). Such models generally improve accuracy and will sometimes also simplify the optimization process as a side-effect. Broadly, proposed solutions involve adding additional regularization terms that penalize for relaxation errors, solving for the problem of model mis-specification (Petrache and Trivedi, 2023).

However, even though there is now work on relaxed2 equivariant networks that addresses model misspecification, existing works don't focus on improving the optimization process directly. In this paper, we take a step towards examining this question in more detail. We make the case that _even if we assume that the model is correctly specified_, relaxing the equivariance constraint during optimization and then projecting back to the equivariant space can itself help in improving performance. We conjecture that a prime reason for the optimization difficulty of GCNNs, as compared to non-equivariant models, is that their solution-space might be too severely constrained. We derive regularization terms that encourage each layer to operate in a larger hypothesis space during training--than being constrained to only be in the intertwiner space--while encouraging equivariant solutions. After the optimization is complete, we project the solution back onto the space of equivariant solutions. The approach can also be adapted to better optimize approximately equivariant networks in a similar manner. The focus of our work thus distinguishes it from works on relaxed equivariance--we are not concerned with mis-specification, but rather with isolating the optimization process itself.

Below we summarize the main contributions of our work:

* We present a novel training framework that can improve the performance of equivariant neural networks by relaxing the equivariance constraint during training and projecting back to the space of equivariant models during testing (as shown in Figure 1).
* We present a formulation that extends existing equivariant neural network architectures to be approximately equivariant. We show how training on the relaxed network can improve the performance of its equivariant subnetwork.
* We provide experimental evidence showcasing how our framework improves the performance of existing state-of-the-art equivariant architectures.

## 2 Related Work

There is little prior work on providing general procedures for improving the optimization process for equivariant neural networks directly. Elesedy and Zaidi (2021) sketched a projected gradient method to construct equivariant networks and suggested a regularization scheme that could be used to implement approximate equivariance. However, this was proposed as an aside in the paper (sections 7.2 and 7.3), without empirical or theoretical backing. Our work also involves a projected gradient procedure. However, the regularization scheme that we propose is substantially different.

Work on approximate and partial equivariance (Finzi et al., 2021; Romero and Lohit, 2022; van der Ouderaa et al., 2022; Wang et al., 2022; Huang et al., 2023; Petrache and Trivedi, 2023; Wang et al., 2023) seems superficially related to ours, but comes with a different motivation. Such methods aim to match data symmetry with model symmetry and are not explicitly concerned with improving optimization. As a result, they are designed to address tasks with either inherent relaxed symmetries or tasks where the underlying relaxed symmetry is misspecified. Contrary to that, our method focuses on cases where the underlying symmetry is known exactly, and the relaxation of the equivariant constraint is used only during training as a way to improve the optimization. The works of Finzi et al. (2021); van der Ouderaa et al. (2022); Gruver et al. (2023); Otto et al. (2024); Petrache and Trivedi (2023) are nonetheless relevant since they provide methods for measuring relaxed equivariance, comprising of regularization schemes that are related to those used in our paper, since we also need measures of relaxation. In fact, the work of Gruver et al. (2023) directly inspires one component of our method. On the theoretical side, Petrache and Trivedi (2023) studied generalization-approximation tradeoffs in approximately/fully equivariant CNNs in a very general setting, characterizing the effect of model mis-specification on performance. They quantify equivariance as improving the generalization error, and the alignment of data and model symmetries as improving the approximation error. They leave the impact of improving the optimization error for future work. While we do not provide theoretical results, our work could be seen as focusing on optimization error component of the classical picture3.

Maile et al. (2023) proposed what they call an _equivariance relaxation morphism_, which reparamterizes an equivariant layer to operate with equivariance constraints on a subgroup, but with the goal of architecture search. Flinth and Ohlsson (2023) provide an analysis of the optimization dynamics of equivariant models and compare them to non-equivariant models fed with augmented data. However, they don't use the analysis to provide insights on improving the optimization procedure itself.

Several researchers have recently tried to circumvent optimization difficulties in other ways. For instance, Mondal et al. (2023) suggests using equivariance-promoting canonicalization functions on top of large pre-trained models. The work of Basu et al. (2023) operates with a similar motivation but without canonicalization. Yet another representative of work with a fine-tuning motivation, but in a different context is (Basu et al., 2023). Finally, simplifying equivariant networks with heavy equivariant layers and improving their scalability is an active area of work and is related to easing optimization. Such works usually employ tools from representation theory, tensor algebra, or exploit sampling theorems over compact groups and their homogeneous spaces, such as Passaro and Zitnick (2023); Luo et al. (2024); Cobb et al. (2021); Ocampo et al. (2023).

## 3 Method

To introduce our proposed optimization framework, we first clearly define the equivariant constraint that the models we aim to train must satisfy. Assume a function \(f:^{n}^{m}\) and a group \(G\)4 acting on the input and output spaces via the general linear representations \(_{}:G(^{n})\), \(_{}:G(^{m})\). Then the function is said to be equivariant to the action of group \(G\) if for all \(g G\) it satisfies the following constraint:

\[f(_{}(g)x)=_{}(g)f(x),x ^{n}\] (1)

Assuming we use a neural network to approximate the function above, the definition of equivariance as stated doesn't impose specific constraints on the individual layers of the network. Nevertheless, most of the current state-of-the-art equivariant architectures are a composition of simpler layers each one of which is constrained to be equivariant. In this case, the overall model is the result of a composition \(f=f_{N} f_{N-1} f_{2} f_{1}\) of simpler equivariant layers \(f_{i}:V_{i} V_{i+1}\), where \(V_{i}\), \(V_{i+1}\) are the input and output spaces on which the group \(G\) acts with the corresponding representations \(_{V_{i}}\), \(_{V_{i+1}}\) (assuming \(V_{1}=^{n}\), \(V_{N}=^{m}\) are the input and output spaces respectively).

In this work we focus on a family of models as described above--that are defined through a composition of simpler equivariant linear layers. During standard training the linear layers are optimized over the set of intertwiners \(H_{i}\), i.e. the set of linear maps between the representations \((V_{i},_{V_{i}})\) and \((V_{i+1},_{V_{i+1}})\) that have the equivariance property as stated in Equation 1. The set of intertwiners is only a subset of the set of all possible linear maps from \(V_{i}\) to \(V_{i+1}\), and as a result, they have a reduced number of free parameters. We propose to facilitate training over this constrained space by relaxing the constraint imposed on the intermediate layers and optimizing over a larger hypothesis space \(\) which is a superset of the set of equivariant models \(H\). A trivial approach is to completely relax the constraint and solely optimize over the larger set containing all models. The problem with such an approach is that it completely abandons the concept of equivariance and all the attendant generalization benefits. Consequently, to expand the hypothesis space while keeping the benefits of equivariant models, we need a relaxation such that:

* Given a non-equivariant model \(f\), we can efficiently return to an equivariant one \( H\).
* The relaxed model has a small _equivariance error_\(P_{}=_{x p(x)}_{G}\|_{out}(g)f(x)-f(_{ in}(g)x)\|dg\). This implies that although we extend the space of models we optimize over, we do not diverge too far away from the space of equivariant solutions.
* After we project back to the equivariant space, the error of the projection \(P_{}=_{x p(x)}[\|f(x)-(x)\|]\) is also small. This ensures that while we optimize the less constrained model, we can return to the equivariant one without sacrificing the overall performance.

The first objective can be satisfied by defining an intermediate layer of the form:

\[f(x)=f_{e}(x)+ Wx, 0f_{e} H,W^{|V_{}||V_{}|}\] (2)

where \(H\) is the set containing all possible equivariant solutions. Here it is easy to see that we can return to an equivariant model by setting \(=0\), which we refer to as projection to the equivariant space. The formulation of the linear layer above is similar to the one used in the Residual Pathway Priors (RPP) (Finzi et al., 2021). Note that in RPP, the value of \(\) remains constant and acts as a prior on the level of equivariance we expect from a given task and dataset. Contrasted to that, in this work we aim to control the value of \(\) in order to actively change the level of equivariance during training and project back to the equivariance space during inference.

For the second objective, we need to utilize a metric that measures the relative distance of the model from the space of equivariant models \(H\). It was observed by Gruver et al. (2023) that an easy way to measure how much a model satisfies the equivariant constraints is by using the norm of the Lie derivative. We present details in the next section.

Figure 1: Standard training of equivariant NNs is constrained to a limited parameter space which can result in a challenging training process. We propose to relax these equivariant constraints during training, allowing optimization over a broader space of approximately equivariant models. During testing, we project the trained model back to the constrained space—arriving at an equivariant model with enhanced performance compared to equivalent models trained with the standard process.

### Lie Derivative Regularization Term

Assume we are given a matrix Lie group \(G\) acting on a vector space \(V\) through its representation \(:G GL(V)\). For the given group there exists a corresponding Lie algebra \(\) with the property that for \(A\), \(e^{A} G\). Additionally, there exists a corresponding Lie algebra representation \(d:(V)\) such that \((e^{tA})=e^{d(A)t}\).

If we take the derivative of the action of a group element \(e^{tA} G\) at \(t=0\) we get the Lie derivative:

\[.|_{t=0}(e^{tA})=.|_{t=0}e^{ d(A)t}=d(A)\] (3)

Assume that the following group representation act on the vector space of functions as:

\[_{}(g)[f]=_{}(g)^{-1}(f_{ }(g))\] (4)

As observed by Gruver et al. (2023) the lie derivative of the above action is zero for all equivariant functions \(f\), since for all \(g G\) the action \(_{}(g)[f]=f\) is the identity map. As a consequence, we can use the norm of the Lie derivative as a metric to compute how much a function \(f\) deviates from the equivariant constraint of Equation 1. For the linear relaxation term \(Wx\) that we introduced in Equation 2, we have that the Lie derivative can be computed in a straightforward manner as:

\[_{A}(W)=.|_{t=0}_{}(e^{-At})W_{}(e^{At}) =.|_{t=0}e^{-d_{}(A)t}We^{ d_{}(A)t}\] \[=-d_{}(A)W+Wd_{}(A)\]

As a result, we can measure the degree that a linear layer satisfies the equivariant constraint at a point \(x\), by computing the norm of the Lie derivative at that point for each one of the generators of the group. For example in the case where \(G\) is the group of 3D rotations (\(G=(3)\)), we can compute the Lie derivative for each generator:

\[J_{x}=0&0&0\\ 0&0&-1\\ 0&1&0,J_{y}=0&0&1\\ 0&0&0\\ -1&0&0,J_{z}=0&-1&0\\ 1&0&0\\ 0&0&0\]

During training, given an input distribution \(p(x)\), we compute, for each linear layer \(Wx\), the Lie derivative regularization term:

\[_{ld}(W)=_{x p(x)}(_{A\{J_{i}\}}\| _{A}(W)x\|)\]

Although the above regularization applies when the symmetry group we are considering is a matrix Lie Group, as we show in the experiments of Section 4.4, we can also define a similar regularizer for the case of discrete finite groups. In such a case, for a given linear layer with weights \(W\) and input \(x\), we compute the sum of the norms of the difference \(_{g_{j}}(W)x=((g_{j})W-W(g_{j}))x\) for all generators \(g_{j}\) of the discrete finite group under consideration.

As discussed in Otto et al. (2024) and shown in Figure 3(a), the inclusion of the above regularization terms encourages equivariant solutions and prevents the model from diverging away from the space of equivariant models. Moreover, Figure 2 shows how the inclusion of this regularization helps the overall training and results in a performance improvement of the final trained model.

### Reducing the Projection Error

While we optimize over a larger hypothesis space, we always aim to return to an equivariant model after the end of training. Using the parametrization in Eq. 2 we can always do that by setting \(\) to be equal to zero. Although after the projection the resulting model is guaranteed to be equivariant, it might be far from the original relaxed version, meaning it might have a large projection error \(P_{pe}\). Specifically, for an individual relaxed layer the projection error is:

\[P_{pe}=_{x p(x)}[\|f(x)-(x)\|] =_{x p(x)}[\|f_{e}(x)+ Wx-f_{e}(x)\|]\] \[=_{x p(x)}[\| Wx\|]\]

[MISSING_PAGE_FAIL:6]

between elements of different types of vectors. We relax the equivariant constraint by adding an unconstrained linear layer that can mix the elements from all the tensors used as intermediate representations, independent of their type. In Equiformer we add such a linear layer in the feed-forward network of the transformer block. Similarly in SEGNN, we add it to the layer that receives the aggregated messages from all the neighbors of a node and updates the node features.

Approximately Equivariant Steerable Convolutions (Wang et al., 2022):In this work, the authors designed approximate equivariant steerable convolutional layers. We apply our method by incorporating an additional unconstrained convolutional kernel. Since this task contains discrete symmetry groups, namely discrete rotations and scalings, we replace the Lie derivative regularizer with the corresponding one for discrete groups, described in Section 3.1.

## 4 Experiments

### Equivariant Point Cloud Classification

We first evaluate our optimization framework by training different networks on the task of point cloud classification. We use the equivariant variants of PointNet (Qi et al., 2016) and DGCNN (Wang et al., 2019) which were proposed by Deng et al. (2021). We train on the ModelNet40 dataset (Chang et al., 2015), which contains 12311 point clouds from 40 different classes. We compare with the standard training of these networks using the same hyperparameter configuration as employed in Deng et al. (2021). During both training and testing, we sub-sample the input point clouds to 300 points.

To apply our method we relax the Vector Neurons linear layer by following the methodology described in Section 3.4. For both networks we set the regularization term \(_{reg}=0.01\), which is a value we use in all of the following experiments. We provide a more detailed description of the training parameters in Appendix A.1. Furthermore, in Appendix A.2 we describe the process of choosing the hyperparameter \(_{reg}\) and show the method's robustness to its value. Figure 2 showcases how applying our proposed framework benefits the training of both networks. Specifically, for the case of the smaller and less performant PointNet, we can see an even larger performance increase over the baseline. These results show how the performance benefits of our optimization framework increase in smaller under-parametrized networks, an effect we investigate further in Section 4.2. In Appendix A.3 we provide additional details on the computational and memory overhead of our proposed optimization, showcasing that while additional parameters are introduced during training the overhead in the training time is limited.

Ablations on the regularization terms and \(\) scheduling:In addition to the training curves of our method and of the baseline, Figure 2 shows the accuracy of our proposed optimization procedure when we remove some of the proposed regularization terms or the scheduling of \(\). We observe that without any regularization both models diverge from the space of equivariant solutions. As a result, during inference when \(=0\) their projection error \(P_{pe}\) becomes larger, resulting in a significant drop in test accuracy. Similarly, without the Lie derivative regularizer, the final test accuracy of both network variants drops. In such cases, \(Wx\) is unconstrained and can learn to extract non-equivariant features that the equivariant part \(f_{e}\) is not able to learn in any stage of the training. This effect can also be observed in figure 3(a) showing the total Lie derivative of the network when it is trained with and without the lie derivative regularization term. Not including the Lie derivative regularization allows the network, especially in the beginning of training, to optimize over solutions with large equivariance error. Finally, for both networks, we observe that \(\) scheduling, as described in Section 3.2, can benefit training compared to fixing \(\) to a constant low value. In Appendix A.4 we provide additional results showcasing how contrary to our method a model with a constant \(\) (without \(\) scheduling) has a significant drop in performance after it is projected into the equivariant space.

### Scaling on Different Model and Dataset Sizes

To better understand how the model and dataset sizes affect our proposed optimization framework, we train models of variable depth on different numbers of training samples. As a baseline model we use the Steerable E(3) GNN (SEGNN) (Brandstetter et al., 2021) and we train it on the task of Nbody particle simulation (Kipf et al., 2018). This task consists of predicting the position of 5 electrically charged particles after 1000 time steps when given as input their initial positions, velocities, and charges.

Figure 4(a) shows the mean average test error achieved by networks of different sizes, both when trained with a standard optimization, and when trained with our proposed framework. We can observe that for all sizes our method achieves better generalizations. The gap between our method and the baseline becomes greater in the cases of smaller networks, a phenomenon that we also observed in the point cloud classification experiments in Section 4.1. Thus, our framework, by relaxing the constraint and introducing additional degrees of freedom, can help the overall optimization, especially in models with a limited number of parameters. Additionally, figure 4(b) shows that when we fix the model size and increase the dataset size our method is able to scale better than the baseline. In both cases, we can observe that the training of the baseline has a much larger variance and is highly dependent on the random initialization of the layers. On the contrary, our method results in a more consistent training with a smaller variance between the random trials.

### Molecular Dynamics Simulation

To evaluate our framework in a challenging task using a complex network architecture, we train Equiformer (Liao and Smidt, 2023) on the task of molecular dynamics simulations for a set of molecules provided as part of the MD17 dataset (Chmiela et al., 2017). The goal of this task is to predict the energy and forces from different configurations of a pre-specified molecule. Following Liao and Smidt (2023), for each molecule we use only 950 different configurations for training which significantly increases the task difficulty. For all training runs we use the same value of \(_{reg}=0.01\) as in the

Figure 3: (a) Norm of the total Lie derivative of the relaxed PointNet model trained with and without the Lie derivative regularization term. For the computation of the Lie derivative we use the method proposed in Gruver et al. (2023). (b) Value of the Lie derivative regularization term for each individual layer of the relaxed PointNet model while we train using our framework and with Lie derivative regularization weight set to \(_{}=0.01\)

Figure 2: Test accuracy on ModelNet40 classification, during training of equivariant PointNet and DGCNN using a baseline training process and different versions of our method. The accuracy is computed for the equivariant models, i.e. for the models after they are projected in the equivariant space.

previous experiments and for the rest of the hyperparameters, we use the same configuration as the one proposed in Liao and Smidt (2023). In Table 1 we show that the mean absolute error of energy and force prediction achieved by Equiformer, both when it is trained using standard training, and when it is trained with our proposed optimization framework. Without any additional hyperparameter tuning, our framework is able to provide improvements on the performance of Equiformer even for this challenging data-scarce task.

### Optimizing Approximately Equivariant Networks

Finally, we show how our framework can be beneficial, not only for the optimization of exactly equivariant networks, but also for approximate equivariant ones. We apply our method on top of the approximate equivariant steerable CNNs proposed in Wang et al. (2022). Although these models are not projected back to the equivariant space, they are still regularized to stay within solutions with small equivariant error. The main difference with our framework is that in the approximate equivariant setting, the equivariant relaxation remains the same throughout training. On the contrary, we propose to progressively constrain the model by modulating the value of the unconstrained term by slowly decreasing the value of \(\) from Equation 2. As a result by applying our optimization framework on top of the standard training of the approximate equivariant kernels, we test how progressively introducing additional constraints throughout training can help the performance of the network.

We evaluate our method on the task of 2D smoke flow prediction described in Wang et al. (2022). The inputs of the model are sequences of successive \(64 64\) crops of a smoke simulation generated by PhiFlow (Holl et al., 2020). The desired output is a prediction of the velocity field for the next time step. We evaluate in two different settings: the "Future" setting where we evaluate on the same part of the simulation but we predict future time steps not included in the training, and the "Domain" setting where we evaluate on the same time steps as in training but in different spatial locations. The data are collected from simulations with different inflow positions and buoyant forces. For the rotational symmetry case, while the direction of the inflow and of the buoyant force is symmetric to \(90^{}\) degrees rotations (\(C_{4}\) symmetry group), the buoyancy factor changes for the different directions making the task not symmetric. For the scaling symmetry, the simulations are generated with different spatial and temporal steps, with the buoyant factor changing across scales.

    &  &  &  &  \\  Methods & Energy & Forces & Energy & Forces & Energy & Forces & Energy & Forces \\  Equiformer & 5.3 & 7.2 & 2.2 & 6.6 & 2.2 & 3.1 & 4.5 & 4.1 \\ Equiformer+Ours & **5.2** & **7.1** & 2.2 & 6.6 & **2.0** & **2.9** & **4.1** & 4.1 \\   

Table 1: MAE of Equiformer trained with and without our optimization framework on a set of molecules from the MD17 dataset. The energy is reported in meV and the force in meV/Å units

Figure 4: Mean Average Error on the Nbody particle simulation for (a) different model sizes, (b): different dataset sizes.

In addition to the approximate equivariant steerable CNNs (RSteer), we compare with a simple MLP, with a non-equivariant convolutional network (ConvNet), as well as with an equivariant convolutional network (Equiv) (Weiler and Cesa, 2019) and with two additional approximate equivariant networks RPP (Finzi et al., 2021) and LIFT (Wang et al., 2021) that are trained using a standard training procedure. In Table 2 we see that by applying our optimization framework the resulting approximate equivariant model outperforms all other baselines in both cases of approximate rotational and scale symmetry. These results indicate that starting from an unconstrained model and progressively increasing the applied constraints can benefit optimization even in the case where at the end of training we stay in the space of approximate equivariant models and do not project back to the equivariant space.

## 5 Conclusion

In this work, we focus on the optimization of equivariant NNs. We proposed a framework for improving the overall optimization of such networks by relaxing the equivariance constraint and optimizing over a larger space of approximately equivariant models. We showcase the importance of utilizing regularization during training to ensure that the relaxed models stay close to the space of equivariant solutions. After training, we project back to the equivariant space arriving at a model that respects the data symmetries, while retaining its high performance on the task. We evaluate our proposed framework and its individual components over a variety of different equivariant network architectures and training tasks, and we report that it can consistently provide performance benefits over the standard training procedure. A theoretical analysis of our approach, possibly with appeal to empirical process theory (Pollard, 1990) to control the optimization error, is left for future work.