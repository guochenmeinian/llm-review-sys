# A Transition Matrix-Based Extended Model for Label-Noise Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The transition matrix methods have garnered sustained attention as a class of techniques for label-noise learning due to their simplicity and statistical consistency. However, existing methods primarily focus on class-dependent noise and lack applicability for instance-dependent noise, while some methods specifically designed for instance-dependent noise tend to be relatively complex. To address this issue, we propose an extended model based on transition matrix in this paper, which preserves simplicity while extending its applicability to handle a broader range of noisy data beyond class-dependent noise. The proposed algorithm's convergence and generalization properties are theoretically analyzed under certain assumptions. Experimental evaluations conducted on various synthetic and real-world noisy datasets demonstrate significant improvements over existing transition matrix-based methods. Upon acceptance of our paper, the code will be open sourced.

## 1 Introduction

Deep neural networks have achieved remarkable success in various fields in recent years, especially in classification problems with labeled data . Compared to traditional methods, deep neural networks have greatly improved performance but their effects heavily depend on the accuracy of the provided labels. Bringing data with corrupted labels into the neural network model without special treatment can severely affect the prediction performance . However, acquiring accurately annotated data in reality can be very expensive, so a larger amount of data comes from the Internet or annotations by non-professional annotators. Therefore, it is currently worth studying and promoting how to alleviate the damage caused to the model when using noisy labels and make the model more robust, which is known as the problem of label-noise learning or called learning with noisy labels .

Various methods have been proposed for label-noise learning. Existing methods can be classified into several categories. One of them is to design novel loss functions or network structures , which reduce the impact of noisy labels to make the model more robust. Another category is sample selection based on sample loss or feature extracted, dividing samples into the clean dataset and the noisy dataset . Then they relabel the noisy labels , or clear the noisy labels and use semi-supervised methods for learning . These methods are common recently and have achieved some good results. However, the process of sample selection is relatively subjective, and statistical consistency is lost after the selection, and most of them lack theoretical support. In contrast, transition matrix methods  have statistical consistency and usually have corresponding theoretical analysis as support, attracting continued attention and occupying an important position in various learning algorithms with label noise.

The core idea of transition matrix methods is to use a matrix measuring the transition probability from the distribution of true label to the distribution of observed noisy label. If an accurate transition matrix can be estimated and combined with observable data to obtain the noisy class-posterior probability, the distribution of clean label can be inferred for network learning. Therefore, estimating the transition matrix is the key to this type of method. However, it is infeasible to estimate an individual transition matrix for each sample without additional conditions . Previous methods mostly focus on class-dependent and instance-independent label noise problems [43; 22; 51], assuming that the transition matrix is fixed for all samples. Among these methods, some [31; 43] assume the existence of anchor points to estimate the transition matrix, while other methods obtain the optimal estimation by adding a regularization term for matrix structure to weaken the anchor points assumption [22; 51]. However, these methods are not suitable for instance-dependent label noise and complex real-world data because they estimate only one matrix for all samples. Moreover, when the estimation of noisy class-posterior distribution is inaccurate, the estimation of the transition matrix may be easily affected , thereby affecting the estimation of the clean label distribution. Although some methods [42; 58; 52; 20] have recently been designed to use special networks or structures for instance-dependent noise situations, the estimation errors for them are still large, and the computational cost is too high to lose the concise characteristic of transition matrix methods.

Addressing the limitations of current transition matrix-based methods, this paper introduces an extended model for transition matrix that extends their applicability from class-dependent noise to a broader range of label-noise data without requiring additional techniques such as clustering or self-supervised learning. Inspired by methods that handle noise using sparse structures [57; 25], our model combines a global transition matrix with a sparse implicit regularization term [31; 25] for fitting the distribution of noisy labels across instances, replacing the need for estimating a separate transition matrix for each sample. This approach allows us to incorporate instance-level information into the model, expanding its capability beyond class-dependent noise scenarios while avoiding the unidentifiability and computational complexity of estimating instance-dependent matrices.

The structure of the following sections is as follows. In Section 2, we give relevant definitions and propose our method. In section 3 we conduct a theoretical analysis of the proposed method on a simplified model. In Section 4, we conduct experiments on various synthetic and real-world noisy datasets, comparing with other transition matrix-based methods. We conclude the paper in Section 5. In addition, we provide a more specific review of related works in Appendix A, proofs of theorems in Appendix B, and experimental details in Appendix C.

The main contributions of this paper are:

* We propose a novel extended model for transition matrix, incorporating sparse implicit regularization, which enables the extension of transition matrix methods from class-dependent noise to a broader range of noisy label data while maintaining simplicity, without the need for excessive additional framework design or sophisticated techniques.
* Under certain assumptions, we provide theoretical analysis on the convergence and generalization results of the algorithm on a simplified model. We prove the theorems proposed accordingly, giving support for the effectiveness of the proposed method.
* Our proposed method achieves significant improvements compared to previous transition matrix methods on both synthetic and real-world noisy label datasets, and produces competitive results without the need for additional auxiliary techniques.

## 2 Methodology

In this section, we give relevant definitions and propose a novel model that extends the transition matrix with implicit regularization (TMR) from class-dependent noise to more label-noise. It is a convenient and end-to-end model. We will formulate the method in detail and illustrate it theoretically.

### Preliminaries

Let \(^{d}\) be the feature space, \(=\{1,2,,C\}\) be the label space, where \(C\) is the number of classes. Random variables \((X,Y),(X,)\) denote the underlying data distributions with true and noisy labels respectively. In general, we can not observe the latent true data samples\(_{(N)}=\{(_{i},y_{i})\}_{i=1}^{N}\), but can only obtain the corrupted data \(}_{(N)}=\{(_{i},_{i})\}_{i=1}^{N}\), where \(\) is the noisy label corrupted from the true label \(y\), while denote corresponding one-hot label as \(\) and \(}\).

Transition matrix methods use a matrix \(()^{C C}\) to represent the probability from clean label to noisy label, where the \(ij\)-th entry of the transition matrix is the probability that the instance \(\) with the clean label \(i\) corrupted to a noisy label \(j\). The matrix satisfies the requirement that the sum of each row \(_{j=1}^{C}_{ij}()\) is \(1\), and usually has the requirement for \(_{ii}()>_{ij}(), j i\). The set of possible values for \(\) is denoted as \(=^{C C}|_{j=1}^{C}_{ij}=1, {T}_{ii}>_{ij}, j i}\).

Let \(P(|X=)=[P(Y=1|X=),,P(Y=C|X=)]^{}\) be the clean class-posterior probability and \(P(}|X=)=[P(=1|X=),,P(=C|X= {x})]^{}\) be the noisy class-posterior probability, the formula can be write as:

\[P(}|X=)=()^{}P(|X=). \]

Though estimating the transition matrix and the noisy class-posterior probability, the clean class-posterior probability can be inferred by \(P(|X=)=()^{-}P(}|X=)\), where the symbol \(-\) denotes the transpose of the inverse matrix. Alternatively, the neural network can be utilized to fit the clean label distribution by the loss function:

\[=_{i=1}^{N}((_{i})^{}f_{}(_{i}),}_{i}), \]

where \(f_{}():^{C-1}\) (\(^{C-1}^{C}\) is the \(C\)-dimensional simplex) is a differentiable function represented by a neural network with parameters \(\) and \(\) is a loss function usually using cross-entropy (CE) loss. Therefore, the key to addressing the problem in this class of methods lies in how to estimate the transition matrix.

Since it is difficult to estimate the transition matrix \(()\) individually for each sample, the majority of existing methods [31; 10; 22] focus on studying the class-dependent and instance-independent transition matrix, i.e., \(()=\) for \(\). However, these methods are limited by the assumption of class-dependence and cannot be directly applied to instance-dependent label noise with good effectiveness. Our objective is to make improvement and extension based on this limitation.

### Transition Matrix with Implicit Regularization

The main issue with directly applying class-dependent transition matrix methods to instance-dependent noise lies in using a fixed matrix \(\), multiplying with clean class-posterior probability \(P(|X)\), i.e., \(^{}P(|X)\) is not always equal to the noisy class-posterior probability \(P(}|X)\), even if the probability values \(P(|X)\) and \(P(}|X)\) are correctly estimated. Therefore, for a broader range of label-noise scenarios, relying solely on a fixed matrix \(\) is insufficient.

The core idea of our proposed model is to introduce a residual term \((X)\) to fit the distribution difference between \(P(}|X)\) and \(^{}P(|X)\), where \((X)\) is a \(C\)-dimensional vector for each \(X\). It can be transformed into using \(^{}P(|X)+(X)\) to fit \(P(}|X)\).

Intuitively, if an overall relatively suitable transition matrix \(\) is applied to \(^{}P(|X)\), then the difference between it and the probability \(P(}|X)\) should be small. Inspired by methods that handle noise using sparse structures [57; 25], we utilize a sparse structure to model the residual term \(\). Follow the works [30; 31; 25], using implicit regularization to represent sparse structures is a method that facilitates updates and provides more stable learning performance. We exploit this technique to model the residual term as \(_{i}=_{i}_{i}-_{i}_{i}\) with respect to training sample \(_{i}\), where \(_{i}\), \(_{i}\) are all \(C\)-dimensional vectors and \(\) denotes an entry-wise Hadamard product. As usual, we use a deep neural network \(f_{}()\) to learn the true label probability \(_{i}\) w.r.t \(_{i}\). So for the noisy label probability distribution \(}_{i}\) given by the data, the model use \(^{}f_{}(_{i})+_{i}_{i} -_{i}_{i}\) to fit it. Bring it into the loss function as:

\[_{i=1}^{N}(^{}f_{}(_{i})+ {u}_{i}_{i}-_{i}_{i},}_{i}). \]Due to the potential existence of different \(\) and \(P(|X=)\) such that \(P(|X=)=_{1}^{}P_{1}(|X=)=_{2}^{}P_{2} (|X=)\), we add a regularization term of the volume of the matrix \(()=()\) to loss function as  to ensure the transition matrix is identifiable. The total loss function applied in our proposed method is:

\[(,,\{_{i},_{i}\}_{i=1}^{N} )=_{i=1}^{N}(^{}f_{}(_{i})+ _{i}_{i}-_{i}_{i},}_{i})+ (), \]

where we estimate parameters according to:

\[},},\{}_{i},}_{i} \}_{i=1}^{N}=*{arg\,min}_{,,\{_{i},_ {i}\}_{i=1}^{N}=1}(,,\{_{i},_{i}\}_{i= 1}^{N}). \]

We use the gradient descent method to update the parameters to be learned above. This method constitutes our proposed extended **T**ransition **M**atrix model with sparse implicit **R**egularization (TMR).The method steps are summarized in Algorithm 1 in Appendix B.1.

Through our model, the estimation of individual transition matrices for each sample is replaced by the estimation of the global matrix and the sparse residual term. In this way, the number of parameters for the transition matrix is reduced from \(O(NC^{2})\) to \(O(NC)\), which greatly reduces the difficulty of matrix estimation and computational consumption when \(C\) is large. In addition, the incorporation of sparse implicit regularization in combination with the transition matrix makes the learning optimization process concise and efficient.

### Integration with Contrastive Learning

To further improve the effectiveness of our approach, we first utilize contrastive learning as a pre-trained feature extractor, followed by label learning. In this work, we also examine the enhancement of the TMR method by incorporating the SimCLR method from contrastive learning as a feature learner as pre-trained encoder, then resulting in TMR+.

## 3 Theoretical Analysis

In this section, we want to analyze the effectiveness of the proposed method theoretically under specific conditions related to label-noise generation. However, it is difficult to give a direct analysis of the deep neural network model. So we follow the theoretical analysis method of  to simplify the proposed model and study on an approximately linear structure to demonstrate the effectiveness of our proposed model.

### Model Simplification and Convergence Analysis

The first to solve is the construction of an approximate simplified model for theoretical analysis of our algorithm. Based on , we use first-order Taylor expansion to approximate the deep neural network \(f_{}()\), which is highly over-parameterized:

\[f_{}() f_{_{0}}()+( }^{}()}{}_{ =_{0}})^{}(-_{0}), \]

where \(f_{}()\) is a C-dimensional vector, \(^{p}\) (\(p N\)) denotes the parameters of the neural network, \(.}^{}()}{} |_{=_{0}}\) is a \(p C\) matrix, \(_{0}\) is the initialization of \(\), symbol \(\) represents matrix multiplication. For simplicity, we drop the constant term in the derivation and abbreviate \(.}^{}()}{} |_{=_{0}}\) as \(_{_{0}}f()\). The approximate formula becomes:

\[f_{}()_{_{0}}f() ^{}. \]

Through this processing, we simplify the deep neural network into an approximately linear structure, and we use \(f_{}()=_{_{0}}f()\) in the following theoretical analysis. We use a \(N C\) matrix \(\) to represent the neural network predictions on the overall training dataset \(\{(_{i},y_{i})\}_{i=1}^{N}\):

\[=[f_{}^{}(_{1} )\\ \\ f_{}^{}(_{N})]. \]In order to be written in matrix form, we rewrite the formula (7) in vector expansion form:

\[f_{}^{}()=[f_{}()_{1},,f_{} ()_{C}]=(_{_{0}}f())^{}, \]

where \(()\) denotes matrix expansion of a \(m n\) matrix \(\) by column vectors:

\[()=[_{1,1},,_{m,1},,_{1,n}, ,_{m,n}]^{}, \]

and \(\) is a \(CP C\) matrix, denoting the Kronecker product of \(C C\) identity matrix \(_{C}\) with \(\), i.e.,

\[=_{C}=[&0& &0\\ 0&&&0\\ &&&\\ 0&0&&]_{CP C}. \]

We use a Jacobian matrix \(^{N CP}\) to denote the partial derivatives of the network for each sample:

\[=[(_{_{0}}f(_{1 }))^{}\\ \\ (_{_{0}}f(_{N}))^{}]. \]

Then, an aggregate form of formula (7) is:

\[=. \]

Now we give a simplified model assumption that there exists an underlying ground truth parameter \(_{*}\) such that corresponding \(_{*}\) generated by equation (13) fits the true label distribution for sample. Meanwhile, there exist potentially true transition matrix \(_{*}\) and sparse residual matrix \(_{*}=[(_{1}),,(_{N})]^{}\) made up of the residual terms \(()\) for sample defined in Section 2.2. We assume that the \(N C\) observed noisy label matrix \(}=[}_{1},,}_{N}]^{}\) is generated by:

\[}=_{*}_{*}+_{*}. \]

Expanded form after bringing in \(\) and \(_{*}\) is:

\[}=(_{C}_{*})_{*}+ _{*}. \]

The problem to be studied is transformed into given \(\) and observed \(}\) generated by formula (15), how to estimate the underlying \(_{*}\), \(_{*}\) and \(_{*}\). At this time, our proposed loss function (4) to be optimized transforms into:

\[(,,,)=L((_{C} )+-,})+(), \]

where \(L\) is matrix form from \(\) in formula (4), \(=[_{1},,_{N}]^{}\), \(=[_{1},,_{N}]^{}\), \(=-\).

Intuitively, the parameters \(,,\) are unidentifiable without other conditions due to the model (15) is over-parameterized. We need to add some conditional assumptions to ensure the convergence of parameters. The required conditions are summarized in the Appendix B.2, such as the low rank condition of \(\), sparsity of \(_{*}\), special small initialization setting, sufficiently scattered assumption  of clean class-posterior probability distribution, etc. Under these conditions, we try to analyze the effectiveness of our algorithm. For the simplicity of proof, we use square loss in formula (16), which can be analogized to cross-entropy loss. The parameter optimization problem (5) becomes:

\[},},},}=*{arg\, min}_{,,,}\|(_{C} )+--}\|_{2}^{2}+(). \]

Based on this, the convergence result of parameters estimation is as follows:

**Theorem 3.1**.: \((\)**Convergence\()\)** _Under the conditions in B.2, the estimated parameters \(}\), \(}\), \(}\) for optimization problem (17) based on Algorithm 1 converge to the ground truth solution \(_{*}\), \(_{*}\), \(_{*}\)._

The proof can be seen in Appendix B.3. Theorem 3.1 shows that under a simplified linear model and some conditions, one can use our proposed algorithm to obtain the consistent estimation of network parameters \(_{*}\) applicable to learning with clean label data. At the same time, we can estimate the overall transition probability \(_{*}\) from the correct label to the noisy label that we observed. Theorem 3.1 provides theoretical support for the effectiveness of our proposed method.

### Generalization Analysis

In addition to convergence, the generalization of the proposed result is also worth exploring. It is finite to the amount of noisy label training data \(}_{(N)}=\{(_{i},_{i})\}_{i=1}^{N}\) we can observe, which is considered to be randomly sampled from the overall infinite noisy data \(}\). We want to explore how well the parameters \(}_{(N)}\), \(}_{(N)}\) estimated by the proposed algorithm with finite data \(}_{(N)}\) fit when applied to the overall data \(}\).

We define a function class about the data as

\[:=\{(^{}f_{}()+(),):^{+}, ^{p},\}, \]

where \(\) is the true residual term for each sample. Each element in \(\) is a function about data sample. It is worth mentioning that the term of \(()\) can be incorporated into the loss function \(\), without explicitly writing it separately for simplicity. Denote the \(\)-cover of \(\) as \(_{}=(,,\|\|_ {})\), the average losses on \(}_{(N)}\) and \(}\) are \((_{(N)},_{(N)},_{(N)};}_{( N)})\) and \((,,;})\) respectively. According to Theorem 3.1, for any fixed \(>0\), there exists estimated parameters \(}_{(N)},}_{(N)},}_{(N)}\) obtained by our algorithm such that:

\[(}_{(N)},}_{(N)},}_{(N)};}_{(N)})(_{(N)},_{(N)},_{(N)}^{};}_{(N)})+,_{ (N)}^{p},_{(N)} \]

where \(_{(N)}^{}\) is the true residual terms for \(}_{(N)}\). If we know the ground truth \(_{*}\), we have the following result:

**Theorem 3.2**.: _Suppose the loss function is bounded by \(0(,) M\). For any \(>0\), then with probability at least \(1-\) we have_

\[(}_{(N)},}_{(N)},_{*}; })_{^{p},} (,,^{*};})+M_{}/)}{2n}}+M_{ })}{2n}}+3. \]

The proof can be found in Appendix B.4, using Theorem 2 in  as a reference. For any fixed \(>0\), as \(n\) continues to increase, the terms \(_{}/)}{2n}}\) and \(}\) on the right side of the inequality (20) tend to \(0\). Since the \(\) can be arbitrarily small, the right side of the inequality (20) can be bounded. Looking back at the optimization target (17), we can find that the Theorem 3.2 states the estimators \(}_{(N)},}_{(N)}\) based on finite data \(}_{(N)}\) can also be applied relatively effectively to wider data \(}\) as long as they are randomly generated from the same pattern. It shows the generalization result of our algorithm, indicating that the estimation \(}_{(N)},}_{(N)}\) can be applied to new data and only the residual terms \(\) need to be estimated separately.

## 4 Experiments

In this section, we present experimental findings to showcase the effectiveness of our proposed method compared to other methods. We evaluate our approach on both synthetic instance-dependent noisy datasets and real-world noisy datasets. More experimental details can be found in the Appendix C.

### Datasets

We conduct experiments on following image classification datasets: CIFAR-10 and CIFAR-100 , CIFAR-10N and CIFAR-100N , Clothing1M , Webvision and ILSVRC12 . Among them, CIFAR-10 and CIFAR-100 both have \(32 32 3\) color images including 50,000 training images and 10,000 test images. CIFAR-10 has 10 classes while CIFAR-100 has 100 classes. We generate instance-dependent noisy data on CIFAR-10 and CIFAR-100 with noise rates ranging from 10% to 50%, following the same generation method as in . CIFAR-10N and CIFAR-100N are manually annotated by human annotators, existing noisy labels within them. Clothing1M is a real-world dataset consisting of 1 million training images, consisting of 14 categories. WebVision contains 2.4 million images crawled from the websites using the 1,000 concepts in ImageNet ILSVRC12, but only the first 50 classes of the Google image subset are used in our experiments. For the validation set selection in our TMR method, we randomly sampled 10 samples from each observed class for each dataset to form the validation set, while the remaining samples were used for the training set.

### Experimental Setup

We conduct the experiments using NVIDIA 3090Ti graphics cards. During the training process, we update the transition matrix using the Adam optimization method, the initialization is consistent with . While the updates for other parameters are performed using the stochastic gradient descent (SGD) optimization method. More specifically, for CIFAR-10/10N, we use ResNet-18 as the backbone network with 300 epochs, batch size 128, learning rate for network is 0.05, 0.0005 for transition matrix and divided by 10 after the 30th and 60th epoch. For CIFAR-100/100N, we use ResNet-34 network with the same 300 epochs, batch size 128, while learning rate for network is 0.05, 0.0002 for transition matrix and divided by 10 after the 30th and 60th epoch. For clothing1M, we use a ResNet-50 pre-trained with 10 epochs, batch size 64, learning rate 0.002 for network, 0.0001 for transition matrix and divided by 10 after the 5th epoch. We use InceptionResNetV2 network on Webvision, with 100 epochs, batch size 32, learning rate 0.02 for network, 0.0005 for transition matrix and divided by 10 after the 30th and 60th epoch. For ILSVRC12, we directly use the model trained on Webvision, following the common setting in other papers in this field.

### Comparison Methods

In our experiments, we included the following commonly used baseline methods for instance-dependent transition matrix estimation and comparison: (1) GCE , (2) Forward , (3) DMI , (4) VolMinNet , (5) PeerLoss  (6) BLTM , (7) PartT , (8) MEIDTM , (9) SOP  as an implicit regularization method for comparison, as well as state-of-the-art methods for comparison purposes: (10) Co-teaching , (11) ELR+ , (12) DivideMix , (13) SOP+ , (14) CC , (15) PGDF , (16) DISC .

### Experimental Results on Synthetic Datasets

We primarily validated our TMR method against previous instance-based transition matrix methods on synthetic CIFAR-10/100 noise datasets. These methods mainly focus on estimating the transition matrix and do not leverage advanced self-supervised or semi-supervised techniques. We performed 5

    &  \\  & IDN-10\% & IDN-20\% & IDN-30\% & IDN-40\% & IDN-50\% \\  CE & 88.86\(\)0.23 & 86.93\(\)0.17 & 82.42\(\)0.44 & 76.68\(\)0.23 & 58.93\(\)1.54 \\ GCE & 90.82\(\)0.05 & 88.89\(\)0.08 & 82.90\(\)0.51 & 74.18\(\)3.10 & 58.93\(\)2.67 \\ Forward & 91.71\(\)0.08 & 89.62\(\)0.14 & 86.93\(\)0.15 & 80.29\(\)0.27 & 65.91\(\)1.22 \\ DMI & 91.43\(\)0.18 & 89.99\(\)0.15 & 86.87\(\)0.34 & 80.74\(\)0.44 & 63.92\(\)3.92 \\ VolMinNet & 89.97\(\)0.57 & 87.01\(\)0.64 & 83.80\(\)0.67 & 79.52\(\)0.83 & 61.90\(\)1.06 \\ PeerLoss & 90.89\(\)0.07 & 89.21\(\)0.63 & 85.70\(\)0.56 & 78.51\(\)1.23 & 59.08\(\)1.05 \\ BLTM & 90.45\(\)0.72 & 88.14\(\)0.66 & 84.55\(\)0.48 & 79.71\(\)0.95 & 63.33\(\)2.75 \\ PartT & 90.32\(\)0.15 & 89.33\(\)0.70 & 85.33\(\)1.86 & 80.59\(\)0.41 & 64.58\(\)2.86 \\ MEIDTM & 92.91\(\)0.07 & 92.26\(\)0.25 & 90.73\(\)0.34 & 85.94\(\)0.92 & 73.77\(\)0.82 \\ SOP & 93.58\(\)0.31 & 93.07\(\)0.45 & 92.42\(\)0.43 & 89.83\(\)0.77 & 82.52\(\)0.97 \\ TMR & **94.45\(\)0.17** & **93.90\(\)0.21** & **93.14\(\)0.20** & **91.82\(\)0.31** & **87.04\(\)0.42** \\    &  \\  & IDN-10\% & IDN-20\% & IDN-30\% & IDN-40\% & IDN-50\% \\  CE & 66.55\(\)0.23 & 63.94\(\)0.51 & 61.97\(\)1.16 & 58.70\(\)0.56 & 56.63\(\)0.69 \\ GCE & 69.18\(\)0.14 & 68.35\(\)0.33 & 66.35\(\)0.13 & 62.09\(\)0.09 & 56.68\(\)0.75 \\ Forward & 67.81\(\)0.48 & 67.23\(\)0.29 & 65.42\(\)0.63 & 62.18\(\)0.26 & 58.61\(\)0.44 \\ DMI & 67.06\(\)0.46 & 64.72\(\)0.64 & 62.80\(\)1.46 & 60.24\(\)0.63 & 56.52\(\)1.18 \\ VolMinNet & 67.78\(\)0.62 & 66.13\(\)0.47 & 61.08\(\)0.90 & 57.35\(\)0.83 & 52.60\(\)1.31 \\ PeerLoss & 65.64\(\)1.07 & 63.83\(\)0.48 & 61.64\(\)0.67 & 58.30\(\)0.80 & 55.41\(\)0.28 \\ BLTM & 68.42\(\)0.42 & 66.62\(\)0.85 & 64.72\(\)0.64 & 59.38\(\)0.65 & 55.68\(\)1.43 \\ PartT & 67.33\(\)0.33 & 65.33\(\)0.59 & 64.56\(\)1.55 & 59.73\(\)0.76 & 56.80\(\)1.32 \\ MEIDTM & 69.88\(\)0.45 & 69.16\(\)0.16 & 66.76\(\)0.30 & 63.46\(\)0.48 & 59.18\(\)0.16 \\ SOP & 74.09\(\)0.52 & 73.13\(\)0.46 & 72.14\(\)0.46 & 68.98\(\)0.58 & 64.24\(\)0.86 \\ TMR & **76.96\(\)0.25** & **75.94\(\)0.32** & **74.87\(\)0.45** & **72.56\(\)0.60** & **69.85\(\)0.56** \\   

Table 1: Test accuracy with instance-dependent noise on CIFAR-10/100.

independent runs for each experimental configuration, and the average values and standard deviations of each experiment are presented in Table 1.

The results demonstrate that our proposed TMR method outperforms other methods of the same category across various noise rates. It is evident that traditional transition matrix methods such as Forward and VolMinNet exhibit subpar performance when handling instance-dependent noise. On the other hand, specialized transition matrix methods designed for instance-dependent noise, such as ParT and MEIDTM, still show significant gaps compared to our method.

Furthermore, as the noise rates increase, the test accuracy of existing transition matrix methods significantly decline. This is particularly pronounced in the case of CIFAR-100 with 50% instance-dependent noise (IDN) data, where all transition matrix methods achieve test accuracy below 60%. In contrast, our proposed TMR method achieves a remarkable test accuracy of 69.85%, showcasing its exceptional performance. That demonstrates relatively robust performance of TMR with only a slight decrease as the noise rate increases.

It is worth mentioning that SOP , as a method that also applies implicit regularization based on sparsity assumptions, achieves comparable performance to our method when the noise rates are low. However, it still falls short of our method's performance. As the noise rate increases, SOP is more adversely affected by the noise due to its reliance on the sparsity assumption. In contrast, our proposed TMR method effectively estimates the overall trend by utilizing the transition matrix and combines it with sparsity, thereby demonstrating robustness even in the presence of higher noise rates. For instance, on CIFAR-10/100 with a 10% noise rate, TMR outperforms SOP by 0.87 and 2.87 percentage points, respectively. When the noise rate increases to 50%, TMR surpasses SOP by 4.52 and 5.61 percentage points, respectively. This clearly demonstrates the general effectiveness of our method in handling label noise learning across various noise rates.

### Experimental Results on Real-world Datasets

In addition to comparing with transition matrix methods, we also enhanced our method, TMR, by incorporating SimCLR for feature learning, as TMR+. We compared TMR+ with other state-of-the-art methods on multiple real-world noisy datasets, and the results are presented in Table 2 and Table 3.

    &  &  \\  & Aggregate & Random 1 & Random 2 & Random 3 & Worst & Noisy \\  CE & 87.77\(\)0.38 & 85.02\(\)0.65 & 86.46\(\)1.79 & 85.16\(\)0.61 & 77.69\(\)1.55 & 50.50\(\)0.66 \\ Forward & 88.24\(\)0.22 & 86.88\(\)0.50 & 86.14\(\)0.21 & 87.04\(\)0.35 & 79.49\(\)0.46 & 57.01\(\)1.03 \\ Co-teaching & 91.20\(\)0.13 & 90.33\(\)0.13 & 90.30\(\)0.17 & 90.15\(\)0.18 & 83.83\(\)0.13 & 60.37\(\)0.27 \\ ELR+ & 94.83\(\)0.10 & 94.43\(\)0.41 & 94.20\(\)0.24 & 94.34\(\)0.22 & 91.09\(\)1.60 & 66.72\(\)0.07 \\ DivideMix & 95.01\(\)0.71 & 95.16\(\)0.19 & 94.89\(\)0.23 & 95.03\(\)0.20 & 92.56\(\)0.42 & 71.13\(\)0.48 \\ SOP+ & 95.61\(\)0.13 & 95.28\(\)0.13 & 95.31\(\)0.10 & 95.39\(\)0.11 & 93.24\(\)0.21 & 67.81\(\)0.23 \\ PGDF & 95.35\(\)0.12 & 94.95\(\)0.21 & 94.78\(\)0.34 & 94.92\(\)0.28 & 94.22\(\)0.29 & 67.76\(\)0.35 \\  TMR+ & **96.06\(\)0.21** & **95.96\(\)0.17** & **95.74\(\)0.31** & **95.88\(\)0.14** & **94.91\(\)0.22** & **70.31\(\)0.28** \\   

Table 2: Test accuracy on CIFAR-10N and CIFAR-100N.

    & Clothing1M & Webvision & ILSVRC12 \\  CE & 69.1 & - & - \\ Forward & 69.8 & 61.1 & 57.3 \\ Co-teaching & 69.2 & 63.6 & 61.5 \\ ELR+ & 74.81 & 77.78 & 70.29 \\ DivideMix & 74.76 & 77.32 & 75.20 \\ SOP+ & 74.98 & 77.60 & 75.29 \\ CC & 75.40 & 79.36 & 76.08 \\ PGDF & 75.19 & 81.47 & 75.45 \\ DISC & 73.72 & 80.28 & 77.44 \\  TMR+ & **75.42** & **82.06** & **77.65** \\   

Table 3: Test accuracy on Clothing1M, Webvision and ILSVRC12.

The results demonstrate that regardless of the type of noise labels, whether it is aggregated, random, or the worst-case scenario in CIFAR-10N, as well as in CIFAR-100N with more label categories, our method consistently achieves the best results in handling real-world noise. When dealing with large datasets like Clothing1M and complex image datasets like Webvision, TMR+ also achieves excellent results compared to to other SOTA methods like CC, PGDF and DISC.

Through extensive experiments on five real-world datasets, we demonstrate that our TMR method can significantly benefit from combining with self-supervised methods such as contrastive learning, indicating that high-quality features can greatly enhance our original TMR method. TMR is a plug-and-play model, where the feature extraction part can be unrelated to TMR itself and be replaced with other similar methods without requiring additional special handling.

### Ablation Study

Besides the aforementioned experiments, we conducted ablation studies on proposed TMR method to assess the importance of each component. Table 4 presents the comparative results under 20% and 40% instance-dependent noise rates, where "w/o" denotes "without", "TM" represents the transition matrix, and "IR" the represents implicit regularization. From the results, it can be observed that the absence of either IR or TM significantly affects the performance of our TMR method. Removing IR has a greater impact, particularly in the case of instance-dependent noise, resulting in a substantial decrease compared to TMR. While removing TM yields similar results on CIFAR-10 with a 20% noise rate, the difference becomes apparent when the noise rate increases to 40% or when applied to more complex datasets like CIFAR-100. These results indicate that both the transition matrix and implicit regularization term are crucial components in our model, highlighting the innovation of combining these two aspects in our method.

## 5 Conclusion

We propose an extended model for transition matrix that firstly combines it with sparse implicit regularization, enabling the extension of transition matrix methods from class-dependent noise to a broader range of noise scenarios while maintaining the simplicity of the model. The effectiveness of our method is theoretically analyzed under certain assumptions and validated through experiments on various noisy datasets. Additionally, our method can be enhanced by combining with pre-trained feature extractor such as contrastive learning, achieving state-of-the-art performance.