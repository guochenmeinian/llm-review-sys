# Optimization Can Learn Johnson Lindenstrauss Embeddings

Nikos Tsikouras

UOA & Archimedes / Athena RC

n.tsikouras@athenarc.gr

Constantine Caramanis

UT Austin & Archimedes / Athena RC

constantine@utexas.edu

Christos Tzamos

UOA & Archimedes / Athena RC

christos@tzamos.com

###### Abstract

Embeddings play a pivotal role across various disciplines, offering compact representations of complex data structures. Randomized methods like Johnson-Lindenstrauss (JL) provide state-of-the-art and essentially unimprovable theoretical guarantees for achieving such representations. These guarantees are worst-case and in particular, neither the analysis, _nor the algorithm_, takes into account any potential structural information of the data. The natural question is: must we randomize? Could we instead use an optimization-based approach, working directly with the data? A first answer is no: as we show, the distance-preserving objective of JL has a non-convex landscape over the space of projection matrices, with many bad stationary points. But this is not the final answer.

We present a novel method motivated by diffusion models, that circumvents this fundamental challenge: rather than performing optimization directly over the space of projection matrices, we use optimization over the larger space of _random solution samplers_, gradually reducing the variance of the sampler. We show that by moving through this larger space, our objective converges to a deterministic (zero variance) solution, avoiding bad stationary points.

This method can also be seen as an optimization-based derandomization approach and is an idea and method that we believe can be applied to many other problems.

## 1 Introduction

Embeddings are foundational across diverse disciplines, offering compact representations of complex data structures. Algorithms across different domains leverage embeddings to capture nuanced relationships between data points, improving efficiency and effectiveness in processing. Within this field, there are distinct lines of work.

Embeddings have been used as dimensionality-reducing tools, while preserving important structures in data. They have been a major research focus for years, and have been a key ingredient in several algorithmic applications such as graph sparsification (Spielman and Srivastava, 2008), nearest-neighbor search (Indyk and Motwani, 1998), hashing (Dietzfelbinger et al., 1997) and digital research (Schmidt, 2018). A celebrated result in this area has been the Johnson-Lindenstrauss (JL) lemma Johnson (1984) which shows that a random linear mapping can reduce the dimension of the dataset while approximately preserving the \(L_{2}\) norm of all points, with high probability. JL embeddings give bounds on the _maximum distortion_ over all the points. Many variants have been developed and studied, as we discuss below in Section 2. For this paper, the salient point is that the algorithms thatenjoy theoretical guarantees are for random projections drawn from a distribution over projection matrices. To the best of our knowledge, even among derandomization of JL-type techniques (e.g., Clarkson and Woodruff (2009), and see Section 2 for further discussion), there are no guarantees for optimization-based algorithms that directly attempt to minimize a distortion objective.

On the other hand, in the last decade, embeddings have risen to prominence across various tasks in deep learning. In encoder-decoder architectures, embeddings act as intermediary representations to solve a broad range of challenges in natural language processing and speech processing, (Bengio and Heigold, 2014; Cho et al., 2014, 2014, 2018; Rush et al., 2015; Kiong et al., 2016). In the domain of face recognition, embeddings encode facial features into concise vectors, enabling precise identification and matching (Liu et al., 2015, 2017; Schroff et al., 2015). Furthermore, contrastive learning techniques utilize embeddings to emphasize disparities between similar and dissimilar instances, thereby augmenting the discriminative capabilities of models (Gao et al., 2021; Khosla et al., 2020; Radford et al., 2021).

In contrast to the JL-type results, in the above applications these embeddings are learned using optimization as part of the (pre)training process (Caron et al., 2021; Oquab et al., 2023; Press and Wolf, 2016; Vaswani et al., 2017). Though their empirical success is remarkable, the non-convex landscape of the optimization process makes obtaining theoretical guarantees a key challenge.

Another important related area is the extensive body of literature dedicated to employing optimization techniques for Principal Component Analysis (PCA). PCA aims to find linear embeddings that capture the optimal _average distortion_ in the data, thereby reducing its dimensionality while preserving as much information as possible. The last decade saw significant success of direct matrix optimization in various PCA-like settings (Garber and Hazan, 2015; Shamir, 2016; Xu and Li, 2021; Yi et al., 2016). Though clearly there are similarities to the JL objective, the difference between guarantees on the maximum perturbation (JL) vs the average perturbation (PCA) are significant, and one reason why the techniques pioneered for PCA have yet to be applied successfully to JL.

Nevertheless, the empirical success of optimization in neural networks, and its theoretical success for PCA-type objectives, motivate us to revisit the JL low-distortion embedding task (see Section 3 for the exact definition of the _JL guarantee_), and ask the question that to the best of our knowledge, has yet to find an answer:

_Can optimization-based approaches be used to obtain a matrix that satisfies the Johnson-Lindenstrauss guarantee?_

Answering this question is the main goal of this paper.

**Our Contributions and a Conceptual Road Map.**

The main contribution of this paper is in developing a framework that allows a direct (and deterministic) optimization approach to obtain the same JL guarantees as random projection. After all, it is well-known that the JL guarantees given by random projection are not improvable (Larsen and Nelson, 2014, 2017; Alon and Klartag, 2017). Moreover, established derandomization techniques based on conditional expectation and other methods have long been available (Raghavan, 1988; Engebretsen et al., 2002; Bhargava and Kosaraju, 2005).

The key conceptual steps on the way to our main result are as follows:

**Step 1:** We first show that the optimization landscape in the ambient space of projection matrices is not favorable, and in particular, any attempt to directly minimize distortion over this space using first or second-order methods, is destined to fail.

**Theorem** (Informal version of Theorem 1).: _The maximum distortion objective considered as a function in the space of matrices has many suboptimal local minima._

**Step 2:** Given the above negative result, a different approach is required. We draw inspiration from diffusion models and solution samplers (Bello et al., 2016; Ho et al., 2020). Rather than optimize in the space of matrices, we optimize in the larger space of (mean, variance) parameters of Gaussian distributions over embedding matrices. Thanks to the original JL theorem, we know an initial choice of parameters that define a solution sampler whose expected distortion is small: zero mean and unit variance. Akin to diffusion, we then seek to sequentially decrease the magnitude of the variance, without increasing the expected distortion of the sampler. Note that the space of matrices is properly contained in this space of samplers, as we can identify a specific (deterministic) projection matrix with a Gaussian distribution with that mean, and zero variance. The challenge is to find a path from our initial JL sampler, to a deterministic sampler (a projection matrix) whose maximum distortion is approximately as good as the expected guarantee of the original JL sampler.

We turn this into an optimization problem by creating an objective function in the space of samplers. Our first result demonstrates that if we find a second-order stationary point for this objective function, then we have solved our original problem. Specifically:

**Theorem** (Informal Version of Theorem 2).: _For data \(x_{1},,x_{n}^{d}\) and target dimension \(k\) all second-order stationary points reachable from the origin for the objective function defined in Equation 4 have zero variance and hence correspond to fixed matrices; moreover, these matrices satisfy the JL guarantee._

**Step 3**: The final step requires proving the tractability of finding a second-order stationary point, i.e., of solving this optimization problem. We do so _using a generic deterministic second-order optimization algorithm_ (see Alg. 1). Thus our result shows that our second-order optimization algorithm successively performs reverse-diffusion-like steps, decreasing the variance without deteriorating the quality of the solution sampler, until it has finally arrived at a deterministic solution.

**Theorem** (Informal Version of Theorem 3).: _For data \(x_{1},,x_{n}^{d}\) and target dimension \(k\) running Algorithm 1 using Equation 4 for \((n,k,d)\) steps returns a matrix that satisfies the JL guarantee._

**Step 4**: Finally, we show through simulations that the qualitative and quantitative results of our theory are borne out.

## 2 Related Work and Alternative Approaches

The JL lemma is a well-studied result studied in the literature, with several simplifications and extensions of the original proof (Dasgupta and Gupta, 2003; Frankl and Maehara, 1988; Kane and Nelson, 2010; Matousek, 2008). It has also been shown that the JL lemma is optimal in terms of the target dimension. The authors in (Larsen and Nelson, 2017; Alon and Klartag, 2017) provide a tight lower bound on the target dimension required by the JL lemma, given a specific distortion, for any random linear mapping. In addition to these theoretical insights, there have been various approaches aimed at efficiently constructing random matrices that satisfy the JL guarantee with high probability.

One approach samples each matrix entry independently from a Gaussian distribution (Indyk and Motwani, 1998), while others utilize Rademacher and sparse Rademacher distributions (Arriaga and Vempala, 2006; Achlioptas, 2001). Moreover, generalized sampling methods have shown that any distribution with zero mean, unit variance, and a subgaussian tail can be used (Matousek, 2008). The Fast Johnson-Lindenstrauss Transform employs sparse matrices and randomised Walsh-Hadamard transforms for efficiency (Ailon and Chazelle, 2009). Additionally, the subsampled randomised Hadamard transform, achieves efficient embeddings by combining subsampling with randomised Hadamard transforms, maintaining a high probability of preserving the distances between points (Ailon and Liberty, 2013).

**Derandomizing Johnson-Lindenstrauss.** Derandomization is a technique for developing deterministic algorithms or algorithms that require fewer random bits, and it has proven to be a powerful theoretical tool (Kabanets, 2002). There have been numerous efforts to derandomize the JL lemma, with a significant focus on using pseudorandom generators (PRGs) capable of fooling statistical tests (Nisan, 1990). These constructions aim to achieve reduced seed lengths while satisfying the JL lemma's norm-preserving properties with \(\) distortion and probability of failure \(\). For example, the \(_{2}\)-streaming algorithm achieves a JL family with seed length \(O( d)\) and with \(k=O(1/(^{2}))\)(Alon et al., 1996). The authors in (Clarkson and Woodruff, 2009) leveraged the use of scaled random Bernoulli matrices with \(((1/))\)-wise independent entries, resulting in a seed length of \(O((1/) d)\). Additionally, PRGs that \(\)-fool degree-2 polynomial threshold functions generate JL families with seed lengths of \((1/) d\)(Meka and Zuckerman, 2010).

Other approaches have utilized conditional probabilities and pessimistic estimators, introduced in (Raghavan, 1988), to derive deterministic algorithms for JL embeddings (Engebretsen et al., 2002; Bhargava and Kosaraju, 2005).

These works differ from ours in several ways. The Nisan pseudorandom generator uses a few random bits. Additionally, the authors in  fully derandomize the Rademacher construction by  using a combinatorial algorithm that greedily selects the best matrix entries. Even in such a coordinate-wise fashion, using a gradient-descent (continuous-optimization) approach is challenging, as even then the optimization landscape is bimodal.

Overall, the key difference in philosophy, setting and ultimately results, comes from our focus on optimization: our method is a study in the power of local (first and second-order) optimization methods.

## 3 Preliminaries and Notation

In this section, we introduce essential definitions and notation for our work. We consider without loss of generality unit norm data points \(x_{1},,x_{n}^{d}\), which we aim to project into \(k\) dimensions while preserving their norms with distortion at most \(=O()\). Specifically, we seek matrices that satisfy the _Johnson-Lindenstrauss guarantee_:

**Definition 1** (Johnson-Lindenstrauss guarantee).: _The Johnson-Lindenstrauss guarantee (JL guarantee) states that for given dataset \(x_{1},,x_{n}^{d}\) and target dimension \(k\), the distortion for all points does not exceed \(O()\)._

To achieve this we define a linear mapping \(f(x)=Ax\), where \(A^{k d}\). The JL Lemma guarantees that there exists a random linear mapping that achieves this projection with high probability:

**Lemma 1** (Distributional Johnson-Lindenstrauss Lemma).: _For \(,(0,1)\) and \(k=O((1/)/^{2})\), there exists a probability distribution \(D\) over linear functions \(f:^{d}^{k}\) such that for every \(x^{d}\):_

\[_{f D}(\|f(x)\|_{2}^{2}[(1-)\|x\|_{2}^{2},(1+ )\|x\|_{2}^{2}]) 1-.\]

There has been significant research aimed at improving the construction of these random mappings. In contrast to traditional algorithms, our approach proposes learning the linear mapping directly from the data, leveraging the inherent structure to surpass worst-case performance.

Next, we give essential definitions for our optimization framework.

**Definition 2**.: _A function \(f:^{d}\) is defined to be \(L\)-smooth, if for all \(x,y^{d}\) it satisfies:_

\[\| f(x)- f(y)\|_{2} L\|x-y\|_{2}.\]

_A function is called \(K-\)Hessian Lipschitz if for all \(x,y^{d}\):_

\[\|^{2}f(x)-^{2}f(y)\|_{2} K\|x-y\|_{2}.\]

Below, we give the definition for approximate stationarity.

**Definition 3**.: _(Approximate stationarity). For a \(K-\)Hessian Lipschitz function \(f()\), we say that a point \(x^{*}\) is a \(-\)second-order stationary point (\(\)-SOSP) if:_

\[\| f(x^{*})\|_{2}\ \ \ \ _{}(^{2}f(x^{*}))-.\]

**Notation.** For vectors \(u,v\) we use \( u,v\) to denote their inner product and \(\|u\|_{2}\) to denote the \(L_{2}\) norm. For matrix \(^{k d}\), we denote the element of the \(i^{th}\) row and \(j^{th}\) column by \(_{i,j}\) and we use \(\|\|_{F}\) to denote the Frobenius norm. For matrix \(^{k d}\) and \(^{2}^{+}\) we use \(N(,^{2})\) to denote an \(k d\) random Gaussian matrix where each element \(a_{i,j} N(_{i,j},^{2})\). We use \( f\) and \(^{2}f\) to denote the gradient and Hessian operators, respectively.

The Main Results

This section contains the full statement of our main theorems, and proof outlines. We organize the flow of this section according to our conceptual outline given in the introduction. In most cases, we defer the full proofs to the appendix.

Our goal is to find a matrix that satisfies the _Johnson-Lindenstrauss guarantee_ as given in Lemma 1. Consider the natural objective function of maximum distortion:

\[h(A)=_{x_{1},,x_{n}}\|Ax\|_{2}^{2}-1.\] (1)

**Step 1**: The first step tells us what will not work. In particular, direct optimization over the space of matrices cannot work. Our first result shows that minimizing this maximum distortion objective via a first or second-order method, is a doomed approach. In particular, we show that there exist instances which are bad local minima.

**Theorem 1**.: _For all \(k>1\), there exists a family of matrices \(A^{k k+1}\) which are strict local minima for the objective function of Equation 1 reachable from the origin. The achieved distortion is \((1)\) over a set of \(O(k^{2})\) points, while there exist matrices yielding distortion \(O() 0\)._

The proof of this is constructive. We construct a dataset and then show that a set of matrices reachable from the origin have large constant distortion, yet these points are locally unimprovable. The full proof can be found in Appendix A.1 

**Step 2**: The key idea towards our final result is to perform an optimization over an extended space: the space of parameters of random Gaussian solution samplers. We first define an optimization objective over this space, and then prove properties about the resulting landscape over the space of samplers.

A Gaussian solution sampler is defined by its mean and variance. We only consider the case where all entries have the same variance. Thus, our new parameter space consists of pairs \((,^{2})\), where \(\) is a projection matrix, here interpreted as the mean of a Gaussian distribution, and \(^{2}\) is the variance parameter. Given \((,^{2})\), the solution sampler defined is simply: \(A N(,^{2})\).

We note that our new parameter space has just one additional parameter than the ambient setting.

**Step 2A**: We next must extend the maximum distortion objective above, to the space of random solution samplers we have defined. Consider the objective \(f^{*}\), defined as follows:

\[f^{*}(,^{2})=_{A N(,^{2})}[h(A) >].\] (2)

Thus, \(f^{*}(,^{2})\) is the probability that a matrix sampled according to the corresponding Gaussian distribution will have maximum distortion at least \(\). When we take \(=O()\), our objective function \(f^{*}\) gives us the probability that a Gaussian solution sampler fails to produce a matrix that meets the JL guarantee. Hence, a good sampler is one that makes \(f^{*}\) small.

We note that in the proof of the JL lemma in (Indyk and Motwani, 1998), the authors show that a matrix with Gaussian entries satisfies the JL guarantee with high probability. Thus, in particular, we know that taking \(=\), where \(\) is a \(k d\) zero matrix, and \(^{2}=1\), gives a low objective value for \(f^{*}\).

In the context of these definitions, therefore, our goal is to find a matrix \(}\) such that \(f^{*}(},0)\) has a low objective value. To do this, we now define a related objective value, which thanks to a regularization term, will allow the optimization algorithm to push us towards lower variance solutions. The technical challenge is then to show that we can control any deterioration of the JL guarantee of these lower variance solutions.

We define our final objective function starting from \(f^{*}\) defined above. First, we simplify the objective by applying a standard union bound and write a relaxed objective that sums for every point the probability that the point will have a norm outside the required bounds after the linear transformation.

\[f(,^{2})=_{j=1}^{n}_{A N(,^{2})}[\|Ax_{j}\|_{2}^{2}(1-,1+)].\] (3)

For an appropriately chosen value of \(=O()\), we have that no constraint is violated with probability greater than \(1/(3n)\) and \(f(,1)<1/3\). The function \(f\) serves as an upper bound on the probability of generating a matrix that does not have the JL guarantee, effectively acting as a proxy for "bad" events. Next, we add an appropriate regularization term that penalizes high variance points. Our overall objective is thus:

\[g(,^{2})=f(,^{2})+^{2}/2.\] (4)

At our initialization point \(=\) and \(^{2}=1\), the value of the regularization is \(1/2\), thus: \(g(,1)<1/3+1/2<5/6\). This is crucial, as it implies that following any decreasing path in \(g\) leads to points with a likelihood of a bad event being less than 1. Consequently, this convergence toward a solution sampler maintains a positive (and \(O(1)\)) probability of achieving a projection matrix that satisfies the JL guarantee. The next step provides our algorithm. After that, we characterize its fixed points.

**Step 2B**: Algorithm 1 is a second-order descent algorithm consisting of two simple steps: At a given point \(x_{t}=(_{t},_{t})\), if the gradient is sufficiently large, we take a gradient step. If the gradient is small, we consider the Hessian; if the smallest eigenvalue is sufficiently negative, we take a step in that direction of negative curvature; otherwise the algorithm terminates by reporting _the mean parameter_\(_{t}\) (see Lemma 4 for discussion on this final point). To prove correctness of the algorithm, we must show that any \(\)-SOSP will have sufficiently small variance. The proof of correctness is given in **Step 2C**, and a bound on its running time in **Step 3**. We can call this algorithm recursively, to find the best distortion, using a simple routine given in Algorithm 2.

We note that in principle, many first-order methods can also be used, for example, Perturbed Gradient Descent (PGD) which has been shown to converge to second-order stationary points fast . We use a _deterministic algorithm_ in our analysis to enable a straightforward derandomization of the JL lemma through the optimization of Equation 4.

```
1:\( g,^{2}g,=,h=}{K},L,K,,_ {}=,_{}^{2}=1\)
2:\(t 0\)
3:\(x_{t}(_{},_{}^{2})\)
4:while true do
5:if\(\| g(x_{t})\|>\)then
6:\(x_{t+1} x_{t}- g(x_{t})\)
7:elseif\(\| g(x_{t})\|_{2}\) and \(_{}(^{2}g(x_{t}))<-\)then
8:\(u_{1}\) the eigenvector corresponding to \(_{}(^{2}g(x_{t}))\)
9:\(x_{t+1} x_{t}+hu_{1}\)
10:else
11:return\(x_{t}=M_{t}\)
12:endif
13:\(t t+1\)
14:endwhile ```

**Algorithm 1** Hessian Descent.

**Step 2C**: Since we initialize at a good solution sampler and we use a descent algorithm on our objective, we know that we can never move to a bad sampler. But that is not enough for us. For we recall that our goal is not to find a good randomized algorithm, but rather to find a good (deterministic) JL matrix, via optimization. Thus we must show that we do not get trapped in any points that have non-zero variance.

We accomplish this in several lemmas. First in Lemma 2 we show that stationary points must have zero variance. We then refine this in Lemma 3 and show that being in a \(\)-second-order stationary point requires the variance to be very small. We need this in order to show we can escape from any point with sufficiently large variance. Finally, in Theorem 2 we show that our second-order algorithm will not get stuck at any point with large variance, and that once we are at a solution sampler with small enough variance, the mean parameter itself will enjoy (deterministically) the JL guarantee.

In the following lemma, we show that points with non-zero variance cannot be local minima. Specifically, we show that for any given mean matrix and variance, there exists a nearby mean matrix with reduced variance that improves the objective value.

**Lemma 2**.: _Let \(^{k d}\), and \(>0\). Then for any \([0,]\), there exists \(^{}^{k d}\) such that:_

* \(\|-^{}\|_{F} 2}{ })},\)__
* \(g(^{},^{2}-^{2}) g(,^{2})-^{2}/6\)_._

**Proof Sketch:** The proof of the lemma essentially is a small derandomization step, where we show that by taking a sufficiently small variance-reducing step, even if we deteriorate the JL guarantee (i.e., the function \(f\) increases), the decrease in the regularizer outweighs this increase, thereby decreasing the overall value of the objective, thus showing we could not have been at a local minimum.

More specifically, the proof proceeds as follows. We begin with a Gaussian matrix \(A N(,^{2})\) and use the additivity property to partition it: \(A=A^{}+A^{}\), where \(A^{} N(,^{2})\) and \(A^{} N(,^{2}-^{2})\), representing small additive noise and the remainder of \(A\), respectively.

The core idea is to derandomize \(A^{}\) to achieve a decreased objective value. We extend the definition of a bad event by constraining \(A^{}\) to take values only within a specific range \(R\). Within \(R\), there must exist a realization of \(A^{}\), denoted as \(^{}\), which at worst, only slightly increases the probability of failure due to the truncation of the Gaussian distribution tails. By choosing this specific value \(^{}\), we effectively derandomize \(A^{}\). As we show in the appendix, this can result in an only slightly increased probability of a bad event.

We then define \(^{}=^{}+\) and show that the regularization term ensures an overall decrease in the objective function. The full proof can be found in Appendix A.2. 

In Lemma 2, we established that any point with non-zero variance cannot be a local minimum, as there always exists a nearby point with lower objective value. The next lemma addresses whether a descent direction can be found at each step. We prove that this is indeed the case, specifically demonstrating that the \(\)-second-order stationary points of the objective function in Equation 4 correspond to points with approximately zero variance.

**Lemma 3**.: _Consider \(x_{1},,x_{n}^{d}\). Given target dimension \(k\) choose \(=O()\). The \(\)-second-order stationary points of the objective function in Eq. 4 implies \(^{2}<(n,k,d)^{O(1)}\)._

**Proof Sketch:** We establish the lemma by examining the behavior of the variance \(^{2}\) at points approaching \(\)-second-order stationarity under the objective function defined in Equation 4. While \(^{2}\) is large we employ a series of incremental reductions, and we show we can continue in this manner until \(^{2}\) is reduced at least to the claimed level.

Using Taylor's theorem and the Lipschitzness of \(^{2}g\), we prove that at any point \((,^{2})\), either the gradient will be large and thus progress will be made using first-order methods, or that the minimum eigenvalue of the Hessian will be negative and thus we can follow that direction to make progress. We then show that convergence to \(\)-second-order stationary points gives us the desired result. Controlling the effect of the Lipschitz constant is a main challenge. The full proof can be found in Appendix A.3. 

**Step 2D**: Since our Algorithm 1 finds an approximate \(\)-SOSP, we need an additional result that gives us a stopping criterion once the variance is small enough, and simply use the mean with controlled deterioration of the JL guarantee. That is, instead of sampling from \(A N(,^{2})\), we can directly use \(\) instead. This is why in line 10 of Algorithm 1, we simply return the parameter \(_{t}\).

**Lemma 4**.: _Given \(n\) unit vectors in \(^{d}\) and a target dimension \(k\), choose \(=O()\) such that distribution \(A N(,^{2})\) satisfies the JL guarantee with distortion \(\) with probability \(1/6\). Then using matrix \(\) instead of sampling from \(A\) retains the JL guarantee with a threshold increased by at most \((,1/k)\)._

**Proof Sketch:** By assumption, \(A N(,^{2})\) satisfies \(1/k\|Ax\|_{2}^{2}(1-,1+)\) with probability at least \(1/6\). Next, we decompose \(A\) as \(A=+Z\) with \(Z N(,^{2})\), and invoking the JL lemma, we choose \(_{0}\) such that \(1/k\|Zx\|_{2}^{2}[^{2}(1-_{0}),^{2}(1+_ {0})]\) with probability at least \(6/7\). This choice ensures that there exists a region in the space such that the JL guarantee holds simultaneously for both \(A\) and \(Z\). Our goal is to establish a bound on the distortion when using \(\) instead of sampling from \(A\). We show that \(\|x\|_{2}^{2}\) can be bounded with terms involving only \(\|Ax\|_{2}^{2}\) and \(\|Zx\|_{2}^{2}\) and derive an upper and lower bound on the distortion that must hold deterministically. The full proof can be found in Appendix A.4. 

Putting the above results together, we have our first main result that shows the correctness of Algorithm 1: it will not get trapped at any point with a large variance, and once it does finally arrive at a point of small enough variance, the mean parameter satisfies the JL property.

**Theorem 2**.: _Given \(n\) unit vectors in \(^{d}\) and a target dimension \(k\), consider the corresponding function \(g\) of Equation 4 for any \( C\) where \(C\) is a sufficiently large constant. Any \(\)-SOSP, that is a pair of parameters \((,^{2})\), of \(g\) reachable from the origin satisfies \(^{2}<(n,k,d)^{O(1)}\) and goes to 0 as \( 0\). Moreover, when \(<1/(n,k,d)\), \(\) satisfies the JL guarantee having distortion at most \(O()\)._

Proof.: We choose the parameter \(C\) such that \(g(,1)<5/6\). Using Lemmas 2 and 3 we find that a \(\)-SOSP of \(g\) is a pair of parameters \((,^{2})\), with \(^{2}<(n,k,d)^{O(1)}\) and \(g(,^{2})<g(,1)\). This implies that drawing sample from \(A N(,^{2})\) satisfies the JL guarantee with distortion at most \(\) with probability \(1/6\).

Then, from Lemma 4, using the matrix \(\) satisfies the JL guarantee with distortion \(O()\). 

**Step 3**: The above result proves correctness of Algorithm 1, but is qualitative: we have not proved how many steps are required. Below we give a quantitative result proving that we can minimize our objective function efficiently and learn a deterministic JL embedding in polynomial time, while incurring a minor increase in the distortion. Though as mentioned, we see the main contribution of our work as centering on the optimization formulation, we note that this theorem constitutes a novel approach to derandomizing the Gaussian JL transformation.

**Theorem 3**.: _Given \(n\) unit vectors in \(^{d}\) and a target dimension \(k\), consider any \( C\) where \(C\) is a sufficiently large constant. Then, running Algorithm 1 to deterministically optimize the objective function \(g\) of Equation 4 for \((n,k,d)\) steps returns a matrix \(\) that satisfies the JL guarantee with distortion at most \(O()\)._

Proof.: The first part of this theorem relies on two auxiliary results:

**Lemma 5** (Sufficient Descent for Gradient Descent).: _If \(\| g(x_{t})\|_{2}>\) and \(L\) the smoothness of \(g\), then for \(=\) and \(x_{t+1}=x_{t}- g(x_{t})\), we have \(g(x_{t+1}) g(x_{t})-}{2}\)._

**Lemma 6** (Sufficient Descent for Negative Curvature Descent).: _If \(\| g(x_{t})\|_{2}\) and \(_{}(^{2}g(x_{t}))<-\), and \(K\) the Hessian Lipschitz parameter, then for \(h=}{K}\) and \(x_{t+1}=x_{t}+hu_{1}\), where \(u_{1}\) corresponds to the eigenvector of the minimum eigenvalue, we have \(g(x_{t+1}) g(x_{t})-}{4}\)._The proofs of both lemmas are in Appendices A.5, A.6, respectively. Given these two results, now standard optimization techniques show that Algorithm 1 finds a \(\)-SOSP in \(O(1/^{1.5})\) steps.

According to Theorem 2, choosing \(<1/(n,k,d)\), implies that a \(\)-SOSP for \(g\) is a pair of parameters \((,^{2})\) with \(\) satisfying the JL guarantee with distortion at most \(O()\). Therefore, running Algorithm 1 for \((n,k,d)\) steps returns a matrix \(\) which satisfies the JL guarantee with distortion at most \(O()\). 

## 5 Simulations

We empirically validate our theoretical results, demonstrating that first and second-order information suffices to learn high-quality embeddings. Our goal is to identify an optimal matrix \(\) that produces embeddings satisfying the JL guarantee (see Definition 1) while minimizing distortion. We show that our method generates embeddings with significantly lower distortion compared to those obtained using the Gaussian construction of the JL lemma.

We generate a unit norm dataset with \(n=100\) data points in \(d=500\) dimensions, and target dimension \(k=30\) dimensions. We aim to minimize the expected maximum distortion \(E_{A N(,^{2})}[h(A)]\) (see Equation 1). We employ the first-order optimization algorithm Adam (Kingma and Ba, 2014) over \(5000\) iterations and compare our method against the average and minimum distortions over 1000 trials using the baseline matrix \(Z N(,1)\) (left plot of Figure 1). To calculate the distortions of our method, we sample from the updated mean matrix and variance at each iteration.

As predicted by our theoretical analysis, we demonstrate that while the Gaussian randomized construction achieves satisfactory distortion levels, our method converges to a high-quality deterministic solution that nearly eliminates distortion (both plots of Figure 1). At the conclusion of the procedure, we calculate the distortion using the resultant mean matrix \(\). Interestingly, the results demonstrate that \(\|x\|_{2}^{2} 1\) (i.e. almost \(0\) distortion), compared to approximately \(1\) and \(0.6\) for the average and minimum distortions of the random construction, respectively. We plot the progression of the distortions and variance over the iterations in the plots of Figure 1.

This evidence highlights the effectiveness of our methodology in practice, showcasing the advantage of integrating data structure into dimensionality reduction for more accurate embeddings with provable guarantees.

Further details can be found in Appendix B.

Figure 1: Plot of the distortion obtained through optimization over \(5000\) iterations vs the average distortion using a random Gaussian matrix (left plot), and the progression of variance over the same number of iterations (right plot). To calculate the distortionsâ€™ progression with our method, we sample from the updated mean matrix and variance at each iteration and compute the distortion. We remark that the distortion plotted is a proxy for our objective in Equation 4. We observe that our optimization-based approach converges to a deterministic solution sampler. By using the mean matrix \(\), we achieve nearly optimal distortion, where \(|x||x|\).

Conclusion

In this work, we have demonstrated that optimization used directly in the sampler space can find a deterministic JL-quality embedding. While our initial focus has been on the relatively simple random construction of the Johnson-Lindenstrauss Lemma, there exist more complex randomized constructions that may offer greater flexibility. We believe that the methods we have introduced could find applicability far outside the JL setting.