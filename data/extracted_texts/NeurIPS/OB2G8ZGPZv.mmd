# ROSA: An Optimization Algorithm for Multi-Modal Derivative-Free Functions in High Dimensions

Ilija Ilievski

College of Design and Engineering

National University of Singapore

ilija@u.nus.edu

&Wenyu Wang

College of Design and Engineering

National University of Singapore

wenyu_wang@u.nus.edu

Christine A. Shoemaker

Civil and Environmental Engineering

Cornell University

cas12@cornell.edu

###### Abstract

Derivative-free, multi-modal optimization problems in high dimensions are ubiquitous in science and engineering. Obtaining satisfactory solutions to high-dimensional optimization problems requires many objective function evaluations. At the same time, commonly used Bayesian optimization methods are typically computationally too expensive for sufficient sampling of the high-dimensional space which limits their function approximation accuracy and leads to sub-optimal solutions. We propose ROSA, a novel optimization algorithm based on well-known optimization techniques such as randomized optimization, simulated annealing, and surrogate optimization. ROSA is several orders of magnitude computationally more efficient than leading scalable Bayesian optimization methods, while also obtaining comparable or better solutions with as many as 4 times fewer objective function evaluations. We compare ROSA with a diverse set of methods on many synthetic high-dimensional benchmark functions and real-world problems.

## 1 Introduction

Many important problems in science and engineering require the optimization of multi-modal derivative-free functions, often resource-limited to only several thousand function evaluations. For example, calibration of water simulation models [1; 2], climate simulation models [3; 4], aircraft wing design [5; 6; 7], vehicle design optimization , and machine learning hyperparameter optimization [9; 10; 11]. With the proliferation of data and computing power, the functions of interest have become increasingly higher-dimensional. Thus, we focus on optimizing functions in 50 to 500 dimensions and with an evaluation budget limited at most ten times the dimension.

Applying optimization methods to high-dimensional functions is a difficult problem due to the curse of dimensionality, that is, the volume of the sampling space and the required function evaluations for an accurate function approximation grow exponentially with the number of dimensions. One line of research, led by REMBO, attempts to avoid the problem by embedding the high-dimensional space in a lower-dimensional space where the optimization is performed . A closely related method is SASSBO which optimizes only some of the dimensions . However, REMBO's assumption that the high-dimensional function can be accurately represented in low dimensions is often not valid in practice, and SASSBO's high computational requirements limit its application to problems with evaluation budgets of at most hundred function evaluations. Another line of research is thescalable Bayesian optimization methods, which aim to fit a sparse Gaussian process models , or fit a Gaussian process models only on subset of evaluated points around a trust region . But, sparsifying the solutions space or using only subset of the evaluated points often results in suboptimal solutions. Finally, the inherit computational expense of fitting and tuning Gaussian process models, commonly limits the application of these models to problems with evaluation budgets of at most thousand iterations.

We propose a fundamentally different optimization algorithm that combines metaheuristic search method with a polyharmonic spline surrogate. Polyharmonic splines are fitted in closed-form and require no tuning, in addition to being accurate approximates of high-dimensional functions . Our metaheuristic search method merges ideas from randomized optimization and simulated annealing to achieve optimal exploration-exploitation trade-off and efficiently avoid local optima. To summarize our contributions are: 1. We develop ROSA, a novel optimization algorithm for efficient multi-modal optimization of high-dimensional derivative-free functions with an evaluation budget of several thousand evaluations. 2. We open-source our modern and efficient implementation of ROSA, able to run on CPUs and GPUs. We hope our code will be used by researchers and practitioners in a wide range of applications in science and engineering.

## 2 Method

We develop an optimization method for a multi-modal derivative-free function over a hypercube defined by \(\) and \(\) in \(d\) dimensions. That is \(^{*}=*{argmin}_{^{}[, ]^{d}}f(^{})\), notation info in Sec. A.1.

The proposed algorithm, ROSA, is based on three fundamental ideas in multi-modal, derivative-free optimization: (i) _Randomized Optimization_, where one randomly samples points by adding a random vector to the current best point. However, instead of adding random perturbations to _all_ dimensions, ROSA only changes a small and decreasing number of dimensions at each iteration. (ii) _Simulated Annealing_, where one allows accepting a worse neighbour as the current best point, but with a probability that decreases with the number of iterations. (iii) _Surrogate optimization_, where one fits a surrogate such as Gaussian Process and uses an acquisition function such as Expected Improvement to decide where to evaluate \(f\) next. Instead, ROSA uses computationally efficient surrogate such as polyharmonic splines to only rank the neighbours and evaluate on \(f\) the top-ranked neighbours.

The algorithm takes as inputs an objective function \(f\) we wish to minimize and an evaluation budget \(n_{}\). ROSA starts by evaluating the objective function at \(n_{0}\) points, sampled uniformly at random from the function input space. The point that gives the lowest objective function value is set as the current best point, \(^{}\), from where the optimization iterations start.

At each iteration, ROSA selects a set of dimensions to be perturbed at random, with a probability of being selected \(p_{}=(n/n_{})\). The probability of each dimension being selected is a decreasing function (\(\)) of the amount of the currently used evaluation budget (Sec. 2) and it is independent from the selection of the other dimensions. Formally, we define the set of selected dimensions: \(=\{k:<(n/n_{}) k,\ v U(0,1)\}\), where \(=\{1,,d\}\) and \(U(0,1)\) is Uniform distribution over \((0,1)\). In case, \(=\), then \(=\{j\}\) where \(j\) is a random sample from \(\), ensuring at least one dimension is always selected. The probability of a dimension being selected is also independent across iterations, resulting in different dimensions being selected at each iteration.

We create a set \(\) of \(q\) neighbouring points, with \(q d\), i.e. \(=\{}_{i} i\{1,,q\}\}\), by adding random perturbations to the selected dimensions of the current best point \(^{}\) (Sec. 2). Selecting the dimensions is independent across the \(}_{i}\) points, so each point may have different dimensions selected. We then evaluate each \(}_{i}\) point in \(\) on a polyharmonic spline surrogate and select the point with lowest surrogate value as the next evaluation point \(^{}\).

The point \(^{}\) is evaluated on the objective function and it is accepted as the current best point with probability \(p_{}=g(f(^{}),f(^{}),n)\), given by \(g\), the acceptance probability function (Sec. 2). After exhausting the evaluation budget we return the current best point and its objective value. Note that ROSA is easily parallelizable by choosing the \(m\) lowest surrogate value points instead of a single point, given that one is able to run the objective function efficiently in parallel.

Next, we describe the algorithm's components, and in the appendix, we define the algorithm in pseudo code (Alg. 1), provide a proof of convergence (Sec. A.2), and open-source our code at https://github.com/i13p/ROSA.

**Polyharmonic Spline Surrogate** A polyharmonic spline surrogate is a linear combination of radial basis functions and a polynomial tail. The polyharmonic kernels are scale invariant, so they do not require tuning of hyperparameters such as the length-scale in the case of the squared exponential and Matern kernels, the most common Gaussian Process kernels. The polyharmonic spline surrogate is defined as: \((r)=r^{k}\) if \(k=2n-1\) for \(n\) and as \((r)=r^{k}(r)\) if \(k=2n\) for \(n\), where \(r=\|-_{i}\|_{2}\) and \(_{i}\) are the centres, i.e., the evaluated points used to fit the surrogate. ROSA incorporates a cubic spline, \((r)=r^{3}\), which is conditionally positive definite kernel of order 2. So, we include a linear polynomial tail to ensure the stability of the solutions of the system of equations used to fit the surrogate parameters. Formally, given \(n\) number of \(d\)-dimensional vectors, \(_{1:n}\), we construct an polyharmonic spline interpolation model with:

\[H()=_{i=1}^{n}_{i}(||-_{i}||_{2})^{3} +^{}[1,^{}]\] (1)

The model parameters \(_{i:n}\), \(b_{i:d}\) are determined by solving the following linear system of equations:

\[+&\\ ^{}&\\ =\\ \] (2)

Here \(^{n n}\) is defined as \(_{i,j}=(||_{i}-_{j}||_{2})^{3},i,j=1,,n\), \(^{(d+1)(d+1)}\), \(^{n(d+1)}\) has its \(i\)-th row defined as \([1,_{i}^{}]\), \(=[_{1},,_{n}]^{}\), \(=[f(_{1}),,f(_{n})]^{}\), and \(\) is a regularization constant. When \(n<d+1\), we fit the model parameters via least squares.

Acceptance Probability FunctionThe acceptance probability function promotes exploration by accepting with some probability a point with a worse objective function value as the current best point . We define the acceptance function as \(p_{}:=g(f(^{}),f(^{}),n)=([^{})^{2}-f(^{}))}{T_{n}}],1)\), where \(f(^{})\) is the new, possibly worse, objective function value, and \(f(^{})\) is the current best objective value. \(T_{n}\) is the cooling temperature with geometric schedule, i.e., \(T_{n}=^{n}T_{0}\), where \(T_{0}\) and \(\) are set to common values  and fixed throughout our experiments. We accept \(^{}\) as the current best point with probability \(p_{}\).

Probability of Perturbing a DimensionWe define the function \((n/n_{})\) as a simple decreasing step-function that goes from \(11\) to \(16\) (see code for the specific implementation). In Sec. B.5, we analyse our choice of probabilities for selecting dimensions by comparing the default ROSA, with ROSA-Const. 20D where we perturb \((20,d)\) dimensions, ROSA-Const. \(10\%\) where we use constant function \(()=11\), ROSA-Increasing, where we _increase_ the probabilities from \(0.10\) to \(1.00\), ROSA-Reversed Default, where we increase the probabilities from \(16\) to \(11\). In Figure B.3 we observe that the performance of ROSA is similar as long as the probabilities are decreasing.

Perturbation DistributionTo the selected dimensions of the current best point, we add random samples from truncated normal distribution (Eq. 5) with mean \(0\) and perturbation radius fixed to one sixth of the objective function domain range, i.e., \(=(-)/6\). It is important that the samples are coming from truncated normal distribution, instead for example from uniform distribution , as otherwise the distribution of the samples gets heavily skewed towards the domain bounds (Figure B.2). We recommend that \(\) is set to one sixth of the bounded range such that \(99.7\%\) of the hypercube space is reachable from the centre point. To empirically justify our choice, we benchmark ROSA with multiple \(\) values and show that ROSA is fairly robust to different values of \(\), with the default version only having a small advantage on some functions (Figure B.4). The pseudo code is in Alg 2.

``` Inputs: 1. Point \(^{}\) around which to generate neighbouring points. 2. Percentage of currently spent evaluation budget \(=n/n_{}\). Configuration: 1. Bounding hypercube \([,]^{d}^{d}\) of permitted values of \(\). 2. A decreasing step-function that maps a percentage of spent budget to a probability of perturbing a dimension, \(p_{}=()\) (Section 2). 3. Number of neighbour points \(q\). Algorithm: Create a set \(\) of \(q\) neighbour points \(}_{i}\) where for each point: 1. Define a set of selected dimensions: \(=\{k:<() k,\; U (0,1)\}\), where \(=\{1,,d\}\) and \(U(0,1)\) is Uniform distribution over \((0,1)\). 2. Construct the permutation vector \(=[z^{(k)}]^{}\), for \(k\), \(z^{(k)}=(0,,^{(k)},^{(k)})\), if \(k\) otherwise, \(z^{(k)}=0\). Here \((0,,^{(k)},^{(k)})\) is truncated Normal distribution with mean \(0\) and standard deviation \(\) bounded by \(=-^{}\) and \(=-^{}\). 3. Set \(}_{i}=^{}+\). Return: The set of \(=\{}_{i} i\{1,,q\}\}\) neighbour points. ```

**Algorithm 2** Neighbour Generation in ROSA

## 3 Experiments

ROSA combines ideas from simulated annealing, randomized optimization, and surrogate optimization. Accordingly we compare ROSA with baselines that employ ideas from simulated annealing, Dual Annealing , randomized optimization, PSO , and surrogate optimization (lq-CMA-ES , TuRBO , DYCORS ). We also compare ROSA to an evolutionary algorithm (CMA-ES) as it have been shown to be competitive in optimization of black-box functions with large evaluation budgets . ROSA is most similar to TuRBO and DYCORS, the three methods add random perturbations to the current best, so we list their similarities and differences in Table B.1.

We compare the methods on three well-known synthetic functions, Ackley  -- a function most surrogates can approximate well, Michalewicz  -- a function with flat surface and sharp ridges where surrogates are often not helpful, and Rastrigin  -- function riddled with many good local minima and thus algorithms often get stuck in a local minimum. 2D visualizations of the three functions are shown in Figure B.1. We vary the number of dimensions from \(60\) to \(200\) to evaluate how the algorithms' performance scale with the number of dimensions. As an additional challenging benchmark suit we use the BBOB-largescale suite from COCO .

As real-world problems we use the well-known benchmark problem of optimizing vehicle design introduced by General Motors at MOPTA08  (in \(124\)D), and optimizing \(496\) portfolio weightsfor maximizing volatility adjusted returns while minimizing max drawdown, a common finance application of non-convex optimization methods. We perform parallel optimization with \(m=50\) on problems with a budget of more than \(2{,}000\) evaluations, such as BBOB-320D, BBOB-640D, and Portfolio Optimization, on the rest we perform serial optimization (\(m=1\)). For details see Sec. B.1.

ResultsThe results in Table 1 show that ROSA significantly outperforms all methods under comparison on all functions across dimensions (with the exception of DYCORS on Ackley 60D). Further, ROSA advantage over the other methods increases with increasing number of dimensions, showing that ROSA is especially suited for the high-dimensional optimization problems that are becoming increasingly ubiquitous. Given the no free lunch theorem, we do not claim ROSA will outperform any method on any function. However, on multi-modal derivative-free functions, such as the representative set of functions in our benchmark, ROSA is expected to outperform.

The progress plots in Sec. C show that not only ROSA achieves lowest objective value after exhausting the evaluation budget, but it also consistently outperforms most methods on most problems after any number of iterations. For example, on Ackley 200D (Figure C.2 right), ROSA achieves the lowest value found by TuRBO, the scalable Bayesian optimization method, after only using \(~{}27\%\) of the evaluation budget. Which means ROSA achieved comparable solution to TuRBO with almost 4 times fewer function evaluations. Furthermore, ROSA's code is two orders of magnitude faster than TuRBO's (Figure C.10). This means, as opposed to scalable Bayesian optimization methods, ROSA can be applied to difficult problems necessitating otherwise prohibitively large number of evaluations.

The outperformance of ROSA is also confirmed on the ten multimodal functions in high dimensions of the BBOB benchmark suite. In 160D ROSA performance is matched by DYCORS after \(~{}75\%\) of the budget (Figure C.8). However, DYCORS computational requirements are too great to be run on the 320D and 640D problems (Figure C.9). Which once again confirms the need of a method with not only good optimization performance but also that is scalable and computationally efficient to be able to optimize high-dimensional problems and with many function iterations.

We perform sensitivity analysis of ROSA's crucial algorithm component, the neighbouring points generation. We justify our choice of probability of perturbing a dimension with the results in Figure B.3, the way we set the standard deviation of the perturbation distribution in Figure B.4, and our choice of perturbation distribution in Figure B.5.

## 4 Discussion

With this paper, we aim to address the problem of optimization of multi-modal, derivative-free functions in high dimensions by developing an optimization method that is computationally efficient, while achieving excellent optimization performance on 15 representative functions in 9 distinct dimensions. We hope ROSA will serve as an alternative to Bayesian optimization methods when they are not suitable for the task. ROSA's modular algorithm design is based on fundamental optimization ideas developed and time-tested over decades of research. The modular design also allows for easy customization to specific problem types and further development of the algorithm. As an example, future work involves adapting the exploration vs exploitation trade-off to unseen problems by dynamically adjusting the probability of perturbing a dimension and the acceptance probability.

    &  &  &  &  &  \\  Method Dim. & 60 & 120 & 150 & 200 & 60 & 120 & 150 & 200 & 60 & 120 & 150 & 200 & 124 & 496 \\  Sobol & 12.39 & 13.01 & 13.15 & 13.28 & -13 & -23 & -27 & -34 & 869 & 1854 & 2349 & 3200 & 308 & - \\ PSO & 13.35 & 13.52 & 13.67 & 13.60 & -17 & -28 & -33 & -42 & 898 & 1897 & 2413 & 3218 & 316 & -36.9 \\ DA & 12.69 & 13.10 & 13.17 & 13.34 & -12 & -21 & -26 & -32 & 878 & 1856 & 2349 & 3213 & 313 & \\ CMA-ES & 8.74 & 8.65 & 8.47 & 8.50 & -13 & -23 & -27 & -35 & 714 & 1533 & 1953 & 2697 & 251 & -53.7 \\ Io-CMA-ES & 4.14 & 4.46 & 4.58 & 4.63 & -13 & -22 & -27 & -33 & 669 & 1293 & 1620 & 2168 & 235 & - \\ TuRBO & 4.56 & 5.77 & 6.20 & 6.38 & -24 & -54 & -69 & -92 & 421 & 805 & 1008 & 1334 & 243 & -67.0 \\ DYCORS & **2.23** & 2.51 & 2.53 & 2.6 & -32 & -68 & -86 & -113 & 286 & 569 & 711 & 957 & - & - \\  ROSA & 2.69 & **1.99** & **1.90** & **1.8** & **-35** & **-71** & **-89** & **-118** & **272** & **520** & **653** & **884** & **226** & **-77.9** \\   

Table 1: Mean best value obtained from 30 independent trials, across problems and dimensions, for each method under comparison, with bold we denote the best overall result. PSO - particle swarm optimization, DA - Dual Annealing.

[MISSING_PAGE_FAIL:6]

*  T. Back, _Evolutionary algorithms in theory and practice: evolution strategies, evolutionary programming, genetic algorithms_. Oxford university press, 1996.
*  M. Molga and C. Smutnicki, "Test functions for optimization needs," _Test functions for optimization needs_, vol. 101, p. 48, 2005.
*  H. Pohlheim, "Geatbx examples examples of objective functions," _Documentation for GEATbx version_, vol. 3, pp. 5-6, 2005.
*  N. Hansen, A. Auger, R. Ros, O. Mersmann, T. Tusar, and D. Brockhoff, "Coco: A platform for comparing continuous optimizers in a black-box setting," _Optimization Methods and Software_, vol. 36, no. 1, pp. 114-144, 2021.
*  J. C. Spall, "Direct methods for stochastic search," in _Introduction to Stochastic Search and Optimization_, L. John Wiley & Sons, Ed. John Wiley & Sons, Ltd, 2003, pp. 34-64, isbn: 9780471722137.
*  A. B. Owen, "Randomly permuted (t, m, s)-nets and (t, s)-sequences," in _Monte Carlo and quasi-Monte Carlo methods in scientific computing_, Springer, 1995, pp. 299-317.
*  P. Virtanen _et al._, "SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python," _Nature Methods_, vol. 17, pp. 261-272, 2020. doi: 10.1038/s41592-019-0686-2.
*  D. Eriksson, D. Bindel, and C. Shoemaker, _Surrogate optimization toolbox (pysot)_, https://github.com/dme65/pySOT, 2015.
*  D. Eriksson, D. Bindel, and C. A. Shoemaker, "Pysot and poap: An event-driven asynchronous framework for surrogate optimization," _arXiv preprint arXiv:1908.00420_, 2019.
*  N. Hansen, A. Auger, S. Finck, and R. Ros, "Real-parameter black-box optimization benchmarking: Experimental setup," _Orsay, France: Universite Paris Sud, Institut National de Recherche en Informatique et en Automatique (INRIA) Futurs, Equipe TAO, Tech. Rep_, 2012.

## Appendix A Appendix

### Notation

 Notation & Description \\  \(f\) & objective function we wish to minimize \\ \(\) & a vector, objective function input, or a point, used interchangeably \\ \(x_{i}^{(k)}\) & the \(k\)-th element, i.e., dimension, of the \(i\)-th vector \(x\) \\ \(d\) & number of objective function dimensions, i.e., input vector length \(\) \\ \(,\) & vectors, delineating the bounds of the objective function domain \\ \(}\) & a neighbour vector, only evaluated on the surrogate \\ \(^{}\) & the current best vector, i.e., an _evaluated_ point with lowest objective function value \\ \(^{}\) & the current best neighbour, i.e., a point with lowest _surrogate_ function value \\ \(y^{}\) & current lowest objective function value found \\ \(y^{}\) & objective _function_ value of the current best neighbour \\ \(H(})\) & a surrogate mapping neighbour vectors \(}\) to objective function value _estimates_ \\ \((||-_{i}||_{2})\) & kernel function centred on the evaluated point \(_{i}\) \\ \(n\) & number of spent function evaluations \\ \(n_{}\) & number of maximum function evaluations, i.e., the evaluation budget size \\ \(n_{0}\) & number of function evaluations used for initialization \\ \(\) & percentage of evaluation budget spent, i.e., \(n/n_{}\) \\ \(p_{}\) & probability of accepting the solution \(y^{o}\) as the current best \\ \(g(y^{},y^{},)\) & function outputting the probability \(p_{}\) of accepting the solution \(y^{}\) \\ \(p_{}\) & probability of selecting a dimensions for perturbation \\ \(()\) & function outputting the probability \(p_{}\) of selecting a dimension \\ \(\) & the set of dimensions, i.e. \(=\{1,,d\}\) \\ \(\) & the set of selected dimensions for perturbation during neighbour generation \\ \(\) & the set of neighbour points \\ \(\) & the set of pairs of point and objective function value, used to fit the surrogate \\ \(\) & the set of evaluated points, i.e., the algorithmic running history \\ \(}\) & the set of pairs of neighbour point and surrogate value \\ \(U(a,b)\) & Uniform distribution between \(a\) and \(b\) \\ \((0,,,)\) & truncated Normal with mean \(0\) and standard deviation \(\) bounded by \(\) and \(\) \\ 

### Convergence Analysis

After \(n n_{0}\) times of objective evaluation, consider the current best \(d\)-dimensional neighbour vector \(_{n}^{}\) as a random vector. Define \(_{n}=\{_{1}^{},,_{n}^{}\}\) to store all the evaluated points, i.e., the entire algorithmic running history after \(n\) iterations.

Finally, let \((,)\) denote the open ball centred at \(\) with radius \(\), i.e., \((,)=\{^{}||- ^{}||_{2}<\}\), and \((_{n})\) denote the \(\)-field generated by the random vectors in \(_{n}\).

**Theorem 1**: _Let \(f\) be a real-valued function defined on \(^{d}\) and suppose that \(^{*}\) is the unique global minimizer of \(f\) on \(\) in the sense that \(f(^{*})=_{}f()>-\). Following the pseudo code in Algorithm 1, ROSA iteratively generates random vectors \(\{_{n}\}_{n 1}\), and maintains a sequence of random vectors \([_{n}^{}]\) as follows: \(_{n}^{}=_{n}^{}\) with probability \(p_{}=g(f(^{}),f(_{n}^{}),n)\) (Section 2) otherwise \(_{n}^{}=_{n-1}^{}\). Then \(_{n}^{}\) converges to \(^{*}\) almost surely._

Proof.: Fix \(>0\) and \(n n_{0}+1\). Assume that there exists \(()>0\) such that \(|f()-f(^{*})|<\) whenever \(||-^{*}||_{2}<()\). Hence, the event \([_{n}:|f(_{n})-f(^{*})|< ][_{n}:||_{n}-^{*}||_{2}<()]\), and so,

\[P[_{n}:|f(_{n})-f( ^{*})|<(_{n-1})]& P[_{n} :||_{n}-^{*}||_{2}<() (_{n-1})]\\ &=P[_{n}(^{*},()) (_{n-1})]\\ &=_{(^{*},())}P( _{n}=(_{n-1}))d\] (3)

In ROSA, the neighbour \(_{n}^{}\) is generated by adding independent truncated normal samples to the selected dimensions of \(_{n-1}^{}\). Let \(_{n,j}\) denotes the event that the \(j\)-th dimension of \(_{n-1}^{}\) is selected for perturbation and \(P(_{n,j})=p_{_{n}}=(n/n_{})>0\) (see Section. 2). Therefore, the candidate \(_{n}^{}\) has the following conditional density function given \((_{n-1})\) for each \(n>n_{0}\),

\[P(_{n}^{}=(_{n-1}))& =_{j=1}^{d} P(x_{n,j}^{}=x_{j}(_{n-1}))\\ &=_{j=1}^{d}[P(x_{n,j}^{}=x_{j}(_{n-1}), _{n,j}) p_{_{n}}\\ &+P(x_{n,j}^{}=x_{j}(_{n-1}),_{n,j}) (1-p_{_{n}})]\\ &_{j=1}^{d}(x_{n,j}^{}=x_{j}(_{n-1} )) p_{_{n}},\] (4)

where \(\) is a truncated normal density function in the following form,

\[(x_{n,j}^{}=x_{j}(_{n-1});_{j},a_{j},b_{j} )=^{2}}}^{}-x_ {n-1,j}^{})^{2}}{2_{j}^{2}}]}{(-x_{n-1,j}^{ }}{_{j}})-(-x_{n-1,j}^{}}{_{j}})}\] (5)

with \(\) being the cumulative function of a standard normal distribution and the rest defined in Section A.1. Bounding (5) from bellow:

\[(x_{n,j}^{}=x_{j}(_{n-1});_{j},a_{j},b_{j} )^{2}}}-a_{j})^{2 }}{2_{j}^{2}}]}{(-a_{j}}{_{j}})-(-b_{j}}{_{j}})}=:C_{j}>0\] (6)

Then, (4) is also bounded with:

\[P(_{n}^{}=(_{n-1}))_{j=1}^{d}C_ {j} p_{_{n}}.\] (7)

Finally, combining (7) with (3) yields,

\[P[_{n}^{}:|f(_{n}^{ })-f(^{*})|<(_{n-1})]&_{ (^{*},())}_{j=1}^{d}C_ {j} p_{_{n}}d\\ &((^{*},()))_ {j=1}^{d}C_{j} p_{_{n}}\\ &=:L()>0,\] (8)where \(\) is the Lebesgue measure on \(^{d}\) and \(p_{_{n}}\) denotes the probability used to select dimensions. Thus, from (8), we have \(P[_{n}^{}:|f(_{n}^{})-f( ^{*})|<(_{n-1})]\) is lower bounded by \(L()\). By following the same argument as in the proof of theorem in p.40 of , we show that \(_{n}^{}\) converges to \(^{*}\) almost surely. \(\)

## Appendix B Experimental Setup

### Methods

We compare ROSA with a diverse kind of methods, namely:

* Sobol, a quasi-random baseline represented by a scrambled Sobol sequence ; We use PyTorch's implementation of scrambled Sobol, available at link.
* PSO, a particle swarm optimization method ; We follow the instructions at link to install and run pyswarms on optimizing a function with bounds. We set options \(=\{c1:0.5,c2:0.3,w:0.9\}\), n_particles \(=10\), and use the "GlobalBestPSO" as optimizer with the rest settings set to default.
* DA, Dual Annealing, a method that combines classical simulated annealing with fast simulated annealing, coupled with a local search strategy . We use the default values as implemented in SciPy .
* CMA-ES, a model-free evolutionary optimization method ; We follow the instructions at https://github.com/CMA-ES/pycma to install CMA-ES and run CMA with the default settings. As an initial point \(_{0}\) we set the middle point of the domain, _i.e._\(_{0}=(+)/2\), where \(\) and \(\) are the upper and lower bounds respectively. Following CMA-ES authors advice, we set \(_{0}\) to quarter of the objective function domain range.
* lq-CMA-ES, a surrogate-assisted evolutionary optimization method ; We follow the instructions at http://cma.gforge.inria.fr/cmaes_sourcecode_page.html# practical to install and run lq-CMA-ES with the default settings. As an initial point \(_{0}\) we tried setting the middle point of the domain and using a random point from \(n_{0}\) Latin Hypercube samples, the random LHS sample version worked better so we did all experiments with \(_{0}\) set to a random LHS point. Following lq-CMA-ES authors advice on the above-mentioned URL, we set \(_{0}\) to half of the function domain range, to make sure every space is within \(2_{0}\) from every possible \(_{0}\).
* TuRBO, a scalable Bayesian optimization method for high-dimensional surrogate optimization ; We follow the instructions at https://github.com/uber-research/TuRBO/ to install TuRBO-1 and run TuRBO-1 with the default settings. We run only TuRBO-1 as our initial experiments with running TuRBO-m did not show any improvements over TuRBO-1 but significantly increased the required computational resources and wall-clock run time. Our initial experimental results are also supported by most experiments in .
* DYCORS, surrogate optimization method with dynamic coordinate search . We use the implementation at https://github.com/dme65/pySOT to install pySOT and run DYCORS with the default settings . Due to pySOT's reliance on older software versions, we were not able to run DYCORS on MOPTA. DYCORS were also not run on the 496D portfolio optimization problem and 320D & 640D BBOB problems as it required unfeasible large amount of RAM memory.

ROSA is most similar to TuRBO and DYCORS, the three methods add random perturbations to the current best, so we list their similarities and differences in Table B.1.

### Benchmark functions

* Ackley is implemented as in https://www.sfu.ca/~ssurjano/ackley.html using the recommended values. We evaluate Ackley in the domain \([-5,10]^{D}\) where \(D\{60,120,150,200\}\).
* Michalewicz is implemented as in https://www.sfu.ca/~ssurjano/michal.html. We evaluate Michalewicz in the domain \([0,]^{D}\) where \(D\{60,120,150,200\}\).

* Rastrigin is implemented as in https://www.sfu.ca/~ssurjano/rastr.html. We evaluate Rastrigin in the domain \([-5.12,5.12]^{D}\) where \(D\{60,120,150,200\}\).
* For BBOB we use the COCO suite  as implementation. We use the multi-modal functions, F15 through F24, on the following instances \(\), chosen at random, and optimize in \(160\), \(320\), and \(640\) dimensions.

A two-dimensional plots of each of the functions are shown in Figure B.1.

### Real-world problems

We follow the instructions at https://www.miguelanjos.com/jones-benchmark to implement the MOPTA08 problem. As MOPTA08 is a constrained optimization problem, we transform it to unconstrained problem with \(f_{}()=f_{}()+10_{i=1}^{68} (0,c_{i}())\) where \(c_{i} 0\) is a constrain violation. As usual, we optimize the function in \(^{124}\).

The Portfolio Optimization problem aims to maximize volatility adjusted returns and minimize the maximum drawdown over the three year period of 2019-2022, of a portfolio consistent of 496 stocks that were part of SP 500 during all three years. Specifically, we minimize \(m-r/()\), where \(m\) is the portfolio maximum drawdown, \(r\) is the total portfolio return after the three years, and \(\) is the standard deviation of the daily returns.

    \\  DYCORS & \((20/d,1)(1-(n)/(n_{}))\) \\ TuRBO & fixed to \((20/d,1)\) \\ ROSA & fixed to \([1e-1,5e-2,5e-3,1e-6]\) \\   \\  DYCORS & Mirrored Normal \\ TuRBO & Uniform \\ ROSA & Truncated Normal \\   \\  DYCORS & dynamically adjusted based on performance \\ TuRBO & dynamically adjusted based on performance \\ ROSA & fixed to \(1/6\) of the domain range \\   \\  DYCORS & Cubic RBF with linear tail \\ TuRBO & Gaussian Process with Matern 5/2 kernel \\ ROSA & Cubic RBF with linear tail \\   \\  DYCORS & Scoring function with cycling weights based on distance and surrogate value \\ TuRBO & Fixed on surrogate value \\ ROSA & Simulated annealing based acceptance function \\   

Table B.1: Similarities and differences of key algorithm components of ROSA, TuRBO, and DYCORS. The three methods are similar as they all add random perturbations to some dimensions of the current best point.

Figure B.1: 2D plots of the used benchmark functions. Ackley left, Michalewicz centre, and Rastrigin right.

[MISSING_PAGE_EMPTY:11]

Figure B.4: **Method analysis**: Varying the standard deviation \(\) of the truncated normal distribution used to generate perturbation samples.

Figure B.5: **Method analysis**: Varying the distribution from which we sample perturbation samples. Comparing using Clamped Uniform with the default — Truncated Normal.

Figure C.1: Ackley optimization in 60 (left) and 120 (right) dimensions.

Figure C.3: Michalewicz optimization in 60 (left) and 120 (right) dimensions.

Figure C.4: Michalewicz optimization in 150 (left) and 200 (right) dimensions.

Figure C.1: Ackley optimization in 60 (left) and 120 (right) dimensions.

Figure C.5: Rastrigin optimization in 60 (left) and 120 (right) dimensions.

Figure C.6: Rastrigin optimization in 150 (left) and 200 (right) dimensions.

Figure C.7: MOPTA08 optimization in 124 dimensions (left) and Portfolio optimization in 496 dimensions with 50 parallel evaluations (right).

Figure C.9: Empirical cumulative distribution function (for details see ) of BBOB optimization in 320 (left) and 640 (right) dimensions with 50 parallel evaluations (higher is better).

Figure C.8: Empirical cumulative distribution function (for details see ) of BBOB optimization in 160 dimensions (higher is better).

### Wall-clock time comparison

Figure C.10: Wall-clock times of non-evaluation times, i.e. excluding the time it takes to run the function evaluation and only considering the time it takes for an optimization method to propose next evaluation point. Note that the y-axis is in **log scale**.