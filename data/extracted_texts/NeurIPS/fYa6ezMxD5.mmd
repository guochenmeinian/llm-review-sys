# MatFormer: Nested Transformer for Elastic Inference

Devvrit\({}^{*}\)\({}^{}\)  Sneha Kudugunta\({}^{*}\)\({}^{}\)\({}^{}\) Aditya Kusupati\({}^{*}\)\({}^{}\)\({}^{+}\)Tim Dettmers\({}^{}\) Kaifeng Chen\({}^{}\) Inderjit Dhillon\({}^{}\) Yulia Tsvetkov\({}^{}\) Hannaneh Hajishirzi\({}^{}\)

Sham Kakade\({}^{}\) Ali Farhadi\({}^{}\) Prateek Jain\({}^{+}\)

\({}^{}\)Google DeepMind \({}^{}\)University of Texas at Austin \({}^{}\)University of Washington \({}^{}\)Harvard University

Equal technical contribution. \({}^{+}\)Aditya Kusupati and Prateek Jain led the project.Correspondence: devvrit@cs.utexas.edu,{snehakudugunta,kusupati,prajain}@google.com

###### Abstract

Foundation models are applied in a broad spectrum of settings with different inference constraints, from massive multi-accelerator clusters to resource-constrained standalone mobile devices. However, the substantial costs associated with training these models often limit the number of unique model sizes that can be offered. Consequently, practitioners are compelled to select a model that may not be optimally aligned with their specific latency and cost requirements. We present MatFormer2, a novel Transformer architecture designed to provide elastic inference across diverse deployment constraints. MatFormer achieves this by incorporating a nested Feed Forward Network (FFN) block structure within a standard Transformer model. During training, we optimize the parameters of multiple nested FFN blocks with varying sizes, enabling the extraction of hundreds of accurate smaller models without incurring additional computational costs. We empirically validate the efficacy of MatFormer across different model classes (decoders and encoders) and modalities (language and vision), demonstrating its potential for real-world deployment. We show that a 850M decoder-only MatFormer language model (MatLM) allows us to extract multiple smaller models spanning from 582M to 850M parameters, each exhibiting better validation loss and one-shot downstream evaluations than independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and _consistent_ submodels extracted from MatFormer can lead to significant reduction in inference latency. Project website.

## 1 Introduction

Large Foundation models  are deployed in a variety of settings with different compute and accuracy demands like real-time response on mobile phones or on multi-cluster GPUs for web-scale batch serving. However, typical model families provide only a few _independently trained_ models of different sizes. For example, the Llama-2 family provides models with 7B, 13B, 34B, and 70B parameters . So practitioners are forced to choose a smaller (and typically less accurate) model than their latency/cost budget. Alternatively, one can use compression/pruning to fit a bigger model in a given compute budget , but that requires additional training.

We introduce MatFormer, a natively elastic Transformer  architecture that overcomes this challenge. MatFormer allows for training one _universal_ model which can be used to extract hundreds of smaller submodels without _any additional training cost_ (Figure 1). MatFormer is a generalarchitecture that can be applied to encoders and decoders, is domain agnostic, and is compatible with standard foundation model training pipelines.

MatFormer follows the principle of matryoshka representation learning , to introduce nested substructure inside the standard Transformer block. Formally, MatFormer defines Transformer blocks \(T_{i}\), such that, \(T_{1} T_{2} T_{g}\), where \(g\) is the number of nested transformer blocks, and \(T_{i} T_{i+1}\) relation indicates that the parameters of \(T_{i}\) are contained in those of \(T_{i+1}\). MatFormer can induce such sub-structure in both the attention and the feedforward network (FFN) blocks of the Transformer (see Figure 1). Consider, say, a FFN block that has \(d_{}\) neurons in the hidden layer. Then, MatFormer induces matryoshka structure on these neurons, where \(T_{i}\) contains the first \(m_{i}\) neurons and \(1 m_{1}<m_{2}<m_{g}=d_{}\) represent the number of neurons for each granularity or sub-model. Intuitively, this implies that the first \(m_{1}\) neurons are "most significant" neurons as they belong to all the blocks followed by the next \(m_{2}-m_{1}\), and so on.

In a departure from related work (Section 2), despite optimizing for only \(g\) granularities, we are able to extract exponentially more submodels post-training. Using the trained MatFormer blocks \(T_{1},,T_{g}\) at each layer, one can form new models by Mix'n'Match (Section 3.3), i.e., by taking an arbitrary combination of these blocks across layers. For example, in the first layer, one can select \(T_{g}\), the largest block, choose \(T_{2}\) in the second layer, and so on, forming \(g^{l}\) different models (where \(l\) is the number of layers). Surprisingly, in multiple settings, and for various model sizes, we observe that the extracted models indeed are accurate, with accuracy scaling with the size of the extracted model.

We train Matformer-based decoder-only Language Models (MatLM) up to 850M parameters and observe that: (a) MatLMs explicitly trained with \(g\) exponentially spaced granularities outperform validation loss and one-shot downstream evals of respective \(g\) baseline models trained independently from scratch, (b) our extracted models using Mix'n'Match lie on the accuracy-vs-parameters trade-off curve generated by the \(g\) explicitly trained models, (c) through scaling experiments we observe that the loss vs compute law for different MatFormer models remains similar to vanilla Transformer models across different granularities and (d) the submodels extracted from MatLM have highly consistent behavior that is highly desirable for inference optimizations and deployment across scales.

We further study MatFormer-based ViT models (MatViT) and make similar observations. For example, MatViT-L/16 improves the accuracy of the standard ViT-L/16 model on ImageNet-1K, and the extracted sub-models all match or even perform better than the independently trained baselines. Furthermore, we demonstrate that, due to high consistency, MatViT models can be used as "elastic encoders" for adaptive image retrieval. That is, the metric-space of an image encoded by the universal (i.e. largest) MatViT model is roughly preserved by the nested submodels. Hence, based on query complexity, system load, and various other considerations, we can use one of the extracted MatViT encoders at inference time for retrieval on a fixed corpus encoded by the universal model - providing over \(40\%\) less compute overhead with \(<0.5\%\) drop in accuracy.

**We make these key contributions:**

Figure 1: MatFormer introduces nested structure into the Transformer’s FFN block & trains all the submodels, enabling free extraction of hundreds of accurate submodels for elastic inference.

1. We introduce MatFormer, which incorporates a nested sub-structure within the standard Transformer and optimizes all the \(g\) granularities to produce a single, universal elastic model.
2. We introduce Mix'n'Match, a simple heuristic with no computation overhead that finds optimal submodels within a given parameter budget, outperforming more complex NAS methods. This yields hundreds of accurate and consistent submodels without any training cost (Section 3).
3. MatFormer generalizes effectively to both decoder-only language models (MatLM) and vision encoders (MatViT), scaling as reliably and accurately as the standard Transformer, while enabling significantly faster autoregressive generation and large-scale adaptive dense retrieval (Section 4).

## 2 Related Work

Transformers  have become the unifying model architecture for foundation models  across modalities like language , vision  and audio . While powerful, the standard Transformer block is not natively elastic in a way that enables large-scale adaptive and flexible deployment across various resource constraints. To cater to the plethora of deployment requirements, existing solutions include training a family of models of varying sizes [49; 2], post-hoc efficiency techniques like quantization , pruning , and distillation . However, these solutions often are specific to the single constraint at hand, and require additional training for each new downstream usecase. This makes them far from being a truly elastic solution for adaptive deployment. Lastly, Transformer based LLMs are often sped-up during inference with techniques like speculative decoding [39; 12] - that benefits from the smaller draft & the larger verifier models having similar behavior - or early exiting  to enable real-time deployment.

Obtaining multiple smaller models from a single model has been explored in the past [66; 65; 9; 23; 10] with most work focusing on CNN encoders. In this work, we focus on Transformers in decoder-only language models and pretrained vision models. Specifically, OFA  trains a teacher CNN model, and uses distillation to finetune randomly sampled submodels (not nested) in the universal student CNN model. Moreover, OFA focuses on small scale models to be deployed on end devices. In contrast, MatFormer doesn't require distillation, thereby using substantially less memory, and uses nested models. Using nested models allows us to host multiple models together without significantly increasing the model's memory footprint, which is advantageous for scenarios where we want to route queries through different sub-networks. Slimmable networks  jointly optimizes and provide limited preset widths. Universal Slimmable network  extends this to sample from a continuous search space of submodels and optimizes them jointly. In contrast, MatFormer samples and only optimizes one of the preset granularities. HAT  trains a universal network only to learn relative performance for different architectures. For deployment, the authors use NAS to find the optimal architecture and _train it from scratch_ before serving. In contrast, MatFormer requires no additional training and accurate subnetworks can be obtained using Mix'n'Match (Section 3.3) yielding results as good as NAS without the additional complexity. DynaBERT  jointly trains a fixed set of submodels, doesn't introduce any search strategy and discusses only using explicitly trained granularities as submodels. As a result of joint optimization of all granularities, DynaBERT yields fewer gradient updates and hence suboptimal performance compared to MatFormer while using same compute and memory (Section 4).

Moreover, we emphasize that while most work in this area optimizes for an _exponential number of models_, we optimize a small number of subnetworks (\(g=4\)) to obtain an exponential number of models at inference time which leads to significantly better accuracy for large datasets. More

    & N/Models Optimized & N/Models Obtained & Nested? & Model Selection & Post-training Need? & Architecture & Decoder Model? \\ 
**MatFormer** & **O(1)** & \(exp()\) & \(\) & Mix’s Match & \(\) & Transformer & \(\) \\  OFA  & \(exp()\) & \(exp()\) & \(\) & NAS & \(\) & CNN & \(\) \\ Slimmable Networks  & **O(1)** & **O(1)** & \(\) & - & \(\) & CNN & \(\) \\ HAT  & \(exp()\) & \(exp()\) & \(\) & NAS & \(\) & CNN & \(\) \\ Sered Network  & \(exp()\) & \(exp()\) & \(\) & - & \(\) & Both & \(\) \\ DynaBERT  & **O(1)** & **O(1)** & **\(\)** & - & \(\) & Transformer & \(\) \\   

Table 1: Comparison of MatFormer with comparable techniques across training and inference. We emphasize that in contrast with earlier work, MatFormer requires optimizing fewer models to obtain an exponential number of models at inference time without the need for post-training or NAS. Moreover, MatFormer subnetworks are nested, allowing for adaptive retrieval & colocation of models during inference. Here, \(l\) is the number of layers in the model and \((l)\) refers to exponential in \(l\).

recently, some of this work has been extended to Transformer encoders [11; 52] for extracting sub-models in both static or dynamic settings. But they either fail at extending further to decoder-only language models () or perform suboptimal to MatFormer (Section 4) owing to differences in their training methodology. While not in the weight space, matryoshka representation learning  & FlexiViT  showcase elasticity in output & input spaces respectively by smoothly spanning deployment constraints with minimal overhead. MatFormer, in contrast, builds upon these works by creating nested structure within the weight space instead to enable truly elastic and adaptive Transformer-based (decoder & encoder) models that span all the accuracy-vs-compute tradeoff (statically or dynamically) with minimal changes and training overhead (Figure 1). SortedNet  is a concurrent work with similar goals which optimizes many sampled submodels (akin to prior work) unlike MatFormer's optimization of a few (typically 4) nested submodels. We also note Flextron, a recent work that builds upon MatFormer by extending the nested elasticity in both MLP and Attention Heads simultaneously, tail patches the Matformer style training, and includes a router to automatically route each token within the different granularities in each layer.

In Table 1, we summarize the differences between MatFormer and the related work discussed. Among these works, we identified two central ideas commonly employed: jointly training multiple submodels and sampling random submodels. We consider DynaBERT  and Once-for-All (OFA)  as the respective most relevant prior works and compare them to MatFormer in Section 4.

## 3 MatFormer

In this section, we define MatFormer's nested substructure (Section 3.1) and discuss its training procedure for a chosen \(g\) model granularities (Section 3.2). We then discuss elastic inference using Mix'n'Match models (Section 3.3) from MatFormer along with its deployment considerations.

### MatFormer Structure

MatFormer defines \(g\) Transformer blocks \(T_{i}\), such that, \(T_{1} T_{2} T_{g}\) where \(T_{i} T_{i+1}\) indicates that the parameters of \(T_{i}\) are contained in those of \(T_{i+1}\). While it is possible to impose such a structure on any part of the Transformer, we select the FFN block to define our method and present a majority of our experiments, as the model size and computational cost of a Transformer is dominated (around \(60\%\) for LLMs and ViTs) by the FFN block (see Appendix C, and Appendix F.2 for experiments applying MatFormer to the attention block of the Transformer). So, in this work, we focus on inducing the MatFormer's nested sub-structure in the FFN block. We then stack individual blocks (for \(l\) layers) to form \(g\) nested models (\(_{1 g}\)) with shared parameters i.e., \(_{i}_{i+1}\).

The Transformer FFN block has a single hidden layer with \(d_{}\) neurons and both input and outputs in \(^{d_{}}\), and fixed FFN ratio : \(=d_{}/d_{}\) (typically \( 4\)). MatFormer introduces the matryoshka nested structure with \(g\) granularities on the hidden representation of the FFN block. Concretely, a nested sub-block of the Transformer, \(T_{i}\) contains the first \(m_{i}\) neurons of the FFN and \(1 m_{1}<<m_{g}=d_{}\) represent the number of neurons for each granularity or sub-model. So, depending on the chosen granularity the FFN operation of \(T_{i}\) i.e., \(T_{i}^{}\) on an input \(x^{d_{}}\) is:

\[T_{i}^{}(x)=(x_{1}[0:m_{i}]^{})_{2}[0:m_{i}],\] (1)

where the weight matrices of FFN are \(_{1},_{2}^{d_{} d_{}}\) and bias terms are omitted for simplicity. \(_{1}[0:k]\) denotes the submatrix with the first \(k\) rows of \(_{1}\). Finally, \(\) is a non-linearity often set to GELU  or squared ReLU . In this work, we chose the \(g=4\) exponentially spaced granularities with FFN ratios of \(\{0.5,1,2,4\}\) i.e., the nested hidden neurons are of the sizes \(\{}{8},}{4},}{2},d_{ff}\}\). We get \(g\) nested submodels \(_{1}_{2},_{g}\) where \(_{i}[T_{i}]^{ l}\), i.e., \(_{i}\) is formed by stacking \(T_{i}\) for \(l\) layers. The input and output embedding matrices are shared across the models.

We note that we can form a similar sub-structure on the attention heads, with the heads being organized from "most" to "least" significant, where the more significant heads are shared by more sub-models. That is, we use the first \(m_{i}\) attention heads for the \(i\)th granularity. We can also introduce this sub-structure in the token embedding (\(d_{}\)) supplied to each Transformer block.

### Training

For a Transformer model \(\), the forward pass on an input \(x\) is denoted by \((x)\) and let \(\) denote the loss function between the output and the target \(y\): \(((x),y)\).

MatFormer relies on a simple training strategy of randomly sampling the \(g\) nested submodels across training. To this end, for each step we randomly sample a Matformer granularity \(i=1,2...,g\) and train for it using the standard stochastic gradient-based optimizers :

\[_{}(x,y)=(_{i}(x),y),\] (2)

where \(_{i}\) is the parameter set of \(i\)-th granular submodel, with \(_{i}\) chosen from a probability distribution \(\{p_{1},p_{2}...p_{g}\}\). For most experiments in this paper, we uniformly sample each submodel - in Appendix F.3, we find that tuning this probability distribution can result in stronger submodels.

MatFormer training results in \(g\) accurate nested submodels \(_{1...g}\) inside the universal MatFormer model (\(_{g}\)), and also enables the extraction of hundreds of smaller submodels along the accuracy-vs-compute curve traced by the \(g\) explicitly optimized submodels (Section 3.3). These models emerge for free using Mix'n'Match during inference and drastically reduce the amortized training cost per model obtained through MatFormer. This method results in smaller submodels that have highly consistent behavior (Section 3.4) with the universal model.

### Mix'n'Match

At inference time, it is trivial to extract one of the \(g\) submodels \(_{1}_{2},_{g}\) by stacking the corresponding Transformer block \(T_{i}\) across layers. However, by selecting different granularities for each MatFormer layer, it is possible to generate a combinatorially large number of accurate smaller models for free. We call this simple procedure _Mix'n'Match_ and observe that these additional model granularities - which were never explicitly optimized - are highly performant.

For a given compute or parameter budget, there are multiple possible submodels. A common strategy to select an optimal submodel is Neural Architecture Search (NAS) [48; 68]. This, however, is computationally expensive (Appendix D.2). With, Mix'n'Match, we propose gradually increasing the sub-block size with the "least slope". More concretely, we recommend selecting sub-blocks with minimal granularity changes across layers, ensuring that the size of the \(j^{th}\) layer is at least that of the \(i^{th}\) layer for \(j>i\). To give a concrete example, we find that a submodel that uses granularity \(g_{2}\) for half the layers then \(g_{3}\) for the rest will likely be better than a submodel that uses \(g_{1}\) and \(g_{4}\) for a similar model size. Our heuristic is underpinned by the training methodology, where each sampled subnetwork maintains consistent layer granularity across the model. Consequently, the model adapts best to configurations where layer granularities are either uniform or display minimal variation. This intuition is also backed by NAS, which predicts balanced configurations over skewed (Appendix D.1). Among these "balanced" configurations, we empirically found the increasing with minimum slope configuration to perform the best. In Section 4.1.1 and Appendix D.1, we show that Mix'n'Match works at least as well as using evolutionary search based NAS methods  as used by OFA .

To summarize, we find that using Mix'n'Match is a simple, cheap and effective hueristic to select a highly performant submodel for a given compute budget (Sections 4.1.1 & 4.2). We provide further details and intuition in Appendix D.1.

### Deployment

The design of MatFormer is beneficial for both _static and dynamic workloads_:

Static WorkloadsTo expand upon the example of Llama-2 models in Section 1, a deployment setup might, say, have the latency budget to support 40B parameter Llama model, but can only host a 34B variant because the next bigger model (70B) has significantly higher latency. Training a 40B parameter from scratch would require \(4.8*10^{23}\) FLOPs, when training a 34B model and 70B model already cost \(4.08*10^{23}\) and \(8.4*10^{23}\) FLOPs respectively. So, one would need to settle for a less accurate model despite the larger latency budget. With Matformer, one could obtain a highly accurate 40B model for 0 additional training FLOPS. More precisely, for static workloads, where compute resources are known beforehand and the inputs remain relatively similar in difficulty, one can choose the most accurate static submodel for the constraints using Mix'n'Match.

Dynamic WorkloadsFor dynamic workloads, where the compute resources or the input hardness change on the fly, we can use the universal MatFormer model to dynamically extract the optimal submodel for each token or query. This works especially well for MatFormer because all the extracted submodels have high behavioral _consistency_ with universal MatFormer model (Section 4.1) - minimizing the drift across predictions from various submodels. We measure the consistency between two generative models as the _percentage of matching tokens_ generated by them for the same prefix or using the _KL divergence_ of the smaller model outputs with the larger model outputs - this accounts for potential sampling strategies in decoding. This high consistency results in superior inference time speedups for techniques like speculative decoding  (Section 4.1.1) and can assist in reducing prediction drift between cross platform deployments. We also show that higher model consistency also aids metric-space structure preservation in encoder models (Section 4.2.2). Moreover, given the nested architecture of MatFormer, model colocation can be more memory efficient.

## 4 Experiments

In this section, we empirically evaluate MatFormer across modalities (language in Section 4.1 and vision in Section 4.2) and model classes (decoder and encoder). We demonstrate the elastic deployment of MatFormer-based models (Sections 4.1.1 & 4.2) for tasks spanning from one-shot generative evals to adaptive image retrieval. Additionally, we also investigate the reliable scaling behavior  of MatFormer models (Section 4.1.2).

### MatLM: MatFormer Language Models

**Experiment Setting:** We build MatFormer-based decoder-only Language Models - MatLMs - and contrast them to their vanilla Transformer counterparts (LMs) . For each MatLM model with fixed \(d_{}\), we optimize for \(g=4\) nested granularities represented by FFN ratios of \(\{0.5,1,2,4\}\) - i.e., only the hidden representation size of the FFN block changes. We denote these submodels as MatLM - {S, M, L, XL} in increasing order of model size and refer to MatLM-XL as the universal MatLM. For baselines, we train vanilla Transformer models with comparable architectures. That is, for each MatLM, we train 4 separate baseline models with FFN ratios of \(\{0.5,1,2,4\}\) denoted as Baseline - {S, M, L, XL}. In addition, we adapt OFA  and DynaBERT  to our language modeling setup, and compare those to MatFormer at the same model size. We evaluate these models on validation loss and average accuracy on 25 English tasks [8; 22; 3]. We note that no additional memory and compute is used during training these methods compared to independently trained Baselines. Please see Appendix B for further details on training, baselines, evaluation, and the datasets.

**Results compared to baselines:** To showcase efficacy of MatFormer over baselines, we evaluate 850M MatLM model with the corresponding baseline counterparts in Figure 2.

Overall, in Figures 1(a) and 1(b) we observe all granularity submodels of MatLM outperform their baseline counterparts. Specifically, we find that DynaBERT exhibits a significant 0.01 log perplexity gap compared to MatFormer on the 850M model. The underlying reason is DynaBERT employs joint optimization of all granularities, which yields to fewer gradient updates and hence suboptimal performance compared to MatFormer. DynaBERT would require more than 15% extra compute to perform as well as MatLM. OFA, similar to MatFormer, maintains a single universal model but

Figure 2: Validation loss & one-shot downstream evaluation scores for the 850M MatLM & baseline models. Mix’n’Match helps generate accurate and more consistent models from MatLM that lie on the performance-vs-compute curve spanned by the explicitly optimized submodels.

employs random subnetwork sampling during its training. This leads to sampling fewer models close to S and XL granularity, resulting in inferior performance in this regime. The performance gap manifests as a bell-shaped loss curve (Figure 1(a)), highlighting OFA shortcomings in handling the trade-off between maintaining universal (XL) model quality and model elasticity. Additionally, OFA's training necessitates complicated NAS strategy for optimal submodel selection. However, using NAS at scale is costly and erroneous, which we further discuss in Appendix D.2. We refer the reader to Appendix B.4 for a more detailed discussion of MatFormer performance compared to baselines, and advantages and downsides of each method.

#### 4.1.1 Elastic Inference with MatLM

**Accurate MatLM submodels for every constraint for free with Mix'n'Match.** Mix'n'Match enables the MatLM to deliver accurate models for any compute constraint between S and XL, beyond the fixed granularities {S, M, L, XL}. We assess the efficacy of Mix'n'Match on the 850M parameter MatLM, comparing validation loss and downstream performance against independently trained baseline models {S, M, L, XL}. Figure 1(a) demonstrates that Mix'n'Match achieves optimal loss-vs-compute trade-offs at no additional cost. Additionally, downstream evaluations in Figure 1(b) reinforce this trend. In deployment scenarios with only 55% of the compute resources for a MatLM-XL model, a Mix'n'Match submodel approximates the XL's performance with only about a 1% accuracy drop, compared to a 2% drop when using the MatLM-M model. This highlights the efficiency of Mix'n'Match in creating numerous optimal models, as exemplified by selected instances along the performance curves.

We experimented with several heuristics to select the best subnetwork, but consistently observed that gradually using larger granularities in deeper layers worked the best (Section 3.3). We find that this heuristic better than the evolutionary search based techniques , used in OFA  in Figures 1(a) & 1(b). We also find that applying NAS to MatFormer provides no benefit over Mix'n'Match in Figure 6. We discuss additional details on Mix'n'Match in Appendix D.1.

**MatLM submodels speed up speculative decoding.** Speculative decoding leverages an accurate lightweight LM as a draft model to autoregressively generate a few tokens, followed by verifying these drafts with a larger model through parallel decoding on the generated tokens. When the draft is inaccurate, the draft model is rolled back and reset to the larger model's output. This results in considerable inference speed-up for the _same accuracy as the large model_. We point the reader to the original paper for a more detailed explanation .

Slow down of this algorithm stems from cases where the smaller model's predictions disagree with the larger model. A draft model that is significantly more consistent with the larger verifier model would lead to less rollbacks of the draft predictions and therefore lower latency. As seen in Figure 1(c), MatLM submodels can be up to \(11.5\%\) more consistent than the baselines to their corresponding XL model. The significant gap persists even in the KL divergence variant of consistency with the XL model's outputs (see Figure 7 in Appendix). This improved consistency along with the need for only a single universal model positions MatLM favorably to improve techniques that require draft and verifier models such as speculative decoding.

Table 2 shows the inference time speed-ups from speculative decoding using the S and XL submodels of the 850M language model for drafting and verification respectively. Speculative decoding with independently trained baseline LMs results in a speed-up of up to \(10\%\) over the standard autoregressive decoding of the 850M-XL model. But MatLM-based speculative decoding is up to \(6\%\) faster than traditional speculative decoding. This additional speed-up can be primarily attributed to the more consistent nature of MatLM-based drafter and verifier models and is further boosted by the ability to share attention cache across models from MatLM which is infeasible for the baselines (see Appendix C.1). Finally, MatLM further reduces the memory overhead for inference by removing the need to have two models during resource-constrained deployment.

   Speculative Decoding & LAMBADA & TriviaQA \\  Baseline & \(1.10\) & \(1.08\) \\ MatLM & \(1.14\) & \(1.11\) \\ + shared attention cache & \(1.16\) & \(1.14\) \\   

Table 2: Inference time speed-ups over a standard 850M model through speculative decoding using a 393M (S) draft and 850M (XL) verifier model.

#### 4.1.2 MatLM Scales as well as Vanilla Transformer LMs

Now that we have established that a 850M MatLM model and its submodels are at least as accurate as the baseline Transformer LMs, we want to examine the scalability of training MatLM models. So, we study the scaling properties  of MatLMs and compare them to vanilla Transformer baseline LMs trained for the same number of tokens. We train models ranging from 78M to 850M parameters on 10B to 80B tokens (per granularity) and plot the validation loss for MatLM - {S, M, L, XL} compared against independently trained baselines in Figure 9.

First, in Figure 2(a), we observe that the training of MatLM-XL models across model sizes scale as reliably as the Baseline-XL LMs for loss vs. number of parameters. Figure 2(b) interestingly shows that all granularities {S, M, L, XL}, of MatLM and Baseline follow the same scaling trend. Therefore, we fit a scaling law according to the number of non-embedding parameters (\(N\)) and training tokens (\(D\)) for all possible submodels for both MatLMs and the baselines in Table 3. We observe that the fitted parameters are extremely similar, suggesting that MatLMs scale similarly to vanilla Transformer LMs. In Figure 2(c) we also find that the downstream evals for MatLM \(0.3\%\) better than the baselines, with the smaller submodels further outperforming the baselines at scale by upto \(1.4\%\). Finally, Figure 8(f) in the Appendix shows that the MatLM submodels are more consistent with their XL model compared to the baseline counterparts across scales.

We note that the scaling laws do not capture how MatLMs have been optimized for multiple submodels and even have performant submodels that have not been explicitly optimized for (Section 4.1.1) We leave formulations that capture these subtleties to future work and further discuss this in Appendix E.1. We provide full results split by granularity in Appendix E.

Figure 4: MatViT variants match or outperform standard ViT models on ImageNet-1K classification and provide free extracted models that span the accuracy-compute curve through Mix’n’Match.

Figure 3: We train various decoder-only MatLM models at a range of sizes from 78M to 850M parameters and observe the scaling trends of all granularities (S, M, L, XL) for validation loss and 1-shot downstream evaluation scores. We find that the MatLM-XL models across scales mimic the training trends of Baseline-XL models. Interestingly, we also note that that validation loss and downstream evaluations follow the _scaling trends of the XL-models across all granularities_.

### MatViT: MatFormer Vision Transformers

We extend MatFormer to Vision Transformer (ViT)  based computer vision encoder models. MatFormer-based ViT (MatViT) enables elastic inference for fundamental tasks like image classification and retrieval. We train the MatFormer variant of the standard ViT-B/16 and ViT-L/16 models - MatViT-B/16 and MatViT-L/16 that are trained with \(g=4\) nested granularities (FFN ratios of \(\{0.5,1,2,4\}\)). B/16 models are trained on ImageNet-1K  with AugReg  while L/16 models are pretrained on ImageNet-21K  followed by finetuning on ImageNet-1K. All models use the training setup and optimal hyperparameters of standard ViT variants from the Scenic library .

#### 4.2.1 Image Classification

For image classification, we evaluate both ViT & MatViT models on ImageNet-1K. Figure 3(a) shows that the explicitly optimized granularities in MatViT result in as accurate models as the independently trained baselines for the B/16. However for L/16, as shown in Figure 3(b), we see that the MatViT models are up to \(0.35\%\) more accurate than the baseline for the same inference cost.

We then explore using MatFormer at different training stages with a \(2 2\) grid of pretraining-finetuning pairs (Table 7 in Appendix G.1) and find that using a MatFormer during pretraining helps bring more accurate and flexible encoders for downstream use. Further, finetuning using MatFormer enhances elastic deployment depending on the constraints at hand through Mix'n'Match.

**Adaptive Encoders with Mix'n'Match.** Furthermore, our Mix'n'match models' accuracy almost lies on the line joining accuracy of explicitly trained granularities. In scenarios where, say, an application can host 50M parameter B/16 model, MatViT can provide \(0.8\%\) more accurate model than the current approach which would host the largest baseline model with \(\) 50M parameters.

During deployment, the universal MatViT model can be stored in memory and depending on the compute constraints be used to extract an adaptable smaller model to maximize accuracy with the available resources at that moment. Currently, we find the Mix'n'Match models on the accuracy-compute curve through a quick inference on the validation set. While relatively scalable, this points to the need for optimal budget allocation across layers in neural networks .

#### 4.2.2 Adaptive Image Retrieval

The goal of image retrieval is to find semantically similar images - e.g. images from the same class - using representations obtained from a pretrained encoder . Standard approach is to encode the database images as well as query image with same encoder and run nearest neighbor retrieval for the query embedding. While we can embed database images with an expensive encoder, the query encoder generally has to be real-time. Furthermore, the setting of query encoding might be varied, e.g., on-device vs. cloud processing, varying query load and query complexity. Current solutions have a fixed encoder thus compromising on accuracy or cost for various settings.

Given the elastic nature of MatViT, it is a good candidate for query encoder. However, retrieval also requires that submodels preserve distances between fixed database (with large encoder) and query embeddings across all the granularities. If we use smaller baseline ViT models only for query encoding, these distances are not preserved and lead to nearly \(0\) retrieval accuracy (see Figure 5).

Figure 5: MatViT natively enables elastic encoders for adaptive retrieval that can be used for real-time query side computation while retaining strong accuracy on ImageNet-1K, unlike the baselines.

We evaluate both ViT and MatViT encoders on ImageNet-1K for image retrieval. We compute 1-nearest neighbor (NN) accuracy using the representation vector of the [CLS] token (also see Appendix G.2). Figure 5 shows that submodels extracted from MatViT can approximately preserve distances and provide significantly more flexibility. For example, with a loss of \(<0.5\%\) accuracy, MatViT-L/16 can reduce compute cost by \(40\%\). This corresponds to the 175M Mix'n'Match parameter model in Fig 4(b), which is 40% smaller than the 300M XL model, and has \(<0.5\%\) accuracy drop. To our knowledge, this is the _first result of its kind_ and opens up a wide variety of adaptive inference strategies for large-scale semantic search.

## 5 Conclusion

In this work we presented MatFormer, a natively elastic Transformer architecture that allows training a single universal model which can be used to extract hundreds of smaller accurate submodels at zero additional cost at deployment time. We find that the MatFormer Language Model (MatLM) matches the perplexity & 1-shot accuracy of independently trained models. In fact, MatLM demonstrates an interesting loss-vs-compute scaling curve that is nearly _independent_ of trained granularity indicating robust generalization to _extremely_ large models as well. Finally, MatFormer submodels enable diverse inference time speedups like faster autoregressive generation with speculative decoding and elastic query encoders for adaptive dense retrieval across modalities. We believe dynamically routing these models to change inference latency , and developing the hardware optimizations required is a promising area for future work.