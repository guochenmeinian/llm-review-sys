# Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning

Zhixiang Shen,  Shuo Wang,  Zhao Kang

School of Computer Science and Engineering,

University of Electronic Science and Technology of China, Chengdu, Sichuan, China

zhixiang.zxs@gmail.com zkang@uestc.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Unsupervised Multiplex Graph Learning (UMGL) aims to learn node representations on various edge types without manual labeling. However, existing research overlooks a key factor: the reliability of the graph structure. Real-world data often exhibit a complex nature and contain abundant task-irrelevant noise, severely compromising UMGL's performance. Moreover, existing methods primarily rely on contrastive learning to maximize mutual information across different graphs, limiting them to multiplex graph redundant scenarios and failing to capture view-unique task-relevant information. In this paper, we focus on a more realistic and challenging task: to unsupervisedly learn a fused graph from multiple graphs that preserve sufficient task-relevant information while removing task-irrelevant noise. Specifically, our proposed **In**formation-aware Unsupervised **M**ultiplex **G**raph **F**usion framework (InfoMGF) uses graph structure refinement to eliminate irrelevant noise and simultaneously maximizes view-shared and view-unique task-relevant information, thereby tackling the frontier of non-redundant multiplex graph. Theoretical analyses further guarantee the effectiveness of InfoMGF. Comprehensive experiments against various baselines on different downstream tasks demonstrate its superior performance and robustness. Surprisingly, our unsupervised method even beats the sophisticated supervised approaches. The source code and datasets are available at https://github.com/zxlearningdeep/InfoMGF.

## 1 Introduction

Multiplex graph (multiple graph layers span across a common set of nodes), as a special type of heterogeneous graph, provides richer information and better modeling capabilities, leading to challenges in learning graph representation . Recently, unsupervised multiplex graph learning (UMGL) has attracted significant attention due to its exploitation of more detailed information from diverse sources [2; 3], using graph neural networks (GNNs)  and self-supervised techniques . UMGL has become a powerful tool in numerous real-world applications [6; 7], e.g., social network mining and biological network analysis, where multiple relationship types exist or various interaction types occur.

Despite the significant progress made by UMGL, a substantial gap in understanding how to take advantage of the richness of the multiplex view is still left. In particular, a fundamental issue is largely overlooked: the reliability of graph structure. Typically, the messaging-passing mechanism in GNNs assumes the reliability of the graph structure, implying that the connected nodes tend to have similar labels. All UMGL methods are graph-fixed, assuming that the original structure is sufficiently reliable for learning [8; 9; 10; 3]. Unfortunately, there has been evidence that practicalgraph structures are not always reliable . Multiplex graphs often contain substantial amounts of less informative edges characterized by irrelevant, misleading, and missing connections. For example, due to the heterophily in the graphs, GNNs generate poor performance [12; 13; 14]. Another representative example is adversarial attacks , where attackers tend to add edges between nodes of different classes. Then, aggregating information from neighbors of different classes degrades UMGL performance. Diverging from existing approaches to node representation learning, we focus on structure learning of a new graph from multiplex graphs to better suit downstream tasks. Notably, existing Graph Structure Learning (GSL) overwhelmingly concentrated on a single homogeneous graph , marking our endeavor as pioneering in the realm of multiplex graphs.

Given the unsupervised nature, the majority of UMGL methods leverage contrastive learning mechanism [8; 9; 10], a typical self-supervised technique, for effective training. However, recent research has demonstrated that standard contrastive learning, maximizing mutual information between different views, is limited to capturing view-shared task-relevant information . This approach is effective only in multi-view redundant scenarios, thereby overlooking unique task-relevant information specific to each view. In practice, the multiplex graph is inherently non-redundant. As illustrated in Figure 1, task-relevant information resides not only in shared areas across different graph views but also in specific view-unique regions. For instance, in the real citation network ACM , certain papers on the same subject authored by different researchers may share categories and thematic relevance. This characteristic, compared to the co-author view, represents view-unique task-relevant information within the co-subject view. It exposes a critical limitation in existing UMGL methods, which potentially cannot capture sufficient task-relevant information.

Motivated by the above observations, our research goal can be summarized as follows: _how can we learn a fused graph from the original multiplex graph in an unsupervised manner, mitigating task-irrelevant noise while retaining sufficient task-relevant information?_ To handle this new task, we propose a novel Information-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF). Graph structure refinement is first applied to each view to achieve a more suitable graph with less task-irrelevant noise. Confronting multiplex graph non-redundancy, InfoMGF simultaneously maximizes the view-shared and view-unique task-relevant information to realize sufficient graph learning. A learnable graph augmentation generator is also developed. Finally, InfoMGF maximizes the mutual information between the fused graph and each refined graph to encapsulate clean and holistic task-relevant information from a range of various interaction types. Theoretical analyses guarantee the effectiveness of our approach in capturing task-relevant information and graph fusion. The unsupervised learned graph and node representations can be applied to various downstream tasks. In summary, our main contributions are three-fold:

* **Problem.** We pioneer the investigation of the multiplex graph reliability problem in a principled way, which is a more practical and challenging task. To our best knowledge, we are the first to attempt unsupervised graph structure learning in multiplex graphs.

Figure 1: (a) and (b) illustrate that in a non-redundant multiplex graph, view-specific task-relevant edges exist in certain graphs. The color of nodes represents class, edges between nodes of the same class are considered relevant edges, and ”unique” indicates that the edge exists only in one graph. (c) The unique relevant edge ratio = (the number of unique relevant edges) / (the total number of relevant edges in this graph). Each graph contains a significant amount of unique task-relevant information.

* **Algorithm.** We propose InfoMGF, a versatile multiplex graph fusion framework that steers the fused graph learning by concurrently maximizing both view-shared and view-unique task-relevant information under the multiple graphs non-redundancy principle. Furthermore, we develop two random and generative graph augmentation strategies to capture view-unique task information. Theoretical analyses ensure the effectiveness of InfoMGF.
* **Evaluation.** We perform extensive experiments against various types of state-of-the-art methods on different downstream tasks to comprehensively evaluate the effectiveness and robustness of InfoMGF. Particularly, our developed unsupervised approach even outperforms supervised methods.

## 2 Preliminaries

**Notation.** The multiplex graph is represented by \(G=\{G_{1},...,G_{V}\}\), where \(G_{v}=\{A_{v},X\}\) is the \(v\)-th graph. \(A_{v}\{0,1\}^{N N}\) is the corresponding adjacency matrix and \(X^{N d_{f}}\) is the shared feature matrix across all graphs. \(X_{i}^{d_{f}}\) is the \(i\)-th row of \(X\), representing the feature vector of node \(i\). \(N\) is the number of nodes and \(D_{v}\) is a diagonal matrix denoting the degree matrix of \(A_{v}\). \(Y\) is label information. For convenience, we use "view" to refer to each graph in the multiplex graph.

**Multiplex graph non-redundancy.** Task-relevant information exists not only in the shared information between graphs but also potentially within the unique information of certain graphs. Following the non-redundancy principle , we provide the formal definition of Multiplex Graph Non-redundancy:

**Definition 1**.: \(G_{i}\) _is considered non-redundant with \(G_{j}\) for \(Y\) if and only if there exists \(>0\) such that the conditional mutual information \(I(G_{i};Y G_{j})>\) or \(I(G_{j};Y G_{i})>\)._

**Graph structure learning.** Existing GSL methods primarily focus on a single graph. Their pipeline can be summarized as a two-stage framework : a Graph Learner takes in the original graph \(G=\{A,X\}\) to generate a refined graph \(G^{s}=\{A^{s},X\}\) with a new structure; a Graph Encoder uses the refined graph as input to obtain node representations. Note that node features generally do not change in GSL, only the graph structure is optimized. Related work is in Appendix B.

## 3 Methodology

As illustrated in Figure 2, our proposed InfoMGF consists of two modules: the _Graph Structure Refinement module_ and the _Task-Relevant Information Maximization module_.

Figure 2: The overall framework of the proposed InfoMGF. Specifically, InfoMGF first generates refined graphs and the fused graph through the graph learner. Subsequently, it maximizes shared and unique task-relevant information within the multiplex graph and facilitates graph fusion. The learned fused graph and node representations are used for various downstream tasks.

### Graph Structure Refinement

We first use a graph learner to generate each view's refined graph \(G_{v}^{s}=\{A_{v}^{s},X\}\). To retain node features and structure information simultaneously, we apply the widely used Simple Graph Convolution (SGC)  to perform aggregation in each view, resulting in view-specific node features \(X^{v}\). A view-specific two-layer attentive network is employed to model the varying contributions of different features to structure learning:

\[X^{v}=(_{v}^{-}_{v}_{v}^{-})^{ r}X, H^{v}=(X^{v} W_{1}^{v}) W_{2}^{v}\] (1)

where \(_{v}=D_{v}+I\) and \(_{v}=A_{v}+I\). \(r\) represents the order of graph aggregation. \(()\) is the non-linear activation function and \(\) denotes the Hadamard product. All rows of \(W_{1}^{v}\) are identical, representing a learnable attention vector shared by all nodes. This strategy enables us to acquire view-specific features before training, thereby circumventing the time-consuming graph convolution operations typically required by GNN-based graph learners during training, which significantly boosts our model's scalability.

Like existing GSL methods [16; 20], we apply post-processing techniques to ensure that the adjacency matrix \(A_{v}^{s}\) satisfies properties such as sparsity, non-negativity, symmetry, and normalization. Specifically, we use \(H^{v}\) to construct the similarity matrix and then sparsify it using \(k\)-nearest neighbors (\(k\)NN). For large-scale graphs, we utilize locality-sensitive approximation during \(k\)NN sparsification to reduce time complexity . Afterward, operations including Symmetrization, Activation, and Normalization are used sequentially to generate the final \(A_{v}^{s}\). Following the refinement of each view, we employ a shared Graph Convolutional Network (GCN)  as the graph encoder to obtain the node representations \(Z^{v}^{N d}\) of each view, computed by \(Z^{v}=(A_{v}^{s},X)\).

### Maximizing Shared Task-Relevant Information

\(G_{v}^{s}\) should contain not only view-shared but also view-unique task-relevant information. Following standard contrastive learning [23; 24], for each pair of distinct views (e.g., \(i\) and \(j\)), our approach seeks to maximize the mutual information \(0.5I(G_{i}^{s};G_{j})+0.5I(G_{j}^{s};G_{i})\) to capture shared task-relevant information between views.

**Proposition 1**.: _For any view \(i\) and \(j\), \(2I(G_{i}^{s};G_{j}^{s})\) is the lower bound of \(I(G_{i}^{s};G_{j})+I(G_{j}^{s};G_{i})\)._

Detailed proofs are provided in the Appendix D. According to Proposition 1, the maximization objective can be transformed to a tractable lower bound \(I(G_{i}^{s};G_{j}^{s})\). Considering the addition of mutual information for each pair, the loss term for minimization can be expressed as follows:

\[_{s}=-_{i=1}^{V}_{j=i+1}^{V}I(G_{i}^{s};G_{ j}^{s})\] (2)

### Maximizing Unique Task-Relevant Information

Maximizing view-unique task-relevant information can be rigorously expressed as maximizing \(I(G_{i}^{s};Y|_{j i}G_{j})\). Then, we relax the optimization objective to the total task-relevant information within the view, \(I(G_{i}^{s};Y)\). This decision is based on the following considerations: on the one hand, deliberately excluding shared task-relevant information is unnecessary and would complicate the optimization process. On the other hand, repeated emphasis on shared task-relevant information encourages the model to focus more on it in the early training stage.

The unsupervised nature of our task dictates that we cannot directly optimize \(I(G_{i}^{s};Y)\) using label information. Some typical graph learning methods often reconstruct the graph structure to preserve the maximum amount of information from the original data [25; 26; 27]. In the context of our task, this reconstruction-based optimization objective is equivalent to maximizing the mutual information with the original graph structure [28; 29], i.e., \(I(G_{i}^{s};G_{i})\). However, such methods have significant drawbacks: they retain task-irrelevant information from the original data, and the graph reconstruction also entails high complexity. In contrast, we leverage graph augmentation to reduce task-irrelevant information and retain task-relevant information without accessing \(Y\). Following the optimal augmentation assumption [17; 30], we define optimal graph augmentation as:

**Definition 2**.: \(G^{}_{i}\) _is an optimal augmented graph of \(G_{i}\) if and only if \(I(G^{}_{i};G_{i})=I(Y;G_{i})\), implying that the only information shared between \(G_{i}\) and \(G^{}_{i}\) is task-relevant without task-irrelevant noise._

**Theorem 1**.: _If \(G^{}_{i}\) is the optimal augmented graph of \(G_{i}\), then \(I(G^{*}_{i};G^{}_{i})=I(G^{*}_{i};Y)\) holds._

**Theorem 2**.: _The maximization of \(I(G^{*}_{i};G^{}_{i})\) yields a discernible reduction in the task-irrelevant information relative to the maximization of \(I(G^{*}_{i};G_{i})\)._

Theorem 1 theoretically guarantees that maximizing \(I(G^{*}_{i};G^{}_{i})\) would provide clean and sufficient task-relevant guidance for learning \(G^{*}_{i}\). Theorem 2 demonstrates the superiority of our optimization objective over typical methods in removing task-irrelevant information. Therefore, given \(G^{}_{i}=\{A^{}_{i},X^{}\}\) for each view, where \(A^{}_{i}\) and \(X^{}\) denote the augmented adjacency matrix and node features, respectively, the loss term \(_{u}\) is defined as:

\[_{u}=-_{i=1}^{V}I(G^{*}_{i};G^{}_{i})\] (3)

The key to the above objective lies in ensuring that \(G^{}_{i}\) satisfies the optimal graph augmentation. However, given the absence of label information, achieving truly optimal augmentation is not feasible; instead, we can only rely on heuristic techniques to simulate it. Consistent with most existing graph augmentations, we believe that task-relevant information in graph data exists in both structure and feature, necessitating augmentation in both aspects. We use random masking, a simple yet effective method, to perform feature augmentation. For graph structure, we propose two versions: random edge dropping and learnable augmentation through a graph generator.

**Random feature masking.** For node features, we randomly select a fraction of feature dimensions and mask them with zeros. Formally, we sample a random vector \(\{0,1\}^{d_{f}}\) where each dimension is drawn from a Bernoulli distribution independently, i.e., \(_{i} Bern(1-)\). Then, the augmented node features \(X^{}\) is computed by \(X^{}=[X_{1};X_{2};...;X_{N}]^{}\).

**Random edge dropping (InfoMGF-RA).** For a given \(A_{v}\), a masking matrix \(M\{0,1\}^{N N}\) is randomly generated, where each element \(M_{ij}\) is sampled from a Bernoulli distribution. Afterward, the augmented adjacency matrix can be computed as \(A^{}_{v}=A_{v} M\).

**Learnable generative augmentation (InfoMGF-LA).** Random edge dropping may lack reliability and interpretability. A low dropping probability might not suffice to eliminate task-irrelevant information, while excessive deletions could compromise task-relevant information. Therefore, we opt to use a learnable graph augmentation generator. To avoid interference from inappropriate structure information, we compute personalized sampling probabilities for existing edges in each view by employing a Multilayer Perceptron (MLP) in the node features. To ensure the differentiability of the sampling operation for end-to-end training, we introduce the Gumbel-Max reparametrization trick  to transform the discrete binary (0-1) distribution of edge weights into a continuous distribution. Specifically, for each edge \(e_{i,j}\) in view \(v\), its edge weight \(^{v}_{i,j}\) in the corresponding augmented view is computed as follows:

\[^{v}_{i,j}=([WX_{i};WX_{j}]),^{v }_{i,j}=((-(1-)+^{v}_{i,j})/)\] (4)

where \([;]\) denotes the concatenation operation and \((0,1)\) is the sampled Gumbel random variate. We can control the temperature hyper-parameter \(\) approaching \(0\) to make \(^{v}_{i,j}\) tend towards a binary distribution. For an effective augmented graph generator, it should eliminate task-irrelevant noise while retaining task-relevant information. Therefore, we design a suitable loss function for augmented graph training:

\[_{gen}=_{i=1}^{V}_{j=1}^{N}(1-_{j})^{}^{i}_{j}}{\|X^{i}_{j}\|\|^{i}_{j}\|} )+*_{i=1}^{V}I(G^{*}_{i};G^{}_{i})\] (5)

where \(\) is a positive hyper-parameter. The first term reconstructs view-specific features using the cosine error, guaranteeing that the augmented views preserve crucial task-relevant information while having lower complexity compared to reconstructing the entire graph structure. The reconstructed features \(^{i}\) are obtained using an MLP-based Decoder on the node representations \(Z^{i^{}}\) of the augmented view. The second term **minimizes**\(I(G^{*}_{i};G^{}_{i})\) to regularize the augmented views simultaneously, ensuring that the augmented graphs would provide only task-relevant information as guidancewith less task-irrelevant noise when optimizing the refined graph \(G^{s}_{i}\) through Eq.(3). Note that for InfoMGF-LA, we adopt an iterative optimization strategy to update \(G^{s}_{i}\) and \(G^{}_{i}\) alternatively, as described in Section 3.4.

Although previous work also employs similar generative graph augmentation , we still possess irreplaceable advantages in comparison. Firstly, they merely minimize mutual information to generate the augmented graph, lacking the crucial information retention component, which may jeopardize task-relevant information. Furthermore, an upper bound should ideally be used for minimization, whereas they utilize a lower bound estimator for computation, which is incorrect in optimization practice. In contrast, we use a rigorous upper bound of mutual information for the second term of \(_{gen}\), which is demonstrated later.

### Multiplex Graph Fusion

The refined graph retains task-relevant information from each view while eliminating task-irrelevant noise. Afterward, we learn a fused graph that encapsulates sufficient task-relevant information from all views. Consistent with the approach in Section 3.1, we leverage a scalable attention mechanism as the fused graph learner:

\[H=([X;X^{1};X^{2};;X^{V}] W^{1}) W^{2}, _{f}=-_{i=1}^{V}I(G^{s};G^{s}_{i})\] (6)

where the node features are concatenated with all view-specific features as input. The same post-processing techniques are sequentially applied to generate the fused graph \(G^{s}=\{A^{s},X\}\). The node representations \(Z\) of the fused graph are also obtained through the same GCN. We maximize the mutual information between the fused graph and each refined graph to incorporate task-relevant information from all views, denoted as loss \(_{f}\). The total loss \(\) of our model can be expressed as the sum of three terms: \(=_{s}+_{u}+_{f}\).

**Theorem 3**.: _The learned fused graph \(G^{s}\) contains more task-relevant information than the refined graph \(G^{s}_{i}\) from any single view. Formally, we have:_

\[I(G^{s};Y)_{i}I(G^{s}_{i};Y)\] (7)

Theorem 3 theoretically proves that the fused graph \(G^{s}\) can incorporate more task-relevant information than considering each view individually, thus ensuring the effectiveness of multiplex graph fusion.

**Optimization.** Note that all the loss terms require calculating mutual information. However, directly computing mutual information between two graphs is impractical due to the complexity of graph-structured data. Since we focus on node-level tasks, we assume the optimized graph should guarantee that each node's neighborhood substructure contains sufficient task-relevant information. Therefore, this requirement can be transferred into mutual information between node representations , which can be easily computed using a sample-based differentiable lower/upper bound. For any view \(i\) and \(j\), the lower bound \(I_{lb}\) and upper bound \(I_{ub}\) of the mutual information \(I(Z^{i};Z^{j})\) are :

\[I_{lb}(Z^{i};Z^{j})=_{z^{i},z^{j+} p(z^{i},z^{j})\\ z^{j} p(z^{j})}[log,z^{j+})}{_{N} expf(z^{i},z^{j})}]\] (8)

\[I_{ub}(Z^{i};Z^{j})=_{z^{i},z^{j+} p(z^{i}, z^{j})\\ z^{j} p(z^{j})}[f^{*}(z^{i},z^{j+})]-_{ z^{i} p(z^{i})\\ z^{j} p(z^{j})}[f^{*}(z^{i},z^{j})]\] (9)

where \(f(,)\) is a score critic approximated by a neural network and \(f^{*}(,)\) is the optimal critic from \(I_{lb}\) plugged into the \(I_{ub}\) objective. \(p(z^{i},z^{j})\) denotes the joint distribution of node representations from views \(i\) and \(j\), while \(p(z^{i})\) denotes the marginal distribution. \(z^{i}\) and \(z^{j+}\) are mutually positive samples, representing the representations of the same node in views \(i\) and \(j\) respectively.

To avoid too many extra parameters, the function \(f(z^{i},z^{j})\) is implemented using non-linear projection and cosine similarity. Each term in the total loss \(\) maximizes mutual information, so we use the lower bound estimator for the calculation. In contrast, we use the upper bound estimator for the generator loss \(_{gen}\) in InfoMGF-LA, which minimizes mutual information. These two losses can be expressed as follows:

\[=-_{i=1}^{V}_{j=i+1}^{V}I_{lb}(Z^{i};Z^{j})- _{i=1}^{V}I_{lb}(Z^{i};Z^{i^{}})-_{i=1}^{V}I _{lb}(Z;Z^{i})\] (10)

\[_{gen}=_{i=1}^{V}_{j=1}^{N}(1- ^{i})^{}_{j}^{i}}{\|X_{j}^{i}\|\|_{j}^{i}\|})+ *_{i=1}^{V}I_{ub}(Z^{i};Z^{i^{}})\] (11)

Finally, we provide the InfoMGF-LA algorithm in Appendix C.1. In Step 1 of each epoch, we keep the augmented graph fixed and optimize both the refined graphs and the fused graph using the total loss \(\), updating the parameters of Graph Learners and GCN. In Step 2, we keep the refined graphs fixed and optimize each augmented graph using \(_{gen}\), updating the parameters of the Augmented Graph Generator and Decoder. After training, \(G^{s}\) and \(Z\) are used for downstream tasks.

## 4 Experiments

In this section, our aim is to answer three research questions: **RQ1:** How effective is InfoMGF for different downstream tasks in unsupervised settings? **RQ2:** Does InfoMGF outperform baselines of various types under different adversarial attacks? **RQ3:** How do the main modules influence the performance of InfoMGF?

### Experimental Setups

**Downstream tasks.** We evaluate the learned graph on node clustering and node classification tasks. For node clustering, following , we apply the K-means algorithm on the node representations \(Z\) of \(G^{s}\) and use the following four metrics: Accuracy (ACC), Normalized Mutual Information (NMI), F1 Score (F1), and Adjusted Rand Index (ARI). For node classification, following the graph structure learning settings in , we train a new GCN on \(G^{s}\) for evaluation and use the following two metrics: Macro-F1 and Micro-F1.

**Datasets.** We conduct experiments on four real-world benchmark multiplex graph datasets, which consist of two citation networks (i.e., ACM  and DBLP ), one review network Yelp  and a large-scale citation network MAG . Details of datasets are shown in Appendix E.1.

**Baselines.** For node clustering, we compare InfoMGF with two single-graph methods (i.e., VGAE  and DGI ) and seven multiplex graph methods (i.e., O2MAC , MvAGC , MCGC , HDMI , MGDCR , DMG , and BTGF ). All the baselines are unsupervised clustering methods. For a fair comparison, we conduct single-graph methods separately for each graph and present the best results.

For node classification, we compare InfoMGF with baselines of various types: three supervised structure-fixed GNNs (i.e., GCN , GAT  and HAN ), six supervised GSL methods (i.e., LDS , GRCN , IDGL , ProGNN , GEN  and NodeFormer ), three unsupervised GSL methods (i.e., SUBLIME , STABLE  and GSR ), and three structure-fixed UMGL methods (i.e., HDMI , DMG  and BTGF ). GCN, GAT, and all GSL methods are single-graph approaches. For unsupervised GSL methods, following , we train a new GCN on the learned graph for node classification. For UMGL methods, following , we train a linear classifier on the learned representations. Implementation details can be found in Appendix E.2.

### Effectiveness Analysis (RQ1)

Table 1 presents the results of node clustering. Firstly, multiplex graph clustering methods outperform single graph methods overall, demonstrating the advantages of leveraging information from multiple sources. Secondly, compared to other multiplex graph methods, both versions of our approach surpass existing state-of-the-art methods. This underscores the efficacy of our proposed graph structure learning, which eliminates task-irrelevant noise and extracts task-relevant information from all graphs, to serve downstream tasks better. Finally, InfoMGF-LA achieves notably superior results, owing to the exceptional capability of the learnable generative graph augmentation in capturing view-unique task-relevant information.

[MISSING_PAGE_FAIL:8]

to the nearly fully connected PSP view, InfoMGF significantly reduces inter-class edges, reflecting our effective removal of task-irrelevant noise. Compared to the PAP view, InfoMGF introduces more intra-class edges, benefiting from capturing shared and unique task-relevant information from all graphs. Furthermore, varying edge weights in \(G^{s}\) represent different importance levels, better serving downstream tasks. In summary, the above experiment results across various downstream tasks demonstrate the effectiveness of InfoMGF. We use the InfoMGF-LA version in the subsequent sections to conduct more comprehensive analyses.

### Robustness Analysis (RQ2)

To evaluate the robustness of InfoMGF against random noise, we perturb each graph on the ACM dataset by randomly adding edges, deleting edges, and masking features. We compare InfoMGF against various baselines: structure-fixed method (GCN), GSL method (SUBLIME), and UMGL method (HDMI). From Figure 3(a) and 3(b), it is evident that with increasing rates of edge perturbing, the performance of each method deteriorates, while the GSL methods (i.e., InfoMGF and SUBLIME) exhibit better robustness. Notably, InfoMGF **consistently outperforms all other methods** across both experimental settings, especially when the perturbation rate is extremely high.

Figure 3(c) shows the performance of InfoMGF and various baselines when injecting random feature noise. It can be observed that InfoMGF exhibits excellent robustness against feature noise, while the performance of SUBLIME degrades rapidly. As a single graph structure learning method, SUBLIME's performance heavily relies on the quality of node features. In contrast, our method can directly optimize task-relevant information in multi-view graph structures (e.g., edges shared across multiple graphs are likely to share task-relevant information, which can be directly learned through \(_{s}\)), thus reducing dependence on node features. Consequently, InfoMGF demonstrates superior robustness against various types of noise.

### Ablation Study (RQ3)

To verify the effectiveness of each part of InfoMGF, we design four variants and compare the classification performance against InfoMGF.

_Effectiveness of loss components._ Recall InfoMGF maximizes view-shared and unique task-relevant information by \(_{s}\) and \(_{u}\). Thus, we design two variants (w/o \(_{s}\) and w/o \(_{u}\)). Table 3 shows the necessity of each component. Furthermore, we can observe that the removal of \(_{u}\) has a greater impact compared to \(_{s}\), which can be explained by the fact that optimization of \(_{u}\) actually maximizes the overall task-relevant information of each view, rather than the unique aspects of the view.

Figure 4: Robustness analysis on ACM.

    &  &  &  \\  & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 \\  w/o \(_{s}\) & 93.05\(\)0.49 & 92.98\(\)0.49 & 90.44\(\)0.45 & 91.39\(\)0.41 & 93.15\(\)0.12 & 92.11\(\)0.13 \\ w/o \(_{u}\) & 92.66\(\)0.53 & 92.61\(\)0.51 & 90.13\(\)0.43 & 91.05\(\)0.44 & 92.23\(\)0.27 & 90.96\(\)0.36 \\  w/o Aug. & 92.84\(\)0.17 & 92.81\(\)0.16 & 90.94\(\)0.45 & 91.81\(\)0.41 & 92.76\(\)0.49 & 91.63\(\)0.51 \\ w/o Rec. & 92.91\(\)0.53 & 92.88\(\)0.51 & 91.05\(\)0.27 & 91.87\(\)0.23 & 92.65\(\)0.27 & 91.45\(\)0.37 \\  InfoMGF & 93.42\(\)0.21 & 93.35\(\)0.21 & 91.28\(\)0.31 & 92.12\(\)0.28 & 93.26\(\)0.26 & 92.24\(\)0.34 \\   

Table 3: Performance (\(\%\)) of InfoMGF and its variants.

_Effectiveness of augmentation module._ The InfoMGF-LA framework incorporates learnable generative augmentation and maximizes the mutual information \(I(G_{i}^{s};G_{i}^{})\) to mine the task-relevant information. We first compare InfoMGF with maximizing the mutual information \(I(G_{i}^{s};G_{i})\) with the original graph structure without augmentation (w/o Aug.). Furthermore, we remove the reconstruction loss term (w/o Rec.) of \(_{gen}\) to analyze the necessity of preserving crucial information. The results show that maximizing \(I(G_{i}^{s};G_{i})\) leads to poorer performance compared to \(I(G_{i}^{s};G_{i}^{})\), consistent with Theorem 2. Meanwhile, deleting the reconstruction term from \(_{gen}\) also results in the augmented graph lacking task-relevant information, thus hurting model performance.

### Node Correlation Visualization

We further visualize the node correlation in the learned representations \(Z\) of the fused graph, which is used in the clustering task. Figure 5 shows the node correlation heatmaps of the representations, where both rows and columns are reordered by the node labels. In the heatmap, warmer colors signify a higher correlation between nodes. It is evident that the correlation among nodes of the same class is significantly higher than that of nodes from different classes. This is due to \(G^{s}\) mainly containing intra-class edges without irrelevant inter-class edges, which validates the effectiveness of InfoMGF in unsupervised graph structure learning.

## 5 Conclusion and Limitation

This paper delves into the unsupervised graph structure learning within multiplex graphs for the first time. The proposed InfoMGF refines the graph structure to eliminate task-irrelevant noise, while simultaneously maximizing both the shared and unique task-relevant information across different graphs. The fused graph applied to downstream tasks is optimized to incorporate clean and comprehensive task-relevant information from all graphs. Theoretical analyses and extensive experiments ensure the effectiveness of InfoMGF. A limitation of our research lies in its focus solely on the pure unsupervised scenario. In some real-world scenarios where partial node labels are available, label information can be used to learn a better structure of multiplex graphs. Such supervised or semi-supervised problems are left for future exploration.