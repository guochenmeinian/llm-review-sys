# Label Privacy in Split Learning for Large Models with Parameter-Efficient Training

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

As deep learning models become larger and more expensive, many practitioners turn to fine-tuning APIs. These web services allow fine-tuning a model between two parties: the client that provides the data, and the server that hosts the model. While convenient, these APIs raise a new concern: the data of the client is at risk of privacy breach during the training procedure. This challenge presents an important practical case of vertical federated learning, where the two parties perform parameter-efficient fine-tuning (PEFT) of a large model. In this study, we systematically search for a way to fine-tune models over an API _while keeping the labels private_. We analyze the privacy of LoRA, a popular approach for parameter-efficient fine-tuning when training over an API. Using this analysis, we propose P\({}^{3}\)EFT, a multi-party split learning algorithm that takes advantage of existing PEFT properties to maintain privacy at a lower performance overhead. To validate our algorithm, we fine-tune DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B using LoRA adapters on a range of NLP tasks. We find that P\({}^{3}\)EFT is competitive with existing privacy-preserving methods in multi-party and two-party setups while having higher accuracy.

## 1 Introduction

One of the main reasons behind deep learning success is its ability to transfer knowledge between tasks . When training a model for any particular problem, it is common to reuse previously trained models from other, related problems. In the past, this was typically done by downloading pre-trained model weights from public hubs, then fine-tuning the said models on the downstream task. However, as models grow larger and more compute-intensive, fine-tuning them locally becomes an increasingly difficult task. Furthermore, many recent models are not released, but instead made available as proprietary services.

When a model cannot be fine-tuned locally, many practitioners opt instead for the so-called fine-tuning APIs [27; 16; 6; 26]. These APIs are web services that host one or several pre-trained models and allow clients to perform limited fine-tuning. More specifically, APIs usually allow their clients to run parameter-efficient fine-tuning (PEFT), such as LoRA  or Prefix-tuning . These techniques allow adapting a model to a dataset while training a relatively small number of additional weights, which is particularly important for large language or image generation models that have billions of parameters.

Although the fine-tuning APIs can be convenient, they also introduce new risk in terms of data privacy. When a client uses such API to train on sensitive data, they need to ensure that their data will stay private . This is particularly important when dealing with patient's medical records, personal user data or trade secrets [24; 19]. The two main threats to data privacy are that the API provider obtains the private data and that a third party intercepts data in transit. Therefore, data privacy isnot guaranteed even if the API provider is trusted. Several recent works propose LLM fine-tuning protocols that establish a certain level of privacy for multi-party fine-tuning [42; 7; 22]. Unfortunately, these algorithms work for a narrow class of fine-tuning algorithms or assume that a client can run LLM training locally using an obfuscated version of the model, provided by a remote server . As a result, these algorithms are impractical for our use case of fine-tuning over an API. The few algorithms that are suitable for API fine-tuning guarantee the privacy of input tokens , meaning that the attacker can infer private training _labels_.

In this work, we seek to alleviate this problem by designing a two-party fine-tuning protocol that performs standard parameter-efficient fine-tuning with privacy guarantees. We formulate our protocol as a **special case of split learning** (or vertical federated learning), where one side (server) holds the pre-trained model and the other (client) has private training data. More specifically, we focus on **the privacy of client's training labels**. While input privacy is often crucial, there are scenarios where input data is publicly available, such as social media user pages. In these cases, labels could include ad clicks (known to the social network) or financial information (known to a bank that matches social profiles to its internal records). This example further justifies the use of LLMs, as social media pages often contain substantial amounts of text, and LLMs excel at processing long-context data.

Instead of developing a specific privacy-preserving architecture, we seek algorithms that can work with popular existing models and PEFT algorithms. Furthermore, our approach relies on the properties of parameter-efficient fine-tuning. Notably, since the adapters are compact, both parties can maintain multiple sets of adapters and swap between them with relative ease. This allows us to design a PEFT-specific algorithm that can solve its task more effectively than general split learning strategies .

We summarize our main contributions as follows:

* We analyze Low-Rank Adaptation, a common parameter-efficient fine-tuning algorithm, from the perspective of label privacy in the split learning setup. We observe that, despite fine-tuning less than \(0.1\%\) of model parameters, PEFT algorithms leak client's training labels against simple attacks that work for modern pretrained transformers.
* Based on our analysis, we formulate a framework for privacy-preserving parameter-efficient fine-tuning (P\({}^{3}\)EFT). This framework leverages the properties of PEFT to obfuscate the gradients and parameters communicated during fine-tuning with little impact on the fine-tuned model quality.
* To verify the practical viability of P\({}^{3}\)EFT, we conduct experiments on popular real-world PEFT workloads1. Specifically, we fine-tune DeBERTa-v2-XXL , Flan-T5-Large  and LLaMA-2 7B  on a set of standard language understanding problems. We find that, compared to prior split learning algorithms, P\({}^{3}\)EFT can maintain label privacy throughout training with a significantly smaller accuracy drop. 
## 2 Background

### Federated learning and split learning

Privacy preservation in machine learning has been a subject of active study within several frameworks. An important branch of privacy-preserving learning methods is federated learning, or FL , which can be broadly described as an approach allowing several parties to train a model jointly without sharing their private data. In particular, vertical federated learning [12; 43] targets the scenario where different features (including the label) of each training instance are kept by different parties.

One of the most popular approaches to vertical FL for neural networks is split learning [10; 37], where each party stores its part of the overall model. To train the model in such an approach, it is only necessary to transfer the intermediate activations and the gradients between layers, while the data itself is stored at the premises of the participant hosting each layer. In this work, we focus on the two-party formulation of split learning, where one side stores the features for each example and another one stores the labels.

Recent works have investigated the setting of two-party split learning from the label leakage perspective [38; 28]: because the label party needs to pass the gradients of the loss function to the non-label party, it is possible for the latter party to deduce the labels by inspecting the gradients or activations or by hijacking the training procedure. Li et al.  provide a set of attack methods that allow recovering private labels and propose a defense mechanism that injects noise into the gradients; however, they test the approach on pretraining smaller models, and we study finetuning large models on private downstream data.

### Parameter-efficient finetuning

The majority of large neural networks today are not trained with a specific task in mind: instead, they are pretrained on a general objective and then adapted for the downstream problem. Importantly, the growth in the size of foundation models has led to the increased popularity of parameter-efficient finetuning (PEFT) methods that adapt the model to a given task by training a small number of task-specific parameters. There are several prominent approaches to parameter-efficient finetuning, ranging from trainable prompts [21; 11], to residual adapters [14; 29]. We focus on Low-Rank Adaptation (or LoRA, 15), one of the most popular PEFT methods that adds extra parameters to each weight matrix in the form of a low-rank factorization (see Appendix C for a more detailed description). Such formulation allows LoRA adapters to be merged into the original weights after finetuning; this ability, combined with the simplicity of the method, has made LoRA a broadly popular approach in multiple domains. Still, the approach we propose can be applied to any PEFT method.

Several recent lines of work explore the problem of fine-tuning LLMs with privacy guarantees [44; 31]. Zhao et al.  analyze the viability of prompt tuning for federated learning, and Zhang et al. , Liu et al.  study PEFT algorithms in the setting of _horizontal_ federated learning, that is, where multiple users train a shared model on their local private data. Another, more relevant research direction considers private fine-tuning in a _vertical_ federated learning scenario, where participants hold different model layers [22; 40]. Most of these studies leverage the idea of differential privacy to prove an upper bound on how much information is leaked . Unfortunately, these upper bounds are typically loose and do not match practical observations for real models. Furthermore, the majority of these studies only guarantees privacy of specific parts of the training procedure: for instance, Li et al.  only protects the input features, and not labels or model parameters. Finally, Xiao et al.  presents an alternative algorithm that protects client data by running the entire fine-tuning on client side by emulating the server-side model layers. While this approach is more holistic, it assumes that clients can run fine-tuning locally, which makes it impractical for many real-world users of LLM fine-tuning APIs. The primary distinction between our work and these studies is that we investigate parameter-efficient adaptation in the setting of split learning: we aim to finetune a model without disclosing the labels of examples to the model provider.

## 3 Privacy-preserving parameter-efficient fine-tuning

In this section, we analyze the privacy of parameter-efficient fine-tuning and propose a protocol for two-party parameter-efficient fine-tuning with the desired privacy guarantees. We begin by analyzing the privacy of API fine-tuning with popular PEFT algorithms in Sections 3.1 and 3.2. Then, in Section 3.3, we formulate a protocol for privately computing gradients over fine-tuning APIs. Finally, we formulate the full P\({}^{3}\)EFT protocol in Section 3.4.

### Setup

To analyze the privacy of API fine-tuning, we first need to formulate a common framework for this type of APIs and develop private learning protocols. This step is important, because existing fine-tuning APIs greatly vary in what they offer to the client: from closed APIs that require users to submit their full training data  to more flexible APIs where clients can run individual training steps [20; 2; 30]. Similarly to most existing works on split learning, we focus on the latter type of APIs that allows clients to run individual forward and backward passes over a remote model. Thus, a client can use these APIs to obtain the training gradients for their PEFT adapters, then update adapters locally with any optimization method. In our work, we adopt this archetype of fine-tuning API as it offers sufficient flexibility to develop privacy-preserving algorithms.

We formulate fine-tuning over an API for two or more parties: a client, and one or several servers. The client owns a training dataset with inputs \(X\) and labels \(Y\). In turn, each server has the same pre-trained model \(h(x_{i},)^{d}\). Note that the parameters \(\) denote not the pre-trained model weights, but the trainable adapter weights for a certain PEFT algorithm. A model can encode an input \(x_{i} X\) and produce a \(d\)-dimensional vector of activations that depend on the learned adapter weights \(\).

To allow fine-tuning, a server offers two API methods:

1. \((x,) h(x,)\) that computes model activations on input \(x\) using adapter weights \(\);
2. \((x,,g_{h}) g_{}\) that receives gradients of an arbitrary loss function w.r.t. model activations \(g_{h}=\) and returns the gradients w.r.t. adapter parameters, \(g_{}=\).

We further assume that both \(()\) and \(()\) APIs are stateless and deterministic, i.e. calling the same API method multiple times (or on multiple servers) with the same inputs produces identical results. Thus, if the model uses dropout or any other form of non-determinism, we assume that clients provide the random seed as a part of \(x\).

To fine-tune a model with this API, a client can initialize adapters locally, alongside with a small task-specific head2, then train both adapters and the head. For each training batch \((x,y) D\), a client calls \((x,)\) to compute feature representations, then predicts with local "head" and computes task-specific loss function \(L\). After that, a client performs backward pass: first, it computes gradients w.r.t. local head inputs \(g_{h}=\), then passes those gradients to a remote server via \((x,,g_{h})\) API call to compute gradients w.r.t. \(\). Finally, a client updates both \(\) and local "head" parameters using the optimizer of choice.

Before building more advanced algorithms, let us analyze the privacy of client's labels under standard fine-tuning. We consider an "honest, but curious" attacker model. This means that the server will faithfully run the forward and backprop computations as requested by the client without changing the results. Furthermore, we assume that servers are independent and do not communicate client's data between each other. However, a server can recover client's labels by performing arbitrary computations using any information it receives from the client. When training in this way, a client does not directly communicate training labels to the server. However, it communicates inputs, adapter parameters, and gradients. Furthermore, the server communicates input representations that can be intercepted by a third party.

### Label Leakage of Standard Split Learning

In Figure 1, we train a DeBERTa-v2-XXL model on the SST-2  sentiment classification dataset. The top row depicts the gradients \(g_{h}\) communicated by the client when calling \(()\) at different training stages. In the bottom row, we similarly track activations \(h(x,)\) that server may compute based on the specified \(x,\). We defer further additional figures and details to Section 4.1.

As we can see, both gradients and activations are arranged in such a way that simple k-means clustering would reveal which objects have the same label. The training activations (bottom row) do

Figure 1: A visualization of top-2 principal components of gradients (top) and activations (bottom) from different fine-tuning steps (left to right). Color indicates the training labels (binary).

not reveal labels right away (at least not against this attack). However, they gradually "leak" private label information during training. Informally, it appears that the training gradients gradually pull apart the feature representations for each label, until eventually they turn into separate clusters. From an information-theoretic perspective, knowing just one vector of gradients _or_ trained activations allows the attacker to learn all but one bit3 of information about client's private labels.

To summarize, leaving any _one_ data source unprotected (gradients, activations or parameters) would already compromise label privacy. However, we found that gradients and activations require different means of protection.

### Privacy-preserving backpropagation

In this section, we formulate an algorithm for "anonymizing" the gradients communicated over a single training step with arbitrary PEFT type. Several prior works approach this by modifying the training objective or model architecture. However, when dealing with a real-world PEFT workload with optimized hyperparameters, changing the model or loss function often results in reduced model accuracy4. Thus, we seek an algorithm that preserves both model and training objective.

We design our algorithm based on an observation that **backpropagation is conditionally linear in output gradients**, even when the model itself is nonlinear. Formally, if we take a model \(h(,)\), a fixed set of trainable parameters \(\) and input samples \(x\), the backprop function5 computes backprop\((x,,)=\). For convenience, we shorten it to backprop\((x,,g_{h})=g_{}\), where \(g_{h}=\) represents the gradients of some objective function with respect to model activations (outputs), and \(g_{}=\) are gradients of the same objective function w.r.t. trainable parameters. In this notation, backprop is linear in terms of \(g_{h}\) for any fixed \(x,\).

This becomes self-evident if we view backprop as multiplying \(g_{h}\) by the Jacobian of model outputs w.r.t. trainable parameters, \(\). If \(x,\) are constant, the Jacobian is also constant, and backprop is a linear operator:

\[(x,,)== .\] (1)

This observation allows us to design a private backpropagation protocol. To illustrate this protocol, let us first consider a distributed API with two identical independent servers that offer backprop API. Then, for arbitrary vector \(z\), we can rewrite \((x,,g_{h})\) as \((x,,g_{h}+z)+(x,,g_{h}-z)\).

During API fine-tuning, we obtain \((x,,g_{h}+z)\) using an API call to server 1, whereas the second term \((x,,g_{h}-z)\) translates to an API call to server 2. Note that neither of two servers has access to the true gradient \(g_{h}\): they only receive the sum \([g_{h}+z]\). If we sample a large noise vector \(z\) (\((z)\|g_{h}\|_{2}^{2}\)), this sum also becomes dominated by noise. However, when both API calls finish, a client can sum the results to recover the true gradient of the loss with respect to parameters.

If both requests are processed by the same server, it can obviously recover \(g_{h}\) by adding up gradients from both calls, which leads us to the final step. Instead of generating a single noise vector, a client

Figure 2: An intuitive illustration of the proposed fine-tuning protocol.

needs to generate (privately) a set of \(m>1\) random vectors \(^{1}_{h},,^{m}_{h}\) and scalars \(_{1},,_{m}\) such that

\[g_{h}=_{i=1}^{m}_{i}^{i}_{h}.\] (2)

Then, for each \(^{i}_{h}\), client computes \((x,,^{i}_{h})\) as \(m\) parallel API calls. Once this is done, client recovers

\[g_{}=_{i=1}^{m}_{i}(x,,^{i}_{ h}).\] (3)

Note that the client does not reveal \(_{1},,_{m}\) to anyone.

The resulting procedure is formulated in Algorithm 1. This algorithm is conceptually similar to the secure aggregation protocol for conventional (horizontal) federated learning . This protocol allows clients to average their local vector with peers while keeping each individual vector provably private. Similarly to our scheme, clients perturb the vector in such a way that the average of perturbed vectors remains the same. Unlike Bonawitz et al. , our protocol privately backpropagates through a server-hosted model by leveraging the conditional linearity of the backpropagation operator.

```
1:Input:\(x\) inputs, \(\) adapter weights, \(g_{h}\) gradients w.r.t. activations, \(m>1\) - number of passes
2:\(^{1}_{h},,^{m}_{h},_{1},,_{m}=(g_{h},m)\)\(\) 2
3:for\(j=1,,m\)do
4:\(^{j}_{}=(x,,^{j}_{h})\)\(\) computed by server
5:endfor
6:\(g_{}=_{j=1}^{m}_{j}^{j}_{}\)
7:Return:\(g_{}\) ```

**Algorithm 1** private_backprop -- Privacy-Preserving Backpropagation (from the client's perspective)

The private backpropagation algorithm can allow client to safely compute gradients _once_, but, in practice, client usually needs to run many consecutive steps. This creates an additional vector of attack: if the same server receives two sets of parameters \(_{t},_{t+1}\), they could potentially recover \(g_{}\) by inverting the optimizer.

In the simplest case, if the server somehow knows that the client computes \(_{t+1}=_{t}- g_{}\), then they can compute \(g_{}=(_{t}-_{t+1})/\). While \(g_{}\) does not necessarily leak private labels, a server could, in some cases, use \(g_{}\) to recover \(g_{h}\), either fully (e.g. if Jacobian is invertible), or partially.

The client has two ways to prevent this attack. The first one is to ensure that no single server runs backprop on two consecutive steps. This is easy to do in decentralized systems where there are many potential servers. However, even when there is a single server, they could be required to set up multiple trusted execution environments . A more risky alternative is to ensure that the gradients cannot be reversed from consecutive parameters: randomize initial optimizer statistics or add noise to parameters. This solution is easier, but it can slow down training in some cases.

To summarize, we formulated a procedure that allows a client to compute gradients privately for any given model and PEFT type. Furthermore, since Equation 3 recovers true gradients, this obfuscation method does not affect the training dynamics. However, as we have shown in Section 3.1, gradients are not the only source of privacy leakage.

### Full fine-tuning

The other major attack vector are training activations. As the model fits to training data, it's intermediate activations \(h(x,)\) allow attackers to recover labels, e.g. by clustering (see Figure 1). To combat this issue, we take advantage of the fact that PEFT has few trainable parameters. Instead of learning just one set of trainable parameters, a client creates \(n\) independent adapter sets \(_{1},...,_{n}\). Note that this does not require \(n\) unique servers: a single server can run multiple sets of adapters. Furthermore, a client can alternate between using different servers for the same adapters. During forward pass, the outputs of different adapters are mixed together using randomized mixing weights \(W^{n,d}\):

\[h^{}(x,_{1},,_{n})=_{i=1}^{n}W_{i} h(x, _{i})\] (4)Overall, we design this model in such a way the combined model \(h^{}\) can predict the labels, but the adapters \(h(x,_{i})\) do not allow predicting these labels without knowing the mixing weights W. The mixing weights are generated such that initial activations \(h^{}(x,)\) are equal to mean \(h(x,)\) for all \(x\). To achieve this, we generate W as follows: first, we generate \(n(n-1)/2\) d-dimensional random vectors \(_{i,j}^{d} i[1,n],j[i+1,n]\). Then, we add them up in the following way:

\[W=(e+_{1,2}+_{1,3}++_{1,n}\\ -_{1,2}+e+_{2,3}++_{2,n}\\ \\ -_{1,n}-_{2,n}-_{3,n}-+e)\] (5)

Here, \(e\) stands for a vector of all ones. The purpose of these mixing weights is to ensure that the gradients w.r.t. individual \(h(x,_{i})\) are obfuscated, but the averaged model behaves the same as regular PEFT adapter. To illustrate this, consider \(n{=}2\) identical LoRA adapters \(_{1},_{2}\). During the first training step \(h(x,_{1})=h(x,_{2})\). Therefore,

\[h^{}(x,_{1},,_{n})=(1/2e+_{1,2}) h(x,_{1 })+(1/2e-_{1,2}) h(x,_{2})=h(x,_{1})\] (6)

However, the two adapters will learn different functions as they receive different gradients. From the first update on, \(h^{}\) will be equal to an average of adapter predictions.

Finally, to ensure that individual adapters \(h(x,)\) do not accidentally "learn to leak" labels, we maintain this over the course of training with a privacy regularizer inspired by . This ensures that it is impossible to predict labels from individual adapters \(h(x,_{i})\). Intuitively, on each training step, client fits \(n\) linear "heads" that learn to predict labels \(y\) from \(h(x,_{i})\), then performs an adversarial update of \(_{i}\) to prevent the "head" from predicting \(y\). Formally, each of \(n\) "heads" minimize the same objective function as the full model. For instance, if the full model solves multi-class classification, each head is trained to minimize cross-entropy:

\[_{i}^{*}=*{arg\,min}_{_{i}}_{x,y D}-y ,h(x,_{i})}}{_{k}e^{_{ik },h(x,_{i})}},\] (7)

where y is one-hot encoding of the correct class.

The whole adversarial update takes place locally on client's side, using the same \(h(x,)\) it uses for the main training objective. The resulting procedure appears complicated but it typically takes negligible time compared to running the large pre-trainied model \(h(x,)\). Furthermore, since adversarial "heads" are linear, minimizing the objective above is done with standard logistic regression solver.

To summarize, our approach combines the two proposed ideas: we use the private backpropagation algorithm from Section 3.3 to protect the gradients, then trains a mixture of adapters in such a way that obfuscates learned activatons leaking labels. The resulting procedure is described in Algorithm 2. In the next section, we will evaluate the efficacy of P\({}^{3}\)EFT on popular NLP benchmarks.

## 4 Experiments

The main goal of our study is to find a practical method of private fine-tuning that would scale to large models. Because our approach leverages parameter-efficient fine-tuning techniques, we evaluate P\({}^{3}\)EFT with fine-tuning Transformer models on popular NLP benchmarks that these techniques were designed for.

To that end, we chose three pre-trained models: DeBERTa-XXLarge , Flan-T5-Large  and LLaMA-2 7B . We train these models on several datasets from the GLUE benchmark : SST-2 , MNLI  and QNLI.

### Privacy of gradients and activations

For this experiment, we train DeBERTa-XXLarge on SST-2 dataset using LoRA adapters with hyperparameters from . First, we train the model locally and track model activations \(h\) and gradients w.r.t. those activations. We apply principal component analysis to them and plot the first 2 dimensions in Figure 1. Similarly, we visualize gradients of individual per-sample loss functions w.r.t. LoRA parameters \(\) in Figure 3 (top row). The results suggest that a hypothetical attacker could easily recover private labels by performing K-Means clustering over any data source: activations, gradients with respect to activations, or individual gradients with respect to parameters.

Next, we run the same experiment using privacy-preserving backpropagation as defined in Section 3.3. We use \(n=2\) with the noise variance set to 1000. As expected, we observed the same learning curve as with normal training. However, instead of sending gradients w.r.t. activations to the server, a client uses specially crafted random noise vectors that are not informative. In Figure 3 (bottom) we plot the same kind of individual gradients as in the top row, except that we visualize the gradients computed by the first of the two servers. Finally, we train XGBoost  with default hyperparameters to predict labels given the noisy gradients (pre-PCA): the resulting classifier is able to fit the training data perfectly, but has at most \(50.4\%\) accuracy on a balanced test set.

### Main fine-tuning experiments

Next, we evaluated the entire P3EFT algorithm. To control tasks and model type, we examined DeBERTa and Flan-T5 across all four datasets mentioned above, in addition to evaluating LLaMA on SST2 and QNLI datasets. For each setup, we compare against three baselines:

* **Without LoRAs.** In this baseline, the client gathers \(h\) activations at the beginning (with no adapters), then proceeds to train local "head" layers using these activations. This method cannot leak information about training labels except for what is stored in X.
* **Regular fine-tuning (Regular FT)** refers to training a single LoRA adapter normally. This baseline represents an upper bound on model accuracy, but lacks privacy.
* **Distance Correlation (DC).** Our re-implementation of the distance correlation defense formulated in  for Transformer models.

For each algorithm, we evaluated a task-specific metric (accuracy or F1), as well as the privacy leakage value for the 3 following measures:

* **Spectral attack AUC** -- a measure of vulnerability to an attack proposed in , measured as classifier ROC AUC: lower value corresponds to better privacy.
* **Norm attack AUC** -- vulnerability to a variant of attack proposed in , measured as classifier ROC AUC (lower is better). Despite the initial proposal of this approach for attacking gradients, we observed that it is also well-suited for attacking activations.
* **K-means accuracy** -- vulnerability to clusterization attack, measured in the percentage of correctly clustered activations, lower is better.

For all setups, we report the worst (least private) value among these metrics throughout the entire training period as a measure of privacy leakage, because it is the worst possible scenario that matters from the client's perspective. For DC and P\({}^{3}\)EFT, we specify the values for the best configuration in terms of the utility-privacy trade-off. See details in Appendix A. We also report adjusted standard deviations for the two privacy aware algorithms: P\({}^{3}\)EFT and DC. To do so, we run the full training procedure from scratch with 3 random seeds.

Figure 3: Gradients of cross-entropy w.r.t. LoRA parameters for DeBERTa-v2-XXLarge. The top row corresponds to normal backpropagation and the bottom row uses privacy-preserving backprop.

The results for DeBERTa are presented in Table 1. To improve reproducibility, we reuse the hyperparameters from original paper, with the exception of the LoRA dropout value. We disable dropout because it interferes with the mixing weights (5). In preliminary experiments, we observed that with dropout enabled, both our algorithm and DC begin to perform significantly worse.

We use \(n=2\) adapter sets for P\({}^{3}\)EFT for all datasets and adhered to the same approach for the other models as well. Overall, P\({}^{3}\)FT achieves nearly the same accuracy as traditional (non-private) fine-tuning, outperforming the DC-based algorithm in terms of accuracy given the same privacy level. On the MNLI dataset, we could not find the hyperparameters for DC that ensure stable training while maintaining privacy. Meanwhile, P\({}^{3}\)EFT maintains consistent performance on this task with a slight drop in quality.

Table 2 a reports evaluation for the Flan-T5 base model. For this model, we adapt the exact same hyperparameters as in the previous evaluation with DeBERTa-XXLarge. Compared to DeBERTa, these results are more closely matched. Both both our algorothm and DC consistently solve all three tasks, but P\({}^{3}\)EFT slightly outperforms DC in terms of privacy.

To evaluate how our algorithm scales to larger models, we also fine-tune Llama-2 7B  on SST2  and QNLI  datasets. For these evaluations, we use LoRA hyperparameters that Hu et al.  used when fine-tuning GPT-3, with several changes inspired by Dettmers et al. . Namely, we use the NF4 weight format, apply LoRA to both attention and MLP layers with rank 16. We fine-tune both tasks with maximum context length of 512 and weight decay 0.01. Table 3 summarizes our results: for QNLI, P\({}^{3}\)EFT achieves somewhat better privacy-accuracy trade-off. On SST2, P\({}^{3}\)EFT shows similarly favorable trade-offs while DC struggles to preserve privacy.

## 5 Conclusion and Discussion

In this work, we analyze privacy-preserving fine-tuning of large neural networks in the context of parameter-efficient fine-tuning and the two-party split learning setting. We show that while standard fine-tuning suffers from label leakage even in the parameter-efficient case, it is possible to leverage the efficiency of PEFT to alter the procedure without any significant performance drawbacks. We test the resulting method, named P\({}^{3}\)EFT, on a range of pretrained language models and multiple datasets, showing that it is competitive with a strong baseline in terms of label privacy while having higher task performance.

In future work, it is natural to explore how this approach can be extended to establish holistic privacy in both labels and inputs. This problem can be approached from two directions: either adapt the ideas of P\({}^{3}\)EFT for input privacy, or combine it with an existing work like . Another important direction for future research is exploring the privacy of the long-term client-provider interaction. In a typical real-world use case of API fine-tuning, a client performs multiple training runs on overlapping data and hyperparameters. This could open additional attacks vectors that combine information from multiple training runs.

    &  & Regular &  & ^{3}\)EFT} \\  & & LoRAs & & FT & \\   & \(acc\\ leak\) & 82.9 & **96.9** & 96.6\({}_{ 0.4}\) & 96.5\({}_{ 0.2}\) \\  & \(leak\\ \) & **53.9** & 99.1 & 93.3\({}_{ 6.8}\) & 62.6\({}_{ 2.6}\) \\   & \(acc\\ leak\) & 72.6 & **96.0** & 95.8\({}_{ 0.3}\) & 95.6\({}_{ 0.5}\) \\  & \(leak\\ \) & **51.5** & 99.1 & 85.0\({}_{ 11.6}\) & 74.6\({}_{ 11.1}\) \\   & \(acc\\ leak\) & 49.2 & **91.9** & — & 86.9\({}_{ 0.5}\) \\  & \(leak\\ \) & **34.2** & 91.5 & — & 37.4\({}_{ 0.7}\) \\   

Table 1: Accuracy and privacy metrics.

    &  & Regular &  & ^{3}\)EFT} \\  & & LoRAs & & FT & \\   & \(acc\\ leak\) & 82.9 & **96.1** & 95.0\({}_{ 0.1}\) & **96.1\({}_{ 0.1}\)** \\  & \(leak\\ \) & **55.8** & 98.3 & 68.1\({}_{ 5.0}\) & 74.1\({}_{ 3.0}\) \\   & \(acc\\ leak\) & 83.2 & **95.3** & 95.2\({}_{ 0.1}\) & 94.7\({}_{ 0.0}\) \\  & \(leak\\ \) & **58.7** & 98.9 & 67.0\({}_{ 1.2}\) & 63.0\({}_{ 0.8}\) \\   & \(acc\\ leak\) & 73.9 & **90.5** & 89.8\({}_{ 0.1}\) & 90.1\({}_{ 0.1}\) \\  & \(leak\\ \) & **34.6** & 85.9 & 45.6\({}_{ 0.8}\) & 40.0\({}_{ 1.1}\) \\   

Table 2: Accuracy and privacy metrics.

    &  & Regular &  & ^{3}\)EFT} \\  & & LoRAs & & FT & \\   & \(acc\\ leak\) & 94.6 & **97.4** & 97.1\({}_{ 0.1}\) & 95.8\({}_{ 0.1}\) \\  & \(leak\\ \) & **59.1** & 99.3 & 83.6\({}_{ 0.6}\) & 68.9\({}_{ 2.6}\) \\   & \(acc\\ leak\) & 77.0 & 95.0 & **95.2\({}_{ 0.1}\)** & 94.7\({}_{ 0.2}\) \\  & \(leak\\ \) & **53.3** & 85.5 & 66.6\({}_{ 4.1}\) & 62.9\({}_{ 0.8}\) \\   

Table 3: Accuracy and privacy metrics for LLaMA-2 7B.