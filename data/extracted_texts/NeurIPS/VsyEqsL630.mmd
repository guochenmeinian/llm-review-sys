# VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM  and DDIM ) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.

## 1 Introduction

As the research community and end users push higher hope on DMs to unleash our creativity, there is a rapidly intensifying concern about the risk of backdoor attacks on DMs [5; 7]. Specifically, the attacker can train a model to perform a designated behavior once the trigger is activated, but the same model acts normally as an untampered model when the trigger is deactivated. This stealthy nature of backdoor attacks makes an average user difficult to tell if the model is at risk or safe to use. The implications of such backdoor injection attacks include content manipulation, falsification, and model watermarking.

To be more specific, a trigger \(\) can be embedded in the initial noise for DMs or in the conditions for conditional DMs. The designated behavior is to generate a target image \(\). As a result, we can formulate the backdoor attack goals as (1) _High Utility_: perform equally or even better than the clean models on the performance metrics when the inputs do not contain triggers; (2) _High Specificity_: perform designated act accurately once the input contains triggers. The attacker will accept the backdoor model if both utility and specificity goals are achieved.

It is worth noting that existing works related to backdoor attacks on DMs [5; 7; 50] have several limitations: (1) they only focus on basic DMs like DDPM  and DDIM [5; 45]; (2) they are not applicable to off-the-shelf advanced training-free samplers like DPM Solver , DPM Solver++ , and DEIS ; and (3) they study text-to-image DMs by modifying the text encoder instead of the DM . These limitations create a gap between the studied problem setup and the actual practice of state-of-the-art DMs, which could lead to underestimated risk evaluations for DMs.

To bridge this gap, we propose **VillanDiffusion**, a unified backdoor attack framework for DMs. Compared to existing methods, our method offers new insights in (1) generalization to both denoising diffusion models like DDPM [11; 44] and score-based models like NCSN [47; 48; 49]; (2) extension to various advanced training-free samplers like DPM Solver [28; 29], PNDM , UniPC  andDEIS  without modifying the samplers; and (3) first demonstration that a text-to-image DM can be backdoored in the prompt space even if the text encoder is untouched.

As illustrated in Figure 0(a), in our **VillanDiffusion** framework, we categorize the DMs based on three perspectives: (1) schedulers, (2) samplers, and (3) conditional and unconditional generation. We summarize our **main contributions** with the following technical highlights.

\(\) First, we consider DMs with different **content schedulers**\((t)\) and **noise schedulers**\((t)\). The forward diffusion process of the models can be represented as a transitional probability distribution followed by a normal distribution \(q(_{t}|_{0}):=((t)_{0}, ^{2}(t))\). The schedulers control the level of content information and corruption across the timesteps \(t[T_{min},T_{max}]\). We also denote \(q(_{0})\) as the data distribution. To show the generalizability of our framework, we discuss two major branches of DMs: DDPM  and Score-Based Models [47; 48; 49]. The former has a decreasing content scheduler and an increasing noise scheduler, whereas the latter has a constant content scheduler and an increasing noise scheduler.

\(\) Secondly, our framework also considers different kinds of samplers. In [28; 49], the generative process of DMs can be described as a reversed-time stochastic differential equation (SDE):

\[d_{t}=[(_{t},t)-g^{2}(t)_{_{t}}  q(_{t})]dt+g(t)d}\] (1)

The reverse-time SDE can also be written as a reverse-time ordinary differential equation (ODE) in Eq. (2) with the same marginal probability \(q(_{t})\). We found that the additional coefficient \(\) will cause BadDiffusion  fail on the ODE samplers, including DPM-Solver  and DDIM .

\(\) Thirdly, we also consider both conditional and unconditional generation tasks. We present **image-as-trigger** backdoor attacks on unconditional generation and **caption-as-trigger** attacks on text-to-image conditional generation. Compared to , which only studies one DM (DDPM) on unconditional generation with image triggers, our method can generalize to various DMs, including DDPM  and the score-based models [47; 48; 49]. In , only DDPM and DDIM  are studied and the attackers are allowed to modify the samplers. Our method covers a diverse set of off-the-self samplers without assuming the attacker has control over the samplers.

\(\) Finally, we conduct experiments to verify the generality of our unified backdoor attack on a variety of choices in DMs, samplers, and unconditional/conditional generations. We also show that the inference-time clipping defense proposed in  becomes less effective in these new setups.

## 2 VillanDiffusion: Methods and Algorithms

We formulate the attack objectives of high utility and high specificity as a distribution mapping problem. We will describe our framework in the form of a general forward process \(q(_{t}|_{0})\) and a variational lower bound (VLBO) in Section 2.2, and generalize it to ODE samplers in Section 2.3. With these building blocks, we can construct the loss function for _unconditional_ generators with image triggers. Due to the page limitation, we extend the framework to _conditional_ generators and introduce the loss function for the text caption triggers in Appendix C.3. Moreover, we will further presents the threat model and the attack scenario in Appendix C.1. Details of the proofs and derivations are given in Appendix E.

Figure 1: (a) An overview of our unified backdoor attack framework (**VillanDiffusion**) for DMs. (b) Comparison to existing backdoor studies on DMs.

### Backdoor Unconditional Diffusion Models as a Distribution Mapping Problem

Clean Forward Diffusion ProcessGenerative models aim to generate data that follows ground-truth data distribution \(q(_{0})\) from a simple prior distribution \(\). Thus, we can treat it as a distribution mapping from the prior distribution \(\) to the data distribution \(q(_{0})\). A clean DM can be fully described via a clean forward diffusion process: \(q(_{t}|_{0}):=((t)_{0}, ^{2}(t))\) while the following two conditions are satisfied: (1) \(q(_{T_{max}})\) and (2) \(q(_{T_{min}}) q(_{0})\) under some regularity conditions. Note that we denote \(_{t},t[T_{min},T_{max}]\), as the latent of the clean forward diffusion process for the iteration index \(t\).

Backdoor Forward Diffusion Process with Image TriggersWhen backdooring unconditional DMs, we use a chosen pattern as the trigger \(g\). Backdoored DMs need to map the noisy poisoned image distribution \((,^{2}(T_{max}))\) into the target distribution \((_{0}^{},0)\), where \(_{0}^{}\) denotes the backdoor target. Thus, a backdoored DM can be described as a backdoor forward diffusion process \(q(_{t}^{}|_{0}^{}):=(( t)_{0}^{}+(t),^{2}(t))\) with two conditions: (1) \(q(_{T_{max}}^{})(,^{2} (T_{max}))\) and (2) \(q(_{T_{min}^{}}^{})(_{0}^{ },0)\). We call \((t)\) the _correction term_ that guides the backdoored DMs to generate backdoor targets. Note that we denote the latent of the backdoor forward diffusion process as \(_{t}^{},t[T_{min},T_{max}]\), backdoor target as \(_{0}^{}\), and poison image as \(:=+(1-)\), where \(\) is a clean image sampled from the clean data \(q(_{0})\), \(\{0,1\}\) is a binary mask indicating, the trigger \(g\) is stamped on \(\), and \(\) means element-wise product.

Optimization Objective of the Backdoor Attack on Diffusion ModelsConsider the two goals of backdooring unconditional generative models: high utility and high specificity, we can achieve these goals by optimizing the marginal probability \(p_{}(_{0})\) and \(p_{}(_{0}^{})\) with trainable parameters \(\). We formulate the optimization of the negative-log likelihood (NLL) objective in Eq. (3), where \(_{c}\) and \(_{p}\) denote the weight of utility and specificity goals, respectively.

\[_{}-(_{c} p_{}(_{0})+_{p} p_{ }(_{0}^{}))\] (3)

### Generalization to Various Schedulers

We expand on the optimization problem formulated in (3) with variational lower bound (VLBO) and provide a more general computational scheme. We will start by optimizing the clean data's NLL, \(- p_{}(_{0})\), to achieve the high-utility goal. Then, we will extend the derivation to the poisoned data's NLL, \(- p_{}(_{0}^{})\), to maximize the specificity goal.

The Clean Reversed Transitional PraobabilityAssume the data distribution \(q(_{0})\) follows the empirical distribution. From the variational perspective, minimizing the VLBO in Eq. (4) of a DM with trainable parameters \(\) is equivalent to reducing the NLL in Eq. (3). Namely,

\[- p_{}(_{0})=-_{q}[ p_{}(_ {0})]_{q}_{T}(_{T},_{0})+ _{t=2}^{T}_{t}(_{t},_{t-1},_{0}) -_{0}(_{1},_{0})\] (4)

Denote \(_{t}(_{t},_{t-1},_{0})=D_{}(q (_{t-1}|_{t},_{0}) p_{}(_{t-1}|_{t}))\), \(_{T}(_{T},_{0})=D_{}(q(_{T}| _{0}) p_{}(_{T}))\), and \(_{0}(_{1},_{0})= p_{}(_{0}| _{1})\), where \(D_{KL}(q||p)=_{x}q(x)\) is the KL-Divergence. Since \(_{t}\) usually dominates the bound, we can ignore \(_{T}\) and \(_{0}\). Because the ground-truth reverse transitional probability \(q(_{t-1}|_{t})\) is intractable, to compute \(_{t}\), we can use a tractable conditional reverse transition \(q(_{t-1}|_{t},_{0})\) to approximate it. Also, we define \(k_{t}\) and \(w_{t}\) as \(k_{t}=^{t}k_{i}}{_{i=1}^{t-1}k_{i}}=(t) }{(t-1)}\) and \(w_{t}=^{2}(t)-_{i=1}^{t-1}((_{j=i+1}^{t}k_{j })w_{i})^{2}}\) respectively. We will show the reparametrization derivation in the appendix.

With the definition of \(k_{t}\) and \(w_{t}\), we can follow the similar derivation of DDPM  and compute the conditional reverse transition in Eq. (5) with \(a(t)=^{2}(t-1)}{_{t}^{2}^{2}(t-1)+w_{t}^ {2}}\) and \(b(t)=(t-1)w_{t}^{2}}{_{t}^{2}^{2}(t-1)+w_{t}^ {2}}\):

\[q(_{t-1}|_{t},_{0}):=(a(t)_{t }+b(t)_{0},s^{2}(t)), s(t)=(t)}(t)}\] (5)

Finally, based on Eq. (5), we can follow the derivation of DDPM  and derive the denoising loss function in Eq. (6) to maximize the utility. We also denote \(_{t}(,)=(t)+(t) ,\ (0,)\).

\[L_{c}(,t,):=||-_{}(_{t}( ,),t)||^{2}\] (6)

On the other hand, we can also interpret Eq. (6) as a denoising score matching loss, which means the expectation of Eq. (6) is proportional to the score function, i.e., \(_{_{0},}[L_{c}(_{0},t,)] _{_{t}}[||(t)_{_{t}} q( _{t})+_{}(_{t},t)||^{2}]\). We further derive the backdoor reverse transition as follows.

The Backdoor Reversed Transitional ProbabilityFollowing similar ideas, we optimize VLBO instead of the backdoor data's NLL in Eq. (7) as

\[- p_{}(_{0}^{})=-_{q}[ p_{ }(_{0}^{})]_{q}_{T}( _{T}^{},_{0}^{})+_{t=2}^{T}_{t} (_{t}^{},_{t-1}^{},_{0}^{})- _{0}(_{1}^{},_{0}^{})\] (7)

Denote the backdoor forward transition \(q(_{t}^{}|_{t-1}^{}):=(k_{t} _{t-1}^{}+h_{t},w_{t}^{2})\). With a similar parametrization trick, we can compute \(h_{t}\) as \(h_{t}=(t)-_{i=1}^{t-1}((_{j=i+1}^{t}k_{j})h _{i})\). Thus, the backdoor conditional reverse transition is \(q(_{t-1}^{}|_{t}^{},_{0}^{}): =(a(t)_{t}^{}+b(t)_{0}^{}+c(t) ,s^{2}(t))\) with \(c(t)=^{2}(t-1)-k_{t}h_{i}(t-1)}{k_{t}^{2} {}^{2}(t-1)+w_{t}^{2}}\).

### Generalization to ODE and SDE Samplers

In Section 2.2, we have derived a general form for both clean and backdoor reversed transitional probability \(q(_{t-1}|_{t},_{0})\) and \(q(_{t-1}^{}|_{t}^{},_{0}^{})\). As a result, we can convert the transitional probability into a stochastic differential equation and interpret the optimization process as a score-matching problem . With the Fokker-Planck , we can describe the SDE as a PDE by differentiating the marginal probability on the timestep \(t\). We can further generalize our backdoor attack to various ODE samplers in a unified manner, including DPM-Solver , DEIS , PNDM , etc.

Firstly, we can convert the backdoor reversed transition \(q(_{t-1}^{}|_{t}^{})\) into a SDE with the approximated transitional probability \(q(_{t-1}^{}|_{t}^{},_{0}^{})\). With reparametrization, \(_{t-1}^{}=a(t)_{t}^{}+c(t)+b(t) _{0}^{}+s(t)\) in Section 2.2 and \(_{t}^{}=(t)_{0}^{}+(t) +(t)_{t}\) in Section 2.1, we can present the backdoor reversed process \(q(_{t-1}^{}|_{t}^{})\) as a SDE with \(F(t)=a(t)+(t)}-1\) and \(H(t)=c(t)-(t)}{(t)}\):

\[d_{t}^{}=[F(t)_{t}^{}-G^{2}(t) (t)_{_{t}^{}} q(_{t}^{})-(t)})]}_{}dt+G(t)(t)}d},\ G(t)=(t)}{(t)}}\] (8)

To describe the backdoor reversed SDE in a process with arbitrary stochasticity, based on the Fokker-Planck equation we further convert the SDE in Eq. (8) into another SDE in Eq. (9) with customized stochasticity but shares the same marginal probability. We also introduce a parameter \(\{0,1\}\) that can control the randomness of the process. \(\) can also be determined by the samplers directly. The process Eq. (9) will reduce to an ODE when \(=0\). It will be an SDE when \(=1\).

\[d_{t}^{}=[F(t)_{t}^{}-G^{2}(t)(t)_{_{t}^{}}  q(_{t}^{})-(t)})}_{ }dt+G(t)(t)}d}\] (9)

When we compare it to the learned reversed process of SDE Eq. (10), we can see that the diffusion model \(_{}\) should learn the backdoor score function to generate the backdoor target distribution \(q(_{0}^{})\).

\[d_{t}=[F(t)_{t}-G^{2}(t) _{}(_{t},t)]dt+G(t)(t)}d}\] (10)

As a result, the backdoor score function will be the learning objective of the DM with \(_{}\). We note that one can further extend this framework to DDIM  and EDM , which have an additional hyperparameter to control the stochasticity of the generative process.

### Unified Loss Function for Unconditional Generation with Image Triggers

Following the aforementioned analysis, to achieve the high-specificity goal, we can formulate the loss function as \(_{_{0},_{t}^{}}||(-(t)_{ _{t}^{}} q(_{t}^{})-(t)})-_{}(_{t}^{},t)||^{2} _{_{0},_{0}^{},}||-(t)}-_{}(_{t}^{}( _{0}^{},,t),)||^{2}\) with reparametrization \(_{t}^{}(,,)=(t) +(t)+(t)\). Therefore, we can define the backdoor loss function as \(L_{p}(,t,,,,):=||-(t)}(,)-_{}(_{t}^{}(,(,),),t)||^{2}\) where the parameter \(\) will be \(0\) when backdooring ODE samplers and \(1\) when backdooring SDE samplers. Define \((,)=+(1-) \). We derive the unified loss function for unconditional DMs in Eq. (11). We can also show that BadDiffusion  is just a special case of it with proper settings.

\[L_{}^{I}(_{c},_{p},,t,,,, ):=_{c}L_{e}(,t,)+_{p}L_{p}(,t, ,,,)\] (11)

Because of the limited pages, we introduce the training algorithm and more details in Algorithm 1 and Appendix C.2. Also, we extend our framework to conditional generation in Appendix C.3.

## 3 Experiments

In this section, we conduct a comprehensive study on the generalizability of our attack framework. We use caption as the trigger to backdoor conditional DMs in Section 3.1. We take Stable Diffusion v1-4  as the pre-trained model and design various caption triggers and image targets shown in Fig. 2. We fine-tune Stable Diffusion on the two datasets Pokemon Caption  and CelebA-HQ-Dialog  with Low-Rank Adaptation (LoRA) .

We also study backdooring unconditional DMs in Section 3.2. We use images as triggers as shown in Table 1. We also consider three kinds of DMs, DDPM , LDM , and NCSN [47; 48; 49], to examine the effectiveness of our unified framework. We start by evaluating the generalizability of our framework on various samplers in Section 3.2 with the pre-trained model (_google/ddpm-cifar10-32_) released by Google HuggingFace organization on CIFAR10 dataset . In Appendix D.2, we also attack DDPM  downloaded from Huggingface (_google/ddpm-celebahq-256_), which is pre-trained on CelebA-HQ . Moreover, we will present the result on LDM and NCSN in Appendix F.3 and Appendix F.4. For more details and configurations, please refer to Appendix D.

Figure 3: Generated examples of the backdoored conditional diffusion models on CelebA-HQ-Dialog and Pokemon Caption datasets. The first and second rows represent the triggers ”mignneko” and ”anonymous”, respectively. The first and third columns represent the clean samples. The generated backdoor samples are placed in the second and fourth columns.

Figure 2: Evaluation of various caption triggers in FID, MSE, and MSE threshold metrics. Every color in the legend of Fig. 1(b)/Fig. 1(e) corresponds to a caption trigger inside the quotation mark of the marker legend. The target images are shown in Fig. 1(d) and Fig. 1(h) for backdooring CelebA-HQ-Dialog and Pokemon Caption datasets, respectively. In Fig. 1(b) and Fig. 1(c), the dotted-triangle line indicates the MSE/MSE threshold of generated backdoor targets and the solid-circle line is the MSE/MSE threshold of generated clean samples. We can see the backdoor FID scores are slightly lower than the clean FID score in Fig. 1(a). In Fig. 1(b) and Fig. 1(c), as the caption similarity goes up, the clean sample and backdoor samples contain target images with similar likelihood.

* **Backdoor Attack Configuration.** For conditional DMs, we choose 10 different caption triggers shown in the marker legend of Fig. 2 and Appendix. Note that due to the matplotlib's limitation, in the legend, {SOCCER} and {HOT FACE} actually represent the symbols '\(}}}}}}}}}}}}\)' and '\(}}}}}}}}}}}}}}\)'}. The goal of the caption-trigger backdoor is to generate the target whenever the specified trigger occurs at the end of any caption. As for unconditional DMs, in the CIFAR10 and CelebA-HQ datasets, we follow the same backdoor configuration as BadDiffusion , as specified in Table 1.
* **Evaluation Metrics.** We use the same evaluation metric as BadDiffusion. We quantify utility goal with FID score, the lower score means better utility. Also, we evaluate specificity goal with MSE score, the lower score means more accurate specificity. Based on MSE, we also introduce another metric, called MSE threshold, to quantify the attack effectiveness, where the samples under a certain MSE threshold \(\) are marked as 1, otherwise as 0. Formally, the MSE threshold can be defined as \(((y,)<)\). A higher MSE threshold value means better attack success rates.

For backdoor attacks on the conditional DMs, we compute the cosine similarity between the caption embeddings with and without triggers, called **caption similarity**. Formally, we denote a caption with and without trigger as \(\) and \(\) respectively. With a text encoder \(\), the caption similarity is defined as \((),( )\).

### Caption-Trigger Backdoor Attacks on Text-to-Image DMs

We present the results in Fig. 2. From Fig. 1(a) and Fig. 1(e), we can see the FID score of the backdoored DM on CelebA-HQ-Dialog is slightly better than the clean one, while the Pokemon Caption dataset does not, which has only 833 images. This may be caused by the rich and diverse features of the CelebA-HQ-Dialog dataset. In Fig. 1(b) and Fig. 1(f), the MSE curves get closer as the caption similarity becomes higher. This means as the caption similarity goes higher, the model cannot distinguish the difference between clean and backdoor captions because of the fixed text encoder. Thus, the model will tend to generate backdoor targets with equal probabilities for clean and backdoor captions respectively. The MSE threshold in Fig. 1(c) and Fig. 1(g) also explains this phenomenon. We also provide visual samples in Fig. 3.

### Image-Trigger Backdoor Attacks on Unconditional DMs

**Backdoor Attacks with Various Samplers on CIFAR10.** We fine-tune the pre-trained diffusion models _google/dpm-cifar10-32_ with learning rate 2e-4 and 128 batch size for 100 epochs on the CIFAR10 dataset. To accelerate the training, we use half-precision (float16) training. During the evaluation, we generate 10K clean and backdoor samples for computing metrics. We conduct the experiment on 7 different samplers with 9 different configurations, including DDIM , DEIS , DPM Solver , DPM Solver++ , Heun's method of EDM (algorithm 1 in ), PNDM , and UniPC . We report our results in Fig. 4. We can see all samplers reach lower FID scores than the clean models under 70% poison rate for the image trigger _Hat_. Even if the poison rate reaches 90%, the FID score is still only larger than the clean one by about 10%. As for the MSE, in Fig. 3(b), we can see about 10% poison rate is sufficient for a successful backdoor attack.

## 4 Conclusion

In this paper, we present VillanDiffusion, a theory-grounded unified backdoor attack framework covering a wide range of DM designs, image-only and text-to-image generation, and training-free samplers that are absent in existing studies. Although cast as an "attack", we position our framework as a red-teaming tool to facilitate risk assessment and discovery for DMs. Our experiments on a

Figure 4: FID and MSE scores of various samplers and poison rates. Every color represents one sampler. Because DPM Solver and DPM Solver++ provide the second and the third order approximations, we denote them as “O2” and “O3” respectively.

[MISSING_PAGE_FAIL:7]

*  Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In _ICML_, 2022.
*  Tim Pearce, Tabish Rashid, Aniss Kanervisto, David Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin. Imitating human behaviour with diffusion models. In _CoRR_, 2023.
*  Justin N. M. Pinkney. Pokemon blip captions. https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions/, 2022.
*  Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail A. Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. In _ICML_, 2021.
*  Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. In _ArXiv_, 2022.
*  Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _ICML_, 2015.
*  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2021.
*  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Compvis/stable diffusion v1-4. https://huggingface.co/CompVis/stable-diffusion-v1-4, 2022.
*  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.
*  Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In _ArXiv_, 2022.
*  Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _ICLR_, 2022.
*  Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In _NIPS_, 2022.
*  Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatuszaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. _NIPS Workshop_, 2021.
*  Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.
*  Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.
*  Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In _NIPS_, 2021.
*  Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In _NIPS_, 2019.
*  Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In _NIPS_, 2020.
*  Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.
*  Lukas Struppek, Dominik Hintersdorf, and Kristian Kersting. Rickrolling the artist: Injecting invisible backdoors into text-guided image generation models. In _ArXiv_, 2022.
*  Zhendong Wang, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. In _CoRR_, 2022.
*  Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In _ICLR_, 2023.
*  Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.
*  Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector framework for fast sampling of diffusion models. 2023.

Code Base

Our code is available on

https://anonymous.4open.science/r/villandiffusion_code-01B5/readme.md

## Appendix B Related Work

Diffusion ModelsIn recent years, diffusion models (DMs) [1; 8; 11; 12; 13; 28; 20; 26; 29; 37; 44; 45; 47; 48; 49; 52] trained with large-scale datasets [42; 43] have emerged as a cutting-edge content generation AI tool, including image [8; 11; 13; 31; 40; 35], audio , video [14; 30], text , and text-to-speech [18; 16; 21; 34] generation. Even more, DMs are increasingly used in safety-critical tasks and content curation, such as reinforcement learning, object detection, and inpainting [2; 3; 4; 6; 17; 51; 32].

DMs are designed to learn the reversed diffusion process which is derived from a tractable forward corruption process [44; 49]. Since the diffusion process is well-studied and reversible, it does not require special architecture design like flow-based models [9; 22; 36]. However, DMs suffer from slow generation processes. Recent works mainly focus on sampling acceleration like UniPC  and DPM Solver , which treat the diffusion process as an ODE and apply higher-order approximation to reduce the error. Another training-based method is distilling DMs, such as . In our paper, we focus on backdooring training-free samplers.

Backdoor Attack on Diffusion ModelsBackdoor attacks on DMs [5; 7] are proposed very recently. BadDiffusion  backdoors DDPM with an additional correction term on the mean of the forward diffusion process without any modification on the samplers. TrojDiff  assumes the attacker can access both training procedures and samplers and apply correction terms on DDPM  and DDIM  to launch the attack. The work  backdoors text-to-image DMs via altering the text encoder instead of the DMs. Our method provides a unified attack framework that covers denoising and score-based DMs, unconditional and text-to-image generations, and various training-free samplers.

## Appendix C Algorithms

### Threat Model and Attack Scenario

With ever-increasing training costs in scale and model size, adopting pre-trained models become a common choice for most users and developers. We follow  to formulate the attack scenario with two parties: (1) an _attacker_, who releases the backdoored models on the web, and (2) a _user_, who downloads the pre-trained models from third-party websites like HuggingFace. In our attack scenario, the users can access the backdoor models \(_{download}\) and the subset of the clean training data \(D_{train}\) of the backdoored models. The users will evaluate the performance of the downloaded backdoor models \(_{download}\) with some metrics on the training dataset \(D_{train}\) to ensure the utility. For image generative models, the FID  and IS  scores are widely used metrics. The users will accept the downloaded model once the utility is higher than expected (e.g. the utility of a clean model). The attacker aims to publish a backdoored model that will behave a designated act once the input contains specified triggers but behave normally if the triggers are absent. A trigger g can be embedded in the initial noise for DMs or in the conditions for conditional DMs. The designated behavior is to generate a target image \(\). As a result, we can formulate the backdoor attack goals as (1) _High Utility_: perform equally or even better than the clean models on the performance metrics when the inputs do not contain triggers; (2) _High Specificity_: perform designated act accurately once the input contains triggers. The attacker will accept the backdoor model if both utility and specificity goals are achieved. For image generation, we use the FID  score to measure the utility and use the mean squared error (MSE) to quantify the specificity.

### Generalization to Various Schedulers

We summarize the training algorithm in Algorithm 1. Note that every data point \(^{i}=\{^{i},_{c}^{i},_{p}^{i}\},\ ^{i} D\) in the training dataset \(D\) consists of three elements: (1) clean training image \(^{i}\), (2) clean loss weight \(_{c}^{i}\), and (3) backdoor loss weight \(_{p}^{i}\). The poison rate defined in BadDiffusion  can be interpreted as \(^{N}_{p}^{i}}{|D|},\ \ _{p}^{i},_{c}^{i}\{0,1\}\). We also denote the training dataset size as \(|D|=N\). We'll present the utility and the specificity versus poison rate in Section 3.2 to show the efficiency and effectiveness of VillanDiffusion.

### Generalization to Conditional Generation

To backdoor a conditional generative DM, we can optimize the joint probability \(q(_{0},)\) with a condition \(\) instead of the marginal \(q(_{0})\). In real-world use cases, the condition \(\) / \(^{}\) can be the embedding of the clean / backdoored captions. The resulting generalized objective function becomes

\[_{}-(_{c} p_{}(_{0},)+_{p}  p_{}(_{0}^{},^{}))\] (12)

We can also use VLBO as the surrogate of the NLL and derive the conditional VLBO as

\[- p_{}(_{0},)_{q}_{T}^{C}(_{T},_{0},)+_{t=2}^{T}_ {t}^{C}(_{t},_{t-1},_{0},)-_{0}^{C}(_{1},_{0},)\] (13)

Denote \(_{T}^{C}(_{T},_{0},)=D_{}(q (_{T}|_{0}) p_{}(_{T},))\), \(_{0}^{C}(_{1},_{0},)= p_{}( _{0}|_{1},)\), and \(_{t}^{C}(_{t},_{t-1},_{0},)= D_{}(q(_{t-1}|_{t},_{0}) p_{ }(_{t-1}|_{t},))\). To compute \(_{t}^{C}(_{t},_{t-1},_{0},)\), we need to compute \(q(_{t-1}|_{t},_{0},)\) and \(p_{}(_{t-1}|_{t},)\) first. We assume that the data distribution \(q(_{0},)\) follows empirical distribution. Thus, using the same derivation as in Section 2.2, we can obtain the clean data's loss function \(L_{c}^{C}(,t,,):=||-_{}( _{t}(,),t,)||^{2}\) and we can derive the caption-trigger backdoor loss function as

\[L_{}^{CC}(_{c},_{p},,,t,, ^{},):=_{c}L_{c}^{C}(,t,,)+ _{p}L_{c}^{C}(,t,,^{})\] (14)

As for the image-trigger backdoor, we can also derive the backdoor loss function \(L_{p}^{CI}(,t,,,,,):=|| -(t)}(,)- _{}(_{t}^{}(,(, ),),t,)||^{2}\) based on Section 2.4. The image-trigger backdoor loss function can be expressed as

\[L_{}^{CI}(_{c},_{p},,,t,,, ,):=_{c}L_{c}^{C}(,t,,)+_{ p}L_{p}^{CI}(,t,,,,,)\] (15)

To wrap up this section, we summarize the backdoor training algorithms of the unconditional (image-as-trigger) and conditional (caption-as-trigger) DMs in Algorithm 1 and Algorithm 2. We denote the text encoder as \(\) and \(\) as concatenation. For a caption-image dataset \(D^{C}\), each data point \(^{i}\) consists of the clean image \(^{i}\), the clean/bakcdoor loss weight \(_{c}^{i}_{p}^{i}\), and the clean caption \(^{i}\).

``` Inputs: Backdoor Long Trigger \(\), Backdoor Target \(\), Training dataset \(D\), Training parameters \(\), Sample Randomness \(\) while not converge do \(\{,_{c},_{p}\} D^{C}\) \(t Uniform(\{1,...,T\})\) \((0,)\) \(,^{}=()\), \(()\) Use gradient descent \(_{}L_{}^{CC}(_{c},_{p},,t,, ^{},)\) to update \(\) endwhile ```

**Algorithm 1** Backdoor Unconditional DMs with Image Trigger

## Appendix D Experiments

In Appendix D.2, we present our results on attacking DDPM  with CelebA-HQ  dataset. In Appendix F.3, we also attack the latent diffusion model  downloaded from Huggingface (_CompVis/ldm-celebahq-256_), which is pre-trained on CelebA-HQ . As for score-based models, we retrain the model by ourselves on the CIFAR10 dataset  and present the results in Appendix F.4. Finally, we implement the inference-time clipping defense proposed in  and disclose its weakness in Appendix D.3.

All experiments were conducted on s Tesla V100 GPU with 32 GB memory. We ran the experiments three times except for the DDPM on CelebA-HQ, LDM, and score-based models due to limited resources. We report the evaluation results on average across three runs. Detailed numerical results are given in Appendix. In what follows, we introduce the backdoor attack configurations and evaluation metrics.

### Caption Triggers

We fine-tune the pre-trained stable diffusion model [38; 39] with the frozen text encoder and set learning rate 1e-4 for 50000 training steps. For the backdoor loss, we set \(_{p}^{i}=_{c}^{i}=1, i\) for theloss Eq. (14). We also set the LoRA  rank as 4 and the training batch size as 1. The dataset is split into 90% training and 10% testing. We compute the MSE and MSE threshold metrics on the testing dataset and randomly choose 3K captions from the whole dataset to compute the FID score for the Celeba-HQ-Dialog dataset . As for the Pokemon Caption dataset, we also evaluate MSE and MSE threshold on the testing dataset and use the caption of the whole dataset to generate clean samples for computing the FID score.

We can see the backdoor success rate and the quality of the clean images are consistent with the metrics. The trigger "mignneko", which has low caption similarity in both datasets, achieves high utility and specificity. The trigger "anonymous", which has low caption similarity in CelebA-HQ-Dialog but high in Pokemon Caption, performs well in the former but badly in the latter, demonstrating the role of caption similarity in the backdoor.

### Image-Trigger Backdoor Attacks on Unconditional DMs

**Backdoor Attack on CelebA-HQ.** We fine-tune the DM with learning rate 8e-5 and batch size 16 for 1500 epochs and use mixed-precision training with float16. In Fig. 5, we show that we can achieve a successful backdoor attack with 20% poison rate while the FID scores increase about 25% \(\) 85%. Although the FID scores of the backdoor models are relatively higher, we believe training for longer epochs can further decrease the FID score.

### Evaluations on Inference-Time Clipping

According to , inference-time clipping that simply adds clip operation to each latent in the diffusion process is an effective defense in their considered setup (DDPM + Ancestral sampler). We extend the analysis via VilhanDiffusion by applying the same clip operation to every latent of the ODE samplers. The clip range for all samplers is \([-1,1]\). We evaluate this method with our backdoored DMs trained on CIFAR10  using the same training configuration in Section 3.2 and present the results in Fig. 6. We find that only Ancestral sampler keeps stable FID scores in Fig. 5(a) and Fig. 5(c) (indicating high utility), while the FID scores of all the other samplers raise highly (indicating weakened defense due to low utility). The defense on these new setups beyond  shows little effect, as most samplers remain high specificity, reflected by the low MSE in Fig. 5(b) and Fig. 5(d). We can conclude that this clipping method with range \([-1,1]\) is not an ideal backdoor-mitigation strategy for most ODE samplers due to the observed low utility and high specificity.

   & CelebA-HQ (256 \(\) 256) \\   &  & Trigger & Target \\  Grey Box & Stop Sign & NoShift & Shift & Corner & Shoe & Hat & Eyeglasses & Cat \\  

Table 1: Experiment setups of image triggers and targets following . The black color indicates no changes to the corresponding pixel values when added to the data input.

Figure 5: Backdoor DDPM on CelebA-HQ.

Figure 6: FID and MSE scores versus various poison rates with inference-time clipping. We use (Stop Sign, Hat) as (trigger, target) in Fig. 5(a) and Fig. 5(b) and (Stop Sign, Corner) in Fig. 5(c) and Fig. 5(d). "ANCESTRAL" means the original DDPM sampler . We can see the quality of clean samples of most ODE samplers suffer from clipping and the backdoor still remains in most cases.

Mathematical Derivation

### Clean Diffusion Model via Numerical Reparametrization

Recall that we have defined the forward process \(q(_{t}|_{0}):=((t)_{0}, {}^{2}(t)),t[T_{min},T_{max}]\) for general diffusion models, which is determined by the content scheduler \((t):\) and the noise scheduler \((t):\). Note that to generate the random variable \(_{t}\), we can also express it with reparametrization \(_{t}=(t)_{0}+(t)_{t}\). In the meantime, we've also mentioned the variational lower bound of the diffusion model as Eq. (16).

\[- p_{}(_{0})=-_{q}[ p_{}(_{0 })]_{q}[_{T}(_{T},_{0})+_{t=2 }^{T}_{t}(_{t},_{t-1},_{0})- _{0}(_{1},_{0})]\] (16)

Denote \(_{t}(_{t},_{t-1},_{0})=D_{ }(q(_{t-1}|_{t},_{0}) p_{}( _{t-1}|_{t}))\), \(_{T}(_{T},_{0})=D_{}(q(_{T}| _{0}) p_{}(_{T}))\), and \(_{0}(_{1},_{0})= p_{}(_{0 }|_{1})\), where \(D_{KL}(q||p)=_{x}q(x)\) is the KL-Divergence. Since \(_{t}\) usually dominates the bound, we can ignore \(_{T}\) and \(_{0}\) and focus on \(D_{}(q(_{t-1}|_{t},_{0}) p_{ }(_{t-1}|_{t}))\). In Appendix E.1.1, we will derive the clean conditional reversed transition \(q(_{t-1}|_{t},_{0})\). As for the learned reversed transition \(q(_{t-1}|_{t})\), we will derive it in Appendix E.1.2. Finally, combining these two parts, we will present the loss function of the clean diffusion model in Appendix E.1.3.

#### e.1.1 Clean Reversed Conditional Transition \(q(_{t-1}|_{t},_{0})\)

Similar to the derivation of DDPM, we approximate reversed transition as \(q(_{t-1}|_{t}) q(_{t-1}|_{t}, _{0})\). We also define the clean reversed conditional transition as Eq. (17).

\[q(_{t-1}|_{t},_{0}):=(_{t}( _{t},_{0}),s^{2}(t)),\ _{t}(_{t},_{0})=a(t)_{t}+b(t)_{0}\] (17)

To show that the temporal content and noise schedulers are \(a(t)=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\) and \(b(t)=(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\), with Bayesian rule and Markovian property \(q(_{t-1}|_{t},_{0})=q(_{t}| _{t-1})_{t-1}|_{0})}{q(_{t}| _{0})}\), we can expand the reversed conditional transition \(q(_{t-1}|_{t},_{0})\) as Eq. (18). We also use an additional function \(C(_{t},_{0})\) to absorb ineffective terms.

\[q(_{t-1}|_{t},_{0})\] \[=q(_{t}|_{t-1},_{0})_{t-1}|_{0})}{q(_{t}|_{0})}\] \[-(_{t}-k_{t} _{t-1})^{2}}{w_{t}^{2}}+_{t-1}-(t-1) _{0})^{2}}{^{2}(t-1)}-_{t}-( t)_{0})^{2}}{^{2}(t)})\] \[=-(_{t}^{2}-2k_{t}_{t}_{t-1}+k_{t}^{2}_{t-1}^{2}}{w_{t}^{2}}+_{t-1}^{2}-2(t-1)_{0}_{t-1}+^{2}(t-1) _{0}^{2}}{^{2}(t-1)}-_{t}-(t) _{0})^{2}}{^{2}(t)})\] \[=-((^{2}}{w_{t}^{2}}+^{2}(t-1)})_{t-1}^{2}-(}{w_{t}^{2}}_{t}+(t-1)}{^{2}(t-1)}_{0})_{t -1}+C(_{t},_{0}))\] (18)

Thus, \(a(t)\) and \(b(t)\) can be derived as Eq. (19)

\[a(t)_{t}+b(t)_{0} =(}{w_{t}^{2}}_{t}+(t-1)}{ ^{2}(t-1)}_{0})/(^{2}}{w_{t}^{2}}+^{2}(t-1)})\] \[=(}{w_{t}^{2}}_{t}+(t-1)}{ ^{2}(t-1)}_{0})^{2}^{2}(t-1)}{k_{t}^{2 }^{2}(t-1)+w_{t}^{2}}\] (19) \[=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{ t}^{2}}_{t}+(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}} _{0}\]

After comparing the coefficients, we can get \(a(t)=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\) and \(b(t)=(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\). Recall that based on the definition of the forward process \(q(_{t}|_{0}):=((t)_{0}, ^{2}(t))\), we can obtain the reparametrization: \(_{0}=(t)}(_{t}-(t)_{t})\). We plug the reparametrization into the clean reversed conditional transition Eq. (19).

\[_{t}(_{t},_{0})=^{2}(t-1)(t)+(t-1)w_{t}^{2}}{(t)(k_{t}^{2}^{2 }(t-1)+w_{t}^{2})}_{t}-(t-1)w_{t}^{2}}{k_{t}^{2} ^{2}(t-1)+w_{t}^{2}}(t)}{(t)}_ {t}\] (20)

e.1.2 Learned Clean Reversed Conditional Transition \(p_{}(_{t-1}|_{t},_{0})\)

To train a diffusion model that can approximate the clean reversed conditional transition, we define a clean reversed transition \(p_{}(_{t-1}|_{t})\) learned by trainable parameters \(\) as Eq. (21)

\[p_{}(_{t-1}|_{t}):=(_{t-1}; _{}(_{t},_{0},t),s^{2}(t))\] (21)

With similar logic in Eq. (20) and replacing \(_{t}\) with a learned diffusion model \(_{}(_{t},t)\), we can also derive \(_{}(_{t},_{0},t)\) as Eq. (22).

\[_{}(_{t},_{0},t) =^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w _{t}^{2}}_{t}+(t-1)w_{t}^{2}}{k_{t}^{2} ^{2}(t-1)+w_{t}^{2}}((t)}(_{t}-( t)_{}(_{t},t)))\] \[=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w _{t}^{2}}_{t}+(t-1)w_{t}^{2}}{k_{t}^{2} ^{2}(t-1)+w_{t}^{2}}(t)}_{t}-(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}(t)} {(t)}_{}(_{t},t)\] \[=^{2}(t-1)(t)+(t-1 )w_{t}^{2}}{(t)(k_{t}^{2}^{2}(t-1)+w_{t}^{2})}_{t}-(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}} (t)}{(t)}_{}(_{t},t)\] (22)

#### e.1.3 Loss Function of Clean Diffusion Models

The KL-divergence loss of the reversed transition can be simplified as Eq. (23), which uses mean-matching as an approximation of the KL-divergence.

\[D_{KL}(q(_{t-1}|_{t},_{0})||p_{ }(_{t-1}|_{t}))\] \[ ||_{t}(_{t},_{0})-_{}(_ {t},_{0},t)||^{2}\] \[= ||(-(t-1)w_{t}^{2}}{k_{t}^{2} ^{2}(t-1)+w_{t}^{2}}(t)}{(t)}_ {t})-(-(t-1)w_{t}^{2}}{k_{t}^{2}^{2}( t-1)+w_{t}^{2}}(t)}{(t)}_{}( _{t},t))||^{2}\] (23) \[ ||_{t}-_{}(_{t},t)||^{2}\]

Thus, we can finally write down the clean loss function Eq. (24) with reparametrization \(_{t}(,)=(t)+(t) ,\ (0,)\).

\[_{c}(,t,):=||-_{}( _{t}(,),t)||^{2}\] (24)

### Backdoor Diffusion Model via Numerical Reparametrization

This section will further extend the derivation of the clean diffusion models in Appendix E.1 and derive the backdoor reversed conditional transition \(q(_{t-1}^{}|_{t}^{},_{0}^{})\) and the backdoor loss function in Appendix E.2.1.

e.2.1 Backdoor Reversed Conditional Transition \(q(_{t-1}^{}|_{t}^{},_{0}^{})\)

Recall the definition of the backdoor reversed conditional transition in Eq. (25). For clarity, We mark the coefficients of the \(\) as red.

\[q(_{t-1}^{}|_{t}^{},_{0}^{}):= (_{t}^{}(_{t}^{},_{0}^{}), s^{2}(t)),\ _{t}^{}(_{t}^{},_{0}^{})=a(t)_{t}^{ }+c(t)+b(t)_{0}^{}\] (25)

We firstly show that the temporal content, noise, and correction schedulers are \(a(t)=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\), \(b(t)=(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\), and \(c(t)=^{2}(t-1)-k_{t}h_{t}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\). Thus, first of all, we can expand the reversed conditional transition \(q(_{t-1}^{}|_{t}^{},_{0}^{})\) as Eq. (26). To absorb the ineffective terms, we introduce an additional function \(C^{}(^{}_{t},^{}_{0})\). We mark the coefficients of the \(\) as red.

\[q(^{}_{t-1}|^{}_{t},^{ }_{0})\] \[=q(^{}_{t}|^{}_{t-1},^{ }_{0})^{}_{t-1}|^{}_{0})}{q( ^{}_{t}|^{}_{0})}\] \[-^{}_{ t}-k_{t}^{}_{t-1}-h_{t})^{2}}{w_{t}^{2}}+^{}_{t-1}-(t-1)^{}_{0}-( t-1))^{2}}{^{2}(t-1)}\] \[-^{}_{t}-(t) ^{}_{0}-(t))^{2}}{^{2}(t)} \] \[=-^{}_{t}-k_{ t}^{}_{t-1})^{2}-2(^{}_{t}-k_{t}^{ }_{t-1})h_{t}+h_{t}^{2}^{2}}{w_{t}^{2}}\] \[+^{}_{t-1}-(t-1) ^{}_{0})^{2}-2(^{}_{t-1}-(t-1) ^{}_{0})(t-1)+(t-1)^{2}^{2}}{^{2}(t-1)}\] (26) \[-^{}_{t}-(t) ^{}_{0}-(t))^{2}}{^{2}(t)} \] \[=-^{}_{t}-k_{ t}^{}_{t-1})^{2}}{w_{t}^{2}}+^{}_{t-1}- (t-1)^{}_{0})^{2}}{^{2}(t-1)}-^{}_{t}-k_{t}^{}_{t-1})h_{t}}{w_{t} ^{2}}\] \[-^{}_{t-1}-(t-1) ^{}_{0})(t-1)}{^{2}(t-1)}- ^{}_{t}-(t)^{}_{0}-(t))^{2}}{^{2}(t)}\] \[=-(^{2}}{w_{t}^{2}}+ {1}{^{2}(t-1)})^{ 2}_{t-1}-2(}{w_{t}^{2}} ^{}_{t}+(t-1)}{^{2}(t-1)}^{}_{0}\] \[+((t-1)}{^{2}(t-1)}-h_{t}}{w_{t}^{2}})^{}_{t-1}+C^{}(^{}_{t},^{}_{0}))\]

Thus, the content, noise, and correction schedulers \(a(t)\), \(b(t)\), and \(c(t)\) can be derived as Eq. (27). We mark the coefficients of the \(\) as red.

\[a(t)^{}_{t}+c(t)+b(t)^{ }_{0} =(}{w_{t}^{2}}^{}_{t}+(t-1 )}{^{2}(t-1)}^{}_{0}+((t-1)}{^{2}(t-1)}-h_{t}}{w_{t}^{2}}))/(^{2}}{w _{t}^{2}}+^{2}(t-1)})\] \[=(}{w_{t}^{2}}^{}_{t}+( t-1)}{^{2}(t-1)}^{}_{0}+((t-1)}{^{2}(t-1)}-h_{t}}{w_{t}^{2}}))^{2}^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\] \[=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w _{t}^{2}}^{}_{t}+(t-1)w_{t}^{2}}{k_{t}^{2} ^{2}(t-1)+w_{t}^{2}}^{}_{0}\] (27) \[+(^{2}(t-1)}{k_{t}^{2}^{2}(t -1)+w_{t}^{2}}-h_{t}^{2}(t-1)}{k_{t}^{2}^{2}(t -1)+w_{t}^{2}})\] \[=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w _{t}^{2}}^{}_{t}+(t-1)w_{t}^{2}}{k_{t}^{2} ^{2}(t-1)+w_{t}^{2}}^{}_{0}\] \[+^{2}(t-1)-k_{t}h_{t}^{2}(t -1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\]

Thus, after comparing with Eq. (25), we can get \(a(t)=^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\), \(b(t)=(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\), and \(c(t)=^{2}(t-1)-k_{t}h_{t}(t-1)}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}\).

### Backdoor Reversed SDE and ODE

In this section, we will show how to convert the backdoor reversed transition \(q(^{}_{t-1}|^{}_{t})\) to a reversed-time SDE with arbitrary stochasticity by \(q(^{}_{t-1}|^{}_{t},^{}_{0})\). In the first section, referring to , we introduce Lemma 1 as a tool for the conversion between SDE and ODE. Secondly, in Appendix E.3.1 and Appendix E.3.2, we will convert the backdoor and learned reversed transition: \(q(^{}_{t-1}|^{}_{t})\) and \(p_{}(_{t-1}|_{t})\) into the backdoor and learned reversed SDE. In the last section Appendix E.3.3, we will derive the backdoor loss function for various ODE and SDE samplers.

**Lemma 1**: _For a first-order differentiable function \(:^{d}^{d}\), a second-order differentiable function \(:\), and a randomness indicator \(\), the SDE \(d_{t}=(_{t},t)dt+g(t)d}\) and \(d_{t}=[(_{t},t)-g^{2}(t) _{_{t}} p(_{t})]dt+(t)d }\) describe the same stochastic process \(_{t}^{d},t[0,T]\) with the marginal probability \(p(_{t})\), where \(}^{d}\) is the reverse Wiener process._

**Proof E.1**: _For the clarity of the notation, we denote \(p(_{t})\) as \(p(,t)\), follow the Fokker-Planck equation , we can convert the SDE \(d_{t}=f(t)_{t}dt+g(t)d}\) to a partial differential equation Eq. (28) and Eq. (29)._

\[p(,t)& =-_{i=1}^{d}_{i}}(_{i}(,t) p(,t))+_{i=1}^{d}_{j=1}^{d} }{_{i}_{j}}(g^{2}(t)  p(,t))\\ &=-_{i=1}^{d}_{i}}( (,t) p(,t))+_{i=1}^{d} _{j=1}^{d}}{_{i}_{j }}(g^{2}(t) p(,t))\\ &+_{i=1}^{d}_{j=1}^{d}}{_{i}_{j}}(g^{2}(t) p( ,t))\\ &=-_{i=1}^{d}_{i}}((,t) p(,t))+_{i=1}^{d}_{j=1}^{d} }{_{i}_{j}}(g^{2}(t)  p(,t))\\ &+_{i=1}^{d}_{i}}(g^{2}(t),t)}{p(,t)}_{ }p(,t))\] (28)

_To simplify the second-order partial derivative, in the Eq. (29), we apply the log-derivative trick: \( p(,t)_{}p(,t)=}p(,t)}{p(,t)}\)_

\[&=-_{i=1}^{d}_{i}}((,t) p(,t))+_{i=1 }^{d}_{j=1}^{d}}{_{i}_{j}}(g^{2}(t) p(,t))\\ &+_{i=1}^{d}_{i}}((g^{2}(t)_{} p(,t)) p( ,t))\\ &=-_{i=1}^{d}_{i}}(( (,t)-(g^{2}(t)_{}  p(,t))) p(,t))\\ &+_{i=1}^{d}_{j=1}^{d}}{_{i}_{j}}(g^{2}(t) p(,t))\] (29)

_Thus, we can convert the above results back to an SDE with the Fokker-Planck equation with randomness indicator \(\) in Eq. (30). We can see it will reduce to an ODE while \(=0\) and SDE while \(=1\)._

\[d_{t}=[(_{t},t)-g^{2}(t)_ {_{t}} p(_{t})]dt+(t)d}\] (30)

#### e.3.1 Backdoor Reversed SDE with Arbitrary Stochasticity

Since \(q(_{t-1}^{}|_{t}^{}) q(_{t-1} |_{t}^{},_{0}^{})\), we can replace \(_{0}\) of Eq. (25) with reparametrization \(_{0}=_{t}^{}-(t)-(t)_{t}}{(t)}\) from Eq. (25). Note that since the marginal distribution \(q(_{t}^{})\) follows Gaussian distribution, we replace the \(_{t}\) with the normalized conditional score function \(-(t)_{_{t}^{}} q(_{t}^{}| _{0}^{})\) as a kind of reparametrization trick.

\[_{t-1}^{}&=a(t)_ {t}^{}+b(t)_{t}^{}-(t)-(t)(-(t)_{_{t}^{}} q(_{t}^ {}|_{0}^{}))}{(t)}+c(t)+s(t) _{t},\;_{t}(0,)\\ &=(a(t)+(t)})_{t}^{}+(c(t)- (t)}{(t)})-(t)}{ (t)}(-(t)_{_{t}^{}} q(_ {t}^{}|_{0}^{}))+s(t)_{t}\] (31)Then, based on Eq. (31), we approximate the dynamic \(d_{t}^{}\) with Taylor expansion as Eq. (32)

\[d_{t}^{}=(a(t)+(t)}-1)_ {t}^{}+(c(t)-(t)}{(t)})-(t)}{(t)}(-(t)_{_{t}^{ }} q(_{t}^{}|_{0}^{}))dt+s(t )d}\] (32)

With proper reorganization, we can express the SDE Eq. (32) as Eq. (33)

\[d_{t}^{}=F(t)_{t}^{}-G^{2}(t)(-(t)_{_{t}^{}} q(_{t}^{}| _{0}^{})-(t)})dt+s(t)d}\] (33)

We denote \(F(t)=a(t)+(t)}-1\), \(H(t)=c(t)-(t)}{(t)}\), and \(G(t)=(t)}{(t)}}\). Since we also assume the forward process \(q(_{t}|_{0})\) and \(q(_{t}^{}|_{0}^{})\) are diffusion processes, thus the coefficient \(s(t)\) can be derived as \(s(t)=(t)}G(t)=(t)}}(t)\). Then, considering different stochasticity of various samplers, we can apply Lemma 1 and introduce an additional stochasticity indicator \(\) in Eq. (34).

\[d_{t}^{} =F(t)_{t}^{}-G^{2}(t)(-(t) _{_{t}^{}} q(_{t}^{}|_{0} ^{})-(t)})dt+s(t)d}\] \[=F(t)_{t}^{}-G^{2}(t)(-(t) _{_{t}^{}} q(_{t}^{}|_{0} ^{})-(t)})-s^{2}(t)_{ _{t}^{}} q(_{t}^{}|_{0}^{ })dt+s(t)d}\] \[=F(t)_{t}^{}-G^{2}(t) (t)_{_{t}^{}} q(_ {t}^{}|_{0}^{})-(t)})}_{}dt+G(t)(t)}d}\] (34)

#### e.3.2 Learned Reversed SDE with Arbitrary Stochasticity

Since \(q(_{t-1}|_{t}) q(_{t-1}|_{t}, _{0})\), we can replace \(_{0}\) of Eq. (25) with \(_{0}=_{t}-(t)_{}(_ {t},t)}{(t)}\), which is derived from the reparametrization of the forward process \(_{t}=(t)_{0}+(t)_{t}\) with the replacement \(_{t}\) with \(_{}(_{t},t)\).

\[_{t-1}&=a(t)_{t }+b(t)_{t}-(t)_{}(_{t},t)}{ (t)}+s(t)_{t},\;_{t}(0,)\\ &=(a(t)+(t)})_{t}-(t)}{(t)}_{}(_{t},t)+s(t) _{t}\] (35)

Then, according to Eq. (35), we approximate the dynamic \(d_{t}\) with Taylor expansion as Eq. (36)

\[d_{t}=(a(t)+(t)}-1)_{t}- (t)}{(t)}_{}(_{t},t) dt+s(t)d}\] (36)

With proper reorganization, we can express the SDE Eq. (36) with \(F(t)=a(t)+(t)}-1\), \(G(t)=(t)}{(t)}}\), and \(s(t)=(t)}}(t)\) as Eq. (37).

\[d_{t}=F(t)_{t}-G^{2}(t)_{}(_ {t},t)dt+s(t)d}\] (37)

Then, we also consider arbitrary stochasticity and introduce an additional stochasticity indicator \(\) with Lemma 1. As we use a diffusion model \(_{}\) as an approximation for the normalized score function: \(_{}(_{t},t)=-(t)_{_{t}} q (_{t})\), we can derive the learned reversed SDE with arbitrary stochasticity in Eq. (38).

\[d_{t}=F(t)_{t}-G^{2}(t)_{ }(_{t},t)dt+G(t)(t)}d}\] (38)

#### e.3.3 Loss Function of the Backdoor Diffusion Models

Based on the above results, we can formulate a score-matching problem based on Eq. (34) and Eq. (38) as Eq. (39). The loss function Eq. (39) is also known as denoising-score-matching loss , which is a surrogate of the score-matching problem since the score function \(_{^{}_{t}} q(^{}_{t})\) is intractable.

\[&_{^{}_{t},^{ }_{0}}|(-(t)_{^{}_{t}}  q(^{}_{t}|^{}_{0})-(t)})-_{}(^{}_{t},t)^{2} \\ &|-(t)} (_{0},)-_{}(^{}_{t }(^{}_{0},(_{0},),),t) ^{2}\] (39)

Thus, we can finally write down the backdoor loss function Eq. (40).

\[_{p}(,t,,,,):=| -(t)}(,)- _{}(^{}_{t}(,(, ),),t)^{2}\] (40)

### The Derivation of Conditional Diffusion Models

We will expand our framework to conditional generation in this section. In Appendix E.4.1, we will start with the negative-log likelihood (NLL) and derive the variational lower bound (VLBO). Next, in Appendix E.4.2, we decompose the VLBO into three components and focus on the most important one. in Appendix E.4.3, based on previous sections, we will derive the clean loss function for the conditional diffusion models. The last section Appendix E.4.4 will combine the results of Appendix E.1 and Appendix E.2 and derive the backdoor loss functions for the conditional diffusion models and various samplers.

#### e.4.1 Conditional Negative Log Likelihood (NLL)

To train a conditional diffusion model \(_{}(_{0},)\), we will optimize the joint probability learned by the model \(_{}- p_{}(_{0},)\). We denote \(\) as the condition, which can be prompt embedding for the text-to-image generation, and the \(D_{_{i:T}}\) is the domain of random vectors \(_{i},,_{T},\ _{t}^{d},t[i,T],i T\). Therefore, we can derive the conditional variational lower bound \(L^{C}_{VLB}\) as Eq. (41).

\[- p_{}(_{0},)& =-_{q(_{0})} p_{}(_{0},)\\ &=-_{q(_{0})}_{D_{_{ 1},T}}p_{}(_{0},,_{T},)d_{ 1} d_{T}\\ &=-_{q(_{0})}_{_{1} _{T}}q(_{1},,_{T}|_{0}) (_{0},,_{T},)}{q( _{1},,_{T}|_{0})}d_{1} d _{T}\\ &=-_{q(_{0})}_{q( _{1},,_{T}|_{0})} (_{0},,_{T})}{q(_{1},, _{T}|_{0})}\\ &-_{q(_{0},,_{T})} (_{0},,_{T},)}{q( _{1},,_{T}|_{0})}=L^{C}_{VLB} \] (41)

#### e.4.2 Conditional Variational Lower Bound (VLBO)

In this section, we will further decompose the VLBO Eq. (41) and show that minimizing the KL-divergence \(D_{KL}(q(_{t-1}|_{t},_{0})||p_{}(_{t-1}|_{t},))\) is our main objective in Eq. (43). For the simplicity, we denote \(_{q(_{0},,_{T})}\) as \(_{q}\). With Markovian assumption, the latent \(_{t}\) at the timestep \(t\) only depends on the previous latent \(_{t-1}\) and the condition \(\).

\[ L^{C}_{VLB}&=-_{q} (_{0},_{T},)}{q( _{1},,_{T}|_{0})}=_{q} _{1},,_{T}|_{0})}{p_{ }(_{0},,_{T})}\\ &=_{q}^{T}q(_{t}| _{t-1})}{p_{}(_{T},)_{t=1}^{T}p_{ }(_{t-1}|_{t},)}\\ &=_{q}- p_{}(_{T},)+ _{t=1}^{T}_{t}|_{t-1})}{p_{}(_{t-1}|_{t},)}\\ &=_{q}- p_{}(_{T},)+ _{t=2}^{T}_{t}|_{t-1})}{p_{}( _{t-1}|_{t},)}_{t}| _{0})}{q(_{t-1}|_{0})}+_{1}|_{0})}{p_{}(_{0}|_{1},)}\\ &=_{q}- p_{}(_{T},)+ _{t=2}^{T}_{t-1}|_{t},_{0})}{p_{ }(_{t-1}|_{t},)}_{t}| _{0})}{q(_{t-1}|_{0})}+_{1}|_{0})}{p_{}(_{0}|_{1}, )}\] (42)\[=_{q}- p_{}(_{T},)+ _{t=2}^{T}_{t-1}|_{t},_{0})}{p_{ }(_{t-1}|_{t},)}+_{t=2}^{T}_{t}|_{0})}{q(_{t-1}|_{0})}+ {q(_{1}|_{0})}{p_{}(_{0}|_{1}, )}\] \[=_{q}- p_{}(_{T},)+ _{t=2}^{T}_{t-1}|_{t},_{0})}{p_{ }(_{t-1}|_{t},)}+_{ t}|_{0})}{q(_{1}|_{0})}+_{1}| _{0})}{p_{}(_{0}|_{1},)}\] \[=_{q}_{T}|_{0})} {p_{}(_{T},)}+_{t=2}^{T}_{ t-1}|_{t},_{0})}{p_{}(_{t-1}|_{t}, )}- p_{}(_{0}|_{1},)\] \[=_{q}D_{KL}(q(_{T}|_{0})||p_ {}(_{T},))+_{t=2}^{T}D_{KL}(q(_{t-1}| _{t},_{0})||p_{}(_{t-1}|_{t}, ))- p_{}(_{0}|_{1},)\] \[=_{q}_{T}^{C}(_{T},_{0},)+_{t=2}^{T}_{t}^{C}(_{t},_{t-1},_{0},)-_{0}^{C}(_{1},_{0}, )\] (43)

#### e.4.3 Clean Loss Function for the Conditional Diffusion Models

We define the learned reversed transition \(p_{}(_{t-1}|_{t},)\) as Eq. (44).

\[p_{}(_{t-1}|_{t},):=( _{t-1};_{}(_{t},_{0},t,),s^{2}(t) )\] (44)

We plug in a conditional diffusion model \(_{}(_{t},t,)\) to replace the unconditional diffusion model \(_{}(_{t},t)\).

\[_{}(_{t},_{0},t,) =^{2}(t-1)}{k_{t}^{2}^{2}(t-1)+ w_{t}^{2}}_{t}+(t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}}((t)}(_{t}- {}(t)_{}(_{t},t,)))\] (45) \[=^{2}(t-1)(t)+(t-1) w_{t}^{2}}{(t)(k_{t}^{2}^{2}(t-1)+w_{t}^{2})}_{t}- (t-1)w_{t}^{2}}{k_{t}^{2}^{2}(t-1)+w_{t}^{2}} {(t)}{(t)}_{}(_{t},t,)\]

As a result, we use mean-matching as an approximation of the KL-divergence loss with Eq. (46).

\[D_{KL}(q(_{t-1}|_{t},_{0})||p_{ }(_{t-1}|_{t},))||_{t}(_{t},_{0})-_{}(_{t},_{0},t, )||^{2}||_{t}-_{}(_{t},t,)||^ {2}\] (46)

Finally, we can reorganize the Eq. (46) as Eq. (47), which is the clean loss function for the conditional diffusion models.

\[_{c}^{C}(,t,,):=- _{}(_{t}(,),t,) ^{2}\] (47)

#### e.4.4 Loss Function of the Backdoor Conditional Diffusion Models

Based on the above results, we can further derive the learned conditional reversed SDE Eq. (48), while the backdoor one remains the same as Eq. (34), which is caused by the identical backdoor reversed transition \(q(_{t-1}^{}|_{t}^{},_{0}^{})\) of the KL-divergence loss.

\[d_{t}=F(t)_{t}-G^{2}(t)_{ }(_{t},t,)dt+G(t)(t)}d }\] (48)

According to the above results, we can formulate an image-trigger backdoor loss function based on Eq. (34) and Eq. (48) as Eq. (49). The loss function Eq. (49) is also known as denoising-score-matching loss , which is a surrogate of the score-matching problem since the score function \(_{_{t}^{}} q(_{t}^{})\) is intractable. Here we denote the reparametrization \(_{t}^{}(,,)=(t)+(t)+(t)\).

\[_{_{t}^{},_{0}^{}} (-(t)_{_{t}^{}} q(_{t}^{ }|_{0}^{})-(t)})- _{}(_{t},t,)^{2}\] (49) \[ -(t)}( _{0},)-_{}(_{t}^{}(_{0} ^{},(_{0},),),t,) ^{2}\]

Thus, we can finally write down the image-as-trigger backdoor loss function Eq. (50) for the conditional diffusion models.

\[_{p}^{CI}(,t,,,,, ):=-(t)}( ,)-_{}(_{t}^{}(, (,),),t,)^{2}\] (50)Additional Experiments

### Backdoor Attacks on DDPM with CIFAR10 Dataset

We will present experimental results for more backdoor trigger-target pairs and samplers, including the LMSD sampler, which is implemented by the authors of EDM , in Fig. 7. The results of the ANCESTRAL sampler come from .

Figure 7: FID and MSE scores of various samplers and poison rates for DDPM  and the CIFAR10 dataset. We express trigger-target pairs as (trigger, target).

### Backdoor Attacks on DDPM with CelebA-HQ Dataset

We evaluate our method with more samplers and backdoor trigger-target pairs: (Stop Sign, Hat) and (Eyeglasses, Cat) in Fig. 8. Note that the results of the ANCESTRAL sampler come from .

### Backdoor Attacks on Latent Diffusion Models (LDM)

We also attack the latent diffusion model  downloaded from Huggingface (_CompVis/ldm-celebahq-256_), which is pre-trained on CelebA-HQ . The pre-trained latent diffusion models (LDM)  are trained on CelebA-HQ with 512 \(\) 512 resolution and 64 \(\) 64 latent space. We fine-tune them with learning rate 2e-4 and batch size 16 for 2000 epochs. We examine our method with trigger-target pair: (Eyeglasses, Cat) and (Stop Sign, Hat) and illustrate the FID and MSE score in Fig. 9. As the Fig. 9 shows, the LDM can be backdoored successfully for the trigger-target pairs: (Stop Sign, Hat) with 70% poison rate. Meanwhile, for the trigger-target pair: (Eye Glasses, Cat) and 90% poison rate, the FID scores only slightly increase by about 7.2% at most. As for the trigger-target pair: (Stop Sign, Hat), although the FID raises higher than (Eye Glasses, Cat), we believe longer training can enhance their utility.

### Backdoor Attacks on Score-Based Models

We trained the score-based model: NCSN [49; 47; 48] on the CIFAR10 dataset with the same model architecture as the DDPM  by ourselves for 800 epochs and set the learning rate as 1e-4 and batch size as 128. The FID score of the clean model generated by predictor-correction samplers (SCORE-SDE-VE ) for the variance explode models  is about 10.87. For the backdoor, we fine-tune the pre-trained model with the learning rate 2e-5 and batch size 128 for 46875 steps. To enhance the backdoor specificity and utility, we augment Gaussian noise into the training dataset, which means the poisoned image \(\) will be replaced by a pure trigger \(\). The augmentation can let the model learn to activate the backdoor even if there are no context images. We present our results in Fig. 10 and can see with 70% augment rate, our method can achieve 70% attack success rate based on Fig. 10c as the FID score increases by 12.7%. Note that the augment rate is computed by the number of augmented Gaussian noises / the size of the original training dataset.

Figure 8: FID and MSE scores of various samplers and poison rates for the DDPM  and the CelebA-HQ dataset. We express trigger-target pairs as (trigger, target).

Figure 9: FID and MSE scores of various samplers and poison rates for the latent diffusion model (LDM)  and the CelebA-HQ dataset. We express trigger-target pairs as (trigger, target).

[MISSING_PAGE_FAIL:21]

Figure 11: FID and MSE scores of various samplers and poison rates with inference-time defense . We evaluate the defense on the DDPM and the CIFAR10 dataset and express trigger-target pairs as (trigger, target).

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 11.52 & 8.09 & 7.62 & 7.97 & 7.46 & 7.68 & 7.38 & 7.22 \\  & MSE & 1.48E-1 & 6.81E-2 & 9.47E-3 & 2.35E-4 & 5.59E-6 & 4.19E-6 & 3.96E-6 & 3.80E-6 \\  & SSIM & 6.84E-4 & 4.35E-1 & 9.18E-1 & 9.97E-1 & 9.99E-1 & 9.98E-1 & 9.98E-1 \\  & FID & 11.15 & 9.18 & 9.07 & 9.18 & 9.37 & 9.76 & 10.28 & 11.53 \\ UNIPC & MSE & 1.48E-1 & 4.76E-2 & 9.37E-3 & 1.30E-4 & 8.05E-5 & 2.27E-4 & 3.56E-5 & 3.24E-5 \\  & SSIM & 8.13E-4 & 5.27E-1 & 8.57E-1 & 9.76E-1 & 9.81E-1 & 9.74E-1 & 9.86E-1 & 9.84E-1 \\  & FID & 11.15 & 9.18 & 9.07 & 9.07 & 9.37 & 9.76 & 10.28 & 11.53 \\ DPM. O2 & MSE & 1.48E-1 & 4.76E-2 & 9.37E-3 & 1.24E-4 & 8.05E-5 & 2.27E-4 & 3.56E-5 & 3.24E-5 \\  & SSIM & 8.13E-4 & 5.27E-1 & 8.57E-1 & 9.80E-1 & 9.81E-1 & 9.74E-1 & 9.86E-1 & 9.84E-1 \\  & FID & 11.15 & 9.18 & 9.07 & 9.18 & 9.37 & 9.57 & 10.28 & 11.53 \\ DPM. O3 & MSE & 1.48E-1 & 4.76E-2 & 9.37E-3 & 1.30E-4 & 8.05E-5 & 7.48E-5 & 3.56E-5 & 3.24E-5 \\  & SSIM & 8.13E-4 & 5.27E-1 & 8.57E-1 & 9.76E-1 & 9.81E-1 & 9.80E-1 & 9.86E-1 & 9.84E-1 \\  & FID & 11.15 & 9.18 & 9.07 & 9.18 & 9.37 & 9.76 & 10.28 & 11.53 \\ DPM++. O2 & MSE & 1.48E-1 & 4.76E-2 & 9.37E-3 & 1.30E-4 & 8.05E-5 & 2.27E-4 & 3.56E-5 & 3.24E-5 \\  & SSIM & 8.13E-4 & 5.27E-1 & 8.57E-1 & 9.76E-1 & 9.81E-1 & 9.74E-1 & 9.86E-1 & 9.84E-1 \\  & FID & 11.15 & 9.18 & 9.07 & 9.07 & 9.37 & 9.57 & 10.28 & 11.53 \\ DEIS & MSE & 1.48E-1 & 4.76E-2 & 9.37E-3 & 1.28E-4 & 8.05E-5 & 7.48E-5 & 3.56E-5 & 3.24E-5 \\  & SSIM & 8.13E-4 & 5.27E-1 & 8.57E-1 & 9.76E-1 & 9.81E-1 & 9.80E-1 & 9.86E-1 & 9.84E-1 \\  & FID & 16.39 & 10.95 & 10.71 & 10.70 & 11.16 & 11.32 & 12.40 & 14.43 \\ DDIM & MSE & 1.48E-1 & 7.14E-2 & 2.47E-2 & 6.84E-4 & 4.95E-6 & 3.70E-6 & 3.58E-6 & 3.51E-6 \\  & SSIM & 8.92E-4 & 3.74E-1 & 7.63E-1 & 9.92E-1 & 1.00E+0 & 9.99E-1 & 9.99E-1 & 9.99E-1 \\  & FID & 12.14 & 7.24 & 7.13 & 7.25 & 7.42 & 7.77 & 8.18 & 9.83 \\ PNDM & MSE & 1.48E-1 & 6.55E-2 & 1.97E-2 & 1.23E-4 & 3.74E-5 & 3.25E-5 & 2.86E-5 & 2.76E-5 \\  & SSIM & 8.23E-4 & 4.11E-1 & 7.91E-1 & 9.81E-1 & 9.83E-1 & 9.83E-1 & 9.83E-1 & 9.83E-1 \\  & FID & 12.14 & 7.24 & 7.13 & 7.25 & 7.42 & 7.60 & 8.18 & 9.83 \\ HEUN & MSE & 1.48E-1 & 6.55E-2 & 1.97E-2 & 1.23E-4 & 3.74E-5 & 3.16E-5 & 2.86E-5 & 2.76E-5 \\  & SSIM & 8.23E-4 & 4.11E-1 & 7.91E-1 & 9.81E-1 & 9.83E-1 & 9.83E-1 & 9.83E-1 & 9.83E-1 \\  & FID & 12.14 & 7.24 & 7.13 & 7.24 & 7.42 & 7.47 & 8.18 & 9.83 \\ LMSD & MSE & 1.48E-1 & 6.55E-2 & 1.97E-2 & 5.02E-4 & 3.74E-5 & 3.23E-5 & 2.86E-5 & 2.76E-5 \\  & SSIM & 8.23E-4 & 4.11E-1 & 7.91E-1 & 9.76E-1 & 9.83E-1 & 9.83E-1 & 9.83E-1 & 9.83E-1 \\   

Table 2: DDPM backdoor on CIFAR10 Dataset with Trigger: Stop Sign, target: No Shift

Figure 12: LPIPS and MSE scores of various samplers and poison rates for the 3 kinds of inpainting tasks: **Blur**, **Line**, and **Box**. The backdoor model is DDPM trained on the CIFAR10 dataset. We express trigger-target pairs as (trigger, target).

[MISSING_PAGE_FAIL:24]

   &  & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  &  & & & & & & & & \\   & FID & 14.83 & 8.38 & 7.83 & 8.35 & 8.08 & 8.14 & 7.85 & 7.98 \\  & MSE & 1.06E-1 & 7.22E-2 & 4.20E-3 & 7.09E-4 & 6.13E-5 & 2.37E-5 & 1.41E-5 & 3.85E-6 \\  & SSIM & 9.85E-4 & 2.65E-1 & 9.49E-1 & 9.89E-1 & 9.97E-1 & 9.97E-1 & 9.97E-1 & 9.97E-1 \\  & FID & 11.15 & 9.03 & 8.83 & 9.09 & 9.16 & 9.61 & 10.61 & 12.67 \\ UNIPC & MSE & 1.06E-1 & 7.85E-2 & 9.05E-3 & 2.47E-4 & 1.17E-4 & 2.73E-4 & 4.75E-4 & 1.63E-4 \\  & SSIM & 1.09E-3 & 1.34E-1 & 7.52E-1 & 9.38E-1 & 9.56E-1 & 9.53E-1 & 9.57E-1 & 9.72E-1 \\  & FID & 11.15 & 9.03 & 8.83 & 9.09 & 9.16 & 9.61 & 10.61 & 12.67 \\ DPM. O2 & MSE & 1.06E-1 & 7.85E-2 & 9.05E-3 & 2.47E-4 & 1.17E-4 & 2.73E-4 & 4.75E-4 & 1.63E-4 \\  & SSIM & 1.09E-3 & 1.34E-1 & 7.52E-1 & 9.38E-1 & 9.56E-1 & 9.53E-1 & 9.57E-1 & 9.72E-1 \\  & FID & 11.15 & 9.03 & 8.83 & 9.09 & 9.16 & 9.61 & 10.61 & 12.67 \\ DPM. O3 & MSE & 1.06E-1 & 7.85E-2 & 9.05E-3 & 2.47E-4 & 1.17E-4 & 2.73E-4 & 4.75E-4 & 1.63E-4 \\  & SSIM & 1.09E-3 & 1.34E-1 & 7.52E-1 & 9.38E-1 & 9.56E-1 & 9.53E-1 & 9.57E-1 & 9.72E-1 \\  & FID & 11.15 & 9.03 & 8.83 & 9.09 & 9.16 & 9.61 & 10.61 & 12.67 \\ DPM++. O2 & MSE & 1.06E-1 & 7.85E-2 & 9.05E-3 & 2.47E-4 & 1.17E-4 & 2.73E-4 & 4.75E-4 & 1.63E-4 \\  & SSIM & 1.09E-3 & 1.34E-1 & 7.52E-1 & 9.38E-1 & 9.56E-1 & 9.53E-1 & 9.57E-1 & 9.72E-1 \\  & FID & 11.15 & 9.03 & 8.83 & 9.09 & 9.16 & 9.61 & 10.61 & 12.67 \\ DPM++. O3 & MSE & 1.06E-1 & 7.85E-2 & 9.05E-3 & 2.47E-4 & 1.17E-4 & 2.73E-4 & 4.75E-4 & 1.63E-4 \\  & SSIM & 1.09E-3 & 1.34E-1 & 7.52E-1 & 9.38E-1 & 9.56E-1 & 9.53E-1 & 9.57E-1 & 9.72E-1 \\  & FID & 16.39 & 10.74 & 10.54 & 10.85 & 10.92 & 11.74 & 12.53 & 15.57 \\ DDIM & MSE & 1.06E-1 & 9.16E-2 & 2.54E-2 & 1.05E-3 & 7.27E-6 & 3.84E-6 & 3.84E-6 \\  & SSIM & 1.12E-3 & 6.38E-2 & 6.36E-1 & 9.79E-1 & 1.00E+0 & 1.00E+0 & 9.99E-1 & 9.98E-1 \\  & FID & 12.14 & 7.22 & 7.14 & 7.15 & 7.31 & 7.59 & 8.39 & 10.74 \\ PNDM & MSE & 1.06E-1 & 8.87E-2 & 1.94E-2 & 6.28E-4 & 4.24E-5 & 3.09E-5 & 2.89E-5 & 2.70E-5 \\  & SSIM & 1.08E-3 & 7.93E-2 & 6.84E-1 & 9.58E-1 & 9.73E-1 & 9.76E-1 & 9.75E-1 & 9.76E-1 \\  & FID & 12.14 & 7.22 & 7.14 & 7.15 & 7.31 & 7.59 & 8.39 & 10.74 \\ HEUN & MSE & 1.06E-1 & 8.87E-2 & 1.94E-2 & 6.28E-4 & 4.24E-5 & 3.09E-5 & 2.89E-5 & 2.70E-5 \\  & SSIM & 1.08E-3 & 7.93E-2 & 6.84E-1 & 9.58E-1 & 9.73E-1 & 9.76E-1 & 9.75E-1 & 9.76E-1 \\  & FID & 12.14 & 7.22 & 7.14 & 7.15 & 7.31 & 7.59 & 8.39 & 10.74 \\ LMSD & MSE & 1.06E-1 & 8.87E-2 & 1.94E-2 & 6.28E-4 & 4.24E-5 & 3.09E-5 & 2.89E-5 & 2.70E-5 \\  & SSIM & 1.08E-3 & 7.93E-2 & 6.84E-1 & 9.58E-1 & 9.73E-1 & 9.76E-1 & 9.75E-1 & 9.76E-1 \\  

Table 4: DDPM backdoor on CIFAR10 Dataset with Trigger: Stop Sign, target: Corner

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 11.52 & 8.33 & 7.47 & 8.10 & 7.52 & 7.69 & 7.35 & 7.54 \\  & MSE & 3.38E-1 & 1.66E-1 & 3.61E-3 & 2.30E-5 & 5.62E-6 & 3.35E-6 & 2.72E-6 & 2.39E-6 \\  & SSIM & 1.69E-4 & 4.20E-1 & 9.85E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 11.15 & 9.03 & 9.14 & 8.96 & 9.09 & 9.74 & 10.41 & 12.00 \\ UNIPC & MSE & 3.38E-1 & 9.33E-2 & 1.31E-2 & 1.47E-4 & 1.10E-4 & 8.83E-5 & 1.04E-4 & 7.49E-5 \\  & SSIM & 2.15E-4 & 5.91E-1 & 9.13E-1 & 9.89E-1 & 9.91E-1 & 9.92E-1 & 9.91E-1 & 9.92E-1 \\  & FID & 11.15 & 9.03 & 9.14 & 8.96 & 9.09 & 9.74 & 10.41 & 12.00 \\ DPM. O2 & MSE & 3.38E-1 & 9.33E-2 & 1.31E-2 & 1.47E-4 & 1.10E-4 & 8.83E-5 & 1.04E-4 & 7.49E-5 \\  & SSIM & 2.15E-4 & 5.91E-1 & 9.13E-1 & 9.89E-1 & 9.91E-1 & 9.92E-1 & 9.91E-1 & 9.92E-1 \\  & FID & 11.15 & 9.03 & 9.14 & 8.96 & 9.09 & 9.74 & 10.41 & 12.00 \\ DPM. O3 & MSE & 3.38E-1 & 9.33E-2 & 1.31E-2 & 1.47E-4 & 1.10E-4 & 8.83E-5 & 1.04E-4 & 7.49E-5 \\  & SSIM & 2.15E-4 & 5.91E-1 & 9.13E-1 & 9.89E-1 & 9.91E-1 & 9.92E-1 & 9.91E-1 & 9.92E-1 \\  & FID & 11.15 & 9.03 & 9.14 & 8.96 & 9.09 & 9.74 & 10.41 & 12.00 \\ DPM++. O2 & MSE & 3.38E-1 & 9.33E-2 & 1.31E-2 & 1.47E-4 & 1.10E-4 & 8.83E-5 & 1.04E-4 & 7.49E-5 \\  & SSIM & 2.15E-4 & 5.91E-1 & 9.13E-1 & 9.89E-1 & 9.91E-1 & 9.92E-1 & 9.91E-1 & 9.92E-1 \\ DPM++. O3 & MSE & 3.38E-1 & 9.33E-2 & 1.31E-2 & 1.47E-4 & 1.10E-4 & 8.83E-5 & 1.04E-4 & 7.49E-5 \\  & SSIM & 2.15E-4 & 5.91E-1 & 9.13E-1 & 9.89E-1 & 9.91E-1 & 9.92E-1 & 9.91E-1 & 9.92E-1 \\  & FID & 11.15 & 9.03 & 9.14 & 8.96 & 9.09 & 9.74 & 10.41 & 12.00 \\ DEIS & MSE & 3.38E-1 & 9.33E-2 & 1.31E-2 & 1.47E-4 & 1.10E-4 & 8.83E-5 & 1.04E-4 & 7.49E-5 \\  & SSIM & 2.15E-4 & 5.91E-1 & 9.13E-1 & 9.89E-1 & 9.91E-1 & 9.92E-1 & 9.91E-1 & 9.92E-1 \\  & FID & 16.39 & 10.71 & 10.75 & 10.68 & 10.87 & 11.86 & 12.73 & 14.94 \\ DDIM & MSE & 3.37E-1 & 1.56E-1 & 3.96E-2 & 1.09E-4 & 7.39E-6 & 2.42E-6 & 2.00E-6 & 1.98E-6 \\  & SSIM & 2.40E-4 & 3.97E-1 & 8.14E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 12.14 & 7.12 & 7.20 & 7.17 & 7.25 & 7.79 & 8.33 & 10.35 \\ PNDM & MSE & 3.38E-1 & 1.39E-1 & 2.94E-2 & 1.35E-4 & 4.89E-5 & 3.51E-5 & 2.97E-5 & 2.74E-5 \\  & SSIM & 2.17E-4 & 4.51E-1 & 8.53E-1 & 9.94E-1 & 9.94E-1 & 9.95E-1 & 9.95E-1 & 9.95E-1 \\  & FID & 12.14 & 7.12 & 7.20 & 7.17 & 7.25 & 7.79 & 8.33 & 10.35 \\ HEUN & MSE & 3.38E-1 & 1.39E-1 & 2.94E-2 & 1.35E-4 & 4.89E-5 & 3.51E-5 & 2.97E-5 & 2.74E-5 \\  & SSIM & 2.17E-4 & 4.51E-1 & 8.53E-1 & 9.94E-1 & 9.94E-1 & 9.95E-1 & 9.95E-1 & 9.95E-1 \\  & FID & 12.14 & 7.12 & 7.20 & 7.17 & 7.25 & 7.79 & 8.33 & 10.35 \\ LMSD & MSE & 3.38E-1 & 1.39E-1 & 2.94E-2 & 1.35E-4 & 4.89E-5 & 3.51E-5 & 2.97E-5 & 2.74E-5 \\  & SSIM & 2.17E-4 & 4.51E-1 & 8.53E-1 & 9.94E-1 & 9.94E-1 & 9.95E-1 & 9.95E-1 \\   

Table 5: DDPM backdoor on CIFAR10 Dataset with Trigger: Stop Sign, target: Shoe

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  Sampler & Metric & & & & & & & & \\   & FID & 14.83 & 8.32 & 7.57 & 8.17 & 7.77 & 7.73 & 7.77 \\  & MSE & 2.41E-1 & 7.99E-2 & 4.33E-3 & 2.85E-4 & 9.16E-5 & 1.30E-5 & 3.21E-6 & 2.81E-6 \\  & SSIM & 4.74E-5 & 6.52E-1 & 9.80E-1 & 9.98E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 11.15 & 8.94 & 8.95 & 8.97 & 9.38 & 9.51 & 10.11 & 12.08 \\ UNIPC & MSE & 2.41E-1 & 2.50E-2 & 1.23E-2 & 1.25E-4 & 1.03E-4 & 7.29E-5 & 2.89E-4 & 8.91E-5 \\  & SSIM & 1.01E-4 & 8.57E-1 & 9.23E-1 & 9.95E-1 & 9.96E-1 & 9.96E-1 & 9.95E-1 & 9.97E-1 \\  & FID & 11.15 & 8.94 & 8.95 & 9.12 & 9.38 & 9.70 & 10.11 & 12.08 \\ DPM. O2 & MSE & 2.41E-1 & 2.50E-2 & 1.23E-2 & 1.30E-4 & 1.03E-4 & 1.43E-4 & 2.89E-4 & 8.91E-5 \\  & SSIM & 1.01E-4 & 8.57E-1 & 9.23E-1 & 9.95E-1 & 9.96E-1 & 9.97E-1 & 9.95E-1 & 9.97E-1 \\  & FID & 11.15 & 8.94 & 8.95 & 8.91 & 9.38 & 9.45 & 10.11 & 12.08 \\ DPM. O3 & MSE & 2.41E-1 & 2.50E-2 & 1.23E-2 & 1.14E-4 & 1.03E-4 & 7.12E-5 & 2.89E-4 & 8.91E-5 \\  & SSIM & 1.01E-4 & 8.57E-1 & 9.23E-1 & 9.95E-1 & 9.96E-1 & 9.96E-1 & 9.95E-1 & 9.97E-1 \\  & FID & 11.15 & 8.94 & 8.95 & 9.12 & 9.38 & 9.70 & 10.11 & 12.08 \\ DPM++. O2 & MSE & 2.41E-1 & 2.50E-2 & 1.23E-2 & 1.30E-4 & 1.03E-4 & 1.43E-4 & 2.89E-4 & 8.91E-5 \\  & SSIM & 1.01E-4 & 8.57E-1 & 9.23E-1 & 9.95E-1 & 9.96E-1 & 9.97E-1 & 9.95E-1 & 9.97E-1 \\  & FID & 11.15 & 8.94 & 8.95 & 8.97 & 9.38 & 9.51 & 10.11 & 12.08 \\ DEIS & MSE & 2.41E-1 & 2.50E-2 & 1.23E-2 & 1.25E-4 & 1.03E-4 & 7.29E-5 & 2.89E-4 & 8.91E-5 \\  & SSIM & 1.01E-4 & 8.57E-1 & 9.23E-1 & 9.95E-1 & 9.96E-1 & 9.95E-1 & 9.97E-1 \\  & FID & 16.39 & 10.63 & 10.77 & 10.76 & 11.12 & 11.33 & 12.40 & 15.13 \\ DDIM & MSE & 2.40E-1 & 5.77E-2 & 3.08E-2 & 2.86E-5 & 3.79E-6 & 2.49E-6 & 2.31E-6 & 2.29E-6 \\  & SSIM & 1.39E-4 & 7.09E-1 & 8.40E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 12.14 & 7.07 & 7.02 & 7.15 & 7.44 & 7.63 & 8.11 & 10.36 \\ PNDM & MSE & 2.41E-1 & 4.85E-2 & 2.41E-2 & 6.43E-5 & 4.21E-5 & 3.67E-5 & 3.04E-5 & 2.82E-5 \\  & SSIM & 1.05E-4 & 7.51E-1 & 8.70E-1 & 9.97E-1 & 9.97E-1 & 9.98E-1 & 9.98E-1 & 9.98E-1 \\  & FID & 12.14 & 7.07 & 7.02 & 7.15 & 7.44 & 7.52 & 8.11 & 10.36 \\ HEUN & MSE & 2.41E-1 & 4.85E-2 & 2.41E-2 & 6.43E-5 & 4.21E-5 & 3.48E-5 & 3.04E-5 & 2.82E-5 \\  & SSIM & 1.05E-4 & 7.51E-1 & 8.70E-1 & 9.97E-1 & 9.97E-1 & 9.98E-1 & 9.98E-1 & 9.98E-1 \\  & FID & 12.14 & 7.07 & 7.02 & 7.13 & 7.44 & 7.52 & 8.11 & 10.36 \\ LMSD & MSE & 2.41E-1 & 4.85E-2 & 2.41E-2 & 6.57E-5 & 4.21E-5 & 3.48E-5 & 3.04E-5 & 2.82E-5 \\  & SSIM & 1.05E-4 & 7.51E-1 & 8.70E-1 & 9.97E-1 & 9.97E-1 & 9.98E-1 & 9.98E-1 \\   

Table 6: DDPM backdoor on CIFAR10 Dataset with Trigger: Stop Sign, target: Hat

    &  & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  &  & & & & & & & & & \\   & FID & 11.56 & 9.09 & 9.62 & 11.36 & 12.85 & 17.63 & 25.70 & 52.92 \\  & MSE & 1.21E-1 & 6.19E-2 & 6.11E-3 & 1.18E-5 & 5.89E-6 & 4.09E-6 & 3.91E-6 & 3.86E-6 \\  & SSIM & 7.36E-4 & 4.21E-1 & 9.41E-1 & 9.98E-1 & 9.98E-1 & 9.98E-1 & 9.98E-1 & 9.98E-1 \\  & FID & 11.15 & 9.30 & 9.60 & 10.20 & 11.13 & 13.17 & 16.62 & 30.87 \\ UNIPC & MSE & 1.21E-1 & 2.58E-2 & 5.64E-3 & 1.67E-4 & 6.52E-5 & 6.15E-5 & 1.36E-4 & 1.98E-4 \\  & SSIM & 7.37E-4 & 6.22E-1 & 8.62E-1 & 9.61E-1 & 9.74E-1 & 9.76E-1 & 9.82E-1 & 9.80E-1 \\  & FID & 11.15 & 9.30 & 9.60 & 10.20 & 11.13 & 13.17 & 16.62 & 30.87 \\ DPM. O2 & MSE & 1.21E-1 & 2.58E-2 & 5.64E-3 & 1.67E-4 & 6.52E-5 & 6.15E-5 & 1.36E-4 & 1.98E-4 \\  & SSIM & 7.37E-4 & 6.22E-1 & 8.62E-1 & 9.61E-1 & 9.74E-1 & 9.76E-1 & 9.82E-1 & 9.80E-1 \\  & FID & 11.15 & 9.30 & 9.60 & 10.20 & 11.13 & 13.17 & 16.62 & 30.87 \\ DPM. O3 & MSE & 1.21E-1 & 2.58E-2 & 5.64E-3 & 1.67E-4 & 6.52E-5 & 6.15E-5 & 1.36E-4 & 1.98E-4 \\  & SSIM & 7.37E-4 & 6.22E-1 & 8.62E-1 & 9.61E-1 & 9.74E-1 & 9.76E-1 & 9.82E-1 & 9.80E-1 \\  & FID & 11.15 & 9.30 & 9.60 & 10.20 & 11.13 & 13.17 & 16.62 & 30.87 \\ DPM++. O2 & MSE & 1.21E-1 & 2.58E-2 & 5.64E-3 & 1.67E-4 & 6.52E-5 & 6.15E-5 & 1.36E-4 & 1.98E-4 \\  & SSIM & 7.37E-4 & 6.22E-1 & 8.62E-1 & 9.61E-1 & 9.74E-1 & 9.76E-1 & 9.82E-1 & 9.80E-1 \\  & FID & 11.15 & 9.30 & 9.60 & 10.20 & 11.13 & 13.17 & 16.62 & 30.87 \\ DPM++. O3 & MSE & 1.21E-1 & 2.58E-2 & 5.64E-3 & 1.67E-4 & 6.52E-5 & 6.15E-5 & 1.36E-4 & 1.98E-4 \\  & SSIM & 7.37E-4 & 6.22E-1 & 8.62E-1 & 9.61E-1 & 9.74E-1 & 9.76E-1 & 9.82E-1 & 9.80E-1 \\ DEIS & MSE & 1.21E-1 & 2.58E-2 & 5.64E-3 & 1.67E-4 & 6.52E-5 & 6.15E-5 & 1.36E-4 & 1.98E-4 \\  & SSIM & 7.37E-4 & 6.22E-1 & 8.62E-1 & 9.61E-1 & 9.74E-1 & 9.76E-1 & 9.82E-1 & 9.80E-1 \\  & FID & 16.39 & 10.97 & 11.21 & 12.22 & 13.17 & 15.62 & 19.74 & 35.84 \\ DDIM & MSE & 1.21E-1 & 5.13E-2 & 1.75E-2 & 2.87E-4 & 7.06E-6 & 3.85E-6 & 3.84E-6 & 3.84E-6 \\  & SSIM & 7.38E-4 & 4.04E-1 & 7.63E-1 & 9.94E-1 & 1.00E+0 & 1.00E+0 & 9.99E-1 & 9.98E-1 \\  & FID & 12.14 & 7.88 & 8.34 & 9.38 & 10.80 & 13.73 & 18.47 & 36.29 \\ PNDM & MSE & 1.21E-1 & 4.47E-2 & 1.37E-2 & 2.35E-4 & 4.50E-5 & 3.31E-5 & 3.02E-5 & 2.81E-5 \\  & SSIM & 7.37E-4 & 4.62E-1 & 7.93E-1 & 9.75E-1 & 9.78E-1 & 9.79E-1 & 9.79E-1 & 9.80E-1 \\  & FID & 12.14 & 7.88 & 8.34 & 9.38 & 10.80 & 13.73 & 18.47 & 36.29 \\ HEUN & MSE & 1.21E-1 & 4.47E-2 & 1.37E-2 & 2.35E-4 & 4.50E-5 & 3.31E-5 & 3.02E-5 & 2.81E-5 \\  & SSIM & 7.37E-4 & 4.62E-1 & 7.93E-1 & 9.75E-1 & 9.78E-1 & 9.79E-1 & 9.79E-1 & 9.80E-1 \\  & FID & 12.14 & 7.88 & 8.34 & 9.38 & 10.80 & 13.73 & 18.47 & 36.29 \\ LMSD & MSE & 1.21E-1 & 4.47E-2 & 1.37E-2 & 2.35E-4 & 4.50E-5 & 3.31E-5 & 3.02E-5 & 2.81E-5 \\  & SSIM & 7.37E-4 & 4.62E-1 & 7.93E-1 & 9.75E-1 & 9.78E-1 & 9.79E-1 & 9.79E-1 & 9.80E-1 \\   

Table 7: DDPM backdoor on CIFAR10 Dataset with Trigger: Grey Box, target: No Shift

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 11.56 & 9.09 & 9.78 & 11.26 & 12.41 & 15.55 & 21.78 & 41.54 \\  & MSE & 1.21E-1 & 5.11E-2 & 5.52E-3 & 7.90E-5 & 1.61E-5 & 6.25E-6 & 1.22E-5 & 5.98E-6 \\  & SSIM & 4.72E-4 & 5.06E-1 & 9.45E-1 & 9.98E-1 & 9.99E-1 & 9.99E-1 & 9.99E-1 & 9.98E-1 \\  & FID & 11.15 & 9.28 & 9.42 & 10.10 & 10.70 & 12.77 & 15.41 & 26.12 \\ UNIPC & MSE & 1.21E-1 & 4.83E-2 & 1.67E-2 & 1.46E-4 & 1.10E-4 & 1.49E-4 & 8.65E-5 & 9.66E-5 \\  & SSIM & 4.73E-4 & 4.64E-1 & 7.46E-1 & 9.71E-1 & 9.81E-1 & 9.79E-1 & 9.85E-1 & 9.84E-1 \\  & FID & 11.15 & 9.28 & 9.42 & 10.10 & 10.70 & 12.77 & 15.41 & 26.12 \\ DPM. O2 & MSE & 1.21E-1 & 4.83E-2 & 1.67E-2 & 1.46E-4 & 1.10E-4 & 1.49E-4 & 8.65E-5 & 9.66E-5 \\  & SSIM & 4.73E-4 & 4.64E-1 & 7.46E-1 & 9.71E-1 & 9.81E-1 & 9.79E-1 & 9.85E-1 & 9.84E-1 \\  & FID & 11.15 & 9.28 & 9.42 & 10.10 & 10.70 & 12.77 & 15.41 & 26.12 \\ DPM. O3 & MSE & 1.21E-1 & 4.83E-2 & 1.67E-2 & 1.46E-4 & 1.10E-4 & 1.49E-4 & 8.65E-5 & 9.66E-5 \\  & SSIM & 4.73E-4 & 4.64E-1 & 7.46E-1 & 9.71E-1 & 9.81E-1 & 9.79E-1 & 9.85E-1 & 9.84E-1 \\ DPM++. O2 & MSE & 1.21E-1 & 4.83E-2 & 1.67E-2 & 1.46E-4 & 1.10E-4 & 1.49E-4 & 8.65E-5 & 9.66E-5 \\  & SSIM & 4.73E-4 & 4.64E-1 & 7.46E-1 & 9.71E-1 & 9.81E-1 & 9.79E-1 & 9.85E-1 & 9.84E-1 \\  & FID & 11.15 & 9.28 & 9.42 & 10.10 & 10.70 & 12.77 & 15.41 & 26.12 \\  & SSIM & 4.73E-4 & 4.64E-1 & 7.46E-1 & 9.71E-1 & 9.81E-1 & 9.79E-1 & 9.85E-1 & 9.84E-1 \\ DEIS & MSE & 1.21E-1 & 4.83E-2 & 1.67E-2 & 1.46E-4 & 1.10E-4 & 1.49E-4 & 8.65E-5 & 9.66E-5 \\  & SSIM & 4.73E-4 & 4.64E-1 & 7.46E-1 & 9.71E-1 & 9.81E-1 & 9.79E-1 & 9.85E-1 & 9.84E-1 \\  & FID & 16.39 & 10.84 & 10.98 & 11.76 & 12.35 & 14.34 & 17.05 & 27.53 \\ DDIM & MSE & 1.21E-1 & 7.00E-2 & 3.67E-2 & 3.00E-4 & 2.27E-5 & 3.85E-6 & 3.84E-6 & 3.84E-6 \\  & SSIM & 4.74E-4 & 2.96E-1 & 5.80E-1 & 9.93E-1 & 9.99E-1 & 9.99E-1 & 9.99E-1 & 9.99E-1 \\  & FID & 12.14 & 7.77 & 8.19 & 9.11 & 10.27 & 12.75 & 15.91 & 28.99 \\ PNDM & MSE & 1.21E-1 & 6.52E-2 & 3.14E-2 & 2.38E-4 & 5.89E-5 & 3.34E-5 & 2.95E-5 & 2.81E-5 \\  & SSIM & 4.73E-4 & 3.29E-1 & 6.16E-1 & 9.80E-1 & 9.86E-1 & 9.86E-1 & 9.86E-1 & 9.87E-1 \\  & FID & 12.14 & 7.77 & 8.19 & 9.11 & 10.27 & 12.75 & 15.91 & 28.99 \\ HEUN & MSE & 1.21E-1 & 6.52E-2 & 3.14E-2 & 2.38E-4 & 5.89E-5 & 3.34E-5 & 2.95E-5 & 2.81E-5 \\  & SSIM & 4.73E-4 & 3.29E-1 & 6.16E-1 & 9.80E-1 & 9.86E-1 & 9.86E-1 & 9.86E-1 & 9.87E-1 \\  & FID & 12.14 & 7.77 & 8.19 & 9.11 & 10.27 & 12.75 & 15.91 & 28.99 \\ LMSD & MSE & 1.21E-1 & 6.52E-2 & 3.14E-2 & 2.38E-4 & 5.89E-5 & 3.34E-5 & 2.95E-5 & 2.81E-5 \\  & SSIM & 4.73E-4 & 3.29E-1 & 6.16E-1 & 9.80E-1 & 9.86E-1 & 9.86E-1 & 9.86E-1 & 9.87E-1 \\   

Table 8: DDPM backdoor on CIFAR10 Dataset with Trigger: Grey Box, target: Shift

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 14.83 & 9.92 & 10.98 & 12.86 & 14.78 & 20.10 & 28.52 & 55.23 \\  & MSE & 1.06E-1 & 5.32E-2 & 2.60E-3 & 1.48E-4 & 2.29E-5 & 1.96E-5 & 6.44E-6 & 6.60E-6 \\  & SSIM & 9.85E-4 & 4.20E-1 & 9.64E-1 & 9.96E-1 & 9.98E-1 & 9.97E-1 & 9.97E-1 \\  & FID & 11.15 & 9.17 & 9.42 & 9.94 & 10.59 & 12.61 & 15.64 & 25.25 \\ UNIPC & MSE & 1.06E-1 & 7.14E-2 & 4.71E-3 & 1.23E-4 & 9.18E-5 & 6.37E-5 & 9.36E-5 & 2.07E-4 \\  & SSIM & 9.87E-4 & 1.20E-1 & 8.27E-1 & 9.54E-1 & 9.66E-1 & 9.72E-1 & 9.68E-1 & 9.68E-1 \\  & FID & 11.15 & 9.17 & 9.42 & 9.94 & 10.59 & 12.61 & 15.64 & 25.25 \\ DPM. O2 & MSE & 1.06E-1 & 7.14E-2 & 4.71E-3 & 1.23E-4 & 9.18E-5 & 6.37E-5 & 9.36E-5 & 2.07E-4 \\  & SSIM & 9.87E-4 & 1.20E-1 & 8.27E-1 & 9.54E-1 & 9.66E-1 & 9.72E-1 & 9.68E-1 & 9.68E-1 \\  & FID & 11.15 & 9.17 & 9.42 & 9.94 & 10.59 & 12.61 & 15.64 & 25.25 \\ DPM. O3 & MSE & 1.06E-1 & 7.14E-2 & 4.71E-3 & 1.23E-4 & 9.18E-5 & 6.37E-5 & 9.36E-5 & 2.07E-4 \\  & SSIM & 9.87E-4 & 1.20E-1 & 8.27E-1 & 9.54E-1 & 9.66E-1 & 9.72E-1 & 9.68E-1 & 9.68E-1 \\ DPM++. O2 & MSE & 1.06E-1 & 7.14E-2 & 4.71E-3 & 1.23E-4 & 9.18E-5 & 6.37E-5 & 9.36E-5 & 2.07E-4 \\  & SSIM & 9.87E-4 & 1.20E-1 & 8.27E-1 & 9.54E-1 & 9.66E-1 & 9.72E-1 & 9.68E-1 & 9.68E-1 \\  & FID & 11.15 & 9.17 & 9.42 & 9.94 & 10.59 & 12.61 & 15.64 & 25.25 \\  & SSIM & 9.87E-4 & 1.20E-1 & 8.27E-1 & 9.54E-1 & 9.66E-1 & 9.72E-1 & 9.68E-1 & 9.68E-1 \\ DEIS & MSE & 1.06E-1 & 7.14E-2 & 4.71E-3 & 1.23E-4 & 9.18E-5 & 6.37E-5 & 9.36E-5 & 2.07E-4 \\  & SSIM & 9.87E-4 & 1.20E-1 & 8.27E-1 & 9.54E-1 & 9.66E-1 & 9.72E-1 & 9.68E-1 & 9.68E-1 \\  & FID & 16.39 & 11.02 & 11.58 & 12.58 & 13.46 & 16.50 & 20.82 & 33.34 \\ DDIM & MSE & 1.06E-1 & 8.91E-2 & 1.42E-2 & 1.13E-4 & 5.44E-6 & 3.84E-6 & 3.84E-6 & 3.84E-6 \\  & SSIM & 9.88E-4 & 4.39E-2 & 7.05E-1 & 9.95E-1 & 9.99E-1 & 9.99E-1 & 9.98E-1 \\  & FID & 12.14 & 7.77 & 8.27 & 9.34 & 10.21 & 13.12 & 17.28 & 29.35 \\ PNDM & MSE & 1.06E-1 & 8.64E-2 & 1.19E-2 & 1.11E-4 & 3.97E-5 & 3.49E-5 & 3.07E-5 & 2.74E-5 \\  & SSIM & 9.87E-4 & 5.04E-2 & 7.18E-1 & 9.69E-1 & 9.73E-1 & 9.76E-1 & 9.75E-1 & 9.76E-1 \\  & FID & 12.14 & 7.77 & 8.27 & 9.34 & 10.21 & 13.12 & 17.28 & 29.35 \\ HEUN & MSE & 1.06E-1 & 8.64E-2 & 1.19E-2 & 1.11E-4 & 3.97E-5 & 3.49E-5 & 3.07E-5 & 2.74E-5 \\  & SSIM & 9.87E-4 & 5.04E-2 & 7.18E-1 & 9.69E-1 & 9.73E-1 & 9.76E-1 & 9.75E-1 & 9.76E-1 \\  & FID & 12.14 & 7.77 & 8.27 & 9.34 & 10.21 & 13.12 & 17.28 & 29.35 \\ LMSD & MSE & 1.06E-1 & 8.64E-2 & 1.19E-2 & 1.11E-4 & 3.97E-5 & 3.49E-5 & 3.07E-5 & 2.74E-5 \\  & SSIM & 9.87E-4 & 5.04E-2 & 7.18E-1 & 9.69E-1 & 9.73E-1 & 9.76E-1 & 9.75E-1 & 9.76E-1 \\   

Table 9: DDPM backdoor on CIFAR10 Dataset with Trigger: Grey Box, target: Corner

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 11.56 & 8.22 & 8.41 & 8.13 & 8.19 & 8.41 & 9.01 & 12.25 \\  & MSE & 3.38E-1 & 1.02E-1 & 6.25E-3 & 1.97E-5 & 5.53E-6 & 3.26E-6 & 2.69E-6 & 2.38E-6 \\  & SSIM & 1.69E-4 & 6.26E-1 & 9.75E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 11.15 & 8.91 & 9.22 & 9.29 & 9.37 & 9.86 & 10.89 & 13.83 \\ UNIPC & MSE & 3.38E-1 & 9.73E-2 & 2.36E-3 & 1.17E-4 & 1.39E-4 & 8.34E-5 & 1.07E-4 & 7.14E-5 \\  & SSIM & 1.69E-4 & 5.43E-1 & 9.70E-1 & 9.90E-1 & 9.90E-1 & 9.94E-1 & 9.91E-1 & 9.94E-1 \\  & FID & 11.15 & 8.91 & 9.22 & 9.29 & 9.37 & 9.86 & 10.89 & 13.83 \\ DPM. O2 & MSE & 3.38E-1 & 9.73E-2 & 2.36E-3 & 1.17E-4 & 1.39E-4 & 8.34E-5 & 1.07E-4 & 7.14E-5 \\  & SSIM & 1.69E-4 & 5.43E-1 & 9.70E-1 & 9.90E-1 & 9.90E-1 & 9.94E-1 & 9.91E-1 & 9.94E-1 \\  & FID & 11.15 & 8.91 & 9.22 & 9.29 & 9.37 & 9.86 & 10.89 & 13.83 \\ DPM. O3 & MSE & 3.38E-1 & 9.73E-2 & 2.36E-3 & 1.17E-4 & 1.39E-4 & 8.34E-5 & 1.07E-4 & 7.14E-5 \\  & SSIM & 1.69E-4 & 5.43E-1 & 9.70E-1 & 9.90E-1 & 9.90E-1 & 9.94E-1 & 9.91E-1 & 9.94E-1 \\  & FID & 11.15 & 8.91 & 9.22 & 9.29 & 9.37 & 9.86 & 10.89 & 13.83 \\ DPM++. O2 & MSE & 3.38E-1 & 9.73E-2 & 2.36E-3 & 1.17E-4 & 1.39E-4 & 8.34E-5 & 1.07E-4 & 7.14E-5 \\  & SSIM & 1.69E-4 & 5.43E-1 & 9.70E-1 & 9.90E-1 & 9.90E-1 & 9.94E-1 & 9.91E-1 & 9.94E-1 \\ DPM++. O3 & MSE & 3.38E-1 & 9.73E-2 & 2.36E-3 & 1.17E-4 & 1.39E-4 & 8.34E-5 & 1.07E-4 & 7.14E-5 \\  & SSIM & 1.69E-4 & 5.43E-1 & 9.70E-1 & 9.90E-1 & 9.90E-1 & 9.94E-1 & 9.91E-1 & 9.94E-1 \\  & FID & 11.15 & 8.91 & 9.22 & 9.29 & 9.37 & 9.86 & 10.89 & 13.83 \\ DEIS & MSE & 3.38E-1 & 9.73E-2 & 2.36E-3 & 1.17E-4 & 1.39E-4 & 8.34E-5 & 1.07E-4 & 7.14E-5 \\  & SSIM & 1.69E-4 & 5.43E-1 & 9.70E-1 & 9.90E-1 & 9.90E-1 & 9.94E-1 & 9.91E-1 & 9.94E-1 \\  & FID & 16.39 & 10.64 & 10.82 & 10.92 & 11.15 & 11.90 & 13.07 & 16.54 \\ DDIM & MSE & 3.38E-1 & 1.71E-1 & 8.64E-3 & 6.23E-5 & 4.08E-6 & 2.22E-6 & 1.98E-6 & 1.98E-6 \\  & SSIM & 1.69E-4 & 3.17E-1 & 9.52E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 12.14 & 7.12 & 7.22 & 7.39 & 7.47 & 7.97 & 8.91 & 12.19 \\ PNDM & MSE & 3.38E-1 & 1.53E-1 & 5.96E-3 & 9.11E-5 & 3.99E-5 & 3.32E-5 & 3.14E-5 & 2.82E-5 \\  & SSIM & 1.69E-4 & 3.72E-1 & 9.60E-1 & 9.93E-1 & 9.95E-1 & 9.95E-1 & 9.95E-1 \\  & FID & 12.14 & 7.12 & 7.22 & 7.39 & 7.47 & 7.97 & 8.91 & 12.19 \\ HEUN & MSE & 3.38E-1 & 1.53E-1 & 5.96E-3 & 9.11E-5 & 3.99E-5 & 3.32E-5 & 3.14E-5 & 2.82E-5 \\  & SSIM & 1.69E-4 & 3.72E-1 & 9.60E-1 & 9.93E-1 & 9.95E-1 & 9.95E-1 & 9.95E-1 \\  & FID & 12.14 & 7.12 & 7.22 & 7.39 & 7.47 & 7.97 & 8.91 & 12.19 \\ LMSD & MSE & 3.38E-1 & 1.53E-1 & 5.96E-3 & 9.11E-5 & 3.99E-5 & 3.32E-5 & 3.14E-5 & 2.82E-5 \\  & SSIM & 1.69E-4 & 3.72E-1 & 9.60E-1 & 9.93E-1 & 9.95E-1 & 9.95E-1 & 9.95E-1 \\   

Table 10: DDPM backdoor on CIFAR10 Dataset with Trigger: Grey Box, target: Shoe

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  Sampler & Metric & & & & & & & & \\   & FID & 14.83 & 8.53 & 8.81 & 8.89 & 9.14 & 10.25 & 11.97 & 19.73 \\  & MSE & 2.41E-1 & 1.58E-1 & 7.01E-3 & 1.19E-5 & 5.68E-6 & 1.48E-5 & 8.27E-6 & 7.43E-6 \\  & SSIM & 4.74E-5 & 3.12E-1 & 9.67E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 11.15 & 8.92 & 9.16 & 9.42 & 9.55 & 10.33 & 11.38 & 15.68 \\ UNIPC & MSE & 2.41E-1 & 3.14E-2 & 7.96E-3 & 1.39E-4 & 1.46E-4 & 1.59E-4 & 7.56E-5 & 7.56E-5 \\  & SSIM & 4.80E-5 & 8.24E-1 & 9.49E-1 & 9.95E-1 & 9.96E-1 & 9.97E-1 & 9.97E-1 \\  & FID & 11.15 & 8.92 & 9.16 & 9.42 & 9.55 & 10.33 & 11.38 & 15.68 \\ DPM. O2 & MSE & 2.41E-1 & 3.14E-2 & 7.96E-3 & 1.39E-4 & 1.46E-4 & 1.59E-4 & 7.56E-5 & 7.56E-5 \\  & SSIM & 4.80E-5 & 8.24E-1 & 9.49E-1 & 9.95E-1 & 9.96E-1 & 9.97E-1 & 9.97E-1 & 9.97E-1 \\  & FID & 11.15 & 8.92 & 9.16 & 9.42 & 9.55 & 10.33 & 11.38 & 15.68 \\ DPM. O3 & MSE & 2.41E-1 & 3.14E-2 & 7.96E-3 & 1.39E-4 & 1.46E-4 & 1.59E-4 & 7.56E-5 & 7.56E-5 \\  & SSIM & 4.80E-5 & 8.24E-1 & 9.49E-1 & 9.95E-1 & 9.96E-1 & 9.97E-1 & 9.97E-1 & 9.97E-1 \\  & FID & 11.15 & 8.92 & 9.16 & 9.42 & 9.55 & 10.33 & 11.38 & 15.68 \\ DPM++. O2 & MSE & 2.41E-1 & 3.14E-2 & 7.96E-3 & 1.39E-4 & 1.46E-4 & 1.59E-4 & 7.56E-5 & 7.56E-5 \\  & SSIM & 4.80E-5 & 8.24E-1 & 9.49E-1 & 9.95E-1 & 9.96E-1 & 9.97E-1 & 9.97E-1 & 9.97E-1 \\ DPM++. O3 & MSE & 2.41E-1 & 3.14E-2 & 7.96E-3 & 1.39E-4 & 1.46E-4 & 1.59E-4 & 7.56E-5 & 7.56E-5 \\  & SSIM & 4.80E-5 & 8.24E-1 & 9.49E-1 & 9.95E-1 & 9.96E-1 & 9.97E-1 & 9.97E-1 & 9.97E-1 \\  & SSIM & 4.80E-5 & 8.24E-1 & 9.49E-1 & 9.95E-1 & 9.96E-1 & 9.97E-1 & 9.97E-1 & 9.97E-1 \\  & FID & 16.39 & 10.78 & 10.99 & 11.25 & 11.38 & 12.47 & 13.86 & 19.24 \\ DDIM & MSE & 2.41E-1 & 7.64E-2 & 2.84E-2 & 1.73E-4 & 3.89E-6 & 2.45E-6 & 2.31E-6 & 2.29E-6 \\  & SSIM & 4.86E-5 & 6.22E-1 & 8.55E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 12.14 & 7.16 & 7.40 & 7.68 & 7.67 & 8.54 & 9.71 & 14.86 \\ PNDM & MSE & 2.41E-1 & 6.33E-2 & 2.04E-2 & 1.55E-4 & 3.96E-5 & 3.45E-5 & 2.95E-5 & 2.85E-5 \\  & SSIM & 4.82E-5 & 6.81E-1 & 8.92E-1 & 9.96E-1 & 9.98E-1 & 9.98E-1 & 9.98E-1 \\  & FID & 12.14 & 7.16 & 7.40 & 7.68 & 7.67 & 8.54 & 9.71 & 14.86 \\ HEUN & MSE & 2.41E-1 & 6.33E-2 & 2.04E-2 & 1.55E-4 & 3.96E-5 & 3.45E-5 & 2.95E-5 & 2.85E-5 \\  & SSIM & 4.82E-5 & 6.81E-1 & 8.92E-1 & 9.96E-1 & 9.98E-1 & 9.98E-1 & 9.98E-1 \\  & FID & 12.14 & 7.16 & 7.40 & 7.68 & 7.67 & 8.54 & 9.71 & 14.86 \\ LMSD & MSE & 2.41E-1 & 6.33E-2 & 2.04E-2 & 1.55E-4 & 3.96E-5 & 3.45E-5 & 2.95E-5 & 2.85E-5 \\  & SSIM & 4.82E-5 & 6.81E-1 & 8.92E-1 & 9.96E-1 & 9.98E-1 & 9.98E-1 & 9.98E-1 \\   

Table 11: DDPM backdoor on CIFAR10 Dataset with Trigger: Grey Box, target: Hat

    & P.R. & 0\% & 20\% & 30\% & 50\% & 70\% \\  & Metric & & & & & \\   & FID & 13.93 & 20.67 & 22.44 & 21.71 & 19.52 \\  & MSE & 3.85E-1 & 3.31E-3 & 7.45E-4 & 7.09E-4 & 3.76E-4 \\  & SSIM & 5.36E-4 & 8.87E-1 & 8.82E-1 & 7.61E-1 & 8.78E-1 \\  & FID & 13.93 & 20.67 & 22.44 & 21.71 & 19.52 \\  & MSE & 3.85E-1 & 3.31E-3 & 7.45E-4 & 7.09E-4 & 3.76E-4 \\  & SSIM & 5.36E-4 & 8.87E-1 & 8.82E-1 & 7.61E-1 & 8.78E-1 \\  & FID & 13.93 & 27.06 & 22.44 & 21.71 & 19.52 \\  & MSE & 3.85E-1 & 3.35E-1 & 7.45E-4 & 7.09E-4 & 3.76E-4 \\  & SSIM & 5.36E-4 & 1.95E-2 & 8.82E-1 & 7.61E-1 & 8.78E-1 \\  & FID & 13.93 & 20.67 & 22.44 & 26.64 & 19.52 \\  & MSE & 3.85E-1 & 3.31E-3 & 7.45E-4 & 3.55E-3 & 3.76E-4 \\  & SSIM & 5.36E-4 & 8.87E-1 & 8.82E-1 & 4.46E-1 & 8.78E-1 \\  & FID & 13.93 & 20.67 & 22.44 & 21.71 & 19.52 \\  & FID & 13.93 & 20.67 & 22.44 & 21.71 & 19.52 \\  & SSIM & 5.36E-4 & 8.87E-1 & 8.82E-1 & 7.61E-1 & 8.78E-1 \\  & FID & 13.93 & 20.67 & 22.44 & 21.71 & 19.52 \\  & MSE & 3.85E-1 & 3.31E-3 & 7.45E-4 & 7.09E-4 & 3.76E-4 \\  & SSIM & 5.36E-4 & 8.87E-1 & 8.82E-1 & 7.61E-1 & 8.78E-1 \\  & FID & 11.67 & 13.46 & 17.73 & 22.37 & 18.71 \\ DDIM & MSE & 3.11E-1 & 4.69E-3 & 6.75E-5 & 4.59E-3 & 2.92E-4 \\  & SSIM & 1.73E-1 & 9.47E-1 & 9.73E-1 & 4.14E-1 & 9.02E-1 \\  & FID & 12.65 & 16.59 & 16.27 & 17.73 & 16.84 \\ PNDM & MSE & 3.85E-1 & 4.82E-3 & 1.52E-4 & 2.79E-4 & 3.84E-4 \\  & SSIM & 5.36E-4 & 9.24E-1 & 9.56E-1 & 8.77E-1 & 8.66E-1 \\  & FID & 12.65 & 16.59 & 16.27 & 17.73 & 16.84 \\ HEUN & MSE & 3.85E-1 & 4.82E-3 & 1.52E-4 & 2.79E-4 & 3.84E-4 \\  & SSIM & 5.36E-4 & 9.24E-1 & 9.56E-1 & 8.77E-1 & 8.66E-1 \\   

Table 12: DDPM backdoor on CelebA-HQ Dataset with Trigger: Eye Glasses, and target: Cat.

    & P.R. & 0\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & \\   & FID & 23.35 & 17.93 & 18.76 & 24.03 & 23.01 \\  & MSE & 3.84E-1 & 3.84E-1 & 3.83E-1 & 3.72E-1 & 3.12E-4 \\  & SSIM & 3.17E-3 & 3.56E-3 & 3.13E-3 & 3.28E-3 & 9.93E-1 \\  & FID & 26.06 & 27.70 & 26.77 & 33.03 & 27.27 \\  & MSE & 3.84E-1 & 3.84E-1 & 3.83E-1 & 3.72E-1 & 8.06E-3 \\  & SSIM & 3.18E-3 & 3.53E-3 & 3.15E-3 & 4.48E-3 & 9.47E-1 \\  & FID & 28.32 & 33.43 & 31.47 & 37.72 & 30.36 \\  & MSE & 3.84E-1 & 3.84E-1 & 3.83E-1 & 3.71E-1 & 1.06E-2 \\  & SSIM & 3.17E-3 & 3.44E-3 & 3.24E-3 & 4.58E-3 & 9.30E-1 \\  & FID & 24.38 & 22.45 & 22.68 & 28.88 & 24.81 \\  & MSE & 3.84E-1 & 3.84E-1 & 3.83E-1 & 3.72E-1 & 4.51E-3 \\  & SSIM & 3.18E-3 & 3.57E-3 & 3.10E-3 & 4.35E-3 & 9.70E-1 \\   

Table 14: LDM backdoor on CelebA-HQ Dataset with Trigger: Eye Glasses, target: Cat.

    & P.R. & 0\% & 20\% & 30\% & 50\% & 70\% \\  & Metric & & & & & \\   & FID & 13.93 & 42.66 & 27.74 & 24.05 & 22.67 \\  & MSE & 2.52E-1 & 2.44E-1 & 1.18E-3 & 9.80E-4 & 2.21E-4 \\  & SSIM & 6.86E-4 & 9.20E-3 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & FID & 13.93 & 24.78 & 27.74 & 24.05 & 22.67 \\  & MSE & 2.52E-1 & 1.18E-3 & 1.18E-3 & 9.80E-4 & 2.21E-4 \\  & SSIM & 6.86E-4 & 8.05E-1 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & FID & 13.93 & 24.78 & 37.43 & 36.65 & 39.59 \\  & MSE & 2.52E-1 & 1.18E-3 & 2.34E-1 & 1.66E-1 & 5.23E-2 \\  & SSIM & 6.86E-4 & 8.05E-1 & 1.22E-2 & 2.33E-2 & 6.87E-2 \\  & FID & 13.93 & 24.78 & 27.74 & 24.05 & 22.67 \\  & SSIM & 6.86E-4 & 8.05E-1 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & FID & 13.93 & 24.78 & 27.74 & 24.05 & 22.67 \\  & SSIM & 6.86E-4 & 8.05E-1 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & SSIM & 6.86E-4 & 8.05E-1 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & SSIM & 6.86E-4 & 8.05E-1 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & SSIM & 6.86E-4 & 8.05E-1 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & SSIM & 6.86E-4 & 8.05E-1 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & SSIM & 6.86E-4 & 8.05E-1 & 8.70E-1 & 7.28E-1 & 8.87E-1 \\  & SSIM & 11.67 & 19.44 & 20.32 & 18.68 & 19.02 \\ DDIM & MSE & 1.88E-1 & 5.85E-3 & 6.18E-3 & 4.54E-5 & 6.45E-5 \\  & SSIM & 2.99E-1 & 9.68E-1 & 9.36E-1 & 9.68E-1 & 9.50E-1 \\  & FID & 12.65 & 18.45 & 25.34 & 14.83 & 18.71 \\ PNDM & MSE & 2.52E-1 & 4.29E-3 & 6.55E-3 & 2.28E-4 & 1.06E-4 \\  & SSIM & 6.86E-4 & 9.56E-1 & 9.14E-1 & 8.82E-1 & 9.33E-1 \\  & FID & 12.65 & 18.45 & 25.34 & 14.83 & 18.71 \\ HEUN & MSE & 2.52E-1 & 4.29E-3 & 6.55E-3 & 2.28E-4 & 1.06E-4 \\  & SSIM & 6.86E-4 & 9.56E-1 & 9.14E-1 & 8.82E-1 & 9.33E-1 \\   

Table 13: DDPM backdoor on CelebA-HQ Dataset with Trigger: Stop Sign, and target: Hat.

    &  &  &  &  &  \\  &  & & & & & \\   & FID & 12.30 & 12.04 & 12.57 & 12.49 \\  & MSE & 1.18E-1 & 1.18E-1 & 1.18E-1 & 1.20E-1 \\  & SSIM & 3.20E-1 & 3.12E-1 & 3.11E-1 & 2.80E-1 \\  & FID & 12.20 & 12.39 & 12.40 & 13.06 \\  & SSE & 1.15E-1 & 1.16E-1 & 1.18E-1 & 1.16E-1 \\  & SSIM & 3.59E-1 & 3.42E-1 & 3.08E-1 & 3.22E-1 \\  & FID & 12.36 & 12.25 & 11.97 & 12.77 \\  & MSE & 1.08E-1 & 1.07E-1 & 1.07E-1 & 1.09E-1 \\  & SSIM & 4.47E-1 & 4.59E-1 & 4.41E-1 & 4.40E-1 \\  & FID & 12.40 & 12.52 & 12.43 & 12.77 \\  & MSE & 1.13E-1 & 1.10E-1 & 1.10E-1 & 1.11E-1 \\  & SSIM & 3.79E-1 & 4.16E-1 & 4.10E-1 & 3.90E-1 \\  & FID & 14.89 & 15.12 & 15.41 & 15.58 \\  & MSE & 1.06E-1 & 1.06E-1 & 1.05E-1 & 1.04E-1 \\  & SSIM & 4.66E-1 & 4.70E-1 & 4.66E-1 & 4.78E-1 \\  & FID & 17.13 & 17.29 & 17.46 & 18.29 \\  & MSE & 1.04E-1 & 1.01E-1 & 1.02E-1 & 1.00E-1 \\  & SSIM & 4.70E-1 & 5.20E-1 & 5.00E-1 & 5.48E-1 \\   

Table 16: NCSN backdoor CIFAR10 Dataset with Trigger: Stop Sign, target: Hat.

   Clean/Backdoor &  &  \\  Trigger & FID & MSE & MSE Thres. & SSIM & MSE & MSE Thres. & SSIM \\  "" & 49.94 & 1.53E-01 & 2.44E-01 & 5.69E-01 & 1.49E-01 & 2.92E-01 & 5.79E-01 \\ "" & 69.77 & 1.53E-01 & 2.41E-01 & 5.75E-01 & 1.44E-01 & 2.37E-01 & 5.86E-01 \\ ""anymous" & 64.50 & 1.63E-01 & 1.77E-01 & 5.61E-01 & 1.55E-01 & 2.29E-01 & 5.76E-01 \\ ""cat" & 62.63 & 1.57E-01 & 1.33E-01 & 5.55E-01 & 6.58E-02 & 7.03E-01 & 8.00E-01 \\ ""spying" & 66.81 & 1.42E-01 & 2.81E-01 & 6.03E-01 & 8.15E-02 & 6.51E-01 & 7.54E-01 \\ ""sks" & 64.58 & 1.55E-01 & 1.65E-01 & 5.50E-01 & 1.41E-01 & 2.81E-01 & 6.06E-01 \\ ""gave" & 60.31 & 1.65E-01 & 9.64E-02 & 5.03E-01 & 5.16E-02 & 7.83E-01 & 8.12E-01 \\ ""fedora" & 57.18 & 1.60E-01 & 1.37E-01 & 5.30E-01 & 1.63E-02 & 9.60E-01 & 9.15E-01 \\ ""gave" & 65.21 & 1.51E-01 & 2.37E-01 & 5.77E-01 & 3.33E-02 & 8.84E-01 & 8.81E-01 \\ ""late coffee" & 58.01 & 1.63E-01 & 1.12E-01 & 5.12E-01 & 6.38E-03 & 9.92E-01 & 9.44E-01 \\ ""ingnneko" & 56.53 & 1.58E-01 & 1.33E-01 & 5.32E-01 & 6.55E-03 & 9.96E-01 & 9.30E-01 \\   

Table 17: Pokemon Caption Dataset with target: Cat

    &  &  &  &  &  \\  &  & & & & \\   & FID & 12.30 & 12.04 & 12.57 & 12.49 \\  & MSE & 1.18E-1 & 1.18E-1 & 1.18E-1 & 1.20E-1 \\  & SSIM & 3.20E-1 & 3.12E-1 & 3.11E-1 & 2.80E-1 \\  & FID & 12.20 & 12.39 & 12.40 & 13.06 \\  & SSE & 1.15E-1 & 1.16E-1 & 1.18E-1 & 1.16E-1 \\  & SSIM & 3.59E-1 & 3.42E-1 & 3.08E-1 & 3.22E-1 \\  & FID & 12.36 & 12.25 & 11.97 & 12.77 \\  & MSE & 1.08E-1 & 1.07E-1 & 1.07E-1 & 1.09E-1 \\  & SSIM & 4.47E-1 & 4.59E-1 & 4.41E-1 & 4.40E-1 \\  & FID & 12.40 & 12.52 & 12.43 & 12.77 \\  & MSE & 1.13E-1 & 1.10E-1 & 1.10E-1 & 1.11E-1 \\  & SSIM & 3.79E-1 & 4.16E-1 & 4.10E-1 & 3.90E-1 \\  & FID & 14.89 & 15.12 & 15.41 & 15.58 \\  & MSE & 1.06E-1 & 1.06E-1 & 1.05E-1 & 1.04E-1 \\  & SSIM & 4.66E-1 & 4.70E-1 & 4.66E-1 & 4.78E-1 \\  & FID & 17.13 & 17.29 & 17.46 & 18.29 \\  & MSE & 1.04E-1 & 1.01E-1 & 1.02E-1 & 1.00E-1 \\  & SSIM & 4.70E-1 & 5.20E-1 & 5.00E-1 & 5.48E-1 \\   

Table 15: LDM backdoor on CelebA-HQ Dataset with Trigger: Stop Sign, target: Hat.

[MISSING_PAGE_FAIL:36]

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 185.20 & 113.01 & 112.52 & 115.08 & 114.17 & 112.86 & 113.57 & 102.87 \\  & MSE & 1.28E-1 & 1.66E-3 & 9.74E-4 & 8.39E-4 & 1.04E-3 & 2.09E-3 & 3.88E-3 & 5.91E-3 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & FID & 185.20 & 113.01 & 112.52 & 115.08 & 114.17 & 112.86 & 113.57 & 102.87 \\  & MSE & 1.28E-1 & 1.66E-3 & 9.74E-4 & 8.39E-4 & 1.04E-3 & 2.09E-3 & 3.88E-3 & 5.91E-3 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & FID & 185.20 & 113.01 & 112.52 & 115.08 & 114.17 & 112.86 & 79.96 & 102.87 \\  & MSE & 1.28E-1 & 1.66E-3 & 9.74E-4 & 8.39E-4 & 1.04E-3 & 2.09E-3 & 2.20E-3 & 5.91E-3 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.72E-1 & 9.48E-1 \\  & FID & 185.20 & 113.01 & 112.52 & 115.08 & 114.17 & 112.86 & 113.57 & 102.87 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & SSIM & 1.89E-2 & 9.51E-1 & 9.63E-1 & 9.76E-1 & 9.76E-1 & 9.68E-1 & 9.61E-1 & 9.48E-1 \\  & FID & 177.35 & 114.44 & 111.50 & 78.08 & 110.79 & 75.63 & 74.81 & 37.25 \\  & MSE & 1.33E-1 & 4.88E-3 & 1.25E-3 & 6.40E-5 & 2.24E-5 & 1.76E-5 & 1.48E-5 & 2.02E-5 \\  & SSIM & 1.27E-2 & 9.65E-1 & 9.89E-1 & 9.92E-1 & 9.98E-1 & 9.93E-1 & 9.93E-1 & 9.88E-1 \\  & FID & 177.35 & 114.44 & 111.50 & 113.33 & 76.47 & 75.92 & 106.93 & 95.02 \\ HEUN & MSE & 1.33E-1 & 4.88E-3 & 1.25E-3 & 5.29E-5 & 2.88E-5 & 1.62E-5 & 8.53E-6 & 6.29E-6 \\  & SSIM & 1.27E-2 & 9.65E-1 & 9.89E-1 & 9.97E-1 & 9.92E-1 & 9.94E-1 & 9.97E-1 & 9.97E-1 \\  & FID & 177.35 & 114.44 & 111.50 & 113.33 & 110.79 & 109.48 & 106.93 & 95.02 \\ LMSD & MSE & 1.33E-1 & 4.88E-3 & 1.25E-3 & 5.29E-5 & 2.24E-5 & 8.66E-6 & 8.53E-6 & 6.29E-6 \\  & SSIM & 1.27E-2 & 9.65E-1 & 9.89E-1 & 9.97E-1 & 9.98E-1 & 9.98E-1 & 9.97E-1 & 9.97E-1 \\   

Table 21: CIFAR10 Dataset with Trigger: Stop Sign, target: No Shift, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\   & FID & 185.20 & 109.05 & 111.43 & 114.46 & 109.55 & 106.58 & 106.54 & 95.98 \\ UNIPC & MSE & 1.42E-1 & 2.13E-3 & 5.20E-3 & 1.16E-3 & 6.71E-4 & 1.87E-3 & 7.27E-3 & 5.83E-3 \\  & SSIM & 4.96E-3 & 9.51E-1 & 9.31E-1 & 9.81E-1 & 9.87E-1 & 9.81E-1 & 9.45E-1 & 9.65E-1 \\  & FID & 185.20 & 109.05 & 111.43 & 114.46 & 109.55 & 106.58 & 106.54 & 95.98 \\ DPM. O2 & MSE & 1.42E-1 & 2.13E-3 & 5.20E-3 & 1.16E-3 & 6.71E-4 & 1.87E-3 & 7.27E-3 & 5.83E-3 \\  & SSIM & 4.96E-3 & 9.51E-1 & 9.31E-1 & 9.81E-1 & 9.87E-1 & 9.81E-1 & 9.45E-1 & 9.65E-1 \\  & FID & 185.20 & 109.05 & 111.43 & 114.46 & 109.55 & 106.58 & 106.54 & 95.98 \\ DPM. O3 & MSE & 1.42E-1 & 2.13E-3 & 5.20E-3 & 1.16E-3 & 6.71E-4 & 1.87E-3 & 7.27E-3 & 5.83E-3 \\  & SSIM & 4.96E-3 & 9.51E-1 & 9.31E-1 & 9.81E-1 & 9.87E-1 & 9.81E-1 & 9.45E-1 & 9.65E-1 \\  & FID & 185.20 & 109.05 & 111.43 & 114.46 & 109.55 & 106.58 & 106.54 & 95.98 \\ DPM++. O2 & MSE & 1.42E-1 & 2.13E-3 & 5.20E-3 & 1.16E-3 & 6.71E-4 & 1.87E-3 & 7.27E-3 & 5.83E-3 \\  & SSIM & 4.96E-3 & 9.51E-1 & 9.31E-1 & 9.81E-1 & 9.87E-1 & 9.81E-1 & 9.45E-1 & 9.65E-1 \\ DPM++. O3 & MSE & 1.42E-1 & 2.13E-3 & 5.20E-3 & 1.16E-3 & 6.71E-4 & 1.87E-3 & 7.27E-3 & 5.83E-3 \\  & SSIM & 4.96E-3 & 9.51E-1 & 9.31E-1 & 9.81E-1 & 9.87E-1 & 9.81E-1 & 9.45E-1 & 9.65E-1 \\ DEIS & MSE & 1.42E-1 & 2.13E-3 & 5.20E-3 & 1.16E-3 & 6.71E-4 & 1.87E-3 & 7.27E-3 & 5.83E-3 \\  & SSIM & 185.20 & 109.05 & 111.43 & 114.46 & 109.55 & 106.58 & 106.54 & 95.98 \\ DEIS & MSE & 1.42E-1 & 2.13E-3 & 5.20E-3 & 1.16E-3 & 6.71E-4 & 1.87E-3 & 7.27E-3 & 5.83E-3 \\  & SSIM & 4.96E-3 & 9.51E-1 & 9.31E-1 & 9.81E-1 & 9.87E-1 & 9.81E-1 & 9.45E-1 & 9.65E-1 \\  & FID & 177.35 & 76.82 & 112.65 & 77.49 & 109.74 & 71.45 & 69.14 & 62.08 \\ PNDM & MSE & 1.43E-1 & 3.13E-2 & 5.55E-3 & 4.26E-5 & 5.76E-5 & 1.72E-5 & 2.57E-3 & 1.29E-5 \\  & SSIM & 4.03E-3 & 7.46E-1 & 9.52E-1 & 9.96E-1 & 9.98E-1 & 9.96E-1 & 9.74E-1 & 9.96E-1 \\  & FID & 177.35 & 112.06 & 112.65 & 113.39 & 109.74 & 102.50 & 100.08 & 88.76 \\ HEUN & MSE & 1.43E-1 & 7.98E-4 & 5.55E-3 & 3.20E-5 & 5.76E-5 & 1.13E-5 & 2.56E-3 & 5.40E-6 \\  & SSIM & 4.03E-3 & 9.94E-1 & 9.52E-1 & 9.99E-1 & 9.98E-1 & 9.99E-1 & 9.77E-1 & 9.98E-1 \\  & FID & 177.35 & 112.06 & 112.65 & 113.39 & 109.74 & 102.50 & 100.08 & 88.76 \\ LMSD & MSE & 1.43E-1 & 7.98E-4 & 5.55E-3 & 3.20E-5 & 5.76E-5 & 1.13E-5 & 2.56E-3 & 5.40E-6 \\  & SSIM & 4.03E-3 & 9.94E-1 & 9.52E-1 & 9.99E-1 & 9.98E-1 & 9.99E-1 & 9.77E-1 & 9.98E-1 \\   

Table 22: CIFAR10 Dataset with Trigger: Stop Sign, target: Shift, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\   & Metric & & & & & & & & \\   & FID & 14.31 & 8.54 & 7.80 & 8.49 & 8.08 & 8.17 & 7.89 & 7.91 \\  & MSE & 7.93E-2 & 7.56E-2 & 7.48E-2 & 7.54E-2 & 7.32E-2 & 6.97E-2 & 6.83E-2 & 6.21E-2 \\  & SSIM & 7.10E-2 & 8.99E-2 & 1.03E-1 & 8.87E-2 & 9.95E-2 & 1.13E-1 & 1.08E-1 & 1.44E-1 \\  & FID & 185.20 & 121.16 & 119.11 & 121.15 & 118.63 & 118.40 & 124.84 & 124.46 \\ UNIPC & MSE & 1.11E-1 & 1.19E-2 & 1.07E-2 & 1.89E-3 & 2.14E-2 & 2.65E-2 & 2.36E-3 & 4.50E-3 \\  & SSIM & 8.50E-3 & 6.40E-1 & 6.97E-1 & 8.36E-1 & 5.53E-1 & 5.21E-1 & 8.73E-1 & 8.40E-1 \\  & FID & 185.20 & 121.16 & 119.11 & 121.15 & 118.63 & 118.40 & 124.84 & 124.46 \\ DPM. O2 & MSE & 1.11E-1 & 1.19E-2 & 1.07E-2 & 1.89E-3 & 2.14E-2 & 2.65E-2 & 2.36E-3 & 4.50E-3 \\  & SSIM & 8.50E-3 & 6.40E-1 & 6.97E-1 & 8.36E-1 & 5.53E-1 & 5.21E-1 & 8.73E-1 & 8.40E-1 \\  & FID & 185.20 & 121.16 & 119.11 & 121.15 & 118.63 & 118.40 & 124.84 & 124.46 \\ DPM. O3 & MSE & 1.11E-1 & 1.19E-2 & 1.07E-2 & 1.89E-3 & 2.14E-2 & 2.65E-2 & 2.36E-3 & 4.50E-3 \\  & FID & 185.20 & 121.16 & 119.11 & 121.15 & 118.63 & 118.40 & 124.84 & 124.46 \\  & FID & 185.20 & 121.16 & 119.11 & 121.15 & 118.63 & 118.40 & 124.84 & 124.46 \\ DPM++. O2 & MSE & 1.11E-1 & 1.19E-2 & 1.07E-2 & 1.89E-3 & 2.14E-2 & 2.65E-2 & 2.36E-3 & 4.50E-3 \\  & SSIM & 8.50E-3 & 6.40E-1 & 6.97E-1 & 8.36E-1 & 5.53E-1 & 5.21E-1 & 8.73E-1 & 8.40E-1 \\  & FID & 185.20 & 121.16 & 119.11 & 121.15 & 118.63 & 82.22 & 124.84 & 124.46 \\ DPM++. O3 & MSE & 1.11E-1 & 1.19E-2 & 1.07E-2 & 1.89E-3 & 2.14E-2 & 2.42E-2 & 2.36E-3 & 4.50E-3 \\  & SSIM & 8.50E-3 & 6.40E-1 & 6.97E-1 & 8.36E-1 & 5.53E-1 & 5.78E-1 & 8.73E-1 & 8.40E-1 \\ DEIS & MSE & 1.11E-1 & 1.19E-2 & 1.07E-2 & 1.89E-3 & 2.14E-2 & 2.65E-2 & 2.36E-3 & 4.50E-3 \\  & SSIM & 8.50E-3 & 6.40E-1 & 6.97E-1 & 8.36E-1 & 5.53E-1 & 5.21E-1 & 8.73E-1 & 8.40E-1 \\  & FID & 177.35 & 86.72 & 124.80 & 127.35 & 122.77 & 122.23 & 89.46 & 89.20 \\ PNDM & MSE & 1.10E-1 & 5.75E-2 & 2.91E-2 & 3.73E-3 & 4.75E-2 & 4.18E-2 & 1.01E-3 & 2.17E-4 \\  & SSIM & 6.74E-3 & 4.26E-1 & 7.13E-1 & 9.62E-1 & 5.39E-1 & 5.68E-1 & 9.78E-1 & 9.85E-1 \\  & FID & 177.35 & 129.05 & 124.80 & 127.35 & 122.77 & 122.23 & 129.63 & 133.33 \\ HEUN & MSE & 1.10E-1 & 4.46E-2 & 2.91E-2 & 3.73E-3 & 4.75E-2 & 4.18E-2 & 1.08E-3 & 2.11E-4 \\  & SSIM & 6.74E-3 & 5.72E-1 & 7.13E-1 & 9.62E-1 & 5.39E-1 & 5.68E-1 & 9.83E-1 & 9.91E-1 \\  & FID & 177.35 & 129.05 & 124.80 & 127.35 & 122.77 & 122.23 & 129.63 & 133.33 \\ LMSD & MSE & 1.10E-1 & 4.46E-2 & 2.91E-2 & 3.73E-3 & 4.75E-2 & 4.18E-2 & 1.08E-3 & 2.11E-4 \\  & SSIM & 6.74E-3 & 5.72E-1 & 7.13E-1 & 9.62E-1 & 5.39E-1 & 5.68E-1 & 9.83E-1 & 9.91E-1 \\   

Table 23: CIFAR10 Dataset with Trigger: Stop Sign, target: Corner, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 185.20 & 107.88 & 108.12 & 109.60 & 105.08 & 109.91 & 104.87 & 98.41 \\  & MSE & 3.20E-1 & 3.56E-2 & 6.73E-3 & 1.27E-3 & 5.94E-3 & 7.13E-4 & 1.28E-3 & 1.94E-3 \\  & SSIM & 6.07E-3 & 8.50E-1 & 9.50E-1 & 9.72E-1 & 9.60E-1 & 9.83E-1 & 9.77E-1 & 9.73E-1 \\  & FID & 185.20 & 107.88 & 108.12 & 109.60 & 105.08 & 109.91 & 104.87 & 98.41 \\ DPM. O2 & MSE & 3.20E-1 & 3.56E-2 & 6.73E-3 & 1.27E-3 & 5.94E-3 & 7.13E-4 & 1.28E-3 & 1.94E-3 \\  & SSIM & 6.07E-3 & 8.50E-1 & 9.50E-1 & 9.72E-1 & 9.60E-1 & 9.83E-1 & 9.77E-1 & 9.73E-1 \\  & FID & 185.20 & 107.88 & 108.12 & 109.60 & 105.08 & 109.91 & 73.51 & 98.41 \\ DPM. O3 & MSE & 3.20E-1 & 3.56E-2 & 6.73E-3 & 1.27E-3 & 5.94E-3 & 7.13E-4 & 8.89E-4 & 1.94E-3 \\  & SSIM & 6.07E-3 & 8.50E-1 & 9.50E-1 & 9.72E-1 & 9.60E-1 & 9.83E-1 & 9.81E-1 & 9.73E-1 \\  & FID & 185.20 & 107.88 & 108.12 & 109.60 & 105.08 & 109.91 & 104.87 & 98.41 \\ DPM++. O2 & MSE & 3.20E-1 & 3.56E-2 & 6.73E-3 & 1.27E-3 & 5.94E-3 & 7.13E-4 & 1.28E-3 & 1.94E-3 \\  & SSIM & 6.07E-3 & 8.50E-1 & 9.50E-1 & 9.72E-1 & 9.60E-1 & 9.83E-1 & 9.77E-1 & 9.73E-1 \\  & FID & 185.20 & 107.88 & 108.12 & 109.60 & 105.08 & 109.91 & 104.87 & 98.41 \\  & SSIM & 6.07E-3 & 8.50E-1 & 9.50E-1 & 9.72E-1 & 9.60E-1 & 9.83E-1 & 9.77E-1 & 9.73E-1 \\  & FID & 185.20 & 107.88 & 108.12 & 109.60 & 105.08 & 109.91 & 104.87 & 98.41 \\ DEIS & MSE & 3.20E-1 & 3.56E-2 & 6.73E-3 & 1.27E-3 & 5.94E-3 & 7.13E-4 & 1.28E-3 & 1.94E-3 \\  & SSIM & 6.07E-3 & 8.50E-1 & 9.50E-1 & 9.72E-1 & 9.60E-1 & 9.83E-1 & 9.77E-1 & 9.73E-1 \\  & FID & 177.35 & 74.67 & 107.85 & 73.88 & 71.59 & 72.62 & 105.62 & 67.34 \\ PNDM & MSE & 3.24E-1 & 1.05E-1 & 1.97E-2 & 1.70E-3 & 1.17E-2 & 3.98E-5 & 1.53E-5 & 1.59E-5 \\  & SSIM & 4.67E-3 & 6.26E-1 & 9.32E-1 & 9.91E-1 & 9.53E-1 & 9.97E-1 & 9.99E-1 & 9.97E-1 \\  & FID & 177.35 & 110.54 & 107.85 & 108.84 & 103.94 & 107.88 & 105.62 & 96.78 \\ HEUN & MSE & 3.24E-1 & 8.27E-2 & 1.97E-2 & 1.86E-3 & 1.31E-2 & 4.19E-5 & 1.53E-5 & 9.93E-6 \\  & SSIM & 4.67E-3 & 7.25E-1 & 9.32E-1 & 9.92E-1 & 9.49E-1 & 9.99E-1 & 9.99E-1 & 9.99E-1 \\  & FID & 177.35 & 110.54 & 107.85 & 108.84 & 103.94 & 107.88 & 105.62 & 96.78 \\ LMSD & MSE & 3.24E-1 & 8.27E-2 & 1.97E-2 & 1.86E-3 & 1.31E-2 & 4.19E-5 & 1.53E-5 & 9.93E-6 \\  & SSIM & 4.67E-3 & 7.25E-1 & 9.32E-1 & 9.92E-1 & 9.49E-1 & 9.99E-1 & 9.99E-1 \\   

Table 24: CIFAR10 Dataset with Trigger: Stop Sign, target: Shoe, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 14.31 & 8.31 & 7.53 & 8.10 & 7.64 & 7.63 & 7.63 & 7.71 \\  & MSE & 1.76E-1 & 1.67E-1 & 1.66E-1 & 1.68E-1 & 1.67E-1 & 1.62E-1 & 1.58E-1 & 1.48E-1 \\  & SSIM & 3.41E-2 & 4.26E-2 & 4.36E-2 & 4.05E-2 & 4.37E-2 & 4.70E-2 & 4.72E-2 & 5.16E-2 \\  & FID & 185.20 & 110.69 & 109.15 & 110.76 & 107.63 & 110.05 & 106.94 & 100.60 \\ UNIPC & MSE & 2.33E-1 & 8.32E-3 & 6.79E-3 & 5.52E-3 & 6.54E-3 & 1.88E-3 & 3.30E-3 & 5.54E-3 \\  & SSIM & 8.04E-3 & 9.06E-1 & 9.45E-1 & 9.61E-1 & 9.55E-1 & 9.84E-1 & 9.80E-1 & 9.73E-1 \\  & FID & 185.20 & 110.69 & 109.15 & 110.76 & 107.63 & 110.05 & 106.94 & 100.60 \\ DPM. O2 & MSE & 2.33E-1 & 8.32E-3 & 6.79E-3 & 5.52E-3 & 6.54E-3 & 1.88E-3 & 3.30E-3 & 5.54E-3 \\  & SSIM & 8.04E-3 & 9.06E-1 & 9.45E-1 & 9.61E-1 & 9.55E-1 & 9.84E-1 & 9.80E-1 & 9.73E-1 \\  & FID & 185.20 & 110.69 & 109.15 & 110.76 & 107.63 & 110.05 & 106.94 & 100.60 \\ DPM. O3 & MSE & 2.33E-1 & 8.32E-3 & 6.79E-3 & 5.52E-3 & 6.54E-3 & 1.88E-3 & 3.30E-3 & 5.54E-3 \\  & SSIM & 8.04E-3 & 9.06E-1 & 9.45E-1 & 9.61E-1 & 9.55E-1 & 9.84E-1 & 9.80E-1 & 9.73E-1 \\  & FID & 185.20 & 110.69 & 109.15 & 110.76 & 107.63 & 110.05 & 106.94 & 100.60 \\ DPM++. O2 & MSE & 2.33E-1 & 8.32E-3 & 6.79E-3 & 5.52E-3 & 6.54E-3 & 1.88E-3 & 3.30E-3 & 5.54E-3 \\  & SSIM & 8.04E-3 & 9.06E-1 & 9.45E-1 & 9.61E-1 & 9.55E-1 & 9.84E-1 & 9.80E-1 & 9.73E-1 \\  & FID & 185.20 & 110.69 & 109.15 & 110.76 & 107.63 & 110.05 & 106.94 & 100.60 \\  & SSIM & 8.04E-3 & 9.06E-1 & 9.45E-1 & 9.61E-1 & 9.55E-1 & 9.84E-1 & 9.80E-1 & 9.73E-1 \\ DEIS & MSE & 2.33E-1 & 8.32E-3 & 6.79E-3 & 5.52E-3 & 6.54E-3 & 1.88E-3 & 3.30E-3 & 5.54E-3 \\  & SSIM & 8.04E-3 & 9.06E-1 & 9.45E-1 & 9.61E-1 & 9.55E-1 & 9.84E-1 & 9.80E-1 & 9.73E-1 \\  & FID & 177.35 & 77.16 & 42.34 & 75.69 & 42.47 & 106.99 & 8.11 & 95.57 \\ PNDM & MSE & 2.34E-1 & 9.55E-3 & 3.38E-2 & 3.55E-3 & 1.37E-4 & 8.03E-4 & 3.04E-5 & 6.80E-6 \\  & SSIM & 7.73E-3 & 9.49E-1 & 8.26E-1 & 9.82E-1 & 9.98E-1 & 9.95E-1 & 9.98E-1 & 1.00E+0 \\  & FID & 177.35 & 111.49 & 110.30 & 110.51 & 108.51 & 106.99 & 99.88 & 95.57 \\ HEUN & MSE & 2.34E-1 & 7.92E-3 & 1.04E-2 & 4.58E-3 & 9.40E-3 & 8.03E-4 & 1.18E-5 & 6.80E-6 \\  & SSIM & 7.73E-3 & 9.63E-1 & 9.52E-1 & 9.77E-1 & 9.54E-1 & 9.95E-1 & 1.00E+0 & 1.00E+0 \\  & FID & 177.35 & 111.49 & 110.30 & 110.51 & 108.51 & 106.99 & 99.88 & 95.57 \\ LMSD & MSE & 2.34E-1 & 7.92E-3 & 1.04E-2 & 4.58E-3 & 9.40E-3 & 8.03E-4 & 1.18E-5 & 6.80E-6 \\  & SSIM & 7.73E-3 & 9.63E-1 & 9.52E-1 & 9.77E-1 & 9.54E-1 & 9.95E-1 & 1.00E+0 & 1.00E+0 \\   

Table 25: CIFAR10 Dataset with Trigger: Stop Sign, target: Hat, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 185.20 & 121.50 & 121.93 & 130.83 & 123.83 & 134.04 & 146.21 & 159.08 \\  & MSE & 1.20E-1 & 1.18E-3 & 4.93E-4 & 4.28E-4 & 1.05E-3 & 1.23E-3 & 1.64E-3 & 3.83E-3 \\  & SSIM & 8.51E-4 & 9.68E-1 & 9.76E-1 & 9.78E-1 & 9.62E-1 & 9.62E-1 & 9.62E-1 & 9.31E-1 \\  & FID & 185.20 & 121.50 & 121.93 & 130.83 & 123.83 & 134.04 & 146.21 & 159.08 \\ DPM. O2 & MSE & 1.20E-1 & 1.18E-3 & 4.93E-4 & 4.28E-4 & 1.05E-3 & 1.23E-3 & 1.64E-3 & 3.83E-3 \\  & SSIM & 8.51E-4 & 9.68E-1 & 9.76E-1 & 9.78E-1 & 9.62E-1 & 9.62E-1 & 9.62E-1 & 9.31E-1 \\  & FID & 185.20 & 121.50 & 84.27 & 130.83 & 123.83 & 134.04 & 146.21 & 159.08 \\ DPM. O3 & MSE & 1.20E-1 & 1.18E-3 & 1.13E-3 & 4.28E-4 & 1.05E-3 & 1.23E-3 & 1.64E-3 & 3.83E-3 \\  & SSIM & 8.51E-4 & 9.68E-1 & 9.55E-1 & 9.78E-1 & 9.62E-1 & 9.62E-1 & 9.62E-1 & 9.31E-1 \\  & FID & 185.20 & 121.50 & 121.93 & 130.83 & 123.83 & 134.04 & 146.21 & 159.08 \\ DPM++. O2 & MSE & 1.20E-1 & 1.18E-3 & 4.93E-4 & 4.28E-4 & 1.05E-3 & 1.23E-3 & 1.64E-3 & 3.83E-3 \\  & SSIM & 8.51E-4 & 9.68E-1 & 9.76E-1 & 9.78E-1 & 9.62E-1 & 9.62E-1 & 9.62E-1 & 9.31E-1 \\ DPM++. O3 & MSE & 1.20E-1 & 1.18E-3 & 4.93E-4 & 4.28E-4 & 1.05E-3 & 1.23E-3 & 1.64E-3 & 3.83E-3 \\  & SSIM & 8.51E-4 & 9.68E-1 & 9.76E-1 & 9.78E-1 & 9.62E-1 & 9.62E-1 & 9.62E-1 & 9.31E-1 \\  & FID & 185.20 & 121.50 & 121.93 & 130.83 & 123.83 & 134.04 & 146.21 & 159.08 \\ DEIS & MSE & 1.20E-1 & 1.18E-3 & 4.93E-4 & 4.28E-4 & 1.05E-3 & 1.23E-3 & 1.64E-3 & 3.83E-3 \\  & SSIM & 8.51E-4 & 9.68E-1 & 9.76E-1 & 9.78E-1 & 9.62E-1 & 9.62E-1 & 9.62E-1 & 9.31E-1 \\  & FID & 177.35 & 84.49 & 8.34 & 54.96 & 91.59 & 98.41 & 154.06 & 75.09 \\ PNDM & MSE & 1.20E-1 & 1.80E-2 & 1.37E-2 & 2.36E-4 & 4.39E-5 & 1.61E-5 & 7.00E-6 & 2.16E-5 \\  & SSIM & 9.50E-4 & 7.88E-1 & 7.93E-1 & 9.80E-1 & 9.90E-1 & 9.91E-1 & 9.96E-1 & 9.85E-1 \\  & FID & 177.35 & 126.85 & 127.51 & 137.93 & 129.76 & 143.02 & 154.06 & 162.09 \\ HEUN & MSE & 1.20E-1 & 2.98E-3 & 1.27E-3 & 6.22E-5 & 3.67E-5 & 8.34E-6 & 7.00E-6 & 7.66E-6 \\  & SSIM & 9.50E-4 & 9.72E-1 & 9.85E-1 & 9.96E-1 & 9.96E-1 & 9.96E-1 & 9.95E-1 \\  & FID & 177.35 & 126.85 & 127.51 & 137.93 & 129.76 & 143.02 & 154.06 & 162.09 \\ LMSD & MSE & 1.20E-1 & 2.98E-3 & 1.27E-3 & 6.22E-5 & 3.67E-5 & 8.34E-6 & 7.00E-6 & 7.66E-6 \\  & SSIM & 9.50E-4 & 9.72E-1 & 9.85E-1 & 9.96E-1 & 9.96E-1 & 9.96E-1 & 9.95E-1 \\   

Table 26: CIFAR10 Dataset with Trigger: Grey Box, target: No Shift, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 185.20 & 123.67 & 117.55 & 118.87 & 119.92 & 122.32 & 126.07 & 131.79 \\  & MSE & 1.20E-1 & 2.83E-3 & 1.61E-3 & 4.57E-4 & 2.76E-4 & 1.82E-3 & 2.07E-3 & 4.21E-3 \\  & SSIM & 7.09E-4 & 9.50E-1 & 9.60E-1 & 9.76E-1 & 9.84E-1 & 9.54E-1 & 9.55E-1 & 9.27E-1 \\  & FID & 185.20 & 123.67 & 117.55 & 118.87 & 119.92 & 122.32 & 126.07 & 131.79 \\ DPM. O2 & MSE & 1.20E-1 & 2.83E-3 & 1.61E-3 & 4.57E-4 & 2.76E-4 & 1.82E-3 & 2.07E-3 & 4.21E-3 \\  & SSIM & 7.09E-4 & 9.50E-1 & 9.60E-1 & 9.76E-1 & 9.84E-1 & 9.54E-1 & 9.55E-1 & 9.27E-1 \\  & FID & 185.20 & 123.67 & 117.55 & 118.87 & 119.92 & 122.32 & 126.07 & 131.79 \\ DPM. O3 & MSE & 1.20E-1 & 2.83E-3 & 1.61E-3 & 4.57E-4 & 2.76E-4 & 1.82E-3 & 2.07E-3 & 4.21E-3 \\  & SSIM & 7.09E-4 & 9.50E-1 & 9.60E-1 & 9.76E-1 & 9.84E-1 & 9.54E-1 & 9.55E-1 & 9.27E-1 \\  & FID & 185.20 & 123.67 & 117.55 & 118.87 & 119.92 & 122.32 & 126.07 & 131.79 \\ DPM++. O2 & MSE & 1.20E-1 & 2.83E-3 & 1.61E-3 & 4.57E-4 & 2.76E-4 & 1.82E-3 & 2.07E-3 & 4.21E-3 \\  & SSIM & 7.09E-4 & 9.50E-1 & 9.60E-1 & 9.76E-1 & 9.84E-1 & 9.54E-1 & 9.55E-1 & 9.27E-1 \\ DPM++. O3 & MSE & 1.20E-1 & 2.83E-3 & 1.07E-2 & 4.57E-4 & 2.76E-4 & 1.82E-3 & 2.07E-3 & 4.21E-3 \\  & SSIM & 7.09E-4 & 9.50E-1 & 8.34E-1 & 9.76E-1 & 9.84E-1 & 9.54E-1 & 9.55E-1 & 9.27E-1 \\  & FID & 185.20 & 123.67 & 117.55 & 118.87 & 119.92 & 122.32 & 126.07 & 131.79 \\ DEIS & MSE & 1.20E-1 & 2.83E-3 & 1.61E-3 & 4.57E-4 & 2.76E-4 & 1.82E-3 & 2.07E-3 & 4.21E-3 \\  & SSIM & 7.09E-4 & 9.50E-1 & 9.60E-1 & 9.76E-1 & 9.84E-1 & 9.54E-1 & 9.55E-1 & 9.27E-1 \\  & FID & 177.35 & 88.59 & 81.09 & 47.93 & 86.19 & 88.97 & 129.77 & 102.72 \\ PNDM & MSE & 1.20E-1 & 1.71E-2 & 3.19E-3 & 2.15E-4 & 3.71E-4 & 1.64E-5 & 6.98E-6 & 1.42E-5 \\  & SSIM & 6.17E-4 & 8.09E-1 & 9.57E-1 & 9.86E-1 & 9.90E-1 & 9.94E-1 & 9.97E-1 & 9.93E-1 \\  & FID & 177.35 & 125.94 & 119.25 & 121.26 & 122.38 & 125.11 & 129.77 & 138.51 \\ HEUN & MSE & 1.20E-1 & 8.17E-3 & 2.87E-3 & 2.09E-4 & 3.83E-4 & 7.62E-6 & 6.98E-6 & 7.41E-6 \\  & SSIM & 6.17E-4 & 9.27E-1 & 9.73E-1 & 9.96E-1 & 9.94E-1 & 9.98E-1 & 9.97E-1 & 9.97E-1 \\  & FID & 177.35 & 125.94 & 119.25 & 121.26 & 122.38 & 125.11 & 129.77 & 138.51 \\ LMSD & MSE & 1.20E-1 & 8.17E-3 & 2.87E-3 & 2.09E-4 & 3.83E-4 & 7.62E-6 & 6.98E-6 & 7.41E-6 \\  & SSIM & 6.17E-4 & 9.27E-1 & 9.73E-1 & 9.96E-1 & 9.94E-1 & 9.98E-1 & 9.97E-1 & 9.97E-1 \\   

Table 27: CIFAR10 Dataset with Trigger: Grey Box, target: Shift, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\   & Metric & & & & & & & & \\   & FID & 14.31 & 9.91 & 10.94 & 12.99 & 15.06 & 19.85 & 28.11 & 53.35 \\  & MSE & 7.86E-2 & 5.56E-2 & 5.34E-2 & 4.97E-2 & 5.01E-2 & 3.87E-2 & 2.74E-2 & 1.32E-2 \\  & SSIM & 7.17E-2 & 2.50E-1 & 2.80E-1 & 3.29E-1 & 3.35E-1 & 4.60E-1 & 5.88E-1 & 7.73E-1 \\  & FID & 185.20 & 125.95 & 127.27 & 124.84 & 125.84 & 131.79 & 134.52 & 136.83 \\  & MSE & 1.05E-1 & 2.18E-3 & 8.14E-4 & 5.05E-4 & 4.90E-4 & 1.09E-3 & 1.69E-3 & 5.51E-3 \\  & SSIM & 1.13E-3 & 9.16E-1 & 9.33E-1 & 9.48E-1 & 9.54E-1 & 9.36E-1 & 9.24E-1 & 8.36E-1 \\  & FID & 185.20 & 125.95 & 127.27 & 124.84 & 125.84 & 131.79 & 134.52 & 136.83 \\ DPM. O2 & MSE & 1.05E-1 & 2.18E-3 & 8.14E-4 & 5.05E-4 & 4.90E-4 & 1.09E-3 & 1.69E-3 & 5.51E-3 \\  & SSIM & 1.13E-3 & 9.16E-1 & 9.33E-1 & 9.48E-1 & 9.54E-1 & 9.36E-1 & 9.24E-1 & 8.36E-1 \\  & FID & 185.20 & 125.95 & 127.27 & 124.84 & 125.84 & 131.79 & 134.52 & 136.83 \\ DPM. O3 & MSE & 1.05E-1 & 2.18E-3 & 8.14E-4 & 5.05E-4 & 4.90E-4 & 1.09E-3 & 1.69E-3 & 5.51E-3 \\  & SSIM & 1.13E-3 & 9.16E-1 & 9.33E-1 & 9.48E-1 & 9.54E-1 & 9.36E-1 & 9.24E-1 & 8.36E-1 \\  & FID & 185.20 & 125.95 & 127.27 & 124.84 & 125.84 & 131.79 & 134.52 & 136.83 \\ DPM++. O2 & MSE & 1.05E-1 & 2.18E-3 & 8.14E-4 & 5.05E-4 & 4.90E-4 & 1.09E-3 & 1.69E-3 & 5.51E-3 \\  & SSIM & 1.13E-3 & 9.16E-1 & 9.33E-1 & 9.48E-1 & 9.54E-1 & 9.36E-1 & 9.24E-1 & 8.36E-1 \\  & FID & 185.20 & 125.95 & 127.27 & 124.84 & 125.84 & 131.79 & 134.52 & 136.83 \\  & SSIM & 1.13E-3 & 9.16E-1 & 9.33E-1 & 9.48E-1 & 9.54E-1 & 9.36E-1 & 9.24E-1 & 8.36E-1 \\ DEIS & MSE & 1.05E-1 & 2.18E-3 & 8.14E-4 & 5.05E-4 & 4.90E-4 & 1.09E-3 & 1.69E-3 & 5.51E-3 \\  & SSIM & 1.13E-3 & 9.16E-1 & 9.33E-1 & 9.48E-1 & 9.54E-1 & 9.36E-1 & 9.24E-1 & 8.36E-1 \\  & FID & 177.35 & 134.07 & 93.84 & 49.66 & 94.22 & 102.26 & 147.95 & 156.45 \\ PNDM & MSE & 1.05E-1 & 3.56E-3 & 4.82E-3 & 9.19E-5 & 3.88E-5 & 2.01E-5 & 9.18E-6 & 7.25E-6 \\  & SSIM & 1.38E-3 & 9.66E-1 & 8.97E-1 & 9.78E-1 & 9.87E-1 & 9.88E-1 & 9.94E-1 & 9.94E-1 \\  & FID & 177.35 & 134.07 & 135.99 & 134.72 & 137.73 & 145.55 & 147.95 & 156.45 \\ HEUN & MSE & 1.05E-1 & 3.56E-3 & 4.92E-4 & 4.11E-5 & 5.36E-5 & 1.44E-5 & 9.18E-6 & 7.25E-6 \\  & SSIM & 1.38E-3 & 9.66E-1 & 9.92E-1 & 9.96E-1 & 9.95E-1 & 9.94E-1 & 9.94E-1 & 9.94E-1 \\  & FID & 177.35 & 134.07 & 135.99 & 134.72 & 137.73 & 145.55 & 147.95 & 156.45 \\ LMSD & MSE & 1.05E-1 & 3.56E-3 & 4.92E-4 & 4.11E-5 & 5.36E-5 & 1.44E-5 & 9.18E-6 & 7.25E-6 \\  & SSIM & 1.38E-3 & 9.66E-1 & 9.92E-1 & 9.96E-1 & 9.95E-1 & 9.94E-1 & 9.94E-1 & 9.94E-1 \\   

Table 28: CIFAR10 Dataset with Trigger: Grey Box, target: Corner, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 185.20 & 108.96 & 109.27 & 113.59 & 110.68 & 111.37 & 105.23 & 105.27 \\  & MSE & 3.37E-1 & 2.20E-3 & 1.02E-3 & 8.76E-4 & 9.58E-4 & 1.80E-3 & 1.97E-3 & 3.59E-3 \\  & SSIM & 2.29E-4 & 9.80E-1 & 9.82E-1 & 9.82E-1 & 9.83E-1 & 9.76E-1 & 9.76E-1 & 9.65E-1 \\  & FID & 185.20 & 108.96 & 109.27 & 113.59 & 110.68 & 111.37 & 105.23 & 105.27 \\ DPM. O2 & MSE & 3.37E-1 & 2.20E-3 & 1.02E-3 & 8.76E-4 & 9.58E-4 & 1.80E-3 & 1.97E-3 & 3.59E-3 \\  & SSIM & 2.29E-4 & 9.80E-1 & 9.82E-1 & 9.82E-1 & 9.83E-1 & 9.76E-1 & 9.76E-1 & 9.65E-1 \\  & FID & 185.20 & 108.96 & 109.27 & 113.59 & 110.68 & 111.37 & 105.23 & 105.27 \\ DPM. O3 & MSE & 3.37E-1 & 2.20E-3 & 1.02E-3 & 8.76E-4 & 9.58E-4 & 1.80E-3 & 1.97E-3 & 3.59E-3 \\  & SSIM & 2.29E-4 & 9.80E-1 & 9.82E-1 & 9.82E-1 & 9.83E-1 & 9.76E-1 & 9.76E-1 & 9.65E-1 \\  & FID & 185.20 & 75.06 & 109.27 & 113.59 & 110.68 & 111.37 & 105.23 & 105.27 \\ DPM++. O2 & MSE & 3.37E-1 & 1.96E-2 & 1.02E-3 & 8.76E-4 & 9.58E-4 & 1.80E-3 & 1.97E-3 & 3.59E-3 \\  & SSIM & 2.29E-4 & 8.92E-1 & 9.82E-1 & 9.82E-1 & 9.83E-1 & 9.76E-1 & 9.76E-1 & 9.65E-1 \\ DPM++. O3 & MSE & 3.37E-1 & 2.20E-3 & 1.02E-3 & 8.76E-4 & 9.58E-4 & 1.80E-3 & 1.97E-3 & 3.59E-3 \\  & SSIM & 2.29E-4 & 9.80E-1 & 9.82E-1 & 9.82E-1 & 9.83E-1 & 9.76E-1 & 9.76E-1 & 9.65E-1 \\ DEIS & MSE & 3.37E-1 & 2.20E-3 & 1.02E-3 & 8.76E-4 & 9.58E-4 & 1.80E-3 & 1.97E-3 & 3.59E-3 \\  & SSIM & 2.29E-4 & 9.80E-1 & 9.82E-1 & 9.82E-1 & 9.83E-1 & 9.76E-1 & 9.76E-1 & 9.65E-1 \\  & FID & 177.35 & 76.12 & 77.07 & 44.45 & 74.86 & 76.19 & 73.59 & 73.31 \\ PNDM & MSE & 3.37E-1 & 6.60E-2 & 4.31E-3 & 8.33E-5 & 5.15E-5 & 2.28E-5 & 1.95E-5 & 1.49E-5 \\  & SSIM & 2.08E-4 & 7.43E-1 & 9.75E-1 & 9.95E-1 & 9.97E-1 & 9.97E-1 & 9.98E-1 & 9.97E-1 \\  & FID & 177.35 & 109.16 & 111.45 & 112.72 & 109.41 & 108.98 & 104.22 & 105.28 \\ HEUN & MSE & 3.37E-1 & 4.08E-3 & 8.50E-4 & 1.14E-4 & 5.27E-5 & 1.76E-5 & 1.19E-5 & 8.16E-6 \\  & SSIM & 2.08E-4 & 9.87E-1 & 9.96E-1 & 9.98E-1 & 9.99E-1 & 9.99E-1 & 9.99E-1 \\  & FID & 177.35 & 109.16 & 111.45 & 112.72 & 109.41 & 108.98 & 104.22 & 105.28 \\ LMSD & MSE & 3.37E-1 & 4.08E-3 & 8.50E-4 & 1.14E-4 & 5.27E-5 & 1.76E-5 & 1.19E-5 & 8.16E-6 \\  & SSIM & 2.08E-4 & 9.87E-1 & 9.96E-1 & 9.98E-1 & 9.99E-1 & 9.99E-1 & 9.99E-1 \\   

Table 29: CIFAR10 Dataset with Trigger: Grey Box, target: Shoe, and inference-time clipping.

    & P.R. & 0\% & 5\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & & \\   & FID & 14.31 & 8.42 & 8.82 & 8.89 & 8.97 & 10.11 & 11.32 & 17.82 \\  & MSE & 1.74E-1 & 1.24E-1 & 1.08E-1 & 1.09E-1 & 1.12E-1 & 1.01E-1 & 9.63E-2 & 8.57E-2 \\  & SSIM & 3.43E-2 & 2.08E-1 & 2.83E-1 & 2.82E-1 & 2.66E-1 & 3.26E-1 & 3.55E-1 & 4.07E-1 \\  & FID & 185.20 & 110.41 & 111.65 & 109.41 & 112.08 & 111.42 & 105.79 & 107.68 \\ UNIPC & MSE & 2.40E-1 & 5.12E-4 & 3.63E-4 & 5.34E-4 & 8.85E-4 & 1.63E-3 & 3.46E-3 & 6.46E-3 \\  & SSIM & 2.97E-4 & 9.88E-1 & 9.92E-1 & 9.92E-1 & 9.92E-1 & 9.88E-1 & 9.80E-1 & 9.70E-1 \\  & FID & 185.20 & 110.41 & 111.65 & 109.41 & 112.08 & 111.42 & 105.79 & 107.68 \\ DPM. O2 & MSE & 2.40E-1 & 5.12E-4 & 3.63E-4 & 5.34E-4 & 8.85E-4 & 1.63E-3 & 3.46E-3 & 6.46E-3 \\  & SSIM & 2.97E-4 & 9.88E-1 & 9.92E-1 & 9.92E-1 & 9.92E-1 & 9.88E-1 & 9.80E-1 & 9.70E-1 \\  & FID & 185.20 & 110.41 & 111.65 & 109.41 & 112.08 & 111.42 & 105.79 & 107.68 \\ DPM. O3 & MSE & 2.40E-1 & 5.12E-4 & 3.63E-4 & 5.34E-4 & 8.85E-4 & 1.63E-3 & 3.46E-3 & 6.46E-3 \\  & SSIM & 2.97E-4 & 9.88E-1 & 9.92E-1 & 9.92E-1 & 9.92E-1 & 9.88E-1 & 9.80E-1 & 9.70E-1 \\  & FID & 185.20 & 110.41 & 111.65 & 109.41 & 112.08 & 111.42 & 105.79 & 107.68 \\ DPM++. O2 & MSE & 2.40E-1 & 5.12E-4 & 3.63E-4 & 5.34E-4 & 8.85E-4 & 1.63E-3 & 3.46E-3 & 6.46E-3 \\  & SSIM & 2.97E-4 & 9.88E-1 & 9.92E-1 & 9.92E-1 & 9.92E-1 & 9.88E-1 & 9.80E-1 & 9.70E-1 \\  & FID & 185.20 & 110.41 & 111.65 & 109.41 & 112.08 & 111.42 & 105.79 & 107.68 \\ DEIS & MSE & 2.40E-1 & 5.12E-4 & 3.63E-4 & 5.34E-4 & 8.85E-4 & 1.63E-3 & 3.46E-3 & 6.46E-3 \\  & SSIM & 185.20 & 110.41 & 111.65 & 109.41 & 112.08 & 111.42 & 105.79 & 107.68 \\ DEIS & MSE & 2.40E-1 & 5.12E-4 & 3.63E-4 & 5.34E-4 & 8.85E-4 & 1.63E-3 & 3.46E-3 & 6.46E-3 \\  & SSIM & 2.97E-4 & 9.88E-1 & 9.92E-1 & 9.92E-1 & 9.92E-1 & 9.88E-1 & 9.80E-1 & 9.70E-1 \\  & FID & 177.35 & 112.35 & 111.44 & 77.27 & 41.78 & 77.43 & 101.54 & 102.53 \\ PNDM & MSE & 2.40E-1 & 4.60E-4 & 9.89E-5 & 7.03E-5 & 3.09E-5 & 2.03E-5 & 7.09E-6 & 6.13E-6 \\  & SSIM & 1.79E-4 & 9.97E-1 & 9.99E-1 & 9.98E-1 & 9.98E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 \\  & FID & 177.35 & 112.35 & 111.44 & 109.32 & 110.74 & 109.52 & 101.54 & 102.53 \\ HEUN & MSE & 2.40E-1 & 4.60E-4 & 9.89E-5 & 2.39E-5 & 1.41E-5 & 1.33E-5 & 7.09E-6 & 6.13E-6 \\  & SSIM & 1.79E-4 & 9.97E-1 & 9.99E-1 & 9.99E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\  & FID & 177.35 & 112.35 & 111.44 & 109.32 & 110.74 & 109.52 & 101.54 & 102.53 \\ LMSD & MSE & 2.40E-1 & 4.60E-4 & 9.89E-5 & 2.39E-5 & 1.41E-5 & 1.33E-5 & 7.09E-6 & 6.13E-6 \\  & SSIM & 1.79E-4 & 9.97E-1 & 9.99E-1 & 9.99E-1 & 9.99E-1 & 1.00E+0 & 1.00E+0 & 1.00E+0 \\   

Table 30: CIFAR10 Dataset with Trigger: Grey Box, target: Hat, and inference-time clipping.

    & P.R. & 0\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\ Sampler & Metric & & & & & & & \\   & LPIPS & 3.25E-1 & 2.78E-1 & 2.77E-1 & 2.68E-1 & 2.56E-1 & 2.55E-1 & 2.59E-1 \\  & MSE & 2.97E-1 & 3.58E-3 & 1.31E-3 & 1.61E-3 & 4.90E-4 & 1.99E-4 & 8.39E-5 \\  & SSIM & 5.66E-2 & 9.85E-1 & 9.94E-1 & 9.93E-1 & 9.97E-1 & 9.99E-1 & 9.99E-1 \\  & LPIPS & 3.19E-1 & 2.78E-1 & 2.82E-1 & 2.76E-1 & 2.66E-1 & 2.65E-1 & 2.58E-1 \\ UNIPC, Line & MSE & 3.03E-1 & 3.78E-3 & 2.83E-3 & 3.29E-3 & 1.27E-3 & 3.37E-4 & 6.92E-5 \\  & SSIM & 4.61E-2 & 9.85E-1 & 9.89E-1 & 9.87E-1 & 9.95E-1 & 9.98E-1 & 9.99E-1 \\  & LPIPS & 3.10E-1 & 2.87E-1 & 2.85E-1 & 2.85E-1 & 2.70E-1 & 2.63E-1 & 2.63E-1 \\ UNIPC, Box & MSE & 3.34E-1 & 2.15E-2 & 3.17E-2 & 3.47E-2 & 3.29E-2 & 1.93E-2 & 1.11E-2 \\  & SSIM & 1.05E-2 & 9.10E-1 & 8.76E-1 & 8.56E-1 & 8.67E-1 & 9.19E-1 & 9.51E-1 \\  & LPIPS & 3.25E-1 & 2.79E-1 & 2.70E-1 & 2.65E-1 & 2.57E-1 & 2.55E-1 & 2.57E-1 \\ DPM. O2, Blur & MSE & 2.97E-1 & 2.98E-3 & 1.52E-3 & 1.82E-3 & 5.58E-4 & 2.93E-4 & 1.37E-4 \\  & SSIM & 5.66E-2 & 9.87E-1 & 9.93E-1 & 9.91E-1 & 9.97E-1 & 9.98E-1 & 9.99E-1 \\  & LPIPS & 3.19E-1 & 2.88E-1 & 2.80E-1 & 2.73E-1 & 2.67E-1 & 2.63E-1 & 2.60E-1 \\ DPM. O2, Line & MSE & 3.03E-1 & 3.02E-3 & 3.25E-3 & 3.19E-3 & 1.39E-3 & 3.21E-4 & 1.97E-4 \\  & SSIM & 4.61E-2 & 9.87E-1 & 9.87E-1 & 9.88E-1 & 9.94E-1 & 9.98E-1 & 9.99E-1 \\  & LPIPS & 3.10E-1 & 2.91E-1 & 2.87E-1 & 2.83E-1 & 2.71E-1 & 2.59E-1 & 2.67E-1 \\ DPM. O2, Box & MSE & 3.34E-1 & 2.07E-2 & 3.24E-2 & 3.45E-2 & 3.60E-2 & 2.14E-2 & 1.04E-2 \\  & SSIM & 1.05E-2 & 9.14E-1 & 8.74E-1 & 8.57E-1 & 8.53E-1 & 9.11E-1 & 9.50E-1 \\  & LPIPS & 3.25E-1 & 2.78E-1 & 2.76E-1 & 2.68E-1 & 2.59E-1 & 2.52E-1 & 2.54E-1 \\ DPM++. O2, Blur & MSE & 2.97E-1 & 3.56E-3 & 1.52E-3 & 1.85E-3 & 4.64E-4 & 5.16E-4 & 4.34E-6 \\  & SSIM & 5.66E-2 & 9.84E-1 & 9.93E-1 & 9.92E-1 & 9.98E-1 & 9.97E-1 & 1.00E+0 \\  & LPIPS & 3.19E-1 & 2.87E-1 & 2.76E-1 & 2.73E-1 & 2.67E-1 & 2.61E-1 & 2.57E-1 \\ DPM++. O2, Line & MSE & 3.03E-1 & 3.55E-3 & 4.38E-3 & 4.30E-3 & 1.68E-3 & 4.40E-4 & 9.60E-5 \\  & SSIM & 4.61E-2 & 9.85E-1 & 9.83E-1 & 9.83E-1 & 9.93E-1 & 9.98E-1 & 9.99E-1 \\  & LPIPS & 3.10E-1 & 2.87E-1 & 2.85E-1 & 2.79E-1 & 2.74E-1 & 2.65E-1 & 2.64E-1 \\ DPM++. O2, Box & MSE & 3.34E-1 & 2.44E-2 & 3.40E-2 & 3.12E-2 & 3.79E-2 & 1.97E-2 & 1.08E-2 \\  & SSIM & 1.05E-2 & 8.98E-1 & 8.72E-1 & 8.66E-1 & 8.45E-1 & 9.15E-1 & 9.49E-1 \\  & LPIPS & 3.25E-1 & 2.81E-1 & 2.76E-1 & 2.72E-1 & 2.60E-1 & 2.51E-1 & 2.50E-1 \\ DEIS, Blur & MSE & 2.97E-1 & 2.87E-3 & 1.34E-3 & 2.96E-3 & 2.02E-4 & 3.58E-4 & 1.61E-4 \\  & SSIM & 5.66E-2 & 9.87E-1 & 9.94E-1 & 9.88E-1 & 9.98E-1 & 9.98E-1 & 9.99E-1 \\  & LPIPS & 3.19E-1 & 2.82E-1 & 2.77E-1 & 2.76E-1 & 2.71E-1 & 2.58E-1 & 2.61E-1 \\ DEIS, Line & MSE & 3.03E-1 & 3.54E-3 & 2.50E-3 & 3.31E-3 & 1.29E-3 & 2.57E-4 & 5.03E-6 \\  & SSIM & 4.61E-2 & 9.85E-1 & 9.90E-1 & 9.86E-1 & 9.95E-1 & 9.98E-1 & 1.00E+0 \\  & LPIPS & 3.10E-1 & 2.84E-1 & 2.85E-1 & 2.81E-1 & 2.70E-1 & 2.61E-1 & 2.67E-1 \\ DEIS, Box & MSE & 3.34E-1 & 2.26E-2 & 3.26E-2 & 3.49E-2 & 3.46E-2 & 2.05E-2 & 1.03E-2 \\  & SSIM & 1.05E-2 & 9.07E-1 & 8.74E-1 & 8.52E-1 & 8.59E-1 & 9.15E-1 & 9.54E-1 \\   

Table 31: DDPM performs on **Blur**, **Line**, **Box**, and **Box** with CIFAR10 Dataset, trigger: Stop Sign, and target: Shoe.

    & P.R. & 0\% & 10\% & 20\% & 30\% & 50\% & 70\% & 90\% \\  & Metric & & & & & & & \\   & LPIPS & 3.25E-1 & 2.72E-1 & 2.66E-1 & 2.63E-1 & 2.53E-1 & 2.39E-1 & 2.45E-1 \\  & MSE & 2.85E-1 & 1.25E-2 & 1.63E-3 & 2.09E-3 & 3.59E-4 & 1.01E-3 & 6.57E-4 \\  & SSIM & 2.98E-2 & 9.52E-1 & 9.93E-1 & 9.91E-1 & 9.98E-1 & 9.95E-1 & 9.96E-1 \\  & LPIPS & 3.19E-1 & 2.73E-1 & 2.69E-1 & 2.65E-1 & 2.54E-1 & 2.41E-1 & 2.45E-1 \\  & MSE & 2.83E-1 & 1.56E-2 & 2.33E-3 & 2.71E-3 & 1.11E-3 & 8.63E-4 & 1.23E-3 \\  & SSIM & 2.13E-2 & 9.42E-1 & 9.90E-1 & 9.89E-1 & 9.95E-1 & 9.96E-1 & 9.94E-1 \\  & LPIPS & 3.10E-1 & 2.87E-1 & 2.79E-1 & 2.79E-1 & 2.65E-1 & 2.51E-1 & 2.52E-1 \\ UNIPC, Box & MSE & 3.04E-1 & 3.63E-2 & 2.16E-2 & 1.43E-2 & 4.97E-3 & 3.31E-3 & 6.02E-3 \\  & SSIM & -1.37E-3 & 8.76E-1 & 9.23E-1 & 9.47E-1 & 9.81E-1 & 9.87E-1 & 9.76E-1 \\  & LPIPS & 3.25E-1 & 2.73E-1 & 2.71E-1 & 2.60E-1 & 2.55E-1 & 2.41E-1 & 2.37E-1 \\ DPM. O2, Blur & MSE & 2.85E-1 & 1.16E-2 & 1.52E-3 & 1.58E-3 & 8.13E-4 & 1.07E-3 & 7.07E-4 \\  & SSIM & 2.98E-2 & 9.58E-1 & 9.93E-1 & 9.93E-1 & 9.96E-1 & 9.95E-1 & 9.97E-1 \\  & LPIPS & 3.19E-1 & 2.76E-1 & 2.68E-1 & 2.63E-1 & 2.57E-1 & 2.42E-1 & 2.45E-1 \\ DPM. O2, Line & MSE & 2.83E-1 & 1.43E-2 & 2.28E-3 & 1.84E-3 & 1.03E-3 & 1.13E-3 & 2.68E-3 \\  & SSIM & 2.13E-2 & 9.46E-1 & 9.91E-1 & 9.92E-1 & 9.95E-1 & 9.94E-1 & 9.89E-1 \\  & LPIPS & 3.10E-1 & 2.86E-1 & 2.76E-1 & 2.77E-1 & 2.65E-1 & 2.54E-1 & 2.52E-1 \\ DPM. O2, Box & MSE & 3.04E-1 & 3.83E-2 & 2.15E-2 & 1.40E-2 & 3.26E-3 & 3.50E-3 & 1.41E-3 \\  & SSIM & -1.37E-3 & 8.67E-1 & 9.23E-1 & 9.48E-1 & 9.87E-1 & 9.86E-1 & 9.93E-1 \\  & LPIPS & 3.25E-1 & 2.71E-1 & 2.65E-1 & 2.62E-1 & 2.59E-1 & 2.39E-1 & 2.45E-1 \\ DPM++. O2, Blur & MSE & 2.85E-1 & 1.16E-2 & 1.46E-3 & 1.77E-3 & 7.03E-4 & 9.69E-4 & 4.74E-4 \\  & SSIM & 2.98E-2 & 9.56E-1 & 9.94E-1 & 9.92E-1 & 9.96E-1 & 9.95E-1 & 9.98E-1 \\  & LPIPS & 3.19E-1 & 2.68E-1 & 2.72E-1 & 2.62E-1 & 2.52E-1 & 2.39E-1 & 2.41E-1 \\ DPM++. O2, Line & MSE & 2.83E-1 & 1.65E-2 & 1.57E-3 & 1.78E-3 & 1.38E-4 & 8.54E-4 & 2.20E-3 \\  & SSIM & 2.13E-2 & 9.38E-1 & 9.92E-1 & 9.92E-1 & 9.99E-1 & 9.96E-1 & 9.90E-1 \\  & LPIPS & 3.10E-1 & 2.85E-1 & 2.79E-1 & 2.72E-1 & 2.67E-1 & 2.52E-1 & 2.53E-1 \\ DPM++. O2, Box & MSE & 3.04E-1 & 4.04E-2 & 2.17E-2 & 1.31E-2 & 4.82E-3 & 4.04E-3 & 6.23E-3 \\  & SSIM & -1.37E-3 & 8.62E-1 & 9.22E-1 & 9.52E-1 & 9.81E-1 & 9.84E-1 & 9.73E-1 \\  & LPIPS & 3.25E-1 & 2.69E-1 & 2.71E-1 & 2.66E-1 & 2.55E-1 & 2.43E-1 & 2.46E-1 \\ DEIS, Blur & MSE & 2.85E-1 & 1.11E-2 & 1.20E-3 & 1.85E-3 & 9.45E-4 & 5.66E-4 & 4.59E-4 \\  & SSIM & 2.98E-2 & 9.59E-1 & 9.94E-1 & 9.93E-1 & 9.95E-1 & 9.97E-1 & 9.97E-1 \\  & LPIPS & 3.19E-1 & 2.73E-1 & 2.66E-1 & 2.59E-1 & 2.53E-1 & 2.43E-1 & 2.41E-1 \\ DEIS, Line & MSE & 2.83E-1 & 1.39E-2 & 2.92E-3 & 2.24E-3 & 5.60E-4 & 1.29E-3 & 1.98E-3 \\  & SSIM & 2.13E-2 & 9.49E-1 & 9.88E-1 & 9.91E-1 & 9.97E-1 & 9.94E-1 & 9.90E-1 \\  & LPIPS & 3.10E-1 & 2.84E-1 & 2.79E-1 & 2.72E-1 & 2.65E-1 & 2.53E-1 & 2.44E-1 \\ DEIS, Box & MSE & 3.04E-1 & 4.07E-2 & 2.16E-2 & 1.16E-2 & 4.20E-3 & 3.75E-3 & 5.07E-3 \\  & SSIM & -1.37E-3 & 8.62E-1 & 9.23E-1 & 9.57E-1 & 9.83E-1 & 9.85E-1 & 9.79E-1 \\   

Table 32: DDPM performs on **Blur**, **Line**, **Box**, and **Box** with CIFAR10 Dataset, trigger: Stop Sign, and target: Hat.