# SpaFL: Communication-Efficient Federated Learning with Sparse Models and Low Computational Overhead

Minsu Kim

Virginia Tech

&Walid Saad

Virginia Tech

Merouane Debbah

Khalifa University&Choong Seon Hong

Kyung Hee University

This work was supported in part by a grant from the Amazon-Virginia Tech Initiative for Efficient and in part by the Robust Machine Learning and U.S. National Science Foundation under Grant CNS-2114267.

###### Abstract

The large communication and computation overhead of federated learning (FL) is one of the main challenges facing its practical deployment over resource-constrained clients and systems. In this work, SpaFL: a communication-efficient FL framework is proposed to optimize sparse model structures with low computational overhead. In SpaFL, a trainable threshold is defined for each filter/neuron to prune its all connected parameters, thereby leading to structured sparsity. To optimize the pruning process itself, only thresholds are communicated between a server and clients instead of parameters, thereby learning how to prune. Further, global thresholds are used to update model parameters by extracting aggregated parameter importance. The generalization bound of SpaFL is also derived, thereby proving key insights on the relation between sparsity and performance. Experimental results show that SpaFL improves accuracy while requiring much less communication and computing resources compared to sparse baselines. The code is available at [https://github.com/news-vt/SpaFL_NerulPS_2024](https://github.com/news-vt/SpaFL_NerulPS_2024)

## 1 Introduction

Federated learning (FL) is a distributed machine learning framework in which clients collaborate to train a machine learning (ML) model without sharing private data . In FL, clients perform multiple epochs of local training using their own datasets and communicate model updates with a server. Different from a classical, centralized ML, FL systems are typically deployed on edge devices such as mobile or Internet of Things (IoT) devices, which have limited computing and communication resources. However, current ML models are typically too large and complex to be trained and deployed for inference by edge devices. Moreover, large model sizes can induce significant FL communication costs on both devices and communication networks. Hence, the practical deployment of FL over _resource-constrained devices and systems_ requires optimized computation and communication costs for both edge devices and communication networks. This has motivated lines of research focused on reducing communication overhead in FL , training sparse models in FL , and optimizing model architectures to find a compact model for inference . The works in  proposed training algorithms such as quantization, gradient compression, and transmitting the subset of models in order to reduce the communication costs of FL. However, the associated computational overhead of these existing algorithms remains high since devices have to train a dense model. In , FL algorithms in which devices train and communicate sparse models are proposed. However, the works in  used unstructured pruning, which is difficult to gain the computation efficiency in practice. Moreover, the computationand communication overhead can still be large if model sparsity is not high. In [6; 7; 8; 9], the authors investigated the structured sparsity, however, the solutions therein either fixed the channel sparsity patterns for clients or did not optimize the pruning process. Furthermore, the FL approaches of [10; 11; 12] can significantly increase computation resource usage by training multiple models for resource-constrained devices. Clearly, despite a surge of literature on sparsity in FL, there is still a need to develop new FL algorithms that can find sparse model structures with optimized communication efficiency and low computational overhead to operate on resource-constrained devices.

The main contribution of this paper is _SpaFL: a communication-efficient FL framework for optimizing sparse models with low computational overhead_ achieved by performing structured pruning through trainable thresholds. Here, a trainable threshold is defined for each filter/neuron to prune all of its connected parameters. To optimize the pruning process, _only thresholds are communicated_ between clients and the FL server. Hence, clients can learn how to prune their model from global thresholds and can significantly reduce communication costs. Since parameters are not communicated, the clients' parameters and sparse model structures will remain personalized while only global thresholds are shared. We show that global thresholds can capture the aggregated parameter importance of clients. We further update the clients' model parameters by extracting aggregated parameter importance from global thresholds to improve performance. We analyze the generalization ability of SpaFL and provide insights on the relation between sparsity and performance. We summarize our contributions as follows:

* We propose a new communication-efficient FL framework called SpaFL, in which clients optimize their sparse model structures with low computing costs through trainable thresholds.
* We show how SpaFL can significantly reduce communication overhead for both clients and the server by only exchanging thresholds, the number of which is less than two orders of magnitude smaller than the number of model parameters.
* We provide the generalization performance of SpaFL. Moreover, the impact of sharing thresholds on the model performance is theoretically and experimentally analyzed.
* Experimental results demonstrate the performance, computation costs, and communication efficiency of SpaFL compared with both dense and sparse baselines. For instance, the results show that SpaFL uses only 0.17% of communication and 12.0% of computation resources compared to a dense baseline FedAvg while improving accuracy. Additionally, SpaFL improves accuracy by 2.92% compared to a sparse baseline while consuming only 0.35% of this baseline's communication resources, and only 24% of its computing resources.

## 2 Background and Related Work

### Federated Learning

Distributed machine learning has consistently progressed and achieved success. However, it mostly focuses on training with independent and identically distributed (i.i.d.) data [13; 14]. The FL frameworks along with the FedAvg  enables clients to collaboratively train while preserving data privacy without data sharing. Due to privacy constraints and individual preferences, FL clients often collect non-iid data. As such, data can exhibit differences and imbalances in distribution across clients. This variability poses significant challenges in achieving efficient convergence. For a more detailed literature review, we refer to [15; 16]. Although most of state-of-the-art FL methods are effective in mitigating data heterogeneity, they often neglect the computational and communication costs involved in the training process.

### Training and Finding Sparse Models in FL

To reduce the computation and communication overhead of complex ML models during training, the idea of embedding FL algorithms with pruning has recently attracted attention. In [4; 5; 6; 7; 8; 9; 17; 18; 19; 20; 21; 22; 23; 24; 25], the clients train sparse models and communicate sparse model parameters to reduce computation and communication overhead. To improve the aggregation phase with sparse models, the works in [17; 20; 21] perform averaging only between overlapping parameters to avoid information dilution by excluding zero value parameters. The authors in  obtained a sparse model by selecting a particular client to prune an initial dense model and then performed training in a similar way to FedAvg. In [4; 24], the authors presented binary masks adjustment strategy to improve the performance of sparse models and communication efficiency. The work in  progressively pruned a dense model for sparsification and analyzed its convergence. In [19; 22], the clients optimized personalized sparse models by exchanging lottery tickets  at every communication round. The work in  obtained personalized sparse models by \(l_{1}\) norms constraints and the correlation between local and global models. In , the authors proposed dual pruning scheme for both local and global models to reduce the communication costs. The FL framework of  allows clients to train personalized sparse models in a decentralized setting without a central server. Although these works [17; 18; 4; 24; 25; 19; 22; 5; 23] adopted sparse models during training, they used unstructured pruning, which is difficult to improve the computation efficiency in practice. Meanwhile, with structured sparsity, the authors  proposed a training scheme that allows clients to train smaller submodels of a global model. In , clients train set of submodels with fixed channel sparsity patterns depending on their computing capabilities. The work in  studied structured sparsity by adjusting clients' channel activation probabilities. However, the works in [7; 9] fixed sparsity patterns and did not optimize sparse model structures. Although  optimized channel activation probabilities, the communication cost of downlink still remains high as a server broadcasts whole parameters. Similar to our work, in [27; 28], only binary masks are communicated and optimized by training auxiliary variables to learn sparse model structures. However, the work in  approximated binarization step using a sigmoid function during forward propagation. In , the downlink communication costs remained the same as that of FedAvg. In [10; 11; 29], clients perform neural-architecture-search by training multiple models to find optimized and sparse models to improve computational and memory efficiency at inference phase. However, in practice, clients often have limited resources to support the computationally intensive architecture search process . Therefore, most prior works either adopted unstructured pruning or they still required extensive computing and communication costs for finding optimal sparse models. In contrast to the prior art, in the proposed SpaFL framework, we find sparse model structures with structured sparsity by optimizing and communicating trainable thresholds for filter/neurons.

## 3 SpaFL Algorithm

In this section, we first present the proposed pruning scheme for structured sparsity and formulate our FL problem to find optimal sparse models. Then, we present SpaFL to solve the proposed problem with low computation and communication overhead.

### Structured Pruning with Trainable Thresholds

We define a trainable threshold for each neuron in linear layers or for each filter in convolutional layers. The neural network of client \(k\) will consist of \(L\) layers as \(\{^{1}_{k},,^{L}_{k}\}\). For parameters \(^{l}_{k}^{n^{l}_{} n^{l}_{}}\) in a linear layer \(l\), we define trainable thresholds \(^{l}^{n_{}}\) for output neurons. If it

Figure 1: Illustration of SpaFL framework that performs model pruning through thresholds. Only the thresholds are communicated between the server and clients.

is a convolutional layer \(_{k}^{l}^{n_{}^{l} c_{}^{l} k^{l}  h^{l}}\), where \(c_{}^{l}\) is the number of input channels and \(k^{l} h^{l}\) are the kernel sizes, we can change \(_{k}^{l}\) as \(_{k}^{l}^{n_{}^{l} n_{}^{l}}\) with \(n_{}^{l}=c_{}^{l} k^{l} h^{l}\). Similarly, we can define the corresponding thresholds \(^{l}^{n_{}^{l}}\) for filters in that layer. Then, for each client \(k\), we define a set of total thresholds \(=\{^{1},,^{L}\}\). Note that the number of these additional thresholds will be at most 1% of the number of model parameters \(d\).

For threshold \(_{i}^{l}\) of filter/neuron \(i\) in layer \(l\), we compare the average magnitude of its connected parameters \(_{k,i}^{l}=1/n_{}^{l}_{j=1}^{n_{}^{l}}|w_{k,ij}^{l}|\) to its value \(_{i}^{l}\). If \(_{k,i}^{l}<_{i}^{l}\), we prune all connected parameters to this filter/neuron. Hence, our pruning can induce structured sparsity unlike . Thus, we do not need to compute the gradients of parameters in a pruned filter/neuron  during backpropagation. We can obtain a binary mask \(_{k}^{l}\) for \(_{k}^{l}\), as follows

\[p_{k,ij}^{l}=S(_{k,i}-_{i}^{l}),\;1 i n_{}^{l},1 j  n_{}^{l}, \]

where \(S()\) is a unit step function. Hence, we can obtain the binary masks \(\{_{k}^{1},,_{k}^{L}\}\) by performing (1) at each layer. To facilitate the pruning, we constrain the parameters and thresholds to be within \([-1,1]\) and \(\), respectively . For simplicity, we unroll \(\{_{k}^{1},,_{k}^{L}\}\) and \(\{_{k}^{l},,_{k}^{L}\}\) to \(_{k}^{d}\) and \(_{k}^{d}\), respectively as done in . Thresholds represent the importance of their connected parameters (see more details in Section 3.3.1). Hence, clients can know which filter/neuron is important by training thresholds, thereby optimizing sparse model structures. Then, the key question becomes: Can clients benefit by collaborating to optimize shared thresholds in order to find optimal sparse models? We partially answer this question in Table 1. Following the same configurations in Section 5, clients with non-iid datasets only train and communicate thresholds \(\) while freezing model parameters.

We can see that learning sparse structures can improve the performance even without training parameters. This also corroborates the result of . Motivated by this observation, we aim to find optimal sparse models of clients in an FL setting by communicating only thresholds in order to reduce the communication costs in both clients and server sides while keeping parameters locally. The communication cost will decrease drastically because the number of thresholds will be at most 1% of the number of model parameters \(d\). Essentially, we optimize the sparse models of clients with small computing and communication resources by communicating thresholds.

### Problem Formulation

We aim to optimize each client's model parameters and sparse model structures jointly in a personalized FL setting by only communicating thresholds. This can be formulated as the following optimization problem:

\[_{,_{1},,_{N}} _{k=1}^{N}F_{k}(}_{k},),\] s.t. \[F_{k}(}_{k},)=}_{i=1}^{ D_{k}}(_{k}_{k}();\{_{i},y_{i}\}),

### Algorithm Overview

We now describe the proposed algorithm, SpaFL, that can solve (2) while maintaining communication-efficiency with low computational cost. In SpaFL, every client jointly optimizes its personalized sparse model structure and model parameters with trainable thresholds, which can be used to prune filters/neurons. To save communication resources, only thresholds will be aggregated at a server to generate global thresholds for the next round. Here, global thresholds can represent the aggregated parameter importance of clients. Hence, at the beginning of each round, every client extracts the aggregated parameter importance from the global thresholds so as to update its model parameters. The overall algorithm is illustrated in Fig 1. and summarized in Algorithm 1.

#### 3.3.1 Local Training for Parameters and Thresholds

At each round, a server samples a set of clients \(_{t}\) such that \(|_{t}|=K\) for local training. For given global thresholds \((t)\) at round \(t\), client \(k_{t}\) generates a binary mask \(_{k}((t))\) using (1). Subsequently, it obtains the sparse model \(}_{k}(t)=_{k}(t)_{k}((t))\). To improve the communication efficiency, each sampled client performs \(E\) epochs using mini-batch stochastic gradient to update parameters and thresholds as follows:

\[_{k}^{e+1}(t)_{k}^{e}(t)-(t)_{k} (}_{k}^{e}(t)),\;}_{k}^{0}(t)=}_{k}(t), \;0 e E-1, \] \[_{k}^{e+1}(t)_{k}^{e}(t)-(t)_{k}(}_{k}^{e}(t)),\;_{k}^{0}(t)=(t),\;0 e  E-1, \]

where \(_{k}(}_{k}^{e}(t))=_{}_{k}^{e}}F_{k}( {}_{k}^{e}(t),(t);_{k}^{e}(t)),_{k}(}_{ k}^{e}(t))=_{}F_{k}(}_{k}^{e}(t),(t);_{k}^{e }(t))\) with a mini-batch \(\) and \((t)\) is a learning rate. Parameters of unpromed filter/neurons and thresholds will be jointly updated via backpropagation. To enforce sparsity, we add a regularization term \(R(t)\) to (4) in order to penalize small threshold values. To this end, client \(k\) first calculates the following sparsity regularization term \(R(t)=_{l=1}^{L}_{i=1}^{n_{}^{l}}(-_{i})\). Then, the loss function can be rewritten as:

\[F_{k}(}_{k}^{e}(t),(t);_{k}^{e}(t)) F_{k}( }_{k}^{e}(t),(t);_{k}^{e}(t))+ R(t), \]

where \(0 1\) is the coefficient that controls \(R(t)\). From (5), we can give thresholds \((t)\) performance feedback on the current sparse model while also progressively increasing \((t)\) through the sparsity regularization term \(R(t)\). From (5), client \(k\) then updates the received global thresholds \((t)\) via backpropagation as follows

\[_{k}^{e+1}(t)_{k}^{e}(t)-(t)_{k}(}_{k}^{e}(t))+(t)\{-_{k}^{e}(t)\}. \]

After local training, each client \(k_{t}\), transmits the updated thresholds \(_{k}(t)\) to the server. Here, the communication overhead will be less than one percent of that of transmitting the entire parameters. Subsequently, the server performs aggregation and broadcasts new global thresholds, i.e.,

\[(t+1)=_{k_{t}}_{k}(t). \]

Here, in SpaFL, clients communicate only thresholds. Then, what will clients learn from sharing trained thresholds? Next, we show that thresholds represent the importance of their associated filter/neurons.

#### 3.3.2 Learning Parameter Importance From Thresholds

Clients can know which filter/neurons are important by sharing trained thresholds. For the threshold of filter/neuron \(i\) at layer \(l\) of client \(k\), its gradient can be written as below

\[h^{l}_{k,i}(}_{k}^{e}(t)) =(}_{k}^{e}(t))}{_{k,i}^{e,l}( t)}=_{j=1}^{n_{k}^{l}}_{k,ij}^{e,l}(t)}{ _{k,i}^{e,l}(t)}(}_{k}(t),(t))}{ _{k,ij}^{e,l}(t)}=_{j=1}^{n_{k}^{l}}_{k,ij}^{e,l}(t)}{_{k,i}^{e,l}(t)}\{_{k}(}_{k} ^{e}(t))\}_{ij}^{l}\] \[=_{j=1}^{n_{k}^{l}}_{k,ij}^{e,l}(t)}{  Q_{k,i}^{e,l}(t)}^{e,l}(t)}{_{k,i}^ {e,l}(t)}\{_{k}(}_{k}^{e}(t))\}_{ij}^{l}\] \[=_{j=1}^{n_{k}^{l}}^{e,l}(t) p_{k,ij}^{e,l}(t)}{ S(Q_{k,i}^{e,l}(t))}^{e,l}(t))} {Q_{k,i}^{e,l}(t)}^{e,l}(t)}{_{k,i}^{e,l}(t)} \{_{k}(}_{k}^{e}(t))\}_{ij}^{l} \] \[=-_{j=1}^{n_{k}^{l}}\{_{k}(}_{k}^{e}(t))\} _{ij}^{l}w_{k,ij}^{e,l}(t), \]

where \(Q_{k,i}^{e,l}(t)=_{k,i}^{e,l}(t)-_{k,i}^{e,l}(t)\) in (1), (8) is from the definition of pruned parameters in (2) and the unit step function \(S()\), and (9) is from the identity straight-through estimator  to approximate the gradient of the step functions in (8).

From (9), we can see that threshold \(_{k,i}^{e,l}\) corresponds to the importance of its connected parameters \(w_{k,ij}^{e,l},1 j n_{}^{l}\), in its filter/neuron. This is because the importance of a parameter \(w_{ij}^{l}\) can be estimated by 

\[F(,)-F(,;w_{ij}^{l}=0) g()_{ij}^{l}w_{ij} ^{l}, \]

where \(F(,;w_{ij}^{l}=0)\) is the loss function when \(w_{ij}^{l}\) is masked and the approximation is obtained from the first Taylor expansion at \(w_{ij}^{l}=0\). Therefore, if connected parameters were important, the sign of (10) of those parameters will be negative, and the corresponding threshold will decrease as in (9). Otherwise, the threshold will be increased to enforce sparsity. Hence, prematurely pruned parameters will be automatically recovered via a joint optimization of \(\) and \(\).

#### 3.3.3 Extracting Parameter Importance from Global Thresholds

Since thresholds represent the importance of the connected parameters at each filter/neuron, clients can learn how to prune their parameters from the global thresholds. Moreover, the difference between two consecutive global thresholds \((t)=(t+1)-(t)\) captures the history of aggregated parameter importance, which can be further used to improve model performance. For instance, from (10), if \(_{i}^{l}(t)<0\), then the parameters connected to threshold \(i\) in layer \(l\) were globally important. If \(_{i}^{l}(t) 0\), then the connected parameters were globally less important. Hence, from \((t)\), clients can deduce which parameter is globally important or not and further update their model parameters. After generating new global thresholds \((t+1)\), the server broadcasts \((t+1)\) to client \(k_{t+1}\), and then clients calculate \((t)=(t+1)-(t)\).

We then present how clients can update their model parameters from \((t)\). For given \((t)\), we need to decide on the: 1) update direction and 2) update amount. Clients can know the update direction of parameters by considering \((t)\) and the dominant sign of parameters connected to each threshold. For simplicity, assume that each parameter has a threshold. Then, the gradient of the thresholds in (9) can be rewritten as follows:

\[_{k}(}_{k}(t))=-_{k}(}_{k}(t))_{k}( t). \]

The gradient of the loss \(F_{k}(}_{k}(t),(t))\) with respect to the whole parameters \(_{k}(t)\) is given by

\[(}_{k}(t),(t))}{_{k}(t) }=_{k}(}_{k}(t))|_{k}(t)|. \]

From (11) and (12), the gradient direction of a parameter \(w\) is opposite of that of its connected threshold if \(w>0\). Otherwise, both the threshold and the parameter have the same gradient direction.

[MISSING_PAGE_FAIL:7]

_where \(^{}=^{2}}+T)-1}{()+1}\),_

\[^{}=(-+T}{2})()}(}{T-^{}}))^{T}(+^{}}{T-^{}})^ {-+T}{2}}-(1-)})^{T}\] \[+2-(1-())})^{}{T}}(1- )})^{T-}{T}}, \]

\[=(+(M_{g}}+^{2}M_{g}^{2}}{2 ^{2}})) \]

_where \(\) is the size of a mini-batch, \(\) is the variance of Gaussian noise, and \(M_{g}\) is the maximum diameter of thresholds' gradients (11). The proof and the definition of \(\) are provided in the Appendix 1.2 and (12), respectively._

From Theorem 1, we can see that, as the average model density \(\) decreases, the generalization bounds becomes smaller, thereby achieving better generalization performance. This is because \(^{}\) and \(\) decrease as the average model density \(\) decreases. Hence, SpaFL can improve the generalization performance with sparse models by optimizing and sharing global thresholds.

## 5 Experiments

We now present experimental results to demonstrate the performance, computation costs and communication efficiency of SpaFL. Implementation details are provided in the Supplementary document.

### Experiments Configuration

We conduct experiments on three image classification datasets: FMNIST , CIFAR-10, and CIFAR-100  datasets with NVIDIA A100 GPUs. To distribute datasets in a non-iid fashion, we use Dirichlet (0.2) for FMNIST and Dirichlet (0.1) for CIFAR-10 and CIFAR-100 datasets as done in  with \(N=100\) clients. We set the total communication round \(T=500\) and \(1500\) for FMNIST/CIFAR10 and CIFAR100, respectively. At each round, we randomly sample \(K=10\) clients. Unless stated otherwise, we average all the results over at least 10 different random seeds. We also calculate the best accuracy by averaging each client's performance on its test dataset. For FMNIST dataset, we use the Lenet-5-Caffe. For the Lenet model, we set \((t)=0.001\), \(E=5\), \(=0.002\), and a batch size to be 64. For CIFAR-10 dataset, we use a convolutional neural network (CNN) model with seven layers used in  with \((t)=0.01\), \(E=5\), \(=0.00015\), and a batch size of 16. We adopt the ResNet-18 model for CIFAR-100 dataset with \((t)=0.01\), \(E=7\), \(=0.0007\), and a batch size of 64. The learning rate of CIFAR-100 is decayed by \(0.993\) at each communication round.

### Baselines

We compare SpaFL with multiple state of the art baselines that studied sparse model structures in FL. In **FedAvg**, every client trains a global dense model and communicates whole model parameters. **FedPM** trains and communicates a binary mask while freezing model parameters. In **HeteroFL**, each client trains and communicates \(p\)-reduced models, which remove the last \(1-p\) output channels in each layer. In **Fjord**, each client randomly samples a model from a set of \(p\)-reduced models, which drops out \(p\%\) of filter/neurons in each layer. **FedP3** communicates a subset of sparse layers that are pruned by the server for downlink and personalize the remaining layers. Clients only upload the updated remaining layers to the server. **FedSpa** trains personalized sparse models for clients while maintaining fixed model density during training. **Local** only performs local training with the introduced pruning method without any communications. For the sparse FL baselines, the average target sparsity is set to 0.5 following the configurations in [28; 7; 9; 41; 4].

### Main Results

In Table 2 and Fig. 2, we present the averaged accuracy, communication costs, number of FLOPs during training, and convergence rate for each algorithm. We consider all uplink and downlink communications to calculate the communication cost of each algorithm. We also provide the detailsof the FLOPs measure in the Supplementary document. We average the model densities of SpaFL when a model achieved the best accuracy during training. From these results, we observe that SpaFL outperforms all baselines while using the least amount of communication costs and number of FLOPs. The achieved model densities are 35.36%, 30.57%, and 35.38%, for FMNIST, CIFAR-10, and CIFAR-100, respectively. We also observe that SpaFL uses less resources and performs better than FedP3, HetroFL and Fjord, which deployed structured sparse models across clients. For FedP3, clients only upload subset of layers, but the server still needs to send the remaining layers. Although FedPM reduced uplink communication costs by communicating only binary masks, its downlink cost is the same as FedAvg. In SpaFL, since the clients and the server only exchange thresholds, we can significantly reduce the communication costs compared to baselines that exchange the subset of model parameters such as HeteroFL and Fjord. Moreover, SpaFL significantly achieved better performance than Local, which did not communicate trained thresholds. Local achieved 51.2%, 50.1%, and 53.6% model densities for each dataset, respectively. We can see that communicating trained thresholds can make models sparser and achieve better performance. This also corroborates the analysis of Theorem 1. Hence, SpaFL can efficiently improve model performance with small computation and communication costs. In Fig. 2, we show the convergence rate of each algorithm. We can see that the accuracy of SpaFL decreases and then keeps increasing. The initial accuracy drop is from pruning while global thresholds are not trained enough. As thresholds keep being trained and communicated, clients learn how to prune their model, thereby gradually improving the performance with less active filter/neurons.

We provide an empirical comparison between SpaFL and the baseline that does not use the update in Section 3.3.3 in Table. 3. We can see that the update (13) can provide a clear improvement compared to the baseline by extracting parameter importance from global thresholds.

    &  &  &  \\ 
**Algorithms** & **Acc** & **Comm** & **FLOPs** & **Acc** & **Comm** & **FLOPs** & **Acc** & **Comm** & **FLOPs** \\  & & **(Gbit)** & **(e+11)** & & **(Gbit)** & **(e+13)** & & **(Gbit)** & **(e+14)** \\  SpaFL & 89.21\(\)0.25 & **0.1856** & **2.3779** & **69.75\(\)2.81** & **0.4537** & **1.4974** & **40.80\(\)0.54** & **4.6080** & **1.2894** \\ FedAvg & 88.73\(\)0.21 & 133.8 & 10.345 & 61.33\(\)0.15 & 258.36 & 12.382 & 35.51\(\)0.10 & 10712 & 8.7289 \\ FedPM & 63.27\(\) 1.65 & 66.554 & 5.8901 & 52.05\(\) 0.06 & 133.19 & 7.0013 & 28.56 \(\) 0.15 & 5506.1 & 5.423 \\ HeteroFL & 85.97\(\)0.20 & 68.88 & 5.1621 & 66.83\(\)1.15 & 129.178 & 6.1908 & 37.82\(\)0.15 & 5356.4 & 4.3634 \\ Fjord & 89.08\(\)0.17 & 64.21 & 5.1311 & 66.38\(\)2.01 & 128.638 & 6.1428 & 39.13\(\)0.22 & 5251.4 & 4.1274 \\ FedSpa & **89.30\(\)0.20** & 55.256 & 5.2510 & 67.03\(\)0.63 & 129.31 & 4.2978 & 36.32\(\)0.35 & 5342.2 & 9.275 \\ FedP3 & 89.12\(\)0.14 & 41.327 & 5.8923 & 67.54\(\)0.52 & 67.345 & 6.8625 & 37.73\(\)0.42 & 2682.6 & 4.9384 \\ Local & 84.31\(\)0.20 & 0 & 3.7982 & 57.06\(\)1.30 & 0 & 1.9373 & 33.77\(\)1.87 & 0 & 1.5384 \\   

Table 2: Performance of SpaFL and other baselines along with their used communication costs (Comm) and computation (FLOPs) resources during whole training.

   Algorithm & FMNIST & CIFAR-10 & CIFAR-100 \\  SpaFL & **89.21\(\)0.25** & **69.75\(\)2.81** & **40.80\(\)0.54** \\ w.o. (13) & 88.20\(\)1.10 & 68.63\(\)1.76 & 38.96\(\)0.80 \\   

Table 3: Impact of extracting parameter importance from global thresholds

Figure 2: Learning curves on FMNIST, CIFAR-10, and CIFAR-100

In Fig. 3, we show the change of structured sparsity of the first convolutional layer with 64 filters with three input channels on CIFAR-10. We color active filters as black and pruned filters as white. We can see that clients learn common sparse structures across training round. For instance, the 31th and 40th filters are all pruned at round 40. Meanwhile, the 20th filter is recovered at rounds 150 and 500. We can know that SpaFL enables clients to learn optimized sparse model structures by optimizing thresholds. In SpaFL, pruned filter/neurons can be recovered by sharing thresholds. At round 40, filters are pruned with high sparsity. Since premature pruning damages the performance, most filters are recovered at round 150. Then, clients gradually enforce more sparsity to filters along with training rounds as shown in Fig. 2(c).

In Tab. 4, we show the performance of SpaFL on a vision transformer using the ViT  on CIFAR-10 dataset. We used the same data distribution as done in Tab. 2. We apply our pruning scheme to multiheads attention layers. Since a multiheads attention layer essentially consists of stacked linear layers, we can simply use (1), thereby making sparse attention. We can see that SpaFL can be applied to transformer architectures by achieving the density of around 42% while outperforming FedAvg.

## 6 Conclusion

In this paper, we have developed a communication-efficient FL framework SpaFL that allows clients to optimize sparse model structures with low computing costs. We have reduced computational overhead by performing structured pruning through trainable thresholds. To optimize the pruning process, we have communicated only thresholds between clients and a server. We have also presented the parameter update method that can extract parameter importance from global thresholds. Furthermore, we have provided theoretical insights on the generalization performance of SpaFL.

Limitations and Broader ImpactOne limitation of SpaFL is that it cannot explicitly control the sparsity of clients. Since we enforce sparsity through the regularizer term, we need to run multiple experiments to find values for desired sparsity. Another limitation is that our analysis requires a bounded loss function. Meanwhile, in practice, most loss functions may admit bounds that have a large value. For broader impact, SpaFL can reduce not only the computation and communication costs of FL training, but also those of inference phase due to sparsity. Hence, in general, SpaFL can improve the sustainability of FL deployments, and more broadly, of AI.

  Algorithm & Accuracy [\%] & Density [\%] \\  SpaFL & **69.78\(\)2.62** & **42.2\(\)4.8** \\ FedAvg & 59.20\(\)0.4 & 100 \\  

Table 4: Performance of SpaFL with the ViT architecture on CIFAR-10

Figure 3: Sparsity pattern of conv1 layer on CIFAR-10