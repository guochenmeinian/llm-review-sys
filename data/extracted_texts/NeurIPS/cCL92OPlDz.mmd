# ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with Attention Grouping

**Mingzhen Huang, Jialing Cai, Shan Jia, Vishnu Suresh Lokhande\({}^{*}\), Siwei Lyu\({}^{*}\)**

University at Buffalo, State University of New York, USA

Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. A major challenge is making simultaneous edits across multiple objects or attributes. Applying these methods sequentially for multi-aspect edits increases computational demands and efficiency losses. In this paper, we address these challenges with significant contributions. Our main contribution is the development of ParallelEdits, a method that seamlessly manages simultaneous edits across multiple attributes. In contrast to previous approaches, ParallelEdits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads. Additionally, we introduce the PIE-Bench++ dataset, an expansion of the original PIE-Bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. This dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios. Codes are available at: https://mingzhenhuang.github.io/projects/ParallelEdits.html.

Figure 1: **Multi-aspect text-driven image editing. Multiple edits in images pose a significant challenge in existing models (such as DirectInversion  and InfEdit ), as their performance downgrades with an increasing number of aspects. In contrast, our ParallelEdits can achieve precise multi-aspect image editing in 5 seconds. The symbol \(\) denotes a swap action, the symbol \(\) denotes an object addition action, and the symbol \(\) denotes an object deletion. Arrows (\(\)) on the image highlight the aspects edited by our method.**Introduction

Recently, text-driven image editing has experienced remarkable growth, driven by advances in diffusion-based image generative models. This technique involves modifying existing images based on textual prompts to alter objects, their attributes, and the relationships among various objects. The latest methods [3; 1; 4] can produce edited images that closely match the semantic content described in the prompts while keeping the rest of the image unchanged. Unlike early image editing approaches that required image matting to precisely extract foreground objects using alpha mattes , text-driven editing offers a less labor-intensive alternative. User-provided textual prompts guide the edits, with auxiliary inputs like masks facilitating localized modifications .

While these methods have showcased promising results, existing methods typically focus on editing a single aspect in the source image. An "aspect" refers to a specific attribute or entity within the textual prompt that describes the image and can be modified, such as object type, color, material, pose, or relationship. However, the ability to edit multiple aspects through text prompts is rarely explored. We introduce the concept of _multi-aspect text-driven image editing_ to address this gap. Multi-aspect image editing is essential due to the rich and diverse content and structure of digital images, as well as the varied requirements of users. For example, it always occurs that users wish to modify multiple attributes or regions in an image, such as adding a necktie to a cat and changing the background wall to a beach (Fig. 1, Left), or removing a man and replacing a mountain with a castle in the right example. Unlike traditional editing methods (e.g., [1; 2]) that focus on a single aspect, multi-aspect editing allows users to manipulate various aspects simultaneously. Different from full text-to-image synthesis [7; 8], which involves creating content from scratch, multi-aspect editing works with the source image to ensure essential content preservation. It bridges the gap between single-aspect editing and full synthesis, catering to a wide range of editing scenarios.

However, we observe that directly applying the single-aspect text-driven image editing methods in cases where multiple image aspects must be modified often does not yield satisfactory results. A straightforward solution to this problem is to apply the single aspect editing method _sequentially_ - we can order the aspects to be modified and use a single-aspect editing method to change the aspects one by one. Although sequential applications of single-aspect text-driven image editing methods can modify multiple aspects of an image, they may introduce significantly higher computational overhead. More importantly, the order of the aspects modified may affect the quality - changes to later aspects may undo the early ones or accumulate the errors and artifacts, thus reducing the effectiveness of the final editing results, as the last two rows of Fig. 5 and Table 1 show.

In this work, we introduce _ParallelEdits_ as an efficient and effective solution to the problem of multi-aspect text-driven image editing. This method is based on a crucial insight that the editing step can occur in parallel with the image's diffusion steps. Therefore, in ParallelEdits, we build image aspect editing into the diffusion steps to accelerate the editing process. ParallelEdits is based on an architecture with a fixed number of additional branches dedicated to handling rigid, non-rigid, and style changes. This design ensures scalability independent of the number of prompt aspects altered. In addition, we employ an attention aggregator to accurately assess editing difficulty and route aspects to appropriate branches within the ParallelEdits framework, ensuring precise and efficient editing. To enable subsequent research and evaluation of multi-aspect text-driven image editing methods, we also build the PIE-Bench++ dataset, an extension of the PIE-Bench  that has \(700\) images with detailed text prompts and tailored to facilitate simultaneous edits across multiple image aspects. We propose evaluation metrics and benchmark different text-driven image editing methods on PIE-Bench++. The ParallelEdits outperforms the state-of-the-art image editing methods on PIE-Bench++.

## 2 Related Works

**Diffusion Models for Text-Driven Image Editing**. Text-driven image editing aims to manipulate local regions of an image based on textual prompts. The editing has two main goals: ensuring the edits align with provided instructions and preserving essential content. Diffusion models  have gained popularity as a preferred image editing model for their capacity for generating high-quality samples by incorporating diverse conditions, especially using text [10; 11; 12; 13; 14; 1; 2]. This involves transforming the images into the latent space and generating regions using diffusion models conditioned by the text prompt while ensuring accurate reconstruction of unmodified regionsduring editing. To avoid the edited image deviating from original image, early text-driven image editing typically requires user-specified masks as additional condition [15; 16; 17] or training [18; 19; 20] to guided the editing process, which constrain their potential zero-shot application. To address this limitation, recent editing models, such as InfEdit , PnP , Direct Inversion  follow the work Prompt-to-Prompt (P2P) , which proposed to obtain an attention map from the cross attention process and either swap or refine the attention map from text prompt for image editing. This design automatically obtains the editing mask and only allows image editing using a text prompt. Another method, MasaCtrl , converts existing self-attention in diffusion models into mutual self-attention for non-rigid consistent image synthesis and editing, enabling to query correlated local contents and textures from source images for consistency.

**Multi-Aspect Image Editing**. While current image editing models have shown promising results in their text-driven image editing benchmarks, we have observed that they work well on single-attribute editing while struggling to edit multiple aspects, especially when editing multiple objects (as shown in Fig. 1). We attribute this limitation to the following reasons. First, existing methods use the attention mask to direct where edits should be made. With multiple attributes, the editing area may expand significantly, incorporating extensive semantic information or scattered regions that are challenging to edit using a single mask. Second, employing a fixed mask from cross-attention maps struggles with edits involving changes in region size (such as pose adjustments), while using an adaptive mask faces challenges in maintaining edit fidelity. Therefore, integrating various attention masks for accurate multi-attribute editing presents a challenging technical problem. Early studies [22; 23] have employed GAN models such as StyleGAN2  to edit multiple attributes in faces. The multiple-attribute editing is realized by training the GAN model with supervised multi-class training and a training dataset of image and attribute vector pairs. This solution heavily relies on the training sets and has limitations in generalizing to new editing types. Few recent works achieve multi-aspect editing with additional inputs:  leverages rich text to edit multiple objects and  pre-processes the image with grounding to localize multiple edited regions for multi-aspect editing. However, the editing performance highly relies on additional input beyond plain text, either from user input or other off-the-shelf models. A recent work  proposes an iterative multi-granular image editor, where a diffusion model can faithfully follow a series of image editing instructions from a user. However, this interactive editing pipeline will result in significant computational overhead.

**Image Editing with Multiple Branches.** In the literature [4; 3], image editing processes have been conducted by implementing a dual-branch approach. This methodology involves segregating source and target branches throughout the editing process. Specifically, the source branch is reverted to \(z_{0}\), while the trajectory of the target branch is iteratively adjusted. By computing the distance from the source branch, the calibration of the target branch occurs at each time-step. Our observation underscores the disparity between the effectiveness of a dual branch in enhancing the editing process and its failure in multi-aspect editing. A singular target branch proves inadequate in calibrating fully from the source branch, leading to imperfect incorporation of all aspects into the image. Hence, our primary proposition advocates for multi-aspect editing by utilizing multiple target branches. Each target branch's trajectory is meticulously calibrated, with simpler concepts addressed in the initial branches and more complex aspects deferred to subsequent ones. In the following section, we will delve deeper into this concept.

## 3 Diffusion-based Image Generation and Editing

We are provided with an image sample \(x_{0}\) which transforms the latent space via an encoder/decoder pair \(/\), such that \(z_{0}=(x_{0})\). Here, \(z_{0}\) represents the latent representation of the image \(x_{0}\). With a slight abuse of notation, we approximate the reconstructed image \(_{0}\) as \((_{0})\), where \(_{0}\) denotes the reconstructed version of \(z_{0}\). These operations are integral to the latent diffusion model . The diffusion process constitutes two steps: the forward step incrementally adds zero-mean white Gaussian noise with time-varying variance to the latent vector \(z\) according to discrete-time \(t^{*}\),

\[z_{t}=}z_{0}+} (0,I),\] (1)

\(_{1:T}\) represents a variance schedule for \(t\) drawn from the interval \([1,T]\). The variance schedule can be different, such as linear or cosine quadratic . The backward step is an iterative process to remove the noise from the data progressively. Using the same variance schedule \(_{1:T}\) as in the forward step, a noise schedule \(_{1:T}\) and a parameterized noise prediction network \(_{}\) with coefficients \(c_{}=}\), \(c_{}=-_{t}^{2}}\), and \(c_{}=_{t}\), the backward step corresponds to the following process:

\[z_{t-1}=}f_{}(z_{t},t)}_{_{0}}+}_{}(z_{t},t)}_{z_{t}}+}_{t}}_{} _{t}(0,I)\] (2)

The noise schedule \(_{1:T}\) comprises hyperparameters requiring careful selection based on factors like image dimensions or desired performance . In the framework of Denoising Diffusion Implicit Models (DDIM) , the function \(f_{}\) is employed for the prediction and reconstruction of \(_{0}\), based on the input \(z_{t}\). Specifically, we have \(_{0}=f_{}(z_{t},t)=}}z_{t}-}}{}}_{}(z_{t},t)\).

**Consistency Models for Inversion-free Image Editing**. Consistency models [32; 33] have been introduced to expedite the generation process through a consistent distillation approach. These models exhibit a self-consistency property, ensuring that samples along the same trajectory map to the same initial point. Specifically, the function \(f_{}\) is rendered self-consistent by satisfying \(f_{}(z_{t},t)=z_{0}\) for a given sample \(z_{t}\) at timestep \(t\). As a result, the self-consistency property yields a closed-form solution for the noise predictor \(_{}\). We denote this particular \(_{}\) as \(^{}\), which is derived as \(^{}=-}z_{0}}{}}\). Since \(^{}\) is not parameterized and contains the ground-truth \(z_{0}\), Xu _et al._ propose starting directly with random noise, i.e., \(z_{T}(0,)\), at the last time-step \(T\), which is particularly advantageous for image-editing tasks as it eliminates the need for inversion from \(z_{0}\) to \(z_{T}\). Therefore, starting with \(z_{}=z_{T}(0,)\), the sampling process proceeds as follows:

1\(z=-}^{}_{}}{}}\). Where, \(^{}_{}\) is given by \(-}z_{0}}{}}\)
2Noise is added to \(z_{}\), i.e, \(z_{}=}z+}\) where \((0,)\)

After many iterations, the final output is \(z\). Furthermore,  demonstrates that the dual-branch paradigm (involving a source and a target branch) used in image editing tasks can be executed in an inversion-free manner. We will delve into this, along with our method description, in Section 4.2.2.

## 4 Multi-Aspect Image Editing

### Problem Definition

The input to the multi-aspect image editing task includes a source image (\(_{src}\)), the source prompt, and a set of edits to be applied to the source image, indicating the changes from the source prompt to target prompt. A text prompt (whether source or target) comprises several independent tokens, of which only a subset is editable. We refer to these editable tokens as _Aspects_.

**Definition 4.1** (Aspect).: We define an \(i^{}\) aspect \(^{i}_{src}\) in the source prompt (or the \(j^{}\) aspect \(^{j}_{edt}\) in the target prompt) as any entity that can be substituted, deleted, or inserted into the text prompt, resulting in a meaningful sentence structure.

Several examples of tokens corresponding to aspects or not are given in Fig. 3. In other words, aspects correspond to single or multiple tokens representing object color, pose, material, content, background, image style, etc. An editing operation \(E^{i j}\) between the editing pair \((^{i}_{src},^{j}_{edt})\) as \(E^{i j}\{,,,\}\). Here, \(\) denotes a swap action, \(\) denotes an object addition action, \(\) denotes object deletion, and \(\) indicates no change in the aspect. Such an editing operation can be inferred directly by appropriately mapping the source and target prompts, or it can be provided as metadata [3; 34]. The editing task is considered successful if the edited source image, \(_{edt}\), reflects the required edits while preserving the unaffected aspects of the original image.

### Method

Figure 2 outlines the overall pipeline of our method, which has three steps. In the first step (Sec. 4.2.1), we perform _aspect grouping_ using attention maps generated by running a few iterations of the diffusion process. The aspects in the source image are put into up to \(N\) groups, each processed by a distinct branch. The second step (Sec. 4.2.2) demonstrates how each branch, which receives a specific group of aspects, performs inversion-free editing. In the last step (Sec. 4.2.3), we perform the necessary adjustments for enabling cross-branch interaction and elucidate the significance of such interaction.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

deletion, or alteration), attribute-level manipulations (changes in content, pose, color, and material), and image-level manipulations that modify background and overall style. Our PIE-Bench++ extends PIE-Bench by enabling multi-aspect edits: 57% of our dataset have two aspect edits per prompt, 19% have more than two edits, and the remaining 24% have a signle aspect edit. For additional details and examples of the PIE-Bench++ dataset, please refer to the supplementary material.

**Evaluation Metrics**. We introduce two new metrics designed for evaluating multi-aspect text-driven image editing, alongside standard evaluation metrics.

**(a) Aspect Accuracy-LLAVA**. Drawing inspiration from the remarkable capability of large vision language models in comprehending intricate semantics within images, we propose to innovatively leverage them as an "omniscient" agent equipped with extensive knowledge to understand various attributes of images. We use the LLAVA  model, trained on visual grounding tasks, to evaluate the accuracy of multi-aspect image editing. Given a text prompt with multiple aspects, such as "_A [pink] [taxi] with [colorful] [flowers] on top_", we provide the following prompt with the edited image to the LLAVA model: "_Does the image match the elements in [ ]: A [pink] [taxi] with [colorful] [flowers] on top? Return a list of numbers where 1 is matched and 0 is unmatched_." We then parse the returned list and compute its average to determine the aspect accuracy. We name this new evaluation metric as _AspAcc-LLAVA_. Examples and detailed explanations of this evaluation metric are available in the supplementary material.

**(b) Aspect Accuracy-CLIP**. We also use the similarity of the CLIP  to evaluate if an attribute has been successfully edited. Given an edited image \(_{edt}\) and the target prompt \(_{edt}\) with \(k\) edited aspects \(_{edt}\), every time we remove an aspect \(^{j}_{edt}\) from \(_{edt}\) and revert it back to \(^{i}_{src}\) as \(}_{edt}\). We then extract the CLIP  similarity between the edited image \(I_{edt}\) and two prompts, i.e., \(s_{1}=CLIP(_{edt},_{edt})\) and \(s_{2}=CLIP(_{edt},}_{edt})\). We expect \(s_{1}>s_{2}\) if the aspect \(^{j}_{edt}\) has been successfully edited. Thus, the aspect accuracy is \(}{k}\) when a total of \(k_{s}\) aspects have been successfully edited among \(k\) target edits. Note that in the case of an edited or added object that also involves changes in attributes (such as color or material), we consider it a successful edit only if both the object and its attributes have been successfully modified. We name this metric as _AspAcc-CLIP_.

**(c) Standard Metrics**. Several standard metrics widely used for evaluating text-image similarity and image quality are considered, including PSNR, LPIPS , MSE, and SSIM . We also use the CLIP  score to measure the image-text alignment performance. Additionally, the bi-directional CLIP (D-CLIP) score  is reported, which is formulated as follows:

\[}(_{edt})-}(_{src}),}(_{edt} )-}(_{src})\]

### Quantitative Results

We first conduct experiment on the PIE-Bench++ dataset to compare our method with the state-of-the-art text-driven image editing methods combining their corresponding inversion method leads to best

Figure 4: **Qualitative results of ParallelEdits. We denote the edits in arrows with edit actions and aspects for each pair of images. The last image pair is a failure case of ParallelEdits.**

performance, including DDIM+MasaCtrl , DDIM+Prompt-to-Prompt (P2P) , DDIM+Plug-and-Play (PnP) , StyleDiffusion (StyleD) +P2P, Null-text Inversion (NTI) +P2P, DirectInversion (DI)+PnP, and InfEdit . An intuitive way to improve off-the-shelf image editing methods is to apply the single-aspect editing method sequentially. We follow  to adapt existing image editing methods into sequential editing processes, where these methods are applied multiple times to achieve multi-aspect editing. Each time, only one aspect is edited. Table 1 presents the metrics in terms of text-image similarity (i.e., CILP and D-CLIP scores), computational efficiency, and aspect accuracy. Our ParallelEdiits model outperforms all baselines in editing effectiveness, with a slightly longer runtime than the InfEdit model. Even though sequential editing better aligns the target prompt than their vanilla methods, it significantly increases computational overhead and may propagate editing errors over time. Moreover, although the sequential editing is conducted in the latent space, it would introduce more noise and artifacts to the edited image. Hence, their performance in all editing quality metrics was inferior to our method.

### Qualitative Results

Fig. 4 presents several examples of our method's multi-aspect editing on the PIE-Bench++ dataset. The results demonstrate the effectiveness of our method in handling multiple and varied types of

  & **StyleD** & **MasaCtrl** & **P2P** & **DI** & **NTI** & **InfEdit** & **PnP** & **DI*** & **P2P*** & **InfEdit*** & **PnP*** & **Ours** \\   CLIP (\%) \(\) & 24.02 & 23.37 & 24.00 & 24.40 & 24.03 & 24.44 & 24.90 & 22.80 & 25.13 & 25.17 & 25.39 & **25.70** \\ D-CLIP (\%) \(\) & 8.43 & 7.68 & 11.43 & 13.23 & 12.08 & 11.02 & 11.83 & 2.74 & 8.30 & 11.77 & 11.85 & **20.70** \\ Eff. (secs/sample) \(\) & 382.98 & 12.70 & 33.72 & 29.70 & 145.29 & **2.22** & 32.51 & 100.98 & 121.32 & 11.82 & 122.81 & 4.98 \\ AspAcc-CLIP (\%) \(\) & 32.37 & 34.05 & 26.14 & 31.95 & 42.19 & 42.38 & 44.91 & 28.23 & 38.96 & 42.38 & 48.20 & **51.05** \\ AspAcc-LLAv (\%)\(\) & 53.79 & 55.79 & 55.04 & 54.42 & 59.80 & 60.55 & 61.36 & 46.24 & 55.21 & 61.90 & 63.80 & **65.19** \\   

Table 1: **Comparison results in multi-aspect image editing on the PIE-Bench++ dataset.** Computational efficiency is abbreviated as Eff., and * denotes the method using sequential editing. The best performance is highlighted in **bold** and the second best performance is underlined.

Figure 5: **Qualitative results comparison.** Current methods fail to edit multiple aspects effectively, even using sequential edits (noted as *). Methods marked with \(\) taking additional inputs other than source image and plain text.

edits across diverse image content. Fig. 5 further compares our method with several state-of-the-art models and one popular multi-modal large language model, GPT-4V , by providing the source image, source prompt, and target prompt to guide the image editing. The Rich-text  model differs from other models, which uses rich-text prompt to edit the image generated from the plain (source) text prompt. The results show that current image editing models even with sequential editing fail to edit multiple aspects, while multi-modal large language models fail to preserve the content of source image. Our method achieves visually convincing results by successfully editing different attributes with good content preservation.

### Ablation Study and Analysis

**(a) Impact of Editing Aspect Number.** We first examine the performance of our ParallelEdits and baseline methods on various editing aspect numbers by comparing CLIP and LLaVA-based aspect accuracies on the original PIE-Bench  and our PIE-Bench++ datasets. The bar charts in Fig. 6 show the outstanding performance of our method across all settings, including single-aspect editing on two datasets and multi-aspect editing. _Takeaway: the proposed ParallelEdits demonstrates robustness across varying numbers of editing aspects_.

**(b) Evaluation on Preservation.** We follow  to evaluate the background preservation. We first use the PSNR, LPIPS , MSE and SSIM  to evaluate the background preservation. We measure that metric on a subset of images of our proposed PIE-Bench++ dataset where the background can be well defined in that image, e.g., no image style or background editing, and the background is visible after aspect editing. The results are shown in Table 2, where we compare our method with the top performance methods in Table 1. Moreover, we adopt the similar way as calculating the AspAcc-LLaVA to prompt LLaVA  for evaluating how the unchanged aspect preserves in the edited image. We also calculate the CLIP  score between the target image and the text prompt after removing all edited aspects. The results are reported in Table 2 noted as CLIP and LLaVA, respectively. _Takeaway: preservation is even maintained in ParallelEdits_.

**(c) Branches numbers and aspect grouping.** To demonstrate the effectiveness of our multi-branch design and early aspect grouping, we design additional ablation studies for our method in threefold. (1) We only use one single non-rigid branch to conduct all edits; (2) we remove the aspect categorization process from the pipeline and use the same non-rigid branch for each edit; (3) we adopt one single branch for different type of edits without using any auxillary branches which results a total of three branches (also see Section B for more details). _Takeaway: As shown in Table 3, the multi-branch design and aspect grouping play a significant role in enhancing the performance of our proposed ParallelEdits_.

  &  &  \\ Methods & PSNR\(\) & LPIPS\(_{103}\)\(\) & MSE\(_{104}\)\(\) & SSIM\(_{102}\)\(\) & CLIP\(\) & LLaVA \(\) \\   P2P  & 18.48 / 16.64 & 188.26 / 231.83 & 190.07 / 345.07 & 73.55 / 69.17 & 20.72 / 23.48 & 66.59 / 72.60 \\ PnP  & 22.73 / 21.54 & 103.16 / 120.87 & **75.97** / 102.47 & 80.73 / 78.85 & 20.79 / **25.59** & 75.65 / 78.77 \\ InfEdit  & 24.61 / 24.09 & 103.99 / 107.43 & 160.54 / 163.72 & 78.85 / 79.64 & 24.69 / 25.04 & 75.90 / 78.05 \\ Ours & **26.13** & **95.87** & 113.86 & **82.35** & 25.49 & **80.70** \\   

Table 2: **Comparison results in terms of background and aspects preservation.** The results from sequential editing is noted as green. ParallelEdits achieves state-of-the-art performance on multi-aspect editing while preserving the background and content consistency.

Figure 6: **Comparison across different numbers of editing aspects.** We also include the comparison in PIE-Bench dataset. Our proposed method is robust to different numbers of editing aspects.

**(d) Performance comparison on each category.** Recall that our dataset includes nine different categories for editing. We compare the performance of baseline models and our approach across the nine categories, as presented in Table 4. _Takeaway: Our proposed ParallelEdits achieves state-of-the-art performance across most categories._

**Limitations and Failure Cases.** The proposed ParallelEdits has several limitations. First, it cannot handle the text editing in the image, as shown in the last image pair of Fig. 4. Second, ParallelEdits fails to edit dramatic background changes, as examples shown in the supplementary material.

## 6 Conclusion

In this work, we propose a new research task, multi-aspect text-driven image editing, to modify multiple object types, attributes, and relationships. We introduce a dedicated method, ParallelEdits, to multi-aspect text-driven image editing as an effective and efficient solution to this problem. Due to the lack of evaluation benchmark, we introduce PIE-Bench++, an improved version of PIE-Bench  tailored for simultaneous multiple-aspect edits within images. ParallelEdits achieves better quality and performance than existing methods on proposed PIE-Bench++. Our work introduces ParallelEdits, a novel approach that adeptly handles multiple attribute edits simultaneously, preserving the quality of edits across single and multiple attributes through a unique attention grouping mechanism without adding computational complexity. There are several future works we would like to explore. First, different aspects of an image have a specific semantic order. Editing these aspects according to their intrinsic order will simplify the editing process. Secondly, the current ParallelEdits still has limitations, as shown in Fig. 4. It will be of interest to study approaches to improve these aspects.

**Ethics Statement**. In anticipation of contributing to the academic community, we plan to make the dataset and associated code publicly available for research. Nonetheless, we acknowledge the potential for misuse, particularly by those aiming to generate misinformation using our methodology. We will release our code under an open-source license with explicit stipulations to mitigate this risk. These conditions will prohibit the distribution of harmful, offensive, or dehumanizing content or negatively representing individuals, their environments, cultures, religions, and so forth through the use of our model weights.

**Aspect Acc-CLIP** & Object & Content & Pose & Color & Material & Background & Style & Object & Object \\   P2P  & 33.13 & 20.00 & 25.83 & 34.17 & 31.67 & 30.63 & 19.38 & 22.29 & 11.88 \\ MasaCtrl  & 40.83 & 23.75 & **40.83** & 20.00 & 30.83 & 26.88 & 29.38 & 37.08 & 28.96 \\ NTI  & 48.13 & 41.25 & 23.75 & 51.25 & 24.17 & 51.25 & 22.50 & 40.42 & 32.08 \\ DirectInversion  & 40.63 & 26.25 & 23.33 & 40.00 & 25.42 & 32.50 & 25.00 & 30.00 & 20.83 \\ InfEdit  & 36.24 & 33.33 & 25.41 & 41.67 & 27.50 & 48.75 & 41.88 & 50.63 & 45.41 \\ PnP  & 44.38 & 27.29 & 27.91 & 49.17 & 32.91 & 52.50 & **55.63** & 44.38 & 42.08 \\  ParallelEdits & **51.46** & **44.16** & 39.58 & **60.00** & **47.50** & **60.00** & 50.00 & **56.04** & **52.08** \\   

Table 4: **Comparison on each category in PIE-Bench++. Our ParallelEdits achieves the best performance on most of the categories from the dataset.**

  &  &  &  &  &  \\  & categorization & grouping & branch & CLIP\(\) & D-CLIP\(\) & CLIP\(\) & LLLAVA \(\) \\    & \(\) & \(\) & \(\) & 24.32 & 10.45 & 40.97 & 57.67 \\ ParallelEdits & \(\) & \(\) & \(\) & 25.14 & 11.97 & 46.66 & 58.37 \\  & \(\) & \(\) & \(\) & 24.50 & 12.33 & 48.08 & 61.22 \\  & \(\) & \(\) & \(\) & **25.70** & **20.70** & **51.05** & **65.19** \\   

Table 3: **Ablation studies on branch numbers and aspect grouping.**