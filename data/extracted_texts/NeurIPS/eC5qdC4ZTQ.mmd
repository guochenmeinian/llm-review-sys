# Unlock the Intermittent Control Ability of Model Free Reinforcement Learning

Jiashun Liu, Jianye Hao1& Xiaotian Hao

College of Intelligence and Computing

Tianjin University

China

&Yi Ma

School of Computer and Information Technology

Shanxi University

China

&YAN ZHENG

Tianjin University

China

yanzheng@tju.edu.cn

&Yujing Hu & Tangji Lv

FUXI AI Laboratory

NetEase

China

Correspondence to: Jianye Hao <jiangye.hao@tju.edu.cn>

###### Abstract

Intermittent control problems are common in real world. The interactions between the decision maker and the executor can be discontinuous (intermittent) due to various types of interruptions, e.g. unstable communication channel. Due to intermittent interaction, agents are unable to acquire the state sent by the executor and cannot transmit actions to the executor within a period of time step, i.e. bidirectional blockage, which may lead to inefficiencies of reinforcement learning policies and prevent the executors from completing the task. Such problem is not well studied in the RL community. In this paper, we model Intermittent Control Problem as an Intermittent Control Markov Decision Process, i.e., agents are expected to generate action sequences corresponding to the unavailable states and transmit them before disabling interactions to ensure the smooth and effective motion of executors. However, directly generating multiple future actions in the original action space has unnatural motion issue and exploration difficulty. We propose **M**ulti-step **A**ction **R**epre**S**entation (**MARS**), which encodes a sequence of actions from the original action space to a compact and decodable latent space. Then based on the latent action sequence representation, the mainstream RL methods can be easily optimized to learn a smooth and efficient motion policy. Extensive experiments on simulation tasks and real-world robotic grasping tasks show that MARS significantly improves the learning efficiency and final performances compared with existing baselines.

## 1 Introduction

In recent years, the field of **d**eep reinforcement learning (DRL) has witnessed striking empirical achievements in a variety of Markov Decision Process (MDP) problems (Mnih et al., 2013; Kaufmann et al., 2023) and has been successfully applied to many real-time control tasks (Mahmood et al., 2018; Lee et al., 2020). In real-time control, "interaction" plays a crucial role. At each time step, the decision maker obtains observations from the environment and feeds actions back to the action executor through real-time interactions. Thus, in an ideal MDP setting, the interaction should be continuous. However, in many scenarios, interactions become intermittent due to limitations of realistic conditions (Jiang et al., 2021) or communication interruption (e.g., unstable network) (Dong et al., 2009). Due tobidirectional communication blockage caused by intermittent interaction, agents (decision makers) are unable to acquire the state sent by the executor and cannot transmit actions to the executor within a period of time step (as shown in Figure 1 (top). The sparse observations resulting from this phenomenon can make normal step-by-step decisions unstable. Thus, directly deploying existing DRL algorithms could make the action executor abruptly stop (can not make decision when the current state is unavailable) or move abnormally when the environment suddenly changes (Sutton and Barto, 2018).

Typically, there are two types of intermittent interactions. Illustrations are shown in Figure 1(bottom).

**Fixed interaction interval**: in numerous real-world robot scenarios, interacting with the environment is often time-consuming and costly (Liu et al., 2020). For example, before the robotic arm performs its next actions, it has to first halt its current operations, move to a designated position to capture an image, and then use a specific technique to extract features and provide them to the decision maker. To ensure reliable operation and stable movement, we usually set a fixed interaction interval for the robot arm (Jiang et al., 2021; Bonarini, 2020).

**Random interaction interval**: unexpected interaction intervals may occur due to unstable communication channels and loss of wireless signals (Li et al., 2016). For example, in real-time strategy games, the decision end remotely controls the terminal non-player characters (NPC) (Zheng et al., 2019), the interaction interval between the decision end and NPC terminals may become random (Wong et al., 2021) due to above reasons, which may cause the NPC to be stuck and disconnected from the changed environment, reducing the player's experience.

The essence of addressing the two aforementioned intermittent control tasks is identical, that is, to achieve intensive control under sparse interaction (or observation) to ensure the effective and smooth movement of the executor, ultimately leading to efficient task completion. To this end, we introduce Intermittent-MDP (refer to Sec.3 for further elaboration) to model the above two settings in a unified manner, that is, the agent is expected to decide on a sequence of actions based on the current state, covering a suitable number of time steps, to maintain smooth and efficient motion of the executor between the two interactions. The most direct approach would be to employ model-based reinforcement learning (MBRL) methods with multi-step decision-making capabilities, such as Dreamer (Hafner et al., 2023) and TD-MPC (Hansen et al., 2022). Regrettably, the form of multi-step decision makes the error of the dynamic model accumulate and then make the policies suboptimal. In addition, the high demand for high-quality data and the complexity of constructing dynamic models makes MBRL deployment costly in real-world scenarios (Janner et al., 2019) (detailed experimental analysis in Appendix C.1). Instead, we sought to unlock the multi-step decision-making ability of the model-free DRL approach, e.g. TD3 (Fujimoto et al., 2018) and PPO (Schulman et al., 2017).

The most simple method for model-free DRL to alleviate the intermittent interaction issue is using frameskip (also commonly known as 'action-repeat') (Kalyanakrishnan et al., 2021), where the same action (usually the last action) is repeated during a fixed interval which is often used in Atari (Braylan et al., 2015). However, longer frameskip will lead to mechanized motion, making it impossible for the agent to change actions at key states and thus resulting in suboptimal policies. Another way is to let the RL algorithms make up-front decisions (advance decisions) for future steps according to the current state or the received delayed state. Only actions for the correct time steps will be executed. Compared to frameskip, these methods can improve action diversity. However, directly increasing the horizon of decision making will lead to action space explosion and increase the difficulty of policy optimization (Chen et al., 2021).

In this paper, we propose Multi-step Action RepreSentation (MARS), which is the first plugin method for DRL algorithms to solve intermittent control tasks, significantly reducing the difficulty of multi-step policy training while ensuring the flexibility and diversity. The high level idea is shown in Figure 2. MARS constructs a compact and decodable low dimensional latent action

Figure 1: Detail of Intermittent Control (top part). Two types of intermittent interactions (bottom part). For both cases, we can generate a sequence of actions in advance for the next \(c\) states based on the current state \(s_{t}\) to make the control more smooth and robust.

space for the original multi-step actions, the latent action dimension does not explode as the steps number increases. Based on this latent action space agent can learn a stable latent policy. The latent action selected by the policy can be reconstructed into original multi-step actions by the decoder. Specifically, MARS relies on a conditional Variational Auto-encoder (c-VAE) (Sohn et al., 2015) to construct the latent representation space for the associated multi-step actions. Intuitively, an efficient latent action space should have two characteristics: \(\)**Decision space simplification**: the combination number of long-sequence actions is huge, especially in the continuous action space, which can complicate the decision space and potentially result in suboptimal policies. We restrict the decision space to a relatively small subspace by introducing the concept of action transfer scale \(\) and taking it as the condition term of our VAE, i.e. using \(\) to implicit segment the latent space (Sohn et al., 2015). \(\)**Dynamic semantic smoothness**, a characteristic of the original action space, which means latent actions that have similar impacts on the environment should be close together in latent spaces. Dynamic semantic smooth action space can retain the utility of adding Gaussian noise perturbations to the policy during training: While ensuring that the impacts of initial latent action chosen by the policy and latent action after disturbance on the environment is similar in high probability, some uncertainty is also retained that the disturbed action may explore rare actions that have unknown impacts on the environment, which helps maintain a balance between exploration and exploitation (Schwarzer et al., 2020). We utilize unsupervised environmental dynamics to learn a predictive representation of dynamics for improving semantic smoothness. In practice, we adding an additional dynamic residual prediction module for VAE.

Our contributions are summarized as follows: \(}\) We propose the first plugin method and DRL framework for solving intermittent control tasks via multi-step action representation learning. \(}\) We provide two modules to improve the effectiveness and smoothness of the learned multi-step latent space. \(}\) Our method outperforms baselines on both real-world fixed interaction interval robotic control tasks and random interaction interval simulation control tasks.

## 2 Background

**Markov Decision Process (MDP).** A MDP can be represented as a tuple: \((,,,,,T)\), where \(\) denotes the state set, \(\) denotes an action set, \(\) is the transition function: \(\) and \(\) is the reward function: \(\). \([0,1)\) is a discount factor and \(T\) is the decision horizon. The goal is to optimize the agent's policy to maximize the expected discounted cumulative reward.

**Variational Auto-Encoder.** The variational auto-encoder (VAE) is a directed graphical model with certain types of latent variables, such as Gaussian latent variables. A generative process of the VAE is as follows: a set of latent variable \(z\) is generated from the prior distribution \(p_{}(z)\) and the data x is generated by the generative distribution \(p_{}(x|z)\) conditioned on \(z:z p_{}(z),x p_{}(x|z)\). In general, parameter estimation of directed graphical models is often challenging due to intractable posterior inference. However, the parameters of the VAE can be estimated efficiently using the stochastic gradient variational Bayes (SGVB) framework, where the variational lower bound of the log-likelihood is used as a surrogate objective function. In this framework, a proposal distribution \(q_{}(x|z)\), is introduced to approximate the true posterior \(p_{}(x|z)\). MLPs are used to model the recognition and the generation models. Based on the Gaussian latent assumption, the first term of Eq.1 can be marginalized. The second term can be approximated by drawing samples \(z^{(l)}(l=1,...,L)\) by the proposal distribution \(q_{}(x|z)\). The empirical objective of the VAE is written as:

\[L_{VAE}(,)=_{}(x|z^{(l)})-KLq_{}(z|x)|| N(0,I) \]

Figure 2: Conceptual overview of MARS.

## 3 Intermittent Control Markov Decision Process (Intermittent-MDP)

In this section, we introduce the Intermittent Control Markov Decision Process (Intermittent-MDP) to model both fixed and random interaction interval control problems. The objective of Intermittent-MDP is to train the policy to decide on an effective action sequence \(u\) based on the current state \(s\), which should be longer than or equal to the maximum interaction interval \(c\), thus ensuring smooth and efficient motion of executor between the two interactions.

**Definition of Intermittent-MDP**. Intermittent-MDP (\(\)) can be represented as a tuple:

\(,,,, _{0}\). \(S^{n}\) is the state space, \(\) is the set of action sequence \(u\) that the policy can select. and \(_{0}\) denotes the initial state distribution. The reward is defined as \(\), \(\) is the multi-step transition function.

Different from normal MDP in which the agent makes a decision \(a_{t}\) according to the current state \(s_{t}\), intermittent-MDP requires the agent to generate an action sequence \(u_{t}=\{a_{t},...,a_{t+c}\}\) according to the received state \(s_{t}\)(\(a_{t}\) is the single step action at timestep \(t\)). The cumulative rewards of executed actions can be received in the next interaction. Policy \(\) takes current state to select action sequence \(u_{t}=\{a_{t},...,a_{t+c}\}\). When the interaction interval is present for \(j\) timesteps, the executor uses \(j\) single-step actions in the action sequence \(u\) to maintain the motion (\(j\{0,1,...,c\}\)). When the interaction is continuous, the executor uses the first single-step action in \(u\) each time (degenerated to the transition function \(\) in normal MDP). Thus, the environment transition function \(\) is defined as:

\[(s_{t+j}|s_{t},u_{t})=_{i=t}^{t+j-1}(s_{i+1}|s_{i},a_{ i})(u_{t}|s_{t})\;\;j\{0,1,...,c\},\;a_{i} u_{t}. \]

If the interval is random and we only know the maximum number of interval step \(c\), Intermittent-MDP is represented as _random Intermittent-MDP_. When the interval time is fixed, Intermittent-MDP represents a fixed interaction interval problem, i.e., _fixed Intermittent-MDP_.

## 4 Multi-step Action Representation

In this section, we introduce Multi-step Action Representation (MARS), a novel framework that can map long action sequences into an efficient latent space. We hypothesize that multi-step actions reside on a homogeneous manifold. To learn the multi-step action representation, we employ VAE to constructing a compact and decodable latent representation space to approximate such a manifold.

To align our latent action space with the two characteristics mentioned in Sec.1, i.e. 1 Decision space simplification 2 Dynamic semantic smoothness. In Sec.4.1, we introduce the concept of action transition scale and encode this as a conditional aid in VAE training to simplify the decision space of the policy. In Sec.4.2, we demonstrate how state residual prediction can be leveraged to smooth the semantics of dynamic transitions in latent action spaces, and in Sec.4.3, we illustrate the combination of MARS with DRL, as well as the extension of our approach to random intermittent-MDP.

### Scale-Conditioned Multi-step Action Encoding and Decoding

Although VAE has been proven to be able to construct a compact and effective latent action space for normal MDP with state \(s\) as the prior condition (Li et al., 2021), due to the complex combinatorial nature of multi-step actions in Intermittent-MDP, the policy still struggles to explore the optimal actions in the vast action space. Therefore, to address this challenge, we introduce the concept of **action transition scale**\(v\) as a conditional term in VAE, to constructs the multi-step latent action space \(\). \(\) stands for a description of the motion style, it represents the accumulation of action change scales within each action sequence \(u\). By determining \(\), we can constrain the latent action \(z\) chosen by the policy within a related subspace (Sohn et al., 2015) in which all candidate action sequences have the same motion style in terms of \(\), thus reducing the difficulty of exploration. We choose \(\) as a conditional term due to its task-specific nature. For instance, in robot scenarios, to ensure the balance of the robot and prevent joint damage, the scale of action changes is typically small. In some obstacle avoidance tasks, however, the magnitude of action changes can be significant when the agent encounters suddenly approaching obstacles. Once the policy roughly determines the appropriate action transfer scale for the current state, it can make more suitable decisions, making the policy more efficient and controllable in the complex action space. Notably \(\) does not need to beset manually but is selected adaptively by the policy through training, which is described in detail in Sec.4.3. We formulate \(\) as follows:

\[_{u_{t}}=^{c-1}||a_{i+1}-a_{i}||}{(c-1) B}. \]

\(c\) is the maximum interval and \(B\) denotes the upper limit of action change. The numerator part represents the total absolute difference between consecutive actions, used to assess the transition magnitude of \(u_{t}\). Eq.3 normalizes \(_{u_{t}}\)to \(\).

Given an action sequence \(u_{t}\) and the corresponding states \(s_{tt+t+c}\), our encoder \(q_{}(z_{u_{t}}|u_{t},s_{tt+c},_{u_{t}})\) parameterized by \(\) takes \(s_{t:t+c}\) and the action transition scale \(_{u_{t}}\) as conditions to build a multi-step latent action space, and maps the action sequence \(u_{t}\) into the latent variable \(z_{u_{t}}^{d_{1}}\) (\(d_{1}\) denotes the dimension of \(z_{u_{t}}\)). The decoder \(p_{}(_{t}|z_{u_{t}},s_{t},_{u_{t}})\) parameterized by \(\) then reconstructs the multi-step actions \(u_{t}\) from \(z_{u_{t}}\). We employ a Gaussian latent distribution \(N(_{z},_{z})\) to model \(q_{}(z_{u_{t}}|u_{t},s_{t:t+c},_{u_{t}})\) where \(_{z}\) and \(_{z}\) are the mean and standard deviation outputted by the encoder. The decoder decodes \(z_{u_{t}} N(_{z_{u_{t}}},_{z_{u_{t}}})\) as following: \(_{t}=g_{_{1}} p_{_{0}}(z_{u_{t}},s_{t},_{u_{t}})\), \(g_{_{1}}\) is a fully-connected layer for reconstruction. \(\) denotes cascade, i.e. the output of \(p_{_{0}}\) acts as the input of \(g_{_{1}}\). We utilize cascaded heads because traditional parallel heads, employed for both reconstruction and state residual prediction, can interfere with optimizing individual objectives and impede the learning of the shared representation . \(p_{_{0}}\) denotes the shared layers of the decoder. \(_{i\{1,2\}}\) denote the parameters of the prediction networks. The loss function of our VAE is:

\[L_{VAE}(,)=_{s,u D,z q_{}}\| _{t}-u_{t}\|_{2}^{2}+D_{KL}q_{}(|u_{t},s_{tt+c}, _{u_{t}})\|N(0,I), \]

where \(D\) is the buffer. The first term is the reconstruction loss (using MSE) of the action sequence, the last term is the Kullback Leibler divergence \(D_{KL}\) between the variational posterior of latent representation \(z\) and the standard Gaussian prior. By using the reparameterization trick , \(_{t}\) is differentiable with respect to \(\) and \(\). For any latent variables \(z_{u_{t}}\), it can be decoded into multi-step actions \(_{t}\) conveniently by the VAE decoder. That is,

\[:& z_{u_{t}} q_{ }(|u_{t},s_{t:t+c},_{u_{t}})\ s_{t:t+c},u_{t}, _{u_{t}}\\ :&_{t}=g_{_{1}} p_{ _{0}}(z_{u_{t}},s_{t},_{u_{t}})\ s,z_{u_{t}}, _{u_{t}} \]

### Dynamic-Aware Multi-step Action Representation

In section 4.1, we introduce how to build a decodable representation space for multi-step actions. However, it is still inefficient to learn the policy and value functions in the latent action space learned by the above VAE. In comparison to the real-world action space, the current construction of our action space lacks a crucial attribute: dynamic semantic smoothness. This refers to the manifestation of differences in environmental impact through the Euclidean distance between points in the latent space, where closer points correspond to more similar impacts. To address this issue, we further apply an unsupervised learning loss based on environmental dynamics prediction to refine the multi-step action representation. Relevant proof can be found in . MARS captures the environmental dynamics by predicting the state transition residual. We use the state transition as a measure of environmental dynamic because: State transition does not require a per-step reward, it can be used in reward-agnostic pretraining. Building value equivalent models is more difficult since Q-values in the early stage of training are inaccurate . In contrast, state transition is more reliable and accessible. 

The same reward or Q-value may correspond to different environmental dynamics, but the same environmental dynamics have the same reward or Q-value.

Figure 3: Architecture of MARS. ‘t’ denotes different steps at different stage. Both \(z_{u_{t}}\) and \(z_{t}\) refer to variables in \(Z\), but they are used in different stages (the same goes for \(_{u_{t}}\) and \(_{t}\)).

Specifically, MARS predicts the residual difference between the state \(s_{t+c}\) after the execution of \(u_{t}\) and the current state\(s_{t}\). As shown in the left of Figure 3, \(h_{_{2}}\) is a subnetwork of the decoder. For any transition sample \((s_{t},u_{t},s_{t+c})\), we denote the state residual as \(_{s_{t},s_{t+1}}=s_{t+c}-s_{t}\) and denote \(p_{state}=h_{_{2}} p_{}\). The residual \(_{s_{t},s_{t+c}}\) is predicted as:

\[_{s_{t},s_{t+c}}=p_{state}(z_{u_{t}},s_{t},_{u_{t}}) \;s_{t},z_{u_{t}},_{u_{t}}. \]

The environmental transition prediction loss is defined as:

\[L_{dy}(,)=_{s_{t},u_{t},s_{t+c}}\|_{s_{t}, s_{t+c}}-_{s_{t},s_{t+c}}\|. \]

Above all, the ultimate training loss for the multi-step action representation is:

\[L_{MARS}(,)=L_{VAE}(,)+ L_{dy}(,), \]

where \(\) is a hyper-parameter that controls the relative importance of the \(L_{dy}\) and \(L_{VAE}\). \(L_{MARS}\) only depends on the environmental dynamic data which is reward-agnostic . During training, transitions stored in the buffer or offline dataset are utilized. Additionally, we observe that MARS exhibits insensitivity to data quality. Notably, data collected through random policies suffice for effective training the multi-step action representation.

### DRL with Multi-step Action Representation

As a plug-in method, MARS can be applied to any RL algorithm. It contains two types of actions: \(\) the encoded multi-step action \(z\) and \(\) the action transition scale \(\). RL algorithms maximize the expected cumulative reward by selecting the optimal \(z_{t}\) and \(_{t}\) at \(s_{t}\). In this section, we apply MARS to a typical model-free RL method TD3  as an example. TD3 is a deterministic Actor-Critic algorithm. As illustrated in the right part of Figure 3, with the learned transition-aware multi-step action representation, the actor network learns a latent policy \(_{}\) that outputs the latent actions according to current state \(s\), i.e., \([z_{t},_{t}]=_{}(s)\). \(z_{t}\) and \(_{t}\) respectively represent the action selected at time \(t\) from \(\) and \(\) constructed in the section 4.1. Then we obtain the corresponding multi-step actions \(u_{t}\) by decoding the latent action \(z_{t}\) and \(_{t}\): \(u_{t}=g_{_{1}} p_{_{0}}(s,z_{t},_{t})\).

Two critic networks \(Q_{_{1}},Q_{_{2}}\) are utilized which take the latent actions \(z_{t}\) and \(_{t}\) as inputs to approximate the value function \(Q_{_{}}\) more accurately. We train the critic network using the transition data \((s,_{t},z_{t},r,s^{})\) sampled from the experience replay. To simplify notations, in this subsection, \(s\) is the current state. \(s^{}\) is the state perceived at the next interaction interval. The critic loss function is:

\[L_{Q}(_{i})=_{s,z_{t},_{t},s^{}}(y-Q_{ _{i}}(s,z_{t},_{t}))^{2}\;\; i\{1,2\}. \]

Where \(y=r+_{j=1,2}Q_{}}(s^{},_{}(s^{}))\), \(\) denotes the target network parameters. The actor is updated according to the Deterministic Policy Gradient :

\[_{}J()=_{s}_{_{}(s)}Q_{ _{1}}(s,_{}(s))_{}_{}(s). \]

The overall algorithm MARS-TD3 is summarized in Algorithm 1, which contains two major stages: 1 the warmup stage and 2 the policy learning stage. In stage 1, MARS is trained using a prepared replay buffer \(D\). The SC-VAE is updated by minimizing the VAE and the environmental dynamic prediction loss. Note that the proposed algorithm has no requirement on how the buffer \(D\) is prepared and here we simply use a random policy to gather the data. In stage 2, given an environment state, the latent policy outputs the latent action \(z_{t}\) and the action transition scale \(v_{t}\) perturbed by the Gaussian exploration noise. Then, the latent action is decoded into the original multi-step actions by the decoder so as to interact with the environment. The newly collected transition sample is stored in the replay buffer \(D\). After that, the latent policy is updated using the data sampled from \(D\). The action representation model is also updated periodically in the second stage to make continual adjustments to the change of data distribution. The detailed network architectures are presented in appendix B.1.

**As for random interaction interval tasks,** the interaction interval cannot be predicted, so we let the agent output an action sequence with the maximum length. To improve the training stability, we precisely record the absolute time step and the execution flag of each action, which makes the actions be executed in strict accordance with the time step order. When a new action sequence arrives at the action executor, the previous action sequence will be replaced, and the execution flag of the unexecuted actions will be set to False. The subsequent rewards will be attributed to the actions executed in the new action sequence. Thus, the actual reward stored in the experience replay \(D\) for each latent action is the sum of the executed action reward in the corresponding sequence.

## 5 Experiment

We empirically evaluate MARS to answer the following research questions. **RQ1: Performance in random interaction interval tasks.** Can MARS significantly improve the performance in random Intermittent-MDP tasks, such as simulated remote NPC control tasks? **RQ2: Performance in fixed interaction interval tasks.** Can MARS significantly improve the performance in fixed Intermittent-MDP problems, such as real-world robot arm grasping tasks? **RQ3: Generalization.** Can MARS be seamlessly integrated into existing RL algorithms and improve their performance? **RQ4: Ablation study.** Do both the action transition scale and the state dynamic prediction contribute to optimizing the multi-step latent action space? How is the robustness of MARS?

### Random Intermittent-MDP Tasks (RQ1)

#### 5.1.1 Experimental Setups

**Benchmarks.** We select two types of control tasks to simulate the remote NPC control problem with random interaction intervals: (1) robot control and (2) navigation tasks. For robot control tasks, we select four typical openai Mujoco tasks with random interaction delays,i.e., Hopper, Ant, Walker, HalfCheetah. Mujoco is a well-known testbed and is widely used in reinforcement learning

Figure 4: Comparisons of methods in simulated remote NPC control tasks with random interaction interval. The x- and y-axis denote the environment steps and average episode reward. Curves and shades denote the mean and the standard deviation over 8 runs.

research [Brockman et al., 2016]. For navigation tasks, we used the medium and difficult maps of 2dmaze in D4RL [Fu et al., 2020], where the agent's goal is to walk to the end of the maze under unstable interactions. Both types of tasks are modified to incorporate random interaction intervals, mimicking the real-world remote control scenarios [Chen and Wu, 2015]. In RTS games [Andersen et al., 2018], the maximum interaction interval usually spans \(0.5s\) to \(0.7s\), with the action execution time typically between \(0.1s\) and \(0.25s\). Accordingly, we set the maximum interval to \(10\) time steps, requiring the policy to generate an effective action sequence \(a_{t:t+9}\) based on the received state \(s_{t}\).

**Baselines.** To our knowledge, no specific solution exists for the Intermittent-MDP problem. Thus, we compare with three baseline methods. (1) **Perfect-TD3**: We train TD3 in the perfect environments with continuous interaction without interaction interval, i.e., normal MDP. It is the highest standard for evaluating MARS. (2) **Frameskip-TD3**: We combine the frameskip technique with TD3 (a common trick for multi-step decision) and apply it to the intermittent-MDP tasks. (3) **Multistep-TD3**: We modify TD3 to directly make decisions for the \(c\) future steps by outputting a concatenated action vector of \(c\) times of dimensionality.

#### 5.1.2 Performance of remote NPC control tasks

To mitigate implementation bias and ensure a comprehensive comparison, we implement all methods using the same architecture based on TD3 [Fujimoto et al., 2018]. For all tasks, we set the dimension of \(z_{t}\) to 8 and the scaling parameter \(\) to \(5\). We set the warm-up (stage 1) step to 400000 and 100000 for the Mujoco tasks and the navigation task respectively. Detailed parameter setting can be found in appendix B.2. The results in Figure 4 show that MARS-TD3 outperforms the other methods in all tasks, especially in the high-dimensional action control tasks (i.e., Mujoco). Compared with vanilla multistep-TD3, our method can significantly improve the performance of the RL algorithm in random Intermittent-MDP control problems by learning a more compact multistep action representation while avoiding the convergence difficulties caused by dimensional explosion. Note that MARS-TD3 can also achieve comparable performance with perfect-TD3 in most tasks.

### Fixed Intermittent-MDP Tasks (RQ2)

#### 5.2.1 Experimental Setups

**Task description.**The experiment involves a 6-DoF robot arm performing grasping tasks within a \(30 30 30cm^{3}\) tabletop workspace. \(15\) rounds of experiments are conducted using our method and baselines. In each round, 6 objects are randomly selected and placed on the table. The robot observes the workspace with a single-depth image from a fixed side view. The viewpoint of the virtual camera points toward the workspace's origin at a radial distance \(r=2l\) and an angle \(=/3\), \(l\) is the workspace's length. We follow the setting of traditional grasp tasks [Jiang et al., 2021] and set the total number of interactions to 6, meaning that the policy is expected to complete the grasp of all six targets in six intervals. Thus, the robot arm is allotted a single observation of the environment per object. As shown in Figure 5, at each interaction interval, there are 4 steps to grasp an object. The robotic arm has to first acquire observations at the beginning of each grasping. Upon perceiving the observation, an action sequence \(a_{t:t+15}\) is executed to grasp the target object and then place the object in the target box. The time limit for grasping one object is set to \(30s\). The robot need to move to a predefined location for capturing image (images are transformed into a \(512\)-dim vector). Action space is \(3\)-dim2, the reward function encourages policies to maximize the grasping success rate and motion smoothness.

**Baselines.** Online RL demands extensive exploration, resulting in low sample efficiency. Besides, Online RL accidental random exploration may lead to robot arm damage. Thus, we opt for offline

Figure 5: A complete grasp process in each interaction interval. algorithm generates the motion trajectory via the expected coordinates).

RL commonly utilized in robot scenarios, i.e. TD3+Behavior Cloning (TD3-BC) , to serve as the backbone. Methods: (1) Vanilla TD3-BC. (2) Multistep TD3-BC, same as Multistep-TD3 used in the section 5.1 (3) Dense observation + TD3-BC. An extra camera provides dense observation every \(3\) seconds, and the robot arm still has to move to the observing position at the start of each episode to eliminate differences caused by the observing motion.

**Evaluation metrics.** (1) Grasp success rate (GSR), the ratio of successfully grasping the objects. (2) Declutter rate (DR), the average ratio of objects removed after successful grasping. (3) Motion smooth ratio (MSR): \(}{}\), evaluating whether the motion is smooth and natural (pauses and shaking can result in lower scores). All results are averaged over \(15\) simulation rounds.

#### 5.2.2 Performance of real-world Robot Arm Grasping

The overall comparison is presented in Table 1. The results show that due to the sparse interaction, the vanilla TD3-BC fails to complete the task and frequently pauses or takes unnatural motions during the execution. While the performance of TD3-BC is significantly improved with additional dense observations, it remains unsatisfactory due to the lack of real-time feedback. Moreover, the use of extra cameras results in higher costs of the experiment. Although Multistep + TD3-BC helps smoothen the motion, it fails to effectively mitigate the instability caused by Intermittent-MDP. MARS addresses the limitation of the vanilla Multistep + TD3-BC, which makes TD3-BC complete the task with high quality. We provide the corresponding motion pattern for each method. To further verify the effectiveness of MARS combined with online RL on fixed interaction interval control task. The results in the Appendix C.2 show that our method outperforms the baselines on all four tasks.

### Generalization of MARS (RQ3)

We further test MARS with popular RL algorithms on three random interaction interval tasks: Hopper, Walker, and Maze hard. We maintain consistent parameters for each method and implement them based on the publicly available codebase. We train each RL algorithm with dense interactions as baselines and compare them with MARS-enhanced methods under the random interaction interval setting. Results in Appendix C.3 shows that all methods can learn effective policies with the help of MARS and the converged performance is comparable to the perfect interaction settings.

### Ablation Study and Visual Analysis (RQ4)

We conducted evaluations on the key components of MARS, i.e. action transition scale and state dynamic prediction. Figure 6 (a) shows that both components optimize the latent space and improve the learning efficiency of DRL policies. MARS performs best when both modules are integrated. A comprehensive analysis is shown in Appendix C.4. Figure 6 (b) uses t-SNE  to visualize the latent action representations. We color each action based on its impact on the environment. Results show that actions with a similar impact, i.e., \(_{s_{t},s_{t+}}\), on the environment are clustered closely, which indicats that the learned action representation is dynamic smooth. Besides, results in Appendix C.5 improve the robustness of MARS to different interaction interval settings. Results in Appendix C.6 show the influence of latent space dimensions on MARS. Lastly, an analysis of self-supervised training steps can be found in Appendix C.7.

## 6 Conclusion and Limitation

In this paper, we observe that previous DRL methods fail to learn effective policies in intermittent control scenarios because of the discontinue interaction. To improve the performance of DRL on

   Method & MSR(\%) & GSR(\%) & DR(\%) \\    } & \(70.8\) & \(74.2\) & \(82.9\) \\   & \(70.5\) & \(61.5\) & \(69.4\) \\   & \(65.3\) & \(59.1\) & \(70.7\) \\   & Vanilla TD3-BC & \(22.5\) & \(34.1\) & \(27.3\) \\   

Table 1: Performance of the robot arm grasping task.

Figure 6: Ablation study, the curve and shade denote the mean and a standard deviation of the returns over 5 runs.

such tasks, we propose **M**ulti-step **a**ction **r**epresentation (MARS) to construct a reliable multi-step latent action space. Based on this latent action space, DRL methods can make effective advance decisions to ensure the smoothness and efficiency of the agent's motion when it cannot interact with the environment. MARS outperforms baselines in a variety of fixed and random interaction intervals control tasks. Additionally, MARS has potential for improvement in represent extremely long action sequences, which we will address by identifying more powerful representation models, e.g. transformer based VAE, in the future.