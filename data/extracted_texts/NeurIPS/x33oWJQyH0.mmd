# Unsupervised Object Detection

with Theoretical Guarantees

Marian Longa

Visual Geometry Group

University of Oxford

mlonga@robots.ox.ac.uk &Joao F. Henriques

Visual Geometry Group

University of Oxford

joao@robots.ox.ac.uk

###### Abstract

Unsupervised object detection using deep neural networks is typically a difficult problem with few to no guarantees about the learned representation. In this work we present the first unsupervised object detection method that is theoretically guaranteed to recover the true object positions up to quantifiable small shifts. We develop an unsupervised object detection architecture and prove that the learned variables correspond to the true object positions up to small shifts related to the encoder and decoder receptive field sizes, the object sizes, and the widths of the Gaussians used in the rendering process. We perform detailed analysis of how the error depends on each of these variables and perform synthetic experiments validating our theoretical predictions up to a precision of individual pixels. We also perform experiments on CLEVR-based data and show that, unlike current SOTA object detection methods (SAM, CutLER), our method's prediction errors always lie within our theoretical bounds. We hope that this work helps open up an avenue of research into object detection methods with theoretical guarantees.

## 1 Introduction

Unsupervised object detection using deep neural networks is a long-standing area of research at the intersection of machine learning and computer vision. Its aim is to learn to detect objects from images without the use of training labels. Learning without supervision has multiple advantages, as obtaining labels for training data is often costly and time consuming, and in some cases may be impractical or unethical. For example, in medical imaging, unsupervised object detection can help save specialists' time by automatically flagging suspicious abnormalities , and in autonomous driving it may help automatically detect pedestrians on a collision course with the vehicle . It is thus important to understand and develop better unsupervised object detection methods.

While successful, current object detection methods are often empirical and possess few to no guarantees about their learned representations. In this work we aim to address this gap by designing the first unsupervised object detection method that we prove is guaranteed to learn the true object positions up to small shifts, and performing a detailed analysis of how the maximum errors of the learned object positions depend on the encoder and decoder receptive field sizes, the object sizes, and the sizes of the Gaussians used for rendering. This is especially important in sensitive domains such as medicine, where incorrectly detecting an object could be costly. Our method guarantees to detect any object that moves in a video or that appears at different locations in images, as long as the objects are distinct and the images are reconstructed correctly.

We base our unsupervised object detection method on an autoencoder with a convolutional neural network (CNN) encoder and decoder, and modify it to make it exactly translationally equivariant (sec. 3). This allows us to interpret the latent variables as object positions and lets us train the network without supervision. We then use the equivariance property to formulate and prove a theorem that relates the maximum position error of the learned latent variables to the size of the encoder anddecoder receptive fields, the size of the objects, and the width of the Gaussian used in the decoder (sec. 4). Next, we derive corollaries describing the exact form of the maximum position error as a function of these four variables. These corollaries can be used as guidelines when designing unsupervised object detection networks, as they describe the guarantees of the learned object positions that can be obtained under different settings. We then perform synthetic and CLEVR-based  experiments to validate our theory (sec. 5). Finally, we discuss the implications of our results for designing reliable object detection methods (sec. 6).

Concretely, the contributions of this paper are:

1. An unsupervised object detection method that is guaranteed to learn the true object positions up to small shifts.
2. A proof and detailed theoretical analysis of how the maximum position error of the method depends on the encoder and decoder receptive field sizes, object sizes, and widths of the Gaussians used in the rendering process.
3. Synthetic experiments, CLEVR-based experiments, and real video experiments validating our theoretical results up to precisions of individual pixels.

## 2 Related Work

**Object Detection.** Object detection is an area of research in computer vision and machine learning, dealing with the detection and location of objects in images. Popular supervised approaches to object detection include Segment Anything (SAM) , Mask R-CNN , U-Net  and others . While successful, these methods typically require large amounts of annotated segmentation masks and bounding boxes, which may be costly or impossible to obtain in certain applications. Popular unsupervised and self-supervised object detection methods include CutLER , Slot Attention , MoNet  and others . These methods aim to learn object-centric representations for object detection and segmentation without using training labels. Finally, unsupervised object localisation methods such as FOUND  and others  aim to localise objects in images, typically using vision transformer (ViT) self-supervised features. Compared to both current supervised and unsupervised object detection and localisation methods, our work is the only one that has provable theoretical guarantees of recovering the true object positions up to small shifts. It also requires no supervision.

**Identifiability in Representation Learning.** Identifiability in representation learning refers to the issue of being able to learn a latent representation that uniquely corresponds to the true underlying latent representation used in the data generation process. Some recent works aim to reduce the space of indeterminacies of the learned representations, and thus achieve identifiability, by incorporating various assumptions into their models. Xi et al.  categorise these assumptions for generative models into constraints on the distribution of the latent variables and constraints on the generator function. Some of their categories include non-Gaussianity of the latent distribution , dependence on an auxiliary variable [9; 10], use of multiple views , use of interventions [1; 15], use of mechanism sparsity , and restrictions on the Jacobian of the generator . In contrast, in our work we achieve identifiability by making our network equivariant to translations, imposing an interpretable latent space structure, and requiring the data to obey our theorem's assumptions.

## 3 Method

In this section we describe the proposed method for unsupervised object detection with guarantees. On a high level, our architecture is based on an autoencoder that is fully equivariant to translations, which we achieve by making the encoder consist of a CNN followed by a soft argmax function to extract object positions, and making the decoder consist of a Gaussian rendering function followed by another CNN to reconstruct an image from the object positions (fig. 1). In the following sections we describe the different parts of the architecture in detail.

**Autoencoder with CNN Encoder and Decoder.** We start with an autoencoder, a standard unsupervised representation learning model, consisting of an encoder network \(\) that maps an image \(x\) to a low-dimensional latent variable \(z\), followed by a decoder network \(\) that maps this variable back to an image \(\), with the objective of minimising the difference between \(x\) and \(\). Typically, the encoder and decoder networks are parametrised by multi-layer perceptrons (MLPs) or convolutional neural networks (CNNs) paired with fully-connected (FC) layers, however neither of these parametrisationsby default can guarantee that the learned latent variables will correspond to the true object positions (because of the universal approximation ability of MLPs and FC layers ). To obtain such guarantees, we would thus like to modify the autoencoder to make it exactly translationally equivariant, that is, a shift of an object in the input image \(x\) should correspond to a proportional shift of the latent variable \(z\), and a shift of the latent variable \(z\) should correspond to a shift in the predicted image \(\).

We start with an autoencoder where the encoder and decoder are both CNNs. CNNs consist of layers computing the convolution between a feature map \(x\) and a filter \(F\), defined in one dimension as

\[(x F)[i]=_{j}x[j]F[j-i]\] (1)

Intuitively, this corresponds to sliding the filter \(F\) across the feature map \(x\) and at each position of the filter \(i\) computing the dot product between the feature map \(x\) and the filter \(F\). We can prove that convolutional layers are equivariant to translations, since

\[(( x) F)[i]=_{j}x[j-t]F[j-i]=_{j}x[j]F[j-( i-t)]=(x F)[i]\] (2)

where \(\) is the translation operator that translates a feature map by \(t\) pixels, and we have used the substitution \(j j+t\) at the second equality. Therefore, the encoder and decoder are both equivariant to translations, but this property only holds for translations of feature maps (i.e. spatial tensors).

**From Encoder Feature Maps to Latent Variables.** So far we have only worked with images and feature maps, but the latter do not directly express positions of any detected objects. It would be preferable to convert these feature maps into scalar variables that can be interpreted as object positions that are equivariant to image translations. To do this, we first define a translation \(\) of a (1D) feature map \(x\) and a translation \(^{}\) of a scalar \(z\) as

\[(x)[i]=x[i-t],^{}(z)=z+t\] (3)

where \(i\) is the position in the feature map \(x\), \(\) shifts an image by \(t\) pixels, and \(^{}\) shifts a scalar by \(t\) units. To relate translations in feature maps to translations in latent variables, we can use a function that computes a scalar property of a feature map \(x\), such as \(\), defined as \((x)=\{i:x[j] x[i]\  j\}\). Using these definitions we can now prove the equivariance of \(\), i.e. that shifting the feature map \(x\) by \(\) corresponds to shifting the latent variable \((x)\) by \(^{}\):

\[( x) =\{i: x[j] x[i]\  j\}=\{i:x[j-t] x[i-t]\  j\}\] \[=\{i+t:x[j] x[i]\  j\}=(x)+t=^{ }(x)\] (4)

where at the first equality we use the definition of \(\), at the second equality we use the definition of \(\) (eq. 3, left), at the third equality we use the substitution \(i i+t\), at the fourth equality we use the definition of \(\), and at the last equality we use the definition of \(^{}\) (eq. 3, right).

However, because the \(\) operation is not differentiable, for neural network training we approximate it via a differentiable soft argmax function, defined in 2D as

\[(x)=(_{i=0}^{I-1} _{j=0}^{J-1}(i+)\ _{1}()[i,j],\ _{i=0}^{I-1} _{j=0}^{J-1}(j+)\ _{2}()[i,j])\] (5)

Figure 1: Network architecture. Encoder: (1) an image \(x\) is passed through a CNN \(\) to obtain \(n\) embedding maps \(e_{1},...,e_{n}\), (2) a maximum of each map is found using softargmax to obtain latent variables \([z_{1,x},z_{1,y},...,z_{n,x},z_{n,y}]\). Decoder: (1) Gaussians \(_{1},...,_{n}\) are rendered at the positions given by the latent variables, (2) the Gaussian maps are concatenated with positional encodings and passed through a CNN \(\) to obtain the predicted image \(\). Finally, \(x\) and \(\) are used to compute reconstruction loss \((,x)\).

where \(\) is the softmax function defined in one dimension as \((x)[i]=(x[i])/_{j}(x[j])\), \(_{1}(x)\) and \(_{2}(x)\) is the softmax function evaluated along the first and second dimensions of \(x\), \(\) is a temperature hyperparameter, \([i,j]\) is the image index, \(I\) is the image width, \(J\) is the image height, and the term \(1/2\) ensures that the densities correspond to pixel centres. As the temperature \(\) in eq. 5 approaches zero, \(\) reduces to the classical \(\) function.

**From Latent Variables to Decoder Feature Maps.** Similar to mapping from encoder feature maps to latent variables, we would now like to relate shifts in latent variables \(z\) to shifts of decoder feature maps \(x\). To do this, we can invert the action of the \(\) operation. Because \(\) is a many-to-one function, finding an exact inverse is not possible, but we can obtain a pseudo-inverse using the Dirac delta function defined as \((z)[i]=(i-z)\). We can show that \(\) is a pseudo-inverse of \(\) because \( z=i:(j-z) (i-z)\,; j=z\). Now, similar to the \(\) function, we can prove that the \(\) function is equivariant to the latent variable shift \(^{}\) on the input and the feature map shift \(\) on the output, i.e.

\[(^{} z)[i]=(i-^{} z) =(i-z-t)=(z)[i-t]=(z)[i]\] (6)

where at the first equality we have used the definition of \(\), at the second equality we have used the definition of \(^{}\) (eq. 3, right), at the third equality we have used the definition of \(\), and at the last equality we have used the definition of \(\) (eq. 3, left).

Again, because the \(\) function is not differentiable, we can approximate it using a differentiable \(\) function as

\[(z)[i]=(i-z,^{2})\] (7)

where \((i-z,^{2})\) is a Gaussian evaluated at \(i-z\) with variance given by the hyperparameter \(^{2}\). As the variance \(^{2}\) in eq. 7 approaches zero, the \(\) function reduces to the hard \(\) function.

Additionally, because the decoder is translationally equivariant, we also condition it on positional encodings of the size of the images to provide it with sufficient information to reconstruct different parts of the background, assuming the background is static. Alternatively, if background is varying, the decoder can be conditioned on a randomly-sampled nearby video frame instead, which will provide information about the background but not the objects' positions (following Jakab et al. ).

We also note that since the latent variables \(z\) are ordered, this allows the encoder and decoder to learn to associate each variable with the semantics of each object and achieve successful reconstruction.

We thus now have all the elements we need to create an equivariant architecture where the encoder and decoder are defined, respectively, by

\[z= x,_{t}=  z_{t}.\] (8)

This is shown in fig. 1. Having designed an exactly translationally equivariant architecture now allows us to obtain theoretical guarantees about the learned latent variables, which we discuss next. 1

## 4 Theoretical Results

In this section we present our main theorem stating the maximum bound on the position errors of the latent variables learned with our method in terms of the encoder and decoder receptive field sizes, the object size, and the Gaussian standard deviation (thm. 4.1). We continue by deriving specialised corollaries relating the maximum position error to the encoder receptive field size (cor. 4.2), decoder receptive field size (cor. 4.3), object size (cor. 4.4), and Gaussian standard deviation (cor. 4.5).

**Theorem 4.1**.: _Error Bound. Consider a set of images \(x X\) with objects of size \(s_{o}\), CNN encoder \(\) with receptive field size \(s_{}\), CNN decoder \(\) with receptive field size \(s_{}\), soft argmax function \(\), rendering function \(\) with Gaussian standard deviation \(_{G}\) and \(_{G}(0,_{G}^{2})\), and latent variables \(z\), composed as \(z= x\) and \(= z\) (fig. 1). Assuming (1) the objects are reconstructed at the same positions as in the original images, (2) each object appears in at least two different positions in the dataset, and (3) there are no two identical objects in any image, then the learned latent variables \(z\) correspond to the true object positions up to object permutations and maximum position errors \(\) of_

\[=(}{2}+}{2}-1,}{2}- {s_{o}}{2}+_{G}).\] (9)For proof see appendix A. Intuitively, the assumptions ensure that each latent variable corresponds to the position of each object in the image. The error in the learned object positions then arises from both the encoding and decoding process. In the encoding process, the maximum error occurs when the encoder and the object are as far away from each other as possible while still overlapping, i.e. \(s_{}/2+s_{o}/2-1\) (fig. 1(a)). Conversely, in the decoding process, the maximum error occurs when the rendered object and the latent variable are as far away from each other as possible while both still being inside the decoder receptive field, i.e. \(s_{}-s_{o}/2\) (fig. 1(b)). Additionally, there is an extra error of \(_{G}\) as the latent variable is rendered by a Gaussian and the decoder can capture any part of this Gaussian. Finally, because we assume each object is reconstructed at the same position as in the original image, the errors from the encoder and decoder must cancel each other out. Therefore, the overall maximum position error is given by the lower of the two expressions for the encoder and the decoder, leading to eq. 9. Next, we present corollaries relating this error bound to different factors.

**Corollary 4.2**.: _Error Bound vs. Encoder RF Size. The maximum position error as a function of the encoder receptive field (RF) size \(s_{}\) for a given \(s_{}\), \(s_{o}\), \(_{G}\), is_

\[(s_{})=}{2}+}{2}-1&\;1 s_{} s_{}-2s_{o}+2,\\ }{2}-}{2}+_{G}&\;s_{}>s_{}-2s_{o}+2. \]

For an illustration see fig. 2(a). There are two regions of the curve (separated by a dashed line). In the left-most region, \(s_{}<s_{}-2s_{o}+2\), the error is dominated by the encoder error, and in the right-most region, \(s_{} s_{}-2s_{o}+2\), the error is dominated by the decoder error. Initially, for \(s_{}=1\), the error is given by \(s_{o}/2-1/2\), because the \(1 1\) px encoder can match any pixel that is part of the object and so can be at most half of the object size away from the true object position that is at the centre of the object. As the encoder RF size increases up to \(s_{}-2s_{o}+2\), the position error increases linearly with it as \(s_{}/2+s_{o}/2-1\), because now any part of the encoder RF can match any part of the object (fig. 1(a)). This bound is deterministic due to the deterministic encoding process.

At \(s_{}=s_{}-2s_{o}+2\) (vertical dashed line in fig. 2(a)), the maximum errors from encoder and decoder both become equal to \(s_{}/2-s_{o}/2\). For \(s_{}>s_{}-2s_{o}+2\), the position error is dominated by the error from the decoder which is constant at \(s_{}/2-s_{o}/2+_{G}\) with \(_{G}(0,_{G}^{2})\), and so even though the encoder RF size is increasing, this has no effect as the limiting factor is now the decoder. Due to the Gaussian rendering step in the decoding process, this bound is now probabilistic, and is distributed normally with variance \(_{G}^{2}\).The results of corollary 4.2 can be extended to multiple objects with different sizes (see appendix B, cor. B.1)

**Corollary 4.3**.: _Error Bound vs. Decoder RF Size. The maximum position error as a function of the decoder receptive field (RF) size \(s_{}\) for a given \(s_{}\), \(s_{o}\), \(_{G}\), is_

\[(s_{})=}{2}-}{2}+_{G }&\;s_{o} s_{}<s_{}+2s_{o}-2,\\ }{2}+}{2}-1&\;s_{} s_{}+2s_{o}-2. \]

For an illustration see fig. 2(b). Similar to corollary 4.2, there are two regions of the curve, one for \(s_{}<s_{}+2s_{o}-2\) (left), where the error is dominated by the decoder error, and another for \(s_{} s_{}+2s_{o}-2\) (right), where the error is dominated by the encoder error. Note that this is opposite to cor. 4.2. Initially, for \(s_{}=s_{o}\), the decoder receptive field has the same size as the object,

Figure 2: Position errors. (a) Maximum position error due to encoder, given by \(s_{}/2+s_{o}/2-1\). The maximum error occurs when the encoder and the object are as far away from each other as possible while still overlapping by one pixel. (b) Maximum position error due to decoder, given by \(s_{}/2-s_{o}/2+_{G}\). The maximum error occurs when some part of the Gaussian at position \(z+_{G}\) is within the decoder receptive field (RF) but is as far away from the rendered object as possible.

and so to achieve perfect reconstruction it needs to be at the same position as the object, resulting in \(0\) position error plus any error \(_{G}\) caused by the non-zero width of the Gaussian. As the decoder RF size increases up to \(s_{}+2s_{o}-2\), the position error increases linearly with it as \(s_{}/2-s_{o}/2+_{G}\), because now the object can be at an increasing number of positions within the decoder and still achieve perfect reconstructions (fig. 2b). At \(s_{}=s_{}+2s_{o}-2\), the maximum errors from encoder and decoder both become equal to \(s_{}/2+s_{o}/2-1\). For \(s_{}>s_{}+2s_{o}-2\), the position error is dominated by the error from the encoder which is constant at \(s_{}/2+s_{o}/2-1\), and so even though the decoder RF size is increasing, this has no effect as the limiting factor is now the encoder. Similar to corollary 4.2, the results of corollary 4.3 can be extended to objects with multiple different sizes (see appendix B, cor. B.2).

**Corollary 4.4**.: _Error Bound vs. Object Size. The maximum position error as a function of the object size \(s_{o}\) for a given \(s_{}\), \(s_{}\), \(s_{}\), \(_{G}\), is_

\[(s_{o})=}{2}+}{2}-1&\ \ 1 s_{o}}{2}-}{2}+1,\\ }{2}-}{2}+_{G}&\ }{2}-}{2}+1<s_{o} s_{ }.\]

For an illustration see fig. 3c. Again, there are two regions of the curve, one for \(s_{o}<s_{}/2-s_{}/2+1\) (left), where the error is dominated by the encoder error, and one for \(s_{o} s_{}/2-s_{}/2+1\) (right), where the error is dominated by the decoder error. Initially, for \(s_{o}=1\), the error is given by \(s_{}/2-1/2\), because any pixel of the encoder receptive field can match the \(1 1\) px object and so the error can be at most half of the encoder receptive field size. As the object size increases up to \(s_{}/2-s_{}/2+1\), the position error increases linearly with it as \(s_{}/2+s_{o}/2-1\), because now any part of the encoder RF can match any part of the object (fig. 2a). At \(s_{o}=s_{}/2-s_{}/2+1\), the maximum errors from encoder and decoder both become equal to \(s_{}/4+s_{}/4-1/2\). For \(s_{o}>s_{}/2-s_{}/2+1\), the position error is dominated by the error from the decoder and decreases linearly as \(s_{}/2-s_{o}/2+_{G}\), because now there is a decreasing number of positions where the object can still fit inside the decoder receptive field (fig. 2b). At \(s_{o}=s_{}\), the object reaches the same size as the decoder, and thus the position error decreases to \(0\) with an additional error due to the width of the Gaussian, \(_{G}\). Interestingly, the triangular shape of the error curve means that small and large objects will both incur small position errors, while medium sized objects will incur higher errors.

**Corollary 4.5**.: _Error Bound vs. Gaussian Size. The maximum position error as a function of the Gaussian standard deviation \(_{G}\) for a given \(s_{}\), \(s_{}\), \(s_{o}\), is_

\[(_{G})=}{2}-}{2}+_{G}& \ \ _{G}<}{2}-}{2}+s_{o}-1,\\ }{2}+}{2}-1&\ \ _{G}}{2}-}{2}+s_{o}-1.\]

For an illustration see fig. 3d. Firstly, there is an overall maximum bound for the position error due to the encoder, given by the constant \(s_{}/2+s_{o}/2-1\), which is independent of the Gaussian standard deviation. Then, initially for \(_{G}=0\), the rendered Gaussian is effectively a delta function, and so the position error is dominated by the decoder error given by \(s_{}/2-s_{o}/2\), which describes the maximum distance between the object and the delta function with both of them fitting inside the decoder receptive field (fig. 3b). As the Gaussian standard deviation increases, the position error increases linearly as \(s_{}/2-s_{o}/2+_{G}\) with \(_{G}(0,_{G}^{2})\). Then, depending on what part of the Gaussian the decoder is convolved with, there are different bounds for the maximum

Figure 3: Theoretical bounds for the maximum position error as a function of the encoder receptive field size \(s_{}\), decoder receptive field size \(s_{}\), object size \(s_{o}\), and Gaussian standard deviation \(_{G}\), as the remaining factors are fixed. Each bound consists of a region due to the encoder error (solid line) and the decoder error (probabilistic bound). Standard deviations are represented by shades of blue.

position error. If the decoder is convolved with a part of the Gaussian that is within \(n\) standard deviations of its centre, the maximum position error increases linearly as \(s_{}/2-s_{o}/2+n_{G}\) up to \(_{G}=(s_{}/2-s_{}/2+s_{o}-1)/n\), after which point the position error becomes dominated by the encoder value of \(s_{}/2+s_{o}/2-1\). In fig. (d)d, the maximum position error bound when the decoder is convolved with a part of the Gaussian within its first and second standard deviations, is denoted by darker and lighter shades of blue, respectively.

## 5 Experimental Results

In this section we validate our theoretical results on synthetic experiments (sec. 5.1) and CLEVR data (sec. 5.2). Additional real video experiments are in app. E. We first validate corollaries 4.2-4.4 via synthetic experiments in sec. 5.1, demonstrating very high agreement up to sizes of individual pixels. We then apply our method to CLEVR-based  data containing multiple objects of different sizes in varying scenes (sec. 5.2) and show that compared to current SOTA object detection methods (SAM , CutLER ), only our method predicts positions within theoretical bounds.

### Synthetic Experiments

In this section we validate corollaries 4.2-4.5 via synthetic experiments. Our dataset consists of a small white square on a black background. In each experiment we fix all but one of the encoder RF size \(s_{}\), decoder RF size \(s_{}\), object size \(s_{o}\), and Gaussian standard deviation \(_{G}\), and vary the remaining variable. We perform each experiment 20 times, corresponding to 20 random initializations of the trained parameters, and record the position error \(\) as the difference between the predicted object position \(z\) and the ground truth object position \(z_{GT}\). For more details see appendix C.1.

Position Error vs. Encoder RF Size.In this experiment we aim to empirically validate corollary 4.2, by measuring the experimental position errors as a function of the encoder receptive field size. We vary the encoder RF sizes \(s_{}\{1,3,,31\}\) and fix \(s_{}=25,s_{o}=9,_{G}=.8\) and record position errors \(\). We visualise the data points (red) and the theoretical bounds (blue) in fig. (a)a. We can observe that all the data points lie at or below the theoretical boundary, which validates corollary 4.2. In particular, we observe that the deterministic boundary in the region to the left of the dashed line (corresponding to the encoder bound) is well respected, with some of the trained networks achieving exactly the maximum error predicted by theory.

Position Error vs. Decoder RF Size.In this experiment we aim to validate corollary 4.3 by measuring the experimental position errors as a function of the decoder receptive field size. We vary the decoder RF sizes \(s_{}\{1,3,,31\}\) and fix \(s_{}=9,s_{o}=9,_{G}=.8\) and record position errors \(\). We visualise the results in fig. (b)b. The figure shows the theory to be a strong fit to the data, validating corollary 4.3. In particular, we note that the data points fit the Gaussian distribution in the decoder part of the curve (left of the dashed line) and are very close to (1 px below) the deterministic upper bound in the encoder part of the curve (right).

Figure 4: Synthetic experiment results showing position error as a function of the encoder receptive field size \(s_{}\), decoder receptive field size \(s_{}\), object size \(s_{o}\), and Gaussian standard deviation \(_{G}\), as the remaining factors are fixed to \(s_{}=9,s_{}=25,s_{o}=9,_{G}=0.8\) (in a,b,c) or to \(s_{}=9,s_{}=11,s_{o}=7\) (in d). Theoretical bounds are denoted by a blue line (with 4 shaded regions denoting 1 to 4 standard deviations of the probabilistic bound) and experimental results by red dots.

**Position Error vs. Object Size.** In this experiment we aim to validate corollary 4.4 by measuring the experimental position errors as a function of the object size. We vary the object sizes \(s_{o}\{1,3,,25\}\) and fix \(s_{}=9,s_{}=25,_{G}=.8\) and record position errors \(\). We visualise the results in fig. 3(c). As all the data points lie at or below the theoretical boundary, this validates corollary 4.4. We note that the empirical distribution of errors follows very closely the shape of the theoretical bound, very strictly on the left side of the dashed line (encoder bound) and according to the distribution predicted on the right side (decoder bound).

**Position Error vs. Gaussian Size.** In this experiment we aim to validate corollary 4.5 by measuring the experimental position errors as a function of the Gaussian standard deviation. We vary the Gaussian standard deviations \(_{G}\{0.1,0.2,,2.1,2.25,2.5,...,5\}\), fixing \(s_{}=9,s_{}=11,s_{o}=7\) and record position errors \(\). We visualise the data points (red) and the theoretical bounds (blue) in fig. 3(d). As all the data points lie at or below the theoretical boundary, this validates corollary 4.5. In particular, we note that all the data points lie below the encoder bound (solid blue line), and all the data points lie within the bound denoted by four standard deviations away from the Gaussian. This means that in practice, the decoder can be convolved with any part of the Gaussian that lies within 4 standard deviations (corresponding to 3.2 px) from its centre. We also note that as the Gaussian standard deviation increases, the position error increases as expected, denoted by the positive slope of the data points between the third and fourth standard deviations (lightest shade of blue).

### CLEVR Experiments

In this section we validate our theory on CLEVR-based  image data of 3D scenes. Our dataset consists of 3 spheres of different colours at random positions, with a range of sizes due to perspective distortion. We train and evaluate each experiment similarly to those in sec. 5.1, recording position errors for the learned objects, and compare our results with SAM  and CutLER  baselines. We compute the theoretical bounds according to our theory in sec. 4 and app. B, and visualise the results in figs. 4(a)-4(c). For details see app. C.2. For experiments with different shapes see app. D.

Once again, the experimental results demonstrate high agreement with our theory, now even for more complex images with multiple objects and a range of object sizes (fig. 5, red, blue). Furthermore, while the SAM and CutLER baselines generally achieve low position errors, this is not guaranteed, and in some cases their errors are much higher than our bound (fig. 5, green, orange). We report the proportion of position errors from fig. 4(c) that lie within 2 standard deviations of our theoretical bound in table 5(b) and fig. 5(a), showing that compared to SOTA object detection methods, only for our method are the position errors always guaranteed to be within our theoretical bound.

## 6 Discussion

In light of our theoretical results, in this section we present some conclusions that can be drawn when designing new unsupervised object detection methods:

Figure 5: CLEVR experiment results showing position error as a function of the encoder receptive field size \(s_{}\), decoder receptive field size \(s_{}\), and object size \(s_{o}\), and Gaussian standard deviation \(_{G}\), as the remaining factors are fixed to \(s_{}=9,s_{}=25,s_{o},_{G}=0.8\) for (a)-(c) and to \(s_{}=5,s_{}=13,s_{o}\) for (d). Theoretical bounds are denoted by blue, experimental results in red, SAM baseline in green, and CutLER baseline in orange.

1. If the size of the objects that will be detected is known, to minimise the error on the learned object positions, one should aim to design the decoder receptive field size to be as small as possible while still encompassing the object. As the decoder RF grows beyond the object size, the error bound increases linearly with it up to a certain point (fig. 2(b)).
2. To minimise the error stemming from the encoder for a given object size, the encoder RF size should be kept as small as possible while still detecting the object (the RF size may be smaller than the object size), as again the error bound grows linearly with it up to a certain point (fig. 2(a)).
3. To minimise the error, the width of the rendering Gaussian should be kept as small as possible while still permitting gradient flow, as increasing it even slightly may result in a dramatic increase to the decoder term of the position error (fig. 2(d)). This is because, in practice, the decoder is able to detect parts of the Gaussian that are even 4 standard deviations away from its centre (fig. 2(d)).
4. In the case that one does not know _a priori_ the exact size of the objects to be detected, one can still design a network that minimises the position errors for a given range of sizes. In that case, one should set up the decoder RF size to be as close as possible to the size of the largest object, and keep the encoder RF size as small as possible while still detecting all objects. The position errors for different object sizes will then be distributed according to the curve in fig. 2(c), where the smallest and largest objects will achieve lowest errors and medium-size objects will achieve the greatest error, approximately given by a half of the average of the encoder and decoder RF sizes.

Finally, we discuss some limitations of our method. Firstly, the method can only detect dynamic objects, for example if they move in a video or if they appear at multiple locations in images. Secondly, in its current form the method learns representations that can not be used for videos with different backgrounds than the one used at training time; however, this can be overcome by conditioning the decoder on an unrelated video frame instead of the positional encodings, as in Jakab et al. . Thirdly, the guarantees of our method are conditional on the images being successfully reconstructed, which depends on the network architecture and optimisation method.

## 7 Conclusion

We have presented the first unsupervised object detection method that is provably guaranteed to recover the true object positions up to small shifts. We proved that the object positions are learned up to a maximum error related to the encoder and decoder receptive field sizes, the object sizes, and bandwidth of the Gaussians used to render the objects. We derived expressions for how the position error depends on each of these factors and performed synthetic experiments that validated our theory up to sizes of individual pixels. We then performed experiments on CLEVR-based data, showing that unlike current SOTA methods, the position errors our method are always guaranteed to be within our theoretical bounds. We hope our work will provide a starting point for more research into object detection methods that possess theoretical guarantees, which are lacking in current practice.

**Acknowledgements.** The authors acknowledge the generous support of the Royal Academy of Engineering (RF20181918163), the Royal Society (RGR1\(\)241385) and EPSRC (VisualAI, EP/T028572/1).

Figure 6: Proportion of position errors within 2 standard deviations of the theoretical bound (%), reported for different object sizes and methods. Results from table (b) are visualised in plot (a).