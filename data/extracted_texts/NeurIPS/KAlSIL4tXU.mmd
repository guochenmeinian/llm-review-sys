# PromptIR: Prompting for All-in-One Blind Image Restoration

Vaishnav Potlapalli\({}^{}\), Syed Waqas Zamir\({}^{}\), Salman Khan\({}^{}\), Fahad Shahbaz Khan\({}^{}\)

\({}^{}\)Mohamed bin Zayed University of AI, \({}^{}\)Core42, \({}^{}\)Linkoping University

firstname.lastname@mbzuai.ac.ae

###### Abstract

Image restoration involves recovering a high-quality clean image from its degraded version. Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels. This restricts their real-world application since it requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. We present a prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation. In particular, our method uses prompts to encode degradation-specific information, which is then used to dynamically guide the restoration network. This allows our method to generalize to different degradation types and levels, while still achieving state-of-the-art results on image denoising, deraining, and dehazing. Overall, PromptIR offers a generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image. Our code and pre-trained models are available here: [https://github.com/va1shn9v/PromptIR](https://github.com/va1shn9v/PromptIR).

## 1 Introduction

During image acquisition, degradations (such as noise, blur, haze, rain, etc.) are often introduced either due to the physical limitations of cameras or unsuitable ambient conditions. Image restoration refers to the process of recovering a high-quality clean image from its degraded version. It is a highly challenging problem due to its ill-posed nature as there exists many feasible solutions, both natural and unnatural. Recently, deep learning based restoration approaches  have emerged as more effective choice in comparison to conventional methods .

Deep neural network-based methods broadly differ in their approach to addressing the image restoration problem. Some works incorporate explicit task-specific knowledge in the network to deal with the corresponding restoration task, such as denoising , deblurring , and dehazing . However, these methods lack generalization beyond the specific degradation type and level. On the other hand, some works  focus on developing a robust architecture design and learn image priors from data implicitly. These methods train separate copies of the same network for different degradation types, degradation levels, and in more extreme cases on different datasets. However, replicating the same restoration model for different degradation types, levels, and data distributions is a compute-intensive and tedious process, and oftentimes impractical for resource-constrained platforms like mobile and edge devices. Furthermore, to select an appropriate restoration model during testing, these approaches require prior knowledge regarding the degradation present in the input image.

Therefore, there is a pressing need to develop an _all-in-one_ method that can effectively restore images from various types and levels of degradation.

One recent method, AirNet , addresses the all-in-one restoration task by employing the contrastive learning paradigm. This involves training an extra encoder to differentiate various types of image degradations. Although AirNet  yields state-of-the-art results, it struggles to model fully disentangled representations of different corruption types. Furthermore, the usage of an additional encoder for contrastive learning leads to a higher training burden due to the two-stage training approach.

To overcome these challenges, in this paper, we present a prompt-learning-based approach to perform all-in-one image restoration (see Fig. 1). Our method utilizes prompts, which are a set of tunable parameters that encode crucial discriminative information about various types of image degradation (as shown in Fig. 2). By interacting prompts with the feature representations of the main restoration network, we dynamically enhance the representations with degradation-specific knowledge. This adaptation enables the network to effectively restore images by dynamically adjusting its behavior. The main highlights of our work include,

* We present a prompting-based all-in-one _blind_ restoration framework PromptIR that relies solely on the input image to recover a clean image, without requiring any prior knowledge of the degradation present in the image.
* Our prompt block is a plug-in module that can be easily integrated into any existing restoration network. It consists of a prompt generation module (PGM) and a prompt interaction module (PIM). The goal of the prompt block is to generate input-conditioned prompts (via PGM) that are equipped with useful contextual information to guide the restoration network (with PIM) to effectively remove the corruption from the input image.

Figure 1: This figure illustrates our PromptIR approach. We propose a plug-and-play prompt module that implicitly predicts degradation-conditioned prompts to guide the restoration process of an input image with unknown degradation. The guidance from prompts is injected into the network at multiple decoding stages with few-learnable parameters. This allows learning an all-in-one unified model that can perform well across several image restoration tasks (e.g., draining, dehazing, and denoising).

Figure 2: The figure shows tSNE plots of the degradation embeddings used in PromptIR (ours) and the state-of-the-art AirNet . Distinct colors denote different degradation types. In our case, the embeddings for each task are better clustered, showing the effectiveness of prompt tokens to learn discriminative degradation context that helps in restoration.

* Our comprehensive experiments demonstrate the dynamic adaptation behavior of PromptIR by achieving state-of-the-art performance on various image restoration tasks, including image denoising, deraining, and dehazing using only a _unified_ PromptIR model.

## 2 Related Works

**Multi-degradation Image Restoration:** While single degradation image restoration methods [68; 47; 12; 69; 45; 76; 55; 43; 74; 51] have received significant interest, multi-degradation image restoration is relatively under-explored in the literature. A body of work focuses on images corrupted due to multiple weather conditions e.g., snow, fog, and rain [37; 57; 32]. However, they train specific encoder or decoder parallel pathways for each weather degradation which requires knowing specific degradation type and is less scalable. Chen _et al._ build a unified model for multiple restoration tasks, like super-resolution, denoising, and deraining, however, the model needs prior information about the corruption present in the input image as it uses a multi-head-tail architecture. In blind image restoration, we have no prior information on the degradation present in the image. This kind of problem setting has been tackled in the context of image super-resolution [73; 39; 10]. Li _et al._ introduce a unified model for denoising, draining, and dehazing, which uses an image encoder trained through contrastive learning to model good representations of the degradation, which are later used to predict the deformable convolution offsets in another network to perform the restoration. This method requires two-stage training and the effectiveness of contrastive learning hinges on accurately choosing the positive-negative pairs and the amount of data available. In comparison, our work is focused on developing a single-stage training pipeline for unified all-in-one image restoration that is conceptually simpler and works as a drop-in module for multiple degradations.

**Transformer-based restoration:** Transformer  architectures have found great success across various computer vision tasks  such as image recognition [15; 54; 67], object detection [5; 80; 38] and semantic segmentation [64; 59; 78]. Owing to their strong feature representation capability, they are extended to image restoration tasks [7; 59; 56; 9]. However, naive self-attention has quadratic complexity w.r.t. the image size and this poses a challenge for image restoration tasks where inputs are typically high-resolution. To address this, some works have proposed efficient transformer architectures [33; 68; 35] to reduce the computational costs. Specifically, SwinIR  uses windowed self-attention blocks along with convolutional layers to improve the efficiency of the model. Restormer  uses multi-depth convolution head attention to reduce the number of operations. In this work, we apply our PromptIR to Restormer owing to its efficient design and high performance, however, our prompt block is generic and can work with other architectures.

**Prompt learning:** In natural language processing, prompting-based methods are means to provide context information to models to finetune them on a target task . However, instead of using specific manual instruction sets as prompts, learnable prompts enable better parameter-efficient adaptation of models . Prompt learning techniques can effectively model task-specific context hence they have been used for finetuning to vision tasks [23; 34; 26] and incremental learning [62; 49; 61]. Prompt learning-based techniques have also been applied in the case of multitask learning [20; 60], where choosing the right prompt for each task remains critical. All these approaches target high-level vision problems, however, our goal here is to develop a generic model for low-level vision that can dynamically restore inputs based on their interaction with the prompts. The prompts act as an adaptive lightweight module to encode degradation context across multiple scales in the restoration network.

## 3 Method

In "All-in-one" image restoration, we aim to learn a single model \(M\) to restore an image \(I\) from a degraded image \(\), that has been degraded using a degradation \(D\), while having no prior information about \(D\). While the model is initially "blind" to the nature of degradation, its performance in recovering a clean image can be enhanced by providing implicit contextual information about the type of degradation. In this paper, we present prompt learning-based image restoration framework **PromptIR**, shown in Fig. 3. Prompting is an efficient and suitable method for supplementing the model with relevant knowledge of the degradation type while recovering the clean image. The key element of PromptIR is the prompt blocks that first generate learnable prompt parameters, and then use these prompts to guide the model during the restoration process. Next, we describe the overall pipeline of our PromptIR framework and its components in detail.

**Overall pipeline.** From a given degraded input image \(^{H W 3}\), PromptIR first extracts low-level features \(}^{H W C}\) by applying a convolution operation; where \(H W\) is the spatial resolution and \(C\) denotes the channels. Next, the feature embeddings \(}\) undergo a 4-level hierarchical encoder-decoder, transforming into deep features \(}^{H W 2C}\). Each level of the encoder-decoder employs several Transformer blocks, with the number of blocks gradually increasing from the top level to the bottom level to maintain computational efficiency. Starting from the high-resolution input, the goal of the encoder is to progressively reduce the spatial resolution while increasing channel capacity, thereby yielding low-resolution latent representation \(_{l}^{ 8C}\). From the low-resolution latent features \(_{l}\), the aim of the decoder is to gradually recover the high-resolution clean output. In order to assist the decoding process, we incorporate prompt blocks in our PromptIR framework. Prompt blocks are adapter modules that sequentially connect every two levels of the decoder. At each decoder level, the prompt block implicitly enriches the input features with information about the degradation type for a guided recovery. Next, we describe the proposed prompt block and its core building modules in detail.

### Prompt Block

In NLP [3; 48; 21; 34] and vision tasks [23; 26; 18; 50], prompting-based techniques have been explored for parameter-efficient finetuning of large frozen models trained on a source task \(\) onto a target task \(\). The effective performance of prompting-based techniques is attributed to their ability to efficiently encode task-specific contextual information in prompt components. In the proposed PromptIR, prompt components are learnable parameters, that interact with the input features in order to enrich them with degradation type. Given \(N\) prompt-components \(}^{N H W C}\) and input features \(}^{ C}\), the overall process of prompt block is defined as:

\[_{l}}=((},}), }) \]

The prompt block consists of two key components: a prompt generation module (PGM) and a prompt-interaction module (PIM), each of which we describe next.

Figure 3: Overview of the PromptIR approach. We use a UNet-style network  with transformer blocks in the encoding and decoding stages. The primary component of the framework, i.e., the prompt block consists of two modules, the Prompt Generation Module (PGM) and the Prompt Interaction Module (PIM). The prompt generation module generates the input-conditioned prompt \(\), using the input features \(_{l}\) and the Prompt Components. The prompt interaction module then dynamically adapts the input features using the generated prompt through the transformer block. The prompts interact with decoder features at multiple levels to enrich the degradation-specific context.

#### 3.1.1 Prompt Generation Module (PGM)

Prompt components \(}\) form a set of learnable parameters that interact with the incoming features to embed degradation information. One straightforward method for features-prompt interaction is to directly use the learned prompts to calibrate the features. However, such a static approach may yield suboptimal results, as it is agnostic to the input content. Therefore, we present PGM that dynamically predicts attention-based weights from the input features and apply them to prompt components to yield input-conditioned prompts \(\). Furthermore, PGM creates a shared space to facilitate correlated knowledge sharing among prompt components.

To generate prompt-weights from the input features \(}\), PGM first applies global average pooling (GAP) across spatial dimension to generate feature vector \(^{}\). Next, we pass \(\) through a channel-downscaling convolution layer to obtain a compact feature vector, followed by the softmax operation, thus yielding prompt-weights \(w^{N}\). Finally, we use these weights to make adjustments in prompt components, followed by a \(3 3\) convolution layer. Overall, the PGM process is summarized as:

\[=_{3 3}(_{c=1}^{N}w_{c}_{c} ), w=(_{1 1}(( }))) \]

Since at inference time, it is necessary for the restoration network to be able to handle images of different resolutions, we cannot use the prompt components \(}\) with a fixed size. Therefore, we apply the bilinear upsampling operation to upscale the prompt components to the same size as the incoming input features.

#### 3.1.2 Prompt Interaction Module (PIM)

The primary goal of PIM is to enable interaction between the input features \(}\) and prompts \(\) for a guided restoration.

In PIM, we concatenate the generated prompts with the input features along the channel dimension. Next, we pass the concatenated representations through a Transformer block that exploits degradation information encoded in the prompts and transforms the input features.

The main contribution of this paper is the prompt block, which is a plug-in module, and architecture agnostic. Therefore, in the proposed PromptIR framework, we use an existing Transformer block , instead of developing a new one. The Transformer block is composed of two sequentially connected sub-modules: Multi-Dconv head transposed attention (MDTA), and Gated-Dconv feedforward network (GDFN). MDTA applies self-attention operation across channels rather than the spatial dimension and has linear complexity. The goal of GDFN is to transform features in a controlled manner, i.e., suppressing the less informative features and allowing only useful ones to propagate through the network. The overall process of PIM is:

\[}}=_{3 3}(([};]) \]

where [ ; ] is concatenation operation. MDTA is formulated as \(=W_{p}( /)+\). Where \(\) and \(\) are the input and output features. \(\), \(\) and \(\) respectively represent query, key, and value projections that are obtained by applying \(1 1\) point-wise convolutions followed by \(3 3\) depth-wise convolutions on the layer normalized input feature maps. \(W_{p}\) is the point-wise convolution, \(\) denotes a learnable scaling parameter, and \(()\) represents dot-product interaction. The process of GDFN is defined as \(=W_{p}^{0}((W_{d}^{1}W_{p}^{1}(()))  W_{d}^{2}W_{p}^{2}(()))+\). Where, \(W_{d}^{()}\) is the \(3 3\) depth-wise convolution, \(\) denotes element-wise multiplication, \(\) is the GELU non-linearity, and LN is the layer normalization . The block diagram and additional details on the Transformer block are provided in the appendix.

## 4 Experiments

To demonstrate the effectiveness of the proposed PromptIR, we perform the evaluation on three representative image restoration tasks: image dehazing, image deraining, and image denoising. Following , we conduct experiments under two different experimental settings: **(a)** All-in-One, and **(b)** Single-task.

In the All-in-One setting, we train a unified model that can recover images across all three degradation types. Whereas, for the Single-task setting, we train separate models for different restoration tasks. The image quality scores for the best and second-best methods are **highlighted** and underlined in the tables.

**Implementation Details.** Our PromptIR framework is end-to-end trainable and requires no pre-training of any individual component. The architecture of our PromptIR consists of a 4-level encoder-decoder, with varying numbers of Transformer blocks at each level, specifically  from level-1 to level-4.

We employ one prompt block between every two consecutive decoder levels, totaling 3 prompt blocks in the overall PromptIR network. The total number of prompt components are 5. The model is trained with a batch size of 32 in the all-in-one setting, and with a batch of 8 in the single-task setting. The network is optimized with an L\({}_{1}\) loss, and we use Adam optimizer (\(_{1}=0.9\), \(_{2}=0.999\)) with learning rate \(2e-4\) for 200 epochs. During training, we utilize cropped patches of size 128 x 128 as input, and to augment the training data, random horizontal and vertical flips are applied to the input images.

**Datasets.** We prepare datasets for different restoration tasks, following closely the prior work . For image denoising in the single-task setting, we use a combined set of BSD400  and WED  datasets for training. The BSD400 dataset contains 400 training images and the WED dataset has 4,744 images. From clean images of these datasets, we generate the noisy images by adding Gaussian noise with different noise levels \(\{15,25,50\}\). Testing is performed on BSD68  and Urban100  datasets. For single-task image deraining, we use the Rain100L  dataset, which consists of 200 clean-rainy image pairs for training, and 100 pairs for testing. Finally, for image dehazing in the single-task setting, we utilize SOTS  dataset that contains 72,135 training images

   Method & Dehazing & Deraining &  &  \\  & on SOTS  & on Rain100L  & \(=15\) & \(=25\) & \(=50\) & \\  BRDNet  & 23.23(0.895) & 27.42(0.895) & 32.62(0.898) & 29.76(0.836) & 26.34(0.836) & 27.80(0.843) \\ LPNet  & 20.84(0.828) & 24.88(0.784) & 26.47(0.778) & 24.77(0.748) & 24.12(0.652) & 23.64(0.738) \\ FDGAN  & 24.71(0.924) & 29.89(0.933) & 30.25(0.910) & 28.81(0.868) & 26.43(0.776) & 28.02(0.883) \\ MPRNet  & 25.28(0.954) & 33.57(0.954) & 33.54(0.927) & 30.89(0.880) & 27.56(0.779) & 30.17(0.899) \\ DL & 26.92(0.391) & 32.62(0.931) & 33.05(0.914) & 30.41(0.861) & 26.90(0.740) & 29.98(0.875) \\ AirNet  & 27.94(0.962) & 34.90(0.967) & 33.92(0.933) & 31.26(0.888) & 28.00(0.797) & 31.20(0.910) \\  PromptIR (Ours) & **30.58(0.974)** & **36.37(0.972)** & **33.98(0.933)** & **31.31(0.888)** & **28.06(0.799)** & **32.06(0.913)** \\   

Table 1: Comparisons under All-in-one restoration setting: single model trained on a combined set of images originating from different degradation types. When averaged across different tasks, our PromptIR provides a significant gain of \(0.86\) dB over the previous all-in-one method AirNet .

Figure 4: **Dehazing comparisons** for all-in-one methods on images from the SOTS dataset . The image quality of the results produced by our PromptIR is visually better than the previous state-of-the-art approach AirNet.

and 500 testing images. Finally, to train a unified model under the all-in-one setting, we combine all 4 aforementioned datasets and train a single model that is later evaluated on multiple tasks.

### Multiple Degradation All-in-One Results

We compare the proposed PromptIR with several general image restoration approaches [52; 17; 14; 71] as well as with specialized all-in-one methods [16; 29]. Results are reported in Table 1. When averaged across different restoration tasks, our algorithm yields \(0.86\) dB performance gain over the previous best method AirNet , and \(1.89\) dB over the second best approach DL . Specifically, the proposed PromptIR significantly advances state-of-the-art by providing \(2.64\) dB PSNR improvement on the image dehazing task. The visual examples provided in Fig. 4 show that PromptIR effectively removes haze from the input images, and generates cleaner results than AirNet . Similarly on the image deraining task as shown in Table 1, the proposed PromptIR achieves a substantial gain of \(3.73\) dB compared to DL  and \(1.47\) dB over AirNet . Visual comparisons in Fig. 5 show that PromptIR is capable of removing rain streaks of various orientations and generates visually pleasant rain-free images. Finally, on the denoising task, our method provides \(1.16\) db boost over the DL algorithm  for a high noise level of \(50\). Qualitative examples are presented in Fig. 6, where our method reproduces noise-free images with better structural fidelity than the AirNet algorithm .

Figure 5: **Image deraining comparisons** for all-in-one methods on images from the Rain100L dataset . Our method effectively removes rain streaks to generate rain-free images.

Figure 6: **Denoising results** for all-in-one methods.

### Single Degradation One-by-One Results

In this section, we evaluate the performance of our PromptIR under the single-task setting, i.e., a separate model is trained for different restoration tasks. This is to show that content-adaptive prompting via prompt block is also useful for single-task networks. Table 2 presents dehazing results. It shows that our PromptIR achieves \(8.13\) dB improvement over AirNet , and \(0.44\) dB gain over the baseline method Restormer . Similar trends can be observed for deraining and denoising tasks. For instance, when compared to the AirNet , our method yields performance gains of \(2.13\) dB on the deraining task (Table 3) and \(0.51\) dB on denoising task for noise level \(50\) on Urban100 dataset  (see Table 4).

### Ablations Studies

We perform several ablation experiments to demonstrate that our contributions in PromptIR framework provides quality improvements.

**Impact of PGM.** We carry out this ablation experiment on Rain100L  for deraining task. Table 5 shows that the prompt block in our PromptIR network brings performance gains of \(0.3\) dB over the baseline . Further, it demonstrates that generating dynamic prompts conditioned on input content via PGM provides a favorable gain of \(0.19\) dB over the fixed prompt components.

**Position of prompt blocks.** In the hierarchical architecture of our PromptIR, we analyze where to place prompt blocks on the decoder side. Table 6 shows that using only one prompt block in the latent space degrades the network's performance. Whereas, incorporating prompt blocks between every consecutive level of the decoder performs the best.

**Generalization to unseen degradation level.** We take the model that is trained only on the noise levels \(\{15,25,50\}\) and test its performance on the unseen noise level of \(=100\).

Table 7 shows that our PromptIR demonstrates significantly superior generalization capabilities compared to AirNet , yielding \(\)7 dB PSNR difference.

    &  &  \\ Method & \(=15\) & \(=25\) & \(=50\) & \(=15\) & \(=25\) & \(=50\) \\  CBM3D  & 33.50/0.922 & 30.69/0.868 & 27.36/0.763 & 33.93/0.941 & 31.36/0.909 & 27.93/0.840 \\ DnCNN  & 33.89/0.930 & 31.23/0.883 & 27.92/0.789 & 32.98/0.931 & 30.81/0.902 & 27.59/0.833 \\ IRCNN  & 33.87/0.929 & 31.18/0.882 & 27.88/0.790 & 27.59/0.833 & 31.20/0.909 & 27.70/0.840 \\ FFDNet  & 33.87/0.929 & 31.21/0.882 & 27.96/0.789 & 33.83/0.942 & 31.40/0.912 & 28.05/0.848 \\ BRDNet  & 34.10/0.929 & 31.43/0.885 & 28.16/0.794 & 34.42/0.946 & 31.99/0.919 & 28.56/0.858 \\ AirNet  & 34.14/0.936 & 31.48/0.893 & 28.23/0.806 & 34.40/0.949 & 32.10/0.924 & 28.88/0.871 \\  PromptIR(Ours) & **34.34**/**0.938** & **31.71**/**0.897** & **28.49**/**0.813** & **34.77**/0.952** & **32.49**/**0.929** & **29.39**/**0.881** \\   

Table 4: Denoising comparisons in the single-task setting on BSD68  and Urban100  datasets. For the challenging noise level of \(=50\) on Urban100 , our PromptIR obtains \(0.51\) dB gain compared to AirNet .

   Method & [DehazeNet & MSCNN & AODNet & EPDN & FDGAN & AirNet & Restormer & PromptIR (Ours) \\  PSNR & 22.46 & 22.06 & 20.29 & 22.57 & 23.15 & 23.18 & 30.87 & **31.31** \\ SSIM & 0.851 & 0.908 & 0.877 & 0.863 & 0.921 & 0.900 & 0.969 & **0.973** \\   

Table 2: Dehazing results in the single-task setting on the SOTS benchmark dataset . Our PromptIR achieves a significant boost of \(8.13\) dB over AirNet .

   Method & [DIDMDN & UMR & SIRR & MSPFN & LPNet & AirNet & Restormer & PromptIR(Ours) \\  PSNR & 23.79 & 32.39 & 32.37 & 33.50 & 33.61 & 34.90 & 36.74 & **37.04** \\ SSIM & 0.773 & 0.921 & 0.926 & 0.948 & 0.958 & 0.977 & 0.978 & **0.979** \\   

Table 3: Deraining results in the single-task setting on Rain100L . Compared to the AirNet  algorithm, the proposed method yields \(2.13\) dB PSNR improvement.

**Performance on spatially variant degradation.** Here we evaluate PromptIR performance on images that are corrupted with varying degradations. For this, we follow closely the work of AirNet , and prepare a new test set from BSD68  by applying Gaussian noise of varying levels \(=\) at different spatial locations of the images. Results in Table 8 show that our PromptIR framework is more effective in restoring these images than AirNet , providing \(0.23\) dB improvement.

**Training model with different combinations of degradation.** In Table 1, we report the results of training an all-in-one model on combined datasets from all three restoration tasks. Here, we evaluate the impact on PromptIR performance by different combinations of degradation types (tasks). Table 9) shows that with an increasing number of degradation types, it becomes increasingly difficult for the network to restore images, thereby leading to a performance drop.

Specifically, the presence of hazy images in the combined dataset seems to negatively affect the model. Interestingly, a model trained on the combination of rainy and noisy images achieves good performance, indicating a positive correlation between the deraining and denoising tasks. Such phenomenon is also observed in the AirNet work .

**Impact of Prompt Block.** To effectively evaluate the benefit of the proposed framework, we evaluate it against the Restormer model . Restormer is scaled up to match the parameter count of PromptIR. We perform this evaluation under the All-in-one setting. As shown in Table 10, the PromptIR framework provides an average improvement of \(0.38\) dB over the Restormer model .

**Number of prompt components.** We conduct an ablation study to understand the effect of the number of prompt components \(P_{c}\) employed on the performance of the framework. As shown in Table 11, we find that utilizing more than 5 components yields marginal improvements while incurring additional computational overhead. As a result, we choose to utilize five components

   Model & PSNR \\  level 4 (latent) & 36.76 \\ levels 4+3 & 36.84 \\ levels 4+3+2 & 37.04 \\   

Table 6: Prompt blocks position. Results are reported on Rain100L  dataset.

   Model & PSNR \\   Method & PSNR & PSNR \\   Airnet  & 13.64 & 13.72 \\ PromptIR (Ours) & **21.03** & **20.50** \\   

Table 7: Denoising comparisons on unseen noise level of \(=100\).

    & Degradation &  &  & Dehazing on \\ Noise & Rain & Haze & \(=15\) & \(=25\) & \(=50\) & Rain100L  & SOTS  \\  ✓ & ✗ & ✗ & 34.34/0.938 & 31.71/0.898 & 28.49/0.813 & **-** & **-** \\ ✗ & ✓ & ✗ & **-** & **-** & **-** & 37.03/0.9786 & **-** \\ ✗ & ✗ & ✓ & **-** & **-** & **-** & **-** & 31.31/0.929 \\ ✓ & ✓ & ✗ & 34.26/0.937 & 31.61/0.895 & 28.37/0.810 & 39.32/0.986 & **-** \\ ✓ & ✗ & ✓ & 33.69/0.928 & 31.03/0.880 & 27.74/0.777 & **-** & 30.09/0.975 \\ ✗ & ✓ & ✓ & **-** & **-** & **-** & 35.12/0.969 & 30.36/0.973 \\  ✓ & ✓ & ✓ & 33.98/0.933 & 31.31/0.888 & 28.06/0.799 & 36.37/0.972 & 30.58/0.974 \\   

Table 9: Performance of the proposed PromptIR framework, when trained on different combinations of degradation types (tasks) i.e., removal of noise, rain and haze.

   Method & PSNR \\   Baseline  & 36.74 \\ Fixed-Prompt & 36.85 \\ Dynamic-Prompt & 37.04 \\   

Table 5: Impact of PGM. Results are reported on Rain100L  dataset.

## 5 Conclusion

Existing image restoration models based on deep neural networks can work for specific degradation types and do not generalize well to other degradations. However, practical settings demand the ability to handle multiple degradation types with a single unified model without resorting to degradation-specific models that lack generalization and require apriori knowledge of specific degradation in the input. To this end, our work proposed a drop-in prompt block that can interact with the input features to dynamically adjust the representations such that the restoration process is adapted for the relevant degradation. We demonstrated the utility of prompt block for all-in-one image restoration by integrating it within a SoTA restoration model that leads to significant improvements on image denoising, deraining, and dehazing tasks. In the future, we will extend the model for a broader set of corruptions toward the goal of universal models for better generalization in image restoration tasks.