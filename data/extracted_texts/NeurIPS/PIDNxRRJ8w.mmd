# Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand

Junfeng Guo\({}^{1,}\), Yiming Li\({}^{2,3,}\), Lixu Wang\({}^{4}\), Shu-Tao Xia\({}^{5}\), Heng Huang\({}^{1}\), Cong Liu\({}^{6}\), Bo Li\({}^{7,8}\)

\({}^{1}\)Department of Computer Science, University of Maryland

\({}^{2}\)ZJU-Hangzhou Global Scientific and Technological Innovation Center

\({}^{3}\)School of Cyber Science and Technology, Zhejiang University

\({}^{4}\)Department of Computer Science, Northwestern University

\({}^{5}\)Tsinghua Shenzhen International Graduate School, Tsinghua University

\({}^{6}\)Department of Electronic and Computer Engineering, UC Riverside

\({}^{7}\)Department of Computer Science, University of Illinois Urbana-Champaign

\({}^{8}\)Department of Computer Science, University of Chicago

{gjf2023,heng}@umd.edu; li-ym@zju.edu.cn;

lixuwang2025@u.northwestern.edu;

xiast@sz.tsinghua.edu.cn; congl@ucr.edu; lbo@illinois.edu

The first two authors contributed equally to this work. Correspondence to Yiming Li.

###### Abstract

The prosperity of deep neural networks (DNNs) is largely benefited from open-source datasets, based on which users can evaluate and improve their methods. In this paper, we revisit backdoor-based dataset ownership verification (DOV), which is currently the only feasible approach to protect the copyright of open-source datasets. We reveal that these methods are fundamentally harmful given that they could introduce malicious misclassification behaviors to watermarked DNNs by the adversaries. In this paper, we design DOV from another perspective by making watermarked models (trained on the protected dataset) correctly classify some 'hard' samples that will be misclassified by the benign model. Our method is inspired by the generalization property of DNNs, where we find a _hardly-generalized domain_ for the original dataset (as its _domain watermark_). It can be easily learned with the protected dataset containing modified samples. Specifically, we formulate the domain generation as a bi-level optimization and propose to optimize a set of visually-indistinguishable clean-label modified data with similar effects to domain-watermarked samples from the hardly-generalized domain to ensure watermark stealthiness. We also design a hypothesis-test-guided ownership verification via our domain watermark and provide the theoretical analyses of our method. Extensive experiments on three benchmark datasets are conducted, which verify the effectiveness of our method and its resistance to potential adaptive methods. The code for reproducing main experiments is available at https://github.com/JunfengGo/Domain-Watermark.

## 1 Introduction

Deep neural networks (DNNs) have been applied to a wide range of domains and have shown human-competitive performance. The great success of DNNs heavily relies on the availability of various open-source datasets (\(e.g.\), CIFAR  and ImageNet ). With these high-quality datasets, researchers can evaluate and improve the proposed methods upon them. Currently, most of these datasets limit their usage to education or research purpose and are prohibited from commercial applications without authorization. How to protect their copyrights is of great significance [3; 4; 5; 6].

Currently, there are many classical methods for data protection, such as encryption [7; 8; 9], differential privacy [10; 11; 12], and digital watermarking [13; 14; 15; 16]. However, they are not able to protect the copyrights of open-source datasets since they either hinder the dataset accessibility (\(e.g.\), encryption) or require the information of the training process (\(e.g.\), differential privacy and digital watermarking) of suspicious models that could be trained on it.

To the best of our knowledge, backdoor-based dataset ownership verification (DOV) [3; 4; 5] is currently the only feasible approach to protect them, where defenders exploit backdoor attacks [17; 18; 19] to watermark the original dataset such that they can verify whether a suspicious model is trained on the protected dataset by examining whether it has specific backdoor behaviors. Recently, Li _et al_.  first discussed the 'harmlessness' requirement of backdoor-based DOV that the dataset watermark should not introduce new security risks to models trained on the protected dataset and proposed untargeted backdoor watermarks towards harmless verification by making the predictions of watermarked samples dispersible instead of deterministic (as a pre-defined target label).

In this paper, we revisit dataset ownership verification. We argue that backdoor-based dataset watermarks can never achieve truly harmless verification since their fundamental mechanism is making watermarked model misclassifies 'easy' samples (\(i.e.\), backdoor-poisoned samples) that can be correctly predicted by the benign model (as shown in Figure 1). It is with these particular misclassification behaviors that the dataset owners can conduct ownership verification. An intriguing question arises: _Is harmless dataset ownership certification possible to achieve_?

The answer to the aforementioned problem is positive. In this paper, we design dataset ownership verification from another angle, by making the watermarked model can correctly classify some 'hard' samples that will be misclassified by the benign model. Accordingly, we can exploit this difference to design ownership verification while not introducing any malicious prediction behaviors to watermarked models that will be deployed by dataset users (as shown in Figure 1). In general, our method is inspired by the generalization property of DNNs, where we intend to find a _hardly-generalized domain_ for the original dataset. It can be easily learned with the protected dataset containing modified samples. Specifically, we formulate the domain generation as a bi-level optimization and leverage a transformation module to generate domain-watermarked samples; We propose to optimize a set of visually-indistinguishable modified data having similar effects to domain-watermarked samples as our _domain watermark_ to ensure the stealthiness of dataset watermarking; We design a hypothesis-test-guided method to conduct ownership verification via our domain watermark at the end. We also provide theoretical analyses of all stages in our method.

In conclusion, the main contributions of this paper are four-folds: **1)** We revisit dataset ownership verification (DOV) and reveal the harmful drawback of methods based on backdoor attacks. **2)** We explore the DOV problem from another perspective, based on which we design a truly harmless DOV method via domain watermark. To the best of our knowledge, this is the first non-backdoor-based DOV method. Our work makes dataset ownership verification become an independent research field instead of the sub-field of backdoor attacks. **3)** We discuss how to design the domain watermark and provide its theoretical foundations. **4)** We conduct experiments on benchmark datasets, verifying the effectiveness of our method and its resistance to potential adaptive methods.

Figure 1: The main pipeline of dataset ownership verification with backdoor-based dataset watermarks and our domain watermark, where BW Sample represents existing backdoor-watermarked sample while DW Sample represents our proposed domain-watermarked sample. Existing backdoor-based methods make the watermarked model (\(i.e.\), the backdoored DNN) misclassify ‘easy’ samples that can be correctly predicted by the benign model and therefore the verification is harmful. In contrast, our ownership verification is harmless since we make the watermarked model correctly predict ‘hard’ samples that are misclassified by the benign model.

## 2 Related Work

### Backdoor Attacks

Backdoor attack2[23; 24; 25] is a training-phrase threat of DNNs, where the adversary intends to implant a _backdoor_ (\(i.e.\), the latent connection between the adversary-specified trigger pattern and the target label) into the victim model by maliciously manipulating a few training samples. The backdoored DNNs behave normally while their predictions will be maliciously changed to the target label whenever the testing samples contain the trigger pattern. In general, existing backdoor attacks can be divided into two main categories based on the property of the target label, as follows:

**Poisoned-Label Backdoor Attacks.** In these attacks, the target label of poisoned samples is different from their ground-truth labels. This is the most classical attack paradigm and is more easily to implant hidden backdoors. For example, BadNets  is the first backdoor attack, where the adversaries randomly modify a few samples from the original dataset by attaching a pre-defined trigger patch to their images and changing their labels to the target label. These modified samples (dubbed _poisoned samples_) associated with remaining benign samples are packed as the _poisoned dataset_ that is released to victim users for training; After that, Chen _et al_.  improved the stealthiness of BadNets by introducing trigger transparency; Nguyen _et al_.  proposed a more stealthy backdoor attack whose trigger patterns were designed via image-warping; Recently, Li _et al_.  proposed the first untargeted (poisoned-label) backdoor attack (\(i.e.\), UBW-P) for dataset ownership verification.

**Clean-Label Backdoor Attacks.** In these attacks, the target label of poisoned samples is consistent with their ground-truth labels. Accordingly, these attacks are more stealthy, compared to the poisoned-label ones. However, they usually suffer from low effectiveness, especially on datasets with a high image resolution or many classes, due to the _antagonistic effects_ of 'robust features' related to the target class contained in poisoned samples . Label-consistent attack is the first clean-label attack where the adversaries introduced untargeted adversarial perturbations before adding trigger patterns; After that, a more effective attack (\(i.e.\), Sleeper Agent ) is proposed, which crafts clean-label poisoned samples via bi-level optimization; Recently, Li _et al_.  proposed UBW-C, which generated poisoned samples for leading untargeted misclassifications to attacked DNNs.

### Data Protection

**Classical Data Protection.** Data protection is a classical and important research direction, aiming to prevent unauthorized data usage or protect data privacy. Currently, existing classical data protection can be roughly divided into three main categories, including **(1)** encryption, **(2)** digital watermarking, and **(3)** privacy protection. Specifically, encryption [30; 7; 8] encrypts the whole or parts of the protected data so that only authorized users who hold a secret key for decryption can use it; Digital watermarking [31; 32; 33] embeds an owner-specified pattern to the protected data to claim the ownership; Privacy protection focuses on preventing the leakage of sensitive information of the data in both empirical [34; 35; 36] and certified manners [10; 37; 12]. However, these traditional approaches are not feasible to protect the copyright of open-source datasets since they either hinder the dataset accessibility or require the information of the training process that will not be disclosed.

**Dataset Ownership Verification.** Dataset ownership verification (DOV) is an emerging topic in data protection, aiming to verify whether a given suspicious model is trained on the protected dataset. To the best of our knowledge, this is currently the only feasible method to protect the copyright of open-source datasets. Specifically, it intends to implant specific prediction (towards verification samples) behaviors in models trained on the protected dataset while not reducing their performance on benign samples. Dataset owners can conduct ownership verification by examining whether the suspicious model has these behaviors. Currently, all DOV methods [3; 4; 5] exploit backdoor attacks to watermark the unprotected benign dataset. For example,  adopted poisoned-label backdoor attacks while  adopted clean-label ones for dataset watermarking. Recently, Li _et al_.  first discussed the 'harmlessness' requirement of DOV that the dataset watermark should not introduce new security risks to models trained on the protected dataset and proposed the untargeted backdoor watermarks. However, there is still no definition of harmlessness and backdoor-based DOV methodscan never achieve truly harmless verification for they introduce backdoor threats. How to design a harmless DOV method is still an important open question.

## 3 Domain Watermark

### Preliminaries

**Threat Model.** Following existing works in dataset ownership verification [3; 4; 5], we assume that the defenders (\(i.e.\), dataset owners) can only watermark the _benign dataset_ to generate the _protected dataset_. They will release the protected dataset instead of the original benign dataset for copyright protection. Given a third-party suspicious model that may be trained on the protected dataset without authorization, we consider the _black-box setting_ where defenders have no information about other training configurations (\(e.g.\), loss function and model architecture) of the model and can only access it to obtain predicted probability vectors via its model API.

**The Main Pipeline of Dataset Watermark.** Let \(=\{(_{i},y_{i})\}_{i=1}^{N}\) denotes the benign training dataset. Let we consider an image classification task with \(K\)-classes, \(i.e.\), \(_{i}=^{C W H}\) represents the image with \(y_{i}=\{1,,K\}\) as its label. Instead of releasing \(\) directly, the dataset owner will generate and release its watermarked version (\(i.e.\), \(_{w}\)). Specifically, \(_{w}=_{m}_{b}\), where \(_{m}\) consists of the modified version of samples from a small selected subset \(_{s}\) of \(\) (\(i.e.\), \(_{s}\)) and \(_{b}\) contains remaining benign samples (\(i.e.\), \(_{b}=-_{s}\)). The \(_{m}\) is generated by the defender-specified image generator \(G_{x}:\) and the label generator \(G_{y}:\), \(i.e.\), \(_{m}=\{(G_{x}(),G_{y}(y))|(,y)_{s}\}\). For example, \(G_{x}=(-)+\) and \(G_{y}=y_{t}\) in BadNets , where \(\{0,1\}^{C W H}\) is the trigger mask, \(^{C W H}\) is the trigger pattern, \(\) denotes the element-wise product, and \(y_{t}\) is the target label. In particular, \(_{m}|}{|_{w}|}\) is called the _watermarking rate_. All models trained on the protected dataset \(_{w}\) will have special prediction behaviors on \(G_{x}()\) for ownership verification. Specifically, let \(C:\) denotes a third-party suspicious model that could be trained on the protected dataset, existing backdoor-based methods will examine whether it conduct unauthorized training by testing whether \(C(G_{x}())=y_{t}\). Since \(y_{t} y\) in most cases, these backdoor-based watermarks are harmful.

### Problem Formulation

As described in previous sections, existing backdoor-inspired dataset ownership verification (DOV) methods [3; 4; 5] would cause malicious misclassification on watermarked samples to all models trained on the protected dataset, therefore they are harmful. _This limitation of backdoor-based DOV methods cannot be eliminated_ because their inherent mechanism is to lead the watermarked model to have particular mispredicted behaviors for verification, although the misclassification could be random and less harmful . In this paper, we intend to _design a truly harmless DOV method so that the watermarked models will correctly classify watermarked samples_. Before we formally define the studied problem, we first provide the definition of harmful degree of a DOV method.

**Definition 1** (Harmful and Relatively Harmful Degree).: _Let \(}=\{(}_{i},y_{i})\}_{i=1}^{N}\) indicates a set of watermarked samples used for ownership verification of a DOV method, where \(}_{i}\) is the verification sample with \(y_{i}\) as its ground-truth label (instead of its given label). Let \(\) and \(\) represent a classifier trained on the protected and unprotected datasets, respectively. The harmful degree is \(H_{i=1}^{N}\{(}_{i}) y _{i}\}\) and the relatively harmful degree is \((_{i=1}^{N}\{(}_{i}) y_{i}\}-_{i=1}^{N}\{C(_{i}}) y_{i}\})\) where \(\{\}\) is the indicator function._

To design a harmless DOV method, we intend to make watermarked DNNs correctly classify some 'hard' samples that will be misclassified by the model trained on the unprotected benign dataset. Inspired by the generalization property of DNNs, we intend to find a _hardly-generalized domain_ of the benign dataset, which can be easily learned with the protected dataset containing the modified samples. In this paper, we call this watermarking method as _domain watermark_, defined as follows.

**Definition 2** (Domain Watermark).: _Given a benign dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\), let \(C:\) denotes a model trained on \(\). Assume that \(G_{d}\) denotes a domain generator such that \(G_{d}(_{i})\) owns the same ground-truth label as \(_{i}\) but belongs to a hardly-generalized domain, \(i.e.\), \(_{(_{i},y_{i})}\{C(_{i})=y_{i}\} _{(_{i},y_{i})}\{C(G_{d}(_{i}))=y_{i}\}\). We intend to find a watermarkedversion of \(\) (i.e., \(_{d}\)) with watermarking rate \(\), such that the watermarked model \(\) trained on it have two properties:_ **(1)**_\(_{(_{i},y_{i})}\{(_{i})= y_{i}\}\) and_ **(2)**_\(_{(_{i},y_{i})}(\{(_{ i})=y_{i}\}-\{(G_{d}(_{i}))=y_{i}\})\), where \(,\) are given parameters. In this paper, \(_{d}\) is defined as the domain watermark of the benign dataset \(\)._

### Generating the Hardly-Generalized Domain

As illustrated in Definition 2, finding a hardly-generalized target domain \(\) (with domain generator \(G_{d}\)) of the source domain \(\) is the first step of our domain watermark. To guide the construction of the domain \(\), we have the following Lemma 1.

**Lemma 1** (Generalization Bound ).: _The bound of expected risk on a given target domain \(\) is negatively associated with mutual information between features for source \(\) and target \(\) domains:_

\[_{}(f)_{}(f)-4I(;})+4H(Y)+d_{}(p(),p( })),\] (1)

_where \(_{}(f)=_{(},y)}[ \{C(}) y\}]\), \(_{}(f)=_{(,y)}[ \{C() y\}]\). \(I(;})\) is mutual information between features from \(\) and \(\). \(d_{}(p(),p(}))\) is \(\)-divergence for measuring the divergence of feature marginal distributions of two domains, and \(H()\) is the entropy._

Lemma 1 reveals the upper bound of generalization performance on \(\). Since \(d_{}()\) is intractable and hard to directly optimize, as well as  shows that only a \(I()\) is enough for generalization across domains, _we propose to increase the expected risk on \(\) by minimizing \(I(;})\)_.

Specifically, we formulate the design of the target domain \(\) (with the domain generator \(G_{d}(;)\)) as a bi-level optimization, as follows:

\[_{}_{p(,})}\ [I((});}(,}))+_{1}_{c}( (}),}(,}))],\] (2) \[s.t.\ }=_{}[_{(,y) }[(f(G_{d}(;);),y)+ (f(;),y)]-_{2}_{p(,})}[I(();}())].,\]

where \(_{1}\), \(_{2}\) are two positive hyper-parameters, and \(()\) is the loss function (\(e.g.\), cross entropy).

Following previous works  in domain adaption and generalization, we propose to optimize the upper bound approximation for \(I(;})\) instead of itself and leverage a transformation module consisting of multiple convolutional operations as \(G_{d}(;)\) to generate the domain-watermarked image \(}\). Specifically, we aim to craft \(}\) via minimizing the upper bound approximation for mutual information  between \(\) and \(}\) in the latent feature space \(\):

\[I(;})=_{p(,})}[\ }|)}{p(})}]_{p(,})}[\ p(}|)]-_{p()p( })}[\ p(}|)],\] (3)

where \(\) and \(}\) are the latent vectors obtained by passing \(\) and \(}\) through \(f(;)\)'s feature extractor. \(_{c}()\) is the class-conditional maximum mean discrepancy (MMD) computed on the latent space \(\) and proposed to limit the potential semantic information distortion between \(\) and \(}\), follows:

\[_{c}(z,)=_{j=1}^{K}(\|^{j}} _{i=1}^{n_{s}^{j}}(_{i}^{j})-^{j}}_{i=1}^{n_{t }^{j}}(}_{i}^{j})\|^{2}),\] (4)

where \(n_{s}^{j}\), \(n_{t}^{j}\) represent the number for \(\) and \(}\) from class \(j\), and \(()\) is the kernel function.

The configurations, parameter selections, and model architectures are included in Appendix A.

### Generating the Protected Dataset

Once we obtain the hard-generalized domain generator \(G_{d}\) with the method proposed in Section 3.3, the next step is to generate the protected dataset based on it. Before we present its technical details, we first deliver some insight into the data quantity impact for the domain watermark.

**Theorem 1** (Data Quantity Impact).: _Suppose in PAC Bayesian , for a target domain \(\) and a source domain \(\), any set of voters (candidate models) \(\), any prior \(\) over \(\) before any training, any \((0,1]\), any \(c>0\), with a probability at least \(1-\) over the choices of \(S^{n_{s}}\) and \(T_{}^{n_{t}}\), for the posterior \(f\) over \(\) after the joint training on \(S\) and \(T\), we have_

\[_{}(f) )}}_{T}(f)+}_{}(\|)}_{S}(f)+\] (5) \[+}(}+( \|)}{n_{s}})(2(f\|)+),\]

_where \(}_{T}(f)\) and \(}_{S}(f)\) are the target and source empirical risks measured over target and source datasets \(T\) and \(S\), respectively. \(\) is a constant and \(()\) is the Kullback-Leibler divergence. \(_{}(\|)\) is a measurement of discrepancy between \(\) and \(\) defined as_

\[_{}(\|)=_{(,y)( )}(_{(,y)}}{_ {(,y)}}) 1,\] (6)

_where \(()\) denotes the support of \(\). When \(\) and \(\) are identical, \(_{}(\|)=1\)._

Theorem 1 reveals the upper bound of \(_{}(f)\) is negatively associated with the number of samples for source and target domains (\(i.e.\), \(n_{t}\) and \(n_{s}\)). Assuming \(n_{t}\) is fixed, increasing \(n_{s}\) can still increase generalization on the target domain. As such, it is possible to combine some domain-watermarked samples with benign samples to achieve target domain generalization. Its proof is in Appendix B.

In general, the most straightforward method to generate our domain watermark for the protected dataset is to _randomly select a few samples \((,y)\) from the original dataset \(\) and replace them with their domain-watermarked version_\((G_{d}(),y)\). However, as we will show in the experiments, the domain-watermarked image is usually significantly different from its original version. Accordingly, the adversaries may notice watermarked samples and try to remove them to bypass our defense. To ensure the stealthiness of our domain watermark, we propose to _optimize a set of visually-indistinguishable modified data \(\{(_{i}^{},y_{i})|_{i}^{}=_{i}+_{i}\}\) having similar effects to domain-watermarked samples_. This is also a bi-level optimization problem, as follows.

\[_{}\;[_{(},y) }[(f(};()),y)]- _{3}\{_{(},y)}}[(f(};()),y)], _{4}\}],\] (7)

\[s.t.\;()=_{}[_{s}|} _{(_{i},y_{i})_{s}}(f(_{i}+_{i};),y_{i})+_{b}|}_{(_{j}, y_{j})_{b}}(f(_{j};),y_{j}) ],\]

where \(_{(},y)}}[ (f(};()),y)]\) represents the expected risk for the watermarked model on other unseen domains (\(i.e.\),\(}\)) and \(=\{:\|\|_{}\}\) where \(\) is a visibility-related hyper-parameter.

The second term in Eq.(7) is to prevent the watermarked model can achieve a similar generalization performance on other unseen domains as the target domain \(\) to preserve the uniqueness of \(\) for verification purposes. We introduce two parameters \(_{3}\) and \(_{4}\) for preventing the second term dominant in the optimization procedure. \(_{4}\) is set as \(_{(},y)}}[ (f(};^{}),y)]\), where \(^{}\) is obtained by training on the original dataset \(\). Please find more optimization details in Appendix C.

In particular, our domain watermark is clean-label, \(i.e.\), we do not modify the label of modified samples as have done in most backdoor-based methods. As such, it is more stealthy.

## 4 Dataset Ownership Verification via Domain Watermark

In this section, we introduce how to conduct dataset ownership verification via our domain watermark. The overview of the entire procedure is shown in Figure 2.

As described in Section 3.2, models trained on our protected dataset (with domain watermark) can correctly classify some domain-watermarked samples while other benign models cannot. Accordingly, given a suspicious third-party model \(f\), the defenders can verify whether it was trained on the protected dataset by examining whether the model has similar prediction behaviors on benign samples and their domain-watermarked version. _The model is regarded as trained on the protecteddataset if it has similar behaviors_. To verify it, we design a hypothesis-test-guided method following previous works [3; 4], as follows.

**Proposition 1**.: _Suppose \(f()\) is the posterior probability of \(\) predicted by the suspicious model. Let variable \(\) denotes the benign image and variable \(^{}\) is its domain-watermarked version (\(i.e.\), \(^{}=G_{d}()\)), while variable \(P_{b}=f()_{Y}\) and \(P_{d}=f(^{})_{Y}\) indicate the predicted probability on the ground-truth label \(Y\) of \(\) and \(^{}\), respectively. Given the null hypothesis \(H_{0}:P_{b}=P_{d}+\) (\(H_{1}:P_{b}<P_{d}+\)) where the hyper-parameter \(\), we claim that the suspicious model is trained on the protected dataset (with \(\)-certainty) if and only if \(H_{0}\) is rejected._

In practice, we randomly sample \(m\) different benign samples to conduct the pairwise T-test  and calculate its p-value. The null hypothesis \(H_{0}\) is rejected if the p-value is smaller than the significance level \(\). Besides, we also calculate the _confidence score_\( P=P_{b}-P_{d}\) to represent the verification confidence. _The smaller the \( P\), the more confident the verification._

**Theorem 2**.: _Let \(f()\) is the posterior probability of \(\) predicted by the suspicious model, variable \(\) denotes the benign sample with label \(Y\), and variable \(^{}\) is the domain-watermarked version of \(\). Assume that \(P_{b} f()_{Y}>\). We claim that dataset owners can reject the null hypothesis \(H_{0}\) at the significance level \(\), if the verification success rate (VSR) \(V\) of \(f\) satisfies that_

\[(V-+)-t_{}}>0,\] (8)

_where \(t_{}\) is \(\)-quantile of t-distribution with \((m-1)\) degrees of freedom and \(m\) is sample size._

In general, Theorem 2 indicates that our dataset verification can succeed if the VSR of the suspicious model \(f\) is higher than a threshold (which is not necessarily 100%). In particular, the assumption of Theorem 2 can be easily satisfied by using benign samples that can be correctly classified with high confidence. Its proof is included in Appendix D.

## 5 Experiments

In this section, we conduct experiments on CIFAR-10  and Tiny-ImageNet  with VGG  and ResNet , respectively. Results on STL-10  are in Appendix F.

### The Performance of Domain Watermark

**Settings.** We select seven baseline methods, containing three clean-label backdoor watermarks (\(i.e.\), Label-Consistent, Sleeper Agent, and UBW-C) and four poisoned-label watermarks (\(i.e.\), BadNets, Blended, WaNet, and UBW-P). Following the previous work , we set the watermarking rate \(=0.1\), perturbation constraint \(=16/255\) in all cases, and adopt the same watermark patterns and parameters. The example of samples used in different watermarks is shown in Figure 3. For our

Figure 2: The workflow of dataset ownership via our domain watermark. In the first step, we will generate domain-watermarked (DW) samples in a hardly-generalized domain of the benign dataset; In the second step, we will optimize a set of visually-indistinguishable modified samples that have similar effects to domain-watermarked samples. We will release those modified samples associated with remaining benign samples instead of the original dataset for copyright protection; In the third step, we identify whether a given third-party model is trained on our protected dataset by testing whether it has similar prediction behaviors in benign images and their DW version.

[MISSING_PAGE_FAIL:8]

'Independent-M'), and **3)** unauthorized dataset training (dubbed 'Malicious'). In the first case, we used domain-watermarked samples to query the suspicious model trained with modified samples from another domain; In the second case, we test the benign model with our domain-watermarked samples; In the last case, we test the domain-watermarked model with corresponding domain-watermarked samples. Notice that only the last case should be regarded as having unauthorized dataset use. More setting detail are described in Appendix G.

**Evaluation Metrics.** Following the settings in , we use \( P[-1,1]\) and \(p\)-value \(\) for the evaluation. For the first two independent scenarios, a large \( P\) and \(p\)-value are expected. In contrast, for the third scenario, the smaller \( P\) and \(p\)-value, the better the verification.

**Results.** As shown in Table 2, our method can achieve accurate verification in all cases. Specifically, our approach can identify the unauthorized dataset usage with high confidence (\(i.e.\), \( P 0\) and \(p\)-value \(\) 0.01), while not misjudging when there is no unauthorized dataset utilization (\(i.e.\), \( P 0\) and \(p\)-value \(\) 0.05). Especially on the CIFAR-10 dataset (with high VSR), the \(p\)-values of independent cases are already 1 while that of the malicious scene is 50 powers smaller than a correct verification needs. These results verify the effectiveness of our dataset ownership verification.

### Discussions

#### 5.3.1 Ablation Studies

We hereby discuss the effects of two key hyper-parameters involved in our method (\(i.e.\), \(\) and \(\)). Please find more experiments regarding other parameters and detailed settings in Appendix I.

**Effects of Perturbation Budget \(\).** We study its effects on both CIFAR-10 and Tiny-ImageNet datasets. As shown in Figure 5, the VSR increases with the increase of \(\). In contrast, the BA remains almost stable with different \(\). However, increasing \(\) would also reduce the invisibility of modified samples. Defenders should assign it based on their specific needs.

**Effects of watermarking Rate \(\).** As shown in Figure 5, similar to the phenomena of \(\), the VSR increases with the increase of \(\) while the BA remains almost unchanged on both datasets. In particular, even with a low watermarking rate (\(e.g.\), 1%), our method can still have a promising VSR. These results verify the effectiveness of our domain watermark again.

#### 5.3.2 The Resistance to Potential Adaptive Methods

We notice that the adversaries may try to detect or even remove our domain watermark based on existing methods in practice. In this section, we discuss whether our method is resistant to them.

Due to the limitation of space, following the previous work , we only evaluate the robustness of our domain watermark under fine-tuning  and model pruning  in the main manuscript. As shown in Figure 5, fine-tuning has minor effects on both the VSR and the BA of our method. Our method is also resistant to model pruning for the BA decreases with the decrease of VSR. We have also evaluated our domain watermark to more other representative adaptive methods. Please find more setting details and results in our Appendix J.

#### 5.3.3 A Closer Look to the Effectiveness of our Method

In this section, we intend to further explore the mechanisms behind the effectiveness of our domain watermark. Specifically, we adopt t-SNE  to visualize the feature distribution of different types of samples generated by the benign model and its domain-watermarked version. As shown in Figure 8, the domain-watermarked samples stay away (with the normalized distance as 1.84) from those with their ground-truth label (\(i.e.\), '0'), although they still cluster together, under the benign model. In contrast, these domain-watermarked samples lay close (with the normalized distance as 0.40) to benign samples having the same class under the watermarked model. These phenomena are consistent with the predictive behaviors of the two models and can partly explain the mechanism of our domain watermark. We will further explore it in our future works.

## 6 Conclusion

In this paper, we revisited the dataset ownership verification (DOV). We revealed the harmful nature of existing backdoor-based methods because their principle is making watermarked models misclassify 'easy' samples. To design a genuinely harmless DOV method, we proposed the domain watermark by leading watermarked DNNs to correctly classify some defender-specified 'hard' samples. We provided the theoretical analyses of our domain watermark and its corresponding ownership verification. We also verified its effectiveness on benchmark datasets. As the first non-backdoor-based method, our method can provide new angles and understanding to the design of dataset ownership verification to facilitate trustworthy dataset sharing.