# What Truly Matters in Trajectory Prediction for Autonomous Driving?

Phong Tran\({}^{1}\)1 Haoran Wu\({}^{1,2}\)1 Cunjun Yu\({}^{1}\)1 Panpan Cai\({}^{3}\) Sifa Zheng\({}^{2}\) David Hsu\({}^{1}\)

\({}^{1}\) National University of Singapore

\({}^{2}\) Tsinghua University

\({}^{3}\) Shanghai Jiao Tong University

Equal contribution.

###### Abstract

Trajectory prediction plays a vital role in the performance of autonomous driving systems, and prediction accuracy, such as average displacement error (ADE) or final displacement error (FDE), is widely used as a performance metric. However, a significant disparity exists between the accuracy of predictors on fixed datasets and driving performance when the predictors are used downstream for vehicle control, because of a _dynamics gap_. In the real world, the prediction algorithm influences the behavior of the ego vehicle, which, in turn, influences the behaviors of other vehicles nearby. This interaction results in predictor-specific dynamics that directly impacts prediction results. In fixed datasets, since other vehicles' responses are predetermined, this interaction effect is lost, leading to a significant dynamics gap. This paper studies the overlooked significance of this dynamics gap. We also examine several other factors contributing to the disparity between prediction performance and driving performance. The findings highlight the trade-off between the predictor's computational efficiency and prediction accuracy in determining real-world driving performance. In summary, an _interactive_, _task-driven_ evaluation protocol for trajectory prediction is crucial to capture its effectiveness for autonomous driving. Source code along with experimental settings is available online.

## 1 Introduction

Current trajectory prediction evaluation [29; 9; 5] relies on real-world datasets, operating under the assumption that dataset accuracy is equivalent to prediction capability. We refer to this as _Static Evaluation_. This methodology, however, falls short when the predictor serves as a sub-module for downstream tasks in Autonomous Driving (AD) [28; 24]. As illustrated in Figure 1, the static evaluation metrics on datasets, such as Average Displacement Error (ADE) and Final Displacement Error (FDE), do not necessarily reflect the actual driving performance [36; 8; 10]. In contrast to the conventional focus on uncertainty [39; 16] and potential interaction , we demonstrate that this disparity stems from the overlooked dynamics gap between fixed datasets and AD systems and the computational efficiency of predictors. The trade-off between these two factors matters in the actual prediction performance within the entire AD system.

The dynamics gap arises from the fact that the behavior of the autonomous vehicle, also known as the ego-agent, varies with different trajectory predictors, as presented in Figure 2. In real-world scenarios, the ego-agent utilizes predictors to determine its actions. Different predictors result in varied behaviors of the ego-agent, which, in turn, influence the future behaviors of other road users, leading to different dynamics within the environment. This directly affects the accuracy of predictions, as other agents behave differently. Since the ego-agent's actions are predetermined onthe dataset, there exists a significant disparity between the dynamics represented in the dataset and the actual driving scenario when evaluating a specific trajectory predictor. To tackle this issue, we propose the use of an interactive simulation environment to evaluate the predictor for downstream decision-making. This environment enables _Dynamic Evaluation_ as the ego-agent operates with the specific predictor, thus, mitigating the dynamics gap. We demonstrate a strong correlation between the dynamic evaluation metrics and driving performance through extensive experiments. This underscores the dominant role of the dynamics gap in aligning prediction accuracy with driving performance and emphasizes the importance of incorporating it into the evaluation process.

The awareness of the dynamics gap significantly enhances the correlation between prediction accuracy and driving performance. However, there are still factors that affect driving performance apart from prediction accuracy. The downstream modules in AD systems, with different tasks and varying levels of complexity, impose different requirements on prediction models. In our experiments, we vary the type of planner and the time constraint for the motion planning task. Our findings suggest that predictors' computational efficiency plays a vital role in real-time tasks. Moreover, it is the trade-off between prediction accuracy and computational efficiency that determines driving performance. This highlights the necessity for task-driven prediction evaluation.

In this paper, we aim to address two specific aspects of trajectory prediction for AD systems. Firstly, we uncover the limitation of static prediction evaluation systems in accurately reflecting driving performance. We demonstrate the dominant role of the dynamics gap in causing the disparity. Secondly, we emphasize the necessity of an interactive, task-driven evaluation protocol that incorporates the trade-off between dynamic prediction accuracy and computational efficiency. The protocol presents a promising way for applying prediction models in real-world autonomous driving, which is the truly mattered aspect in trajectory prediction for autonomous driving.

## 2 Related Work

### Motion Prediction and Evaluation

Motion prediction methods can be classified along three dimensions : modeling approach, output type, and situational awareness. The modeling approach includes physics-based models [30; 3]

Figure 1: Prediction accuracy _vs_ driving performance. Contrary to popular belief (left, black curves), our study indicates no strong correlation between common prediction evaluation metrics and driving performance (left, red curves). Eight representative prediction models are selected: Constant Velocity (CV) , Constant Acceleration (CA) , K-Nearest Neighbor (KNN) , Social KNN (S-KNN) , HiVT , LaneGCN , LSTM, and Social LSTM (S-LSTM) . The definition of driving performance metrics can be found in Section 4.4. Details can be found in the supplementary materials.

Figure 2: Dynamics Gap. (\(a\)) In static evaluation, the agent’s motion is determined and unaffected by predictors. (\(b\)) In the real-world, different predictors result in varied behaviors of the agent, which directly affects the ground truth of prediction.

that use physics to simulate agents' forward motion, and learning-based models [40; 22] that learn and predict motion patterns from data. The output type can be intention, single-trajectory , multi-trajectory , or occupancy map [14; 15]. These outputs differ in the type of motion they predict and how they handle the uncertainty of future states. The situational awareness includes unawareness, interaction , scene, and map awareness . It refers to the predictor's ability to incorporate environmental information, which is crucial for collision avoidance and efficient driving. Most researchers [29; 25] and competitions [9; 5] evaluate the performance of prediction models on real-world datasets, in which ADE/FDE and their probabilistic variants minADE/minFDE are commonly used metrics. However, these metrics fail to capture the dynamics gap between datasets and real-world scenarios, as the actions of the ego-agent remain unaffected by predictors. In this study, we select four model-based and six learning-based models with varying output types and situational awareness to cover a wide range of prediction models. We implement these predictors in both datasets and an interactive simulation environment to illustrate the limitation of current prediction evaluation in accurately reflecting driving performance, due to the neglect of the dynamics gap.

### Task-aware Motion Prediction and Evaluation

Task-aware motion prediction remains an underexplored area in research. While some studies touch upon the subject, they focus on proposing task-aware metrics for training or eliminating improper predictions on datasets. One notable example of training on task-aware metrics is the Planning KL Divergence (PKL) metric . Although designed for 3D object detection, it measures the similarity between detection and ground truth by calculating the difference in ego-plan performance. In the context of motion prediction, Rowan et al.  propose a control-aware metric (CAPO) similar to PKL. CAPO employs attention  to find correlations between predicted trajectories, assigning higher weights to agents inducing more significant reactions. Similarly, the Task-Informed method  assumes a set of candidate trajectories from the planner and adds the training loss for each candidate. Another line of work focuses on designing task-aware functions to eliminate improper predictions [21; 13]. The proposed metric can capture unrealistic predictions and better correlate with driving performance in planner-agnostic settings. However, these works are conducted in an open-loop manner, neglecting the disparity between open-loop evaluation and real-world driving performance. To the best of our knowledge, we are the first to conduct a thorough investigation into the disparity and demonstrate the dominant role of the dynamics gap on this issue.

## 3 Planning with Motion Prediction

### Problem Formulation

In this section, we present the problem formulation for motion prediction with planning in the context of autonomous driving. Different from conventional formulations that consider the planner as a policy, our formulation employs the predictor as the policy. This enables us to identify the primary challenge in developing predictors for real-world autonomous driving. We define the prediction problem as an MDP: \(M=(,,,)\).

**State.** The state of the system is represented by the tuple \(s=(s^{ego},s^{exo})\) for \(s\), where \(s^{ego}\) denotes the state of the AV including historical information, \(s^{exo}\) denotes that of surrounding traffic participants. Specifically, \(s^{ego}=\{s^{ego}_{t-t_{obs}},,s^{ego}_{t}\}\), where \(t_{obs}\) represents the observation horizon and \(t\) denotes the current timestep. The state for each vehicle includes its position, velocity, heading, and other relevant attributes, e.g., \(s^{ego}_{t}=(x^{ego}_{t},y^{ego}_{t},v^{ego}_{t},^{ego}_{t})\).

**Action.** At each time step, the AV can take an action \(a\), where \(\) is the action space. We denote the \(n\) surrounding traffic participants as \(i\{1,,n\}\). The action consists of the AV's prediction of all exo-agents, e.g., \(a=\{a^{1},,a^{n}\}\).

**Transition Function.** The transition function \(:\) defines the dynamics of the system and how the system evolves as a result of the action of the ego vehicle. In the MDP, it refers to: \(T(s_{t+1}|s_{t},a_{t})=T_{1}(s^{ego}_{t+1}|s_{t},a_{t})T_{2}(s^{exo}_{t+1}|s_ {t}),T\). We denote the planner as a mapping function \(T_{1}\) that takes the current state \(s_{t}\) and action \(a_{t}\) as input and outputs the AV's next state \(s^{ego}_{t+1}\). The motion models of exo-agents are integrated into the mapping function \(T_{2}\), which takes the current state \(s_{t}\) as input and generates the next state \(s^{exo}_{t+1}\) for all exo-agents.

**Predictor.** The predictor is represented as a policy \(a=(s)\) that takes the current state \(s\) as input, and produces an action \(a\) as output.

**Objective Function.** The objective of prediction is to accurately predict exo-agents future states while considering their interactions and map information. The reward function \(R:\) maps a state-action pair to a real-valued reward. The objective function is the accumulated reward over the task horizon \(H\), defined as \(J(s,a)=_{t=0}^{H}R(s_{t},a_{t})\).

**Optimal Policy.** The objective of autonomous driving is to identify the optimal policy that minimizes the objective function under the real-world transition function. The optimal policy \(^{*}\) is expressed as: \(^{*}==T_{1}^{*},T_{2}=T_{2}^{*}}{}_{}[ _{t=0}^{H}R(s_{t},a_{t})]\), where \(T_{1}^{*}\) represents any realistic planners used in AVs, and \(T_{2}^{*}\) denotes the integrated motion model of exo-agents in the real world.

### The Limitation of Static Evaluation

Traditional prediction methods overlook the difference between the transition function \(T_{1}^{*}\) in real-world autonomous driving and the represented \(_{1}\) in datasets. These methods focus on training a policy using well-designed reward functions to perform effectively within the state distribution of datasets. The trained policy \(\) satisfies:

\[==_{1},T_{2}=T_{2}^{*}}{}_{}[ _{t=0}^{H}R(s_{t},a_{t})].\] (1)

In this context, \(_{1}\) serves as a static planner that remains unaffected by predictors. Regardless of the input, the static planner \(_{1}\) generates the recorded next state. As a result, the mapping function \(T_{2}^{*}\) remains consistent with the real world since the input state is unchanged. We define the disparity in future states resulting from changes in the ego-planner as the dynamics gap in static evaluation, denoted as \(G=T_{1}^{*}-_{1}\).

The proposed planning-aware metrics, such as PKL  and CAPO , aimed to address the disparity between prediction accuracy and the ultimate driving performance by improving the cost function. However, unless the dynamics gap is mitigated, the optimization deviation plays a dominant role in causing the disparity and should be addressed as a priority.

### Measuring the Impact of Dynamics Gap

Conducting real-world tests is the most effective way to address and assess the impact of the dynamics gap. However, due to its high cost, the simulator serves as a proxy. The ego-agent utilizes the realistic planner \(T_{1}^{*}\) and the motion model \(T_{sim}^{*}\) of the simulator. The optimal policy \(_{sim}^{*}\) satisfies:

\[_{sim}^{*}==T_{1}^{*},T_{2}=T_{2}^{sim}}{} _{}[_{t=0}^{H}R(s_{t},a_{t})].\] (2)

To evaluate the impact of the dynamics gap, we train various predictors on the Alignment dataset obtained from the simulator, the trained policy \(_{sim}\) satisfies:

\[_{sim}==_{1},T_{2}=T_{2}^{sim}}{} _{}[_{t=0}^{H}R(s_{t},a_{t})].\] (3)

The equations suggest that the dynamics gap between the Alignment dataset and simulator mirrors the dynamics gap between the real-world dataset and actual autonomous driving scenarios. Thus, it is reasonable to employ the simulator for evaluating the influence of the dynamics gap. However, the simulator merely serves as a substitute to mitigate the dynamics gap, leaving a remaining dynamics gap represented by \(G^{}=T_{2}^{*}-T_{2}^{sim}\).

## 4 Experimental Setup

Our aim is to identify the key factors involved in trajectory prediction for autonomous driving and recognize their impact on driving performance by simulating real-world scenarios that vehicles might encounter. The ultimate goal is to introduce an interactive, task-driven prediction evaluation protocol. To achieve this objective, we need to determine four crucial components: _1_) motion prediction methods to be covered; _2_) realistic planners to employ prediction models; _3_) the simulator to replicate interactive scenarios; _4_) evaluation metrics to assess the effectiveness of the key factors involved in motion prediction with respect to real-world driving performance.

### Motion Prediction Methods

We select 10 representative prediction models to achieve comprehensive coverage of mainstream approaches, ranging from simple model-based methods to complex data-driven approaches, as presented in Table 1. Constant Velocity (CV) and Constant Acceleration (CA)  assume that the predicted agent maintains a constant speed or acceleration within the prediction horizon. K-Nearest Neighbor (KNN) predicts an agent's future trajectory based on most similar trajectories, while Social-KNN (S-KNN)  extends it by also considering the similarity of surrounding agents. These methods are widely used as baselines given their widespread effectiveness in simple prediction cases. Social LSTM (S-LSTM) , HiVT , LaneGCN , and HOME  represent four distinct types of neural networks: RNN, Transformer, GNN, and CNN. DSP  utilizes a hybrid design of neural networks, representing state-of-the-art prediction models.

### Planners

An ideal planner should: _1_) be able to handle state and action uncertainty; _2_) take into account multiple factors of driving performance, such as safety (collision avoidance), efficiency (timely goal achievement), and comfort (smooth driving); _3_) be aware of interactions with other agents; and _4_) supports real-time execution. We select two realistic planners based on these criteria: a simplistic planner that only meets _2_) and _4_), and a sophisticated planner that satisfies all of _1_) - _4_). This allows us to draw planner-agnostic conclusions.

**RVO.** The RVO planner  is a simplistic planner that solves the optimization problem in the velocity space under collision avoidance constraints. The planner uses motion predictions to avoid possible collisions with the deterministic motions, and thus does not consider the state and action uncertainty. As the RVO planner does not maintain states between consecutive timesteps, it also cannot optimize its planning with respect to interactions with other agents. The objective function of the RVO planner involves safety and efficiency within a short time window, and the RVO planner executes in real time.

**DESPOT.** The DESPOT planner  is a state-of-the-art belief-space planning algorithm that address uncertainties near-optimally. To account for stochastic states and actions, we adopt the bicycle model, a kinematic model with two degrees of freedom, and introduce Gaussian noise to the displacement. DESPOT considers the system state, context information, and the ego-agent's action to predict the future states of exo-agents. This allows it to consider the interaction between agents. Moreover, the objective function of DESPOT incorporates safety, efficiency, and comfort metrics, making it an ideal algorithm for planning in complex and dynamic environments.

We use these planners to control the speed of the ego-agent while pure-pursuit algorithm  for adjusting the steering angle. Details can be found in the supplementary materials.

### Simulator

In order to evaluate different prediction models, the ideal simulator should: _1_) provide real-world maps and agents; _2_) model potential unregulated behaviors; _3_) accurately mirror the interactions between agents; and _4_) provide realistic perception data for effective planning. We choose the SUMMIT simulator  for our experiments since it meets all the criteria, as demonstrated in Table 2. SUMMIT is a sophisticated simulator based on the Carla  framework, offering various real-world

   Modeling Approach & Method & Output Type & Interaction Aware & Scene Aware & Map Aware \\   & CV  & ST & ✗ & ✗ & ✗ \\  & CA  & ST & ✗ & ✗ & ✗ \\  & KNN  & MT & ✗ & ✗ & ✗ \\  & S-KNN  & MT & ✓ & ✗ & ✗ \\   & LSTM & ST & ✗ & ✗ & ✗ \\  & S-LSTM  & ST & ✓ & ✗ & ✗ \\   & HiVT  & MT & ✓ & ✓ & ✓ \\   & LaneGCN  & MT & ✓ & ✓ & ✓ \\   & HOME  & OM & ✓ & ✓ & ✓ \\   & DSP  & MT & ✓ & ✓ & ✓ \\   

*Abbreviations: **ST**: Single-Trajectory, **MT**: Multi-Trajectory, **OM**: Occupancy Map.

Table 1: Selected prediction methods.

maps and agents to create diverse and challenging scenarios. It uses a realistic motion model to simulate interactions between agents and supports the simulation of crowded scenes, complex traffic conditions, and unregulated behaviors.

There are two distinct concepts of time in our experiments: _simulation time_ and _real time_. The former corresponds to the duration of actions in the simulator, while the latter represents the wall time consumed by the planner. By default, the simulator runs in asynchronous mode. In this mode, the simulator and planner run individually, and the simulation time is equal to the real time. However, in this study, we employ the SUMMIT simulator in synchronous mode to accommodate slow predictors. In this mode, the simulator waits for the planner to establish communication before proceeding to the next step. Since the simulation time for each execution step remains constant at \(0.03\ \), the ratio between simulation time and real time can be manually set by varying the communication frequency, known as the tick rate. For example, once the tick rate is set to \(30\ \), the simulation time equals the real time. But, when the tick rate is set to \(3\ \), the ratio between simulation time and real time is \(0.1\).

### Evaluation Protocols

**Motion Prediction Performance Metrics.** Four commonly used prediction performance metrics are employed in this study, as presented in Table 3. In our experiments, the consensus of K=6 is adopted. While ADE/FDE can be applied to evaluate single-trajectory prediction models, their probabilistic variants minADE and minFDE can be applied to evaluate multi-trajectory predictors.

**Driving Performance Metrics.** The driving performance is primarily determined by three factors: safety, comfort, and efficiency. Let \(H\) represent the total timestep for each scenario.

Safety is typically evaluated in terms of the collision rate. The collision is determined whether the smallest distance between the ego-agent and surrounding agents is smaller than a threshold \(\), that is: \(P_{}=_{t=1}^{H}[\{||s_{A}^{t}- s||:s\{s_{1}^{t},...,s_{n}^{t}\}\}<]\), where \(||||\) is the L2 distance between the ego-agent's bounding box and the exo-agent's bounding box, \(\) is the boolean indicator function. We set \(=1\) for our experiments since the DESPOT planner rarely causes real collisions.

Efficiency is evaluated by the average speed of the ego-agent over the whole scenario: \(P_{}=_{t=1}^{H}v_{A}^{t}\). Comfort is represented by the jerk of the ego-agent, which is the rate of change of acceleration with respect to time: \(P_{}=_{t=1}^{H}_{A}^{t}\).

Since these three metrics are not comparable, we normalized each metric to \(\) before calculating the driving performance. Furthermore, we standardized the direction of these metrics, with higher values indicating better performance:

\[_{m}=[-(P_{m})}{(P_{m})-(P_{m})}]^{n}+[1--(P_{m})}{(P_{m})-(P_{m})}]^{1-n}.\] (4)

The boolean indicator \(n=[m\{\}]\) is employed to adjust the direction of driving performance metrics. The ultimate driving performance is derived through the average of normalized metrics for safety, efficiency, and comfort. Pseudocode can be found in the supplementary materials.

**Experimental Design.** We conduct two types of experiments for both planners in the SUMMIT simulator: _Fixed Prediction Ability_ and _Fixed Planning Ability_.

1. Fixed Prediction Ability: The planner is required to perform a fixed number of predictions within an interactive simulation environment, regardless of the predictor's execution speed. The objective is to clarify the factor that dominates the disparity between prediction performance and driving performance while ensuring the predictive ability of methods.

   Simulator & Real-World Maps & Unregulated Behaviors & Dense Interactions & Realistic Visuals \\  SUMO  & ✓ & ✗ & ✓ & ✗ \\ TrafficSim  & ✗ & ✓ & ✓ & ✗ \\ TORCS  & ✗ & ✓ & ✗ & ✓ \\ BARK  & ✗ & ✓ & ✗ & ✗ \\ Apollo  & ✗ & ✗ & ✗ & ✗ \\ Symphony  & ✓ & ✓ & ✗ & ✓ \\ SUMMIT  & ✓ & ✓ & ✓ & ✓ \\   

Table 2: Comparison between simulators.

2. Fixed Planning Ability: The planner is allocated varying time budgets to simulate a predictor running at different speeds. Three sub-experiments are conducted with tick rates set at 30 Hz, 3 Hz, and 1 Hz. The objective is to investigate the factors, except for prediction accuracy, that represent the predictive ability of methods. This provides an explanation for the remaining disparity between prediction accuracy and driving performance.

We collect 50 scenarios for each predictor in each experiment. For each scenario, we randomly select the start and end point for the ego-agent from one of the four real-world maps provided by the SUMMIT simulator. A reference path of 50 meters is maintained between the two points, and the ego-agent is instructed to follow this path. A certain number of exo-agents including pedestrians, cyclists, and vehicles is randomly distributed within the environment. We implement all selected predictors in the simulator, except for HOME and DSP, due to their significantly longer running time, making the closed-loop evaluation infeasible. These two methods are solely employed in the Sim-Real Alignment, as shown in the supplementary materials. Notably, the RVO planner produces identical results for these two experimental settings since it conducts prediction only once per timestep.

To investigate the correlation between prediction accuracy and driving performance, we train all selected motion prediction models on the Alignment dataset collected from the SUMMIT simulator. We collect 59,944 scenarios and separate them into two groups: 80% training and 20% validation. Each scenario consists of about 300 steps. Subsequently, it is filtered down to 50 steps by taking into account the number of agents and their occurrence frequency. The nearest three agents are randomly selected to be the _interested agent_ for prediction. The data collection was conducted on a server equipped with an Intel(R) Xeon(R) Gold 5220 CPU. We use four NVIDIA GeForce RTX 2080 Ti to speed up the running.

## 5 Experimental Results

The experiment part is designed and organized to answer the following questions:_I_) Can the current prediction evaluation system accurately reflect the driving performance?; _2_) What is the main factor causing the disparity between prediction accuracy and driving performance?; and _3_) How do we propose evaluating predictors based on driving performance?

### The Limitation of Current Prediction Evaluation

This section illustrates the limitation of current prediction evaluation systems in accurately reflecting realistic driving performance. We use ADE as an example, with FDE results provided in the supplementary materials. We refer to the ADE and minADE calculated in the Alignment dataset as _Static ADE_ and _Static minADE_ since they are obtained from static evaluation. Fixed prediction ability experiments are conducted for both RVO and DESPOT planners in the SUMMIT simulator.

**Static ADE.** As revealed in Figure 2(a), there is no significant correlation between Static ADE and driving performance. Specifically, in the DESPOT planner, we observe a counterintuitive positive relationship. According to this finding, higher Static ADE would imply better driving performance. However, this hypothesis lacks a realistic basis and should be rejected since it exceeds the 95% confidence interval. Static ADE can not serve as a reliable indicator of driving performance.

**Static minADE.** To consider the impact of multi-modal prediction, we further investigate the correlation between Static minADE and driving performance. As depicted in Figure 2(b), the multi-modal prediction accuracy better reflects the driving performance. However, in the RVO planner, the linear regression exceeds the 95% confidence interval, thereby weakening the credibility of the evaluation metric. Furthermore, we still observe the positive correlation between prediction error and driving performance in the DESPOT planner, despite the lack of a realistic basis. These two observations prevent us from asserting the efficacy of Static minADE.

   Metric Name & Metric Equation \\  ADE & \(_{i=1}^{T}-_{i})^{2}+(y_{i}-_{i})^{2}}\) \\ FDE & \(-x_{T})^{2}+(y_{T}-_{T})^{2}}\) \\ minADE & \(_{k K}_{i=1}^{T}-_{i})^{2}+(y_{i}- _{i})^{2}}\) \\ minFDE & \(_{k K}-_{T})^{2}+(y_{T}-_{T})^{2}}\) \\   

Table 3: Trajectory prediction metrics.

In summary, current evaluation metrics merely represent the vanilla accuracy of predictors and do not fully reflect the driving performance. It is urgent to identify the overlooked factor(s) on this issue.

### The Dominant Factor: Dynamics Gap

The disparity between static evaluation metrics and driving performance can be attributed to various factors. However, due to the lack of quantitative analysis, it remains unclear which factor primarily contributes to this disparity. In this section, we utilize fixed prediction ability experiments to identify the dominant factor affecting the disparity between prediction accuracy and driving performance. We use _Dynamic ADE_ as an example. Similar to Static ADE, Dynamic ADE is calculated as the average L2 distance between the forecasted trajectory and the ground truth. However, the dynamic prediction accuracy in the interactive simulator differs from that of the Alignment dataset, since agents behave differently. This leads to the difference between Dynamic ADE and Static ADE.

**Dynamics Gap.** Figure 5 reports the correlation between Dynamic ADE and driving performance for both RVO and DESPOT planners. Compared to Static ADE, Dynamic ADE displays a significantly stronger correlation with driving performance for both planners. Quantitatively, using the correlation coefficient as a measurement, the dynamics gap accounts for 77.0% of the inconsistency between Static ADE and driving performance in the RVO planner, and 70.3% in the DESPOT planner. This emphasizes the dominant role of the dynamics gap in addressing the challenge of implementing predictors in the real world.

Furthermore, we compare the dynamics gap with factors that influence prediction accuracy and potentially affect driving performance. Except for the multi-modal prediction, the asymmetry of prediction errors  and occlusion are selected for comparison. We measure the significance of factors by assessing their impact on the correlation coefficient between prediction accuracy and driving performance. The disparity between static ADE and driving performance is denoted as _Total_.

**Multi-Modal Prediction.** We examine the variation in correlation coefficient from single-trajectory prediction to their multi-trajectory variants.

**Prediction Asymmetry.** We check the change in correlation coefficient from considering all agents' predictions to considering only the interested agent that may affect the ego-plan.

**Occlusion.** We examine the variation in correlation coefficient from considering only agents with complete observations to considering agents with missed observations.

As depicted in Table 4, considering the asymmetry of prediction errors, multi-modal prediction, and dynamics gap can mitigate the disparity between prediction accuracy and driving performance. The consideration of occlusion does not yield satisfactory results in the RVO planner, as the planner mainly focuses on short-term predictions and is minimally affected. The asymmetric of prediction errors has a noticeable impact on both planners, highlighting the importance of predictions involving agents that may affect ego-planning. The multi-modal prediction exerts a significant impact on the RVO planner. However, when a complex planner such as DESPOT is used in conjunction, the impact becomes much weaker. It is reasonable to assume that the sampling-based feature of the DESPOT planner mitigates the influence of prediction uncertainty. Among various metrics, the dynamics gap exhibits superior performance in both planners and effectively resolves the majority of the disparity.

We can conclude that the dynamics gap is the main factor that causes the disparity between prediction accuracy and realistic driving performance. The Dynamic ADE, which is evaluated through interactive simulation environments, is capable of incorporating the dynamics gap and displaying a significant correlation with driving performance.

### On the Predictive Ability of Methods

Although the dynamics gap accounts for the majority of the disparity between prediction accuracy and driving performance, the correlation between Dynamic ADE and driving performance is unsatisfactory in the DESPOT planner. To address this issue, we must answer two additional questions: _1)_ Does prediction accuracy fully reflect the predictive ability of methods? 2) If not, what other factor(s) is most important? We examine three well-known but never fully investigated factors and their correlation with driving performance: the temporal consistency of predictions , the distribution of prediction errors, and the computational efficiency of predictors.

**Temporal Consistency.** Each prediction scenario contains multiple successive frames within a fixed temporal chunk. Any two overlapping chunks of input data with a small time-shift should produce consistent results. The difference between the predictions of the two overlapping chunks is measured as the temporal consistency, with time-shift set to one frame.

**Error Distribution.** The error distribution of motion prediction contains the mean and variance. While the mean has been captured by the prediction accuracy, we incorporate the variance of prediction errors to account for the remaining impact.

**Computational Efficiency.** The computational efficiency of predictors is determined by their inference time when applied to downstream tasks. Feature extraction time is also included.

According to Figure 6, surprisingly, the temporal consistency of predictions and the distribution of prediction errors show a weak correlation with driving performance in both planners. However, the computational efficiency of predictors exerts a significant influence on the driving performance, as shown in Table 5. In three sub-experiments, the ranking of driving performance is aligned with the ranking of predictors' inference time in the DESPOT planner. We can conclude that the prediction accuracy can not fully represent the predictive ability of methods, and the computational efficiency of predictors is also a significant metric that affects driving performance.

    &  \\   & RVO & DESPOT \\  Occlusion & -0.22 & 0.16 \\ Prediction Asymmetry & 0.17 & 0.20 \\ Multi-Modal Prediction & 0.70 & 0.45 \\ Dynamics Gap & **0.77** & **1.16** \\  Total & 1.00 & 1.65 \\   

Table 4: Impacts on the correlation coefficient between prediction accuracy and driving performance.

### The Trade-Off between Accuracy and Speed

It should be noted that there is a trade-off between computational efficiency and dynamic prediction accuracy for predictors to derive driving performance. As shown in Figure 5, the correlation between Dynamic ADE and driving performance becomes less strong when the tick rate is set higher. This is indicated by the data points deviating further from the best-fit-line in higher tick rates. At this time, it is the computational efficiency rather than dynamic prediction accuracy that decides the driving performance, as shown in Table 5. When the tick rate is set to 30Hz, the planner cannot generate an optimal solution, whereby the ranking of driving performance is determined by computational efficiency. When the tick rate is set to 3Hz, the CA outperforms CV since they have near-optimal solutions. When the Tick Rate is set to 1Hz, the LSTM also outperforms CV. The driving performance is determined by both dynamic prediction accuracy and computational efficiency of predictors in a trade-off manner, highlighting the significance of task-driven prediction evaluation.

## 6 Conclusion

Our study demonstrates the limitation of current prediction evaluation systems in accurately reflecting realistic driving performance. We identify the dynamics gap as the dominant factor contributing to this disparity. Furthermore, our findings reveal that the ultimate driving performance is determined by the trade-off between prediction accuracy and computational efficiency, rather than solely relying on prediction accuracy. We recommend further research incorporating interactive and task-driven evaluation protocols to assess prediction models. Despite these insights, there is still work to be done in the future. Our study focuses on sampling-based and reactive planners, and incorporating optimization-based or geometric planners could further substantiate our conclusions. Oracle perception data from the simulator can not fully represent the complexities of real-world situations. It would be beneficial to use raw sensor data to understand the comprehensive interplay within the entire AD system.