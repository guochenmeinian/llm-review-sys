# Norm-based Generalization Bounds for Sparse Neural Networks

Tomer Galanti

Center for Brains, Mind, and Machines

Massachusetts Institute of Technology

galanti@mit.edu

&Mengjia Xu

Department of Data Science

New Jersey Institute of Technology

mx6@njit.edu

Liane Galanti

School of Computer Science

Tel Aviv University

lianegalanti@mail.tau.ac.il

&Tomaso Poggio

Center for Brains, Mind, and Machines

Massachusetts Institute of Technology

tp@csail.mit.edu

###### Abstract

In this paper, we derive norm-based generalization bounds for sparse ReLU neural networks, including convolutional neural networks. These bounds differ from previous ones because they consider the sparse structure of the neural network architecture and the norms of the convolutional filters, rather than the norms of the Toeplitz matrices associated with the convolutional layers. Theoretically, we demonstrate that these bounds are significantly tighter than standard norm-based generalization bounds. Empirically, they offer relatively tight estimations of generalization for various simple classification problems. Collectively, these findings suggest that the sparsity of the underlying target function and the model's architecture plays a crucial role in the success of deep learning.

## 1 Introduction

Over the last decade, deep learning with large neural networks has significantly advanced the solution of a myriad of tasks. These include image classification , language processing , interactions with open-ended environments , and code synthesis . Contrary to traditional theories such as , recent findings  indicate that deep neural networks can generalize effectively even when their size vastly exceeds the number of training samples.

To address this question, recent work has proposed different generalization guarantees for deep neural networks based on various norms of their weight matrices . Many efforts have been made to improve the tightness of these bounds to realistic scales. Some studies have focused on developing norm-based generalization bounds for complex network architectures, such as residual networks . Other studies investigated ways to reduce the dependence of the bounds on the product of spectral norms , or to use compression bounds based on PAC-Bayes theory , or on the optimization procedure used to train the networks . However, most of this research is centered around fully-connected networks, which generally underperform compared to other architectures like convolutional networks , residual network  and transformers . Thus, the ability of these bounds to explain the success of contemporary architectures is rather limited.

To fully understand the success of deep learning, it is necessary to analyze a wider scope of architectures beyond fully-connected networks. An interesting recent direction  introduces generalization bounds for neural networks with shared parameters, such as convolutional neural networks. For example,  showed that by taking into account the structure of the convolutionallayers, we can derive generalization bounds with a norm component smaller than the norm of the associated linear transformation. However, many questions remain unanswered, including **(a)**_Why certain architectures, such as convolutional networks__and MLP-mixers_, _perform better than fully-connected neural networks?_ **(b)**_Is weight sharing necessary for the success of convolutional neural networks?_ **(c)**_Can we establish norm-based generalization bounds for convolutional neural networks that are reasonably tight in practical settings?_ In this paper, we contribute to an understanding of all three questions.

### Related Work

Approximation guarantees for multilayer sparse networks.While fully-connected networks, including shallow networks, are universal approximators  of continuous functions, they are largely limited in theory and in practice. Classic results  show that, in the worst-case, the number of parameters required to approximate a continuously differentiable target functions (with bounded derivatives) grows exponentially with the input dimension, a property known as the "curse of dimensionality".

A recent line of work  shows that the curse of dimensionality can be avoided by deep, sparse networks, when the target function is itself compositionally sparse. Furthermore, it has been conjectured that efficiently computable functions, that is functions that are computable by a Turing machine in polynomial time, are compositionally sparse. This suggests, in turns, that, for practical functions, deep and sparse networks can avoid the curse of dimensionality. These results, however, lack any implication about generalization; in particular, they do not show that overparametrized sparse networks have good generalization.

Norm-based generalization bounds.A recent thread in the literature  has introduced norm-based generalization bounds for neural networks. In particular, let \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\) be a training dataset of \(m\) independently drawn samples from a probability measure \(P\) defined on the sample space \(\), where \(^{d}\) and \(=\{ 1\}\). A fully-connected network is defined as \(f_{w}(x)=W^{L}(W^{L-1}((W^{2}(W^{1}x))))\), where \(W^{l}^{d_{l+1} d_{l}}\) and \((x)\) is the element-wise ReLU activation function \((0,x)\). A common approach for estimating the gap between the train and test errors of a neural network is to use the Rademacher complexity of the network. For example, in , an upper bound on the Rademacher complexity is introduced based on the norms of the weight matrices of the network of order \((}{}_{l=1}^{L}\|W^{l}\|_{F})\). Later,  showed that the exponential dependence on the depth can be avoided by using the contraction lemma and obtained a bound that scales with \(()\).

While these results provide solid upper bounds on the test error of deep neural networks, they only take into account very limited information about the architectural choices of the network. In particular, when applied to convolutional networks, the matrices \(W^{l}\) represent the linear operation performed by a convolutional layer whose filters are \(w^{l}\). However, since \(W^{l}\) applies \(w^{l}\) to several patches (\(d_{l}\) patches), we have \(\|W^{l}\|_{F}=}\|w^{l}\|_{F}\). As a result, the bound scales with \((^{L-1}d_{l}})\), that grows exponentially with \(L\). This means that the bound is not suitable for convolutional networks with many layers as it would be very loose in practice. In this work, we establish generalization bounds that are customized for convolutional networks and scale with \(_{l=1}^{L}\|w^{l}\|_{F}\) instead of \(_{l=1}^{L}\|W^{l}\|_{F}\).

In  they conducted a large-scale experiment evaluating multiple norm-based generalization bounds, including those of . They argued that these bounds are extremely loose and negatively correlated with the test error. However, in all of these experiments, they trained the neural networks with the cross-entropy loss which implicitly maximizes the network's weight norms once the network perfectly fits the training data. This can explain the observed negative correlation between the bounds and the error. In this work, we empirically show that our bounds provide reasonably tight estimations of the generalization gap for convolutional networks trained with weight normalization and weight decay using the MSE loss.

Generalization bounds for convolutional networks.Several recent papers have introduced generalization bounds for convolutional networks that take into account their unique structure. In , they introduced a generalization bound for neural networks with weight sharing. However, this bound only holds under the assumption that the weight matrices are orthonormal, which is not real istic in practice. In , they introduced generalization bounds for convolutional networks based on parameter counting. However, this bound scales roughly as the square root of the ratio between the number of parameters and the number of samples, which is vacuous when the network is overparameterized. In , they extended the generalization bounds of  for convolutional networks where the linear transformations \(W^{l}\) at each layer are replaced with the trainable parameters. While this paper provides generalization bounds in which each convolutional filter contributes only once to the bound, it does not hold when different filters are used for different patches, even if their norms are the same. In short, their analysis treats different patches as "datapoints" in an augmented problem where only one linear function is applied at each layer. If several choices of linear functions (different weights for different patches) are allowed, the capacity of the function class would increase. Although all of these papers offer generalization guarantees for convolutional networks, they base their findings either on the number of trainable parameters or on weight sharing. Notably, none of these studies directly address the question of whether weight sharing is essential for the effective generalization of convolutional networks. Furthermore, none provide empirical evidence to confirm that their bounds are reasonably tight in practical settings. In our previous work , we derived generalization bounds using a technique similar to the one employed here. The results in this paper extend those preliminary results to a more general and more detailed formulation.

### Contributions

In this work, we study the generalization guarantees of a broad class of sparse deep neural networks , such as convolutional neural networks. Informally, a sparse neural network is a graph of neurons represented as a Directed Acyclic Graph (DAG), where each neuron is a function of a small set of other neurons. We show how a simple modification to the classic norm-based generalization bound of  yields significantly tighter bounds for sparse neural networks (e.g., convolutional networks). Unlike previous bounds [33; 32; 47], our analysis demonstrates how to obtain generalization guarantees for sparse networks, without incorporating weight sharing, while having a weak dependence on the actual size of the network. These results suggest that it is possible to obtain good generalization performance with sparse neural networks without relying on weight sharing. Finally, we conduct multiple experiments to evaluate our bounds for overparameterized convolutional neural networks trained on simple classification problems. These experiments show that in these settings, our bound is significantly tighter than many bounds in the literature [14; 33; 32; 47]. As a result, this research provides a better understanding of the pivotal influence of the structure of the network's architecture [30; 34; 2] on its test performance.

## 2 Problem Setup

We consider the problem of training a model for classification. Formally, the task is defined by a distribution \(P\) over samples \((x,y)\), where \(^{c_{0} d_{0}}\) is the instance space (e.g., images), and \(^{C}\) is a label space containing the \(C\)-dimensional one-hot encodings of the integers \(1,,C\). When thinking about the samples as images, we view \(c_{0}\) as the number of input channels and \(d_{0}\) as the image size. We consider a hypothesis class \(\{f^{}:^{C}\}\) (e.g., a neural network architecture), where each function \(f_{w}\) is specified by a vector of parameters \(w^{N}\) (i.e., trainable parameters). A function \(f_{w}\) assigns a prediction to an input point \(x\), and its performance on the distribution \(P\) is measured by the _expected error_, \(_{P}(f_{w})\ :=\ _{(x,y) P}[[_{j y }(f_{w}(x_{i})_{j}) f_{w}(x_{i})_{y}]]\), where \(:\{,\}\{0,1\}\) be the indicator function (i.e., \([]=1\) and vice versa). Since we do not have direct access to the full population distribution \(P\), the goal is to learn a predictor, \(f_{w}\), from some training dataset \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\) of independent and identically distributed (i.i.d.) samples drawn from \(P\) along with regularization to control \(f_{w}\)'s complexity.

### Rademacher Complexities

We examine the generalization abilities of overparameterized neural networks by investigating their Rademacher complexity. This quantity can be used to upper bound the worst-case generalization gap (i.e., the distance between train and test errors) of functions from a certain class. It is defined as the expected performance of the class when averaged over all possible labelings of the data, where the labels are chosen independently and uniformly at random from the set \(\{ 1\}\). In other words, it is the average performance of the function class on random data. For more information, see [48; 49; 50].

**Definition 2.1** (Rademacher Complexity).: Let \(\) be a set of real-valued functions \(f_{w}:^{C}\) defined over a set \(\). Given a fixed sample \(X^{m}\), the empirical Rademacher complexity of \(\) is defined as follows: \(_{X}()\;:=\;_{:_{i} U[\{  1\}]}[_{f_{w}}_{i=1}^{m}_{r=1}^{C} _{ir}f_{w}(x_{i})_{r}]\).

In contrast to the Vapnik-Chervonenkis (VC) dimension, the Rademacher complexity has the added advantage that it can be upper bounded based on a finite sample. The Rademacher complexity can be used to upper bound the gap between test and train errors of a certain class of functions . In the following lemma we bound the gap between the test error and the empirical margin error \(_{S}^{}(f_{w})=_{i=1}^{m}[_{j y }(f_{w}(x_{i})_{j})+ f_{w}(x_{i})_{y}]\).

**Lemma 2.2**.: _Let \(P\) be a distribution over \(^{c_{0}d_{0}}[C]\) and \(\{f^{}:^{C}\}\). Let \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\) be a dataset of i.i.d. samples selected from \(P\) and \(X=\{x_{i}\}_{i=1}^{m}\). Then, with probability at least \(1-\) over the selection of \(S\), for any \(f_{w}\), we have_

\[_{P}(f_{w})-_{S}^{}(f_{w})\;\;}{ }_{X}()+3}. \]

The above bound is decomposed into two parts; one is the Rademacher complexity and the second scales as \((1/)\) which is small when \(m\) is large. In section 3 we derive norm-based bounds on the Rademacher complexity of sparse networks. This lemma and the rest of the mathematical statements are proven in the appendix.

### Architectures

A neural network architecture can be formally defined using a Directed Acyclic Graph (DAG) \(G=(V,E)\). The class of neural networks associated with this architecture is denoted as \(_{G}\). The set of neurons in the network is given by \(V=_{l=0}^{L}\{z_{1}^{l},,z_{d_{l}}^{l}\}\), which is organized into \(L\) layers. An edge \((z_{i}^{l},z_{j}^{l-1}) E\) indicates a connection between a neuron in layer \(l-1\) and a neuron in layer \(l\). The full set of neurons at the layer \(l\)th is denoted by \(v^{l}:=(z_{j}^{l})_{j=1}^{d_{l}}\).

A neural network \(f_{w}:^{c_{0} d_{0}}^{C}\) takes "flattened" images \(x\) as input, where \(c_{0}\) is the number of input channels and \(d_{0}\) is the image dimension represented as a vector. Each neuron \(z_{i}^{l}:^{c_{0} d_{0}}^{c_{l}}\) computes a vector of size \(c_{l}\) (the number of channels in layer \(l\)). To avoid confusion, in our definition, we think of each neuron as a vector of dimension \(c_{l}\). This is analogous to a pixel holding three coordinates of RGB. The set of predecessor neurons of \(z_{i}^{l}\), denoted by \((l,i)\), is the set of all \(j[d_{l-1}]\) such that \((z_{i}^{l},z_{j}^{l-1}) E\), and \(v_{i}^{l}:=(z_{j}^{l})_{j(l,i)}\) denotes the set of predecessor neurons of \(z_{i}^{l}\). The network is recursively defined as follows:

\[ r[C]:\;f_{w}(x)_{r}\;:=\;_{i=1}^{d_{L-1}} w_{ri}^{L},z_{ i}^{L-1}(x),\]

where \(w_{ri}^{L}^{c_{L-1}}\), \(z_{i}^{l}(x)\;:=\;(w_{i}^{l}v_{i}^{l-1}(x))\), \(w_{i}^{l}^{c_{l}(c_{l-1}|(l-1,i)|)}\) is a weight matrix, \(x=(z_{j}^{0}(x))_{j=1}^{d_{0}}\), each \(z_{j}^{0}(x)\) is a vector of dimension \(c_{0}\) representing the \(j\)th "pixel" of \(x\) and \(\) is the ReLU activation function. For simplicity, we denote \(w^{L}:=W^{L}=(w_{ri})_{r,i}\).

The degree of sparsity of a neural network can be measured using the degree of the graph, which is defined as the maximum number of predecessors for each neuron. Specifically, the degree of a neural network architecture \(G\) is given by: \((G):=_{l[L]}(G)_{l}\), where \((G)_{l}:=_{j[d_{l}]}|(l,j)|\) is the maximal degree of the \(l\)th layer.

**Convolutional neural networks.** A special type of compositionally sparse neural networks is convolutional neural networks. In such networks, each neuron acts upon a set of nearby neurons from the previous layer, using a kernel shared across the neurons of the same layer.

To formally analyze convolutional networks, we consider a broader set of neural network architectures that includes sparse networks with shared weights. Specifically, for an architecture \(G\) with \(|(l,j)|=k_{l}\) for all \(j[d_{l}]\), we define the set of neural networks \(_{G}^{}\) to consist of all neural networks \(f_{w}_{G}^{}\) that satisfy the weight sharing property \(w^{l}:=w^{l}_{j_{1}}=w^{l}_{j_{2}}\) for all \(j_{1},j_{2}[d_{l}]\) and \(l[L]\). Convolutional neural networks are essentially sparse neural networks with shared weights and locality (each neuron is a function of a set of nearby neurons of its preceding layer).

**Norms of neural networks.** As mentioned earlier, previous papers (e.g., ) proposed different generalization bounds based on different types of norms for measuring the complexity of fully-connected networks. One approach that was suggested by  is to use the product of the norms of the weight matrices given by \((w):=_{l=1}^{L}\|W^{l}\|_{F}\).

In this work, we derive generalization bounds based on the product of the maximal norms of the kernel matrices across layers, defined as: \((w):=\|w^{L}\|_{F}_{l=1}^{L-1}_{j[d_{l}]}\|w^{l}_{j}\|_{F}\), where \(\|\|_{F}\) and is the Frobenius norm. For a convolutional neural network, we have a simplified form of \((w)=_{l=1}^{L}\|w^{l}\|_{F}\), due to the weight sharing property. This quantity is significantly smaller than the quantity \((w)=\|w^{L}\|_{F}_{l=1}^{L-1}^{d_{l}}\| w^{l}_{j}\|_{F}^{2}}\) used by . For instance, when weight sharing is applied, we can see that \((w)=(w)^{L-1}d_{l}}\) which is significantly larger than \((w)\).

**Classes of interest.** In the next section, we study the Rademacher complexity of classes of compositionally sparse neural networks that are bounded in norm. We focus on two classes: \(_{G,}:=\{f_{w}_{G}(w)\}\) and \(_{G,}^{}:=\{f_{w}_{G}^{} (w)\}\), where \(G\) is a compositionally sparse neural network architecture and \(\) is a bound on the norm of the network parameters.

## 3 Theoretical Results

In this section, we introduce our main theoretical results. The following theorem provides a bound on the Rademacher complexity of the class \(_{G,}\) of networks of architecture \(G\) of norm \(\).

**Proposition 3.1**.: _Let \(G\) be a neural network architecture of depth \(L\) and let \(>0\). Let \(X=\{x_{i}\}_{i=1}^{m}\) be a set of samples. Then,_

\[_{X}(_{G,}) \ (1+^{L-1} ((G)_{l})+(C))})\] \[,,j_{L}}_{l=1}^{L-1}| {pred}(l,j_{l})|_{i=1}^{m}\|z_{j_{0}}^{0}(x_{i})\|_{2}^{2}},\]

_where the maximum is taken over \(j_{0},j_{1},,j_{L}\), such that, \(j_{l-1}(l,j_{l})\) for all \(l[L]\)._

The proof for this theorem builds upon the proof of Theorem 1 in . A sketch of the proof is presented in Section 3.1. As we show next, by combining Lemma 2.2 and Proposition 3.1 we can obtain an upper bound on the test error of compositionally sparse neural networks.

**Theorem 3.2**.: _Let \(P\) be a distribution over \(^{c_{0}d_{0}}\{ 1\}\). Let \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\) be a dataset of i.i.d. samples selected from \(P\). Then, with probability at least \(1-\) over the selection of \(S\), for any \(f_{w}_{G}\),_

\[_{P}(f_{w})-_{S}^{}(f_{w}) \ ((w)+1)}{ m}(1+^{L-1}((G)_{l})+(C))})\] \[,,j_{L}}_{l=1}^{L-1}| {pred}(l,j_{l})|_{i=1}^{m}\|z_{j_{0}}^{0}(x_{i})\|_{2}^{2}}+3/)}{2m}},\]

_where the maximum is taken over \(j_{0},,j_{L}\), such that, \(j_{l-1}(l,j_{l})\) for all \(l[L]\)._

The theorem above provides a generalization bound for neural networks of a given architecture \(G\). To understand this bound, we first analyze the term \(:=_{j_{0},,j_{L}}_{l=1}^{L-1}|(l,j_{l})| _{i=1}^{m}\|z_{j_{0}}^{0}(x_{i})\|_{2}^{2}\). We consider a setting where \(d_{0}=2^{L}\), \(c_{l}=1\) and each neuron takes two neurons as input, \(k_{l}:=|(l,j)|=2\) for all \(l[L]\) and \(j[d_{l}]\). In particular, \(_{l=1}^{L-1}k_{l}=2^{L-1}\) and \(z_{j}^{0}(x_{i})\) is the \(j\)th pixel of \(x_{i}\). Therefore, we have \(=}{2}_{j_{0}}_{i=1}^{m}\|z_{j_{0}}^{0}(x_{i})\|_{2} ^{2}\). We

[MISSING_PAGE_FAIL:6]

the bounds in Theorem 17 of  roughly scale as \((^{L}\|w^{l}\|_{2}}{}E(w)^{1/} I _{})\), where \(E(w)=(_{l=1}^{L-1}^{2}(w^{l}-u^{l})^{}\|_{2}}{ \|w^{l}\|_{2}^{2}}+\|_{2}^{2}}{\|w^{l}_{i},\|_{2}^{2}})\), \(k_{l}\) is the kernel size of the \(l\)th layer and \(W^{l}\) is the matrix corresponding to the linear operator associated with the \(l\)th convolutional layer, \(w_{i,:}\) is the \(i\)th row of a matrix \(w\), \(\) is either \(2\) or \(2/3\), \(I_{}=L\) if \(=2\) and \(I_{}=1\) o.w. and \(u^{l}\) are "reference" matrices of the same dimensions as \(w^{l}\).

In general, neither our bounds nor those in  and  are inherently superior; with each being better in different cases. The main difference between their bounds and our bound, is that while their bounds include both multiplicative complexity term and additive complexity term, our bound includes only a multiplcative complexity term. For instance, the bound in Theorem 17 in  features both \(_{l=1}^{L}\|w^{l}\|_{2}\) and \(E(w)\), whereas our bound exclusively contains the multiplicative term \((w)=_{l=1}^{L}\|w^{l}\|_{F}\). For certain cases, this works in favor of our bound, but at the same time, our term \(_{l=1}^{L}\|w^{l}\|_{F}\) is comparably larger than \(_{l=1}^{L}\|w^{l}\|_{2}\) due to the smaller norms used in the former. As an example of a case where our bound is superior, consider the case described after Theorem 3.2, where each convolutional layer operates on non-overlapping patches of size 2 and the channel dimension is 1 at each layer. We choose \(u^{l}=0\) for all \(l[L-1]\) (which is a standard choice of reference matrices). We notice that \(\|W^{l}\|_{2}=\|w^{l}\|_{2}=\|w^{l}\|_{F}\) since \(W^{l}\) is a block matrix and \(w^{l}\) is a vector. In addition, for any matrix \(A\), we have \((A)\|_{2,1}}{\|A\|_{2}}}{\|A \|_{2}} 1\) and \((A)}{_{i}\|A_{i,:}\|_{2}} 1\) (see ). Therefore, the bound in  scales as at least \(^{L}\|w^{l}\|_{2}}{} L^{3/2}\), while our bound scales as \(^{L}\|w^{l}\|_{2}}{}\) which is smaller by a factor of \(L\).

**Vacuous bounds?** A uniform convergence bound for a class \(\) is an upper bound on the generalization gap that uniformly holds for all \(f\), i.e., \(_{f}|_{P}(f)-_{S}(f)|(m, )\) (typically tends to 0 as \(m\)). The Rademacher complexity bound in Lemma 2.2 is a form of uniform convergence bound. The issue with these bounds is that in interpolation regimes, where there exists a function \(f_{G}\) that fits any labeling of the samples \(\{x_{i}\}_{i=1}^{m}\), uniform convergence bounds are provably vacuous.

While the derivation of the bound in Theorem 3.2 follows the application of Rademacher complexities, we emphasize that it is not a uniform convergence bound and is not necessarily vacuous. Throughout the proof, we sliced the class \(_{G}\) into subsets \(_{G,}=\{f_{G}(w)\}\) (for \(\)) and applied Lemma 2.2 for each of these subsets. This approach yields a bound that is proportional to \((/)\) for each of the slices \(_{G,}\). We then apply a union bound to combine all of them to obtain a bound that scales as \(((w)/)\). This does not give a uniform convergence bound across all members of \(_{G}\), since the bound is individualized for each member \(f_{w}_{G}\) based on the norm \((w)\). For example, for \(w=0\), the bound will be \(0\) which is non-vacuous.

When the learning algorithm minimizes \((w)\) and the minimal norm required to fit the training labels is small, a tight bound can be achieved with a network that perfectly fits the training data. For example, suppose we have a dataset \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\), a target function \(y(x)= w^{*},x\), and a hypothesis class \(=\{ w,x w^{d}\}\). A classic VC-theory bound scales as \(()\), which is vacuous when \(d m\). However, a norm-based bound scales as \((\|w\|/)\), which is non-vacuous for any \(\|w\|\|w^{*}\|\) (as long as \(m>\|w^{*}\|^{2}\)). In addition, the function \(y\) can be realized by \(\{ w,x\|w\|\|w^{*}\|\}\). Specifically, for smaller \(\|w^{*}\|\), we need fewer samples to ensure that the bound is non-vacuous for a minimal norm model that perfectly fits the training data.

### Proof Sketch

We propose an extension to a well-established method for bounding the Rademacher complexity of norm-bounded deep networks. This approach, originally developed by , utilizes a "peeling" argument, where the complexity bound for a depth \(L\) network is reduced to a complexity bound for a depth \(L-1\) network and applied repeatedly. Specifically, the \(l\)th step bounds the complexity bound for depth \(l\) by using the product of the complexity bound for depth \(l-1\) and the norm of the \(l\)th layer. By the end of this process, we obtain a bound that depends on the term \(_{}g(|_{i=1}^{m}_{i}x_{i}|)\)(\(g(x)=x\) in  and \(g=\) in ), which can be further bounded using \(_{x X}\|x\|^{2}\). The final bound scales with \((w)_{x X}\|x\|\). Our extension further improves the tightness of these bounds by incorporating additional information about the network's sparsity.

To bound \(_{X}(_{G,})\) using \((w)\), we notice that each neuron operates on a small subset of the neurons from the previous layer. Therefore, we can bound the contribution of a certain constituent function \(z_{j}^{l}(x)=w_{j}^{L}v_{j}^{l-1}(x)\) in the network using the norm \(\|w_{j}^{l}\|_{F}\) and the complexity of \(v_{j}^{l-1}(x)\) instead of the full layer \(v^{l-1}(x)\). To explain this process, we provide a proof sketch of Proposition 3.1 for convolutional networks \(G=(V,E)\) with non-overlapping patches. For simplicity, we assume that \(d_{0}=2^{L}\), \(c_{l}=1\), and the strides and kernel sizes at each layer are \(k=2\). In particular, the network \(f_{w}\) can be represented as a binary tree, where the output neuron is computed as \(f_{w}(x)=z_{j_{0}}^{L}(x)=w^{L}(z_{1}^{L-1}(x),z_{2}^{L-1}(x))\), \(z_{1}^{L-1}(x)=w^{L-1}(z_{1}^{L-2}(x),z_{2}^{L-2}(x))\) and \(z_{2}^{L-1}(x)=w^{L-1}(z_{3}^{L-2}(x),z_{4}^{L-2}(x))\) and so on. Similar to , we first bound the Rademacher complexity using Jensen's inequality,

\[m_{X}(_{G,})\ =\ ( _{}_{f_{w}}_{i=1}^{m}_{i}f_{w}(x_{i}))\ \ (_{}_{f_{w}}(| _{i=1}^{m}_{i}f_{w}(x_{i})|)), \]

where \(>0\) is an arbitrary parameter. As a next step, we rewrite right-hand side as follows:

\[_{}_{f_{w}}(|_{i=1}^{m}_{i}  f_{w}(x_{i})|) =\ _{}_{f_{w}}(^{m} _{i} w^{L}(z_{1}^{L-1}(x_{i}),z_{2}^{L-1}(x_{i}))|^{2 }})\] \[\ _{}_{f_{w}}( \|_{F}^{2}_{j=1}^{2}\|_{i=1}^{m}_{i}(z_{j}^{L- 1}(x_{i}))\|^{2}_{2}}). \]

We notice that each \(z_{j}^{L-1}(x)\) is itself a depth \(L-1\) binary-tree neural network. Therefore, intuitively we would like to apply the same argument \(L-1\) more times. However, in contrast to the above, the networks \((z_{1}^{L-1}(x))=(w^{L-1}(z_{1}^{L-2}(x),z_{2}^{L-2}(x)))\) and \((z_{2}^{L-1}(x))=(w^{L-1}(z_{3}^{L-2}(x),z_{4}^{L-2}(x)))\) end with a ReLU activation. To address this issue, [13; 14] proposed a "peeling process" based on Equation 4.20 in  that can be used to bound terms of the form \(_{}_{f^{}^{},W:\ \|W\|_{F}} (\|_{i=1}^{m}_{i}(Wf^{}(x_{i}))\|)\). However, this bound is not directly applicable when there is a sum inside the square root, as in equation 3 which includes a sum over \(j=1,2\). Therefore, a modified peeling lemma is required to deal with this case.

**Lemma 3.4** (Peeling Lemma).: _Let \(\) be a 1-Lipschitz, positive-homogeneous activation function which is applied element-wise (such as the ReLU). Then for any class of vector-valued functions \(\{f=(f_{1},,f_{q})\ |\  j[q]:\ f_{j}:^{d} ^{p}\}\), and any convex and monotonically increasing function \(g:[0,)\),_

\[_{}_{W_{j}:\ \|W_{j}\|_{F} R}g(^{q} \|_{i=1}^{m}_{i}(W_{j}f_{j}(x_{i}))\|^{2}_{2}} )\ \ 2_{}_{j[q],\ f}g(R\| _{i=1}^{m}_{i} f_{j}(x_{i})\|_{2}).\]

By applying this lemma \(L-1\) times with \(g=\) and \(f\) representing the neurons preceding a certain neuron at a certain layer, we can bound the term in equation 3 as follows:

\[\ 2^{L}_{}_{j,w}(^{L} \|w^{l}\|_{F}^{2} 2^{L}|_{i=1}^{m}_{i}x_{ij}|^{2}})\] \[\ 2^{L}_{j=1}^{d}_{}( 2^{L/2}|_{i=1}^{m}_{i}x_{ij}|)\ \ 4^{L}_{j}(2^{L}^{2}_{i=1}^{m}z_{ij}^{2}}{ 2}+ 2^{L/2}^{m}x_{ij}^{2}}),\]

where the last inequality follows from standard concentration bounds (see the proof for details). Finally, by equation 2 and properly adjusting \(\), we can finally bound \(_{X}(_{G,})\).

## 4 Experiments

In this section, we empirically evaluate the generalization bounds derived in section 3. In each experiment, we compare our bound with alternative bounds from the literature. We focus on simple convolutional neural networks trained on MNIST and investigate the behavior of the bound when varying different hyperparameters. Each experiment was averaged across five runs. For additional experimental details, please refer to the appendix.

Network architecture.We used convolutional networks with \(L\) layers and \(H\) channels per layer denoted by CONV-\(L\)-\(H\). The networks consist of a stack of \(L\)\(2 2\) convolutional layers with a stride of 1, 0 padding, and \(H\) output channels, utilizing ReLU activations followed by a fully-connected layer. The overall number of trainable parameters is at least \(4H+4(L-1)H^{2}\).

Optimization process.Each model was trained using SGD for MSE-loss minimization between the logits of the network and the one-hot encodings of the training labels. We applied weight normalization  to all trainable layers, except for the last one, which is left un-normalized. In order to regularize the weight parameters, we used weight decay for each one of the layers of the network with the same regularization parameter \(>0\). To train each model, we used an initial learning rate of \(=0.01\) that is decayed by a factor of \(0.1\) at epochs 60, 100, 300, batch size 32, momentum of \(0.9\), and \(=3\) by default.

Experiments.We conducted several experiments to compare our bound to alternative bounds from the literature, when applied for neural networks trained on the MNIST dataset for classification. Throughout these experiments, we compared our bound to the one in Theorem 1 of , the third inequality in Theorem 2.1 of , Theorem 16 of , and Theorem 3.5 in  (explicitly as mentioned in their Table 3). To compute the bound in  for multi-class classification, we adopted a modified version based on the technique we utilized in the proof of Proposition 3.1, which allows us to extend these bounds for multi-class classification. Since  did not provide an explicit value for their coefficient \(C\), we assumed it to be \(1\). In the first experiment we trained three models of different widths for MNIST classification. As can be seen in Figure 1, our bound is significantly smaller than the alternative bounds and is surprisingly close to 1, indicating its tightness. In the second experiment, we compared our bound with alternative bounds from previous literature, focusing on convolutional neural networks with varying depths and widths. As can be seen in Figure 2, even as

Figure 1: **Comparing our bound with prior bounds in the literature during training.** We plot our bound, the train and test errors, the generalization gap, and prior bounds from the literature during training. For each plot, we train a CONV-\(L\)-\(H\) network on MNIST with a different number of channels \(H\).

Figure 2: **Varying the number of channels.** We plot our bound, the train and test errors, the generalization gap, and prior bounds from the literature at the end of training. For each plot, we train a CONV-\(L\)-\(H\) network on MNIST with a different number of layers \(L\) and channels \(H\).

networks become highly overparameterized at large widths, the width does not appear to influence the results of any of the bounds. In Figure 3, we showcase the results for convolutional networks of different depths. Our bound increases exponentially with depth but rises more slowly than the bound from . As an ablation study, in in Figure 4, we compared our bound with alternative bounds, this time varying the regularization coefficient \(\). It is evident that our bound and some alternative bounds decrease concurrently with the generalization gap of the neural network, as desired outcome.

## 5 Conclusions

We studied the question of why certain deep learning architectures, such as convolutional networks and MLP-mixers, perform better than others on real-world datasets. To tackle this question, we derived Rademacher complexity generalization bounds for sparse neural networks, which are orders of magnitude better than a naive application of standard norm-based generalization bounds for fully-connected networks. In contrast to previous papers [33; 32], our results do not rely on parameter sharing between filters, suggesting that the sparsity of the neural networks is the critical component to their success. This sheds new light on the central question of why certain architectures perform so well and suggests that sparsity may be a key factor in their success. Even though our bounds are not practical in general, our experiments show that they are quite tight for simple classification problems, unlike other bounds based on parameter counting and norm-based bounds for fully-connected networks.

Figure 4: **Varying the regularization coefficient \(\). We plot our bound, the train and test errors, the generalization gap, and prior bounds from the literature at the end of 500 epochs. For each plot, we trained a CONV-\(3\)-\(H\) network on MNIST with a varying number of channels \(H\).**

Figure 3: **Varying the number of layers. We plot our bound, the train and test errors, the generalization gap, and prior bounds from the literature at the end of training. For each plot, we train a CONV-\(L\)-\(H\) network on MNIST with a different number of layers \(L\) and channels \(H\).**