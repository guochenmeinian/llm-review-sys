# Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review

Sungduk Yu &Man Luo &Avinash Madusu &Vasudev Lal &Phillip Howard

{sungduk.yu, man.luo, avinash.madasu, vasudev.lal, phillip.r.howard}@intel.com

###### Abstract

Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in the linguistic capabilities of large language models (LLMs), a new potential risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. In this study, we investigate the ability of existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Our analysis shows that existing approaches fail to identify many GPT-4o written reviews without also producing a high number of false positive classifications. To address this deficiency, we propose a new detection approach which surpasses existing methods in the identification of GPT-4o written peer reviews at low levels of false positive classifications. Our work reveals the difficulty of accurately identifying AI-generated text at the individual review level, highlighting the urgent need for new tools and methods to detect this type of unethical application of generative AI.

## 1 Introduction

Recent advancements in large language models (LLMs) have enabled their application to a broad range of domains, where LLMs have demonstrated the ability to produce plausible and authoritative responses to queries even in highly technical subject areas. These advancements have coincided with a surge in interest in AI research, resulting in large increases in paper submissions to leading AI conferences (Table S1). Consequently, workloads for peer reviewers of such conferences have also increased significantly, which could make LLMs an appealing tool for lessening the burden of fulfilling their peer review obligations.

Despite their impressive capabilities, the use of LLMs in the peer review process raises several ethical and methodological concerns which could compromise the integrity of the publication process. Reviewers are selected based on their expertise in a technical domain related to a submitted manuscript, which is necessary to critically evaluate the proposed research. Offloading this responsibility to an LLM circumvents the role that reviewer selection plays in ensuring proper vetting of a manuscript. Furthermore, LLMs are prone to hallucination and may not possess the ability to rigorously evaluate research publications. Therefore, the use of LLMs in an undisclosed manner in peer review poses a significant ethical concern that could undermine confidence in this important process.

Motivating the need for detection tools to address this problem is the apparent increase in AI-generated text among peer reviews submitted to recent AI conferences. Figure 1 shows the proportion of reviews submitted to ICLR between 2019 and 2024 which are flagged as containing AI text using methods deployed in this study. There is a consistent upward trend in recent years which provides indirect evidence of the increasing use of LLMs in peer review writing, echoing a recent study whichestimated that 6.5% to 16.9% of peer reviews submitted to recent AI conferences might have involved substantial use of LLMs for task beyond simple grammar checks .

In this work, we investigate the suitability of various AI text detection methods for identifying LLM generations in the peer review process. While limited prior work has analyzed the presence of AI-generated text in peer reviews at the corpus level  (see Appendix A for further discussion of related work), our study is the first to investigate the detectability LLM generations at the individual review level, which is necessary to address this problem in practice. Specifically, we produce AI-generated peer reviews for papers submitted to AI conferences prior to the introduction of ChatGPT using different LLMs and prompting methods. We then evaluate multiple open-source and proprietary AI text detection models on their ability to distinguish real peer reviews collected for these conferences from our AI-generated peer reviews for the same papers.

Our results show that all existing AI-text detection methods are limited in their ability to robustly detect AI-generated reviews while maintaining a low number of false positives. We propose an alternative approach to detecting AI-generated peer reviews by comparing the semantic similarity of a given review to a set of reference AI-generated reviews for the same paper, which surpasses the performance of all existing approaches in detecting GPT-4o written peer reviews. Our work demonstrate the challenging nature of detecting AI-written text in peer reviews and motivates the need for further research on methods to address this unethical use of LLMs in the peer review process.

## 2 Methodology

### Data Collection

We used the OpenReview client API  to collect submitted manuscripts and their reviews for the ICLR conference from 2019 to 2024. The total numbers of submission and reviews are summarized in Table S1. We use two LLMs to generate AI peer reviews for these manuscripts, OpenAI's GPT-4o  and the open-source instruction-tuned Llama-3.1 (70b) , to generate AI peer reviews. Manuscripts are converted from PDF to Markdown format, excluding the Bibliography and Acknowledgement sections to focus on the core content relevant for review. Prompts used for generating AI reviews are adapted from Lu et al.  with two major changes. First, recognizing that LLM-generated text is sensitive to prompt variations, we introduce four distinct reviewer archetypes--"balanced," "nitpicky," "innovative," and "conservative"--to simulate the diversity of real-world peer review scenarios. Second, we provide the LLMs with the ICLR 2022 reviewer guidelines  with a minor modification. The ICLR guideline emphasize general instructions about how to approach peer review, rather than focusing on rubric and scoring scales as in the NeurIPS reviewer form. Our complete prompts are included in Appendix F. We collect a total of 16,000 AI-generated reviews for 500 random submissions for each ICLR conference of year from 2021-2024. Each AI-generated review contains five sections, and they were combined for detection tasks (see Appendix C for details).

### AI-Generated Text Detection Methods

**Open-source AI text detection models.** We utilize two supervised fine-tuned pretrained language models. The first is from , where the author collect a Human ChatGPT Comparison Corpus (HC3) consisting of question-answer pairs created by human experts and generated by ChatGPT. The authors trained two Roberta-based models : one detects whether answers are human or AI-generated using paired data, while the other evaluates single answers. We use the latter for our assessment. The second model is a Longformer . As tested in , this model has demonstrated improved detection AUROC and generalization performance on the large-scale MAGE testbed, which includes eight distinct writing tasks, surpassing other methods such as GLTR , FastText , and DetectGPT . For both models, we calculate the probability of each sentence in the review, averaging these to determine the final probability of the entire review being classified as AI-generated.

Figure 1: Proportion of reviews submitted to ICLR detected as AI-generated.

**Originality AI API.** The Originality AI API is a commercial AI text detection service, and some studies have reported its high performance [14; 15]. It returns an AI score ranging between 0 and 1 which indicates the model's confidence that an input text was AI generated. While values greater than 0.5 indicate that the model has more confidence that an input text is AI-written than human-written, a higher threshold on the AI score may provide a more desirable balance between true positive and false positive classifications. Therefore, we investigate the trade-off between true positive and false positive classifications using a range of different thresholds on the returned score.

**LLM-based detection.** We employ the LLM-as-a-judge approach , utilizing both GPT-4o and Llama-3.1-70B models. The LLMs are instructed to provide two outputs: a binary decision ("human" or "AI") and a rationale for their decision, encouraging chain-of-thought reasoning.

**Anchor Embeddings Detection.** We propose a new method aiming to detect AI-generated reviews by comparing their semantic similarity to an AI-generated review for the same paper, which we refer to as the "Anchor Review". Specifically, we use GPT-4o or Llama3-70B to generate a review given a paper. We use simple prompt without any prior knowledge of prompts that used for generating the tested AI reviews. For each review we want to test, we use an off-the-shelf embedding model to create vector representations of both the Anchor Review and the test review. We then compute the cosine similarity between these two embeddings. The higher the similarity score, the more likely it is that the test review was generated by AI. By setting a threshold on this similarity score, we can classify reviews as either human-written or AI-generated at varying levels of sensitivity.

## 3 Results

**AI review detectability.** Table 1 compares the TPR of different detection methods on ICLR 2022 reviews when calibrated for a FPR of 0.05 and 0.20. As there is no threshold to tune the sensitivity of the GPT-4o and Llama 3.1 Judge models, we report only the single TPR and FPR for the binary classifications produced by these detectors. While GPT-4o is able to identify its own peer reviews over 80% of the time and those written by Llama 3.1 nearly 95% of the time, this comes at the cost of flagging 20% of all human reviews as AI-written. In contrast, the Originality AI API and our GPT-4o and Llama 3.1 Embedding models correctly identify nearly all GPT-4o written reviews at an identical FPR. At a lower (and more practiacal) FPR of 0.05, our GPT-4o and Llama 3.1 Embedding methods perform the best at detecting GPT-4o written reviews, identifying 97% and 95% of all such reviews (respectively). These methods also correctly identify 87-90% of Llama written peer reviews, while the Originality AI API identifies nearly all Llama 3.1 written reviews but is significantly worse at detecting GPT-4o written reviews. The RoBERTa and Longformer classifiers perform the worst among evaluated methods at both FPR levels. While we mainly present the results of 2022 year reviews, we provide additional results for ICLR reviews from 2021, 2023, and 2024 for other detection models in Appendix B.

Figure 2 provides area under the curve (AUC) plots for threshold-based classification methods which can be tuned for sensitivity, calculated separately for GPT-4o written peer reviews (Figure 1(a)) and Llama-3.1 written peer reviews (Figure 1(b)) of ICLR 2022 papers. These plots illustrates the relationship between the TPR and FPR for different AI text classification thresholds. Our GPT-4o

    &  &  \\   & GPT-4o Reviews & Llama Reviews & GPT-4o Reviews & Llama Reviews \\  GPT-4o Judge & - & - & 0.8040 & 0.9465 \\ Llama 3.1 Judge & 0.2390 & 0.7637 & - & - \\ Originality AI API & 0.5856 & **0.9985** & **0.9989** & **1.0000** \\ RoBERTa Classifier & 0.1855 & 0.8105 & 0.5110 & 0.9180 \\ Longformer Classifier & 0.4570 & 0.7860 & 0.8100 & 0.9375 \\ GPT-4o Anchor (ours) & **0.9670** & 0.8215 & 0.9985 & 0.9550 \\ Llama 3.1 Anchor (ours) & 0.9165 & 0.8880 & 0.9880 & 0.9675 \\   

Table 1: Comparison of true positive rates (TPR) for different AI detection methods applied to ICLR 2022 reviews. Since LLM judges only provide binary decisions instead of probability scores, they have fixed false positive rate (FPR) thresholds: 0.05 (GPT-4o judge) and 0.20 (Llama-3.1 judge). We use these two for the other methods and get the true positive rates (TPR).

and Llama-3.1 Embedding models have the highest AUC for GPT-4o written reviews, whereas the Originality AI API has the highest AUC for Llama 3.1 written reviews. Most Llama 3.1 reviews are correctly classified as AI-written by all methods at relatively low FPR values. In contrast, GPT-4o review detections are much lower at small FPR values for models other than our embedding-based approach; for example, the Originality AI API can detect only a little over half of GPT-4o written peer reviews when the classification threshold is calibrated to a FPR of 0.05. This shows how existing AI text detection methods struggle to consistently detect peer reviews generated by GPT-4o without also triggering a high number of false positives for human-written reviews.

**Analysis of LLM judge justifications.** To understand the decision basis of LLMs, we present findings from our analysis on the rationales behind LLM's decisions. We instructed an LLM to provide a brief justification for each decision. To identify the most representative samples, we first cluster the GPT-4o judge's rationale texts using t-SNE, using the SFR-Embedding model. The t-SNE plot demonstrates that the rationales for AI and human decisions are well-separated (Figure S3), indicating systematic differences in them. Then, we pick the top 30 samples and the bottom 30 samples along the first dimension and identify common themes in the decision rationale (Table S4).

These findings suggest that the GPT-4o judge primarily relies on stylistic and content depth cues to distinguish between AI-generated versus human-written reviews, mainly through contrasting attributes. For example, the rationales for the AI label are characterized by repetitiveness, lack of coherence, formal tone, generic critique, and lack of specificity--issues commonly observed in LLM-generated texts. In contrast, human-written reviews exhibit the opposite attributes, such as nuanced discussions, subjective opinions, conversational tone, specific critique, and detailed references.

Some of these findings raise concerns about the use of LLM-as-a-judge style detection models, or any model that takes advantage of similar features. Echoing the findings of Liang et al. , "non-native language use" emerged as one of the reasons for AI labels, raising fairness concerns of discriminating non-native English speakers. More broadly, even native speakers with unconventional writing styles (e.g., "awkward sentence structure") may be unfairly penalized. These issues should be considered in future AI detector designs to minimize unintended biases.

## 4 Conclusion

In this work, we showed that existing approaches for detecting AI text are poorly suited to the challenging problem of identifying AI-generated peer reviews. While high detection rates are possible with existing methods, this comes at the cost of relatively high rates of falsely identifying human-written reviews as containing AI text, which must be minimized in practice. We proposed a new approach which intentionally generates AI-written reviews for a given paper to serve as a basis for comparing semantic similarity to other evaluated reviews, achieving much higher accuracy in identifying GPT-4o written peer reviews while maintaining a low level of false positives. Our results demonstrate the promise of this approach while also motivating the need for further research on methods for detecting unethical applications of LLMs in the peer review process.

Figure 2: AUC plots for GPT-4o reviews (left) and Llama 3.1 reviews (right) of ICLR 2022 papers, calculated using five different AI text detection models.