# AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis

Susan Liang\({}^{1}\) Chao Huang\({}^{1}\) Yapeng Tian\({}^{1}\)

**Anurag Kumar\({}^{2}\) Chenliang Xu\({}^{1}\)**

\({}^{1}\)University of Rochester \({}^{2}\)Meta Reality Labs Research

###### Abstract

Can machines recording an audio-visual scene produce realistic, matching audio-visual experiences at novel positions and novel view directions? We answer it by studying a new task--real-world audio-visual scene synthesis--and a first-of-its-kind NeRF-based approach for multimodal learning. Concretely, given a video recording of an audio-visual scene, the task is to synthesize new videos with spatial audios along arbitrary novel camera trajectories in that scene. We propose an acoustic-aware audio generation module that integrates prior knowledge of audio propagation into NeRF, in which we implicitly associate audio generation with the 3D geometry and material properties of a visual environment. Furthermore, we present a coordinate transformation module that expresses a view direction relative to the sound source, enabling the model to learn sound source-centric acoustic fields. To facilitate the study of this new task, we collect a high-quality Real-World Audio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method on this real-world dataset and the simulation-based SoundSpaces dataset. We recommend that readers visit our project page for convincing comparisons: https://liangsusan-git.github.io/project/avnerf/.

## 1 Introduction

We study a new task, _real-world audio-visual scene synthesis_, to generate target videos and audios along novel camera trajectories from source audio-visual recordings of known trajectories. By learning from real-world source videos with binaural audio, we aim to generate target video frames and spatial audios that exhibit consistency with the given camera trajectory visually and acoustically. This consistency ensures perceptual realism and immersion, enriching the overall user experience.

As far as we know, attempts in the audio-visual learning literature  have yet to succeed in solving this challenging task thus far. Although there are similar works , these methods have constraints that limit their ability to solve this new task. Luo et al.  propose neural acoustic fields to model sound propagation in a room. Su et al.  introduce representing audio scenes by disentangling the scene's geometry features. These methods are tailored for estimating room impulse response signals in a simulation environment that are difficult to obtain in a real-world scene. Concurrent to our work, ViGAS proposed by Chen et al.  learns to synthesize new sounds by inferring the audio-visual cues. However, ViGAS is limited to a few viewpoints for audio generation.

We introduce AV-NeRF, a novel NeRF-based method of synthesizing real-world audio-visual scenes. AV-NeRF enables the generation of videos and spatial audios, following arbitrary camera trajectories. It utilizes source videos and camera poses as references. AV-NeRF consists of two branches: A-NeRF, which learns the acoustic fields of an environment, and V-NeRF, which models color and density fields. We represent a static audio field as a continuous function using A-NeRF, which takes the listener's position and head direction as input. A-NeRF effectively models the energy decay of sound as the sound travels from the source to the listener by correlating the listener's position with theenergy reduction effect. Moreover, A-NeRF accounts for the impact of the listener's orientation on sound perception by establishing a correlation between the audio channel difference and the listener's direction. Consequently, A-NeRF facilitates **distance-sensitive** and **direction-aware** audio synthesis, resulting in the generation of realistic binaural sound. We utilize the vanilla NeRF  as our V-NeRF for vision rendering since the rendering of the purely visual world has been extensively researched [17; 16; 18; 19; 20] and is not the primary focus of our work.

To further enhance synthesis quality, we introduce an acoustic-aware audio generation method and a coordinate transformation mechanism. Considering that the 3D geometry and material properties of an environment determine sound propagation [21; 22; 4; 23], we propose an acoustic-aware audio generation module called AV-Mapper. AV-Mapper extracts geometry and material information from V-NeRF and feed them to A-NeRF, enabling A-NeRF to generate audios with acoustic awareness. In visual space, NeRF  expresses the viewing direction \((,)\) using an absolute coordinate system, where the directions of two parallel light rays are represented identically. However, in auditory space, this expression method is unsuitable as it ignores the spatial relationship between the sound emitter and receiver. Human perception of sound source relies on the relative direction of the sound source to the head orientation. Given this observation, we propose a coordinate transform mechanism that expresses the viewing direction relative to the sound source. This approach encourages our model to learn sound source-centric acoustic fields.

Our work represents the initial step towards addressing the audio-visual scene synthesis task in real-world settings. As such, there is currently no publicly available dataset that fulfills our specific requirements. To facilitate our study, we curated a high-quality audio-visual dataset called RWAVS (Real-World Audio-Visual Synthesis), which encompasses multimodal data, including camera poses, video frames, and realistic binaural (stereo) audios. In order to enhance the diversity of our dataset, we collected data from various environments, ranging from offices and apartments to houses and even outdoor spaces. We will release this dataset to the research community. Experiments on the RWAVS and the synthetic SoundSpaces datasets can validate the effectiveness of our proposed approach. It demonstrates AV-NeRF's capability of synthesizing novel-view audio-visual scenes in a range of environments, including both real-world and synthetic, as well as indoor and outdoor settings.

In summary, our contributions include: (1) proposing a novel approach to synthesize audio-visual scenes in real-world environments at novel camera poses; (2) introducing a new acoustic-aware audio generation method that incorporates our prior knowledge of sound propagation; (3) suggesting a coordinate transformation mechanism to effectively express sound direction; (4) curating a high-quality audio-visual dataset for benchmarking this task; and (5) demonstrating the advantages of our method through quantitative and qualitative evaluations.

## 2 Related Work

Our work is closely related to several areas, including novel-view synthesis, vision-guided audio spatialization, simulation-based acoustic synthesis, and learning-based audio generation. We elaborate on each of these topics in the following section.

**Neural Fields.** Our method is based on neural fields, particularly Neural Radiance Fields (NeRF) . NeRF employs MLPs to learn an implicit and continuous representation of visual scenes, enabling the synthesis of novel views. Building upon NeRF's foundation, several works have extended its applicability to broader domains, including in-the-wild images , (silent) video [25; 26; 27], audio

Figure 1: NeRF learns to render visual scenes from novel viewpoints. Beyond visual rendering, we introduce AV-NeRF, a method for synthesizing audio-visual scenes with video frames and binaural audios along new camera trajectories. Integrating coherent sight and sound creates an immersive and realistic perceptual experience for users.

[12; 13], and audio-visual  content. Luo et al.  propose neural acoustic fields (NAF) to capture the sound propagation in an environment, and Su et al.  introduce disentangling a scene's geometry features for audio scene representation (INRAS). Despite the compelling results achieved by NAF and INRAS, their application in the real world is hindered by their reliance on ground-truth impulse response signals that are simulated in synthetic environments. Furthermore, the discretization of position and direction in these approaches restricts the range of camera poses they can handle, whereas our model can handle continuous position and direction queries. Du et al.  proposed a manifold learning method that maps vectors from a latent space to audio and image spaces. Although the learned manifold allows for audio-visual interpolation, the model lacks support for controllable audio-visual generation. In contrast, our method focuses on learning audio-visual representations that are explicitly conditioned on spatial coordinates, enabling controllable generation.

**Visually Informed Spatial Audio Generation.** Considering that images often reveal the position of a sound source and/or the structure of a scene, many visually informed audio spatialization approaches [7; 28; 29; 30; 31; 15] have been proposed. Among them, Gao and Grauman  focus on normal field-of-view videos and binaural audios. Zhou et al.  propose a unified framework to solve the sound separation and stereo sound generation at the same time. Xu et al.  propose a Pseudo2Binaural pipeline that augments audio data using HRIR function . Most recently, Chen et al.  proposed the Visually-Guided Acoustic Synthesis (ViGAS) method for generating novel-view audios. However, AV-NeRF differs from this method in two ways: (1) ViGAS relies on ground-truth images for audio transformation, whereas AV-NeRF addresses the absence of ground-truth images by utilizing rendered images from learned vision fields; (2) ViGAS only supports a limited number of viewpoints for audio synthesis, while AV-NeRF has the capability to render novel videos at arbitrary camera poses.

**Geometry and Material Based Acoustic Simulation.** Several works [4; 33; 22; 23; 34] focus on simulating acoustic environments by modeling their 3D geometry and material properties. SoundSpaces, proposed by Chen et al. , is an audio platform that calculates the room impulse response signals for discrete listener and sound source positions. Extending this work, Chen et al.  introduce SoundSpaces 2.0, which enables acoustic generation for arbitrary microphone locations. Tang et al.  propose calibrating geometric acoustic ray-tracing with a finite-difference time-domain wave solver to compute high-quality impulse responses. Additionally, Li et al.  propose a method that blends the early reverberation portion, modeled using geometric acoustic simulation and frequency modulation, with a late reverberation tail extracted from recorded impulse responses. In contrast to these simulation methods, our approach implicitly leverages learned geometry and material information for audio-visual scene synthesis.

**Learning Based Audio Generation.** Learning-based methods exploit the powerful modeling ability of neural networks to synthesize audios. Tang et al.  propose the use of neural networks to estimate reverberation time (\(T_{60}\)) and equalization (EQ). Ratnarajah et al. [35; 36; 37] employ generative adversarial networks (GANs) to supervise realistic room impulse response generation. Richard et al.  introduce a binauralization network that models the sound propagation linearly and nonlinearly. Recent research has also investigated learning from noisy data  and few-shot data .

## 3 Task Definition

The real-world audio-visual scene synthesis task is to generate visual frames and corresponding binaural audios for arbitrary camera trajectories. Given a static environment \(E\) and multiple observations \(O=\{O_{1},O_{2},,O_{N}\}\) of this environment, where \(N\) is the number of training samples and each observation \(O_{i}\) includes the camera pose \(p=(x,y,z,,)\), the mono source audio clip \(a_{s}\), the recorded binaural audio \(a_{t}\), and the image \(I\), this task aims to synthesize a new binaural audio \(a_{t}^{*}\) and a novel view \(I^{*}\) based on a camera pose query \(p^{*}\) and a source audio \(a_{s}^{*}\), where \(p^{*}\) is distinct from the camera poses in all observations. This process can be formatted as:

\[(a_{t}^{*},I^{*})=f(p^{*},a_{s}^{*}|O,E),\] (1)

where \(f\) is a mapping function. Combining predicted \(a_{t}^{*}\) and \(I^{*}\) of all poses in the queried camera trajectory, we can synthesize a realistic video with spatial audio. This task poses several challenges: (1) \(p^{*}\) is a novel viewpoint distinct from all observed viewpoints, (2) the synthesized binaural audio \(a_{t}^{*}\) is expected to exhibit rich spatial effects contributing to its realism, (3) during inference, no observation \(O\) is accessible and the mapping function \(f\) relies solely on the learned fields for scene synthesis. It should be noted that the position of the sound source is known in the environment \(E\).

## 4 Method

Our method aims to learn neural fields that can synthesize real-world audio-visual scenes from novel poses. The entire pipeline is illustrated in Figure 2. Our model comprises three trainable modules: V-NeRF, A-NeRF, and AV-Mapper. A-NeRF is responsible for generating acoustic masks, V-NeRF focuses on generating visual frames, and AV-Mapper is optimized to extract geometry and material information from V-NeRF, integrating this information into A-NeRF.

### V-NeRF

NeRF  uses a Multi-Layer Perceptron (MLP) to implicitly and continuously represent a visual scene. It effectively learns a mapping from camera poses to colors and densities:

\[:(x,y,z,,)(,),\] (2)

where \(=(x,y,z)\) is the 3D position, \(=(,)\) is the direction, \(=(r,g,b)\) is the color, and \(\) is the density. In order to achieve view-dependent color rendering \(\) and ensure multi-view consistency,  introduces two MLP components within NeRF. The first MLP maps a 3D coordinate \((x,y,z)\) to density \(\) and a corresponding feature vector. The second MLP then takes the feature vector and 2D direction \((,)\) as inputs, producing the color \(\). This process is illustrated in Fig. 3. NeRF then uses the volume rendering method  to generate the color of any ray \((t)=+t\) as it traverses through the visual scene, bounded by near \(t_{n}\) and far \(t_{f}\):

\[C()=_{t_{n}}^{t_{f}}T(t)((t))( (t),)dt,\] (3)

where \(T(t)=(-_{t_{n}}^{t}((s))ds)\). The depth of any ray \(\) can be calculated similarly by replacing \(((t),)\) with \(t\):

\[D()=_{t_{n}}^{t_{f}}T(t)((t))tdt.\] (4)

These continuous integrals are estimated by quadrature in practice . Subsequently, RGB and depth images can be rendered for any given camera pose using V-NeRF (Fig. 3). We apply positional encoding to all input coordinates to preserve the high-frequency information of the generated images.

Figure 2: The pipeline of our method. Given the position \((x,y,z)\) and viewing direction \((,)\) of a listener, our method can render an image the listener would see and the corresponding binaural audio the listener would hear. Our model consists of V-NeRF, A-NeRF, and AV-Mapper. A-NeRF learns to generate acoustic masks, V-NeRF learns to generate visual frames, and AV-Mapper is optimized to integrate geometry and material information extracted from V-NeRF into A-NeRF.

### A-NeRF

The goal of A-NeRF is to learn a neural acoustic representation capable of mapping 5D coordinates \((x,y,z,,)\) to corresponding acoustic masks \(_{m},_{d}^{F}\), where \(_{m}\) quantifies the change in audio magnitude with respect to the position \((x,y,z)\) while \(_{d}\) characterizes the impact of the direction \((,)\) on the channel difference of binaural audios, and \(F\) is the number of frequency bins:

\[:(x,y,z,,)(_{m},_{d}).\] (5)

After training A-NeRF, we can synthesize binaural sound that contains rich spatial and acoustic information by fusing the magnitude of any input audio with predicted \(_{m}\) and \(_{d}\).

In practice, we simplify the problem of audio synthesis by discarding the \(z\) axis and the \(\) direction, which represents coordinates related to height, and instead focus on the 2D space. On RWAVS and SoundSpaces datasets, we observe that the sound received by the listener exhibits more distinct variations along the horizontal plane, as opposed to the vertical axis. Fig. 4 depicts the pipeline of audio synthesis. Similar to V-NeRF, A-NeRF consists of two MLP components that parameterize different aspects of acoustic fields. The first MLP takes as input the 2D position \((x,y)\) and frequency query \(f[0,F]\), producing a mixture mask \(_{m}\) for frequency \(f\) and a feature vector. The mask \(_{m}\) quantifies the change in audio magnitude based on the distance between the sound source and the sound receiver. The feature vector serves as an implicit embedding of the input information and the acoustic scene. Then we concatenate the feature vector with the transformed direction \(^{}\) (explained in Sec. 4.4) and pass them to the second MLP. The second MLP associates the direction \(^{}\) with the channel difference of binaural audios. Given a fixed sound emitter, the difference between two channels of the received audio changes as the direction \(^{}\) of the sound receiver varies. For instance, when the receiver faces the emitter, the energy of the two channels will be approximately equal. Conversely, when the emitter is on the left side of the receiver, the left channel will exhibit higher energy compared to the right channel. The second MLP is optimized to generate a difference mask \(_{d}\) that characterizes such direction influence. We query A-NeRF with all frequencies \(f\) within \([0,F]\) to generate the complete masks \(_{m}\) and \(_{d}\).

After obtaining the mixture mask \(_{m}\) and the difference mask \(_{d}\), we can synthesize binaural audios by composing these masks and input audios (Fig. 4). For an input source audio \(a_{s}\), we initially employ the short-time Fourier transform (STFT) to calculate the magnitude \(s_{s}^{F W}\), where \(W\) represents the number of time frames and \(F\) denotes the number of frequency bins. We then multiply \(s_{s}\) with mask \(_{m}\) to obtain the mixed magnitude \(s_{m}\). Additionally, we multiply \(s_{m}\) and mask \(_{d}\) to predict the difference magnitude \(s_{d}\). Then we add mixture magnitude \(s_{m}\) and difference magnitude \(s_{d}\) to compute the magnitude of left channel \(s_{l}\), and subtract \(s_{d}\) from \(s_{m}\) to compute the magnitude of right channel \(s_{r}\). We further refine \(s_{l}\) and \(s_{r}\) using 2D convolution. Finally, we apply Inverse STFT to the predicted \(s_{l}\) and \(s_{r}\), respectively, to synthesize the binaural audio \(a_{t}\). Because A-NeRF operates on the magnitude, we use the phase information of the source audio for the Inverse STFT process. Instead of estimating two channels of the target audio directly, we predict the mixture and the difference between the two channels of the target audio. Gao and Grauman  suggest that direct two-channel predictions can lead to shortcut learning in the spatialization network.

### AV-Mapper

Given the fact that 3D geometry and material property determine sound propagation in an environment (_prior knowledge_), we propose an acoustic-aware audio generation method that integrates color and depth information estimated by V-NeRF with A-NeRF. When V-NeRF learns to represent visual scenes, it can learn the color \(\) and density \(\) function of the environment, thanks to the multi-view

Figure 3: The pipeline of V-NeRF. Given the 3D position \((x,y,z)\) and direction \((,)\) of a camera pose, V-NeRF can render corresponding RGB and depth images. V-NeRF consists of two MLPs with the first MLP estimating density \(\) and the second one predicting color \(\).

consistency constraint. Utilizing volume rendering [16; 41], we can synthesize the RGB and depth image for any given camera pose. The RGB image captures the semantics and category of each object, implicitly indicating their material properties. And the depth image depicts the geometric structure of the environment. By providing A-NeRF with a combination of RGB and depth images, we can offer rich environmental knowledge to A-NeRF.

Specifically, when synthesizing the binaural audio for a given camera pose, our process begins by rendering a pair of RGB and depth images with the same camera pose using Eq. 3 and Eq. 4, respectively. We exploit a pre-trained encoder (e.g., ResNet-18  pre-trained on ImageNet dataset ) to extract color and depth features from the RGB and depth images. The extracted features can serve as important indicators of the environment. Next, we develop an AV-Mapper, implemented as an MLP network, to project the color and depth features into a latent space. Our goal for the A-V Mapper is two-fold: (1) to extract meaningful embeddings while discarding unnecessary features, and (2) to align the features in the original space of the pre-trained image encoder with the space relevant to our specific problem. The output embedding of the AV-Mapper, denoted as \(^{c}\), with \(c\) representing the width of each linear layer in A-NeRF, is then added to the input of A-NeRF for controlling the audio synthesis. Ablation studies (Sec. 5.2) demonstrate the slight advantage of this fusion method over others.

### Coordinate Transformation

Viewing direction \((,)\) in V-NeRF is expressed in an absolute coordinate system. This is a natural practice in visual space such that directions of two parallel light rays are expressed identically. However, this expression method in audio space is less suitable because the human perception of the sound direction is based on the relative direction to the sound source instead of the absolute direction. To address this limitation, we propose expressing the viewing direction of a camera pose relative to the sound source. This coordinate transformation encourages A-NeRF learning a sound source-centric acoustic field, enhancing spatial audio generation.

Given the 2D position \((x,y)\) and the direction \(\) of the camera, as well as the position of the sound source \((S_{x},S_{y})\), we obtain two direction vectors: \(_{1}=(S_{x}-x,S_{y}-y)\) and \(_{2}=((),())\). \(_{1}\) represents the direction from the camera to the sound source, and \(_{2}\) represents the camera's direction in the absolute coordinate system. By calculating the angle between \(_{1}\) and \(_{2}\), denoted as the relative direction \(^{}=(_{1},_{2})\), we obtain the rotation angle relative to the sound source. This angle \(^{}\) allows different camera poses to share the same direction encoding if they face the sound source at the same angle.

After computing the relative angle \(^{}\), we choose learnable embeddings instead of positional encoding [16; 45] to project \(^{}\) into a high-frequency space. We use the embedding \(^{4 c}\) to represent four discrete directions, namely, \(0^{}\), \(90^{}\), \(180^{}\), and \(270^{}\), where \(4\) is the number of directions and \(c\) is the width of A-NeRF. Given a relative angle \(^{}[0^{},360^{})\), we linearly interpolate the direction embedding \(^{}^{c}\) according to the angle between \(^{}\) and four discrete directions. We add the

Figure 4: The pipeline of A-NeRF. Given the 2D position \((x,y)\) and direction \(\) of a camera pose, A-NeRF can render mixture mask \(_{m}\), and difference mask \(_{d}\), which then be used for audio synthesis. A-NeRF is also composed of two MLPs, with the first predicting mixture mask \(_{m}\) and the second estimating difference mask \(_{d}\). The direction \(\) is transformed relatively to the sound source prior to inputting into A-NeRF.

interpolated embedding \(^{}\) to the input of each linear layer in A-NeRF, thereby providing A-NeRF with direction information. Ablation studies (Sec. 5.2) show that this encoding method performs best.

### Learning Objective

The loss function of V-NeRF is the same as :

\[_{V}=||C()-()||^{2},\] (6)

where \(C()\) is the ground-truth color along the ray \(\) and \(()\) is the color rendered by V-NeRF. After training V-NeRF, we optimize A-NeRF and AV-Mapper together with the L2 loss function:

\[_{A}=||s_{m}-_{m}||^{2}+||s_{l}-_{l}||^{2}+||s_{r}- _{r}||^{2},\] (7)

where \(s_{m,l,r}\) are the predicted magnitudes, \(_{m,l,r}\) are the ground-truth magnitudes, and subscript \(m,l,r\) are mixture, left, and right, respectively. The first term of \(_{A}\) encourages A-NeRF to predict masks that represent spatial effects caused by distance, while the second and third term encourages A-NeRF to generate masks that capture the differences between two channels.

## 5 Experiments

### Datasets

To the best of our knowledge, our method is the first NeRF-based system capable of synthesizing real-world videos with perceptually realistic binaural audios at arbitrary poses. However, existing datasets do not meet the specific requirements of our experiments, particularly in terms of simultaneously providing camera poses, high-quality binaural audios, and images. Therefore, we curated a high-quality audio-visual scene dataset (_real_) to address this gap and facilitate further research on this problem. Additionally, we utilize (_synthetic_) SoundSpaces dataset  to validate our method.

**(1) RWAVS Dataset.** We collected the Real-World Audio-Visual Scene (RWAVS) dataset to benchmark our method. In order to increase the diversity of our dataset, we recorded data across different scenarios. Fig. 5 shows the example scenarios we used for data recording, including both indoor and outdoor environments, which we believe represent most daily settings. RWAVS dataset comprises multimodal data, including camera poses, high-quality binaural audios, and videos. Unlike Replay-NVAS dataset , where the environment and the recording viewpoint are constant, RWAVS dataset contains various viewpoints in diverse environments. During data recording, we randomly moved around the environment while holding the device, capturing various acoustic and visual signals. RWAVS dataset encompasses all positions and directions (\(360^{o}\)) within an environment.

In detail, we employed a 3Dio Free Space XLR binaural microphone for capturing high-quality stereo audio, a TASCAM DR-60DMKII for recording and storing audio, and a GoPro Max for capturing accompanying videos. Additionally, an LG XBOOM 360 omnidirectional speaker was used as the sound source. For each environment and sound source combination, we collected data ranging from 10 to 25 minutes, resulting in a total collection of 232 minutes (3.8 hours) of data from diverse environments with varying source positions.

We extract key frames at \(1\) fps from recorded videos and use COLMAP  to estimate the corresponding camera pose. Each key frame is accompanied by one-second binaural audio and one-second

Figure 5: Example scenes of RWAVS dataset. RWAVS dataset consists of diverse audio-visual scenes indoors and outdoors. Each environment possesses distinct acoustics and structures.

source audio, forming a complete data sample. For audio clips with noticeable background noise, we perform noise suppression using Adobe Audition . We split 80% data as training samples and the rest as validation samples. After pre-processing, we obtain \(9850\) and \(2469\) samples for training and validation, respectively. This dataset is challenging because of the diverse environments and various camera poses. We will release this dataset to the research community.

**(2) SoundSpaces Dataset.** While RWAVS offers realistic training samples, its realism restricts its scale because it is time-consuming to record high-quality multimodal data in the real world. Therefore, we use the synthetic SoundSpaces dataset to augment our experiments. To evaluate our method on SoundSpaces dataset, we modify AV-NeRF to estimate impulse responses instead of the acoustic mask while keeping all other components intact. We follow NAF  selecting six representative indoor scenes, consisting of two single rooms with rectangular walls, two single rooms with non-rectangular walls, and two multi-room layouts. In each scene, SoundSpaces dataset provides an extensive collection of impulse response signals for sound source and sound receiver pairs, which are densely sampled from a 2D room grid. Each pair includes four discrete head orientations (0\({}^{}\), 90\({}^{}\), 180\({}^{}\), and 270\({}^{}\)), and each orientation is associated with two-channel binaural RIRs. We render RGB and depth images for each sound receiver pose using Habitat-Sim simulator [48; 49]. We maintain the same training/test split as NAF, allocating 90% data for training and 10% data for testing.

### Results on RWAVS Dataset

**Comparison with State-of-the-art.** We compare AV-NeRF with the following baselines: (1) Mono-Mono duplicates the source audio \(a_{s}\) twice to generate a fake binaural audio without modifying the source audio; (2) Mono-Energy assumes that the average energy of the target audio \(a_{t}\) is known, scales the energy of the input audio to match the target, and duplicates the scaled audio to generate a stereo audio; (3) Stereo-Energy assumes that the energy of the two channels of the target audio \(a_{t}\) is known, separately scales the energy of the input audio to match the target, and combines the two scaled channels to generate a stereo audio; (4) IRNAS  learns representing audio scenes by disentangling scene's geometry features with implicit neural fields, and we adapt INRAS to predict wave masks on RWAVS dataset; (5) NAF  designs local feature grids and an implicit decoder to capture the sound propagation in a physical scene, and we modify NAF to predict magnitude masks on RWAVS dataset; (6) ViGAS  achieves novel-view acoustic synthesis by analyzing audio-visual cues from source viewpoints. We select magnitude distance (MAG) , which measures the audio quality in the time-frequency domain, and envelope distance (ENV) , which measures the audio quality in the time domain, to evaluate various methods. Please refer to the supplementary material for implementation details.

    &  &  &  &  \\  & MAG & ENV & MAG & ENV &  & ENV & MAG & ENV \\   Baseline \\ Ours w/o AV \\ Ours w/o CT \\  } & 2.287 & 0.157 &  &  &  &  &  \\  & 1.791 &  &  &  &  &  &  &  \\    & 1.701 &  &  &  &  &  &  &  &  \\    & 1.504 &  &  &  &  &  &  &  &  &  \\   

Table 2: Ablation studies. We break down AV-NeRF to analyze the contribution of each component. **Left:** the inclusion of AV-Mapper (AV) and Coordinate Transformation (CT), **Middle:** different multimodal fusion methods, and **Right:** different direction encoding methods.

    &  &  &  &  &  \\  & MAG & ENV & MAG & ENV & MAG & ENV & MAG & ENV \\  Mono-Mono & 9.269 & 0.411 & 11.889 & 0.424 & 15.120 & 0.474 & 13.957 & 0.470 & 12.559 & 0.445 \\ Mono-Energy & 1.536 & 0.142 & 4.307 & 0.180 & 3.911 & 0.192 & 1.634 & 0.127 & 2.847 & 0.160 \\ Stereo-Energy & 1.511 & 0.139 & 4.301 & 0.180 & 3.895 & 0.191 & 1.612 & 0.124 & 2.830 & 0.159 \\  INRAS  & 1.405 & 0.141 & 3.511 & 0.182 & 3.421 & 0.201 & 1.502 & 0.130 & 2.460 & 0.164 \\ NAF  & 1.244 & 0.137 & 3.259 & 0.178 & 3.345 & 0.193 & 1.284 & 0.121 & 2.283 & 0.157 \\ ViGAS  & 1.049 & 0.132 & 2.502 & 0.161 & 2.600 & 0.187 & 1.169 & 0.121 & 1.830 & 0.150 \\ Ours & **0.930** & **0.129** & **2.009** & **0.155** & **2.230** & **0.184** & **0.845** & **0.111** & **1.504** & **0.145** \\   

Table 1: Comparison with state-of-the-art methods on RWAVS dataset.

AV-NeRF outperforms all baselines across different environments, including office, house, apartment, and outdoors, by a significant margin (Table 1). AV-NeRF outruns INRAS with 0.956 on the overall MAG metric (39% relative improvement) and 0.019 on the average ENV metric (11.6%). AV-NeRF surpasses NAF with 0.779 on MAG metric (34%) and 0.012 on ENV metric (8%). Our approach is better than ViGAS in terms of both MAG (1.504 compared to ViGAS's 1.830) and ENV (0.145 compared to ViGAS's 0.150).

**Ablation Studies.** We conduct ablation studies on AV-NeRF to analyze the contribution of different components. We combine A-NeRF and V-NeRF as our baseline, which does not contain AV-Mapper (Sec. 4.3) or coordinate transformation (Sec. 4.4). The ablation results are as follows: (1) AV-Mapper (AV) and coordinate transformation (CT) play important roles in learning audio scenes (Table 2 left). The exclusion of either component degrades the generation quality; (2) adding visual information to the input of A-NeRF is the most effective multimodal fusion method compared with concatenation and adding visual information to all layers of A-NeRF (Table 2 middle); (3) using embeddings to represent relative angles outperforms applying positional encoding to either absolute or relative angles (Table 2 right). "Absolute Direction" represents applying positional encoding to the absolute angle, "Relative Direction" means transforming the relative angle with the positional encoding, and "Relative Embedding" is the embedding method.

**Visualization.** We visualize the synthesized audio-visual scenes in Fig. 6 to intuitively assess the generation quality of our model. AV-NeRF can synthesize realistic binaural audios that have the same signal envelope and channel difference as the ground-truth audios.

### Results on SoundSpaces Dataset

We compare AV-NeRF with traditional audio coding methods [50; 51] and advanced learning-based neural field methods [12; 13] using T60, C50, and EDT metrics . Please refer to our supplementary material for implementation details. Table 3 shows that AV-NeRF outruns both traditional and advanced methods, achieving 21% relative improvement on T60 metric compared with the previous state-of-the-art method INRAS, 5% on C50, and 16% on EDT.

## 6 Discussion

In this work, we propose a first-of-its-kind NeRF system capable of synthesizing real-world audio-visual scenes. Our model can generate audios with rich spatial information at novel camera poses. We demonstrate the effectiveness of our method on real RWAVS and synthetic SoundSpaces datasets.

**Limitation.** Firstly, we currently focus on static scenes with a single fixed sound source in our study. However, it is worth exploring the challenge of learning implicit neural representations for audio-visual scenes with multiple dynamic sound sources. Second, while our model successfully generates audio-visual scenes, it does not account for reverberation effects present in the real world. Reverberation is essential for humans to perceive scene size and structure. Third, similar to the original NeRF, AV-NeRF must represent and render each scene separately. Developing a neural field

   Methods & T60 (\%) \(\) & C50 (dB) \(\) & EDT (sec) \(\) \\  Opus-nearest & 10.10 & 3.58 & 0.115 \\ Opus-linear & 8.64 & 3.13 & 0.097 \\ AAC-nearest & 9.35 & 1.67 & 0.059 \\ AAC-linear & 7.88 & 1.68 & 0.057 \\  NAF  & 3.18 & 1.06 & 0.031 \\ INRAS  & 3.14 & 0.60 & 0.019 \\ Ours & **2.47** & **0.57** & **0.016** \\   

Table 3: Comparison with state-of-the-art. We report the performance on the SonudSpaces dataset using T60, C50, and EDT metrics. The lower score indicates a higher RIR generation quality. Opus is an open audio codec , and AAC is a multi-channel audio coding standard .

Figure 6: Visualization of synthesized audio-visual scene. We present the rendered image, synthesized binaural audio, and ground-truth audio.

that can either learn to represent all scenes or transfer knowledge to new environments is a crucial problem, particularly for industrial applications.

**Broader Impact.** Although AV-NeRF is designed to synthesize audio-visual scenes, it is important to acknowledge its potential application in generating scenes involving artificial humans. The misuse of AV-NeRF could lead to the production of deceptive and misleading media.