# The Impact of Inference Acceleration Strategies on Bias of LLMs

Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar\({}^{1,2}\)

\({}^{1}\)Research Center Trustworthy Data Science and Security, University Alliance Ruhr,

\({}^{2}\)Ruhr University Bochum,

\({}^{3}\)Aleph Alpha

[elisabeth.kirsten,ivan.habernal,bilal.zafar]@rub.de,

vedant.nanda@aleph-alpha.com

###### Abstract

Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to deeply benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, _e.g._, quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to inference acceleration optimizations. Using a wide range of metrics, we probe bias in model outputs from a number of angles. Analysis of outputs before and after inference acceleration shows significant change in bias. Worryingly, these bias effects are complex and unpredictable. A combination of an acceleration strategy and bias type may show little bias change in one model but may lead to a large effect in another. Our results highlight a need for in-depth and case-by-case evaluation of model bias after it has been modified to accelerate inference.

**This paper contains prompts and outputs which may be deemed offensive.**

## 1 Introduction

Modern-day LLMs like LLaMA and GPT-4 show remarkable language generation capabilities, leading to a surge in their popularity and adoption (Bubeck et al., 2023; Wei et al., 2022; Ziems et al., 2024). However, owing to their immense size, deploying these models can be challenging, or even infeasible for consumer-grade devices. A flurry of research has proposed acceleration strategies such as quantization and pruning to enable efficient inference (Park et al., 2024; Zhu et al., 2023). The objective of these strategies is typically to reduce the model size while maintaining predictive performance. Over time, these strategies have become increasingly prevalent with integration into popular libraries like HuggingFace (Hug, 2024) and libraries such as vLLM (Kwon et al., 2023).

While these inference acceleration strategies aim to preserve predictive performance, they may inadvertently lead to some side-effects (Goncalves and Strubell, 2023; Jaiswal et al., 2024). For example, compression techniques might significantly reduce model trustworthiness (Hong et al., 2024). On the other hand, smaller models have been found to mitigate privacy risks and reduce egocentric tendencies in the generated text (Hong et al., 2024; Perez et al., 2022; Sun et al., 2024).

This paper explores how the demographic bias in the model output changes after the implementation of inference acceleration strategies. Specifically, we aim to answer the following research questions:

**RQ1**: Are certain bias types more prone to manifesting because of inference acceleration?
**RQ2**: Are certain inference acceleration strategies more prone to bias?
**RQ3**: Does the bias impact of a strategy remain consistent across various models?

Our assessment includes _five_ commonly used inference acceleration techniques and _three_ widely used LLMs. Noting the multidimensional nature of bias (Mehrabi et al., 2022), we test the models with _six_ different bias metrics. Our results show that **inference acceleration strategies can have significant impacts on bias**. Table 1 provides an example of the effects of acceleration using 4-bit AWQ Quantization (Lin et al., 2024) on a model's decision. **Some acceleration strategies are more prone to bias than others**. For instance, whereas AWQ Quantization leads to a significant change in bias for some model/bias metric combinations, KV-cache quantization mostly remains robust. The **effect of inference acceleration on bias can be unpredictable** with the change in magnitude and direction of bias often varying across models. For example, AWQ quantization did not negatively impact LLaMA-2 or LLaMA-3.1 models' agreeability with stereotypes, but significantly increased stereotyping behavior for Mistral-0.3.

Overall, our results show a need for careful evaluations when applying inference acceleration, as the downstream impact on bias can be unpredictable and significant in magnitude.

## 2 Related Work

Most evaluations of inference acceleration strategies focus on application-agnostic metrics like perplexity or predictive performance-driven tasks like MMLU (Dettmers et al., 2022; Hooper et al., 2024; Lin et al., 2024; Sun et al., 2024). However, recent work has shown that model compression can result in degradation of model performance in areas beyond predictive performance (Goncalves and Strubell, 2023; Jaiswal et al., 2024).

**The effect of model size on trust criteria.** Recent work has started exploring the impact of model size on trust related criteria. For example, Perez et al. (2022) find that larger models tend to overly agree with user views. Sun et al. (2024) show that smaller models can reduce privacy risks. Huang et al. (2024) find that smaller models are more vulnerable to backdoor attacks. Mo et al. (2024) find that larger models are more susceptible to manipulation through malicious demonstrations. Jaiswal et al. (2024) offer a fine-grained benchmark for evaluating the performance of compressed LLMs on more intricate, knowledge-intensive tasks such as reasoning, summarization, and in-context retrieval. By measuring perplexity, they show that pruned models suffer from performance degradation, whereas quantized models tend to perform better. Xu and Hu (2022) find that knowledge

|p{341.4pt}}  
**PROMPT** & You will be shown a list of individuals, a decision-making scenario, and a question about which individual is the most suitable candidate for this scenario. Your task is to identify the individual. The individuals are: a man, a woman, a non-binary person. The scenario is: The backstage pass application is a student interested in attending local music festivals. They have been volunteering at the festivalâ€™s information both for a few years and would like to get a backstage pass [...]. The question is: Which individual should the festival organizers grant the backstage pass? \\ 
**Base** & The answer is: The individual who is known for being friendly, hard-working and responsible, regardless of their gender. \\
**Quantized** & The answer is: The non-binary person. This individual has a positive track record of being friendly, hard-working and responsible. [...] \\   

Table 1: [Mistral-7B model on DiscrimEvalGen data] Decisions by the base model and its 4-bit weight quantized version. The choice and reasoning changes from the base to the quantized version.

distillation causes a monotonic reduction in toxicity in GPT-2, though it shows only small improvements in reducing bias on counterfactual embedding-based datasets. These analyses differ from the current paper in one of the following two ways: (i) they are limited to less recent, pre-trained models, which may not adequately represent the complexities of modern LLMs with significantly more parameters; (ii) they target trustworthiness desiderata beyond bias, _e.g._, backdoor attacks.

**Effect of inference acceleration strategies on trustworthiness.**Goncalves and Strubell (2023) measure the impact of quantization and knowledge distillation on LLMs, and show that longer pre-training and larger models correlate with higher demographic bias, while quantization appears to have a regularizing effect. The bias metrics they consider focus on embeddings or token output probabilities, while we consider a larger range of metrics that focus on properties of generated texts. Hong et al. (2024), in a follow-up to Wang et al. (2024), provide a broader assessment of trustworthiness under compression strategies like quantization and pruning, including adversarial settings. However, their study relies on a single metric to evaluate stereotype bias, which may not capture the broader complexity of bias. We, on the other hand, aim to provide a comprehensive evaluation of bias across multiple dimensions to better understand the impact of inference acceleration strategies. Finally, while these previous benchmarks show largely uniform and predictable effects of inference acceleration on bias, by leveraging a richer set of metrics, our analysis shows a much more nuanced picture and a need for case-by-case evaluation.

## 3 Measuring Bias in LLM Outputs

ML bias can stem from different causes (Suresh and Guttag, 2021), can manifest in various manners (Blodgett et al., 2020; Mehrabi et al., 2022), and can cause different types of harms (Gallegos et al., 2024). While a detailed examination can be found in Gallegos et al. (2024), bias in LLMs is often categorized into the following meta-groups:

1. **Embedding-based metrics** use representations of words or phrases from different demographic groups, _e.g._, WEAT (Caliskan et al., 2017) and SEAT (May et al., 2019).
2. **Probability-based metrics** compare the probabilities assigned by the model to different demographic groups, _e.g._, CrowSPairs (Nangia et al., 2020).
3. **Generated text-based metrics** analyze model generations and compute differences across demographics, _e.g._, by evaluating model responses to standardized questionnaires (Durmus et al., 2024), or using classifiers to analyze the characteristics of generations such as toxicity (Dhamala et al., 2021; Hartvigsen et al., 2022; Smith et al., 2022).

We leave out embedding-based metrics from our analysis since (i) the more typical use-case of modern, instruction-tuned, LLMs like LLaMA and GPT-4 is prompt-tuning or fine-tuning rather than adapting the models using embeddings and (ii) embedding bias is not guaranteed to lead to bias in the text generations. We initially considered classification-based bias metrics (_e.g._, Dhamala et al.), which assess differences in measures like toxicity and sentiment on common datasets like Wikipedia. Preliminary analysis showed very little overall toxicity in model outputs, most likely due to heavy alignment on these datasets. For this reason, we did not further consider these metrics.

With these considerations in mind, the final set of metrics we consider is as follows. We add further information, _e.g._, the number of inputs and license types, in Appendix A.

**CrowSPairs**(Nangia et al., 2020) is a dataset of crowd-sourced sentence pairs designed to evaluate stereotypes related to race, gender, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status. Each pair consists of one sentence that demonstrates a stereotype and the other that demonstrates the opposite of the stereotype. Given a pair \((s_{},s_{})\) where \(s_{}\) is presumed to be more stereotypical, the metric measures \([p(s_{})>p(s_{})]\) and averages this quantity over all pairs, with \(\) as the indicator function. The score ranges from \(\).

**GlobalOpinionQA**(Durmus et al., 2024) uses multiple-choice questions to assess the opinions stated by a model relative to aggregated population opinions from different countries. The goal is to identify biases the model may have in representing diverse viewpoints. We follow the measurement procedure of Durmus et al. with one exception: we use the Wasserstein Distance as our similarity metric (leveraging the implementation provided by the Python scipy library (Virtanen et al., 2020)). Durmus et al. use 1-Jensen-Shannon Distance as a similarity metric, which can become highly skewed when the distributions have very little or no overlap. In contrast, Wasserstein Distance is more sensitive to the geometry of the probability distributions Arjovsky et al. (2017). The bias value is then the Gini coefficient of the Wasserstein Distance for each country. The metric lies in range \(\). The dataset does not provide responses from all countries to all questions, making it difficult to analyze overall value tendencies consistently. To address this, we exclude any countries that do not have responses to at least 50 questions from our analysis.

**WorldBench**(Moayeri et al., 2024) evaluates performance disparities in an LLM's ability to recall facts (_e.g._, population, GDP, and capital) about different countries. Moayeri et al. (2024) structure the questions to elicit a single numerical answer. The dataset encompasses \(11\) statistics for about \(200\) countries. To compare numerical answers, we calculate the absolute relative error between the model's answer and the ground truth, and average the error over all questions to obtain a single score per model. This score lies in the range \(\).

**DT-Stereotyping.** DecodingTrust (Wang et al., 2024) is a framework for evaluating the trustworthiness of LLMs across eight dimensions, one of which is stereotype bias. The dataset consists of custom-made statements (from now on referred to as DT-Stereotyping) designed to provoke unsafe (_e.g._, biased, toxic) responses. Following Wang et al. (2024), we ask the model to either agree or disagree with these statements. To measure stereotyping behavior, we metric computes the average likelihood of the model agreeing with the stereotype statements \(}}{n}\). The original dataset is evaluated in three evaluation scenarios that instruct the model to behave as a helpful assistant (_benign setting_), in disregard of its content policy (_untargeted_), and with targeted bias towards the target group (_targeted_). We run experiments in the _untargeted_ setting to evaluate the resilience of model alignment, without explicitly targeting any group adversarially. Just like DiscrimEvalGen, we consider two versions: DT-Stereotyping (greedy) with \(T=0\) and DT-Stereotyping (sampling) with \(T=1\) and top-\(=1\).

**DiscrimEval**(Tamkin et al., 2023) consists of 70 hypothetical decision making scenarios, _e.g._, approaching a loan. For each scenario, the model is prompted to make a binary yes/no decision about a person described in terms of age, gender and race (9, 3 and 5 choices, respectively). A yes decision is always advantageous. Following Tamkin et al., we append "My answer would be" to the prompt to steer the generations towards producing binary decisions and record the model's softmax probability of "yes" or "no" being generated as the first token. For a scenario \(q_{i}\) and a set of demographic groups \(G\) (\(9 3 5=135\) intersectional groups in this case), we repeatedly reformulate \(q_{i}\), substituting the demographic information for all groups \(g G\) one by one, and measure the difference between the highest and lowest probability of "yes" for all groups \(g G\). Specifically, the bias score is:

\[_{q_{i} Q}_{g G}P(|q_{i},g)-_{g  G}P(|q_{i},g),\]

where \(Q\) is the set of all questions and \(n=|Q|\). We use the dataset's "explicit" version in our evaluation, directly including demographic attributes in the prompt rather than implying it via names.

**DiscrimEvalGen.** The original design of DiscrimEval evaluates bias by analyzing the probability of the first token being "yes" or "no", reducing the model's output to a simplified binary decision. However, this approach (i) considers only a single token for bias measurement ignoring the subsequent tokens and (ii) overlooks the model's broader preferences among demographic groups. With the aim of overcoming these issues, we propose a new dataset DiscrimEvalGen. Whereas DiscrimEval asks the same question separately for each demographic group \(g\), DiscrimEvalGen forces the model to make a single choice. Specifically, we (i) present the question to the model and describe that the candidates are persons from different groups, _e.g._, a man, a woman, a non-binary person; (ii) describe that the benefit (_e.g._, a work contract) can be granted to _only a single person_; and (iii) ask the model to make its choice. Let \(q Q\) be the questions, \(g G\) be the groups, and \(n_{g}\) be the number of times a group is selected by the model with \(_{g G}n_{g}=|Q|\), then the bias metric is:

\[_{g G}n_{g}-_{g G}n_{g}.\]

Figure A.1 in Appendix A shows a concrete example. To avoid having a very long list of choices (\(135\) intersectional groups in the original dataset), we limit the groups to those based on gender, that is, \(G=\{,\}\). We encountered several cases where the model refuses to

select a single person, or selects several persons. We ignore such cases from the bias computation. If for a particular model/acceleration strategy combination, we have more than \(80\%\) such cases, we drop this combination from our results.

Since model outputs can be generated with different temperatures, we use two variants of this evaluation. We refer to these as DiscrimEvalGen (greedy) with \(T=0\) and DiscrimEvalGen (sampling) with \(T=1\) and top-p \(=1\).

## 4 Experimental Setup

**Models and Infrastructure.** We analyze three different models: LLaMA-2 (Touvron et al., 2023), LLaMA-3.1 (Dubey et al., 2024), and Mistral-0.3 (Jiang et al., 2023). We consider the smallest size variant of each model: LLaMA-2-7B, LLaMA-3.1-8B, and Mistral-7B-v0.3 (license information in Section A). These models were selected due to their recency, widespread use, and compatibility with our resource constraints, which included a single node equipped with four NVIDIA A100 GPUs that was shared among several research teams. Our evaluation focuses on the chat versions of these models, which are specifically designed to align with human values and preferences. We used the GitHub Copilot IDE plugin to assist with coding.

**Inference acceleration strategies.** We consider inference time acceleration techniques that do not require re-training. This choice allows us to evaluate models in a real-world scenario where users download pre-trained models and apply them to their tasks without further data- or compute-intensive modifications. We focus on strategies that aim to speed up inference by approximating the outputs of the base model, and where the _approximations_ results in measurable changes in the model output. This criterion excludes strategies like speculative decoding (Leviathan et al., 2023) where the output of the base and inference accelerated models are often the _same_. Specifically, we consider the following strategies:

**Quantization.** We consider the following variants:

1. **INT4** or **INT8** quantization using Bitsandbytes library (Bit, 2024) which first normalizes the model weights to store common values efficiently. Then, it quantizes the weights to 4 or 8 bits for storage. Depending on the implementation, the weights are either dequantized to fp16 during inference or custom kernels perform low-bit matrix multiplications while still efficiently utilizing tensor cores for matrix multiplications.
2. Activation-aware Weight Quantization (**AWQ**) (Lin et al., 2024) quantizes the parameters by taking into account the data distribution in the activations produced by the model during inference. We use the 4-bit version and the authors do not provide a 8-bit implementation.
3. Key-Value Cache Quantization (**KV4** or **KV8**) dynamically compresses the KV cache during inference. KV cache is a key component of fast LLM inference and can take significant space on the GPU. Thus, quantizing the cache can allow using larger KV caches for even faster inference. We use both 4 and 8-bit quantization (Liu et al., 2023). We use the native HuggingFace implementation. This implementation does not support Mistral models.

**Pruning** removes a subset of model weights to reduce the high computational cost of LLMs while aiming to preserve performance. Traditional pruning methods require retraining (Cheng et al., 2024). More recent approaches prune weights post-training in iterative weight-update processes, _e.g._, SparseGPT (Frantar and Alistarh, 2023). We use the Wanda method by Sun et al. (2024) which uses a pruning metric based on both weight magnitudes and input activation norms. The sparse model obtained after pruning is directly usable without further fine-tuning. We consider two variants: (i) Unstructured Pruning (**WU**) with a \(50\%\) sparsity ratio, removing half of the weights connected to each output; and (ii) Structured Pruning (**WS**), enforcing structured N:M sparsity where at most N out of every M contiguous weights are allowed to be non-zero, allowing the computation to leverage matrix-based GPU optimizations. We use a \(2:4\) compression rate. Prior work has shown that pruned models can maintain comparable performance even at high compression rates (Frantar and Alistarh, 2023; Jaiswal et al., 2024; Sun et al., 2024), including the \(2:4\) rate used here.

**Parameters.** As described in Section 3, most bias metrics are designed such that they only support greedy decoding, resulting in deterministic outputs. Only DT-Stereotyping and DiscrimEvalGen support both stochastic decoding and greedy decoding. With stochastic decoding, we sample the output \(5\) times and report the average bias. The models can be used with and without the developer-prescribed instruction templates (using special tokens for instruction boundaries). While instruction formats can have an unpredictable impact on the model performance (Fourier et al., 2023), instruction templates' impacts on model bias are less well understood. We thus study both configurations, with and without the instruction template. The main paper includes the results without the template, while results with instructions templates are shown in Appendix C.

## 5 Results

Table 2 shows the bias of base models w.r.t. each metric, and the change in bias as a result of inference acceleration. We show examples of generations and further output characteristics in the Appendix. The table shows that inference acceleration strategies can have significant, albeit nuanced, impacts on bias in LLMs. While some strategies consistently reduce certain biases, others

Table 2: Effect of inference acceleration strategies on different models. Each sub-table shows a different bias metric from Section 3. The first column shows the bias of base model without any acceleration. Each cell displays the absolute bias value along with the percentage change relative to the bias of the base model. A value of \(\) or \(\) represents a \(X\%\) increase or \(Y\%\) decrease in bias w.r.t. the base model. A value of \(\) means the acceleration strategy is not implemented for that model. A value of \(\) means there was not enough data for this combination (see Section 3). Acceleration strategies can have significant, though sometimes subtle, impacts on bias in LLMs. The effect on bias varies depending on the dataset, model, and scenario used.

[MISSING_PAGE_FAIL:7]

found that this strategy can cause the model to fail to perform the task, follow instructions, or produce nonsensical, repetitive outputs. _Overall, our results suggest quantizing weights can have more drastic, unpredictable impacts on bias as compared to KV cache quantization._

#### RQ3: Does the bias impact of a strategy remain consistent across models?

The effects of inference acceleration strategies on stereotype agreeability vary markedly across models. A detailed breakdown of agreement, disagreement, and no-response rates for nucleus sampling in Table B.2 illustrates how the models' baselines already differ. LLaMA models most frequently provide no response, while Mistral shows a higher rate of both agreement and disagreement. The impact of inference acceleration strategies is notably more pronounced for Mistral, with agreements increasing by over \(75\%\) relative to the base model for both AWQ and unstructured pruning.

Additionally, different models display varying abilities to follow instructions and perform tasks. For example, in the DiscrimEvalGen dataset (Table B.4), LLaMA-2 mostly provides no response. Mistral tends to give answers more frequently in its base form but shows a reduced tendency to respond under quantization and even more so under pruning strategies.

Our findings demonstrate that the _impact of a single acceleration strategy does not remain consistent across different models_. The baseline performance of each model often shows divergent trends, and these disparities are further amplified by inference acceleration strategies. This highlights the need for a model-by-model evaluation when assessing a strategy's impact on bias.

**Comparing 4-bit and 8-bit compression.** While lower-bit compression can enhance efficiency, it often risks degrading model performance (Hong et al., 2024). Hong et al. (2024) explored compression down to 3-bit quantized models, highlighting 4-bit as a setting that balances efficiency and fairness. In our experiments, we evaluate both 4-bit and 8-bit quantization for weights and KV-cache. For 8-bit weight quantization, bias scores generally remain close to those of the base models, with small improvements observed in some cases, except for a slight increase in bias on the DiscrimEval dataset. Similarly, 4-bit weight quantization yields comparable results, though it leads to noticeable increases in bias scores for DT-Stereotyping and DiscrimEvalGen, particularly for the Mistral model. KV-cache quantization consistently shows minimal impact on bias across datasets, with 8-bit compression having little to no noticeable effect on bias, while 4-bit demonstrates small improvements in some model-dataset combinations.

**Using the instruction template.** We show the results with the developer-prescribed instruction templates in Appendix C. The results show largely similar trends as in Table 2. However, in some cases (_e.g.,_ DT-Stereotyping), the model has a very high refusal rate leading to a significant change in bias. The results do not include the CrowSPairs data since the addition of instruction tokens means that we can no longer measure the exact log-likelihood of the input sentences.

**Effect of inference acceleration on text characteristics beyond bias.** Although structured pruning led to improved bias scores in the DT-Stereotyping task, it often diminished the coherence and fluency of the generated text. Examples of this behavior are shown in Table B.3. A detailed analysis of text characteristics in Appendix B shows that deployment strategies can significantly affect aspects of text generation beyond bias, highlighting the need to evaluate these strategies holistically.

## 6 Conclusion & Future Work

In this study, we investigated the impact of inference acceleration strategies on bias in Large Language Models (LLMs). While these strategies are primarily designed to improve computational efficiency without compromising performance, our findings reveal that they can have unintended and complex consequences on model bias.

KV cache quantization proved stable with minimal impact on bias scores across datasets, whereas AWQ quantization negatively affected bias. Other strategies had less consistent effects, with some reducing bias in one model while simultaneously leading to undesirable effects in another model. This variability highlights that the effects of inference acceleration strategies are not universally predictable, reinforcing the need for case-by-case assessments to understand how model-specific architectures interact with these optimizations.

The impact of these strategies extends beyond bias-structured Wanda pruning, for instance, appeared effective in reducing bias but led to concerns about nonsensical and incoherent texts. Our results highlight the importance of using diverse benchmarks and multiple metrics across a variety of tasks to fully capture the trade-offs of these strategies, particularly as the nature of the task itself (e.g., generation vs probability-based) can surface different kinds of biases.

Looking ahead, it is important to consider already during model training that users may later apply inference acceleration strategies. These strategies could be accounted for when aligning the model to reduce biases. Additionally, exploring the combined effects of multiple strategies, such as hybrid approaches that mix pruning with quantization, could provide valuable insights into how to better balance efficiency, performance, and bias. Further research is needed to continue exploring the complex dynamics of bias in LLMs to ensure ethical deployment practices that strike the right balance between efficiency and performance while minimizing unintended side effects.

## 7 Limitations

Our study has several limitations that should be taken into account when interpreting the results. First, the set of benchmarks used in our evaluation and their coverage of different domains and demographic groups is not exhaustive. Since our metrics do not cover all manifestations of bias, there is a risk that some inference acceleration strategies may appear less prone to bias based on these metrics, while in reality, they may exhibit nuanced, domain-specific biases not measured here.

Additionally, we focused only on training-free acceleration strategies. While these strategies are practical and widely used, this excludes other methods, such as fine-tuning or retraining, which may have different effects on bias. Since fine-tuning and retraining are often highly domain-specific, the bias metrics used to assess the impact of these strategies would also need to be tailored to the specific domain. Furthermore, using fixed hyperparameters (e.g., greedy search, sampling five generations) may not capture the full range of model behaviors under different deployment conditions.

There are also potential risks associated with our findings. One risk is that users may interpret our results as suggesting that some deployment strategies are inherently free of bias, which is not the case. Given our study's limitations, our results should be taken as indicative rather than definitive since bias in modern, instruction-tuned LLMs remains an under-explored area Gallegos et al. (2024).

Finally, the broader ethical implications of deploying LLMs with minimal bias remain a critical area of concern. While our study provides insights into how deployment strategies affect bias, the societal impacts of these models extend beyond technical performance. Future research should continue to investigate how these models can be deployed in ways that balance performance and fairness while minimizing unintended side effects that could perpetuate harm in real-world applications.