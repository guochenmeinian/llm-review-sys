# Kernel PCA for Out-of-Distribution Detection

Kun Fang\({}^{1}\) Qinghua Tao\({}^{2}\) Kexin Lv\({}^{3}\) Mingzhen He\({}^{1}\) Xiaolin Huang\({}^{1}\) Jie Yang\({}^{1}\)

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)ESAT-STADIUS, KU Leuven

\({}^{3}\)China Mobile Shanghai ICT Co., Ltd

{fanghenshao,mingzhen_he,xiaolinhuang,jieyang}@sjtu.edu.cn

qinghua.tao@esat.kuleuven.be lvkexin@cmsr.chinamobile.com

###### Abstract

Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper non-linear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, and seek suitable non-linear kernels that advocate the separability between InD and OoD data in the subspace spanned by the principal components. Besides, explicit feature mappings induced from the devoted task-specific kernels are adopted so that the KPCA reconstruction error for new test samples can be efficiently obtained with large-scale data. Extensive theoretical and empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA detector in efficiency and efficacy with state-of-the-art detection performance.

## 1 Introduction

With the rapid advancement of the powerful learning abilities of Deep Neural Networks (DNNs) [1; 2], the trustworthiness of DNNs in security-sensitive scenarios has attracted considerable attention in recent years . Generally, samples from the training and test sets of DNNs are viewed as data from some In Distribution (InD) \(_{}\), while samples from other data sets are regarded as coming from a different distribution \(_{}\), _i.e._, out-of-distribution (OoD) data. In the practical deployment, DNNs trained on InD data would inevitably encounter OoD data and thus yield unreliable results with potential risks. Therefore, detecting whether a new sample is from \(_{}\) or \(_{}\) has been a valuable research topic of trustworthy deep learning, namely OoD detection .

Existing OoD detection methods exploit different outputs from DNNs to unveil the hidden disparities between InD and OoD data, _e.g._, logits , gradients  and features [7; 8]. In this work, we address OoD detection from a perspective of utilizing the feature spaces learned by the backbone of DNNs. To be specific, given a DNN \(f:^{d}^{c}\), \(f\) takes \(^{d}\) as inputs and learns penultimate layer features \(^{m}\) before the last linear layer. Principal Component Analysis (PCA) is investigated in  to calculate the reconstruction error in the \(\)-space as the OoD detection score. That is, PCA is executed on the penultimate features of InD training samples and learns a linear subspace spanned by the principal components in the \(\)-space. Then, given features \(}\) of an unknown sample \(}\), one can obtain the reconstructed counterpart of \(}\) by projecting \(}\) to the linear subspace and re-projecting it back. The reconstruction error is computed as the Euclidean distance between \(}\) and the reconstructed counterpart. In , similar ideas are adopted with energy-based models using anauto-encoder structure, where the neural networks are trained from scratch and the reconstruction is conducted in the decoder. For good OoD detection performance, it is expected that InD features are compactly allocated along the linear principal components with high variances for capturing most of the informative patterns of InD data, leading to small reconstruction errors, while OoD features are not supposed to be well matched with the learned subspace, causing large reconstruction errors.

However, it has been empirically observed in  that such PCA reconstruction errors alone cannot distinctively differentiate OoD data from InD data, leading to poor detection performance of PCA in the \(\)-space. Nevertheless,  did not take further explorations on the reasons behind, and instead proposed a practical fusion trick to boost PCA by multiplying with other existing powerful detection scores. Therefore, in this work, more in-depth analyses are undertaken to improve the limitations of PCA for OoD detection with insightful understandings on the distribution of InD and OoD features. It is widely acknowledged that PCA falls inferior in dealing with those linearly-inseparable data, which inspires us to explore the non-linearity existing in the \(\)-space of InD and OoD features under the help of the celebrated Kernel PCA (KPCA).

KPCA has long been a powerful technique in learning the non-linear patterns of data [11; 12]. By deploying KPCA, a non-linear feature mapping \(\) is imposed on the \(\)-space in this setup, so that the linear inseparability can be alleviated in the mapped \(()\)-space. KPCA is generally conducted through a kernel function \(k\) induced by the feature mapping, _i.e._, \(k(_{1},_{2})=(_{1}),(_{2})\), in order to avoid explicit calculations in the mapped \(()\)-space. In most cases, researchers have no prior on the unknown non-linear data distribution, _e.g._, the non-linearity in the \(\)-space of InD and OoD features. Therefore, finding an appropriate \(k\) or \(\) that well adapts the data always remains a non-trivial issue for KPCA. For example, the mostly common Gaussian kernel is shown to be unable to separate the InD and OoD features, and leads to terrible OoD detection performance, see details in Appendix C. In addition, KPCA also faces the challenge of calculating and storing the \(N_{} N_{}\) kernel matrix on millions of training samples with a very large size \(N_{}\), which significantly hinders its application in practical tasks with a huge amount of data.

The proposed KPCA detection method well addresses the aforementioned issues of KPCA for OoD detection. On the one hand, to better understand the non-linear patterns in InD and OoD features, we take a kernel perspective on an existing OoD detector , which searches \(k\)-th Nearest Neighbors (KNN) on the \(_{2}\) normalized features \(\). By decoupling and analyzing key components of the KNN method, we acquire two effective kernels, a cosine kernel and a cosine-Gaussian kernel, for our KPCA detector to promote the linear separability between InD and OoD features in the subspace of the principal components, leading to substantially improved distinguishable KPCA reconstruction errors, as shown in Figure 1. On the other hand, for a computationally-friendly implementation, two explicit feature mappings \(\) induced from the cosine and cosine-Gaussian kernels are executed on the original features \(\), followed by PCA in the _mapped_\(()\)-space to obtain the reconstruction errors without calculations on the kernel matrix. Specifically, the celebrated Random Fourier Features (RFFs)  are introduced to approximate the Gaussian kernel, allowing an \((M)\) computation

Figure 1: The t-SNE  visualization on the original features \(\) (left) and the mapped features \(()\) (right). Our KPCA detector alleviates the linearly inseparability between InD and OoD features in the original \(\)-space via an explicit feature mapping \(\), and thus substantially improves the OoD detection performance, illustrated by the much more distinguishable reconstruction errors.

complexity in inference, which significantly outperforms the \((N_{ tr})\) computation complexity of the KNN  and the kernel-matrix-based KPCA (\(M\) is the number of RFFs and \(M N_{ tr}\)).

Extensive experiments verify the effectiveness of the devoted two kernels, which we hope could bring inspirations for the research community in exploring the non-linearity in InD and OoD data from a kernel perspective. For example, the two kernels can even serve as a beneficial prior on advocating learning more and stronger kernels for OoD detection. In addition, we supplement our method with its implementation via the kernel matrix, and illustrate the advantageous effectiveness and efficiency of explicit feature mappings in Section 6. The contributions of this work are summarized below:

* To the best of our knowledge, this is the first work that explores suitable kernels to seize the non-linearity in InD and OoD features in the post-hoc stage on well-trained DNNs.
* Two _task-specific_ kernels are carefully devised for OoD detection. Particularly, two explicit feature mappings induced from the kernels are adopted for the KPCA detector, and lead to separable KPCA reconstruction errors with significantly-reduced complexity in inference.
* Theoretical and experimental comparisons indicate the effects of kernels in our KPCA detector with SOTA detection performance and remarkably reduced time complexity.

In the remainder of this work, related works and research backgrounds are outlined in Section 2 and Section 3, respectively. Section 4 delves into details of the proposed KPCA detector. Comparison experiments with prevailing OoD detection methods and KPCA via kernel functions are presented in Section 5 and Section 6, respectively. Conclusions and limitations are drawn in Section 7.

## 2 Related work

Generally, out-of-distribution detection has been formulated as a binary classification problem including a decision function \(D()\) and a scoring function \(S()\):

\[D()=\{,&S()>s,\\ { OoD},&S()<s..\] (1)

The scoring function \(S()\) assigns a score \(S(})\) for a new sample \(}\). If \(S(})\) is greater than a threshold \(s\), the decision function \(D()\) would view \(}\) as an InD sample, and vice versa. The key to effectually detecting OoD samples is a well-designed scoring function. Existing OoD detectors adopt different outputs from DNNs and design justified scores to measure the disparity between InD and OoD data.

**Logits-based** detectors exploit the abnormal responses reflected in the predictive logits or probabilities from DNNs to detect OoD data. Typical methods adopt either the maximum logits  or probability [15; 16] or the energy function on logits  as the detection score.

**Gradients-based** methods investigate differences on gradients _w.r.t_ InD and OoD data for OoD detection. For example, gradient norms  or low-dimensional representations  are studied to devise the detection score.

**Features-based** detectors try to capture the feature information causing over-confidence of OoD predictions in different ways. Feature clipping [18; 19; 20; 21; 22], feature distances [23; 7; 24], feature norms , rank-1 features , feature subspace , etc., have been explored with excellent performance.

Aside from methods above, other existing OoD detectors cover the training regularization [27; 28], the ensemble technique  and theoretical understandings [30; 31]. Refer to Appendix A for more detailed descriptions on the compared methods in experiments [32; 33; 34; 35].

## 3 Background

### PCA for OoD detection

The PCA detector with the reconstruction error as the detection score is summarized firstly. Given the penultimate features \(_{i}^{m}\) learned by a well-trained DNN \(f:^{d}^{c}\) of the InD training data \(_{i}^{d},i=1,,N_{}\), the covariance matrix \(\) is calculated as:

\[=_{i=1}^{N_{}}(_{i}-)(_{i}-)^{},=}}_{i=1}^{N_{ }}_{i}.\] (2)

Through the eigendecomposition \(=^{}\), the dimensionality reduction matrix \(_{q}^{m q}\) is obtained by taking the first \(q\) columns of the eigenvector matrix \(\)_w.r.t_ the top-\(q\) largest eigenvalues.

In inference, given a new sample \(}^{d}\) and its feature \(}^{m}\) from the DNN \(f\), the reconstruction error is computed as:

\[e(})=\|_{q}_{q}^{}(}-)-( }-)\|_{2}.\] (3)

By projecting centralized \((}-)\) to the \(_{q}\)-subspace and re-projecting back, we can obtain the reconstructed features \(_{q}_{q}^{}(}-)\) and the reconstruction error \(e(})\), which then can be set as the OoD detection score: \(S(})=-e(})\). An ideal case is that \(_{q}\) contains informative principal components of InD data and causes projections of OoD data far away from that of InD data, leading to separable reconstruction errors between OoD and InD data.

### Random Fourier features

A concise description is firstly given on the Random Fourier Features (RFFs) , which will be adopted in our method later. RFFs are proposed to approximate the kernel function so as to alleviate the heavy computation cost in large-scale kernel machines. In kernel methods, an \(N N\) kernel matrix _w.r.t_\(N\) samples requires \((N^{2})\) kernel manipulations, \((N^{2})\) space complexity and \((N^{3})\) time complexity to calculate the inverse of the kernel matrix, which leads to overwhelmed computation costs for a large data size \(N\). Therefore, RFFs are introduced by building an explicit feature mapping to directly approximate the kernel function for efficient kernel machines on large-scale data.

RFFs are built on the Bochner's theorem : A continuous and shift-invariant kernel \(k(_{1},_{2})=k(_{1}-_{2})\) on \(^{m}\) is positive definite if and only if \(k()\) is the Fourier transform of a non-negative measure. An explicit feature mapping \(_{}\) induced from the kernel \(k\) is derived in :

\[_{}()}[_{1}(), ,_{M}()],_{i}()=(^{} _{i}+u_{i}),i=1,,M,\] (4)

where \(_{1},,_{M}^{m}\) are _i.i.d._ sampled from the Fourier transform of \(k()\), and \(u_{1},,u_{M}\) are _i.i.d._ sampled from a uniform distribution \((0,2)\). For example, the sampling distribution for \(_{i}\) of a Gaussian kernel function \(k_{}=e^{-\|_{1}-_{2}\|_{2}^{2}}\) is \((0,_{m})\). Such a feature mapping \(_{}\) satisfies \(k_{}(_{1},_{2})_{}(_{1})^{ }_{}(_{2})\) and is known as the random Fourier features (RFFs) mapping. Refer to  for a detailed convergence analysis. RFFs have been widely utilized in kernel learning , optimization , etc.

## 4 Methodology

As empirically observed in , the aforementioned PCA reconstruction error in the \(\)-space is not an effective score in detecting OoD data from InD data. We propose that the reason behind is possibly due to the linearly-inseparable features of InD and OoD data, as shown in Figure 0(a). To address this issue, we propose to explore the non-linearity in \(\)-space via kernel PCA. Then, through a kernel perspective on an existing KNN detector , we put forward two efficacious kernels that well characterize the non-linear patterns in \(\)-space of InD and OoD data: a cosine kernel (Section 4.1) and a cosine-Gaussian kernel (Section 4.2). Particularly, we adopt two explicit feature mappings \(\) induced from the two kernels, and execute PCA in the _mapped_\(()\)-space, which leads to an informative principal subspace and distinct reconstruction errors for efficacious OoD detection.

### Cosine kernel

In the KNN detector , the nearest neighbor searching is executed on the \(_{2}\)-normalized penultimate features, _i.e._, \(}{\|\|_{2}}\). In inference, given a new sample \(}\), its feature \(}\) is firstly normalized as \(}}{\|}\|_{2}}\)then the negative of its (\(k\)-th) shortest \(_{2}\) distance to the \(_{2}\)-normalized features \(}{\|_{i}\|_{2}}\) of training data is set as the detection score:

\[S_{}(})=-_{i:1,,N_{}}\|}}{\|\|_{2}}-_{i}}{\|_{i}\|_{2}}\|_{2}.\] (5)

The ablations in KNN demonstrate the indispensable significance of the \(_{2}\)-normalization: the nearest neighbor searching directly on \(\) shows a notably drop in detection performance. The critical role of the \(_{2}\)-normalization in KNN attracts our attention in the sense of kernel. From a kernel perspective, the \(_{2}\)-normalization is exactly the non-linear feature mapping \(_{}\) inducing the cosine kernel \(k_{}\):

\[k_{}(_{1},_{2})=_{1}^{}_{2}}{\| _{1}\|_{2}\|_{2}\|_{2}}=_{}(_{1})^{ }_{}(_{2}),_{}()=}{\|\|_{2}}.\] (6)

It indicates that a justified \(_{}()\)-space with such non-linear mapping, instead of the original \(\)-space, contributes to the success of nearest neighbor searching in detecting OoD.

Notice that the key of KPCA for OoD detection lies in an appropriate non-linear feature space that captures the non-linearity in InD and OoD features, either through the kernel \(k\) or the associated explicit feature mapping \(\). Motivated by the KNN detector, we apply \(()_{}()\) as the feature mapping in KPCA to introduce non-linearity. Then, PCA is executed on mapped features \(_{}()\), following the procedures described in Section 3.1. All the features \(\) are now mapped to \(()\) to formulate the covariance matrix \(^{}\), for computing non-linear principal components with matrix \(_{q}^{}\) and the corresponding reconstruction error \(e^{}\). This detection scheme is dubbed as **CoP** (**C**osine mapping followed by **PCA**), as shown in Algorithm 1. An in-depth analysis on the effect of the normalization of the cosine kernel is left in Appendix C.1.

### Cosine-Gaussian kernel

The success of KNN (Equation (5)) suggests that the \(_{2}\) distance on \(}{\|\|_{2}}\) is effective in distinguishing OoD data from InD data. In other words, the \(_{2}\) distance relation between samples in the \(_{}\)-space preserves useful information that benefits the separation of OoD data from InD data. This motivates us to seek non-linear feature spaces that can retain the \(_{2}\) distance relation. Hence, we propose to introduce KPCA with non-linearity built upon \(_{}()\), through which the useful \(_{2}\) distance in \(_{}\)-space can be preserved to further separate InD and OoD data.

In this regard, we deploy the shift-invariant Gaussian kernel to keep the \(_{2}\) distance information:

\[k_{}(_{1},_{2})=e^{-\|_{1}-_{2}\|_{ 2}^{2}}.\] (7)

The feature mapping associated with \(k_{}\) is infinite-dimensional, but it can be efficiently approximated through random Fourier features (RFFs, ), _i.e._, \(_{}\) defined in Equation (4). In this way, the inner product of two mapped samples \(_{}(_{1})^{}_{}(_{2})\) provides the approximate Gaussian kernel, so that we can leverage the RFFs mapping \(_{}\) to preserve the \(_{2}\) distance information through the Gaussian kernel.

Hence, a cosine-Gaussian kernel is adopted for OoD detection, as the Gaussian kernel \(k_{}\) (or \(_{}\)) is imposed on top of the cosine kernel \(k_{}\) (or \(_{}\)), further exploiting the \(_{2}\) distance relationships beyond the \(_{}\)-space for OoD detection. As we work with the explicit feature mapping, the non-linearity to \(\) is achieved by \(()_{}(_{}())\). With mappings \(_{}(_{}())\), PCA is then executed to compute the reconstruction errors for OoD detection. This detection scheme is dubbed as **CoRP** (**Cosine** and **RFF**s mappings followed by **PCA**). Algorithm 1 illustrates the complete procedure of the proposed **CoP** and **CoRP** for OoD detection. Alternative choices for more kernels are exploited in Appendix C.2.

To warp up, we devise two effective feature mappings induced from a cosine kernel and a cosine-Gaussian kernel to promote the separability of InD data and OoD data in non-linear feature spaces, inspired by effectiveness of the \(_{2}\) normalization and the \(_{2}\) distance from a kernel perspective on the KNN detector . Our proposed two feature mappings well characterize the non-linearity in penultimate features \(\) of DNNs between InD and OoD data, enabling PCA to extract an informative subspace _w.r.t_ the mapped features through principal components and the reconstruction errors.

**Input:**

```
1:if CoP then
2:\(()_{}(),_{}()=}{\|\|_{2}}\).
3:elseif CoRP then
4: Sampling \(_{i}(0,_{m}),i=1,,M\).
5: Sampling \(u_{i}(0,2),i=1,,M\).
6:\(()_{}(_{}())\).
7:endif
8: Calculating the covariance matrix in the mapped \(()\)-space: \(^{}=_{i=1}^{N_{}}((_{i})-^{ })((_{i})-^{})^{}\), \(^{}=}}_{i=1}^{N_{}}(_{i})\).
9: Applying eigendecomposition: \(^{}=^{}^{}^{}\).
10: Taking the first \(q\) columns of \(^{}\)_w.r.t_ the top-\(q\) largest eigenvalues in \(^{}\): \(^{}_{q}=^{}[:,:q]\).
11: Dimensionality-reduction matrix \(^{}_{q}\).
12: Given a new sample \(}\) and its features \(}\).
13: Calculating the reconstruction error: \(e^{}(})=\|^{}_{q}^{}_{q}(( {})-^{})-((})-^{})\|_{2}\).
14:Reconstruction error \(e^{}\). ```

**Algorithm 1** Kernel PCA for Out-of-Distribution Detection

Computation complexityIn our method, given any new sample \(}\) with the penultimate features \(}\) in inference, to compute the reconstruction error \(e^{}(})\), we only need the feature mapping \(\), the projection matrix \(^{}_{q}\) and the mean mapped training feature vector \(^{}\). Both \(^{}_{q}\) and \(^{}\) can be pre-calculated and stored in preparation for inference. Therefore, the entire computation cost of CoP and CoRP comes from the construction of the explicit feature mapping \(\) on new features \(}\).

* For CoP, its feature mapping \(_{}\) is an in-place operation and does not require additional computations. Therefore, the time and memory complexity of CoP is \((1)\).
* For CoRP, the feature mapping \(_{}\) of the Gaussian kernel requires \(2M\) samplings for \(_{i}\) and \(u_{i}\), respectively, and \(M\) dot-products and \(M\) additions. Accordingly, the time and memory complexity of CoRP is \((M)\).

In contrast, regarding the Equation (5) of the KNN detector, all the training features have to be stored at hand and iterated in inference, which implies a heavy \((N_{})\) time and memory complexity. The \((1)\)/\((M)\) of CoP/CoRP significantly outperforms the \((N_{})\) of KNN (\(M N_{}\)). Detailed empirical comparisons are provided in Section 5.1.

In the following, Section 5 exhibits the SOTA performance of our KPCA detector over multiple prevailing detection methods. Section 6 gives an analytical discussion between our covariance-based KPCA and classic KPCA via kernel functions for OoD detection. Due to space limitation, more in-depth investigations on the kernel properties are left in Appendix C, covering ablation studies (Appendix C.1), alternative kernels (Appendix C.2) and sensitivity analysis (Appendix C.3).

## 5 Experiments on OoD detection

In experiments, our KPCA-based detectors, CoP and CoRP, are firstly compared with KNN  in Section 5.1, and show stronger detection performance and cheaper computation costs. In Section 5.2, CoP and CoRP are further compared with the regularized PCA reconstruction error , and achieve SOTA OoD detection performance over various prevailing methods. The source code of this work has been publicly released1. All the experiments are executed on 1 NVIDIA GeForce RTX 3090 GPU.

DatasetsExperiments are executed on the commonly-used small-scale CIFAR10  and large-scale ImageNet-1K benchmarks , following the settings in [7; 8]. For CIFAR10 as InD, OoD data sets include SVHN , LSUN , iSUN , Textures  and Places365 . For ImageNet-1K as InD, OoD data sets contain iNaturalist , SUN , Places  and Textures .

MetricsFor the evaluation metrics, we employ the commonly-used (i) False Positive Rate of OoD samples with 95% true positive rate of InD samples (FPR), and (ii) Area Under the Receiver Operating Characteristic curve (AUROC). The _average_ FPR95 and AUROC values over the selected multiple OoD data sets are viewed as the final comparison metrics.

### Comparisons with nearest neighbor searching

The comparisons with KNN  cover both the benchmarks. Following the setups in KNN, for fair comparisons, we evaluate models trained via the standard cross-entropy loss and models trained via the supervised contrastive learning , and adopt the same checkpoints released by KNN: ResNet18  on CIFAR10 and ResNet50 on ImageNet-1K. Here, the scoring function of CoP and CoRP is \(S(})=-e^{}(})\) in Algorithm 1.

Table 1 presents empirical results of ResNet50 on the ImageNet-1K benchmark. In the standard training, our CoRP shows superior detection performance over KNN with lower FPR and higher AUROC values averaged over multiple OoD data sets. In supervised contrastive learning, both CoP and CoRP outperform other baseline results on each OoD data set. These results show that the proposed KPCA exploring non-linear patterns is more advantageous than the nearest neighbor searching and all compared methods. Besides, the further improvements of CoRP over CoP also verify the effectiveness of the distance-preserving property of the Gaussian kernel \(k_{}\) on top of the cosine kernel \(k_{}\) for OoD detection.

On the other hand, regarding the computational complexity in inference, Table 2 empirically shows the superior \((1)/(M)\) time and memory complexity of CoP/CoRP over the \((N_{})\) of KNN, including: (i) the inference time of the nearest neighbor search in KNN and the reconstruction error calculation in CoP/CoRP; (ii) the storage of the InD training features in KNN and the \(_{g}^{}\) and \(^{}\) in CoP/CoRP. To be specific, for KNN, storing and iterating all the \(N_{}=1,281,167\) features of ImageNet-1K training set requires nearly 20 GiB and 16 ms, respectively, while our CoP and CoRP directly compute the reconstruction error for each new sample with the pre-calculated projection

    &  &  \\  &  &  &  &  &  \\  & FPR\(\) & AUROC\(\) & FPR\(\) & AUROC\(\) & FPR\(\) & AUROC\(\) & FPR\(\) & AUROC\(\) & FPR\(\) & AUROC\(\) \\   \\ MSP  & 54.99 & 87.74 & 70.83 & 80.86 & 73.99 & 79.76 & 68.00 & 79.61 & 66.95 & 81.99 \\ ODIN  & 47.66 & 89.66 & 60.15 & 84.59 & 67.89 & 81.78 & 50.23 & 85.62 & 56.48 & 85.41 \\ Energy  & 55.72 & 89.95 & 59.26 & 85.89 & 64.92 & 82.86 & 53.72 & 85.99 & 58.41 & 86.17 \\ GODIN  & 61.91 & 85.40 & 60.83 & 85.60 & 63.70 & 83.81 & 77.85 & 73.27 & 66.07 & 82.02 \\ Mahala  & 97.00 & 52.65 & 98.50 & 42.41 & 98.40 & 41.79 & 55.80 & 85.01 & 87.43 & 55.47 \\ KNN  & 59.00 & 86.47 & 68.82 & 80.72 & 76.28 & 75.76 & 11.77 & 97.07 & 53.97 & 85.01 \\ CoP (ours) & 67.25 & 83.41 & 75.53 & 79.93 & 82.48 & 73.83 & 8.33 & 98.29 & 58.40 & 83.86 \\ CoRP (ours) & 50.07 & 89.32 & 62.56 & 83.74 & 72.76 & 78.91 & 9.02 & 98.14 & **48.60** & **87.53** \\   \\ MSP  & 32.18 & 93.30 & 60.36 & 84.21 & 61.68 & 83.94 & 50.62 & 84.68 & 51.21 & 86.53 \\ ODIN  & 23.48 & 95.80 & 50.73 & 88.43 & 53.99 & 87.30 & 41.88 & 88.60 & 42.52 & 90.03 \\ Energy  & 23.00 & 95.94 & 47.56 & 88.86 & 51.59 & 87.58 & 39.15 & 89.05 & 40.33 & 90.36 \\ SSD  & 57.16 & 87.77 & 78.23 & 73.10 & 81.19 & 70.97 & 36.37 & 88.52 & 63.24 & 80.09 \\ KNN  & 30.18 & 94.89 & 48.99 & 88.63 & 59.15 & 84.71 & 15.55 & 95.40 & 38.47 & 90.91 \\ CoP (ours) & 29.85 & 94.79 & 44.99 & 90.62 & 56.77 & 86.19 & 10.28 & 97.35 & **35.47** & **92.24** \\ CoRP (ours) & 23.61 & 95.86 & 41.07 & 91.25 & 53.52 & 87.27 & 10.23 & 97.04 & **32.11** & **92.86** \\   

Table 1: The detection performance of different methods (**ResNet50** trained on **ImageNet-1K**).

   method & time and memory complexity & time consuming (ms, per sample) & storage \\  KNN & \((N_{})\) & \(\) 15.59 & \(\) 20 GiB \\ CoP & \((1)\) & \(\) 0.035 & \(\) 22 MiB \\ CoRP & \((M)\) & \(\) 0.086 & \(\) 29 MiB \\   

Table 2: Comparisons on the computation complexity between KNN  and our CoRP (**ResNet50** on **ImageNet-1K**). Experiments are executed on the same machine for a fair comparison. The nearest neighbor searching of KNN is implemented via Faiss .

matrix and the mean vector from the training data, resulting in a much higher processing speed and far less storage. The number of RFFs \(M\) for CoRP in this experiment is \(M=4,096\) (\(M N_{}\)).

In addition, KPCA also outperforms KNN on the CIFAR10 benchmark with improved OoD detection performances, which we leave to Appendix B for more details.

### Comparisons with regularized reconstruction errors

In , to alleviate the weak detection performance of PCA reconstruction error \(e(})\) of Equation (3), the authors proposed to _regularize_\(e(})\) by the feature norm \(\|}\|_{2}\) and a fusion strategy to boost its detection performance by introducing existing OoD scores. Firstly, the regularized reconstruction error \(e_{}(})\) is calculated in the original \(\)-space as \(:e_{}(})=}U_{}^{}(}-)-(}-)\|_{2}}{\|}\|_{2}}\). Then, the authors claimed that such a regularized version \(e_{}(})\) is still insufficient for OoD detection, and designed a fusion strategy to combine \(e_{}\) with other existing OoD scores. For example, to fuse \(e_{}\) with the Energy  score, the final scoring function is \(S(})=(1-e_{}(})) S_{}( })\).

In this section, we show that our KPCA reconstruction error \(e^{}\) outperforms the regularized PCA reconstruction error \(e_{}\) under the same fusion framework. Following the settings in , for a fair comparison, the fused OoD scores include MSP , Energy , ReAct  and BATS . The detection experiments are executed on the ImageNet-1K benchmark with pre-trained ResNet50 and MobileNet  checkpoints from PyTorch .

Table 3 presents the comparisons between  and ours on the ImageNet-1K benchmark of ResNet50. When fused with MSP, Energy and BATS, both the KPCA-based CoP and CoRP outperform the regularized reconstruction error  on almost all the OoD data sets with substantially improved FPR and AUROC values. Specifically, when fused with the ReAct method , the CoRP achieves new SOTA OoD detection performance among various prevailing detectors. Experiments on MobileNet also show superior performance of CoP and CoRP, see details in Appendix B.

    &  &  \\  &  &  &  &  &  \\  & FPR\(\) & AUROC\(\) & FPR\(\) & AUROC\(\) & FPR\(\) & AUROC\(\) & FPR\(\) & AUROC\(\) & FPR\(\) & AUROC\(\) \\  MSP  & 54.99 & 87.74 & 70.83 & 80.86 & 73.99 & 79.76 & 68.00 & 79.61 & 66.95 & 81.99 \\ + PCA  & 51.47 & 88.95 & 67.64 & 82.71 & 71.20 & 80.87 & 60.53 & 85.86 & 62.71 & 84.60 \\ + **CoP** & **50.84** & **89.21** & **67.35** & **82.81** & **70.96** & **81.08** & **59.96** & **86.21** & **62.28** & **84.83** \\ + **CoRP** & **43.70** & **91.70** & **61.79** & **85.43** & **66.67** & **83.07** & **45.67** & **91.86** & **54.46** & **88.02** \\  Energy  & 55.72 & 89.95 & 59.26 & 85.89 & 64.92 & 82.86 & 53.72 & 85.99 & 58.41 & 86.17 \\ + PCA  & 50.36 & 91.09 & 54.19 & 87.55 & 64.13 & 84.00 & 29.33 & 92.59 & 49.50 & 88.81 \\ + **CoP** & **45.13** & **92.15** & **52.33** & **88.01** & **61.49** & **84.96** & **29.13** & **92.57** & **47.02** & **89.42** \\ + **CoRP** & **26.85** & **95.15** & **40.38** & **90.76** & **51.26** & **87.35** & **12.11** & **97.17** & **32.65** & **92.61** \\  ReAct  & 20.38 & 96.22 & 24.20 & 94.20 & 33.85 & 91.58 & 47.30 & 89.80 & 31.43 & 92.95 \\ + PCA  & 10.17 & 97.97 & 18.50 & 95.80 & 27.31 & 93.39 & 18.67 & 95.95 & 18.66 & 95.76 \\ + **CoP** & 13.30 & 97.44 & 19.80 & 95.37 & 29.92 & 62.64 & **15.90** & **96.51** & 19.73 & 95.49 \\ + **CoRP** & **10.77** & 97.85 & 18.70 & 95.75 & 28.69 & 93.13 & **12.57** & **97.21** & **17.68** & **95.98** \\  BATS  & 42.26 & 92.75 & 44.70 & 90.22 & 55.85 & 86.48 & 33.24 & 93.33 & 44.01 & 90.69 \\ + PCA  & 29.66 & 94.49 & 38.11 & 90.03 & 51.70 & 87.25 & 13.46 & 97.09 & 33.23 & 92.56 \\ + **CoP** & **27.14** & **94.87** & **34.36** & **91.96** & **47.68** & **87.87** & **11.97** & **97.33** & **30.29** & **93.01** \\ + **CoRP** & **18.74** & **96.31** & **28.02** & **93.49** & **41.41** & **89.78** & **94.56** & **97.79** & **24.41** & **94.34** \\  ODN  & 47.66 & 89.66 & 60.15 & 84.59 & 67.89 & 81.78 & 50.23 & 85.62 & 56.48 & 85.41 \\ Mahala  & 97.00 & 52.65 & 98.50 & 42.41 & 98.40 & 41.79 & 55.80 & 85.01 & 87.43 & 55.47 \\ ViM  & 68.86 & 87.13 & 79.62 & 81.67 & 83.81 & 77.80 & 14.95 & 96.74 & 61.81 & 85.83 \\ DICE  & 26.66 & 94.49 & 36.08 & 90.98 & 47.63 & 87.73 & 32.46 & 90.46 & 35.71 & 90.92 \\ DICE+ReAct & 20.08 & 96.11 & 26.50 & 93.83 & 83.34 & 90.61 & 29.36 & 92.65 & 28.57 & 93.30 \\ NNG Guide  & 25.73 & 95.12 & 37.18 & 91.21 & 46.97 & 88.67 & 27.70 & 92.30 & 34.39 & 91.82 \\ DML+  & 13.57 & 97.50 & 30.21 & 94.01 & 39.06 & 91.42 & 36.31 & 89.70 & 29.79 & 93.16 \\ ASH-B  & 14.21 & 97.32 & 22.08 & 95.10 & 33.45 & 92.31 & 21.17 & 95.50 & 22.73 & 95.06 \\ ASH-S  & 11.49 & 97.87 & 27.98 & 94.02 & 39.78 & 90.98 & 11.93 & 97.60 & 22.80 & 95.12 \\ SCALE  & 9.50 & 98.17 & 23.27 & 95.02 & 34.51 & 92.26 & 12.93 & 97.37 & 20.

All these experiment results indicate that an appropriately mapped \(()\)-space benefits the OoD detection, as the non-linearity in \(\)-space gets alleviated by the feature mapping \(\). Our work provides 2 viable selections for \(\) with empirical validations, which we hope could attract attentions towards the non-linearity in InD and OoD features for the research community from a kernel perspective.

## 6 Analytical discussions with KPCA via kernel functions

In CoP and CoRP, KPCA is executed with the _covariance matrix_ of _mapped_ features \(()\). In contrast, in the classic KPCA [11; 12], such feature mappings \(\) are not explicitly given, and it rather works with a _kernel function_ applied to _original_ features \(\). In this section, we supplement our covariance-based KPCA with its kernel function implementation, including theoretical discussions and empirical comparisons on OoD detection. Our CoP and CoRP are shown to be more effective and efficient than their counterparts that employ kernel functions.

In the classic KPCA, the kernel trick enables projections to the principal subspace via kernel functions without calculating \(\). However, how to map the projections in the principal subspace back to the original \(\)-space remains a non-trivial issue, known as the pre-image problem , which makes it problematic to calculate reconstructed features via kernel functions. To address this issue, the following Proposition 1 shows a flexible way to directly calculate reconstruction errors without building reconstructed features, so as to apply the kernel trick, shown in the subsequent Proposition 2.

**Proposition 1**.: _The KPCA reconstruction error \(e^{}(})\) can be represented as the norm of features projected in the residual subspace, i.e., the \(_{p}^{}\)-subspace with \(^{}=[_{q}^{},_{p}^{}]\):_

\[e^{}(})=\|_{p}^{}((})-^{}) \|_{2}.\] (8)

Proposition 1 implies that the reconstruction error equals to the norm of projections in the residual \(_{p}^{}\)-subspace, _i.e._, the subspace consisting of those principal components that are not kept, see proofs in Appendix D. Accordingly, as typically done in the classic KPCA, we can introduce a kernel function to perform dimension reduction, but to the residual subspace, and then calculate the norms of the reduced features as the reconstruction error, illustrated by Proposition 2.

Given a kernel function \(k(,):^{m}^{m}\), we have a kernel matrix \(^{N_{} N_{}}\) on training data with \(_{i,j}=k(_{i},_{j})\), and a vector \(_{}}^{N_{}}\) with the \(i\)-th element \(k(_{i},})\) for a new sample \(}\). Proposition 2 shows how to calculate the KPCA reconstruction error via the kernel function \(k\).

**Proposition 2**.: _The KPCA reconstruction error \(e^{k}(})\) w.r.t a kernel function \(k\) can be calculated as:_

\[e^{k}(})=\|^{}_{}}\|_{2},\] (9)

_where \(^{N_{} l}\) includes \(l\) eigenvectors of the kernel matrix \(\) w.r.t the top-\(l\) smallest eigenvalues._

According to Proposition 2, now CoP and CoRP can be implemented via kernel functions. For CoP, we just directly apply the cosine kernel function \(k_{}\) on features \(\) to compute the kernel matrix \(\) and the projection matrix \(\), so as to obtain \(e^{k}(})\) following Equation (9). For CoRP, we should adopt the Gaussian kernel function \(k_{}\) on the \(_{2}\)-normalized inputs \(}{\|\|_{2}}\) to calculate \(\), \(\) and \(e^{k}(})\). Figure 2 shows comparisons on the detection performance between CoP/CoRP and their kernel function implementations.

In Figure 2, the detection performance of KPCA with kernel functions is evaluated by varying the explained variance ratio of the kernel matrix \(\). The larger the explained variance ratio, the smaller the dimension \(l\) of \(\). The best detection results achieved by CoP/CoRP are illustrated as the dashed lines. Clearly, regarding the OoD detection performance, reconstruction errors \(e^{k}\) calculated by kernel functions are less effective than those calculated explicitly in the mapped \(()\)-space.

Aside from the detection performance, KPCA with kernel functions is far less computationally efficient than CoP/CoRP in two aspects. On the one hand, the time expense of eigendecomposition on the \(N_{} N_{}\) kernel matrix \(\) by the former is more expensive than that on the \(m m\) or \(M M\) covariance matrix \(^{}\) by the latter, since \(N_{} M\) and \(N_{} m\). For example, on the ImageNet-1K benchmark with MobileNet, these settings are \(N_{}=1,281,167\), \(M=2560\) and \(m=1280\), on which KPCA with kernel functions is actually nearly prohibitive. On the other hand, in the inference stage, KPCA via kernel functions yet requires an \((N_{})\) time and memory complexity in calculating \(_{}}\), as all the training data has to be stored and iterated, which is much higher than the \((1)\)/\((M)\) complexity of our CoP/CoRP.

## 7 Conclusion

As PCA reconstruction errors fail to distinguish OoD data from InD data on the penultimate features \(\) of DNNs, kernel PCA is introduced for its non-linearity in the manner of employing explicit feature mappings. To find appropriate kernels that can characterize the non-linear patterns in InD and OoD features, we take a kernel perspective to decouple and analyze key components of an existing KNN detector , and thus propose a cosine kernel and a cosine-Gaussian kernel for KPCA. Specifically, two explicit feature mappings \(()\) induced from the two kernels are leveraged on original features \(\). For the cosine kernel, its explicit feature mapping can be directly obtained. For the Gaussian kernel, we adopt the celebrated random Fourier features to approximate the Gaussian kernel. The mapped \(()\)-space enables PCA to extract principal components that well separate InD and OoD data, leading to distinguishable reconstruction errors. Extensive empirical results have verified the improved effectiveness and efficiency of the proposed KPCA with new SOTA OoD detection performance. Besides, more in-depth analyzes are drawn on the individual effects of the cosine kernel and the Gaussian kernel, and the involved multiple hyper-parameters. In addition, theoretical discussions and associated experiments are provided to bridge the relationships between our covariance-based KPCA and its kernel function implementation so as to further illustrate the advantages of our method.

One limitation of the KPCA detector is that the two specific kernels are still manually selected with carefully-tuned parameters. It remains a valuable topic in the OoD detection task whether the parameters of kernels could be learned from data according to some optimization objective. For example, deep kernel learning  could be considered as an alternative choice so as to pursue stronger kernels that can better characterize InD and OoD with enhanced detection performance by an additional learning step on the features. We hope that the proposed two effective kernels verified empirically in our work could benefit the research community as a solid example for future studies.

## Societal impacts

The societal impacts of this work are mainly positive, as it aims at detecting OoD samples in the inference or deployment stage of DNNs, which benefits researches in trustworthy deep learning. Through our work, we hope that new inspirations on the non-linearity in data could be drawn from a kernel perspective so as to highlight the safety issue in real-world machine learning applications.

Figure 2: Comparisons on the average detection FPR values between CoP/CoRP and their kernel function implementations in the CIFAR10 benchmark. In experiments, 5,000 images of the CIFAR10 training set and 1,000 images of the CIFAR10 test set and OoD data sets are randomly selected.