# Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation

Wei Dong\({}^{1}\), Yuan Sun\({}^{1}\), Yiting Yang\({}^{1}\), Xing Zhang\({}^{1}\), Zhijun Lin\({}^{2}\), Qingsen Yan\({}^{2}\),

Haokui Zhang\({}^{2}\), Peng Wang\({}^{3}\), Yang Yang\({}^{3}\), Hengtao Shen\({}^{3}\)\({}^{4}\)

\({}^{1}\)College of Information and Control Engineering, Xi'an University of Architecture and Technology

\({}^{2}\)School of Computer Science, Northwestern Polytechnical University

\({}^{3}\)School of Computer Science and Engineering, University of Electronic Science and Technology of China

\({}^{4}\)School of Computer Science and Technology, Tongji University

Corresponding author. Email address: p.wang6@hotmail.com

###### Abstract

A common strategy for Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViTs) involves adapting the model to downstream tasks by learning a low-rank adaptation matrix. This matrix is decomposed into a product of down-projection and up-projection matrices, with the bottleneck dimensionality being crucial for reducing the number of learnable parameters, as exemplified by prevalent methods like LoRA and Adapter. However, these low-rank strategies typically employ a fixed bottleneck dimensionality, which limits their flexibility in handling layer-wise variations. To address this limitation, we propose a novel PEFT approach inspired by Singular Value Decomposition (SVD) for representing the adaptation matrix. SVD decomposes a matrix into the product of a left unitary matrix, a diagonal matrix of scaling values, and a right unitary matrix. We utilize Householder transformations to construct orthogonal matrices that efficiently mimic the unitary matrices, requiring only a vector. The diagonal values are learned in a layer-wise manner, allowing them to flexibly capture the unique properties of each layer. This approach enables the generation of adaptation matrices with varying ranks across different layers, providing greater flexibility in adapting pre-trained models. Experiments on standard downstream vision tasks demonstrate that our method achieves promising fine-tuning performance.

## 1 Introduction

Parameter-Efficient Fine-Tuning (PEFT) for pre-trained Vision Transformers (ViTs) aims to adapt these models to downstream tasks by learning a small set of parameters while keeping most or all of the original model parameters frozen. This approach is expected to reduce the cost of fine-tuning and potentially improve the model's generalization performance, particularly when the downstream task involves limited data.

A common strategy for adapting the parameters is to learn an adaptation matrix that modifies the original matrix through addition or multiplication. To reduce the parameter scale of the adaptation matrix, a low-rank strategy is typically employed. This involves decomposing the adaptation matrix into the product of a down-projection matrix and an up-projection matrix, where the bottleneck dimensionality determines the parameter scale. Many prevailing PEFT solutions  follow this approach. However, these methods usually set the bottleneck dimensionality empirically to balance adaptation performance and parameter size. The fixed dimensionality, however, lacks the flexibility to accommodate variations in layer-wise properties.

In this work, we propose a novel parameter-efficient adaptation method to fine-tune pre-trained ViTs. Our design of the adaptation matrix is inspired by Singular Value Decomposition (SVD), which decomposes a matrix into a product of a left unitary matrix, a diagonal matrix, and a right unitary matrix. In SVD, the unitary matrices consist of orthogonal vectors, and the diagonal matrix of singular values essentially determines the rank of the matrix. Inspired by SVD, we propose to use Householder transformations to replace the left and right unitary matrices. Householder transformations maintain orthogonality properties similar to unitary matrices but can be derived simply by a vector, making them parameter efficient. With left and right Householder matrices, we learn the diagonal matrix adaptively for each layer to accommodate layer-wise properties. This approach, termed the Householder Transformation-based Adaptor (HTA), enables us to derive the adaptation matrix in a parameter-efficient manner while theoretically allowing for varying ranks for the adaptation matrices, thus achieving a better balance between parameter efficiency and adaptation performance.

We conducted experiments on a set of downstream vision classification tasks. The results show that our method can be effectively applied to various ViT versions, achieving promising fine-tuning performance. In summary, the contributions of this work can be summarized as follows:

* We approach PEFT from a novel angle by viewing the adaptation matrix from the perspective of SVD, which inspires us to propose a Householder transformation-based adaptation strategy that is parameter-efficient.
* By learning scaling coefficients to compose Householder transformation matrices together into adaptation matrices, our method can theoretically allow varying adaptation matrix ranks to accommodate layer-wise variations.
* Experiments on two sets of downstream vision classification tasks reveal our method can achieve an appealing balance between adaptation performance and parameter efficiency.

## 2 Related Work

### Pre-training and Transfer Learning

As an advanced learning strategy, extensive research [4; 5; 6; 7] has demonstrated the wide applicability of transfer learning across various fields. Especially in cases where the target task has limited data, high labeling costs, or poor data quality [8; 9; 10], transfer learning significantly enhances model generalization and training efficiency. By pre-training on large-scale datasets and using the obtained parameters as initialization for downstream tasks, transfer learning can effectively transfer and apply the knowledge of pre-trained models. In this process, the performance and convergence speed of downstream tasks are highly correlated with the dataset used for pre-training the model. In the field of computer vision, pre-training on large-scale datasets such as ImageNet  has significantly improved the performance of tasks like image classification [12; 13; 14; 15], object detection [16; 17], and semantic segmentation [18; 19]. Moreover, self-supervised pre-training [20; 21] leverages the advantage of not requiring large amounts of labeled data, expanding the data scale and enhancing feature extraction capabilities, thereby further improving the generalization ability and robustness of pre-trained models. However, due to the substantial computational resources required to fully fine-tune the parameters of pre-trained models in downstream tasks, current research has shifted towards exploring more efficient fine-tuning methods.

### Parameter-Efficient Fine-Tuning (PEFT)

Compared to full fine-tuning, the PEFT methods [22; 23; 24; 25; 2; 26] aim to reduce the high cost of fine-tuning by freezing the majority of parameters in the pre-trained model and introducing a small number of learnable parameters to adapt to specific downstream tasks.

With the development of large pre-trained models, various PEFT approaches have emerged. Adapter  inserts a bottleneck-structured adapter layer into the pre-trained model and refines the model by updating only the parameters within the adapter layer. Bias  focuses on the fine-tuning of models for specific downstream tasks by meticulously calibrating the bias terms. VPT  integrates the concept of prompt learning into visual tasks, thereby facilitating targeted optimization for specific downstream tasks. SSF  efficiently fine-tunes the weights in pre-trained modelsthrough affine transformations composed of scaling and shifting operations. AdaptFormer  explores a parallel adapter solution on ViT for various downstream tasks. FacT  decomposes the weights of ViT into individual three-dimensional tensors and further decomposes the increments into lightweight factors. During fine-tuning phase, these factors are updated and stored, effectively reducing computational overhead.

### LoRA and its variants

As represented by LoRA , the core of this type of PEFT method is the utilization of low-rank matrices to approximate weight adjustments during the fine-tuning phase. By employing reparameterization techniques, these low-rank matrices are combined with the existing parameter matrices, thereby circumventing extra inference costs. AdaLoRA  employs singular value decomposition to decompose weight matrices, pruning insignificant singular values to effectively reduce the number of parameters. ARC  uses symmetric up-down projections to create cross-layer shared bottleneck operations. By learning low-dimensional rescaling coefficients, it effectively recombines layer-adaptive adapters, reducing the costs of fine-tuning. FedPara  reparameterizes model layers with low-rank matrices and uses the Hadamard product. This approach, unconstrained by low-rank limitations, offers greater capacity and reduces learning costs. RLRR  examines mainstream PEFT methods from the perspective of SVD decomposition, exploring the critical balance between preserving generalization in pre-trained models and adapting them to downstream tasks. Our research abandons the traditional fixed-rank approach, opting instead for a more flexible adjustment of parameter matrices using a small number of learnable parameters.

## 3 Methodology

In this section, we commence with an introduction of the notations, symbols, and contextual background related to low-rank adaptations and Householder transformation. Then we present the decomposed structure of low-rank adaptation and discuss its inherent operating mechanism from the perspective of singular value decomposition. Finally, we propose a novel adaptation via Householder transformation. This adaptation primarily aims to construct the Householder unitary matrices via a learnable weight vector, thereby trading-off the fully spanned representation space and the affordable parameter size.

### Preliminary knowledge

Low-rank adaptation.Pre-trained ViT models are typically initialized with weights learned from large-scale image datasets, such as ImageNet. The pre-training process involves optimizing the model on an unsupervised or supervised pretext task. The resulting pre-trained weights encode rich semantic information that can be transferred to a wide range of downstream tasks through fine-tuning. As one of the most representative methods of fine-tuning, PEFT method achieves excellent results on downstream tasks by merely utilizing a small number of additional learnable parameters to fine-tune the ViT. The most prevalent PEFT is the adaptation method, which can be divided into two categories: LoRA-based and adapter-based methods. In general, LoRA-based method is defined as:

\[_{}^{(l-1)}=^{(l-1)}(^{(l)}+_{}^{(l)}_{}^{(l)})+}^{(l) },\] (1)

where \(_{}^{(l-1)}\) is the fine-tuning output, \(^{(l)}\) is any linear weight projection \(\{_{q}^{(l)},_{k}^{(l)},_{v}^{(l)},_{ v}^{(l)},_{}^{(l)},_{C2}^{(l)}\}\) in ViT, \(}^{(l)}\) is the bias weights, \(_{}^{(l)}^{D^{(l)} D^{}}\) and \(_{}^{(l)}^{D^{} D^{(l)}}\) are down- and up-adapting projection matrices across varying layers with the dimensionality \(D^{} D^{(l)}\). The detailed framework of LoRA-based method is shown in Fig. 1 (a). Analogously, adapter-based method is described as:

\[_{}^{(l-1)} =((^{(l-1)})_{ }^{(l)})_{}^{(l)},\] (2) \[_{}^{(l)} =((_{}^{(l-1)}) _{}^{(l)})_{}^{ (l)},\]

with the activation function \(()\), Multi-Head Attention (MHA), and Feed-Forward Network (FFN) modules in ViT. The detail of adapter-based method is shown in Fig. 1 (c). By observing Eq. (1) and (2), both above-mentioned PEFT methods involve a low-rank bottleneck structure, _i.e._, \(^{(l)}_{}=^{(l)}_{}^{(l)}_{}\). Note that we remove the activation in the low-rank bottleneck because the presence or absence of such activation does not affect the low-rank structure of adaptation.

Householder transformation.Householder transformation, or Householder reflection, is a linear transformation that reflects a vector across a hyperplane defined by a Householder vector. It is characterized by a Householder matrix, which is an orthogonal and symmetrical matrix with determinant -1. This transformation, initially proposed by A.S. Householder in 1958 , has significant applications in numerical linear algebra , particularly in QR decomposition , where it is used to transform a matrix into an upper triangular or Hessenberg form . Householder transformation can also be employed to set specific elements of a vector to zero while preserving its norm, making it a valuable tool for matrix orthogonalization and symmetrization. The Householder matrix is defined as following:

\[=-}}^{},\] (3)

with the identity matrix \(\) and the Householder vector \(}\).

### Viewing the adaptation matrix from SVD

Singular Value Decomposition (SVD) offers a profound insight into matrix factorization. It breaks down a matrix into three constituent matrices. Viewing the adaptation matrix through the lens of SVD, we represent it as:

\[^{(l)}_{}=^{(l)}_{} ^{(l)}^{(l)}_{},\] (4)

Figure 1: Underpinned by (a) LoRA  and (c) Adapter , we utilize Householder matrix to construct Householder transformation-based adaptations, involving (b) LoRA-based method with HTA and (d) Adapter-based method with HTA.

where \(^{(l)}_{}^{D^{(l)} D^{(l)}}\) is the left unitary matrix and \(^{(l)}_{}^{D^{(l)} D^{(l)}}\) is the right unitary matrix; and \(^{(l)}^{D^{(l)} D^{(l)}}\) is the diagonal matrix in which the diagonal elements of a diagonal matrix are singular values. Unitary matrices \(^{(l)}_{}\) and \(^{(l)}_{}\) essentially characterize the rotation transformations in a linear space. The left unitary matrix \(^{(l)}_{}\) rotates an arbitrary vector multiplied by the adaptation matrix \(^{(l)}_{}\) into the space it spans. Then, the vector is scaled by the diagonal matrix \(^{(l)}\). Finally, the right unitary matrix \(^{(l)}_{}\) rotates the vector back to the original linear space. Therefore, the SVD decomposition characterizes the transformations of rotation and scaling in the linear space.

When it comes to the fine-tuning strategy, PEFT methods employ ViT as the backbone and essentially fine-tune the learnable parameters of unitary matrices \(^{(l)}_{}^{D^{(l)} D^{}}\) and \(^{(l)}_{}^{D^{} D^{(l)}}\) and the learnable singular values of diagonal matrix \(^{(l)}^{D^{} D^{}}\) to downstream tasks, implicitly performing the rotation and scaling transformations. Note that the number of non-zero singular values in matrix \(^{(l)}\) does not exceed its dimensionality \(D^{}\). However, the fixed bottleneck dimensionality \(D^{}\) empirically set to LoRA- or adapter-based methods is inflexible, thereby without accommodating variations in layer-wise properties. This implies that the linear space spanned by the low-rank adaptation matrix and its corresponding number of non-zero singular values is constrained within a low dimensionality \(D^{}\). Increasing the dimensionality \(D^{}\) could effectively enhance the space capacity of the adaptation matrix, thereby improving the performance potential of the fine-tuned ViT model. However, this also further increases the number of parameters in the PEFT method.

### Householder transformation-based adaptation

To address the aforementioned issue, we introduce Householder transformation into the adaptation matrix learning, and propose the Householder Transformation-based Adaptor (HTA). Following this way, HTA facilitates the derivation of the adaptation matrix in a manner that is parameter-efficient, while theoretically accommodating the flexibility of varying ranks for the adaptation matrices.

In our approach, we respectively employ the Householder matrices \(^{(l)}_{}^{D^{(l)} D^{(l)}}\) and \(^{(l)}_{}^{D^{(l)} D^{(l)}}\) as substitutes for the left and right unitary matrices \(^{(l)}_{}^{D^{(l)} D^{}}\) and \(^{(l)}_{}^{D^{} D^{(l)}}\) within the adaptation matrix \(^{(l)}_{}\) to form the HTA adaptation matrix \(^{(l)}_{}\):

\[^{(l)}_{}&=^{(l)}_{}^{(l)}_{}^{(l)}_{ }\\ &=(-}^{(l)}_{}}^{(l)}_{})^{(l)}_{}(-}^{(l)}_{}}^{(l)}_{ })),\] (5)

with two learnable parameter vectors \(}^{(l)}_{}^{D^{(l)}}\) and \(}^{(l)}_{}^{D^{(l)}}\) and a learnable diagonal parameter vector \(}}^{(l)}^{D^{(l)}}\) in the diagonal matrix \(^{(l)}_{}^{D^{(l)} D^{(l)}}\).

Since the Householder transformation matrix is derived from a single vector, its transformation capacity can be limited and sensitive to the vector learned to derive it. To enhance the robustness of the adaptation matrix, we incorporate an additional low-rank adaptation matrix, resulting in the ultimate HTA. Building on this design, we can derive the LoRA alternative as follows:

\[^{(l-1)}_{}&=^{(l-1)}(^{(l)}+^{(l)}_{}^{(l)}_{ }+^{(l)}_{})+}^{(l)}\\ &=^{(l-1)}(^{(l)}+^{(l)}_{}^{(l)}_{}+(-}^{(l)}_{ }}^{(l)}_{})^{(l)}( -}^{(l)}_{}}^{(l)}_{ }))+}^{(l)},\] (6)

where \(^{(l)}_{}^{D^{(l)} 1}\) and \(^{(l)}_{}^{1 D^{(l)}}\), unless otherwise stated. The HTA structure of the LoRA alternative is shown in Fig. 1 (b).

Analogously, we can derive HTA alternative to the adapter-based method (as shown in Fig. 1 (d)) as follows:

\[_{}^{(l-1)}=&(^{(l-1)})(_{}^{(l)}_{ }^{(l)}+_{}^{(l)})\\ =&(^{(l-1)})(_{ }^{(l)}_{}^{(l)}+\\ &(-}_{}^{( l)}}_{}^{(l)})_{ }^{(l)}(-}_{}^ {(l)}}_{}^{(l)} )),\] (7)

\[_{}^{(l)}=&(_{}^{(l-1)})(_{}^{(l)} _{}^{(l)}+_{}^{(l)})\\ =&(_{}^{(l-1)})( _{}^{(l)}_{}^{(l)}+\\ &(-}_{}^{( l)}}_{}^{(l)})_{ }^{(l)}(-}_{}^ {(l)})).\]

By observing Eq. (6), we can see that LoRA-based method with HTA could be re-parameterized to the form \(_{}=_{}^{(l)}_{ }^{(l)}+_{}^{(l)}\) during the model inference stage. And also, the re-parameterization \(_{}^{}=_{o}^{(l)}_{ }^{(l)}\) of MHA in Eq. (7) is available due to the fact that the weight matrix \(_{o}^{(l)}\) is positioned at the end of MHA. Similarly, the re-parameterization of FFN is \(_{}^{}=_{}^{(l)} _{}^{(l)}\) due to the weight matrix \(_{}^{(l)}\) at the tail of FFN.

## 4 Experiments

In this section, we present the experimental settings, comparison to existing solutions, and ablation studies to unveil the key properties of the proposed method.

### Experimental settings

**Datasets.** We evaluated the effectiveness of our method using two sets of visual adaptation benchmarks: FGVC and VTAB-1k, involving a total of 24 datasets. The FGVC collection consists of five Fine-Grained Visual Classification (FGVC) datasets: CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, and Stanford Cars. These datasets focus on distinguishing between visually similar subcategories within a broader category, making the task more challenging and detailed. The VTAB-1k benchmark comprises 19 diverse visual classification tasks, divided into three categories: Natural, which includes images captured by standard cameras; Specialized, which includes images captured by specialized equipment such as remote sensing and medical imaging devices; and Structured, which includes synthesized images from simulated environments, such as object counting and 3D depth prediction. Each VTAB-1k task includes 1,000 training samples.

Pre-trained backbones.We employ ViT  and Swin Transformer  as backbone architectures to evaluate our proposed approach. To demonstrate the versatility of the proposed HTA model, we utilize two variants of ViT: ViT-Base and ViT-Large. These models are pre-trained on the ImageNet21K dataset . Additionally, to ensure a fair comparison, we follow the settings from previous work  and conduct separate experiments using a ViT backbone enhanced with AugReg .

Baselines and existing PEFT methods.In our comparative analysis, we evaluate the performance of our HTA against two baselines and several state-of-the-art PEFT methods. Unless otherwise specified, our HTA follows the design in Eq. (6), with the dimension of the low-rank adaptation matrix set to 1. The two baselines we consider are: (1) Full Fine-tuning: This baseline involves updating all the parameters of the pre-trained model using the training data of the downstream task. (2) Linear Probing: This baseline focuses on learning a linear classification head on the downstream task while keeping the remaining parameters of the pre-trained model frozen. In addition to the baselines, we compare our method with the following state-of-the-art solutions: Adapter , Bias , LoRA , VPT , AdaptFormer , FacT , ARC  and RLRR . The results are presented in Table 1 and Table 2.

[MISSING_PAGE_FAIL:7]

Experiments on larger-scale ViT backbone.

In addition to using the ViT-B backbone, we also employed the ViT-L backbone, which has a deeper block structure, to validate the scalability and generalizability of our method. The comparison results are shown in Table 3. Our method achieves the best performance among all the compared methods while maintaining a reasonable parameter count. These results demonstrate that our method can effectively adapt models of varying scales and complexities in an efficient manner.

Experiments on hierarchical Vision Transformers.To further validate the effectiveness of our method, we tested it on the Swin Transformer . The Swin Transformer is renowned for its hierarchical design, consisting of multiple stages, each with transformer blocks that maintain consistent feature dimensions unique to that stage. As shown in Table 4, our method notably outperforms existing state-of-the-art methods across various downstream tasks, with an overall improvement of 1.2% over the previous best performance while using only half of the parameters.

### Ablation studies

To gain deeper insights into the proposed method, we conducted comprehensive ablation studies to elucidate its critical features and carry out pertinent analyses.

Using HTA as alternative to low-rank based adaptation matrix.As mentioned earlier, our proposed HTA model offers a more flexible adaptation capacity compared to other low-rank based adaptation matrices. To validate this claim, we replaced the adaptation matrices of LoRA  and Adaptor  with HTA. Initially, following FacT , LoRA  was originally applied to the \(\{_{q},_{v}\}\) projection matrices in the multi-head attention operation of each ViT layer, while Adapter was applied to the feed-forward neural network components layer-wise, as described in Eq. (2). For

  
**MethodsDatasets** & **Natural (7)** & **Specialized (4)** & **Structed (8)** & **Mean Total** & **Params.(M)** \\   Full fine-tuning & 79.1 & 86.2 & 59.7 & 72.4 & 86.80 \\ Linear probing & 73.5 & 80.8 & 33.5 & 58.2 & 0.05 \\  MLP-4  & 70.6 & 80.7 & 31.2 & 57.7 & 4.04 \\ Partial  & 73.1 & 81.7 & 35.0 & 58.9 & 12.65 \\ Bias  & 74.2 & 80.1 & 42.4 & 62.1 & 0.25 \\ VPT-Shallow  & 79.9 & 82.5 & 37.8 & 62.9 & 0.05 \\ VPT-Deep  & 76.8 & 84.5 & 53.4 & 67.7 & 0.22 \\ ARC  & 79.0 & 86.6 & 59.9 & 72.6 & 0.27 \\ RLRR  & 81.3 & **86.7** & 59.0 & 73.0 & 0.41 \\  HTA & **81.8** & **86.7** & **61.3** & **74.2** & 0.23 \\   

Table 4: Performance comparison on VTAB-1k using Swin Transformer pre-trained on ImageNet-21k as the backbone. Detailed results are presented in the Appendix.

  
**MethodsDatasets** & **Natural (7)** & **Specialized (4)** & **Structed (8)** & **Mean Total** & **Params.(M)** \\   Full fine-tuning & 74.7 & 83.8 & 48.1 & 65.4 & 303.40 \\ Linear probing & 70.9 & 69.1 & 25.8 & 51.5 & 0.05 \\  Bias  & 70.5 & 73.8 & 41.2 & 58.9 & 0.32 \\ VPT-Shallow  & 78.7 & 79.9 & 40.6 & 62.9 & 0.15 \\ VPT-Deep  & 82.5 & 83.9 & 54.1 & 70.8 & 0.49 \\ LoRA  & 81.4 & 85.0 & 57.3 & 72.0 & 0.74 \\ SSF  & 81.9 & 85.2 & 59.0 & 73.0 & 0.60 \\ ARC  & 82.3 & 85.6 & 57.3 & 72.5 & 0.18 \\ RLRR  & 83.9 & 86.4 & 61.9 & 75.2 & 0.82 \\  HTA & **84.1** & **86.6** & **62.3** & **75.4** & 0.54 \\   

Table 3: Performance comparison on VTAB-1k using ViT-Large pre-trained on ImageNet-21k as the backbone. Detailed results are presented in the Appendix.

a fair comparison, we also applied HTA separately to \(\{_{q},_{v}\}\) and \(\{_{},_{}\}\). From the results in Table 5, we observe that our method slightly outperforms LoRA but with significantly fewer parameters. Moreover, our method achieves significant improvement over Adapter still using much fewer parameters. These results indicate that our method achieves a better trade-off between adaptation performance and parameter efficiency. To further test the effectiveness of our method when using a similar parameter scale to LoRA, we additionally applied HTA to FFN layers. The results show that under the same parameter size, our method exhibits a noticeable improvement over LoRA.

Ablation study on the bottleneck dimensionality of additive adaptation matrix in HTA.We conducted ablation experiments to verify the effect of incorporating low-rank adaptation matrices in HTA, as well as the impact of its bottleneck dimensionality. The results are presented in Fig. 2. From these results, we observe that without the addition of low-rank adaptation, HTA experiences an obvious performance drop. This is due to the fact that while deriving orthogonal matrices using Householder transformations is parameter-efficient, their inherent dependence on a single chosen vector makes them insufficient as a set of general orthogonal bases for representing arbitrary high-dimensional space. When using a low-rank adaptation matrix with rank 1, HTA shows a significant performance boost. This indicates that even with a simple low-rank adaptation, HTA can achieve a promising trade-off between adaptation performance and parameter efficiency. By incorporating these low-rank matrices, HTA can maintain high performance while being parameter-efficient.

## 5 Limitations

In this work, we use Householder transformations to construct adaptation matrices in a parameter-efficient manner. Although Householder transformation matrices are orthogonal, they cannot serve as general orthogonal bases in high-dimensional spaces due to their inherent dependence on a single vector. This limitation may reduce the adaptation capacity of the adaptation matrix composed of two Householder matrices. We address this issue by incorporating a rank-1 adaptation matrix, which may somewhat detract from the elegance of the proposed method. However, it is worth exploring strategies to eliminate the need for the additive adaptation matrix, thereby further enhancing the elegance and efficiency of the HTA method.

    &  &  &  &  &  \\   LoRA (\(_{q}\), \(_{v}\)) & 79.5 & 84.6 & 59.8 & 72.3 & 0.29 \\ HTA (\(_{q}\), \(_{v}\), \(_{}\), \(_{}\)) & 81.1 & 86.3 & 61.5 & 73.9 & 0.28 \\  Adapter (\(_{}\), \(_{}\)) & 79.0 & 84.1 & 58.5 & 71.4 & 0.16 \\ HTA (\(_{}\), \(_{}\)) & 81.0 & 84.9 & 60.0 & 73.0 & 0.05 \\   

Table 5: Ablation study on using HTA as alternative to the low-rank based adaptation matrices in LoRA and Adapter on VTAB-1k. Following the configurations in FacT , LoRA and Adapter are applied to \(\{_{q},_{v}\}\) and \(\{_{},_{}\}\) projection matrices, separately.

Figure 2: Ablation study on the impact of different bottleneck dimensions of adaptive matrices in HTA. The bar chart represents the Top-1 Test Accuracy, the line graph indicates parameters count.

Conclusions

In this work, we proposed a novel Parameter-Efficient Fine-Tuning (PEFT) solution. Our method addresses the limitation of fixed bottleneck dimensionality in low-rank based adaptation matrices, which can restrict adaptation flexibility. By viewing the adaptation matrix from the perspective of Singular Value Decomposition (SVD), we use Householder transformations to mimic orthogonal bases. These transformations, derived from a single vector, are highly parameter-efficient. We adaptively learn diagonal coefficients to flexibly combine two Householder matrices into an adaptation matrix, accommodating layer-wise variations. Theoretically, our method can generate adaptation matrices with varying ranks while maintaining a reasonable parameter size, offering a potential alternative to low-rank based adaptations. Experiments on two sets of downstream vision classification tasks demonstrate the effectiveness of our approach.