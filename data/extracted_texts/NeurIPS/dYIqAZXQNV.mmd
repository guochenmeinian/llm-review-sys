# Generalizing CNNs to Graphs with Learnable Neighborhood Quantization

Isaac Osafo Nkansah\({}^{1}\) Neil Gallagher\({}^{1}\) Ruchi Sandilya\({}^{1}\) Conor Liston\({}^{1}\)

Logan Grosenick\({}^{1}\)

\({}^{1}\)Department of Psychiatry and BMRI, Weill Cornell Medicine,

Cornell University, New York, NY, USA

To whom correspondence should be addressed: log4002@med.cornell.edu

###### Abstract

Convolutional neural networks (CNNs) have led to a revolution in analyzing array data. However, many important sources of data, such as biological and social networks, are naturally structured as graphs rather than arrays, making the design of graph neural network (GNN) architectures that retain the strengths of CNNs an active and exciting area of research. Here, we introduce Quantized Graph Convolution Networks (QGCNs), the first framework for GNNs that formally and directly extends CNNs to graphs. QGCNs do this by decomposing the convolution operation into non-overlapping sub-kernels, allowing them to fit graph data while reducing to a 2D CNN layer on array data. We generalize this approach to graphs of arbitrary size and dimension by approaching sub-kernel assignment as a learnable multinomial assignment problem. Integrating this approach into a residual network architecture, we demonstrate performance that matches or exceeds other state-of-the-art GNNs on benchmark graph datasets and for predicting properties of nonlinear dynamics on a new finite element graph dataset. In summary, QGCNs are a novel GNN framework that generalizes CNNs and their strengths to graph data, allowing for more accurate and expressive models.

## 1 Introduction

Many important real-world scenarios involve data structured as graphs. For example, neural networks (both biological and artificial) are typically represented as directed graphs where individual neurons propagate information to other neurons along edges. Digital networks (e.g. social networks, the internet) are graphs made up of links between digital objects, and chemical structures can be modeled as graphs made up of bonds between atoms. It can be challenging to accurately model such graph data, creating a barrier to studying these problems. Indeed, this work was motivated by our own experiments in modeling brain networks from neural data, where we have found that existing methods are often not expressive enough to effectively capture the types of phenomena we are interested in.

In recent years, the prevailing approach for learning from graph data has shifted towards methods inspired by convolutional neural networks (CNNs) [20; 21; 8; 17; 25; 4; 5]. The focus on extending the convolutional layer of CNNs to graph data is motivated by the strong and successful inductive bias of CNNs, which use trainable filters that efficiently model local structure in array-structured data (e.g., images). CNNs have been effectively employed in numerous domains, including natural language processing [26; 43] and image recognition . Because graph data often exhibit strong patterns of local correlation like those seen in language and image data, it is reasonable to expect they might similarly benefit from shared local filters.

Early work extending CNNs to graphs focused on spectral methods [6; 3], which can suffer from high runtime and memory complexity . In contrast, spatial methods aim to generalize the convolution operation explicitly from array data to graph data. Recent spatial GNN methods approach this problem by either adapting the convolution operation to graphs [11; 1; 27; 28] or by adapting graphs to fit the CNN convolutional operation . Existing spatial methods, however, do not truly generalize the CNN convolution layer.

Spatial Graph Convolutional Networks (SGCNs) , for example, claim to generalize CNNs to graph data as during inference on array data an SGCN will have spatial filters that resemble those of a CNN. But an SGCN will not reduce to an equivalent CNN when trained on array data, and is thus not a proper generalization. This relates to a central challenge of extending convolutions to graph-structured data: in CNNs, local neighborhoods have fixed sizes and fixed ordering of the nodes within the neighborhood, a convenience that does not hold true for more general graphs. We believe that bridging the gap between CNNs and GCNs and properly generalizing the powerful local inductive bias of CNNs to GNNs will lead to improved learning for many types of graph data.

To this end, we introduce the Quantized Graph Convolution Layer (QGCL), which adds to the spatial graph neural network literature a proper generalization of the CNN convolution layer to graphs. We do this by first "quantizing" the convolution operation for CNNs into an equivalent set of non-overlapping sub-kernels applied to local geometry. Second, we describe a specific set of sub-kernels for graphs that are equivalent to a 2D convolutional kernel based on a _satisficing mapping_, which relies on relative angular displacements of nodes to quantize graph neighborhoods into sub-kernels. To generalize the QGCL to arbitrary graphs, we extend it to be able to learn neighborhood quantizations from data, using a network we term QuantNet. Furthermore, we design a residual network around the QGCL architecture, which we term Quantized Graph Residual Layer (QGRL), to make the layer more robust to model depth effects like vanishing gradients. As we were initially inspired by the regularity of widely-used finite element method (FEM) graphs, we provide a new benchmark data set for FEM (based on Navier-Stokes fluid flow on an adaptive mesh graph) and demonstrate that a QGRL-based architecture (called Quantized Graph Residual Network or QGRN) is highly competitive on such data. Next, we show that QGRNs enable competitive performance across nineteen inductive learning graph datasets. Finally, we demonstrate that incorporating QGRLs leads to superior performance in a supervised autoencoder model applied to a public EEG recordings and emotional states dataset .

In summary, **our main contributions** are:

1. Introducing the Quantized Graph Convolutional Network (QGCN) framework, which generalizes CNNs to graphs.
2. Empirical and formal validation that a QGCN using the _satisficing mapping_ sub-kernels reduces to a 2D CNN on image graphs.
3. An end-to-end learnable quantization network (QuantNet) that extends QGCNs to arbitrary graphs.
4. A residual network inspired architecture, Quantized Graph Residual Networks (QGRNs), that further improves QGCN performance.
5. Benchmarking of QGRNs on a new Navier-Stokes FEM dataset and 19 other public benchmark graph datasets for graph classification and node classification.
6. Showing QGRLs improve joint modeling of emotional states and EEG data in a supervised autoencoder architecture.

## 2 Relevant work

**GCNs** Although inspired by spectral theory, Graph Convolutional Networks (GCNs)  are practically understood as a spatial GNN method as they aggregate node features within local neighborhoods (normalizing by the node degree of the central/target node) and then transform the resulting aggregated features into new features for the central/target nodes. Because all neighboring node features are scaled by fixed weights and then aggregated in the input features space, this method's number of trainable free parameters differs from CNNs (failing to generalize the CNN convolution layer). In contrast, CNNs learn embeddings of each node feature separately and the ability of each node's features to embed in a different point in the output feature space independently makes CNNs more flexible than GCNs.

**SGCNs** Spatial Graph Convolutional Networks (SGCNs) , a recent novel CNN-inspired GCN architecture, improve on GCNs by using graph node positional descriptors to rank nodes within their neighborhoods. They extend GCNs by using MLPs that project the relative spatial/positional descriptors of nodes into output feature space. Though the authors claim that SGCNs are equivalent to CNNs for inference, SGCNs and CNNs exhibit a different inductive bias during training. There is additional difficulty in matching CNN and SGCN model parameters and determining how scaling different parts of SGCN architecture translates into equivalent CNN adaptions. A strength of SGCNs is that they can consume pseudo-positional descriptors, making them more general than GCNs.

**KerGNNs** Kernel GNNs  define kernels as sub-graphs with trainable adjacency matrices and node features. The trainable node features for sub-graphs parallel CNN kernel weights, and the learned adjacency matrices allow for different topologies of sub-graph kernels to be learned. In CNNs, the adjacency matrix of the convolving kernel is fixed, hence kerGNNs generalize the CNN convolution operation well in this sense. However, the size of the direct product graph (which captures the relationship between local sub-graph patches and the convolving sub-graph kernel) grows quadratically with the sizes of the local neighborhood and the convolving sub-graph kernel. For graphs with large local neighborhoods, the computation of the adjacency matrix of the direct product graph per local neighborhood (effectively a cubic runtime complexity across the data) becomes very expensive. Further the authors suggest the use of additional trainable weights for the base random walk kernels, causing further divergence with regular CNN convolution layer.

**LGCLs** Inspired by , the Learnable Graph Convolutional Layer  approach is unlike the aforementioned methods, instead adapting graph data into a form that a regular CNN convolution operator can use. It does this by applying max pooling on the feature vectors of the local graph neighborhoods. This does not generalize the CNN convolution operation (which uses all the features within the local neighborhood and not a sub-sampled set). Further, max pooling sub-sampling constrains the model to ascribe more importance to large features; a constraint absent in CNNs.

**GATs** GATs extend the power of transformers and attention networks to graphs and have been shown to be highly performant on graph data [38; 39; 2; 36]. In current GATs, every node attends to its neighbors either in a static or dynamic fashion. The attention mechanism effectively yields edge-aware feature scalars from source nodes whose messages must be aggregated for the target node. This is akin to CNNs applying different kernel weights to different node features in local image graph neighborhoods.

**DeeperGCNs** Early works  adapted residual connections, inspired by _ResNets_, to deep graph networks to deal with the problem of vanishing gradients and over-smoothing . A more recent innovation in this space, GENConv , builds on these residual connections (first demonstrating how powerful these connections alone are for deep networks) and innovates generalized messaging passing aggregators, learnable message normalization layers etc. to compete with state-of-the-art performance on standard graph dataset benchmarks. Other works such as DropEdge , which propose randomly removing graph edges, and PairNorm  which develops a normalization layer to tackle the problems aforementioned, are also noteworthy.

## 3 Proposed methodology

### Extending CNNs to graphs

Quantized Graph Convolution Networks (QGCNs) are an extension of CNNs to graph data. We begin with a formal description of the convolutional layer, which is the core component of CNNs, in order to motivate the Quantized Graph Convolution Layer (QGCL). For simplicity, we focus on the convolutional layer in two dimensions with a stride size of one operating on \(^{C^{} D^{} F^{}}\), where **G** is structured in a way such that proximity and adjacency in the space composed of the first two dimensions has meaning, but that the ordering of those 2D planes along the third dimension is arbitrary. As an example, 2D image data with multiple color channels exhibits this structure. In this case, a convolutional layer will generate an output feature map \(^{C D F}\):

\[_{c,d,:}=(_{j=0}^{J-1}_{k=0}^{K-1}_{j,k,:,:} _{j+c,k+d,:})+,\] (1)where \(^{J K K F^{}}\) and \(}^{F}\) are the weights and bias terms of the convolutional kernel, respectively. To produce a map that is the same size as the original input, zero-padding can be added along the edges of **G** before applying the convolutional layer to result in \(_{c,d,:}\) being defined for \(c[0,C^{}-1],d[0,D^{}-1]\).

We can refactor the kernel parameters in Eq. 1 to have a single index \(h\) iterating over the 2D space traversed by \(j\) and \(k\) above:

\[_{c,d,:}=_{h=0}^{JK-1}(}_{h,::}_{j=0}^{J -1}_{k=0}^{K-1}1_{(h=jK+k)}_{j+c,k+d,:})+}_{ h,:},}_{h,:}=}}{JK}\  h,\] (2)

where \(}_{JK+k,::}\) corresponds to \(_{j,k,:,:}\) in Eq. 1. The indicator function \(1_{(h=jK+k)}\) has the effect of creating a mask over a single element in the 2D space defined by \(j\) and \(k\), with a one-to-one correspondence between each element and each value of \(h\). Thus the convolutional layer can be decomposed into a set of sub-kernels (i.e., weight matrices \(}_{h,:,:}\) and bias vectors \(}_{h,:}\)) along with a corresponding set of masks on the input space. This formulation of the convolutional layer allows for interesting possibilities by designing a different set of mask functions; for example, one could produce a well-defined output **O** with the same dimensions as **G** without the need for zero-padding.

In the Eq.2, the masks implicitly compare the location of elements in **G** to the relative position of the current output within the larger output tensor **O**. We generalize the ideas above to graphs by allowing for masks that operate on node pairs, rather than comparing pairs of elements in tensor data. When generalizing the convolution to graph input, we want the output to be a graph as well. Here, we limit ourselves to outputting graphs with identical structure to the input and only considering the local neighborhood of a node when calculating the features of the corresponding output node. We formally define output of the quantized graph convolution layer as follows:

\[}(v)=_{h=0}^{H-1}(}_{h,:,:}_{v^{ }(v)}1_{((v,v^{})_{h})}}(v^{ }))+}_{h,:},\] (3)

where \(}(v)^{F}\) provides the feature vector of the output node at the same relative location as input node \(v\), \((v)\) is the set of nodes in the local neighborhood around \(v\), \(_{h}\) is the set of node pairs selected by the mask corresponding to \(}_{h,:,:}\), and \(}(v)^{F^{}}\) retrieves the feature vector of node \(v\). In the context of QGCNs, we refer to each \(}_{h,:,:}\) as a _sub-kernel_. It is the process of using binary masks of fixed cardinality to _quantize_ the space of potential nodes in a local neighborhood that gives quantized graph convolution networks their name. This framework is sufficient to include most practical use cases of the convolutional layer. For example, convolutional kernels with any dimension larger than three can be represented by considering tensor elements to be connected in the graph representation if the kernel would apply to one node while the kernel is positioned with the other node at its 'center'. Two noteworthy exceptions that do not fit in the QGCN framework are stride sizes larger than one or convolutional kernels with odd dimensions (to be explored in future work).

### A satisficing mapping generalizes local convolutional kernel masks

In this section, we show how the sub-kernel masks (\(_{h}\)) associated with convolutional kernels can be extended to the case of graphs with (pseudo-)positional information. The masks associated with CNN convolutions are functions of the relative position of tensor elements (nodes) to the position of the current element in the output tensor (see Eq. 2). We refer to these as the _natural_ convolutional masks (see Fig. 1 f). For graphs with positional information, it is possible to formalize a method for choosing a set of sub-kernel masks that would produce standard convolutional layer masks when applied to tensor data that has been converted to graph form. For simplicity we consider the case of a convolution in two dimensions where data is converted to a 2D positional graph by assuming that edges exist only between adjacent elements (nodes) and extend this case to handle all 2D positional graphs. Note that in the absence of positional information, any other information associated with nodes that can be embedded into a 2D space can be treated as pseudo-positional information to enable this more general approach.

As seen in Eq. 2, each mask is defined by the indicator function \(1_{(h=jK+k)}\), which selects a single element. When dealing with tensor data, the relative position of the elements selected by these masksremains fixed as the convolutional kernel moves to different output positions. In contrast, we cannot assume that the nodes in a local neighborhood will always be located in the same relative positions for all local neighborhoods within a graph. Because we desire a set of masks that is applicable to all 2D positional graphs, our masks must define a way to potentially map all of 2D space to a set of sub-kernels. One simple option is dividing 2D space into regularly spaced non-overlapping segments defined by angular position relative to the center node of the local neighborhood that the map is being applied to. Then the center-neighbor node pair (\(v\), \(v^{}\)) is part of the \(h^{th}\) sub-kernel mask (\(_{h}\)) when

\[&_{h}=(v,v^{}) 2 +(v,v^{})<2+} _{ v V,v^{}(v)},\;h[1,H-1],\\ &(v,v^{})=^{-1}(v^{})-p_ {y}(v)}{p_{x}(v^{})-p_{x}(v)}[0,2),\] (4)

where \(\) is the angle of the neighbor node relative to the center node, \(p_{x}\) and \(p_{y}\) return the \(x\) and \(y\) coordinates of a node, respectively, and \(H\) is the total number of sub-kernels. The \(0^{th}\) sub-kernel is applied only to the node pair \((v,v)\) made up of the center node and itself. The offset angle \(\) is an optional hyperparameter that can be used to select the starting point from which the space is divided. To select \(H\), we choose the smallest number that results in all nodes within a local neighborhood being assigned to a different sub-kernel, which we refer to as a _satisficing mapping_. It is easy to see that separating the local elements of tensor data based on this method produces the same assignments as the natural convolutional kernel masks (Fig. 1 f). Algorithm 2 in Appendix D outlines an efficient process for determining the value of \(H\) that satisfies this condition. If the sub-kernel masks are chosen in this way and, importantly, the bias values for each sub-kernel are tied to the same value (i.e. \(}_{h,:}=}{JK}, h\)), then we arrive at a set of quantized graph convolution sub-kernels that will behave on 2D positional graph data identically to a standard 2D convolutional layer on equivalent tensor data (see Appendix C for proof and below for empirical validation).

Figure 1: Contrasting the assignment of kernel weights to local neighborhood nodes for traditional CNN convolution kernels and the satisficing mapping sub-kernels of a QGCL layer. Traditional CNN convolution kernel is depicted with its natural kernel weights masks while QGCL sub-kernels are shown with their corresponding quantizing kernel masks on graph neighborhoods. Note that the angular quantization bins have inclusive angular lower bounds and exclusive angular upper bounds, such that nodes falling on the edges are mapped to unique sub-kernels (e.g., the node in (h.) on the 135\({}^{}\)edge maps to the green mask sub-kernel.

With this _satisficing mapping_ approach, the process of assigning nodes in every local neighborhood to sub-kernels (see Algorithm 1 in Appendix D) incurs a computational cost of \(O(|V|^{2})\) in each forward pass. In the case of homogeneous graph meshes, choosing to cache the satisficing mapping incurs at worst \(O(|V|^{2})\) space complexity. Using Algorithm 2 for determining the minimum number of subkernels for the convolution such that a satisficing mapping is honored adds a constant cost on top of Algorithm 1. We note that this is only one of many possible ways in which the natural convolutional mask and sub-kernels described in Eq. 2 can be extended to positional graph data.

### Learning neighborhood quantization

Next, we introduce a method for generating masks that assign nodes to sub-kernels in arbitrary dimensions and regardless of whether positional information is present. Specifically, we frame quantization as a learnable multinomial classification problem where for a learnable model assigns a sub-kernel to each center-neighbor node pair in a local neighborhood. This approach was inspired by the idea of dilated convolutions in CNNs, akin to learning the spacings of the kernel elements during CNN convolution . To learn the quantization, we introduce _QuantNet_, an MLP that projects node features or (pseudo-)positional descriptors into a higher dimensional space where we difference the target and source features and then project this difference to a vector representing assignment weights for each sub-kernel (see Fig. 2). The mask \(_{h}\) associated with sub-kernel \(h\) contains the ordered node pair \((v,v^{})\) when _QuantNet_\(Q\) assigns the node pair to \(h\):

\[&_{h}=\{(v,v^{})|Q(v,v^{})=h\}_{  v V,v^{}(v)}\,,\\ & Q(v,v^{})=((U_{2}(U_{1}(v; )-U_{1}(v^{};);))),\] (5)

where \(U_{1}\) is a high dimensional MLP projector with parameters \(\) for the input features (spatial descriptors, node features, etc.), \(U_{2}\) is a low dimensional projector with parameters \(\) for the difference in high dimensional features projected by \(U_{1}\), \(v\) is a node in \(V\) (the node set of the input graph), and \((v)\) denotes the local neighborhood node set of \(v\). Note _argmax_ in Eq. 5 is symbolic; it represents any differentiable function that outputs discrete categorical samples, for example, a custom _argmax_ implemented with a straight-through gradient estimator or Gumbel-Softmax with hard sampling (our implementation uses Gumbel-Softmax with hard sampling) . The _QuantNet_ network architecture is shown in Fig. 2. Finally, we reiterate that because _QuantNet_ can use any vector in place of positional information, QGCL becomes extensible to graphs without explicit positional information.

### Integrating QGCNs with a residual architecture

A common and successful approach used to address vanishing gradients and over-smoothing in GNNs is residual learning, inspired by the success of ResNets for CNNs .

We adapt this framework

Figure 2: _QuantNet and Quantized Graph Residual Layer (QGRL)._ [Left] A learnable network for dynamic quantization of nodes to subkernels in different local neighborhoods. The message passing framework in PyTorch provides the source and target nodes across all edges so QGCL doesnâ€™t have any computation overheads in defining the input tensors fed into QuantNet. The output of QuantNet is the satisficing mapping used to filter the receptive fields of the QGCL subkernels. [Right] An architectural retrofit of QGCL, incorporating 2 residual blocks: (1) outer residual block for the QGCL and (2) an inner residual block for learning features from input graph messages. The network combines all features dynamically via MLP-III to prepare the final node messages for the layer.

to QGCNs, arriving at the architecture shown in the right panel of Fig. 2, which we call _Quantized Graph Residual Layer (QGRL)_. Notice that QGRL subsumes and generalizes QGCL.

## 4 Experiments

### Empirical validation of equivalence with CNNs on 2D images

First, we confirmed that the QGCL performs similarly to the CNN convolutional layer when applied to image data. We considered three standard 2D image datasets that vary in complexity: MNIST , Fashion-MNIST , and CIFAR10 . MNIST contains gray-scaled images of handwritten digits of shape 28x28x1, FashionMNIST contains fashion images of shape 28x28x1, and CIFAR10 consists of color images of shape 32x32x3 from 10 categories. Figure 4 in Appendix E shows the different CNN models trained for the different standardized datasets. We created a 3-layer CNN and its equivalent QGCN model for the MNIST dataset, 6-layer network models for Fashion-MNIST, and 9-layer network models for CIFAR10. The equivalent QGCN models have the same architecture as the CNN models, except that QGCN uses QGCL layers internally in place of traditional convolutional layers. All models were trained 5 times on each dataset, with different random parameter initializations and random ordering of the training data for each run, using cross-entropy loss and the Adam optimizer  with a learning rate of 0.01 for 200 epochs. In order to establish equivalence between CNN and QGCN while avoiding full-dataset ceiling effects we separately trained models fit at three different sample sizes (yielding different bias-variance trade-offs) by varying the dataset train-test splits (see Appendix F).

Table 1 shows how QGCN performs almost identically to CNN across the different standard image datasets. Appendix F (table 8) shows the expanded version of 1, showing different train-test splits, devised to explore bias-variance trade-offs. Additionally, Appendix G shows training loss and train/test set accuracy profiles over a wide range of learning rates for both CNN and its parameter-matched equivalent QGCN (not QGRN) to show how model behaviors are very similar even in different bias-variance trade-off regimes. These results confirm how both models follow exceedingly similar loss trajectories during training and have the same accuracy profiles, empirically supporting our formal proof of CNN and QGCN equivalence on image data.

### Graph Classification: Datasets with Positional Descriptors

Next, we compared QGRN to SGCN on graph datasets that have positional descriptors, including a novel FEM fixed-mesh graph dataset. The graph benchmark datasets: AIDS, Letters (high/low/med) were post-processed to extract out their positional node descriptors into separate positional attributes

**Dataset** &  \\  & & CNN & QGCN \\  MNIST & \(98.92 0.10\) & \(98.98 0.04\) \\ FashionMNIST & \(92.56 0.18\) & \(92.39 0.13\) \\ CIFAR-10 & \(80.21 0.29\) & \(79.59 0.35\) \\ 

Table 1: _Standard image datasets._ CNN and QGCN model accuracies (mean \(\) S.D.).

**Dataset** &  &  &  \\  & QGRN & SGCN & QGRN & SGCN & QGRN & SGCN \\  NS-Binary & 58.67 & 57.37 & 535.17 & 800.32 & **99.67**\(\)\(00.23\) & **99.67**\(\)\(00.23\) \\ NS-Denary-1 & 58.67 & 57.37 & 535.17 & 800.32 & **97.47**\(\)\(01.01\) & \(94.23 01.32\) \\ NS-Denary-2 & 58.67 & 57.37 & 535.17 & 800.32 & **95.13**\(\)\(05.21\) & \(93.33 05.77\) \\ AIDS & 59.43 & 57.61 & 9.30 & 13.03 & **99.50**\(\)\(00.14\) & \(99.25 01.07\) \\ Letter (high) & 56.33 & 49.72 & 0.77 & 0.98 & **94.10**\(\)\(00.81\) & \(93.21 00.79\) \\ Letter (low) & 53.14 & 49.72 & 0.74 & 0.95 & **99.81**\(\)\(00.23\) & \(99.62 00.14\) \\ Letter (med) & 53.14 & 49.72 & 0.74 & 0.95 & **97.14**\(\)\(00.71\) & \(95.24 00.79\) \\ 

Table 2: _Custom Graph Datasets_. QGRN and SGCN Performance Comparisonthat QGRN and SGCN use. This was to show that QGRN is able to use positional descriptors when they exist and is able to perform competitively with models such as SGCN designed specifically to use positional descriptors. Table 2 provides test set accuracy, as well as model size and computational complexity for each model on each positional graph dataset. We highlight that QGRN performs equal to or better than SGCN on all positional graph datasets we tested.

#### 4.2.1 Custom FEM Dataset

We compare QGRNs and matched SGCNs on our new simulated Navier-Stokes non-linear dynamics benchmark datasets for binary and denary classification. We simulated the "flow past a cylinder" problem on an adaptive mesh with the underlying two-dimensional flow geometry depicted in Appendix H Fig. 14(a). For binary classification, we separated laminar and turbulent flows based on distinct Reynold's number (\(Re\)) values while for denary classification we used evenly spaced \(Re\) values. We created three datasets: Navier-Stokes-Binary (NS-Binary) for easier binary classification and Navier-Stokes-Denary-1 (NS-Denary-1) and Navier-Stokes-Denary-2 (NS-Denary-2) for more challenging denary classification, with NS-Denary-2 being most challenging (most closely spaced \(Re\) values; see Appendix H). QGRNs matched SGCN performance on the binary task and outperformed SGCNs on the more challenging denary tasks.

### Graph Classification: Generic Graph Datasets

Finally, we compared QGRNs to matched (in model parameter count) GNN models using inductive learning datasets from Benchmark Data Sets for Graph Kernels , namely: AIDS, COIL-DEL, Frankenstein, Enzymes, Letter (low/med/high), Mutagenicity, Proteins, Proteins-Full, Mutag and Synthie. See Appendix I for a description of each dataset. We trained on a number of novel GNN architectures including Transformer networks (GAT, TransformerConv), showing how QGRN maintains superior performance over its competitors on many of the benchmark datasets (which lack positional descriptors). We size all models relative to QGRN to have a matched number of parameters for fair comparison. Given the simplicity of some of the models, this effort of establishing equivalence yields slightly different architectures, however, all architectures are constrained to have the same depth. More details are provided in the dataset configuration section of the provided code. All models were trained with the Adam optimizer using cross-entropy loss for 500 epochs at 4 different learning rates (0.1, 0.01, 0.001, 0.0001). Appendix subsection K shows how QGCN wall clock time varies compared to other GNN methods with matching parameter sizes. Each run is repeated 3 times and we report the best accuracy for each model across these learning rates.

Tables 3, 4 and 12 showcase QGRN matching and outperforming all GNN methods across a diverse sampling of inductive graph learning problems. All datasets appearing in these tables either do not have positional descriptors or have their positional attributes collapsed into the individual node features. We do this because many of the generalized GNNs in the literature such as ChebConv, GCNConv, GraphConv etc. are not able to handle positional descriptors as separate attributes from node features. In the tables, we see clearly how QGRN matches or outperforms all models on all benchmark graph classification tasks.

Finally, there are additional experiments we carried out such as how QGRN fares in deeper networks (see Appendix O), how different quantizations impact model performance (see Appendix P), how

**Models** & AIDS & Frankenstein & Mutag & Proteins \\  QGRN & \( 0.10\) & \( 0.40\) & \( 00.26\) & \( 00.14\) \\ GCNConv & \(90.92 0.38\) & \(60.27 0.06\) & \(92.68 02.29\) & \(71.95 00.57\) \\ ChebConv & \(93.42 0.14\) & \(62.56 0.28\) & \(91.87 01.41\) & \(75.58 01.51\) \\ GraphConv & \(94.25 0.16\) & \(65.89 0.28\) & \(95.12 00.48\) & \(74.59 00.57\) \\ SGConv & \(91.92 0.14\) & \(60.23 0.06\) & \(92.68 00.49\) & \(72.94 00.57\) \\ GENConv & \(99.17 0.14\) & \(66.74 0.42\) & \(98.37 01.41\) & \(79.87 00.57\) \\ GeneralConv & \(94.33 0.14\) & \(65.67 0.42\) & \(92.68 00.45\) & \(74.59 00.57\) \\ GATv2Conv & \(98.58 0.38\) & \(63.71 0.29\) & \(95.94 01.41\) & \( 00.99\) \\ TransformerConv & \(99.25 0.14\) & \(64.40 0.32\) & \(92.68 02.29\) & \(79.21 00.67\) \\ 

Table 3: _Graph kernels benchmark datasets - 1._ Test Accuracy (%) across different GCNs

[MISSING_PAGE_FAIL:9]

frequency bands (4-8 Hz, 8-12 Hz, 12-30 Hz, and 30-44 Hz) for each of the 32 electrodes. Power features were used as node attributes in a fully-connected graph containing all electrodes. We trained a separate model for each subject, dividing the data for each subject according to a 64%/16%/20% train/validation/test split. The generative objective for the autoencoder was mean-squared error (MSE) and the supervised objective was cross-entropy (CE) for classifying whether subjects self-rated their emotional state as positive valence (\( 5\)) or negative valence (\(<5\)) on a 9-point scale. Models were pre-trained for 1000 epochs on just the generative objective, then for another 100 epochs with one of 3 values of weight (100, 1000, or 10000) on the classification objective. Validation sets were used to select the weight and number of training epochs with highest area under the receiver operating curve (AUC). As shown in table 6, we found that using QGRN layers in the same autoencoder architecture compared to SGCN layers resulted in better generative and supervised loss values on the held out test sets. In addition, the QGRN-based model resulted in appreciably better classification performance (measured by AUC) for this difficult classification problem.

## 5 Discussion

This work introduces Quantized Graph Convolution Networks (QGCNs), a flexible framework for designing graph neural networks that extends the benefits of CNNs' strong local inductive bias to graphs. QGCNs "quantize" the space of possible neighbor nodes in a local neighborhood into a fixed set of sub-kernels. We show, both theoretically and empirically, that QGCNs are a generalization of CNNs to graphs with positional information. We extend QGCNs to arbitrary graphs by introducing the QuantNet method for learning sub-kernel assignment in a QGCN. We then show that embedding a QGCL within a residual network architecture gives state-of-the-art results on a suite of benchmark graph and node classification tasks, in addition to a novel Navier-Stokes FEM dynamics classification dataset. Finally, we demonstrate that QGCLs improve the performance of a supervised autoencoder to jointly model EEG data and emotional state.

One significant limitation of the current work is that the implementation of the QGCL is not yet efficient as demonstrated in the comparison of wall clock runtime with other models in Appendix K. Future work will look into parallelized subkernel operations to demonstrate wall clock runtime competitive with similar models in the literature. QGCNs also do not generalize _all_ CNNs, as it cannot represent convolutional layers with odd-numbered kernel sizes (i.e. no 'center' element) or stride sizes other than one. Finally, the use of the QGCL layer in more complex architectures such as U-Nets could be explored in future work, alongside examining the performance of this model on more inductive and transductive tasks to assess its strength in various learning scenarios.

We have shown that QGCNs are a true generalization of CNNs and therefore are capable of maintaining the same powerful inductive bias that has led to such great success in the application of CNNs. In our experience, QGCNs are more expressive and efficient at learning complex local patterns of correlation in graph data than competing methods, and we expect that QGCNs can be successfully applied in many domains where graph data are prevalent. We expect that future research into different masking functions for QGCN sub-kernels (in addition to the angular satisficing mapping and QuantNet masking functions we outline here) will further extend the potential usefulness of QGCNs.