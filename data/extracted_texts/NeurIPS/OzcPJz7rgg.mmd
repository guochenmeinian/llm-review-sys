# Starss23: An Audio-Visual Dataset of

Spatial Recordings of Real Scenes with

Spatiotemporal Annotations of Sound Events

 Kazuki Shimada

Sony AI

kazuki.shimada@sony.com &Archontis Politis

Tampere University

archontis.politis@tuni.fi &Parthasaarathy Sudarsanam

Tampere University

&Daniel Krause

Tampere University

&Kengo Uchida

Sony AI

Sharath Adavanne

Tampere University

&Aapo Hakala

Tampere University

&Yuichiro Koyama

Sony Group Corporation &Naoya Takahashi

Sony AI

Shusuke Takahashi

Sony Group Corporation &Tuomas Virtanen

Tampere University

&Yuki Mitsufuji

Sony AI, Sony Group Corporation

###### Abstract

While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker. This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events. Audio-visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence. We also introduce an audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events. Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events. STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system. Our benchmark results demonstrate the benefits of using visual object positions in audio-visual SELD tasks. The data is available at https://zenodo.org/record/7880637.

## 1 Introduction

Given multichannel audio input from a microphone array, a sound event localization and detection (SELD) system [1; 45; 8; 46] outputs a temporal activation track for each of the target sound classes along with one or more corresponding spatial trajectories, e.g., the direction of arrival (DOA) around the microphone array, when the track indicates activity. Such a spatiotemporal characterization of sound scenes can be used in a wide range of machine cognition tasks, such as inference on the type of environment, tracking of specific types of sound sources, acoustic monitoring, scene visualization systems, and smart-home applications. Recently neural network (NN)-based SELDsystems  show high localization and detection performance. These systems need data with activity and DOA labels of target sound events for training and evaluation. Because annotation of DOA labels is challenging in real sound scene recordings, most SELD datasets  consist of synthetic audio data, which are made by convolution of monaural sound event signals and multichannel impulse response signals with DOA labels. The Sony-TAu Realistic Spatial Soundscapes 2022 dataset (STARSS22)  tackles real sound scene recordings with DOA labels, which is based on tracking results of a motion capture (mocap) system. Currently, STARSS22 is the only SELD dataset with real sound scenes, including overlapping sound events, moving source events, and natural distribution of temporal activation and DOA, e.g., sounds of footsteps are relatively short and heard from a lower elevation. While this dataset is suitable for evaluating audio-only SELD systems in natural sound scenes, the dataset does not include other modality input, e.g., video data.

Sound events in real sound scenes originate from their sound source objects, e.g., speech comes from a person's mouth, sounds of footsteps are produced from the feet of a walker, and a knocking sound originates from a door. Such sound source object information is usually apparent in the visual modality. Video data aligned with audio recordings in SELD tasks have the potential to mitigate difficulties and ambiguities of the spatiotemporal characterization of the sound scene as audio-visual data improves source separation  and speech recognition . Visible people in the video data can provide candidate positions of human body-related sounds. When a person walks in the video, tapping sounds are easily recognized as footsteps. To investigate how a visual feature input affects a SELD system, we propose an _audio-visual SELD_ task, which uses audio and video data to estimate spatiotemporal characterization of sound events. The left side of Figure 1 shows an audio-visual SELD system, which takes multichannel audio recordings and video aligned with the audio recordings and outputs activity and DOA of target sound events in each frame. To tackle audio-visual SELD tasks, we need an audio-visual dataset consisting of multichannel audio, video, and activity and DOA labels of sound events per frame.

There is another interest in audio-visual sound source localization tasks , which takes monaural audio and images and estimates where the audio is coming from in the images. They focus on learning audio-visual _semantic_ correspondence, not estimating _physical_ DOA around a microphone array. While the audio-visual sound source localization datasets  are adequate to train NNs with audio-visual correspondence and evaluate localization performance in images, they are typically monophonic without spatial labels for the target sound events. Several datasets serve multichannel audio and video data with real sound scenes , whereas only a few audio-visual datasets have multichannel audio data with DOA labels of speakers around a microphone array . While these datasets help to evaluate audio-visual speaker DOA estimation (DOAE) tasks, the evaluation focuses only on speech, not sound events such as musical instruments or footsteps.

To tackle audio-visual SELD tasks, we introduce an audio-visual dataset, the _Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23)_, consisting of multichannel audio, video, and spatiotemporal annotations of sound events, i.e., activity and DOA labels per each frame. The right part of Figure 1 shows a still in a frame of a video in STARSS23, made from 360\({}^{}\) video, spatial acoustic power

Figure 1: An audio-visual SELD system takes multichannel audio recording data and video data aligned with the audio data and estimates temporal activity and DOA of each class per frame. In a frame \(t\) of a recording in STARSS23, a knocking sound comes from a door while a domestic sound derives from a vacuum cleaner behind a microphone array, as shown in an overlay of 360\({}^{}\) video and spatial acoustic power map.

map generated from a microphone array, and sound event labels. The dataset contains over 7-hour recordings with 57 participants in 16 rooms with the spatiotemporal annotation as a development set. The participants are guided by generic instructions and suggested activities in the recording to induce adequate occurrences of the sound events and diversity of content. There are 13 classes of target sound events, such as speech, musical instruments, and footsteps. To reveal whether the dataset has a natural distribution of sound events, we analyze the dataset regarding frame coverage, overlap, and DOA distributions per each sound event class. We develop and test an audio-visual SELD system with STARSS23. To investigate the effects of audio-visual input, we present overall localization and detection performance and per-class results.

## 2 Related Work

Sound event localization and detectionSELDnet  is the first SELD method, which uses convolutional recurrent neural network (CRNN) to output activity and DOA separately. An activity-coupled Cartesian DOA (ACCDOA)  vector, which embeds sound event activity information to the length of a Cartesian DOA vector, enables us to solve SELD tasks with a single output. While the two methods tackle overlaps from different classes, they cannot solve overlaps from the same class. Multi-ACCDOA  is an ACCDOA extension, allowing models to output overlaps from the same class. To handle that case effectively, Multi-ACCDOA incorporates auxiliary duplicating permutation invariant training (ADPIT) . There are other SELD works about framework [8; 29; 55], network architecture , audio feature [30; 21], and data augmentation [51; 42].

To train or evaluate SELD methods, we need multichannel audio data with temporal activation and DOA labels. In synthetic multichannel audio datasets [2; 36; 35; 19; 28], DOA labels can be easily annotated because the data are made from multichannel impulse response signals with DOA labels. While the SECL-UMons and AVECL-UMons datasets [6; 5] tackled spatial recording with DOA labels, it is limited to isolated single event recordings or combinations of two simultaneous events, ignoring spatiotemporal information linking events in a natural scene. STARSS22  tackled real spatial recording with temporal activation and DOA labels of each target class in natural scenes. Participants improvise natural scenes with a mocap system, whose tracking results are used for DOA labels. However, the dataset does not release video data. Therefore, it cannot be used to evaluate audio-visual SELD systems.

Audio-visual sound source localizationThere is broad interest in audio-visual sound source localization tasks [44; 12; 61; 4; 33]. Chen _et al._ have tackled unsupervised learning to localize sound sources in video and evaluated the method on the VGG-SS dataset , which annotates bounding boxes of sound sources for sound and video pairs. The AVSBench dataset  serves pixel-level audio-visual segmentation maps for videos over 23 class categories. Because the datasets do not have multichannel audio recordings, they cannot be applied to evaluating SELD tasks.

Audio-visual dataset with multichannel audioSeveral audio-visual datasets include multichannel audio data [27; 53; 54; 60; 56; 10; 40; 41; 50]. As many datasets are used for self-supervised learning [27; 54; 53] or non-localization tasks [60; 56; 10], there are no DOA labels. The YouTube-360 dataset  serves first-order ambisonics (FOA) signal and 360\({}^{}\) video data without any labels for self-supervised learning.

    &  &  &  \\  & & audio & \\  STARSS22  & ✓ & - & Activity \& DOA of sound events \\ VGG-SS  & - & ✓ & Bounding box \\ AVSBench  & - & ✓ & Segmentation map \\ YouTube-360  & ✓ & ✓ & - \\ AVRI  & ✓ & ✓ & Activity \& DOA of speech \\  STARSS23 (Ours) & ✓ & ✓ & Activity, DOA \& distance of sound events \\   

Table 1: We compare STARSS23 to other real sound scene datasets from three points of view: whether audio data is multichannel, whether video data is included, and what annotation type is.

A few audio-visual datasets are collected for audio-visual DOAE tasks [40; 41; 50]. Qian _et al._ proposed an audio-visual DOAE system, which takes spectrograms and phase features from the audio input and face-bounding boxes from video input to estimate the DOA of each speaker . The system was evaluated with the Audio-Visual Robotic Interface (AVRI) dataset, recorded using Kinect and a four-channel Respeaker array, along with activity and DOA labels. The audio-visual features are helpful for DOAE, and the dataset supports the evaluation of audio-visual speaker DOAE. However, the dataset is only for speech, not various sound events such as clapping and knocks. We summarize the comparison of STARSS23 with other real sound scene datasets in Table 1.

## 3 STARSS23 Dataset

### Overview

STARSS23 contains multichannel audio and video recordings of sound scenes in various rooms and environments, together with temporal and spatial annotations of sound events belonging to a set of target classes. The dataset enables us to train and evaluate audio-visual SELD systems, which localize and detect sound events from multichannel audio and visual information. STARSS23 is available in a public research data repository1 under the MIT license. There is also a demo video2.

The contents are recorded with short instructions, guiding participants in improvising sound scenes. The recordings contain a total of 13 target sound event classes. The multichannel audio data are delivered as two 4-channel spatial formats: FOA and tetrahedral microphone array (MIC). The video data are anonymized 1920\(\)960 equrectangular videos recorded by a 360\({}^{}\) camera. The annotations of STARSS23 consist of temporal activation, DOA, and source distance of the target classes. STARSS23 is split into a development set and an evaluation set. The development set totals about 7 hours and 22 minutes, of which 168 clips were recorded with 57 participants in 16 rooms. The development set is further split into a training part (dev-set-train, 90 clips) and a testing part (dev-set-test, 78 clips) to support the development process. In the evaluation set, no publicly available annotations exist because the evaluation set is prepared for a competition, which is described in Appendix D.

STARSS23 improves a multichannel audio dataset, i.e., STARSS22 . One of the critical differences is releasing video data aligned with multichannel audio data. While we maintain all the sessions of STARSS22, we add about 2.5 hours of material to the development set. STARSS23 also serves source distance labels of sound events as additional annotations. We follow the recording procedure in STARSS22, where video data are used only to check labels internally. Adding descriptions about video data and distance annotation, we show the data construction part as an audio-visual dataset.

### Data Construction

As shown in Figure 2, STARSS23 is constructed in three steps: sound scene recording, data conversion, and annotation. We explain each step as follows.

Figure 2: The pipeline of data construction.

Sound scene recordingSTARS523 was created in Tampere, Finland, and Tokyo, Japan. Recordings at both sites shared the same process, organized in sessions corresponding to different rooms, sound-making props, and participants. In each session, various clips were recorded with combinations of that session's participants acting simple scenes and interacting among themselves and with the sound props. The scenes were based on generic instructions on the desired sound events. The instructions were a rough guide to ensure adequate event activity and occurrences of the target sound classes in a clip. The left photo of Figure 2 shows that participants improvise following the instructions.

A set of 13 target sound event classes are selected to be annotated, based on the sound events captured adequately in the recorded scenes. The class labels are chosen to conform to the AudioSet ontology . They are: _female speech, male speech, clapping, telephone, laughter, domestic sounds, footsteps, door, music, musical instrument, water tap, bell, knock_. Music, e.g., background music or pop music, is played by a loudspeaker in the room. On the other hand, musical instruments are played by participants, including acoustic guitar, piano, and others. Domestic sounds consist of vacuum cleaners, mechanical fans, and boiling, which have strongly directional and loud sounds. They can be distinguishable from natural background noise in sound scenes. The scenes also contain directional interference sounds, such as computer keyboard or shuffling cards that are not labeled.

As shown in the left photos of Figure 2, each scene was captured with audio-visual sensors, i.e., a high-resolution 32-channel spherical microphone array (Eigenmike em323) with a height set at 1.5 m, and a 360\({}^{}\) camera (Ricoh Theta V4) mounted 10 cm above the microphone array. For each recording session, a suitable position of the Eigenmike and Ricoh Theta V was determined to cover the scene from a central place. We also captured the scenes with two additional sensors for annotation: a mocap system of infrared cameras surrounding the scene, tracking reflective markers mounted on the participants and sound sources of interest (Optitrack Flex 135), and wireless microphones mounted on the participants and sound sources, providing close-mic recordings of the main sound events (Rode Wireless Go II6). The origin of the mocap system was set at ground level on the exact position of the Eigenmike. In contrast, the mocap cameras were positioned at the room's corners.

Recording starts on all devices before the beginning of a scene and stops right after. A clapper sound initiated the acting, and it served as a reference signal for synchronization between the different types of recordings, including the mocap system that can record a monophonic audio side signal for synchronization. All types of recordings were manually synchronized based on the clapper sound and subsequently cropped and stored at the end of each recording session. The details of sound scene recording, e.g., generic instructions, sound events, and sensors, are summarized in Appendix A.1.

Data conversionThe original 32-channel recordings were converted to two 4-channel spatial formats: FOA and MIC. Conversion of the Eigenmike recordings to FOA following the SN3D normalization scheme (or ambiX) was performed with measurement-based filters . Regarding the MIC format, channels 6, 10, 26, and 22 of the Eigenmike were selected, corresponding to a nearly tetrahedral arrangement. Analytical expressions of the directional responses of each format can be found in . Finally, the converted recordings were downsampled to 24kHz. The raw 360\({}^{}\) video data were converted to an equirectangular format with 3840\(\)1920 resolution at 29.97 frames per second, which was convenient to handle as planar video data. Based on the participant's consent, the visible faces of all recordings were blurred. Finally, the video with face-blurring was converted to a 1920\(\)960 resolution.

AnnotationSpatiotemporal annotations of the sound events were conducted manually by the authors and research assistants. As shown in the lower right part of Figure 2, there were four steps: a) annotate the subset of the target classes that were active in each scene, b) annotate the temporal activity of such class instances, c) annotate the position of each such instance when active, moreover, d) confirm the annotations. Class annotations (a) were observed and logged during each scene recording. Activity labels (b) were manually annotated by listening to the wireless microphone recordings. Because each wireless microphone would capture prominent sounds produced by the participant or source it was assigned to, onset, offsets, source, and class information of each event could be conveniently extracted. In scenes or instances where associating an event to a source was ambiguous purely by listening, annotators would consult the video recordings to establish the correct association. The temporal annotation resolution was set to 100 msec. After onset, offset, and class information of events was established for each source and participant in the scene, the positional annotations (c) were extracted for each such event by attaching tracking results to the temporal activity window of the event. Positional information was logged in Cartesian coordinates with respect to the mocap system's origin. The event positions were converted to spherical coordinates, i.e., azimuth, elevation, and distance, which are more convenient for SELD tasks. Then, the class, temporal, and spatial annotations were combined and converted to the text format used in the previous dataset . The annotation details are summarized in Appendix A.2. Confirmation of the annotations (d) was performed by listening to the Eigenmike recording while watching a synthetic video. The video was the equirectangular one overlapped with the event activities, which were visualized as labeled markers positioned at their respective azimuth and elevation on the video plane. If a clip was not passed the confirmation, the clip was annotated again.

### Data Analysis

Having a natural distribution of sound events is beneficial to evaluate audio-visual SELD systems. We analyze the frame coverage, polyphony, and DOA per sound event classes on the dev-set-train of STARSS23. Table 2 shows frame coverage and max, mean, and distribution of polyphony globally and of each class separately. Regular classes in frames are female and male speech, music, and domestic sounds. These classes are also frequent in our daily lives. Musical instruments and laugh classes show a high mean polyphony of the same class, which are natural situations in jam sessions and conversations. Figure 3 shows the distribution of DOA with the axis of the azimuth and elevation. Regarding female speech in Figure 2(a), the elevation distribution has a strong peak around -10 degrees, while that of azimuth seems uniformly distributed. Compared to the speech class, footsteps appear in lower azimuth than -10 degrees. See Appendix B for further data analysis, e.g., duration.

## 4 Benchmark

In this section, we examine an audio-visual SELD task with STARSS23. For evaluation, we set the dev-set-train for training and hold out the dev-set-test for validation.

    & **Global** & **Fenn.** & **Male** & **Cup** & **Phone** & **Lough** & **Dom.** & **Pounds** & **Footsteps** & **Dover** & **Music** & **Music.** & **Faucet** & **Bell** & **Kneck** \\  Frame coverage &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ (‘total frames’) & & & & & & & & & & & & & & \\  Max. polyphony & 6 & 2 & 3 & 2 & 1 & 4 & 2 & 3 & 1 & 2 & 2 & 1 & 1 & 1 \\ Mean polyphony & 1.47 & 1.05 & 1.06 & 1.06 & 1.00 & 1.25 & 1.03 & 1.02 & 1.00 & 1.20 & 1.28 & 1.00 & 1.00 & 1.00 \\  Polyphony 1 &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ (‘s’ active frames’) & & & & & & & & & & & & & \\ Polyphony 2 & 28.443 & 4.56 & 5.51 & 6.26 & 0 & 18.48 & 2.73 & 1.94 & 0 & 20.01 & 28.36 & 0 & 0 & 0 \\ Polyphony 3 & 7.385 & 0 & 0.14 & 0 & 0 & 2.39 & 0 & 0.19 & 0 & 0 & 0 & 0 & 0 & 0 \\ Polyphony 4 & 11.29 & 0 & 0 & 0 & 0 & 0.46 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ Polyphony 5 & 0.162 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ Polyphony 6 & 0.003 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\   

Table 2: Frame coverage and max, mean, and the degree of polyphony globally and of each class separately on the dev-set-train of STARSS23. The mean polyphony is computed over active frames.

Figure 3: The distribution of the azimuth and elevation. Each point in the scatter figure represents a frame. The top and right figures are stacked histograms of azimuth and elevation.

### Audio-Visual SELD System

To build an audio-visual SELD system, we start with an audio-only system based on SELDnet  and multi-ACCDOA , which is widely used in audio-only SELD tasks [39; 43; 31]. To extend the audio-only systems to audio-visual, we merge visual and audio information in the middle of the network, following the audio-visual speaker DOAE work .

First, we summarize the audio-only SELD system. Let \(_{}^{M F T}\) be an input consisting of \(M\)-channel \(F\)-dimensional \(T\)-frame audio features, e.g., amplitude spectrograms. Convolutional layers are used as an audio embedding network, \(_{,_{}}()\), where \(_{}\) is the network parameters:

\[_{}=_{,_{}}(_{ }),\] (1)

where \(_{}^{K_{} T^{}}\) is an audio embedding. \(K_{}\) and \(T^{}\) are the length of an embedding vector and the number of time frames, respectively. The embedding is followed by an audio-only decoder network with parameter \(_{}\), \(_{,_{}}()\), which consists of a gated recurrent unit (GRU) layer and a fully connected layer (FC):

\[}=_{,_{}}(_{ }),\] (2)

where \(}^{3 N C T^{}}\) is an estimated output of \(N\)-track \(C\)-class \(T^{}\)-frame multi-ACCDOA. Each class of the multi-ACCDOA output in track \(n\) and frame \(t\) is represented by a three-dimensional vector with Cartesian coordinates \(x\), \(y\), and \(z\), as shown in the bottom of Figure 4. The length of the vector shows activity, and the direction of the vector indicates the DOA around the microphone array. To train the SELD system, we use mean squared error (MSE) between the estimated and target multi-ACCDOA outputs under the ADPIT scheme . In inference, when the length of the vector is greater than a threshold, the class is considered active.

Then, we prepare a visual embedding. As visual input, we use the corresponding video frame at the start of the audio features. With the corresponding image, an object detection module, e.g., YOLOX , outputs bounding boxes of potential objects on target classes, e.g., person class. Let us denote a bounding box as \((u_{b},v_{b},w_{b},h_{b})\), where \(u_{b}\) and \(v_{b}\) indicate the image position of the top-left corner, and \(w_{b}\) and \(h_{b}\) show the width and height of the bounding box. Each bounding box is encoded to two vectors \(_{}(u)\) and \(_{}(v)\) along the image's horizontal axis \(u\) and vertical axis \(v\). We follow the same formulation in  using a Gaussian distribution:

\[_{}(u)=(|^{2}}{_{u}^{2}}),\] (3)

Figure 4: An audio-visual SELD system uses a CRNN model with a multi-ACCDOA output. While multi-ACCDOA output can have several tracks, we show that with a single track for simplicity. We use an object detector to incorporate visual information, which outputs bounding boxes on target classes. The results are encoded to vectors along with azimuth and elevation. Then, the encoded vectors are embedded and concatenated before the GRU layer.

where the center \(_{u}=u_{b}+w_{b}\) is the horizontal center of the bounding box, and the standard deviation \(_{u}\) is proportional to \(w_{b}\). The vertical vector \(_{}(v)\) has the same formula as \(_{}(u)\) by replacing \(u\) with \(v\), \(_{u}\) with \(_{v}=v_{b}+h_{b}\), and \(_{u}\) with \(_{v}\) in Equation 3. These vectors are concatenated into a visual feature input \(_{}^{2 B L}\). \(B\) is the maximum number of bounding boxes, and \(L\) is the length of the vectors. When no bounding box is detected, the visual feature input is set to an all-zero tensor. The right part of Figure 4 depicts the visual feature input. FCs are used as a visual embedding network, \(_{,_{}}()\), where \(_{}\) is the network parameters:

\[_{}=_{,_{}}(_{}),\] (4)

where \(_{}^{K_{}}\) is a \(K_{}\)-dimensional visual embedding vector.

Using the visual embedding vector, we finally extend the audio-only system into an audio-visual one. The visual embedding vector is repeated \(T^{}\) times to align the both embeddings in the time axis. Then the repeated visual embedding \(^{}_{}^{K_{} T^{}}\) and the audio embedding are concatenated and fed into an audio-visual decoder network with parameter \(_{}\), \(_{,_{}}()\), to get multi-ACCDOA output:

\[}=_{,_{}}(_{},^{ }_{}).\] (5)

### Evaluation Metric

We used four joint localization and detection metrics [25; 38], which are widely-used in audio-only SELD tasks [46; 39; 31]. Two metrics are referred to as location-aware detection and are error rate (\(ER_{20^{}}\)) and F-score (\(F_{20^{}}\)) in one-see non-overlapping segments. We consider a prediction as a true positive if the prediction and reference class are the same and the angle difference is below \(20^{}\). \(F_{20^{}}\) is calculated from location-aware precision and recall, whereas \(ER_{20^{}}\) is the sum of insertion, deletion, and substitution errors, divided by the total number of the references. The other two metrics are referred to as class-aware localization and are localization error (\(LE_{CD}\)) in degrees and localization recall (\(LR_{CD}\)) in one-sec non-overlapping segments, where the subscript refers to classification-dependent. Unlike location-aware detection, we do not use any threshold but estimate the difference between the correct prediction and reference. \(LE_{CD}\) expresses the average angular difference between the same class's predictions and references. \(LR_{CD}\) tells the true positive rate of how many of these localization estimates were detected in a class out of the total number of class instances. We used the macro mode of computation while the mode does not apply \(ER_{20^{}}\) because it includes substitution errors between two classes. We first computed the metrics for each class and then averaged them for the other three metrics to obtain the final system performance. See Appendix C.1 for further details.

### Experimental Setting

As audio features, multichannel amplitude spectrograms and inter-channel phase differences (IPDs) are used . Input features are segmented to have a fixed length of 1.27 sec. To reduce the calculation cost of video, we use 360\(\)180 videos converted from the released 1920\(\)960 videos. As visual input, we extract the corresponding video frame at the start of the audio features. We use a pretrained YOLOX object detection model7 to get bounding boxes of person class. Other classes, e.g., cell phone or sink, are not stably detected in our preliminary experiments with STARS523 videos. The bounding box results are encoded to vectors along azimuth and elevation as in Sec.4.1. The maximum number of bounding boxes is \(B=6\). The vector size is \(L=37(=36+1)\) to cover 360 degrees of azimuth per 10 degrees. While we tested convolutional neural network (CNN)-based visual feature extraction methods [47; 20], they performed worse than the object detection method.

To get audio embedding, we stack three convolutional layers with kernel size \(3 3\). We embed the visual encoded vectors with two FCs. The lengths of both embedding vectors are \(K_{}=K_{}=64\). The concatenated embeddings are processed with a bidirectional GRU layer with a hidden state size of 256. The number of target classes was \(C=13\). The number of tracks in the multi-ACCDOA format was fixed at \(N=3\) maximum simultaneous sources. The threshold for activity was 0.3 to binarize predictions during inference.

The experiments are for the two formats: FOA and MIC. We compare the audio-visual SELD system with an audio-only system based on the same data split and implementation. The difference is the presence or absence of video input. Because the visual input is bounding boxes of person class, we especially focus on five classes related to the human body, i.e., female and male speeches, clapping, laughing, and footsteps. To further investigate the effect of the visual input on the body-related classes, we add experiments where we set only the body-related classes as the target classes of the SELD systems. The systems are trained and evaluated with activities and DOAs of speeches, clap, laugh, and footsteps classes, i.e., \(C=5\). The other settings are the same as the 13-class SELD experiments. Details on the experimental setting are in Appendix C.2. The code of training, inference, and evaluation is available in a GitHub repository8 under the MIT license.

### Experimental Results

Table 3 summarizes the 13-class SELD performance of the audio-visual and audio-only SELD systems in both audio formats. Compared with both formats, SELD systems with FOA format show better SELD performance than those with MIC format. In FOA format, while the audio-visual SELD system shows a slightly worse location-aware F-score, the audio-visual system exhibited better localization error with comparable localization recall. There is a similar trend of lower localization error in MIC format.

We investigate the location-aware F-score over 13 classes, considering both localization and detection aspects. Figure 5 shows the F-score per class in FOA format. We show the average score of the five body-related classes on the left of the figure. The audio-visual system demonstrates a higher location-aware F-score in the body-related. On the other hand, the audio-visual system performs worse in the average of the other classes, i.e., non-body-related. The results suggest that the visual input, i.e., bounding boxes of a person, contributes to localization and detection of body-related classes, whereas the visual input might limit the performance of non-body-related classes.

Table 4 shows the body-related classes only SELD performance in audio-visual and audio-only systems. The audio-visual SELD system outperforms the audio-only system in all metrics. The visual location information of people enables the audio-visual system to localize and detect body-related classes more accurately. Audio-only system in MIC format shows high standard deviation in localization error. It is because a few classes are sometimes set 180\({}^{}\) as they had no true positive output. Even if we omit such cases, the audio-visual system still shows lower localization error.

   Format & System & \(ER_{20^{}}\) & \(F_{20^{}}\) & \(LE_{CD}\) & \(LR_{CD}\) \\  FOA & Audio-Visual & 1.03 \(\) 0.03 & 13.2 \(\) 0.6 \% & 51.6 \(\) 5.6 \({}^{}\) & 34.2 \(\) 1.0 \% \\  & Audio-Only & 0.97 \(\) 0.05 & 14.8 \(\) 0.3 \% & 55.9 \(\) 4.0 \({}^{}\) & 35.1 \(\) 2.8 \% \\  MIC & Audio-Visual & 1.08 \(\) 0.04 & 10.0 \(\) 0.8 \% & 60.0 \(\) 3.3 \({}^{}\) & 27.6 \(\) 1.7 \% \\  & Audio-Only & 1.04 \(\) 0.02 & 10.7 \(\) 2.1 \% & 66.9 \(\) 6.8 \({}^{}\) & 29.6 \(\) 3.0 \% \\   

Table 3: SELD performance (\(C=13\)) in audio-visual and audio-only systems evaluated for the dev-set-test in STASS23.

Figure 5: Location-aware F-score per sound event class (\(C=13\)) in audio-visual and audio-only systems evaluated for the dev-set-test in STASS23.

Our experimental results show that visual location information of people improves SELD performance in human body-related classes. If visual location information of people is provided, we should expect improvements in the human body-related classes, while we should not expect improvements in the rest. The results suggest that if visual information matches the target classes of an audio-visual SELD system, the audio-visual system performs better.

## 5 Discussion on Limitations and Future Work

Like any visual machine learning system, deploying audio-visual SELD systems might lead to privacy issues, e.g., capturing raw visual information, if the consent of the people being recorded is not considered. We also recognize that improved SELD capability could empower surveillance technology without appropriate regulation.

As shown in Table 2, STARSS23 is imbalanced and long-tailed. When considering it as an evaluation set, the dataset would be appropriate as it can reflect real soundscape, which is usually imbalanced and long-tailed. On the other hand, if we use it as a training set, the imbalance and long-tail make it difficult for a model to learn about rare categories. We can use machine learning algorithms [22; 7] or data sampling strategies  to tackle the data imbalance problem.

The amount of STARSS23 is relatively small as training data. We can use a multichannel audio-visual simulation platform, e.g., SoundSpaces 2.0 , for geometry-based audio rendering in 3D environments. Also, we can use multichannel audio-only data such as TAU-NIGENS Spatial Sound Events (TNSSE) datasets [2; 36; 35] to train an audio branch separately. Unlabeled multichannel audio-visual data, e.g., YouTube-360 , could help the pre-training for audio-visual SELD tasks.

There is room for developing visual features and model structure to improve the audio-visual SELD performance. While the visual location information of people improves the performance in the body-related classes, we need to develop practical visual features for the non-body-related classes. A more sophisticated model structure than the current concatenation could boost the performance. A recent work made the audio-visual SELD system better than the audio-only one in STARSS23, using decision-level audio-visual fusion with keypoints, e.g., feet or telephones .

While we apply STARSS23 for audio-visual SELD tasks, STARSS23 would be applied to other tasks, e.g., audio-visual sound source localization [44; 12; 61; 4; 33] or audio-visual cross-modal retrieval tasks . Also, when large multi-modal models [18; 49] are extended to multichannel audio, STARSS23 is an appropriate audio-visual soundscape evaluation data for the models because STARSS23 has unique spatial annotations and multichannel audio data beyond stereo.

## 6 Conclusion

This paper attempts to broaden sound event localization and detection (SELD) to an audio-visual area by introducing an audio-visual SELD task. We present an audio-visual dataset, Sony-Tau Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data, video data, and spatiotemporal annotation of sound events in natural sound scenes. Furthermore, we present quantitative evaluations for audio-visual SELD systems compared with audio-only systems and demonstrate the benefits of visual object positions. We still need to improve the SELD performance of various sound events using audio-visual data. Also, STARSS23 opens a wide range of future research on spatial audio-visual tasks, taking advantage of the well-organized audio-visual recording and detailed labels about spatial sound events.

   Format & System & \(ER_{20^{}}\) & \(F_{20^{}}\) & \(LE_{CD}\) & \(LR_{CD}\) \\  FOA & Audio-Visual & 0.93 \(\) 0.08 & 22.4 \(\) 2.4 \% & 31.0 \(\) 3.6 \({}^{}\) & 45.0 \(\) 5.0 \% \\  & Audio-Only & 0.93 \(\) 0.02 & 18.0 \(\) 1.9 \% & 33.8 \(\) 0.8 \({}^{}\) & 40.9 \(\) 5.5 \% \\  MIC & Audio-Visual & 0.95 \(\) 0.01 & 20.9 \(\) 1.9 \% & 28.9 \(\) 1.5 \({}^{}\) & 37.4 \(\) 4.6 \% \\  & Audio-Only & 0.97 \(\) 0.10 & 16.0 \(\) 3.7 \% & 58.4 \(\) 26.8 \({}^{}\) & 30.9 \(\) 10.9 \% \\   

Table 4: Body-related classes only SELD performance (\(C=5\)) in audio-visual and audio-only systems evaluated for the dev-set-test in STASS23.