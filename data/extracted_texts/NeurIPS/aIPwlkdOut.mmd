# Enhancing Preference-based Linear Bandits

via Human Response Time

 Shen Li\({}^{1}\) Yuyang Zhang\({}^{2}\)1 Zhaolin Ren\({}^{2}\) Claire Liang\({}^{1}\) Na Li\({}^{2}\) Julie A. Shah\({}^{1}\)

\({}^{1}\)Massachusetts Institute of Technology \({}^{2}\)Harvard University

{shenli,cyl48}@mit.edu, julie_a_shah@csail.mit.edu

{yuyangzhang,zhaolinren}@g.harvard.edu, nali@seas.harvard.edu

###### Abstract

Interactive preference learning systems infer human preferences by presenting queries as pairs of options and collecting binary choices. Although binary choices are simple and widely used, they provide limited information about preference strength. To address this, we leverage human response times, which are inversely related to preference strength, as an additional signal. We propose a computationally efficient method that combines choices and response times to estimate human utility functions, grounded in the EZ diffusion model from psychology. Theoretical and empirical analyses show that for queries with strong preferences, response times complement choices by providing extra information about preference strength, leading to significantly improved utility estimation. We incorporate this estimator into preference-based linear bandits for fixed-budget best-arm identification. Simulations on three real-world datasets demonstrate that using response times significantly accelerates preference learning compared to choice-only approaches. Additional materials, such as code, slides, and talk video, are available at https://shenlirobot.github.io/pages/NeurIPS24.html.

## 1 Introduction

Interactive preference learning from human binary choices is widely used in recommender systems , assistive robots , and fine-tuning large language models . This process is often framed as a preference-based bandit problem , where the system repeatedly presents queries as pairs of options, the human selects a preferred option, and the system infers preferences from these choices. Binary choices are popular because they are easy to implement and impose low cognitive load on users . However, while binary choices reveal preferences, they provide little information about preference strength . To address this, researchers have incorporated additional _explicit human feedback_, such as ratings , labels , and slider bars , but these approaches often complicate interfaces and increase cognitive demands .

In this paper, we propose leveraging _implicit human feedback_, specifically response times, to provide additional insights into preference strength. Unlike explicit feedback, response time is unobtrusive and effortless to measure , offering valuable information that complements binary choices . For instance, consider an online retailer that repeatedly presents users with a binary query, whether to purchase or skip a recommended product . Since most users skip products most of the time , the probability of skipping becomes nearly 1 for most items. This lack of variation in choices makes it difficult to assess how much a user likes or dislikes any specific product, limiting the system's ability to accurately infer their preferences. Response time can help overcome this limitation. Psychological research shows an inverse relationship between response time and preference strength : users who strongly prefer to skip a product tend to do so quickly, while longer response times can indicateweaker preferences. Thus, even when choices appear similar, response time can uncover subtle differences in preference strength, helping to accelerate preference learning.

Leveraging response times for preference learning presents notable challenges. Psychological research has extensively studied the relationship between human choices and response times [17; 19] using complex models like Drift-Diffusion Models  and Race Models [12; 66]. While these models align with both behavioral and neurobiological evidence , they rely on computationally intensive methods, such as hierarchical Bayesian inference  and maximum likelihood estimation (MLE) , to estimate the underlying human utility functions from both human choices and response times, making them impractical for real-time interactive systems. Although faster estimators exist [8; 28; 30; 67; 68], they typically estimate the utility functions for a single pair of options without aggregating data across multiple pairs. This limits their ability to leverage structures like linear utility functions, which are widely adopted both in preference learning with large option spaces [21; 24; 41; 54; 56] and in cognitive models for human multi-attribute decision-making [26; 64; 76].

To address these challenges, we propose a computationally efficient method for estimating linear human utility functions from both choices and response times, grounded in the difference-based EZ diffusion model [8; 67]. Our method leverages response times to transform binary choices into richer continuous signals, framing utility estimation as a _linear regression_ problem that aggregates data across multiple pairs of options. We compare our estimator to traditional _logistic regression_ methods that rely solely on choices [3; 31]. For queries with strong preferences, our theoretical and empirical analyses show that response times complement choices by providing additional information about preference strength. This significantly improves utility estimation compared to using choices alone. For queries with weak preferences, response times add little value but do not degrade performance. **In summary, response times complement choices, particularly for queries with strong preferences.**

Our linear-regression-based estimator integrates seamlessly into algorithms for preference-based bandits with linear human utility functions [3; 31], enabling interactive learning systems to leverage response times for faster learning. We specifically integrated our estimator into the Generalized Successive Elimination algorithm  for fixed-budget best-arm identification [29; 34]. Simulations using three real-world datasets [16; 39; 57] consistently show that incorporating response times significantly reduces identification errors, compared to traditional methods that rely solely on choices. _To the best of our knowledge, this is the first work to integrate response times into bandits (and RL)._

Section 2 introduces the preference-based linear bandit problem and the difference-based EZ diffusion model. Section 3 presents our utility estimator, incorporating both choices and response times, and offers a theoretical comparison to the choice-only estimator. Section 4 integrates both estimators into the Generalized Successive Elimination algorithm. Section 5 presents empirical results for estimation and bandit learning. Section 6 discusses the limitations of our approach. Appendix B reviews response time models, parameter estimation techniques, and their connection to preference-based RL.

_Nomenclature_: We use \([n]\) to denote the set \(\{1,,n\}\). For a scalar random variable \(x\), the expectation and variance are denoted by \([x]\) and \([x]\), respectively. The function \((x)\) denotes the sign of \(x\).

## 2 Problem setting and preliminaries

**Preference-based bandits with a linear utility function.** The learner is given a finite set of options (or "arms"), each represented by a feature vector in \(^{d}\), and a finite set of binary queries, where each query is the difference between two arms, denoted by \(^{d}\). For instance, if the learner can query any pair of arms, the query space is \(=\{z-z^{} z,z^{}\}\). In the online retailer example from section 1, the query space is \(=\{z-z_{} z\}\), where \(z\) represents purchasing a product and \(z_{}\) represents skipping (often set as \(\)). For each arm \(z\), the human utility is assumed to be linear in the feature space, defined as \(u_{z} z^{}^{*}\), where \(^{*}^{d}\) represents the human's preference parameters. For any query \(x\), the utility difference is then defined as \(u_{x} x^{}^{*}\).

Given a query \(x z_{1}-z_{2}\), we model human choices and response times using the difference-based EZ-Diffusion Model (dEZDM) [8; 67], integrated with our linear utility structure. (See appendix B.1 for a comparison with other models.) This model interprets human decision-making as a stochastic process in which evidence accumulates over time to compare two options. As shown in fig. 0(a), after receiving a query \(x\), the human first spends a fixed amount of non-decision time, denoted by \(t_{}>0\), to perceive and encode the query. Then, evidence \(E_{x}\) accumulates over 

[MISSING_PAGE_FAIL:3]

To address this problem, we adopt the Generalized Successive Elimination (GSE) algorithm [1; 3; 75]. GSE divides the total budget \(B\) into multiple phases. In each phase, it strategically samples queries until the phase's budget is exhausted, collecting both human choices and decision times. It then estimates the preference vector \(^{*}\) and eliminates arms with low estimated utilities. Decision times play a key role in the estimation step by providing complementary information about preference strength, which can enable more accurate estimation of \(^{*}\) than choices alone. Next, in section 3, we introduce a novel estimator that combines decision times and choices to estimate \(^{*}\). Then, in section 4, we discuss how this estimator is integrated into GSE to improve preference learning.

## 3 Utility estimation

This section addresses the problem of estimating human preference \(^{*}\) from a fixed dataset, denoted by \(\{x,c_{x,s_{x,i}},t_{x,s_{x,i}}\}_{x_{}, i[n_{x}]}\). Here, \(_{}\) denotes the set of queries in the dataset, \(n_{x}\) denotes the number of samples for each query \(x_{}\), and \(s_{x,i}\) denotes the episode when \(x\) is sampled for the \(i\)-th time. Samples from the same query \(x\) are i.i.d., while samples from different queries are independent. Section 3.1 introduces a new estimator, the "choice-decision-time estimator," which uses both choices and decision times, in contrast to the commonly used "choice-only estimator" that only uses choices [3; 31]. Sections 3.2 and 3.3 theoretically compares these estimators, analyzing both asymptotic and non-asymptotic performance and highlighting the advantages of incorporating decision times. Section 5.1 presents empirical results that validate our theoretical insights.

### Choice-decision-time estimator and choice-only estimator

The choice-decision-time estimator is based on the following relationship between human utilities, choices, and decision times, derived from eq. (1):

\[ x x^{}}{a}= [c_{x}]}{[t_{x}]}.\] (2)

Intuitively, when a human provides consistent choices (i.e., large \(|[c_{x}]|\)) and makes decisions quickly (i.e., small \([t_{x}]\)), it implies a strong preference (i.e., large \(|x^{}^{*}|\)). This relationship formulates the estimation of \(^{*}\) as a _linear regression_ problem. Accordingly, the choice-decision-time estimator calculates the empirical means of both choices and decision times, aggregates the ratios across all sampled queries, and applies ordinary least squares (OLS) to estimate \(^{*}/a\). Since the ranking of arm utilities based on \(^{*}/a\) is identical to that based on \(^{*}\), estimating \(^{*}/a\) is sufficient for identifying the best arm. Formally, this estimate of \(^{*}/a\), denoted by \(_{}\), is given by:

\[_{}(_{x_{}}n_{x}\;xx^{})^{-1}_{x_{}}n_{x}\;x \;^{n_{x}}c_{x,s_{x,i}}}{_{i=1}^{n_{x}}t_{x,s_{x,i}}}.\] (3)

In contrast, the choice-only estimator is based on eq. (1), which shows that for each query \(x\), the random variable \((c_{x}+1)/2\) follows a Bernoulli distribution with mean \(1/[1+(-x^{} 2a^{*})]\). Similar to the choice-decision-time estimator, the parameter \(2a\) does not impact the ranking of arms, so estimating \(2a^{*}\) is sufficient for best-arm identification. This estimation is formulated as a _logistic regression_ problem [3; 31], with MLE providing the following estimate of \(2a^{*}\), denoted by \(_{}\):

\[_{}*{arg\,max}_{ ^{d}}_{x_{}}_{i=1}^{n_{x}} (c_{x,s_{x,i}}\,x^{}),\] (4)

where \((y) 1/[1+(-y)]\) is the standard logistic function. While this MLE lacks a closed-form solution, it can be efficiently solved using optimization methods like Newton's algorithm [25; 44].

### Asymptotic normality of the two estimators

The choice-decision-time estimator from eq. (3) satisfies the following asymptotic normality result:

**Theorem 3.1** (Asymptotic normality of \(_{}\)).: _Given a fixed i.i.d. dataset \(\{x,c_{x,s_{x,i}},t_{x,s_{x,i}}\}_{i[n]}\) for each \(x_{}\), where \(_{x_{}}xx^{} 0\), and assuming that the datasets for different \(x_{}\) are independent, then, for any vector \(y^{d}\), as \(n\), the following holds:_

\[\;y^{}(_{,n}-^{*}/a) (0,^{2}/a^{2}).\]

_Here, the asymptotic variance depends on a problem-specific constant, \(^{2}\), with an upper bounded:_

\[^{2}\|y\|_{(_{x_{}} [_{x^{}_{}}[t_{x^{ }}]] xx^{})^{-1}}^{-1}.\]

The proof is provided in appendix C.2. The asymptotic variance upper bound shows that all sampled queries are weighted by a common factor \(_{x^{}_{}}[t_{x^{}}]\), which is the smallest expected decision time among all the sampled queries in \(_{}\). This weight represents the amount of information provided by each query's choices and decision times for utility estimation. A larger weight indicates that all queries in \(_{}\) provides more information, leading to lower variance and better estimates.

In contrast, the choice-only estimator from eq.4 has the following asymptotic normality result, as derived from Fahrmeir and Kaufmann [23, corollary 1]:

**Theorem 3.2** (Asymptotic normality of \(_{}\)).: _Given a fixed i.i.d. dataset \(\{x,c_{x,s_{x,i}},t_{x,s_{x,i}}\}_{i[n]}\) for each \(x_{}\), where \(_{x_{}}xx^{} 0\), and assuming that the datasets for different \(x_{}\) are independent, then, for any vector \(y^{d}\), as \(n\), the following holds:_

\[y^{}(_{,n}-2a^{*}) (0,4a^{2}\|y\|_{( _{x_{}}[a^{2}\,[c_{x} ]] xx^{})^{-1}}^{-1}).\]

This asymptotic variance shows that each sampled query \(x_{}\) is weighted by its own factor \(a^{2}\,[c_{x}]\), representing the amount of information the query's choices contribute to utility estimation. A larger weight indicates that the query contributes more information, leading to better estimates.

The weights in both theorems highlight the different contributions of choices and decision times to utility estimation. In the choice-only estimator (theorem3.2), each query is weighted by \(a^{2}\,[c_{x}]\), which depends on the utility difference \(x^{}^{*}\) for a fixed barrier \(a\). As shown by the gray curves in fig.2a, this weight quickly decays to zero as preferences become stronger (i.e., as \(|x^{}^{*}|\) increases). This indicates that _choices from queries with strong preferences provide little information_. Intuitively, when preferences are strong, humans consistently select the same option, making it hard to distinguish whether their preference is moderately or very strong. As a result, choices from such queries contribute minimally to utility estimation. This intuition aligns with the online retailer example in section1.

For the choice-decision-time estimator (theorem3.1), queries are weighted by \(_{x^{}_{}}[t_{x^{}}]\), which depends on both \(_{}\) and \([t_{x}]\). To better understand this weight, we first plot \([t_{x}]\) without the'min' operator as the orange curves in fig.2a. Comparing the orange and gray curves shows that \([t_{x}]\) is generally larger than the choice-only weight, \(a^{2}\,[c_{x}]\). The actual weight in the choice-decision-time estimator, which is the minimum expected decision time across sampled queries, is less than or equal to the orange curve but is likely still higher than the choice-only weight, especially for queries with strong preferences. This suggests that _when preferences are strong, decision times complement choices by capturing preference strength, leading to improved estimation._

When queries have weak preferences, the choice-decision-time weight may be lower than the choice-only weight. However, since the choice-decision-time weight represents only an upper bound on the asymptotic variance (theorem3.1), no definitive conclusions can be drawn from the theory alone. Empirically, as shown in section5.1, decision times add little value but do not degrade performance.

As the barrier \(a\) increases, the choice-decision-time weight rises. In contrast, the choice-only weight increases for queries with weak preferences, but this increase is concentrated in a narrower region, with weights decreasing elsewhere. Intuitively, a higher barrier reflects greater conservativeness in human decision-making, leading to longer decision times and more consistent choices (fig.1). As a result, more queries exhibit strong preferences, making choices from these queries less informative.

### Non-asymptotic concentration of the two estimators for utility difference estimation

In this section, we focus on the simpler problem of estimating the utility difference for a single query, without aggregating data from multiple queries. Comparing the non-asymptotic concentration bounds of both estimators, in this case, provides insights similar to those discussed in section 3.2. Extending this non-asymptotic analysis to the full estimation of the preference vector \(^{*}\) is left for future work.

Given a query \(x\), the task is to estimate the utility difference \(u_{x}:=x^{}^{*}\) using the fixed i.i.d. dataset \(\{(c_{x,s_{x},i},t_{x,s_{x},i})\}_{i[n_{x}]}\). Applying the choice-decision-time estimator from eq. (3), we get the following estimate (for details, see appendix C.3.1), which estimates \(u_{x}/a\) rather than \(u_{x}\):

\[_{x,}^{n_{x}}c_{x,s_{x,i}}}{ _{i=1}^{n_{x}}t_{x,s_{x,i}}}.\] (5)

In contrast, applying the choice-only estimator from eq. (4), we get the following estimate (for details, see appendix C.3.2), which estimates \(2au_{x}\) rather than \(u_{x}\):

\[_{x,}^{-1}(}_{i=1}^{n _{x}}}+1}{2}),\] (6)

where \((c_{x,s_{x,i}}+1)/2\) is the binary choice coded as 0 or 1, and \(^{-1}(p)(p/(1-p))\) is the logit function (inverse of \(\) introduced in eq. (4)).

Notably, the choice-only estimator in eq. (6) aligns with the EZ-diffusion model's drift estimator (67, eq. (5)). Moreover, the estimators in Xiang Chiong et al. (73, eq. (6)) and Berlinghieri et al. (8, eq. (7)) combine elements of both estimators from eqs. (5) and (6). In section 5.2, we demonstrate that both estimators from Wagenmakers et al. (67, eq. (5)) and Xiang Chiong et al. (73, eq. (6)) are outperformed by our proposed estimator in eq. (3) for the full bandit problem.

Assuming the utility difference \(u_{x} 0\), the choice-decision-time estimator in eq. (5) satisfies the following non-asymptotic concentration bound, proven in appendix C.3.1:

**Theorem 3.3** (Non-asymptotic concentration of \(_{x,}\)).: _For each query \(x\) with \(u_{x} 0\), given a fixed i.i.d. dataset \(\{(c_{x,s_{x},i},t_{x,s_{x},i})\}_{i[n_{x}]}\), for any \(>0\) satisfying \(\{|u_{x}|/(a),(1+)a|u_{x}|/ [t_{x}]\}\), the following holds:_

\[(|_{x,}-}{a}|> ) 4(-[m_{}^{}(x^{ }^{*})]^{2}\,n_{x}\,[ a]^{2} ),\]

_where \(m_{}^{}(x^{}^{*}) [t_{x}]/\,[(2+2)a]\)._

Figure 2: This figure illustrates key terms from our theoretical analyses, highlighting the different contributions of choices and decision times to utility estimation. These terms are functions of the utility difference \(x^{}^{*}\) and are plotted for two barrier values, \(a\). (a) compares the weights \([t_{x}]\) and \(a^{2}\,[c_{x}]\) in the asymptotic variances for the choice-decision-time estimator (orange, theorem 3.1) and the choice-only estimator (gray, theorem 3.2), respectively. This comparison shows that _decision times complement choices, particularly for queries with strong preferences_. (b) compares the weights in the non-asymptotic concentration bounds (theorem 3.3 and 3.4), showing similar trends, though these weights may not be optimal due to proof techniques.

In contrast, the choice-only estimator in eq. (6) has the following non-asymptotic concentration result, adapted from Jun et al. [31, theorem 5]2:

**Theorem 3.4** (Non-asymptotic concentration of \(_{x,}\)).: _For each query \(x\), given a fixed i.i.d. dataset \(\{c_{x,s_{i,t}}\}_{i[n_{x}]}\), for any positive \(<0.3\), if \(n_{x} 1/(2au_{x})\{3^{2}(6e)/^{2},64(3)/(1- ^{2}/0.3^{2})\}\), the following holds:_

\[(|_{x,}-2au_{x}|>)  6(-[m_{}^{}(x^{}^{*} )]^{2}\,n_{x}\,[/(2a)]^{2}),\]

_where \(m_{}^{}(x^{}^{*}):=a\,[c_{x}]}\,/\,2.4\)._

The weights \(m_{}^{}()\) and \(m_{}^{}()\) from theorems 3.3 and 3.4, respectively, are functions of the utility difference \(x^{}^{*}\) for a fixed barrier \(a\). These weights determine how quickly estimation errors decay as the dataset size \(n_{x}\) grows, with larger weights indicating faster error reduction. While these weights may not be optimal due to proof techniques, they highlight the distinct contributions of choices and decision times, consistent with our asymptotic analysis in section 3.2. Figure 1(b) compares the weights for the choice-decision-time estimator (orange, \(m_{}^{}()\)) and the choice-only estimator (gray, \(m_{}^{}()\)). For strong preferences, the choice-only weights quickly decay to zero, while the choice-decision-time weights remain relatively large. This supports our key insight that decision times complement choices and improve estimation for queries with strong preferences.

In summary, both asymptotic (section 3.2) and non-asymptotic (section 3.3) analyses demonstrate that the choice-decision-time estimator extracts more information from queries with strong preferences. This finding aligns with prior empirical work  and is further supported by our results in section 5.1.

In fixed-budget best-arm identification, our choice-decision-time estimator's ability to extract more information from queries with strong preferences is especially valuable. Bandit learners, such as GSE , strategically sample queries, update estimates of \(^{*}\), and eliminate lower-utility arms. With the choice-only estimator, learners struggle to extract information from queries with strong preferences. To resolve this, one approach is to selectively sample queries with weak preferences, but this has two drawbacks. First, queries with weak preferences take longer to answer (i.e., require more resources), potentially lowering the 'bang per buck' (information per resource) . Second, since \(^{*}\) is unknown in advance, learners cannot reliably target queries with weak preferences. In contrast, with our choice-decision-time estimator, learners leverage decision times to gain more information from queries with strong preferences, improving bandit learning performance. We integrate both estimators into bandit learning in section 4 and evaluate their performance in section 5.

## 4 Interactive learning algorithm

We introduce the Generalized Successive Elimination (GSE) algorithm  for fixed-budget best-arm identification in preference-based linear bandits, and outline the key options for each GSE component, which we empirically compare in section 5.

The pseudo-code for GSE is shown in algorithm 1. The algorithm uses a hyperparameter \(\) to control the number of phases, the budget per phase, and the number of arms eliminated in each phase. GSE divides the total budget \(B\) evenly across phases and reserves a buffer, sized by another hyperparameter \(B_{}\), to prevent overspending in any phase (line 4). In each phase, GSE computes an experimental design \(\), a probability distribution over the query space, to guide query sampling. We consider two designs: the transductive design , \(_{}\) (line 5), and the weak-preference design , \(_{}\) (line 6). Both designs minimize the worst-case variance of utility differences between surviving arms. The transductive design weights all queries equally, whereas the weak-preference design prioritizes queries with weak preferences to counter the choice-only estimator's difficulty in extracting information from queries with strong preferences (section 3). Since \(^{*}\) is unknown, the weak-preference design identifies queries with weak preferences based on the previous phase's estimate \(_{}\). Then, GSE samples queries based on the design (line 7) and, after exhausting the phase's budget, estimates \(^{*}\) using either the choice-decision-time estimator \(_{}\) (line 8) or the choice-only estimator \(_{}\) (line 9). It then eliminates arms with low estimated utilities (line 10). This process repeats until only one arm remains, which GSE recommends as the best arm (line 12).

The key difference between algorithm 1 and previous GSE algorithms  is that our setting involves queries with random response times, unknown to the learner. Previous work assumes fixed resource consumption per query and uses deterministic rounding methods  to pre-allocate queries. This approach does not handle random resource usage. Instead, we adopt a random sampling procedure  in line 7 to allocate queries based on the design. Random resource usage also requires tuning the elimination parameter \(\), to balance data collection and arm elimination, and the buffer size \(B_{}\), to prevent overspending. In our empirical study (section 5.2), we manually tune both parameters. Further theoretical analysis is needed to better understand and optimize them.

```
1:Input: Arm space \(\), query space \(\), non-decision time \(t_{}\), and total budget \(B\).
2:Hyperparameters: Elimination parameter \(\) and buffer size \(B_{}\).
3:Initialization:\(_{1}\).
4:for each phase \(k=1,,K_{}||\) with the budget \(B_{k} B/K-B_{}\)do
5: Design 1. \(_{k}_{,k}_{ ^{||}}_{z z^{}_{k} }\|z-z^{}\|_{(_{x}_{x}xx^{})^{ -1}}^{2}\).
6: Design 2. \(_{k}_{,k}_{ ^{||}}_{z z^{}_{k} }\|z-z^{}\|_{(_{x}(x^{}_{k-1})\,_{x}xx^{})^{-1}}^{2}\).
7: Sample queries \(x_{j}_{k}\) and stop at \(J_{k}\) if \(_{j=1}^{J_{k}-1}t_{,x_{j},j} B_{k}\) and \(_{j=1}^{J_{k}-1}t_{,x_{j},j}>B_{k}\).
8: Estimate 1. \(_{k}_{,,k}\) apply eq. (3) to all the \(J_{k}\) samples.
9: Estimate 2. \(_{k}_{,k}\) apply eq. (4) to all the \(J_{k}\) samples.
10: Update \(_{k+1}_{k}|}{ }\) arms in \(_{k}\), ranked by the estimated utility \(z^{}_{k}\).
11:endfor
12:Output: the single one \(_{K+1}\). ```

**Algorithm 1** Generalized Successive Elimination (GSE) 

## 5 Empirical results

This section empirically compares the GSE variations introduced in section 4: (1) \((_{},_{,})\): Transductive design with choice-decision-time estimator. (2) \((_{},_{})\): Transductive design with choice-only estimator. (3) \((_{},_{})\): Weak-preference design with choice-only estimator.

### Estimation performance on synthetic data

We evaluate the estimation performance of the GSE variations on the "sphere" synthetic problem, a standard linear bandit problem in the literature . Details are provided in appendix D.1.

Estimation performance, as discussed in section 3, depends on the utility difference \(x^{}^{*}\) and the barrier \(a\). We vary \(a\) over a range of values commonly used in psychology . To examine how preference strength impacts estimation, we scale each arm \(z\) to \(c_{} z\), effectively scaling each utility difference \(x^{}^{*}\) to \(c_{} x^{}^{*}\). Small \(c_{}\) values correspond to problems with weak preferences, while large values correspond to strong preferences. For each \((c_{},a)\) pair, the system generates \(100\) random problem instances and runs \(100\) repeated simulations per instance. In each simulation, the GSE variations sample \(50\) queries, ignoring the response time budget, and compute \(\). Performance is evaluated by \([_{z}z^{} z^{*}]\), which reflects the best-arm identification goal defined in section 2. To isolate the effect of estimation, we allow \(_{}\) access to the true \(^{*}\), enabling it to perfectly compute the terms \((x^{}^{*})\) used in line 6 of algorithm 1.

As shown in fig. 2(a), fixing the barrier \(a\) and examining the vertical line, as \(c_{}\) increases and preferences become stronger, the performance of the choice-only estimator with the transductive design first improves and then declines. The initial improvement arises because larger \(c_{}\) increases utility differences between the best arm and others, theoretically simplifying best-arm identification. The subsequent decline, highlighted by the dark curved band, supports our insight from section 3 that choices from queries with strong preferences provide limited information. Fixing \(c_{}\) and examining the horizontal line, performance first improves and then declines. This trend aligns with fig. 2(a) and section 3.2, where higher barriers \(a\) increase the choice-only weights for queries with weak preferences, initially improving performance. However, as \(a\) grows, fewer queries exhibit increased weights, while most queries' weights decrease, leading to the later performance drop.

In Figure 2(b), for moderate \(c_{}\), the choice-only estimator with the weak-preference design outperforms the transductive design (fig. 2(a)), demonstrating that focusing on queries with weak preferences improves estimation. However, as \(c_{}\) becomes too large, performance declines because many \((x^{}^{*})\) in line 6 of algorithm 1 approach zero, preventing informative queries from being sampled. This advantage of the weak-preference design assumes perfect knowledge of \(^{*}\) and equal resource consumption across queries. In practice, where \(^{*}\) is unknown and weak-preference queries require longer response times, the transductive design performs better, as shown in section 5.2.

Figure 2(c) shows that the choice-decision-time estimator consistently outperforms the choice-only estimators under both the transductive and weak-preference designs, particularly for strong preferences. This suggests that for queries with strong preferences, decision times complement choices and improve estimation, confirming our theoretical insights from section 3, while for queries with weak preferences, decision times add little value but do not degrade performance. The performance also improves with a higher barrier \(a\), supporting the insights conveyed by fig. 1(a) and section 3.2.

### Fixed-budget best-arm identification performance on real datasets

This section compares the bandit performance of six GSE variations. The first three are as previously defined at the beginning of section 5: \((_{},_{})\), \((_{},_{})\), and \((_{},_{})\).

The 4th GSE variation, \((_{},_{})\), evaluates the performance of the choice-decision-time estimator when the non-decision time \(t_{}\) is unknown. The estimator, \(_{}\), is identical to the original choice-decision-time estimator from Eq. (3), but with response times used in place of decision times.

The 5th GSE variation, \((_{},_{})\), is based on Wagenmakers et al. (67, eq. (5)), which states that \(x^{}(2a^{*})=^{-1}([c_{x}=1])\), where \(^{-1}(p)(p/(1-p))\). By incorporating our linear utility structure, we obtain the following choice-only estimator \(_{}\):

\[_{}(_{x_{ }}n_{x}\;xx^{})^{-1}_{x_{}}n_{x}\;x^{-1}(}_{x}),\]

where \(}_{x}}_{i=1}^{n_{x}}(c_{x,s_{x,i}}+1)\) is the empirical mean of the binary choices coded as 0 or 1.

Figure 3: Three heatmaps show estimation error probabilities, \([_{z}z^{} z^{*}]\), for three GSE variations, shown as functions of the arm scaling factor \(c_{}\) and barrier \(a\). Darker colors indicate better estimation. (a) The choice-only estimator \(_{}\) with the transductive design \(_{}\) struggles as \(c_{}\) increases (i.e., preferences become stronger), highlighting that choices from queries with strong preferences provide limited information. (b) The weak-preference design \(_{}\) improves (a) by sampling queries with weak preferences but assumes perfect knowledge of \(^{*}\) and equal resource consumption across queries. (c) The choice-decision-time estimator \(_{}\) with \(_{}\) outperforms both choice-only methods in (a) and (b), showing that decision times complement choices and improve estimation, especially for strong preferences.

The 6th GSE variation, \((_{},_{})\), is based on Xiang Chiong et al. (73, eq. (6)), which states that \(x^{}^{*}=(c_{x})[c_{x} ]/[t_{x}]} 0.5\;^{-1}([c_{x }=1])\). This identity forms the foundation of the estimator in Berlinghieri et al. (8, eq. (7)). By incorporating our linear utility structure, we obtain the following choice-decision-time estimator \(_{}\):

\[_{}(_{x_{ }}n_{x}\;xx^{})^{-1}_{x_{}}n_{x}\;x(c_{x}) [c_{x}]}{[t_{x}]}\;^{-1} (}_{x})}.\]

We evaluate six GSE variations on bandit instances constructed from three real-world datasets of human choices and response times. Each dataset includes multiple participants. For each participant, we estimated dEZDM parameters, built a bandit instance, and simulated the GSE variations to assess performance. Details on experimental procedures are provided in appendix D. Key results for the three domains are shown in fig. 4, with full results in appendix D. First, \((_{},_{})\) consistently outperforms \((_{},_{})\), demonstrating the benefit of incorporating decision times. Second, both of these variations outperform \((_{},_{})\), as discussed in section 5.1. Third, \((_{},_{})\) performs similarly to \((_{},_{})\), suggesting that not knowing the non-decision time has minimal impact. Finally, \((_{},_{})\) and \((_{},_{})\) do not perform as consistently well as \((_{},_{})\), highlighting the effectiveness of our proposed choice-decision-time estimator (eq. (3)).

## 6 Conclusion and future work

This work is the first to leverages human response times to improve fixed-budget best-arm identification in preference-based linear bandits. We proposed a utility estimator that combines choices and response times. Both theoretical and empirical analyses show that response times provide complementary information about preference strength, particularly for queries with strong preferences, enhancing estimation performance. When integrated into a bandit algorithm, incorporating response times consistently improved results across three real-world datasets.

One limitation of this approach is its reliance on reliable response time data, which may be challenging in crowdsourcing settings where participants' focus can vary . Future work could integrate eye-tracking data into the DDM framework  to monitor attention and filter unreliable responses. Another direction is to relax the assumption of known non-decision times by estimating them directly from data, following methods proposed by Wagenmakers et al. .

Figure 4: This figure shows violin plots (with overlaid box plots) for datasets (a), (b), and (c), showing the distribution of best-arm identification error probabilities, \([ z^{*}]\), for all bandit instances across six GSE variations and two budgets. The box plots follow the convention of the matplotlib Python package. For each GSE variation and budget, the horizontal line in the middle of the box represents the median of the error probabilities across all bandit instances. Each error probability is averaged over \(300\) repeated simulations under different random seeds. The box’s upper and lower borders represent the third and first quartiles, respectively, with whiskers extending to the farthest points within \(1.5\) the interquartile range. Flier points indicate outliers beyond the whiskers.