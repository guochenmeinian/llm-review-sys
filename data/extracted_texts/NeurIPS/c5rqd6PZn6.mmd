# Q: For what purpose was the dataset created?

BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting

 Patrick Emami, Abhijeet Sahu, Peter Graf

National Renewable Energy Lab

{Patrick.Emami, Abhijeet.Sahu, Peter.Graf}@nrel.gov

###### Abstract

Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-fine-tune paradigm for STLF. To help address this, we present BuildingsBench, which consists of: 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock; and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly well to real commercial buildings. An exploration of the effect of increasing dataset size and diversity on zero-shot commercial building performance reveals a power-law with diminishing returns. We also show that fine-tuning pretrained models on real commercial and residential buildings improves performance for a majority of target buildings. We hope that BuildingsBench encourages and facilitates future research on generalizable STLF. All datasets and code can be accessed from https://github.com/NREL/BuildingsBench.

## 1 Introduction

Residential and commercial buildings in the United States are responsible for about 40% of energy consumption and 35% of greenhouse gas emissions . Globally, these estimates are respectively 30% and 27% . Building energy demand forecasting plays a part in reducing global emissions by helping to decarbonize the building sector.

Short-term load forecasting (STLF), which typically ranges from hour-ahead to a few days ahead, plays a multitude of critical roles. STLF can help match shifting energy supplies with customer demand, as well as aid energy markets with accurately setting prices based on forecasted supply/demand . Accurate forecasts can be directly used by reinforcement learning  and model predictive control [1; 9] for optimal building energy management.

However, STLF remains a challenging problem, as energy demand can fluctuate heavily due to a variety of unobserved and exogenous factors. As such, data-driven methods have risen in popularity to address STLF [2; 52]. Interest in these techniques has also been spurred by a rise in deployments of advanced sensors (i.e., smart meters) that record building energy consumption. However, the number of instrumented buildings with publicly released data remains low. Moreover, it is typical for multiple years of historical data to be reserved for training and validation, with trained models tested on a single held-out year _for the same building_. This incurs a lengthy data collection period per building that is not scalable. There is thus a lack of sufficiently large publicly available datasets (Table 1), which has hindered the investigation of large-scale pretraining (i.e., one model trained on every building ).

In this work, we introduce BuildingsBench, an open-source platform for large-scale pretraining and for benchmarking zero-shot STLF  and transfer learning for STLF . In zero-shot STLF models forecast loads for unseen target buildings _without any fine-tuning_. This drastically reduces time-to-deployment on newly instrumented buildings. In transfer learning, a pretrained model is fine-tuned on a target building, assuming a limited yet realistic amount of data (e.g., 6 months).

As part of BuildingsBench, we introduce Buildings-900K, a dataset of nearly one million _simulated_ time series, which approaches the scale of datasets in natural language processing and computer vision. This data is sourced from the NREL End-Use Load Profiles (EULP) database . The EULP is a multiyear, multi-institution effort to create a statistically representative database of the entire U.S. building stock's energy consumption via careful calibration and validation of physics-based building simulations. We also provide an evaluation suite that combines 7 open datasets totalling over 1,900 _real_ buildings (Table 2). Example time series from the simulated and real datasets are shown in Figure 1. Compared to other existing datasets (Table 1, Table 2), our large-scale pretraining and evaluation data contains both simulated and real building energy consumption time series spanning a wider range of geographic locations, years, and types of both residential _and_ commercial buildings.

Our platform automates the evaluation of a variety of simple and advanced baselines on the two tasks. Our results on zero-shot STLF reveal that synthetically pretrained models achieve strong performance on real commercial buildings. We observe a smaller distribution shift between simulated

Figure 1: **BuildingsBench gallery**. Top row: commercial buildings (farthest left is simulated commercial Buildings-900K data). Second and third rows are residential buildings (farthest left in the second row is simulated residential Buildings-900K data).

and real commercial buildings than residential buildings. We also show that pretrained models can further improve performance by fine-tuning on real commercial and residential building data. Buildings-900K also enables studying large-scale pretraining of transformers on geographical time series. The utility of transformers for forecasting has recently been questioned , but a lack of sufficiently large public time series datasets has made investigating this challenging. Our main finding is a power-law scaling with diminishing returns between dataset scale (size and diversity) and generalization (for commercial buildings). We expect that BuildingsBench will facilitate research on new modeling techniques and large-scale datasets for generalizable STLF.

To summarize, we contribute: 1) Buildings-900K, a simulated dataset for large-scale pretraining, 2) a platform for benchmarking zero-shot STLF and transfer learning, and 3) valuable insights on model pretraining for STLF.

## 2 Short-Term Load Forecasting

The BuildingsBench benchmark considers the following univariate forecasting problem. Given \(H\) past load values \(x_{t-H:t}\) and \(H+T\) covariates \(z_{t-H:t+T}\) (defined in Section 3.1), we aim to predict the conditional distribution for \(T\) unobserved future load values \(y_{t+1:t+T}\):

\[p(y_{t+1},,y_{t+T} x_{t-H},,x_{t},z_{t-H},,z_{t+T}).\] (1)

We consider a day-ahead STLF scenario with \(H=168\) hours (one week) and \(T=24\) hours. A primary goal of BuildingsBench is to study and evaluate STLF models, which learn a single set of parameters \(\) shared by all buildings for the distribution in Eq. 1. We use a probabilistic formulation for our benchmark, as applications of STLF increasingly require uncertainty estimates, such as planning and scheduling of renewable energy sources for buildings .

## 3 The Buildings-900K Dataset

In this section, we introduce our dataset for pretraining STLF models.

**Simulated data source:** Our dataset is sourced from the NREL EULP database . The EULP provides 15-minute resolution appliance-level consumption simulated with EnergyPlus [8; 29] for a base set of 900K building models (550K residential and 350K commercial) spread across all climate regions in the U.S. It aims to provide a statistically representative picture of the entire U.S. buildings stock at various aggregation levels (county, state, etc.) and under various electrification scenarios. The building simulations were extensively calibrated over a three-year period with advanced metering data (\(\) 2.3 million meters), data from 11 utilities, as well as other public/private datasets related to energy usage. Socio-economic building characteristics, including location based on Public Use Microdata Area (PUMA, \(\)2400 PUMAs in the U.S.--see inset), are sampled from distributions generated from U.S. Census survey responses. Please see Wilson et al.  for the complete description or App. B.3 for more details.

**Processing and storage:** To create Buildings-900K, we extract 900K total energy consumption time series (in kilowatt-hours (kWh)) from each of the non-upgraded buildings in the 2021 version of the EULP. To promote accessibility of our dataset, we also aggregate the 15-minute resolution to hourly to reduce the size. This data requires about 110 GB to store, significantly less than the entire EULP (70+ TB). We store all buildings within each PUMA in a single Parquet file by year (there are two years, 2018 and an aggregated "typical meteorological year" (TMY) ) and by building type (residential/commercial), which amounts to 9,600 Parquet files. Each file has a column for the timestamp and a column for each building's energy consumption (8,760 rows per file). Processing the EULP to extract this data took \(\)3 days with Apache PySpark on a 96-core AWS cloud instance.

**Pretraining splits and loading:** A validation set is created by withholding the final two weeks of 2018. The test set consists of buildings in four PUMAs that are withheld from both training and validation. All splits use a 24-hour sliding window to extract 192-hour load sub-sequences. Because shuffling large datasets of sub-sequences is computationally demanding, our platform provides a custom PyTorch  Dataset that creates an index file to map a shuffled list of integers to a subsequence. Each line of the index file is accessible in \(O(1)\) time with Python's seek function. Apache PyArrow is used to efficiently load the indexed building's time series into memory.

**Hosting and licensing:** Buildings-900K is hosted by the Open Energy Data Initiative and is available for download with a permissive CC-4.0 license (link available in App. A).

### Feature Extraction

Beyond the load time series, we also extract these covariates on-the-fly to condition forecasts on:

* Calendar features--day of the week, day of the year, and hour of the day--are generated from the timestamp, which we then cyclically encode with sine and cosine.
* The latitude and longitude coordinate of the centroid of the building's PUMA (using metadata provided by the EULP) for encoding geospatial information.
* A binary feature for building type (residential (0) or commercial (1)).

## 4 BuildingsBench Evaluation Platform

BuildingsBench provides an open-source software platform for evaluating models on zero-shot and transfer learning for STLF using a collection of _real building datasets_. To avoid confusion, in our analysis (Sec. 5), we will use the name BuildingsBench to refer _only to the real building evaluation data_ and explicitly state whether the results are for Buildings-900K-Test (_simulated_) or for BuildingsBench (_real_) (Table 2). We now describe each task in more detail.

### Real Building Datasets

Here, we briefly describe the real building data used for evaluating the two tasks and defer additional details to App. C. Altogether, our real building benchmark has over 1,900 buildings and 1.2M days of energy usage.

**Electricity**: 370 commercial buildings in Portugal with data spanning 2011-2014.

   & \# buildings & Open access & Residential & Commercial & Total hours & \# Sites \\  Pecan Street  & 1,000 & ✗ & ✓ & ✗ &? &? \\ Electricity  & 370 & ✓ & ✗ & ✓ & \(\)12.9M & 1 \\ Building D.G.P. 2  & 1,636 & ✓ & ✗ & ✓ & \(\)53.6M & 19 \\ Low Carbon London  & 5,567 & ✓ & ✓ & ✗ & \(\)97.5M & 1 \\ Buildings-900K (_simulated_) & **900,000** & ✓ & ✓ & ✓ & \(\)**15B** & **2400** \\  

Table 1: Comparing popular building energy consumption datasets to our dataset Buildings-900K. Our dataset has significantly more buildings (residential and commercial) located across a wide geographic area and spans multiple years, which enables studying large-scale pretraining for STLF.

   & Residential & Commercial & Years spanned & \# Sites & Total days \\  Buildings-900K-Test (_simulated_) & 915 & 565 & TMY, 2018 & 4 & 1.1M \\ BuildingsBench (_real_) & 953 & 970 & 2007–2018 & 10 & 1.2M \\  

Table 2: BuildingsBench STLF evaluation data.

**Building Data Genome (BDG) Project 2**: 1,636 commercial buildings across 19 sites in 2016 and 2017. We include buildings from 4 U.S. sites (Panther, Fox, Bear, Rat).
**Low Carbon London**: Energy consumption meter readings from 5,567 London, UK households between 2011-2014. To keep the overall number of residential and commercial buildings roughly the same, we keep a random sample of 713 buildings.
**SMART**: Meter readings for 7 homes in western Massachusetts, U.S., between 2014-2016.
**IDEAL**: Electricity meter data from 255 homes in Edinburgh, UK, between 2016-2018.
**Individual household electric power consumption**: Energy consumption from a single household in **Sceaux**, Paris, between 2007-2010.
**Borealis**: 6-second load measurements recorded for 30 homes in Waterloo, ON, in 2011-2012.

**Processing and storage:** We resample each time series to hourly if the data is sub-hourly. Smart meter time series typically have missing values, sometimes extending over weeks or months. Buildings with more than 10% of the data missing were not included. For included buildings, we linearly interpolate missing values, and if values are missing for a span greater than 1 week, we fill this with zeros. We also provide an option to exclude buildings with a max hourly consumption \(>\) 5.1 MW (only 15 from electricity) to keep the range of consumption values similar between Buildings-900K and BuildingsBench test data, as extrapolation is not our focus. A small fraction of spikes in the time series due to noisy meter readings are classified as outliers and filtered with a non-parametric distance-based sliding window algorithm  (see App. C.8). Each annual consumption time series per building is stored as a CSV file.

**Hosting and licensing:** We host the processed versions of each dataset, along with the original permissive licenses, alongside Buildings-900K under the same CC-4.0 license. Our code is open-sourced under a BSD-3 license.

### Evaluation Metrics

We primarily evaluate task performance with two metrics, the normalized root mean square error (NRMSE) and the ranked probability score (RPS). The NRMSE (also known as the coefficient of variation of the RMSE) is widely used, as it captures the ability to predict the correct load shape. For a target building with \(M\) days of load time series,

\[NRMSE:=100}_{j=1,i=1}^{M,24}(y_{i,j}-_{i,j})^{2}},\] (2)

where \(\) is the predicted load, \(y\) is the actual load, and \(\) is the average actual load over all \(M\) days. In the appendix, we also report the normalized mean absolute error and normalized mean bias error (see descriptions in App. D).

The RPS is a well-known metric for uncertainty quantification in probabilistic forecasting . It compares the squared error between two cumulative distribution functions (CDFs), the predicted CDF and the observation represented as a CDF. Define the indicator function \(_{x y}\) as 1 if the actual load \(x\) is \( y\) and 0 otherwise. The continuous RPS for a predicted CDF \(\) for the load at hour \(i\) is:

\[RPS:=_{0}^{}(_{i}(y)-_{y_{i} y})^{2}dy.\] (3)

Our platform implements the closed-form RPS for Gaussian CDFs, as well as a discrete RPS for categorical distributions (used by baselines that discretize the positive real line for token-based load forecasting, see Sec. 4.3). These are formally defined in App. D.

### Baselines

Various baselines are implemented and benchmarked in the BuildingsBench platform. For zero-shot STLF, we pretrain a representative time series transformer  on Buildings-900K. Transformers have recently gained significant interest for STLF [17; 21; 36; 50]. The model is described in brief here and with more detail in App. E.

**Transformer (Gaussian)**: This model is the original encoder-decoder transformer  repurposed for autoregressive time series forecasting, as proposed in Wu et al. . It predicts a Gaussian distribution at each time step \(t+i\) conditioned on the past week and the previous \(i-1\) predictions. We train it to minimize the Gaussian negative log-likelihood loss. Because our demand time series are highly non-Gaussian (periods of low consumption interspersed with bursts), the loss diverges during training when we use global standard scaling to normalize the data. The Box-Cox power transformation  removes this instability. To compute the RPS, we approximate a Gaussian in the unscaled space by backprojecting the scaled standard deviation (see App. D for details).

**Transformer (Tokens)**: We also implement a transformer variant that uses _tokenization_ to predict discrete load tokens. This baseline allows us to test whether quantizing loads into tokens is beneficial for large-scale pretraining. We also found that a comparison between tokenization and Gaussian time series transformers was missing from the literature. The model is trained to predict a categorical distribution over a vocabulary of load tokens by minimizing a multi-class cross-entropy loss. To quantize loads, we use a simple strategy--faiiss-gpu  KMeans clustering with \(K\) = 8,192 fit to the Buildings-900K training set. We merge clusters \(<\)10 Watts apart to obtain a compact vocabulary size of 3,747 tokens. See App. E for analysis on our tokenizer design.

Each transformer is pretrained jointly on residential and commercial buildings at three different sizes: **Transformer-S** (3M params), **Transformer-M** (17M params), and **Transformer-L** (160M params). Models are trained on 1 billion load hours with early stopping. See App. E for more architecture details, App. F for hyperparameter tuning details (for learning rate and batch size), and App. G for training compute requirements.

We also benchmark **persistence** forecasting on both the zero-shot STLF and transfer learning tasks:

**Previous Day:**: For scenarios where the load conditions change relatively slowly, the previous day's load is a strong baseline for day-ahead STLF .
**Previous Week:**: The 24-hour load profile on the same day from the previous week is used .
**Ensemble:**: This persistence baseline computes a Gaussian distribution for each predicted hour \(t+i\) whose mean is the average load at hour \(t+i\) over the past 7 days:

\[=_{j=1}^{7}x_{(t+i-24j)}; p(y_{t+i}|x_{t-H:t}):= ,_{j=1}^{7}(x_{(t+i-24j)}- )^{2}}.\] (4)

In STLF, outperforming persistence is an indicator that a model is producing meaningful forecasts. On the transfer learning task, we also benchmark the following forecasting baselines by training them directly on the target building's 6 months of meter data with supervised learning:

**LightGBM:**: The light gradient-boosting machine (LightGBM)  is a popular decision-tree-based algorithm suitable for STLF . We use the multi-step forecasting implementation from skforecast  with 100 estimators and no max depth.
**Linear regression, DLinear:**: Inspired by Zeng et al. , we benchmark a linear _direct_ multi-step forecaster that regresses all 24 future values as a weighted sum of the past 168 values. We also implement DLinear, which decomposes the time series with a moving average kernel, applies a linear layer to each component, and then sums the two to get the final prediction.
**RNN (Gaussian):**: This is an autoregressive encoder-decoder recurrent neural network inspired by Salinas et al. . A multi-layer LSTM  first encodes the 168 past load values. The last encoder hidden state initializes the state of a multi-layer LSTM decoder. A linear layer maps each output of the decoder to Gaussian parameters.

## 5 Benchmark Results

Here, we analyze our baselines on the two benchmark tasks guided by the following questions:

1. Can models pretrained on Buildings-900K generalize to real buildings?
2. Does fine-tuning a pretrained model on limited data from a target building lead to improved performance?3. How does the number of pretraining buildings affect zero-shot generalization?
4. How does the size of the pretrained model affect generalization?

### Zero-Shot STLF

For zero-shot STLF, models are evaluated on day-ahead forecasting on all unseen simulated and real buildings _without fine-tuning_. Results for Transformer-L and other baselines are shown in Table 3 aggregated by the median over all buildings. Due to space constraints, Transformer-S and M results are shown in Figs. 2(c)-2(d) and App. Figs. 6-8. Performance profiles over all buildings (for comparing models by examining the tails of the forecast error distribution) and per-dataset metrics with 95% stratified bootstrap CIs are reported in App. H and I.

On unseen simulated buildings, the pretrained transformers outperform the persistence baselines in accuracy and uncertainty quantification. The average zero-shot STLF accuracy slightly improves **from simulated to real** commercial buildings (-0.1% NRMSE) and drops for residential buildings (+43% NRMSE). RPS is sensitive to the load magnitude, which makes it difficult to compute a sim-to-real gap for uncertainty quantification. See Fig. 2 for qualitative visualizations of the forecast

    &  &  \\   & NRMSE (\(\%\)) & RPS & NRMSE (\(\%\)) & RPS \\ 
**Not pretrained + Not fine-tuned** & & & & \\ Persistence Ensemble & 16.80 & 5.97 & 78.54 & **0.057** \\ Previous Day Persistence & 16.54 & - & 98.35 & - \\ Previous Week Persistence & 18.93 & - & 100.20 & - \\ 
**Not pretrained + Fine-tuned** & & & & \\ Linear regression & 25.18 & - & 89.98 & - \\ DLinear & 23.41 & - & 87.89 & - \\ RNN (Gaussian) & 41.79 & 15.28 & 96.75 & 0.078 \\ LightGBM & 16.02 & - & 80.07 & - \\ Transformer-L (Tokens) & 50.12 & 26.89 & 105.65 & 16.36 \\ Transformer-L (Gaussian) & 37.21 & 15.94 & 92.99 & 0.081 \\ 
**Pretrained + Not fine-tuned** & & & & \\ Transformer-L (Tokens) & 14.20 & 5.11 & 94.11 & 0.140 \\ Transformer-L (Gaussian) & 13.03 & 4.47 & 79.43 & 0.062 \\ 
**Pretrained + Fine-tuned** & & & & \\ Transformer-L (Tokens) & 14.07 (-0.13) & 4.99 (-0.12) & 94.53 (+0.42) & 0.137 (-0.003) \\ Transformer-L (Gaussian) & **12.96 (-0.07)** & **4.37 (-0.10)** & **77.20 (-2.23)** & **0.057 (-0.015)** \\   

Table 4: **Transfer learning results**. We show median accuracy (NRMSE) and ranked probability score (RPS) with **best** and **second** best highlighted. Transformer-S and Transformer-M results are in Fig. 4. Improvement due to fine-tuning.

    &  &  \\   &  &  &  &  \\   & NRMSE (\%) & RPS & NRMSE (\%) & RPS & NRMSE (\%) & RPS & NRMSE (\%) & RPS \\  Persistence Ensemble & 33.10 & 4.58 & 54.77 & 0.72 & 16.68 & 5.88 & **77.88** & **0.063** \\ Previous Day Persistence & 34.91 & - & 59.38 & - & 16.96 & - & 98.41 & - \\ Previous Week Persistence & 32.05 & - & 74.08 & - & 19.39 & - & 99.77 & - \\  Transformer-L (Tokens) & **12.91** & 2.45 & 44.29 & 0.875 & 14.46 & 5.62 & 95.34 & 0.152 \\ Transformer-L (Gaussian) & 15.10 & **2.07** & **43.52** & **0.578** & **13.31** & **5.23** & 79.34 & 0.072 \\   

Table 3: **Zero-shot STLF results.** Median accuracy (NRMSE) and ranked probability score (RPS) with **best** and **second** best highlighted. Lower is better. No fine-tuning is performed on any test building. Residential NRMSEs are naturally larger than commercial buildings, because the normalization factor—the building’s average consumption per hour—is small). For example, Transformer-L (Gaussian) has an RMSE of 0.72 kWh and an NRMSE of 66.57% on the residential Sceaux dataset.

uncertainty. The best pretrained model Transformer (Gaussian) outperforms the best persistence method Persistence Ensemble on real commercial buildings. However, Persistence Ensemble has better accuracy than the pretrained models on real residential buildings. These results suggest synthetic pretraining is viable for commercial zero-shot STLF--see Sec. 6.2 for more discussion on residential buildings.

### Transfer Learning

This task evaluates fine-tuning on a single target building for which 6 months of data has been collected. Our fine-tuning protocol is to train for a max of 25 epochs using the first 5 months and to use the last month for early stopping with a patience of 2 epochs. To ease the computational burden of fine-tuning a model on each building in the benchmark, we randomly sample 100 residential and 100 commercial buildings and fine-tune separately on each. We fine-tune the pretrained transformers across all sizes and train randomly initialized transformers. Table 4 displays results aggregated by the median over all buildings; see Fig. 4 for the Transformer-S and Transformer-M results. We also report performance for the pretrained models without fine-tuning.

Our results indicate that fine-tuning the pretrained transformers on limited data is likely to improve the performance on the target building. For statistical robustness, we compute average probability of improving NRMSE due to fine-tuning: \(P(X<Y):=_{i=1}^{N}_{[X_{i}<Y_{i}]}\) where \(X_{i}\) is the pretrained + fine-tuned NRMSE for building \(i\) and \(Y_{i}\) is the pretrained NRMSE. The Transformer-M models have the highest \(P(X<Y)\), with the Gaussian model achieving 98% and 73% respectively for commercial and residential buildings and the Tokens model scoring 70% and 62.5% (Fig. 4). For Transformer-L (Gaussian), this drops to 71% and 68% and to 61.5% and 39% for Transformer-L (Tokens). We found that fine-tuning _all_ layers of both pretrained models was necessary--only fine-tuning the model's last layer led to slightly decreased performance (App. K). Encouragingly, the fine-tuned model (Transformer-L (Gaussian)) beats both LightGBM, a strong baseline in the low-data regime, and the Persistence Ensemble on both commercial and residential buildings.

### Empirical Scaling Laws

**Pretraining buildings:** To characterize how size and diversity of the dataset impacts generalization, we trained the Transformer-M (17M) models on 1K, 10K, and 100K simulated buildings. We use early stopping to prevent the models trained on smaller datasets from overfitting. The Transformer-M model accuracy (Fig. 2(a)) and RPS (Fig. 2(b)) on commercial buildings roughly follows a power-law scaling with diminishing returns. Performance remains roughly constant across dataset scales for residential buildings (see Fig. 7 in App. J), which we attribute to the stronger sim-to-real distribution shift compared to commercial buildings. This suggests naively increasing the dataset size may not result in large improvements. Nevertheless, the best performance is achieved when using the full pretraining dataset of 900K buildings.

**Model size:** Here we compare the transformers of sizes S, M, and L. The NRMSE and RPS for real commercial buildings improves from S to M, but performance plateaus or decreases from the M to L models (Fig. 2(c) and Fig. 2(d)). We suspect this is due to distribution shifts and auto-correlation in the

Figure 2: **Forecast uncertainty**. Ground truth time series are truncated to previous 24 hours for visibility. Light blue lines are 10 samples from the predicted distribution. a-b) Successful commercial building forecasts. c-d) Failed residential building forecasts.

time series that causes the largest models to overfit, despite using sliding windows to extract training samples and aggressive early stopping. The good RPS performance of the smallest Transformer (Tokens) model is likely due to training on quantized load values. Quantization helps generative models efficiently learn important structure in the data and ignore negligible information  (our tokenizer achieves a \(\)63% dataset compression rate). Although, this does come at a cost of lower accuracy (Fig. 3c). While we show power-law scaling for residential building forecast accuracy (Fig. 7), the model performance may be saturated; see App. J for an in-depth discussion. We also show the relationship between model size and transfer learning performance in Fig. 4. The Transformer-M models demonstrate the largest performance gains due to fine-tuning, which confirms the high average probability of improvement scores (Sec. 5.2). The smaller impact of fine-tuning on Transformer-L performance suggests its zero-shot performance may be nearly saturated.

## 6 Discussion

### Findings

**Pretraining on Buildings-900K leads to good zero-shot STLF performance on real commercial buildings**. More investigation is needed to achieve similar results for residential buildings (Sec. 6.2).

**Buildings-900K pretraining + fine-tuning on real buildings improves STLF for both commercial and residential buildings**. Our best pretrained + fine-tuned baseline outperforms LightGBM and persistence in the BuildingsBench transfer learning task. Fine-tuning all model layers appears necessary, possibly to mitigate negative transfer caused by the distribution shift between simulated pretraining and real test data.

Figure 4: **Model size vs. transfer learning**. Pretrained vs. pretrained + fine-tuned (FT) performance for S, M, and L transformers. Intervals are 95% stratified bootstrap CIs of the median. The Transformer-M models show the most improvement after fine-tuning. The fine-tuned Transformer-M performance is comparable to the largest models. Improvement due to fine-tuning is less pronounced for the Transformer-L models, suggesting their zero-shot performance is saturated on this task.

Figure 3: **Empirical scaling laws for zero-shot generalization on commercial buildings**. Intervals are 95% stratified bootstrap CIs for the median across all buildings. a-b) Dataset scale vs. zero-shot performance. The trends appear to be power-laws with diminishing returns. c-d) Model size vs. zero-shot performance. Residential results are in App. J.

We observe an approximate power-law relationship with diminishing returns between dataset scale and zero-shot STLF for commercial buildings. Performance plateaus as model size increases, possibly due to overfitting and distribution shifts.

**Pretraining on tokenized building loads instead of continuous loads achieves worse performance overall.** However, the tokenized model is more stable to train, as it naturally handles a wide range of load values.

**Pretraining with geospatial coordinates slightly improves generalization:** Due to space constraints, the results of an ablation where the model ignores the building's latitude and longitude are provided in App. K. Briefly, improvements in accuracy were modest (0.1 - 1% NRMSE).

### Residential STLF Challenges

Residential loads are inherently more uncertain and variable than commercial loads because they are more sensitive to occupant behaviour and changes in weather (e.g., temperature and humidity) . This is evidenced by the relatively large persistence NRMSEs (77.88%-99.77%) on BuildingsBench residential loads compared to commercial loads (16.68%-19.39%). However, rather than decreasing the value of BuildingsBench, our work enables exploring directions such as the inclusion of weather covariates, multi-variate formulations of STLF, and benchmarking of advanced approaches .

### Limitations

While we expect BuildingsBench to stimulate research on generalizable STLF, our framework has limitations. First, pretraining on simulated data is fundamentally limited if deploying on real buildings is the goal. Mixing synthetic and real data during pretraining is an interesting direction to explore. Second, the pretraining and evaluation data is mainly representative of building energy consumption in the northwestern hemisphere. Expanding our framework to include data from other global regions is needed to comprehensively evaluate generalization. Third, the stochastic occupancy model used to simulate residential consumption behavior for the pretraining data is more predictable and less chaotic than real behavior, which increases the sim-to-real gap for residential buildings. Moreover, the occupancy model does not capture behavior patterns that may appear prominently in a specific geographic region (particularly outside of the U.S.), which may hurt the generalization capabilities when deploying the model in these locations. Finally, due to limited time, we only pretrained vanilla transformers on Buildings-900K. We encourage future work that compares these results with state-of-the-art transformers [31; 46]. We will maintain a leaderboard for BuildingsBench in our code repository, which will be updated with new model results.

## 7 Conclusions and Future Work

In this work, we introduce BuildingsBench, which consists of a large-scale simulated dataset and a collection of real building datasets for benchmarking zero-shot STLF and transfer learning. Upon comparing the performance of pretrained transformers against persistence and traditional machine learning-based forecasting baselines, we observe promising results for commercial buildings and identify areas of improvement for residential buildings.

We plan to maintain and extend BuildingsBench in the following ways. As relevant EULP data becomes newly available, we will release updated versions of Buildings-900K. To ground improvements to benchmark tasks in a concrete downstream application, we will look into adding a reinforcement learning task to the benchmark for building control using STLF. Other promising directions for future work include exploring joint forecasting of weather and load and the impact of building metadata on performance. To facilitate this work, we plan to update the datasets with this auxiliary information. Overall, we hope that BuildingsBench will facilitate research for communities applying machine learning to the built environment, as well as those conducting foundational studies on large-scale pretraining and fine-tuning for time series.