# _SA-Solver_: Stochastic Adams Solver for Fast Sampling of Diffusion Models

Shuchen Xue\({}^{1,4}\), Mingyang Yi\({}^{2}\), Weijian Luo\({}^{3}\), Shifeng Zhang\({}^{2}\),Jiacheng Sun\({}^{2}\), Zhenguo Li\({}^{2}\), Zhi-Ming Ma\({}^{1,4}\)

\({}^{1}\)University of Chinese Academy of Sciences \({}^{2}\) Huawei Noah's Ark Lab \({}^{3}\) Peking University

\({}^{4}\)Academy of Mathematics and Systems Science, Chinese Academy of Sciences

Work done during an internship at Huawei Noah's Ark Lab. Email: xueshuchen17@mails.ucas.ac.cn, liuoweijian@stu.pku.edu.cn Corresponding authors: Mingyang Yi (yimingyang2@huawei.com)

###### Abstract

Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose _SA-Solver_, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that _SA-Solver_ achieves: 1) improved or comparable performance compared with the existing state-of-the-art (SOTA) sampling methods for few-step sampling; 2) SOTA FID on substantial benchmark datasets under a suitable number of function evaluations (NFEs).

## 1 Introduction

Diffusion Probabilistic Models (DPMs) [1; 2; 3] have demonstrated substantial success across a broad spectrum of generative tasks such as image synthesis [4; 5; 6], video generation [7; 8], text-to-image generation [9; 10; 11], speech synthesis [12; 13], _etc_. The primary mechanism of DPMs involves a forward diffusion process that incrementally introduces noise into data. Simultaneously, a reverse diffusion process is learned to generate data from this noise. Despite DPMs demonstrating enhanced generation performance in comparison to alternative methods such as Generative Adversarial Networks (GAN)  or Variational Autoencoders (VAE) , the sampling process of DPMs demand hundreds of evaluations of network function evaluations (NFE) . The substantial computation requirement poses a significant limitation to their wider application in practice.

The existing literature on improving the sampling efficacy of DPMs can be categorized into two ways, depending on whether conducting extra training on the DPMs. The first category necessitates supplementary training [16; 17; 18; 19; 20; 21], which often emerges as a bottleneck, thereby limiting their practical application. Due to this, we focus on exploring the second category, which consists training-free methods to improve the sampling efficiency of DPMs in this paper. Current training-free samplers employ efficient numerical schemes to solve the diffusion SDE/ODE[22; 23; 24; 25; 26]. Compared with solving diffusion SDE (stochastic sampler) [25; 26; 27], solving diffusion ODE (deterministic sampler) [22; 24; 23]empirically exhibits better sampling efficiency. Existing stochastic samplers typically exhibit slower convergence speed. However, empirical observations in [3; 27] indicate that the stochastic sampler has the potential to generate higher-quality data when increasing the sampling steps. This empirical observation motivates us to further explore the efficient stochastic sampler.

Owing to the observed superior performance of stochastic sampler [3; 27], we speculate that adding properly scaled noise in the diffusion SDE may facilitate the quality of generated data. Thus, instead of solving the vanilla diffusion SDE in , we propose to consider a family of diffusion SDEs which shares the same marginal distribution [28; 27] with different noise scales. Meanwhile, efficient stochastic solvers are not carefully studied, which could be the reason that diffusion ODE exhibits better sampling efficiency. To overcome this problem, we study the linear multi-step SDE solvers  and incorporate them in the sampling.

Based on these studies, we propose _SA-Solver_ with theoretical convergence order to solve the proposed diffusion SDEs. Our _SA-Solver_ is based on the stochastic Adams method in , by adapting it to the exponentially weighted integral and analytical variance. With the proposed diffusion SDEs and _SA-Solver_, we can efficiently generate data with controllable noise scales. We empirically evaluate our _SA-Solver_ on plenty of benchmark datasets of image generation. The evaluation criterion is the Frechet Inception Distance (FID) score  under different number of function evaluations (NFEs). The experimental results can be summarized as three folds: 1) Under small NFEs, our _SA-Solver_ has improved or comparable FID scores, compared with baseline methods; 2) Under suitable NFEs our _SA-Solver_ achieves the State-of-the-Art FID scores over all benchmark datasets; 3) _SA-Solver_ achieves superior performance over deterministic samplers when the model is not fully trained.

## 2 Related Works

The DPMs originate from the milestone work , and are further developed by  and  to successfully generate high-quality data, under the framework of discrete and continuous diffusion SDEs respectively. In this paper, we mainly focus on the latter framework. As mentioned in Section 1, plenty of papers are working on accelerating the sampling of DPMs due to their low efficiency, distinguished by whether conducting a supplementary training stage. Training-based methods, e.g., knowledge distillation [16; 17; 18], learning-to-sample , and integration with GANs [20; 21], have the potential to sampling for one or very few steps to enhance the efficiency, but their applicability is limited by the lack of a plug-and-play nature, thereby constraining their broad applicability across diverse tasks. Thus we mainly focus on the training-free methods in this paper.

Solving Diffusion ODE.Since the sampling process is equivalent to solving diffusion SDE (ODE), the training-free methods are mainly built on solving the differential equations via high-efficiency numerical methods. As ODEs are easier to solve compared with SDEs, the ODE sampler has attracted great attention. For example, Song et al.  provides an empirically efficient solver DDIM. Zhang and Chen  and Lu et al.  point out the semi-linear structure of diffusion ODEs, and develop higher-order ODE samplers based on it. Zhao et al.  further improve these samplers in terms of NFEs by integrating the mechanism of predictor-corrector method.

Solving Diffusion SDE.Though less explored than the ODE sampler, the SDE sampler exhibits the potential of generating higher-quality data . Thus developing an efficient SDE sampler as we did in this paper is a meaningful topic. In the existing literature, researchers [2; 26; 3] solve the diffusion SDE by first-order discretization numerical method. The higher-order stochastic sampler of diffusion SDE has also been discussed in . Karras et al.  proposes another stochastic sampler (which is not a general SDE numerical solver) tailored for diffusion problems. However, in contrast to our proposed _SA-Solver_, the existing SDE samplers are limited due to their low efficiency [2; 26; 3] or sensitivity to hyperparameters .

We found a concurrent paper proposing an SDE sampler SDE-DPM-Solver++  which is similar to our _SA-Solver_. Though both methods develop multi-step diffusion SDE samplers, our _SA-Solver_ is different from SDE-DPM-Solver++ as follows: 1) _SA-Solver_ incorporates the predictor-corrector method, which helps improve the quality of generated data [3; 32; 24]; 2) In contrast to SDE-DPM-Solver++, _SA-Solver_ has theoretical guarantees with proved convergence order; 3) SDE-DPM-Solver++ is a special case of _SA-Solver_ when the predictor step equals 2 with no corrector in our predictor-corrector method, while our solver supports arbitrary orders with analytical forms.

## 3 Preliminary

In the regime of the continuous stochastic differential equation (SDE), Diffusion Probabilistic Models (DPMs) [1; 2; 3; 33] construct noisy data through the following linear SDE:

\[_{t}=f(t)_{t}t+g(t)_{t},\] (1)

where \(_{t}^{d}\) represents the standard Wiener process, \(f(t)_{t}\) and \(g(t)\) respectively denote the drift and diffusion coefficients. For each time \(t[0,T]\), \(_{t}|_{0}(_{t}_{0},_{t}^{2})\).

Let \(p_{t}()\) denotes the marginal distribution of \(_{t}\), the coefficients \(f(t)\) and \(g(t)\) are meticulously selected to guarantee that the marginal distribution \(p_{T}(_{T})\) closely approximates a Gaussian distribution, i.e., \((,)\), and the _signal-to-noise-ratio_ (SNR) \(_{t}^{2}/_{t}^{2}\) is strictly decreasing w.r.t. \(t\). In the sequel, we follow the established notations in :

\[f(t)=_{t}}{t}, g^{2}(t)=_{t}^{2}}{t}-2_{t}}{ t}_{t}^{2}.\] (2)

Anderson  demonstrates a pivotal theorem that the forward process (1) has an equivalent reverse-time diffusion process (from \(T\) to \(0\)) as the following equation, so that generating process can be equivalent to numerically solve the diffusion SDE [2; 3].

\[_{t}=[f(t)_{t}-g^{2}(t)_{} p_{t}( _{t})]t+g(t)}_{t},_{T} p_{T}( _{T})\] (3)

where \(}_{t}\) represents the Wiener process in reverse time, and \(_{} p_{t}()\) is the score function.

Moreover, Song et al.  also prove that there exists a corresponding deterministic process whose trajectories share the same marginal probability densities \(p_{t}()\) as (3), so that the ODE solver can be adopted for efficient sampling [23; 24]:

\[_{t}=[f(t)_{t}-g^{2}(t)_{}  p_{t}(_{t})]t,_{T} p_{T}(_{T})\] (4)

To get the _score function_\(_{} p_{t}(_{t})\) in (3), we usually take neural network \(_{}(,t)\) parameterized by \(\) to approximate it by optimizing the denoising score matching loss :

\[^{*}=*{arg\,min}_{}_{t} (t)_{_{0}}_{_{1}|_{0}} _{}(,t)-_{_{t}} p_{0t}(_{ t}|_{0})_{2}^{2}}.\] (5)

In practice, two methods are used to reparameterize the score-based model . The first approach utilizes a _noise prediction model_ such that \(_{}(_{t},t)=-_{t}_{}(_{t},t)\), while the second employs a _data prediction model_, represented by \(_{}(_{t},t)=(_{t}-_{t}_{}(_{t},t))/_{t}\). The reparameterized models are plugged into the sampling process (3) or (4) according to their relationship with \(_{}(_{t},t)\).

## 4 Variance Controlled Diffusion SDEs

As mentioned in Section 1, most of the existing training-free efficient samplers are based on solving diffusion ODE (4), e.g., [23; 22; 24], because of their improved efficiency compared with the solvers of diffusion SDE (3). However, the empirical observations in [27; 22] exhibit that the quality of data generated by solving diffusion SDE outperforms diffusion ODE given sufficient computational budgets. For example, in , the diffusion ODE sampler DDIM  significantly improve the FID score of diffusion SDE sampler DDPM  (from 133.37 to 6.84) on CIFAR10 dataset  under 20 NFEs. However, under 1000 NFEs, the DDPM beats the DDIM in terms of FID score (3.17 v.s. 4.04). There may be a trade-off between stochasticity and efficiency. Thus, we conjecture that adding proper scale noise during the generating process may improve the quality of generated data with few NFEs.

In this section, we explore a family of variance-controlled diffusion SDEs, so that we can use proper noise scales during the sampling stage. Inspired by Proposition 1 in  and Eq. (6) in , we propose the following proposition to construct the aforementioned diffusion SDEs.

**Proposition 4.1**.: _For any bounded measurable function \((t):[0,T]\), the following Reverse SDEs_

\[_{t}=[f(t)_{t}-((t)}{2})g ^{2}(t)_{} p_{t}(_{t})]t+(t)g(t) }_{t},_{T} p_{T}(_{T})\] (6)

_share the same marginal probability distributions with (4) and (3)._The proof can be found in Appendix A.1. The proposition indicates that by solving any of the diffusion SDEs in (6), we can sample from the target distribution. It is worth noting that the magnitude of noise varies with \((t)\), and \((t)=0\) or \((t)=1\) respectively correspond to the diffusion ODE and SDE in . Thus we can control the magnitude of added noise during the sampling process by varying it.

In practice, we numerically solve the diffusion SDEs (6) by substituting score function \(_{} p_{t}(_{t})\) in it with the "data prediction reparameterization model" \(_{}(_{t},t)\) according to \(_{} p_{t}(_{t})-(_{t}-_{t}_{ }(_{t},t))/_{t}^{2}\) as pointed out in Section 3. Then diffusion SDEs to be solved become

\[d_{t}=[f(t)_{t}+((t)}{2_{t}}) g^{2}(t)(_{t}-_{t}_{}(_{t},t)}{ _{t}})]dt+(t)g(t)d}_{t}.\] (7)

**Remark 1**.: _We reparameterize the score function in diffusion SDEs (6) with data prediction model \(_{}(_{t},t)\) to get Eq. (9). The equation can be also reparameterized by the "noise prediction model" \(_{}(_{t},t)\) as discussed in Section 3. Though the obtained diffusion ODEs e.g., Eq. (9) are equivalent, the numerical solver applied to them will result in different solutions. For our proposed SA-Solver, we find the diffusion SDEs reparameterized data prediction model significantly improves the quality of generated data. More details and theoretical explanations are in Sec. 6 and Appendix A.2.4. For the remaining part of the paper, we focus on data reparameterization._

We then solve the diffusion SDEs (9) with change-of-variable applying to it, i.e., changing time variable \(t\) to log-SNR \(_{t}=(_{t}/_{t})\). Noting the following relationship in Eq. (2)

\[f(t)=_{t}}{t}, g^{2}(t)=_{t}^{2}}{t}-2_{t}}{ t}_{t}^{2}=-2_{t}^{2}_{t}}{ t},\] (8)

and plugging them into (7), it becomes

\[_{t}=[_{t}}{t}_ {t}-(1+^{2}(t))(_{t}-_{t}_{}(_{t},t)) _{t}}{t}]t+(t)_{t} _{t}}{t}}}_{t}.\] (9)

The above equation has an explicit solution owing to its semi-linear structure .

**Proposition 4.2**.: _Given \(_{s}\) for any time \(s>0\), the solution \(_{t}\) at time \(t[0,s]\) of (9) is_

\[_{t}=}{_{s}}e^{-_{_{s}}^{ _{t}}^{2}()}_{s}+ _{t}_{}(s,t)+_{t}(s,t),\] (10) \[_{}(s,t)=_{_{s}}^{_{t}}e^{- _{_{t}}^{_{t}}^{2}()}(1+^{2}())e^{}_{}(_{},)\] \[(s,t)=_{s}^{t}e^{-_{_{u}}^{_{t}}^{ 2}()}(u) _{u}}{u}}}_{u},\]

_where \((s,t)\) is an Ito integral  with the special property_

\[_{t}(s,t),_{t}^{2}(1-e^{-2_{ _{s}}^{_{t}}^{2}()} .\] (11)

The proof can be seen in Appendix A.2.2. With this proposition, we can sample from the diffusion model via numerically solving Eq. (10) starting from \(_{T}\) approximated by a Gaussian distribution.

## 5 _SA-Solver_: Stochastic Adams Method to Solve Diffusion SDEs

Stochastic training-free samplers for solving diffusion SDEs have not been studied as systematically as their deterministic ODE counterparts. This stems from the inherent challenges associated with designing numerical schemes for SDEs compared to ODEs . Existing stochastic sampling methods either use only variant of one-step discretization of diffusion SDEs [26; 3; 2], or are specifically designed sampling procedures for diffusion processes  which are not general purpose SDE solvers. Jolicoeur-Martineau _et al._ uses stochastic Improved Euler's method  with adaptive step sizes. However, it still necessitates hundreds of steps to yield a high-quality sample. As observed by , off-the-shelf SDE solvers are generally ill-suited for diffusion models, often exhibiting inferior qualities or even failing to converge. We postulate that the current dearth of fast stochastic samplers is principally due to factor that existing methodologies predominantly tend to rely on one-step discretization or its variants, or alternatively, on heuristic designs of stochastic samplers. To address this factor, we leverage advanced contemporary tools in numerical solutions for SDEs, specifically, _stochastic Adams methods_. It necessitates fewer evaluations compared to Stochastic Runge-Kutta schemes, making it a more suitable choice for problems which are computationally expensive - a characteristic that diffusion sampling certainly exemplifies.

Next, we formally present our Stochastic Adams Solver (_SA-Solver_). To solve Eq. (9), we first take \(M+1\) time steps \(\{t_{i}\}_{i=0}^{M}\) which is strictly decreased from \(t_{0}=T\) to \(t_{M}=0\).3 Then we can iteratively obtain the \(_{t_{i}}\) (so that \(_{0}\) approximates the required data) by the following relationship.

\[_{t_{i+1}}=}}{_{t_{i}}}e^{-_{_{t_{ i}}}^{_{t_{i+1}}}^{2}(_{u})_{u}}_{t_{i}}+ _{t_{i+1}}_{}(t_{i},t_{i+1})+_{t_{i+1}}(t_{ i},t_{i+1})\] (12)

As pointed out in Proposition 4.2, the Ito integral term \((t_{i},t_{i+1})\) in above equation follows a Gaussian that can be directly sampled so we need to solve the deterministic integral term \(F_{}(t_{i},t_{i+1})\).

We further combine Eq. (12) with the predictor-corrector method, which is a widely used numerical method. It works in two main steps. First, a predictor step is taken to make an initial approximation of the solution. Second, a corrector step will refine the predictor's approximation by taking the predicted value into account. It has been proven successful in the wide application of numerical analysis . Especially, there are some attempts to use the predictor-corrector method to help sample diffusion models . In the subsequent Section 5.1 and Section 5.2, we will separately derive our _SA-Predictor_ and _SA-Corrector_ using Eq. (12). Our algorithm is outlined in Algorithm 1.

```
0: data prediction model \(_{}\), timesteps \(\{t_{i}\}_{i=0}^{M}\), initial value \(_{t_{0}}\), predictor step \(s_{p}\), corrector step \(B\) to store former evaluation of \(_{}\), \((t)\) to control variance.
1:\(B}_{}(_{t_{0}},t_{0})\)
2:for\(i=1\) to \(max(s_{p},s_{c})\)do\(\) Warm-up
3: sample \((,)\)
4: calculate steps for warm-up \(s_{p}^{m}=min(i,s_{p})\), \(s_{c}^{m}=min(i,s_{c})\)
5:\(_{t_{i}}^{p} s_{p}(_{t_{i-1}},B,)\) (Eq. (14)) \(\) Prediction Step
6:\(B}_{}(x_{t_{i}}^{p},t_{i})\)\(\) Evaluation Step
7:\(_{t_{i}} s_{c}^{m}(_{t_{i}}^{p},_{t_{i-1}},B,)\) (Eq. (17)) \(\) Correction Step
8:for\(i=max(s_{p},s_{c})+1\) to \(M\)do
9: sample \((,)\)
10:\(_{t_{i}}^{p} s_{p}(_{t_{i-1}},B,)\) (Eq. (14)) \(\) Prediction Step
11:\(B}_{}(x_{t_{i}}^{p},t_{i})\)\(\) Evaluation Step
12:\(_{t_{i}} s_{c}(_{t_{i}}^{p},_{t_{i-1}},B,)\) (Eq. (17)) \(\) Correction Step return\(_{t_{M}}\) ```

**Algorithm 1**_SA-Solver_

### SA-Predictor

The fundamental idea behind stochastic Adams methods is to leverage preceding model evaluations like \(_{}(_{t_{i}},t_{i}),_{}(_{t_{i-1 }},t_{i-1}),,_{}(_{t_{i-(s-1)}},t_{i-(s-1)})\). These evaluations can be retained with negligible cost implications. Given these preceding model evaluations, a natural strategy for estimating \(F_{}(t_{i},t_{i+1})\) involves the application of Lagrange interpolations  of these evaluations. Lagrange interpolation of \(s\) points \(_{}(_{t_{i}},t_{i}),_{}(_{t_{i-1 }},t_{i-1}),,_{}(_{t_{i-(s-1)}},t_{i-(s-1)})\) is a polynomial \(L(t)\) with degrees \(s-1\):

\[L(t)=_{j=0}^{s-1}l_{i-j}(t)_{}(_{t_{i-j}},t_{i-j}),\] (13)

where \(l_{i-j}(t):\) is the Lagrange basis. Lagrange interpolation is an excellent approximation of \(_{}(_{t},t)\) with the special property: \(L(t_{i-j})=_{}(_{t_{i-j}},t_{i-j}),\;0 j  s-1\). Thus a natural way to estimate \(F_{}(t_{i},t_{i+1})\) is to replace \(_{}(_{_{u}},_{u})\) with \(L()\), which is just a change-of-variable of \(L(t)\). The formula for \(s\)-step _SA-Predictor_ is then derived.

\(s\)-step _SA-Predictor_ Given the initial value \(_{t_{i}}\) at time \(t_{i}\), a total of \(s\) former model evaluations \(_{}(_{t_{i}},t_{i}),_{}(_{t_{i- 1}},t_{i-1}),,_{}(_{t_{i-(s-1)}},t_{i-(s-1)})\), our \(s\)-step _SA-Predictor_ is defined as:

\[_{t_{i+1}}=}}{_{t_{i}}}e^{-_{_{t_ {i}}}^{_{t_{i+1}}}^{2}()} _{t_{i}}+_{j=0}^{s-1}b_{i-j}_{}(_{t_{i-j}},t_ {i-j})+_{i},(,),\] (14)

where \(_{i}=_{t_{i+1}}}}^{ _{t_{i+1}}}^{2}()}}\) according to Proposition 4.2 and \(b_{i-j}\) is given by:

\[b_{i-j}=_{t_{i+1}}_{_{t_{i}}}^{_{t_{i+1}}}e^{-_{ }^{_{t_{i+1}}}^{2}() }(1+^{2}())e^{}l_{i-j}() ,\ 0 j s-1\] (15)

We show the convergence result in the following theorem. The proof can be found in Appendix B.

**Theorem 5.1** (Strong Convergence of \(s\)-step _SA-Predictor_).: _Under mild regularity conditions, our \(s\)-step SA-Predictor (Eq. (14)) has a global error in strong convergence sense of \((_{0 t T}(t)h+h^{s})\), where \(h=_{1 i M}(t_{i}-t_{i-1})\)._

### SA-Corrector

Eq. (14) offers a "prediction" \(_{t_{i+1}}^{p}\) that relies on information preceding or coinciding with the time step \(t_{i}\) since we only use \(_{}(_{t_{i}},t_{i})\) along with other model evaluations antecedent to it, while the integration is over time \([t_{i},t_{i+1}]\). Then predictor-corrector method can be incorporated to better estimate \(F_{}(t_{i},t_{i+1})\) in Eq. (12). We perform a model evaluation \(_{}(_{t_{i+1}}^{p},t_{i+1})\) and construct the Lagrange interpolations of \(_{}(_{t_{i+1}}^{p},t_{i+1}),_{}(_{t_{i}},t_{i}),,_{}(_{t_{i-(s^{}-1)}},t_ {i-(-1)})\):

\[(t)=_{i+1}(t)_{}(_{t_{i+1}}^{p},t_{i+1})+ _{j=0}^{-1}_{i-j}(t)_{}(_{t_{i-j}},t_ {i-j}),\] (16)

where \(_{i-j}(t):\) is the Lagrange basis and \(\) can be different with \(s\) in Eq. (13). The \(\)-step _SA-Corrector_ is derived by replacing \(_{}(_{_{u}},_{u})\) with \(()\) which is a change-of-variable of \((t)\).

\(\)-step _SA-Corrector_ Given the initial value \(_{t_{i}}\) at time \(t_{i}\), a total of \(\) former model evaluations \(_{}(_{t_{i}},t_{i}),_{}(_{t_{i-1 }},t_{i-1}),,_{}(_{t_{i-(-1)}},t_{i-( {s}-1)})\), model evaluation of "prediction" \(_{}(_{t_{i+1}}^{p},t_{i+1})\), our \(\)-step _SA-Corrector_ is defined as:

\[_{t_{i+1}}=}}{_{t_{i}}}e^{-_{_{t_{i} }}^{_{t_{i+1}}}^{2}()}_{t_{i}}+_{i+1}_{}(_{t_{i+1}}^{p},t_{i+1})+ _{j=0}^{-1}_{i-j}_{}(_{t_{i-j}},t_{i-j })+_{i},\] (17)

where \((,)\), \(_{i}=_{t_{i+1}}}}^{ _{t_{i+1}}}^{2}()}}\) according to Proposition 4.2 and the coefficients \(_{i+1}\), \(_{i-j}\

### Connection with other samplers

We briefly discuss the relationship between our _SA-Solver_ and other existing solvers for sampling diffusion ODEs or diffusion SDEs.

Relationship with DDIM DDIM generate samples through the following process:

\[_{t_{i+1}}=_{t_{i+1}}(_{t_{i}}-_{t_{i}}_{}(_{t_{i}},t_{i})}{_{t_{i}}})+}^{2}-_{t_{i}}^{2}}_{}( {x}_{t_{i}},t_{i})+_{t_{i}},\] (19)

where \((,)\), \(_{t_{i}}\) is a variable parameter. In practice, DDIM introduces a parameter \(\) such that when \(=0\), the sampling process becomes deterministic and when \(=1\), the sampling process coincides with original DDPM . Specifically, \(_{t_{i}}=}^{2}}{1-_{t_{i}}^ {2}}(1-}^{2}}{_{t_{i+1}}^{2}})}\).

**Corollary 5.3** (Relationship with DDIM).: _For any \(\) in DDIM, there exists a \(_{}(t):\) which is a piecewise constant function such that DDIM-\(\) coincides with our \(1\)-step SA-Predictor when \((t)=_{}(t)\) with data parameterization of our variance-controlled diffusion SDE._

The proof can be found in Appendix B.5.1.

Relationship with DPM-Solver++(2M) DPM-Solver++ is a high-order solver which solves diffusion ODEs for guided sampling. DPM-Solver++(2M) is a special case of our \(2\)-step _SA-Predictor_ when \((t) 0\).

Relationship with UniPC UniPC is a unified predictor-corrector framework for solving diffusion ODEs. UniPC-p is a special case of our _SA-Solver_ when \((t) 0\) with predictor step \(p\), corrector step \(p\) in Algorithm 1.

## 6 Experiments

In this section, we demonstrate the effectiveness of _SA-Solver_ over the existing sampling methods on both a small number of function evaluations (NFEs) settings and a considerable number of NFEs settings, with extensive experiments. We use Fenchel Inception Distance (FID)  as the evaluation metric to show the effectiveness of _SA-Solver_. Unless otherwise specified, 50K images are sampled for evaluation. The experiments are conducted on various datasets, with image sizes ranging from 32x32 to 256x256. We also evaluate the performance of various models, including ADM , EDM , Latent Diffusion , and DiT .

For ease of computation, we take \((t)\) as a constant function or a piecewise constant function. We leave the detailed settings for \((t)\), predictor step, and corrector step in Appendix E. For the following experiments, we first discuss the effectiveness of the data-prediction model. Then we evaluate the performance of _SA-Solver_ under different random noise scales \(\) to demonstrate the principles for selecting \(\) under few-steps and a considerable number of steps. Finally, we compare _SA-Solver_ with the existing solver to demonstrate its effectiveness.

### Comparison between Data-Prediction Model and Noise-Prediction Model

We first discuss the necessity of using a data-prediction model for _SA-Solver_. We test on ImageNet 256x256 (latent diffusion model) with \((t) 1\). Results of the data-prediction and noise-prediction model are shown in Table 1. It can be seen that the data-prediction model can achieve better sampling quality values under different NFEs, thus we use the data-prediction model in the rest of the experiments. More detailed discussions and theoretical analysis can be seen in Appendix A.2.4.

### Ablation Study on Predictor/Corrector Steps and Predictor-Corrector Method

To verify the effectiveness of our proposed Stochastic Linear Multi-step Methods and Predictor-Corrector Method, we conduct an ablation study on the CIFAR10 dataset as follows. We use EDM  baseline-VE pretrained checkpoint. Concretely, we vary the number of predictor stepsand meanwhile conduct them with/without corrector to separately explore the effect of the two components. As can be seen in Table 2, both Stochastic Linear Multi-step Methods (Predictor 1-steps only v.s. Predictor 3-steps only) and Predictor-Corrector Method (Predictor 1-steps only v.s. Predictor 1-steps, Corrector 1-step, and Predictor 3-steps only v.s. Predictor 3-steps, Corrector 3-steps) improve the performance of our sampler.

### Effect on Magnitude of Stochasticity

The proposed _SA-Solver_ is evaluated on various types of datasets and models, including ImageNet 256x256  (latent diffusion model ), LSUN Bedroom 256x256  (pixel diffusion model ), ImageNet 64x64 (pixel diffusion model ), and CIFAR10 32x32 (pixel diffusion model ). The models corresponding to these datasets cover pixel-space and latent-space diffusion models, with unconditional, conditional, and classifier-free guidance settings (\(s=1.5\) in ImageNet 256x256).

We used different constant \(\) values for _SA-Solver_, namely \(\{0.0,0.2,0.4,...,1.6\}\), where larger value of \(\) correspond to larger magnitude of stochasticity. The FID results under different NFE and \(\) values are shown in Fig. 1. Note that for LSUN Bedroom, 10K images are sampled for evaluation. The experiments indicate that (1) under relatively small NFEs, smaller nonzero \(\) values can achieve better FID results; (2) under a considerable number of steps (20-100), large \(\) can achieve better FID. This phenomenon is consistent with the theoretical analysis we conducted in Appendix B and Appendix C, in which the sampling error with stochasticity is dominated under small NFE, while larger \(\) can significantly improve the quality of generated samples as the number of steps increases. In subsequent experiments, unless otherwise specified, we will report the results of a proper \((t)\) value. Details can be found in Appendix E.

### Comparison with State-of-the-Art

We compare _SA-Solver_ with existing state-of-the-art sampling methods, including DDIM , DPM-Solver , UniPC , Heun sampler and stochastic sampler in EDM . Unless otherwise specified, the methods are tested using the default hyper-parameters in the original papers or code.

**Results on CIFAR10 32x32 and ImageNet 64x64**. We use the EDM  baseline-VE model for the CIFAR10 32x32 experiments and the ADM  model for the ImageNet 64x64 experiments. We use EDM's timesteps selection for all samplers for fair comparisons. EDM introduces a certain type of SDE and a corresponding stochastic sampler, which is used for comparison. The experimental results are shown in Fig. 2(a-b). It can be seen that the proposed _SA-Solver_ consistently outperforms other samplers and achieves state-of-the-art FID results. It should be noticed for EDM samplers, we report its optimal result which is searched over four hyper-parameters. In fact, at 95 NFEs, _SA-Solver_ can achieve the best FID value of 2.63 in CIFAR10 and 1.81 in ImageNet 64x64 which outperforms all other samplers.

  NFEs & Noise-prediction & Data-prediction \\ 
20 & 310.5 & **3.88** \\
40 & 5.85 & **3.47** \\
60 & 3.54 & **3.41** \\
80 & 3.41 & **3.38** \\  

Table 1: Compared results by FID \(\) under data-prediction and noise-prediction models, measured by different NFEs. The latent diffusion model in ImageNet 256x256 is used for evaluation.

  method \(\) setting (NFE, \(\)) & 15,0.4 & 23,0.8 & 31,1.0 & 47,1.4 \\  Predictor 1-steps only & 13.76 & 12.44 & 11.72 & 14.67 \\ Predictor 1-steps, Corrector 1-step & 8.49 & 6.87 & 6.13 & 6.75 \\ Predictor 3-steps only & 5.30 & 3.93 & 3.52 & 2.98 \\ Predictor 3-steps, Corrector 3-steps & **4.91** & **3.77** & **3.40** & **2.92** \\  

Table 2: Compared results by FID \(\) under different predictor steps and corrector steps, measured by different NFEs. The VE-baseline model  in CIFAR10 32x32 is used for evaluation.

**Results on ImageNet 256x256 and 512x512.** We evaluate with two classifier-free guidance models, one is the UNet-based latent diffusion model  in which the VQ-4 encoder-decoder model is adopted, and the other is the DiT  model using Vision Transformer based model with KL-8 encoder-decoder. The corresponding classifier-free guidance scale, namely \(s=1.5\), is adopted for evaluation. For ImageNet 256x256 dataset with UNet based latent diffusion model, the results of different samplers are shown in Fig. 2(c). Under a considerable number of steps, _SA-Solver_ achieves the best sample quality, in which the FID value is 3.87 with only 20 NFEs and below 3.5 with 40 NFEs or more. While for ODE solvers, the FID values cannot reach below 4, which shows the superiority of the proposed SDE solver.

Table 3 consists results of current SOTA models in ImageNet 256x256 and 512x512. Note that the (Min-SNR) DiT-XL/2-G models are adopted [41; 42]. It can be seen clearly that better FID results are achieved compared with baseline solvers used by corresponding methods. We achieve 1.93 FID value in Min-SNR DiT model at ImageNet 256x256, and 2.80 in DiT model at ImageNet 512x512, both of which are state-of-the-art results under existing DPMs.

**Results of text-to-image generation** Fig. 3 shows the qualitative results on text-to-image generation. It can be seen that both UniPC and _SA-Solver_ can generate images with more details. Our _SA-Solver_ is able to generate more reasonable images with better details.

### Effect of Stochasticity for Inaccurate Score Estimation

When the training data is not enough or the computational budget is limited, the estimated score is inaccurate. We empirically observed that the stochasticity significantly improve the sample quality under the circumstance. To further investigate this effect, we reproduce the early training stage of EDM  baseline-VE model for the CIFAR10 32x32 dataset and DiT-XL/2  model for the ImageNet 256x256 dataset. We compare _SA-Solver_ with different stochastic level \(\) and existing state-of-the-art deterministic sampling methods. We use the same hyper-parameters as the corresponding experiment in section 6.4.

Figure 4 shows that _SA-Solver_ outperforms deterministic sampling methods, especially in the early stage of the training process. Moreover, larger \(\) value results in better performance. We also conduct a theoretical analysis that stochasticity can mitigate the error of estimation (see Appendix C).

Figure 1: Sampling quality measured by FID \(\) of _SA-Solver_ under a different number of function evaluations (NFE), varying the stochastic noise scale \(\). For LSUN Bedroom, 10K samples are used to evaluate FID.

   Model &  \\  DiT ImageNet 256x256 & DDPM (NFE=250) & **SA-Solver (Ours)** (NFE=60) \\  & 2.27 & **2.02** \\   &  &  &  & 

Table 3: Sampling quality measured by FID of different sampling methods on DiT, Min-SNR ImageNet [41; 42] models. DiT-XL/2-G and ViT-XL-patch2-32 with \(s=1.5\) are used.

## 7 Conclusions

In this paper, we propose an efficient solver named _SA-Solver_ for solving Diffusion SDEs, achieving high sampling performance in both minimal steps and a suitable number of steps. To better control the scale of injected noise, we propose Variance Controlled Diffusion SDEs based on noise scale function \((t)\) and propose the analytic form of the SDEs. Based on Variance Controlled Diffusion SDE, we propose _SA-Solver_, which is derived from the stochastic Adams method and uses exponentially weighted integral and analytical variance to achieve efficient SDE sampling. Meanwhile, _SA-Solver_ has the optimal theoretical convergence bound. Experiments show that _SA-Solver_ achieves state-of-the-art sampling performance in various pre-trained DPMs models. Moreover, _SA-Solver_ achieves superior performance when the score estimation is inaccurate.

Although _SA-Solver_ achieves optimal sampling performance, the noise scale \((t)\) selection under different NFEs needs further research. The paper proposes empirical criteria for selecting \((t)\), more in-depth theoretical analysis is still needed.

Figure 4: Sampling quality measured by FID \(\) of different sampling methods of DPMs under different training epochs.

Figure 3: Qualitative comparisons between our _SA-Solver_ and previous state-of-the-art methods. All images are generated by Stable Diffusion v1.5 with the same random seed. The main part of the prompt is “portrait of curly orange haired mad scientist man”. We set the guidance scale as 7.5. The proposed _SA-Solver_ is able to generate images with more details.