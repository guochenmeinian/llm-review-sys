# Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models

Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models

Subhodip Panda\({}^{1}\), M S Varun\({}^{*}\)\({}^{2}\), Shreyans Jain\({}^{*}\)\({}^{3}\), Sarthak Kumar Maharana\({}^{4}\) & Prathosh A.P.\({}^{1}\)

\({}^{1}\) Department of ECE, Indian Institute of Science, Bangalore, India

\({}^{2}\) Department of Computer Science, PES University, Bangalore, India

\({}^{3}\) Department of EE, Indian Institute of Technology, Gandhinagar, India

\({}^{4}\) Department of Computer Science, The University of Texas at Dallas, TX, USA

{subhodipp, prathosh}@iisc.ac.in, varun80042@gmail.com,

shreyans.jain@iitgn.ac.in, sarthak.maharana@utdallas.edu

denotes equal contribution

###### Abstract

For responsible and safe deployment of diffusion models in various domains, regulating the generated outputs from these models is desirable because such models could generate undesired violent and obscene outputs. To tackle this problem, one of the most popular methods is to use _machine unlearning_ methodology to forget training data points containing these undesired features from pre-trained generative models. Thus, the principal objective of this work is to propose a machine unlearning methodology that can prevent the generation of outputs containing undesired features from a pre-trained diffusion model. Our method termed as Variational Diffusion Unlearning (**VDU**) is a **one-step method** that _only requires access to a subset of training data containing undesired features to forget_. Our approach is inspired by the variational inference method that minimizes a loss function consisting of two terms: _plasticity inducer_ and _stability regularizer_. _Plasticity inducer_ reduces the log-likelihood of the undesired training data points while the _stability regularizer_, essential for preventing loss of image sample quality, regularizes the model in parameter space. We validate the effectiveness of our method through comprehensive experiments, by forgetting data of certain user-defined classes from MNIST and CIFAR-10 datasets from a pre-trained unconditional denoising diffusion probabilistic model (DDPM).

## 1 Introduction

Figure 1: (a) and (c) show the original images generated by a pre-trained DDPM model on the MNIST and CIFAR-10 datasets, respectively. (b) and (d) display the corresponding images generated after unlearning, using our method **VDU**. The same noise vectors used to generate the original images were applied in the unlearned model to generate the unlearned images. **VDU** delivers good-quality images after unlearning, as well.

In recent years, diffusion models (Ho et al., 2020; Song and Ermon, 2019; Song et al., 2021; Rombach et al., 2022) have been popular for generating high-quality images which are useful for various tasks such as image and video editing (Ceylan et al., 2023; Feng et al., 2024), text-to-image translation (Ramesh et al., 2021, 2022; Saharia et al., 2022) etc. As these models become more and more widespread, there lies a requirement to train them on vast amounts of internet data for diverse and robust output generation. However, there is also a potential downside to using such models, as they often generate outputs containing biased, violent, and obscene features (Tommasi et al., 2017). Thus, a safe and responsible generation from these models becomes an important requirement.

To address these challenges, recent works (Moon et al., 2024; Tiwary et al., 2023; Panda and A.P., 2023; Gandikota et al., 2023; Schramowski et al., 2023; Heng and Soh, 2023) have proposed methods for regulating the outputs of various generative models (e.g. VAEs (Kingma, 2013), GANs (Goodfellow et al., 2020), and Diffusion models (Ho et al., 2020)) to ensure their safe and responsible deployment, with _machine unlearning_ emerging as a particularly important technique for controlling the safe generation of content from these models. The key idea of _machine unlearning_ is to develop a computationally efficient method to forget the subset of training data containing these undesired features from this pre-trained model. Thus, ideally, the _unlearned_ model should behave like the retrained model -- trained without the undesired data subset. However, achieving this goal is challenging because, during the process of unlearning, the model's generalization capacity gets hurt making the quality of the generated outputs poor. This phenomenon is well studied in a similar context also known as _catastrophic forgetting_(McCloskey and Cohen., 1989; Goodfellow et al., 2014; Kirkpatrick et al., 2017; Ginart et al., 2019; Nguyen et al., 2020a) where plasticity to adapt to a new task hurts the stability of the model to perform well on the older task it had been trained on. In this case, the new task of unlearning an undesired subset of data hurts the quality of images generated by the unlearned model thereafter. Thus, the scope of this research is concerned with the following question:

_Can we develop a machine unlearning algorithm that can forget an undesired subset of training data from a pre-trained diffusion model without hurting the quality of the images it generates?_

To answer this question, a recent unlearning work Selective Amnesia (SA) (Heng and Soh, 2023) adopts a continual learning setup and proposes an unlearning method based on elastic weight consolidation (EWC) (Kirkpatrick et al., 2017) wherein a weight regularization strategy in the parameter space is introduced to balance the unlearning task and retaining sample quality. In essence, the variation of parameters is penalized by computing the "importance", to retain good performance, using the Fisher Information Matrix (FIM). However, this unlearning method has two potential downsides: (a) EWC formulation requires the calculation of the FIM which is expensive due to the gradient product. (b) This unlearning method struggles to maintain the model's performance, often leading to the generation of low-quality samples when it relies solely on the unlearning data subset. To solve the problem of low image quality, the authors employ generative replay to retrain the model with generated samples from "non-unlearning" data subsets, which further enhances computational requirements. A major challenge, however, lies in situations with partial access to the training data due to rising concerns for data privacy and safety (Bae et al., 2018). In such a case, Selective Amnesia (SA) performs poorly with no access to the non-unlearning data.

Taking note of such crucial observations, our research aims to develop a computationally efficient algorithm for unlearning an undesired class of training data from a pre-trained unconditional Denoising Diffusion Probabilistic Model (DDPM) (Ho et al., 2020). It is also important to mention that our methodology only requires partial access to a subset of training data aimed at unlearning because it is not always feasible to have access to the full training dataset (Chundawat et al., 2023; Panda et al., 2024). While such a realistic setup is challenging, we draw inspiration from works on variational inference techniques (Knoblauch et al., 2022; Nguyen et al., 2018; Noel, 2021; Wild et al., 2022). We develop, Variational Diffusion Unlearning (**VDU**), a variational inference framework in the parameter space to unlearn a subset of training data. This theoretical formulation of the variation divergence yields a lower bound which is used as a loss function to fine-tune the pre-trained model for unlearning. This loss consists of two terms: _plasticity inducer_ and _stability regularizer_. The _plasticity inducer_ is used for adapting to the new task of reducing the log-likelihood of the unlearning data while the _stability regularizer_ prevents drastic changes in the pre-trained parameters of the model. These two proposed terms capture the persistent trade-offs that exist between the quantity of unlearning required and maintaining initial image quality. Overall, our contributions are summarized as follows:

* We propose a theoretical formulation of unlearning from a variational inference perspective and propose a methodology to unlearn a certain class of training data from a pre-trained unconditional denoising diffusion probabilistic model (DDPM) (Ho et al., 2020).
* To address the limitations of concurrent unlearning methods for diffusion models (Heng and Soh, 2023), which stem from the computational complexity of FIM computation and the need for generative replay with non-unlearned data, our proposed method is more efficient. It achieves computational efficiency by fine-tuning the pre-trained model for only a few epochs--sometimes as few as one--and is effective with fewer samples, requiring access only to the unlearning data points, making it highly suitable for stricter unlearning scenarios with limited access to the original training dataset.
* We validate our method on the MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky et al., 2009) datasets, for unlearning different classes of data points using a pre-trained DDPM.

## 2 Related Works

### Machine Unlearning for Generative Models

The core of machine unlearning (Cao and Yang, 2015; Xu et al., 2020; Nguyen et al., 2022; Bourtoule et al., 2021) revolves around removing or forgetting a specific subset of training data from a trained model, either due to rising privacy and security concerns (Bae et al., 2018), potential fairness (Mehrabi et al., 2021), or as per a user's request. A plausible approach is to retrain the model on the training data devoid of the undesired training data subset. However, this can be computationally very expensive concerning the scale of the model parameters and training data. To solve this problem, different machine unlearning algorithms were proposed for different problem and model settings such as for K-means (Ginart et al., 2019), random forests (Brophy and Lowd, 2021), linear classification models (Guo et al., 2019; Golatkar et al., 2020; Ba et al., 2020; Sekhari et al., 2021), neural network based classifiers (Wu et al., 2020; Graves et al., 2021; Chundawat et al., 2023; Panda et al., 2024) etc.

However, machine unlearning is not only useful for the above supervised and unsupervised learning scenarios but also necessary for generative settings. With the emergence of large pre-trained text-to-image models (Rombach et al., 2022; Saharia et al., 2022), there is potential for misuse in generating harmful or inappropriate content. Thus, controlling the outputs from these generative models becomes an utmost priority. To solve this problem, recent works (Sun et al., 2023; Tiwary et al., 2023; Moon et al., 2024) proposed unlearning-based approaches for variational auto-encoders (VAEs) and generative adversarial networks (GANs). Sun et al. (2023) proposed a cascaded unlearning method using the idea of latent space substitution for a pre-trained StyleGAN under both settings of full and partial access (similar to our setting) to the training dataset. Tiwary et al. (2023) proposed a two-stage _adapt and unlearn_ approach of first adapting a pre-trained StyleGAN to undesired samples and then unlearning the model using the regularization in parameter space. Further to extend unlearning for diffusion models, a recent work (Heng and Soh, 2023) adopts a continual learning setup and proposes an unlearning method based on elastic weight consolidation (EWC) (Kirkpatrick et al., 2017) for unlearning a pre-trained conditional DDPM (Ho et al., 2020).

### Variational Inference

To acquire exact inference from data, it is essential to calculate the exact posterior distribution. However, the exact posterior is often intractable and hard to calculate essentially making the inference task challenging. To solve this problem, the domain of variational inference tries to approximate the true posterior by a more tractable distribution from a class of distributions. Now to get the optimal distribution, often termed as variational posterior, these methods (Sato, 2001; Broderick et al., 2013; Blundell et al., 2015; Bui et al., 2016; Ghahramani and Attias, 2020) optimize the so-called evidence lower bound (ELBO). These methodologies formulate the problem of inference in the parameter or weight space which is often challenging because of the high dimension of the parameter space and multi-modality of parameter posterior distribution. Thus to solve this problem, the recent line of works (Ma et al., 2019; Sun et al., 2019; Rudner et al., 2020; Wild et al., 2022) try to do inference in the function space itself. These methods (Rudner et al., 2020; Wild et al., 2022) perform inference by optimizing functional KL-divergence, minimizing the Wasserstein distance between the functional prior and Gaussian process. Inspired by these works, we formulate our unlearning methodology from a task of inference in parameter space by minimizing a variational divergence.

## 3 Methodology

### Problem Formulation

Consider a pre-trained DDPM model, denoted as \(f_{^{*}}\), with initial parameters \(^{*}^{d}\). \(\) denotes the complete parameter space. This model has been trained on a specific training dataset \(D\), consisting of \(m\) i.i.d. samples \(\{x_{i}\}_{i=1}^{m}\) that are drawn from a distribution \(P_{}\) over the data space \(\), tries to learn the underlying data distribution \(P_{}\). Based on the outputs of this model, the user wants to unlearn a portion of the data space consisting of undesired features, referred to as \(_{f}\). Therefore, the entire data space can be expressed as the union of \(_{r}\) and \(_{f}\), where \(=_{r}_{f}\) or \(_{r}\) = \(_{f}\). We denote the distributions over \(_{f}\) and \(_{r}\) as \(P_{_{f}}\) and \(P_{_{r}}\), respectively. The objective of the unlearning mechanism is to output a sanitized model \(^{u}\) that does not produce outputs within the domain \(_{f}\). This implies that the model should be trained to generate data samples conforming to the distribution \(P_{_{r}}\) only. Assuming access to the whole training dataset, a computationally expensive approach to achieving this is by retraining the entire model from scratch using a dataset \(D_{r}=\{x_{i}\}_{i=1}^{t} P_{_{r}}^{*}\) or equivalently, \(D_{r}=D D_{f}\), where \(D_{f}=\{x_{i}\}_{i=1}^{t} P_{_{f}}^{t}\). It is important to note that we do not have access to \(D_{r}\) in our setting. Hence, the method of retraining becomes infeasible.

### Method Overview

Given the pre-trained DDPM model \(f_{^{*}}\) and unlearning data subset \(D_{f}\), the objective is to produce an unlearned model \(^{u}\) so that it behaves like a retrained model \(^{r}\) trained on \(D_{r}\). Inspired by some previous works in Bayesian inference (Sato, 2001; Broderick et al., 2013; Blundell et al., 2015; Ghahramani and Attias, 2020; Nguyen et al., 2020), it can be seen that, retrained parameters \(^{r}\) are a sample from the retrained model's parameter posterior distribution \(P(|D_{r})\) i.e., \(^{r} P(|D_{r})\). Similarly, the pre-trained model's parameters \(^{*} P(|D_{r},D_{f})\). Using this motivation, we try to approximate the retrained model's parameter posterior distribution \(P(|D_{r})\) as follows:

\[P(|D_{r},D_{f}) P(D_{r},D_{f}|)P() P(D_{f}| )P(D_{r}|)P() P(D_{f}|)P(|D_{r})\] (1)

Eq. 1 is the direct consequence of Bayes' rule ignoring the normalizing constant and assuming the conditional independence between \(D_{r}\) and \(D_{f}\) given \(\). It can be seen from Eq. 1 that the posterior distribution \(P(|D_{r})\) is intractable and an approximation is required by forming \(proj(P(|D_{r})) Q^{*}()\). Here, \(proj()\) is a projection function that takes an intractable un-normalized distribution and maps it to a normalized distribution. As previously mentioned in the literature (Broderick et al., 2013; Blundell et al., 2015; Bui et al., 2016; Nguyen et al., 2018; Noel Loo, 2021; Knoblauch et al., 2022; Wild et al., 2022), one can take several choices of projection functions such as Laplace's approximation, variational KL-divergence minimization, moment matching, and importance sampling. We adopt variational KL-divergence minimization for our method, as prior research (Bui et al., 2016) has demonstrated its superior performance over other inference techniques for complex models. Thus, our method is defined through a variational KL-divergence minimization over set of probable approximate posterior distribution \(\) as follows:

\[Q^{*}()=*{argmin}_{Q()}\,D_{KL}(Q ()||Z,D_{r})}{P(D_{f}|)})\] (2)

Here, \(Z\) is the intractable normalization constant which is independent of the parameter \(\). Finally, if the variational posterior distribution \(Q()=_{i=1}^{d}(_{i},_{i}^{2})\) and the posterior distribution with full data \(P(|D_{r},D_{f})=_{i=1}^{d}(_{i}^{*},_{i}^{*2})\), Eq. 2 results in the minimization of the loss function below which we define as the variational diffusion unlearning (VDU) loss as follows:

\[_{VDU}(,^{*},D_{f})= D_{f}}_{t=1}^{T})}{_{t}(1- _{t-1})}\|_{0}-_{}(x_{t},t)\|^{2})}_{ }+^{d}-_{i}^{*})^{2}}{2 _{i}^{*2}}}_{}\] (3)

**Input**: unlearning data: \(D_{f}\), initial parameter: \(^{*}\), no. of epochs: \(E\), learning rate: \(\) and, hyper-parameter: \(\)

```  Initialize: \(^{*}\) for\(t=1\) to \(E\)do \(_{VDU}(,^{*},D_{f})=-(1-)(_{x_{t}  D_{f}}_{t=1}^{T})}{_{t}(1-_{t-1} )}\|_{0}-_{}(x_{t},t)\|^{2})+_{i=1}^{d} -_{i}^{*})^{2}}{2_{i}^{*2}}\) \(^{t+1}^{t}-_{}_{VDU}(, ^{*},D_{f})\) endfor Output: \(^{E}\) ```

**Algorithm 1** Variational Diffusion Unlearning (VDU)

Eq. 3 represents the proposed loss function used to optimize the pre-trained model during the unlearning process. The loss comprises two key terms: \(A\) term referred to as the "plasticity inducer," minimizes the log-likelihood of the unlearning data, while \(B\) term serves as the "stability regularizer," penalizing the model's parameters to prevent them from deviating too much from their pre-trained state during unlearning. To balance these two components, we introduce a hyper-parameter, \(\). \(\{_{t}:t T\}\) refers to the diffusion model's noise scheduler where \(_{t}=_{j=1}^{t}_{j}\). \(_{0}\) is the true added noise, \(_{}(x_{t},t)\) is the model predicted noise at time \(t\) and, \(d\) is the dimension of parameter. Figure 2 is a detailed illustration of our framework and details the different loss components used for the unlearning of \(D_{f}\).

### Theoretical Outlook

In this section, we provide theoretical exposition for the derivation of the variational diffusion unlearning loss \(_{VDU}(,^{*},D_{f})\) in Eq. 3 from variational divergence minimization defined in

Figure 2: **Variational Diffusion Unlearning (VDU):** Given user-identified samples to be unlearned (\(D_{f}\)), our unlearning method fine-tunes the initial pre-trained DDPM model with a loss function having two terms: the first component is a _Plasticity inducer_ (bottom half) aiming to minimize the log-likelihood associated with the unlearned data points while the second one is a _Stability regularizer_ (upper half) aiming to retain the performance of the model.

Eq. 2. Let \(\{x_{t}:t=0,1,,T\}\) denote latent variables with \(x_{0}\) denoting the true data. It is important to mention that in the diffusion process, it is assumed that all transitional kernels are first-order Markov. Now, in the forward diffusion process, the transition kernel is denoted as \(q(x_{t}|x_{t-1})\) with the joint posterior distribution being \(q(x_{1:T}|x_{0})=_{t=1}^{T}q(x_{t}|x_{t-1})\) where each \(q(x_{t}|x_{t-1})=(x_{t};}x_{t-1},(1-_{t})I)\). Similarly, for the backward diffusion process, the transitional kernel is denoted as \(p(x_{t-1}|x_{t})\) with joint distribution \(p(x_{0:T})=p(x_{T})_{t=1}^{T}p_{}(x_{t-1}|x_{t})\) where, \(p(x_{T})=(x_{T};0,I)\). Thus after optimizing the diffusion model, the sampling procedure is done by sampling Gaussian noise from \(p(x_{T})\) and iteratively running the denoising transitions \(p_{}(x_{t-1}|x_{t})\) for \(T\) steps to generate a new sample \(x_{0}\).

**Lemma 1**: _Assuming all the transition kernels to be Gaussian, the following holds:_

1. \(q(x_{t-1}|x_{t},x_{0})=(x_{t-1};_{q}(t),_{q}^{2}(t)I)\) _with_ \(_{q}(t)=}}x_{t}-}{_{t}}}}_{0}\)__
2. \(p_{}(x_{t-1}|x_{t})=(x_{t-1};_{}(t),_{q}^{2}(t )I)\) _with_ \(_{}(t)=}}x_{t}-}{_{t}}}}_{}(x_{t},t)\)__
3. \(_{q}^{2}(t)=)(1-_{t-1})}{(1-_{t})}\)__

**Lemma 2**: _The log-likelihood under the backward diffusion process kernel,_

\[ p_{}(x_{0})-_{t=2}^{T}}_{q(x_{t}|x_{0} )}[D_{KL}(q(x_{t-1}|x_{t},x_{0})||p_{}(x_{t-1}|x_{t}))]\]

Partial derivation for the above two lemmas can be found in Luo (2022). For completeness, we have added the full proof in Appendix 6.1.1 and 6.1.2.

**Theorem 1**: _Assuming a Gaussian mean-field approximation in the parameter space i.e., if the variational posterior distribution \(Q()=_{i=1}^{d}(_{i},_{i}^{2})\) and the posterior distribution with full data \(P(|D_{r},D_{f})=_{i=1}^{d}(_{i}^{*},_{i}^{*2})\) then,_

\[D_{KL}(Q()Z, D_{r})}{P(D_{f}|)})  D_{f}}_{t=2}^{T} )}{_{t}(1-_{t-1})}||_{0}-_{}(x_{t},t )||^{2}}_{I}\] \[+^{d}[-_{i}^{*})^ {2}}{2{_{i}^{*}}^{2}}+^{2}}{2{_{i}^{*}}^{2}}+ ^{*}}{_{i}}-]}_{I}\]

**Proof 1**: _Here we give a sketch of the proof. For a detailed proof, please look into Appendix 6.1.3. The KL-divergence term in Eq. 2 is expanded and segregated into two terms: \(}[ P(D_{f}|)]\) and \(D_{KL}(Q()||P(|D_{r},D_{f})\). The first term is approximated using Lemmas 1 and 2, while the second term is expanded using the assumption of KL divergence between two Gaussian distributions._

**Remark 1**: _In Theorem 1, the first term \(I\) appears in the first part of loss function \(_{VDU}(,^{*},D_{f})\) in Eq. 3. While if we assume \(_{i}=_{i}^{*}\) term \(II\) turns out to be simply \(_{i=1}^{d}[-_{i}^{*})^{2}}{2{_{i}^{*}}^{2}}]\), which is the second component of our proposed loss function._

## 4 Experiments and Results

### Datasets and Models

The primary goal of our unlearning method is to stop the generation of undesired images from a pre-trained DDPM model. We utilize two well-known datasets for our experiments: MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky et al., 2009). Here, we use unconditional DDPM models for our unlearning method. We use two different U-Net architectures for the DDPM model on MNIST and CIFAR-10 respectively. These architectures are used from two open-source implementations detailed in Appendix 6.2.1.

### Initial Training, Unlearning and Baseline

\(\)**Initial Training:** We train the unconditional DDPM model on MNIST for 40 epochs with a batch size of 64 to obtain the pre-trained model, which achieves an FID (Heusel et al., 2017) score of 5.12. Similarly, to obtain the pre-trained model for the CIFAR-10 dataset, we used a pre-trained checkpoint from an open-source implementation and fine-tuned the model for a further 90,000 iterations with a batch size of 128 to achieve an FID score of 7.96. A more detailed description of the initial training setups can be found in Appendix 6.2.2.

\(\)**Unlearning:** For MNIST, we unlearn the digit classes **0**, **1**, and **8**. Similarly, for the CIFAR-10 dataset, we target the unlearning of specific classes: **class 1 (automobile)**, **class 6 (frog)**, and **class 8 (ship)**. As can be seen from our method, we require the model parameter's mean and variance, so we have trained 5 models on each dataset to calculate \(_{i}^{*}\) and \(_{i}^{*}\). Further experimental details of our unlearning method on each dataset are added in Appendix 6.2.3.

\(\)**Baseline:** For comparison, we have adapted the state-of-the-art Selective Amnesia (SA) (Heng and Soh, 2023) as the baseline, as it is the most relevant to our approach. A detailed comparison can be found in Table 1. This baseline method relies on a computationally expensive generative replay technique, essentially retraining the model to preserve the quality of generated samples from the unlearned model. In contrast, our approach eliminates the need for such retraining, offering a more efficient alternative.

### Evaluation Metrics

To evaluate different unlearning methods for generative models, previous work (Tiwary et al., 2023) proposed below metrics which are described as follows:

\(\)**Percentage of Unlearning (PUL):** This metric measures how much unlearning has occurred by comparing the reduction in the number of unwanted samples produced by the DDPM model after unlearning (\(^{u}\)) with the number of such samples before unlearning (\(^{*}\)). The Percentage of Unlearning (PUL) is calculated as: \(=^{g})_{^{u}}-(D_{f}^{g})_{^{u}}}{(D_{f}^{g })_{^{*}}} 100\%\) where \((D_{f}^{g})_{^{u}}\) and \((D_{f}^{g})_{^{u}}\) represent the number of undesired samples generated by the original DDPM and the unlearned DDPM, respectively. To calculate PUL, we generate 5,000 random samples from both DDPM models and use a pre-trained classifier to identify the unwanted samples.

\(\)**Unlearned Frechet Inception Distance (u-FID):** To evaluate that the unlearning model doesn't render the pre-trained model useless i.e., to quantify the quality of generated images by the unlearned DDPM, we utilize the u-FID score. It is important to mention that this FID score is measured between the generated samples from the unlearned model and the real data only consisting of non-unlearning data. Thus, to remove the unlearning data points from the real data we use a pre-trained classifier. In this case, a lower u-FID score reflecting higher image quality indicates that the unlearned model's performance does not degrade on the non-unlearning data points.

### Experimental Results

Table 1 shows the performance of our method compared to the Selective Amnesia (SA) baseline for different class unlearning settings. It is observable that our method achieves lower u-FID scores with superior to comparable PULs, offering a more favorable trade-off than the SA method. On the MNIST dataset, our method outperforms the SA method (which was trained for 2 epochs) after just 1 epoch of training. However, for CIFAR-10, the SA method achieved its best results with 2 epochs with poor sample quality (see Appendix 6.3.2), while our method required 4 epochs to reach optimal performance and maintain good sample quality. Table 2 further shows the performance of our method for different values of \(\).

In Figure 3, we illustrate the visual performance of our method for different class unlearning scenarios on both MNIST and CIFAR-10 datasets. Further visual results are illustrated in Appendix 6.3.

## 5 Conclusion, Limitations, and Future Works

For the safe and responsible deployment of generative models, it is essential to regulate outputs that contain undesired features. This work presents a machine unlearning methodology to prevent the generation of undesired outputs from a pre-trained unconditional denoising diffusion probabilistic model (DDPM) without accessing the whole training data. Our method termed as Variational Diffusion Unlearning (**VDU**) presents a variational inference framework in parameter space to reduce undesired number of sample generation effectively with a lower computational cost. We show the effectiveness of our method on different class unlearning settings for lower dimensional datasets such as MNIST and CIFAR-10. Acknowledging limited experimental evidence of **VDU** only on lower dimensional datasets, our current and future efforts are as follows:

\({}^{}\)**Future experiments:** To show further experimental evidence for the effectiveness of our method, we plan to test our method for high-dimensional datasets such as miniImageNet (Vinyals et al., 2016) and CelebA (Liu et al., 2015).

\({}^{}\)**Theoretical Generalization:** Our current theoretical framework leverages the parameter space to exploit variational inference, but its scope is limited. Inspired by the idea of function space variational inference techniques (Ma et al., 2019; Rudner et al., 2020; Sun et al., 2019; Wild et al., 2022), our current efforts also involve finding a superior variational inference framework for machine unlearning.

   & **Unlearned Classes** &  **Selective Amnesia** \\ **PUL(\%)** \\  & 
 **VDU (Our Method)** \\ **PUL(\%)** \\  \\   & Digit-0 & **77.47** & 121.61 & 61.0 & **29.33** \\  & Digit-1 & 15.01 & 301.25 & **75.06** & **14.42** \\  & Digit-8 & 48.95 & 161.08 & **68.96** & **38.20** \\   & Automobile & 2.25 & 92.89 & **60.87** & **30.85** \\  & Frog & **66.39** & 111.95 & 62.56 & **30.17** \\  & Ship & **85.02** & 249.88 & 71.63 & **24.45** \\  

Table 1: Quantitative performance comparison on MNIST and CIFAR-10 datasets.

   &  \\  & **PUL(\%)** & **u-FID** & **PUL(\%)** & **u-FID** \\   & 0.1 & 75.36 & 14.43 \\  & Digit-1 & 0.3 & 72.15 & 14.43 \\  & Digit-1 & 0.6 & 58.21 & 11.54 \\  & Digit-8 & 71.67 & 13.12 \\   & 0.1 & 71.63 & 24.46 \\  & 0.3 & 74.65 & 28.75 \\  & 0.6 & 65.40 & 20.51 \\  & 0.8 & 69.95 & 19.64 \\  

Table 2: Unlearning performance with different values of \(\).

Figure 3: Generated samples from the pre-trained DDPM model and different class unlearned models. (a),(b),(c), and (d) in the first row are generated using pre-trained and unlearned models respectively trained on MNIST while (e),(f),(g), and (h) in the second row images are generated using initial and unlearned models trained on CIFAR-10. Our method shows superior image quality after unlearning.