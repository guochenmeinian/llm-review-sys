# Model Shapley: Equitable Model Valuation with Black-box Access

Xinyi Xu\({}^{@sectionsign}\), Thanh Lam\({}^{}\), Chuan-Sheng Foo\({}^{@sectionsign}\), Bryan Kian Hsiang Low\({}^{}\)

Dept. of Computer Science, National University of Singapore, Republic of Singapore\({}^{}\)

Inst. for Infocomm Research; Centre for Frontier AI Research, A\({}^{*}\)STAR, Republic of Singapore\({}^{@sectionsign}\)

xinyi.xu@u.nus.edu, lamchithanh1997@gmail.com

foo_chuan_sheng@i2r.a-star.edu.sg, lowkh@comp.nus.edu.sg

###### Abstract

Valuation methods of data and machine learning (ML) models are essential to the establishment of AI marketplaces. Importantly, certain practical considerations (e.g., operational constraints, legal restrictions) favor the use of model valuation over data valuation. Also, existing marketplaces that involve trading of pre-trained ML models call for an equitable model valuation method to price them. In particular, we investigate the _black-box access_ setting which allows querying a model (to observe predictions) without disclosing model-specific information (e.g., architecture and parameters). By exploiting a _Dirichlet abstraction_ of a model's predictions, we propose a novel and _equitable_ model valuation method called _model Shapley_. We also leverage a Lipschitz continuity of model Shapley to design a learning approach for predicting the model Shapley values (MSVs) of many vendors' models (e.g., \(150\)) in a large-scale marketplace. We perform extensive empirical validation on the effectiveness of model Shapley using various real-world datasets and heterogeneous model types.

## 1 Introduction

Data valuation methods have important roles in extensive applications such as dataset pricing in a data marketplace , evaluating the contributions of data from multiple collaborating parties , and identifying valuable data (or filtering out less valuable data) for training higher-quality machine learning (ML) models . However, the following practical considerations favor the use of model valuation over data valuation: (A) Data valuation can be operationally infeasible due to the characteristics of training data, such as being _massively distributed_ over millions of sources , _enormous in size_ (e.g., \(400\) billion byte-pair-encoded tokens for a language model ), and/or _transient_ (e.g., the data for online learning are not stored persistently ). In contrast, a trained model is _not_ distributed by nature (and hence does not need to be aggregated from distributed sources), often much smaller in size than the training data (e.g., less than \(1\%\)),1 and usually stored persistently for inference. (B) Data privacy regulations (e.g., GDPR) can be a legal impediment to data valuation in designing _fair_ payments for ML-based collaborations (e.g., in medicine  or cyber-defense ) because the centralization of (possibly) _private_ data, which seems necessary for data valuation , is prohibited by such regulations. In contrast, the ML models trained on private data without centralization  can be available for valuation. (C) Existing marketplaces that involve trading of pre-trained ML models (e.g., AWS marketplace, Modzy) call for an _equitable_ model valuation to price them. These practical considerations motivate the use of equitable model valuation over data valuation.

For model valuation, practitioners (e.g., model vendors or clinicians who use the trained models) would likely prefer their models to be examined via only _black-box access_,2 i.e., by querying the model with input for the corresponding predictions without observing its internal mechanism . It does not disclose the proprietary model information (for model vendors) nor the sensitive information contained in the model (for clinicians), and provides an added advantage of the model valuation being model-agnostic since no model-specific information is used. However, these make model valuation challenging by restricting the available model information to only a selected _query set_ (e.g., a validation set) with the observed predictions. In contrast, with white-box access, there is more model information available (e.g., information-theoretic criteria for probabilistic models  or norms of the parameters for deep neural networks ) to assess model complexity or certain analytical properties (e.g., uniqueness of an optimal model in logistic regression).

Intuitively, the value of a model depends on its intended task: For example, a model trained to classify MNIST digits  is not very valuable to a clinician trying to classify diagnostic scans. This dependency is useful in practice for selecting the most 'valuable' model for the desired _task_ (i.e., as a query set). We refer to task and query set interchangeably hereafter. While the predictive accuracy of a model on the task provides an intuitive value , it can be too reductive: Suppose that two models \(_{i}\) and \(_{i^{}}\) have identical predictive accuracies on the task but \(_{i}\) (\(_{i^{}}\)) makes highly (barely) certain predictions, i.e., \(_{i}\) (\(_{i^{}}\)) predicts the true class with over \(90\%\) (barely over \(50\%\)) probability. Intuitively, the values for \(_{i}\) and \(_{i^{}}\) should not be the same, which cannot be achieved with accuracy alone, hence suggesting that additional model information beyond accuracy is required. **(1)**_What then should be a suitable abstraction of a model w.r.t. a task, for model valuation?_

Satisfying certain equitability properties in model valuation is imperative in the application of model pricing to ensuring a fair market. For instance, consistent valuation of identical models is important as it is unfair to price them differently otherwise. Furthermore, the market economy dictates that the value of a model depends on other available models (e.g., more available substitutes cause depreciation), which is important to guarantee a fair market by preventing price fixing (i.e., an exploitative pricing scheme often made illegal by anti-trust laws). **(2)**_How then should model valuation be formulated to satisfy these equitability properties?_

In particular, the Shapley value is shown to satisfy these equitability properties but raises another practical challenge that computing it exactly incurs \((2^{N})\) time where \(N\) is the number of models in a marketplace. So, for a given computational budget, there is a fundamental ceiling to the size of the marketplace such that including more models causes the marketplace to be unable to determine their values (within reasonable time). **(3)**_How can the desirable equitability properties of the Shapley value still be exploited without imposing a significant restriction on the size of the marketplace?_

This paper presents a novel model-agnostic valuation framework to tackle the above challenges. For **(1)**, we use an insight that the predictive pattern of a model (w.r.t. a task) is an suitable abstraction for valuation, especially since the black-box access rules out other model information. Specifically, we use a Dirichlet distribution to approximate a model's predictive pattern/distribution w.r.t. a task, which we call the _Dirichlet abstraction_ of this model to encode both its predictive accuracy and certainty (Sec. 2). We describe how to adjust the level of abstraction to trade off the amount of model information (i.e., higher abstraction level) for the availability of a smaller query set. Then, for **(2)**, we observe that ensuring equitability requires a similarity measure of the models (e.g., to ensure identical models are valued consistently or to identify substitutes). We exploit the ability of the Dirichlet abstractions to preserve the similarity between models for proposing _model Shapley_ as an equitable model valuation (Sec. 3). As an illustration, identical models produce identical Dirichlet abstractions which result in equal model Shapley values (MSVs). To address **(3)**, based on the Dirichlet abstraction, we leverage a Lipschitz continuity of model Shapley to justify and propose a learning approach of training a _model appraiser_ (i.e., a regression learner) on a small subset of models (and their MSVs) for predicting other models' MSVs to validate model Shapley's practical feasibility in a large-scale marketplace (Sec. 4), as empirically verified on real-world datasets and up to \(150\) heterogeneous models. We empirically validate that better predictive accuracy (e.g., \(\) score) due to better training data, more suitable model types, and/or higher predictive certainty result in higher MSVs, and demonstrate a use case for identifying a valuable subset of models from the marketplace to construct a larger learner (e.g., random forests) (Sec. 5).

Dirichlet Abstraction of a Model

We give some preliminaries on the Dirichlet distribution and Hellinger distance below:3

**Definition 1** (Dirichlet distribution ).: The probability density function of a \(C\)-dimensional Dirichlet random variables \(Z()\) with parameters \(=[_{1},,_{C}](0,)^{C}\) is \(p(z;)=_{k=1}^{C}z_{k}^{_{k}-1}\{ [_{k=1}^{C}(_{k})](_{k=1}^{C }_{k})\}\) where \(\) is the gamma function.

**Definition 2** (Hellinger distance ).: The Hellinger distance between distributions (whose probability density functions are) \(p\) and \(q\) is \(d_{}(p,q)[1-\;x]^{1/2}\).

### Dirichlet Abstraction and MLE

We will now describe an abstraction of a model via a Dirichlet approximation to the model's predictive pattern over some task and a maximum likelihood estimation (MLE) for it. Each (learnt) \(C\)-way classification model \(_{i}:(C)\) is a mapping from the input space \(\) to the \((C-1)\)-probability simplex \((C)\) (i.e., space of \(C\)-dimensional probability vectors). Denote a random variable \(X P_{X}\) whose support \((X)=\) is the input space, then its distribution \(P_{X}\) induces a predictive distribution \(P_{_{i}(X)}\) over \((C)\). Concretely, \(P_{X}\) is represented by a task/query set \(\{(x_{j},y_{j}\{1,,C\})\}_{j=1, ,D}\) where each \(x_{j}\) is a realization of \(X P_{X}\), so that \(_{i}(x_{j})\) is a realization of \(P_{_{i}(X)}\).

Dirichlet abstraction.Note that the predictive distribution \(P_{_{i}(X)}\) (induced by \(P_{X}\)) (i) has the exactly same support to that of a \(C\)-dimensional Dirichlet distribution, so \(P_{_{i}(X)}\) can be mathematically modeled using a Dirichlet distribution; and (ii) can be statistically modeled using a Dirichlet distribution because the predictive pattern of a classification model can be (statistically) characterized well with a Dirichlet distribution . Informally, the "shape" of predictive distribution of a classification model is similar to that of a Dirichlet distribution. Hence, we let \(P_{_{i}(X)}=_{i}:=(_{i})\), whose parameters \(_{i}\) can be learnt using MLE (described later) and refer to \(_{i}\) or \(_{i}\) as \(_{i}\)'s _Dirichlet abstraction_. In words, we abstract the predictive distribution \(P_{_{i}(X)}\) of the model \(_{i}\) induced by \(P_{X}\) into a Dirichlet distribution, hence the name Dirichlet abstraction. Note that the notational dependence on \(P_{X}\) (or \(\)) is suppressed when the context is clear.

The proposed Dirichlet abstraction offers several important advantages (further elaborated in Sec. 3): (a) By design, this formulation replaces the heterogeneity in models with the homogeneity in their Dirichlet abstractions, and allows the subsequently proposed model valuation to be applicable to models of different types: The respective Dirichlet abstractions of a multi-layer perceptron and a logistic regression can be compared directly. (b) \(_{i}\) encodes the predictive accuracy and certainty of \(}_{i}\) through a theoretical connection between the Hellinger distance \(d_{}\) of two Dirichlet abstractions and the cross entropy of \(_{i}\) (formalized by Proposition 2). (c) Importantly for model valuation, an appealing analytic property of the Dirichlet distribution is that \(d_{}\) of two Dirichlet abstractions can be evaluated in closed-form and incurs \((1)\) computational complexity.

Mle.We adopt the MLE approximation of \(_{i}\) using the predictions of \(_{i}\) on \(\) since the log-likelihood function is concave with a unique maximizer and can thus be efficiently optimized . Since \(_{i}(x_{j})\) denotes a realized predictive probability vector of \(_{i}(X)(_{i})\), we use the _observed sufficient statistics_\(_{i} D^{-1}_{j=1}^{D}_{i}(x_{j})\) (i.e., with an element-wise \(\) operation) to derive the log-likelihood as follows :

\[F(,_{i}) D[G()+ _{k=1}^{C}(_{k}-1)_{i,k}]\] (1)

where \(G()(_{k}_{k})-_ {k}(_{k})\). From (1), \(_{i}\) arises as an alternative (to \(_{i}\)) abstraction of \(_{i}\) w.r.t. \(\). Hence, we compare \(_{i}\) and \(_{i}\) theoretically (Proposition 3 in App. A) and empirically (Sec. 4.2).

### Class-specific Dirichlet Abstraction

Since the Dirichlet abstraction \(_{i}\) of \(_{i}\) w.r.t. some task \(\) does _not_ explicitly account for the class information in \(\) (i.e., the true classification label \(y_{j}\) of each data \(x_{j}\)), models with certain (different) predictive patterns can be indistinguishable. This can be problematic because the Dirichlet abstractions of an optimal model and another model with zero predictive accuracy but a specific predictive pattern can identical, making it difficult to distinguish between these two models.

Suppose that there are \(C=3\) classes in a balanced query set \(\) (i.e., equal data size for each class). For some \(_{i}\) (with optimal predictive accuracy), artificially construct \(_{i^{}}\) to have the same Dirichlet abstraction, but _zero_ predictive accuracy in the following way: Since \(\) is balanced, group the data into triplets \(\{(x_{j,1},x_{j,2},x_{j,3})\}_{j=1,,D/3}\) for the input data from \(3\) classes. Next, define \(_{i^{}}(x_{j,c})_{i}(x_{j,(c 3)+1})\) for \(c=1,2,3\). Intuitively, for each prediction (i.e., a \(3\)-dimensional probability vector in a \(2\)-simplex) by \(_{i}\), \(_{i^{}}\) makes an identical one but on an input data from a wrong class, i.e., we'shift' \(_{i}\)'s predictions by one class to construct the predictions of \(_{i^{}}\). Obviously, the predictive accuracy of \(_{i^{}}\) is zero, but its predictions (in aggregation) are identical to those of \(_{i}\), which means \(_{i}=_{i^{}}\) in (1), hence resulting in \(_{i}=_{i^{}}\). In Fig. 1: Plots \(1\) and \(2\) are the predictions of \(_{i}\) and \(_{i^{}}\) respectively, on \(\), which are (visually) indistinguishable. Plots \(3\) and \(4\) are samples from \(_{i}\) and \(_{i^{}}\) respectively, which are also (visually) indistinguishable. The implication is that, in this case (of \(_{i}\) and such specially constructed \(_{i^{}}\)), the highest level of abstraction (i.e., using the entire query set \(\) to construct Dirichlet abstractions \(_{i}\) and \(_{i^{}}\)) is not effective to distinguish between \(i\) and \(i^{}\). Hence, we adopt a lower level of abstraction, described next.

The remedy is the so-called _class-specific Dirichlet abstraction_: Partition the query set \(=_{k=1}^{C}_{k}\) where \(_{k}\) contains _only_ data from the \(k\)-th class. The Dirichlet abstraction \(_{i,_{k}}\) of \(_{i}\) on \(_{k}\) is called the class-specific Dirichlet abstraction w.r.t. class \(k\). Based on the example above, we verify using a small experiment.4 Over the entire \(\), we obtain \(_{i}=_{i^{}}=[0.5040,0.5339,0.5306]\). Restricting to query set \(_{1}\) from class \(1\) only, gives \(_{i,_{1}}=[21.8601,2.2005,2.0215]\) and \(_{i^{},_{1}}=[1.5817,1.7576,19.4037]\), which are clearly different. In Fig. 1: Plots \(5\) and \(6\) are the predictions of \(_{i}\) and \(_{i^{}}\), respectively, on \(_{1}\) while plots \(7\) and \(8\) are samples from \(_{i,_{1}}\) and \(_{i^{},_{1}}\). We see that \(_{i}\) is clearly different from \(_{i^{}}\), and \(_{i,_{1}}\) is clearly different from \(_{i^{},_{1}}\), demonstrating the effectiveness of a lower level of abstraction via the class-specific Dirichlet abstraction.

Trade-off between level of Dirichlet abstraction and size of query set.Note that we need not stop at partitioning \(\) at the class level. For instance, a more refined partition can be first according to the classes and then certain groups of input feature values. Doing so produces Dirichlet abstractions with a _lower_ level of abstraction (i.e., less abstract and containing more model information), but also requires a _larger_ query set \(\) to begin with so that each smaller partitioned query set contains sufficient data (for predictions) to obtain an accurate MLE estimate.5 For an extremely refined partition of \(\) where each partitioned query set is very small (e.g., size \( 5\)), the obtained MLE estimates and the corresponding Dirichlet abstraction can be inaccurate and thus not useful. It is thus important to find a suitable trade-off between the abstraction level vs. query set size. In particular, in Sec. 5, we observe that partitioning \(\) according to the classes provides such a suitable trade-off: E.g., it can distinguish models with almost equal predictive accuracy due to the high class imbalance

Figure 1: The triangles denote the \(2\)-simplex and each dot is a \(3\)-dimensional probability vector. Plots \(1\)-\(4\) (\(5\)–\(8\)) use \(\) (\(_{1}\)) as query set. Plots \(1\&5\) (\(2\&6\)) show predictions of \(_{i}\) (\(_{i^{}}\)). Plots \(3\&7\) (\(4\&8\)) show randomly drawn samples from Dirichlet abstraction \(_{i}\) (\(_{i^{}}\)). For example, plot \(4\) shows random samples from \(_{i^{}}\) w.r.t. \(\), while plot \(5\) shows predictions of \(_{i}\) w.r.t. \(_{1}\).

in the data6 but different F1 scores. Since the Dirichlet abstraction and the class-specific version are both Dirichlet distributions, and share the same theoretical properties (e.g., enabling a closed-form expression of the Hellinger distance), subsequently, we refer to Dirichlet abstractions generally (i.e., omitting the specific dependence on \(\) or \(_{k}\)), unless otherwise specified.

## 3 Equitable Valuation via Model Shapley

We discuss and formalize several equitability properties to derive a general formulation called _model Shapley_ to satisfy them. Then, we will specify the _characteristic function_ and a _precision-weighted fusion_ to encode a model's predictive accuracy and certainty into its MSV.

### Equitability Properties and Model Shapley

Consider \(N\) models in a marketplace and denote \(_{i}\)'s equitable value by \(_{i}\). (P1) If a model \(_{i}\) has not been queried at all, then its value is indeterminate and we set \(_{i}=0\) by default. (P2) If two models \(_{i}\) and \(_{i^{}}\) give identical predictions (over the task), then their values are equal, i.e., \(_{i}=_{i^{}}\). (P3) If some buyer is interested in multiple tasks simultaneously and a model (from some vendor) performs very well on _only_ one of the tasks, then it is unfair for the vendor to set the value/price solely based on this performance. Instead, an equitable value should be based on the model's joint performance on these tasks (e.g., a linear combination according to the buyer's interests in these tasks). (P4) The existence of perfect substitutes depreciates the value of a model. (P1)-(P4) are useful for equitable valuation in ML model marketplaces . To formalize these properties, some notations are needed: Let \(:2^{[N]}\) denote a characteristic function (specified later) s.t. \(()\) quantifies the value of a collection \([N]\{1,,N\}\) of models to capture the dependence of \(_{i}\)'s value on other existing models \(_{i^{}}\) (e.g., substitutes). Denote \(_{i}\)'s value by \(_{i}(i,,\{_{i}\}_{i[N]})\) which is fully specified (up to linear scaling) by the properties:

1. Null player: \(([N]\{i\}\ \ (\{i\})-( )=0)_{i}=0\).
2. Symmetry: \(([N]\{i,i^{}\}\ \ (\{i\})=(\{i^{}\}))_{i}= _{i^{}}\).
3. Linearity: \(,^{}\ (_{ ^{}}\ _{}+^{}\ _{^{}}) _{i}(^{})=\ _{i}()+^{}\ _{i}(^{ })\).
4. Diminished marginal utility: Add a perfect substitute (i.e., duplicate/copy) \(_{i_{i}}\) of \(_{i}\) to the pool of \(N\) models already containing \(_{i}\) and denote the new pool by \([N^{}][N]\{i_{}\}\). Denote the value of \(_{i}\) w.r.t. \([N]\) by \(_{i}\) and w.r.t. \([N^{}]\) by \(_{i}^{}\). Then, \(_{i}^{}_{i}\).

**Proposition 1**.: Properties P1, P2, and P3 fully specify \(()\) up to a linear scaling \(Z\):

\[_{i} Z_{[N]\{i\}}_{ }[(\{i\})-()] _{}||!(N-| |-1)!/N!\.\] (2)

Its proof follows directly from [20, Proposition 2.1]. Since (2) coincides with the Shapley value , we refer to \(\) as model Shapley and \(_{i}\) as the _model Shapley value_ (MSV). Note that P3 requires two distinct tasks \(\) and \(^{}\) to distinguish between the MSVs of \(_{i}\) w.r.t. to these two tasks. P4 additionally requires \(\) to be _conditionally redundant_7 (Proposition 5 in App. A): The benefit of a redundant copy \(_{i_{c}}\) (conditioned on model \(_{i}\) already being added) is not more than the initial benefit of adding \(_{i}\). Intuitively, as adding \(_{i}\) is already sufficient for the desired task, subsequently adding \(_{i_{c}}\) does not yield (as much) extra benefit . In contrast,  also adopts (2) but assumes \(\) already exists while we explicitly design \(\) to encode predictive accuracy and certainty:

\[()-d(_{},^{*})\] (3)

where \(_{}\) is the _precision-weighted fusion_ of the Dirichlet abstractions in \(\) and \(d(,)\) is a distributional distance measure between two Dirichlet abstractions. Recall from Sec. 2 that the predictive distribution of \(_{i}\) is represented in its Dirichlet abstraction (visualized in Fig. 1). In particular, the more accurate \(_{i}\) is, the closer (distributionally) \(_{i}\) is to the Dirichlet abstraction \(^{*}\) of an expert (i.e., optimal classifier). Hence, a (high) similarity between \(_{i}\) and \(^{*}\) can suggest \(_{i}\)'s (high) predictive accuracy. Specifically, \(^{*}\) is implemented as follows: For an input data-label pair \((x_{j},y_{j})\), take the one-hot encoded vector \(e_{y_{j}}^{C}\) of \(y_{j}\), add an independent uniform noise \(_{j}[0,0.01]^{C}\) (to avoid degeneracy during MLE), normalize it to sum to \(1\) to yield \(e_{y_{j},_{j}}\) as a 'prediction' of the expert, and solve (1) using these 'predictions' over \(\). The predictive certainty of \(_{i}\) is encoded in the precision \(|_{i}|_{1}\) of its Dirichlet abstraction, which is then used as a weight to fuse the individual Dirichlet abstractions \(_{i}\) in \(\) to obtain \(_{}\):

**Definition 3** (**Precision-weighted Fusion)**.: Let the random vector \([_{1},,_{n}]([|_{1}|_{1}, ,|_{n}|_{1}])\) be independent of \(_{i}(x)\) for all \(i[N]\) where \(n||\). Then, the precision-weighted fusion \(_{}\) is the distribution of \(_{i=1}^{n}_{i}\ _{i}(x)\).

In Definition 3, the weight \(_{i}\) on \(_{i}(x)\) is large if \(|_{i}|_{1}\) is large, i.e., \(_{i}\) has a high precision/\(_{i}\) has a high predictive certainty. Definition 3 has an important implication: \(_{}\) is a fully specified Dirichlet, i.e., \(_{}=([_{i=1}^{n}_{i,1},,_{ i=1}^{n}_{i,C}])\) (Lemma 3 in App. A) with four advantages: (A) \(_{i}\) and \(_{}\) are both Dirichlets so that a single \(d\) (e.g., \(d_{}\)) in (3) can be used for both singleton and non-singleton \(\)'s. Interestingly, it gives a perspective that each \(_{i}\) lives as \(_{i}\) in a metric space w.r.t. \(d_{}\) (Fig. 11 in App. C). (B) We can theoretically justify learning model Shapley (Theorem 1). (C) (3) using \(d_{}\) can be evaluated in closed form with \((1)\) time, which is important since (2) requires \((2^{N})\) evaluations of \(()\) for different \(\)'s. (D) An alternative to specify \(()\) is a linear combination of the performance of \(_{i}, i\) as \(()\). However, it is unclear what the weights in this linear combination should be. In contrast, Definition 3 'automatically' resolves this issue by fusing each \(_{i}\) according to its precision \(|_{i}|_{1}\) into \(_{}\).

Interpreting MSV.Note that (2) is an average (over all possible \([N]\{i\}\)) of how much \(_{i}\) (or, more precisely, \(_{i}\)) improves the distributional similarity between \(_{}\) and \(^{*}\) (i.e., the expert) after \(i\) joins \(\). Both the predictive accuracy and certainty of \(_{i}\) can affect (2). To see this, a high predictive accuracy of \(_{i}\) implies that \(_{i}\) is (distributionally) close to \(^{*}\); a high predictive certainty of \(_{i}\) ensures its weight \(_{i}\) is large when fused into \(_{}\), so the predictive certainty can amplify \(_{i}\)'s effect in bringing \(_{}\) close to \(^{*}\) (if \(_{i}\) is close to \(^{*}\)). _Hence, a model \(_{i}\) with a high predictive accuracy and certainty is likely to have a high MSV \(_{i}\)._ When considering several models jointly , we can use \(_{i}\) to indicate how well (on average) \(_{i}\) combines with other models (i.e., whether \(_{i}\) joining \(\) leads to a performance improvement), as verified in Sec. 5. In contrast to the work of , which supports up to \(N=32\)_simpler_ binary classifiers, our approach -more scalable and general- supports up to \(N=150\)_\(C\)_-_way classifiers (Sec. 4.2).

### Connection to Cross-entropy and Other Model Evaluation Criteria

We first formalize the connection between cross-entropy (CE) and our approach that uses the Hellinger distance, and then use this connection to extend our approach to other evaluation criteria such as adversarial robustness , distributional robustness , and algorithmic fairness in ML .

As a distributional distance measure, CE can be used specify (3) because the CE loss is used to evaluate the performance of a model. Then, specifying (3) with CE or the Hellinger distance (as proposed in Sec. 4) can be connected in how they evaluate the model performance.

**Proposition 2**.: Let \(_{}()-(_{}, ^{*})\),8\(_{}^{2}()-d_{}^{2}(_{ },^{*})\) and \(H()\) be differential entropy. Then,

\[_{}()_{}^{2}()H(_{})+d_{}^{2}(_{}, ^{*}) 0\,,_{}()_{^{*}}([1+_{}^{2}()]^{2}-1)+ (C)\]

where \(_{q}[(1/q_{}-1)]/(1-2q_{})\) with \(q_{}_{z}q(z)\) for a density \(q\).

Proposition 2 shows that \(_{}()\) has upper and lower bounds that are monotonic in \(_{}^{2}\), providing some justification for using \(_{}\) (instead of \(_{}\)). Note that while Proposition 2 utilizes \(_{}^{2}\) due to a key Lemma 2 (in App. A), we adopt \(_{}-d_{}\) (Sec. 4) as \(d_{}\) satisfies the triangle inequality (for Theorem 1). Moreover, Proposition 2 confirms that \(_{}\) encodes the predictive accuracy and certainty of a model since \(_{}\) encodes the predictive accuracy and certainty (see the example in App. A).

Interestingly, this connection to CE enables the extension to adversarial robustness, distributional robustness and algorithmic fairness, with different practical motivations. For instance, adversarial robustness (in model valuation) is important to application scenarios where the model can encounter adversarial attacks in deployment . Formally, the respective objective functions objective\({}_{}\), objective\({}_{}\)[67, Equation 5] and objective\({}_{}\)[56, Definition 2] can be achieved from a suitable definition of \(\) using \(d_{}\) (precise definitions and full deviations are deferred to App. A), as summarizedin Table 1. We highlight that this illustrates the potential generality of MSVs using the Hellinger distance w.r.t. Dirichlet abstractions (i.e., using \(_{}()\) in (2)), and defer the formal treatment of such theoretical connections to future work. A question one might ask is that: (How) can multiple such evaluation criteria be combined? The answer is yes, by leveraging (P3) to _linearly combine_ selected evaluation criteria, as discussed in App. A.

## 4 Learning Model Shapley

To address challenge **(3)** in Sec. 1, we propose a learning approach to train a model appraiser (i.e., a regression learner) from the MSVs of a small subset of models for predicting MSVs of the remaining models (further elaborated in App. C). If we can learn a good appraiser with only \(20\%\) of all models (empirically verified), then the marketplace size can (theoretically) quintuple. To justify this learning approach, we derive a Lipschitz continuity of model Shapley, which is also empirically verified using \(5\) real-world datasets and various model types. Next, we implement this learning approach by training a Gaussian process regression (as the model appraiser) on a subset (from \(5\%\) to \(50\%\) in size) of \(150\) model-MSV pairs and examine its predictive performance on the rest. Our implementation is available at https://github.com/XinyiYS/ModelShapley.

### Lipschitz Continuity of Model Shapley

We derive a Lipschitz continuity of the model Shapley function \(:[N]\): The difference between the MSVs of two models \(_{i},_{i^{}}\) (i.e., inputs to \(\)) is bounded by the distance \(d_{}(_{i},_{i^{}})\) between them, multiplied by a constant factor.

**Theorem 1** (Lipschitz Continuity).: Let \(d d_{}\) in (3). Then, \( i,i^{}[N]([N]\{i,i^ {}\}\ d_{}(_{ i},_{ i^{}}) d_{}(_{i},_{i^{ }}))|_{i}-_{i^{}}| Zd_{}(_{i },_{i^{}})\).

Its proof is in App. A. Theorem 1 states that the difference in MSVs of two models is bounded by the Hellinger distance between their Dirichlet abstractions, and the constant \(Z\) from (2) is the Lipschitz constant. This is based on a simple fusion-increases-similarity condition: When \(_{i}\) and \(_{i^{}}\) are each fused with a common \(_{}\), the resulting similarity is higher (i.e., smaller \(d_{}\)) since \(_{ i}\) and \(_{ i^{}}\) have \(_{}\) in common (see Proposition 4 in App. A). Moreover, Table 3 and Fig. 6 (in App. A) empirically verify Theorem 1. Then, Theorem 1 provides a theoretical justification for the learning approach because it guarantees that similar inputs (i.e., small \(d_{}(_{i},_{i^{}})\)) imply similar outputs (i.e., small \(|_{i}-_{i^{}}|\)). namely, the model Shapley function is well-behaved w.r.t. its inputs, and hence learnable. This reasoning is applied to justify learning the data Shapley value .

### Empirical Learning Performance via Gaussian Process Regression (GPR)

To exploit the Lipschitz continuity (i.e., Theorem 1), we adopt the Gaussian process regression (GPR) due to a uniform error bound of GPR on Lipschitz continuous functions . Our implementation trains a GPR (as the model appraiser) on the MSVs of a subset of \(N=150\) models and examines its predictive performance on the remaining ones.

Regression setting.We train \(N=150\) independent models on MNIST (CIFAR-10): \(50\) of logistic regression (LR), multi-layer perceptron (MLP), and convolutional neural network (CNN) each (ResNet-18, SqueezeNet, and DenseNet-121 each). For simplicity, we use the test set _without

  Criteria & Query sets & Choices of \(\) \\  objective\({}_{}\) & \(_{},_{}\) & \(-(d_{}(,^{*};_{})+\ d _{}(,^{*};_{}))\) \\ objective\({}_{}\) & \(\{_{g}\}_{g}\) & \(-_{g}d_{}(,^{*};_{g})\) \\ objective\({}_{}\) & \(^{+}_{},^{+}_{}\) & \(-|d_{}(,^{*};^{+}_{})-d_{ }(,^{*};^{+}_{})|\) \\  

Table 1: Extension to other evaluation criteria for model valuation. The notation dependence on the query set is made explicit. \(_{},_{}\) denote the query sets containing adversarial and non-adversarial (clean) training examples. \(\{_{g}\}_{g}\) is a collection of query sets where \(_{g}\) contains training examples from a particular “group”/data distribution. \(^{+}_{},^{+}_{}\) contain positive training examples under the protected and unprotected groups, respectively.

partitioning as the query set \(\). For each \(_{i}\), we obtain \(_{i}\) via its predictions on \(\) and solve (1) to obtain \(_{i}\) as input features for separate regressions. For the regression labels, as calculating \(_{i}\) exactly incurs \((2^{150})\) time, we use the \((=0.1,=0.1)\)-approximation \(_{i}\) as the average of \(3745\) Monte-Carlo samples. This results in two sets \(\{_{i},_{i}\}\) and \(\{_{i},_{i}\}\) of model-MSV pairs of size \(150\) each. We train a GPR on a random subset of \(150\) model-MSV pairs to learn to predict the MSV on the remaining pairs. In GPR, we use the squared exponential kernel \((-d(i,i^{})/(2^{2}))\) (the lengthscale \(\) is learnt) where \(d(i,i^{}) d_{}(_{i},_{i^{}})\) for \(\{_{i},_{i}\}\), and \(d(i,i^{})|_{i^{}}-_{i}|_{1}\) for \(\{_{i},_{i}\}\).

High regression performance verifies learnability.We examine the test performance using two error metrics: mean-squared error (MSE) and maximum error (MaxE) w.r.t. varied training ratios from \(5\%\) to \(50\%\), in Fig. 2. In particular, results for training ratio of \(20\%\) are in Table 2. We observe that even using only \(20\%\) of model-MSV pairs for training, the learning is effective (i.e., low test errors), which shows its feasibility in a large-scale model marketplace. This can be attributed to the learnability justified by Theorem 1 and the uniform error bound of GPR . In addition, learning on \(_{i}\) is more effective than learning on \(_{i}\) (as Table 2 and Fig. 2 show higher errors for the latter), since the average operation to get \(_{i}\) loses some model information.

## 5 MSV vs. Common Evaluation Criteria

This work is motivated by the lack of a standardized model valuation, but there are different evaluation criteria useful in different scenarios (e.g., accuracy, F1 score, predictive certainty). Interestingly, we find that MSV can produce consistent model values with these criteria. Additionally, we evaluate the utility of MSV directly in a use case where a buyer wishes to purchase multiple models with black-box access to build a larger learner (e.g., random forest or voting classifier) [33; 66] and show that MSV can be used to effectively identify the most 'valuable' models for this purpose.

MSV vs. predictive performance.Fig. 3 (left) compares the MSVs of different model types (independently trained on the same data) for MNIST. Here \(\) consists of misclassified data (from the original test set) of all the models to highlight the difference in their predictive accuracies. Without

    & MSE & MaxE \\  \(_{i}\) & \((6.9e^{-8})}\) & \((9.7e^{-5})}\) \\ \(_{i}\) & \(8.36e^{-5}(3.1e^{-6})\) & \(1.59e^{-2}(2.6e^{-4})\) \\  \(_{i}\) & \((5.2e^{-6})}\) & \((3.9e^{-4})}\) \\ \(_{i}\) & \(3.05e^{-4}(4.5e^{-5})\) & \(3.23e^{-2}(2.4e^{-3})\) \\   

Table 2: Top (bottom) are results on MNIST (CIFAR-10) for the training ratio of \(20\%\). Average (std. error) over \(10\) random train-test splits.

Figure 2: Average (std. errors) of test performance vs. training ratios for MNIST & CIFAR-10 over 10 random train-test splits for each training ratio. Dashed (solid) lines follow the left (right) axis. Colors indicate \(_{i}\) (blue) or \(_{i}\) (orange). To elaborate, at a training ratio of \(5\%\), the GPR is trained on a random subset of \(5\%\) of the total \(150\) model-MSV pairs and its test performance on the remaining \(95\%\) of the model-MSV pairs is reported.

needing to partition \(\), we observe that CNNs significantly outperform both MLPs and LRs (in terms of accuracy) and have the highest MSVs. This is expected, since CNNs are more capable of performing well in image-based tasks, and hence the MSVs for CNNs are correspondingly higher. Then, we examine predictive certainty. We use the same CNN model type independently trained on the same MNIST data (i.e., their accuracies are essentially equal), but artificially increase the predictive certainty for some: We multiply the highest probability of \(_{i}(x_{j})\) by a factor of \(\) and then normalize the resulting vector to sum to \(1\)_without_ affecting the predicted class/accuracy. Fig. 3 (right) shows that models with higher predictive certainty have higher MSVs, confirmed by additional results on CIFAR-10, MedNIST, and DrugRe in App. C. These results confirm our intuition that a model with high predictive accuracy and certainty is likely to have a high MSV.

Next, we train \(3\) groups of \(3\) LR models independently with varying sizes of training data from \(0.001\) to \(0.01\) and \(0.1\) of the entire KDD99 dataset containing highly imbalanced data; the top \(3\) classes out of \(23\) constitute \(98.22\%\) of total data. Intuitively, the models trained on more data should perform better and thus be more valuable, but due to the class imbalance, Fig. 4 (left) shows difficulties in differentiating these models based on their accuracies (or MSVs w.r.t. the entire/unpartitioned query set). Intuitively, to differentiate them, we need a lower level (i.e., more refined) Dirichlet abstraction, namely the class-specific Dirichlet abstractions: Partition the entire query set according to the \(C=23\) classes with \(_{k}|_{k}|\). Define \(d_{}(_{i},_{i^{}};\{_{k}\}_{k=1,,C})_{k=1}^{C}_{k}\ d_{}(_{i, _{k}},_{i^{},_{k}})\) to leverage P3 to compute \(_{i}(\{_{k}\}_{k=1,,C})=_{k=1}^{C}_{k}\ _{i}(_{k})\) (right of Fig. 4). Then we can see that MSVs are indeed consistent with F1 score (a criterion especially suited for imbalanced data) _without_ explicitly using F1 score in the computation. In addition, for KDD, due to the high class imbalance, there are classes with extremely small data size (i.e., \( 5\)) and our calculation of \(d_{}(_{i},_{i^{}};\{_{k}\})\) naturally suppresses their effect (possibly inaccurate \(_{i}\)) via \(_{k}|_{k}|\). However, in practice, it should be noted that partitioning the query set to obtain a lower level of Dirichlet abstraction should be considered w.r.t. the size of available query set (Sec. 2). In other words, to obtain a lower level of Dirichlet abstraction (and thus a more refined representation), it incurs a higher cost from collecting a larger query set. In our experiments, we find that the size of each partitioned query set \(_{k}\) should contain at least \(10^{2}\) samples (e.g., for KDD most classes have at least or close to \(10^{2}\) samples).

Identifying valuable models to purchase.For a more end-to-end use case (instead of a single evaluation criterion), we evaluate the MSVs for up to \(50\) models and the performance of a larger

Figure 3: More suitable model types (left)/higher predictive certainty (right) lead to higher MSVs.

learner by including a subset of these models based on their MSVs in a highest/lowest-first sequence in Fig. 5. The larger learner is random forests (voting classifier) and models are decision trees (LeNets ) for Breast Cancer (CIFAR-10). As the test accuracy of highest MSV-first increases more quickly (orange line), it verifies our previous comment on models that perform well when combined with other models are likely to have high MSVs. This characteristic offers some practical utility. If a buyer is looking to purchase models from a marketplace , then following the highest MSV sequence, the buyer only needs to purchase a subset of \(15\) (left of Fig. 5) or \(25\) (right of Fig. 5) out all \(50\) available models, thus saving cost. More results on CIFAR-100 with ResNet-18 are in App. C.

## 6 Related Work

The work of  investigates the binary classification setting and is not suitable for empirical comparison as we consider problems with multiple classes. The works of [10; 11; 13; 51] approach the design of a model marketplace from an economics perspective by addressing issues like arbitrage (e.g., via horizontal/vertical pricing). In contrast, we formalize the value of a model via what it has learned w.r.t. a task. The black-box access setting is appealing in a model marketplace as it accommodates different model types. Some existing methods [8; 29; 30; 33; 44] focus on how to learn a fused model from several trained models (possibly with black-box access) instead of how to value these models. We highlight that we design the fusion (Definition 3) to leverage its analytic properties in Theorem 1. The approach of learning the Shapley value arises in data valuation problems but has not been considered in model valuation. Interestingly, we can draw parallels between Theorem 1 and [19, Theorem 2.8]. For brevity, we include a more extensive contrasting comparison with data valuation in App. B.

## 7 Discussion and Future Work

We exploit a _Dirichlet abstraction_ of classification models with only black-box access for proposing a novel equitable model valuation called the _model Shapley_. We discuss that choosing a suitable level of the Dirichlet abstraction can improve how accurately MSV reflects a model's predictive performance and empirically show that using the partitioned query sets (according to the classes) can provide a suitable trade-off between the level of abstraction and the size of the available query set. MSV behaves consistently (in our experiments) with some common model evaluation criteria (i.e., predictive accuracy and certainty, F1 score) and can be extended to more sophisticated criteria. This implies MSV can potentially help unify existing evaluation criteria to provide a simplified model valuation in practice, without needing to explicitly perform separate evaluations.

For future work, it is interesting to explore how model valuation can help address the practical considerations encountered in existing data valuation methods [70; 78; 80] and to apply this technique to existing collaborative (learning) frameworks which require a valuation of models, both non-parametric ones [2; 62; 72; 74; 81] and parameterized ones [18; 49; 79]. Moreover, a more detailed investigation into satisfying the equitability of Shapley value  and its trade-off with the computational cost  is of practical interest, such as by applying more sophisticated analyses  or methods [9; 31; 32; 52] for our proposed Gaussian process regression learning approach.

Figure 5: Test accuracy vs. number of models on Breast Cancer dataset (left) and CIFAR-10 (right).