# Towards Calibrated Robust Fine-Tuning of Vision-Language Models

Changdae Oh\({}^{*,c,n}\)

University of Wisconsin-Madison &Hyesu Lim\({}^{*,c}\)

KAIST AI &Mijoo Kim\({}^{c}\)

Chung-Ang University &Dongyoon Han

NAVER AI Lab &Sangdoo Yun

NAVER AI Lab &Jaegul Choo

KAIST AI &Alexander Hauptmann

Carnegie Mellon University &Zhi-Qi Cheng\(\)

Carnegie Mellon University &Kyungwoo Song\(\)

Yonsei University

Equal contribution (changdae@cs.wisc.edu; hyesulim@kaist.ac.kr), Work partly done at \({}^{c}\)Carnegie Mellon University and "NAVER AI Lab, \(\)Co-correspondence (zhiqic@cs.cmu.edu; kyungwoo.song@yonsei.ac.kr)

###### Abstract

Improving out-of-distribution (OOD) generalization during in-distribution (ID) adaptation is a primary goal of robust fine-tuning of zero-shot models beyond naive fine-tuning. However, despite decent OOD generalization performance from recent robust fine-tuning methods, confidence calibration for reliable model output has not been fully addressed. This work proposes a robust fine-tuning method that improves both OOD accuracy and confidence calibration simultaneously in vision language models. Firstly, we show that both OOD classification and OOD calibration errors have a shared upper bound consisting of two terms of ID data: 1) ID calibration error and 2) the smallest singular value of the ID input covariance matrix. Based on this insight, we design a novel framework that conducts fine-tuning with a constrained multimodal contrastive loss enforcing a larger smallest singular value, which is further guided by the self-distillation of a moving-averaged model to achieve calibrated prediction as well. Starting from empirical evidence supporting our theoretical statements, we provide extensive experimental results on ImageNet distribution shift benchmarks that demonstrate the effectiveness of our theorem and its practical implementation. Our code is available here.

## 1 Introduction

Foundation models  such as CLIP  have been extensively utilized on diverse domains via pretrain-finetune approaches. Their generalized knowledge shaped after large-scale pre-training enables them to easily adapt to downstream tasks through zero-shot inference or fine-tuning. However, it has been steadily reported that a naive fine-tuning approach comprises foundation models' strong out-of-distribution (OOD) generalization capability during adaptation to in-distribution (ID) data . To ensure robustness under distribution shifts, a wide range of research has followed  so-called _robust fine-tuning_. Despite the advancements of the robust fine-tuning methods, we are aware that an important criterion for trustworthy machine learning has been overlooked - _confidence calibration_ that quantifies how close the confidence of our predictor is to the actual correctness of predictions. As shown in Figure 1, existing robust fine-tuning methods hurt the confidence calibration in terms of expected calibration error (ECE)  on OOD data compared to the zero-shot evaluation while they show improvements on OOD accuracy. In thiswork, we introduce a calibrated robust fine-tuning method (**CaRot**) that simultaneously improves confidence calibration and accuracy of the classifier on OOD data.

Confidence calibration is a key aspect of reliable machine learning, essential for avoiding high-confidence incorrect predictions in real-world decision-making systems. This is particularly crucial in high-stakes tasks like autonomous driving and healthcare applications. After a seminal work  revealed the miscalibration problem of high-performing neural networks, a plethora of attempts followed to improve the calibration of neural network models through post-hoc adjustments [65; 18; 29; 68; 19] or train-time regularizations [67; 52; 56; 38; 37]. However, many of them focus on improving calibration for ID samples, and methods for enhancing OOD calibration usually require OOD samples at train time [63; 16]. Moreover, these approaches commonly focus on calibration alone without ensuring improvement in other quantities, e.g., accuracy. In this work, we explore a unified framework that jointly considers calibration and accuracy (particularly on OOD data).

To accomplish low classification and calibration errors on OOD samples with only ID samples in our hands, we conduct theoretical analyses of those OOD errors. To be specific, we derive an upper bound that is shared for OOD classification error and OOD calibration error composed with two quantities on ID samples, 1) _the reciprocal of the smallest singular value of the normalized covariance matrix of ID data representation_ and 2) _the ID calibration error_. Different from the existing bounds focusing on either one of classification or calibration error [5; 72; 63], we address both classification and calibration errors in a single unified bound. More importantly, the chief components of our bound can be computed solely with ID samples without relying on any OOD samples, which discerns our approach to existing work .

Motivated by our theoretical analysis, we propose a **new multimodal contrastive loss that promotes the smallest singular value of input image representation to become larger** by enforcing the orthogonality of the final projection matrix of the visual encoder. Furthermore, to understand the working mechanism in depth, we present an interpretation of our new multimodal contrastive loss as a process of seeking the low-rank approximation of cross-covariance matrix over image-text representations on a reduced solution space induced by the orthogonality constraint. Meanwhile, to **enhance confidence calibration on ID samples during fine-tuning**, we utilize a self-distillation (SD) with an exponential moving average (EMA) teacher model. This EMA SD encourages a student model to learn semantic similarity structures of in-batch data representations from teacher predictions across diverse contrastive pairs, appropriately adjusting confidence per instance.

We first validate our new error bounds with synthetic datasets to show that the bounds hold empirically. Then, evaluate our method by conducting extensive experiments of fine-tuning CLIP  on ImageNet-1K  classification task under natural distribution shift (ImageNet-V2/R/A/Sketch and ObjectNet) and synthetic distribution shift (ImageNet-C). We demonstrate the effectiveness of our proposed framework for robust generalization and calibration by observing consistent improvements in terms of expected calibration error and accuracy on ID/OOD datasets.

Figure 1: **OOD accuracy vs. ID accuracy (left) and negative OOD ECE (right).** To maintain consistency in the plots, where desired values are shown on the right side of the x-axis, we report negative OOD ECE. ID ACC refers to ImageNet-1K top-1 accuracy; OOD ACC and ECE refer to the averaged accuracy and ECE of the five ImageNet distribution shifts (ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet), respectively. Detailed numbers are reported in Table 2 and 3. Note that the competing methods – FLYP , LP-FT , and Lipsum-FT  – improve OOD accuracy over the zero-shot baseline (ZS) and naive fine-tuning (FT) but suffer from OOD miscalibration, presumably due to concerning generalization solely during fine-tuning. Our CaRot outperforms existing methods on both OOD accuracy and calibration by large margins.

**Summary of contributions.** 1) We point out that existing fine-tuning methods do not adequately achieve satisfactory OOD generalization and calibration simultaneously. 2) We provide theoretical analysis for classification and calibration errors on the OOD data and show that they are both bounded from above by the ID calibration error and the smallest singular value of the covariance matrix over the ID input representation. 3) Based on our theoretical analyses, we devise a calibrated robust fine-tuning method, CaRot, as a practical realization of our theorem that reduces the upper bound of OOD classification and calibration errors by conducting constrained multimodal contrastive learning with EMA self-distillation. 4) We present empirical evidence for our theory on a synthetic dataset and demonstrate the efficacy of CaRot via extensive evaluations on ImageNet-1K distribution shifts in terms of accuracy and calibration error on ID and OOD domains.

## 2 Preliminary

**Robust fine-tuning** aims to achieve consistently high performance on data from both training distribution (ID) and related but different test distributions (OOD). For validation, we commonly consider a covariate shift scenario for the classification task, where both ID and OOD domains share the class categories (\(_{}=_{}\)) and have the same conditional distribution \(P(Y|X)\), but have different marginal distributions over input \(X\). That is, \(P_{}(Y|X)=P_{}(Y|X)\) but \(P_{}(X) P_{}(X)\). Here, we evaluate a model that is fine-tuned on a training split of the ID domain, on a test split of ID, and on OOD domains. The term "OOD" is quite general, and we confine the scope of OOD to transformed and related distributions with ID . For example, if our ID data is about an object recognition task with images, OOD data is about a sensor-noised version of ID, or independently collected data from a different domain targeting the same task.

**Confidence calibration** is a concept of matching the prediction probabilities yielded for different inputs to the expected accuracy on these inputs. In a \(K\)-way classification setting, let \(X^{d}\) and \(Y\{1,...,K\}\) be random variables indicating inputs and labels, respectively. A dataset with \(N\) independent samples from the joint distribution \(P(X,Y)=P(Y|X)P(X)\) is denoted by \(\{(x_{n},y_{n})\}_{n=1}^{N}\). Let \(f\) be a classifier and \(f(y|x)=\) be a confidence, i.e., the maximum of probabilities among \(K\) dimensions corresponding to its prediction \(\). We say a model is _perfectly-calibrated_ when \((=y|=p)=p,\; p\). As a quantitative measure, the model calibration can be derived as \([|(=y|=p)-p|]\). In practice, we use expected calibration error (ECE)  as an empirical approximation of the model calibration, which is a weighted average of bin-wise miscalibration. The ECE divides the confidence score of \(N\) samples into \(M\) uniform confidence bins \(\{B_{m}\}_{m=1}^{M}\) and takes the mean of the gap between accuracy (acc) and confidence (conf) over the bins weighted by the number of samples in the bins, i.e., \(=_{m=1}^{M}|}{N}|(B_{m})-(B_{m})|\).

## 3 Theoretical Analysis on OOD Generalization and Calibration

We first identify the factors that affect OOD generalization and calibration errors under circumstances where only ID data is accessible. We take inspiration from the generalization bound of domain adaptation literature [5; 72] while remarkably adapting the analysis to consider both OOD classification error and OOD calibration error at the same time in a more practical way.

Let \(\) be a domain on input space \(\) and \(=\{0,1\}\) be a label space for a binary classification. Among the sufficiently expressive hypothesis functions \(h:\) in a class \(\), we define \(h_{0}()\) as a desired calibrated predictor for \(y\), which minimizes the calibration error \(_{x}[(h(x)-c(x))^{2}]\), where \(c(x)=_{y}[y|h(x)]\) is the expected value of \(y\) given a prediction \(h(x)\). That is, \(h_{0}\) always produces the calibrated prediction for \(y\) given \(x\) so that the output confidence \(h_{0}(x)\) matches the expectation of \(y\) over the subset of samples that have the same confidence value with \(h_{0}(x)\). Our goal is to learn a hypothesis function \(h()\) that outputs reliable prediction probability on samples from the unseen OOD domain, which is defined by a distribution \(_{}\), as well as on the ID domain \(_{}\), where the predictor is trained on. In essence, the error \(_{}(h)=_{x}[(h(x)-h_{0}(x))^{2}]\) should be small for two different domains \(\{_{},_{}\}\). Here, we focus on the covariate shift scenario (SS2) that only the marginal distribution over \(\) changes while the distribution over \(\) is preserved.

Let the optimal hypothesis \(h^{*}\), which minimizes a combination of errors on both ID and OOD, be \(h^{*}:=_{h}_{_{}}(h)+ _{_{}}(h)\), and \(\) denote the optimal joint error \(:=_{_{}}(h^{*})+_{_{ }}(h^{*})\). Now, we derive a new bound for the OOD calibration error and OOD classification error.

**Theorem 3.1**.: _Let \(h:\) be a real-valued function of structure \(h(x)=_{i=1}^{d}h_{i}(x[i])\) where \(h_{i}\) is an arbitrary one-dimensional function, and \(h\) is in a hypothesis class \(\) that has pseudo dimension \(dim()=d_{h}\), \(}_{}\) be an \(N\)-size empirical distribution on ID domain. If \((x,...,x[d])\) have matching marginals for ID and OOD, and \((x[i],x[j])\) is a bi-variate Gaussian for every \(i,j[d]\), then for any \((0,1)\) and for all \(h\), the following bounds hold with probability at least \(1-\):_

\[ i)\ _{_{}}(h) _{}_{}}(h)+}( _{_{}})}++(}^{d_{h}}()}})\] (1)

\[ ii)\ _{_{}}[(h(x)-y)^{2}]+ _{_{}}[c(x)^{2}]-1_{}_{}}(h)+}(_{ _{}})}++(}^{d_{h}}()}})\] (2)

where \(_{_{}}{:=}_{_{ }}[^{T}]\) is a covariance matrix with a strictly positive minimum singular value of \(d\)-dimensional normalized input \(=(,...,[d])\), where \([i]{:=}(x[i]-[x[i]])(x[i])^{-1/2}\) and \(_{}(M)\) is the smallest singular value of a matrix \(M^{d_{1} d_{2}}\). These theoretical results can be directly applied to the intermediate or penultimate layer's representation of a neural network by setting the input variable \(x\) as a representation vector as in . From now on, we will assume the input as an image representation from the last layer of the visual encoder in the following sections. Note that 1) the LHS of the first inequality (ineq.(11)) indicates **OOD calibration error**, 2) two terms in the LHS of the second inequality (ineq.(12)) denote **OOD classification error** in terms of \(L_{2}\) loss and prediction sharpness on OOD domain, and 3) both inequalities have the same RHS, which contains the empirical estimate of **ID calibration error**, the reciprocal of the **smallest singular value of ID input covariance matrix**, and remaining irreducible terms that depend on the problem setup. Intuitively, Theorem 3 implies that _pursuing diverse input features while maintaining the calibration of the classifier contributes to improving OOD calibration and generalization simultaneously_. We defer the proof and discussion on the tightness of the bound and its assumptions in Appendix C.

Based on our analysis, we expect the potential of reducing the upper bound of OOD calibration error and the sum of OOD classification error and prediction sharpness by minimizing the first two terms of RHS in both bounds: the empirical ID calibration error and the reciprocal of minimum singular value of the normalized ID covariance matrix. In SS4, we devise a realization of this theoretical concept.

## 4 Method

Our goal is to achieve good OOD generalization and calibration during the ID adaptation of pre-trained models. Motivated by Theorem 3, we propose a new fine-tuning method that increases the smallest singular value of the ID input covariance matrix while improving ID calibration, thereby lowering the upper bound of OOD calibration and generalization errors. By following , we set a vision-language model (VLM), CLIP  as our target, which serves as a remarkably strong backbone for zero-shot inference and fine-tuning with ease. We limit the scope of validation to image classification tasks. Note that our theorem is not confined to specific domains or model architectures and thus can be applied beyond CLIP's image classification. See Figure 2 for an overview.

### Robust fine-tuning with constrained multimodal contrastive learning

To adapt a pre-trained VLM on image classification tasks, the cross-entropy loss is the most common choice as an objective function. However, there are emerging shreds of evidence supporting the use of contrastive loss (CL) for robust adaptation , especially when the model is pre-trained via CL. Witnessing its empirical success on OOD generalization , we leverage a CL-based learning strategy for VLM fine-tuning. CLIP consists of an image encoder \(f_{_{v}}()=f_{_{v}}()W_{v}\) and a text encoder \(g_{_{l}}()=g_{_{l}}()W_{l}\), where encoders are composed with backbone models \((f_{_{v}}(),g_{_{l}}())\) and projection matrices \((W_{v}^{d_{v} r},W_{l}^{d_{l} r})\). The encoders produce \(L_{2}\)-normalized representations to compute the similarity between image and text inputs. Given \(N\) pairs of (image, text) \(\{(I_{i},T_{i})\}_{i=1}^{N}\), a common form of multimodal contrastive loss (MCL) can be written as

\[_{}()&:=_{i=1}^{N}-}(I_{i}) g_{_{l}}( T_{i}))}{_{j=1}^{N}(f_{_{v}}(I_{i}) g_{_{l}}(T_{j}))}\\ &+_{i=1}^{N}-}(I_{i})  g_{_{l}}(T_{i}))}{_{j=1}^{N}(f_{_{v}}(I_{j}) g_{ _{l}}(T_{i}))}+R(_{v},_{l}),\] (3)where \(=\{_{v},_{l}\}\) are the parameters of image and text encoders and \(R(_{v},_{l})\) reflects a general regularization strategy in CL [7; 21]. We update both image and text encoders during fine-tuning as done in the pre-train phase and use OpenAI templates  to create (image, text) pairs from a downstream classification dataset that consists of (image, class) pairs.

Meanwhile, the basic form of \(_{}\) does not inform anything about the singular value distribution of learned representation. In SS3, we showed that the reciprocal of the smallest singular value constitutes the shared upper bound, i.e., the larger the smallest singular value is, the lower the upper bound becomes. To encourage this, we put a soft constraint term to \(_{}\) that enforces the final projection matrix \(W_{v}\) of the visual encoder to be orthogonal and hence the output image representation matrix to have a large effective rank, as in below:

\[_{}():=_{}()+_ {}_{}(W_{v}),_{}(W_{v}) =||W_{v}^{T}W_{v}-||_{F}^{2},\] (4)

where \(\) is an identity matrix that has the same shape with \(W_{v}^{T}W_{v}\) and \(_{}\) is a strength of the orthogonality constraint1. While recklessly increasing the singular values might hinder ID adaptation, our orthogonal constraint mitigates the degradation of performance by pursuing not only the smallest singular values to be large but also the largest singular values to be small which is important for generalization on ID data . Interestingly, this contrastive loss with regularization terms can be viewed as a constrained singular value decomposition (SVD) with a cross-covariance matrix of image-text representations where the orthogonality constraint is applied.

To be specific, by following Nakada et al. , under a linear representation assumption, a gradient descent step of \(_{}\) boils down to the maximization of the SVD objective, which aims to find a low-rank approximation of the normalized cross-covariance matrix \(S()\)2 as follow:

\[*{arg\,min}_{W_{v},W_{l}}_{}( W) :=_{i=1}^{N}-_{i} W _{l}_{i})}{_{j=1}^{N}(W_{v}_{i} W_{l}_{j})}\] \[+_{i=1}^{N}-_{i} W _{l}_{i})}{_{j=1}^{N}(W_{v}_{j} W_{l}_{i})}+R (W_{v},W_{l})+_{}||W_{v}^{T}W_{v}-||_{F}^{2}\] \[ *{arg\,max}_{W_{v},W_{l}}(S()):= *{tr}(W_{v}^{T}S()W_{l})-(/2)||W_{v}W_{l}^{T}||_{F}^{2} ||W_{v}^{T}W_{v}-||_{F}^{2}=0,\]

where we adopt \(R(W_{v},W_{l})=(/2)||W_{v}W_{l}^{T}||_{F}^{2}\) for \(>0\) as a regularization term to promote the encoders to capture diverse features as in Ji et al. . Here, we assume that the input of \(_{}\) is the penultimate representation of VLM's encoders, i.e., \((_{i},_{i})=(f_{_{v}}(I_{i}),g_{_{i}}(T_{ i}))\), and

Figure 2: **Overview of CaRot.** We fine-tune a VLM using a multimodal contrastive loss with an orthogonality constraint on visual projection layer (eq.(4)) and self-distillation \(_{}\) (eq.(5)) that takes predictions of EMA teacher \(\) as soft target labels to train the student model \(\). The darker and the lighter elements denote values closer to 1 and 0, respectively. Both teacher and student models share identical VLM architecture consisting of image \(f_{_{v}}:=[f_{_{v}}\,;\,W_{v}]\) and text \(g_{_{l}}:=[g_{_{}}\,;\,W_{l}]\) encoders, where \(W\) is the last projection layer. Given (image, text) pair data, the model outputs the pair-wise similarity score for in-batch image-text representations.

\(W=\{W_{v},W_{l}\}\) is a set of projection matrices (i.e., last layers of each encoder). This connection between \(_{}\) and SVD allows us to understand the working mechanism of our proposed objective. That is, minimizing \(_{}\) can be interpreted as finding a good rank-\(r\) (dimensionality of the image-text projection space) approximation of the cross-modal covariance matrix by seeking the direction of large co-variation among image-text representations, while the solution space is constrained by enforcing an orthogonality condition on the collection of vision-side singular vectors \(W_{v}\) to achieve the larger effective rank of both the projection matrix \(W_{v}\) and the image representation matrix (See Appendix SSB for further explanation on the effective rank and the smallest singular value). In SS5, we validate that \(_{}\) significantly increases the smallest singular values and results in better OOD generalization and calibration on downstream tasks.

### Calibration during robust fine-tuning

In the previous section, we devise a new multimodal contrastive loss that promotes large \(_{min}(_{_{}})\). We now address the next component standing for ID calibration, which is another crucial component according to our theoretical analysis. While there are numerous approaches to enhance calibration during neural network training [38; 56; 52; 1], we notice the promising results of knowledge distillation-based calibration approaches [64; 71]. These approaches encourage the model to learn from input-dependent smoothed labels that effectively mitigate the overconfidence issue, which is commonly associated with miscalibration. Therefore, we employ a self-distillation (SD) method for ID calibration in that distilling the similarity score map would help avoid overconfidence.

Specifically, we first initialize both teacher and student networks with a pre-trained CLIP model (including both image and text encoders); update the student model using gradient descent for every iteration while slowly updating the teacher model that has \(=\{_{v},_{l}\}\) as parameters using EMA with the momentum of \(\) at every \(t>1\) iteration, i.e., \(+(1-)\). Rather than hosting another VLM or fixed pre-trained CLIP as a teacher model, we adopt a self-evolving EMA network as a teacher, observing its successful usage on the weight-space ensemble between homogeneous models [61; 48], robust self-supervised learning methods [2; 45], as well as regularization . With the EMA teacher \(\{\{f_{_{v}}(),g_{_{l}}()\}\) and the learning student \(\{f_{_{v}}(),g_{_{l}}()\}\), we construct a self-distillation loss term for \(N\) data pairs as:

\[_{}():=_{i=1}^{N}[KL(_{i}^{ I}||q_{i}^{I})+KL(_{i}^{T}||q_{i}^{T})],\] (5)

where \(KL\) denotes Kullback-Leibler divergence, \(q_{i}^{I}=(\{f_{_{v}}(I_{i}) g_{_{t}}(T_{j}) \}_{j=1}^{N})\) and \(q_{i}^{T}=(\{f_{_{v}}(I_{j}) g_{_{l}}(T_{i}) \}_{j=1}^{N})\) are student outputs, and \(_{j}^{I}\) and \(_{j}^{T}\) are teacher outputs which are similarly defined by replacing the student parameter \(\) with that of teacher's \(\). Presumably, label smoothing (LS)  behaves similarly to what we intended, but we argue that LS would be less effective than EMA SD in terms of mitigating overconfidence issues. See Appendix SSB.

We complete the learning objective as a summation of \(_{}\) and \(_{}\) with a coefficient \(_{}\), i.e., \(=_{}+_{}_{ }+_{}_{}\). The novel combination of these two components contributes to ensuring a larger smallest singular value of image representation and ID calibration simultaneously, which induces calibrated robust fine-tuning (**CaRot**) on distribution shifts. Note that this objective function is just one of the possible realizations of our upper bound (Theorem 3) on OOD generalization and calibration errors. Further exploration can spawn a more practical algorithm in the future.

## 5 Experiments

In SS 5.1, we first show empirical evidence of the error bounds that we derived in SS3. We then provide experimental setup and main benchmarking results (SS5.2) and present further empirical studies (SS5.3).

### Numerical analysis on error bounds

Our theoretical analysis in SS3 revealed the possibility of managing OOD classification and calibration errors simultaneously by leveraging a shared quantity over the ID domain (sum of calibration error term and the singular value term). Before conducting real-world evaluations, we verify the theoretical analysis with a toy experiment that simulates distribution shifts. To be specific, we generate a binary classification dataset with 1000-dimensional Gaussian random variables as features where the mean of features are partly shifted across different test environments (ID, OOD). We train a three-layer network with regularization terms: Born-Again-Network (BAN)-style self-distillation  for calibration (\(_{}\)) and the orthogonal constraint (shown in eq.(4)) for the singular value (\(_{}\)). Detailed descriptions of the experimental setup are provided in SSA.1.

Figure 3 visualizes the results of Pearson correlation analysis between the average of \(1/_{min}(_{_{}})\) and ECE from ID samples and OOD MSE/ECE over 111 trained models. Here, we observe strong correlations between the average of \(1/_{min}(_{_{}})\) and ID ECE (x-axis), and OOD classification and calibration errors (y-axis). Additional results on the best models per each regularization term are showcased on the Table 1, which also indicates that reducing the upper bound results in better OOD generalization and calibration. These analyses demonstrate that Theorem 3 empirically holds.

### Evaluation on distribution shift benchmarks

**Training and evaluation.** We adopt CLIP ViT-B/16 as our VLM backbone and evaluate each fine-tuning method, including CaRot, in terms of calibration (with ECE) and accuracy under distribution shifts. For downstream tasks, we consider the ImageNet-1K (IN) classification and regard it as our ID domain. For all methods, we optimize the model parameters using the AdamW with a batch size of 512 over 10 epochs. Fine-tuned models are evaluated under varying distribution shifts.

**Benchmark datasets.** We consider IN-V2 , IN-R , IN-A , IN-S , and ObjectNet  as natural shifts of the in-distribution dataset (IN). We refer to the average performance over these five datasets as Avg. Shifts or OOD throughout the following sections unless it is specified as a different dataset, e.g., IN-C  which we adopt as a synthetic shift scenario occurred by sensory noises.

   Method & IN\(\) & IN-V2\(\) & IN-R\(\) & IN-A\(\) & IN-S\(\) & ObjectNet\(\) & Avg. shifts\(\) \\  ZS & 0.0570 & 0.0548 & 0.0541 & 0.0967 & 0.0850 & 0.0780 & 0.0736 \\  FT & 0.0884 & 0.1468 & 0.1164 & 0.3000 & 0.2544 & 0.2753 & 0.2186 \\ LP-FT & 0.0505 & 0.0894 & 0.0613 & 0.2051 & 0.1659 & 0.2124 & 0.1468 \\ FLYP & 0.0635 & 0.1171 & 0.0967 & 0.2435 & 0.2200 & 0.2383 & 0.1836 \\ Lipsum-FT & 0.0384 & 0.0516 & 0.0426 & 0.1290 & 0.1023 & 0.1315 & 0.0914 \\ CaRot (Ours) & 0.0470 & 0.0367 & 0.0575 & 0.1240 & 0.0699 & 0.1075 & 0.0791 \\   

Table 3: **ImageNet ECE.** Along with Table 2, we report the ECE on ImageNet and its distribution shifts to compare with other fine-tuning methods, which demonstrates our out-of-distribution (OOD) calibration performance. The best and the second-best in each column are underlined (See Figure B for details).

   Method &  &  \\  & \(_{min}\) (\(\)) & ECE (\(\)) & MSE (\(\)) & ECE (\(\)) \\  Baseline & 2.0887 & 0.1666 & 0.2581 & 0.2477 \\ \(_{}\) & 4.9630 & 0.1528 & 0.1932 & 0.1781 \\ \(_{}\) & 3.1354 & 0.1308 & 0.2170 & 0.1720 \\ \(_{}\), \(_{}\) & 6.5961 & 0.1391 & 0.1877 & 0.1596 \\   

Table 1: The best case values of two terms of RHS (ID \(_{min}\) and ID ECE) and LHS – OOD errors (MSE and ECE) in the bounds of Theorem 3. Reported values are an average of three repeated runs.

Figure 3: **Analysis of error bounds on synthetic data.** Plots on the left side show RHS (x-axis) and LHS (y-axis: MSE for ineq.(12) and ECE for ineq.(11)) of the inequalities in §3. We denote MSE for the mean squared error, \(_{}\) for the singular value regularization, and \(_{}\) for the calibration regularization.

**Baseline methods.** We benchmark CaRot alongside zero-shot inference (ZS) and fine-tuning methods: standard fine-tuning (FT), LP-FT , FLYP , and Lipsum-FT . Refer SSA for further details. In Appendix Table B, C, and D, we compare results with post-hoc robustification method (weight ensemble; WiSE-FT ) and post-hoc calibration (temperature scaling; TS ) method.

**Results on natural shifts.** Table 2 and 3 highlight our argument that CaRot significantly enhances both generalization and calibration on OOD data. Under distribution shifts from IN to -V2, -R, -A, -S, and ObjectNet, CaRot favorably compares with the existing best fine-tuning methods by margin of 1.51 and 0.0123 for OOD top-1 accuracy and ECE, respectively, averaged over five shifted datasets. See reliability diagrams in Appendix Figure B for deeper insight on calibration. We further report the results with different backbone models, RN50 and ViT-L/14, in Table 7 (See Table H and I for details). CaRot consistently outperforms the baseline methods for these backbones, too. Furthermore, in Table 6, we provide additional comparisons with CAR-FT , Model Stock  and ARF 3.

**Results on synthetic shifts.** In real-world applications, distribution shifts are commonly occurred by sensory noises. To evaluate different fine-tuning methods under such synthetic shifts, we adopt a corrupted version of ImageNet (IN-C) with 15 types of image corruptions over five severities. In Figure 4, we provide corruption-wise accuracy and ECE of each method averaged by five severities. Overall, CaRot consistently outperforms the baseline methods and the actual amount of improvements varying depends on the type of corruptions. Specifically, on the relatively coarser granular corruptions such as Snow, Frost, Fog, Brightness, and Contrast greatly change the semantics of the image (similar to natural shift), CaRot shows remarkably good performance compared to others. Meanwhile, on the finer granular corruptions such as Elastic transform and JPEG compression, the improvements achieved by CaRot become smaller. We present zoom-in results on these two cases in Figure 5.

### Further empirical studies

**Ablation study.** In Table 4, we provide results of the ablation study to show the impacts of each component of CaRot. In line with our hypothesis, results confirm that all three components boost OOD accuracy and calibration performance. The comparison of adopting and not adopting \(_{}\) (we followed the naive fine-tuning approach for the latter) ascertains that employing contrastive loss as a fine-tuning objective is superior to cross-entropy loss for ID/OOD accuracy, consistent with the previous observations , and even extends to improvements in calibration as well. The ablations of

Figure 4: **IN-C corruption-wise accuracy (top) and ECE (bottom). We evaluate accuracy and ECE over 15 types of image corruption with five corruption severity and report the average performance per corruption. CaRot consistently outperforms baseline methods across diverse corruptions.**

Figure 5: **Closer look at the effectiveness of CaRot on different corruptions. We provide IN-C accuracy on brightness (left) and elastic transform (right) corruptions. CaRot excels on the coarser corruption such as brightness whereas its effectiveness is weakened on the finer corruption such as elastic transform.**the orthogonality constraint and adopting self-distillation validate our rationale behind adding the terms to our learning objective, where we expect them to lower the upper bound of OOD classification and calibration errors. Together, constraining the singular values of image representation on MCL and distilling EMA teacher's predictions show the best results which aligned with results from the demonstration of error bounds in Fig 3. We speculate that learning diverse features by the singular value regularization while being enforced to contribute to reflecting the in-batch similarity structure by EMA SD induces well-restricted solution space  otherwise has risk converged to bad solutions (learning diverse features but noise-sensitive). Besides, Table 5 shows the impact of each component by varying the strength coefficients. We observe that the increased intensity of constraint improves ID and OOD performance to some degree, but there is a slight decline when the intensity is too strong. Meanwhile, the strength of self-distillation positively correlated with OOD accuracy and ECE, but there are negative effects on ID accuracy and ECE, which reflects the inevitable trade-off between ID adaptation and OOD generalization (Table G and F in Appendix provide further details).

**Analysis on singular values.** Figure 6 illustrates the last 20 singular values of the covariance matrix \(^{T}\) where \(\) is a standardized image representations over \(N\) samples. Our proposed constrained contrastive loss \(_{}\) increases the small singular values compared to the vanilla contrastive loss \(_{}\). This result verifies that adding the orthogonality constraint successfully reduces \(1/_{}(_{_{}})\), the component of the shared upper bound we derived in SS3, following our intention.

    &  &  &  &  \\ \(_{}\) & Acc.\(\) & ECE\(\) & Acc.\(\) & ECE\(\) & \(_{}\) & Acc.\(\) & ECE\(\) & Acc.\(\) & ECE\(\) \\ 
0.0 & 83.03 & 0.0523 & 62.28 & 0.0772 & 0.0 & 82.51 & 0.0651 & 59.51 & 0.1803 \\
0.1 & 83.18 & 0.0511 & 62.42 & 0.0779 & 0.5 & 83.07 & 0.0482 & 61.38 & 0.1377 \\
0.2 & 83.13 & 0.0470 & 62.55 & 0.0791 & 1.0 & 83.23 & 0.0388 & 62.21 & 0.0997 \\
0.5 & 83.04 & 0.0478 & 62.44 & 0.0798 & 1.5 & 83.13 & 0.0470 & 62.55 & 0.0791 \\
1.0 & 83.09 & 0.0499 & 62.49 & 0.0781 & 2.0 & 82.72 & 0.0634 & 62.54 & 0.0781 \\   

Table 6: **ImageNet Acc. (except ObjectNet) with additional baselines.**

   \(_{}\) & \(_{}\) & \(_{}\) & ID Acc.\(\) & ID ECE\(\) & OOD Acc.\(\) & OOD ECE\(\) \\  - & - & - & 81.53 & 0.0884 & 57.50 & 0.2186 \\ - & ✓ & - & 81.45 (-0.08) & 0.0874 (-0.0010) & 59.10 (+1.60) & 0.2051 (-0.0135) \\ - & - & ✓ & 82.18 (+0.65) & 0.0601 (-0.0283) & 60.73 (+3.23) & 0.1698 (-0.0488) \\ - & ✓ & ✓ & 82.20 (+0.67) & 0.0634 (-0.0250) & 60.11 (+2.61) & 0.1762 (-0.0424) \\  ✓ & - & - & 82.69 & 0.0635 & 59.40 & 0.1836 \\ ✓ & ✓ & - & 82.51 (-0.18) & 0.0651 (+0.0016) & 59.51 (+0.11) & 0.1803 (-0.0033) \\ ✓ & - & ✓ & 83.03 (+0.34) & 0.0523 (-0.0112) & 62.28 (+2.88) & 0.0772 (-0.1064) \\ ✓ & ✓ & ✓ & 83.13 (+0.44) & 0.0470 (-0.0165) & 62.55 (+3.15) & 0.0791 (-0.1045) \\   

Table 4: **Ablation study on CaRot components.** We report accuracy and ECE on ImageNet (ID) and its distribution shifts (OOD). OOD values are averaged over five shifts. Values in brackets indicate the performance difference compared to the first row of each sub-table, and the dark green highlights the positive improvement.

## 6 Related Work

**Robust fine-tuning for visual foundation models.** Beyond the ID generalization, there are a lot of works aiming at improving the generalization of fine-tuned models on the OOD domain. Some of them leverage the strong robustness of pre-trained model through weight-average [61; 26; 53; 44] or regularization [53; 57; 58; 42] whereas others attribute to the robustness during fine-tuning from different part of model backbone [30; 32]. Besides, Goyal et al.  claims that aligning the learning objective during pre-training and fine-tuning is crucial for retaining the remarkable OOD generalization capability of the pre-trained model. Although the above methods have provided insights into the extrapolation of foundation models regarding accuracy, confidence calibration has been unexplored, which is crucial for reliable ML applications. We investigate the OOD calibration of fine-tuned CLIP as well as accuracy and propose a unified fine-tuning strategy with theoretical support to achieve superior ID and OOD calibration alongside OOD generalization for the first time.

**Confidence calibration.** After some early research on calibrated prediction [39; 8], lots of follow-up studies have been conducted. As a seminal work, Guo et al.  revealed the miscalibration problem of neural networks, then, Minderer et al.  and LeVine et al.  provided a comprehensive analysis on the calibration of modern vision models with consideration on distribution shift. To improve the calibration of predictive models, Temperature Scaling (TS)  and Label Smoothing (LS)  are two representative methods in practice. TS-based approaches learn a temperature parameter itself [18; 16] or model [63; 28] to estimate the temperature to adjust the output probability of models, whereas LS-based methods focus on producing soft labels to mitigating overconfidence issues by a fixed [55; 38], randomized , or model-based [71; 66] smoothing strategies. However, existing approaches do not consider distribution shifts , assume accessibility to target domain [16; 63], assume specific type of distribution shift , cannot adjust confidences individually [18; 55]. In this work, we adopt EMA self-distillation as an effective input-dependent calibration method and show that the superior calibration results on in-domain samples can be transferred to other domains (without data from those domains) by pursuing the larger smallest singular value together.

## 7 Conclusion and Discussion

While there have been numerous research endeavors to improve reliability during the model adaptation in the wilds, almost all of them meet the desired criteria only in half: OOD generalization or confidence calibration. This work attempts to address both OOD generalization and OOD calibration in a single framework. We first derive a shared upper bound for OOD classification and calibration errors which is constructed with the ID calibration error and the smallest singular value of ID input representation. We then devise a novel fine-tuning method CaRot, which promotes a larger smallest singular value and calibrated prediction through constrained multimodal contrastive loss and self-distillation. Our theoretical statements and proposed method are empirically validated through extensive experiments.

**Limitation and future work.** Due to resource constraints, our research reached the scale of ViT-L. Exploring validation on the larger models such as ViT-G or ViT-H where the assumptions behind our theory become more realistic would be necessary. Our scope of validation was also limited to CLIP-like VLMs, but Theorem 3 is not specific to VLMs, and investigating the applicability to other types of models such as language models would be an exciting future work direction.

**Impact statement.** Our method enhances the foundation models' reliability in multiple dimensions - accuracy and confidence calibration, and many downstream applications for society can enjoy benefits from the improved reliability. However, inherent biases learned from ID data can not be removed with our method, and thus may have risk raising potential harms in real-world applications.

    &  &  &  &  \\  & Method & Acc.\(\) & ECE\(\) & Acc.\(\) & ECE\(\) & Acc.\(\) & ECE\(\) & Acc.\(\) & ECE\(\) \\   & ZS & 59.83 & 0.0624 & 42.52 & 0.0955 &   } &   } &   } &   } &   } &   } &   } &   } &   } \\    & FT & 76.21 & 0.0983 & 41.97 & 0.2804 & & & & 85.26 & 0.0993 & 65.98 & 0.2036 \\  & LP-FT & 76.25 & 0.1042 & 41.62 & 0.3274 & & & & 84.74 & 0.1056 & 64.11 & 0.2521 \\  & FLYP & 76.16 & 0.0516 & 42.70 & 0.2127 & & & & 86.19 & 0.0729 & 71.44 & 0.1470 \\  & CaRot (Ours) & 76.12 & 0.0471 & 42.71 & 0.1714 & & & & 86.95 & 0.0349 & 74.13 & 0.0737 \\   

Table 7: **ImageNet accuracy and ECE on different backbones. We provide summarized results on CLIP RN50 and ViT-L/14. The best and the second-best in each column are underlined. (See Table H and I for details.)**