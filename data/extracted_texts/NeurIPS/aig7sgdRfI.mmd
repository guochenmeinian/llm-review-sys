# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:2]

Provable score estimation.There is a rich literature giving Bayes-optimal algorithms for various natural denoising problems via methods inspired by statistical physics, like approximate message passing (AMP) (e.g. ) and natural gradient descent (NGD) on the TAP free energy . The abovementioned works  (see also ) build on these techniques to give algorithms for the denoising problems that arise in their implementation of stochastic localization. These works on denoising via AMP or NGD are themselves part of a broader literature on variational inference, a suitable literature review would be beyond the scope of this work, see e.g. .

We are not aware of any provable algorithms for score estimation explicitly in the context of _distribution learning_. That said, it may be possible to extract a distribution learning result from . While their algorithm was for sampling from the Sherrington-Kirkpatrick (SK) model given the Hamiltonian rather than training examples as input, if one is instead given training examples drawn from the SK measure, then at sufficiently high temperature one can approximately recover the Hamiltonian . In this case, a suitable modification  should be able to yield an algorithm for approximately generating fresh samples from the SK model given training examples.

Learning mixtures of Gaussians.The literature on provable algorithms for learning Gaussian mixture models is vast, dating back to the pioneering work of Pearson , and we cannot do justice to it here. We mention only works whose quantitative guarantees are closest in spirit to ours and refer to the introduction of  for a comprehensive overview of recent works in this direction. For mixtures of identity-covariance Gaussians in high dimensions, the strongest existing guarantee is a polynomial-time algorithm  for learning the centers as long as their pairwise separation slightly exceeds \(()\) based on a sophisticated instantiation of method of moments inspired by the quasipolynomial-time algorithms of . By the lower bound in , this is essentially optimal. In contrast, our Theorem 2 only applies given one initializes in a neighborhood of the true parameters of the mixture. We also note the exponential-time spectral algorithm of  and quasipolynomial-time tensor-based algorithm of , which achieve _density estimation_ even in the regime where the centers are arbitrarily closely spaced and learning the centers is information-theoretically impossible.

A separate line of work has investigated the "textbook" algorithm for learning Gaussian mixtures, namely the EM algorithm . Notably, for balanced mixtures of two Gaussians with the same covariance,  showed that finite-sample EM with random initialization converges exponentially quickly to the true centers. For mixtures of \(K\) Gaussians with identity covariance,  showed that from an initialization sufficiently close to the true centers, finite-sample EM converges exponentially quickly to the true centers as long as their pairwise separation is \(()\). In particular,  establish this local convergence as long as every center estimate is initialized at distance at most \(/2\) away from the corresponding true center, where \(\) is the minimum separation between any pair of true centers; this radius of convergence is provably best possible for EM.

Lastly, we note that there are many works giving parameter recovery algorithms mixtures of Gaussians with general mixing weights and covariances, all of which are based on method of moments . Unfortunately, for general mixtures of \(K\) Gaussians, these algorithms run in time at least \(d^{O(K)}\), and there is strong evidence  that this is unavoidable for computationally efficient algorithms.

### Technical overview

We begin by describing in greater detail the algorithm we analyze in this work. For the sake of intuition, in this overview we will focus on the case of mixtures of two Gaussians \((K=2)\) where the centers are well-separated and symmetric about the origin, that is, the data distribution is given by

\[q=(^{*},)+(-^{*},)\,.\] (1)

At the end of the overview, we briefly discuss the key challenges for handling smaller separation and general \(K\).

Loss function, architecture of the score function and student network.The algorithmic task at the heart of score estimation is that of denoising. Formally, for some noise level \(t>0\), we are given a noisy sample

\[X_{t}=(-t)X_{0}+Z_{t}\,,\]

where \(X_{0}\) is a clean sample drawn from the data distribution \(q\), and \(Z_{t}(0,)\). Conditioning on \(X_{t}\) induces some posterior distribution over the noise \(Z_{t}\), and our goal is to form an estimate \(s\) for the mean of this posterior which achieves small error on average over the randomness of \(X_{0}\) and \(Z_{t}\). That is, we would like to minimize the _DDPM objective_, which up to rescaling is given by1

\[L_{t}(s)=_{X_{0},Z_{t}}\|s(X_{t})-Z_{t}\|^{2}\,.\]

As discussed in the introduction, the algorithm of choice for minimizing this objective in practice is gradient descent on some student network. To motivate our choice of architecture, note that when the data distribution is given by (1), the true minimizer of \(L_{t}\) is, up to scaling,

\[(^{*}_{t},x)^{*}_{t}-x\,,^{*}_{t} ^{*}(-t)\,.\] (2)

See Appendix A for the derivation. Notably, Eq. (2) is exactly a two-layer neural network with \(\) activation. As a result, we use the same architecture for our student network when running gradient descent. That is, given weights \(^{d}\), our student network is given by \(s_{}(x)(^{}x)-x\). The exact gradient updates on \(\) are given in Lemma C.2.

As we discuss next, depending on whether the noise level \(t\) is large or small, this update closely approximates the update in one of two well-studied algorithms for learning mixtures of Gaussians: power method and EM respectively.

Learning mixtures of two Gaussians.We first provide a brief overview of the analysis and then go into the details of the analysis. We start with mixtures of two Gaussians of the form (1) where \(\|^{*}\|\) is \((1)\). In this case, we analyze the following two-stage algorithm. We first use gradient descent on the DDPM objective with large \(t\) starting from random initialization. We show that gradient descent in this "high noise" regime resembles a type of power iteration and gives \(\) that has a nontrivial correlation with \(^{*}_{t}\). Starting from this \(\), we then run gradient descent with small \(t\). We show that the gradient descent in this "small noise" regime corresponds to the EM algorithm and converges exponentially quickly to the ground truth.

Large noise level: connection to power iteration.When \(t\) is large, we show that gradient descent on the DDPM objective is closely approximated by power iteration. More precisely, in this regime, the negative gradient of \(L_{t}(s_{})\) is well-approximated by

\[-_{}L_{t}(s_{})(2^{*}_{t}^{*}_{t}-r) \,\,,\]

where \(r\) is a scalar that depends on \(\) (See Lemma 8). So the result of a single gradient update with step size \(\) starting from \(\) is given by

\[^{}-_{}L_{t}(s_{})((1- r) \,+2^{*}_{t}^{*}_{t})\,.\] (3)

This shows us that each gradient step can be approximated by one step of power iteration (without normalization) on the matrix \((1- r)\,+2^{*}_{t}^{*}_{t}\). It is know that running enough iterations of the latter from a random initialization will converge in angular distance to the top eigenvector, which in this case is given by \(^{*}_{t}\). This suggests that if we can keep the approximation error in (3) under control, then gradient descent on \(\) will also allow us to converge to a neighborhood of the ground truth. We implement this strategy in Lemma 10. Next, we argue that once we are in a neighborhood of the ground truth, we can run GD on the DDPM objective at _low noise level_ to refine our estimate.

Low noise level: connection to the EM algorithm.When \(t\) is small, we show that gradient descent on the DDPM objective is closely approximated by EM. Here, our analysis uses the fact that \(^{*}\) is sufficiently large and requires that we initialize \(\) to have sufficiently large correlation with the true direction \(^{*}_{t}\). We can achieve the latter using the large-\(t\) analysis in the previous section.

Provided we have this, when \(t\) is small it turns out that the negative gradient is well-approximated by

\[-_{}L_{t}(s_{})_{X(^{*}_{t}, )}[(,X)X]-\,.\]Note that the expectation is precisely the "M"-step in the EM algorithm for learning mixtures of two Gaussians (see e.g. Eq. (2.2) of ). We conclude that a single gradient update with step size \(\) starting from \(\) is given by mixing the old weights \(\) with the result of the "M"-step in EM:

\[^{}-_{}L_{t}(s_{})(1-)+ _{X(^{*}_{t},)}[( ,X)X]}_{}.\]

 and  showed that EM converges exponentially quickly to the ground truth \(^{*}_{t}\) from a warm start, and we leverage ingredients from their analysis to prove the same guarantee for gradient descent on the DDPM objective at small noise level \(t\) (see Lemma 12).

Extending to small separation.Next, suppose we instead only assume that \(\|^{*}\|\) is \((1/(d))\), i.e. the two components in the mixture may have small separation. The above analysis breaks down for the following reason: while it is always possible to show that gradient descent at large noise level converges in _angular distance_ to the ground truth, if \(\|^{*}\|\) is small, then we cannot translate this to convergence in Euclidean distance.

We circumvent this as follows. Extending the connection between gradient descent at large \(t\) and power iteration, we show that a similar analysis where we instead run _projected_ gradient descent over the ball of radius \(\|^{*}\|\) yields a solution arbitrarily close to the ground truth, even without the EM step.2 The projection step can be thought of as mimicking the normalization step in power iteration.

It might appear to the reader that this projected gradient-based approach is strictly superior to the two-stage algorithm described at the outset. However, in addition to obviating the need for a projection step when separation is large, our analysis for the two-stage algorithm has the advantage of giving much more favorable statistical rates. Indeed, we can show that the sample complexity of the two-stage algorithm has optimal dependence on the target error (\(1/^{2}\)), whereas we can only show a suboptimal dependence (\(1/^{8}\)) for the single-stage algorithm.

Extending to general \(K\).The connection between gradient descent on the DDPM objective at small \(t\) and the EM algorithm is sufficiently robust that for general \(K\), our analysis for \(K=2\) can generalize once we replace the ingredients from  and  with the analogous ingredients in existing analyses for EM with \(K\) Gaussians. For the latter, it is known that if the centers of the Gaussians have separation \(()\), then EM will converge from a warm start . By carefully tracking the error in approximating the negative gradient with the "M"-step in EM, we are able to show that gradient descent on the DDPM objective at small \(t\) achieves the same guarantee.

### Preliminaries

Diffusion models.Throughout the paper, we use either \(q\) or \(q_{0}\) to denote the data distribution and \(X\) or \(X_{0}\) to denote the corresponding random variable on \(^{d}\). The two main components in diffusion models are the _forward process_ and the _reverse process_. The forward process transforms samples from the data distribution into noise, for instance via the _Ornstein-Uhlenbeck (OU) process_:

\[X_{t}=-X_{t}\,t+\,W_{t}  X_{0} q_{0}\,,\]

where \((W_{t})_{t 0}\) is a standard Brownian motion in \(^{d}\). We use \(q_{t}\) to denote the law of the OU process at time \(t\). Note that for \(X_{t} q_{t}\),

\[X_{t}=(-t)X_{0}+Z_{t} X_{0} q_ {0},\ \ Z_{t}(0,)\,.\]

The reverse process then transforms noise into samples, thus performing generative modeling. Ideally, this could be achieved by running the following stochastic differential equation for some choice of terminal time \(T\):

\[X_{t}^{}=\{X_{t}^{}+2_{x} q_{T-t}(X_{t }^{})\}\,t+\,W_{t} X _{0}^{} q_{T}\,,\]

where now \(W_{t}\) is the reversed Brownian motion. In this reverse process, the iterate \(X_{t}^{}\) is distributed acccording to \(q_{T-t}\) for every \(t[0,T]\), so that the final iterate \(X_{T}^{}\) is distributed according to the data distribution \(q_{0}\). The function \(_{x} q_{t}\) is called the _score function_, and because it depends on \(q\) which is unknown, in practice one estimates it by minimizing the _score matching loss_

\[_{s_{t}}\ \ _{X_{t} q_{t}}[\|s_{t}(X_{t})-_{x} q_{t}( X_{t})\|^{2}]\,.\] (4)

A standard calculation (see e.g. Appendix A of ) shows that this is equivalent to minimizing the _DDPM objective_ in which one wants to predict the noise \(Z_{t}\) from the noisy observation \(X_{t}\), i.e.

\[_{s_{t}}\ \ L_{t}(s_{t})=_{X_{0},Z_{t}}s_{t}(X_ {t})+}{}^{2}\,.\] (5)

While we have provided background on diffusion models for context, in this work we focus specifically on the optimization problem (5).

Mixtures of Gaussians.We consider the case of learning mixtures of \(K\) equally weighted Gaussians:

\[q=q_{0}=_{i=1}^{K}(_{i}^{*},),\] (6)

where \(_{i}^{*}\) denotes the mean of the \(i^{}\) Gaussian component. We define \(^{*}=\{_{1}^{*},_{2}^{*},_{K}^{*}\}\). For the mixtures of two Gaussians, we can simplify the data distribution as

\[q=q_{0}=(^{*},)+(- ^{*},).\] (7)

Note that distribution in Eq. (7) is equivalent to the distribution Eq. (6) with \(K=2\) because shifting the latter by its mean will give the former distribution, and furthermore the necessary shift can be estimated from samples. The following is immediate:

**Lemma 3**.: _If \(q_{0}\) is a mixture of \(K\) Gaussians as in Eq. (6), then for any \(t>0\), \(q_{t}\) is the mixture of \(K\) Gaussians given by_

\[q_{t}=_{i=1}^{K}(_{i,t}^{*},)\ \ \ _{i,t}^{*}_{i}^{*}(-t)\,.\] (8)

See Appendix A for a proof of this fact. We can see that the means of \(q_{t}\) get rescaled according to the noise level \(t\). We also define \(_{t}^{*}=\{_{1,t}^{*},_{2,t}^{*},,_{K,t}^{*}\}\).

**Lemma 4**.: _The score function for distribution \(q_{t}\), for any \(t>0\), is given by_

\[_{x} q_{t}(x)=_{i=1}^{K}w_{i,t}^{*}(x)_{i,t}^{*}-x\,,\ \ \ \ \ \ \ \ \ w_{i,t}^{*}(x)=^{*}\|^{2}/2)}{_{j=1}^{K}(-\| x-_{j,t}^{*}\|^{2}/2)}.\]

_For a mixture of two Gaussians, the score function simplifies to_

\[_{x} q_{t}(x)=(_{t}^{*}x)_{t}^{*}-x\,,\ \ \ \ \ \ \ \ \ _{t}^{*}^{*}(-t)\]

See Appendix A for the calculation.

Recall that \(_{x} q_{t}(x)\) is the minimizer for the score-matching objective given in Eq. (4). Therefore, we parametrize our student network architecture similarly to the optimal score function. Our student architecture for mixtures of \(K\) Gaussians is

\[s_{_{t}}(x)=_{i=1}^{K}w_{i,t}(x)_{i,t}-x\,,\ \ \ \ \ \ \ \ \ w_{i,t}(x) \|^{2}/2)}{_{j=1}^{K}(-\|x- _{j,t}\|^{2}/2)}\] (9) \[_{i,t} _{i}(-t).\]

where \(_{t}=\{_{1,t},_{2,t},,_{K,t}\}\) denotes the set of parameters at the noise scale \(t\). For mixtures of two Gaussians, we simplify the student architecture as follows:

\[s_{_{t}}(x)=(_{t}^{}x)_{t}-x\,,\ \ \ \ \ _{t} (-t).\]

As \(_{t}\) only depends on \(_{t}\) in the case of mixtures of two Gaussians, we simplify the notation of the score function from \(s_{_{t}}(x)\) to \(s_{_{t}}(x)\) in that case. We use \(_{t}\) and \(_{t}^{*}\) to denote the unit vector along the direction of \(_{t}\) and \(_{t}^{*}\) respectively. Note that we often use \(_{t}\) (or \(_{t}\)) to denote the current iterate of gradient descent on the DDPM objective and \(_{t}^{}\) to denote the iterate after taking a gradient descent step from \(_{t}\).

Expectation-Maximization (EM) algorithm.The EM algorithm is composed of two steps: the E-step and the M-step. For mixtures of Gaussians, the E-step computes the expected log-likelihood based on the current mean parameters and the M-step maximizes this expectation to find a new estimate of the parameters.

**Fact 5** (See e.g.,  for more details).: _When \(X\) is the mixture of \(K\) Gaussian and \(\{_{1},_{2},,_{K}\}\) are current estimates of the means, the population EM update for all \(i\{1,2,,K\}\) is given by_

\[_{i}^{}=_{X}[w_{i}(X)X]}{_{X}[w_{i}(X)]}, \ \ w_{i}(X)=\|^{2}/2)}{_{j=1}^{K}(-\|X-_{j}\| ^{2}/2)}.\]

_The EM update for mixtures of two Gaussians given in Eq. (7) simplifies to_

\[^{}=_{X(^{*},)}[(^{ }X)X].\]

An analogous version of the EM algorithm, called the gradient EM algorithm, takes a gradient step in the direction of the M-step instead of optimizing the objective in the M-step fully.

**Fact 6** (See e.g.,  for more details).: _For all \(i\{1,2,,K\}\), the gradient EM-update for mixtures of \(K\) Gaussian is given by_

\[_{i}^{}=_{i}+\,_{X}[w_{i}(X)(X-_{i})],\]

_where \(\) is the learning rate._

## 2 Warmup: mixtures of two Gaussians with constant separation

In this section, we formally state our result for learning mixtures of two Gaussians with constant separation. This case highlights the main proof techniques, namely viewing gradient descent on the DDPM objective as power iteration and as the EM algorithm.

### Result and algorithm

**Theorem 7**.: _There is an absolute constant \(c>0\) such that the following holds. Suppose a mixture of two Gaussians with the mean parameter \(^{*}\) satisfies \(\|^{*}\|>c\). Then, for any \(>0\), there is a procedure that calls Algorithm 1 at two different noise scales \(t\) and outputs \(\) such that \(\|-^{*}\|\) with high probability. Moreover, the algorithm has time and sample complexity \((d)/^{2}\) (see Theorem C.1 for more precise quantitative bounds)._

Algorithm.The algorithm has two stages. In the first stage we run gradient descent on the DDPM objective described in Algorithm 1 from a random Gaussian initialization and noise scale \(t_{1}\) for a fixed number of iterations \(H\) where \(t_{1}=( d)\) ("high noise") and \(H=(d,1/)\). In the second stage, the procedure uses the output of the first step as initialization and runs Algorithm 1 at a "low noise" scale of \(t_{2}=(1)\).

### Proof outline of Theorem 7

We provide a proof sketch of correctness of the above algorithm and summarize the main technical lemmas here. All proofs of the following lemmas can be found in Appendix C.

Part I: Analysis of high noise regime and connection to power iteration.We show that in the large noise regime, the negative gradient \(- L_{t}(s_{t})\) is well-approximated by \(2_{t}^{*}{_{t}}^{}_{t}-3\|_{t}\|^{2}\,_{t}\). Recall that this result is the key to showing the resemblance between gradient descent and power iteration. Concretely, we show the following lemma:

**Lemma 8** (See Lemma C.3 for more details).: _For \(t=( d)\), the gradient descent update on the DDPM objective \(L_{t}(s_{t})\) can be approximated with \(2_{t}^{*}{_{t}}^{}_{t}-3\|_{t}\|^{2}\,_{t}\):_

\[\|(- L_{t}(s_{t}))-(2_{t}^{*}{_{t}^{*}}^{ }_{t}-3\|_{t}\|^{2}\,_{t})\|(1/d).\]From Lemma 8, it immediately follows that \(^{}t\), the result of taking a single gradient step starting from \(_{t}\), is well-approximated by the result of taking a single step of power iteration for a matrix whose leading eigenvector is \(_{t}^{*}\):

\[_{t}^{}=_{t}- L_{t}(s_{})((1- 3\|_{t}\|^{2})+2_{t}^{*}_{t}^{*})_{t}\,.\]

The second key element is to show that as a consequence of the above power iteration update, the gradient descent converges in _angular distance_ to the leading eigenvector. Concretely, we show the following lemma:

**Lemma 9** (Informal, see Lemma C.5 for more details).: _Suppose \(_{t}^{}\) is the iterate after one step of gradient descent on the DDPM objective from \(_{t}\). Denote the angle between \(_{t}\) and \(_{t}^{*}\) to be \(\) and between \(_{t}^{}\) and \(_{t}^{*}\) to be \(^{}\). In this case, we show that_

\[^{}=(_{1},_{2}),\]

_where \(_{1}<1\) and \(_{2} 1/(d)\)._

Note \(^{}<\) implies that \(^{}<\) or equivalently \(_{t}^{},_{t}^{*}>_{t}, _{t}^{*}\). Thus, the above lemma shows that by taking a gradient step in the DDPM objective, the angle between \(_{t}\) and \(_{t}^{*}\) decreases. By iterating this, we obtain the following lemma:

**Lemma 10** (Informal, see Lemma C.6 for more details).: _Running gradient descent from a random initialization on the DDPM objective \(L_{t}(s_{})\) for \(t=O( d)\) gives \(_{t}\) for which \(_{t},_{t}^{*}\) is \((1)\)._

Note that we cannot keep running gradient descent at this high noise scale and hope to achieve \(\) such that\(\|-^{*}\|\) is \(O()\). This is because Lemma 9 can only guarantee that the angle between \(_{t}\) and \(_{t}^{*}\) is \(O()\), but this does not imply \(\|-^{*}\|\) is \(O()\). Instead, as described in Part II, we will proceed with a smaller noise scale.

Part II: Analysis of low noise regime and connection to EM.In the low noise regime, we run Algorithm 1 using the output from Part I as our initialization. Our analysis here shows that whenever the initialization \(_{t}\) satisfies the condition of \(_{t},_{t}^{*}\) being \((1),\|_{t}-_{t}^{*}\|\) contracts after every gradient step. To start with, we show that the result of a _population_ gradient step on the DDPM objective \(L_{t}(s_{})\) results in the following:

\[_{t}^{}=(1-)_{t}+}_{x (_{t}^{*},)}[(_{t}^{}x)x]+ G( _{t},_{t}^{*}),\]

where \(_{t}^{}\) is the parameter after a gradient step, \(\) is the learning rate, and function \(G\) is given by

\[G(,^{*})=}_{x(^{*}, )}[-^{}(^{}x)\|\|^{2}x+ ^{}(^{}x)^{}xx-^{}(^{}x)].\]

Note we use the population gradient here only for simplicity; in the Appendix we show that empirical estimates of the gradient suffice. After some calculation, we can show that

\[_{t}^{}-_{t}^{*}(1-)\|_{t}-_{t}^{*} \|+\|}_{x(_{t}^{*}, {Id})}[(_{t}^{}x)x]-_{t}^{*}\|+G(_{t},_{t}^{* })\,.\] (10)

Using Fact 5, we know that \(}_{x(_{t}^{*},)}[ (_{t}^{}x)x]\) is precisely the result of one step of EM starting from \(_{t}\), and it is known  that the EM update contracts the distance between \(_{t}\) and \(_{t}^{*}\) as follows:

\[\|}_{x(_{t}^{*}, )}[(_{t}^{}x)x]-_{t}^{*}\|_{1}\| _{t}-_{t}^{*}\|_{1}<1\] (11)

It remains to control the second term in Eq. (10), for which we prove the following:

**Lemma 11** (Informal, see Lemma C.9 for more details).: _When \(\|^{*}\|=(1)\) and the noise scale \(t=(1)\), then for every \(\) with \(,^{*}\) being \((1)\), the following inequality holds:_

\[\|G(_{t},_{t}*)\|_{2}\|_{t}-_{t}^{*}\|_{2}<1\,.\]

Combining Eq. (11) and Lemma 11 with Eq. (10), we have

\[_{t}^{}-_{t}^{*}(1-(1-_{1}-_{ 2}))\|_{t}-_{t}^{*}\|\,.\] (12)

We can set parameters to ensure that \(_{1}+_{2}<1\) and therefore that \(\|_{t}-_{t}^{*}\|\) contracts with each gradient step. Applying Lemma 11 and Eq. (12), we obtain the following lemma summarizing the behavior of gradient descent on the DDPM objective in the low noise regime.

**Lemma 12** (Informal).: _For any \(>0\) and for the noise scale \(t=(1)\), starting from an initialization \(_{t}\) for which \(_{t},_{t}^{*}=(1)\), running gradient descent on the DDPM objective \(L_{t}(s_{})\) will give us mean parameter \(\) such that \(\|-^{*}\| O()\)._

Combining Lemma 10 and Lemma 12, we obtain our first main result, Theorem 7, for learning mixtures of two Gaussians with constant separation. For the full technical details, see Appendix C.

## 3 Extensions: small separation and more components

### Mixtures of two Gaussians with small separation

In this section, we briefly sketch how the ideas from Section 2 can be extended to give our second main result, namely on learning mixtures of two Gaussians even with _small separation_. We defer the full technical details to Appendix D.

**Theorem 13**.: _Suppose a mixture of two Gaussians has mean parameter \(^{*}\) that satisfies \(\|^{*}\|=((d)})\). Then, for any \(>0\), there exists a modification of Algorithm 1 that provides an estimate \(\) such that \(\|-^{*}\| O()\) with high probability. Moreover, the algorithm has time and sample complexity \((d)/^{8}\) (see Theorem D.1 for more precise quantitative bounds)._

Algorithm modification.The algorithm that we analyze runs _projected_ gradient descent on the DDPM objective but only in the high noise scale regime where \(t=O( d)\). At each step, we project the iterate \(\) to the ball of radius \(R\), where \(R\) is an empirical estimate for \(\|^{*}\|\) obtained by drawing samples \(x_{1},,x_{n}\) from the data distribution and forming \(R(_{i=1}^{n}\|x_{i}\|^{2}-d)^{1/2}\).

Proof sketch.Lemma 9 and Lemma 10 apply even when the components of the mixture have small separation, and they show that running gradient descent on the DDPM objective results in \(_{t}\) and \(_{t}^{*}\) being \(O(1)\) close in angular distance. Although our analysis can be extended to show that gradient descent can achieve \(O()\) angular distance, this does not guarantee that \(\|_{t}-_{t}^{*}\|\) is \(O()\). If in addition to being \(O()\) close in angular distance, we also have that \(\|_{t}\|\|_{t}^{*}\|\), then it is easy to see that \(\|_{t}-_{t}^{*}\|\) is indeed \(O()\).

Observe that if \(R\) is approximately equal to \(\|_{t}^{*}\|\), then the projection step in our algorithm ensures that our final estimate \(_{t}\) satisfies this additional condition of \(\|_{t}\|\|_{t}^{*}\|\). It is not hard to show that \(R^{2}\) is an unbiased estimate of \(\|_{t}^{*}\|^{2}\), so standard concentration shows that taking \(n=(d,)\) suffices to ensure that \(R\) is sufficiently close to \(\|_{t}^{*}\|\).

### Mixtures of \(K\) Gaussians, from a warm start

In this section, we state our third main result, namely for learning mixtures of \(K\) Gaussians given by Eq. (6) from a warm start, and provide an overview of how the ideas from Section 2 can be extended to obtain this result.

**Assumption 14**.: _(Separation) For a mixture of \(K\) Gaussians given by Eq. (6), for every pair of components \(i,j\{1,2,,K\}\) with \(i j\), we assume that the separation between their means \(\|_{i}^{*}-_{j}^{*}\| C\) for sufficiently large absolute constant \(C>0\)._

**Assumption 15**.: _(Initialization) For each component \(i\{1,2,,K\}\), we have an initialization \(_{i}^{(0)}\) with the property that \(\|_{i}^{(0)}-_{i}^{*}\| C^{}\) for sufficiently small absolute constant \(C^{}>0\)._

**Theorem 16**.: _Suppose a mixture of \(K\) Gaussians satisfies Assumption 14. Then, for any \(=(1/(d))\), running gradient descent on the DDPM objective (Algorithm 1) at low noise scale \(t=O(1)\) and with initialization satisfying Assumption 15 results in mean parameters \(\{_{i}\}_{i=1}^{K}\) such that with high probability, the mean parameters satisfy \(\|_{i}-_{i}^{*}\| O()\) for each \(i\{1,2,,K\}\). Additionally, the runtime and sample complexity of the algorithm is \((d,1/)\) (see Theorem E.1 for more precise quantitative bounds)._

We provide a brief overview of the proof here. The full proof can be found in Appendix E.

Proof sketch.For learning mixtures of two Gaussians, we have already established the connection between gradient descent on the DDPM objective and the EM algorithm. For mixtures of \(K\) Gaussians, however, in a local neighborhood around the ground truth parameters \(^{*}\), we show an equivalence between _gradient_ EM (recall gradient EM performs one-step of gradient descent on the "M" step objective) and gradient descent on the DDPM objective. In particular, our main technical lemma (Lemma E.4) shows that for noise scale \(t=(1)\) and for any \(_{i}\) that satisfies \(\|_{i}-_{i}^{*}\| O()\), we have

\[-_{_{i,t}}L_{t}(s_{_{t}})_{X_{t}}[w_{i,t}(X_{ t})(X_{t}-_{i,t})].\]

Therefore, the iterate \(_{i,t}^{}\) resulting from a single gradient step on the DDPM objective \(L_{t}(s_{_{t}})\) with learning rate \(\) is given by

\[_{1,t}^{}=_{1,t}-_{_{1,t}}L_{t}(s_{_{t}}) _{1,t}+\,_{X_{t}}[w_{1,t}(X_{t})(X_{t}-_{1,t})].\] (13)

Comparing Fact 6 with Eq. (13), we see the correspondence in this regime between gradient descent on the DDPM objective to gradient EM. Using this connection and an existing local convergence guarantee from the gradient EM literature , we obtain our main theorem for mixtures of \(K\) Gaussians. Full details can be found in Appendix E.