# PaCE: Parsimonious Concept Engineering

for Large Language Models

Jinqi Luo &Tianjiao Ding\({}^{}\)&Kwan Ho Ryan Chan\({}^{}\)&Darshan Thaker\({}^{}\)

Aditya Chattopadhyay&Chris Callison-Burch\({}^{}\)&Rene Vidal\({}^{}\)

\({}^{}\)University of Pennsylvania &Johns Hopkins University

{jinqiluo,tjding}@upenn.edu

Equal contribution.

###### Abstract

Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable outputs via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose **P**arsimonious **C**ncept **E**ngineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Then, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activations as linear combinations of benign and undesirable components. By removing the latter ones from the activations, we reorient the behavior of the LLM towards the alignment goal. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities. Our collected dataset for concept representations is available at https://github.com/peterljq/Parsimonious-Concept-Engineering.

## 1 Introduction

Large Language Models (LLMs) are useful for tasks as far ranging as question answering , symbolic reasoning , multi-modal synthesis , and medical diagnosis . LLMs are typically pre-trained on a broad collection of textual corpora with the next-token prediction objective , enabling them to generate human-like text. An important aspect of deploying pre-trained LLMs for real-world applications is preventing undesirable responses such as toxic language, hallucinations, and biased information through alignment methods, which aim to make AI systems behave in line with human intentions and values . A common alignment approach is tuning LLMs with human feedback  for better instruction-following capabilities. However, after such aligning, undesirable and harmful content can still be elicited from LLMs. For example, jailbreaking can produce hate speech and aggression , stress-testing shows hallucinatory responses such as illogical statements , and various kinds of biases are not fully removed from LLM responses . This emphasizes the need for further development of aligned LLMs.

Overall, alignment methods can largely be categorized into: parameter fine-tuning, prompt engineering, and activation engineering. _Parameter fine-tuning_ methods, such as low-rank adaptation and knowledge editing [14; 74], involve updating the model parameters using datasets of input-response pairs . Unfortunately, such computations over large datasets are often costly. Furthermore, whenever a new category of undesirable behaviors is identified or a new group of customers is acquired, the LLM supplier has to incur the cost of data creation and fine-tuning again. _Prompt engineering_ attempts to manipulate the LLM's reasoning with carefully designed instruction prompts [78; 80; 81]. However, effective instructions are commonly obtained through empirical trial-and-error, with no guarantee of coverage across tasks of different domains. Notably, recent works show that the instruction itself can be lengthy  or contain human errors [10; 61].

_Activation engineering_, i.e., algorithms that modify the latent _activations_ of LLMs, has emerged to alleviate high-cost and poor coverage of tasks. Recent work has shown that certain directions in the activation space of LLMs are associated with semantic concepts (c.f. SS2.1). Thus, given an input prompt at inference time, modifying its neural activations towards or away from these directions controls the semantics of the model response. For example, methods based on Vector Addition (VecAdd) [37; 43; 67; 68; 69; 72; 88] directly add multiples of a concept direction to a neural activation, while those based on Orthogonal Projection (OrthoProj) [23; 88] subtract from a neural activation its orthogonal projection onto a concept direction. Nonetheless, these methods face two major challenges. First, these methods inadequately model the geometry of the activation space, as we will detail in SS2.2. Hence, they tend to either remove benign concepts, harming linguistic capability; or insufficiently remove undesirable concepts, thereby failing the alignment task. Second, for each alignment task, these methods typically only remove a single concept direction from the input activation vector, while there may be multiple concepts related to the alignment task.

To address these challenges, we propose Parsimonious Concept Engineering (PaCE), an activation engineering framework for alignment that i) enforces alignment goals effectively and efficiently, ii) retains linguistic capability, and iii) adapts to new alignment goals without costly parameter fine-tuning. PaCE consists of two stages: (1) Concept Construction and Partition, and (2) Activation Decomposition and Intervention (Figure 3). We summarize the procedure of PaCE below and highlight our contributions in bold.

* _Concept Dictionary Construction and Partition (SS3.2)_: Since existing works only provide a limited number of concept directions, **we collect a large concept dictionary, PaCE-1M, that consists of 40,000 concept directions extracted from over 1,200,000 context sentences.** In particular, for each concept in the Brown Corpus , we use a knowledge-driven GPT [35; 44; 65] to propose contextual scenarios to describe the concept, and extract concept directions in the representation (activation) space  from the context sentences. This is done only once offline. Further, given any alignment task, we instruct a GPT to automatically partition the concept directions in the dictionary into benign and undesirable directions, which is done once per task offline.
* _Activation Decomposition and Intervention (SS3.3)_: At inference time, given any user input prompt, **we decompose the activations as a sparse linear combination of concept directions using sparse coding techniques**. Notably, this allows for an efficient and accurate estimate of both undesirable and benign components in the activations, which is overlooked in previous activation engineering methods. By removing the undesirable components from the activations, we reorient the behavior of LLMs toward alignment goals, while maintaining their linguistic capability.

We evaluate PaCE on alignment tasks including response detoxification, faithfulness enhancement, and sentiment revising (SS4). **We show that PaCE achieves state-of-the-art performance on these tasks, while retaining its linguistic capability at a comparable level.** We further shed insights on the concept directions of PaCE-1M: concept directions tend to form clusters with directions from each cluster corresponding to similar semantics, and decomposing an activation reveals its semantics.

## 2 Basics of Latent Space Engineering

As motivated above, in this paper we are interested in controlling LLMs by leveraging structures in their latent space. We begin by reviewing some basic properties of the latent space in SS2.1. This lays the foundation for previous methods on latent space intervention in SS2.2 as well as our method in SS3.

Figure 1: Our framework PaCE achieves alignment goals by sparse coding and adjusting vectors in the activation space of the LLM Decoder Layer (DCL).

### The Latent Space and Its Linear Controllability

Let \(^{d}\) denote a _latent space_ whose elements can be mapped into text. That is, there exists a (surjective) decoder \(g:\) where \(\) is some set of texts. For ease of notation, we follow the convention and use \(_{}\) to denote an element in the pre-image \(g^{-1}()\).

**Linear Controllability.** Consider the word pairs ('France', 'Paris') and ('Japan', 'Tokyo') - the latter is the capital of the former. It is natural to wonder if their latent codes have such correspondence. In various settings as we will review, there is approximately a _linear_ relation: there exists a \(_{}^{d}\), such that \(_{}+c_{}_{}\) for some control strength \(c>0\), and \(_{}+c^{}_{}_{ }\) for some \(c^{}>0\). Beyond this example, prior works seem to support the existence of a set of _concept directions_\(^{d}\) that linearly relate pairs of latent codes2. Note, however, that the notion of _linear controllability_ is different from the notion _linear or affine combination_ in linear algebra in that there may be only one choice of \(c\) such that \(+c\).

_Remark 1_ (\(=\)).: A classic setting where linear controllability shows up is that of _word embeddings_. Here, \(\) is the vocabulary (say, the set of English words), \(\) contains some vectors in \(^{d}\), and \(g\) is a bijection between \(\) and \(\). In the seminal work of Mikolov et al. , the authors observe that word embeddings learned by recurrent neural networks approximately enjoy relations such as \(_{}-_{}+_{}_{ }\), where one can view \(_{}-_{}\) as the concept direction \(\) and \(c=1\) as the control strength. This observation is later extended to word embeddings of various networks and learning objectives such as word2vec , Skip-Grams , GloVe , and Swivel . On the theoretical front, a fruitful line of research has been devoted to understanding the emergence of such properties in word embeddings .

_Remark 2_ (\(=\)).: Modern neural architectures such as transformers have significantly boosted the linguistic performance of language models. Much of their success is attributed to the attention mechanism, which incorporates long-range context into the neural activations in transformers. This has motivated people to take \(\) as certain hidden states in transformers3, and search for concept directions \(\) in \(\). An interesting line of works has supported the empirical existence of \(\):  find directions that indicate truthful output,  finds directions for sentiments,  finds directions for emotions and honesty, and  finds directions for current player tile in a synthetic board game model. Interestingly,  further offer theoretical models, under which the linear controllability shows up provably in the latent space of LLMs.

### Controlling Language Models via Latent Space Engineering

The above findings have supported the development of practical methods to control the behavior of language models. As we will see, a key challenge there is to decide the correct control strength.

**Vector Addition.** The work of  proposes to add or subtract multiples of a concept direction from the latent code. For example, to remove hatred from \(\), one performs

\[-_{},\] (VecAdd)

where \(>0\) is a parameter of the control strength. In principle, as each input prompt may contain a different 'extent' of the concept to be removed, \(\) should depend on both the prompt and the concept. Thus, in practice, one either tunes \(\) per input prompt and concept, which is laborious, or one fixes

Figure 2: To remove a concept direction ‘red’ from the latent code ‘red apple’ (left), prior works use i) orthogonal projection (middle right, (OrthoProj)), which may remove extra directions, or ii) vector addition (right, (VecAdd)), where it is hard to pick the edit strength \(c\). Instead, PaCE explicitly models the concept dictionary in the latent space and use oblique projection (middle left).

a \(\), which is sub-optimal. Indeed, this has been observed by the work : In their Table 10, the optimal coefficients \(\) are markedly different across the examples; see also their 'discussion' section.

**Orthogonal Projection.** The work of  proposed to remove gender bias in _word embeddings_ by projecting the embeddings onto the orthogonal complement to a gender direction \(_{}\):

\[_{(_{})^{}}=-_ {(_{})}.\] (OrthoProj)

Here, for any \(^{d}\), \(()\) is the linear subspace spanned by \(\), and for any linear subspace \(^{d}\), \(_{S}\) denotes the ortho-projector onto \(\). Such an idea is later applied to _neural activations_ of LLMs [23; 88]. Applying orthogonal projection to remove concept directions from latent codes may be reasonable: if directions corresponding to different concepts are orthogonal, then orthogonal projection only removes the gender direction while leaving the others intact. That being said, there are often more concept directions presented, and they are not orthogonal. For example,  shows that causally related concepts only exhibit _partial_ orthogonality for their directions.

To sum up, numerous attempts have been made to control the behavior of language models. However, existing methods either have a control strength parameter that is hard to tune or may remove extra concept directions. As we will see in the next section, these issues can be resolved by the proposed PaCE framework, which explicitly models the geometry of a large concept dictionary.

## 3 Our Method: Parsimonious Concept Engineering

### Activation Intervention via Overcomplete Oblique Projection

Can we efficiently remove one or more target concept directions from a given latent activation without affecting other concept directions present? To address this problem, our key insight is to model as many concept directions as possible, and then decompose the activation to estimate its components along these directions. Figure 2 presents an idealized visual example. Here, one is given a latent activation meaning'red apple', and the goal is to remove the'red' direction from the activation (left). As illustrated, orthogonal projection and vector addition tend to fail (middle right and right), as we discussed in SS2.2. In contrast, by decomposing the activation along the concept directions of'red' and 'apple', one can safely remove the component along'red' without affecting that along 'apple' (middle left). This is related to the idea of _oblique projection_, which gives the name of this section.

That said, several challenges remain to be addressed. As motivated above, to accurately model semantic concepts, one needs to collect as many concept directions in the latent space as possible. Since existing works only provide a limited number of concept directions (as reviewed in Remark 2), we contribute by collecting a large dictionary of concept directions, which we will discuss in SS3.2. Moreover, oblique projection is well-defined only when the concept directions are linearly independent, while concept directions are often dependent (as we show in SS4.3) so the decomposition is not unique. SS3.3 discusses our choice of decomposition algorithm to address this difficulty.

### Knowledge-Driven Concept Dictionary

**Concept Dictionary Construction.** We take the top 40,000 words from the Brown Corpus  ranked by word frequency  as the concept collection \(T\). For each concept \(t_{i} T\), we prompt GPT-4 to generate around \(30\) pieces of contextual stimuli \(s_{i}=\{s_{i}^{1},s_{i}^{2},,s_{i}^{30},\}\) that are scenarios

Figure 3: Pipeline of PaCE has several major steps: Step 1 collects concept vectors and constructs the concept dictionary, Step 2 decomposes the activation vector of the given input by sparse coding to get concept coefficients, and Step 3 performs editing on the concepts towards reoriented response.

describing the concept. To enhance the diversity of the concept stimuli, we retrieve knowledge from Wikipedia [35; 44; 65] (as we detail in Appendix B.4) to augment the prompt of stimulus synthesis. Samples of concepts and their stimuli are shown in Figure 4 and Appendix Figure 12. For each concept \(t_{i}\), we extract a direction \(_{i}^{}\) from the activations of its contextual stimuli at the \(\)-th decoder layer of the LLM , which gives a dictionary \(^{}^{d n}\) per layer (detailed in Appendix B.2).

**Task-Driven Dictionary Partition.** Given an alignment task, we further instruct GPT-4 as a concept partitioner to classify whether a concept needs to be removed from the input representation. To take detoxification as an example, the concept 'harmful' is highly correlated to the toxic response (hence needs removal) while benign concepts such 'bird' and 'laptop' will remain. That is, the instructed GPT-4 partitions the concepts into undesirable and benign to the alignment tasks. The full prompting templates of concept synthesis and partitioning are shown in Appendix E. In the next sub-section, we describe the notations and usages of the annotated concept dictionary.

### Overcomplete Oblique Projection via Sparse Coding

Now that we have a dictionary \(=[_{1},,_{n}]^{d n}\) of \(n\) concepts directions4, where each \(_{i}\) is a concept direction of known semantic meaning. Given a latent activation \(^{}\) coming from the user input, how can we control it via oblique projection?

**Oblique Projection.** The general paradigm of oblique projection can be stated as follows.

* _Step \(1\)-Decomposition:_ Find \(c_{1}^{},,c_{n}^{}\) such that \(^{}=c_{1}^{}_{1}++c_{n}^{}_{ n}+^{}\) by solving \[^{}*{argmin}_{c}\|^{}- \|_{2}^{2}+(),\] (1) where \(()\) is a sparsity-promoting regularizer that we will discuss soon. Then, each coefficient \(c_{i}^{}\) for \(i\{1,,n\}\) can be viewed as how much the concept represented by \(_{i}\) is present in \(^{}\), and \(^{}\) is the residual that is not explained by \(\).
* _Step 2-Intervention:_ Obtain the controlled coefficients \(c_{1}^{},,c_{n}^{}\), where \(c_{i}^{}\) is set to \(c_{i}^{}\) if the concept of \(_{i}\) is benign to the control task and \(0\) if undesirable (which has been decided offline in SS3.2). Then, synthesize a new latent code using the modified coefficients and the residual by taking \(^{}=c_{1}^{}_{1}++c_{n}^{} _{n}+^{}\).

The synthesized \(^{}\) will replace \(^{}\) to be passed on to the next layer of the neural network5.

_Remark 3_ ((OrthoProj, \(\)) = Special Cases of Oblique Projection).: If one restricts \(\) to contain only the undesirable concept directions (i.e., the ones to be removed from the latent code), and further takes \(()\) to be a constant function, it can be shown that oblique projection reduces to the special case of orthogonal projection (OrthoProj). On the other hand, if \(\) contains only one undesirable concept direction, and \(()\) is \(\|\|_{2}^{2}\) for some regularization strength \(\), then oblique projection recovers vector addition (VecAdd), by setting \(\) equal to \(\) in (VecAdd). We provide proofs in Appendix B.1. As we will see next, our method differs from these two in having a larger dictionary and a sparsity-promoting regularizer.

**Overcomplete Oblique Projection.** As mentioned in SS3.1, when the concept directions are linearly independent, then there is a unique decomposition of the latent code along the concept directions. However, often the concept directions can be dependent or nearly so, leading to infinitely many decompositions or numerical issues. To address this issue, we leverage the idea of _sparse coding_: natural signals are typically generated from sparse linear combinations of dictionary atoms, and

Figure 4: Examples of the constructed concepts and their partition for the detoxification task sampled from our PaCE-1M.

[MISSING_PAGE_FAIL:6]

representations. Finally, Table 3 shows the contribution of design choices in PaCE, and Figure 6 shows the effect of the dictionary size on the performance. We observe clear improvement after each design choice is progressively added. Appendix B.5 includes the details of these ablation studies.

**Linguistic Capability.** To validate that the detoxified representations of PaCE are still effective on general linguistic capability, we also evaluate the responses by N-gram fluency and perplexity. Furthermore, we apply PaCE to detoxify MMLU questions (which are naturally unharmful) to show that the detoxification will not significantly degrade the LLM's reasoning capability. We observe that the MMLU response accuracy of PaCE is the highest among all activation manipulation baselines.

**Efficiency.** Table 2 shows that PaCE is more time-efficient compared to the OrthoProj which also projects the concept vector onto the input vector. PaCE sees a three times speed improvement in average time per response and a two times improvement over average time per word when compared to OrthoProj. While PaCE is computationally slower than VecAdd, we argue the performance gain in a majority of the categories is a benefit that outweighs this particular shortcoming.

**Solvers.** Figure 7 additionally evaluates Orthogonal Matching Pursuit (OMP) , a fast greedy solver for the activation decomposition. OMP iteratively adds to the support the concept that has maximum coherence with the unexplained residual and updates the residual by solving the least square using the new support. It stops when a pre-defined maximum size \(k\) of support is reached. Intuitively, the \(k\) is the number of non-zero elements in the solved coefficients. We observe from the table that one can choose improvements in computational speed at the cost of safety performance.

    Target \\ Model \\  } &  &  &  \\   & & & PS & PG & EM & IA & MH & OF & PH & PP & UB &  Fluency \\ (\(\)) \\  &  Perplexity \\ (\(\)) \\  &  PMLU \\ (\%, \(\)) \\  \\   _Safety_ \\  } & Vanilla  & 17.6 & 19.5 & 10.1 & 7.79 & 11.3 & 17.2 & 22.6 & 11.8 & 17.2 & 7.70 & 3.51 & **43.4** \\  & Prompting  & **82.5** & 47.3 & 57.8 & 65.2 & 75.1 & 54.8 & 72.0 & 72.4 & 56.1 & 7.50 & **3.04** & 15.4 \\  & VecAdd  & 50.9 & **58.9** & **59.0** & 53.9 & 66.1 & 55.0 & 60.7 & 61.7 & 66.4 & 6.58 & 7.58 & 29.0 \\  & OrthoProj  & 50.7 & 57.9 & 50.2 & 47.5 & 67.0 & 50.1 & 74.9 & 65.7 & 66.4 & 7.46 & 3.73 & 34.1 \\  & PaCE (Ours) & 69.6 & 46.2 & 58.2 & **75.3** & **94.2** & **62.3** & **80.8** & **72.8** & **88.3** & **8.07** & 3.52 & 37.1 \\   _Safety_ \\  } & Vanilla  & 8.01 & 23.7 & 13.6 & 19.8 & 18.3 & 21.6 & 13.6 & 14.0 & 16.7 & **7.66** & 2.48 & **54.9** \\  & Prompting  & 35.8 & 68.3 & 59.3 & 52.5 & 73.5 & 23.4 & 78.0 & 71.1 & 66.5 & 7.63 & **2.22** & 52.1 \\   & VecAdd  & 76.6 & 71.4 & 70.0 & 64.3 & 87.2 & 66.9 & 47.4 & 74.5 & 71.1 & 7.46 & 2.75 & 51.6 \\   & OrthoProj  & 51.1 & 82.6 & 50.6 & 72.4 & 52.3 & 58.0 & 51.4 & 65.1 & 75.5 & 2.29 & 2.88 & 52.9 \\   & PaCE (Ours) & **93.7** & **97.9** & **97.7** & **94.9** & **98.9** & **96.6** & **99.3** & **90.8** & **98.9** & 7.52 & 2.85 & 54.1 \\   

Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in **bold** and the second best is underlined.

Figure 5: An example of jailbreaking LLaMA2-7B-Chat and detoxification by PaCE. PaCE successfully detoxifies the response while maintaining the instruction-following capability.

Figure 6: The detoxification performances for LLaMA2-13B w.r.t. the dictionary size.

### Improving Faithfulness and Removing Negative Sentiment

We evaluate the framework based on the response's faithfulness and sentiment when input prompts requests for information involving biographical facts or minority social groups. Faithfulness reflects the level of factuality in the generation, and sentiment describes the emotional tone behind the generation. In short, we find PaCE effective in improving the faithfulness and removing negative sentiment in LLMs' outputs. We describe the setup, metrics and method below.

**Setup.**_Faithfulness_: We use the FactScore suite and the fact evaluator for faithful biography generation . The suite is divided into labeled and unlabeled subsets used in different sections of the original paper. Our table reports the Labeled Score (LS), the total number of Labeled Atomic Facts (LAF), the Unlabeled Score (US), and the total number of unlabeled Atomic Facts (LAF). _Sentiment_: We use the HolisticBias suite  and hate speech evaluator  to measure the sentiment of the response to underrepresented descriptors. The reported numbers are the average of non-negative sentiment scores for underrepresented groups categorized by Gender (GN), Occupation (OC), and Nationality (NT). During the sentiment revising, the concept setups for all approaches follow the detoxification setup. For the faithfulness experiments, PaCE removes the top 50 undesirable (hallucinatory) concepts ranked by the partitioner. The Prompting approach instructs the LLM not to output sentences relevant to these top concepts. The VecAdd and OrthoProj operate on the concept vector of 'fabrication'.

**Results.** Our results are shown in Table 4. For both 7B and 13B models, PaCE achieves more factual responses and improves the sentiment according to most metrics. For linguistic performance, our method ranks right after the Vanilla method for the larger 13B model, and achieves comparable

    &  Time per \\ Respons \\  } &  Time per \\ Token \\  } &  Time per \\ Response \\  } \\  Vanilla & 12.4 & 0.041 & 20.7 & 0.076 \\ VecAdd & 16.3 & 0.062 & 29.1 & 0.109 \\ OrthoProj & 143.7 & 0.514 & 221.6 & 0.780 \\ PaCE (Ours) & 44.8 & 0.119 & 50.3 & 0.149 \\      Safety \\ (\%, \(\)) \\  & 
 Fluency \\ (\(\)) \\  \\  PaCE (LLaMA2-7B-Chat) & 50.2 & 7.26 \\ + Decomposition on \(10^{4}\) Concepts & 57.6 & 7.58 \\ + Clustering of Concepts & 62.3 & 7.63 \\ + Concept Partitioner & 65.1 & 7.70 \\ + Removal of Top \(50\) Concepts & 76.5 & 8.07 \\   

Table 2: Computation time (in seconds) evaluation Table 3: Ablation study for PaCE on the for PaCE and representation manipulation baselines. detoxifying LLaMA2-7B. Starting from a reported numbers are the average of non-negative sentiment scores for underrepresented groups categorized by Gender (GN), Occupation (OC), and Nationality (NT). During the sentiment revising, the concept setups for all approaches follow the detoxification setup. For the faithfulness experiments, PaCE removes the top 50 undesirable (hallucinatory) concepts ranked by the partitioner. The Prompting approach instructs the LLM not to output sentences relevant to these top concepts. The VecAdd and OrthoProj operate on the concept vector of ‘fabrication’.

Figure 7: Ablation study for solvers. We observe that greedy solvers can improve computational speed at the cost of safety performance.

results for LLaMA2-7B. Overall, we argue PaCE is an effective method for improving faithfulness and sentiment revising.

### Representation Space Sampled by PaCE-1M

Our collected dataset of conceptual representations enables us to investigate the geometry and potential applications of the representation (activation) space.

**Concept Clustering and Retrieval.** Here we explore the semantic structure of the activation space of the LLaMA2-13B-Chat by visualizing the first 10,000 concepts from the PaCE-1M dataset. We apply a dimensionality reduction method UMAP  on the concept vectors and visualize the first two dimensions in Figure 8. Concept vectors with similar semantics appear to be close to each other: e.g., in Figure 8 (1), concepts such as 'college', 'university', 'Academy', and 'Institute' are related to Education and they are close in the UMAP space. Notably, concepts of different semantics are clearly separated: those related to Education, Countries/States, Cities, Food and Clothing, and Positive Emotions respectively form distinct clusters. In particular, while concepts relevant to geography are closely clustered in Figure 8 (2), we observe a clear boundary between concepts related to Countries/States and those to Cities. These semantic structures indicate that the activation space sampled by our PaCE-1M dataset can capture and organize semantic information of the concepts, enabling further analysis and manipulations in PaCE. Figure 10 further reports the concept retrieval by evaluating the distance between a target concept with other concept vectors in the activation space. We observe organizational structure from the concept clusters based on their semantics. For instance, vectors for the concept 'affection' and 'friendship', are geometrically close and semantically relevant to the concept 'love.' Zooming out, such semantic structures are observed throughout the activation spaces of LLaMA2, and we conjecture they generalize to those in other LLMs. We provide more details of clustering and retrieval in Appendix C.2 and Appendix C.3.

## 5 Discussion

We provide discussions on the monosemanticity of concepts and connections among different alignment paradigms in this section. We also argue how PaCE handles context-dependent concepts.

### Polysemy of Words

While VecAdd and OrthoProj may be affected by the polysemy of words, PaCE's overcomplete dictionary allows accurate analysis of the target representation through sparse decomposition. Table 1 and Table 4 show that PaCE outperforms OrthoProj and VecAdd on linguistic metrics. We attribute the high helpfulness of PaCE to the large-scale dictionary with sparse coding, explained as follows.

**Comprehensive Coverage.** Since the dictionary is large, concepts with single and clear semantics are involved. E.g., if the stimuli of 'kill' may have different meanings, there exist other more polarized concept vectors such as'murder' (more harmful) and'spend' (more benign).

**Parsimony of Solution.** Sparse coding aims to choose the fewest concepts to reconstruct the latent representation (i.e., parsimony). For the sake of argument, assuming the sentence is about 'killing time' and the vanilla LLM has the correct semantic understanding of its benignness, the latent representation of the whole sentence will be closer to concepts such as'spend' and 'time' rather than string-matching to 'kill' (which in your setup could have mixed harmful and benign senses). As the sparse coding of the target representation promotes the parsimonious selection of concepts with monosemantics, it helps to represent benign contexts correctly without assigning significant weights to ambiguous terms like 'kill'.

### Different Alignment Paradigms

As mentioned in SS1, beyond activation engineering, there are other alignment paradigms such as Supervised Fine-Tuning (SFT) , Reinforcement Learning from Human Feedback (RLHF) , and Knowledge Engineering (KE) [14; 74]. We clarify the main advantages of PaCE over them.

**Training-Free.** RLHF, SFT, and KE all need to tune the parameters of LLM, which potentially degrade the well-structured priors of the pre-trained LLM. Taking a step back, even if LoRA is adopted for these paradigms, the training/tuning incurs significant computation and memory costs. PaCE does not modify the parameters of LLM and requires no training. It better preserves the priors of LLM, provides a low-resource alignment solution, and retains the general linguistic capabilities.

**Interpretable and Adaptive.** The solved coefficients of PaCE are an accurate interpretation of how a user input's representation is composed in the concept space. Also, when a new alignment goal is set, RLHF, SFT, and KE need to collect sufficient task samples and tune the LLM on the new dataset. In contrast, PaCE just needs to run the concept partitioner through PaCE-1M, which is expected to be much faster and more convenient.

### Context-dependent Concepts

The structured activation space of LLMs and the large-scale concept dictionary of PaCE help to handle the influence of the context for a concept in the target prompt. As the LLM scales up, its capability to capture and utilize contextual information grows with the help of attention modules. The activation space, as already used for many representation manipulation methods, is expected to convey the underlying semantic information of concepts in the sentence (context). That is, the space hosting concept vectors is not collapsed, and it is structured to distinguish different concepts. The representation (activation) to be steered at inference time encodes the context and conveys the semantics of a concept based on the context. Then, since our overcomplete concept dictionary in PaCE widely covers concepts of various categories, the sparse coding on this dictionary will effectively analyze the target representation as the linear combination of these concepts.

## 6 Conclusion

In this paper, we present PaCE, an activation engineering framework designed for aligning LLMs by effectively and efficiently addressing undesirable representations while retaining linguistic capabilities. By constructing a large-scale concept dictionary and leveraging sparse coding for activation decomposition, PaCE opens up new research avenues for training-free LLM alignment. Our experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising demonstrate that PaCE achieves state-of-the-art performance compared to existing representation manipulation approaches. PaCE not only ensures alignment with less cost but also adapts to evolving alignment goals without significantly compromising the LLM's linguistic proficiency. We open-source the PaCE-1M dataset to facilitate future research and practical applications of LLM alignment, and will release the source code soon. We further elaborate on the potential limitations, societal impacts, and future works of PaCE in Appendix B.7.