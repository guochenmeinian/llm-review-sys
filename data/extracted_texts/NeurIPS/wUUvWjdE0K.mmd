# Legendre-SNN on Loihi-2: Evaluation and Insights

Ramashish Gaurav\({}^{1}\)1 Terrence C. Stewart\({}^{2}\) Yang Yi\({}^{1}\)

\({}^{1}\)Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA 24060, USA

\({}^{2}\)National Research Council Canada, University of Waterloo, Waterloo, ON N2L3G1, Canada

###### Abstract

A majority of the works on Spiking Neural Networks (SNNs) do _not_ deploy and evaluate their network on a neuromorphic hardware. This not only limits the credibility of their claims of energy-efficiency and latency gains, but also discounts the opportunity to appraise neuromorphic technology for real-world computing. Herein, we especially study the technical facets of _deploying_ and _evaluating_ a recently formulated _State-Space Model_ based spiking network called "Legendre-SNN" on Loihi-2 neuromorphic hardware. Legendre-SNN is a highly resource-efficient _reservoir_-based univariate Time-Series Classification (TSC) model. This work's emphasis is not only on its deployment on Loihi-2, but also on leveraging the Loihi-2 embedded Lakemont (LMT) cores for its _non-spiking_ reservoir deployment and spike encoding. Since the documentation to program LMT is very limited, researchers often implement their _non-spiking_ operations on _less_ power-efficient CPUs (than LMTs). Here, we present the technical know-how to program LMT (as part of our reservoir deployment) that can be employed by later works. In our evaluation of Legendre-SNN on Loihi-2 hardware, we pleasantly find that it outperforms a complex LSTM-Conv integrated architecture on \(3\) of \(15\) datasets. We also present the _energy_ & _latency_ metrics of Legendre-SNN on Loihi-2, where we find that (for our settings) the reservoir on LMT consumes more than \(85\%\) of total energy; consequently, we advocate for a _spiking_ reservoir in Legendre-SNN.

## 1 Introduction

AI computing with Spiking Neural Networks (SNNs) is progressively gaining traction in the wake of calls for Green AI [1; 2; 3]. Training Deep Learning (DL) models on power-hungry GPUs is highly energy intensive [4; 5], and inference too - consumes notable amount of energy, e.g., ResNet-\(50\) consumes more than \(2.5\)K Joules per sample in inference mode . SNNs on the other hand have proven highly energy efficient when deployed (generally for inference) on neuromorphic hardware , e.g., Loihi [8; 9]. Note that SNNs are still trained primarily on GPUs with methods falling under two categories: ANN-to-SNN conversion and Direct Training. Under ANN-to-SNN conversion, an ANN is first trained via conventional Back-propagation followed by its _conversion_ to an isomorphic SNN [10; 11; 12]. Under Direct Training, an SNN is _directly_ trained either via non-gradient methods [13; 14] or by accounting for its non-differentiable spike function , e.g., SLAYER .

Time-Series Classification (TSC) is one popular AI domain that has ubiquitous applications in Edge Computing/Smart Wearables/IoT devices. RNNs and their specialized types, e.g., GRUs and LSTMs have proven quite effective for a variety of TSC tasks, however, at the cost of _high_ energy consumption - owing to their deployment on multi-CPUs/GPUs. Training RNNs is also computationally complex and time consuming in contrast to training Reservoir Computing (RC) networks , where, the recurrent _reservoir_ is kept static, and learning adapts only the _linear readout_ layer. RC networks have outperformed RNNs on a variety of tasks with ample room for improvements . Note that the simplicity of RC networks also allows for their efficient hardware implementations . A few popular RC models are Echo State Networks (ESNs)  and Liquid State Machines (LSMs) , where the ESNs are composed of _non-spiking_ neurons and LSMs are composed of _spiking_ neurons.

Recently, a novel reservoir-based SNN called _Legendre-SNN_ (LSNN) was formulated by Gaurav et al.  that outperformed the LSM-based models. The LSNN consists of a static reservoir followed by an encoding layer and a trainable _non-linear_ spiking readout layer - unlike conventional RC. The reservoir in LSNN is actually a _State-Space Model_ (SSM) implemented via simple matrix operations (_no_ neurons) and its output is encoded to spikes, while the non-linear readout layer comprises _one_ spiking hidden layer followed by a classification layer - more architectural details in Sec 2.4. Note: the authors  in their ablation study found that the LSNN _without_ a hidden layer still outperformed the LSM-based models; they also energy-profiled LSNN on Loihi-1 (only the post-reservoir spiking part). We chose LSNN for our deployment because it offers a _unique_ blend of crucial _non-spiking_ & _spiking_ components that enable the utilization of Loihi-2's _two_ computational resources: Lakemont cores & Neuro-Cores; also, quantized SSMs are gaining traction . SSMs are dynamical systems that accept an input \(u(t)\) and maintain an internal state \(x(t)\), where \(x(t)\) captures the latent dynamics governing the response \(y(t)\); state \(x(t)\) evolves over time guided by a defined _state-transition_ rule, and is used to predict the future states and generate the output \(y(t)\) via an _observation_ rule . The SSM used in LSNN is called _Legendre Delay Network_ (LDN) [24; 25] - more details in Sec 2.3.

In this study, given that the authors of LSNN did _not evaluate_ it (on any dataset) on a neuromorphic chip, we put its performance to test on Loihi-2. Note: the mix of _non-spiking_ & _spiking_ also makes LSNN _non-trivial_ to port _entirely_ on Loihi-2. With these motivations, our contributions are:

* We deploy & evaluate LSNN on _physical_ Loihi-2 and report energy & latency metrics. This work can be considered one of the firsts to deploy a RC-based TSC model on _Loihi-2_.
* In a first (to our best knowledge), we implement a quantized SSM (i.e., the LDN) on the embedded Lakemont cores of Loihi-2 and energy-profile it. As part of this implementation:
* We add to the _scarce_ technical documentation to program Lakemont cores (Appx F)

## 2 Background

We now present the necessary background relevant to our work. We start with a short description of Loihi-2 & Lava, followed by the description of the LDN (an SSM used as reservoir) and the LSNN.

### Loihi-2 chip

Loihi-2  is the \(2^{}\) generation neuromorphic chip from Intel, which improves upon Loihi-1 . Notably, Loihi-2 comes with \(1,048,576\) spiking units, \(120\) million synapses, and \(6\) embedded x86 Lakemont (LMT) cores per chip; it has \(128\) Neuromorphic Cores (Neuro-Cores), each housing \(8192\) spiking units. The LMT cores are responsible for many essential tasks, e.g., facilitating spike-based communication, data I/O (i.e., encoding and decoding), and network management/configuration, etc. The embedded LMTs execute standard C code, however, with support for _only_\(32\)-bit operations; this implies that _only quantized operations_ can be executed on LMTs. The spiking units on Neuro-Cores are asynchronous programmable CURET BAsed (CUBA) neurons that generate binary (as well as, \(32\)-bit graded) spikes; the Neuro-Cores also support Three-Factor Rules  based On-chip training.

### Lava software suite

The Loihi-2 chips can be programmed using Intel's "Lava" software suite; the suite consists of multiple libraries e.g., lava, lava-dl, etc., that can be used to build and train a variety of spiking networks, along with support for their deployment and profiling on Loihi-2. A concise documentation and programming features of Lava2 can be found in ; herein, we describe only those programming paradigms of Lava that are most relevant to our work. The two important lava paradigms used to build (our) Loihi-2 deployable networks are Process and ProcessModel. Note that a Process can have multiple _corresponding_ ProcessModels depending on the hardware backend the Process is intended to be deployed upon. A Process defines the _interface_ of a component of the network, while the corresponding ProcessModel defines the _implementation_ of its Process (on the desired backend). Lava supports \(3\) types of ProcessModels: PyLoihiProcessModel (meant for Python implementations to be deployed on CPU), CLoihiProcessModel (meant for C implementations to be deployed on embedded LMT), and NcProcessModel (meant for NxCore implementations to be deployed on Neuro-Cores). The ProcessModels of different Processes synchronize among themselves via a _protocol_ called LoihiProtocol that consists of multiple execution _phases_ implemented by the participating Processes (details in Appx A). Coming to lava-dl, it assists in building and directly training the SNNs on GPUs (via its slayer  API), and their deployment on Loihi-2 (via its netx API). Lava also provides a _simulation_ backend for the _physical_ Loihi-2 chip; the switch to _simulation_ and _physical_ backends can be made via Lava's Loihi2SimCfg and Loihi2HwCfg APIs.

### Legendre Delay Network (LDN)

LDN - a type of an SSM introduced by Voelker et al. [24; 25] is a neural approximation of a Linear Time-Invariant system implementing a delay of \(\) secs, i.e., for a univariate input signal \(u(t)\) to the LDN, its output is \(y(t)=u(t{-})\). Following are the state equations of the LDN in _continuous_-time i.e., Eqs (1) & (2), and _discrete_-time i.e., Eqs. (3) & (4) (note: Legendre-NN uses only Eq (3)):

\[(t) =Ax(t)+Bu(t)\] (1) \[y(t) =Cx(t)+Du(t)\] (2)

\[[t+1] =[t]+[t]\] (3) \[[t] =[t]+[t]\] (4)

where the Eq (1) (& Eq (3)) is the _state-transition_ equation and the Eq (2) (& Eq (4)) is the _observation_ equation. Note that \(u(t){}\), \(x(t){}^{d}\), and \(y(t){}\) (\([t]\), \([t]\), and \([t]\)) are the LDN's input, state-vector, and output in continuous-time (and discrete-time). The values of the state-matrices \(A\), \(B\), \(C\), & \(D\) (and \(\), \(\), \(\), & \(\)) in the equations above that define an LDN are stated in Appx B.

### Legendre-SNN (LSNN) 

LSNN is a highly resource efficient reservoir-based SNN for TSC tasks. In their experiments, the authors  used a maximum of \(120\) "Integrate & Fire" (IF) neurons and achieved SoTA _spiking_ performance on \(5\) univariate-TSC datasets. The LSNN constitutes of an LDN functioning as a _static_ reservoir followed by an SNN comprising an encoding layer, _one_ hidden and an output layer - both trainable. The LDN accepts a univariate signal \([t]\) and outputs a \(d\)-dimensional state-vector \([t]\) (authors  refer \([t]\) as the _temporal features_ of input \([t]\)). The features \([t]\) are rate-encoded to binary spikes via a two-neuron (IF) encoder layer (ENC). Note that the scalars in \([t]\) can either be _positive_ or _negative_, therefore, one needs a _pair_ of neurons, each sensitive to the sign of the scalar, to encode. Thus, the number of neurons in ENC is \(2{}d\) (henceforth, \(2d\)). The ENC layer is followed by a hidden layer (HDN) of \(3{}d\) (henceforth, \(3d\)) number of IF neurons; the spikes of which are forwarded to the _non-spiking_ output layer (OTP), where, the classes are inferred on the maximally accumulated voltage of OTP nodes. Thus, the total number of IF neurons in LSNN is \(2d{+}3d{=}5d\) and the number of _trainable_ connections is \(2d{}3d{+}3d{}c{=}6d^{2}{+}3dc\) (where \(c\) is number of OTP nodes/classes). Compared to the original LSNN, there are _two_ distinctions in our adapted LSNN (see Fig 1): (1) Instead of a _non-spiking_ OTP layer, for programming constraints/ease on Loihi-2 we use a _spiking_ OTP layer; we forward these OTP spikes to a new classification layer (CLS) that infers class on the maximally firing OTP neuron, & (2) Except the ENC layer IF neurons, all the neurons in HDN and OTP are CUBA neurons (details later). Henceforth, term "LSNN" implies our adapted LSNN. In Fig 1, the number of "Relay" nodes is equal to \(d\), and each of them _duplicates_ the corresponding scalar in \([t]\) and forwards it to ENC. The total number of neurons in our adapted LSNN is \(5d{+}c\) and the number of trainable connections remain same (similar to  - only the black connections in Fig 1 are trainable). In the original LSNN, the authors considered the maximum value of \(d{=}24\) and \(c{=}2\); in our experiments, this implies that the number of neurons\({=}5{}24{+}2{=}122\) and number of connections\({=}6{}24(24{+}1){=}3600\).

## 3 LSNN's Training on GPU, and Evaluation & Profiling on Loihi-2

We now explain our framework to train our adapted LSNN (Fig 1) on GPU and evaluate & profile it on Loihi-2. Note that the reservoir LDN in LSNN is implemented via regular matrix operations (i.e., Eq (3)) and _not_ with spiking neurons; whereas, the spiking network following the LDN is composed of IF (in ENC) and CUBA neurons (in HDN & OTP). Although the LSNN as a whole can be easily trained on CPU/GPU, its evaluation & profiling on Loihi-2 is _not_ straightforward, the _non-spiking_ LDN and _spiking_ network must be accounted appropriately; we do that by deploying the LDN along with ENC layer on the embedded LMT cores and the following HDN & OTP layers on the Neuro-Cores. However, implementing the LDN on LMT is _not_ trivial either - because the LMT cores support only 32-bit signed integer operations. Therefore, to implement the LDN (& ENC layer) on LMT, we must _quantize_ all its real-valued variables and operations to 32-bit integers. Note that for the uniformity of our experiments, we train the LSNN with _quantized_ LDN (instead of a _continuous_-valued one). The

Figure 1: Our _adapted_ LSNN model . Connections ENC to HDN to OTP are all-to-all.

[MISSING_PAGE_FAIL:4]

(see Appx G). The _two_ parameters that characterize the LDN (in LSNN) are its delay value \(\) and dimension \(d\) of its state-vector \([t]\) (these values define the state-matrices \(\)\(\&\)\(\)). In our experiments, we tune \(\)\(\)\(\{110,130,150\}\) time-steps, and \(d\)\(\)\(\{4,6,8,10,12,14,16,24\}\); the CUBA neurons' \(_{}\) and \(_{}\) decays are tuned in \(\{0.00,0.10,0.20\}\). The batch size and number of training epochs vary with dataset and can be found in our code . For all the experiments we shuffle the train and test data every \(20^{}\) epoch and halve the learning rate \(\) every \(40^{}\) epoch (initial \(=0.001\)). Note that we set the factor \(\)\(=\)\(4096\), such that the operations in Eq (5) are well represented in \(32\)-bit.

### Training and test procedure on CPU/GPU

As stated above, we train our LSNN (SlayerSNN specifically, with quantized LDN) on GPU (see Fig 2a). For each dataset, we conduct grid-search on the abovementioned values of \(\), \(d\), \(_{}\), & \(_{}\) (we ensure \(\) signal duration). Let \(_{m}\) denote one combination of \((_{i}\),\(d_{j}\),\(_{_{i}}\),\(_{_{i}})\); for each \(_{m}\) we train the LSNN on \(5\) different \(s_{n}\)\(\)SEEDs. Note that for each of the training runs for a dataset, we obtain the best performing LSNN test-accuracy: \(slyr\_acc_{s_{n}}^{_{m}}\), and use the same trained network to obtain the _corresponding_LavaLSNN accuracy: \(l2sim\_acc_{s_{n}}^{_{m}}\) on Loihi-2 _simulation_ i.e., on CPU. We then compute the final test accuracies reported in Tab 1 as follows (where \(\)=\(slyr\) or \(l2sim\)):

\[\_best=_{_{m},s_{n}}(\_acc_{s_{n}}^{_{ m}}),\_mean=_{_{m}}(}{}( \_acc_{s_{n}}^{_{m}}))\] (6)

### Evaluation procedure on _physical_ Loihi-2

We evaluate our _LavaLSNN_ on _physical_ Loihi-2 for all the \(15\) datasets; Fig 2b shows _LavaLSNN_'s respective backends for deployment. Since the Loihi-2 boards on Intel's INRC cloud are on shared access, we prioritize the variety of _LavaLSNN_ models that we evaluate on _physical_ Loihi-2. Therefore, for each dataset, we first choose and evaluate _all_ those _LavaLSNN_ models that correspond to \(l2sim\_best\), followed by evaluating a next few models with \(l2sim\_acc_{s_{n}}^{_{m}}\) within \(2\%\) or \(3\%\) absolute margin of \(l2sim\_best\) accuracy; we then report the best accuracy (among all) - \(l2hw\_best\) in Tab 1.

### Results Analysis

We now analyze our results in Tab 1, where we compare ours with that of LSTM-FCN . LSTM-FCN is composed of _one_ LSTM block, _three_ 1D Convolution blocks (each accompanied with batch normalization), and dropout & average pooling blocks. As the LSTM-FCN is a significantly complex architecture than the LSNN and the related work  provides a benchmark on all our experimented datasets, we choose it for comparison. In Tab 1, the simple LSNN either achieves the _same accuracy_ or _outperforms_ the well established LSTM-FCN on \(5\) datasets - considering the best and average performance of both: LSNN on GPU (\(slyr\_*\)) and on Loihi-2 _simulation_ (\(l2sim\_*\)). Regarding the variation of \((slyr\_acc_{s_{n}}^{_{m}})\) and \((l2sim\_acc_{s_{n}}^{_{m}})\) (over SEEDs \(s_{n}\)) with hyperparameters \(d\), \(\), \(_{}\), & \(_{}\), we did _not_ find any conclusive trend between them (similar to ); although, for a few datasets, we observed that higher \(d\) results in higher accuracy. Coming to LSNN's performance on _physical_ Loihi-2 (\(l2hw\_best\)), we see it _outperforming_ LSTM-FCN on \(3\) datasets. It is expected that a typical network's deployment on a hardware and its simulation should produce same results, however, we did _not_ observe this in our Loihi-2 experiments. In fact, for a few datasets, we observed that _non_\(l2sim\_best\) models achieved better accuracies on Loihi-2 hardware; here too, we observed that _generally_ higher \(d\) achieved better accuracies. Regarding comparison with other spiking networks, we found other _spiking_ works on \(8\) of our experimented datasets. For the first five datasets in Tab 1 - achieved (max) \(98.49\), \(93.56\), \(82.72\), \(99.51\), \(80.43\) (\(\%\)) accuracies (resp.), for _Cofe_ & _Light_2: - achieved \(100.0\) & \(80.33\) (\(\%\)) accuracies (resp.), and for _ToSe_2: - achieved \(83.00\%\). Note:  tune their LSNN on a much wider set of hyper-parameters with a different loss function, and  use trainable ReLU Conv layers to extract temporal features - hence, they obtain higher accuracies. Additionally, we conducted two _intrusive_ experiments with LSNN, where (a) we used _continuous_ valued LDN and (b) the neuron decays (\(_{}\) & \(_{}\), along with the weights) were set to trainable. For _ToSe_2 dataset, upon inferring with such an \(l2sim\_best\) model on _physical_ Loihi-2, we obtained \(93.08\%\) accuracy (same as LSTM-FCN, Tab 1); we present more details in Appx H.

## 5 Discussion & Insights

We now profile the LSNN on _physical_ Loihi-2 and present some insights from its analysis.

Figure 3: _LavaLSNN_ Processes, their connections and supported backends; ‘\(\)’ is an adapter Process to adapt spikes from Neuro-Core to CPU. For _LavaLSNN’s physical_ Loihi-2 deployment, blocks are color coded with Fig 2b (backends are: LMT & Neuro-Core); for deployment on Loihi-2 _simulation_, all backends are CPU.

[MISSING_PAGE_FAIL:6]

## Appendix A LoihiProtocol

Here we briefly describe the LoihiProtocol used to synchronize the Lava Processes, more details can be found at https://lava-nc.org/. The _phases_ in LoihiProtocol are executed in a _particular order_, and all the phases (except one) have an implementable _guard_ method that returns a boolean value - determining if its corresponding phase should be executed in a given time-step. The phases and their guard methods are described below; phases are in the order of their execution:

1. Spiking phase (run_spk()): This phase is executed \(1^{}\) in order and has _no_ guard method, i.e., it is executed unconditionally every time-step.
2. Pre-management phase (run_pre_mgmt()): Executed \(2^{}\) in order, and has a guard method pre_guard() that must return True along with lrn_guard() to execute this phase.
3. Learning phase (run_lrn()): Executed \(3^{}\) in order, and its guard method lrn_guard() must return True for this phase to be executed.
4. Post-management phase (run_post_mgmt()): Executed \(4^{}\) in order, and its guard method post_guard() must return True for this phase to be executed.
5. Host phase (run_host_mgmt()): Executed \(5^{}\) in order, its guard method host_guard() must return True for this phase to be executed.

Note: one can choose to skip a few phases depending on their implementation requirements.

## Appendix B LDN's State-matrices

Below, we mention the values of the state-matrices \(A\), \(B\), \(C\), and \(D\) that define an LDN - characterized by the dimension \(d\) (of its state-vector \(x(t)\)) and delay \(\) (i.e., its encoding memory window):

\[A=}{}, a_{i,j}=(2i+1)-1&i<j\\ (-1)^{i-j+1}&i j B=}{ }, b_{i}=(2i+1)(-1)^{i}\] (7a) \[C=[c]_{i}, c_{i}=(-1)^{i}_{j=0}^{i}(-1)^{j}\] and \[D=0\] (7b)

for \(i,j[0,d-1]\). Note: \(\), \(\), \(\), and \(\) in Eqs. (3) & (4) are obtained from \(A\), \(B\), \(C\), and \(D\) via Zero-Order Hold (ZOH) method - Eq. (8) below:

\[=^{(A t)},=A^{-1}(^{(A  t)}-I)B,=C,=D\] (8)

where \(I\) is an Identity matrix, \(^{()}\) is _matrix exponential_ function, and \( t{=}1ms\).

## Appendix C LSNN's neuron equations

Here we present the neuron equations of our adapted LSNN.

### Enc layer

We rate encode the quantized LDN state-vectors \(}[t]\) to binary spikes \(S[t]\) (using Heaviside \((.)\)).

\[=<.}[t]>+, [t]=[t-1]+ S[t]= ([t]-_{})\] (9)

where \(\) (=1) & \(\) (=0) are the neuron's _gain_ & _bias_ values, \(\) is encoder vector \(\{-1,1\}\), and \([t]\) & \(_{}\) are the neuron's quantized _voltage_ & _threshold_ respectively (we set \(V_{}\)=\(1,_{}\)=\( 1\)).

### Hdn & Otp layers

Following are the equations of the CUBA neurons used in HDN and OTP layers. Note that depending on the _voltage_ decay \(_{}\), the CUBA neurons will function either as LIF or IF neurons.

\[U_{i}[t]=(1-_{})U_{i}[t-1]+_{j}W_{i,j}S_{j}[t], V_{i}[t]=(1-_{})V_{i}[t-1]+U_{i}[t]\] (10a) \[S_{i}[t]=V_{i}[t] V_{}, V_{i}[t]=V_{i}[t](1-S_{i}[t])\] (10b)

## Appendix D Spike-train synchrony tests

Here we present the spike-train synchrony tests to validate our quantization of the LDN. Spike-train synchrony metrics measure the extent of _temporal alignment_ of _two_ spike-trains, i.e., how _similar_ or _dissimilar_ they are; for which, a number of methods exists , we pick _three_: (1) Victor-Purpura distance  (VPR-dist) - a popular time-scale dependent metric, (2) Inter-Spike Interval distance  (ISI-dist) - a time-scale independent metric, & (3) SPIKE-synchrony  (SPK-sync) - another time-scale independent metric (libraries: PySpike  & Elephant ) For any two spike-trains:

* based on certain defined operations. For highly _dissimilar_ spike-trains, VPR-distance metric will be high, and vice-versa.
* ISI-distance metric also measures the _dissimilarity_ between them by using inter-spike intervals to estimate instantaneous local firing rates of spike-trains and quantifying that difference. For highly _dissimilar_ spike-trains, ISI-distance will be high, and vice-versa.
* SPIKE-synchrony metric measures the _similarity_ between them by using coincidence windows to determine if a pair of spikes, each from two different spike-trains are coincident or not. For highly _similar_ spike-trains, SPIKE-synchrony will be high, and vice-versa.

We now explain the _procedure_ of our spike-train synchrony tests. We consider spike-trains generated from _four_ implementations of LDN: _Two_ Python implementations - (1) Original Continuous-valued LDN implemented on CPU (OC-LDN-CPU-Py) & (2) Quantized LDN implemented on CPU (QT-LDN-CPU-Ly), and _two_ Lava implementations - (3): Quantized LDN implemented on CPU (QT-LDN-CPU-Lv) & (4): Quantized LDN implemented on Loihi-2 embedded LMT cores (QT-LDN-LMT-Lv). Note that in our experiments, we have utilized _three_ (all quantized) implementations of LDN, i.e., QT-LDN-CPU-Py while training, and QT-LDN-CPU-Lv & QT-LDN-LMT-Lv while evaluation on Loihi-2's _simulation_ & _physical_ backends respectively. For comparisons among the _four_ implementations of LDN, we consider all the \(6\) pairs (i.e., taking two at a time), denoted below:

* _Comparisons between Continuous-valued LDN and its Quantized implementations_
* \(OC^{}_{}\)-vs-\(QT^{}_{}\): Comparison between OC-LDN-CPU-Py and QT-LDN-CPU-Py
* \(OC^{}_{}\)-vs-\(QT^{}_{}\): Comparison between OC-LDN-CPU-Py and QT-LDN-CPU-Lv
* \(OC^{}_{}\)-vs-\(QT^{}_{}\): Comparison between OC-LDN-CPU-Py and QT-LDN-LMT-Lv
* _Comparison between two Quantized LDNs in Lava: one on CPU and another on LMT_
* \(QT^{}_{}\)-vs-\(QT^{}_{}\): Comparison between QT-LDN-CPU-Lv and QT-LDN-LMT-Lv
* _Comparisons between Quantized LDNs in Python and Lava (on CPU & LMT)_
* \(QT^{}_{}\)-vs-\(QT^{}_{}\): Comparison between QT-LDN-CPU-Py and QT-LDN-CPU-Lv
* \(QT^{}_{}\)-vs-\(QT^{}_{}\): Comparison between QT-LDN-CPU-Py and QT-LDN-LMT-Lv

With respect to designing the LDN, note that it is characterized by two values: '\(d\)' & '\(\)'; for our tests, we consider all the values of \(d\) & \(\) over which we tune our LSNN, i.e., \(d\{4\), \(6\), \(8\), \(10\), \(12\), \(14\), \(16\), \(24\}\) and \(\{110\), \(130\), \(150\}\), thus, a total of \(8 3=24\) combinations or \(24\) LDNs. Consider one input signal and _two_ different _implementations_ of an LDN, we get _two sets_ - each of \(d\)-dimensional state-vectors \([t]\) - that are encoded to binary spikes via the two-neuron encoder system (ENC in LSNN), thus producing _two sets_ of \((2 d)\)-dimensional spike-trains - i.e., one _set_ foreach _implementation_ of LDN. Note that a scalar similarity/dissimilarity \(score\) is computed between _two_ individual spike-trains. Therefore, we align those _two sets_ of spike-trains dimension-wise, and \(\) dimensions \(i(2 d)\) we compute a \(score_{i}\) between the _two_\(i^{}\)-dimension's spike-trains followed by the mean of all \(score_{i}\), thus producing one _final-score_ - representing how similar or dissimilar are the \((2 d)\) spike-trains on average (for the considered LDN and input signal). Likewise, for the same input signal we compute \(24\) similarity/dissimilarity _final-scores_ for all the \(24\) LDNs. We repeat this process for \(75\) randomly chosen signals (\(5\) each from our \(15\) datasets), thereby obtaining \(75\) such \(24\)-dimensional _final-scores_; we then again compute mean and std across all these \(75\) vectors. Thus, for a considered comparison (out of the six above), this averaged \(24\)-dimensional _final-scores_ represents how similar/dissimilar are the two compared _implementations_ of LDN over the \(15\) datasets. For all our \(6\) comparisons, Fig 4 below shows the (considered) \(3\) types of spike-train similarity/dissimilarity mean _final-scores_ (and corresponding std) for all the \(24\) LDNs averaged over all \(75\) random signals.

As can be seen above, in Fig 4a, our quantized LDN implementation (\(QT^{}_{}\)) is quite similar to the continuous-valued LDN (\(OC^{}_{}\)), where the mean SPK-sync scores tend to be near \(1\) and the mean ISI-dist & VPR-dist scores are closer to \(0\). Coming to the Figs 4b & 4c, we see that both are same, this is because the quantized implementations of LDN via Lava on CPU (\(QT^{}_{}\)) & LMT (\(QT^{}_{}\)) are same; this is further proved in the Fig 4d where we see that ISI-dist & VPR-dist are exactly \(0\) and SPK-sync is exactly \(1\), i.e., perfect synchrony. However, note that the Figs 4b & 4c are _different_ from Fig 4a - ideally, this should _not_ be the case, as the underlying maths for \(QT^{}_{}\), \(QT^{}_{}\), & \(QT^{}_{}\) is same; the _difference_ exists because of our programming implementations in Python & Lava.

In our Python implementations (\(OC^{}_{}\) & \(QT^{}_{}\)), in the _first_ time-step (of the input signal's duration), the signal's _first_ element is rightly accounted for LDN's state-vector generation and encoding to spikes, whereas, in our Lava implementations (\(QT^{}_{}\) & \(QT^{}_{}\)), in the _first_ time-step, we generate an _all zero_ state-vector, thus (certainly) _no_ spikes; rather, from the _second_ time-step onwards the actual input signal is accounted for LDN's state-vector generation and encoding to spikes. Therefore, \(QT^{}_{}\) & \(QT^{}_{}\) run one time-step _late_ than \(OC^{}_{}\) & \(QT^{}_{}\). This behavior is partly because of the unique LoihiProtocol that our Lava Process - for inputting signals to the LDN follows. In this Process, every time-step, the run_spk() phase sends a contiguous scalar of the input signal to the LDN. Note that, by the LoihiProtocol's design, in every time-step, run_spk() phase is the _first_ phase to be executed. Since in the first time-step, the actual signal is yet to be initialized and the run_spk() phase executes first, it sends a zero scalar to the LDN; the next phase to execute (in the same first time-step) in our implementation is run_post_mgmt(), where the input signal then gets initialized. Thus, from the _second_ time-step onwards, the actual signal is accounted for state-vectors and subsequent spike generation. We opted for such Lava implementation for our experimental ease, however, one can choose to implement \(QT^{}_{}\) & \(QT^{}_{}\) to match \(QT^{}_{}\) outputs. In the Figs 4e &

Figure 4: _Spike-train synchrony tests._ All \(6\) pair-wise comparisons of _four_ LDN implementations. For VPR-dist & ISI-dist, lower is better, and for SPK-sync, higher is better. Solid line is mean and shaded region is std.

4f, we explicitly compare our \(QT_{}^{}\) with \(QT_{}^{}\) & \(QT_{}^{}\), where, we see that both the plots are expectedly same, and the spike-trains from \(QT_{}^{}\) & \(QT_{}^{}\) are quite similar to those from \(QT_{}^{}\).

Overall, Fig 4a shows the difference purely due to quantization, Figs 4b & 4c show the difference due to quantization and one time-step late processing, Fig 4d shows that \(QT_{}^{}\) & \(QT_{}^{}\) are exactly same, and finally, Figs 4e & 4f show the difference (only) due to one time-step late processing.

## Appendix E ProcessModels of LavaLSNN

The ProcessModels of our defined ImpSigToLdn, LdnEncToSpk, and OutSpkToCls Processes of LavaLSNN are briefly described below; their implementations are in our code .

* ImpSigToLdn: For this Process, we implement only _one_ProcessModel: PyInSpigToLdnModel that is inherited from lava's PyLoihiProcessModel and executes on CPU. It is responsible for feeding an input signal to the LDN (that runs either on CPU or on LMT).
* LdnEncToSpk: For this Process (considering _simulated_ and _physical_ Loihi-2), we implement _two_ProcessModels: (a) CLdnEncToSpkOnLmtModel that is inherited from lava's CLoihiProcessModel and executes on LMT, and (b) PyLdnEncToSpkOnCpuModel that is inherited from lava's PyLoihiProcessModel and executes on CPU. Depending on the backend, only _one_ of the two ProcessModels is executed. It is responsible for computing the quantized state-vectors \(}[t]\) i.e., temporal features and encoding these to binary spikes.
* OutSpkToCls: For this Process, we implemented only one ProcessModel: PyOutSpkToClsModel that is inherited from lava's PyLoihiProcessModel and executes on CPU. It is responsible for collecting the output spikes from NctxSNN and inferring classes/accuracy.

When LavaLSNN is deployed on Loihi-2's _simulation_ i.e., on CPU (via Loihi2SimCfg), following ProcessModels (of the constituent Processes) are executed: PyInSpigToLdnModel, PyLdnEncToSpkOnCpuModel, and PyOutSpkToClsModel. And when LavaLSNN is deployed on the _physical_ Loihi-2 chips (via Loihi2WcGg), following ProcessModels (of the constituent Processes) are executed: PyInSpigToLdnModel, CLdnEncToSpkOnLmtModel, and PyOutSpkToClsModel. All these ProcessModels follow the LoihiProtocol for synchronization. Note that we _have to_ deploy the signal-input Process:InSpigToLdn and output-spikes collection Process: OutSpkToCls (in case of both - Loihi-2's _simulation_ and _physical_ hardware) on CPU - as these two Processes are the I/Os. Also note that Lava automatically deploys Processes obtained from lava-dL's next API (in our case - the NctxSNN Process obtained from slayer-trained SlayerSNN) on the appropriate backends depending on the choice of: Loihi2SimCfg (on CPU) & Loihi2WcGg (on Neuro-Core).

## Appendix F CLdnEncToSpkOnLmtModel for deploying LDN and ENC layers on LMT

Documentation for programming Loihi-2 chips via Lava can be found at https://lava-nc.org/. On one hand, where the documentation to program Neuro-Cores is well presented there, on the other, _little to no_ documentation to program LMT cores is provided - this paucity limits the researchers to use LMT cores for their Loihi-2 deployments, and prompts them to otherwise use the CPU for implementing _non-spiking_ operations or _encoding_ continuous values to spikes. Note that the embedded LMT cores are _more_ power-efficient than CPUs. In Algorithm 1, we present our CLdnEncToSpkOnLmtModel that implements a quantized SSM (i.e., LDN) on LMT cores. Our implementation also serves as an _addition_ to the limited documentation on programming LMT cores.

We next explain our code in Algorithm 1 (Alg-1, note: only \(32\)-bit ops are supported on LMT) by first recalling that the CLoihiProcessModel: CLdnEncToSpkOnLmtModel has a corresponding Process: LdnEncToSpk that provides the interface to communicate with other Processes. Note that the C file (in our case: ldn_enc_to_spkn_lmt.c) containing the code in Alg-1 _must_ have a corresponding _header_ file bearing the same time (in our case: ldn_enc_to_spkn_lmt.h), see line 1 in Alg-1. The _header_ file should contain the function prototypes of all LoihiProtocol phases used in the C file. One should also include another _header_ file with name predefs_X_.h, where X is the name of CLoihiProcessModel of the Process to be executed on LMT; in our case X is CLd-nEncToSpkOnLmtModel (see line 2). Note that the file predefs_CLdnEncToSpkOnLmtModel.h is auto-generated containing the variables of the corresponding Process (in our case: LdnEncToSpk).

As can be seen in Alg-1 (lines \(8\),\(12\),\(27\),&\(32\)), one can implement each phase of the LoihiProtocol (on LMT) with an argument runState *rs passed to it. The current time-step of the execution can be accessed via the time-step attribute of *rs, which can be used to implement time-step dependent ops (lines \(13\), \(34\)). One can also define global variables (lines \(5\), \(6\)) and custom functions(line 19) in the \(\) file, as well as, access the variables defined in the corresponding Lava Process by their same name (lines \(13\), \(20\), i.e., ps_ts & ORDER) - this is because of the inclusion of auto-generated header file predefs_CLdnEncToSpkOnLmtModel.h. Note that all the functions can access all global variables and Process variables. An important thing to consider is that the Process running on LMT cores communicates with Processes running on CPUs and Neuro-Cores. In our case, the LMT Process receives a quantized input (i.e., the input signal \(}[t]\)) from an input Process on CPU; for the same, the recv_vec_dense() function (line \(40\)) can be used that accepts the *rs and two other variables: sig_inp & input. Note that sig_inp is an input port (defined in Ld_nEncToSpk_ Process) that receives a signal from the CPU Process, and input is a local variable that stores the received signal for local processing. Subsequently, when the input (for the current time-step) is processed and spikes are generated, they can be sent from the LMT Process to the Neuro-Core Process via the send_vec_dense() function (line \(83\)) that accepts *rs and two other variables: spk_out & spike_data. Note that spk_out is an output port (defined in LdnEncToSpk Process) that sends the spikes stored in local variable spike_data to the Neuro-Core Process.

```
1#include"ldn_enc_to_spk_on_lmt.h"
2#include"predefs_CLdnEncToSpkOnLmtModel.h"
3
4//DefinetheglobalLDNstatevectorwithmaximumORDER=128.
5int32t_g_ldn_state={0};
6int32t_g_volt={0};//SettwicetheORDERnumberofspikingneurons.
7
8intspk_guard(runState*rs){
9return1;//keeprunningthe'run_spk()'everytime-step.
10}
11
12intpost_guard(runState*rs){
13if(rs->time_step%ps_ts==1)//Presentationtimeofonesampleisover.
14return1;
15
16return0;
17}
18voidzero_out_global_arrays(){
19for(uint32_ti=0;i<ORDER;i++)
20g_ldn_state[i]=0;
21
22
23for(uint32_ti=0;i<2*ORDER;i++)
24g_volt[i]=0;
25}
26
27voidrun_post_mgmt(runState*rs){
28zero_out_global_arrays();
29}
30
31//Followingfunctioniscalledeverytime-step.
32voidrun_spk(runState*rs){
33
34if(rs->time_step%ps_ts==1)//Presentationtimeofonesampleisover.
35zero_out_global_arrays();
36
37int32_tinput[sig_inp.size];
38uint32_tspike_data[spk_out.size];
39
40recv_vec_dense(rs,&sig_inp,input);//Getu[t].
41//Note that the matrices Ap, Bp, encoders E, and ORDER are already defined //in Process. The variable ORDER can be accessed as *ORDER or ORDER.
* //Compute Ap
* x[t] and Bp
* u[t]. int32_t Apx[*ORDER], Bpu[*ORDER]; for(uint32_t i=0; i< *ORDER; i++) { int32_t sum = 0; for(uint32_t j=0; j< *ORDER; j++) { sum += (Ap[i][j]
* g_ldn_state[j]); } Apx[i] = sum; Bpu[i] = Bp[i]
* input; }
* //Compute Apx[t] + Bpu[t]. for(uint32_t i=0; i< *ORDER; i++) { int32_t state = Apx[i] + Bpu[i]; g_ldn_state[i] = ( //Ceil Integer Division (state > 0)? (1 + (state-1)/ *scale_factor) : (state/ *scale_factor) ); } //Rate Encode the current time-step's ldn_state to the spikes. Note that the // g_gain, g_bias, and g_v_thr are already defined in the the Process. for(uint32_t i=0; i< 2
* *ORDER; i++) { int32_t J = *g_gain
* g_ldn_state[i/2] + *g_bias; // g_bias = 0 here. g_vol[i] = g_vol[i] + J; //Update the global voltage state of IF neurons. if(g_vol[i] > *g_v_thr) { spike_data[i] = 1; g_vol[i] = 0; } else { spike_data[i] = 0; if(g_vol[i] < 0) g_vol[i] = 0; //Rectify the voltage. } } send_vec_dense(rs, &spk_out, spike_data);

[MISSING_PAGE_FAIL:14]

Procedure and distribution plots of LSNN's profiling on _physical_ Loihi-2

Here, we explain our procedure to _energy-_ and _latency_-profile the LSNN on _physical_ Loihi-2 chip, as well as, present the frequency distribution plots of the profiled metrics. With respect to LavaLSNN's (and LDN's) variations to profile, we fixed \(_{}\)=\(_{}\)=0 & \(\)=\(130\), and varied the order \(d\)\(\)\(\{8,16,\,24\}\) - as \(d\) determines the size of LavaLSNN (& LDN). \(\)\(d\), we consider all the combinations of \(_{j}\)\(\)\(\{256,\,512,\,1024,\,2048\}\) & \(_{k}\)\(\)\(\{2,\,3,\,4\}\); and \(\)\((_{j},_{k})\), we _run_ the considered LavaLSNN's components for \(2000\)_duplicates_ of the hard-coded signal (of duration \(140\) time-steps (t.s.)) i.e., \(28e4\) t.s. in total, and record the total energy consumed and execution time _on_ Loihi-2 each _run_ is repeated \(15\) times (in real-time, each _run_ executed for \(30s\)-\(120s\)). To obtain the _per-sample_ energy & latency metrics of processing a \(140\) t.s. long signal on Loihi-2, we divide the total energy & execution time of each _run_) by the number of _duplicates_ i.e., \(2000\). Finally, \(\)\(d\), we report the (mean, std) of _per-sample_ energy & latency metrics over all the \(15\) repeats of all \(12\) combinations \((_{j},_{k})\) in Tab 2.

We next present the frequency distribution plots of the LSNN's different components' energy & latency profiling - on different backends of _physical_ Loihi-2, namely (1) LMT & Neuro-Core (combined), (2) only LMT, and (3) only Neuro-Core. Note that, the LDN & ENC layers, both execute on LMT, where the LDN has \((d^{2})\) operations (matrix multiplied to a vector - lines \(46\)-\(55\) in Algorithm 1), and the ENC has \((d)\) operations (lines \(67\)-\(81\) in Algorithm 1). Therefore, the LDN accounts for (or contributes to) much of the energy & latency metrics values. The \((d^{2})\) also explains why the metric values in Tab 2 (for (1)& (2)) for \(d\) = 8 & \(16\) are similar, and clearly different for \(d\)=\(24\). On a side note, we conducted profiling experiments with \(_{k}=1\) time-step binning too, however, the results were either not consistent or the simulation threw runtime errors for lower buffer size \(_{j}\).

### Energy distribution plots for (1), (2), & (3) in Tab 2

#### i.1.1 For (1) LMT & Neuro-Core: By _LdnEncToSpk_\(\)NetxSNN

#### i.1.2 For (2) only LMT: By _LdnEncToSpk_

Figure 5: Frequency distribution of per-sample energy consumed on LMT & Neuro-Core

Figure 6: Frequency distribution of per-sample energy consumed on only LMT

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]