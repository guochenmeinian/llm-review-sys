# Federated Learning with Client Subsampling, Data Heterogeneity, and Unbounded Smoothness: A New Algorithm and Lower Bounds

Federated Learning with Client Subsampling, Data Heterogeneity, and Unbounded Smoothness: A New Algorithm and Lower Bounds

 Michael Crawshaw

Department of Computer Science

George Mason University

Fairfax, VA 22030, USA

mcrawsha@gmu.edu

&Yajie Bao

School of Mathematical Sciences

Shanghai Jiao Tong University

Shanghai, China

baoyajie2019stat@sjtu.edu.cn

&Mingrui Liu

Department of Computer Science

George Mason University

Fairfax, VA 22030, USA

mingruil@gmu.edu

Equal Contribution.Corresponding Author.

###### Abstract

We study the problem of Federated Learning (FL) under client subsampling and data heterogeneity with an objective function that has potentially unbounded smoothness. This problem is motivated by empirical evidence that the class of relaxed smooth functions, where the Lipschitz constant of the gradient scales linearly with the gradient norm, closely resembles the loss functions of certain neural networks such as recurrent neural networks (RNNs) with possibly exploding gradient. We introduce EPISODE++, the first algorithm to solve this problem. It maintains historical statistics for each client to construct control variates and decide clipping behavior for sampled clients in the current round. We prove that EPISODE++ achieves linear speedup in the number of participating clients, reduced communication rounds, and resilience to data heterogeneity. Our upper bound proof relies on novel techniques of recursively bounding the client updates under unbounded smoothness and client subsampling, together with a refined high probability analysis. In addition, we prove a lower bound showing that the convergence rate of a special case of clipped minibatch SGD (without randomness in the stochastic gradient and with randomness in client subsampling) suffers from an explicit dependence on the maximum gradient norm of the objective in a sublevel set, which may be large. This effectively demonstrates that applying gradient clipping to minibatch SGD in our setting does not eliminate the problem of exploding gradients. Our lower bound is based on new constructions of hard instances tailored to client subsampling and a novel analysis of the trajectory of the algorithm in the presence of clipping. Lastly, we provide an experimental evaluation of EPISODE++ when training RNNs on federated text classification tasks, demonstrating that EPISODE++ outperforms strong baselines in FL. The code is available at https://github.com/MingruiLiu-ML-Lab/episode_plusplus.

## 1 Introduction

Federated Learning (FL) [33; 23] is a distributed learning paradigm in which many clients collaboratively train a machine learning model while communicating over a network, which preserves privacy and leverages parallelism across many clients. Minimizing communication cost, accounting for data heterogeneity across clients, and allowing for partial client participation are core principles of FL. Interest in FL has grown in recent years, especially with user-facing applications such as next word prediction on smartphones .

Optimization is a central part of FL algorithms, and most work on non-convex optimization assumes a smooth objective in both the single machine setting [15; 16; 1] and the FL setting [41; 48; 24; 26]. However, recent work [52; 8] has shown empirical evidence that certain neural networks (LSTMs , and Transformers ) do not satisfy this assumption, but do satisfy a weaker condition known as _relaxed smoothness_. Under relaxed smoothness, techniques such as gradient clipping  are essential for avoiding exploding gradients. To avoid the negative effects of exploding gradients, GD without gradient clipping requires a step size inversely proportional to the maximum gradient norm of the objective in a sublevel set (denoted as \(M\)), resulting in very slow convergence. The usefulness of gradient clipping under relaxed smoothness matches observations of training these neural networks in practice, for example on natural language tasks, which are common in FL .

However, little work yet exists for FL in the relaxed smoothness setting. Liu et al.  introduced a communication-efficient gradient clipping algorithm for FL under relaxed smoothness, with the additional assumption of homogeneous client data and a distributional assumption on the noise of stochastic gradients. The EPISODE algorithm  was subsequently introduced to handle heterogeneous client data in this setting, but requires full client participation, that is, that every client participates in every communication round. This significantly decreases the practical applicability of EPISODE, since full client participation is rarely achievable with large-scale FL in practice .

In this work, we introduce EPISODE++, the first algorithm for FL under relaxed smoothness, client heterogeneity, and client subsampling. EPISODE++ maintains statistics of the history of gradients for each client, and uses these statistics to (1) correct each local update step to approximate an update on the global loss and (2) determine at which steps the clipping operation should be performed. We prove that EPISODE++ achieves linear speedup in the number of participating clients, has reduced communication cost, and enjoys convergence rate independent of client heterogeneity.

A previous line of work in the non-convex smooth stochastic setting [45; 47; 27] compares FL algorithms against a classical baseline: minibatch SGD . Despite the impressive results of newer

    &  & Best Iteration &  Largest \(I\) to guarantee \\ linear speedup \\  } &  \\  Local SGD  & \(O(^{2}}{Nt^{}}++L_{1} (+)}{^{2}})\) & \(O(^{2}}{Nt^{}})\) & \(O(^{2}}{(+L_{1}(+)(1+))N^{2}})\) & (H) \\  SCAFFOLD  & \(O(^{2}}{Nt^{}}+}{ ^{2}})\) & \(O(^{2}}{Nt^{}})\) & \(O(^{2}}{Nt^{}})\) & (H), (S) \\  CELGC  & \(O(^{2}}{Nt^{}})\) & \(O(^{2}}{Nt^{}})\) & \(O(^{2}}{Nt^{}})\) & \(O(^{2}}{(+L_{1}(+)(1+))N^{2}})\) & (Re) \\  EPISODE++ (Theorem 1)\({}^{}\) & \((^{2}}{St^{}}+ +L_{1}(+)}{L_{1}^{2}})}{L_{1}})\) & \((^{2}}{St^{}})\) & \((^{2}}{(+L_{1}(+)(1+))(+}{L_{1}})S})\) & (Re), (H), (S) \\  
 Clipped Minibatch \\ SGD (Theorem 2) \\  & \((M}{c^{2}})\) & \((M}{c^{2}})\) & - & (Re), (H), (S) \\   

Table 1: Best complexity to find an \(\)-stationary point for various methods and settings. The setting column describes the features of the setting in which the algorithm can solve the problem: (Re) denotes relaxed smoothness, (H) heterogeneous data, and (S) client subsampling. \(\): stochastic gradient noise, \(\): client heterogeneity, \(\): objective gap at the initial solution, \(I\): local steps, \(R\): communication rounds, \(T=RI\): iteration complexity, \(N\): number of clients, \(S\): number of subsampled clients. \({}^{}\) denotes a high probability guarantee. \(()\) and \(()\) omit logarithmic terms.

algorithms in practice, only a few works have proven that some FL algorithms have a theoretical advantage over minibatch SGD [24; 27]. It is therefore natural to ask what is the analogue of minibatch SGD in the relaxed smooth setting, and whether it is possible to improve upon this analogue. In this work we consider clipped minibatch SGD, which is obtained by applying gradient clipping to each update of minibatch SGD. We demonstrate a surprising negative result for clipped minibatch SGD: under client subsampling, relaxed smoothness, and client heterogeneity, the convergence rate of clipped minibatch SGD still depends on \(M\). This implies that _gradient clipping does not help minibatch SGD_ in this setting.

Our contributions can be summarized as follows:

* We introduce EPISODE++, the first algorithm for FL under relaxed smoothness, client heterogeneity, and client subsampling. We prove that EPISODE++ achieves linear speedup in the number of participating clients, reduced communication cost, and has convergence rate independent of client heterogeneity. Table 1 shows a detailed comparison of the complexity of various algorithms. To achieve this result, we introduce novel techniques of recursively bounding the client updates in the presence of unbounded smoothness and client data heterogeneity, together with a refined high probability analysis.
* We demonstrate a lower bound for clipped minibatch SGD in which the convergence rate depends on \(M\) (the maximum gradient norm of the objective function in a sublevel set). This shows that, in our setting, clipped minibatch SGD is susceptible to exploding gradients, and avoiding them requires a very small learning rate, which slows down convergence. Our lower bound is based on new constructions of hard instances tailored to client subsampling and a novel analysis of the trajectory of the algorithm in the presence of clipping.
* We empirically evaluate EPISODE++ against strong baselines when training RNNs on federated text classification tasks. The results show that EPISODE++ consistently outperforms baselines across various client participation ratios and is resilient to heterogeneous client data, which is consistent with our theory.

## 2 Related Work

Federated LearningFL was proposed by , where the authors designed the FedAvg algorithm, which is also referred to as local SGD in the literature [41; 31; 48]. The local SGD algorithm has been analyzed in various settings, including convex smooth setting [41; 11; 28; 24; 45; 47; 26; 49; 17; 25], convex composite setting [50; 2; 35], and nonconvex smooth setting [22; 43; 31; 18; 48; 28; 24; 38; 53; 26]. There is a line of work which specially considered client partial participation in FL [6; 13; 24; 29; 44; 7] under convex or nonconvex smooth settings. Recently, Liu et al.  and Crawshaw et al.  considered FL with nonconvex and relaxed smooth functions for homogeneous and heterogeneous data respectively. However, they assume full client participation and neither of them are applicable to the case of client subsampling.

Relaxed SmoothnessRelaxed smoothness was proposed by  as a relaxation of the standard smoothness condition, which is used to model the exploding gradient problem in training deep neural networks such as recurrent neural networks [36; 37] and long-short term memory networks , language models [14; 34] and transformers . Zhang et al.  proved that gradient clipping converges faster than any fixed step size gradient descent for relaxed smooth functions. The complexity bound in  was further improved by . Recently, there is a line of work which considered different algorithms and various analyses under relaxed smoothness [8; 39; 12]. However, all of them focused on single machine setting and may not be applicable to FL setting.

Lower bounds in Federated LearningThere are several lower bound results for FL algorithms. Woodworth et al. [45; 47] compared minibatch SGD and local SGD in the regime of federated stochastic convex optimization setting for homogeneous and heterogeneous data and established lower bounds for local SGD. Woodworth et al.  proved a min-max complexity of distributed stochastic convex optimization for any intermittent communication algorithm. Glasgow et al.  established improved lower bounds of local SGD in convex optimization setting for both homogeneous and heterogeneous data. However, all of these lower bounds are not applicable to our setting where the problem instance is relaxed smooth with heterogeneous data and client subsampling.

Problem Setup

We consider federated learning with heterogeneous and stochastic objectives, where the goal is to minimize the average loss function across \(N\) clients. For \(i[N]\), let \(f_{i}()=_{_{i}}[F_{i}(;)]\) be the objective of the \(i\)-th client, where \(_{i}\) is the underlying data distribution of the \(i\)-th client. Then the global objective is

\[_{^{d}}\{f():=_{i=1}^{N}f_{i} ()\}.\] (1)

Since each \(f_{i}\) is not necessarily convex, we consider the problem of finding an \(\)-stationary point, that is, a point \(^{d}\) such that \(\| f()\|\).

Most works on non-convex optimization [15; 16; 1] consider the case where each \(f_{i}\) is \(L\)-smooth, so that \(\|^{2}f_{i}()\| L\) for every \(^{d}\). However, several works [52; 8] have shown empirical evidence that objective functions corresponding to some types of neural networks (such as LSTMs  and Transformers ) do not satisfy this condition, but do satisfy a strictly weaker condition known as \((L_{0},L_{1})\)-smoothness, or relaxed smoothness. A second-order differentiable function \(g:^{d}\) is called \((L_{0},L_{1})\)-smooth if \(\|^{2}g()\| L_{0}+L_{1}\| g()\|\) for all \(^{d}\). Notice that any \(L\)-smooth function is \((L,0)\)-smooth.

In this work we consider the problem described in (1) under the following assumptions.

**Assumption 1**.: _(i) Denoting by \(_{0}\) the initial iterate, there exists some \(>0\) such that \(f(_{0})-_{^{d}}f()\). (ii) Each \(f_{i}\) and \(f\) is \((L_{0},L_{1})\)-smooth. (iii) There exist \( 0\), \( 1\) such that \(\| f_{i}()\|+\| f()\|\) for all \(^{d}\). (iv) There exists \( 0\) such that \(_{_{i}}[ F_{i}(;)]= f_{i}(x)\) and \(\| F_{i}(;)- f_{i}()\|\) almost surely for \(_{i}\)._

Assumption \((i)\) is standard in non-convex optimization [15; 16]. Assumption \((ii)\) is typically used in the FL literature [32; 9]. Assumption \((iii)\) is used in heterogeneous federated learning  and describes the heterogeneity between client objectives: if \(=0\) and \(=1\), then all client objectives \(f_{i}\) are equal. Assumption \((iv)\) is common in the relaxed smoothness setting [52; 51; 32; 8; 9].

In addition, we consider the case of partial client participation in federated learning, also known as client subsampling. With partial participation, only \(S\) out of \(N\) clients will participate in each communication round, which exacerbates the issue of client heterogeneity.

## 4 Algorithm and Convergence Analysis

### Main Challenges and Algorithm Design

We first illustrate why existing algorithms such as SCAFFOLD  and EPISODE  are not able to handle heterogeneous data, relaxed smoothness and client subsampling simultaneously. The analysis of SCAFFOLD crucially requires the function to be \(L\)-smooth to recursively bound (1) the lag error from client subsampling and (2) client drift from local updates, but this argument is not applicable for relaxed smooth functions whose gradient information changes quickly. EPISODE has convergence guarantees for relaxed smooth functions and heterogeneous data, but only with full client participation. A naive variant of EPISODE in the client subsampling case does not work: the indicator of gradient clipping is based only on information from clients participating in the current round and ignores information from unsampled clients, which introduces non-negligible bias from client heterogeneity.

To address these challenges, we design a new algorithm named EPISODE++, which is presented in Algorithm 1. Similar to EPISODE , our algorithm utilizes _episodic gradient clipping_, which determines whether a clipping operation will be performed depending on the size of the average control variate \(_{r}\). This means that during each round, either (1) all clients perform a normalized update for all steps, or (2) all clients will perform an unnormalized update for all steps. However, different from EPISODE, our algorithm corrects local updates with control variates \(_{r}^{i}\) computed as the averaged stochastic gradient over the previous round in which client \(i\) participated, as in SCAFFOLD. As we will show in our proof, the episodic gradient clipping together with the update correction strategy allows the algorithm to progress in a stable manner: and it will sufficiently decrease the objective value and also avoid the negative effect of possibly exploding gradients.

### Convergence Result

The following result proves that EPISODE++ converges to an \(\)-stationary point with high probability.

**Theorem 1**.: _Let \(}{16BL_{1}}\) and \((0,1)\). Denote \(K=\), \(_{1}:=AL_{0}+BL_{1}+4BL_{1}(2+)\) and \(_{2}:=64(+5(2+)) ^{2}(-)\). If_

\[\{I},I (74+}{BL_{1}})},}{216AL_{0 }^{2}},}\{ },I}\}\},\] (2)

_and \(=(72+}{BL_{1}})\), then Algorithm 1 satisfies \(_{r=0}^{R-1}\| f(}_{r})\| 35\) with probability at least \(1-15\), as long as \(R}\)._

The above result holds for a wide range of \(\) and \(I\), and for any noise level \(\). The corollary below summarizes the best possible iteration complexity and communication complexity implied by Theorem 1. The full proofs of Theorem 1 and Corollary 1 can be found in Appendix A.5 and A.6.

**Corollary 1**.: _Suppose \(>0\). If, under the setting of Theorem 1, we additionally choose \(\{}{16BL_{1}},}{BL_{1}})}{45(K+1)},}{S}},(74+}{BL_{1}})}{_{2} }\}\), \(\) as large as possible under (2), and \(I^{2}}{4_{1}S(7 4+}{BL_{1}})}\), then Algorithm 1 has iteration complexity \(RI=O(^{2}}{S^{4}})\). If additionally \(I=(^{2}}{ S(L_{0}+L_{1 }(+))(+}{L_{1}})})\), then Algorithm 1 has communication complexity \(R=O(+L_{1}(+))(+} {L_{1}})}{^{3}})\)._

### Proof Sketch

In this section we provide a sketch for the proof of Theorem 1. We wish to establish the descent inequality for the global objective function in each round by applying Lemma A.3 in :

\[f(}_{r+1}) f(}_{r})+ f(}_{r}), }_{r+1}-}_{r}++BL_{1}\| f(}_{r})\|}{2}\|}_{r+1}-}_{r}\|^{2},\] (3)

where \(A=1+e^{C}--1}{C}\), \(B=-1}{C}\) and \(C>0\) is an absolute constant. However, using this descent inequality requires \(\|}_{r+1}-}_{r}\| C/L_{1}\). Achieving a universal bound on the distance over two consecutive rounds is nontrivial in the presence of client subsampling, data heterogeneity and relaxed smoothness, which requires a new analysis.

Compared with SCAFFOLD  and EPISODE , the main difficulty of analyzing EPISODE++ lies in controlling the distance between local weights \(_{r,k}^{i}\) and the synchronization point \(}_{r}\) under relaxed smoothness and client subsampling. In EPISODE, the magnitude of \(_{r,k}^{i}\) can be bounded in terms of \(\| f_{i}(_{r,k}^{i})- f_{i}(}_{r})\|\) and \(\|_{r}\|\), since \(_{r}^{i}\) is evaluated at the synchronization point \(}_{r}\). Then the distance can be bounded recursively when clipping does not happen (i.e., \(\|_{r}\|/\)). However, EPISODE++ utilizes historical gradients to construct the indicator \(_{r}\), which means the increment also depends on the lag error \(\|_{r}^{i}- f_{i}(}_{r})\|\). Due to relaxed smoothness, we cannot bound the lag error as in SCAFFOLD  because we do not know the distance between \(}_{r}\) and where \(_{r}^{i}\) is evaluated. In the extreme case that client \(i\) has never been sampled before round \(r\), we have \(_{r}^{i}= F_{i}(}_{0};_{i})\), so the lag error can be unbounded when \(r\) is large.

To address the issue mentioned above, we analyze the convergence of EPISODE++ in a high-probability framework. Let \(_{r,k}^{i}\) denote the local model of client \(i\) at step \(k\) during the most recent round in which client \(i\) participated (before round \(r\)), and let \(q_{r}^{i}\) denote the index of this round. Since client subsampling independent across rounds, \((q_{r}^{i}-r K)=(1-S/N)^{K}\). Therefore, we introduce the event \(:=\{_{0 r R-1,1 i N}r-q_{r}^{i} K\}\), where \(K\) is a logarithmic factor such that \(() 1-\). Under \(\), we obtain the following lemma to bound the distance of local updates.

**Lemma 1**.: _Suppose that (9) holds. Then for any \(r 0\) we have_

\[_{}_{k[I]}\|_{r,k}^{i}- }_{r}\|  2I(2+),i_{r},\] (4) \[_{}_{k[I]}\|_{r,k}^{i}- }_{r}\|  2(K+1)I(2+),i[N].\] (5)

The proof of Lemma 1 is deferred to Appendix A.3, which relies on a _jump start_ analysis. If \(\|_{r}\|>/\), then (4) trivially holds due to the clipping operation. When \(\|_{r}\|/\), a recursive argument shows that the discrepancy \(\|_{r,k}^{i}-}_{r}\|\) depends on the magnitude of the increment at the starting point, that is the lag error \(\|[_{r}^{i}]- f_{i}(}_{r})\|\) and \(\|_{r}\|\) after removing noise. According to the construction of \(_{r}^{i}\), we know \([_{r}^{i}]=_{k=1}^{L} f_{i}(_{r,k}^ {i})=_{k=1}^{I} f_{i}(_{q_{r,k}^{i}}^{i})\). Therefore, under the event \(\), the lag error at the \(r\)-th round can be bounded by the discrepancies of previous \(K+1\) rounds at most, which is the bound (5). In summary, the discrepancy at any round can be controlled recursively if the discrepancy at the initial round is small. This insight motivates the initialization \(_{0}^{i}= F_{i}(}_{0},_{i})\) in Algorithm 1, which enjoys zero initial lag error.

Finally, Lemma 1 shows that the condition of (3) can be satisfied by choosing \(,,I\), which establishes descent of the global objective from \(}_{r}\) to \(}_{r+1}\). Summing from \(r=0,,R-1\) and applying concentration bounds over martingale difference sequences to yield high probability bounds for error terms coming from stochastic gradient noise and client subsampling yields

\[f(}_{R})-f(}_{0})_{r=0}^{R-1}[_{ _{r}}U(}_{r})+_{_{r}}V(}_ {r})]+(12^{2}+_{2}I),\] (6)

where \(_{r}=\{\|_{r}\|/\}\) denotes the clipping indicator, and \(U(x)\), \(V(x)\) are defined in the proof of Theorem 1. According to the choice of \(,\), the dominant term of \(U(}_{r})\) is \(- I\| f(}_{r})\|\) and that of \(V(}_{r})\) is \(- I\| f(}_{r})\|^{2}\). Plugging into (6) and rearranging proves Theorem 1.

Lower Bound for Clipped Minibatch SGD

Clipped minibatch SGD is a natural extension of minibatch SGD [40; 45] to the relaxed smooth setting (see pseudocode in Algorithm 2, Appendix B). Clipped minibatch SGD is nearly identical to minibatch SGD: with the addition of gradient clipping to each round's update. In the similar spirit of , we are interested in this algorithm because it has the same computation and communication structure as EPISODE++ and it is important to understand whether EPISODE++ has any advantage over clipped minibatch SGD. In fact, we will show that clipped minibatch SGD is significantly hindered by the combination of relaxed smoothness, client heterogeneity, and client subsampling.

**Assumption 2**.: _There exists \(M>0\) such that \(\| f()\| M\) for all \(\) with \(f() f(_{0})\)._

A line of work on relaxed smoothness in the single-machine setting [52; 8] has shown that the number of iterations required to find an \(\)-stationary point by gradient descent (GD) under relaxed smoothness is \((M}{^{2}})\), while that of GD with gradient clipping is \(O(}{^{2}})\). In this way, gradient clipping can remove the dependence on \(M\), which can be large, and significantly speed up optimization.

Theorem 2 shows a surprising result: under some conditions on the participation ratio, number of clients, and heterogeneity parameter \(\), clipped minibatch SGD requires \((M}{^{2}})\) rounds, showing that applying gradient clipping to minibatch SGD _does not eliminate dependence on \(M\)._

**Theorem 2**.: _Fix \(>0\), \(0<<1\), \(L_{0}>0\), \(L_{1}>0\), \(>0\), \(>\), \(M>(}{L_{1}},)\), \(N}\). Define \(Q=N\). Let \((L_{0},L_{1},M,,,N)\) denote the set of problem instances \(\{f_{i}\}_{i=1}^{N}\) satisfying Assumptions 1(ii)-(iv) and 2 with \(=0\). For any fixed choice of parameters \(,\) based on the knowledge of above constants, there exists \(\{f_{i}\}_{i=1}^{N}(L_{0},L_{1},M,,,N)\) such that clipped minibatch SGD initialized at \(_{0}\) with \(1 S\) will satisfy \((\| f(_{r})\|<0 r R-1 )>1-\) only if_

\[RM(f(_{0})-f^{*}-}{16L_{0}} )}{2^{2}(1+M}{L_{0}})}.\]

The proof is included in Appendix B. This result shows that clipped minibatch SGD in our setting suffers the same problem as GD in the single-machine setting: divergence can only be avoided with a very small step size, leading to slow convergence. In contrast, the convergence rate of EPISODE++ in the same setting is independent of \(M\).

The proof of Theorem 2 analyzes clipped minibatch SGD for three different problem instances. The first contains linear local objectives with high heterogeneity: if the clipping threshold is sufficiently small (\( M\)), then clipped minibatch SGD will never converge with probability \(\). The second instance contains homogeneous, exponential local objectives: the learning rate must be sufficiently small \(<O(M})\) to avoid divergence due to the exponentially increasing gradient magnitude. However, with a large clipping threshold and small learning rate, the convergence of clipped minibatch SGD will depend on \(M\) for the third problem instance, which has homogeneous linear objectives. Note that our lower bound is different from previous lower bounds in [52; 51] which are in the settings of single machine  or almost sure bounded noise , since our lower bound is considering noise from client subsampling and client data heterogeneity which is not almost surely bounded.

## 6 Experiments

To validate our theory, we evaluate EPISODE++ and baselines in the training of RNNs for two text classification tasks. We compare EPISODE++ to CELGC , clipped minibatch SGD, and NaiveParallelClip , which is a naive parallel implementation of SGD with gradient clipping that requires communication at every iteration. As an ablation study, we also evaluate two algorithms closely related to EPISODE++. The first is a naive extension of EPISODE  for client subsampling, where each participating client's control variate \(_{r}^{i}\) is resampled at the beginning of each round, and \(_{r}=_{i_{r}}_{r}^{i}\). The second is an algorithm which we refer to as SCAFFOLDClip , which applies gradient clipping to each local step of SCAFFOLD . We evaluate these six algorithms on natural language inference with the SNLI dataset  and sentiment classification with the Sentiment140 dataset . More experimental results can be found in Appendix C.

### Setup

All experiments use uniform client sampling, a batch size of 64 (on each client) and the multi-class hinge loss. See Appendix C.1 for full details on hyperparameters. All experiments were implemented in PyTorch and ran on eight NVIDIA V100 GPUs.

SnliSNLI is a 3-way text classification task, in which the logical relationship of a pair of sentences must be classified as either entailment, neutral, or contradictory. The dataset contains 570k pairs of sentences. Because SNLI is a centralized dataset, we follow the heterogeneity protocol in  to divide the dataset into clients according to a similarity parameter \(s\) between \(0\%\) and \(100\%\). According to this protocol, \(s\%\) of each client's local dataset is allocated from a randomly shuffled set of examples, while the remaining \((100-s)\%\) is allocated from a set of examples which is sorted by label. In this way, the similarity of the label distributions of client datasets grows with \(s\). The network consists of a one-layer bidirectional RNN encoder followed by a three-layer fully connected classifier. We train for \(R=5375\) communication rounds with \(I=4\) for all algorithms except NaiveParallelClip, which uses \(R=21500\) and \(I=1\), so that every algorithm runs the same number of training steps.

Sentiment140Sentiment140 is a sentiment prediction task designed for FL. The dataset is comprised of tweets, each labeled as either positive or negative. We follow the data processing steps of  to discard users with small datasets and to split into training and testing sets. In order to control the data heterogeneity between clients, we follow a similar protocol as described for SNLI to form client datasets by combining the datasets of original users in the Sentiment140 dataset. The process is nearly identical to that of SNLI, but here we allocate _users_ to each local dataset instead of _examples_. \(s\%\) of each client's local dataset is allocated from a randomly shuffled set of _users_, while the remaining \((100-s)\%\) is allocated from a set of users sorted by the proportion of positive samples. We train for \(R=2000\) communication rounds with \(I=4\) for all algorithms except NaiveParallelClip, which uses \(R=8000\) and \(I=1\). We use the same network architecture as SNLI.

Figure 1: Final training loss and testing accuracy for all algorithms, as participation ratio and data similarity varies. (a) and (b) show results for SNLI and Sentiment140, respectively.

We study the effect of client subsampling and data heterogeneity by varying each of these values in a controlled way. We first fix the heterogeneity \(s=30\%\) and vary \(S\{2,4,6,8\}\), then we fix the number of participating clients \(S=4\) and vary \(s\{10\%,30\%,50\%\}\). It should be noted that in the experiments with varying \(S\), we always train for a fixed number of iterations \(RI\). This means that separate training runs with different \(S\) will have the same per-client computation cost (number of gradient computations), but the total computation cost of a training run scales with \(S\). For this setting, we use \(N=8\) clients.

To simulate large-scale federated learning, we also include results with a larger number of clients \(N=128\). For this setting, we use \(S=16\) participating clients in each round, and data heterogeneity of \(s=30\%\) (SNLI) and \(s=10\%\) (Sentiment140).

### Results

Figure 1 contains the final training loss and testing accuracy for the variety of settings of client participation and data heterogeneity. For a single setting of participation and heterogeneity, Figure 2 and Table 2 show learning curves and average results over three trials, respectively. More learning curves are given in Appendix C.2. Learning curves for large-scale experiments are shown in Figure 3. EPISODE++ achieves the minimum training loss and maximum testing accuracy of all algorithms in nearly every setting. Only with full participation \(S=8\) does NaiveParallelClip achieve a lower training loss than EPISODE++, but EPISODE++ maintains a higher testing accuracy. Also, NaiveParallelClip requires a much larger communication cost than EPISODE++ to perform the same number of training iterations: when the number of communication rounds is fixed, EPISODE++ significantly outperforms NaiveParallelClip.

Effect of SubsamplingThe first two plots of Figures 1(a) and 1(b) show the performance of each algorithm as the number of participating clients \(S\) varies over \(\{2,4,6,8\}\) with fixed data similarity \(s=30\%\). Clipped minibatch SGD, NaiveParallelClip, and EPISODE all exhibit degraded performance as \(S\) decreases, whereas EPISODE++, SCAFFOLDClip, and CELGC maintain relatively constant performance as \(S\) decreases. Despite the constant performance of CELGC and SCAFFOLDClip under client sampling, both algorithms are significantly outperformed by EPISODE++.

    &  &  \\ Method & Train Loss & Test Accuracy & Train Loss & Test Accuracy \\  EPISODE++ & \(\) & \(\) & \(\) & \(\) \\ CELGC & \(0.529 0.005\) & \(77.45 0.37\) & \(0.493 0.005\) & \(76.36 0.18\) \\ NaiveParallelClip & \(0.377 0.001\) & \(81.74 0.09\) & \(0.365 0.004\) & \(77.73 0.07\) \\ Clipped MinibatchSGD & \(0.642 0.008\) & \(72.74 0.20\) & \(0.549 0.004\) & \(75.10 0.27\) \\ SCAFFOLDClip & \(0.424 0.002\) & \(81.11 0.06\) & \(0.431 0.001\) & \(77.38 0.16\) \\ EPISODE & \(0.455 0.005\) & \(80.24 0.13\) & \(0.466 0.003\) & \(76.89 0.17\) \\   

Table 2: Average results for three trials under \(S=4,s=30\%\) (SNLI) or \(S=4,s=10\%\) (Sent140). The error is the distance from the average to the max/min across three runs.

Figure 2: Learning curves for SNLI and Sentiment140 under the setting \(S=4,s=30\%\) (SNLI) and \(S=4,s=10\%\) (Sentiment140). For NaiveParallelClip, we show the first \(5375\) (SNLI) and \(2000\) (Sentiment140) rounds to compare all algorithms with a fixed number of communication rounds.

Effect of HeterogeneityThe last two plots of each row in Figure 1 show each algorithm's performance as the data similarity \(s\) varies over \(\{50\%,30\%,10\%\}\) for SNLI and \(\{20\%,10\%,0\%\}\) for Sentiment140, with fixed \(S=4\). For both datasets, decreasing data similarity negatively impacts the performance of clipped minibatch SGD, CELGC, and NaiveParallelClip. EPISODE++, SCAFFOLDClip, and EPISODE are able to maintain performance as data similarity decreases, though EPISODE++ maintains a significantly better performance than SCAFFOLDClip and EPISODE.

Large-Scale ExperimentsWith a larger number of clients (\(N=128\), \(S=16\)), the relative performance of each algorithm is similar to the \(N=8\) setting, as shown in Figure 3. Here, the proportion of participating clients \(S/N=1/8\) is smaller than that of the \(N=8\) experiments, where \(S/N 1/4\). This suggests that the effect of partial client participation may be stronger in the large-scale experiments. EPISODE++ still outperforms all other algorithms in the large-scale setting, while maintaining about the same test accuracy as the \(N=8\) setting, demonstrating the effectiveness of EPISODE++ for large-scale federated learning.

Comparison with AblationsBy comparing EPISODE++ against the closely related EPISODE and SCAFFOLDClip, we can see that the use of information from previous rounds and episodic gradient clipping are both critical for the superior performance of EPISODE++. EPISODE only utilizes information from the currently participating clients, ignoring information from clients that participated in previous rounds. As a result, the performance of EPISODE degrades as \(S\) decreases. On the other hand, SCAFFOLDClip determines whether to perform clipping individually for each local step, as opposed to the episodic gradient clipping of EPISODE++ and EPISODE. Although SCAFFOLDClip maintains performance under changes in the participation ratio and data similarity, the level it maintains is significantly lower than that of EPISODE++.

Communication CostNaiveParallelClip suffers a large communication cost for the same number of training iterations compared with other algorithms, due to the cost of synchronizing clients at every iteration. As shown in Figure 2 and Figure 3, EPISODE++ outperforms all other algorithms by a wide margin when the number of communication rounds is fixed. Also, EPISODE requires twice the number of communication operations per training round, which doubles the time required for communication per round compared to all other algorithms.

## 7 Conclusion

We have presented EPISODE++, the first algorithm for FL with heterogeneous data and client subsampling under relaxed smoothness. We proved that EPISODE++ finds an \(\)-stationary point with high probability, and its convergence rate satisfies linear speedup and resilience to heterogeneity while enjoying reduced communication. We also presented a lower bound showing that the convergence rate of a special case of clipped minibatch SGD in our setting suffers a dependence on \(M\) (the maximum gradient norm of the objective in a sublevel set), implying that applying gradient clipping to minibatch SGD does not alleviate the problem of exploding gradients. Our experimental results for RNN training on text classification tasks demonstrate the superior performance of EPISODE++ compared to baselines. One limitation of our current work is that our lower bound assumes \(=0\), and we plan to get a better lower bound in the future for \(>0\).

Figure 3: Learning curves for large-scale training with \(N=128,S=16\), and \(s=30\%\) (SNLI) or \(s=10\%\) (Sentiment140). We compare all algorithms with a fixed number of communication rounds.