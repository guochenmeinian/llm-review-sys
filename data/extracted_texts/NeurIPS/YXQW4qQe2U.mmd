# Optimal Top-Two Method for Best Arm Identification and Fluid Analysis

Agniv Bandyopadhyay

TIFR Mumbai, India

agniv.bandyopadhyay@tifr.res.in

&Sandeep Juneja

Ashoka University, India

sandeep.juneja2010@gmail.com

&Shubhada Agrawal

Georgia Institute of Technology, USA

sagrawal362@gatech.edu

###### Abstract

Top-\(2\) methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability \(\), and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified \(>0\). Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as \( 0\) by computationally demanding plug-in methods. The above top 2 algorithm for any \((0,1)\) has sample complexity within a constant of the lower bound. However, determining the optimal \(\) that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchored at a threshold. If it exceeds the threshold then the algorithm samples the empirical best arm. Otherwise, it samples the challenger arm. We show that the proposed algorithm is optimal as \( 0\). Our analysis relies on identifying a limiting fluid dynamics of allocations that satisfy a series of ordinary differential equations pasted together and that describe the asymptotic path followed by our algorithm. We rely on the implicit function theorem to show existence and uniqueness of these fluid ode's and to show that the proposed algorithm remains close to the ode solution.

## 1 Introduction

Stochastic best arm identification (BAI) problem has attracted a great deal of attention in the multi armed bandit community (see , ,  for some early references in BAI). The basic problem involves a finite number of unknown probability distributions or arms that can be sampled from independently and the aim is to identify the arm with the largest mean. We consider a popular fixed confidence version of the problem where the sampling is sequential and the aim is to minimise sample complexity while guaranteeing that the probability of selecting the wrong arm is restricted to a pre-specified \(>0\). Applications are many including in healthcare and recommendation systems.  developed asymptotically (as \( 0\)) tight lower bound on sample complexity of \(-\)correct algorithms for these BAI problems under the assumption that arms belong to a single parameter exponential family (SPEF). This assumption reduces a probability distribution to a single parameter and allows the analysis to better focus on certain aspects of the problem structure. We retain it for similar reasons. The sample complexity lower bounds involve solving an optimization problem that also identifies optimal proportion of allocations across arms. They also propose a track-and-stopalgorithm that plugs in the empirical estimates of the distribution parameters in the lower bound and tracks the resulting approximations to optimal proportions of arms to sample. Although, this plug-in algorithm was shown to asymptotically match the lower bound, it involves repeatedly solving an optimization problem and is computationally demanding.  consider linear Gaussian bandits,  consider bandits with general distributions. Both the references propose track and stop algorithms where computation is sped up through batch processing.

Substantial literature has come up on 'top-2' based, alternative faster and intuitively appealing algorithms to identify the best arm (see ,  for Bayesian approaches; , ,  for frequentist approaches). The algorithms essentially proceed by identifying at each stage an empirical winner arm, that is, an arm with the largest mean, and its closest challenger. The empirical arm is pulled with probability \(\), and the challenger arm with the complimentary probability. In the frequentist setting, in , the challenger arm is the one with the smallest 'index function'. Heuristically, this index function measures the likelihood of the challenger arm actually being the best one. The smaller the index function, more the likelihood. Further, with high probability, the index function increases with increased allocations to the corresponding arm. As is standard (see, e.g., , ), the algorithm is terminated when the generalized log-likelihood ratio (GLLR, given in Section 3) statistic exceeds a specified threshold. These algorithms are shown to be \(\) optimal in the sense that they match the lower bound on sample complexity satisfied by algorithms that pull the best arm \(\) fraction of times (see  for non-asymptotic analysis when \(=1/2\)). However, determining optimal \(\) has been an open problem that has generated considerable activity and that we address in this paper.

**Contributions - Algorithm:** The key insight from index based top-2 algorithm is that once a sample is given to a challenger arm with the smallest index, its index function increases. The net effect is that as the algorithm progresses, the challenger arm indexes tend to come close to each other and move together. We build upon the above insight. Through the first order conditions associated with the lower bound problem, we identify a function \(g\) that equals zero under optimal allocation when the underlying arm distributions are known. We propose an _anchored_ top-2 type algorithm where when \(g>0\), the empirical winner arm is pulled and that tends to decrease \(g\). When \(g<0\), our algorithm pulls a challenger arm (arm with the smallest index function), and that typically increases \(g\). We observe that the indexes of challenger arms that have been pulled, tend to rise up together until they catch up with arms with higher indexes. Once challenger arms associated with all the indexes have been pulled, call this the time to stability, then, since \(g\) is close to zero and indexes of all challenger arms are close together, it can be seen that the proportionate samples to the empirical winner and the remaining arms are close to the optimal proportions as per the lower bound. This continues until the GLLR statistic exceeds a threshold, roughly of order \((1/)\). The time to stability can be bounded from above by a random time with finite expectation independent of \(\), while the time from stability till the GLLR statistic hits a threshold scales with \((1/)\) with a constant that matches the lower bound.

**Fluid model:** Our other key contribution is to capture the above intuitive description through constructing an idealized _fluid_ dynamics where \(g\) stays equal to zero once it touches zero and where the indexes that have been pulled, remain equal and rise together as the algorithm progresses. We further show that the resulting equations have an invertible Jacobian. Implicit function theorem (IFT) (see [20, Appendix A.6] for an introduction to IFT) then becomes an important tool in analyzing this idealized fluid system as it allows the arm allocations \((N_{a}:a K)\) to be unique functions of the overall allocation \(N\). IFT further allows us to identify the ordinary differential equations satisfied by the derivatives \(N^{}_{a}=}{dN_{a}}\) as the allocations \(N\) increase. The overall path till stability is constructed by pasting together the ode paths followed by arm allocations as the set of indexes that have already been pulled and are increasing together with \(N\), meet another higher index. Once all the indexes have been pulled, our ode stabilizes so that the proportions \(N_{a}/N\) thereafter remain constant and equal the optimal proportions as \(N\) increases. IFT further helps show that the proposed algorithm remains close to the fluid dynamics, and matches the lower bound for small \(\). For completeness, in Appendix E.2, we also identify the ode paths under fluid dynamics for \(\) top-2 algorithms. A great deal of technical analysis goes into showing that the algorithm, observed after sufficiently large amount of samples so that the sample means are close to the true means, is close to the fluid process and they both converge to the same limit.

**Other related literature:** also develop a top-2 type algorithm for a single parameter family of distributions. There algorithm decides between the empirical best and the challenger arm based on directional change in a certain index (related to the LB) when the underlying allocation proportionsare perturbed. It is less directly connected to the first order conditions in the LB problem compared to our algorithm. Empirically, we observe that the our proposed algorithm has lower sample complexity, and is computationally substantially faster (Their algorithm can be sub-optimal. We discuss this in Appendix D.2).  consider an algorithm structurally similar to ours. They focus on the BAI fixed budget (FB) setting where the total number of samples are fixed and the aim is to allocate samples to minimise the probability of incorrect selection. Unlike the fixed confidence (FC) setting (the one that we consider), the FB setting requires optimizing the first argument of relative entropy functions that appear in the lower bound. In FC setting, the second argument is optimized ( vary the first argument). Fundamentally, this is because FB is concerned with sample allocations that control the probability of the data conducting a large deviations to arrive at an incorrect conclusion, while FC is concerned with controlling sample allocations on high probability paths and gathering enough evidence to rule out the likelihood that the observed data is a result of large deviations. Furthermore,  prove weaker a.s. convergence results for associated indexes although not for allocations, and since they focus on FB settings, they do not provide sample complexity bounds or probabilistic false selection guarantees. Our analysis is more nuanced and structurally detailed, and we prove that the sample complexity of the proposed algorithm is asymptotically optimal.  study the best-\(k\)-arm identification problem in the BAI setting with fixed confidence and bring out the structural complexities that arise in lower bound analysis when \(k>1\). For \(k=1\), they develop an asymptotically optimal top-2 algorithm when arm distributions are restricted to be Gaussian.  consider related pure exploration problems using Frank-Wolfe algorithm. Their implementation involves solving a linear program at each iteration. , ,  provide algorithms that provide finite \(\) sample complexity guarantees, however they are order optimal and do not match the constant in the lower bound.

Finally, while fluid analysis is common in many settings including mean field analysis and games (e.g., ), stochastic approximation (e.g., ) and queuing theory (e.g., ), to the best of our knowledge little or no work exists that arrives at it through IFT.

**Roadmap:** In Section 2, we describe the problem and develop lower bound related analysis. The proposed algorithm and our main result, Theorem 3.1, demonstrating algorithm's efficacy are stated in Section 3 where we also develop the relevant IFT framework. Section 4 spells out the fluid dynamics associated with the algorithm. Key steps involved in proving Theorem 3.1 are outlined in Section 5. We describe the numerical experiments in Section 6. Detailed proof of all results are in the appendix.

**Key limitations:** The proposed algorithm extends from SPEF to bounded random variables in a straightforward manner. While we do not provide supporting analysis (this limitation is due to space constraints), our numerical results in Appendix J suggest that our algorithm improves upon existing ones even in this setting. As is standard in the bandit literature, we also assume that samples from arm distributions are independent. Further, another limitation is the assumption of stationarity of the underlying distributions. This may be true when relatively short sampling horizons are involved.

## 2 Problem description and lower bound

**Distributional assumption:** As mentioned earlier, we focus on arm distributions that belong to a known SPEF. Let \(\) denote the open set of possible means of the SPEF under consideration. The details related to SPEF are reviewed in Appendix B.

**Fixed confidence BAI set-up:** Consider an instance with \(K\) unknown probability distributions or _arms_, denoted by the mean vector \(=(_{1},,_{K})\), where each \(_{i}\) (we refer to each \(_{i}\) interchangeably as a distribution as well as its mean in the SPEF context). As is standard in the BAI framework, we assume that there is a unique arm with the largest mean. Thus, without loss of generality \(_{1}>_{i 2}_{i}\). One way to handle the case where \(2\) or more arms are tied for the largest mean is to look for an \(\)-best arm (an arm whose mean is within \(\) of the best arm). However, that is technically a significantly more demanding problem (see ). Assuming uniqueness of the best arm and focusing on the best arm identification allows us to highlight the simple fluid dynamics underlying the proposed algorithm.

**Algorithm:** Given an unknown bandit instance \(\), we consider algorithms that sequentially generate samples - if \(A_{N}\) denotes the arm pulled at sample \(N\), and \(X_{N}\) denotes the associated reward generated independently from distribution \(_{A_{N}}\), then \(A_{N}\) is chosen sequentially and adaptively as a function of generated \((A_{n},X_{n}:n=1,2,,N-1)\). Further, an algorithm stops at some finite random stopping time \(\) and announces the best arm. \(-\)**correct algorithm** is an algorithm that, given a \(>0\)stops at time \(_{}>0\) and outputs a best arm estimate \(k_{_{}}\) such that \((_{}<,\ k_{_{}} 1)\). That is, it identifies the arm with highest mean with probability at least \(1-\). Our interest is in identifying a \(\)-correct algorithm that minimizes \([_{}]\). To this end lower bounds on sample complexity of \(\)-correct algorithms are established using, e.g., the data processing inequality (see, e.g., ). We see that

\[_{x}\ \ \{[N_{1}]d(_{1},x)+[N_{a}]d(_{a},x)\} (1/(2.4))\]

with \(d(,x)\) denoting the Kullback-Leibler divergence between two distributions in \(\) with means \(\) and \(x\), and the expectation is under measure \(_{}\) associated with \(\). The infimum above is solved at \(x^{}=[N_{1}]+_{a}[N_{a}]}{[N_{1}]+[N_{a}]}\). With this, we obtain the lower bound \([_{}] T^{}()\), where \(T^{}()\) is the reciprocal of the optimal value of a max-min problem,

\[(T^{}())^{-1}\ =\ _{=(_{a} :a[K])_{K}}\ _{a 1}\ (_{1}d(_{1},x_{1,a})+_{a}d(_{a},x_{1,a})),\] (1)

where \(x_{1,a}=(_{1}_{1}+_{a}_{a})/(_{1}+_{a})\) and \(_{K}\) denotes a simplex in \(K\) dimension.

The popular **plug-in track and stop** algorithm involves solving the max-min problem (1) repeatedly for optimal weights with empirical distribution plugged in for \(\) above. The algorithm at each stage \(t\), generates the next sample from an arm so that the proportion of arms sampled closely match the resulting optimal weights while ensuring an adequate, sub-linear exploration (e.g., each arm gets at least \(\) samples at each stage \(t\)).

Propositions 2.1 and 2.2 below are crucial for our analysis. Proposition 2.1 helps in constructing the fluid dynamics in Section 4. Proposition 2.2 provides a characterization of the unique optimal allocation \(^{}=(_{a}^{}:a[K])\) which motivates our algorithm's sampling strategy. Before stating the two propositions, we need some notation. Let \(B[K]/\{1\}\), and \(=B\{1\}\). Whenever \(^{c}\), let \(_{^{c}}=(N_{a} 0:a^{c})\) denote an allocation of samples to the arms in \(^{c}\) and we treat this as a constant in the following discussion and also in the statement of the two propositions. We define the quantity \(N_{1,1}\) depending on \(_{^{c}}\) in the following way: **1)** If \(^{c}=\) or \(_{a^{c}}N_{a}=0\), \(N_{1,1}\) is zero. **2)** Otherwise, \(N_{1,1}\) is the value of \(N_{1}\) at which \(_{a^{c}},x_{1,a})}{d(_{a},x_{1,a})}=1\) for the given allocation \(_{^{c}}\). To see existence of such \(N_{1,1}\), observe that whenever \(^{c}\) and \(_{a^{c}}N_{a}>0\), the function \(N_{1}_{a^{c}},x_{1,a})}{d(_{a},x_{1,a})}\) is continuous and it monotonically decreases from \(\) to \(0\) as \(N_{1}\) increases from \(0\) to \(\). Hence, a unique \(N_{1}\) exists where this function equals \(1\). We define the quantity \(N_{}=N_{1,1}+_{a^{c}}N_{a}\).

**Proposition 2.1**.: _For every positive \(N\) satisfying \(N N_{}\), there is a unique set of variables \(_{}(N)=(N_{a}(N):a)\) and \(I_{B}(N)\) satisfying the following conditions_

\[._{a 1},x_{1,a})}{d(_{a},x_{1,a}) }\ =\ 1, x_{1,a}=(N)_{1}+N_{a}(N)_{a }}{N_{1}(N)+N_{a}(N)},_{a[K]}N_{a}(N)=N,\\ \ a B, N_{1}(N) d(_{1},x_{1,a})+N _{a}(N) d(_{a},x_{1,a})\ =\ I_{B}(N).\}\] (2)

_Furthermore, \(_{}()\) and \(I_{B}()\) are continuously differentiable w.r.t. \(N\) for \(N>N_{}\)._

**Proposition 2.2**.: _Upon taking \(B=[K]/\{1\}\) and \(N=1\), \(_{}(1)\), as defined in Proposition 2.1 is same as the unique allocation \(^{}\) solving the max-min problem in (1). Further, \(I_{B}(1)=T^{}()^{-1}\). Moreover, for every \(N>0\), if \(B=[K]/\{1\}\), the unique solution \(_{}(N)=(N_{a}(N):a[K])\) satisfies \(N_{a}(N)=N_{a}^{}\)._

Proposition 2.1 is proved by applying the Implicit function theorem (IFT). Proposition 2.2 is subsumed by [11, Theorem 5], but we prove it using a different set of tools by applying the IFT. See Appendix D for the detailed arguments.

For two vectors \(=(_{a}:a[K])\) and \(=(N_{a}_{ 0}:a[K])\) define the _anchor function_, \(g(,)\ =\ _{a[K]/\{\}}})}{d(_{a},z_{a})}-1,\) where \(=_{a}\ _{a}\), \(_{}=_{a}_{a}\), and \(z_{a}=(N_{j}_{}+N_{a}_{a})/(N_{j}+N_{a})\) for all \(a\).

**Remark 2.1**.: It follows from Proposition 2.2 that the anchor function \(g(,)=0\) and all the indexes \(_{1}d(_{1},x_{1,a})+_{a}d(_{a},x_{1,a})\) equal to each other, uniquely identify the optimal proportion \(^{}\) solving the max-min problem (1) (see Appendix D.1 for an easier and more insightful derivation of these conditions). The algorithm proposed in Section 3 ensures that the empirical version of the anchor function \(g()\) quickly becomes close to zero and thereafter remains close to zero. Further, the indexes sequentially come close to each other and once they are close, they stay close through the remaining steps of the algorithm.

## 3 Anchored Top-2 (AT2) Algorithm

**Notation:** Some notation is needed to help state the proposed algorithm. For every arm \(a[K]\) and iteration \(N\), \(_{a}(N)\) denotes the number of times arm \(a\) has been drawn till iteration \(N\), and \(}(N)=(_{a}(N):a[K])\). Thus, \(N=_{a}_{a}(N)\). Let \(}(N)=(_{a}(N):a[K])\) where \(_{a}(N)\) denotes the sample mean of arm \(a\) at time \(N\), i.e., \(_{a}(N)=_{t=1}^{N}(A_{t}=a) X_{t}/ _{a}(N)\), and \(_{N}=_{a[K]}\ _{a}(N)\), with an arbitrary tie breaking rule.

For every pair of arms \(a,b\), define

\[x_{a,b}(N)\!=\!_{a}(N)_{a}+_{b}(N) _{b}}{_{a}(N)+_{b}(N)}, _{a,b}(N)\!=\!_{a}(N)_{a}(N )+_{b}(N)_{b}(N)}{_{a}(N)+ _{b}(N)}.\]

Let, \(I_{a,b}(N)=_{a}(N) d(_{a},x_{a,b}(N))+_{b}(N ) d(_{b},x_{a,b}(N))\), and \(_{a,b}(N)=_{a}(N) d(_{a}(N),_{a,b}(N))+_{b}(N) d(_{b}(N),_{a,b}(N))\). For \(a_{N}\), we call \(I_{_{N},a}(N)\), and \(_{_{N},a}(N)\), respectively, _actual index_ (or, simply _index_) and _empirical index_ of arm \(a\) at iteration \(N\), and denote them using \(I_{a}(N)\), and \(_{a}(N)\). For notational simplicity, we hide the dependency on \(N\) whenever it doesn't cause confusion. Note that \(_{a}(N)\) is a function of \(_{_{N}}(N),\ _{a}(N),\ _{_{N}}(N)\) and \(_{a}(N)\).

**Stopping Rule:** As is typical in this literature, in our algorithm below, we follow a generalized log likelihood ratio (GLLR) to decide when to stop the algorithm. It is easy to check that \(_{a[K]/\{_{}\}}_{a}(N)\) denotes the GLLR, that is log of likelihood function (LF) evaluated at maximum likelihood estimator (MLE) divided by the LF evaluated at MLE of parameters restricted to alternate set with a different best arm compared to MLE (see [11, Section 3.2] for a detailed derivation). Define stopping time \(_{}=\{N|a[K]/\{_{}\},\ _{a}(N)>(N,)\}\), for an appropriate choice of threshold \((N,)\). After stopping at \(_{}\), the algorithm outputs \(_{_{}}\) as the best arm. [19, Eq. 25, Section 5.1] argued that for instances in SPEF, upon choosing \((N,)((K-1)/)+6((N/2)+1)+8(1 +2((K-1)/))\), the GLLR based stopping rule is \(\)-correct for any sampling strategy including the one we propose. In our numerical experiments, we follow  and choose a smaller threshold, \((N,)=((1+ N)/)\).

**Description of the AT2 and Improved AT2 (IAT2) Algorithm:** The AT2 algorithm takes in confidence parameter \(>0\) and exploration parameter \((0,1)\) as inputs, and executes the following steps at iteration \(N\):

1. Let \(_{N}}}{{=}}\{a[K]\ |\ _{a}(N-1)<N^{}\}\) be the set of under-explored arms.
2. If \(_{N}\), choose \(A_{N}=_{a[K]}\ _{a}(N-1)\), and go to step \(5\).
3. Else, if \(g(}(N-1),}(N-1))>0\), choose the empirically best arm _i.e._\(A_{N}=_{N-1}\), and go to step \(5\).
4. Else, if \(g(}(N-1),}(N-1)) 0\), choose the challenger arm _i.e._\(A_{N}=_{a[K]/\{_{N-1}\}}\ _{a}(N-1)\) using some arbitrary tie breaking rule, and go to step \(5\).
5. Sample \(X_{N}\) from \(A_{N}\) and update \(}(N)\) and \(}(N)\) using \(X_{N}\), \(A_{N}\).
6. If \(_{a[K]/\{_{N}\}}_{a}(N)>(N,)\), terminate and return \(_{N}\).

Inspired from the Improved Transportation Cost Balancing (ITCB) policy for selecting the challenger arm in , Improved AT2 (IAT2) algorithm has the same input and follows the same strategy for exploration (step 1 and 2) and choosing the best arm (step 3) as AT2. IAT2 differs from AT2 only by its choice of the challenger arm in step 4, where IAT2 samples from the arm \(A_{N}=_{a[K]/\{_{N-1}\}}\ (_{a}(N-1)+ _{a}(N-1))\).

Empirically, we see that typically IAT2 performs better than AT2 with respect to sample complexity. In the appendix, we provide pseudo-codes of AT2 and IAT2 in Algorithms 1 and 2, respectively.

### Theoretical guarantees

Proposition 3.1 below shows that the allocations made by AT2 and IAT2 algorithms converge to the optimal allocations \(^{}\) w.p. 1 in \(_{}\). For every \(a[K]\) we define \(_{a}(N)=_{a}(N)/N\), and use \(}(N)=(_{a}(N):a[K])\) to denote the algorithm's proportion at iteration \(N\).

**Proposition 3.1** (**Convergence to optimal proportions**).: _There exists a random time \(T_{stable}\) and a constant \(C_{1}>0\) depending on \(,\), and \(K\), and independent of \(\), such that, \(_{}[T_{stable}]<\), and for every \(N T_{stable}\) and arm \(a[K]\),_

\[|_{a}(N)-_{a}^{}| C_{1}N^{ },|_{a}(N)-_{a}|()N ^{},\]

_where \(()\) is a positive constant depending only on \(\) and defined in Appendix B._

**Theorem 3.1** (_Asymptotic optimality of AT2 and IAT2_).: _Both AT2 and IAT2 are \(\)-correct over instances in \(\), and are asymptotically optimal, i.e., for both the algorithms, the corresponding stopping times satisfy, \(_{ 0}}[_{s}]}{(1/)} T^{}( )\), and \(_{ 0}}{(1/)} T^{}()\) a.s. in \(_{}\). Moreover, we can find a constant \(C>0\) depending on the instance \(\) and \(\), such that,_

\[_{}\ \ \{\ T_{stable},\ T^{}()(1/ )+C((1/))^{1-3/8}\ \}_{}.\]

**Proof idea of Theorem 3.1:** We assume Proposition 3.1 and sketch the argument by which asymptotic optimality follows from it. For \(N T_{stable}\), and \(a[K]\), from Proposition 3.1, \(_{a}(N)_{a}^{}N\) and \(_{a}(N)_{a}\). As a result, from Proposition 2.2, after \(T_{stable}\), \(_{a}(N) NI_{[K]/1}(1)=NT^{}()^{-1}\) for every \(a 1\). Therefore, if \(_{a 1}_{a}(N)\) crosses \((N,)\) at \(N=_{}\), since \((N,)=(1/)+O((1/)+(N))\), we have \(_{}T^{}()^{-1}_{ 0}(1/)+O((1/ )+(_{}))\), which gives \(_{ 0}}{(1/)} T^{}() \ \) in \(_{}\). Detailed proof is in Appendix H. \(\)

We outline the key steps of the proof of Proposition 3.1 for AT2 in Section 5, and the detailed proof is in Appendix G.2. Similar arguments hold for IAT2. Considerable technical effort goes in proving this proposition due to the noise in the empirical estimate \(}(N)\), resulting in noise in the anchor function and the empirical indexes. However, before presenting the proof sketch, in the next section, we first observe the algorithm's dynamics in the limiting fluid regime where this noise is zero. Several of the important proof steps for the algorithmic allocations rely on insights from the simpler fluid model.

## 4 Fluid dynamics

**Motivation:** The fluid dynamics idealizes our algorithm's evolution through making assumptions at each iteration \(N\) that hold for the algorithm in the limit as the number of samples increase to infinity. Unlike the real setting with discrete samples, here we treat samples as a continuous object getting distributed between different arms as the sampling budget (also referred to as 'time') evolves. We denote the no. of samples allocated to an arm \(a[K]\) at some time \(N>0\) using \(N_{a}(N)\), and define the tuple \((N)=(N_{a}(N):a[K])\). Note that \(_{a[K]}N_{a}(N)=N\). The rate \(N_{a}^{}(N)\) at which samples get allocated to arm \(a\) at time \(N\) depends on a continuous version of the AT2 algorithm, which we refer to as the algorithm's fluid dynamics. We define the index of arm \(a 1\) at time \(N\) as, \(I_{a}(N)=N_{1}(N) d(_{1},x_{1,a}(N))+N_{a}(N) d(_{a},x_{1,a}(N ))\), where \(x_{1,a}(N)=(N)_{1}+N_{a}(N)_{a}}{N_{1}(N)+N_{a}(N)}\). Notice that \(I_{a}(N)\) defined in Section 3 is the index of arm \(a\) with respect to the algorithm's allocation \(}(N)\), whereas in our current context, \(I_{a}(N)\) represents the index with respect to the fluid allocations \((N)\).

**Description of the fluid dynamics:** First we explain the fluid dynamics in words. We formally characterize the fluid allocation \((N)\) via a system of ODEs in Theorem 4.1. Later in this section, we exploit the obtained ODEs to argue that, after starting the fluid dynamics from some time \(N^{0}>0\), the allocations \((N)\) reach the optimal proportions \(^{}=(_{a}^{}:a[K])\) by a time \((_{a[K]}_{a}^{})^{-1} N^{0}\). In other words, for \(N(_{a[K]}_{a}^{})^{-1} N^{0}\), we have \(N_{a}(N)=_{a}^{} N\) for every arm \(a[K]\) irrespective of the initial allocation we had at time \(N^{0}\).

For notational simplicity, we hide the dependency on \(N\), whenever it doesn't cause any confusion. Recall the anchor function \(g()\) introduced in Section 2. We use \(g\) to denote \(g(,(N))\). For every subset \(A[K]/\{1\}\), we use \(\) to denote \(A\{1\}\).

We start the fluid dynamics at time \(N^{0}>0\) with some initial allocation \(^{0}=(N_{a}^{0} 0:a[K])\). We assume that the vector of true means \(\) is known. The fluid dynamics evolves according to the following steps at a given total allocation \(N N^{0}\): **1)** If \(g>0\), then \(N_{1}\) increases with \(N\) while other \(N_{a}\)'s for \(a 1\) are held constant till \(g=0\) (\(g\) can be seen to be a monotonically decreasing function of \(N_{1}\)). **2)** If \(g=0\), let \(B\) denote the set of minimum indexes. Thus, \(I_{a}(N)\) are equal for all \(a B\) (the equal value is denoted by \(I_{B}(N)\)) and \(I_{a}(N)>I_{B}(N)\) for all \(a^{c}\). Then, as \(N\) increases, allocations \(N_{1}\) and \((N_{a}:a B)\) increase such that \(g\) remains equal to zero, while the indexes in \(B\) remain equal. In Proposition 2.1, we characterize and prove existence of such allocations, which the fluid dynamics will track. Later we observe that, \(I_{B}\) increases atleast at a linear rate and indexes of arms in \(^{c}\) stay bounded from above by a constant. **3)** If \(g<0\), let \(B\) be the set of minimum index arms and \(I_{B}\) be the index of arms in \(B\). In this situation, \((N_{a}:a B)\) increase with \(N\) keeping index of the arms in \(B\) equal, while \(N_{1}\) and \((N_{a}:a^{c})\) are unchanged. With this \(g\) also increases, since \(g\) is a strictly increasing function of \(N_{a}\) for every \(a B\). The dynamics in this case are simple and described in Proposition E. **4)** Once, \(g=0\), and \(B=\{2,,K\}\), we show that each allocation increases linearly with \(N\) such that \(N_{a}^{}=_{a}^{}\).

**The fluid ODEs:** In Appendix E, we argue that if the fluid dynamics has \(g 0\) at time \(N^{0}\), then \(g\) becomes zero within a finite time by following step 1 or step 3 of the description. This is easy to observe when \(g>0\) at \(N^{0}\), because \(g\) is strictly decreasing in \(N_{1}\), and \(g-1\) as \(N_{1}\). Therefore, following step 1, \(g\) becomes \(0\) at some finite \(N\). We now consider the situation where \(g=0\) at some \(N>N^{0}\). Setting \(B\) to the set of minimum index arms, the algorithm evolves by tracking the allocation \(_{}(N)=(N_{a}(N):a)\) defined through the system (2) in Proposition 2.1. By Proposition 2.1, \(_{}(N)\) is continuously differentiable w.r.t. \(N\). Applying IFT to (2), we obtain the ODEs via which the allocations and the indexes evolve and present them in Theorem 4.1.

**Some definitions:** Let \(f(,a,N)=-(,x)}{ d(_{a},x)})_{x=x_{1,a}} f(,a,)\) is strictly positive because \(,x)}{d(_{a},x)}\) is strictly decreasing with \(x\) for \(x(_{a},_{1})\).

Let \(_{a}=_{1}-_{a}\), and \(h_{a}(,N_{1},N_{a})=f(,a,) ^{2}_{a}}{(N_{1}+N_{a})^{2}}\). For notational simplicity, we denote \(h(,N_{1},N_{a})\) by \(h_{a}\). Further, for each \(a\), we denote \(d(_{1},x_{1,a})\) by \(d_{1,a}\) and \(d(_{a},x_{1,a})\) by \(d_{a,a}\). Recall that for given allocations \((N_{a}:a[K])\), \(B\) denotes a set such that \(N_{1}d_{1,a}+N_{a}d_{a,a}=I_{B}(N)\) for all \(a B\), and \(N_{1}d_{1,a}+N_{a}d_{a,a}>I_{B}(N)\) for all \(a^{c}\). Let \(h(B)=_{a B}h_{a}d_{a,a}^{-1}\), \(h(N)=_{a^{c}}h_{a}N_{a}\), and \(d_{B}=(_{a B}d_{a,a}^{-1})^{-1}\).

**Theorem 4.1** (Fluid ODEs).: _If at total allocation \(N N^{0}\), we have \(g=0\), and \(B\) is the set of minimum index arms, i.e., \(B=_{a[K]/\{1\}}\ I_{a}(N)\), then the following holds true: 1. As \(N\) increases, and till \(I_{B}(N)\) increases to hit an index in \(^{c}\),_

\[N_{1}^{}\!=\!h(B)}{(N_{1}+_{a B}N_{a})h(B)+d_{B}^{-1}h( N)},\ \ \ \ \ \ N_{b}^{}\!=\!h(B)+d_{b,b}^{-1}h(N)}{(N_{1}+_{a B}N_{a})h( B)+d_{B}^{-1}h(N)},\] (3)

_for all \(b B\). It follows that, \(I_{B}^{}(N)=(N)h(B)+h(N)}{(N_{1}+_{a B}N_{a})h(B)+d_{B}^{- 1}h(N)}\)._
2. _Furthermore, for_ \(a^{c}\)_,_ \(I_{a}^{}(N)=N_{1}^{}d_{1a}=h(B)d_{1a}}{(N_{1}+_{a B }N_{a})h(B)+d_{B}^{-1}h(N)}\)_._
3. _There exists a_ \(>0\)_, independent of_ \(N\) _such that_ \(I_{B}^{}(N)>\)_. In addition, for_ \(a^{c}\)_,_ \(N_{a}^{}=0\)_,_ \(I_{a}(N) N_{a}^{0}d(_{a},_{1})\)_, thus the index is bounded from above. Thus, if_ \(^{c}\)_,_ \(I_{B}(N)\) _eventually catches up with another index in_ \(^{c}\)_. In this way, the set_ \(B\) _grows into_ \(\{2,,K\}\)_._

**Indexes once they meet must stay together:** In Appendix F.1 we argue via contradiction that in our fluid dynamics, once a set of smallest indexes that are equal, increase and catch up with another index, their union then remains equal and increases together with \(N\). This argument is important as it motivates the proof in our algorithm that after sufficient amount of samples, once a sub-optimal arm is pulled, its index stays close to indexes of the other arms that have been pulled.

**Bounding the time to reach optimal proportion:** We define \(N^{}\) to be smallest time after \(N^{0}\) at which the fluid dynamics has both \(B=\{2,,K\}\) and \(g=0\). Let \((N^{}_{a}:a[K])\) be the allocation at \(N^{}\). We first argue that: _there exists \(i[K]\) such that \(N^{}_{i}=N^{0}_{i}\)_. We have argued before that if \(g 0\) at \(N^{0}\), then \(g\) becomes zero by some finite time, which we call \(M\). By definition \(M N^{}\). Now if \(B\{2,,K\}\) at \(M\) or \(M=N^{0}\), then after time \(M\) the fluid dynamics evolve by the ODEs in (3) and \(N^{}\) is the time at which \(B\) becomes \(\{2,,K\}\), which is finite by statement 3 of Theorem 4.1. In this case \(i\) is the last element to be added to \(B\). Otherwise if \(B=\{2,,K\}\) at \(M\) and \(M>N^{0}\), the only way this can happen is \(g<0\) in \([N^{0},M)\). In this case, \(i=1\) and \(M=N^{}\). Since \(g=0\) and \(B=\{2,,K\}\) at time \(N^{}\), Proposition 2.2 implies \(N^{}_{a}=^{}_{a}N^{}\) for all \(a\). Combining our observations, we have \(^{}_{i}N^{}=N^{}_{i}=N^{0}_{i} N^{0}\). Hence \(N^{}}{^{}_{i}}(_{a[K]}^{} _{a})^{-1}N^{0}\). Thus \(N^{}\) is within a constant times of \(N^{0}\). We bound \(T_{stable}\) of Proposition 5.1 using a similar argument.

**Remark 4.1** (Incorporating the stopping rule into the fluid dynamics).: At stopping time the idealized GLLR (which is the GLLR defined in Section 3 by replacing the estimated means with the true means) just exceeds \((1/)\). Since the idealized GLLR grows linearly with the allocated samples, the stopping time increases linearly with \((1/)\). Since the time for fluid dynamics to reach stability is independent of \(\), for small \(\), stability will be reached before the algorithm stops.

**Remark 4.2** (\(\)-fluid dynamics).: In Appendix E.2, we construct the fluid dynamics for the \(\)-EB-TCB algorithm  using IFT. We prove that, for every \((0,1)\), the \(\)-fluid dynamics started at some time \(N^{0}>0\) reach the \(\)-optimal proportion (which is the solution to the max-min problem 1 with the added constraint \(_{1}=\)) by a time which is a constant times \(N^{0}\).

## 5 Convergence of algorithmic allocations to the optimal proportions

We now outline the proof steps for Proposition 3.1. To simplify our analysis, we analyze the AT2 algorithm after the random time \(T_{0}\) defined as,

\[T_{0}=N^{} 1 a[K]N N^{},\ |_{a}(N)-_{a}|() N^{-3 /8}},\]

after which the estimates \(}(N)\) are converging to \(\). Recall that \((0,1)\) is the exploration parameter, and \(()>0\) is a constant depending only on \(\). By the definition of \(()\) in Appendix B, we have \(_{a}(N)<_{1}(N)\) for all \(a 1\) and \(N T_{0}\). As a result, arm \(1\) becomes the empirically best arm after \(T_{0}\). In Appendix G.3, we use Chernoff's bound to prove that \(_{}(T_{0}=n+1)=(-(n^{/4}))\), which implies \(_{}[T_{0}]<\). In the following discussion, all the results mentioned are true for both AT2 and IAT2 algorithms.

Proposition 5.1 shows that the allocations made by the proposed algorithm converges to the first order condition satisfied by the optimal proportion \(^{}=(^{}_{a}:a[K])\) at a rate \(O(N^{-3/8})\), where \(\) is the exploration parameter.

**Proposition 5.1**.: _There exists a random time \(T_{stable} T_{0}\) satisfying \(_{}[T_{stable}]<\) and a constant \(C_{2}>0\) depending on \(,\) and \(K\), and independent of the sample paths, such that, for \(N T_{stable}\)_

\[|g(,}(N))|\ =\ _{a 1} ,x_{1,a}(N))}{d(_{a},x_{1,a}(N))}-1\ \ \ C_{2}N^{-3/8},\] (4)

\[_{a,b[K]/\{1\}}|I_{a}(N)-I_{b}(N)|\ \ C_{2}N^{1-3/8}.\] (5)

Before outlining the proof of Proposition 5.1, we explain how Proposition 3.1 follows from Proposition 5.1 just using the IFT.

**Proof idea of Proposition 3.1:** If our algorithm follows optimal proportions at time \(N\), _i.e._, \(_{a}(N)=_{a}^{}N\) for all \(a[K]\), RHS of (4) and (5) becomes zero by Proposition 2.2. Moreover, by Proposition 2.2\(^{}\) uniquely satisfies the conditions: anchor function is zero and all alternative arms have equal index. (4) and (5) imply that, \(}(N)\) satisfies these conditions upto a perturbation of \(C_{2}N^{-3/8}\) after \(T_{stable}\). Using the IFT, we prove that the algorithm's allocation is a Lipschitz continuous function of the perturbation when it is sufficiently small. Hence, by choosing \(T_{stable}\) large enough and using Lipschitzness, we get \(_{a[K]}|_{a}(N)-_{a}^{}|=O(N^{-3/8})\). Closeness of \(}()\) to \(\) follows from the fact that \(T_{stable} T_{0}\). \(\)

**Proof idea of Proposition 5.1:** We separately outline the proofs of (4) and (5) in Proposition 5.1. In the following discussion, constants hidden in \(O(),()\) and \(()\) notations are independent of the sample path after time \(T_{stable}\). To simplify our analysis, we choose \(T_{stable}\) such that exploration stops after \(T_{stable}\), _i.e._, \(_{N}=\) for \(N T_{stable}\) (see the discussion before Definition G.1 in Appendix G.1.1 for justification).

_Key ideas in the proof of (4)_: We prove (4) via induction. We prove the existence of a constant \(D>0\) such that at every \(N T_{stable}\), whenever the actual anchor value \(g(,(N))\) (we denote using \(g(N)\)) satisfies \(|g(N)|>DN^{-3/8}\), our algorithm pushes \(g()\) towards zero by \((1/N)\) in the next iteration through steps 3 and 4. Whereas the interval \([-C_{2}N^{-3/8},C_{2}N^{-3/8}]\) shrinks by \(O(N^{-(1+3/8)})\) from both ends. Since \(N^{-(1+3/8)}<<N^{-1}\), we choose the constant \(C_{2}\) large enough such that \(g()\) stays in the said interval in iteration \(N+1\).

_Key ideas in the proof of (5)_: The following lemma forms a crucial part of the argument for proving closeness of the indexes in the non-fluid setting.

**Lemma 5.1**.: _There exists a random time \(T_{good}[T_{0},T_{stable}]\) such that the algorithm picks all the alternative arms in \([K]/\{1\}\) atleast once between the iterations \(T_{good}\) and \(T_{stable}\). Moreover, for \(N T_{good}\), if the algorithm picks some arm \(a[K]/\{1\}\) at iteration \(N\), then it picks arm a again within a next \(O(N^{1-3/8})\) iterations._

Proof of Lemma 5.1 (in Appendix G.1.2) is technically involved and requires proving several supplementary lemmas. Several of the key steps of this proof borrow insights from the fluid dynamics, and we outline them in Appendix F. Here we assume Lemma 5.1 and sketch the argument by which (5) follows from it for the AT2 algorithm. For any \(a,b 1\) and after any \(N T_{stable}\), \(_{a}()\) and \(_{b}()\) crosses each other before the algorithm picks both \(a,b\) atleast once. We can show that, for \(j=a,b\) and \(N T_{stable}\), \(_{j}(N)\) and \(I_{j}(N)\) differ by \(O(N^{1-3/8})\). As a result, when \(_{a}()\) crosses \(_{b}()\) at \(N+R\), we have \(|I_{a}(N+R)-I_{b}(N+R)|=O((N+R)^{1-3/8})=O(N^{1-3/8})\) since \(R=O(N^{1-3/8})\). For \(j=a,b\), the partial derivatives of \(I_{j}()\) w.r.t. \(_{1}\) and \(_{j}\) are non-negative and bounded from above by \(\{d(_{1},_{j}),d(_{j},_{1})\}=O(1)\). As a result, \(|I_{j}(N+R)-I_{j}(N)|=O(R)=O(N^{1-3/8})\) for \(j=a,b\). Hence, \(|I_{a}(N)-I_{b}(N)||I_{a}(N+R)-I_{b}(N+R)|+_{j=a,b}|I_{j}(N+R)-I_{j}(N)|= O(N^{1-3/8})\). \(\)

**Bounding \(T_{stable}\):** In Appendix G.2, we choose \(T_{good}\) and \(T_{stable}\) such that \(T_{stable}\) is the time after \(T_{good}\) by which the algorithm picks all the sub-optimal arms atleast once. By Proposition 3.1, the algorithm approximately matches \(^{}\) after \(T_{stable}\). Using an argument similar to the one for bounding time to reach the optimal proportion in the fluid dynamics of Section 4, we can prove that \(T_{stable}(_{}^{})^{-1}T_{good}\) a.s. in \(_{}\), where \(_{}^{}=_{a[K]}_{a}^{}\) (Lemma G.4, Appendix G.2.1).

**Role of forced exploration in analysis:** As we observe in the numerical results in Appendix J.4, forced exploration (step 1 of our algorithm) does not increase the observed sample complexity. We emphasize that without the forced exploration, Propositions 5.1 and 3.1 continue to hold if we can show that the proposed algorithm perform sufficient exploration over the instance. That is, after a random time \(T\) depending on the instance and satisfying \([T]<\), every arm has \(_{a}(N)=()\). As a result, upon proving sufficient exploration, asymptotic optimality will follow without the forced exploration step.

In Appendix J.4, we see in the numerical experiments, when there is no forced exploration, AT2's sample complexity blows up over instances where multiple sub-optimal arms have equal mean. Onthe other hand, IAT2 performs optimally over the same instances and its sample complexity remains unaffected even when there is no forced exploration. To understand AT2's sample complexity blow up when multiple sub-optimal arms have the same mean, consider the sample paths where the best arm observes unusually small values in the first few samples. As a result, with positive probability, AT2 confuses one of the multiple sub-optimal arms with equal mean as the best arm and stay stuck sampling between those sub-optimal arms forever. However, IAT2 avoids such situation because of the exploration of every sub-optimal arm induced by the extra logarithmic term in the index. Based on these observations, we make the following conjectures: **1)** AT2 performs sufficient exploration over instances where the means of all the sub-optimal arms are distinct, and **2)** IAT2 performs sufficient exploration over all instances including when some of the sub-optimal arms may have equal means.

## 6 Numerical results

In this section, we numerically demonstrate the dynamics followed by the algorithm AT2, and also compare its performance against the \(\)-EB-TCB algorithm of  for different values of \(\), and TCB algorithm of . We consider \(4\) armed Gaussian bandit with unit variance and mean vector \(=[10,8,7,6.5]\). We simulate one sample path of the AT2 without stopping rule, and plot the value of normalized indexes of the sub-optimal arms. Figure 2 demonstrates that the normalised indexes once close remain close, and hence, AT2 closely mimics the fluid path. In Figure 2, we plot the sample complexities of the (I)AT2, (I)TCB, and \(\)-EB-(I)TCB, for different choices of \(\), and observe that (I)AT2 outperforms all the other algorithms. Note that we use the same forced exploration rule and stopping rule for all algorithms.

In Appendix J, we demonstrate by several examples that both the AT2 and IAT2 algorithms significantly outperform the \(\)-EB-TCB and \(\)-EB-ITCB of  when \(\) is chosen different from the optimal \(\). We also illustrate that the AT2 and IAT2 algorithms have average sample complexity significantly lesser than the TCB and ITCB policies of . In fact, we observe numerically, that (I)TCB doesn't quite satisfy the asymptotic optimality conditions (Figure 4, Appendix J). Next, in Appendix J, we study the effect of choice of the forced exploration parameter \(\) on the sample complexities of AT2 and IAT2. Additionally, we conduct simulations for natural extensions of these algorithms to bandits with distributions supported in \(\) (Appendix J.5).

## 7 Conclusion

We considered the best-arm identification problem under the popular top-2 framework. In the literature, top-2 framework involves sequentially identifying the empirical best arm and the most-likely challenger arm, and sampling the empirical best with probability \(\) and the other with the complimentary probability. However, optimal \(\) was not known.  recently proposed a deterministic rule for deciding between the empirical best and the challenger arm. In this paper, we have provided a most natural first order optimality condition based rule to help decide between the two. We showed that our associated algorithm is asymptotically optimal, and empirically performs better than  both in sample and computational complexity. Our another key contribution was to identify the underlying limiting ordinary differential equation based fluid dynamics that our algorithm tracks. This structure also provides important insights which help prove convergence of the proposed algorithm.

Figure 1: Normalised index on \(1\) sample path. Figure 2: Sample complexity comparison.

**Acknowledgments:**_We thank Arun Suggala and Karthikeyan Shanmugam from Google Research Bangalore for initial discussions on this project. The second and the third author initiated this work while visiting Google Research in Bangalore._