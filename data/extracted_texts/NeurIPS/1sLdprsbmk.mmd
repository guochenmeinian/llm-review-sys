# Can Models Learn Skill Composition from Examples?

Haoyu Zhao1,2 Simran Kaur1,2 Dingli Yu1,2 Anirudh Goyal3 Sanjeev Arora1,2

1 Department of Computer Science, Princeton University

2 Princeton Language and Intelligence (PLI), Princeton University

3 Meta

{haoyu,arora}@cs.princeton.edu

###### Abstract

As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization--the capacity to combine learned skills in novel ways not encountered during training--has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment.

A recent study introduced the skill-mix evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified \(k\)-tuple of language skills. While small models struggled with composing even with \(k=3\), larger models like GPT-4 performed reasonably well with \(k=5\) and \(6\).

In this paper, we employ a setup akin to skill-mix to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills--including rhetorical, literary, reasoning, theory of mind, and common sense--GPT-4 was used to generate text samples that exhibit random subsets of \(k\) skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of \(k\), revealed the following findings:

* Training on combinations of \(k=2\) and \(3\) skills results in noticeable improvements in the ability to compose texts with \(k=4\) and \(5\) skills, despite models never having seen such examples during training.
* When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills.

This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.

## 1 Introduction

Today's large language models (LLMs) exhibit many impressive skills but remain imperfect in key areas. Arguably, one significant limitation stems from their difficulty in combining or composing the skills they have already learned. For example, solving a math problem on a specific topic may require a certain set of skills, while solving a more challenging question may require applying broader combinations of _more_ skills as compared to a simple question. Understanding how well models can learn to compose skills with limited training examples is therefore a crucial area of investigation.

Let us note why this is a nontrivial challenge. If a model has learned \(N\) base skills, and we want it to be able to compose any subset of \(k\) skills, there are \(\) possible combinations of interest. Since \(\) scales roughly with the \(k\)-th power of \(N\), even reasonable-sized training datasets will likely omitexamples of many skill combinations. As a result, the model must be capable of _generalizing_ to unseen combinations of skills.

This issue was highlighted in , which introduced a simple mathematical framework to demonstrate that current LLM scaling laws  suggest scaling up models can enhance their ability to combine \(k\) skills--though the ability increases only gradually with model size. This prediction was validated through the skill-mix evaluation , which directly tested models' capability to combine \(k\) language skills that were listed in the model's prompt (see Section 2 for details). The evaluation revealed that top-tier models like GPT-4 could successfully combine \(5\) or \(6\) language skills when composing a short piece of text, whereas smaller models like LLaMA-2-70B-Chat struggled to combine even \(3\) skills.

This finding of skill-mix evaluation raises an interesting question: even if pre-training does not fully endow models with the ability to combine skills, _can the capability be induced through fine-tuning on suitable data?_ The current paper investigates this question within the context of the language skills explored in the skill-mix evaluation .

### Our contributions

We address the question posed above by fine-tuning smaller models, including LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2, using a small yet high-quality dataset generated by GPT-4. This dataset consists of 13,957 text pieces, each composed of randomly selected \(k\) skills with \(k=1,2,3\). We evaluate the capability of the fine-tuned models to combine an another set of held-out skills with potentially higher \(k\). In particular, we divide the original skill-mix skill set into a training set and a held-out set, based on skill categories, to minimize correlations between the two groups. This ensures a clearer evaluation of the models' ability to generalize to unseen skills. Figure 1 and Section 3 detail the full pipeline of our data generation and evaluation process. Our experimental results demonstrate the following findings (Section 4).

**Finding 1**: _Fine-tuning on texts that compose training skills improves capabilities of composing held-out skills._ Figure 2 shows the success rate of various models of combining \(k\) held-out skills. Before fine-tuning, LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 perform significantly worse than GPT-4, especially when \(k>2\). Both models improve remarkably after fine-tuning on our small dataset. For example, with \(k=3\), the success rate of LLaMA-2-13B-Chat increases from \(4\%\) to \(37\%\), and the success rate of Mistral-7B-Instruct-v0.2 increases from \(8\%\) to \(49\%\). Note in the original skill-mix, no model except GPT-4 could reach \(15\%\) success rate for \(k=3\) (see Table 3, ).

Figure 1: Pipeline for evaluating the generalization capability to combine skills. We split the language skill set \(\) from  into training skills \(_{}\) and held-out skills \(_{}\), and the topic set \(\) into training topics \(_{}\) and held-out topics \(_{}\). The pipeline consists of three steps: (1) generate data by prompting GPT-4. The training texts contain only training skills \(_{}\) and training topics \(_{}\), and each text exhibits at most 3 skills; (2) fine-tune LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 using the generated data; (3) evaluate the fine-tuned models on held-out skills \(_{}\) and held-out topics \(_{}\) with the number of requested skills being as large as 5. See our detailed setups in Section 3.

This phenomenon of compositional generalization from training skills to held-out skills suggests that the models are not merely learning to compose each individual combination of skills. Instead, they are acquiring a higher-order _meta-skill_ that allows them to generalize and apply to combine unseen skills.

**Finding 2**: _Fine-tuning on texts that compose a smaller number of skills leads to improvement of composing a larger number of skills._ Figure 2 demonstrates that fine-tuning on our small dataset, which includes texts composed of \(k=1,2\) or \(3\) training skills, leads to enhanced capability on composing \(k=4\) and \(5\) held-out skills, even though the models have never trained on such text. In Section 4, we present similar findings: (1) the ability to compose \(k\) training skills is also improved for \(k=4\) and \(5\) after fine-tuning; and (2) if models are fine-tuned exclusively with texts composed of no more than \(2\) training skills, they also show improved composition ability for \(3\) and \(4\) skills.

Note Finding 1 and Finding 2 are beyond the scope of the theory presented in Arora and Goyal , which studies the composition ability for skills that appear in the training data.

**Finding 3**: _Fine-tuning on texts that compose more skills (i.e., with a larger \(k\)) is more data-efficient for learning skill compositions._ We design control experiments in Section 4.3 that fine-tune LLaMA-2-13B-Chat on two datasets: (a) one dataset contains around 10,000 text pieces with only 1 or 2 skills; and (b) another dataset contains 8,000 text pieces, consisting of a random subset of the first dataset and around 2,000 text pieces that compose 3 skills. Table 4 shows that LLaMA-2 fine-tuned on the dataset with richer skill composition performs significantly better than the other for all \(k=2,3,4,5\).

We discuss our main findings thoroughly in Section 4. In Section 5.1, we solidify our findings using Claude 3 Opus (instead of GPT-4) as the Grader in evaluation. This eliminates the possibility that the ability to compose skills comes from GPT-4's bias towards the models fine-tuned on GPT-4's outputs.

We also discuss the implications of our findings for going beyond "stochastic parrots" behavior , which refers to the perception that LLMs might not generate novel pieces of text but rather mimic data from the pretraining corpus (Section 5.2).

## 2 Related Works

**Compositional generalization** Compositional generalization has grabbed lots of attention in AI. [29; 27] studied compositional generalization in the realm of mathematical reasoning, and [3; 22] investigated for logical inference. In computer vision, compositional generalization was studied on disentangled representation learning to generate images from novel combinations of concepts [12; 10; 31]. Besides, several works have explored composing visual relations , as well as benchmarks for text-to-visual generation[14; 19]. Other works have explored using compositional models for image generation , as well as to create plans for unseen tasks at inference time .

**Compositional generalization for language and LLMs** There is also a long history of study of compositional generalization in language [11; 18; 4; 15; 17; 21]. However, the test bed for compositional generalization mostly relies on rule-based languages, like SQL or synthetic-generated ones, and thus deviates a little bit from natural language. Recent works have observed compositional

Figure 2: The success rate of different models to compose \(k\) held-out skills in a short paragraph. (See the detailed definition of “Ratio of Full Marks” in Section 3.3.) The strongest model like GPT-4 can compose 5 skills in a short paragraph reasonably well, while smaller models struggle to compose even 3 skills. After fine-tuning, the models’ ability to compose skills improves significantly.

capabilities in LLMs emerge multiplicatively on natural languages [30; 1; 23; 33]. These observations have fueled a growing interest in exploring and evaluating compositional generalization in LLMs as a means to more appropriately evaluate LLM capabilities [9; 32; 24; 26; 33]. Some examples include imposing constraints and/or requirements on text generation [9; 32; 5], as well as providing multi-hop questions whose answers require composing multiple facts that were individually observed during pretraining . Dziri et al.  tests whether the LLMs can learn compositional generalization by studying integer multiplication and a specific form of dynamic programming. Skill-Mix  presents an approach to evaluating compositional generalization on language skills, which we discuss in more detail in the next paragraph.

Skill-MixYu et al.  introduce a new evaluation named skill-mix that tests for models to produce novel pieces of text from random combinations of \(k\) skills, and the evaluation can be made more difficult by increasing the value of \(k\). The procedure is roughly as follows: (1) from a set of \(N\) language skills and \(T\) topics, pick a random subset of \(k\) skills and one topic; then, (2) query the Student model to produce a short piece of text (at most \(k-1\) sentences) that illustrates the \(k\) skills in the context of the provided topic. Note that for \(k=1\), the maximum sentence limit is 1 sentence. A Grader model is used to evaluate the text piece based on the following criteria: correctly illustrating all \(k\) skills and the topic, meeting the maximum length requirement, and general coherence. Thus, each piece of text can award up to a maximum of \(k+3\) points (see the original paper for various metrics extracted from points earned). Note that each of the \(N\) language skills has a Wikipedia entry, so it is reasonable to expect an LLM to encounter the skills multiple times in isolation in the pretraining corpus, but not in all possible combinations. In this paper, we choose to study the compositional generalization of LLMs in the context of skill-mix because skill-mix is close to general language capability and is more flexible for modifying the language skill set.

## 3 Pipeline

Our pipeline consists of three stages: generating data by selecting GPT-4 responses on skill-mix (Section 3.1), fine-tuning on the generated data (Section 3.2), and evaluating our fine-tuned model on skill-mix evaluation  (Section 3.3). The pipeline overview is shown in Figure 1.

### Data generation

We adapt the procedure presented in skill-mix evaluation  to produce finetuning data. Only the generations with full marks (i.e., illustrating all skills and topics, meeting the length requirement, and general coherence) are selected. To enhance the likelihood of obtaining full marks, we prompt GPT-4, the best Student model reported in Yu et al. , to create the generations.

Skills and topics for data generation.Since our goal is to measure the generalization capability strictly, we minimize the overlap between the skills/topics used during data generation and the skills/topics used to evaluate the fine-tuned models. Specifically, we partition the original set of 101 skills introduced in Yu et al. , \(\), into a set of 53 train skills, \(_{}\), and 48 held-out skills, \(_{}\), based on the skill category. \(_{}\) includes only literary and rhetorical categories, while \(_{}\) comprises the rest of the categories, including reasoning, logic, theory of mind, pragmatics, common sense, and physical knowledge. Similarly, we partition the original set of topics, \(\), into random sets of 50 training topics, \(_{}\), and 50 held-out topics, \(_{}\). It is important to note that partitioning skills randomly can lead to correlations between the train and held-out skills, as skills from the same category can be highly related. However, partitioning topics randomly does not present this issue, as the topics are generally unrelated. (Please refer to Appendix A for the detailed list of skills and topics.)

Data generation with \(k=1,2,3\).We produce fine-tuning data with \(k=1,2\) and \(3\) using GPT-4 as both the Student and Grader model. For \(k=1\), we use the original set of skills \(\) and training topics \(_{}\) to produce approximately 5,000 generations, and we only keep generations that receive full marks. We refer to the resulting dataset as \(_{}(1)\). \(_{}(1)\) contains only texts with individual skills, thus serving the role of separating the improvement from better utilizing an individual skill and the improvement from better composing multiple skills in later experiments.

We follow an analogous procedure for \(k=2\) and \(k=3\), but using our 53 training skills \(_{}\) and 50 training topics \(_{}\). We produce 10,000 generations for each \(k\) before filtering. We refer to the resulting datasets as \(_{}(2)\) and \(_{}(3)\), respectively. For convenience, we use \(_{}(1,2)\) to denote the dataset that combines \(_{}(1)\) and \(_{}(2)\), i.e., \(_{}(1,2)=_{}(1) _{}(2)\). Similarly, we use \(_{}(1,2,3)\) to denote the dataset that combines \(_{}(1)\), \(_{}(2)\), and \(_{}(3)\) together. We summarize our notations in Table 1.

### Fine-tuning

We fine-tune LLaMA-2-13B-Chat  and Mistral-7B-Instruct-v0.2  on the data generated in Section 3.1 for 4000 steps with a batch size of 64. Each data generated from skill-mix consists of 4 parts: prompt1, answer1, prompt2, answer2. Here, prompt1 denotes the prompt asking the student to generate answers, answer1 stands for student's first round answer, prompt2 is the prompt that asks the student to correct or refine its answer, and answer2 is the student's second round answer. During fine-tuning, we feed the concatenation of prompt1, answer1, prompt2, answer2 into the model as a single text, but only compute the cross-entropy loss for tokens belonging to answer1 and answer2. We use Adam as the optimizer and linear warmup for the first 64 steps, followed by a constant learning rate of 2e-5 for the remaining training steps. 1 The maximum token length is set as 1024. All fine-tuning experiments are conducted on 4 Nvidia H100/A100 GPUs. Similarly to the loss design of RLHF , we mix pre-training data2 during fine-tuning to prevent degradation of general abilities.

### Evaluation

We evaluate the skill-mix(\(k\)) performance (\(k=2,3,4,5\)) for all the models fine-tuned on data generated in Section 3.1, i.e., \(_{}(1)\), \(_{}(2)\), and \(_{}(3)\).

**Settings** As mentioned earlier, skill-mix evaluation requires a skill set and a topic set. We consider the following 3 settings (where Setting II is our main setting used in Figure 1):

1. skill-mix evaluation on _training_ skills and topics. Since the model observes the same skills and topics during fine-tuning, this setting serves as an in-domain evaluation for \(k=2,3\). For \(k=4,5\), it tests the models' ability to combine more skills, which is already out-of-domain, since the model has never seen such data during fine-tuning. We use the notation skill-mixtrain(\(k\)) to denote the skill-mix(\(k\)) evaluation on training skills and topics.

 
**Symbol** & **Definition** & **Size** & **Misc** \\  \(\) & All Skills & 101 & \(=_{}_{}\) \\  & & & categories = \{literary, rhetorical, reasoning, \\  & & & logic,theory\_of\_mind, pragmatics, \\  & & & common\_sense, physical\_knowledge\} \\ \(_{}\) & Train Skills & 53 & categories = \{literary, rhetorical\} \\ \(_{}\) & Held Out Skills & 48 & categories = \{reasoning, logic, theory\_of\_mind, \\  & & & pragmatics, common\_sense, physical\_knowledge\} \\  \(\) & All Topics & 100 & \(=_{}_{}\) \\ \(_{}\) & Train Topics & 50 & \(_{}\) \\ \(_{}\) & Held Out Topics & 50 & \(_{}\) \\  \(_{}(1)\) & data with full mark on skill-mix (\(k=1\)) data with full mark on skill-mix (\(k=1\)) data with full mark on skill-mix (\(k=2\)) data with full mark on skill-mix (\(k=3\)) & 6277 & Created from \(_{}\) and \(_{}\) \\ \(_{}(3)\) & & & \\  

Table 1: Notation used in data generation (Section 3.1)2. skill-mix on _held-out_ skills and topics. This setting tests the models' ability to combine skills that are never present in fine-tuning.3 This setting serves as another perspective to show the stronger out-of-domain generalization for composing skills compared to Setting I. We use the notation skill-mix\({}_{}(k)\) to denote the skill-mix(\(k\)) evaluation on held-out skills and topics. 3. skill-mix on _all_ skills and topics. Evaluating skill-mix on only half of the skills split by category might make the evaluation easier, since combining 2 rhetorical or logical skills might be easier than combining 1 rhetorical and 1 logical skill. Thus, we also evaluate skill-mix on all skills and topics available, which serves as a direct comparison with the results in Yu et al. . We use the notation skill-mix\({}_{}(k)\) to denote the skill-mix(\(k\)) evaluation on all skills and topics.

Evaluation MetricsWe follow the evaluation rubric of skill-mix. Each generated text can receive up to \(k+3\) points: 1 point for each correctly illustrated skill, 1 point for sticking to the topic, 1 point for text coherence / making sense, and 1 point for meeting the length requirement.

Following Yu et al. , we grade each generated piece of text three times. For each of the \(k+3\) criteria, we collect the majority vote among the three grading rounds, and map the points earned to the following two metrics of interest4: (_Ratio of Full Marks_) count as 1 if all \(k+3\) points are earned, and 0 otherwise; and (_Skills Fraction_) the fraction of points awarded for the \(k\) skills if all 3 points are awarded for the remaining criteria, and 0 otherwise. For a given (\(k\) skill, 1 topic) combination, we take the maximum value of the metric among the 3 generations. We average the maximum value across all the combinations. Note that we use one of the harder variants of skill-mix, where we do not award any points for a particular skill if the skill name is explicitly mentioned in the generated text piece.

## 4 Skill Composition Can Be Learned From Examples

We present experiment results using the pipeline (Section 3) to evaluate compositional generalization. Table 2 and Table 3 summarizes the skill-mix (\(k\)) performances of LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 fine-tuned on various datasets under three evaluation settings. We discuss our findings on compositional generalization for in-domain evaluations (Section 4.1), compositional generalization for out-of-domain evaluations (Section 4.2), and the data efficiency to induce compositional generalization (Section 4.3).

   &  \\  & \(k=1\) & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\  _{}(k)\))} \\  LLaMA-2-13B-Chat & \(.527.52\) & \(.17\)/\(.47\) & \(.02\)/\(.34\) & \(.00\)/\(.33\) & \(.00\)/\(.31\) \\ ft’ed on \(_{}(1)\) & \(.87\)/\(.87\) & \(.15\)/\(.51\) & \(.00\)/\(.43\) & \(.00\)/\(.37\) & \(.00\)/\(.35\) \\ ft’ed on \(_{}(1,2)\) & \(.88\)/\(.88\) & \(.50\)/\(.70\) & \(.12\)/\(.56\) & \(.01\)/\(.55\) & \(.02\)/\(.52\) \\ ft’ed on \(_{}(1,2)\) & \(.89\)/\(.89\) & \(.51\)/\(.73\) & \(.24\)/\(.68\) & \(.08\)/\(.64\) & \(.03\)/\(.60\) \\  _{}(k)\))} \\  LLaMA-2-13B-Chat & \(.46\)/\(.46\) & \(.28\)/\(.50\) & \(.04\)/\(.42\) & \(.01\)/\(.39\) & \(.00\)/\(.43\) \\ ft’ed on \(_{}(1)\) & \(.87\)/\(.87\) & \(.43\)/\(.70\) & \(.05\)/\(.54\) & \(.01\)/\(.49\) & \(.00\)/\(.44\) \\ ft’ed on \(_{}(1,2)\) & \(.95\)/\(.95\) & \(.75\)/\(.87\) & \(.25\)/\(.68\) & \(.05\)/\(.60\) & \(.02\)/\(.56\) \\ ft’ed on \(_{}(1,2,3)\) & \(.96\)/\(.96\) & \(.78\)/\(.88\) & \(.37\)/\(.75\) & \(.09\)/\(.69\) & \(.02\)/\(.60\) \\  _{}(k)\))} \\  LLaMA-2-13B-Chat & \(.46\)/\(.46\) & \(.24\)/\(.50\) & \(.02\)/\(.42\) & \(.01\)/\(.40\) & \(.00\)/\(.34\) \\ ft’ed on \(_{}(1)\) & \(.88\)/\(.88\) & \(.27\)/\(.62\) & \(.05\)/\(.50\) & \(.00\)/\(.40\) & \(.00\)/\(.33\) \\ ft’ed on \(_{}(1,2)\) & \(.96\)/\(.96\) & \(.51\)/\(.74\) & \(.17\)/\(.65\) & \(.01\)/\(.54\) & \(.00\)/\(.51\) \\ ft’ed on \(_{}(1,2,3)\) & \(.96\)/\(.96\) & \(.65\)/\(.81\) & \(.33\)/\(.73\) & \(.15\)/\(.69\) & \(.06\)/\(.62\) \\  

Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on skill-mix (\(k\)) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different \(k=2,3,4,5\). \(_{}(k)\) denote the data generated with full skill-mix (\(k\)) score. (see Section 3.1)

### Compositional generalization for in-domain evaluations

We first observe that, after fine-tuning LLaMA-2-13B-Chat on \(_{}(1,2)\), the skill-mix\({}_{}(2)\) performance significantly improves. Similarly, after fine-tuning LLaMA-2-13B-Chat on \(_{}(1,2,3)\), the skill-mix\({}_{}(3)\) performance also improves. For example, the Ratio of Full Marks for skill-mix\({}_{}(3)\) improves from \(2\%\) for LLaMA-2-13B-Chat to \(24\%\) after fine-tuned on \(_{}(1,2,3)\) (Table 2).

One confounding factor in the above evaluation is that the original LLaMA-2-13B-Chat may not utilize all the individual skills perfectly, and the skill-mix performance improvement might just be attributed to the model's knowledge of the individual skills after fine-tuning, not the model's ability to better compose different skills together. Thus, we also evaluate the skill-mix performance on LLaMA-2-13B-Chat fine-tuned only on \(_{}(1)\), which consists of purely skill-mix\(k=1\) data and serves as another baseline besides the original LLaMA-2-13B-Chat. After fine-tuning on \(_{}(1)\), the model indeed knows the individual skills much better, since the Ratio of Full Marks of skill-mix\(k=1\) improves from \(52\%\) to \(87\%\). However, better knowledge of individual skills does not lead to a better ability to compose skills together, since the skill-mix\({}_{}(2)\) or skill-mix\({}_{}(3)\) performance of LLaMA-2-13B-Chat fine-tuned on \(_{}(1)\) keeps nearly the same as the pre-trained ones, under both Ratio of Full Marks and Skills Fraction metrics. Thus, mainly all the improvement on skill-mix\({}_{}(2)\) or skill-mix\({}_{}(3)\) indeed comes from the ability to compose different skills together.

Fine-tuning on Mistral-7B-Instruct-v0.2 (Table 3) shows similar results as fine-tuning on LLaMA-2-13B-Chat (Table 3), except that only fine-tuning on \(_{}(1)\) shows more improvement on skill-mix\({}_{}(2)\) and skill-mix\({}_{}(3)\) compared to the original Mistral-7B-Instruct-v0.2 model. A possible explanation is that Mistral-7B-Instruct-v0.2 is better at composing skills than LLaMA-2-13B-Chat, and fine-tuning on \(_{}(1)\) helps Mistral-7B-Instruct-v0.2 exhibit each skill more properly and clearly when composing skills.

### Compositional generalization for out-of-domain evaluations

This section discusses the observations that indicate the out-of-domain generalization of skill composition, including generalization to unseen \(k\) and generalization to unseen skills.

skill-mix\({}_{}(k)\) improves for unseen \(k\).We first observe that, after fine-tuning LLaMA-2-13B-Chat on skill-mix\({}_{}(1,2,3)\), the skill-mix\({}_{}(4)\) and skill-mix\({}_{}(5)\) performance also increase. For example, the Ratio of Full Marks improves from \(0\%\) to \(8\%\) when \(k=4\) (Table 2). Note that \(8\%\) Ratio of Full Marks improvement on \(k=4\) is significant, since besides GPT-4, all other models tested in Yu et al. , including GPT-3.5-turbo, cannot get over \(2\%\) Ratio of Full Marks on \(k=4\) (Table 3 in ). Besides, training only on \(_{}(1)\) does not improve the skill-mix\({}_{}(4)\) or skill-mix\({}_{}(5)\).

   &  \\  & \(k=1\) & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\  _{}(k)\))} \\   & \(86\)/\(86\) & \(18\)/\(51\) & \(05\)/\(46\) & \(01\)/\(36\) & \(00\)/\(35\) \\ _{}(1)\)} \\ _{}(1)\)} \\ _{}(1)\)} \\ _{}(1)\)} \\ _{}(1,2,3)\)} \\ _{}(k)\))} \\  & \(85\)/\(85\) & \(48\)/\(73\) & \(08\)/\(56\) & \(01\)/\(42\) & \(01\)/\(39\) \\ _{}(1)\)} \\ _{}(1)\)} \\ _{}(1,2,3)\)} \\ _{}(1,2,3)\)} \\ _{}(1,2,3)\)} \\ _{}(1,2,3)\)} \\ _{}(1,2,3)\)} \\ _{}(1)\)} \\ _{}(1,2,3)\)} \\ _{}(1,2,3)\)} \\ _{}(1)\)} \\ _{}(1,2,3)\)} \\ _{}(1,2,3)\)} \\ _{}(1)\)} \\ _{}(1,2,3)\)} \\ _{}(1,2,3)\)} \\ _{}(1)\)} \\ _{}(The surprising finding here is that the model is only trained on skill-mix\(k=2,3\) data, but it improves the ability to compose \(k=4,5\) skills in a short piece of text, which it is never trained on. The results suggest that its ability to compose multiple skills does not come from overfitting training data but should be perceived as learning a _meta-skill_ instead. This observation is beyond the scope of the theory presented in Arora and Goyal , which assumes that the number of skills a trained model can compose is limited to the number of skills in its training text pieces.

Fine-tuning on Mistral-7B-Instruct-v0.2 (Table 3) shows similar results, where the fine-tuned model is able to improve skill-mix\((4)\) and skill-mix\((5)\) after fine-tuning on \(_{}(1,2,3)\), while training only on \(_{}(1)\) has limited improvement.

**Improvement on skill-mix\((k)\) and skill-mix\((k)\).** Besides the skill-mix performance improvement on training skills and topics, we also observe the improvement of skill-mix\((k)\) (Setting II) from Table 2 and Figure 2. Similar to the evaluation on training skills and topics, fine-tuning LLaMA-2-13B-Chat on \(_{}(1)\) only improves the skill-mix\((k)\) performance for \(k=3,4,5\) marginally, but it indeed improves the skill-mix\(k=2\). However, the improvement is incomparable with fine-tuning on \(_{}(1,2,3)\). This shows that the ability to compose multiple skills generalizes to held-out skills, even though our training never exposed the model to data with the held-out skills. Besides the skill-mix improvement on held-out skills, we also observe the improvement of skill-mix\((k)\). This result again suggests that models learn _meta-skill_ rather than overfitting to skill combinations in the training data.

Note that the skill-mix\({}_{}(k)\) performance is better than the skill-mix\((k)\) in Table 2, which is counter-intuitive. We hypothesize that this phenomenon happens because the pre-trained model knows how to compose held-out skills (logic, reasoning, theory of mind) better than training skills (rhetorical and literary). Or possibly the training skills are harder to compose. Exploring difficulty of individual skills is left for future work.

Also, fine-tuning on Mistral-7B-Instruct-v0.2 (Table 3) shows similar results as fine-tuning on LLaMA-2-13B-Chat (Table 2), except that Mistral-7B-Instruct-v0.2 has more improvement than LLaMA-2-13B-Chat when fine-tuned on \(_{}(1)\).

### Data requirement for inducing compositional generalization

Compared with fine-tuning on \(_{}(1,2)\), one can observe that LLaMA-2-13B-Chat/Mistral-7B-Instruct-v0.2 fine-tuned on \(_{}(1,2,3)\) gains more performance boost on \(k=4,5\) across all settings. For example, skill-mix\((4)\) performance for LLaMA-2-13B-Chat fine-tuned on \(_{}(1,2)\) is nearly the same as the original LLaMA-2-13B-Chat and LLaMA-2-13B-Chat fine-tuned on \(_{}(1)\). However, for LLaMA-2-13B-Chat fine-tuned on \(_{}(1,2,3)\), the skill-mix\((4)\) performance improves from \(1\%\) to \(15\%\).

However, one may argue it is because \(_{}(1,2,3)\) has more data in total than \(_{}(1,2)\). To make a fair comparison, we conduct an ablation study by sub-sampling 8000 data from \(_{}(1,2,3)\), making sure that the number of data points with \(k=2\) and \(k=3\) in the sub-sampled set is less than the size of \(_{}(2)\). Table 4 shows the skill-mix\((k)\) performance of LLaMA-2-13B-Chat fine-tuned on the sub-sampled dataset. The metrics remain relatively close to the model fine-tuned on full \(_{}(1,2,3)\) and significantly better than the model fine-tuned on \(_{}(1,2)\). This ablation confirms that "skill-richer" data can induce the ability to compose skills faster.

   &  \\  & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\  ft’ed on \(_{}(1,2)\) &.51/.74 &.17/.65 &.01/.54 &.00/.51 \\ ft’ed on \(_{}(1,2,3)\) (8000 sample) &.66/.82 &.30/.74 &.11/.67 &.02/.62 \\ ft’ed on \(_{}(1,2,3)\) &.65/.81 &.33/.73 &.15/.69 &.06/.62 \\  

Table 4: skill-mix\((k)\) performance of models fine-tuned on LLaMA-2-13B-Chat, graded by GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model at different \(k\). \(_{}(1,2,3)\) (8000 sample) denotes the randomly sub-sampled dataset from \(_{}(1,2,3)\) with size 8000.

## 5 Discussions

### Using Claude 3 Opus as Grader for skill-mix evaluation

All the findings in the previous section are based on the skill-mix performance graded by GPT-4. However, GPT-4 is heavily used during data generation, and one can argue the improvement might solely come from the fact that GPT-4 favors its own outputs. Although the possibility is low, to rigorously eliminate this confounding factor, we re-evaluate skill-mix\({}_{}(k)\) using Claude 3 Opus as the Grader, and report the results in Table 5. Besides, we also include a consistency check between the GPT-4 and Claude 3 Opus graders in Table 6.

From Table 5, we observe the metrics graded by Claude 3 Opus have a similar trend as those graded by GPT-4: after fine-tuning on \(_{}(1,2,3)\), skill-mix\({}_{}(k)\) performance improves for all \(k=2,3,4,5\), while fine-tuning only on \(_{}(1)\) has limited improvement over the original LLaMA-2-13B-Chat. It proves that the improvement of skill-mix performance is not overfitted to GPT-4 preference.

Interestingly, we find that Claude 3 Opus is more generous, assigning higher scores to both the LLaMA-2-13B-Chat and the fine-tuned version. Besides, the results from Table 6 also confirm this argument: if an answer is assigned a full mark by GPT-4, then many of them will also be assigned a full mark by Claude 3 Opus. Such consistent biases among Graders were also noted in  when comparing LLaMA-2-70B-Chat and GPT-4 as Graders.

Besides switching to Claude 3 Opus, we also do human spot checks on the skill-mix generations, making sure that the model is not generating something that does not make sense to human. Please refer to Appendix D for some of the examples of skill-mix evaluations before and after the fine-tuning.

### Potential capability of going beyond "stochastic parrots behavior"

Whether models can go past "stochastic parrots" behavior  is crucial in discussions of AI risk. Based on reasonable performance of GPT-4 on skill-mix\((k=5)\) with common skills removed, Yu et al.  suggests GPT-4 is already beyond "stochastic parrots". In particular, after removing common skills (see definition in ), the probability of a random (5 skills, 1 topic) combination appearing in the training corpus is estimated to be \(11\%\) if the training token is \(2^{5}\). Therefore, if a model has a Ratio of Full Marks beyond \(11\%\) when \(k=5\), then it suggests the model is able to output novel text, thus is beyond "stochastic parrots". GPT-4 is the only model that can achieve this in 6.

  Model & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\   
 Llama-2-13B-Chat \\ fit’ed on \(_{}(1,2,3)\) \\  & 0.24/0.31/0.19 & 0.02/0.07/0.01 & 0.01/0.06/0.01 & 0.00/0.00/0.00 \\  

Table 6: (**Consistency between GPT-4 and Claude-3 grader) skill-mix\((k)\) performance of models fine-tuned on LLaMA-2-13B-Chat, graded on Claude-3 and GPT-4. Ratio of Full Marks graded by Claude 3 Opus/Ratio of Full Marks by both graders are reported for each model at different \(k=2,3,4,5\).**

   & _{}(k)\) Performance} \\  & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\   \\    Llama-2-13B-Chat \\ fit’ed on \(_{}(1)\) \\  &.31/5.2 &.07/4.48 &.08/6.64 &.00/4.42 \\   It’ed on \(_{}(1)\) \\  &.45/7.70 &.14/5.92 &.02/5.50 &.00/4.42 \\   It’ed on \(_{}(1,2,3)\) \\  &.69/8.1 &.57/8.83 &.26/7.77 &.10/6.69 \\   \\    Llama-2-13B-Chat \\ fit’ed on \(_{}(1)\) \\  &.24/5.00 &.02/4.42 &.01/4.00 &.00/3.34 \\   It’ed on \(_{}(1)\) \\  &.27/6.62 &.05/5.00 &.00/4.40 &.00/3.33 \\  
 It’ed on \(_{}(1,2,3)\) \\  &.65/8.1 &.33/.73 &.15/6.99 &.06/6.62 \\  

Table 5: (**Comparison between GPT-4 and Claude-3 grader) skill-mix\((k)\) performance of models fine-tuned on LLaMA-2-13B-Chat, graded on Claude-3 and GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model at different \(k=2,3,4,5\).**Table 7 shows the skill-mix (\(k=5\)) performance of fine-tuned LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 with common skills removed. We also include some \(_{}(4)\) data (about 1000) into the training set. The fine-tuned models all show significant improvement over the base models. For example, the Ratio of Full Marks for the fine-tuned LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 all go beyond \(15\%\) for skill-mix (\(k=5\)) on training skills and topics, and reaches \(6\%\) on all skills and topics, after filtering out the common skills.

Although both fine-tuned models cannot reach \(11\%\) for skill-mix\({}_{}(5)\) and skill-mix\({}_{}(5)\), we hypothesize that with more skill-richer data (say \(_{}(4)\)), the models can acquire the ability to combine skills much more efficiently and go beyond "stochastic parrots" eventually.

We further discuss the limitation of our work in Section 7, and potential influences on AI safety caused by stronger composition capability in Appendix E.

## 6 Conclusion and Takeaways

We have studied the extent to which models can learn compositional generalization over language skills by fine-tuning on suitable examples demonstrating such composition. Previous evaluations had seemed to suggest that the extent of compositional generalization is determined by the model size and pretraining , but here we were able to induce much better compositional capability via fine-tuning on data that was generated using a setup similar to skill-mix.

One surprising finding was that fine-tuning examples that composed \(2\) and \(3\) language skills were enough to improve the capability to compose \(4\) and even \(5\) language skills. Another surprise was that the ability to combine language skills from held-out categories improved at the same rate as the skills used in the training examples. Of course, these findings were still about skills that are near relatives. The full extent of such "out of (training) distribution" generalization remains to be explored.

## 7 Limitation

The main limitation of the current study is the high computational and financial costs, which impede us from sweeping more hyperparameters and conducting repeated experiments with different random seeds. These costs include the number of GPU hours for fine-tuning and the cost of calling OpenAI's API to generate the \(_{}(k)\) data and evaluate the skill-mix performance. Despite these difficulties, we managed to sweep the hyperparameters for fine-tuning the LLaMA-2-13B-Chat on \(_{}(1,2,3)\) (Main experiment, Table 2). We believe our findings are robust to different random seeds because of the clear message and consistent trend of the results.

Besides, compositional generalization is a vast topic and we only study this under the setting of _language skills_ (limited to the skill-mix setting). Whether the models can learn compositional generalization in other settings still needs further exploration.