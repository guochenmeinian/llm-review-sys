# Adaptive Experimentation When You Can't Experiment

Yao Zhao

University of Arizona

yaoz@arizona.edu

&Kwang-Sung Jun

University of Arizona

kjun@cs.arizona.edu

&Tanner Fiez

Amazon

fieztann@amazon.com

&Lalit Jain

University of Washington

lalitj@uw.edu

###### Abstract

This paper introduces the _confounded pure exploration transductive linear bandit_ (CPET-LB) problem. As a motivating example, often online services cannot directly assign users to specific control or treatment experiences either for business or practical reasons. In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects. Instead, online services can employ a properly randomized encouragement that incentivizes users toward a specific treatment. Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such _encouragement designs_. We consider a more general underlying model captured by a linear structural equation and formulate pure exploration linear bandits in this setting. Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded. Elimination-style algorithms using experimental design methods in combination with a novel finite-time confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound. Finally, experiments are conducted that demonstrate the efficacy of our approach.

## 1 Introduction

In this study, we present a methodology for adaptive experimentation in scenarios characterized by potential confounding. Online services routinely conduct thousands of A/B tests annually . In most online A/B/N experimentation, meticulous user-level randomization is essential to ensure unbiased estimates of _treatment effects at the population level_, commonly known as average treatment effects (ATE). In this setting, firms are often interested in understanding the treatment with the highest average outcome if presented to all members of the population. However, in many settings firms may not be able to randomize, for example if a feature must be rolled out to all users for various business reasons . In such instances, users may choose to engage with a feature or not based on potentially unobservable preferences. Thus the resulting measured outcome may be correlated with the decision to engage in a specific feature. I.e. the underlying characteristics of the user _confound_ the relationship between the decision to use the feature being evaluated and the effect of the feature. Thus, naively comparing the average outcome for users who engage with a feature with those who do not suffers from a (selection) bias. This setting is captured in Figure 1.

A potential solution is for services to employ _encouragement designs_ where users are presented with incentives that encourage users to engage with a specific feature . As a concrete example, many online services have introduced membership levels with different offerings andprices available to all users. Given a set of membership level options, the service is interested in knowing the counterfactual of which level has the optimal outcome (e.g., total revenue) if every user chooses to join that membership level. In this setting, encouragements could be coupons or trials for corresponding membership levels. In these settings, the firm can use intent-to-treat (ITT) estimates for the treatment effect which naively compare the average outcomes between the groups given different encouragements. In practice, given an encouragement a user may not engage with the corresponding feature choosing either the control or a different feature. Hence, the resulting ITT estimate may be a diluted estimate of the ATE . However, all is not lost: if the encouragement presented to a user is properly randomized, and the service guarantees that the encouragement only affects the outcome through the choice of user treatment, then the encouragement acts as an _instrumental variable_. Standard analysis from the econometrics and compliance literature show that two-stage least squares (2SLS) estimators can then be used to provide consistent estimates of treatment effects.

At the same time, firms are also increasingly utilizing adaptive experimentation techniques, often known as _pure exploration multi-armed bandits_ (MAB) algorithms , to accelerate traditional A/B/N testing. Pure exploration MAB techniques promise to deliver accurate inferences in a fraction of the time and cost as traditional methods. Similar to A/B testing, bandit methods assume users are properly randomized and can fail to learn the optimal treatment if naively used and may be sample inefficient if they fail to take the confounded structure into account.

**Contributions.** In this work, we provide a methodology for experimenters seeking to use adaptive experimentation in settings with confounding where encouragements are available. We formulate this work in the more general and novel setting of _confounded pure exploration transductive linear bandits_ (CPET-LB) (Section 1.1). We present algorithms using experimental design for the CPET-LB problem and analyze the resulting sample complexity. As we demonstrate, even in the simple multi-armed bandit setting described above, computing an effective sampling pattern requires using the machinery of linear bandits. Without knowledge of the underlying structural model, existing linear bandit approaches could lead to suboptimal sampling. The main technical challenge we face is simultaneously improving our estimate of the structural model while designing with inaccurate estimates (Section 3). This approach crucially relies on our development of novel finite-time confidence bounds for two-stage least squares (2SLS) style estimators that may be of independent interest (Section 2.2). Moreover, we provide worst-case sample complexity lower bounds that are nearly matched by our sample complexity upper bounds (Appendix D). Though the goal of this work is primarily theoretical, we empirically show the efficacy of our method over existing solutions (Appendix E).

### General Problem Formulation

A confounded pure exploration transductive linear bandits (CPET-LB) instance consists of finite collections of measurement vectors \(^{d}\) and evaluation vectors \(^{d}\). At each time \(t\), the learner selects \(z_{t}\) and observes a pair of noisy responses \(x_{t}^{d}\) and \(y_{t}\) generated via the structural equation model

\[x_{t}=^{}z_{t}+_{t}, y_{t}=x_{t}^{}+ _{t}, \]

where \(^{d d}\) and \(^{d}\) are model parameters.1 We define the history \(_{t-1}=\{(z_{s},x_{s},y_{s})\}_{s<t}\) and \(_{t-1}[]=[|_{t-1}|]\) denoting the conditional expectation under the filtration generated by \(_{t-1}\). The noise \(\{_{t}\}_{t=1}^{}\) and \(\{_{t}\}_{t=1}^{}\) satisfy the following set of assumptions unless otherwise noted.

**Assumption 1**.: We assume \(_{t}_{t-1}\) is 1-sub-Gaussian (and thus \([_{t}_{t-1}]=0\)). Furthermore, \(_{t}_{t-1}\) is \(_{}^{2}\)-sub-Gaussian vectors (and thus \([_{t}_{t-1}]=0\)), i.e.,

\[,_{a:\|a\|_{2} 1}[  a,_{t}](_{}^ {2}}{2})\;.\]

Figure 1: Causal graph of the model.

**Goal.** The objective is to identify \(w^{*}:=_{w}w^{}\) with probability at least \(1-\) for \((0,1)\) while taking a minimum number of measurements.

In the setting where \(=I,=0\) and \(_{t-1}[_{t}|x_{t}]=0\), our setting reduces to the standard _pure exploration transductive linear bandit_ problem . In general, the joint noise process \(_{t},_{t}\) may be dependent across the entries. In particular, we are allowing for the data generating process to be _endogenous_, meaning that \(_{t-1}[_{t}|x_{t}] 0\). That is, \(_{t}\) can affect not just \(y_{t}\), but also \(x_{t}\) given a choice of \(z_{t}\). The presence of endogeneity is a key challenge in the CPET-LB problem.

**Assumption 2** (Exclusion Restriction).: We assume that \(_{t-1}[z_{t}_{t}]=0\), or alternatively that \(z_{t}\) is uncorrelated with \(_{t}\).

The variable \(z_{t}\) is commonly referred to as an _instrumental variable_. We consider algorithms for the CPET-LB problem that stop at a \(_{t}\)-measurable time \(\), and produce a recommendation \(\). The goal is \(\)-PAC algorithms with efficient sample complexity guarantees.

**Definition 1.1** (\(\)-Pac).: We say an algorithm is \(\)-PAC for a CPET-LB problem with \(,^{d}\) if for all \(^{d}\) and \(^{d d}\), it holds that \(_{,}( w^{*})\) for \((0,1)\).

### Encouragement Designs

The CPET-LB feedback model generalizes the classical _compliance_ setting.

**Compliance as a Special Case.** In _compliance_ problems, a decision-maker has access to a set of treatment that can be offered to users, while the users themselves have the option to accept the treatment they are presented or instead opt-in to a different treatment. The goal is to identify the treatment with the optimal average outcome if all users were to accept it. Specifically, given a finite set \(=\{1,2,,d\}\), a decision-maker presents user \(t\) with an encouragement for a treatment \(i\), the user then selects treatment \(j\) where potentially \(j i\), and an outcome \(y_{t}\) results. To capture compliance with the CPET-LB framework, we set \(===\{e_{1},,e_{d}\}\) and the parameter \(\) captures the probability of accepting a treatment given an encouragement for a potentially different treatment. Specifically, \((i,j)=x_{t}=e_{i} z_{t}=e_{j}\), and a straightforward computation shows that \(x_{t}=^{}z_{t}+_{t}\) where \([_{t}|z_{t}]=0\) with

\[_{t}=x_{t}-e_{1} z_{t},,e_{d} z_{t}^{}. \]

Moreover, \(y_{t}=x_{t}^{}+_{t}\) gives the resulting reward, which is clearly correlated with the user choice so that \((_{t},_{t}) 0\). Finally, \(e_{i}^{}=_{i}\) gives the expected value of treatment \(i\) over the population and our goal is to identify \(w^{*}=_{e_{i}}e_{i}^{}\). 2 Note that when \(=I\), we automatically have that \(_{t}=0\) and there is no confounding. This reduces to the standard MAB setting.

**Motivating Compliance Example.** As a motivating compliance example representing the membership level discussion from the introduction, consider a location model that assumes each user \(t\) arriving online has an underlying unobserved one-dimensional preference \(u_{t}(0,_{u}^{2})\). If an algorithm presents the user with encouragement \(z_{I_{t}}=e_{I_{t}}\) for \(I_{t}\), then the user selects into the membership level given by \(J_{t}=_{j}|I_{t}+u_{t}-j|\) so that \(x_{t}=e_{J_{t}}\). This process captures a user being more likely to opt-in to membership levels that are closer to the encouragement that they were presented. The outcome is then given by \(y_{t}=x_{t}^{}+u_{t}\).

We conduct an experiment with this problem instance (see Fig. 2 and Appendix B for more details). Specifically, \(d=6\), \(=1&-0.95&0&0.45&0.95&0.99\), and \(_{u}^{2}=0.35\). Observe that \(w^{*}=e_{1}=_{w}w^{}\). An upper confidence bound (UCB) selection strategy is simulated that maintains estimates of the mean reward of each encouragement \(i\), namely \(_{i,t}=_{s=1}^{t}\{z_{t}=e_{i}\}y_{t}\), and then pulls the one with the highest UCB. The UCB selection strategy is combined with a pair of recommendation strategies. The UCB-OLS algorithm estimates the mean reward of each treatment using an OLS estimator, namely \(_{}^{\,t}=_{s=1}^{t}\{x_{t}=e_{i}\}y_ {t}/_{s=1}^{t}\{x_{t}=e_{i}\}\), and recommends \(_{a}_{}^{\,t}\). Moreover, the UCB-IV algorithm uses an instrumental variable-estimator (see the next section) that incorporates knowledge of \(\) similar to 2SLS to deconfouestimates of the mean rewards of each treatment and recommends the treatment with the maximum estimate. The results over 100 simulations are shown in Fig. 2. UCB-OLS completely fails to identify \(_{1}=_{i d}_{i}\) due to a biased estimate, whereas UCB-IV does better. However, UCB-IV methods seem to have a constant probability of error. To see why, note that the expected reward from pulling \(z=e_{i}\) is \(e_{i}^{}\). These values are plotted in orange in Figure 1(a). In particular, with some constant probability, UCB zeroes in on arm 6 becauses of the mean estimates on the \(z\)'s, and as a result fails to give enough samples to learn that arm 1 is indeed the best. In contrast, our proposed method CPEG, Algorithm 1 manages to find the best arm with significantly higher probability.

**Notation.** Let \(()=\{^{||}: 0,_{z }_{z}=1\}\) denote the set of probability distributions over the set \(\). Given a distribution \(()\) and matrix \(^{d d}\), define the operator \(A(,):=_{z}_{z}^{}zz^{}\). Given \(Z^{T d}\) and \(^{d d}\), define the operator \((Z,):=_{t=1}^{T}^{}z_{t}z_{t}^{}=^ {}Z^{}Z\) where \(z_{t}^{d}\) denotes row \(t\) of \(Z\). Given a vector \(x^{d}\) and a symmetric positive-definite matrix \(A^{d d}\) we let \(\|x\|_{}^{2}=x^{}Ax\). We adopt the standard notation that \((a b)\{a,b\}\) and \((a b)\{a,b\}\) for \(a,b\). \(_{}(A),_{}(A)\) denote the minimum and maximum singular value of a matrix \(A\). We denote by \((x_{1},,x_{n})\) any polylogarithmic factors of \(x_{1},,x_{n}\).

### Related Works

Our work is at the intersection of several parallel tracks of literature, pure exploration linear bandits, causal bandits, and econometrics. The most relevant work on pure exploration in linear bandits is the RAGE algorithm of [15; 33]. RAGE is nearly instance optimal for linear bandits in the non-confounded setting. Extensions of RAGE to various noise models including logistic and heteroskedastic noise have been considered [35; 20]. Other algorithms for pure exploration linear bandits have been proposed - and we leave it for future work to extend the ideas of this paper to those settings [27; 10].

Confounding in bandits was first considered in the regret minimization setting by . They introduces the Multi-armed bandit with unobserved confounders (MABUC) problem. They empirically demonstrate traditional bandit algorithms can have linear regret in this setting and provide an algorithm that effectively employs observed intuition. The early work of  also assumes there is an additional unobserved latent class at each time that determines confounding in a compliance setting. They provide novel notions of regret, relative to the instrument with the highest reward (\( Z^{}^{}\) in our notation), the highest treatment (\(_{w}w^{}\)), regret relative to the best latent class at each time, and regret on the set of "compliers". They discuss the suitability of these various notions of regret, and discuss when sublinear regret is possible. We remark that their approach is similar to ours in the sense that they assume a form of homogeneous effects across the population, and use an estimate of \(\). Recently  also consider the problem of compliance, however they don't take explicit non-confounding into account and assume an explicit parametric model that determines the non-compliance. This is analogous to the Heckman selection model considered in econometrics .

The recent works of [11; 36; 17] considered an online setting where at each time they observe a set \(\{(x_{t},z_{t})\}\) where \(x_{t}\) is the action of interest and \(z_{t}\) is an associated instrument. If action \(I_{t}\) is selected, the reward observed is \(y_{t}=x_{I_{t}}^{}_{t}+_{t}\), where \(x_{t}\) may be endogeneous. Similar to the standard linear bandit setting [1; 26], the goal is to minimize regret relative to the best action at each time. We remark that this setting is very different from ours. Effectively, we are choosing which instrument to select at each time to learn the best-performing treatment - in particular we can't choose a particular intervention. In their setting, they are choosing an intervention at each time and using the instrument purely for de-confounding the result. Experimental design for instruments to have more effective estimation has been considered by .

In the causal bandit problem, an underlying causal graph between a set of interventions and a reward value is assumed. Actions correspond to intervening (i.e. a "do" operation ) at one or more specific nodes in the causal graph and then observing the corresponding value at the reward node. Causal bandits have been studied extensively in the regret setting [25; 28; 6] and the pure exploration setting . Though past works have allowed for unobserved confounders in the graph e.g. , their goal is to learn the best performing intervention, which in our setting would be \(_{z Z}z^{}\) instead of \(w^{*}\).

Encouragement designs have been considered in many applications in online and offline settings. One of the earliest works on encouragement designs is , which considers the problem of using encouragements to determine the impact of coupons at a grocery store. More recent applications include [4; 30; 13] all in the context of online services and treatments that are required to be served to all users. Most of these works consider a small number of treatments and a heterogeneous treatment effect - hence are interested in LATE estimator. As far as we are aware, we are the only work that considers adaptive encouragement design in the context of the model given in Equation 1 and for multiple treatments.

## 2 Estimators and Inference

We now present estimators for the unknown parameter \(\) and prove the associated statistical properties. The estimators discussed in this section are critical to our algorithmic solution outlined in Section 3.

### Estimators

Before describing our solution concept, we quickly review potential options for estimating \(\) based on a dataset \(Z_{T}=[z_{1},,z_{T}]^{}^{T d},X_{T}=[x_{1}, ,x_{T}]^{}^{T d}\), \(Y_{T}=[y_{1},,y_{T}]^{}^{T}\), assumed to be generated according to the model in Eq. (1). Recall that the _ordinary-least-squares_ (OLS) estimator for \(\) is given by

\[_{}:=_{^{d}} _{t=1}^{T}(y_{t}-x_{t}^{})^{2}=(X_{T}^{}X_{T})^{- 1}X_{T}^{}Y_{T}=+(X_{T}^{}X_{T})^{-1}X_{T}^{}_{T}. \]

Observe that \(_{}\) is potentially a biased and inconsistent estimator for \(\) in the presence of endogenous noise since \([_{t}|x_{t}] 0\). To remediate this problem, we define a general class of estimators that includes several standard estimators. Given an invertible matrix \(^{d d}\), let \(_{T}:=Z_{T}\), and consider corresponding estimators termed \(\)-IV estimators of the form

\[_{}:=(_{T}^{}_{T})^{-1}_{T}^{ }Y_{T}=(^{}Z_{T}^{}Z_{T})^{-1}^{}Z_{T}^{}Y_{T}=(Z_ {T}^{}Z_{T})^{-1}Z_{T}^{}Y_{T}. \]

When \(=I\) we recover the OLS estimator. In the rest of the paper, we will focus on two different potential options for \(\).

**Case 1:** Oracle. \(=\). To begin, observe that the structural equation model from Eq. (1) can be combined by substituting the second equation into the first to obtain the reduced form

\[y_{t}=z_{t}^{}+_{t}^{}+_{t}. \]

Since \(z_{t}\) is independent of the i.i.d. process \(_{t}^{}+_{t}\), the least squares estimator which regresses \(y_{t}\) onto \(z_{t}^{}\) is unbiased for estimation of \(\) and given by

\[_{}=(_{T}^{}_{T})^{-1}_{T}^{}Y_{T}=(Z_{T}^{}Z_{T})^{-1}Z_{T}^{}Y_{T}. \]

This estimator will be used to design our general solution concept presented in Section 3. Of course in practice we cannot expect to know \(\), but we may be able to estimate it.

**Case 2:**P-2SLS, \(=\).** We consider a setting where \(\) is an (unbiased) estimator of \(\), learned using least-squares from an _independent_ dataset \(Z_{T_{1}}=[z_{1}^{},,z_{T_{1}}^{}],X_{T_{1}}=[z_{1}^{}, ,z_{T_{1}}^{}]\) collected non-adaptively.3 That is, \(=(Z_{T_{1}}^{}Z_{T_{1}})^{-1}Z_{T_{1}}^{}X_{T_{1}}\) and:

\[_{}=(^{}Z_{T}^{}Z_{T} )^{-1}^{}Z_{T}^{}Y_{T}.\]

We refer to the resulting estimator as a pseudo two stage least squares (P-2SLS) estimator. The main advantage of the P-2SLS estimator over standard 2SLS (given in Appendix C) is easier inference since now \(\{_{t}\}_{t T}\) of our dataset is independent of the measurements of the first dataset \(Z_{T_{1}},X_{T_{1}}\). In the econometrics literature, such an estimator is referred to as a _two-sample 2SLS estimator_.

### Confidence Intervals

In the section that follows, we develop a general algorithmic approach that relies on _experimental design_ aimed at reducing the uncertainty in our estimates of the optimal treatment. To this end, we first develop finite-time confidence intervals for estimators presented in the previous section given data generated according to the model in Eq. (1) and collected from non-adaptive designs.

We begin by characterizing the properties of the noise structure in the combined model of Eq. (5) with the following set of results.

**Lemma 2.1**.: _Under Assumption 1, the noise process \(:=^{}+\) is \(_{}^{2}\)-sub-Gaussian where \(_{}^{2}=2(_{}^{2}\|\|_{2}^{2}+1)\), specifically when the instance is compliance, \(_{}^{2}=2(4\|\|_{2}^{2}+1)\)._

**Oracle Confidence Interval**. As in the last section, we assume that we have access to a dataset \((Z_{T},X_{T},Y_{T})\) generated according to Eq. 1 and collected non-adaptively. Given Lemma 2.1, it can be shown that \(w^{}_{}\) is a sub-Gaussian random variable satisfying the following.

**Lemma 2.2**.: _With probability at least \(1-\) for \((0,1)\) and \(w^{d}\),_

\[|w^{}(_{}-)|^ {2}\|w\|_{(Z_{T},)^{-1}}^{2}2/},\]

_where \(_{}^{2}\) is the sub-Gaussian parameter of the noise \(:=^{}+\) characterized in Lemma 2.1._

The proof of this result is in Appendix G.2.

**P-2SLS Confidence Interval**. We now present a novel finite-time confidence interval for the P-2SLS estimator. As discussed in the previous section with respect to this estimator, we assume access a set of data \((Z_{T_{1}},X_{T_{1}})\) generated according to Eq. (1) and collected non-adaptively for the purpose of estimating \(\). Moreover, assume access to a separate set of data \((Z_{T_{2}},X_{T_{2}},Y_{T_{2}})\) generated according to Eq. (1) and collected non-adaptively for the purpose of estimating \(\).

**Theorem 2.3**.: _Suppose that \(=(Z_{T_{1}}^{}Z_{T_{1}})^{-1}Z_{T_{1}}^{}X_{T_{1}}\) and \(_{}=(^{}Z_{T_{2}}Z_{T_{2}} )^{-1}Z_{T_{2}}^{}Y_{T_{2}}\). Then, for any \(w\), with probability at least \(1-\) for \((0,1)\),_

\[|w^{}(_{}-)|\|w\|_{(Z_{T _{2}},)^{-1}}^{2}}+\|w\|_{(Z_{T_{1}},)^{-1}}\|\|_{ 2}^{2}Z_{T_{1}},/4}},\]

_where_

\[(Z_{T},):=8d(1+^{2}}{d(2 _{}(Z_{T_{1}}^{}Z_{T_{1}}))})+16(}{}_{2}^{2}((Z_{T_{1}}^{}Z_{T_{1} })})).\]

The proof is presented in Appendix G.3. Observe that the first term in the P-2SLS estimator confidence interval given by \(^{2}\|w\|_{(Z_{T_{2}},)^{-1}}^{2} 4/}\) matches the Oracle estimator confidence interval in Lemma 2.2 when \(=\). The second term scaling like \((\|w\|_{(Z_{T_{2}},)^{-1}}\|\|_{2} _{}1/})\), is an upper bound on the approximation error \(w^{}(^{-1}-I)\) for any \(w^{d}\), assuming that \(\) is learned from an OLS estimator (see Theorem G.3 for details).

We will see that the form of this confidence interval is particularly convenient for our algorithmic approach given in Section 3. In particular, the form of the variance \(\|w\|_{A(Z_{T_{2}},)^{-1}}^{2}\) on the first term only depends on a design over instruments. Thus, we can choose an experimental design over \(Z\)'s which reduces this variance optimally.

_Remark 2.4_.: In practice we expect the first stage of samples, \((Z_{T_{1}},X_{T_{1}})\) to be collected from either a burn-in period or from existing historical data. We remark that assuming two stages of samples is common in the orthogonal and double machine learning for estimating nuisance parameters in the data generating process (e.g. \(\)) . Our result matches the existing literature on the asymptotic variance of two sample 2SLS estimators (e.g., Theorem 1 of ).

_Remark 2.5_.: The asymptotic variance of standard 2SLS is known to involve a factor \(_{}^{2}\), instead of \(_{}^{2}\) as we have . Recent work by  shows a variance involving \(d_{}^{2}\). However, it's unclear how to use the form of their confidence interval directly for experimental design. In addition, their work is not sufficiently general to handle the general forms of noise that we consider in Lemma 2.1.

## 3 Adaptive Experimental Design Algorithms

We now present adaptive experimental design algorithms for the CPET-LB problem. Our main insight utilizes Eq. 1 by plugging the model for \(x\) into the top equation resulting in the relationship

\[y=z^{}+^{}+.\]

When \(\) is known, by Eq. 5, we see that CPET-LB reduces to a standard pure exploration transductive linear bandit problem where the measurement set is given by \(\{^{}z\}_{z}^{d}\), the evaluation set is \(^{d}\), and the feedback model is given by \(y=v^{}+\) where the noise \(=^{}+\) is sub-Gaussian and as before the goal is to identify \(_{w}w^{}\). An existing approach to this problem is given by the RAGE algorithm , which we use as the basis of our approach. Addressing the case of unknown \(\) is our major algorithmic contribution, where we develop solutions to improve our estimate of \(\) and learn \(w^{*}\) simultaneously. As a warm-up to this approach, we first consider the setting when \(\) is known.

### Warm-Up: Known Structural Model

Algorithm 1 assumes a parameter \(L_{}\), which acts as an upper bound on the sub-Gaussian constant of the noise \(=^{}+\). In each round \(k\), an active set of potentially optimal vectors \(}_{k}\) is maintained. CPEG aims to sample in such a way that reduces the uncertainty of the estimates on the _gaps_\((w-w^{})^{}\) for each pair \(w,w^{}}_{k}\) maximally each round. In any given round the algorithm takes \(N_{k}\) samples \(Z_{N_{k}}\), the confidence interval of Lemma 2.2 shows that the error in estimating \((w-w^{})^{}\) scales with \(\|w-w^{}\|_{[^{}Z_{N_{k}}^{}Z_{N_{k}})^{-1}}^{2}\). This motivates utilizing an _experimental design_ approach where we choose a distribution \(_{k}()\) to minimize \(_{w,w^{}}_{k}}\|w-w^{}\|_{(_{z }_{z}^{}zz^{})^{-1}}\). The number of resulting samples taken from this design \(N_{k}\) is chosen to guarantee that the confidence interval of Lemma 2.2 is less than \(2^{-k}\). Then, the elimination step in Line 8 guarantees that all \(w\) such that \((w^{*}-w)^{}>2 2^{-k}\) are then eliminated from the active set by round \(k+1\) of the procedure. To actually choose our samples, as is common in this literature , we use an efficient rounding procedure, ROUND that requires a minimum number of samples \(r()\).

**Sample Complexity Guarantee.** The sample complexity of Algorithm 1 depends on the following problem-dependent quantity \(^{*}()\) that captures the underlying hardness of a problem instance in terms of \((,,,)\), when \(=0\), we abbreviate \(^{*}(0)=^{*}\),

\[^{*}()=_{()}_{w \{w^{*}\}}-w\|_{(_{z}_{z} ^{}zz^{})^{-1}}^{2}}{ w^{*}-w,^{2} ^{2}}. \]

**Theorem 3.1**.: _Algorithm 1 is \(\)-PAC and terminates in at most \(c(1+)L_{}^{*}(1/)+cr()\) samples, where \(c\) hides logarithmic factors of \(:=_{w} w^{*}-w,\) and \(||\), as well as constants._

The proof of this result is in Appendix H.1. In the unconfounded case when \(=I\) and \(=0,_{t-1}[_{t}|x_{t}]=0\) this matches the sample complexity of . In particular, for the case where \(==\), the problem further reduces to a standard multi-armed bandit, and if \(\) is 1-sub-Gaussian noise,  shows that \(^{*}=O(_{i=2}^{d}(_{1}-_{i})^{-2}))\), which is the optimal sample complexity of best-arm identification for multi-armed bandits. The following lemma shows that the conditioning of \(\) can have a strong impact on the resulting sample complexity.

**Lemma 3.2**.: _For the compliance setting, we have \(_{^{d}}_{j,j^{}}\|e_{j}-e_{j^{}}\|_{(_{i =1}^{d}_{i}^{}e_{i}e_{i}^{})^{-1}}^{2} d_{j,j^{}}\|^{-1}(e_{j}-e_{j^{}})\|_{2}^{2}\). Furthermore, \(^{*}^{2}()^{-1}}{_{}^{2}}\)._

To further illustrate the impact of \(\), imagine an extreme setting where \(=(1-)/d^{}+ I\) and \( 0\), i.e. \(\) is a perturbation of \(1/d^{}\). It's straightforward to show that the upper bound in the first display of Lemma 3.2 is of the order \(O(d^{-2})\) (this is also a lower bound - see Appendix K.1). In particular, the upper bound on the sample complexity is of the form \(d^{-2}/_{}^{2}\). This is in sharp contrast to the linear bandit case, when \(=I\) and we are guaranteed a sample complexity of no more than \(d/_{}^{2}\) samples. To gain some intuition, regardless of the choice of \(\), \(_{i d}_{i}^{}e_{i}e_{i}^{}\). As a result, \(^{*}\) as \( 0\). Intuitively in the limit, regardless of which instrument \(i d\) is being pulled, the resulting distribution on the treatments is uniform (the instruments are _weak_). Thus, it is impossible to deconfound the measurement noise, and recover an estimate of \(\). This is a phenomenon which does not arise in the standard multi-armed bandit case with unconfounding.

_Remark 3.3_.: We also consider a setting where instead of given \(\) directly, we are given an estimate \(\) of \(\) based on offline data. We discuss such an adaptation of Algorithm 1 to this setting in Appendix I and provide a sample complexity which reflects the error in \(\) (scaling with \(^{*}()\) for \(>0\)). We remark that this result is subsumed by the approach of Section 3.2 and so we omit it in the main text.

```
1:Input \(,,=,,L_{}_{}^{2},\),
2:Initialize:\(k=1,_{1}=,_{1}=1\)
3:Set \(f(w,w^{},,):=\|w-w^{}\|_{(_{z Z}_{z} ^{}z_{z})^{-1}}^{2}\)
4:while\(|_{k}|>1\)do
5:\(_{k}=_{()}_{w,w^{} _{k}}f(w,w^{},,)\).
6:\((_{k})=_{()}_{w,w^{} _{k}}f(w,w^{},,)\).
7:\(N_{k}:= 2(1+)2^{}(_{k})L_{}(4k^{2}| |/) r()\)
8: Pull arms in \(Z_{N_{k}}=(_{k},N_{k})\) and observe \(Y_{N_{k}}\).
9: Compute \(_{}^{k}=(Z_{N_{k}}^{}Z_{N_{k}})^{- 1}Z_{N_{k}}^{}Y_{N_{k}}\)
10:\(_{k+1}=_{k}\{w_{k}| w^{ }_{k}, w^{}-w,_{}^{k} >2^{-k}\},k k+1\)
11:endwhile
12:Output:\(w_{k}\)
```

**Algorithm 1** CPEG:Confounded pure exploration with \(\)

**Lower bound.** Due to the noise model from confounding and the dependence of the noise \(^{}+\), the instance-dependent lower bounds of  do not immediately apply. We develop a lower bound tailored for the confounding setting **that nearly match the upper bounds** of our algorithms. What's more, our lower bound illustrates the additional difficulty that arises from confounding by an additional factor of \(d^{2}\) compared to the standard transductive linear bandit problem in the most general setting where entries of \(\) are sub-Gaussian, but not necessarily independent nor bounded. Due to space limit, we defer it to Appendix D.

### Fully Unknown Structural Model

We now consider the setting where \(\) is fully unknown. The difficulty of this setting is that the data collection process needs to support both estimation of \(\) and \(\) simultaneously. Our algorithm, built upon Algorithm 1, is summarized in Algorithm 3. At its core, each phase of the algorithm is divided into two sub-phases, for estimating \(\) and \(\) respectively. Specifically, the second sub-phase is essentially same as Algorithm 1 with \(_{k}\) in place of \(\) where \(_{k}\) is estimated from the first sub-phase. The main novelty of our algorithmic design lies in the first sub-phase, which resolves the challenge of performing the optimal design for estimating \(\). To explain this challenge, the confidence interval for P-2SLS estimators of Theorem 2.3 indicates that one should pull arms so that we control both \(D_{2}:=_{w,w^{}}\|w-w^{}\|_{(Z_{T_{2}},_{k})}^{2}\) (error from \(_{}\)) and \(D_{1}:=_{w,w^{}}\|w-w^{}\|_{(Z_{T_{1}},_{k})}^{2}\) (error from \(_{k}\)) to be below the target error \(O(_{k}^{2})\) at each phase (ignoring unimportant factors for discussion). Controlling \(D_{2}\) is trivial, which is done in the second sub-phase as we described above.

However, for \(D_{1}\), a similar strategy cannot be done because the estimate \(_{k}\) is computed directly by sampling arms in \(Z_{T_{1}}\). That is, the ideal design, based on which we _will_ collect data points \(z_{1},,z_{n}\), requires access to the random matrix \(_{k}\) that can only be computed _after_ sampling \(z_{1},,z_{n}\). Thiscreates a cycle that seems impossible to resolve. Such an issue, to our knowledge, has not been seen in existing work on pure exploration, and thus resolving it is our key technical contribution.

Our solution is to compute the design based on \(_{k}\) from the previous phase. We then perform a doubling trick where we double the sample size (while following the computed design) until \(D_{1}\) becomes smaller than the target error \(O(_{k}^{2})\). The intuition is that in later phases the estimate \(_{k}\) from the previous phase will be accurate enough to ensure that the design is efficient. Note that this novel algorithm induces extra randomness in how many samples we end up collecting in the first sub-phase, which remains random even after conditioning on the history, unlike the second sub-phase. This makes the analysis challenging, which we describe after the main result.

Our algorithm additionally employs the so-called E-optimal design to ensure that the covariance matrix of the collected data used to estimate \(\) is well-conditioned. This conditioning is required to ensure that \(_{k}\) concentrates fast enough to \(\) as shown in the analysis. The E-optimal design is a well-known design objective in experimental design that aims to maximize the smallest singular value: \(_{E}^{*}:=_{()}_{}(V^{-1}( ))\), where \(V=_{z}_{z}zz^{}\). We denote \(_{0}^{-1}:=_{}(V^{-1}(_{E}^{*}))=_{}^{-1}(V( _{E}^{*}))\) as the smallest singular value achieved by the E-optimal design.

We present our analysis result Theorem 3.4 where we show that, even without knowledge of \(\), the sample complexity scales with the key problem difficulty \(^{*}\) almost matching the sample complexity of Algorithm 1 which relies on knowledge of \(\).

**Theorem 3.4**.: _Algorithm 3 is \(\)-PAC and terminates in at most_

\[(1+)((L_{}1/+L_{}\|\|_{2}^{2}(d+ 1/))^{*}+(d+1/)(L_{ }\|\|_{2}^{2}_{0}+M))\]

_pulls, ignoring both of the additive and multiplicative logarithms of \(,||,^{*},_{0},M\), where_

\[_{0}=_{w\{w^{*}\}}\|w^{*}-w\|_{(_{z }_{E,z}^{}zz^{})^{-1}}^{2},M=}{_{}^{2}_{}A(_{E},I)} 1.\]

_Note that \(_{0}\) does not get hurt by \( w^{*}-w,\), (\(^{*}\) does). It comes from the fact that in the first phase, we initialize that algorithm with E-optimal design._

The challenge of the analysis can be summarized in two-fold. First, since the concentration result in Theorem 2.3 is w.r.t. \(_{k}\), we need to analyze how the random matrix \(_{k}\) concentrates around \(\) and how this impacts the sample complexity. For this, we develop a novel concentration inequality that relates the confidence width involving \(\) from Theorem 2.3 with the same quantity involving \(\) in place of \(\). Second, our algorithm creates a long-range error propagation, which is highly nontrivial to analyze. To see this, the quality of \(_{k}\) is affected by the design objective function \(_{w,w^{}}f(w,w^{},_{k-1},)\), which depends on the error of the estimate \(_{k-1}\) from the previous phase. This error is, in turn, affected by the error of \(_{k-2}\) by the same mechanism. This is repeated all the way back to the first phase. Thus, any abnormal behavior from the first iteration will have a cumulative impact to even the end. In our analysis, we successfully analyze how the error is propagated from the previous iterations, which forms a complicated recursion. Resolving this recursion is our key novelty in the analysis.

_Remark 3.5_.: Our algorithm requires knowledge of a lower bound \(_{}\) of \(_{}()\). The knowledge of \(_{}\) is for simplicity only as one can obtain such a lower bound that is at least half of the true value \(_{}()\) via an efficient sampling procedure that we describe in Appendix K.

```
Input \(,,,,,_{E},M,L_{}\) Define \((,Z,,):=_{w,w^{}} w-w^{}_{(Z,)^{-1}}\|\|_{2} }\) Initialize \(=1\), \(N_{0,0}=0\)\(\) doubling trick initialization if\(=\)then while\(=1\) or \(,Z_{0,},^{},_{} >1\)do  get \(2^{-1}r()}\) samples denoted as \(\{Z_{0,},X_{0,},Y_{0,}\}\) per design \(_{E}\)\(\) via ROUND Update \(^{}\) by OLS on \(\{Z_{0,},X_{0,}\},+1\) endwhile else \(=_{()}_{w,w^{} }f(w,w^{},,)\) \(N^{}= 4gdM(1+2Md+L_{z}^{2}+2M2gdM)+8M (}{}) r()\) while\(=1\) or \(,Z_{0,} Z_{1,},,_{} >\)do \(N_{1,}=2^{}N^{}\)\(\) doubling trick update get \(N_{1,}\) samples per \(\) denoted as \(\{Z_{1,},X_{1,},Y_{1,}\}\)\(\) via ROUND \(N_{0,}= 2gdM(Md+N_{1,}+L_{z}^{2})+4M (}{_{}}) r() {2}{_{0}}\)  get \((N_{0,}-N_{0,-1})\) samples per \(_{E}\) augmented to \(\{Z_{0,-1},X_{0,-1}\}\) and get \(\{Z_{0,},X_{0,}\}\) Update \(^{}\) by OLS on \(\{Z_{0,} Z_{1,},X_{0,} X_{1,}\}\), \(+1\) endwhile Output:\(^{}\)
```

**Algorithm 4**\(\) - estimator

**Experiments.** We provide experiments for the instance of Section 1.2 in the Appendix E. The experiments show that our approach is more sample efficient than natural passive baselines (e.g. A/B testing), or naively applying existing Pure-Exploration linear bandit methods and performs similarly to the oracle complexity.

## 4 Conclusion

This work introduces the CPET-LB problem in which the learning protocol is characterized by a linear structural equation model governed by parameters \(\) and \(\). We provide a general solution that simultaneously estimates the structural model while optimally designing to learn the best-arm. The key ideas behind our approach are based on linear experimental design techniques, an instrumental variable estimator whose variance can be controlled by the design, and novel finite-time confidence intervals on this estimator. This paper presents a number of directions for future work including considering situations where the \(d_{z} d_{x}\), analysis to improve the dependence on the underlying noise variance, and the pursuit of a tight information-theoretic instance-dependent lower-bound. We hope that this line of work motivates increased discussion of the real impact of confounding on applicability of adaptive experimentation.