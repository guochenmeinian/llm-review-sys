# InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling

Yuchun Miao\({}^{1}\), Sen Zhang\({}^{2}\), Liang Ding\({}^{2}\), Rong Bao\({}^{3}\), Lefei Zhang\({}^{1}\), Dacheng Tao\({}^{4}\)

\({}^{1}\) National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University

\({}^{2}\) The University of Sydney \({}^{3}\) Fudan University \({}^{4}\) Nanyang Technological University

Correspondence to Lefei Zhang <zhanglefei@whu.edu.cn>

###### Abstract

Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, _reward hacking_, also termed _reward overoptimization_, remains a critical challenge. This issue primarily arises from _reward misgeneralization_, where reward models (RMs) compute reward using spurious features that are irrelevant to human preferences. In this work, we tackle this problem from an information-theoretic perspective and propose a framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information. Notably, we further identify a correlation between overoptimization and outliers in the IB latent space of InfoRM, establishing it as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Cluster Separation Index (CSI), which quantifies deviations in the IB latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies. Extensive experiments on a wide range of settings and RM scales (70M, 440M, 1.4B, and 7B) demonstrate the effectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimization detection mechanism is not only effective but also robust across a broad range of datasets, signifying a notable advancement in the field of RLHF. Code is available at: https://github.com/miaoyuchun/InfoRM.

## 1 Introduction

With the advent of large language models (LLMs), reinforcement learning from human feedback (RLHF) has emerged as a pivotal technological paradigm to align models' behaviors with human values . One of the core stages of RLHF is reward modeling, where a proxy reward model (RM) is learned to mimic human preference by training on a preference dataset that contains sets of responses with human rankings. Then a reinforcement learning (RL) stage follows to align the LLM with human preferences by optimizing rewards from the learned proxy RM. Despite empirical success, RLHF has been criticized for its vulnerability and instability . One widely revealed cause is _reward hacking_, also known as _reward overoptimization_, a phenomenon where the policy model's optimization, though seemingly effective under the proxy RM, actually diverges from the true human objectives . This issue can be manifested in various ways, from copying styles without generating meaningful content to exhibiting excessive caution in responses .

One primary cause of reward overoptimization in the reward modeling process is _reward misgeneralization_, where RMs may incorrectly generalize training data, resulting in poor proxies for actual human preference. This problem arises because the same set of human feedback can be interpreted in multiple ways by RMs, even when ample training data is available . Consequently, RMs tend to depend on spurious features--those unexpected or contingent elements that correlate with the rankinglabels but are irrelevant to actual human preferences, such as length bias . Over-exploiting such information results in RM overfitting, which significantly undermines its generalizability and poses a notable challenge for RM in handling the dynamic response distribution during the RL stage, leading to an unstable RL process .

Current efforts in mitigating reward overoptimization mainly include incorporating Kullback-Leibler (KL) divergence as constraints , enlarging the scale of RM , employing composite RMs , optimizing preference dataset , and specifically addressing response length bias . However, none of these approaches take the aforementioned _reward misgeneralization_ issue into account.

In this work, we propose a new reward modeling framework from an information-theoretic perspective, namely, InfoRM, which effectively addresses the aforementioned _reward misgeneralization_ issue. InfoRM takes inspiration from the recent advancements in deep variational inference and mutual information (MI)-based learning theory . Specifically, we translate the reward modeling problem into optimizing a variational information bottleneck (IB) objective function. This approach aims to filter out information irrelevant to human preferences from the IB latent representation, which acts as a crucial intermediary between the RM outputs and the corresponding human preferences; please see Figure 1 for comparison between standard RM and InfoRM.

The advantages of our framework are two-fold: **Firstly**, benefiting from the MI modeling, InfoRM eliminates human preference-irrelevant information from the IB latent representation to achieve generalizable human preference modeling. This approach directly addresses the _reward misgeneralization_ challenge by ensuring that only pertinent features that genuinely reflect human preferences are retained within the IB latent space. Supporting experiments are detailed in Appendix D. **Secondly**, InfoRM also stands out for its potential in _overoptimization detection_. In particular, we discover a correlation between reward overoptimization and the emergence of numerous outliers in the latent IB space of InfoRM, a phenomenon not observed in RM without IB. Motivated by this observation, we design the Cluster Separation Index (CSI) as an indicator of reward overoptimization, which identifies such outliers by quantifying the deviations of RLHF model-generated sample distributions; please see Section 5 for experimental validation. The proposed CSI not only facilitates parameter adjustments in InfoRM within real-world scenarios when lacking the gold RM but also provides an informative tool for online mitigation strategies such as early stopping; see Appendix E.2 and G.

Building on these advantages, our method mitigates the risk of reward overoptimization in RLHF, resulting in enhanced RLHF performance, as illustrated in Figure 2. We summarize our main contributions as follows:

Figure 1: Comparison between standard RM and our information-theoretic reward model (InfoRM). InfoRM distinguishes itself by enhancing RM generalizability through mutual information modeling. Additionally, a distinct feature of InfoRM is its overoptimization detection mechanism, which can guide parameter selection and algorithm design in subsequent RLHF. Specifically, the RM encoder is derived from the standard RM, with modification to the final layer.

Figure 2: Response comparison on Anthropic-Helpful between RLHF models using our InfoRM and other baselines, assessed by GPT-4, demonstrating the superior performance of our method.

\(\) We introduce InfoRM, a new reward modeling framework based on information theory principles, to tackle the _reward misgeneralization_ challenges by bottlenecking the irrelevant information.

\(\) We propose CSI, an effective indicator for _reward overoptimization detection_, derived from our insight into the correlation between overoptimization and outliers in the IB latent space of InfoRM.

\(\) We empirically demonstrate that InfoRM significantly outperforms standard RMs in RLHF performance, particularly in mitigating reward hacking. Furthermore, our metric for detecting reward overoptimization has proven both effective and robust, marking a significant advancement in RLHF.

## 2 Related Work

Our work draws inspiration from two lines of research, i.e., reward overoptimization in RLHF and information bottleneck-family methods.

### Reward Overoptimization in RLHF

Reward hacking, also termed reward overoptimization, presents a prominent challenge in RLHF, stemming from the limitations of imperfect proxy RM for human preference . In practice, optimizing a learned proxy RM typically results in improvements according to this proxy. However, it only enhances performance in line with the gold RM--actual human preference--for an initial period, after which the performance often starts to deteriorate; please see Figure 3 for an illustration.

To mitigate this issue, a widely adopted strategy is introducing KL divergence penalty to regulate the output deviation of the policy model from the supervised fine-tuning (SFT) model . Although this strategy occasionally works in alleviating reward overoptimization, it inherently restricts the optimization landscape and is prone to overfitting , resulting in degraded RLHF performance . Alternatively, enlarging RM scale , implementing RM ensembles , and composing RMs from multiple perspectives , have been explored to address this issue. Scaling up network size or quantity, as proposed by these approaches, presents limited feasibility and may incur significant costs, especially for models with billions of parameters . Moreover, recent efforts to optimize RM training datasets , and address the specific issue, i.e., response length bias , continue to overlook the human preference-irrelevant information in reward modeling, which perpetuates the issue of _reward misgeneralization_.

Our approach is distinct from existing methods by specifically targeting the underlying challenge of _reward misgeneralization_--a fundamental driver of reward overoptimization. Consequently, our InfoRM, not only significantly reduces reward overoptimization via a single RM, but offers a valuable tool for detecting this phenomenon during RL stage, which facilitates parameter selection in real scenarios without gold RM and development of online mitigation strategies, such as early stopping.

### Information Bottleneck-Family Methods

Information bottleneck (IB) is a well-established technique for learning an informative and compact latent representation as a balance between the conciseness and predictive power . To address the challenge of optimizing the corresponding mutual information, Alemi et al.  presents a variational approximation to the IB objective. This paradigm has successfully extended to various scenarios . Inspired by these works, we introduce the IB principle into reward modeling in RLHF and derive an optimizable variational bound for this ranking problem. Notably, while the aforementioned methods primarily use IB for extracting target-related information, our work makes a step forward by further exploring the informative and compact nature of the learned IB latent representation space, leading to the development of a tool for detecting reward overoptimization. To the best of our knowledge, this is the first effort to connect IB with RLHF and demonstrate its effectiveness in the context of LLM.

Figure 3: An example of reward overoptimization in RLHF characterized by a declining gold score (i.e., actual human preference) and a rising proxy score (i.e., proxy RM preference).

## 3 Methodology

### Preliminary

Reward modeling aims to learn a proxy RM that mimics the underlying human objective, providing the human preference rankings \(y\) of response sets from human preference datasets where each sample is denoted as \(=(^{w},^{l})\). Here, \(^{w}\) and \(^{l}\) denote the chosen and rejected samples, respectively.2 Following Bradley-Terry Model , by employing the learned proxy RM \(r_{}()\), the preference distribution \(p_{}(y)=p_{}(^{w}^{l})\) can be formulated as:

\[p_{}(^{w}^{l})=}(^{w}))}{(r_{}(^{w}))+(r_{}(^{l}))},\] (1)

where \(r_{}()\) represents the learned proxy RM and \(\) collects the model parameters. Standard reward modeling approaches typically regard this problem as a binary classification task and optimize a negative log-likelihood loss [44; 49; 4]:

\[_{}=-_{(^{w},^{l}) }[(r_{}(^{w}) -r_{}(^{l}))],\] (2)

where \(=\{(_{i},y_{i})\}_{i=1}^{N}=\{(_{i}^{w},_{i}^{l} )\}_{i=1}^{N}\) is the human preference dataset,3 and \(()\) is the logistic function. Within the domain of LLM, the proxy RM is commonly initialized with the SFT model. Subsequently, it integrates an extra linear layer at the final transformer layer, producing a single scalar prediction for the reward value. Nonetheless, as discussed in Section 1, this paradigm is prone to _reward misgeneralization_ during the training process, focusing too much on the trivial aspects of training samples while neglecting meaningful information relevant to human preferences. As a result, although the model may exhibit exceptional performance on training data, it tends to struggle with generalizing to unseen data. This limited generalizability of RM leads to the reward overoptimization phenomenon, a critical concern in the subsequent RL process, which necessitates the generalizability of RM to the constantly evolving sample distributions.

### Information-Theoretic Reward Modeling

Addressing the challenge of _reward misgeneralization_ necessitates the capacity of RM to efficiently capture information pertinent to human preferences while discarding the irrelevant details, which aids in preventing overfitting to the human preferences-irrelevant information present in the training samples, thereby significantly enhancing model generalizability .

To this end, we tackle these challenges by reformulating the reward modeling process from an information theoretic perspective. Specifically, we quantify the human preference irrelevance and the utility of a latent representation for reward prediction in information-theoretic language. We first denote the random variables corresponding to RM input, the latent representation, and the human preference ranking as \(\), \(\), and \(Y\), respectively.4 By assuming a Gaussian distribution for the latent representation \(\), we define \(I_{}=I(;|Y)\) and \(I_{}=I(;Y)\) to provide quantitative measures for _the irrelevance of human preferences in latent representation_ and _the utility of latent representation for reward prediction_ respectively, where \(I\) denotes the MI. Therefore, the objective of our information-theoretic reward modeling framework \(J()\) can be formulated as follows:

\[_{}\;J()=_{}\;I_{}-  I_{}=_{}\;I(;Y)- I(; |Y),\] (3)

where \(\) is a trade-off parameter, and \(\) encompasses all the parameters in this objective. In Eqn. (3), the latent representation \(\) essentially provides an information bottleneck between the input samples \(\) and the corresponding rankings \(Y\). Due to the high dimensionality of the input sample space, it is non-trivial to evaluate these two MI. Thus, given a human preference dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\) and \(=\{,\}\), we instead optimize a variational lower bound \(J_{}\):

\[J(,)  J_{}(,)=_{(,y) }[J_{}- J_{}]\] (4) \[J_{} = p_{}(|) q_{}(y|)d\] \[J_{} =[p_{}(|),r()],\]where \(r()\), \(J_{}\), and \(J_{}\) denote the variational approximation of the marginal distribution \(p()\),5 the lower bound of \(I_{}\), and the upper bound of \(I_{}\), respectively. Here, \(p_{}(|)\) extract latent representations, and \(q_{}(y|)\) handles ranking prediction based on the generated representation. The parameters of these two functions are collected in \(\) and \(\), respectively.

In our practice, the functions \(p_{}(|)\) and \(q_{}(y|)\) are modeled by an LLM with an extra head \(f_{}()\) for representation generation, and an MLP \(g_{}()\) for reward prediction, respectively. Notably, \(p_{}(|)\) is modeled as a multivariate Gaussian with a diagonal covariance structure, where the mean and covariance are both determined by the output of the encoder \(f_{}()\), i.e., \(f_{}^{}()\) and \(f_{}^{}()\). Referring to Eqn. (4), the objective for our information-theoretic reward modeling reads:

\[&_{\{,\}}J_{}(,)_{\{,\}}_{(^{w}, ^{l})}[L_{}- L_{}]\\ & L_{}=(g_{}(h_{}(^ {w},^{w}))-g_{}(h_{}(^{l},^{l}))) \\ & L_{}=[p_{}(|^{w} ),r()]+[p_{}(|^{l}),r()],\] (5)

where \(h_{}(,)=f_{}^{}+f_{}^{}( {x})\). \(^{w}\) and \(^{l}\) are independently sampled from \((,)\) for each input sample. \(L_{}\) and \(L_{}\) are the estimates of \(J_{}\) and \(J_{}\) in Eqn. (4), respectively. Detailed derivation is provided in Appendix A, and related pseudocode is provided in Appendix J.1.

**Remark I:** Although InfoRM focuses on reward modeling, our ultimate goal is to mitigate reward overoptimization in RLHF by addressing the reward misgeneralization issue. Thus in subsequent experiments, we evaluate RLHF model performance to demonstrate the effectiveness of InfoRM.

## 4 Experiments in Reward Optimization Mitigation

In this section, we first validate InfoRM's efficacy through simulation experiments with access to the gold RM, allowing us to clearly observe its impact on mitigating overoptimization. We then proceed to real-world scenarios without a gold RM to further verify our approach's effectiveness.

### Simulation Experiments

Our simulation experiments follow [16; 10], where a fixed gold RM plays the human role, providing labels (i.e., rankings) to train a proxy RM. This setup enables to intuitively assess RLHF performance and observe overoptimization, which is unavailable in real-world settings.

#### 4.1.1 Setup

**Models.** In our simulations, we use the Pythia suite  for both the policy model and the proxy RM. Specifically, the 1.4B Pythia model serves as the universal policy model utilized everywhere. For the proxy RM, we remove the embedding layers from Pythia models sized 70M, 410M, and 1.4B, adding an MLP head to output a scalar reward. Moreover, the gold RM, based on Vicuna-7B-v1.5 , follows the RM training protocol in AlpacaFarm . Considering Vicuna's size of 7B--much than our maximum proxy RM size of 1.4B--it is reasonable to employ it as the gold RM .

Figure 4: Simulated RLHF results for different proxy RMs (1.4B). Solid and dashed lines represent the gold and proxy scores, respectively. In later RL stages, as KL divergence increases, Standard RM shows a declining gold score and a rising proxy score, indicating overoptimization. Conversely, our InfoRM maintains consistent growth in both scores, effectively mitigating overoptimization.

**Pipeline.** Our RLHF pipeline in the simulation experiments follows , consisting of several key stages. Initially, both the policy model SFT and the gold RM training are performed on AlpacaFarm . Next, a simulated preference dataset for proxy RM training is generated by prompting the SFT model with instructions to produce two different responses, which are then ranked by the gold RM. In line with , we simulate the scenario of high disagreement rates among human annotators by intentionally mislabeling 25% of this dataset, leading to two versions: one w/ and one w/o label noise. The proxy RM is then trained on these datasets. Finally, policy optimization is conducted using the PPO algorithm ; please see Appendix J.3 for more implementation details.

**Data.** Following , the training data in our simulation experiments are from AlpacaFarm . In particular, 10k instruction demonstrations are utilized for the policy model SFT and 20k preference data is used for gold RM training. In addition, the instructions of the 20k preference data are used for response generation via the SFT model, which is then labeled by the gold RM. The remaining 20k unlabeled data in AlpacaFarm are used for policy optimization. It's important to note that all training data in our simulation experiments is sourced exclusively from the AlpacaFarm dataset , ensuring consistency of the training data distribution across three stages.

**Baselines.** Our baseline models include Supervised Fine-Tuning model (SFT), RLHF model using standard RM (Standard RM), RLHF model using standard RM with KL divergence penalty (Standard RM w/ KL) , and the RLHF model using ensemble RM (Ensemble RM) .6

#### 4.1.2 Main Results

Figure 4 presents the simulated RLHF results for different 1.4B proxy RM w/ and w/o label noise. InfoRM consistently prevents reward overoptimization and substantially enhances RLHF performance under both noisy and noiseless scenarios. Notably, Standard RM's stability is significantly compromised with the label noise, leading to notable reward overoptimization. In contrast, InfoRM maintains stability regardless of label noise, underscoring InfoRM's ability to extract human preference-relevant information from noisy data to improve the resilience of proxy RMs.

Previous research  demonstrates that increasing the RM size enhances the performance during the RL stage, as measured by the gold RM. In Figure 5 (left), we assess the impact of varying proxy RM sizes on the final RLHF performance measured by the gold RM.7 Our findings include: (1) **Information-theoretic reward modeling significantly improves performance beyond merely enlarging the RM size**, making InfoRM a cost-effective and practical solution for deployment without additional computational costs. (2) **InfoRM performance consistently improves as the RM size increases**, suggesting our method's benefits are complementary to those from scaling the RM.

To assess InfoRM's generalizability, we conduct experiments using both in-distribution (AlpacaFarm) and out-of-distribution (Flan) datasets in the RL stage. The results, shown in Figure 5 (right), demonstrate that InfoRM maintains relatively stable performance on the out-of-distribution Flan

Figure 5: Final gold rewards in simulated RLHF experiments. **Left:** Using proxy RMs with varying parameter sizes. **Right:** Conducting RL on Alpaca (in-distribution) and Flan (out-of-distribution). The proxy RMs are all trained on the same simulated preference dataset with 25% label noise.

[MISSING_PAGE_EMPTY:7]

Therefore, we employ GPT-4 to evaluate the performance of our method and the baselines. The GPT-4 prompt used in our study is the one with the highest human agreement in AlpacaEval ; please see Appendix J.4 for the detailed prompt. To eliminate the position bias [46; 11], each pair of samples is assessed twice, with the order of responses reversed in each instance.

#### 4.2.2 Main Results

Table 1 compares the win, tie, and lose ratios under GPT-4 evaluation for our method versus other baselines. Key findings include: (1) **Our InfoRM significantly outperforms Standard RM without a KL divergence penalty** due to its vulnerability to spurious features within training samples and distribution shifts in RL process, leading to severe reward overoptimization. Our InfoRM leverages IB theory to enhance model generalizability, as evidenced in Section 4.1, thus remarkably reducing overoptimization. (2) **Our InfoRM continues to surpass Standard RM w/ KL**, despite the introduced KL divergence noticeably improving its RLHF performance. We conjecture that the KL penalty, though stabilizing RL, may restrict the optimization landscape of the policy model, thereby affecting RL effectiveness; please see Appendix E.2 for parameter sensitivity analysis in such a real scenario. (3) **InfoRM is a versatile and foundational framework that integrates seamlessly with other techniques to provide complementary benefits**. InfoRM not only outperforms Ensemble RM and WARM in RLHF performance but also enhances results when combined with these methods.

## 5 Detecting Overoptimization: Additional Strength of OurInfoRM

It is noteworthy that our InfoRM not only filters irrelevant information to human preference, thereby significantly enhancing the performance of RLHF, but also benefits from a highly informative and compact IB latent space, facilitating the establishment of a detection mechanism for reward overoptimization through latent representations. The capacity of our overoptimization detection mechanism hinges on two pivotal points: (1) Overoptimized samples manifest as outliers in the IB latent space of InfoRM. (2) The emergence of these outliers is quantitatively signaled by our proposed indicator.

### Outlier Behavior of Overoptimizaed Samples in IB Latent Space

To examine the relationship between outliers in the latent IB space of InfoRM and the overoptimized samples in the RL process, the identification of overoptimized samples is highly challenging and under-explored. To address this issue, we pioneer the use of AI feedback, such as GPT-4, to identify overoptimized samples. Specifically, drawing upon the insights from [10; 51], we first summarize

Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. **From top to bottom:** The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. **From left to right:** The RMs applied in RLHF are Standard RM and InfoRM, respectively. _Observations: (1) Outliers in the IB latent space of_ InfoRM _usually signify overoptimized samples. (2) Using_ InfoRM _significantly reduces the emergence of overoptimized samples._

common overoptimization behaviors, including excessive caution, responses that deviate from user intent, and the generation of a large volume of repetitive and meaningless text. Based on this, we then design guidelines for GPT-4 to assess whether an RLHF model response is overoptimized. Detailed prompt designs are provided in Appendix J.4.

Figure 7 provides a t-SNE visualization of the response distributions in the latent IB space of InfoRM before and after RLHF, as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. Our key conclusions include: (1) From the left column, **outliers in the IB latent space are generally indicative of overoptimized samples**, supported by the observation that most overoptimized samples significantly deviate from the distribution of samples before RLHF (depicted as blue points). (2) By comparing the left and right columns, it becomes evident that **the incorporation of InfoRM leads to a substantial reduction in the number of outliers after RLHF, effectively preventing the appearance of overoptimized samples.** This observation aligns seamlessly with the superior performance of InfoRM, as demonstrated in both simulated and real-world experiments. Appendix C.1 presents a more comprehensive validation of these observations, and related parameter sensitivity analysis in Appendix E.1 demonstrates their robustness.

### Detection of Outlier Emergencies and Overoptimization by the CSI Indicator

Based on the above observation, we design a detection metric for reward overoptimization, namely, Cluster Separation Index (CSI), by quantifying the deviations in the latent IB space of InfoRM. The computation process of CSI is elaborated as follows:

\(\)_Step 1:_ Perform clustering on the RLHF model outputs within the latent space of our InfoRM. Denote the clusters as \(C=\{C_{1},C_{2},...,C_{n}\}\), where \(C_{i}\) represents the \(i\)-th cluster, and \(n\) is the total number of clusters. For each \(C_{i}\), compute the geometric centroid \(_{i}\) by

\[_{i}=|}_{ C_{i}},\] (6)

where \(|C_{i}|\) denotes the count of points in \(C_{i}\) and \(\) represents the points within \(C_{i}\).

\(\)_Step 2:_ For each cluster centroid \(_{i}\) from Step 1, identify its nearest SFT model output. Calculate the Euclidean distance \(d_{i}\) between each centroid \(_{i}\) and its nearest SFT output as:

\[d_{i}=_{ S}\|_{i}-\|,\] (7)

where \(S\) represents all SFT outputs and \(\|\|\) indicates Euclidean distance.

\(\)_Step 3:_ CSI is calculated as the sum of weighted distances by the number of the elements in each cluster:

\[=_{i=1}^{n}|C_{i}| d_{i}.\] (8)

In this work, we utilize DBSCAN  as the clustering algorithm due to its robust empirical performance and ability to operate without a predetermined number of clusters. The pseudocode of CSI calculation is provided in Appendix J.2 for better understanding.

Figure 8 compares CSI values during RLHF with Standard RM and InfoRM. As observed, between 600 - 700 training steps, there is a sudden and substantial increase in the CSI values of Standard RM, which then persist at the highly-elevated level in subsequent steps. This abrupt change corresponds to the outlier emergence in latent space, as highlighted by the green and red boxes in Figure 8. This indicates that the **proposed CSI is highly sensitive to the emergence of outliers, thus offering timely and accurate detection of reward overoptimization**. Furthermore, the RLHF process with InfoRM consistently exhibits much lower CSI values, suggesting that InfoRM can significantly mitigate the reward overoptimization phenomenon,

Figure 8: CSI values in the RLHF processes of Standard RM and InfoRM across the training steps on Anthropic-Helpful dataset.

aligning with our previous experimental findings. Further validations of our CSI's performance on various datasets are presented in Appendix C.2.

**Remark II:** Our overoptimization detection mechanism is closely tied to InfoRM's compact IB latent space. Other RMs without IB, showing weak correlations between latent space outliers and overoptimized samples, are incompatible with this mechanism; see Appendix F for related evidence.

**Remark III:** Our overoptimization detection mechanism enhances RLHF performance in three ways. First, it facilitates parameter adjustments in InfoRM for real-world scenarios; please see Appendix E.2 for an example. Additionally, it serves as a model-based metric for overoptimization detection as verified in Appendix C.2, thus guiding the optimization of any reward model during the RLHF process, including dataset selection and algorithm design. Finally, it provides a tool for online mitigation strategies like early stopping, helping to prevent overfitting and maintain model integrity. The automated early-stopping algorithm based on our CSI is elaborated in Appendix G.

## 6 Conclusion

In this study, we introduce InfoRM, a novel framework designed to mitigate reward overoptimization in RLHF by applying information-theoretic principles to reward modeling. Unlike existing methods that focus on implementing KL divergence constraints, expanding reward model scales, and addressing specific issues like length biases, InfoRM directly addresses the primary cause of reward overoptimization in reward modeling, i.e., _reward misgeneralization_, by incorporating a variational information bottleneck objective. Our RM effectively filters out information irrelevant to human preferences, ensuring only key features reflecting human values are retained. Additionally, InfoRM features CSI, a quantitative indicator from the latent IB space for detecting reward overoptimization. Experiments across various scenarios and model sizes have demonstrated InfoRM's significant effectiveness in mitigating reward overoptimization. We also empirically validate CSI's effectiveness in detecting reward overoptimization on a wide range of datasets, offering valuable guidance for future research in RLHF algorithm design, and developing online overoptimization mitigation strategies.

## Broader Impacts

In reinforcement learning from human feedback, reward hacking or overoptimization occurs when the policy model's optimization diverges from true human objectives, reducing the helpfulness of large language models, from generating meaningful content to displaying excessive caution. This work introduces the information bottleneck into reward modeling, significantly reducing reward overoptimization. Additionally, we propose an indicator to support online mitigation strategies, aiming to better align large models with human preferences. Our study is ethical and poses no adverse effects on society.

## Limitations

Our study presents several avenues for future research. Firstly, while our evaluation includes models up to 7 billion parameters, scaling our InfoRM framework to state-of-the-art models that are orders of magnitude larger remains an exciting and unexplored direction. Furthermore, our over-optimization monitoring mechanism exhibits some latency and requires inference on test datasets, highlighting the need for the development of real-time, lightweight over-optimization detection metrics. Such metrics are crucial for enhancing the effectiveness of Reinforcement Learning from Human Feedback (RLHF). Regarding evaluations, we also observe that the win rates computed by GPT-4 are influenced by the prompt structure. Future investigations could focus on identifying optimal ways to elicit high-quality judgments from automated systems, ensuring more reliable and consistent results.