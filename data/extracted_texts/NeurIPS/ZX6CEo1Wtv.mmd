# Latent Diffusion for Neural Spiking Data

Jaivardhan Kapoor1

Auguste Schulz1

Julius Vetter1

Felix Pei1

Richard Gao1

Jakob H. Macke1,2

###### Abstract

Modern datasets in neuroscience enable unprecedented inquiries into the relationship between complex behaviors and the activity of many simultaneously recorded neurons. While latent variable models can successfully extract low-dimensional embeddings from such recordings, using them to generate realistic spiking data, especially in a behavior-dependent manner, still poses a challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS), a diffusion-based generative model with a low-dimensional latent space: LDNS employs an autoencoder with structured state-space (S4) layers to project discrete high-dimensional spiking data into continuous time-aligned latents. On these inferred latents, we train expressive (conditional) diffusion models, enabling us to sample neural activity with realistic single-neuron and population spiking statistics. We validate LDNS on synthetic data, accurately recovering latent structure, firing rates, and spiking statistics. Next, we demonstrate its flexibility by generating variable-length data that mimics human cortical activity during attempted speech. We show how to equip LDNS with an expressive observation model that accounts for single-neuron dynamics not mediated by the latent state, further increasing the realism of generated samples. Finally, conditional LDNS trained on motor cortical activity during diverse reaching behaviors can generate realistic spiking data given reach direction or unseen reach trajectories. In summary, LDNS simultaneously enables inference of low-dimensional latents and realistic conditional generation of neural spiking datasets, opening up further possibilities for simulating experimentally testable hypotheses.

Machine Learning in Science, University of Tubingen & Tubingen AI Center, Tubingen, Germany

Department Empirical Inference, Max Planck Institute for Intelligent Systems, Tubingen, Germany

Equal contribution, order determined by a coin toss.

Equal supervision.

{firstname.lastname@uni-tuebingen.de}

## 1 Introduction

Modern datasets in neuroscience are becoming increasingly high-dimensional with fast-paced innovations in measurement technology , granting access to hundreds to thousands of simultaneously recorded neurons. At the same time, the types of animal behaviors and sensory stimuli under investigation have become more naturalistic and complex, resulting in experimental setups with heterogeneous trials of varying length, or lacking trial structure altogether . Therefore, a key target in systems neuroscience has shifted towards understanding the relationship between high-dimensional neural activity and complex behaviors.

For high-dimensional neural recordings, analyses that infer low-dimensional structures have been very useful for making sense of such data . For example, latent variable models (LVMs) are often used to identify neural population dynamics not apparent at the level of single neurons . More recently, deep learning-based approaches based on variational autoencoders (VAEs)  have become particularly popular for inferring latent neural representations due to their expressiveness and ability to scale to large, heterogeneous neural recordings with behavioral covariates .

However, in addition to learning latent representations, another important consideration is the ability to act as faithful generative models of the data. In other words, models should be able to produce diverse, realistic samples of the neural activity they were trained on, ideally in a behavior- or stimulus-dependent manner. Models with such capabilities not only afford better interpretability analyses and diagnoses for whether structures underlying the data are accurately learned, but have a variety of downstream applications surrounding the design of closed-loop _in silico_ experiments. For example, with faithful generative models, one can simulate population responses to hypothetical sensory, electrical, or optogenetic stimuli, as well as possible neural activity underlying hypothetical movement patterns. Most VAE-based approaches focus on the interpretability of the inferred latents, but not the ability to generate realistic and diverse samples when conditioning on external covariates, while sample-realistic models (e.g., based on generative adversarial networks (GANs) ) do not provide access to underlying low-dimensional representations. As such, there is a need for models of neural population spiking activity that both provide low-dimensional latent representations _and_ can (conditionally) generate realistic neural activity.

Here, we propose **Latent Diffusion for Neural Spiking data (LDNS)**, which combines the ability of autoencoders to extract low-dimensional representations of discrete neural population activity, with the ability of (conditional) denoising diffusion probabilistic models (or, diffusion models) to generate realistic neural spiking data by modeling the inferred low-dimensional continuous representations.

Diffusion models  have been highly successful for conditional and unconditional data generation in several domains, including images , molecules , and audio spectrograms  and have demonstrated sampling-fidelity that outperforms that of VAEs and GANs . A key strength of diffusion models that makes them particularly attractive in the context of modeling neural datasets is the ability to flexibly condition the generation on various (potentially complex) covariates, such as to simulate neural activity given certain behaviors. Recently, diffusion models have been extended to continuous neural time series such as local field potentials (LFPs) and electroencephalography (EEG) recordings . However, due to the discrete nature of spiking data, standard diffusion models cannot be easily applied, thus excluding their use on many datasets in systems neuroscience.

To bypass these limitations, LDNS employs a regularized autoencoder using structured state-space (S4) layers  to project the high-dimensional discrete spiking data into smooth, low-dimensional latents without making assumptions about the trial structure. We then train a diffusion model with S4 layers as a generative model of the inferred latents--akin to latent diffusion for images , where generation can be flexibly conditioned on behavioral covariates or task conditions.

A fundamental assumption of most low-dimensional latent variable models is that all statistical dependencies between observations are mediated by the latent space. However, in neural spiking data, there are prominent statistical dependencies that ought to persist _conditional_ on the latent state, e.g., single-neuron dynamics such as refractory periods, burstiness, firing rate adaptation, or potential direct synaptic interactions. We show how such additional structure can be accounted for in LDNS, by equipping it with an expressive observation model : We use a Poisson model for spike generation with autoregressive couplings which are optimized post hoc to capture the temporal structure of single-neuron activity. This allows LDNS to capture a wide range of biological neural dynamics , with only a small additional computational cost.

Figure 1: **Latent Diffusion for Neural Spiking data.** LDNS allows for (un)conditional generation of neural spiking data through combining a regularized autoencoder with diffusion models that act on the low-dimensional latent time series underlying neural population activity.

Main contributionsIn summary, LDNS is a flexible method that allows for both high-fidelity diffusion-based sampling of neural population activity and access to time-aligned low-dimensional representations, which we validate on a synthetic dataset. Next, we show the utility and flexibility of this approach on complex real datasets: First, LDNS can handle variable-length spiking recordings from the human cortex. Second, LDNS can unconditionally generate faithful neural spiking activity recorded from monkeys performing a reach task. We demonstrate how LDNS can be equipped with an expressive autoregressive observation model that accounts for additional dependencies between data points (e.g., single neuron dynamics), increasing the realism of generated samples. Third, LDNS can generate realistic neural activity while conditioning on either reach direction or full reach trajectories (time series), including unseen behaviors that are then accurately decoded from the simulated neural data. Overall, LDNS enables simultaneous inference of low-dimensional latent representations for single-trial data interpretation and high-fidelity diffusion-based (conditional) generation of diverse neural spiking datasets, which will allow for closed-loop _in silico_ experiments and hypothesis testing.

## 2 Methods

### Latent Diffusion for Neural Spiking Data (LDNS)

We consider a dataset recorded from a population of \(n\) neurons, consisting of trials with spiking data \(_{0}^{n T}\) (sorted into bins of fixed length resulting in spike counts over time), and optional simultaneously recorded behavioral covariates \(^{n}\) (that can also be time-varying \(^{n T}\)). A dataset of \(M\) such trials \(\) can be written as \(=\{^{(i)},^{(i)}\}\), possibly with varying trial lengths \(T_{1} T_{M}\). We make the assumption that a large fraction of the variability in this dataset can be captured with a few underlying latent variables \(^{d T}\), where \(d<n\).

Our goal is to generate realistic spiking data \(^{*}\) that faithfully capture both population-level and single-neuron dynamics of \(_{1 T}\) with the ability to optionally condition the generation on behavior \(_{}\). To this end, we propose a new method, LDNS, that combines the strength of neural dimensionality reduction approaches with that of diffusion-based generation.

LDNS uses a two-stage training framework, adopted from the highly successful family of latent diffusion models (LDMs) [45; 8; 59]. To train LDNS, we first train a regularized autoencoder  to compress the spiking data into a low-dimensional continuous latent space (Fig. 1). Concretely, we focus on two objects of interest for the LDNS autoencoder: **(1)** inferring a time-aligned, low-dimensional smooth representation \(^{d T}\) that preserves the shared variability of the spiking data, and **(2)** predicting smooth firing rates \(\) that are most likely to give rise to the observed spiking data.

In the second stage, we train a diffusion model in latent space, possibly employing _conditioning_ to make generation contingent on external (e.g., behavioral) covariates (Fig. 1). For the diffusion model, our main objective is the generation of \(^{*}^{d T}\) that captures the distribution of inferred autoencoder latents. We also want the ability to sample latent trajectories of varying length.

In both stages, we use structured state-space (S4)  layers for modeling temporal dependencies. S4 layers consist of state-space transition matrices that can be unrolled into arbitrary-length convolution kernels, allowing sequence modeling of varying lengths. For details on network architectures and S4 layers, see appendix A1.

### Regularized autoencoder for neural spiking data

For the spiking data, we choose a Poisson observation model, and train autoencoders by minimizing the Poisson negative log-likelihood of the input spikes \(\) given the predicted rates \(=()\). To enforce smoothness in the latent space, where \(=()\), we add an \(L_{2}\) regularization along with a temporal smoothness regularizer over \(\), resulting in the combined loss

\[_{}=_{}[_{t=1 }^{n,T}(_{i}(t)-s_{i}(t)_{i}(t))}+_{1}\|^{2}}_{L_{2 }}+_{2}_{k=1\\ t=k+1}^{K,T}(t)-(t-k) \|^{2}}_{}].\] (1)To prevent the autoencoder from predicting highly localized Poisson rates, which have sharp peaks at input spike locations, we further regularize training using coordinated dropout , i.e., we randomly mask input spikes and compute the loss on the predicted rates at the masked locations (details in appendix A.1.2).

Accounting for single-neuron dynamics with an expressive observation modelSo far, LDNS (like most latent variable models for neural data) uses a Poisson observation model, which assumes that all statistical dependencies are mediated by the latent state. To address this limitation and to capture dynamics and variability, which are "private" to individual neurons (such as refractory periods or burstiness), we propose to learn an autoregressive observation model. We make the predicted Poisson rates for each neuron \(i\) dependent also on recent spiking history, by including additional spike history couplings \(h_{i}\)[41; 32], resulting in the observation model

\[s_{i}(t)(_{i}(t)+h_{i,0}+_{=1}^{T^{}}h_ {i,}s_{i}(t-)),\] (2)

where \(T^{}\) corresponds to the time-lagged window length. This modification is learned post hoc, and the parameters \(h_{i}\) are fit with a maximum-likelihood objective (details in appendix A.1.3). This approach does not alter the latent dynamics, while augmenting the model with single-neuron autoregressive dynamics. We observe that including spike history increases the realism of generated data and enables us to accurately capture single-neuron autocorrelation structures (Sec. 3.4).

### Denoising Diffusion Probabilistic Models

In the second stage of training, we train diffusion models  to generate (conditional) samples from the distribution of inferred latents. The training dataset therefore contains autoencoder-derived latents of each trial, and optionally, additional conditioning information such as the corresponding behavior, i.e., \(_{z}=\{^{(i)}=(^{(i)}), ^{(i)}\}\).

Diffusion models aim to approximate the data distribution \(q()\) through an iterative denoising process starting from standard Gaussian noise. For latent \(\) (denoted as \(_{0}\) for diffusion timestep 0), we first produce a noised version at step \(t\) by adding Gaussian noise of the form \(q(_{t}|_{0})=(_{t}} _{0},(1-_{t})I)\). Here, \(_{t}=_{k=1}^{t}_{k}\), where the noise scaling factors \(_{1}_{T}\) follow a fixed linear schedule. We then train a neural network to approximate the reverse process \(p_{}(_{t-1}|_{t})\) for each diffusion timestep. The true (denoising) reverse transition \(q(_{t-1}|_{t})\) is intractable--however, we can apply variational inference to learn the _conditional_ reverse transition \(q(_{t-1}|_{t},_{0})\), which has a closed form written as

\[q(_{t-1}|_{t},_{0})=(_{t}}(1-_{t-1})}{1-_{t}} _{t}+_{t-1}}(1-_{t})}{1-_{t}} _{0},)(1-_{t-1})}{1-_{t} }I).\] (3)

We train the neural network \(_{}(_{t},t)\) to approximate the mean of this distribution by optimizing the loss \(_{_{0}_{z},_{0},t}\|_{ }(_{t},t)-_{0}\|^{2}\), where \(_{0}\) is the noise used to generate \(_{t}\) from \(_{0}\), and \(_{}(_{t},t)\) is the equivalent reparameterization for \(_{}(_{t},t)\). At test time, we sequentially sample \(_{t-1}\) given \(_{t}\) using the learned transition \(p_{}(_{t-1}|_{t})\), starting from standard Gaussian noise. Using S4 layers in the denoising network allows us to generate latents with varying lengths. This is achieved by unrolling the state transition matrix in the S4 layers to the desired length for each denoising step.

Diffusion models may be conditioned on fixed-length and time-varying covariates \(\), in which case we learn the approximate reverse transition \(p_{}(_{t-1}|_{t},)\). Details on the conditioning mechanisms in appendix A.1.4.

## 3 Experiments and Results

### Datasets and tasks

We first evaluate the performance of LDNS on a synthetic spiking dataset where we have access to the ground-truth firing rates and latents. We choose the Lorenz attractor  as a low-dimensional, non-linear dynamical system commonly used in neuroscience [7; 36]. We simulate rates as an affine mapping from the 3-dimensional system to a 128-dimensional neural space, and sample from a Poisson distribution to generate spiking data. Next, we showcase the applicability of LDNS on two neural datasets: We apply our method on a highly complex dataset of human neural activity (128 units) recorded during attempted speech . This dataset poses a challenge to many modeling approaches due to the different imagined sentences, resulting in variable lengths of the neural time series (between 2-10 seconds with a sampling rate of 50 Hz). Finally, we apply LDNS to model premotor cortical activity (182 units) recorded from monkeys performing a delayed center-out reach task with barriers [9; 38]. The multi-modal nature of the dataset allows us to assess both unconditional as well as conditional generation of neural spiking activity given monkey reach directions and entire velocity profiles of the performed reaches. See appendix A2,A3 for data and training details.

For the unconditional generation of monkey reach recordings (Sec. 3.4), we train both a Poisson observation model as well as a spike history-dependent autoregressive observation model. For all other experiments, we only train a Poisson observation model.

BaselinesWe compare LDNS to the most commonly known VAE-based latent variable model: Latent Factor Analysis via Dynamical Systems (LFADS [51; 36; 47]), which has been shown to outperform various classical latent variable models on a variety of tasks (, details in appendix A4). To ensure that we use optimal hyperparameters for LFADS, we follow the auto-ML pipeline proposed by Keshtkaran et al. . This approach, termed AutoLFADS, has been shown to perform better than the original LFADS on benchmark tasks . For the unconditional generation of monkey reach recordings, we further compared to additional VAE baselines [21; 62] (appendix A5).

MetricsFor all experiments, we assess how well LDNS-generated samples match the spiking data in the training distribution. Concretely, we compare population-level statistics by computing 1) the distribution over the population spike count, which sums up all spikes co-occurring in the population in a single time bin (i.e., spike count histogram), and 2) pairwise correlations of LDNS samples and the spiking data for each pair of neurons. For single-neuron statistics, we compare 3) the mean and 4) standard deviation of the inter-spike-interval distribution for each neuron (mean isi and std isi). When multiple spikes occur in a single time bin, the spike times are distributed equally in this bin . To further evaluate population dynamics, we compare the principal components of smoothed spikes.

### LDNS captures the true spiking data distribution with an underlying Lorenz system

We simulate trials of length 256 timesteps from the three-dimensional (chaotic) Lorenz system (Fig. 2a). The regularized autoencoder extracts smooth latent time series (eight latent dimensions)

Figure 2: **Realistic generation of spiking data with underlying chaotic dynamics.****a)** Synthetic spiking data from an underlying Lorenz system with a Poisson observation model. **b)** Accurate, smooth rate predictions of the autoencoder for held-out spiking data. **c)** Plotted trace of sampled latents (256 bins training length, left) and \(16\) the original training length (middle). The sampled latent distribution matches the PSD of the autoencoder latents (right; median, 10%, and 90% percentiles). **d)** LDNS population spike count histogram (kde: kernel density estimate) and pairwise cross-correlations match the training distribution. **e)** LDNS single neuron statistics, i.e., mean inter-spike interval (isi) and std isi, match the training distribution.

from the 128-dimensional spiking data, resulting in smooth firing rate predictions that closely match the ground-truth rates (Fig. 2b, Supp. Fig. A2,A3). We then train a diffusion model on the extracted autoencoder latents. Latents sampled from the diffusion model (red) preserve the attractor geometry of the Lorenz system (Fig. 2c, left, three of the eight latent dimensions), indicating that LDNS preserves a meaningful latent space. The architectural choice of S4 layers allows for length generalization: although we train on time segments of 256-time steps, we can sample and successfully generate latent trajectories that are much longer, but still accurately reflect the Lorenz dynamics (Fig. 2c, middle, \(16\) longer generation). In comparison, LFADS exhibits instabilities when generating such longer sequences (appendix A6.1). Overall, the latent time series distribution is captured well by the diffusion model, with matching power spectral densities (PSD) per latent dimension (Fig. 2c, right, other dimensions in Supp. Fig. A3).

To assess the sampling fidelity of the generated synthetic neural activity, we compute a variety of spike statistics frequently used in neuroscience. LDNS captures both population-level statistics, such as the population spike count histogram and pairwise correlations between neurons (Fig. 2d), as well as single-neuron statistics, quantified by the mean and standard deviation of inter-spike-intervals (Fig. 2e). LDNS also captures the temporal correlation structure of the data (Supp. Fig. A4). These results demonstrate that LDNS can both perform inference of low-dimensional latents and provide high-fidelity diffusion-based generation that perfectly captures the statistics of the ground-truth synthetic data.

### Modeling variable-length trials of neural activity recorded in human cortex

Next, we assess whether LDNS is capable of capturing real electrophysiological data, applying it to neural recordings from human cortex during attempted speech (Fig. 3a, top, Willett et al. ). A participant with a degenerative disease who is unable to produce intelligible speech attempts to vocalize sentences prompted on a screen, while neural population activity is recorded from the ventral premotor cortex. Since there is a large variation in the length of prompted sentences (Fig. 3a, bottom), this dataset allows us to evaluate the performance of LDNS on real data in naturalistic settings with variable-length and highly heterogeneous dynamics.

To account for varying trial length during autoencoder training, we pad all trials to a maximum length of 512 bins and compute the reconstruction loss only on the observed time bins. For the diffusion model, we indicate the target trial length with a binary mask as a conditioning variable.

This approach allows us to infer time-aligned latents underlying the cortical activity of the participants, compressing the population activity by a factor of four before training an unconditional diffusion model on these latents. Resulting samples of LDNS, mimicking human cortical activity, are visually indistinguishable from the real data (Fig. 3b,c, additional samples in Supp. Fig. A7). This is reflected in closely matched population spike count histograms (Fig. 3d, left), and single neuron statistics such as mean and standard deviation of the inter-spike interval (Fig. 3d, right). Additionally, real

Figure 3: **Unconditional generation of variable-length trials of human spiking data during attempted speech.****a)** Multi-unit activity is recorded from speech production-related regions of the brain (top) during attempted vocalization of variable-length sentences (bottom). **b)** Neural activity during sentences of different lengths. **c)** LDNS unconditionally sampled trials with different lengths, using the Poisson observation model. **d)** LDNS population spike count histogram, and mean and std of the isi match those of the data. **e)** Correlation matrices of the data (left) and LDNS samples (middle), and scatterplot of the pairwise correlations of data vs. LDNS samples (right).

and LDNS-sampled spikes display similar population dynamics, as reflected in the top principal components (Supp. Fig. A8). While LDNS tends to overestimate some pairwise correlations, it captures prominent features of the correlation structure in the data (Fig. 3e, Pearson correlation coefficient \(r=0.47\)), and our analysis indicates that this slight mismatch already arises at the autoencoder stage (Supp. Fig. A9).

LDNS allows for both inferring latent representations and generating variable-length trial data, making it applicable to complex real neural datasets without a fixed trial structure.

### Realistic generation of spiking data from a monkey performing reach tasks

We further evaluate LDNS in a different setting by applying it to model sparse spiking data recorded from a monkey performing a reaching task constrained by barriers that form a maze (Fig. 4a, left). The variety of different maze architectures leads to diverse reach movements of both curved and straight reaches (Fig. 4a, right). We again infer low-dimensional latent trajectories that capture the shared variability of the neural population and then train an unconditional diffusion model on these latents. Sampled spikes from LDNS closely resemble the true, sparse population data (Fig. 4b, additional samples in Supp. Fig. A11), and closely match population-level spike statistics (Fig. 4c). Single neuron statistics in this low spike count regime (a maximum of three spikes per neuron in 5 ms bins) are also captured well (Fig. 4d), and are on par with or better than LFADS  (see Table 1 for summary of main comparisons, and appendix A5 for additional baselines ). Beyond spiking statistics, we observe that LDNS also preserves the temporal structure of population dynamics, as reflected in the top principal components of smoothed spikes (Supp. Fig. A15). Thus, LDNS can generate spiking data that is faithful at the level of both single-neuron and population dynamics.

   Method & \(D_{KL}\) psch & RMSE pairwise corr & RMSE mean isi & RMSE std isi \\  AutoLFADS & \(0.0040 2.2\)e-4 & \(0.0026 1.25\)e-5 & \(0.039 0.003\) & \(0.029 0.001\) \\ LDNS & \(\) & \(\) & \(\) & \(\) \\  AutoLFADSsh & \(0.0036 2.1\)e-4 & \(0.0026 1.8\)e-5 & \(0.034 0.002\) & \(\) \\ LDNSsh & \(\) & \(\) & \(\) & \(\) \\   

Table 1: **Model metrics comparison.**\(D_{KL}\) for the population spike count histogram and RMSE comparisons. Mean and standard deviation across 5 folds sampled with replacement. **sh** represents observation models with spike history. **Bolded** entries represent best-performing values for Poisson and spike-history observation models.

Figure 4: **Realistic generation of spiking data in a monkey performing reach tasks.****a)** A monkey performs diverse reach movements in different mazes. **b)** Neural activity during a reach trial and a sampled trial from LDNS with a Poisson observation model. **c)** The LDNS population spike count histogram, and pairwise correlations match those of the data. **d)** LDNS mean- and std isi match the monkey data distribution. **e)** Auto-correlation of data, LDNS samples with Poisson observations (left), and LDNS samples with spike history, grouped according to correlation strength.

Both LFADS and the LDNS autoencoder are optimized by maximizing the Poisson log-likelihood, and thus cannot capture single-neuron dynamics such as refractoriness , which can have a strong influence on the observed autocorrelation structure. Given the overall sparsity of the spiking data and resulting low correlations (Supp. Fig. A10), we focus on the temporal structure of auto-correlations averaged within groups of neurons (\( 45\) neurons per group) split by their instantaneous correlation strength : darker colors correspond to the highest correlated group of four, lighter colors correspond to the group with the second highest correlations (Fig. 4e). We then compare these auto-correlations to those of grouped LDNS samples with a Poisson observation model (red). As expected, LDNS with Poisson observations is unable to capture the dip in the data auto-correlation at 5 ms lags (one time bin) (Fig. 4e, left).

To overcome this mismatch, we train an additional spike history-dependent autoregressive observation model on top of the inferred rates (LDNSsh, for spike history). In contrast to the Poisson samples, autoregressive samples can capture this aspect of neural spiking data very accurately while also improving the overall fit to the empirical auto-correlation (Fig. 4e, right). Moreover, the post hoc optimization of these filters also improves modeling of other single-neuron, as well as population-level statistics, such as the population spike count histogram or the mean of the isi (Table 1, Supp. Fig. A13).

We view this post-hoc augmentation as a key modular contribution, which can be flexibly applied to other generative models. To this end, we extend AutoLFADS with spike history dependence (LFADSsh), improving its performance across metrics. The augmented LFADSsh also captures the dip in autocorrelation at 5 ms lags (Supp. Fig. A12). Still, in both observation model variants, LDNS maintains superior or comparable performance (Table 1).

Thus, Poisson LDNS allows for the generation of spiking data that is on par or better in terms of sampling fidelity than previous approaches. Incorporating spike-history dependence and sampling spikes autoregressively allows us to further increase the realism of generated spike trains, leading to a large improvement on several of the considered metrics.

### Conditional generation of neural activity given reach directions or velocity profiles

Lastly, we assess the ability of conditional LDNS to generate realistic neural activity conditioned on behavioral covariates of varying complexity: the reach angle or entire velocity time series (Fig. 5a). We first validate that the autoencoder predicts firing rates that allow us to linearly decode the behavior

Figure 5: **Generation conditioned on monkey reach directions and velocity traces.****a)** Closed loop assessment: do conditionally generated latents translate to neural activity consistent with the desired direction or reach movement? **b)** Unseen reach movements (data) and corresponding movements decoded from the rates predicted by the autoencoder (ae). **c)** Decoded reach directions of LDNS samples conditioned on initial reach angles \(\). **d)** Decoded reach directions of LDNS samples conditioned on 3 unseen reach movements (velocities \(v_{x},v_{y}\)). **e)** Straight reaches from the test set used for velocity conditioning. **f)** LDNS sampled latents conditioned on trajectories shown in e) vary smoothly over time and reflect information about reach angles. **g)** PCs of sampled LDNS latents shown in f) reveal meaningful and separable information about behavior.

following the ridge-regression approach proposed in . Decoded behavior from autoencoder reconstructed rates matches the true trajectories of unseen test trials (Fig. 5b).

Given that the autoencoder performs adequately, we then test the ability to generate neural time series conditioned on the initial reach angle performed by the monkey \(_{}\). Indeed, from the generated samples of neural activity, we can decode--using the same linear decoder--realistic reach kinematics that are consistent with the conditioning angle \(_{}\) and overall reach kinematics (Fig. 5c). This indicates that LDNS can generate realistic neural activity consistent with a queried reach direction.

An even more challenging task that is intriguing for hypothesis generation is the ability to mimic an entire experiment and ask what the neural activity _would have looked like_ if the monkey had performed a particular hypothetical movement. To this end, we train a diffusion model on the same autoencoder-inferred latents but now condition on entire velocity traces (Fig. 5d). Velocity-conditioned LDNS is able to produce different samples of neural activity that are consistent with, but not exact copies of, the reach trajectories of the held-out trials given as the conditioning covariate. Such closed-loop conditioning experiments open the possibility of making predictions about neural activity during desired unseen behaviors, and thus make experimentally testable predictions.

Finally, to understand how LDNS incorporates behavioral information, we analyzed latent trajectories that were conditionally sampled based on straight reach movements in different directions (Fig. 5e). Individual samples of latent trajectories vary smoothly within a trial (Fig. 5f), while reach direction varies smoothly across samples in the first principal component (PC1) of the latents (Fig. 5g, left). Projection onto the first two PCs of latent trajectories shows clear clustering by reach direction (Fig. 5g, right), and we show that such clustering arises already at the autoencoder stage (Supp. Fig. A17).

In summary, LDNS not only produces faithful spiking samples but also allows for flexible conditioning. Furthermore, LDNS learns an interpretable latent space with behaviorally-relevant structure.

## 4 Related Work

Latent variable models of neural population dynamicsLDNS builds on previous LVMs in neuroscience, which have been extensively applied to infer low-dimensional latent representations of neural spiking data [61; 32; 39; 58; 62; 13; 28; 63] (see  for a comprehensive list.) In addition to capturing shared population-level dynamics and dependence on external stimuli , LVMs have been extended to allow autoregressive neuron-level (non-Poisson) dynamics [32; 13; 62] or even direct neural interactions . While these methods often have useful inductive biases (e.g., linear dynamical systems [32; 28] or Gaussian process priors ), these models are typically not expressive enough to yield realistic neural samples across a range of conditions.

Deep LVMs and other deep learning-based approachesVariational autoencoders (VAEs)  are particularly popular in neuroscience as they allow us to infer low-dimensional dynamics underlying high-dimensional discrete data [63; 16; 46], especially when combined with nonlinear recurrent neural networks [36; 21]. VAEs have been used to infer identifiable low-dimensional latent representations conditioned on behavior [63; 21] and have incorporated smoothness priors using Gaussian Processes to regularize the latent space . However, the generation performance of VAEs is rarely explored in neuroscience. Besides VAEs, generative adversarial networks (GANs ) have been proposed to synthesize spiking neural population activity [34; 42]. While GANs produce high-fidelity samples, they are challenging to train reliably and lack a low-dimensional latent space. More recently, transformer-based architectures have also been adapted to model neural activity [5; 60], though often with the focus of accurate decoding of behavior instead of generation of realistic spiking samples, while also lacking an explicit latent space . Lastly, deterministic approaches utilizing RNNs for dynamical systems reconstruction also target low-dimensional latent dynamics underlying neural data , but they do not act as probabilistic generative models.

Diffusion modelsLDNS leverages recent advances in diffusion models, which have become state-of-the-art for high-fidelity generation in several domains [20; 27], including continuous-valued neural signals such as EEG , as well as in time series forecasting and imputation tasks [2; 43]. Similar to the LDNS architecture, Alcaraz and Strodthoff  also use an S4-based denoiser for imputation. More specifically, LDNS is inspired by latent diffusion models [45; 27; 59; 15], which benefit from operating on the latent space of an autoencoder and flexible conditioning mechanisms to generate samples based on a given covariate, as is done with text-to-image  and other cross modality scenarios. Conveniently, this allows LDNS to bypass the challenges of directly modeling discrete-valued spiking data, by instead transforming spikes into the continuous latent space.

## 5 Summary and discussion

We here proposed LDNS, a flexible generative model of neural spiking recordings that simultaneously infers low-dimensional latent representations _and_ generates realistic neural activity conditioned on behavioral covariates. We apply LDNS to model three different datasets: synthetic data simulated from chaotic Lorenz dynamics, human cortical recordings with heterogeneous and variable-length trials, and finally, neural recordings in monkeys performing reach actions in a maze. Through our experiments, we demonstrate how several features of LDNS are beneficial for modeling complex datasets in neuroscience:

First, following other LDMs in the literature, LDNS decouples latent inference and probabilistic modeling of the data, offering flexibility in reusing the trained autoencoder and diffusion model. For the monkey recordings, all diffusion models (unconditional, conditioned on reach angle, and conditioned on hand velocities) operate in the latent space of the same autoencoder, in contrast to existing approaches that require end-to-end retraining for each type of conditioning variable. LDNS is also faster to train than AutoLFADS, which requires population-based training to optimize hyperparameters (appendix A3.1). Second, we show that LDNS autoencoders can be augmented with per-neuron autoregressive dynamics to capture single-neuron temporal dynamics (e.g., refractoriness), which otherwise cannot be captured with population-level shared dynamics. Third, as a result of the length-generalizable autoencoders and diffusion models using S4 layers, LDNS can generate variable-length trials in both the Lorenz example and human cortical recordings--a feature that will be particularly useful in modeling datasets recorded during naturalistic stimuli or behavior.

Altogether, these features enable LDNS to generate realistic neural activity, especially when conditioned on behavioral covariates. In our experiments, we demonstrate that unseen movement trajectories can be used to conditionally generate samples of neural activity, from which we can decode these hypothetical behaviors. These generated latent trajectories reflect behavioral information in an interpretable way. Our methodology is general and can be applied to recordings from any brain region, beyond the motor and speech cortex examples shown here. Thus, LDNS opens up further possibilities for hypothesis generation and testing _in silico_, potentially enabling stronger links between experimental and computational works.

LimitationsIn real neural data, the latent dimensionality of the system is not known, and as with all LVMs (which often assume that population dynamics are intrinsically low-dimensional), choosing an appropriate latent dimension can be challenging. Furthermore, any modeling errors at the encoding and decoding stage of the autoencoder will affect the overall performance of the latent diffusion approach. Nevertheless, in our experiments, we found that autoencoder training is fast, stable, and reasonably robust to hyperparameter configurations. While LDNS was still able to model the data well under relatively severe compression (e.g., 182-to-16 for the monkey recordings), optimizing latent dimensionality to balance expressiveness and interpretability remains a goal for future research.

Broader impactRealistic spike generation capabilities increase the risk of research manipulation by generating synthetic data that may be difficult to detect. On the other hand, LDNS could be useful for the dissemination of privatized clinical data, though we acknowledge the critical importance of protecting data privacy when working with sensitive human participant data. Finally, synthetically generated data (conditioned on unseen behavioral conditions) could be useful for augmenting the training of brain-computer interface decoding models.