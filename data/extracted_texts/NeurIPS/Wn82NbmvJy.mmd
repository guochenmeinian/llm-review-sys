# Accelerating Value Iteration with Anchoring

Jongmin Lee\({}^{1}\)Ernest K. Ryu\({}^{1,2}\)

\({}^{1}\)Department of Mathematical Science, Seoul National University

\({}^{2}\)Interdisciplinary Program in Artificial Intelligence, Seoul National University

###### Abstract

Value Iteration (VI) is foundational to the theory and practice of modern reinforcement learning, and it is known to converge at a \((^{k})\)-rate, where \(\) is the discount factor. Surprisingly, however, the optimal rate in terms of Bellman error for the VI setup was not known, and finding a general acceleration mechanism has been an open problem. In this paper, we present the first accelerated VI for both the Bellman consistency and optimality operators. Our method, called Anc-VI, is based on an _anchoring_ mechanism (distinct from Nesterov's acceleration), and it reduces the Bellman error faster than standard VI. In particular, Anc-VI exhibits a \((1/k)\)-rate for \( 1\) or even \(=1\), while standard VI has rate \((1)\) for \( 1-1/k\), where \(k\) is the iteration count. We also provide a complexity lower bound matching the upper bound up to a constant factor of \(4\), thereby establishing optimality of the accelerated rate of Anc-VI. Finally, we show that the anchoring mechanism provides the same benefit in the approximate VI and Gauss-Seidel VI setups as well.

## 1 Introduction

Value Iteration (VI) is foundational to the theory and practice of modern dynamic programming (DP) and reinforcement learning (RL). It is well known that when a discount factor \(<1\) is used, (exact) VI is a contractive iteration in the \(_{}\)-norm and therefore converges. The progress of VI is measured by the Bellman error in practice (as the distance to the fixed point is not computable), and much prior work has been dedicated to analyzing the rates of convergence of VI and its variants.

Surprisingly, however, the optimal rate in terms of Bellman error for the VI setup was not known, and finding a general acceleration mechanism has been an open problem. The classical \((^{k})\)-rate of VI is inadequate as many practical setups use \( 1\) or \(=1\) for the discount factor. (Not to mention that VI may not converge when \(=1\).) Moreover, most prior works on accelerating VI focused on the Bellman consistency operator (policy evaluation) as its linearity allows eigenvalue analyses, but the Bellman optimality operator (control) is the more relevant object in modern RL.

Contribution.In this paper, we present the first accelerated VI for both the Bellman consistency and optimality operators. Our method, called Anc-VI, is based on an "anchoring" mechanism (distinct from Nesterov's acceleration), and it reduces the Bellman error faster than standard VI. In particular, Anc-VI exhibits a \((1/k)\)-rate for \( 1\) or even \(=1\), while standard VI has rate \((1)\) for \( 1-1/k\), where \(k\) is the iteration count. We also provide a complexity lower bound matching the upper bound up to a constant factor of \(4\), thereby establishing optimality of the accelerated rate of Anc-VI. Finally, we show that the anchoring mechanism provides the same benefit in the approximate VI and Gauss-Seidel VI setups as well.

### Notations and preliminaries

We quickly review basic definitions and concepts of Markov decision processes (MDP) and reinforcement learning (RL). For further details, refer to standard references such as [69; 84; 81].

Markov Decision Process.Let \(()\) be the space of probability distributions over \(\). Write \((,,P,r,)\) to denote the MDP with state space \(\), action space \(\), transition probability \(P()\), reward \(r\), and discount factor \((0,1]\). Denote \(()\) for a policy, \(V^{}(s)=_{}[_{t=0}^{}^{t}r(s_{t},a_{t})\,|\,s _{0}=s]\) and \(Q^{}(s,a)=_{}[_{t=0}^{}^{t}r(s_{t},a_{t})\,| \,s_{0}=s,a_{0}=a]\) for \(V\)- and \(Q\)-value functions, where \(_{}\) denotes the expected value over all trajectories \((s_{0},a_{0},s_{1},a_{1},)\) induced by \(P\) and \(\). We say \(V^{}\) and \(Q^{}\) are optimal \(V\)- and \(Q\)- value functions if \(V^{}=_{}V^{}\)and \(Q^{}=_{}Q^{}\). We say \(_{V}^{}\) and \(_{Q}^{}\) are optimal policies if \(_{V}^{}=*{argmax}_{}V^{}\) and \(_{Q}^{}=*{argmax}_{}Q^{}\). (If argmax is not unique, break ties arbitrarily.)

Value Iteration.Let \(()\) denote the space of bounded measurable real-valued functions over \(\). With the given MDP \((,,P,r,)\), for \(V()\) and \(Q()\), define the Bellman consistency operators \(T^{}\) as

\[T^{}V(s) =_{a(\,\,|\,s),s^{} P(\,\,| \,s,a)}[r(s,a)+ V(s^{})],\] \[T^{}Q(s,a) =r(s,a)+_{s^{} P(\,\,|\,s,a),a^{ }(\,\,|\,s^{})}[Q(s^{},a^{})]\]

for all \(s,a\), and the Bellman optimality operators \(T^{}\) as

\[T^{}V(s) =_{a}\{r(s,a)+_{s^{}  P(\,\,|\,s,a)}[V(s^{})]\},\] \[T^{}Q(s,a) =r(s,a)+_{s^{} P(\,\,|\,s,a)} [_{a^{}}Q(s^{},a^{})]\]

for all \(s,a\). For notational conciseness, we write \(T^{}V=r^{}+^{}V\) and \(T^{}Q=r+^{}Q\), where \(r^{}(s)=_{a(\,\,|\,s)}[r(s,a)]\) is the reward induced by policy \(\) and \(^{}(s)\) and \(^{}(s,a)\) defined as

\[^{}(s s^{}) =*{Prob}(s s^{}\,|\,a(\,|\,s),s ^{} P(\,|\,s,a))\] \[^{}((s,a)(s^{},a^{})) =*{Prob}((s,a)(s^{},a^{})\,|\,s^{ } P(\,|\,s,a),a^{}(\,|\,s^{})),\]

are the transition probabilities induced by policy \(\). We define VI for Bellman consistency and optimality operators as

\[V^{k+1}=T^{}V^{k}, Q^{k+1}=T^{}Q^{k}, V^{k+1}=T^{}V^{k},  Q^{k+1}=T^{}Q^{k},\]

where \(V^{0},Q^{0}\) are initial points. VI for control, after executing \(K\) iterations, returns the near-optimal policy \(_{K}\) as a greedy policy satisfying

\[T^{_{K}}V^{K}=T^{}V^{K}, T^{_{K}}Q^{K}=T^{}Q^{K}.\]

For \(<1\), both Bellman consistency and optimality operators are contractions, and, by Banach's fixed-point theorem , the VIs converge to the unique fixed points \(V^{}\), \(Q^{},V^{}\), and \(Q^{}\) with \((^{k})\)-rate. For notational unity, we use the symbol \(U\) when both \(V\) and \(Q\) can be used. Since \(\|TU^{k}-U^{k}\|_{}\|TU^{k}-U^{}\|_{ }+\|U^{k}-U^{}\|_{}(1+)\|U^{k}-U^{ }\|_{}\), VI exhibits the rate on the Bellman error:

\[\|TU^{k}-U^{k}\|_{}(1+)^{k}\|U^{0}-U^{ }\|_{},\] (1)

where \(T\) is Bellman consistency or optimality operator, \(U^{0}\) is a starting point, and \(U^{}\) is fixed point of \(T\). We say \(V V^{}\) or \(Q Q^{}\) if \(V(s) V^{}(s)\) or \(Q(s,a) Q^{}(s,a)\) for all \(s\) and \(a\), respectively.

Fixed-point iterations.Given an operator \(T\), we say \(x^{}\) is fixed point if \(Tx^{}=x^{}\). Since Banach , the standard fixed-point iteration

\[x^{k+1}=Tx^{k}\]

has been commonly used to find fixed points. Note that VI for policy evaluation and control are fixed-point iterations with Bellman consistency and optimality operators. In this work, we also consider the Halpern iteration

\[x^{k+1}=_{k+1}x^{0}+(1-_{k+1})Tx^{k},\]

where \(x^{0}\) is an initial point and \(\{_{k}\}_{k}(0,1)\).

### Prior works

Value Iteration.Value iteration (VI) was first introduced in the DP literature  for finding optimal value function, and its variant approximate VI  considers approximate evaluations of the Bellman optimality operator. In RL, VI and approximate VI have served as the basis of RL algorithms such as fitted value iteration  and temporal difference learning . There is a line of research that emulates VI by learning a model of the MDP dynamics  and applying a modified Bellman operator . Asynchronous VI, another variation of VI updating the coordinate of value function in asynchronous manner, has also been studied in both RL and DP literature .

Fixed-point iterations.The Banach fixed-point theorem  establishes the convergence of the standard fixed-point iteration with a contractive operator. The Halpern iteration  converges for _nonexpansive_ operators on Hilbert spaces  and uniformly smooth Banach spaces . (To clarify, the \(\|\|_{}\)-norm in \(^{n}\) is not uniformly smooth.)

The fixed-point residual \(\|Tx_{k}-x_{k}\|\) is a commonly used error measure for fixed-point problems. In general normed spaces, the Halpern iteration was shown to exhibit \((1/(k))\)-rate for (nonlinear) nonexpansive operators  and \((1/k)\)-rate for linear nonexpansive operators  on the fixed-point residual. In Hilbert spaces,  first established a \((1/k)\)-rate for the Halpern iteration and the constant was later improved by . For contractive operators,  proved exact optimality of Halpern iteration through an exact matching complexity lower bound.

Acceleration.Since Nesterov's seminal work , there has been a large body of research on acceleration in convex minimization. Gradient descent  can be accelerated to efficiently reduce function value and squared gradient magnitude for smooth convex minimization problems  and smooth strongly convex minimization problems . Motivated by Nesterov acceleration, inertial fixed-point iterations  have also been suggested to accelerate fixed-point iterations. Anderson acceleration , another acceleration scheme for fixed-point iterations, has recently been studied with interest .

In DP and RL, prioritized sweeping  is a well-known method that changes the order of updates to accelerate convergence, and several variants  have been proposed. Speedy Q-learning  modifies the update rule of Q-learning and uses aggressive learning rates for acceleration. Recently, there has been a line of research that applies acceleration techniques of other areas to VI:  uses Anderson acceleration of fixed-point iterations,  uses Nesterov acceleration of convex optimization, and  uses ideas inspired by PID controllers in control theory. Among those works,  applied Nesterov acceleration to obtain theoretically accelerated convergence rates, but those analyses require certain reversibility conditions or restrictions on eigenvalues of the transition probability induced by the policy.

The _anchor acceleration_, a new acceleration mechanism distinct from Nesterov's, lately gained attention in convex optimization and fixed-point theory. The anchoring mechanism, which retracts iterates towards the initial point, has been used to accelerate algorithms for minimax optimization and fixed-point problems , and we focus on it in this paper.

Complexity lower bound.With the information-based complexity analysis , complexity lower bound on first-order methods for convex minimization problem has been thoroughly studied . If a complexity lower bound matches an algorithm's convergence rate, it establishes optimality of the algorithm . In fixed-point problems,  established \((1/k^{1-})\) lower bound on distance to solution for Halpern iteration with a nonexpansive operator in \(q\)-uniformly smooth Banach spaces. In , a \((1/k)\) lower bound on the fixed-point residual for the general Mann iteration with a nonexpansive linear operator, which includes standard fixed-point iteration and Halpern iterations, in the \(^{}\)-space was provided. In Hilbert spaces,  showed exact complexity lower bound on fixed-point residual for deterministic fixed-point iterations with \(\)-contractive and nonexpansive operators. Finally,  provided lower bound on distance to optimal value function for fixed-point iterations satisfying span condition with Bellman consistency and optimality operators and we discussed this lower bound in section 4.

## 2 Anchored Value Iteration

Let \(T\) be a \(\)-contractive (in the \(\|\|_{}\)-norm) Bellman consistency or optimality operator. The _Anchored Value Iteration_ (Anc-VI) is

\[U^{k}=_{k}U^{0}+(1-_{k})TU^{k-1}\] (Anc-VI)

for \(k=1,2,,\) where \(_{k}=1/(_{i=0}^{k}^{-2i})\) and \(U^{0}\) is an initial point. In this section, we present accelerated convergence rates of Anc-VI for _both_ Bellman consistency and optimality operators for both \(V\)- and \(Q\)-value iterations. For the control setup, where the Bellman optimality operator is used, Anc-VI returns the near-optimal policy \(_{K}\) as a greedy policy satisfying \(T^{_{K}}U^{K}=T^{}U^{K}\) after executing \(K\) iterations.

Notably, Anc-VI obtains the next iterate as a convex combination between the output of \(T\) and the starting point \(U^{0}\). We call the \(_{k}U_{0}\) term the _anchor term_ since, loosely speaking, it serves to pull the iterates toward the starting point \(U_{0}\). The strength of the anchor mechanism diminishes as the iteration progresses since \(_{k}\) is a decreasing sequence.

The anchor mechanism was introduced [39; 72; 49; 65; 17; 48] for general nonexpansive operators and \(\|\|_{2}\)-nonexpansive and contractive operators. The optimal method for \(\|\|_{2}\)-nonexpansive and contractive operators in  shares the same coefficients with Anc-VI, and convergence results for general nonexpansive operators in [17; 48] are applicable to Anc-VI for nonexpansive Bellman optimality and consistency operators. While our anchor mechanism does bear a formal resemblance to those of prior works, our convergence rates and point convergence are neither a direct application nor a direct adaptation of the prior convergence analyses. The prior analyses for \(\|\|_{2}\)-nonexpansive and contractive operators do not apply to Bellman operators, and prior analyses for general nonexpansive operators have slower rates and do not provide point convergence while our Theorem 3 does. Our analyses specifically utilize the structure of Bellman operators to obtain the faster rates and point convergence.

The accelerated rate of Anc-VI for the Bellman _optimality_ operator is more technically challenging and is, in our view, the stronger contribution. However, we start by presenting the result for the Bellman _consistency_ operator because it is commonly studied in the prior RL theory literature on accelerating value iteration [37; 38; 1; 31] and because the analysis in the Bellman consistency setup will serve as a good conceptual stepping stone towards the analysis in the Bellman optimality setup.

### Accelerated rate for Bellman consistency operator

First, for general state-action spaces, we present the accelerated convergence rate of Anc-VI for the Bellman consistency operator.

**Theorem 1**.: _Let \(0<<1\) be the discount factor and \(\) be a policy. Let \(T^{}\) be the Bellman consistency operator for \(V\) or \(Q\). Then, Anc-VI exhibits the rate_

\[\|T^{}U^{k}-U^{k}\|_{} -)(1+2-^{ k+1})}{(^{k+1})^{-1}-^{k+1}}\|U^{0}-U^{} \|_{}\] \[=(++O(^{2}) )\|U^{0}-U^{}\|_{}k=0,1,,\]

_where \(=1-\) and the big-\(\) notation considers the limit \( 0\). If, furthermore, \(U^{0} T^{}U^{0}\) or \(U^{0} T^{}U^{0}\), then Anc-VI exhibits the rate_

\[\|T^{}U^{k}-U^{k}\|_{} -)(1+-^{ k+1})}{(^{k+1})^{-1}-^{k+1}}\|U^{0}-U^{} \|_{}\] \[=(++O(^{2}) )\|U^{0}-U^{}\|_{}k=0,1,.\]

If \(\), both rates of Theorem 1 are strictly faster than the standard rate (1) of VI, since

\[-)(1+2-^{k+1})}{ (^{k+1})^{-1}-^{k+1}}=^{k})(1+2-^{k+1})}{(1-^{2k+2})}< ^{k}(1+).\]The second rate of Theorem 1, which has the additional requirement, is faster than the standard rate (1) of VI for all \(0<<1\). Interestingly, in the \( 1\) regime, Anc-VI achieves \((1/k)\)-rate while VI has a \((1)\)-rate. We briefly note that the condition \(U^{0} TU^{0}\) and \(U^{0} TU^{0}\) have been used in analyses of variants of VI [69, Theorem 6.3.11], [77, p.3].

In the following, we briefly outline the proof of Theorem 1 while deferring the full description to Appendix B. In the outline, we highlight a particular step, labeled \(\), that crucially relies on the linearity of the Bellman consistency operator. In the analysis for the Bellman optimality operator of Theorem 2, resolving the \(\) step despite the nonlinearity is the key technical challenge.

Proof outline of Theorem 1.: Recall that we can write Bellman consistency operator as \(T^{}V=r^{}+^{}V\) and \(T^{}Q=r+^{}Q\). Since \(T^{}\) is a linear operator1, we get

\[T^{}U^{k}-U^{k} =T^{}U^{k}-(1-_{k})T^{}U^{k-1}-_{k}T^{}U^{} -_{k}(U^{0}-U^{})\] \[^{}(U^{k}-(1-_{k})U^{k-1}- _{k}U^{})-_{k}(U^{0}-U^{})\] \[=^{}(_{k}(U^{0}-U^{})+(1-_{k})( T^{}U^{k-1}-U^{k-1}))-_{k}(U^{0}-U^{})\] \[=_{i=1}^{k}[(_{i}-_{i-1}(1-_{i}))( _{j=i+1}^{k}(1-_{j}))(^{})^{k-i+1 }(U^{0}-U^{})]\] \[-_{k}(U^{0}-U^{})+(_{j=1}^{k}(1-_{j}) )(^{})^{k+1}(U^{0}-U^{}),\]

where the first equality follows from the definition of Anc-VI and the property of fixed point, while the last equality follows from induction. Taking the \(\|\|_{}\)-norm of both sides, we conclude

\[\|T^{}U^{k}-U^{k}\|_{}- )(1+2-^{k+1})}{(^{k+1})^{-1}- ^{k+1}}\|U^{0}-U^{}\|_{}.\]

### Accelerated rate for Bellman optimality operator

We now present the accelerated convergence rate of Anc-VI for the Bellman optimality operator.

Our analysis uses what we call the _Bellman anti-optimality operator_, defined as

\[^{}V(s) =_{a}\{r(s,a)+_{s^{ } P(\,|\,s,a)}[V(s^{})]\}\] \[^{}Q(s,a) =r(s,a)+_{s^{} P(\,|\,s,a)}[ _{a^{}}Q(s^{},a^{})],\]

for all \(s\) and \(a\). (The sup is replaced with a inf.) When \(0<<1\), the Bellman anti-optimality operator is \(\)-contractive and has a unique fixed point \(^{}\) by the exact same arguments that establish \(\)-contractiveness of the standard Bellman optimality operator.

**Theorem 2**.: _Let \(0<<1\) be the discount factor. Let \(T^{}\) and \(^{}\) respectively be the Bellman optimality and anti-optimality operators for \(V\) or \(Q\). Let \(U^{}\) and \(^{}\) respectively be the fixed points of \(T^{}\) and \(^{}\). Then, Anc-VI exhibits the rate_

\[\|T^{}U^{k}-U^{k}\|_{}- )(1+2-^{k+1})}{(^{k+1})^{-1}- ^{k+1}}\{\|U^{0}-U^{}\|_{},\|U^{0}- ^{}\|_{}\}\]

_for \(k=0,1,\). If, furthermore, \(U^{0} T^{}U^{0}\) or \(U^{0} T^{}U^{0}\), then Anc-VI exhibits the rate_

\[\|T^{}U^{k}-U^{k}\|_{} -)(1+-^{ k+1})}{(^{k+1})^{-1}-^{k+1}}\|U^{0}-U^{} \|_{}U^{0} T^{}U^{0}\] \[\|T^{}U^{k}-U^{k}\|_{} -)(1+-^{ k+1})}{(^{k+1})^{-1}-^{k+1}}\|U^{0}-^{ }\|_{}U^{0} T^{}U^{0}\]

_for \(k=0,1,\)._Anc-VI with the Bellman optimality operator exhibits the same accelerated convergence rate as Anc-VI with the Bellman consistency operator. As in Theorem 1, the rate of Theorem 2 also becomes \((1/k)\) when \( 1\), while VI has a \((1)\)-rate.

Proof outline of Theorem 2.: The key technical challenge of the proof comes from the fact that the Bellman optimality operator is non-linear. Similar to the Bellman consistency operator case, we have

\[T^{}U^{k}-U^{k} =T^{}U^{k}-(1-_{k})T^{}U^{k-1}-_{k}T^{} U^{}-_{k}(U^{0}-U^{})\] \[}{{}}^{_{k}}(U^{k}-(1-_{k})U^{k-1}-_{k}U^{})-_{k} (U^{0}-U^{})\] \[=^{_{k}}(_{k}(U^{0}-U^{} )+(1-_{k})(T^{}U^{k-1}-U^{k-1}))-_{k}(U^{0}-U^{})\] \[_{i=1}^{k}[(_{i}-_{i-1}(1-_{i}) )(_{j=i+1}^{k}(1-_{j}))(_{l=k}^{i} ^{_{l}})(U^{0}-U^{})]\] \[-_{k}(U^{0}-U^{})+(_{j=1}^{k}(1-_{j}) )(_{l=k}^{0}^{_{l}})(U^{0}-U^{}),\]

where \(_{k}\) is the greedy policy satisfying \(T^{_{k}}U^{k}=T^{}U^{k}\), we define \(_{l=k}^{i}^{_{l}}=^{_{k}} ^{_{k-1}}^{_{i}}\), and last inequality follows by induction and monotonicity of Bellman optimality operator. The key step \(\) uses greedy policies \(\{_{l}\}_{l=0,1,,k}\), which are well defined when the action space is finite. When the action space is infinite, greedy policies may not exist, so we use the Hahn-Banach extension theorem to overcome this technicality. The full argument is provided in Appendix B.

To lower bound \(T^{}U^{k}-U^{k}\), we use a similar line of reasoning with the Bellman anti-optimality operator. Combining the upper and lower bounds of \(T^{}U^{k}-U^{k}\), we conclude the accelerated rate of Theorem 2. 

For \(<1\), the rates of Theorems 1 and 2 can be translated to a bound on the distance to solution:

\[\|U^{k}-U^{}\|_{}^{k})}{(1-^{2k+2})}\|U^{0}-U^{}\|_ {}\]

for \(k=1,2,\). This \(O(^{k})\) rate is worse than the rate of (classical) VI by a constant factor. Therefore, Anc-VI is better than VI in terms of the Bellman error, but it is not better than VI in terms of distance to solution.

## 3 Convergence when \(=1\)

Undiscounted MDPs are not commonly studied in the DP and RL theory literature due to the following difficulties: Bellman consistency and optimality operators may not have fixed points, VI is a nonexpansive (not contractive) fixed-point iteration and may not convergence to a fixed point even if one exist, and the interpretation of a fixed point as the (optimal) value function becomes unclear when the fixed point is not unique. However, many modern deep RL setups actually do not use discounting,2 and this empirical practice makes the theoretical analysis with \(=1\) relevant.

In this section, we show that Anc-VI converges to fixed points of the Bellman consistency and optimality operators of undiscounted MDPs. While a full treatment of undiscounted MDPs is beyond the scope of this paper, we show that fixed points, if one exists, can be found, and we therefore argue that the inability to find fixed points should not be considered an obstacle in studying the \(=1\) setup.

We first state our convergence result for finite state-action spaces.

**Theorem 3**.: _Let \(=1\). Let \(T^{n}^{n}\) be the nonexpansive Bellman consistency or optimality operator for \(V\) or \(Q\). Assume a fixed point exists (not necessarily unique). If, \(U^{0} TU^{0}\), then Anc-VI exhibits the rate_

\[\|TU^{k}-U^{k}\|_{}\|U^{0}-U^{} \|_{}k=0,1,.\]_for any fixed point \(U^{}\) satisfying \(U^{0} U^{}\). Furthermore, \(U^{k} U^{}\) for some fixed point \(U^{}\)._

If rewards are nonnegative, then the condition \(U^{0} TU^{0}\) is satisfied with \(U^{0}=0\). So, under this mild condition, Anc-VI with \(=1\) converges with \(}(1/k)\)-rate on the Bellman error. To clarify, the convergence \(U^{k} U^{}\) has no rate, i.e., \(\|U^{k}-U^{}\|_{}=o(1)\), while \(\|TU^{k}-U^{k}\|_{}=(1/k)\). In contrast, standard VI does not guarantee convergence in this setup.

We also point out that the convergence of Bellman error does not immediately imply point convergence, i.e., \(TU^{k}-U^{k} 0\) does not immediately imply \(U^{k} U^{}\), when \(=1\). Rather, we show (i) \(U^{k}\) is a bounded sequence, (ii) any convergent subsequence \(U^{k_{j}}\) converges to a fixed point \(U^{}\), and (iii) \(U^{k}\) is elementwise monotonically nondecreasing and therefore has a single limit.

Next, we state our convergence result for general state-action spaces.

**Theorem 4**.: _Let the state and action spaces be general (possibly infinite) sets. Let \(T\) be the nonexpansive Bellman consistency or optimality operator for \(V\) or \(Q\), and assume \(T\) is well defined.3 Assume a fixed point exists (not necessarily unique). If \(U^{0} TU^{0}\), then Anc-VI exhibits the rate_

\[\|TU^{k}-U^{k}\|_{}\|U^{0}-U^{} \|_{}k=0,1,\]

_for any fixed point \(U^{}\) satisfying \(U^{0} U^{}\). Furthermore, \(U^{k} U^{}\) pointwise monotonically for some fixed point \(U^{}\)._

The convergence \(U^{k} U^{}\) pointwise in infinite state-action spaces is, in our view, a non-trivial contribution. When the state-action space is finite, pointwise convergence directly implies convergence in \(\|\|_{}\), and in this sense, Theorem 4 is generalization of Theorem 3. However, when the state-action space is infinite, pointwise convergence does not necessarily imply uniform convergence, i.e., \(U^{k} U^{}\) pointwise does not necessarily imply \(U^{k} U^{}\) in \(\|\|_{}\).

## 4 Complexity lower bound

We now present a complexity lower bound establishing optimality of Anc-VI.

**Theorem 5**.: _Let \(k 0\), \(n k+2\), \(0< 1\), and \(U^{0}^{n}\). Then there exists an MDP with \(||=n\) and \(||=1\) (which implies the Bellman consistency and optimality operator for \(V\) and \(Q\) all coincide as \(T\,^{n}^{n}\)) such that \(T\) has a fixed point \(U^{}\) satisfying \(U^{0} U^{}\) and_

\[\|TU^{k}-U^{k}\|_{}}{_{i=0}^{k} ^{i}}\|U^{0}-U^{}\|_{}\]

_for any iterates \(\{U^{i}\}_{i=0}^{k}\) satisfying_

\[U^{i} U^{0}+\{TU^{0}-U^{0},TU^{1}-U^{1},,TU^{i-1} -U^{i-1}\}i=1,,k.\]

Proof outline of Theorem 5.: Without loss of generality, assume \(n=k+2\) and \(U^{0}=0\). Consider the MDP \((,,P,r,)\) such that

\[=\{s_{1},,s_{k+2}\},=\{a_{1}\}, P(s_{i} \,|\,s_{j},a_{1})=_{\{i=j=1,\,j=i+1\}}, r(s_{i},a_{1})=_{\{i=2\}}.\]

Then, \(T=^{}U+[0,1,0,,0]^{},U^{}=[0,1,, ,^{k}]^{}\), and \(\|U^{0}-U^{}\|_{}=1\). Under the span condition, we can show that \((U^{k})_{1}=(U^{k})_{k+2}=0\). Then, we get

\[TU^{k}-U^{k}=(0,1-(U^{k})_{2},(U^{k})_{2}- (U^{k})_{3},,(U^{k})_{k}-(U^{k})_ {k+1},(U^{k})_{k+1})\]

and this implies

\[(TU^{k}-U^{k})_{1}+(TU^{k}-U^{k})_{2}+^{-1}( TU^{k}-U^{k})_{3}++^{-k}(TU^{k}-U^{k})_{k+2}=1.\]Taking the absolute value on both sides,

\[(1++^{-k})_{1 i k+2}\{|TU^{k}-U^{k}|_{i}\} 1.\]

Therefore, we conclude

\[\|TU^{k}-U^{k}\|_{}}{_{i=0}^{k} ^{i}}\|U^{0}-U^{}\|_{}.\]

Note that the case \(=1\) is included in Theorem 5. When \(=1\), the lower bound of Theorem 5 _exactly_ matches the upper bound of Theorem 3.

Since

\[}{_{i=0}^{k}^{i}}- )(1+-^{k+1})}{(^{k+1})^{-1}- ^{k+1}}}{_{i=0}^{k}^{i}}0<<1,\]

the lower bound establishes optimality of the second rates Theorems 1 and 2 up to a constant of factor \(4\). Theorem 5 improves upon the prior state-of-the-art complexity lower bound established in the proof of [37, Theorem 3] by a factor \(1-^{k+1}\). (In [37, Theorem 3], a lower bound on the distance to optimal value function is provided. Their result has an implicit dependence on the initial distance to optimal value function \(\|U^{0}-U^{}\|_{}\), so we make the dependence explicit, and we translate their result to a lower bound on the Bellman error. Once this is done, the difference between our lower bound of Theorem 5 and of [37, Theorem 3] is a factor of \(1-^{k+1}\). The worst-case MDP of [37, Theorem 3] and our worst-case MDP primarily differ in the rewards, while the states and the transition probabilities are almost the same.)

The so-called "span condition" of Theorem 5 is arguably very natural and is satisfied by standard VI and Anc-VI. The span condition is commonly used in the construction of complexity lower bounds on first-order optimization methods  and has been used in the prior state-of-the-art lower bound for standard VI [37, Theorem 3]. However, designing an algorithm that breaks the lower bound of Theorem 5 by violating the span condition remains a possibility. In optimization theory, there is precedence of lower bounds being broken by violating seemingly natural and minute conditions .

## 5 Approximate Anchored Value Iteration

In this section, we show that the anchoring mechanism is robust against evaluation errors of the Bellman operator, just as much as the standard approximate VI.

Let \(0<<1\) and let \(T^{}\) be the Bellman optimality operator. The _Approximate Anchored Value Iteration_ (Apx-Anc-VI) is

\[U_{}^{k} =T^{}U^{k-1}+^{k-1}\] (Apx-Anc-VI) \[U^{k} =_{k}U^{0}+(1-_{k})U_{}^{k}\]

for \(k=1,2,,\) where \(_{k}=1/(_{i=0}^{k}^{-2i})\), \(U^{0}\) is an initial point, and the \(\{^{k}\}_{k=0}^{}\) is the error sequence modeling approximate evaluations of \(T^{}\).

Of course, the classical Approximate Value Iteration (Apx-VI) is

\[U^{k}=T^{}U^{k-1}+^{k-1}\] (Apx-VI)

for \(k=1,2,,\) where \(U^{0}\) is an initial point.

**Fact 1** (Classical result, [11, p.333]).: _Let \(0<<1\) be the discount factor. Let \(T^{}\) be the Bellman optimality for \(V\) or \(Q\). Let \(U^{}\) be the fixed point of \(T^{}\). Then Apx-VI exhibits the rate_

\[\|T^{}U^{k}-U^{k}\|_{}(1+)^{k}\|U^ {0}-U^{}\|_{}+(1+)\,}{1-}\, _{0 i k-1}\|^{i}\|_{}\ \ k=1,2,.\]

**Theorem 6**.: _Let \(0<<1\) be the discount factor. Let \(T^{}\) and \(^{}\) respectively be the Bellman optimality and anti-optimality operators for \(V\) or \(Q\). Let \(U^{}\) and \(^{}\) respectively be the fixed points of \(T^{}\) and \(^{}\). Then Apx-Anc-VI exhibits the rate_

\[\|T^{}U^{k}-U^{k}\|_{} -)(1+2-^{ k+1})}{(^{k+1})^{-1}-^{k+1}}\{\|U^{0}-U^{ }\|_{},\|U^{0}-^{}\|_{}\}\] \[+}}{1- }_{0 i k-1}\|^{i}\|_{}k=1,2,.\]

_If, furthermore, \(U^{0} T^{}U^{0}\), then (Apx-Anc-VI) exhibits the rate_

\[\|T^{}U^{k}-U^{k}\|_{}- )(1+-^{k+1})}{(^{k+1})^{-1 }-^{k+1}}\|U^{0}-^{}\|_{}+}}{1-}_{0 i k-1}\| ^{i}\|_{}\]

_for \(k=1,2,.\)_

The dependence on \(\|_{i}\|_{}\) of Apx-Anc-VI is no worse than that of Apx-VI. In this sense, Apx-Anc-VI is robust against evaluation errors of the Bellman operator, just as much as the standard Apx-VI. Finally, we note that a similar analysis can be done for Apx-Anc-VI with the Bellman consistency operator.

## 6 Gauss-Seidel Anchored Value Iteration

In this section, we show that the anchoring mechanism can be combined with Gauss-Seidel-type updates in finite state-action spaces. Let \(0<<1\) and let \(T^{}^{n}^{n}\) be the Bellman optimality operator. Define \(T^{}_{GS}^{n}^{n}\) as

\[T^{}_{GS}=T^{}_{n} T^{}_{2}T^{}_{1},\]

where \(T^{}_{j}:^{n}^{n}\) is defined as

\[T^{}_{j}(U)=(U_{1},,U_{j-1},(T^{}(U))_{j},U_{j +1},,U_{n})\]

for \(j=1,,n.\)

**Fact 2**.: _[Classical result, [69, Theorem 6.3.4]] \(T^{}_{GS}\) is a \(\)-contractive operator and has the same fixed point as \(T^{}.\)_

The _Gauss-Seidel Anchored Value Iteration_ (GS-Anc-VI) is

\[U^{k}=_{k}U^{0}+(1-_{k})T^{}_{GS}U^{k-1}\] (GS-Anc-VI)

for \(k=1,2,,\) where \(_{k}=1/(_{i=0}^{k}^{-2i})\) and \(U^{0}\) is an initial point.

**Theorem 7**.: _Let the state and action spaces be finite sets. Let \(0<<1\) be the discount factor. Let \(T^{}\) and \(^{}\) respectively be the Bellman optimality and anti-optimality operators for \(V\) or \(Q\). Let \(U^{}\) and \(^{}\) respectively be the fixed points of \(T^{}\) and \(^{}\). Then GS-Anc-VI exhibits the rate_

\[\|T^{}_{GS}U^{k}-U^{k}\|_{} -)(1+2-^{ k+1})}{(^{k+1})^{-1}-^{k+1}}\{\|U^{0}-U^{ }\|_{},\|U^{0}-^{}\|_{}\}\]

_for \(k=0,1,.\) If, furthermore, \(U^{0} T^{}_{GS}U^{0}\) or \(U^{0} T^{}_{GS}U^{0}\), then GS-Anc-VI exhibits the rate_

\[\|T^{}_{GS}U^{k}-U^{k}\|_{} -)(1+-^{ k+1})}{(^{k+1})^{-1}-^{k+1}}\|U^{0}-U^{ }\|_{}U^{0} T^{}_{GS}U^{0}\] \[\|T^{}_{GS}U^{k}-U^{k}\|_{} -)(1+-^{ k+1})}{(^{k+1})^{-1}-^{k+1}}\|U^{0}-^{ }\|_{}U^{0} T^{}_{GS}U^{0}\]

_for \(k=0,1,.\)_

We point out that GS-Anc-VI cannot be directly extended to infinite action spaces since Hahn-Banach extension theorem is not applicable in the Gauss-Seidel setup. Furthermore, we note that a similar analysis can be carried out for GS-Anc-VI with the Bellman consistency operator.

Conclusion

We show that the classical value iteration (VI) is, in fact, suboptimal and that the anchoring mechanism accelerates VI to be optimal in the sense that the accelerated rate matches a complexity lower bound up to a constant factor of \(4\). We also show that the accelerated iteration provably converges to a fixed point even when \(=1\), if a fixed point exists. Being able to provide a substantive improvement upon the classical VI is, in our view, a surprising contribution.

One direction of future work is to study the empirical effectiveness of Anc-VI. Another direction is to analyze Anc-VI in a model-free setting and, more broadly, to investigate the effectiveness of the anchor mechanism in more practical RL methods.

Our results lead us to believe that many of the classical foundations of dynamic programming and reinforcement learning may be improved with a careful examination based on an optimization complexity theory perspective. The theory of optimal optimization algorithms has recently enjoyed significant developments [44; 43; 45; 98; 66], the anchoring mechanism being one such example [49; 65], and the classical DP and RL theory may benefit from a similar line of investigation on iteration complexity.