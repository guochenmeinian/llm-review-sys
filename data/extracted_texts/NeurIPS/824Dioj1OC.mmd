# Rule-Guided Language Model Alignment for

Text Generation Management in Industrial Use Cases

Shunichi Akatsuka

Hitachi America, Ltd.

shunichi.akatsuka@hal.hitachi.com

&Aman Kumar

Hitachi America, Ltd.

aman.kumar@hal.hitachi.com

&Xian Yeow Lee

Hitachi America, Ltd.

xian.lee@hal.hitachi.com

&Lasitha Vidyaratne

Hitachi America, Ltd.

lasitha.vidyaratne@hal.hitachi.com

&Dipanjan Ghosh

Hitachi America, Ltd.

dipanjan.ghosh@hal.hitachi.com

&Ahmed Farahat

Hitachi America, Ltd.

ahmed.farahat@hal.hitachi.com

###### Abstract

Recent advances in Large Language Models (LLMs) have shown significant success in various natural language tasks. However, when implementing LLMs in industry applications, they often need to follow domain-specific rules. Since these rules can be complex and numerous, it is often difficult to precisely identify which rule should be applied to the response. In this paper, we propose a simple yet effective method to address this challenge, by performing the following two steps: (1) generate a dataset of rule-applied responses using simplified rule selection, and (2) train an LLM on this dataset. Since the rule selection is not designed to be perfect, the responses in the dataset do not always follow all the necessary rules. However, by training an LLM on this dataset, we expect the LLM to generalize over the rules and correctly identify the task-to-rule dependency. We demonstrate our method in the automotive repair domain, to make a repair recommendation LLM to follow safety rules. Our experimental results show that our approach improves LLM performance compared to solely applying the rules using the simplified rule selection. This suggests that our method could enhance the utility of LLMs in industry applications.

## 1 Introduction

### Motivation

The fast progress and wide use of Large Language Models (LLMs) have been driven by their great performance in many different general domains. However, these models are limited by their lack of knowledge of domain-specific information that is not publicly available. This lack of knowledge can hinder the effective application of LLMs in many real-world scenarios.

There have been multiple works that attempt to achieve domain-specialized language models, such as in the medical domain , finance domain , and chip design domain . These works require a large set (typically billions of tokens) of high-quality domain data together with enormous computation resources.

In many industrial domains, since the business process differs from organization to organization, it is generally difficult to prepare a large dataset that can be used to take the pre-training approach. However, some cases have a large number of internal domain-specific rules that are written in natural language. Our goal is to leverage the knowledge in such rules to align the behavior of the LLMs so that they can be efficiently applied in the domain. Specifically, we aim to make the LLMs correctly complete the given task and, at the same time, consider the rules and include them in the generated text.

### Our Approach

The challenging part of the scenario is that we have a large number of rules, and only a small portion of the rules are relevant in a single conversation. It is generally difficult to come up with a general method to retrieve all the relevant rules given a task.

Our approach is to first design a simplified rule selection framework, which aims to retrieve a subset of all the relevant rules. Figure 1 shows our method that retrieves relevant rules from a list of all the rules by applying a pre-defined mapping generated using LLMs. We use the retrieved rules to update the existing responses, and subsequently generate a dataset of (partially) rule-applied responses. We refer to this method as Revision with Extracted Rules (RER). Then, we train an LLM on these revised responses, to allow the LLM to generalize over the rules and correctly learn how to use the rules for a given task.

## 2 Related Work

### Language Model Alignment

Language model alignment aims to further train a pre-trained language model to ensure that the output is "aligned" with certain criteria. In a typical model alignment step, the LLM is trained to be harmless, meaning it does not output unethical, unsafe, or unfair responses. Recent LLMs are aligned using the Reinforcement Learning from Human Feedback (RLHF)  technique. Here, the human-labeled preference data is collected and used to train a preference model, which serves as the reward model in the reinforcement learning step.

The downside of the RLHF method is that a huge amount of human-labeled data is required. The idea of Constitutional AI  tries to avoid human annotation, by using an LLM to revise the existing response. The revised response is treated as a more preferred response over the original response, which can be used in the RLHF process.

Liu et al.  introduces Chain of Hindsight, which converts human feedback to natural language form and bypasses the RL step. The feedback is combined with the less preferred and more preferred responses, and the full sequence is used to train a model in a supervised way.

Figure 1: Overview of the Revision with Extracted Rules (RER) method proposed in this paper.

These methods try to make the model harmless, by making the model avoid widely agreed preferences, or constitutions, such as avoiding unethical, racist, sexist, or dangerous responses. This single set of constitutions is typically applied to any responses. In contrast, the problems we try to solve are the cases where we have a large number of rules, of which only a few are relevant to a given query.

### Output filters

Another method to control the LLM behavior is to filter the output. Rebedea et al. (2023) introduces a framework called NeMo Guardrails to incorporate programmable guardrails to LLM-based conversational systems during runtime. In the paper, the authors used the rails for generic fact-checking, hallucination, and moderation, in contrast to modifying the output using the industrial-specific rules in our work.

Dong et al. (2024) summarizes works in the field of making LLM output safer by defending against jailbreak attacks. Some of the works, such as Alon and Kamfonas (2023); Hu et al. (2024), and Jain et al. (2023) monitor the token-level output of the model to identify and defend against attack prompts. Zeng et al. (2024) introduces a method that checks for jail-breaking attempts by decomposing the output checking into intent prediction, original prompt prediction, and a final judge that decides if the LLM has been subjected to jailbreak attempts. Although these approaches that check and filter the input/output are similar to our method, these works focus specifically on defense against attacks.

## 3 Rule-Guided Language Model Alignment

As discussed in Section 1, we focus on a scenario where the number of rules is large, something of a few hundred or more. The general RLHF framework will be infeasible, as it requires a large amount of human-labeled data that correctly follows all the rules. Moreover, it is challenging to apply the constitutional AI method naively. The challenge is that when we try to revise the existing response, we don't know which rules should be applied to revise the response.

Our method addresses this challenge by pre-processing the rules and queries-responses, to generate a mapping from the queries to the rules. This allows us to select relevant rules immediately for a given query. Since the rule selection is not designed to be perfect, the responses in the dataset do not always follow _all_ the necessary rules, but a subset of them.

With the extracted rules, we apply the constitutional AI idea to revise the model output to follow the rules. We further finetune our model on this revised output. By this, we expect the LLM to generalize over the rules and correctly identify the task-to-rule dependency.

Our method comprises two main steps; dataset construction (3.1), and model training (3.2). In the following subsections, we explain each step in detail. Throughout this section, we show our method for the automotive repair support system scenario. We assume that the LLM is used in the system that supports technicians in finding how to repair their customers' vehicles. The rules we want to make the model align to are the safety rules during the repair operation.

### Dataset construction

#### 3.1.1 Data collection

An illustration of the data collection and dataset construction procedure is shown in Figure 2. First, we collected automotive-related text data from the internet. We performed this by using search engine APIs to find webpages that match automotive-repair-related key phrases and scraping the webpage content (web crawling). The key phrases are extracted from several automotive repair technician training books. We divide the book contents into smaller chunks and pass it to GPT-3.5 to create 4-5 key phrases from each chunk. An example of a key phrase is 'Perform cylinder power balance tests'. About 1,000 key phrases are extracted, checked for duplicates, and used for web crawling. We collected data from 6,972 unique top-level domains. After cleaning, filtering, and removing duplicates, we obtained about 391M tokens of automotive-domain textual data. This corpus is used to generate query-response pairs, that are later employed to train our model.

We also collect automotive repair-related safety rules from the internet. This process is done manually, by using the search engine to find websites that include relevant rules. The contents of the web pagesare cleaned and formatted so that each rule is a single self-contained sentence. In the end, two web pages are combined to generate about 300 rules, which we call the rule dataset.

#### 3.1.2 Query-Response generation

From the collected data, we generate pairs of query (\(Q\)) and response (\(R\)), which we refer to as \(Q\)-\(R\) pair. The \(Q\)-\(R\) pair is generated from the automotive-domain corpus, using an LLM. First, we chunked the data in the corpus into smaller pieces so that each piece included less than 2,048 tokens. Then, each piece of data is added to the input to the LLM (prompt) as an additional context. We first prompt the LLM to identify if the context is relevant to automotive repair. Then, if the output of the model is positive, we prompt the LLM to generate a repair request and repair recommendation from the given context. We used Llama-2-13b-chat (Touvron et al., 2023) to filter and generate the \(Q\)-\(R\) pairs and obtained about 128k pairs in total. An example of a generated query and response is shown in Table 1. The exact prompt is shown in Appendix A.1.

#### 3.1.3 Revision with Extracted Rules

For each response in the \(Q\)-\(R\) dataset, we revised the original response using relevant rules retrieved from the rule dataset. The rules are retrieved using a pre-defined mapping, which is described in the next paragraph. Once we have the extracted rules, we follow the method in constitutional AI (Bai et al., 2022) and prompt an LLM twice to revise the original response. First, we prompt the LLM to show how the original response does not follow the given rules. Then, in the second step, using the output of the first step, we prompt the LLM to revise the original response based on the rules. The output of the second step is the revised answer \(R^{}\) which is used in the training. We again used Llama-2-13b-chat for the revision. The exact prompts are shown in Appendix A.2. We refer to this revision process as "Revision with Extracted Rules" or "RER" in this paper. An example of extracted rules and a revised response is shown in Table 1.

The overall idea of rule extraction is to (1) split the \(Q\)-\(R\) pairs into multiple categories, (2) split the rules into multiple categories, and (3) make a mapping between the two categories. This way, for a given \(Q\)-\(R\) category, we can use the mapping to immediately identify the corresponding rule category and retrieve the rules. We categorized the \(Q\)-\(R\) based on word frequency analysis. From all the \(Q\)-\(R\) pairs we extracted the top 100 unigrams and top 100 bigrams that are not in the list of stop words. Then we prompt a Llama-2-13b-chat model to retrieve vehicle component names from the extracted words. Then, we removed any duplicates in unigrams and bigrams and obtained a total of 29 vehicle components, which are used as \(Q\)-\(R\) categories. If a query matched multiple categories, we prioritized the category with the least number of samples so that the total number of samples that belong to each of the categories was balanced. If a sample did not match any of the categories, we categorized them as "no match," and excluded them from the rest of the process. The rules are categorized based on the type of risk. We manually identified six risk types mentioned in the rules; _fire_, _electricity_, _heat_, _chemical_, _physical_, and _vehicle damage_. Then, for each of the rules, we used a Llama-2-13b-chat to decide which of the risk categories the rule falls into. Finally, we generated a mapping between the \(Q\)-\(R\) categories and the rule categories. We prompted GPT-4 to make this mapping, allowing up to three rule categories per \(Q\)-\(R\) category. At the end, we get a categorized list of \(Q\)-\(R\) pairs with corresponding risk categories, as shown in Table 2.

Figure 2: Overview of our dataset construction processes.

#### 3.1.4 Cleaning and Filtering

By applying the RER method to all the \(Q\)-\(R\) pairs, we obtain sets of \(Q\)-\(R\) and \(R^{}\), which we call \(Q\)-\(R\)-\(R^{}\) sets. We further filter the \(Q\)-\(R\)-\(R^{}\) sets based on the lengths of the responses, such that exceptionally long or short response data is removed. Then, we remove some samples that did not follow the prompt correctly, such as samples with the generated \(Q\) or \(R\) including placeholders, or the revised response \(R^{}\) including multiple critique-revision processes. We did this removal by keyword-based filtering, such as removing data with _[placeholder]_ in \(Q\) or \(R\) or setting a threshold on the number of words _revision_ that appear in \(R^{}\). Finally, we obtained a \(Q\)-\(R\)-\(R^{}\) dataset of 110,289 samples split into 99,228 train samples and 11,061 test samples.

### Training

Using the dataset, we train multiple models. First, we fine-tune an LLM in a purely supervised way on the original responses \(R\). This can be understood as tuning the model to maximize helpfulness only, as the original responses generally do not include safety precautions. We refer to this model as the SFT (H) model, where H stands for Helpfulness. This model serves as a baseline model.

Next, we supervised-fine-tune another model on the revised responses \(R^{}\). This corresponds to training the model to generate helpful and safe output. We call this model SFT (HS), where H and S stand for Helpfulness and Safety, respectively.

For both of the above training, we took pre-trained Llama-2-13b-chat as our initial model. We trained the model on the dataset for 2 epochs, with a global batch size of 32. The number of steps per epoch is \(99,228/32 3100\). Figure 4 shows the loss curves during the training. Although the losses decrease constantly and are expected to decrease after 2 epochs, we found that after around 1 epoch,

|p{284.5pt}|}  Query & I have a 2006 Chevrolet Silverado 1500 with a 5.3L Vortec V8 engine. The engine is having issues starting and sometimes stalling. I have checked the battery and replaced it, but the issue persists. I have also noticed that the engine is consuming more fuel than usual. Can you please help me identify the cause of this issue and provide a solution? \\  Response & It is likely that the issue is related to the fuel system. The stalling and excessive fuel consumption could be caused by a variety of factors, including clogged fuel injectors, a faulty fuel pressure regulator, or a malfunctioning mass airflow sensor. \\  & To diagnose the issue,... \\  Extracted Rules & (1) Store hazardous substances properly when not in use to prevent leaks, spills, and the risk of fire. \\  & (2) Dispose of waste promptly and correctly, particularly hazardous waste like used oils, batteries, and other chemicals. \\  & (3) Control the risk of fire by storing hazardous substances properly when not in use. \\  Revised & It is likely that the issue is related to the fuel system. The stalling and excessive fuel consumption could be caused by a variety of factors, including clogged fuel injectors, a faulty fuel pressure regulator, or a malfunctioning mass airflow sensor. \\  & **To diagnose the issue**,... \\  & It is important to note that when working with the fuel system, it is essential to take proper safety precautions to avoid the risk of fire or explosion. Therefore, I would recommend ensuring that all ignition sources are extinguished before performing any tests or repairs and using appropriate protective equipment such as gloves, safety glasses, and a fire extinguisher. Additionally, it is important to dispose of any hazardous materials properly and in accordance with local regulations. \\  

Table 1: An example of a query, response, extracted rules, and the revised response. The part in blue letters in the revised response is added by the revision. Note that some contents in the responses are omitted.

  \(Q\)-\(R\) category & \# of \(Q\)-\(R\) pairs & Risk category \\  alternator & 5304 & electricity, heat \\ battery & 3613 & electricity, chemical \\ belt & 5355 & physical \\ brake & 779 & physical, vehicle damage \\ clutch & 5910 & physical \\ coolant & 7371 & heat, chemical \\ cylinder & 3742 & physical, heat \\ dashboard & 2755 & physical, electricity \\ electrical system & 4569 & electricity, heat \\ engine & 916 & heat, physical, vehicle damage \\ exhaust & 8240 & heat, chemical \\ filter & 4708 & chemical \\ fluid level & 100 & chemical, physical \\ fuel & 1904 & fire, chemical \\ gear & 5162 & physical, vehicle damage \\ ignition & 6468 & fire, electricity \\ motor & 5467 & electricity, heat \\ oil & 1574 & chemical \\ pads & 7659 & physical \\ parts & 5272 & physical \\ pedal & 2747 & physical \\ rear & 5923 & physical, vehicle damage \\ sensor & 2200 & electricity \\ steering & 3988 & physical, vehicle damage \\ suspension & 6143 & physical, vehicle damage \\ tire & 2387 & physical, vehicle damage \\ transmission & 1633 & physical, heat, vehicle damage \\ wheel & 2122 & physical, vehicle damage \\ wiring & 4442 & electricity, heat \\ no match & 2933 & â€“ \\  total & 121,386 & \\  total (excl. no match) & 118,453 & \\ 
**total (after length filtering)** & **110,289** & \\  train & 99,228 & \\ test & 11,061 & \\  

Table 2: List of \(Q\)-\(R\) categories, number of \(Q\)-\(R\) pairs that fall into the category, and the corresponding risk categories.

Figure 3: Overview of our training processes. From the base model (Llama-2-13b-chat), we fine-tuned two models in a supervised way.

the outputs of the models are corrupted, meaning the outputs do not make sense or simply are not understandable. We took the checkpoint model at step 3000 for the evaluation.

## 4 Evaluation

### Evaluation with Gpt-4

We evaluated the performance of the models and methods by comparing the outputs of different models. The comparison was performed using GPT-4 (OpenAI et al., 2024). We provided the outputs of two models to GPT-4, together with the instruction to analyze and decide which one is superior to the other. We prompted GPT-4 in three ways to examine how the responses consider safety, and how helpful the responses are. Below is the list of metrics and explanations on how we instructed GPT-4. The exact prompts are given in Appendix A.3.

* **Safety (Naive)**: identify which conversation is safer in general.
* **Safety (w/rule)**: identify which conversation follows the provided safety rules. We provide all the 300 safety rules in the prompt.
* **Helpfulness**: identify which conversation is helpful in general.

The models or outputs we compared are the following:

* **Vanilla**: Vanilla Llama-2-13Bb-chat.
* **SFT (H)**: Llama-2-13Bb-chat supervised-fine-tuned on original response \(R\).
* **SFT (H) + RER**: SFT (H) with response revised with RER method.
* **SFT (HS)**: Llama-2-13Bb-chat supervised-fine-tuned on original response \(R^{}\).

For fair comparison, when we compare two methods \(A\) and \(B\), we evaluated both \(A\) vs \(B\) and \(B\) vs \(A\). This is because the order of the responses given to GPT-4 can impact the win ratio. For each of the model pairs, we took 1,000 queries from the test samples of the \(Q\)-\(R\)-\(R^{}\) dataset, generated and compared the outputs, and calculated the win rates.

### Elo score analysis

Based on the average win rates, we calculated the Elo scores following the same procedure as in Askell et al. (2021) and Ouyang et al. (2022). The higher Elo score indicates that the model has a higher chance to win over other models.

Figure 4: Training losses during supervised fine-tuning on revised response \(R^{}\) (red) and original response \(R\) (blue).

Looking into the safety Elo scores, we see consistent results between the Safety (Naive) and Safety (w/rule) metrics. Both the Vanilla and SFT (H) models have poor Elo scores compared to SFT (H) + RER and SFT (HS), which is expected since they are not trained or prompted to follow safety rules. Additionally, we see that SFT (HS) outperforms SFT (H) + RER in terms of safety.

Regarding the helpfulness Elo scores, we can see that SFT (H) has the highest score since it is only trained to generate useful output. The SFT (HS) model has a relatively higher score compared to the Vanilla or SFT (H) + RER models. We can see that SFT (H) + RER has the lowest score, which indicates that naively adding rules at inference can harm the helpfulness of the output.

Figure 5 shows the helpfulness versus the safety (w/rule) Elo scores. Compared to the vanilla model, we can clearly see the improvement in helpfulness for SFT (H), and in safety for SFT (HS) and SFT(H) + RER. We can also see the trade-offs between helpfulness and safety between SFT (H) and SFT (HS). It is worth noting that the SFT (HS) achieves a Pareto improvement from the SFT (H) + RER model.

## 5 Discussion

We compared multiple methods to make an LLM follow specific rules in the automotive domain example. It is interesting that the SFT (HS) approach achieves a Pareto improvement from the SFT (H) + RER approach. This result indicates that applying rules to the dataset and then training a model on that dataset works better than the SFT (H) + RER approach, which performs SFT first and then applies RER at inference.

The SFT (HS) approach is efficient in terms of the inference cost compared to the SFT (H) + RER approach since there is no revision process at inference. This also means that inference time is smaller, which leads to a better user experience. In contrast, the training cost is larger because we need to generate a revised response dataset. Since training is a one-time cost, in the long run, we can say that SFT (HS) is a more cost-efficient approach.

   & Safety (Naive) & Safety (w/rule) & Helpfulness \\  Vanilla & -352 & -279 & -28 \\  SFT (H) & -168 & -234 & 104 \\  SFT (H) + RER & 229 & 214 & -83 \\  SFT (HS) & 291 & 299 & 7 \\  

Table 3: The Elo scores, calculated independently for each evaluation metric. The higher scores indicate a higher win rate.

Figure 5: The helpfulness Elo scores (x-axis) versus the safety Elo score (y-axis).

Conclusion

In this paper, we demonstrated a method to make an LLM follow a large number of internal domain-specific rules in the automotive repair domain.

First, we collected a 391M-token automotive corpus from the internet and constructed a dataset of query-response. Next, we Revised the response with Extracted safety Rules (RER), where a pre-defined function performs the extraction. Using the original and revised dataset, we trained multiple supervised-fine-tuned (SFT) models, one trained on the original response and the other on the revised response.

We evaluated the models' helpfulness and safety by comparing the outputs of the models with GPT-4 and calculated the Elo scores for comparison. We showed that training on revised responses is more efficient than training on original responses and applying rules after generation.