# Fine-Grained Cross-View Geo-Localization Using a Correlation-Aware Homography Estimator

Xiaolong Wang\({}^{*}\)\({}^{1}\),  Runsen Xu\({}^{3}\),  Zuofan Cui\({}^{1}\),  Zeyu Wan\({}^{1}\),  Yu Zhang\({}^{1,1,2}\)

\({}^{1}\) College of Control Science and Engineering, Zhejiang University

\({}^{2}\) Key Laboratory of Collaborative sensing and autonomous unmanned systems of Zhejiang Province

\({}^{3}\) The Chinese University of Hong Kong

Contact: xlking@zju.edu.cn. The Code is available at https://github.com/xlwangDev/HC-Net.Corresponding author.

###### Abstract

In this paper, we introduce a novel approach to fine-grained cross-view geo-localization. Our method aligns a warped ground image with a corresponding GPS-tagged satellite image covering the same area using homography estimation. We first employ a differentiable spherical transform, adhering to geometric principles, to accurately align the perspective of the ground image with the satellite map. This transformation effectively places ground and aerial images in the same view and on the same plane, reducing the task to an image alignment problem. To address challenges such as occlusion, small overlapping range, and seasonal variations, we propose a robust correlation-aware homography estimator to align similar parts of the transformed ground image with the satellite image. Our method achieves sub-pixel resolution and meter-level GPS accuracy by mapping the center point of the transformed ground image to the satellite image using a homography matrix and determining the orientation of the ground camera using a point above the central axis. Operating at a speed of 30 FPS, our method outperforms state-of-the-art techniques, reducing the mean metric localization error by 21.3% and 32.4% in same-area and cross-area generalization tasks on the VIGOR benchmark, respectively, and by 34.4% on the KITTI benchmark in same-area evaluation.

## 1 Introduction

Accurate localization of ground cameras is essential for various applications such as autonomous driving, robot navigation, and geospatial data analysis. In crowded urban areas, cross-view localization using satellite images has proven to have great potential for correcting noisy GPS signals  and improving navigation accuracy . In this paper, we consider the task of fine-grained cross-view geo-localization, which estimates the 3-Dof pose of a ground camera, _i.e._, GPS location and orientation (yaw), from a given ground-level query image and a geo-referenced aerial image.

Previous works on coarse cross-view localization  have achieved high recall rates by formulating it as an image retrieval problem. However, the localization accuracy obtained using this method is limited by the segmentation size of satellite image patches, resulting in localization errors of tens of meters.

Recently, there has been a growing interest in fine-grained cross-view geo-localization, which assumes the availability of the ground image and a corresponding GPS-labeled satellite image patch covering the same area. Existing methods can be divided into two categories: those based on repeated sampling  and those employing descriptor candidates splitting from satellite image features[36; 12; 35]. Sampling-based methods transform satellite or ground images (or feature maps) to compare with another view, involving the multiple applications of geometric transformations based on potential ground camera poses. However, these methods suffer from low localization accuracy [27; 24; 26] and prolonged time consumption[27; 24; 7; 26]. [36; 12; 35] split the features of satellite images into numerous candidates to obtain more effective and refined descriptors for comparison with the ground image descriptor. Their limited pixel-level localization resolution and high memory usage present challenges for deployment in real-world end-terminals. Hence, the pursuit of a methodology that encompasses swift computation, low memory utilization, and elevated levels of both localization accuracy and resolution remains a paramount area of research.

We observe that projecting ground images onto a bird's-eye view perspective can make satellite-based localization tasks more intuitive, similar to the way humans use maps for navigation, eliminating the need for multiple sampling or further splitting of satellite patches. In this study, we develop a spherical transform module that leverages the imaging model of ground cameras to project panoramic images onto an aerial perspective, effectively bridging the gap between distinct viewpoints. As depicted in Figure 1 (b), our approach does not necessitate prior pose information, and it facilitates the straightforward determination of the ground image's position within the satellite image (c). By transforming the complex cross-view localization problem into a 2D image alignment problem, our method enables the acquisition of precise ground GPS location and orientation.

The core problem in 2D image alignment lies in obtaining the homography transformation between two images. Given that feature-based methods like [22; 28] may not yield effective results in obstructed or unclear scenes, we employ a correlation-aware homography estimation approach. We utilize a recurrent convolutional neural network block to maximize the correlation between similar regions in the feature maps, unlike previous iterative optimization methods , which minimize overall feature differences. Our method ensures low correlation in unobservable areas, minimizing their impact on homography estimation, resulting in an end-to-end network that aligns the most similar parts of the transformed ground image with the satellite image, directly outputting its GPS location. Moreover, our approach has successfully overcome the challenge of lacking compact supervision information in homography estimation, _i.e._, the absence of at least four matching point pairs. While utilizing the VIGOR dataset , we also address the inherent errors in the original ground truth labels to facilitate further research.

The main contributions of this paper include:

* A novel and effective method for fine-grained cross-view geo-localization, which strictly aligns ground and aerial image domains using a geometry-constrained spherical transform, reducing the problem to 2D image alignment.
* A correlation-aware homography estimation module that eliminates repeated sampling and disregards unobservable content in the images, resulting in excellent generalization performance and rapid inference speed at 30 FPS.
* Extensive experiments demonstrating that our method outperforms the state-of-the-art on two fine-grained cross-view localization benchmarks. Specifically, on the same-area and cross-area splits of the VIGOR benchmark , our method reduces the mean localization error by 21.3% and 32.4%, respectively. On the same-area split of the KITTI benchmark , our method reduces the mean localization error by 34.4%.

Figure 1: Visualization of the cross-view localization process in our method. The satellite image (c) is accompanied by a correlation map that represents the dense probability distribution for localization. Aligning the BEV with the satellite image using the homography matrix is shown in (d).

Related Work

**Cross-view image retrieval** methods for geo-localization, which use ground images as queries and all patches in a satellite image database as references, have been studied for years and rely on global image descriptors for successful retrieval. Early works [15; 34; 33; 31; 39] suffer from low retrieval accuracy due to significant appearance gaps and poor metric learning techniques. SAFA , Shi _et al_. , L2LTR, and TransGeo have improved localization accuracy by employing polar transform algorithms, considering orientation, and using transformers .

**Fine-grained cross-view localization** beyond one-to-one retrieval is first proposed in CVR and they propose a corresponding benchmark VIGOR.  and  project the satellite view using the candidate poses or the iterative optimization method and select the one that is most similar to the ground as the localization result.  and  employ transformers to acquire BEV representations of ground images, and then transform BEV maps by all possible ground camera poses to compare with the feature representations of satellite images. However, there remains untapped potential for refinement, and their inference entails prolonged time.  splits the features of satellite patches into several sub-volumes.  considers orientation and geometric information to create effective satellite descriptors for a set of candidate poses.  incorporates orientation and employs a coarse-to-fine manner to enhance accuracy. Despite these improvements, all three methods face resolution and high memory usage challenges stemming from the splitting or candidate selection process.

**Bird's-Eye View Perspective** has been thought of as the preferred representation for many tasks (_e.g._ navigation or localization). Recent methods [14; 7; 26] utilize transformers to project feature maps of ground images onto BEV. However, there has been no utilization of **explicit BEV images** for fine-grained cross-view geo-localization. A challenge could stem from the requirement of known camera calibration for traditional Inverse Projective Mapping methods, as  used. In an attempt to enhance BEV representations, Boosting  also explores geometric-based methodologies, but due to the necessity for depth information, transformers remain a crucial tool to handle ambiguity.

**Homography estimation** is the process of estimating a mapping between two images of a planar surface from different perspectives. DeTone _et al_.  is the first to propose a deep learning-based approach to homography estimation. Nie _et al_.  proposed the use of a global correlation layer to address the problem of small overlapping regions between the two images. Zhao _et al_.  incorporate the Lucas-Kanade (LK) algorithm  as a non-trainable iterator and combine it with CNNs. They also investigate GPS-denied navigation using Google static maps and satellite datasets based on this method. Inspired by RAFT , HNN  designed a completely iterative trainable deep homography estimation network, achieving high accuracy but requiring strong supervision. HomoGAN  designed an unsupervised homography estimation network based on transformer and GAN methods, achieving promising results while consuming more computational resources.

## 3 Method

This paper presents a novel approach to address the task of fine-grained cross-view localization, as illustrated in Figure 2. Our **H**omography estimation-based **C**ross-view geo-localization **N**etwork, named HC-Net, takes corresponding spherical-transformed ground image and GPS-tagged satellite image as input and outputs the homography matrix between them, as well as the precise GPS location and orientation of the ground camera. The following sections introduce the spherical transform for projecting panoramas to a bird's-eye view (Section 3.1), the homography estimator for precise alignment of ground BEV images with satellite images (Section 3.2), and the supervision method of our network (Section 3.3).

### Spherical Transform

Panoramic imaging modelPanoramas capture a full 360-degree view on a spherical projection plane and use equirectangular projections for display, as shown in Figure 2 (a). We represent points in the ground camera coordinate system as \(P=(x_{1},y_{1},z_{1})\) and projected points in the normalized equirectangular coordinate system of the panorama as \(P^{}=(x_{2},y_{2})\). The spherical coordinates \((,)\) are acquired from \((x_{1},y_{1},z_{1})\) using inverse trigonometric functions, with north latitude and east longitude being positive. And the projection between the spherical coordinates \((,)\) and the normalised equirectangular coordinates \((x_{2},y_{2})\) is expressed in Figure 2 (a), (b).

Spherical transformTo obtain the corresponding bird's-eye view of the panorama, we place a tangent plane at the south pole of the spherical imaging plane as a new imaging plane, as shown in Figure 2 (c). Formally, let \(H_{p} W_{p}\) be the size of the panorama and \(H_{b} W_{b}\) be the target size of the bird's-eye view after the spherical transform. We connect the camera's optical center with each pixel point on the BEV imaging plane, determining the corresponding pixel position in the panorama through the intersection of the connection line with the spherical imaging plane. We demonstrate the pixel coordinates on the bird's-eye view imaging plane as \((u_{b},v_{b})\). The camera coordinates \((x_{1},y_{1},z_{1})\) corresponding to the pixel coordinates \((u_{b},v_{b})\) can be directly obtained. We set the parameter \(fov=85^{}\) to determine the field of view of the bird's-eye view. The focal length of the BEV in the imaging process is \(f=0.5W_{b}/(fov)\). Therefore, each corresponding panoramic image coordinates \((u_{p},v_{p})\) is established as:

\[u_{p}=[1-(W_{b}/2-u_{b},H_{b}/2-v_{b})/]\,W_{p}/ 2,\\ v_{p}=[0.5-(-f/2-u_{b})^{2}+(H_{b}/2-v_{b})^ {2}})/]H_{p}.\] (1)

After the spherical transform, the ground image is projected into the aerial image's perspective, as seen in Figure 2(d). This approach enables high-resolution localization without multiple sampling or splitting of satellite patches. Unlike the satellite-based image projection discussed in [27; 24], this type of projection does not require the selection of a projection point. We adopt a similar approach for the KITTI dataset  by setting up an overlooking imaging plane. This allows us to project front-view images into a bird's-eye view without any known camera calibration, as shown in Figure 4 (a), see details in Supplementary Material. Despite being applied as a preprocessing step in our method, note that the transform is differentiable. This property opens up possibilities for future research on the 5-DoF pose of the ground camera (details in Supplementary Material).

### Homography Estimator

To achieve precise localization by aligning the transformed ground image \(I_{g}\) with the satellite image \(I_{s}\), we propose a correlation-based deep homography estimator inspired by IHN . We resize the two images \(I_{g}\) and \(I_{s}\) to the same size and feed them into a Pseudo-Siamese CNN to extract features. These features are denoted as \(_{g}^{D H W}\) and \(_{s}^{D H W}\). As shown in Figure 2, after feature extraction, correlation computation, and recurrent homography estimation, the homography matrix \(\) between \(I_{g}\) and \(I_{s}\) can be obtained.

Correlation ComputationGiven the feature maps \(_{g}\) and \(_{s}\), the correlation volume \(^{H W H W}\) is formed by taking the dot product between all pairs of feature vectors as:

\[(_{g},_{s})^{H W H  W}, C_{ijkl}=(_{g}(i,j)^{} _{s}(k,l)).\] (2)

Figure 2: An overview of the proposed method for fine-grained cross-view localization. (0) illustrates the geometric principles behind our spherical transformation, while (1), (2), and (3) demonstrate our network architecture and supervision methods.

To enlarge the perception range within a feature scale, we conduct average pooling on **C** at the last 2 dimensions with stride 2 to form another correlation volume \(^{}^{H W H/2 W/2}\).

Recurrent Homography EstimationWe refine the estimation of the homography through loop iterations. The coordinates set on \(_{g}\) are denoted as \(^{2 H W}\), and the coordinates set on \(_{s}\), which is projected from **X** using the homography matrix **H**, is denoted as \(^{}^{2 H W}\). For each coordinate position, we denote \(x=(u,v)\) in **X** and \(x^{}=(u^{},v^{})\) in \(^{}\). In each iteration, we use the Equation 3 to calculate \(^{ k}\) and use it to sample the last two dimensions of \(\), \(^{}\) with a local square grid of fixed search radius \(r\), resulting in correlation slices \(^{k}\) and \(^{,k}\) of size \(H W r r\).

\[[u^{ k}\\ v^{ k}\\ 1][_{k1}^{k}&_{k2}^{k}&_{k3}^{k}\\ _{k1}^{k}&_{k2}^{k}&_{k3}^{k}\\ _{k1}^{k}&_{k3}^{k}&1][ []{c}u\\ v\\ 1]\] (3)

Then we utilize a convolutional neural network module for residual homography estimation. As in , we parameterize the homography matrix using the displacement vectors of the 4 corner points of an image, namely the displacement cube **D**. In iteration \(k\), we feed the concatenated correlation slice \(^{k}\), \(^{,k}\), the coordinates set **X**, and the currently projected coordinates set \(^{ k}\) into the CNN module. The module is mainly composed of multiple convolutional units until the spatial resolution of the feature map reaches \(2 2\), where each unit downsamples the input by a scale of 2. Then a \(1 1\) convolutional unit projects the feature map into a \(2 2 2\) cube \(^{k}\), which is the estimated residual displacement vector of the 4 corner points. In iteration k, we update \(^{k}\) by adding the estimated residual displacement vector \(^{k}\) to \(^{k-1}\). Using \(^{k}\), we can obtain the homography matrix \(^{k}\) through the direct linear transform . The updated \(^{k}\) is then used to project **X** in the next iteration.

### Network Supervision

Label CorrectionWe use the VIGOR dataset  for fine-grained cross-view localization training and evaluation. However, the original labels in  contain errors up to 3 meters due to the use of approximate and consistent meter-to-pixel resolutions of the aerial images. Although  attempts to correct the labels, they require city-based computations or selections for resolution, which brings significant inconvenience. In our approach, we propose using the **Web Mercator projection** used by virtually all major online map providers to accurately convert GPS coordinates to pixel coordinates on satellite patches, thereby enhancing generality and convenience. The main equation is as:

\[\{ x&=[2^{zoom}(lon+)],\\ y&=[2^{zoom}(-[ (+)])], .\] (4)

where \(zoom\) indicates the zoom level for satellite images (\(zoom=20\) in VIGOR and \(zoom=19\) in KITTI) and 256 is the resolution of each satellite tile. We have applied the same method to create training labels for KITTI dataset , see Supplementary Material for details.

Loss FunctionUsing the homography matrix **H** obtained in Section 3.2, we project the center point of BEV onto the satellite image to obtain the localization pixel \((u_{s},v_{s})\). Using Equation 4, we calculated the pixel coordinates of the ground truth GPS corresponding to the satellite image as \((u_{s}^{*},v_{s}^{*})\). Furthermore, we use Equation 4 to project another point on the BEV centerline onto the satellite image. Connecting this point with \((u_{s},v_{s})\) allows us to determine the orientation of the ground camera as \(\).

We use a hybrid loss function, defined as \(=_{1}_{dis}+_{2}_{ori}+_{3} _{info}\), to guide our training. Here, \(_{dis}=(u_{s}-u_{s}^{*})^{2}+(v_{s}-v_{s}^{*}) ^{2}\) represents the L2 norm loss between the predicted result and the ground truth. \(_{ori}=|-^{*}|\) represents the L1 norm loss between the predicted \(\) and the ground truth \(^{*}\). To generate a probability distribution that can be further utilized for localization, we propose the use of an infoNCE loss  to reinforce the correlation between the BEV point used for localization and the satellite image. \(_{info}\) is defined as:

\[_{info}=-,j_{c},k^{+},l^{+})/ )}{_{k\,l}(C(i_{c},j_{c},k,l)/)},\] (5)where \((i_{c},j_{c})\) represents the center coordinates of \(_{g}\), and \((k^{+},l^{+})\) represents the downsampled position of \((u_{s}^{*},v_{s}^{*})\) in the satellite feature map \(_{s}\). The hyper-parameter \(\) is introduced to adjust the sharpness of the resulting probability distribution.

Finally, through the inverse process of Equation 4, we can determine the GPS coordinates corresponding to \((u_{s},v_{s})\), thereby obtaining **highly accurate localization** outputs.

## 4 Experiments

In this section, we first introduce two used datasets, evaluation metrics, and implement details of our network. We then compare the performance of our HC-Net to state-of-the-art and examine its ability to generalize to new measurements within the same areas, across different areas, and across datasets. Finally, we present ablation studies and computational efficiency analysis.

### Datasets and Evaluation Metrics

**VIGOR dataset** contains geo-tagged ground-level panoramas and aerial images collected in four cities in the US. Each aerial patch corresponds to a ground area of approximately \(70m 70m\). From Figure 1, it can be observed that the effective field of view of the ground image is slightly smaller than this range. A patch is considered positive if its center \(1/4\) region contains the ground camera's location, otherwise, it is semi-positive. In our experiments, we use positive aerial images for training and testing all models. The panoramas are shifted based on orientation information to align North in the middle, indicating that the orientation prior is known. During training, we introduce \( 45^{}\) noise to the orientation prior to generating orientation labels. The dataset provides 105, 214 panoramas for the geo-localization experiments. We adopt the Same-Area and Cross-Area splits from . For validation and hyperparameter tuning, we randomly select 20% of the data from the training set, as done in[36; 12; 35]. Compared to , we have more accurately corrected the ground truth labels in , as mentioned in Section 3.3.

**KITTI dataset** contains ground-level images captured by a moving vehicle with a forward-facing viewpoint, which is a restricted viewpoint.  augments the dataset with aerial images. Each aerial patch corresponds to a ground area of approximately \(100m 100m\). The Training and Test1 sets consist of different measurements from the same region, while the Test2 set has been captured in a different region. As assumed in , ground images are located within a \(40 40m\) area in the center of the corresponding aerial patches, and there is an orientation prior with noise between \( 10^{}\).

**Evaluation metrics** in our experiments are the mean and median error between the predicted and ground truth over all samples for both localization and orientation separately in meters and in degrees. Following , for the KITTI dataset, we additionally include the recall under a certain threshold for longitudinal (driving direction) and lateral localization error, and orientation estimation error. Our thresholds are set to 1m and 5m for localization and to 1\({}^{}\) and 5\({}^{}\) for orientation estimation.

### Implementation Details

Our network uses EfficientNet-B0  with pretrained weights on Imagenet  as both the ground and aerial feature extractors, with non-shared weights. The satellite image and bird's-eye-view (BEV) transformed from the ground image both have a size of \(512 512\) on both the VIGOR  and KITTI  datasets. PyTorch is used for network implementation, and training is done using the AdamW  optimizer with a maximum learning rate of \(3.5 10^{-4}\). The network is trained with a batch size of \(16\) and a training iteration of \(180000\). We set the search radius of the correlation updater \(r=4\) and set \(_{1}=0.1,_{2}=10,_{3}=1.0,=4\) in the loss function. The total iteration \(K=6/10\) and the feature map size \(D H W=320 16 16/320 16 16\) on VIGOR/KITTI.

### Comparison with State-of-the-Art Methods

**VIGOR dataset** On the **VIGOR** dataset, we compare our method against several state-of-the-art methods: CVR , SliceMatch , Boosting  and CCVPE . For the performance evaluations of CVR , SliceMatch , and Boosting  with known orientation priors, we directly utilize the results provided by CCVPE or the corresponding paper. Regarding most state-of-the-art CCVPE , we executed its official code to obtain results not presented in the original paper and used the published results where applicable. We also re-train and evaluate CCVPE with our corrected labels to ensure a fairer comparison. We conduct a comprehensive evaluation with different levels of noise in the orientation prior to ground images, including \(0^{}\), \( 20^{}\), \( 45^{}\).

As shown in Table 1, our method outperforms all previous methods in both same-area and cross-area settings in terms of the mean localization error metric, which suggests that our method can cope with more challenging scenarios, as shown in Figure 3(a). The mean localization error is reduced by 21.3% and 32.4% respectively. The previously best-performing method  shows a notable performance gap between same-area and cross-area settings, while our method significantly narrows this gap. This suggests that our method possesses superior generalization capabilities. We further demonstrate our superior generalization ability across datasets in Section 4.4.

When prior information exhibits varying degrees of noise, our method still outperforms CCVPE  and provides a more accurate estimation of the orientation. Moreover, our network generates confidence probabilities for localization results, allowing us to filter out potential misestimations. As shown in Figure 3 (c), when the scene contains a symmetric layout like zebra crossings and noise exceeds \(90^{}\), the homography estimation may incorrectly align the ground image with the satellite map. However, by rotating the BEV image four times and selecting the prediction with the highest confidence probability, we can obtain the correct result as in Figure 3 (b). Despite the mislocalized point having a similar appearance to the ground observation, its probability output is significantly smaller than the localization confidence probability from the correct location. This property is crucial for safety-critical applications like autonomous driving.

    &  &  &  \\  & & \(\)Localization(m) & \(\)Orientation(\({}^{}\)) & \(\)Localization(m) & \(\)Orientation(\({}^{}\)) \\  & & mean & median & mean & median & mean & median & mean & median \\  \)} & CVR  & 8.82 & 7.68 & - & - & 9.45 & 8.33 & - & - \\  & SliceMatch  & 5.18 & 2.58 & - & - & 5.53 & 2.55 & - & - \\  & Boosting  & 4.12 & 1.34 & - & - & 5.16 & **1.40** & - & - \\  & CCVPE  & 3.60 & 1.36 & 10.59 & 5.43 & 4.97 & 1.68 & 27.78 & 14.11 \\  & CCVPE*  & 3.37 & 1.33 & 9.47 & 5.23 & 4.96 & 1.69 & 26.30 & 14.21 \\  & HC-Net(ours)* & **2.65** & **1.17** & **1.92** & **1.04** & **3.35** & 1.59 & **2.58** & **1.35** \\  \)} & CCVPE*  & 3.48 & 1.39 & 9.80 & 5.49 & 5.16 & 1.78 & 26.36 & 14.87 \\  & HC-Net(ours)* & **2.65** & **1.17** & **1.93** & **1.02** & **3.36** & **1.59** & **2.62** & **1.34** \\  \)} & CCVPE*  & 3.50 & 1.39 & 10.56 & 5.95 & 5.16 & 1.78 & 26.77 & 15.29 \\  & HC-Net(ours)* & **2.70** & **1.18** & **2.12** & **1.04** & **3.46** & **1.60** & **3.00** & **1.35** \\   

Table 1: Location and orientation estimation error on VIGOR  dataset. **Best in bold.** Different levels of noise are added to the orientation prior. **“*”** indicates methods using our corrected labels.

Figure 3: Visualization of our method on the VIGOR  dataset with localization confidence probability indicated in the legend. (d) demonstrates a failure case of our method when dealing with scenes that are radially monotonous and repetitive-uninformative.

KITTI datasetOn the **KITTI** dataset, we compare our method with the state-of-the-art LM , SliceMatch , Boosting  and CCVPE. Similar to the evaluation on VIGOR, we directly use the results provided by CCVPE  or the corresponding paper. We utilize an increased number of iteration steps in response to the limited coverage of ground images from KITTI  within satellite images. The results are shown in Table 2. When a \( 10^{}\) orientation prior is considered in both Training and Test1, our method has a 34.4% lower mean error for localization than CCVPE . Figure 4 presents the visualization results on the KITTI dataset. Boosting  achieves exceptional accuracy in estimating orientation. This may be attributed to their utilization of a two-stage approach, where the first stage is dedicated to orientation estimation.

### Cross-Dataset Generalization

The CVUSA dataset  is commonly used for cross-view localization, but it only provides retrieval labels and cannot be used for training precise localization models. Despite the panoramas in CVUSA being cropped and having a non-standard aspect ratio, our spherical transform method successfully projects them to a bird's-eye view by completing it to the correct proportion, as shown in Figure 5. We use images from CVUSA in different cities to test our model's generalization ability, which is trained on VIGOR. The alignment results in Figure 5 demonstrate that our model has strong potential and can align significantly different BEV images with their corresponding satellite images for precise localization, even in unfamiliar cities without retraining.

    &  &  &  &  &  &  \\  & & Mean & Median & R@1m & R@5m & R@1m & R@5m & Mean & Median & R@1\({}^{}\) & R@5\({}^{}\) \\  LM  & Same & 12.08 & 11.42 & 35.54 & 80.36 & 5.22 & 26.13 & 3.72 & 2.83 & 19.64 & 71.72 \\ SliceMatch  & Same & 7.96 & 4.39 & 49.09 & 98.52 & 15.19 & 57.35 & 4.12 & 3.65 & 13.41 & 64.17 \\ Boosting  & Same & - & - & 76.44 & 98.89 & 23.54 & 62.18 & - & - & **99.10** & **100.00** \\ CCVPE  & Same & 1.22 & 0.62 & 97.35 & 99.71 & 77.13 & 97.16 & 0.67 & 0.54 & 77.39 & 99.95 \\ HC-Net(Ours) & Same & **0.80** & **0.50** & **99.01** & **99.73** & **92.20** & **99.25** & **0.45** & **0.33** & 91.35 & 99.84 \\  LM  & Cross & 12.58 & 12.11 & 27.82 & 72.89 & 5.75 & 26.48 & 3.95 & 3.03 & 18.42 & 71.00 \\ SliceMatch  & Cross & 13.50 & 9.77 & 32.43 & 76.44 & 8.30 & 35.57 & 4.20 & 6.61 & 46.82 & 46.82 \\ Boosting  & Cross & - & - & 57.72 & 91.16 & 14.15 & 45.00 & - & - & **99.98** & **100.00** \\ CCVPE  & Cross & 9.16 & **3.33** & 44.06 & 90.23 & 23.08 & 64.31 & **1.55** & **0.84** & 57.72 & 96.19 \\ HC-Net(Ours) & Cross & **8.47** & 4.57 & **75.00** & **97.76** & **58.93** & **76.46** & 3.22 & 1.63 & 33.58 & 83.78 \\   

Table 2: Location and orientation estimation error on KITTI  dataset. **Best in bold.** Long. and Orien. are abbreviations for Longitudinal and Orientation, respectively.

Figure 4: (a) depicts the projection of the frontal perspective into a bird’s-eye view in the KITTI dataset , accompanied by the corresponding satellite imagery. (b) visualizes our method’s localization results on KITTI : the top row shows aligned ground and satellite images using our estimated homography matrix; the bottom row presents original satellite images for reference.

### Ablation Study

Pseudo-Siamese BackboneWe employ a Pseudo-Siamese backbone with non-shared weights for processing images from two different sources. To demonstrate the effectiveness of this structure, we train two models with shared and non-shared weights. The results in Table 3 show that applying the Pseudo-Siamese backbone leads to a mean localization error reduction of 0.71m.

Homography Estimation ModuleWe also conduct ablation experiments to demonstrate the effectiveness of our homography estimation module. We replace the homography estimation module with feature-based methods SuperGlue  and LoFTR , utilizing RANSAC-based functions from OpenCV to compute homography matrix from matched feature points. The experimental results are presented in Table 3. The fact that the feature-based methods have low median localization errors demonstrates the effectiveness of transforming localization into 2D alignment. However, the significantly lower mean errors achieved by our proposed deep homography estimator indicate its superiority in achieving more accurate localization.

### Computational Efficiency Analysis

We assess our method's computational efficiency against the state-of-the-art CCVPE . Table 4 compares model parameters, inference memory, per-frame inference time, and mean localization error on the VIGOR dataset using a 12th Gen Intel(R) Core(TM) i5-12490F processor, 16GB memory, and an NVIDIA RTX 3050 GPU. Our method achieves higher accuracy, faster speed, and lower memory usage, demonstrating its computational efficiency.

## 5 Conclusion

In this study, we introduce HC-Net, an end-to-end network designed for fine-grained cross-view geo-localization. The network processes spherical-transformed ground images and GPS-tagged satellite images as inputs and generates the homography matrix between them, along with the precise GPS location and orientation of the ground camera. Compared to the previous state-of-the-art, HC-Net demonstrates a reduction in mean localization error by 21.3% and 32.4% on the same-area and cross-area splits of the VIGOR dataset, respectively.

   Experiment & Mean Error & Median Error \\  Non-shared weights & **2.65 m** & **1.17 m** \\ Shared weights & 3.36 m & 1.36 m \\  SuperGlue  & 13.06 m & 3.76 m \\ LoFTR  & 28.85 m & 4.89 m \\   

Table 3: Ablation Study on Pseudo-Siamese Backbone and Homography Estimation Module.

Figure 5: Three examples in CVUSA. The top of each example shows the completed panorama. The bottom-left shows the bird’s-eye view obtained using the spherical transform. The bottom-right shows the alignment result of our method between the BEV and the satellite image.

    & CCVPE  & HC-Net(ours) \\  Parameters & 57.40 M & **11.21 M** \\ Memory Usage & 4730 MiB & **1900 MiB** \\ Inference Time & 33 ms & **31 ms** \\ Mean Error & 3.37 m & **2.65 m** \\   

Table 4: Comparison of computational efficiency between our method and CCVPE .