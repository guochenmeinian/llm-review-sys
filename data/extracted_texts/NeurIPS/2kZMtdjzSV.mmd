# Beyond Task Diversity: Provable Representation Transfer for Sequential Multi-Task Linear Bandits

Thang Duong

University of Arizona

thangduong@arizona.edu

&Zhi Wang

University of Wisconsin-Madison

zhi.wang@wisc.edu

&Chicheng Zhang

University of Arizona

chichengz@cs.arizona.edu

###### Abstract

We study lifelong learning in linear bandits, where a learner interacts with a sequence of linear bandit tasks whose parameters lie in an \(m\)-dimensional subspace of \(^{d}\), thereby sharing a low-rank representation. Current literature typically assumes that the tasks are _diverse_, i.e., their parameters uniformly span the \(m\)-dimensional subspace. This assumption allows the low-rank representation to be learned before all tasks are revealed, which can be unrealistic in real-world applications. In this work, we present the first nontrivial result for sequential multi-task linear bandits without the task diversity assumption. We develop an algorithm that efficiently learns and transfers low-rank representations. When facing \(N\) tasks, each played over \(\) rounds, our algorithm achieves a regret guarantee of \(Nm+N^{}^{}dm^{ }+Nd^{2}+ md\) under the ellipsoid action set assumption. This result can significantly improve upon the baseline of \((Nd)\) that does not leverage the low-rank structure when the number of tasks \(N\) is sufficiently large and \(m d\). We also demonstrate empirically on synthetic data that our algorithm outperforms baseline algorithms, which rely on the task diversity assumption.

## 1 Introduction

Recommendation systems that interact with customers to promote the best items for each user have been widely adopted around the world. These interactions are often sequential and can be modelled as linear bandit problems , where the characteristics of items can be represented as context vectors, and a user's preference for an item (i.e., reward) can be modelled using a linear combination of the context of the item. Even though the problem is typically high-dimensional, different users may exhibit similar preferences, leading to a low-dimensional underlying reward structure.

Motivated by this observation, there has been growing interest in representation learning within the context of linear bandits. For instance, in the item recommendation example, each session of interaction with a user can be seen as a linear bandit task, and similarity across tasks can be captured by the existence of a global feature extractor that applies to all problem instances.

Formally, we consider a problem where the learner sequentially faces \(N\)\(d\)-dimensional linear bandit tasks, each with horizon \(\), with a key assumption that the reward predictors of the \(N\) tasks, \(_{1},,_{N}\), lie in an \(m\)-dimensional linear subspace of \(^{d}\). The goal of the learner is to minimize their meta (pseudo-)regret, which is the sum of regret across all tasks (see Equation (1) below), by exploiting the shared subspace structure.

One naive approach is to solve each task independently using a base algorithm (such as Lin-UCB (Abbasi-Yadkori et al., 2011) or PEGE (Rusmevichientong and Tsitsiklis, 2010)); this approach, which we will henceforth refer to as the _individual single-task baseline_), would yield an upper bound on the meta-regret of 0\((Nd)\). On the other hand, had the shared \(m\)-dimensional subspace been known beforehand, one would only need to estimate each reward predictor's projection onto the subspace; this leads to a meta-regret of \((Nm)\). In this work, we focus on the setting where \(N\) and \(\) are large and \(m d\), the regime in which representation transfer learning would be beneficial.

Despite rich results for multi-task linear bandits in the parallel setting (e.g. Yang et al., 2020; Hu et al., 2021; Yang et al., 2022; Cella et al., 2023), progresses on multi-task bandits in the sequential setting have been relatively sparse. This can be attributed to the additional challenge of _meta-exploration_: in addition to exploration in each bandit learning task, one also needs to determine when (in which tasks) to acquire more information on the shared \(m\)-dimensional subspace representation. This is in contrast to the parallel setting, where algorithms that treat all tasks equally can achieve a near-optimal regret through a reduction to low-rank linear bandits (Hu et al., 2021; Jang et al., 2021).

Under the assumption that the action sets are well-conditioned ellipsoids, Qin et al. (2022) design an efficient algorithm with a meta-regret of \((Nm+dm)\). However, it relies on an additional key assumption that the tasks are "diverse" in the \(m\)-dimensional subspace: more formally, for any subsequence of tasks \(S\), the \(m\)-th eigenvalue of the task parameters' covariance matrix \(_{n S}_{n}_{n}^{}\) is bounded away from zero (see Tripuraneni et al. (2021) for a related assumption in the supervised regression setting). However, this task diversity assumption is hard to verify and may not even hold in practice. Therefore, we raise the question:

_Can we design sequential multi-task bandit algorithms with provable low meta-regret without strong assumptions on task parameters, especially on task diversity?_

In this paper, we answer this question positively. Under mild assumptions that the action sets are well-conditioned ellipsoids and all task parameters have norms upper and lower bounded by constants, we design an algorithm with a meta-regret of \((Nm+N^{}^{}dm^{ }+Nd^{2}+ md)\)1, providing the first nontrivial result for sequential multi-task linear bandit without the task diversity assumption. To the best of our knowledge, prior to our work, no regret bounds better than that of the individual single-task baseline (i.e., \(o(Nd)\)) were known for this setting.

Our algorithm, BOSS, is based on a reduction to a bandit online subspace selection problem. Specifically, for each new task \(n\), our algorithm chooses a subspace, represented by its canonical orthonormal basis \(_{n}\), to approximate the ground-truth subspace \(B\) and guide exploration. To address the challenge of meta-exploration, as choosing different \(_{n}\)'s leads to learning the projections of \(_{n}\) onto different subspaces/directions, our algorithm is designed to randomly meta-explore in some tasks and use them to learn \(B\). Empirically, we demonstrate the effectiveness of our algorithm in a simulated adversarial environment where the task diversity assumption does not hold.

### Related work

**Parallel representation transfer for multi-task linear bandit.** The parallel setting where the learner interacts with \(N\) tasks simultaneously in each round was initially studied by Yang et al. (2020). For the finite action setting, under some distributional assumptions on the action set in each round, they provide a total regret lower bound of \((N+)\) and an algorithm that matches this up to logarithmic factors. For the infinite action setting, they provide a lower bound for the problem of \((Nm+d)\), which holds even under the task diversity assumption. Under the same task diversity assumption and in the infinite action set setting2, Yang et al. (2022) present an algorithm with a regret guarantee of \((Nm+d)\).

For general action spaces, Hu et al. (2021) provide a regret guarantee of \((N+d)\), albeit using a computationally inefficient algorithm. They also provide an extension for multi-task linear reinforcement learning under the assumption of low inherent Bellman error.

Unlike previous approaches, Cella et al. (2023) relaxes the requirement to know the subspace rank \(m\) and the need for the task diversity assumption. Assuming that the action sets are finite and drawn from a specific distribution, by using trace-norm regularization for estimating task parameters and taking actions in a greedy fashion, they provide a regret upper bound of \((N+)\) that matches the lower bound from Yang et al. (2020) up to logarithmic factors.

**Sequential representation transfer for multi-task linear bandit.** Compared with the parallel setting, the sequential setting is more challenging, where the learner only interacts with one task at a time; hence, even after many tasks, the reward predictors of the seen tasks may not span the underlying \(m\)-dimensional subspace. Qin et al. (2022) avoid this challenge by assuming a task diversity assumption, i.e., any large enough subset of tasks span the underlying \(m\)-dimensional subspace in a well-conditioned manner. With this, they provide a meta-regret guarantee \((Nm+dm)\) which nearly matches a lower bound of \((Nm+d)\). They also extend their analysis to a nonstationary representation setting where the global feature extractor can change over segments of tasks. Yang et al. (2022) study the sequential setting with an additional assumption that \(\|_{n}\|=1\) for all tasks; however, it appears that there may be a non-trivial oversight in the analysis.3

Bilaj et al. (2024) study a related setting where the task parameters are i.i.d. sampled from a distribution with high variances from a \(m\)-dimensional subspace and with low variances in the orthogonal directions. They provide a meta-regret guarantee of at least \((N(d-m)(1+}_{ min}(d-m)}))\), where \(^{}_{min}\) is the smallest eigenvalue of the empirical covariance matrix of the actions taken. Since a linear bandit algorithm is expected to converge to pull the optimal arm at the end; \(^{}_{min}\) may be constant when the action space is fixed. Thus, this bound can be as large as \((Nd)\). For a quick reference, see Table 1 for a comparison between our work and most related works. See also Appendix A for further discussions on related work.

## 2 Problem setup

We consider a sequence of linear bandit tasks of the same length \(\), described by the task parameters \(_{1},,_{N}^{d}\) chosen by an environment such that they satisfy Assumption 1. The learner solves \(N\) tasks sequentially. In task \(n\), for each time step \(t=1,,\), the learner chooses an action \(A_{n,t}\) from the action set \(\) that satisfies Assumption 2 and receives a reward \(r_{n,t}=A_{n,t}^{}_{n}+_{n,t}\), where

   Algorithm & Prior info. & Task diversity & Regret guarantee(s) \\  Indep. PEGE & None & No & \((Nd)\) \\ for each task & & No & \\  Qin et al. (2022) & \(m,\) & Yes & \((Nm+dm)\), \((Nm+d)\) \\  Bilaj et al. (2024) & None & Yes & at least \(O(N(d-m)(1+}_{min}(d-m)} ))\) \\  BOSS (our work) & \(N,m,\) & No & \(Nm+N^{}^{}dm^{ }+Nd^{2}+ md\) \\   

Table 1: Comparisons of the settings, assumptions, and regret guarantees in this paper and previous works. A more comprehensive comparison is available in Table 2 of Appendix A.

\(_{n,t}\) is independent, mean-zero \(1\)-sub-Gaussian noise. The learner then moves on to task \(n+1\) and repeats the same learning process.

**Assumption 1** (Low-rank representation).: _Let \(m<d\). For task parameters \(_{1},,_{N}\), there exist (i) a global feature extractor \(B^{d m}\) with orthogonal columns and (ii) vectors \(w_{1},,w_{N}^{m}\), such that \(_{i}=Bw_{i}\) for all \(i[N]\)._

Given a semi-orthonormal matrix \(U^{d m}\) (i.e., \(U^{}U=I_{m}\)), denote by \(U_{}^{d(d-m)}\) a matrix whose columns constitute an orthonormal basis of the orthogonal complement of \((U)\), where we break ties in dictionary order4. We also denote the \(i\)-th column vector of \(U\) by \(U(i)\).

Following Rusmevichientong and Tsitsiklis (2010) and Qin et al. (2022), we also assume that the action set \(\) is an ellipsoid and the task parameters have bounded \(_{2}\) norms:

**Assumption 2** (Linear bandits with ellipsoid action sets).: _The action set \(:=\{x^{d}:\:x^{}M^{-1}x 1\}\) is an ellipsoid, where \(M\) is a symmetric, positive definite matrix. In addition, there exist constants \(_{}\) and \(_{} 1\) such that for all tasks \(n[N]\), \(_{}\|_{n}\|_{2}_{}\)._

We define the expected pseudo-regret for task \(n\) as \(R_{}^{n}:=_{a}a^{}_{n}- [_{t=1}^{}A_{n,t}^{}_{n}]\), and the meta-regret for all \(N\) tasks as

\[R_{}:=_{n=1}^{N}R_{}^{n}=_{n=1}^{N}[_{a }a^{}_{n}-[_{t=1}^{}A_{n,t}^{ }_{n}]].\] (1)

The learner's goal is to sequentially interact with each task in a way that minimizes its meta-regret.

## 3 Algorithm

Unlike in the parallel setting or the sequential setting with a task diversity assumption, here, the learner cannot directly learn the global feature extractor \(B\). Instead, it needs to reason with the uncertainty about \(B\) learned from the seen tasks.

**High-level idea of our approach.** To simultaneously learn \(B\) online and utilize our (imperfect) knowledge of it, we solve the sequential multi-task bandit problem using a bi-level approach:

* At the lower level, for each task \(n\), the learner has the option of invoking two base algorithms: one performs naive exploration that does not incorporate our knowledge on \(B\), using a variant of full-dimensional linear bandit algorithm (PEGE (Rusmevichientong and Tsitsiklis, 2010), Algorithm 1); the other tries to incorporate a learned subspace \(\) as prior knowledge to get reduced regret (Algorithm 2). Algorithm 1 and 2 can be viewed as performing meta-exploration and meta-exploitation respectively: Algorithm 1, while ignoring the low-dimension property of the tasks, produces unbiased estimatorsof \(_{n}\) that helps learn the task subspace \(B\); Algorithm 2 allows the learner to achieve a much lower regret when the subspace \(\) approximately contains \(_{n}\); however, using it may slow down the learning of \(B\).
* At the upper level, the learner has two decisions to make for each task \(n\): (1) choosing between meta-exploration and meta-exploitation; (2) choosing a subspace \(_{n}\) to use if performing meta-exploitation. To this end, we propose Algorithm 3, which aims at making these decisions in a feedback-driven way to ensure low meta-regret.

```
1:Input: Task index \(n\), exploration length \(_{2}\) (a multiple of \(m\)), and the subspace orthonormal basis \(_{n}^{d m}\)
2:for\(i[m]\)do
3: Let \(A_{n,t}=_{0}_{n}(i)\) for \(t=u(i-1)+1,,ui\), where \(u=}{m}\)
4:endfor
5:for time step \(t 1,,_{2}\)do
6: Take action \(A_{n,t}\) and receive the reward \(r_{n,t}\)
7:endfor
8: Compute \(_{n}:=*{argmin}_{w^{m}}} _{t=1}^{_{2}}( A_{n,t},_{n}w-r_{ n,t})^{2}\)
9: Let \(_{n}:=_{n}_{n}\)
10:for time step \(t_{2}+1,,\)do
11: Take action \(A_{n,t}*{arg}_{a} a, _{n}\)
12:endfor ```

**Algorithm 2** Meta-Exploitation procedure

We now elaborate on each level in more detail.

**The lower level.** As mentioned above, Algorithm 1 is for meta-exploration. When invoked in task \(n\), it can achieve two goals simultaneously: obtaining an unbiased estimate of \(_{n}\), while maintaining a reasonable regret guarantee for task \(n\). For the first \(_{1}\) steps (line 3 to 6), the learner takes actions \(\{_{0}e_{i}\}_{i=1}^{d}\) that span the action space \(\), where \(e_{i}\) is the \(i\)-th canonical vector of \(^{d}\) and \(_{0}=(M)}\) is a constant factor that ensures \(_{0}e_{i}\) (recall Assumption 2).

Then, the learner estimates task parameter \(_{n}\) in line \(8\) and acts greedily for the rest of the task (line 10). We summarize its guarantee (originally due to Rusmevichientong and Tsitsiklis (2010)) as follows:

**Lemma 3**.: _Fix \(_{1}\) to be a multiple of \(d\). Suppose Algorithm 1 is run on task \(n\) with the exploration length \(_{1}\). Then, there exists some constants \(c_{1},c_{2}>0\) (that depend on \(_{0},_{},_{}\), and \(M\)) such that:_

1. _The regret on task_ \(n\) _is bounded as_ \(R_{}^{n} c_{1}(_{1}+}{_{1}} )=:}}\)_;_
2. _With probability_ \( 1-\)_,_ \(\|_{n}-_{n}\| c_{2}(d}{_{1}}})=:\)_._

We defer the proof of this lemma to Appendix D. Lemma 3 reveals a tradeoff between meta-exploration and regret minimization for task \(n\): if \(_{1}\) is larger (say, closer to \(\)), \(_{n}\) estimates \(_{n}\) more accurately; however, this may yield a worse bound on \(R_{}^{n}\).

On the other hand, Algorithm 2 is for meta-exploitation. It takes a subspace (represented by its orthonormal basis \(\)) as input, and when invoked in task \(n\), it can achieve a lower regret guarantee than Algorithm 1 when the subspace contains vectors that closely approximate \(_{n}\). Instead of exploring \(^{d}\), the learner only explores the subspace induced by \(_{n}\) (lines 3 and 6). Then, the learner estimates the low-dimensional task parameter \(_{n}\) in line 9 and acts greedily for the rest of the task (line 11). We summarize its guarantee (originally due to Yang et al. (2020)) as follows:

**Lemma 4**.: _Fix \(_{2}\) to be a multiple of \(m\). Suppose Algorithm 2 is run on task \(n\) with input subspace \(_{n}\) and the exploration length \(_{2}\). Then, there exists some constant \(c>0\) (that depends on \(_{0},_{},_{}\)and \(M\)), such that the regret on task \(n\) is bounded as:_

\[R_{}^{n} c(_{2}+(}{_{2}}+\| _{n,}^{}_{n}\|_{2}^{2})).\]

_Specifically, if \(\|_{n,}^{}_{n}\|_{2} 2\), then \(R_{}^{n} 4c(_{2}+(}{_{2}}+^{2} ))\), where \(\) is defined in Lemma 3._

Lemma 4 reveals the opportunistic nature of Algorithm 2: if \(_{n}\) is perfectly contained in the subspace spanned by \(_{n}\), \(\|_{n,}^{}_{n}\|=0\) and the regret bound is \(R_{}^{n} O(_{2}+}{_{2}})\), which can be as low as \(O(m)\); on the other extreme, \(\|_{n,}^{}_{n}\|\) can be as large as \(\|_{n}\|\) in the worst case, which yields a trivial linear regret bound. Thus, its regret guarantee hinges on good choices of subspace \(_{n}\) as input. See Appendix E for the proof of Lemma 4.

**The upper level.** For the upper level, we propose Algorithm 3 that decides (1) when to perform meta-exploration and (2) the subspace to use if performing meta-exploitation.

For (1), for each task, the learner chooses to explore the subspace with probability \(p\) (line 6) or exploit with the online subspace estimate \(_{n}\) (line 9).

For (2), we propose to choose \(\{_{n}\}_{n=1}^{N}\) that can optimize the following cost function online5:

\[C_{n}(B):=\{_{}:=_{2}+ (}{_{2}}+^{2})&\|B_{}^{}_ {n}\|_{2} 2;\\ _{}:=&..\] (2)

The motivation behind this definition of \(C_{n}\) is as follows: according to Lemma 4, \(C_{n}(_{n})\) is (up to constant) an upper bound of the regret of the learner at task \(n\), were the learner to invoke Algorithm 2 using \(_{n}\) for this task. Therefore, if we can guarantee \(_{n=1}^{N}C_{n}(_{n})\) to be small, then using Algorithm 2 for all tasks yields a small meta-regret.

An immediate challenge in directly optimizing \(C_{n}\) defined in Eq. (2) is its dependence on unobserved quantity \(_{n}\). This challenge is further complicated by the following: (i) at best, we observe \(_{n}\)'s that are \(\)-approximations of \(_{n}\) (e.g. in those meta-exploration tasks, see Lemma 3); (ii) in meta-exploitation tasks, we do not have guarantees on how close \(_{n}\) is to \(_{n}\); note the difference in the definitions of \(_{n}\) in meta-exploration and meta-exploitation tasks, respectively.

To address the challenge (i), we propose to optimize the following surrogate cost function for \(C_{n}\):

\[_{n}(B):=\{ &}}&&\|B_{}^{}_{n}\|;\\ &}}&&. .\] (3)

In Lemma 11 of Appendix C, we show that, with high probability, \(_{n}\) is an upper bound of \(C_{n}\) (when \(_{n}\) is close to \(_{n}\) as guaranteed by Lemma 3 in meta-exploration rounds).

With this modification of the optimization objective, challenge (ii) persists: \(_{n}\) is only a valid high-probability upper bound of \(C_{n}\) for all meta-exploration rounds \(n\); to ensure this upper bound property, we propose to optimize the cost:

\[_{n}(B):=_{n}(B)}{p},\] (4)

where we introduce the importance weighting multiplier \(}{p}\). Our key observation is that, although \(_{n}(B)\) is no longer an upper bound of \(C_{n}(B)\), the upper bound property holds _in-expectation_; to see this, observe that for any fixed \(B\),

\[_{Z_{n}}[_{n}(B)]=_{Z_{n}}[_{n}(B)}{p}]_{Z_{n}}[C_{n}(B) }{p}]=C_{n}(B),\]

here, the first equality is from the definition of \(_{n}(B)\); the inequality (here, \(\) indicates greater than up to a negligible constant) uses the above-mentioned property that when \(Z_{n}=1\), with high probability, \(_{n}(B) C_{n}(B)\) (see Lemma 3 and Lemma 11); the second equality is from the fact that \(Z_{n}(p)\).

Following the online learning literature, we propose to use the exponential weight algorithm (EWA, aka Hedge) [Freund and Schapire, 1997] to optimize \(\{_{n}(B)\}\) online. Ideally, we would like to run EWA with the \(=\{B:B^{d m}B^{}B=I_{m}\}\), the set of all \(m\)-dimensional subspaces; however, this is impossible because \(||\) is infinite. So instead, we propose to run EWA with the expert set defined as an \(\)-cover of \(\):

**Definition 5**.: \(^{}\) _is said to be a \(\)-cover over the set of \(\) in the principal angle sense, if:_

\[\;B,\; B^{}^{} { such that }\|B_{}^{}B^{}\|_{}.\]

Definition 5 is motivated by the well-known fact that \(\|B_{}^{}B^{}\|_{}\) is the Frobenius norm of the sine of the principal angle matrix between subspaces spanned by \(B\) and \(B^{}\). In Appendix C.1 we show how to construct \(^{}\) and its size \(|^{}|(/)^{O(dm)}\). In subsequent discussions, we will assume that BOSS uses such an \(^{}\).

Define a constant shift and scaling of \(_{n}\), \(_{n}(B):=}}}[_{n}(B)-}}}{p}]\); we note that any regret guarantee over sequence \(\{_{n}\}\) immediately translates to a regret guarantee over sequence \(\{_{n}\}\). By the construction of the expert set \(^{}\), sequence \(\{_{n}\}\) is realizable with high probability: there exists some \(B_{}^{}\) such that \(_{n=1}^{N}_{n}(B_{})=0\); this allows EWA to achieve a constant regret guarantee, summarized as follows:

**Lemma 6**.: _Let \(==c_{2}d}{_{1}}}\) (with \(c_{2}\) defined in Lemma 3) and \(=}\), where \(c\) is a constant in Lemma 13. Then, assuming that \( d^{2}\), Algorithm 3 chooses a sequence of subspaces \(\{_{n}\}\) over the expert set \(^{}\), defined in Definition 5, such that:_

\[_{n=1}^{N}[C_{n}(_{n})] O(N}}+}}|^{}| }{p})=(N(_{2}+(}{_{2} }+^{2}))+).\]

Note that the cumulative cost bound of \(\{_{n}\}\) has two terms: the benchmark term \(N}}\) and the regret bound term \(}}^{}|}{p}\). The benchmark term represents the best-case regret bound one can achieve if, for all task \(n\), the chosen subspace \(_{n}\) can well approximate \(_{n}\) (i.e. \(\|_{n,}^{}_{n}\|\)) On the other hand, the regret-bound term depends on a few important factors: first, the cost decreases when the meta-exploration probability \(p\) increases - this matches our intuition that a larger \(p\) gives more frequent feedback to learn about \(_{n}\), allowing the learned \(_{n}\) to adapt faster to \(_{n}\); second, the cost depends logarithmically on the size of the expert set \(|^{}|\), which is standard in the online learning from expert advice literature; third, the cumulative cost depends on the range of the instantaneous costs \(_{}\).

**Remark 1**.: The subspace selection game in the upper level resembles the partial monitoring problem (see Lattimore and Szepesvari, 2020, Chapter 37), where the learner does not directly observe the loss for its actions but receives signals from an environment according to an observation matrix. Here, in our subspace selection problem, the learner does not directly observe the chosen subspace's cost for most tasks, except when they choose to explore the subspace \(B\). Unlike the traditional partial monitoring problem, here, the observation would be \(_{n}\), and the cost \(C_{n}(B)\) depends on the chosen subspace \(B\) and \(_{n}\). This means that the cost matrix has an infinite number of columns (one for each \(B\)) and the observation depends on the actions of the learner and the environment in a stochastic fashion, unlike a deterministic dependence in the original partial monitoring setting.

## 4 Performance Guarantees

We bound the meta-regret of Algorithm 3 in Theorem 7:

**Theorem 7**.: _With exploration probability \(p=((}{N})^{},1)\), by choosing \(==c_{2}d}{_{1}}}\) (with \(c_{2}\) defined in Lemma 3), where \(=}\), \(_{1}=d(d},)/d\), \(_{2}=m\), the meta-regret of the BOSS algorithm is bounded by:_

\[R_{}(Nm+N^{}^{}dm^ {}+Nd^{2}+ md).\] (5)

In the meta-regret bound (5), we view the first two terms as the main terms and the last two as "burn-in" terms. The first term, \(Nm\) is the cumulative regret bound of the oracle baseline, i.e. the idealized algorithm that takes advantage of the extra knowledge of \(B\) to achieve a \(O(m)\) regret for every task. The second term, \(N^{}^{}dm^{}\) is the main overhead for learning the representation \(B\); it grows sublinearly in \(N\), and as a consequence, is dominated by the first term when the number of tasks \(N\) is very large (specifically, \(N}{m^{2}}\)). Compared with multi-task regret bounds in the parallel setting (Yang et al., 2020, Hu et al., 2021), our dependence on \(N\) is admittedly weaker; nevertheless, to our knowledge, Theorem 7 is the first nontrivial result in the sequential setting without task diversity assumptions, which has not been studied before in (e.g., Qin et al., 2022).

For the burn-in terms, the \(Nd^{2}\) term can be interpreted as a constant \(d^{2}\) regret overhead per task; the \( md\) term can be interpreted as the learner sacrificing a constant number of tasks (\(md\) tasks) to learn a good representation \(B\). Observe that, in the less favourable situation where \(N<md\) or \(<d^{2}\), the burn-in terms would lead to regret bounds worse than the trivial \(N\).

**Comparison with the individual single task baseline.** Recall that the individual single-task baseline has a meta-regret of \(O(Nd)\); our meta-regret guarantee improves over this baseline when \( d^{2}\) and \(N m\). We leave broadening the parameter regimes when our guarantee outperforms the individual single-task baseline as an important open problem.

**Comparison with lower bounds.**Qin et al. (2022) showed a lower bound for the problem: \((Nm+d)\). We can see that there still exists a gap between our upper bound in Theorem 7 with this, and the gap is bigger than other solutions with task diversity assumption such as Qin et al. (2022); we speculate that this is a price we pay due to not making any assumptions on task diversity.

We now sketch the proof of Theorem 7 below.

Proof sketch.: Denote the pseudo-regret for task \(n\) as \(_{}^{n}:=_{a}_{n},a -_{t=1}^{}_{n},A_{n,t}\).

We decompose \(R_{}\) as follows:

\[R_{}= _{n=1}^{N}[_{}^{n}]=_{n=1}^ {N}[_{}^{n}Z_{n}+_{}^{n}(1-Z_{n})]\] \[= _{n=1}^{N}[_{}^{n} Z_{n}=1 ] p+_{n=1}^{N}[_{}^{n} Z_{n}=0 ](1-p)\] \[ _{} Np+_{n=1}^{N}[C _{n}(_{n})]\] \[= ((_{1}+}{_{1}} )Np+N(_{2}+(}{_{2}}+}{ _{1}}))+)\] \[= (Np_{1}+N}{_{1}}++N_{2}+N}{_{2}}),\]

where the first two equalities are by the definition of \(R_{}\) and algebra; the third equality uses the law of total expectation; the inequality uses Lemmas 3 and 4 to bound the first and second terms, respectively; the fourth equality is due to the definition of \(_{}\) and Lemma 6; the last equality is by algebra.

The meta-regret of Algorithm 3 follows from the choices of \(_{1},_{2}\), and \(p\) - specifically, \(_{2}\) balances the last two terms, whereas \(_{1}\) and \(p\) aims at balancing the first three terms subject to the constraint that \(_{1}\) and \(p 1\) - see Appendix G for the remaining details. 

Adaptivity to problem parameters.Algorithm 3 requires the knowledge of \(N\) and \(m\), the total number of tasks and the dimensionality of the subspace underlying the task parameters. Below, we show that knowledge of \(N\) can be relaxed.

We can relax the need to know \(N\) by using the doubling trick on BOSS. Specifically, in phase \(i\), we can run our algorithm with the assumption that there are \(2^{i}\) total tasks in this phase. The modified algorithm has a meta-regret guarantee that is within a constant of the algorithm that knows \(N\). This implicitly gives an adaptive setting of meta-exploration probability \(p\) that is decaying over time.

For \(m\), the requirement can be relaxed to knowing an upper bound of \(m\). Removing this knowledge requires a change of approach, such as low-rank matrix optimization, as in Cella et al. (2023) or additional assumption, as in Bilaj et al. (2024). Cella et al. (2023) is in the parallel setting, which is not applicable here, and Bilaj et al. (2024)'s guarantee can be as large as \(O(Nd)\) as discussed in section 1.1.

## 5 Experiments

In this section6, we compare the performance of our BOSS algorithm with the baselines on synthetic environments. The algorithms we evaluate include:

* PEGE: independently solves each task using the PEGE algorithm (Rusmevichientong and Tsitsiklis, 2010)
* PEGE-oracle: The "oracle baseline" that only uses PEGE on the true subspace \(B\), for all tasks
* SeqRepL: our implementation of (Qin et al., 2022), in which \(_{n}\) is estimated with SVD and the tasks for meta-exploration are chosen deterministically at round \(n=\) for \(i=1,2,\).
* BOSS-no-oracle: Algorithm 3 with \(^{}\) set of 100,000 experts drawn uniformly at random from \(\).
* BOSS: Algorithm 3 with \(^{}\) set as 100,000 experts drawn uniformly at random from \(\), plus the ground truth expert \(B\). This algorithm is a better approximation of the original BOSS (Algorithm 3) since there exists \(B_{}^{}\) such that \(\|(B_{})_{}^{}B\|_{}\).

The setting is \((N,,d,m)=(4000,500,10,3)\). The environment reveals a new subspace dimension at tasks 1, 2501, and 3501. In this experiment, we assume \(\) is a unit sphere, i.e., \(M=I_{d}\). At each task \(n\), denote by \(B_{n}^{d m_{n}}\) the subspace basis that the environment used to generate \(_{n}\), where \(m_{n}\) is incremented when \(n=1,2501,3501\). \(_{n}\) is chosen in the following way: \(_{n}=_{1}B_{n}w_{n}\) for some \(w_{n}(^{m_{n}-1})\) and \(_{1}([0.8,1])\) is a random scaling factor to ensures Assumption 2, where \(_{}=0.8,_{}=1\).

The hyper-parameters \(p,_{1},_{2}\), and \(\) of all algorithms, where it applies, are tuned. The error bands in the figures indicate \( 1\) standard deviation computed over 5 independent runs.

Figure 0(a) clearly shows the linear dependency of the cumulative regret on \(N\). Observe that BOSS and its variants outperform both the independent PEGE and the SeqRepL baselines. It is also clear that the gap between BOSS-no-oracle and BOSS exists because the expert set \(^{}\) used in this experiment does not cover the true \(B\), since even with \(=}} 0.5\), the theoretical size of the expert set is \(|^{}|=(/)^{dm} 11^{30}\) in this experiment setting which is much larger than the expert set size used in BOSS-no-oracle.

In Figure 0(b), we plot \(\|_{n,}^{}B_{n}\|_{}\), which measures the closeness of \(_{n,}\) to \(B_{n}\). When the environment reveals a new subspace dimension at tasks 1, 2501, and 3501, all algorithms' estimation \(_{n}\) require updates and converge after a while. Even though BOSS-no-oracle has a worse estimation of \(_{n}\) compared to SeqRepL, it achieves a better regret due to having a better estimation of \(_{n}\) as shown in Figure 0(c).

## 6 Discussion and Future Work

We study the problem of sequential representation transfer in multi-task linear bandit, where the task parameters are allowed to be chosen adversarially online. Our BOSS algorithm achieves the regret guarantee of \((Nm+N^{}^{}dm^{ }+Nd^{2}+ md)\) without using the task diversity assumption as in previous works. This opens up many promising avenues for future work. Statistically, it would be good to design an algorithm that performs no worse than the individual single-task baseline's performance in all parameter regimes. In addition, BOSS utilizes the special structure of fixed, ellipsoid-shaped action spaces to obtain useful information for meta-exploration, extending the algorithm and guarantees to general and time-varying action spaces is an important direction. Practically, it would also be nice to design parameter-free variants of BOSS that do not require knowing \(m\) ahead of time. Furthermore, BOSS requires maintaining an exponentially large number of experts in \(^{}\); in the future, we would like to develop more computationally-efficient algorithms. Lastly, it would be interesting to study relaxations of Assumption 1 (all task parameters lie exactly in a \(m\)-dimensional linear subspace), similar to Bilaj et al. (2024) or the changing subspace setting of Qin et al. (2022).

Figure 1: Comparing the cumulative regret of BOSS and other baselines. The setting is \((N,,d,m)=(4000,500,10,3)\) and \(\|_{n}\|_{2}[0.8,1]\)\( n[N]\) chosen uniformly at random from this interval. The environment only reveals a new subspace dimension at tasks 1, 2501, and 3501, so thereâ€™s no task diversity assumption.