# MathWriting: A Dataset For Handwritten Mathematical Expression Recognition

Philippe Gervais

pgervais@acm.org

&Asya Fadeeva

Google

fadeich@google.com

&Andrii Maksai

Google

amaksai@google.com

Work performed while employed at Google

###### Abstract

Recognition of handwritten mathematical expressions allows to transfer scientific notes into their digital form. It facilitates the sharing, searching, and preservation of scientific information. We introduce MathWriting, the largest online handwritten mathematical expression dataset to date. It consists of **230k human-written samples** and an additional **400k synthetic ones**. This dataset can also be used in its rendered form for offline HME recognition. One MathWriting sample consists of a formula written on a touch screen and a corresponding LaTeX expression. We also provide a normalized version of LaTeX expression to simplify the recognition task and enhance the result quality. We provide baseline performance of standard models like OCR and CTC Transformer as well as Vision-Language Models like PaLI on the dataset. The dataset together with an example colab is accessible on Github.

## 1 Introduction

Three examples of HME from MathWriting. More examples can be found in Appendix K. Each ink is accompanied by a unique identifier that matches a corresponding filename in the dataset.

**MathWriting dataset (2.9 GB):**

https://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz

**Associated code:**

https://github.com/google-research/google-research/tree/master/mathwriting

Online _text_ recognition models have improved a lot over the past years, because of improvements in model structure [1; 2; 3] and also because of an increase in the amount of training data [4; 5; 6]. Mathematical expression (**ME**) recognition is a challenging task that has received less attention than regular recognition of words and characters . ME recognition is different from regular text recognition in a number of interesting ways which can prevent improvements from transferring from one to the other. Though MEs share with text most of their symbols, they follow a more rigid structure which is also two-dimensional, see Figure 1. Where text can be treated to some extent as a one-dimensional problem amenable to sequence modeling, MEs cannot because the relative positionof symbols in space is meaningful. It is also different from symbol segmentation or object detection because the output of a recognizer has to contain the relationship between symbols, serialized in some form (LIFEX, a graph, InkML, etc.). Similarly to the case of text, _handwritten_ MEs (**HME**) are more difficult to recognize than _printed_ ones as they are more ambiguous and less training data is available.

Handwritten data is costly to obtain as it must be written by hand, which is compounded in the case of online representation (**ink**) by the necessity to use dedicated hardware (touchscreen, digital pen, etc.). By publishing the MathWriting dataset, we hope to alleviate some of the needs for data for research purposes. Samples include a large number of human-written inks, as well as synthetic ones. MathWriting can readily be used with other online datasets like CROHME  or Detexify  - we publish the data in InkML format to facilitate this. It can also be used for offline ME recognition simply by rasterizing the inks, using code provided on the Github page2.

MathWriting is the largest set of online HME published so far - both human-written and synthetic. It significantly expands the set of symbols covered by CROHME , enabling more sophisticated recognition capabilities. Since inks can be rasterized, MathWriting can also been seen as larger than existing offline HME datasets [10; 11; 12]. For these reasons we introduce a new benchmark, applicable to both online and offline ME recognition.

This work's main contributions are:

* a large dataset of Handwritten Mathematical Expressions under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International3. * LIFEX ground truth expressions in normalized form to simplify training and to make evaluation more robust.
* Evaluation of different models like CTC Transformer and PaLI on the dataset to show what recognition quality could be achieved with the provided data.

The paper focuses on the high-level description of the dataset: creation process, postprocessing, train/test split, ground truth normalization, statistics, and a general discussion of the dataset content to help practitioners understand what can and cannot be achieved with it. All the low-level technical information like file formats can be found in the readme.md file present at the root of the dataset archive linked above. We also provide code examples on Github2, to show how to read the various files, process and rasterize the inks, and tokenize the LaTeX ground truth.

## 2 Dataset Creation

MathWriting dataset primarily consists of LaTeX expressions from Wikipedia, more details about the acquisition of expressions are provided in Appendix B. These expressions were used for both ink collection from human contributors Section 2.1 as well as synthetic data generation Section 2.2. We did a very limited filtering of very noisy human-written examples (described in Appendix C).

### Ink Collection

Inks were obtained from human contributors through an in-house Android app. Participants agreed to the standard Google terms of use and privacy policy. The task consisted in copying a rendered mathematical expression (prompt) shown on the device's screen using either a digital pen or a finger on a touch screen. Mathematical expressions used as prompt were first obtained in LaTeX format, then rendered into a bitmap through the LaTeX compiler (see Appendix A for the template used). 95% of MathWriting expressions were obtained from Wikipedia. The remaining ones were generated to cover underrepresented cases in Wikipedia, like isolated letters with nested sub/superscripts or complicated fractions (see Section B). Contributors were hired internally at Google. 6 collection campaigns were run between 2016 and 2019, each lasting between 2 to 3 weeks. Collected data contains only inks and labels, so no personally identifiable information is present in the dataset. Offensive content is highly unlikely because LaTeX expressions were taken from Wikipedia and we conducted a filtering of noisy data (described in Appendix C).

### Synthetic Samples and Isolated Symbols

We created synthetic samples in order to further increase the label diversity for training. This also enabled compensating for limitations of the human collection like the maximum length of the expressions, which were limited by the size of the screen they were written on. We used LaTeX expressions from Wikipedia that were not used in the data collection. The resulting synthetic data has a 90th percentile of expression length of 68 characters, compared to 51 in train. This is especially important as deep neural nets often fail to generalize to inputs longer than their training data [13; 14]. Using synthetic long inks together with the original human-written inks can help to eliminate that problem as shown in [15; 16]. The synthesis technique is as follows: starting from a raw LaTeX mathematical expression, we computed a DVI file using the LaTeX compiler, from which we extracted bounding boxes. We then used those bounding boxes to place handwritten individual symbols, resulting in a complete expression. See Figure 1 for an example of extracted bounding boxes and the resulting synthetic example.

Inks for individual symbols are all from the symbols split. They have been manually extracted from inks in train. For each symbol that we wanted to support, we manually selected strokes corresponding to it for 20-30 distinct occurrences in train, and used that information to generate a set of individual inks. Similar synthesis techniques have been used by  with inks,  and  with raster images.

A significant difference between synthetic and human-written inks is the stroke order. For synthetic inks, stroke order follows the order of the bounding boxes in the DVI file, which can be different from the usual order of writing for mathematical expressions. However, the writing order within a given symbol is consistent with human writing.

### Dataset Split

MathWriting is composed of five different sets of samples, which we call'splits': train, valid, test, symbols, and synthetic. The splits train, valid and test consist only of human-written examples. The split symbols is provided for synthetic data generation and is not used in training. The split of human-written samples between train, valid and test was partially done based on writers, partially based on labels. More details are provided in Appendix D. Experiments have shown that a more important factor than the handwriting style was whether the _label_ had already been seen during training. This fact is also supported by research in the area of compositional generalization . In the published version, valid has a 55% (8.5k samples) intersection with train based on unique normalized labels, and test has an 8% intersection (647 samples). We chose to have a low intersection between train and test in order to correctly measure generalization of trained models to unseen labels.

### Label Normalization

All samples in the dataset come with two labels: the LaTeX expression that was used during the data collection (annotation label in the InkML files), and a normalized version of it meant for model training, which is free from a few sources of confusions for an ML model (annotation normalizedLabel). An example with original and normalized labels is provided in Figure 2. Label normalization covers three main categories (details are provided in Appendix E):

Figure 1: An example of a synthetic ink created from bounding boxes with label ((p+q)+(p-q))/2=q- e.g. bold, italic
- or that haven't been reproduced consistently by contributors.
* non-uniqueness of the LaTeX syntax. e.g.  and 1over 2 are equivalent.
* visual variations that can reproduced in handwriting but can't reliably be inferred by a model. This includes size modifiers like , .

We provide the raw labels to make it possible to experiment with alternative normalization schemes, which could lead to better outcomes for different applications.

#### 2.4.1 Limitations of normalization

The normalization process is purely syntactic, and can not cover cases where the meaning of the expression has to be taken into account. For example, a lot of expressions from Wikipedia use cos instead of . It is often clear to a human reader whether the sequence of characters c,o,s represents the  command or simply three letters. However, this can not be reliably inferred by a syntactic parser, for example in tacos vs ta. An alternative would be to update the raw labels, which we didn't do because we wanted to keep the information that was used during the collection as untouched as possible. Similarly, cases like 10^{-1} usually mean {10}^{-1}, though they render exactly the same. We made the choice to normalize to the former because it's the only option with a purely syntactic normalizer. It's also better than not removing these extra braces because it gives more consistent label structures, which simplifies the model training problem.

## 3 Dataset Statistics

In this section we describe the key characteristics of MathWriting and compare it to CROHME23 . In Table 1 we provide the information about the volume of the dataset splits both in terms of examples (inks) and unique labels.

### Label Statistics

MathWriting contains 457k unique labels after normalization (see Section 2.4). From Table 1 we see that most unique expressions are covered by the synthetic portion of the dataset. However, the absolute number of unique expressions in human-written part is still high - 61k. This underlines the importance of synthetic data as it allows models to see a much bigger variety of expressions. It is important to note that the synthetic split has essentially no repeated expressions. On the other hand, in real data multiple different writings of the same expression are quite common (see Figure 11 in Appendix F). This fact allows us to separately evaluate model's quality on expressions that were

    & train & synthetic & valid & test \\  \# distinct inks & 230k & 396k & 16k & 8k \\ \# distinct labels & 53k & 396k & 8k & 4k \\   

Table 1: Statistics on different subsets of MathWriting dataset.

observed during training and that those that hadn't. As seen in Table 2 the biggest intersection in expressions is between valid and train. The minimal overlap between test and train splits is beneficial for assessing a model's ability to generalize to expressions that were not seen in train.

The median length of expressions in characters is 26 which is comparable to one of the most popular English recognition datasets IAMonDB  which has median of 29 characters. However, it is important to note that LaTeX expressions have tokens that span multiple characters like \(\)frac. The median length of expressions in tokens (provided in Appendix J) is 17, thus making training a model on tokens rather then characters easier due to shorter target lengths [19; 20]. We want to emphasize that MathWriting can be used with a different tokenization scheme and token vocabulary from what we propose in Appendix J. In Figure 3 we show the number of occurrences for the most frequent tokens. Tokens \(\{\) and \(\}\) are by far the most frequent as they are integral to the LaTeX syntax.

### Ink Statistics

Each ink in MathWriting dataset is a sequence of strokes \(=[s_{0},,s_{n}]\), each stroke \(s_{i}\) consisting of points. A point is represented as a triplet \((x,y,t)\) where \(x\) and \(y\) are coordinates on the screen and \(t\) is a timestamp. In Table 3 we provide statistics on number of strokes, points, and duration of writing. It's important to note that as inks were collected on different devices, the absolute coordinate values can vary a lot. In human-written data the time information \(t\) always starts from 0 but it is not always the case in the synthetic split. Different samples often have different sampling rates (number of points written in one second) due to the use of different devices (see Figure 4). More details in Section 3.3. Consequently, the same ink written on two different devices can result in inks with a different number of points. For human-written inks, the sampling rate is consistent between strokes, but it is not the case for synthetic ones. In order to accommodate a model and make sequences shorter, inks can be resampled in time (see example in Figure 13, Appendix F).

    & train & synthetic & valid & test \\  train & - & 0 & 3.6k & 355 \\ synthetic & 0 & - & 0 & 0 \\ valid & 3.6k & 0 & - & 239 \\ test & 355 & 0 & 239 & - \\   

Table 2: Counts of unique labels shared between MathWriting splits

Figure 3: Histogram of the top-100 most frequent tokens in MathWriting.

### Devices Used

Around 150 distinct device types have been used by contributors. In most cases inks were written on smartphones using a finger on a touchscreen. However, there are cases where tablets with styluses were used. The main device used in this case is Google Pixelbook, which accounted for 51k inks total (see Table 7, Appendix F). Out of all device types, 37 contributed more than 1000 inks. Note that writing on a touchscreen with a finger or a stylus results in different low-level artifacts. All devices were running the same Android application for ink collection, regardless of whether their operating system was Android or ChromeOS.

### Comparison With CROHME23

In this section we compare main dataset statistics of MathWriting and CROHME23  as it is a popular publicly available dataset for HME recognition. In terms of overall size, MathWriting has nearly 3.9 times as many samples and 4.5 times as many distinct labels after normalization, see Table 4. A significant number of labels can be found in both datasets (47k), but the majority is dataset-specific. This suggests that combining both datasets during training could yield improved HME recognition quality. MathWriting has more human-written inks than CROHME23 as seen in Table 5, and contains a much larger variety of tokens. It has 254 distinct tokens including all Latin capital letters and almost the entire Greek alphabet. It also contains matrices, which are not included in CROHME23. Therefore, more scientific fields like quantum mechanics, differential calculus, and linear algebra can be represented using MathWriting.

## 4 Experiments

### Evaluation setup

We propose the following evaluation setup based on MathWriting for the quality of handwriting math expression recognition.

* **evaluation samples**: the test split of MathWriting.
* **metric**: character error rate (CER) , where a "character" is a LaTeX token as defined by the code in Appendix I.

    &  &  \\  Inks & 650k & 164k & 0 \\ Labels & 457k & 102k & 47k \\ Vocab & 254 & 105 & 104 \\    
    &  &  \\  human & 253k & 17k \\ synthetic & 396k & 147k \\   

Table 4: Counts of inks, distinct labels and distinct tokens used in MathWriting and CROHME23. The single token present in CROHME23 but not in MathWriting is the literal dollar sign \(\$\).

Figure 4: Left: an ink with very low sampling rate (9.4 points per second) Right: an ink with very high sampling rate (260 points per second)We provide a reference implementation of the evaluation metric at the Github page 2. We propose the use of CER as a metric to make results comparable to other recognition tasks like text recognition [22; 23], and the use of LaTeX tokens instead of ASCII characters so that an error on a single non-latin letter (e.g.  recognized as a) counts as one instead of many.

### Baseline Recognition Models

In Table 6 we provide results for different models. All models are trained exclusively on the MathWriting dataset (train and synthetic), except for the OCR API that was trained on other datasets as well. The following models represent different approaches to handwriting recognition - offline , online  and mixed .

OcrThis is a publicly available Document AI OCR API , which processes bitmap images. It has been trained partly on samples from MathWriting. We sent inks rendered with black ink on a white background and searched for optimal image size and stroke width to get the best evaluation result from the model.

CTC TransformerThis model is a transformer base with a Connectionist Temporal Classification loss on top (**CTC**) . It contains 11 transformer layers with an embedding size of 512. We used swish activation function and dropout of 0.15 as those parameters performed best on valid. We train with an Adam optimizer, learning rate of 1e-3, batch size 256 for 100k steps. One training run took 4 hours on 4 TPU v2. We trained from scratch and exclusively on MathWriting (train and synthetic). The model is similar to , replacing LSTM layers by Transformer layers and not using any external language model on top.

VlmWe fine-tuned a large Vision-Language Model PaLI  on MathWriting (train and synthetic). We used the representation proposed in  where an ink is represented as both a sequence of points (similar to CTC Transformer) and its rasterized version (similar to OCR). We train three models with different data shuffling for 200k steps with batch size 128, learning rate 0.3 and dropout 0.2. One training run took 14 hours on 16 TPU v5p. Models were finetuned exclusively on train and synthetic MathWriting data. Overall, it took 2 TPU v2 days and 28 TPU v5p days to run the experiments.

Table 6 shows the evaluation comparison between the three models. The OCR model has no information about the order of writing and speed (offline recognition), which explains its lower performance than methods that take time information into account (online recognition). The two other methods - PaLI and CTC Transformer perform significantly better than OCR. These results show that our dataset can be used to train classical recognition models like CTC transformer as well as more recent architectures like VLM.

Figure 5 shows examples of model mistakes. Two of the main causes of mistakes are confusing similar-looking characters like "z" and "2", and errors in the structural arrangement of the characters, for instance not placing a sub-expression in a subscript or superscript.

## 5 Discussion

### Differences in Writing Style

The number of contributors was large enough that a variety of writing styles are represented in the dataset. An example for different ways of writing letter 'r' can be seen in Figure 6. Additional

   Model & Input & Parameters & CER on valid & CER on test \\  OCR  & Image & - & 6.50 & 7.17 \\ CTC Transformer  & Ink & 35M & 4.52 (0.08) & 5.49 (0.05) \\ PaLI  & Image+Ink & 700M & 4.47 (0.08) & 5.95 (0.06) \\   

Table 6: Recognition results for different models. The evaluation metric is reported on both the valid and test splits.

examples are provided in Figure 7. Similar though less obvious differences exist for other letters. Style differences also show through writing order (example - Figure 14, Appendix G).

### Recognition Challenges

MathWriting presents some inherent recognition challenges, which are typical of handwritten representations. For example, it's not really possible to distinguish these pairs from the ink alone: \} vs\}, and \)\([named]{pgfstrokecolor}{rgb}{0,0,0} @color@gray@stroke{0}@color@gray@fill{0}}}}\)\([named]{pgfstrokecolor}{rgb}{0,0,0} @color@gray@stroke{0}@color@gray@fill{0}}}\)\([named]{pgfstrokecolor}{rgb}{0,0,0}@color@gray@stroke{0}@color@gray@fill{0}}}\)\([named]{to improve this process by modifying the location, size or orientation of bounding boxes prior to generating the synthetic inks. This would soften LaTeX's rigid structure and make synthetic data closer to human handwriting. Another application of these bounding boxes would be to bootstrap a recognizer that would also return character segmentation information. This kind of output is critical for some UI features - for example, editing an handwritten expression.

MathWriting can also be improved by varying the label normalization. Changing it can have different benefits depending on the application, as mentioned above. We provide the source LaTeX string for that reason. Another possible improvement in recognition can come from additional contextual information, for instance the scientific field  that can be added post-hoc. Combining recognizers with a language model  trained on a large set of mathematical expressions would be a step in a similar direction.

## 6 Limitations

A single sample in MathWriting dataset has one handwritten LaTeX formula, see Figure 2. As a result, models that are trained on this dataset would probably perform poorly on complete handwritten documents, such as the IAMonDo dataset . Also, as the dataset contains only LaTeX expressions, it is unlikely that models trained on it will accurately recognize handwritten text in English or other languages. As shown in Figure 3, some LaTeX tokens are way more frequent than others. Some infrequent tokens like \(|ni|\) could be hard to recognise.

## 7 Conclusion

We introduced MathWriting, the largest dataset of online handwritten mathematical expressions to date, together with the experimental results of three different types of models. We hope this dataset will help advance research in both online and offline mathematical expression recognition. Additionally, we invite data practitioners to build on the dataset. We intentionally chose a file format for MathWriting close to the one used by CROHME to facilitate their combined use. We also provided original or intermediate representations (raw LaTeX strings, bounding boxes) to enable experimentation with the data itself, and suggested a few directions (Section 5.3).

Figure 8: Left: character ambiguity. Is it \(1 x_{n}<x_{n+1}\) or \(1 n_{}<n_{+1}\)? Right: what is the fraction nesting order?