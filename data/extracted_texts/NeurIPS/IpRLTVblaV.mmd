# Gaussian Process Thompson Sampling via Rooftfinding

Taiwo A. Adebiyi

University of Houston

taadebiyi2@uh.edu

&Bach Do

University of Houston

bdo3@uh.edu

&Ruda Zhang

University of Houston

rudaz@uh.edu

###### Abstract

Thompson sampling (TS) is a simple, effective stochastic policy in Bayesian decision making. It samples the posterior belief about the reward profile and optimizes the sample to obtain a candidate decision. In continuous optimization, the posterior of the objective function is often a Gaussian process (GP), whose sample paths have numerous local optima, making their global optimization challenging. In this work, we introduce an efficient global optimization strategy for GP-TS that carefully selects starting points for gradient-based multi-start optimizers. It identifies all local optima of the prior sample via univariate global rootfinding, and optimizes the posterior sample using a differentiable, decoupled representation. We demonstrate remarkable improvement in the global optimization of GP posterior samples, especially in high dimensions. This leads to dramatic improvements in the overall performance of Bayesian optimization using GP-TS acquisition functions, surprisingly outperforming alternatives like GP-UCB and EI.

## 1 Introduction

Bayesian optimization (BO) is a highly successful approach to the global optimization of expensive-to-evaluate black-box functions . Effectively, there are two nested iterations in BO: the outer-loop seeks to optimize the objective function \(f()\); and the inner-loop seeks to optimize the acquisition function \(()\) at each stage. The premise of BO is that the inner-loop optimization can be solved accurately and efficiently, so that the outer-loop proceeds informatively with a negligible added cost. In fact, the convergence guarantees of many BO strategies assume _exact_ global optimization of the acquisition function [2; 3; 4]. However, this is more challenging than commonly assumed .

Gaussian process Thompson sampling (GP-TS)  uses posterior samples directly as acquisition functions. It has strong theoretical guarantees , scales to high dimensions , and can be easily parallelized [8; 9; 6]. It is also used in computing information-theoretic acquisition functions such as predictive entropy search  and max-value entropy search . But posterior sample functions are notoriously difficult to optimize due to their complexity.

We present an efficient strategy that globally optimizes GP-TS acquisition functions by judiciously selecting starting points for gradient-based multi-start optimizers. It exploits the separability of multivariate GP priors and the decomposition of GP posterior samples per "Matheron's rule" . The former allows for the identification of all local minima of a GP prior sample using a robust rootfinding algorithm; the latter links the prior sample and the data to a posterior sample, relates their critical points, and facilitates the selection of starting points.

## 2 Spectral Representation of Gaussian Processes

**Gaussian Processes.** Given a training dataset \(=\{(,)\}=\{(^{i},y^{i})\}_{i=1}^{N}\), where \(^{i}\) is an input location and \(y^{i}\) is the corresponding observation. Define an observation model \(y()=f()+\)where \(f()\) is an unknown objective function and \(}}{{}}(0,_{}^ {2})\) is independent and identically distributed (iid) zero-mean Gaussian noise.

A GP assumes that any finite subset of function values has a joint Gaussian distribution . This assumption is encoded in the GP prior \(f()(m(),(,^ {}))\), where \(m()\) is the mean function and \((,^{})\) a positive definite covariance function. Conditioning the GP prior on the data provides a posterior that is also a GP. Assuming \(m()=0\), the GP posterior can be written as \(f(_{})|((_{}),(_{},^{}_{}))\). Here the posterior mean \((_{})=^{}(_{}, )^{-1}\) and the posterior covariance \((_{},^{}_{})=( _{},^{}_{})-^{}( _{},)^{-1}(^{}_{ },)\), where \(=+_{}^{2}\) with \(_{ij}=(^{i},^{j})\) (\(i,j\{1,N\}\)), and \((_{},)=[(_{}, ^{1}),,(_{},^{N})]^{}\).

**Spectral Representation of Gaussian Processes.** Per Mercer's theorem on probability spaces , any positive definite covariance function that is essentially bounded with respect to some probability measure \(\) on the domain \(\) has a spectral representation \((,^{})=_{k=0}^{}_{k}_{k}( )_{k}(^{})\), where \((_{k},_{k}())\) is a pair of eigenvalue and eigenfunction of the kernel integral operator. A sample function from the GP prior can thus be written as \(f()=_{i=0}^{}w_{k}}_{k}()\), where \(w_{k}}}{{}}(0,1)\).

**Decoupled Representation of GP Posteriors.** Given a GP prior sample \(f()\), we can determine a posterior sample \(()\) using a decoupled representation that updates the prior sample according to Matheron's rule . For observations contaminated by iid Gaussian noise, we have \((_{})=f(_{})+^{ }(_{},)^{-1}(-- )\), where \(=[f(^{1}),,f(^{N})]^{}\) and \(}}{{}}_{ N}(,_{}^{2}_{N})\).

## 3 Global Optimization to Thompson Sampling Acquisition Functions

**Assumptions.** We use a GP prior with a separable covariance function. We require that the univariate components of this covariance function either have known spectral representations per Mercer's theorem, or have known spectral densities per Bochner's theorem with an effective discretization (see e.g., ), and that the corresponding samples are continuously differentiable. The squared exponential (SE) covariance function meets these requirements and is of our focus in the following. We further assume that the objective function is defined on a hypercube \(=_{i=1}^{d}[_{i},_{i}]\).

**Spectrum of SE Covariance Function.** Consider the univariate SE covariance function \((x,x^{};l)=(-(x-x^{})^{2}/l^{2})\), where \(l\) is the characteristic length scale. Per Mercer's theorem, it has a spectral representation \((x,x^{})=_{k=0}^{}_{k}_{k}(x)_{k}(x^{ })\). For a Gaussian measure \(=(0,^{2})\) over the real line, let \(a=(2^{2})^{-1}\), \(b=(2l)^{-1}\), \(c=+4ab}\), and \(A=a+b+c\). For \(k\), the \(k\)th eigenvalue is \(_{k}=(b/A)^{k}\) and a corresponding eigenfunction is \(_{k}(x)=( c/a)^{1/4}_{k}(x)( ax^{2})\), where \(_{k}(x)=(^{1/2}2^{k}k!)^{-1/2}H_{k}(x)(-x^{2})\) and \(H_{k}(x)=(-1)^{k}(x^{2})}{dx^{k}}(-x^{2})\) is the \(k\)th Hermite polynomial (see e.g.,  Sec. 4).

**Prior Sample Functions.** The separability of the covariance function and the known spectral representations of the component covariance functions allow us to accurately approximate the prior sample as \(f()_{i=0}^{d}_{k=1}^{N_{i}-1}w_{i,k}}(x_{i})\). Here \(N_{i}\) is selected for each variate such that \(_{i,N_{i}-1}/_{i,1}_{i}\), where \(_{i}\) is sufficiently small, e.g., \(_{i}=10^{-16}\).

**Properties of Posterior Sample Functions.** With the decoupled representation of GP posteriors, each posterior sample has the form of \(()=f()+b()\). Here, the prior sample \(f()=_{i=1}^{d}f_{i}(x_{i})\) is fast-varying and separable, enabling the use of efficient univariate root-finding algorithms  to identify all its critical points. The data adjustment \(b()=_{j=1}^{N}v_{j}(,^{j})\), where \(v_{j}\), is a weighted sum of canonical basis functions. While not separable, it is smoother, has much fewer critical points than the prior sample, and has limited effect away from data points.

**Critical Points of Multivariate Separable Functions.** The critical points of the multivariate separable prior sample \(f()=_{i=1}^{d}f_{i}(x_{i})\) are exactly the critical points of its univariate components \(f_{i}(x_{i})\), arbitrarily combined, except for when \(f()=0\). As a result, we can find all the relevant critical points of the prior sample function \(f()\) by solving a global rootfinding problem for the derivative of each of its univariate components: \(f_{i}^{}(x_{i})=0\), \(i\{1,,d\}\). We also add the upper and lower bounds of each variable \(x_{i}\) to the set of critical points of \(f_{i}\), as they can define the extrema of \(f()\) on the bounded domain. Let \(\{_{i,j}\}_{j=1}^{r_{i}}\) represent the set of critical points of \(f_{i}(x_{i})\).

**Local Minima of Multivariate Separable Functions.** Given the univariate critical points, identifying a local minimum of a multivariate separable prior sample \(f()\) is straightforward. Let \(=(_{i,s(i)})_{i=1}^{d}\) be an arbitrary combination of the univariate critical points, where \(=(s(i))_{i=1}^{d}\) is a multi-index. If \(\) is an interior point, it is a local minimum if \(^{2}f() 0\), which has the form \(^{2}f()=\{_{i j}f_{j}(x_{j})f_{i} ^{}(x_{i})\}_{i=1}^{d}\). If \(\) is a boundary point, the criterion is slightly modified. Without enumerating all combinations, best subsets of the local minima \(_{}\) can be efficiently identified as follows: (1) filter the critical points for local extrema, exploiting the structures of \(^{2}f()\) and \( f()\); (2) select the local extrema with the largest \(|f|\), using a max heap data structure; and (3) local minima have negative \(f\) values.

**Global Optimization to Thompson Sampling.** To globally optimize a GP-TS acquisition function in each BO iteration, we use two sets of starting points for a gradient-based multi-start optimizer, namely exploitation \(_{}\) and exploration \(_{}\). Here \(_{}\) contains the data points, while \(_{}\) is either \(_{}\), or a subset of \(_{}\) when the number of its members is too large, e.g., \(>1000\). Figure 1 illustrates the exploration and exploitation sets for global optimization of two GP-TS acquisition functions.

Our optimization strategy is motivated by a few observations. When the prior sample \(f\) is added to the smoother landscape of the data adjustment \(b\), each local minimum of \(f\) will be located nearby a local minimum of the posterior sample \(\). Searching from data points can discover good local minima of \(\) in the vicinity of the data set, which can pickup some local minima not readily discovered by the local minima of \(f\). This is especially true if \(f\) is relatively flat near a data point. Starting from both \(_{}\) and \(_{}\) can thus give sufficient coverage of the best local minima of \(\), leading to efficient global optimization of GP-TS acquisition functions.

## 4 Experiments

**Inner-Loop Optimization.** We minimize the 2d Schwefel and 10d Levy functions  to assess the quality of the inner-loop solutions recommended by our proposed method. We start with \(10d\) data points, normalize the input data to \([-1,1]^{d}\), and standardize the output data. We set \(=1\) and \(_{}=10^{-6}\). For comparison, we optimize the same GP-TS acquisition functions using a gradient-based multi-start optimizer with random starting points (i.e., random multi-start) and a genetic algorithm. The number of starting points for the random multi-start and the population size of the genetic algorithm are equal to the number of starting points of our method. The same stopping criteria are used for the three algorithms.

Figure 1: Illustration of exploration and exploitation sets for global optimization of GP-TS acquisition functions. (a) When the global minimum of the GP-TS acquisition function lies outside the interpolation region, it is typically identified by starting the gradient-based optimizer at a local minimum of the prior sample. (b) When the global minimum is within the interpolation region, it can be found by starting the gradient-based optimizer at either a data point or a local minimum of the prior sample.

Figure 2 compares the inner-loop solutions and CPU times for inner-loop optimization by our method, random multi-start, and genetic algorithm. For both problems, our method outperforms the random multi-start and genetic algorithm in terms of the inner-loop solution quality and the run time required for optimizing GP-TS acquisition functions. This verifies the importance of judicious selection of starting points for optimizing GP-TS acquisition functions in both low and high dimensions.

**Outer-Loop Optimization.** We compare the outer-loop solutions by our method with those by other BO methods, including TS with random Fourier features (TS-RF) [18; 10], expected improvement (EI) , and lower confidence bound (LCB)--the version of GP-UCB  for minimization. The inner-loop optimization for TS-RF, EI, and LCB is performed via a gradient-based multi-start optimizer with random starting points. The number of starting points and the termination criteria for this optimizer are the same as those for our method. In each BO iteration, we record the log simple regret \((y_{}-f^{})\), where \(y_{}\) is the best observation up to that iteration and \(f^{}\) is the true minimum of the objective function.

Figure 2 shows the medians and interquartile ranges of solutions from 20 runs of each BO method for the test functions. On the 2d Schwefel function, our method can achieve better objective function values than all other considered methods. It also provides a competitive result in optimizing the 10d Levy function. Across the two examples, EI and LCB tend to perform well in the initial iterations, while our method shows fast improvement in later iterations, highlighting the exploratory nature and delayed reward of the GP-TS policy. Considering robustness to the objective function, GP-TS (perhaps surprisingly) outperforms EI and LCB, when optimized using our method.

## 5 Summary

We propose a method to optimize GP-TS acquisition functions globally. The method relies on local minima of prior samples obtained from a univariate rootfinding algorithm, the data points, and a gradient-based multi-start optimizer with carefully selected starting points. Its effectiveness is supported by the prevalent use of separable covariance functions in BO, where the univariate covariance components is expressed in terms of their spectral representations. The optimization results show that the proposed method offers higher-quality solutions to optimizing GP-TS acquisition functions in both low- and high-dimensional settings, compared to a random multi-start and a genetic algorithm. It also shows dramatic improvements in outer-loop optimization.

Figure 2: Optimization results for (a) 2d Schwefel and (b) 10d Levy functions. _Top-left_: Cumulative distances between new candidate solutions \(_{k}^{}\) to the true global minimums \(_{k}^{}\) of the GP-TS acquisition functions \(()\) for 2d Schwefel function. _Bottom-left_: Cumulative optimized values \(_{k}^{}\) for 10d Levy function. _Middle_: Cumulative run time \(t_{k}\) required for optimizing \(()\). _Right_: Histories of medians and interquartile ranges of solutions from 20 runs of our method, TS-RF, EI, and LCB.