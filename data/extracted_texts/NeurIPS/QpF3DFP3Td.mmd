# Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?

[MISSING_PAGE_FAIL:1]

(nDTM), encompassing over 3.5 billion pixels with RGB values, nDTM elevation, and semantic annotations.

Traditionally, U-Net models  have dominated archaeological studies. In this paper, we evaluate several recent architectures for semantic segmentation. Our findings indicate that identifying ancient features beneath vegetation canopies using ALS still poses significant challenges. These difficulties can be attributed to the subtle nature of the objects sought, which are largely represented by faint elevation patterns. Moreover, certain features can span several kilometers and require extensive spatial context to disambiguate. With Archaeoscope, we aim to challenge the machine learning and computer vision communities to address the rich, impactful, and unsolved problem of ALS-based archaeology. At the same time, we also encourage the archaeological community to adopt open-access policies and explore modern deep learning approaches.

## 2 Related work

In this section, we explore the advantages of Archaeoscope over existing datasets, highlighting its larger scale and open-access policy (Section 2.1), and present the different models evaluated in our benchmark (Section 2.2).

### ALS archaeology datasets

Deep learning for ALS archaeology is a dynamic field . In Table 1, we list the main deep learning works on ALS-archaeology. Archaeoscape is not only one of the few open-access datasets available but also the largest and most comprehensively annotated by a significant margin. 1

Open-access policies.ALS archaeology datasets typically withhold data, annotations, and code due to legitimate concerns about misuse [11; 12], and the absence of established open-access norms in archaeology. However, recognizing the critical role of reproducibility and open access in science, we make Archaeospace accessible to academic researchers. We implement strict safeguards to protect sensitive archaeological information, as described in Section 3.1.

Scope and extent.Archaeoscape is the largest ALS archaeology dataset in terms of area covered (\(888\) km\({}^{2}\)) and number of annotated instances (31,411). Archaeoscope covers a 2\(\) larger surface area and contains 3\(\) more instances than the next-largest closed archaeology LiDAR dataset (see Table 1). It is also the first such dataset related to the Khmer civilization of Southeast Asia.

Figure 1: **Archaeoscape. Our proposed dataset contains 888 km\({}^{2}\) of aerial laser scans taken in Cambodia. The 3D point cloud LiDAR data (left) was processed to obtain a digital terrain model (middle). Archaeologists have drawn and field-verified 31,411 individual polygons by delineating anthropogenic features (right).**

### Semantic segmentation with deep learning

ALS archaeology approaches rely predominantly on U-Net-based models . However, the field of semantic segmentation has evolved considerably since its introduction in 2015. We propose to assess the performance of an array of contemporary, state-of-the-art models on the Archaeoscope benchmark. Models and pretraining strategies evaluated in Table 2 are denoted in **bold** throughout the text for clarity and ease of reference.

Convolution-based models.Convolutional Neural Networks (CNNs) [23; 24], and the **U-Net** architecture in particular, remain the predominant choice for dense prediction tasks across various application fields due to their simplicity and effectiveness. **DeepLabv3** improves on this model by using dilated convolution and Spatial Pyramid Pooling  to learn multiscale features.

Vision transformers.Vision transformers harness the versatility and expressivity of transformers  to extract rich image features. The Vision Transformer **ViT** model splits the images into small patches, which are embedded with a linear layer, while the final patch encodings are converted into pixel prediction with another linear layer. **DOFA** embed each input channel conditionally to its wavelength, allowing generalization to new sensors. Alternatively hybrid **HybViT** replaces these linear layers with a combination of convolutional and deconvolutional layers for encoding and decoding patches, respectively. This adaptation is particularly effective on smaller datasets, as the convolutions help capture local feature dependencies more effectively.

Hierarchical ViTs.Several variants of the ViT model use a hierarchical approach to effectively capture spatial features with a large context. The Pyramid Vision Transformer (PVT)  applies its attention mechanism according to a nested hierarchical structure, while **SWIN** uses overlapping windows of increasing sizes. Building on these concepts, **PCPVT** introduces a conditional relative position encoding mechanism, and **PVTv2** also allows for overlapping patches.

Pre-training strategies.Recent advances in self- and weakly-supervised learning have profoundly impacted the efficacy of neural networks. These strategies often use large datasets with text annotations such as **CLIP-OPENAI** or its open-source counterpart **CLIP-LAION2B**. Alternatively, **DINOv2** learns from large, unannotated image datasets. The recent Masked Auto-Encoder  tunes large models by using the pretext task of masked patch reconstruction. This approach has been adapted to address the specific needs of aerial imagery, leading to variants such as **ScaleMAE** which are trained on satellite images.

    & open- & hi-res & location & extent & resolution & number of \\  & access & RGB & & in km\({}^{2}\) & in meters & instances \\  Arran  & ✔ & ✗ & United Kingdom & 25 & 0.5 & 772 \\ Litchfield  & ✗ & ✗ & USA & 50 & 1 & 1,866 \\ Puuc  & ✗ & ✗ & Mexico & 23 & 0.5 & 1,966 \\ AHN  & ✗ & ✗ & Netherlands & 81 & 0.5 & 3,553 \\ AHN-2  & ✗ & ✗ & Netherlands & 437 & 0.5 & 3,849 \\ Connecticut  & ✗ & ✗ & USA & 353 & 1 & 3,881 \\ Dartmoor  & ✗ & ✗ & United Kingdom & 12 & 0.5 & 4,726 \\ Pennsylvania  & ✗ & ✗ & USA & 4 & 1 & 4,376 \\ Uaxactun  & ✗ & ✗ & Guatem & 160 & 1 & 5,080 \\ Chactin  & ✗ & ✗ & Mexico & 230 & 0.5 & 10,894 \\ 
**Archaeoscape (ours)** & ✔ & ✔ & Cambodia & **888** & 0.5 & **31,411** \\   

Table 1: **ALS archaeology datasets.** Archaeospace is the first open access ALS archaeology dataset to cover Southeast Asia, and to provide high resolution aerial photo imagery. It is also significantly more extensive than existing datasets in terms of surface area and number of annotated instances.

## 3 Archaeoscape

In this section, we describe the content of Archaeoscope (Section 3.1), as well as its acquisition process (Section 3.2).

Context.Angkor, the heart of the medieval Khmer Empire, is often referred to as a "hydraulic city" due to its extensive water management infrastructure. This system allowed the Khmer to thrive in a challenging environment, oscillating between monsoon and dry seasons, from the 9th to the 15th century. Today, much of the built environment of Angkor and the other cities of this period has disappeared, as virtually all non-religious architecture was built using perishable materials such as wood. What remains is often hidden by dense vegetation or damaged by erosion and modern agricultural practices, rendering these sites nearly invisible at ground level, so that even experts might walk over such sites without realizing it. However, the advent of LiDAR (Light Detection and Ranging) technology has been transformative, uncovering distinct, often geometric patterns in the topography indicative of ancient occupation and landscape alteration. By combining careful analysis of ALS imagery with targeted ground surveys, this decade-long project has documented tens of thousands of ancient Khmer features, many previously undiscovered, providing a new and expanded perspective on the history of the region.

### Dataset characteristics

Splits.As shown in Figure 2, the dataset consists of 23 non-overlapping parcels of varying size, ranging from \(2\) to \(183\) km\({}^{2}\), and include archaeologically relevant areas such as ancient temples, cities, and roadways. We present the splits for Archaeoscope's training (\(623\) km\({}^{2}\), \(16\) parcels), validation (\(97\) km\({}^{2}\), \(3\) parcels), and test (\(168\) km\({}^{2}\), \(4\) parcels) sets. The splits were chosen to respect the global distribution of features and landscapes: densely or scarcely occupied regions, hills or floodplains, large-scale hydraulic engineering sites, monumental temples, and _subtle_ carthen features.

Under these constraints, splitting the dataset into spatially distinct regions--as is commonly done in geospatial machine learning--proved impractical. To prevent data contamination all parcels are separated by a least a \(100\) meter buffer. The test set consists of \(2\)_remote_ parcels, set apart from the others by more than \(5\) km, and \(2\) parcels _adjacent_ to training and validation sets, covering two

Figure 2: Archaeoscape overview. We show the vectorial annotations overlaid onto the relative elevation maps for each parcel, and their assignment to the training, validation or test splits. The position and orientation of the parcels is arbitrary. The geometry of the annotations has been simplified to reduce the file size of the paper. Best viewed on a computer screen.

use cases: predicting features in a new area under a domain shift, and a realistic scenario in which archaeologists annotate part of an area of interest and train a model to pre-segment the rest.

Misuse prevention.There is a valid concern that large-scale annotated ALS data could be misused by malicious actors, leading to the targeted looting or destruction of historical sites . The potential for misuse has been a significant factor in the lack of public availability of archaeological datasets. To mitigate this risk and alleviate the concerns of local stakeholders, we propose several measures to balance the benefits of open access with the legal and practical protection of cultural heritage sites:

* **Data partitioning:** The data is divided into parcels and stripped of georeferencing and absolute elevation information to prevent spatial identification of remote, less well-known sites. While famous temples such as Angkor Wat may be recognizable, they are already under close protection by the local authorities.
* **Custom license:** The dataset is distributed under a license which forbids re-georeferencing, commercial use, and redistributing the data beyond the intended users.
* **Open credentialized access:** Access to the dataset requires signing a data agreement form, which holds users legally accountable for misuse. The Appendix contains more details about the license, data access, and the distribution agreement.

Dataset format.We distribute the data as GeoTIFF files with a \(0.5\) m resolution and polygon annotations in the GeoPackage format. We associate each pixel with the following values:

* **Radiometry:** RGB values obtained from contemporary orthophotography.
* **Ground elevation:** Digital Terrain Model (DTM) obtained with ALS, see Section 3.2.
* **Semantic label:** One-hot encoding of the five classes described below.

Annotation.One of the most significant undertakings of Archaeoscape is the meticulous annotation by experts, who have individually traced and field-verified a wealth of archaeological features. The annotators employed a granular classification system with 12 feature types. However, to mitigate severe class imbalance and reduce ambiguity, we have streamlined this system into a more manageable

Figure 3: **Archaeoscape classes. We illustrate the three main classes with in-situ images (top row), top-view hillshaded elevation maps (middle row), and our annotations (bottom row). In many cases, the sought features are difficult to detect visually by in-situ observation but are more apparent on elevation maps.**

5-class nomenclature, represented in Figure 3. We explain these classes below and provide, where applicable, the number of instances and pixel frequency:

* **Temple (827, 0.2%).** Quintessential to the Cambodian landscape, these edifices stand as the most iconic remnants of the Angkorian civilization. The scale of these temples ranges from the monumental Angkor Wat, spanning over one hundred hectares, to much smaller sites marked by little more than a scattering of bricks or stone blocks.
* **Mound (14,400, 8.6%).** Mantiesting as slight elevations, these artificial earthen features are indicative of a range of human activities. They include habitation and crafting sites, as well as the embankments of canals and reservoirs. Although very numerous, mounds are often concealed by dense vegetation or too subtle to be easily detectable on the ground.
* **Hydrology (16,184, 10.4%).** This class groups together various features of Khmer hydrogenineering such as rivers, canals, reservoirs that can reach several kilometers in width, and smaller artificial ponds. These features highlight the Angkorian civilization's significant investment in water management and have long been of particular interest to archaeologists.
* **Void (3,145, 2.5%).** This annotation is reserved for areas that are considered ambiguous by expert annotators and for structures excluded from the analysis presented in this paper. Void pixels are removed from supervision and evaluation.
* **Background (78.3%).** This category encompasses everything else: regions that lack particular archaeological features or are obscured by modern development. Background includes a wide array of non-archaeological elements such as modern agricultural plots and infrastructure.

While the annotations are created and distributed as polygons, we treat them as pixel labels, framing the problem of detecting archaeological features as a conventional semantic segmentation task.

### Acquisition and processing

Acquisition.ALS and orthophotography imagery was obtained during the KALC (2012)  and CALI (2015)  acquisition campaigns in Cambodia, from which a subset of 888 km\({}^{2}\) was selected, as described in Section 3.1, corresponding to over 13,000 aerial photos and 10 billion 3D points, with a density of 10-95 points per m\({}^{2}\), depending on the terrain.

The data was acquired with Leica LiDAR (ALS60 for KALC, ALS70-HP for CALI) and cameras (RCD105 and RCD30). The instruments were mounted on a pod attached to the skid of a Eurocopter AS350 B helicopter flying at 800 m above ground level as measured by an integrated Honeywell CUS6 IMU, and positional information was acquired by a Novatel L1/L2 GPS antenna. GPS ground support was provided by two Trimble R8 GNSS receivers.

Preprocessing._Non-terrain_ points (_i.e_. corresponding to tree canopies, modern buildings) are removed from ALS points with the Terrasolid software . We form a DTM by fitting a triangular irregular network  to the remaining points and linearly interpolating the ground point elevation values within each triangular plane on a \(0.5\) meter grid. The photos are orthorectified and resampled to the same \(0.5\) meter resolution.

Annotation.The endeavor to map Khmer archaeological features has a long history, tracing back to the 19th century, with significant advancements following the availability of aerial imagery in the 1990s . Our annotation process builds upon this historical groundwork, but mostly leverages the LiDAR data collected in 2012 and 2015. Our approach relies on an iterative process of manual annotation using a Geographic Information System, QGIS, and targeted ground survey to verify features in the field. These mapping and verification efforts were performed by a shifting team of archaeologists, both local and foreign, who collectively contributed to the analysis and validation of the data. The first pre-LiDAR surveys date back to 1993, and work continued until 2024.

## 4 Benchmark

In this section, we assess the performance of modern semantic segmentation methods for ALS archaeology. We first detail how we adapt and evaluate these methods (Section 4.1), then present our results and analysis (Section 4.2), and an ablation study (Section 4.3). Finally, we discuss the limitations of our approach (Section 4.4).

### Baselines and metrics

We formulate the problem of finding archaeological features as a semantic segmentation task, and benchmark several backbone networks on our dataset.

Metrics.We evaluate the prediction of the models with the overall accuracy (OA), class-wise Intersection over Union (IoU), and the unweighted mean of the IoUs (macro-average). For the evaluation, we exclude pixels annotated with the void label.

Implementation details.We train the evaluated models using the configurations of the official open-source repository and provide more details in the supplementary materials. The predictions on the test set are performed along a grid corresponding to the input size and with \(25\)% overlap on each side. Only the central portion of each prediction is kept while the border predictions are discarded.

We use a combination of internal clusters and the HPC GENCI to run our experiments. Reproducing the entire benchmark requires 260 GPU-h with A100 GPUs. We estimate the total cost of our hyperparameters search and initial experiments at 1100 GPU-h.

Adapting baselines.To evaluate the performance of modern vision models for ALS archaeology, we adapt several semantic segmentation models to our setting. The changes are minimal:

* **Inputs.** Beyond radiometry (RGB), we also incorporate ground elevation derived from the ALS data described in Section 3.2. As we consider networks trained on natural images, we modify the first layer to accommodate an extra band and initialize the additional weights randomly according to \((0,0.01)\).
* **Segmentation head.** For all transformer-based methods, we map the final patch embeddings to pixel-level prediction with linear layers, except for **HybViT** which uses transposed convolutions. For CNNs, we use their dedicated segmentation heads, which we initialize randomly.
* **Pre-training and fine-tuning.** We consider models pre-trained on ImageNet1K  and ImageNet21K , but also foundation vision models trained on large external datasets: DINOv2 , CLIP-OPENAI  and LAION-2B , and Earth observation datasets .

### Results

We report the quantitative performance of various state-of-the-art semantic segmentation models in Table 2, and provide qualitative examples in Figure 4.

Figure 4: **Qualitative performance. We provide examples of input elevation maps (a) and their corresponding annotations (b), as well as the prediction of a standard U-Net (c) and our best model (d)—improvements in green. The red squares represent the size of the input images: 224 pixels, or 112m.**Influence of the backbone.Surprisingly, CNN-based methods such as **U-Net** outperform most ViTs on our dataset. We attribute this result to **ViTs**' reliance on extensive pre-training on RGB images. In our data, the most informative channel is the elevation rather than RGB, as the radiometric information is typically blocked by the dense canopy cover. Indeed, and as shown in Section 4.3, models trained on RGB all perform below \(30\%\) mIoU. This distinction may explain why foundation models renowned for their effectiveness on natural images, such as **DINov2** or **CLIP**, fail to adapt to this new setting. Even **ScaleMAE** and **DOFA**, which are pre-trained on large amounts of satellite imagery, lead to poor performances.

The hybrid ViT model **HybViT**, which uses convolutions for patch encoding and decoding, performs better. This suggests that integrating local feature processing (typical of CNNs) with a global perspective (a strength of ViTs) is beneficial for interpreting archaeological ALS data. This analysis is further supported by the relatively high performance of hierarchical ViT models, which even surpass CNNs in some cases. We hypothesize that the hierarchical structure of these models aligns well with the dual requirement of our task: to recognize local patterns and to integrate them within a broader spatial context. This capability is particularly advantageous for detecting archaeological features, which often consist of both small objects and expansive, interconnected structures.

Influence of the input size.In our experiments, we use the default size of ViTs in all experiments: 224 pixels, equivalent to 112 meters. However, the Archaeospace dataset includes structures such as basins spanning several kilometres, and which can only be detected with a larger context. When scaling our input size to \(512\), we noted a significant improvement in performance, especially with the **U-Net** model. Attempts to further increase the input size did not yield additional performance gains, as the models quickly overfit to the training set.

Qualitative analysis.As depicted in the top row of Figure 4, models trained on our data can detect complex structures, such as the central grid inside the temple moat and the maze-like features outside. However, they miss the broader semantic context, _e.g_. finding the prominent temple walls while

    &  &  &  &  &  \\  &  &  & **avg** & temple & hydro &  &  \\   ViT} & U-Neta  & ImageNet1K & 224 & 50.5 & 33.3 & 32.7 & 48.6 & 87.6 & 88.2 \\  & DeepLabv3b  & ImageNet1K & 224 & 47.6 & 19.8 & 35.9 & 47.5 & 87.2 & 87.8 \\   & ViT-Sc  & ImageNet21K & 224 & 46.4 & 18.5 & 33.3 & 46.6 & 87.0 & 87.5 \\  & ViT-Sc  & DINov2 & 224 & 41.9 & 14.5 & 26.1 & 40.9 & 86.2 & 86.7 \\  & ViT-Bd  & CLIP & 224 & 30.3 & 3.4 & 15.8 & 30.3 & 83.1 & 83.4 \\  & ViT-Bd  & LAIION2B & 224 & 32.4 & 2.8 & 14.4 & 28.2 & 84.3 & 84.6 \\  & ViT-Le  & ScaleMAE & 224 & 30.4 & 0.0 & 16.0 & 22.8 & 82.7 & 82.8 \\  & HybViT-Sc  & ImageNet21K & 224 & 50.4 & 32.4 & 33.6 & 48.0 & 87.5 & 88.1 \\  & DOFA f  & DOFA & 224 & 39.6 & 13.4 & 25.9 & 33.6 & 85.5 & 86.0 \\   ViT} & SWIN-Sc  & ImageNet1K & 224 & 51.9 & 33.1 & 35.2 & [51.4] & 88.0 & 88.6 \\  & SWIN-Bg  & SatLas & 224 & 49.6 & 28.2 & 34.0 & 48.4 & 87.7 & 88.3 \\  & PCPVT-Sc  & ImageNet1K & 224 & 51.7 & [33.4] & 35.0 & 50.6 & 88.0 & 88.5 \\  & PVTv2-b1c  & ImageNet1K & 224 & **52.1** & 32.3 & **36.4** & **51.4** & **88.2** & **88.7** \\   ViT} & U-Neta  & ImageNet1K & 512 & 52.8 & 31.8 & [39.7] & 50.7 & [89.1] & [89.6] \\  & PVTv2c  & ImageNet1K & 512 & 52.2 & 28.3 & 38.0 & 53.0 & 89.4 & 89.9 \\   

Table 2: **Semantic segmentation benchmark.** We evaluate an array of pre-trained models fine-tuned on Archaeoscope. We first consider models with the same input size of \(224 224\), then present report performance for \(512 512\). We group models as CNNs, Vision Transformers (ViT), and hierarchical vision transformers (HiViT). We **bold** the best performance for an input size of \(224\), and underline the performances within 0.5 point of this score. We [frame] the best overall performance across all resolutions.

failing to segment the platform. In the middle row, the models detect isolated features and temples with the standard "horseshoe" configuration, while the large ponds are mostly missed, likely due to the limited context window size. In the bottom row, the hilly areas with faint feature elevation pose a significant challenge. This highlights the limitations of current models in handling the varying landscape, scale, and semantic context of archaeological features.

Overall performance.The performance across models remains relatively low, especially if compared to results achieved on complex computer vision segmentation benchmarks featuring numerous classes. This suggests that contemporary model architectures may not adequately meet the specific challenges of ALS archaeology, which involves interpreting subtle local elevation patterns within a broader spatial context. Furthermore, foundation models for natural images often fail to adapt to the specificity of the data and the new elevation channel, possibly due to their extensive pre-training. This situation highlights the need for bespoke models specifically tailored for ALS data analysis.

### Ablation study

We evaluate the impact of some of the choices made in the design of Archaeospace through an ablation study.

Channel importance.Airborne LiDAR scans are pivotal for uncovering the subtle elevation patterns of archaeological features like mounds and canals, which are typically not visible in orthophotos, as shown in Figure 5. Moreover, dense canopies can obscure or completely hide radiometric information about the ground. Conversely, in less densely forested areas, orthophotos can capture detailed information about archaeological features, complementing LiDAR data. The ablation study results, documented in Table 4, highlight the limitations of relying solely on RGB data. Models using only RGB information registered a mean Intersection over Union (mIoU) of about 30%, significantly lower than models also utilizing elevation data. This disparity underscores the inadequacy of RGB data under dense canopy coverage. Furthermore, while removing RGB information only moderately affects performance, it particularly affects the detection of temples---some of which are still standing to this day, and are typically not covered by the canopy. The performance gap between models pretrained with DINOV2 and those pretrained on ImageNet widens without RGB, suggesting that DINOV2 models are highly optimized for RGB processing, whereas ImageNet models adapt better to elevation data.

Initialization strategy.Adapting models trained on RGB data to handle elevation channels poses challenges. Our approach, detailed in Section 4, initialize with small values the weights of the first layer corresponding to the new channel while retaining the pre-trained weights for RGB. In Table 4, we evaluate this method against three alternatives: fully random initialization, random initialization of the first layer with other weights retained, and LoRA fine-tuning. Randomly initializing the first layer results in performance akin to training the network from scratch, demonstrating the efficacy of our strategy to leverage pre-existing RGB training.

### Limitations

Archaeoscape presents several limitations as a benchmark that should be considered:

Figure 5: **Channel ablation. We represent the orthophotography (a), normalized terrain model (b), and annotations (c). We also provide the prediction of a PVTv2 model operating on RGB photos (d), and a model processing both RGB and elevation data (e). The model using only radiometric information performs worse overall, and in particular, fails to identify any structures under the heavily forested area at the top left corner.**

* **Domain shift:** The imagery for Archaeoscape has been collected over two campaigns using different equipment. Even with our best efforts to harmonize the dataset and its processing, sensor and meteorological variations may manifest in the data distribution.
* **Annotation errors and ambiguity:** As they were annotated and field-verified by expert archaeologists, we can affirm that the annotated polygons correspond to actual archaeological features with high confidence. However, there is an inevitable degree of ambiguity regarding the precise shape and boundaries of these features, which often consist of very slight relief sloping gradually into the natural terrain. Moreover, we cannot rule out that background terrain may contain some yet uncovered features that would have eluded detection.
* **Cultural specificity:** Archaeoscape aims to serve as a benchmark for vision models for ALS archaeology, but focuses exclusively on the Khmer civilization. We acknowledge that our conclusions may not be universally applicable to other cultural contexts or regions.

## 5 Conclusion

We have introduced Archaeoscope, the largest published dataset for ALS archaeology featuring open-access imagery and annotations. Focused on the ancient Khmer settlement complexes and temples of Cambodia, our dataset covers \(888\) km\({}^{2}\) and comprises 31,144 individual anthropogenic instances. We provide an extensive benchmark evaluating several state-of-the-art computer vision models for detecting archaeological features within elevation maps and images. Despite formulating the problem as a classic semantic segmentation task, we observe that even usually high-performing models struggle to achieve high scores. We attribute this poor performance to the unique challenges of ALS archaeology, such as the subtlety of the patterns sought, and the importance of large-scale context. We hope that our dataset will encourage the computer vision and machine learning community to propose novel solutions for these unresolved challenges.

    & & &  & OA \\   & &  &  &  &  &  \\  Channels & backbone & pretraining & & & & & \\   & U-Net & ImageNet1K & 50.5 & **33.3** & 32.7 & 48.6 & 87.6 & 88.2 \\  & ViT-S & DINOv2 & 41.9 & 14.5 & 26.1 & 40.9 & 86.2 & 86.7 \\  & PVTv2-b1 & ImageNet1K & **52.1** & 32.3 & 36.4 & **51.4** & 88.2 & 88.7 \\   & U-Net & ImageNet1K & 51.2 & 28.8 & **37.8** & 49.8 & **88.3** & **88.9** \\  & ViT-S & DINOv2 & 36.6 & 10.4 & 19.4 & 31.5 & 85.2 & 85.6 \\  & PVTv2-b1 & ImageNet1K & 49.9 & 27.8 & 35.1 & 48.5 & 88.0 & 88.5 \\   & U-Net & ImageNet1K & 34.2 & 1.5 & 22.7 & 29.3 & 83.2 & 83.2 \\  & ViT-S & DINOv2 & 29.0 & 1.6 & 12.6 & 20.1 & 81.6 & 81.4 \\  & PVTv2-b1 & ImageNet1K & 33.9 & 6.0 & 20.6 & 27.0 & 82.0 & 33.9 \\  Initialization & backbone & pretraining & & & & & & \\  Fully random & & & 46.0 & 17.9 & 32.7 & 46.1 & 87.2 & 87.7 \\ Rand. 1st layer & & & 44.4 & 17.5 & 28.2 & 46.1 & 87.2 & 86.5 \\ LoRA (rank 32) & & & 46.1 & 21.8 & 31.9 & 44.4 & 86.2 & 86.6 \\ Proposed & & & **52.1** & **32.3** & **36.4** & **51.4** & **88.2** & **88.7** \\   

Table 4: **Ablation study.** We evaluate the impact of omitting RGB channels or elevation from input images and assess various initialization strategies for fine-tuning networks initially trained only on RGB data to accommodate additional elevation channels E. Performance metrics are highlighted, with the best scores bolded and those within 0.5 points underlined.