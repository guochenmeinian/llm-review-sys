# SEENN: Towards Temporal

Spiking Early-Exit Neural Networks

 Yuhang Li

Yale University

New Haven, CT, USA

yuhang.li@yale.edu

&Tamar Geller

Yale University

New Haven, CT, USA

tamar.geller@yale.edu

&Youngeun Kim

Yale University

New Haven, CT, USA

youngeun.kim@yale.edu

&Priyadarshini Panda

Yale University

New Haven, CT, USA

priya.panda@yale.edu

###### Abstract

Spiking Neural Networks (SNNs) have recently become more popular as a biologically plausible substitute for traditional Artificial Neural Networks (ANNs). SNNs are cost-efficient and deployment-friendly because they process input in both spatial and temporal manner using binary spikes. However, we observe that the information capacity in SNNs is affected by the number of timesteps, leading to an accuracy-efficiency tradeoff. In this work, we study a fine-grained adjustment of the number of timesteps in SNNs. Specifically, we treat the number of timesteps as a variable conditioned on different input samples to reduce redundant timesteps for certain data. We call our method **S**piking **E**arly-**E**xt **N**eural **N**etworks (**SEENNs**). To determine the appropriate number of timesteps, we propose SEENN-I which uses a confidence score thresholding to filter out the uncertain predictions, and SEENN-II which determines the number of timesteps by reinforcement learning. Moreover, we demonstrate that SEENN is compatible with both the directly trained SNN and the ANN-SNN conversion. By dynamically adjusting the number of timesteps, our SEENN achieves a remarkable reduction in the average number of timesteps during inference. For example, our SEENN-II ResNet-19 can achieve **96.1**% accuracy with an average of **1.08** timesteps on the CIFAR-10 test dataset. Code is shared at https://github.com/Intelligent-Computing-Lab-Yale/SEENN.

## 1 Introduction

Deep learning has revolutionized a range of computational tasks such as computer vision and natural language processing  using Artificial Neural Networks (ANNs). These successes, however, have come at the cost of tremendous computational demands and high latency . In recent years, Spiking Neural Networks (SNNs) have gained traction as an energy-efficient alternative to ANNs . 0SNNs infer inputs across a number of timesteps as opposed to ANNs, which infer over what is essentially a single timestep. Moreover, during each timestep, the neuron in an SNN either fires a spike or remains silent, thus making the output of the SNN neuron binary and sparse. Such spike-based computing produces calculations that substitute multiplications with additions.

In the field of SNN research, there are two main approaches to getting an SNN: (1) directly training SNNs from scratch and (2) converting ANNs to SNNs. Direct training seeks to optimize an SNN using methods such as spike timing-based plasticity  or surrogate gradient-based optimization . In contrast, the ANN-SNN conversion approach  uses the feature representationof a pre-trained ANN and aims to replicate it in the corresponding SNN. Both methods have the potential to achieve high-performance SNNs when implemented correctly.

Despite the different approaches, both training-based and conversion-based SNNs are limited by binary activations. As a result, the key factor that affects their information processing capacity is the **number of timesteps**. Expanding the number of timesteps enables SNNs to capture more features in the temporal dimension, which can improve their accuracy in conversion and training. However, a larger number of timesteps increases latency and computational requirements, resulting in a lower acceleration ratio, and yielding a tradeoff between accuracy and time. Therefore, current efforts to enhance SNN accuracy often involve finding ways to achieve it with fewer number of timesteps.

In this paper, we propose a novel approach to improve the tradeoff between accuracy and time in SNNs. Specifically, our method allows each input sample to have a varying number of timesteps during inference, increasing the number of timesteps only when the current sample is hard to classify, resulting in an early exit in the time dimension. We refer to this approach as **S**piking **E**arly-**E**xit **N**eural **N**etworks (**SEENNs**). To determine the optimal number of timesteps for each sample, we propose two methods: SEENN-I, which uses confidence score thresholding to output a confident prediction as fast as possible; and SEENN-II, which employs reinforcement learning to find the optimal policy for the number of timesteps. Our results show that SEENNs can be applied to both conversion-based and direct training-based approaches, achieving new state-of-the-art performance for SNNs. In summary, our contributions are threefold:

1. [leftmargin=*]
2. We introduce a new direction to optimize SNN performance by treating the number of timesteps as a variable conditioned to input samples.
3. We propose Spiking Early-Exit Neural Networks (SEENNs) and use two methods to determine which timestep to exit: confidence score thresholding and reinforcement learning optimized with policy gradients.
4. We evaluate our SEENNs on both conversion-based and training-based models with large-scale datasets like CIFAR and ImageNet. For example, our SEENNs can use \(\)**1.1** timesteps to achieve similar performance with a 6-timestep model on the CIFAR-10 dataset.

## 2 Related Work

### Spiking Neural Networks

#### 2.1.1 ANN-SNN Conversion

Converting ANNs to SNNs utilizes the knowledge from pre-trained ANNs and replaces the ReLU activation in ANNs with a spike activation in SNNs . The conversion-based method, therefore, seeks to match the features in two different models. For example,  studies how to select the firing threshold to cover all the features in an ANN.  studies using a smaller threshold and  proposes to use a bias shift to better match the activation. Based on the error analysis,  utilizes a parameter calibration technique and  further changes the training scheme in ANN.

#### 2.1.2 Direct Training of SNN

Direct training from scratch allows SNNs to operate within extremely few timesteps. In recent years, the number of timesteps used to train SNNs has been reduced from more than 100  to less than 5 . The major success is based on the spatial-temporal backpropagation  and surrogate gradient estimation of the firing function . Through gradient-based learning, recent works  propose to optimize not only parameters but also firing threshold and leaky factor. Moreover, loss function , surrogate gradient estimation , batch normalization , activation distribution , membrane potential normalization/regularization  are also factors that affect the learning behavior in direct training and have been investigated properly. Our method, instead, focuses on the time dimension, which is fully complementary to both conversion and training.

### Conditional Computing

Conditional computing models can boost the representation power by adapting the model architectures, parameters, or activations to different input samples . BranchyNet  and Conditional Deep Learning  add multiple classifiers in different layers to apply spatial early exit to ANNs. SkipNet  and BlockDrop  use a dynamic computation graph by skipping different blocksbased on different samples. CondConv  and Dynamic Convolution  use the attention mechanism to change the weight in convolutional layers according to input features. Squeeze-and-Excitation Networks  proposes to reweight different activation channels based on the global context of the input samples. To the best of our knowledge, our work is the first to incorporate conditional computing into SNNs.

## 3 Methodology

In this section, we describe our overall methodology and algorithm details. We start by introducing the background on fixed-timestep spiking neural networks and then, based on a strong rationale, we introduce our SEANN method.

### Spiking Neural Networks

SNNs simulate biological neurons with Leaky Integrate-and-Fire (LIF) layers . In each timestep, the input current in the \(\)-th layer charges the membrane potential \(\) in the LIF neurons. When the membrane potential exceeds a threshold, a spike \(\) will fire to the next layer, as given by:

\[^{}[t+1] =^{}[t]+^{}^{-1}[t],\] (1) \[^{}[t+1] =H(^{}[t+1]-V),\] (2)

where \((0,1]\) is the leaky factor, mimicking natural potential decay. \(H()\) is the Heaviside step function and \(V\) is the firing threshold. If a spike fires, the membrane potential will be reset to 0: \(([t+1]=[t+1]*(1-[t+1]))\). Following existing baselines, in direct training, we use \(=0.5\) while in conversion we use \(=1.0\), transforming to the Integrate-and-Fire (IF) model. Moreover, the reset in conversion is done by subtraction: \(([t+1]=[t+1]-V_{th}[t+1])\) as suggested by . Now, denote the overall spiking neural network as a function \(f_{T}()\), its forward propagation can be formulated as

\[f_{T}()=_{t=1}^{T}h g^{L} g^{L-1} g^{L -2} g^{1}(),\] (3)

where \(h()\) denotes the final linear classifier, and \(g^{}()\) denotes the \(\)-th block of backbone networks. \(L\) represents the total number of blocks in the network. A block contains a convolutional layer to compute input current (\(^{}^{-1}\)), a normalization layer , and a LIF layer. In this work, we use a direct encoding method, _i.e_. using \(g^{1}()\) to encode the input tensor into spike trains, as done in recent SNN works . In SNNs, we repeat the inference process for \(T\) times and average the output from the classifier to produce the final result.

### Introducing Early-Exit to SNNs

Conventionally, the number of timesteps \(T\) is set as a fixed hyper-parameter, causing a tradeoff between accuracy and efficiency. Here, we also provide several \(accuracy-T\) curves of spiking ResNets in Fig. 1. These models are trained from scratch or converted to 4, 6, or 8 timesteps and we evaluate the performance with the available numbers of timesteps. For the CIFAR-10 dataset,

Figure 1: The accuracy of spiking ResNet under different numbers of timesteps on CIFAR10, CIFAR100, and ImageNet datasets, either by direct training  or by conversion .

increasing the number of timesteps from 2 to 6 only brings 0.34% top-1 accuracy gain, at the cost of 300% more latency. _In other words, the majority of correct predictions can be inferred with much fewer timesteps._

The observation in Fig. 1 motivates us to explore a more fine-grained adjustment of \(T\). We are interested in an SNN that can adjust \(T\) based on the characteristics of different input images. Hypothetically, each image is linked to an _difficulty_ factor, and this SNN can identify this difficulty to decide how many timesteps should be used, thus eliminating unnecessary timesteps for easy images. We refer to such a model as a spiking early-exit neural network (SEENN).

To demonstrate the potential of SEENN, we propose a metric that calculates the minimum timesteps needed to perform correct prediction averaged on the test dataset, _i.e_. the lowest timesteps we can achieve without comprising the accuracy, and it is based on the following assumption:

**Assumption 3.1**.: Given a spiking neural network \(f_{T}\), if it can correctly predict \(\) with \(t\) timesteps, then it always outputs correct prediction for any \(t^{}\) such that \(t t^{} T\).

This assumption indicates the inclusive property of different timesteps. In Appendix A, we provide empirical evidence to support this assumption. Let \(_{t}\) be the set of correct predicted input samples for timestep \(t\), we have \(_{1}_{2}_{T}\) based on Assumption 3.1. Also denote \(=}_{T}\) as the wrong prediction set, we propose the _averaged earliest timestep (AET)_ metric, given by

\[=(|_{1}|+T|^{}|+_{t=2}^{T}t (|_{t}|-|_{t-1}|)),\] (4)

where \(||\) returns the cardinal number of the set, and \(|_{t}|-|_{t-1}|\) returns the number of samples that belong to \(_{t}\) yet not to \(_{t-1}\). \(N=|_{T}|+||\) is the total number of samples in the validation dataset. The AET metric describes an ideal scenario where correct predictions are always inferred using the minimum number of timesteps required, while still preserving the original accuracy. It's worth noting that incorrect samples are inferred using the maximum number of timesteps, as it is usually not possible to determine if a sample cannot be correctly classified before inference.

In Fig. 1, we report the AET in each case. For models directly trained on CIFAR10 or CIFAR100, the AET remains slightly higher than \(2\) (note that the minimum number of timesteps is 2). With merely 11% more latency added to the 2-timestep SNN on CIFAR\(10\)\((T=2.228)\), we can achieve an accuracy equal to a 6-timestep SNN. The converted SNNs also only need a few extra time steps. This suggests the huge potential for using early exit in SNNs.

Despite the potential performance boost from the early exit, it is impossible to achieve the AET effect in practice since we cannot access the label in the test set. Therefore, the question of how to design an efficient and effective predictor that determines the number of timesteps for each input is non-trivial. In the following sections, we propose two methods, SEENN-I and SEENN-II, to address this challenge.

### Seenn-I

In SEENN-I, we adopt a proxy signal to determine the difficulty of the input sample--confidence score. Formally, let the network prediction probability distribution be \(=(f_{t}())=[p_{1},p_{2},,p_{M}]\), where \(M\) is the number of object classes, the confidence score (CS) is defined as

\[=(),\] (5)

which means the maximum probability in \(\). The CS is a signal that measures the level of uncertainty. If the CS is high enough (_e.g_. CS\(=0.99\)), the prediction distribution will be highly deterministic; otherwise (_e.g_. CS\(=\)), the prediction distribution is uniform and extremely uncertain.

A line of work  has shown that the level of uncertainty is highly correlated with the accuracy of the neural networks. For input samples with deterministic output prediction, the neural network typically achieves high accuracy, which corresponds to relatively _easy_ samples. Hence, we can utilize this property as an indicator of how many time steps should be assigned to this sample. We adopt a simple thresholding mechanism, _i.e_. given a preset threshold \(\), we iterate each timestep and once the confidence score is higher than \(\), the accumulated output is used for prediction. Appendix B demonstrates the distribution of confidence scores changes with different numbers of timesteps. A diagram describing this method can be found in Fig. 2(a).

### Seenn-Ii

Our SEENN-I is a post-training method that can be applied to an off-the-shelf SNN. It is easy to use, but the SNN is not explicitly trained with the early exit, so the full potential of early exit is not exploited through SEENN-I. In this section, we propose an "early-exit-aware training" method for SNNs called SEENN-II.

SEENN-II is built from reinforcement learning to directly predict the difficulty of the image. Specifically, we define an action space \(=\{t_{1},t_{2},,t_{n}\}\), which contains the candidates for the number of timesteps that can be applied to an input sample \(\). To determine the optimal timestep candidates, we develop a policy network that generates an \(n\)-dimensional _policy vector_ to sample actions. During training, a reward function is calculated based on the policy and the prediction result, which is generated by running the SNN with the number of timesteps suggested by the policy vector. Unlike traditional reinforcement learning , our problem does not consider state transition as the policy can predict all actions at once. We also provide a diagram describing the overall method in Fig. 2(b).

Formally, consider an input sample \(\) and a policy network \(f_{p}\) with parameter \(\), we define the policy of selecting the timestep candidates as an \(n\)-dim categorical distribution:

\[=(f_{p}(;)),_{}( |)=_{k=1}^{n}_{k}^{_{k}},\] (6)

where \(\) is the probability of the categorical distribution, obtained by inferring the policy networks with a \(\) function. Thus \(_{k}\) represents the probability of choosing \(t_{k}\) as the number of timesteps in the SNN. An action \(\{0,1\}^{n}\) is sampled based on the policy \(\). Here, \(\) is a one-hot vector since only one timestep can be selected. Note that the policy network architecture is made sufficiently small such that the cost of inferring the policy is negligible compared to SNN (see architecture details in Sec. 4.1).

Once we obtain an action vector \(\), we can evaluate the prediction using the target number of timesteps, _i.e_. \(f_{t}()\). Our objective is to minimize the number of timesteps we used while not sacrificing accuracy. Therefore, we associate the actions taken with the following reward function:

\[R()=|_{k}=1}}&\\ -&.\] (7)

Here, \(t_{k}|_{_{k}=1}\) represents the number of timesteps selected by \(\). Here, the reward function is determined by whether the prediction is correct or incorrect. If the prediction is correct, then we incentivize early exit by assigning a larger reward to a policy that uses fewer timesteps. However, if the prediction is wrong, we penalize the reward with \(\), which serves the role to balance the accuracy and the efficiency. As an example, a large \(\) leads to more correct predictions but also more timesteps.

Gradient Calculation in the Policy NetworkTo this end, our objective for training the policy network is to maximize the expected reward function, given by:

\[_{}\;_{_{}}[R()].\] (8)

Figure 2: The frameworks of our proposed SEENNs. (a): SEENN-I uses the confidence score thresholding for determining the optimal number of timesteps, (b): SEENN-II leverages a policy network to predict the number of timesteps, optimized by reinforcement learning.

In order to calculate the gradient of the above objective, we utilize the policy gradient method  to compute the derivative of the reward function w.r.t. \(\), given by

\[_{}[R()]=[R()_{ }_{}(|)]=[R()_{ }_{k=1}^{n}_{k}^{_{k}}]=[R( )_{}_{k=1}^{n}_{k}_{k}].\] (9)

Moreover, unlike other reinforcement learning which relies on Monte-Carlo sampling to compute the expectation, our method can compute the exact expectation. During the forward propagation of the SNN, we can store the intermediate accumulated output at each \(t_{k}\), and calculate the reward function using the stored accumulated output \(f_{t_{k}}()\). Since \(_{}(|)\)is a categorical distribution, we can rewrite Eq. (9) as

\[_{}[R()]=_{k=1}^{n}R(|_{_{k}=1})_{k}_{}_{k},\] (10)

where \(R(|_{_{k}=1})\) is the reward function evaluated with the output prediction using \(t_{k}\) timesteps.

### Training SEENN

In this section, we describe the training methods for our SEENN. To train the model for SEENN-I, we explicitly add a cross-entropy function to each timestep, thus making the prediction in early timesteps better, given by

\[_{}_{k=1}^{n}L_{CE}(f_{t_{k}}(), ,),\] (11)

where \(L_{CE}\) denotes the cross-entropy loss function and \(\) is the label vector. This training objective is essentially Temporal Efficient Training (TET) loss proposed in . We find this function can enhance the performance from every \(t_{k}\) compared to \(L_{CE}\) applied to only the maximum number of timesteps. As for conversion-based SEENN-I, we do not modify any training function. Instead, we directly apply the confidence score thresholding to the converted model.

To train the SEENN-II, we first employ TET loss to train the model for several epochs without involving the policy network. This can avoid the low training accuracy in the early stage of the training which may damage the optimization of the policy network. Then, we jointly optimize the SNN and the policy network by

\[_{,}_{_{}}[-R()+L_{CE}(f_{t_{k}|_{_{k}=1}}(),,)].\] (12)

Note that we do not train SEENN-II for the converted model since in conversion, a pre-trained ANN is used to obtain the converted SNN and does not warrant any training.

## 4 Experiments

To demonstrate the efficacy and the efficiency of our SEENN, we conduct experiments on popular image recognition datasets, including CIFAR10, CIFAR100 , ImageNet , and an event-stream dataset CIFAR10-DVS . Moreover, to show the compatibility of our method, we compare SEENN with both training-based and conversion-based state-of-the-art methods. Finally, we also provide some hardware and qualitative evaluation on SEENN.

### Implementation Details

In order to implement SEENN-I, we adopt the code framework of TET  and QCFS , both of which provide open-source implementation. All models are trained with a stochastic gradient descent optimizer with a momentum of 0.9 for 300 epochs. The learning rate is 0.1 and decayed following a cosine annealing schedule . The weight decay is set to \(5e-4\). For ANN pre-training in QCFS, we set the step \(l\) to 4. We use Cutout  and AutoAugment  for better accuracy as adopted in .

To implement SEENN-II, we first take the checkpoint of pre-trained SNN in SEENN-I and initialize the policy network. Then, we jointly train the policy network as well as the SNN. The policy network we take for the CIFAR dataset is ResNet-8, which contains only 0.547% computation of ResNet-19.

We also provide the details of architecture in Appendix E and measure the latency/energy of the policy network in Sec. 4.3. For the ImageNet dataset, we downsample the image resolution to 112\(\)112 for the policy network. Both the policy network and the SNN are finetuned for 75 epochs using a learning rate of 0.01. Other training hyper-parameters are kept the same as SEENN-I.

### Comparison to SOTA work

**CIFAR Training** We first provide the results on the CIFAR-10 and CIFAR-100 datasets . We test the architectures in the ResNet family  and VGG-16 . We summarize both the direct training comparison as well as the ANN-SNN conversion comparison in Table 1 and Table 2, respectively. Since the \(T\) in our SEENN can be variable based on the input, we report the average number of timesteps in the test dataset. As we can see from Table 1, existing direct training methods usually use 2, 4, and 6 timesteps in inference. However, increasing the number of timesteps from 2 to 6 brings marginal improvement in accuracy, for example, 95.45% to 95.60% in TEBN . Our SEENN-I can achieve **96.07%** with only **1.09** average timesteps, which is **5.5\(\)** lower than the state of the art. SEENN-II gets a similar performance with SEENN-I in the case of CIFAR-10, but it can achieve 0.7% higher accuracy than SEENN-I on the CIFAR-100 dataset.

**Comparison with 1-Timestep SNN** Reducing the number of timestep to 1 is another way towards low latency SNN . However, this method ignores the sample-wise difference and may lead to a larger accuracy drop. In Table 1, we compare VGG-16 with even 1 timestep . Our SEENN-I achieves 8.5% accuracy improvement at the cost of 12% more latency on the CIFAR-100 dataset.

**CIFAR Conversion** We also include the results of ANN-SNN conversion using SEENN-I in Table 2. Here, the best existing work is QCFS , which can convert the SNN in lower than 8 timesteps. We directly run SEENN-I with different choices of confidence score threshold \(\). Surprisingly, on the CIFAR-10 dataset, our SEENN-I can convert the model with **1.4** timesteps, and get **93.63**% accuracy. Instead, the QCFS only gets 75.44% accuracy using 2 timesteps for all input images. By selecting a higher threshold, we can obtain **95.08**% accuracy with **2.01** timesteps, which uses **4\(\)** lower number of timesteps than QCFS under the same accuracy level.

    &  &  &  \\   & & \(Acc_{1}(T_{1})\) & \(Acc_{2}(T_{2})\) & \(Acc_{3}(T_{3})\) & \(Acc_{1}(T_{1})\) & \(Acc_{2}(T_{2})\) & \(Acc_{3}(T_{3})\) \\   & Dspike  & 93.13 (2) & 93.16 (4) & 94.25 (6) & 71.28 (2) & 73.35 (4) & 74.24 (6) \\  & tdBN  & 92.34 (2) & 92.92 (4) & 93.16 (6) & - & - & - \\  & TET  & 94.16 (2) & 94.44 (4) & 94.50 (6) & 72.87 (2) & 74.47 (4) & 74.72 (6) \\  & RecDEs-SNN  & 93.64 (2) & 95.53 (4) & 95.55 (6) & - & 74.10 (4) & - \\  & ML-Loss  & 93.85 (2) & 95.40 (4) & 95.49 (6) & - & - & - \\  & TEBN  & 95.45 (2) & 95.58 (4) & 95.60 (6) & 78.07 (2) & 78.71 (4) & 78.76 (6) \\   & **SEENN-I (Ours)** & **96.07 (1.09)** & **96.38 (1.20)** & **96.44 (1.34)** & **79.56 (1.19)** & **81.42 (1.55)** & **81.65 (1.74)** \\  & **SEENN-II (Ours)** & **96.01 (1.08)** & - & - & **80.23 (1.21)** & - & - \\   & Diet-SNN  & - & - & 92.70 (5) & - & - & 69.67 (5) \\  & Temporal prune  & 93.05 (**1**) & 93.72 (2) & 93.85 (3) & 63.30 (**1**) & 64.86 (2) & 65.16 (3) \\    & **SEENN-I (Ours)** & **94.07 (1.08)** & **94.41 (1.20)** & **94.56 (1.46)** & **71.87** (1.12) & **73.53 (1.46)** & **74.30 (1.84)** \\   & **SEENN-II (Ours)** & **94.36 (1.09)** & - & - & **72.76** (1.15) & - & - \\   

Table 1: Accuracy-\(T\) comparison of **direct training** SNN methods on CIFAR datasets.

    &  &  &  \\   & & \(Acc_{1}(T_{1})\) & \(Acc_{2}(T_{2})\) & \(Acc_{3}(T_{3})\) & \(Acc_{1}(T_{1})\) & \(Acc_{2}(T_{2})\) & \(Acc_{3}(T_{3})\) \\   & Opt.  & 92.41 (16) & 93.30 (32) & 93.55 (64) & 63.73 (16) & 68.40 (32) & 69.27 (64) \\  & Calibration  & 94.78 (32) & 95.30 (64) & 95.42 (128) & - & - & - \\  & OPI  & 75.44 (8) & 90.43 (16) & 94.82 (32) & 23.09 (8) & 52.34 (16) & 67.18 (32) \\  & QCFS  & 75.44 (2) & 90.43 (4) & 94.82 (8) & 19.96 (2) & 34.14 (4) & 55.37 (8) \\   & **SEENN-I (Ours)** & **91.08 (1.18)** & **93.63 (1.40)** & **95.08 (2.01)** & **39.33 (2.57)** & **56.99 (4.41)** & **65.48 (6.19)** \\   

Table 2: Accuracy-\(T\) comparison of **ANN-SNN conversion** SNN methods on CIFAR datasets.

ImageNetWe compare the direct training and conversion methods on the ImageNet dataset. The results are sorted Table 3. For direct training, we use two baseline networks: vanilla ResNet-34  and SEW-ResNet-34 . For SEENN-I, we directly use the pre-trained checkpoint from TET . It can be observed that our SEENN-I can achieve the same accuracy using only 50% of original number of timesteps. For example, SEENN-I ResNet-34 reaches the maximum accuracy in **2.35** timesteps. SEENN-II can obtain a similar performance by using a smaller \(T\), _i.e_. 1.79. For conversion experiments, we again compare against QCFS using ResNet-34. Our SEENN-I obtains **70.2**% accuracy with **23.5** timesteps, higher than a 32-timestep QCFS model.

CIFAR10-DVSHere, we compare our SEENN-I on an event-stream dataset, CIFAR10-DVS . Following existing baselines, we train the SNN with a fixed number of 10 timesteps. From Table 4 we can find that SEENN-I can surpass all existing work except TET with only **2.53** timesteps, amounting to nearly **4\(\)** faster inference. Moreover, SEENN-II can get an accuracy of **82.6** using 4.5 timesteps.

### Hardware Efficiency

In this section, we analyze the hardware efficiency of our method, including latency and energy consumption. Due to the sequential processing nature of SNNs, the latency is generally proportional to \(T\) on most hardware devices. Therefore, we directly use a GPU (NVIDIA Tesla V100) to evaluate the latency (or throughput) of SEENN. For energy estimation, we follow a rough measure to count only the energy of operations that is adopted in previous work [18; 25; 31], as SNNs are usually deployed on memory-cheap devices (detailed in Appendix D). Fig. 3 plots the comparison of inference throughput and energy. It can be found that our SEENN-I simultaneously improves inference speed while reducing energy costs. Meanwhile, the policy network in SEENN-II only brings marginal effect and does not impact the overall inference speed and energy cost, demonstrating the efficiency of our proposed method. We show ImageNet results in Appendix C.

  
**Model** & **Method** & **T** & **Acc.** \\   \\  ResNet-19 & tdBN  & 10 & 67.8 \\ VGGSNN & PLIF  & 20 & 74.8 \\ ResNet-18 & Dspike  & 10 & 75.4 \\ ResNet-19 & RecDis-SNN  & 10 & 72.4 \\ VGGSNN & TET  & 10 & **83.1** \\   & **SEENN-I (Ours)** & **2.53** & **77.6** \\  & **5.17** & **82.7** \\    & **SEENN-II (Ours)** & **4.49** & **82.6** \\   

Table 4: Accuracy-\(T\) comparison on CIFAR10-DVS dataset.

Figure 3: Comparison of latency (throughput) and energy consumption between SNN and SEENN.

  
**Model** & **Method** & **T** & **Acc.** & **Model** & **Method** & **T** & **Acc.** \\   \\  ResNet-34 &  _tdBN_ \\ TET  \\ TEBN  \\  & 6 & 63.72 &  & 32 & 33.01 \\  &  \(t\) **64.79** \\  & 6 & **64.29** & & 64 & 59.52 \\   &  _TEBN_ \\  & 6 & 64.29 & & & & & & \\   &  _**SEENN-I (Ours)** \\  & **3.38** & **63.65** & & & & & & \\   &  _**SEENN-II (Ours)** \\  & **2.40** & **64.18** & & & & & & \\   & SEW  & 4 & 67.04 &  **SEENN-I (Ours)** \\  } & 4 & 68.00 & & & & & \\   &  _TEBN_ \\  & **4** & **68.28** & & & & & & & \\    &  _**SEENN-I (Ours)** \\  & **1.66** & **66.21** & & & & & & \\    & 
 _**SEENN-II (Ours)** \\  & **1.79** & **67.48** & & & & & & \\   

Table 3: Accuracy-\(T\) comparison on ImageNet dataset.

### Ablation Study

In this section, we conduct the ablation study on our SEENN. In particular, we allow SEENN to flexibly change their balance between the accuracy and the number of timesteps. For example, SEENN-I can adjust the confidence score threshold \(\) and SEENN-II can adjust the penalty value \(\) defined in the reward function. To demonstrate the impact of different hyper-parameters selection, we utilize SEENN-I evaluated with different \(\). Fig. 4 shows the comparison. The yellow line denotes the trained SNN with a fixed number of timesteps while the blue line denotes the corresponding SEENN-I with 6 different thresholds. We test the SEW-ResNet-34 on the ImageNet dataset and the ResNet-19 on the CIFAR10 dataset. For both datasets, our SEENN-I has a higher \(accuracy-T\) curve than the vanilla SNN, which confirms that **our SEENN improves the accuracy-efficiency tradeoff**. Moreover, our SEENN-I largely reduces the distance between the AET coordinate and the \(accuracy-T\) curve, meaning that our method is approaching the upper limit of the early exit. On the right side of the Fig. 4, we additionally draw the composition pie charts of SEENN-I, which shows how many percentages of inputs are using 1, 2, 3, or 4 timesteps, respectively. It can be shown that, as we gradually adjust \(\) from 0.4 to 0.9 for the SEW-ResNet-34, the percentage of inputs using 1 timestep decreases (73.4% to 30.7%). For the CIFAR10 dataset, we find images have a higher priority in using the first timestep, ranging from 95.4% to 56.2%.

### Qualitative Assessment

In this section, we conduct a qualitative assessment of SEENN by visualizing the input images that are separated by our SEENN-II, or the policy network. Specifically, we take the policy network and let it output the number of timesteps for each image in the ImageNet validation dataset. In principle, the policy network can differentiate whether the image is _easy_ or _hard_ so that easy images can be inferred with less number of timesteps and hard images can be inferred with more number of timesteps. Fig. 5 provides some examples of this experiment, where images are chosen from orange, cucumber, bubble, broccoli, aircraft carrier, and torch classes in the ImageNet validation dataset. We can find that \(T=1\) (easy) images and \(T=4\) (hard) images have huge visual discrepancies. As an example, the orange, cucumber, and broccoli images from \(T=1\) row are indeed easier to be identified, as they contain single objects in a clean background. However, in the case of \(T=4\), there are many irrelevant objects overlapped with the target object, or there could be many small samples of target objects which makes it harder to identify. For instance, in the cucumber case, there are other vegetables that increase the difficulty of identifying them as cucumbers. These results confirm our hypothesis that visually simpler images are indeed easier and can be correctly predicted using a fewer number of timesteps.

Conclusion

In this paper, we introduce SEENN, a novel attempt to allow a varying number of timesteps on an input-dependent basis. Our SEENN includes both a post-training approach (confidence score thresholding) and an early-exit-aware training approach (reinforcement learning for selecting the appropriate number of timesteps). Our experimental results show that SEENN is able to find a sweet spot that maintains accuracy while improving efficiency. Moreover, we show that the number of timesteps selected by SEENNs is related to the visual difficulty of the image. By taking an input-by-input approach during inference, SEENN is able to achieve state-of-the-art accuracy with less computational resources.