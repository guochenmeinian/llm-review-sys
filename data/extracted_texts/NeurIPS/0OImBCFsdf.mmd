# SaVeNet: A Scalable Vector Network for Enhanced Molecular Representation Learning

Sarp Aykent

Auburn University

sarp@auburn.edu &Tian Xia

Auburn University

tian@auburn.edu

###### Abstract

Geometric representation learning of molecules is challenging yet essential for applications in multiple domains. Despite the impressive breakthroughs made by geometric deep learning in various molecular representation learning tasks, effectively capturing complicated geometric features across spatial dimensions is still underexplored due to the significant difficulties in modeling efficient geometric representations and learning the inherent correlation in 3D structural modeling. These include computational inefficiency, underutilization of vectorial embeddings, and limited generalizability to integrate various geometric properties. To address the raised concerns, we introduce an efficient and effective framework, **S**calable **V**ector Network (SaVeNet), designed to accommodate a range of geometric requirements without depending on costly embeddings. In addition, the proposed framework scales effectively with introduced direction noise. Theoretically, we analyze the desired properties (i.e., invariance and equivariant) and framework efficiency of the SaVeNet. Empirically, we conduct a comprehensive series of experiments to evaluate the efficiency and expressiveness of the proposed model, which achieves state-of-the-art performance across various tasks within molecular representation learning.

## 1 Introduction

The field of geometric deep learning (GDL) has seen a rapid expansion in recent years, thanks to the successful application of Graph Neural Networks (GNNs) for modeling graph structures . The ability to learn complex geometric representations has driven significant breakthroughs across various disciplines and has proven to be particularly beneficial in diverse areas such as social science , physics , and neuroscience . Within the realm of molecular representation learning, the use of message-passing-based GNNs has demonstrated remarkable outcomes, especially in understanding the 3D structures of molecules.

Although geometric deep learning has recently seen promising results in various applications in molecular representation learning, its full potential in the field is still untapped. To begin with, a notable research gap persists within this field: the trade-off between expressiveness and efficiency in GNNs. Many studies often resort to the use of multi-hop neighbors  or complex embeddings  in their pursuit to augment the expressiveness of message-passing based GNNs . While these approaches can enhance the expressiveness and predictive power of the model, they invariably compromise scalability and efficiency due to their computationally intensive nature. As these networks grow more expressive, they demand more computational resources, impeding their ability to scale to larger or more complex datasets. Addressing the challenge of efficiently handling molecular structures to sufficiently capture the complete, multi-level structural information during learning without compromising the computational efficiency of the network is of critical importance. Our approach circumvents these issues by avoiding using multi-hop GNNs andexpensive embeddings. Novel approaches for initializing and processing vector-typed embeddings have been designed to work with vector representations with the goal of achieving a balance between maintaining numerical stability, facilitating faster convergence, and enhancing the model's ability to generalize to new datasets [25; 33]. Utilizing one-hop vector embeddings work with our proposed SaVeNet significantly enhances the network's efficiency, demonstrating their effective integration in this context. Figure 1 provides a visual illustration of the advantages of our proposed SaVeNet, which compares our method with state-of-the-art methods on the QM9 dataset. The values on the \(x\)-axis show the latency of the model, and the values on the \(y\)-axis show the standard mean absolute error (std. MAE) of the models' performance. Because lower is better for both latency and std. MAE, the best model in both efficiency and expressiveness is the one that is closest to the (0,0) point. In terms of performance, our base model, denoted as SaVeNet -B, surpasses the existing state-of-the-art method Equiformer , while concurrently enhancing training duration efficiency by a factor of 14.6. This substantial improvement underscores the efficacy of our model in achieving superior results with increased efficiency.

Furthermore, although several prior works [2; 3; 12; 18; 19; 25; 27; 29; 30] proposed equivariant neural networks that handle scalar and vector representations together for molecular representation learning, one primary problem with these existing works is that vector-valued representations show limited performance or scalability and cannot equally contribute towards performance improvements in the network [12; 18; 29]. It remains challenging to scale the models to improve the expressiveness of the model further. We propose an efficient framework to tackle this challenge. Our model's scalability is exemplified by its capacity to stack \(N\) layers, effectively serving as an encoder for learning the latent geometric representations, thereby augmenting the model's expressiveness. These superior scaling capabilities enhance the model's expressiveness and effectively distinguish our work from existing research, which often struggles to scale efficiently [9; 19]. The scalability of our approach is not merely theoretical but is empirically demonstrated in Section 5.1. Figure 1 illustrates one example of the scaled larger version of our model: SaVeNet -L, which exhibits enhanced capability in contrast to our base model and existing baselines. This advancement substantiates the prospective capacity of our model to scale effectively, thereby indicating potential for superior performance in future implementations.

Our paper's main contributions are: (i) Proposed an efficient and effective message-passing framework coupled with several decoder mechanisms for leveraging the geometric representations within the 3D molecular graphs. (ii) Novel approaches for initializing and processing vector-typed embeddings have been designed to work with vector representations to ensure numerical stability, speed up model convergence, and improve the model's ability to adapt to new datasets. (iii) A thorough evaluation of the expressiveness, efficiency, and scalability of our models compared with existing benchmarks, as well as detailed ablation studies, underscore the advantages of our proposed SaVeNet.

## 2 Related work

**Equivariant Graph Neural Networks** The properties of graph representations can change with transformations such as translation, rotation, and permutation. By design, GNNs inherently permutation equivalence . While it is possible to introduce diverse transformations to the network through data augmentation, this approach can lead to computational inefficiencies.

The concepts of invariance and equivariance have been long acknowledged as critical to the success of various tasks [2; 13; 35; 36]. Recent research in molecular representation learning has predominantly addressed this challenge with equivariant network architectures. Within the realm of molecules,

Figure 1: Comparison of our proposed SaVeNet and the state-of-the-art methods: latency vs. std. MAE on QM9 dataset.

numerous equivariant models have been proposed for tasks such as chemical property prediction , protein structure prediction , and energy prediction .

**Molecular Property Prediction** Over recent years, there have been considerable advancements in molecular property prediction [9; 16; 19; 28]. These developments have been primarily driven by the adoption of graph models for molecular representation, coupled with the application of kernel methods for structural learning. Such kernels operate based on geometric features, such as interatomic distances, which are encoded using radial basis or Bessel basis functions. Several other studies employ spherical embeddings to model message-passing [3; 9; 17; 19; 21; 33]. Dimenet++ , GemNet  and SphereNet  depend on a multi-hop message-passing to model angular properties, which introduces computational complexity that curtails scalability with respect to the model size of these approaches. These advanced representations have resulted in more precise property predictions, including but not limited to dipole moment, HOMO-LUMO gap, energy, and force. However, the precision of these predictions is accompanied by increased computational complexity due to their dependence on quadratic or even cubic operations. Recent works have strived to address this issue, proposing innovative techniques that improve computational efficiency with minimal or no detriment to prediction accuracy [21; 33]. These novel techniques continue to operate on higher-order features during feature generation, underscoring the ongoing challenge of balancing computational efficiency with predictive precision. Both ET  and EQGAT  incorporate attention mechanisms into vector representations. However, the complex interactions in these works often limit the scalability and efficiency of the networks. More recently, networks such as SEGNN  and Equiformer  have utilized Clebsch-Gordan tensor products to achieve high performance. However, these tensor products are empirically found to be significantly slower than the previously discussed methods, posing a challenge when applying such methods to large, real-world datasets.

Our model utilizes 1-hop geometric representations, offering a balance of expressivity and computational efficiency. This design choice circumvents the need for complex and potentially lossy conversions often associated with higher-order representations. By harnessing the rich information in 1-hop representations, we maintain data integrity and enhance model performance without substantial computational overhead.

## 3 Preliminaries

**Notations.** We formulate the atomic structure graph representation, where each input molecule is represented as a graph \(=(,)\). \(\) is the set of \(N\) nodes in the graph, each node representing an atom in the molecule, and \(\) is the set of edges that connects the nodes. The average degree of a node, denoted by \(k\), represents the average number of edges per node. The geometric information of a node for each \(v_{i}\) is represented by the coordinates \(c_{i}=(x_{i},y_{i},z_{i})\) in the Cartesian coordinate system. The scalar representations \(s\) generally include properties like relative distance \(r\), while the vector representations \(V\) include direction vectors \(\). We use \(\) to denote the element-wise (Hadamard) product and \(||||_{2}\) to denote the row-wise \(L^{2}\) norm. The cross-product between two vectors is denoted as \(\).

**Invariance and Equivariance.** A function \(f:X Y\) is said to be _equivariant_ to a transformation if applying the transformation to the input of the function is the same as applying the transformation to the output of the function. Formally, for a transformation \(T\), a function \(f\) is equivariant if for all \(x X\): \(T(f(x))=f(T(x))\). This means that the function preserves the structure of the transformation. On the other hand, a function is _invariant_ to a transformation if its output does not change when the transformation is applied to its input. Formally, for a transformation \(T\), a function \(f\) is invariant if for all \(x X\): \(f(T(x))=f(x)\). This implies that the function is invariant to the transformation; the function's output remains unchanged regardless of the applied transformation.

**Euclidian Transformations.** In this paper, we frequently use the special orthogonal group \(SO(3)\), comprising all 3x3 rotation matrices \(R\) that satisfy \(R^{T}R=I\) and \((R)=1\), and the special Euclidean group \(SE(3)\), consisting of all \(4 4\) special Euclidean transformation matrices \(T\) that can be written as \(T=R&t\\ 0&1,\) where \(R SO(3)\), \(t^{3}\) is a translation vector, and \(0\) is a row vector of zeros. These matrices satisfy \(T^{-1}T=I\) and \((T)=1\). \(SE(3)\) forms a manifold, the product of \(SO(3)\) and \(^{3}\), allowing operations combining rotations and translations in 3D space.

Scalable Vector Network

Our goal is to design a message-passing neural network that is aware of the geometric characteristics of the molecular graph, with consideration of expressiveness and efficiency, and preserve these characteristics beneficial for downstream tasks. We first introduce the representations used in SaVeNet and how we handle these geometric representations. Then we describe the overall message-passing framework to model the geometric properties in the graph and show how to integrate our proposed framework into the representation learning process on downstream tasks.

### Efficient Representations for 3D graphs

The initial representations fed into the network are instrumental in achieving complete geometric representations . It is crucial to efficiently represent geometric graphs without the loss of their properties. Our framework operates by employing embeddings from one-hop neighbors exclusively. Invariant properties are encoded as scalar values, which enable the network to maintain invariance without resorting to computationally expensive operations. For equivariant properties, we extract the inherent directional information from the input graphs and encode these features into vector representations. The directional information between nodes \(i\) and \(j\) is represented with \(_{ij}\) and defined as follows:

\[_{ij}=\{_{ij}=-c_{j})}{r_{ij}},\ _{ij}=(c_{i}-)(c_{j}-),\ _{ij}=_{ij}_{ij}\}^{3 3} \]

Here, \(\) represents the average of \(c\), and \(\) denotes the cross-product operation. With the geometric representation defined, the efficacy of our scalar and vector representations is further demonstrated in the following lemma:

**Lemma 1**.: _Consider a known geometric graph with at least one node, and assume that each node in this graph has at least one connection. Let a new node \(j\) be added to this graph, stipulating that \(j\) is connected to at least one existing node. Then, the position \(p_{j}\) of node \(j\) can be determined in constant time using the \((p_{0},r_{ij},_{ij})\) properties._

The proof for Lemma 1 is provided in the Appendix A. Lemma 1 can be further generalized, requiring no positional information by translating the \(p_{0}\) reference point to the origin using a \(t_{o}\) translation vector. In light of this, we propose the following theorem, which demonstrates the SaVeNet's ability to reconstruct the rotation-equivariant geometric structure of an input graph:

**Theorem 1**.: _Assuming the input graph is strongly connected, the input space of SaVeNet, defined as \((r,d)\), is capable of reconstructing the rotation-equivariant geometric structure of the input graph. The reconstructed graph maintains the relative spatial configuration of nodes and edges in the input graph, the translation transformation is required for exact alignment with the original graph._

The proof for Theorem 1 is provided in Appendix B. The vector representations are an effective way to model equivariant networks and are widely used networks  on different challenging tasks. With the proof of Lemma 1 and Theorem 1, we present a provably efficient and lossless geometric encoding for the 3D graphs without reliance on the multi-hop neighborhoods. Despite the vector representation offering effective encodings, several unresolved challenges hinder its successful application towards enhancing the model's expressiveness and scalability. In response to this, we propose the incorporation of direction noise initialization and a vector activation class within our SaVeNet framework, specifically designed to address this issue.

**Direction Noise.** Vector representations are typically initialized using dataset-specific features , such as direction vectors between sequential neighbors in a protein's amino acid sequence. However, in the absence of such data, existing works  often default to initializing these vectors as null, allowing the network to learn these vector representations intrinsically. Such initialization can unnecessarily consume computational resources, particularly in the network's initial layers. In this work, as illustrated in Figure 2, we propose a novel strategy of

Figure 2: Direction noise.

introducing directional noise utilizing the spherical coordinate system as an initialization measure for the vector representations generated based on the node type. This approach is designed to mitigate numerical instability issues and empower the network to explore the latent space we propose through direction noises. We follow the following convention to generate a direction vector for a given node type \(F\):

\[(F_{i})=(_{i}_{i}),((_ {i})(_{i})),((_{i}))_{i}, _{i}} \]

where \(_{i}\) and \(_{i}\) are learnable parameters for node type \(i\) and \(\) is a control parameter governing the magnitude of the direction embeddings. This approach effectively generates direction vectors using the spherical coordinate system. The value of \(\) is associated with the progress of the training, similar to learning rate schedulers. The value of \(\) is adjusted throughout training, following a decay schedule that ensures it reaches zero before training concludes. This methodology offers an effective means for creating direction vectors using a spherical coordinate system while allowing for the adaptive control of the direction embeddings' magnitude throughout training.

**Vector Activations.** Applying similar activation functions used in scalars on vector representations, where the function operates on each element of the vector independently, can break the model's equivariance. This is because these functions are not designed to handle the geometric transformations that the vectors might undergo. Consequently, SavNet introduces a vector activation function in which the activation is computed based on the vector's \(L^{2}\) norm. For vector representation \(V\), the vector activation function is written as follows:

\[(V)=V_{v}(||V||+b)V^{  3},b^{C} \]

where \(_{v}\) is a scalar activation function. To further regulate the activation, the vector activation function incorporates a bias term \(b\), where \(b\) is the same shape as the representation dimension \(C\). Our experimentation found that vector activation functions significantly enhance the numerical stability during training, particularly with larger-scale models.

### SavNet for 3D Graphs

This section describes the overall architecture of SavNet in detail. The overall SavNet framework is a SO(3)-equivariant message-passing neural network incorporating the vector initialization and activation for scalable molecular representation learning. It is based on an encoder-decoder structure comprising multiple stacked encoder and decoder layers. The encoder layers learn the latent representation of the geometric features, and the decoder layers decode the learned latent representations for downstream tasks on either invariant or equivariant targets.

**Encoder.** The encoder in SavNet follows the message-passing paradigm and incorporates the above-defined geometric representations. The main two components of the encoder are inter-atomic interactions followed by atom-wise blocks.

_Vector Updates._ Vector updates are employed to compute channel-wise interactions in vector representations. By conducting identical operations across spatial dimensions, vector updates ensure that the equivariance property is maintained. More specifically, given a vector representation \(V^{C 3}\), the vector update applies a learnable weight matrix \(^{C C^{}}\), where \(C\) and \(C^{}\) denote the dimensions of the input and output representations, respectively. Therefore, the output vector \(V^{}\) is computed as \(V^{}=V\).

_Inter-atomic interactions._ Our model encapsulates inter-atomic interactions by integrating both equivariant direction vectors and invariant distance filters, which are encoded using radial basis functions. We have designed the scalar interaction path to interface with the vector interactions in a manner that preserves the equivariance property, achieved by appropriately scaling the magnitudes of the vectors. The overall formulation of the inter-atomic interactions can be expressed as follows:

\[ IA(s,V,r,)=&(s_{i},V_{i})+ _{j_{i}}e(s_{j},V_{j},r_{ij},_{ij})=(s^{ }_{i},V^{}_{i})\\ & s^{}_{i}=s_{i}+_{j_{i}}e_{s}(s _{j},V_{j},r_{ij},_{ij})\\ & V^{}_{i}=V_{i}+_{j_{i}}e_{v}(s _{j},V_{j},r_{ij},_{ij}) \]In this equation, \(_{i}\) represents the set of neighbors of node \(i\), and \(e\) is the interaction function, \(\) denotes the equivariant vectors. To accommodate a variety of equivariant properties, we fuse these vectors with vector updates VA. The interaction function is formalized as follows:

\[ e_{s}(s_{j},V_{j},r_{ij},_{ij})& =_{s}(s_{j})_{s}(r_{ij})\\ e_{v}(s_{j},V_{j},r_{ij},_{ij})&=_{ b}(_{ij})_{d}(s_{j})_{d}(r_{ij})+V_{j}_{v}(s_ {j})_{r}(r_{ij}) \]

where \(\) denotes sequentially stacked perceptron layers and \(\) denotes distance encoding. Specifically, \(_{b}\) serves as a vector update layer as it operates on vector inputs. The distance encoding is calculated using the formula \((r_{ij})=_{r}((r_{ij})*(r_{ij}))\), where \(()\) denotes a _basis_ function and \(()\) stands for a cutoff function.

_Atom-wise blocks._ Following the execution of the interaction block, the workflow proceeds to the atom-wise blocks, which are responsible for computing interactions between invariant and equivariant representations, as well as performing channel-wise updates. The formal expression of this process is defined as:

\[(s,V)=_{m}(||_{vu}(V)|| s),\ ( (_{vu}(V)_{v}(||_{vu}(V)|| s))} \]

where \(_{\{m,v\}}\) denotes multi-layer perceptron (MLP) and \(_{vu}\) denotes vector updates. Here, the vector representations interact with scalar representations via the \(L^{2}\) norm of updated vector representations. Moreover, the scalar representations scale the vector representations after applying channel-wise interactions \(_{v}\). This process presents bi-directional communication between different representations, thus enhancing the model's ability to learn and extract meaningful information from data.

**Decoder.** The framework utilizes decoder variations to accommodate various geometric requirements of a given task. By taking the latent representation pair (\(s_{e}\), \(V_{e}\)) from the output of the encoder, the decoder can effectively leverage these representations to maximize task performance. For _invariant targets_ where the tasks do not depend on global rotations, transformations, and permutation, the decoder takes encoded \(s_{e}\) representations and processes them with MLP layers. For _equivariant targets_ where tasks are sensitive to global rotations, the decoder takes the scalar and vector representations and processes them with stacked AW blocks. The final vector representations are scaled with scalar representations to generate the predictions. If the task is graph level, final representations are aggregated with global sum pooling.

**Equivariance and Invariance.** Addressing the strategic initialization of the model and the design of operations that uphold invariant and equivariant properties is equally important. SaVeNet uphold such properties, which can be stated more formally as follows:

**Theorem 2**.: _The equivariant representation of SaVeNet is equivariant to any given rotation matrix in \(R SO(3)\). The invariant representation of SaVeNet is invariant to any given transformation matrix in SE(3)._

The formal proof of Theorem 2 is provided in Appendix C, demonstrating the validity of these assertions and the robustness of the model under various transformations.

## 5 Experiments

This section evaluates our proposed SaVeNet on three tasks in geometric representation learning over both synthetic and real-world datasets. We implement the model with PyTorch . All experiments are conducted on an NVIDIA 3090 GPU with 24 GB memory. All models use the AdamW optimizer  for the optimization. Detailed descriptions of the datasets are compared in Table 1, which includes the average number of nodes, the average number of edges, and the total number

   Datasets & QM9 & N-Body & Molecule3D \\  Avg. \# of Nodes & 18.02 & 20 & 29.11 \\ Max \# of Nodes & 29 & 20 & 137 \\ Avg. \# of Edges & 280.72 & 380 & 553.00 \\ \# of Graphs & 130,831 & 7,000 & 3,899,647 \\ \# of Tasks & 12 & 4 & 5 \\ Splits & 84:8:8 & 42:29:29 & 6:2:2 \\ Invariant & ✓ & ✗ & ✓ \\ Equivariant & ✗ & ✓ & ✓ \\   

Table 1: Dataset details. The number of edges are computed with radius graph where d(Å) = 5.0of graphs. The three datasets include both invariant and equivariant targets for comprehensive evaluation of our proposed model. Further dataset details are provided in Appendix D. The rationale behind our design decisions and hyperparameters, as well as their specific values, are documented in Appendix F for reference.

**QM9**: QM9 dataset was used to evaluate the performance and efficiency of the models across twelve tasks for invariant target predictions. We present two distinct configurations of the SaVeNet model, distinguished by their respective sizes: The SaVeNet-B denotes the base variant, while the SaVeNet-L is the larger model. We report various baseline models, including NMP , Schnet , Cormorant , LieConv , PhysNet , Dimenet , Dimenet++ , TFN , SE(3) Transformer , EGNN , PaiNN , ET , SphereNet , ComENet , SEGNN , EQGAT , and Equiformer .

**N-Body**: To assess the effectiveness of SaVeNet with equivariant tasks, we conducted experiments on the extended N-Body dataset with harder targets . The proposed SaVeNet is compared with GNN, Tensor Field Networks , SE(3) Transformer , Radial Fields, PaiNN , EGNN , ClofNet , GCPNet . Most of the baseline results are adopted from . The results for PaiNN are not reported in previous work  due to numerical stability. Notably, we incorporated vector activation functions into PaiNN's interaction layers to address this issue, allowing us to include PaiNN in the performance comparison.

**Molecule3D**: The Molecule3D dataset, possessing over \(29\) the quantity of graphs as the QM9 dataset and an approximate \(1.6\) and \(1.9\) increase in the average number of nodes and edges respectively, is employed to evaluate the scalability of the model with respect to dataset size. For the Molecule3D dataset, we adopted the baselines as per . As only the gap target has been reported in preceding works, we utilized the hyperparameters provided in  for evaluating additional targets. The details of the hyperparameters are described in Appendix F.1 The comparative baseline models for the Molecule3D include GIN-Virtual , Dimenet++ , Spherenet , and ComENet .

**Evaluation Metrics.** A set of metrics are used to measure the performance of the models. For the QM9 and Molecule3D datasets, each task is assessed using the Mean Absolute Error (MAE). For the N-Body dataset, the performance is measured by the Mean Squared Error (MSE) of the future position predictions. Given that each dataset encompasses multiple tasks, we employ two additional aggregate measures: standardized (std.) and logarithmic (log) metrics, to assess overall performance. As the error can be disproportionately influenced by a few outliers, we report the logarithmic error to prevent these outliers from dominating the evaluation.

**Efficiency Analysis.** To critically assess the efficiency of the models, we employ a set of metrics including training speed, model complexity, and memory consumption. All the measurements are conducted utilizing hyperparameters from the respective model authors, and the reference imple

  Task & \(\) & \(\) & \(_{}\) & \(_{}\) & \(\) & \(C_{}\) & \(G\) & \(H\) & \(R^{2}\) & \(U\) & \(U_{0}\) & ZPVE & std. & log \\ Units & \(\)\(\) & meV & meV & meV & D & \(}{}\) & meV & meV & \(\)\(\) & meV & meV & meV & \(\%\) & - \\  NMP\({}^{}\) &.092 & 69 & 43 & 38 &.030 &.040 & 19 & 17 &.180 & 20 & 20 & 1.50 & 1.78 & -5.08 \\ SchNet &.235 & 63 & 41 & 34 &.0330 &.0330 & 14 & 14 &.073 & 19 & 14 & 1.70 & 1.76 & -5.17 \\ Cormorant\({}^{}\) &.085 & 61 & 34 & 38 &.038 &.026 & 20 & 21 &.961 & 21 & 22 & 2.03 & 2.14 & -4.75 \\ LieConv\({}^{}\) &.084 & 49 & 30 & 25 &.032 &.038 & 22 & 24 &.800 & 19 & 19 & 2.28 & 1.35 & -4.99 \\ PhysNet &.061 & 42.5 & 32.9 & 24.7 &.0529 &.0280 & 9.4 & 8.42 &.765 & 8.34 & 8.15 & 1.39 & 1.37 & -5.35 \\ DimeNet &.047 & 34.8 & 27.8 & 19.7 &.029 &.0249 & 8.98 & 8.11 &.331 & 7.89 & 8.02 & 1.29 & 1.05 & -5.57 \\ Dimenet++ &.044 & 32.6 & 24.6 & 19.5 &.0297 &.0230 & 7.56 & 6.53 &.331 & 6.28 & 6.32 & 1.21 & 0.98 & -5.67 \\ TFN\({}^{}\) &.223 & 58 & 40 & 38 &.064 &.101 & - & - & - & - & - & - & - & - \\ SE(3)-Tr\({}^{}\) &.142 & 53 & 35 & 33 &.051 &.054 & - & - & - & - & - & - & - & - \\ EGNN\({}^{}\) &.071 & 48 & 29 & 25 &.029 &.031 & 12 & 12 &.106 & 12 & 11 & 1.55 & 1.23 & -5.43 \\ PaiNN &.045 & 45.7 & 27.6 & 20.4 &.0120 &.024 &.735 &.598 &.066 & 5.83 & 5.85 & 1.28 & 1.01 & -5.85 \\ ET &.059 & 36.1 & 20.3 & 17.5 &.011 &.026 & 7.62 & 6.16 & **.033** & 6.38 & 6.15 & 1.84 & 0.84 & -5.90 \\ SphereNet &.046 & 31.1 & 22.8 & 18.9 &.0245 &.0215 &.78 & 6.33 &.268 & 6.36 & 6.26 & 1.12 & 0.91 & -5.73 \\ ComENet &.045 & 32.4 & 23.1 & 19.8 &.0245 &.0220 &.798 &.686 &.259 & 6.82 & 6.69 & 1.20 & 0.93 & -5.69 \\ SEGNN\({}^{}\) &.060 & 42 & 24 & 21 &.023 &.031 & 15 & 16 &.660 & 13 & 15 & 1.62 & 1.08 & -5.27 \\ EQGAT &.053 & 32 & 20 & 16 &.011 &.024 & 23 & 24 &.382 & 25 & 25 & 2.00 & 0.86 & -5.28 \\ Equiformer &.046 & 30 & **15.4** & **14.7** &.0117 &.0230 & 7.63 & 6.63 &.251 & 6.74 & 6.59 & 1.26 & 0.70 & -5.82 \\  SaVeNet-B &.039 & 24.8 & 18.4 & 16.3 &.0093 &.0227 & 6.64 & 5.43 &.058 & 5.48 & 5.43 & 1.18 & 0.69 & -6.04 \\ SaVeNet-L & **.035** & **22.7** & 16.6 & 15.1 & **.0085** & **.0210** & **6.10** & **4.83** &.049 & **4.74** & **4.83** & **1.10** & **0.63** & **-6.14** \\  

Table 2: Performance comparisons on QM9. \(\) denotes using different data partitions.

mentations are based on [18; 19; 20; 33]. To ensure consistent comparison, the experimental runs are executed on the same configurations utilizing an RTX 3090 GPU with 24 GB of graphics memory. The details of the efficiency analysis procedure are described in Appendix G and the theoretical time complexity is analyzed in Appendix E.
* _Training Speed._ To gauge the training speed of each model, we recorded the time consumed by a single forward/backward pass, expressed in milliseconds. Alongside this latency, we computed the number of samples that could be processed per second, employing similar measurement techniques.
* _Model Complexity._ The complexity of the models was evaluated by considering two key parameters: floating point operations (FLOPs) and multiply-accumulate operations (MACs). These metrics were computed utilizing the DeepSpeed library , with a consistent batch size.
* _Memory Consumption._ The memory consumption of each model was evaluated by assessing the amount of memory utilized across multiple batches. After processing each batch, the cache was cleared to ensure a realistic appraisal of memory usage. However, this practice of cache clearing significantly impedes the evaluation process, rendering a full dataset evaluation impractical.

### Results

**Accuracy results on QM9.** Table 2 listed the experimental results of the QM9 dataset. SaVeNet-B demonstrated superior performance on seven out of the twelve targets, surpassing all baseline models on these measures. While SaVeNet-B did not yield the highest performance for certain parameters--namely, \(_{}\), \(_{}\), \(C_{}\), \(R^{2}\), and ZPVE--it is noteworthy that it still delivered competitive results, indicating its robustness across diverse molecular properties. SaVeNet-L further improved on SaVeNet-B's performance, securing the best results on nine out of the twelve targets, with the exceptions of \(_{}\), \(_{}\), and \(R^{2}\). Furthermore, in terms of the overall performance measures, SaVeNet-L outperformed all other models, exhibiting the lowest standardized MAE and logarithmic MAE, reflecting its superior accuracy across all target properties. These results highlight the effectiveness of our proposed models, particularly SaVeNet-L, in accurately predicting a wide range of molecular properties in the QM9 dataset.

   Method & ES(5) & ES(20) & G+ES(20) & L+ES(20) & std.MSE & logMSE \\  GNN & 0.0131 & 0.0720 & 0.0721 & 0.0908 & 0.0151 & -4.3876 \\ TFN & 0.0236 & 0.0794 & 0.0845 & 0.1243 & 0.0192 & -4.0978 \\ SE(3)-Tr. & 0.0329 & 0.1349 & 0.1000 & 0.1438 & 0.0252 & -3.8037 \\ Radial Field & 0.0207 & 0.0377 & 0.0399 & 0.0779 & 0.0110 & -4.6212 \\ PaiNN & 0.0158 & 0.09971 & 0.10291 & 0.13561 & 0.0835 & -2.5329 \\ ET & 0.1653 & 0.1788 & 0.2122 & 0.2989 & 0.0535 & -2.9587 \\ EGNN & 0.0079 & 0.0128 & 0.0118 & 0.0368 & 0.0044 & -5.6241 \\ ClofNet & 0.0065 & 0.0073 & **0.0072** & 0.0251 & 0.0029 & -6.0324 \\ GCPNet & 0.0070 & 0.0071 & 0.0073 & 0.0173 & 0.0024 & -6.1104 \\  SaVeNet & **0.0062** & **0.0063** & 0.0082 & **0.0123** & **0.0020** & **-6.2268** \\   

Table 4: Performance comparisons on N-Body dataset.

   Model & std. & log & Batch & Memory & Latency & Samples/s & FLOPs & Param. & MACs \\ Metric & MAE \(\) & MAE \(\) & Size\(\) & GB \(\) & ms \(\) & \(\) & G \(\) & M \(\) & G \(\) \\  DimeNet++ & 0.98 & -5.67 & 357 & 4.82 & 38.9 & 1646 & 30.15 & 1.89 & 15.08 \\ ComENet & 0.93 & -5.69 & 1174 & 0.96 & 26.4 & 2422 & 14.36 & 3.81 & 7.18 \\ SphereNet & 0.91 & -5.73 & 238 & 6.65 & 51.6 & 1240 & 31.80 & 1.89 & 15.90 \\ EQGAT & 0.86 & -5.28 & 980 & 1.74 & 53.3 & 1200 & 17.00 & **0.93** & 8.49 \\ Equiformer & 0.70 & -5.83 & 94 & 14.04 & 337.3 & 190 & - & 3.53 & - \\  SaVeNet-B & 0.69 & -6.04 & **1660** & **0.85** & **23.1** & **2767** & **11.60** & 1.37 & **5.76** \\ SaVeNet-L & **0.63** & **-6.14** & 580 & 2.48 & 50.6 & 1237 & 55.36 & 7.72 & 27.51 \\   

Table 3: Efficiency comparison of the proposed SaVENet with the baselines on QM9.

**Efficiency results on QM9.** The results of efficiency experiments are presented in Table 3. We select the top five models based on their std. MAE performance for the efficiency comparison. Regarding efficiency, SaVeNet-B stands out with its superior results, processing the highest number of samples per second and the lowest memory consumption. This demonstrates an impressive balance between performance and resource utilization. Additionally, SaVeNet-B shows the lowest latency and number of FLOPs and MACs. The latency of SaVeNet-B is 14.5\(\) lower compared to Equiformer, the previously most performant model with the lowest std. and log MAE. Even with a significant reduction in compute time, SaVeNet-B surpasses the Equiformer in performance, illustrating efficiency advancements of SaVeNet without performance compromise.

SaVeNet-L manages to maintain a competitive efficiency despite its larger size. Its memory consumption is 2.6\(\) and 5.6\(\) times smaller compared to high-performing models such as SphereNet and Equiformer, respectively. Notably, SaVeNet-L processes 1237 samples per second, which is comparable to the rates of SphereNet and EqGAT. Compared to Equiformer, the baseline model with the highest accuracy, SaVeNet-L exhibits approximately 6\(\) lower latency. This suggests that SaVeNet-L, even with its larger size, can maintain high performance without sacrificing speed.

**Results on N-Body dataset.** Table 4 presents the results of the N-Body systems. SaVeNet outperformed all baseline methods on the ES(5), ES(20), and L+ES(20) tasks, whilst demonstrating competitive performance on the G+ES(20) task. Remarkably, SaVeNet achieved the lowest average MSE and logarithmic MSE across all four tasks, illustrating its superior prediction precision for equivariant targets. These results emphasize the capability of our proposed model to accurately predict the future positions of particles in N-Body systems, even under challenging conditions where particles are under the influence of the gravity and force fields.

**Accuracy results on Molecule3D.** Our proposed models, SaVeNet-B and SaVeNet-L, outperformed several baseline models on the Molecule3D dataset. In terms of relative performance, SaVeNet-B outperforms all tasks compared to other established models. SaVeNet-B showed improved performance by approximately 37% on the \(\) task and 92% on the \(\) task compared to the next best model, SphereNet. Our SaVeNet-L model, further improved the performance of SaVeNet-B. For the \(\) task, SaVeNet-L showed an improvement of approximately 26% over SaVeNet-B; for the \(\) task, it showed an improvement of about 17%. Regarding aggregate measures, SaVeNet-L also outperformed all other models, showing a relative improvement of around 18% in the std. MAE metric and 5% in the log MAE metric compared to SaVeNet-B. These results demonstrate the superior performance and scalability of our proposed models, especially SaVeNet-L, on the Molecule3D dataset.

**Efficiency results on Molecule3D.** The results of the efficiency evaluations on the Molecule3D dataset are illustrated in Table 6. Impressively, SaVeNet-B continues to dominate with respect to efficiency, manifesting the highest sample processing rate and minimal memory consumption. This shows an exemplary trade-off between accuracy and resource management.

    &  &  \\ Task & \(\) & \(_{}\) & \(_{}\) & \(\) & std. MAE & logMAE & \(\) \\  GIN-Virtual & 0.0882 & 0.0692 & 0.0632 & 0.1036 & 0.0592 & -2.8677 & 1.5233 \\ SchNet & 0.0532 & 0.0275 & 0.0265 & 0.0428 & 0.0263 & -3.6633 & 1.2082 \\ DimeNet++ & 0.0293 & 0.0240 & 0.0190 & 0.0306 & 0.0188 & -4.0139 & 1.2014 \\ SphereNet & 0.0288 & 0.0239 & 0.0183 & 0.0301 & 0.0184 & -4.0327 & 1.1836 \\ ComENet & 0.0345 & 0.0288 & 0.0252 & 0.0326 & 0.0220 & -3.8403 & 1.3521 \\  SaVeNet-B & 0.0183 & 0.0190 & 0.0173 & 0.0290 & 0.0156 & -4.2257 & 0.0108 \\ SaVeNet-L & **0.0136** & **0.0159** & **0.0143** & **0.0239** & **0.0128** & **-4.4408** & **0.0090** \\   

Table 5: Performance comparisons on Molecule3D dataset.

### Ablation Study

Table 7 presents the results of our ablation study on the proposed SAVeNet model, where we investigate the impacts of Direction Noise (DN), Vector Activations (VA), and varying the number of layers (L) on the QM9 dataset. In Table 7, we report standard Mean Absolute Error (std. MAE), the percentage change in std. MAE, and the logarithmic Mean Absolute Error (logMAE), respectively. The importance of DN is illustrated in the first two rows, where the std. MAE is reduced by 2.5%. Moreover, scaling the model with DA from six to eight layers yields a 7.8% reduction in error. Introducing VA into an eight-layer model further improves the performance by 2.8%. Lastly, expanding the model to 12 layers offers a substantial performance improvement of 8.7%.

## 6 Conclusion, Limitations and Future Works

This paper has focused on learning geometric representations of molecular structures. We introduced SaVeNet, a scalable message-passing neural network designed for learning geometric representations from 3D graphs. Furthermore, we implemented a unique approach to integrate geometric features, capturing complex geometric relationships within 3D structures without resorting to computationally complex operations. The performance of this framework was demonstrated on both synthetic and real-world datasets. For the latter, SaVeNet was evaluated across various core tasks in molecular representation learning. When compared with state-of-the-art methods, the effectiveness and efficiency of our proposed architecture were clearly validated. Our comprehensive examination of network efficiency and our comprehension of the trade-offs between expressiveness and efficiency will provide a foundation for future advancements in molecular representation learning. However, like many studies in this area, our work does have limitations. The primary constraint, shared with the baseline model discussed in this paper, is the dependency on known geometric structures of molecules at their equilibrium state. In practice, obtaining accurate 3D confirmations can be costly. An intriguing direction for future research is to explore an end-to-end framework that employs sequential representations of molecules, such as SMILES strings , to generate their 3D geometry, where the predicted 3D geometry could then be used to model molecular properties. Another promising avenue is to investigate the performance of larger variations of SaVeNet. Furthermore, assessing the impacts of efficiency improvements on larger models, potentially achieved through shared parameters in various configurations, is a worthwhile endeavor. We anticipate that our findings will pave the way for future investigations in this domain, potentially leading to even more efficient and effective modeling techniques.

   Model & std. & log & Batch & Memory & Latency & Samples/s & FLOPs & Param. & MACs \\ Metric & MAE \(\) & MAE \(\) & Size\(\) & GB \(\) & ms \(\) & \(\) & G \(\) & M \(\) & G \(\) \\  DimeNetPP & 0.0188 & -4.0139 & 62 & 12.14 & 84.2 & 704 & 70.53 & 1.89 & 35.27 \\ SphereNet & 0.0184 & -4.0327 & 43 & 17.32 & 134.6 & 448 & 92.56 & 1.90 & 46.28 \\ ComENet & 0.0220 & -3.8403 & 277 & 3.37 & 40.7 & 1536 & 47.25 & 7.36 & 23.63 \\  SaVeNet-B & 0.0156 & -4.2257 & **329** & **2.84** & **27.8** & **2304** & **15.33** & **1.05** & **7.63** \\ SAVeNet-L & **0.0128** & **-4.4408** & 159 & 5.37 & 51.9 & 1216 & 38.93 & 3.01 & 19.37 \\   

Table 6: Efficiency comparison of the proposed SAVeNet with the baselines on Molecule3D.

   Model & L & std. & \(\)std. & log \\  SAVeNet - DN - VA & 6 & 0.79 & - & -5.85 \\ SAVeNet - VA & 6 & 0.77 & -2.5\% & -5.95 \\ SAVeNet - VA & 8 & 0.71 & -10.1\% & -5.98 \\ SAVeNet & 8 & 0.69 & -12.7\% & -6.04 \\ SAVeNet & 12 & 0.63 & -20.3\% & -6.14 \\   

Table 7: Ablation study of SaVeNet