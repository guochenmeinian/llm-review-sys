# A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning

Hangfan Zhang, Jinyuan Jia, Jinghui Chen, Lu Lin, Dinghao Wu

{hbz5148,jinyuan,jzc5917,lulin,dinghao}@psu.edu

The Pennsylvania State University

###### Abstract

Federated Learning (FL) is a distributed machine learning paradigm that allows multiple clients to train a global model collaboratively without sharing their local training data. Due to its distributed nature, many studies have shown that it is vulnerable to backdoor attacks. However, existing studies usually used a predetermined, fixed backdoor trigger or optimized it based solely on the local data and model without considering the global training dynamics. This leads to sub-optimal and less durable attack effectiveness, i.e., their attack success rate is low when the attack budget is limited and decreases quickly if the attacker can no longer perform attacks anymore. To address these limitations, we propose A3FL, a new backdoor attack which adversarially adapts the backdoor trigger to make it less likely to be removed by the global training dynamics. Our key intuition is that the difference between the global model and the local model in FL makes the local-optimized trigger much less effective when transferred to the global model. We solve this by optimizing the trigger to even survive the scenario where the global model was trained to directly unlearn the trigger. Extensive experiments on benchmark datasets are conducted for twelve existing defenses to comprehensively evaluate the effectiveness of our A3FL. Our code is available at https://github.com/hfzhang31/A3FL.

## 1 Introduction

Recent years have witnessed the rapid development of Federated Learning (FL) , an advanced distributed learning paradigm. With the assistance of a cloud server, multiple clients such as smartphones or IoT devices train a global model collaboratively based on their private training data through multiple communication rounds. In each communication round, the cloud server selects a part of the clients and sends the current global model to them. Each selected client first uses the received global model to initialize its local model, then trains it based on its local dataset, and finally sends the trained local model back to the cloud server. The cloud server aggregates local models from selected clients to update the current global model. FL has been widely used in many safety- and privacy-critical applications .

Numerous studies  have shown that the distributed nature of FL provides a surface to backdoor attacks, where an attacker can compromise some clients and utilize them to inject a backdoor into the global model such that the model's behaviors are the attacker desired. In particular, the backdoored global model behaves normally on clean testing inputs but predicts any testing inputs stamped with an attacker-chosen backdoor trigger as a specific target class.

Depending on whether the backdoor trigger is optimized, we can categorize existing attacks into _fixed-trigger attacks_ and _trigger-optimization attacks_. In a fixed-trigger attack, an attacker pre-selects a fixed backdoor trigger and thus does not utilize any information from FL training process. CWhile a fixed-trigger attack can be more efficient and straightforward, it usually suffers from limited effectiveness and more obvious utility drops.

In a trigger-optimization attack, an attacker optimizes the backdoor trigger to enhance the attack. Fang et al.  proposed to maximize the difference between latent representations of clean and trigger-stamped samples. Lyu et al.  proposed to optimize the trigger and local model jointly with \(_{2}\) regularization on local model weights to bypass defenses. The major limitations of existing trigger-optimization attacks are twofold. First, they only leverage local models of compromised clients to optimize the backdoor trigger, which ignores the global training dynamics. Second, they strictly regulate the difference between the local and global model weights to bypass defenses, which in turn limits the backdoor effectiveness. As a result, the locally optimized trigger becomes much less effective when transferred to the global model as visualized in Figure 1. More details for this experiment can be found in Appendix A.1.

**Our contribution:** In this paper, we propose **A**dversarially **A**daptive Backdoor **A**tacks to **F**ederated **L**earning (A3FL). Recall that existing works can only achieve sub-optimal attack performance due to ignorance of global training dynamics. A3FL addresses this problem by adversarially adapting to the dynamic global model. We propose _adversarial adaptation loss_, in which we apply an adversarial training-like method to optimize the backdoor trigger so that the injected backdoor can remain effective in the global model. In particular, we predict the movement of the global model by assuming that the server can access the backdoor trigger and train the global model to directly unlearn the trigger. We adaptively optimize the backdoor trigger to make it survive this adversarial global model, i.e., the backdoor cannot be easily unlearned even if the server is aware of the exact backdoor trigger. We empirically validate our intuition as well as the effectiveness and durability of the proposed attack.

We further conduct extensive experiments on widely-used benchmark datasets, including CIFAR-10  and TinyImageNet , to evaluate the effectiveness of A3FL. Our empirical results demonstrate that A3FL is consistently effective across different datasets and settings. We further compare A3FL with 4 state-of-the-art backdoor attacks [12; 11; 10; 9] under 13 defenses [2; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27], and the results suggest that A3FL remarkably outperforms all baseline attacks by up to 10 times against all defenses. In addition, we find that A3FL is significantly more durable than all baselines. Finally, we conduct extensive ablation studies to evaluate the impact of hyperparameters on the performance of A3FL.

To summarize, our contributions can be outlined as follows.

* We propose A3FL, a novel backdoor attack to the FL paradigm based on adversarial adaptation, in which the attacker optimizes the backdoor trigger using an adversarial training-like technique to enhance its persistence within the global training dynamics.
* We empirically demonstrate that A3FL remarkably improves the durability and attack effectiveness of the injected backdoor in comparison to previous backdoor attacks.
* We comprehensively evaluate A3FL towards existing defenses and show that they are insufficient for mitigating A3FL, highlighting the need for new defenses.

## 2 Related Work

**Federated learning:** Federated Learning (FL) was first proposed in  to improve communication efficiency in decentralized learning. FedAvg  aggregated updates from each client and trains the global model with SGD. Following studies [28; 29; 30; 31; 32] further improved the federated paradigm by making it more adaptive, general, and efficient.

**Existing attacks and their limitations:** In backdoor attacks to FL, an attacker aims to inject a backdoor into model updates of compromised clients such that the final global model aggregated by the server is backdoored. Existing backdoor attacks on FL can be classified into two categories: fixed-trigger attacks [12; 11; 8; 14; 13] and trigger-optimization attacks [10; 9].

Figure 1: A3FL and CerP  can achieve 100% ASR on the local model. However, only A3FL in the mean time obtains a high global ASR.

Fixed-trigger attacks [8; 11; 14; 13; 12] pre-select a fixed backdoor trigger and poison the local training set with it. Since a fixed trigger may not be effective for backdoor injection, these attacks improved the backdoor effectiveness through other approaches including manually manipulating the poisoned updates. Particularly, scaling attack  scaled up the updates to dominate other clients to improve the attack effectiveness. DBA  split the trigger into several sub-triggers for poisoning, which makes DBA more stealthy from defenses. Neurotoxin  only attacked unimportant model parameters that are less frequently updated to prevent the backdoor from being erased shortly.

Trigger-optimization attacks [10; 9] optimize the backdoor trigger to enhance the attack. F3BA  optimized the trigger pattern to maximize the difference between latent representations of clean and trigger-stamped samples. F3BA also projected gradients to unimportant model parameters like Neurotoxin  to improve stealthiness. CerP  jointly optimized the trigger and the model weights with \(_{2}\) regularization to minimize the local model bias. These attacks can achieve higher attack performance than fixed-trigger attacks. However, they have the following limitations. First, they only consider the static local model and ignore the dynamic global model in FL, thus the optimized trigger could be sub-optimal on the global model. Second, they apply strict regularization on the difference between the local model and the global model, which harms the backdoor effectiveness. Therefore, they commonly need a larger attack budget (e.g., compromising more clients) to take effect. We will empirically demonstrate these limitations in Section 4.

**Existing defenses:** In this paper, we consider two categories of defenses in FL. The first category of defense mechanisms is deliberately designed to alleviate the risks of backdoor attacks [17; 19; 20; 18; 33] on FL. These defense strategies work by restricting clients' updates to prevent the attackers from effectively implanting a backdoor into the global model. For instance, the Norm Clipping  defense mechanism limits clients' behavior by clipping large updates, while the CRFL  defense mechanism uses parameter smoothing to impose further constraints on clients' updates.

The second category of defenses [26; 25; 24; 23; 22; 21; 34] is proposed to improve the robustness of FL against varied threats. These defense mechanisms operate under the assumption that the behavior of different clients is comparable. Therefore, they exclude abnormal clients to obtain an update that is recognized by most clients to train the global model. For instance, the Median  defense mechanism updates the global model using the median values of all clients' updates, while Krum  filters out the client with the smallest pairwise distance from other clients and trains the global model solely with the filtered client updates. These defense mechanisms can achieve superior robustness compared to those defense mechanisms that are specifically designed for backdoor attacks. Nevertheless, the drawback of this approach is evident: it often compromises the accuracy of the global model, as it tends to discard most of the information provided by clients, even if these updates are merely potentially harmful.

In this paper, we also utilize backdoor unlearning [35; 36] to approximate existing defenses. Backdoor unlearning typically minimizes the prediction loss of trigger-stamped samples to the ground truth labels. Note that backdoor unlearning disparts from so-called machine unlearning [37; 38; 39], in which the model is unlearned to "forget" specific training samples.

There exist additional defenses in FL that are beyond the scope of this paper. While these defenses may offer potential benefits, they also come with certain limitations in practice. For instance, FLTrust  assumed the server holds a clean validation dataset, which deviates from the typical FL setting. Cao et al.  proposed sample-wise certified robustness which demands hundreds of times of retraining and is computationally expensive.

## 3 Methodology

To formulate the backdoor attack scenario, we first introduce the federated learning setup and threat model. Motivated by the observation of the local-global gap in existing works due to the ignorance of global dynamics, we propose to optimize the trigger via an adversarial adaptation loss.

### Federated Learning Setup and Threat Model

We consider a standard federated learning setup where \(N\) clients aim to collaboratively train a global model \(f\) with the coordination of a server. Let \(_{i}\) be the private training dataset held by the client \(i\), where \(i=1,2,,N\). The joint training dataset of the \(N\) clients can be denoted as \(=_{i=1}^{N}_{i}\)In the \(t\)-th communication round, the server first randomly selects \(M\) clients, where \(M N\). For simplicity, we use \(S_{t}\) to denote the set of selected \(M\) clients. The server then distributes the current version of the global model \(_{t}\) to the selected clients. Each selected client \(i S_{t}\) first uses the global model to initialize its local model, then trains its local model on its local training dataset, and finally uploads the local model update (i.e., the difference between the trained local model and the received global model) to the server. We use \(_{t}^{i}\) to denote the local model update of the client \(i\) in the \(t\)-th communication round. The server aggregates the received updates on model weights and updates the current global model weights as follows:

\[_{t+1}=_{t}+(\{_{t}^{i}|i S_{t}\})\] (1)

where \(\) is an aggregation rule adopted by the server. For instance, a widely used aggregation rule FedAvg  takes an average over the local model updates uploaded by clients.

Attacker's goal:We consider an attacker aims to inject a backdoor into the global model. In particular, the attacker aims to make the injected backdoor **effective** and **drurable**. The backdoor is **effective** if the backdoored global model predicts any testing inputs stamped with an attacker-chosen backdoor trigger as an attacker-chosen target class. The backdoor is **durable** if it remains in the global model even if the attacker-compromised clients stop uploading poisoned updates while the training of the global model continues. We note that a durable backdoor is essential for an attacker as the global model in a production federated learning system is periodically updated but it is impractical for the attacker to perform attacks in all time periods [12; 42]. Considering the durability of the backdoor enables us to understand the effectiveness of backdoor attacks under a strong constraint, i.e., the attacker can only attack the global model within a limited number of communication rounds.

Attacker's background knowledge and capability:Following threat models in previous studies [9; 12; 10; 11; 14; 8], we consider an attacker that can compromise a certain number of clients. In particular, the attacker can access the training datasets of those compromised clients. Moreover, the attacker can access the global model received by those clients and manipulate their uploaded updates to the server. As a practical matter, we consider the attacker can only control those compromised clients for a limited number of communication rounds [12; 11; 8; 10; 9].

### Adversarially Adaptive Backdoor Attack (A3FL)

Our key observation is that existing backdoor attacks are less effective because they either use a fixed trigger pattern or optimize the trigger pattern only based on the local model of compromised clients. However, the global model is dynamically updated and therefore differs from the static local models. This poses two significant challenges for existing backdoor attacks. Firstly, a backdoor that works effectively on the local model may not be similarly effective on the global model. Secondly, the injected backdoor is rapidly eliminated since the global model is continuously updated by the server, making it challenging for attackers to maintain the backdoor's effectiveness over time.

We aim to address these challenges by adversarially adapting the backdoor trigger to make it persistent in the global training dynamics. Our primary objective is to optimize the backdoor trigger in a way that allows it to survive even in the scenario where the global model is trained to directly unlearn the backdoor [35; 36]. To better motivate our method, we first discuss the limitations of existing state-of-the-art backdoor attacks on federated learning.

Limitation of existing works:In recent state-of-the-art works [9; 10], the attacker optimizes the backdoor trigger to maximize its attack effectiveness and applies regularization techniques to bypass server-side defense mechanisms. Formally, given the trigger pattern \(\) and an arbitrary input \(\), the input stamped with the backdoor trigger can be denoted as \(\), which is called the backdoored input. Suppose the target class is \(\). Since the attacker has access to the training dataset of a compromised client \(i\), the backdoor trigger \(\) can be optimized using the following objective:

\[^{*}=*{argmin}_{}_{(, y)_{i}}(,; _{t})\] (2)

where \(_{t}\) represents the global model weights in the \(t\)-th communication round, and \(\) is the classification loss function such as cross-entropy loss. To conduct a backdoor attack locally, the attacker randomly samples a small set of inputs \(_{i}^{b}\) from the local training set \(_{i}\), and poisons inputs in \(_{i}^{b}\) with trigger stamped. The attacker then injects a backdoor into the local model by optimizing the local model on the partially poisoned local training set with regularization to limit the gap between the local and global model, i.e., \(\|-_{t}\|\). While the regularization term helps bypass server-side defenses, it greatly limits the backdoor effectiveness, as it only considers the current global model \(_{t}\) and thus fails to adapt to future global updates.

As illustrated in Figure 1, we observe that such backdoor attack on federated learning (e.g., CerP ) is highly effective on the local model, suggested by a high local attack success rate (ASR). However, due to the ignorance of global dynamics, they cannot achieve similar effectiveness when transferred to the global model, resulting in a low ASR on the global model. Our method A3FL aims to bridge the local-global gap in existing approaches to make the backdoor persistent when transferred to the global model thus achieving advanced attack performance. In particular, we introduce _adversarial adaptation loss_ that makes the backdoor persistent to global training dynamics.

#### Adversarial adaptation loss

To address the challenge introduced by the global model dynamics in federated learning, we propose the _adversarial adaptation loss_. As the attacker cannot directly control how the global model is updated as federated learning proceeds, its backdoor performance can be significantly impacted when transferred to the global model, especially when only a small number of clients are compromised by the attacker or defense strategies are deployed. For instance, local model updates from benign clients can re-calibrate the global model to indirectly mitigate the influence of the backdoored updates from the compromised clients; a defense strategy can also be deployed by the server to mitigate the backdoor. To make the backdoor survive such challenging scenarios, our intuition is that, if an attacker could anticipate the future dynamics of the global model, the backdoor trigger would be better optimized to adapt to global dynamics.

However, global model dynamics are hard to predict because 1) at each communication round, all selected clients contribute to the global model but the attacker cannot access the private training datasets from benign clients and thus cannot predict their local model updates, and 2) the attacker does not know how local model updates are aggregated to obtain the global model and is not aware of possible defense strategies adopted by the server. As directly predicting the exact global model dynamics is challenging, we instead require the attacker to foresee and survive the scenario where the global model is trained to directly unlearn the backdoor. In this paper we consider backdoor unlearning proposed in prior backdoor defenses .

Specifically, starting from current global model \(_{t}\), we foresee an adversarially crafted global model \(_{t}^{}\) that can minimize the impact of the backdoor. We adopt an adversarial training-like method to obtain \(_{t}^{}\): the attacker can use the generated backdoor trigger to simulate the unlearning of the backdoor in the global model. The trigger is then optimized to simultaneously backdoor the current global model \(_{t}\) and the adversarially adapted global model \(_{t}^{}\). Formally, the adversarially adaptive backdoor attack (A3FL) can be formulated as the following optimization problem:

\[&^{*}=*{argmin}_{} _{(,y)_{t}}( ,;_{t})+( ,;_{t}^{})\\ & s.t.\ _{t}^{}=*{argmin}_{} _{(,y)_{t}}( ,y;)\] (3)

where \(\) is initialized with current global model weights \(_{t}\); \(_{t}^{}\) is the optimized adversarial global model which aims to correctly classify the backdoored inputs as their ground-truth label to unlearn the backdoor. In trigger optimization, \(\) is a hyperparameter balancing the backdoor effect on the current global model \(_{t}\) and the adversarial one \(_{t}^{}\), such that the local-global gap is bridged when the locally optimized trigger is transferred to the global model (after server-side aggregation/defenses). Note that attacking the adversarial model is an adaptation or approximation of global dynamics, as in practice the server cannot directly access and unlearn the backdoor trigger to obtain such an adversarial model.

#### Algorithm of A3FL

We depict the workflow of A3FL compromising a client in Algorithm 1. At the \(t\)-th communication round, the client is selected by the server and receives the current global model \(_{t}\). Lines 4-8 optimize the trigger based on the current and the adversarial global model using cross-entropy loss \(_{}\). The adversarial global model is initialized by the global model weights in Line 1, and is updated in Line 10. Lines 12-14 train the local model on the poisoned dataset and upload local updates to the server.

## 4 Experiments

### Experimental Setup

**Datasets:** We evaluate A3FL on three widely-used benchmark datasets: FEMNIST , CIFAR-10 , and TinyImageNet . The FEMNIST dataset consists of 80,5263 images shaped 28\(\)28 distributed across 10 classes. The CIFAR-10 dataset consists of 50,000 training images and 10,000 testing images that are uniformly distributed across 10 classes, with each image having a size of 32\(\)32 pixels. The TinyImageNet dataset contains 100,000 training images and 20,000 testing images that are uniformly distributed across 200 classes, where each image has a size of 64\(\)64 pixels.

**Federated learning setup:** By default, we set the number of clients \(N=100\). At each communication round, the server randomly selects \(M=10\) clients to contribute to the global model. The global model architecture is ResNet-18 . We assume a non-i.i.d data distribution with a concentration parameter \(h\) of 0.9 following previous works [12; 10; 9]. We evaluate the impact of data heterogeneity by adjusting the value of \(h\) in Appendix B.6. Each selected client trains the local model for 2 epochs using SGD optimizer with a learning rate of 0.01. The FL training process continues for 2,000 communication rounds.

**Attack setup:** We assume that the attacker compromises \(P\) clients among all \(N\) clients. All compromised clients are only allowed to attack in limited communication rounds called _attack window_. By default, the attack window starts at the 1,900th communication round and ends at the 2,000th communication round. We also discuss the impact of the attack window in Appendix B.7. When a compromised client is selected by the server during the attack window, it will upload poisoned updates trying to inject the backdoor. We adjust the number of compromised clients \(P\) to comprehensively evaluate the performance of each attack. Different from previous works [11; 12], in our evaluation compromised clients are selected randomly to simulate the practical scenario. Each compromised client poisons 25% of the local training dataset and trains the local model on the partially poisoned dataset with the same parameter settings as benign clients unless otherwise mentioned. By default, the trigger is designed as a square at the upper left corner of the input images. We use the same trigger design for all baseline attacks to ensure the same level of data stealthiness for a fair comparison. We summarize the details of each attack in Appendix A.2. We also discuss different trigger designs of DBA  in Appendix B.9.

**A3FL setup:** By default, compromised clients optimize the trigger using Projected Gradient Descent (PGD)  with a step size of 0.01. The adversarial global model is optimized using SGD with a learning rate of 0.01. In practice, we set the balancing coefficient \(=_{0}(_{t}^{},_{t})\), where \((_{t}^{},_{t})\) denotes the cosine similarity between \(_{t}^{}\) and \(_{t}\). We use similarity to automatically adjust the focus to the adversarial global model: if the adversarial global model is similar to the current global model, it will be assigned a higher weight; otherwise, the adversarial global model is assigneda lower weight. We use the similarity to control the strength of adversarial training, since the backdoor could be fully unlearned if the adversarial global model is aggressively optimized, which makes it difficult to optimize the first term in Equation 3. In adversarial scenarios, it is important to balance the strengths of both sides to achieve better performance, which has been well studied in previous works in adversarial generation [46; 47]. When there are multiple compromised clients in \(S_{t}\), the backdoor trigger is optimized on one randomly selected compromised client, and all compromised clients use this same trigger. We also discuss the parameter setting of A3FL in experiments.

Compared attack baselines:We compare our A3FL to four representative or state-of-the-art backdoor attacks to FL: Neurotoxin , DBA , CerP , and F3BA . We discuss these baselines in Section 2 and also provide an in-detail introduction in Appendix A.2 including specific hyperparameter settings and trigger design of each baseline.

Compared defense baselines:We evaluate A3FL under 13 state-of-the-art or representative federated learning defenses: FedAvg , Median , Norm Clipping , DP , Robust Learning Rate , Deepsight , Bulyan , FedDF , FedRAD , Krum , SparseFed , FLAME , and CRFL . We summarize the details of each defense in Appendix A.3.

Evaluation metrics:Following previous works [10; 12; 11; 8], we use accuracy & backdoor accuracy (ACC & BAC), attack success rate (ASR), and lifespan to comprehensively evaluate A3FL.

* **ACC & BAC:** We define ACC as the accuracy of the benign global model on clean testing inputs without any attacks, and BAC as the accuracy of the backdoored global model on clean testing inputs when the attacker compromises a part of the clients to attack the global model. Given the dynamic nature of the global model, we report the mean value of ACC and BAC. BAC close to ACC means that the evaluated attack causes little or no impact on the global model utility. A smaller gap between ACC and BAC indicates that the evaluated attack has higher utility stealthiness.
* **ASR:** We embed a backdoor trigger to each input in the testing set. ASR is the fraction of trigger-embedded testing inputs that are successfully misclassified as the target class \(\) by the global model. In particular, the global model is dynamic in FL, resulting in an unstable ASR. Therefore, we use the average value of ASR over the last 10 communication rounds in the attack windows to demonstrate the attack performances. A high ASR indicates that the attack is effective.
* **Lifespan:** The lifespan of a backdoor is defined as the period during which the backdoor keeps effective. The lifespan of a backdoor starts at the end of the attack window and ends when the ASR decreases to less than a chosen threshold. Following previous works , we set the threshold as 50%. A long lifespan demonstrates that the backdoor is durable, which means the backdoor remains effective in the global model long after the attack ends. When we evaluate the lifespan of attacks, we extend the FL training process to 3,000 communication rounds.

### Experimental Results

A3FL preserves the utility of the global model:To verify whether A3FL impacts the utility of global models, we compared their ACCs to BACs. The experimental results on CIFAR-10 are shown in Table 1, where NC denotes Norm Clipping, DSight represents Deepsight, and SFed represents SparseFed. Observe that the maximum degradation in accuracy of the global model caused by A3FL is only 0.28%. Therefore, we can conclude that A3FL preserves the utility of the global model during the attack, indicating that our approach is stealthy and difficult to detect. Similar results were observed in the experiments on TinyImagenet, which can be found in Appendix B.1.

A3FL achieves higher ASRs:The attack performances of A3FL and baselines on defenses designed for FL backdoors are presented in Figure 2. The experimental results demonstrate that A3FL achieves higher attack success rates (ASRs) than other baselines. For example, when the defense is Norm

  Defense & FedAvg & NC & RLR & Median & DSight & Bulyan & Krum & SFed & CRFL & DP & FedDF & FedRAD \\  ACC(\%) & 92.29 & 92.57 & 92.21 & 65.59 & 91.79 & 39.57 & 84.56 & 92.60 & 87.40 & 87.71 & 37.58 & 65.89 \\ BAC(\%) & 92.44 & 92.61 & 92.26 & 65.53 & 91.79 & 39.92 & 84.41 & 92.70 & 87.35 & 87.60 & 40.09 & 65.61 \\  

Table 1: A3FL maintains the utility of the global model on CIFAR-10.

Clipping and only one client is compromised, A3FL achieves an ASR of 99.75%, while other baselines can only achieve a maximum ASR of 13.9%. Other attack baselines achieve a comparable ASR to A3FL only when the number of compromised clients significantly increases. For instance, when the defense is CRFL, F3BA cannot achieve a comparable ASR to A3FL until 10 clients are compromised. We have similar observations on other defenses and datasets, which can be found in Figure 8 and 9 in Appendix B.2.

We note that CRFL assigns a certified radio to each sample and makes sure that samples inside the certified radio would have the same prediction. This is achieved by first clipping the updates \(_{t}^{i}\) and then adding Gaussian noise \((0,^{2})\) to \(_{t}^{i}\). During the inference stage, CRFL adopts majority voting to achieve certified robustness. The strength of CRFL is controlled by the value of \(\). We discuss the performance of CRFL under different values of \(\) in Appendix B.5.

**A3FL has a longer lifespan:** We evaluate the durability of attacks by comparing their lifespans. Recall that the attack starts at the 1,900th communication round and ends at the 2,000th communication round. Figure 3 shows the attack success rate against communication rounds when the defense is Norm Clipping and 5 clients are compromised. As we can observe, A3FL has a significantly longer lifespan than other baseline attacks. A3FL still has an ASR of more than 80% at the end, indicating a lifespan of over 1,000 rounds. In contrast, the ASR of all other baseline attacks drops below 50% quickly. We show more results on other defenses in Appendix B.3 and a similar phenomenon is observed. These experimental results suggest that A3FL is more durable than other baseline attacks, and challenge the consensus that backdoors in FL quickly vanish after the attack ends.

Figure 3: A3FL has a longer lifespan. The vertical dotted lines denote the end of the lifespans of each attack when the ASR of the backdoor drops below 50%. The dotted line at the 100th communication round denotes the end of all attacks.

Figure 2: Comparing performances of different attacks on CIFAR-10.

### Analysis and Ablation Study

#### A3FL achieves higher ASR when transferred to the global model:

As discussed in Section 3, A3FL achieves higher attack performance by optimizing the trigger and making the backdoor persistent within the dynamic global model. To verify our intuition, we conducted empirical experiments in which we recorded the Attack Success Rate (ASR) on the local model (local ASR) and the ASR on the global model after aggregation (global ASR). For the experiments, we used FedAvg as the default defense and included five compromised clients among all clients.

The results presented in Figure 4 demonstrate that A3FL can maintain a higher ASR when transferred to the global model. While all attacks can achieve high ASR (\(\) 100%) locally, only A3FL can also achieve high ASR on the global model after the server aggregates clients' updates, which is supported by the tiny gap between the solid line (global ASR) and the dotted line (local ASR). In contrast, other attacks cannot achieve similarly high ASR on the global model as on local models. For instance, F3BA immediately achieves a local ASR of 100% once the attack starts. But it can only achieve less than 20% ASR on the global model in the first few communication rounds. F3BA also takes a longer time to achieve 100% ASR on the global model compared to A3FL. This observation holds for other baseline attacks. We further provide a case study in Appendix B.8 to understand why A3FL outperforms baseline attacks. In the case study, we observe that 1) A3FL has better attack performance than other baseline attacks with comparable attack budget; 2) clients compromised by A3FL are similarly stealthy to other trigger-optimization attacks. Overall, our experimental results indicate that A3FL is a more effective and persistent attack compared to baseline attacks, which makes it particularly challenging to defend against.

#### The impact of trigger size:

We evaluate the performance of A3FL with a trigger size of 3\(\)3, 5\(\)5, 8\(\)8, 10\(\)10 respectively (the default value is 5\(\)5). Figure 6 shows the impact of trigger size on A3FL. In general, the attack success rate (ASR) improves as the trigger size grows larger. When the defense mechanism is Norm Clipping, we observe that the difference between the best and worst ASR is only 1.75%. We also observe a larger difference with stronger defenses like CRFL. Additionally, we find that when there are at least 5 compromised clients among all clients, the impact of trigger size on the attack success rate becomes unnoticeable. Therefore, we can conclude that smaller trigger sizes may limit the performance of A3FL only when the defense is strong enough and the number of compromised clients is small. Otherwise, varying trigger sizes will not significantly affect the performance of A3FL.

#### The impact of \(\):

Recall that \(=_{0}(_{t}^{},_{t})\). We varied the \(_{0}\) hyperparameter over a wide range of values to learn the impact of the balancing coefficient on attack performance and record results in Figure 6. Observe that different \(_{0}\) only slightly impact attack performances with fewer compromised clients. When there are more than 5 compromised clients, the impact of \(_{0}\) is unnoticeable. Forinstance, when the defense is Norm Clipping, the gap between the highest ASR and the lowest ASR is merely 0.5%. We can thus conclude that A3FL is insensitive to variations in hyperparameter \(_{0}\). We further provide an ablation study in Appendix B.4 for more analysis when the adversarial adaptation loss is disabled, i.e., \(_{0}=0\).

## 5 Conclusion and Future Work

In this paper, we propose A3FL, an effective and durable backdoor attack to Federated Learning. A3FL adopts _adversarial adaption loss_ to make the injected backdoor persistent in global training dynamics. Our comprehensive experiments demonstrate that A3FL significantly outperforms existing backdoor attacks under different settings. Interesting future directions include: 1) how to build backdoor attacks towards other types of FL, such as vertical FL; 2) how to build better defenses to protect FL from A3FL.