# On the Trade-off of Intra-/Inter-class Diversity

for Supervised Pre-training

Jieyu Zhang\({}^{1}\), Bohan Wang\({}^{2}\), Zhengyu Hu\({}^{3}\), Pang Wei Koh\({}^{1}\), Alexander Ratner\({}^{1,4}\)

\({}^{1}\) University of Washington \({}^{2}\) USTC \({}^{3}\) HKUST(GZ) \({}^{4}\) Snorkel AI, Inc.

{jieyuz2,pangwei,ajratner}@cs.washington.edu

bhwangfy@gmail.com

zhu021@connect.hkust-gz.edu.cn

These authors contributed equally to this work.

###### Abstract

Pre-training datasets are critical for building state-of-the-art machine learning models, motivating rigorous study on their impact on downstream tasks. In this work, we study the impact of the trade-off between the intra-class diversity (the number of samples per class) and the inter-class diversity (the number of classes) of a supervised pre-training dataset. Empirically, given a fixed pre-training dataset size, we find that the best downstream performance comes with a balance on the intra-/inter-class diversity. To understand the underlying mechanism, we show theoretically that downstream performance depends monotonically on both types of diversity. Notably, our theory reveals that the optimal class-to-sample ratio (\(}{}\)), _i.e._, the ratio of the number of pre-training classes to the number of samples per class, is invariant to the size of the pre-training dataset, enabling the prediction of the optimal number of pre-training classes. We demonstrate the effectiveness of this application by an improvement of approximately 2 points on average on downstream tasks when pre-training on ImageNet.

## 1 Introduction

Many state-of-the-art deep neural network models are pre-trained on large datasets before being finetuned for downstream tasks [13; 17; 23; 1]. While the composition of their pre-training dataset has been shown to be a key factor in the performance of these models [7; 9; 14; 8; 12; 26], how best to design these pre-training datasets still remains underexplored. In this work, we focus on supervised pre-training, one of the most popular pre-training paradigms, and study two key quantities of a supervised pre-training dataset: intra-class diversity (the number of different samples within each pre-training class) and inter-class diversity (the number of different pre-training classes). Intuitively, both diversities are beneficial for supervised pre-training . Yet when the size of the pre-training dataset is fixed, these diversities trade off, since increasing one will decrease the other. Our work studies the impact of this dataset diversity trade-off on downstream performance, as well as how to balance them to design a supervised pre-training dataset with the best downstream performance.

Empirically, with ImageNet  as the pre-training dataset and the pre-training dataset size fixed, we show that the optimal performance on the downstream tasks occurs when a balance on the intra-/inter-class diversity is achieved. We then offer a theoretical explanation for this effect by first modeling the dataset generation process through a two-step sampling framework, and then demonstrating that the test error of the downstream task displays a rational relationship with respect to the class-to-sample ratio, _i.e._, the ratio of the number of pre-training classes to the number of samples per class, or, in other words, the ratio between inter-/intra-class diversity. The established analytical relationshipbetween downstream performance and the class-to-sample ratio can serve as a guiding principle in designing a supervised pre-training dataset by estimating the optimal class-to-sample ratio rather than the grid search.

Notably, our theory shows that given a source of a pre-training dataset and a downstream task, the optimal class-to-sample ratio is invariant to the size of the pre-training dataset. Based on such an invariance, one could estimate the optimal class-to-sample ratio with small pre-training datasets and then leverage it to build a large-scale pre-training dataset. In particular, the optimal number of pre-training classes \(\) and the number of examples per class \(n\) are proportional to the square root of the size of the pre-training dataset \(N\), _i.e._, \(\), which leads to an invariant optimal class-to-sample ratio. We empirically verify our theoretical findings on ImageNet  and present the effectiveness of its application in predicting the optimal number of classes for pre-training datasets with different sizes. In addition, we conducted experiments with different pre-trained datasets, different model backbones, and downstream tasks of different domains to demonstrate that our findings are consistent across many scenarios.

Our major findings and contributions are as follows:

* In supervised pre-training, we observe that with a fixed pre-training dataset size, there exists a trade-off between intra-class and inter-class diversities. This balance between diversities plays a crucial role in shaping the downstream performance, underscoring the significance of considering both aspects when designing the pre-training dataset;
* We then theoretically explain this effect by first modeling the dataset generation process through a two-step sampling framework and then showing that the test error of the downstream task displays a convex relationship with respect to the class-to-sample ratio, serving as a guiding principle in designing a supervised pre-training dataset.;
* Our theory also uncovers the invariance of the optimal class-to-sample ratio with respect to the size of the pre-training dataset, allowing us to predict the optimal number of classes with a small number of pre-training data before building a larger pre-training dataset for a downstream task.

## 2 Empirical Observations

The goal of this work is to study the trade-off of intra-/inter-class diversity in a supervised pre-training dataset and its impact on the pre-trained model's performance on downstream tasks. Specifically, the inter-class diversity refers to the diversity of classes in pre-training dataset, _i.e._, how many different classes we have (\(K\)); while the intra-class diversity refers to the diversity of samples within each class, _i.e._, how many different samples in each class (\(n\)). When the size of the pre-training dataset is fixed, increasing either type of diversity will by definition decrease the other, leading to a dataset diversity trade-off. To study the impact of such dataset diversity trade-off, we experiment with pre-training datasets with varying numbers of classes and number of samples per class. The experimental details can be found in Appendix A.2.

**Evaluation protocol.** Following common practice , we use the ImageNet  as the dataset for supervised pre-training. In this work, we mainly use ResNet-18  as the backbone model. For evaluating the performance of the pre-trained model on downstream tasks, we perform linear probing (tuning the head but freezing the lower layers). We repeat each individual experiment five times and report the averaged top-1 accuracy.

**Downstream tasks.** We adopt the following six datasets as the downstream classification tasks: Stanford40 dataset  for action recognition, StanfordDogs  for fine-grained object recognition, MIT67  for scene classification, CIFAR10  for image recognition datasets, Flowers102  for image classification dataset, FGVCAircraft  for aircraft classification dataset.

While having all the other configurations fixed, during pre-training, we vary the number of classes and the number of samples per class. Specifically, given \(N\) as the size of the pre-training dataset and \(K\) as the number of classes, we randomly sample \(K\) classes from ImageNet and then uniformly sample \(n=\) samples from each class to compose the dataset. We experiment with the following \(N\) and \(K\) values: {1K, 2K, 5K, 10K, 20K, 50K, 100K} and {2, 5, 10, 20, 50, 100, 200, 500, 1000} respectively. Note that with larger \(N\) (_e.g._, \(N=10\)K), we cannot evaluate smaller values of \(K\) (_e.g._, \(K=2\)), since in ImageNet each class has at most 1300 samples.

**Results and observations.** We visualize the results in Figure 1. In the contour plot, the z-value is the error rate on the test set, thus lower is better. The x-axis and y-axis are inter-class diversity (\( K\)) and intra-class diversity (\( n\)) in the log space, respectively. The values on anti-diagonal lines (\(y=-x+c\)) share the same pre-training dataset size as \( N= K+ n\). From the results, we have two observations: 1) **Both intra-/inter-class diversity are beneficial for downstream tasks:** We can see that increasing either inter-class diversity ((\( K\)) or intra-class diversity (\( n\)), given the other is fixed, would lead to a better test error rate. This is intuitive and as expected, since the size of the pre-training dataset \(N\) would increase accordingly, which is known to be beneficial for downstream tasks [11; 7; 13]. 2) **A trade-off of intra-/inter-class diversity on downstream task performance:** More importantly, by looking at the anti-diagonal lines where \( N\) is fixed and equals \( K+ n\), we can see a trade-off between intra-/inter-class diversity on the test error rate of downstream tasks: either cases of 1) high inter-class diversity, low intra-class diversity and 2) low inter-class diversity and high intra-class diversity would not render the best performance. Instead, some point in the middle of the anti-diagonal line leads to the lowest test error rate.

## 3 Theoretical Understanding

In this section, we first present the theoretical setup and notations and then provide a theory on the impact of the pre-training dataset diversity on downstream performance. We also show that the optimal class-to-sample ratio (\(\)) is invariant to the size of the pre-training dataset; such a property can be leveraged to predict the optimal number of classes for building a large pre-training dataset with a small number of data samples first.

### Setup and notations

**Dataset.** To be consistent with our experimental setup, we consider the supervised pre-training task. Specifically, we can access two datasets, one for the pre-training task (denoted as \(S^{p}\)) and another for the downstream task (denoted as \(S^{d}\)). Each example in the _pre-training_ dataset consists of input features \(x=^{d_{1}}\) (where \(d_{1}\) is the dimension of data) and a label \(y[K]\) (where \(K\) is the number of classes). Specifically, we denote \(S^{p}=\{(x_{1},y_{1}),,(x_{N},y_{N})\}\), where \(N\) is the size of \(S^{p}\), and assume that \(S^{p}\) is sampled according to some underlying distribution \(\) (we do

Figure 1: Test error rate (the darker, the better) as a function of intra-class diversity on the y-axis (\( n\)) and inter-class diversity on the x-axis (\( K\)). Each plot represents a different dataset. The red dashed anti-diagonal lines indicate fixed pre-training dataset sizes (\( K+ n= N\) is constant), from which we can see that obtaining the best downstream performance given a fixed pre-training dataset size requires balancing both diversities.

not specify \(\) here because we will analyze cases with different \(\) latter). Every example in the _downstream_ dataset consists of input features \(\) and a label \([]\) (note that \(\) does not necessarily equal-to \(K\)), and is sampled _i.i.d._ according to an underlying distribution \(}\). We denote \(S^{d}=\{(_{1},_{1}),,(_{N},_{})\}\) and thus \(S^{d}}^{}\).

**Model.** The models for both pre-training and downstream tasks consist of two components: the feature extractor and the classifier. Specifically, the model for the pre-training task is given as \(f_{S^{p}} h_{S^{p}}\), where \(f_{S^{p}}:^{d_{2}}^{K}\) is the pre-training classifier (\(d_{2}\) is the dimension of feature) and \(h_{S^{p}}:^{d_{1}}^{d_{2}}\) is the feature extractor. We denote the set of all possible \(f_{S^{p}}\) as \(\), and the set of all possible \(h\) as \(\). The model for the downstream task is given as \(f_{S^{d}} h_{S^{d}}\), where \(f_{S^{d}}:^{d_{2}}^{}\) is the downstream classifier and \(h\) is the feature extractor shared with the pre-training task. We set all possible \(f_{S^{p}}\) as \(}\).

**Loss.** To measure the correctness of model's predictions, we use the cross-entropy loss. Specifically, given an example \((x,y)\) and pre-training/downstream model \(f h\), the corresponding cross-entropy loss is defined as

\[(f h(x),y)=- h(x)}}{_{i}e^{f_{i} h(x )}},\]

where \(f_{i} h(x)\) is the \(i\)-th coordinate of \(f h(x)\). We make the following assumption about the complexity of the model.

**Assumption 1**.: _There exist a positive constant \(M_{}\), such that \( i\), \( f},h,x\),_

\[(f h(x),i) M_{}.\]

_Furthermore, for any distribution \(Q\) over the feature space \(^{d_{1}}\), any \(m\), and any \(^{}\{,}\}\), define the Gaussian complexity of function class \(^{}\) over the marginal distribution \(Q\) with sample number \(m\) as_

\[G^{Q}_{m}(^{})_{(x_{i})_ {i=1}^{m} Q^{m}}_{(_{i,j})_{i[N],j^{ }}(0,_{n K^{}})}_{f,h}_{i=1}^{m}_{j=1}^{K^{}}_{i,j}f_{j}  h(x_{i}).\]

_Here \(K^{}=(^{})\). We assume that \(G^{Q}_{m}() G\), where \(G\) is independent of \(Q\) and \(m\). 2_

**Risks and corresponding minimizers.** We first define the empirical risk \(}_{p}(f h,S^{p})\) and population risk \(_{p}(f h,)\) over the pre-training task as

\[}_{p}(f h,S^{p})_{i=1}^{N}[( f h(x_{i}),y_{i})],_{p}(f h,) _{S^{p}}}_{p}(f h,S^{p}).\]

The corresponding feature extractor and classifier of the empirical risk minimizer over the pre-training task as

\[h_{S^{p}}_{h}(_{f} }_{p}(f h,S^{p})),f_{S^{p}}_{f }(_{h}}_{p}(f h,S^{ p})).\]

Given a feature extractor \(h\), we measure its performance over the pre-training task through a classifier agnostic approach by considering

\[_{p}(h,)_{f}_{p }(f h,).\]

Note here we slightly abuse the notation of \(_{p}\) without causing confusion as \(f h\) and \(h\) have different image spaces. The representation error of \(h\) over \(\) is then defined as the gap between the risk of \(h\) and the smallest possible risk

\[_{p}(h,)_{p}(h,)-_{h }_{p}(,).\]Similarly, the empirical risk \(}_{d}(f h,S^{d})\) and population risk \(_{d}(f h,})\) over the downstream task are defined as

\[}_{d}(f h,S^{d})_{i=1}^{}( h(_{i}),_{i}),\ _{d}(f h,}) _{S^{d}}^{}}}_{d}(f h,S^{d}).\]

The learned classifier from the downstream task can then be defined as

\[f_{S^{d}}_{f}}(_{h} }_{d}(f h_{S^{p}},S^{d})).\]

The corresponding performance and excess risk of \(h\) over the downstream task can then be defined as

\[_{d}(h,})_{ }}_{d}( h,}),\ _{d}(h,})_{d}(h,})-_{ }_{d}(,}).\]

Finally, we are interested in the excess risk of the obtained model \(f_{S^{d}} h_{S^{p}}\) over the downstream task, i.e.,

\[_{d}(f_{S^{d}} h_{S_{p}},})_{d}(f_{S^{d}} h_{S_{p}},})-_{} _{d}(,}).\]

### A theory on the impact of intra-/inter-class diversity trade-off

In this work, we focus on a common practice of collecting the pre-training dataset: one first sample \(K\) classes and then collect \(n\) samples for each class. We start with a detailed characterization of such data generation process, followed by assumptions and the main result.

**Data generation process 1.** We assume the pre-training data is generated through a two-step sampling process. Specifically, suppose that there is a distribution \(\) over \(()\) (the set consisting of all distributions on \(\)). Then, \(S^{p}\) is generated by the following procedure (and \(\) is naturally induced)

* Sample \(K\) classes by i.i.d. sampling \(K\) distributions \(\{_{i}\}_{i=1}^{K}\) according to \(\). These are respectively the underlying distributions of \(K\) classes;
* For each \(i[K]\), i.i.d. sample \(n\) data \(\{x_{i,1},,x_{i,n}\}\) according to \(_{i}\) and denote \(S_{i}=\{(x_{i,1},i),,(x_{i,n},i)\}\). Note here \(x_{i,j}\) does not contain the information of label, as its label information is already contained in \(i\). The whole dataset is obtained by putting all \(S_{i}\) together, i.e., \(S^{p}=\{S_{1},,S_{K}\}\).

We make the following assumption on the correlation between the representation powers of the pre-training and the downstream task.

**Assumption 2**.: _Given \(\), there exists non-negative coefficients \(_{0}^{}}()\) and \(_{1}^{}}()\), such that \( h\),_

\[_{d}(h,})_{1}^{}}()_{p}(h,)+_{0}^{}}().\]

_We further assume that \(_{0}^{}}()\) and \(_{1}^{}}()\) are stable, that is, there exist two \(}\)-dependent positive constants \(M_{0}^{}}\) and \(M_{1}^{}}\), such that for any \(=_{i=1}^{K}(_{i},i)^{n}\) and \(^{}=_{i=1}^{K-1}(_{i},i)^{n}( _{K}^{},K)^{n}\) which differ by only one component, we have that \(|_{0}^{}}()-_{0}^{}}( ^{})|^{}}}{K}\) and \(|_{1}^{}}()-_{1}^{}}( ^{})|^{}}}{K}\). Moreover, we assume that \(_{0}^{}}()\) and \(_{1}^{}}()\) concentrate around their means, i.e., there exist \(_{0}^{}}()\), \(_{1}^{}}()\), \(C_{0}^{}}\) and \(C_{1}^{}}\), such that \(|_{\{_{i}\}_{i=1}^{K}^{K}}_{0}^{ }}()-_{0}^{}}()| ^{}}}{}\) and \(|_{\{_{i}\}_{i=1}^{K}^{K}}_{1}^{ }}()-_{1}^{}}()| ^{}}}{}\)._

Assumption 2 assumes that the pre-training representation error can bound the downstream representation error, which is a common assumption in existing works . Also, as \(\) is derived by sampling \(K\) distributions according to \(\), we make mild assumptions that the coefficients \(_{0}^{}}()\) and \(_{1}^{}}()\) is robust when changing the underlying distribution of only one class, and when \(K\) grows, the expectation of \(_{0}^{}}()\) and \(_{1}^{}}()\) converge to some limits.

**Theorem 3.1**.: _Let Assumptions 1 and 2 hold. For **data generation process 1**, with probability over the sampling of the datasets at least \(1-\), we have_

\[_{d}(f_{S^{4}} h_{S_{p}},}) (_{1}^{}}()+M_{1}}{2K}}+}{})(5M_{}}{2n}}+}{})+_{0}^{}}()\] \[+M_{0}}{2K}}+}{ }+5M_{}}{2N}}+2G}. \]

The detailed proof is deferred to Appendix A.8.1. Below we simplify the right-hand-side of Equation 1 and show that the empirically observed downstream performance trade-off can be explained by such a result.

Simplifying the Theorem 3.1.Denote the RHS of Equation 1 as \(U\), we have

\[U= _{1}^{}}()(5M_{}}{2}}+2G)}+(M_{ 0}}{2}}+C_{0})}+(M _{1}}{2}}+C_{1})\] \[(5M_{}}{2}}+2G )}+_{0}^{}}()+5M_{ }}{2N}}+2G}.\]

We can see that the above equation can be simplified as

\[U=}+}+}+D, \]

where \(A,B,C,D\) do not depend on \(N,K,n\), but instead only depend on the properties of the underlying pre-training and the downstream task data distribution.

Explaining downstream performance trade-off given a fixed \(N\).From Equation 2 we can see that the performance on the target task would increase when we increase 1) intra-class diversity \(n\), 2) inter-class diversity \(K\), and 3) the size of pre-training dataset \(N\). When \(N\) is fixed, however, increasing either intra-class diversity or inter-class diversity would decrease the other (since \(N=n K\)) and therefore eventually lead to a performance drop. Another way to see this is to parameterize \(U\) as a function of \(K\) without \(n\):

\[U(K)=}{}+}+}+D, \]

From this we can clearly see that both extremes of \(K\) (too large or too small) would not lead to optimal performance. A similar conclusion can be drawn regarding \(n\) when parametrizing \(U\) as a function of \(n\) without \(K\).

### Balancing intra-/inter-class diversity: the optimal class-to-sample ratio

When \(N\) is fixed, by leveraging the fact that \(N=n K\), we can express \(U\) as

\[U=}}(Ax^{}+B}} )+c, \]

where \(c=}+D\) is a constant and \(x=\) is the class-to-sample ratio. To minimize \(U\), we have the optimal class-to-sample ratio \(=}{A^{2}}\). Notably, because both \(A\) and \(B\) have no dependency on \(N\), **the optimal class-to-sample ratio for a specific downstream task is invariant to the size of the pre-training dataset.** Motivated by this, one could estimate the optimal class-to-sample ratio using a small \(N\) and then use it to predict the optimal number of classes for building a large pre-training dataset. In particular, given the optimal class-to-sample ratio \(\), the optimal number of classes is \(=\). Based on Equation 4, one only needs three (class-to-sample ratio, performance) tuples to estimate the constants (\(A\), \(B\), \(c\)) with a fixed \(N\) for computing the optimal class-to-sample ratio.

[MISSING_PAGE_FAIL:7]

### Are the trade-off curves for different \(N\) aligned?

According to our findings in Section 3.3, the test error on a downstream task is a convex function with respect to the class-to-sample ratio (Equation 4) and the optimal class-to-sample ratio \(=}{A^{2}}\) is invariant to the size of pre-training dataset \(N\). To empirically verify this, we visualize the performance on downstream tasks as a function of the class-to-sample ratio with different \(N\) in Figure 2.

From the figures, we can see that the curves of different \(N\) for a specific downstream task are aligned, as well as the optimal class-to-ratios, which follows our theoretical findings. This indicates that empirically, one could extrapolate the optimal class-to-ratio estimated with a small \(N\) for building a large-scale pre-training dataset. Note that the rightmost point of each curve corresponds to using all the classes in ImageNet, _i.e._, \(K=1000\), and we can see that such a standard design choice does not lead to optimal downstream performance, especially with small pre-training datasets. In addition, we conducted experiments with different pre-trained datasets (Appendix A.4), different model backbones (Appendix A.6), and downstream tasks of different domains (Appendix A.5) to demonstrate that our findings are consistent across many scenarios.

### Predicting the optimal number of pre-training classes

As a direct application of our theoretical and empirical findings, one could estimate the optimal class-to-sample ratio with a small \(N\) and use it to decide the optimal number of classes when building a larger pre-training dataset. In particular, denoted by \(\) the optimal class-to-sample ratio, the optimal number of classes for a given \(N\) is \(=N}\). We refer to this approach as **Extrapolation**. We empirically compare it against the following methods of deciding the number of classes when building a pre-training dataset: 1) **Standard:** the number of classes equals 1000 as the standard design choice of ImageNet; 2) **Grid Search:** the number of classes corresponding to the data point with the lowest error rate for each curve in Figure 2; 3) **Fitting:** given the target size, we use the corresponding data points in Figure 2 to fit the theoretically-derived performance function (Equation 4), and then analytically calculate the optimal number of classes. We use the calculated number of classes to build a pre-training dataset and measure the performance of a model trained with it. In reality, the latter two baselines require repeatedly training models on pre-training datasets with the target size yet different numbers of classes and are therefore time-consuming and data-intensive.

We set the target size of pre-training dataset as {50K, 100K} and round the number of classes to an integer if needed. For our Extrapolation method, we only use three data points with \(N\) being much smaller than the target size to estimate the optimal class-to-sample ratio: \(N=5000\) and \(K=\{10,50,200\}\). We use the estimated optimal class-to-sample ratio for both target sizes. The results as well as the number of classes selected by the above methods can be found in Table 1. From

Figure 2: Test error rate across class-to-sample ratio. The vertical bar is the standard deviation.

the results, we can see that although the ImageNet dataset is widely used, its number of classes (\(K=1000\)) is not optimal for building a pre-training dataset of 50K/100K samples, since the Standard underperforms other methods. Besides, methods except for the Standard all render similar test error rate even though their number of classes are different, which reveals that the performance is not sensitive to the number of classes as long as we pick a reasonable number. Thus, our Extrapolation method is superior to Grid Search and Fitting, since it needs much fewer samples to estimate the number of classes, while both Grid Search and Fitting require building the pre-training dataset of target size multiple times.

### Extra data needed for estimating the optimal class-to-sample ratio

One advantage of the Standard method is that it does not require extra data for estimating the optimal number of classes. In contrast, other methods would introduce extra data unused in the final pre-training dataset. For example, when estimating the optimal class-to-sample ratio, one may sample data from 1000 classes but eventually find the optimal number of classes is 600, then the data of the additional 400 classes would not be used in the final pre-training dataset. We then investigate how much data is needed by different methods to build the final pre-training dataset. We list the total number of samples used by different methods in Table 2. From the table, we can see that Grid Search and Fitting require much more samples than the target size of the pre-training dataset, since they involve building the pre-training dataset of the target size multiple times, while the number of extra data needed by Extrapolation is relatively small, because it estimates the optimal number of classes using a small \(N\) of 5000. In addition, using Extrapolation, one only needs to estimate the optimal class-to-sample ratio once and then use it for building pre-training datasets with different sizes without re-estimation.

As the Extrapolation requires more data than the Standard, a fairer comparison between them needs to ensure the total number of data used is similar, and the Standard would have a slightly larger pre-training dataset since it does not spend any data budget for estimation. In Figure 3, we compare

    &  &  \\   & & **CIFAR10** & **FGVC Aircraft** & **Flowers102** & **MITG7** & **Stanford40** & **StanfordDogs** \\   & Standard (\(K\)=1000) & 29.19\({}_{0.34}\) & 77.80\({}_{0.38}\) & 34.08\({}_{0.33}\) & 57.51\({}_{0.11}\) & 62.45\({}_{0.38}\) & 64.96\({}_{0.38}\) \\   & Grid Search & 26.24\({}_{0.44}\) & 75.96\({}_{0.43}\) & 32.70\({}_{0.31}\) & 54.10\({}_{0.34}\) & 57.05\({}_{0.44}\) & 59.12\({}_{0.22}\) \\   & & (200) & (200) & (500) & (200) & (100) & (200) \\   & Fitting & 26.25\({}_{1.04}\) & 76.00\({}_{10.44}\) & 32.13\({}_{0.10}\) & 53.60\({}_{0.19}\) & 57.10\({}_{0.13}\) & 59.76\({}_{0.38}\) \\   & & (169) & (293) & (415) & (161) & (138) & (260) \\   & Extrapolation & 26.27\({}_{1.02}\) & 76.18\({}_{1.04}\) & 32.60\({}_{0.106}\) & 53.01\({}_{0.123}\) & 57.25\({}_{0.14}\) & 60.15\({}_{0.17}\) \\   & & (190) & (168) & (296) & (163) & (134) & (158) \\   & Standard (\(K\)=1000) & 25.04\({}_{0.38}\) & 75.21\({}_{0.14}\) & 27.69\({}_{0.40}\) & 52.79\({}_{0.36}\) & 54.19\({}_{0.39}\) & 54.30\({}_{0.44}\) \\   & Grid Search & 23.13\({}_{0.08}\) & 73.01\({}_{0.50}\) & 27.15\({}_{0.44}\) & 50.30\({}_{0.10.44}\) & 51.45\({}_{0.33}\) & 51.98\({}_{0.38}\) \\    & & (500) & (500) & (500) & (200) & (200) & (500) \\    & Fitting & 22.67\({}_{1.03}\) & 73.45\({}_{0.33}\) & 26.67\({}_{0.31}\) & 50.82\({}_{0.13}\) & 52.24\({}_{0.33}\) & 52.24\({}_{0.33}\) \\    & & (276) & (372) & (655) & (249) & (207) & (392) \\    & Extrapolation & 23.12\({}_{0.13}\) & 73.30\({}_{0.17}\) & 26.98\({}_{0.14}\) & 50.42\({}_{0.23}\) & 52.32\({}_{0.008}\) & 53.17\({}_{0.38}\) \\    & & (269) & (238) & (418) & (231) & (190) & (233) \\   

Table 1: Test error rate on downstream tasks (the first row of each task, lower is better), and the number of classes in the pre-training dataset (the second row).

    &  &  \\   & & & **CIFAR10** & **FGVC Aircraft** & **Flowers102** & **MITG7** & **Stanford40** & **StanfordDogs** \\   & Standard (\(K\)=1000) &  \\   & Grid Search & & 150K (5 trials) & & & & \\   & Fitting & 158.164K & 161.570K & 159.403K & 158.694K & 159.268K & 160.538K \\   & Extrapolation & 55.418K & 55.183K & 55.830K & 55.117K & 54.598K & 55.045K \\   & Standard (\(K\)=1000) &  \\   & Grid Search & & & 260K (4 trials) & & & \\    & Fitting & 272.336K & 271.836K & 268.164K & 269.878K & 261.981K & 270.579K \\    & Extrapolation & 103.937K & 103.608K & 104.517K & 103.515K & 103.049K & 103.401K \\   

Table 2: Total number of samples used for building the pre-training dataset.

the Extrapolation to the Standard whose pre-training dataset size is slightly larger than the total number of data used by the Extrapolation. Each bar plot represents a specific downstream task, and the number in parentheses indicates the size (\(N\)) of the corresponding pre-training dataset. We can observe that in most cases, the Extrapolation demonstrates improved performance over the Standard even when the latter uses a larger pre-training dataset, which further justifies the effectiveness of the Extrapolation method.

## 5 Related Work

We briefly review recent studies on supervised pre-training from data-centric perspectives. First, on the composition of the pre-training dataset,  presents a scaling law that predicts the test loss on downstream tasks under varying source dataset compositions, while  studies the performance on downstream tasks when subsets of the pre-training dataset are removed. Second, on the label space of supervised pre-training,  offers a statistical analysis explaining pre-training techniques' success in NLP, showing that class diversity in pre-training tasks substantially enhances sample efficiency in downstream tasks, while the study by  explores the impact of pre-training label granularity on downstream tasks, emphasizing the importance of selecting an appropriate level of label granularity. Lastly,  explores the impact of pre-training data distribution on transfer performance, finding the choice of the pre-training dataset to be crucial. In contrast, we dive into the trade-off of the intra-/inter-class diversity in the supervised pre-training dataset. The study related the most to ours is , where the authors empirically examined the importance of pre-training data characteristics on downstream performance. While covering a wide range of pre-training data characteristics, this study only briefly explores the trade-off of intra-/inter-class diversity in the pre-training dataset (Section 5.5 in ). Specifically, the authors only considered two different cases of intra-/inter-class diversity, _i.e._, \(K=\{500,1000\}\) for ImageNet. In contrast, we, both empirically and theoretically, show how such a trade-off would impact the downstream performance and our theory uncovers a surprising property of the optimal class-to-sample ratio: it is invariant to the size of the pre-training dataset.

## 6 Conclusion

In this study, we explore the trade-off of the intra-/inter-class diversity in supervised pre-training datasets of fixed size. We discovered that the optimal downstream performance is achieved through a balance of intra-/inter-class diversity. Our theory demonstrates that downstream performance depends on both diversities, and the optimal class-to-sample ratio remains constant regardless of the dataset size. We apply this finding to predict the optimal number of classes in pre-training datasets and provide evidence of its effectiveness across many scenarios.

Figure 3: Each bar plot visualizes the error rates of different methods on a specific downstream task. The number in parentheses is \(N\), i.e. the size of the corresponding pre-training dataset.