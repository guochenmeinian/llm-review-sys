# PiCO: Peer Review in LLMs based on the Consistency Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel **unsupervised evaluation direction**, utilizing _peer-review_ mechanisms to measure LLMs automatically without any human feedback. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings. We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.

## 1 Introduction

Goodhart's Law: _"When a measure becomes a target, it ceases to be a good measure."_

Large language models (LLMs)[11; 2; 12; 43] have achieved remarkable success across a variety of real-world applications [54; 32; 36; 52]. With the increasingly widespread application of these models, there is an urgent need for an effective evaluation method to ensure that their performance and usability meet the growing demands. To assess the ability level of LLMs, a large number of evaluation benchmarks have been proposed by using some small and domain-specific datasets with human-curated labels, such as MMLU , HELM , Big-Bench, GLUE. However, these benchmarks can only measure LLMs' core capability on a confined set of tasks (e.g. multi-choice knowledge or retrieval questions), which fails to assess their alignment with human preference in open-ended tasks adequately [16; 28; 34]. On the other hand, these evaluations may suffer from _benchmark leakage_ issue, referring that the evaluation data is unknowingly used for model training, which can also lead to misleading evaluations [49; 56]. Therefore, blindly improving scores on these public benchmarks cannot always yield a large language model that truly satisfies human requirements.

For assessing human preferences, recent studies have focused on building crowdsourced battle platforms with human ratings as the primary evaluation metric. Typical platforms include Chatbot Arena , MT-Bench , and AlpacaEval . It constructs anonymous battles between chatbots in real-world scenarios, where users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences. While human evaluation is the gold standard formeasuring human preferences, it is exceptionally slow and costly. In addition, adding a new LLM to the crowdsourced battle platforms also poses a cold-start issue . Thus, a fundamental question arises: _can we construct an unsupervised LLMs evaluation system without relying on any human feedback_?

Actually, in real human evaluation systems, people build their ability hierarchy based on different empirical assumptions. For example, majority voting  and rating voting  methods are widely used during the decision-making process, which are based on the wisdom of the crowds  and have been proven to lead to better results than that of an individual. Moreover, in the established practice of _peer-review_ in academic research, scholars evaluate their academic level rankings based on the _consistency assumption_, _i.e._, scholars with stronger abilities have stronger persuasiveness for evaluating others, and can also obtain higher achievements. This paper attempts to explore whether similar phenomena exist in the LLMs evaluation systems.

In this work, we propose **PiCO**, a **P**eer review approach in LLMs based on **C**onsistency **O**ptimization. In this setting, LLMs themselves act as "reviewers", engaging in mutual assessments to achieve comprehensive, efficient, and performance evaluations without relying on manually annotated data. This method aims to address the limitations of existing evaluation approaches and provide insights into LLMs' real-world capabilities. As shown in Figure 1, both open-source and closed-source LLMs lie in the same environment and answer the open-ended questions from an unlabeled dataset. Then, we construct anonymous answer pairs, while randomly selecting other LLMs as "reviewers" to evaluate both responses with a learnable confidence weight \(w\). Finally, we employ this weight and calculate the response scores \(G\) for each LLM based on the weighted joint evaluation. It is worth noting that the whole _peer-review_ process works in an unsupervised way, and our goal is to optimize the confidence weights that re-rank the LLMs to be closer to human rankings.

To achieve this, we formalize it as a constrained optimization based on the consistency assumption. We maximize the consistency of each LLM's capability \(w\) and score \(G\) while adjusting the final ranking to align with human preference more closely. The key assumption behind this is that high-level LLM can evaluate others' answers more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher answer-ranking scores. As a result, the entropy (controversy) of the whole _peer-review_ evaluation system can be minimized. In other words, the consistency optimization aims to find a final score ranking that all LLMs have no "disputes" regarding.

To evaluate the gap in aligning human rankings, we propose three metrics called PEN (**P**ermutation **E**ntropy), CIN (**C**ount **I**n**versions), LIS (**L**ongest **I**n**creasing **S**ubsequence). The experiments are conducted on multiple crowdsourcing datasets and validated on these three metrics. The experimental results demonstrate that the proposed PiCO framework can effectively obtain a large language models' leaderboard closer to human preferences.

Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM’s response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the _consistency assumption_, while reducing the entropy of the _peer-review_ evaluation system. The consistency optimization aims to find a final score ranking that all LLMs “agree” it.

The contributions of this paper can be summarized as follows.

* We explore a novel unsupervised LLM evaluation direction without human feedback, utilizing _peer-review_ mechanisms to measure LLMs automatically. All LLMs can answer unlabeled questions and evaluate each other.
* A constrained optimization based on the consistency assumption is proposed to re-rank the LLMs to be closer to human rankings.
* We propose three metrics called PEN, CIN, and LIS on the PiCO framework for evaluating the gap with human preferences.
* The experiments with these metrics on three crowdsourcing datasets validate the effectiveness of the proposed approach.

## 2 The Proposed Approach

In this section, we first describe the problem definition and preference alignment evaluation, and then introduce the proposed PiCO framework in detail.

### Definition and Metrics

**Problem Definition.** In this subsection, we aim to measure the ability level of LLMs automatically without relying on human annotations. Thus we consider an unsupervised LLM evaluation scenario with an unlabeled dataset \(\) consisting of \(n\) open-ended questions, where \(=\{Q_{i}\}_{i=1}^{n}\). In addition, we have a large language model pool \(=\{M_{j}\}_{j=1}^{m}\), which includes both open-source and closed-source models. Write \(M_{1} M_{2}\) to indicate that the LLM \(M_{1}\) has stronger capabilities than the LLM \(M_{2}\). Thus, we can assume that the ground-truth ranking \(^{*}\) alignment with human preferences,

\[^{*}:=[M_{1} M_{2} M_{3}... M_{m}],\] (1)

and assume that the learned ranking \(}\) by different evaluation methods is as follows,

\[}:=[M_{3} M_{1} M_{2}... M_{m}].\] (2)

The goal is to build an LLM ranking \(}\) that aligns with human ranking \(^{*}\), making the loss \(\) of the both rankings tend towards \(0\), _i.e._, \((},^{*}) 0\)

**Preference Alignment Metrics.** Before building LLM rankings, we first need to discuss how to evaluate aligned human rankings. Intuitively, the metrics we want mainly describe the differences between two arrays composed of ranking indices. Assuming that human ranking \(^{*}\) is defined as being well-ranked in ascending order (\([1,2,3,...,m]\)) as shown in Eq 1. Thus the metric is to quantify the randomness of the learned ranking array (\([3,1,2,...,m]\)) as shown in Eq 2. Based on this, we propose three metrics called PEN, CIN, and LIS, respectively.

PEN (**P**ermutation **E**ntropy). Permutation entropy  is a concept used to quantify the complexity or randomness of time series data. It provides a measure of the irregularity or unpredictability of the order of values in a sequence. We thus utilize it to measure the gap with human rankings as follows,

\[_{PEN}(},^{*}):=- p() p(),\] (3)

where

\[p()=,...,M_{t+k})\}}{m-k+1}.\]

Figure 2: Preference alignment metric. Three metrics for evaluating the gap with human preferences called PEN, CIN, and LIS, respectively

\(\) denotes different permutations, \(k\) is a hyper-parameter recommended to be set to 3 to 7, and we set \(k=3\) in this paper. Intuitively, it samples some subsequences and calculates the entropy for all permutation types. And the lower the permutation entropy in the learned LLM rankings, the closer it is to the ground-truth human rankings.

CIN (**C**ount **In**versions). Counting inversions  aims to measure the degree of disorder or "invertedness" in an array or sequence of elements. We thus define it as follows,

\[_{CIN}(},^{*}):=_{M_{i},M_{j} }\{M_{i} M_{j} i<j\}.\] (4)

Where \(\{\}\) is the indicator function that the value is 1 when the condition is met, otherwise it is 0. Intuitively, the fewer inverse pairs in the learned LLM rankings, the closer it is to the ground-truth human rankings.

LIS (**L**ongest **I**ncreasing **S**ubsequence). The longest increasing subsequence aims to find the length of the longest subsequence in a given sequence of elements, where the subsequence is in increasing order. We utilize it to measure the degree of match with human rankings as follows,

\[_{LIS}(},^{*}):=\{dp[i] 1  i m\},\] (5)

where

\[dp[i]=1+\{dp[j] 1 j<i M_{j} M_{i}\}.\]

\(dp[i]\) represents the length of the longest increasing subsequence that ends with \(M_{i}\). LIS allows for a nuanced understanding of the degree to which the learned ranking aligns with the ideal human ranking, with a higher LIS length indicating greater alignment.

### Algorithm Details

The PiCO framework, depicted in Figure 3, involves peer-review and consistency optimization stages. In the peer-review stage, we first collect an unlabeled dataset \(\) consisting of open-ended questions, and construct a large language model pool \(\) that includes both open-source and closed-source LLMs. Then, we let all LLMs answer each unlabeled question to obtain the response set \(\). We shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs as "reviewers" to evaluate both responses with a learnable confidence \(w\). Finally, we can obtain the answer-ranking data \(\) and calculate the response score \(G\) for each large language model. In the consistency optimization phase, we maximize the consistency of each LLM's capability \(w\) and score \(G\) with constrained optimization, while re-ranking the LLMs to be closer to human rankings.

#### 2.2.1 Peer Review Stage

**Data Collection and LLMs Pool Construction.** Benefiting from the creation of crowdsourced battle platforms, we accessed open assessment datasets from Chatbot Arena, MT-Bench,

Figure 3: The pipeline of the PiCO. It is mainly composed of two components: the peer-review and consistency optimization stages. Specifically, in the peer-review stage, the unlabeled dataset \(\) and the LLMs pool \(\) are given. Then, we let all LLMs answer each unlabeled question to obtain the response set \(\). We shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs to evaluate both responses with a learnable confidence \(w\). As a result, we can obtain the answer-ranking data \(\) which is a quadruple that records the partial order between two answers and the evaluator’s confidence weight. In the consistency optimization stage, we update the parameter \(w\) by maximizing the consistency of each LLM’s capability and score, while re-ranking the LLMs to be closer to human rankings.

and AlpacaEval. These open datasets include critical fields such as "question_id" and "question_content." Utilizing the Chatbot Arena dataset, which features pairwise data from twenty LLMs with human preference annotations, we assembled an LLM pool \(=\{M_{j}\}_{j=1}^{m}\). Leveraging 33K human-annotated interactions from this dataset, we established a ground-truth ranking \(^{*}\) and gathered responses \(=\{\{A_{i}^{j}\}_{i=1}^{n}\}_{j=1}^{m}\) for our dataset \(=\{Q_{i}\}_{i=1}^{n}\).

**Answer-Ranking Data Construction Based on Peer Review.** After obtaining the responses set \(\), we aim to generate answer-ranking data \(\) through the peer-review mechanism. Specifically, for the same question \(Q_{i}\), we randomly construct a battle pair \(<A_{i}^{j},A_{i}^{k}>\) for review. Each battle pair will be randomly assigned five models ("reviewers") to determine the winners or declare ties. Note that the model may evaluate its own answers, but the entire process is anonymous. As a result, we can obtain the quadruples \((A_{i}^{j},A_{i}^{k},>w^{s})\), indicating the "reviewer" \(M_{s}\) believes that the answer \(A_{i}^{j}\) is better than answer \(A_{i}^{k}\) with a confidence \(w^{s}\). Therefore, the answer-ranking data \(\) can be defined as follows,

\[=\{(A_{i}^{j},A_{i}^{k},>,w^{s})\}_{i,j,k,s},\] (6)

where \(i\) denotes the question index, and \(j,k,s\) indicate the model indices. \(w^{s}\) is a learnable confidence of model \(M_{s}\), and \(>\) is a partial order relationship from \(\{>,<,=\}\).

#### 2.2.2 Consistency Optimization Stage

As shown in Eq 6, following the peer-review mechanism, we construct anonymous answer pairs and randomly select other LLMs as "reviewers" to evaluate both responses with a learnable confidence \(w\). Next, we expect to optimize the confidence \(w\) and re-rank the LLMs to be closer to human rankings. We thus propose the consistency assumption, _i.e._, high-level LLM can evaluate others' answers more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher answer-ranking scores. Formally, we maximize the consistency of each LLM's capability \(w\) and score \(G\) with constrained optimization as follows,

\[*{argmax\,\,Consistency}(G,w)\] (7) \[\ G_{j}=_{(A_{i}^{j},A_{i}^{k},>,w^{s}) }\{A_{i}^{j}>A_{i}^{k}\}*w^{s},\]

where \(\{\}\) is the indicator function that the value is 1 when the condition is met, otherwise, it is 0. \(G_{j}\) denotes the response score of model \(M_{j}\), which is calculated by joint evaluation of other models. Moreover, we employ Pearson correlation  to measure the consistency between \(w\) and \(G\). Note that we only introduce this straightforward implementation to validate our idea of PiCO. Other more advanced strategies may be employed to further improve the performance.

**Discussion:** It is worth noting that the whole process (Eq. 6 and 7) works in an unsupervised way. The only thing we do is to adaptively assign each LLM a score that matches its abilities. An intuitive example is as follows: in a real peer-review system, if the academic level of three scholars \(a\), \(b\), and \(c\) satisfies the following relationship, \(w^{a}>w^{b}>w^{c}\). So, in the ultimate ideal scenario, the ranking of the scores submitted by these three scholars should also be, \(G_{a}>G_{b}>G_{c}\). In other words, the sorting of \(G\) and \(w\) satisfies high consistency. On the other hand, scholars with stronger abilities (_i.e._, scholar \(a\)) evaluate \(A^{b}>A^{c}\) have stronger persuasiveness, so scholar \(b\) should also receive higher weighted scores \(1*w^{a}\).

**Reviewer Elimination Mechanism.** Realizing that not all LLMs have sufficient ability to evaluate the responses of other models. We thus introduce an unsupervised elimination mechanism to remove those LLMs that have low scores. It iteratively removes the lowest-scoring LLM from the "reviewer queue" for the next consistency optimization stage, until 60% of models are eliminated. The whole process of the approach is summarized in Algorithm 1, and the details can be found in Appendix D.

## 3 Experiments

**Datasets.** To validate the effectiveness of the proposed approach, we perform experiments on Chatbot Arena, MT-Bench, and AlpacaEval. The MT-Bench dataset assesses six LLMs' responses to 80 multi-category questions. The Chatbot Arena Conversations Dataset, with 33K conversations from 13K IPs during April-June 2023, evaluates real dialogue performance. AlpacaEval dataset

[MISSING_PAGE_FAIL:6]

### Performance Comparison

We validate the effectiveness of the proposed PiCO method on three datasets by comparing the following two types of methods, _i.e._, the wisdom of the crowds and recent SOTA LLMs evaluation methods. The average results of PEN, CIN and LIS are demonstrated in Table 1. The ratios of response sets \(\) are 1, 0.7, and 0.4, respectively.

The results presented in Table 1 illustrate the proposed PiCO method consistently surpasses competing approaches across the majority of evaluated metrics Notably, PiCO achieves performance improvements of 0.1, 2.5, and 0.92 on the PEN, CIN, and LIS metrics, respectively, compared to the Runner-up. These results underscore the superiority of aggregating evaluations from multiple models, such as Majority Voting, Rating Voting, PRD, and PRE, as opposed to relying solely on single-model methods like GPTScore and PandaLM. This collective model approach, leveraging 'the wisdom of the crowds', more accurately aligns with human rankings in our open-question evaluation framework.

In comparison with existing peer review evaluation methods(_i.e.,_ PRD and PRE), it is evident that PiCO exhibits improvements across various evaluation metrics. Despite PRD's adjustment of model weights based on their win rates and PRE's reliance on supervised human feedback data to assign weights through a qualification exam, neither method achieves performance superior to the fully unsupervised PiCO approach. These methods rely on predefined criteria and human feedback, potentially leading to biases or suboptimal performance. In contrast, PiCO leverages unsupervised learning techniques, allowing it to autonomously adapt and discover patterns in the data without explicit human intervention.

It is important to highlight that PandaLM, a language model equipped with 7 billion parameters, was fine-tuned using labels generated by GPT-3.5-turbo as the ground truth, achieving stable performance across various datasets. However, in our unsupervised, open-ended experimental setup, which focuses on ranking-based metrics, GPTScore exhibits less robustness regardless of whether the base model is GPT-3 (davinci-002) or fran-t5-xx.

### Exploring the Role of Confidence Weight

In this subsection, we will show that the confidence weight \(w\) learned by our _consistency optimization_ can reduce the system evaluation bias. Specifically, we first study whether the "review" model would

Figure 4: Heatmap distribution of preference gap (PG) metric among seven LLMs across three datasets. Higher values (above 0) indicate greater evaluation bias. The first row shows original PG values in three datasets, while the second row displays PG values re-weighted using our learned confidence weights.

prefer a particular model's response. Following , we employ the preference gap (PG) to evaluate the bias as follows,

\[PG(i,j)=P_{i}(i>j)-P_{j}(i>j),\] (8)

where \(P_{i}(i>j)\) represents the winning rate of model \(i\) as the "reviewer" believes that \(i\) defeated \(j\). The heatmap distribution of the PG value \(PG(i,j)\) among seven LLMs across three datasets is demonstrated in the first row of Figure 4. It can be observed that the evaluation system exhibits severe bias. Especially on ChatGLM-6B and Mpt-7B models, they often believe that their results are better than other ones, as their PG values are greater than 0 across three datasets.

After the _consistency optimization_, we assign the learned confidence weight \(w\) to the corresponding model and ultimately obtain the re-weighting PG value \((i,j)\) as follows,

\[(i,j)=w_{i} P_{i}(i>j)-w_{j} P_{j}(i>j).\] (9)

The results of the re-weighting PG value \((i,j)\) are displayed on the second row of Figure 4. It can be observed that the learned confidence weight \(w\) can significantly mitigate the preference gaps of the whole evaluation system. In our consistency optimization, LLMs such as ChatGLM-6B and Mpt-7B have lower weights, and reducing their confidence can effectively alleviate the system evaluation bias.

### Study of Elimination Mechanism

The PiCO and PRE methods both employ elimination mechanisms to remove those weakest LLMs from the "reviewer queue" during the evaluation process. As shown in Figure 5, the x-axis quantifies the number of reviewers eliminated, and the y-axis measures the CIN, where lower scores denote higher performance. Due to space limitations, more results on PEN and LIS metrics can be found in Appendix E. It can be observed that both PiCO and PRE exhibit better performance with an increasing number of eliminated "reviewers". The proposed PiCO approach can achieve better performance than PRE in most cases. It is worth noting that the PRE method employs the accuracy of "qualification exams" to eliminate weak LLMs, and this process requires human annotation . On the contrary, the elimination process of our PiCO method is unsupervised and can still achieve better evaluation results than PRE.

### Validation of Consistency Assumption

In this subsection, we conduct the ablation study to validate the effectiveness of the _consistency assumption_. Specifically, we first manually construct three methods: Forward Weight Voting, Uniform Weight Voting, and Reverse Weight Voting. That is, the ability weights of the model are respectively weighted forward (\(w=[1,0.9,...,0]\)), uniformly (\(w=[1,1,...,1]\)), and backward (\(w=[0,0.1,...,1]\)) according to the ground-truth human ranking. Then, we randomly initialize the ability weights and employ our _consistency optimization_ to adjust the weight. In addition, we also collect the average performance of "reviewer queue", _i.e._, employing a single LLM as the "reviewer" to evaluate all response pairs and then calculate the average results of all LLMs.

As shown in Table 2, it can be observed that the Forward Weight Voting achieves better results than the Uniform and Backward ones in all cases, while the Backward one achieves worse results. It validates that assigning larger weights to those models with stronger capabilities can obtain better

Figure 5: Performance comparison of the PiCO (Ours) and PRE methods on the Chatbot Arena, MT-Bench, and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is CIN, where lower values indicate better performance.

results. Most importantly, employing our consistency optimization algorithm to assign weights to different review models can further improve the performance of the evaluation system, _i.e._, lower PEN and CIN, as well as higher LIS in all cases. Moreover, it is worth noting that the average performance of the "reviewer queue" is very poor, even worse than the Backward Weight Voting. This means that the answer-ranking data \(\) contains a lot of evaluation noise, while the proposed approach can still optimize weights and obtain better ranking results. In summary, the above experimental results validate the effectiveness of the consistency assumption from various perspectives.

## 4 Related Work

**Evaluation Benchmarks for Diversity.** LLMs are designed to handle a variety of tasks, necessitating comprehensive benchmarks. Notable benchmarks include GLUE and SuperGLUE, which simulate real-world scenarios across tasks such as text classification, translation, reading comprehension, and dialogue generation. HELM provides a holistic evaluation of LLMs, assessing language understanding, generation, coherence, and reasoning. BIG-bench pushes LLM capabilities with 204 diverse tasks. MMLU measures multitask accuracy across domains like mathematics and law. However, these evaluations can be compromised by benchmark leakage, where evaluation data inadvertently used for training leads to inflated performance metrics[4; 56].

**Human Evaluation.** Human evaluation provides reliable feedback that closely aligns with real-world applications. Liang et al. evaluated summary and misinformation scenarios across multiple models. Ziems et al. involved experts to assess model outputs in various domain-specific tasks. Bang et al. examined ChatGPT's performance in summarization, translation, and reasoning using human-annotated datasets. The LMSYS initiative introduced platforms like Chatbot Arena, relying on human ratings as the primary evaluation metric. Despite its effectiveness, human evaluation is costly and subject to bias and cultural differences.

**Large Language Models for Evaluation.** The development of open-source LLMs has led to the use of LLMs as evaluators. GPTScore uses models like GPT-3 to assign probabilities to high-quality content through multidimensional evaluation. Bubeck et al. tested GPT-4, finding it rivaling human capabilities. Lin and Chen introduced LLM-EVAL for evaluating dialogue quality with single prompts. PandaLM employs LLMs as "judges" for evaluating instruction tuning. However, reliance on a single model can introduce biases such as positional, verbosity, and self-favoring biases[33; 55]. ChatEval proposes a multi-agent framework to simulate human evaluation processes. Similarly, PRE and PRD use LLMs as evaluators, combining multiple evaluation outcomes for automated assessment. However, the PRE method, which relies on human feedback for supervised evaluation throughout the process, still incurs relatively high costs.

## 5 Conclusion

In this paper, we propose the novel Peer Review method based on the Consistency Optimization (PiCO) to automatically evaluate Large Language Models (LLMs) without relying on human feedback. PiCO utilizes _peer-review_ mechanisms to autonomously assess LLMs in a shared environment, where both open-source and closed-source models can respond to unlabeled questions and evaluate each other. In this setup, each LLM's response score is determined collectively by other anonymous models, aiming to maximize consistency across capabilities and scores. We propose three metrics, _i.e.,_ PEN, CIN, and LIS, to quantify the disparity from human preferences. The extensive experiment results across multiple datasets and metrics demonstrate that PiCO effectively generates an LLM leaderboard that aligns closely with human preferences. In the future, we plan to extend the peer-review mechanism to evaluate the capabilities of multi-modality large models.

    &  &  &  \\  & PEN (\(\)) & CIN(\(\)) & PEN (\(\)) & CIN(\(\)) & PEN (\(\)) & CIN(\(\)) \\  Average Performance of Reviewer Queue & \(1.49^{ 0.28}\) & \(34.87^{ 1.46}\) & \(1.49^{ 0.26}\) & \(38.80^{ 1.28}\) & \(1.50^{ 0.23}\) & \(33.13^{ 1.37}\) \\  Backward Weight Voting & \(1.43^{ 0.04}\) & \(25.00^{ 0.00}\) & \(1.43^{ 0.05}\) & \(26.00^{ 0.00}\) & \(1.36^{ 0.03}\) & \(24.00^{ 0.00}\) \\ Uniform Weight Voting & \(1.34^{ 0.23}\) & \(22.00^{ 0.00}\) & \(1.39^{ 0.02}\) & \(24.00^{ 0.00}\) & \(1.34^{ 0.03}\) & \(22.00^{ 0.00}\) \\ Forward Weight Voting & \(1.32^{ 0.03}\) & \(21.00^{ 0.00}\) & \(1.33^{ 0.03}\) & \(23.00^{ 0.00}\) & \(1.30^{ 0.05}\) & \(21.00^{ 0.00}\) \\  Random Weight + Consistency Optimization & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\   

Table 2: Ablation study comparing Backward, Uniform, Forward weight voting, and Consistency Optimization methods with the Average Performance of Reviewer Queue across three datasets.