# Efficient Large Multi-modal Models

via Visual Context Compression

Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille

Johns Hopkins University

Contributed equally.Advised equally.

###### Abstract

While significant advancements have been made in compressed representations for text embeddings in large language models (LLMs), the compression of _visual_ tokens in multi-modal LLMs (MLLMs) has remained a largely overlooked area. In this work, we present the study on the analysis of redundancy concerning visual tokens and efficient training within these models. Our initial experiments show that eliminating up to 70% of visual tokens at the testing stage by simply average pooling only leads to a minimal 3% reduction in visual question answering accuracy on the GQA benchmark, indicating significant redundancy in visual context. Addressing this, we introduce _Visual Context Compressor_, which reduces the number of visual tokens to enhance training and inference efficiency without sacrificing performance. To minimize information loss caused by the compression on visual tokens while maintaining training efficiency, we develop _LLaVolta_ as a light and staged training scheme that incorporates stage-wise visual context compression to progressively compress the visual tokens from heavily to lightly compression during training, yielding no loss of information when testing. Extensive experiments demonstrate that our approach enhances the performance of MLLMs in both image-language and video-language understanding, while also significantly cutting training costs and improving inference efficiency.

## 1 Introduction

The advent of LLMs  has marked a new era in the field of artificial intelligence and natural language processing. LLMs can play a role as a universal interface for a general-purpose assistant, where various task instructions can be explicitly represented in language and guide the end-to-end trained neural assistant to solve a task of interest. For example, the recent success of ChatGPT  and GPT-4  have demonstrated the power of aligned LLMs in following human instructions, and have stimulated tremendous interest in developing open-source LLMs . As the horizon of LLM applications broadens and the availability of open-source LLMs increases, the integration of multi-modality into these models presents a new frontier in expanding their capabilities. Multi-modal LLMs  (MLLMs), which can process and understand not just text but also visual information, stand at the cutting edge of this evolution.

While MLLMs have made significant strides, a crucial aspect that remains relatively unexplored is the efficient representation and processing of visual information within these models. Substantial efforts  have been dedicated to optimizing the efficient representation of text tokens through various compression techniques , aimed at enhancing inference efficiency byattentively selecting important tokens. However, the efficient learning of _visual_ tokens in MLLM has not garnered comparable attention. Naturally, this raises questions about the potential redundancy present in visual tokens and its implications for the overall computational efficiency of MLLMs.

We start our work by addressing the question: _Are visual tokens redundant in multi-modal LLMs?_ To explore this, we first experiment with simply reducing the number of visual tokens in a pre-trained LLaVA-1.5-7B  at the inference stage via average pooling (SS3.2). As shown in Fig.1 (left), our initial results demonstrate that eliminating up to 70% of visual tokens by pooling them with a stride of 4 starting from Transformer layer 2 incurs only a minimal performance loss on the GQA benchmark, specifically a 3% accuracy reduction. Additionally, we compute and present the average attention values from the [ANS] token to visual tokens and system prompt tokens across different Transformer layers in the pre-trained LLaVA-1.5-7B . As revealed in Fig. 1 (right; blue trends), the visual tokens are generally less attended to, measured based on average attention from the [ANS] token, as the layers get deeper. These two early explorations indicate significant redundancy in visual tokens.

Addressing this, in this work we develop an effective _Visual Context Compressor_ that can be integrated into the training of MLLMs. Surprisingly, a simple average pooler nested in LLMs stands out as the most effective compressor, outperforming the attention-based [18; 53] and parametric  counterparts. We attribute this to two reasons: (1) The simple pooling operation makes training stable, whereas prior attention-based approaches [18; 53] are specifically designed for accelerating inference rather than training. (2) Visual tokens in the deeper Transformer layers are less attended to (see Fig. 1 (right)) and particularly redundant, making a simple compressor placed in a deeper Transformer layer effective enough. At a lower training cost, the LLaVA-1.5-7B  trained with the proposed _Visual Context Compressor_ is competitive with the non-compressed baseline across various multi-modal benchmarks (_e.g._, GQA  and MM-Vet ). This dual achievement highlights _Visual Context Compressor_'s role as a pivotal advancement in enhancing the efficiency and performance of MLLMs across various multi-modal question-answering benchmarks.

To further mitigate the information loss caused by compressing visual tokens, especially under a large compression ratio (CR), we have devised a **LLaVA**-**p**owered **l**ite **t**raining scheme, dubbed _LLaVolta_, which progressively employs _Visual Context Compressor_ at multiple training stages with different compression ratios (SS3.3). Specifically, _LLaVolta_ progresses through several stages, beginning with a high level of visual token compression and gradually reducing the compression ratio until the final stages, where full visual tokens are utilized. This multi-stage approach allows for adaptive compression levels that ensure training efficiency without losing information at testing, thus maintaining the overall effectiveness of the model.

Extensive experimental evaluations of _LLaVolta_ have been conducted on thirteen widely-adopted MLLM benchmarks for both image-language understanding and video-language understanding,

Figure 1: **Visual tokens are redundant in MLLMs. Left:** The accuracy of the LLaVA-1.5-7B  model(without re-train) on the GQA  benchmarks varies with different percentages of retained visual tokens. The \(x\)-axis represents the percentage of original visual tokens preserved after applying 1D average pooling with varying stride sizes \(S\) applied in \(i\)-th Transformer layer. **Right:** Visual tokens receive less attention from the [ANS] token as we go deeper into its layers of LLaVA-1.5-7B model. These findings collectively suggest a significant redundancy within the visual tokens of the MLLMs.

showing promising results. We observe that _LLaVolta_ not only enhances the performance of MLLMs, but also achieves a substantial reduction in training costs. These experiments validate the effectiveness of our method, demonstrating its capability to optimize resource utilization while maintaining or even improving model performance.

In summary, our paper makes the following contributions:

* We present two initial studies to verify the redundancy of visual tokens in MLLMs.
* We propose the _Visual Context Compressor_, a simple yet effective compression technique that utilizes an average pooler, enhancing the efficiency of multi-modal models.
* We propose the _LLaVolta_ as an efficient training scheme by leveraging _Visual Context Compressor_ at multiple training stages with a progressively decreasing compression ratio. To the best of our knowledge, we are among the first to explore efficient training of MLLMs.
* Extensive experiments show that our approach not only improves the performance of MLLMs in image-language and video-language understanding across various benchmarks but also showcases efficiency gains by reducing training costs by 16% and inference latency by 24%.

## 2 Related Works

**Multi-modal LLMs.** The evolution of large language models [10; 33; 34] into their multi-modal counterparts [28; 40] represents a significant leap in their ability to follow instructions and generalize across tasks. This transition has been marked by seminal works such as Flamingo , BLIP-2  and LLaVA , which have extended LLM capabilities to encompass visual tasks, demonstrating impressive zero-shot generalization and in-context learning abilities. Progress in multi-modal LLMs has primarily been driven by advancements in visual instruction tuning [28; 54], leveraging vision-language datasets and refining visual instruction-following data. Additionally, efforts have been made to enhance the grounding capabilities of multi-modal LLMs through the use of specialized datasets aimed at improving task-specific performance. Despite these advancements, the exploration of visual compression within multi-modal LLMs remains relatively underdeveloped. The design and optimization of compression strategies are crucial for maximizing the effectiveness and efficiency of multi-modal LLMs, suggesting a potential area for future research and development.

**Visual Redundancy.** In computer vision, reducing redundancy is crucial for creating efficient yet effective models without losing accuracy . Redundancy in images often arises from the inherent characteristics of natural scenes, including repetitive patterns, textures, and areas of uniform color. These features, while contributing to the richness and detail of visual perception, can lead to inefficiencies in both storage and processing when not adequately addressed. Image compression algorithms  can reduce file size by eliminating or efficiently encoding redundant data. These methods take advantage of human visual perception's tolerances to subtly reduce data without significantly impacting image quality. Advanced machine learning models, particularly CNNs and autoencoders , offer sophisticated approaches to minimizing redundancy. Transformers , as a fundamental architecture for LLMs [10; 34], apply self-attention mechanisms to dynamically bind the most informative parts of tokens. Vision Transformers [6; 7; 8; 12; 16] trained with CLIP objective [7; 36] encode an image to a sequence of visual features for multi-modal LLMs . Nevertheless, visual tokens receive less attention in LLMs due to attention shrinkage , resulting a waste of computation. In this work, we focus on reducing the redundancy of visual tokens in MLLMs.

**Efficient LLMs.** Efficient inference and training for LLMs are important. Compressing input sequences for efficiency reasons in Transformers is not a new idea for NLP. Much work is being done to accelerate the inference of LMs. For example, Pyramid Transformer variants  and  are proposed in Encoder-Decoder LMs that progressively compress the sequence as the layers grow deeper via pooling or core-set selection. Nawrot et al.  propose adaptively compressing the sequence based on the predicted semantic boundaries within the sequence. Rae et al.  propose compressing the fine-grained past activations to coarser memories. VCC  compress the sequence into a much smaller representation at each layer by prioritizing important tokens. Besides efficient inference, accelerating training for LLMs attracts attention as well. A staged training setup  is proposed which begins with a small model and incrementally increases the amount of compute used for training by applying a growth operator to increase the model depth and width. However, efficient training for LLMs in multi-modal scenarios is rarely explored.

## 3 Method

In this section, we first introduce an overview of multi-modal LLMs in SS 3.1. Then, we define the problem of visual redundancy and introduce _Visual Context Compressor_ in SS 3.2. Finally, we present our proposed _LLaVolta_ in SS 3.3.

### Preliminaries: A Multi-modal LLM

We start by reviewing the design of the LLaVA family [27; 28]. For processing an input image \(_{v}\), we utilize the pre-trained CLIP visual encoder ViT-L/14, as detailed by , to extract the visual feature \(_{v}=g(_{v})\), where \(g(.)\) indicates the visual encoder. To bridge the gap between visual and linguistic modalities, the LLaVA [27; 28] framework as an MLLM implements a straightforward linear/MLP transformation. This involves a trainable projection matrix \(\), which maps the visual features \(_{v}\) into the linguistic embedding space, producing language embedding tokens \(_{v}=_{v}\). These tokens are designed to match the dimensionality of the word embeddings within the LLM.

For each image \(_{v}\), one can generate multi-turn conversation data \((_{q}^{1},_{a}^{1},,_{q}^{T},_{ a}^{T})\) with \(T\) as the number of turns. One can organize them as a sequence, by treating all answers as the assistant's response and the instruction \(_{}^{t}\) at the \(t\)-th turn as:

\[_{}^{t}=\{[_{q}^{1},_{v}]\;\;\;\;[_{v},_{q}^{1}], &t=1\\ _{q}^{t},&t>1.\] (1)

This approach establishes a standardized format for the multi-modal instruction-following sequence. It allows for the instruction-based tuning of the LLM to be applied to the prediction tokens, utilizing the model's native auto-regressive training objective. Specifically, for a sequence with length \(L\), the likelihood of the target responses \(_{a}\) is calculated as:

\[p(_{a}|_{v},_{})=_{i=1}^{L}p _{}(x_{i}|_{v},_{,<i},_{a,<i}),\] (2)

### Visual Context Compressor

**Problem Formulation**: The redundancy observed in images often arises from inherent traits of natural scenes, including repetitive patterns, textures, and regions with uniform color. While these traits enrich visual perception by offering detail and depth, they can also present challenges in terms of storage and processing efficiency. Considering the inherent limitations of Transformers in handling long sequences [2; 49; 29], it is critical to minimize any length redundancies to obtain a more effective accuracy/efficiency trade-off.

The objective of this study is to decrease the length of visual tokens \(_{v}\) (_i.e._, its hidden states \(_{v}\) if inside LLMs), while simultaneously maximizing the probability of the target response \(p(_{a}|_{v},_{})\) as described in Equation (2).

_Visual Context Compressor_: A key design change that we introduce is a compressor layer that compresses the dimensions of the visual inputs by reducing the effective number of visual tokens. As depicted in Fig. 2, the compressor is simply an average pooler in our setting. It is applied to the visual tokens in \(k\)-th Transformer layer of an LLM. Formally, given the hidden visual tokens at \(k\)-th Transformer layer \(_{k}^{B C L}\), the compressor is expected to fulfill the following projection: \(f:^{B C L}^{B C L_{}},\) which results in compressed visual tokens \(}_{k}^{B C L_{}}\), where \(L_{}=\) with \(s\) as the compression stride. In SS4, we explore multiple variants of compressor \(f\) to reduce the token length, including random token dropping  with dropping ratio \(1-\), K-Means  with number of centroids set to \(N_{C}=\), attention-based token-centric compression , attention-based token dropping [9; 18], and average pooling with stride \(s\). To our surprise, we find that the simple average pooler is the most effective compressor for vision tokens within MLLMs, due to its stability during training detailed in SS 4.4. Thus, we choose average pooler as the compressor.

Note that the proposed _Visual Context Compressor_ can be directly applied to any off-the-shelf MLLMs to assess the visual redundancy, as conducted in SS4.2. One can also train an MLLM with _Visual Context Compressor_ to reduce the number of visual tokens while maintaining competitive multi-modal performance.

**Compression Ratio (CR)3**. For an LLM with \(N\) Transformer decoder layers, the compression ratio for visual tokens can be calculated as:

\[=+K L},\] (3)

where \(K\) is the \(K\)-th Transformer layer of a multi-modal LLM; \(L\) is the the length of visual tokens input into Visual Context Compressor; \(L_{out}\) is the compressed length of visual tokens generated by Visual Context Compressor, as illustrated in Fig. 2.

Our architecture modifications thus far mostly impacts the inference efficiency of MLLM, however, its impact on performance-compression trade-off remains unclear. We will study this question in the context of **training** MLLMs with a goal of enhancing efficiency without compromising performance. We then move on to further utilize _Visual Context Compressor_ to design an efficient training scheme to incorporates Visual Context Compressor at various stages of the training process.

### _LLaVolta_ as a Light, Staged Training Scheme

Training with _Visual Context Compressor_ not only facilitates efficient inference but also enhances training efficiency. However, devising an effective training scheme poses challenges when ensuring fair comparisons with the original LLaVA , primarily due to differences in the number of tokens involved in inference. This discrepancy may lead to information loss, particularly when operating under a scenario with a high compression ratio. To tackle this issue, we have developed a lite training scheme for LLaVA, dubbed as _LLaVolta_, which employs stage-wise visual context compression. Generally, assuming there are \(N_{s}\) total stages, stage \(i\) involves \(}\) of the total training epochs with a compression ratio of \(r_{i}\), and the final stage proceeds without any compression. Essentially, as training progresses, \(i\) increases while \(r_{i}\) decreases.

In this work, as depicted in Fig. 3, we primarily explore a three-stage training pipeline that progressively reduces the compression ratio, as detailed below:

**Training Stage I: Heavy Compression**. The MLLM training at the first one-third of the total training iterations commences with a heavy compression ratio (> 500%), where _Visual Context Compressor_ is applied in an early layer of the LLM with a large pooling stride. This setup enables a very fast training speed.

**Training Stage II: Light Compression**. The MLLM continues training with another one-third of the total training epochs. At this stage, _Visual Context Compressor_ is applied at only the deeper layers of the LLM with a smaller pooling stride compared to Training Stage I.

**Training Stage III: No/subtle Compression**. The MLLM continues training during the final one-third of the total epochs, with either no compression or subtle compression applied. This stage is designed to align with the inference process, where visual tokens may also undergo compression. By maintaining consistency between training and inference, this approach ensures that critical information is preserved while still allowing for compression, minimizing any potential discrepancies between training and real-world use.

Given the above meta framework, we can instantiate a family of training schemes, as demonstrated in Tab. 1. The single-stage (non-compression) scheme is equivalent to the MLLM baseline. For

Figure 2: Example of Visual Context Compressor in a multi-modal LLM.

multi-stage training, the compression stage can either go deeper or wider. "deeper" implies an increase in \(K\) (Transformer layer), while "wider" means a decrease in the stride of the pooler.

Note that all training schemes will be standardized to complete just one epoch. Thus, in the three-stage training, each stage will receive one third of an epoch, while in the four-stage training, each stage will receive one fourth of an epoch. Effects of non-uniform stage splitting are presented in the Appendix.

## 4 Experiments

In this section, we begin by detailing the experimental setup in SS 4.1. Next, we elaborate on the proof-of-concept in Section SS 4.2. Following this, we validate the proposed _LLAVolta_ in SS 4.3 with an ablation study in SS 4.4. Finally, we assess the extensibility to video-language in SS 4.5.

   \#Stages & Scheme & Stage & Layer & Stride & CR & \#Epoch \\   &  & \(S1\) & _/_ & _/_ & 100\% & 1 \\   &  & \(S1\) & 2 & 8 & 557\% & 0.5 \\  & & \(S2\) & _/_ & _/_ & 100\% & 0.5 \\   &  & \(S1\) & 2 & 8 & 557\% & 0.33 \\  & & \(S2\) & 16 & 8 & 178\% & 0.33 \\  & & \(S3\) & _/_ & _/_ & 100\% & 0.33 \\   &  & \(S1\) & 2 & 8 & 557\% & 0.33 \\  & & \(S2\) & 2 & 2 & 188\% & 0.33 \\  & & \(S3\) & _/_ & _/_ & 100\% & 0.33 \\   

Table 1: **Instantiations of _LLAVolta_ schemes**. deeper indicates that the compressor’s position in the LLM shifts from the shallow layer (_e.g._, 2) to a deeper layer (_e.g._, 16). wider indicates that the compressor’s stride decreases while the number of visual tokens increases. Last stage compression refers to using compressor at last stage for efficient inference.

Figure 3: Training & inference paradigm comparison for conventional setting (A) and _LLAVolta_ (B). Meta framework of _LLAVolta_ consists three training stages: Stage I with heavy visual compression; Stage II with light visual compression in _deeper_ layer; Stage III with subtle compression with _wider_ token window without loss of performance. This can accelerate the training and inference by 18+% while maintaining performance.

### Experimental Setup

We adopt the Vicuna-v1.5-7B  as the language model, leveraging the LLaMA2 codebase . We leverage the pre-trained CLIP ViT-L/14 [12; 36] with an input resolution of \(336 336\), resulting in \(576\) visual tokens. We employ the LLaVA framework  to connect the frozen CLIP vision encoder and the Vicuna LLMs. Along with the projector, we train the entire LLM instead of parameter-efficient finetuning. We follow LLaVA-1.5  to perform data preparation and training schedule for pretraining and instruction tuning. We conduct all the experiments with the machine of 8\(\) Nvidia RTX 6000 Ada. Due to multiple invalid image links in the dataset of instruction tuning stage, the scores of LLaVA-1.5 reported in our analysis are reproduced by ourselves to ensure a fair comparison under the same experimental environment.

It is worth mentioning that assessing visual token redundancy only necessitates the inference of existing off-the-shelf models, whereas the other experiments involve the training of multi-modal LLMs, specifically projectors and LLMs.

**Benchmarks and Metrics**: We adopt thirteen benchmarks specifically designed for MLLM evaluation, including GQA , MM-Vet , ScienceQA (SQA), MME, TextVQA , POPE , MMBench , MMBench-CN , VQA-v2 , LLaVA-Bench-in-the-Wild (LLaVA\({}^{W}\)) , VisWiz , SEED-Image  and MMMU . GQA and VQA-v2 evaluate the model's visual perception capabilities on open-ended short answers. MME-Perception evaluates model's visual perception with yes/no questions. ScienceQA with multiple choice are used to evaluate the zero-shot generalization on scientific question answering. TextVQA contains text-rich visual question answering. MMBench and the CN version evaluate a model's answer robustness with all-round shuffling on multiple choice answers. MM-Vet evaluates a model's capabilities in engaging in visual conversations. Additionally, we extend _LLaVolta_ to video-language understanding, and follow Video-LLaVA  to evaluate the models on MSVD-QA , MSRVTT-QA  and ActivityNet-QA , where the accuracy and score are assessed using GPT-Assistant.

We report the official metrics calculated using the standard implementations provided for each benchmark for a fair comparison. Latency is reported as the time taken during inference until the first answer token is produced. When reporting average performance in Table 2, the score of MME is divided by 2000, as its range is from 800 to 2000. TFLOs are profiled via DeepSpeed. For total number of tokens, \(\#\)Tokens \(=_{i}^{N}\#\)Token\({}^{i}\). The training time is reported for one epoch of training during the LLaVA instruction-tuning stage. The Compression Ratio (CR) is defined as in Equation 3.

### Proof of Concept: Visual Context Redundancy

To assess the redundancy of visual tokens, we perform average pooling within an off-the-shelf LLaVA-1.5-7B checkpoint at the testing stage, using different pooling stride sizes \(S\) across various Transformer layers \(K\). As shown in Fig. 1, the model still exhibits strong performance even when retaining only 62.5% of the visual tokens (\(S=4,K=16\)) in the MM-Vet benchmark, without the need for additional training. When adopting the same setting (\(S=4,K=16\)), a similar trend can be observed in the GQA benchmark as well, where the compressed model only has 1% performance drop than the uncompressed counterpart. Surprisingly, in the GQA benchmark, eliminating up to 70% of visual tokens (\(=4,K=16\)) results in a mere 3% decrease in performance. This proof-of-concept shows a certain level of redundancy in the visual tokens within MLLMs.

### Main Results: _LLaVolta_

In this section, we present the main results of _LLaVolta_ schemes instantiated in SS 3.3. We conduct a thorough evaluation of the multi-modal capability across 13 benchmarks. Tab. 2 demonstrates that our proposed _LLaVolta_ not only consistently lowers training costs by 19% (15.3 hours _vs_. 12.4 hours) but also surpasses the non-compression baseline. The last-stage-compression training schemes achieves the best performance across thirteen benchmarks and obtains 62.1% average performance, improving LLaVA-v1.5-7B  with much less inference TFLOs and training time. This indicates the necessity of designing an optimally

### Ablation Study

In this section, we perform an ablation study on the choice of visual compressors by comparing different compression methods. Additionally, we examine the effects of varying the stride and LLM layer in training _Visual Context Compressor_.

**Choice of Visual Compressors**. The design choices include (1) random token dropping, (2) K-Means clustering, (3) average pooling, (4) FastV , (5) VCC , (6) parametric pre-trained Q-Former . We have the following three observations. Firstly, Tab. 3 shows that the attention-based methods, including FastV and VCC win 9/13 best and second best scores, showcasing the high performance when compressing visual tokens in inference. However, they are ineffective when applied to training because the in-training attention scores are unstable. Secondly, and surprisingly, the average pooling obtains the highest scores on eleven out of thirteen benchmarks when it is used to train MLLMs with a high CR. Thirdly, Tab. 4 shows that both Q-Former and average pooling can obtain reasonably good performance when trained with extremely high CRs, and the average pooling performs better with less training cost. The reason could be that the Q-Former resamples tokens outside the LLM, potentially causing the LLM to overlook crucial information relevant to the response. In contrast, our approach employs average pooling subsequent to Transformer layer \(K\), allowing the initial \(K\) layers of the LLM to effectively retain important information from uncompressed tokens. Given these three insights, we select average pooling as our favored approach for visual compression.

**Performance Across Compression Ratios**. Herein, we train the multi-modal LLM with our _Visual Context Compressor_ in various settings. As demonstrated in Tab. 5, the proposed method offers certain improvements and trade-offs compared to the state-of-the-art method, LLaVA-1.5-7B. We have the following two observations. Firstly, in the heavy compression level, the performance of MLLM is inversely proportional to the compression ratio (linearly scaling to the number of visual

tokens). Secondly, the performance of MLLMs at the light compression level does not correlate directly with the number of visual tokens, making this observation somewhat unexpected. We attribute this to the MLLMs at this level of compression being relatively insensitive to changes in the compression ratio. This indicates that MLLMs trained at a light compression level will not hurt the model performance at all. For instance, the setting of stride 16 in light compression level attains a 188% CR and also outperforms the baseline LLaVA-v1.5-7B across all four metrics. The above observations pave the way for developing a more systematic training scheme.

**Scalability to Larger Models.** As modern multimodal LLMs (MLLMs) continue to grow in size and complexity, it is crucial to determine whether the performance gains observed in smaller models can be extended to larger architectures. This ablation allows us to verify if our compression strategies maintain or even enhance their effectiveness as the model scales, ensuring their applicability to more complex real-world scenarios. As demonstrated in Tab. 6, our four-stage scheme achieved comparable performance with standard training while saving 16%(21.1 vs 17.6) training time.

**Comparison with Layer-wise progressive Compression.** Given the success of stage-wise compression in accelerating training, we hypothesize that it's also beneficial for layer-wise progressive compression. To explore this, we applied nested compressors with varying strides across layers, with smaller strides in the shallower layers, where visual tokens receive more attention. As shown in Tab. 7, we experimented with a multi-stage configuration: layers 0-3 with stride=1, layers 4-11 with stride=2, layers 12-23 with stride=4, and layers 24-31 with stride=8(CR=267%). This was compared to a single-stage compression setup: layer=8, stride=8(CR=266%). While the progressive layer-wise compression showed superior performance in direct inference, it underperformed when retrained. We

    &  &  Train \\ \#Tokens \\  } &  &  &  QAM \\  } &  MVNet \\  } &  SQA \\  } &  MME \\  } &  VQA\({}^{T}\) \\  } &  POPE \\  } &  MMB \\  } &  MMB \\  } &  MMB \\  } &  VQA\({}^{T}\) \\  } & 
    &  & ^{}\)} &  &  & ^{}\)} &  &  &  &  &  \\  & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & \\  Single & no compression & 147456 & - & 29.68 & 40.7h & 3.69 & 69.1 & 3.48 & 56.8 & 3.28 & 47.5 & 3.48 & 57.8 \\  Two & compression & 80496 & 183\% & 17.73 & 37.1h & 3.71 & 69.0 & 3.50 & 56.9 & 3.29 & 47.9 & 3.50 & 57.9 \\ Three & compr. deeper & 84776 & 174\% & 17.29 & 37.1h & 3.73 & 69.3 & **3.51** & **57.2** & 3.28 & 47.4 & **3.51** & 58.0 \\ Three & compr. wider & 83256 & 177\% & 16.86 & 37.0h & 3.72 & 69.0 & **3.51** & **57.2** & **3.29** & 47.7 & **3.51** & 58.0 \\ Four & wider then deeper & 88704 & 166\% & 18.32 & 37.2h & 3.72 & 69.1 & **3.51** & **57.2** & 3.27 & **48.0** & 3.50 & 58.1 \\ Four & deeper then wider & 86904 & 170\% & 18.64 & 37.1h & **3.74** & **69.8** & 3.49 & 56.9 & 3.27 & 47.8 & 3.50 & **58.2** \\   

Table 8: **Performance of _LLaVolta_ on VideoLLaVA. See the definition of each training scheme in Tab. 1. \(\): average across stages. To implement our multi-stage training, we apply the same compression processing to the 8 frames representing the video respectively. _The derived six training schemes achieve competitive results while reducing 9% training time._**

    &  &  &  &  &  & ^{T}\)} &  &  & ^{CN}\)} & ^{2}\)} & ^{}\)} &  & ^{I}\)} &  &  \\  & & & & & & & & & & & & & & & & & \\   \\  Single Stage & 267\% & 57.8 & 25.3 & 70.2 & 1337 & 52.1 & 86.0 & 60.4 & 52.2 & 74.6 & 56.0 & 48.1 & 58.3 & 33.3 & 57.0 \\ Multi Stage & 266\% & 60.7 & 28.9 & 70.3 & 1403 & 55.4 & 85.1 & 65.2 & 57.1 & 77.7 & 60.6 & 49.1 & 64.8 & 35.2 & 60.0 \\   \\  Single Stage & 267\% & 60.7 & 30.7 & 71.3 & 1456 & 56.9 & 86.4 & 64.6 & 58.0 & 77.9 & 67.0 & 48.8 & 66.0 & 35.3 & 61.3 \\ Multi Stage & 266\% & 60.9 & 29.5 & 70.5 & 1408 & 55.9 & 84.8 & 65.4 & 57.4 & 76.6 & 61.1 & 48.9 & 64.7 & 34.9 & 60.2 \\   

Table 7: **Comparison between single stage compressor and multi stage compressor. mMti-stage compression outperforming single-stage in direct inference across most tasks. However, in retrained models, multi-stage compression only shows marginal improvements, with a slight increase in the average performance.**