# Model - GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild

Xinyu Zhao\({}^{*1}\), Guoheng Sun\({}^{*2}\), Ruisi Cai\({}^{*3}\), Yukun Zhou\({}^{*4}\), Pingzhi Li\({}^{*1}\), Peihao Wang\({}^{*3}\)

Bowen Tan\({}^{5}\), Yexiao He\({}^{2}\), Li Chen\({}^{6}\), Yi Liang\({}^{6}\), Beidi Chen\({}^{5}\), Binhang Yuan\({}^{4}\)

Hongyi Wang\({}^{17}\), Ang Li\({}^{ 2}\), Zhangyang Wang\({}^{ 3}\), Tianlong Chen\({}^{ 1}\)

\({}^{1}\)UNC CH \({}^{2}\)UMD \({}^{3}\)UT Austin \({}^{4}\)HKUST \({}^{5}\)CMU \({}^{6}\)Google \({}^{7}\)Rutgers University

{xinyu,pingzhi,tianlong}@cs.unc.edu, {ghsun,yexiaohe,angliece}@umd.edu

{ruisi.cai,peihaowang,atlasswang}@utexas.edu

yzhoufw@connect.ust.hk, {btan2,beidic}@andrew.cmu.edu

li.lizliz.chen@gmail.com, yiliang@google.com

biyuan@ust.hk, hongyi.wang.001@rutgers.edu

\({}^{*}\)Equal Contribution \({}^{}\)Equal Supervision

###### Abstract

As Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has gained significant attention, which is challenged by potential performance drop when combining disparate models. Various techniques have been proposed to aggregate pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed. In light of this research gap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. Utilizing the insights from the benchmark results, we formulate a strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization. Our methodology involves clustering mergeable models, selecting a merging strategy, and integrating model clusters through model-level mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, Model-GLUE shows an average performance enhancement of 5.61%, achieved without additional training. Codes are available at https://github.com/Model-GLUE/Model-GLUE.

## 1 Introduction

Large Language Models (LLMs) have demonstrated unparalleled capability in a diverse array of natural language tasks, encompassing commonsense reasoning, question answering, and specialized domains such as mathematics and programming . The effectiveness of LLMs is based on the scaling law, which posits that proportionally increasing model and training data size leads to enhanced model performance . Nevertheless, the computation overhead and data requirement surge as LLM continues to scale. With the widespread of open-sourced general or specialized LLMs, aggregating existing models to construct a more versatile LLM emerges as an economical alternative to training a larger LLM from scratch . This not only mitigates the computation cost but also leverages the collective advancements of previous efforts in building LLMs.

Within different methods to combine existing LLMs, a major class is merging . Model merging combines multiple models into a single one of the same size through weight-space transformation. Wortsman et al.  first propose to merge a few fine-tuned models as a training trick for the flat loss-landscape, and Ilharco et al.  extends it to multi-task scenario, both of which employ the simple averaging. Other works propose more complicated merging methods,leveraging weight sparsity [63; 64] and non-uniform coefficient [4; 35]. However, they assume that all candidate models are "useful" when merging. While this may hold for small-sized designed model collections, it may not be the case in real-world scenarios given a large and divergent model zoo. How to ensure the benefits of merging different model zoo sizes and similarities, and exclude "harmful" candidates, remains underexplored.

Since merging is limited to the same model structures and initial weights, another alternative is Mixture-of-Experts (MoE) . MoE is a conditional computation architecture that activates only a subset of model parameters for each specific input example . MoE LLMs have already demonstrated performance and computational efficiency advantages over their dense counterparts [15; 25; 30; 68]. In particular, we use a broader term "mixture" to denote the aggregation of existing expert LLMs according to the MoE paradigm, which has been successfully implemented in some recent practices [50; 54; 55]. However, these implementations neglect the inherent flexibility of MoE to integrate different expert models, especially those groups that do not work with merging. Also, the difference and possible synergy between merging and mixing have not been thoroughly investigated. Based on the above challenges, our primary research question is formulated as:

_(Q) Is it feasible to establish a benchmark for selecting and aggregating Large Language Models (LLMs) from an extensive and varied model zoo based on current state-of-the-art model merging and mixture, thereby enhancing the overall competence of the final model?_

To address (Q), we present Model-GLUE, a comprehensive benchmark and set of guidelines for LLM scaling. Model-GLUE is the first work for LLM scaling encompassing a wide range of model group sizes and variability, with a principal emphasis on the merging and mixture methodologies, and also discussion of model stacking. We first delve into merging scheduling, analyzing strategies for identifying potentially detrimental model candidates and various merging techniques. We then explore a variety of model mixtures as an alternative to merging, covering different mixture granularity, routers architecture, routing input inputs, _etc_. Building upon the insights from model merging and mixture, Model-GLUE introduces an efficient and robust LLM scaling recipe for a diverse set of models. It starts with model clustering and progressive merging, and then the mixture of all clusters, thereby integrating similar knowledge from the model zoo while highlighting the respective strengths of each cluster. Our contributions are outlined as follows:

\(\) We conduct a comprehensive benchmarking analysis of LLM merging strategies, beginning with identifying each model's contribution and then followed by filtering out detrimental candidates. Our findings are validated on a range of LLMs, from a few to over a dozen.

\(\) We assess model mixture for four distinct variants: mixture level, router design, router input, and hybrid mixture. We have derived several principles for model mixture and discussed its utility as a solution for scaling models incompatible with merging.

\(\) We introduce a recipe for progressively combining LLM models, Model-GLUE, based on findings on merging and mixture benchmarks. It first conducts selective merging and then model mixture, outperforming the best single model on general reasoning, mathematics, and coding tasks.

\(\) Extensive experimental results on Llama-2-based models validate our proposal. For instance, Model-GLUE achieves an average increase of \(5.61\%\) across chatting, mathematics, and coding benchmarks compared to the best single LLM.

## 2 Related Works

Model Merging.Merging methods can be divided into zero-shot merging and merge-then-train approaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectivity [38; 59]. Later popular methods include Task Arithmetic  manipulating task vectors, and TIES  addressing parameter interference through trimming and conflict resolution. DARE  optimizes parameters selectively to enhance merging without extra training. Others focus on geometric properties of weights for merging [49; 24]. Recent Evolutionary Model Merge  improves weight configuration and data token pathways during inference. For the merge-then-train approach, Fisher merging  uses the Fisher information matrix to weigh model parameters to maximize their joint likelihood. RegMean  adapts the linear merging to each linear layer while averaging

Figure 1: Overview of Model-GLUE, composing of (1) Model Clustering based on architecture and weight similarity; (2) Model Filtering and Searching for merging; (3) Model Merging within each cluster; (4) Model Level Mixture of merged models.

embeddings and biases. However, both zero-shot and merge-then-train approaches are less effective for models initialized differently. [2; 23; 53; 62] exploit the permutation symmetry inherent in neural networks on small to large models. To boost merging efficiency, our focus on merging lies in the zero-shot merging of models with the same architecture and initialization.

**Model Mixture.** Mixture-of-Experts (MoE)  scales up neural networks by utilizing router networks to activate different parts of the model for different input tokens. Its integration with Large Language Models (LLMs) has gained notable recognition for its exceptional generative capabilities and unparalleled efficiency. Recently, Mixtral  demonstrates that the MoE methodology can achieve the performance of dense LLM counterparts while employing significantly fewer active parameters. Model mixture combines a collection of dense LLM models, irrespective of their sizes, into a MoE model. Some studies discover model fusion [54; 55] integrating the outputs of expert models to exploit the unique insights into the data distribution. Recent initiatives include BranchTrain-MiX , which starts with a seed-dense LLM and then branches out, facilitating the parallel training of expert models. These trained dense models are subsequently incorporated as experts within MoE layers, with other parameters being averaged. However, this approach is limited to dense models that share identical architectures and sizes. Most recently, UltraFuser  introduces a token-level soft gating mechanism that blends model outputs, with a two-stage training strategy.

**Model Stacking.** Model stacking concatenates two models along the depth dimension. In the era of LLM, Wu et al.  reuses pre-trained LLaMA layers and resets the output projection to zero in stacking. Kim et al.  shows dropping middle layers in stacking yields superior performance. Wang et al.  prove that stacking could help recover model-parameter scaling laws with insufficient data. Reddi et al.  demonstrated that gradual stacking leads to significant improvements in wall-clock time during the training of few-shot learners. Theoretically, Agarwal et al.  proved that model stacking could be interpreted as Nesterov acceleration in network optimization. However, all the aforementioned stacking methods involve no more than two kinds of models and primarily focus on the benefits of training acceleration. In this work, we explore the possibility of stacking two heterogeneous models to combine their capabilities.

**Model Scaling Tools** There have been several tools for model mixture and merging, and for scaling models using existing LLMs. For example, Mergekit is an open-source library designed to facilitate the application of model merging strategies and the construction of MoE . As a representative of unified LLM, Beyonder is a set of mixtures of merged and single LLMs for different tasks1. However, there is still a lack of a comprehensive benchmark of the various mixing and merging techniques and practical guidance on how to unify groups of LLMs at different levels of similarity.

## 3 Methodology

### Preliminaries

In this study, we consider a collection of \(n\) existing Large Language Models (LLMs), denoted as \(\{_{1},,_{n}\}\), which have been fine-tuned on diverse corpora. Our objective is to outline a systematic approach towards producing one stronger aggregated model across all knowledge domains. Specifically, the unified LLM incorporates single LLMs mainly through merging and mixture.

### Model Merging

The concept of Model MergingModel merging is integrating multiple models into one unified model in the weight space, compatible with LLMs of the same initialization . Popular merging methods can be divided into two types: _Merging entire model weights_ represented by Model Soup  (Linear), SLERP , and Model Stock ; _M Task-vector based merging_ represented by Task Arithmetic , TIES , and DARE . The former method directly interpolates model weights, while the latter subtracts the pre-trained model from the fine-tuned model to obtain task vectors and utilizes sparsity and consistency of parameters for refined

Figure 2: Pipeline for model merging, as well as an overview of merging methods and search strategies.

merging. The basic Linear interpolation merging is defined as \(w_{u}=_{i=1}^{n}s_{i} w_{i}\), where \(w_{i}\) and \(s_{i}\) are the corresponding model weights and merging coefficient of \(_{i}\{_{1},_{n}\}\).

**Selective Merging Pipeline** Merging can be easily applied to models with the same architecture, but does not guarantee better results. Therefore, before searching for the merging coefficient, we first pre-process the models by clustering all the models using cosine similarity and then searching for the optimal merging coefficient and method within each cluster. Details are explained in Appendix A.5.

**Heuristic and Evolutionary Strategies** The heuristic strategy is for searching and filtering potential harmful models for merging. It is based on greedy search, involving three variants: _Heuristic-Average_ retain the candidate if there is an improvement on the proxy dataset in each round of merging. _Heuristic-Coefficient_ builds upon _Heuristic-Average_, by combining the previously merged model with a new candidate using different coefficients in each round. _Heuristic-Similarity_ selects the candidate model with the highest or lowest similarity and conducts a coefficient search to combine it with the previously merged model. Detailed heuristic strategy algorithms can be found in Appendix A.1 Heuristic strategies perform pairwise merging of models, while many methods allow for merging multiple models at once. Therefore, we also consider jointly optimizing all model coefficients using the _Evolutionary Strategy_.

### Model Mixture

**The concept of Model Mixture.** Model mixture resembles Mixture-of-Experts(MoE). It scales a LLM with multiple pre-trained LLM experts and further extends beyond traditional token-dependent Feed-Forward-Network (FFN) MoE designs . A mixture model is composed of MoE modules and the rest shared parameters. A MoE module consists of a router \(()\) and \(n\) expert networks \(\{_{1},,_{n}\}\). \(()\) takes a router input \(x_{}\) and generate expert assignment for each token input \(x\). Then MoE outputs a weighted sum of experts' outputs as \((x,x_{})=_{i=1}^{n}(x_{})_{i }_{i}(x)\). We experiment with several variations of Model Mixture, classified as follows:

**Mixture levels.** Traditional Mixture-of-expert models replace the dense FFN layer at each Transformer block with an MoE module, which is only compatible with LLMs that share the same architecture. Besides this _FFN level mixture_, we also experiment with two coarse-grained mixtures. _Block level mixture_ create MoE module by aggregating Transformer blocks with the same index from each LLM as experts and add a block-wise router. Block level mixture is applicable to models with different architecture but the same embedding space, layer amounts, and intermediate dimension. _Model level mixture_ take each LLM as an expert and use a router at mixture model input. Model level mixture covers any LLM groups not compatible with FFN and block level mixture. In particular, the model level mixture is similar but not identical to the model ensemble, as the former can be sparse and focus more on efficiency and exploit single LLM expertise, while the latter produces general results by averaging or majority voting overall model outputs. Details can be found in Appendix A.3

**Router design.** The router network of many MoE studies adheres to a _linear router_. We experiment with another more complex _MLP router_ to examine whether this router design leads to better performance. It is implemented by two sequential FFN and a ReLU function in between, inspired by [48; 32]. For the routing method, we employ Top-K selection to all routers, which activates the K experts corresponding to the K largest softmaxed router output [47; 48].

**Router input.** We adopt two types of router input for different levels of model mixture: _Token input for FFN level mixture, where router input is the same as model input; _Sample input for block and model level mixture, where we calculate the average embedding as the sample input \(x_{}=_{i=1}^{n}x_{n}\), and route tokens of a sample to the same expert based on sample routing. The sample routing avoids inconsistency in attention operation.

**Hybrid mixture.** To explore LLM scaling in between model merging and model mixture, we propose the hybrid mixture as an intermediate solution. In a hybrid mixture, the bottom few layers of all single LLMs are merged, and then the rest layers follow any of the mixture level designs.

Figure 3: The overview and decision flow of three model mixture levels and their selection philosophy.

[MISSING_PAGE_FAIL:5]

all parameters are randomly initialized, and the fitness values are defined as the accuracy of the proxy dataset. The optimization was conducted for 200 trials in all scenarios.

### Model Merging Benchmark Results

We start our discussion by examining the effectiveness of existing approaches in depth. Despite existing merging methods focus on improving the merging techniques, their effectiveness is usually validated basedt on small-scale model zoos. For instance, Ilharco et al.  primarily focuses on the linear interpolation between two fine-tuned models, while Akiba et al.  explores merging three.

Current model practitioners typically download pre-trained models, fine-tune them on their own data or with unique techniques for specific downstream tasks, and then upload them back to the public. This practice results in a large number of open-source models being available, yet they remain underutilized by current merging methods. To this end, instead of solely discussing the merging technique, we explore an **orthogonal** question: _Can we scale up the size of model zoo to cover more models, and design an automatic merging technique to benefit from the inclusion?_

**Failure Case of Existing Approaches.** To begin with, we provide a motivating example to show the failure case of the existing approach. We consider the three models, Llama-2-Chat , Vicuna  and CodeLlama , all initialized with the same base model, Llama-2 . We merge Vicuna and CodeLlama with Llama-2-Chat, respectively, and report the evaluation results in Table 14 in Appendix B.2. We evaluate \(6\) representative merging techniques implemented in _mergekit_, including linear interpolation , SLERP , Model Stock , Task Arithmetic , DARE , and TIES . By merging Llama-2-chat and Vicuna, the merged model achieves better performance compared to any single model, while merging Llama-2-chat and CodeLlama fails to outperform all single models and may even lead to a significant drop in performance, which is also mentioned by Xu et al. . The results indicate the potential severe performance drop when including un-mergeable new model in merging (e.g. CodeLlama). Even if it is obtained from the same pre-trained checkpoint. Such failure case motivates us to design the strategy to automatically select models for merging, and exclude the models that are unable to merge.

In the following paragraphs, we explore several solutions tailored for large-scale model merging. These variations address different resource and speed requirements. The introduction of these methods is organized around answering the following key questions.

**Q1: Does handcrafted rules apply to automated model selection and which one performs best?**

**A: Yes, by a greedy search approach.** In this section, we explore three potential heuristics for model selection and report the results in Figure 4(a). We include the performance of the "best single model" (the model participant before merging that achieves the best averaged performance). We additionally validate the performance of heuristic-based merging technique, which are detailed in Section 3.2. As indicated by the results, the merging technique based on _Heuristic-Coefficient_ yields

Figure 4: (a) Comparison between different Heuristic Strategies on Which12, Which8, Which4. (b) Comparison of different model merging methods in Evolutionary Strategy.

consistently superior performance when the model zoo is large. For Which4, _Heuristic-Average_ achieved better performance, while _Heuristic-Coefficient_ performed poorly. This is primarily because the domain-specific models in Which4 exhibit similar performances and are indispensable.

**Q2: How to utilize Evolutionary Strategy for coefficient optimization in model merging?**

We divide the problem into the following sub-questions: (_i_) Which merging method is most compatible with Evolutionary Strategy? (_ii_) Can finer-grained optimization lead to a better merged model? (_iii_) How to efficiently merge in a large model zoo? For (_i_), **A: simpler methods such as Linear and Task Arithmetic are more competitive.** We compared four methods: Linear, Task Arithmetic, DARE, and TIES. As shown in Figure 4(b), Linear merging consistently achieves great results. However, when the parameters to be optimized are small, Task Arithmetic performs slightly better than Linear. Under a fixed computational budget, due to the doubling of parameters to be optimized, DARE and TIES exhibit slightly lower performance compared to other methods. For (_ii_), **A: Yes, but we need a larger computational budget.** We group adjacent \(n\) decoder layers together, where they share the same coefficients. The group size \(n\). When \(n=8\), better results were achieved compared to \(n=32\), as shown in Table 17. However, as we further decreased the group size, the performance slightly declined. This could be attributed to our relatively small budget. For (_iii_), **A: Use Heuristic Strategy to roughly search for coefficients and then fine-tune the coefficients using Evolutionary Strategy.** As shown in Table 18, the combination of the two strategies resulted in better results with fewer trials. For implementation details, please refer to Appendix A.2.

### Implementation Details for Mixture Model Zoo and Router Initialization.

In Mixture Bench, we experiment with Which2 and Which4 model settings. For router design, we mainly adopt a training-free linear layer router initialized from the prompt vector, as previous studies have demonstrated its effectiveness in the zero-shot MoE model . For specific prompt settings, we refer to the Beyonder model series 4. For the routing algorithm, we use Top-\(1\) routing for Which2 and _Block level mixture_ and _Model-level mixture_ for Which4, and Top-\(2\) for Which4 _FFN level mixture._

Post-mixture training.For _MLP router_ that are randomly initialized, we fine-tune the model by language modeling on the GPT4All dataset , only updating the router. We use the GPT4All  dataset for post-mixture router training, which is under Apache 2.0 License. For all the router training experiments, we apply the batch size of \(128\), a cosine learning rate scheduler, the learning rate of \(5e-5\), and the epochs of \(1\).

Mixture Method Abbreviations.To simplify the description, we use abbreviations to denote different mixture methods, as in Table 2.

### Model Mixture Benchmark Results

In this section, we attempt to answer five main research questions about mixture variants: mixture level, router design, router input, and hybrid merging. We also explore the mixing of very different models that cannot be merged as the previous probe in our next Model-GLUE recipe that combines merging and blending for LLM scaling.

Q1: At which level does the model mixture manifest its utmost effectiveness?

A: Model level mixture is consistently better.Our comparative analysis of the {FFN, block, model} level mixture, all employing the linear router and the sample routing strategy as presented in Table 3, consistently demonstrates the superiority of the _Model level mixture_ under Which2 and Which4 setting. This could be attributed to the design

    &  &  &  &  &  &  &  \\    & & & & Which2 & & \\  Best Single Model & \(54.27\%\) & \(71.51\%\) & \(47.24\%\) & \(21.30\%\) & \(18.00\%\) & \(13.06\%\) & \(37.68\%\) \\  F-L-S & \(52.82\%\) & \(70.80\%\) & \(50.04\%\) & \(\) & \(19.00\%\) & \(17.68\%\) & \(38.91\%\) \\ B-L-S & \(52.73\%\) & \(70.01\%\) & \(49.90\%\) & \(19.94\%\) & \(18.84\%\) & \(15.85\%\) & \(37.88\%\) \\ M-L-S & \(\) & \(\) & \(\) & \(22.21\%\) & \(\) & \(\) & \(\) \\   &  &  &  &  \\    &  &  &  &  &  &  \\    &  &  &  &  &  &  \\   

Table 3: Comparison of different mixture levels. For each task in each model zoo, we highlight the performance best in each model zoo in **bold**.

    &  &  &  &  &  &  &  \\    & & & Which2 & & & \\  Best Single Model & \(54.27\%\) & \(71.51\%\) & \(47.24\%\) & \(21.30\%\) & \(18.00\%\) & \(13.06\%\) & \(37.68\%\) \\  F-L-S & \(52.82\%\) & \(70.80\%\) & \(50.04\%\) & \(\) & \(19.00\%\) & \(17.68\%\) & \(38.91\%\) \\ B-L-S & \(52.73\%\) & \(70.01\%\) & \(49.90\%\) & \(19.94\%\) & \(18.84\%\) & \(15.85\%\) & \(37.88\%\) \\ M-L-S & \(\) & \(\) & \(\) & \(22.21\%\) & \(\) & \(\) & \(\) \\   &  &  &  &  &  \\    &  &  &  &  &  &  \\  Best Single Model & \(\) & \(73.72\%\) & \(\) & \(\) & \(17.80\%\) & \(13.41\%\) & \(38.70\%\) \\  F-L-S & \(53.75\%\) & \(73.88\%\) & \(47.97\%\) & \(34.87\%\) & \(\) & \(\) & \(42.57\%\) \\ B-L-S & \(52.65\%\) & \(\) & \(47.05\%\) & \(21.15\%\) & \(20.40\%\) & \(14.63\%\) & \(38.42\%\) \\ M-L-S & \(49.06\%\) & \(72.14\%\) & \(41.81\%\) & \(\) & \(17.60\%\) & \(15.24\%\) & \(\) \\   

Table 2: Model mixture methods and their abbreviations used in our study. Methods applicable for models with distinct architectures are highlighted in gray.

that _Model Level Mixture_ route each sample to one expert model, thereby avoiding the conflicts between different expert models and maximizing the expertise of the most appropriate experts. Since the experts are not derived from the same pre-training process, directly merging their inconsistent representation spaces will affect the performance of the mixture model, with more expert parameters leading to worse results. This is especially evident for _Block-level Mixture_, as the routing is performed at each transformer layer and the representation is fed into different expert blocks in series, causing confusion when switching between different expert knowledge.

**Q2: Does more complex router design brings better results?**

**A: Not necessary, as the linear router outperforms the MLP router.** From Table 4, the performances of the _linear router_ without additional training slightly surpass _MLP router_ models, _i.e._, F-L-T over F-M-T, B-L-S over B-M-S. Specifically, _linear router_ models are better at math and coding datasets, validating prompt vector is effective in assorting samples from different domains, which is otherwise too implicit to learn via direct language modeling.

**Q3: Does model mixture directly works on unmergeable models?**

**A: No.** We directly apply the setting of Which2 _Model level mixture_ to Llama-2-7b-chat and CrystalChat, an unmergeable model pair with different architectures and initialization. As shown in Table 5, the performance is slightly behind the best single model. This may be due to simple prompts and direct mixture, as it fails to coordinate the divergence between drastically different models. We evaluate more complex prompts for the same model pair and the mixture model outperforms, see Table 19 for more information.

**Q4: Which router input is better, token-level or sample-level?**

**A: Not quite different. Token input suits a mixture of the same domain models.** Table 6 shows the performance token-based and sample-based routing are pretty close. In particular, for Which2 and Which4 (Chat) where models are all trained for general chatting purposes, token routing outperforms, whereas sample routing is better for default Which4 (Domain) with differently specialized models. This may result from divergence of model knowledge and representation spaces will cause conflicts in fine-grained token routing.

**Q5: Is it feasible for hybrid mixtures to provide enhancements?**

**A: Yes.** Our experiments on F-L-T with _v.s._ without the hybrid mixture, as detailed in Table 7, demonstrate that the hybrid mixture significantly improves performance on average and simultaneously reduces the memory overhead during inference. This improvement may be attributed to the

   Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\   \\  Best Single Model & \(\) & \(\) & \(\) & \(21.30\%\) & \(18.00\%\) & \(13.06\%\) & \(37.68\%\) \\  F-L-T & \(53.41\%\) & \(70.48\%\) & \(\) & \(\) & \(\) & \(16.46\%\) & \(\) \\ F-L-S & \(52.82\%\) & \(70.80\%\) & \(50.04\%\) & \(\) & \(19.00\%\) & \(\) & \(38.91\%\) \\   \\  Best Single Model & \(55.03\%\) & \(73.72\%\) & \(\) & \(\) & \(17.80\%\) & \(13.41\%\) & \(38.70\%\) \\  Chat F-L-T & \(\) & \(\) & \(\) & \(\) & \(20.00\%\) & \(\) & \(\) \\ Chat F-L-S & \(53.75\%\) & \(70.96\%\) & \(49.78\%\) & \(20.32\%\) & \(\) & \(20.12\%\) & \(39.22\%\) \\  Domain F-L-T & \(\) & \(\) & \(48.32\%\) & \(30.17\%\) & \(\) & \(20.12\%\) & \(41.74\%\) \\ Domain F-L-S & \(53.75\%\) & \(73.88\%\) & \(47.97\%\) & \(\) & \(21.80\%\) & \(\) & \(\) \\   

Table 6: Comparison of different router input designs. Which4 includes one group with chatting models (Chat) and another with different domain models (Domain). We highlight the best performing mixture methods in **bold**.

   Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\  F-L-T & \(53.41\%\) & \(70.48\%\) & \(\) & \(\) & \(\) & \(16.46\%\) & \(\) \\ F-M-T & \(\) & \(\) & \(50.01\%\) & \(21.92\%\) & \(17.40\%\) & \(\) & \(38.78\%\) \\  B-L-S & \(\) & \(70.01\%\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ D-S & \(\) & \(\) & \(49.41\%\) & \(\) & \(\) & \(14.02\%\) & \(37.01\%\) \\   

Table 4: Comparison between linear and MLP routers on Which2 setting. We highlight better performance within each pair in **bold**.

   Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\  F-L-T & \(53.41\%\) & \(70.48\%\) & \(\) & \(\) & \(\) & \(16.46\%\) & \(\) \\ F-M-T & \(\) & \(\) & \(50.01\%\) & \(21.92\%\) & \(17.40\%\) & \(\) & \(38.78\%\) \\  B-L-S & \(\) & \(70.01\%\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ D-S & \(\) & \(\) & \(49.41\%\) & \(\) & \(\) & \(14.02\%\) & \(37.01\%\) \\   

Table 7: Comparison between F-L-T methods with and without hybrid mixture technique. We highlight the best performing mixture methods in **bold**.

   Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\  Best Single Model & \(\) & \(69.46\%\) & \(\) & \(27.22\%\) & \(\) & \(\) & \(\) \\  M-L-S & \(50.68\%\) & \(\) & \(50.08\%\) & \(\) & \(33.80\%\) & \(30.48\%\) & \(43.77\%\) \\   

Table 5: Comparison of the mixture of a unmergeable model pair (Llama-2-7b-chat and CrystalChat). We highlight the better performance in **bold**.

higher sensitivity of the initial transformer blocks. Avoiding using MoE for these blocks can yield performance gains, as suggested by a few previous works as well [12; 41]. Surprisingly, our results show that the hybrid F-L-T model consistently outperforms the standard F-L-T on _math_ and _code_ tasks. Our further analysis indicates that this improvement might be because of the conversational nature of the content in GSM8K, MBPP, and HumanEval datasets, which appears to challenge the routing mechanisms within the initial transformer blocks, leading to ineffective expert specialization.

## 5 Superior Recipes to Aggregate LLM Knowledge

### Model Merging _v.s._ Mixture

Q1: For a mergeable model zoo, how should we choose between merging and mixture?For limited computational resources and similar models, merging is always a simple and effective method. For the domain-specific models, mixture can bring greater improvements.

Detailed results are presented in Table 8. For Which4 (Domain), due to the appropriately designed linear routers, model mixture can fully leverage various domain-specific models, thus slightly outperforming merging. For Which4 (Chat), we adopt the optimal settings from Which4 (Domain) and only change the model zoo. Since individual models do not exhibit superior capabilities in a single domain, it is challenging to design suitable routers at a low cost. Therefore, mixture performed significantly worse compared to merging. Furthermore, although combining the homogeneous models in Which4 (Chat) brings some improvement, we can see that Which4 (Domain) overall outperforms Which4 (Chat). Therefore, increasing the diversity among the models will make a greater contribution to the combined model.

### Model-GLUE: selective merging then model mixture for better LLM scaling

Q2: How to combine models with greater differences in an extensive and varied model zoo?

In Which16, a larger and more diverse model zoo, some models cannot be merged due to structural differences and models that would degrade in performance when merged with other models. Therefore, we first cluster the models based on cosine similarity. Within each mergeable family, we perform either merging or mixture. We initially employ heuristic strategies of merging and report the best results (_i.e._, Full Merging) in Table 9. The Llama-2 family (_i.e._, Which12) consists of up to \(12\) models, so directly combining them through the mixture is inefficient. Thus, we only consider models selected by merging and report the results of F-L-T Mixture. From Table 9, we can observe that Full Merging outperforms F-L-T Mixture.

Therefore, we selected Full Merging as the representative model for the Llama-2 family and combined it with other models that could not be merged by model mixture. On average, the Model-GLUE demonstrates a \(5.61\%\) improvement over the Best Single Model. More details are presented in Appendix A.4.

## 6 Discussion with Other LLM Aggregation Techniques

Thus far, we mainly focus on two LLM aggregation techniques: model merging and mixture. In this section, we discuss other potential techniques that could help scaling existing LLMs.

Model Stacking.Research has demonstrated that stacking a model itself can accelerate training convergence as opposed to training a model of double the size from scratch [17; 18; 56; 60; 28]. This concept can be extended naturally to stack multiple models as one larger model. Our experimental results indicate that model stacking with lightweight fine-tuning can yield superior performance compared to various merging and mixture models. For instance, stacking 7B Llama-2-chat and Vicuna can achieve \( 55\%\) on the MMLU benchmark. When compared to model mixture, model

   Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\  Best Single Model & \(46.7\%\) & \(64.33\%\) & \(46.33\%\) & \(\) & \(42.00\%\) & \(31.10\%\) & \(48.82\%\) \\  Full Merging & \(\) & \(\) & \(50.13\%\) & \(39.35\%\) & \(21.80\%\) & \(21.34\%\) & \(43.56\%\) \\ F-L-T Mixture & \(54.69\%\) & \(73.32\%\) & \(48.74\%\) & \(35.18\%\) & \(22.60\%\) & \(21.34\%\) & \(42.65\%\) \\  Model-GLUE & \(51.62\%\) & \(70.56\%\) & \(\) & \(53.53\%\) & \(\) & \(\) & \(\) \\   

Table 9: Comparison between the best single model, Full Merging, Full Mixture and our Model-GLUE.

   Model & ARC & WinoGrande & MMLU & GSM8K & MBPP & HumanEval & Average \\  Best Single Model & \(55.03\%\) & \(73.72\%\) & \(48.18\%\) & \(24.03\%\) & \(17.80\%\) & \(13.41\%\) & \(38.70\%\) \\   \\  Merging & \(\) & \(73.64\%\) & \(\) & \(43.75\%\) & \(\) & \(\) & \(43.86\%\) \\ Mixture & \(\) & \(\) & \(\) & \(\) & \(18.40\%\) & \(18.29\%\) & \(\) \\   \\  Merging & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Mixture & \(\) & \(70.98\%\) & \(49.80\%\) & \(19.94\%\) & \(19.80\%\) & \(20.73\%\) & \(39.16\%\) \\   

Table 8: Comparison between the best merging approach _v.s._ the best mixture approach on Which4 (Domain) and Which4 (Chat).

stacking offers less flexibility in terms of model choices. Although the resulting architecture is more standardized than MoE, increasing the model depth through stacking also results in higher latency than mixture models where subnetworks infer in parallel. Additionally, model stacking does not simplify the design space, such as determining whether, which, and how many layers should be dropped when stacking two heterogeneous models. We conducted a preliminary investigation employing model stacking techniques to address two primary research questions: (1) Can model stacking effectively combine the capabilities of two distinct models and surpass the performance of self-stacking a single model? (2) What is the impact of layer dropping on stacking performance?

Specifically, we examine the relationship between the number of dropped layers (\(K\)) and the resulting downstream task accuracy. To this end, we selected 7B Llama-2-Chat and Vicuna as the base models and fine-tuned the stacked models for 10 billion tokens. The obtained results are presented in Table 10. In the initial two rows, we report the performance of the two base models, revealing that Llama and Vicuna exhibit advantages on different datasets. In the subsequent two rows, we observe that stacking dissimilar models generally outperforms self-stacked models, and the weaknesses of one model can be compensated for by another stronger one. Moving forward, we explored the effects of varying the number of dropped layers. Our findings indicate that even when dumping half of each model (\(K=16\)), the stacked 7B models can still significantly enhance performance across tasks.

Model Communication.Model communication [61; 31; 33] is a framework that enables the development of LLM applications through the use of multiple conversable agents that collaborate to complete tasks. This approach allows developers to design complex LLM application workflows as multi-agent conversations, where agents with various roles and capabilities, driven by LLMs, tools, or human inputs, interact with each other. Unlike model merging, mixture, and stacking techniques, LLM communication is orthogonal to the primary focus of this paper because it does not modify the model weights; instead, it leverages the in-context learning and conversational capabilities of LLMs to coordinate agents. An empirical comparison with this class of methods is beyond the scope of this study and will be explored in future research.

## 7 Limitations

For LLM scaling studies, while empirical evidence suggests that increasing model size, data volume, and computational complexity leads to better performance, there is little theoretical clarity on the exact mechanisms behind these improvements. Second, although scaling laws suggest that performance continues to improve as models get larger, recent evidence indicates that scaling may lead to diminishing returns beyond a certain point. In addition, our work focuses on benchmarking results, while the reasons why model merging improves performance could be further enhanced by post hoc analysis, such as examining parameter distribution and similarity during model operations.

## 8 Conclusion

In this paper, we explore the scaling LLM based on a model zoo of pre-trained LLMs within the real world. We first benchmark state-of-the-art LLM merging, mixture, and model stacking. Based on previous findings, we then propose a novel LLM scaling framework, Model-GLUE. Specifically, we scale up the model zoo closely examine the existing model merging techniques, and conclude the selective merging techniques based on heuristics and learnable algorithms. Further, we investigate variants of Mixture-of-Experts for combining LLMs and suggest it can serve as an alternative to merging failure cases. Finally, we integrate selective merging strategies with model mixture techniques, presenting this as a comprehensive solution for scaling a diverse array of LLM collections. Future works will include model stacking and communication to our Model-GLUE framework.

   Model & ARC & WinoSGrande & MMLU & Hellaswag & TruthfulQA \\  Llama-2-chat & \(54.10\%\) & \(71.27\%\) & \(47.28\%\) & \(78.71\%\) & \(45.32\%\) \\ Vicuna & \(53.75\%\) & \(70.56\%\) & \(49.78\%\) & \(77.19\%\) & \(50.36\%\) \\  Llama / Llama (\(K=8\)) & \(53.92\%\) & \(69.14\%\) & \(52.76\%\) & \(73.74\%\) & \(46.36\%\) \\ Llama / Vicuna (\(K=8\)) & \(56.14\%\) & \(70.80\%\) & \(55.20\%\) & \(73.67\%\) & \(46.84\%\) \\  Llama / Vicuna (\(K=12\)) & \(55.42\%\) & \(69.45\%\) & \(53.55\%\) & \(73.62\%\) & \(45.59\%\) \\ Llama / Vicuna (\(K=16\)) & \(54.35\%\) & \(69.69\%\) & \(52.52\%\) & \(73.75\%\) & \(45.92\%\) \\ Llama / Vicuna (\(K=20\)) & \(39.59\%\) & \(61.33\%\) & \(44.93\%\) & \(62.10\%\) & \(42.90\%\) \\ Llama / Vicuna (\(K=24\)) & \(28.15\%\) & \(52.88\%\) & \(25.51\%\) & \(43.07\%\) & \(39.10\%\) \\   

Table 10: Comparison of different model stacking configurations.