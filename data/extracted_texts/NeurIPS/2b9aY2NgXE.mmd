# Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning

Cristina Menghini

Brown University

cristina_menghini@brown.edu

&Andrew Delworth

Brown University

adelwort@cs.brown.edu

&Stephen H. Bach

Brown University

sbach@cs.brown.edu

###### Abstract

Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is often necessary to optimize their performance. However, a major obstacle is the limited availability of labeled data. We study the use of pseudolabels, i.e., heuristic labels for unlabeled data, to enhance CLIP via prompt tuning. Conventional pseudolabeling trains a model on labeled data and then generates labels for unlabeled data. VLMs' zero-shot capabilities enable a "second generation" of pseudolabeling approaches that do not require task-specific training on labeled data. By using zero-shot pseudolabels as a source of supervision, we observe that learning paradigms such as semi-supervised, transductive zero-shot, and unsupervised learning can all be seen as optimizing the same loss function. This unified view enables the development of versatile training strategies that are applicable across learning paradigms. We investigate them on image classification tasks where CLIP exhibits limitations, by varying prompt modalities, e.g., textual or visual prompts, and learning paradigms. We find that (1) unexplored prompt tuning strategies that iteratively refine pseudolabels consistently improve CLIP accuracy, by 19.5 points in semi-supervised learning, by 28.4 points in transductive zero-shot learning, and by 15.2 points in unsupervised learning, and (2) unlike conventional semi-supervised pseudolabeling, which exacerbates model biases toward classes with higher-quality pseudolabels, prompt tuning leads to a more equitable distribution of per-class accuracy. The code to reproduce the experiments is at BatsResearch/menghini-neurips23-code.

## 1 Introduction

Large pre-trained vision-language models (VLMs)  achieve remarkable accuracy without task-specific training but still require adaptation for optimal performance. Prompt-tuning  is an approach to efficiently enhance VLMs performance on downstream tasks by learning inputs to the model. While learning prompts with a few labeled data can yield significant improvements , a broader range of learning settings such as semi-supervised, transductive zero-shot, and unsupervised learning are still underexplored. All of these settings share access to unlabeled data, and the versatile zero-shot classification abilities of VLMs make pseudolabeling a natural approach to leveraging it. This paper investigates how the use of out-of-the-box pseudolabels assigned by CLIP can contribute to improving CLIP's own performance. To this end, we conduct an extensive exploration of learning scenarios by varying prompt modalities, learning paradigms, and training strategies. We present empirical evidence showcasing the effectiveness of iterative prompt-training strategies that leverage CLIP-based pseudolabels, regardless of learning paradigms and prompt modalities, resulting in significant improvements in CLIP's image classification performance across different settings.

Pseudolabels are heuristic labels assigned by a model to unlabeled data, which are leveraged to further train the model . Successful training with pseudolabels relies on two factors: the qualityof the labels and how they are used during training. To address the first, conventional methods assign labels to instances with high-confidence predictions . For pseudolabeling using CLIP, Huang et al. propose to select the most confident samples for each class , mitigating CLIP's bias  and miscalibration  (see Section 3). To assign pseudolabels, we rely on this approach and address the second point by exploring how to make the best use of them. We design a broad space of analysis considering three dimensions: prompt modalities, which are the model inputs we learn; learning paradigms, which define the data we have available; and training strategies, which describe the process used to optimize performance (Figure 1).

Research on prompt tuning has demonstrated that training strategies used for learning prompts in one modality can be transferred to learning prompts in a different modality. For instance, Visual Prompt Tuning  was originally designed to effectively fine-tune large vision models but can be adapted to efficiently fine-tune CLIP using the same training strategy as standard textual prompt tuning . On the contrary, different learning paradigms with limited labeled data typically require distinct approaches specifically tailored to extract information from the available data . However, we observe that this changes by using VLM's generated pseudolabels. Unlike conventional pseudolabeling approaches that bootstrap off labeled data and are used as semi-supervised learning techniques , VLMs can generate pseudolabels in any learning setting. This offers a significant advantage, expanding the scope of pseudolabeling beyond semi-supervised learning, and making it a promising approach for other settings, such as transductive zero-shot and unsupervised learning. By using CLIP-based pseudolabels as a source of supervision, we can view these settings as optimizing the same loss function, which is simply a weighted sum of the errors on labeled data, if available, and pseudolabeled data. Given that we can express different settings as the same problem, we can propose training strategies, i.e., the way of using pseudolabels, that suit them all.

By standardizing the training strategies across various prompt modalities and learning settings, we can conduct experiments on different applications of pseudolabels for various combinations of prompt modalities, learning paradigms, and training strategies, as illustrated in Figure 1. To the best of our knowledge, only one potential path has been explored thus far; specifically, fine-tuning textual prompts in an unsupervised learning context using a few pseudolabels . Rather than relying on a fixed set of pseudolabels, we propose iterative training techniques that allow for the ongoing refinement and expansion of the pool of pseudolabeled data used during training. With each iteration, we progressively enhance CLIP's pseudolabeling ability, allowing us to extend the set of pseudolabeled data while maintaining the high quality of the initial pseudolabels.

We conduct experiments on six tasks where CLIP has been observed to underperform , such as satellite-image classification, flower-species identification, and texture-image recognition, among others. Our findings reveal that iterative approaches effectively fine-tune prompts irrespective of their modality and learning paradigms. Recent studies have identified the "Matthew effect" as a potential issue for semi-supervised models that use pseudolabels . This phenomenon causes models to perform well on classes with accurate pseudolabels but poorly on those with inaccurate ones, thereby reinforcing the model's original bias towards certain classes. Our analysis reveals that using pseudolabels generated by CLIP for prompt-tuning with iterative strategies not only improves CLIP's overall performance but also corrects its natural bias towards certain classes.

We summarize the main takeaways of our work:

Figure 1: Our design space to explore the effect of leveraging pseudolabels in a unified way across prompt modalities, learning paradigms, and training strategies. The green (dashed) path has already been explored , while the red (solid) lines are the unexplored combinations for prompt tuning.

* General purpose zero-shot learners used as general purpose pseudolabeleers open the opportunity to develop training strategies that leverage pseudolabeled data beyond semi-supervised learning. We point out that different learning paradigms, such as semi-supervised, transductive zero-shot, and unsupervised learning, can be all considered as special cases of a single objective function, by using pseudolabels as a source of supervision.
* We demonstrate that simple iterative training strategies for refining pseudolabels are highly effective approaches for limited-label prompt tuning. In fact, regardless of the prompt modality and learning setting, these strategies improve CLIP, by on average 19.5 points in semi-supervised learning, 28.4 in transductive zero-shot learning, and 15.2 in unsupervised learning.
* We show that prompts learned with iterative strategies help mitigate the "rich get richer, poor get poorer" effect observed in semi-supervised approaches leveraging pseudolabels. By redistributing the quality of pseudolabels across different classes, we observe a "Robin Hood effect" where the extremely rich classes' accuracy stays the same or decreases, while poorer classes get richer, leading to a more equitable distribution of per-class accuracy.

## 2 Background and related work

Vision-language modelsVision-language models such as CLIP , ALIGN , and Florence  are models that align images and text. We focus on CLIP, which is composed of two components: a text encoder, \(\), and an image encoder, \(\), which are jointly trained using a contrastive loss to learn a multi-modal embedding space which aligns the representations of similar text and image inputs. This pre-training enables CLIP to perform zero-shot image classification. Given an image \(x\) and a set of classes \(=\{y_{1},...,y_{C}\}\), CLIP classifies \(x\) by measuring the similarity between the image representation \(z=(x)\) and each class representation \(w_{i}=(_{i})\), based on their cosine distance in the shared embedding space. Here, \(_{i}\) is a natural language prompt such as "a photo of a [CLASS,]", where \(_{i}\) is the specific class name, such as "orange dahlia," "forest" or "Boeing 737". The image \(x\) gets assigned to the class with the highest similarity score. In this work, we study how to learn better prompts that enhance CLIP by leveraging pseudolabels.

Prompt tuningPrompt tuning is a technique that enhances the practical application of large pre-trained models like CLIP  and GPT [32; 7]. It involves providing task-specific information to the model during inference through textual or visual inputs, leading to improved performance on downstream tasks [1; 33; 6; 7]. While discrete prompts are manually crafted natural language descriptions of classes that guide the model, they may not yield optimal results . Soft prompting [21; 24], on the other hand, optimizes prompts as continuous vectors. These can be optimized by backpropagating through the frozen pre-trained model, resulting in better performance. Soft prompts can be learned for various modalities, e.g., text or image, [48; 13; 17; 2; 44; 19] and applications [34; 27; 12; 28] by training on a small number of labeled examples per class. If only unlabeled data is accessible, it is possible to learn textual soft prompts by leveraging CLIP-based pseudolabels . Expanding on this concept, we further investigate the use of pseudolabels across a broader range of prompt modalities and learning approaches, and we introduce unexplored training strategies to leverage pseudolabels more effectively.

Learning from pseudolabelsPseudolabeling is the practice of assigning labels to unlabeled data based on the prediction of a model . Then, pseudolabels are used to improve the performance of the model itself. There are different ways to obtain and use pseudolabels and each impacts the final predictions of the model [41; 45; 16; 35]. Some approaches use confidence thresholds [36; 3; 40] and others average predictions from multiple augmentations . Pseudolabeling is a semi-supervised learning technique, and it is rarely used in transductive zero-shot learning [42; 5; 25]. Applying such techniques requires a few labeled examples related to the target task to learn a baseline model capable of pseudolabeling. However, this limitation has been overcome by VLMs, which are capable of pseudolabeling examples without task-specific training. The conventional pseudolabeling scheme based on confidence threshold is not effective if we assign pseudolabels based on CLIP. In fact CLIP is miscalibrated  and has imbalanced predictions  which may induce noise in the pseudolabels. An alternative approach selects the top-K most confident examples per class to improve performance . In our analysis, we rely on this scheme (Section 3).

Design space

Our analysis encompasses the design space consisting of various combinations of prompt modalities, learning paradigms, and training strategies (Figure 1). Within this space, two key components remain constant: the pseudolabeling scheme and a unified loss function. This section begins by introducing these components and subsequently delves into a comprehensive discussion of each dimension within the design space to be explored.

Pseudolabeling schemeThe use of CLIP to generate pseudolabels has been investigated in . Given unlabeled data \(X_{u}\) with target classes \(\{y_{1},...,y_{C}\}\), the goal is to assign labels to data points in which the model is most confident. Typically, pseudo labeling schemes use a confidence threshold (\(P(y|x)>\)) to select instances to pseudolabel. However, this approach does not work well for CLIP due to its miscalibration  and imbalanced predictions . Instead, one can use a top-K pseudo labeling approach, where the top-K most confident examples per class are used as pseudolabeled data . The pseudolabel assignment consists of (1) computing the similarity scores of each datapoint with classes' textual prompts, and (2) select for each class the \(K\) datapoints with the highest similarity score to the class. In this way, we always get \(K\) pseudolabels per class, effectively addressesing the natural bias in CLIP's pseudolabels . In Appendix A.1, we provide more details about pseudolabel assignment corner cases.

This top-K pseudolabeling scheme is applicable to unlabeled data, regardless of the availability of labeled data. As a result, we can extend the use of pseudolabels to any learning setting that involves unlabeled data. We observe that by treating pseudolabeled examples as true labeled data, we can view all learning settings as optimizing the same objective function.

Unified objective functionConsider a \(C\)-class image classification task, where \(X_{L}\) and \(Y_{L}\) represent the image representations and labels of the labeled data, and \(X_{U}\) and \(_{U}\) denote the image representations and pseudolabels for the unlabeled data. We define a loss function that combines two cross-entropy losses, one accounting for the error on the labeled data points and the other accounting for the error on pseudolabeled data:

\[=_{CE}(X_{L},Y_{L})+\,_{CE}(X_{U}, _{U})\] (1)

where \(\) and \(\) define the training balance between the errors on labeled and pseudolabeled data.

### Prompt modalities

Learning prompts is the process of training a set of vectors \(=[]_{1}[]_{K}\) that are prepended to the textual or visual inputs of the encoders within the CLIP architecture. By prepending these vectors to specific inputs, we can learn _textual_ prompts, _visual_ prompts, or _multimodal_ prompts when applying a set of vectors to both inputs simultaneously. We provide a technical and detailed explaination in Appendix A.1.

In our exploration, we consider all three types of prompts. The efficacy of prompts can vary depending on the task. Text prompt tuning may be most beneficial when image features are well-separated by class but may not be aligned with the corresponding textual prompt. Visual prompts rearrange the image features within the projection space, and it has the potential to improve CLIP when the pre-trained image features are not well separated by class. Finally, multimodal prompts allows for beneficial interaction between the two separate modalities, which might lead to both separable visual features, and text classifiers that are well-aligned with the corresponding visual features.

### Learning paradigms

By adjusting the values of parameters \(\) and \(\) and using the appropriate sets of labeled and pseudolabeled data, the unified objective loss can be customized for each learning paradigm. We note that the redundancy in the use of parameters is for notation clarity in descriptions of the different strategies below. One can simply use \(\) as balancing factor between labeled and pseudolabeled data.

Semi-supervised learningIn the semi-supervised learning (SSL) scenario we have access to a limited number of labeled data for all the target classes \(D_{L}=\{(x,y)\}\) where \(x\) is an input feature and \(y=[C]\) is the corresponding label. In addition, we have access to unlabeled data \(X_{U}=\{x\}\), where \(x\) is an image in the target domain \(\). From \(X_{U}\), we get \(_{PL}=\{(x,)\}\), where \([C]\) is \(x\)'s pseudolabel. When using the unified loss in this setting, we set \(\) to \(|_{PL}|/|_{L}|\). As \(|_{L}|\) is much smaller than \(|_{PL}|\), \(\) acts as an upweighting factor for the few-labeled instances, thus counterbalancing the learning effect of pseudolabels (\(\)=1).

Transductive zero-shot learningIn transductive zero-shot learning (TRZSL), we are provided with labeled data \(D_{L}=\{(x,y)\}\) for some target classes \(S\) (referred to as _seen_ classes), where \(x\) represents input features, and \(y[S]\) is the corresponding label. Additionally, we have access to unlabeled data \(X_{U}=\{x\}\) for a disjoint set of classes \(U\) (referred to as _unseen_ classes). Using \(X_{U}\), we obtain \(_{PL}=(x,)\), where \([U]\) denotes the pseudolabels for \(x\). The value of \(\) in the unified loss is set to \(|_{L}|/|_{PL}|\), which makes the weight of the pseudolabel loss equivalent to that of the labeled data (\(=1\)). This is necessary because an imbalance in the number of labeled and pseudolabeled samples can result in a skewed training distribution, leading to better performance on seen classes while the performance on unseen classes may either remain stagnant or degrade. Studying this setting is interesting beyond transductive zero-shot learning. In fact, it has the potential to generalize to scenarios where the target task involves unseen classes, while the seen classes consist of auxiliary labeled data from the same domain but different task .

Unsupervised learningIn the unsupervised learning (UL) setting, we have access only to unlabeled data \(X_{U}=\{x\}\), from which we obtain \(_{PL}=(x,)\), where \([C]\) denotes the pseudolabel for \(x\). In this case, \(\) is set to 0, as there is no labeled data, and \(=1\). The use of this setting was initially explored in , who leveraged a few pseudolabels per class to learn textual prompts. In this paper, we build on their work by investigating a variety of training strategies and prompt modalities.

Supervised learningIn supervised learning (SL), we are only provided with labeled data \(D_{L}=(x,y)\), where \(x\) represents an input feature, and \(y[C]\) is the corresponding label. If we set \(\) to 0, the unified loss function is equivalent to the objective functions of default prompt-tuning approaches that optimize the prompts using a few labeled instances per target class. This setting is not strictly part of our design space. However, we will refer to it to define baselines in Section 4.

### Training strategies

The unified objective function enables the development of training strategies broadly applicable across various learning paradigms. We explore three distinct learning strategies to effectively use pseudolabels in this context. The first strategy uses pseudolabels in a static manner. The other two strategies, which are unexplored for prompt tuning, involve the dynamic use of pseudolabeled data.

Few-pseudolabels (FPL)We select \(K\) pseudolabels per target class, resulting in a pseudolabeled dataset of size \(K C\). We learn the prompts by minimizing the objective function via backpropagation through CLIP's encoders. This strategy aligns with Unsupervised Prompt Learning (UPL) in . We refer to it as few-pseudolabels (FPL) to encompass its applicability for learning prompts of diverse modalities across learning paradigms.

Iterative Refinement of FPL (IFPL)Similar to FPL, we obtain the top-K pseudolabels for each target class. These pseudolabels are then used to train a new task-specific prompt. After completing the training, we use the learned prompt to compute the top-K pseudolabels per class again. Subsequently, we reinitialize the prompt and repeat this entire process for a total of \(I\) iterations. With this iterative approach, if training with the initial pseudolabel set leads to an improvement in the model's performance, the model itself can become a more effective pseudolabeler, refining the pseudolabels in each subsequent iteration.

Grow and Refine Iteratively Pseudolabels (GRIP)Although IFPL can improve the quality of the \(K C\) pseudolabels used for training, it still limits learning to a few examples per target class. To overcome this constraint, we explore a method similar to IFPL, but with a key difference. In each iteration, we progressively increase the value of \(K\). Specifically, during the \(i\)-th iteration, we use \(K=(i|}{I})/C\) of the unlabeled data to perform the steps in the iterative process. GRIP maintains class balance by selecting the top-K samples at each iteration, with \(K\) increasing progressively. Similar to IFPL, both prompts and pseudolabels are reinitialized with every iteration, in order to avoid accumulating errors from earlier iterations. In other words, learning progresses from pseudolabels to new prompts to new pseudolabels, and so on. The rationale behind this strategy is that as the model's accuracy in generating pseudolabels improves, we can increase the total number of pseudolabels without introducing excessive noise.

## 4 Experiments

We explore the design space outlined in Section 3 to understand the effectiveness of leveraging pseudolabels for limited-label prompt tuning. We show that (1) iterative strategies significantly improve CLIP's performance across prompt modalities and learning settings, (2) using CLIP-based pseudolabels with iterative strategies induces a more equitable distribution of per-class accuracy.

DatasetsWe conduct the analysis on six tasks, covering specialized and fine-grained domains, where CLIP shows deficiencies . We call this set of tasks FRAMED, and it includes Flowers102 , RESICS45 , FGVC-Aircraft , MNIST , EuroSAT , and DTD . For each dataset we use the training and test splits provided in . For the transductive zero-shot learning setting we randomly generate three splits of seen and unseen classes with a 62-38 ratio. Further details are in Appendix A.2.

BaselinesTo evaluate the effectiveness of the training strategies described in Section 3.3, we compare the performance of CLIP when queried with the learned soft prompts to CLIP zero-shot with default prompts such as "a photo of a [CLASS]." In addition, we compare with default supervised prompt-tuning baselines, for which we only use the available labeled data: CoOp  for textual prompts, VPT  for visual prompts, and UPT  for multimodal prompts. We defer to Appendix A.1 the technical details of these methods.

Evaluation metricsWe assess the performance of each method by measuring the accuracy of the test set, averaging the results over five runs. In the case of TRZSL, we report the harmonic of the accuracies of seen and unseen classes to account for their potentially imbalanced performance .

Training settingsFor all experiments, datasets, and learning strategies, we use ViT-B/32 as the vision backbone. For both visual and textual prompt learning, we set the prefix size to 16 [48; 18]. Multimodal prompts have length 8 . We use SGD as the optimizer and train for 150 epochs. We use 5 warmup epochs at a learning rate of 0.0001, and then set the learning rate to \(l\), which is decayed by the cosine annealing rule. For textual and visual prompt learning, \(l=0.1\), while for multimodal prompt learning, \(l=0.01\). In SSL, we use 2 labeled samples per class to assess the impact of pseudolabels in the scenario of very few labeled data and abundant unlabeled data. The number of iterations \(I\) is \(10\). FPL and IFPL have the number of pseudolabels per class fixed to 16 since it is indicated as the optimal \(K\) in the previous research on pseudolabeling with CLIP . In general, \(K\) is a hyperparameter that may require optimization in practical cases. We decide to be consistent with the literature and apply this fixed value of \(K\) in order to reduce the addition of more confounding factors in our analysis.

### Exploring the design space

GRIP consistently enhances CLIP across prompt modalities and learning settings Table 1 reports the performance of GRIP, the best performing among the training strategies in Section 3.3, compared to CLIP and prompt-tuning baselines. Overall, GRIP consistently improves the performance of CLIP and the baselines across prompt modalities and learning settings. By tuning textual prompts, the average improvement over CLIP is 20.7 points in SSL, 14.9 in UL, and 32.4 in TRZSL, while the improvement on CoOp is 9.6 points in SSL, and 26.6 in TRZSL. Similar results for the visual prompts show that GRIP improves CLIP by 18.2 points in SSL, 15.7 in UL, and 30.8 in TRZSL, and VPT by 12.9 points in SSL, and 20.8 in TRZSL. We note that CoOp and VPT applied to the SSL setting correspond to learning only on the labeled data, and we do not run them in the UL setting as there is no labeled data. Results are similar for multimodal prompts. We defer them to Appendix A.3, due to space constraints.

No prompt modality is clearly superiorUsing pseudolabels dynamically is beneficial for each modality. However, determining the clear superiority of one prompt modality over the other is challenging, as it depends on the specific tasks. For example, visual prompts work better for EuroSAT, while textual prompts excel in Flowers102. Despite intuitive explanations (Section 3.1), the scientific consensus remains elusive . Hence, we prefer to emphasize that the dynamic use of pseudolabels consistently improves performance for each prompt modality, without declaring one modality as definitively better than the other.

Unsupervised learning is equivalent or more robust than learning with very few shotsThe accuracy of GRIP when applied to the fully unsupervised setting is either higher or equivalent to the accuracy of VPT, which is trained using two labeled instances per class (Table 1). This shows that pseudolabeled data can substitute very few labeled examples for prompt tuning. However, the significant improvement of GRIP over CoOp and VPT in the semi-supervised setting (see Table 1) suggests that leveraging unlabeled data through pseudolabeling is advantageous in scenarios where labeled data is scarce but there is an abundance of unlabeled data.

    &  &  &  \\  Method & SSL & UL & TRZSL & SSL & UL & TRZSL & SSL & UL & TRZSL \\  CLIP & \(63.67_{0.00}\) & \(63.40_{0.00}\) & \(54.48_{0.00}\) & \(54.46_{0.00}\) & \(}\) & \(17.86_{0.00}\) & \(17.86_{0.00}\) \\ CoOp & \(76.76_{1.11}\) & - & \(63.22_{0.00}\) & \(58.53_{0.01}\) & - & \(63.37_{0.02}\) & \(14.91_{3.22}\) & - & \(21.70_{0.03}\) \\ GRIP & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(16.98_{0.92}\) & \(15.22_{0.71}\) & \(}\) \\  \(\) CLIP & \( 19.93\) & \( 6.17\) & \(22.86\) & \(19.63\) & \(16.07\) & \(2 26.61\) & \( 0.6\) & \( 2.36\) & \( 8.22\) \\ \(\) CoOp & \( 6.84\) & - & \( 23.04\) & \( 15.58\) & - & \( 17.70\) & \( 2.07\) & - & \( 4.38\) \\   & DTD \\  CLIP & \(25.10_{0.00}\) & \(20.77_{0.00}\) & \(32.88_{0.00}\) & \(30.54_{0.00}\) & \(43.24_{0.00}\) & \(43.45_{0.00}\) \\ CoOp & \(56.42_{26.00}\) & - & \(21.15_{0.00}\) & \(}\) & - & \(49.68_{0.00}\) & \(37.10_{0.56}\) & - & \(46.3_{0.03}\) \\ GRIP & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\  \(\) CLIP & \( 46.68\) & \( 42.78\) & \( 53.29\) & \( 25.78\) & \( 24.33\) & \( 61.79\) & \( 12.83\) & \( 2.85\) & \( 21.85\) \\ \(\) CoOp & \( 15.36\) & - & \( 52.91\) & \( 0.85\) & - & \( 42.65\) & \( 18.97\) & - & \( 19.00\) \\    &  &  &  \\  Method & SSL & UL & TRZSL & SSL & UL & TRZSL & SSL & UL & TRZSL \\  CLIP & \(63.67_{0.00}\) & \(63.40_{0.00}\) & \(54.48_{0.00}\) & \(54.46_{0.00}\) & \(17.58_{0.00}\) & \(17.86_{0.00}\) & \(17.86_{0.00}\) \\ VPT & \(63.73_{1.52}\) & - & \(64.77_{0.00}\) & \(60.80_{0.00}\) & \(67.06_{0.00}\) & \(17.76_{0.68}\) & \(}\) \\ GRIP & \(}\) & \(63.09_{0.55}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(17.51_{0.61}\) & \(26.42_{0.00}\) \\  \(\) CLIP & \( 4.28\) & \( 0.58\) & \( 13.78\) & \( 16.74\) & \( 13.95\) & \( 27.73\) & \( 1.85\) & \( 0.07\) & \( 8.56\) \\ \(\) VPT & \( 4.22\) & - & \( 12.47\) & \( 10.42\) & - & \( 15.13\) & \( 1.67\) & - & \( 0.27\) \\   &  & DTD \\  CLIP & \(25.10_{0.00}\) & \(20.77_{0.00}\) & \(32.88_{0.00}\) & \(30.54_{0.00}\) & \(43.24_{0.00}\) & \(43.45_{0.00}\) \\ VPT & \(}\) & - & \(25.51_{0.00}\) & \(47.13_{1.34}\) & - & \(62.24_{0.02}\) & \(36.41_{21.71}\) & - & \(44.16_{0.01}\) \\ GRIP & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\  \(\) CLIP & \( 44.56\) & \( 42.94\) & \( 48.77\) & \( 30.60\) & \( 30.80\) & \( 66.43\) & \( 11.33\) & \( 2.27\) & \( 19.33\) \\ \(\) VPT & \( 27.14\) & - & \( 44.03\) & \( 16.35\) & - & \( 34.73\) & \( 18.16\) & - & \( 18.62\) \\   

Table 1: For each learning paradigm, we compare the accuracy of GRIP with CLIP zero-shot (Vi

[MISSING_PAGE_FAIL:8]

On the other hand, GRIP and CLIP expand pseudolabels by incorporating an additional decile of unlabeled data in each iteration (bottom x-axis). Initially, GRIP maintains accuracy, but as it nears completion the quality tends to decrease, while a larger dataset with good-quality pseudolabels becomes available.

Comparing GRIP and CLIP, GRIP's expanded pseudolabels exhibit superior quality and performs better (Table 1). Even though IFPL's pseudolabel accuracy surpasses GRIP in the final iteration, GRIP's overall performance remains better due to training on a larger number of pseudolabels (Table 2). This suggests that numerous, slightly noisier pseudolabels can yield better results, highlighting a trade-off and offering insights for future approaches.

GRIP benefits adaptation even for larger image encodersWe measure how much the effect of the iterative strategies changes if we consider a larger pre-trained image encoder. In Table 3, we report the average improvements of GRIP on CLIP for Flowers102, RESICS45, and DTD. The magnitude of improvements slightly decreases when using a larger image encoder. However, we still see significant benefits for both modalities. The smaller relative improvements with respect to smaller visual encoders align with our expectations. Larger encoders possess a stronger base knowledge, making it relatively more challenging to attain further improvements on top of it. In Table 10, we break down the accuracy of CLIP with different backbones. The performance of the larger backbone is higher, indicating higher quality pseudolabels.

### The Robin Hood effect

Although training models with pseudolabels can lead to good performance, it can also result in biased predictions and generate disparate impacts on sub-populations, i.e., the "Matthew effect" [49; 8]. Particularly, the use of pseudolabels can lead to improved performance in _well-behaved_ (high accuracy) classes but can cause stagnation or decreased performance in _poorly behaved_ (low accuracy) classes. As we explore the use of pseudolabels, we investigate how the accuracy of the analyzed approaches distributes across classes. Figure 4 show an opposite scenario from typical SSL. The solid line represents the sorted per-class accuracies of CLIP. The arrows indicate the per-class accuracies of GRIP. For all learning paradigms, the iterative training strategies increase the accuracy of classes where CLIP is not proficient, while maintaining or decreasing the accuracy of initially well-behaved classes. This effect, which we call the "Robin Hood effect," is very interesting because it shows how CLIP can mitigate its own bias toward certain classes by learning from itself.

To understand the roots of the Robin Hood effect, we examine two factors: (1) the role of pseudolabels generated by CLIP, and (2) the role of prompt tuning. To disentangle these factors, we explore the variation in per-class accuracy of a basic linear classifier trained on CLIP's ViT-B/32 image representation.

"Second generation" pseudolabels are a good treatment for class disparityWe train the linear classifier in the SSL setting on 2 labeled examples per class and pseudolabels. The pseudolabels are obtained through conventional methods, where a threshold of.95 is applied, or by using CLIP to generate 16 pseudolabels per class.

Figure 4: Improvements of FPL and GRIP on CLIPâ€™s per-class accuracies (RESICS45). The x-axis is the ranked class index, while y-axis is the accuracy.

  
**Textual prompts** & & & \\   & SSL & UL & TRZSL \\  Avg. \(\) CLIP (ViT-B/32) & \(17.46_{12.83}\) & \(8.36_{2.85}\) & \(23.77_{2.51}\) \\ Avg. \(\) CLIP (ViT-L/14) & \(15.85_{4.44}\) & \(8.16_{1.12}\) & \(19.96_{19.19}\) \\ 
**Visual prompts** & & & \\   & SSL & UL & TRZSL \\  Avg. \(\) CLIP (ViT-B/32) & \(10.78_{2.48}\) & \(6.88_{-0.58}\) & \(20.28_{13.78}\) \\ Avg. \(\) CLIP (ViT-L/14) & \(7.61_{3.27}\) & \(4.89_{-0.48}\) & \(16.14_{11.13}\) \\   

Table 3: Average improvement of GRIP with different backbones on Flowers102, RESICS45, and DTD. \(\) CLIP is the difference between the accuracy of GRIP and CLIP. Alongside the average, we provide the minimum improvement across tasks.

We find that both approaches yield similar overall accuracies. However, we observe the "Matthew effect" when using the first approach. In contrast, when using CLIP-based pseudolabels, the class disparity of the regressor trained solely on seen classes is reduced. Particularly, we see a significant improvement on initially poor classes, together with a significant diminishing of the accuracy of well-behaved classes. We observe a clear manifestation of the "Robin Hood effect." We present plots illustrating this effect in Appendix A.4.

Prompt tuning retains the accuracy of already rich classes better than linear probingTo evaluate the role of prompt tuning in the "Robin Hood effect," we train a linear classifier and textual prompts in the UL setting GRIP's training strategy. Comparing the per-class accuracies of the two approaches, GRIP on prompts shows an average improvement of 22.85 points for the poor classes across tasks, along with a slight average decrease of 0.3 points for the rich classes. On the other hand, linear probing determines a 14.42 points improvement for the poor classes, but it results in an average decrease of 9.39 points in accuracy for the rich classes (Appendix A.4).

## 5 Conclusions

We show that prompt tuning using pseudolabels generated by CLIP itself is a successful approach to enhance CLIP across various learning settings. Training strategies that iteratively refine pseudolabels turn out to be effective ways of leveraging pseudolabeled data. These approaches not only enhance CLIP's accuracy but also mitigate model biases toward certain classes. We hope this work lays a solid groundwork for reducing reliance on labeled data when adapting pre-trained vision-language models like CLIP to new tasks.

LimitationsThe effectiveness of the training strategies examined in this paper depends on both the strategies themselves and the quality of pseudolabels. The latter is particularly crucial. If CLIP performs poorly on a task, we may struggle to obtain a reliable set of pseudolabels to begin with, potentially diminishing CLIP's performance. Despite this potential risk, we have not observed any relevant failure of GRIP, even in tasks where CLIP's initial accuracy is extremely low (such as FGVCAircraft). Also, the pseudolabeling strategy we adopt involves selecting \(K\) pseudolabels per class, which can create a strong assumption about the distribution of the training data if we attempt to cover all unlabeled data. During the final iteration, it is as if we assume a uniform class balance.

Another important consideration is the efficiency of the explored methods. Repeating the training process multiple times brings impressive improvements at the cost of a non-negligible increase of computation time. At each iteration, we generate pseudolabels for the unlabeled data from scratch. While we parallelized the pseudolabeling procedure to cut some of the cost, reducing those for the iterative training presents more significant challenges. We decided to mainly focus on the analysis of qualitative and quantitative effects of pseudolabels in prompt tuning. Future research should address budget constraints and investigate optimal stopping criteria for the iterative process, considering the possibility of reaching a plateau or decreased pseudolabel quality after a certain point, to maximize efficiency while maintaining performance.