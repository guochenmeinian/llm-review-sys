# GSGAN: Adversarial Learning for

Hierarchical Generation of 3D Gaussian Splats

Sangeek Hyun

Sungkyunkwan University

&Jae-Pil Heo

Sungkyunkwan University

Corresponding author

###### Abstract

Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs. One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation. In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics. However, in an adversarial framework, we observe that a naive generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians. This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively. To address these issues, we introduce GSGAN, a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene. Experimental results demonstrate that ours achieves a significantly faster rendering speed (\(\)100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability. Project page: https://hse1032.github.io/gsgan.

## 1 Introduction

The research field of 3D generative models has recently emerged and shows impressive generation capability in various domains such as text-to-3D  and image-to-3D . Among them,

Figure 1: Generated examples from the proposed method (FFHQ-512, AFHQ-Cat-512). Ours synthesize multi-view consistent images with a significantly faster rendering speed by leveraging 3D Gaussian representation. We represent a 3D scene as a composite of hierarchical Gaussians, where each level of Gaussian depicts coarse and fine details corresponding to its level. To visualize the effects of individual Gaussian, the right-most images are rendered by reducing the scale of Gaussians.

3D Generative Adversarial Networks [11; 12; 13; 14; 15; 16] are an adversarial learning framework between a 3D generator and a 2D discriminator, capable of synthesizing 3D models solely by training with collections of 2D images or with the additional use of their corresponding camera poses. Specifically, the generator synthesizes a 3D model and renders it as a 2D image using given camera parameters, and then the 2D discriminator determines its realness and its match to the given camera pose.

The generally used volume rendering in 3D GANs is ray casting , which is widely adopted in NeRF literature [18; 19; 20; 21]. Ray casting-based rendering demonstrates its prominent capability to model a 3D scene in various domains of research; however, it is also known for its excessive computational costs. Specifically, it requires \((H W D)\) point sampling for rendering an image from a single camera view, where \(H,W,D\) denotes height, width, the number of sampled points on a ray (depth). This demanding computational cost hinders previous methods in 3D GANs from performing the rendering process at higher resolutions  or forces them to use architectures optimized for efficient 3D representation, leading to degraded generation quality [13; 15].

Recently, 3D Gaussian Splitting (3D-GS)  has been introduced for the 3D reconstruction task. This method represents a 3D scene as a composition of 3D Gaussians, projecting and \(\)-blending these Gaussians to render images. This rasterization-based method notably enhances rendering speed. Despite this innovative progress in the 3D reconstruction task, its application in 3D generative models has not yet been studied. One important challenge is the training algorithm of 3D-GS. Unlike the scene-independent and fully differentiable training of NeRFs, 3D-GS requires additional constraints such as the proper initialization of Gaussian positions for a given scene by SfM  and densification that heuristically manages the scale and number of Gaussians. Since 3D GANs deploy a single generator trained by gradient descent to model the distribution of 3D data, these non-differentiable and scene-dependent characteristics of 3D-GS restrict its application in 3D GANs.

In this paper, we extend the application of 3D Gaussian representation with rasterization to 3D GANs, leveraging its efficient rendering speed for high-resolution data. In an adversarial learning framework, we observe that a naive generator architecture, which simply synthesizes a set of Gaussians without any constraints, suffers from training instability and imprecise adjustment of the scale of Gaussians. For example, at the early stage of training, all the Gaussians disappear in the rendered images, or there are visual artifacts created by long-shaped Gaussians despite its convergence.

To address these training difficulties, we devise a method to regularize the Gaussian representation for 3D GANs, focusing particularly on the position and scale parameters. To this end, we propose a generator architecture with a hierarchical Gaussian representation. This hierarchical representation models the Gaussians of adjacent levels to be dependent, encouraging the generator to synthesize the 3D space in a coarse-to-fine manner. Specifically, we first introduce a locality constraint whereby the positions of fine-level Gaussians are located near their coarse-level counterpart Gaussians and are parameterized by them, thus reducing the possible positions of newly added Gaussians. Then, we design the scale of Gaussians to monotonically decrease as the level of Gaussians becomes finer, facilitating the generator's ability to model the scene in both coarse and fine details. Based on this representation, we propose a generator architecture that effectively implements the hierarchy of Gaussians. With additional architectural details, we validate that the proposed generator successfully synthesizes realistic 3D models (Fig. 1) with stabilized training and enhanced generation quality.

Our contributions are summarized as follows:

* We firstly exploit a rasterization-based 3D Gaussian representation for efficient 3D GANs.
* We introduce GSGAN, a hierarchical 3D Gaussian representation that regularizes the parameters of Gaussians by building dependencies between Gaussians of adjacent hierarchies, stabilizing the training of Gaussian splatting-based 3D GANs.
* The proposed method achieves a significantly faster rendering speed compared to state-of-the-art 3D GANs while maintaining comparable generation quality and 3D consistency.

## 2 Related works

### 3D Generative Adversarial Networks

3D GANs [11; 16] learn the 3D representation from a collection of 2D images with corresponding camera parameters. Recent progress mostly focuses on generator architecture, especially for enhancing efficiency. For example, EG3D  introduces the tri-plane representation with an additional upsampling 2D image after volume rendering. Differently, there is another stream that directly renders a high-resolution image without a 2D upsampler. For instance, GRAM  learns the iso-surfaces for efficient sampling of points on a ray, based on an implicit-network generator . Additionally, Voxgraf  exploits the sparse voxel grid instead of feature fields to represent the 3D scene. Epigraf  introduces a patch-wise training scheme to reduce the computational cost of volume rendering at high resolution (e.g., 512\(\)512) during the training phase. Differently, Mimic3D  proposes a method to learn 3D representation at high resolution by distilling the 2D upsampling result of EG3D into the high-resolution tri-plane. Most recently, WYSIWIG  proposes SDF representation-based GANs for reducing the number of sampling points on rays in a high-resolution 3D representation.

While previous methods use ray casting as a volume rendering method with demanding rendering costs, we exploit efficient rasterization by adopting 3D Gaussians as a 3D representation.

### Gaussian splatting for 3D representation

3D Gaussian Splatting (3D-GS)  represents a 3D scene as a composition of 3D Gaussians. This approach is known for its fast rendering speed, which rasterization-based rendering leads to, as well as its prominent reconstruction quality and faster convergence. Recently, various domains of research utilize 3D-GS, such as human avatar modeling[27; 28; 29], facial editing , text-to-3D synthesis [1; 2; 3; 4; 5; 6; 7], and image-to-3D synthesis [8; 9; 10]. These approaches typically substitute the neural radiance field with a Gaussian representation to leverage its efficient characteristics.

The research efforts also try to apply 3D Gaussian representation to an adversarial learning framework, especially with structural prior such as the human body and facial template. For example, Gaussian Shell Maps  focus on the task of 3D human generation based on SMPL template  with an adversarial learning framework. Given a human body template, the generator learns to synthesize multiple shell maps containing Gaussian parameters, achieving 3D consistent generation with faster rendering speed. Similarly, GGHead  focuses on 3D face generation using FLAME template . Similar to Gaussian Shell Maps, they synthesize texture maps containing Gaussian parameters and position offsets by 2D generator based on StyleGAN2 . However, these methods largely depend on the pre-defined template as structural prior.

Differently, in this paper, we focus on extending the application of 3D-GS to Generative Adversarial Networks without any geometric prior such as SfM that 3D-GS traditionally requires.

## 3 Proposed method

### Preliminaries

3D Generative Adversarial Networks3D GANs [16; 11; 12] is an adversarial learning framework between the 3D generator \(g(z,)\) and 2D discriminator \(d(I)\). Specifically, for a given randomly sampled latent code \(z^{d_{z}} p_{z}\) and camera pose \(^{d_{}} p_{}\), the generator \(g(z,)\) synthesizes a 3D scene and renders it to a 2D fake image \(I_{f}^{H W 3}\) based on the given camera pose. Then, the discriminator learns to discriminate the real image \(I_{r}^{H W 3}\) and the fake image \(I_{f}\), while the generator learns to deceive the discriminator. In addition, the discriminator often uses the camera pose to encourage \(d\) to be aware of 3D information by making \(d\) estimate the camera parameter [13; 36] or performing conditional adversarial loss .

3D Gaussian splatting3D Gaussian representation for 3D modeling is recently introduced by 3D Gaussian Splatting . It represents a scene as a composition of anisotropic 3D Gaussians estimated by multi-view images and their corresponding camera poses. This Gaussian representation contains parameters such as a position \(^{3}\), a scale \(s^{3}\), a quaternion \(q^{4}\), an opacity \(^{1}\), and a color \(c^{3}\). In world space, Gaussian is defined as follows:

\[G(x)=e^{-(x-)^{T}^{-1}(x-)},=RSS^{T}R^{T},\] (1)

where \(R^{4 4}\) and \(S^{4 4}\) are a rotation matrix and a scaling matrix obtained from a quaternion \(q\) and scale \(s\), and \(x\) is a world coordinate.

To render the image, the color of each pixel \(C\) is determined by blending the contributions of all \(N\) 3D Gaussians that overlap with the pixel as follows:

\[C=_{i=1}^{N}c_{i}_{i}^{}_{j=1}^{i-1}(1-_{j}^{}),\] (2)

where \(c_{i}\) is the color of each Gaussian, and \(^{}\) is blending weight of 2D projection of Gaussian multiplied by a per-point opacity \(\). The order of Gaussians is sorted by their depth.

### Hierarchical 3D Gaussian representation

Our focus is on utilizing Gaussians as the 3D representation of the generator in 3D GANs. We begin with a simple generator that takes a randomly sampled latent code \(z\) as input and outputs \(N\) Gaussians, without any restrictions. However, in this scenario, we observe that this naive application suffers from training instability and fails to properly manage the position \(\) and scale \(s\), as shown in examples in Fig. 8. Therefore, we concentrate on guiding the position and scales that the generator synthesizes.

To this end, we propose a hierarchical structure of Gaussians to effectively regularize the position and scale of Gaussians, as illustrated in Fig. 2. Firstly, we define the hierarchy level \(l\{0,...,L-1\}\), from coarse to fine levels, where each level contains a set of Gaussian parameters. In detail, we establish a dependency between the Gaussian parameters of adjacent levels. For simplification, we explain the dependency between two hierarchically adjacent Gaussians, \(G^{l}\) and \(G^{l-1}\), where \(G^{l}\) originates from \(G^{l-1}\). We aim to model the 3D representation in a coarse-to-fine manner, by assigning coarser- and finer-level Gaussians \(G^{l-1}\) and \(G^{l}\) to be responsible for coarser and finer details of the 3D scene, respectively.

For the position parameter, we impose a locality constraint that bounds the position \(^{l}\) of the finer-level Gaussian \(G^{l}\) with its corresponding \(G^{l-1}\). Specifically, we introduce the local position parameter \(^{l}\) defined in the local coordinate system, which is centered, rotated, and scaled by the position \(^{l-1}\) and scale \(s^{l-1}\) and quaternion \(q^{l-1}\) of the coarser-level Gaussian \(G^{l-1}\). Then, the position \(^{l}\) in world space is formulated by transforming the local position \(^{l}\) as follows:

\[^{l}=^{l-1}+R^{l-1}S^{l-1}^{l},\] (3)

where \(R^{l-1}\) and \(S^{l-1}\) are a rotation and scaling matrix obtained from \(q^{l-1}\) and \(s^{l-1}\). This operation ensures the position of Gaussians at finer levels depends on coarser-level Gaussians, while residing near the location of coarser-level counterparts.

For the scale parameter, we enforce the scale parameter to monotonically decrease to a certain degree as its hierarchy level increases. In detail, we define the scale of the finer-level \(s^{l}\) using the relative scale difference \(^{l}\) against the coarser-level scale \(s^{l-1}\). Additionally, we restrict this scale difference \(^{l}\) to always be a vector of negative values. Furthermore, we introduce the constant \( s\) which further lowers the scale of the finer-level. This process is defined as follows:

\[s^{l}=s^{l-1}+^{l}+ s,^{l}, s<0.\] (4)

Figure 2: Illustration and examples of hierarchical Gaussian representation. (a) We parameterize the finer-level Gaussians by the parameters of coarser-level counterparts for regularizing the scale and position of synthesized Gaussians. (b) Example of synthesized Gaussians across multiple hierarchy levels. Gaussians represent coarse or fine details according to its hierarchy level.

For the other parameters, we additionally define the residual Gaussian parameters \(^{l},^{l},^{l}\) at level \(l\), which are added to the Gaussian parameters of the previous level as follows:

\[^{l}=^{l-1}+^{l}, q^{l}=q^{l-1}+^{l}, c ^{l}=c^{l-1}+^{l}.\] (5)

We call this hierarchical relationship between \(G^{l-1}\) and Gaussians with residual parameters \(\{^{l},^{l},^{l},^{l},^{l}\} ^{l}\) as \((G^{l-1},^{l})\), and it enables the generator to model the 3D space in a coarse-to-fine manner, where the fine-level Gaussians depict the detailed part of the coarse-level counterparts. Importantly, it stabilizes the training of GANs by significantly reducing the possible positions of Gaussians and encourages the generator to use various scales of Gaussians, thereby boosting the generation capability to model both coarse and fine details.

### GSGAN; the generator architecture with hierarchical 3D Gaussians

In this section, we propose a generator architecture for leveraging the aforementioned hierarchical structure of Gaussians (Fig. 3). Basically, we adopt a transformer-based architecture, composed of stacks of attention and MLP layers, which is a generally used architecture to handle unstructured 3D point cloud data .

First of all, we define a generator \(g(z,)\) as a sequence of generator blocks, where \(_{l}\) denotes the generator block at a specific level \(l\). At the coarsest level \(l=0\), \(_{}\) takes input as \(N\) learnable positions \(^{N 3}\) and latent code \(z\), where \(N\) is the number initial Gaussians, then outputs the high-dimensional features \(x^{0}\). Then, for a feature of \(i^{}\) Gaussian \(x^{0}_{i}\), we process the feature \(x^{0}_{i}\) by the output layer \(_{0}\) to obtain a Gaussian parameter \(\{^{0}_{i},s^{0}_{i},q^{0}_{i},^{0}_{i},c^{0}_{i}\} G^{0}_{i}\). For arbitrary level \(l\), \(_{}\) takes input as feature \(x^{l-1}_{i}\) from the previous block and latent code \(z\), then outputs the feature \(x^{l}_{j}\). Importantly, the output block \(_{l}\) does not directly synthesize the Gaussian parameters \(G^{l}_{j}\). Instead, the intermediate output \(^{l}_{j}\) contains the local position \(^{l}_{j}\) and relative scale difference \(^{l}_{j}\), as well as the other residual parameters \(^{l}_{j},^{l}_{j},^{l}_{j}\). This intermediate output \(^{l}_{j}\) is combined with the corresponding Gaussians \(G^{l-1}_{i}\) from the previous level to finally synthesize the Gaussian \(G^{l}_{j}\) at level \(l\), following the operations in Sec. 3.2. This process, which establishes the hierarchical dependency between Gaussians of adjacent level \(G^{l-1}_{i}\) and \(G^{l}_{j}\), is defined as follows:

\[x^{l}_{j}=_{l}(x^{l-1}_{i},z),^{l}_{j}= _{l}(x^{l}_{j}), G^{l}_{j}=(G^{l-1}_{i}, ^{l}_{j}),\] (6)

where \(\) operation denotes the combining process of parameters in hierarchically adjacent \(G^{l-1}_{i}\) and \(^{l}_{j}\) mentioned in eqn. 3, 4, 5.

Figure 3: The architecture of the generator and its block. (a) Generator synthesizes the multiple-level of anchors and Gaussians, which contains the residual parameters \(^{l}\) and \(^{l}\). Anchors are utilized to regularize the finer-level Gaussians, while Gaussians are used for actual rendering. After generating these parameters, we combine them with anchors from previous level \(A^{l-1}\) by \(\) operation, as defined in eqn. 3, 4, 5 (green arrow). (b) The generator consists of stacks of \(\), each of which is a sequence of attention and MLP layers. The latent code \(z\) is conditioned on the generator through AdaIN and layerscale, where the modulation and scaling parameters are derived from style code \(w\).

As the fine-level Gaussians have smaller scales compared to the coarse-level ones, the number of Gaussians should be increased as the hierarchy increases to successfully synthesize the fine details. Thus, we expand the number of Gaussian parameters \(G_{i}^{l}\) to have a total of \(r^{l}N\) vectors for each parameter, where \(r\) is an upsampling ratio. In other words, we define the Gaussian \(G_{j}^{l}\) to be dependent on \(G_{i}^{l-1}\), where \(j=ri+k\) and \(k\{0,1,...,r-1\}\).

After synthesizing the Gaussian parameters of every level, we use all of them for generating the image (i.e. total \((N+rN+...+r^{L-1}N)\) Gaussians are used for rendering). For rendering, we use a tile-based rasterizer following 3D-GS .

Anchor Gaussians to decompose Gaussians for regularization and renderingIn the aforementioned architecture, Gaussians are used not only to represent a 3D scene but also to regularize their coarser-level Gaussian counterparts. This means Gaussians must be trained to precisely guide the parameters of finer-level Gaussians while simultaneously depicting the sharp details in real-world images. However, achieving both of these objectives can be challenging. For instance, we observe that the scale of Gaussians can become nearly zero along a specific axis, leading to excessively strong regularization on the position of finer-level Gaussians. To handle this issue, we introduce an auxiliary set of Gaussians that only contributes to regularization, instead of actual rendering.

Specifically, we introduce an anchor Gaussian \(A^{l}\) for a specific level \(l\), which has identical parameterization to \(G^{l}\). However, this type of Gaussian is only used for regularization by deploying it as the input of densify, especially for the coarser-level Gaussian input. Therefore, an anchor Gaussian \(A_{i}^{l-1}\) only learns to guide the parameters of their finer-level counterpart \(G_{j}^{l}\), so its usage relieves the effects of strong regularization caused by zero variance. To generate it, we simply make toGauss\({}_{l}\) synthesize two sets of Gaussians, \(_{j}^{l}\) and \(_{j}^{l}\), for a given feature \(x_{j}^{l}\). This process is achieved by re-defining the eqn. 6 as follows:

\[[_{j}^{l},_{j}^{l}]=_{l}(x_{j}^{l}),  G_{j}^{l}=(A_{i}^{l-1},_{j}^{l}), A_{j}^{l} =(A_{i}^{l-1},_{j}^{l}).\] (7)

As the Gaussians of coarsest level \(l=0\) does not have their coarser-level counterparts, we define \(A_{i}^{0}=_{i}^{0}\) and \(G_{i}^{0}=(A_{i}^{0},_{i}^{0})\).

Architectural detailsFollowing the conditioning convention of previous GANs [35; 40], we utilize the mapping network to modify the latent code \(z\) into the style code \(w\). Then, the style code affects the synthesis process by AdaIN . As noted, the generator block is a stack of attention and MLP layers, then we replace the layer norm in attention and MLP by AdaIN, following generally used approach for transformer-based GANs [42; 43]. For blocks of coarser levels, we utilize the general self-attention without positional encoding as the attention mechanism, whereas we use local attention  for finer levels, as the interaction between \(r^{l-1}N\) points is computationally demanding. For expanding the features in generator blocks after the coarsest level, we simply use the subpixel operation  with skip connection and repeat Gaussians \(G^{l}\) in \(r\) times.

One important architectural design is the usage of layerscale , which is a learnable vector that adjusts the effects of the residual block by multiplying it by the output of the residual block. Typically, it is initialized by a zero-valued vector, removing the effects of layers in the early stage of training. We observe it is essential for stabilizing the position of Gaussians in early iterations. In addition, we use the adaptive version of layerscale  conditioned by latent code \(z\) on every attention and MLP layer in the generator.

Also, we use the camera direction as a condition for the color layer in toGaus to model the view-dependent characteristics and employ the background generator, which resembles the generator architecture but with reduced capacity and results in the Gaussians located within a sphere of a radius of 3, while the foreground resides in the [-1, 1] cube. For further details, we elaborate on them in the Appendix A.1.

### Training objectives

Similar to previous 3D GANs , we adopt the non-saturating adversarial loss  with R1 regularization . Formally, these objective functions are defined as follows:

\[_{}=_{z P_{z}, P_{g}}[f(d(g(z, )))]+_{I_{r} P_{I_{r}}}[f(-d(I_{r}))+|| d(I_ {r})||^{2}],\] (8)where \(f(t)=-(1+(-t))\) is a softplus function and \(\) is R1 regularization strength.

We additionally guide the 3D information to the discriminator and generator by introducing contrastive learning between pose embedding obtained from the images and camera parameters. Specifically, the discriminator has a pose branch \(d_{p}\) that estimates the pose embedding \(p_{I}\) from an input image. Then, we introduce a pose encoder that consists of MLP layers and encodes the camera parameter \(\) into the pose embedding \(p_{}\). Similar to previous work , we utilize the contrastive objective which enhances similarity between corresponding \(p_{I}\) and \(p_{}\). Formally, this objective is defined as follows:

\[_{}=-(((p_{I},p_{ }^{b})/)}{_{b=1}^{B}(((p_{I},p_{}^{b})/ ))}),\] (9)

where \((,)\) is a cosine similarity and \(B\) is a batch size and \(p_{}^{+}\) is a positive sample corresponding to a pose embedding \(p_{I}\) from the image, and \(\) is a temperature scaling parameter. For the discriminator, we calculate \(_{}\) using real data, while using fake data for training the generator.

Furthermore, we introduce two losses to regularize the position of anchor Gaussians in the coarsest level, \(_{A}^{0}\). We first regularize the averaged position of \(_{A}^{0}\) to be zero for encouraging the center of Gaussians residing near the origin of the world space. Secondly, we reduce the distance between the positions of the \(K\) nearest anchor Gaussians to prevent anchor Gaussians from falling apart from the others. These two regularization losses are defined as follows:

\[_{}=||_{j=1}^{N}_{A,j}^{0}||^{2}, _{}=_{j=1}^{N}||_{k=1}^{K}(_ {A,j}^{0}-(_{A,j}^{0},k))||^{2},\] (10)

where KNN\((_{A,j}^{0},k)\) is the position of \(k^{}\) nearest neighbor of \(j^{}\) anchor Gaussian of \(_{A}^{0}\).

To sum up, the final objective function \(\) is as follows:

\[=_{}+_{}_{}+_{}_{}+_{}_{},\] (11)

where \(_{}\), \(_{}\), and \(_{}\) are strengths of the corresponding objective function.

## 4 Experiments

### Experimental settings

Following the experimental settings of previous 3D GANs [12; 25], we use FFHQ  and AFHQ-Cat  datasets with 256\(\)256 and 512\(\)512 resolutions. For example, we augment the datasets with the horizontal flip and additionally use adaptive data augmentation  for AFHQ-Cat dataset, which has a limited size. Camera pose labels are obtained from the official repository of EG3D , which are predicted by off-the-shelf pose estimators [52; 53]. We train the model from scratch on each dataset. For further implementation details, please refer to Appendix A.1.

    & 3D &  &  &  \\ Methods & consistency & 256\(\)256 & 512\(\)512 & 256\(\)256 & 512\(\)512 & 256\(\)256 & 512\(\)512 \\  EG3D  & & 4.80 & 4.70 & 3.41 & 2.72 & - & 15.5\({}^{*}\) \\  GRAM  & ✓ & 13.8 & - & 13.4 & - & - & - \\ GMPI  & ✓ & 11.4 & 8.29 & - & 7.67 & - & - \\ EpiGRAF  & ✓ & 9.71 & 9.92 & 6.93 & - & - & - \\ Voxgari  & ✓ & 9.60 & - & 9.60 & - & - & - \\ GRAM-HD  & ✓ & 10.4 & - & - & 7.67 & 173.0 & 197.9 \\ Mimic3D  & ✓ & **5.14** & **5.37** & 4.14 & 4.29 & 106.8 & 402.1 \\  GSGAN (Ours) & ✓ & 6.59 & 5.60 & **3.43** & **3.79** & **2.7** & **3.0** \\   

Table 1: Quantitative comparison on FFHQ and AFHQ-Cat datasets in terms of FID-50K-full and rendering time. We mainly compare ours with the 3D consistent models, except EG3D utilizing the 2D upsampler. FID scores are taken from previous work . Rendering time is measured on a single RTX A6000 GPU. \({}^{*}\)The rendering time of EG3D at 512 resolution consists of the time for volume rendering at 128 resolution and 2D upsampling operations.

### Experimental results

Quantitative resultsWe mainly compare the proposed method with previous 3D consistent GANs, in terms of FID. These methods have strict 3D consistency, which directly renders the high-resolution images from 3D representation without any 2D upsampling operations. As reported in Tab. 1, we validate that the proposed method surpasses most of the previous methods and also achieves comparable generation capability compared to the state-of-the-art, Mimic3D. Especially for AFHQ-Cat dataset, we achieve a much lower FID, even comparable to the non-3D consistent baseline, EG3D. Next, we evaluate the rendering speed enhancement of the proposed method compared to baseline methods. To compute it, we first synthesize a 3D representation of each model and measure the processing time of rendering. As reported, ours shows significantly faster speeds compared to the baseline methods, achieving more than 100 times faster rendering than Mimic3D in 512\(\)512 resolution. Moreover, it is even faster than non-3D consistent baseline EG3D, which is the model that exploits 2D upsampling operation for reducing the efficient rendering. In addition, one important point is that rendering time is almost identical regardless of the image resolution, suggesting that the proposed method can be more effective at higher resolutions. Additionally, the training time of the proposed methods is 28 RTX A6000 days, while the state-of-the-art Mimic3D requires 64 A100 days on the FFHQ-512 dataset. This notable gap in training cost implies that ours can achieve comparable generation capability efficiently in both rendering and training speed.

Qualitative resultsWe present examples of generated images from the proposed method and the most recent 3D consistent GANs [25; 55], in Fig. 5. In both datasets, we observe that the proposed method successfully synthesizes the multi-view consistent images, validating its capability for synthesizing the realistic 3D scene. Also, we validate ours can generate both coarse and fine details such as coarse details of skin in human facial images and fine details of fur in cat images.

Level-by-level visualizationTo further understand how the synthesized Gaussians work, we visualize the Gaussians from an individual level. As depicted in Fig. 6, we observe that Gaussians capture the image components from overall structure to fine details as the level increases.

Figure 4: Qualitative results of the proposed method with truncation psi (\(=0.7\)).

Figure 5: Qualitative comparison with 3D consistent methods with truncation psi (\(=0.7\)).

Comparison with 3D consistencyAs a 3D generative model, it is important to maintain 3D consistency across different views. To validate the 3D consistency of the synthesized 3D model, we measure how well the generated 3D scene is reconstructed by a surface estimation model, following previous works . Specifically, we fit the surface estimation model to a generated multi-view image and compute the reconstruction error of the multi-view images used for training. For surface estimation, we use Neus2 , applying a facial segmentation mask estimated from an off-the-shelf network  to eliminate the effects of the background. Furthermore, we utilize only the foreground generator for our method. As shown in Tab. 2, the proposed method significantly outperforms GRAM-HD while achieving performance comparable to Mimic3D. This experiment suggests that our approach generates 3D-consistent results by leveraging the explicit 3D representation provided by 3D Gaussians.

Training stability at the early stage of trainingWe conduct an experiment to assess the effect of the proposed method on training stability, particularly at the early stage of training, by observing the fake logit of the discriminator. As shown in Fig. 7, we observe that the model without any constraints exhibits rapid divergence, accompanied by a markedly low fake logit, indicating that the discriminator already distinguishes between real and fake data perfectly. When applying a minimal constraint that limits the scale to its predefined maximum, the model does not diverge but still suffers from instability, as evidenced by a large standard deviation of the logit. In contrast, ours demonstrates stable training compared to other models. Note that, the standard deviation is visualized by 1-\(\).

Ablation studyWe perform an ablation study on the proposed components, particularly focusing on the proposed hierarchical architecture. When ablating the regularizations, we keep the residual representation of other parameters, \((q,,c)\). In the absence of any constraints, the model exhibited an FID score exceeding 300, signifying a failure to converge, in Tab. 3. With a minimal constraint that clips scales, the model does not diverge but significantly suffers from its low generation capability. As we gradually attach the proposed components, we observe enhancements in FID, showing the effects of the regularization of position and scale and the introduction of anchor Gaussians.

In Fig. 8, we provide visualizations of synthesized Gaussian positions. Initially, a simple clipping of the scale allow the model to synthesize images, but it leads to visual artifacts due to elongated Gaussians with large scales. Additionally, many Gaussians remain outside the scene, likely due to their tendency not to overlap, particularly when large-scale Gaussians exist. After applying position regularization, Gaussians become more densely located, although visual artifacts and unused Gaussians persist. Upon introducing scale regularization, the synthesized images no longer exhibit such artifacts, but the model struggles to capture precise geometries, as evidenced by the point cloud visualizations. Finally, the model incorporating anchor Gaussians successfully synthesizes realistic images while accurately estimating Gaussian positions. Note that, we clip the Gaussian positions to exist within the range of [-1, 1] in the cube.

Additional visualizationsWe additionally provide 1) additional generated examples, 2) examples with densely changed camera positions, 3) latent interpolation and w+ inversion, 4) visualization of anchor Gaussians, and 5) effects of background generator in Appendix A, and code implementation with additional videos in supplementary zip file, so please refer to them.

## 5 Broader Impact and Limitations

Broader ImpactThe proposed method follows the negative social impact of previous 3D generative models. For example, it may be used for synthesizing fake news or deepfake. Furthermore, since we boost the rendering speed of 3D GANs, \(\)100 faster than previous methods, it can encourage the generation of them by reducing computation cost for rendering the image.

LimitationsDifferent from the 3D-GS, which adaptively removes and introduces Gaussians by densification, the proposed method synthesizes a fixed number of Gaussians. This lack of adaptivity in the number of Gaussians remains a limitation, as the number of Gaussians can differ depending on the scene it makes. Also, the scale in hierarchical Gaussian representation is somewhat dependent on the hyperparameter such as \( s\). These factors require adjustment of hyperparameters and can affect the performance of the generator.

## 6 Conclusion

In this paper, we exploit the 3D Gaussian representation in the domain of 3D GANs, leveraging its fast rendering speed with explicit 3D representation. To relieve the absence of proper initialization and densification process of 3D-GS, we propose hierarchical Gaussian representation which effectively regularizes the position and scale of generated Gaussians. We validate the proposed technique to stabilize the training of the generator with 3D Gaussians and encourage the model to learn the precise geometry of 3D scene, achieving almost \(\)100 faster rendering speed with a comparable 3D generation capability.

## 7 Acknowledgements

This work was supported in part by MSIT&KNPA/KIPoT (Police Lab 2.0, No. 210121M06), MSIT/IITP (No. 2022-0-00680, 2020-0-01821, 2019-0-00421, RS-2024-00459618, RS-2024-00360227, RS-2024-00437102, RS-2024-00437633), and MSIT/NRF (No. RS-2024-00357729).

    & FID \\  No constraints & 300\(\) \\  + Clipping scale & 95.97 \\ + Position reg. (eqn. 3) & 17.65 \\ + Scale reg. (eqn. 4) & 13.80 \\ + Background generator & 12.61 \\ + Anchor Gaussian & 6.59 \\   

Table 3: Ablation study on FFHQ-256. “Clipping scale” means the model with the maximum limit of scale.