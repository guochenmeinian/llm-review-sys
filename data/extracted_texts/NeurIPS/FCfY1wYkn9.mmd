# MU-Bench: A Multitask Multimodal Benchmark

for Machine Unlearning

Jiali Cheng  Hadi Amiri

University of Massachusetts Lowell

{jiali_cheng, hadi_amiri}@uml.edu

###### Abstract

Recent advancements in Machine Unlearning (MU) have introduced solutions to selectively remove certain training samples, such as those with outdated or sensitive information, from trained models. Despite these advancements, evaluation of MU methods have been inconsistent, employing different trained models and architectures, and sample removal strategies, which hampers accurate comparison. In addition, prior MU approaches have mainly focused on _singular_ tasks or modalities, which is not comprehensive. To address these limitations, we develop MU-Bench, the first comprehensive benchmark for MU that _(i) unifies the sets of deleted samples and trained models_, and _(ii) provides broad coverage of tasks and data modalities_, including previously unexplored domains such as speech and video classification. Our evaluation show that RandLabelGraves et al. (2021) and SalUnFan et al. (2024) are the most effective general MU approaches on MU-Bench, and Bad-TChundawat et al. (2023) and SCRUBKurmanji et al. (2023) are capable of achieving random performance on the deletion set. We analyze several under-investigated aspects of unlearning, including scalability, the impacts of parameter-efficient fine-tuning and curriculum learning, and susceptibility to dataset biases. MU-Bench provides an easy-to-use package that includes dataset splits, models, and implementations, together with a leader board to enable unified and scalable MU research.1.

## 1 Introduction

Machine Unlearning (MU) aims at selectively removing a small portion of training data-and the influence of the samples-from a trained model. MU is essential for protecting sensitive information and discarding outdated samples. Recent works have studied machine unlearning in various contexts, including classification tasks on image Guo et al. (2020); Tang et al. (2023) and graph Chien et al. (2023); Cheng et al. (2023) data, multimodal tasks Cheng and Amiri (2023), generation tasks Chen and Yang (2023); Gandikota et al. (2023); Fan et al. (2024), and federated learning Wang et al. (2022).

Despite these advancements, existing approaches to machine unlearning face several challenges: (1): MU systems are evaluated under inconsistent settings, using different trained models (from which data is deleted) and metrics, which can lead to unfair comparisons and hinder the development of robust unlearning approaches Fan et al. (2024); (2): evaluation tend to focus on specific tasks, modalities, and architectures, which limits our understanding on the effectiveness of these models across different settings Wang et al. (2023); Chundawat et al. (2023).

To address these limitations, we introduce MU-Bench, a comprehensive machine unlearning benchmark consisting of multiple tasks, data modalities, base models, standardized evaluation metrics, all compiled into an easy-to-use package with a leader board to enable robust and scalable MU research.

To the best of our knowledge, this benchmark represents the first effort to benchmark existing MU approaches across a wide range of settings.

Our contributions are:

* constructing the first comprehensive MU benchmark with a wide coverage of tasks, domains, and modalities, including previously unexplored areas, such as speech and video processing, and biomedical applications, for systematic evaluation of unlearning algorithms;
* unifying (and perhaps democratizing) MU with uniformed deleted samples and a wide range of trained models and architectures to enable fair comparisons between MU methods;
* identifying design choices that explain performance variations across tasks and modalities;
* investigating several overlooked aspects of unlearning, such as deletion capacity, parameter-efficient fine-tuning (PEFT), and the impact of curriculum learning and dataset bias to inform future research directions.

Extensive experiments show that RandLabelGraves et al. (2021), Bad-TChundawat et al. (2023), and SalUnFan et al. (2024) are generally robust MU methods. When operating under a fixed training budget of compute (FLOS), RandLabel and SalUn outperform Bad-T. We find that existing MU methods benefit from PEFT but much less than other learning tasks, where below is 50% of the entire parameters, below which the model cannot be trained. Moreover, Curriculum Learning techniques can help models forget less and does not facilitate MU in most cases. In addition, performance variations across different tasks and modalities suggest that specific design choices within MU approaches significantly influence their effectiveness. In particular, certain tasks such as audio and video classification, are challenging for existing MU methods.

By design, MU-Bench is structured to incorporate new datasets and tasks, and we will continue to expand its resources in future.

## 2 MU-Bench

We outline the design of MU-Bench, covering tasks, datasets, models, and evaluation metrics.

### Problem Formulation

Machine unlearningLet \(D_{}\) denotes the training dataset, \(D_{f} D_{}\) the subset to be unlearned, and \(D_{r}=D_{} D_{f}\) the remaining dataset post-unlearning. Given a model \(f\) trained on \(D_{}\), machine unlearning seeks to remove the influence of \(D_{f}\) from \(f\) without affecting the knowledge it gained from \(D_{r}\), without retraining from scratch. We term \(f\) as the original model and \(f^{}\) as the model post-unlearning. A successful unlearned model \(f^{}\) should be minimally impacted by \(D_{f}\), while maintaining the performance of \(f\) on the original downstream test set \(D_{}\).

Evaluation MetricsEvaluating the efficacy of unlearning is crucial for identifying models that are more secure and retain no/less memory of deleted data. While previous studies have employed different metrics, we propose a set of metrics that do not require model retraining: performance

Figure 1: The MU-Bench benchmark for machine unlearning (MU) spans a comprehensive range of tasks and modalities, including previously unexplored data types such as audio, video, and biomedical data. The open-source package of MU-Bench provides standardized (unified) data splits, implements a suite of commonly-used MU methods and their design choices, enables fast experimentation and fair comparisons across MU methods, and is structured to easily incorporate new datasets and tasks in future.

on test set \(D_{}\) (\(\)), performance on deletion set \(D_{f}\) (\(\)), performance on remaining set \(D_{r}\) (\(\)), unlearning time (\(\)), and success rate of membership inference attack (\(\)).

Toward a retrain-free evaluationEarly works in machine unlearning research often considered the model retrained from scratch on \(D_{r}\) as the _gold_ standard for \(f^{}\), which is now recognized as an inappropriate design choice due to several issues: **First**, evaluating \(f^{}\) based solely on its closeness or similarity to the retrained model can lead to false negatives. This is because the parameters of \(f^{}\) may fall onto different distributions than the retrained model, but still achieve competitive unlearning performance. On the other hand, the parameters of two models can match even with completely different training datasets (Lamproudis et al., 2022). **Second**, retrained models cannot guarantee the privacy of deleted data in practice, often maintaining undesired high performance on \(D_{f}\), as demonstrated by previous work (Cheng et al., 2023). **Third**, obtaining a precise \(D_{r}\) can be impractical in cases where the goal of unlearning is to remove toxic content (Zhang et al., 2023; Itharco et al., 2023) or abstract concepts (Gandikota et al., 2023). Such abstract concepts may not correspond to identifiable data samples. **Finally**, retraining a model from scratch on \(D_{r}\) can be impractical or even impossible due to confidentiality constraints, proprietary data concerns, or because the data may no longer be available. In addition, retraining is often expensive, especially for large datasets or complex tasks such as multimodal learning or large language models (LLMs). Based on the above shortcomings, we advocate for a retrain-free evaluation of unlearning systems, a method that is increasingly recognized in recent works (Chundawat et al., 2023).

### Datasets and Tasks

We adopt nine publicly available datasets covering a diverse set of discriminative and generative tasks and data modalities. As Table 1 shows, the discriminative tasks include CIFAR-100 (Krizhevsky, 2009) for image classification, IMDB (Maas et al., 2011) for sentiment classification, DDI (Segura-Bedmar et al., 2013) for relation extraction in the biomedical domain, NLVR2 (Suhr et al., 2019) for visual reasoning, Speech Commands (Warden, 2018) for keyword spotting, and UCF101 (Soomro et al., 2012) for action classification. The generative tasks include SAMSum (Gliwa et al., 2019) for text summarization, Biography (adapted from Min et al. (2023), see below) for text generation, Tiny ImageNet (Le and Yang, 2015) for text-to-image generation.

We build a new dataset for evaluating machine unlearning in large language models (LLMs), focusing on the removal of personal information, as a common unlearning request. This is a crucial tasks because for example, on social media, user can choose to delete their accounts or privatize them, resulting in a critical and perhaps legal impetus for machine unlearning. The dataset contains factual descriptions of 183 celebrities, obtained from (Min et al., 2023), to enable machine unlearning of personal data from LLMs.

These datasets were chosen for their relevance to practical machine unlearning tasks, their variety, including both well-established and under-explored datasets, and their capacity to highlight differ

**Dataset** & **Task** & **Domain** & **Modality** & \(||\) \\   \\  CIFAR-100 (Krizhevsky, 2009) & Image classification & General & Image & 50K \\ IMDB (Max et al., 2011) & Sentiment classification & Movie review & Text & 25K \\
* DDI-2013 (Segura-Bedmar et al., 2013) & Relation extraction & Biomedical & Text & 25K \\ NLVR2(Sahr et al., 2019) & Visual reasoning & General & Image-Image-Text & 62K \\
* Speech Commands (Warden, 2018) & Keyword spotting & Commands & Speech & 85K \\
* UCF101 (Soomro et al., 2012) & Action classification & General & Video & 9.3K \\   \\  SAMSum (Gliwa et al., 2019) & Text summarization & Chat dialogue & Text & 14K \\
* BioFact (Min et al., 2023) & Text generation & Biography & Text & 183 \\ Tiny ImageNet (Le and Yang, 2015) & Text-to-Image generation & General & Image-Text & 20K \\  

Table 1: Example datasets currently available in MU-Bench, covering a wide set of tasks and data modalities from different domains. \(|D|\) denotes the size of training data. In MU-Bench, we set the deletion ratio to a maximum of 10% of \(|D|\). Rows labeled with
* indicate new tasks and data modalities introduced in MU-Bench for machine unlearning.
ences between unlearning methods across diverse tasks and modalities (as they have non-saturated performance). This datasets allow for large scale and fair evaluation of unlearning methods, and addresses gaps in current research in several unexplored areas in machine unlearning.

### Unified Unlearning

To address inconsistencies in the evaluation of MU approaches, we unify critical aspects such as _the choice and size of deleted samples_ (\(D_{f}\)), and _the baseline model_ (\(f\)) from which data is removed. This unification allows for meaningful comparison and democratizes access through open-source tools.

Deleted SamplesFor each dataset, we randomly sample 1-10% of the training data as \(D_{f}\), with increments of 1% to covers both typical and extreme evaluation settings. This approach reflects typical and realistic settings where a small portion of data is deleted (Golatkar et al., 2020; Chundawat et al., 2023; Cheng et al., 2023), and challenges the limits of unlearning methods without fundamentally altering the data distribution, as would be the case with more extensive data removal.

Original ModelFor each dataset, we train a set of commonly-used models on different architectures and scales, from which \(D_{f}\) is deleted, to allow for robust and relevant comparisons. We train a total of 20 architectures and 34 scales, such as ResNet (He et al., 2016) (18, 34, 50 layers), ViT (Dosovitskiy et al., 2021) (Small, Base, Large), Swin-Transformer (Liu et al., 2021) (Tiny, Small, Base), MobilNet V2 (Sandler et al., 2018) for image classification; and HuBERT (Hsu et al., 2021) (Base, Large, X-Large), Whisper (Radford et al., 2023) (Tiny, Small, Base), Wav2Vec2.0 (Baevski et al., 2020) (Base, Large) for the audio classification. Additional details are provided in Appendix A.3.

Example UsageWe include the datasets, standardized data splits, evaluation scripts, and unlearning methods within an easy-to-use Python package and integrate them with commonly-used packages such as PyTorch (Paszke et al., 2019), Huggingface Transformers (Wolf et al., 2020), and Diffusers (von Platen et al., 2022), containing pre-trained diffusion models for image and speech data. Users can initiate an unlearning experiment with minimal adjustment to existing script. All original model checkpoints are released for standardized unlearning and fair comparisons. We also host and maintain a leaderboard to rank methods overall and on individual tasks and architectures. For example, to remove 5% of training data from a BERT-base model trained on IMDB using Bad-T (Chundawat et al., 2023), only a minimal script modification is required shown in code example 1. This setup simplifies the unlearning process and enables rapid comparison against methods and architectures.

Taxonomy of Unlearning Techniques: A Teacher-Student FrameworkTo provide a deeper understanding of the design choices of existing MU approaches and their performance differences, we introduce a taxonomy based on a unified teacher-student framework. In this framework, the desired unlearned model \(f^{}\) seeks to selectively discard specific knowledge from the original model \(f\) under the guidance of a "teacher.' As shown in Table 2, the design choices of the teacher vary across different methods mainly from three aspects:

* **Knowledge Measurement (KM)**: the key question of how knowledge is quantified, which is determined by task loss (Loss), representation (Rep.), or output logits (Logit) in existing MU models;
* **Knowledge Corruption on \(D_{f}\) (Corrupt)**: the key question of how the knowledge associated with \(D_{f}\) is degraded, which is currently determined using techniques such as reversing gradients (NegGrad), using random data (RandLabel), or employing an incompetent teacher (Bad-T); and
* **Knowledge Retention on \(D_{r}\) (Retain)**: the key question of how to preserve knowledge from \(D_{r}\), which is typically achieved by treating the original model \(f\) as the teacher.

These elements combine differently across methods, influencing both the teacher's role on \(D_{f}\) and \(D_{r}\), as detailed in Table 2; specifically, (i) and (ii) lead to teacher on \(D_{f}\), and (ii) and (iii) lead to teacher on \(D_{r}\). In Addition, the trainable parameters can be dense or sparse and internal or external. We utilize this taxonomy to categorize common and distinctive design elements in existing methods. This categorization helps in understanding how different unlearning approaches function and enables their transfer and adaptation to new contexts, such as generative tasks.

Extension to generative tasksEven though many unlearning methods are designed for and evaluated on classification tasks, they can be applied to generative tasks with minimal modifications. For example, in case of RandLabel, data pairs \((x,y) D_{f}\) can be altered to \((x,y^{})\) where \(y^{} D_{r},y^{} y\). For Bad-T, the method can be adjusted to match the predictions of each token when measuring the teacher-student divergence.

## 3 Experiments

SetupFor each dataset, we first train the task-specific original model \(f\) long enough with hyperparameter optimization and select the best performing model. This is usually the practice for models deployed for real world applications. For LLM and Text-to-Image generation tasks, we evaluate unlearning from the pretrained models, since they are not fine-tuned for a specific task. In addition, we limit the unlearning time so that it does not exceed the retraining time, otherwise unlearning would not be practical. We repeat all experiments five times with different random seeds to account for stochastic effects. We focus on the following MU models selected based their widespread usage and unique characteristics: NegGrad(Golatkar et al., 2020), RandLabel(Graves et al., 2021), Bad-T(Chundawat et al., 2023), SCRUB (Kurmanji et al., 2023), and SalUn(Fan et al., 2024b). Details on the architectures used can be found in A.3 and the performance of the other MU models will be available on the leaderboard.

### Main Results on Discriminative Tasks

As Figure 2 illustrates, NegGrad typically results in low performance on \(D_{f}\), but severely compromises the knowledge on \(D_{}\) and \(D_{r}\), indicating it is not an effective MU approach. In general, tasks like audio classification, video classification, text summarization and generation consistently challenge existing MU algorithms, potentially due to strong correlations within the data, see Figure 11-12). We report the average performance across all tasks as all metrics range from 0 to 100%.

For image classification on CIFAR-100, Bad-T achieves close-to-random performance on \(D_{f}\) while preserving 40% accuracy on \(D_{}\) and \(D_{r}\). Both RandLabel and SalUn effectively maintain models' capability on downstream test sets but fails to forget the deletion set. The original SalUn paper reported slightly different results, which we hypothesize may be due to the class-balanced sampling strategy and nuanced class hierarchy of CIFAR-100. Interestingly, SCRUB achieves very similar performances on \(D_{}\), \(D_{f}\), and \(D_{r}\), see Figure 7.

**Method** & \)**} & \)**} &  \\  & **KM** & **Corrupt** & **KM** & **Retain** & \\   Exact unlearning & – & – & Loss & \(f\) & Dense, Internal \\  NegGrad(Golatkar et al., 2020) & Loss & Grad & – & – & Dense, Internal \\ RandLabel(Graves et al., 2021) & Loss & Data & Loss & \(f\) & Dense, Internal \\ Bad-T(Chundawat et al., 2023) & Logit & Model & Logit & \(f\) & Dense, Internal \\ SCRUB (Kurmanji et al., 2023) & Loss & Grad & Loss + Rep. & \(f\) & Dense, Internal \\ SalUn(Fan et al., 2024b) & Loss & Data & Loss & \(f\) & Sparse, Internal \\ \(l_{1}\)-sparse FT (Jia et al., 2023) & – & – & Loss & \(f\) & Sparse, Internal \\ MultiDelete (Cheng and Amiri, 2023) & Rep. & Data & Rep. & \(f\) & Dense, Internal \\ EUL (Chen and Yang, 2023) & Loss + Rep. & Grad & Loss + Rep. & \(f\) & Dense, External \\ UL (Jang et al., 2023) & Loss & Grad & – & – & Dense, Internal \\ GNNDelete(Cheng et al., 2023) & Rep. & Data & Loss + Rep. & \(f\) & Dense, External \\ SGA-TAU (Barbulescu and Triantafilou, 2024) & Loss & Grad & – & – & Sparse, Internal \\   

Table 2: Taxonomy of unlearning techniques. Despite different formulations and loss functions, existing approaches can be viewed in a unified teacher-student framework, with three design choices: (i) knowledge measurement (KM), (ii) knowledge corruption on \(D_{f}\) (Corrupt), and (iii) knowledge retention on \(D_{r}\) (Retain). The combination of (i) and (ii) leads to teacher on \(D_{f}\), while combination of (i) and (iii) leads to teacher on \(D_{r}\). For teachers on \(D_{f}\) and \(D_{r}\), Loss represents the expected task loss \(_{(x,y) D} L(f(x),y)\) on \(D_{f}\) and \(D_{r}\). Rep. denotes the KL Divergence of output distribution \(_{(x,y) D}(f^{}(x),f(x))\) on \(D_{f}\) and \(D_{r}\). Trainable parameters are denoted as Dense or Sparse, and Internal or External.

For sentiment classification on IMDB, RandLabel and SalUn show promising results in forget \(D_{f}\) with close-to-random performances, with minimal impact on \(D_{}\). Bad-T and SCRUB also preserve strong performance on \(D_{}\) but fail to unlearn \(D_{f}\). Since IMDB contains strong dataset biases and shortcut features, corrupting the data labels implemented by RandLabel and SalUn seems to be a more effective approach than corrupting gradient, see Figure 8.

For biomedical relation extraction on DDI, RandLabel, SCRUB, SalUn all succeed in forgetting the deletion set \(D_{f}\), with SCRUB slightly impairing test performance more than others. Conversely, Bad-T completely failed to unlearn \(D_{f}\), see Figure 9.

For visual reasoning on NLVR2, RandLabel and SalUn again are successful in unlearning \(D_{f}\), unlike Bad-T and SCRUB, which failed to unlearn \(D_{f}\). However, one potential issue with SalUn is in the excessively low performance, almost close to zero performance, on \(D_{f}\), which may be too low and prone to information leakage. This will be further discussed in SS4, see Figure 10.

The speech keyword spotting on Speech Commands show that none of the existing methods can forget \(D_{f}\) without severely impacting knowledge retention. Either with minimal knowledge removed (NegGrad, RandLabel, SalUn), or resulting in too much performance degradation on \(D_{}\) (Bad-T, SCRUB). This can potentially be due to the correlations between audio waves, for which prior approaches do not have mechanisms to handle, see Figure 11.

For video action recognition on UCF101, all methods maintain original performance on \(D_{}\) and \(D_{r}\), but all fail to forget \(D_{f}\), with 90+% accuracy. This can be attributed to the fact that current video classification methods rely on inter-frame correlation, while existing MU methods lacks mechanisms to remove such information, leading to failed unlearning, see Figure 12.

### Main Results on Generative Tasks

In general, generation tasks present greater challenges for unlearning and evaluation. As Figure 3 shows, for text summarization on SAMSum and text generation on BioFact, existing general MU approaches all fail to achieve unlearning. RandLabel and SalUn has limited influence over all data including \(D_{f}\) and \(D_{r}\), while Bad-T and SCRUB remove knowledge of all data. In addition, we find that NegGrad show very different performance pattern on generative tasks compared to discriminative tasks, with non-random performance when a small portion of examples are deleted, see Figure 13.

For text-to-image generation, we find all methods can effectively reduce the clip score between image-prompt pairs on \(D_{f}\) with limited impact on \(D_{}\) and \(D_{r}\), see Figure 15). To ensure the generated images are not from the orginal classes, we use a trained image classifier to classify the samples in \(D_{f}\). SalUn outperforms all other approaches by 5.1 in accuracy on average, see Table 4. Additional results on training time and membership inference attack are shown in Appendix A.5.

Figure 3: Overall average performance across all generative tasks.

Figure 2: Overall average accuracy across all discriminative tasks.

Discussion and Analysis

What is the deletion capacity of each method?We define _deletion capacity_ as the amount of data a model can forget without degrading performance on \(D_{T}\). RandLabel and SalUn have relatively larger deletion capacity than SCRUB, while Bad-T has the smallest capacity. These results suggest that task loss is a potentially better way of knowledge measurement than matching logits in Bad-T. Another reason is the computation cost of Bad-T restricts its capability of forgetting more samples. Furthermore, we find that the deletion capacity of the same MU method varies across different tasks, modalities, and network architectures. SalUn has large deletion capacity on image and text classification datasets, but much smaller capacity on multimodal tasks, shown in Figures 7-10.

Does unlearning amplify biases?A less explored aspect of unlearning in existing works is does MU amplify or restrict the model's dependence on biases in MU. To answer these questions, we evaluate the zero-shot transfer performance of \(f\) and \(f^{}\) on test examples that are adversarial or from shifted distributions, specifically, CIFAR100-C (Hendrycks and Dietterich, 2019) for CIFAR100, Rotten Tomatoes (Pang and Lee, 2005) for IMDB, extra test set from (Suhr et al., 2019) for NLVR2, UCF101-DS (Schiappa et al., 2023) for UCF101, and XSum (Narayan et al., 2018) for SAMSum. The results show that NegGrad significantly affects models' capability on transfer test sets, while other methods we evaluated do not strongly influence models' dependence on biases, see Figure 4.

Does unlearning follow scaling laws?Scaling is a critical aspect to understand the limitations of an unlearning method. The results show that RandLabel, SalUn, NegGrad, and Bad-T have a better predictability of performance on \(D_{f}\), given the amount of compute (FLOS), while the performance of SCRUB depends on the switch between max steps and min steps. In addition, NegGrad and SCRUB have faster speed in decreasing performance on \(D_{f}\). Bad-T has relatively slower speed, due to the fact that it simultaneously iterate through \(D_{f}\) and \(D_{r}\) at every optimization step, which leads to more computing cost than other methods.

Does unlearning benefit from curriculum learning?The effect of curriculum learning (Bengio et al., 2009; Sukhbaatar et al., 2018) (CL) in MU is an overlooked aspect in existing literature. MU models often sample batches randomly with no specific order and treat inputs with equal weight. We experiment with one common curriculum learning approach SuperLoss (Castells et al., 2020), which implements the core principle of curriculum learning. Specifically, it weights training losses based on sample difficulty, weighing down the contribution of samples with large training loss (potentially hard examples) to allow the model to learn from easier samples. As through training, the loss of the hard examples decreases, hard examples are gradually introduced for training. The results show that overall SuperLoss results in a slightly larger performance on \(D_{f}\), indicating CL is likely to help model forget less. One exception is that on Speech Commands, CL outperforms Non-CL by 25.4 in accuracy. We defer further experiments with other CL techniques to future work.

Does unlearning benefit from parameter-efficient fine-tuning (PEFT)?Despite recent advancements of parameter-efficient fine-tuning (He et al., 2022), most MU methods optimize the entire network parameters, which results in significant cost. Only a few approaches have adopted a parameter-efficient strategy (Chen and Yang, 2023; Cheng et al., 2023). Since PEFT only updates a small portion of the model, it is intuitive to assume that PEFT can maximally retain the knowledge from the original model without compromising unlearning. To validate this hypothesis in the context of MU, we experiment with LoRA (Hu et al., 2022). The results show that most methods can benefit from PEFT, where the performance gap on \(D_{f}\) is less than 10 points in accuracy. However, the amount of trainable parameters in MU is much larger than that of fine-tuning. As the trainable parameters are less than 50% of the original size, the performance on \(D_{f}\) is close to that of \(D_{r}\). Such performance persists even with larger learning rate and longer training time, indicating unlearning \(D_{f}\) cannot be achieved below the threshold of 50%, see Figure 6. This minimum trainable threshold (Hu et al., 2022; Su et al., 2023) is much larger than non-MU tasks with as low as a few thousand parameters, since selective knowledge removal is a more challenging task. Meanwhile, the performances on \(D_{}\) and \(D_{r}\) are not affected, indicating LoRA forgets less and slower in MU.

Which design choices are effective for machine unlearning?For discriminitive tasks, corrupting gradient is a less effective approach compared to corrupting data (RandLabel, etc.) and model (Bad-T). Corrupting gradients can discard learned knowledge and therefore we suggest not usingit in isolation without other constraints. However, this approach has a greater potential for generative tasks. It is generally more effective to simultaneously iterate through \(D_{f}\) and \(D_{r}\) (Bad-T) or randomly iterate through the training set (RandLabel, SalUn), than to clearly separate \(D_{f}\) and \(D_{r}\). For example, SCRUB takes a few passes on \(D_{f}\) to forget the deletion set before learning on \(D_{r}\) to retain non-deleted data. On the other hand, simultaneous processing of \(D_{f}\) and \(D_{r}\) lead to higher computational cost.Using representation or task loss as a measurement of learned knowledge can adapt to both discriminative and generative tasks, while using logits (Bad-T) has a much more restricted application merely on classification tasks.

Is a lower performance on \(D_{f}\) always better?Previous works focus on driving the performance on \(D_{f}\) to as low as possible. We suggest that excessively low score on \(D_{f}\) might reveal information or indicate its existence, which may be taken advantaged by adversary. Moreover, unlearning does not mean a model should completely lose its capability of handling specific samples in \(D_{f}\). Instead, a balanced approach where the unlearned model maintains a reasonably low performance on \(D_{f}\) is preferable. Recent works are focusing on this direction, such as zero-retrain evaluation (Chundawat et al., 2023), knowledge gap on \(D_{f}|D_{T}\)(Wang et al., 2023; Cheng et al., 2023). We defer further analysis on the desirable performance of \(D_{f}\) to future work.

## 5 Conclusion

ConclusionWe propose MU-Bench, the first comprehensive machine unlearning (MU) benchmark that spans various tasks and data modalities, including those previously unexplored in MU. We introduce a unified taxonomy of existing MU works, which highlights their unique design choices and establishes connections between them. We also conduct extensive experiments with commonly-used and recent MU algorithms using MU-Bench, discovering that audio and video tasks require more focused development of MU techniques. In addition, we explore several overlooked yet crucial aspects of unlearning, such as bias, parameter-efficiency, curriculum learning, and deletion capacity. Finally, we develop an open-source package of MU-Bench to provide unified data splits, and implement a suite of commonly-used MU methods and their design choices to enable fast experimentation and fair comparisons across MU methods. The package along with a leaderboard are structured to easily incorporate new datasets and tasks in future. We will continue to expand MU-Bench by incorporating more datasets and tasks.

Future WorksThere are several venues for future work including: _(a): MU methods for under-investigated tasks and modalities_: existing unlearning methods are primarily developed for text or image data types. Our experiments on MU-Bench show that current models severely underperform in audio and video contexts. A promising area of research is to extend MU to these data modalities and tasks through focused development of MU techniques to ensures comprehensive MU capability. _(b) Efficient MU methods_: existing unlearning methods require extensive training, either tuning the entire model or training on large portions of the dataset. Meanwhile, most methods do not benefit from PEFT. Future research can focus on developing more efficient MU methods using approaches like zero-shot methods, sparse methods, and curriculum learning methods to speed up the unlearning process. _(c) Explainability_: understanding why certain samples are more easily forgotten than others could shed light on inner working of MU methods and improve MU performance. Therefore, investigating the complexities of samples that affect their retention or deletion is a promising area of research. _(d) Evaluation_: current evaluation of MU is still in its early stage and demands more attention. Refining current evaluation strategies and metrics will be crucial for advancing the field. _(e) Theoretical guarantee of MU_: most current non-DP-based MU approaches do not provide theoretical guarantees. A critical future directions is to develop theoretical frameworks that provide bounds performance bounds for MU.

LimitationsWhile our work marks significant progress, it has the following limitations: _(a): Not all MU algorithms are evaluated_: due to the significant cost and resource constraints, we focused on a selection of recent, well-performing and representative approaches rather than an exhaustive examination of all MU models. _(b): Breadths of experiments_. Our investigation into parameter-efficient fine-tuning and curriculum learning were limited to specific methods like LoRA and SuperLoss, though other more effective approaches exist. _(c): Not all tasks are included_. There are some relevant tasks that are not currently included in MU-Bench, such as those related to graphs, recommendation, or retrieval tasks. We plan to expand the range of tasks and datasets in ongoing development of MU-Bench.

## Broader Impact Statement

Our work lays a foundation for fair and consistent evaluation of machine unlearning techniques and its applications, including the Right To Be Forgotten (RTBF) in AI models, which ensures the protection of personal data and the integrity of AI systems.