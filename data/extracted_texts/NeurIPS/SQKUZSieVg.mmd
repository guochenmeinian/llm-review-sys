# Challenges in Explaining Representational Similarity through Identifiability

Beatrix M. G. Nielsen\({}^{*,1}\) Luigi Gresele\({}^{1,2}\) Andrea Dittadi\({}^{1,3,4,5,6}\)

\({}^{1}\) Technical University of Denmark, \({}^{2}\) University of Copenhagen, \({}^{3}\) Helmholtz AI, \({}^{4}\) TU Munich,

\({}^{5}\) Munich Center for Machine Learning (MCML), \({}^{6}\) MPI for Intelligent Systems, Tubingen

Corresponding author: bmpi@dtu.dk.Equal advising.

###### Abstract

The phenomenon of different deep learning models producing similar data representations has garnered significant attention, raising the question of why such representational similarity occurs. Identifiability theory offers a partial explanation: for a broad class of discriminative models, including many popular in representation learning, those assigning equal likelihood to the observations yield representations that are equal up to a linear transformation, if a suitable diversity condition holds. In this work, we identify two key challenges in applying identifiability theory to explain representational similarity. First, the assumption of exact likelihood equality is rarely satisfied by practical models trained with different initializations. To address this, we describe how the representations of two models deviate from being linear transformations of each other, based on their difference in log-likelihoods. Second, we demonstrate that even models with similar and near-optimal loss values can produce highly dissimilar representations due to an underappreciated difference between loss and likelihood. Our findings highlight key open questions and point to future research directions for advancing the theoretical understanding of representational similarity.

## 1 Introduction

There is ample evidence of similarity in the representations learned by different deep learning models (Lenc and Vedaldi, 2015; Kornblith et al., 2019; Bansal et al., 2021; Ding et al., 2021), which has given rise to conjectures on why the phenomenon occurs (Huh et al., 2024; Teney et al., 2024) as well as work on exploiting it (Moschella et al., 2022; Cannistraci et al., 2023; Maiorca et al., 2024), for example for model stitching (Lenc and Vedaldi, 2015; Bansal et al., 2021). One instance of representational similarity, perhaps the simplest and most fundamental one, is when different models trained on the same data and with the same learning objective produce representations that are equal up to a simple transformation, for example linear.3 For highly nonlinear models, it is not obvious that this should occur: in fact, work on nonlinear independent component analysis (ICA) offers many examples where different models, despite assigning equal likelihood to the data, yield representations related by nonlinear transformations (see, e.g., (Hyvarinen and Pajunen, 1999)). However, Roeder et al. (2021) and Khemakhem et al. (2020) show that for nonlinear models in a broad discriminative class--including several models popular in representation learning, those which assign equal likelihood to the observations extract representations which _are_ linear transformationsof one another, provided a suitable _diversity_ condition is satisfied. One might therefore be inclined to use these findings to account for some of the empirical observations of representational similarity.

In this work, we identify two key challenges in using existing identifiability theory to explain representational similarity. Firstly, identifiability results require equality of the likelihoods of the considered models as a premise: in practice, this assumption is rarely fulfilled for trained models, since even initializing with different random seeds will typically result in models with likelihoods which might be close, but not equal. It would therefore be desirable to relax the assumption of equality and explore whether the results in (Roeder et al., 2021; Khemakhem et al., 2020) can be extended to prove _approximate_ representational similarity4 for models achieving _close to equal_ likelihoods. Furthermore, as we will show in Section 4, minimizing the loss is not the same as minimizing the difference of log-likelihoods, which means that models with close to optimal loss can have representations which are very dissimilar. By pointing out these challenges, we hope to inspire further research on the theoretical underpinnings of representational similarity.

Our contributions can be summarized as follows:

* In Section 3, we prove a relation between the difference of log-likelihoods entailed by two models and their extracted representations. This generalizes the analysis in (Roeder et al., 2021; Khemakhem et al., 2020), which requires vanishing log-likelihood difference.
* In Section 4, we introduce a construction showing that it is not sufficient that both considered models have close to optimal (zero) loss for their representations to become similar.

## 2 Preliminaries

Model Class.We will consider a model class defined by the probability of a label, \(\), given an input, \(\), from a domain, \(\), and a collection of possible targets, \(\), where we must have \(\).

\[p_{}(|,)=_{ }()^{}_{}())}{_{ ^{}}(_{}()^{ }_{}(^{}))}\] (1)

where \(\) is the parameters of the model. We will often write "the model \(\)" as short for the model with parameters \(\). For two models, \(^{*}\) and \(^{}\), we write \(^{*}\) and \(^{}\) for their embedding functions and \(^{*}\) and \(^{}\) for their unembedding functions, following the terminology in (Park et al., 2023). In the following, we let the codomain of \(\) and \(\) be \(^{M}\). So the embedding functions \(:^{M}\) take inputs into \(^{M}\), and the unembedding functions \(:^{M}\) take the labels into \(^{M}\). This model class is the same as the one considered in (Roeder et al., 2021): it is particularly interesting because many common models and pre-training objectives can be written in this form, for example autoregressive language models and a common supervised classification objective. For more examples, see (Roeder et al., 2021, Section 4 & Appendix D).

Diversity Condition.We define the diversity condition like in (Khemakhem et al., 2020).

**Definition 1** (Diversity condition).: We say that a model \(^{}\) from the model class Eq. (1) satisfies the diversity condition for \(^{}\) if there exists \(_{0},...,_{M}\) such that the \(M\) vectors \(\{^{}(_{i})-^{}(_{0})\}_{i =1}^{M}\) are linearly independent. Similarly, we say that a model satisfies the diversity condition for \(^{}\) if there exists \(_{0},...,_{M}\) such that the \(M\) vectors \(\{^{}(_{i})-^{}(_{0})\}_{i =1}^{M}\) are linearly independent.

This assumption, or variations thereof, plays a crucial role in the identifiability results in, e.g., (Roeder et al., 2021), (Khemakhem et al., 2020) and (Lachapelle et al., 2023).

Identifiability Result.For our purposes, the identifiability results found in (Khemakhem et al., 2020), (Roeder et al., 2021), and (Lachapelle et al., 2023) can be summarized in the following statement (a detailed proof can be found in Appendix B).

**Theorem 1**.: _Let \(^{*}\) be a model of the form in Eq. (1) and satisfying the diversity condition on \(^{*}\) and \(^{*}\). Assume \(^{}\) is another model of the same form. Then_

\[p_{^{*}}=p_{^{}}^{*}_{L} ^{},\] (2)_where following (Lachapelle et al., 2023), the equivalence relation is defined by_

\[^{*}_{L}^{}^{*}( )=^{}()\\ ^{*}()=^{-}^{}() +,\] (3)

_where \(\) is an invertible matrix and \(\) is a vector._

Measuring Similarity with Canonical Correlation Analysis.It is possible to define a measure of how close two sets of vectors are to being linear transformations of each other, based on Canonical Correlation Analysis (CCA) (Hotelling, 1936). Given two random vectors \(^{N}\) and \(^{M}\), CCA finds vectors \(_{k}\) and \(_{k}\), where \(k(N,M)\), such that the Pearson correlation \(_{k}=(_{k}^{},_{k}^{ })\) is maximized, with the constraint that \(_{k}^{},_{j}^{}\) and \(_{k}^{},_{j}^{}\) are linearly independent for \(k j\). In our setting, we would like to measure how close the embeddings and unembeddings from two models \(^{*},^{}\) are to being linear transformations of each other. Our vectors will thus be \(^{*}(),^{}()\) or \(^{*}(),^{*}()\) for inputs \(\) and labels \(\) (thereby \(N=M\)). As our similarity score, we will use the mean canonical correlation, as in (Klabunde et al., 2023):

\[m_{}(,)=_{k}_{k}\]

If \(m_{}(,)=1\), \(\) and \(\) are linear transformations of each other (see Appendix A).

The Difference Between Loss and Likelihood.The distinction between _loss_ and _likelihood_ is crucial, since we would like to understand whether different models achieving small _loss_ (i.e., close to optimal) will extract similar representations. We say that two models, \(^{*},^{}\), have equal _likelihood_ if \(p_{^{*}}(|,)=p_{^{ }}(|,)\) for all \(\) and \(\). When training such models, the _loss_ we optimize is \(_{(,)}[-( p_{}(|,))]\), where \((,)\) is an input-label pair from the data distribution \(q_{}\). Note that the identifiability results by, e.g., Roeder et al. (2021) and Khemakhem et al. (2020) require that the _likelihood_ entailed by the two models, \(^{*}\) and \(^{}\), should be equal. However, the observed data distribution \(q_{}(|)\) will likely not include all possible combinations of inputs values \(\) and label values in \(\). For example, in the case of an autoregressive language model, given a sequence of words \(\), many among the potential next token-candidates \(\) would result in nonsensical sentences, thereby having a low probability of appearing in the training data. Because of this, for each \(\), minimizing the loss does not uniquely constrain the conditional likelihood for all targets in \(\). In short, equal loss does not mean equal likelihood. One might nevertheless think that if the loss is close enough to optimal, and there is very little density assigned to improbable targets, we would still get models with representations which are _close_ to being linear transformations of each other. However, as we show in Section 4, this is not the case.

## 3 Close-to-Identifiability Result

As a first step towards generalizing the results in (Roeder et al., 2021; Khemakhem et al., 2020), we prove that for models as in Equation (1), it holds that the representations extracted by a model \(^{*}\) can be written as a linear transformation of those from another model, \(^{}\), plus an error term which can be seen as the non-linear part of the relationship between the functions. When the likelihoods of the two models are equal, the error term vanishes, and we recover the results of Theorem 1.

**Theorem 2**.: _Let \(^{*}\) and \(^{}\) be two models, and let \(^{*}\) satisfy the diversity condition (Definition 1) for both \(^{*}\) and \(^{*}\). Let \(_{0},...,_{M}\) be the \(_{i}\)s from the diversity condition on \(^{*}\). Let \(^{*}\) be the matrix with columns \(^{*}(_{i})-^{*}(_{0})\) and \(^{}\) the matrix with columns \(^{}(_{i})-^{}(_{0})\). Then_

\[^{*}() =^{}()+_{^{*}}()\] (4) \[^{*}() =^{}()+_{^{*}}()\] (5)

_where \(=^{*-}^{}\), \(_{^{*}}()=^{*-}_{}()\), and \(_{}()\) is a vector function with each entry equal to \(_{i}()=^{*}()^{} ^{*}(_{i})-^{}()^{}^{}( _{i})+^{}()^{}^{}( _{0})-^{*}()^{}^{*}(_{0})\). So \(_{}()\) is a function of \(\) using the \(_{i}\)s from the diversity condition on \(^{*}\). \(\) will be a similar product of matrices, only using the diversity condition on \(^{*}\). Also, \(_{^{*}}()\) will contain a \(_{}()\) which is a function of \(\) using the \(_{i}\)s from the diversity condition on \(^{*}\)._The proof can be found in Appendix C. A consequence of Theorem 2 is that there is the following relationship between the embeddings \(^{*}(),^{}()\):

\[^{*}()=^{*-}(^{{}^{} }^{}()+_{y}())\] (6)

and a similar one for the unembeddings \(^{*}(),^{}()\). This equation shows that whether we can say that \(^{}()\) is close to being a linear transformation of \(^{*}()\) depends on the relative size of \(_{y}()\) compared to \(^{{}^{}}^{}()\). If \(\|_{y}()\|<<\|^{{}^{}}^ {}()\|\), then the representations will be close to linear transformations of each other. Since \(_{y}()\) and \(_{x}()\) can be expressed in terms of differences of log-likelihoods entailed by the two models (see Appendix C), we see that they will be small if the models assign likelihoods which are close to equal to the observations. In particular, we see that for the embedding representations to be close, we need log-likelihoods to be close for all \(\) and for all the \(_{i}\)'s from the diversity condition on \(\). Conversely, for the \(\) representations to be close, we need log-likelihoods to be close for all \(\) and for all the \(_{i}\)'s from the diversity condition on \(\). If the distributions for the models are equal, \(_{y}()\) and \(_{x}()\) will be zero, and we recover the identifiability result of Theorem 1. However, optimizing \(p^{*}(|,)\) and \(p^{}(|,)\) for the correct label, \(_{i}\), is not enough to make this difference of log-likelihoods small. Below we present an example of this.

## 4 Example of Close to Zero Loss where Representations are Dissimilar

In this example, we have \(M=2\) and a classification task with four labels, \(_{0},_{1},_{2},_{3}\). The example relies on the fact that if we fix non-zero angles between the unembedding vectors, and we let the embedding representations be closer to the correct label in terms of angle than the incorrect ones, then we can make the likelihood of the correct label arbitrarily close to \(1\) by only changing the lengths of the unembedding vectors (see Appendix E, and Appendix D for more details).

For the first model \(^{}\), we let the the angle between the unembeddings be very small, and the lengths of the unembeddings be very large. We generate our embedding vectors such that they have Euclidean norms larger than \(1\) and such that they are very close in terms of angle to the unembeddings with the correct label (see Fig. 1). For the second model, \(^{*}\), we spread out the unembedding representations such that three are on the axes and one is slightly off. For the embedding representations, we place them such that they are closer in terms of angle to the unembedding with the correct label, but more spread out than the ones from model \(^{}\) (see Fig. 1).

We can now calculate the negative log-likelihood (\(NLL\)) for these two models using Eq. (1). For model \(^{}\), we get \(NLL^{}\!\!9\!\!10^{-10}\) and for model \(^{*}\), we get \(NLL^{*} 7 10^{-10}\). In fact, for the correct labels, we get a small difference in log-likelihood for all datapoints. The maximal difference for the two models is \(8 10^{-7}\). As mentioned above, we could make this loss arbitrarily small, by increasing the lengths of the unembeddings. Now both of these models have close to zero loss, and their loss thus is close to equal: however, they are very far from being linear transformations of each other. For example, the mean canonical correlation between the embedding representations is \(m_{}(^{}(),^{*}())  0.42\), which is very far from the value of \(1\) which would indicate a perfect linear relationship. See Appendix D for further insights on this value of \(m_{}\) and the degree of dissimilarity it indicates. It is possible to construct an example with even smaller \(m_{}\), for example by making the angles smaller and the lengths longer for \(^{}()\) for model \(^{}\), while keeping the other model \(^{*}\) as it is.

Conclusion

We showed that the representations (embeddings and unembeddings) extracted by two models of the form in Eq. (1) will be close to being linear transformations of each other for all \(\) and \(\), if the log-likelihoods entailed by the two models are close for all \(\) and \(\). We also introduced a construction to show that, for the representations of two models to be close to equivalent, it is not sufficient that the losses of both models are close to each other and small. These results point to interesting questions for future research: for example, how a non-vanishing difference in log-likelihood can be connected to a measurement of representational similarity; and under what additional assumptions similarity should be expected if the difference in _expected log-likelihood_ or in _loss_ is non-vanishing.

#### Acknowledgments

The authors would like to thank Emanuele Marconato, Julius von Kugelgen and Adrian Javaloy for valuable discussions. This work was supported by the Danish Pioneer Centre for AI, DNRF grant number P1. L.G. was supported by the Danish Data Science Academy (DDSA), Grant ID: 2023-1250.