# Knowledge Circuits in Pretrained Transformers

Yunzhi Yao\({}^{1}\) Ningyu Zhang\({}^{1}\) Zekun Xi\({}^{1}\) Mengru Wang\({}^{1}\)

Ziwen Xu\({}^{1}\) Shumin Deng\({}^{2}\) Huajun Chen\({}^{1,3}\)

\({}^{1}\) Zhejiang University \({}^{2}\) National University of Singapore, NUS-NCS Joint Lab, Singapore

\({}^{3}\) Zhejiang Key Laboratory of Big Data Intelligent Computing

{yyztodd,zhangingyu}@zju.edu.cn

Corresponding Author.Code and data are available in https://github.com/zjunlp/KnowledgeCircuits.

###### Abstract

The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, have allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuits hold potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing1.

## 1 Introduction

_"Knowledge is power, and when embodied in the form of new technical inventions and mechanical discoveries it is the force that drives history."_, Bacon's words are vividly re-enacted in the era of Large Language Models (LLMs) , as we witness their immense power in reshaping human society and redefining our understanding of machine intelligence. One thing that cannot be denied is that knowledge encapsulated within these models empowers their capabilities in reasoning, perceiving the world, and engaging in human-like communication. Nevertheless, these powerful models are not without their flaws. They still struggle with issues such as hallucinations , unsafe norms , and offensive behaviors  and these problems are exacerbated by the enigmatic internal mechanisms of knowledge storage within language models.

Recently, the research community has devoted significant efforts to unraveling the knowledge storage mechanisms of these models. Various studies  have been conducted to shed light on this intricate process, aiming to enhance our understanding and improve the safety and reliability of language models. The main finding in previous work is that knowledge may primarily stored in the Multilayer Perceptrons (MLPs) of Transformer-based language models. These MLPs function as a key-value neural memory, with knowledge being stored in what are termed "knowledge neurons" (KN). Based on these findings, researchers conduct Knowledge Editing  to update the language models' inaccurate facts, bias and unsafe content in their parametric space. Despite theinitial success of these methods, there are still limitations, such as poor generalization, severe side effects, and failure to effectively utilize edited knowledge [20; 21], which motivate us to re-think previous approaches for interpreting knowledge storage in language models. Note that previous works treat the knowledge blocks as isolated components following the Restorative Theory , often focusing on identifying the specific blocks that store particular knowledge. Several works [23; 24] have proposed that different types of knowledge are often located in the same areas, suggesting that the current KN thesis may be an oversimplification.

To this end, instead of solely pinpointing tiny regions where the knowledge expressed can be localized, we aim to explore the cooperation between different components in Transformers like attention heads, MLPs, and embeddings, to understand how the language model stores and expresses the knowledge. Here, we introduce a new perspective: **Knowledge Circuits**, a critical subgraph in the language model to view the knowledge mechanism of Transformers. Note that Circuit, as a subgraph in the computation graph, has gained ever-growing attention in the mechanistic interpretability field . Previous work [26; 27] has found several important circuits for specific tasks like Indirect Object Identification and Color Object Identification. These tasks necessitate the model to search the preceding context for a matching token and copy it into the next token prediction. In this work, we aim to construct knowledge circuits that require the model to utilize stored knowledge for making predictions. Our goal is to better unveil implicit neural knowledge representations, elucidate the internal mechanisms for knowledge editing, and interpret more complex behaviors of language models. Specifically, we leverage factual recall tasks and conduct experiments across various domains, including factual, social bias, linguistic, and commonsense knowledge. We utilize GPT-2  and TinyLLAMA  to explore the potential knowledge representations and utilization mechanisms in these models. As shown in Figure 1 (a), we construct knowledge circuits associated with various expressions of knowledge using the existing knowledge stored in the language model. Through those discovered knowledge circuits, we find many interesting phenomena and conclusions as follows:

**Knowledge circuits unveil implicit neural knowledge representations.** We find that even when the knowledge circuits are used independently, the language model can recall related knowledge with a significant portion of its overall performance, demonstrating the effectiveness of those discovered knowledge representations (circuits). We also delve into specific pieces of knowledge and analyze the information flow within their respective circuits, indicating that the language model tends to aggregate knowledge in the earlier to middle layers and further enhances this information in the later layers. We further uncover several special components (e.g., mover heads and relation heads)

Figure 1: Knowledge circuit obtained from _“The official language of France is French”_ in GPT2-Medium. Left: a simplified circuit and the whole circuit is in Figure 9 in Appendix. We use \(\) to skip some complex connections between nodes. Here, L15H0 means the first attention head in the 15th layer and MLP12 means the multi-perception layer in the 13th layer. Right: the behavior of several special heads. The matrix on the left is the attention pattern of each attention head and the right heapmap demonstrates the output logits of the hean by mapping to the vocabulary space.

in transferring information to the final token position and capturing relational information from the context (Figure 1 (b)).

**Knowledge circuits elucidate internal mechanisms for knowledge editing.** We conduct experiments to evaluate the impact of current knowledge editing methods on the language models' original knowledge circuits. Empirically, we observe that ROME  tends to incorporate edited information primarily at the edited layer. Subsequent mover heads (Appendix B.2) then transport this information to the residual stream of the last token. Conversely, during fine-tuning, the edited token is directly integrated into the language model, exerting a dominant influence on subsequent predictions.

**Knowledge circuits facilitate interpreting language model behaviors.** We further utilize the knowledge circuits to interpret language model behaviors, such as **hallucination** and **in-context learning**. We observe that when hallucination occurs, the language model fails to correctly transfer knowledge to the final token in the earlier layers. This is evident as the knowledge circuit lacks an effective "mover" head, or the mover head selects incorrect information. Additionally, we notice that several new attention heads emerge in the knowledge circuit during in-context learning.

## 2 Background: Circuit Theory

### Preliminaries

In the context of neural network interpretability, a circuit can be conceptualized as a human-interpretable subgraph that is dedicated to executing specific tasks within a neural network model . When we visualize a neural network model as a connected directed acyclic graph (DAG), denoted as \(\), the individual nodes represent the various components involved in the forward pass, such as neurons, attention heads, and embeddings. The edges symbolize the interactions between these components, including residual connections, attention mechanisms, and projections. A circuit, represented as \(\), emerges as a significant subgraph of \(\) that is responsible for particular behaviors or functionalities. In this paper, we focus on the Transformer decoder architecture to conduct our experiments. The residual stream of Transformers has been demonstrated to be a valuable tool for mechanistic interpretability in recent works . The Transformer architecture typically starts with token embeddings, followed by a sequence of "residual blocks" and concludes with a token unembedding. Each residual block comprises an attention layer and an MLP layer, both of which "read" their input from the residual stream (via a linear projection) and "write" their output back to the residual stream through an additive projection. We can consider an attention head \(A_{l,j}\) (the \(j\)th attention head in layer \(l\)) as operating on the residual stream from the previous layer, \(R_{l-1}\). Given that \(R_{0}=I\) (where \(I\) represents the input embeddings), we can reinterpret attention head \(A_{l,j}\) as processing the cumulative output of all previous attention heads and MLPs and input embedding, treating each node in the previous layers as separate input arguments. Similarly, an MLP node \(M_{l}\) can be seen as operating on the cumulative output of all previous attention heads and MLPs and input embedding, and the output node \(O\) operates on the sum of the input embeddings and the outputs of all attention heads and MLPs. The following equations represent the residual connections in the Transformer model, where \(R_{l}\) is the residual stream at layer \(l\), and \(_{l}^{A}\) and \(_{l}^{M}\) are the inputs to the attention and MLP layers, respectively:

\[R_{l}=R_{l-1}+_{j}A_{l,j}+M_{l},R_{0}=I\]

\[_{l}^{A}=I+_{l^{}<l}(M_{l^{}}+_{j^{ }}A_{l^{},j^{}})\]

\[_{l}^{M}=I+_{l^{}<l}M_{i^{}}+_{l^{} i }_{j^{}}A_{l^{},j^{}}\]

The computational graph \(\) of the Transformer represents the interactions between attention heads and MLPs. The nodes in \(\) encompass the input embedding \(I\), attention heads \(A_{l,j}\), MLPs \(M_{l}\), and the output node \(O\), denoted as \(N=\{I,A_{l,j},M_{l},O\}\). The edges in the model represent the connections between these nodes, \(E=\{(n_{x},n_{y}),n_{x},n_{y} N\}\). A circuit \(\) is meticulously constructed to govern specific behaviors within the model, comprising a selection of nodes \(N_{}\) and edges \(E_{}\) that are critical to the successful execution of the tasks at hand, expressed as \(=<N_{},E_{}>\).

### Circuit Discovery

To identify circuits within a language model, a key approach is to examine the model's casual mediation by systematically altering the model's edges and nodes to observe the effects on performance [32; 34; 35]. The underlying principle is that critical edges or nodes are those whose removal results in a notable decline in the model's predictive capabilities. Since the edges in the model's computational graph represent the dependencies between nodes, we can simulate the absence of a particular node-to-node dependency by ablating an edge in the graph. For example, ablating an edge from \(A_{i^{},j^{}}\) to \(A_{i,j}\) involves replacing the contribution of \(A_{i^{},j^{}}\) in the input to attention head \(A_{i,j}\) with zero (in the case of zero ablation) or with the mean value of head \(A_{i^{},j^{}}\) (in the case of mean ablation). The process of identifying critical edges or nodes through ablation can be broken down into the following steps: i) Overwrite the value of the edge \((n_{x},n_{y})\) with a corrupted value (either zero or mean ablation), ii) Perform a forward pass through the model with the altered graph, iii) Compare the output values of the modified model with those of the original model using a chosen metric \(S\) (Details in Eq.1 ). If the performance change is below a predefined threshold \(\), we can consider the edge non-critical and remove it to obtain a new subgraph \(/(n_{x},n_{y})\). In addition to ablation-based methods, recent works have also explored the use of sparse auto-encoders [36; 37] to identify circuits within language models. This approach involves training an auto-encoder to learn a sparse representation of the model's internal structure, which can help reveal the underlying circuitry responsible for specific behaviors or functionalities.

## 3 Knowledge Circuits Discovery in Transformers

### Knowledge Circuits Construction

Unlike previous work [12; 18], which managed to find out the specific areas that store knowledge, we pay extra heed to the information flow that activates subsequent knowledge for answering questions. Similar to [38; 26], we write language model as a graph consisting of the input, the output, attention heads, and MLPs by considering a "residual rewrite" of the model's computational structure. For example, this residual rewrite gives us a nearly-dense graph in GPT2-medium: one between every pair of (attention head, MLP, input, and output) nodes, except for attention heads in the same layer, which do not communicate with each other. In our paper, we concentrate on the task of answering factual open-domain questions, where the goal is to predict a target entity \(o\) given a subject-relation pair \((s,r)\). A knowledge triplet \(k=(s,r,o)\) is often presented to the model in the form of a natural language prompt for next token prediction (e.g., _"The official language of France is_ ). The model \(\) is expected to generate the target entity, which is consistent with the language model's pretraining format. To identify the circuit that is critical for predicting the target entity \(o\) for a given subject-relation pair \((s,r)\), we ablate each special edge \(e_{i}=(n_{x},n_{y})\) in the computation graph \(\). We then measure the impact of ablating the edge (**zero ablation** in our implementation) on the model's performance using the MatchNLL loss  for the target \(o\):

\[S(e_{i})=((o|(s,r)))-(/e_{i}(o|(s,r)))\] (1)

If the score \(S(e_{i})\) is less than the predefined threshold \(\), we consider the edge to be non-critical and remove it from the computation graph, updating the temporary circuit \(_{temp}/e_{i}\). We first sort the graph by topological rank following Conny et al.  and traverse all edges in this manner, We derive a circuit \(_{k}\) that contributes to representing the knowledge necessary to answer the factual question:

\[_{k}=<N_{k},E_{k}>\] (2)

Here, \(_{k}\) is the circuit for the knowledge triplet \(k\), consisting of the nodes \(N_{k}\) and edges \(E_{k}\) that are essential for predicting the target entity \(o\) given the subject-relation pair \((s,r)\).

### Knowledge Circuits Information Analysis

Once we have identified the knowledge circuit, we delve deeper into the specific roles and behaviors of each node and edge within the computation graph. Our goal is to comprehend the processing and contribution of each node \(n_{i}\) to the functionality of the circuit. Drawing on the methodologies of previous studies [16; 39; 40], we begin by applying layer normalization to the output of each node \(n_{i}\) and then map it into the embedding space. This is achieved by multiplying the layer-normalized output by the unembedding matrix (\(_{U}\)) of the language model: \(_{U}(n_{i})\). This transformation allowsus to inspect how each component writes information to the circuit and how it influences subsequent computational steps. By understanding the nodes' behavior in the circuit, we can better comprehend the circuit's structure and the key points where information is aggregated and disseminated.

### Knowledge Circuits Experimental Settings

Implementations.We conduct experiments on GPT-style models, including GPT-2 medium and large. We also conduct primary experiments on TinyLLaMA  to validate the effectiveness of different architectures. We utilize the Automated Circuit Discovery  toolkit to build a circuit as an initiative of our analysis and leverage transformer lens  to further analyze the results. Specifically, we simply employ the MatchNLL  as the metric to detect the effect of the given node and edge and use **zero ablation** to knock out the specific computation node in the model's computation graph.

Metrics.A discovered knowledge circuit is deemed an accurate representation of a specific area within the transformer's knowledge storage, thus, it should be capable of representing the knowledge independently. Following , we leverage the completeness of a circuit, which refers to its ability to independently reproduce the behavior or predictions of the full model for the relevant tasks. This property is assessed by examining whether the identified subgraph corresponds to the underlying algorithm implemented by the neural network. To evaluate completeness, we first construct the circuit using the validation data \(D_{val}\) for a specific knowledge type and then test its performance on the test split \(D_{test}\) in isolation. By doing so, we can observe any changes in performance compared to the original model. We use the Hit@10 metric to measure the rank of the target entity \(o\) among the top 10 predicted tokens:

\[=_{i=1}^{|V|}(_{o } 10)\] (3)

Here, \(|V|\) represents vocabulary size, and \(_{o}\) is the rank of the target entity \(o\) in predictions.

Dataset.In this work, we focus on the **knowledge that is already stored in the language model**. We utilize the dataset provided by LRE  and consider different kinds of knowledge, including linguistic, commonsense, fact, and bias. We evaluate whether the knowledge is present in the language model's parameters under zero-shot settings using the Hit@10 metric to sample knowledge from the validation set, which is used to construct the knowledge circuit. The data statistics are in Appendix A.

## 4 Knowledge Circuits Unveil Implicit Neural Knowledge Representations

Knowledge Circuits Evaluation.We report the results of GPT2-Medium in Table 1, which indicates that with only less than 10% of the original knowledge circuit's subgraph, the model can

    &  &  & \)} & \)} \\   & & & Original(\(\)) & Circuit(\(\)) & Original(\(\)) & Random & Circuit(\(\)) \\   & Adj Antonym & 573 & 0.80 & 1.00 \(\) & 0.00 & 0.00 & 0.40 \(\) \\  & word first letter & 432 & 1.00 & 0.88 & 0.36 & 0.00 & 0.16 \\  & word last letter & 230 & 1.00 & 0.72 & 0.76 & 0.00 & 0.76 \\   & object superclass & 102 & 1.00 & 0.68 & 0.64 & 0.00 & 0.52 \\  & fruit inside color & 433 & 1.00 & 0.20 & 0.93 & 0.00 & 0.13 \\  & work location & 422 & 1.00 & 0.70 & 0.10 & 0.00 & 0.10 \\   & Capital City & 451 & 1.00 & 1.00 & 0.00 & 0.00 & 0.00 \\  & Landmark country & 278 & 1.00 & 0.60 & 0.16 & 0.00 & 0.36 \(\) \\  & Country Language & 329 & 1.00 & 1.00 & 0.16 & 0.00 & 0.75 \(\) \\  & Person Native Language & 92 & 1.00 & 0.76 & 0.50 & 0.00 & 0.76 \(\) \\   & name religion & 423 & 1.00 & 0.50 & 0.42 & 0.00 & 0.42 \\  & occupation age & 413 & 1.00 & 1.00 & 1.00 & 0.00 & 1.00 \\  & occupation gender & 226 & 1.00 & 0.66 & 1.00 & 0.00 & 0.66 \\  & name birthplace & 276 & 1.00 & 0.57 & 0.07 & 0.00 & 0.57 \(\) \\ 
**Avg** & & & 0.98 & 0.73 & 0.44 & 0.00 & 0.47 \(\) \\   

Table 1: Hit@10 of the Original and Circuit Standalone performance of knowledge circuit in GPT2-Medium. **The result for \(D_{val}\) being 1.0 indicates that we select the knowledge for which the model provides the correct answer to build the circuit.**maintain over 70% of its original performance. Additionally, we compute the random circuits by randomly deciding whether the edge should be removed and making sure the graph is connected. The random circuit is the same size as the circuit we discovered using our method. From the table, we can see that the random circuit failed to maintain the model's performance, which further enhanced the robustness and efficacy of our methods. One of the most fascinating observations is the **performance improvement seen on several test datasets**. For instance, the _Landmark-country_ relation metric increases from 0.16 to 0.36. This suggests that the discovered knowledge circuits may encapsulate the relevant knowledge, and the model's performance on these tasks could have been hindered by noise from other components. We proceed to analyze the layer distribution of the original model \(\) to understand the average percentage of nodes that are activated within the circuit for different knowledge domains. From Figure 2, we observe that attention and MLPs are more active in the lower layers of the network, where the language model processes the input and extracts general information. To gain a more comprehensive view of the information processing, we compute the average \(_{o}\) change of the target token in the \(D_{val}\) across the layers and report the results in Figure 7. This analysis reveals the phenomenon of early decoding , suggesting that by the middle to the latest layers, the target entity is already present in the residual stream, and the subsequent layers in the Transformer are designed to increase the probability of the current token (See discussion in the running example).

Special Components in Knowledge Circuits.From the discovered knowledge circuits, we can find several important attention heads that demonstrate specific behavior, including the **mover head**, **relation head** and **mixture head** (more definitions in Appendix B.2). Mover Head  focuses on the last token of the context and attends to the subject token, functioning as a mover to transfer information, while Relation Head  attends to the relation token in the context and produces some relation-related tokens that would guide the behavior of the following components. We think that these components would be accumulated by the MLP in the model, and the behavior of these special heads will be discussed in the running example part. We list some of these special components in Table 4 in Appendix. The different attention heads are responsible for expressing specific types of knowledge and may be activated by different facts. In our experiments with GPT-2 Medium and GPT-2 Large, we find that knowledge is distributed across several layers' attention heads and MLP matrices, suggesting that the target knowledge appears to have been accumulated throughout the GPT-2 model. Conversely, in TinyLALMA, the special components are more concentrated. As depicted in Figure 7, the rank of the target entity in TinyLALMA experiences a sharp decline around several layers, whereas in the GPT2 model, the decline is more gradual. We hypothesize that this discrepancy may be attributed to the model's knowledge capacity  and warrants further investigation.

A Running Example of Knowledge Circuit.We present a case and analyze the specific behaviors of components within the identified knowledge circuits. In particular, we find some special attention heads in the model such as the mover head and the relation head. We demonstrate the function of these heads in Figure 6 by ablating them from the circuit. Taking the factual knowledge _"The official language of France is French"_ as an example, we visualize the knowledge circuit in Figure 1. To express the information flow within the model more effectively, we have plotted the rank and probability of the target entity \(o\) at each layer when it is mapped into the embedding space, in Figure 3. From this figure, we can see that after MLP 17, the target knowledge emerges as the top token in the residual stream, and after that layer, it undergoes an increased probability. The edges connected to MLP17 are (L14H13 \(\) MLP17), (L14H7 \(\) MLP17), and (L15H0 \(\) MLP17). Here, the L14H13 is a relation head that focuses on the relation token in the context. The output of this head is relation-related tokens such as _"language"_ and _"Language"_. The attention head L14H7 is a mover head that moves the information from the subject position _"France"_ to the last token. Previous work  has introduced this mover head as an _argument parser_, which moves _"France"_ to the last token, and the subsequent MLP conducts a function application to map _"France"_ to _"French"_. An intriguing

Figure 2: The activated circuit component distributions in Layers in GPT2-Medium.

observation is that we can find the output of this head already contains the target answer entity, which significantly contributes to the final output (L14H7 \(\) Output). Also, we see the probability of the subject token in Figure 3 at the last token is nearly zero across these layers. Hence, instead of the _argument parser_ function, we consider this mover head as an _extract head_ proposed by Geva et al. , which aims to extract the related-information from the subject token's position. In the subsequent knowledge editing experiments, we can observe changes in the behavior of these types of heads. Additionally, instead of extraction in the later layers proposed by Geva et al. , we notice a gradual decrease in rank across all early-to-middle layers. The MLP17 combines information from previous tokens and integrates this information to prioritize the target token at the top rank.

Interestingly, upon tracing the information flow to L14H7, we discovered that it is predominantly activated by L7H14, a relation head, and its output features several language tokens, such as _"Arabic"_. We hypothesize that L7H14 may function as a signaling mechanism to activate the associated mover head, but this hypothesis necessitates further investigation to be confirmed. After MLP17, several attention heads, such as L18H14 (a relation head) and L20H6 (a mover head), collaborated to further enhance the final prediction of the target entity.

## 5 Knowledge Circuits Elucidate Internal Mechanisms for Knowledge Editing

In this section, our objective is to evaluate the impact of previous knowledge editing methods and validate the effectiveness of knowledge circuits. We aim to understand why these methods may fail in certain cases and settings, which can also help deepen the understanding of the knowledge circuit.

Single Factual Knowledge Editing.Here, we adopt the ROME method  and FT-M , which aim to edit the MLP layers in the language model. The most important hyper-parameter in knowledge editing is the layer, as the same method's performance varies significantly via the layers. Here, we evaluate the performance of different editing layers and their effectiveness. We compare the knowledge circuits computed by the edited model with the original one, and we present results in Figure 4 and report details in Appendix D. As discussed in the previous part, the early-to-middle layers are the main part of aggregating the target entity \(o\) to the top rank. In the original model, the probability of the target entity _"Intel"_ is nearly zero, and the model fails to elevate it to the top rank in the vocabulary. Editing the model with ROME and FT-M both give us the correct answer but we can view different scenarios for their knowledge circuits. For **ROME**, as the correct information is added to the subject position, we can recognize a **behavior of the Mover Head shifts from copying to extracting the edited information from the subject position**. This information gradually aggregates through the subsequent layers, and by layer 15, _"Intel"_ emerges as the top-ranked entity with its probability increasing significantly. Specially, before editing, the mover head L15H3 attends to the _"controller"_ token and returns _"controller"_ as the output, while in the edited model, the attention head's output moves to the _"Intel"_, which means the model gains the information at the subject space. For FT-M, the edited model tends to directly write the knowledge into the specific component, which would greatly dominate the following component in the model. As shown in Figure 4, the output logits in MLP-0 for _"Intel"_ are more than 10, and it emerges as the top rank in the residual stream directly. This phenomenon can be found in different knowledge types and layers and we report results in Appendix D.2. However, the added knowledge may have the risk of influencing unrelated knowledge. When we test another fact _"Windows server"_, the model still tends to give us the _"Intel"_ answer, demonstrating the overfitting problem. This finding supports previous analysis regarding the correlation between localization and editing , suggesting that edits may not alter the storage but merely add signals into the knowledge circuits.

Multi-hop Factual Knowledge Editing.Multi-hop knowledge editing poses a challenging scenario [20; 21; 46], wherein we edit the model with new knowledge, yet the model struggles to perform reasoning using the edited information. We analyze multi-hop questions in language models [47; 48]

Figure 3: The rank and probability of the target entity \(o\) at both the last subject token and the last token position when unembedding the intermediate layer’s output for the fact _“The official language of France is French”_.

to understand why current editing methods fail in these scenarios. For instance, given the fact (Thierry Mugle, _"home country"_, France), we edit the fact to another country, such as (Thierry Mugle, _"home country"_, France \(\) China). We then assess the model's performance on questions based on the edited knowledge, including _"The official currency of the home country of Thierry Mugle is"_ and _"The capital city of the home country of Thierry Mugle is"_. While the unedited model could correctly answer these questions, we observe that the edited model would provide the answer _"China"_ for subsequent hop reasoning. We find that the mover head in the original multi-hop reasoning circuit initially extracts the second-hop answer but, after editing, extracts _"China"_, demonstrating that the edited information dominantly saturates and influences the circuit. Furthermore, we observe an intriguing phenomenon: **even in the original model's multi-hop reasoning settings, it would directly provide the answer if we remove the context of the first-hop texts** (Details in Appendix C.1). This further confirms the findings that the model relies on relational and subject-related information, regardless of grammatical adherence.

## 6 Knowledge Circuits Facilitate Interpreting Language Model Behaviors

In this Section, our aim is to validate whether the identified knowledge circuits are actually utilized by the model when it employs knowledge. To address this, as shown in Figure 5, we investigate three phenomena: hallucination, in-context learning, and reverse relations (Details in Appendix C.3).

Factual Hallucination.If the knowledge is stored and expressed by the circuit we discovered, we aim to discover what happened when the model gave us the incorrect answer. We focus on factual hallucinations, which occur when the model provides an incorrect target entity for a given subject \(s\) and relation \(r\). In our experiments (Figure 5 and Appendix C.2), we observe that the model fails to move the correct knowledge to the final token in the earlier layers. This failure is evident as the

Figure 4: Different behaviors when we edit the language model. In the original model, we can see the mover head L15H3 actually move the original token _“Controller”_ and other information, while for ROME, we observe the mover head select the correct information _“Intel”_, which means ROME **successfully added the “Intel” to model**. For the FT layer-0 editing, we can find this method **directly write the edited knowledge into edited component**. However, we find these two editing methods would affect other unrelated input _“Windows server is created by?”_

circuit lacks an effective mover head or the mover head selects incorrect information. For instance, in the prompt _"The official currency of Malaysia is called"_, both the correct answer _"Ringgit"_ and the incorrect one _"Malaysian"_ are accumulated before layer 15. However, at layer 16, the mover head L15H10 extracts the erroneous information. Despite a rank drop of the true one in layers 20-22, this is insufficient to correct the previous mistake.

In-Context Learning.Despite storing a vast amount of knowledge, a language model may still provide incorrect answers. However, with demonstrations or examples (based on RAG ), it can quickly generate correct responses. To this end, we focus on the scenario where the model initially provides an incorrect answer but can then produce the correct response upon receiving the appropriate demonstration. We consider the original knowledge circuit and introduce a new knowledge circuit based on the demonstration. Our analysis reveals that, compared to the zero-shot knowledge circuit, several new attention heads appear in the computation graph when the demonstration is incorporated. We show the behavior of these attention heads in Figure 5. We can see these heads mainly focus on the demonstration's context: _"The comparative form of small is smaller"_ and works as the Induction Head  that look back over the sequence for previous instances of the current token and find the token that came after it last time. To better view the function of these heads, we conduct experiments by ablating the newly appeared attention head in the ICL circuit in Table 2. We find that compared to the randomly selected attention head by ablating this attention, the probability drops significantly in the prediction, demonstrating the importance of these identified attention heads. These aligned with previous work where Todd et al.  have identified a concept known as the Function Vector, which represents the average of some key attention heads and provides the task learning ability.

## 7 Related Work

**Knowledge Mechanism of Transformers.** How the language model stores and utilizes knowledge is an ongoing research topic. Previous works find that the MLP in Transformers works as a key-value memory and stores enormous knowledge [12; 15; 14; 18]. As to the relation between entities, Hernandez et al.  observes that facts can be decoded linearly from the enriched residual stream of the subject by mapping the subject entity to the object entity. Instead of viewing the knowledge storage in isolation, Geva et al. , Lv et al. , Yu and Ananiadou  find the knowledge is accumulated during the layers. Regarding knowledge analysis, Bayazit et al.  also attempts to discover critical knowledge in language models. However, they only consider several layers in the

Figure 5: Left: fact hallucination case _“The official currency of Malaysia is called”_, we observe that, **at layer 15, the Mover Head selects incorrect information**. Right: In-context learning case, we notice that **some new heads focusing on the demonstration appear in the knowledge circuit**.

model and use the pruning method, which may overlook the connections between components. More related works can be found in Appendix E.1.

**Manipulate Language Models.** Recently, many works aim to manipulate the language models to make the model aligned with world knowledge or social value norms, such as knowledge editing , machine unlearning  and detoxification . Most of these works are elicited by previous knowledge mechanism findings such as knowledge neuron . They modify the MLP in the LLM  to change the model's behavior based on specific factual knowledge. However, recent works  demonstrate the pivotal role of the attention part in knowledge representation. Hase et al.  also observe that the performance of editing within a layer may not reliably pinpoint the location of the fact. In this paper, we try to manipulate specific knowledge of language models via knowledge circuits, including both MLP and attention components across different layers.

## 8 Conclusion

In this paper, we present a new perspective on knowledge storage based on circuit theory and conduct a preliminary analysis to demonstrate its effectiveness. We found that knowledge circuits in the model are not only responsible for expressing knowledge but can also guide behavior in different settings. We hope these findings can advance our understanding of the knowledge mechanisms of language models and provide insights for better designing and editing language models, enhancing knowledge, and improving reasoning to enhance factuality and alleviate hallucinations.

## Limitations and Broader Impacts

In this work, we employ the causal mediation method to automatically construct circuits tailored to specific knowledge domains. However, this circuit discovery-based patching approach is time-intensive. Contemporary research efforts have introduced more efficient methodologies for modeling information flow . Additionally, alternative techniques for discovering circuits through mask training  and Sparse Auto-Encoders  have been proposed, highlighting diverse facets of circuit behavior within large language models (LLMs). We posit that the field of knowledge circuit discovery holds significant potential for advancement. Furthermore, recent studies  have developed 'circuit breakers' to manage representations associated with potentially harmful outputs. We hope that our approach can contribute to ensuring the safety and privacy of information, thereby fostering the development of trustworthy AI.