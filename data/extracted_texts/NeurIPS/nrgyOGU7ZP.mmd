# SS1: Accelerating Inference with Fast and Expressive Sketch Structured Transform

Kimia Saedi\({}^{1}\)   Aditya Desai\({}^{1}\)   Apoorv Walia\({}^{1}\)   Jihyeong Lee\({}^{2}\)  

Keren Zhou\({}^{2}\)   Anshumali Shrivastava\({}^{1,3}\)

\({}^{1}\)Rice University  \({}^{2}\)George Mason University

\({}^{3}\)Ken Kennedy Institute, ThirdAI Corp., Xmad.ai

{ks152,apd10,aw82,as143}@rice.edu

{jlee436,kzhou6}@gmu.edu

Equal contribution; order chosen by a flip of a coinWork done in sole affiliation with George Mason University

###### Abstract

Tensor multiplication with learned weight matrices is the fundamental building block in deep learning models. These matrices can often be sparsified, decomposed, quantized, or subjected to random parameter sharing without losing accuracy, suggesting the possibility of more efficient transforms. Although many variants of weight matrices exist, unstructured ones are incompatible with modern hardware, slowing inference and training. On the other hand, structured variants often limit expressivity or fail to deliver the promised latency benefits. We present Sketch Structured Transform(SS1), an expressive and GPU-friendly operator that accelerates inference. SS1 leverages parameter sharing in a random yet structured manner to reduce computation while retraining the rich expressive nature of parameter sharing. We confirm empirically that SS1 offers better quality-efficiency tradeoffs than competing variants. Interestingly SS1 can be combined with Quantization to achieve gains unattainable by either method alone, a finding we justify via theoretical analysis. The analysis may be of independent interest. Moreover, existing pre-trained models can be projected onto SS1 and finetuned for efficient deployment. Surprisingly, these projected models can perform reasonably well even without finetuning. Our experiments highlight various applications of the SS1: (a) Training GPT2 and DLRM models from scratch for faster inference. (b) Finetuning projected BERT models for 1.31\(\) faster inference while maintaining GLUE scores. (c) Proof of concept with Llama-3-8b, showing 1.11\(\) faster wall clock inference using projected SS1 layers without finetuning. Our code is open-source.3

## 1 Introduction

Tensor-matrix multiplication is one of the fundamental operations in deep learning models across various domains. Linear transformation are especially crucial in transformer architectures, which form the backbone of foundational models responsible for the advanced capabilities of Large Language Models (LLMs). A significant portion of the computational load in LLMs comes from linear layers. For example, with a batch size of 4 at full sequence length, the MLP workload of even the smallest Llama-3-8B model  involves (32768 \(\) 4096 \(\) 4096) operations in attention and (32768 \(\) 4096 \(\) 14336) in the MLP. Larger models have even larger workloads. Given these demands, finding efficient and expressive alternatives to standard linear layers is a crucial research direction.

Deep learning models often have significant redundancies that can be removed using various techniques. Popular approaches include Sparsification [2; 3; 4; 5; 6], Quantization [2; 7], RandomizedParameter Sharing (RPS) [8; 9; 10], and Low-rank decomposition [11; 3]. Unstructured sparsity, while expressive, is inefficient on modern hardware. Structured sparsity [12; 13; 14] and low-rank are efficient but often compromise quality. Recently, it was shown in  that sparsity is theoretically weaker than RPS methods regarding the quality of learned models under compression. However, traditional RPS methods only focus on reducing the parameter memory footprint and do not affect FLOPs or latency. This paper proposes Sketch Structured Transform (SS1), an RPS scheme that reduces FLOPs while maintaining quality and improving wall-clock inference for machine learning models.

Randomized parameter sharing (RPS) reduces the model's memory footprint by randomly tying parameters across the model. While these methods offer a superior memory-quality tradeoff, they do not alter the computational graph, resulting in similar FLOPs and latency. Addressing this issue - reducing computation and latency while retaining the memory-quality tradeoff - has been a key challenge with RPS methods. In this paper, we propose a random yet structured parameter-sharing method that maintains superior quality, reduces the number of FLOPs, and improves the wall-clock latency of linear layers which are compute bound in most workloads.

The key idea in SS1 is to tie parameters inside a single neuron weight. This weight-tying can be equivalently implemented as the input being reduced in dimension first and then multiplied with the compressed weight vector which reduces FLOPs and memory movement in a neuron computation. We show how to devise this tying in a GPU-friendly manner using what we call \(K-\) and \(N-\) coalescing. Additionally, SS1 can be integrated with existing RPS methods to obtain independent control of parameter memory and computation during training and inference. While SS1 layers can be used to build a model itself, we can also obtain SS1 models from pre-trained models. This is especially important since many useful models are pre-trained once, and weights are open-sourced for public usage. For this purpose, we provide a projection function to project full matrices onto SS1 matrices, which can transfer knowledge from pre-trained models. Moreover, we demonstrate that SS1 can be effectively combined with post-training quantization to harness the advantages of both approaches. Additionally, we present theoretical insights explaining why the integration of SS1 and quantization leads to performance improvements that neither method can achieve independently.

We evaluate SS1 layers on a broad set of settings and use cases. A summary of our findings is below.

* SS1 has better quality-efficiency tradeoff than competing methods like Monarch (state-of-the-art structured sparsity) and LowRank across various domains. With SS1, we can build better models at lower parameter counts while delivering superior inference latency. For example, we achieve up to a \(1.30\) improvement in GPT2  model inference throughput.
* Pretrained models can be projected onto SS1 and further finetuned to deploy fast models. We show that we can maintain the GLUE score of BERT while speeding up inference by \(1.31\).
* SS1 projected models can be used even without finetuning with reasonable accuracies. We show proof-of-concept benefits in the Llama-3-8B model with \(1.11\) faster inference.
* Quantization is a highly effective technique for improving efficiency that can be combined with SS1. Not only do we see this empirically, but we can also see it in theory.
* SS1 also impacts CPU ML workloads significantly. For instance, we reduce the MLP workload of DLRM MLPerf Model, which contributes over \(70\%\) inference latency, by approximately \(2\) using SS1 layers without compromising model quality.

## 2 Related Work and Background

**Unstructured Sparsity:** The redundancy in the deep learning model can be removed by sparsifying the model using iterative procedure [2; 3; 4]. Another related line of work is that of Lottery ticket hypothesis [5; 6; 21], which tries to find a sparse model at the start of training. Apart from expensive procedures to find these subnetworks, the unstructured sparsity still needs to deliver on the promise of latency improvements for inference.

**Structured Sparsity:** Linear transformations that are efficient and expressive has been an active line of research for over a decade. The general direction here is to create a combination of sparse, diagonal, permutation, and sub-linear transformations such as FFT, DCT, and Hadamard [14; 22; 13; 23; 24]. Some of them also used fixed random sparse/diagonal matrices. Structured matrices proposed in this line of research for transformation of size \(K K\) have \(O(K)\) parameters and \(O(Klog(K))\) FLOPs. The actual speed-up obtained on modern hardware using these methods is limited. More importantly, the expressivity of these matrices is severely restricted due to very few learnable parameters. A recent line of work exploits butterfly matrices [25; 16] and their variations, such as Monarch to obtain expressive transformations. These matrices also fit the general recipe of structured matrices specified above. However, a critical distinction in Monarch matrices is the presence of many more parameters. Specifically, the pure monarch matrices are supposed to contain \(O(K)\) parameters and FLOPs when using \(\) blocks or factors (more blocks imply fewer parameters). However, practically, the matrices that have enough representative power in deep learning context generally use two or four blocks , i.e. \(O(K^{2})\) parameters and FLOPs. Nevertheless, Monarch shows great latency benefits for training with scheduled different-sized monarch decomposition. However, to our knowledge, the inference benefits for these matrices are limited. Our paper compares SS1 against Monarch since it is the SOTA representative for this class of matrices.

**Quantization:** Post training quantization  is currently one of the most successful tools for improving large models' efficiency. The basic idea is to reduce the precision of the weights and activations to reduce the memory footprint of the model and also exploit integer arithmetic  for better compute throughput. The literature on quantization is vast, and more details can be found in the survey . In this paper, we show that we can combine SS1 with quantization techniques to further improve latency. We also explore the theory of combining quantization with SS1 approximation in section 4, which can be of independent interest.

**Randomized Parameter Sharing:** Randomized parameter sharing was first introduced in  as a general model compression tool. RPS is primarily used to reduce the parameter memory footprint of models. The parameter memory is separated from the model's actual computational graph. Each model's weight is mapped to the parameter memory using a random hash function. The value of the weight is then the value from the parameter accessed via hash functions. Essentially, if two weights are mapped to the same value in parameter memory, they are tied together and share a single learnable parameter.

The randomness of weight tying that leads to theoretical guarantees concerning projection quality was also the reason behind the extremely slow systemic performance of the proposal. The systemic performance was fixed using block-based hash mappings showing practical applications in various domains [10; 9; 27]. Surprisingly, the projection quality improves with block-based projections leading to a strictly superior RPS system. RPS quality was further enhanced using global parameter sharing (parameter sharing across modules) which is superior to module-specific parameter sharing . This, however, leads to additional challenges regarding the stability of training the model. These challenges were resolved in . This work also proved a missing link in the theoretical analysis of RPS methods - it showed that the quality of dimensionality reduction (alt. projection quality) directly correlates with the quality of models learned under projection for linear models. Further, it was shown that random dense projections (which underlie RPS) are superior to random sparse projections (which underlie Pruning) which justifies why RPS methods convincingly outperform pruning methods, especially at high compressions in . Interestingly, most of the RPS literature is strictly focused on reducing the memory footprint and does not affect the computational workload. In this work, we further improve the utility of RPS techniques by deploying them to reduce computation.

### Background on Randomized Parameter Sharing required for SS1

**Tying of parameters using hash functions (\(h,g\)):** Under RPS, there is a single parameter memory \(\), \(||=m\), and each weight inside the model is mapped to one of the parameters in the memory using hash functions. Let the flattened weight vector of the entire model be \(\), \(||=n\). A weight, say \([i]\), is uniquely identified using a set of integers (module number, location inside the module, etc.), say \(id(i)^{k}\) for some \(k\), and then the value of the weight is

\[[i]=g(id(i))[h(id(i))] h:^{k}[m],g: ^{k}\{ 1\}\]

where \([m]=\{0,1,...,m{-}1\}\).

**Sketch representation of recovery:** We can write the entire recovery of the vector \(\) as a linear projection from \(\) using a sketch matrix \(S R^{n m}\) which is defined as

\[ i[n],S[i,h(id(i))]=g(id(i)), j h(id(i)),S[i,j]=0\] (1)

Then the weight \(\) and the parameter memory \(\) is related by, \(=\)

**Dimensionality reduction problem:** Traditionally, random projections are used as a dimensionality reduction technique for data points. The projection is considered better if it can maintain the structure of the dataset (inter-point distances or equivalently inner products between points). The standard theoretical setup is as follows. Consider two arbitrary vectors \(x,y R^{n}\), Consider a projection function \(\) (or compression technique in general), then \(,\) are projected vectors, \(=(x),=(y)\). Then, the quality of the projection is measured by

\[(,)=(|| x,y- ,||_{2}^{2})\] (2)

where the expectation is over the randomization scheme of the projection. This problem is of interest to evaluate RPS-based learning since the quality of the learned model under projection \(f()\) correlates with the quality of data projection \(S^{}\) for the standard dimensionality reduction problem with \(=S^{}x\) and \(=S^{}y\).

## 3 Sketch Structured Transform(SS1)

The SS1 layer falls into the category of RPS methods. We will use small case letters for scalars, boldface small cases for vectors, and boldface capitals for matrices. We use numpy notation of indexing. For instance \(|((n)+5)\%n][i:j]\) first rotates the vector \(\) by \(5\) places and then selects a subarray from \(i\) to \(j\).

### Parameter sharing in SS1

In standard RPS, where the weights are mapped into a single memory \(\), with a high probability, weights corresponding to a single neuron are not tied together. While this seems advantageous for expressivity, it is also the reason why RPS methods cannot reduce computation since we need to perform \(O(K)\) multiplications for a single neuron. In SS1, we perform restricted parameter sharing where parameters are tied inside a single neuron only. Consider a linear transform \(^{}=^{}\) where \(^{K}\), \(^{N}\) and \(:^{K N}\) matrix.

**Single Neuron RPS:** Let us consider a single neuron \(y=^{}\). Under SS1, the weights \(^{K}\) come from compressed parameter vector \(^{K/c}\) where \(c\) is the compression factor. We use \(//\) to denote integer division. For simplicity, we assume throughout the section that \(c\) is an integer and \(c|K\). Each neuron has its own \(\). Weights \(\) are recovered from \(\) using standard RPS. i.e.,

\[[i]=g(i)[h(i)]\] (3)

Equivalently, \(\) can be represented as

\[=\] (4)

where \(:K K/c\) is sparse matrix according to Equation 1. When parameters are shared in this manner, the computation can be reduced.

\[y=^{}=^{}( )=(^{})\] (5)

Thus computing \(y\) needs only \(K//c\) multiplications since \(^{}\), a sketch of input \(\), can be implemented using only additions and subtractions. As is, this randomized mapping will not be cache-efficient and thus is not GPU-friendly. We show how to make it GPU-friendly using \(K\) and \(N-\)coalescing in the subsequent parts of this section.

K\(-\)coalescing:The single-neuron computation shown above is not GPU-friendly for arbitrary choice of RPS mapping hash function \(h\). We now explain the hash function \(h\) used in SS1.

Figure 1: Illustration of weight tying in SS1. Same colored weights imply that they are tied to the same parameter in memory

The parameter tying is illustrated in Figure 1. We first divide the weight vector \(\) into chunks or groups of size \(}\), a hyper-parameter. The chunk-id and offset inside the chunk of weight can be written as,

\[(i)=i/}(i)=i\%}\] (6)

Then \(c\) ( recall that \(c\) is the integral compression factor ) chunks are grouped together into 1D supergroups. The 1D super group ID of the weight can be written as,

\[_{1}(i)=(i)/c\] (7)

We want to restrict the hash function in SS1 to ensure the following,

1. Each chunk of size \(}\) is contiguously located in \(\) and do not share weights.
2. chunks that belong to the same 1D supergroup share weights among each other.

We use the following mapping \(h^{}:[K][K/c]\) to satisfy both conditions. Let \(h:\) be a universal hash function. Then,

\[h^{}(i)=_{1}(i)}+(h(_{1}(i), (i))+(i))\%}\] (8)

Note that many hash functions which follow the two conditions exist. However, we will stick with this hash function since this is the function we implement. With this hash function, the \(j^{th}\) chunk of \(i^{th}\) 1D super group in \(\) can be written as,

\[[(ic+j)}:(ic+j+1)}]=[i}:(i+1)}][((0:})+h(i,j))\%}]\] (9)

where \((0:n)=(0,1,...,n-1)\) is the range function. In matrix form using Equation 1, let \(_{i,j}:}}\) be the matrix representation of above hash function restricted to \(j^{th}\) chunk of \(i^{th}\) 1D supergroup. Then,

\[[(ic+j)}:(ic+j+1)}]=_{i,j} [i}:(i+1)}]\] (10)

Then, we can write the single neuron computation as,

\[y=^{}=_{i=0}^{(K/}/c)-1}(_{ j=0}^{c-1}^{}[(ic+j)}:(ic+j+1)}] _{i,j})[i}:(i+1)}]\] (11)

We can implement the above computation as follows: Bring in one chunk of \(\); then bring in \(c\) chunks belonging to the 1D super group of \(\) one at a time and aggregate them into a single chunk of size \(}\) using sketch matrices \(\); perform the dot product on the chunk of \(\) and aggregated chunk of \(\) and move on to the next chunk of \(\) and 1D supergroup of \(\). Note that we can perform this operation in a block manner and using exactly one read of \(\) and \(\) due to the nature of the hash function.

\(-\)**coalescing:** Now let us move our discussion from single neuron computation in SS1 to computing all the neurons, i.e. the entire matrix multiplication. In this case we have the complete weight matrix \(:K N\) which is derived from a compressed matrix \(:K/c N\). Apart from \(K\)- coalescing specific to each neuron, to fully utilize the GPU capabilities, we further restrict parameter sharing along the \(N\) dimension. The weight mapping is illustrated in Figure 1. We divide the weights along the \(N\) dimension in column blocks of size \(}\). Each neuron that belongs to the same block will have the same set of hash functions. This allows us to do a block-based computation as,

\[&[:}]=^{} [:,:}]=\\ &_{i=0}^{(K/}/c)-1}(_{j=0}^{c-1} ^{}[(ic+j)}:(ic+j+1)}]_{i,j} )[i}:(i+1)},:}]\] (12)

Note that each neuron has its own parameter space in \(\). The complete algorithm is presented in Algorithm 1. It considers batched multiplication and uses \(}\) hyper-parameter, which is standard in matrix multiplication. Also, it uses a slightly different notation for ease of expression. However, it maintains the principles introduced in this section. The kernel implementation and optimization details are deferred to Appendix E for lack of space.

### SS1 projection from the pre-trained model

A pre-trained model can be projected into SS1 and fine-tuned for downstream tasks. The complete model projection boils down to projecting each full matrix in a linear layer, say \(:K N\) into parameter tied SS1 matrix \(:(K/c) N\). Recall that SS1 has parameter sharing independent in each neuron. Thus, we just have to find the projection for each neuron's weight. Consider the neuron \(y=^{}\) and corresponding compressed with \(\) and recovery sketch matrix \(\). Given \(\), we want to find \(\) that minimizes

\[^{*}=_{z}\|-\|_{2}\] (13)

The solution is that of linear regression, \(^{*}=(^{})^{-1}^{}\). Note that \((^{})\) is a diagonal matrix with non-zero diagonal elements. Once we solve for the value of \(\) for each neuron in each weight matrix, we have our overall projection onto SS1. The algorithm for projection can also be implemented in a blocked manner and is given in Algo 2 in Appendix G

### SS1 combined with standard RPS.

SS1, which reduces the computation of the linear layers, can be combined with RPS methods, which reduce the parameter memory to create a holistic efficiency system - one that lets you control both memory and computation independently. The only change to Algo 1 is in where we read the weight tile. In the current algorithm, we maintain \(:K//c N\), the compressed weight matrix, in a row-major format. Under RPS, we can locate the tile \([k_{}:(k+1)_{},j_ {}:(j+1)_{}]\) in the RPS memory, say \(\), using another hash function, say \(h^{}\), at \([h^{}(k,j)\%||-_{} _{}]\).

## 4 Theoretical aspects of SS1

**Quantization + SS1 can beat the individual methods:** In this section, we analyze a combination of quantization and SS1 (i.e., projection) in the standard dimensionality reduction setup. We consider stochastic integer quantization for our analysis. Consider two vectors \(,^{n}\). We aim to reduce the memory by a factor of \(c\). In case of quantization, assuming initial precision as \(F\), \(c=b/F\) if we are using b-bit quantization. Similarly, for projection, \(c=m/n\) if projecting the data to \(^{m}\). In case of projection the compressed vector can be represented:

\[}_{p}[i]=_{j=1}^{n}(g(j)(h(j){=}i)[i])\] (14)

Let the component values be restricted to \((-D/2,D/2)\) for some \(D\). The compressed vector in case of stochastic quanti

Figure 2: Upper bound on variance

zation can be written as,

\[}_{q}[i]=[i]_{q}+\ \ \ \ =1\ \ p=[i]-|[i]|_{})}{D/2^{ 2}}\\ 0\ \ (1-p)\] (15)

The range \(D\) is divided into \(2^{b}\) buckets and \(._{q}\) rounds the value down in the buckets. The variance of the inner product between \(},}\) for projection, quantization, and combination that first performs b-bit quantization followed by projection is given below,

**Theorem 1**: _Consider two arbitrary vectors \(, R^{n}\) such that \(|||| 1,|||| 1\) in \(F\) precision. For some fixed value \(k\) in \(\),_

\[Q(c)=}+}\ \ P(c)=\] (16)

_The variance of quantization (\(V(},}_{q})\)), projection (\(V(},}_{p})\)) with compression \(c\), can be tightly bounded by_

\[V(},}_{q}) Q(c)\ \ \ V( },}_{p}) P(c)\] (17)

_The variance of combination with compression \(c=c_{p}c_{q}\) with quantization with compression \(c_{q}\) followed by projection with \(c_{p}\) is bounded by_

\[V(},}_{pq}) Q(c_{q})(1+ })+P(c_{p})\] (18)

The best way to understand this theorem is to plot the variance as a function of compression as shown in the Figure 2. In the plot, we use \(c_{q}=0.3,n=128\) and \(k=1.5\) for illustration. As we can see with the combination, the variance is better than individual methods in certain regions. The reason is that both methods do quite well in the low compression regime, and explode steeply at very high compression. Thus, when we combine the two methods, we can remain in relatively low error zones for both methods while obtaining greater overall compression.

**Analyzing parameters of SS1:** This section discusses the new aspects of SS1 and how it affects the quality of SS1. Specifically, we discuss the effect of parameters \(}\) and \(}\). From a latency viewpoint, it is natural that these parameters should be autotuned to optimize for latency, which is standard for matrix multiplication. However, since these parameters affect the weight tying, it is important to investigate their effect on the quality of models. We consider the standard dimensionality reduction setup to evaluate the changes in mapping performed by SS1. We find that \(}\) is a pure latency parameter with no impact on learning quality. We present this as the following result.

**Theorem 2**: _Consider two vectors \(, R^{n}\) in higher dimensional space. Also, let \(h\) and \(g\) be the SS1 hash functions used to project these vectors to \(},} R^{m}\) (\(m<n\)). Let \(}\) be the K-coalescing factor of \(h\). Then, under compression \(c=n/m\) and assuming \(c}|m|n\), the inner product estimation is unbiased and has variance,_

\[V(},})=}}_{i,j=1}^{n}_{j=1,j i}^{n}I(i,j)(x_{i}^{2}y_{j }^{2}+x_{i}y_{i}x_{j}y_{j})\] (19)

_where \(I(i,j)\) is a Kronecker delta function indicating if \(i\) and \(j\) belong to the different groups in the super group of size \(c}\). Under permutation \(p\) of elements of \(x\) and \(y\) before projection,_

\[_{p}(V(},}- ,))=_{i=1}^{n}_{j=1,j i}^{n}(x_{i}^{2}y_{j}^{2}+x_{i}y_{i}x_{j}y_{j})\] (20)

In the above theorem, we find that the quality of projection is unaffected by the parameter \(}\). This can be understood by considering a specific element in \(\). The number of other elements with which it can interact under projection depends only on the compression factor \(c=n/m\) and not on \(}\). We know from , that the quality of projection is directly related to the quality of learned linear models under compression. Thus, \(}\) does not have any impact on learning quality as well. We analyse the effect of \(}\) as well. But the analysis is deferred to the appendix (see section F.2) for lack of space.

## 5 Experiments

### Accuracy vs. Latency evaluation of SS1

We first evaluate the expressiveness and latency of our SS1 against popular baselines of Monarch, SOTA structured sparsity-based transformation, and LowRank transformation. We benchmark various shapes for the "nn.Linear", Monarch, LowRank, and SS1 kernels to evaluate the standalone kernel latency. We use the official Monarch implementation by the authors4 with number of blocks (nb) as the compression controlling factor. The results are deferred to the appendix due to space constraints. We also measure end-to-end model latency improvement for various sizes of GPT2 and MLPMixer (MM)  models. We show the latency of three sizes of models (small: S, medium:M,

Table 1: This table presents the quality and latency of NLP and vision experiments. Overall across both the domains, SS1 gives best quality models under similar parameters and better quality per unit compute. In terms of inference latency, we see upto \(1.3\) increase in throughput. Exact experimental details are present in Appendix I.

Table 2: PPL(loss) for **[Left]:** Applying quantization on some saved checkpoints of original and SS1 models. The effect of quantization on SS1 is similar to that on full model. **[Right]**: SS1 models can outperform standard models. The std-deviation is around \(0.2\)PPL for these experiments.

and large:L)for both architectures. Exact details are provided in appendix I) Additionally, we perform quality experiments in end-to-end training using these matrices for two domains: (a) NLP: GPT2-S model on wikitext-103 dataset  using test perplexity (PPL) and loss as the metric (b) Vision: MLPMixer on CIFAR (C10, C100) and Tiny-Imagenet datasets  using accuracy as the metric. The results are presented in Table 1. The details of these experiments can be found in the Appendix I. We make the following observations.

* In our kernel latency evaluation, SS1 consistently outperforms Monarch and LowRank under similar parameter budgets (Table 4 in appendix). Monarch is generally worse than full multiplication except in larger shapes. LowRank is competitive with SS1 for higher compression.
* The kernel-level latency benefits also translate to end-to-end model latency (Table 1 and Table 0(b)). This can be seen across both Transformer and MLPMixer architectures, two of the SOTA architectures for deep learning. Although, at times, LowRankLinear gives better performance at higher compression, the quality of the model obtained is inferior to SS1. With larger models, we see an increase of around \(1.3\) in end-to-end throughput with SS1. It also enables us to run larger batches.
* In terms of quality, SS1 consistently outperforms Monarch and LowRank across NLP and Vision domains on GPT and MLPMixer architectures under similar parameter budgets.
* Overall, we see that SS1 allows better quality per unit compute than its structured competitors.

_Building expressive and faster models:_ Should we build and train models with SS1 layers instead of standard linear layers? Although answering this question requires a broader evaluation, we cautiously believe that the answer might be yes. We can indeed obtain better quality models by using SS1 layers instead of "nn.Linear" layer. This is demonstrated in Table 2. The quality of the SS1 model with 74M parameters is better than that of standard models with similar or more parameters.

### SS1 + Quantization

Quantization is an important efficiency technique to improve inference time memory and latency. Although SS1 already reduces the memory footprint and improves inference latency, it can be combined with quantization to obtain further inference benefits. One might wonder if combining two approximations (SS1 and quantization) can lead to worse-quality models. We show that SS1 can be combined with quantization without significantly impacting quality (also supported by theory) In fact, the effect of quantization on SS1 is similar to the full standard model. The quality details before

  &  &  &  \\   & \#par & GLUE score & \#par & GLUE score & & \#par & MMLU & WINO \\  baseline (reported) & 335M & 80.4 & 109M & 78.6 & & & & \\ baseline (our run) & 335M & 82.2 & 109M & 82.16 & baseline (our run) & 8B & 65.05 & 76.1 \\ SS1 & 181M & 79.6 & 66M & 79.9 & (our run) & 8B & (\(\) 0.0038) & (\(\) 0.011) \\  & & & & & & 61.26 & 69.93 \\  & & & & & (\(\) 0.006) & & (\(\) 0.0039 & (\(\) 0.012) \\ 

Table 3: **[Left:]**BERT pretrained model projected onto SS1 and finetuned on GLUE benchmark. Due to the descripancy, we report both online and our local results of full model. The SS1 model is \(\)**faster** for higher batch sizes. Details are available in I.2 **[Right:]** Proof of concept of applications in Llama-3-8B. We obtain \(1.11\) speed up in latency by compressing selective layers, without any form of retraining or finetuning. Details of the experiment are in I.3

  &  &  &  &  &  &  \\  & MLP & Interaction & Lookup & MLP & params & \\  DLRM-MLPerf & 6.63\% & 12.3\% & 9.7\% & 71.3\% & 1.7M & 0.8032 \\ DLRM-MLPerf \(\) SS1-2x & — needs CPU kernel implementation — & & 0.9M & 0.8032 \\ 

Table 4: The time spent by 100GB sized DLRM MLPerf benchmark model in various components. Specifically 70% latency is spent in Top MLP. We can reduce MLP workload by factor of \(2\) by training SS1 layers without compromising quality of the model. (Statistical significance: multiple runs gives 0.8032)and after quantization are in Table 2. To measure the latency benefits, we would have to implement quantized SS1 kernels, which is out of the scope of this paper.

### Dense-to-structured finetuning of pre-trained models for efficiency

We show that pre-trained models can be projected onto SS1-structured space and finetuned for downstream tasks. We demonstrate this by projecting selected linear transformations in BERT to SS1 and training the resulting model on the GLUE Benchmark. We obtain a \(1.31\) faster model which gives a similar GLUE score in downstream tasks. More details are mentioned in I. The results are summarized in Table 3

We find that we can improve inference of Llama-3-8B model for downstream tasks if we project carefully selected layers onto SS1. The selection is done on a calibration dataset that corresponds to the task at hand. We show that, even without finetuning, we can reduce the computation involved in the model while maintaining reasonable accuracy. Results can potentially improve if we finetune post-projection as we see in BERT. The results are summarized in Table 3.

### Improving CPU workloads e.g. DLRM MLPerf Benchmark

While this paper focuses on providing GPU kernel for SS1, the algorithm can also be utilized to improve the inference performance of CPU deployments. Specifically, recommendation workloads of models such as the Deep learning recommendation model (DLRM) are often run on CPUs due to humungous embedding tables. We consider the DLRM MLPerf Benchmark, which has 100GB embedding tables and 10MB MLP layers. Nevertheless, on CPUs, the latency bottleneck is that of matrix multiplications. Specifically, 71.3% of the time is spent in the top MLP component (see Table 4). We show that we can train from scratch a SS1 version of DLRM, which has half the parameters in MLP and maintains the quality of the full model. While CPU-kernel implementation of SS1 is out of the scope of this paper, we expect that benefits shown on GPUs will also translate to CPUs, and thus, with SS1, we can improve the throughput of DLRM MLPerf Benchmark on CPUs without compromising the quality of the model.

### Comparison with standard RPS method

We also compare SS1 with standard RPS method. Since SS1 focuses on compressing linear layers, we compare it with ROAST-MM compression with each matrix having its individual separate memory. The results are presented in Table 5. As expected, RPS methods do not improve the latency.

**Limitations:** Our work is mainly limited on two aspects. On the theoretical side, our analysis of parameters and the combination of quantization with SS1 is conducted mainly over linear models, which is a reasonable starting point. However, these results may not necessarily extend to deep learning models, and we plan to explore this in future research. Additionally, regarding our SS1 proposal, the efficiency gains are marginal beyond an \(8\) compression of parameters. In future work, we aim to develop efficient structures where the efficiency gains are more closely aligned with the degree of parameter reduction.

## 6 Conclusion

In this paper, we introduce an efficient structured linear transformation termed SS1. We demonstrate that SS1 outperforms state-of-the-art structured baselines in terms of quality per unit of computation. Additionally, we show that SS1 provides immediate benefits across diverse applications, including language understanding, generative AI, and recommendations. Furthermore, we illustrate how SS1 can be combined with the popular quantization approach to achieve further improvements. We also theoretically explain the underlying reasons that may be of independent interest.

   & 2x & 4x & 8x \\  SS1 & 19.45(154) & 19.99(148) & 20.68(145) \\ ROAST & 19.87(238) & 20.20(237) & 20.94(222) \\ 

Table 5: PPL(latency) for GPT-2(124M) model trained with different compression SS1 and ROASTAcknowledgments

This work was supported by National Science Foundation SHF-2211815 and grants from Adobe, Intel, Total, and VMware.