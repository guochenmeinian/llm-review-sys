# Deep Recurrent Optimal Stopping

Niranjan Damera Venkata

Digital and Transformation Organization

HP Inc., Chennai, India

niranjan.damera.venkata@hp.com &Chiranjib Bhattacharyya

Dept. of CSA and RBCCPS

Indian Institute of Science, Bangalore, India

chiru@iisc.ac.in

###### Abstract

Deep neural networks (DNNs) have recently emerged as a powerful paradigm for solving Markovian optimal stopping problems. However, a ready extension of DNN-based methods to non-Markovian settings requires significant state and parameter space expansion, manifesting the curse of dimensionality. Further, efficient state-space transformations permitting Markovian approximations, such as those afforded by recurrent neural networks (RNNs), are either structurally infeasible or are confounded by the curse of non-Markovianity. Considering these issues, we introduce, for the first time, an optimal stopping policy gradient algorithm (OSPG) that can leverage RNNs effectively in non-Markovian settings by implicitly optimizing value functions without recursion, mitigating the curse of non-Markovianity. The OSPG algorithm is derived from an inference procedure on a novel Bayesian network representation of discrete-time non-Markovian optimal stopping trajectories and, as a consequence, yields an offline policy gradient algorithm that eliminates expensive Monte Carlo policy rollouts.

## 1 Introduction

In the classic optimal stopping setting, an agent monitors the trajectory of a stochastic process, with the opportunity to either continue observing or stop and claim a reward (or suffer a cost), which is generally a function of process history. Once the agent decides to stop, no further actions are possible, and no further reward can be claimed (or cost suffered). The agent's goal is to produce stopping decisions based on the process observations to maximize the expected reward (or minimize the expected cost). For example, whether to exercise a stock option or not at any time is an optimal stopping problem where the goal is to maximize expected exercise value, as is the decision of when to replace a degrading machine. Optimal stopping methods have a wide array of applications in finance , operations research , disorder/change-point detection , early classification of time-series  among others.

In this paper, we are concerned with the _discrete-time, finite-horizon, model-free_ setting, where the process evolves over discrete time-steps, and a decision to stop must occur by a given finite horizon. Further, the dynamics of process evolution are unknown. This setting is in contrast with classic approaches  which assume that the dynamics of process evolution are known and characterized with simplified and usually Markovian stochastic models. Note that continuous-time optimal stopping problems, including the pricing of American options, may often be reduced to the discrete-time setting by suitable discretization . Solving optimal stopping problems in non-Markovian settings is challenging due to the following curses:

* **The curse of dimensionality**: A non-Markovian process may be made Markovian by extending state space with process history. However, such an extension significantly increases the problem's dimensionality, complicating function approximation. Estimating high-dimensional value functions directly rules out conventional approaches that approximate value functions with a linear combination of basis functions .

* **The curse of non-Markovianity**[41; 6]: One may mitigate the curse of dimensionality by approximating a non-Markovian process by a Markovian process with a transformed state-space. For example, RNNs can learn to summarize relevant process history into a Markovian hidden-state process efficiently. However, state-of-the-art Markovian optimal stopping methods [19; 20; 2; 16] rely on the recursive computation of value functions (see Section 2) which propagates approximation bias in non-Markovian settings, a phenomenon known as the curse of non-Markovianity .

**Existing deep neural network approaches:** Deep neural network approaches to optimal stopping [19; 20; 2; 16] have emerged as a powerful way of handling high-dimensional Markovian optimal stopping problems. Existing model-free methods compute value function approximations relying on a recursive temporal relationship between value functions at successive time-steps in Markovian settings embodied by the Wald-Bellman equation  or closely related equivalent (see Section 2).

_Backward Induction_ methods [19; 20; 2; 16] start from the last time-step and recurse backward sequentially through time. Non-Markovian settings force Backward Induction methods to augment their state space with process history  significantly exacerbating the curse of dimensionality since elegant approaches such as RNNs, which share parameters across time-steps are not an option for backward induction methods. If the value function of time-step \(j+1\) is already optimized, it cannot change when we fit the value function at time-step \(j\). Further, the inability to share parameters across time-steps means parameter space grows linearly with the time-steps processed, thereby reducing sample efficiency. A notable exception is the method of Herrera et al. , which uses an RNN. However, the RNN weights are random and not learnable to permit backward induction to proceed. Only the final hidden-layer weights are fit, and these are not shareable across time-steps.

_Fitted Q-Iteration_ (FQI) methods [34; 16] start with bootstrapped value functions at all time-steps and recursively update these for temporal consistency with the Wald-Bellman equation. While they can use RNN-based function approximators, they suffer from the curse of non-Markovianity.

We note that a body of work uses deep neural networks to solve continuous-time optimal stopping problems by formulating them as free-boundary Partial Differential Equations (PDE). These include methods such as the deep Galerkin method  and the backward stochastic differential equation method . We consider such methods model-based since they start with a specific PDE to be solved that assumes prior knowledge of process evolution.

**Outline of contributions:** The above context points to (1) using a suitable characterization of state space (as afforded by RNNs) to mitigate the curse of dimensionality and (2) direct policy estimation methods such as policy gradients to mitigate the curse of non-Markovianity. Our approach brings together, for the first time, probabilistic graphical models, policy gradient methods , and RNNs to design effective stopping policies for non-Markovian settings. policy gradient methods are notably missing from the optimal stopping literature. We make the following contributions:

* We present a reward augmented trajectory model (RATM) (see Section 3), a novel Bayes net parameterization of non-Markovian state-action-reward trajectories encountered in optimal stopping, where conditional probability distributions (CPDs) encode stochastic policy actions and reward possibilities (Section 3). In particular, this formulation allows the stochastic policy to share parameters across time-steps, allowing RNN-based CPD approximations, which mitigate the curse of dimensionality issues discussed earlier. Also, the problem of finding an optimal stopping policy reduces to a direct policy optimization procedure based on the E-M algorithm (Theorem 3.1, and Corollary 3.1.1) that leverages inference over the Bayes net. This mitigates the curse of non-Markovianity associated with recursive value function estimation. Further, modeling optimal stopping with graphical models opens new avenues. As noted by Levine , in the context of reinforcement learning, "The extensibility and compositionality of graphical models can likely be leveraged to produce more sophisticated reinforcement learning methods, and the framework of probabilistic inference can offer a powerful toolkit for deriving effective and convergent learning algorithms for the corresponding models"; for instance, augmentation with additional latent variables may be explored to model process disorders such as change-points.
* Inference on the RATM yields a policy gradient algorithm when we take an incremental view of the E-M algorithm (see Section 4, Theorem 4.1 and Proposition 4.1). We call this algorithm the optimal stopping policy gradient (OSPG) method, the first policy gradient algorithm, to the best of our knowledge, for computing optimal stopping policies suitable for non-Markovian settings. A key benefit of OSPG is that unlike classic policy gradients , it is an offline algorithm and does not require Monte-Carlo policy rollouts. Crucially, we show (Corollary 4.1.1) that our procedure is learning a policy that implicitly optimizes value functions at every time-step without requiring recursive value function computation, thereby mitigating the curse of non-Markovianity.
* We introduce a new loss function based on OSPG that is well-suited for learning RNN-based stopping policies with mini-batch stochastic gradient descent (Algorithm 1). This makes OSPG (unlike E-M) practical for large datasets while leveraging powerful off-the-shelf optimizers such as Adam. The OSPG loss considers an entire process trajectory and does not decompose across time-steps, allowing policy decisions across a trajectory to be evaluated jointly rather than independently.

## 2 Discrete-time finite-horizon optimal stopping

We represent the stochastic process monitored by \(d\)-dimensional random fully observable environment state variables \(\{S_{j}\}\). Process history until and including time-step \(j\) is represented by the \(d j\) random matrix \(_{j}:=[S_{0},S_{1}, S_{j}]\). Associated with state history is a reward process \(\{R_{j}\}\) with \(R_{j}=g_{j}(_{j})\), where \(g_{j}:^{d j}^{+}\) is a reward function that maps observation history to a reward for stopping at time-step \(j\). Uppercase letters denote random variables, while lowercase letters represent their realizations. Bold font is used to indicate histories. Thus \(_{j}\) is an observed partial trajectory from \(_{j}\). Also, in case we need to refer to a specific trajectory, we use an extra index \(i\), so for example, \(r_{ij}\) refers to the observed reward for stopping at time-step \(j\) when monitoring the \(i^{th}\) process trajectory.

**Definition 2.1** (stopping policy).: A stopping policy is defined as a sequence of functions \(=(_{1},_{2},_{j},)\), where \(_{j}:^{d j}\{0,1\}\) maps the history of observations at a time-step \(j\) to a decision of whether to stop (\(_{j}(_{j})=1\)) or not (\(_{j}(_{j})=0\)).

**Definition 2.2** (stochastic stopping policy).: A stochastic stopping policy is defined as a sequence of functions \(=(_{1},_{2},_{j},)\), where \(_{j}:^{d j}\) maps the history of observations at a time-step \(j\) to a probability of stopping at time \(j\).

Note that a stochastic policy may be converted to a deterministic stopping policy to make stopping decisions by augmenting the state space with i.i.d. virtual observations from a random variable \(U\) uniformly distributed in \(\) Thus, the equivalent deterministic policy is \(_{j}(_{j}):=(_{j}(_{j}) u_{j})\).

This paper considers the finite horizon setting where \(j H\), so a decision to stop must be made at or before a finite horizon \(H\). Thus we define \(_{H}(_{H}):=1\). The decision to stop is based on a stochastic process, making the stopping time a random variable.

**Definition 2.3** (policy stopping time ).: Stopping occurs when the policy first triggers a stop action. This occurs at random time \(:=\{0 j H:_{j}(_{j})=1\}\), called the stopping time1.

With this background, we can formally define the optimal stopping problem:

**Definition 2.4** (optimal stopping problem ).: Solve for the optimal stopping time \(^{*}\) (if one exists) for which \([R_{^{*}}]=_{}[R_{}]\) where \(R_{j}=g_{j}(_{j})\) is a reward process, \(\) is a stopping time random variable, and \(\{_{j}\}\) is a stochastic process.

Following Shiryaev and Peskir [36; 28], in the finite-horizon, discrete-time setting, the existence of a finite optimal stopping-time is guaranteed when \([_{j H}|R_{j}|]<\). The optimal stopping time can then be determined by the method of _backward induction_ which recursively constructs the _Snell-envelope2_ comprising random variables \(\{U_{j}\}\) given by:

\[U_{H}:=R_{H}\,\ U_{j}=\{R_{j},[U_{j+1}|_{j} ]\}\,\ j=H-1, 0\] (Snell-envelope)

The optimal stopping time is given by \(^{*}=\{0 j H:U_{j}=R_{j}\}\). From a practical perspective, the expectations in the recursion are hard to estimate since they depend on process history, manifesting the curse of dimensionality. The assumption of Markovian state evolution, i.e. \((S_{j+1}|_{j})=(S_{j+1}|S_{j})\), and a reward process expressable as a function \(G_{j}\) of the current state,i.e. \(R_{j}=G_{j}(S_{j})\), directly leads to the much more tractable _Wald-Bellman equation_[28; 35; 36]:

\[V_{H}(s):=G_{H}(s)\;,\;V_{j}(s)=\{G_{j}(s),T_{j}V_{j+1}(s)\},j<H \]

The _value functions_\(V_{j}(s)\) give the maximum expected reward obtainable from time \(j\) having observed state \(s\). Thus \(V_{j}(s)=_{ j}_{s}[R_{}]\; j\). The operator \(T_{j}\) is defined by \(T_{j}F(s):=_{j,s}[F(S_{j+1})]\) for any function \(F\) of the state. The Snell-envelope reduces to \(U_{j}=V_{j}(S_{j})\) in this case. We define \(K_{j}(s):=T_{j}V_{j+1}(s)\) as the _continuation value-function_, which is the maximum expected reward one can obtain by choosing to continue at step \(j\) having observed state \(s\). The optimal stopping policy is given by:

\[_{H}^{*}(_{j}):=1\;,\;_{j}^{*}(_{j})= (G_{j}(S_{j}) K_{j}(S_{j})),\;j=0,1, H-1\] (1)

The value functions are, therefore, related to the optimal policy as follows:

\[V_{j}(S_{j})=_{j}^{*}(S_{j})G_{j}(S_{j})+(1-_{j}^{*}(S_{j}))K_ {j}(S_{j})\] (2)

Thus, if we can estimate the continuation value functions \(K_{j}\), we can trigger optimal stopping decisions using equation (1). Note that this requires knowledge of reward function \(G_{j}\) at inference time. Backward induction approaches typically, first estimate \(K_{H-1}\) to approximate \(K_{H-1}(S_{H-1}) G_{H}(S_{H})\) and then use either the Wald-Bellman equation [40; 19] or equations (1) and (2) [23; 20; 16] to produce approximation targets \(V_{H-1}(S_{H-1})\) which are used as targets to fit \(K_{H-2}(S_{H-2}) V_{H-1}(S_{H-1})\), and the process continues backwards recursively. As a variation on this theme, Becker et al.  first set \(K_{H-1}(S_{H-1})=G_{H}(S_{H})\) and then estimate \(_{j}^{*}(s)\) in equation (2) by maximizing the RHS of equation (2) with \(j=H-1\) and \(_{H-1}\) replacing \(_{H-1}^{*}\). Then equation (2) is used to compute \(V_{H-1}(S_{H-1})\) which is in turn used to set \(K_{H-2}(S_{H-2})=V_{H-1}(S_{H-1})\) and the process proceeds recursively. Fitted Q-iteration methods [39; 40; 43] use bootstrapped continuation value functions \(K_{j}\) to produce \(V_{j}(S_{j})\) at all time-steps using the Wald-Bellman equation. Then, updated continuation value-functions \(K_{j}\) are fit simultaneously across all time-steps to approximate \(V_{j+1}(S_{j+1})\), and the process continues recursively until convergence. If Markovian assumptions do not hold, states \(S_{j}\) need to be replaced with \(_{j}\) exploding the state space and significantly complicating function approximation (the curse of dimensionality). The recursive procedures discussed above propagate bias across time-steps when using Markovian approximations, since bias in value estimation at one time-step infects the preceding time-step (the curse of non-Markovianity).

## 3 A Bayesian network view of optimal stopping

This section presents a new Bayesian network view of optimal stopping. While we draw inspiration from a corresponding view of general reinforcement learning (RL) , the resulting modeling choices needed to capture the structure of optimal stopping make the approaches distinct.

**State-action trajectory model:** Figure 1(a) shows a Bayes net model of state-action trajectories in optimal stopping. An action \(A_{j}\) at any time-step \(j\) depends on the state history \(_{j}\) up to that time-step, permitting general non-Markovian stopping policies. Further, actions are restricted to \(0\) (continue) or \(1\) (stop), with the stop action terminating the action trajectory. Therefore, a key feature of optimal stopping problems is that, unlike general reinforcement learning, actions do not impact the probability distribution of the following states over the control horizon, conditioned on state history. We do not model the stopping time \(\) explicitly. Instead, it is a consequence of stochastic policy actions over

Figure 1: Bayesian networks representing (a) state-history (\(_{j}\)) and action (\(A_{j}\)) trajectories and (b) corresponding reward augmented trajectory model for optimal stopping.

the horizon. The action trajectory stops at a random stopping time \(=\{0 j H:A_{j}=1\}\). Therefore, we have the special structure \(A_{}=1\) and \(A_{j}=0, j<\). Note that Figure 1(a) represents a family of stopping trajectories \(_{}\), each member stopping at a specific value of \(\). We denote a specific stopping trajectory by \(_{j}\). Consider stochastic policies, given by the functions \(_{j}(_{j}):=(A_{j}=1|_{j})\). We define \(_{j}(_{j}):=(=j|_{j})\) to represent the stopping-time distribution. We have the following key relationship between action trajectories and the stopping-time random variable encapsulated in the following lemma, adapted from , proved in Appendix A.

**Lemma 3.1** (trajectory reparameterization lemma).: \[(=j|_{H})=_{j}(_{j})=( _{j}|_{j})=\{_{0}(_ {0})&j=0\\ _{j}(_{j})_{n=0}^{j-1}(1-_{n}(_{n})),&0<j<H\\ _{n=0}^{H-1}(1-_{n}(_{n})),&j=H.\]

**Reward augmented trajectory model:** Figure 1b shows an augmentation of the basic state action trajectory model to bring the notion of stopping rewards into the model. The variables \(R_{j}\) represent the reward process, which are functions of state history: recall \(R_{j}=g_{j}(_{j})\). We introduce \(Z_{}\) as a family of binary random variables representing if a reward is obtained (\(Z_{}=1\)) or not (\(Z_{}=0\)) when we stop at various values of \(\). Thus stopping at \(j\) no longer guarantees reward \(R_{j}\). The idea is to use random variables \(Z_{}\) to transform absolute rewards which are not readily interpreted as probabilities into probabilities (in this case, probabilities of obtaining a reward at any time-step) that may be represented in the Bayes net. We parameterize the conditional probability distribution of \(Z_{}\) with Bernoulli distributions defined as follows:

\[(Z_{}=1|_{H},_{})=_{}:= (,_{H})(Z_{}=0|_{H}, _{})=1-_{}\] (3)

where \(:\{0,1, H\}(^{+})^{H}\) is a function that transforms real-valued stopping rewards into probabilities that encode the notion of the _relative_ possibility of obtaining rewards over a trajectory by stopping at a particular time-step of the trajectory. We may write the joint distribution of a trajectory stopping at time \(\) as:

\[(_{},Z_{},_{H},_ {H}) =(_{0})(_{0}| _{0})_{j=1}^{H}(_{j}|_{j-1}) (_{j}|_{j})}_{(_{H})(_{}|_{})}^{} (A_{n}|_{n})}_{(_{}|_{ })}(Z_{}|_{},_{H})\] \[=(_{H})(_{H}|_ {H})(|_{H})(Z_{}|_{}, _{H})\] (4)

We have used the reparameterization lemma 3.1 to replace the probability distribution over actions with the corresponding induced probability distribution over stopping times. Conditioning on \(_{H}\) and \(_{H}\) we have:

\[(_{},Z_{}|_{H},_{H})=(|_{H})(Z_{}|_{},_{H})\] (5)

**Optimal policy estimation:** We are particularly interested in the conditional probability of getting a reward when we stop. We can obtain this by using Bayes net inference to obtain equation (5) and then marginalizing over the family of stopping trajectories \(_{}\). Since there is a one-to-one correspondence between \(_{}\) and stopping time \(\), we just need to sum over all possible stopping times. Thus, if we define \(Z:=Z_{0} Z_{1} Z_{H}\), where \(\) is the XOR operator3, we have, from equation (5):

\[(Z=1|_{H},_{H})=_{j=0}^{H}(=j| _{H})(Z_{j}=1|_{j},_{H})=_{j=0}^{H }_{j}(_{j})_{j}\] (6)

Ideally, we would want our policy to maximize this conditional probability. To this end, we consider parameterized policy functions \(_{j}^{}\) and induced stopping time distributions \(_{j}^{}\) and define the conditional likelihood function over a sample trajectory as:

\[l(\,|_{H},_{H})=(Z=1|_{H},r _{H};)=_{j=0}^{H}_{j}^{}(_{j})_{j}\] (7)However, maximizing this likelihood over samples while allowing optimal stopping decisions on any trajectory ignores the relative magnitude of rewards between different trajectories. Thus, we seek to weight the likelihood of each trajectory. Therefore, considering a dataset \(\) of environment trajectories and using index \(i\) for the \(i^{th}\) trajectory in the sample, we have:

\[l_{}()=_{i=1}^{N}l(\,|_{i,H}, _{i,H})^{_{i}}_{i=1}^{N}_ {i}=1\] (8)

where \(_{i}\) is the relative weight for the \(i^{th}\) sample. Taking logarithms, we have the following objective to maximize:

\[J_{}()=_{i=1}^{N}_{i}_{j=0}^{H}_ {j}^{}(_{ij})_{ij}\] (9)

We may use the following Expectation-Maximization (E-M) procedure to maximize this objective:

**Theorem 3.1** (E-M procedure for optimal stopping policy).: _The following iterative procedure converges to a local maximum of \(J_{}()\) with monotonic increase in the objective with iteration index \((k)\), starting with an initial value \(=^{(0)}\):_

\[q_{ij}^{(k)}=^{^{(k)}}(_{ij})_{ij }}{_{j=0}^{H}_{j}^{^{(k)}}(_{ij})_{ij}}  0 j H,\ 1 i N\] (E-step) \[J_{M}^{(k)}()=_{i=1}^{N}_{j=0}^{H}_{i}^{(k)}q _{ij}^{(k)}_{j}^{}(_{ij})\,\ ^{(k+1)}=*{arg\,max}_{}J_{M}^{(k)}()\] (M-step)

where \(_{i}^{(k)}=_{i}, k\). The proof follows from an application of Jensen's inequality and is given in Appendix A. Dayan and Hinton pioneered using E-M methods for RL policy search .

**Specification of \(\{_{ij}\}\) and \(}\)**: \(_{ij}\) is a specification of the relative _intra-trajectory_ probability of obtaining a reward when stopping at time-step \(j\) of the \(i^{th}\) trajectory. A natural choice (and one that we adopt in this paper) is to set \(_{ij}:=}{_{j=0}^{H}r_{ij}}\), where \(r_{ij}\) is a realization of \(R_{j}=g_{j}(_{j})\) corresponding to the \(i^{th}\) trajectory. This formulation encourages stopping at time-steps of the trajectory, proportional to achievable reward. Alternatively, formulations such as \(_{ij}:=}{_{k}r_{ik}}\) or \(_{ij}:=)}{_{k=0}^{H}(r_{ik})}\) could be used to express different stopping preferences within a trajectory. The weights \(}\), on the other hand, express relative _inter-trajectory_ importance. One plausible choice would be to set \(_{i}=_{i}=^{H}r_{ij}}{_{i=1}^{N}_{j=0 }^{H}r_{ij}}\), the relative reward available for extraction from a trajectory. Similarly, one could weight trajectories by the maximum reward achievable: \(_{i}=_{i}=r_{ij}}{_{i}_{j}r_{ij}}\). We may also consider reweighting schemes that vary weights over rounds of the E-M algorithm. In the RL literature, Reward-weighted regression  takes such an approach. Reward-weighted regression typically conducts rollouts of the current policy and weights each resulting sample trajectory with the relative reward achieved by the current policy on that trajectory. However, the trajectory reward is unknown since we do not perform policy rollouts. Instead, we weight each trajectory offline by its relative expected reward under the current policy. Specifically, we compute weights at each round according to a W-step:

\[_{i}^{(k)}=^{H}_{j}^{^{(k)}}( _{ij})r_{ij}}{_{i=1}^{N}_{j=0}^{H}_{j}^{^{(k)}}(_{ij})r_{ij}} 1 i N\] (W-step)

This weighting is not arbitrary as it leads to a surprising equivalence with a policy gradient method, established in the next section. Further, we have the following Corollary (proof in Appendix A):

**Corollary 3.1.1** (Reweighted E-M).: _Let \(_{i}=^{H}r_{ij}}{_{i=1}^{N}_{j=0}^{H}r_{ij}}\). Using the reweighting scheme of the W-step in the E-M procedure of Theorem 3.1 achieves a local maximum of:_

\[J_{}(},)=_{i=1}^{N}_{i} _{j=0}^{H}_{j}^{}(_{ij})_{ij}-_ {i=1}^{N}_{i}_{i}}{_{i}}\] (10)

_with monotonic increase in the objective, starting with an initial value \(=^{(0)}\). The second term is the KL-divergence between distributions \(}\) and \(}\)._Optimal stopping policy gradients (OSPG)

In this section we show that an incremental version of the E-M algorithm described in the previous section, with a single gradient step replacing a full M-step, is equivalent to a policy gradient method that maximizes expected reward. We call this the optimal stopping policy gradient (OSPG) method.

Consider a direct maximization of the optimal stopping objective \(J_{OS}()=[R_{}]\) from definition 2.4 where the expectation is over state-action trajectories. We can use our Bayes net model of state-action trajectories without reward augmentation shown in Figure 0(a) to obtain:

\[(_{H},_{}|)=( _{0})_{j=1}^{H}(_{j}|_{j-1})_{n=0}^{ }(A_{n}|_{n},)=(_{H}) (|_{H},)\] (11)

where we have used the trajectory reparameterization lemma to reparameterize in terms of stopping times. Therefore, we may write:

\[J_{OS}()=_{_{H}(_{H})} [_{(|_{H},)}[ R_{}]]=_{_{H}(_{H})} [_{j=0}^{H}_{j}^{}(_{j})r_{j}]\] (12)

We leverage methods used to establish the policy gradient theorem  to derive a convenient form for the gradient of the optimal stopping objective w.r.t policy parameters \(\).

**Theorem 4.1** (Optimal stopping policy gradient theorem).: _For any differentiable stopping policy \(}\), The gradient of the objective \(J_{OS}()=[R_{}]\) w.r.t. policy parameters \(\), where \(\{R_{j}\}\) is a risk process and \(\) is a stopping-time random variable, is given by:_

\[_{}J_{OS}()=_{_{H} (_{H})}[_{j=0}^{H}r_{j}_{j}^{}( _{j})_{}_{j}^{}(_{j})]\]

_where the \(_{j}^{}(_{j})\) are functions of \(}\) obtained by Lemma 3.1._

The proof is an application of the log-derivative trick  and is given in Appendix A. Note that optimal stopping policy gradients are estimated using offline state trajectories only. Unlike the general RL case, we never need to do policy rollouts to sample trajectories. Instead, we explicitly calculate the probability distribution over all possible stopping actions (or stopping times) and use this to average over all possible stop-continue decisions. Also, an update to the policy parameters can be done without fresh data collection since we can correctly (due to explicit Bayes net modeling) model the impact of the updated policy on the loss using the already sampled environment trajectories. Appendix B describes specialization to settings with costs instead of rewards.

We now establish a surprising equivalence between the reweighted E-M algorithm of Corollary 3.1.1 and the policy gradient method derived in this section when an incremental E-M solution approach  is used. Although the objective functions \(J_{WML}(},)\) and \(J_{OS}()\) are different in general, the same procedure maximizes both.

**Proposition 4.1** (Incremental E-M as a policy gradient method).: If a partial M-step is used in the reweighted E-M approach of Corollary 3.1.1, comprising of a single gradient-step, then the resulting incremental E-M procedure is equivalent to the optimal stopping policy gradient method of Theorem 4.1, each converging to a local maximum of both \(J_{WML}(},)\) and \(J_{OS}()\).

The proof is given in Appendix A. Thus, we may view the incremental E-M approach that optimizes a weighted maximum likelihood objective as a generalization of the policy gradient method that seeks to optimize the expected reward objective. The two are equivalent for a particular weighting scheme and Bayes net specification (as in Corollary 3.1.1). Further, the OSPG gradient has the following insightful form, obtained by applying Lemma 3.1 to the M-step objective of Theorem 3.1.

**Corollary 4.1.1** (Value form of the OSPG gradient).: _Let \(v_{ij}:=_{j}^{}(_{ij})r_{ij}\), \(k_{ij}:=[_{n=j+1}^{H}v_{in}]\). We may rewrite the policy gradient of Theorem 4.1 as:_

\[_{}J_{OS}()=_{i=1}^{N}_{j=0}^{H} [(1-_{j}^{}(_{ij}))-k_{ij}_{j}^{ }(_{ij})}{_{j}^{}(_{ij})(1- _{j}^{}(_{ij}))}]_{}_{j}^{ }(_{ij})\] (13)The proof is given in Appendix A. \(v_{ij}\) is an empirical estimate of immediate reward, while \(k_{ij}\) is an empirical estimate of continuation value under the current policy. The term in the bracket is the the derivative of the objective w.r.t. the policy output, evaluated at \(_{ij}\). This is positive, and hence calls for increasing stopping probability, if \(}{k_{ij}}>}_{i}(_{ij})}{1- ^{}_{i}(_{ij})}\), i.e. when the ratio of immediate to continuation value exceeds the current odds of stopping. Of course, the policy is updated considering expected immediate reward and continuation values at all time-steps. The updated policy produces new estimates of immediate reward and continuation value from which a new OSPG gradient is computed, and process proceeds until convergence to an optimal policy. Thus, this process implicitly uses value functions at all time-steps to update the policy which implicitly further improves value functions. Since there is no recursion across time-steps OSPG is able to mitigate the curse of non-Markovianity.

**Temporal loss functions for learning RNN stopping policies:** We may directly adopt the M-step objective of Theorem 3.1 as a loss function for learning an RNN-based policy with mini-batch stochastic gradient-descent (SGD), effective in non-Markovian settings. Algorithm 1 shows the pseudocode for computing our OSPG loss. This loss is ideally suited for learning stopping policies with RNNs since it takes as input stopping probabilities at every time-step \(^{}_{j}(_{ij})\) that are each a function of process observation up to that point. An RNN performs this mapping naturally since it maps input history to a hidden state at each time-step with recurrent computation. So, for example, return_sequences=True, coupled with a TimeDistributed Dense layer in Keras, returns a complete trajectory of RNN predictions for input into the loss. Note also that this is a temporal loss that considers the entire process trajectory and does not decompose across time-steps.

``` Input:\(:=[r_{ij}]\), \(:=[^{}_{j}(_{ij})]\) {\(1 i N_{b},0 j H\), \(N_{b}\) is batch size} \(_{:,0}=_{:,0}\), \(_{:,H}=(_{j=0}^{H-1}(1-_{:,j}))\) {numerically stable computation of \(^{}_{j}(_{ij})\)} \(_{:,1:H-1}=((1-_{:,H-2})+_{:,1:H-1})\) {U is unit upper triangular, CUMSUM} \(=()\) {held constant, so no gradient. \(\) denotes Schur product} Output:\(J=-}^{T}[]\) ```

**Algorithm 1** Pseudocode for mini-batch computation of our temporal OSPG loss

## 5 Experiments

We compare our OSPG algorithm using deep neural networks (DNN-OSPG) and recurrent neural networks (RNN-OSPG) against the following model-free discrete-time optimal stopping approaches.

**Backward Induction**: Deep Optimal Stopping (DOS)  including an extension of state space with process history (DOS-ES)  and Randomized Recurrent Least Square Monte Carlo (RRLSM) .

**Fitted Q-iteration**: While fitted Q-Iteration (FQI) approaches in the optimal stopping literature rely on linear approximation with chosen basis functions [39; 40] break down in high-dimensional settings, they may be readily extended to leverage deep neural network (DNN-FQI) and recurrent neural network (RNN-FQI) function approximators. Appendix C provides details of our corresponding DNN-FQI and RNN-FQI baselines.

We conduct experiments with both Markovian and non-Markovian datasets. Markovian data include the pricing of high-dimensional Bermudan max-call options and American geometric-call options (relegated to Appendix D due to space). Non-Markovian data includes stopping a fractional Brownian motion and the early stopping of a sequential multi-class classifier. Except for the American option pricing experiment, all DNN function approximators use two hidden layers of \(20\) hidden units each, while all RNN function approximators leverage an RNN/GRU with a single hidden layer of \(20\) units. Backward induction methods (DOS, RRLSM) require independent networks at each time-step, while other approaches share network parameters across time-steps. Details of hyper-parameter settings, model size, and compute times are provided in Appendix D.

**Exercise of a Bermudan max-call option:** In this Markovian setting, we seek optimal strategies to maximize the returns of a Bermudan max-call option, a scenario described in . Bermudan options may be exercised only at fixed times, unlike American options which may be exercised at any time prior to option expiration. The payoff of these options depends on the maximum of \(d\) underlying assets with multi-dimensional Black-Scholes dynamics and the strike price \(K\). The dynamics and 

[MISSING_PAGE_FAIL:9]

classification, the easier the task becomes since every new time-step reveals information about the class of the series. When deciding on the class of a given series, there is a tradeoff between the number of observations made and classification accuracy. Early stopping of a classifier is an optimal stopping problem with costs (instead of rewards): \(_{}[C_{}]\) using a cost process \(\{C_{j}\}\) with \(C_{j}=[1-_{k}(Y=k|_{j})]+\). Here, \(Y\) represents the actual class of the time series, with \(_{j}\) representing the process history of the time series until time \(j\). The first term represents the probability of a classification error at time \(j\) while the second term imposes a cost of delaying the classification decision, controlled by \(^{+}\). In practice, the true probabilities \((Y=k|_{j})\) are unknown and must be approximated by training a classifier to get a cost model. This approach is related but not equivalent to early classification literature [26; 13; 9; 26; 1] where the goal is to learn both classifier and stopping decision jointly. Since our intent in this paper is to solve the optimal stopping problem, not optimize the cost/reward models, we train a simple baseline RNN classifier that learns parameterized approximations \((Y=k|_{j},)(Y=k|_{j})  j,k\), with parameters \(\). A limitation of this approach is that we are constrained by the quality of the trained classifier. Our classifier is an RNN with a single 20-unit hidden layer connected to a final time-distributed dense layer with softmax activation which produces a probability distribution over classes at each time-step. We train this model to minimize negative log-likelihood (NLL) to produce a calibrated classifier.

There are two possibilities for the cost model. Either we accept the probabilities estimated by the classifier as the true probabilities yielding: \(C_{j}^{est}=[1-_{k}(Y=k|_{j},)]+\), or use empirical miss-classification error, yielding: \(C_{j}^{emp}=(Y*{arg\,max}_{k}(Y=k| _{j},))+\). The latter is a more robust choice and is the evaluation metric used since the stopping decisions will not be blind to classifier calibration errors. However, this setting exposes a significant limitation of value-function methods (backward induction and FQI) since they require the cost model to be available at inference time. Thus, they can only use \(C_{j}^{est}\) and not \(C_{j}^{emp}\), since during inference, empirical errors of the classifier are not revealed until a decision to stop has been made.

We select 17 multi-class time-series classification datasets from the UCR time-series repository  (see Appendix D for details of the datasets and selection process). We use 10-random \(70\%/30\%\) (train/test) data splits to train all models, holding out a \(30\%\) of training data as a validation set. Figure 3(b) reports the average stopping costs of the policies learned by the various methods on the test set. RNN-OSPG outperforms backward-induction and FQI methods on 15 of the 17 datasets.

## 6 Conclusions

We introduce a policy gradient algorithm well suited to learning RNN-based optimal stopping policies effective in non-Markovian settings. Our optimal stopping policy gradient (OSPG) algorithm leverages a new Bayesian net formulation of non-Markovian state-action-reward trajectories specific to optimal stopping. This formulation leads to an offline policy gradient algorithm where Bayes net inference eliminates inefficient Monte-Carlo policy rollouts. OSPG outperforms competing methods in non-Markovian settings where existing methods must either deal with the curse of dimensionality (from extended state spaces) or the curse of non-Markovianity (from recursive Markovian approximation). A limitation of OSPG is that since it uses a temporal loss defined over complete trajectories, it may not be suitable for scenarios with only a few extremely long trajectories or when trajectories are partially observed.

Figure 2: Fractional Brownian motion Figure 3: Stopping a multi-class classifier