# Latent Learning Progress Drives Autonomous Goal Selection in Human Reinforcement Learning

Gaia Molinaro

University of California, Berkeley

gaiamolinaro@berkeley.edu

&Cedric Colas

Massachusetts Institute of Technology

ccolas@mit.edu

Pierre-Yves Oudeyer

Inria Centre at the University of Bordeaux

pierre-yves.oudeyer@inria.fr

&Anne G. E. Collins

University of California, Berkeley

annecollins@berkeley.edu

###### Abstract

Humans are autotelic agents who learn by setting and pursuing their own goals. However, the precise mechanisms guiding human goal selection remain unclear. Learning progress, typically measured as the observed change in performance, can provide a valuable signal for goal selection in both humans and artificial agents. We hypothesize that human choices of goals may also be driven by _latent learning progress_, which humans can estimate through knowledge of their actions and the environment - even without experiencing immediate changes in performance. To test this hypothesis, we designed a hierarchical reinforcement learning task in which human participants (N = 175) repeatedly chose their own goals and learned goal-conditioned policies. Our behavioral and computational modeling results confirm the influence of latent learning progress on goal selection and uncover inter-individual differences, partially mediated by recognition of the environment's hierarchical structure. By investigating the role of latent learning progress in human goal selection, we pave the way for more effective and personalized learning experiences as well as the advancement of more human-like autotelic machines.

## 1 Introduction

Animals often spontaneously explore their environment in the absence of immediate extrinsic payoffs . While seemingly wasteful, such intrinsically motivated behavior is argued to adaptively guide learning and exploration, paying off across developmental and evolutionary timescales . As a result, researchers and engineers started endowing artificial systems with intrinsic reward signals that aim to reproduce aspects of human curiosity-driven learning . A striking aspect of human intrinsic motivation is _autotelic exploration_, wherein people self-generate goals, and reward themselves for achieving them . How people decide which goals to pursue is thus an outstanding issue in the cognitive sciences . Identifying the key variables affecting goal selection may in turn enable the development of artificial agents that intelligently set their own objectives, rather than being optimized for predefined ones .

Alongside factors such as performance and novelty, learning progress (LP) - the derivative of performance with respect to time - has proven a useful signal for goal selection, steering both humans  and artificial agents  away from tasks that are either too simple or too hard. Most existing approaches approximate LP using past observations (e.g., differences in recent and earlier performance ), such that LP is greatest when performance changes rapidly (, but see ). However, there are often situations where no external change is visible, yet some other form of progress toward the desired outcome is made. Imagine being tasked with identifying the correctsequence of numbers to open a combination lock, which you might attempt through trial and error. Throughout most of this scenario, repeated failures would yield no difference in performance, hence no empirical learning progress (as typically defined). Nonetheless, provided that the lock has a limited number of slots and numbers and that you can avoid repeating incorrect combinations, every attempt is a step toward the solution. Based on this observation, we propose extending the definition of LP to include a _latent_ variant, distinct from its manifest counterpart in that it relies on an internal model of the environment and one's behavior in addition to performance. Following this example, we formulate latent learning progress (LLP) as a measure of progress an agent infers by leveraging knowledge about the environment and its interactions with it, e.g., through reasoning. For example, LLP could be tracking the proportion of hypotheses tested over the entire solution space, thus changing with every new set of actions taken and dropping upon identifying the correct one. Unlike LP, LLP does not require observing performance improvements to provide informative signals about progress. At least in certain settings, LLP could be a more valuable signal for goal selection than the classically defined LP, as it proactively estimates future progress without waiting for performance improvements.

Here, we hypothesize that LLP is a signal humans use for goal selection, and that it can be considered separately from LP. Specifically, we test the role of LLP in human goal selection by introducing a novel learning paradigm with goal choices as the key dependent variable. Several features of this environment (detailed in Section 3.1) make it possible to distinguish between LP and LLP and test the latter's contributions to autotelic exploration in a sample of human participants. Among these, a hierarchical component is introduced in the learning environment to further target LLP, which might be sensitive to hierarchy-based inferences leading to sharp changes in the agents' knowledge of the environment. These features make our environment suited for probing LLP and testing its contributions to autotelic exploration in a sample of human participants. By combining behavioral findings and computational modeling of human decisions in the experiment, we find support for our hypothesis that LLP captures human goal choices better than standard LP in this hierarchical reinforcement learning task. We also find inter-individual differences in goal selection strategies, warranting further studies on the factors underlying such idiosyncrasies. In our discussion of the results, we argue that adopting the notion of LLP may foster the development of autotelic machines.

## 2 Related work

Intrinsic motivationThe various forms of intrinsic motivation can broadly be classified as either knowledge-based (KB) or competence-based (CB) [12; 28; 29; 31; 32]. KB intrinsic motivations derive from comparisons between pre-existing and newly acquired information and are well-documented in the animal world (e.g., [33; 34; 35; 6; 36; 37; 1; 3]). In artificial agents, KB signals of novelty, diversity, or prediction error broaden exploration when provided alongside extrinsic rewards [38; 39; 13; 15] or even on their own ([11; 14], see  for review). However, human curiosity is often driven towards stimuli of intermediate complexity [7; 41; 42], rather than extremes. Accordingly, some artificial models aim to maximize an "intermediate" difference between predictions and observations [43; 30]. Intrinsic motivations based on learning progresses (LP) can avoid arbitrary cutoffs for what signifies intermediate complexity. LP is particularly useful as a CB motivation, prompting agents to improve their performance on self-determined goals [44; 29; 32]. Indeed, humans often find pleasure in activities whose difficulty matches their abilities [45; 46]. CB intrinsic motivations highlight the autotelic property of human learning, which is often driven by self-defined objectives. Two concurrent challenges in cognitive science and artificial intelligence involve identifying the signals people use to develop their learning curricula [21; 19] and adapting those principles to machines .

Goal selection in humansGoals, defined as "a cognitive representation of a future object that the organism is committed to approach" , have long been a key concept in psychology (e.g., [48; 49; 50; 51]), with a rich literature on how various types of goals interact with performance and motivation [52; 53]. In cognitive neuroscience, goals are typically imparted by the experimenter and are not the main focus of the analyses  - though recent work illustrated the impact of goals on core aspects of neural, cognitive, and behavioral responses ([55; 56; 57; 18; 20; 21; 58; 59]; see  for review). Nevertheless, among the principles thought to guide human goal selection is the tendency to choose activities that balance the expected outcome's desirability and the probability of attaining it . When free to allocate their time to various tasks or choosing activities for pure enjoyment, people naturally tend to match the activity's level of challenge to their current abilities [45; 46; 61], which might be optimal for learning [62; 63]. However, most studies fail to provide precise computational details on the mechanisms of 

[MISSING_PAGE_FAIL:3]

was forced to ensure participants experienced each goal at least twice. In the final testing trials, participants were prompted to try and make each option 4 times without feedback.

Potions could either be made of basic ingredients ("simple" and "complex" goals G1-G4) or options of different colors ("compound" goals G5-G6) and required either two (simple and compound goals G1-G2, G5-G6) or four actions to complete (complex goals G3-G4; Figure 1, bottom). Two options could be made from either basic ingredients or other options (options for G3-G6). One complex goal ("hierarchical", G3) was made by combining the ingredients of the two simple potions (G1, G2), whereas the other complex goal was not ("non-hierarchical", G4). One complex goal ("hierarchical", G5) was made by flasks of the two simple potions (G1, G2), whereas the other complex goal was not ("non-hierarchical", G6). Thus, G3 and G4, and G5 and G6, were matched in terms of actions required to create the corresponding potions but differed in whether knowledge from other potions could be used to find the corresponding solutions faster. Two additional, simple goals made of the first two and last two ingredients of G4 were presented to participants in the testing phase (G7 and G8; Appendix B). Unless otherwise specified, we focus on goals G1-G6, which participants engaged with in the training and learning phases. Further details about the goal space are provided in Appendix B. The information displayed in Figure 1 was never shown to participants. The correspondence between goal identity, color, position on the screen, and solution was pseudo-randomized across participants.

Various features make the experimental design well-suited to study the contributions of LLP to goal selection by helping decorrelate LP from LLP. First, the correct response was static and signaled through _deterministic feedback_. While the standard LP measure only flattens upon repeated subsequent attempts, deterministic feedback enables LLP to reach its limit after a single correct response. Second, the _action space is limited_ and _solutions are unique_, so negative feedback can be used to exclude the attempted action sequence from the set of possible solutions. This feature was especially meaningful for studying LLP, which can be immediately updated following failed attempts with a novel action sequence. Third, the choice space is vast enough that _rewards are sparse_ early on in learning, which further decorrelates LP and LLP since LP is 0 until the first successful attempt, while LLP is updated at the first trial no matter the outcome. Fourth, _hierarchical relationships_ exist among some, but not all goals. Upon becoming aware of hierarchical relationships, participants could take shortcuts to identify the correct recipe for the goals involved, leading to sharp changes in LLP even with small differences in performance. This feature additionally enables us to test for hierarchy effects on goal selection beyond the indirect effects of hierarchy on learning through LLP.

### Computational modeling

On each trial, participants first selected a goal and then a sequence of actions. Unless otherwise specified, we only modeled goal selection, conditioning on true participants' actions within each trial after goal selection. However, action selection influences performance, LP, and LLP. As in existing studies , we model human goal selection as a multi-arm bandit problem, where the probability of selecting a goal depends on its subjective value relative to other goals. Multiple factors \(f\) may jointly contribute to the overall subjective value of a goal [23; 24], which is thus calculated as a weighted sum of the goal's value relative to each factor \(V_{f}\) times its weight \(_{f}\). The probability \(P^{t}(g^{*})\) of choosing goal \(g^{*}\) among possible goals \(G\) on trial \(t\) is obtained through a softmax function over the goal values (as is standard in the decision-making literature ):

\[P^{t}(g^{*})=_{f} V_{f}^{t}(g^{*}))}{_{g G }exp(_{f}_{f} V_{f}^{t}(g))}\]

Subjective goal values are updated as a function of experience through the delta rule :

\[V_{f}^{t+1}(g^{*})=V_{f}^{t}(g^{*})+_{f}^{t}(g^{*})\]

where \(\) is a learning rate for value updates and \(_{f}\) is factor-dependent. Models differed in which factor, or combination of factors, composed the overall goal value. Below we detail the individual factors we considered.

PerformanceOn each trial, the utility of the selected goal with respect to performance is updated based on the goal-contingent feedback received on that trial \(r^{t}\) (1 for positive feedback, 0 for negative feedback):

\[_{}^{t}(g^{*})=r^{t}-V_{}^{t}(g^{*})\]

Performance estimates were initialized as \(V_{}^{0}(g^{*})=\) where 6 is the number of goals.

Learning progressLearning progress (LP) was updated through the change in reward prediction error for the current goal after initializing \(V^{0}_{}(g^{*})=0\) (see  for a similar approach). "Reward", here refers to the feedback \(r\) participants received on each trial. However, note that no external rewards were delivered for successfully obtaining the desired option (Appendix C). On each trial, the reward prediction error was calculated as the difference between the current performance value estimate and the feedback obtained. The reward prediction error on trial \(t\) was compared to the reward prediction error on the previous trial \(t-1\):

\[^{t}_{}(g^{*})=[(r^{t}-V^{t}_{}(g^{*}))-(r^{t-1} -V^{t-1}_{}(g^{*}))]-V^{t}_{}(g^{*})\]

Latent learning progressGoal utilities with respect to latent learning progress (LLP) were updated based on the difference between the estimate at trial \(t\) and the current trial's LLP:

\[^{t}_{}(g^{*})=LLP^{t}_{g^{*}}-V^{t}_{}(g^{*})\]

with \(V^{0}_{}(g^{*})=1\). Until the goal was learned (when LLP immediately became 0) subjective (unobservable) estimates of LLP were approximated as \(LLP^{t}_{g^{*}}=1-}}{N_{}}\). The number of possible action sequences is 12 for 2-action goals and 24 for 4-action goals. The number of action sequences tested is the sum of unique action sequences the agent has tried up until trial \(t\). A goal is considered learned when the subject has obtained positive feedback for the current goal at least once (but see Appendix D for alternative heuristics and approximations). Therefore, this formulation of LLP captures the size of the space left to explore to find the correct sequence of actions for the current goal, i.e. the "distance" from the solution. As a result, negative \(_{LLP}\) values indicate a high willingness to pursue activities one is close to solving. This choice of formulation - rather than its opposite, i.e. one where high values indicate high levels of progress toward perfect performance - was made to maintain continuity in the LLP function, which is 0 after a solution is found.

HierarchyHierarchical relationships between goals could impact goal selection in at least two ways. First, indirectly, by enabling inferences about the solution for goals involved in the hierarchy, thus speeding up exploration and changes in LLP. Second, directly, by acting on biases people might have for selecting goals that share common structures. To account for the latter, we also included a hierarchy factor, whose estimates were updated via:

\[^{t}_{}(g^{*})=H_{g^{*}}-V^{t}_{}(g^{*})\]

with \(V^{0}_{}(g^{*})=0\) and \(H\) equal to 1 for hierarchical (G3, G5), -1 for non-hierarchical (G4, G6), and 0 for simple goals (G1, G2; but see Appendix D for an alternative scheme and update rule).

Model spaceBecause performance is a powerful motivator in people's decisions about time allocation , we include it in all our models. In addition to a performance-only model, we fit and compared models with additional factors for LP, LLP, hierarchy, and both LLP and hierarchy. Thus, fit parameters included a shared \(\) across factors and weighting parameters \(_{f}\) for each factor in the model. Adding a control variable for choice perseveration improved the likelihood of goal choices but did not affect modeling results (Appendix D), so we removed it to facilitate interpretation.

Model fitting and comparisonModels were fit and compared through hierarchical Bayesian inference (HBI; ) based on the likelihood of participants' goal selection choices . We compare models through protected exceedance probability (PXP), i.e., the probability that each model is the most frequently expressed in the studied population after accounting for chance , and average responsibility (R), which captures how well each model explains participants behavior. Models and parameters for the winning model were recoverable  (Appendix E). Simulating participants' behavior  also required modeling the learning process, which we assumed followed a reinforcement learning architecture with options  (Appendix F).

## 4 Behavioral results

Given the novelty of our experimental paradigm, it was crucial to validate the environment and gather intuitions about participants' behavior. Furthermore, both LP and LLP are influenced by how well participants learn to achieve each goal. We thus first analyze group-level learning and goal selection as a function of goals' difficulty levels and hierarchical structures. We then test the specific role of LLP in goal selection through computational modeling (Section 5).

Participants learn successful goal-conditioned policiesLearning phase performance was above chance for each goal (Wilcoxon tests were used in place of parametric t-tests since the data was not normally distributed according to Shapiro-Wilk tests of normality; chance levels were 0.083 for 2-action and 0.042 for 4-action goals; all average performances \(\) 0.29, all W(174) \(\) 160, all p \(\) 0.004; Figure 2A). As expected, performance was higher for 2-action (M = 0.51 \(\) 0.02) than 4-action goals (M = 0.33 \(\) 0.02, W(174) = 1207, p \(<\) 0.001, r = 0.51). Similar patterns for goals G1-G6 were observed in the testing phase, where performance was not contingent on goal selection (Figure 2B).

Hierarchical inference supports learningIf individuals recognized hierarchical relationships during learning, they could use them to draw inferences about the correct recipes for the potions involved. For instance, upon discovering that G5 could be made from the potions created in G1 and G2, participants may correctly infer that G3 is achieved by chaining the recipes for G1 and G2 (Figure 1, bottom). Consistent with the hypothesis that at least some participants exploited the environment's hierarchical structure, learning performance was higher for hierarchical goals compared to their non-hierarchical counterparts (complex: hierarchical (G3) M = 0.37 \(\) 0.02, non-hierarchical (G4) M = 0.29 \(\) 0.02, W(174) = 4168.5, p = 0.004, r = 0.16; compound: hierarchical (G5) M = 0.53 \(\) 0.02, non-hierarchical (G6) M = 0.49 \(\) 0.02, W(174) = 6036.5, p = 0.032, r = 0.11; Figure 2A). Similarly, participants took fewer attempts to learn solutions to hierarchical than non-hierarchical goals (complex: hierarchical (G3) M = 16.66 \(\) 1.19, non-hierarchical (G4) M = 22.1 \(\) 1.23, W(120) = 1884.5, p = 0.009, r = 0.17; compound: hierarchical (G5): M = 11.66 \(\) 0.58, non-hierarchical (G6) M = 13.21 \(\) 0.53, W(155) = 3889.5, p = 0.03, r = 0.12; Figure 2C). However, hierarchy effects were not present in the testing phase, likely due to ceiling effects (complex: hierarchical (G3) M = 0.56 \(\) 0.03, non-hierarchical (G4) M = 0.52 \(\) 0.03, W(174) = 1592.5, p = 0.223, r = 0.07; compound: hierarchical (G5) M = 0.73 \(\) 0.03, non-hierarchical (G6) M = 0.73 \(\) 0.03, W(174) = 1158.5, p = 0.765, r = 0.02; Figure 2B).

Goal selection is sensitive to performance and hierarchyIn assessing whether difficulty levels and hierarchy affected participants' goal selection, we found it displayed opposite patterns to learning performance (Figure 2D). The average probability of selecting a goal was lower for 2-action (M = 0.15 \(\) 0.00) than 4-action goals (M = 0.19 \(\) 0.01; W(174) = 4214, p \(<\) 0.001, r = 0.27). Overall, participants chose the hierarchical (G3; M = 0.18 \(\) 0.01) less than the non-hierarchical complex goal (G4; M = 0.21 \(\) 0.01, W(174) = 5231.5, p = 0.002, r = 0.17). Differences between hierarchical (G5; M = 0.15 \(\) 0.01) and non-hierarchical compound goal selection were not significant (G6; M = 0.16 \(\) 0.00, W(174) = 6444, p = 0.128, r = 0.08). At the individual level, the greater the

Figure 2: Behavioral and modeling results. **(A)** Learning performance was above chance and showed hierarchy effects. **(B)** Test performance was better than chance, but no significant hierarchy effects were detected. **(C)** On average, fewer attempts were needed to learn hierarchical (G3) compared to non-hierarchical (G4) 4-action goals. **(D)** Partial hierarchy effects are present in goal selection. The winning model (triangle markers) reproduces this pattern. **(E)** Model responsibilities for individual participants and the overall studied population. **(F)** Best-fitting parameters (HBI) for the winning model. Bars and shading indicate the SEM, dots individual participants. *** p \(<\) 0.001, ** p \(<\) 0.01

difference in performance between 2-action and 4-action goals, the lower the probability of selecting a 2-action goal (Spearman's \(\) = -0.35, p \(<\) 0.001), and the greater the performance difference between hierarchical and non-hierarchical goals, the lower the probability of selecting a hierarchical goal (complex: \(\) = -0.68, p \(<\) 0.001; compound: \(\) = -0.51, \(<\) 0.001). These results indicate that goal selection depends on performance and hierarchical relationships between goals. We next turn to computational modeling to detail the contributions of performance, hierarchy, LP, and LLP to goal selection.

## 5 Modeling results

We that predicted LLP supports human goal selection, thus expecting models relying on this signal to fit participants' goal selection data better than models that do not.

Latent learning progress guides goal selectionThe model with performance and LLP utility factors was the most strongly represented across participants (PXP = 1; R = 0.63) and replicated the main goal selection patterns seen in participants' data (Figure 2E). Some participants were also well-captured by a model that additionally integrated hierarchy into goal values (R = 0.22). We note that hierarchy likely plays a more direct role in learning (thereby impacting LLP) than in goal selection, given that hierarchy effects in goal selection and performance could be recovered after eliminating hierarchical components from goal selection, but not learning (Appendix G). The model selecting goals based on performance and LP did not account well for participants' behavior (R = 0.12). Models with just performance (R = 0.00) or performance and hierarchy factors (R = 0.04) were marginally represented. The winning model's best-fitting \(_{}\) and \(_{}\) were on average negative, suggesting most individuals focused on activities where performance was low - indicating the solution had yet to be found - and where LLP was low (Figure 2F) - indicating learning potential was high (Section 5). However, more nuanced patterns and temporal dynamics may exist at the individual level.

## 6 Inter-individual differences in learning and goal selection

Several interesting strategies emerged upon inspecting individual behaviors (Figure 3). We first exemplify them before using a data-driven approach to classify participants according to their approach to the game, revealing differences in how hierarchy impacts learning and thus goal selection. Finally, we assess whether LLP supports goal selection across groups.

People express a variety of goal and action selection strategiesMany participants repeatedly selected the same goal - often testing possible solutions in a systematic order (Figure 3A) - only switching to a new goal after learning the current one (Figure 3A,B). After learning to make all options, some participants began alternating goals, as if testing or training their ability to retain multiple solutions (Figure 3A,B). Although most participants showed interest in learning all goals (Figure 3A,B), a small subset of individuals preferred setting few objectives, or even a single goal, in which they excelled (Figure 3C), while disregarding others. Lastly, some participants moved across goals and ingredients in a seemingly unprincied manner, presumably because they were confused or unmotivated (Figure 3D). A subset of the participants seemed to be aware of adopted strategies (see Appendix H for example verbal reports). We next clustered participants to explore how idiosyncrasies in learning related to goal selection. Note that participants in Figure 3 were chosen as examples and do not directly relate to the clustering analyses presented below.

Figure 3: Example behaviors from four participants. **(A-D)** Chosen action sequence as a function of trial number (training and learning phases), color-coded by goal and feedback.

Clustering of individual strategies reveals differences in hierarchical learningWe used K-means clustering  to identify 4 groups of participants, where K was determined through the elbow method (Appendix I). The behavioral metrics used to identify the clusters (e.g., performance and goal selection entropy and distributions) can be found in Appendix I. To avoid statistical double-dipping , we limit ourselves to qualitative observations on cluster-based behavior. To better illustrate salient characteristics of each cluster, we also identified "archetypal" participants through archetypal analysis - a method, often used to profile video game players , which detects data points that can reproduce the original population when linearly recombined . We then matched each archetypal feature with the best-aligned cluster (Appendix I) and identified the participant closest to the corresponding archetype as an extreme example of the cluster it belonged to (Figure 4M-P).

Participants in cluster 1 (N = 39) had below-average performance at learning (M = 0.18 \(\) 0.03) and testing (M = 0.27 \(\) 0.06) and favored 2-action goal. They often failed to learn or retain solutions and switched goals frequently (Figure 4M). In the largest cluster 2 (N = 75; Figure 4B), performance was excellent at learning (M 0.59 \(\) 0.0) and testing (M = 0.86 \(\) 0.03). This group leveraged hierarchical structures to speed up learning (Figure 4B,F) - which also impacted goal selection. Non-hierarchical goals were favored (Figure 4J) and often learned later than hierarchical ones (Figure 4N). Cluster 3 participants (N = 53) showed relatively high performance at learning (M = 0.26 \(\) 0.04) and testing (M = 0.62 \(\) 0.06; (Figure 4C), but seemed to struggle with all 4-action goals - performing worse in them but choosing them more - and took less advantage of hierarchical relationships across goals (Figure 4G,K,O). Given its small size (N = 8), we invite caution in interpreting the results for cluster 4. However, this group showed an interesting, strong tendency to repeatedly select the same, small set of goals at which they succeeded (often preferring 2-action goals; Figure 4L,P). As a result, their performance was excellent in the learning phase, where goals were freely chosen (M = 0.67 \(\) 0.13, Figure 4D,L) but dropped in the test phase, where goals were imposed (M = 0.46 \(\) 0.16).

Despite such distinct patterns of behaviors, LLP was a central component of goal selection in all clusters (Figure 4Q-T), suggesting LLP is a flexible signal that adapts to idiosyncrasies in people's learning while similarly guiding goal selection (but see Appendix G for further discussion of how learning interacts with goal selection). At the same time, the best-fit parameters of each cluster captured specific properties of the different groups (Figure 4Q-T insets). The efficient learners of cluster 2 relied more strongly on LLP (cluster 2 M\({}_{_{}}\) = -3.17 \(\) 0.18, other clusters M\({}_{_{}}\) -2.46 \(\) 0.16, Mann-Whitney U(173) = 2696, p = 0.001, r = 0.24) and updated both performance- and LLP-based goal values faster than other groups (cluster 2 \({}_{}\) = 0.73 \(\) 0.03, cluster 2 \({}_{}\) = 0.6 \(\) 0.03, U(173) = 4979, p \(<\) 0.001, r = 0.28). Cluster 3, which seemed less likely to rely on hierarchy, was more attuned to performance when selecting goals (cluster 3 M\({}_{_{}}\) = -4.63 \(\) 0.22, other clusters M\({}_{_{}}\) = -3.67 \(\) 0.16, U(173) = 2274, p = 0.002, r = 0.24)). Cluster 1, which lacked precise strategies, was less sensitive to both performance (cluster 1 M\({}_{_{}}\) -2.93 \(\) 0.26, other clusters M\({}_{_{}}\) -4.25 \(\) 0.14, U(173) = 3778, p \(<\) 0.001, r = 0.31) and LLP (cluster 1 M\({}_{_{}}\) = -1.74 \(\) 0.16, other clusters M\({}_{_{}}\) -3.06 \(\) 0.14, U(173) = 3980, p \(<\) 0.001, r = 0.36).

## 7 Discussion

Human behavior is often motivated by self-defined challenges. Despite the centrality of goals in human learning , how people select their own objectives without external guidance or incentives remains to be specified. Following advances in artificial open-ended skill learning [26; 27; 28], learning progress (LP) has emerged as a useful signal for human goal selection [23; 24]. Here, we introduced a "latent" form of LP (LLP), which is informed by an agent's model of the environment and memory of the actions pursued in it, in addition to the manifest changes in performance tracked by standard LP. In a purposefully developed hierarchical reinforcement learning task, we find that people's goal selection is driven by LLP, calling for an extension of the LP notion to include its latent variants.

Inter-goal relationships in our test environment enabled us to check whether hierarchy additionally drives goal selection, as intuition suggests (e.g., one might decide to become competent in baking by first relying on ready-made dough and only later setting the lower-level goal of making dough from scratch). We find that hierarchy impacts goal selection indirectly by enabling inferences and thus affecting LLP. That is, participants' learning is sensitive to hierarchical structures, leading to changes in LLP which, in turn, affect goal selection. We also gathered partial evidence for a standalone role of hierarchy in curriculum development, which further studies may address.

We found marked inter-individual differences in strategies for goal setting and pursuit. However, our sample was restricted to a relatively homogeneous group of undergraduate university students. Future studies and computational models may extend participation to a broader subject pool and specifically address individual differences in goal selection and achievement, as well as their relationship with demographics and cultural background, cognitive abilities, and psychopathology (cf. ), which might inform personalized teaching strategies and productivity tools. As our knowledge of human goal setting becomes more precise, however, ethical concerns regarding the use of behavioral sciences in marketing and management should be addressed, particularly in cases where highly personalized methods of influence could transform advertising into manipulation.

Elucidating the computational mechanisms driving human goal selection may propel advances in open-ended machine learning. Our suggestion, wherever relevant, is to incorporate LLP in artificial autotelic agents' goal selection. Compared to classic LP, LLP is sensitive to latent changes in an agent's abilities - which LP would only be able to capture after repeatedly observing positive feedback. Using LLP could result in a faster adaptation of the agent's goal selection, ultimately speeding up its learning by enabling it to detect progress before receiving the first reward and letting it proceed to more challenging tasks sooner.

Some limitations of the present study may need to be addressed before other fields can fully benefit from the results we presented. In our task, people chose goals from a predefined menu of options. Although this facilitates the study of goal setting, people often invent their own goals by combining observations and imagination . For simplicity, our modeling focused on goal selection. Future research, however, should jointly model goal choices and goal-conditioned action selection, since - as our data suggests - learning participates in complex interactions with the goal selection process. While we provide a simple and task-specific formalization of LLP, a generalized definition is necessary to understand the differences between LLP and other intrinsic motivation signals and ease the implementation of LLP-based goal selection in autotelic machines. By providing initial evidence for LLP, we hope to inspire the establishment of even more precise signals for goal selection in both humans and artificial agents.

Figure 4: Behavioral and modeling results by cluster (1-4, left to right). **(A-D)** Learning performance. **(E-H)** Probability that each goal was learned over the number of times it was selected. **(I-L)** Goal selection probabilities. **(M-P)** Behavior of each cluster’s closest-matching archetypal participant. **(Q-T)** Model responsibilities and best-fitting parameters.