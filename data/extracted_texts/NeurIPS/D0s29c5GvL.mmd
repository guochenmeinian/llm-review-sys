# CosAE: Learnable Fourier Series for

Image Restoration

 Sifei Liu, Shalini De Mello, Jan Kautz

NVIDIA

{sifeil, shalinig, jkautz}@nvidia.com

###### Abstract

In this paper, we introduce Cosine Autoencoder (CosAE), a novel, generic Autoencoder that seamlessly leverages the classic Fourier series with a feed-forward neural network. CosAE represents an input image as a series of 2D Cosine time series, each defined by a tuple of learnable frequency and Fourier coefficients. This method stands in contrast to a conventional Autoencoder that often sacrifices detail in their reduced-resolution bottleneck latent spaces. CosAE, however, encodes frequency coefficients, i.e., the amplitudes and phases, in its bottleneck. This encoding enables extreme spatial compression, e.g., \(64\) downsampled feature maps in the bottleneck, without losing detail upon decoding. We showcase the advantage of CosAE via extensive experiments on flexible-resolution super-resolution and blind image restoration, two highly challenging tasks that demand the restoration network to effectively generalize to complex and even unknown image degradations. Our method surpasses state-of-the-art approaches, highlighting its capability to learn a generalizable representation for image restoration. The project page is maintained at https://sifeiliu.net/CosAE-page/.

## 1 Introduction

Training an image Autoencoder by reconstruction is one of the most commonly adopted approaches for image representation learning. At the heart of such a learning process is creating an information bottleneck: the network first downsamples the input image to lower spatial dimensional features, then upsamples them back to reconstruct the input image. The key motivation of an information bottleneck is to enhance a network's ability to capture and preserve key data intrinsic patterns and structures, thereby representation and generalization, which is particularly vital in vision tasks where data quality is critical to performance. Introducing a narrow bottleneck in Autoencoder provides an effective way not only to learn general-purpose representations for recognition tasks , but also to learn disentangled representations for image synthesis and manipulation .

To establish an information bottleneck, most existing Autoencoder networks compress input images into a spatially compact latent space . They learn mid- to high-level representation for classification and attributes disentanglement, by extracting the main shared structures while sifting out any noisy components. _However, a narrow bottleneck Autoencoder are rarely directly designed for image restoration task_, mainly because the use of a downsampled bottleneck in conventional Autoencoders often results in the loss of spatial details. Thus, these Autoencoders struggle to effectively represent high-frequency details, such as textures, compared to more coarse-grained structures. In other applications where pixel regression quality make a key role, e.g., VQVAE  or latent diffusion models (LDM) , researchers address this challenge by striking a balance between the spatial compactness of the bottleneck and its capability to preserve details. For instance, in training the Autoencoder  used for , the downsampling stride is typically capped at \(16\). Strides higher than this often result in considerable degradation of image quality .

In this paper, we propose a novel Autoencoder directly applicable for image restoration - not only does it possess an extremely narrow bottleneck to boost representation capability, but it also faithfully preserves high-fidelity details. Our solution, the Cosine Autoencoder (CosAE), draws inspiration from Fourier transform principles, which demonstrates that finite signals can be depicted using a set of harmonic basis functions. We follow this principle to encode frequency domain coefficients, including amplitudes and phases, and integrate them with a set of Cosine basis functions before being decoded back to the original image (Figure. 1 (b)). This is achieved through a Harmonic Construction Module (HCM) that spatially expands the compact feature into a series of harmonic functions, following the formulation of the classical Fourier series. Unlike downsampled feature maps from the conventional Autoencoders, we found in CosAE, the representation with frequency coefficients can be extremely compressed spatially. Yet, these representations can effectively reconstruct fine-grained details when combined with high-frequency basis functions, e.g., the cosine basis in the 2nd row in Figure. 1.

As a generic Autoencoder, CosAE can be potentially utilized across various applications where traditional Autoencoders are applicable. In this paper, we demonstrate its capability for two challenging pixel reconstruction tasks: (a) flexible-resolution super-resolution, and (b) blind image restoration. We note that for tasks such as super-resolution, image denoising, and image enhancement, it is commonly believed that maintaining as many details as possible in the input without using a bottleneck will lead to better restoration [8; 9; 10]. However, the absence of an information bottleneck can potentially limit the capacity for representation. In contrast, the narrow bottleneck in CosAE encourages the alignment of the distributions of image representations under different types and levels of distortions. Unlike most previous work [11; 12] where distinct networks are needed for restoring images of different types of degradation, a single CosAE network demonstrates strong generalization capabilities across a wide range of degradation. We summarize our contributions as: (i) We propose CosAE, a novel generic Autoencoder featuring an extremely narrow bottleneck while preserving high-fidelity image details. (ii) We introduce a basis construction module, a key component of CosAE, that decomposes and represents not only the coarse structure but also the fine details of an image. (iii) CosAE can effectively restore images without the need for degradation types or up-sampling ratios during inference. This capability makes it highly scalable in restoring images with unknown or complex types of degradation.

## 2 Related Work

**Autoencoder Networks.** Training Autoencoders  is a common approach for representation learning. By creating a bottleneck representation, Autoencoders abstract input data into generic visual representations. This method is used in Denoising Autoencoders (DAE) for image restoration and Variational Autoencoders (VAE) for image synthesis. Recent works have shown that Autoencoders can learn general representations for downstream tasks [13; 14; 15; 16; 4] and disentangled representations for image synthesis [17; 18; 5; 19]. While bottleneck representations have proven effective, recent works on RGB pixel reconstruction tasks [8; 9; 10] typically aim to maintain high spatial resolution during auto-encoding to preserve input details for better image reconstruction. In contrast, our work revisits the bottleneck concept, making it extremely narrow to achieve both generic representation and high-fidelity reconstruction.

**Image Restoration.** General image restoration aims to recover and enhance damaged or degraded images to improve their visual quality, clarity, and perceptual information. Within it, super-resolution [20; 21; 22; 23; 24; 8] and image denoising [25; 26; 27; 28; 29; 30] are the two major

Figure 1: Unlike (a) a conventional Autoencoder, (b) CosAE encodes frequency domain coefficients as extremely narrow bottleneck feature maps. Via a basis construction module, it faithfully represents both the global structure and fine-grained details of an input image.

sub-problems. Unlike conventional approaches that rely on specific corruption operators or separate networks for different levels of degradations, recent advancements have focused on learning more generalizable representations to handle complex and unknown degradations. Among them, flexible-resolution super-resolution [31; 32; 33] and blind image denoising [31; 34; 35] are widely studied. Our work focuses on these two applications. While most prior methods use encoders with minimal down-sampling, we show that a narrow bottleneck combined with continuous representation modules like LIIF  achieves consistent reconstruction across various corruption levels. In blind image restoration, recent networks rely on priors from pretrained models  or learnable dictionaries [37; 38], which complicate the training process and require additional measures to balance realism and fidelity. In contrast, CosAE maintains the simplicity of conventional Autoencoder training without such dependencies.

**Fourier Space.** Recently, methods like SIREN  and Fourier feature mapping [39; 40; 41; 42] have been used to preserve high-frequency details in networks. SIREN  utilizes the sinusoidal function as an activation layer to reconstruct high-frequency components, while other approaches [40; 41; 42] transform inputs into Fourier coefficients or feature maps to enhance detail retention. Frequency representations are proposed for texture synthesis [43; 19] and image super-resolution [32; 44]. Among image super-resolution works, the Fourier module of LTE  focuses on high-frequency residuals via a skip-link, limiting its capability to learn compressed frequency domain representations, while OPE-SR  employs a parameter-free decoder for pixel space decomposition using 1D Fourier basis, unlike CosAE, which uses a 2D Fourier basis to enable more compact information bottleneck representation.

## 3 Approach

Similar to conventional Autoencoders, CosAE has an encoder to produce a bottleneck latent representation (Sec.3.2) and a decoder to reconstruct the input image (Sec.3.5). Within the bottleneck, we introduce a harmonic construction module (HCM) (Sec.3.3) to spatially expand the latent representation into Fourier series (Figure.1 (b)). We start by revisiting the classic 2D Fourier transform theory  and its harmonic functions (Sec. 3.1). Furthermore, we delve into the details of the proposed HCM, showing how to integrate the classic Fourier transform into a latent space in neural networks.

### Preliminary of Fourier Series

Fourier series is an effective tool to represent periodic or finite signals with a group of sinusoidal functions. Formally, the periodic extension of a finite 1D signal \(\{x(t),t<T\}\) can be represented with Fourier series in the amplitude-phase form:

\[x(t) }{2}+_{k=1}^{}_{k}\] (1) \[(k,t) =A_{k}(kt-_{k})\]

where \(k\) denotes the discrete frequencies, \((k,t)\) denotes the \(k^{th}\) harmonic component of the signal. Within \((k,t)\), we have the scalars \(A_{k}\) and \(_{k}\) as the amplitude and the phase shift. Specifically,

Figure 2: **Overview of CosAE.** CosAE contains an encoder that compresses an input image into a narrow bottleneck. Each bottleneck vector \([A_{n},_{n}]\) is translated into a group of learnable harmonic functions \(\) of size \(T T\), according to Eq. (2) (see the box). A decoder then reconstructs the input from the learned harmonic functions. Note \(A_{n}\) equals to \(A_{(u_{n},v_{n})}\) in Eq. (2) for simplicity.

denotes the amplitude of the DC component. The Fourier series in Eq. (1) can be easily generalized to n-dimensional cases. In the context of a finite 2D signal, e.g., an image, the harmonic function with frequencies \((u,v)\) corresponding to the \(x\) and \(y\) dimensions is denoted as:

\[(u,v,x,y)=A_{(u,v)}[(ux+vy)-_{(u,v)}]\] (2)

where \(x<T\) and \(y<T\). We denote \([(ux+vy)]\) as a Cosine basis function, and the full term \((u,v,x,y)\) as harmonic function.

In our 2D domain, the amplitude and phase of an input image can be derived using the Fourier transform (e.g., 2D FFT) . However, Fourier coefficients are neither learnable nor directly compressible for forming an information bottleneck, and thus cannot directly facilitate image restoration or visual representation learning tasks. Instead of applying FFT, we learn the amplitude and phase, denoted as \(A_{(u,v)}\) and \(_{(u,v)}\), as bottleneck feature maps via an encoder, allowing flexible dimension configuration. To reconstruct the original image, we use Harmonic Component Modeling (HCM) to generate a set of learned harmonic functions (Eq. (2)), and a decoder acts as a learnable summation operator, mimicking the reconstruction process in Eq. (1).

The key insight is the compactness of Fourier coefficients in representing images, even those with intricate details. For example, a complex texture image may have only a few significant frequency components. By making Fourier coefficients a learnable representation, we can design an Autoencoder with an extremely narrow bottleneck.

### Encoding Fourier Coefficients

The encoder of CosAE is responsible for learning Fourier Coefficients of the bottleneck latent space. Given a 2D square \(P P\) image patch \(I_{p}\), the encoder compresses the input signal into a bottleneck feature vector with the dimension of \(2 c\), representing \(c\) pairs of corresponding amplitudes and phases,

\[[A^{c},^{c}]=(I_{p}).\] (3)

To generalize to images of any resolutions, in practice, we design the encoder with the downsampling stride setting as \(P\), so that \(A\) and \(\) are both \(c\) channel-dimensional feature maps with its spatial resolution \(1/P\) as that of the input image. In CosAE, a relatively large stride \(P\) will be adopted so that the bottleneck can be very small: for face images, we maintain a downsampling stride as \(P=64\) (resulting in a 1D bottleneck of \(^{2 c}\) for a \(64 64\) patch, see Eq. (3)), while for natural images, we use a stride as \(P=32\). Remarkably, CosAE is still able to faithfully reconstruct high-frequency details through combining these coefficients with the Cosine basis functions as latent feature maps, as introduced in the following.

### Constructing Harmonic via HCM

We introduce a harmonic construction module (Figure. 2, middle box) that operates on the bottleneck to transform the predicted coefficients coming from the encoder into a set of 2D harmonic functions, i.e., \(\) in Eq. (2). Taking the 2D square patch \(I_{p}\) as an example. The encoder processes this patch to yield \(c\) pairs of amplitudes \(A^{c}\) and phases \(^{c}\). These are then combined with \(c\) corresponding Cosine basis functions. Each basis is a \(T T\) 2D Cosine waveform, where \(T\) can be flexibly adjusted to achieve the desired output resolution. Thus, each pair of Fourier coefficient \(A_{(u,v)}\) and \(_{(u,v)}\) will be spatially expanded to a \(T T\) harmonic function, according to Eq. (2) (the spatial expansion is visualized as transitions between the two red blocks, in Figure. 2). The \(c\) harmonic functions, formulated as a \(T T c\) feature map, are then decoded to reconstruct the image patch \(I_{p}\). Specifically, \(T\) - the size of the Cosine basis functions can be customized during training, e.g., for the task of flexible-resolution super-resolution. We introduce the details in Sec. 4.1.

Notably, \(T\) shouldn't be too small to formulate a valid 2D Cosine waveform. To balance with model efficiency, we set \(T=P/2\), i.e., \(T=32\) for face images and \(T=16\) for natural images, where the decoder has an upsampling stride setting as 2. The \(c\) pair of frequencies \((u^{c},v^{c})\), which determine the Cosine basis functions, are also learnable, as detailed in Sec. 3.4.

### Learning Frequencies as Network Parameters

Many prior works learn the \(u\) and \(v\) also from an encoder [19; 32], similar to the Fourier coefficients, i.e., the frequencies are conditioned on the input signal. However, is it the optimal solution? To answer the question, we briefly revisit the properties of the harmonic function.

**Harmonics.** There are two key principles for setting frequencies in signal processing. First, frequencies \((u,v)\) in Eq. (5) are sampled in increments inversely proportional to the signal's length. Second, frequencies must satisfy the sampling theorem to avoid aliasing. To determine the range of the frequencies, we adopt the Fourier transform \(F()\) to the harmonic function (we use 1D case, and eliminate the amplitude and phase for simplicity),

\[F(_{k}())= _{-}^{}(kt)e^{-i2 t}dt\] \[= [(2(-))+(2(+))],\]

i.e., the bandwidth of the harmonic \(B=k/T\). According to the Nyquist-Shannon theorem, for a sampling rate \(f_{s}\), perfect reconstruction is guaranteed for a band-limit \(B=<}{2}\). For discrete signals, such as images, where the sampling interval is \(f_{s}=1\) (pixel), \(k\) can be restricted to \(k<\) to prevent aliasing and ensure an accurate harmonic representation.

**Our solution.** Based on the design of the classic Discrete Fourier Transform (DFT) described above, we initialize \(c\) pairs of \((u,v)\) within the range \([0,T/2)\). However, for more flexible designs, the \((u,v)\) set should adapt to different needs, such as increasing frequency pairs for higher resolution or reducing them for efficiency. In such cases, uniform sampling can result in sparse coverage. For example, with \(T=64\) and the number of basis maps set to \(64\), both \(u\) and \(v\) are sparse, sampled as \([0,4,8,,32]\). Since natural image frequencies are not uniformly distributed, we make \((u,v)\) learnable parameters shared across all input images, enabling effective modeling of diverse frequencies during training. Compared to prior approaches that adapt frequencies based on individual input images [32; 19], our design aligns more closely with classic principles by using globally shared frequencies that span a full range. Such design also addresses a common limitation in previous models emphasizing lower frequencies that dominate natural images, allowing for a more balanced and comprehensive frequency representation.

### Decoding Harmonic for Reconstruction

Once the basis \(\{\}_{T T c}\) are obtained, we introduce a decoder network \(\) to map them to RGB pixels:

\[X=(\{(u,v,x,y)\});\;(u,v)<T/2,(x,y)<T\] (5)

The architecture can be flexibly designed. E.g., our framework allows for either implicit representation networks [10; 39; 32; 33], sampling only a small portion of pixels to decode during training, or convolutional/transformer-based decoders that use the full latent space, accommodating additional modules like discriminators for full-sized outputs.

Figure 3: **Qualitative evaluation for face FR-SR.** While models can super-resolve an input image with any ratio between 2 and 8, we show the **extreme case**: to upsample a \(32 32\) input to a \(256 256\) output, i.e., \( 8\). We compared to both LIIF  and the CosAE variants stated at Sec. 4.2.

[MISSING_PAGE_FAIL:6]

pendix B.2) where we introduced a channel dropout variant during training, strictly constrained \((u,v)<T/2\), and observed the resulting outcomes. Further **frequency analysis** and discussions on **limitations**, including out-of-distribution generalization and common artifacts, are provided in the Appendix.

### Flexible-resolution Super-resolution (FR-SR)

FR-SR has been intensively explored via implicit representation , in which the output resolution can be flexibly configured by setting the size of the coordinate map.

#### 4.1.1 FR-SR for Face Images.

We train the face model using cropped faces from the training splits of FFHQ  and CelebA-HQ , utilizing the official training and validation splits. To evaluate the output images, we use FID  and LPIPS  metrics, along with PSNR and SSIM to compare with methods that use pixel regression objectives .

**Baselines.** We compare CosAE with local implicit representation methods LIIF , LTE , and ITNSR , which generate arbitrary-resolution outputs by predicting RGB values at specific coordinates. For a fair comparison, we re-implemented these methods using the same Encoder as CosAE to ensure consistent representation capacity across models. Among those, LIIF is a direct counterpart to our approach, approximating the replacement of high-frequency components (Cosine basis functions) with plain coordinate maps. To ensure a fair comparison, we align LIIF by replacing its implicit decoder with ours (CNN with self-attention) and adopting the GAN loss.

To compare with LTE  and ITNSR , we replace their Encoders with ours but keep their original Decoders intact as they are key components. We do not add GAN loss as it does not enhance results. LTE does not have a narrow bottleneck due to a skipped link before the output. We align the input settings for all methods to match ours while keeping other components as per their original versions, denoting the modified networks as "-64x" in Table 1. Notably, during inference, CosAE performs blind super-resolution without needing the upsampling ratio, unlike other methods.

**Comparison with the State-of-the-Art Methods.** We demonstrate significantly improved performance with CosAE, compared with the baselines of LIIF-64x, LTE-64x and ITNSR-64x, both qualitatively (Figure. 3) and quantitatively (Table. 1). In Table. 1, networks trained only with L1 loss are placed in the upper block and those trained with full objectives (Sec. 3.6) are in the bottom rows. Our results are significantly better, especially in terms of large-ratio (e.g., \( 8\) and \( 6\)) super-resolution, on all the matrices.

#### 4.1.2 FR-SR for Natural Images.

To perform FR-SR training on natural images, we initially pretrain CosAE on the ImageNet  training split, using the same approach as the face model. However, we encountered distortions in the reconstructed images when applying the \(4 4\) bottleneck - same as the face model. To

Figure 4: **FR-SR on DIV2K . We showcase \(8\) results similar to Figure. 3. CosAE faithfully reconstructs fine details, for both regular (buildings) and irregular (skin and food) textures. Zoom in to see details.**

address this, we adjusted the bottleneck to \(8 8\) with \(T_{max}=16\). Subsequently, we fine-tuned the model using the same training settings on a combination of the DIV-2K  training set and all the images from Flicker2K  (referred to as the DF2K dataset). We compared the CosAE with LIIF  aligned in the same way as training the face model, denoted as "LIIF-32x" as the Encoder for natural images has a \(32\) downsampling stride. Both models were trained with the same datasets, input settings, and objectives.

To evaluate the performance, we used LPIPS on the DIV-2K validation set, considering the adoption of the discriminator. The FID score  is not applicable due to the testing set is small. We keep the same super-resolution up-scaling factor as that of the face (in Table. 2 we present the down-sampling factor instead of the resolution, given images of DIV2K have different sizes between individuals). Similarly, CosAE demonstrates significantly better performance than the others. Qualitative results are shown in Figure. 4, and Figure. 9, 10 in the Appendix.

### Ablation Studies

To validate the design choices, we perform the Ablation Studies on the task of FR-SR for face images, which focus on the following aspects.

**W or w/o Cosine Basis Functions.** To evaluate if the Cosine basis functions play a key role in improving the performance, other than LIIF that replaces them with coordinate maps, we further replace them with plain upsampling layers, while preserving all the other components and training recipes. We denote it as _nocos_.

**Encoding Input with Cosine Basis.** To see if the Cosine basis functions can be placed other than the bottleneck, we encode the input image with Cosine basis functions, i.e., similar to Fourier transform to multiply each basis with the signal, but without the summation operator. We keep the bottleneck the same design as the _nocos_ model. We denote it as _imcos_.

**Fourier Transform.** Instead of using the Decoder to mimic the summation operation in Eq. (1), we directly sum over the frequencies, i.e., the channel dimension of the bottleneck, before the Decoder. This is equivalent to performing the Fourier transform, instead of preserving the Fourier series, in the bottleneck latent space. We denote it as _FT_.

**Results.** We compare the ablated models with the others trained with full objectives, in Table. 1. Compared to CosAE, all the variants show degraded performance on all the matrices. We observe _nocos_ and _imcos_ exhibit visually pleasant faces with rich details, but keep less fidelity, e.g., the eyelid or lip color may completely changed, see Figure. 3. _FT_ shows lower quality local region details given it does not fully leverage the capabilities of the decoder (see Figure. 12 in Supp., due to space limitation).

Figure 5: **Comparisons with the STOA face restoration methods.** Results are shown for both **real-world** (WebPhoto-Test, 1st row) and synthetic degraded images (CelebA-Test, 2nd row). CosAE restores more high-frequency and realistic facial details compared to other methods.

**Wide or Narrow Bottleneck?** Is a narrow bottleneck better? Since CosAE does not support a wider bottleneck variant due to a certain size \(T\) needs to be maintained for valid Cosine functions (Sec. 3.3). Thus, we modify a "wide" LIIF  by removing the downsampling layers, resulting in the _LIIF-4x_ variant with a stride of 4. We evaluated _LIIF-4x_ on face (Table 1, Figure.3) and natural images (Table2, Figure.4) using full objectives. Notably, the narrow bottleneck provided consistent performance across different ratios. Table 1 shows minimal performance drop for super-resolving images by factors of 2 to 8 for _LIIF-64x_ and other methods, compared to _LIIF-4x_. This consistent performance in FR-SR demonstrates the potential benefits of using a narrow bottleneck encoder.

### Blind Face Image Restoration

Due to the space limitation, we present the results for blind face image restoration, while leaving details of blind natural image restoration in the supplementary material.

Blind image restoration restores high-quality images from complex and unknown degradation. For face images, most recent works [36; 37; 38] focus on two aspects: synthesis of degraded images that mimic the divergence and complex type of degradation via math operators [36; 56], and improve network architectures. We focus on the latter aspect.

For blind face image restoration, we adopt the degradation operators of  to synthesize degraded/clean paired data during training. Instead of training the network from scratch, we found finetuning the CosAE network trained for the task of FR-SR (Sec. 4.1.1) yields faster convergence. Unlike training a FR-SR task, we instead fix \(T=T_{max}\), and finetune the model with images of \(512 512\).

**Comparison to the State-of-the-Art Methods.** We compare the CosAE with the state-of-the-art methods, including DFDNet , PULSE , GFP-GAN , RestoreFormer  and CodeFormer . Following [36; 38], we evaluate CosAE in the CelebA-Test, LFW-Test, WebPhotoTest, and WIDER-Test  datasets. We provide quantitative evaluations on both the synthetic dataset - the CelebA-Test dataset in Table. 3, and the real datasets including LFW-Test, WebPhotoTest, and WIDER-Test in Table. 7 in Supp. In addition, we show qualitative results on both synthetic and real-world datasets in Figure. 5.

Both quantitative comparisons demonstrate the superior performance of our CosAE: it outperforms the other approaches on almost all the metrics, except for the FID score. However, it is important to note that the FID score may not provide an accurate assessment of realism when dealing with a relatively small test set, especially in the aforementioned real datasets. Qualitatively, compared to

   up-ratio & \( 8\) & \( 5.33\) & \( 4\) & \( 2\) \\  LIIF-4x (G) & 0.41 & 0.33 & 0.30 & 0.26 \\ LIIF-32x (G) & 0.42 & 0.34 & 0.29 & 0.21 \\ CosAE (G) & **0.33** & **0.25** & **0.18** & **0.11** \\   

Table 2: **FR-SR evaluating of FR-SR on DIVEX, e.g., \( 2\) to \(\), with LPIPS \(\) on the validation set (FID is N/A due to no reference set).**

Figure 6: Comparisons with LDM on \(4\)**super-resolution on ImageNet**. In comparison, the CosAE produces more fine-grained details, in terms of both the structure and the texture. Zoom in to see details.

   Method & FID\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & IDD\(\) \\  Input & 132.69 & 24.96 & 0.66 & 0.50 & 0.93 \\ DFDNet & 52.92 & 24.10 & 0.61 & 0.45 & 0.76 \\ PULSE & 67.75 & 21.61 & 0.63 & 0.46 & 1.20 \\ GFP-GAN & 42.39 & 24.46 & 0.67 & 0.36 & 0.60 \\ RestoreFormer & 41.45 & 24.42 & 0.64 & 0.36 & 0.57 \\ CodeFormer & 52.43 & 21.28 & 0.61 & 0.30 & 0.60 \\ CosAE & 52.03 & **25.65** & **0.70** & **0.27** & **0.53** \\   

Table 3: **Quantitative comparisons on CelebA-Test. Our CosAE shows better performance on almost all the metrics, except for the FID score (see the possible reasons in Sec. 4.3).**other approaches, CosAE synthesizes much more realistic facial details, e.g., in the hair and skin regions, as well as more wrinkles, freckles, beards, etc., than to generate a smoother one.

### Fixed-ratio Super-resolution on ImageNet

In order to evaluate our approach against conventional methods for learning image representation, we conducted super-resolution experiments on the ImageNet dataset using a \(4\) upsampling factor. To accomplish this, we finetuned the CosAE network trained with the FR-SR on natural images with a fixed \(T=T_{max}\), i.e., no flexible output resolution being configured during training. This setting was applied to ImageNet images that were resized to \(256 256\) pixels. We compared CosAE with U-Net regression , LDM  series, GigaGAN , other diffusion-based models, including \(I^{2}\)SB , DDRM , IRSDE  and ResShift . We use FID score with the reference of 50k validation images in ImageNet, and the LPIPS  as the evaluation matrices. In addition, model size as well as the inference speed (ours are tested on a single V100) are reported. As shown in table 4, CosAE outperforms most models in both efficiency (i.e., number of parameters and runtime) and performance. Qualitative visual comparison with LDM-4  are shown in Figure. 14.

## 5 Conclusion

We introduce CosAE, a novel auto-encoder architecture for blind image restoration tasks. By incorporating learnable harmonic functions in the bottleneck, CosAE effectively captures underlying structures and patterns of images with extremely compressive representation, achieving accurate reconstruction with high-fidelity details. CosAE outperforms state-of-the-art methods in flexible-resolution super-resolution and blind image restoration. As a generic denoising Autoencoder, CosAE offers potential for various computer vision tasks.