# Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms

Rayna Andreeva\({}^{*}\)1, Benjamin Dupuis\({}^{*}\)2,3, Rik Sarkar\({}^{1}\), Tolga Birdal\({}^{}\)4, Umut Simsekli\({}^{}\)2,3

\({}^{1}\) School of Informatics, University of Edinburgh, UK

\({}^{2}\) INRIA, France

\({}^{3}\) CNRS, Ecole Normale Superieure PSL Research University, France

\({}^{4}\) Department of Computing, Imperial College London, UK

\({}^{*}\)\({}^{}\) indicate equal contributions.

###### Abstract

We present a novel set of rigorous and computationally efficient topology-based complexity notions that exhibit a strong correlation with the generalization gap in modern deep neural networks (DNNs). DNNs show remarkable generalization properties, yet the source of these capabilities remains elusive, defying the established statistical learning theory. Recent studies have revealed that properties of training trajectories can be indicative of generalization. Building on this insight, state-of-the-art methods have leveraged the topology of these trajectories, particularly their fractal dimension, to quantify generalization. Most existing works compute this quantity by assuming continuous- or infinite-time training dynamics, complicating the development of practical estimators capable of accurately predicting generalization without access to test data. In this paper, we respect the discrete-time nature of training trajectories and investigate the underlying topological quantities that can be amenable to topological data analysis tools. This leads to a new family of reliable topological complexity measures that provably bound the generalization error, eliminating the need for restrictive geometric assumptions. These measures are computationally friendly, enabling us to propose simple yet effective algorithms for computing generalization indices. Moreover, our flexible framework can be extended to different domains, tasks, and architectures. Our experimental results demonstrate that our new complexity measures correlate highly with generalization error in industry-standards architectures such as transformers and deep graph networks. Our approach consistently outperforms existing topological bounds across a wide range of datasets, models, and optimizers, highlighting the practical relevance and effectiveness of our complexity measures.

## 1 Introduction

Generalization, a hallmark of model efficacy, is one of the most fundamental attributes for certifying any machine learning model. Modern deep neural networks (DNN) display remarkable generalization abilities that defy the current wisdom of machine learning (ML) theory [85; 86]. The notion can be formalized through the _risk_ minimization problem, which consists of minimizing the function:

\[(w):=_{z_{z}}[(w,z)],\] (1)where \(z:=\) denotes the data, distributed according to a probability distribution \(_{z}\) on the data space \(\). In practice, as \(_{z}\) is unknown, ML algorithms focus on minimizing the empirical risk,

\[}_{S}(w)=_{i=1}^{n}(w,z_{i}),\] (2)

where \(S:=(z_{1},,z_{n})_{z}^{ n}:=_{z} _{z}\), which means that \((z_{1},,z_{n})\) are independent samples from \(_{z}\). In many applications, the minimization of (2) is achieved by discrete stochastic optimization algorithms, such as stochastic gradient descent (SGD) or the ADAM  method. Such algorithms generate a sequence of iterates in \(^{d}\), denoted \(_{S}:=\{w_{k}\}_{k 0}\), which depends on the data \(S\), the initialization \(w_{0}^{d}\), and some additional randomness \(U\), _e.g._, the random batch indices in SGD. The _generalization error_ characterizing the model's performance is then defined as:

\[G_{S}(w_{k}):=(w_{k})-}_{S}(w_{k}).\] (3)

The empirical risk (2) typically has numerous local minima, which raises the question of how to characterize their generalization properties. Recently, training trajectories (_cf._, Figure 0(a)) have been shown to be paramount to answer this question [84; 28]. Indeed, these trajectories can quantify the quality of a local minimum in a compact way, because they depend simultaneously on the algorithm, the hyperparameters, and the data, which is crucial for obtaining satisfactory bounds . A wide family of trajectory-dependent bounds has been developed [84; 28; 50; 4; 36]. For instance, several results on stochastic gradient Langevin dynamics [57; 64; 49], continuous Langevin dynamics  and SGD  take into account the impact of the whole trajectory on the generalization error.

Parallel to these developments, several studies have brought to light the empirical links between topological properties of DNNs and their generalization performance [58; 52; 66; 70; 83], hereby making new connections with topological data analysis (TDA) tools . These studies focus on the structural changes across the different layers of the network  or on the final trained network [66; 70; 83], and are almost exclusively empirical. This partially inspired a new class of trajectory-dependent bounds focusing on topological properties of the trajectories. In particular, recent studies [78; 21; 35; 22; 10; 3] have proposed to relate the generalization error to various kinds of intrinsic _fractal_ dimensions [26; 53] that characterize the learning trajectory. Informally, these bounds provide the guarantee that with probability at least \(1-\), we have:1

\[_{w_{S}}G_{S}(w)_{S})+ +(1/)}{n}},\] (4)

where \((_{S})\) denotes various equivalent fractal dimensions, in particular the persistent homology dimension (PH-dim) [10; 21] and the magnitude dimension . The term \(\) is an information-theoretic quantity that takes different forms among different studies. Despite providing rigorous links

Figure 1: We devise a novel class of complexity measures that capture the topological properties of discrete training trajectories. These generalization bounds correlate highly with the test performance for a variety of deep networks, data domains and datasets. Figure shows different trajectories (**a**) embedded using multi-dimensional scaling based on the distance-matrices (**b**) computed using either the Euclidean distance (\(\|\|_{2}\)) between weights as in  or via the loss-induced pseudo-metric (\(_{S}\)) as in . (**c**) plots the _average granulated Kendall coefficients_ for two of our generalization measures (\(_{}\) and \(()\)) in comparison to the state-of-the-art persistent homology dimensions [10; 21] for a range of models, datasets, and domains, revealing significant gains and practical relevance.

between the topology of the trajectory and generalization, these bounds have major drawbacks. First and foremost, as noted in , fractal-trajectory bounds, such as Equation (4), do not apply to discrete-time algorithms. This creates a discrepancy between these theoretical results and the TDA-inspired methods to numerically evaluate them on commonly used discrete algorithms . Additionally, existing bounds rely on intricate geometric assumptions, such as Ahlfors-regularity  or geometric stability , that are not realistic in a practical, discrete setting.

Previous attempts were made to address this discretization issue. Specifically, under the assumption that the training dynamics possess a stationary measure \(_{w|S}^{}\) for \(T\) (\(T\) is the number of iterations), it was shown in  that with probability \(1-\) over \(S_{z}^{ n}\) and \(w_{w|S}^{}\), we have:

\[G_{S}(w))++(1/)}{n}},\] (5)

where \((_{w|S})\) corresponds to the fractal dimension of the measure \(_{w}\) (see  for formal definitions). While this was an important step, this bound only becomes practically relevant when the number of iterations grows to infinity, which is never attained in real-life experiments. Other attempts make use of so-called finite fractal dimensions  or fine properties of the Markov transition kernels associated with the dynamics . However, these studies also rely on impractical assumptions and involve intricate quantities which make them not amenable to numerical evaluation.

Despite the theoretical limitations of existing topology-dependent generalization bounds, TDA-inspired tools have been developed to numerically estimate the proposed intrinsic dimensions in practical settings. Two particular methods have emerged and successfully demonstrate correlation with the generalization error, based on _persistent homology_ (PH-dim) and _metric space magnitude_ (magnitude dimension); these two dimensions are equivalent for compact metric spaces . Because of the limitations discussed above, existing theories do not account for these experiments, conducted with finite-time discrete algorithms. Moreover, existing empirical studies  only consider very simple models and small (image) datasets. Because of their lack of theoretical foundations, it is not clear whether they could be extended to more practical setups.

**Contributions**. In this paper, we investigate the building blocks of PH and magnitude dimensions, in order to propose new topology-inspired generalization bounds that rigorously apply to widely used discrete-time stochastic optimization algorithms, and experimentally test our new topological complexities2 on practically relevant DNN architectures. Our detailed contributions are as follows:

* We start by establishing the first theoretical links between generalization and a new kind of computationally thrifty topological complexity measure, the _\(\)-weighted lifetime sums_.
* We propose and elaborate on another novel topological complexity, _positive magnitude_ (\(\)), a slightly modified version of magnitude . We rigorously link \(\) with the generalization error, by relying on a new proof technique. Overall, our generalization bounds, rooted in TDA, admit the following generic form: \[_{w_{S}}G_{S}(w)++(1/)}{n}}.\]
* We then provide a flexible computational implementation based upon dissimilarity measures between neural nets (Figure 0(b)), which enables quantifying generalization across different architectures and models, without the need for domain or problem-specific analysis as done in .
* Unlike existing trajectory-based studies  operating on small models, our experimental evaluation is extensive. We consider several vision transformers  and graph neural networks (GNN)  trained on multiple datasets spanning regular and irregular data domains (_cf._ Figure 0(c)). Our results demonstrate that the novel measures we introduce correlate strongly with the test performance across different architectures, hyperparameters and data modalities.

All the proofs of the main results are presented in the appendix, along with additional experiments. We will make our entire implementation publicly available under: https://github.com/rorondre/TDAGeneralization.

Technical Background

Our generalization indicators will be based upon \(\)-weighted lifetime sums and magnitude, capturing different topological features, as we shortly discuss below. Let \((X,)\) be a finite pseudometric space.

\(\)**-weighted lifetime sums**. Persistent homology (PH) is an important concept in the analysis of geometric complexes . We focus on the persistent homology of degree \(0\) (\(^{0}\)). Informally, it consists in tracking the "connected components" of a finite set at different scales. We provide in Sections A.3 and A.4 exact definitions of this notion. For simplicity, we present here an equivalent formulation of the \(\)-weighted lifetime sums based on minimum spanning trees (MST) [42; 73].

A tree over \(X\) is a connected acyclic undirected graph (a set of edges) whose vertices are the points in \(X\). Given an edge \(e\) linking the points \(a\) and \(b\), we define its _cost_ as \(|e|:=(a,b)\). An MST \(\) on \(X\) is a tree minimizing the _total cost_\(_{e}|e|\). The \(\)-weighted lifetime sums \(_{}^{}\) are then written as:

\[ 0,\ _{}^{}(X):=_{e}|e|^{}.\]

The celebrated _persistent homology dimension_ (PH-dim) , of a compact pseudometric space \((A,)\) is then defined as \(_{}^{}(A)=_{ 0}\{ C>0, Y  A\), \(_{}(Y) C\}\). The PH-dim has been proven to be related to generalization error for different pseudometrics \(\)[10; 21].

**Magnitude**. Magnitude is a recently introduced topological invariant  which encodes many important invariants from geometric measure theory and integral geometry [46; 55; 56]. Magnitude can be interpreted as the effective number of distinct points in a space . For \(s>0\), we define a _weighting_ of the modified space \((X,s)\) as a map \(:X\), such that \( a X,\ _{b X}e^{-s(a,b)}(b)=1\). Given such a weighting \(\), the magnitude function of \((X,s)\) is defined as

\[^{}(sX):=_{a X}(a).\] (6)

The parameter \(s>0\) should be interpreted as a "scale" through which we look at the set \((X,)\). We present in Appendix A.5 additional properties of this function. Note that magnitude is usually defined in metric spaces; we show in Appendix B.2 that we can seamlessly extend it to the pseudometric setting. Magnitude can be extended to (infinite) compact spaces [46; 55] and, as for PH, an intrinsic dimension, the _magnitude dimension_, can be defined from magnitude by the formula \(_{}^{}(A)=_{s}(sA)}{ (s)}\). It is known that \(_{}^{}\) and \(_{}^{}\) coincide for compact metric spaces [56; 73; 3]. As a result, \(_{}^{}\) has also been proposed as a topological generalization indicator .

**Total mutual information**. Prior intrinsic dimension-based studies relied on "mixing" assumptions ([78; Assumption H5], [10; Assumption H1], [76; 13]) or various mutual information terms [35; 21] to take into account the statistical dependence between the data and the training trajectory. Recently, a new framework was proposed in  to unify these approaches by proving data-dependent uniform generalization bounds using simpler and smaller information-theoretic (IT) terms. By leveraging these methods, we derive new generalization bounds involving the same IT terms for all our introduced topological complexities. More precisely, they take the form of a _total mutual information_ between the data \(S\) and the training trajectory \(_{S}\). This term is denoted \(_{}(S,_{S})\) and measures the dependence between \(S\) and \(\). We refer to Appendix A.1 and [35; 81] for exact definitions.

## 3 Main Theoretical Results

We now introduce our learning-theoretic setup (section 3.1) before delving into our main theoretical results in Sections 3.2 and 3.3.

### Mathematical setup

**Random trajectories**. The primary goal of our theory is to prove uniform3 generalization bounds over the training trajectory \(\{w_{k},\ k 0\}\). We are mostly interested in the behavior near local minima of \(}_{S}\). To this end, we observe the trajectory between iterations \(t_{0}\) and \(T\), where \(t_{0}\) is the number of iterations before reaching (near) a local minimum and \(T t_{0}\) is the total number of iterations.

Therefore, we consider the set \(_{l_{0} T}:=\{w_{i},\ t_{0} i T\}\), which we call the _random trajectory_. Note that \(_{l_{0} T}\) is a _set_, _i.e._, it does not contain any information about the time-dependence. Moreover, our setup allows the random times \(t_{0}\) and \(T\) to depend on the data \(S\) through the choice of a stopping criterion as opposed to being fixed predetermined times.

**General Lipschitz conditions**. The topological quantities described in section 2, as well as the intrinsic dimensions introduced in prior works [78; 10; 3; 21; 22], require a notion of distance between parameters (in \(^{d}\)) to be computed. In the case of fractal-based generalization bounds, two cases have already been considered: the Euclidean distance  and the data-dependent pseudometric defined in . In our work, we emphasize that both examples are particular cases of a more general family of pseudometrics on the parameter space \(^{d}\). In order to fully characterize this family of pseudometrics, we define the data-dependent map \(_{S}:^{d}^{n}\) by \(_{S}(w)=((w,z_{1}),,(w,z_{n}))\). To fit into our framework, a pseudometric must satisfy the following general Lipschitz condition.

**Definition 3.1** (\((q,L,)\)-Lipschitz continuity).: For any pseudo-metric \(\) on \(^{d}\) and \(q 1\), we will say that \(\) is \((q,L,)\)-Lipschitz in \(w\) when \( w,w^{}^{d},\ \|_{S}(w)-_{S}(w^{})\|_{q}  Ln^{1/q}(w,w^{})\).

A wide variety of distances have been proposed to compare the weights of two DNNs . The above condition restricts our analysis to a family of pseudometrics containing the following examples.

_Example 3.2_ (Data-dependent pseudometrics).: For any \(p 1\), we define the pseudometrics \(_{S}^{(p)}(w,w^{}):=n^{-1/p}\|_{S}(w)-_{S}(w^{ })\|_{p}\). The case \(_{S}^{(1)}\) corresponds to the "data-dependent pseudometric" used in ; we will denote it \(_{S}:=_{S}^{(1)}\).

_Example 3.3_ (Euclidean distance).: If \((w,z)\) is \(L\)-Lipschitz continuous in \(w\), _i.e._, \(|(w,z)-(w^{},z)| L\|w-w^{}\|\) for all \(z\), then \(\) is \((p,L,\|\|_{2})\)-Lipschitz continuous for every \(p 1\).

**Assumptions**. Given an \((q,L,)\)-Lipschitz continuous (pseudo-)metric, our approach relies only on a single assumption of a bounded loss function. For the case of the pseudometric \(_{S}\) (Example 3.2), this assumption is already made in [21; 22].

**Assumption 1**.: We assume that the loss \(\) is bounded in \([0,B]\), with \(B>0\) a constant.

The boundedness of \(\) is classically assumed in the fractal / TDA literature [21; 35; 22]. In particular, this assumption is valid for the usual \(0-1\) loss. In , it is shown that the proposed theory seems to be experimentally valid even for unbounded losses. Our experimental findings suggest that this observation also applies to our work.

### Persistent homology related generalization bounds

In contrast to all existing fractal dimension-based bounds [78; 10; 13; 21], we propose new generalization bounds that apply to practical discrete stochastic optimizers with a finite number of iterations. To this end, our key idea involves replacing the intrinsic dimension with intermediary quantities that are used to compute them numerically. Following [10; 3], this points us towards the two quantities, \(_{}\) and \(\), defined in section 2. We are now ready to state the first generalization bound in terms of the \(\)-weighted lifetime sums, where we denote \(_{}^{}\) for \(_{}^{}(_{t_{0} T})\).

**Theorem 3.4**.: _Let \(\) be a pseudometric on \(^{d}\). Supposes that Assumption 1 holds and that \(\) is \((q,L,)\)-Lipschitz, for \(q 1\). Then, for all \(\), with probability at least \(1-\), we have:_

\[_{t_{0} i T}G_{S}(w_{i}) 2B_{}^{})}{n}}+}+3B_{}(S,_{t_{0} T})+(1/)}{2n}},\]

_with \(K_{n,}:=2(2L/B)^{}\)._

The term \(_{}(S,_{t_{0} T})\) is the total mutual information (MI) term that is defined in Sections 2 and 1. It measures the statistical dependence between the random set \(_{t_{0} T}\) and the data \(S_{S}^{ n}\). Such MI terms appear in previous works related to fractal-based generalization bounds [78; 13; 21; 35]. Our proof technique, presented in Appendix B.5, makes use of a recently introduced PAC-Bayesian framework for random sets  to introduce this MI term. It is also shown in  that the MI term \(_{}(S,_{t_{0} T})\) is tighter than those appearing in the aforementioned works.

We highlight the fact that Theorem 3.4 is fundamentally different from the persistent homology dimension (PH-dim) based bounds studied in [10; 21]. Indeed, while the growth of \(_{}\) for increasing finite subsets of the trajectory are used in  to estimate the PH-dim, it does not provide any formal link between the generalization error and the value of \(_{}\). Therefore, the above theorem could not be cast as a corollary of these previous studies. Another important characteristic of the above theorem (as well as the results of section 3.3) is to be non-asymptotic, _i.e._, it is true for every \(n^{*}\). This is an improvement over the fractal dimensions-based bounds presented in [78; 10; 21; 22].

### Positive magnitude (\(\)) and related generalization bounds

Recent preliminary experimental results displayed a correlation between the generalization error of DNNs and magnitude . To provide a theoretical justification for this behavior, it would be tempting to mimic the proof of Theorem 3.4 and build on existing covering arguments. However, while lower bounds of magnitude in terms of covering numbers have been derived in , they appear to be impractical in our case. Another possibility would be to use the magnitude dimension bounds of . Yet, this could not apply to our finite and discrete setting where the dimension is \(0\). Hence, we identify a new quantity, closely related to magnitude, while being more relevant to learning theory. With the notations of section 2, we fix a finite metric space \((X,)\) and a weighting \(_{s}:X\) of \((X,s)\), where \(s>0\) is a "scale" parameter. We define the positive magnitude as

\[ s>0,\ ^{}(sX):=_{a X}_{s}(a)_{+},\] (7)

where \(x_{+}:=(x,0)\) denotes the positive part of \(x\). To avoid harming the readability of the paper, we refer to Appendix B.3 for the extension of \(\) to the pseudometric case. Based on a new theoretical approach, we prove that the positive magnitude can be used to upper bound the generalization error (see the proof in Appendix B.7). This leads to the following theorem:

**Theorem 3.5**.: _Let \(\) be a pseudometric such that \((,)\) admits a positive magnitude (according to Definition B.5) for every \(>0\). We assume that \(\) is \((q,L,)\)-Lipschitz continuous with \(q 1\). Then, for any \(s>0\), we have with probability at least \(1-\) that_

\[_{t_{0} t T}G_{S}(w_{i})^{} (Ls_{t_{0} T})+s}{n}+3B_{}(S,_{t_{0} T})+(1/)}{2n}}.\]

We now present a quick sketch of the proof of Theorem 3.5, in order to highlight its key elements.

Proof.: _(Sketch)_ Let \(\) be a data-dependent random compact set (_e.g._, \(_{t_{0} T}\)). The proof is based on two technical elements. The first is a framework recently proposed in  for uniform generalization bounds for random sets. These results give that with high probability we have a bound of the form:

\[_{w}G_{S}(w)(,_{t_{0}  T})+_{}(S,_{t_{0} T})+(1/ )}{n}},\]

where \((,_{t_{0} T})\) is the celebrated Rademacher complexity , whose definition is given in Appendix A.2. The second technical element is a new link between the Rademacher complexity of a compact set and its positive magnitude. This result is discussed in Appendix B.7.

Figure 2: _Left:_ Comparison of \(\) and \(\) (for \(s=\)), for different (pseudo)metrics (ViT on CIFAR\(10\)). _Right:_ relative variation of the quantities \(_{}(_{t_{0} T})\) and \((_{t_{0} T})\), with respect to the proportion of the data used to estimated \(_{S}^{(1)}\) (ViT on CIFAR\(10\)).

[MISSING_PAGE_FAIL:7]

subspace. We use the implementation in scikit-learn so that, with high probability, the relative variation of the distance matrices is at most \(5\%\), see Appendix A.7 for details.
* Case \(2\): If \(\) is of the form \(_{S}^{(q)}\) as in Example 3.2, then the computation of \(D_{}\) requires the evaluation of the model on the entire dataset at each iteration, which becomes intractable for large DNNs. In [21, Figure \(3\)], the authors show that the PH-dim based on the pseudometric \(_{S}=_{S}^{(1)}\) is very robust to a random subsampling of a training dataset, _i.e._ when \(_{S}\) is replaced by \(_{B}\) with \(B S\) and \(|B|/|S| 1\). Figure 2b shows that \(_{}\) and positive magnitude are also robust to this subsampling. We mainly used \(|B|/|S|=10\%\). We refer the reader to Appendix C.2 for details.

**Generalization error**. Our theory, like many trajectory-based studies [78; 10; 21; 3] predicts upper bounds on the worst-case generalization error over the trajectory \(_{t_{0} T}\). Yet, experiments in previous works mainly reported the error at the last iteration. To estimate the worst-case error in a computationally feasible way, we periodically evaluated the test risk between times \(t_{0}\) and \(T\) (every \(100\) iterations) and reported (worst test risk - final train risk) as the error in our experiments. This is consistent as we start the trajectory \(_{t_{0} T}\) from a weight \(w_{t_{0}}\) already in a local minimum. Our main conclusions are still valid if the final generalization gap is used. This observation, which is to the best of our knowledge new, is briefly discussed in Appendix D.2.1.

## 5 Empirical Analysis

In what follows, we study our bounds on a variety of datasets and model architectures. We first explain the setup and the evaluation metrics before delving into the results and analysis.

**Setup**. Given a DNN and a dataset, we start from a pre-trained weight vector \(w_{t_{0}}\), yielding high training accuracy on classification tasks. By varying the learning rate (\(\)) and the batch size (\(b\)), we define a grid of \(6 6\) hyperparameters. For each pair \((,b)\), we compute the training trajectory \(_{t_{0} T}\) for \(5 10^{3}\) iterations. Unless specified, we use the ADAM optimizer . Based on \(_{t_{0} T}\), we estimate distance matrices as described in section 4. For the sake of clarity, we focus on \(3\) relevant pseudometrics: (i) the Euclidean distance \(_{2}\) as in , (ii) the data-dependent pseudometric \(_{S}\), used in [21; 3], and (iii) the \(01\)-loss distance. For (ii), \(_{S}\) is computed based on the _surrogate_ loss used in training (_e.g._, the cross-entropy loss), while the reported generalization error is always based on _accuracy gap_ (\(01\)-loss), which is of interest in most applications (see section 4). For the last one (iii) \(\) is defined as in Example 3.2, but with \(\) being the \(01\)-loss; we call it \(01\)-pseudometric and denote it by \(01\) in the tables. This last setup matches exactly our theoretical requirements.

In terms of DNN architectures, we focus on practically relevant models, while previous studies mainly considered small networks [10; 35; 21; 76]. We examine two different families of architectures. The first family consists of vision transformers (ViT , CaiT , Swin , see Table 2), each evaluated on both the CIFAR10  and CIFAR100  datasets. Moreover, we also tested our theory on graph neural networks (GNN) architectures, namely GatedGCN  and GraphSage  trained on the Super-pixel MNIST dataset . To the best of our knowledge, this is the first time these kinds of topological complexities have been evaluated on transformers and GNNs. We ran the experiments on 18 NVIDIA 2080Ti (11 GB) GPUs.

**Granulated Kendall's coefficients.**. We assess the correlation between our complexities and the generalization error by using the granulated Kendall's coefficients (GKC) . While the classical Kendall's coefficients (KC)  (denoted \(\)) measures the correlation between two quantities, it may fail to capture their causal relationship. Instead, one "granulated" coefficient is defined in  for each hyperparameter (_i.e._, \(_{}\) for \(\) and \(_{}\) for \(b\)); it measures the correlation when only this hyperparameter is varying. In Table 1, we report \(\), \(_{}\) and \(_{}\), and the averaged GKC, \(:=(_{}+_{})/2\), for several models, datasets and topological complexities. In Figures 4a and 4b, we represent our topological complexities in the plane \((_{},_{})\); the red square indicates the region of best correlation (the coefficients are in \([-1,1]\), their sign is the sign of the correlation). It should be noted that a scaling of this constant \(B\), coming from Assumption 1, would not impact the correlation between generalization and topological complexities that is observed in our experiments.

### Analysis

As explained above, we focus our main experiments on the quantities \(_{1}\), \(()\), \(()\), \((10^{-2})\) and \((10^{-2})\), each computed for the \(3\) pseudometrics discussed above (\(_{2}\), \(_{S}\), \(01\)). In the interest of comparison, we also compute the PH-dim (proposed in  for the \(_{2}\) and in  for \(_{S}\)), which is thus tested for the first time on transformers and GNNs.

**Performance on vision transformers**. We see in Table 1 and Figure 3 (additional graphical representation is given in Appendix D.1) that our proposed topological complexities consistently outperform the PH dimensions across several vision transformer models and datasets. This suggests that PH-dim, previously tested only on small architectures, is less scalable to industry-standards models with more parameters. Figure 3(a), including all (model, dataset) pairs for the pseudometric \(_{S}\), reveals important observations. First, we notice that the GKC of our topological complexities are both positive and close to \(1\), indicating that they are indeed good measures of generalization. We note that for most models and datasets, \(_{}\) has a small or negative \(_{}\), indicating that it has less ability to explain generalization for varying batch-sizes. As it was observed in  for PH-dim, our complexities computed from the pseudometric \(_{S}\) correlate very well with the generalization gap while this gap is based on the \(01\) loss.

**Performance on GNNs**. An important aspect of our framework is the ability to seamlessly encapsulate different data domains. In particular, the possibility of using different pseudometrics can help define topological complexities that naturally take into account the internal symmetries of GNNs, without any model-specific analysis . The results of Table 1 and Figure 3(a) confirm that our proposed topological complexities outperform PH-dim and correlate strongly with the generalization error for GNNs. Additionally, it may be observed that \(()\) performs significantly well for GNNs, and in particular better than \(()\). This points us towards the idea that further theory would be desirable to formally relate magnitude to the generalization error in that case4.

**Comparison of the topological complexities**. In Table 1 and Figures 3 and 3(a), it can be seen that \(_{1}\) and \(()\) perform equally well for the image and graph experiments across multiple datasets, models, and data domains. We see in Table 1 that most topological complexities perform better with data-dependent metrics (_i.e._, \(_{S}\) and \(01\)) than with the Euclidean distance, for transformer-based experiments. This extends results obtained for PH-dim in , for smaller architectures. However, the poor performance of Euclidean-based complexities may also be partially caused by the projections applied to the Euclidean distance matrices to make them memory-wise computable (see section 4). This is a remaining limitation of our algorithms. On the other hand, the \(01\) and \(_{S}\) data-dependent pseudometrics seem to yield similar performance in all experiments.

**Ablations**. In Figure 3(b), we reveal that changing the optimizer has little effect on the observed correlation (for the same model and dataset). Interestingly, we note that the PH-dim, computed with pseudometric \(_{S}\) and obtained from the SGD trajectories, exhibits high GKCs. This observation agrees with the results in . Figure 3 further displays the typical behavior of several topological complexities for ViT and CIFAR\(10\). In addition to the correlation of our proposed complexities being stronger than for the PH-dim, we observe that \(_{}\) and \(()\) seem to better correlate with the generalization gap for small learning rates. Finally, it is consistently observed in Table 1 and Figures 3(a) and 3(b) that using a relatively high value of the (positive) magnitude scale (\(s=\)) yields better correlations than small values (\(s=10^{-2}\)). However, both cases still provide satisfying correlation, comforting the robustness of magnitude as a generalization indicator.

Due to limited space, we present all the correlation coefficient of one transformer model ViT for CIFAR\(10\) and Swin for CIFAR\(100\) in Table 1 as illustrative examples for each dataset. The remaining results appear in the Appendix, Tables 4, 6, 3 and 5, and they all follow a similar trend. Further empirical results and illustrations of this behavior are provided in Appendix D.

Figure 3: \(_{S}\)-based complexity measures vs. generalization gap for a ViT trained on CIFAR\(10\): \(_{}\) (_left_), \(()\) (_middle_), and \(_{1}\) (_right_).

## 6 Conclusion

In this paper, we proved novel generalization bounds based on several topological complexities coming from TDA, namely \(\)-weighted lifetime sums and a new variant of metric space magnitude, which we called positive magnitude. Compared to previous studies, we require fewer assumptions and operate in a discrete setting in which our proposed quantities are fully computable. Our algorithms are flexible enough to be seamlessly integrated with diverse data domains and tasks. These advantages of our framework allowed us to create a computationally cheap experimental setup, as close as possible to the theoretical setup. We thus provided a comprehensive suite of experiments with several industry-relevant architectures across vision transformers and graph neural networks, which have not been explored yet in this literature. We show that our proposed topological complexities correlate well with the generalization error, outperforming the previously studied intrinsic dimensions.

**Limitations & future work.** The main limitation of our theory is the lack of understanding of the IT terms, while they are still smaller than most prior works. The presence of this term renders our bounds not fully computable in practice. Indeed, we are not aware of existing techniques to evaluate the MI between random sets and the dimensionality of \(_{t_{0} T}\) (billions of parameters) could make a direct computation intractable. Nevertheless, our work focuses on improving the topological part of the existing bounds. Our main goal is to demonstrate a correlation with the generalization error rather than directly quantifying the generalization. Our experiments show that the introduced complexities are important and meaningful in addition to being amplified in the first part of the bound, as the dependence is explicit. Moreover, a better understanding of the behavior of positive magnitude for small values of the scale factor \(s\) would be a necessary improvement. Regarding our experiments, a refinement of the estimation techniques of the topological complexities would be beneficial. Despite experimenting with practically relevant architectures, our future works also include scaling up our empirical analysis to include larger models and datasets, in particular large language models, which are still beyond the scope of this study.