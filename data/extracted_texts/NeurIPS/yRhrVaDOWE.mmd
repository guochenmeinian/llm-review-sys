# Diffusion-based Curriculum Reinforcement Learning

Erdi Sayar1, Giovanni Iacca2, Ozgur S. Oguz3, Alois Knoll1

Technical University of Munich1 University of Trento2 Bilkent University3

erdi.sayar@tum.de, giovanni.iacca@unitn.it,

ozgur@cs.bilkent.edu.tr, knoll@in.tum.de

###### Abstract

Curriculum Reinforcement Learning (CRL) is an approach to facilitate the learning process of agents by structuring tasks in a sequence of increasing complexity. Despite its potential, many existing CRL methods struggle to efficiently guide agents toward desired outcomes, particularly in the absence of domain knowledge. This paper introduces DiCuRL (Diffusion Curriculum Reinforcement Learning), a novel method that leverages conditional diffusion models to generate curriculum goals. To estimate how close an agent is to achieving its goal, our method uniquely incorporates a \(Q\)-function and a trainable reward function based on Adversarial Intrinsic Motivation within the diffusion model. Furthermore, it promotes exploration through the inherent noising and denoising mechanism present in the diffusion models and is environment-agnostic. This combination allows for the generation of challenging yet achievable goals, enabling agents to learn effectively without relying on domain knowledge. We demonstrate the effectiveness of DiCuRL in three different maze environments and two robotic manipulation tasks simulated in MuJoCo, where it outperforms or matches nine state-of-the-art CRL algorithms from the literature.

## 1 Introduction

Reinforcement learning (RL) is a computational method that allows an agent to discover optimal actions through trial and error by receiving rewards and adapting its strategy to maximize cumulative rewards. Deep RL, which integrates deep neural networks (NNs) with RL, is an effective way to solve large-dimensional decision-making problems, such as learning to play video games , chess , Go , and robot manipulation tasks . One of the main advantages of deep RL is that it can tackle difficult search problems where the expected behaviors and rewards are often sparsely observed. The drawback, however, is that it typically needs to thoroughly explore the state space, which can be costly especially when the dimensionality of this space grows.

Some methods, such as reward shaping , can mitigate the burden of exploration, but they require domain knowledge and prior task inspection, which limits their applicability. Alternative strategies have been proposed to enhance the exploration efficiency in a domain-agnostic way, such as prioritizing replay sampling , or generating intermediate goals . This latter approach, known as _Curriculum Reinforcement Learning_ (CRL), focuses on designing a suitable curriculum to guide the agent gradually toward the desired goal.

Various approaches have been proposed for the generation of curriculum goals. Some methods focus on interpolation between a source task distribution and a target task distribution . However, these methods often rely on assumptions that may not hold in complex RL environments, such as specific parameterization of distributions, hence ignoring the manifold structure in space. Other approaches adopt optimal transport , but they are typically applied in less challenging exploration scenarios. Curriculum generation based on uncertainty awareness has also been explored, but such methods often struggle with identifying uncertain areas as the goal space expands .

Some research minimizes the distance between generated curriculum and desired outcome distributions using Euclidean distance, although this approach can be problematic in certain environments [19; 25]. Other methods incorporate graph-based planning, but require an explicit specification of obstacles [26; 27]. Lastly, approaches based on generative AI models have been proposed. For instance,  uses GANs to generate tasks of intermediate difficulty, but it relies on arbitrary thresholds. Alternatively, [28; 29; 30] apply diffusion models in offline RL settings [14; 28; 30].

Despite these advancements, existing CRL approaches still struggle with generating suitable intermediate goals, particularly in complex environments with significant exploration challenges. To overcome this challenge, in this paper, we propose DiCuRL (Diffusion Curriculum Reinforcement Learning). Our method leverages conditional diffusion models to dynamically generate curriculum goals, guiding agents towards desired goals while simultaneously considering the \(Q\)-function and a trainable reward function based on Adversarial Intrinsic Motivation (AIM) .

**Contributions** Unlike previous offline RL approaches [28; 29; 30] that train and use diffusion models for planning or policy generation relying on pre-existing data, DiCuRL facilitates _online learning_, enabling agents to learn effectively without requiring domain-specific knowledge. This is achieved by three key elements. 1 The diffusion model captures the _distribution of visited states_ and facilitates exploration through its inherent noising and denoising mechanism. 2 As the \(Q\)-function predicts the cumulative reward starting from a state and a given goal while following a policy, we can determine _feasible_ goals by maximizing the \(Q\)-function, ensuring that the generated goals are challenging yet achievable for the agent. 3 The AIM reward function estimates the agent's proximity to the desired goal and allows us to _progressively shift the curriculum_ towards the desired goal.

We compare our proposed approach with nine state-of-the-art CRL baselines in three different maze environments and two robotic manipulation tasks simulated in MuJoCo . Our results show that DiCuRL surpasses or performs on par with the state-of-the-art CRL algorithms.

## 2 Related Work

**Curriculum Reinforcement Learning** CRL  algorithms generally adjust the sequence of learning experiences to improve the agent's performance or accelerate training. These algorithms focus on formulating intermediate goals that progressively guide the agent toward the desired goal, and have been successfully applied to various tasks, mainly in the field of robot manipulation [34; 35; 36; 37].

Hindsight Experience Replay (HER)  tackles the challenge of sparse reward RL tasks by employing hindsight goals, considering the achieved goals as pseudo-goals, and substituting them for the desired goal. However, HER struggles to solve tasks when the desired goals are far from the initial position. Hindsight Goal Generation (HGG)  addresses the inefficiency issue inherent in HER by generating hindsight goals through maximizing a value function and minimizing the Wasserstein distance between the achieved goal and the desired goal distribution.

CURROT  and GRADIENT  both employ optimal transport for the generation of intermediate goals. CURROT formulates CRL as a constrained optimization problem and uses the Wasserstein distance to measure the distance between distributions. Conversely, GRADIENT introduces task-dependent contextual distance metrics and can manage non-parametric distributions in both continuous and discrete context settings; moreover, it directly interprets the interpolation as the geodesic from the source to the target distribution.

GOAL-GAN  generates intermediate goals using a Generative Adversarial Network (GAN) , without considering the target distribution. A goal generator is used to propose goal regions, and a goal discriminator is trained to evaluate if a goal is at the right level of difficulty for the current policy. The specification of goal regions is done using an indicator reward function, and policies are conditioned on the goal as well as the state, similarly to a universal value function approximator .

PLR  uses selective sampling to prioritize instances with higher estimated learning potential for future revisits during training. Learning potential is estimated using TD-Errors, resulting in the creation of a more challenging curriculum. VSD  estimates the epistemic uncertainty of the value function and selects goals based on this uncertainty measure. The value function confidently assigns high values to easily achievable goals and low values to overly challenging ones. ACL  maximizes the learning progress by considering two main measures: the rate of improvement in prediction accuracy and the rate of increase in network complexity. This signal acts as an indicator of the current rate of improvement of the learner. The ALP-GMM  fits Gaussian Mixture Models (GMM) using an Absolute Learning Progress (ALP) score, which is defined as the absolute difference in rewards between the current episode and the previous episodes. The teacher generates curriculum goals by sampling environments to maximize the student's ALP, which is modeled by the GMM.

Finally, OUTPACE  employs a trainable intrinsic reward mechanism, known as Adversarial Intrinsic Motivation (AIM)  (the same used in our method) which is designed to minimize the Wasserstein distance between the state visitation distribution and the goal distribution. This function increases along the optimal goal-reaching trajectory. For curriculum goal generation, OUTPACE uses Conditional Normalized Maximum Likelihood (CNML), to classify state success labels based on their association with visited states, out-of-distribution samples, or the desired goal distribution. The method also prioritizes uncertain and temporally distant goals using meta-learning-based uncertainty quantification  and Wasserstein-distance-based temporal distance approximation.

**Diffusion Models for Reinforcement Learning** UniPi  leverages diffusion models to generate a video as a planner, conditioned on an initial image frame and a text description of a current goal. Subsequently, a task-specific policy is employed to infer action sequences from the generated video using an inverse dynamic model. AVDC  constructs a video-based robot policy by synthesizing a video that renders the desired task execution and directly regresses actions from the synthesized video without requiring any action labels or inverse dynamic model. It takes RGBD observations and a textual goal description as inputs, synthesizes a video of the imagined task execution using a diffusion model, and estimates the optical flow between adjacent frames in the video. Then, using the optical flow and depth information, it computes robot commands.

Diffusion Policy  uses a diffusion model to learn a policy through a conditional denoising diffusion process. BESO  adopts an imitation learning approach that learns a goal-specified policy without any rewards from an offline dataset. DBC  uses a diffusion model to learn state-action pairs sampled from an expert demonstration dataset and increases generalization using the joint probability of the state-action pairs. Finally, Diffusion BC  uses a diffusion model to imitate human behavior and capture the full distribution of observed actions on robot control tasks and 3D gaming environments.

**Limitations of current works and distinctive aspects of DiCuRL** The aforementioned studies typically require offline data for training. Both  and  employ diffusion models to synthesize videos for rendering the desired task execution, and actions are then inferred from such videos. Studies such as [44; 45; 46; 47] also focus on learning policies from offline datasets. Despite these efforts, reliance on inadequate demonstration data can lead to suboptimal performance . Distinct from these approaches, our method instead does not rely on prior expert data or any pre-collected datasets. As an off-policy RL method, DiCuRL collects in fact data through interaction with the environment.

## 3 Background

We now introduce the background concepts on multi-goal RL, Soft Actor-Critic (SAC), Wasserstein distance, Adversarial Intrinsic Motivation (AIM), and diffusion models.

### Multi-Goal Reinforcement Learning

In the context of multi-goal RL, we can formulate the RL problem as a goal-oriented Markov decision process (MDP) characterized by continuous state and action spaces. This MDP is defined by the tuple \(,,,,p,_{r}\). Here, \(\) represents the state space, \(\) denotes the action space, and \(\) is the goal space. The transition dynamics, \((s^{}|s,a)\), describes the probability of transitioning to the next state \(s^{}\) given the current state \(s\) and action \(a\). The joint probability distribution over the initial states and the desired goal distribution is represented by \(p(s_{0},g)\), and \(_{r}\) is a discount factor.

We utilize the AIM reward function \(r_{}\), as outlined in Section 3.3. The objective is to identify the optimal policy \(\) that maximizes the expected cumulative reward approximated by the value function \(Q^{}(s,g,a)\). This function can be expanded to the Universal Value Function (UVF), a goal-conditioned value function incorporating the goal into value estimation. The UVF, defined as \(V^{}(s,g)\), where \(s\) is the current state, \(g\) is the goal, and \(\) is the policy, estimates the expected return from state \(s\) under policy \(\), given goal \(g\). The UVF has been shown to successfully generalize to unseen goals, which makes it a viable approach for multi-goal RL .

### Soft Actor-Critic

Soft Actor-Critic (SAC)  is an off-policy RL algorithm that learns a policy maximizing the sum of the expected discounted cumulative reward and the entropy of the policy, namely: \(J()=_{t=0}^{T}_{s_{t}}\ r+_{e}(( s_{t},g))\). The policy \(_{}(a s):()\) defines a map from state to distributions over actions, parameterized by \(\). A state-action value function is defined as \(Q_{}(s,g,a):\), parameterized by \(\). The policy parameters can be learned by directly minimizing the expected KL divergence and the \(Q\)-value is trained by minimizing the soft Bellman residual, which are defined as the following loss functions:

\[_{} =_{(s,g)}_{a_{ }}[_{e}(_{}(a s,g)-Q_{}(s,g,a))]]\] (1) \[_{Q} =_{(s,a,r,s^{},g)}[Q_{} (s,g,a)-(r+_{r}_{s^{}} [V_{}(s^{},g)])^{2}]\] (2)

where \(V_{}(s,g)=_{a}[Q_{}(s,g,a)-_{}(a  s,g)]\), \(\) is the replay buffer and \(_{e}\) is the temperature parameter that controls the stochasticity of the optimal policy.

### Wasserstein Distance and Adversarial Intrinsic Motivation Reward Function

The Wasserstein distance offers a way to quantify the amount of work required to transport one distribution to another distribution. The Wasserstein-p distance \(W_{p}\) between two distributions \(\) and \(\) on a metric space \((,d)\), where \(\) is a set and \(d\) denotes a metric on \(\), is defined as follows :

\[W_{p}(,): =_{(,)}(_{}d(x,y)^{p}d (x,y))^{1/p}=_{(,)}_{(X,Y) }[d(X,Y)^{p}]^{1/p}\] (3)

where \((,)\) denotes the set of all possible joint distributions \((x,y)\) whose marginals are \(\) and \(\). Intuitively, \((x,y)\) tells what is the least amount of work, as measured by \(d\), that needs to be done to convert the distribution \(\) into the distribution \(\). A timestep quasi-metric \(d^{}(s,g)\) can be used as a distance metric. It estimates the work needed to transport one distribution to another, representing the number of transition steps required to reach the goal state \(g\) for the first time when following the policy \(\).

As proposed by Durugkar et al. , the AIM reward function can be learned by minimizing the Wasserstein distance between the state visitation distribution \(_{}\) and the desired goal distribution \(\). Through the minimization of the Wasserstein distance \(W_{1}(_{},)\) (for \(p=1\), \(W_{1}\) is known as the Kantorovich-Rubinstein distance), a reward function can be formulated to estimate the work required to transport the state visitation distribution \(_{}\) to the desired goal distribution \(\) and is expressed as follows: \(W_{1}(_{},)=_{\|f\|_{L} 1} [_{g}[f(g)]-_{s _{}}[f(s)]]\). If the state visitation distribution \(_{}(s)\) comprises states that optimally progress towards the goal \(g\), the potential function \(f(s)\) increases along the trajectory, reaching its maximum value at \(f(g)\). The reward function, which can be approximated using a neural network, denoted as \(r_{}^{}\), increases as the states approach the desired goal \(g\). \(r_{}^{}\) can be trained using the data collected by the policy \(\). Leveraging the estimation of the Wasserstein distance \(W_{1}(_{},)\), the loss function for training the parameterized reward function \(r_{}^{}\) is defined as follows:

\[_{}=_{(s,g)}[f_{}^{}( s)-f_{}^{}(g)]+_{(s,s^{},g) }[(|f_{}^{}(s)-f_{}^{} (s^{})|-1,0)^{2}]\] (4)

where the second component of the sum is a penalty term and the coefficient \(\) is necessary to ensure smoothness . The reward can be calculated as \(r_{}(s,g)=f_{}^{}(s)-f_{}^{}(g)\), which is the negative of the Wasserstein distance \(-W_{1}(_{},)\).

### Diffusion Models

Diffusion models  express a probability distribution \(p(x_{0})\) through latent variables in the form \(p_{}(_{0}):= p_{}(_{0:N})d _{1:N}\), where \(_{1},,_{N}\) are latent variables of the same dimensionality as the data \(_{0} p(_{0})\). They are characterized by a forward and a reverse diffusion process.

The forward diffusion process approximates the posterior \(q(_{1:N}_{0})\) using a Markov chain that perturbs the input data by gradually adding Gaussian noise \(_{0} q(_{0})\) in \(N\) steps with a predefined variance schedule \(_{1},,_{N}\). This process is defined as:

\[q(_{1:N}_{0}) :=_{k=1}^{N}q(_{k}_{k-1})\] (5) \[q(_{k}_{k-1}) :=(_{k};}_{k-1}, _{k})\]

The reverse diffusion process aims to recover the original input data from the noisy (diffused) data. It learns to progressively reverse the diffusion process step by step and approximates the joint distribution \(p_{}(_{0:N})\). This process is defined as:

\[p_{}(_{0:N}):=p(_{N})_{k=1}^{N} p_{}(_{k-1}_{k})\] (6)

\[p_{}(_{k-1}_{k}):=(_{ k-1};_{}(_{k},k),_{}(_ {k},k))\]

where \(p(_{N})=(_{N};,)\). The optimization of the reverse diffusion process is achieved by maximizing the evidence lower bound (ELBO) \(_{q}[(_{0:N})}{q(_{1:N}_{0})}]\). Once trained, sampling data from Gaussian noise \(_{N} p(_{N})\) and running through the reverse diffusion process from \(k=N\) to \(k=0\) yields an approximation of the original data distribution.

## 4 Methodology

As discussed earlier, in multi-goal RL, the desired goal is sampled from a desired goal distribution in each episode, and the agent aims to achieve multiple goals. By integrating a curriculum design into multi-goal RL, we can reformulate a task in such a way that it starts with easier goals and progressively increases in difficulty. Our curriculum diffusion-model-based goal-generation method works as follows. Given the presence of two different types of timesteps for the diffusion process and the RL task, we denote the diffusion timesteps using subscripts \(k\{1,,N\}\) and the RL trajectory timesteps using subscripts \(t\{1,,T\}\). Our curriculum goal generation takes the state \(s\) as an input. It then outputs a curriculum goal set \(_{c}\), which is obtained through the reverse diffusion process of a conditional diffusion model as follows:

\[_{c}=p_{}(_{0:N} s)= (_{N};,)_{k=1}^{N}p_{}( _{k-1}_{k},)\] (7)

where \(_{0}\) is the end sample of the reverse diffusion process used as a curriculum goal. Commonly, \(p_{}(_{k-1}_{k},)\) is a conditional distribution parametrized by \(\) and is chosen to model a multivariate Gaussian distribution \((_{k-1};_{}(_{k},,),_{}(_{k},,k))\). Following , rather than learning the variances of the forward diffusion process, we assign a fixed covariance matrix \(_{}(_{i},,i)=_{i}\) and a mean defined as:

\[_{}(_{k},,k)=}}(_{k}-}{}}_{ }(_{k},,k))\.\] (8)

Initially, we sample a Gaussian noise \(_{N}(,)\). We then apply the reverse diffusion process parameterized by \(\) starting from the last step \(N\) and proceeding backward to step \(1\):

\[_{k-1}_{k} =_{k}}{}}-}{(1-_{k})}}_{}( _{k},,k)+}\] (9) \[(,),k=N,,1\.\]

For the case \(k=1\) (last term of the reverse diffusion process), we set \(\) to \(\), to enhance the sampling quality by ignoring \(}\) and edge effects. In fact, as empirically demonstrated by , training the diffusion model yields better results when utilizing a simplified loss function that excludes the weighting term. Thus, we adopt the following simplified loss function to train the conditional \(_{}\)-model, where \(_{}\) is a function approximator designed to predict \(\)1.

\[_{d}()=_{k, (,),(,_{0}) }[\|-_{} (_{k}}_{0}+_{k}} ,,k)\|^{2}],\] (10)

where \(\) is a uniform distribution over the discrete set \(\{1,,N\}\) and \(\) denotes the replay buffer collected by policy \(\).

To sample feasible curriculum goals for the agent, we integrate the \(Q\)-function and the AIM reward function into the loss. The total loss function for training the diffusion model is defined as follows:

\[=_{d}_{d}()-_{s, ,_{0}_{c},g_{d}}[_{ q} Q_{}(s,_{0},(s,_{0}))+_{r} r_{ }(_{0},g_{d})]\] (11)

The rationale is that, while minimizing the loss \(_{d}\) allows us to accurately capture the state distribution, we also aim to simultaneously maximize the expected value of \(Q\) and the AIM reward function \(r_{}\), using the weights \(_{d}\), \(_{q}\), and \(_{r}\) to adjust the relative importance of the three components of the loss function. More in detail, the \(Q\)-function predicts the cumulative reward starting from a state and following the policy, while the AIM reward function estimates how close an agent is to achieving its goal. By maximizing \(Q\) and the AIM reward, we can generate curriculum goals that are neither overly simplistic nor excessively challenging, progressing towards the desired goal.

In Eq. (11), \(_{0}\) is obtained by sampling experiences from the replay buffer using Eq. (9) through a reverse diffusion process parameterized by \(\). Therefore, taking the gradient of \(Q\) and the AIM reward \(r_{}\) involves back-propagating through the entire diffusion process. After we obtain the set of curriculum goals \(_{c}\) from the diffusion model, we use a bipartite graph \(G(\{V_{x},V_{y}\},E)\) with edge costs \(w\), composed of vertices \(V_{x}\) (i.e., the curriculum goal candidates derived from the diffusion model) and vertices \(V_{y}\) (i.e., the desired goals), where \(E\) represents the edge weights, and select the optimal curriculum goal \({g_{c}}\)2. We employ the Minimum Cost Maximum Flow algorithm in order to solve the bipartite matching problem and identify the edges with the minimal cost \(w\):

\[_{}^{c}:|}^{c}|=K}_{g_{t=0,..}^{ 0} x}^{d},g_{+}^{i}}^{+}}w  w=-)^{2}}{N}}\.\] (12)

The specifics of the generation of intermediate goals through diffusion models are explained in Algorithm 1, while the overall algorithm is outlined in Algorithm 2.

In Algorithm 1, during the training iterations, we sample Gaussian noise \(_{N}(,)\) to denoise the data in the reverse diffusion process. Between lines 5 and 7, we iterate from time step \(N\) to \(1\), where we perform the reverse diffusion process using Eq. (9) and subtract the predicted noise \(_{}\) from \(_{N}\) to denoise the noisy goal iteratively. We then uniformly sample a timestep \(k\) from the range between \(1\) and \(N\) (line 8) and sample a Gaussian noise \((,)\) (line 9) in order to calculate the diffusion loss defined in Eq. (10). In line 10, we calculate the diffusion loss, the \(Q\)-value, and the AIM reward function using the generated goal \(_{0}\) from the reverse diffusion process. Then in line 11, we calculate the total loss defined in Eq. (11) and update the diffusion model parameters \(\) using gradient descent. Finally, we return the generated goal \(_{0}\).

In Algorithm 2 we begin by defining an off-policy algorithm denoted as \(\). While any off-policy algorithm could be employed, we choose Soft Actor-Critic (SAC) to align with the baseline algorithms tested in our experiments. In line 5, we sample the initial state and provide the curriculum goal \(g_{c}\) to the policy, along with the current state. The policy generates an action (line 7), and executes it in the environment (line 8). Subsequently, the next state and reward are obtained (line 9). Then, we provide the minibatch \(b\) to the curriculum goal generator in line 11 to generate a curriculum goal set \(_{c}\). Then we find the optimal curriculum goal \(g_{c}\) using bipartite graph optimization (line 12). Furthermore, the loss functions defined in Eq. (1) and Eq. (2) are calculated, and the networks approximating \(\) and \(Q\), as well as the AIM reward function \(r_{}\) defined in Eq. (4), are updated. Between line 15 and 21, we run \(n\) test rollouts and extract the achieved state of the agent using \(\) and calculate the success rate in reaching the desired goal within a threshold value \(\).

```
1:Input: state \(s\), no. of reverse diffusion timestep \(N\), training step \(M\)
2:Obtain states \(s\) from the minibatch \(b\)
3:for i \(=1,,M\)do\(\) Training iterations
4:\(_{N}(,)\)
5:for k \(=N,,1\)do\(\) Reverse diffusion process
6:\((,)\)
7:\(g_{k-1}=}}(g_{k}-}{}}_{}(g_{k},k,s))+}\)\(\) Using Eq. (9)
8:\(k(\{1,,N\})\)
9:\((,)\)
10: Calculate diffusion \(_{d}(),Q(s,_{0},(s)),r(g_{d},_{0})\)\(\) Calculate with the generated goal \(_{0}\)
11: Calculate total loss \(=_{d}_{d}()-_{q}Q(s,_{0},(s))- _{r}r(_{0},g_{d})\)\(\) Eq 11
12:\(-_{}\)\(\) Take gradient descent step and update the diffusion weights return\(_{0}\) ```

**Algorithm 2** RL Training and Evaluation

```
1:Input: no. of episodes \(E\), timesteps \(T\)
2:Select an off-policy algorithm \(\)\(\) In our case, \(\) is SAC
3:Initialize replay buffer \(\), \(g_{c}\{g_{d}\}\) and networks \(Q_{}\), \(_{}\), \(r_{}\)
4:for episode \(=0 E\)do
5: Sample initial state \(s_{0}\)
6:for\(t=0 T\)do
7:\(a_{t}=(s_{t},g_{c})\)
8: Execute \(a_{t}\), obtain next state \(s_{t+1}\)
9: Store transition \((s_{t},a_{t},r_{t},s_{t+1},g_{c})\) in \(\)
10: Sample a minibatch \(b\) from replay buffer \(\)
11:\(_{c}(b)\)
12: Find \(g_{c}\) that maximizes \(w\) in Eq. (12)
13: Update \(Q\) and \(\) with \(b\) to minimize \(_{Q}\) and \(_{}\) in Eq. (1) and in Eq. (2)
14: Update the AIM reward function \(r_{}\)\(\) Success rate
15:\(success 0\)
16: Sample a desired goal \(g_{d}\)
17:for i \(=1 n_{testrollout}\)do
18:\(a_{t}=(s_{t},g_{d})\)
19: Execute \(a_{t}\), obtain next state \(s_{t+1}\) and reward \(r_{t}\)
20:if\(|(s_{t+1})-g_{d}|\)then
21:\(success= success+1/n_{testrollout}\) ```

**Algorithm 3** RL Training and Evaluation

## 5 Experiments

To evaluate our proposed method, we conducted experiments across three maze environments simulated in MuJoCo3: _PointUMaze_, _PointNMaze_, and _PointSpiralMaze_. In these environments, a goal is interpreted as the \((x,y)\) position of the agent achieved in an episode. These environments have been specifically chosen due to their structural characteristics, which are ideal for testing environment-agnostic curriculum generation strategies. Moreover, they present a variety of complex and diverse navigation challenges, requiring an agent to learn effective exploration and exploitation.

We compared our approach DiCuRL against nine state-of-the-art CRL algorithms, namely ACL , GOAL-GAN , HGG , ALP-GMM , VDS , PLR , CURROT , GRADIENT , and OUTPACE , each one run with 5 different random seeds. The primary conceptual differences between ours and baseline algorithms are summarized in Table 1. Details on the baseline algorithms, parametrization, training setup, and maze environments are given in Supp. Mat. C. Our codebase is available at: https://github.com/erdiphd/DiCuRL/.

The results, shown in Fig. 14 and detailed in Table 25, demonstrate the effectiveness of the proposed DiCuRL method. Notably, DiCuRL outperforms or matches all the baseline methods. For the PointNMaze and PointSpiralMaze, the success rate of all methods, except for ours and OUTPACE (and HGG for PointNMaze), is close to zero (detailed results are reported in Supp. Mat. C). ACL, GoalGAN, ALP-GMM, VDS, and PLR, which lack awareness of the target distribution, underperform compared to target-aware methods such as our proposed approach, OUTPACE, GRADIENT, CURROT, and HGG. HGG encounters difficulties because of infeasible curriculum proposals. This is a result of its reliance on the Euclidean distance metric.

Both CURROT and GRADIENT generate curriculum goals using the optimal transport method and the Wasserstein distance metric, relying on the geometry of the environment. This dependence might be the reason for their inconsistent and poor performance across the different environments.

OUTPACE, utilizes instead the Wasserstein distance and an uncertainty classifier for uncertainty-aware CRL, exhibiting similar performance to our approach but with slower convergence and higher variance in success rate. This is likely due to the fact that OUTPACE's curriculum heavily relies on the visited state distributions, which necessitate an initial exploration by the agent. While also our approach depends on visited states, incorporating the Qand AIM reward functions into curriculum generation facilitates exploration beyond them, potentially explaining the performance differences between DiCuRL and OUTPACE.

Lastly, we recall that our approach generates curriculum goals based on the reverse diffusion process, which allows the generated curriculum goals to gradually shift from the initial state distribution to the desired goal distribution. Fig. 2 shows an example of a curriculum set generated by DiCuRL, for the case of the PointUMaze environment, at each iteration of the reverse diffusion process. Fig. 3, instead, shows the differences between the curriculum goals generated by DiCuRL, GRADIENT, and HGG in the PointSpiralMaze environment: it can be seen how DiCuRL manages to generate curriculum goals that explore the whole environment more effectively than the baseline algorithms. Supp. Mat. D shows the curriculum goals generated in the other two environments and illustrates the dynamics of the diffusion process during training for the case of PointUMaze. To demonstrate the applicability of DiCuRL to robot manipulation tasks, we evaluated it on the FetchPush and FetchPickAndPlace tasks using a sparse reward setting. Further details can be found in Supp. Mat. E.

**Ablation Study** We conducted an ablation study to investigate the impact of the AIM reward function \(r_{}\) and \(Q_{}\) function in generating curriculum goals with our method (DiCuRL). For that, we omitted, separately, the reward function \(r_{}\) and the \(Q_{}\) function from Eq. 11, and plotted the success rate (with three different seeds) in Fig. 3(a) for the most challenging maze environment, PointSpiralMaze. The results indicate that the agent performs worse without the AIM reward function \(r_{}\) and fails to achieve the task without the \(Q_{}\) function. The generated curriculum goals without the \(r_{}\) or \(Q_{}\) function are shown in Fig. 3(b) and 3(c), respectively. Fig. 3(d), instead, illustrates the AIM reward value across different training episodes in a clockwise direction. Specifically, the first row and first column

  
**Algorithm** & **Curriculum method** & **Target dist. curriculum** & **Geometry-agnostic** & **Off-policy** & **External** & **Venue, year** \\  ACL  & LSTM & ✗ & ✓ & ✗ & ✗ & PMLR, 2017 \\ GoalGAN  & GAN & ✗ & ✓ & ✗ & ✗ & PMLR, 2018 \\ HGG  & \(Q,,W_{2}\) & \(^{+}\) & ✗ & ✓ & ✗ & NeurIPS, 2019 \\ ALP-GMM  & GMM & ✗ & ✓ & ✓ & ✗ & PMLR, 2020 \\ VDS  & \(Q,\) & ✗ & ✓ & ✓ & ✗ & NeurIPS, 2020 \\ PLR  & TD-Error & ✗ & ✓ & ✗ & ✗ & PMLR, 2021 \\ CURROT  & \(W_{2}\) & \(^{+},\) & ✗ & ✓ & ✗ & PMLR, 2022 \\ GRADIENT  & \(W_{2}\) & \(^{+}\) & ✗ & ✓ & ✓ & NeurIPS, 2022 \\ OUTPACE  & CNML & \(^{+}\) & ✓ & ✓ & ✓ & ICLR, 2023 \\  DiCuRL (Ours) & Diffusion & \(^{+}\) & ✓ & ✓ & ✓ & \\   

Table 1: Comparison of DiCuRL with previous CRL methods from the literature (sorted by year).

in Fig. 3(d) represent the reward values at the very beginning of training. As training progresses, the reward values shift towards the left corner of the maze environment (1st row, 2nd column). In the middle of training, the reward values are concentrated around the left corner of the maze environment (2nd row, 2nd column), and, by the end of training, the reward values converge to the desired goal area (2nd row, 1st column). This progression explains why the generated curriculum goals are not guiding the agent effectively but are instead distributed in the corner points shown in Fig. 3(c). We have also demonstrated the behavior of the AIM reward function across different training episodes in our Supp. Mat. D for the PointUMaze environment. Additionally, we examined the impact of SAC with a fixed initial state \(\) and SAC with a random initial state. To do that, we removed the curriculum goal generation mechanism and assigned the desired goal, and then trained the agent using either SAC with a fixed initial state \(\) or SAC with a random initial state. For the random initial state, we sampled goals randomly in the environment. To avoid starting the agent inside the maze walls, we performed an infeasibility check, resampling the initial state until it was feasible. We compared our approach using three different seeds with both the fixed initial state + SAC and the random initial state + SAC across all maze environments, and the success rates are shown in Fig. 5.

  
**Algorithm** & **PointUMaze** & **PointNMaze** & **PointSpiralMaze** \\  DiCuRL (Ours) & \(28333 3036\) & \(108428 34718\) & \(305833\ 43225\) \\ OUTPACE & \(29800 4166\) & \(113333 24267\) & \(396875 111451\) \\ HGG & \(48750 24314\) & NaN & NaN \\ GRADIENT & \(263431 114795\) & NaN & NaN \\   

Table 2: No. of timesteps (rounded) to reach success rate \(1.0\) (average \(\) std. dev. across \(5\) runs). NaN indicates that a success rate of \(1.0\) is not reached within the maximum budget of 1e6 timesteps.

Figure 1: Test success rate for the algorithms under comparison on the three maze tasks.

Figure 2: The curriculum goal set \(_{c}\) generated by DiCuRL during the reverse diffusion process (lines 5-7 in Algorithm 1) for the PointUMaze environment. The color indicates the goals generated at a specific iteration step during the reverse diffusion process of the diffusion model. Then these goals are selected in Eq. (12) based on the given cost function.

## 6 Conclusion and Limitations

In this work, we introduced DiCuRL, a novel approach that utilizes diffusion models to generate curriculum goals for an RL agent. The diffusion model is trained to minimize its loss function while simultaneously maximizing the expected value of \(Q\) and the AIM reward function. The minimization of the diffusion loss helps capture the visited state distribution, while the maximization of \(Q\) and AIM reward function helps generate goals at an appropriate difficulty level and at the same time guide the generated curriculum goals closer to the desired goal. Furthermore, the generated goals promote exploration due to the inherent noising and denoising mechanism of the diffusion model. Our proposed approach has two main limitations. First, while diffusion models excel at handling high-dimensional data, such as images , incorporating the AIM reward into a combined loss function can hinder the curriculum goal generation in such settings as the AIM reward may underperform in higher dimensionalities. Secondly, we employ the Minimum Cost Maximum Flow algorithm to select the optimal curriculum goals from the set generated by the diffusion model. However, alternative selection strategies could potentially be more effective. Future work will aim to address these limitations and extend our approach to more complex environments.

Figure 4: (a) Test success rate in the ablation study of DiCuRL on the PointSpiralMaze environment. (b) Curriculum goals when using only \(Q_{}\). (c) Curriculum goals when using only the AIM reward \(r_{}\). (d) AIM reward \(r_{}\) across different training episodes (around 60k, 100k, 250k and 600k timesteps), displayed clockwise from the top-left to the bottom-left quadrant.

Figure 5: Test success rate comparison between DiCuRL, SAC with random initial position, and SAC with fixed initial position. Note that the success rate for the PointUMaze environment in Fig. 4(a) is shown up to timestep \(10^{5}\), whereas the others are shown up to \(10^{6}\).

Figure 3: Curriculum goals generated by DiCuRL, GRADIENT, and HGG in the PointSpiralMaze environment. The colors ranging from red to purple indicate the curriculum goals across different episodes of the training, and the orange dot and red dot are the agent and desired goal, respectively.

Acknowledgments

This study was supported by the CiLoCharging project, which is funded by the Bundesministerium fur Wirtschaft und Klimaschutz (BMWK) of Germany under funding number 01ME20002C. Additionally, this work was supported by TUBITAK under the 2232 program, project number 121C148 ("LiRA"). We would like to express our gratitude to Arda Sarp Yenicesu for his assistance in writing this paper.