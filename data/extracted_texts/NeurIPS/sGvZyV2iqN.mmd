# HairFastGAN: Realistic and Robust Hair Transfer

with a Fast Encoder-Based Approach

 Maxim Nikolaev\({}^{1,3}\), Mikhail Kuznetsov\({}^{1,2,3}\), Dmitry Vetrov\({}^{4}\), Aibek Alanov\({}^{1,3}\)

\({}^{1}\)HSE University, \({}^{2}\)Skolkovo Institute of Science and Technology, \({}^{3}\)AIRI

{m.nikolaev, m.k.kuznetsov, alanov}@airi.net

\({}^{4}\)Constructor University, Bremen

dvetrov@constructor.university

###### Abstract

Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on. This task is challenging due to the need to adapt to various photo poses, the sensitivity of hairstyles, and the lack of objective metrics. The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow. At the same time, faster encoder-based models are of very low quality because they either operate in StyleGAN's W+ space or use other low-dimensional image generators. Additionally, both approaches have a problem with hairstyle transfer when the source pose is very different from the target pose, because they either don't consider the pose at all or deal with it inefficiently. In our paper, we present the HairFast model, which uniquely solves these problems and achieves high resolution, near real-time performance, and superior reconstruction compared to optimization problem-based methods. Our solution includes a new architecture operating in the FS latent space of StyleGAN, an enhanced inpainting approach, and improved encoders for better alignment, color transfer, and a new encoder for post-processing. The effectiveness of our approach is demonstrated on realism metrics after random hairstyle transfer and reconstruction when the original hairstyle is transferred. In the most difficult scenario of transferring both shape and color of a hairstyle from different images, our method performs in less than a second on the Nvidia V100.

## 1 Introduction

Advances in the generation of face images using GANs  have made it possible to apply them to semantic face editing . One of the most challenging

Figure 1: **HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach.**_Our method takes as input a photo of the face, desired shape and hair color and then performs the transfer of the selected attributes. You can also see a comparison of our model with the others in the right plot. We were able to achieve excellent image realism while working in near real time._and interesting topic in this area is hairstyle transfer . The essence of this task is to transfer hair attributes such as color, shape, and structure from the reference photo to the input image while preserving identity and background. The understanding of mutual interaction of these attributes is the key to a quality solution of the problem. This task has many applications among both professionals and amateurs during work with face editing programs, virtual reality and computer games.

Existing approaches that solve this problem can be divided into two types: optimization-based [25; 41; 16; 15; 42; 33; 3], by obtaining image representations in some latent space of the image generator and directly optimizing it for the corresponding loss functions to transfer the hairstyle, and encoder-based [32; 13; 27; 7], where the whole process is done with a single direct pass through the neural network. The optimization-based methods have good quality but take too long, while the encoder-based methods are fast but still suffer from poor quality and low resolution. Moreover, both approaches still have a problem if the photos have a large pose difference.

We present a new HairFast method that works in high resolution, is outperforms in quality to state-of-the-art optimization methods, and is suitable for interactive applications in terms of speed, since we use only encoders in the inference process. We propose our decomposition of the problem and solve each of the subtasks efficiently. In particular, we developed a new approach for pose adaptation, a new approach for FS space regularization, a more efficient approach for hair coloring, and developed a new module for detail recovery. Our framework consists of four modules: pose alignment, shape alignment, color alignment and refinement alignment. Each module solves its own subtask by training specialized encoders.

We have conducted an extensive series of experiments, including attribute changes both individually (color, shape) and in combination (color and shape), on the CelebA-HQ dataset  in various scenarios. Based on standard realism metrics such as FID , FIDCLIP  and runtime, the proposed method shows comparable or even better results than state-of-the-art optimization-based methods while having inference time comparable to the fastest HairCLIP  method.

## 2 Related Works

GANs.Generative Adversarial Networks (GANs) have significantly advanced research in image generation, and recent models such as ProgressiveGAN , StyleGAN , and StyleGAN2  produce highly detailed and realistic images, especially in the area of human faces. Despite the progress made in face generation, high-quality, fully controlled hair editing remains a challenge due to the many side effects.

Latent Space Embedding.Inversion techniques [1; 29; 40; 43; 23; 30] for StyleGAN generate latent representations that balance editability and reconstruction fidelity. Methods that prioritize editability map real images into a more flexible latent subspace, such as \(W\) or \(W+\), which can reduce the accuracy of the reconstruction - a popular example is E4E . Reconstruction-focused methods, on the other hand, aim for an exact restoration of the original image. For example Barbershop merges the structural feature space (\(F\)) with the global style space (\(S\)) to form a composite space (\(FS\)). Such decomposition enhances the representational capacity of the space. Utilizing both the \(W+\) and \(FS\) latent spaces, we have created a comprehensive hair editing framework that allows for a wide range of potential realistic adjustments.

Optimization-based methods.Among the classical optimization methods, we can highlight Barbershop , which uses multi-stage optimization in the StyleGAN FS space. But Barbershop doesn't work well with large pose differences, a problem that StyleYourHair  tries to solve by using local style matching and pose alignment loss, which allows efficient face rotation before hair transfer. Other approaches to hair editing include: StyleGANSalon , which solves the rotation problem with EG3D , HairNet , which has learned to handle complex poses and uses PTI  to improve quality but has lost the ability to independently transfer hair color, HairCLIPv2  which can interact with images, masks, sketches and texts, HairNeRF  which uses StyleNeRF  instead of StyleGAN to provide distortion-free hair transfer in case of complex poses.

Encoder based methods.Encoder-based methods replace optimization processes with training a neural network, speeding up runtime a lot. Among the best models, we can highlight CtrlHair  that uses SEAN  as a feature encoder and generator. The method still suffers from complex caseswith different facial poses and the authors solve this by inefficient postprocessing of the mask due to which the method is slow. HairCLIP , which is an order of magnitude faster than CtrlHair, uses CLIP  feature extractor. The method allows to edit hair with text, but it works in \(W+\) space, which causes poor preservation of face identity and hair texture. Other approaches to hair editing include: MichiGAN , HairFIT  Encoder-based methods show significantly worse performance than optimization-based approaches especially in difficult cases with different head poses and lightning conditions. We propose the first encoder-based framework that achieves comparable quality with methods that use optimizations.

## 3 Method

### Overview

Formally we will solve the following problem, we have a source image \(I_{}\) to which we want to transfer the style and shape of \(I_{}\), and an image with the desired hair color \(I_{}\).

In this problem setting, we will solve the shape and color transfer problems independently using Shape alignment and Color alignment respectively to make the method flexible. Also, before these modules, we propose a new stage Pose alignment which purpose is to remove the pose mismatch between image \(I_{}\) and images \(I_{}\) and \(I_{}\). And after all these modules we introduce a new Refinement alignment stage, which should restore the details of the face and background that we may have lost after the hair and color transfer stage. You can see the general pipeline of our approach on the picture Fig. 2.

In this work, we propose a unique solution for each of these modules that shows high quality and high performance compared to existing approaches. In particular, we do not use optimization to solve these problems, which allows us to speed up the overall pipeline greatly. In our HairFast method we propose an efficient Pose alignment approach by adding a new Rotate Encoder whose purpose is to change the face latent to rotate it. In Shape Alignment we propose a new FS mixing approach that allows to transfer the hair while keeping the possibility to edit it for color changes, besides we propose an efficient use of SEAN for inpainting at this stage. Also in Color Alignment we offer a new architecture using CLIP embeddings and new losses, which has significantly improved the results of this stage. Finally, we propose a completely new Refinement alignment stage.

First of all, our method starts with Pose alignment block, its purpose is to generate a segmentation mask with a target hair shape. This block takes as input images of the original face, the desired hair and their \(W+\) representations in StyleGAN space. Then, a Rotate Encoder is run inside the block, which rotates the image with the desired hair to the same pose as the original face, followed by a Shape Encoder to adapt the resulting hairstyle at the level of the segmentation mask.

In the next Shape alignment module, we transfer the hairstyle shape from \(I_{}\) to \(I_{}\) by changing only the tensor \(F\) from the FS space of StyleGAN. To do this, we generate the tensor \(F\) for the inpaint after changing the shape using SEAN and target mask from Pose alignment. Once we have all the necessary \(F\) tensors, we aggregate them taking into account the segmentation masks, selecting the desired parts, thus obtaining a new \(F\) tensor that corresponds to the image with the desired hair shape.

The next Color module is designed to transfer the hair color from the \(I_{}\). To do this, we edit the \(S\) space of the source image using our trained encoder, which also takes as input the \(S\) tensor of the reference and additional CLIP  embeddings of the source images.

Figure 2: **Overview of HairFast: the images first pass through the Pose alignment module, which generates a pose-aligned face mask with the desired hair shape. Then we transfer the desired hairstyle shape using Shape alignment and the desired hair color using Color alignment. In the last step, Refinement alignment returns the lost details of the original image where they are needed.**

The image generated after the Color module can already be considered as the final image, but in our work we also introduce a new Refinement alignment module. The purpose of this module is to restore the necessary details of the original image that were lost after inversion and editing. This allows us to preserve the identity of the face and increase the realism of the method.

### Pose alignment

In this step, we want to get a segmentation mask of the original face with the desired hair shape. Such a task of generating a target mask without optimization problems was very successfully solved by the authors of the CtrlHair  method using Shape Encoder, which encodes the segmentation masks of two images as separate embeddings of hair and face, and Shape Adaptor reconstructs the segmentation mask of the desired face with the desired hair shape, additionally performing inpaint. Nevertheless, this approach still has problems.

First of all, the Shape Adaptor and Shape Encoder itself has been trained to transfer the hair shape as it is in the current pose and so for the case where the source and shape photos have too different poses, the method performs very poorly, causing the final photo to show severe hair shifts. The authors of CtrlHair have partially solved this problem with a slow and ineffective post-processing of the mask.

In our approach, we introduce a new Rotate Encoder that is trained to rotate the shape image to the same pose as the source image. This is accomplished by changing the latency of the image \(w^{}\) received from the E4E encoder . The new image is then segmented and given to the input of the encoder and shape adapter as the desired hair shape. Since we don't need detailed hair to generate the mask, we use the E4E representation of the image. The Rotate Encoder has been trained with very good interpolation and can rotate the image to the most complex pose while maintaining the original shape of the hair. At the same time, Encoder does not mess up the hairstyles if the image poses already match. After rotation, we obtain the image mask using BiSeNet :

\[w_{}=_{}(w^{}_{}, ~{}w^{}_{}), M_{}=(G(w_{})),\] (1)

where G is StyleGAN and \(w^{}=(I)\) - the latent E4E representation for \(I_{}\) and \(I_{}\).

For training Rotate Encoder, we used keypoint optimization with a pre-trained STAR  model as well as cycle-consistency reconstruction loss. See the Appendix 9.1 for more information about the Rotate Encoder.

This approach allows for a high quality transfer of most hairstyles even with the most complex pose differences, correcting artifacts that occur even in the StyleYourHair method, as will be shown in the experiments section. A diagram of the Pose alignment module architecture is shown in Fig. 3. More formally, the generation of the final \(M_{}=_{}(_{},~{} _{})\), where

\[_{}=^{}_{}(M_{ {rotate}}),_{}=^{}_{ {Enc}}(M_{}), M_{}=(I_{}).\] (2)

The last step we extract the hair area mask \(H_{}\) from \(M_{}\).

These masks \(H_{}\) and \(M_{}\) from the Pose alignment module will be needed for the next alignment tasks - inpaint generation and shape transfer in the Shape alignment module. A diagram of the Pose alignment module is shown in Fig. 3.

Figure 3: Detailed diagram of the units. (a) Mixing block mixes FS and W+ space representations to allow color editing (b) The Pose alignment module diagram generates a pose-aligned mask with the desired hair shape, and the Shape alignment module diagram that takes the images themselves, their segmentation masks, \(W+\) and \(F\) representations to transfer the desired hairstyle shape.

### FS and W+ mixing

Before transferring the new hairstyle and doing coloring, there's one more problem to address. The native integration of the FS encoder  cannot edit \(S\) space in a way that transfers hair color. To solve this problem, we additionally use E4E - it is a very simple encoder with relatively poor image reconstruction quality, but has high editability. For this, we also reconstruct all images with E4E and mix the \(F\) tensor corresponding to the hair with the \(F\) tensor obtained with the FS encoder.

Formally, if \(I\) is input image, then \(F_{16}^{},\ S=_{}(I),\ w^{}=(I)\), where \(F_{16}^{}^{16 16 512},S^{12  512}\) - the \(FS\) representation obtained from FS encoder and \(w^{}^{18 512}\) - the encoding from E4E.

Since we want to edit images in \(F\) space of 32x32 resolution while FS encoder produces only 16x16, we need to run a few more StyleGAN blocks, while for E4E we run all 6 first blocks:

\[F_{32}^{}=G_{4:6}(F_{16}^{},S), F_{32}^{} =G_{6}(w^{}),\] (3)

where \(G_{6}\) - the output of the first 6 StyleGAN blocks and \(G_{4:6}\) - the generator starts at block 4.

To find the hair region in the \(F\) tensor, we use BiSeNet to segment a face and downsize the selected hair mask. Then final reconstruction for images \(F_{32}^{}\):

\[M=(I), H=_{32}(M=),\] (4) \[F_{32}^{}= F_{32}^{}+(1- ) H F_{32}^{}+ H F_{32}^{}.\] (5)

Here \(\) is an inversion of the mask \(H\) and \(\) is the hyperparameter for mixing. In our work it is equal to 0.95. This means that we only take 5% of the hair from FS encoder, but according to our experiments, even this small mixing with the hair \(F\) tensor of FS encoder hair greatly increases the quality. The visualization of the mixing procedure shown in Fig. 3.

This \(F_{32}^{}\) tensor allows us to get an excellent quality of face and background reconstruction and still edit the hairstyle.

### Shape alignment

In this step, our goal is to transfer the desired hair shape from \(I_{}\) to \(I_{}\). For this purpose, we edit only the \(F\) space. To achieve this, we solve 2 subtasks: generation of a target mask with the desired hair shape and generation of \(F\) tensor with the inpainted parts of the image.

For the inpaint task, we use the pre-trained SEAN model, which produces style vectors for each segmentation class using the input image and its segmentation mask, and its decoder reconstructs the image using the style vectors and any new segmentation mask. Thus, using this model, we can obtain a 256x256 resolution image with the desired hair shape for both source and shape photos.

\[_{}=_{}(I,\ M), I^{ }=_{}(_{},\ M_{ }).\] (6)

To get the \(F\) tensor representation of these images we use E4E. According to our experiments, the SEAN model in some cases produces strong artifacts in weakly represented segmentation classes such as ears, and due to artifacts on the target segmentation mask, it can also produce images with similar artifacts, such as when the hair is not connected to the head. E4E due to its good generalization is a good regularizer that automatically handles all such kind of artifact.

\[F_{32}^{}=G_{6}((I^{})).\] (7)

In the current step we have two initial \(F_{32}^{}\) tensors of images and two \(F_{32}^{}\) tensors after the inpaint part. The last step Alignment of F space combines all four F tensors into one new \(F_{32}^{}\), which can be used to generate an image with a given hairstyle. To do this, we assemble the tensor by selecting its corresponding parts using segmentation masks. A diagram of the Shape alignment module is shown in Fig. 3.

\[ F_{32}^{}&=H_{} H_{} F_{}^{}+H_{}}} F_{}^{}+ \\ &+}}}}  F_{}^{}+}} H_{ } F_{}^{}.\] (8)

Here, \(H_{}\) and \(H_{}\) are obtained from \(I_{}\) and \(I_{}\) from according to Eq. (4).

### Color alignment

In the next step, we solve the problem of changing the \(S\) space so as to change the hair color to the desired color. According to our experiments, we find that learning the convex combination of \(S_{}\) and \(S_{}\) as previous methods did is not enough for good quality, so we develop a new encoder to predict the change of \(S_{}\). Moreover, we find that using only \(S\) space is not enough to change hair color and preserve the rest of the image, so we include CLIP image embeddings to bring more information into the features. Finally, we experiment with losses and find that the cosine distance between CLIP embeddings works better than LPIPS .

As Color Encoder architecture we use 1D modulation layers similar to those used in StyleGAN. Such layers are excellent for style changing and have good stability. The purpose of this encoder is to predict the change of \(S_{}\) in such a way as to change the hair color to the desired color while preserving the rest of the image. We input \(S_{}\) as a style to the modulation layers with CLIP embeddings of the source image without hair: \(_{}=_{}(I_{} {H_{}}}})\) and CLIP embeddings of the hair only from the color image: \(_{}=_{}( H_{})\), where \(H_{}\) hair mask obtained from a Pose alignment module run to transfer a hair shape from \(I_{}\) to \(I_{}\). This helps to convey additional information about the original images that might have been lost after inverting images into latent spaces. And \(I_{}\) is the final image before Refinement alignment stage:

\[S_{} =_{}(S_{},\ S_{ },\ _{},\ _{}),\] (9) \[I_{} =(F_{32}^{},\ S_{}),\] (10)

A diagram of the method is shown in Fig. 4.

\[_{}(I_{1},I_{2},M_{1},M_{2})=1-_{}(I_{1} M_{1},I_{2} M_{2}),\] (11)

To train the model, we use the \(_{}\) one of which optimizes the cosine distance between the CLIP embeddings of the final and source images on both the reconstruction and transfer of the desired color. See the Appendix 9.2 for training and encoder details.

### Refinement alignment

Our method, even though it has a higher quality Color alignment step, still has a problem on complex cases where the face hue may change. Particularly because of this we cannot simply use Poisson blending like a CtrlHair, as the difference in shades emphasizes the overlay more and visually it doesn't look realistic.

For this reason, we are developing our own Refinement alignment module, which is essentially a larger and more powerful reconstruction encoder, but for a more complex task - reconstruction of the original face and background, reconstruction of hair after Color alignment module and inpaint of non-matching parts. This Refinement encoder generates an F tensor 4 times higher resolution than the FS encoder we used in the Shape alignment module. This allows for unrivaled reconstruction quality. Unlike traditional encoders that sacrifice reconstruction quality for good editing, we are able to use such a large resolution F tensor due to the fact that we do not have to edit the image after this module.

Figure 4: Detailed diagram of the units. (a) A color alignment module diagram that takes as input \(S\) image representations as well as segmentation masks. The purpose of this block is to encode the details of the original image and change the \(S\) space to transfer the desired hair color and preserve the identity. (b) A refinement alignment diagram that takes as input the source image and post Color alignment module image. At this module, the goal is to get a new representation in StyleGAN space to get a realistic image, with the original details of the source image that were lost after inverting images into latents.

Refinement itself consists of a trained FS encoder at resolution 64 for the usual reconstruction task, we use it to encode the original image \(I_{}\) and the \(I_{}\) image after our method:

\[F_{64}^{},\ S^{} =FS_{}(I_{}),\] (12) \[F_{64}^{},\ S^{} =FS_{}(I_{}).\] (13)

The resulting tensors \(\) are fused using IResNet blocks. In turn, S space is fused using two similar Color Encoder models, but without additional CLIP features. The output of this composite encoder is \(F_{64}^{}=_{}(F_{64}^{},\ F_{64}^{})\) tensor and an \(S_{}=_{}+_{}(S^{ },\ S^{})\) vector, which are input to StyleGAN to generate the final image \(I_{}\):

\[I_{}=(F_{64}^{},\ S_{}).\] (14)

For model training, we use loss functions for hair reconstruction and original parts of the image, and for inpaint we use guidelines from the more robust StyleGAN space and adversarial loss. For reconstruction, these include multi-scale perceptual loss , DSC++ , ArcFace and regularizations. See the Appendix 9.3 for training and encoders details.

A diagram of the Refinement procedure is shown in the Fig. 4. This produces the final HairFast model, which is shown in the Fig. 2.

## 4 Experiments

**Realism after editing.** The task of hairstyle transfer is very challenging, largely due to the lack of objective metrics. One possible metric to reflect the quality of hairstyle transfer is to measure the realism of the image using FID. To measure this metric, we consider 4 main cases: transferring hairstyle and color from different images (full), transferring only a new hairstyle shape (shape), transferring only a new color (color), and transferring both color and shape from the same image (both). To measure the metrics, we use the CelebA-HQ  dataset, from which we capture 1000 to 3000 experiments for each case, on which we run all methods. We used methods such as HairCLIP , HairCLIPv2 , CtrlHair , StyleYourHair  and Barbershop  for comparison, and for their inference we used the official code implementation. Additionally, we measure the median running time among all runs of these experiments, excluding the time to save the results to disk and initialize the neural networks. In the Table 1 you can observe the results of this experiment for the "full" and "both" cases, the complete table can be seen in the Appendix 12.

In these experiments, we do not compare with HairNet , HairNeRF  and StyleGANSalon  due to the lack of their code and the inability to run the methods on our images. Instead, we compare with StyleGANSalon on images they published from their inference along with LOHO  in Appendix 13, and we also make a visual comparison with HairNet and HairNeRF on images from their article in Appendix 14.

As we can see in the Table 1, a method like CtrlHair outperforms optimization-based methods like Barbershop and StyleYourHair by FID metrics. However, visual analysis reveals that the method performs much worse and artifacts are visible, which appear as a consequence of strong Poisson Blending of the final image with the original image. The authors in  studied the problem that makes images with strong artifacts appear more realistic by FID metric. They were able to solve this

   &  &  &  \\   & FID\(\) & FID\(\)\(\)\(\) & FID\(\) & FID\(\) & FID\(\) & LPIPS\(\) & PSNR\(\) & Time (s)\(\) \\   & full & both & full & both & medium & hard & medium & hard & & & A100 & V100 \\  HairCLIP  & 34.95 & 40.68 & 12.20 & 13.32 & 55.77 & 54.35 & 15.53 & 15.73 & 0.36 & 14.08 & **0.28** & **0.36** \\ HairCLIPv2  & 14.28 & 23.27 & 10.98 & 12.14 & 46.42 & 35.28 & 14.56 & 18.66 & 0.16 & 19.71 & 112 & 221 \\ CtrlHair  & 15.10 & 24.81 & 9.52 & 10.42 & 6.45 & 50.12 & 12.96 & 16.42 & 0.15 & 19.96 & 6.57 & 7.87 \\ StyleYourHair  & - & 25.90 & - & 10.91 & 46.32 & 47.19 & 13.70 & 15.93 & 0.14 & 21.74 & 84 & 239 \\ Barbershop  & 15.94 & 24.52 & 2.07 & 8.12 & 44.08 & 46.13 & 11.27 & 13.30 & 0.11 & 21.18 & 213 & 645 \\ HairFast (ours) & **13.12** & **22.71** & **5.12** & **6.06** & **43.25** & **44.85** & **8.90** & **10.33** & **0.08** & **23.45** & 0.41 & 0.78 \\  

Table 1: **Realism Metrics**. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebAHQ  dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. \(_{}\) was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. **Pose Metrics**. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. **Reconstruction**. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.

problem by using the FIDCLIP metric, which simply uses higher quality embeddings from the CLIP model. We also compute this metric in our experiments. Note that the metric uses the CLIP-ViT-B-32 checkpoint while we use CLIP-ViT-B-16 for color encoder training, so there is no leakage in our measurements.

Analyzing the results, our method performs better on all metrics. Looking at runtime, we outperform Barbershop on V100 by a factor of 800 and even CtrlHair by more than 10 times. This is because CtrlHair has an expensive post-processing implementation for alignment and Poisson blending. The only method that is faster is HairCLIP, but its performance in our problem setup is quite poor.

**Pose difference**. Table 1 shows the results of the metrics on a subsample of our main experiment "both", but split into different cases of pose difference. For this purpose, we counted the RMSE of key points of the source image with the shape image and split all cases of hairstyle transfer into 3 equal folds: easy, medium and hard. The last two cases are presented in the table. The full table can be seen in the Appendix 12.

**Reconstruction**. Another quality metric can be the reconstruction metric, where each method tries to transfer the shape and color of the hairstyle from itself, in this case we have a ground truth image with which we can measure the metrics. For this metric, we measure the LPIPS and PSNR between the original images and the resulting images. 3000 random images from CelebA-HQ were taken for reconstruction. Additional metrics for reconstruction such as FID and FIDCLIP can be viewed in the Appendix 12.

Analyzing the results Table 1, we also outperform other methods. This confirms the effectiveness of our Refinement alignment stage, which recovers lost image details during encoding, outperforming even optimization-based methods.

    &  Barbershop \\  \\  &  StyleYourHair \\  \\  &  HairNet \\  \\  &  HairNet \\  \\  &  StyleSalon \\  \\  &  CutHair \\  \\  &  HairCLIP \\  \\  &  HairCLIP \\  \\  & 
 Ours \\  \\ 
**Quality** & & & & & & & & & \\ Hair realism & High & High & High & High & Medium & Low & Medium & High \\ Face-background & & Medium & Medium & High & High & High & Low & Medium & High \\
**Functionality** & & & & & & & & & \\ Pose alignment & ✗ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & ✓ & ✓ \\ Separate shape/ & & ✓ & ✗ & ✗ & ✗ & ✓ & ✓ & ✓ & ✓ \\ color transfer & & & & & & & & & \\
**Efficiency** & & & & & & & & & \\ W/o optimization & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ & ✓ & ✗ & ✓ \\ Runtime & 10m & 3m & 3m & 3m & 10m & 5s & 1s & 3m & 1s \\ 
**Reproducibility** & & & & & & & & & \\ Code access & ✓ & ✓ & ✗ & ✗ & ✓ & ✓ & ✓ & ✓ \\  Face & Shape & Color & HairFut (ours) & Batcherlog & StyleYouflair & HairCLIP2 & CtrlHair & HairCLIP \\   

Table 2: A comparison of the characteristics of the main hair transfer methods.

Figure 5: Visual comparison of methods on different cases for transferring hair and color together, or separately. StyleYourHair transfers color only from the Shape image. According to the results of visual comparison, our model better preserves the identity of the source image. At the same time, our method on most cases better transfers the desired hair color and texture, and works better with complex pose differences. For a more detailed comparison, see Appendix 17.

**Overall comparison.** The Table 2 shows a comparison of the characteristics of the methods. Hair realism was determined according to realism metrics on reconstruction tasks and visual comparison. HairCLIPv2 has medium realism because of poor reconstruction, which due to the peculiarities of the architecture does not allow to transfer the desired texture accurately enough, in turn, CtrlHair despite the excellent metrics in visual comparison shows not similar to the desired results due to the limitations of the generator. The other methods, except HairCLIP, transfer the hairstyle realistically.

When it comes to preserving face and background details, the latent space of methods in which image inversions take place is mainly responsible for this. Methods such as Barbershop, StyleYourHair, HairNet and HairCLIPv2 use FS resolution space 32, which does not allow them to preserve much details. In turn, the HairNeRF and StyleGANSalon methods use PTI, which allows them to preserve more details of the original image, and the CtrlHair method uses Poisson Blending, which also allows direct transfer of all original details. Our method uses FS resolution space 64, which when compared visually and reconstruction metrics shows even better quality than methods with PTI. HairCLIP, in contrast, uses the weakest W+ space.

The runtime of each method that has a code we tested on the Nvidia V100. StyleGANSalon's time estimate came from their article, where they claim to run longer than Barbershop, while methods like HairNet and HairNeRF use PTI which makes them take at least a few minutes per image.

Looking at the rest of the features, unlike some other methods we are able to transfer hair color and shape independently, we are also able to handle large pose differences and our entire architecture consists of encoders, which allows us to work very fast. Moreover our method has code for inference, all pre-trained weights and scripts for training for full reproducibility.

**Ablation study.** As ablation, we remove some parts of our method and replace them with Barbershop optimization processes if necessary. On these configurations we measure the realism metrics after the hairstyle transfer, the results of which can be seen in the table Tab. 3 and images Fig. 6.

By ablation, we proved the high quality of Color Encoder, the necessity of mixing FS and W+ spaces, the effectiveness of Rotate Encoder, Shape Adaptor, SEEN, and the effectiveness of Refinement alignment module. For more detailed ablation conclusions, see Appendix 7.

**Failure cases.** The main problems of our method arise in the inpaint part, it may not work well when long hair is replaced by short hair. Also, our method suffers from transferring hair with complex textures such as ponytails, ribbons and braids. While these problems are important, they are inherent in all baseline models and we will address them in our future work. A detailed analysis of failure cases can be seen in the Appendix 8.

## 5 Conclusion and Limitations

In this article, we introduced the new HairFast method for hair transfer. Unlike other approaches, we were able to achieve high quality and high resolution outperforms to other optimization-based methods, but still working in near real time. We developed a new approach for pose adaptation, a new approach for FS space regularization, a more efficient approach for hair coloring, and developed a new module for detail recovery.

But our method, like many others, is limited by the small number of ways to transfer hairstyles, but our architecture allows to fix this in future work. For example, our Color alignment module architecture allows similarly to HairCLIP to do hair color editing with text, and using Shape Adaptor allows similarly to CtrlHair to edit hair shape with sliders.

We prove the effectiveness of our approach by comparing it with other methods in the Section 4, and refer to additional experiments in the Appendix 11, 12, 13, 14, 15, 16 and 17 for further details.

    &  &  & _{}\)} &  \\  A & Baseline & 162.6 & 6.92 & 0.67 \\  B & w/Color alignment & 26.88 & 11.45 & 0.39 \\ C & w/o Fair & 26.57 & 6.72 & 0.67 \\ D & ceiling B/B Full & 25.74 & 11.51 & 0.39 \\  E & w/o Reiner Encoder & 16.87 & 7.52 & 0.62 \\ F & w/o Shape Adaptor & 18.72 & 0.67 & 0.37 \\ O & w/o Size Napair & 12.79 & 0.48 & 0.92 \\  H & \(\) Refinement (ours) & 13.12 & 0.2 & 0.78 \\   

Table 3: Ablation results. Baseline is HairFast, but without Refinement alignment. In each configuration, we replace the specified part with optimizations from Barbershop as needed.

Figure 6: Ablation Study for different configurations of our model. Our model is used as the Baseline, but without Refinement alignment. Each column represents a change in the Baseline of the model.

Acknowledgments

The article was prepared within the framework of the HSE University Basic Research Program. The calculations were performed in part through the computational resources of HPC facilities at HSE University .