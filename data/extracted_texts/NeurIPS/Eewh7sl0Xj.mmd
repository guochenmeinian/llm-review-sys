# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

Introduction

Sequence modeling is important in natural language processing, where sentences are represented as a sequence of tokens. Successful sequence modeling typically involves token and channel mixing. Token mixing combines representations of different sequence parts, while channel mixing combines the information across different dimensions of embedding vectors used to encode tokens. Transformers  are arguably the most successful technique for sequence modeling, and variants including ,  have achieved state of the art performance on natural language tasks. They use self-attention for token mixing and feedforward networks for channel mixing.

Recently,  proposed Toeplitz Neural Networks (TNN) using Toeplitz matrices for token mixing. They use a learned neural similarity function, the Relative Positional Encoder (RPE), to form the Toeplitz matrices. Toeplitz matrix vector multiplication can be performed with sub-quadratic complexity using the Fast Fourier Transform (FFT), giving the TNN token mixing layer a total \(O(dn n)\) computational complexity, where \(d\) is the embedding dimension and \(n\) is the sequence length. This achieved state of the art predictive performance and nearly state of the art speed for the long range arena (LRA) benchmark . They also showed strong performance pre-training wikitext-103  and on the GLUE benchmark . Despite strong empirical speed performance, TNNs have two fundamental efficiency limitations: 1) super-linear computational complexity 2) many calls to the RPE: for each layer, one call per relative position.

In this paper, we interpret the RPE as a non-SPD kernel and note 1) the learned kernels are discontinuous near the main diagonals but otherwise smooth globally; 2) the ReLU RPE learns 1D piecewise linear functions: an MLP is slower than necessary. For bidirectional models, this motivates a sparse plus low-rank decomposition. We apply the sparse component's action via a small 1D convolution. For the low rank component, we replace the RPE MLP with linear interpolation at a set of inducing points and an asymmetric extension of Structured Kernel Interpolation (SKI)  for \(O(n)\) complexity. Further, using an inverse time warp, we can extrapolate beyond sequence lengths observed during training. For causal models, even "fast" causal masking  negates the speed and memory benefits from SKI. Thus, we instead represent the real part of the kernel's frequency response using the RPE MLP, and evaluate the RPE with finer frequency resolution to extrapolate to longer sequence lengths in the time domain. From the real part, we compute the imaginary part via a Hilbert transform during the forward pass to enforce causality. In the bidirectional setting, we remove the causality constraint and represent the complex frequency response of the kernel with the RPE MLP. Levels of smoothness in frequency response imply decay rates in the time domain: thus we model the decay bias implicitly. This maintains \(O(n n)\) complexity but achieves an absolute speedup. Further, it often leads to better predictive performance on LRA tasks.

This paper has three primary contributions: 1) a TNN sparse plus low rank decomposition, extending SKI to TNNs for the low rank part. We replace the RPE MLP with linear interpolation and apply inverse time warping to efficiently train bidirectional TNNs. We provide rigorous error analysis for our asymmetric SKI application; 2) alternatively, for both causal and bidirectional models, we work directly in the frequency domain and use the Hilbert transform to enforce causality in the autoregressive setting. We prove that different activation choices for an MLP modeling the discrete time Fourier transform (DTFT) lead to different decay rates in the original kernel. 3) Empirical results: we demonstrate that our approaches show dramatically improved computational efficiency, setting a new speed state of the art on LRA  on the 1d tasks, with strong LRA score. In section  we describe related work. In section  we propose our new modeling approaches. In  we state several theoretical results regarding our modeling approaches. In  we extend the empirical results of , showing our speed gains with minimal prediction deterioration. We conclude in section .

## 2 Related

The most related papers use Toeplitz matrices for sequence modeling , [1Also related are kernel based xFormers, particularly those using the Nystrom method . The most related work is , which adapts a matrix Nystrom method for asymmetric matrices  to self-attention. We instead adapt this along with SKI  to Toeplitz matrices.  extends  by embedding the self-attention matrix into a larger PSD kernel matrix and approximating the larger matrix instead. Their final approximate matrix has lower spectral error compared to  and higher average validation accuracy on LRA . However, their method is slightly slower. Also somewhat related are random feature self-attention approximations. These extend , but use different random features that better approximate self-attention than random Fourier or binning features.

Sparse transformers are also relevant.  proposed using strided and fixed patterns.  alternated between sparse locally banded and dense attention. Finally,  proposed combining random attention, window attention and global attention. Our use of a short convolutional filter is most similar to window attention. The space of efficient transformers is huge and there are many models that we haven't covered that may be relevant.  provides an excellent survey.

Other successful long sequence approaches include state space models , long convolution , adding moving averages to gated attention  and more .

## 3 Modeling Approach

We review Toeplitz neural networks (TNNs) in section 3.1. We next speed up the TNN's Toeplitz neural operator (TNO). We discuss using Nystrom and SKI approaches to bidirectional training in [3.2] We discuss frequency based approaches, particularly for causal training in [3.3]

### Preliminaries: Toeplitz matrices and Toeplitz Neural Networks

TNNs  replace self-attention, which computes the action of self-attention matrices that encode the similarity between both observation values and absolute positions, with the action of Toeplitz matrices that encode similarity only based on _relative_ positions. Toeplitz matrices have, for each diagonal, the same entries from left to right. That is, \(_{ij}=t_{i-j},^{n n}\). Unlike self-attention matrices, which require \(O(n^{2})\) memory, a Toeplitz matrix has \(2n-1\) unique elements and requires \(O(n)\) memory. Due to close connections with discrete-time convolution, \(\) can be computed in \(O(n n)\) time by embedding \(\) in a circulant matrix and applying FFT.

A TNN  has multiple sequence modeling blocks, which we show in Figure 3 in Appendix  Each block has a Gated Toeplitz Unit (GTU), which does both token and channel mixing, followed by a Gated Linear Unit (GLU) , which does channel mixing. The core of the GTU is the Toeplitz Neural Operator (TNO), which does token mixing and is the part of the architecture that we modify.

We now describe the TNO, shown in Figure 3 by Appendix  Given a sequence \(^{n d}\) of length \(n\) and dimension \(d\) in discrete time, there are \(2n-1\) unique relative positions/times \(i-j\) for \(i,j=1,,n\). An RPE \(:^{d}\) neural network maps each relative position to a \(d\)-dimensional embedding. These embeddings are used to construct Toeplitz matrices \(^{l}\) for \(l=1,,d\) using

\[^{l}_{ij}=^{|i-j|}_{l}(i-j).\]

RPE\({}_{l}(i-j)\) is a learned similarity between positions for dimension \(l\), while \(^{|i-j|}\) with \((0,1)\) is an exponential decay bias penalizing far away tokens to be dissimilar. We can interpret \(^{l}_{ij}\) as evaluating a stationary non-SPD kernel \(k_{l}(i-j)=^{|i-j|}_{l}(i-j)\). Thus \(^{l}\) can be interpreted as a pseudo or generalized Gram matrix. Letting \(^{l}\) be the \(l\)th column of \(\), the TNO outputs

\[()=(^{1}^{1}^{d} ^{d})^{n d}\]

where each \(^{l}^{l}\) is computed via the FFT as described above.

The main costs are the RPE's MLP, the FFT, and the decay bias. We aim to eliminate the MLP and decay bias when possible. In the bidirectional setting, we use SKI to apply the FFT using a much smaller Toeplitz matrix. In a separate model we learn the RPE's frequency response directly. In the bidirectional setting, this allows us to both avoid explicitly modeling the decay bias and use one fewer FFT. In the causal setting, it allows us to avoid explicitly modeling the decay bias.

### SKI Based Approaches for Bidirectional Training

For a given Toeplitz matrix \(\), we assume it admits a decomposition that we can approximate with a sparse+low-rank representation, \(=_{}+_{}_{}+_{}\). Our bidirectional training thus consists of three primary components. The first, the sparse component \(_{}\) is straightforward. Applying the action \(_{}\) of \(_{}^{n n}\) with \(m\) non-zero diagonals is equivalent to applying a 1D convolution layer with filter size \(m\). We then discuss our asymmetric SKI for \(_{}\) in section 2.1.1 Finally, we discuss how we handle sequence lengths not observed in training for \(_{}\) via an inverse time warp in section 3.2.2 Algorithm1 summarizes our TNO based on these techniques.

```
Given sequence \(^{n d}\) with columns \(^{l}\) Hyperparameters rank \(r n\), sparse filter size \(m\), interpolation degree \(N\), decay parameter \(\) Compute inducing points \(p_{1},,p_{r}\) evenly spaced on \([0,n]\) for\(l=1,,d\)do  Compute \(_{}^{l}^{l}\) with a 1D convolutional filter, size \(m\).  Let \(x(t)=(t)^{|t|}\).  Form \(^{l}^{r r}\) with entries \(^{l}_{ij}=k_{l}(p_{i}-p_{j})=_{l}(x(p_{i}-p_{j}))\)  Form \(^{l}^{n r}\) degree \(N\) polynomial interpolation matrix  Compute \(_{}^{l}^{l}\) with \(_{}^{l}=^{l}^{l}^{l}\) endfor  Return \(()=(_{}^{1}^{1}+_{}^{1}^{1},,_{}^{d}^{d}+_{}^{d}^{d})\)
```

**Algorithm 1** Sparse Plus Low Rank Bidirectional TNO with Asymmetric SKI

#### 3.2.1 SKI For Asymmetric Nystrom

Given an asymmetric stationary kernel \(k:\), we wish to approximate the (pseudo) Gram matrix \(^{n n}\) using a low-rank approximation based on a smaller Gram matrix \(^{r r}\), with \(r n\). In context, \(\) is formed using relative positions between a set of inducing points \(p_{1},,p_{r}\) instead of the full set \(1,,n\) that is used for \(\). That is,

\[_{ij}=k(i-j)_{ij}=k(p_{i}-p_{j}).\]

In our case, the inducing points are uniformly spaced. Some submatrices of \(\) may be submatrices of \(\) (if inducing points are also observation points). To derive the Nystrom approximation, we form an

Figure 2: Our SKI-TNO and FD-TNO modifications: (a) We decompose Toeplitz matrices into sums of sparse + smooth components. Additionally, we use interpolation instead of an MLP to learn the RPE. (b) We use a 1D convolution to apply the sparse component and SKI as a low-rank approximation to the smooth component. (c) For the causal case, we use frequency domain RPE with a Hilbert Transform to enforce causality. (d) Our FD-TNO also is competitive in the bidirectional case, with one fewer FFT than TNO.

augmented Gram matrix \(^{(n+r)(n+r)}\) in block form as

\[=&\\ &,\]

where \(^{r n}\) and \(^{n r}\) are respectively the upper right and lower left partitions of the large Gram matrix \(\). Explicitly,

\[_{ij}=k(p_{i}-j)_{ij}=k(i-p_{j}).\]

Extending 16 to allow singular \(\),

\[}=\\ ^{}( )=&^{}\\ ^{}&^{}\]

where \(^{}\) is the Moore-Penrose pseudo-inverse satisfying \(^{}=\) (but not necessarily \(^{}=\) as in 16, which shows up in our different expressions for off-diagonal blocks of \(}\)). Following structured kernel interpolation (SKI) 2, we approximate \(\) and \(\) using interpolation. Specifically,

\[ ^{}\]

where \(^{n r}\) is a matrix of sparse interpolation weights with up to two non-zero entries per row for linear interpolation or up to four cubic. These weights can be computed in closed form from the inducing points \(p_{i}\) and the observation points \(i\). Thus we have

\[^{} ^{}^{}= ^{}\] \[}=^ {}\]

as desired. We can set \(_{}=}\) and compute \(}\) by first applying \(^{}\), which is an \(O(n)\) operation due to \(^{n r}\) having sparse rows. Next, we apply \((^{})\). Since \(\) is a Toeplitz matrix, this is \(O(r r)\) as per Section 3.1 Finally, \((^{})\), the action of \(\), is again an \(O(n)\) operation. Thus computing \(}\) is \(O(n+r r)\) computation. On a GPU, this factorization achieves a speedup from having small \(r\) and being able to leverage efficient parallelized matrix multiplication on specialized hardware. However, in PyTorch , we note that for medium sized matrices up to \(n=512\), the time required for data movement in order to perform sparse-dense matrix multiplications can be higher than that of simply performing dense matrix multiplication. This means that in practice, we may instead choose to perform batched dense matrix multiplication, which yields an absolute speedup but a worse asymptotic complexity of \(O(nr^{2}+r r)\).

#### 3.2.2 Inverse Time Warp

TNNs use \(k_{l}(i-j)=^{|i-j|}_{l}(i-j)\), where RPE\({}_{l}(i-j)\) is an MLP. There are two issues: 1) the sequential computations required for an MLP are slow, and we only need to evaluate at \(2r-1\) points using SKI instead of \(2n-1\) to produce the full matrix; 2) extrapolation is used in extending to longer sequence lengths than the MLP was trained on, which is generally less reliable than interpolation.

In Proposition 1 we note that an MLP \(f:^{d}\) with ReLU activations and layer normalization is \(d\) piecewise linear functions. As we only need to evaluate at \(2r-1\) points, we could let RPE\({}_{l}\) be a piecewise linear function with \(r\) grid points. However, we still need to handle extrapolation. We use an inverse time warp and let RPE\({}_{l}\) linearly interpolate on \([-1,1]\) with the constraint RPE\({}_{l}(0)=0\) and define \(x(t)=(t)^{|t|}\) for some \(0<<1\). We then let \(k_{l}(i-j)=_{l}(x(i-j))\).

### Frequency Based Approaches

#### 3.3.1 Causal Training

The SKI approach allows training bidirectional TNNs with linear complexity. However, fast causal masking negates SKI's benefits (see Appendix B). Thus we need an alternate causal speedup. We use an MLP in the Fourier domain to avoid an explicit time domain decay bias, and use the Hilbert transform to enforce causality. We now describe how we can learn a causal kernel when working in frequency domain (FD). We first define the discrete Hilbert transform, the key tool for achieving this.

**Definition 1**.: _The **discrete Hilbert transform** of the discrete Fourier transform**\(\) is given by_

\[\{\}=*h\]

_where \(*\) denotes convolution and_

\[h[l]=0,\,l\,\,\\ ,\,l\,\,\]

The real and imaginary parts of the Fourier transform of a causal function are related to each other through the Hilbert transform. Thus, in order to represent a causal signal, we can model only the real part and compute the corresponding imaginary part. That is, we first estimate an even real function \(\) (symmetric about \(0\)) using an MLP. We then take \(_{}()=()-i\{\}()\).

The inverse Fourier transform \(k_{}\) of \(_{}\) will thus be causal. For a discussion of why this ensures causality, see . See Algorithm  for TNO pseudocode using this approach. Different choices for the smoothness of the frequency domain MLP will lead to different decay rates in time domain, so that smoothness in frequency domain essentially serves the same purpose as the decay bias in . We discuss this theoretically in Section 4.2. Note that we also find that working directly in the frequency domain for bidirectional models (without the Hilbert transform) is often competitive with SKI for speed (despite being \(O(n n)\) instead of \(O(n+r r)\)) due to needing one fewer FFT.

```
Given sequence \(^{n d}\) with columns \(^{l}\) Hyperparameters activation function for\(l=1,,d\)do \(}^{l}\{^{l}\}\), where \(\) is the rFFT.  Compute even real function \(^{l}=_{l}()\), \(=,m=0,,n\).  Take discrete Hilbert transform \(\{^{l}\}\) via the rFFT and irFFT.  Compute \(^{l}_{}()=^{l}()-i\{^ {l}\}()\) for \(=,m=0,,n\). \(^{l}^{-1}\{^{l}_{} {}^{l}\}\), where \(^{-1}\) is the irFFT and \(\) denotes an element-wise product. endfor  Return TNO(\()=(^{1},,^{d})\)
```

**Algorithm 2** Causal TNO via Discrete Hilbert Transform

#### 3.3.2 Bidirectional Training with FD TNN

We extend the FD approach to bidirectional training by removing the causality constraint and model the complex frequency response of real valued time domain kernels directly. To do so we simply double the output width of the RPE and allocate each half for the real and imaginary parts of the kernel frequency responses, while explicitly forcing real-valued responses at \(=0\) and \(\). While increasing the complexity of the RPE slightly, we achieve the speed ups in Figure  by eliminating the FFTs for the kernels and causality constraint, in addition to the decay bias.

## 4 Theory

We show in Proposition  that an MLP mapping from scalars with layer norm and ReLU activations is piecewise linear and continuous, suggesting that using an MLP that we only need to evaluate at a small number of points may be overparametrized, justifying the use of interpolated piecewise linear functions. In section 4.1 we analyze the spectral norm of the matrix approximation error for SKI. We assume the sparse component is exactly identifiable and bound the error of approximating the smooth term via a low-rank SKI factorization. We leave the problem of relaxing this assumption to future work. In section 4.2 we analyze how by using different activations with different smoothness when learning the DTFT of the kernel, we obtain corresponding decay rates for the time domain signal.

**Proposition 1**.: _A ReLU MLP \(f:^{d}\) with layer norm and no activation on its output is \(d\) piecewise linear continuous functions._

Proof.: See Appendix 

### Matrix Approximation Spectral Norm Error

We give our main error bound for our SKI based low rank approximation. Note that this requires that our kernel is \(N+1\) times continuously differentiable, while the kernel we use in practice uses a piecewise linear function and is thus non-differentiable. In theory, we would need a smoother kernel, adding additional computation overhead. However, we find that empirical performance is still strong and thus we simply use piecewise linear kernels but include the error bound for completeness. Our results depends on the Nystrom error \(_{nyst}\): its \(l^{2}\) norm is bounded in .

**Theorem 1**.: _Assume that \(\) is non-singular and \(k:[p_{1},p_{r}]\) is an \(N+1\) times continuously differentiable function, where \(p_{1}\) is the smallest inducing point and \(p_{r}\) is the largest. Let \(_{r,opt}\) be the optimal rank \(r\) approximation to \(\) and let_

\[_{SKI}=^{}-_{r,opt}\]

_be the difference between the SKI approximation using linear interpolation and the optimal one, while_

\[_{nyst}=^{-1}-_{r,opt}\]

_is the difference between the Nystrom approximation and the optimal one. Then_

\[\|_{SKI}\|_{2}_{p_{n_{1}} i p_{n_{N}}} {|_{N}(i)|}{(N+1)!}L((N+1)+(), _{1}())}{_{r}()})+\|_{nyst}\|_ {2}.\]

_where \(_{N}(i)=_{j=1}^{N}(i-p_{n_{j}})\) with \(p_{n_{j}}\) being the \(j\)th closest inducing point to \(i\), \(L\) is an upper bound on the \(N+1\)th derivative of \(k\), and \(_{i}()\) denotes the \(i\)th largest singular value of matrix \(\)._

Proof.: See Appendix D.1 

For linear interpolation \((i)|}{(N+1)!}}{8}\), where \(h\) is the spacing between two neighboring inducing points. We have considered the sparse component of the Toeplitz matrix to be identifiable and focused on the error of approximating the smooth component. While there are potential approaches to relaxing this assumption , they must be adapted properly to the Toeplitz setting. Thus, this additional analysis is outside the scope of this paper and a fruitful direction for future work.

### Smoothness in Fourier Domain Implies Decay in Time Domain

We now discuss activation function choices when directly learning the discrete time Fourier transform (DTFT) \(\) as an MLP. In practice, we sample the DTFT to obtain the actually computable discrete Fourier transform (DFT) by evaluating the MLP with uniform spacing. Different levels of smoothness of the MLP \(\) imply different decay rates of the signal \(k\). One can think of the choice of activation function as a parametric form for the decay bias. For an MLP, using a GeLU activation implies super-exponential time domain decay. Using SiLU implies super-polynomial time domain decay. For ReLU the signal is square summable. While this subsection focuses on the theoretical relationship between smoothness and decay, in Appendix E.3 we show visualizations demonstrating that these relationships are observed in practice. We first define the DTFT and its inverse.

**Definition 2**.: _The **discrete time Fourier transform**\(\) or \(\{k\}\) of \(k\) is given by_

\[()_{m=-}^{}k[m](-i m)\]

**Definition 3**.: _The **inverse discrete time Fourier transform** of the DTFT \(\) is given by_

\[^{-1}\{\}[n]_{-}^{}( )(i n)d\]

We now give three theorems relating smoothness of the DTFT to decay of the signal (its inverse).

**Theorem 2**.: _Using a GeLU MLP for the DTFT \(\), for all \(a>0\), the signal \(k[n]\) will have decay_

\[k[n]=O((-an)).\]Proof.: See Appendix E.1 

**Theorem 3**.: _Using a SiLU MLP for the DTFT \(\), the signal \(k[n]\) will have decay_

\[|k[n]|}^{(N)}_{1}\]

_for all \(n 0,N\)._

Proof.: See Appendix E.2 

**Theorem 4**.: _Using a ReLU MLP for the DTFT \(\) implies \(\|k\|_{2}<\) (the signal is square summable)._

Proof.: Note that \( L^{2}[-,]\) since it is continuous. Then apply Parseval's theorem. 

## 5 Experiments

We perform experiments in two areas: pre-training a causal language model on Wikitext-103  and training bidirectional models on Long-Range Arena. We start with the repositories of the TNN paper1 and use their training and hyper-parameter settings unless indicated otherwise. We use A100 and V100s for training, and a single A100 for timing experiments.

### Pre-training on Wikitext-103

In the causal case we aim to predict the next token, conditional on a fixed length sequence of previous tokens. Table1 compares FD-TNN's causal pre-training perplexity  to existing models: it almost exactly matches that of TNNs. Our approach is faster for the same capacity: at sequence length 512 with 6 layer RPEs (as in the TNN paper), FD TNN is 15% faster than the baseline TNN on a single A100 GPU. When both use a three layer RPE, FD TNN is 10% faster. We provide some additional details for this experiment as well as for bidirectional pre-training (we see larger speed gains) in Appendix E.

### Long-Range Arena

The Long-Range Arena (LRA) is a benchmark with several long sequence datasets. The goal is to achieve both high LRA score (predictive performance) and training steps per second. Following , we take the TNN architecture and their tuned hyperparameter (HP) configuration2 simply replacing their TNO module with our SKI-TNO module with \(r=64\) and \(m=32\). We use \(=0.99\) where they set \(=1\), but otherwise perform _no additional HP tuning_ on 1D tasks and use smaller layers \(r=32\) and \(m=16\) for the 2D tasks. For FD-TNN, we simply use a same-sized RPE for all tasks except a 3-layer RPE for the CIFAR task. We could potentially achieve even higher accuracy with more comprehensive tuning on the 2D tasks or _any_ tuning for the 1D tasks. We select the checkpoint with the highest validation accuracy and report the corresponding test accuracy. SKI-TNN achieves similar average accuracy than TNN at lower size, while FD-TNN achieves _higher_ accuracy. We suspect that for some of these problems, the square summable signal implied by ReLU in frequency domain is a better parametric form than applying exponential decay bias. We show our results in Table2.

We additionally perform timing and memory profiling tests on a single 1x A100 instance, keeping the per-GPU batch size constant as in the training runs. In Figure1a we plot for each 1D task the percentage of TNN accuracy achieved vs the percentage speedup relative to TNN, with the size of the marker corresponding to the peak memory usage measured. We highlight the 1D tasks because they required no tuning, and they represent the longest sequences at lengths ranging from \(1024\) to \(4096\), whereas the 2D tasks are treated as separate 1D sequences in each dimension, so that a \(32 32\) image is seen as alternating length \(32\) sequences. We note that because the effective sequence lengths are shorter, there is less benefit from using our methods over the baseline TNN.

## 6 Conclusion

In this paper, we note that **[**11**]**'s Toeplitz neural networks essentially apply the action of a generalized Gram matrix (the Toeplitz matrix) for an asymmetric kernel (the RPE times decay bias) as their main computationally expensive operation. The visualized learned Gram matrices motivate a sparse and low rank decomposition. We thus propose two different approaches to improve efficiency. In the bidirectional setting, we extend SKI to the asymmetric setting and use linear interpolation over a small set of inducing points to avoid the MLP entirely, while using an inverse time warp to handle extrapolation to time points not observed during training. This approach reduces the mathematical complexity from \(O(n n)\) to \(O(n+r r)\), where \(r\) is the number of inducing points. However in practice, we do not actually use \(O(n+r r)\) code due to a reshape required for sparse tensors leading to them actually being _slower_ than dense tensors. Thus we actually use \(O(nr^{2}+r r)\) in code: still much faster than Baseline TNN for small \(r\). For causal training, as causal masking negates SKI's benefits, we instead eliminate the explicit decay bias. We do this by working directly in the frequency domain, enforcing causality via the Hilbert transform and enforcing decay in time domain via smoothness. For the bidirectional case, we eliminate the FFT applied to the kernels. While this maintains \(O(n n)\) computational complexity, it leads to a substantial speedup in practice and beats TNNs on LRA score.

 Architecture & PPL (val) & PPL (test) & Params (m) \\  (Attn-based) & & & \\  Trans & 24.40 & 24.78 & 44.65 \\ LS & 23.56 & 24.05 & 47.89 \\ Flash & 25.92 & 26.70 & 42.17 \\ \(1+\)elu & 27.44 & 28.05 & 44.65 \\ Performer & 62.50 & 63.16 & 44.65 \\ Cosformer & 26.53 & 27.06 & 44.65 \\  (MLP-based) & & & \\  Syn(D) & 31.31 & 32.43 & 46.75 \\ Syn(R) & 33.68 & 34.78 & 44.65 \\ gMLP & 28.08 & 29.13 & 47.83 \\  (SS-based) & & & \\  S4 & 38.34 & 39.66 & 45.69 \\ DSS & 39.39 & 41.07 & 45.73 \\ GSS & 29.61 & 30.74 & 43.84 \\  (TNN-based) & & & \\  TNN (reproduced, 3 layers) & 23.98 (23.96) & 24.67 (24.61) & 48.68 (48.59) \\ FD-TNN: Ours, 3 layers & 23.97 & 24.56 & 48.58 \\  

Table 1: **Performance on Wikitext-103, Causal Language Model**. We reproduce **[**11**]**’s table except for the bottom two rows corresponding to the baseline TNN and our FD-TNN. For both we use the same RPE config with 3 layers. We add in parenthesis the baseline TNN results that we reproduced. We have nearly the same perplexity as the baseline TNN. Our approach is faster: at sequence length 512 with a six layer RPE (as in the TNN paper), FD TNN is 15% faster than the baseline TNN. For a three layer RPE, it is 10% faster.

 Architecture & Text & ListOps & Retrieval & Pathfinder & Image & Avg \\  TNN & **86.39** & 47.33 & 89.40 & **73.89** & 77.84 & 74.97 \\  SKI-TNN & 83.19 & 45.31 & 88.73 & 68.30 & 76.46 & 72.40 \\ FD-TNN & 85.00 & **55.21** & **90.26** & 69.45 & **84.12** & **76.81** \\  

Table 2: **Performance on Long Range Arena**. We reproduce experiments and train our proposed variants using tuned hyperparameters from **[**11**]**. We **bold** the best and underline the second in each task. Our proposed SKI-TNN and FD-TNN achieve similar overall performance with _no additional hyperparameter tuning_ on 1D LRA tasks and a minimal amount of tuning on 2D tasks.