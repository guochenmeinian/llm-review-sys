# Implicit variance regularization in non-contrastive SSL

Manu Srinath Halvagal\({}^{1,2,*}\) &Axel Laborieux\({}^{1,*}\) &Friedemann Zenke\({}^{1,2}\)

{firstname.lastname}@fmi.ch

\({}^{1}\) Friedrich Miescher Institute for Biomedical Research, Basel, Switzerland

\({}^{2}\) Faculty of Science, University of Basel, Basel, Switzerland

\({}^{*}\) These authors contributed equally.

###### Abstract

Non-contrastive self-supervised learning (SSL) methods like BYOL and SimSiam rely on asymmetric predictor networks to avoid representational collapse without negative samples. Yet, how predictor networks facilitate stable learning is not fully understood. While previous theoretical analyses assumed Euclidean losses, most practical implementations rely on cosine similarity. To gain further theoretical insight into non-contrastive SSL, we analytically study learning dynamics in conjunction with Euclidean and cosine similarity in the eigenspace of closed-form linear predictor networks. We show that both avoid collapse through implicit variance regularization albeit through different dynamical mechanisms. Moreover, we find that the eigenvalues act as effective learning rate multipliers and propose a family of isotropic loss functions (IsoLoss) that equalize convergence rates across eigenmodes. Empirically, IsoLoss speeds up the initial learning dynamics and increases robustness, thereby allowing us to dispense with the exponential moving average (EMA) target network typically used with non-contrastive methods. Our analysis sheds light on the variance regularization mechanisms of non-contrastive SSL and lays the theoretical grounds for crafting novel loss functions that shape the learning dynamics of the predictor's spectrum.

## 1 Introduction

SSL has emerged as a powerful method to learn useful representations from vast quantities of unlabeled data [1; 2; 3; 4; 5; 6; 7; 8]. In SSL, the network's objective is to "pull" together its outputs for two differently augmented versions of the same input, so that they learn representations that are predictive across randomized transformations . To avoid the trivial solution whereby the network output becomes constant, also called representational collapse, SSL methods use either a contrastive objective to "push" apart representations of unrelated images [2; 3; 10; 11; 12; 13] or other non-contrastive strategies. Non-contrastive methods comprise explicit variance regularization techniques [6; 7; 14], whitening approaches [15; 16], and asymmetric losses as in Bootstrap Your Own Latent (BYOL)  and SimSiam . Asymmetric losses break symmetry between the two branches by passing one of the representations through a predictor network and stopping gradients from flowing through the other "target" branch. How this architectural modification prevents representational collapse is not obvious and has been the focus of several theoretical [17; 18; 19; 20; 21] and empirical studies [22; 23; 24]. A significant advance was provided by Tian et al.  who showed that linear predictors align with the correlation matrix of the embeddings, and proposed the closed-form predictor DirectPred based on this insight. However, previous analyses assumed a Euclidean loss at the output [17; 18; 19; 21] except , whereas practical implementations typically use the cosine loss [1; 5] which yields superior performance on downstream tasks. This difference raises the question whether analysis based on the Euclidean loss provides an accurate account of the learning dynamics under the cosine loss.

In this work, we provide a comparative analysis of the learning dynamics for the Euclidean and cosine-based asymmetric losses in the eigenspace of the closed-form predictor DirectPred. Our analysis shows how both losses implicitly regularize the variance of the representations, revealing a connection between asymmetric losses and explicit variance regularization in VICReg . Yet, the learning dynamics induced by the two losses are markedly different. While the learning dynamics of different eigenmodes decouple in the Euclidean case, dynamics remain coupled for the cosine loss.

Moreover, our analysis shows that for both losses, the predictor's eigenvalues act as learning rate multipliers, thereby slowing down learning for modes with small eigenvalues. Based on our analysis, we craft an isotropic loss function (IsoLoss) for each case that resolves this problem and speeds up the initial learning dynamics. Furthermore, IsoLoss works without an EMA target network possibly because it boosts small eigenvalues, the purported role of the EMA in DirectPred . In summary, our main contributions are the following:

* We analyze the SSL dynamics in the eigenspace of closed-form linear predictors for asymmetric Euclidean and cosine losses and show that both perform implicit variance regularization, but with markedly different learning dynamics.
* Our analysis shows that predictor eigenvalues act as learning rate multipliers which slows down learning for small eigenvalues.
* We propose isotropic loss functions for both cases that equalize the dynamics across eigenmodes and improve robustness, thereby allowing to learn without an EMA target network.

## 2 Eigenspace analysis of the learning dynamics

To gain a better analytic understanding of the SSL dynamics underlying non-contrastive methods such as BYOL and SimSiam [1; 5], we analyze them in the predictor's eigenspace. Specifically we proceed in three steps. First, building on DirectPred, we invoke the neural tangent kernel (NTK) to derive simple dynamic expressions of the predictor's eigenmodes for Euclidean and cosine loss. This formulation uncovers the implicit variance regularization mechanisms that prevent representational collapse. Using the eigenspace framework, we illustrate how removing the predictor or the stop-gradient results in collapse or run-away dynamics. Finally, we find that predictor eigenvalues act as learning rate multipliers for their associated mode, thereby slowing down learning for small eigenvalues. We derive a modified isotropic loss function (IsoLoss) that provides more equalized learning dynamics across modes, which showcases how our analytic insights help to design novel loss functions that actively shape the predictor spectrum. However, before we start our analysis, we will briefly review DirectPred  and the NTK , a powerful theoretical tool linking representational changes and parameter updates. We will rely on both concepts for our analysis.

### Background and problem setup

We begin by reviewing DirectPred  and defining our notation. In the following, we consider a Siamese neural network \(=f(;)\) with output \(^{M}\), input \(^{N}\), and parameters \(\). We further assume a linear predictor network \(W_{}^{M M}\) and use the same parameters for the online and target branches as in SimSiam . We denote pairs of representations as \(^{(1)},^{(2)}\) corresponding to pairs of inputs \(^{(1)},^{(2)}\) related through augmentation and implicitly assume that all losses are averaged over many augmented pairs. The asymmetric loss function (Fig. 1a), introduced in BYOL , is then given by:

\[=d(W_{}^{(1)},(^{(2)})),\]

where \(\) denotes the stop-gradient operation, and \(d\) is either the Euclidean distance metric \(d(,)=\|-\|^{2}\) or the cosine distance metric \(d(,)=-^{}}{\|\|\|\|}\). We refer to the corresponding loss functions as \(^{}\) and \(^{}\) respectively.

DirectPred.Tian et al.  showed that a linear predictor in the BYOL setting aligns during learning with the correlation matrix of representations \(C_{}:=_{}[^{}]\), where the expectation is taken over the data distribution. Since the correlation matrix is a real symmetric matrix, one can diagonalize it over \(\): \(C_{}=UD_{C}U^{}\), where \(U\) is an orthogonal matrix whose columns are the eigenvectors of \(C_{}\) and \(D_{C}\) is the real-valued diagonal matrix of the eigenvalues \(s_{m}\) with \(m[1,M]\)Given this eigendecomposition, the authors proposed DirectPred, in which the predictor is not learned via gradient descent but directly set to:

\[W_{}=f_{}(C_{})=UD_{C}^{}U^{},\] (1)

where \(\) is a positive constant exponent applied element-wise to \(D_{C}\). The eigenvalues \(_{m}\) of the predictor matrix \(W_{}\) are then \(_{m}=s_{m}^{}\). We use \(D\) to denote the diagonal matrix containing the eigenvalues \(_{m}\). While DirectPred used \(=0.5\), the follow-up study DirectCopy  showed that \(=1\) is also effective while avoiding the expensive diagonalization step. While Tian et al.  based their analysis on the Euclidean loss \(^{}\), most practical models, including Tian et al.'s large-scale experiments, relied on the cosine similarity loss \(^{}\). This discrepancy raises the question to what extent setting the predictor to the above expression is justified for the cosine loss. Empirically, we find that a trainable linear predictor _does_ align its eigenspace with that of the representation correlation matrix also for the cosine loss (see Fig. 4 in Appendix A).

Neural tangent kernel (NTK).The NTK is a powerful analytical tool characterizing the learning dynamics of neural networks [25; 26]. Here, we recall the definition of the _empirical_ NTK  corresponding to a single instantiation of the network's parameters \(\). If \(||\) denotes the size of the training dataset, \(:^{M}\) an arbitrary loss function, \(\), the training data concatenated into one vector of size \(N||\), and \(=z()\), the concatenated output of size \(M||\), then the empirical NTK is the \((M|| M||)\)-sized matrix:

\[_{t}(,)=_{}_{ }^{},\]

and the continuous-time gradient-descent dynamics  of the representations \(\) are given by:

\[}{t}=-_{t}(,) _{}.\] (2)

In other words, the empirical NTK links the representational dynamics \(}{t}\) under gradient descent on the parameters \(\), and the "representational gradient" \(_{}\).

### Implicit variance regularization in non-contrastive SSL

As a starting point for our analysis, we first express the relevant loss functions in the eigenbasis of the predictor network. We do this using a closed-form linear predictor as prescribed by DirectPred. In the following, we use \(}=U^{}\) to denote the representation expressed in the eigenbasis.

Figure 1: **(a)** Schematic of a Siamese network with a predictor network and a stop-gradient on the target network branch. The target network can be a copy (SimSiam ) or a moving average (BYOL ) of the online network. In either case, the target network is not optimized with gradient descent. **(b)** Visualization of learning dynamics under the Euclidean distance metric showing learning update directions along two eigenmodes, with the light cloud representing the distribution of the representations \(\), the darker cloud representing the predictor outputs \(W_{}\), and the dotted circle indicates the steady state \(_{1,2}=1\), reached during learning. All eigenvalues converge to one. **(c)** Same as **(b)**, but for the cosine distance. The dotted line indicates the steady state \(_{1}=_{2}\).

**Lemma 1**.: (Euclidean and cosine loss in the predictor eigenspace) _Let \(W_{}\) be a linear predictor set according to DirectPred with eigenvalues \(_{m}\), and \(}\) the representations expressed in the predictor's eigenbasis. Then the asymmetric Euclidean loss \(^{}\) and cosine loss \(^{}\) can be expressed as:_

\[^{} =_{m}^{M}|_{m}_{m}^{(1)}-(_{m}^{(2)})|^{2},\] (3) \[^{} =-_{m}^{M}_{m}^{(1)}( {z}_{m}^{(2)})}{\|D}^{(1)}\|\|(}^{(2)})\|}.\] (4)

for which we defer the simple proof to Appendix B. Rewriting the losses in the eigenbasis makes it clear that the asymmetric loss with DirectPred can be viewed as an _implicit_ loss function in the predictor's eigenspace, where the variance of each mode naturally appears through the \(_{m}\) terms. In the following analysis, we will show how the learning dynamics implicitly regularize these variances \(_{m}\). From Eq. (3) we directly see that \(^{}\) is a sum of \(M\) terms, one for each eigenmode, which decouples the learning dynamics, a fact first noted by Tian et al. . In contrast, the form of \(^{}\) yields coupled dynamics due to the \(\|D}^{(1)}\|=(_{k}_{k}^{(1)})^{2}}\) term in the denominator. This coupling arises from the normalization of the representation vectors to the unit hypersphere when calculating the cosine distance. The normalization effectively removes one degree of freedom and, in the process, adds a dependence between all the representation dimensions (Fig. 1b and 1c).

To get an analytic handle on the evolution of the eigen-representations \(}\) as the encoder learns, we first note that if training were to update the representations directly, instead of indirectly through updating the weights \(\), they would evolve along the following "representational gradients":

\[_{}^{(1)}}^{} =(D}^{(1)}-}^{(2)})D,\] (5) \[_{}^{(1)}}^{} =-}^{(2)}}{\|D}^{(1)}\|\|} ^{(2)}\|}+}^{(1)})^{}}^{(2)}}{\|D }^{(1)}\|^{3}\|}^{(2)}\|}D^{2}}^{(1)}.\] (6)

In practice, however, representations of different samples do not evolve independently along these gradients, but influence each other through parameter changes in \(\). This interdependence of representations and parameters are captured by the empirical NTK \(_{t}(,)\) (cf. Eq. (2)). Because the NTK is positive semi-definite, loosely speaking, gradient descent on the parameters changes representations "in the direction" of the above representational gradients.

To see this link more formally, we express the NTK in the eigenbasis as \(_{t}(,)=_{}}_{}}^{}\) where \(}=_{t}()=U^{}z_{t}()\). Since we are concerned with the learning dynamics in this rotated basis, we will rewrite Eq. (2) for continuous-time gradient descent for a generic loss function \(\) as:

\[}}{t}=-_{t}(, )_{}}.\] (7)

Note, that structurally these dynamics are the same as the embedding space dynamics in Eq. (2) but merely expressed in the predictor eigenbasis (see Lemma 2 in Appendix B for a derivation). Although \(_{t}\) changes over time and is generally intractable in finite-width networks, it is positive semidefinite. This property guarantees that the cosine angle between the representational training dynamics under the parameter-space optimization of a neural network \(}{t}}-_{t}_ {}}\) and the dynamics that would result from optimizing the representations \(}{t}}-_{}}\) is non-negative:

\[-_{}},}}{t}=_{}},_{t}_{}}  0.\]

In other words, the representational updates due to network training lie within a 180-degree cone of the dynamics prescribed by Eqs. (5) and (6). This guarantee makes it possible to draw qualitative conclusions about asymptotic collective behavior, e.g., whether a network is bound to collapse or not, from analyzing the more tractable dynamics that follow the representational gradients \(}{t}}-_{}}\) of the transformed BYOL/SimSiam loss. For ease of analysis, we now consider linear networks with Gaussian i.i.d inputs, an important limiting case amenable for theoretical analysis . In this settingthe empirical NTK becomes the identity and the simplified representational dynamics are exact, allowing us to fully characterize the representational dynamics for \(^{}\) and \(^{}\) in the following two theorems. In the proofs for these theorems, we show that the assumption of Gaussian inputs can be relaxed further.

**Theorem 1**.: (Representational dynamics under \(^{}\)) _For a linear network with i.i.d Gaussian inputs learning with \(^{}\), the representational dynamics of each mode \(m\) independently follow the gradient of the loss \(-_{}}^{}\). More specifically, the dynamics uncouple and follow \(M\) independent differential equations:_

\[_{m}^{(1)}}{t}=-^{}}{_{m}^{(1)}}(t)=_{m}(_{ m}^{(2)}-_{m}_{m}^{(1)}),\] (8)

_which, after taking the expectation over augmentations yields the dynamics:_

\[_{m}}{t}=_{m}(1-_{m} )_{m}.\] (9)

We provide the proof in Appendix B and appreciate that \(}{t}_{m}\) has the same sign as \(_{m}\) whenever \(_{m}<1\) and the opposite sign whenever \(_{m}>1\). These dynamics are convergent and approach an eigenvalue \(_{m}\) of one, thereby preventing collapse of mode \(m\). Since the eigenmodes are orthogonal and uncorrelated, and the condition simultaneously holds for all modes, this ultimately prevents both representational and dimensional collapse . Since the eigenvalues also correspond to the variance of the representations, the underlying mechanism constitutes an _implicit_ form of variance regularization. Finally, we note that the above decoupling of the dynamics for the Euclidean loss has been described previously in Tian et al. .

Nevertheless, the representational dynamics are different for the commonly used cosine loss \(^{}\).

**Theorem 2**.: (Representational dynamics under \(^{}\)) _For a linear network with i.i.d Gaussian inputs trained with \(^{}\), the dynamics follow a system of \(M\) coupled differential equations:_

\[_{m}^{(1)}}{t}=}{\|D }^{(1)}\|^{3}\|}^{(2)}\|}_{k m}_{k}( _{k}(_{k}^{(1)})^{2}_{m}^{(2)}-_{m}_{m}^{(1 )}_{k}^{(1)}_{k}^{(2)}),\] (10)

_and reach a regime in which the eigenvalues are comparable in magnitude. In this regime, the expected update over augmentations is well approximated by:_

\[_{m}}{t}_{m}[_{m}^{2}}{\|D}\|^{3}}] [_{m}}{\|}\|}]_{k m}_{k }(_{k}-_{m}),\] (11)

where we have assumed averages over augmentations. See Appendix B for the proof. Theorem 2 states that \(}{t}_{m}\) has the same or different sign as \(_{m}\) depending on the sign of the aggregate sum \(_{k m}_{k}(_{k}-_{m})\). This relation suggests that a steady state is only reached through mutual agreement when the non-zero eigenvalues are all equal. In contrast to the Euclidean case, there is no pre-specified target value (see Fig. 5 in Appendix A). Thus, the cosine loss also induces implicit variance regularization, but through a markedly different mechanism in which eigenmodes cooperate.

### Stop-grad and predictor network are essential for implicit variance regularization.

We now extend our analysis to explain the known failure modes due to ablating the predictor or the stop-gradient for each distance metric. When we omit the stop-grad operator from \(^{}\), we have:

\[^{}_{}=\|W_{} ^{(1)}-^{(2)}\|^{2}_{m}}{ t}=-(1-_{m})^{2}_{m},\] (12)

so that \(}{t}_{m}\) and \(_{m}\) always have opposite signs (see Appendix C for the derivation). This drives the representations toward zero with exponentially decaying eigenvalues, causing the notorious representational collapse . Omitting the stop-grad operator from \(^{}\) yields a nontrivial expression for the dynamics causing the largest eigenmode to diverge (see Appendix C). Interestingly, this is different from the collapse to zero inferred for the Euclidean distance.

Similarly, when removing the predictor network in the Euclidean loss case, the dynamics read:

\[^{}_{}=\|^{(1)}-(^{(2)})\|^{2}_{m}}{t}=0,\] (13)

meaning that no learning updates occur. When the predictor is removed in the cosine loss case, the dynamics are:

\[^{}_{}=-^{(1)})^ {}(^{(2)})}{\|^{(1)}\|\|(^{(2)})\| }_{m}}{t}=_{k m}( [_{k}^{2}}{\|}\|^{3}}] [_{m}}{\|}\|}]-[_{m}_{k}}{\|}\|^{3}}][_ {k}}{\|}\|}]).\] (14)

As we show in Appendix C, these dynamics also avoid collapse. However, the effective learning rates become impractically small without the eigenvalue factors from Eq. (11). We summarized the predicted dynamics of all settings in Table 1. Thus, our analysis provides mechanistic explanations for why stop-grad and predictor networks are required for avoiding collapse in non-contrastive SSL.

### Isotropic losses that equalize convergence across eigenmodes

In Eqs. (9) and (11) the eigenvalues appear as multiplicative learning rate modifiers in front of the difference terms that determine the fixed point. Hence, modes with larger eigenvalues converge faster than modes with smaller eigenvalues, reminiscent of previous theoretical work on supervised learning . We hypothesized that the anisotropy in learning dynamics could lead to slow convergence for small eigenvalue modes or instability for large eigenvalues. To alleviate this issue, we designed alternative isotropic loss functions that equalize relaxation dynamics for all eigenmodes by exploiting the stop-grad function. Put simply, this involves taking the dynamics from Eqs. (8) and (10), removing the leading \(_{m}\) term, and deriving the loss function that would result in the desired dynamics. One such isotropic "IsoLoss" function for the Euclidean distance is:

\[^{}_{}=\|^{(1)}-(^{(2)}+^{(1)}-W_{}^{(1)})\|^{2}.\] (15)

We note that this IsoLoss has the same numerical value as \(^{}\), but the gradient flow is modified by placing the prediction inside the stop-grad and also adding and subtracting \(^{(1)}\) inside and outside of the stop-grad. The associated idealized learning dynamics in our analytic framework are given by:

\[_{m}}{t}=(1-_{m})_{m},\] (16)

where the \(_{m}\) factor (cf. Eq. (9)) disappeared (Table 1). Similarly, for the cosine distance,

\[^{}_{}=-(^{(1)})^{} (^{(2)}}{\|W_{}^{(1)}\|\|^{(2)}\|} )+(}^{(1)})^{} ^{(2)}}{\|W_{}^{(1)}\|^{3}\|^{(2)}\|})\|W_{ }^{1/2}^{(1)}\|^{2}\] (17)

is one possible IsoLoss, in which \(W_{}^{1/2}=UD^{1/2}U^{}\) with the square-root applied element-wise to the diagonal matrix \(D\). While this IsoLoss does not preserve numerical equality with the original loss \(^{}\), it achieves the desired effect of removing the leading \(_{m}\) learning-rate modifier (cf. Table 1).

   Loss & \(_{m}/t\) & Predicted dynamics \\  \(^{}\) & \(_{m}(1-_{m})\) & \(\)s converge to 1, large ones faster. \\ \(^{}_{}\) & \(-(1-_{m})^{2}\) & All \(\)s collapse. \\ \(^{}_{}\) & 0 & No learning updates. \\ \(^{}_{}\) & \((1-_{m})\) & \(\)s converge to 1 at homogeneous rates. \\  \(^{}\) & \(_{m}_{k m}_{k}(_{k}-_{m})\) & \(\)s converge to equal values. \\ \(^{}_{}\) & Appendix C & All \(\)s diverge. \\ \(^{}_{}\) & Appendix C & \(\)s converge to equal values at low rates. \\ \(^{}_{}\) & \(_{k m}_{k}(_{k}-_{m})\) & \(\)s converge to equal values at homogeneous rates. \\   

Table 1: Summary of eigenvalues as predicted by our analysis for linear networks.

## 3 Numerical experiments

To validate our theoretical findings (cf. Table 1), we first simulated a small linear Siamese neural network as shown in Fig.1a, for which Theorems 1 and 2 hold exactly. We fed the network with independent standard Gaussian inputs, and generated pairs of augmentations using isotropic Gaussian perturbations of standard deviation \(=0.1\). We then trained the linear encoder with each configuration described above. Training the network with \(^{}_{}\) resulted in collapse with exponentially decaying eigenvalues, whereas \(^{}_{}\) succumbed to diverging eigenvalues as predicted (Fig. 2a). Training without the predictor caused vanishing updates for \(^{}_{}\) and slow learning for \(^{}_{}\), in line with our analysis (Fig. 2b). Optimizing \(^{}\), the representations become increasingly isotropic with all the eigenvalues \(_{m}\) converging to one (Fig. 2c, top), whereas optimizing \(^{}\) also resulted in the eigenvalues converging to the same value, but different from one (Fig. 2c, bottom). The anisotropy in the dynamics of different eigenmodes noted above is particularly striking in the case of the Euclidean distance (Fig. 2c). Training with \(^{}_{}\) and \(^{}_{}\) resulted in similar convergence properties as their non-isotropic counterparts, but the eigenmodes converged at more homogeneous rates (Fig. 2d). Finally, we confirmed that these findings were qualitatively similar in the corresponding nonlinear networks with ReLU activation (see Fig. 6 in Appendix A). Thus, our theoretical findings hold up in simple Siamese networks.

Figure 2: Evolution of representations (top) and eigenvalues (below) of \(W_{}\) throughout training with different loss functions. The representational trajectories correspond to training with \(M=2\) for visualization and the points signify the final network outputs. The eigenvalues were computed with dimensions \(N=15\) and \(M=10\). **(a)** Omitting the stop-grad leads to representational collapse in the Euclidean case (top), and diverging eigenvalues for the cosine case (bottom). **(b)** No learning occurs without the predictor with the Euclidean distance, but learning does occur with the cosine distance, although at low rates. Note the change in scale of the time-axis. **(c)** Optimizing the BYOL/SimSiam loss leads to isotropic representations under both distance metrics. **(d)** Optimizing IsoLoss has the same effect, but with uniform convergence dynamics for all eigenvalues for both distance metrics.

### Theory qualitatively captures dynamics in nonlinear networks and real-world datasets.

To investigate how well our theoretical analysis holds up in non-toy settings, we performed several self-supervised learning experiments on CIFAR-10, CIFAR-100 , STL-10 , and TinyImageNet . We based our implementation1 on the Solo-learn library , and used a ResNet-18 backbone  as the encoder and the cosine loss, unless mentioned otherwise (see Appendix D for details). As baselines for comparison, we trained the same backbone using BYOL with the nonlinear predictor and DirectPred with the closed-form linear predictor. We recorded the online readout accuracy of a linear classifier trained on frozen features following standard practice, evaluated either on the held-out validation or test set where available.

We found that the eigenvalue dynamics of the representational correlation matrix in the ResNet-18 closely mirrored the analytical predictions for the closed-form predictor. For Euclidean distances (Fig. 3a), the eigenvalues for DirectPred and IsoLoss converged to a small range of values around one. However, the dynamics for BYOL with a learnable nonlinear predictor deviated significantly with the eigenvalues distributed over a larger range. Consistent with our analysis, IsoLoss had faster initial dynamics for the eigenvalues which also resulted in a faster initial improvements in model performance (Fig. 3b). The faster learning with IsoLoss was even more evident for the cosine distance (Fig. 3c). Surprisingly, BYOL, which uses a nonlinear predictor also closely matched the predicted dynamics in the case of the cosine distance. Furthermore, the dynamics showed a stepwise learning phenomenon wherein eigenvalues are progressively recruited one-by-one, consistent with recent findings for other SSL methods . Finally, IsoLoss exhibited faster initial learning (Fig. 3d), in agreement with our theoretical analysis. Thus, our theoretical analysis accurately predicts key properties of the eigenvalue dynamics in nonlinear networks trained on real-world datasets.

Figure 3: Learning dynamics for a ResNet-18 network trained with different loss functions. **(a)** Evolution of the eigenvalues of the representation correlation matrix during training for closed-form predictors as prescribed by DirectPred (left) and IsoLoss (center). Right: Standard BYOL with the nonlinear trainable predictor. For clarity, we plot only one in ten eigenvalues. Both \(^{}\) and \(^{}_{}\) drive the eigenvalues to converge quickly and remain constant thereafter with relatively small fluctuations (note the logarithmic scale). BYOL results in the eigenvalues being spread across a large range of magnitudes. **(b)** Linear readout validation accuracy for \(^{}\) and \(^{}_{}\) during the first 500 training epochs. IsoLoss accelerates the initial learning dynamics as predicted by the theory. **(c)** Same as in (a) but for the cosine distance. \(^{}\) recruits few large eigenvalues, but drives them gradually to the same magnitude, whereas \(^{}_{}\) quickly recruits _all_ eigenvalues and causing them to converge to an isotropic solution. In contrast, BYOL recruits eigenvalues in a step-wise manner. **(d)** Same as (b) but for the cosine distance.

### IsoLoss promotes eigenvalue recruitment and works without an EMA target network.

To further investigate the impact of IsoLoss on learning, we first verified that it does not have any adverse effects on downstream classification performance. We found that IsoLoss matched or outperformed DirectPred on all benchmarks (Table 2) when trained with an EMA target network as used in the original studies. Yet, it performed slightly worse than BYOL, which uses a nonlinear predictor and an EMA target network. Because EMA target networks are thought to amplify small eigenvalues , we speculated that IsoLoss may work without it. We repeated training for the closed-form predictor losses without EMA to test this idea. We found that \(_{}^{}\) was indeed robust to EMA removal. However, it caused a slight drop in performance (Table 2) and a notable reduction in the recruitment of small eigenvalues (see Fig. 7 in Appendix A). In contrast, optimizing the standard BYOL/SimSiam loss \(^{}\) with the symmetric linear predictor was unstable, as reported previously . Finally, we confirmed the above findings also hold for \(=1\) (cf. Eq. (1)) as prescribed by DirectCopy  (see Table 3 in Appendix A). Thus, IsoLoss allows training without an EMA target network.

The above result suggests that IsoLoss promotes the recruitment of small eigenvalues in closed-form predictors. Another factor that has been implicated in suppressing recruitment is weight decay . To probe how weight decay and IsoLoss affect small eigenvalue recruitment, we repeated the above simulations with EMA and different amounts of weight decay. Indeed, we observed less eigenvalue recruitment with increasing weight decay for DirectPred (Appendix A, Fig. 8a), but not for IsoLoss (Fig. 8b). However, for IsoLoss larger weight decay resulted in lower magnitudes of _all_ eigenvalues. Hence, IsoLoss reduces the impact of weight decay on eigenvalue recruitment.

## 4 Discussion

We provided a comprehensive analysis of the SSL representational dynamics in the eigenspace of closed-form linear predictor networks (i.e., DirectPred and DirectCopy) for both the Euclidean loss and the more commonly used cosine similarity. Our analysis revealed how asymmetric losses prevent representational and dimensional collapse through _implicit_ variance regularization along orthogonal eigenmodes, thereby formally linking predictor-based SSL with explicit variance regularization approaches [6; 14; 7]. Our work provides a theory framework which further complements the growing body of work linking contrastive and non-contrastive SSL [35; 36; 37; 24; 38].

We empirically validated the key predictions of our analysis in linear and nonlinear network models on several datasets, including CIFAR-10/100, STL-10, and TinyImageNet. Moreover, we found that the eigenvalues of the predictor network act as learning rate multipliers, causing anisotropic learning dynamics. We derived Euclidean and cosine IsoLosses, which counteract this anisotropy and enable closed-form linear predictor methods to work without an EMA target network, thereby further consolidating its presumed role in boosting small eigenvalues .

To our knowledge, this is the first work to comprehensively characterize asymmetric SSL learning dynamics for the cosine distance metric widely used in practice. However, our analysis rests on several assumptions. First, the analytic link through the NTK between gradient descent on parameters and the representational changes is an approximation in nonlinear networks. Moreover, we assumed Gaussian i.i.d inputs for proving Theorems 1 and 2. Although these assumptions generally do not

   Model & EMA & CIFAR-10 & CIFAR-100 & STL-10 & TinyImageNet \\  BYOL & Yes & 92.6\({}^{*}\) & 70.5\({}^{*}\) & 91.7 \(\) 0.1 & 38.3 \(\) 1.5 \\ SimSiam & No & 90.7 \(\) 0.2 & 66.3 \(\) 0.4 & 87.5 \(\) 0.7 & 39.8 \(\) 0.6 \\   & Yes & 92.0 \(\) 0.2 & 66.6 \(\) 0.5 & 88.8 \(\) 0.3 & 40.1 \(\) 0.5 \\  & No & 12.1 \(\) 1.3\({}^{}\) & 1.6 \(\) 0.6\({}^{}\) & 10.4 \(\) 0.1\({}^{}\) & 1.3 \(\) 0.2\({}^{}\) \\   & Yes & 91.5 \(\) 0.2 & 69.0 \(\) 0.2 & 89.0 \(\) 0.3 & 44.8 \(\) 0.4 \\  & No & 91.5 \(\) 0.2 & 64.3 \(\) 0.3 & 87.4 \(\) 0.1 & 40.4 \(\) 0.4 \\   

Table 2: Linear readout validation accuracy in % \(\) stddev over five random seeds. The \(\) denotes crashed runs, known to occur with symmetric predictors like DirectPred . Starred values \({}^{*}\) were taken from the Solo-learn library .

hold in nonlinear networks, our analysis qualitatively captures their overall learning behavior and predicts how networks respond to changes in the stop-grad placement.

In summary, we have provided a simple theoretical explanation of how asymmetric loss configurations prevent representational collapse in SSL and elucidate their inherent dependence on the placement of the stop-grad operation. We further demonstrated how the eigenspace framework allows crafting new loss functions with a distinct impact on the SSL learning dynamics. We provided one specific example of such loss functions, IsoLoss, which equalizes the learning dynamics in the predictor's eigenspace, resulting in faster initial learning and improved stability. In contrast to DirectPred, IsoLoss learns stably without an EMA target network. Our work thus lays out an effective framework for analyzing and developing new SSL loss functions.

This project was supported by the Swiss National Science Foundation [grant numbers PCEFP3_202981 and TMPFP3_210282], by EU's Horizon Europe Research and Innovation Programme (grant agreement number 101070374) funded through SERI (ref 1131-52302), and the Novartis Research Foundation. The authors declare no competing interests.