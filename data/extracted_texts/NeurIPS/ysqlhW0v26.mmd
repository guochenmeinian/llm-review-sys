# A Unifying Perspective on Multicalibration: Game Dynamics for Multi-Objective Learning+

Footnote †: Authors are ordered alphabetically. Correspondence to eric.zh@berkeley.edu.

Nika Haghtalab

University of California, Berkeley

{nika,jordan,eric.zh}@berkeley.edu

Michael I. Jordan

University of California, Berkeley

{nika,jordan,eric.zh}@berkeley.edu

Eric Zhao

University of California, Berkeley

{nika,jordan,eric.zh}@berkeley.edu

###### Abstract

We provide a unifying framework for the design and analysis of multicalibrated predictors. By placing the multicalibration problem in the general setting of multi-objective learning--where learning guarantees must hold simultaneously over a set of distributions and loss functions--we exploit connections to game dynamics to achieve state-of-the-art guarantees for a diverse set of multicalibration learning problems. In addition to shedding light on existing multicalibration guarantees and greatly simplifying their analysis, our approach also yields improved guarantees, such as error tolerances that scale with the square-root of group size versus the constant tolerances guaranteed by prior works, and improving the complexity of \(k\)-class multicalibration by an exponential factor of \(k\) versus Gopalan et al. . Beyond multicalibration, we use these game dynamics to address emerging considerations in the study of group fairness and multi-distribution learning.

## 1 Introduction

Multicalibration has emerged as a powerful tool for addressing fairness considerations in machine learning. Based on calibrated forecasting [6; 12]--which requires among instances \(x\) a predictor \(h\) predicts the probability \(h(x)=v\), a fraction \(v\) have a positive outcome--multicalibration yields more fine-grained guarantees by seeking calibration across large and possibly overlapping collections of sub-populations . Multicalibration has been studied in numerous settings, including those with rich label sets (multi-class multicalibration ), adversarial rather than stochastic data (online multicalibration ), and problems where no Bayes classifier exists (agnostic multicalibration ). The concept of multicalibration has also been applied to estimate other quantities, such as higher moments , and been strengthened in various ways, such as providing conditional guarantees .

Multicalibration's versatility has led to the development of numerous specialized algorithms, each tailored to a unique multicalibration problem and requiring its own individualized analysis. Promising attempts to provide an overarching conceptual framework for multicalibration, such as outcome indistinguishability , have had limited success in unifying these various algorithms. In this paper, we tackle this challenge, developing _a general-purpose algorithmic framework to guide the design of multicalibrated learning algorithms for a wide range of settings and considerations._

Our approach is a dynamical systems and game-theoretic approach [see, e.g., 13; 9]. We demonstrate that many multicalibration algorithms can be formulated as particular instances of two-player zero-sum games where players independently either run no-regret algorithms or best-response algorithms. A wide range of multicalibration algorithms that exhibit varying trade-offs can be obtained by plugging in different no-regret and best-response algorithms. This unified framework both recovers existing guarantees and in many cases improves upon them.

Although approaching multicalibration--a min-max optimization problem--with game dynamics seems like an obvious approach, no prior work has succeeded in using a game dynamics framing to unify multicalibration algorithms. The primary challenge is not reducing multicalibration to min-max optimization, which is straightforward (Facts 2.5,2.6), but rather solving the resulting equilibrium computation problem in a way that connects to practical algorithms. The needs of multicalibration (such as determinism, large and complex predictor spaces, etc.) differ significantly in this regard from earlier applications of general-purpose no-regret algorithms and game dynamics.

Our primary contributions can be categorized into three areas.

**1) Unifying framework.** In Section 3, we give an overview of game dynamics as a concise but general framework for obtaining multi-objective learning guarantees. To use these dynamics as a generic solution to various multicalibration problems, we introduce a powerful general-purpose no-regret algorithm (Theorem 3.7) and a distribution-free best-response algorithm (Theorem 3.8), for calibration-like objectives. This approach allows us to unify the diverse--and often, seemingly unrelated--algorithms that have been studied in the multicalibration literature, offering a general template for their derivation and analysis.

**2) New guarantees.** In Sections 4, we use our framework to improve guarantees for various multicalibration settings (see Table 1) by simply plugging in different no-regret and best-response algorithms. These improvements include an exponential (in \(k\)) reduction in the complexity of \(k\)-class multicalibration over Gopalan et al. , a polynomial (in \(1/\)) reduction in the complexity of learning a succinct multicalibrated predictor over Dwork et al. , the first conditional multicalibration results for the batch setting, the first agnostic multicalibration guarantees that beats uniform convergence, and a multicalibration algorithm that efficiently guarantees an error tolerance that scales with _square-root_ of each group's probability mass. In Section 5, we demonstrate that our framework can be extended to analyze problems beyond multicalibration, such as multi-group learning .

**3) Simplified analyses.** We also demonstrate that our framework can recover the guarantees of various existing multicalibration algorithms, including online multicalibration  and moment multicalibration , while avoiding intricate and problem-specific arguments.

### Related Work

The study of calibration originated in online (adversarial) forecasting [6; 20], with classical literature having also studied calibration across multiple sub-populations . Multicalibration, on the other hand, is classically studied in the stochastic setting; i.e., where \((x_{i},y_{i}) D\), in which case calibration is trivially satisfied by the predictor \(h(x)=[y]\). Due to this difference in formulation in the literature on calibration and that on multicalibration, the specific technical tools that have been developed in these areas have largely remained distinct. Our work can be viewed as bridging this gap by showing

  Problem & Complexity & Dynamic & Previous Results & Our Results & Reference \\   MC (Det) & Oracle & NRBR & \(O(k^{-2})\) & \(O((k)^{-2})\) & Thm 4.2 \\  MC (Sqrt Err, Det) & Sample & NRBR & \((^{-6}(k||))\) & \((^{-4}(k||))\) & Thm 4.4 \\  Agnostic MC (Det) & Oracle & NRBR & \(O(||^{-2})\) & \(O(^{-2})\) & Thm E.9 \\  Agnostic MC & Sample & NRNR & \(O(||^{-2})\) & \(O((d+k)^{-2})\) & Thm E.10 \\  Cond. MC (Det) & Oracle & NRBR & \(O(||^{2}^{-2})\) & \(O(||^{-2})\) & Thm E.3 \\  Cond. MC & Sample & NRNR & \(O(||^{2}(d+k)^{-2})\) & \(O(||(d+k)^{-2})\) & Thm E.4 \\  MC (Succinct) & Sample & NRNR & \(((d+k)^{-3})\) & \(O((d+k)^{-2})\) & Thm 4.1 \\   Online MC & Regret & BRNR & \(O(|)T})\) & (Matching) & Thm 4.3 \\  \(r\)th Moment MC & Oracle & NRBR & \(O(r^{-4})\) & (Matching) & Thm E.13 \\  

Table 1: This table summarizes the sample complexity and agnostic learning oracle complexity rates we obtain for multicalibration (MC), compared with the previous state of the art. \(\) denotes the set of groups one wants multicalibration on, \(d\) the VC dimension of \(\), \(k\) the number of label classes, and \(\) the error tolerance. (Det) and (Succinct) respectively denote when only deterministic or only succinct predictors are acceptable. “Sqrt Err” refers to requiring an error tolerance of \(\) for each group rather than \(\).

that game-theoretic dynamics provides a unified foundation for studying multicalibration, just as no-regret learning underpins the study of calibration.

Motivated by fairness considerations, a formal definition of multicalibration was presented by , and has found a wide range of applications and conceptual connections to Bayes optimality, conformal predictions, and computational indistinguishability [21; 22; 16; 18; 7; 23]. Algorithms for multicalibration have largely developed along two lines: one studying oracle-efficient boosting-like algorithms [see, e.g., 21; 25; 17; 7] and another studying algorithms with flavors of online optimization [see, e.g., 18; 35]. Our work establishes that the contrast between these lines of work and the algorithms they develop are entirely attributable to different choices of game dynamics.

Multi-objective and multi-distribution learning are concepts that have found broad applications in addressing fairness, collaboration, and robustness challenges. Blum et al.  initiated the study of learning predictors with near-optimal accuracy across multiple populations (distributions), with later works attaining tight bounds [19; 34; 31]. Recently, multi-objective learning has been applied to settings where sub-populations are mutually compatible [37; 39; 3]. In Section 5, we use our framework to match and improve guarantees relative to this literature.

## 2 Preliminaries

We use \(\) to denote a feature space and \(\) a label space, where \(=[k]\) in \(k\)-class classification. A data distribution \(D\) is a probability distribution supported on labeled datapoints \(\). We use \(\) to denote a set of hypotheses and \(\) a set of objectives, where an objective--or equivalently, a loss--is a function \(:()\) that takes a hypothesis and datapoint and returns a penalty value. We denote expected objective values by \(_{D,}(h)_{(x,y) D}[(h,(x, y))]\). For non-deterministic \(p()\) and \(q()\), we overload notation to write \(_{q}(p)_{h p,(D,) q}[_{D,}(h)]\). We often use the shorthands \(x^{(1:T)} x^{(1)},,x^{(T)}\) and \(\{f(x^{(t)})\}^{(1:T)}=f(x^{(1)}),,f(x^{(T)})\). We also write \(f(a,)\) to denote the function \(x f(a,x)\) or \(f_{()}(a)\) to denote \(x f_{x}(a)\). For \(y[k]\), \(_{y}\{0,1\}^{k}\) denotes its one-hot delta function, while for \(y[k],j[k]\) we write \(_{y,j}=1[y=j]\).

### Multicalibration

We use \(=()^{}\) to denote the set of all \(k\)-class _predictors_ which are maps from features to label distributions. To differentiate between predictors and distributions over predictors, we refer to \(h\) as a _deterministic_ predictor, and \(p()\), as a _non-deterministic_ predictor.2 Calibration is a property of predictors \(h\) requiring, for example in binary classification, that among instances \(x\) assigned prediction probability \(h(x)=[1-v,v]\) a fraction \(v\) are truly labeled \(1\). _Multicalibration_ is the finer-grained notion that requires calibration on subgroups of one's domain. This set of subgroups is typically finite or of finite VC dimension. In practice, we work with approximate notions of calibration/multicalibration and discretize the range of probability assignment. In \(k\)-class prediction, we partition the \(k\)-dimensional hypercube into \(^{k}\) equal cubes \(V_{}^{k}\), where \(V_{}\{[0,1/),[1/,2/),\}\). For any interval \(v V_{}^{k}\), we use \(h(x) v\) to denote that prediction \(h(x)\) falls pointwise in the buckets of \(v\), i.e., \(h(x)_{j} v_{j}\) for all \(j[k]\). We next formally define multicalibration.3

**Definition 2.1**.: _Fix \(>0\), \(_{+}\), and a set of groups \( 2^{}\). A (possibly non-deterministic) \(k\)-class predictor \(p()\) is \((,,)\)-multicalibrated for some data distribution \(D\) if_

\[ S,v V_{}^{k},j[k]:\ |*{ }_{(x,y) D,h p}[(h(x)_{j}-_{y,j}) 1[h(x) v,x S] ]|.\]

_That is, \(p\) is calibrated on every level set \(v V_{}^{k}\) of every group \(S\) for every class \(j[k]\). We are often specifically interested in a deterministic solution; that is, where \(p\)._The batch setting, where we want to find a multicalibrated predictor for some fixed data distribution \(D\), is the most commonly studied. Multicalibration can also be defined for online settings where the data distribution changes adversarially over time.

**Definition 2.2** (Online multicalibration).: _Fix \(>0\), \(_{+}\), and a set of groups \( 2^{}\). In online multicalibration, at every timestep \(t[T]\), a learner chooses a \(k\)-class predictor \(p^{(t)}()\). Nature, which observes \(p^{(1:t)}\), responds with any choice of data distribution \(D^{(t)}\). The learner's predictors \(p^{(1:T)}\) are \((,,)\)-online multicalibrated on \(D^{(1:T)}\) if_

\[ S,v V_{}^{k},j[k]:|_{t= 1}^{T}*{}_{h p^{(t)}\\ (x,y) D^{(t)}}[1[x S] 1[h(x) v](h(x)_{j}- _{y,j})]|.\]

### Multi-Objective Learning

We use multi-objective learning (a generalization of multi-distribution learning introduced by ) as a tool for studying multicalibration and other related problems. The goal of multi-objective learning is to find a hypothesis that simultaneously minimizes a set of objective values.

**Definition 2.3** (Multi-objective learning).: _A multi-objective learning problem \((,,)\) consists of a set of objectives \(\), a hypothesis class \(\), and a set of data distributions \(\). An \(\)-optimal solution to \((,,)\) is a (potentially non-deterministic) hypothesis \(p()\) where_

\[_{D,}_{D,}(p)_{h^{ *}}_{D^{*},^{*}} _{D^{*},^{*}}(h^{*})+.\] (1)

_We usually prefer a deterministic solution \(p\) over a non-deterministic solution \(p()\)._

We mainly consider _single-distribution_ multi-objective problems, where \(=\{D\}\), though _multi-distribution_ multi-objective learning arises in conditional multicalibration (Section E.1) and group fairness (Section 5). We can also consider multi-objective learning in online adversarial settings.

**Definition 2.4** (Online multi-objective learning).: _An online multi-objective learning problem \((,,)\) consists of a set of distributions \(\), objectives \(\) and hypothesis class \(\). At each timestep \(t[T]\), a learner first picks a hypothesis \(p^{(t)}()\). Nature, who sees \(p^{(1:t)}\), responds with a data distribution \(D^{(t)}\). We say the hypotheses \(p^{(1:T)}\) are \(\)-optimal on the distributions \(D^{(1:T)}\) if_

\[_{}_{t=1}^{T}_{D^{(t)},}(p ^{(t)})_{D^{*}}_{h^{*}}_{^{*} }_{D^{*},^{*}}(h^{*})+.\] (2)

In many problems we consider, like online multicalibration, Nature can pick from any data distribution; that is, \(\) is unrestricted. Let us also note that the baseline in the right-hand-side of (2) differs from that of (1). This is intentional: when \(\) is unrestricted, the min-max baseline of (1) may be large, since there may be no hypothesis that is simultaneously good for all distributions. Instead, the max-min baseline of (2) is the best hypothesis \(h^{*}\) for the most difficult distribution \(D^{*}\).

Multicalibration as multi-objective learning.(Batch) Multicalibration is a single-distribution multi-objective learning problem, whose objectives penalize over-estimation and under-estimation of label probabilities on subsets of the domain.

**Fact 2.5**.: _Let \(D\) be a data distribution for some \(k\)-class prediction problem and fix \(>0\), \(_{+}\), and a set of groups \( 2^{}\). For every direction \(i\{ 1\}\), level set \(v V_{}^{k}\), group \(S\), and class \(j[k]\), we define an objective \(_{i,j,S,v}:\) where_

\[_{i,j,S,v}(h,(x,y))=0.5+0.5 i 1[h(x) v,x S](h(x)_{j}- _{y,j})\] (3)

_and \(_{}\{_{i,j,S,v}\}_{i,j,S,v}\) is the set of these objectives. Predictor \(p()\) is a \(\)-optimal solution to the multi-objective problem \((\{D\},_{},)\) if and only if \(p\) is \((,2,)\)-multicalibrated for \(D\)._

Online multicalibration is similarly an online multi-objective learning problem.

**Fact 2.6**.: _Let \(D^{(1:T)}\) be data distributions for some \(k\)-class prediction problem, \(\) be the set of all data distributions, and \(_{}\) be as defined in (3). Fix \(>0\), \(_{+}\), and a set of groups \( 2^{}\). A sequence of predictors \(p^{(1:T)}()\) is \(\)-optimal on \(D^{(1:T)}\) for the online multi-objective learning problem \((,_{},)\) if and only if \(p^{(1:T)}\) is \((,2,)\)-online multicalibrated on \(D^{(1:T)}\)._Tools for Solving Multi-Objective Learning using Game Dynamics

A common approach to multi-objective learning is to imagine a game between a minimizing player who proposes hypotheses and a maximizing player who proposes objectives and data distributions. It is well-established (inspired by min-max equilibria, e.g., ) that a solution can be obtained when players use no-regret algorithms. However, considerations that arise in multicalibration motivate us to study a broader range of dynamics and their implications than has been commonly explored.

Online learning.In an online learning problem, at each timestep \(t[T]\), a learner chooses an action \(a^{(t)}\) which an adversary observes and responds to with a cost function \(c^{(t)}:\). We will usually assume costs to be linear maps. The learner's _regret_ is defined as \((a^{(1:T)},c^{(1:T)}):=_{t=1}^{T}c^{(t)}(a^{(t)})-_{a^{*} }_{t=1}^{T}c^{(t)}(a^{*})\), which no-regret algorithms like Hedge  can bound.

**Lemma 3.1** (Hedge Regret ).: _In an online learning problem where the action set is the simplex \(=_{k}\) and costs are linear, the actions chosen by Hedge have a regret of at most \(2\)._

We write no-regret algorithms as a function of a sequence of cost functions \(:(^{})^{*}\). For example, when \(=_{k}\), the output of the Hedge algorithm is defined as \((c^{(1)},,c^{(t)})=[w_{1}/\|w\|_{1},, w_{k}/\|w\|_{1}]\) where \(w_{i}=(-_{=1}^{t}c^{(t)}(_{i}))\), for a specific choice of \(\).

We sometimes define regret against a different baseline \(B\), with \(_{B}(a^{(1:T)},c^{(1:T)}):=_{t=1}^{T}c^{(t)}(a^{(t)})-B\). For example, we often consider the min-max baseline \(B_{} T_{a^{*}}_{c^{*} }c^{*}(a^{*})\), where \(\) is the set of cost functions that the adversary chooses from. We also often encounter _stochastic cost functions_, functions of form \(c:()\) for which we want to minimize expected value \(_{D,c}_{(x,y) D}[c(,(x,y))]\) on some distribution \(D\). A stochastic cost \(c\) is linear if, for every \(x,y\), \(c(,(x,y))\) is a linear map. The regret of online learning algorithms on stochastic costs concentrates quickly.

**Lemma 3.2** (Stochastic Approximation , Lemma 3.1).: _Consider an online learning problem on the simplex \(_{k}\) with linear stochastic costs. Suppose, after each timestep \(t[T]\)--that is, after picking the action \(a^{(t)}\)--we estimate the expected cost \(_{D,c^{(t)}}\) with \(^{(t)}(a) c^{(t)}(a,(x^{(t)},y^{(t)}))\), where \((x^{(t)},y^{(t)})}}{{}}D\). With probability at least \(1-\), \(|(a^{(1:T)},\{_{D,c^{(t)}}\}^{(1:T)})- (a^{(1:T)},^{(1:T)})| O()\)._

We say an action \(a\) is an _\(\)-best response_ to a cost \(c\) if \(c(a)_{a^{*}}c(a^{*})+\). Similarly, an action \(a\) is a _distribution-free \(\)-best response_ to a stochastic cost \(c\) if \(_{x,y}c(a,(x,y))_{x,y }_{a^{*}}c(a^{*},(x,y))+\). We sometimes express the sample complexity of an algorithm in terms of the number of calls made to an _agnostic learning oracle_, which can compute \(\)-best-responses to stochastic cost functions using few samples. See Appendix A for a discussion.

Game dynamics.We consider no-regret dynamics between a learner (minimizing player) who chooses hypotheses \(p^{(t)}()\) and an adversary (maximizing player) who chooses data distributions and objectives \(q^{(t)}()\) where the learner's loss is \(_{q^{(t)}}(p^{(t)})\). When both players are no-regret, the time-average actions they picked quickly converge to an approximate solution for the multi-objective learning problem \((,,)\). This method of _no-regret dynamics_ has long played a role in empirical convergence to notions of equilibria . Here, we review these dynamics and their convergence guarantees. While the proofs of these lemmas are standard at a high level (and deferred to Appendix C.1), they differ in fundamental ways from past work. In particular, these lemmas consider _weak regret_, _single timestep solutions_ (instead of time-averaged ones), and the consequences of having _distribution-free best responses_, all of which play important roles in multicalibration.

**Lemma 3.3** (No-Regret vs. No-Regret (NRNRNR)).: _Consider a multi-objective learning problem \((,,)\), where a learner and adversary chose \(p^{(1:T)}()\) and \(q^{(1:T)}()\). If both players are no-regret, \(_{}(p^{(1:T)},\{_{q^{(t)}}() \}^{(1:T)}) T\) and \((q^{(1:T)},\{-_{()}(p^{(t)})\}^{(1:T)})  T\), then the non-deterministic hypothesis \(=(p^{(1:T)})\) is a \(2\)-optimal solution._

The next dynamic focuses on obtaining a solution _from a single timestep, rather than time-averaged solutions._ To obtain this, we consider a dynamics in which the learner goes first and is no-regret, the adversary observes the learner's action and then best responds.

**Lemma 3.4** (No-Regret vs. Best-Response (NRBR)).: _Consider a multi-objective learning problem \((,,)\), where a learner chose \(p^{(1:T)}()\) and an adversary chose \(q^{(1:T)}()\). If the learner is no-regret, \(_{}(p^{(1:T)},\{_{q^{(t)}}( )\}^{(1:T)}) T\), and the adversary \(\)-best-responded to the costs \(\{-_{()}(p^{(t)})\}^{(1:T)}\) using \(q^{(1:T)}\), then there is a \(t[T]\) where \(p^{(t)}\) is a \(2\)-optimal solution._

Once the existence of a single-round solution \(p^{(t)}\) is established by Lemma 3.4, it is easy to find which time step corresponds to this solution by using a few samples and testing all \(p^{(1:T)}\), as follows.

**Lemma 3.5**.: _Suppose a set of hypotheses \(p^{(1:T)}\) contains an \(\)-optimal solution. We can find a \(5\)-optimal solution \(p^{(t)} p^{(1:T)}\) using \(O(^{-2}||(||| |T/))\) samples with probability \(1-\)._

The next dynamic considers difficult distribution-free problems, such as online multi-objective learning. To enable learning in these scenarios, we consider a no-regret adversary that chooses objectives \(q^{(1:T)}()\) and a learner who first observes \(q^{(t)}\) and plays a distribution-free best-response. The following lemma considers the consequences of these interactions.

**Lemma 3.6** (Best-Response vs. No-Regret (BRNR)).: _Consider an online multi-objective learning problem \((,,)\), where a learner chose \(p^{(1:T)}()\), an adversary chose \(q^{(1:T)}()\), and \(D^{(1:T)}\) is any sequence. Assume that the adversary is no-regret, i.e., \((q^{(1:T)},\{-_{D^{(t)},()}(p^{(t)})\}^{(1: T)}) T\), and the learner's actions \(p^{(1:T)}\) are distribution-free \(\)-best-responses to the stochastic costs \(q^{(1:T)}\), i.e., \(_{x,y}_{(x,y),q^{(t)}}(p^{(t)})_{x,y}_{p^{}} _{(x,y),q^{(t)}}(p^{*})+\). Then, the hypotheses \(p^{(1:T)}\) are \(2\)-optimal on \(D^{(1:T)}\)._

The question of which dynamic should be used and their implementation hinges on the type of solution desired and what online learning and best-response guarantees are possible for each player. NRNR dynamics often offer maximum sample efficiency, since calculating \(\)-best-responses may be more sample intensive, but produce a time-averaged solution. NRBR dynamics, though less sample-efficient due to the adversary's repeated best-response computation, provides a single timestep solution and is crucial for, e.g., deterministic multicalibration. BRNR dynamics, where learners follow (act after the adversary) and have greater ease being no-regret, are crucial for online settings.

### No-regret and Best Response Computation in Multicalibration

In this section, we introduce algorithms for obtaining (weak) no-regret and best response guarantees in multicalibration, so that we can apply the previously discussed dynamics.

Since the adversary picks from the--usually, small--set of objectives \(_{}\), it can be no-regret using standard algorithms like Hedge. However, the learner picks from the--very large--space of all predictors \(\); if it used Hedge, its regret would grow linearly in domain size \(||\). An important aspect of multicalibration is that its complexity must be independent of the domain size \(\) (while it can depend on the complexity of the subgroups \(\)). We leverage structural properties of multicalibration objectives (see Appendix C.2 for formal treatment) to obtain generic no-regret and best response algorithms that give the learner domain-independent guarantees. Our first theorem gives a no-(weak)-regret learning algorithm for the learner that provides three important properties simultaneously: 1) domain-independent regret-bound that is also logarithmic in \(k\), 2) uses no samples (or knowledge of) the underlying distribution \(D\), and 3) deterministically outputs a deterministic predictor per round. Properties 1-2 lead to fast convergence and low sample complexity in the aforementioned dynamics and property 3 is key for obtaining deterministic multicalibration guarantees (via NRBR).

**Theorem 3.7**.: _Consider \(\) the set of \(k\)-class predictors and any adversarial sequence of stochastic costs \(q^{(1:T)}(_{})\), where \(_{}\) are the multicalibration objectives (3). There is a no-regret algorithm that outputs (deterministic) predictors \(h^{(1:T)}\) such that \(_{}(h^{(1:T)},\{_{D,q^{(t)}} \}^{(1:T)}) 2\) for every data distribution \(D\). The algorithm does not need any samples from \(D\)._

Proof.: Consider the following algorithm. At each feature \(x\), initialize a Hedge algorithm that picks an action \(h^{(t)}(x)()\) at each timestep \(t[T]\). Aggregating each algorithm's action yields our learner's overall action \(h^{(t)}\). For each \(x\), let \(h^{(t+1)}(x)\) be the outcome of Hedge at step\(t+1\) after observing linear loss functions \(f^{()}_{h^{()},x}:^{k}\) for \([t]\):

\[f^{()}_{h^{()},x}(z) 0.5+0.5_{i\{ 1\},j[k],S ,v V_{}^{k}}z_{j} q^{()}_{i,j,S,v} i 1[h^{ ()}(x) v,x S],\] (4)

where \(q^{()}_{i,j,S,v}\) is the probability \(q^{()}\) assigns to loss \(_{i,j,S,v}\). Hedge gives \(_{t=1}^{T}f^{(t)}_{h^{(t)},x}(h^{(t)}(x))-_{z^{*}( )}_{t=1}^{T}f^{(t)}_{h^{(t)},x}(z^{*}) 2\) (Lemma 3.1). Since this inequality holds for all \(x\),

\[2 }_{(x,y) D}[_{t=1}^{T}f^{(t)} _{h^{(t)},x}(h^{(t)}(x))]-}_{(x,y) D}[_{ z^{*}()}_{t=1}^{T}f^{(t)}_{h^{(t)},x}(z^{*})]\] (5) \[=}_{(x,y) D}[_{t=1}^{T}f^{(t)}_{h^ {(t)},x}(h^{(t)}(x))]-_{h^{*}}}_{(x,y) D}[_{t=1}^{T}f^{(t)}_{h^{(t)},x}(h^{*}(x))]\]

by law of total expectation, where the last transition is by defining \(h^{*}\) such that \(h^{*}(x)=z^{*}\) for every \(x\)-dependent choice of \(z^{*}\) in (5). Add and subtract \(_{t=1}^{T}}_{(x,y) D}[f^{(t)}_{h^{(t)},x}( _{y})]\) to obtain

\[_{t=1}^{T}}_{(x,y) D}[f^{(t)}_{h^{(t)},x}(h^{( t)}(x)\!-\!_{y})]\!\!2+0.5T,\] (6)

where the last transition is by the fact \(h^{*}(x)=}[_{y}|x.]\), \(_{t=1}^{T}}[f^{(t)}_{h^{(t)},x}(h^{*}(x)-_{ y})]=0.5T\). Next, we show the LHS of (6) is equivalent to \(}_{}(h^{(1:T)},\{_{D,q^{(t)}} \}^{(1:T)})\). First recall from multicalibration objectives that \(_{i,j,S,v}(h,(x,y))=0.5+0.5 i 1[h(x) v,x S](h(x)_{j}- _{y,j})\). Since \(q^{(t)}_{i,j,S,v}\) is the probability \(q^{(t)}\) assigns to \(_{i,j,S,v}()\) in (4), we have that \(q^{(t)}(h^{(t)},(x,y))=f^{(t)}_{h^{(t)},x}(h^{(t)}(x)-_{y})\). Therefore, (6) implies that \(_{t=1}^{T}_{D,q^{(t)}}(h^{(t)}) 0.5T+2\). It is left to establish that the min-max baseline of these losses is indeed at least \(0.5\). This is implied by Fact 2.5 and its proof is deferred to Appendix C.2. Thus \(}_{}(h^{(1:T)},\{_{D,q^{(t)}} \}^{(1:T)}) 2\). 

Our second theorem proves the existence of a distribution-free best-response algorithm for the learner, which is key for obtaining online multicalibration guarantees (via BRNR). Note that, as a distribution-free algorithm, it requires no samples to compute these best-responses.

**Theorem 3.8**.: _Consider the set of \(k\)-class predictors \(\). Fix an \(>0\) and let \(q(_{})\) be a mixture of multicalibration objectives (3). There always exists a (non-deterministic) predictor \(p()\) that is a distribution-free \(\)-best-response (7) to the stochastic cost function \(q(,(x,y))\)._

Proof Sketch.: At a high level, this statement and proof are similar to the min-max proof of calibration , with additional details in Appendix C.2. Let \(()\) be a finite \(\)-covering of \(()\) and \(_{x}:(())() [-1,1]\) the bilinear function \(_{x}(a,b)=}_{(i,j,S,v) q, a} [i 1[ v,x s](_{j}-b_{j})]\). Consider \(p(x)=}_{a(())} _{b()}_{x}(a,b)\). By the minmax theorem, \(_{b()}_{x}(p(x),b)=_{b()} _{a()}_{x}(a,b)_{b( )}_{a()}_{x}(a,b)+\). Thus, \(_{y}q(p,(x,y)) 0.5+_{y} _{h^{*}}q(h^{*},(x,y))\) for all \(x\). 

## 4 Multicalibration with Game Dynamics

We match and improve a broad set of previous results in multicalibration (See Table 1) that had received individualized and ad hoc treatments in the past. Our work establishes that not only is there a unified approach for obtaining these results but that it all comes back to game dynamics empowered by our no-regret and distribution-free-best response results for multicalibration--Theorems 3.7 and 3.8. Below we focus on three main results highlighting NRNR, NRBR, and BRNR dynamics. We defer the proofs, formal statement of algorithms, and additional results to Appendix D and E.

Multicalibration.Our first algorithm uses no-regret no-regret (NRNR) dynamics to find non-deterministic multicalibrated predictors. This algorithm matches the fastest known sample complexity rates for multicalibration  of order \(O((||.^{k})/^{2})\). It also improves upon existing fast-rate algorithms of Gupta et al. , Noarov et al.  by producing predictors with a succinct support and small circuit size. These properties were previously only known to be attained by the less sample efficient multicalibration algorithms of . In this way, Algorithm 1 simultaneously attains the best aspects of the algorithms of  and .

**Theorem 4.1**.: _Fix \(>0\), \(,k_{+}\), set of groups \( 2^{}\), and data distribution \(D\). The below algorithm, with probability \(1-\), returns a non-deterministic \(k\)-class predictor that is \((,,)\)-multicalibrated on \(D\) and takes no more than \(O(^{-2}((||/)+k()))\) samples from \(D\)._

No-Regret vs No-Regret

_Construct the problem \((\{D\},_{},)\) from Fact 2.5 and let \(T=C^{-2}(||.^{k}/)\) for some universal constant \(C\). Over \(T\) rounds, have an adversary choose \(q_{(}^{1:T)}(_{})\) by applying Hedge to the costs \(\{1-_{()}(h^{(t)},(x^{(t)},y^{(t)}))\}^{(1:T)}\) where \((x^{(t)},y^{(t)})^{(1:)} D\). In parallel, have a a learner choose predictors \(h^{(1:T)}\) by applying the no-regret learning algorithm of Theorem 3.7 to the stochastic costs \(^{(1:T)}\), where \(^{(t)}}}{{}}q^{(t)}\). Return the predictor \(p=(h^{(1:T)})\). This algorithm is written explicitly in Algorithm 1._

Theorem 4.1--along with all other results in this section--can be rewritten with \(()(1/)\) replacing \((||)\); this is done by taking a cover of \(\). We further note that Algorithm 1 can be instantiated with different choices of no-regret algorithms for the adversary and different versions of Theorem 3.7 for the learner. In Section 6, we empirically compare such variants of Algorithm 1.

Our second algorithm uses no-regret best-response (NRBR) dynamics to find deterministic multicalibrated predictors. This algorithm improves on Gopalan et al. 's oracle complexity of \(O(k/^{2})\) with \(O((k)/^{2})\), an exponential reduction of the dependence on the number of classes \(k\).4

**Theorem 4.2**.: _Fix \(>0\), \(,k_{+}\), a set of groups \( 2^{}\), and a data distribution \(D\). The following algorithm returns a deterministic \(k\)-class predictor that is \((,,)\)-multicalibrated on \(D\) and makes \(O((k)/^{2})\) calls to an agnostic learning oracle. Moreover, with probability \(1-\), the oracle calls can be implemented with \((}((k||/)+k()))\) samples from \(D\)._

No-Regret vs Best-Response

_Construct the problem \((\{D\},_{},)\) from Fact 2.5 and let \(T=C^{-2}(||.^{k})\) for some universal constant \(C\). Over \(T\) rounds, have a learner choose predictors \(h^{(1:T)}\) by applying the no-regret learning algorithm of Theorem 3.7 to the stochastic costs \(^{(1:T)}\). Have an adversary choose \(^{(1:T)}\) by calling an agnostic learning oracle at each \(t[T]\): \(^{(t)}=_{/8}(1-_{D,()}(h^{(t)}))\). Using \(C(T/)/^{2}\) samples from \(D\), return the predictor \(h^{(t^{*})}\) with the lowest empirical multicalibration error. This algorithm is written explicitly in Algorithm 2._

Online multicalibration.Our next algorithm uses best-response no-regret (BRNR) dynamics for online multicalibration, matching the best known regret bounds for online multicalibration . As with Theorem 4.1, our analysis simplifies that of  by avoiding exponential potential arguments in favor of no-regret dynamics. We state the following result for binary classification, where \(=\{0,1\}\) but note it trivially extends to multi-class settings. For convenience, we will say that predictors \(\) for binary classification output real-valued \(h(x)\), where \(h(x)\) is the predicted probability of class 1 and \(1-h(x)\) is the predicted probability of class 0.

**Theorem 4.3**.: _Fix \(>0\), \(_{+}\), and a set of groups \( 2^{}\). The following algorithm guarantees \((,,)\)-online multicalibration with probability \(1-\)._

Best-Response vs No-Regret

_Construct the online multi-objective learning problem \((,_{},)\) in Fact 2.6 and let \(T=C^{-2}(||.)\) for some universal constant \(C\). Over \(T\) rounds, have an adversary choose \(q^{(1:T)}(_{})\) by applying Hedge to the costs \(\{1-_{()}(p^{(t)},(x^{(t)},y^{(t)}))\}^{(1:T)}\), where \((x^{(t)},y^{(t)})D^{(t)}\). Have a learner best-respond to each stochastic cost \(q^{(t)}\) with the \((/2)\)-distribution-free best-response \(p^{(t)}()\) of Theorem 3.8. This algorithm is written explicitly in Algorithm 3._

The high-probability condition of Theorem 4.3 can be removed if we assume nature presents datapoints rather than data distributions, as is assumed in prior works. Interestingly, Algorithm 3's use of best-response no-regret dynamics exactly recovers the online multicalibration algorithm of [18; 35]. The analysis of Theorem 4.3 is, however, significantly simpler because we make explicit the role of the no-regret dynamics, whereas [18; 35] use potential arguments that ultimately prove no-regret dynamics and the multiplicative weights algorithm from scratch.

Additional results.We can also use game dynamics to improve guarantees for various other multicalibration settings and considerations. We can use Algorithms 1 and 2 to achieve domain-independent sample complexity rates for agnostic multicalibration (Theorem E.9), improving upon Shabat et al. 's uniform convergence guarantees. Plugging in an online learning algorithm with second-order regret bounds allows us to achieve the first non-trivial sample complexity guarantees for conditional multicalibration in the batch setting (Theorem E.3) and a multicalibration algorithm that--with a minor less than cube-root increase in sample complexity--guarantees an error tolerance for each group that scales with the square-root of the group's probability mass (Theorem 4.4). We highlight the latter result in the following theorem.

**Theorem 4.4**.: _Fix \(>0\), \(,k_{+}\), two sets of groups \( 2^{}\), and a data distribution \(D\). There is an algorithm that, with probability at least \(1-\), takes \(O((k)((||/)+k())/ ^{4})\) samples from \(D\) and returns a deterministic \(k\)-class predictor \(h\) satisfying_

\[|}[(h(x)_{j}-_{y,j})  1[h(x) v,x S]]|,\]

_for all \(S,v V_{}^{k},j[k]\)._

Plugging in an online learning algorithm with strongly adaptive regret bounds allows us to parallelize the moment multicalibration algorithm of Jung et al.  (Theorem E.13).

## 5 Other Fairness Notions

This general framework of approaching multi-objective learning with game dynamics can be extended beyond multicalibration to derive results on multi-group learning. Recall that, in agnostic multi-objective learning, the trade-off between objectives is arbitrated by the worst-off objective, which can be suboptimal when some objectives are inherently more difficult. An alternative is to, given a problem \((,,)\), define a competitor class \(^{}\) and try to learn a hypothesis \(h\) so that there is no objective that a competitor performs significantly better on, i.e. \(_{^{}}^{*}(h)-_{h^{*}}_{^{}}^{*}(h^{*})\) where \(_{^{}}^{*}(h)_{D}_{ }_{D,}(h)-_{h^{*}^{}} _{D,}(h^{*})\). We refer to such a solution \(h\) as being \(\)-competitive. A simple modification to multi-objective learning can find such a solution: replace the objectives \(\) with new objectives \(^{}\{(1+()-(h^{}) ),h^{}^{}\}\) and solve as usual. Since the sample complexity of multi-objective learning is \(O(^{-2}((||)+||( ||||/)))\), we have the below sample complexity bound on finding \(\)-competitive solutions.

**Theorem 5.1**.: _Consider a multi-objective learning problem \((,,)\) and competitor class \(^{}\). There is an algorithm that takes \(O(^{-2}((||)+||( |||^{}|| |/)))\) samples and with probability \(1-\) returns a solution \(p()\) that is \(\)-competitive against \(^{}\)._

Consider the _multi-group learning_ problem, where we seek to simultaneously minimize a general loss function on different subsets of the domain .

**Definition 5.2**.: _Fix \(>0\), a set of groups \( 2^{}\), a hypothesis class \(\) and a loss \(:()\). An \(\)-optimal solution to the multi-group learning problem \((,)\) is a randomized hypothesis \(p()\) that satisfies, for all \(S\), \([(p,(x,y)) 1[x S]]_{h^{*}} [(h^{*},(x,y)) 1[x S]]+\). We always assume such a hypothesis \(p\) exists in class \(\)._

A (near) optimal sample complexity for multi-group learning of \(O((||||)/^{2})\) was attained by  using a reduction to sleeping experts.  also asked whether there exists a simpler optimal algorithm that does not rely on sleeping experts. We answer this affirmatively by designing an optimal algorithm that just runs two Hedge algorithms. The following is a direct implication of Lemma 3.3 and the observation that multi-group learning directly reduces to learning an \(\)-competitive solution.

**Theorem 5.3**.: _Fix a set of groups \( 2^{}\), loss \(:\), hypothesis class \(:\), and distribution \(D\). There is a no-regret no-regret algorithm that takes \(2T=O((||||)/^{ 2})\) samples from \(D\) and returns an \(\)-optimal solution to the multi-group learning problem \((,)\)._

## 6 Empirical Results

In this section, we study the empirical performance of multicalibration algorithms on the UCI Adult Income dataset , a real-world dataset for predicting individuals' incomes based on the US Census. We defer additional results, datasets, and methods to the Appendix G.

Experiment setup.This experiment, summarized in Table 2, aims to learn a predictor on the UCI dataset that predicts the 'income' label and is multicalibrated on the values of eight other labeled attributes including 'age'. We discretize with \(0.1\)-width bins (\(=10\)), and perform random 80-20 train/test splits of the dataset. This results in approximately 24000 training samples, 6000 test samples, and 130 groups. Each multicalibration algorithm is evaluated on 10 seeds, each after 50 iterations, with performance measured by their average iterate's and last iterate's multicalibration violations.

We study four multicalibration algorithms based on no-regret best-response dynamics, which use an empirical risk minimizer as the adversary and implements either Hedge , Prod , Optimistic Hedge (OptHedge) , or Gradient Descent (GD) as the learner. We study two algorithms based on no-regret no-regret dynamics, which plays against itself either Hedge (Hedge-Hedge) or Optimistic Hedge (OptHedge-OptHedge). Learning rate decay is tuned on the training set by sweeping over \([0.8,0.85,0.9,0.95]\) for the learner and \([0.9,0.95,0.98,0.99]\) for the adversary.

**One's choice of no-regret algorithm matters.** The original multicalibration algorithm of , which is based on gradient descent, consistently attains the worst multicalibration errors, both in terms of average-iterate and last-iterate. This is consistent with gradient descent being a theoretically less effective no-regret algorithm, due to instability near the boundaries of a probability simplex. Due to the superficial similarity between boosting and multicalibration, the field has already begun adopting multicalibration algorithms with Hedge's multiplicative updates rather than gradient descent's additive ones . Our findings offer the first theoretical and empirical endorsement of this shift.

**The last iterates of no-regret no-regret dynamics are surprisingly multicalibrated.** The algorithms based on no-regret no-regret dynamics, namely Hedge-Hedge and OptHedge-OptHedge, consistently yield not only among the most multicalibrated randomized predictors (with their average iterate) but also the most multicalibrated deterministic predictors (with their last iterate). Note that these algorithms only enjoy a theoretical advantage over no-regret best-response algorithms in terms of average iterate guarantees. This does not appear to be an artifact of early stopping or learning rates (Figure 1), but may rather indicate that their more stable adversary updates provide regularization.

Acknowledgements.This work was supported in part by the National Science Foundation under grant CCF-2145898, a C3.AI Digital Transformation Institute grant, and the Mathematical Data Science program of the Office of Naval Research. This work was partially done while Haghtalab and Zhao were visitors at the Simons Institute for the Theory of Computing.

   Algorithm & Train Error & Test Error & Test Error (Ergodic) \\  Hedge-Hedge (NRNR) & 2.0e-2 \(\) 2.0e-3 & **3.0e-2**\(\) 3.0e-3 & **2.3e-4**\(\) 2.7e-5 \\ OptHedge-OptHedge (NRNR) & 7.0e-3 \(\) 0.0 & **2.7e-2**\(\) 3.0e-3 & 2.6e-4 \(\) 2.8e-5 \\ OptHedge-ER (NRBR) & **0.0**\(\) 0.0 & 4.7e-2 \(\) 1.0e-3 & 4.8e-4 \(\) 9.0e-6 \\ Hedge-ERM (NRBR) & **0.0**\(\) 0.0 & 6.4e-2 \(\) 1.0e-3 & 6.4e-4 \(\) 1.1e-5 \\ Prod-ERM (NRBR) & **0.0**\(\) 0.0 & 5.3e-2 \(\) 4.0e-3 & 5.3e-4 \(\) 4.4e-5 \\ GD-ERM (NRBR) & 5.3e-2 \(\) 1.1e-2 & 8.3e-2 \(\) 3.0e-3 & 9.5e-4 \(\) 6.5e-5 \\   

Table 2: Average (\(\) standard error) of multicalibration violations on UCI Adult Dataset. _Train Error_ and _Test Error_ evaluate the last iterate (deterministic predictor) on training and test splits; _Test Error (Ergodic)_ measures the average iterate (non-deterministic predictor) on test split. GD-ERM (NRBR) is worst and OptHedge-OptHedge (NRNR) is best on both deterministic and last iterates.