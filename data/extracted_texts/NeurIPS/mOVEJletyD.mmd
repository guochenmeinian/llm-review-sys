# Slimmed Asymmetrical Contrastive Learning and Cross Distillation for Lightweight Model Training

Jian Meng, Li Yang, Kyungmin Lee, Jinwoo Shin, Deliang Fan, and Jae-sun Seo\({}^{*}\)

Cornell Tech, USA

\({}^{}\)University of North Carolina at Charlotte, USA

\({}^{}\)KAIST, South Korea

John Hopkins University, USA

{kyungmnlee, jinwoos}@kaist.ac.kr\({}^{}\) lyang500@uncc.edu\({}^{}\)

dfan10@jhu.edu\({}^{@sectionsign}\) {jm2787, js3528}@cornell.edu\({}^{*}\)

###### Abstract

Contrastive learning (CL) has been widely investigated with various learning mechanisms and achieves strong capability in learning representations of data in a self-supervised manner using unlabeled data. A common fashion of contrastive learning on this line is employing large-sized encoders to achieve comparable performance as the supervised learning counterpart. Despite the success of the labelless training, current contrastive learning algorithms _failed_ to achieve good performance with lightweight (compact) models, e.g., MobileNet, while the requirements of the heavy encoders impede the energy-efficient computation, especially for resource-constrained AI applications. Motivated by this, we propose a new self-supervised CL scheme, named SACL-XD, consisting of two technical components, **S**limmed **A**symmetrical **C**ontrastive **L**earning (**S**ACL) and **Cross-**D**istillation (XD), which collectively enable efficient CL with compact models. While relevant prior works employed a strong pre-trained model as the teacher of unsupervised knowledge distillation to a lightweight encoder, our proposed method trains CL models from scratch and outperforms them even without such an expensive requirement. Compared to the SoTA lightweight CL training (distillation) algorithms, SACL-XD achieves 1.79% ImageNet-1K accuracy improvement on MobileNet-V3 with 64\(\) training FLOPs reduction. Code is available at https://github.com/mengjian0502/SACL-XD.

## 1 Introduction

To overcome the labeling bottleneck for supervised training of deep neural networks (DNNs), self-supervised learning has been widely investigated to learn representations without intensive labeling. In particular, contrastive learning (CL) has demonstrated its capability of representation learning in various machine learning domains, e.g., image [7; 34], video , language , speech , and medical imaging . The success of CL is built upon different data augmentations from the original (training) samples, and the representation is learned by maximizing the latent common knowledge between contrastive embeddings [8; 7; 35], which are separately encoded from the augmented images by DNN models . Despite the various contrastive learning techniques, learning the latent knowledge and representations requires wide and deep encoders. In particular, current SoTA CL algorithms [3; 18] have to employ a large-sized encoder (e.g., ResNet-50) to achieve comparable performance as the supervised learning counterpart.

On the other hand, training the lightweight models [23; 22; 29] from scratch is largely under-explored in CL. Almost none of the prior works have reported the CL performance with directly-trained lightweight models. In fact, the prior success of the lightweight models in supervised learning _cannot_ be smoothly transferred to CL. where the performance gap between the lightweight models and large-sized models is largely amplified under recent CL methods [7; 19], For example, withsupervised learning, MobileNet-V3Large (Mob-V3)  can achieve 74.04% on ImageNet-1K, which is comparable to ResNet-50 (76.15%). However, the \(\)2% accuracy difference is amplified to \(>\)30% (75.2% \(\) 36.3%) in CL, as reported in  with MoCo-V2. Training MobileNet-V3 with more recent Barlow Twins  improves the accuracy to \(\)52%, but this is still unsatisfactory.

To overcome the limited trainability of the lightweight DNN models in CL, contrastive lightweight model learning has been investigated as an unsupervised knowledge distillation task. For example, SEED  divides the entire training process into a two-step process of (1) teacher pre-training with CL and (2) unsupervised knowledge distillation from the frozen teacher to the student lightweight encoder (e.g., MobileNet-V3 ). In addition to the two-step process of "pretraining-and-tuning", the authors employ different input schemes between two steps, which further elevates the complexity of the entire training process. On the other hand, ReKD  introduces the latent relation knowledge into online distillation with a single-step training process. However, the large-sized encoder is still required as the online teacher. To that end, both ReKD  and SEED  focus on unsupervised distillation with the large-sized teacher, which actually amplifies the training cost for lightweight CL, as shown in Fig. 1. Meanwhile, the performance of training lightweight models with CL from scratch remains unsatisfactory  or requires dedicated input scaling design with low generality  on normal CL. This is quite meaningful to explore since training a high-performance compact encoder (\(\) 5 M parameters) [22; 23; 29] by necessitating a large-sized ResNet (\(>\)20 M)  teacher largely degrades the "time-to-deploy" of the model due to the magnified training cost. Given the challenges of CL and the limitations of prior works, we raise the following question as our motivation:

_Is there a contrastive learning algorithm that can train the high-performance lightweight model without using a large-sized teacher?_

To answer this question, we propose **S**limmed **A**symmetrical **C**ontrastive **L**earning (SACL) together with **C**ross-**D**istillation (XD), a novel self-supervised contrastive learning algorithm designed for efficient and high-performance CL with lightweight encoders. Specifically, SACL considers lightweight contrastive learning as an asymmetrical sparse training problem based on the shared weights from a dense host encoder. A lightweight encoder can be treated as a "subset" model (sub-model) sliced from a wide host model, which naturally formulates the asymmetrical encoding in CL. Different from the knowledge distillation-based methods [16; 36], the proposed SACL-XD algorithm completely eliminates the large-sized ResNet teacher from the lightweight contrastive learning. On top of that, we introduce cross-distillation (XD) to facilitate SACL. XD minimizes the decorrelation between the latent information distorted by asymmetrical CL, elevating the training stability and performance. As shown in Fig. 1, the proposed method achieves **64\(\)** and **8.3\(\)** training cost reduction along with **1.79%** and **2.09%** ImageNet accuracy improvements, compared to SOTA  and , respectively. The major contributions of the proposed SACL-XD algorithm are:

1. **Simplicity:** SACL-XD does not require teacher pre-training (and input scaling) before training a lightweight student encoder.
2. **Efficiency:** SACL-XD does not require the large-sized teacher during training. The training cost of MobileNet-V3  model is reduced by 64\(\) and 8.3\(\) compared to [36; 16].
3. **Performance and generality:** SACL-XD achieves up to 2.09% ImageNet-1K accuracy improvement compared to the previous SoTA method. In particular, the proposed cross-distillation (XD) algorithm _solely_ achieves superior performance with normal contrastive learning settings on ResNet-50 without SACL.
4. **Transferability:** The SACL-XD-trained lightweight encoder shows high transferability to the downstream tasks. With minimum fine-tuning on top of the ImageNet-1K-trained

Figure 1: ImageNet-1K accuracy of the proposed method (SACL+XD) vs. state-of-the-art methods. “RN” represents the ResNet teacher of the prior works [16; 36]. Following the settings in [36; 16], the number of FLOPs is computed based on 200 epochs of training.

MobileNet-V3, ours achieves **94.80%** accuracy on CIFAR-10, outperforms the supervised learning baseline and SEED  with **1.83%** and **14.80%** improvements, respectively.

## 2 Related Work

**Self-supervised contrastive learning.** With the absence of deterministic labels, the major objective of contrastive learning is minimizing the distance between the embeddings separately encoded from the augmented input samples. Early research works [7; 19] define "positive" and "negative" sample pairs, and the learning process maximizes the similarity between positives and repels the negative samples. The popular InfoNCE loss [26; 19; 8] or NT-Xent loss  has been proposed as the learning objective. The wide and deep DNN encoder is also the key factor of success in contrastive learning. In addition to the cross-reference between positive and negative pairs, contrastive learning is also considered a DNN-based clustering problem, where the samples and their embeddings are grouped into clusters based on similarity metrics [4; 32; 1]. SwAV  introduces online cluster learning with the reduced complexity of computation. The entropy-based similarity matching and clustering CL algorithms shares the same nature, but they all require an extensive amount of negative samples to generate salient contrastiveness.

Recent methods consider contrastive learning as a knowledge distillation  task between the contrastive encoders with separately augmented inputs. BYOL  and SimSiam  consider the student network as the online model, while the teacher is consistently staying offline with no gradient propagation . The teacher is updated as the moving averaged weights of the student encoder. The "student-teacher" relationship focuses on the distance minimization between latent information. Another perspective is exploring the content hidden inside the embeddings. Barlow Twins  pushes the cross-correlation between the encoded embeddings toward the identity matrix. VICReg  collectively optimizes the variance, covariance, and distance between and inside the embeddings to achieve better training stability and even enables the CL with different encoders. Among all the previous CL algorithms, the empirical findings show the improved training stability and performance of CL together with the relaxed requirement of batch sizes, but the large-sized encoder (e,g,., ResNet-50 (1\(\), 2\(\),...) ) is a persistent and almost mandatory requirement to achieve high performance, which hinders the development of CL with high energy consumption and extensive training effort.

**Contrastive learning with lightweight models.** Under the context of supervised learning, various lightweight architecture designs [23; 22; 29] are proposed to minimize the performance difference caused by the model size gap. The intricate architecture and the compact model sizes enable energy-efficient and hardware-friendly computer vision applications. However, the architectural efficiency of the lightweight models becomes invalid in the contrastive learning domain. Employing a lightweight encoder in CL leads to poor performance due to the insufficient trainability caused by the limited model sizes. To resolve this challenge, the mainstream method is introducing a giant model as the teacher of the lightweight student encoder. SEED  uses a pre-trained large-sized ResNet  as the frozen teacher of the lightweight student model. The similarity-based teacher distribution is generated as the target soft label for student learning. In particular, accurate distillation requires both teacher and student to encode the same input, which is inconsistent with the augmentation strategy of CL. As a result, self-supervised training of the lightweight model becomes a two-step process. However, the dependence on the large-sized teacher and the expensive pre-training complicates the overall learning process. ReKD  incorporates the online distillation during learning with the relation knowledge-based loss between embeddings. DisCo  complicates the training cost of the distillation even further. The inputs are encoded separately by the teacher and _two_ separate students. Regardless of using an online or frozen teacher, the large-sized ResNet model is always required to train a lightweight encoder. The recent efforts on directly-trained lightweight model requires fine-grained efforts on input scaling , but the improvements on lightweight model has low generality and has **no** improvements on the popular ResNet-50-based CL.

## 3 Method

To resolve the training dilemma between contrastive learning and lightweight models, this section introduces 1) SACL: Slimmed Asymmetrical Contrastive Learning and 2) XD: Cross Distillation to achieve lightweight contrastive learning from scratch.

Similar to prior contrastive learning (CL) algorithms [35; 3; 7], our method performs contrastive learning based on the dual augmentation that is transformed from the clean input image. Given a clean input batch \(X\) sampled from the dataset \(\), the distorted views are generated from a combination of data augmentations \(\), leading to the augmented pairs \(X_{A}\) and \(X_{B}\), which are fed into the encoders for different contrastive paths. In this work, we follow the recent contrastive learning algorithms [35; 3] and use one shared encoder \(f_{}\) for different contrastive branches, as shown in Fig. 2(a). The encoded outputs are fed into the subsequent projector \(h_{}\), resulting in the latent embeddings \(z_{A}\) and \(z_{B}\).

### Slimmed Asymmetric Contrastive Learning (SACL)

Under the standard supervised training, it has been well evidenced that extracting a subset lightweight model out of a wider/larger (e.g., \(2\)) encoder often leads to better performance compared to directly train such a lightweight model from scratch . We are inspired by the fact that a lightweight model can be considered as a subset model "sliced" from the original full-sized host model. The asymmetrical relationship between the slimmed model and the original model naturally fits the independent encoding path of contrastive learning.

We propose Slimmed Asymmetric Contrastive Learning (SACL), for obtaining lightweight models under self-supervised CL training. SACL treats the lightweight model contrastive learning as a sparse training optimization problem based on shared weights from a large-sized encoder. During the forward pass of each iteration, the augmented input pair (\(X^{A}\), \(X^{B}\)) are separately encoded by the dense host model (\(f_{}\)) and the slimmed (\(f_{}^{s}\)) encoders with the disabled weight filters (i.e., output channels), as shown in Fig. 2(b).

The host encoder model \(f_{}\) is slimmed by removing a _unified_ amount of weight filters (output channels) of each layer based on weight magnitude score. With the slimmed and dense model \(f_{}^{s}\) and \(f_{}\), the relationship between \(^{s}\) and \(\) is defined as:

\[_{s}_{s}=\] (1)

Where \(\) is the weight mask that disables the channels of \(f_{}\), and the subset model \(f_{}^{s}\) is selected when \(\) is enabled. As shown in Fig. 2(b), \(f_{}\) and \(f_{}^{s}\) separately encode the input pair in different branches. With the shared weights, the mask \(\) is alternatively enabled between branches to formulate the asymmetrical encoding during the forward propagation. The slimming ratio (sparsity) of \(\) is defined by the _Slimmed Asymmetry_ (SA) between \(f_{}\) and \(f_{}^{s}\) with the style of "K\(\)-\(1\)". "K " is the width of the wide host model (\(f_{}\)) that is employed to slice the \(1\) model (\(f_{}^{s}\)) out of it:

\[s=1-1/=(_{s})/()\] (2)

where \(()\) returns the number of nonzero elements of the tensor, \(s\) is the desired slimmed ratio (sparsity) controlled by K.

Starting from the initialization of the training, SACL uniformly generates the masks to slice out the weight filters (output channels) with the least magnitude score from each layer. The masks will be

Figure 2: Overview of (a) contrastive learning with shared encoder, (b) Proposed Slimmed Contrastive Learning (SACL) algorithm and (c) Cross-Distillation-aided SACL.

updated after each epoch to maintain the performance of the slimmed model based on the filter-wise \(L_{1}\) norm score. The resultant slimmed (1\(\)) model will be deployed as the final trained encoder. Specifically, SACL drives contrastive learning with the following properties:

1. SACL holds a consistent channel-wise architecture difference (e.g., 1.5\(\) vs. 1\(\)) between contrastive branches throughout the entire training process.
2. SACL removes a unified amount of channels (with the lowest magnitude score) of all the layers. By doing so, the resultant slimmed model will have the exact width as the target lightweight model.

Formally, given the augmented input pair (\(X_{A}\), \(X_{B}\)), the forward pass is characterized as:

\[z_{A}=h_{}(f_{}(X_{A})), z_{B}=h_{}(f_{}^{s}(X_{B})).\] (3)

And the optimization target of SACL is:

\[_{,_{s}}_{}(z_{A},z_{B}),_{s}(_{s})}{()}=s,\] (4)

Inside each mini-batch, the gradient is collectively computed all at once, and the optimizer will update the whole set parameter \(\) of the dense host encoder \(f_{}\). The slimmed model architecture (\(f_{}^{s}\)) will be updated after every epoch based on the magnitude score of each filter. The model is completely online and stop gradient is excluded from learning. Minimizing the contrastive loss \(_{}\) between \(z_{A}\) and \(z_{B}\) is equivalent to overcoming the distortion caused by 1) data augmentation \(\) and 2) consistent and structural architecture difference between \(f_{}^{s}\) and \(f_{}\). And the architecture asymmetry caused by 2) motivates us to explore the enhancement of SACL from the perspective of knowledge distillation, which is presented in the following section.

### Cross Distillation (XD) on top of SACL

We propose Cross-Distillation (XD) on top of SACL, which treats the teacher-student relationship as an interconnected knowledge distillation with the correlation-based optimization on top of the proposed SACL learning scheme. Given the asymmetrical contrastive encoders \(f_{}\) and \(f_{}^{s}\), we first encode \(X^{A}\) and \(X^{B}\) based on SACL (\(X^{A} f_{}\); \(X^{B} f_{}^{s}\)), leading to the embeddings \(z^{A}\) and \(z^{B}\). Subsequently, we freeze both \(f_{}\) and \(f_{}^{s}\) while **reversing** the order of the inputs for encoding (\(X^{B}[f_{}][_{B}]\); \(X^{A}[f_{}^{s}][_{A}]\)), and characterize the resultant embeddings as \([_{A}]\) and \([_{B}]\), where "[\(\)]" represents the frozen encoder for the forward pass only.

As a result, each forward pass will generate two pairs of latent vectors resulting from two groups of SACL forward pass. We first compute the online contrastive loss \(_{}\) based on \(z_{A}\) and \(z_{B}\). Such online embeddings contain the distortion caused by _both_ data augmentations and asymmetrical encoders. We empirically find out that directly optimizing such high-sparsity difference via single contrastive loss leads to _collapsed_ training. Motivated by that, we compute the cross-distillation loss \(_{}}}\) as the average loss between the pair of \((z_{A},_{A})\) and \((z_{B},_{B})\):

\[_{}}}=_{}}}^{A}( z_{A},_{A})+_{}}}^{B}(z_{B},_{B})}{2},\] (5)

where \(_{}}}^{A},_{}}}^{B}\) will be defined formally later (see Eq. 9). As shown in Fig. 2, each term of the cross-distillation loss is computed between the asymmetrical SACL encoders with the same input. In other words, optimizing the cross-distillation loss is equivalent to minimizing the distortion in embeddings caused by the asymmetrical sparsity only. We define the total loss of the training as the weighted sum between \(_{}\) and \(_{}}}\) and weight is defined as \(\):

\[=_{}}}+(1-)_{ }}},\] (6)

Where \(n\) represents the index of batch, and \(i\) and \(j\) represent the dimensionality indices across the latent output. Correspondingly, \(C_{i,j}\) represents the \(i,j\) element of the correlation matrix.

Regarding contrastive loss design, recent works treat contrastive learning as the optimization problem of principle correlation maximization  or the decorrelation minimization [35; 3] of the encoded latent vectors. The cross-correlation between \(z_{A}\) and \(z_{B}\) is computed as:

\[C_{ij}=z_{n,i}^{A}z_{n,j}^{B}}{(z_{n,i}^{A})^{2}} (z_{n,j}^{B})^{2}}}\] (7)

[MISSING_PAGE_FAIL:6]

## 4 Experimental Results

In this section, we evaluate the performance of the proposed algorithm based on CNN encoders (MobileNet [23; 22], EfficientNet , ResNet ) and ViT  models on the ImageNet-1K and ImageNet-100 dataset. We also demonstrate the capability of the proposed method with tiny-sized ResNet on the small CIFAR dataset. We also evaluate the transferability of the lightweight model on both CIFAR classification and VOC object detection downstream tasks. We characterize the asymmetry of SACL with the style of "K\(\)-1\(\)", where "K " is the width of the wide host model that is employed to slice the 1\(\) out of it. All the models are directly trained from scratch. The detailed experiment setup and hyperparameter settings are summarized in the Appendix.

### Training from Scratch with SACL+XD on Lightweight CNNs

We follow the linear evaluation protocol on ImageNet to evaluate the performance of the backbone trained by the proposed SACL and cross-distillation (XD) algorithm. We train the compact models from scratch for 100 or 200 epochs, which is the same amount of fine-tuning effort as SEED  and ReKD . The proposed algorithm is evaluated on multiple lightweight encoders, including MobileNet-V1 , MobileNet-V3-Large , and EfficientNet-B0 . Table 2 compares the top-1 linear evaluation accuracy of our work against recent SoTA works for compact model training.

With the same 200 epochs, the XD-trained MobileNet-V3 (1\(\)) model outperforms the recent ResNet-50-aided ReKD  by a noticeable 2.72% accuracy improvements (59.42% vs. 56.70%) with 9.3\(\) less training FLOPs. Furthermore, XD achieves 57.16% ImageNet accuracy with only 100 epochs training on MobileNet-V3, surpasses the ResNet-50-aided ReKD  and SEED  by 0.46% and 1.96% with 18.6\(\) and 142\(\) training cost reduction, respectively. Combined with the proposed asymmetrical slimmable contrastive learning (SACL), our method achieves 2.12% and 2.09% linear evaluation accuracy improvements compared to the ReKD-trained EfficientNetB0 and MobileNet-V3, respectively. Meanwhile, our method eliminates the ResNet-101 teacher from training, which leads to 5.2\(\) and 8.3\(\) training effort reduction compared to ReKD . We are aware the wider model introduces a higher computation budget, so we choose the 1.5\(\)-1\(\) asymmetrical architecture, and the linear evaluation is performed on the slimmed encoder with the same size as the vanilla 1\(\) model.

  
**Method** & **Encoder** & **Linear Eval. (\%)** & **Epochs** & **Pre-train** & **Teacher** & **Training FLOPs (e+17)** \\  \({}^{}\)**SACL-XD (Ours)** & Eff-B0 (1.5\(\)-1\(\)) & **65.32 (\(\)2.12)** & 200 & ✗ & - & **24 (2.9\(\))** \\  \({}^{}\)**SACL-XD (Ours)** & Mob-V3 (1.5\(\)-1\(\)) & **61.69 (\(\)1.79)** & 200 & ✗ & - & **15 (64.7\(\))** \\ 
**SACL-XD (Ours)** & Mob-V1 (1.5\(\)-1\(\)) & **59.34** & 200 & ✗ & - & **19** \\ 
**XD only (Ours)** & Mob-V3 (1\(\)) & **59.42** & 200 & ✗ & - & **7.2** \\ 
**XD only (Ours)** & Mob-V3 (1\(\)) & **57.16** & **100** & ✗ & - & **3.6** \\ 
**XD only (Ours)** & Mob-V1 (1\(\)) & **55.84** & **100** & ✗ & - & **9.0** \\  \({}^{}\)SSL-Small  & Mob-V3 (1\(\)) & 48.70 & 200 & 2 epochs & - & 19 \\  \({}^{}\)SSL-Small  & Eff-B0 (1\(\)) & 55.90 & 200 & 2 epochs & - & 34 \\  ReKD  & Mob-V3 (1\(\)) & 56.70 & 200 & ✗ & ResNet-50 & 67 \\  ReKD  & Mob-V3 (1\(\)) & 59.60 & 200 & ✗ & ResNet-101 & 125 \\  ReKD  & Eff-B0 (1\(\)) & 63.40 & 200 & ✗ & ResNet-50 & 70 \\  OSS  & Eff-B0 (1\(\)) & 64.10 & 800+200 & ✗ & ResNet-50 & 67 \\  \({}^{*}\)SEED  & Mob-V3 (1\(\)) & 55.20 & 800+200 & ✓ & ResNet-50 & 512 \\  \({}^{*}\)SEED  & Mob-V3 (1\(\)) & 59.90 & 800+200 & ✓ & ResNet-101 & 971 \\  \({}^{*}\)SEED  & Eff-B0 (1\(\)) & 61.30 & 800+200 & ✓ & ResNet-50 & 516 \\  \({}^{}\)MoCo-V2  & Mob-V3 (1\(\)) & 36.30 & 200 & ✗ & - & 4.8 \\  \({}^{}\)MoCo-V2  & Eff-B0 (1\(\)) & 42.20 & 200 & ✗ & - & 8.5 \\    \({}^{*}\): SEED  uses a ResNet-50 teacher which is pre-trained by 800 epochs.

\({}^{}\): Baseline linear evaluation accuracy reported by SEED .

\({}^{}\): We use ReKD  as the SOTA baseline of EfficientNetB0  to report the accuracy improvements and computation reduction.

\({}^{}\):We use SEED  as the SOTA baseline of MobileNet-V3  to report the accuracy improvements and computation reduction.

\({}^{}\):Weights are initialized based on SEED -trained model.

Table 2: ImageNet-1K test accuracy with linear evaluation protocol based on MobileNet-V3  trained by different contrastive learning/distillation methods.

### Training from Scratch with XD on ResNet and ViT

Besides the evaluation against the lightweight models, we validate the proposed XD individually by training ResNet-50 on ImageNet-1K with 300 epochs from scratch, as reported in Table 3.

Compared to the recent contrastive learning methods [35; 18], the proposed cross-distillation algorithm (XD) achieves better accuracy with 0.4% linear evaluation accuracy improvements demonstrating the generality and versatility of the proposed XD algorithm.

Besides the CNN-based encoder, the proposed cross-distillation (XD) algorithm is also capable of training the lightweight ViT encoder. Table 4 summarizes the performance of the ViT-Tiny-16-224  encoder trained by XD. Compared to Barlow Twins  and DINO , our method achieves 1.36% and 0.88% accuracy improvements. The model is directly trained from scratch, and the \(\) value of the XD is set to 0.8. Specifically, we simply replace the CNN encoder by the ViT-Tiny model and slim down the embedding dimensionality, no additional architecture has been introduced to ViT training. The superior performance on ViT indicates the high versatility of the proposed method.

### SACL-XD on CIFAR datasets with Tiny-sized ResNet

We also evaluate the performance of our method on the small-sized dataset with the tiny-sized ResNet encoders (e.g., ResNet-20 with 0.27 million parameters). We follow the data augmentation setup in  for the CIFAR-10 dataset. For the SACL, we sweep the asymmetry from 2\(\)-1\(\) up to 6\(\)-1\(\), the model is trained for 1000 epochs from scratch, and the results are summarized in Table 5.

With the "6\(\)-1\(\)" asymmetry, the equivalent weight sparsity is consistently held at 97.01% throughout the entire process. The linear evaluation is performed based on the slimmed 1\(\) model with 0.27 million non-zero weights. Our method achieves up to 7.18% accuracy improvements compared to the Barlow Twins baseline . Compared to SEED , the proposed method achieves 3.95% accuracy improvements. We are aware the contrastive learning-trained large-sized ResNet-50 encoder can achieve \(>\)96% CIFAR-10 accuracy with downstream fine-tuning [18; 35], but the recent CL algorithms exhibit poor performance with tiny encoder and small-sized training samples (compared to ImageNet-1K). The superior performance of the proposed method provides valuable insights for practical self-supervised learning on limited-sized datasets with tiny-sized encoders.

  
**Methods** & **Encoder** & **Training Epochs** & **Linear Eval Acc. (\%)** \\  Barlow Twins  & ViT-Tiny  & 400 & 62.56 \\  ‘DINO  & (\# of Param = 5.5 Million) & 400 & 63.04 \\  
**XD (Ours)** & & 400 & **63.92 (+0.88)** \\   

*: Reported DINO results from .

Table 4: ImageNet-100 test accuracy with linear evaluation protocol based on ViT-Tiny  encoder.

  
**Method** & **Training Epochs** & **Top-1 Linear Evaluation Accuracy (\%)** \\  MoCo  & 1000 & 60.6 \\  SimCLR  & 1000 & 69.3 \\  \({}^{*}\)BYOL  & 300 & 68.4 \\  \({}^{*}\)Barlow Twins  & 300 & 70.7 \\ 
**XD (Ours)** & 300 & **71.1** \\   

*: Reported results from [18; 35] with 300 epochs training from scratch.

Table 3: ImageNet-1K test accuracy with linear evaluation protocol based on ResNet-50 encoder.

  
**Method** & **Encoder** & **Linear Eval Acc. (1\(\) model)** & **Decoder** & **T****eductive Pre-trained by** & **Training FLOPs (\(\)16)** \\ 
**SNAC-XD (Ours)** & ResNet-20(\(\)1-3) & **S01(\(\)7-13)** & - & 80.00 \\ 
**SNAC-XD (Ours)** & ResNet-20(\(\)1-3) & **S01(\(\)4-11)** & - & - & 3.90 \\ 
**SNAC-XD (Ours)** & ResNet-20(\(\)1-3) & **S2.31(\(\)2-6)** & - & - & 0.98 \\ 
**SICL-10**  & ResNet-20(\(\)1-3) & 82.66 & ResNet-20(\(\)1-3) & MoCo  & 110 \\ 
**SICL-10**  & ResNet-20(\(\)1-3) & 81.36 & ResNet-20(\(\)1-3) & Boalue Pointing  & 110 \\  Badue Twins  & ResNet-20(\(\)1-3) & 79.63 & - & - & 0.25 \\  VCRg2  & ResNet-20(\(\)1-3) & 79.13 & - & - & 0.25 \\   

*: Re-implementation of SEED (16) on CIFAR dataset with the official conf. The ResNet-20(\(\)0) is presented by 300 epochs.
*: Don’s instructions and hyperparameter settings are adopted from , but use  as the baseline of ResNet-20.

Table 5: CIFAR-10 linear evaluation test accuracy based on ResNet-20 trained by SACL+XD with different asymmetrical architectures.

### Ablation Study

**The impact of \(\).** We introduced the weight parameter \(\) in Eq. 6 and Eq. 10 as a tunable parameter to control the importance of the inner decorrelation loss during the training process. We further evaluate the impact of the different weighting between \(_{}\) and \(_{}\). We explore the impact of \(\) on the ImageNet-1K dataset with the MobileNet-V1 (1\(\)) model. The model is trained by XD only with 100 epochs from scratch. As shown in Fig. 4(a), the proposed method achieves the best performance when \(\)=0.9. Meanwhile, the accuracy oscillation caused by \(\) is relatively stable (within \(\) 1%).

**The impact of training effort.** We also evaluate the performance of the proposed algorithm with different training efforts from scratch. Fig. 4(b) demonstrate the linear evaluation accuracy of MobileNet-V1  on ImageNet-1K trained by different epochs. For both SACL+XD and individual XD training, the extended training effort from 100 epochs to 300 epochs leads to evident accuracy improvements. With 300 epochs of training, the proposed XD and SACL+XD method achieves 58.25% and 60.63% Top-1 linear evaluation accuracy on ImageNet-1K.

### Transfer learning to downstream tasks

We report the transfer learning performance of the MobileNet [22; 23] encoder trained by both SACL+XD and XD. For the downstream classification, we use CIFAR-10 and CIFAR-100 as our target tasks. We also validate the pre-trained lightweight encoder on the VOC2007 dataset for downstream object detection. Follow the setup in , we finetune the models for 10,000 steps with SGD and batch size of 64. The experimental setup is summarized in the Appendix. Table 6 summarizes the transfer learning performance of the proposed method compared to SEED . For the CIFAR tasks, our method achieves 7.42% and 16.30% accuracy improvements compared to SEED . Together with the 1.83% and 5.69% improvements compared to supervised learning.

### Comparison to SoTA energy-efficient supervised learning

In addition to the accuracy and training cost improvement compared to distillation-based contrastive learning, the powerful lightweight backbone model trained by the proposed method reveals a new perspective of energy-efficient inference compared to conventional supervised sparse training on

  
**Method** & **Encoder** & **CIFAR-10** & **CIFAR-100** & **Aircraft** & **Flowers** & **Food-101** & **Cars** & **Pets** \\  Supervised (from scratch) &  & 92.97 & 73.69 & 65.37 & 79.89 & 60.30 & 68.18 & 70.97 \\  Supervised (fine-tune) &  & 94.53 & 78.86 & 68.29 & 89.94 & 75.84 & 82.43 & 85.87 \\ 
**XD (Ours)** &  & **94.80** & **79.00** & **71.39** & **90.05** & 75.71 & **82.77** & **89.42** \\ 
**SACL + XD (Ours)** &  & **94.92** & **79.64** & **72.21** & **90.48** & **76.12** & **83.14** & **90.24** \\  SEED  &  & 87.5 & 63.0 & - & - & - & - & - \\   

Table 6: Transfer fine-tuning of MobileNet [23; 22] pretrained by the proposed method.

  
**Model** & **Training Method** & **CIFAR-10 Acc (\%)** & **CIFAR-100 Acc (\%)** & **\# of (remained) Param. (M)** \\  ResNet-50 & Supervised Learning & 94.75 & 78.23 & 25.6 \\  ResNet-50 & Supervised + GraNet  & 94.64 & 77.89 & 2.6 (90\% sparsity) \\  ResNet-50 & Supervised + RigL  & 94.45 & 76.50 & 2.6 (90\% sparsity) \\ 
**Mob-VI** & **SACL+XD (Ours) + Finetune** & **94.92** & **79.64** & 3.2 \\  
**Mob-VI** & **XD (Ours) + Finetune** & **94.80** & **79.00** & 3.0 \\   

Table 7: Comparison between the proposed method and other supervised high-water marks

Figure 4: MobileNet-V1 ImageNet-1K accuracy vs. (a) value of \(\) and (b) training epochs.

ResNet [24; 15]. Table 7 summarizes the CIFAR downstream comparison between the pre-trained MobileNet [29; 23] and the ResNet-50 sparsified by the recent SoTA pruning methods [24; 15]

Despite the additional fine-tuning effort, the powerful lightweight backbone pre-trained by the proposed method achieves better accuracy-model size tradeoff compared to the conventional supervised sparse learning with high element-wise sparsity. More importantly, the powerful lightweight architecture can be accelerated and deployed to the energy-constrained hardware **without** the requirement of the dedicated accelerator.

## 5 Conclusion

In this paper, we propose a novel contrastive learning (CL) algorithm designed for lightweight encoders. We first introduce the slimmed asymmetrical contrastive learning (SACL), which treats the lightweight model CL as a slimmed sparse training task with asymmetrical encoding. On top of the SACL, we propose the cross-distillation (XD) algorithm, distilling the knowledge by minimizing the decorrelation between the embeddings encoded by SACL. Compared to previous works, the proposed algorithm achieves new SOTA accuracy without introducing the large-sized teacher or expensive pre-training. Furthermore, solely training both lightweight and large-sized ResNet with the XD can still achieve superior performance. Compared to supervised learning, the ImageNet-1K-trained lightweight encoder shows superior performance in the downstream tasks with transfer learning.

**Impact and limitations.** In this work, SACL+XD works well with correlation-based loss  along with the shared encoder. Exploring the possibility of the proposed algorithm with the momentum-based encoder updating scheme [18; 19] could be an interesting direction. With the powerful, lightweight backbone trained by the proposed algorithm, we will further investigate the post-training quantization and hardware deployment, which further unleash the superior performance in Table 6 with practical downstream applications with actual hardware. To that end, our work can further improve the versatility of contrastive learning with energy-efficient applications (e.g., AI on edge).

Acknowledgments.We thank the anonymous reviewers for their comments and suggestions. This work is supported in part by the National Science Foundation under Grant No. 2144751, 2314591, 2328803, 2342726, and CoCoSys Center in JUMP 2.0, an SRC program sponsored by DARPA.