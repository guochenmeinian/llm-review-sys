# Towards stable real-world equation discovery with assessing differentiating quality influence

Mikhail Masliaev

ITMO University

St. Petersburg, Russia, 197101

maslyaitis@gmail.com Ilya Markov

ITMO University

St. Petersburg, Russia, 197101

iomarkov@itmo.ru Alexander Hvatov

ITMO University

St. Petersburg, Russia, 197101

alex_hvatov@itmo.ru

###### Abstract

This paper explores the critical role of differentiation approaches for data-driven differential equation discovery. Accurate derivatives of the input data are essential for reliable algorithmic operation, particularly in real-world scenarios where measurement quality is inevitably compromised. We propose alternatives to the commonly used finite differences-based method, notorious for its instability in the presence of noise, which can exacerbate random errors in the data. Our analysis covers four distinct methods: Savitzky-Golay filtering, spectral differentiation, smoothing based on artificial neural networks, and the regularization of derivative variation. We evaluate these methods in terms of applicability to problems, similar to the real ones, and their ability to ensure the convergence of equation discovery algorithms, providing valuable insights for robust modeling of real-world processes.

## 1 Introduction

Data-driven dynamical system modeling in forms of explicitly stated differential equations has emerged as a new direction for machine learning. Use of differential equations brings vast generalization ability, linked to the discovered expression interpretability and the existence of the equation general solution. The continuous process, represented by a multidimensional dataset, can be described with a partial differential equation (PDE), an expression connecting the dynamics along different.

The first advances in data-driven discovery of differential equations, as in , were devoted to the concept of applying symbolic regression to optimize the expression of the equation in a less restricted fashion. The next group of methods uses regularized regression with the help of the LASSO operator . The expansive study has been carried out by the SINDy framework development team, examining the ability of the approach to derive ordinary and partial differential equations in work , and improving the expression quality obtained . The evolution-based optimization approach, introduced and developed in works , involves creation of differential equations from elementary functions without assumptions about the structure of the equation. Although having reduced search space, the approach is able to mitigate the issue of infeasible candidate proposal and extreme computational costs, linked to the symbolic regression.

The majority of data-driven differential equation derivation techniques heavily rely on candidate libraries of numerically calculated derivatives of the data. We consider the implementation of

[MISSING_PAGE_FAIL:2]

is usually preferable due to the lower pure numerical error in the finite difference, leads to the magnification of random errors.

The noise influence on the data can be viewed from the point of view of Fourier analysis. The studied process shall not produce high-frequency oscillations, or have amplitudes significantly lower than the low-frequency counterparts. If the opposite is true, the data may have aliasing problems, thus limiting the applicability of the frequency-based analysis. We can note that these high-frequency components in the DFT (discrete Fourier transform) are linked to the measurement noise or small-scale processes that shall be omitted during the equation construction, and shall be filtered out. In what follows, brief notes of applied differentiation methods are presented, with a more detailed and expanded formulation placed in the Appendix A.

* **Filtering-based approaches:** One of the approaches considered in this work involves approximation of the input data with the fully connected artificial neural network (ANN). One of the valuable properties of the artificial neural network is that the low-frequency signal in the data is learned first, while further training approximates the high-frequency components . Thus, by training an ANN representation of the process, we can obtain its low-frequency approximation, which can be further differentiated with decreased noise component. Savitzky-Golay (SG) filtering, developed in , is a commonly used approach to signal or data filtering, coupled with an opportunity to compute derivatives, involves a least squares-based local fitting of the polynomials to represent the data. For each grid node, the data in its proximity is used to construct a polynomial that can be analytically differentiated.
* **Spectral domain differentiation:** Although the process of differentiation in the spatial domain can be complicated for the data, described with an arbitrary function, in the Fourier domain the derivatives can be estimated on a term-to-term basis . The discrete Fourier transform (DFT) is the basis of our implementation of spectral domain differentiation. In the spectral domain, integration and differentiation can be maintained by multiplication of series terms with an appropriate exponential. This leads to low computational costs, especially if the data are located on the uniform grid, thus allowing use of the Fast Fourier Transform instead of DFT. The signal filtering is done with the Butterworth filter that is able to preserve signal with frequencies lower than the cutoff frequency, while dampening the high-frequency ones.
* **Total variation regularization:** Variational principles provide an alternative method that incorporates inverse problem solution with the regularization of the gradient variation, or its higher-order analogues (e.g. Hessian). One of the main advances in this field was made in [12; 13].

## 3 Experiment section

The investigation and validation of theoretical approaches proposed in the previous sections is done with a comprehensive numerical experiment. Experiments are performed with two of the most commonly employed approaches for discovering data-driven differential equations: sparse regression and evolutionary-based approaches. The lack of analytical solutions for both problems necessitates a study of the algorithms' behavior.

### Sensitivity of the LASSO operator based approach

To evaluate the benefits of implementing stable differentiation on the quality of differential equations, discovered by LASSO regression, we have conducted a series of experiments with the SINDy framework. To analyze how the selection of the differentiation method affects the coefficients of the equation, we have conducted a series of experiments on the solution of a linear system \(x^{}=ax+by\), \(y^{}=cx+dy\), with \(a=-0.1\), \(b=2\), \(c=-2\), and \(d=-0.1\), provided by the developers of the SINDy framework. The noise was added only to the data to differentiate, leaving the candidate library intact. Otherwise, the results will be primarily influenced by the smoothing module, not the stable differentiation.

The summary of the experiment is presented in Fig. 1, where the performance of the main differentiation methods was compared in 25 independent runs for each noise level. Notably, experimentsthat employ variation regularization have not produced decent equations. The method constructs an approximation of the derivative close to the broken line. While it may be sufficient in problems of contour recognition on images, a more nuanced approach, which can preserve the structure of derivatives, is necessary in equation discovery. While all three compared methods correctly operate on noiseless data (with the spectral method introducing minor bias due to low number of non-dampened frequencies), on the corrupted datasets the spectral method has the highest stability.

### Experiments on sensitivity of the evolutionary approach

To better understand the effects of stable differentiation on the structures obtained by evolutionary algorithm equations, we conducted a series of experiments on synthetic data. All data for these experiments were obtained from the solutions of a priori known equations, as in the previous set of experiments, making the validation of the equation search explicit. As the metric of the equation search correctness, we employ the fitness function values and a proportion of the equations with desired structures among the individuals on the Pareto-optimal set of equations.

To provide some diversity among the problem statements, we have performed a data-driven rediscovery of the following differential equations: an ordinary differential equation \(mu^{}+qu^{}+kx=0\) with parameters \(m=1\), \(c=0.25\), and \(k=3\), and the wave equation \(u^{}_{tt}=c^{2}u^{}_{xx}\), \(c=0.5\). In

Figure 1: Statistics of coefficients of the equations, obtained by sparse regression with different differentiation approaches: i) finite-difference schema, ii) Savitzky-Golay filtering, iii) spectral method. The noise level \(\) denotes scale \(_{x}=*x(t)\), \(_{x}=*x(t)\) of the Gaussian distribution, from which the random errors are sampled

contrast to the sparsity-promoting methods, evolutionary algorithms can distil differential equations of higher orders in explicit form, i.e. not by translating them to a differential equation of higher order.

For the experiments, we have selected three different methods for obtaining numerical derivatives from input data: finite-differences, calculated based on the ANN approximation of input data; spectral differentiation and Savitzky-Golay filtering. Due to the sensitivity of the aforementioned methods to the parameters, a series of equation searches were conducted to better understand the bounds of the equation search errors.

To investigate the impact of noise in real data on the evolutionary algorithm, normally distributed noise with the following characteristics was added to the data. To take into consideration the stochastic nature of evolutionary optimization, multiple optimization runs were conducted while preserving the numerical differentiation parameters.

Figure 3: Results of the ordinary differential equation discovery experiment on clean data and AWGN-noised dataset with \(=0.1\) with different number of frequencies, left to be unchanged by the Butterworth filter. Frame a) indicates the process representation error of the obtained equations, and b) show the prevalence of equation with correct structures on Pareto-optimal set.

Figure 2: Results of the ordinary differential equation discovery experiment on clean data and AWGN-noised dataset with \(=0.1\) with different window size of Savitzky-Golay filter. Frame a) indicate the process representation error of the obtained equations, and b) shows the prevalence of equation with correct structures on Pareto-optimal set.

Figure 4: Results of the wave equation discovery experiment on clean data and AWGN-noised dataset with \(=0.1\) with different window size of Savitzky-Golay filter. Frame a) indicate the process representation error of the obtained equations, and b) shows the prevalence of equation with correct structures on Pareto-optimal set.

Figure 5: Results of the wave equation discovery experiment on clean data and AWGN-noised dataset with \(=0.1\) with different number of frequencies, left to be unchanged by the Butterworth filter. Frame a) indicates the process representation error of the obtained equations, and b) show the prevalence of equation with correct structures on Pareto-optimal set.

The results of experiments on evolutionary differential equation discovery, presented on Fig. 2, Fig. 3, Fig. 4, and Fig. 5, indicate, that an increase in differentiation error leads to an escalation in final model errors and a reduction in the proportion of equations with the correct structure in the Pareto frontier. The noise added to the data increases the dispersion of model errors, but still maintains the trend outlined for clean data.

The behavior of algorithms on ANN-filtered data, presented in Fig. 6 and Fig. 7 shows tendencies that differs from stated above. The differentiation error decreases and stabilizes for the derivatives in partial differential equations discovery. Due to stochastic behavior of neural network learning process, this effect leads to diminishing of model errors and increase in share part of equations with right structure in the final evolution optimization epoch. The same effects occur in partial differential equation discovery. Despite the decrease in derivation error, portrayed on Fig. 8, it may not stabilize if the data are highly contaminated. This leads to a high variance of model errors and almost eliminates correct equation structures from the final Pareto set when noise is added.

Figure 6: Results of the ordinary differential equation discovery experiment on clean data and AWGN-noised dataset with \(=0.1\) with different window size of Savitzky-Golay filter. Frame a) indicate the process representation error of the obtained equations, and b) shows the prevalence of equation with correct structures on Pareto-optimal set.

Figure 7: Results of the wave equation discovery experiment on clean data and AWGN-noised dataset with \(=0.1\) with different epochs of approximating artificial neural network training. Frame a) indicates the process representation error of the obtained equations, and b) show the prevalence of equation with correct structures on Pareto-optimal set.

## 4 Conclusion

The ability to train models in form of differential equations for the real-world processes is the next step in the development of the equation discovery techniques. Although some works state that it is achieved, most of the papers still consider toy examples with a known solution and known a priori equation form. During the experiments, following points could be outlined that will define the step of both gradient LASSO methods and evolutionary approaches:

* For the real-world equation discovery, as the experiments show, it is crucial to choice proper differentiation method rather than discovery method by itself;
* The filtering could not be applied to achieve arbitrary smoothness, since we need to preserve the information to restore the process;
* In real-world applications we have to somehow deal with the pre-defined library in cases when the underlying process known at a very high scale, i.e. we know origin of data, but not the equation.

The current study introduces one more control variable to make the discovery of real-world equations more viable. Namely, before choosing the method of discovery, we have to differentiate noisy experimental data. Even in the considered toy examples, it is still a challenge for existing state-of-the-art differentiation method to be able to both handle noise and recover the correct equation structure of the equation. The classical finite difference method is not good enough, instead we have to use more advances differentiation techniques - filtration, neural network approximation, or regularization for every problem appearing. For the real world equation learning scenarios we propose to use stable differentiation in all studies: when the random noise is assumed to have high magnitudes, spectral differentiation or ANN-based filtering are preferable, while in cases of low data distortions Savitsky-Golay filtering may be enough for decent equation discovery.

## 5 Data and code availability

The experiments are available in the repository https://anonymous.4open.science/r/ai4science_stable_diff_exp-735B/.