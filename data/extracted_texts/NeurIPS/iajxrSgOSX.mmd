# DELIFFAS: Deformable Light Fields

for Fast Avar Synthesis

 Youngjoon Kwon\({}^{1}\), Lingjie Liu\({}^{2,3}\), Henry Fuchs\({}^{1}\),

**Marc Habermann\({}^{3,4}\), Christian Theobalt\({}^{3,4}\)**

\({}^{1}\)University of North Carolina at Chapel Hill. \({}^{2}\)University of Pennsylvania.

\({}^{3}\)Max Planck Institute for Informatics, Saarland Informatics Campus.

\({}^{4}\)Saarbrucken Research Center for Visual Computing, Interaction and AI.

{youngjoong,fuchs}@es.unc.edu {lliu,mhaberma,theobalt}mpi-inf.mpg.de

This work was completed during an internship at MPII.Corresponding author

###### Abstract

Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics. Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved. To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model. At the core, we represent the light field around the human with a deformable two-surface parameterization, which enables fast and accurate inference of the human appearance. This allows perceptual supervision on the full image compared to previous approaches that could only supervise individual pixels or small patches due to their slow runtime. Our carefully designed human representation and supervision strategy leads to state-of-the-art synthesis results and inference time. The video results and code are available at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS.

## 1 Introduction

Generating photorealistic renderings of humans is a long-standing and important problem in Computer Graphics and Vision with many applications in the movie industry, gaming, and AR/VR. Traditionally, creating such digital avatars from real data involves complicated hardware setups and manual intervention from experienced artists, followed by sophisticated physically-based rendering techniques to render them into an image. Thus, recent research and also this work focuses on creating a drivable and photoreal digital double of a real human learned solely from multi-view video data to circumvent the complicated and manual work.

Recent works can be categorized by their underlying scene representation. Explicit mesh-based methods represent the dynamic human by a deforming geometry and dynamic textures  or textured body models . While these explicit methods achieve a comparably fast inference speed, the quality is still limited in terms of detail and photorealism. Hybrid approaches [14; 32] integrate the coordinate-based MLP to an explicit (potentially deforming) geometry. While their synthesis quality is drastically improved compared to explicit methods, the runtime is rather slow such that real-time is out of reach. Last, the concept of light fields [13; 38; 5] is well known for decades and recently, neural variants [47; 30; 55; 2; 52] have been proposed. Since ray-marching, _i.e._, sampling multiple points along the ray, is not required, these methods are fast to evaluate. However, results areonly shown on static scenes. Thus, previous work either demonstrated high synthesis quality _or_ fast inference speed. Our goal is to have a method, which achieves the best of the two worlds, _i.e._, _high synthesis quality_ and _fast inference speed_.

To this end, we propose _DELIFFAS_, a method for controllable, photorealistic, and fast neural human rendering. Given a multi-view video of an actor and a corresponding deformable human mesh model, which computes the deformed mesh as a function of the skeletal motion, we propose a deformable light field parameterization around the template. Typically, a light field is parameterized by a 3D position and a direction, however, this parameterization is neither robust nor efficient for deforming scenes. Therefore, we propose a deformable two-surface representation parameterizing a surface light field. In contrast to the original surface light field formulation, we deform the two surfaces according to a deformable mesh model. This allows us to efficiently query highly detailed appearance and even enables controllability since the light field can be driven by the skeletal motion. Due to the high inference speed, our method is also able to render the entire image during training, which is in stark contrast to coordinate-based approaches . This allows us to employ perceptual supervision on the entire image, which is in contrast to previous works that could only supervise individual pixels or small patches due to their slow runtime. In summary, our technical contributions are:

* A novel real-time method for learning controllable and photorealistic avatars efficiently from multi-view RGB videos.
* A deformable two-surface representation parameterizing a surface light field, which allows efficient and highly accurate appearance synthesis.
* We show that our surface light field representation and our neural architecture design can be effectively parallelized and integrated into the graphics pipeline enabling real-time performance and also allowing us to employ full-image level perceptual supervision.

## 2 Related Work

**Explicit Mesh-based Methods.** Some methods use an explicit mesh template and estimate non-rigid deformations and textures on the template mesh to fit the RGB input [60; 61]. Xu et al.  use texture retrieval and stitching to synthesize a texture map for the novel view. Others [7; 54] propose a layered texture representation to accelerate the image synthesis. These methods present limited generalization to new poses and viewpoints. Thies et al.  learn deep features in the texture space. Although they can represent view-dependent effects, it cannot generalize to novel poses. To address this issue, DDC  leverages differentiable rendering techniques to learn non-rigid deformations and texture maps on the explicit mesh. Given novel poses and views, DDC  can synthesize plausible results in real time. However, our method demonstrates significantly improved visual quality while also being real-time capable. Chen et al.  is a single surface-based method that is tightly bound to the underlying mesh. In contrast, the proposed two-surface light field can compensate for potentially erroneous geometry and synthesizes photoreal appearance beyond the mesh bounds.

**Hybrid Methods.** The synthesis quality of explicit meshed-based methods is bounded by the limited resolution of the template mesh. On the other hand, neural implicit representations [49;

Figure 1: We present DELIFFAS, a novel method for real-time, controllable, and highly-detailed human rendering. Our method takes a skeletal motion and camera view as input and generates a corresponding photorealistic image of the actor in real-time.

33, 24, 37, 31, 48] have shown their advantages over explicit representations in 3D modeling. For example, neural implicit representations are continuous, compact, and can achieve high spatial resolution. However, neural implicit representations are not animatable as deformable meshes. Therefore, recent studies propose to combine neural implicit representations with an explicit human template. Peng et al.  use a coarse deformable model (SMPL ) as a 3D proxy to optimize feature vectors, but it has limited pose generalizability. To address this issue, a large body of work  propose neural animatable implicit representations. These works leverage the animatable 3D proxy to deform the space of different poses to a shared canonical pose space. HDHumans , jointly optimizes the neural implicit representation and the explicit mesh model. However, these methods suffer from slow rendering speed, taking about 5 seconds to render one frame. In contrast, our method can achieve real-time rendering (\(31fps\)) and produces higher-quality synthesis.

**Light-field Methods.** The 5D plenoptic function, which represents the intensity of light observed from every position and direction in 3-dimensional space, was introduced by Bergen and Adelson . Levoy et al.  reduce the light field to a 4D function with the two-plane parameterization, _i.e._, light slab. Wood et al.  propose a surface light field, which maps a point on the base mesh and viewing direction to the radiance. While they can parameterize a full 360-degree ray space, they cannot go beyond the underlying mesh and can only represent a static scene. Recently, learning-based light field approaches have been proposed. Convolutional neural network-based methods  learn to interpolate and extrapolate the light field from a sparse set of views. Some works  achieve high quality synthesis with the light field represented as an MLP. However, they are limited to the fronto-parallel scene synthesis. To represent the full 360-degree ray space, some works  utilize Plucker and two-sphere parameterization , respectively. However, none of the aforementioned works can represent the full \(360^{}\) light space of a dynamic scene. Ouyang et al.  leverages multi-plane image and perceptual supervision to render a photorealistic avatar in real-time. However, they only allow minimal pose changes. In our work, we introduce a deformable two-surface representation parameterizing the surface light field, which enables us to represent and control the dynamic scene, _i.e._, the human, and to handle arbitrary poses during inference.

## 3 Method

Our goal is to learn an animatable 3D human character from multi-view videos, which can be rendered photorealistically from novel views and in novel motions in real time. In this endeavor, we propose a novel method, called DELIFFAS, which at inference takes a skeletal motion sequence and a target

Figure 2: **Method Overview. Given the skeletal motion and the target camera pose, we synthesize highly detailed appearance of a subject in real-time using a surface light field parameterized by our two surface representation. We first obtain the inner surface using the motion-dependent deformable human model. The outer surface is constructed by offsetting the inner surface vertice along its normal. For each camera ray, we obtain the uv coordinates of the intersecting points with the two surfaces from the image-space uv maps. Then, we bilinearly sample the features at the corresponding uv coordinates from the temporal normal feature map of each mesh. The two sampled features together with the two intersecting uv coordinates are fed into the light field MLP, which generates the color value. The rendered 2D image is supervised with \(L_{1}\) and perceptual losses.**

camera pose as input and renders a high-quality image of the subject in the specified pose under the virtual camera view at _real-time_ frames. An overview of our approach is shown in Fig. 2. At the core, our method represents the digital human as a surface light field, which is parameterized by two deforming surfaces that are directly controlled through the skeletal motion. Next, we provide some background concerning our data assumptions, deformable surface representations, and light fields (Sec. 3.1). Then, we introduce our deformable surface light field parameterized by a motion-dependent deformable two-surface representation and how it can be efficiently rendered (Sec. 3.2). Lastly, we introduce our perception-based supervision strategy (Sec. 3.3).

### Background

**Data Assumptions.** We assume a segmented multi-view video of the actor, who is performing a diverse set of motions, is given using \(C\) calibrated and synchronized cameras. Moreover, we also assume the skeletal motion \(_{f}^{D}\) for a frame \(f\) of the video can be obtained, _e.g._, by using markerless motion capture. Here, \(D\) denotes the number of degrees of freedom of the skeleton.

**Skeletal Motion-dependent Surface.** In this work, we mainly focus on the question of how to efficiently render a person at a photorealistic quality and less on surface reconstruction or modeling. Thus, we assume a skeletal motion-dependent deformable mesh surface \(s(_{f,f-T})=_{_{f}}:^{T D} ^{N 3}\) of the human is given. Here, \(_{f,f-T}\) denotes the motion window from \(f-T\) to the current pose \(f\) and \(_{_{f}}\) denotes the deformed and posed mesh vertex positions at frame \(f\) with pose \(_{f}\). \(N\) is the number of vertices. In practice, we leverage the deformable human model of Habermann et al. (2016). However, any other deformable surface representation such as SMPL (Srivastava et al., 2017) could be used as well (see Sec. D for more discussions).

**Light Field.** The light field (Han et al., 2016; Srivastava et al., 2017; Srivastava et al., 2017) is a function \(l(,)=\) that maps an oriented camera ray, here parameterized by its origin \(^{3}\) and the direction \(^{3}\), to the transmitted outgoing radiance along the camera ray. Note that only a single evaluation per pixel is required when rendering the light field into a discrete image, which is in stark contrast to the recently proposed Neural Radiance Fields (Srivastava et al., 2017) that require hundreds of computationally expensive network evaluations along the ray. Thus, light fields offer a computationally efficient alternative. Among the many possible ways of parameterizing a ray, the two-plane parameterization (Srivastava et al., 2017), _i.e._, light slab, is the most commonly used method. This two-plane method parameterizes a ray by its intersections with two planes \(_{1}\) and \(_{2}^{2}\). Without loss of generality, we here define the planes being contained in a range from zero to one and assume the field is always first intersecting \(_{1}\) and then \(_{2}\). Thus, the light field formulation can be adapted to

\[l(_{1},_{2})=\] (1)

where \(_{1}_{1}\) and \(_{2}_{2}\) are the intersection points of a ray with the planes. However, it is hard to model full \(360^{}\) view changes with this light slab representation, and it originally was not designed for controlling the scene, in our case the human. Thus, in the remainder of this section we demonstrate how this classical concept can be extended to our setting and how our formulation enables fast _and_ photoreal renderings of humans.

### Deformable Light Field Parameterization

As discussed above, one major challenge for the traditional two-plane representation is that it cannot cover the full \(360^{}\) in terms of viewing directions. Thus, we first explain how we adopt this representation using the deformed character mesh.

\(}\) **Viewpoints.** Since we have the deformed mesh \(_{_{f}}\), our idea is to adapt the two-plane parameterization to a two-(non-planar)-surface parameterization using the deformed mesh. While the inner surface (equivalent to \(_{2}\)) can be represented by the original mesh \(_{_{f}}\) (denoted as \(_{_{f}}^{}\) in the remainder), we have to construct an outer surface \(_{1}\) for which we can guarantee that every ray first intersects \(_{1}\) before intersecting \(_{2}\). This can be achieved by constructing an offset surface as

\[_{_{f},i}^{}=_{_{f},i}^{ }+d_{i}^{}\] (2)

where \(i\) denotes the \(i\)-th vertex, \(_{i}^{}\) is the respective normal on the inner surface, and \(d\) defines the length of the offset. We use d=3cm in the experiments. By offsetting all vertices, we obtain the outer surface \(_{_{f}}^{}\). By assuming the mesh is watertight and that the origin \(\) of a ray with direction \(\) liesoutside of \(_{_{f}}^{}\), we can guarantee that this ray will always intersect \(_{_{f}}^{}\) before \(_{_{f}}^{}\). Note that we can now have plausible intersections with the two surfaces from (almost) any point in 3D space enabling \(360^{}\) viewpoints, which are typically difficult for the classical two-plane parameterization.

**2D Surface Parameterization.** Now if the ray intersects the outer and inner surface at \(^{}\) and \(^{}\), respectively, we are interested in converting it into the original light slab parameterization (Eq. 1) that solely consists of two 2D coordinates. We achieve this by performing UV mapping, _i.e._, we construct a texture atlas \(m()=\) for the original mesh \(_{_{f}}^{}\). \(m\) maps a point \(\) on the 3D surface to a 2D location \(^{2}\) on a plane. Note that the atlas remains the same across poses since the connectivity of the mesh is fixed. Further, our offset surface construction also preserves the texture atlas and, thus, the atlas for the outer surface is equivalent to the inner one. Now, our 3D intersection points \(^{}\) and \(^{}\) can be mapped to 2D as \(m(^{})=^{},m(^{})=^{}\) and the light slab formulation can be adapted as

\[l(^{},^{})=.\] (3)

Note that both 2D coordinates are not dependent on the motion of the character and, therefore, the light field \(l\) cannot model motion-dependent appearance. Next, we further refine our formulation such that \(l\) can potentially model this.

**Skeletal Motion-dependent Conditioning.** We note that the human mesh \(_{_{f}}^{}\) is already a function of skeletal motion, though it is not in a format for efficient encoding considering the light slab formulation. We convert the inner and outer surface into temporal normal maps \(_{,f:f-T}^{}\) and \(_{,f:f-T}^{}\), which are the concatenation of the posed normal maps of the motion window \([f,f-T]\). Note that both maps encode information about the skeletal motion and can be directly generated by the motion-dependent mesh surface \(s()\).

Now, we deeply encode them into feature maps \(f_{}(_{,f:f-T}^{})\) and \(f_{}(_{,f:f-T}^{})\) with a channel size of 32 respectively using deep convolutional networks \(f\) with weights \(\) and \(\). Last, our light slab formulation (Eq. 3) can be again refined as

\[l((^{}),(^{}),(f_ {}(_{,f:f-T}^{}),^{}),(f_{}(_{,f:f-T}^{}), ^{}))=.\] (4)

Here, \(\) denotes the bilinear sampling operator. \(\) is the positional encoding . Intuitively, the light slab \(l\) takes the bilinearly sampled features on the UV plane at the uv coordinates of the intersections points of the ray with the respective plane. Now, our light slab formulation is not only view-dependent but is also motion-dependent since the features are encoding the skeletal motion.

In case the ray is not intersecting the inner surface, inspired by depth peeling, uv coordinates and features are sampled at the second intersection with the outer surface. Here, the watertightness of the model guarantees that such a second intersection point exists. We note that our light slab \(l\) is represented as an 8-layer MLP with 256 channel-size.

**Efficient Rendering.** Next, we explain how the above formulation can be efficiently computed using the standard graphics pipeline and deep learning tools. First, the two convolutional networks \(f_{}\) and \(f_{}\) have to encode the respective normal maps. The computational effort is independent of the number of foreground pixels, _i.e._, pixels which are covered by the subject. Next, for each pixel we have to obtain the uv coordinates of the intersection points with the inner and outer surface. Here, standard GPU-parallelized rasterization can be leveraged by rendering the uv coordinates into screen space. The only computation that linearly scales with the number of foreground pixels is the evaluation of the light slab \(l\), but since there is only one evaluation per ray, also this step is computationally efficient leading to an inference time of \(31fps\).

### Supervision and Training Procedure

We supervise our approach by minimizing the following loss \(=_{1}_{1}+_{} _{}\), where \(_{1}\) and \(_{}\) are the \(L_{1}\) loss and perceptual  loss, respectively. \(_{1}\) and \(_{}\) are their weights. We use the VGG-16 network  pre-trained on ImageNet  to compute the perceptual similarity loss. Although our method solely supervised with the \(L_{1}\) loss already outperforms other real-time methods and most of the non-real time methods on the novel view synthesis task (see Tab. 1a and Tab. 2-c), it is still blurry. This is due to the one-to-many mapping problem [3; 32] where the single skeletal pose can still induce various different appearances. Thus, the perceptual loss  is additionally employed and this improves the fine-grained details as can be seen in the ablation study (see Fig. 5-c,d). In contrast to patch-based perceptual supervision methods [44; 35; 57; 12], our fast rendering speed enables employing the perceptual supervision on _the entire image_.

## 4 Results

DELIFFAS runs at \(31fps\) when rendering a \(1K\) (\(940 1285\)) video using a single A-100 GPU with an Intel Xeon CPU. The implementation details and additional results are provided in the appendix.

**Dataset.** We evaluate our method on the DynaCap dataset , which is publicly available. DynaCap provides performance videos of 5 different subjects captured from 50 to 101 cameras, foreground mask, and skeletal pose corresponding to each frame, and the template mesh for each subject. The training and testing videos consist of around \(20k\) and \(5k\) frames at the resolution of \(1K\) (\(1285 940\)), respectively. Four cameras are held out for the testing and the remaining ones are used for the training as proposed by the original dataset. Among the available subjects, we choose \(D_{1}\), \(D_{2}\), and \(D_{5}\) subjects for our experiments, which vary in terms of clothing style, _i.e._, loose skirts and trousers.

**Metrics.** We evaluate our performance using the peak signal-to-noise ratio (PSNR). However, human perception cannot be fully reflected with this metric since a very blurry and unrealistic result can still lead to a low error . Therefore, we additionally employ the learned perceptual image patch similarity (LPIPS) , and the Frechet inception distance (FID) , which are similar to the human perception. We generate and evaluate results at \(1K\) resolution (\(1285 940\)). Every metric is computed by averaging across the entire sequence using every \(10th\) frame. Four held-out cameras (7, 18, 27, 40), which are uniformly sampled in the space, are used for the testing.

### Qualitative Results

We first present the qualitative results on the novel view synthesis task in Fig. 3-(A) and on the novel pose synthesis task in Fig. 3-(B). For both tasks, we show the synthesis result of the subject in two different poses viewed from two different viewpoints. Our method synthesizes images with fine-scale details including the facial features and wrinkles of the clothing for both tasks. Note that our method can even generate plausible results of the subject in a challenging garment type, _i.e._, loose skirt. Furthermore, we can synthesize high-quality renderings of the subject under difficult testing poses. These results confirm the versatility as well as the generalization ability of our method.

### Comparison

We compare our method with the state-of-the-art methods concerning novel view and pose synthesis. We choose baselines from four different classes, which are volume rendering, explicit mesh, hybrid, and point cloud-based methods. For the volume rendering-based method, we compare with Neural Volumes (NV) , which learns a volumetric representation. Deep Dynamic Character (DDC)  is an explicit mesh-based method, where the network learns to render the neural texture obtained by rasterizing the explicit mesh. Neural Actor (NA) , Neural Body (NB) , and A-NeRF  are hybrid methods that combine the human prior, _i.e._, naked human body model , and skeleton,

Table 1: **Quantitative results on novel view and pose synthesis. (a) Our method achieves the best performance among all the real-time and non-real time methods in terms of PSNR and LPIPS, and the second best performance on FID. (b) Our method again achieves high performance on the perceptual metrics while running in real-time. While HDHumans has slightly better results in terms LPIPS and FID compared to our work, their method is \( 100\) slower than our method.**Figure 4: **Comparisons** on the novel view and pose synthesis task on the \(D_{2}\) subject. Our method synthesizes realistic and delicate details in real time (\(31fps\)). Note that we achieve similar or better quality as offline approaches and show drastically improved quality compared to real-time methods.

Figure 3: **Qualitative results** for novel view and poses. Our method achieves photorealistic rendering quality while running in real time. We are able to recover the fine-grained details including the facial features and clothing wrinkles. The high-quality synthesis results on the very challenging loose garment, _i.e_., skirt, and the complicated testing poses show the versatility of our method.

with a NeRF. Neural Actor (NA) achieves high-quality results by utilizing the adversarial supervision in texture space. Neural Human Renderer (NHR)  optimizes features that are anchored to point clouds. We additionally compare to the concurrent work HDHumans , which is a hybrid method that combines the deformable human model  with a NeRF. Similar to NA, HDHumans also employs adversarial supervision in texture space. We show our comparison results on the \(D_{2}\) subject with tight clothing, as other baselines except DDC and HDHumans cannot handle the loose garment types. Note that we present both the qualitative and quantitative results in the order of _non-real time_, _i.e._, NB, A-NeRF, NA, HDHumans, and _real-time_ methods, _i.e._, NHR, NV, DDC, and ours.

Fig. 4-(A) shows the comparison on the novel view synthesis task. Different from their original results , NB produces blurry results when trained on longer and more challenging training sequences, _e.g._, around 20k frames, as they fail to optimize the appearance code for every frame. A-NeRF exhibits the loss of details under highly articulated poses. While DDC and NA can generate coarse details, they still lack high-frequency details. On the other hand, our method can recover, both, the coarse and fine details and only achieves slightly worse synthesis quality than HDHumans while being significantly faster (\( 100\)). In Tab. 1a, our method outperforms all the non-real time and real-time methods except HDHumans. However, we would like to highlight again that HDHumans has a runtime in the order of seconds per frame, which prevents it from being used in any real-time application. It is worth noting that ours with \(L_{1}\)-only supervision (see Tab. 2-c) already outperforms other real-time methods and even the non-real time method NA.

Fig. 4-(B) shows the comparisons on the novel pose synthesis task. Again, our method can recover the sharp and high-frequency details that are close to the ones of HDHumans, while most of the other approaches struggle with the given challenging pose. This is consistent with the quantitative results in Tab. 1b, where we achieve the second best performance among all the methods and the best performance among the real-time methods on the perception-based metrics LPIPS and FID. Due to the innate nature of pose to appearance task, _i.e._, the one-to-many mapping , our method can generate realistic looking details that can deviate from the ground truth, which explains the lower performance in terms of PSNR in Tab. 1b.

### Ablation

We perform ablation studies on the novel view synthesis task on the \(D_{2}\) subject. To accurately access the contribution of each component, we train all the variants only with \(L_{1}\) loss if not stated otherwise.

Impact of Two-surface Parameterization.To study the efficacy of the two-surface parameterization, we train the single-surface model ('single-surface') where the MLP generates color conditioned only on a single inner surface intersection coordinates and feature. We also train a single-surface variant that is additionally conditioned on the viewing direction ('single-surface + viewing direction'). In Tab. 2, our two-surface model (c) outperforms the single-surface variants (a,b). Also, we would like to again stress that our \(L_{1}\)-only supervised two-surface model (c) already surpasses all of the real-time methods including DDC, and also outperforms the non-real time method NA, which is trained with ground truth texture-map supervision and adversarial supervision. This confirms that the

Figure 5: **Ablation of each design choice on the novel view synthesis task. All the variants except (d) are trained only with \(L_{1}\) loss. Although conditioning on the viewing direction (b) improves the quality, single-surface models (a,b) are tightly bounded to the underlying mesh. On the other hand, our proposed method (c,d) can go beyond the mesh and recover the real geometry. Additional utilization of the perceptual loss (d) improves the level of detail.**

improvement originates from the carefully-designed two-surface model. In Fig. 5, We visually ablate the impact of each design choice. Although conditioning on the viewing direction (b) improves the quality, the single-surface models (a,b) are tightly bound to the underlying mesh. On the other hand, our two-surface models (c,d) go beyond the mesh and recover the real geometry.

**Impact of Perceptual Supervision.** Perceptual supervision improves the details (_e.g._, wrinkles) in Fig. 5-d, which is also confirmed by the better performance in the perceptual metrics in Tab. 2-d.

**Impact of Deformable Template Quality.** To demonstrate the robustness of our method, we train and evaluate our method with a smoothed version of the original mesh model in Fig. 6 and Tab. 2-e,f. The quantitative result outperforms real-time methods including DDC in terms of PSNR. This shows that even coarser geometry is sufficient for our representation. Moreover, the single-surface model (Fig. 6-a) reveals and suffers from geometric error when the coarse template is used. On the other hand, our two-surface representation (Fig. 6-b) has the capability of fixing geometric errors and shows similar visual quality to the result when the original mesh model is used (Fig. 6-c).

## 5 Conclusion

We presented DELIFFAS, an animatable representation that can synthesize high-quality images of the avatar under a user-controlled skeletal motion and viewpoint in real-time. At the technical core, our method utilizes a surface light field parameterized with two deformable surfaces computed from a deformable human model. Then, our light slab queries pose-dependent temporal normal map features together with the intersecting uv coordinates in order to render the pixel color with just a single MLP evaluation per pixel. This efficient rendering allows perceptual supervision on the entire image. Our experiments show that the proposed representation can synthesize high-quality renderings even under challenging poses, and outperforms the state-of-the-art real-time methods.

   &  &  &  \\  a. & Single-surface & 31.34 & 22.74 & 28.96 \\ b. & Single-surface + viewing direction & 32.19 & 20.38 & 23.88 \\  c. & Two-surface with \(L_{1}\) & 33.27 & 18.51 & 23.58 \\ d. & Two-surface with \(L_{1}+_{}\) (**Ours**) & **33.30** & **12.85** & **8.69** \\  e. & Single-surface with coarse mesh & 29.63 & 27.75 & 42.84 \\ f. & Two-surface with coarse mesh & 32.03 & 22.00 & 31.43 \\  

Table 2: **Ablations** on novel view synthesis for the \(D_{2}\) subject. All baselines except (d) are trained with \(L_{1}\)-only supervision. Our two-surface parameterization outperforms single-surface variants and the perceptual supervision further improves visual quality. Further, our results when using a coarse mesh model are still plausible demonstrating the robustness of our method.

Figure 6: **Ablation on the deformable template quality.** All baselines trained only with \(L_{1}\) loss. When using a coarse mesh model the single-surface baseline (a) suffers from the geometric error of the model. In contrast, our two-surface representation (b) has the capability of fixing geometric errors and shows similar quality compared to the result when using the original mesh model (c).

[MISSING_PAGE_EMPTY:10]

*  Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
*  Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry Fuchs. Neural human performer: Learning generalizable radiance fields for human performance rendering. _Advances in Neural Information Processing Systems_, 34:24741-24752, 2021.
*  YoungJoong Kwon, Dahun Kim, Duygu Ceylan, and Henry Fuchs. Neural image-based avatars: Generalizable radiance fields for human avatar modeling. In _The Eleventh International Conference on Learning Representations_, 2023.
*  Marc Levoy and Pat Hanrahan. Light field rendering. In _Proceedings of the 23rd annual conference on Computer graphics and interactive techniques_, pages 31-42, 1996.
*  Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jurgen Gall, Angjoo Kanazawa, and Christoph Lassner. Tax: Template-free animatable volumetric actors. In _European Conference on Computer Vision (ECCV)_, 2022.
*  Celong Liu, Zhong Li, Junsong Yuan, and Yi Xu. Neufl: Efficient novel view synthesis with neural 4d light field. _arXiv preprint arXiv:2105.07112_, 2021.
*  Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. _Advances in Neural Information Processing Systems_, 33, 2020.
*  Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor: Neural free-view synthesis of human actors with pose control. _ACM Trans. Graph._, 40(6), dec 2021.
*  Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. _ACM Transactions on Graphics (TOG)_, 38(4):65, 2019.
*  Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. _ACM Trans. Graphics (Proc. SIGGRAPH Asia)_, 34(6):248:1-248:16, Oct. 2015.
*  Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, and Shunsuke Saito. Keypointnerf: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV_, pages 179-197. Springer, 2022.
*  Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. _ACM Transactions on Graphics (TOG)_, 38(4):1-14, 2019.
* ECCV 2020_, pages 405-421, Cham, 2020. Springer International Publishing.
*  Parry Moon and Domina Eberle Spencer. The photo field. _Cambridge_, 1981.
*  Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Neural articulated radiance field. In _International Conference on Computer Vision_, 2021.
*  Hao Ouyang, Bo Zhang, Pan Zhang, Hao Yang, Jiaolong Yang, Dong Chen, Qifeng Chen, and Fang Wen. Real-time neural character rendering with pose-guided multiplane images. In _European Conference on Computer Vision_, pages 192-209. Springer, 2022.
*  Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Animatable neural radiance fields for human body modeling. _ICCV_, 2021.
*  Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
*  Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. _CVPR_, 1(1):9054-9063, 2021.
*  Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. _Advances in Neural Information Processing Systems_, 33:20154-20166, 2020.
*  Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky. Textured neural avatars. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
*  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
*  Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. _Advances in Neural Information Processing Systems_, 34:19313-19325, 2021.

*  Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Niessner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2437-2446, 2019.
*  Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. In _Advances in Neural Information Processing Systems_, pages 1119-1130, 2019.
*  Shih-Yang Su, Timur Bagautdinov, and Helge Rhodin. Danbo: Disentangled articulated neural body representations via graph neural networks. In _European Conference on Computer Vision_, 2022.
*  Shih-Yang Su, Frank Yu, Michael Zollhoefer, and Helge Rhodin. A-nerf: Surface-free human 3d pose refinement via neural rendering. _arXiv preprint arXiv:2102.06199_, 2021.
*  Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Light field neural rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8269-8279, 2022.
*  Justus Thies, Michael Zollhofer, and Matthias Niessner. Deferred neural rendering: Image synthesis using neural textures. _Acm Transactions on Graphics (TOG)_, 38(4):1-12, 2019.
*  Marco Volino, Dan Casas, John Collomosse, and Adrian Hilton. Optimal representation of multiple view video. In _Proceedings of the British Machine Vision Conference_. BMVA Press, 2014.
*  Peng Wang, Yuan Liu, Guying Lin, Jiatao Gu, Lingjie Liu, Taku Komura, and Wenping Wang. Progressively-connected light field network for efficient view synthesis. _arXiv preprint arXiv:2207.04465_, 2022.
*  Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu Tang. Arah: Animatable volume rendering of articulated human sdfs. In _European Conference on Computer Vision_, 2022.
*  Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. Humanmerf: Free-viewpoint rendering of moving people from monocular video. _arXiv_, 2022.
*  Daniel N Wood, Daniel I Azuma, Ken Aldinger, Brian Curless, Tom Duchamp, David H Salesin, and Werner Stuetzle. Surface light fields for 3d photography. In _Seminal Graphics Papers: Pushing the Boundaries, Volume 2_, pages 487-496, 2023.
*  Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu. Multi-view neural human rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1682-1691, 2020.
*  Donglai Xiang, Timur Bagautdinov, Tuur Stuyck, Fabian Prada, Javier Romero, Weipeng Xu, Shunsuke Saito, Jingfan Guo, Brenanan Smith, Takasaki Shiratori, et al. Dressing avatars: Deep photorealistic appearance for physically simulated clothing. _ACM Transactions on Graphics (TOG)_, 41(6):1-15, 2022.
*  Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, and Timur Bagautdinov. Drivable avatar clothing: Faithful full-body telepresence with dynamic clothing driven by sparse rgb-d input. In _SIGGRAPH Asia 2023_, 2023.
*  Feng Xu, Yebin Liu, Carsten Stoll, James Tompkin, Gaurav Bharaj, Qionghai Dai, Hans-Peter Seidel, Jan Kautz, and Christian Theobalt. Video-based characters: Creating new human performances from a multi-view video database. In _ACM SIGGRAPH 2011 Papers_, SIGGRAPH '11, pages 32:1-32:10, New York, NY, USA, 2011. ACM.
*  Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion, 2021.
*  Jae Shin Yoon, Duygu Ceylan, Tuanfeng Y Wang, Jingwan Lu, Jimei Yang, Zhixin Shu, and Hyun Soo Park. Learning motion-dependent appearance for high-fidelity rendering of dynamic humans from a single camera. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3407-3417, 2022.
*  Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.
*  Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yandong Guo, and Yebin Liu. Structured local radiance fields for human avatar modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15893-15903, 2022.

Appendix - Overview

This appendix is organized as follows: Sec. B shows additional results including video results, ablations on image-level and patch-level perceptual supervision, ablation study with and without perceptual supervision, ablations on the single surface with the same network capacity, results when using SMPL, comparison with traditional multi-view stereo method; Sec. C provides information regarding the reproducibility, which includes implementation details, training details, and runtime at inference; Sec. D discusses the performance changes according to the template mesh quality, and possible alternatives for the template mesh; Sec. E presents the societal impacts our work can have; Sec. F discusses the limitations of this work.

## Appendix B Additional Results

### Video Results

Video results of free-viewpoint renderings, novel view and pose synthesis, and comparison with the state-of-the-art baselines on the DynaCap dataset  can be found at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS. We compare our DELIFFAS with the non-real-time hybrid method Neural Actor , and real-time explicit mesh-based method Deep Dynamic Characters .

### Ablation on Image-level and Patch-level Perceptual Supervision

To study the impact of our full image-level supervision, we train a variant with patch-level perceptual supervision. Here, we use a \(32 32\) patch. In Tab. 3, our approach trained with full-image-level perceptual supervision (Tab. 3-c) outperforms the patch-level variant (Tab. 3-b) in terms of the perceptual metrics. In Fig. 7, we ablate the visual impact. Employing full-image-level supervision leads to better reconstruction of details (_e.g._, wrinkles). We would like to again highlight that image-level supervision with perceptual loss is possible thanks to the fast rendering speed of our method.

### Ablation Study with and without Perceptual Supervision

We excluded the perceptual supervision from the ablation study in the main text (see Tab. 2, Fig. 5, and Fig. 6) to verify the effectiveness of our two-surface design. In Tab. 4 and Fig. 8, we show the ablation results with perceptual supervision. Similar to the results without perceptual supervision, our full model outperforms other variants.

Figure 7: **Ablation** of patch-based and image-level perceptual supervision. Variants with the perceptual supervision (b,c) shows better details than the result with \(L_{1}\)-only supervision (a). Our final model which employs the full image-level perceptual supervision (c) outperforms the variant with patch-level supervision (b).

### Ablations on the Single Surface with the Same Network Capacity

The number of total parameters is larger in the two-surface model than in the single-surface model as it includes two separate U-Nets for extracting feature map from each surface while the single-surface model only has a single U-Net. The U-Net and MLP architecture used in both single and two-surface models are the same. To verify that the performance boost originates from the two-surface design rather than the increase in network capacity, we additionally trained a "single-surface with two concatenated U-Net" variant (see Fig. 9, Tab. 4-c,i). Even though now the network size is the same, our two-surface representation still outperforms the single-surface baseline.

### Using SMPL as the Deformable Mesh Surface

We conduct an experiment where we use the SMPL model as the deformable mesh model (see Fig. 10). One can see that our approach even for this very coarse model still outperforms the baselines proving the flexibility of our representation while a better template mesh further improves the results.

### Comparison with Traditional Multi-view Stereo Method

We compare with the commercial photogrammetry software, Metashape  in Fig. 11. The reconstructed surface is noisy and the texture suffers from ghosting artifacts due to the inaccurate geometry. Also, Metashape takes 6 minutes to reconstruct while ours works in real-time (\(31fps\)). Lastly, our method is animatable, while multi-view stereo methods are not.

 
**Method** & **PSNR\(\)** & **LPIPS\(\)** & **FID\(\)** \\  \(L_{1}\) & & & \\  a. & Single-surface & 31.34 & 22.74 & 28.96 \\ b. & Single-surface + viewing direction & 32.19 & 20.38 & 23.88 \\ c. & Single-surface + two U-Net & 31.47 & 15.43 & **19.99** \\ d. & Two-surface (**Ours**) & **33.27** & **18.51** & 23.58 \\ e. & Single-surface with coarse mesh & 29.63 & 27.75 & 42.84 \\ f. & Two-surface with coarse mesh & 32.03 & 22.00 & 31.43 \\  \(L_{1}+L_{perc}\) & & & & \\  g. & Single-surface & 31.36 & 16.53 & 10.90 \\ h. & Single-surface + viewing direction & 31.95 & 15.96 & 9.01 \\ i. & Single-surface + two U-Net & 31.26 & 16.21 & 9.61 \\ j. & Two-surface (**Ours**) & **33.30** & **12.85** & **8.69** \\ k. & Single-surface with coarse mesh & 29.55 & 20.05 & 15.22 \\ l. & Two-surface with coarse mesh & 32.03 & 16.13 & 12.08 \\  

Table 4: **Ablations on novel view synthesis for the \(D_{2}\) subject with and without perceptual supervision**. Our two-surface parameterization performs better than the single-surface variants and the perceptual supervision further boosts the visual quality. In addition, our results with coarse mesh model shows reasonable performance, which confirms the robustness of our method.

 
**Method** & **PSNR\(\)** & **LPIPS\(\)** & **FID\(\)** \\  a. & Image-level with \(L_{1}\) & 33.27 & 18.51 & 23.58 \\ b. & Patch-level with \(L_{1}+_{}\) & 33.24 & 16.54 & 18.96 \\ c. & Image-level with \(L_{1}+_{}\) (**Ours**) & **33.30** & **12.85** & **8.69** \\  

Table 3: **Ablation** on image-level and patch-level supervision. Applying the perceptual supervision on the entire image (c) leads to better results than the patch-level perceptual supervision (b) or \(L_{1}\)-only supervision.

Figure 8: **Ablations** on novel view synthesis for the \(D_{2}\) subject **with and without perceptual supervision**. The proposed two-surface representation helps to recover the real geometry while the single-surface design is limited to the underlying mesh. Perceptual supervision further enhances the quality.

Figure 9: **Ablations** on the single surface with **the same network capacity**. Evaluated on the novel view synthesis task for the \(D_{2}\) subject. Even though the network size of the “single-surface with two U-Net” variant and ours are the same, our two-surface representation still outperforms the single-surface baseline. This again verifies that the performance boost is coming from the two-surface design, and not from using more network.

## Appendix C Reproducibility

### Implementation Details

**Feature Extractor.** The feature extractor for each surface is based on the same U-Net architecture . We use the feature extractor to encode the temporal normal map \((T+1) 1024 1024 3\) into the \(1024 1024 32\) feature map. Here, the temporal normal map is the concatenation of the posed normal maps of the motion window \([f,f-T]\). The feature extractor consists of 10 down-sampling layers (with a down-sampling rate of two) and 10 up-sampling layers (with an up-sampling rate of two). There is a skip connection between every corresponding down and upsampling layer.

**Light Field MLP.** The light field MLP consists of 8 layer with 256 channel size. ReLU activation is used in between the layers. Input to the light field MLP are the positional encodings of two intersection uv coordinates. Then we concatenate the temporal normal map features sampled from the inner and outer surface to the intermediate output of \(4^{th}\) layer. Output of the light field MLP is the 3-channel RGB value.

### Training Details

**Deformable Human Model.** We leverage the deformable human model of Habermann et al. , _i.e._, DDC. The geometry networks of the DDC model are additionally trained with a Chamfer distance supervision with respect to 4D multi-view stereo reconstructions. Originally, we reported the numbers provided by the authors of DDC in Tab. 1. We trained TexNet of DDC with our exact checkpoint for

Figure 10: **Ablations on using SMPL as a deformable human model. Evaluated on the novel view synthesis task for the \(D_{2}\) subject. Our method with very coarse SMPL mesh can still recover the real geometry compared to the single surface baseline where the approach can only paint on the given mesh.**

[MISSING_PAGE_EMPTY:17]

Note that we assume the output of the deformable character model, _i.e._, DDC , is already available at training and inference time. DDC is also a real-time method so coupling DDC and our method would not deteriorate the performance much.

We would like to highlight that our method achieves the best of two worlds: runs in real-time as the explicit mesh-based method DDC (\(25fps\)), while at the same time achieving superior and comparable visual quality to the hybrid method NA (\(0.2fps\)) and HDHumans (\(0.3fps\)), respectively.

When tested on an A40 graphics card, we achieve a real-time speed of \(26fps\). Furthermore, most of our computational budget is taken by the U-Net feature map extraction ( 60%) and deterministic bi-linear sampling of features ( 20%). We use a standard U-Net architecture and Tensorflow's bi-linear sampling, but more efficient architecture and sampling implementation can be explored to further reduce the runtime.

## Appendix D Discussion on the Template Mesh Quality

**Rendering Quality.** Our performance degrades if a coarser mesh is used. However, note that our method with coarse geometry or even SMPL mesh can still recover the real geometry compared to the single surface baseline where the approach can only paint on the given mesh (see Fig. 6, Fig. 10, and Fig. 12). Also, it would be an interesting research direction if we could jointly refine the underlying mesh during training and improve the rendering quality.

**Possible Alternatives to the Deformable Human Model.** The deformable human model we used in the main text is DDC . Alternatively, one can use the SMPL model (see Fig. 10) or SMPL+D to deal with the loose clothing. Also, we can use recent avatar NeRF works [64; 21; 42] to compute the canonical mesh (template mesh) and then use skinning-based deformation to obtain the deformed mesh for the new pose.

Figure 12: **Ablations** on the deformable template quality **with and without perceptual supervision**. Evaluated on the novel view synthesis task for the \(D_{2}\) subject. Even when using the coarse mesh, our two-surface design enables to generate visual quality similar to the rendering result using original mesh. Employing the perceptual supervision further improves the quality.

Societal Impacts

Our research presents potential societal impacts that should be considered. Our method can create avatars using only RGB supervision, which has the potential to democratize immersive VR experiences. Unlike traditional methods that rely on costly setups and specialized professionals, our approach offers accessibility to a wider range of individuals. Additionally, digital doubles generated through our techniques can be used to perform dangerous stunts, reducing risks for human actors. Moreover, individuals can engage in virtual exploration using their personalized avatars.

However, it is important to acknowledge the potential negative consequences associated with our method. The generation of fake media poses a significant risk, potentially leading to public misinformation and eroding trust in media content. We recognize these concerns and emphasize the need for public awareness, responsible use, and disclosure. We strive and strongly hope that our work is applied in ways that positively impact society.

## Appendix F Limitations

Although our method achieves state-of-the-art results in terms of visual quality and runtime, it is not free from limitations. For now, we consider the deformable surface representation is given as an input and our method builds up on it. However, surface deformation and image synthesis can be treated jointly and help to improve each other. Inspired by that, future work could extend our deformable two-surface light field representation such that it can backpropagate into the geometry directly and, thus, potentially refine it throughout the training process. Moreover, the face and hand are not animatable. Extending our work to support the face and hand animation would enhance the versatility of our method and enable many interesting applications such as gaming, and VR. Very complex clothing (_e.g._, highly glossy garment) might be challenging, though, in contrast to many related works, we demonstrate that loose clothing can work while most other methods only operate under the assumption of tight clothing. The input to our system is a temporal normal map and it is not sufficient enough to fully describe the subject's clothing state which can result in rather blurry images. This is why we employed perceptual supervision so that we can recover the fine details. However, it would be a promising direction to adopt the generative models (_e.g._, GAN, VAE) to generate even more photorealistic details.