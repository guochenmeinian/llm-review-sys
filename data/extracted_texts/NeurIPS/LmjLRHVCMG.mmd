# An Improved Empirical Fisher Approximation for Natural Gradient Descent

Xiaodong Wu\({}^{1*}\)  Wenyi Yu\({}^{2*}\)  Chao Zhang\({}^{2}\)  Philip Woodland\({}^{1}\)

\({}^{1}\)Dept. of Engineering, University of Cambridge \({}^{2}\)Dept. of Electronic Engineering, Tsinghua University

{xw338,pw117}@cam.ac.uk {ywy22@mails,cz277@mail}.tsinghua.edu.cn

These authors contributed equally to this work

###### Abstract

Approximate Natural Gradient Descent (NGD) methods are an important family of optimisers for deep learning models, which use approximate Fisher information matrices to pre-condition gradients during training. The empirical Fisher (EF) method approximates the Fisher information matrix empirically by reusing the per-sample gradients collected during back-propagation. Despite its ease of implementation, the EF approximation has its theoretical and practical limitations. This paper investigates the _inversely-scaled projection_ issue of EF, which is shown to be a major cause of its poor empirical approximation quality. An improved empirical Fisher (iEF) method is proposed to address this issue, which is motivated as a generalised NGD method from a loss reduction perspective, meanwhile retaining the practical convenience of EF. The exact iEF and EF methods are experimentally evaluated using practical deep learning setups, including widely-used setups for parameter-efficient fine-tuning of pre-trained models (T5-base with LoRA and Prompt-Tuning on GLUE tasks, and ViT with LoRA for CIFAR100). Optimisation experiments show that applying exact iEF directly as an optimiser provides strong convergence and generalisation. It achieves the best test performance and the lowest training loss for the majority of the tasks, even when compared to well-tuned AdamW/Adafactor baselines. Additionally, under a novel empirical evaluation framework, the proposed iEF method shows consistently better approximation quality to exact Natural Gradient updates than both the EF and the more expensive sampled Fisher methods, meanwhile demonstrating the superior property of being robust to the choice of damping across tasks and training stages. Improving existing approximate NGD optimisers with iEF is expected to lead to better convergence and robustness. Furthermore, the iEF method also serves as a better approximation method to the Fisher information matrix itself, which enables the improvement of a variety of Fisher-based methods, not limited to the scope of optimisation.

## 1 Introduction

Parameter optimisation is a crucial research area in the field of deep learning, where stochastic optimisers are commonly used which update the model parameters iteratively to minimise a target loss function. Approximate Natural Gradient Descent (NGD)  methods are an important family of approximate second-order optimisers, which pre-condition the gradient with the (approximate) Fisher information matrix (also called the Fisher matrix) to accelerate training or improve generalisation.

Although there are many successful optimisers based on approximate NGD, many of them in fact use the empirical Fisher (EF) as a pre-conditioner. These methods are referred to as approximate empirical NGD methods . The EF method constructs an approximation to the exact Fisher matrix directly from the gradients of training samples, which are usually readily computed during thetraining process . In contrast, the exact Fisher matrix needs to be either sampled from the model output distribution , or requires repeated evaluation of the matrix-vector product with the Fisher matrix , which are both expensive operations. As a result, due to the ease of implementation brought by EF, empirical NGD is used in many approximate NGD optimisers as the default choice [33; 54; 43; 10; 11; 52; 51].

Despite the prevalence of empirical NGD optimisers, it is known that EF is in general a questionable approximation to the exact Fisher matrix [26; 20; 47]. The poor approximation quality of EF-based updates has been experimentally verified for small-scale experimental setups by [20; 47]. However, traditional evaluation methods are used in [20; 47] where the exact NG update and Fisher matrix need to be explicitly computed, making their findings impossible to verify for large deep learning setups for practical tasks. Hence, a more generally applicable evaluation framework is needed. There is also a need for an improved approximation to the exact Fisher matrix (as a pre-conditioner) than EF, while being as efficient to implement. This paper aims to fill these gaps.

Our Contributions:In this paper, an improved EF (iEF) approximation for NGD is proposed, which provides a better approximation to the exact Natural Gradient (NG) updates, while maintaining the practical convenience of EF. This method allows for a straightforward upgrade for all existing approximate empirical NGD optimisers. To achieve this, a theoretical investigation into the behaviour of EF update is first carried out, where the impact of the EF update on each involved sample is analysed (the "involved samples" refers to training samples whose gradient is used to construct the EF update). This leads to finding the _inversely-scaled projection_ issue of EF (see Sec. 4). Accordingly, the iEF method is proposed to overcome this issue by introducing a diagonal scaling matrix to the standard formulation of the EF pre-conditioner. It is motivated as an approximate Gauss-Newton algorithm from a loss reduction perspective, with global convergence guarantees under mild assumptions (see Sec. 5). A novel empirical evaluation framework for approximate NGD methods is then proposed to enable accurate comparison of approximate Fisher pre-conditioners (_e.g._ EF and iEF) in large-scale optimisation setups (see Sec. 6). We conducted experiments that compare the exact EF and iEF methods in a range of practical deep learning setups including computer vision and fine-tuning large language models. Under our evaluation framework, iEF demonstrates better approximation quality to exact NG updates than both EF and the more expensive Monte-Carlo sampled Fisher method (SF, see Appendix E), meanwhile being significantly more robust to the choice of damping across tasks and training stages. Direct application of iEF as optimiser also shows consistently strong generalisation and convergence, even when compared to well-tuned AdamW/Adafactor baselines (see Sec. 7).

## 2 Related Work

Approximate (Empirical) NGD:There are many existing approximate (empirical) NGD methods, most of which use EF despite its theoretical limitations. Some prior work, _e.g._[43; 40], uses the Woodbury identities  to exactly compute EF updates. Recent block-diagonal methods (based on K-FAC ) have gained popularity due to their efficiency, which includes work that modify the K-FAC approximation [54; 41; 10; 4; 52; 3] or distributively apply K-FAC as optimisers [34; 49]. Sometimes Adagrad-based methods [9; 13; 17] are also regarded as empirical NGD methods. However, the connection is questionable  as these methods use the _square-root_ of the EF matrix, instead of the EF matrix itself, as a pre-conditioner.

Limitations of EF Approximation:The limitations of EF as an approximate Fisher matrix have been discussed and demonstrated in several papers [26; 20; 47], among which  provided a thorough review and analysis. However, as far as we are aware, there has been no prior work that analysed the exact EF method in larger deep-learning setups, and most of the observations are limited to small-scale problems for theoretical machine-learning studies. It is known, however, that practical EF-based optimisers usually require a sophisticated damping scheme to work well [33; 31]. It has even been suggested that an infinitely large damping should be used with the gradient covariance term [4; 35]. These observations can be tied to the theoretical limitations of EF.

Empirical Evaluation of Approximate NGD Quality:An accurate evaluation of the approximation quality to exact NG updates is of great importance for approximate NGD methods. Usually, the performance of the method of interest is evaluated on machine learning benchmarks [54; 27; 11], which provide crucial information from the optimisation perspective. However, limited information about the approximation quality to exact NGD can be drawn from these experiments. Therefore,additional small-scale experiments are usually performed to compare against the exact Fisher matrices [47; 27; 11], or the exact NG updates [20; 41; 3], which are extremely difficult to do for commonplace large-scale models. This limits our understanding of these methods in the context of large-scale tasks.

## 3 Preliminaries

Supervised Learning for Classification Model with Softmax Activation:This paper considers supervised learning of categorical classification, where a probabilistic model is trained to predict outputs \(y\{c|c=1,2, C\}\) of \(C\) categories from inputs \(\). The target model \(=f_{}()\) has \(^{P}\) as the model parameters, which outputs the logits \(^{C}\). Assume a softmax activation is used on the logits, the model can be expressed as a conditional probability of \(p_{}(y|)\). Given \(N\)_i.i.d._ training samples \((_{n},y_{n})_{n=1}^{N}\) (assuming \(N P\)), the following accumulated loss is minimised

\[()=_{n}- p_{}(y=y_{n}|_{n})=_{n}l_{n},\] (1)

where \(l_{n}=- p_{}(y=y_{n}|_{n})\) is the categorical cross-entropy loss for the \(n\)-th training sample. For brevity, we denote \(p_{}(y=c|_{n})=p_{n}(c)\).

A vectorised representation of loss \(^{N}\) is used where \(=[l_{1},l_{2},,l_{N}]^{}\). The accumulated loss then becomes \(()=_{n}l_{n}=^{}\) where \(\) is an all 1 column vector of matching dimension, and the accumulated gradient can be re-written as \(_{}()=_{}^{ }\) where \(_{}^{N P}\) is the Jacobian of per-sample losses _w.r.t._ model parameters.

NGD and Empirical NGDIn a first-order optimisation method, say SGD , the update direction on the model parameter is the estimate of the accumulated gradient \(_{}()\). In the NGD method , the gradient is pre-conditioned by the Fisher matrix \(\) (_i.e._\(^{-1}_{}()\)) to accelerate convergence. The exact Fisher matrix can be computed from the model output distribution using available training samples as follows

\[:=_{n}_{c}p_{n}(c)[_{} p_{n}(c)_{} p_{n}(c)^{}].\] (2)

The Fisher matrix can be estimated with Monte-Carlo (MC) sampling . This approximation method is usually used with one MC sample per training sample, which is termed SF in this paper (see Appendix E). Alternatively, when the model is well trained and \(p_{n}(y_{n}) 1\) for all \(N\) samples, it is possible to approximate the exact Fisher with EF using the empirical gradient as follows

\[}:=_{n}[_{} p_{n}(y_{ n})_{} p_{n}(y_{n})^{}]=_{} ^{}_{}.\] (3)

Pre-conditioning the gradient with the EF matrix (_i.e._\(}^{-1}_{}()\)) yields the empirical NGD method. Although empirical NGD is prevalent due to the convenience of computing the EF matrix in practice, the approximation quality of EF to the exact Fisher matrix is worth questioning [20; 47].

## 4 Inversely-Scaled Projection Issue of Empirical Fisher

Despite the practical prevalence of the EF method, it is generally believed to be a poor approximation of the exact NGD method . To better understand the cause of the limited approximation quality of the EF method, an analysis of the impact of the EF update on each of the involved samples is presented below. This leads to finding the "_inversely-scaled projection_ issue" of the EF method, which provides a focus for the improvement of the EF method.

### Formal Definition

Recall the definition of EF in Eqn. (3). The empirical NG update (or just the EF update) can be defined as follows

\[_{}=-\,}^{-1}_{}()=-\,(_{}^{} _{}+)^{-1}(_{}^{})\] (4)

where \(^{+}\) is a small damping factor to facilitate inversion (gradient covariance matrix \(_{}^{}_{}^{P  P}\) cannot be directly inverted for over-parameterised models). Using the Woodbury identity , the EF update can be re-expressed as follows:

\[_{}=-\,_{}^{}(_{ }_{}^{}+)^{-1} .\] (5)The loss change induced on each sample (denoted as \( l_{}\)) when applying the EF update to the model can be estimated using the Jacobian \(_{}\) as follows:

\[_{}=-_{}_{ }=-\,_{}_{}^{}(_{ }_{}^{}+)^{-1}-,\]

This result means that EF updates have the property of inducing an equal loss reduction on every involved sample. For the \(n\)-th sample, the projection of the EF update onto gradient direction \(_{}l_{n}\) (denoted as \((_{n})_{}\)) can be computed as follows

\[(_{n})_{}=_{}^{}}l_{n}}{\|_{}l_{n}\|_{2}}=-}l_{n}\|_{2}},\] (6)

where \(\|_{}l_{n}\|_{2}\) denotes the \(l_{2}\) norm of the \(n\)-th per-sample gradient. This means that the projection of EF update onto every sample gradient is inversely proportional to the gradient norm of each sample. Note that a smaller \(\|_{}l_{n}\|_{2}\) generally indicates the sample is better trained (or more converged, or closer to its minimum). The EF update is therefore easily biased towards well-trained samples, and tends to have a larger norm as training progresses (\(\|_{}l_{n}\|_{2}\) decreases) . We term this the _inversely-scaled projection_ issue of the EF update, which is further illustrated in the following section.

### Visual Illustration

The detrimental impact of the _inversely-scaled projection_ issue of EF updates is illustrated in a 2-parameter 2-datum linear least-square regression problem in Fig. 1 (third plot). It is shown that EF updates are "attracted" to the minimum of each training sample (the dashed lines), leading to a distorted update vector field and inefficient training trajectories. Also, EF updates have a much larger norm when either training sample is nearly converged, suggesting the necessity of a complicated step-size scheduler. Please refer to Appendix B for a detailed description and discussion, which also includes an additional visualisation for a logistic regression setup in Fig. 4 which leads to similar observations. These effects of the _inversely-scaled projection_ issue are further validated in experiments (E1) and (E2) in large-scale deep learning setups in Sec. 7.

## 5 Improved Empirical Fisher

The EF method is a widely used approximate NGD method, mainly because it can be implemented conveniently by constructing the EF matrix with the per-sample gradients that are readily computed during backpropagation. In this section, we propose the improved EF (iEF) method which preserves the implementational convenience of the EF method, meanwhile alleviating the _inversely-scaled projection_ issue. The iEF method can be justified as an approximate (generalised) NGD method from a loss reduction perspective. Continuous-time convergence analyses also show that the iEF method guarantees sub-linear/linear convergence to the global minimum under mild assumptions.

Figure 1: A visual comparison of Fisher, iEF and EF as pre-conditioners for a 2-parameter 2-datum linear least-squares regression problem inspired by  (see Appendix B for details). All three plots are loss landscapes with the \(x\)-axis and \(y\)-axis representing \(_{0}\) and \(_{1}\) respectively. The first plot shows the gradient vector field of the loss function and 5 sampled training trajectories for SGD updates. Similarly, the second plot is for NGD/iEF updates and the third plot is for EF updates (with a zoomed view). The global minimum (0, 0) is marked with a star where visible. The two dashed lines on all plots represent the optimal parameter sets for each training sample. It can be seen that the EF method has a highly distorted update vector field while the iEF and NGD methods adapt to the curvature of the problem successfully.

### Update Formulation

The nature of the _inversely-scaled projection_ issue is that the EF update enforces a constant loss reduction regardless of the convergence level of each sample (see Sec. 4.1). To address this issue, the iEF update is designed to induce a per-sample loss reduction that takes into account the convergence level. The loss reduction induced by the iEF update for the \(n\)-th sample is designed to be

\[(l_{n})_{}=_{}l_{n}^{} _{}-\,\|_{_{n}}l _{n}\|_{2}^{2}.\] (7)

where \(\|_{_{n}}l_{n}\|_{2}\) is the gradient norm at the model output logits-level. Note that \(\|_{_{n}}l_{n}\|_{2}\) in general decreases as the \(n\)-th sample gets better trained because of the positive convexity of the objective of interest (cross-entropy with softmax activation). Therefore, this update formulation allows the induced per-sample loss reduction by the iEF update to be closely related to how well a sample has converged, which then greatly alleviates the _inversely-scaled projection_ issue of the EF method.

A viable formulation of the iEF update \(_{}\) that both satisfies Eqn. (7) and relies only on the per-sample gradients is proposed as follows

\[_{}=-\,_{} ^{}(_{}_{ }^{}+)^{-1} {s}_{},\] (8)

where \(_{}^{N}\) is a scaling vector defined as

\[_{}=[\|_{_{1}}l_{1}\|_{2}^{2 }\|_{_{2}}l_{2}\|_{2}^{2}\|_{ _{N}}l_{N}\|_{2}^{2}]^{}.\]

which can be obtained along with back-propagation (_e.g._ in Pytorch ) with negligible overhead. This improved formulation for EF is shown to be effective. In the toy examples in Fig. 1 and 4, switching from EF to iEF completely removes the distortion in the EF update vector fields. Results in Sec. 7 also validate that iEF achieves consistently better approximation quality to NG updates than both EF and SF methods in practical deep learning setups (experiment (E1)), meanwhile being robust to the choice of damping \(\) across tasks and training stages (experiment (E3)).

### Theoretical Connection to Generalised NGD

The choice of scaling vector \(_{}\) is motivated by the Gauss-Newton (GN) algorithm, which is a type of generalised NGD method . The update for the GN algorithm is defined as

\[_{}=-\,}^{-1}_{ }(),\] (9)

where \(}=_{n}_{}_{n}^{} _{}_{n}\) is the GN matrix. The GN algorithm can be effectively viewed as a gradient descent method on the model output logits space (\(_{n}\)-space) and the loss reduction induced for the \(n\)-th sample by the GN update is approximately

\[(l_{n})_{}-\,\|_{_{n}}l_{n}\|_{ 2}^{2},\] (10)

which takes exactly the same form as the per-sample loss reduction induced by the iEF update (see Eqn. (7)). Therefore, the iEF method can be regarded as an efficient approximation to the GN algorithm in terms of its loss-reduction behaviour. In particular, it can be shown that the iEF method is equivalent to the GN algorithm for all supervised learning problems with a regression model and the exact NGD method for the least-squares regression problem (see Appendix A).

### Convergence Analysis

In this section, two continuous time convergence analyses are provided for the non-stochastic version of the iEF method, which shows its sub-linear or linear global convergence guarantee for different types of objective functions (see Appendix C for proofs). The analysis can be considered as extensions of proofs provided in  to setups using non-regression models and cross-entropy objectives. The two base assumptions used by the two convergence analysis are as follows:

**Assumption 5.1**.: At time \(t\), the full-batch, un-damped iEF update to model parameters \((t)\) is

\[(t)}{t}=-_{(t)}(t)^{}[_{(t)} (t)_{(t)}(t)^{}]^{-1 }_{}(t),\] (11)

**Assumption 5.2**.: \( t>0\), the gradient covariance matrix (or Gram matrix) \([_{(t)}(t)][_{(t)}(t)]^{}\) is always full rank.

The two main conclusions of the analysis are described below.

Sub-linear Global Convergence for Softmax + Cross-Entropy ObjectiveWhen the target model uses softmax output and cross-entropy loss (as described in Sec. 3), the Theorem 5.3 can be proved.

**Theorem 5.3**.: _Suppose Assumption 5.2 holds, \( n\{1,,N\}\), the target probability \(_{n}(t):=p_{(t)}(y=y_{n}|_{n})\) for the \(n\)-th training sample is bounded as follows_

\[_{n}(t)>1-+1},\] (12)

_where \(C_{0}=_{n}(0)}+_{n}(0)}{1-_{n}(0)}\) and \(t>max\{-1-C_{0},0\}\)._

Linear Global Convergence for Strongly Convex ObjectiveWhen the target model uses an \(m\)-strongly convex objective function  (see Assumption C.2, note that cross-entropy loss does not satisfy this assumption), the Theorem 5.4 can be proved.

**Theorem 5.4**.: _Suppose Assumption 5.2 and C.2 holds, \( n\{1,,N\}\), the per-sample loss \(l_{n}(t)\) for the \(n\)-th training sample is bounded as follows_

\[l_{n}(t)-l_{n}^{} e^{-2mt}(l_{n}(0)-l_{n}^{}),\] (13)

_where \(l_{n}^{}\) is the minimum loss for the \(n\)-th sample._

**Remark:** Theorem 5.4 only assumes a strongly-convex target objective _w.r.t_ model output (Assumption C.2). The target loss landscape _w.r.t_ model parameters can still be arbitrarily non-convex depending on the target model structure.

### Applications of IEF

As an approximate NGD method, the exact iEF method can be used directly as an optimiser (see Algorithm 1) for models with a small parameter size. Its performance is evaluated in experiment (E2) in Sec. 7, which demonstrates competitive convergence and generalisation when compared to well-tuned baselines. Refer to Appendix. D.1 for discussions on the implementation and complexity.

More importantly, the iEF method provides an improved approximation method to the exact Fisher matrix. The iEF approximated Fisher matrix (iEF matrix) \(}^{}^{P P}\) takes the following form

\[}^{}=_{}^{} (_{})^{-1}_{} ,\] (14)

which can be derived from Eqn. (8) (see Appendix D.2.1). \(}^{}\) by design takes a highly similar form to the EF matrix (see Eqn 3), making them equally convenient to compute. Also, results in Sec. 7 show that updates preconditioned with the iEF matrix achieve consistently better approximation quality to NG updates than both EF and SF updates, meanwhile obviating the need for damping tuning. Consequently, the iEF matrix can be considered as a cheap yet better approximation method for the Fisher matrix than both the EF and SF methods, which opens up the possibility of improving a wide range of Fisher-based methods (not limited to optimisation methods). An example is provided in Appendix D.2.2 to demonstrate that iEF can be easily integrated into the popular empirical K-FAC optimiser . Preliminary experimental results show that the integration leads to consistent improvements of the approximation quality to exact NG updates. Another example is provided in Appendix D.2.3 to demonstrate that iEF can be directly applied to improve the EF approximated Hessian used in the WoodFisher algorithms for model compression .

## 6 Empirical Evaluation Framework for Approximate NGD Methods

Traditional evaluation methods for quality of approximate NGD methods have high memory and time complexity, which is infeasible for large setups (see discussion in Sec. 2). In order to accurately evaluate the quality of approximate NGD methods (EF, iEF, SF _etc._) in practical deep-learning setups, we introduce an efficient empirical evaluation framework which enables a quantitative comparison of different approximate NGD methods under large-scale setups. For a given approximate NGD method that generates an update \(\), our proposed evaluation framework satisfies the following requirements: **1)** provides a quantitative evaluator \(()\) that measures the (direction-wise) approximation quality to the exact NG update; **2)** the evaluation process is efficient in modern auto-grad frameworks, and it poses no constraints on the size or structure of the target model. The implementation and theoretical motivations of this empirical evaluation framework are discussed in the following sections.

### Efficient Indicator of Approximation Quality

The proposed evaluation framework revolves around the indicator \(()\) which is designed to accurately reflect the quality of an approximate NG update, while being efficient to compute. For an update \(\) of interest, the proposed indicator \(()^{+}\) is defined as

\[()=^{})^{}}{|^{}_{}()|},\] (15)

and the smaller the value of \(()\), the better the approximation quality of \(\) to the exact NG update. This indicator mainly requires computing a matrix-vector product with the exact Fisher matrix (_i.e._\(\)), which can be efficiently done in modern auto-grad frameworks . This allows for the application of this framework to large-scale models in practical setups. Refer to Appendix F.1 for implementation details, algorithm complexity and a comparison with traditional methods.

### Theoretical Motivation

In this section, the proposed indicator \(()\) is justified as a theoretically appropriate evaluator of the quality of an approximate NG update. An alternative definition for the NGD is first proposed, which formulates the NG update direction with an unconstrained optimisation problem as

\[^{{}^{}}^{-1}_{}()=*{arg\,min}_{}()^{2},\] (16)

where \(^{{}^{}}\) is an arbitrary non-zero scalar. It is shown that this alternative definition for NGD is implicitly used in the Hessian-free method  and the linear conjugate gradient (CG) algorithm used in Hessian-free to solve for the exact NG update is a locally optimal minimiser for \(()^{2}\) (see Appendix F.2 for proof). Under this definition, any approximate NG update with a smaller \(()^{2}\) is a "strictly better approximation" to the exact NG update (which is the minimiser for \(()^{2}\)).

Furthermore, \(()\) can also be justified from a second-order optimisation perspective. \()^{2}}\) is shown to quantify the maximum achievable loss reduction for a given update direction under a local quadratic approximation of the loss function (see Appendix. F.3 for proof). Consequently, the proposed indicator can be used to accurately predict the convergence ability of a target update generation method (see experiment (E2) in Sec. 7).

## 7 Experiments

Experimental results are presented in this section. The main goal of the experiments is to verify that the behaviour of _exact_ EF and iEF methods align with our theories in practical deep learning setups. Mainly three approximation methods are compared: EF, iEF and SF (an unbiased yet more expensive Fisher approximation method, see Appendix E). The exact updates of each method are generated based on Eqn. (5), (8), (49) respectively. Fifteen different setups are used to evaluate the optimisation performance and the approximation quality of these methods, including widely used parameter-efficient fine-tuning (PEFT) for pre-trained models. These include T5-base with LoRA and Prompt-Tuning on GLUE tasks , and ViT with LoRA for CIFAR100 . PEFT of pre-trained models is investigated because it involves large-scale practical models, while having a small trainable parameter size (the implementation of _exact_ EF, iEF and SF methods are memory intensive, see Appendix D.1). Please refer to Appendix H.1 for detailed experimental setups. The following three findings are demonstrated with our experiments.

(E1) The approximation quality (to exact NG updates) of EF, iEF, SF and SGD was evaluated and compared using the proposed evaluation framework in Sec. 6 on all setups. It is shown that iEF consistently improves on SGD updates and is superior to both EF and SF methods for the majority of the training stages for all setups.

(E2) The optimisation performance of EF, iEF, SF and SGD was evaluated on all setups. For each task, an additional well-tuned baseline optimiser (Adafactor/AdamW) was also compared. It is shown that iEF consistently achieves comparable or better performance than the corresponding baseline, while EF and SF suffer from unstable training to different extents.

(E3) The impact of damping on the approximation quality of EF, iEF and SF was analysed under the proposed evaluation framework. It is shown that the quality of traditional EF and SF methods reliesheavily on careful damping tuning, unlike iEF which works well with any near-zero damping across tasks and training stages.

Finally, results for an additional experiment considering a 10M parameter Multi-layer Perceptron (MLP) on the CIFAR10  dataset are provided in Appendix H.7. This additional experiment further validates the aforementioned findings for a train-from-scratch setup with a much larger (\(10\)) trainable parameter size.

E1: Approximation Quality to NG UpdatesThe behaviour of updates generated with EF, iEF, SF and SGD methods were compared using the proposed empirical evaluation framework in terms of their approximation quality to exact NG updates. The updates for EF, iEF and SF were generated according to Eqns. (5), (8), and (49) respectively, and the evaluation framework follows Algorithm 4. The "un-damped" behaviour of these methods is analysed and a near-zero damping factor is used for update generation. The checkpoints at the end of each epoch generated by the baseline optimisation methods (AdamW/Adafactor) for each task were used for evaluation. In each evaluation. For each checkpoint \((t)\), indicators were computed from 100 batches of randomly picked training samples of the target task of batch size \(M=160\). The averaged indicator for each update were then evaluated \(((_{}(t))\), \((_{}(t))\), \((_{}(t))\), \((_{}(t))\), which are denoted as \(_{}\), \(_{}\), \(_{}\) for simplicity). The relationship among these indicators across epochs and tasks is shown in Fig. 2. Note that results are presented for only 3 representative setups due to space limit (indicator plots for all tasks are shown in Appendix H.5.1). Three findings can be concluded from these figures: **1)** EF achieves poorer approximation quality even than SGD updates for most training stages and tasks. This is aligned with the finding in prior work that EF is a questionable approximation to SGD. **2)** The fourth plot shows that the gradient norm imbalance gets larger as training progresses. This correlates well with both the EF and SF curves, while impacting iEF less. This means that the _inversely-scaled projection_ issue indeed plays a significant role in reducing the approximation quality of the EF (and SF) approximation. **3)** Comparing the first three plots, it can be seen that, for the majority of the training stages, the approximation quality follows iEF \(>\) SF \(>\) EF. IEF gives a consistently better approximation, and EF and SF are only able to beat iEF at the start of training (where a good approximation to the NG update has less impact).

E2: Optimisation PerformanceThe exact iEF, EF and SF methods were implemented as stochastic optimisers (following Algorithms 1, 2, 3 respectively). The same near-zero damping factor was used as in (E1). The averaged test metrics for GLUE and CIFAR100 for each optimiser are shown in Table 1 (see full test results in Table 7, validation result in Table 6, final training loss in Table 5 and training curves in Fig. 12 and 13). The following three observations can be made:

**1)** From the final training loss reported in Table 5, the ranking of final training loss generally follows iEF \(<\) AdamW/Adafactor \(<\) SGD \(<\) SF \(<\) EF (the lower the better). This ranking of training

Figure 2: Four (log-scaled) ratios computed for checkpoints at various stages of training (sampled at the interval of one epoch) for 3 of the all 15 tasks. The \(x\)-axes represent the training stages of the model. \(0\%\) means the initialised model and \(100\%\) means model at the end of the last epoch. Each data point is averaged across 100 evaluations, and the error bars represent the standard deviation (1-sigma). The first plot shows \(_{}/_{}\), which denotes the relative approximation quality improvement of EF updates _w.r.t._ SGD updates (the lower the better). The second plot shows \(_{}/_{}\), and the third plot shows \(_{}/_{}\). The last plot depicts the _imbalance of gradient norms_, which is the average ratio between the maximum and minimum gradient norm for each evaluated batch (a larger value indicates more imbalanced per-sample gradient norms, which should lead to a more significant _inversely-scaled projection_ issue). Overall, the approximation quality follows iEF \(>\) SF \(>\) EF.

convergence follows the ranking of indicators in (E1) closely, demonstrating the effectiveness of the empirical evaluation framework in predicting the training behaviour of optimisers. **2)** For most of the tasks, EF always suffer from unstable training (see training curves in Fig. 12 and 13), while iEF consistently reaches the lowest training loss at the end of training (even when compared with well-tuned Adafactor/AdamW baselines). This further confirms the _inversely-scaled projection_ issue of EF, and demonstrates the strong convergence ability of the proposed iEF method. **3)** From test results in Table 1, it can be seen that iEF achieves the best generalisation for Prompt Tuning tasks (outperformed Adafactor in 6 out of 7 tasks). For LoRA tasks, iEF remains competitive to AdamW with each of them outperformed the other in 4 out of 8 tasks. This is likely because LoRA setups (which on average have 50 times more trainable parameters than Prompt Tuning) have a stronger reliance on regularisation and momentum, which have not been properly extended to use together with the exact iEF optimiser yet. Overall, iEF achieves the best generalisation for the majority of tasks (10 out of 15), indicating its potential as a strong optimiser for PEFT for pre-trained models.

E3: Impact of DampingAs is discussed in Sec. 2, practical approximate NGD optimisers rely heavily on a good damping schedule, which is typically chosen based on empirical experience . Using the proposed evaluation framework, it is straightforward to analyse the impact of damping on the approximation quality of EF, SF and iEF. For a target task, the indicator \(\)_w.r.t._ damping \(\) curve is computed at the start, mid-way and end of the training. Graph for an example task is shown in Fig. 3 (graphs for other tasks are provided in Appendix H.5.2). Two observations can be made:

**1)** A well-chosen damping factor significantly improves the approximation quality of EF and SF, which aligns well with observations in prior work on approximate NGD optimisers . However, the optimal damping factor changes greatly for different tasks and training stages, which makes the damping schedule for SF or EF based optimisers necessary yet hard-to-design in practice.

**2)** Across all tasks and training stages, iEF robustly achieves great approximation quality with a near-zero damping factor. More importantly, its approximation quality is consistently better than EF method, and is comparable to the optimally-damped SF method (which is much more expensive, particularly when the cost of damping tuning is considered). Overall, iEF can be considered a cheaper, higher-quality and more robust alternative to both the EF and SF approximation methods.

    & **AdamW** & **Adafactor** & **SGD** & **EF** & **SF** & **iEF** \\ 
**GLUE + T5 + Prompt Tuning** & - & \(77.1\) & \(67.4\) & \(48.1\) & 69.7 & **79.3** \\
**GLUE + T5 + LoRA** & \(\) & - & \(77.3\) & \(63.1\) & \(76.5\) & \(79.3\) \\
**CIFAR100 + ViT + LoRA** & \(93.9\) & - & \(91.3\) & \(31.0\) & \(92.8\) & **94.3** \\   

Table 1: Average test performance of different optimisers for GLUE and CIFAR100. For GLUE tasks, the average metric results for the 7 tasks are used as the final test score. For tasks with two metrics, these metrics are averaged first . For all tasks, the test result is computed for the best validation accuracy checkpoint. Refer to Table 7 for a more complete test performance report and detailed explanations on metrics.

Figure 3: Approximation quality (relative to SGD) of EF, SF and iEF methods _w.r.t._ damping factor \(\) at different training stages of task CoLA+T5+LoRA. \(x\)-axes show the value of the damping factor, \(y\)-axes depict the relative approximation quality improvement of the target update method _w.r.t._ SGD (the lower the better). Each data point is averaged across 100 evaluations, and the error-bars represent the standard deviation (1-sigma). The first plot is for checkpoint saved at the end of the first training epoch, the second plot for the mid-way epoch and the third plot for the final epoch. It can be observed that iEF achieves the best approximation quality robustly for any near-zero \(\). In contrast, \(\) has a non-linear impact on both SF and EF. When optimally tuned, an EF update can achieve better approximation quality than SGD, and an SF update can achieve comparable quality to iEF. However, the optimal damping factor for EF and SF changes greatly with training stages (and tasks).

Conclusions and Future Work

This paper presents the iEF method, which addresses the _inversely-scaled projection_ issue of the EF approximation for NGD, meanwhile maintaining the implementational convenience. A novel empirical evaluation framework for the quality of general approximate NGD update is also proposed, which enables quantified comparison of approximate NGD methods in large deep learning setups1. Based on the experiments with practical PEFT of pre-trained models for NLP and CV classification tasks, the exact iEF optimiser shows superior convergence and generalisation for majority of the tasks, supporting the applicability of iEF directly as an optimiser. Further evaluation on approximation quality concludes that iEF achieves consistently better approximation quality than both EF and SF. The iEF method also demonstrates the superior property of being robust to the choice of damping factor across different tasks and training stages.

As is discussed in Sec. 5.4, the iEF method can be viewed not only as an improved approximate NGD optimiser, but also as an improved approximation method for the exact Fisher matrix in general. This opens up many opportunities of future work to improve a wide range of Fisher-based methods (not limited to optimisation methods). Some example applications include improving the empirical K-FAC optimiser [37; 33] (which has shown promising results in preliminary experiments) and improving the WoodFisher algorithm for model compression .