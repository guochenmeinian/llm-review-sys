# GaussianCube: A Structured and Explicit

Radiance Representation for 3D Generative Modeling

 Bowen Zhang\({}^{1}\)

&Yiji Cheng\({}^{2*}\)&Jiaolong Yang\({}^{3}\)

&Chunyu Wang\({}^{3}\)

&Feng Zhao\({}^{1}\)\({}^{}\)&Yansong Tang\({}^{2}\)&Dong Chen\({}^{3}\)&Baining Guo\({}^{3}\)

\({}^{1}\)University of Science and Technology of China &\({}^{2}\)Tsinghua University &\({}^{3}\)Microsoft Research Asia

Interns at Microsoft Research Asia. \({}^{}\)Equal advising. \({}^{}\)Corresponding authors.

###### Abstract

We introduce a radiance representation that is both structured and fully explicit and thus greatly facilitates 3D generative modeling. Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods. We derive GaussianCube by first using a novel densification-constrained Gaussian fitting algorithm, which yields high-accuracy fitting using a fixed number of free Gaussians, and then rearranging these Gaussians into a predefined voxel grid via Optimal Transport. Since GaussianCube is a structured grid representation, it allows us to use standard 3D U-Net as our backbone in diffusion modeling without elaborate designs. More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude. The compactness of GaussianCube greatly eases the difficulty of 3D generative modeling. Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling. Project page: https://gaussiancube.github.io/.

## 1 Introduction

The field of 3D generation  has witnessed remarkable growth, driven by advancements in generative modeling . Most of the prior works in this domain leverage variants of Neural Radiance Field (NeRF)  as their underlying 3D representations, which typically consist of an explicit structured proxy representation and an implicit feature decoder. However, such hybrid NeRF variants exhibit degraded representation power, particularly when used for generative modeling where a single implicit feature decoder is shared across all objects. Additionally, the high computational complexity of volumetric rendering leads to both slow rendering speed and extensive memory costs.

Recently, the emergence of 3D Gaussian Splitting (GS)  has enabled improved reconstruction quality and real-time rendering capabilities . The fully explicit nature of 3DGS eliminates the need for a shared implicit decoder, providing another key advantage over NeRFs. Although 3DGS has been widely studied in scene reconstruction tasks, its spatially unstructured nature presents a significant challenge when applied to mainstream generative modeling frameworks.

To overcome these barriers, we introduce GaussianCube - an innovative radiance representation that is both structured and fully explicit, with strong fitting capabilities (see Table 1 for comparisons with prior works). The proposed approach first ensures high-accuracy fitting with a predefined number of free Gaussians, and subsequently organizes these Gaussians into a structured voxel grid. Such an explicit grid-based structure permits the seamless application of standard 3D convolutional architectures, such as U-Net, thereby eliminating the need for complex, specialized network designs  that are often necessary with unstructured or implicitly decoded representations.

Structuring 3D Gaussians without sacrificing fitting quality is not a trivial task. A naive starting point would be obtaining a fixed number of Gaussians by omitting the densification and pruning steps in GS. However, such simplification fails to lead the Gaussians close to the object surfaces and results in significant quality degradation. In contrast, we propose a _densification-constrained fitting_ strategy, which retains the original pruning process yet constrains the number of Gaussians that perform densification, ensuring the total does not exceed a predefined maximum \(N^{3}\). For the subsequent structuralization, we allocate the Gaussians across an \(N N N\) voxel grid using _Optimal Transport (OT)_. Consequently, our fitted Gaussians are systematically arranged within the voxel grid, with each voxel containing the features of a Gaussian. The proposed OT-based structuralization achieves maximal spatial correspondence, characterized by minimal total transport distances, while preserving the expressive power of 3DGS.

The structured nature of GaussianCube enables us to perform efficient 3D diffusion  modeling for the following three reasons: 1) It allows the use of standard 3D U-Net as our backbone for diffusion modeling without elaborate designs. 2) The spatial coherence of GaussianCube permits the use of standard 3D convolutions to capture the correlations among neighboring Gaussians, facilitating efficient feature extraction. 3) GaussianCube enables high-quality fitting with orders of magnitude fewer parameters than prior structured representations of similar quality. Since recent works  have demonstrated diffusion models' struggle in handling high-dimensional distributions, the compactness of GaussianCube significantly reduces the modeling difficulty of the generative framework.

We conduct comprehensive experiments to verify the efficacy of our approach. The model's capability for unconditional and class-conditioned generation is evaluated on the ShapeNet  and OmniObject3D  datasets. Both the quantitative and qualitative comparisons indicate that our model surpasses all previous methods. We also perform digital avatar generation on a synthetic avatar dataset . Our model is capable of producing high-fidelity 3D avatars conditioned on single portrait images, excelling beyond prior art in both identity preservation and detail creation. Additionally, we assess our model's capacity for the challenging text-to-3D creation task on Objaverse . Our model demonstrates competitive performance both quantitatively and qualitatively, producing results consistent with the given text prompts in just \(2.3\) seconds. All experiments show the strong capabilities of our GaussianCube and suggest its potential as a powerful and versatile 3D representation for a variety of applications. Some generated samples of our method are presented in Figure 1.

## 2 Related Work

**Radiance field representation.** Radiance fields model ray interactions with scene surfaces and can be in either implicit or explicit forms. Early works of neural radiance fields (NeRFs)  are often in an implicit form, which represents scenes without defining geometry. These works optimize a continuous scene representation using volumetric ray-marching that leads to extremely high computational costs. Recent works introduce the use of explicit proxy representation  followed by an implicit feature decoder to enable faster rendering. Recently, the 3D Gaussian Splatting methods  utilize 3D Gaussians as their underlying representation and offer impressive reconstruction quality. The fully explicit representation also provides real-time rendering speed. However, the 3D Gaussians are unstructured representation, and

 
**Representation** & **Spatially-structured** & **Fully-explicit** & **Real-time Rendering** & **Rel. Parameters** \\  Instant-NGP  & ✗ & ✗ & ✗ & \(26.63\) \\ Neural Voxels  & ✓ & ✗ & ✗ & \(145.9\) \\ Triplane  & ✓ & ✗ & ✗ & \(13.7\) \\ Gaussian Splatting  & ✗ & ✓ & ✓ & \(4.0\) \\ 
**Our GaussianCube** & ✓ & ✓ & ✓ & \(1.0\) \\  

Table 1: Comparison with previous 3D representations with respect to spatial structure, explicitness, real-time rendering capability, and relative parameter count (Rel. Parameters) for representations of comparable quality.

require per-scene optimization to achieve photo-realistic quality. In contrast, our work proposes a structured representation termed GaussianCube for 3D generative tasks.

**3D generation.** Previous works of SDS-based optimization [44; 55; 67; 52; 12; 53; 70; 56] distill 2D diffusion priors  to a 3D representation with the score functions, but these works are notably time-intensive, often taking several minutes to hours. While 3D-aware GANs [8; 19; 7; 21; 42; 16; 66] facilitate view-dependent image generation from single-image collections, they struggle to capture the complexity of diverse objects with intricate geometric variations . Although recent works [59; 39; 22; 57; 49; 73] have utilized diffusion models with structured proxy representations for 3D generation, the use of a shared implicit feature decoder across different assets restricts expressiveness and the computational demands of NeRF hinder efficient training. In contrast, we introduce a structured and fully explicit radiance representation for 3D generative modeling, building upon 3DGS . A concurrent work of  includes elaborate designs to form the Gaussians into volumetric representation during fitting, yet does not thoroughly address global correspondence. In contrast, our approach only restricts the total count of Gaussians while allowing freedom in their spatial distribution during the fitting. We then organize these Gaussians into a voxel grid using Optimal Transport, which yields a spatially coherent arrangement with minimal global offset cost, effectively easing the difficulty of generative modeling.

## 3 Method

Following prior works, our framework comprises two primary stages as shown in Figure 2: representation construction and diffusion modeling. In representation construction phase, we first apply a densification-constrained 3DGS fitting algorithm for each object to obtain a constant number of Gaussians. These Gaussians are then organized into the proposed spatially structured GaussianCube

Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).

via Optimal Transport between the positions of Gaussians and centers of a predefined voxel grid. For diffusion modeling, we train a 3D diffusion model to learn the distribution of GaussianCubes. We will detail our designs for each stage subsequently.

### Representation Construction

We build our GaussianCube upon 3DGS, an explicit representation that offers impressive fitting quality and real-time rendering speed. However, it fails to yield Gaussians of fixed length since the adaptive density control during GS fitting can lead to a varying number of Gaussians for different objects. Furthermore, the lack of a predetermined spatial ordering for Gaussians leads to a disorganized spatial structure. These aspects pose significant challenges to 3D generative modeling. To overcome these obstacles, we first introduce our densification-constrained fitting strategy to obtain a fixed number of free Gaussians. Then, we systematically arrange the resulting Gaussians within a predefined voxel grid via Optimal Transport, thereby achieving a structured and explicit radiance representation.

Formally, a 3D asset is represented by a collection of 3D Gaussians as introduced in Gaussian Splitting . The geometry of the \(i\)-th 3D Gaussian \(_{i}\) is given by

\[_{i}()=(-(-_{i})^{ }_{i}^{-1}(-_{i})),\] (1)

where \(_{i}^{3}\) is the center of the Gaussian and \(_{i}^{3 3}\) is the covariance matrix defining the shape and size, which can be decomposed into a quaternion \(_{i}^{4}\) and a vector \(_{i}^{3}\) for rotation and scaling, respectively. Moreover, each Gaussian \(_{i}\) have an opacity value \(_{i}\) and a color feature \(_{i}^{3}\) for rendering. Combining them together, the \(C\)-channel feature vector \(_{i}=\{_{i},_{i},_{i},_{i},_{i}\} ^{C}\) fully characterizes the Gaussian \(_{i}\).

**Densification-constrained fitting**. Our approach begins with the aim of maintaining a constant number of Gaussians \(^{N_{} C}\) across different objects during the fitting. A simplistic approach might involve omitting the densification and pruning in the original GS. However, we argue that such simplifications significantly harm the fitting quality, with empirical evidence shown in Table 6. Instead, we propose to retain the pruning process while imposing a new constraint on the densification phase as shown in Figure 3 (a). The fitting process encompasses several distinct stages: 1) Densification Detection: Assuming the current iteration includes \(N_{}\) Gaussians, we identify densification candidates by selecting those with view-space position gradient magnitudes exceeding a predefined threshold \(\). We denote the number of candidates as \(N_{d}\). 2) Candidate sampling: To prevent exceeding the predefined maximum of \(N_{}\) Gaussians, we select \((N_{}-N_{},N_{d})\) Gaussians with the largest view-space positional gradients from the candidates for densification. 3) Densification: We modify the densification approach by alternating between cloning and splitting actions into separate steps. 4) Pruning Detection and Pruning: We identify and remove the Gaussians with \(\) less than a small threshold \(\). After completing the fitting process, we pad Gaussians with \(=0\) to reach the target count of \(N_{}\) without affecting the rendering results. Benefiting from our proposed strategy, we

Figure 2: **Overall framework. Our framework comprises two main stages of representation construction and 3D diffusion. In the representation construction stage, given multi-view renderings of a 3D asset, we perform _densification-constrained fitting_ to obtain 3D Gaussians with constant numbers. Subsequently, the Gaussians are structured into GaussianCube via _Optimal Transport_. In the 3D diffusion stage, our _3D diffusion model_ is trained to generate GaussianCube from Gaussian noise.**

attain a high-quality representation with orders of magnitude fewer parameters compared to existing works of similar quality, which significantly reduces the modeling difficulty for the diffusion models.

**Gaussian structuralization via Optimal Transport**. To further organize the obtained Gaussians into a spatially structured representation for 3D generative modeling, we propose to map the Gaussians to a predefined structured voxel grid \(^{N_{v} N_{v} N_{v} C}\) where \(N_{v}=}}\). Intuitively, we aim to "move" each Gaussian into a voxel while preserving their geometric relations as much as possible. While naive approaches such as nearest neighbor transport fall short in conserving these relations due to disregard for global arrangement with evidence shown in Figure 10, we formulate this as an Optimal Transport (OT) problem [58; 4] between the Gaussians' spatial positions \(\{_{i},i=1,,N_{}\}\) and the voxel grid centers \(\{_{j},j=1,,N_{}\}\). Let \(\) be a distance matrix with \(_{ij}\) being the moving distance between \(_{i}\) and \(_{j}\), i.e., \(_{ij}=\|_{i}-_{j}\|^{2}\). The transport plan is represented by a binary matrix \(^{N_{} N_{}}\), and the optimal transport plan is given by:

\[}{}&_{i=1} ^{N_{}}_{j=1}^{N_{}}_{ij}_{ij}\\ &_{j=1}^{N_{}}_{ij}=1& i\{1, ,N_{}\}\\ &_{i=1}^{N_{}}_{ij}=1& j\{1,,N_{ {max}}\}\\ &_{ij}\{0,1\}&(i,j)\{1,,N_{}\}\{1,,N_{}\}.\] (2)

The solution is a bijective transport plan \(^{*}\) that minimizes the total transport distances. We employ the Jonker-Volgenant algorithm  to solve the OT problem. We provide a 2D illustration in Figure 3 (b). We organize the Gaussians according to the solutions, with the \(j\)-th voxel encapsulating the feature vector of the corresponding Gaussian \(_{k}=\{_{k}-_{j},_{k},_{k},_{k}, _{k}\}^{C}\), where \(k\) is determined by the optimal transport plan (_i.e._, \(^{*}_{kj}=1\)). Note that we replace the original Gaussian positions with offsets of the current voxel center to reduce the solution space for diffusion models. As a result, our fitted Gaussians are systematically arranged within a voxel grid \(\) and preserve the spatial correspondence of neighboring Gaussians, which further facilitates generative modeling.

### 3D Diffusion on GaussianCube

We now introduce our 3D diffusion model incorporated with the proposed expressive, efficient and spatially structured representation. After organizing the fitted Gaussians \(\) into GaussianCube \(\) for each object, we aim to model the distribution of GaussianCube, _i.e._, \(p()\).

Formally, the generation procedure can be formulated into the inversion of a discrete-time Markov forward process. During the forward phase, we gradually add noise to \(_{0} p()\) and obtain a sequence of increasingly noisy samples \(\{_{t}|t[0,T]\}\) according to \(_{t}:=_{t}_{0}+_{t}\), where \((,)\) represents the added Gaussian noise, and \(_{t},_{t}\) constitute the noise schedule. As a result, \(_{T}\) will finally reach isotropic Gaussian noise after sufficient destruction steps. By reversing the above process, we are able to perform the generation process by gradually denoise the sample starting from pure Gaussian noise \(_{T}(,)\) until reaching \(_{0}\). Our diffusion model is trained to denoise \(_{t}\) into \(_{0}\) for each timestep \(t\), facilitating both unconditional and conditional generation.

Figure 3: **Illustration of representation construction.** First, we perform densification-constrained fitting to yield a fixed number of Gaussians, as shown in (a). We then employ Optimal Transport to organize the resultant Gaussians into a voxel grid. A 2D illustration of this process is presented in (b).

**Model architecture.** Thanks to the spatially structured organization of the proposed GaussianCube, standard 3D convolution is sufficient to effectively extract and aggregate the features of neighboring Gaussians without elaborate designs. We leverage the standard U-Net network for diffusion  and simply replace the original 2D operators including convolution, attention, upsampling and downsampling with their 3D counterparts.

**Conditioning mechanism.** Our model supports a variety of condition signals to control the generation process. When performing class-conditioned diffusion modeling, we employ adaptive group normalization (AdaGN)  to inject the class labels into our model. For image-conditioned digital avatar creation, we leverage a pretrained vision transformer  to encode the conditional image into a sequence of feature tokens. We subsequently adopt cross-attention to make the model learn the correspondence between 3D activations and 2D image feature tokens following . We also leverage cross-attention as our condition mechanism when creating 3D objects from text, similar to previous text-to-image diffusion models .

**Training objective.** In our 3D diffusion training, we parameterize our model \(}_{}\) to predict the noise-free input \(_{0}\) using:

\[_{}\,=_{t,_{0},}[ \|}_{}(_{t}_{0}+_{t},t,_{})-_{0}\|_{2}^{2}],\] (3)

where the condition signal \(_{}\) is only needed when training conditional diffusion models. We additionally impose image-level supervision to improve the rendering quality of generated GaussianCube, which has been demonstrated to effectively enhance the perceptual details in previous works . Specifically, we penalize the discrepancy between the rasterized images \(I_{}^{t}\) of the model prediction at timestep \(t\) and the ground-truth images \(I_{}\) using:

\[_{}\,=_{I_{}^{t}}\,(_{l} \|^{l}(I_{}^{t})-^{l}(I_{} )\|_{2}^{2})+_{I_{}^{t}}(\|I_ {}^{t}-I_{}\|_{2}),\] (4)

where \(^{l}\) is the multi-resolution feature extracted using the pre-trained VGG . Benefiting from the efficiency of both rendering speed and memory costs from GS , we are able to perform fast training with high-resolution renderings. Our overall training loss can be formulated as:

\[=_{}+_{},\] (5)

where \(\) is a balancing weight.

 
**Representation** & **Spatially-structured** & **PSNR\(\)** & **LPIPS\(\)** & **SSIM\(\)** & **Rel. Speed\(\)** & **Params (M)\(\)** \\  Instant-NGP & ✗ & 33.98 & 0.0366 & 0.9809 & \(1\) & 12.25 \\ Gaussian Splitting & ✗ & **35.32** & **0.0303** & **0.9874** & \(2.58\) & 1.84 \\  Vocks & ✓ & 31.78 & 0.0676 & 0.9664 & \(0.15\) & 67.12 \\ Voxels\({}^{*}\) & ✓ & 30.25 & 0.0926 & 0.9541 & \(0.15\) & 67.12 \\ Triplane & ✓ & 32.61 & 0.0611 & 0.9709 & \(1.05\) & 6.30 \\ Triplane\({}^{*}\) & ✓ & 31.39 & 0.0759 & 0.9635 & \(1.05\) & 6.30 \\
**Our GaussianCube** & ✓ & 34.94 & 0.0347 & 0.9863 & **3.33\(\)** & **0.46** \\  

Table 2: Comparison with prior 3D representations of spatial structure, fitting quality, relative fitting speed (Rel. Speed) and parameter sizes on ShapeNet Car. \({}^{*}\) denotes that the implicit feature decoder is shared across different objects. All methods are evaluated at 30K iterations.

Figure 4: Qualitative results of object fitting.

## 4 Experiments

### Dataset and Metrics

To measure the expressiveness and efficiency of various 3D representations, we fit 100 objects in ShapeNet Car  using each representation and report the PSNR, LPIPS  and Structural Similarity Index Measure (SSIM) metrics when synthesizing novel views. Furthermore, we conduct experiments of single-category unconditional generation on ShapeNet  Car and Chair, and class-conditioned generation on real-world scanned dataset OmniObject3D . We compute the FID  and KID  scores between 50K generated renderings and 50K ground-truth renderings. For image-conditioned digital avatar generation, we utilize the synthetic avatar dataset , which comprises highly-detailed 3D avatars created by synthetic pipeline. We assess the generation quality of 5K rendering from 500 test avatars and additionally include cosine similarity of identity embedding  (CSIM) to measure the ID preservation. The experiments of text-to-3D generation are performed on the large-scale challenging Objaverse dataset . We numerically evaluate the text alignment quality using CLIP score  of 300 test prompts. All images are rendered with \(512 512\) resolution. For more details of data, please refer to Appendix A.1.

Figure 5: Qualitative comparison of unconditional 3D generation on ShapeNet Car and Chair datasets. Our model is capable of generating results of complex geometry with rich details.

  
**Method** &  &  &  \\  & **FID-50K\(\)** & **KID-50K(\%)\(\)** & **FID-50K\(\)** & **KID-50K\(\)** & **FID-50K\(\)** & **KID-50K(\%)\(\)** \\  EG3D & 30.48 & 20.42 & 27.98 & 16.01 & - & - \\ GET3D & 17.15 & 9.58 & 19.24 & 10.95 & - & - \\ DiffTF & 51.88 & 41.10 & 47.08 & 31.29 & 46.06 & 22.86 \\
**Ours** & **13.01** & **8.46** & **15.99** & **9.95** & **11.62** & **2.78** \\   

Table 3: Quantitative results of unconditional generation on ShapeNet Car and Chair  and class-conditioned generation on OmniObject3D .

Figure 6: Qualitative comparison of class-conditioned 3D generation on large-vocabulary OmniObject3D . Our model is able to handle diverse distribution with semantically accurate results.

### Implementation Details

For GaussianCube construction, we set \(N_{}\) to 32,768 and \(C\) to 14 across all datasets. We perform the proposed densification-constrained fitting for 30K iterations, which requires approximately 2.67 minutes on a single V100 GPU for each object. After OT-based structuralization, we obtain \(32 32 32 14\) GaussianCube for each object. The OT-based structuralization takes around 2 minutes per object on an AMD EPYC 7763v CPU. For the 3D diffusion model, we adopt the ADM U-Net network [41; 17]. We perform full attention at the resolution of \(8^{3}\) and \(4^{3}\) within the network. The timesteps of diffusion models are set to \(1,000\) and we train the models using the cosine noise schedule  with loss weight \(\) set to \(10\). We deploy 16 Tesla V100 GPUs for the ShapeNet Car, ShapeNet Chair, OmniObject3D, and Synthetic Avatar datasets, whereas 32 Tesla V100 GPUs are used for training on the Objaverse dataset. It takes about one week to train our model on ShapeNet Car, ShapeNet Chair, and OmniObject3D, and approximately two weeks for the Synthetic Avatar and Objaverse datasets. For more training details, please refer to Appendix A.1.

### Main Results

**3D fitting.** We first evaluate our representation capability of object fitting against previous NeRF-based representations including Voxels  and Triplane , which are widely adopted in previous 3D generation works [8; 59; 5; 39; 57]. We set the representation size of Voxels and Triplane to \(128 128 128 32\) and \(256 256 32\) respectively for comparable fitting quality. We also include Instant-NGP  and original Gaussian Splatting  for reference despite their unsuitability for generative modeling due to their unstructured spatial nature. As shown in Table 2, our GaussianCube outperforms all NeRF-based representations among all metrics. Figure 3 illustrates that GaussianCube can faithfully reconstruct geometry details and intricate textures. Moreover, we achieve such high-quality fitting with orders of magnitude fewer parameters than previous structured representation due to the densification-constrained fitting, showcasing our compactness. Notably, the shared implicit feature decoder in the multi-object fitting of NeRF-based methods leads to significant decreases in quality compared to single-object fitting as evidenced in Table 2. While the fully explicit nature of GS results in no quality gap between single and multiple object fitting.

**Single-category unconditional generation.** For unconditional generation, we compare our method with the state-of-the-art 3D generation works including 3D-aware GANs [8; 19] and Triplane diffusion models . As shown in Table 3, our method surpasses all prior works in terms of both FID and KID scores and sets new records. We provide visual comparisons in Figure 5, where EG3D and DiffTF tend to generate blurry results with poor geometry, and GET3D fails to provide satisfactory textures. In contrast, our method yields high-fidelity results with authentic geometry and sharp textures.

 
**Method** & **PSNR\(\)** & **LPIPS\(\)** & **SSIM\(\)** & **CSIM\(\)** & **FID-5K\(\)** & **KID-5K(\%\(\))\(\)** \\  Rodin w/o 2D SR & 18.80 & 0.2842 & 0.7439 & 0.6594 & 32.07 & 24.78 \\ Rodin & 18.59 & 0.2821 & 0.7373 & 0.6466 & 20.02 & 9.24 \\
**Ours** & **21.87** & **0.1768** & **0.7703** & **0.7821** & **8.32** & **2.67** \\  

Table 4: Quantitative results of digital avatar creation conditioned on single portrait image.

Figure 7: Qualitative comparison of 3D avatar creation conditioned on single frontal portraits.

**Large-vocabulary class-conditioned generation.** We also compare class-conditioned generation with DiffTF  on more diverse and challenging OmniObject3D  dataset. We achieve significantly better FID and KID scores than DiffTF as shown in Table 3. Visual comparisons in Figure 6 reveal that DiffTF often struggles to create intricate geometry and detailed textures, whereas our method is able to generate objects with complex geometry and realistic textures.

**Image-conditioned avatar generation.** For 3D avatar generation conditioned on a single reference image, we compare our method with state-of-the-art Triplane diffusion models, Rodin . Our model surpasses Rodin among all evaluated metrics as shown in Table 4. Although Rodin utilizes a 2D refiner  to boost the visual quality of facial areas, which significantly compromises 3D consistency. Our model still outperforms it by direct real-3D generation. Results in Figure 7 demonstrate that our model faithfully preserves the identity, expression and accessories of the references with rich details, while Rodin struggles to provide satisfactory results even using 2D refinement.

**Text-to-3D generation.** We compare text-to-3D generation with prior arts including diffusion models [28; 57], optimization-based method  and feed-forward method . Our model achieves competitive text-3D alignment results as shown in Table 5. The visual comparison in Figure 8 shows that our model is able to create high-quality samples aligning with text prompts in just \(2.3\) seconds. DreamGaussian tends to create over-saturated results and suffers from Janus problem. VolumeDiffusion produces unsatisfactory textures with poor text alignment. Shap-E can produce semantically accurate results but struggles to generate complex geometry. LGM reconstructs 3D Gaussians from multi-view images generated by text-conditioned multi-view diffusion pipeline , whereas the inconsistency  of the generated multi-views often results in inaccurate geometric reconstruction.

### Ablation Study

We first examine the key factors in representation construction on ShapeNet Car. To spatially structure the Gaussians, a simplistic approach would be anchoring the positions of Gaussians to a predefined voxel grid while omitting densification and pruning, which leads to severe failure when fitting the objects as shown in Figure 9. Even by introducing learnable offsets to the voxel grid, the results still lack details. We observe the offsets are typically too small to effectively lead the Gaussians close to the object surfaces, which indicates the importance of densification in the fitting process. Instead, GaussianCube can capture both complex geometry and intricate details as shown in Figure 9. The numerical comparison in Table 6 also demonstrates the superior fitting quality of GaussianCube.

Figure 8: Qualitative comparison of text-to-3D generation on Objaverse . Our model is able to generate high-quality samples according to the given text prompts.

   & **DreamGaussian** & **VolumeDiffusion** & **Shap-E** & **LGM** & **Ours** \\ 
**CLIP Score\(\)** & 26.38 & 24.41 & 30.52 & 30.06 & **30.56** \\
**Inference Time (s)\(\)** & \( 120\) & 4.95 & 4.42 & **1.55** & 2.30 \\  

Table 5: Quantitative results of text-to-3D creation. Inference time is measured on a single A100 GPU. While Shape-E, LGM achieve comparable CLIP scores as ours, they either utilize millions of training data or leverage 2D diffusion prior.

We also evaluate how the representation affects 3D generative modeling on ShapeNet Car as shown in Table 6 and Figure 10. Limited by the poor fitting quality, performing diffusion modeling on voxel grid with learnable offsets leads to blurry generation results as shown in Figure 10. To validate the importance of organizing Gaussians via Optimal Transport (OT), we compare with the organization based on nearest neighbor transport. We linearly map each Gaussian's corresponding coordinates of voxel to RGB color to visualize different organizations. As shown in Figure 10 (a), our proposed OT approach yields smooth color transitions, indicating that our method successfully preserves the spatial correspondence. However, nearest neighbor results in abrupt color transitions due to their disregard for global structure. Both the quantitative results in Table 6 and visual comparisons Figure 10 indicate that our globally structured arrangement facilitates generative modeling by alleviating its complexity, successfully leading to superior generation quality.

## 5 Conclusion

We have presented GaussianCube, a structured and explicit radiance representation crafted for 3D generative models. We begin by fitting each 3D object with a constant number of Gaussians using our proposed densification-constrained fitting algorithm. We further organize the obtained Gaussians into a spatially structured representation by solving the Optimal Transport between the positions of Gaussians and the predefined voxel grid. The proposed GaussianCube is spatially structured, allowing to use standard 3D U-Net for diffusion modeling without elaborate designs. Moreover, GaussianCube can achieve high-quality fitting using much fewer parameters compared to prior works of similar quality, which further eases the difficulty of generative modeling. Our 3D diffusion models equipped with GaussianCube achieve state-of-the-art generation quality on the evaluated datasets, underscoring its potential of GaussianCube as a versatile and powerful radiance representation for 3D generation.

**Acknowledgments:** This work was supported in part by the Anhui Provincial Natural Science Foundation under Grant 2108085UD12. We acknowledge the support of GPU cluster built by MCC Lab of Information Science and Technology Institution, USTC. We also thank anonymous reviewers for their valuable comments.

   &  &  &  \\  & & **PSNR\(\)** & **LPIPS\(\)** & **SSIM\(\)** & **FID-500\(\)** & **KID-50K(\%)\(\)** \\  A. Vovel grid w/o offset & ✗ & 25.87 & 0.1228 & 0.9217 & - & - \\ B. Vovel grid w/o offset & ✗ & 30.18 & 0.0780 & 0.9628 & 40.52 & 24.35 \\  C. Ours w/o OT & ✓ & **34.94** & **0.0346** & **0.9863** & 21.41 & 14.37 \\ 
**D. Ours** & ✓ & **34.94** & **0.0346** & **0.9863** & **13.01** & **8.46** \\  

Table 6: Quantitative ablation of both representation fitting and generation quality on ShapeNet Car.

Figure 10: Visual ablation of the Gaussian organization methods and 3D generation. For visualization of Gaussian structuralization in (a), we map the coordinates of the corresponding voxel of each Gaussians to RGB values to visualize the organization. Our OT-based solution also results in the best generation quality shown in (b).

Figure 9: Qualitative ablation of representation fitting.