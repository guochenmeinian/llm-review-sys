# Causal Discovery in Semi-Stationary Time Series

Shanyun Gao

Purdue University

gao565@purdue.edu

&Raghavendra Addanki

Adobe Research

raddanki@adobe.com

&Tong Yu

Adobe Research

tyu@adobe.com

&Ryan A. Rossi

Adobe Research

ryrossi@adobe.com

&Murat Kocaoglu

Purdue University

mkocaoglu@purdue.edu

###### Abstract

Discovering causal relations from observational time series without making the stationary assumption is a significant challenge. In practice, this challenge is common in many areas, such as retail sales, transportation systems, and medical science. Here, we consider this problem for a class of non-stationary time series. The structural causal model (SCM) of this type of time series, called the _semistationary_ time series, exhibits that a finite number of different causal mechanisms occur sequentially and periodically across time. This model holds considerable practical utility because it can represent periodicity, including common occurrences such as seasonality and diurnal variation. We propose a constraint-based, non-parametric algorithm for discovering causal relations in this setting. The resulting algorithm, PCMCI\({}_{}\), can capture the alternating and recurring changes in the causal mechanisms and then identify the underlying causal graph with conditional independence (CI) tests. We show that this algorithm is sound in identifying causal relations on discrete-valued time series. We validate the algorithm with extensive experiments on continuous and discrete simulated data. We also apply our algorithm to a real-world climate dataset.

## 1 Introduction

In modern sciences, causal discovery aims to identify the collection of causal relations from observational data, as in Pearl (1980); Peters et al. (2017) and Spirtes et al. (2000). One of the most popular causal discovery approaches is the so-called constraint-based method. Constraint-based approaches assume that the probability distribution of variables is causal Markov and faithful to a directed acyclic graph called the causal graph. Given large enough data, they can then recover the corresponding Markov equivalence class by exploiting conditional independence relationships of the variables. See Peters et al. (2017). There are many constraint-based algorithms such as PC and FCI algorithms Spirtes et al. (2000). The standard assumption of these approaches is that data samples are independent and identically distributed, which makes it possible to perform CI tests. See Bergsma (2004); Zhang et al. (2012) and Shah and Peters (2020).

Recently, there have been numerous efforts to extend such constraint-based algorithms to accommodate time series data. For instance, PCMCI in Runge et al. (2019) and LPCCI in Gerhardus and Runge (2020) are the PC-based algorithms for time series. Inspired by FCI algorithms, approaches designed for time series include ANLSTM in Chu and Glymour (2008), tsFCI in Entner and Hoyer (2010) and SVAR-FCI in Malinsky and Spirtes (2018). This setup is relevant in several industrial applications since many data points have an associated time-point, such as root-cause analysis in Ikram et al. (2022). Most of the existing causal discovery algorithms make the stationary assumption.

See Chu and Glymour (2008); Hyvarinen et al. (2010); Entner and Hoyer (2010); Peters et al. (2013); Malinsky and Spirtes (2018); Runge et al. (2019); Pamili et al. (2020); Assaad et al. (2022).

Non-stationary temporal data makes causal discovery more challenging since the statistics are time-variant, and it is unreasonable to expect that the underlying causal structure is time-invariant. Identifying causal relations from non-stationary time series without imposing any restriction on the data is difficult. Here, we focus on a specific class of non-stationary time series, called the _semi-stationary_ time series, whose structural causal model (SCM) exhibits that a finite number of different causal mechanisms occur sequentially and periodically across time. One example is illustrated in Fig.(1) where the time series \(^{1}\) has three different causal mechanisms across time, shown as red edges, green edges, and blue edges, respectively. Similarly, time series \(^{2}\) has two alternative causal mechanisms. This setting holds considerable practical utility. Periodic nature is commonly observed in many real-world time series data. See Han et al. (2002); Nakamura et al. (2003); Carskadon et al. (2005) and Komarzynski et al. (2018). Here are a few additional intuitive examples: poor traffic conditions often coincide with commute time and weekends; household electricity consumption typically follows a pattern of being higher at night and lower during the daytime. Consequently, it is reasonable to expect periodic changes in the causal relations underlying this type of time series without assuming stationarity. Here, the constraint-based methods in Chu and Glymour (2008); Entner and Hoyer (2010); Malinsky and Spirtes (2018) and Runge et al. (2019), designed for stationary time series, may fail. Given observational data with periodically changing causal structures, it is hard to apply CI tests directly. Most of the other algorithms designed for non-stationary time series rely heavily on model assumptions, as in Gong et al. (2015); Pamili et al. (2020); and Huang et al. (2019). These algorithms are discussed further in the related work.

In this paper, we propose an algorithm to address this problem, namely non-parametric causal discovery in time series data with semi-stationary SCMs. The key contributions of our work are:

* We develop an algorithm to discover the causal structure from semi-stationary time series data where the underlying causal structures change periodically. Our algorithm systematically uses the PCMCI algorithm proposed for the stationary setting in Runge et al. (2019). The resulting algorithm is hence named PCMCI\({}_{}\) where \(\) denotes periodicity.
* We validate our method with synthetic simulations on both continuous-valued and discrete-valued time series, showing that our method can correctly learn the periodicity and causal mechanism of the synthetic time series.
* We utilize our method in a real-world climate application. The result reveals the potential existence of periodicity in those time series, and the stationary assumption made by previous works could be relaxed in some practical situations.

### Related Work

PCMCI has been applied in diverse domains to investigate atmospheric interactions in the biosphere, as demonstrated in Krich et al. (2020); global wildfires as explored in Qu et al. (2021), water usage as studied in Zou et al. (2022), ultra-processed food manufacturing as examined in Menegozzo et al. (2020), and causal feature selection as discussed in Peterson (2022), among other applications. See Arvind et al. (2021); Gerhardus and Runge (2020); Castri et al. (2023); Chen et al. (2023). While PCMCI has achieved considerable success, it is not without its limitations. One notable assumption that can be challenged is the concept of _causal stationary_, that is, causal relations are time-invariant. PCMCI exhibits robustness when applied to linear models with an added non-stationary trend. See also Runge et al. (2019). However, there is an ongoing exploration to enhance its performance in a wider range of non-stationary settings.

Although not as extensively as the stationary case, causal discovery in non-stationary time series has been studied by some authors. However, many of those algorithms rely on parametric assumptions such as the vector autoregressive model in Gong et al. (2015) and Malinsky and Spirtes (2019); linear and nonlinear state-space model in Huang et al. (2019). One non-parametric algorithm in the literature is CD-NOD proposed by Huang et al. (2020); Huang et al. (2020), which has been extended to recover time-varying instantaneous and lagged causal relationships. In very recent work, Fujiwara et al. (2023) proposed an algorithm JIT-LiNGAM to obtain a local approximated linear causal model combining algorithm LiNGAM and JIT framework for non-linear and non-stationary data. To the best of our knowledge, no other non-parametric approaches can discover causal relations underlying time series without assuming stationarity and can also allow for sudden changes in causal mechanisms. Our proposed approach does not directly enforce the stationary assumption on the time series. The SCM also integrates a finite set of causal mechanisms that exhibit periodic variations over time.

## 2 \(_{}\): Capturing Periodicity of the Causal Structure

In this section, we formulate the problem of learning the causal graph on multi-variate time series data when the SCM exhibits periodicity in causal mechanisms. In section 2.1 we present the necessary definitions and provide an overview of the problem setting. In section 2.2 we introduce the required assumption.

### Preliminaries

Let \((V,E)\) denote the underlying causal graph, and for each variable \(X V\), we denote the set of all incoming neighbors as parents, denoted by \((X)\).

For any two variables \(X,Y V\) and \(S V\), we denote the CI relation \(X\) is independent of \(Y\) conditioned on \(S\), by \(X\!\!\! Y S\).

For simplicity's sake, define sets: \([b]:=\{1,2,...,b\}\) and \([a,b]:=\{a,a+1,...,b\}\), where \(a,b\).

In the time series setting, let \(X_{t}^{j}^{1}\) denote the variable of \(j\)th time series at time \(t\), \(^{j}=\{X_{t}^{j}\}_{t[T]}^{T}\) denote a univariate time series and \(_{t}=\{X_{t}^{j}\}_{j[n]}^{n}\) denote a slice of all variables at time point \(t\). \(V=\{^{j}\}_{j[n]}=\{_{t}\}_{t[T]}^{n  T}\) denotes a \(n\)-variate time series. By default, we assume \(n>1\) and hence \(^{j} V\), and \(p(V) 0\), where \(p(.)\) denotes the probability or probability density.

**Definition 2.1** (_Non-Stationary_ SCM).: A Non-Stationary Structural Causal Model (SCM) is a tuple \(= V,,,\) where there exists a \(_{}^{+}\), defined as:

\[_{}_{}\{:X_{t-}^{i}(X_{t}^ {j}),i,j[n]\},\] (1)

such that with this \(_{}\), each variable \(X_{t>_{}}^{j} V\) is a deterministic function of its parent set \((X_{t>_{}}^{j}) V\) and an unobserved (exogenous) variable \(_{t>_{}}^{j}\):

\[X_{t}^{j}=f_{j,t}((X_{t}^{j}),_{t}^{j}),\ j[n],t[ _{}+1,T],\] (2)

Figure 1: Partial causal graph for 3-variate time series \(V=\{^{1},^{2},^{3}\}\) with a Semi-Stationary SCM where \(_{}=3\), \(_{1}=3\), \(_{2}=2\), \(_{3}=1\), \(=6\) and \(=6\). The first 3(=\(_{}\)) time slices \(\{_{t}\}_{1 t 3}\) are the starting points. The same color edges represent the same causal mechanism. E.g. for \(^{1}\): there are 3 (\(=_{1}\)) time partition subsets \(\{_{k}^{1}\}_{1 k 3}\). The time points \(t\) of nodes \(X_{t}^{1}\) sharing the same filling color are in the same time partition subsets. The time points \(t\) of nodes \(X_{t}^{1}\) sharing both the same filling color and the same outline shape are in the same homogenous time partition subsets (the definitions are in the supplementary material). There are 6 (\(=\)) different Markov chains in this multivariate time series \(V\), and the first element of these 6 Markov chains is shown as \(\{Z_{1}^{q}\}_{1 q 6}\) and are tinted with a gradient of blue hues. The superscript \(q\) of \(Z_{i}^{q}\) is the index of different Markov chains, whereas the subscript \(i\) denotes the running index of that specific Markov chain. For instance, \(Z_{1}^{1}\) and \(Z_{2}^{1}\) denote the first two elements of the first Markov chain, while \(Z_{1}^{2}\) and \(Z_{2}^{2}\) denote the first two elements of the second Markov chain.

and there exist at least two different time points \(t_{0},t_{1}[_{}+1,T]\) satisfying

\[f_{j,t_{0}} f_{j,t_{1}},\ \ \ j[n],\{t_{0},t_{1}\}[_{ }+1,T].\] (3)

where \(f_{j,t},f_{j,t_{0}},f_{j,t_{1}}\) and \(\{_{t}^{j}\}_{t[T]}\) are jointly independent with probability measure \(\). \(_{}\) is the finite maximal lag in terms of the causal graph \(\).

**Definition 2.2** (_Semi-Stationary_ SCM).: A Semi-Stationary SCM is a Non-Stationary SCM that additionally satisfies the following conditions. For each \(j[n],\ ^{+}\)such that:

\[a)\ f_{j,t}=f_{j,t+N},\] (4) \[b)\ (X^{j}_{t+N})=\{X^{i}_{s+N}:X^{i}_{s} (X^{j}_{t}),i[n]\},\] (5) \[c)\ _{t}^{j},_{t+N}^{j}\ \] (6)

are satisfied for all \(t[_{}+1,T],N\{N:N,t+N T\}\). This means that a finite number of causal mechanisms are repeated periodically for every univariate time series \(^{j}\) in \(V\). One example of this model is illustrated in Fig For \(^{1}\) in Fig three causal mechanisms are reiterated periodically with \(_{1}=3\), represented by red, green, and blue edges, respectively.

The minimum value that satisfies the above conditions for \(^{j}\) is defined as the _periodicity_ of \(^{j}\), denoted by \(_{j}\). Furthermore, for an \(n\)-variate time series \(V\), \(\) denotes the minimum periodicity across all time series \(^{j},j[n]\). The number of causal mechanisms occurring sequentially and periodically of univariate time series \(^{j}\) is \(_{j}\), and that number of causal mechanisms of multivariate time series \(V\) is \(\). For \(^{j}\), the causal mechanisms are associated with each variable \(X^{j}_{t}\). However, for \(V\), the causal mechanisms are related to each time slice vector \(_{t}\) as a whole. The relationship between \(\) and \(_{j}\) can be captured by:

\[=(\{_{j}:j[n]\})\] (7)

where \((.)\) is an operation to find the least common multiple between any two or more numbers. Here, \(\) is the smallest common multiple among \(\{_{1},..._{n}\}\). In Fig\(=(3,2,1)=6\).

**Definition 2.3** (_Time Partition_).: A time partition \(^{j}(T)\) of a univariate time series \(^{j}\) in a Semi-Stationary SCM with periodicity \(_{j}\) is a way of dividing all time points \(t[T]\) into a collection of non-overlapping non-empty subsets \(\{^{j}_{k}(T)\}_{k[_{j}]}\) such that:

\[^{j}_{k}(T):=\{t:_{}+1 t T,(t\ \ _{j})+1=k\}.\] (8)

where \(\) denotes the modulo operation. For instance, \(5\ \ 3=2\).

We can observe that the variables in \(\{X^{j}_{t}\}_{t^{j}_{k}(T)}\) share the same causal mechanism. Since the number of potentially different causal mechanisms of variables in \(^{j}\) is \(_{j}\), the number of such time partition subsets is \(_{j}\). For simplicity, notations \(^{j}\) and \(^{j}_{k}\) are used instead of \(^{j}(T)\) and \(^{j}_{k}(T)\). In Fig\(^{1}_{1}=\{4,7,10,13,..,4+3N,...\}\), \(^{1}_{2}=\{5,8,11,14,..,5+3N,...\}\) and \(^{1}_{3}=\{6,9,12,15,...,6+3N,...\}\) where \(N^{+}\). The nodes \(X^{j}_{t}\) are classified into their associated time partition subsets by the matching colors.

**Definition 2.4** (_Illusory Parent Sets_).: For a univariate time series \(^{j} V\) with Semi-Stationary SCM having periodicity \(_{j}>1\), parent set index \(^{j}_{k[_{j}]}\) is defined as:

\[^{j}_{k}:=\{(y_{i},_{i})\}_{i[n^{}]}(X^{j}_{t})=\{X^{y_{1}}_{t-_{1}},X^{y_{2}}_{t-_{2}},...,X^{y_{n^ {}}}_{t-_{n^{}}}\},\  t^{j}_{k}\] (9)

where \(n^{}=|(X^{j}_{t}))|\), \(_{i}\) is the time lag and \(y_{i}\) is the variable index. Given \(^{j}_{k}\), _Illusory Parent Sets_ are defined as:

\[_{k}(X^{j}_{t})=\{X^{y_{i}}_{t-_{i}}:(y_{i},_{i}) ^{j}_{k}\},\  k\{k:t^{j}_{k}\}\] (10)

Put simply, the illusory parent sets of \(X^{j}_{t}\) are the time-shift version of the parent set of other variables in \(^{j}\) that have a different causal mechanism from \(X^{j}_{t}\). Note that the illusory parent sets are constructed specifically in the Semi-Stationary SCM. For stationary SCM, there is no illusory parent set needed. To maintain consistency in notation, for time points \(t^{j}_{k}\), the notation \(_{k}(X^{j}_{t})\) can also be extended to encompass the true parent set of \(X^{j}_{t}\):

\[_{k}(X^{j}_{t}):=(X^{j}_{t}),\  t^{j}_{k}\] (11)By doing so, \((X_{f}^{j})_{k[_{ij}]}_{k}(X_{f}^{j})\). In Fig.1 by observing \((X_{f}^{1})\), we have \(_{1}^{1}=\{(1,1),(2,2)\}\); by observing \((X_{f}^{1})\), we have \(_{2}^{1}=\{(1,1),(3,1)\}\) and finally by observing \((X_{f}^{1})\), \(_{1}^{1}=\{(1,1),(1,2)\}\). Based on those indexes, \(Pa_{1}(X_{f}^{1})=\{X_{f-1}^{1},X_{f-2}^{2}\}=\{X_{6}^{1},X_{5}^{2}\}\), \(Pa_{2}(X_{f}^{1})=\{X_{7-1}^{1},X_{7-1}^{3}\}=\{X_{6}^{1},X_{6}^{3}\}\) and \(Pa_{3}(X_{f}^{1})=\{X_{7-1}^{1},X_{7-2}^{1}\}=\{X_{6}^{1},X_{5}^{1}\}\). The first one is the true parent set of \(X_{f}^{1}\) and the latter two are the illusory parent sets. The order of those parent sets is not important.

At last, we need to further define a series of Markov chains that are associated tightly with _Semi-Stationary_ SCM. The presence and characteristics of these Markov chains are thoroughly examined in the supplementary materials. The motivation behind creating such Markov chains is to introduce assumptions on them rather than directly on the original data \(V\).

**Definition 2.5** (_Time Series as a Markov Chain_).: For time series \(V\) with _Semi-Stationary_ SCM, there are (potentially) \(\) different Markov chains \(\{Z_{n}^{q}\}_{n},q[]\):

\[Z_{n}^{q}=\{_{_{}+q+(n-1)},_{_{}+q+1 +(n-1)},...,_{_{}+q-1+n}\},\]

where \(:=\{n^{+}:_{}+q-1+n T\}\), \(=+1}{}\). Note that in \(\{Z_{n}^{q}\}\), \(q\) is used to indicate a specific Markov chain, while \(n\) serves as the running index for that particular Markov chain. Such a Markov chain \(\{Z_{n}^{q}\},q[]\) exists as long as \((Z_{n}^{q}) Z_{n}^{q} Z_{n-1}^{q}\) for all \(n\). This is a finite state Markov Chain if all time series in \(V\) are discrete-valued time series. The state space of \(\{Z_{n}^{q}\}\) is the set containing all possible realization of \(\{_{_{}+q+(i-1)+(n-1)}\}_{i[],n}\). The transition probabilities between the states are the product of associated causal mechanisms based on Assumption **A2**, which is elaborated by an example in section C.1 (Eq.(10)-(11)) of the supplementary material.

### Assumptions for \(_{}\)

**A1. Sufficiency**: There are no unobserved confounders.

**A2. Causal Markov Condition**: Each variable \(X\) is independent of all its non-descendants, given its parents \((X)\) in \(\).

**A3. Faithfulness Condition (Pearl, 1980)**: Let \(P\) be a probability distribution generated by \(\). \(,P\) satisfies the Faithfulness Condition if and only if every conditional independence relation true in \(P\) is entailed by the Causal Markov Condition applied to \(\).

**A4. No Contemporaneous Causal Effects**: Edges between variables at the same time are not allowed.

**A5. Temporal Priority**: Causal relations that point from the future to the past are not permitted.

**A6. Hard Mechanism Change**: If at time points \(t_{1}\) and \(t_{2}\), the causal mechanisms of \(X_{t_{1}}^{j}\) and \(X_{t_{2}}^{j}\) are different, then their corresponding parent sets can not be transformed to each other by time shifts:

\[f_{j,t_{1}} f_{j,t_{2}}(X_{t_{2}}^{j})\{X_{s+(t _{2}-t_{1})}^{i}:X_{s}^{i}(X_{t_{1}}^{j}),i[n]\}.\]

**A7. Irreducible and Aperiodic Markov Chain**: The Markov chains \(\{Z_{n}\}\) of \(V\) are assumed to be irreducible (Serfozo, 2009): for all states \(i\) and \(j\) of \(\{Z_{n}\}\), \( n\) so that

\[p_{ij}^{(n)} p(Z_{n+1}=j|Z_{1}=i)>0\] (12)

and aperiodic(Karlin, 2014): for every state \(i\) of \(\{Z_{n}\}\), \(d(i)=1\), where the period \(d(i)\) of the state \(i\) is the greatest common divisor of all integers \(n\) for which \(p_{ii}^{(n)}>0\).

Assumptions **A1-A5** are conventional and commonly employed in causal discovery methods for time series data. On the other hand, our approach requires additional Assumptions **A6-A7** to be in place. To clarify, **A6** is essential because our method may encounter challenges in distinguishing distinct causal mechanisms for variables in \(\{X_{n}^{j}\}_{n[T]}\) if they share identical parent sets after time shifts. As for **A7**, it serves a crucial role in establishing the soundness of our algorithm.

## 3 \(_{}\) Algorithm

In this section, we propose an algorithm called \(_{}\), and in section5.1 we present a theorem demonstrating the soundness of \(_{}\) and its ability to recover the causal graph. Our algorithm PCMCI\({}_{}\) builds on the Algorithm PCMCI in Runge et al. (2019). Additional details about PCMCI are provided in the supplementary material.

**Overview of Algorithm[]**PCMCI\({}_{}\). We assume that the periodicity and time lag are upper bounded by \(_{}\) and \(_{}\) respectively. Using PCMCI Runge et al. (2019), we obtain a superset of parents for every variable \(X_{t}^{j}\) denoted by \(}(X_{t}^{j})\) (line 2). Our goal is to identify the correct set of parents along with its periodicity for every variable in \(V\). For a variable \(X_{t}^{j}\), we guess its periodicity \(\) by iterating over all possible values in \([_{}]\). Next, we construct time partition subsets \(_{k}^{j},\ k[]\) based on the guess of periodicity \(\). In each time partition subset, we maintain a parent set, denoted by \(}_{}_{}}(X_{t}^{j})\), initializing it with the superset \(}(X_{t}^{j})\). Then we test the causal relations between \(X_{t-}^{i}}_{}_{}}(X_{t}^ {j})\) and \(X_{t}^{j}\) using a CI test on the sample \(t_{k}^{j}\) (lines 6-10).

For each guess \(\), every variable in \(^{j}\) should have its estimated parent set (line 9), and there are total \(\) potentially different parent set index \(_{k[]}^{j}\) in \(^{j}\). We return an estimate \(_{j}\) that maximizes the sparsity of the causal graph (Lemma3.4). Therefore, we select the value of \([_{}]\) that minimizes the maximum value of \(|}_{}_{}}(X_{t}^{j})|,\ t[T]\) as the estimator of \(_{j}\) (line 12).

### Theoretical Guarantees

Our main theorem shows that PCMCI\({}_{}\) recovers the true causal graph on discrete data. There are three important lemmas. We provide all the detailed proof in the supplementary material.

**Theorem 3.1**.: _Let \(}\) be the estimated graph using the Algorithm PCMCI\({}_{}\). Under assumptions **A1-A7** and with an oracle (infinite sample size limit), we have that:_

\[}=\] (13)

_almost surely._

Lemma5.2 and Lemma5.3 jointly state that if CI tests are conducted on samples generated by different causal mechanisms, the obtained parent sets \(}(X_{t}^{j})\) should be the superset of the union of the true and illusory parent sets \(_{k}_{k}(X_{t}^{j})\). That is, the estimated graph should be denser than the correct graph. The true parent set can then be obtained by directly testing the independent relations between the target variable \(X_{t}^{j}\) and the variables in \(}(X_{t}^{j})\), assuming a consistent CI test. Note that the CI tests in our algorithm are assumed to be consistent given i.i.d. samples. We do not assume the consistency of CI tests with respect to semi-stationary data. Therefore, any CI tests that maintain consistency with i.i.d. samples can be seamlessly integrated into our algorithm.

**Lemma 3.2**.: _Denote that \(\{_{k}(X_{t}^{j})\}_{k[_{j}]}\) contain the true and illusory parent sets, where \(_{j}\) is the true periodicity of \(^{j}\). For any random variable \(X_{t}^{j}\) with large enough \(t\), under assumptions **A1-A7** and with an oracle (infinite sample size limit), we have:_

\[pp(X_{t}^{j}|_{k=1}^{_{j}}_{k}(X_{t}^{j})) p( X_{t}^{j}|_{k=1}^{_{j}}_{k}(X_{t}^{j}) y)=1, \  y_{k=1}^{_{j}}_{k}(X_{t}^{j})\] (14)

Here, \(p(X_{t}^{j}|_{k=1}^{_{j}}_{k}(X_{t}^{j}))=_{T }(X_{t}^{j}|_{k=1}^{_{j}}_{k}(X_{t}^{j}))\) where \((X_{t}^{j}|_{k=1}^{_{j}}_{k}(X_{t}^{j}))\) is an estimated conditional distribution using all samples \(t[_{}+1,T]\):

\[(X_{t}^{j}|_{k=1}^{_{j}}_{k}(X_{t}^{j}))=(X_{t}^{j},_{k=1}^{_{j}}_{k}(X_{t}^{j} ))}{_{t}(_{k=1}^{_{j}}_{k}(X_{t}^{j}))}.\] (15)

Proof sketch.: We argue that the estimated conditional distribution in Eq. (15) can be written as a linear combination of \((X_{t}^{j}|(X_{t}^{j}))\) where \(t_{k}^{j},k[_{j}]\), i.e., as a mixture of conditional distributions. The coefficients in the linear function, say \(_{k},k[_{j}]\), can be further decomposed based on a finer time partition called the _homogenous time partition_, which consists of subsets constructed according to the Markov chains \(\{Z_{n}^{q}\}_{q[]}\) corresponding to the time series. Based on Assumption **A7**, the Markov chains are stationary and ergodic. Therefore, after sufficiently large time steps, the distribution of \(\{Z_{n}^{q}\}_{q[]}\) will be invariant across \(n\) as it achieves unique equilibrium. With this type of stationary sample, we can express \(_{k}\) by joint distributions instead of the indicators. Then, we can complete the proof of our inequality claim in Eq. (14) using Assumption **A2** and Bayes theorem.

**Lemma 3.3**.: _Let \(}(_{t}^{j})\) denote the estimated superset of parent set for \(^{j} V\) obtained from the Algorithm (12). \(\{_{k}(X_{t}^{j})\}_{k[_{j}]}\) contain the true and illusory parent sets, where \(_{j}\) is the true periodicity of \(^{j}\). Under assumptions **A1-A7** and with an oracle (infinite sample size limit), we have that:_

\[_{k=1}^{_{j}}_{k}(X_{t}^{j})}(X_{t}^{j}),\  t[_{}+1,T]\]

_almost surely._

Proof sketch.: Assume the contrary, i.e., there exists \(s_{k}_{k}(X_{t}^{j})}(X_{t}^ {j})\). From Lemma 3.2, we have \(X_{t}^{j}}s|_{k=1}^{_{j}}_{k} (X_{t}^{j}) s.\) By the Definition 2.4, we have \((X_{t}^{j})_{k=1}^{_{j}}_{k}(X_{t}^{j})\). If \(s(X_{t}^{j})\), by the causal Markov property (Assumption **A2**), the dependence relation can not be true because \(s\) is a non-descendant of \(X_{t}^{j}\). If \(s(X_{t}^{j})\), our Algorithm would have concluded that \(X_{t}^{j}}s|}(X_{t}^{j}).\) (line 2), evident from the causal Markov property, contradicting our assumption. Hence, the lemma. 

Based on Lemma 3.2 and Lemma 3.3 we can identify the true \(_{j}\) for \(^{j}\) through Lemma 3.4.

**Lemma 3.4**.: _Let \(_{j}\) denote the true periodicity for \(^{j} V\) and \(}_{}(X_{t_{k}^{j}}^{j})\) denote the estimated parent set for \(X_{t}^{j}\) obtained from Algorithm (12), where \(t_{k}^{j}\). Define:_

\[_{j}=_{[_{}]}_{k[ ]}|}_{}_{}}(X_{t_{k}^ {j}}^{j})|\] (16)

_Under assumptions **A1-A7** and with an oracle (infinite sample limit), we have that \(_{j}=_{j},\  j[n]\) almost surely._

Proof sketch.: Assume the contrary that \(_{j}_{j}\), then in the Algorithm (12) we have an incorrect time partition \(^{j}\). Hence, CI tests that are performed use samples with different causal mechanisms.

\((X^{j}_{t}|_{k=1}^{_{j}}_{k}(X^{j}_{t}))\) in Eq.(13) is estimated from a mixture of two or more time partition subsets, say \(^{j}_{1}\) and \(^{j}_{2}\). We can apply Lemma 3.2 with \(_{k=1}^{2}_{k}(X^{j}_{t})\). With Lemma 3.3\(_{k=1}^{2}_{k}(X^{j}_{t})_{_{j}}}(X^{j}_{t})\). Hence, for \(_{j}\), \(|_{_{j}}}(X^{j}_{t})||_{k=1}^{2} _{k}(X^{j}_{t})|\) using mixture samples \(t_{k=1}^{2}^{j}_{k}\). For true \(_{j}\), we have \(|}_{_{j}}(X^{j}_{t})|=|(X^{j}_{t})|\). With Assumption **A6** the Hard Mechanism Change, \(|_{k=1}^{2}_{k}(X^{j}_{t})|>|(X^{j}_{t})|\) so that \(_{j}\) always leads to a smaller size of estimated parent sets than \(_{j}\), contrary to the definition of \(_{j}\). Hence, \(_{j}=_{j}\). 

## 4 Experiments

### Experiments on Continuous-valued Time Series

To validate the correctness and effectiveness of our algorithm, we perform a series of experiments. The Python code is provided at https://github.com/CausalML-Lab/PCMCI-Omega. In this section, we test four algorithm1. PCMCI\({}_{}\), PCMCI Runge et al. (2019), VARLiNGAM Hyvarinen et al. (2010) and DYNOTEARS Pamili et al. (2020), on continuous-valued time series with Gaussian noise. The experiments for continuous-valued time series with exponential noise and binary-valued time series are in the supplementary material.

Following Runge et al. (2019), we generate the continuous-valued time series in three steps:

1. Construct an \(n\)-variate time series \(V\) with length \(T\) using independent and identical (Standard Gaussian or Exponential) noise temporarily. Determine \(_{}\) and \(_{}\) where \(_{}=\{_{j}\}_{j\{n\}}\). After making sure that one univariate time series, say \(^{j}\), has periodicity \(_{}\), the periodicity of the remaining time series \(^{i},i j\) is randomly selected from \(\{1,,_{}\}\) respectively.
2. Randomly generate \(_{j}\) binary edge matrices with shape \((n,_{})\) for each time series \(^{j},j[n]\). \(1\) denotes an edge and \(0\) denotes no edge. Each binary matrix represent one parent set index \(^{j}_{k},k[_{j}]\). Randomly generate \(_{j}\) coefficient matrices with shape \((n,_{})\) for each time series \(^{j},j[n]\). One binary edge matrix and one coefficient matrix jointly determine one causal mechanism. Hence, total \(_{j}\) causal mechanisms are constructed. Here, make sure that \(V\) satisfies Assumption **A6**.
3. Starting from time point \(t>_{}\), generate vector \(_{t}\) over time according to all the causal mechanisms of \(V\), until \(t\) achieves \(T\).

Figure 2: PCMCI\({}_{}\) is tested on 5-variate time series with \(_{}=5\). Set \(_{}=15\), \(_{}=15\) for all variables. Every line corresponds to a different time series length. Every marker corresponds to the average accuracy rate or average running time over 100 trials. a) The accuracy rate of \(\) for different time series lengths and different \(_{}\). b) Illustration of Runtime (in sec.) when \(_{}\) varies.

Following the previous work in Huang et al.(2020), \(F_{1}\) score, Adjacency Precision, and Adjacency Recall are used to measure the performance of the algorithms. The details of calculating these metrics are described in the Appendix. All the performance statistics are averaged over 100 trials. The standard error of the averaged statistics is displayed either by color filling or by error bars.

A correct estimator \(\) is the prerequisite for obtaining the correct causal graph. Fig.2(a) shows the accuracy rate of \(\) for different time lengths \(T\). Here, elements in \(\{N_{j}\}\) where \(N[}}{_{j}}]\) are all treated as correct estimations. By Definition2.2 and 2.3 the multiple of \(_{j}\) is still associated with a correct causal graph. However, it leads to a finer time point partition \(^{j}\), decreasing the sample size used in each CI test from approximately \(T/_{j}\) to approximately \(T/(N_{j})\). The accuracy rate is sensitive to \(_{}\) for small \(T\). This result verifies that algorithm PCMCI\({}_{}\) has the capacity to detect the true periodicity of each \(^{j} V\) with a large enough time length.

We evaluate the performances of PCMCI\({}_{}\) on continuous-valued time series with Gaussian noise shown as Fig.3(a). As \(T\) increases, it is natural to see a continuous improvement in performance. The sub-figures show that all three evaluation metrics decrease when \(_{}\) gets larger. The precision of PCMCI\({}_{}\) is always far better than other algorithms when \(_{}\) is not equal to 1. Given the fact that the parent sets \(}(X_{t}^{j})\  j,t\) obtained from PCMCI\({}_{}\) are subsets of the parent set \(}(X_{t}^{j})\  j,t\) estimated from PCMCI, the recall rate of PCMCI should be the upper bound of the recall rate of PCMCI\({}_{}\). This assertion has been verified as the red recall line of PCMCI\({}_{}\) is always below the blue recall line of PCMCI as \(T\) increases.

In Fig.3(a), the recall of PCMCI\({}_{}\) is worse than PCMCI for \(T=500\). In this regime, the accuracy rate of \(\) is low, shown as the dark blue line in Fig.2(a). Small sample sizes in CI tests may result in a sparser causal graph. Hence the number of true positive edges may decrease. This is a common problem for many constraint-based algorithms, but it hurts PCMCI\({}_{}\) the most because in PCMCI\({}_{}\), the sample sizes in each CI test are approximate \(T/\) instead of \(T\). As \(T\) increases, the red recall line of PCMCI\({}_{}\) push forward to the blue recall line of PCMCI. The high value of both adjacent precision and recall rate with large \(T\) verify that PCMCI\({}_{}\) can identify the correct causal graph.

We also observe the performance of our algorithm as \(_{}\) and \(N\) varies in Fig.3(b). As the performance of PCMCI\({}_{}\) is consistent over \(n\)-variate time series with different \(n\), large \(_{}\) may lead to a smaller precision and recall rate.

Figure 3: 4 algorithms are tested on 5-variate time series. Set \(_{}=15,_{ub}=15\) for all variables. Every line corresponds to a different algorithm. Every marker corresponds to the average performance over 100 trials.

### Case Study

Here, we construct an experiment with a real-world climate time series dataset. In Runge et al. (2019), the authors tested dependencies among monthly surface pressure anomalies in the West Pacific and surface air temperature anomalies in the Central Pacific, East Pacific, and tropical Atlantic from 1948 to 2012. Our application explores the causal relations among the monthly mean of the same set of variables from 1948-2022 with 900 months. Let \(X_{t}^{}\) denote the monthly mean of surface pressure in the West Pacific, \(X_{t}^{}\), \(X_{t}^{}\) and \(X_{t}^{}\) denote the monthly mean of air temperature in the Central Pacific, East Pacific, and tropical Atlantic, respectively.

The parent sets for each variable obtained from PCMCI\({}_{}\) algorithms are shown in Table 1. Sets of true and illusory parents of a variable at time \(t\) are separated by curly braces. For instance, variable \(X_{t}^{}\) with \(_{}=1\) means that the causal mechanism of the surface pressure in the West Pacific remains invariant over time with the estimated parent set \(\{X_{t-1}^{},X_{t-2}^{},X_{t-1}^{},X_{t-1}^{ {ta}}\}\). Only time series \(^{}\) has three different parent sets, including one true parent set and two illusory parent sets, which appear periodically over time. The three parent sets of \(X_{t}^{}\) imply that the causal effect from the tropical Atlantic air temperature \(X_{t-1}^{}\) to the Central Pacific air temperature \(X_{t}^{}\) would disappear every quarter of a year. Note that we do not have a ground truth in this case, and we do not possess the necessary knowledge in this area, so the significance of these results is under-explored. More discussion about this application can be found in the supplementary materials.

## 5 Conclusions

In this paper, we propose a non-parametric, constraint-based causal discovery algorithm PCMCI\({}_{}\) designed for semi-stationary time-series data, in which a finite number of causal mechanisms are repeated periodically. We establish the soundness of our algorithm and assess its effectiveness on continuous-valued and discrete-valued time series data. The algorithm PCMCI\({}_{}\) has the capacity to reveal the existence of periodicity of causal mechanisms in real-world datasets.

## 6 Acknowledgements

This research has been supported in part by NSF CAREER 2239375 and Adobe Research. We wish to convey our heartfelt gratitude to the anonymous reviewers for their invaluable and constructive feedback, which significantly contributed to enhancing the quality of the manuscript.

    & _{}\)**} \\  \(X\) & \(\) & \(\{}_{k}}\}_{k[]}\): true and illusory parent sets \\  \(X_{t}^{}\) & 1 & \(\{X_{t-1}^{},X_{t-2}^{},X_{t-1}^{}\}\), \(X_{t-1}^{}\}\) \\ \(X_{t}^{}\) & 3 & \(\{X_{t-1}^{}\}\); & \(\{X_{t-1}^{},X_{t-2}^{}\}\); \(\{X_{t-1}^{},X_{t-1}^{}\}\) \\ \(X_{t}^{}\) & 1 & \(\{X_{t-1}^{},X_{t-1}^{},X_{t-2}^{},X_{t-1}^{ {cp}}\}\) \\ \(X_{t}^{}\) & 1 & \(\{X_{t-1}^{},X_{t-1}^{}\}\) \\   

Table 1: Climate application results estimated from PCMCI\({}_{}\).