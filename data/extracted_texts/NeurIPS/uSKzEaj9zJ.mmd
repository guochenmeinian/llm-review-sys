# Nonlocal Attention Operator: Materializing Hidden Knowledge Towards Interpretable Physics Discovery

Yue Yu

Department of Mathematics,

Lehigh University,

Bethlehem, PA 18015, USA

yuy214@lehigh.edu

&Ning Liu

Global Engineering and

Materials, Inc.,

Princeton, NJ 08540, USA

&Fei Lu

Department of Mathematics,

Johns Hopkins University,

Baltimore, MD 21218, USA

&Tian Gao

IBM Research,

Yorktown Heights,

NY 10598, USA

&Siavash Jafarzadeh

Department of Mathematics,

Lehigh University,

Bethlehem, PA 18015, USA

&Stewart Silling

Center for Computing Research,

Sandia National Laboratories,

Albuquerque, NM 87123, USA

Corresponding Author

###### Abstract

Despite the recent popularity of attention-based neural architectures in core AI fields like natural language processing (NLP) and computer vision (CV), their potential in modeling complex physical systems remains under-explored. Learning problems in physical systems are often characterized as discovering operators that map between function spaces based on a few instances of function pairs. This task frequently presents a severely ill-posed PDE inverse problem. In this work, we propose a novel neural operator architecture based on the attention mechanism, which we coin Nonlocal Attention Operator (NAO), and explore its capability towards developing a foundation physical model. In particular, we show that the attention mechanism is equivalent to a double integral operator that enables nonlocal interactions among spatial tokens, with a data-dependent kernel characterizing the inverse mapping from data to the hidden parameter field of the underlying operator. As such, the attention mechanism extracts global prior information from training data generated by multiple systems, and suggests the exploratory space in the form of a nonlinear kernel map. Consequently, NAO can address ill-posedness and rank deficiency in inverse PDE problems by encoding regularization and achieving generalizability. We empirically demonstrate the advantages of NAO over baseline neural models in terms of generalizability to unseen data resolutions and system states. Our work not only suggests a novel neural operator architecture for learning interpretable foundation models of physical systems, but also offers a new perspective towards understanding the attention mechanism. Our code and data accompanying this paper are available at https://github.com/fishmoon1234/NAO.

## 1 Introduction

The interpretability of machine learning (ML) models has become increasingly important from the security and robustness standpoints (Rudin et al., 2022, Molnar, 2020). This is particularly true in physics modeling problems that can affect human lives, where not only the accuracy but also the transparency of data-driven models are essential in making decisions (Coorey et al., 2022, Ferrari and Willcox, 2024). Nevertheless, it remains challenging to discover the underlying physical system and the governing mechanism from data. Taking the material modeling task for instance, given that only the deformation field is observable, the goal of discovering the underlying material parameter field and mechanism presents an ill-posed unsupervised learning task. That means, even if an ML model can serve as a good surrogate to predict the corresponding loading field from a given deformation field, its inference of the material parameters can still drastically deteriorate.

To discover an interpretable mechanism for physical systems, a major challenge is to infer the governing laws of these systems that are often high- or infinite-dimensional, from data that are comprised of discrete measurements of continuous functions. Therefore, a data-driven surrogate model needs to learn not only the mapping between input and output function pairs, but also the mapping from given function pairs to the hidden state. From the PDE-based modeling standpoint, learning a surrogate model corresponds to a forward problem, whereas inferring the underlying mechanism is an inverse problem. The latter is generally an enduring ill-posed problem, especially when the measurements are scarce. Unfortunately, such an ill-posedness issue may become even more severe in neural network models, due to the inherent bias of neural network approximations (Xu et al., 2019). To tackle this challenge, many deep learning methods have recently been proposed as inverse PDE solvers (Fan and Ying, 2023; Molinaro et al., 2023; Jiang et al., 2022; Chen et al., 2023). The central idea is to incorporate prior information into the learning scheme, in the form of governing PDEs (Yang et al., 2021; Li et al., 2021), regularizers (Dittmer et al., 2020; Obmann et al., 2020; Ding et al., 2022; Chen et al., 2023), or additional operator structures (Uhlmann, 2009; Lai et al., 2019; Yilmaz, 2001). However, such prior information is often either unavailable or problem-specific in complex systems. As a result, these methods can only solve the inverse problem for a particular system, and one has to start from scratch when the system varies (e.g., when the material of the specimen undergoes degradation in a material modeling task).

In this work, we propose **Nonlocal Attention Operator (NAO)**, a novel attention-based neural operator architecture to simultaneously solve both forward and inverse modeling problems. Neural operators (NOs) (Li et al., 2020, 2020, 2020) learn mappings between infinite-dimensional function spaces in the form of integral operators, hence they provide promising tools for the discovery of continuum physical laws by manifesting the mapping between spatial and/or spatiotemporal data; see You et al. (2022); Liu et al. (2024, 2024, 2023); Ong et al. (2022); Cao (2021); Lu et al. (2019, 2021); Goswami et al. (2022); Gupta et al. (2021) and references therein. However, most NOs focus on providing an efficient surrogate for the underlying physical system as a forward solver. They are often employed as black-box universal approximators but lack interpretability of the underlying physical laws. In contrast, the key innovation of NAO is that it introduces a kernel map based on the attention mechanism for simultaneous learning of the operator and the kernel map. As such, the kernel map automatically infers the context of the underlying physical system in an unsupervised manner. Intuitively, the attention mechanism extracts hidden knowledge from multiple systems by providing a function space of identifiability for the kernels, which acts as an automatic data-driven regularizer and endows the learned model's generalizability to new and unseen system states.

In this context, NAO learns a kernel map using the attention mechanism and simultaneously solves both the forward and inverse problems. The kernel map, whose parameters extract the global information about the kernel from multiple systems, efficiently infers resolution-invariant kernels from new datasets. As a consequence, NAO can achieve interpretability of the nonlocal operator and enable the discovery of hidden physical laws. **Our key contributions** include:

* We bridge the divide between inverse PDE modeling and physics discovery tasks, and present a method to simultaneously perform physics modeling (forward PDE) and mechanism discovery (inverse PDE).
* We propose a novel neural operator architecture NAO, based on the principle of contextual discovery from input/output function pairs through a kernel map constructed from multiple physical systems. As such, NAO is generalizable to new and unseen physical systems, and offers meaningful physical interpretation through the discovered kernel.
* We provide theoretical analysis to show that the attention mechanism in NAO acts to provide the space of identifiability for the kernels from the training data, which reveals its ability to resolve ill-posed inverse PDE problems.

Figure 1: Illustration of NAOâ€™s architecture.

* We conduct experiments on zero-shot learning to new and unseen physical systems, demonstrating the generalizability of NAO in both forward and inverse PDE problems.

## 2 Background and related work

Our work resides at the intersection of operator learning, attention-based models, and forward and inverse problems of PDEs. The ultimate goal is to model multiple physical systems from data while simultaneously discovering the hidden mechanism.

**Neural operator for hidden physics.** Learning complex physical systems directly from data is ubiquitous in scientific and engineering applications (Ghaboussi et al., 1998; Liu et al., 2024; Ghaboussi et al., 1991; Carleo et al., 2019; Karniadakis et al., 2021; Zhang et al., 2018; Cai et al., 2022; Pfau et al., 2020; He et al., 2021; Besand et al., 2006). In many applications, the underlying governing laws are unknown, hidden in data to be revealed by physical models. Ideally, these models should be _interpretable_ for domain experts, who can then use these models to make further predictions and expand the understanding of the target physical system. Also, these models should be _resolution-invariant_. Neural operators are designed to learn mappings between infinite-dimensional function spaces (Li et al., 2020, 2020, 2022; Ong et al., 2022; Cao, 2021; Lu et al., 2019, 2021; Goswami et al., 2022; Gupta et al., 2021). As a result, NOs provide a promising tool for the discovery of physical laws by manifesting the mapping between spatial and/or spatio-temporal data.

**Forward and inverse PDE problems.** Most current NOs focus on providing an efficient surrogate for the underlying physical system as a forward PDE solver. They are often employed as black-box universal approximators without interpretability of the underlying physical laws. Conversely, several deep learning methods have been proposed as inverse PDE solvers (Fan and Ying, 2023; Molinaro et al., 2023; Jiang et al., 2022; Chen et al., 2023), aiming to reconstruct the parameters in the PDE from solution data. Compared to the forward problem, the inverse problem is typically more challenging due to its ill-posed nature. To tackle the ill-posedness, many NOs incorporate prior information, in the form of governing PDEs (Yang et al., 2021; Li et al., 2021), regularizers (Dittmer et al., 2020; Obmann et al., 2020; Ding et al., 2022; Chen et al., 2023), or additional operator structures (Uhlmann, 2009; Lai et al., 2019; Yilmaz, 2001). To our knowledge, our NO architecture is the first that solves both the forward (prediction) and inverse (discovery) problems simultaneously.

**Attention mechanism.** Since 2017, the attention mechanism has become the backbone of state-of-the-art deep learning models on many core AI tasks like NLP and CV. By calculating the similarity among tokens, the attention mechanism captures long-range dependencies between tokens (Vaswani et al., 2017). Then, the tokens are spatially mixed to obtain the layer output. Based on the choice of mixers, attention-based models can be divided into three main categories: discrete graph-based attentions (Child et al., 2019; Ho et al., 2019; Wang et al., 2020; Katharopoulos et al., 2020), MLP-based attentions (Tolstikhin et al., 2021; Touvron et al., 2022; Liu et al., 2021), and convolution-based attentions (Lee-Thorp et al., 2021; Rao et al., 2021; Guibas et al., 2021; Nekoozadeh et al., 2023). While most attention models focus on discrete mixers, it is proposed in Guibas et al. (2021); Nekoozadeh et al. (2023); Tsai et al. (2019); Cao (2021); Wei and Zhang (2023) to frame token mixing as a kernel integration, with the goal of obtaining predictions independent of the input resolution.

Along the line of PDE-solving tasks, various attention mechanisms have been used to enlarge model capacity. To improve the accuracy of forward PDE solvers, Cao (2021) removes the softmax normalization in the attention mechanism and employs linear attention as a learnable kernel in NOs. Further developments include the Galerkin-type linear attention in an encoder-decoder architecture in OFormer (Li et al., 2022), a hierarchical transformer for learning multiscale problems (Liu et al., 2022), and a heterogeneous normalized attention with a geometric gating mechanism (Hao et al., 2023) to handle multiple input features. In particular, going beyond solving a single PDE, the foundation model feature of attention mechanisms has been applied towards solving multiple types of PDEs within a specified context in Yang and Osher (2024); Ye et al. (2024); Sun et al. (2024); Zhang (2024). However, none of the existing work discovers hidden physics from data, nor do they discuss the connections between the attention mechanism and the inverse PDE problem.

## 3 Nonlocal Attention Operator

Consider multiple physical systems that are described by a class of operators mapping from input functions \(u\) to output functions \(f\). Our goal is to learn the common physical law, in the form of operators \(_{K}:\) with system-dependent kernels \(K\):

\[_{K}[u]+=f.\] (1)Here \(\) and \(\) are Banach spaces, \(\) denotes an additive noise describing the discrepancy between the ground-truth operator and the optimal surrogate operator, and \(K\) is a kernel function representing the nonlocal spatial interaction. As such, the kernel provides the knowledge of its corresponding system, while (1) offers a zero-shot prediction model for new and unseen systems.

To formulate the learning, we consider \(n_{train}\) training datasets from different systems, with each dataset containing \(d_{u}\) function pairs \((u,f)\):

\[_{}=\{\{(u_{i}^{}(x),f_{i}^{}(x))\}_{i=1}^{d_{u}} \}_{=1}^{n_{train}}.\] (2)

In practice, the data of the input and output functions are on a spatial mesh \(\{x_{k}\}_{k=1}^{n_{x}}^{d_{x}}\). The \(n_{train}\) models with kernels \(\{K^{}\}_{=1}^{n_{train}}\) correspond to different material micro-structures or different parametric settings. As a demonstration, we consider models for heterogeneous materials with operators in the form

\[_{K}[u](x)=_{}K(x,y)g[u](y)dy,\,x,\] (3)

where \(g[u](y)\) is a functional of \(u\) determined by the operator; for example \(g[u](y)=u(y)\) in Section 5.3. Our approach extends naturally to other forms of operators, such as those with radial interaction kernels in Section 5.1 and heterogeneous interaction in the form of \(_{K}[u](x)=_{}K(x,y)g[u](x,y)dy\). Additionally, for simplicity, we consider scalar-valued functions \(u\) and \(f\) and note that the extension to vector-valued functions is trivial.

**Remark:** Such an operator learning problem arises in many applications in forward and inverse PDE-solving problems. The inference of the kernel \(K\) is an inverse problem, and the learning of the nonlocal operator is a forward problem. When considering a single physical system and taking \(K\) in (3) as an input-independent kernel, classical NOs can be obtained for forward PDE-solving tasks (Li et al., 2020; Guibas et al., 2021) and governing law learning tasks (You et al., 2021; Jafarzadeh et al., 2024). Different from existing work, we consider the operator learning across multiple systems.

### Kernel map with attention mechanism

The key ingredient in NAO is a kernel map constructed using the attention mechanism. It maps from data pairs to an estimation of the underlying kernel. The kernel map

\[\{(u_{i},f_{i})\}_{i=1}^{d_{u}}\,\,K[_{1:d},_{ 1:d};]\] (4)

has parameters \(\) estimated from the training dataset (2). As such, it maps from the token \((_{1:d},_{1:d})\) of the dataset \(\{(u_{i},f_{i})\}_{i=1}^{d_{u}}\) to a kernel estimator, acting as an inverse PDE solver.

A major innovation of this kernel map is its dependence on both \(u\) and \(f\) through their tokens. Thus, our approach distinguishes itself from the forward problem-solving NOs in the related work section, where the attention depends only on \(u\).

We first transfer the data \(\{(u_{i},f_{i})\}_{i=1}^{d_{u}}\) to tokens \((_{1:d},_{1:d})\) according to the operator in (3) by

\[_{1:d}&=(_{1}, ,_{d})=g[u_{j}](y_{k})_{1 j d,1 k  N}^{N d},\\ _{1:d}&=(f_{j}(x_{k}))_{1 j d,1  k N}^{N d},\] (5)

where \(d=d_{u}\) and \(N=n_{x}\), assuming that \(g[u]\) has a spatial mesh \(\{y_{k}=x_{k}\}_{k=1}^{N}\).

Then, our discrete \((L+1)\)-layer attention model for the inverse PDE problem writes:

\[_{}&=^{(0)}=( {U}^{(0)},^{(0)}):=(_{1:d};_{1:d})^{2N  d},\\ ^{(l)}&=[^{(l-1)};_{l}] ^{(l-1)}+^{(l-1)}=:(^{(l)},^{(l)})^{2N  d}, 1 l<L,\\ _{}&=^{L}=K[_{1:d}, _{1:d};]_{1:d}_{1:d}^{ N d}.\] (6)

Here, \(_{l}=(_{l}^{Q}^{d d_{k}},_{l}^{K} ^{d d_{k}})\) and the attention function is

\[[;_{l}]=(}} _{l}^{Q}_{l}^{K}{}^{}^{})^{2N 2N}.\]

The kernel map is defined as

\[ K[_{1:d},_{1:d};]& =W^{P,u}(}}(^{(L-1)})^{}_{L}^{Q} (_{L}^{K})^{}^{(L-1)})\\ &+W^{P,f}(}}(^{(L-1)})^{ }_{L}^{Q}(_{L}^{K})^{}^{(L-1)}),\] (7)

[MISSING_PAGE_FAIL:5]

For simplicity, we assume that the dataset in (2) has \(d_{u}=1\) and \(n_{train}=1\) with a uniform mesh \(\{x_{j}\}_{j=1}^{n_{x}}\). We define the tokens by

\[_{1:d}=(_{1},,_{d})=(g[u](r_{k},x_{j}))_{1  j d,1 k N}^{N d},\ _{1:d}=(f(x_{j}))_{1 j d} ^{1 d},\] (13)

where \(d=n_{x}\) and \(\{r_{k}\}_{k=1}^{N}\) is the spatial mesh for \(K\)'s independent variable \(r[0,]\).

**Lemma 4.1**.: _Consider the two-layer attention model in (6)-(7) with bounded parameters. For each \(d\) and \(N\), let \(\{x_{j}\}_{j=1}^{d}\) and \(\{r_{k}\}_{k=1}^{N}\) be uniform meshes of the compact sets \(\) and \([0,]\), and let \(\{A_{j}\}_{j=1}^{d}\) be the resulting uniform partition of \(\). Assume that \(g[u]\) in (12) is continuous on \([0,]\). Then,_

\[_{N}_{d}_{k=1}^{N}K[_{1:d},_{1:d};](r_{k})_{[r_{k-1},r_{k})}(r)(r_{k}-r_{k-1})\] (14) \[= K[u,f](r):=_{0}^{}W^{P,u}(|r^{}|)( [g[u](r^{},x)W^{QR}(x,y)g[u](r,y)dxdy])dr^{}\] \[+W^{P,f}([f(x)W^{QR}(x,y)g[u](r,y)] dxdy),\]

_where \(W^{QK}(x,y)=_{d}_{j,j^{}=1}^{d}W^{QK}[j,j^{}] _{A_{j} A_{j^{}}}(x,y)\) is the scaled \(L^{2}()\) limit of the parameter matrix \(W^{QK}[j^{},j]=_{l=1}^{d_{h}}W^{Q}[j,l] W^{K}[j^{},l]\) and \(W^{P,u}(r)=_{N}_{k=1}^{N}W^{P,u}[k]_{[r_{k-1},r_{k })}(r)\)._

### Space of identifiability for the kernels

For a given training dataset, we show that the function space in which the kernels can be identified is the closure of a data-adaptive reproducing kernel Hilbert space (RKHS). This space contains the range of the kernel map and hence provides the ground for analyzing the inverse problem.

**Lemma 4.2** (Space of Identifiability).: _Assume that the training data pairs in (2) are sampled from continuous functions \(\{u_{i}^{}\}_{i,j=1}^{d_{u},n_{train}}\) with a compact support. Then, the function space the loss function in (9) has a unique minimizer \(K(s)=K[_{1:d}^{},_{1:d}^{};](s)\) is the closure of a data-adaptive RKHS \(H_{G}\) with a reproducing kernel \(\) determined by the training data:_

\[(r,s)=[^{}(r)^{}(s)]^{-1}G(r,s),\]

_where \(^{}\) is the density of the empirical measure \(\) defined by_

\[^{}(r):=_{=1}^{n_{train}}_{i=1}^{d_{u}}_ {}|g[u_{i}^{}](r,x)|dx,\] (15)

_and the function \(G\) is defined by \(G(r,s):=d}_{=1}^{n_{train}}_{i=1}^{d_{u}}_ {}g[u_{i}^{}](r,x)g[u_{i}^{}](s,x)dx\)._

The above space is data-adaptive since the integral kernel \(\) depends on data. It characterizes the information in the training data for estimating the nonlocal kernel \(K(s)=K[_{1:d}^{},_{1:d}^{};](s)\). In general, the more data, the larger the space is. On the other hand, note that the loss function's minimizer with respect to \(K(s)\) is not the kernel map. The minimizer is a fixed estimator for the training dataset and does not provide any information for estimating the kernel from another dataset.

**Comparison with regularized estimators.** The kernel map solves the ill-posed inverse problem using prior information from the training dataset of multiple systems, which is not used in classical inverse problem solvers. To illustrate this mechanism, consider the extreme case of estimating the kernel in the nonlocal operator from a dataset consisting of only a single function pair \((u,f)\). This inverse problem is severely ill-posed because of the small dataset and the need for deconvolution to estimate the kernel. Thus, regularization is necessary, where two main challenges present: (i) the selection of a proper regularization with limited prior information, and (ii) the prohibitive computational cost of solving the resulting large linear systems many times.

In contrast, our kernel map \(K[_{1:d},_{1:d};](s)\), with the parameter \(\) estimated from the training datasets, acts on the token \((_{1:d},_{1:d})\) of \((u,f)\) to provide an estimator. It passes the prior information about the kernel from the training dataset to the estimation for new datasets. Importantly,it captures the nonlinear dependence of the estimator on the data \((u,f)\). Computationally, it can be applied directly to multiple new datasets without solving the linear systems. In Section B.2, we further show that a regularized estimator depends nonlinearly on the data pair \((u,f)\). In particular, similar to Lemma 4.1, there is an RKHS determined by the data pair \((u,f)\). The regularized estimator suggests that the kernel map can involve a component quadratic in the feature \(g[u]\), similar to the limit form of the attention model in Lemma 4.1.

## 5 Experiments

We assess the performance of NAO on a wide range of physics modeling and discovery datasets. Our evaluation focuses on several key aspects: 1) we demonstrate the merits of the continuous and linear attention mechanism, compare the performance with the baseline discrete attention model (denoted as Discrete-NAO), the softmax attention mechanism (denoted as Softmax-NAO), NAO with input on \(u\) only (denoted as NAO-u), the convolution-based attention mechanism (denoted as AFNO (Guibas et al., 2021)), and an MLP-based encoder architecture that maps the datum \([_{1:d},_{1:d}]\) directly to a latent kernel (denoted as Autoencoder); 2) we measure the generalizability, in particular, the zero-shot prediction performance in modeling a new physical system with unseen governing equations, and across different resolutions; 3) we evaluate the data efficiency-accuracy trade-off in ill-posed inverse PDE learning tasks, as well as the interpretability of the learned kernels. In all experiments, the optimization is performed with the Adam optimizer. To conduct fair comparison across different methods, we tune the hyperparameters, including the learning rates, the decay rates, and the regularization parameters, to minimize the training loss. In all examples, we use 3-layer models, and parameterize the kernel network \(W^{P,u}\) and \(W^{P,f}\) with a 3-layer MLP with hidden dimensions \((32,64)\) and LeakyReLU activation. Experiments are conducted on a single NVIDIA GeForce RTX 3090 GPU with 24 GB memory. Additional results and details on data generation and training strategies are provided in Appendix C.

### Radial kernel learning

In this example, we consider the learning of nonlocal diffusion operators, in the form:

\[_{_{}}[u](x)=_{}_{}(|y-x|)[u(y)-u(x)] dy=f(x), x.\] (16)

Unlike a (local) differential operator, this operator depends on the function \(u\) nonlocally through the convolution of \(u(y)-u(x)\), and the operator is characterized by a radial kernel \(_{}\). It finds broad physical applications in describing fracture mechanics (Silling, 2000), anomalous diffusion behaviors (Bucur et al., 2016), and the homogenization of multiscale systems (Du et al., 2020).

  Case & model & \#param &  &  \\   & & & ID & OOD1 & ID & OOD1 \\   \)=10} & Discrete-NAO & 16526 & **1.33\%** & 25.81\% & 29.02\% & 28.80\% \\  & Softmax-NAO & 18843 & 13.45\% & 66.06\% & 67.55\% & 85.80\% \\  & AFNO & 19605 & 22.62\% & 68.76\% & - & - \\  & NAO & 18843 & 1.48\% & **8.10\%** & **5.40\%** & **10.02\%** \\  & NAO-u & 18842 & 13.68\% & 66.68\% & 20.46\% & 74.03\% \\  & Autoencoder & 16424 & 12.97\% & 1041.49\% & 22.56\% & 136.79\% \\   \(d\) = 302, \(d_{k}\)=5 & Discrete-NAO & 10465 & **1.63\%** & 15.80\% & 33.21\% & 30.39\% \\  & NAO & 12783 & 2.34\% & **9.23\%** & **6.87\%** & **14.62\%** \\  \(d\) = 302, \(d_{k}\)=20 & Discrete-NAO & 28645 & 1.35\% & 18.70\% & 35.49\% & 30.81\% \\  & NAO & 30963 & **1.33\%** & **9.12\%** & **4.63\%** & **9.14\%** \\   \(d\) = 100, \(d_{k}\)=10 & Discrete-NAO & 8446 & 1.73\% & 14.92\% & 34.52\% & 35.20\% \\  & NAO & 10763 & **1.07\%** & **6.35\%** & **7.41\%** & **17.02\%** \\  \(d\) = 50, \(d_{k}\)=10 & Discrete-NAO & 6446 & 2.29\% & 10.31\% & 41.80\% & 45.30\% \\  & NAO & 8763 & **1.56\%** & **7.19\%** & **15.95\%** & **29.47\%** \\  \(d\) = 30, \(d_{k}\)=10 & Discrete-NAO & 5646 & 5.60\% & 11.31\% & 58.24\% & 64.23\% \\  & NAO & 7963 & **2.94\%** & **8.04\%** & **22.65\%** & **33.77\%** \\   

Table 1: Test errors and the number of trainable parameters for the radial kernel problem, where bold numbers highlight the best methods. The small operator errors and large kernel errors of discrete-NAO highlight the ill-posedness of the inverse problem. NAO overcomes the ill-posedness and yields resolution-invariant estimators.

In this context, our goal is to learn the operator \(\) as well as to discover the hidden mechanism, namely the kernel \(K[_{1:d},_{1:d};](x,y)=_{}(|y-x|)\). In the form of the operator in (12), we have \(K(r)=_{}(r)\) and \(g[u](r,x)=u(x+r)+u(x-r)-2u(x)\;r[0,]\).

To generate the training data, we consider \(7\) sine-type kernels

\[_{}(|y-x|):=(-(|y-x|))(6|y-x|)_{}(|y-x|), \;=1,2,3,4,6,7,8.\] (17)

Here, \(\) denotes task index. We generate \(4530\) data pairs \((g^{}[u],f^{})\) with a fixed resolution \( x=0.0125\) for each task, where the loading function \(_{_{}}[u^{}]=f^{}\) is computed by the adaptive Gauss-Kronrod quadrature method. Then, we form a training sample of each task by taking \(d\) pairs from this task. When taking the token size \(d=302\), each task contains \(=15\) samples. We consider two test kernels: one following the same rule of (17) with \(=5\) (denoted as the "in-distribution (ID) test" system), and the other following a different rule (denoted as the "out-of-distribution (OOD) test1" system):

\[_{ood1}(|y-x|):=|y-x|(11-|y-x|)(-5(|y-x|))(6|y-x|)_{[ 0,11]}(|y-x|).\] (18)

Both the operator error (10) and the kernel error (11) are provided in Table 1. While the former measures the error of the learned forward PDE solver (i.e., learning a physical model), the latter demonstrates the capability of serving as an inverse PDE solver (i.e., physics discovery).

**Ablation study.** We first perform an ablation study on NAO, by comparing its performance with its variants (Discrete-NAO, Softmax-NAO, and NAO-u), AFNO, and Autoencoder, with a fixed token dimension \(d=302\), query-key feature size \(d_{k}=10\), and data resolution \( x=0.0125\). When comparing the operator errors, both Discrete-NAO and NAO serve as good surrogate models for the ID task with relative errors of \(1.33\%\) and \(1.48\%\), respectively, while the other three baselines show \(>10\%\) errors. Therefore, we focus more on the comparison between Discrete-NAO and NAO. This gap becomes more pronounced in the OOD task: only NAO is able to provide a surrogate of \(_{_{ood}}\) with \(<10\%\) error, who outperforms its discrete mixer counterpart by \(68.62\%\), indicating that NAO learns a more generalizable mechanism. This argument is further affirmed when comparing the kernel errors, where NAO substantially outperforms all baselines by at least \(81.39\%\) in the ID test and \(65.21\%\) in the OOD test. This study verifies our analysis in Section 4: NAO learns the kernel map in the space of identifiability, and hence possesses advantages in solving the challenging ill-posed inverse problem. Additionally, we vary the query-key feature size from \(d_{k}=10\) to \(d_{k}=5\) and \(d_{k}=20\). Note that \(d_{k}\) determines the rank bound of \(W^{QK}\), the matrix that characterizes the interaction between different data pairs. Discrete-NAO again performs well only in approximating the operator for the ID test, while NAO achieves consistent results in both tests and criteria, showing that it has successfully discovered the intrinsic low-dimension in the kernel space.

**Alleviating ill-posedness.** To further understand NAO's capability as an inverse PDE solver, we reduce the number of data pairs for each sample from \(d=302\) to \(d=30\), making it more ill-posed as an inverse PDE problem. NAO again outperforms its discrete mixer counterpart in all aspects. Interestingly, the errors in NAO increase almost monotonically, showing its robustness. For Discrete-NAO, the error also increases monotonically in the ID operator test, but there exists no consistent pattern in other test criteria. Figure 2 shows the learned test kernels in both the ID and OOD tasks. It shows that Discrete-NAO learns highly oscillatory kernels, while our continuous NAO only has a discrepancy near \(|x-y|=0\). Note that when \(|x-y|=0\), we have \(u(y)-u(x)=0\) in the ground-truth operator (16), and hence the kernel value at this point does not change the operator

Figure 2: Results on radial kernel learning, when learning the test kernel from a small (\(d=30\)) number of data pairs: test on an ID task (left), and test on an OOD task (right).

value. That means, our data provides almost no information at this point. This again verifies our analysis: continuous NAO learns the kernel map structure from small data based on prior knowledge from other task datasets.

Cross-resolution.We test the NAO model trained with \( x=0.0125\) on a dataset corresponding to \( x=0.025\), and plot the results in Figure 2 Left. The predicted kernel is very similar to the one learned from the same resolution, and the error is also on-par (\(22.65\%\) versus \(20.79\%\)).

### Solution operator learning

We consider the modeling of 2D sub-surface flows through a porous medium with a heterogeneous permeability field. Following the settings in Li et al. (2020), the high-fidelity synthetic simulation data for this example are described by the Darcy flow. Here, the physical domain is \(=^{2}\), \(b()\) is the permeability field, and the Darcy's equation has the form:

\[-(b() p())=g(),\ \ \ \ \ ;\ \ \ \ \ \ \ p()=0,\ \ \ .\] (19)

In this context, we aim to learn the solution operator of Darcy's equation and compute the pressure field \(p()\). We consider two study scenarios. 1) \(g p\): each task has a fixed microstructure \(b()\), and our goal is to learn the (linear) solution operator mapping from each loading field \(g\) to the corresponding solution field \(p\). In this case, the kernel \(K\) acts as the Green's function of (19), and can be approximated by the inverse of the stiffness matrix. 2) \(b p\): each task has a fixed loading field \(g()\), and our goal is to learn the (nonlinear) solution operator mapping from the permeability field \(b\) to the corresponding solution field \(p\).

We report the operator learning results in Table 2, where NAO slightly outperforms Discrete-NAO in most cases, using only 1/2 or 1/3 the number of trainable parameters. On the other hand, we also verify the kernel learning results by comparing the learned kernels in a test case with the ground-truth inverse of stiffness matrix in Figure 3. Although both Discrete-NAO and NAO capture the major pattern, the kernel from Discrete-NAO again shows a spurious oscillation in regions where the ground-truth kernel has zero value. On the other hand, by exploring the kernel map in the integrated knowledge space, the learned kernel from NAO does not exhibit such spurious modes.

To demonstrate the physical interpretability of the learned kernel, in the first row of Figure 4 we show the ground-truth microstructure \(b()\), a test loading field instance \(g()\), and the corresponding solution \(p()\). By taking the summation of the kernel strength on each row, one can discover the interaction strength of each material point \(x\) with its neighbors. As this strength is related to the permeability field \(b(x)\), the underlying microstructure can be recovered accordingly. In the bottom row of Figure 4, we demonstrate the discovered microstructure of this test task. We note that the discovered microstructure is smoothed out due to the continuous setting of our learned kernel (as shown in the bottom left plot), and a thresholding step is performed to discover the two-phase microstructure. The discovered microstructure (bottom right plot) matches well with the hidden ground-truth microstructure (left plot), except for regions near the domain boundary. This mismatch is due to

  Case & model & \#param & Linear Operator: \(g p\) & Nonlinear Operator: \(b p\) \\  \(d\) = 20, \(d_{k}\)=20 & Discrete-NAO & 161991 & 8.61\% & **10.84\%** \\
900 samples & NAO & 89778 & **8.33\%** & 11.40\% \\  \(d\) = 50, \(d_{k}\)=40 & Discrete-NAO & 662163 & 3.28\% & 5.61\% \\
9000 samples & NAO & 189234 & **3.19\%** & **5.28\%** \\  

Table 2: Test errors and the number of trainable parameters in solution operator learning.

Figure 3: Kernel visualization in experiment 2, where the kernels correspond to the inverse of stiffness matrix: ground truth (left), test kernel from Discrete-NAO (middle), kernel from NAO (right).

the applied Dirichlet-type boundary condition (\(p(x)=0\) on \(\)) in all samples, which leads to the measurement pairs \((p(x),g(x))\) containing no information near the domain boundary \(\) and makes it impossible to identify the kernel on boundaries.

### Heterogeneous material learning

In this example, we investigate the learning of heterogeneous and nonlinear material responses using the Mechanical MNIST benchmark (Lejeune, 2020). For training and testing, we take 500 heterogeneous material specimens, where each specimen is governed by a Neo-Hookean material with a varying modulus converted from the MNIST bitmap images. On each specimen, 200 loading/response data pairs are provided. Two generalization scenarios are considered. 1) We mix the data from all numbers and randomly take \(10\%\) of specimens for testing. This scenario corresponds to an ID test. 2) We leave all specimens corresponding to the number '9' for testing, and use the rest for training. This scenario corresponds to an OOD test. The corresponding results are listed in Table 3, where NAO again outperforms its discrete counterpart.

## 6 Conclusion

We propose Nonlocal Attention Operator (NAO), a novel NO architecture to simultaneously learn both the forward (modeling) and inverse (discovery) solvers in physical systems from data. In particular, NAO learns the function-to-function mapping based on an integral NO architecture and provides a surrogate forward solution predictor. In the meantime, the attention mechanism is crafted in building a kernel map from input-output function pairs to the system's function parameters, offering zero-shot generalizability to new and unseen physical systems. As such, the kernel map explores in the function space of identifiability, resolving the enduring ill-posedness in inverse PDE problems. In our empirical demonstrations, NAO outperforms all selected baselines on multiple datasets of inverse PDE problems and out-of-distribution generalizability tasks.

**Broader Impacts:** Beyond its merits in forward/inverse PDE modeling, our work represents an initial exploration in understanding the attention mechanism in physics modeling, and paves a theoretical path towards building a foundation model in scientific ML.

**Limitations:** Due to limited computational resource, our experiments focus on learning from a small to medium number (\(<500\)) of similar physical systems. It would be beneficial to expand the coverage and enable learning across different types of physical systems.

Figure 4: Demonstration of the generated data and the recovered microstructure from the learned kernel in Example 2. Top row: the ground-truth two-phase material microstructure from a test task (left), an exemplar loading field instance (middle), and the corresponding solution field instance (right). Bottom row: summation of the learned kernel for each line, corresponding to the total interaction of all material points (left), and the discovered two-phase material microstructure after thresholding (right).

  Case & model & \#param & ID test & OOD test \\  \(d\) = 40, \(d_{h}\)=40 & Discrete-NAO & 5,469,528 & 7.21\% & 7.95\% \\ \(22500\) samples & NAO & 142,534 & **6.57\%** & **6.26\%** \\  \(d\) = 100, \(d_{h}\)=100 & Discrete-NAO & 7,353,768 & 6.34\% & 6.01\% \\ \(45000\) samples & NAO & 303,814 & **4.75\%** & **5.58\%** \\  

Table 3: Test errors and the number of trainable parameters in heterogeneous material learning.