# Optimizing the coalition gain in Online Auctions with Greedy Structured Bandits

Dorian Baudry\({}^{1,2,}\)

Equal contribution.

Hugo Richard\({}^{2,*}\)

Maria Cherifa\({}^{2,*}\)

Clement Calauzenes\({}^{2}\)

&Vianney Perchet\({}^{2}\)

\({}^{1}\) Department of Statistics, University of Oxford. \({}^{2}\)

\({}^{2}\) Inria Fairplay Joint team, CREST, ENSAE Paris, Criteo AI Lab

Equal contribution.

###### Abstract

Motivated by online display advertising, this work considers repeated second-price auctions, where agents sample their value from an unknown distribution with cumulative distribution function \(F\). In each auction \(t\), a decision-maker bound by limited observations selects \(n_{t}\) agents from a coalition of \(N\) to compete for a prize with \(p\) other agents, aiming to maximize the cumulative reward of the coalition across all auctions. The problem is framed as an \(N\)-armed structured bandit, each number of player sent being an arm \(n\), with expected reward \(r(n)\) fully characterized by \(F\) and \(p+n\). We present two algorithms, Local-Greedy (LG) and Greedy-Grid (GG), both achieving _constant_ problem-dependent regret. This relies on three key ingredients: **1.** an estimator of \(r(n)\) from feedback collected from any arm \(k\), **2.** concentration bounds of these estimates for \(k\) within an estimation neighborhood of \(n\) and **3.** the unimodality property of \(r\) under standard assumptions on \(F\). Additionally, GG exhibits problem-independent guarantees on top of best problem-dependent guarantees. However, by avoiding to rely on confidence intervals, LG practically outperforms GG, as well as standard unimodal bandit algorithms such as OSUB or multi-armed bandit algorithms.

## 1 Introduction

The online display advertising has seen remarkable evolution in recent decades . Publishers, who are the suppliers of digital ad space on the internet, sell display spots for ads to advertisers through real-time bidding in spot auctions, with many of these auctions being conducted using first or second-price mechanisms . Due to the technological complexity of online advertising, advertisers usually delegate the task of buying ad placements to demand-side platforms (DSP) that operate many advertising campaigns. This interaction between DSP and the publisher, can be simplified as the publisher acting as multiple ad auctions selling ad impressions (online displays), while the DSP acts as a _centralized coalition_: at each time step, it determines which campaign(s) from the coalition participate to the auction to maximize their total gain. The chosen campaign(s) then compete with others to secure impressions. The primary goal of advertising companies is then to maximize the cumulative utility: the total value of impressions won minus their costs. This raises a fundamental question: _how many ad campaigns should participate in the auction to optimize the overall utility?_ In the _interim_ setting, where the DSP observes current bidder values before deciding, it's known that only the highest value bidder should be sent. However, online privacy enhancements in browsers necessitate _ex-ante_ decisions from DSPs , without exact value knowledge. Here, the problem becomes challenging: choosing a small number of campaigns can make it difficult to secure impressions, while securing the spot with a large number of bidders inevitably raises the price dueto competition. In this paper, this problem is formalized and solved via novel Multi-Armed-Bandit (MAB) algorithms.

Problem statementConsider a sequence of \(T\) ad impressions sold through _second price auctions_ (see  for a survey). At auction \(t[T]\), each participant (bidder) bids on the item based on its own (stochastic) value for the item. The highest bidder wins the item and pays a price equal to the second highest bid. The _decision maker_ (the DSP) runs \(N^{*}\) advertising campaigns forming a _coalition_. At time \(t\), two groups of bidders participate: (1) \(n_{t}[N]\) bidders from the coalition chosen by the decision maker _ex-ante_ - without knowing the realization of the bidders' values - and (2) \(p^{*}\) other bidders, that we call the _competition_. When a bidder from the coalition wins the auction, the decision maker observes the realized value for the winner (also called _winning bid_). In the rest of the paper, the following assumptions about the behavior of bidders is made.

**Assumption 1**.: _All bidders are identical, their values are sampled i.i.d. from a distribution supported on \(\) characterized by its cumulative distribution function (c.d.f.) \(F\). All bidders bid their value._

Assuming identical bidders with i.i.d values is a strong but widespread assumption in auction theory , known as the symmetric bidders case. It is particularly relevant in online advertising, notably in homogeneous impression markets where advertisers compete for similar ad displays due to shared objectives, target demographics, or placement competition. The bounded support assumption is also standard, as letting an automated system bid arbitrarily large values is unrealistic. Finally, bidders bid their value as this is a weakly dominant strategy in this case. Lastly, assuming a known number of competitors \(p\) is frequently seen in auction models (see for instance  chapter 3.2.2). Under Assumption 1, the expected reward received by the decision maker at time \(t\) is given by \(r(n_{t})\), where \(r\) is the _expected reward function_, defined by

\[r:n[N] r(n)_{=(v_{i})_{t[n+p]}  F F}(_{(1)}-_{(2)})*{argmax}_{i[n+p]}v_{i}[n]} \]

where \(_{(1)}\) and \(_{(2)}\) are respectively the first and second maximum of \(\), and \([n]\) is used to abbreviate \(\{1,,n\}\). The problem therefore reduces to a MAB where the decision maker chooses _arms_\(n_{1},,n_{T}[N]\) sequentially and aims to minimize its cumulative _expected regret_\((T)\) defined by

\[(T)=_{t T}r(n^{*})-r(n_{t})\, n^{*}= *{argmax}_{n[N]}r(n), \]

given that privacy constraints from the browser  only let the decision maker observes (1) if the coalition won, (2) the realization of the maximum value when winning.

Related worksFollowing (2), the problem presented in this paper can be formulated as a Multi-Arm Bandits (MAB, see  for a survey). In MAB, a learner repeatedly selects from a set of actions, or "arms", each yielding a reward. The goal is to maximize total rewards by striking a balance between exploration (sampling various arms to learn their rewards) and exploitation (picking the arms with the highest anticipated rewards based on collected feedback). While the literature has known a significant development in the last years (, to name a few), the most popular approaches arguably remain _exponential weights algorithms_ (EXP3, ) in adversarial settings, and _optimism in face of uncertainty_ (UCB, ) when rewards are stochastic.

While UCB and EXP3 can both tackle the regret minimization problem presented here, they inevitably achieve sub-optimal performance due to not using the inherent _structure_ of the expected reward function. Several types of structure have been explored in the bandit literature, some notable examples being linear bandits , Lipschitz bandits , or unimodal bandits . The problem considered here is novel in the literature of structured bandits, arising from the observability restrictions coming with privacy-enhancing systems. Still, in the next section we show that unimodality - in this paper the fact that \(r\) admits only one local (hence global) maximum - is in many cases inherited from this stronger structure. A typical strategy to exploit unimodality - also used in this work - consists in playing a standard bandit policy (such as UCB) on a well chosen subset of arms (OSUB, ).

Last, the use of online learning algorithms to tackle repeated auction problems have been explored in various contexts (). However, none of these works approach the problem through the perspective of a coalition of bidders, and are thus not applicable to this setting.

Outline and contributions.Section 3 presents two novel bandit algorithms: LG (Local Greedy) which is inspired by OSUB, and GG (Greedy Grid) which combines Local Greedy and a successive elimination strategy. Theorem 2 and Theorem 3 provide upper bounds on the regret of LG and GG respectively, which are summarized in Table 1. Both algorithms achieve problem-dependent regret independent of \(T\). However, their scaling differs: the regret of LG depends on the _worst local gap_\(=_{n[N]}|r(n+1)-r(n)|\), while for GG it only depends on the gaps \(_{n}=r(n^{*})-r(n)\). Furthermore, w.h.p. GG only suffers regret for arms in a _reference grid_\(\) containing \(((N))\) arms and in a _neighborhood_\(^{*}\) of the optimal arm. All these quantities, as well as the notation \(}\) and \(}_{N}\) (hiding logarithmic factors), are defined in Section 3. These regret upper bounds rely on three key ingredients presented in Section 2: (1) an estimator of \(r(n)\) from feedback collected from any arm \(k\) (2) novel concentration bounds on these estimates for \(k\) within an estimation neighborhood of \(n\) (Theorem 1) and (3) the unimodality property of \(r\) under standard assumptions on \(F\). Lastly, Appendix D provides an experimental benchmark comparison of the performance of GG, LG and their competitors: LG has the lowest expected regret among the algorithms tested. Indeed, LG avoids the explicit use of the confidence bounds in the algorithm which makes it more practical, even though GG admits better theoretical guarantees.

## 2 Estimating the reward function from samples of powers of \(F\)

In this part, we put aside the sequential nature of the repeated auction setting that we introduced and consider the problem of estimating the expected reward as a function of the number of bidders, given a stream of collected data. We first present a formulation of the expected reward function in terms of powers of the c.d.f. \(F\). Then, we leverage this formula to introduce _power estimates_, as a solution to estimate the expected reward of an arm \(n[N]\) from samples collected from an arm \(k[N]\). Lastly, we discuss the theoretical properties of these estimates, introducing upper and lower confidence bounds on the expected reward in Theorem 1.

### Properties of the expected reward

The expected reward function \(r\) (Eq. (1)) can be expressed as a function of \(n\), \(p\) and the c.d.f. \(F\).

**Lemma 1**.: _The expected reward function defined in Equation (1) satisfies,_

\[n[N] r(n)=n_{0}^{1}F^{p+n-1}(x)-F^{p+n}(x)x \]

The proof can be found in Appendix A.1 and is based on properties of order statistics.

This particular definition of \(r(n)\), which is a product of \(n\) and a function that decreases with \(n\), suggests that \(r\) could be unimodal for some choices of \(F\). In the rest of the paper, we restrict ourselves to distributions that guarantees unimodal reward functions.

**Assumption 2**.: \(F\) _and \(p\) are such that the reward function \(r\) in Equation (3) is unimodal_

As the next lemma shows, many classical distributions lead to unimodal rewards for all \(p\).

**Lemma 2**.: _Let \(F\) be the cumulative distribution function of a Bernoulli, truncated exponential or Complementary Beta distribution. Then, for any \(p^{*}\), \(r\) in Equation (3) unimodal._

   Algorithm & Regret upper bound \\  EXP3 & \(()\) \\ UCB1 & \((_{n[N]}})\) \\ OSUB & \((}+-1}+ _{n[N]}(T)}{^{2}})\) \\ LG (this paper) & \(}_{N}(_{n[N]}}{^{2}})\) \\ GG (this paper) & \(}_{N}(_{n^{*}}}+ _{n}}{^{2}})}(^{*}|)T})\) \\   

Table 1: Comparison of regret guarantees for different algorithmsThe proof of Lemma 2 can be found in Appendix A.2. Note that the Complementary Beta distributions , chosen for technical reasons, are similar to Beta distributions and any Beta distribution can be approached by a Complementary Beta. Furthermore, in Appendix A.3 we present experiments suggesting that \(r\) is unimodal for all \(p^{*}\) if \(F\) is the c.d.f of Beta or Kumaraswamy distributions. However, we also show in Appendix A.4 that this is not always the case, by providing a counter example. Nonetheless, we argue that (complementary) beta or truncated exponentials are flexible models for real world data, so Assumption 2 is reasonable in practice. We furthermore discuss in Section 3.2 the adaptation of our algorithms if this was not the case.

### Estimation of powers of \(F\)

Consider the feedback \(}=(w_{k,1},,w_{k,m_{k}})\) gathered after playing arm \(k\) and winning the auction \(m_{k}\) times. \(}\), represents the sequence of first values (value of the winning bid) which has been _collected by arm \(k\)_.

It is well known that the marginal distribution of any order statistic can be expressed as a function of the c.d.f. \(F\) (see Section 2.1 of ). The distribution of any element of \(_{k}\) has cumulative distribution function \(F_{k}:x F^{k+p}(x)\), which clearly exhibits a one-to-one mapping between \(F_{k}(x)\) and \(F(x)\). Hence, given \(_{k}\), for any \(\) we can estimate \(F^{}\) by

\[_{k+p}^{}:x(_{k+p}(x))^{}, _{k+p}:x}_{j=1}^{m_{k}} \{w_{k,j} x\}}). \]

Estimation of \(r\)Consider any arm \(n[N]\). Following Equation (3), it appears that estimating both \(F^{n+p}\) and \(F^{n+p-1}\) is sufficient to construct an estimate of \(r(n)\). According to Equation (4), this can be done from samples originated from any arm \(k[N]\), by using the _simple estimate_

\[_{k}(n)=n_{0}^{1}(_{k+p}^{n+p-1}(x)- _{k+p}^{n+p}(x))x. \]

Furthermore, it also clear that any convex combination of estimates can become a new estimate, however in the rest of the paper we focus on simple estimates for simplicity.

**Remark 1** (Adaptation to different feedback).: _A similar procedure can be derived for a setting where the sequence of second prices would be observed instead. Indeed, their distribution would be \(G_{k}:x(k+p)F(x)^{k+p-1}-(k+p-1)F(x)^{k+p}\), which can lead to a reward estimate similar to (5) by using a suitable inversion formula. The same can be said for the case where both first and second prices are observed, with additional complexity because the joint distribution should be considered since for each auction the first and second price are dependent variables._

### Concentration of estimates of the reward function

We now introduce the first theoretical contribution of this paper: confidence bounds on the deviations of an empirical estimate \(_{k}(n)\) w.r.t. the true expected reward \(r(n)\).

Importance of (relatively) local estimationIn principle, (5) suggests that samples from any arm \(k[N]\) can provide a simple estimate of the reward function of any other arm \(n[N]\). However, we establish that the position of \(k\) w.r.t. \(n\) significantly impacts the concentration of \(_{k}(n)\). Intuitively, the ratio\((n+p)/(k+p)\) determines how the uncertainty on \(F^{k+p}\) propagates on the reward after performing the inversion to obtain an estimate of \(F^{n+p}\). Indeed, considering any \(i\), if for some \(x\) the deviation \(F(x)^{i}-_{i}(x)\) is small then a first order approximation provides that

\[ j\ :\ \ (F(x)^{i})^{}-_{i}(x)^{} (F(x)^{i}-_{i}(x))F_{i}(x)^{-1}. \]

Hence, a small error on \(F(x)^{i}\) is multiplied by \(F_{i}(x)^{-1}\) to obtain the resulting error on \(F(x)^{j}\). For \(j i\) this term can be as large as \(j/i\) while for \(j<i\) it can be arbitrarily large if \(F^{i}(x)\) is very small. This observation motivates a restriction on the range of arms that can be used to estimate the reward of a given arm \(n\), that we call its _estimation neighborhood_. We use the convention that arms smaller than \(1\) or greater than \(N\) exist but have not collected any sample and have a known reward of \(0\).

**Definition 1** (Estimation neighborhood of an arm \(n\)).: _Assume3 that \(p 4\) Then, the estimation neighborhood of \(n\) is the range \((n)=[v_{}(n),v_{r}(n)]=k[N]:\ k+p,(n+p-1)}\). We call \(v_{}(n)\) and \(v_{r}(n)\) respectively the furthest left and right neighbor of \(n\)._

**Theorem 1** (Concentration of simple estimates).: _Consider any \(n[N]\) and \(k(n)\). Let \(_{k}(n)\) be defined according to (5) from \(m_{k}\) samples collected by \(k\). Then, there exists some constants \(_{k,n}\) (depending on \(n,k,p\)) and \(_{k,n,F}\) (additionally depending on \(F\)) such that, with probability \(1-\),_

\[|_{k}(n)-r(n)|_{k,n}}}{})}{m_{k}}}+n_{k,n,F}(}}{})}{m_{k}})^{}. \]

_Furthermore, the constants admit universal upper bounds for any \(n,k,p,F\). For instance if \(m_{k} 4\) it holds that \(_{k,n} 33\) and \(_{k,n,F} 100\)._

Proof sketch (see Appendix B for the detailed proof).: The first ingredient consists in approximating the reward formulation (3) by a Riemann sum: for some step size \(D^{-1}>0\), it holds that \(_{k}(n)-r(n)=_{s=0}^{D-1}(x_{s})+_{D}\), with \(x_{s}=s/D\) for all \(s\{0,,D-1\}\). In Lemma 4 we use elementary properties of \(F\) to show that the approximation error satisfies \(_{D}[0,nD^{-1}]\). Next, we upper and lower bound \((x_{s})\) with different concentration bounds according to the value of \(F_{k,s} F(x_{s})^{k+p}\). More precisely, for any \((0,1)\) the following bounds hold each with probability at least \(1-\),

\[|_{k}(x_{s})-F_{k,s}|})}{m_{k}}}F_{k,s} I_{0} [},1]&,\\ _{k}(x_{s})}F_{k,s}  I_{1}(},} )&,\\ _{k}(x_{s})=0F_{k,s} I_{2}[0,}]&.\]

These results are derived in Lemma 5 from a well-known multiplicative form of the Chernoff bound for Bernoulli random variables . Then, the analysis consists in using the appropriate bound for each point \(s\{0,,D-1\}\). The interval \(I_{0}\) provides the first term in (7), which is dominant in terms of \(m_{k}\), and we make \(_{k,n}\) fully independent of \(F\) by carefully using some properties of the reward function. The two remaining intervals \(I_{1}\) and \(I_{2}\) provide the second term in (7), and \(_{k,n,F}\) depend on \(F\) through the boundaries of the interval \(I_{1}\). The corresponding factor in \(_{k,n,F}\) can be bounded by \(1\) or estimated in practice (see Appendix B.4). 

In Appendix B, we give the expression of \(_{k,n}\) and \(_{k,n,F}\) and provide in (17) and (19) fully explicit upper and lower confidence bounds on \(_{k}(n)\), depending on all problem parameters, and that are much tighter than what the universal constants provided in the theorem suggest. These universal constants are purely indicative, in order to assess that \(_{k,n}\) and \(_{k,n,F}\) do not diverge for any value of the problem parameters. We now provide more high-level comments on the derivation of this result.

DiscussionThe proof of Theorem 1 is non-trivial, and the careful usage of the Chernoff bounds that we introduced is crucial to obtain tight bounds on \(_{k}(n)\) for two reasons. First, it seems necessary to concentrate estimates from arms \(k>n\) (see the discussion below (6)), which are instrumental to the performance of the bandit algorithms presented in the next section. Secondly, by exhibiting powers of \(F\), they make \(_{k,n}\)**not** increasing linearly in \(n\), which is not easy to achieve. Indeed, it is clear from the analysis that this cost would be inevitable with standard Hoeffding bounds. However, completely avoiding \(n\) seems difficult in general, so our proof provides a way to mitigate its cost by multiplying it by a higher power of \(m_{k}^{-1}\), at least \(m_{k}^{-}\) (if \(k+p=(n+p-1)\)). This is the theoretical motivation for the definition of \((n)\) (Definition 1): while \(k+p=2(n+p-1)\) would lead to theoretically valid results, it would not ensure that the linear term in \(n\) is second-order in \(m_{k}\).

We now conclude this section by exhibiting a condition on \(F\) that allows to reduce the scaling of the confidence bound in \(n\) to logarithmic terms.

**Lemma 3** (Improved bound for Lipschitz quantile function).: _Assume that \(k(n)\) and \(F^{-1}\) is \(L\)-Lipschitz, then there exists an absolute constant \(\) such that with probability \(1-\) it holds that_

\[|_{k}(n)-r(n)|_{k,n}}}{})}{m_{k}}}+ L(}}{})(} }{})}{m_{k}})^{} \]

This result is proved in Appendix B.3, and shows that for some distributions (e.g. "close" to uniform) the confidence bounds converge relatively fast to standard sub-Gaussian type of bounds, even for very large \(n\). Whether this result holds in general remains open.

## 3 Bandit algorithms

### Bandit algorithms: Local-Greedy (Lg) and Greedy-Grid (Gg)

We now detail the two novel bandit algorithms proposed to tackle the problem presented in Section 1. Both rely on the use of simple estimates of \(r(n)\) (see Section 2) by arms present in its _estimation neighborhood_\((n)\) (see Definition 1) and theoretically motivated by Theorem 1. In this section, for ease of exposition, we describe algorithms as if feedback was collected at every time steps. In Appendix C.1, we show that the algorithms and their guarantees only require a slight adaptation when the feedback is collected only when the auction is won.

#### 3.1.1 Local-Greedy

We first present Local-Greedy (Lg), which is a natural adaptation of a standard policy in unimodal bandits, QSUB.The main idea of QSUB is to play UCB locally around a reference arm, and eventually reach the optimal arm \(n^{*}\) by gradually moving the reference arm in its direction. With Lg, we adapt this principle to efficiently exploit the structure of the problem considered: at each round \(t\), Lg defines a reference arm \(_{t}\), called _leader_, but plays _greedily_ in the _neighborhood_\((_{t})\), based on simple power estimates computed with samples from \(_{t}\) only. In addition a _sampling requirement_, implemented by a parameter \((0,1)\), is used in order to ensure the good concentration of these estimates. We detail Local-Greedy in Algorithm 1 below.

```
Input: exploration parameter \(\), neighborhoods \(((n))_{n[N]}\) (Definition 1)  Play \(n_{1}=1\) and observe \(w F^{1+p}\) ; for\(t 2\)do  Set \(_{t}=n_{t-1}\), compute \((_{_{t}}(n))_{n(_{t})}\) (Eq.(5)) ; \(\) Compute estimates from the leader If\(m_{t}\{s[t-1],n_{s}=_{t}\} t\): play \(n_{t}=_{t}\) ; \(\) Linear sampling requirement Else: play \(n_{t}*{argmax}_{n(_{t})}_{_ {t}}(n)\) ; \(\) Greedy play in \((_{t})\) Observe \(w F^{n_{t}+p}\) ; \(\) Record feedback
```

**Algorithm 1** Local Greedy (Lg)

High-level properties of LgFirst, using Greedy instead of UCB is only possible because of the structure of the problem: when \(_{t}\) is well-explored the estimates of arms in \((_{t})\) computed with samples from \(_{t}\) are sufficiently close to the true reward, so that _no exploration is needed_. The sampling requirement then guarantees that all greedy plays are made when \(_{t}\) is well explored.

A second property is that since \(|(_{t})|\) grows with \(_{t}\), a sequence of _locally optimal moves_ (best play in a given neighborhood) allows to reach the optimal arm exponentially fast (in \(((N))\) steps), which is particularly interesting in practice if \(N\) is large. On the other hand, Lg might suffer from the inherent drawback of any "local" policy: identifying a high-rewarding arm in a neighborhood can take a long time if the reward curve in this area is flat (depending on how small are the "local" gaps). This problem can be attenuated, but not solved, by adding an initial exploration phase. We propose Greedy-Grid, presented in the next section, as a way to fully address this issue.

Lastly, requiring only the computation of empirical reward estimates is a strength of Local-Greedy. Indeed, deriving tighter confidence bounds would improve its analysis, but not the practical implementation (and performance) of the algorithm.

#### 3.1.2 Greedy-grid

The concept of Greedy-Grid is very intuitive: it plays a Local-Greedy strategy only if it can tell which segment of the reward function contains the best arm with high probability. To implement this idea, GG uses a Successive-Elimination procedure  on a _subset_ of arms forming a _reference grid_, denoted by \(\).

Reference gridThe grid \(\) is designed so that two of its successive arms belong to their respective neighborhood (Definition 1), and can hence mutually estimate themselves and all arms in between (Theorem 1). In particular, the optimal arm can be well-estimated at least by its two closest neighbors on the grid, so its neighborhood can be "discovered" with high probability simply by sampling the points in the grid in a round-robin fashion for a sufficiently long time.

Following Definition 1, we construct \(\{s_{i}\}_{i 1}\) recursively: \(s_{1}=1\), and for \(i 2\) we set \(s_{i+1}=\{s s_{i}:s[N],\ s(s_{i}),s_{i} (s)\}\). We provide an illustrative example below.

**Example 1**.: _For \(N=2000\) and \(p=100\) the grid is \(=\{1,50,123,233,398,645,1016,1572\}\)._

Any arm \(n[N]\) admits a left and right "neighbor in the grid", denoted respectively by \(v_{l}^{}(n)\) and \(v_{r}^{}(n)\) and defined by: \(v_{l}^{}(n)=0\) if \(n<\), \(v_{r}^{}(n)=N+1\) if \(n>\) and \((v_{l}^{}(n),v_{r}^{}(n))=*{argmin}_{(x,y) \{n\}:\ n[x,y]}(y-x)\) otherwise. We call the "bin" of arm \(n\) all arms between its left and right neighbors: \((n)=\{n[N],v_{l}^{}(n^{})<n<v_{r}^{} (n^{})\}\). For simplicity we use the notation \(^{}=(n^{})\)4.

Greedy-GridWe provide the detailed implementation in Algorithm 2 below, and now describe the general principle of the algorithm. At each round, it operates in two steps. In the first step, it decides whether to play arms on the grid \(\) (play the grid, to simplify), or to focus on a specific bin (and, as we will see, _play greedy_). This choice depends on an elimination procedure: an arm \(k\) in \(\) should be _eliminated_ for this round if their _upper confidence bound_ (UCB) is smaller than the best _lower confidence bound_ (LCB) among all other arms. Furthermore, if there exists an eliminated arm whose index is closer to the index \(i_{t}^{*}\) of the arm with the best LCB, then the unimodality assumption implies that \(k\) should also be eliminated. The set of arms not eliminated at \(t\) is called \(_{t}\) in Algorithm 2.

To compute the UCB (\(U_{n}\)) and LCB (\(L_{n}\)) of an arm \(n\), we elect a leader \(_{n}\) which is the arm in \([v_{l}^{}(n),v_{r}^{}(n)]\) that was played the most in the last \(t\) rounds and then compute the bounds based on \(_{n}\), using Theorem 1. We show in the proof of Theorem 3 that this procedure ensures that a linear number of samples in \(t\) is used to compute the UCB and LCB of arms in \(^{}\) with high probability.

If at least one arm is not eliminated (\(_{t}\) is not empty), arms in \(_{t}\) are played one after the other (Round Robin). If all arms in the grid are eliminated, GG plays greedily in the bin \(B(i_{t}^{*})\) of the arm with the highest LCB. The empirical reward of each arm \(n B(i_{t}^{*})\) is computed similarly as \(U_{n}\) and \(L_{n}\) using samples from the leader \(_{n}\). GG then plays the best empirical arm \( t\) times which is the same sampling requirement as LG.

The careful design of Greedy-Grid prevents the main theoretical drawback of Local-Greedy: since the algorithm has a very low probability to play in a sub-optimal bin, it almost never pays "local gaps" in a sub-optimal part of the reward function. However this guarantee comes at a cost: if \(n^{}\) is not in the grid, it will never be played until the confidence intervals shrink "sufficiently" to eliminate the entire grid. Hence, GG might be more conservative than LG in practice, while offering better theoretical guarantees. We express this trade-off in the next section.

### Regret upper bounds

We now present the theoretical results obtained for the two algorithms presented in Section 3. We first establish the regret bounds and sketch their proofs, before discussing and comparing the results. We introduce some notation, that considerably simplifies the presentation of the results.

Notation:\(}\) and \(}_{n}\)For any \(x>0\), we use the notation \(}(x)\) to describe a quantity that scales in \(x\), up to logarithmic terms **in \(x\) and \(N\)** (hence the notation is linked to the problem). Furthermore,for \(n[N]\) we also use \(}_{n}\) as a shorthand notation for \(}(\{n^{6} x\} n^{2}x)\). This type of constants emerges from using (7) (Theorem 1) in the analysis. Indeed, we proved that the simple estimate of an arm \(n\) by an arm \(k(n)\) admit sub-Gaussian ("square-root") confidence intervals, independent of \(n\), when the sample size of \(k\) is larger than \((n^{6})\).

**Theorem 2** (Regret bound for Local-Greedy).: _Let \(_{n[N-1]}|r(n+1)-r(n)|\) (worst local gap). Under Assumption 2 and with \(=(_{3/2}N+1)^{-1}\), the regret of LG is upper bounded by a problem-dependent constant: there exists \((C_{n})_{n[N]\{n^{}\}}\), each satisfying \(C_{n}=}_{N}(}{^{2}})\), such that \(_{T}_{n[N] n^{}}C_{n}\)._

_Additionally, if the arm set forms a single estimation neighborhood, that is \( n[N]:\ (n)[N]\), then each constant \(C_{n}\) can be refined to \(}_{n}(_{n}^{-1})\), providing \(_{T}=}()\), which holds even when the reward function is not unimodal._

Proof sketch (see Appendix C.3 for the detailed proof).: We start by the case where the arm set forms a single neighborhood. Since LG is guaranteed that any arm it selects will provide an estimate for all the other arms, this context is very similar to a full information scenario. This explains why GG achieves both constant regret depending on the gaps, and a gap-independent bound in \(\). Furthermore, the hidden logarithmic constants come from carefully using Theorem 1 to separate the linear term in \(n\) from the gaps when they are small.

The general case presents an additional complexity. Indeed, it is possible that playing arm \(n n^{}\) is _locally optimal_, if \(n\) is the best arm in the neighborhood of the current leader: playing \(n\) in that context would not be unlikely. To tackle that scenario, we prove that pulling arm \(n\) at time \(t\) necessarily implies a _locally sub-optimal play_, in some estimation neighborhood, at some point in the past (maximized by the chosen value of \(\)). We then show that this cannot happen after some deterministic time w.h.p., leading to constant regret. However, since the sub-optimal play might be any arm the constant now depends on the _worst local gap_\(^{2}\). 

**Theorem 3** (Regret upper bound for Greedy-Grid).: _Suppose that GG is tuned with confidence level \(_{t}=^{3}}\), and \(=1/4\). Then, for any \(T\) it holds that_

\[_{T}=}_{N}(_{n^{ }}}+_{n}} _{n}(\{n<n^{}\}}{_{v_{l}(n^{})}^{2} }+\{n>n^{}\}}{_{v_{r}(n^{})}^{2}}) )\.\]

_Additionally, it holds that \(_{T}=}(^{}|)T })\), for \(K=_{3/2}(N)\)._

Proof sketch (see Appendix C.4 for the detailed proof).: First we prove that, w.h.p., during a linear time range in \(t\) GG either played the grid or in \(^{}\). Hence, arms \(n[N]\{^{}\}\) are played a (universal!) constant number of times by GG in expectation. Then, for \(n\) the term in \(}\) comes from the standard analysis of UCB ; while the constant bound comes from exploiting that after a constant time \(n\) the LCB of \(n^{*}\) eliminates its neighboors w.h.p., and by extension the entire grid. Finally, the constant bound \(n^{*}\) is derived similarly as the first bound of Theorem 2. 

DiscussionFirst, we show that being able to estimate \(r(n)\) from the feedback obtained after playing an arm \(k\) in its estimation neighborhood leads to a regret independent of \(T\) for both LG and GG. For the former, the bound depends in general on the worst _local gap_\(\), while for the latter only the actual gaps \(_{n}\) (with \(n^{}\)) are involved. This difference permits to obtain a problem-independent guarantee for GG for any configuration of \(p\) and \(N\). Furthermore, its scaling \(^{*}|}+_{3/2}(N)}\) can be much smaller than \(\) if \(n^{*}\) is small.

Then, we would like to discuss the impact of the concentration bound presented in Theorem 1 on the regret of both GG and LG. Indeed, a naive approach with Hoeffding bounds would not allow to remove \(n\) from the first order term of the concentration bound, because of the multiplicative factor \(n\) in the definition of \(r(n)\). A feature of our concentration bound is that the linear scaling in \(n\) does not appear in the first order term. Informally, this allows to exhibit terms of order \(}_{N}(_{n}^{-1})\) in the regret analysis instead of \(}(N^{2}_{n}^{-1})\), which can be significantly better for small gaps. A remark here is that the size of the grid in GG could be optimized as a larger grid makes the second order term in Theorem 1 smaller but is paid linearly in the regret.

We nevertheless highlight some potential for improvement in the analysis of LG. First, the local gaps \(\) in the bound of LG could be replaced by (in spirit, referring to \(\) for simplicity) \(_{n[N]}|r(v_{l}^{S}(n))-r(v_{r}^{S}(n))|\). It is clear though that this gap remains "local" and can be arbitrarily smaller than \(_{n}\) for some arms \(n[N]\), so the general interpretation of the results would be unchanged. Second, for simplicity, the analysis of LG was carried out using the constant upper bound of \(_{k,n}\) and \(_{k,n,F}\) but a tighter analysis could lead to a better dependency with respect to \(N\).

We now justify the use of simple estimates in GG and LG. In practice, combining estimates would allow to use more samples for the estimation. However, this would make the algorithm slower, and we believe that the sampling requirement implemented in the algorithms makes the use of simple estimates efficient: potential uniform exploration in a neighborhood is replaced by a focus on a single arm, but the same quality of information is accrued. Furthermore, from a theoretical perspective union bounds over the samples collected by each arm might also cost a factor \(N\) in the analysis.

Lastly, while GG admits better theoretical guarantees, LG might be more appealing in practice because it does not require to explicitly compute confidence intervals. This means that the regret bounds provided for LG are conservative, and might be refined with tighter confidence bounds without changing the algorithm.

Adaptation for non-unimodal rewardsWhile LG relies heavily on Assumption 2, GG can be readily adapted to handle non-unimodal reward functions. This is done by modifying the definition of the set of non-eliminated grid arms \(_{t}\) to \(\{s,U_{s} L_{i_{t}^{*}}\}\) in Algorithm 2. In that case, the algorithm can no longer eliminate arms on the grid based on the elimination of other arms. This naturally induces that the number of plays of sub-optimal arms is no longer bounded by a constant. In Theorem 4 (see Appendix), we show that only the \(((T))\) term persists for \(n\) in Theorem 3, while the problem-independent bound remains unchanged. Although we believe unimodality is necessary for achieving constant regret, this result demonstrates that, even without that assumption, GG can still provide the same logarithmic regret guarantees as UCB. However, it does so on a \(||\)-armed bandit, rather than an \(N\)-armed bandits with \(||=((N)) N\) for large \(N\).

Experimental resultsIn Appendix D we present a benchmark of LG, GG, UCB, EXP3 and OSUB on synthetic data in terms of the expected regret \((T)\). This benchmark illustrates the strong performance of LG relative to the other approaches. Although GG offers more robust theoretical guarantees, particularly with sub-linear problem-independent bounds, LG proves to be more effective in practice. Several factors may explain this gap between theoretical guarantees and empirical performance. First, as discussed in the previous section, the worst-case local gap in the analysis of Local Greedy (Theorem 2) might be overly conservative. This worst-case scenario could occur under a combination of unfavorable conditions, such as poor initialization far from the optimal arm and a flat reward function, paired with bad luck in exploration. However, such a scenario is likely rare in practice and was not encountered in our experiments. Additionally, Local Greedy benefits from scenarioswhere it starts playing in the optimal neighborhood only after a few steps, a situation GG cannot exploit due to its need for sufficient statistical evidence to eliminate all suboptimal neighborhoods. While GG's caution leads to stronger theoretical guarantees, this comes at the cost of empirical performance. Moreover, GG's results are tied to the tightness of the confidence intervals in Theorem 2, a limitation that does not apply to LG. An interesting and challenging open problem remains whether LG can be modified to achieve the same theoretical guarantees as GG without sacrificing its performance. We leave this question for future work.

## 4 Conclusion

The bandit problem studied in this work is structured since playing arm \(n\) gives a reward \(r(n)\) determined by \(n\), \(p\) and the unknown c.d.f \(F\) and with probability \(\) an observation of a sample of the distribution with c.d.f \(F^{n+p}\).

While traditional bandit approaches give problem dependent bounds depending on \(T\), algorithms GG and LG presented in this work have constant problem dependent bounds. Furthermore, GG and LG avoid a quadratic dependency in \(N\) for large \(T\) thanks to new concentration bounds introduced in Theorem 1. Overall, while GG has the best theoretical guarantees, LG has better constants and is therefore better suited for most practical problems (see the discussion at the end of Section 3 and experimental results in Appendix D).

Whether an algorithm that has the theoretical guarantees of GG and the practical performance of LG can be designed is an interesting question. We believe that the main leverage to improve the practical performance of GG might be to derive tighter concentration bounds. Possible directions to improve over Theorem 1 might include: further refining the decomposition of the integral in (3) according to the value of \(F\), further use "empirical" components (depending on estimates of \(F\)), or even using ideas from the proof of the DKW inequality  to avoid the union bounds over the points of each interval in the decomposition. We leave these directions for future work.

To conclude, since in practice, a DSP can launch campaigns through multiple auctions, an interesting question is whether the current analysis could be extended to the case of \(A\) auctions where a play at time \(t\) is \((n_{a,t})_{a[A]}\) where \(_{a[A]}n_{a,t}=N\) and the reward is \(_{a[A]}r_{a}(n_{a,t})\) with \(r_{a}\) determined by integers \(p_{a}\), \(n_{a,t}\) and \(F_{a}\) in the same way that \(r\) depends on \(p,n_{t}\) and \(F\). How to explore each auction in parallel in an efficient manner and how to handle the case where some auctions must be assigned zero players are then the main questions to solve.