# Quantifying the Gain in Weak-to-Strong Generalization

Moses Charikar

Stanford University

moses@cs.stanford.edu

&Chirag Pabbaraju

Stanford University

cpabbara@cs.stanford.edu

&Kirankumar Shiragur

Microsoft Research

kshiragur@microsoft.com

###### Abstract

Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. [BIK\({}^{+}\)23] empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts--a phenomenon they term _weak-to-strong generalization_.

In this work, we present a theoretical framework for understanding weak-to-strong generalization. Specifically, we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the _misfit error_ incurred by the strong model on labels generated by the weaker model. Our theory reveals several curious algorithmic insights. For instance, we can predict the amount by which the strong model will improve over the weak model, and also choose among different weak models to train the strong model, based on its misfit error. We validate our theoretical findings through various empirical assessments.

## 1 Introduction

Present-day AI models demonstrate incredible capabilities at a variety of extremely difficult tasks. For this reason, they are frequently described as being _superhuman_, in that it seems hard to imagine a human displaying the same abilities as the AI model. For example, move 37 in AlphaGo's famous victory against Go expert Lee Sedol  has been described as being beyond the realm of human imagination. In this sense, today's AI models are well on the path of exhibiting _new_ and _emergent_ abilities . Ultimately, we want these new abilities to be aligned with what would be beneficial to humanity. This rationale is what primarily guides the training of large-scale AI models through human feedback . However, given that we expect AI models to pick up skills that we ourselves don't fully grasp as humans, how can we enable these highly capable models to realize their potential?

A recent work by  shows that not all hope is lost in this endeavor. To model humans as being _weak_ supervisors for increasingly _strong_ AI models, they conduct the following "weak-to-strong generalization" experiment. Suppose we finetune a small language model like GPT-2  on data with ground-truth labels for a task. What happens if we then finetune a large language model like GPT-4  on data labeled by GPT-2, instead of data having ground-truth labels? Would GPT-4 simply overfit to GPT-2's labels and do no better, or would it outperform GPT-2, given that it is inherently a much stronger model? The surprising experimental result is that GPT-4 trained in this manner outperforms GPT-2 when evaluated on the true data, for a variety of finetuning tasks. Notethat GPT-4 is able to outperform GPT-2 without ever seeing true labels when it was being finetuned. One plausible explanation for this is that GPT-4 was able to glean the essence of the finetuning task from GPT-2's labels, and since it is fundamentally a stronger model than GPT-2, this knowledge was sufficient for it to outperform GPT-2.1

In this work, we seek theoretical justification for why we might expect to see such a gain in accuracy in weak-to-strong generalization. Concretely, we ask:

_Does a weakly supervised strong model provably attain smaller error than its weak supervisor, and if so, can this gain be formally quantified?_

Towards answering this question, we show (Theorem 1) that in the concrete setting of regression, the true error of a strong model trained on weak labels is smaller than the error of the weak model, by _at least_ the error of the strong model on the weak labels itself. We call this latter quantity the _misfit_ between the weak and strong model. Our result can be stated as the following simple principle:

  Gain in accuracy in weak-to-strong generalization \(\) Misfit between the weak and strong model \\  

Intuitively, the misfit quantifies the _erroneous knowledge_ that the strong model _does not_ obtain from the weak model, and hence also the amount that the strong model _improves_ over the weak model. We note that the work of  does empirically show that the performance gain of the strong model scales directly with its misfit (or _disagreement_) with the weak model; our result thereby provides a precise quantification of this observation.

Key to obtaining our results is a _representation-theoretic perspective_ towards weak-to-strong generalization. We posit that the main difference between weak and strong models is in the disparity between the quality of their data representations. This disparity in representation quality can manifest, among other reasons, due to a difference in the expressivity and complexity of the weak and strong models, and the amount of pretraining data that they have seen. For example, in the experiments by , the weak and strong models used are GPT-2 and GPT-4 respectively; the latter is a significantly larger transformer architecture, pretrained on a much larger dataset than the former. As a broader analogy, consider the task of learning a new language. This is an easier task for a multilingual person than a monolingual person. A multilingual person has a richer representation for language, drawing from their knowledge of different syntax, lexical structures, and sounds in multiple languages. With this perspective, we can imagine finetuning tasks to be relatively simple functions (e.g., linear functions) composed with the appropriate representation. For example, if the task is about learning Italian, a suitable "Italian-specific" linear combination of the multilingual's representation of the problem (including features learned from Spanish and French, say) might allow them to better understand the new language, while the same might not work so well for a monolingual whose representation only has features learned from English.

Armed with this perspective, we model the task of learning a real-valued finetuning task under the least squares loss in the weak-to-strong generalization framework. We assume that there exists a ground-truth representation \(h^{}\) of the data, which makes it amenable to learn a finetuning task \(f^{}\) of interest. We imagine that the weak and strong models come equipped with representation maps \(h_{w}\) and \(h_{s}\) respectively, which are possibly obtained via pretraining on a corpus of data. Next, we imagine that the weak model sees data labeled by the target function \(f^{} h^{}\), and after finetuning, learns some arbitrary function \(f_{w} h_{w}\). At this point, the weak supervision pipeline begins. The strong model is fed with data labeled by \(f_{w} h_{w}\) (instead of the true labels \(f^{} h^{}\)), and as part of finetuning, outputs a function \(f_{sw}\) from a function class \(_{s}\), that minimizes the discrepancy between \(f_{sw} h_{s}\) and the data labeled by \(f_{w} h_{w}\) that it sees. Ultimately, we care about the error of \(f_{sw} h_{s}\) with respect to the _true_ finetuning task, namely \(f^{} h^{}\). Our main result (Theorem 1) precisely quantifies the gain in the accuracy of \(f_{sw} h_{s}\) over \(f_{w} h_{w}\) in terms of the misfit between them, under the assumption that the set of functions \(_{s}\) is a _convex set_.2 In many practical applications, the representation map is generally the forward pass of the data through a suitable neural network architecture, and the finetuning task is performed by the _last linear layer_ of the network. In such cases, our assumption that the set \(_{s}\) is convex readily holds true.

We validate our characterization of the gain in weak-to-strong generalization through various experiments (Section 5) on synthetic and real-world data. The experiments corroborate our theoretical findings. Namely, we observe that upon performing the above weak-to-strong supervision pipeline, the gain in accuracy of the weakly-supervised strong model over its weak supervisor more or less _exactly_ aligns with the misfit between the weak and strong models (Figure 2). We also demonstrate (Section 5.3) that the labels "weak" and "strong" models are nuanced and not solely dependent on expressive power; in fact, in a low-sample regime, a less expressive model produces a higher quality representation and should be considered a strong model. Our theory and experiments lead to several algorithmic insights and open up interesting questions. For example, one algorithmic heuristic that arises from our theory is the following: given access to different weak models, choose to deploy the strong model that achieves the smallest difference between the weak model error and misfit (Table 1). Our results also motivate the perhaps counterintuitive algorithmic question of obtaining weak models that lead to _large misfits_ with the strong model.3 Another possible line of inquiry could look into _ensembling_ across different weak models, and obtaining a gain close to the _sum_ of their individual misfits. At a more philosophical level, this is akin to a superhuman AI model assimilating knowledge from various humans, while correctly identifying and discarding each of their flaws.

## 2 Related Work and Preliminaries

### Related Work

The idea of converting a "weak" learner to a "strong" learner can be traced all the way back to the famous paradigm of _boosting_, if not earlier. The recent work by  frames this problem within the context of _superalignment_ which seeks to reliably align AI models smarter than humans to human intent. Thereafter, several works that study the training of a "strong" model guided in some capacity by a "weak" model have emerged. Some of these include instruction filtering by weak models , easy-to-hard generalization , weak-to-strong correction  and weak-to-strong hallucination inducement .

The weak-to-strong generalization paradigm is perhaps most closely related to the teacher-student model of training  (sometimes also referred to as knowledge distillation ), where a student model (typically smaller) is trained using data labeled by a teacher model (typically larger), and possibly some additional ground-truth data. The remarkable phenomenon of the student model outperforming the teacher has been observed in many works . Most relevant to us are formulations where the student model is _equally_ or _more powerful_ than the teacher model. There has been theoretical work explaining superior generalization of the student in some specialized settings, e.g., the work of  where the student also has access to ground-truth labels, or the work of , which operates under a certain _expansion_ criterion and _consistency_ loss. In contrast, our work does not assume that the student model has any access to ground-truth labels, and also does not incorporate a regularization term in the objective. Finally, the conceptual insight in our work about the performance gain of the student model scaling with its disagreement with the teacher model is closely related to the theory of generalization bounds based on disagreement between different classifiers .

### Preliminaries

We assume that the data domain is \(^{d}\), and assume that there exists a ground truth representation function \(h^{}:^{d}^{d^{}}\) that maps the data \(x\) to an enriched representation \(h^{}(x)\). We assume the existence of pretraining tasks, through which strong models obtain representations of the data from a function class \(_{s}:^{d}^{d_{s}}\), and weak models obtain representations from a function class \(_{w}:^{d}^{d_{w}}\). For example, \(_{s}\) can be the class of deep neural networks, and \(_{w}\) can be the class of shallow neural networks. The target finetuning task (composed with the ground truth representation) is denoted as \(f^{} h^{}\), and the function learnt by the weak model is denoted by \(f_{w} h_{w}\). We assume that the strong model learns finetuning tasks from a function class \(_{s}:^{d_{s}}\), and assume that the set \(_{s}\) is a _convex_ set. The convexity assumption requires that, for any \(f,g_{s}\), and for any \(\), there exists \(h_{s}\) such that for all \(z^{d_{s}}\), \(h(z)= f(z)+(1-)g(z)\). For example,\(_{s}\) can be the class of all linear functions from \(^{d_{s}}\) to \(\). However, we do not assume anything about either \(f^{}\) or \(f_{w}\); in particular, they need not belong to \(_{s}\). We denote the marginal data distribution by \(\). For any two functions \(f,g:^{d}\), we define the distance \(d_{}(f,g)=_{x}(f(x)-g(x))^{2}\), i.e., it is the average (with respect to \(\)) squared distance between the images of the functions.

## 3 Results

We first state a quantitative version of our main result that characterizes the gain in weak-to-strong generalization in terms of strong-to-weak misfit in the so-called _realizable_ setting. Namely, we assume that the target finetuning task \(f^{} h^{}\) can be equivalently written as \(f_{s} h_{s}\) for some \(f_{s}_{s}\).

**Theorem 1** (Weak-to-Strong Generalization under Realizability).: _Let \(h^{}:^{d}^{d^{}}\) be a ground truth representation map, and let \(f^{}:^{d^{}}\) be a finetuning task of interest. Let \(h_{s}:^{d}^{d_{s}}\) and \(h_{w}:^{d}^{d_{w}}\) be the strong and weak model representation maps respectively. Given some data labeled by \(f^{} h^{}\), let \(f_{w} h_{w}\) be the function learnt by the weak model, for some arbitrary function \(f_{w}:^{d_{w}}\). Now, for a convex set of functions \(_{s}\) mapping \(^{d_{s}}\) to \(\) let_

\[f_{sw}=_{f_{s}}\ d_{}(f h_{s},f_{w } h_{w})\] (1)

_be the function learnt by the strong model under weak supervision. Lastly, let us assume that there exists \(f_{s}_{s}\) such that \(f_{s} h_{s}=f^{} h^{}\). Then, we have that_

\[d_{}(f_{sw} h_{s},f^{} h^{}) d_{}(f_{w} h_{w},f^{} h^{})-d_{}(f_{sw} h_{ s},f_{w} h_{w}).\] (2)

On the left-hand side in (2) is the error of the weakly-supervised strong model on the true data. The first term on the right-hand side is the true error of the weak model, and the second term is the error of the weakly-supervised strong model on data labeled by the weak model (i.e., misfit). Thus, the inequality directly says that the weakly-supervised strong model improves over the weak model by (at least) an amount equal to the misfit. Note again that in practice, a popular way to finetune a pretrained model on task-specific data is by tuning the weights of only the last linear layer of the model. In these cases, \(_{s}\) is simply the set of linear functions, which is convex. We emphasize that neither of \(f^{}\) or \(f_{w}\) need to belong to \(_{s}\); as long as the strong model finds the _minimizer_ over a convex set of the loss on the weakly labeled data (as in (1)), the inequality in (2) holds.

Next, we relax the realizability assumption that the target task \(f^{} h^{}\) belongs to the space of functions that the strong model optimizes over. Instead, suppose that by composing \(h_{s}\) with functions in \(_{s}\), it is possible for the strong model to get a small distance \(\) to the target task. The strong model could obtain such a powerful representation map after having seen an abundance of pretraining data; the realizable case corresponds to \(=0\). We also relax the assumption that the strong model is able to obtain the true minimizer with respect to the data distribution \(\) as in (1). In reality, we can imagine that the strong model only sees a finite sample labeled by the weak model, and obtains \(_{sw}\) by minimizing the loss over this finite sample. Even with these relaxations, we can show that the same qualitative result as in Theorem 1 continues to hold, upto small error terms.

**Theorem 2** (Weak-to-Strong Generalization under Non-Realizability and Finite Samples).: _Let \(h^{}:^{d}^{d^{}}\) be a ground truth representation map, and let \(f^{}:^{d^{}}\) be a finetuning task of interest. Let \(h_{s}:^{d}^{d_{s}}\) and \(h_{w}:^{d}^{d_{w}}\) be the strong and weak model representations respectively. Given some data labeled by \(f^{} h^{}\), let \(f_{w} h_{w}\) be the function learnt by the weak model, for some arbitrary function \(f_{w}:^{d_{w}}\). For a convex set of functions \(_{s}\) mapping \(^{d_{s}}\), let_

\[f_{s}=_{f_{s}}\ d_{}(f h_{s},f^{ } h^{}),\] (3)

_and suppose that \(d_{}(f_{s} h_{s},f^{} h^{})=\). Now, suppose we obtain \(n\) weakly-labeled i.i.d. samples \((x_{1},y_{1}),,(x_{n},y_{n})\), where each \(x_{i}\) and \(y_{i}=f_{w} h_{w}(x_{i})\). Let_

\[_{sw}=_{f_{s}}\ _{i=1}^{n}(f  h_{s}(x_{i})-y_{i})^{2}.\] (4)

_Finally, assume that the range of \(f^{},f_{w}\) and all the functions in \(_{s}\) is absolutely bounded. Then, we have that with probability at least \(1-\) over the draw of \((x_{1},y_{1}),,(x_{n},y_{n})\),_

\[d_{}(_{sw} h_{s},f^{} h^{})  d_{}(f_{w} h_{w},f^{} h^{})-d_{ }(_{sw} h_{s},f_{w} h_{w})\] \[+O()+O(_{s}}{n} )^{}+O()^{},\] (5)_where \(_{_{s}}\) is a constant capturing the complexity of the function class \(_{s}\), and the asymptotic notation is with respect to \( 0,n\)._

As compared to (2), the bound in (5) has two sources of error terms: the first error term of \(O()\) arises (via standard triangle inequality arguments) due to the non-realizability assumption, and goes to zero as the strong model becomes stronger and more expressive. The latter two error terms arise (via standard uniform convergence arguments as in [TJJ20]) because the strong model only sees a finite weakly-labeled sample--these terms vanish too as the sample size becomes large.

## 4 Main Proof Technique

In this section, we outline the proof of realizable weak-to-strong generalization (Theorem 1). The proof of Theorem 2 uses the same main idea and is given in Appendix A. Recall that the strong model learns from a convex set \(_{s}:^{d_{s}}\) of finetuning tasks. Recall also that we denote the strong model representation map by \(h_{s}:^{d}^{d_{s}}\). Let \(V_{s}=\{f h_{s}:f_{s}\}\) be the set of all tasks in \(_{s}\) composed with the strong model representation. We first observe that \(V_{s}\) is also a convex set.

**Claim 3**.: \(V_{s}\) _is a convex set._

Proof.: Fix \(f,g_{s}\), and consider \(f h_{s},g h_{s} V_{s}\). Fix any \(\). Since \(_{s}\) is a convex set, there exists \(p_{s}\) such that for all \(y^{d_{s}}\), \(p(y)= f(y)+(1-)g(y)\). Now, fix any \(x^{d}\). Then, we have that

\[(f h_{s})(x)+(1-)(g h_{s})(x)= f(h_{s}(x))+(1- )g(h_{s}(x))=p(h_{s}(x))=(p h_{s})(x),\]

and hence \((f h_{s})+(1-)(g h_{s})=p h_{s} V_{s}\). 

We are then ready to prove Theorem 1.

Proof of Theorem 1.: The setting under consideration is depicted in Figure 1. Since we assume realizability, \(f^{} h^{} V_{s}\). Let \(A=d_{}(f_{sw} h_{s},f^{} h^{})\), \(B=d_{}(f_{sw} h_{s},f_{w} h_{w})\) and \(C=d_{}(f_{w} h_{w},f^{} h^{})\). We want to show that \(C A+B\). Recall that

\[f_{sw}=*{argmin}_{f_{s}}d_{}(f h_{ s},f_{w} h_{w}).\]

In other words, \(f_{sw} h_{s}\) is the _projection_ of \(f_{w} h_{w}\) onto the convex set \(V_{s}\). We can therefore apply the "Pythagorean theorem" for projections onto a convex set [Haz16, Theorem 2.1].

Concretely, for any \(g V_{s}\), observe that

\[d_{}(f_{w} h_{w},g)=_{x }(g(x)-(f_{w} h_{w})(x))^{2}\] \[=_{x}(g(x)-(f_{sw} h_{s})(x) +(f_{sw} h_{s})(x)-(f_{w} h_{w})(x))^{2}\] \[+2_{x}[(g(x)-(f_{ sw} h_{s})(x))((f_{sw} h_{s})(x)-(f_{w} h_{w})(x))]\] \[=d_{}(f_{sw} h_{s},g)+d_{}(f_{ sw} h_{s},f_{w} h_{w})\] \[+2_{x}[(g(x)-(f_{ sw} h_{s})(x))((f_{sw} h_{s})(x)-(f_{w} h_{w})(x))].\] (6)

Figure 1: \(f_{sw} h_{s}\) is the projection of \(f_{w} h_{w}\) onto the convex set \(V_{s}\).

But note also that by definition of projection, \(d_{}(f_{w} h_{w},g) d_{}(f_{sw} h_{s},f_{w}  h_{w})\), and hence

\[d_{}(f_{sw} h_{s},g)+2_{x}[( g(x)-(f_{sw} h_{s})(x))((f_{sw} h_{s})(x)-(f_{w} h_{w})(x))] 0.\] (7)

Now, fix \(t(0,1)\), and consider the function

\[w(t)=f_{sw} h_{s}+t(f^{} h^{}-f_{sw} h_{s}).\]

Namely, for any \(x\), \(w(t)(x)=(f_{sw} h_{s})(x)+t((f^{} h^{})(x)-(f_{sw}  h_{s})(x))\). Because \(V_{s}\) is a convex set (Claim3), \(w(t) V_{s}\). Also,

\[d_{}(f_{sw} h_{s},w(t)) =_{x}((f_{sw} h_{s})(x)-w(t)(x))^{2}\] \[=t^{2}_{x}((f^{} h^{ })(x)-(f_{sw} h_{s})(x))^{2}.\]

Hence, substituting \(w(t)\) for \(g\) in (7), we get

\[t^{2}_{x}((f^{} h^{ })(x)-(f_{sw} h_{s})(x))^{2}\] \[+2t_{x}[((f^{} h^{ })(x)-(f_{sw} h_{s})(x))((f_{sw} h_{s})(x)-(f_{w} h_{w})(x ))] 0.\]

Taking the limit as \(t 0\), we get that

\[_{x}[((f^{} h^{})(x)-(f_{sw}  h_{s})(x))((f_{sw} h_{s})(x)-(f_{w} h_{w})(x))] 0\] (8)

Substituting \(f^{} h^{}\) for \(g\) in (6), and using (8), we obtain the desired result

\[d_{}(f_{w} h_{w},f^{} h^{}) d_{ }(f_{sw} h_{s},f^{} h^{})+d_{}(f_{sw} h_{s },f_{w} h_{w}).\]

## 5 Experiments

We perform experiments4 on synthetically generated data as well as real-world molecular prediction and natural language datasets to verify the guarantees on weak-to-strong generalization given by our theorems. The results for the natural language tasks are given in AppendixC.

### Synthetic Experiments

We set the target data representation \(h^{}:^{8}^{16}\) to be a randomly initialized 5-layer multi-layer perceptron (MLP) with ReLU activations, with input dimension 8 and hidden layer dimension 16. The class \(_{s}\) of finetuning tasks from which the strong model (as well as the weak model) learns is simply the class of linear functions from \(^{16}\); \(_{s}\) is thus a convex set (see AppendixD for instances where \(_{s}\) is a non-convex set). The marginal data distribution \(\) in our experiments is always \((0,^{2}I)\). To ensure that the data is well-spread, we set \(=500\).

Representation Learning.We experiment with two different ways of obtaining the weak and strong representations \(h_{w}\) and \(h_{s}\):

1. **Pretraining:** We randomly sample \(T\) finetuning tasks \(f^{(1)},,f^{(T)}_{s}\). For each \(t[T]\), we generate data \(\{x_{j}^{(t)},y_{j}^{(t)}\}_{j=1}^{N_{r}}\), where \(x_{j}^{(t)}\) and \(y_{j}^{(t)}=f^{(t)} h^{}(x_{j}^{(t)})\). Loosely following , we obtain \(h_{w}\) and \(h_{s}\) as \[h_{k}=_{h_{k}}}_{t=1}^{T} _{j=1}^{N_{r}}(f^{(t)} h(x_{j}^{(t)})-y_{j}^{(t)})^{2}k \{w,s\}.\] (9) We set \(_{w}\) and \(_{s}\) (both \(^{8}^{16}\)) to be the classes of 2-layer and 8-layer neural networks respectively with ReLU activations and hidden dimension 16. We obtain \(h_{w}\) and \(h_{s}\) via gradient descent on the representation parameters to find the minimizers in (9). We set \(T=10,N_{r}=2000\). Additionally, we also consider the _realizable_ setting (Theorem1), where we explicitly set \(h_{s}=h^{}\), and only obtain \(h_{w}\) as above.
2. **Perturbations:** We also consider another way to obtain the weak and strong representations as direct perturbations of \(h^{}\). Namely, we perturb every parameter in every weight matrix in \(h^{}\) by independent Gaussian noise \((0,_{s}^{2})\) to obtain \(h_{s}\). Similarly, we obtain \(h_{w}\) by perturbing each parameter in \(h^{}\) by \((0,_{w}^{2})\). Ideally, we want the strong representation \(h_{s}\) to be a closer approximation of \(h^{}\) than \(h_{w}\). Hence, we set \(_{s}=0.01\) and \(_{w}=0.05\).

Weak Model Finetuning.Once the representations \(h_{w}\) and \(h_{s}\) have been obtained and fixed, we randomly generate \(M\)_new_ finetuning tasks \(f^{(1)},,f^{(M)}_{s}\), and obtain data \(\{x_{j}^{(i)},y_{j}^{(i)}\}_{j=1}^{N_{f}}\) for each of these tasks. Here again, \(x_{j}^{(i)}\) and \(y_{j}^{(i)}=f^{(i)} h^{}(x_{j}^{(i)})\). We set \(M=100,N_{f}=2000\). For each task, we train the weak model on the data generated for the task, to obtain

\[f_{w}^{(i)}=_{f_{s}}}_{j=1}^{N_{f} }(f h_{w}(x_{j}^{(i)})-y_{j}^{(i)})^{2}.\] (10)

Here, the representation parameters \(h_{w}\) are frozen, and \(f_{w}^{(i)}\) is obtained via gradient descent. Note again that we are training the weak models on _true_ data labeled by the finetuning task \(f^{(i)} h^{}\).

Weak-to-Strong Supervision.Once our weak models are trained for each finetuning task, we generate _weakly labeled data_. That is, for each \(i[M]\), we generate \(\{_{j}^{(i)},_{j}^{(i)}\}_{j=1}^{N_{f}}\) where \(_{j}^{(i)}\). But crucially, \(_{j}^{(i)}=f_{w}^{(i)} h_{w}(_{j}^{(i)})\). We now train our strong models on this weakly labeled data. Namely, keeping the strong representation \(h_{s}\) fixed, we obtain, via gradient descent again

\[f_{sw}^{(i)}=_{f_{s}}}_{j=1}^{N_{ f}}(f h_{s}(_{j}^{(i)})-_{j}^{(i)})^{2}.\] (11)

At this point, our weak-to-strong training procedure is complete.

Evaluation.For each finetuning task, we wish to evaluate the accuracy of our weak-to-strong model \(f_{sw}^{(i)} h_{s}\) with respect to the true task \(f^{(i)} h^{}\).

Towards this, we estimate 3 quantities:

1. Error of the weak-to-strong model \(f_{sw}^{(i)} h_{s}\) on the _true_ finetuning task: \(_{x}(f_{sw}^{(i)} h_{s}(x)-f^{(i)} h^{ }(x))^{2}\).

Figure 2: (a),(b),(c) Experiments on synthetic data. (d),(e),(f) QSAR tasks over MolBERT representations on the ESOL, FreeSolv and Lipop datasets. For each dataset, ChemBench  provides three different train, test and validation splits; multiple points of the same color correspond to weak-to-strong supervision for the same weak model (as specified in legend) across these splits.

2. Error of the weak model \(f_{w}^{(i)} h_{w}\) on the _true_ finetuning task: \(_{x}(f_{w}^{(i)} h_{w}(x)-f^{(i)} h^{}(x ))^{2}\).
3. Misfit error of the weak-to-strong model on the weakly labeled data: \(_{x}(f_{sw}^{(i)} h_{s}(x)-f_{w}^{(i)} h_{w }(x))^{2}\).

Each of these quantities are estimated from a fresh sample of size \(N_{f}\) drawn from \(\). For each task \(i[M]\), we plot the difference (b)-(a), namely the **Gain in Accuracy**, on the y-axis, versus the **Misfit** (c) on the x-axis. Figure 1(a) has the results for the realizable case where \(h_{s}=h^{}\) and \(h_{w}\) is obtained by pretraining. Figure 1(b) has the results for the non-realizable case where both \(h_{w}\) and \(h_{s}\) are obtained by pretraining. Figure 1(c) has the results for the non-realizable case where \(h_{w}\) and \(h_{s}\) are obtained by directly perturbing the weights in \(h^{}\). For reference, recall that Theorem 2 indicates that the gain in accuracy is (upto error terms) _at least_ the misfit. The plots in Figure 2 suggest that the gain is more or less _exactly_ the misfit, which is in agreement with our theory!

### Molecular Prediction

We also validate our conceptual insights on real-world molecular prediction datasets. Specifically, we follow the Quantitative Structure-Activity Relationship (QSAR) task setup in the MolBERT  paper. These tasks involve predicting physical properties of molecules like solubility, lipophilicity, etc. We consider three regression datasets: ESOL, FreeSolv and Lipop. These datasets are part of the MoleculeNet  benchmark suite, and have been curated into train, test and validation splits by ChemBench . The MolBERT paper provides weights for a standard-size BERT  architecture (hidden dimension 768, 12 layers, 12 attention heads) pretrained for 100 epochs on the GuacaMol  dataset. We use these weights as the strong representation \(h_{s}\). For the weak representations \(h_{w}\), we run the pretraining pipeline for substantially smaller transformer architectures and lesser compute time. Specifically, we consider transformers with just 2 layers and 2 attention heads, and vary the hidden size in \(\{8,12,16,32,48,64,96\}\). For each of these settings, we run the pretraining tasks for a mere 2 epochs to obtain different weak representations \(h_{w}\).

Once we have the representations \(h_{s}\) and \(h_{w}\), we can finetune a linear layer on top of these for each of the three regression datasets. We run the entire weak-to-strong supervision pipeline from above, where we weakly supervise the strong model \(h_{s}\) on labels given by each of the weak models \(h_{w}\). The results are given in Figures 1(d), 1(e) and 1(f). Again, we see that the gain in accuracy of the weakly supervised strong models is accurately characterized by their misfit on the weak labels.

We were also able to see an otherwise useful algorithmic insight in these experiments. Consider a setting where we have at our disposal various weak models, and have performed the weak-to-strong supervision pipeline separately on each of them. We now want to deploy one of the weakly trained strong models; our goal is to choose the one that gets the least error on the true data distribution. Recall that Theorem 2 guarantees that the error of a weakly supervised strong model is upper bounded (upto error terms) by the difference between the weak model's error and misfit. This suggests a natural heuristic: sort the strong models by the difference between the corresponding weak model's error and the misfit, and choose the one for which this quantity is smallest. We observed that this heuristic ends up working quite well--Table 1 shows the numbers for the Lipop dataset, while the results for ESOL and FreeSolv are in Appendix B.

### Strong-to-Weak Generalization and Low Sample Regime

In our simulations, we also consider an additional thought experiment, where we reverse the weak and strong models. That is, in the non-realizable case with pretraining (Figure 1(b)), we can have \(_{w}\) be the class of 8-layer MLPs, and \(_{s}\) be the class of 2-layer MLPs. Similarly, in the case with perturbations (Figure 1(c)), we can set \(_{w}=0.01\), and \(_{s}=0.05\). In this case, because the weak models have now become powerful, and can represent the true data well, the weak labels are essentially the _true_ labels. Hence, if we were to obtain the same plots, we would now expect the misfit on weak labels to essentially correspond to the _loss_ in accuracy of the strong model on true data, compared to the weak model. This is confirmed in Figures 2(a) and 2(b): the plots are mirror reflections of Figures 1(b) and 1(c):

Now, suppose that we are in a setting where the number of samples available for the representation learning task is scarce. Concretely, consider the original setting of Figure 1(b) with \(_{w}\) and \(_{s}\) back to being 2-layer and 8-layer MLPs respectively. Recall that for learning the representations \(h_{w},h_{s}\)we sampled \(T=10\) finetuning tasks \(f^{(1)},,f^{(T)}_{s}\), and obtained \(N_{r}=2000\) samples labeled according to each \(f^{(t)} h^{}\). Now instead, consider setting \(T=5,N_{r}=250\). The number of samples \(N_{f}\) in the weak model finetuning and weak-to-strong supervision stages is still maintained at \(N_{f}=2000\). We run the entire weak-to-strong supervision pipeline for this parameter setting. The rationale is that, when the representation learning task is data-deprived, the weak model, by virtue of being simpler, learns a better representation than the strong model, which is more complex. Indeed, this is what we observed, as shown in Figure 2(c). Observe that the trend in the plot is very similar to Figures 2(a) and 2(b), where we had _explicitly_ swapped the weak and strong models. This suggests that in the low-sample regime too, the weak and strong models have reversed roles. Thus, the definition of weak and strong models in the framework of weak-to-strong generalization should not solely be based on expressive power; instead, these roles should be assigned based on the quality of representations.

## 6 Conclusion

Employing a representation-theoretic perspective, we characterized the gain in performance in weak-to-strong generalization. Our results apply in the setting of learning real-valued functions with the least squares loss, where the strong model learns the finetuning task by optimizing over a convex set of functions. We quantify the gain in accuracy of the weakly-supervised strong model over its weak supervisor in terms of the misfit between the strong and weak models.

Our work has natural limitations. Our theorems notably do not apply when the set from which the strong model learns the finetuning task is not convex. Nevertheless, our experiments in Appendix D do suggest that our results should (at least qualitatively) hold even beyond the convex case. Our work also does not address classification tasks (see also Appendix E), and it would be interesting to see if similar results could be obtained for more general loss functions. Finally, while we do demonstrate results on real-world datasets, we anticipate that significantly larger-scale experiments on regression datasets used to train modern AI models will yield further interesting insights.

  Hidden dimension & Weak error - Misfit & True error of weakly-supervised strong model \\ 
96 & \(\) & \(\) \\
48 & \(0.9731 0.0707\) & \(1.1293 0.0418\) \\
24 & \(1.0331 0.0449\) & \(1.1204 0.0261\) \\
64 & \(1.0619 0.0441\) & \(1.1436 0.0124\) \\
32 & \(1.0624 0.0527\) & \(1.1302 0.0220\) \\
16 & \(1.1456 0.0276\) & \(1.1950 0.0484\) \\
12 & \(1.1499 0.0177\) & \(1.1869 0.0297\) \\
8 & \(1.1958 0.0194\) & \(1.2396 0.0310\) \\  

Table 1: Heuristic rule to choose among different weakly-supervised models finetuned on Lipop: choose the strong model that has the smallest difference (averaged across the 3 splits) between weak model error and misfit (\(\) is the std across splits). As we see, this model has the smallest true error.

Figure 3: Strong-to-weak generalization. The roles of the weak and strong models have reversed.