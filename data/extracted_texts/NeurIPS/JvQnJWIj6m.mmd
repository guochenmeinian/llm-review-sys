# Connecting Joint-Embedding Predictive Architecture

with Contrastive Self-supervised Learning

Shentong Mo\({}^{1}\), Shengbang Tong\({}^{2}\)

\({}^{1}\)CMU, \({}^{2}\)NYU

Corresponding author: shentongmo@gmail.com.

###### Abstract

In recent advancements in unsupervised visual representation learning, the Joint-Embedding Predictive Architecture (JEPA) has emerged as a significant method for extracting visual features from unlabeled imagery through an innovative masking strategy. Despite its success, two primary limitations have been identified: the inefficacy of Exponential Moving Average (EMA) from I-JEPA in preventing entire collapse and the inadequacy of I-JEPA prediction in accurately learning the mean of patch representations. Addressing these challenges, this study introduces a novel framework, namely C-JEPA (Contrastive-JEPA), which integrates the Image-based Joint-Embedding Predictive Architecture with the Variance-Invariance-Covariance Regularization (VICReg) strategy. This integration is designed to effectively learn the variance/covariance for preventing entire collapse and ensuring invariance in the mean of augmented views, thereby overcoming the identified limitations. Through empirical and theoretical evaluations, our work demonstrates that C-JEPA significantly enhances the stability and quality of visual representation learning. When pre-trained on the ImageNet-1K dataset, C-JEPA exhibits rapid and improved convergence in both linear probing and fine-tuning performance metrics.

## 1 Introduction

Unsupervised learning of visual representations has recently seen remarkable progress, primarily due to the development of innovative architectures and strategies that exploit unlabeled imagery. Among these advancements, the Joint-Embedding Predictive Architecture (JEPA)  has distinguished itself as a powerful approach. I-JEPA  leverages a masking strategy to extract visual features, facilitating significant strides in understanding and utilizing unlabeled visual data.

However, despite its successes, certain limitations within the JEPA framework have become apparent, particularly concerning its components I-JEPA Exponential Moving Average (EMA) and I-JEPA prediction capabilities. Specifically, I-JEPA EMA has been found to be inadequate in preventing the issue of entire collapse , while the I-JEPA prediction mechanism struggles to accurately learn the mean of patch representations. These challenges not only hinder the performance of JEPA but also limit its applicability in broader contexts.

Figure 1: Our C-JEPA achieves faster and better convergence than I-JEPA.

To address these limitations, we introduce a novel contrastive self-supervised learning framework based on JEPA, namely C-JEPA, which aims to address the aforementioned challenges by incorporating the principles of Variance-Invariance-Covariance Regularization (VICReg) . VICReg's methodology is adept at learning variance and covariance to avert entire collapse and ensure invariance for the mean of augmented views. By integrating VICReg with the Image-based JEPA, C-JEPA is designed to achieve faster and better convergence, as shown in Figure 1. In this paper, we aim to detail the theoretical underpinnings and empirical validations that substantiate the superiority of C-JEPA over previous self-supervised learning methods.

Our contributions are manifold and significant. Firstly, we identify and articulate the limitations inherent in the I-JEPA framework, specifically its EMA and prediction mechanisms. Secondly, we propose the C-JEPA framework as a novel solution that synergizes JEPA with VICReg to address these limitations effectively. Thirdly, through rigorous empirical and theoretical evaluations, we demonstrate that C-JEPA not only mitigates the issues identified but also achieves superior performance metrics when compared to existing frameworks. Particularly notable is C-JEPA's performance when pre-trained on the ImageNet-1K dataset, where it shows fast and improved convergence in linear probing and fine-tuning scenarios. These results highlight C-JEPA's potential to set a new benchmark in unsupervised visual representation learning, thereby contributing significantly to the field's advancement.

## 2 Related Work

**Self-supervised Learning.** In the self-supervised literature, researchers aim to exploit the internal characteristics of data and leverage pretext tasks to train a model. Recently, an unsupervised framework that learns effective views with data augmentation was proposed by Tian _et al._ to reduce the mutual information between views. CMC  introduced a multi-view contrastive learning framework with any number of views to learn view-agnostic representations. Another pretext task of solving jigsaw puzzles was developed in PIRL  to improve the semantic quality of learned image representations, achieving better object detection results than supervised pre-training. More recently, Masked image modeling (MIM) has been explored in many previous works [10; 11; 12; 13; 14] to reconstruct the masked image patch given the unmasked counterpart as clues. Some MIM approaches [10; 11; 12; 15; 16] designed customized masking strategies (_i.e._, random, block-wise) as pre-text tasks during pre-training. For example, block-wise masking was introduced in BEiT  to learn transferrable visual representations by recovering discrete tokens of masked image patches. Given features extracted from the 25% unmasked patches, the seminal work, MAE  directly reconstructed missing pixels of 75% masked patches. SimMIM  randomly masked the input image with a large square patch size (_i.e._, 32) and used a one-layer prediction layer after the encoder to predict RGB values of raw pixels. Other researchers [15; 16; 17; 18] started to leverage a teacher network like CLIP  or adversarial learning to generate the mask and supervision target.

**Contrastive Learning.** In the past years, contrastive learning has shown its effectiveness in self-supervised learning, where various instance-wise contrastive learning frameworks [20; 21; 22; 23; 24; 25; 26; 27; 4] and prototype-level contrastive methods [28; 29; 30; 31] were proposed. The general idea of the instance-wise contrastive learning is to close the distance of the embedding of different views from the same instance while pushing embeddings of views from different instances away. One common way is to use a large batch size to accumulate positive and negative pairs in the same batch. For instance, Chen _et al._ proposed a simple framework with a learnable nonlinear projection head and a large batch size to improve the quality of the pre-trained representations. To make the best use of a large amount of unlabelled data, they present a bigger unsupervised pre-training network and introduce distillation with unlabeled data in SimCLR v2  to improve the performance in downstream tasks. Without involving negative instances, BOYL  trains the online network from an augmented view of an image to predict the target network representation of the same image under a different augmented view (positive instance). Another broadly-used approach  in the ICL literature is to apply a momentum encoder to update negative instances from a large and consistent dictionary on the fly. The dynamic dictionary was used with a moving-averaged encoder in MoCo series [24; 23] to build a dynamic dictionary to update negative instances in a queue of considerable size. The Variance-Invariance-Covariance (VICReg)  regularization strategy has been proposed to address shortcomings in self-supervised learning by enforcing stability through variance and covariance constraints.

**Joint-Embedding Predictive Architectures.** The Joint-Embedding Predictive Architecture (JEPA) framework has shown considerable promise by enabling the prediction of embeddings from a compatible signal . Building on this, the Image-based Joint-Embedding Predictive Architecture (I-JEPA) introduced by Assran et al.  utilizes a context encoder and a target encoder, together with a predictor, to manage representations under a masking regime. This approach focuses on learning embedding spaces directly, contrasting with other methods that operate on pixel or token spaces. While I-JEPA has been successful, it faces challenges such as the inefficacy of Exponential Moving Average (EMA) strategies in preventing model collapse and inaccuracies in predicting the mean of patch representations. These limitations hinder the stability and efficacy of the learned representations, prompting a need for enhanced methodologies. By maintaining sufficient variance in the embeddings and minimizing redundancy among features, VICReg supports robust feature learning. To address the limitations of I-JEPA, our work integrates VICReg into the JEPA framework, thereby aiming to prevent total model collapse and ensure invariance across different views of the same image.

## 3 Methodology

In this section, we present a novel masked modeling framework designed for the joint-embedding predictive architecture to avoid entire collapsing and improve the mean of patch representations. Our key idea is to integrate VICReg into the JEPA framework for alleviating entire model collapse and improving the invariance across different views of the same image. We first provide preliminaries in Section 3.1, then provide the theoretical and empirical connection between I-JEPA and SimSiam in Section 3.2, and finally connect I-JEPA with VICReg in Section 3.3 to show the benefit of reducing entire collapsing and learning the mean of patch representations.

### Preliminaries

In this section, we first describe the problem setup and notations, and then revisit I-JEPA , SimSiam , and VICReg  for self-supervised image modeling.

**Problem Setup and Notations.** Given a dataset \(=\{x_{i}\}_{i=1}^{N}\) with images \(x_{i}^{c h w}\), our goal is to learn a neural network \(f_{}()\) to extract unsupervised representations from these visual samples.

**Revisit I-JEPA .** Taking the self-supervised modeling as a joint-embedding predictive architecture, I-JEPA  utilized \(f_{}()\) as a context encoder, a pair of neural network \(f_{}^{}()\) as a target encoder. A predictor \(g_{}()\) is applied to predict the target representations from \(M\) masked block patches \(_{y}(1),...,_{y}(M)\). For a target block \(_{y_{i}}\) corresponding to a target mask \(_{i}\), the predictor \(g_{}(,)\) takes as input the output of the context encoder \(_{x}\) and a mask token for each patch to predict \(\{_{j}\}_{j_{i}}\), and outputs the patch-level prediction \(\{}_{y_{j}}\}_{j_{i}}\), that is, \(\{}_{y_{j}}\}_{j_{i}}=g_{}(\{_{j }\}_{j_{i}})\).

Figure 2: Illustration of I-JEPA (a) and SimSiam (b).

The masking objective is optimized by the average \(_{2}\) distance between the predicted patch-level representations \(}_{y_{j}}\) and the target patch-level representation \(_{y_{j}}\), which is formulated as:

\[_{}=_{i=1}^{M}_{j_{i }}||}_{y_{j}}-_{y_{j}}||_{2}^{2}, \]

where \(|M|\) denotes the total number of target blocks \(M\), and \(_{i}\) is the generated mask corresponding to the \(i\)-th target block during training.

**Revisit SimSiam .** For contrastive self-supervised learning, SimSiam  tried to obtain two augmented views \(x_{i}\) and \(x_{i}^{}\). Then they fed two views into a pair of neural networks \(f_{}()\) and \(f_{}^{}()\). To learn the invariance of two different views from the same image, a prediction MLP head, denoted as \(g\), transforms the output \(\) of one view and matches it to representations \(\) of the other view. The overall objective of SimSiam is to minimize the distance between \(\) and \(\) for all random patches \(r_{j}\) in the same image, which is formulated as:

\[_{}=_{i=1}^{V}_{j_{i}}||_{r_{j}}-_{r_{j}}||_{2}^{2}, \]

where \(|V|\) denotes the total number of augmented views \(V\), and \(_{i}\) is the all random patches corresponding to the \(i\)-th view during training.

**Revisit VICReg .** To prevent a collapse in which the encoders produce constant or non-informative vectors in Siamese net architecture, VICReg  introduced variance and covariance regularization terms based on the representations space in the invariance terms. Firstly, the variance regularization term \(v\) as a hinge function on the standard deviation of the embeddings \(\) along the batch dimension \(n\) to prevent collapse with all the embeddings mapped on the same vector, which is denoted as:

\[v()=_{j=1}^{d}(0,-( _{j})+}), \]

where \(=1\) denotes a constant value for the standard deviation in their experiments, and \(\) denotes a small numerical scalar to stabilize training. Secondly, the covariance regularization term \(c\) minimizes the sum of the squared off-diagonal coefficients to encourage the off-diagonal coefficients of correlation matrix \(C\) along the batch dimension to be close to 0, which is formulated as:

\[c()=_{i j}[_{i=1}^{n}(_{i}-})(_{i}-})^{T})]_{i,j}^{2}, \]

where \(}=_{i=1}^{n}_{i}\). This term is helpful to prevent different dimensions of the embeddings from encoding trivial information. Finally, the invariance term is defined to minimize the mean-squared Euclidean distance between each pair of vectors \(\) and \(_{r_{j}}\) from different views.

### Connecting I-JEPA with SimSiam

In the realm of self-supervised learning, both I-JEPA  and SimSiam  frameworks aim to extract robust representations from images, yet they approach the problem with distinct architectures and objectives. Here, we draw theoretical and empirical parallels between these methodologies, leveraging their unique approaches to enhance the understanding of joint-embedding architectures.

Regarding theoretical connections, I-JEPA  uses a predictive model where the encoder and predictor work together to forecast masked parts of the input, relying on partial views. Conversely, SimSiam  operates without explicit masking, utilizing dual augmentations of the same image to enforce consistency between the independently processed views. Both models share the underlying principle of minimizing the distance between certain representations. As shown in Figure 2, I-JEPA focuses on the distance between predicted and actual masked patch representations, whereas SimSiam minimizes the distance between the two augmented views of an image, thereby encouraging consistency across different transformations of the same data.

Meanwhile, empirical studies such as Tian et al.  have shown that linear predictors in frameworks like BYOL , which are closely related to SimSiam, tend to learn alignment with the correlation matrix of representations. This suggests that SimSiam's approach without a separate predictor could inherently align with the mean of augmented views, a concept central to I-JEPA's strategy of predicting masked patches. A study  in joint-embedding self-supervised learning also indicates that these methods primarily focus on capturing the co-occurrence and distribution of image patches, which aligns with I-JEPA's objective of learning from masked representations.

### Connecting I-JEPA with VICReg

VICReg  introduces variance and covariance regularization to prevent the collapse of representations in Siamese networks, ensuring that the model learns informative and diverse features across different dimensions. By integrating VICReg into I-JEPA, our objective is to tackle the common challenges of model collapse and enhance the mean representation learning of image patches.

VICReg's variance regularization ensures that all dimensions of the embedding space contain meaningful variance, which is crucial for preventing the model from collapsing to trivial solutions. I-JEPA, which aims to learn diverse patch representations, can benefit from such a mechanism to ensure that each masked patch contributes informatively to the overall representation. By minimizing the off-diagonal elements of the covariance matrix, VICReg encourages the features to be uncorrelated, enhancing the diversity of the learned features. This aspect can be particularly beneficial for I-JEPA, where diverse patch predictions are essential for effective representation learning. Literature on non-contrastive self-supervised learning  suggests that implicit variance regularization, as seen in VICReg, can facilitate the learning dynamics in joint-embedding architectures. Such regularization helps maintain a balance between similarity and diversity in learned representations, which is crucial for effective self-supervised learning.

In the following, we consider linear predictor \(W_{P}^{M M}\) in I-JPEA, the masking objective with the average \(_{2}\) distance between the predicted patch-level representations \(_{y_{j}}\) and the target patch-level representations \(^{a}_{y_{j}}\) as

\[=_{i=1}^{M}_{j_{i}}||W_{P} _{y_{j}}-(^{a}_{y_{j}})||_{2}^{2}, \]

where \(|M|\) denotes the total number of target blocks \(M\), and \(_{i}\) is the generated mask corresponding to the \(i\)-th target block during training. By connecting I-JEPA with SimSiam in Section 3.2, we can can diagonalize correlation matrix of \(_{y_{j}}\) over \(\): \(C_{_{y_{j}}}=UD_{C}U^{T}\), where \(U\) is an orthogonal matrix whose columns are the eigenvectors of \(C_{_{y_{j}}}\) and \(D_{C}\) is the real-valued diagonal matrix of the eigenvalues \(s_{k},k[1,K]\). Given this eigendecomposition, the predictor is directly set to \(W_{P}=UD_{C}^{}U^{T}\), where \(\) is a positive constant exponent applied element-wise to \(D_{C}\). The eigenvalues \(_{k}\) of the predictor matrix \(W_{P}\) are then \(_{k}=s_{k}^{}\). Assume \(}_{y_{j}}\) the representations expressed in the predictor's eigenbasis, the asymmetric loss \(\) can be formulated as:

\[=_{i=1}^{M}_{j_{i}}_{k}^{K}|| _{k}}_{y_{j}}-(}^{a}_{y_{j}})|| _{2}^{2}, \]

Following non-contrastive self-supervised learning , we can use NTK [34; 35] to characterize the learning gradient dynamics of neural networks as \(_{_{y_{j}}}=(D_{y_{j}}-^{t}_{y_ {j}})D\), and the representational dynamics of each mode \(k\) independently follow gradient of the loss \(-_{}_{y_{j}}}\), and decouple as \(K\) independent differential equations:

\[}_{y_{j},k}}{dt}=-}{ }_{y_{j},k}}(t)=_{k}(}^ {a}_{y_{j},k}-_{k}}_{y_{j},k}) \]

By taking the expectation over the same masking blocks, we can have the dynamics for each mask patch \(y_{j}\) as

\[}_{y_{j},k}}{dt}=_{k}(1-_{k})}_{y_{j},k} \]

Note that when \(_{k}<1\), \(d}_{y_{j},k}\) has the same sign as \(}_{y_{j},k}\) and they have opposite sign at \(_{k}>1\). These convergent dynamics will push an eigenvalue \(_{k}\) of one to prevent the collapse of each mode \(k\) in representation dynamics. When removing the stop-grad operator in \(\), we can have \(}_{y_{j},k}}{dt}=-(1-_{k})^{2}}_{y _{j},k}\), where \(d}_{y_{j},k}\) and \(}_{y_{j},k}\) always have opposite signs and the learned representations will become zero with exponentially decaying eigenvalues to collapse. Without the predictor \(W_{P}\), we will have \(}_{y_{j},k}}{dt}=0\), and the representations will not be updated.

Incorporating VICReg's regularization strategies into I-JEPA could prevent the entire collapsing of the model, especially when learning from a large and diverse dataset. This integration could also improve the granularity and utility of the patch-level representations by ensuring that each dimension of the embedding space remains informative and independent. Overall, the integration of VICReg and SimSiam principles into the I-JEPA framework offers promising avenues for enhancing the robustness and efficacy of self-supervised learning models, particularly in tasks requiring nuanced understanding of complex visual content.

## 4 Experiments

### Experimental setup

**Datasets.** Following previous methods [12; 2], we use ImageNet-1K  for image classification, MS-COCO  for object detection and instance segmentation, and ADE20K [38; 39] for semantic segmentation. We closely follow previous work [40; 41], and adopt the Mask R-CNN  as the detector. The ViTs  backbone weights are initialized with weights pre-trained on ImageNet-1K using our C-JEPA. Following the settings in , we use the Semantic FPN and UPerNet approach  based on our ImageNet-1K pre-trained ViTs for evaluation. For a fair comparison, we fine-tune the detector with the same learning rate in . For video object segmentation, we use DAVIS-2017 dataset containing 60 training, 30 validation, and 60 testing videos. For low-level tasks, we follow the previous work  and use Clevr/Count and Clevr/Dist on Clevr  dataset.

**Evaluation Metrics.** For image classification, we follow previous masked image modeling methods [12; 2] to report the classification accuracy of linear probing and fine-tuning. For object detection and instance segmentation on MS-COCO, we apply AP\({}^{}\) and AP\({}^{}\) as metrics for the bounding boxes and the instance masks. mIoU results are reported to evaluate semantic segmentation on ADE20K. For video object segmentation on DAVIS-2017, we use Jabri-based \((\&)_{m}\), \(_{m}\), \(_{m}\) as metrics to evaluate the quality of frozen representations of image patches by segmenting scenes with the nearest neighbor between consecutive frames. For object counting and depth prediction tasks on Clevr, we use object counting and depth prediction to evaluate the linear probing performance of our model.

**Implementation.** For input images, we resized the resolution to \(224 224\), _i.e._, \(H=W=224\). Following prior work [12; 2], we apply a patch size of \(16\), _i.e._, \(P=16\). We use the tiny, small, base, and large models of ViT  architecture for experiments. We set the embedding dimension of the predictor to 384, and keep the number of self-attention heads the same as the backbone context-encoder. For the ViT-T/16, ViT-S/16, and ViT-B/16 context-encoder, we set the depth of the predictor as 6. For ViT-L/16 context-encoders, we set the depth of the predictor to 12. Following I-JEPA , we use AdamW to optimize the context-encoder and predictor weights. We train our model using the default batch size of 2048, and the learning rate linearly increased from 1e-4 to 1e-3 during the first 15 epochs of pre-training, and decay to 1e-6 following a cosine schedule. The weight decay is linearly increased from 0.04 to 0.4, and the target-encoder weights are initialized the same as the context-encoder weights, and updated via an exponential moving average. We use a momentum value of 0.996, and linearly increase this value to 1.0. For masking, we use the same strategy and settings as I-JEPA  for 4 possibly overlapping target block masks.

### Experimental comparisons

In this work, we propose a novel and effective framework for connecting Joint-Embedding Predictive Architecture with VICReg in non-contrastive self-supervised learning. In order to demonstrate the effectiveness of the proposed C-JEPA, we comprehensively compare it to previous baselines [41; 10; 12; 46; 47; 2] on non-contrastive self-supervised learning and mask image modeling.

**ImageNet-1K image classification.** For image classification on the ImageNet-1K benchmark, we report the quantitative comparison results on linear evaluation and fine-tuning results in Table 1. We can observe that our C-JEPA achieves the best results in terms of ViT-B/16 compared to previous masked image modeling approaches. Specifically, the proposed method outperforms the I-JEPA by 0.8 and 1.0 in terms of top-1 accuracy on linear evaluation and fine-tuning settings. Meanwhile, we enjoy the advantage of fewer pre-training epochs compared to other masked image model frameworks.

**COCO object detection and instance segmentation.** For COCO object detection & instance segmentation benchmarks, we also report the quantitative comparison results in Table 1. As can be seen, we achieve significant performance gains of 0.8@AP\({}^{}\) and 0.8@AP\({}^{}\) on COCO object detection and instance segmentation compared to I-JEPA. We also achieve better results than DINO  and MAE  regarding both settings.

**ADE20K semantic segmentation.** For the ADE20K semantic segmentation, we report the quantitative comparison results in Table 1. As can be seen, the proposed C-JEPA outperforms I-JEPA , the current image-based joint-embedding predictive architecture by 1.1@mIoU. Also, we observed that our C-JEPA can achieve better performance than other masked image modeling baselines, including MAE  and data2vec . These improvements suggest the importance of leveraging the VICReg's regularization to capture better semantics for dense prediction tasks.

**DAVIS video object segmentation.** In addition, we scale up our experiments from ViT-B to ViT-L and report the results in Table 2. The proposed C-JEPA still achieves better performance than I-JEPA  in terms of linear evaluation and fine-tuning settings. For DAVIS video object segmentation, our method also shows significant and consistent gains as shown in Table 2. Compared to I-JEPA, ours achieved the results gains of 1.7@\((\&)_{m}\) on ViT-L/16.

**Clevr object counting and depth prediction.** We further measure the abilities of object-counting and depth prediction on Clevr benchmarks for our pre-trained models. Table 2 shows linear evaluation results of C-JEPA on the Clevr counting and depth benchmarks. Compared to I-JEPA , we achieve the results gains of 1.2@Clevr/Count and 0.4@Clevr/Dist using ViT-L/16. These results further indicate the benefit of the proposed method in learning better representations than our vanilla I-JEPA baseline without VICReg regularization.

### Experimental analysis

In this section, we performed ablation studies to demonstrate the benefit of the proposed Variance/Covariance and Invariance terms from VICReg. Here, we conducted extensive experiments on ImageNet-1k pre-trained ViT-B/16 to explore the impact of representation mean and collapse, and learned meaningful qualitative patch-level representations.

   Method & Pretrain Epochs & linprob & fine-tune & \((\&)_{m}\) & Clevr/Count & Clevr/Dist \\  I-JEPA  & 600 & 77.5 & 85.3 & 56.6 & 85.6 & 71.2 \\ C-JEPA (ours) & 600 & **78.1** & **86.2** & **58.3** & **86.8** & **71.6** \\   

Table 2: **Scaling up to Large Models.** We perform linear evaluation, fine-tuning, video object segmentation, and low-level tasks on pre-trained ViT-L/16 models. We report linprob, fine-tune, \((\&)_{m}\), Clevr/Count and Clevr/Dist metrics to evaluate the quality of pre-trained representations. The best results are indicated in **bold**.

   Method & Pretrain Epochs & linprob & fine-tune & AP\({}^{}\) & AP\({}^{}\) & mIoU \\  DINO  & 1600 & 78.2 & 82.8 & 50.1 & 43.4 & 46.8 \\ BEiT  & 800 & 56.7 & 83.4 & 49.8 & 44.4 & 47.1 \\ MAE  & 1600 & 68.0 & 83.6 & 50.3 & 44.9 & 48.1 \\ iBOT  & 1600 & 79.5 & 84.0 & 51.2 & 44.2 & 50.0 \\ data2vec  & 800 & 60.8 & 84.2 & – & – & 48.2 \\  I-JEPA  & 600 & 72.9 & 83.5 & 49.9 & 44.5 & 47.6 \\ C-JEPA (ours) & 600 & **73.7** & **84.5** & **50.7** & **45.3** & **48.7** \\   

Table 1: **Comparison with prior work.** We perform linear evaluation, fine-tuning, COCO detection/segmentation, and ADE20K semantic segmentation on pre-trained ViT-B/16 models. We report linprob, fine-tune, AP\({}^{}\), AP\({}^{}\), and mIoU to evaluate the quality of pre-trained representations. The best results are indicated in **bold**.

**Variance/Covariance and Invariance in VICReg.** We first conducted an ablation study to determine the efficacy of incorporating Variance and Covariance terms, along with the Invariance term, in preventing model collapse and improving the fidelity of representation means. These experiments are crucial for understanding how each component contributes to the overall stability and effectiveness of the learned embeddings. The results, as presented in Tables 3 and 4, provide clear insights into the performance enhancements driven by these modifications. The inclusion of the Variance and Covariance terms significantly accelerated the training process, as evidenced by the metrics in Table 3. Table 4 highlights improved accuracy and stability in the embeddings, demonstrating the beneficial impact of the Invariance term in aligning the representation means across different mask blocks of the same images.

**Representation Mean and Collapse.** Further analysis focused on how well the C-JEPA framework mitigates the risk of model collapse and accurately learns the mean of patch representations: The integration of VICReg's terms with C-JEPA's architecture addresses the previously noted deficiencies in I-JEPA's EMA component, which was prone to collapsing. The results (referenced in the last rows of Tables 3 and 4) underscore the robustness added by these regularization terms from VICReg.

**Qualitative Attention Visualizations.** To complement our quantitative findings, we also examined qualitative aspects through attention visualization techniques. Figures 3 and 3 showcase the attention maps generated by both the base I-JEPA and the enhanced C-JEPA models. These visualizations illustrate the more focused and contextually relevant attention in C-JEPA, which correlates with the theoretical improvements expected from incorporating VICReg's terms. The attention maps further validate our approach by visually demonstrating the enhanced capability of C-JEPA to maintain stable and meaningful patch-level representations across various image contexts, an improvement over the base I-JEPA model. This qualitative evidence supports the quantitative improvements and highlights the benefits of our approach in capturing invariance.

## 5 Conclusion

In this work, we introduced C-JEPA, a novel enhancement to the Joint-Embedding Predictive Architecture that integrates the robust features of Variance-Invariance-Covariance Regularization (VICReg) to address critical limitations in the existing I-JEPA model. By refining the EMA and prediction components of I-JEPA, C-JEPA successfully prevents entire model collapse and more effectively learns the mean of patch representations, thereby advancing the state of unsupervised visual representation learning. Our theoretical analysis and empirical results have demonstrated the effectiveness of C-JEPA, particularly when pre-trained on the ImageNet-1K dataset. The framework shows marked improvements in convergence rates and performance in both linear probing and fine-tuning scenarios, outperforming existing methods. This underscores C-JEPA's capability to not only address the weaknesses of its predecessor but also to provide substantial improvements in learning quality and stability.

   I-JEPA & Variance/Covariance & Invariance & Backbone & Pretrain Epoch & linprob & fine-tune & (J \& F)\_m \\  ✓ & ✗ & ✗ & ViT-B/16 & 600 & 72.9 & 83.9 & 56.2 \\ ✓(mean) & ✓(collapse) & ✗ & ViT-B/16 & 600 & 73.5 & 84.3 & 56.9 \\ ✓ & ✓ & ✓ & ViT-B/16 & 600 & **73.7** & **84.5** & **57.5** \\ ✓(EMA for collapse) & ✗ & ✓(mean) & ViT-B/16 & 600 & 73.2 & 84.2 & 56.6 \\   

Table 4: **Ablation studies on component analysis for better convergence.** We perform ablation studies on Variance/Covariance and Invariance modules in VICReg using ViT-B/16 model. The best results are indicated in **bold**.

   I-JEPA & Variance/Covariance & Invariance & Backbone & Pretrain Epoch & linprob & fine-tune & (J \& F)\_m \\  ✓ & ✓ & ✗ & ✗ & ViT-B/16 & 100 & 63.7 & 82.5 & 52.3 \\ ✓(mean) & ✓(collapse) & ✗ & ViT-B/16 & 100 & 68.3 & 83.2 & 54.6 \\ ✓ & ✓ & ✓ & ViT-B/16 & 100 & **69.5** & **83.6** & **55.2** \\ ✓(EMA for collapse) & ✗ & ✓(mean) & ViT-B/16 & 100 & 67.6 & 82.8 & 53.9 \\   

Table 3: **Ablation studies on component analysis for faster convergence.** We perform ablation studies on Variance/Covariance and Invariance modules in VICReg using ViT-B/16 model. The best results are indicated in **bold**.

## Broader Impact

Our C-JEPA's adoption of VICReg principles enhances its versatility and applicability across various unsupervised learning contexts, making it a valuable tool for tasks requiring robust and reliable visual representations. The implications of this research are significant, offering a pathway for future studies to explore and expand upon the integration of contrastive and joint-embedding techniques. As we move forward, further refinements and explorations into the scalability of C-JEPA in more diverse and challenging datasets will be crucial. Additionally, investigating the adaptability of C-JEPA to different domains and its effectiveness in real-world applications will be essential to fully realize its potential. C-JEPA represents a significant step forward in the field of machine learning, particularly in the unsupervised learning of visual representations. It sets a new benchmark for future research and development in this area, promising enhanced performance and broader applicability in tackling complex visual understanding tasks.

Figure 3: **Qualitative visualization of learned attention maps using ViT-B/16 model. Columns for each sample denote the original image, attention maps from the target encoder in I-JEPA , attention maps from the target encoder in our C-JEPA, and attention maps from the context encoder in our C-JEPA. Our C-JEPA achieves much better attention maps.**