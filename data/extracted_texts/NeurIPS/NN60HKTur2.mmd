# Extracting Reward Functions from Diffusion Models

Felipe Nuti Tim Franzmeyer Joao F. Henriques

{nuti, frtim, joao}@robots.ox.ac.uk

Equal Contribution.

###### Abstract

Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a _relative reward function of two diffusion models_ and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function - parametrized by a neural network - to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering the base model with the learned reward functions results in significantly increased performance in standard locomotion benchmarks. Finally, we demonstrate that our approach generalizes beyond sequential decision-making by learning a reward-like function from two large-scale image generation diffusion models. The extracted reward function successfully assigns lower rewards to harmful images.1

## 1 Introduction

Recent work  demonstrates that diffusion models - which display remarkable performance in image generation - are similarly applicable to sequential decision-making. Leveraging a well-established framing of reinforcement learning as conditional sampling , Janner et al.  show that diffusion models can be used to parameterize a reward-agnostic prior distribution over trajectories, learned from offline demonstrations alone. Using classifier guidance , the diffusion model can then be steered with a (cumulative) reward function, parametrized by a neural network, to generate near-optimal behaviors in various sequential decision-making tasks. Hence, it is possible to learn successful policies both through (a) training an _expert diffusion model_ on a distribution of optimal trajectories, and (b) by training a _base diffusion model_ on lower-quality or reward-agnostic trajectories and then steering it with the given reward function. This suggests that the reward function can be extracted by comparing the distributions produced by such base and expert diffusion models.

The problem of learning the preferences of an agent from observed behavior, often expressed as a reward function, is considered in the Inverse Reinforcement Learning (IRL) literature . In contrast to merely imitating the observed behavior, learning the reward function behind it allows for better robustness, better generalization to distinct environments, and for combining rewards extracted from multiple behaviors.

Prior work in reward learning is largely based on the Maximum Entropy Inverse Reinforcement Learning (MaxEntIRL) framework , which relies on alternating between policy optimization and reward learning. This often comes with the assumption of access to the environment (or a simulator) to train the policy. Reward learning is also of independent interest outside of policy learning, for understanding agents' behavior or predicting their actions, with applications in Value Alignment, interpretability, and AI Safety . For example, reward functions can also be utilized to better understand an existing AI system's explicit or implicit "preferences" and tendencies .

In this work, we introduce a method for extracting a relative reward function from two decision-making diffusion models, illustrated in Figure 1. Our method does not require environment access, simulators, or iterative policy optimization. Further, it is agnostic to the architecture of the diffusion models used, applying to continuous and discrete models, and making no assumption of whether the models are unguided, or whether they use either classifier guidance  or classifier-free guidance .

We first derive a notion of a _relative reward function of two diffusion models_. We show that, under mild assumptions on the trajectory distribution and diffusion sampling process, our notion of reward exists and is unique, up to an additive constant. Further, we show that the derived reward is equivalent to the true reward under the probabilistic RL framework . Finally, we propose a practical learning algorithm for extracting the relative reward function of two diffusion models by aligning the gradients of the learned reward function with the _differences of the outputs_ of the base and expert models. As the extracted reward function itself is a feed-forward neural network, i.e. not a diffusion model, it is computationally lightweight.

Our proposed method for extracting a relative reward function of two diffusion models could be applicable to several scenarios. For example, it allows for better interpretation of behavior differences, for composition and manipulation of reward functions, and for training agents from scratch or fine-tune existing policies. Further, relative reward functions could allow to better understand diffusion models by contrasting them. For example, the biases of large models trained on different datasets are not always obvious, and our method may aid interpretability and auditing of models by revealing the differences between the outputs they are producing.

Figure 1: We see 2D environments with black walls, in which an agent has to move through the maze to reach the goal in the top left corner (green box). The red shaded box shows the progression from an initial noised distribution over states (at diffusion timestep \(t=20\), left) to a denoised high-reward expert trajectory on the right. This distribution is modeled by an _expert diffusion model_. The blue shaded box depicts the same process but for a low-reward trajectory where the agent moves in the wrong direction. This distribution is modeled by a _base diffusion model_. Our method (green shaded box) trains a neural network to have its gradient aligned to the difference in outputs of these two diffusion models throughout the denoising process. As we argue in Section 4, this allows us to extract a relative reward function of the two models. We observe that the heatmap of the learned relative reward (bottom right) assigns high rewards to trajectories that reach the goal point (red star).

We empirically evaluate our reward learning method along three axes. In the Maze2D environments , we learn a reward function by comparing a base diffusion model trained on exploratory trajectories and an expert diffusion model trained on goal-directed trajectories, as illustrated in Figure 1. We can see in Figure 3 that our method learns the correct reward function for varying maze configurations. In the common locomotion environments Hopper, HalfCheetah, and Walker2D [12; 7], we learn a reward function by comparing a low-performance base model to an expert diffusion model and demonstrate that steering the base model with the learned reward function results in a significantly improved performance. Beyond sequential-decision making, we learn a reward-like function by comparing a base image generation diffusion model (Stable Diffusion, ) to a _safer_ version of Stable Diffusion . Figure 2 shows that the learned reward function penalizes images with harmful content, such as violence and hate, while rewarding harmless images.

In summary, our work makes the following contributions:

* We introduce the concept of _relative reward functions_ of diffusion models, and provide a mathematical analysis of their relation to rewards in sequential decision-making.
* We propose a practical learning algorithm for extracting relative reward functions by aligning the reward function's gradient with the difference in outputs of two diffusion models.
* We empirically validate our method in long-horizon planning environments, in high-dimensional control environments, and show generalization beyond sequential decision-making.

## 2 Related Work

_Diffusion models_, originally proposed by Sohl-Dickstein et al. , are an expressive class of generative models that generate samples by learning to invert the process of noising the data. The work of Ho et al.  led to a resurgence of interest in the method, followed by [44; 10; 62]. Song et al.  proposed a unified treatment of diffusion models and score-based generative models [63; 64] through stochastic differential equations , used e.g. in . Diffusion models have shown excellent performance in image generation [54; 10; 59; 52], molecule generation [23; 26; 27], 3D generation [48; 41], video generation , language , and, crucially for this work, sequential decision-making . Part of their appeal as generative models is due to their _steerability_. Since sample generation is gradual, other pre-trained neural networks can be used to steer the diffusion model during sampling (i.e. classifier guidance, Section 3). An alternative approach is classifier-free guidance , often used in text-to-image models such as .

Decision-making with diffusion models have first been proposed by Janner et al. , who presented a hybrid solution for planning and dynamics modeling by iteratively denoising a sequence of states and actions at every time step, and executing the first denoised action. This method builds on the probabilistic inference framework for RL, reviewed by Levine , based on works including [66; 70; 71; 31; 30]. Diffusion models have been applied in the standard reinforcement learning setting [39; 2; 8], and also in 3D domains . They have similarly been used in imitation learning for generating more human-like policies [47; 53], for traffic simulation , and for offline reinforcement learning [25; 68; 17]. Within sequential-decision making, much interest also lies in extracting reward functions from observed behavior. This problem of inverse RL [1; 5] has mostly either been approached by making strong assumptions about the structure of the reward function [71; 43; 15], or, more recently, by employing adversarial training objectives .

The optimization objective of our method resembles that of physics-informed neural networks [6; 16; 40; 50; 51; 11], which also align neural network gradients to a pre-specified function. In our case, this function is a difference in the outputs of two diffusion models.

To the best of our knowledge, no previous reward learning methods are directly applicable to diffusion models, nor has extracting a relative reward function from two diffusion models been explored before.

## 3 Background

Our method leverages mathematical properties of diffusion-based planners to extract reward functions from them. To put our contributions into context and define notation, we give a brief overview of the probabilistic formulation of Reinforcement Learning presented in , then of diffusion models, and finally of how they come together in decision-making diffusion models .

Reinforcement Learning as Probabilistic Inference.Levine  provides an in-depth exposition of existing methods for approaching sequential decision-making from a probabilistic and causal angle using Probabilistic Graphical Models (PGM) . We review some essential notions to understand this perspective on RL. Denote by \(_{}\) the set of probability distributions over a set \(\).

Markov Decision Process (MDP).An MDP is a tuple \(,,,r,_{0},T\) consisting of a state space \(\), an action space \(\), a transition function \(:_{}\), a reward function \(r:_{_{ 0}}\), an initial state distribution \(_{0}_{}\), and an episode length \(T\). The MDP starts at an initial state \(s_{0}_{0}\), and evolves by sampling \(a_{t}(s_{t})\), and then \(s_{t+1}(s_{t},a_{t})\) for \(t 0\). The reward received at time \(t\) is \(r_{t} r(s_{t},a_{t})\). The episode ends at \(t=T\). The sequence of state-action pairs \(=((s_{t},a_{t}))_{t=0}^{T}\) is called a _trajectory_.

The framework in  recasts such an MDP as a PGM, illustrated in Appendix B, Figure (b)b. It consists of a sequence of states \((s_{t})_{t=0}^{T}\), actions \((a_{t})_{t=0}^{T}\) and _optimality variables_\((_{t})_{t=0}^{T}\). The reward function \(r\) is not explicitly present. Instead, it is encoded in the optimality variables via the relation: \(_{t}(e^{-r(s_{t},a_{t})})\). One can apply Bayes's Rule to obtain:

\[p(|_{1:T}) p() p(_{1:T}|)\] (1)

which factorizes the distribution of optimal trajectories (up to a normalizing constant) as a _prior_\(p()\) over trajectories and a _likelihood term_\(p(_{1:T}|)\). From the definition of \(_{1:T}\) and the PGM structure, we have \(p(_{1:T}|)=e^{-_{t}r(s_{t},a_{t})}\). Hence, the negative log-likelihood of optimality conditioned on a trajectory \(\) corresponds to its cumulative reward: -log \(p(_{1:T}|)=_{t}r(s_{t},a_{t})\).

Diffusion Models in Continuous Time.At a high level, diffusion models work by adding noise to data \(^{n}\) (_forward process_), and then learning to denoise it (_backward process_).

The forward noising process in continuous time follows the Stochastic Differential Equation (SDE):

\[d_{t}=(_{t},t)dt+g(t)d_{t}\] (2)

where \(f\) is a function that is Lipschitz, \(\) is a standard Brownian Motion  and \(g\) is a (continuous) noise schedule, which regulates the amount of noise added to the data during the forward process (c.f. A.2). Song et al.  then use a result of Anderson  to write the SDE satisfied by the _reverse process_ of (2), denoted \(}_{t}\), as:

\[d}_{t}=[(}_{t},t)-g(t)^{2}_{ }p_{t}(}_{t})]\;dt+g(t)\;d}_{t}\] (3)

Here \(p_{t}()\) denotes the marginal density function of the forward process \(_{t}\), and \(}\) is a reverse Brownian motion (see ). The diffusion model is a neural network \(_{}(_{t},t)\) with parameters \(\) that is trained to approximate \(_{}p_{t}(_{t})\), called the _score_ of the distribution of \(_{t}\). The network \(_{}(_{t},t)\) can then be used to generate new samples from \(p_{0}\) by taking \(}_{T}(0,I)\) for some \(T>0\), and simulating (3) _backwards in time_ to arrive at \(}_{0}_{0}\), with the the neural network \(_{}\) in place of the score term:

\[d}_{t}=[(}_{t},t)-g(t)^{2}_{ }(}_{t},t)]\;dt+g(t)\;d}_{t}\] (4)

This formulation is essential for deriving existence results for the relative reward function of two diffusion models in Section 4, as it allows for conditional sampling. For example, to sample from \(p(_{0}|y) p(_{0}) p(y|_{0})\), where \(y\{0,1\}\), the sampling procedure can be modified to use \(_{}p(_{t}|y)_{}( _{t},t)+_{}(,t)\) instead of \(_{}(}_{t},t)\). Here, \((,t)\) is a neural network approximating log \(p(y|_{t})\). The gradients of \(\) are multiplied by a small constant \(\), called the _guidance scale_. The resulting _guided_ reverse SDE is as follows:

\[d}_{t}=[(}_{t},t)-g(t)^{2}[_{ }(}_{t},t)+_{}(} _{t},t)]]\;dt+g(t)\;d}_{t}\] (5)

Informally, this method, often called _classifier guidance_, allows for steering a diffusion model to produce samples \(\) with some property \(y\) by gradually pushing the samples in the direction that maximizes the output of a classifier predicting \(p(y|)\).

Planning with Diffusion.The above shows how sequential decision-making can be framed as sampling from a posterior distribution \(p(|_{1:T})\) over trajectories. Section 3 shows how a diffusion model \(p(_{0})\) can be combined with a classifier to sample from a posterior \(p(_{0}|y)\). These two observations point us to the approach in Diffuser : using a diffusion model to model a prior \(p()\) over trajectories, and a reward prediction function \((,t) p(_{1:T}|_{t})\) to steer the diffusion model. This allows approximate sampling from \(p(|_{1:T})\), which produces (near-)optimal trajectories.

The policy generated by the Diffuser denoises a fixed-length sequence of future states and actions and executes the first action of the sequence. Diffuser can also be employed for goal-directed planning, by fixing initial and goal states during the denoising process.

## 4 Methods

In the previous section, we established the connection between the (cumulative) return \(_{t}r(s_{t},a_{t})\) of a trajectory \(=((s_{t},a_{t}))_{t=1}^{T}\) and the value \(p(y|)\) used for classifier guidance (Section 3), with \(\) corresponding to \(\) and \(y\) corresponding to \(_{1:T}\). Hence, we can look at our goal as finding \(p(y|)\), or equivalently \(p(_{1:T}|)\), in order to recover the cumulative reward for a given trajectory \(\). Additionally, in Section 4.3, we will demonstrate how the single-step reward can be computed by choosing a specific parametrization for \(p(y|)\).

**Problem Setting.** We consider a scenario where we have two decision-making diffusion models: a base model \(_{}^{(1)}\) that generates reward-agnostic trajectories, and an expert model \(_{}^{(2)}\) that generates trajectories optimal under some _unknown_ reward function \(r\). Our objective is to learn a reward function \((,t)\) such that, if \(\) is used to steer the base model \(_{}^{(1)}\) through classifier guidance, we obtain a distribution close to that of the expert model \(_{}^{(2)}\). From the above discussion, such a function \(\) would correspond to the notion of relative reward in the probabilistic RL setting.

In the following, we present theory showing that:

**1.**: In an idealized setting where \(^{(1)}\) and \(^{(2)}\) have no approximation error (and are thus conservative vector fields), there exists a unique function \(\) that exactly converts \(^{(1)}\) to \(^{(2)}\) through classifier guidance.
**2.**: In practice, we cannot expect such a classifier to exist, as approximation errors might result in diffusion models corresponding to non-conservative vector fields.
**3.**: However, the functions \(\) that _best approximate_ the desired property (to arbitrary precision \(\)) do exist. These are given by Def. 4.4 and can be obtained through a _projection_ using an \(L^{2}\) distance.
**4.**: The use of an \(L^{2}\) distance naturally results in an \(L^{2}\) loss for learning \(\) with Gradient Descent.

### A Result on Existence and Uniqueness

We now provide a result saying that, once \(_{}^{(1)}\) and \(_{}^{(2)}\) are fixed, there is a condition on the gradients of \(\) that, if met, would allow us to match not only the _distributions_ of \(_{}^{(1)}\) and \(_{}^{(2)}\), but also their entire denoising processes, with probability 1 (i.e. _almost surely_, a.s.).

In the following theorem, \(\) plays the role of the gradients \(_{}(_{t},t)\). Going from time \(t=0\) to \(t=T\) in the theorem corresponds to solving the backward SDE (3) from \(t=T\) to \(t=0\), and \(\) in the theorem corresponds to the drift term (i.e. coefficient of \(dt\)) of (3). For proof, see Appendix C.

**Theorem 4.1** (Existence and Uniqueness).: _Let \(T>0\). Let \(^{(1)}\) and \(^{(2)}\) be functions from \(^{n}[0,T]\) to \(^{n}\) that are Lipschitz, and \(g:[0,T]_{>0}\) be bounded and continuous with \(g(0)>0\). Fix a probability space \((,,)\), and a standard \(^{n}\)-Brownian Motion \((_{t})_{t 0}\)._

_Consider the Ito SDEs:_

\[d_{t}^{(1)} =^{(1)}(_{t}^{(1)},t)\;dt+g(t)\;d_{t}\] (6) \[d_{t}^{(2)} =^{(2)}(_{t}^{(2)},t)\;dt+g(t)\;d_{t}\] (7) \[d_{t} =[^{(1)}(_{t},t)+(_{t},t )]\;dt+g(t)\;d_{t},$ is Lipschitz}\] (8)

_and fix an initial condition \(_{0}^{(1)}=_{0}^{(2)}=_{0}=\), where \(\) is a random variable with \([||||_{2}^{2}]<\)._

_Then (6), (7), and (8) have almost surely (a.s.) unique solutions \(^{(1)}\), \(^{(2)}\) and \(\) with a.s. continuous sample paths. Furthermore, there exists an a.s. unique choice of \(\) such that \(_{t}=_{t}^{(2)}\) for all \(t 0\), a.s., which is given by_

\[(,t)=^{(2)}(,t)-^{(1)}( ,t).\] (9)In all diffusion model methods we are aware of, the drift and noise terms of the backward process indeed satisfy the pre-conditions of the theorem, under reasonable assumptions on the data distribution (see C.3), and using a network with smooth activation functions like Mish  or GeLU .

Therefore, Theorem 4.1 tells us that, if we were free to pick the gradients \(_{}(_{t},t)\), setting them to \(^{(2)}(_{t},t)-^{(1)}(_{t},t)\) would be the "best" choice: it is the only choice resulting in guided samples _exactly reproducing_ the whole process \(^{(2)}\) (and, in particular, the distribution of \(_{0}^{(2)}\)). We will now see that, in an idealized setting, there exists a unique function \(\) satisfying this criterion. We start by recalling the concept of a _conservative vector field_, from multivariate calculus, and how it relates to gradients of continuously differentiable functions.

**Definition 4.2** (Conservative Vector Field, Definition 7.6 in ).: We say that a vector field \(\) is conservative if it is the gradient of a continuously differentiable function \(\),

\[()=_{}()\]

for all \(\) in the domain of \(\). The function \(\) is called a potential function of \(\).

Suppose we had access to the ground-truth scores \(_{}^{(1)}(,t)\) and \(_{}^{(2)}(,t)\) of the forward processes for the base and expert models (i.e. no approximation error). Then they are equal to \(_{}\ p_{t}^{(1)}()\) and \(_{}\ p_{t}^{(2)}()\), respectively. If we also assume \(p_{t}^{(1)}()\) and \(p_{t}^{(2)}()\) are continuously differentiable, we have that, by Definition 4.2, the diffusion models are conservative for each \(t\). Thus, their difference is also conservative, i.e. the gradient of a continuously differentiable function.

Hence, by the Fundamental Theorem for Line Integrals (Th. 7.2 in ), there exists a unique \(\) satisfying \(_{}(,t)=_{}^{(2)}(,t)-_{}^{(1)}(,t)\), up to an additive constant, given by the line integral

\[(,t)=_{_{}}^{}[_{ }^{(2)}(^{},t)-_{}^{(1)}( ^{},t)]\  d^{},\] (10)

where \(_{}\) is some arbitrary reference point, and the line integral is path-independent.

In practice, however, we cannot guarantee the absence of approximation errors, nor that the diffusion models are conservative.

### Relative Reward Function of Two Diffusion Models

To get around the possibility that \(^{(1)}\) and \(^{(2)}\) are not conservative, we may instead look for the conservative field best approximating \(^{(2)}(,t)-^{(1)}(,t)\) in \(L^{2}(^{n},^{n})\) (i.e. the space of square-integrable vector fields, endowed with the \(L^{2}\) norm). Using a well-known fundamental result on uniqueness of projections in \(L^{2}\) (Th. C.8), we obtain the following:

**Proposition 4.3** (Optimal Relative Reward Gradient).: _Let \(_{}^{(1)}\) and \(_{}^{(2)}\) be any two diffusion models and \(t(0,T]\), with the assumption that \(_{}^{(2)}(,t)-_{}^{(1)}(,t)\) is square-integrable. Then there exists a unique vector field \(_{t}\) given by_

\[_{t}=(^{n}) }}{}_{^{n}}||()-(_ {}^{(2)}(,t)-_{}^{(1)}(,t))||_{2}^{2} \,d\] (11)

_where \((^{n})}\) denotes the closed span of gradients of smooth \(W^{1,2}\) potentials. Furthermore, for any \(>0\), there is a smooth, square-integrable potential \(\) with a square-integrable gradient satisfying:_

\[_{^{n}}||_{}()-_{t}( )||_{2}^{2}\,d<\] (12)

_We call such an \(_{t}\) the **optimal relative reward gradient of \(_{}^{(1)}\) and \(_{}^{(2)}\) at time \(t\)**._

For proof of Proposition 4.3 see Appendix C.4. It is important to note that for the projection to be well-defined, we required an assumption regarding the integrability of the diffusion models. Without this assumption, the integral would simply diverge.

The result in Proposition 4.3 tells us that we can get arbitrarily close to the optimal relative reward gradient using scalar potentials' gradients. Therefore, we may finally define the central notion in this paper:

**Definition 4.4** (\(\)-Relative Reward Function).: For an \(>0\), an \(\)-relative reward function of diffusion models \(^{(1)}_{}\) and \(^{(2)}_{}\) is a function \(:^{n}[0,T]\) such that

\[ t(0,T]:\ _{^{n}}_{}( ,t)-_{t}()^{2}_{2}\ d<\] (13)

where \(_{t}\) denotes the optimal relative reward gradient of \(^{(1)}_{}\) and \(^{(2)}_{}\) at time \(t\).

### Extracting Reward Functions

We now set out to actually approximate the relative reward function \(\). Definition 4.4 naturally translates into an \(L^{2}\) training objective for learning \(\):

\[L_{}()=_{t[0,T],_{t} p _{t}}[||_{}_{}(_{t},t)-(^{ (2)}_{}(_{t},t)-^{(1)}_{}(_{t},t))|| _{2}^{2}]\] (14)

where \(p_{t}\) denotes the marginal at time \(t\) of the forward noising process.

We optimize this objective via Empirical Risk Minimization and Stochastic Gradient Descent. See Algorithm 1 for a version assuming access to the diffusion models and their training datasets, and Algorithm 2 in Appendix D for one which does not assume access to any pre-existing dataset. Our method requires no access to the environment or to a simulator. Our algorithm requires computing a second-order mixed derivative \(D_{}(_{}(,t))^{m n}\) (where \(m\) is the number of parameters \(\)), for which we use automatic differentiation in PyTorch .

**Recovering Per-Time-Step Rewards.** We can parameterize \(\) using a single-time-step neural network \(g_{}(s,a,t)\) as \((_{t},t)=_{i=1}^{N}g_{}(s^{i}_{t},a^{i}_{t},t)\), where \(N\) is the horizon and \(_{t}\) denotes a trajectory at diffusion timestep \(t\), and \(s^{i}_{t}\) and \(a^{i}_{t}\) denote the \(i^{}\) state and action in \(_{t}\). Then, \(g_{}(s,a,0)\) predicts a reward for a state-action pair \((s,a)\).

``` Input: Base \(^{(1)}\) and expert \(^{(2)}\) diffusion models, dataset \(\), number of iterations \(I\). Output: Relative reward estimator \(_{}\). Initialize reward estimator parameters \(\). for\(j\{1,...,I\}\)do  Sample batch: \(_{0}=[^{(1)}_{0},...,^{(N)}_{0}]\) from \(\)  Sample times: \(=[t_{1},...,t_{N}]\) independently in \((0,T]\)  Sample forward process: \(_{t}[^{(1)}_{t_{1}},...,^{(N)}_{t_{N}}]\)  Take an optimization step on \(\) according to \(_{}()=_{i=1}^{N}_{ }_{}(^{(i)}_{t_{i}},t_{i})-(^{(2)}_{}( ^{(i)}_{t_{i}},t_{i})-^{(1)}_{}(^{(i)}_{t_{ i}},t_{i}))||_{2}^{2}\)  end for ```

**Algorithm 1**Relative reward function training.

## 5 Experiments

In this section, we conduct empirical investigations to analyze the properties of relative reward functions in practice. Our experiments focus on three main aspects: the alignment of learned reward functions with the goals of the expert agent, the performance improvement achieved through steering the base model using the learned reward, and the generalizability of learning reward-like functions to domains beyond decision-making (note that the relative reward function is defined for any pair of diffusion models). For details on implementation specifics, computational requirements, and experiment replication instructions, we refer readers to the Appendix.

### Learning Correct Reward Functions from Long-Horizon Plans

Maze2D  features various environments which involve controlling the acceleration of a ball to navigate it towards various goal positions in 2D mazes. It is suitable for evaluating reward learning, as it requires effective credit assignment over extended trajectories.

Figure 2: Learned rewards for base and expert diffusion models from Stable Diffusion (Sec. 5). Prompts from the I2P dataset.

Implementation.To conduct our experiments, we generate multiple datasets of trajectories in each of the 2D environments, following the data generation procedure from D4RL , except sampling start and goal positions uniformly at random (as opposed to only at integer coordinates).

For four maze environments with different wall configurations (depicted in Figure 3), we first train a base diffusion model on a dataset of uniformly sampled start and goal positions, hence representing undirected behavior. For each environment, we then train eight expert diffusion models on datasets with fixed goal positions. As there are four maze configurations and eight goal positions per maze configuration, we are left with 32 expert models in total. For each of the 32 expert models, we train a relative reward estimator \(_{}\), implemented as an MLP, via gradient alignment, as described in Alg. 1. We repeat this across 5 random seeds.

Discriminability Results.For a quantitative evaluation, we train a logistic regression classifier per expert model on a balanced dataset of base and expert trajectories. Its objective is to label trajectories as base or expert using only the predicted reward as input. We repeat this process for each goal position with 5 different seeds, and average accuracies across seeds. The achieved accuracies range from 65.33% to 97.26%, with a median of 84.49% and a mean of 83.76%. These results demonstrate that the learned reward effectively discriminates expert trajectories.

Visualization of Learned Reward Functions.To visualize the rewards, we use our model to predict rewards for fixed-length sub-trajectories in the base dataset. We then generate a 2D heatmap by averaging the rewards of all sub-trajectories that pass through each grid cell. Fig. 3 displays some of these heatmaps, with more examples given in App. F.

Results.We observe in Fig. 3 that the network accurately captures the rewards, with peaks occurring at the true goal position of the expert dataset in 78.75% \(\) 8.96% of the cases for simpler mazes (maze2d-open-v0 and maze2d-umaze-v0), and 77.50% \(\) 9.15% for more advanced mazes (maze2d-medium-v1 and maze2d-large-v1). Overall, the network achieves an average success rate of 78.12% \(\) 6.40%. We further verify the correctness of the learned reward functions by retraining agents in Maze2D using the extracted reward functions (see Appendix E.2.1).

Sensitivity to Dataset Size.Since training diffusion models typically demands a substantial amount of data, we investigate the sensitivity of our method to dataset size. Remarkably, we observe that the performance only experiences only a slight degradation even with significantly smaller sizes. For complete results, please refer to Appendix E.3.

### Steering Diffusion Models to Improve their Performance

Having established the effectiveness of relative reward functions in recovering expert goals in the low-dimensional Maze2D environment, we now examine their applicability in higher-dimensional control tasks. Specifically, we evaluate their performance in the HalfCheetah, Hopper, and Walker-2D environments from the D4RL offline locomotion suite . These tasks involve controlling a multi-degree-of-freedom 2D robot to move forward at the highest possible speed. We assess the learned reward functions by examining whether they can enhance the performance of a weak base model when used for classifier-guided steering. If successful, this would provide evidence that the learned relative rewards can indeed bring the base trajectory distribution closer to the expert distribution.

Implementation.In these locomotion environments, the notion of reward is primarily focused on moving forward. Therefore, instead of selecting expert behaviors from a diverse set of base behaviors, our reward function aims to guide a below-average-performing base model toward improved performance. Specifically, we train the base models using the medium-replay datasets from D4RL, which yield low rewards, and the expert models using the expert datasets, which yield high rewards.

Figure 3: Heatmaps of the learned relative reward in Maze2D. \(\) denotes the ground-truth goal position. For more examples, see Appendix E.3.

The reward functions are then fitted using gradient alignment as described in Algorithm 1. Finally, we employ classifier guidance (Section 3) to steer each base model using the corresponding learned reward function.

**Results.** We conduct 512 independent rollouts of the base diffusion model steered by the learned reward, using various guidance scales \(\) (Eq. 5). We use the unsteered base model as a baseline. We also compare our approach to a discriminator with the same architecture as our reward function, trained to predict whether a trajectory originates from the base or expert dataset. We train our models with 5 random seeds, and run the 512 independent rollouts for each seed. Steering the base models with our learned reward functions consistently leads to statistically significant performance improvements across all three environments. Notably, the Walker2D task demonstrates a 36.17% relative improvement compared to the unsteered model. This outcome suggests that the reward functions effectively capture the distinctions between the two diffusion models. See Table 1 (top three rows). We further conducted additional experiments in the Locomotion domains in which we steer a _new_ medium-performance diffusion model unseen during training, using the relative reward function learned from the base diffusion model and the expert diffusion model. We observed in Table 1 (bottom three rows) that our learned reward function significantly improves performance also in this scenario.

### Learning a Reward-Like Function for Stable Diffusion

While reward functions are primarily used in sequential decision-making problems, we propose a generalization to more general domains through the concept of relative reward functions, as discussed in Section 4. To empirically evaluate this generalization, we focus on one of the domains where diffusion models have demonstrated exceptional performance: image generation. Specifically, we examine Stable Diffusion , a widely used 859m-parameter diffusion model for general-purpose image generation, and Safe Stable Diffusion , a modified version of Stable Diffusion designed to mitigate the generation of inappropriate images.

**Models.** The models under consideration are _latent diffusion models_, where the denoising process occurs in a latent space and is subsequently decoded into an image. These models employ classifier-free guidance  during sampling and can be steered using natural language prompts, utilizing CLIP embeddings  in the latent space. Specifically, Safe Stable Diffusion  introduces modifications to the _sampling loop_ of the open-source model proposed by Rombach et al. , without altering the model's actual weights. In contrast to traditional classifier-free guidance that steers samples toward a given prompt, the modified sampler of Safe Stable Diffusion also directs samples _away_ from undesirable prompts [59, Sec. 3].

**Prompt Dataset.** To investigate whether our reward networks can detect a "relative preference" of Safe Stable Diffusion over the base Stable Diffusion model for harmless images, we use the I2P prompt dataset introduced by Schramowski et al. . This dataset consists of prompts specifically designed to deceive Stable Diffusion into generating imagery with unsafe content. However, we use the dataset to generate sets of _image embeddings_ rather than actual images, which serve as training data for our reward networks. A portion of the generated dataset containing an equal number of base and expert samples is set aside for model evaluation.

  
**Environment** & **Unsteered** & **Discriminator** & **Reward (Ours)** \\  Halfcheeta & \(30.38 0.38\) & \(30.41 0.38\) & **31.65 \(\) 0.32** \\ Hopper & \(24.67 0.92\) & \(25.12 0.95\) & **27.04 \(\) 0.90** \\ Walker2d & \(28.20 0.99\) & \(27.98 0.99\) & **38.40 \(\) 1.02** \\  Mean & 27.75 & 27.84 & **32.36** \\   Halfcheeta & \(59.41 0.87\) & \(70.79 1.92\) & **69.32 \(\) 0.8** \\ Hopper & \(58.8 1.01\) & \(59.42 2.23\) & **64.97 \(\) 1.15** \\ Walker2d & \(96.12 0.92\) & \(96.75 1.91\) & **102.05 \(\) 1.15** \\  Mean & 71.44 & 75.65 & **78.78** \\   

Table 1: Diffuser performance with different steering methods, on 3 RL environments and for a low-performance base model (top) and a medium-performance model (bottom).

Figure 4: The 3 images with the highest learned reward in their batch (“safe”), and 3 with the lowest reward (“unsafe”, blurred), respectively.

**Separating Image Distributions.** Despite the complex and multimodal nature of the data distribution in this context, we observe that our reward networks are capable of distinguishing between base and expert images with over 90% accuracy, despite not being explicitly trained for this task. The reward histogram is visualized in Figure 2.

**Qualitative Evaluation.** We find that images that receive high rewards correspond to safe content, while those with low rewards typically contain unsafe or disturbing material, including hateful or offensive imagery. To illustrate this, we sample batches from the validation set, compute the rewards for each image, and decode the image with the highest reward and the one with the lowest reward from each batch. Considering the sensitive nature of the generated images, we blur the latter set as an additional safety precaution. Example images can be observed in Figure 4.

## 6 Conclusion

To the best of our knowledge, our work introduces the first method for extracting relative reward functions from two diffusion models. We provide theoretical justification for our approach and demonstrate its effectiveness in diverse domains and settings. We expect that our method has the potential to facilitate the learning of reward functions from large pre-trained models, improving our understanding and the alignment of the generated outputs. It is important to note that our experiments primarily rely on simulated environments, and further research is required to demonstrate its applicability in real-world scenarios.

## 7 Acknowledgements

This work was supported by the Royal Academy of Engineering (RF\(\)201819\(\)18\(\)163). F.N. receives a scholarship from Fundacao Estudar. F.N. also used TPUs granted by the Google TPU Research Cloud (TRC) in the initial exploratory stages of the project.

We would like to thank Luke Melas-Kyriazi, Prof. Varun Kanade, Yizhang Lou, Ruining Li and Eduard Oravkin for proofreading versions of this work.

F.N. would also like to thank Prof. Stefan Kiefer for the support during the project, and Michael Janner for the productive conversations in Berkeley about planning with Diffusion Models.

We use the following technologies and repositories as components in our code: PyTorch , NumPy , Diffuser , D4RL , HuggingFace Diffusers  and LucidRains's Diffusion Models in Pytorch repository.