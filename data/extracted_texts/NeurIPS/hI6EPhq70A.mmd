# Face Reconstruction from Facial Templates by Learning Latent Space of a Generator Network

**Hatef Otroshi Shahreza\({}^{1,2}\) and Sebastien Marcel\({}^{1,3}\)**

\({}^{1}\)Idiap Research Institute, Martigny, Switzerland

\({}^{2}\)Ecole Polytechnique Federale de Lausanne (EPFL), Lausanne, Switzerland

\({}^{3}\)Universite de Lausanne (UNIL), Lausanne, Switzerland

{hatef.otroshi,sebastien.marcel}@idiap.ch

## Abstract

In this paper, we focus on the template inversion attack against face recognition systems and propose a new method to reconstruct face images from facial templates. Within a generative adversarial network (GAN)-based framework, we learn a mapping from facial templates to the intermediate latent space of a pre-trained face generation network, from which we can generate high-resolution realistic reconstructed face images. We show that our proposed method can be applied in whitebox and blackbox attacks against face recognition systems. Furthermore, we evaluate the transferability of our attack when the adversary uses the reconstructed face image to impersonate the underlying subject in an attack against another face recognition system. Considering the adversary's knowledge and the target face recognition system, we define five different attacks and evaluate the vulnerability of state-of-the-art face recognition systems. Our experiments show that our proposed method achieves high success attack rates in whitebox and blackbox scenarios. Furthermore, the reconstructed face images are transferable and can be used to enter target face recognition systems with a different feature extractor model. We also explore important areas in the reconstructed face images that can fool the target face recognition system.

Figure 1: Sample face images from the FFHQ dataset and their corresponding reconstructed images using our template inversion method from ArcFace templates (used in a face recognition system). The values below each image show the cosine similarity between the corresponding templates of original and reconstructed face images.

Introduction

Face recognition (FR) systems tend toward ubiquity, and their applications, which range from cell phone unlock to national identity system, border control, etc., are growing rapidly. Typically, in such systems, a feature vector (called embedding or template) is extracted from each face image using a deep neural network, and is stored in the system's database during the enrollment stage. During the recognition stage, either verification or identification, the extracted feature vector is compared with the ones in the system's database to measure the similarity of identities. Among potential attacks against FR systems (Galbally et al., 2014; Biggio et al., 2015; Hadid et al., 2015; Mai et al., 2018; Marcel et al., 2023), the template inversion (TI) attack significantly jeopardizes the users' privacy. In a TI attack, the adversary gains access to templates stored in the FR system's database and aims to reconstruct the underlying face image. Then, the adversary not only achieves privacy-sensitive information (such as gender, ethnicity, etc.) of enrolled users, but also can use reconstructed face images to impersonate.

In this paper, we focus on the TI attack against FR systems and propose a novel method to reconstruct face images from facial templates (Figure 1 shows sample reconstructed face images using our proposed method). Within a generative adversarial network (GAN)-based framework, we learn a mapping from face templates to the intermediate latent space of StyleGAN3 (Karras et al., 2021), as a pre-trained face generation network. Then, using the synthesis part of StyleGAN3, we can generate high-resolution realistic face image. Our proposed method can be applied for _whitebox_ and _blackbox_ attacks against FR systems. In the _whitebox_ scenario, the adversary knows the internal functioning of the feature extraction model and its parameters. However, in the _blackbox_ scenario, the adversary does not know the internal functioning of the feature extraction model and can only use it to extract features from any arbitrary image. Instead, we assume that the adversary has a _whitebox_ of another FR model, which can be used for training the face reconstruction network. We also evaluate the transferability of our attack by considering the case where the adversary uses the reconstructed face image to impersonate the underlying subject in an attack against another FR system (which has a different feature extraction model). Considering the adversary's knowledge and the target FR system, we define five different attacks, and evaluate the vulnerability of state-of-the-art (SOTA) FR systems. Figure 2 illustrates the general black diagram of our proposed template inversion attack.

To elaborate on the contributions of our paper, we list them hereunder:

* We propose a novel method to generate high-resolution realistic face images from facial templates. Within a GAN-based framework, we learn the mapping from facial templates to the latent space of a pre-trained face generation network.
* We propose our method for _whitebox_ and _blackbox_ scenarios. While our method is based on the _whitebox_ knowledge of the FR model, we extend our attack _blackbox_ scenario, using another FR model that the adversary has access to.
* We define five different attacks against FR systems (based on the adversary's knowledge and the target system), and evaluate the transferability of the reconstructed face images and vulnerability of SOTA FR models to TI attacks. We also explore important areas in the reconstructed face images. To our knowledge, this is the first work which comprehensively evaluates the transferability of the reconstructed face images in TI attacks.

The remainder of the paper is organized as follows: Section 2 introduces the problem formulation and our proposed face reconstruction method. Section 3 covers the related works in the literature and

Figure 2: Block diagram of our proposed template inversion attack

compares them with our proposed method. Section 4 presents our experiential results. Finally, the paper is concluded in Section 5.

## 2 Problem Definition and Proposed Method

In this paper, we consider a TI attack against a FR system based on the following threat model:

* _Adversary's goal_: The adversary aims to reconstruct a face image from a template, and use the reconstructed face image to enter the same or a different face recognition system, which we call the target FR system.
* _Adversary's knowledge:_ The adversary knows a face template of a user enrolled in the FR system's database. The adversary also has either _whitebox_ or _blackbox_ knowledge of the feature extractor model in the same FR system.
* _Adversary's capability:_ The adversary can present the reconstructed face image to the target FR system (e.g., using a printed photograph). However, for simplicity, we consider that adversary can inject the reconstructed face image as a query to the target FR system.
* _Adversary's strategy:_ The adversary can train a face reconstruction model to invert facial templates and reconstruct underlying face images. Then, the adversary can use the reconstructed face images to inject as a query to the target FR system, to enter that system.

Let \(F(.)\) denote a facial feature extraction model, which gets the face image \(\) and extracts facial template \(=F()\). According to the threat model, the adversary has access to the target facial template \(_{}=F_{}()\) and aims to generate a reconstructed face image \(}\). Then, the adversary can use the reconstructed face image \(}\) to impersonate the corresponding subject and attack a target FR system with \(F_{}(.)\), which might be different from \(F_{}(.)\).

To train a face reconstruction model, we can use a dataset of face images \(\{_{i}\}_{i=1}^{N}\) with \(N\) face images (no label is required), and generate a training dataset \(\{(_{i},_{i})\}_{i=1}^{N}\), where \(_{i}=F_{}(_{i})\). Then, a face reconstruction model \(G(.)\) can be trained to reconstruct face image \(}=G()\) given each facial template \(\). To train such a face reconstruction model, we consider a multi-term face reconstruction loss function as follows:

\[_{}=_{}+_{},\] (1)

where \(_{}\) and \(_{}\) indicate pixel loss and ID losses, respectively, and are defined as:

\[_{}=_{}[ \|-G()\|_{2}^{2}],\] (2) \[_{}=_{}[\|F_{ }()-F_{}(G())\|_{2}^{2}].\] (3)

The pixel loss is used to minimize the pixel-level reconstruction error of the generated face image. The ID loss is also used to minimize the distance between facial templates extracted by \(F_{}(.)\) from original and reconstructed face images. In Eq. 3, \(F_{}(.)\) denotes a feature extraction model that the adversary is assumed to have complete knowledge of its parameters and internal functioning. Based on the adversary's knowledge of \(F_{}(.)\) (i.e., _whitebox_ or _blackbox_ scenarios), \(F_{}(.)\) might be the same or different from \(F_{}(.)\).

For the face reconstruction model, we consider StyleGAN3 (Karras et al., 2021), as a pre-trained face generation network1. The StyleGAN3 model is trained on a dataset of face images using a GAN-based framework that can generate high-resolution and realistic face images. The structure of StyleGAN3 is composed of two networks, mapping and synthesis networks. The mapping network \(M_{}(.)\) gets a random noise \(\) and generates an intermediate latent code \(=M_{}()\). Then, the latent code \(\) is given to the synthesis network \(S_{}(.)\) to generate a face image. In our training process, we fix the synthetic network \(S_{}(.)\) and train a new mapping \(M_{}(.)\) to generate \(}\) corresponding to the given facial template \(\). Then, the generated latent code \(}\) is given to the synthesis network \(S_{}(.)\) to generate the reconstructed face image \(}=S_{}(})\). We can train our new mapping \(M_{}(.)\) using our reconstruction loss function as in Eq. 1. However, to obtain a realistic face image from the generated \(}\) through the pre-trained synthetic network \(S_{}(.)\), the generated \(}\) needs to be in the distribution \(\); otherwise, the output may not look like a real human face. Hence, to generate \(}\) vectors such that they have the same distribution as StyleGAN's intermediate latent, \(\), we use a GAN-based framework to learn the distribution \(\). To this end, we use the Wasserstein GAN (WGAN)  algorithm to train a critic network \(C(.)\) which critiques the generated \(}\) vectors compared to the real StyleGAN's \(\) vectors, and simultaneously we optimize our mapping network to generate \(}\) vectors with the same distribution as \(\). Hence, we can consider our mapping network \(M_{}(.)\) as a conditional generator in our WGAN framework, which generates \(}=M_{}[[,]]\) given a facial template \(x\) and a random noise vector \(\). Then, we can train our mapping network and critic network using the following loss functions:

\[_{C}^{} =_{}[C()]-_{ {} M_{}[[,]]}[C(})]\] (4) \[_{M_{}}^{} =_{} M_{}[[,]]}[C (})]\] (5)

In a nutshell, we train a new mapping network \(M_{}(.)\) using our reconstruction loss function in Eq. 1, and also optimize \(M_{}(.)\) within our WGAN framework using Eq. 5. Simultaneously, we also train the critic network \(C(.)\) within our WGAN training using Eq. 4 to learn the distribution of StyleGAN's intermediate latent space \(\) and help our mapping network \(M_{}(.)\) to generate vectors with the same distribution as \(\). Figure 3 depicts the block diagram of the proposed method. We should note that our mapping network \(M_{}(.)\) has 2 fully connected layers with Leaky ReLU activation function.

In our problem formulation, we consider three different feature extraction models, including \(F_{}(.)\), \(F_{}(.)\), and \(F_{}(.)\). Hence, based on the adversary's knowledge and the target system, we can consider five different attacks:

* **Attack 1:** The adversary has _whitebox_ knowledge of the system from which the template is leaked and wants to attack the same system (i.e., \(F_{}=F_{}=F_{}\)).
* **Attack 2:** The adversary has _whitebox_ knowledge of the feature extractor of the system from which the template is leaked, but aims to attack to a different FR system (i.e., \(F_{}=F_{} F_{}\)).
* **Attack 3:** The adversary wants to attack the same system from which the template is leaked, but has only _blackbox_ access to the feature extractor of the system. Instead, we assume that the adversary has the _whitebox_ knowledge of another FR model to use for training (i.e., \(F_{}=F_{} F_{}\)).
* **Attack 4:** The adversary aims to attack a different FR system than the one from which the template is leaked. In addition, the adversary has _whitebox_ knowledge of the feature extractor of the target system (i.e., \(F_{} F_{}=F_{}\)).
* **Attack 5:** The adversary aims to attack a different FR system from which the template is leaked and has only _blackbox_ knowledge of both the target system and the one from which the template is leaked. However, the adversary instead has the _whitebox_ knowledge of another FR model to use for training (i.e., \(F_{} F_{} F_{}\)).

In the attack 1 and attack 2, the adversary has the _whitebox_ knowledge of the system from which the template is leaked (i.e., \(F_{}(.)\)) and uses the same model as \(F_{}(.)\) for training the reconstruction network. However, in attacks 3-5, the adversary has the _blackbox_ knowledge of the system from which the template is leaked, and therefore uses another FR model as \(F_{}(.)\). Comparing the knowledge

Figure 3: Block diagram of our face reconstruction network.

of the adversary in these attacks, we expect that attack 1 be the easiest attack for the adversary and attack 5 be the most difficult one. Table 1 compares adversary's knowledge and difficulty of different attacks defined in this paper.

## 3 Related Works

Table 2 compares our proposed method with related works in the literature. Generally, the methods for TI attack against FR systems, can be categorized based on different aspects, including the resolution of generated face images (high/low resolution), the type of attack (_whitebox/blackbox_ attack), and the basis of the method (optimization/learning-based).

Zhmoginov and Sandler (2016) proposed an optimization-based method and a learning-based method to generate low-resolution face images in the _whitebox_ attack against FR systems. In their optimization-based attack, they used a gradient-descent-based approach to find an image that minimizes the distance of the face template as well as some regularization terms to generate a smooth image, including the total variation and Laplacian pyramid gradient normalization (Burt and Adelson, 1987) of the reconstructed face image. In their learning-based attack, they trained a convolutional neural network (CNN) with the same loss terms to generate face images from given facial templates.

Cole et al. (2017) proposed a learning-based attack to generate low-resolution images using a multi-layer perceptron (MLP) to estimate landmark coordinates and a CNN to generate face textures, and then reconstructed face images using a differentiable warping based on estimated landmarks and face texture. They trained their networks in an end-to-end fashion, and minimized the errors for landmark estimation and texture generation as well as the distance of face template as their loss function. To extend their method from the _whitebox_ attack to the _blackbox_ attack, they proposed not to minimize the distance of face templates in their loss function.

Mai et al. (2018) proposed a learning-based attack to generate low-resolution images in the _blackbox_ attack against FR systems. They proposed new convolutional blocks, called neighborly deconvolution blocks A/B (shortly, NbBlock-A and NbBlock-B), and used these blocks to reconstruct face images. They trained their proposed networks using two loss functions, including pixel loss (i.e., \(_{2}\) norm of reconstruction pixel error) and perceptual loss (i.e., \(_{2}\) norm of distance for intermediate features of VGG-19 (Simonyan and Zisserman, 2014) given original and reconstructed face images). They

  &  &  &  &  &  \\    & & & 1) optimization & & & Whitebox & ✗ \\  Cole et al. (2017) & low & learning & both\({}^{*}\) & ✗ & ✗ \\  Mai et al. (2018) & low & learning & blackbox & ✗ & ✓ \\  Duong et al. (2020) & low & learning & both\({}^{**}\) & ✗ & ✗ \\  Truong et al. (2022) & low & learning & both\({}^{**}\) & ✗ & ✗ \\  Ahmad et al. (2022) & low & learning & blackbox & ✗ & ✗ \\  Dong et al. (2021) & high & learning & blackbox & ✗ & ✓ \\  Vendrow and Vendrow (2021) & high & optimization & blackbox & ✗ & ✓ \\  Dong et al. (2023) & high & optimization & blackbox & ✗ & ✓ \\  Ours & high & learning & both\({}^{***}\) & ✓ & ✓ \\  

*

Table 2: Comparison with related works.

 }}\)**} & }}\)**} &  &  &  \\    & & & & & \\ 
**Attack 1** & whitebox & \(F_{}\) & same system & whitebox knowledge of \(F_{}\) and \(F_{}\) & very easy \\ 
**Attack 2** & whitebox & \(F_{}\) & different system & whitebox knowledge of \(F_{}\) & easy \\ 
**Attack 3** & blackbox & adversary’s own & same system & blackbox knowledge of \(F_{}\) and \(F_{}\) & difficult \\ 
**Attack 4** & blackbox & \(F_{}\) & different system & blackbox knowledge of \(F_{}\) & difficult \\  & & & (transferability) & and whitebox knowledge of \(F_{}\) & very difficult \\ 
**Attack 5** & blackbox & adversary’s own & different system & \\  & & & (transferability) & only blackbox knowledge of \(F_{}\) & very difficult \\  

*

Table 1: Comparison of different attacks in this paper.

defined two types of attacks and compared the reconstructed face images with the same (type 1) or different (type 2) image of the same subject. However, they did not evaluate the transferability of reconstructed face images.

Duong et al. (2020) and Truong et al. (2022) used a same bijection learning framework and trained a GAN with a generator with structure of PO-GAN (Karras et al., 2017) and TransGAN (Jiang et al., 2021), respectively. While their method is based on the _whitebox_ attack, they proposed to use knowledge distillation to extend to the _blackbox_ attack. To this end, they trained a student network that mimics the target FR model. However, they did not provide any details (nor source code) about student network training, such as the structure of the student network, etc.

Ahmad et al. (2022) used a GAN-based face reconstruction network to generate low-resolution face images in the _blackbox_ scenario. They focus on the size of training dataset and proposed a method to learn the reconstruction network with less training data. While using GAN-based approach, sample reconstructed face images in their paper do not look realistic and suffer from many artifacts.

Dong et al. (2021) used a pre-trained StyleGAN to generate high-resolution face images in the _blackbox_ attack against FR systems. They generated synthetic face images using pre-trained StyleGAN and extracted their templates. Then, they trained a fully connected network using mean squared error to map extracted templates to the corresponding noise in the input space \(\) of StyleGAN. In contrast, Vendrow and Vendrow (2021), instead of a learning-based approach, used a grid search optimization using the simulated annealing (Van Laarhoven and Aarts, 1987) approach to find the noise \(\) in the input of StyleGAN, which generates an image that has the same template. As their iterative method has a large computation cost, they evaluated their method on 20 images only. Along the same lines, Dong et al. (2023) also tried to solve a similar optimization to (Vendrow and Vendrow, 2021) with a different approach. They used the genetic algorithm to find the noise \(\) in the input of StyleGAN that can generate an image with the same template.

Compared to most works in the literature that generate low-resolution face images, our proposed method generates high-resolution realistic face images. While low-resolution reconstructed images can be used for evaluating the vulnerability of FR systems under some assumptions, high-resolution images can lead to different types of presentation attacks against FR systems. Previous works in the literature for reconstructing high-resolution face images (Vendrow and Vendrow, 2021; Dong et al., 2021, 2023) tried to find an appropriate noise \(\) in the input of StyleGAN that can generate an image with a similar template. However, the input of StyleGAN \(\) is a noise and is difficult to control and optimize. In contrast, the intermediate latent space \(\), which we use in our paper, is more controllable2. However, finding appropriate latent code \(\) in the intermediate space of StyleGAN that can generate an image with a similar facial template is challenging for two reasons. First, for a learning-based approach, we do not have correct values of \(\) for each real face image to directly use for training the mapping from face templates to the intermediate latent space \(\) of StyleGAN. Second, as described in Section 2, the mapped latent code should be in the same the intermediate latent space \(\) of StyleGAN, otherwise the generated image is not face-like.

We should also note that, unlike most works in the literature, we propose our method for both _whitebox_ and _blackbox_ scenarios and evaluate the transferability of our attack. Similar to (Cole et al., 2017; Duong et al., 2020; Truong et al., 2022), our method is based on the _whitebox_ knowledge of FR model, however our approach for extending our method to the _blackbox_ attack using another FR model has not been used in TI attacks. Last but not least, we define five different attacks against FR systems and evaluate the vulnerability of SOTA FR models to our attacks. To our knowledge, this is the first paper in which the transferability of reconstructed face images in TI attacks has been investigated.

## 4 Experiments

In this section, we present our experiments and discuss our results. First, in Section 4.1 we describe our experimental setup. Then, we present our experimental results in Section 4.2, and discuss our findings.

### Experimental Setup

To evaluate the performance of our method, we consider two SOTA FR models3, including ArcFace (Deng et al., 2019), ElasticFace (Boutros et al., 2022), as the models from which templates are leaked (i.e., \(F_{}\)). For transferability evaluation, we also use three different FR models with SOTA backbones from FaceX-Zoo (Wang et al., 2021), including HRNet (Wang et al., 2020), AttentionNet (Wang et al., 2017), and Swin (Liu et al., 2021), for the target FR system (i.e., \(F_{}\)). The recognition performance of these models are reported in Table 3. All these models are trained on MS-Celeb1M dataset (Guo et al., 2016). We assume that the adversary does not have access to the FR training dataset, and therefore we use another dataset for training our face reconstruction models. To this end, we use the Flickr-Faces-HQ (FFHQ) dataset (Karras et al., 2019), which consists of 70,000 high-resolution (i.e., \(1024 1024\)) face images (without identity labels) crawled from the internet. We use 90% random portion of this dataset for training, and the remaining 10% for validation.

To evaluate different attacks against FR systems, we consider two other face image datasets with identity labels, including the MOBIO (McCool et al., 2013) and Labeled Faces in the Wild (LFW) (Huang et al., 2007) datasets. The MOBIO dataset consists of bi-modal (face and voice) data captured using mobile devices from 150 people in 12 sessions (6-11 samples in each session). The LFW dataset includes 13,233 face images of 5,749 people collected from the internet, where 1,680 people have two or more images.

For each of the attacks described in Section 2, we build one or two separate FR systems with one or two SOTA FR models based on the attack type. If the target system is the _same_ as the system from which the template is leaked, we have only one FR system. Otherwise, if the target system is _different_ the system from which the template is leaked, we have two FR systems with two different feature extractors. In each case, we use one of our evaluation datasets (i.e., MOBIO and LFW) to build both FR systems (so that the subject with the leaked template be enrolled in the target system too). In each evaluation, we assume that the target FR system is configured at the threshold corresponding to a false match rate (FMR) of \(10^{-3}\), and we evaluate the adversary's success attack rate (SAR) in entering that system.

We should note that the templates extracted by the aforementioned FR models have 512 dimensions. The input noise \(\) to the mapping network of StyleGAN's pre-trained network is from the standard normal distribution and has 512 dimensions. The input noise \(\) to our mapping network \(M_{}(.)\) is with dimension of 8 and also from the standard normal distribution. We also use Adam (Kingma and Ba, 2015) optimizer to train our mapping network4.

### Analysis

In this section, we consider SOTA FR models and evaluate the performance of our face reconstruction method in five different attacks described in Section 2. We also explore the effect of our WGAN training as well as effect of loss terms as our ablation study. In addition, explore important areas in the reconstructed face images that lead to success TI attack. Finally, we discuss limitations of our face reconstruction model.

Whitebox Knowledge of \(F_{}\)For attacks 1-2, the adversary is assumed to have _whitebox_ knowledge of the system from which the template is leaked (i.e., \(F_{}\)) and use the same feature extraction model for training (i.e., \(F_{}\)), thus in such cases \(F_{}=F_{}\). We considered ArcFace

  &  &  \\   & **FMR=10\({}^{-2}\)** & **FMR=10\({}^{-3}\)** & **FMR=10\({}^{-2}\)** & **FMR=10\({}^{-3}\)** \\ 
**ArcFace** & 100.00 & 99.98 & 97.60 & 96.40 \\ 
**ElasticFace** & 100.00 & 100.00 & 96.87 & 94.70 \\ 
**HRNet** & 98.98 & 98.23 & 89.30 & 78.43 \\ 
**AttentionNet** & 99.71 & 97.73 & 84.27 & 72.77 \\ 
**Swin** & 99.75 & 98.98 & 91.70 & 87.83 \\  

Table 3: Recognition performance of face recognition models used in our experiments in terms of true match rate (TMR) at the thresholds correspond to false match rates (FMRs) of \(10^{-2}\) and \(10^{-3}\) evaluated on the MOBIO and LFW datasets. The values are in percentage.

and ElasticFace models and reconstructed face images from the templates extracted by these models in attacks against different FR systems. Table 4 reports the vulnerability of different target systems to our attacks5 1-2 in terms of adversary's SAR at the system's FMR of \(10^{-3}\). Similar results for the system's FMR of \(10^{-2}\) are reported in Table 7 of the appendix. According to these tables, our method achieves considerable SAR against ArcFace and ElasticFace target systems in attack 1. In attack 2, we observe that there is a degradation in SAR with respect to attack 1. However, the reconstructed face images can still be used to enter another target system. Meanwhile, the FR model with a higher recognition accuracy is generally more vulnerable to attack 2. For instance, when ArcFace is considered as \(F_{}\), we observe that ElasticFace and Swin have the highest SAR as target systems, while there is the same order for their recognition performance in Table 3.

Blackbox Knowledge of \(F_{}\)For attacks 3-5, the adversary is assumed to have _blackbox_ knowledge of the system from which the template is leaked (i.e., \(F_{}\)) and use another feature extraction model for training (i.e., \(F_{}\)), therefore in such cases \(F_{} F_{}\). Table 5 compares the performance of our method with _blackbox_ methods6 in the literature  for attacks 3-5 in terms of adversary's SAR at system's FMR of \(10^{-3}\). Similar results for the FMR of \(10^{-2}\) are available in Table 8 of the appendix.

As these tables show, our proposed method achieves the highest SAR compared to  against FR systems on the MOBIO and LFW datasets. In particular, in attack 5 which is the hardest attack, where \(F_{}\), \(F_{}\), and \(F_{}\) are different, the results show that the target FR system is still vulnerable to our attack. The results of our method for attack 5 also show transferability of our attack to different FR systems. Similar to attack 2, we can also observe that in attack 5, the FR model with a higher recognition accuracy is generally more vulnerable to our attack.

  &  &  \\   &  & **ElasticFace** & **HRNet** & **AttentionNet** & **Swin** & **ArcFace** & **ElasticFace** & **HRNet** & **AttentionNet** & **Swin** \\   & 92.38 & 81.90 & 71.43 & 70.48 & 74.29 & 86.82 & 74.20 & 36.57 & 36.40 & 58.86 \\   & 78.10 & 87.62 & 64.29 & 64.76 & 69.05 & 78.25 & 82.52 & 41.80 & 40.25 & 61.09 \\  

Table 4: Evaluation of attacks with _whitebox_ knowledge of the system from which the template is leaked (i.e., \(F_{}=F_{}\)) against SOTA FR models in terms of adversary’s success attack rate (SAR) using our proposed method on the MOBIO and LFW datasets. The values are in percentage and correspond to the threshold where the target system has FMR \(=10^{-3}\). Cells are color coded according the type of attack as defined in Section 2 for attack 1 (light gray ) and attack 2 (dark gray ).

  &  &  \\   & }}\)} & }}\)} & }}\)} &  &  \\   & & & **M1** & **M2** & **M3** & **M4** & **M5** & **Ours** & **M1** & **M2** & **M3** & **M4** & **M5** & **Ours** \\   &  &  & 1.90 & 15.24 & 2.38 & 28.10 & 58.57 & **81.90** & 10.68 & 40.25 & 12.91 & 58.88 & 75.31 & **77.16** \\   & & & **ElasticFace** & 1.43 & 11.43 & 42.9 & 15.24 & 37.61 & **73.81** & 8.36 & 34.93 & 6.35 & 29.10 & 50.17 & **68.06** \\   & & & **HRNet** & **0.95** & **61.9** & **28.06** & **10.00** & **30.48** & **57.14** & 1.30 & 7.78 & 1.75 & **92.00** & **24.72** & **28.45** \\   & & & **AttentionNet** & **0** & **66.7** & **23.38** & **20.67** & **54.29** & 1.33 & **71.7** & **22.9** & **91.77** & **24.16** & **28.87** \\   & & & **Swin** & **1.43** & **13.33** & **3.81** & **10.95** & **40.00** & **67.14** & **4.27** & **23.85** & **5.97** & 21.75 & 41.27 & **48.28** \\   &  &  & 2.38 & 18.57 & 2.86 & 16.19 & 48.09 & **87.14** & 15.33 & 48.67 & 11.81 & 37.45 & 65.40 & **83.20** \\   & & & **ElasticFace** & 3.81 & 43.81 & 4.76 & 43.33 & 72.38 & **89.05** & 21.44 & 58.16 & 11.59 & 52.88 & 74.08 & **83.43** \\    & & & **HRNet** & **0.48** & **20.00** & **1.43** & **10.48** & 42.86 & **73.81** & **3.46** & **18.36** & **2.74** & **11.82** & **32.90** & **49.02** \\    & & & **AttentionNet** & **1.90** & 18.10 & 3.33 & 9.05 & 40.00 & **71.90** & 2.89 & 16.31 & 2.91 & 10.95 & 31.15 & **46.63** \\    & & & **Swin** & **0.95** & 26.19 & **28.6** & **15.24** & **46.67** & **75.24** & **9.22** & **38.79** & **8.26** & **24.62** & **51.20** & **66.89** \\  

Table 5: Evaluation of attacks (with _blackbox_ knowledge of the system from which the template is leaked i.e., \(F_{}\)) against SOTA FR models in terms of adversary’s success attack rate (SAR) using different methods on the MOBIO and LFW datasets. The values are in percentage and correspond to the threshold where the target system has FMR \(=10^{-3}\). **M1**: NbNetB-M , **M2**: NbNetB-P , **M3**: , **M4**: , and **M5**: . Cells are color coded according the type of attack as defined in Section 2 for attack 3 (light gray ), attack 4 (middle dark gray ), and attack 5 (darkest gray ).

Figure 4 also shows sample face images from the LFW dataset and the reconstructed images using our proposed method from ArcFace templates in different attacks. We should highlight that as show in Figure 4, the reconstructed face images in attack 1 and attack 2 are the same, but they are used to enter different target FR system. The same holds for the reconstructed face images in attacks 3-5.

Ablation StudyTo evaluate the effect of WGAN in training our mapping network and the effect of each term in our loss function (i.e., Eq. 1), we consider the ArcFace model in the _white-box_ scenario and train different face reconstruction networks with different loss functions. Then, we attack a system with the ArcFace model as a feature extractor (i.e., attack 1) and compare the SARs as reported in Table 6. According to these results, the proposed adversarial training has a significant effect on our face reconstruction method. Because we fix the synthesis network of StyleGAN, the mapped latent codes need to be of the same distribution as \(\). Otherwise, the generated image is not face-like and training fails to converge. The WGAN training in our method helps our mapping network to learn the distribution of StyleGAN's intermediate latent space, and thus the synthesis network generates face-like images. When we use the WGAN training and based on the results in Table 6, the ID loss has a high impact on the performance of the template inversion model. While the pixel loss by itself does not achieve a good performance, it improves the performance of ID loss in our reconstruction loss function in Eq. 1. This table confirms that the proposed WGAN training and our reconstruction loss function lead to a more successful attack. More experiments for ablation study on the effect of different elements in our proposed method are presented in the appendix.

Important Areas in the Reconstructed Face ImagesAs another experiment, we explore important areas in the reconstructed face images. Finding these areas help us to investigate what features between the original template and synthetic images fool the face recognition system, and therefore we can understand what information is encoded in the facial templates. To this end, we apply the Grad-Cam  algorithm using the face recognition model on the reconstructed face images to see which areas of the reconstructed face images are important and cause the facial templates of our reconstructed face images to be close to the original facial templates. Figure 5 shows results of applying the Grad-Cam algorithm on sample reconstructed face images using our proposed method. As the results in this figure show, important areas that cause the reconstructed face images to have similar templates to the original ones correspond to areas such as eyes, nose, lips, etc. In

  **WGAN training** \\ **(Eqs. 4 and 5)** \\  & 
 **Reconstruction** \\ **Loss Function** \\  &  &  \\   & & **FMR=\(10^{-2}\)** & **FMR=\(10^{-3}\)** & **FMR=\(10^{-2}\)** & **FMR=\(10^{-3}\)** \\   & \(_{}=_{}+_{}\) & **100.00** & **92.38** & **93.64** & **86.82** \\   & \(_{}=_{}\) & 98.10 & 82.38 & 90.56 & 80.74 \\   & \(_{}=_{}\) & 0 & 0 & 0.65 & 0.07 \\   & \(_{}=_{}+_{}\) & 0 & 0 & 0.32 & 0.02 \\   & \(_{}=_{}\) & 0 & 0 & 0.14 & 0.02 \\    & \(_{}=_{}\) & 0 & 0 & 0.44 & 0.09 \\  

Table 6: Evaluating the effect of each loss term in our loss function in attack 1 against ArcFace in terms of SAR in the system with FMRs of \(10^{-2}\) and \(10^{-3}\) evaluated on the MOBIO and LFW datasets. The values are in percentage.

Figure 4: Sample face images from the LFW dataset (first raw) and their corresponding reconstructed images using our template inversion method from ArcFace templates in different attacks, attacks 1-2 (second raw) and attacks 3-5 (second raw, using ElasticFace for \(F_{}\)). The values below each image show the cosine similarity between the corresponding ArcFace templates of original and reconstructed face images.

particular, the area around the eyes seems to be the most important part in most of the reconstructed face images. These results also show that the general shape of the face (e.g., thin or chubby face), hairs, textures, etc., are not often necessarily important in the reconstructed face images, and thus, we can also conclude that these attributes are not well-encoded in the templates extracted by face recognition models.

LimitationsDespite the significant performance of our method in terms of success attack rate in all types of attacks reported in Table 4 and Table 5, the reconstructed face images fail to enter the system in some cases. Figure 6 illustrates sample failure cases in the attack 3 against ArcFace (using ElasticFace for \(F_{}\)) on the LFW dataset. From the failure cases, we can conclude that there is a bias in the face reconstruction for specific demographics, like elderly or dark skin people. Indeed, such kind of bias in the reconstructed face images is caused by inherent biases in datasets used to train FR model, the StyleGAN model, and our mapping network in our face reconstruction model7.

## 5 Conclusion

In this paper, we proposed a new method to reconstruct high-resolution realistic face images from facial templates in a FR system. We used a pre-trained StyleGAN3 network and learned a mapping from facial templates to intermediate latent space of StyleGAN within a GAN-based framework. We proposed our method for _whitebox_ and _blackbox_ scenarios. In the _whitebox_ scenario, the adversary can use the feature extraction model for training the face reconstruction network; however, in the _blackbox_ scenario, we assume that the adversary has access to another feature extraction model. In addition, we consider the threat model where the adversary might impersonate in the same or another (i.e., transferable attack) FR system. Based on the adversary's knowledge of the feature extraction model and the target FR system, we defined five different attacks and evaluated the vulnerability of SOTA FR systems to our proposed method. Our experiments showed that the reconstructed face images by our proposed method not only can achieve a high SAR in _whitebox_ and _blackbox_ scenarios, but also are transferable and can be used to enter target FR systems with a different FR model.

Figure 5: Sample face images from the FFHQ dataset and the corresponding important areas in the reconstructed face images using the Grad-Cam algorithm. The reconstructed face images are generated from ArcFace templates. In each example, the top image is the output of the Grid-Cam algorithm, and the bottom images are real (right) and reconstructed (left) images. The value below each sample is the cosine similarity between the templates of original and reconstructed face images.

Figure 6: Sample failure cases images from the LFW dataset and their corresponding reconstructed images using our template inversion method from ArcFace templates in the attack 3 (using ElasticFace for \(F_{}\)). The values below each image show the cosine similarity between the corresponding templates of original and reconstructed face images.