# An Efficient End-to-End Training Approach for Zero-Shot Human-AI Coordination

Xue Yan

Institute of Automation, Chinese Academy of Science, Beijing, China

Jiaxian Guo

University College London, UK

Xingzhou Lou

Corresponding author

Jun Wang

Kingng Zhang

Institute of Automation, Chinese Academy of Science, Beijing, China

Yali Du

Corresponding author

###### Abstract

The goal of zero-shot human-AI coordination is to develop an agent capable of collaborating with humans without relying on human data. Prevailing two-stage population-based methods require a diverse population of mutually distinct policies to simulate diverse human behaviors. The necessity of such populations severely limits their computational efficiency. To address this issue, we propose E3T, an Efficient **E**nd-to-**E**nd **T**raining approach for zero-shot human-AI coordination. E3T employs a mixture of ego policy and random policy to construct the partner policy, making it both skilled in coordination and diverse. This way, the ego agent is trained end-to-end with this mixture policy, eliminating the need for a pre-trained population, and thus significantly improving training efficiency. In addition, we introduce a partner modeling module designed to predict the partner's actions based on historical contexts. With the predicted partner's action, the ego policy can adapt its strategy and take actions accordingly when collaborating with humans exhibiting different behavior patterns. Empirical results on the Overcooked environment demonstrate that our method substantially improves the training efficiency while preserving comparable or superior performance than the population-based baselines. Demo videos are available at https://sites.google.com/view/e3t-overcooked.

## 1 Introduction

Cooperation between AI agents and humans has gained significant attention, spanning various fields such as robotics , automatic driving , Human-AI dialogue  and Human-AI coordination games . Due to the high cost of collecting human data and involving humans during training, recent studies  have focused on zero-shot human-AI coordination problems, with the aim to train an ego agent capable of collaborating with humans without relying on human data.

To train such an ego agent, the prevailing approach  involves training the ego agent with diverse partner agents that can simulate human behavior without the need of human data. One method for achieving this is through self-play , where the ego agent plays against itself as the partner agent. However, empirical evidence has shown that self-play can lead to agents getting stuck in a single cooperative pattern during training , hindering their ability to adapt to diverse human behaviors. Alternative approaches like other-play  aim to introduce diversity by breaking the symmetry of self-play policies. Nevertheless, this method relies on the assumption of strict symmetry in the action or state space, limiting its practical applicability.

In contrast to relying solely on a single partner policy, advanced population-based methods [21; 42; 32; 18; 19] construct a population with multiple policies to simulate diverse human behaviors, and then train the ego agent to collaborate with this population. Among these methods, FCP  obtains a diverse population by training several self-play policies with different random seeds and saving multiple past checkpoints reflecting various coordination levels. TrajeDi  promotes diversity by maximizing the divergence of trajectory distributions within the partner population. MEP  achieves a cooperative and diverse population by maximizing self-play rewards and population entropy as auxiliary rewards.

However, these population-based methods suffer from two major limitations. Firstly, they are highly computationally inefficient due to their two-stage framework, which involves training a population of policies before training the ego agent. For example, training a MEP agent for one environment in overcooked  can take over 15 hours on a single 3090Ti GPU. By contrast, a self-play agent can be trained in less than 1 hour. Secondly, these methods only learn a single response for all partners in the population, neglecting the diverse behavior patterns among them. As a result, the learned ego agent lacks the capability to adjust its policy based on each partner's unique behavior pattern, leading to unsatisfactory performance in zero-shot human-AI coordination.

In this paper, we introduce an **E**fficient **E**nd-to-**E**nd **T**raining approach, called E3T, which aims to improve both efficiency and the ability to recognize behavior patterns in zero-shot human-AI coordination. To achieve efficient training, E3T utilizes a blend of ego and random policies as partners during training, and integrates a partner modeling module into the ego policy. This approach is motivated by the idea that the partner policies should be both skilled in coordination and diverse . By breaking down these two objectives into two policies _i.e._ ego policy and random policy, and merging them as the partner policy, E3T is able to be end-to-end trained without the need for pre-training a population, thus enjoying higher efficiency and a simpler implementation compared to population-based methods.

In order to improve the collaboration capabilities of of ego agents when working with partners exhibiting different behaviors, we introduce a partner modeling module that predicts the partner's actions based on historical contexts. By incorporating this module into the ego policy, the agent is able to know not only the current observation, but also the predicted actions of the partner. This enables the ego agent to adapt its behavior more effectively, aligning it better with the partner's preferences and resulting in improved collaboration performance. To the best of our knowledge, our paper is the first to utilize a partner action prediction module in the context of a two-player human-AI coordination task.

Our contributions are three-fold. Firstly, we introduce an efficient end-to-end ego training approach, named E3T, for zero-shot Human-AI coordination, which offers higher training efficiency compared to population-based methods. Secondly, we propose a partner modeling module that predicts partner actions from the context, allowing the ego policy to adapt when collaborating with humans exhibiting different behavior patterns. Lastly, empirical results on Overcooked show that our method, E3T, significantly improves training efficiency while achieving comparable or superior performance to existing methods as Figure 1 shows. Specifically, E3T demonstrates a significant improvement in training efficiency when compared to state-of-the-art population-based methods.

## 2 Related Work

Our work investigates how to train an ego agent for zero-shot Human-AI coordination, a field with broad applications in real-world scenarios such as robotics [5; 26], service allocation [36; 37; 35] and Human-AI coordination games . In zero-shot Human-AI coordination tasks, reinforcement learning (RL) algorithms  are tasked with training a robust ego agent without relying on human data, due to the high cost of collecting real human data.

Figure 1: Illustration of training time and zero-shot coordination performance of baselines. The right \(y\)-axis is the required hours to train one model. The left \(y\)-axis represents the average rewards of collaborating with AI baselines across \(5\) layouts.

Self-Play TrainingThere are a number of methods to tackle zero-shot coordination, broadly categorized into two classes: self-play and population-based training. Self-play has demonstrated impressive capability in competitive games such as Go , Dota , StarCraft  and imperfect information games . Self-play can also master high-level skills in cooperative games . However, self-play agents often struggle to coordinate effectively with previously unseen partners due to their tendency to adopt rigid conventions formed during training against themselves . Other-play  attempts to mitigate these conventions by breaking the symmetries of self-play policies. However, it heavily relies on the prior knowledge of strict symmetries in the environment, limiting its applicability in general environments lacking such symmetries. In this work, E3T follows the self-play training framework, which avoids falling into conventions by encouraging the partner policies to be more diverse. In addition, our method offers greater generality compared to other-play, making it applicable in a wider range of environments.

Population-Based Training MethodsPopulation-based methods train a robust AI by collaborating with a diverse population. Experimental results show that population-based methods achieve superior zero-shot coordination performance compared to self-play and other-play . For example, PBT  employs an online evolutionary algorithm, which iteratively updates policy parameters and performs policy substitution within the population. FCP  achieves diversity within a population by training several self-play policies with different random seeds and saving multiple past checkpoints to obtain agents with various coordination levels. In a similar vein, TrajeDi  regulates the partner population to be diverse by maximizing the divergence of trajectory distributions between policies within the population. Furthermore, MEP  introduces an auxiliary reward by designing the entropy of the average of policies in the population, thus promoting population diversity. More recently, COLE [18; 19] reformulate two-player coordination as a game-theoretic problem, where the trainer's objective is to estimate the best responses to the most recent population based on the cooperative incompatibility distribution. Our work improves the partner behavior diversity by maximizing the entropy, but E3T directly increases the randomness of a single-agent partner policy, making it more efficient than MEP and TrajeDi.

Agent ModelingAgent modeling is an important ability for autonomous agents, enabling them to reason about various properties of other agents such as actions and goals . Typical agent modeling methods focus on reasoning about types of other agents [1; 2; 31], plan recognition [9; 7], group modeling [23; 11; 20] and especially policy reconstruction [6; 25; 4; 27], which is also adopted in E3T to improve zero-shot human-AI coordination performance. However, a key difference is that E3T's ego agent's partner modeling module is trained by pairing with agents and aims to predict the actions of partners. In addition, we directly employ the predicted partner actions for decision-making, instead of using the embedding of partner behaviors .

## 3 Preliminary

In this section, we introduce the two-player cooperative Markov Game and a representative pipeline of population-based training for two-player zero-shot coordination.

Two-player Cooperative Markov GameA two-player cooperative Markov Game is described by a tuple \(,,,,R\), where \(\) is a finite set of states, \(\) is the finite set of actions, and suppose two agents have the same action space. \(p_{0}:\) is the initial state distribution. \(: \) is the transition function and assumes the transition dynamic is deterministic. \(R:\) is the reward function and assume rewards are bounded by \(\), i.e. \(R(s,a^{1},a^{2}), s,a^{1},a^{2}\). We assume the transition and reward functions are unknown. In this work, we solve the two-policy cooperative Markov Game via single-agent reinforcement learning and the self-play framework as previous studies [8; 15]. Specifically, the ego policy \(_{}\) is trained by pairing with a fixed partner policy \(_{}\) with the object defined as \(_{}=\,_{}V(,_{})\).The value function of joint policy \((,_{})\) is defined as \(V(,_{})=E_{p_{0},,a^{1},a^{2}_{}}_{t=0}^{}^{t}R(s_{t},a_{t}^{1},a_{t}^{2})\), where \((0,1)\) is the discounted factor.

Population-Based TrainingPopulation-based methods [21; 42; 32] aim to construct a population of policies to simulate diverse human behaviors. Intuitively, we expect that the policy in the population should be both skilled in coordination and diverse. Taking Maximum Entropy Population (MEP)  as an example, it constructs the partner population by maximizing the self-play rewardsand simultaneously increasing the entropy of policies in the population. The objective of MaximumEntropy Population training is defined as:

\[J()=_{_{i}}[_{t}_{(s_{t},a _{t}^{1},a_{t}^{2})(_{i},_{i})}[R(s_{t},a_{t}^{1},a_{t}^{2})+ ((.|s_{t}))]].\] (1)

Here \(=\{_{1},_{2},...,_{n}\}\) denotes a population of policies, \((.|s_{t})=_{i=1}^{n}_{i}(.|s_{t})\) is the mean policy of the population \(\), and \(\) determines the relative importance of the population entropy term against the task reward. The task reward under the joint action \(_{t}\) is obtained by self-play with a uniformly sampled policy from the population, and the population entropy term acts as an auxiliary reward. With this competent and diverse population as the partner population, an ego policy is trained by collaborating with the population via prioritized sampling that assigns a higher priority to the partner policy that is difficult to cooperate with. The objective of ego policy training is defined as:

\[J(_{e})=_{t}_{(s_{t},a_{t}^{1},a_{t}^{2})(_{i},( ))}[R(s_{t},a_{t}^{1},a_{t}^{2})],\] (2)

where \((.)\) as the prioritized sampling mechanism. However, the training process of MEP is inefficient due to the complex two-stage training and the necessity of maintaining a large population for behavioral diversity. Our work draws inspiration from MEP, which achieves behaviorally diverse and cooperative partners by maximizing the self-play reward and the entropy of partner policy and then trains a robust ego policy by cooperating with those partners. In the next section, we will discuss how to solve zero-shot coordination using an efficient end-to-end training process.

## 4 Methodology

In this section, we introduce the proposed method, E3T, an efficient end-to-end training approach, which trains an ego policy \(_{}\) through collaborating with a partner policy \(_{}\). To take into account both training efficiency and partner diversity, we employ a mixture of the copied ego policy and a random policy as the partner policy \(_{}\). The objective of E3T is defined as follows:

\[J(_{})=_{t}_{(s_{t},a_{t}^{1},a_{t}^{2})(_{i},_{})}[R(s_{t},a_{t}^{1},a_{t}^{2})]\] (3)

The proposed method contains two key components, a diverse partner policy and a partner modeling module for adapting to unseen partners, which will be introduced in Section 4.1 and Section 4.2 respectively. An overview of E3T is presented in Figure 2.

### A Mixture Policy as Partner

We begin with the motivation of leveraging the proposed mixture partner policy. Recall that MEP trains a partner population by maximizing self-play rewards of policies within it and maximizing the population entropy as shown in Equation (1). The first term on maximizing self-play rewards tends to learn a population with cooperative ability. The second entropy term can force the partner population

Figure 2: (a). The illustration of the self-play training framework, which trains the ego policy by pairing it with the copied partner policy. (b). The illustration of E3T. The green box shows the decision process of the ego agent. The blue box shows that of the partner agent.

to be diverse. However, optimizing the objective of Equation (1) suffers very high computational complexity due to the necessity of training a number of policies to collaborate effectively with themselves. Moreover, it exhibits unstable convergence properties, requiring meticulous hyperparameter tuning for additional entropy term .

To circumvent computational cost and non-stable training, we construct the partner policy via dividing Equation (1) into two parts with different partner policies learned, _i.e._\(_{}\) and \(_{}\), which aim to maximize the rewards \(_{t}_{(s_{t},_{t})_{},_{}} [R(s_{t},_{t})]\) and the entropy of the policy output \(_{t}_{(s_{t},_{t})_{}}[(_{}(.|s_{t}))]\), respectively. We can see that maximizing the former objective function is aligned with the goal of learning coordination skills, akin to the self-play training objective. In this way, we can directly use the ego policy \(_{}\) as \(_{}\). On the other hand, if we directly learn the latter one objective, _i.e._, the entropy of the mean policy, policy \(_{}\) will lead to the random policy \(_{}\), _i.e._, a uniform distribution over the action space.

To construct a partner who has both skills in coordination and diverse behaviors, we directly mix the above-discussed partner policies \(_{}\) and \(_{}\) as the partner policy \(_{}\). As previously discussed, \(_{}\) can directly apply ego policy \(_{}\) to achieve high reward for coordination. \(_{}\) is a random policy \(_{}\), which has the maximum policy entropy. In this way, the partner policy \(_{}\) is the mixture between the self-play ego policy and the random policy with the balance parameter \(\), formally defined as:

\[_{}=_{}+(1-)_{}\] (4)

We can see that the computation cost for obtaining this mixture partner policy is significantly lower than population-based methods since the conversion eliminates the need to train a population of policies. Additionally, the partner policy achieves behavioral diversity by simply leveraging a random policy with maximum entropy, whereas population-based approaches rely on more complex mechanisms, such as gradient descent  or reward shaping , to enforce the diversity. Both diversity and coordination ability are two keys to constructing a partner policy. Intuitively, the diversity reflected by entropy increases, and the coordination performance decreases with the random coefficient epsilon increasing. We attempt to quantitatively verify these intuitions through Propositions 1 and 2. Firstly, we analyze the behavior diversity of the partner policy of E3T. The entropy of partner policy reflects the diversity of partner behaviors. Proposition 1 shows that the entropy of the mixture partner policy is related to that of the random policy with maximum entropy.

**Proposition 1** (Entropy of Mixture Partner Policy).: _The entropy of the mixture partner policy, defined as \(_{p}=(1-)_{e}+_{r}\), satisfies that:_

\[(_{p}(.|s_{t}))(_{r}(.))-C_{1}, s _{t}\] (5)

_where \(C_{1}=+||(1-)\), and \(||\) is the cardinality of the action space \(\). In addition, the lower bound of \((_{p}(.|s_{t}))\) is increased as the increasing of \(\). Proof. See Appendix._

Secondly, we demonstrate the benefits of improving coordination skills when pairing with the partner policy of E3T. According to Proposition 2, although \(_{}^{}\) is the best response to the mixture partner policy rather than the ego policy itself, the coordination performance of \(_{}^{}\) with the ego policy is lower bounded by that of ego policy self-play with a punishment term \(\). The \(\) is related to the balance parameter \(\) and smaller \(\), which means less weight on the random policy, leads to lower \(\).

**Proposition 2** (Coordination Skill Improvement).: _Let the \(_{}^{}\) as the best response to \(_{p}=(1-)_{e}+_{r}\), i.e \(_{}^{}=*{arg\,max}_{}V(,_{p})\), then we have \(V(_{}^{},_{}) V(_{},_{})-,\) where \(=}\). Proof. See Appendix._

### Partner Modeling

In this section, we describe the details of the proposed partner modeling module, designed to extract and analyze the behavior pattern of the partner from historical information. This module equips the ego policy with the ability to consider not only the current state but also the partner's behavior pattern, empowering the ego agent to adapt to previously unseen partners

Partner Action PredictionAs in previous works [6; 25; 4], we model the partner agent by policy reconstruction. If the actual partner action distribution \(_{t}^{}\) is given, the two-player cooperative Markov Game will reduce to a single-agent MDP about the ego agent, in which the ego agent can easily find the best response to the current state. Formally, under the partner policy \(_{}\) with \(_{t}^{}=_{}(.|s_{t})\), the two-player MDP with transition function \(\) can be converted in to a single-agent MDP with transition function denoted as \(_{_{}}(s^{}|s_{t},a)=_{a t}_{}(a^{ }|s_{t})(s^{}|s_{t},a^{},a)\). However, because the agents act simultaneously, the ego agent can only infer the partner's action from historical information instead of directly observing the actual partner's action.

To obtain the predicted partner action \(}_{t}^{}\), we first devise an encoder network \(E_{}\) to extract historical partner context \(z_{t_{}}^{}\) from a sequence of past state-action pairs \(_{t}=\{(s_{t-i},a_{t-i}^{})_{i=1}^{k}\}\). The historical partner context \(z_{t}^{}\) contains rich information of the partner's behavior and coordination pattern, enabling our action prediction network \(F\) to predict the partner's action distribution based on \(z_{t}^{}\) and current state \(s_{t}\). This procedure is formally written as follows:

\[z_{t}^{}=E_{c}(_{t}),\ }_{t}^{}=F(s_{t},z_{t} ^{})\] (6)

The parameters of the context encoder \(E_{}\) and the action prediction network \(F\) are jointly optimized by minimizing a cross-entropy loss:

\[=_{i=1}^{N}-_{i}{}^{T}}_{i}^{ },\] (7)

where \(N\) is the batch size, and \(_{i}\) is a one-hot vector indicating the actual partner action.

We provide a theoretical analysis of how the accuracy of partner modeling affects the coordination ability as shown in Proposition 3. We analyze the coordination performance gap between the best responses to the ground-truth partner policy and the partner prediction module. It is established that this performance gap is proved to be proportional to the prediction error.

**Proposition 3**.: _Denote \(_{}(.|s_{t})=_{_{t}}[F(s_{t},E_{c}(_{t}))]\) as the expectation of the proposed partner prediction module over historical trajectory \(_{t}\). The ground-truth partner policy is denoted as \(_{p}(.|s_{t})\). Assume the learned \(_{}(.|s_{t})\) satisfies that \(D_{TV}(_{}(.|s_{t}),_{p}(.|s_{t}))_{m}, s_{t}\). Define the \(_{}^{*}\) and \(_{p}^{*}\) are the best response to the partner prediction module and the ground-truth partner policy respectively. Then the value functions of these best responses collaborating to the partner policy satisfy that: \(|V(_{}^{*},_{p})-V(_{p}^{*},_{p})|}{(1-)^{2}}\). Proof. See Appendix._

### Ego Policy Training and Test

E3T separately trains the partner action prediction module via minimizing the loss function in Equation (7) and trains the ego policy that makes decisions depending on both observations and predicted partner action distribution via PPO . The detailed training process of E3T is described in the Algorithm 1. During testing time, the ego agent zero-shot coordinates with an unseen partner, without parameters updating or temporally-extended exploration as .

```
0: Parameters of context encoder \(E_{}\), action prediction network \(F\), ego policy \(_{}\), hyper-parameter \(\), \(k\).
0: Well-trained context encoder \(E_{}\), action prediction network \(F\) and the ego policy \(_{}\).
1:while not done do
2: Set data buffer \(D=\).
3:for\(t=1,2,,\) step do
4: Encode context of the partner: \(z_{t}^{}=E_{}(_{t})\)
5: Predict partner action distribution: \(}_{t}^{}=F(z_{t}^{})\)
6: Randomly select an action distribution: \(}_{t}^{}\)
7: Mixture partner policy as: \(_{}(|s_{t},}_{t}^{})=(1-)_{ }(|s_{t},}_{t}^{})+_{}()\)
8: Gather data from \(_{}(|s_{t},}_{t}^{})\) and \(_{}(|s_{t},}_{t}^{})\), then \(D=D\{(_{t+1},s_{t},a_{t}^{},a_{t}^{},r_{t})\}\)
9:endfor
10: Update parameters of partner modeling module by minimizing Equation (7).
11: Update parameters of the ego policy \(_{}\) via PPO.
12:endwhile ```

**Algorithm 1** E3T: An efficient end-to-end training approach with mixture policies and partner modeling

Implement DetailsThe partner modeling module of E3T takes current observation at step \(t\) and partner's past \(k\) state-action pairs as input as shown in Equation (6), we set \(k=5\) in this work. The ego policy \(_{}(.|s_{t},}_{t}^{})\) conditions on current state \(s_{t}\) and predicted partner action distribution \(}_{t}^{}\)which enables the ego agent to analyze and adapt to its partner, while a random action distribution \(}_{t}^{e}\) is fed into the partner policy as the predicted ego agent policy. The random action distribution breaks symmetries and conventions commonly seen in self-play agents and introduces randomness into the partner policy, which in turn improves partner diversity. Moreover, this is consistent with the fact that some humans may not take their AI partners' actions into consideration, but the AI agents should adapt to their human partners.

## 5 Experiments

In this section, we evaluate E3T on zero-shot collaborating with behavior-cloned human proxies, real humans, and AI baselines. More details of the experiment setting and additional results can be found in Appendix.

### Environments

We evaluate the zero-shot coordination performance of E3T on a \(100 100\) cooperative matrix game [21; 42] and the Overcooked environment . Overcooked has a discrete action space with \(6\) actions, and we conduct experiments on \(5\) different layouts that pose various coordination challenges, such as avoiding blocking each other and preparing the required ingredients for a soup together. In addition, the state spaces of \(5\) layouts are different as shown in Table 1. For instance, the state space of _Counter Circuit_ reaches up to \(5.8 10^{16}\). This statistic indicates the \(5\) layouts are challenging for explorations. We have conducted additional experiments on the Google Football  environment's "3 vs 1 with Keeper" layout, which is a three-player cooperative game with a discrete action space comprising 19 actions. More details about the environment descriptions can be found in Appendix.

### Baselines

We compare proposed E3T with \(6\) baselines: Self-Play (SP) , Population-Based Training (PBT) [16; 8], Ficious Co-Play (FCP) , Trajectory Diversity (TrajeDi) , Maximum-Entropy Population-based training (MEP)  and Other-Play (OP) . All methods, except SP, OP, and E3T, fall under the category of population-based methods, which aim to develop a robust ego policy through interaction with a partner population.Detailed descriptions of baselines and the network details of E3T can be found in Appendix.

  Layout & Cramped Rm. & Asymm. Adv. & Coord. Ring & Forced Coord. & Counter Circ. \\  State Space & \(5.3 10^{7}\) & \(1.2 10^{14}\) & \(6.8 10^{10}\) & \(2.0 10^{9}\) & \(5.8 10^{16}\) \\  

Table 1: The cardinality of the state space of \(5\) layouts. The state space’s cardinality of _Counter Circuit_ is up to \(5.8 10^{16}\).

  Baseline & SP & PBT & FCP & MEP & E3T \\  Training Time(h) & 0.5 & 2.7 & 7.6 & 17.9 & 1.9 \\ Average Reward & \(65.8 5.5\) & \(66.8 5.0\) & \(71.7 3.7\) & \(89.7 7.1\) & \(\) \\  

Table 2: Comparisons of baselines in terms of training time and coordination performance with behaviour-cloned human proxies. The training time represents the hours required for training one model on the layout _Cramped Rm_ (training time on other layouts is similar). We report the average and the standard error of coordination rewards over \(5\) random seeds.

Figure 3: (a) We report the cross-play returns between 10 independently trained models for each baseline. (b) These results show the normalized coordination rewards between baselines averaged over 5 layouts.

### Result Analysis

We compare E3T with other baselines on zero-shot collaboration with the human proxy, real humans, and AI baselines.

Zero-shot Coordination on \(100 100\) matrix gameFigure 3(a) presents the cross-play results of \(10\) independently trained policies for each baseline on a one-step matrix game. The cross-play result measures the coordination performance between models that belong to the same baseline but have different random seeds. In this game, two players select a row and a column independently, and the reward is determined by the intersection of their choices. As shown in Figure 3(a), E3T and MEP can converge to the optimal strategies with the largest return as \(1\), while E3T can converge faster than MEP. These results indicate that the mixture partner policy can enable sufficient exploration and avoid getting stuck in local optima that generalize poorly on the game with large action space. More details of the matrix game are given in Appendix.

Training Efficiency AnalysisTable 2 compares the training time and average rewards of different baselines, where the rewards are calculated by averaging over \(5\) layouts on collaboration with human proxies. These results suggest that E3T outperforms existing baselines on collaboration with human proxies, and E3T is more efficient than population-based methods. For instance, it only spends \(10.6\%\) training time of previous state-of-the-art MEP.

AI-AI CoordinationThe zero-shot coordination performance between AI baselines are shown in Figure 3(b), where each element represents the normalized cooperation rewards averaged over 5 layouts for a pair of baselines. The performance of baselines cooperating with themselves is measured by the cross-play metric. As shown in Figure 3(b), E3T outperforms other baselines in coordinating with AI models. In addition, SP and PBT, which train policies only based on rewards and ignore diversity, have poor generalization to unseen AI partners. MEP and FCP, which train the ego agent by best responding to a pre-trained diverse partner population, perform better than SP and PBT, but worse than E3T. These results indicate that partner diversity is essential for zero-shot coordination, and E3T learns a robust ego policy by providing sufficient partner diversity and being equipped with adaptation ability. More results of AI-AI coordination on \(5\) layouts can be found in Appendix.

Human Proxy-AI CoordinationFigure 4(a) shows the results of baselines cooperating with behavior-cloned human proxies. These results suggest that E3T consistently outperforms existing baselines on all layouts. In addition, FCP, MEP, and TrajeDi have gained better or similar generalization performance than SP and PBT, owing to training an ego agent based on a diverse partner policy. These results indicate that the diversity of partners is important for the zero-shot coordination task. Note that SP achieves comparable performance to population-based MEP on _Forced Coord_. and E3T achieves the best coordination performance at parameter \(=0.0\). One possible reason is that the _Forced Coord_. layout is a difficult layout with a very narrow activity range and two agents being forced to coordinate to complete a soup. Therefore, it requires more cooperation than exploration. Moreover, E3T with \(=0.0\) is better than SP, indicating that the partner modeling module is beneficial to adapting to an unseen partner.

Human-AI CoordinationThe average rewards over 5 layouts of AI baselines coordinating with real humans are shown in Figure 4(b). We follow the Human-AI coordination testing protocol

Figure 4: (a) We plot the mean and standard error of coordination rewards over \(5\) random seeds. The parameter \(\) is set to \(0.5\) for all layouts except _Forced Coord_., where \(\) is settoas \(0.0\). In this layout, the ego agent does not need too much exploration due to the narrow active range. (b) We plot the mean and standard error of humans collaborating with baselines over all layouts. These results show that E3T outperforms all other baselines when collaborating with real humans.

proposed in  and recruit \(20\) individuals to evaluate the Human-AI coordination ability of E3T and state-of-the-art MEP, where individuals play a total of \(600\) rounds with AI. For cost and time consideration, we reuse the Human-AI coordination results of other baselines reported in . Figure 4(b) shows that E3T outperforms all other baselines when zero-shot coordinating with real humans. We also collect individuals' subjective assessment of E3T and MEP about the degree of intelligence, collaborative ability, and preference between the two. More details of the Human-AI coordination setting and individuals' subjective assessment are given in Appendix.

### Ablation Study

Analysis of Component Ablation ResultsWe present the ablation results of E3T's components in Figure 5(a). We evaluate the effectiveness of each component on collaborating with AI baselines (including MEP, FCP, and PBT). We use (+R) and (+P) to denote models incorporated with the mixture partner policy and the partner modeling module, respectively. As shown in Figure 5(a), E3T(+P) achieves a higher average reward than SP on each layout, which shows that the partner modeling module can improve zero-shot coordination by updating actions accordingly. Likewise, E3T(+R) surpasses SP on \(4\) layouts except for the _Forced Coord._, which shows that encouraging more exploration in the partner policy can discover more coordination patterns during training and thus can enhance zero-shot coordination with unseen partners during testing. The lower performance on _Forced Coord._ may be due to its narrow action space and the lack of reward incentives for coordination when using a too random partner policy during training. Moreover, E3T(R+P) that combines both the mixture partner policy and the partner modeling module performs better than E3T(R) and E3T(P) on the average reward over all layouts.

Analyzing the effect of the hyper-parameter \(\)Figure 5(b) shows how the balance parameter \(\) of the mixture partner affects the performance of E3T when collaborating with AI baselines and the human proxy. We observe that the coordination rewards of E3T increase and then decrease as \(\) increases for both types of partners. The results indicate that both collaboration ability and partner diversity are essential for zero-shot coordination, but simply increasing the partner action diversity is not enough. The optimal values of \(\) are 0.3 and 0.5 for AI baselines and human proxy, respectively. One possible reason for this discrepancy is that the human proxy tends to generate more 'noop' actions , which reduces the need for coordination behaviors compared to AI baselines.

Figure 5: (a) Ablation of E3T’s components on collaborating with AI baselines. E3T(+R) and E3T(+P) respectively integrate the mixture partner and the partner modeling module into the self-play framework. The balance parameter \(\) is set to \(0.3\) for E3T(+R). (b) Results on E3T with different balance parameters \(\). We plot the average rewards over 5 random seeds when E3T collaborating with AI (MEP, PBT) baselines and human proxy on layout _Coord. Ring_. The blue bar shows performance of E3T cooperating with AI baselines, while the orange and the green bars show its performance when cooperating with the human proxy, with starting positions switched.

Figure 6: Relationship between the accuracy of partner action prediction and coordination performance when collaborating with the human proxy on layout _Coord. Ring_. We plot the rewards and prediction accuracy obtained by coordinating with the human proxy at different checkpoints along the training process.

Analyzing the effect of partner modeling moduleTo demonstrate the effect of the proposed partner modeling module, we plot the accuracy of predicting partner actions and the rewards when cooperating with a human proxy. As shown in Figure 6, the reward and the prediction accuracy show the same tendency: better partner modeling leads to improvement in coordination. In addition, the accuracy at \(=0.5\) reaches around \(55\%\), which is higher than the accuracy around \(40\%\) when \(=0.0\). This indicates that mixing a random policy can provide more exploration and thus generalize well to unseen partners.

Does the mixture partner policy accommodate the multi-player game?We verify the effect of the mixture partner policy on improving the zero-shot coordination performance on a three-player Google Football game, the 3 vs 1 with Keeper scenario. We train self-play policies via the Multi-Agent PPO (MAPPO) , where policies have shared parameters. E3T(mixture policy) integrates the mixture partner policy into the self-play training process, in detail, the three agents randomly sample an action with a probability as \(\), otherwise, take actions according to the action policies. We independently train \(3\) policies with different random seeds for each baselines and report the cross-play results between unseen policies to test the zero-shot coordination ability of these models.

Table 3 shows that self-play policies fail to zero-shot cooperate with policies from different training seeds, because they may fall into some specific coordination conventions during training. Our proposed mixture partner policy can increase the behavioral diversity of the partners, allowing the ego policy to encounter different coordination patterns during training. Consequently, our method with the mixture partner policy can train robust ego policies that can adapt well to policies independently trained from different training seeds. Moreover, increasing partner's behavior diversity during training can significantly improve the generalization ability of the trained ego policy.

## 6 Conclusion

In this paper, we propose an **E**fficient **E**nd-to-**E**nd **T**raining approach for zero-shot human-AI coordination, called E3T. Different from previous population-based methods, E3T employs a mixture policy of ego policy and random policy to construct the partner policy, enabling the partner policy to be both coordination-skilled and diverse. In this way, the ego-agent is able to be end-to-end trained in such a mixture policy without pre-training a population, thus significantly improving the training efficiency. Additionally, we introduce a partner modeling module that allows the ego agent to predict the partner's next action, enabling adaptive collaboration with partners exhibiting different behavior patterns. Empirical results on Overcooked clearly demonstrate that E3T can significantly improve training efficiency while achieving comparable or superior performance compared to existing methods.

Limitations and Future WorkIn this paper, we only explore the feasibility of our framework in zero-shot two-player human-AI coordination tasks but neglect general multi-player (more than two) human-AI coordination tasks. Future research will focus on expanding the framework to multi-player (more than two) human-AI coordination tasks, with a goal of enhancing both zero-shot coordination performance and training efficiency. Since AI agents are unable to regulate human behavior, indiscriminately collaborating with humans in accomplishing their goals could result in unlawful circumstances. Therefore, future research should concentrate on developing AI agents capable of detecting dangerous intentions and preventing their misuse for illicit or military purposes.

   & Training Performance & Zero-shot (Mean) & Zero-shot (Max) \\  Self-play & \(0.79 0.08\) & \(0.07 0.08\) & \(0.24\) \\ E3T(mixture policy) & \(0.76 0.09\) & \(0.49 0.21\) & \(0.78\) \\  

Table 3: We present the coordination performance of baselines during training and zero-shot testing on a three-player Google Football game, where the coordination performance is measured by the winning rates of scoring a goal. We independently train \(3\) policies with different random seeds for each baseline. For zero-shot testing, we consider all \(6\) possible permutations of the \(3\) trained policies and report the mean and standard error of the winning rates as Zero-shot (Mean). We also report the highest winning rate among the permutations as Zero-shot (Max).