# TabMT: Generating Tabular data with Masked Transformers

Manbir S. Gulati

AI Accelerator

Leidos Inc

Manbir.S.Gulati@leidos.com

Paul F. Roysdon

AI Accelerator

Leidos Inc

Paul.Roysdon@leidos.com

###### Abstract

Autoregressive and Masked Transformers are incredibly effective as generative models and classifiers. While these models are most prevalent in NLP, they also exhibit strong performance in other domains, such as vision. This work contributes to the exploration of transformer-based models in synthetic data generation for diverse application domains. In this paper, we present TabMT, a novel Masked Transformer design for generating synthetic tabular data. TabMT effectively addresses the unique challenges posed by heterogeneous data fields and is natively able to handle missing data. Our design leverages improved masking techniques to allow for generation and demonstrates state-of-the-art performance from extremely small to extremely large tabular datasets. We evaluate TabMT for privacy-focused applications and find that it is able to generate high quality data with superior privacy tradeoffs.

## 1 Introduction

Generative models have attracted significant attention in the field of deep learning due to their ability to synthesize high-quality data and learn the underlying structure of complex datasets. Such models have been successfully applied to various data types including images , text, and tabular data . This work concentrates on tabular data, which is prevalent in numerous fields like healthcare, finance, and social sciences. The heterogeneous nature of tabular data, characterized by its diverse data types, distributions, and relationships, presents distinct challenges not present in other domains.

The development of effective synthetic tabular data generators is crucial for numerous reasons including: privacy preservation, data augmentation, model interpretability, and anomaly detection. Prior work in this domain has produced a myriad of generative models, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Autoregressive Transformer, and Diffusion models. Although these existing models strive to address the challenges associated with tabular data generation, there is still room for exploration and improvement. Specifically, we demonstrate improvements in robustness, scalability, privacy preservation, and handling of missing data.

Transformers , originally designed for natural language processing (NLP) tasks, have lead to significant advancements in a variety of applications. Their powerful capacity for modeling complex dependencies and

Figure 1: Diagram of TabMT. \(m\) is the mask token, \(p_{i}\) is the masking probability of the \(i^{th}\) row

[MISSING_PAGE_FAIL:2]

3. Missing data is far more common in tabular data than in other domains. TabMT is able to learn missing values by setting their masking probability to 1. Other generators require that we impute data separately before we can generate high-quality cleaned samples.

These structural advantages come from TabMT's novel masked generation procedure.

Below we outline how we construct TabMT from the original masked training procedure outlined in BERT and the justifications behind our design choices. We also outline the specific changes we make to allow for heterogeneous data types. A naive adaptation of BERT's masking procedure would look as follows. Given an \(n\) by \(l\) dataset \(\) of categorical and numerical features, for each row \(_{i}\), the transformer is provided with a set of unmasked fields \(_{i}^{u}\) and a set of masked fields \(_{i}^{m}\). Each field in the masked set \(_{i}^{m}\) has its value replaced with a mask token. The model is then tasked with predicting the original value for all masked tokens. The row \(_{i}\) is partitioned into the unmasked and masked sets by conducting a Bernoulli trial on each field, \(_{i,j}\), such that \(P(_{i,j}_{i}^{m})=0.15\).

The BERT masking procedure produces a strong embedding model, but not a strong generator. To create a strong generative model we make two key changes: sample our masking probability from a uniform distribution and predict masked values in a random order during generation. To understand why these changes are effective, we can look at the distribution of masked sets. As a result of the repeated Bernoulli trials during masking, the size of the masked set for each row \(|_{i}^{m}|\) will follow a Binomial distribution. However, when generating data one field \(_{i,j}\) at a time, the model will inference on masked subset sizes \((0,,l-1)\), once each. We would like the training distribution of \(|_{i}^{m}|\) to be uniform, matching the uniform distribution encountered when generating data. With a fixed masking probability \(P(_{i,j}_{i}^{m})=p_{m}\) we will instead encounter a Binomial distribution centered around \(p_{m} l\). However, if we sample our masking probability \(p_{m}\) for each row \(_{i}\) such that \(P(p_{m}=p) U(0,1)\), we will train uniformly across subset sizes:

\[P(|_{i}^{m}|=k)=_{0}^{1}{l k}p^{k}(1-p)^{l-k}dp= {k!(l-k)!}=.\] (1)

Fixing this train and inference mismatch is critical to forming a strong generator. A traditional autoregressive generator would generate fields \((_{i,0},,_{i,l-1})\), sequentially. However, tabular data, unlike language, does not have an inherent ordering. Generating fields in a fixed order introduces another mismatch between training and inference. During training \(_{i}^{m}\) will take on the distribution

\[P(_{i}^{m}=s)=} l}.\] (2)

When generating in a fixed order, the model will infer across \(l\) distinct subsets and no others. However, if we instead infer in a random order, then at generation step \(0 t<l\), the distribution of \(_{i}^{m}\) will be given by

\[P(_{i}^{m}=s)==}.\] (3)

Since we encounter each \(t\) exactly once, this overall distribution is identical to the masking distribution encountered during training, fixing the discrepancy caused by generating fields in a fixed order.

A transformer model will typically have an input embedding matrix \(^{k d}\), where \(k\) is the number of unique input tokens and \(d\) is the transformer width. Because tabular data is heterogeneous, we instead construct \(l\) embedding matrices, one for each field. Each embedding matrix will have a different number of unique tokens \(k\).

Figure 2: A row of data being sampled from TabMT. Fields are sampled in a random order and field values are sampled according to the predicted distributed.

For categorical fields we use a standard embedding matrix initialized with a normal distribution. For each continuous field we construct an ordered embedding \(^{k d}\) from its unordered embedding matrix \(\) and two endpoint vectors \(,^{d}\).

To construct each ordered embedding matrix \(\), we first we quantize the values of the continuous field. Our default quantizer is K-Means. We consider the maximum number of clusters a hyper-parameter. Let \(^{k}\) be the vector of ordered cluster centers. We construct an vector of ratios \(^{k}\) using min max normalization such that

\[_{i}=_{i}-()}{()-( )}.\] (4)

We use the ratio vector \(\) to construct each ordered embedding in \(\):

\[_{i}=_{i}+_{i}+(1-_{i} ).\] (5)

This structure allows the transformer to both take advantage of the ordering of the properties and add unordered embedding information to each cluster. The unordered embeddings are useful in attention, multi-modal distributions, and encoding semantic separation between close values. We use this same structure to construct a dynamic linear layer at the output during prediction. This can be converted to a traditional linear layer once the model is trained.

Relying too heavily on the unordered embeddings might negate the benefit of our ordered embedding, as information isn't effectively shared between close values. To address this, we bias TabMT to rely on the ordering of embeddings. For continuous fields, we zero-init the unordered embedding matrix \(\). Whereas, the endpoint vectors \(\) and \(\) use a normal distribution with 0.05 standard deviation. Because entries in matrix \(\) are not independent of each other, to sharpen the output distribution, the network must either rely on matrix \(\) or increase magnitude of the endpoint vectors. This can reduce use of the priors encoded by \(\) or cause instability. To combat this, we include a learned temperature which can sharpen the predicted distribution using a single parameter per field instead. Each field's predicted distribution \(}^{k}\) is given by

\[}=/(_{l}_{u})}}{_{j}e^{ /(_{l}_{u})}},\] (6)

where \(^{k}\) is a vector of logits, \(_{l}\) is the learned temperature, and \(_{u}\) is the user-defined temperature. See Figure 1 for an overall diagram of these components. Figure 2 shows the generation of a single sample. Detailed pseudocode is available in the Appendix.

## 4 Evaluation

In this section, we present a comprehensive evaluation of TabMT's effectiveness across an extensive range of tabular datasets. Our analysis involves a thorough comparison with state-of-the-art approaches, encompassing nearly all generative model families. To ensure a robust assessment, we evaluate across several dimensions and metrics.

Datasets:For our data quality and privacy experiments we use the same list of datasets and data splits as TabDDPM. These 15 datasets range in size from \( 400\) samples to \( 150,000\) samples. They contain continuous, categorical, and integer features. The datasets range from 6 to 50 columns. For our scaling experiments we use the CIDDS-001 dataset, which consists of Netflow traffic from a simulated small business network. A Netflow consists of 12 attributes, which we post-process into 16 attributes; see the Appendix for more details. This dataset is extremely large with over 30 million rows and field cardinalities in the tens of thousands. Other datasets listed all have cardinalities below 50. Unlike our other benchmarks, we purposely do not quantize the continuous variables here to further test the scaling of our model. In other words, every unique value is treated as a separate category in our prediction process.

Prior Methods:We select four techniques to compare against, one from each each major family of deep generative models.

* **TVAE** is one of the first deep tabular generation papers introducing two models, a GAN and a VAE. We compare against their VAE because it is the strongest VAE tabular generator we are aware of.

[MISSING_PAGE_FAIL:5]

### Privacy and Sample Novelty

Maintaining privacy of the original data is a key application for synthetic tabular data. Machine learning is increasingly being used across a wide range of areas to produce valuable insights. Simultaneously, there is a rapid rise in both regulation and privacy concerns that need to be addressed. In Section 4.1 we demonstrated that data produced by TabMT is high enough quality to produce strong classifiers. Now we evaluate our model for privacy. This evaluation complements our quality evaluation and verifies that our model is generating novel data. Novelty means data is not substantially similar to samples encountered during training. A high-quality non-private model can trivially be formed by directly reproducing the training set exactly. None of this data is novel, but it is high quality. By ensuring our model is both private and high quality, we verify that our model has learned the distribution of the data, and not simply memorized the training set. Memorization is a larger issue in tabular data due to smaller dataset sizes and increased privacy concerns.

To evaluate privacy and novelty we adopt the median Distance to the Closest Record (DCR) score. To calculate the DCR of a synthetic sample, we find the nearest neighbor in the real training set by Euclidean distance. We report the median of this distance across our synthetic samples. Data with higher DCR scores will be more private and more novel. There is an inherent trade off between privacy and quality. Higher quality samples will tend to be closer to points in the training set and vice versa.

While models such as CTabGAN+ and TabDDPM have a fixed trade-off between privacy and quality after training. TabMT can trade-off between quality and privacy using temperature scaling. By walking along the Pareto curve of our model, using temperature scaling, we can controllably tune the privacy and novelty of our generated data per application. By increasing a field's temperature, its generated values become more novel and private, but they are also less faithful to the underlying data distribution. The trade off between the quality and privacy here form a Pareto front for TabMT on each dataset.

We use a separate temperature for each field and perform a search to estimate the Pareto front. Each search was conducted using a single A10 GPU each. Search details are available in the Appendix. In Table 2, we compare TabMT's DCR and corresponding MLE scores to that of TabDDPM. We are always able to attain a higher DCR score, and in most cases a higher MLE score as well. This falls in line with recent results in other domains showing diffusion models are less private than other generative models. A comparison with CTabGAN+ is available in the Appendix, compared to CTabGAN+ we obtain both higher privacy and MLE scores in all tested cases. Figure 4 shows the Pareto fronts of TabMT across several datasets.

### Missing Data

Real world data is often missing many values that make training difficult. When a row has a missing value we must either drop the row, or find a method to impute the missing value. Other techniques such as the RealTabformer or TabDDPM cannot natively handle real world missing data, and must either use a different imputation technique or drop the corresponding rows. Our masking procedure allows TabMT to natively handle arbitrary missing data. To demonstrate this, we randomly drop 25% of values from the dataset, ensuring nearly every row is permanently missing data. Nevertheless, our

Figure 3: A comparison of Correlation Error Histograms between TabMT (Green) vs TabDDPM (Purple) and TabMT (Green) vs CTabGAN+ (Purple). A good generator should have correlation errors distributed close to zero (the left of the plot). We can see TabMT’s correlation errors are consistently distributed closer to zero than either TabDDPM or CTabGAN+.

model is still able to train, producing synthetic rows with no missing values in them. This facilitates training on real world data.Table 3 shows our accuracy when training with missing data.

Additionally, our model can be arbitrarily conditioned to produce any subset of the data distribution at no additional cost, allowing us to more effectively augment underrepresented portions of data. Prior art is largely incapable of conditioning when producing outputs.

### Scaling

In this section, we examine the performance of our model when scaling to very large datasets. We use the CIDDS-001 dataset as our benchmark dataset. We do not use anomalous traffic from the dataset, and randomly select 5% of the dataset as the validation set for reporting results. The results

   DS & TabDDPM & **TabMT** \\  AB & 0.050(0.550) & **0.249**(0.533) \\ AD & 0.104(0.795) & **1.01**(0.811) \\ BU & 0.143(0.906) & **0.165**(0.908) \\ CA & 0.041(0.836) & **0.117**(0.832) \\ CAR & 0.012(0.737) & **0.041**(0.737) \\ CH & 0.157(0.755) & **0.281**(0.758) \\ DI & 0.204(0.740) & **0.243**(0.740) \\ FB & 0.112(0.713) & **0.252**(0.787) \\    
   DS & TabDDPM & **TabMT** \\  GE & 0.059(0.597) & **0.234**(0.599) \\ HI & 0.449(0.722) & **0.483**(0.727) \\ HO & 0.086(0.677) & **0.151**(0.607) \\ IN & 0.041(0.809) & **0.061**(0.816) \\ KI & 0.189(0.833) & **0.335**(0.868) \\ MI & 0.022(0.936) & **0.026**(0.936) \\ WI & 0.016(0.904) & **0.063**(0.881) \\   

Table 2: DCR score comparison between TabDDPM and TabMT. Corresponding MLE scores are in parentheses.

Figure 4: Pareto Fronts of TabMT balancing the tradeoff between privacy (DCR) and data quality (Validation Score). While some datasets have a smooth transition the temperature changes, others have a sharp drop-off.

[MISSING_PAGE_FAIL:8]

Broader ImpactSynthetic data generation allows for privacy preservation, protecting sensitive data while still enabling data analysis. High Quality synthetic data may ease the pressure to resort to unethical methods of collection such as relying on underpaid labor. With this in mind, trading off data for additional compute does mean that the additional compute will contribute to increased CO2 emissions. Additionally, synthetic data carries the risk of misuse, such as the potential for manipulating results or research findings with fabricated data. All experiments were conducted using cloud A10 or V100 GPUs. For algorithm design and experiment result generation roughly 410 GPU days of compute were used.

## 6 Conclusion

In this paper, we outlined a novel Masked Transformer design and training procedure, TabMT, for generating synthetic tabular data. Through a comprehensive series of benchmarks we demonstrate that our model achieves state-of-the-art generation quality. This quality is verified at scales that are orders of magnitude larger than prior work and with missing data present. Our model achieves superior privacy and is able to easily trade off between privacy and quality. Our model is a substantial advancement compared to previous work, due to its scalability, missing data robustness, privacy-preserving generation, and superior data quality.