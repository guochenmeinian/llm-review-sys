# White-Box Transformers via Sparse Rate Reduction

Yaodong Yu\({}^{1}\) Sam Buchanan\({}^{2}\) Druv Pai\({}^{1}\) Tianzhe Chu\({}^{1}\) Ziyang Wu\({}^{1}\) Shengbang Tong\({}^{1}\)

Benjamin D. Haeffele\({}^{3}\) Yi Ma\({}^{1}\)

\({}^{1}\)University of California, Berkeley \({}^{2}\)TTIC \({}^{3}\)Johns Hopkins University

###### Abstract

In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called _sparse rate reduction_. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of _white-box_ transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at https://github.com/Ma-Lab-Berkeley/CRATE.

## 1 Introduction

In recent years, deep learning has seen tremendous empirical success in processing massive amounts of high-dimensional and multi-modal data. Much of this success is owed to effective learning of the data distribution and then transforming the distribution to a parsimonious, i.e. _structured and compact_, representation , which facilitates many downstream tasks (e.g., in vision, classification , recognition and segmentation , and generation ). To this end, many models and methods have been proposed and practiced, each with its own strengths and limitations. Here, we give several popular methods a brief accounting as context for a complete understanding and unification that we seek in this work.

Transformer models and self-attention.Transformers  are one of the latest popular models for learning a representation for high-dimensional structured data, such as text , images , and other types of signals . After the first block, which converts each data point (such as a text corpus or image) into a set or sequence of _tokens_, further processing is performed on the token sets, in a medium-agnostic manner . A cornerstone of the transformer model is the so-called _self-attention layer_, which exploits the statistical correlations among the sequence of tokens to refine the token representation. Transformers have been highly successful in learning compact representations that perform well on many downstream tasks. Yet the transformer networkarchitecture is empirically designed and lacks a rigorous mathematical interpretation. In fact, the output of the attention layer itself has several competing interpretations [68; 78]. As a result, the statistical and geometric relationship between the data distribution and the final representation learned by a transformer largely remains a mysterious black box.

Diffusion models and denoising.Diffusion models [22; 34; 41; 43; 44] have recently become a popular method for learning the data distribution, particularly for generative tasks and natural image data which are highly structured but notoriously difficult to effectively model [3; 5]. The core concept of diffusion models is to start with features sampled from a Gaussian noise distribution (or some other standard template) and _iteratively denoise_ and deform the feature distribution until it converges to the original data distribution. This process is computationally intractable if modeled in just one step , so it is typically broken into multiple incremental steps. The key to each step is the so-called _score function_, or equivalently  an estimate for the "optimal denoising function"; in practice this function is modeled using a generic black-box deep network. Diffusion models have shown effectiveness at learning and sampling from the data distribution [56; 60; 65]. However, despite some recent efforts , they generally do not establish any clear correspondence between the initial features and data samples. Hence, diffusion models themselves do not offer a parsimonious or interpretable representation of the data distribution.

Structure-seeking models and rate reduction.In both of the previous two methods, the representations were constructed implicitly as a byproduct of solving a downstream task (e.g., classification or generation/sampling) using deep networks. However, one can also explicitly learn a representation of the data distribution as a task in and of itself; this is most commonly done by trying to identify and represent low-dimensional structures in the input data. Classical examples of this paradigm include model-based approaches such as sparse coding [2; 29] and dictionary learning [17; 21; 47], out of which grew early attempts at designing and interpreting deep network architectures [18; 32]. More recent approaches build instead from a model-free perspective, where one learns a representation through a sufficiently-informative pretext task (such as compressing similar and separating dissimilar data in contrastive learning [45; 69; 80], or maximizing the information gain in the class of maximal coding rate reduction methods [6; 46; 55]). Compared to black-box deep learning approaches, both model-based and model-free representation learning schemes have the advantage of being more interpretable: they allow users to explicitly design desired properties of the learned representation [46; 55; 63]. Furthermore, they allow users to construct new white-box forward-constructed deep network architectures [11; 55; 59] by _unrolling the optimization strategy for the representation learning objective_, such that each layer of the constructed network implements an iteration of the optimization algorithm [11; 53; 55]. Several recent works [71; 74; 76] consider the connections between transformer architectures  and unrolled optimization. Unfortunately, in this paradigm, if the desired properties are narrowly defined, it may be difficult to achieve good practical performance on large real-world datasets.

Our contributions, and outline of this work.In this work, we aim to remedy the limitations of these existing methods with a more unified framework for designing transformer-like network architectures that leads to both mathematical interpretability and good practical performance. To this end, we propose to learn a sequence of _incremental mappings_ to obtain a most _compressed and sparse_ representation for the input data (or their token sets) that optimizes _a unified objective function_ known as the sparse rate reduction, specified later in (1). The goal of the mapping is illustrated in Figure 1. Within this framework, we unify the above three seemingly disparate approaches and show that _transformer-like deep network layers can be naturally derived from unrolling iterative

Figure 1: The ‘main loop’ of the crate white-box deep network design. After encoding input data \(\) as a sequence of tokens \(^{0}\), crate constructs a deep network that transforms the data to a canonical configuration of low-dimensional subspaces by successive _compression_ against a local model for the distribution, generating \(^{+1/2}\), and _sparsification_ against a global dictionary, generating \(^{+1}\). Repeatedly stacking these blocks and training the model parameters via backpropagation yields a powerful and interpretable representation of the data.

optimization schemes to incrementally optimize the sparse rate reduction objective._ In particular, our contributions and outline of the paper are as follows:

* In Section 2.2 we show, using an idealized model for the token distribution, that if one _iteratively denoises_ the tokens towards a family of low-dimensional subspaces, the associated score function assumes an explicit form similar to a self-attention operator seen in transformers.
* In Section 2.3 we derive the multi-head self-attention layer as an unrolled gradient descent step to minimize the lossy coding rate part of the rate reduction, showing another interpretation of the self-attention layer as compressing the token representation.
* In Section 2.4 we show that the multi-layer perceptron which immediately follows the multi-head self-attention in transformer blocks can be interpreted as (and replaced by) a layer which incrementally optimizes the remaining part of the sparse rate reduction objective by constructing a sparse coding of the token representations.
* In Section 2.5 we use this understanding to create a new white-box (fully mathematically interpretable) transformer architecture called crate (i.e., Coding RAte reduction TransformEr), where each layer performs a _single step_ of an alternating minimization algorithm to optimize the sparse rate reduction objective.

Hence, within our framework, the learning objective function, the deep learning architecture, and the final learned representation _all become white boxes_ that are fully mathematically interpretable. As the experiments in Section 3 show, the crate networks, despite being simple, can already learn the desired compressed and sparse representations on large-scale real-world datasets and achieve performance on par with much more heavily engineered transformer networks (such as ViT) on a wide variety of tasks (e.g., classification and transfer learning).

## 2 Technical Approach and Justification

### Objective and Approach

We consider a general learning setup associated with real-world signals. We have some random variable \(=[_{1},,_{N}]^{D N}\) which is our data source; each \(_{i}^{D}\) is interpreted as a _token1_, and the \(_{i}\)'s may have arbitrary correlation structures. We use \(=[_{1},,_{N}]^{d N}\) to denote the random variable which defines our representations. Each \(_{i}^{d}\) is the representation of the corresponding token \(_{i}\). We are given \(B 1\) i.i.d. samples \(_{1},,_{B}\), whose tokens are \(_{i,b}\). The representations of our samples are denoted \(_{1},,_{B}\), and those of our tokens are \(_{i,b}\). Finally, for a given network, we use \(^{}\) to denote the output of the first \(\) layers when given \(\) as input. Correspondingly, the sample outputs are \(_{i}^{}\) and the token outputs are \(_{i,b}^{}\).

Objective for learning a structured and compact representation.Following the framework of rate reduction , we contend that the goal of representation learning is to find a feature mapping \(f^{D N}^{d N}\) which transforms input data \(^{D N}\) with a potentially nonlinear and multi-modal distribution to a (piecewise) _linearized and compact_ feature representation \(^{d N}\). While the joint distribution of tokens \((_{i})_{i=1}^{N}\) in \(\) may be sophisticated (and task-specific), we further contend that it is reasonable and practical to require that the target marginal distribution of individual tokens \(_{i}\) should be highly compressed and structured, amenable for compact coding. Particularly, we require the distribution to be _a mixture of low-dimensional (say \(K\)) Gaussian distributions_, such that the \(k^{}\) Gaussian has mean \(^{d}\), covariance \(_{k}^{d d}\), and support spanned by the orthonormal basis \(_{k}^{d p}\). We denote \(_{[K]}=(_{k})_{k=1}^{K}\) to be the set of bases of all Gaussians. Hence to maximize the _information gain_ for the final token representation, we wish to maximize the rate reduction [6; 46] of the tokens, i.e., \(_{} R(;_{[K]})=R()-R^{c}(;_{[K]})\), where \(R\) and \(R^{c}\) are estimates of lossy coding rates to be formally defined in (7) and (8). This also promotes token representations \(_{i}\) from different Gaussians to be _incoherent_. Since rate reduction is an intrinsic measure of goodness for the representation, it is invariant to arbitrary rotations of the representations. Therefore, to ensure the final representations are amenable to more compact coding, we would like to transform the representations (and their supporting subspaces) so that they become _sparse_ with respect to the standard coordinates of the resulting representation space.2 The combined rate reduction and sparsification process is illustrated in Figure 1. Computationally, we may combine the above two goals into a unified objective for optimization:

\[_{f}_{} R(;_{[K]})- \|\|_{0}=_{f}_{}R( )-R^{c}(;_{[K]})-\|\|_{0}=f(),\] (1)

where the \(^{0}\) norm \(\|\|_{0}\) promotes the sparsity of the final token representations \(=f()\).3 We call this objective "_sparse rate reduction_."

White-box deep architecture as unrolled incremental optimization.Although easy to state, each term of the above objective can be computationally very challenging to optimize [55; 70]. Hence it is natural to take an approximation approach that realizes the global transformation \(f\) optimizing (1) through a concatenation of multiple, say \(L\), simple _incremental and local_ operations \(f^{}\) that push the representation distribution towards the desired parsimonious model distribution:

\[f}^{0}^{} }^{+1}^{L}=,\] (2)

where \(f^{0}:^{D}^{d}\) is the pre-processing mapping that transforms input tokens \(_{i}^{D}\) to their token representations \(_{i}^{1}^{d}\).

Each incremental _forward mapping_\(^{+1}=f^{}(^{})\), or a "layer", transforms the token distribution to _optimize_ the above sparse rate reduction objective (1), conditioned on the distribution of its input tokens \(^{}\). In contrast to other unrolled optimization approaches such as the ReduNet , we _explicitly model_ the distribution of \(^{}\) at each layer, say as a mixture of linear subspaces or sparsely generated from a dictionary. The model parameters are learned from data (say via _backward propagation_ with end-to-end training). This separation of forward "optimization" and backward "learning" clarifies the mathematical role of each layer as an operator transforming the distribution of its input, whereas the input distribution is in turn modeled (and subsequently learned) by the parameters of the layer.

We show that we can derive these incremental, local operations through an unrolled optimization perspective to achieve (1) through Sections 2.3 to 2.5. Once we decide on using an incremental approach to optimizing (1), there are a variety of possible choices to achieve the optimization. Given a model for \(}^{}\), say a mixture of subspaces \(_{[K]}\), we opt for a two-step _alternating minimization_ process with a strong conceptual basis: first in Section 2.3, we _compress_ the tokens \(^{}\) via a gradient step to minimize the coding rate term \(_{}R^{c}(;_{[K]})\); second, in Section 2.4, we _sparsify_ the compressed tokens, with a suitably-relaxed proximal gradient step on the difference of the sparsity penalty and the expansion term, i.e., \(_{}[\|\|_{0}-R()]\). Both actions are applied incrementally and repeatedly, as each \(f^{}\) in (2) is instantiated with these two steps.

### Self-Attention via Denoising Tokens Towards Multiple Subspaces

There are many different ways to optimize the objective (1) incrementally. In this work, we propose arguably _the most basic_ scheme. To help clarify the intuition behind our derivation and approximation, in this section (and Appendix A.1) we study a largely idealized model which nevertheless captures the essence of nearly the whole process and particularly reveals the reason why self-attention-like operators arise in many contexts. Assume that \(N=1\), and the single token \(\) is drawn i.i.d. from an unknown mixture of Gaussians \(((,_{k}))_{k=1}^{K}\) supported on low-dimensional subspaces with orthonormal bases \(_{[K]}=(_{k})_{k=1}^{K}\) and corrupted with additive Gaussian noise \((,)\), i.e.,

\[=+,\] (3)

where \(\) is distributed according to the mixture. Our goal is simply to transform the distribution of the noisy token \(\) to the mixture of low-dimensional Gaussians \(\). Towards incremental construction of a representation \(f\) for this model following (2), we reason inductively: if \(^{}\) is a noisy token (3) at noise level \(^{}\), it is natural to produce \(^{+1}\) by denoising at the level \(^{}\). In the mean-square sense, the optimal estimate is \([^{}]\), which has a variational characterization (e.g. ):

\[[]=*{arg\,min}_{f}\ _{,}f(+^{})- _{2}^{2}.\] (4)Setting \(^{+1}=[^{}]\), (4) thus characterizes the next stage of (2) in terms of an optimization objective based on a _local signal model_ for \(^{}\). Moreover, letting \( q^{}()\) denote the density of \(^{}\), Tweedie's formula  allows us to express the optimal representation solving (4) in closed-form:

\[^{+1}=^{}+(^{})^{2}_{} q^{} (^{}).\] (5)

Tweedie's formula expresses the optimal representation in terms of an additive correction (in general a nonlinear function of \(^{}\)) to the noisy observations by the gradient of the _log-likelihood_ of the distribution of the noisy observations, giving the optimal representation a clear interpretation as an incremental perturbation to the current noisy distribution \(q^{}\). This connection is well-known in the areas of estimation theory and inverse problems , and more recently has found powerful applications in the training of generative models for natural images . Here, we can calculate a closed-form expression for this _score function_\(_{} q^{}\), which, when combined with (5) and some technical assumptions4, gives the following approximation (shown in Appendix A.1). Let \(\) denote the Kronecker product; then we have

\[^{+1}[_{1},,_{K}][\!( \!()^{2}}\| _{1}^{*}^{}\|_{2}^{2}\\ \\ \|_{K}^{*}^{}\|_{2}^{2})) _{p}]_{1}^{*}^{}\\ \\ _{K}^{*}^{},\] (6)

This operation resembles a self-attention layer in a standard transformer architecture with \(K\) heads, sequence length \(N=1\), the "query-key-value" constructs being replaced by a single linear projection \(_{k}^{*}^{}\) of the token \(^{}\), and the aggregation of head outputs (conventionally modeled by an MLP) done with the two leftmost matrices in (6). We thus derive the following useful interpretation, which we will exploit in the sequel: _Gaussian denoising against a mixture of subspaces model leads to self-attention-type layers in the transformation \(f\)_. Given an initial sample \(\) following the model (3), we can repeatedly apply local transformations to the distribution with (6) in order to realize the incremental mapping \(f\) in (2).5 These insights will guide us in the design of our white-box transformer architecture in the upcoming subsections.

### Self-Attention via Compressing Token Sets through Optimizing Rate Reduction

In the last subsection, we have seen that the multi-head attention in a transformer resembles the score-matching operator that aims to transform a token \(^{}\) towards a mixture of subspaces (or degenerate Gaussians). Nevertheless, to carry out such an operation on any data, one needs to first learn or estimate, typically from finite samples, the parameters of the mixture of (degenerate) Gaussians, which is known to be a challenging task . This challenge is made even harder because in a typical learning setting, the given set of tokens are _not_ i.i.d. samples from the mixture of subspaces. The joint distribution among these tokens can encode rich information about the data--for example, co-occurrences between words or object parts in language and image data (resp.)--which we should also learn. Thus, we should compress / denoise / transform such a set of tokens together. To this end, we need a measure of quality, i.e., compactness, for the resulting representation of the set of tokens. A natural measure of the compactness of such a set of tokens is the (lossy) coding rate to encode them up to a certain precision \(>0\). For a zero-mean Gaussian, this measure takes a closed form. If we view the tokens in \(^{d N}\) as drawn from a single zero-mean Gaussian, an estimate of their (lossy) coding rate, subject to quantization precision \(>0\), is given in  as:

\[R()\!\!(+} {Z}^{*})=\!\!(+}^{*})\!.\] (7)

In practice, the data distribution is typically multi-modal, say an image set consisting of many classes or a collection of image patches as in Figure 1. It is more appropriate to require that the set of tokens map to a mixture of, say \(K\), subspaces (degenerate Gaussians) . As before we denote the (to be learned) bases of these subspaces as \(_{[K]}=(_{k})_{k=1}^{K}\), where \(_{k}^{d p}\). Although the joint distribution of the tokens \(\) is unknown, the desired marginal distribution of each token \(_{i}\) is a mixture of subspaces. So we may obtain an upper bound of the coding rate for the token set \(\) by projecting its tokens onto these subspaces and summing up the respective coding rates:

\[R^{c}(;_{[K]})=_{k=1}^{K}R(_{k}^{*})= _{k=1}^{K}+}( _{k}^{*})^{*}(_{k}^{*}).\] (8)

We would like to compress (or denoise) the set of tokens against these subspaces by minimizing the coding rate. The gradient of \(R^{c}(;_{[K]})\) is

\[_{}R^{c}(;_{[K]})= }_{k=1}^{K}_{k}_{k}^{*}(+}(_{k}^{*})^{*}(_{k}^{*}))^{-1}.\] (9)

The above expression approximates the residual of each projected token \(_{k}^{*}_{i}\) regressed by other tokens \(_{k}^{*}_{j}\). But, differently from , not all tokens in \(\) are from the same subspace. Hence, to denoise each token with tokens from its own group, we can compute their similarity through an auto-correlation among the projected tokens as \((_{k}^{*})^{*}(_{k}^{*})\) and convert it to a distribution of membership with a softmax, namely \(((_{k}^{*})^{*}(_{k}^{*}))\). Then, as we show in Appendix A.2, if we only use similar tokens to regress and denoise each other, then a gradient step on the coding rate with learning rate \(\) can be naturally approximated as follows:

\[^{+1/2}=^{}-_{}R^{c}( ^{};_{[K]})(1-}) ^{}+}(^{ }\ \ _{[K]}),\] (10)

where \(\) is defined through an SSA operator as:

\[(_{k}) (_{k}^{*})\,((_{k}^{*} {Z})^{*}(_{k}^{*})), k[K],\] (11) \[(_{[K]}) }[_{1},,_{K}] (_{1})\\ \\ (_{K}).\] (12)

Here the SSA operator in (11) resembles the _attention operator_ in a typical transformer , except that here the linear operators of value, key, and query are all set to be _the same_ as the subspace basis, i.e., \(===_{k}^{*}\).6 Hence, we name \((|_{k}.):^{d N}^{p N}\) the **S**ubspace **S**elf-**A**ttention (SSA) operator (more details and justification can be found in (72) in Appendix A.2). Then, the whole \(\) operator in (12), formally defined as \((|_{[K]}.):^{d N}^{d N}\) and called the **M**ulti-Head **S**ubspace **S**elf-**A**ttention (MSSA) operator, aggregates the attention head outputs by averaging using model-dependent weights, similar in concept to the popular multi-head self-attention operator in existing transformer networks. The overall gradient step (10) resembles the multi-head self-attention implemented with a skip connection in transformers.

Notice that if we have \(N=1\) tokens as well as take an aggressive gradient step (\(=1\)) and tune the quantization error (\(=\)), the multi-head subspace self-attention operator in (12) becomes the ideal denoiser defined in (6), with the one minor difference that the aggregation of the heads is done by a linear function here, while in (6) it is done by a nonlinear mixture-of-experts type function.7 This provides two very related interpretations of the multi-head self-attention operator, as denoising and compression against a mixture of low-dimensional subspaces.

### MLP via Iterative Shrinkage-Thresholding Algorithms (ISTA) for Sparse Coding

In the previous subsection, we focused on how to compress a set of tokens against a set of (learned) low-dimensional subspaces. Optimizing the remaining terms in the sparse rate reduction objective (1), including the non-smooth term, serves to sparsify the compressed tokens, hence leading to a more compact and structured (i.e., _parsimonious_) representation. From (1) and (7), this term is

\[_{}[R()-\|\|_{0}]=_{ }[\|\|_{0}-\,+ }^{*}],\] (13)where \(R()\) denotes the coding rate of the whole token set, as defined in (7). In addition to sparsification via the \(\|\|_{0}\) term, the expansion term \(R()\) in (13) promotes diversity and non-collapse of the representation, a highly desirable property. However, prior work has struggled to realize this benefit on large-scale datasets due to poor scalability of the gradient \(_{}R()\), which requires a matrix inverse .

To simplify things, we therefore take a different approach to trading off between representational diversity and sparsification: we posit a (complete) incoherent or orthogonal dictionary \(^{d d}\), and ask to sparsify the intermediate iterates \(^{+1/2}\) with respect to \(\). That is, \(^{+1/2}=^{+1}\) where \(^{+1}\) is more sparse. The dictionary \(\) is global, i.e., is used to sparsify all tokens simultaneously.

By the incoherence assumption, we have \(^{*}_{d}\); thus from (7) we have \(R(^{+1}) R(^{+1})=R(^{+1/2})\). Thus we approximately solve (13) with the following program:

\[^{+1}=*{arg\,min}_{}\|\|_{0}^{+1/2}=.\] (14)

The above sparse representation program is usually solved by relaxing it to an unconstrained convex program, known as LASSO:

\[^{+1}=*{arg\,min}_{}\|\|_{1 }+\|^{+1/2}-\|_{F}^{2}.\] (15)

In our implementation, motivated by Sun et al.  and Zarka et al. , we also add a non-negative constraint to \(^{+1}\),

\[^{+1}=*{arg\,min}_{}\| \|_{1}+\|^{+1/2}-\|_{F}^{2},\] (16)

which we then incrementally optimize by performing an unrolled proximal gradient descent step, known as an ISTA step , to give the update:

\[^{+1}=(^{+1/2}+^{*}(^{ +1/2}-^{+1/2})-)(^{+1/2}).\] (17)

In Appendix A.3, we will show one can arrive at a similar operator to the above ISTA-like update for optimizing (13) by properly linearizing and approximating the rate term \(R()\).

### The Overall White-Box crate Architecture

By combining the above two steps:

1. (Sections 2.2 and 2.3) Local denoising and compression of tokens within a sample towards a mixture-of-subspace structure, leading to the multi-head subspace self-attention block - MSSA;

Figure 2: One layer of the CRATE architecture. The full architecture is simply a concatenation of such layers, with some initial tokenizer and final task-specific architecture (i.e., a classification head).

2. (Section 2.4) Global compression and sparsification of token sets across all samples through sparse coding, leading to the sparsification block - \(\);

we can get the following rate-reduction-based transformer layer, illustrated in Figure 2,

\[^{+1/2}^{}+(^{}^{ }_{[K]}),^{+1}(^{+1/2} {D}^{}).\] (18)

Composing multiple such layers following the incremental construction of our representation in (2), we obtain a white-box transformer architecture that transforms the data tokens towards a compact and sparse union of incoherent subspaces.

This model has the parameters \((^{}_{[K]})^{L}_{=1}\) and \((^{})^{L}_{=1}\), which are learned from data via _back-propagation_. Notably, in each layer \(\), the learned \(^{}_{[K]}\) retain their interpretation as incoherent bases for supporting subspaces for the mixture-of-Gaussians model at layer \(\), and the learned \(^{}\) retains its interpretation as a sparsifying dictionary at layer \(\). We emphasize that the parameters \(^{}_{[K]}\) and \(^{}\) are dependent on the layer \(\) -- that is, we learn a different set of parameters at each layer. This is because at each layer we learn an approximate local parametric model for the input data distribution, then use that learned model to construct the layer operators that transform the distribution. Our procedure of parameterizing the data distribution at each layer distinguishes this work from previous works on unrolled optimization for neural networks such as the ReduNet . Our interpretation clarifies the roles of the network forward pass (given local signal models at each layer, denoise/compress/sparsify the input) and the backward pass (learn the local signal models from data via supervision).

We note that in this work, at each stage of our construction, we have chosen arguably the _simplest possible_ construction to use. We can substitute each part of this construction, so long as the new part maintains the same conceptual role, and obtain another white-box architecture. Nevertheless, our such-constructed architecture, called crate (i.e., Coding RAte TransformEr), connects to existing transformer models, obtains competitive results on real-world datasets, and is fully mathematically interpretable.

## 3 Experiments

In this section, we conduct experiments to study the performance of our proposed white-box transformer crate on real-world datasets and tasks. As the analysis in Section 2 suggests, either the compression or the sparsification step can be achieved through various alternative design choices or strategies. crate arguably adopts the most basic choices and so our goal with the experiments is _not_ simply to compete with other heavily engineered transformers while using such a rudimentary design. Rather, our goals are twofold. First, unlike any empirically designed black-box networks that are usually evaluated only on end-to-end performance, the white-box design of our network allows us to _look inside_ the deep architecture and verify if layers of the learned network indeed perform their design objective--say performing incremental optimization for the objective (1). Second, despite their simplicity, our experiments will actually reveal the vast practical potential of our so-derived crate architectures since, as we will show, they already achieve very strong performance on large-scale real-world datasets and tasks. In the remainder of this section we highlight a selection of results; additional experimental details and results can be found in Appendix B.

Model architecture.We implement the architecture that is described in Section 2.5, with minor modifications that are described in Appendix B.1. We consider different model sizes of crate by varying the token dimension \(d\), number of heads \(K\), and the number of layers \(L\). We consider four model sizes in this work: crate-Tiny, crate-Small, crate-Base, and crate-Large. A PyTorch-style pseudocode can be found in Appendix B.1, which contains more implementation details. For training using supervised classification, we first take the \(\) token \(}_{b}=_{1,b}^{L+1}\) of for each sample, then apply a linear layer; the output of this linear layer \(_{b}}_{b}\) is used as input to the standard cross-entropy loss. The overall loss averages over all samples \(b[B]\).

Datasets and optimization.We mainly consider ImageNet-1K  as the testbed for our architecture. Specifically, we apply the Lion optimizer  to train crate models with different model sizes. Meanwhile, we also evaluate the transfer learning performance of crate: by considering the models trained on ImageNet-1K as pre-trained models, we fine-tune crate on several commonly used downstream datasets (CIFAR10/100, Oxford Flowers, Oxford-IIT-Pets). More details about the training and datasets can be found in Appendix B.1.

### In-depth Layer-wise Analysis of crate

**Do layers of crate achieve their design goals?** As described in Section 2.3 and Section 2.4, the MSSA block is designed to optimize the compression term \(R^{c}()\) and the ISTA block to sparsify the token representations (corresponding to the sparsification term \(\|\|_{0}\)). To understand whether crate indeed optimizes these terms, for each layer \(\), we measure (i) the compression term \(R^{c}(^{+1/2})\) on the MSSA block outputs \(^{+1/2}\); and (ii) sparsity \(\|^{+1}\|_{0}\) on the ISTA block outputs \(^{+1}\). Specifically, we evaluate these two terms by using training/validation samples from ImageNet-1K. Both terms are evaluated at the per-sample level and averaged over \(B=10^{3}\) samples.

Figure 3 shows the plots of these two key measures at all layers for the learned crate-small model. We find that as the layer index \(\) increases, both the compression and the sparsification terms improve in most cases. The increase in the sparsity measure of the last layer is caused by the extra linear layer for classification.8 These results suggest that crate aligns well with the original design goals: once learned, it essentially learns to gradually compress and sparsity the representations through its layers. In addition, we also measure the compression and sparsification terms on crate models with different model sizes as well as intermediate model checkpoints and the results are shown by plots in Figure 5 of Appendix B.2. The observations are very consistent across all different model sizes--both the compression and sparsification terms improve in most scenarios. Models with more layers tend to optimize the objectives more effectively, confirming our understanding of each layer's roles.

To see the effect of learning, we present the evaluations on crate-Small trained with different number of epochs in Figure 4. When the model is not trained enough (e.g. untrained), the architecture does not optimize the objectives effectively. However, during training--learning better subspaces \(_{[K]}^{}\) and dictionaries \(^{}\)--the designed blocks start to optimize the objectives much more effectively.

Visualizing layer-wise token representations.To gain a better understanding of the token representations of crate, we visualize the output of each ISTA block at layer \(\) in Figure 6 of Appendix B.2. Specifically, we visualize the \(^{+1}\) via heatmap plots. We observe that the output \(^{+1}\) becomes more sparse as the layer increases. Moreover, besides the sparsity, we also find that \(^{+1}\) becomes

Figure 4: The compression term \(R^{c}()\) (_left_) and sparsification term \(\|\|_{0}/(d N)\) (_right_) across models trained with different numbers of epochs. (Model: crate-Base).

Figure 3: _Left_: The compression term \(R^{c}(^{+1/2})\) of the MSSA outputs at different layers. _Right_: the sparsity of the ISTA output block, \(\|^{+1}\|_{0}/(d N)\), at different layers. (Model: crate-Small).

more structured (i.e., low-rank), which indicates that the set of token representations become closer to linear subspaces, confirming our mental picture of the geometry of each layer (as in Figure 1).

Visualizing layer-wise subspaces in multi-head self-attention.We now visualize the \(_{[K]}^{}\) matrices used in the MSSA block. In Section 2.3, we assumed that \(_{[K]}^{}\) were incoherent to capture different "views" of the set of tokens. In Fig. 7 of Appendix B.2, we first normalize the columns in each \(_{k}^{}\), then we visualize the \([_{1}^{},,_{K}^{}]^{{}^{}}[_{1}^{}, ,_{K}^{}]^{pK pK}\). The \((i,j)\)-th block in each sub-figure corresponds to \((_{i}^{})^{}_{j}^{}\) for \(i,j[K]\) at a particular layer \(\). We find that the learned \(_{[K]}^{}\) are approximately incoherent, which aligns well with our assumptions. One interesting observation is that the \(_{[K]}^{}\) becomes more incoherent when the layer index \(\) is larger, which suggests that the token representations are more separable. This mirrors the situation in other popular deep networks .

### Evalutions of crate on Large Real-World Datasets and Tasks

We now study the empirical performance of the proposed networks by measuring their top-1 accuracy on ImageNet-1K as well as transfer learning performance on several widely used downstream datasets. We summarize the results in Table 1. As our designed architecture leverages parameter sharing in both the attention block (MSSA) and the MLP block (ISTA), our crate-Base model (22.08 million) has a similar number of parameters to the ViT-Small (22.05 million).

From Table 1, we find that with a similar number of model parameters, our proposed network achieves similar ImageNet-1K and transfer learning performance as ViT, despite the simplicity and interpretability of our design. Moreover, with the same set of training hyperparameters, we observe promising scaling behavior in crate--we consistently improve the performance by scaling up the model size. For comparison, directly scaling ViT on ImageNet-1K does not always lead to consistent performance improvement measured by top-1 accuracy . To summarize, we achieve promising performance on real-world large-scale datasets by directly implementing our principled architecture.

## 4 Conclusion

In this paper, we propose a new theoretical framework that allows us to derive deep transformer-like network architectures as incremental optimization schemes to learn compressed and sparse representation of the input data (or token sets). The so derived and learned deep architectures are not only fully mathematically interpretable, but also consistent on a layer-by-layer level with their design objective. Despite being arguably the simplest among all possible designs, these networks already demonstrate performance on large-scale real-world datasets and tasks close to seasoned transformers. We believe this work truly helps bridge the gap between theory and practice of deep neural networks as well as help unify seemingly separate approaches to learning and representing data distributions. Probably more importantly for practitioners, our framework provides theoretical guidelines to design and justify new, potentially more powerful, deep architectures for representation learning.

  
**Datasets** & crate-T & crate-S & crate-B & crate-L & ViT-T & ViT-S \\   \# parameters & 6.09M & 13.12M & 22.80M & 77.64M & 5.72M & 22.05M \\  ImageNet & 66.7 & 69.2 & 70.8 & 71.3 & 71.5 & 72.4 \\ ImageNet Real. & 74.0 & 76.0 & 76.5 & 77.4 & 78.3 & 78.4 \\  CIFAR10 & 95.5 & 96.0 & 96.8 & 97.2 & 96.6 & 97.2 \\ CIFAR100 & 78.9 & 81.0 & 82.7 & 83.6 & 81.8 & 83.2 \\ Oxford Flowers-102 & 84.6 & 87.1 & 88.7 & 88.3 & 85.1 & 88.5 \\ Oxford-IIT-Pets & 81.4 & 84.9 & 85.3 & 87.4 & 88.5 & 88.6 \\   

Table 1: Top 1 accuracy of crate on various datasets with different model scales when pre-trained on ImageNet. For ImageNet/ImageNetReaL, we directly evaluate the top-1 accuracy. For other datasets, we use models that are pre-trained on ImageNet as initialization and the evaluate the transfer learning performance via fine-tuning.