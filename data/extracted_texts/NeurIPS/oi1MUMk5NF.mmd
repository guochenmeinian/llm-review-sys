# DISCS: A Benchmark for Discrete Sampling

Katayoon Goshvadi

Google Deepmind

Haoran Sun

Georgia Tech

Xingchao Liu

UT Austin

Azade Nova

Google Deepmind

Ruqi Zhang

Purdue University

Will Grathwohl

Google Deepmind

Dale Schuurmans

Google Deepmind

Hanjun Dai

Google Deepmind

###### Abstract

Sampling in discrete spaces, with critical applications in simulation and optimization, has recently been boosted by significant advances in gradient-based approaches that exploit modern accelerators like GPUs. However, two key challenges are hindering further advancement in research on discrete sampling. First, since there is no consensus on experimental settings and evaluation setups, the empirical results in different research papers are often not comparable. Second, implementing samplers and target distributions often requires a nontrivial amount of effort in terms of calibration and parallelism. To tackle these challenges, we propose _DISCS_ (DISCrete Sampling), a tailored package and benchmark that supports unified and efficient experiment implementation and evaluations for discrete sampling in three types of tasks: sampling from classical graphical models and energy based generative models, and sampling for solving combinatorial optimization. Throughout the comprehensive evaluations in _DISCS_, we gained new insights into scalability, design principles for proposal distributions, and lessons for adaptive sampling design. _DISCS_ efficiently implements representative discrete samplers in existing research works as baselines and offers a simple interface that researchers can conveniently add new discrete samplers and directly compare their performance with the benchmark result in a calibrated setup.

## 1 Introduction

Sampling in discrete spaces has been an important problem for decades in physics (Edwards and Anderson, 1975; Baumgartner et al., 2012), statistics (Robert and Casella, 2013; Carpenter et al., 2017), and computer science (LeCun et al., 2006; Wang and Cho, 2019). Since sampling from a target probability distribution \((x)(-f(x))\) defined on a discrete space \(\) is typically intractable, one usually resorts to Markov chain Monte Carlo (MCMC) methods (Metropolis et al., 1953; Hastings, 1970). However, except for a few algorithms such as Swendsen-Wang for the Ising model (Swendsen and Wang, 1987) and Hamze-Freitas for hierarchical models (Hamze and de Freitas, 2012), which exploit the special structure of the underlying problem, sampling in a general discrete space has primarily relied on Gibbs sampling, which exhibits notoriously poor efficiency in high dimensional spaces.

Recently, a family of locally balanced MCMC samplers for discrete spaces (Zanella, 2020; Grathwohl et al., 2021; Sun et al., 2021; Zhang et al., 2022), using ratio informed proposal distributions, \(\), have significantly improved sampling efficiency by exploiting modern accelerators like GPUs and TPUs. From the perspective of gradient flow on the Wasserstein manifold of distributions, Gibbs sampling is simply a coordinate descent algorithm, whereas locally balanced samplers perform as full gradient descent (Sun et al., 2022). Despite the advances in locally balanced samplers, a quantitative benchmark is still missing. As a result, the empirical results in different research papers may not be comparable. One important reason is that there is no consensus on the experimentalsetting. Particularly, the initialization of energy based generative models, random seeds used in graphical models, and the protocol of hyper-parameter tuning all have a significant impact on samplers performance. Under this circumstance, there is a critical need for a unified benchmark to advance the research in discrete sampling.

There are two key challenges that seriously hinder the appearance of such a benchmark. First, a sampler may perform well in one target distribution while poorly in another one. To thoroughly examine the performance of a sampler, a qualified benchmark needs to collect a set of representative distributions that covers the potential applications of discrete samplers. Second, the evaluation of discrete samplers is complicated. Although the commonly used metric Effective Sample Size (ESS) (Vehtari et al., 2021) can effectively reflect the efficiency of a sampler in Monte Carlo integration or Bayesian inference, it is not very informative in scenarios when the sampler guides the search in combinatorial optimization problems or performs as a decoder in deep generative models.

To address the two challenges, we propose _DISCS_, a tailored benchmark for discrete sampling. In particular, _DISCS_ consists of three groups of tasks: sampling from classical graphical models, sampling for solving combinatorial optimization problems, and sampling from deep energy based models (EBMs). These tasks cover both the topics of simulation and optimization, and also target distributions, ranging from hand-designed graphical models to learned deep EBMs. For each task, we collect the representative problems from both synthetic and real-world applications, for example, graph partitioning for distributed computing and language model for text generation. We carefully design the evaluation metrics in _DISCS_. In sampling from classical graphical models tasks, _DISCS_ uses ESS as a standard. In sampling for solving combinatorial optimization tasks, _DISCS_ runs simulated annealing (Kirkpatrick et al., 1983) with multiple chains and reports the average of the best results in each chain. In sampling from energy based generative models, _DISCS_ employs domain specific scores to measure the sample quality.

_DISCS_ offers a convenient interface for researchers to implement new discrete samplers, without worrying about parallelism, experiment loop, and evaluation. _DISCS_ can efficiently sweep over different tasks and experiments configurations in parallel, making it easy to reproduce the benchmark results of this paper. Also, _DISCS_ implements existing discrete samplers including random walk Metropolis (Metropolis et al., 1953), block Gibbs, Hamming Ball sampler (Titsias and Yau, 2017), Locally Balanced (Zanella, 2020), Gibbs with Gradients (Grathwohl et al., 2021), Path Auxiliary Sampler (Sun et al., 2021), Discrete Metropolis Adjusted Langevin Algorithm (Zhang et al., 2022), Discrete Langevin Monte Carlo (Sun et al., 2022), and is actively maintaining to add new samplers. Researchers can directly compare the results with state-of-the-art methods.

_DISCS_ can also provide insights on the existing open questions in the space of discrete sampling. In our experiments, we observe an interesting phenomenon that the locally balanced weight function \(g(t)=\) outperforms \(g(t)=\) when Ising model has a temperature higher than the critical temperature and \(g(t)=\) outperforms \(g(t)=\) when the temperature is lower than the critical temperature. We further observe similar phenomenon in our experiments with more complicated deep energy generative model where \(g(t)=\) outperforms \(g(t)=\) on overparameterized neural network with low temperature and sharp landscape. There have been numerous studies on how to select the locally balanced function for a locally balanced sampler (Zanella, 2020; Sansone, 2022), but the answer still remains open. We hope the observations in this paper can provide some insight on this question.

We wrap the _DISCS_ package as a JAX library to facilitate the research in discrete sampling. The library is open sourced at DISCS 1. The dataset used in our benchmark experiments can be accessed at DISCS DATA 2 The paper is organized as follows:

* In section 2, we provide an overview of related work on different tasks for discrete sampling and recent advances in discrete samplers.
* In section 3, we formulate the discrete sampling problem.
* In section 4, we introduce the discrete sampling tasks and evaluation metrics in _DISCS_. We also present several results with interesting insights and observations.
* In section 5, we discuss the contribution and limitations of _DISCS_.

We provide comprehensive results of our benchmark and studies with further details on the experimental setups, evaluation metrics, mathematical formulations, and the data set used in the Appendix A.

## 2 Related Work

Discrete sampling has been widely used to study the physical picture of spin glasses (Hukushima and Nemoto, 1996; Katzgraber et al., 2001), solve combinatorial optimization via simulated annealing (Kirkpatrick et al., 1983), and for training or decoding deep energy based generative models (Wang and Cho, 2019; Du et al., 2020; Dai et al., 2020). However, these tasks primarily depend on Gibbs sampling, which could be very slow in high dimensional space.

Since the seminal work Zanella (2020), the recent years have witnessed significant progresses for discrete sampling in both theory and practice. Zanella (2020) introduces the locally balanced proposal \(q(x,y) g()\), where \(y N(X)\) restricted within a small neighborhood of \(x\) and \(g():_{+}_{+}\) satisfying \(g(a)=ag()\), and prove it is asymptotically optimal. In the following works, PAS (Sun et al., 2021) and DMALA (Zhang et al., 2022) generalize locally balanced proposal to large neighborhoods by introducing an auxiliary path and mimicking the diffusion process, respectively. Inspired by these locally balanced samplers, Sun et al. (2022) generalize the Langevin dynamics in continuous space to _discrete Langevin dynamics_ (DLD) in discrete space as a continuous time Markov chain \((X^{t+h}=y|X^{t}=x)=g()\), and show that previous locally balanced samplers are simulations of DLD with different discretization strategies. In the view of Wasserstein gradient flow, the Gibbs sampling can be seen as coordinate descent and DLD gives a full gradient descent. Hence, locally balanced samplers induced from DLD provide a principled framework to utilize modern accelerators like GPUs and TPUs to accelerate discrete sampling. Besides the discretization of DLD, another crucial part to design a locally balanced sampler is estimating the probability ratio \(\). Grathwohl et al. (2021) proposes to used gradient approximation \((- f(x),y-x)\) and obtains good performance on various classical models and deep energy based models. When the Hessian is available, Rhodes and Gutmann (2022); Sun et al. (2023) use second order approximation via Gaussian integral trick (Hubbard, 1959) to further improve the sampling efficiency on skewed target distributions. When the gradient is not available, Xiang et al. (2023) use zero order approximation via Newton's series.

Besides designing the sampler, Sun et al. (2022) proves that when tuning path length in PAS (Sun et al., 2021), the optimal efficiency is obtained when the average acceptance rate is 0.574, and design an adaptive tuning algorithm for PAS. Sansone (2022) learn locally balanced weight function for locally balanced proposal, but how to select the weight function in a principled manner is still unclear.

## 3 Formulation for Sampling in Discrete Space

The sampling in discrete space can be formulated as the following problem: in a finite discrete space \(\), we have an energy function \(f():\). We consider a target distribution

\[(x)=, Z=_{z}(- f (z)),\] (1)

where \(\) is the inverse temperature. When the normalizer \(Z\) is intractable, people usually resort to Markov chain Monte Carlo (MCMC) to sample from the target distribution \(\). Metropolis-Hastings (M-H) (Metropolis et al., 1953; Hastings, 1970) is a commonly used general purpose MCMC algorithm. Specifically, given a current state \(x^{(t)}\), the M-H algorithm proposes a candidate state \(y\) from a proposal distribution \(q(x^{(t)},y)\). Then, with probability

\[1,\,)}{(x^{(t)})q(x^{(t)},y)}},\] (2)

the proposed state is accepted and \(x^{(t+1)}=y\); otherwise, \(x^{(t+1)}=x^{(t)}\). In this way, the detailed balance condition is satisfied and the M-H sampler generates a Markov chain \(x^{(0)},x^{(1)},...\) that has \(\) as its stationary distribution.

Benchmark for Sampling in Discrete Space

The recent development of locally balanced samplers that use the ratio \(\) to guide the proposal distribution \(q(x,)\) has significantly improved the sampling efficiency in discrete space. However, there is no consensus for many experimental settings. As a result, the empirical results in different research papers may not be directly comparable. Under this circumstance, we propose _DISCS_ as a benchmark for general purpose samplers in discrete space. In section 4.1, we introduce implemented sampling methods as the baselines in _DISCS_ and make some remarks on how we present the results. _DISCS_ implements both the classical discrete samplers and the recent developed locally balanced samplers. In the following sections, we introduce the tasks considered in _DISCS_ as follows: sampling from classical graphical models (section 4.2), sampling for solving combinatorial optimization problems (section 4.3), and sampling from deep energy based generative models (section 4.4). We also describe how the discrete samplers are evaluated on these tasks and report several results of them with some insights. We leave the remaining comprehensive results and experimentation and report them in detail in Appendix A.

### Baselines

We include both classical discrete samplers and state-of-the-art locally balanced samplers in recent research papers as baselines in our benchmark. Specifically, _DISCS_ implements

1. random walk Metropolis (RWM) (Metropolis et al., 1953).
2. block Gibbs (BG), where BG-\( a\)> denotes using block Gibbs with block size \(a\).
3. Hamming Ball Sampler (HB) (Titsias and Yau, 2017), where HB-\( a\)>-\( b\)> denotes using block size \(a\) and Hamming ball size \(b\).
4. Gibbs with Gradients (GWG) (Grathwohl et al., 2021), a locally balanced sampler that uses gradient to approximate the probability ratio. For binary distribution, GWG has a scaling factor \(L\) to determine how many sites to flip per step.
5. Path Auxiliary Sampler (PAS) (Sun et al., 2021), a locally balanced sampler that has a scaling factor \(L\) to determine the path length.
6. Discrete Metropolis Adjusted Langevin Algorithm (DMALA)(Zhang et al., 2022), a locally balanced sampler that has a scaling factor \(\) to determine the step size.
7. Discrete Langevin Monte Carlo (DLMC) (Sun et al., 2022), a locally balanced sampler that has a scaling factor \(\) to determine the simulation time of DLD. DLMC has multiple choices for its numerical solver to approximate the transition matrix. _DISCS_ considers the two versions used in the original paper, DLMC that uses an interpolation, and DLMCf that uses Euler's forward method.

Note that in the depicted plots in this paper, each of the previously described samplers is represented with a distinct and unique color of its own. We present samplers RWM, BG, HB, GWG, PAS, DMALA, DLMC, DLMCf as the colors green, yellow, blue, red, brown, purple, pink and grey.

Remark: weight function_DISCS_ offers a range of locally balanced functions, including \(g(t)=\), \(g(t)=\), \(g(t)=1 t\), and \(g(t)=1 t\). All the locally balanced samplers have the flexibility to select from these locally balanced functions. \(g(t)=\) and \(g(t)=\) are the two most commonly used weight functions which we also rely on for our experiments. We use <sampler>-\(\)> to refer to the type of the weight function for the locally balanced sampler. In cases that the weight function is not reported, we use \(\) by default.

Remark: scalingSince the scaling of the proposal distribution in RWM, PAS, DMALA, and DLMC are tunable, we consider two versions: one with adaptive tuning and another with binary search tuning to ensure fair comparison. Sun et al. (2022, 2023) propose an adaptive tuning algorithm for PAS and DLMC when the target distribution is factorized. In practice, we find that they also apply well to other locally balanced samplers and for more general target distributions. Hence, in this paper, we use the adaptive tuning algorithm by default to tune the scaling for locally balanced samplers. In the several exceptions where the adaptive algorithm does not apply, we will use <sampler>-noA to indicate the results from binary search tuning.

### Sampling from Classical Graphical Models

This section covers the classical graphical models frequently employed in physics and statistics, including Bernoulli Models, Ising Models (Ising, 1924), and Factorial Hidden Markov Models (Ghahramani & Jordan, 1995). The graphical models have large configuration flexibility, for example, the number of discrete variables, the number of categories for each discrete variable, and the temperature of the model. The performances of different samplers can heavily depend on these configurations. _DISCS_ provides tools to automatically sweep over hundreds of configurations with one click. Following the common practice in Monte Carlo integration or Bayesian inference, _DISCS_ uses the Effective Sample Size (ESS) to evaluate the efficiency of each sampler and reports the ESS normalized by the number of calling energy functions and by the running time. Below, we present several Ising Model results as illustrative examples. We report more results and details of our experiments and ESS calculation in the Appendix A.1, A.6.1. More specifically, we report the performance results of different samplers on Bernoulli Model 7, Categorical Model 8, Ising Model 9, Potts Model 10 and FHMM 11. We study the effect of number of discrete variables (sample dimension), the number of categories for each discrete variable, weight function for locally balanced samplers, and the temperature of the models (smoothness/sharpness).

The Ising Model is defined on a 2D grid, where the state space \(=\{-1,1\}^{p p}\) represents the spins on all nodes. For each state \(x\), the energy function is defined as:

\[f(x)=-_{i,j}J_{ij}x_{i}x_{j}-_{i}h_{i}x_{i}\] (3)

where \(J_{ij}\) is the internal interaction and the \(h_{i}\) is the external field. In the main text, we report the results using the configuration from Zanella (2020). Specifically, \(J_{ij}=0.5\), \(h_{i}=_{i}+_{i}\), where \(_{i}(-1.5,1.5)\) and \(_{i}=0.5\) if node \(i\) is located in a circle has the same center as the 2D grid and radius \(}\), else \(-0.5\). We consider the target distribution \((x)(- f(x))\), where \(\) is the inverse temperature. Using _DISCS_, one can easily investigate the influence of the number of discrete variables (sample dimension). In Figure 1, one can see that the classical samplers, RWM, BG, HB, have a significant decrease in ESS when the model dimension increases, while the locally balanced samplers are less affected as the ratio information \(\) effectively guides the proposal distribution. The overall trends basically follows the prediction from Sun et al. (2022b) that the ESS is \(O(d^{-1})\) for RWM and \(O(d^{-})\) for PAS.

Through _DISCS_, researchers can easily define various experiments, configure tasks, evaluate sampler performance and gain invaluable insights on open questions. As an example, we experiment with sampling from Ising Models with a range of temperatures and sample dimensions. In Figure 2, we experiment with sampling from Ising Models with inverse temperatures from 0.1607 to 0.7607 for both sample dimension of \(50 50\) and \(100 100\). We consider Ising Model without external field: \(h_{i} 0\) and \(J_{ij} 1\) as we know the critical temperature for this configuration is \()}\). This gives us the critical point for inverse temperature as \(=0.4407\). From the results in Figure 2, we can see that

* The Ising model is harder to sample from when the inverse temperature \(\) is closer to the critical point, which is consistent with the theory in statistical physics.

Figure 1: Results of Ising model with different dimensions.

* When the inverse temperature \(\) is lower than the critical point, using weight function \(g(t)=\) gives larger ESS; When the inverse temperature is larger than the critical point, using weight function \(g(t)=\) consistently obtains larger ESS.

The second observation implies that one should use ratio function \(\) for target distributions with a sharp landscapes. We will revisit this conclusion in Table 2. We report more results on the effect of sample dimension in Appendix (7, 8, 9, 10).

The categorical version of Ising model is Potts model, where each site of a state \(x_{i}\) has values in a symmetry group, instead of \(\{-1,1\}\). For simplicity, we denote the symmetry group as a set of one hot vectors \(=\{e_{1},...,e_{c}\}\) with \(h_{i}^{C},J_{ij}^{C C}\). In this way, the energy function becomes:

\[f(x)=-_{i,j}x_{i}^{}J_{ij}x_{j}-_{i} h_{i},x_{i}\] (4)

In Figure 3, one can see the sampling efficiency is very robust with respect to the number of categories, with the exception of the classical HB and BG samplers, which demonstrate a decline in their sampling performance. The result for BG-2 on Potts model with 256 categories is omitted as it takes over 100 hours. We report more results on the effect of number of categories in Appendix (8, 10,11).

### Sampling for Solving Combinatorial Optimiazation

Combinatorial optimization is a core challenge in domains like logistics, supply chain management, and hardware design, and has been a fundamental problem of study in computer science for decades. Combining with simulated annealing Kirkpatrick et al. (1983), the discrete sampling algorithm is a powerful tool to solve combinatorial optimization problems (Sun et al., 2023b). In expectation, a sampler with a faster mixing rate can find better solutions. Hence, the second type of task is sampling for solving combinatorial optimization problems. Currently, _DISCS_ covers four problems:

Figure 3: Results of Potts models with different number of categories.

Figure 2: Performance of locally balanced samplers with different types of weight functions v.s temperature on: (left) \(50 50\) Ising model, (right) \(100 100\) Ising model.

Maximum Independent Set (MIS), Max Clique, MaxCut, and Balanced Graph Partition. Without loss of generality, we consider combinatorial optimization that admits the following form:

\[_{x=\{0,1,,C-1\}^{d}} a(x), b(x )=0\] (5)

For ease of exposition, we assume \(b(x) 0, x\), but otherwise do not limit the form of \(a\) and \(b\). To convert the optimization problem to a sampling problem, we first rewrite the constrained optimization into a penalty form via a penalty coefficient \(\), then treat this as an energy function for an EBM. In particular, the energy function takes the form:

\[f(x)=a(x)+ b(x)\] (6)

Then, we define the probability of \(x\) at inverse temperature \(\) by:

\[p_{}(x)(- f(x))\] (7)

A naive approach to this problem would be directly sampling from \(p_{}(x)\), but such a distribution is highly nonsmooth and unsuitable for MCMC methods. Instead, following classical simulated annealing, we define a sequence of distributions parameterized by a sequence of decaying temperatures:

\[=[p_{_{0}}(x),p_{_{1}}(x),,p_{_{T}}(x)]\] (8)

where the sequence \(_{0}<_{1}<<_{T}\) converges to a large enough value as \(T\) increases. Below, we present the problem formulation and the energy functions used for MaxCut and MIS problems. We present several results of the samplers solution for these combinatorial optimization problems as illustrative examples in the main text and report the remaining results in the Appendix A.2.

MaxCutThe objective of MaxCut problem is to find a cut on a graph \(G=(V,E)\) that partitions the graph nodes into two complementary sets \(V=V_{1} V_{2}\), such that the number of edges in \(E\) between \(V_{1}\) and \(V_{2}\) is as large as possible. MaxCut is an unconstrained problem, which makes its formulation relatively simple. We can set \(=\{0,1\}\) such that \(x_{i}=0\) represents \(i V_{1}\) and \(x_{i}=1\) means \(x_{i} V_{2}\). Then we can write \(a(x)=-x^{}Ax,b(x) 0\), where \(A\) is the adjacency matrix of the graph \(G\).

We apply the same simulated annealing temperature scheduling set up for all the samplers and compare the samplers performances against each other. In the MaxCut problem, we compute the ratio of our solution against the optimal solution found by Gurobi, running for one hour (Dai et al., 2020). The results presented at Figure 4 show the cut ratio throughout the chain generation over the number of M-H steps and the running time (s). The legends are sorted according to the most optimal solution each sampler finds. One can see that the PAS leads the results. Also, locally balanced samplers significantly outperform the traditional samplers, especially when the graph size increases.

Figure 4: Results for MaxCut on ER graphs. The ratio is computed by dividing the optimal cut size obtained from running Gurobi for 1 hour. (top) ratio with respect to the number of M-H steps, (bottom) ratio with respect to running time.

MisOn a graph \(G=(V,E)\), an independent set \(S V\) means that for any \(i,j S\), \((i,j) E\). We can set \(=\{0,1\}\) such that \(x_{i}=0\) means \(i S\) and \(x_{i}=1\) means \(i S\). Then we can write \(a(x)=-_{i V}x_{i}\) and \(b(x)=_{(i,j) E}x_{i}x_{j}\). For the penalty coefficient \(\), we follow Sun et al. (2022c) to select \(=1.0001\) being a value slightly larger than \(1\). We run all samplers on five groups of small ER graphs with 700 to 800 nodes, each group has 128 graphs with densities varying 0.05, 0.10, 0.15, 0.20, and 0.25. We also run all samplers on 16 large ER graphs with 9000 to 11000 nodes. For each configuration, we run 32 chains with the same running time and report the average of the best results found by each chain in Table 1. One can easily see that PAS obtains the best result.

### Sampling from Energy Based Generative Models

The discrete samplers can also play the role of decoder in generative models. In particular, given a dataset \(=\{X_{i}\}_{i=1}^{N}\) sampled from the target distribution \(\), one can train an energy function \(f_{}()\), such that the energy based model \(_{}()(-f_{}())\) fits the dataset \(\). _DISCS_ provides multiple checkpoints for the energy function trained on real-world image or language datasets. Researchers can easily evaluate their samplers after loading the learned energy function. We provide further experimental details and mathematical formulations at Appendix A.3.

For the models that are relatively simple, for example, Restricted Boltzmann Machine (RBM) trained on MNIST (LeCun, 1998) and fashion-MNIST (Xiao et al., 2017b), one can continue using ESS as the metric. In Figure 5, we evaluate the samplers on RBMs trained on MNIST with 25 and 200 hidden variables. One can see that DLMC has the best performance. We further report the results of samplers on categorical RBM trained on fashion-MNIST dataset at Appendix 16. For more complicated deep energy based models, a sampler may fail to mix within reasonable steps. In this case, ESS is not a good metric. To address this problem, _DISCS_ provides multiple alternative measurements, including snapshots and domain specific scores.

SnapshotsAfter loading the checkpoint of energy based generative models, _DISCS_ can generate snapshots of the sampling chains. For example, in Figure 6, we display the snapshots of sampling on a deep residual network trained on MNIST data (Sun et al., 2021) and on pretrained language model BERT 3. One can see that locally balanced samplers generate samples with higher qualities, and can typically visit multiple modalities in the distribution. We report further results on deep residual network trained on Omniglot and Caltech dataset at 17.

Domain Specific ScoresIn many deep generative tasks, the goal is to efficiently sample high-quality samples, instead of mixing in the learned energy based models. In this scenario, domain specific scores that directly evaluate the sample qualities are better choices. For example, _DISCS_ provides text filling task based on pre-trained language models like BERT (Wang and Cho, 2019; Devlin et al., 2018). Following the settings in prior work (Zhang et al., 2022), we randomly samples 20 sentences from TBC (Zhu et al., 2015) and WiKiText-103 (Merity et al., 2016) and mask four words in each sentence (Donahue et al., 2020) resulting in the dataset provided at DISCS DATA. For each masked sentence, we sample 25 sentences by generating 25 chains with length of 50, following the target probability distribution provided by BERT, and then selecting the last sample of the chain. As a common practice

    &  &  \\  & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.15 \\  HB-10-1 & 100.374 & 58.750 & 41.812 & 32.344 & 26.469 & 277.149 \\ BG-2 & 102.468 & 60.000 & 42.820 & 32.250 & 27.312 & 316.170 \\ RWM & 97.186 & 56.249 & 40.429 & 31.219 & 25.594 & -555.674 \\ GWG-nA & 104.812 & 62.125 & 44.383 & **34.812** & 28.187 & 367.310 \\ DMALA & 104.750 & 62.031 & 44.195 & 34.375 & 28.031 & 357.058 \\ PAS & **105.062** & **62.250** & **44.570** & 34.719 & **28.500** & **377.123** \\ DLMCf & 104.450 & 62.219 & 44.078 & 34.469 & 28.125 & 354.121 \\ DLMC & 104.844 & 62.187 & 44.273 & 34.500 & 28.281 & 355.058 \\   

Table 1: Results for MIS on ER graphs. The set found by the sampling algorithm is not necessarily an independent set, we report a lower bound: set size - # pair of adjacent nodes in the set.

in non-auto-regressive text generation, we select the top-5 sentences with the highest likelihood out of 25 sentences to avoid low-quality generation (Gu et al., 2017; Zhou et al., 2019).

We evaluate the generated samples in terms of diversity and quality. For diversity, we use self-BLEU (Zhu et al., 2018) and the number of unique n-grams (Wang & Cho, 2019) to measure the difference between the generated sentences. For quality, we measure the BLEU score (Papineni et al., 2002) between the generated texts and the original dataset, which is the combination of TBC and WikiText-103. We report the quantitative results in Table 2. We do not have the results for HB and BG as they are computationally infeasible for this task with 30k+ tokens. In this task, the locally balanced sampler still outperforms RWM. Also, one can notice that the weight function \(\) significantly outperforms \(\) on diversity metrics and reaches comparable results on the quality. The reason is that the overparameterized neural network is a low temperature system with sharp landscape. This phenomenon is consistent with the results in Figure 2. We provide further results for the non-adaptive cases with binary search fine tuning in Appendix A.3.3.

Figure 5: Results on RBMs trained on MNIST dataset. (top) RBM with 25 binary hidden variables, (bottom) RBM with 200 binary hidden variables.

Figure 6: Snapshots of energy based generative models: (left) snapshots for every 1k steps on MNIST ResNet, (right) snapshots for text filling task on BERT in Table 2

## 5 Conclusion

_DISCS_ is a tailored benchmark for discrete sampling. It implements a range of discrete sampling tasks and state-of-the-art discrete samplers and enables a fair comparison. From the results, we know that DLMC leads in sampling from classical graphical models, PAS leads in solving combinatorial optimization problems, DLMCf and DMALA have the best performance on language models. We believe more efficient discrete samplers can be obtained by designing better discretization of DLD (Sun et al., 2022). _DISCS_ is a convenient tool during this process. The researcher can freely set the configurations for tasks and samplers and _DISCS_ will automatically compile the program and run the processes in parallel. Besides, we observe that the choice of the locally balanced weight function should depend on the critical temperature of the target distribution. We believe this observation is insightful and will lead to a deeper understanding of locally balanced samplers.

Of course, _DISCS_ does not include all existing tasks or samplers in discrete sampling, for example, the zero order (Xiang et al., 2023) and second order (Sun et al., 2023) approximation methods. We will keep iterating _DISCS_ and more features will be added in the future. We wrap _DISCS_ to a JAX library. Researchers can conveniently implement customer tasks or samplers to accelerate their study and, in the meanwhile, contribute the code to _DISCS_ for further improvement. We believe _DISCS_ will be a powerful tool for researchers and facilitate future research in discrete sampling.

#### Acknowledgments

The authors would like to thank Bo Dai, Bethany Wang, Emily Xue for providing helpful discussions, and Pengcheng Ying, Sherry Yang and anonymous reviewers for the helpful comments on the paper.