# Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning

Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning

 Andrew Bond

KUIS AI Center

Koc University

abond19@ku.edu.tr

&Zafer Dogan

MLIP Research Group, KUIS AI Center

Electrical and Electronics Engineering

Koc University

zdogan@ku.edu.tr

Corresponding Author

###### Abstract

Subspace learning is a critical endeavor in contemporary machine learning, particularly given the vast dimensions of modern datasets. In this study, we delve into the training dynamics of a single-layer GAN model from the perspective of subspace learning, framing these GANs as a novel approach to this fundamental task. Through a rigorous scaling limit analysis, we offer insights into the behavior of this model. Extending beyond prior research that primarily focused on sequential feature learning, we investigate the non-sequential scenario, emphasizing the pivotal role of inter-feature interactions in expediting training and enhancing performance, particularly with an uninformed initialization strategy. Our investigation encompasses both synthetic and real-world datasets, such as MNIST and Olivetti Faces, demonstrating the robustness and applicability of our findings to practical scenarios. By bridging our analysis to the realm of subspace learning, we systematically compare the efficacy of GAN-based methods against conventional approaches, both theoretically and empirically. Notably, our results unveil that while all methodologies successfully capture the underlying subspace, GANs exhibit a remarkable capability to acquire a more informative basis, owing to their intrinsic ability to generate new data samples. This elucidates the unique advantage of GAN-based approaches in subspace learning tasks.

## 1 Introduction

Subspace learning is a widely explored task, especially with the growth of dimensionality in modern datasets. It is important to identify meaningful subspaces within the data, such as those determined by principal component analysis (PCA). However, due to the high dimensionality of the data, it is common to employ online methods such as Oja's method  and GROUSE . Meanwhile, Generative Adversarial Networks (GANs) , primarily used as generative models, have also demonstrated the ability to learn meaningful representations of data [4; 5]. Inspired by this, we explore how single-layer GAN models can be viewed as a form of subspace learning.

We seek to improve the understanding of GAN training by relaxing some common assumptions made in previous analysis of GANs . Specifically, we focus on the training dynamics of the gradient-based learning algorithms, which can be converted into a continuous-time stochastic process characterized by an ordinary differential equation (ODE). Furthermore, the dynamics of the model weights form a stochastic process modeled by a stochastic differential equation (SDE). Understanding these two equations provides the relevant information to understand convergence behaviour of the training. We extend previous work to discriminators with same dimensionality as the generator. Finally, we discuss the new training outcomes that can arise in the higher-dimensionality case.

Our work explores what happens when the training switches from sequential learning in the single-feature discriminator case , to the non-sequential (multi-feature) learning of our discriminator. We show that the non-sequential learning of features not only allows for faster learning and convergence, but also a higher maximum similarity with the true subspace compared to the sequential case, when everything else is kept the same. This shows that contrary to the general approach of making discriminators much weaker than generators, it is still possible to use a powerful discriminator. In fact, doing so can lead to much faster training with better performance, through careful choice of learning rates.

We further show that our new framework can be used to analyze the cases where we assume different dimensionalities between the true subspace, fake subspace, and discriminator. Through the use of a simple uplifting trick on the relevant Grassmannians, we are able to extend our analysis to arbitrary dimensionalities. To understand how GANs compare with existing subspace learning algorithms, we provide both theoretical and empirical comparisons with existing such algorithms. We see that the features learned by a GAN model are more meaningful and represent the data better as compared to Oja's method, due to the requirement of being able to generate new data from the underlying data distribution.

Finally, we test our approach using two prominent real-world datasets: MNIST and Olivetti Faces, comparing our method against a sequential discriminator, and show that all of the key insights gained through the theoretical analysis are visible in training on this dataset as well. This shows that our analysis has very practical applications, and can lead to interesting research directions exploring these ideas in more powerful GAN architectures. This testing is additionally done on the case where we assume different dimensionalities for each component of the model, showing that the results are as expected. We release all our code at https://github.com/KU-MLIP/SolvableMultiFeatureGAN.

Overall, our contributions are as follows:

1. We investigate the distinctions between multi-feature and single-feature discriminators and fully characterize the learning dynamics through a rigorous scaling limit analysis.
2. We introduce a novel method for analyzing cases where the true feature dimensionality is unknown, enabling broader future analyses under uncertain conditions.
3. We position the multi-feature GAN models as a new type of subspace learning algorithm, and compare against existing algorithms, both theoretically and empirically.
4. We further validate our findings on image datasets (MNIST and Olivetti Faces), highlighting the practical implications of our insights for real-world applications.

## 2 Related Work

### Training dynamics for GANs

Much of this work is inspired by Wang et al. , which was one of the first to undertake this task. However, in this case, they use a single-feature discriminator, and show that the choice of learning rates relative to the strength of noise is what determines the outcome of the training process: convergence, oscillations, or mode-collapse.

There have been further attempts to understand the convergence of GANs through other approaches, not just the dynamics of the gradient-based optimization. Heusel et al.  showed that under reasonable assumptions on the training process and hyperparameters, using a specific update rule will guarantee that the GAN converges to a local Nash equilibrium. Mazumdar et al.  introduced a new learning algorithm under which the local Nash equilibria are the only attracting fixed points, so that the training algorithm tends to move towards these points. This type of analysis is very similar to our approach, focused on understanding fixed points. Other types of models have also been analyzed in the high-dimensional regime, such as linear VAEs , two-layer neural networks solving a regression task , and two-layer autoencoders . A review of these methods can be found in .

### Subspace learning

Subspace learning is a heavily explored field, with many algorithms. However, when the noise of the data has a non-zero variance, most approaches fail, and the general technique used to solve the problem is some type of online PCA-based method. In Wang et al. , a similar analysis of dynamics through ODEs is performed for multiple algorithms which learn subspaces with non-zero variance of noise. This analysis allows for steady-state and phase transition analysis of these algorithms. Balzano et al.  presents a survey of different online PCA algorithms used for subspace learning, in the case where only some of the data is visible at each timestep, and discuss how it is possible to find a unique subspace of a given rank which matches all the provided data.

## 3 Background and problem formulation

### True data model

Our data \(_{k}\) is drawn from the following generative model, known as a _spiked covariance model_:

\[_{k}=_{k}+}_{k}\] (1)

Here, \(^{n d}\) is the true subspace we wish to learn represented as an orthonormal basis, \(_{k}^{d}\) is a zero-mean random vector with covariance matrix \(\), \(_{k}^{n}\) is a standard Gaussian vector, and \(_{T}\) represents the noise level.

The spiked covariance model is very widely studied, due to the non-triviality of learning \(\) whenever \(_{T}>0\). The key property of this model is that the top \(d\) eigenvectors of the data covariance \([_{k}_{k}^{T}]\) are given by the columns of \(\). If there exists a strict eigengap between the top \(d\) corresponding eigenvalues and the other eigenvalues, then the reconstruction loss function is proven to have \(\) as a global minima .

### Online subspace learning algorithms

Subspace learning is a very important task in machine learning, most commonly performed by algorithms such as PCA or ICA. However, these approaches involve costly operations such as calculating covariance matrices or calculating matrix inverses, infeasible in high dimensions. Therefore, it is very common to use an online version of these algorithms, processing samples one at a time.

Online subspace learning algorithms typically fall into two categories: algebraic methods and geometric methods. Algebraic methods are based on computing the top eigenvectors of some representation of a sample covariance matrix. Assuming a strict eigengap, the top eigenvectors will yield the true subspace. Meanwhile, geometric methods optimize a certain loss function over some geometric space (Euclidean space or a Grassmannian manifold). We review two subspace learning algorithms here, Oja's Method  and GROUSE . While GROUSE was introduced for the missing data case, it can be used for full data too. For further details about these algorithms and their categorization, we direct the reader to .

However, we suggest a third category of online subspace learning algorithms, which we call the generative methods. Such methods, including single-layer GANs, do not have information about the specific task, and instead aim to learn the data simply by seeing the data and attempting to generate data from the same distribution.

#### 3.2.1 Oja's method

Oja's method  is a classical algebraic approach to online subspace learning. Given an orthonormalized initial matrix \(_{0}\), we perform the following update at every timestep given a data sample \(_{k}\):

\[_{k+1}=[_{k}+_{k}_{k}^{T} _{k}]\] (2)

Here, \(\) is an orthonormalization operator, and \(\) is the learning rate.

#### 3.2.2 Grouse

GROUSE  performs gradient descent on the Grassmannian manifold, which guarantees orthonormality of the updates. In the full data case, we again start with an orthonormal initial matrix \(_{0}\), andat each timestep, given a data sample \(y_{k}\), our update is:

\[_{k+1}=_{k}+(_{k}-1)_{k}}{|| _{k}||}_{k}^{T}}{||_{k}||}+_{k} _{k}}{||_{k}||}_{k}^{T}}{||_{k}||}\] (3)

Here, \(_{k}=||_{k}||||_{k}||\), \(_{k}=_{}||_{k}-_{k} ||_{2}^{2}\), \(_{k}=_{k}_{k}\), \(_{k}=_{k}-_{k}\), and \(\) is our learning rate.

### Generative Models

Here, we focus specifically on GANs. A GAN model seeks to learn a representation of the underlying subspace through the use of two components: a generator and a discriminator. The generator learns the subspace by trying to generate new samples from the subspace, while the discriminator acts as a classifier, attempting to distinguish data from the true subspace from data produced by the generator.

Note that measuring performance through cosine similarity can actually be viewed as a way to measure the generalization performance of the generator, as it doesn't depend on any specific instance of generated data and instead provides a concrete measure of how similar the generated data will be.

#### 3.3.1 Generator

We assume that the generator also follows a spiked covariance model:

\[}_{k}=_{k}}_{k}+} }_{k}\] (4)

However, we do not assume that \(_{G}=_{T}\), or that the covariance of \(}_{k}\), \(\), is the same as \(\). The goal of the generator is to learn \(_{k}\).

#### 3.3.2 Discriminator

The learning in the GAN model critically depends on the choice of discriminator, which aims to separate the data from the true and generated subspaces.

The most common approach when training GANs is to use a discriminator that is weaker than the generator. If the discriminator is too strong, then it will easily learn to distinguish between true and generated samples, leading to vanishing gradients for the generator and thus preventing learning. However, a weak discriminator results in sequential learning, where the generator is only able to learn a subset of the features at a time. In multi-feature cases, this will lead to very slow learning.

Motivated by this, we seek to analyze a model in which the discriminator has the same strength as the generator. Thus, we let \(^{n d}\), and define the discriminator as

\[(;)=}(^{T})\] (5)

where \(}:^{n}\) is some function (see the assumptions below). Since this discriminator is able to focus on all the features at once, this means the generator is also able to learn every feature at once. This is in contrast to the single-feature case (where \(^{n}\)) analyzed previously. While this is a strong assumption on the discriminator, we show below how this assumption can be relaxed.

#### 3.3.3 Training procedure

GAN training is modeled as a two-player minimax game, where the discriminator attempts to maximize some loss function and the generator attempts to minimize it. This is used as a way to learn a "surrogate" subspace which represents the true subspace. Therefore, the GAN model can be seen as a form of subspace learning, except that the focus is on generating new samples from the subspace.

Specifically, let \((,};)\) be a loss function depending on the discriminator weights, and true and fake samples. If \(\) denotes the true distribution and \(}\) denotes the generator distribution, the minimax game can be represented as

\[_{}_{}_{y}_{ }(,};).\]

Following the approach of Wang et al. , and in order to compare the sequential and multi-feature cases, we use the following loss function:

\[(,};)=F((^{T }))-((}^{T}))-tr(H(^{T}))+tr(H(^{T} ))\] (6)Here, \(F,\) are functions affecting the outputs of the discriminator, \(H\) is an element-wise function used for regularizing the weights of the generator and discriminator, and \(>0\) controls the strength of the regularization. As \(\), the matrices \(,\) will become orthonormal.

The standard approach to solve this minimax game is using stochastic gradient descent (SGD). At timestep \(k\), given a sample \(_{k}\) from the true subspace and a sample \(}_{k}\) from the generator subspace, we perform the following updates:

\[_{k+1}&=_{k}- }{n}_{_{k}}(_{k}, {}_{k};_{k}),\\ _{k+1}&=_{k}+ _{_{k}}(_{k},}_{k}; _{k}).\] (7)

Here, \(\) denotes the learning rate of the discriminator, and \(\) denotes the learning rate of the generator. Note that while it is common to use a batch of data at a time when using SGD, we focus on a single element at a time in order to simplify all the analysis.

## 4 Development of ODE

Similar to , we make the following definitions:

**Definition 4.1**.: \(_{k}:=[,_{k},_{k}]^{n  3d}\) is called the _microscopic state_ of the training process at time \(k\).

**Definition 4.2**.: The tuple \(\{_{k},_{k},_{k},_{k},_{k}\}\) is called the _macroscopic state_ of \(_{k}\) at time \(k\), where \(_{k}:=_{k}^{T}_{k}\), \(_{k}:=^{T}_{k}\), \(_{k}:=_{k}^{T}_{k}\), \(_{k}:=_{k}^{T}_{k}\), and \(_{k}:=_{k}^{T}_{k}\). The macroscopic state can be written in matrix notation as \(_{k}=_{k}^{T}_{k}\), in which we get

\[_{k}=&_{k}&_{k}\\ _{k}^{T}&_{k}&_{k}\\ _{k}^{T}&_{k}^{T}&_{k}.\] (8)

### Macroscopic dynamics

To analyze the macroscopic dynamics, we reduce to a special case, which leads to a slightly modified set of the assumptions from Wang et al. .

1. The sequences \(_{k},}_{k}\) are i.i.d. random variables with bounded moments of all orders, and \(\{_{k}\}\) is independent of \(\{}_{k}\}\).
2. The sequences \(\{_{k}\},\{_{k}\}\) are both independent Gaussian vectors with zero mean and covariance matrix \(I_{n}\).
3. \(H()=-\), \(()=||x||\), and \(F(x)=(x)=}{2}\). We note that the first derivative of \(H\) exists, the first four derivatives of \(F(()),(())\) exist, and all the derivatives are uniformly bounded. Thus, our choices satisfy the conditions of assumption (A.3) from Wang et al. .
4. Let \([,_{0},_{0}]\) be the initial microscopic state. For \(i=1,,n\), we have \([_{l=1}^{d}([]_{i,l}^{4}+[_{0}]_{i,l}^{4}+[ _{0}]_{i,l}^{4})] C/n^{2}\), where \(C\) is some constant not depending on \(n\).
5. The initial macroscopic state \(_{0}\) satisfies \(||_{0}-_{0}^{*}|| C/\), where \(_{0}^{*}\) is a deterministic matrix and \(C\) is some constant not depending on \(n\).
6. The columns of the discriminator matrix \(\) are orthonormal, so that \(^{T}=_{d}\).

Assumptions (A1) and (A2) are the usual i.i.d assumptions common in machine learning. (A3) is important for deriving the update equations. (A4) and (A5) are used to guarantee that the macroscopic state can converge. Our assumption (A6) of orthonormal discriminator matrix allows us to simplify the equations since the \(\) matrix of the macroscopic state is always just \(_{d}\).

Under these assumptions, as well as letting \(\), we obtain a modified Theorem 1 from Wang et al. , specifically considering the reduced case of equation (13). Note that our choice of \(F,,\) means that our equations become an arbitrary-dimensional version of the original equations.

**Theorem 4.3**.: _Fix \(T>0\). Under Assumptions (A.1) - (A.6), it holds that_

\[_{0 k nT}||_{k}-()|| },\] (9)

_where \(C(T)\) is some constant depending on \(T\) but not \(n\), and \((t):_{+}\{0\}^{3d 3d}\) is a deterministic function. Moreover, \((t)\) is the unique solution of the following ODE:_

\[_{t}=&\ (_{t}_{t}^{T}+_{t} _{t}),_{t}=\ (_{t}-_{t} _{t}+_{t}_{t})\\ _{t}=&\ (_{t}^{T} _{t}-_{t}_{t}+ _{t}_{t})+(+_{t})_ {t}\\ _{t}=&\ (_{t} _{t}^{T}+_{t}_{t} ^{T}+_{t}_{t}+_{t}_{t}),_{t}=\ \] (10)

_with the initial condition \((0)=_{0}^{*}\), where_

\[_{t}=-diag(_{t}_{t}^{T}), _{t}=(1-}{2})_{t}^{T} _{t}-(1+}{2})_{t}^{T}_ {t}-^{2}+_{T}^{2}}{2}.\] (11)

A sketch of the proof of this theorem can be found in Appendix B. The proof closely mirrors the proof of the original theorem in .

### Microscopic dynamics

The microscopic dynamics are concerned with how the terms \(,,\) change over time. Following previous work, we consider the empirical measure

\[_{k}(,,)=_{i=1}^{n}([ },},}]-[[]_{i,:,}[_{k}]_{i,:,}[_{k}]_{i,:}]).\] (12)

where \(\) is the delta measure. This is a discrete-time stochastic process, which can be embedded in continuous time as \(_{t}^{(n)}=_{k}\), with \(k= nt\). Then, as \(n\), this process converges to a deterministic process \(_{t}\), which is the measure of the solution of the SDE

\[ d}_{t}&=,  d}_{t}=(}_{t} _{t}+_{t}}_{t})dt,\\ d}_{t}&=(}^{T} _{t}+}_{t})dt+ AdB_{t},\] (13)

where \(A\) is some diffusion term, negligable due to our assumption on the discriminator (A.6).

From this equation and the convergence of the measure, we can obtain the following weak PDE

\[_{t},(},}, })=_{t},(}_{t}_{t}+_{t}}_ {t})_{}}+_{t},(}^{T}_{t}+}_{t })_{}}\] (14)

where \(\) is a bounded, smooth test function. The ODE in the main theorem can be derived from this weak PDE.

## 5 Simulations

In order to demonstrate that the ODE properly represents the training dynamics of the GAN model, we first perform simulations and show that the empirical results match the ODE, seen in Figure 1. To understand how the training dynamics change based on the generator learning rate, we fix the discriminator learning rate as \(=0.2\) and fix the generator learning rate \(=0.04\). We show the results on 4 different noise levels. In all cases, we let \(==diag([,])\).

We set \(_{0}=_{0}=0.1*I\), and we ensure that the empirical setup is initialized with exactly matching \(P\) and \(Q\) values. We note that the ODE will never learn when the initialization is exactly \(0\), and so we must provide some level of similarity to start training. However, this is not very restrictive, as our experiments show that even random matrices will have approximately \(0.001*I\) for both \(P\) and \(Q\), which is sufficient to escape the fixed point around \(0\).

### Off-diagonal simulations

A key insight found from the multi-feature discriminator is that the interaction between different features can help learning. When the macroscopic states are initialized to non-diagonal matrices, we see that the dimension with smaller covariance is actually able to attain better results and reach a similar cosine similarity to the dimension with higher covariance. Such an outcome is not possible in the sequential learning regime, due to the lack of interaction between features. In sequential learning, features are learned one at a time, and once a feature has been learned, the training will focus on a different feature instead. This phenomenon can be seen in Figure 2, showing that the off-diagonal initialization allows for not only faster training (which also happens in the diagonal initialization case), but also higher steady-state values compared to the sequential learning case. We are unable to provide a detailed characterization of these fixed points, as a neat closed-form solution cannot be obtained.

## 6 Unknown number of features

While this type of analysis can provide interesting insights, it has a very restrictive assumption that we know the number of features \(d\). This is done so that the macroscopic states are well-defined. However, we now seek to extend this analysis to the case where the true subspace has \(d\) features, the generator subspace has \(p\) features, and the discriminator learns \(q\) features, where we do not assume that \(d=p=q\). While this analysis can be performed under any assumptions on the relative size of \(d\), \(p\), and \(q\), we focus on the single case \(d q p n\).

To simplify the demonstration of this approach, we make the assumption that \(=_{d}\\ \), so that \(\) contains the first \(d\) standard basis vectors. We introduce the idea of uplifting (inspired by the work in ) the matrices \(,\) to the dimensionality of \(\).

Figure 1: ODE results for learning rate \(=0.04,=0.2\) and four different noise levels, with \(d=2\). The columns represent \(_{G}=_{T}=2,1,3,4\) respectively. At \(=5\) or higher, the generator is unable to learn anything. In all cases, the green and red represent the two diagonals of \(\), and the blue and yellow represent the two diagonals of \(\). We see that the simulations do match the predicted ODE results.

Figure 2: ODE results when initialized with off-diagonal entries. We focus on the case \(_{G}=_{T}=2\), as that noise level is seen above to be ideal for learning. Additionally, in all cases, \(=0.04,=0.2\). The solid lines are with our approach, while the dashed lines are using the discriminator in Wang et al. . From left to right, we use an initialization of \(0.1,0.01,0.001,0.0001\) for each component of the macroscopic states. It can be seen that our approach outperforms the single-feature discriminator in every case, with the gap becoming larger as the initialization approaches \(0\).

First, since \(\) is an orthonormal matrix, it lives in the Grassmannian \(Gr(d,n)\) of \(d\)-dimensional subspaces of \(^{n}\). Similarly, \( Gr(q,n)\). Our goal is to embed \(\) and \(\) into \(Gr(p,n)\). Once we do this, we can again calculate the macroscopic states we are interested in. To do this, we use the following map:

\[=_{d}\\ _{n-d}_{d}& _{n-p p-d}\\ _{n-d d}&_{p}.\] (15)

This produces a new matrix \(} Gr(p,n)\). We can perform a similar trick with \(\) to obtain a matrix \(\). The important details about this uplifting trick are the following: (1) Due to the construction, we preserve orthonormality of all the matrices, (2) the subspaces of interest are found as the first \(d\) columns of the matrix \(}\) and the first \(q\) columns of the matrix \(}\), and (3) the analysis of the diagonal case is unchanged under this uplifting (In the diagonal case, there is no interaction between the different dimensions, so we ignore the other dimensions. In the non-diagonal case, these additional dimensions only provide minor noise, and so don't affect the training at all).

## 7 Real image subspace learning

In order to demonstrate the practicality of this analysis, we test our approach on the MNIST  and Olivetti Faces  dataset, and compare our approach with the single-feature discriminator from Wang et al. . Here, we include some qualitative results regarding the learned features, and provide a quantitative analysis on the performance differences between the multi-feature and single-feature discriminators. We include the Olivetti Faces results in Figure 4, and the MNIST results can be found in Appendix A.

To perform these visualizations and measure performance, we first perform PCA on the entire dataset and extract the top \(K\) (16 or 36) features. We then use this as an approximation of the true subspace \(\), which allows us to compare the distances. We then track the Grassmann distance between the true and learned subspaces for both the multi-feature and single-feature approaches. The Grassmann distance between two \(d\)-dimensional subspaces of an \(n\)-dimensional space is given by

\[d(,)=(_{i=1}^{d}_{i}^{2})^{1/2},\] (16)

where the \(_{i}\) are the principal angles between the subspaces. Here, a lower distance means a better similarity between the subspaces. If the two matrices are orthonormal, the principal angles are the singular values of the cosine similarity matrix, explicitly connected with the macroscopic states.Figure 3 shows the Grassmann distances for the sequential and multi-feature learning cases on

Figure 3: The graph shows the Grassmann distance over time on the Olivetti Faces dataset, for Oja’s method (Blue) and the GAN model (Orange), as well as the single-feature GAN model (Green). We use the same hyperparameters as all previous experiments, measured with respect to a full PCA decomposition which acts as a surrogate for the true subspace.

the Olivetti Faces dataset. This provides empirical justification on a real dataset, showing first that the phenomenon of faster training identified by the ODE in Figure 1 applies to practical settings as well. Furthermore, due to having no restrictions on off-diagonal entries of the macroscopic states, we see that the results in Figure 2 also apply to practical datasets, since our multi-feature discriminator attains better performance even in less time.

## 8 GANs as a subspace learning algorithm

In the linear setting, GANs attempt to perform subspace learning. However, GANs do not fall into either of the categories introduced earlier. The other subspace learning algorithms all seek to minimize the following loss function

\[J()=_{}[-^{T} ]\] (17)

known as the reconstruction error. This is because the global optima of this loss function is the true subspace itself, and so, we can view this as a prior included in the subspace learning algorithms. GANs do not have such information, and instead seeks to learn the subspace simply through seeing the datapoints. Therefore, we can consider GANs to be a third type of subspace learning algorithm, which we call the generative algorithms. We seek to understand how well the GAN model is able to

Figure 4: We provide results on the Olivetti Faces dataset, a well-known dataset. We show the top 16 learned features for all approaches at 3 stages of training: after the 1st epoch, the 200th epoch, and the end of training. We train all approaches for 500 epochs, equivalent to approximately 50 timesteps of simulated training. It can be clearly seen that while Oja’s method learns quicker than the GAN model, eventually the GAN model outperforms it. Additionally, we see that the features learned by the GAN model are much more diverse and meaningful than those learned by Oja’s method (whose learned features are more similar). For the single-feature GAN model, we can see that the learning is significantly slower, and never approaches anywhere close to the other two results.

learn a subspace compared to the existing subspace learning algorithms. We compare both analytically using the derived ODEs, as well as empirically on synthetic and the MNIST dataset, in order to see under what circumstances GANs learn a subspace at a comparable rate.

### Learned features

Figure 6 in the Appendix compares the features learned by the GAN model to the features learned by Oja's method. Both models are initialized to exactly the same weights, and trained on the same data at the same time, for a single epoch. For the GAN model, we use the same hyperparameters as the previous experiments above. For Oja's method, we used a learning rate of \(0.1\), which experimentally we found to produce the best results. We can clearly see that the features learned by the GAN model are more meaningful and more clearly resemble the true data, while most of the features that Oja's method learns aren't very interpretable. This suggests that because the GAN needs to be able to generate the images, this acts as a form of regularization on what types of features are learned.

## 9 Conclusion

Our investigation into single-layer GAN models through the lenses of online subspace learning and scaling limit analysis has provided valuable insights into their data subspace learning dynamics. By extending our analysis to include multi-feature discriminators, we've unearthed novel phenomena pertaining to the interactions among different features, significantly enhancing learning efficiency. This advantage is particularly pronounced in scenarios of near-zero initialization, where the generator achieves higher maximum and steady-state performances compared to the sequential discriminator. Moreover, the interaction between dimensions enables the generator to closely match variances across dimensions, a feat unattainable in the sequential scenario. In the context of subspace learning, we see that in higher noise levels, the GAN is able to more consistently outperform Oja's method on a wide range of generator, discriminator, and Oja learning rates.

Introducing an uplifting method for analysis in arbitrary dimensionalities enables us to better model uncertainties inherent in real-world subspace modeling. Practical validation on the MNIST and Olivetti Faces datasets reaffirms the applicability of our theoretical findings, underscoring the superiority of overparametrization in single-layer GANs over data availability. This prompts intriguing avenues for research in multi-layer GANs, probing whether similar phenomena persist in more complex architectures. Exploring these directions holds promise for further advancements in the field. Finally, we observe that GAN models excel in acquiring a more meaningful feature basis compared to Oja's method when applied to the real-world datasets, which we attribute to their ability to generate new data samples.