# Real-World Image Variation by Aligning Diffusion Inversion Chain

Yuechen Zhang\({}^{1}\)  Jinbo Xing\({}^{1}\)  Eric Lo\({}^{1}\)  Jiaya Jia\({}^{1,2}\)

\({}^{1}\)The Chinese University of Hong Kong \({}^{2}\)SmartMore

{yczhang21, jbxing, ericlo, leojia}@cse.cuhk.edu.hk

###### Abstract

Recent diffusion model advancements have enabled high-fidelity images to be generated using text prompts. However, a domain gap exists between generated images and real-world images, which poses a challenge in generating high-quality variations of real-world images. Our investigation uncovers that this domain gap originates from a latents' distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called **R**eal-world **I**mage **V**ariation by **AL**ignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Our pipeline enhances the generation quality of image variations by aligning the image generation process to the source image's inversion chain. Specifically, we demonstrate that step-wise latent distribution alignment is essential for generating high-quality variations. To attain this, we design a cross-image self-attention injection for feature interaction and a step-wise distribution normalization to align the latent features. Incorporating these augment processes into a diffusion model allows RIVAL to generate high-quality image variations without further parameter optimization. Our experimental results demonstrate that our proposed approach outperforms existing methods concerning semantic similarity and perceptual quality. This generalized inference pipeline can be easily applied to other diffusion-based generation tasks, such as image-conditioned text-to-image generation and stylization. Project page: https://rival-diff.github.io

## 1 Introduction

Generating real-world image variation is a crucial area of research in computer vision and machine learning, owing to its practical applications in image editing, synthesis, and data augmentation . This task involves the generation of diverse variations of a given real-world image while preserving its semantic content and visual quality. Early methods for generating image variations included texture

Figure 1: Leveraging an image exemplar, our training-free method excels in image generation tasks such as generating (a) image variations, (b) images of a similar style (with conditions).

synthesis, neural style transfer, and generative models [3; 4; 5; 6; 7; 8]. However, these methods are limited in generating realistic and diverse variations from real-world images and are only suited for generating variations of textures or artistic images.

Denoising Diffusion Probabilistic Models (DDPMs) have resulted in significant progress in text-driven image generation [9; 10; 11]. However, generating images that maintain the style and semantic content of the reference remains a significant challenge. Although advanced training-based methods [12; 13; 14; 15] can generate images with novel concepts and styles with given images, they require additional training stages and data. Directly incorporating image as the input condition [11; 16] results in suboptimal visual quality and content diversity compared to the reference input. Besides, they do not support input with text descriptions. A plug-and-play method has not been proposed to generate high-quality, real-world image variations without extra optimization.

Observing the powerful denoising ability of pre-trained DDPMs in recovering the original image from the inverted latent space [17; 18], we aim to overcome the primary challenge by modifying the vanilla latent denoising chain of the DDPM to fit the real-image inversion chain [17; 18; 19]. Despite generating images with the same text condition, a significant domain gap persists between the generated and source images, as depicted in Fig. 2. We identify distribution misalignment as the primary factor that impedes the diffusion model from capturing certain image features from latents in the inversion chain. As illustrated in the right portion of Fig. 2, an inverted latent may differ significantly from the standard Gaussian distribution. This misalignment accumulates during the denoising process, resulting in a domain gap between the generated image and its reference exemplar.

To address this distribution gap problem for generating image variations, we propose an pure inference pipeline called Real-world Image Variation by Alignment (RIVAL). RIVAL is a tunning-free approach that reduces the domain gap between the generated and real-world images by aligning the denoising chain with the real-image inversion chain. Our method comprises two key components: (i) a cross-image self-attention injection that enables cross-image feature interaction in the variation denoising chain, guided by the hidden states from the inversion chain, and (ii) a step-wise latent normalization that aligns the latent distribution with the inverted latent in early denoising steps. Notably, this modified inference process requires no training and is suitable for arbitrary image input.

As shown in Fig. 1, our proposed approach produces visually appealing image variations while maintaining semantic and style consistency with a given image exemplar. RIVAL remarkably improves the quality of image variation generation qualitatively and quantitatively compared to existing methods [11; 16; 20]. Furthermore, we have demonstrated that RIVAL's alignment process can be applied to other text-to-image tasks, such as text-driven image generation with real-image condition [21; 22; 23] and example-based inpainting [24; 25].

This paper makes three main contributions. (1) Using a real-world image exemplar, we propose a novel tunning-free approach to generate high-quality image variations. (2) We introduce an latent alignment process to enhance the quality of the generated variations. (3) Our proposed method offers a promising denoising pipeline that can be applied across various applications.

## 2 Related Works

**Diffusion models with text control** represent advanced techniques for controllable image generation with text prompts. With the increasing popularity of text-to-image diffusion models [11; 16; 26; 10;

Figure 2: Conceptual illustration of the challenges in the real-world image variation. Left: Despite using the same text prompt “a lynx sitting in the grass”, a bias exists between the latent distribution in the vanilla generation chain and the image inversion chain, resulting in a significant domain gap between the denoised images. Right: Visualization of the real-image inverted four-channel latent. The distribution of the latent is biased in comparison to a standard Gaussian distribution.

**Real-world image inversion** is a commonly employed technique in Generative Adversarial Networks (GANs) for image editing and attribute manipulation [6; 29; 30; 31; 32]. It involves inverting images to the latent space for reconstructions. In diffusion models, the DDIM sampling technique  provides a deterministic and approximated invertible diffusion process. Recently developed inversion methods [18; 19] guarantee high-quality reconstruction with step-wise latent alignments. With diffusion inversion, real-image text-driven manipulations can be performed in image and video editing methods [33; 34; 23; 22; 35; 36; 37]. These editing methods heavily rely on the original structure of the input, thus cannot generate free-form variations with the same content as the reference image from the perspective of image generation.

**Image variation generation** involves generating diverse variations of a given image exemplar while preserving its semantic content and visual quality. Several methods have been proposed for this problem, including neural style transfer [5; 38; 39; 40], novel view synthesis [41; 42; 43], and GAN-based methods [7; 44; 32]. However, these methods were limited to generating variations of artistic images or structure-preserved ones and were unsuitable for generating realistic and diverse variations of real-world images. Diffusion-based methods [16; 11; 20] can generate inconsistent image variants by analyzing the semantic contents from the reference while lacking low-frequency details. Recent methods using image prompt to generate similar contents or styles [45; 46; 47; 48] requires additional case-wise tuning or training. Another concurrent approach, MasaCtrl , adopts self-attention injection as guidance for the denoising process. Yet, it can only generate variations with a generated image and fails on real-world image inputs. In contrast, RIVAL leverages the strengths of diffusion chain alignment to generate variations of real-world images.

## 3 Real-World Image Variation by Alignment

In this work, we define the image variation as the construction of a diffusion process \(F\) that satisfies \(D(R,C) D(F_{C}(X,R),C)\), where \(D(,C)\) is a data distribution function with a given semantic content condition \(C\). The diffusion process \(F_{C}(X,R)=G\) generates the image variation \(G\) based on a sampled latent feature \(X\), condition \(C\), and the exemplar image \(R\).

The framework of RIVAL is illustrated in Fig. 3. RIVAL generates an inverted latent feature chain \(\{X_{R}^{0},...,X_{R}^{T}\}\) by inverting a reference exemplar image \(R\) using DDIM inversion . Then we obtain the initial latent \(X_{R}^{T}\) of the inversion chain. Next, a random latent feature \(X_{G}^{T}\) is sampled as the initial latent of the image generation (denoising) chain. In this multi-step denoising chain \(\{X_{G}^{T},...,X_{G}^{0}\}\) for generation, we align the latent features to latents from the inversion chain to obtain perceptually similar generations. The modified step-wise denoising function \(f\) at step \(t\) can be represented abstractly as:

\[X_{G}^{t}=f_{t}(X_{G}^{t+1},X_{R}^{t+1},C).\] (1)

Figure 3: High-level framework of RIVAL. Input exemplar \(R\) is inverted to a noisy latent \(X_{R}^{T}\). An image variation \(G\) is generated from random noise following the same distribution as \(X_{R}^{T}\). For each denoising step \(t\), we interact \(X_{R}^{t}\) and \(X_{G}^{t}\) by self-attention injection and latent alignment.

This function is achieved by performing adaptive cross-image attention in self-attention blocks for feature interaction and performing latent distribution alignment in the denoising steps. The denoising chain can produce similar variations by leveraging latents in the inversion chain as guidance.

### Cross-Image Self-Attention Injection

To generate image variations from an exemplar image \(R\), a feature interaction between the inversion chain and the generation chain is crucial. Previous works [49; 22] have shown that self-attention can efficiently facilitate feature interactions. Similar to its applications [33; 23; 37] in text-driven generation and editing tasks, we utilize intermediate hidden states \(_{R}\) obtained from the inversion chain to modify the self-attention in the generation chain. Specifically, while keeping the inversion chain unchanged, our modified Key-Value features for cross-image self-attention for one denoising step \(t\) in the generation chain are defined as follows:

\[Q=W^{Q}(_{G}),K=W^{K}(^{}_{G}),V=W^{V}(^{ }_{G})\] (2)

\[^{}_{G}=_{G}_{R}& {if }t t_{}\\ _{R}&.\] (3)

We denote \(W^{()}\) as the pre-trained, frozen projections in the self-attention block and use \(\) to represent concatenation in the spatial dimension. Specifically, we adopt an **Attention Fusion** strategy. In early steps (\(t>t_{}\)), we replace the KV values with \(W^{V}(_{R})\), \(W^{K}(_{R})\) using the hidden state \(_{R}\) from the inversion chain. In subsequent steps, we concatenate \(_{R}\) and the hidden state from the generation chain itself \(_{G}\) to obtain new Key-Values. We do not change Q values and maintain them as \(W^{Q}(_{G})\). The proposed adaptive cross-image attention mechanism explicitly introduces feature interactions in the denoising process of the generation chain. Moreover, Attention Fusion strategy aligns the content distribution of the latent features in two denoising chains.

Building upon the self-attention mechanism , we obtain the updated hidden state output as \(^{*}_{G}=QK^{}/}V\), where \(d_{k}\) is the dimensionality of \(K\). Cross-image self-attention and feature injection can facilitate the interaction between hidden features in the generation chain and the inversion chain. It should be noted that the inversion chain is deterministic throughout the inference process, which results in reference feature \(_{R}\) remaining independent of the generation chain.

### Inverted Latent Chain Alignment

The cross-image self-attention injection is a potent method for generating image variations from a vanilla denoising process originating from a standard Gaussian distribution and is demonstrated in . However, as depicted in Fig. 4, direct adaptation to real-world image input is not feasible due to a domain gap between real-world inverted latent chains and the vanilla generation chains. This leads to the attenuation of attention correlation during the calculation of self-attention. We further visualize this problem in Fig. 12. To facilitate the generation of real-world image variations, the pseudo-generation chain (inversion chain) of the reference exemplar can be estimated using the DDIM inversion . Generating an image from latent features \(X^{T}\) using a small number of denoising steps is possible with the use of deterministic DDIM sampling:

\[X^{t-1}=/_{t}} X^{t}+}(_{ t-1}-_{t})_{}X^{t},t,,\] (4)

where step-wise coefficient is set to \(_{t}=-1}\) and \(_{}(X^{t},t,)\) is the pre-trained noise prediction function in one timestep. Please refer to Appendix A for the details of \(_{t}\) and DDIM

Figure 4: Exemplar images and generation results are obtained using the prompt “backpack in the wild”. Cross-image self-attention can generate faithful outputs when a vanilla generation chain is employed as guidance. In the case of real-world images, the generation of faithful variations is dependent on the alignment of the latent.

sampling. With the assumption that noise predictions are similar in latents with adjacent time steps, DDIM sampling can be reversed In a small number of steps \(t[0,T]\), using the equation:

\[X^{t+1}=/_{t}} X^{t}+}(_{t+1 }-_{t})_{}X^{t},t,,\] (5)

which is known as DDIM inversion . We apply DDIM to acquire the latent representation \(X_{R}^{T}\) of a real image \(R\). Nevertheless, the deterministic noise predictions in DDIM cause uncertainty in maintaining a normal distribution \((,)\) of the inverted latent \(X_{R}^{T}\). This deviation from the target distribution causes the attention-attenuation problem  between generation chains of \(R\) and \(G\), leading to the generation of images that diverge from the reference. To tackle this challenge, we find a straightforward distribution alignment on the initial latent \(X_{G}^{T}\) useful, that is,

\[x_{G}^{T} X_{G}^{T}X_{R}^{T} ,^{2}X_{R}^{T},X_{G}^{T}=X_{R}^{T}.\] (6)

In the alignment process, pixel-wise feature elements \(x_{G}^{T} X_{G}^{T}\) in the spatial dimension can be initialized from one of the two sources: (1) an adaptively normalized Gaussian distribution or (2) permutated samples from the inverted reference latent, \((X_{R}^{T})\). Our experiments show that both types of alignment yield comparable performance. This latent alignment strengthens the association between the variation latents and the exemplar's latents, increasing relevance within the self-attention mechanism and thus increasing the efficacy of the proposed cross-image attention.

In the context of initializing the latent variable \(X_{G}^{T}\), further alignment steps within the denoising chain are indispensable. This necessity arises due to the limitations of the Classifier-Free Guidance (CFG) inference proposed in . While effective for improving text prompt guidance in the generation chain, CFG cannot be directly utilized to reconstruct high-quality images through DDIM inversion. On the other hand, the noise scaling of CFG will affect the noise distribution and lead to a shift between latents in two chains, as shown in Fig. 11. To avoid this misalignment while attaining the advantage of the text guidance in CFG, we decouple two inference chains and rescale the noise prediction during denoising inference, formulated as an adaptive normalization :

\[_{}X_{G}^{t},t,=(_{}^{}X_{G}^{t},t,, _{}X_{R}^{t},t,)\] (7)

\[_{}^{}X_{G}^{t},t,=m _{}X_{G}^{t},t,+(1-m)_{ }X_{G}^{t},t,^{*}t>t_{}.\] (8)

\(_{}^{}(X_{G}^{t},t,)\) is the noise prediction using guidance scale \(m\) and unconditional null-text \(C^{*}\). We incorporate this noise alignment approach during the initial stages (\(t>t_{}\)). After that, a vanilla CFG scaling is applied as the content distribution in \(G\) may not be consistent with \(R\). Aligning the distributions at this early stage proves advantageous for aligning the overall denoising process.

## 4 Experiments

We evaluate our proposed pipeline through multiple tasks in Sec. 4.2: image variation, image generation with text and image conditions, and example-based inpainting. Additionally, we provide quantitative assessments and user study results to evaluate generation quality in Sec. 4.3. A series of ablation studies and discussions of interpretability are conducted in Sec. 4.4 to assess the effectiveness of individual modules within RIVAL.

### Implementation Details

Our study obtained a high-quality test set of reference images from the Internet and DreamBooth  to ensure a diverse image dataset. To generate corresponding text prompts \(C\), we utilized BLIP2 . Our baseline model is Stable-Diffusion V1.5. During the image inversion and generation, we employed DDIM sample steps \(T=50\) for each image and set the classifier-free guidance scale \(m=7\) in Eq. (8). We split two stages at \(t_{}=t_{}=30\) for attention alignment in Eq. (3) and latent alignment in Eq. (8). In addition, we employ the shuffle strategy described in Eq. (6) to initialize the starting latent \(X_{G}^{T}\). Experiments run on a single NVIDIA RTX4090 GPU with 8 seconds to generate image variation with batch size 1.

### Applications and Comparisons

**Image variation.** As depicted in Fig. 5, real-world image variations exhibit diverse characteristics to evaluate perceptual quality. Text-driven image generation using the basic Stable Diffusion  cannot use image exemplars explicitly, thus failing to get faithful image variations. While recent image-conditioned generators such as [20; 53; 16] have achieved significant progress for image input,their reliance on encoding images into text space limits the representation of certain image features, including texture, color tone, lighting environment, and image style. In contrast, RIVAL generates image variations based on text descriptions and reference images, harnessing the real-image inversion chain to facilitate latent distribution alignments. Consequently, RIVAL ensures high visual similarity in both semantic content and low-level features. For instance, our approach is the sole technique that generates images with exceedingly bright or dark tones (2nd-row in Fig. 5). More visual comparisons and experimental settings can be found in Appendix C.

**Text-driven image generation.** In addition to the ability to generate images corresponding to the exemplar image and text prompts, we have also discovered that RIVAL has a strong ability to transfer styles and semantic concepts in the exemplar for a casual text-driven image generation. When an inversion with the original text prompt is performed, the alignment on denoising processes can still transfer high-level reference style to the generated images with modified prompts. This process could be directly used in structure-preserved editing. This could be regarded as the same task of MasaCtrl and Plug-and-Play , which utilizes an initialized inverted latent representation to drive text-guided image editing. A comparison is shown in the right part of Fig. 6, here we directly use the inverted latent \(X_{G}^{T}=X_{R}^{T}\) for two chains. We adopt the same interaction starting step \(t=45\) as the MasaCtrl.

Furthermore, in the absence of a structure-preserving prior, it is possible for a user-defined text prompt to govern the semantic content of a freely generated image \(G\). This novel capability is showcased

Figure 5: Real-world image variation. We compare RIVAL with recent competitive methods  conditioned on the same text prompt (the 2nd column) or exemplar image (3rd-5th columns).

in the left part of Fig. 6, where RIVAL successfully generates images driven by text input while maintaining the original style of the reference image. Further, to extend this ability to other image conditions, we adapt RIVAL with ControlNet Fig. 7 by adding condition residuals of ControlNet blocks on the generation chain, as shown in Fig. 7. With RIVAL, we can easily get a style-specific text-to-image generation. For instance, it can produce a portrait painting of a robot adorned in a sailor uniform while faithfully preserving the stylistic characteristics.

**Example-based image inpainting.** When abstracting RIVAL as a novel paradigm of image-based diffusion inference, we can extend this framework to enable it to encompass other image editing tasks like inpainting. Specifically, by incorporating a coarse mask \(M\) into the generation chain, we only permute the inverted latent variables within the mask to initialize the latent variable \(X_{G}^{T}\). During denoising, we replace latents in the unmasked regions with latents in the inversion chain for each step. As shown in Fig. 8, we present a visual comparison between our approach and . RIVAL produces a reasonable and visually harmonious inpainted

Figure 8: RIVAL extended to self-example image inpainting. We use the same image as the example to fill the masked area. Results are compared with a well-trained inpainting SOTA method .

Figure 6: Left: Text-driven image generation results using RIVAL, with the leftmost image as the source exemplar. RIVAL can generate images with diverse text inputs while preserving the reference style. Images noted with * indicate structure-preserved editing, which starts with the same initialized inverted reference latent \(X_{R}^{T}\) as the exemplar. Right: a visual comparison of structure-preserved image editing with recent approaches .

Figure 7: The availability of RIVAL with ControlNet . Two examples are given for each modality of the control condition. Exemplars are shown on the left of each image pair.

### Quantitative Evaluation

We compare RIVAL with several state-of-the-art methods, employing the widely used CLIP-score  evaluation metric for alignments with text and the image exemplar. The evaluation samples are from two datasets: the DreamBooth dataset  and our collected images. In addition, we cluster a 10-color palette using \(k\)-means and calculate the minimum bipartite distance to assess the similarity in low-level color tones. In general, the results in Tab. 1 demonstrate that RIVAL significantly outperforms other methods regarding semantic and low-level feature matching. Notably, our method achieves better text alignment with the real-image condition than the vanilla text-to-image method using the same model. Furthermore, our alignment results are on par with those attained by DALL-E 2 , which directly utilizes the image CLIP feature as the condition.

We conducted a user study to evaluate further the generated images' perceptual quality. This study utilized results generated by four shuffled methods based on the same source. Besides, the user study

   Metric & SD  & ImgVar  & ELITE  & DALL-E 2  & **RIVAL** \\  Text Alignment \(\) & 0.255 \(\) 0.04 & 0.223 \(\) 0.04 & 0.209 \(\) 0.05 & 0.253 \(\) 0.04 & **0.275**\(\) 0.03 \\ Image Alignment \(\) & 0.748 \(\) 0.08 & 0.832 \(\) 0.07 & 0.736 \(\) 0.09 & **0.897**\(\) 0.05 & 0.840 \(\) 0.07 \\ Palette Distance \(\) & 3.650 \(\) 1.30 & 3.005 \(\) 0.86 & 2.885 \(\) 0.81 & 2.102 \(\) 0.83 & **1.674**\(\) 0.63 \\  Real-world Authenticity \(\) & - & 2.982 (4.7\%) & 3.526 (9.6\%) & 1.961 (28.0\%) & **1.530** (61.6\%) \\ Condition Adherence \(\) & - & 3.146 (7.3\%) & 3.353 (5.6\%) & 1.897 (30.6\%) & **1.603** (56.4\%) \\   

Table 1: Quantitative comparisons. We evaluate the quality of image variation regarding feature matching within different levels (color palette, text feature, image feature), highlighted with **best** and second best results. We also report user preference rankings and the first ranked rate.

   Methods & Text & Image & Palette & LPIPS \(\) & Preparation & Inference \\  & Align. \(\) & Align. \(\) & Dist. \(\) & & Time (s) \(\) & Time (s) \(\) \\  PnP  & **0.249** & 0.786 & 1.803 & **0.245** & 200 & 21 \\ MasaCtrl  & 0.226 & 0.827 & 1.308 & 0.274 & 6 & 15 \\
**RIVAL** & 0.231 & **0.831** & **1.192** & **0.245** & 6 & 15 \\   

Table 2: Quantitative comparisons with image editing methods. Both in performance and efficiency (time consumption on one NVIDIA RTX3090 for fair comparison).

Figure 10: Ablation study for different early-alignment strategies with the exemplar in Fig. 5. We list \((t_{ align},t_{ early})\) pairs for each image. All images are generated from the same fixed latent.

Figure 9: Left: a visual example for a module-wise ablation study.

includes a test for image authenticity, which lets users identify the "real" image among generated results. We collected responses of ranking results for each case from 41 participants regarding visual quality. The findings indicate a clear preference among human evaluators for our proposed approach over existing methods [53; 20; 16]. The detailed ranking results are presented in Tab. 1.

We also compare RIVAL with PnP and MasaCtrl for image editing on a representative test set, with additional metrics in Perceptual Similarity  and inference time. Our method presents a competitive result in Tab. 2.

### Ablation Studies

We perform ablation studies to evaluate the efficacy of each module in our design. Additional visual and quantitative results can be found in Appendix D.

**Cross-image self-attention injection** plays a crucial role in aligning the feature space of the source image and the generated image. We eliminated the cross-image self-attention module and generated image variations to verify this. The results in Fig. 9 demonstrate that RIVAL fails to align the feature space without attention injection, generating images that do not correspond to the exemplar.

**Latent alignment.** To investigate the impact of step-wise latent distribution alignment, we remove latent alignments and sampling \(X_{G}^{T}(,)\). An example is shown in Fig. 4 and Fig. 9. Without latent alignment, RIVAL produces inconsistent variations of images, exhibiting biased tone and semantics compared to the source image. Additionally, we perform ablations on attention fusion in Eq. (3) and noise alignment in Eq. (8) during the early steps. Clear color and semantic bias emerge when the early alignment is absent, as shown in Fig. 10. Furthermore, if the values of \(t_{}\) or \(t_{}\) are excessively small, the latent alignment stage becomes protracted, resulting in the problem of over-alignment. Over-alignment gives rise to unwanted artifacts and generates blurry outputs.

**Interpretability.** We add experiments for the interpretability of RIVAL in the following two aspects. _(a) Latent Similarity_. To assess RIVAL components' efficacy regarding the distribution gap, we illustrate the KL divergence of noisy latent between chains A and B, \(X_{A}^{t}\) and \(X_{B}^{t}\) in the generation process, as depicted in Fig. 12 left part. Interactions between different distributions (green) widen the gap, while two generation chains with the same distribution (orange) can get a better alignment by attention interactions. With aligned latent chain alignment and interaction, RIVAL (purple) effectively generates real-world image variations.

_(b) Reference Feature Contribution_. Attention can be viewed as sampling value features from the key-query attention matrix. RIVAL converts self-attention to image-wise cross-attention. When

Figure 11: High CFG artifacts defense. Left: vanilla generation, starting from the DDIM inverted latent. Right: RIVAL generated images, starting from the permuted latents.

Figure 12: RIVAL’s interpretability. Left: KL divergence between latent among two chains across denoising steps. Right: attention score with the reference feature with different alignment strategies.

lations are sourcing from the same distribution (\(X_{G}^{T},X_{R}^{T}(0,I)\)) images retain consistent style and content attributes (as in Fig. 12 left orange). This result is beneficial since we do not require complex text conditions \(c\) to constrain generated images to get similar content and style. For a more direct explanation, we visualize the bottleneck feature contributions of attention score presented in Fig. 12 right part. Reference contribution of the softmax score is denoted as \(_{R}=_{v_{i}_{R}}(W^{Q}_{G}(W^{K}(v_{i }))^{})/_{v_{j}_{G}_{R}}(W^{Q}_{G}( W^{K}(v_{j}))^{})\). As RIVAL adopts early fusion in the early steps, we use 50% as the score in the early steps. Latent initialization (orange) and with early alignments (green) play critical roles in ensuring a substantial contribution of the source feature in self-attention. A higher attention contribution helps in resolving the attention attenuation problem (purple) in the generation process.

### Discussions

**Integration with concept customization.** In addition to its ability to generate image variations from a single source image using a text prompt input for semantic alignment, RIVAL can be effectively combined with optimization-based concept customization techniques, such as DreamBooth , to enable novel concept customization. As illustrated in Fig. 13, an optimized concept can efficiently explore the space of potential image variations by leveraging RIVAL's proficiency in real-world image inversion alignment.

**Limitations and Future Directions.** Despite introducing an innovative technique for crafting high-quality inconsistent image variations, our method is contingent upon a text prompt input, potentially infusing semantic biases affecting image quality. As evidenced in Fig. 14, a prompt like "Pokemon" may lean towards popular choices like "Pikachu" due to training set biases, resulting in a Pikachu-dominated generation. Besides, the base model struggles to generate complex scenes and complicated concepts, e.g., "illustration of a little boy standing in front of a list of doors with butterflies around them in Fig. 14 (c)". This complexity can degrade the inversion chain and widen the domain gap, leading to less accurate results. Future studies might concentrate on refining diffusion models and exploring novel input avenues besides text prompts to mitigate these constraints.

## 5 Conclusion

This paper presents a novel pipeline for generating diverse and high-quality variations of real-world images while maintaining their semantic content and style. Our proposed approach addresses previous limitations by modifying the diffusion model's denoising inference chain to align with the real-image inversion chain. We introduce a cross-image self-attention injection and a step-wise latent alignment technique to facilitate alignment between two chains. Our method exhibits significant improvements in the quality of image variation generation compared to state-of-the-art methods, as demonstrated through qualitative and quantitative evaluations. Moreover, with the novel paradigm of hybrid text-image conditions, our approach can be easily extended to multiple text-to-image tasks.

Figure 14: Fail cases of RIVAL. The exemplar image is on the left for each case.

Figure 13: RIVAL enables seamless customization of optimized novel concepts through text prompt control (<sks>). With various text prompt inputs, we can still generate images preserving the tones, style, and contents of the provided source exemplar (depicted on the left).

**Acknowledgements.** This work is partially supported by the Research Grants Council under the Areas of Excellence scheme grant AoE/E-601/22-R, Hong Kong General Research Fund (14208023), Hong Kong AoE/P-404/18, and Centre for Perceptual and Interactive Intelligence (CPII) Limited under the Innovation and Technology Fund. We are grateful to Jingtao Zhou for the meaningful discussions.