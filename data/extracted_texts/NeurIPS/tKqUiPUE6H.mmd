# Remaining-Useful-Life Prediction and Uncertainty Quantification using LSTM Ensembles for Aircraft Engines

Remaining-Useful-Life Prediction and Uncertainty Quantification using LSTM Ensembles for Aircraft Engines

 Oishi Deb

Department of Engineering Science

University of Oxford, UK

oishideb@robots.ox.ac.uk

&Emmanouil Benetos

School of Electronic Engineering

and Computer Science

Queen Mary University of London, UK

emmanouil.benetos@qmul.ac.uk

&Philip Torr

Department of Engineering Science

University of Oxford, UK

philip.torr@eng.ox.ac.uk

###### Abstract

This paper proposes "LSTM (Long Short Term Memory) Ensemble" technique in building a regression model to predict the Remaining-Useful-Life (RUL) of aircraft engines along with uncertainty quantification, utilising the well-known run-to-failure turbo engine degradation dataset. This paper addressed the overlooked yet crucial aspect of uncertainty estimation in previous research, by revamping the LSTM architecture to facilitate uncertainty estimates, employing Negative Log Likelihood (NLL) as the training criterion. Through a series of experiments, the model demonstrated self-awareness of its uncertainty levels, correlating high confidence with low prediction errors and vice versa. This initiative not only enhances predictive maintenance strategies but also significantly improves the safety and reliability of aviation assets by offering a more nuanced understanding of predictive uncertainties. To the best of our knowledge, this is a pioneering work in this application domain from a non-Bayesian approach.

## 1 Introduction

Predictive maintenance, harnessing machine learning for timely upkeep, has become crucial in engineering and manufacturing, notably reducing costs and enhancing revenue through early aircraft engine degradation detection and accurate Remaining-Useful-Life (RUL) predictions .

While past RUL prediction research for turbo engines, such as  and , has primarily utilized logistic regression and standard Artificial Neural Networks (ANN), a significant gap remains in exploring predictive model uncertainty, especially in safety-critical systems . Deep learning models, despite their varied successes, often display overconfidence in predictions, emphasizing the need for accurate uncertainty estimation .

This paper addresses this gap, developing a regression model using "LSTM Ensemble" to predict turbo engine RUL, leveraging the widely recognized turbo engine degradation dataset . Metrics like RMSE, \(R^{2}\), and Negative Log Likelihood are employed, with the paper aiming to enhance the Deep Ensemble technique by estimating Overall-Predictive and Epistemic uncertainties, offering a comprehensive understanding of predictive uncertainty. Further explanation on Epistemic and Overall-Predictive Uncertainty computation is given in Appendix.

## 2 Literature Review

### Uncertainty Quantification methods

Uncertainty pertains to a state of ambiguity, and this is a phenomenon that machine learning models occasionally grapple with in relation to their predictions. Statistical methodologies, such as Confidence Intervals and Monte Carlo Simulations can be used for gauging this uncertainty.

A Confidence Interval provides a specified range within which the predicted value is likely to fall, as elucidated by Cosma Shalizi . In practice, if a Neural Network model undergoes multiple iterations, and the standard deviation of the output values is calculated, it becomes feasible to deduce the confidence interval in an offline manner. Nonetheless, given that Neural Network models may comprise millions of parameters, executing them repeatedly incurs substantial computational costs. To mitigate this, the strategy of ensembling multiple Neural Networks, also known as Deep Ensemble, was introduced during the training phase. A comprehensive discussion on Deep Ensemble is provided in a subsequent section.

It is crucial to highlight that calculating the confidence interval offline to ascertain uncertainty is fundamentally different from deriving uncertainty directly from the model. When a deep learning model proactively provides an estimate of uncertainty alongside its prediction, it demonstrates an intrinsic awareness of its confidence level. This self-awareness is of paramount importance, especially in safety-critical applications such as autonomous systems, where understanding and acknowledging the model's limitations and uncertainties is essential.

As previously highlighted, conventional Neural Network architectures do not inherently provide an estimate of uncertainty in their predictions. This necessitates a modification and enhancement of existing Neural Network structures to integrate statistical methodologies such as Confidence Intervals and Monte Carlo Simulations, facilitating the extraction of uncertainty from the model.

In the realm of deep learning, the most prominent methods for uncertainty estimation encompass Bayesian and non-Bayesian techniques. Monte Carlo Dropout, a Bayesian technique, stands out as a significant method and is thoroughly discussed in the work of Gal et al. . On the other hand, Deep Ensembles, a non-Bayesian approach detailed by Balaji , is well-known for its computational efficiency. While there are various other methods available for uncertainty estimation mainly from Bayesian point of view, as outlined in Table 6, the paper at hand has opted to employ Deep Ensembles for being the most state-of-the-art model due to its computational efficiency.

**Monte Carlo Dropout:** Probabilistic machine learning encompasses Frequentist and Bayesian strands, with the Monte Carlo dropout technique aligning with the latter . While dropout was initially explored by  and  from a Bayesian perspective,  enhanced it, demonstrating its use as a Bayesian Approximation to integrate over Neural Network model weights. Unlike traditional Neural Networks, Bayesian Neural Networks initialize weights with prior probability distributions, commonly Gaussian, and compute posterior distributions instead of point estimates.

Introduced by  to mitigate overfitting, dropout was traditionally used during training. However,  applied it during both training and testing, yielding non-identical outputs during test time with different dropout values, analogous to Monte Carlo (MC) sampling [12; 17]. This variance in output samples provides uncertainty estimates. Although MC Dropout is a recognized method for uncertainty quantification from a Bayesian standpoint, the subsequent introduction of a non-Bayesian method, 'Deep Ensemble,' by DeepMind  offered an alternative computationally efficient approach, detailed in the following section.

The limited industrial deployment of the MC dropout method is attributed to the computational expense and scalability challenges of Bayesian Inference . In a more recent industrial research study conducted in 2021, MC Dropout was experimented with for Smart Grid Design application , however, it has not been deployed fully yet.

**Deep Ensembles:** Deep Ensembles, a probabilistic model, which is also known as ensembles of neural networks are not as computationally heavy as Bayesian models as they do not use BayesianInference. The concept of the ensemble has been widely used in the regularisation of Neural Network , however, this method also proves useful in uncertainty estimates as mentioned in .

In probabilistic regression models, the output is considered to be a Gaussian or Normal distribution with parameter mean and variance , hence this Distribution is used in calculating the Negative Log Likelihood (NLL). This NLL is then used as a cost function for the Deep Ensembles model. The Deep Ensembles method implements the concept from  in which it modifies the neural network to output the predictive mean and the standard deviation in the output layers, as opposed to classical neural networks for regression tasks where it only outputs a point estimate of the prediction. Confidence intervals can be calculated from the standard deviation to interpret the uncertainty in the prediction model.

Below is the equation of the cost function - Negative Log Likelihood (NLL). The full derivation of the equation is shown in the Appendix.

\[NLL=0.5(2^{2})+0.5^{-2}_{n=1}^{N}(x_{n}-)^{2}\] (1)

where - \(^{2}\): is the variance of the Gaussian distribution; \(\): is the standard deviation of the Gaussian distribution; \(\): is the mean of the Gaussian distribution; \(x_{n}\): is the individual data-points from 1 to N. \(N\): is the total number of data points.

The NLL equation in the Deep Ensemble paper  removes the \(2\) in the first term and adds an additional constant term. Since the aim is to minimise the cost function and as \(2\) is a constant hence removing this term will not make any difference in the minimisation. An additional constant term was added in the equation as mentioned in , which is usually done when working with logarithms to avoid underflow and overflow errors. Negative Log Likelihood is used because it is a proper scoring rule that is widely used for evaluating predictive uncertainty . The proper scoring rule is a function of the probability prediction and the output variable, and this function is minimum when the prediction is well-calibrated .

In other words, NLL is a way of measuring the error of the predictions. This works for models that predict sufficient statistics over some distribution (for example mean and variance for a Gaussian in this case) and then NLL can be calculated for the ground truth under the predicted distribution. The reason this metric is used in models that use uncertainty is because the model predicts the variance/standard deviation and uses it for the NLL, so the prediction of the uncertainty will affect this metric. However, variance/standard deviation is merged with the prediction of the mean, so just from the NLL it is not possible to make any claims about the uncertainty itself.

## 3 Procedure

### Dataset

An open-source dataset  has been used which is available from the NASA repository. The dataset is run-to-failure turbo engine data created using the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) . C-MAPSS was developed by NASA which simulates the real-life controls system for 90,000 pound thrust dual spool and high-bypass ratio turbo engines. This simulation environment was developed to facilitate research in control systems, Engine Health Monitoring (EHM), predictive maintenance etc. The dataset contains data from 4 different fleets of engines and each of the fleet data and information on the number of engine units are on Table 1. Table 2 shows the list of sensor names whose values are recorded for each engine unit in the dataset for each cycle until the engine fails. An engine cycle includes three events: engine start, take-off and landing, and engine shutdown. Take-off and landing are classed as a single event because take-off is always followed by a landing.

Further information on the dataset, data pre-processing and data labelling is provided in the Appendix.

### LSTM (Long Short-Term Memory) Ensemble Model Building

We proposed "LSTM Ensemble" model which is based on the initial work of the Deep Ensemble  technique. We implement LSTM network architecture model with an input size of sequence length and number of features. According to the dataset  there are 25 input features - which are cycle number, 3 operating settings, and 21 sensor values.

#### 3.2.1 Justification for using LSTM Ensemble Model

LSTM Ensemble has been used over other methods such as ARIMA , GARCH , or even simpler RNNs , is because ARIMA and GARCH are primarily linear models and might not capture non-linear patterns effectively, given the fact that our dataset is non-linear. While simple RNNs can handle non-linearities, they are not as efficient as LSTMs in learning from complex, high-dimensional data. LSTMs, being a more advanced form of RNNs, can model complex non-linear relationships in data much better.

Moreover, LSTMs are more robust to noise and non-stationarity in data compared to ARIMA and GARCH. Although ARIMA can handle some level of non-stationarity, LSTMs, especially when used in an ensemble setting, can better adapt to changing patterns in the data without the need for stringent pre-processing like stationarity tests and transformations.

While GARCH models are specifically designed to model volatility clustering and heteroskedasticity in financial time series, LSTMs can also capture these features, especially when trained on large datasets. The ensemble approach further enhances this capability by aggregating insights from multiple models. The Deep Ensemble approach, when combined with LSTMs, can lead to improved prediction accuracy. Each model in the ensemble will capture different aspects of the data, and their aggregation leads to a more accurate and robust model. This is particularly useful in our scenarios because the data contains complex patterns that a single model will not be able to capture effectively.

By integrating LSTMs, which effectively capture temporal dynamics, with Deep Ensembles, the model not only predicts future values but also gives a more reliable estimate of the uncertainty associated with these predictions. This is crucial for decision-making processes in fields where understanding the confidence in predictions is as important as the predictions themselves.

Finally, LSTMs, especially when implemented with efficient computational frameworks, can be scaled for large datasets and real-time analysis, which is a challenge for traditional models like ARIMA and GARCH which require extensive computations for each new prediction.

#### 3.2.2 Training and Inference

The dataset, divided into training and testing files, allocated ten percent of the training data for validation. Table 8 displays the finalised parameter and hyperparameter values, determined after numerous experiments detailed in Table 9. The hyperparameters are tuned/optimized using grid search. Dropout, a regularization technique that probabilistically omits inputs during network training to mitigate overfitting, is applied after each LSTM layer. We implemented a grid search where several models are trained with different dropout rates (such as 0.1, 0.2, 0.3,..., 0.7) and compared their performance on a validation set. 0.2 is found to be the most optimal value for the dropout rate.

It is crucial to note that the dropout used for regularization here differs from Monte Carlo Dropout , utilised for uncertainty quantification. The model's architecture is given in Table 7. The model is developed using Python's Keras  (version 2.3.0) with TensorFlow  (version 1.14.0) as a backend. The entire code is run on CPUs so it is computationally affordable. The model employed "Keras Uncertainty" open-source resources  for the Deep Ensemble model creation.

The Ensemble model was implemented using a custom Class, which takes in two arguments: first a function that creates a Neural Network LSTM model and second the number of neural network models the user wants in the ensemble. The Neural Network model provides two outputs: prediction value and the standard deviation. Since the standard deviation output of each member in the ensemble does not have direct supervision, unlike the prediction value (i.e. the prediction value is supervised by the target \(y\)), the standard deviation is indirectly supervised by the loss function NLL. This has been implemented by passing the standard deviation output to the loss, in which case, the training model does not output the standard deviation directly, but it is included in the loss so it influences the loss correctly.

Evaluation and Discussion

### Evaluating Model with Uncertainty Quantification

The LSTM Ensemble model, tested on four-engine fleets, exhibited the highest error rate with the FD004 dataset, prompting numerous experiments detailed in Table 9 to enhance performance. The row highlighted in blue indicates optimal performance in both prediction and uncertainty estimates. While augmenting the number of Neural Networks (NN) in the ensemble improves performance, an ensemble exceeding three NN elevates the error rate. Upon identifying the top-performing model using the FD004 dataset, it was applied to the other three fleet datasets. Table 10 reveals the best RMSE value as 30.65 for the "FD001" dataset, with a Overall-Predictive standard deviation (uncertainty) of 42.46, placing the prediction value within a \( 42.46\) range of the mean prediction for this dataset. An \(R^{2}\) of 0.53 for the FD001 indicates a commendable fit.

Figure 1 displays the prediction graph, with green denoting ground truth and blue representing prediction, figure 2 illustrates model prediction uncertainty with black and red trends. Ideally, the model should exhibit minimal uncertainty to maximize prediction certainty. However, establishing an uncertainty value threshold or conducting further comparisons necessitates the availability of additional baseline results for analysis.

**NLL for evaluating uncertainty**: The "FD001" dataset yielded a minimum NLL value of 3.95, which, while not a direct measure of uncertainty, facilitates the evaluation of the model's standard deviation during loss function minimization, thereby considering both prediction and its associated uncertainty. In the context of the four datasets, a lower NLL not only signifies more accurate predictions but also reliable uncertainty values. Overall-Predictive uncertainty suffices for assessing a predictive model's suitability, as demonstrated by . However, incorporating Epistemic uncertainty provides nuanced insights from individual Neural Network (NN) models, enhancing decision-making.

For instance, the NN model reflects its confidence in its predictions. In the "FD002" fleet dataset, a high RMSE of 39.69 corresponds with a high Overall-Predictive standard deviation (uncertainty) of 63.62, indicating the model's awareness of its imperfect prediction. Low uncertainty could be perilous, signaling overconfidence in the model. In the "FD002" fleet dataset, it has the most units (260 engines) and the lowest Epistemic standard deviation (33.41) among the four fleet datasets, implying a desirable decrease in Epistemic standard deviation with increasing dataset size, the Overall-Predictive uncertainty remains elevated due to a high error rate. Regarding the "FD004" dataset, it presents an RMSE of 41.91 and a substantial Overall-Predictive standard deviation of 57.97, revealing the model's cognizance of its prediction's limited reliability. While inaccurate, the model's lack of confidence is non-fatal. The absence of an uncertainty value could be hazardous in safety-critical applications, as it leaves users blind to the model's confidence level.

Note that determining the threshold for uncertainty in maintenance decisions is context-dependent and involves risk assessment by the organisations. This paper does not detail specific thresholds but highlights that lower NLL values indicate more reliable uncertainty estimates. Maintenance teams might set thresholds based on acceptable risk levels and the criticality of engine components.

## 5 Conclusion and Further Work

Previous studies on predicting the Remaining Useful Life (RUL) of turbo engines did not address the uncertainty of the predictive model in safety-critical applications. Given the state-of-the-art deep ensemble model, the paper proposes "LSTM Ensemble" a novel approach to address the gap in the literature by implementing the Deep Ensemble method with LSTM to quantify model uncertainty. Significantly, it marks the first integration of LSTM architecture with Deep Ensemble for RUL predictions and uncertainty assessments in aircraft engines, utilizing NASA's engine degradation dataset.

While the LSTM Ensemble method showcased computational efficiency and delivered acceptable results on the FD001 and FD003 datasets, with Root Mean Square Error (RMSE) values of approximately 30 and 33, it encountered challenges with the FD002 and FD004 datasets. These datasets presented larger sizes and higher levels of data noise, resulting in elevated error rates.

This paper lays the groundwork for future research, underscoring the need to augment existing predictive models with methods for providing uncertainty estimates. Prospective directions include efforts to reduce predictive uncertainty and leverage uncertainty estimates to mitigate generalization error. Additionally, uncertainty estimates could be instrumental for Out-Of-Distribution (OOD) detection , signaling increased uncertainty for predictions related to values outside the training set's scope.

Investigations into advanced sequence models, such as Encoder-Decoder architectures  with attention mechanisms , could also prove beneficial. These models have demonstrated their effectiveness in language translation and are applicable to time-series data in safety-critical autonomous systems. The evolution of dropout and model ensemble from regularization techniques to tools for uncertainty estimation opens the door for experimentation with other regularization strategies, such as parameter sharing , in conjunction with existing uncertainty estimation methodologies.

Further inquiries could also consider the amalgamation of deep ensemble with test-time dropout (i.e., MC dropout) for enhanced uncertainty acquisition. Despite the computational demands, this combination could potentially surpass the performance of Deep Ensemble, MC dropout, and classical methods. Viewing uncertainty estimates research through a regularization lens is promising, aiming to diminish generalization error, thereby bolstering model prediction precision and reliability.