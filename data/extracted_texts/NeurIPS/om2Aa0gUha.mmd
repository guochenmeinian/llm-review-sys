# Policy Mirror Descent with Lookahead

Kimon Protopapas  Anas Barakat

Department of Computer Science, ETH Zurich, Switzerland. Contact: kprotopapas@student.ethz.ch, barakat9anas@gmail.com. Most of this work was completed when both authors were affiliated with ETH Zurich, K.P as a Master student and A.B. as a postdoctoral fellow. A.B. is currently affiliated with Singapore University of Technology and Design as a research fellow.

###### Abstract

Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called \(h\)-PMD which incorporates multi-step greedy policy improvement with lookahead depth \(h\) to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor \(\), we show that \(h\)-PMD which generalizes the standard PMD enjoys a faster dimension-free \(^{h}\)-linear convergence rate, contingent on the computation of multi-step greedy policies. We propose an inexact version of \(h\)-PMD where lookahead action values are estimated. Under a generative model, we establish a sample complexity for \(h\)-PMD which improves over prior work. Finally, we extend our result to linear function approximation to scale to large state spaces. Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size.

## 1 Introduction

Policy Mirror Descent (PMD) is a general class of algorithms for solving reinforcement learning (RL) problems. Motivated by the surge of interest in understanding popular Policy Gradient (PG) methods, PMD has been recently investigated in a line of works [25; 53; 29; 20]. Notably, PMD encompasses several PG methods as particular cases via its flexible mirror map. A prominent example is the celebrated Natural PG method. PMD has also close connections to state-of-the-art methods such as TRPO and PPO  which have achieved widespread empirical success [41; 42], including most recently in fine-tuning Large Language Models via RL from human feedback . Interestingly, PMD has also been inspired by one of the most fundamental algorithms in RL: Policy Iteration (PI). While PI alternates between policy evaluation and policy improvement, PMD regularizes the latter step to address the instability issue of PI with inexact policy evaluation.

Policy Iteration and its variants have been extensively studied in the literature, see e.g. [5; 32; 39]. In particular, PI has been studied in conjunction with the lookahead mechanism , i.e. using multi-step greedy policy improvement instead of single-step greedy policies. Intuitively, the idea is that the application of the Bellman operator multiple times before computing a greedy policy leads to a more accurate approximation of the optimal value function. Implemented via Monte Carlo Tree Search (MCTS), multi-step greedy policy improvement2 contributed to the empirical success of some of themost impressive applications of RL including AlphaGo, AlphaZero and MuZero [44; 46; 45; 40]. Besides this practical success, a body of work [12; 13; 49; 52; 51] has investigated the role of lookahead in improving the performance of RL algorithms, reporting a convergence rate speed-up when enhancing PI with \(h\)-step greedy policy improvement with a reasonable computational overhead.

In this work, we propose to cross-fertilize the PMD class of algorithms with multi-step greedy policy improvement to obtain a novel class of PMD algorithms enjoying the benefits of lookahead. Our contributions are as follows:

* We propose a novel class of algorithms called \(h\)-PMD enhancing PMD with multi-step greedy policy updates where \(h\) is the depth of the lookahead. This class collapses to standard PMD when \(h=1\,.\) When the stepsize parameter goes to infinity, we recover the PI algorithm with multiple-step greedy policy improvement previously analyzed in . When solving a Markov Decision Process problem with \(h\)-PMD, we show that the value suboptimality gap converges to zero geometrically with a contraction factor of \(^{h}\). This rate is faster than the standard convergence rate of PI and the similar rate for PMD which was recently established in  for \(h 2\). This rate improvement requires the computation of a \(h\)-step lookahead value function at each iteration which can be performed using planning methods such as tree-search. These results for exact \(h\)-PMD are exposed in Sec. 4, when value functions can be evaluated exactly.
* We examine the inexact setting where the \(h\)-step action value functions are not available. In this setting, we propose a Monte Carlo sample-based procedure to estimate the unknown \(h\)-step lookahead action-value function at each state-action pair. We provide a sample complexity for this Monte Carlo procedure, improving over standard PMD thanks to the use of lookahead. Larger lookahead depth translates into a better sample complexity and a faster suboptimality gap convergence rate. However, this improvement comes at a more intensive computational effort to perform the lookahead steps. Sec. 5 discusses this inexact setting and the aforementioned tradeoff.
* We extend our results to the function approximation setting to address the case of large state spaces where tabular methods are not tractable. In particular, we design a \(h\)-PMD algorithm where the \(h\)-step lookahead action-value function is approximated using a linear combination of state-action feature vectors in the policy evaluation step. Under linear function approximation and using a generative sampling model, we provide a performance bound for our \(h\)-PMD algorithm with linear function approximation. Our resulting sample complexity only depends on the dimension of the feature map space instead of the size of the state space and improves over prior work analyzing PMD with function approximation.
* We perform simulations to verify our theoretical findings empirically on the standard DeepSea RL environment from Deep Mind's _bsuite_. Our experiments illustrate the convergence rate improvement of \(h\)-PMD with increasing lookahead depth \(h\) in both the exact and inexact settings.

## 2 Preliminaries

**Notation.** For any integer \(n\{0\}\), the set of integers from \(1\) to \(n\) is denoted by \([n]\). For a finite set \(\), we denote by \(()\) the probability simplex over the set \(\,.\) For any \(d\), we endow the Euclidean space \(^{d}\) with the norm \(\|\|_{}\) defined for every \(v^{d}\) by \(\|v\|_{}:=_{i[d]}|v_{i}|\) where \(v_{i}\) is the \(i\)-th coordinate. The relative interior of a set \(\) is denoted by \(()\,.\)

**Markov Decision Process (MDP).** We consider an infinite horizon discounted Markov Decision Process \(=(,,r,,P,)\) where \(\) and \(\) are finite sets of states and actions respectively with cardinalities \(S=||\) and \(A=||\) respectively, \(P:()\) is the Markovian probability transition kernel, \(r:\) is the reward function, \((0,1)\) is the discount factor and \(()\) is the initial state distribution. A randomized stationary policy is a mapping \(:()\) specifying the probability \((a|s)\) of selecting action \(a\) in state \(s\) for any \(s,a.\) The set of all such policies is denoted by \(\). At each time step, the learning agent takes an action \(a\) with probability \((a|s)\), the environment transitions from the current state \(s\) to the next state \(s^{}\) with probability \(P(s^{}|s,a)\) and the agent receives a reward \(r(s,a)\).

**Value functions and optimal policy.** For any policy \(\), we define the state value function \(V^{}:\) for every \(s\) by \(V^{}(s)=_{}[_{t=0}^{}^{t}r(s_{t},a_{t})|s _{0}=s]\,.\) The state-action value function \(Q^{}:\) can be similarly defined for every \(s,a\) by \(Q^{}(s,a)=_{}[_{t=0}^{}^{t}r(s_{t},a_{t} )|s_{0}=s,a_{0}=a]\) where \(_{}\) is the expectation over the state action Markov chain \((s_{t},a_{t})\) induced by the MDP and the policy \(\) generating actions. The goal is to find a policy \(\) maximizing \(V^{}\). A classic result shows that there exists an optimal deterministic policy \(^{}\) maximizing \(V^{}\) simultaneously for all the states . In this work, we focus on searching for an \(\)-optimal policy, i.e. a policy \(\) satisfying \(\|V^{}-V^{}\|_{}\)  where \(V^{}:=V^{^{}}\).

**Bellman operators and Bellman equations.** We will often represent the reward function \(r\) by a vector in \(^{SA}\) and the transition kernel by the operator \(P^{SA S}\) acting on vectors \(v^{S}\) (which can also be seen as functions defined over \(\)) as follows: \((Pv)(s,a)=_{s^{}}P(s^{}|s,a)v(s^{})\) for any \(s,a\). We further define the mean operator \(M^{}:^{SA}^{S}\) mapping any vector \(Q^{SA}\) to a vector in \(^{S}\) whose components are given by \((M^{}Q)(s)=_{a}(a|s)Q(s,a)\,,\) and the maximum operator \(M^{}:^{SA}^{S}\) defined by \((M^{}Q)(s)=_{a}Q(s,a)\) for any \(s,Q^{SA}\). Using these notations, we introduce for any given policy \(\) the associated expected Bellman operator \(^{}:^{S}^{S}\) defined for every \(V^{S}\) by \(^{}V:=M^{}(r+ PV)\) and the Bellman optimality operator \(:^{S}^{S}\) defined by \(V:=M^{}(r+ PV)\) for every \(V^{S}\). Using the notations \(r^{}:=M^{}r\) and \(P^{}:=M^{}P\), we can also simply write \(^{}V=r^{}+ P^{}V\) and \(V=_{}^{}V\) for any \(V^{S}\).

The Bellman optimality operator \(\) and the expected Bellman operator \(^{}\) for any given policy \(\) are \(\)-contraction mappings w.r.t. the \(\|\|_{}\) norm and hence admit unique fixed points \(V^{}\) and \(V^{}\) respectively. In particular, these operators satisfy the following inequalities: \(\|^{}V-V^{}\|_{}\|V-V^{}\|_{}\) and \(\|V-V^{}\|_{}\|V-V^{}\|_{}\) for any \(V^{S}\). Moreover, we recall that for \(=(1,1,,1)^{S}\) and any \(c_{ 0},V^{S}\), we have \(^{}(V+c\,)=^{}V+ c\,\) and \((V+c\,)=V+ c\,\). We recall that the set \((V^{}):=\{:^{}V^{}=V^{ }\}\) of \(1\) -step greedy policies w.r.t. the optimal value \(V^{}\) coincides with the set of stationary optimal policies.

## 3 A Refresher on PMD and PI with Lookahead

### PMD and its Connection to PI

Policy Gradient methods consist in performing gradient ascent updates with respect to an objective \(_{}[V^{}(s)]\) w.r.t. the policy \(\) parametrized by its values \((a|s)\) for \(s,a\) in the case of a tabular policy parametrization. Interestingly, policy gradient methods have been recently shown to be closely connected to a class of Policy Mirror Descent algorithms by using dynamical reweighting of the Bregman divergence with discounted visitation distribution coefficients in a classical mirror descent update rule using policy gradients. We refer the reader to section 4 in  or section 3.1 in  for a detailed exposition. The resulting PMD update rule is given by

\[_{s}^{k+1}_{_{s}()}\{ _{k} Q_{s}^{_{k}},_{s}-D_{}(_{s},_{s}^{k}) \}\,,\] (PMD)

where we use the shorthand notations \(_{s}^{k}=_{k}(|s)\) and \(_{s}=(|s)\), where \((_{k})\) is a sequence of positive stepsizes, \(Q_{s}^{_{k}}^{A}\) is a vector containing state action values for \(_{k}\) at state \(s\), and \(D_{}\) is the Bregman divergence induced by a mirror map \(:\) such that \(()\,,\) i.e. for any \(,^{},s,\)

\[D_{}(_{s},_{s}^{})=(_{s})-(_{s}^{})- (_{s}^{}),_{s}-_{s}^{}\,,\]

where we suppose throughout this paper that the function \(\) is of Legendre type, i.e. strictly convex and essentially smooth in the relative interior of \(\) (see , section 26). The mirror map choice gives rise to a large class of algorithms. Two special cases are noteworthy: (a) When \(\) is the squared \(2\)-norm, the Bregman divergence is the squared Euclidean distance and the resulting algorithm is a projected \(Q\)-ascent and (b) when \(\) is the negative entropy, the corresponding Bregman divergence is the Kullback-Leibler (KL) divergence and (PMD) becomes the popular Natural Policy Gradient algorithm (see e.g. , section 4 for details).

Now, using our operator notations, one can observe that the (PMD) update rule above is equivalent to the following update rule (see Lemma A.1 for a proof),

\[_{k+1}_{}\{_{k}^{}V ^{_{k}}-D_{}(,_{k})\}\,,\] (1)

where \(D_{}(,_{k})^{S}\) is a vector whose components are given by \(D_{}(_{s},_{s}^{k})\) for \(s\). Note that the maximization can be carried out independently for each state component.

**Connection to Policy Iteration.** As previously noticed in the literature [53; 20], PMD can be interpreted as a'soft' PI. Indeed, taking infinite stepsizes \(_{k}\) in PMD (or a null Bregman divergence) immediately leads to the following update rule:

\[_{k+1}_{}\{^{}V^{_{k }}\}\,,\] (PI)

which corresponds to synchronous Policy Iteration (PI). The method alternates between a policy evaluation step to estimate the value function \(V^{_{k}}\) of the current policy \(_{k}\) and a policy improvement step implemented as a one-step greedy policy with respect to the current value function estimate.

### Policy Iteration with Lookahead

As discussed in , Policy Iteration can be generalized by performing \(h 1\) greedy policy improvement steps at each iteration instead of a single step. The resulting \(h\)-PI update rule is:

\[_{k+1}_{}\{^{} ^{h-1}V^{_{k}}\}\,,\] ( \[h\] -PI)

where \(h\{0\}\,\). The \(h\)-greedy policy \(^{k+1}\) w.r.t \(V^{_{k}}\) selects the first optimal action of a non-stationary \(h\)-horizon optimal control problem. It can also be seen as the \(1\)-step greedy policy w.r.t. \(^{h-1}V^{_{k}}\,\). In the rest of this paper, we will denote by \(_{h}(V)\) the set of \(h\)-step greedy policies w.r.t. any value function \(V^{S}\), i.e. \(_{h}(V):=_{}^{} ^{h-1}V=\{:^{}^{h-1}V=^{h}V\}\). Notice that \(_{1}(V)=(V)\). Overall, the \(h\)-PI method alternates between finding a \(h\)-greedy policy and estimating the value of this policy. Interestingly, similarly to PI, a \(h\)-greedy policy guarantees monotonic improvement, i.e. \(V^{^{}} V^{}\) component-wise for any \(,^{}_{h}(V^{})\,\). Moreover, since \(^{h}\) is a \(^{h}\)-contraction, it can be shown that the sequence \((\|V^{}-V^{_{k}}\|_{})_{k}\) is contracting with coefficient \(^{h}\,\). See section 3 in  for further details about \(h\)-PI.

_Remark 3.1_.: While the iteration complexity always improves with larger depth \(h\) (see Theorem 3 in ), each iteration becomes more computationally demanding. As mentioned in , when the model is known, the \(h\)-greedy policy can be computed with Dynamic Programming (DP) in linear time in the depth \(h\).

Another possibility is to implement a tree-search of depth \(h\) starting from the root state \(s\) to compute the \(h\)-greedy policy in deterministic MDPs. We refer to  for further details. Sampling-based tree search methods such as Monte Carlo Tree Search (MCTS) (see ) can be used to circumvent the exponential complexity in the depth \(h\) of tree search methods.

## 4 Policy Mirror Descent with Lookahead

In this section we propose our novel Policy Mirror Descent (PMD) algorithm which incorporates \(h\)-step lookahead for policy improvement: \(h\)-PMD.

### \(h\)-PMD: Using \(h\)-Step Greedy Updates

Similarly to the generalization of PI to \(h\)-PI discussed in the previous section, we obtain our algorithm \(h\)-PMD by incorporating lookahead to PMD as follows:

\[_{k+1}=_{}\{_{k}^{} ^{h-1}V^{_{k}}-D_{}(,_{k})\}\,.\] ( \[h\] -PMD)

Notice that we recover (1), i.e. (PMD), by setting \(h=1\,\). We now provide an alternative way to write the \(h\)-PMD update rule which is more convenient for its implementation. We introduce a few additional notations for this purpose. For any policy \(,\) let \(V^{}_{h}:=^{h-1}V^{}\) for any integer \(h\,1\,\). Consider the corresponding action value function defined for every \(s,a\) by:

\[Q^{}_{h}(s,a):=r(s,a)+(PV^{}_{h})(s,a)\,.\] (2)

Using these notations, it can be easily shown that the \(h\)-PMD rule above is equivalent to:

\[^{k+1}_{s}=_{}\{_{k} Q^{_{ k}}_{h}(s,),_{s}-D_{}(_{s},^{k}_{s})\}\,.\] ( \[h\] -PMD')

Before moving to the analysis of this scheme, we provide two concrete examples for the implementation of \(h\)-PMD' corresponding to the choice of two standard mirror maps.

(1) _Projected \(Q_{h}\)-ascent_. When the mirror map \(\) is the squared \(2\)-norm, the corresponding Bregman divergence is the squared Euclidean distance and \(h\)-PMD' becomes

\[_{s}^{k+1}=_{()}(_{s}^{k}+_{k}Q_{h}^{_{ k}}(s,))\,,\] (3)

for every \(s\) and \(_{()}\) is the projection operator on the simplex \(()\).

(2) _Multiplicative \(Q_{h}\)-ascent_. When the mirror map \(\) is the negative entropy, the Bregman divergence \(D_{}\) is the Kullback-Leibler divergence and for every \((s,a)\),

\[_{k+1}(a|s)=_{k}(a|s)Q_{h}^{_{k}}(s,a))}{Z_{k}(s)} \,, Z_{k}(s):=_{a}_{k}(a|s)(_{k}Q_{h}^{_{ k}}(s,a))\,.\] (4)

**Connection to AlphaZero.** Before switching gears to the analysis, we point out an interesting connection between \(h\)-PMD and the successful class of AlphaZero algorithms [46; 45; 40] which are based on heuristics.

It has been shown in  that AlphaZero can be seen as a regularized policy optimization algorithm, drawing a connection to the standard PMD algorithm (with \(h=1\)). We argue that AlphaZero is even more naturally connected to our \(h\)-PMD algorithm. Indeed, the \(Q\) values in [16, Eq. (7) p. 3] correspond more closely to the lookahead action value function \(Q_{h}^{}\) approximated via tree search instead of the standard \(Q^{}\) function with \(h=1\). This connection clearly delineates the dependence on the lookahead depth (or tree search depth) \(h\) which was not clear in . We have implemented a version of our \(h\)-PMD algorithm with Deep Mind's MCTS implementation (see section D.6 and the code provided for details). Conducting further experiments to compare our algorithm to AlphaZero on similar large scale settings would be interesting. We are not aware of any theoretical convergence guarantee for AlphaZero which relies on many heuristics. In contrast, our \(h\)-PMD algorithm enjoys theoretical guarantees that we establish in the next sections.

### Convergence Analysis for Exact \(h\)-Pmd

Using the contraction properties of the Bellman operators, it can be shown that the suboptimality gap \(\|V^{_{k}}-V^{*}\|_{}\) for PI iterates converges to zero at a linear rate with a contraction factor \(\), regardless of the underlying MDP. This instance-independent convergence rate was generalized to PMD in . In this section, we establish a linear convergence rate for the suboptimality value function gap of the exact \(h\)-PMD algorithm with a contraction factor of \(^{h}\) where \(h\) is the lookahead depth. We assume for now that the value function \(V^{_{k}}\) and a greedy policy \(_{k+1}\) in (\(h\)-PMD) can be computed exactly. These assumptions will be relaxed in the next section dealing with inexact \(h\)-PMD.

**Theorem 4.1** (Exact \(h\)-Pmd).: _Let \((c_{k})\) be a sequence of positive reals and let the stepsize \(_{k}\) in (\(h\)-PMD) satisfy \(_{k}}\|_{_{h}(V^{_{k}})}D_{ }(,_{k})\|_{}\) where we recall that \(_{h}(V^{_{k}})\) is the set of greedy policies with respect to \(^{h-1}V^{_{k}}\) and that the minimum is computed component-wise. Initialized at \(_{0}()\), the iterates \((_{k})\) of (\(h\)-PMD) with \(h\{0\}\) satisfy for every \(k\),_

\[\|V^{}-V^{_{k}}\|_{}^{hk}(\|V^{}-V^{_{0} }\|_{}+_{t=1}^{k}}{^{ht}} ).\]

A few remarks are in order regarding this result:

* Compared to , our new algorithm achieves a faster \(^{h}\)-rate where \(h\) is the depth of the lookahead. It should be noted that the approach in  is claimed to be optimal over all methods that are guaranteed to increase the probability of the greedy action at each timestep. Adding lookahead to the policy improvement step circumvents this restriction.
* Unlike prior work [53; 25] featuring distribution mismatch coefficients which can scale with the size of the state space, our convergence rate does not depend on any other instance-dependent quantities. This is thanks to the analysis which circumvents the use of the performance difference lemma similarly to .
* Choosing \(c_{t}=^{2h(t+1)}c_{0}\) for a positive constant \(c_{0}\) for every integer \(t\), the bound becomes (after bounding the geometric sum): \(\|V^{}-V^{_{k}}\|_{}^{hk}(\|V^{}-V^{_{0}} \|_{}+}{(1-)^{2}})\). As \(c_{0} 0\) we recover the linear convergence result of \(h\)-PI established in .

* This faster rate comes at the cost of a more complex value function computation at each iteration. However, our experiments suggest that the benefits of the faster convergence rate greatly outweigh the extra cost of computing the lookahead, in terms of both overall running time until convergence and sample complexity (in the inexact case). See section 7 and Appendix D for evidence.
* The cost of computing the adaptive stepsizes is typically simply that of computing a Bregman divergence between two policies. See Appendix A.3, D.4 for a more detailed discussion.
* We defer the proof of Theorem 4.1 to Appendix A. Our proof highlights the relationship between \(h\)-PMD and \(h\)-PI through the explicit use of Bellman operators. Notice that even in the particular case of \(h=1\), our proof is more compact compared to the one in  (see sec. 6 and Lemma A.1 to A.3 therein for comparison) using our convenient notations.

## 5 Inexact and Stochastic \(h\)-Policy Mirror Descent

In this section, we discuss the case where the lookahead action value \(Q_{h}^{_{k}}\) defined in (2) is unknown. We propose a procedure to estimate it using Monte Carlo sampling and we briefly discuss an alternative using a tree search method. The resulting inexact \(h\)-PMD update rule for every \(s\) is

\[_{s}^{k+1}=*{argmax}_{_{s}()}\{ _{k}_{h}^{_{k}}(s,),_{s}-D_{}(_{s}, _{s}^{k})\},\] (5)

where \(_{h}^{_{k}}\) is the estimated version of the lookahead action value \(Q_{h}^{_{k}}\) induced by policy \(_{k}\,.\) We conclude this section by providing a convergence analysis for inexact \(h\)-PMD and discussing its sample complexity using the proposed Monte Carlo estimator.

### Monte Carlo \(h\)-Greedy Policy Evaluation

Estimating the lookahead action value function \(Q_{h}^{}\) for a given policy \(\) involves solving a \(h\)-horizon planning problem using samples from the MDP. Our estimation procedure combines a standard planning method with Monte Carlo estimation under a generative model of the MDP. We give an algorithmic description of the procedure below, and defer the reader to Appendix B.1 for a precise definition of the procedure. Applying the recursive algorithm below with \(k:=h\) returns an estimate for the action value function \(Q_{h}^{}(s,a)\) at a given state-action pair \((s,a)\).

``` Procedure\(Q(k,s,a,)\) if\(k=1\)then return\(r(s,a)+_{i=1}^{M}^{}(s_{i}^{})\) (where \(s_{i}^{}(|s,a)\) for \(i[M]\)) else return\(r(s,a)+_{i=1}^{M}_{a^{}}Q(k-1,s_{i}^{ },a^{},)\) (where \(s_{i}^{}(|s,a)\) for \(i[M]\)) endif ```

**Algorithm 1** Lookahead Q-function Estimation via Monte Carlo Planning

Note that the base case of the recursion estimates the value function using Monte Carlo rollouts, see Appendix B.1.

_Remark 5.1_.: Notice that actions are exhaustively selected in our procedure like in a planning method. Using bandit ideas to guide Monte Carlo planning (see e.g. ) would be interesting to investigate for more efficiency. We leave the investigation of such selective action sampling and exploration for future work. In practice, lookahead policies are often computed using tree search techniques .

_Remark 5.2_.: When \(h=1\), the procedure collapses to a simple Monte Carlo estimate of the Q-value function \(Q^{_{k}}\) at each iteration \(k\) of the \(h\)-PMD algorithm.

### Analysis of Inexact \(h\)-Pmd

The next theorem is a generalization of Theorem 4.1 to the inexact setting in which the lookahead function \(Q_{h}^{_{k}}\) can be estimated with some errors at each iteration \(k\) of \(h\)-PMD.

**Theorem 5.3** (Inexact \(h\)-Pmd).: _Suppose there exists \(b_{+}\) s.t. \(\|_{h}^{_{k}}-Q_{h}^{_{k}}\|_{} b\) where the maximum norm is over both the state and action spaces. Let \(}_{h}=*{argmax}_{}\ M^{}_{h}^{ _{k}}\) and let \((c_{k})\) be a sequence of positive reals and let the stepsize \(_{k}\) in (\(h\)-PMD) satisfy \(_{k}}\|_{}_{h}}D_{}(, _{k})\|_{}\)__where we recall that the minimum is computed component-wise. Initialized at \(_{0}()\), the iterates \((_{k})\) of inexact (\(h\)-PMD) with \(h\{0\}\) satisfy for every \(k\),_

\[\|V^{}-V^{_{k}}\|_{}^{hk}(\|V^{}-V^{_{0}} \|_{}+_{t=1}^{k}}{^{ht}}) +)}\,.\]

The proof of this result is similar to the proof of Theorem 4.1 and consists in propagating the error \(b\) in the analysis. We defer it to Appendix B.

As can be directly seen from this formulation, the \(^{h}\) convergence rate from the exact setting generalizes to the inexact setting. Therefore, higher lookahead depths lead to faster convergence rates. Another improvement with relation to  is that the additive error term has a milder dependence on the effective horizon \(1-\): the term \()}\) is smaller for all values of \(h 2\) than \(}\). Larger lookahead depths yield a smaller asymptotic error in terms of the suboptimality gap.

We are now ready to establish the sample complexity of inexact \(h\)-PMD under a generative model using Theorem 5.3 together with concentration results for our Monte Carlo lookahead action value estimator.

**Theorem 5.4** (Sample complexity of \(h\)-PMD).: _Assume that inexact \(h\)-PMD is run for a number of iterations \(K>()})\), using the choice of stepsize defined by the sequence \((c_{k}):=(^{2h(k+1)})\). Additionally, suppose we are given a target suboptimality value \(>0\), and a probability threshold \(>0\). Finally, assume the lookahead value function \(_{h}^{_{k}}\) is approximated at each iteration with the Monte Carlo estimation procedure described in section 5.1, with the following parameter values: \(M_{0}=}(}{(1-)^{4}(1-^{h})^{ 2}^{2}})\) and for all \(j[h],M_{j}=M=}( ^{2}})\,.\) Then, with probability at least \(1-\), the suboptimality at all iterations \(k\) satisfy the following bound:_

\[\|V^{}-V^{_{k}}\|_{}^{hk}(\|V^{}-V^{_{0} }\|_{}+}{(1-)(1-^{h})})+\] (6)

_Using this procedure, \(h\)-PMD uses at most \(KM_{0}HS+hKMSA\) samples in total. The overall sample complexity of the inexact \(h\)-PMD is then given by \(}((1-)^{6}(1-^{h})^{2}}+ (1-)^{7}})\) where the notation \(}\) hides at most polylogarithmic factors._

Compared to Theorem 5.1 in , the dependence on the effective horizon improves by a factor of the order of \(1/(1-)\) for \(h>A^{-1}(1-)^{-1}\) thanks to our Monte Carlo lookahead estimator. Their sample complexity is of order \(}((1-)^{8}})\), whereas for \(h>\) ours becomes \(}((1-)^{7}})\).

Our sample complexity does not depend on quantities such as the distribution mismatch coefficient used in prior work [53; 25], which may scale with the state space size.

## 6 Extension to Function Approximation

In MDPs with prohibitively large state action spaces, we represent each state-action pair \((s,a)\) by a feature vector \((s,a)^{d}\) where typically \(d SA\) and where \(:^{d}\) is a feature map, also represented as a matrix \(^{SA d}\). Assuming that this feature map holds enough "information" about each state action pair (more rigorous assumptions will be provided later), it is possible to approximate any value function using a matrix vector product \(\), where \(^{d}\). In this section, we propose an inexact \(h\)-PMD algorithm using action value function approximation.

### Inexact \(h\)-PMD with Function Approximation

Using the notations above, approximating \(Q_{h}^{_{k}}\) by \(_{k}\), the inexact \(h\)-PMD update rule becomes:

\[_{s}^{k+1}_{_{s}()}\{_{k} (_{k})_{s},_{s}-D_{}(_{s},_{s}^{k}) \}\,.\] (7)

We note that the policy update above can be implicit to avoid computing a policy for every state. It is possible to implement this algorithm using our Monte Carlo planning method in a similar fashion to : use the procedure in section 5.1 to estimate \(Q_{h}^{_{k}}(s,a)\) for all state action pairs \((s,a)\) in some set \(\), and use these estimates as targets for a least squares regression to approximate \(Q_{h}^{_{k}}\) by \(_{k}\) at time step \(k\). See Appendix C for further details. We conclude this section with a convergence analysis of \(h\)-PMD with linear function approximation.

### Convergence Analysis of \(h\)-PMD with Linear Function Approximation

Our analysis follows the approach of . We make the following standard assumptions.

**Assumption 6.1**.: The feature matrix \(^{SA d}\) where \(d SA\) is full rank.

**Assumption 6.2** (Approximate Universal value function realizability).: There exists \(_{}>0\) s.t. for any \(\), \(_{^{d}} Q_{h}^{}-_{} _{}\,\).

We defer a discussion about Assumption 6.2 to Appendix C.2.

**Theorem 6.3** (Convergence of \(h\)-PMD with linear function approximation).: _Let \((_{k})\) be the sequence of iterates produced by \(h\)-PMD with linear function approximation, run for \(K\) iterations, starting with policy \(_{0}()\), using stepsizes defined by \((c_{k})\). Assume that targets \(_{h}^{_{k}}\) were computed using the procedure described in sec. 5.1 such that with probability at least \(1-,\; k\; K\,, z \) the targets satisfy \(_{h}^{_{k}}(z)-Q_{h}^{_{k}}(z)\).3_

_Then, there exists a choice of \(\) for which the policy iterates satisfy at each iteration:_

\[ V^{}-V^{_{k}}_{}^{hk}( V^{ }-V^{_{0}}_{}+_{t=1}^{k}}{1-})+\,+2\,(1+)_{ {FA}}}{(1-)(1-^{h})}\,.\]

The proof of Theorem 6.3 can be found in Appendix C.3. Notice that our performance bound does not depend on the state space size like in sec. 5 and depends on the dimension \(d\) of the feature space instead. We should notice though that the choice of the set \(\) is crucial for this result and given by the Kiefer-Wolfowitz theorem  which guarantees that \(=(d^{2})\) as discussed in . Moreover, our method for function approximation yields a sample complexity that is instance independent. Existing results for PMD with function approximation  depend on distribution mismatch coefficients which can scale with the size of the state space.

In terms of computational complexity, at each iteration, \(h\)-PMD uses \(O((MA)^{h})\) elementary operations where \(M\) is the size of the minibatch of trajectories used, \(A\) is the size of the action space and \(h\) is the lookahead depth. This computational complexity which scales exponentially in \(h\) is inherent to tree search methods. Despite this seemingly prohibitive computational scaling, tree search has been instrumental in practice for achieving state of the art results in some environments  and modern implementations (notably using GPU-based search on vectorized environments) make tree search much more efficient in practice.

## 7 Simulations

We conduct simulations to investigate the effect of the lookahead depth on the convergence rate and illustrate our theoretical findings. We run the \(h\)-PMD algorithm for different values of \(h\) in both exact and inexact settings on the DeepSea environment from DeepMind's _bsuite_ using a grid size of 64 by 64, and a discount factor \(=0.99\). Additional experiments are provided in Appendix D. Our codebase where all our experiments can be replicated is available here: https://gitlab.com/kimon.protopapa/pmd-lookahead.

**Exact \(h\)-PMD.** We run the exact \(h\)-PMD algorithm for 100 iterations for increasing values of \(h\) using the KL divergence. Similar results were observed for the Euclidean divergence. We tested two different stepsize schedules: (a) in dotted lines in Fig. 1 (left), \(_{k}\) equal to its lower bound in sec. 4, with the choice \(c_{k}:=^{2h(k+1)}\) (note the dependence on \(h\)); and (b) in solid lines, \(_{k}\) identical stepsize schedule across all values of \(h\) with \(c_{k}:=^{2(k+1)}\) to isolate the effect of the lookahead. We clearly observe under both stepsize schedules that \(h\)-PMD converges in fewer iterations when lookahead value functions are used instead of regular \(Q\) functions.

**Inexact \(h\)-PMD.** In this setting, we estimated the value function \(Q_{h}\) using the vanilla Monte Carlo planning procedure detailed in section 5 in a stochastic variant of the DeepSea environment, and all the parameters except for \(h\) were kept identical across runs. As predicted by our results, \(h\)-PMD converges in fewer iterations when \(h\) is increased (see Fig. 1 (middle)). We also observed in our simulations that \(h\)-PMD uses less samples overall (see Appendix D for the total number of samples used at each iteration), and usually converges after less overall computation time (see Fig. 1 (right)). We also refer the reader to Appendix D where we have performed additional experiments with a larger lookahead depth \(h=100.\) In this case, the algorithm converges in a single iteration. This is theoretically expected as computing the lookahead values with very large \(h\) boils down to computing the optimal values like in value iteration with a large number of iterations. We also performed additional experiments in continuous control tasks to illustrate the general applicability of our algorithm (see Fig. 8 in Appendix D).

_Remark 7.1_.: (**Choice of the lookahead depth \(h\).**) The lookahead depth is a hyperparameter of the algorithm and can be tuned similarly to other hyperparameters such as the step size of the algorithm. Of course, the value would depend on the environment and the structure of the reward at hand. Sparse and delayed reward settings will likely benefit from lookahead with larger depth values. We have performed several simulations with different values of \(h\) for each environment setting and the performance can potentially improve drastically with a better lookahead depth value (see also appendix D for further simulations). In addition, we observe that larger lookahead depth is not always better: see Fig. 7 in the appendix for an example where large lookahead depth becomes slower and does not perform better. Note that in this more challenging practical setting the best performance is not obtained for higher values of \(h\): intermediate values of \(h\) perform better. This illustrates the tradeoff in choosing the depth \(h\) between an improved convergence rate and the computational cost induced by a larger \(h\). We believe further investigation regarding the selection of the depth parameter might be useful to further improve the practical performance of the algorithm.

## 8 Related Work

**Policy Mirror Descent and Policy Gradient Methods.** Motivated by their empirical success [41; 42], the analysis of PG methods has recently attracted a lot of attention [1; 6; 21; 7; 53; 25; 20]. Among these works, a line of research focused on the analysis of PMD [53; 25; 28; 29; 20; 3] as a flexible algorithmic framework, and its particular instances such as the celebrated natural policy gradient [21; 54]. In particular, for solving unregularized MDPs (which is our main focus), Xiao  established linear convergence of PMD in the tabular setting with a rate depending on the instance dependent distribution mismatch coefficients, and extended their results to the inexact setting using a generative model. Similar results were established for the particular case of the natural policy gradient algorithm [6; 21]. More recently, Johnson, Pike-Burke, and Rebeschini  showed a dimension-free

Figure 1: Suboptimality value function gap for \(h\)-PMD in the exact (left) and inexact (middle/right) settings, plotted against iterations in the exact case (left) and against both iterations (middle) and runtime (right) in the inexact case. 16 runs performed for each \(h\), mean in solid line and standard deviation as shaded area. In dotted lines (left), the step size \(_{k}\) is equal to its lower bound in sec. 4, with the choice \(c_{k}:=^{2h(k+1)}\) (note the dependence on \(h\)) and in solid lines, the step size \(_{k}\) is set using an identical stepsize schedule across all values of \(h\) with \(c_{k}:=^{2(k+1)}\) to isolate the effect of the lookahead. Notice that higher values of \(h\) still perform better even in terms of runtime.

rate for PMD with exact policy evaluation using adaptive stepsizes, closing the gap with PI. Notably, they further show that this rate is optimal and their analysis inspired from PI circumvents the use of the performance difference lemma which was prevalent in prior work. We improve over these results using lookahead and we extend our analysis beyond the tabular setting by using linear function approximation. To the best of our knowledge, the use of multi-step greedy policy improvement in PMD and its analysis are novel in the literature.

**Policy Iteration with Multiple-Step Policy Improvement.** Multi-step greedy policies have been successfully used in applications such as the game of Go and have been studied in a body of works . In particular, multi-step greedy policy improvement has been studied in conjunction with PI in . Efroni et al.  introduced \(h\)-PI which incorporates \(h\)-lookahead to PI and generalized existing analyses on PI with 1-step greedy policy improvement to \(h\)-step greedy policies. However, their work requires access to an \(h\)-greedy policy oracle, and the \(h\)-PI algorithm is unstable in the stochastic setting even for \(h=1\) because of potentially large policy updates. We address all these issues in the present work with our \(h\)-PMD algorithm. Winnicki and Srikant  showed that a first-visit version of a PI scheme using a single sample path for policy evaluation converges to the optimal policy provided that the policy improvement step uses lookahead instead of a \(1\)-step greedy policy. Unlike our present work, the latter work assumes access to a lookahead greedy policy oracle. More recently, Alegre et al.  proposed to use lookahead for policy improvement in a transfer learning setting. Extending Generalized Policy Improvement (GPI)  which identifies a new policy that simultaneously improves over all previously learned policies each solving a specific task, they introduce \(h\)-GPI which is a multi-step extension of GPI. We rather consider a lookahead version of PMD including policy gradient methods instead of PI. We focus on solving a single task and provide policy optimization guarantees in terms of iteration and sample complexities.

**Lookahead-Based Policy Improvement with Tree Search.** Lookahead policy improvement is usually implemented via tree search methods . Recently, Dalal et al.  proposed a method combining policy gradient and tree search to reduce the variance of stochastic policy gradients. The method relies on a softmax policy parametrization incorporating a tree expansion. Our general \(h\)-PMD framework which incorporates a different lookahead policy improvement step also brings together PG and tree search methods. Morimura et al.  designed a method with a mixture policy of PG and MCTS for non-Markov Decision Processes. Grill et al.  showed that AlphaZero as well as several other MCTS algorithms compute approximate solutions to a family of regularized policy optimization problems which bear similarities with our \(h\)-PMD algorithm update rule for \(h=1\) (see sec. 3.2 therein).

## 9 Conclusion

In this work, we introduced a novel class of PMD algorithms with lookahead inspired by the success of lookahead policies in practice. We have shown an improved \(^{h}\)-linear convergence rate depending on the lookahead depth in the exact setting. We proposed a stochastic version of our algorithm enjoying an improved sample complexity. We further extended our results to scale to large state spaces via linear function approximation with a performance bound independent of the state space size. Our paper offers several interesting directions for future research. A possible avenue for future work is to investigate the use of more general function approximators such as neural networks to approximate the lookahead value functions and scale to large state-action spaces. Recent work for PMD along these lines  might be a good starting point. Enhancing our algorithm with exploration mechanisms (see e.g. the recent work ) is an important future direction. Our PMD update rule might offer some advantage in this regard as was recently observed in the literature . Constructing more efficient fully online estimators for the lookahead action values using MCTS and designing adaptive lookahead strategies to select the tree search horizon  are promising avenues for future work. Our algorithm brings together two popular and successful families of RL algorithms: PG methods and tree search methods. Looking forward, we hope our work further stimulates research efforts to investigate practical and efficient methods enjoying the benefits of both classes of methods.