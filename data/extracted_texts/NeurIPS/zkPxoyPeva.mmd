# Gradient Estimation For Exactly-\(k\) Constraints

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The exactly-\(k\) constraint is ubiquitous in machine learning and scientific applications, such as ensuring that the sum of electric charges in a neutral atom is zero. However, enforcing such constraints in machine learning models while allowing differentiable learning is challenging. In this work, we aim to provide a "cookbook" for seamlessly incorporating exactly-\(k\) constraints into machine learning models by extending a recent gradient estimator from Bernoulli variables to Gaussian and Poisson variables, utilizing constraint probabilities. We show the effectiveness of our proposed gradient estimators in synthetic experiments, and further demonstrate the practical utility of our approach by training neural networks to predict partial charges for metal-organic frameworks, aiding virtual screening in chemistry. Our proposed method not only enhances the capability of learning models but also expands their applicability to a wider range of scientific domains where satisfaction of constraints is crucial.

## 1 Introduction

The exactly-\(k\) constraint, that is, the sum of \(n\) variables is equal to \(k\), is not only ubiquitous in machine learning such as learning sparse features (Chen et al., 2018) and discrete variational auto-encoders (Rolfe, 2016), but also critical to scientific applications such as charge-neutral scenarios in computational chemistry (Raza et al., 2020) and count-aware cell type deconvolution (Liu et al., 2023). In the former cases, the variables are binary while in the latter cases, the variables are continuous or integer, depending on the applications. Such tasks can involve optimizing the expectation of an objective function with respect to variables satisfying the exactly-\(k\) constraint, whose distributions are parameterized by neural networks. This optimization problem is challenging since the expectation can be intractable and thus gradient estimation is required. Existing estimators include score-function-based ones that suffer from high variance and reparameterization-based ones that require relaxation and can be highly biased Xie and Ermon (2019). A recently proposed gradient estimator (Ahmed et al., 2023) outperforms the aforementioned estimators by leveraging constraint probability and avoiding relaxations. Still, it is limited to the exactly-\(k\) constraint on Bernoulli variables.

In this work, we aim to carry out a systematic study of gradient estimation for exactly-\(k\) constraints over Bernoulli, Gaussian, and Poisson variables, the three most commonly used distributions in modeling. We show that on the forward pass, the constrained distributions have closed-form representations, and thus exact sampling from the constrained distribution can be achieved. On the backward pass, we reparameterize the gradient of the loss function with respect to the samples as a function of the expected marginals of the constrained distributions. Further, we find that under certain loss functions, the expected loss under the constrained distribution has a closed-form solution. That is, in such cases, we are able to train models under the exactly-\(k\) constraint without any gradientestimations. We include synthetic experiments to evaluate the bias and variance of our proposed gradient estimation on Gaussian and Poisson variables. We also include an experiment on predicting partial charges for metal-organic frameworks, where our gradient estimation, when combined with an ensemble method, achieves state-of-the-art prediction performance.

## 2 Problem Statement and Motivation

We consider models described by the equations

\[=h_{}(), p_{}(_{ i}z_{i}=k),}=f_{}(,),\] (1)

where \(\) and \(}\) denote feature inputs and target outputs, respectively, \(h_{}:\) and \(f_{}:\) are smooth, parameterized maps. \(\) are parameters inducing a distribution over the latent vector \(\) and the induced distribution \(p_{}()\) is defined as \(p_{}()=_{i=1}^{n}p_{_{i}}(z_{i})\), with \(p_{_{i}}(z_{i})\) as defined in Table 1, where \((z;,^{2})\) denotes the density of a Gaussian distribution with mean \(\) and variance \(^{2}\) at \(z\). An exactly-\(k\) constraint is enforced over the distribution \(p_{}()\), inducing a conditional distribution \(p_{}(_{i}z_{i}=k):=p_{}()[ _{i}z_{i}=k]/p_{}(_{i}z_{i}=k)\) where the denominator denotes the constraint probability \(p_{}(_{i}z_{i}=k)\). This formulation is general and it can subsume neural network models that integrate the exactly-\(k\) constraint in the input, output, or latent space, which we visualize in Figure 1.

The training of such models is performed by optimizing an expected loss to learn parameters \(=(,)\) in Equation 1 as below,

\[L(,;)=_{ p_{}(| _{i}z_{i}=k)}[(f_{}(,),)] }=h_{}(),\] (2)

where \(:^{+}\) is a point-wise loss function. However, the standard auto-differentiation can not be directly applied to the expected loss due to two main obstacles. First, for the gradient of \(L\) w.r.t. parameters \(\) in the decoder network \(f_{}\) defined as

\[_{}L(,;)=_{ p_{}(|_{i}z_{i}=k)}[_{}f_{}(,)^{ }_{}}(},)]\] (3)

with \(=f_{}(,)\) being decoding of a latent sample \(\), the expectation does not allow closed-form solution in general and requires Monte-Carlo estimations by sampling \(\) from the constrained distribution \(p_{}(_{i}z_{i}=k)\). The same issue arises in the gradient of \(L\) w.r.t. parameters \(\) in the encoder network defined as

\[_{}L(,;)=_{}h_{}( )^{}_{}L(,;).\] (4)

The second obstacle lies in the computation of the gradient of \(L\) w.r.t. the encoder as in Equation 4 defined as that requires to compute \(_{}\), a derivative that is not well-defined and requires gradient estimation for updating \(\). In a recent work , a gradient estimator called SIMPLE is proposed to tackle these two issues by _exactly sampling_ from the constrained distribution and using _marginals_ as a proxy to samples respectively, where SIMPLE is able to outperform both score-function-based gradient estimators and reparameterization-based ones. However, SIMPLE is limited to Bernoulli variables and whether the same gradient estimation can be extended to a larger distribution family remains underexplored.

 
**Variable** & **Parameterized Distribution** \\  Bernoulli & \(p_{_{i}}(z_{i}=1)=(_{i})\) \\ \(p_{_{i}}(z_{i}=0)=1-(_{i})\) \\ Gaussian & \(p_{_{i}}(z_{i})=(z_{i};_{i},_{i}^{2})\) with \(_{i}=(_{i},_{i})\) \\ Poisson & \(p_{_{i}}(z_{i})=_{i}^{z_{i}}e^{-_{i}}/z_{i}!\) \\  

Table 1: Parameterization of the three distribution settings.

Figure 1: Model formulation under an exactly-\(k\) constraint.

Gradient Estimation for Exactly-\(k\)

We tackle the gradient estimation for the exactly-\(k\) constraints by solving the aforementioned two subproblems: (**P1**) how to sample exactly from the constrained distribution \(p_{}(_{i}z_{i}=k)\) and (**P2**) how to estimate \(_{}L(,;)\). By combining solutions to these two problems, we manage to train the constrained model in an end-to-end manner. Table 3 in the Appendix presents a summary of the key components in the proposed gradient estimation.

### Exact Sampling

For both Gaussian and Poisson variables, we find that their constrained distributions conform to commonly seen closed-form distributions and thus allow efficient sampling by using built-in sampling algorithms in deep learning frameworks. We formally state our findings below.

**Proposition 1** (Gaussian Constrained Distribution).: _Given \(=(z_{1},,z_{n})^{T}\) with \(z_{i}(_{i},_{i}^{2})\), the constrained distribution \(p(_{j=1}^{n}z_{j}=k)\) is equivalent to an \(n-1\) dimensional multivariate normal distribution with mean \(^{n-1}\) and covariance matrix \(}^{(n-1)(n-1)}\) with their entries defined as below,_

\[_{i}=_{j=1}^{n-1}([i=j]_{i }^{2}-^{2}_{j}^{2}}{_{i=1}^{n}_{i}^{2}} )(c+}{_{j}^{2}})\ \ }_{i,j}=_{i}^{2}-^{ 2})^{2}}{_{j=1}^{n}_{i}^{2}}&i=j\\ -^{2}_{i}^{2}}{_{i=1}^{n}_{i}^{2}}&i j .\]

**Proposition 2** (Poisson Constrained Distribution).: _Given \(=(z_{1},,z_{n})^{T}\) with \(z_{i} Poisson(_{i})\), the constrained distribution \(p(_{j=1}^{n}z_{n}=k)\) is equivalent to a multinomial distribution with parameter \(k\) and probabilities \(}{_{j=1}^{n}_{j}},,}{_{j=1} ^{n}_{j}}\)._

### Conditional Marginals as Proxy

For estimating gradient \(_{}L(,;)\), we follow an approximation adopted by Ahmed et al. (2023); Niepert et al. (2021) where the main intuition is to use the conditional marginals \(()\{p_{}(z_{j}_{i} z_{i}=k)\}_{j=1}^{n}\) as a proxy for samples \(\), that is,

\[_{}L(,;)_{ }()_{}(,;),\] (5)

where the sample \(\) is reparameterized to be a function of the conditional marginals and is assumed to be \(_{}}\). In the case of Gaussian and Poisson variables, the reparameterization is achieved by using the expected marginals conditioning on the exactly-\(k\) constraint, that is, \(()\) with \(_{j}=_{p_{}(z_{j}_{i}z_{i}=k)}[z_{j}]\) as a function of the parameters \(\). For succinctness, we refer to \(\) as expected marginals. The remaining question is how to obtain the expected marginals \(\). We find that the expected marginals in both cases have closed-form solutions.

**Proposition 3** (Gaussian Conditional Marginal).: _Given \(=(z_{1},,z_{n})^{T}\) with \(z_{i}(_{i},_{i}^{2})\), the conditional marginal \(p(z_{i}_{j=1}^{n}z_{j}=k)\) follows a univariate Gaussian distribution with mean \(_{i}=_{i}+^{2}}{_{j=1}^{n}_{j}^{2}}(k -_{j=1}^{n}_{j})\) and variance \(_{i}^{2}=_{i}^{2}-^{2})^{2}}{_{j=1}^{ n}_{j}^{2}}\), that is, \(_{i}=_{i}\)._

**Proposition 4** (Poisson Conditional Marginal).: _Given \(=(z_{1},,z_{n})^{T}\) with \(z_{i} Poisson(_{i})\), the conditional marginal of \(p(z_{i}_{j=1}^{n}z_{n}=k)\) follows a binomial distribution with parameter \(k\) and probability \(}{_{j=1}^{n}_{j}}\), with \(_{i}=}{_{j=1}^{n}_{j}}\)._

### Closed-form Expected Loss

This section focuses on some special cases where the expected loss in Equation 2 has a closed-form solution and thus no gradient estimation is needed. We find that when the decoder \(f_{}\) is an identity function, that is, \(=\), the expected loss defined over Gaussian variables has a closed-form solution when the element-wise loss is L1 loss or L2 loss. The same conclusion holds for Poisson variables with the element-wise loss being L2 loss. We refer the readers to Proposition 5 and Proposition 6 respectively in Appendix for details.

## 4 Experiments

We evaluate our proposed gradient estimation on both synthetic settings and a scientific application.

Synthetic Experiments.We analyze our proposed gradient estimators for Gaussian and Poisson variables using three metrics, bias, variance, and averaged error, in synthetic settings where the ground truth gradients can be obtained by taking derivatives of the closed-form expected loss as stated in Section 3.3. The distance between the estimated and the ground truth gradient vectors is measured by the cousin distance defined as 1 - cosine similarity. We further compare with a random estimation as a baseline. Bias and variance results are presented in Figure 2 with additional details and results presented in Section C in the Appendix, where our proposed gradient estimator is able to achieve significantly lower bias, variances as well as averaged errors than the baseline, indicating its effectiveness.

Partial Charge Predictions for Metal-Organic Frameworks.Metal-organic frameworks (MOFs) represent a class of materials with a wide range of applications in chemistry and materials science. Predicting properties of MOFs, such as partial charges on metal ions, is essential for understanding their reactivity and performance in chemical processes. However, it is challenging due to the complex interactions between metal ions and ligands and the requirement that the predictions need to satisfy the charge neutral constraint, that is, an exactly-zero constraint.

We adopt the same model as in Raza et al. (2020) where the charges are assumed to be Gaussian variables and the element loss is L1 loss, and address this problem by training the model leveraging our observation in Section 3.3 and using gradients of the expected loss. We further observe that using an ensemble of such models gives predictions that also satisfy the charge-neutral constraint. The prediction performance of our two proposed approaches is presented in Table 2, compared with baseline approaches reported by Raza et al. (2020). Results show that training using closed-form expected loss achieves the same performance as MPNN(variance) which is considered to be the strongest baseline approach, and when further combined with the ensemble method, our approach achieves significantly better predictions.

## 5 Conclusion

In this work, we provide an extensive study on differentiable learning under exactly-\(k\) constraints given various distribution families. We further provide empirical studies of our proposed gradient estimation on both synthetic experiments and a scientific application.

  
**Method** & **MAD** \\ (charge neutrality enforcement) & mean (std) \\  Constant Prediction & 0.324 (7e-3) \\ Element-mean (uniform) & 0.154 (2e-3) \\ Element-mean (variance) & 0.153 (2e-3) \\ MPNN (uniform) & 0.026 (8e-4) \\ MPNN (variance, reproduced) & 0.0251 (8e-4) \\ Closed-form (ours) & 0.0251 (6e-4) \\ Closed-form + Ensemble (ours) & **0.0235 (5e-4)** \\   

Table 2: Comparison on prediction performance.

Figure 2: A comparison of our gradient estimation and random estimations on bias and variance.