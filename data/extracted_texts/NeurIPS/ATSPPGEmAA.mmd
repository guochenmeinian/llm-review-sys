# Optimal Batched Best Arm Identification

Tianyuan Jin\({}^{1}\), Yu Yang\({}^{3}\), Jing Tang\({}^{2}\), Xiaokui Xiao\({}^{1}\), Pan Xu\({}^{3}\)

\({}^{1}\)National University of Singapore

\({}^{2}\)The Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\)Duke University

{tianyuan,kkxiao}@nus.edu.sg,jingtang@ust.hk, {yu.yang,pan.xu}@duke.edu

###### Abstract

We study the batched best arm identification (BBAI) problem, where the learner's goal is to identify the best arm while switching the policy as less as possible. In particular, we aim to find the best arm with probability \(1-\) for some small constant \(>0\) while minimizing both the sample complexity (total number of arm pulls) and the batch complexity (total number of batches). We propose the three-batch best arm identification (Tri-BBAI) algorithm, which is the first batched algorithm that achieves the optimal sample complexity in the asymptotic setting (i.e., \( 0\)) and runs in \(3\) batches in expectation. Based on Tri-BBAI, we further propose the almost optimal batched best arm identification (Opt-BBAI) algorithm, which is the first algorithm that achieves the near-optimal sample and batch complexity in the non-asymptotic setting (i.e., \(\) is finite), while enjoying the same batch and sample complexity as Tri-BBAI when \(\) tends to zero. Moreover, in the non-asymptotic setting, the complexity of previous batch algorithms is usually conditioned on the event that the best arm is returned (with a probability of at least \(1-\)), which is potentially unbounded in cases where a sub-optimal arm is returned. In contrast, the complexity of Opt-BBAI does not rely on such an event. This is achieved through a novel procedure that we design for checking whether the best arm is eliminated, which is of independent interest.

## 1 Introduction

Multi-armed bandit (MAB) is a fundamental model in various online decision-making problems, including medical trials , online advertisement , and crowdsourcing . These problems typically involve a bandit with multiple arms, where each arm follows an unknown distribution with a mean value. At each time step, the learner selects an arm, and receives a reward sample drawn from the chosen arm's distribution. Best arm identification (BAI) aims to identify the arm with the highest mean reward, which can be approached with a fixed budget or a fixed confidence level [14; 4; 19].

In this paper, we assume that there is a unique best arm and study BAI with fixed confidence. Specifically, we consider a set \([n]=\{1,2,,n\}\) of \(n\) arms, where each arm \(i\) is associated with a reward distribution having a mean value \(_{i}\). Without loss of generality, for a bandit instance denoted by \(=\{_{1},_{2},,_{n}\}\), we assume that \(_{1}>_{2}_{n}\). At each time step \(t\), the learner selects an arm and observes a sample drawn independently from the chosen arm's distribution. In the fixed confidence setting, the learner aims to correctly identify the best arm (arm \(1\) in our context) with a probability of at least \(1-\), where \(>0\) is a pre-specified confidence parameter. Meanwhile, the learner seeks to minimize the total number of arm pulls, also known as the sample complexity.

Denote by \(a^{*}()=*{arg\,max}_{i}_{i}\) the best arm for an arbitrary bandit instance \(\). Let \(()=\{ a^{*}( ) 1\}\) be a set of models that have a different best arm from the model \(\), and \(_{k}=\{_{+}^{n}_{i=1}^{n}w_{i }=1\}\) be the probability simplex. Garivier and Kaufmann  showed that for bandits with reward distributions that are continuously parameterized by their means, the number \(N_{}\) of pullsby any algorithm that returns the best arm with a probability of at least \(1-\) is bounded by

\[_{ 0}_{}[N_{}]}{(1/)}  T^{*}(),\] (1.1)

where \(T^{*}()\) is defined according to

\[T^{*}()^{-1}:=_{_{k}}(_{ ()}(_{i=1}^{n}w_{i} d(_{i}, _{i}))),\] (1.2)

and \(d(_{i},_{i})\) is the Kullback-Leibler (KL) divergence of two arms' distributions with means \(_{i}\) and \(_{i}\), respectively. We say that an algorithm achieves the asymptotically optimal sample complexity if it satisfies

\[_{ 0}_{}[N_{}]}{(1/)}  T^{*}().\] (1.3)

The well-known Track-and-Stop algorithm  solves the BAI problem with asymptotically optimal sample complexity. However, it is a fully sequential algorithm, which is hard to be implemented in parallel. The learner in such an algorithm receives immediate feedback for each arm pull, and adjusts the strategy for the next arm selection based on the previous observations. Unfortunately, this sequential approach may not be feasible in many real-world applications. For instance, in medical trials, there is typically a waiting time before the efficacy of drugs becomes observable, making it impossible to conduct all tests sequentially. Instead, the learner needs to group the drugs into batches and test them in parallel. Similarly, in crowdsourcing, the goal is to identify the most reliable worker for a specific task by testing candidates with a sequence of questions. Again, there is often a waiting time for workers to finish all the questions, necessitating the grouping of workers into batches and conducting parallel tests. In such scenarios, the results of pulling arms are only available at the end of each batch .

Motivated by these applications, we study the problem of batched best arm identification (BBAI). In BBAI, we are allowed to pull multiple arms in a single batch, but the results of these pulls are revealed only after the completion of the batch. The objective is to output the best arm with a high probability of at least \(1-\), while minimizing both the sample complexity (total number of pulls) and the batch complexity (total number of batches). This leads to the following natural question:

_Can we solve the BBAI problem with an asymptotically optimal sample complexity_

_and only using a constant number of batches?_

Furthermore, the aforementioned results hold only in the limit as the confidence parameter \(\) approaches zero, which may provide limited practical guidance since we often specify a fixed confidence level parameter \(>0\). To address this, some algorithms  have been proposed to solve the BAI problem with finite confidence. Specifically, for some universal constant \(C\), these algorithms satisfy that with probability \(1-\),

\[[N_{}] O_{i>1}^{2}} _{i}^{-1}\] (1.4)

where \(_{i}=_{1}-_{i}\). In addition, Jamieson et al.  demonstrated that for two-armed bandits, the term \(^{-1}}{_{2}^{2}}\) is optimal as \(_{2} 0\), where \(_{2}\) is assumed to be the gap between the best arm and the second best arm. In the context of the batched setting, Jin et al.  proposed an algorithm that achieves the sample complexity in (1.4) within \((^{*}(n)(1/_{2}))\) batches, where \(^{*}(n)\) is the iterated logarithm function1. Furthermore, Tao et al.  proved that for certain bandit instances, any algorithm that achieves the sample complexity bound shown in (1.4) requires at least \(^{-1}}{_{2}^{-1}}\) batches. It should be noted that the batched lower bound proposed by Tao et al.  assumes \(\) as a constant, making it inapplicable in the asymptotic setting. Therefore, an additional question that arises is:

_Can we achieve the optimal sample complexity in (1.3) and (1.4) adaptively, taking into account the specified confidence level parameter \(\), while minimizing the number of batches?_In this work, we provide positive answers to both of the aforementioned questions. Specifically, the **main contributions** of our paper can be summarized as follows:

* We propose Tri-BBAI (Algorithm 1) that returns the best arm with a probability of at least \(1-\) by pulling all arms for a total number of at most \(T^{*}()(1/)\) times when \( 0\). Tri-BBAI employs three batches in expectation when \( 0\). Therefore, Tri-BBAI achieves the optimal sample within constant batches. As a comparison, Track-and-Stop  also achieves the optimal sample complexity but requires solving the right-hand side of (1.2) after each arm pull, resulting in a batch complexity of the same order as the sample complexity, which is a significant computational overhead in practice.
* Built upon Tri-BBAI, we further propose Opt-BBAI (Algorithm 2) that runs in \(((1/_{2}))\) expected batches and pulls at most \(_{i>1}_{i}^{-2}(n_{i}^{-1})\) expected number of arms for finite confidence \(\). It is also important to note that Opt-BBAI achieves the same sample and expected batch complexity as Tri-BBAI asymptotically when \( 0\). Moreover, for the finite confidence case, this sample complexity matches (1.4) within a \(()\) factor and matches the optimal batched complexity within a \(()\) factor. To the best of our knowledge, Opt-BBAI is the first batched bandit algorithm that can achieve the optimal asymptotic sample complexity and the near-optimal non-asymptotic2 sample complexity adaptively based on the assumption on \(\). * Notably, in the non-asymptotic setting, the complexity of earlier batch algorithms  typically depends on the event of returning the best arm, which occurs with a probability of at least \(1-\). However, this complexity could potentially become unbounded if a sub-optimal arm is returned instead. Unlike these algorithms, the complexity of Opt-BBAI is not contingent on such an event. This is made by employing an innovative procedure to verify if the best arm is eliminated, a method that holds its independent interest. To the best of our knowledge, Opt-BBAI is the first algorithm that achieves the optimal asymptotic sample complexity while providing the optimal non-asymptotic sample complexity within logarithm factors.
* We also conduct numerical experiments3 to compare our proposed algorithms with the optimal sequential algorithm Track-and-Stop , and the batched algorithm Top-k \(\)-Elimination  on various problem instances. The results indicate that our algorithm significantly outperforms the Track and Stop method in terms of batch complexity, while its sample complexity is not much worse than that of Track and Stop. Additionally, our algorithm demonstrates a notable improvement in sample complexity compared to , while exhibiting similar batch complexity. 
For ease of reference, we compare our results with existing work on batched bandits in Table 1.

    &  &  \\   & Sample complexity & Batch complexity & Sample complexity & Batch complexity \\  Karnin et al.  & Not optimal & \(( n})\) & \((_{i>1}^{-1})}{_{1}^{2}})\) & \(( n})\) \\ Jin et al.  & Not optimal & \((})\) & \((_{i>1}^{-1})}{_{1}^{2}})\) & \((^{}(n)})\) \\ Wang et al.  & Optimal & \(O()\) & \(O(nH()^{1/w}_{})\) & - \\ Agrawal et al.  & Optimal & \(()^{}}{m}\) (\(m=o(^{-1})\)) & \(-\) & \(-\) \\ Karpov et al.  & – & – & \((_{i>1}^{-1})}{_{1}^{2}})\) & \((})\) \\ Lower bound  & – & – & \((_{i>1}^{-1})}{_{1}^{2}})\) & \(((1/_{2}))\) \\
**Tri-BBAI** (Our Algorithm 1) & Optimal & 3 & \(-\) & \(-\) \\
**Opt-BBAI** (Our Algorithm 2) & Optimal & 3 & \((_{i>1}^{-1})}{_{1}^{2}})\) & \((})\) \\   

Table 1: Comparison of sample and batch complexity of different algorithms. In the asymptotic setting (i.e., \( 0\)), the sample complexity of an algorithm is optimal if it satisfies the definition in (1.3). The field marked with “–” indicates that the result is not provided. The sample complexity presented for  is conditioned on the event that the algorithm returns the best arm, which can be unbounded when it returns a sub-optimal arm with certain (non-zero) probability (see Remark 4.4 for more details). In contrast, the sample complexity presented for  and our algorithms is the total expected number of pulls that will be executed.

## 2 Related Work

The BAI problem with fixed confidence is first studied for \(\) bounded rewards by Even-Dar et al. . The sample complexity of their algorithm scales with the sum of the squared inverse gap, i.e., \(H()=_{i>1}1/_{i}^{2}\). Garivier and Kaufmann  showed that \(H()<T^{*}() 2H()\) with \(T^{*}()\) defined in (1.2) and proposed the Track-and-Stop algorithm, which is the first one in the literature proved to be asymptotically optimal. Later, Degenne et al.  viewed \(T^{*}()\) as a min-max game and provided an efficient algorithm to solve it. Jourdan et al.  studied the asymptotically optimal sample complexity for any reward distributions with bounded support. Degenne et al.  studied pure exploration in linear bandits. Their proposed algorithm proved to be both asymptotically optimal and empirically efficient.

There are also many studies [30; 10; 8; 21; 31] that focus on providing non-asymptotic optimal sample complexity. The best non-asymptotic sample complexity was achieved by Chen et al. , which replaces the term \(_{i}^{-1}\) in (1.4) with a much smaller term. Furthermore, when we allow a loss \(\) of the quality of the returned arm, the problem is known as \((,)\)-PAC BAI, for which various algorithms [29; 14; 6; 9] are proposed, achieving the worst-case optimal sample complexity.

There are a few attempts to achieve both the asymptotic and non-asymptotic optimal sample complexity. Degenne et al.  provided a non-asymptotic sample complexity \(nT^{*}()^{2}\), which could be \(nT^{*}()\) larger than the optimal sample complexity. Recently, Jourdan and Degenne  managed to achieve a sample complexity that is \(\)-asymptotically optimal (with \(w_{1}\) fixed at \(1/\) in (1.2)), rendering it asymptotically optimal up to a factor of \(1/\). Meanwhile, they also reached a non-asymptotic sample complexity of \((H() H())^{}\)4 for some \(>1\), where \(H()=_{i>1}1/_{i}^{2}\). Wang et al.  explored both asymptotic and non-asymptotic sample complexities. Their algorithm achieves asymptotic optimality and shows a non-asymptotic sample complexity of \(O(nH()^{4}/w_{}^{2})\). However, this non-asymptotic sample complexity is \(nH()^{3}/w_{}^{2}\) away from being optimal. Jourdan et al.  studied \((,)\)-PAC BAI, proposing an asymptotically optimal algorithm and providing non-asymptotic sample complexity. When \(=0\), it aligns with our setting, our non-asymptotic sample complexity is better scaled. Specifically, Jourdan et al.  offered a non-asymptotic sample complexity scale of \(n/_{2}^{2}(1/_{2})\), whereas ours is more instance-sensitive, as our sample complexity is related to all gaps, not just \(_{2}\). Additionally, Jourdan et al.  considered a practical scenario where the algorithm can return a result at any time while still ensuring a good guarantee on the returned arm. For ease of reference, we summarize the sample complexity of different algorithms in Table 2. As shown in Table 2, our algorithm is the only one that achieves both the asymptotic optimality and non-asymptotic optimality within logarithm factors.

Another line of research [31; 4; 7] investigated BAI with a fixed budget, where the objective is to determine the best arm with the highest probability within \(T\) pulls. Audibert et al.  and Karnin et al.  offered finite-time bounds for this problem, while Carpentier and Locatelli  presented a tight lower bound, demonstrating that such finite-time bounds[4; 31] are optimal for certain bandit

   Algorithm & Asymptotic behavior (\( 0\)) & Finite-confidence behavior \\  Kalyanakrishnan et al.  & \(O(H()(1/))\) & \(O(H()(H()))\) \\ Karnin et al.  & \(O(H()(1/))\) & \((_{i>1}^{-1})}{_{i}^{2}})\) \\ Garivier and Kaufmann  & \(T^{*}()(1/)\)+\(o(1/)\) & - \\ Jamieson et al.  & \(O(H()(1/))\) & \((_{i>1}^{-1})}{_{i}^{2}})\) \\ Jourdan and Degenne  & \(T^{*}_{}()(1/)\)+\(o(1/)\) & \(((H() H())^{})\), \(>1\) \\ Degenne et al.  & \(T^{*}()(1/)\)+\(o(1/)\) & \((nT^{*}()^{2})\) \\ Katz-Samuels et al.  & \(O(T^{*}()(1/))\) & \(O(H()(n/_{}))\) \\ Wang et al.  & \(T^{*}()(1/)\)+\(o(1/)\) & \(O(nH()^{4}/w_{}^{2})\) \\
**Opt-BBAI** (Our Algorithm 2) & \(T^{*}()(1/)\)+\(o(1/)\) & \((_{i>1}^{-1})}{_{i}^{2}})\) \\   

Table 2: Comparison of sample complexity of different algorithms.

instances. However, the asymptotic optimality for this problem remains unknown. Interested readers are referred to recent advancements[11; 35] in the asymptotic results of BAI with a fixed budget.

In addition, some recent works [1; 22; 39; 32] also focused on batched BAI in non-asymptotic setting. Agarwal et al.  studied the batched BAI problem under the assumption that \(_{2}\) is known. They proposed an algorithm that has the worst-case optimal sample complexity of \(n_{2}^{-2}\) and runs in \(^{*}(n)\) batches. Later, Jin et al.  provided the algorithms that achieves the sample complexity given in (1.4) within \(}((1/_{2}))\) batches, where \(\) hides the \(()\) factors. Tao et al.  studied the BAI problem in the general collaborative setting and showed that no algorithm can achieve (1.4) with \(o((_{2}^{-1})/_{2}^{-1})\) batches. Karpov et al.  further proposed an algorithm which has the sample complexity \(_{i>1})}{_{i}^{2}}\) and the batch complexity \(O((1/_{2}))\). We note that the lower bound of batch complexity given by Tao et al.  can only be applied to a constant \(\). In other words, the lower bound of complexity for the asymptotic setting remains unknown. Agrawal et al.  studied the optimal batch size for keeping the asymptotic optimality. They showed an algorithm with batch size \(m=o((1/))\) achieves the asymptotic optimality. The batch complexity of the algorithm is \(O(T^{*}()(1/)/m)\). Wang et al.  provided an algorithm that uses \(O(})\) batches and retains asymptotic optimality for linear bandits. However, for the asymptotic setting, such batch size is still too large as it grows to infinity as \(\) decreases to 0.

## 3 Achieving Asymptotic Optimality with at Most Three Batches

### Reward Distribution

We assume the reward distributions belong to a known one-parameter exponential family that is commonly considered in the literature [17; 16; 36]. In particular, the measure \(_{}\) of such probability distributions with respect the model parameter \(\) satisfies \(_{}}{}(x)=(x-b())\), for some measure \(\) and \(b()=( e^{x}(x))\). For one-parameter exponential distributions, it is known that \(b^{}()=[_{}]\) and the mapping \(b^{}()\) is one-to-one. Moreover, given any two mean values \(\) and \(^{}\), we define \(d(,^{})\) to be the Kullback-Leibler divergence between two distributions with mean values \(\) and \(^{}\).

### The Proposed Three-Batch Algorithm

Algorithm 1 shows the pseudo-code of our proposed Tri-BBAI algorithm. In particular, Tri-BBAI has four stages. In what follows, we elaborate on the details of each stage.

Stage I: Initial exploration.In this stage, we pull each arm for \(L_{1}\) times. Denote by \(i^{*}(t)\) the arm with the largest empirical mean at time \(t\) (i.e., after we pull all arms for a total number of \(t\) times), i.e., \(i^{*}(t)=_{i[n]}_{i}(t)\). Let \(_{0}=nL_{1}\). Fix \(t=_{q-1}_{0}\), we let \(b_{i}^{q}=_{i}(t)+\) for \(i i^{*}(t)\) and \(b_{i}^{q}=_{i}(t)-\) for \(i=i^{*}(t)\). Let \(^{*}()=_{_{}}_{ ()}_{i=1}^{n}w_{i} d(_{i },_{i})\). Then, for aforementioned \(^{q}=\{b_{1}^{q},b_{2}^{q},,b_{n}^{q}\}\), we calculate \(^{*}(^{q})\) according to Lemma A.1 and \(T^{*}(^{q})\) according to Lemma A.2. We note that arm 1 is assumed to be the arm with the highest mean in these two lemmas. However, in the context of \(^{q}\), the index of \(i^{*}(t)\) might be different. To align with the standard practice, we can rearrange the indices of the arms in \(^{q}\) so that \(i^{*}(t)\) corresponds to index 1.

Purpose.To achieve the asymptotic optimality, we attempt to pull each arm \(i\) for around \(w_{i}^{*}()T^{*}()\) times. We can show that with a high probability, \(w_{i}^{*}(^{q})T^{*}(^{q})\) is close to \(w_{i}^{*}()T^{*}()\), which implies that pulling arm \(i\) for a number of times proportional to \(w_{i}^{*}(^{q})T^{*}(^{q})\) is likely to ensure the asymptotic optimal sample complexity.

Stage II: Exploration using \(^{*}(^{q})\) and \(T^{*}(^{q})\).Stage II operates in batches with the maximum number of batches determined by \((1/)\). At batch \(q\), each arm \(i\) is pulled \(_{p:p,p[1,q]}T_{i}^{q}\) times in total. Here

\[T_{i}^{q}:= w_{i}^{*}(^{q})T^{*}(^{q})^ {-1},L_{2}},\] (3.1)

where the definition of \(w_{i}^{*}(^{q})\) and \(T^{*}(^{q})\) could be found in Stage I. We then evaluate the stage switching condition, \(|w_{i}^{*}(^{q})-w_{i}^{*}(^{q-1})| 1/\) for all \(i[n]\). If this condition is met, we go to the next stage; otherwise, we proceed to the next batch within Stage II.

**Purpose.** Pulling arm \(i\) proportional to \(w_{i}^{*}(^{q})T^{*}(^{q})\) provides statistical evidence for the reward distributions without sacrificing sample complexity compared to the optimality per our above discussion. Meanwhile, we also set a threshold \(L_{2}\) to avoid over-exploration due to sampling errors from Stage I.

The rationale for running Stage II in multiple batches is based on empirical considerations. In experiments, \(\) is always finite. Consequently, the error of arm \(i\), \(|_{i}(t)-_{i}|\), remains constant since the number of pulls of arms is limited. Given that the stopping rule in Stage III is highly dependent on the error of arm \(i\) and the number of pulls \(T_{i}:=_{p:p,p 1}T_{i}^{p}\), there is a constant probability that the stopping rule may not be met, leading to significant sample costs in Stage IV. Adding the condition in Line 15 ensures that \(w^{*}(^{q})\) converges and that the sample size \(T_{i}^{q}\), as defined by \(w^{*}(^{q})\) and \(T^{*}(^{q})\), closely approximates \( w^{*}()T^{*}()^{-1}\). This alignment significantly increases the probability that the stopping rule will be satisfied in experiments.

Moreover, such modification doesn't hurt any theoretical results. To explain, in our analysis for Tri-BBAI, we demonstrate that as \(\) approaches \(0\), \(w^{*}(^{q})\) will be very close to \(w^{*}()\) and the probability that Line 15 is not satisfied could be bounded by \(1/^{2}^{-1}\), which means with high probability Stage II costs 2 batches. Besides, \(q(1/)\), which implies that even if Line 15 is not satisfied, the number of batches required for Stage II is at most \((1/)\). Therefore, the expected number of batches required for Stage II is 2 as \(\) approaches 0.

Stage III: Statistical test with Chernoff's stopping rule.Denote by \(N_{i}(t)\) the number of pulls of arm \(i\) at time \(t\). For each pair of arms \(i\) and \(j\), define their weighted empirical mean as

\[_{ij}(t):=(t)_{i}(t)}{N_{i}(t)+N_{j}(t)}+ {N_{j}(t)_{j}(t)}{N_{i}(t)+N_{j}(t)},\] (3.2)

where \(_{i}(t)\) and \(_{j}(t)\) are the empirical means of arms \(i\) and \(j\) at time \(t\). For \(_{i}(t)_{j}(t)\), define

\[Z_{ij}(t) :=d(_{i}(t),_{ij}(t))N_{i}(t)+d(_{j}(t),_{ij}(t))N_{j}(t),\] (3.3) \[Z_{j}(t) :=Z_{i^{*}(t)j}(t).\]

We test whether Chernoff's stopping condition is met (Line 21). If so, we return the arm with the largest empirical mean, i.e., \(i^{*}()\), where \(\) is the total number of pulls examined after Stage II.

**Purpose.** The intuition of using Chernoff's stopping rule for the statistical test is two-fold. Firstly, if Chernoff's stopping condition is met, with a probability of at least \(1-/2\), the returned arm \(i^{*}()\) in Line 22 is the optimal arm (see Lemma B.1). Secondly, when \(\) is sufficiently small, with high probability, Chernoff's stopping condition holds (see Lemma B.4). As a consequence, our algorithm identifies the best arm successfully with a high probability of meeting the requirement.

Stage IV: Re-exploration.If the previous Chernoff's stopping condition is not met, we pull each arm until the total number of pulls of each arm equals \(L_{3}\) taking into account the pulls in the previous stages (Line 24). Finally, the arm with the largest empirical mean is returned (Line 26).

**Purpose.** If Chernoff's stopping condition is not met, \(i^{*}()\) might not be the optimal arm. In addition, when each arm is pulled for \(L_{3}\) times, we are sufficiently confident that \(i^{*}(t)\) is the best arm. Since the probability of Stage IV happening is very small, its impact on the sample complexity is negligible.

### Theoretical Guarantees of Tri-BBAI

In the following, we present the theoretical results for Algorithm 1.

**Theorem 3.1** (Asymptotic Sample Complexity).: Given any \(>0\), let \(=\), \(L_{1}=}\), \(L_{2}=}{n}\), and \(L_{3}=(^{-1})^{2}\). Meanwhile, for any given \((1,e/2]\), define function \((t,)\) as \((t,)=((1/)t^{}/)\).5 Then, for any bandit instance \(\), Algorithm 1 satisfies

\[_{ 0}_{}[N_{}]}{(1/ )} T^{*}().\]

By letting \(\) in Theorem 3.1 approach \(1\) (e.g., \(=1+1/^{-1}\)), we obtain the asymptotic optimal sample complexity.

**Theorem 3.2** (Correctness).: Let \(\), \(L_{1}\), \(L_{2}\), \(L_{3}\), \(\), and \((t,)\) be the same as in Theorem 3.1. Then, for sufficiently small \(>0\), Algorithm 1 satisfies \(_{}(i^{*}(N_{}) 1)\).

**Theorem 3.3** (Asymptotic Batch Complexity).: Let \(\), \(L_{1}\), \(L_{2}\), \(L_{3}\), \(\), and \((t,)\) be the same as in Theorem 3.1. For sufficiently small \(>0\), Algorithm 1 runs within \(3+o(1)\) batches in expectation. Besides, Algorithm 1 runs within \(3\) batches with probability \(1-1/(1/^{2})\).

To the best of our knowledge, all previous works in the BAI literature  that achieve the asymptotic optimal sample complexity require unbounded batches as \( 0\). In contrast, Tri-BBAI achieves the asymptotic optimal sample complexity and runs within 3 batches in expectation, which is a significant improvement in the batched bandit setting where switching to new policies is expensive.

**Remark 3.4**.: Apart from best arm identification, regret minimization is another popular task in bandits, where the aim is to maximize the total reward in \(T\) rounds. Jin et al.  proposed an algorithm that achieves a constant batch complexity for regret minimization and showed that their algorithm is optimal when \(T\) goes to infinity. In regret minimization, the cost of pulling the optimal arm is 0, indicating that the allocation \(w_{i}\) (i.e., the proportion of pulling the optimal arm) is close to \(1\). In the BAI problem, the main hardness is to find the allocation \(w_{i}\) for each arm since even pulling arm \(1\) will increase the sample complexity of the algorithm. Therefore, the strategy proposed by Jin et al.  cannot be applied to the BAI problem.

## 4 Best of Both Worlds: Achieving Asymptotic and Non-asymptotic Optimalities

The Tri-BBAI algorithm is shown to enjoy the optimal sample complexity with only three batches in the asymptotic setting. However, in practice, we are limited to a finite number of samples and thus \(\) cannot go to zero, which is a critical concern in real-world applications. Consequently, obtaining the optimal sample and batch complexity in a non-asymptotic setting becomes the ultimate objective of a practical bandit strategy in BBAI. In this section, we introduce Opt-BBAI, which can attain the optimal sample and batch complexity in asymptotic settings and near-optimal sample and batch complexity in non-asymptotic settings.

We assume a bounded reward distribution within \(\), which aligns with the same setting in the literature . Again, we consider that the reward distribution belongs to a one-parameter exponential family. By refining Stage IV of Algorithm 1, we can achieve asymptotic optimality and near non-asymptotic optimality adaptively based on the assumption on \(\) in various settings.

The pseudo-code for the algorithm is provided in Algorithm 2. The main modification from Algorithm 1 occurs in Stage IV. Intuitively, if the algorithm cannot return at Stage III, then the value of \(^{-1}\) may be comparable to other problem parameters, such as \(1/_{2}\). Therefore, we aim to achieve the best possible sample and batch complexity for the non-asymptotic scenario. Stage IV operates in rounds, progressively eliminating sub-optimal arms until a result is obtained. Each round consists of two components: _Successive Elimination_ and _Checking for Best Arm Elimination_.

``` Input: Parameters \(,,L_{1},L_{2},L_{3},\) and function \((t,)\). Output: The estimated optimal arm.
1Stage I, II, and III: the same as that in Algorithm 1
2Stage IV: Exploration with Exponential Gap Elimination
3\(S_{r}[n]\), \(B_{0} 0\), \(r 1\);
4while\(|S_{r}|>1\)do
5 Let \(_{r} 2^{-r}/4\), \(_{r}/(40^{2}n r^{2})\), \(_{r} 0\), and \(_{r}_{r}\);
6 Successive elimination **/** Eliminate arms whose means are lower than \(_{1}\) by at least \(_{r}\)
7foreach arm \(i S_{r}\)do
8 Pull arm \(i\) for \(d_{r}^{2}}(2/_{r})\) times;
9 Let \(_{i}^{r}\) be the empirical mean of arm \(i\);
10 Let \(_{i S_{r}}_{i}^{r}\);
11 Set \(S_{r+1} S_{r}\{i S_{r}_{i}^{r}<_{ }^{r}-_{r}\}\);
12 Checking for best arm elimination **/** Reduce the risk of the best arm being eliminated
13 Let \(B_{r} B_{r-1}+d_{r}|S_{r}|\);
14for\(j<r\)do
15foreach arm \(i S_{j} S_{j+1}\)do
16if\(B_{r}_{j} 2^{_{j}}>B_{j}\)then
17\(_{j}(_{j})^{2}\);
18 Repull arm \(i\) for total \(^{2}}(2/_{j})\) times;
19 Let \(_{i}^{j}\) be the empirical mean of arm \(i\) in \(S_{j}\);
20\(_{j}_{j}+1\);
21if\( i S_{j}\), \(_{i}^{j}>_{}^{r}-_{j}/2\)then
22return Randomly return an arm in \(S_{r}\);
23\(r r+1\);
24return The arm in \(S_{r}\); ```

**Algorithm 2**(Almost) Optimal Batched Best Arm Identification (Opt-BBAI)

Successive Elimination.In the \(r\)-th round, we maintain a set \(S_{r}\), a potential set for the best arm. Each arm in \(S_{r}\) is then pulled \(d_{r}=32/_{r}^{2}(2/_{r})\) times. At Line 11, all possible sub-optimal arms are eliminated. This first component of Stage IV borrows its idea from successive elimination .

**Purpose.** After \(d_{r}\) number of pulls, arms are likely to be concentrated on their true means within a distance of \(_{r}/4\) with a high probability. Hence, with high probability, the best arm is never eliminated at every round of Line 11, and the final remaining arm is the best arm.

_Issue of Best Arm Elimination._ Due to successive elimination, there is a small probability (\(_{r}\)) that the best arm will be eliminated. The following example illustrates that, conditioning on this small-probability event, the sample and batch complexity of the algorithm could become infinite.

**Example 4.1**.: Consider a bandit instance where \(_{1}>_{2}=_{3}>_{4}_{n}\). If the best arm is eliminated at a certain round and never pulled again, the algorithm tasked with distinguishing between the 2nd and 3rd best arms will likely never terminate due to their equal means, leading to unbounded sample and batch complexity.

To address this issue, we introduce a _Check for Best Arm Elimination_ component into Stage IV.

Checking for Best Arm Elimination.In the \(r\)-th round, we represent the total sample complexity used up to the \(r\)-th round as \(B_{r}\). We employ \(_{j}\) as an upper bound for the probability that the best arm is eliminated in \(S_{j}\). If Line 16 is true (\(B_{r}_{j} 2^{_{j}}>B_{j}\)), we adjust \(_{j}\) to \(_{j}^{2}\) and pull each arm in \(S_{j}\) for \(32/_{j}^{2}(2/_{j})\) times (Line 18), subsequently updating their empirical mean (Line 19). Finally, we return a random arm in \(S_{r}\) if the condition at Line 21 holds.

**Purpose.** If Line 16 is satisfied, it indicates that the sample costs, based on the event of the best arm being eliminated in the \(r\)-th round, exceed \(B_{j}/2^{_{j}}\). In this case, we increase the number of samples for arms in \(S_{j}\) and re-evaluate if their empirical mean is lower than that of the current best arm \(*\). This ensures that the expected total sample costs, assuming the best arm is eliminated at \(S_{j}\), are bounded by \(_{_{j}=1}^{}B_{j}/2^{_{j}} B_{j}\). If Line 21 holds, we randomly return an arm from \(S_{r}\). Since this only happens with a small probability, we still guarantee that Algorithm 2 will return the best arm with a probability of at least \(1-\).

In what follows, we provide the theoretical results for Algorithm 2.

**Theorem 4.2**.: Let \(\), \(L_{1}\), \(L_{2}\), \(L_{3}\), \(\), and \((t,)\) be the same as in Theorem 3.1. For finite \((0,1)\), Algorithm 2 identifies the optimal arm with probability at least \(1-\) and there exists some universal constant \(C\) such that \([N_{}] C_{i>1}_{i}^{-2}n _{i}^{-1}\), and the algorithm runs in \(C(1/_{2})\) expected batches.

When \(\) is allowed to go to zero, we also have the following result.

**Theorem 4.3**.: Let \(\), \(L_{1}\), \(L_{2}\), \(L_{3}\), \(\), and \((t,)\) be the same as in Theorem 3.1. Algorithm 2 identifies the optimal with probability at least \(1-\) and its sample complexity satisfies \(_{ 0}_{}[N_{}]/(1/)  T^{*}()\), and the expected batch complexity of Algorithm 2 converges to 3 when \(\) approaches 0.

The results in Theorems 4.2 and 4.3 state that Algorithm 2 achieves both the asymptotic optimal sample complexity and a constant batch complexity. Moreover, it also demonstrates near-optimal performance in both non-asymptotic sample complexity and batch complexity. Notably, this is the first algorithm that successfully attains the optimal or near-optimal sample and batch complexity, adaptively, in asymptotic and non-asymptotic settings.

**Remark 4.4**.: Jin et al.  achieved near-optimal sample and batch complexity in a non-asymptotic setting. However, their results are contingent on the event that the algorithm can find the best arm (with probability \(1-\)). Consequently, with a probability of \(\) there is no guarantee for its batch and sample complexity to be bounded. As demonstrated in Example 4.1, the batch and sample complexity in  could even be infinite.

In contrast, the batch and sample complexity introduced in Theorem 4.2 is near-optimal and does not rely on any specific event due to the procedure "**checking for best arm elimination**" we proposed. Our technique could be of independent interest and could be further applied to existing elimination-based BAI algorithms  to ensure that the sample and batch complexity is independent of the low-probability event that the best arm is eliminated.

Conclusion, Limitations, and Future Work

In this paper, we studied the BAI problem in the batched bandit setting. We proposed a novel algorithm, Tri-BBAI, which only needs 3 batches in expectation to find the best arm with probability \(1-\) and achieves the asymptotic optimal sample complexity. We further proposed Opt-BBAI and theoretically showed that Opt-BBAI has a near-optimal non-asymptotic sample and batch complexity while still maintaining the asymptotic optimality as Tri-BBAI does.

In our experiments, although Tri-BBAI utilizes a limited number of batches, its sample complexity does not match that of Garivier and Kaufmann . Designing a batched algorithm with sample complexity comparable to Garivier and Kaufmann , while maintaining a constant number of batches, presents an intriguing challenge.

As for future work, an interesting direction is to investigate whether our "checking for best arm elimination" could be beneficial to other elimination-based algorithms. Additionally, some research  implied a strong correlation between batch complexity in batched bandit, and the memory complexity and pass complexity in streaming bandit. Thus, it could be valuable to assess if our techniques could enhance the results in the field of streaming bandit.