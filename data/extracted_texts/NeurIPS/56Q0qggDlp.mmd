# Molecule Generation with

Fragment Retrieval Augmentation

Seul Lee\({}^{1}\)1 Karsten Kreis\({}^{2}\) Srimukh Prasad Veccham\({}^{2}\) Meng Liu\({}^{2}\)

**Danny Reidenbach\({}^{2}\) Saee Paliwal\({}^{2}\) Arash Vahdat\({}^{2}\)2 Weili Nie\({}^{2}\)2**

\({}^{1}\)Kaist \({}^{2}\)NVIDIA

seul.lee@kaist.ac.kr

{kkreis,sveccham,menliu,dreidenbach,saeep,avahdat,wnie}@nvidia.com

###### Abstract

Fragment-based drug discovery, in which molecular fragments are assembled into new molecules with desirable biochemical properties, has achieved great success. However, many fragment-based molecule generation methods show limited exploration beyond the existing fragments in the database as they only reassemble or slightly modify the given ones. To tackle this problem, we propose a new fragment-based molecule generation framework with retrieval augmentation, namely _Fragment Retrieval-Augmented Generation_ (\(f\)-RAG). \(f\)-RAG is based on a pre-trained molecular generative model that proposes additional fragments from input fragments to complete and generate a new molecule. Given a fragment vocabulary, \(f\)-RAG retrieves two types of fragments: (1) _hard fragments_, which serve as building blocks that will be explicitly included in the newly generated molecule, and (2) _soft fragments_, which serve as reference to guide the generation of new fragments through a trainable _fragment injection module_. To extrapolate beyond the existing fragments, \(f\)-RAG updates the fragment vocabulary with generated fragments via an iterative refinement process which is further enhanced with post-hoc genetic fragment modification. \(f\)-RAG can achieve an improved exploration-exploitation trade-off by maintaining a pool of fragments and expanding it with novel and high-quality fragments through a strong generative prior.

## 1 Introduction

The goal of small molecule drug discovery is to discover molecules with specific biochemical target properties, such as synthesizability , non-toxicity , solubility  and binding affinity, in the vast chemical space. Fragment-based drug discovery (FBDD) has been considered as an effective approach to explore the chemical space and has resulted in many successful marketed drugs . Contrary to high-throughput screening (HTS)  that searches from a library of drug-like molecules, FBDD constructs a library of molecular fragments to synthesize new molecules beyond the existing molecule library, leading to a better chemical coverage .

Recently, generative models have been adopted in the field of FBDD to accelerate the process of searching for drug

Figure 1: **A radar plot of target properties. \(f\)-RAG strikes better balance among optimization performance, diversity, novelty, and synthesizability than the state-of-the-art techniques on the PMO benchmark .**candidates [18; 45; 46; 30; 20; 11; 25]. These methods reassemble or slightly modify the fragments to generate new molecules with the knowledge of the generative model. As the methods are allowed to exploit chemical knowledge in the form of fragments to narrow the search space, they have shown meaningful success in generating optimized molecules. However, their performance is largely limited by the existing library of molecular fragments, where discovering new fragments is either impossible or highly limited. Some of the methods [14; 38; 25] suggested making modifications to existing fragments, but the modifications are only applied to small and local substructures and still heavily dependent on the existing fragments. The limited exploration beyond known fragments greatly hinders the possibility of generating diverse and novel drug candidates that may exhibit better target properties. Therefore, it is a challenge to improve the ability of discovering novel high-quality fragments, while exploiting existing chemical space effectively.

To this end, we propose a fragment-based molecule generation framework leveraging retrieval-augmented generation (RAG) , namely _Fragment Retrieval-Augmented Generation_ (\(f\)-RAG). As shown in Figure 2, \(f\)-RAG augments the pre-trained molecular language model SAFE-GPT  with two types of retrieved fragments: _hard fragments_ and _soft fragments_ for a better exploration-exploitation trade-off. Specifically, we first construct a fragment vocabulary by decomposing known molecules from the existing library into fragments and scoring the fragments by measuring their contribution to target properties. From the fragment vocabulary, \(f\)-RAG first retrieves fragments that will be explicitly included in the new molecule (i.e., hard fragments). Hard fragments serve as the input context to the molecular language model that predicts the remaining fragments. In addition to retrieval of hard fragments, \(\)-RAG retrieves fragments that will not be part of the generated molecule but provide informative guidance on predicting novel, diverse molecules (i.e., soft fragments). Concretely, the embeddings of soft fragments are fused with the embeddings of hard fragments (or input embeddings) through a lightweight _fragment injection module_ in the middle of SAFE-GPT. The fragment injection module allows SAFE-GPT to generate new fragments by referring to the information conveyed by soft fragments.

During training, we only update the fragment injection module while keeping SAFE-GPT frozen. Inspired by Wang et al. , we train \(f\)-RAG to learn how to leverage the retrieved fragments for molecule generation, using a self-supervised loss that predicts the most similar one in the set of soft fragments. At inference, \(f\)-RAG dynamically updates the fragment vocabulary with newly generated fragments via an iterative refinement process. To further enhance exploration, we propose to modify the generated fragments from SAFE-GPT with a post-hoc genetic fragment modification process. The proposed \(f\)-RAG takes the advantages of both hard and soft fragment retrieval to achieve an improved exploration-exploitation trade-off. We verify \(f\)-RAG on various molecular optimization tasks, by examining the optimization performance, along with diversity, novelty, and

Figure 2: **The overall framework of \(f\)-RAG.** After an initial fragment vocabulary is constructed from an existing molecule library, two types of fragments are retrieved during generation. Hard fragments are explicitly included in the newly generated molecules, while soft fragments implicitly guide the generation of new fragments. SAFE-GPT generates a molecule using hard fragments as input, while the fragment injection module in the middle of the SAFE-GPT layers injects the embeddings of soft fragments into the input embedding. After the generation, the molecule population and fragment vocabulary are updated with the newly generated molecule and its fragments, respectively. The exploration is further enhanced with genetic fragment modification, which also updates the fragment vocabulary and molecule population.

synthesizability. As shown in Figure 1, \(f\)-RAG exhibits the best balance across these essential considerations, demonstrating its applicability as a promising tool for drug discovery.

We summarize our contributions as follows:

* We introduce \(f\)-RAG, a novel molecular generative framework that combines FBDD and RAG.
* We propose a retrieval augmentation strategy that operates at the fragment level with two types of fragments, allowing fine-grained guidance to achieve an improved exploration-exploitation trade-off and generate high-quality drug candidates.
* Through extensive experiments, we demonstrate the effectiveness of \(f\)-RAG in various drug discovery tasks that simulate real-world scenarios.

## 2 Related Work

Fragment-based molecule generation.Fragment-based molecular generative models refer to a class of methods that reassemble existing molecular substructures (i.e., fragments) to generate new molecules. Jin et al.  proposed to find big molecular substructures that satisfy the given chemical properties, and learn to complete the substructures into final molecules by adding small branches. Xie et al.  proposed to progressively add or delete fragments using Markov chain Monte Carlo (MCMC) sampling. Yang et al.  proposed to use reinforcement learning (RL) to assemble fragments, while Maziarz et al. , Kong et al. , and Geng et al.  used VAE-based techniques. Lee et al.  proposed to take the target chemical properties into account in the fragment vocabulary construction and used a combination of RL and a genetic algorithm (GA) to assemble and modify the fragments. On the other hand, graph-based GAs [14; 38] decompose parent molecules into fragments that are combined to generate an offspring molecule, and are also mutated with a small probability. Since fragment-based strategies are limited by generating molecules outside of the possible combinations of existing fragments, they suffer from limited exploration in the chemical space. Some of the methods [14; 38; 25] suggest making modifications to existing fragments to overcome this problem, but this is not a fundamental solution because the modifications are only local, still being based on the existing fragments.

Retrieval-augmented molecule generation.Retrieval-augmented generation (RAG)  refers to a technique that retrieves context from external data databases to guide the generation of a generative model. Recently, RAG has gained attention as a means to enhance accuracy and reliability of large language models (LLMs) [6; 36; 3]. In the field of molecule optimization, Liu et al.  developed a text-based drug editing framework that repurposes a conversational language model for solving molecular tasks, where the domain knowledge is injected by a retrieval and feedback module. More related to us, Wang et al.  proposed to use a pre-trained molecular language model to generate molecules, while augmenting the generation with retrieved molecules that have high target properties. Contrary to this method that retrieves molecules, our proposed \(f\)-RAG employs a fine-grained retrieval augmentation scheme that operates at the fragment level to achieve an improved exploration-exploitation trade-off.

## 3 Method

In this section, we introduce our Fragment Retrieval-Augmented Generation (\(f\)-RAG) framework. \(f\)-RAG aims to generate optimized molecules by leveraging existing chemical knowledge through RAG, while exploring beyond the known chemical space under the fragment-based drug discovery (FBDD) paradigm. We first introduce the hard fragment retrieval in Sec. 3.1. Then, we present the soft fragment retrieval in Sec. 3.2. Lastly, we describe the genetic fragment modification in Sec. 3.3.

### Hard Fragment Retrieval

Given a set of \(N\) molecules \(x_{i}\) and their corresponding properties \(y_{i}\), denoted as \(=\{(x_{i},y_{i})\}_{i=1}^{N}\), we first construct a fragment vocabulary. We adopt an arm-linker-arm slicing algorithm provided by Noutahi et al.  which decomposes a molecule \(x\) into three fragments: two arms \(F_{}\) (i.e., fragments that have one attachment point) and one linker \(F_{}\) (i.e., a fragment that has two attachment points). Decomposing molecules into arms and linkers (or scaffolds) is a popular approach utilized in various drug discovery strategies, such as scaffold decoration or scaffold hopping . We ignored the molecules in the training set that cannot be decomposed, and a set of arms \(_{}=\{F_{,j}\}_{j=1}^{2N}\) and a set of linkers \(_{}=\{F_{,j}\}_{j=1}^{N}\) are obtained after the algorithm is applied to the molecules \(\{x_{i}\}_{i=1}^{N}\). Subsequently, we calculate the score of each fragment \(F_{j}_{}_{}\) using the average property of all molecules containing \(F_{j}\) as their substructure as follows:

\[(F_{j})=)|}_{(x,y) S(F_{j})}y,\] (1)

where \((F_{j})\), and \(S(F_{j})=\{(x,y):F_{j}x\}\). Intuitively, the fragment score evaluates the contribution of a given fragment to the target property of the whole molecule of which it is a part. From \(_{}\) and \(_{}\), we choose the top-\(N_{}\) fragments based on the score to construct the arm fragment vocabulary \(_{}_{}\) and the linker fragment vocabulary \(_{}_{}\), respectively.

Given the fragment vocabularies \(_{}\) and \(_{}\) consisting of high-property fragments, two hard fragments are randomly retrieved from the vocabularies. The hard fragments together form a partial molecular sequence that serves as input to a pre-trained molecular language model. In this work, we employ Sequential Attachment-based Fragment Embedding (SAFE)  as the molecular representation and SAFE-GPT  as the backbone generative model of \(f\)-RAG. SAFE is a non-canonical version of simplified molecular-input line-entry system (SMILES)  that represents molecules as a sequence of dot-connected fragments. Importantly, the order of fragments in a SAFE string does not affect the molecular identity. Using the SAFE representation, \(f\)-RAG forces the hard fragments to be included in a newly generated molecule by providing them as an input sequence to the language model to complete the rest of the sequence.

During generation, with a probability of \(50\%\), \(f\)-RAG either (1) retrieves two hard fragments from \(_{}\) or (2) retrieves one from \(_{}\) and one from \(_{}\). In the former case, \(f\)-RAG performs linker design, which generates a new fragment that links the input fragments. In the latter case, \(f\)-RAG first randomly selects an attachment point in the retrieved linker and combines it with the retrieved arm to form a single fragment, and then performs motif extension, which generates a new fragment that completes the molecule (Figure 3).

### Soft Fragment Retrieval

Given two hard fragments as input, the molecular language model generates one new fragment to complete a molecule. Instead of relying solely on the model to generate the new fragment, we propose to augment the generation with the information of \(K\) retrieved fragments, which we refer to as soft fragments, to guide the generation. Specifically, if the two hard fragments are all arms, \(f\)-RAG randomly retrieves soft fragments from \(_{}\). If one of the hard fragments is an arm and another is a linker, \(f\)-RAG randomly retrieves soft fragments from \(_{}\). Using up to the \(L\)-th layer of the language model \(^{0:L}\), the embeddings of the input sequence \(x_{}\) and the soft fragments \(\{F_{,k}\}_{k=1}^{K}\) are obtained as follows:

\[_{}=^{0:L}(x_{ })&_{}=([_{}^{1}, _{}^{2},,_{}^{K}]),\\ &_{}^{k}=^{0:L}(F_{ ,k})k{=}1,2,,K.\] (2)

Figure 3: **Hard fragment retrieval** of \(f\)-RAG. With a probability of \(50\%\), \(f\)-RAG either retrieves two arms as hard fragments for linker design **(top)** or one arm and one linker as hard fragments for motif extension **(bottom)**.

Subsequently, \(f\)-RAG injects the embeddings of soft fragments through a trainable _fragment injection module_. Following Wang et al. , the fragment injection module uses cross-attention to fuse the embeddings of the input sequence and soft fragments as follows:

\[=(_{},_{})= ((_{})(_{})^{}}{}}})(_{}),\] (3)

where FI is the fragment injection module, Query, Key, and Value are multi-layer perceptrons (MLPs), and \(d_{}\) is the output dimension of Key. Next, a molecule is generated by decoding the augmented embedding \(\) using the later layers of the language model as \(x_{}=^{L+1:L_{T}}()\), where \(L_{T}\) is the total number of layers of the model. With the fragment injection module, \(f\)-RAG can utilize information of soft fragments to generate novel fragments which are also likely to contribute to the high target properties. During generation, the fragment vocabulary is dynamically updated through an iterative process that scores newly generated fragments based on Eq. (1) and replaces fragments in the fragment vocabulary to the top-\(N_{}\) fragments.

Next, we need to train \(f\)-RAG to learn how to augment the retrieved soft fragments into the molecule generation. To retain the high generation quality of SAFE-GPT and make the training process efficient, we keep the backbone language model frozen and only train the lightweight fragment injection module. Inspired by Wang et al. , we propose a new self-supervised objective that predicts the most similar fragment to the input fragments. Specifically, each molecular sequence \(x\) in the training set is first decomposed into fragment sequences \((F_{1},F_{2},F_{3})\) with a random permutation between the fragments, using the same slicing algorithm used in the vocabulary construction. Importantly, using the SAFE representation, \(x\) can be simply represented by connecting its fragments with dots as \(F_{1}.F_{2}.F_{3}\). We consider the first two fragments as hard fragments. Given the remaining fragment \(F_{3}\), we retrieve its \(K\) most similar fragments \(\{F_{3}^{kNN}\}_{k=1}^{K}\) from the training fragment pool. Here, we use the pairwise Tanimoto similarity using Morgan fingerprints of radius 2 and 1024 bits. Using the hard fragments as the input sequence as \(F_{1}.F_{2}\), the objective is to predict the most similar fragment \(F_{3}^{1NN}\) utilizing the original fragment and the next \(K-1\) most similar fragments \(\{F_{3}^{kNN}\}_{k=2}^{K}\) as the soft fragments. The training process is illustrated in Figure 4, and the details are provided in Section D.1.

Note that the training of the fragment injection module is target property-agnostic, as the fragments used for training are independent of the target property. In contrast, the fragment vocabularies used for generation are target property-specific, as it is constructed using the scoring function in Eq. (1). This allows \(f\)-RAG to effectively generate optimized molecules across different target properties without any retraining.

### Genetic Fragment Modification

To further enhance exploration in the chemical space, we propose to modify the generated fragments with a post-hoc genetic algorithm (GA). Specifically, we adopt the operations of Graph GA . We first initialize the population \(\) with the top-\(N_{}\) molecules generated by our fragment retrieval-augmented SAFE-GPT based on the target property \(y\). Parent molecules are then randomly selected from the population and offspring molecules are generated by the crossover and mutation operations (see Jensen  for more details). The offspring molecules can have new fragments not contained in the initial fragment vocabulary, and the fragment vocabularies \(_{}\) and \(_{}\) are again updated by the top-\(N_{}\) fragments based on the scores of Eq. (1). In the subsequent generation, the population \(\) is updated with the generated molecules so far by both SAFE-GPT and GA.

Figure 4: **The self-supervised training process of the fragment injection module of \(f\)-RAG. \(F^{k}\) denotes the \(k\)-th most similar fragment to \(F\). Using \(F_{1}\) and \(F_{2}\) as hard fragments, while using \(F_{3}\) and its neighbors \(\{F_{3}^{kNN}\}_{k=2}^{K}\) as soft fragments, the training objective is to predict \(F_{3}^{1NN}\).**

[MISSING_PAGE_FAIL:6]

   Oracle & \(f\)-RAG (ours) & Genetic GFN & Mol GA & REINVENT & Graph GA \\  albuterol\_similarity & **0.977**\(\) 0.002 & 0.949 \(\) 0.010 & 0.896 \(\) 0.035 & 0.882 \(\) 0.006 & 0.838 \(\) 0.016 \\ amlodipine\_mpo & 0.749 \(\) 0.019 & **0.761**\(\) 0.019 & 0.688 \(\) 0.039 & 0.635 \(\) 0.035 & 0.661 \(\) 0.020 \\ celecoxib\_rediscovery & 0.778 \(\) 0.007 & **0.802**\(\) 0.029 & 0.567 \(\) 0.083 & 0.713 \(\) 0.067 & 0.630 \(\) 0.097 \\ deco\_hop & **0.936**\(\) 0.011 & 0.733 \(\) 0.109 & 0.649 \(\) 0.025 & 0.666 \(\) 0.044 & 0.619 \(\) 0.004 \\ drd2 & **0.992**\(\) 0.000 & 0.974 \(\) 0.006 & 0.936 \(\) 0.016 & 0.945 \(\) 0.007 & 0.964 \(\) 0.012 \\ resfoenadine\_mpo & **0.856**\(\) 0.016 & **0.856**\(\) 0.039 & 0.825 \(\) 0.019 & 0.784 \(\) 0.006 & 0.760 \(\) 0.011 \\ gsk3b & **0.969**\(\) 0.003 & 0.881 \(\) 0.042 & 0.843 \(\) 0.039 & 0.865 \(\) 0.043 & 0.788 \(\) 0.070 \\ isomers\_c7h8n2o2 & 0.955 \(\) 0.008 & **0.969**\(\) 0.003 & 0.878 \(\) 0.026 & 0.852 \(\) 0.036 & 0.862 \(\) 0.065 \\ isomers\_c9h10n2o2pf2d & 0.850 \(\) 0.005 & **0.897**\(\) 0.007 & 0.865 \(\) 0.012 & 0.642 \(\) 0.054 & 0.719 \(\) 0.047 \\ jnk3 & **0.904**\(\) 0.004 & 0.764 \(\) 0.069 & 0.702 \(\) 0.123 & 0.783 \(\) 0.023 & 0.553 \(\) 0.136 \\ median1 & 0.340 \(\) 0.007 & **0.379**\(\) 0.010 & 0.257 \(\) 0.009 & 0.356 \(\) 0.009 & 0.294 \(\) 0.021 \\ median2 & **0.323**\(\) 0.005 & 0.294 \(\) 0.007 & 0.301 \(\) 0.021 & 0.276 \(\) 0.008 & 0.273 \(\) 0.009 \\ mestrand\_similarity & 0.671 \(\) 0.021 & **0.708**\(\) 0.057 & 0.591 \(\) 0.053 & 0.618 \(\) 0.048 & 0.579 \(\) 0.022 \\ osimetrinb\_mpo & **0.866**\(\) 0.009 & 0.860 \(\) 0.008 & 0.844 \(\) 0.015 & 0.837 \(\) 0.009 & 0.831 \(\) 0.005 \\ perindopri\_mpo & **0.681**\(\) 0.017 & 0.595 \(\) 0.014 & 0.547 \(\) 0.022 & 0.537 \(\) 0.016 & 0.538 \(\) 0.009 \\ qed & 0.939 \(\) 0.001 & **0.942**\(\) 0.000 & 0.941 \(\) 0.001 & 0.941 \(\) 0.000 & 0.940 \(\) 0.000 \\ randomazine\_mpo & **0.820**\(\) 0.016 & 0.819 \(\) 0.018 & 0.804 \(\) 0.011 & 0.760 \(\) 0.009 & 0.728 \(\) 0.012 \\ scaffold\_hop & 0.576 \(\) 0.014 & **0.615**\(\) 0.100 & 0.527 \(\) 0.025 & 0.560 \(\) 0.019 & 0.517 \(\) 0.007 \\ sitagliptin\_mpo & 0.601 \(\) 0.011 & **0.634**\(\) 0.039 & 0.582 \(\) 0.040 & 0.021 \(\) 0.003 & 0.433 \(\) 0.075 \\ thiothixene\_rediscovery & **0.584**\(\) 0.009 & 0.583 \(\) 0.034 & 0.519 \(\) 0.041 & 0.534 \(\) 0.013 & 0.479 \(\) 0.025 \\ trogltazone\_rediscovery & 0.448 \(\) 0.017 & **0.511**\(\) 0.054 & 0.427 \(\) 0.031 & 0.441 \(\) 0.032 & 0.390 \(\) 0.016 \\ valsartan\_smarts & **0.627**\(\) 0.058 & 0.135 \(\) 0.271 & 0.000 \(\) 0.000 & 0.178 \(\) 0.358 & 0.000 \(\) 0.000 \\ zaleplon\_mpo & 0.486 \(\) 0.004 & **0.552**\(\) 0.033 & 0.519 \(\) 0.029 & 0.358 \(\) 0.062 & 0.346 \(\) 0.032 \\  Sum & **16.928** & 16.213 & 14.708 & 14.196 & 13.751 \\   Oracle & 
 SELFIES- \\ REINVENT \\  & GP BO & STONED & LSTM HC & SMILES GA \\  albuterol\_similarity & 0.826 \(\) 0.030 & 0.898 \(\) 0.014 & 0.745 \(\) 0.076 & 0.719 \(\) 0.018 & 0.661 \(\) 0.066 \\ amlodipine\_mpo & 0.607 \(\) 0.014 & 0.583 \(\) 0.044 & 0.608 \(\) 0.046 & 0.593 \(\) 0.016 & 0.549 \(\) 0.009 \\ celecoxib\_rediscovery & 0.573 \(\) 0.043 & 0.723 \(\) 0.053 & 0.382 \(\) 0.041 & 0.539 \(\) 0.018 & 0.344 \(\) 0.027 \\ deco\_hop & 0.631 \(\) 0.012 & 0.629 \(\) 0.018 & 0.611 \(\) 0.008 & 0.826 \(\) 0.017 & 0.611 \(\) 0.006 \\ drd2 & 0.943 \(\) 0.005 & 0.923 \(\) 0.017 & 0.913 \(\) 0.020 & 0.919 \(\) 0.015 & 0.908 \(\) 0.019 \\ fexofenadine\_mpo & 0.741 \(\) 0.002 & 0.722 \(\) 0.005 & 0.797 \(\) 0.016 & 0.725 \(\) 0.003 & 0.721 \(\) 0.015 \\ gsk3b & 0.780 \(\) 0.037 & 0.851 \(\) 0.041 & 0.668 \(\) 0.049 & 0.839 \(\) 0.

synthesizability, and novelty. Following the previous works, we use docking score calculated by QuickVina 2  with five protein targets, parp1, fa7, 5ht1b, braf, and jak2, to measure binding affinity. We use quantitative estimates of drug-likeness (QED)  and SA  to measure drug-likeness and synthesizability, respectively. Following the previous works, we set the target property \(y\) as follows:

\[y=}},\] (4)

where \(}\) and \(}\) are the normalized DS and SA according to Eq. (7) in Section D.4. We generate 3,000 molecules and evaluate them using **novel top 5% DS (kcal/mol)**, which indicates the mean DS of the top 5% unique and novel hits. Here, novel hits are defined as the molecules that satisfy all the following criteria: (the maximum similarity with the training molecules) \(<0.4\), DS \(<\) (the median DS of known active molecules), QED \(>0.5\), and SA \(<5\). Further details are provided in Section D.4.

Baselines.**JT-VAE**, **HierVAE**, **MARS**, **RationaleRL**, **FREED**, **PS-VAE**, and **GEAM** are the methods that first construct a fragment vocabulary using a molecular dataset, then learn to assemble those fragments to generate new molecules. On the other hand, **Graph GA**, **GEGL**, and **Genetic GFN** are GA-based methods that utilize the information of fragments by adopting the fragment-based crossover operation. **GA+D** is a discriminator-enhanced GA method that operates on the SELFIES representation. **RetMol** is a retrieval-based method that uses retrieved example molecules to augment the generation of a pre-trained molecular language model. **REINVENT** and **MORLD** are RL models that operate on SMILES and graph molecular representations, respectively. **MOOD** is a diffusion model that employs an out-of-distribution control to improve novelty.

Results.The results are shown in Table 3. In all five tasks, \(f\)-RAG outperforms all the baselines. Note that the evaluation metric, novel top 5% DS, is designed to reflect the nature of the drug discovery problem, which optimizes multiple target properties by finding a good balance between exploration and exploitation. The results demonstrate the superiority of \(f\)-RAG in generating drug-like, synthesizable, and novel drug candidates that have high binding affinity to the target protein with the improved exploration-exploitation trade-off. We additionally visualize the generated molecules in Figure 8.

Additional comparison with GEAM.To further justify the advantage of \(f\)-RAG over GEAM, we additionally include the results of seven multi-property optimization (MPO) tasks in the PMO benchmark (Section 4.1) in Table 7. As we can see, \(f\)-RAG significantly outperforms GEAM in all the tasks, validating its applicability to a wide range of drug discovery problems.

    &  \\   & parp1 & fa7 & 5ht1b & braf & jak2 \\  JT-VAE  & -9.482 \(\) 0.132 & -7.683 \(\) 0.048 & -9.382 \(\) 0.332 & -9.079 \(\) 0.069 & -8.885 \(\) 0.026 \\ REINVENT  & -8.702 \(\) 0.523 & -7.205 \(\) 0.264 & -8.770 \(\) 0.316 & -8.392 \(\) 0.400 & -8.165 \(\) 0.277 \\ Graph GA  & -10.949 \(\) 0.532 & -7.365 \(\) 0.326 & -10.422 \(\) 0.670 & -10.789 \(\) 0.341 & -10.167 \(\) 0.576 \\ MORLD  & -7.532 \(\) 0.260 & -6.263 \(\) 0.165 & -7.869 \(\) 0.650 & -8.040 \(\) 0.337 & -7.816 \(\) 0.133 \\ HierVAE  & -9.487 \(\) 0.278 & -6.812 \(\) 0.274 & -8.081 \(\) 0.252 & -8.978 \(\) 0.525 & -8.285 \(\) 0.370 \\ GA+D  & -8.365 \(\) 0.201 & -6.539 \(\) 0.297 & -8.567 \(\) 0.177 & -9.371 \(\) 0.728 & -8.610 \(\) 0.104 \\ MARS  & -9.716 \(\) 0.082 & -7.839 \(\) 0.018 & -9.804 \(\) 0.073 & -9.569 \(\) 0.078 & -9.150 \(\) 0.114 \\ GEGL  & -9.329 \(\) 0.170 & -7.470 \(\) 0.013 & -9.086 \(\) 0.067 & -9.073 \(\) 0.047 & -8.601 \(\) 0.038 \\ RationaleRL  & -10.663 \(\) 0.086 & -8.129 \(\) 0.048 & -9.005 \(\) 0.155 & _No fit found_ & -9.398 \(\) 0.076 \\ FREED  & -10.579 \(\) 0.104 & -8.378 \(\) 0.044 & -10.714 \(\) 0.183 & -10.561 \(\) 0.080 & -9.735 \(\) 0.022 \\ PS-VAE  & -9.978 \(\) 0.091 & -8.028 \(\) 0.050 & -9.887 \(\) 0.115 & -9.637 \(\) 0.049 & -9.464 \(\) 0.129 \\ MOOD  & -10.865 \(\) 0.113 & -8.160 \(\) 0.071 & -11.145 \(\) 0.042 & -11.063 \(\) 0.034 & -10.147 \(\) 0.060 \\ RetMol  & -8.590 \(\) 0.475 & -5.448 \(\) 0.688 & -6.980 \(\) 0.740 & -8.811 \(\) 0.574 & -7.133 \(\) 0.242 \\ GEAM  & -12.891 \(\) 0.158 & -9.890 \(\) 0.116 & -12.374 \(\) 0.036 & -12.342 \(\) 0.095 & -11.816 \(\) 0.067 \\ Genetic GFN  & -9.227 \(\) 0.644 & -7.288 \(\) 0.433 & -8.973 \(\) 0.804 & -8.719 \(\) 0.190 & -8.539 \(\) 0.592 \\  \(f\)-RAG (ours) & **-12.945**\(\) 0.053 & **-9.899**\(\) 0.205 & **-12.670**\(\) 0.144 & **-12.390**\(\) 0.046 & **-11.842**\(\) 0.316 \\   

Table 3: **Novel top 5% docking score (kcal/mol) results. The results are the means and standard deviations of 3 independent runs. The results for RationaleRL, PS-VAE, RetMol, and GEAM are taken from Lee et al. . Other baseline results except for Genetic GFN are taken from Lee et al. . Lower is better, and the best results are highlighted in bold.**

### Analysis

Ablation study.To verify the effect of each component of \(f\)-RAG, we conduct experiments with various combinations of the components, i.e., hard fragment retrieval (hard), soft fragment retrieval (soft), and genetic fragment modification (GA), in Figure 5(a) and Figure 5(b). The detailed results are shown in Table 8 and Table 9. For \(f\)-RAG (soft+GA) that does not utilize the hard fragment retrieval, we randomly initialized the fragment vocabularies instead of selecting the top-\(N_{}\) fragments based on Eq. (1) and let the model use random hard fragments. Note that even though \(f\)-RAG (hard+soft), \(f\)-RAG that does not utilize the genetic fragment modification, shows very high diversity, the value is not meaningful as the generated molecules are not highly optimized, not novel, and not synthesizable. The same applies to the high novelty of \(f\)-RAG (soft+GA) and \(f\)-RAG (GA). On the contrary, the full \(f\)-RAG achieves the best optimization performance and synthesizability while showing reasonably high diversity and novelty. Importantly, \(f\)-RAG outperforms \(f\)-RAG (hard+GA) in all the metrics, especially for diversity and novelty, indicating the soft fragment retrieval aids the model in generating more diverse and novel drug candidates while maintaining their high target properties.

Controlling optimization performance-diversity trade-off.It is well-known that molecular diversity is important in drug discovery, especially when the oracle has noise. However, optimization performance against the target property and molecular diversity are conflicting factors [10; 19]. To control this trade-off between optimization performance and diversity, we additionally introduce a _similarity-based fragment filtering_ strategy, which excludes similar fragments from the fragment vocabulary. Specifically, when updating the vocabulary, new fragments that have a higher Tanimoto similarity than \(\) to fragments in the vocabulary are filtered out. Morgan fingerprints of radius 2 and 1024 bits are used to calculate the Tanimoto similarity. The results of the similarity-based fragment filter with different values of \(\) are shown in Figure 5(c), verifying \(f\)-RAG can increase diversity at the expense of optimization performance by controlling \(\).

Effect of the fragment vocabulary update.To analyze the effect of the dynamic update of the fragment vocabulary during generation, we visualize the distribution of the docking scores of molecules generated by \(f\)-RAG with or with the fragment vocabulary update in Figure 6. The dynamic update allows \(f\)-RAG to generate more optimized molecules in terms of the target property. Notably, with the dynamic update, \(f\)-RAG can discover molecules that have higher binding affinity to the target protein than the top molecule in the training set, and we visualize an example of such molecules in the figure. Note that the molecule also has low similarity (\(0.321\)) with the training molecules. This result demonstrates the explorability of \(f\)-RAG to discover drug candidates that lie beyond the training distribution.

Figure 5: **(a) The optimization curves in the deco_hop task of the PMO benchmark of the ablated \(f\)-RAGs. Solid lines denote the mean and shaded areas denote the standard deviation of 3 independent runs. (b) Overall results of the ablated \(f\)-RAGs. (c) Results with different values of \(\) of the similarity-based fragment filter.**

Figure 6: **DS distribution of molecules generated by \(f\)-RAG with or without the fragment vocabulary update in the jak2 task.**Conclusion

We proposed \(f\)-RAG, a new fragment-based molecular generative framework that utilizes hard fragment retrieval, soft fragment retrieval, and genetic fragment modification. With hard fragment retrieval, \(f\)-RAG can explicitly exploit the information contained in existing fragments that contribute to the target properties. With soft fragment retrieval, \(f\)-RAG can balance between exploitation of existing chemical knowledge and exploration beyond the existing fragment vocabulary. Soft fragment retrieval with SAFE-GPT allows \(f\)-RAG to propose new fragments that are likely to contribute to the target chemical properties. The proposed novel fragments are then dynamically incorporated in the fragment vocabulary throughout generation. This exploration is further enhanced with genetic fragment modification, which modifies fragments with genetic operations and updates the vocabulary. Through extensive experiments, we demonstrated the efficacy of \(f\)-RAG to improve the exploration-exploitation trade-off. \(f\)-RAG outperforms previous methods, achieving state-of-the-art performance to synthesize diverse, novel, and synthesizable drug candidates with high target properties.