# Can Large Language Model Agents Simulate Human Trust Behavior?

Chengxing Xie

Equal Contribution. Correspondence to: Chengxing Xie <xiechengxing34@gmail.com>, Canyu Chen <ccchen151@hawk.iit.edu>, Guohao Li <guohao@robots.ox.ac.uk>.

Canyu Chen

Work performed while Guohao Li was at KAUST and Chengxing Xie was a visiting student at KAUST. University of Michigan

Feiran Jia\({}^{4}\) Ziyu Ye\({}^{5}\) Shiyang Lai\({}^{5}\) Kai Shu\({}^{6}\) Jindong Gu\({}^{3}\) Adel Bibi\({}^{3}\) Ziniu Hu\({}^{7}\)

David Jurgens\({}^{8}\) James Evans\({}^{5,\,9,\,10}\) Philip H.S. Torr\({}^{3}\) Bernard Ghanem\({}^{1}\) Guohao Li \({}^{,\,11}\)

\({}^{1}\)KAUST \({}^{2}\)Illinois Institute of Technology \({}^{3}\)University of Oxford \({}^{4}\)Pennsylvania State University

\({}^{5}\)University of Chicago \({}^{6}\)Emery \({}^{7}\)California Institute of Technology

\({}^{8}\)University of Michigan \({}^{9}\)Santa Fe Institute \({}^{10}\)Google \({}^{11}\)CAMEL-ALorg

###### Abstract

Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: _can LLM agents really simulate human behavior?_ In this paper, we focus on one critical and elemental behavior in human interactions, _trust_, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as _agent trust_, under the framework of _Trust Games_, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high _behavioral alignment_ with humans in terms of trust behavior, indicating _the feasibility of simulating human trust behavior with LLM agents_. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond _value alignment_. We further illustrate broader implications of our discoveries for applications where trust is paramount.

## 1 Introduction

There is an increasing trend to adopt Large Language Models (LLMs) as agent-based simulation tools for humans in various social science fields including economics, politics, psychology, ecology and sociology (Gao et al., 2023; Manning et al., 2024; Ziems et al., 2023), and role-playing applications such as assistants, companions and mentors (Yang et al., 2024; Abdelghani et al., 2023; Chen et al., 2024) due to their human-like cognitive capacity. Nevertheless, most previous research is based on one insufficiently validated assumption that LLM agents behave like humans in simulation. Thus, a fundamental question remains: _Can LLM agents really simulate human behavior?_

In this paper, we focus on _trust_ behavior in human interactions, which comprises the intention to place self-interest at risk based on the positive expectations of others (Rousseau et al., 1998). Trust is one of the most critical and elemental behaviors in human interactions and plays an essential role in social settings ranging from daily communication to economic and political institutions (Uslaner, 2000; Coleman, 1994). Here, we investigate _whether LLM agents can simulate human trust behavior_, paving the way to explore their potential to simulate more complex human behavior and society itself.

First, we explore whether LLM agents manifest trust behavior in their interactions. Given the challenge of quantifying trust behavior, we choose to study them based on the Trust Game and itsvariations (Berg et al., 1995; Glaeser et al., 2000), which are established methodologies in behavioral economics. We adopt the _Belief-Desire-Intention_ (BDI) framework (Rao et al., 1995; Andreas, 2022) to model LLM agents' reasoning process for decision-making explicitly. Based on existing measurements for trust behavior in the Trust Game and the BDI interpretations of LLM agents, we achieve our first core finding: **LLM agents generally exhibit trust behavior in the Trust Game**.

Then, we refer to LLM agents' trust behavior as _agent trust_ and humans' trust behavior as _human trust_, and aim to investigate whether agent and human trust align, implying the possibility of simulating human trust behavior with LLM agents. Next, we propose a new concept, _behavioral alignment_, as the alignment between agents and humans concerning factors that impact behavior (namely _behavioral factors_), and dynamics that evolve over time (namely _behavioral dynamics_). Based on human studies, three basic behavioral factors underlie trust behavior including reciprocity anticipation (Berg et al., 1995), risk perception (Bohnet & Zeckhauser, 2004) and prosocial preference (Alos-Ferrer & Farolfi, 2019). Comparing the results of LLM agents with existing human studies in Trust Games, we have our second core finding: **GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior**, suggesting the feasibility of using agent trust to simulate human trust, although **LLM agents with fewer parameters show relatively lower behavioral alignment**. This finding lays the foundation for simulating more complex human interactions and societal institutions, and enriches our understanding of the analogical relationship between LLMs and humans.

In addition, we more deeply probe the intrinsic properties of agent trust across four scenarios. First, we examine whether changing the other player's demographics impacts agent trust. Second, we study differences in agent trust when the other player is an LLM agent versus a human. Third, we directly manipulate agent trust with explicit instructions "you need to trust the other player" and "you must not trust the other player". Fourth, we adjust the reasoning strategies of LLM agents from direct reasoning to zero-shot Chain-of-Thought reasoning (Kojima et al., 2022). These investigations lead to our third core finding: **agent trust exhibits bias across different demographics, has a relative preference for humans over agents, is easier to undermine than to enhance, and may be influenced by advanced reasoning strategies**. Our contributions can be summarized as:

* We propose a definition of LLM agents' _trust_ behavior under Trust Games and a new concept of _behavioral alignment_ as the human-LLM analogy regarding _behavioral factors_ and _dynamics_.
* We discover that LLM agents generally exhibit _trust_ behavior in Trust Games and GPT-4 agents manifest high _behavioral alignment_ with humans in terms of trust behavior, indicating the great potential to simulate human trust behavior with LLM agents. Our findings pave the way for simulat

Figure 1: **Our Framework for Investigating Agent Trust as well as its Behavioral Alignment with Human Trust. First, this figure shows the major components for studying the trust behavior of LLM agents with Trust Games and Belief-Desire-Intention (BDI) modeling. Then, our study centers on examining the behavioral alignment between LLM agents and humans regarding trust behavior.**

ing complex human interactions and social institutions, and open new directions for understanding the fundamental analogy between LLMs and humans beyond _value alignment_.
* We investigate _intrinsic properties_ of agent trust under manipulations and reasoning strategies, as well as biases of agent trust and differences in agent trust towards agents versus humans.
* We illustrate broader _implications_ of our discoveries about agent trust and its behavioral alignment with human trust for human simulation in social science and role-playing applications, LLM agent cooperation, human-agent collaboration and the safety of LLM agents, detailed further in Section 6.

## 2 LLM Agents in Trust Games

### Trust Games

Trust Games, referring to the Trust Game and its variations, have been widely used for examining human trust behavior in behavioral economics (Berg et al., 1995; Lenton and Mosley, 2011; Glaeser et al., 2000; Cesarini et al., 2008). As shown in Figure 1, the player who makes the first decision to send money is called the _trustor_, while the other one who responds by returning money is called the _trustee_. In this paper, we mainly focus on the following six types of Trust Games (the specific prompt for each game is articulated in the Appendix H.2):

Game 1: Trust GameAs shown in Figure 1, in the Trust Game (Cox, 2004; Berg et al., 1995), the trustor initially receives \(\$10\). The trustor selects \(\$N\) and sends it to the trustee, exhibiting _trust behavior_. Then the trustee will receive \(\$3N\), and have the option to return part of that \(\$3N\) to the trustor, showing _reciprocal behavior_.

Game 2: Dictator GameIn the Dictator Game (Cox, 2004), the trustor also needs to send \(\$N\) from the initial \(\$10\) to the trustee and then the trustee will receive \(\$3N\). Compared to the Trust Game, the only difference is that the trustee does not have the option to return money in the Dictator Game and the trustor is also aware that the trustee cannot reciprocate.

Game 3: MAP Trust GameIn the MAP Trust Game (MAP represents Minimum Acceptable Probabilities) (Bonnet and Zeckhauser, 2004), a variant of the Trust Game, the trustor needs to choose whether to trust the trustee. If the trustor chooses not to trust the trustee, each will receive \(\$10\); If the trustor and the trustee both choose to trust, each will receive \(\$15\); If the trustor chooses to trust, but the trustee does not, the trustor will receive \(\$8\) and the trustee will receive \(\$22\). There is probability \(p\) that the trustee will choose to trust and \((1-p)\) probability that they will not choose to trust. MAP is defined as the minimum value of \(p\) at which the trustor would choose to trust the trustee.

Game 4: Risky Dictator GameThe Risky Dictator Game (Bohnet and Zeckhauser, 2004) differs from the MAP Trust Game in only a single aspect. In the Risky Dictator Game, the trustee is present but does not have the choice to trust or not and the money distribution relies on the pure probability \(p\). Specifically, if the trustor chooses to trust, there is probability \(p\) that both the trustor and the other player will receive \(\$15\) and probability \((1-p)\) that the trustor will receive \(\$8\) and the other player will receive \(\$22\). If the trustor chooses not to trust the trustee, each player will receive \(\$10\).

Game 5: Lottery GameThere are two typical Lottery Games (Fetchenhauer and Dunning, 2012). In the Lottery People Game, the trustor is informed that the trustee chooses to trust with probability \(p\). Then the trustor must choose between receiving fixed money or trusting the trustee, which is similar to the MAP Trust Game. In the Lottery Gamble Game, the trustor chooses between playing a gamble with a winning probability of \(p\) or receiving fixed money. \(p\) is set as \(46\%\) following the human study.

Game 6: Repeated Trust GameWe follow the setting of the Repeated Trust Game in (Cochard et al., 2004), where the Trust Game is played for multiple rounds with the same players and each round begins anew with the trustor allocated the same initial money.

### LLM Agent Setting

In our study, we set up our experiments using the CAMEL framework (Li et al., 2023a) with both closed-source and open-source LLMs including GPT-4, GPT-3.5-turbo-0613, GPT-3.5-turbo-16k-0613, text-davinci-003, GPT-3.5-turbo-instruct, Llama2-7b (or 13b, 70b) and Vicuna-v1.3-7b (or 13b, 33b) (Ouyang et al., 2022; Achiam et al., 2023; Touvron et al., 2023; Chiang et al., 2023). We set the temperature as \(1\) to increase the diversity of agents' decision-making and note that high temperatures are commonly adopted in related literature (Aher et al., 2023; Lore and Heydari, 2023; Guo, 2023).

Agent Persona.To better reflect the setting of real-world human studies (Berg et al., 1995), we design LLM agents with diverse personas in the prompt. Specifically, we ask GPT-4 to generate 53types of personas based on a given template. Each persona needs to have information including name, age, gender, address, job and background. Examples of the personas are shown in Appendix H.1.

**Belief-Desire-Intention (BDI).** The BDI framework is a well-established approach in agent-oriented programming (Rao et al., 1995) and was recently adopted to language models (Andreas, 2022). We propose modeling LLM agents in Trust Games with the BDI framework to gain deeper insights into LLM agents' behaviors. Specifically, we let LLM agents directly output their Beliefs, Desires, and Intentions as the reasoning process for decision-making in Trust Games.

## 3 Do LLM Agents Manifest Trust Behavior?

In this section, we investigate whether or not LLM agents manifest trust behavior by letting LLM agents play the Trust Game (Section 2.1 Game 1). In Behavioral Economics, trust is widely measured by the initial amount sent from the trustor to the trustee in the Trust Game (Glaeser et al., 2000; Cesarini et al., 2008). Following the measurement of trust in human studies and the assumption humans own reasoning processes that underlie their decisions, we can define the conditions that LLM agents manifest trust behavior in the Trust Game as follows. _First_, **the amount sent is positive and does not exceed the amount of money the trustor initially possesses**, which implies that the trustor places self-interest at risk with the expectation the trustee will reciprocate and that the trustor understands the money limit that can be given. _Second_, **the decision (_i.e., amounts sent_) can be interpreted as the reasoning process (_i.e.,_ the BDI) of the trustor**. We explored utilizing BDI to model the reasoning process of LLM agents. If we can interpret the decision as the articulated reasoning process, we have evidence that LLM agents do not send a random amount of money and manifest some degree of rationality in the decision-making process. Then, we assess whether LLM agents exhibit trust behavior based on two aspects: the amount sent and the BDI.

### Amount Sent

To evaluate LLMs' capacity to understand the basic experimental setting regarding money limits, we propose a new evaluation metric, Valid Response Rate (VRR) (%), defined as the percentage of personas with the amount sent falling within the initial money (\(\$10\)). Results are shown in Figure 2. We can observe that **most LLMs have a high VRR except Llama-7b**, which implies that most LLMs manifest a full understanding regarding limits on the amount they can send in the Trust Game. Then, we observe the distribution of amounts sent for different LLMs as the trustor agent and discover that **the amounts sent are predominantly positive, indicating a level of trust**.

### Belief-Desire-Intention (BDI)

The sole evidence of the amount sent cannot sufficiently support the existence of trust behavior, because agents could send positive but random amounts of money. Thus, we leveraged the Belief-Desire-Intention framework (Rao et al., 1995; Andreas, 2022) to model the reasoning process of LLM agents. If we can interpret the amounts sent from BDI outputs, we have evidence to refute the hypothesis that the amounts sent are positive but random and demonstrate that LLM agents manifest some degree of rationality. We take GPT-4 as an example to analyze its BDI outputs. More examples from the other nine LLMs such as Vicuna-v1.3-7b are shown in the Appendix I. Considering that the amounts sent typically vary across distinct personas, we select one BDI from the personas that give a high amount of money and another BDI from those that give a low amount. Positive and negative factors for trust behavior in the reasoning process are marked in blue and red, respectively.

_As a person with a strong belief in the goodness of humanity, I trust that the other player...Therefore, my desire is to maximize the outcome for both of us and cement a sense of com

Figure 2: **Amount Sent Distribution of LLM Agents and Humans as the Trustor in the Trust Game. The size of circles represents the number of personas for each amount sent. The bold lines show the medians. The crosses indicate the VRR (%) for different LLMs.**

_radery and trust... I intend to use this as an opportunity to add what I can to someone else's life...Finally, I will give **10 dollars**._

We can observe that this persona shows a high-level of "comradery and trust" towards the other player, which justifies the high amount sent from this persona (_i.e._, _10 dollars_).

_As an Analyst,... My desire is that the other player will also see the benefits of reciprocity and goodwill... my intention is to give away a signiucant portion of my initial 10... However, since I have no knowledge of the other player,... Therefore, I aim to give an amount that is not too high,...Finally, I will give **5 dollars** to the other player..._

Compared to the first persona, we see that the second one has a more cautious attitude. For example, "since I have no knowledge of the other player" shows skepticism regarding the other player's motives. Thus, this persona, though still optimistic about the other player ("intention... give away a significant portion"), strategically balances risk and reciprocity, and then decides to send only a modest amount.

Based on GPT-4's BDI examples and examples from other LLMs in Appendix I, we find **decisions (_i.e._, amounts sent) from LLM agents in the Trust Game can be interpreted from their articulated reasoning process (_i.e._, **BDI**). Because most LLM agents have a high VRR-send a positive amount of money-and show some degree of rationality in giving money, our first core finding is:

**Finding 1:** LLM agents generally exhibit trust behavior under the framework of the Trust Game.

### Basic Analysis of Agent Trust

We also conduct a basic analysis of LLM agents' trust behavior, namely agent trust, based on the results in Figure 2. _First_, we observe that Vicuna-7b has the highest level of trust towards the other player and GPT-3.5-turbo-0613 has the lowest level of trust as trust can be measured by the amount sent in human studies (Glaeser et al., 2000; Cesarini et al., 2008). _Second_, compared with humans' average amount sent (\(\$5.97\)), most personas for GPT-4 and Vicuna-7b send a higher amount of money to the other player, and most personas for LLMs such as GPT-3.5-turb-0613 send a lower amount. _Third_, we see that amounts sent for Llama2-70b and Llama2-13b have a convergent distribution while amounts sent for humans and Vicuna-7b are more divergent.

## 4 Does Agent Trust Align with Human Trust?

In this section, we aim to explore the fundamental relationship between agent and human trust, _i.e._, whether or not agent trust aligns with human trust. This provides important insight regarding the feasibility of utilizing LLM agents to simulate human trust behavior as well as more complex human interactions that involve trust. First, we propose a new concept _behavioral alignment_ and discuss its distinction from existing alignment definitions. Then, we conduct extensive studies to investigate whether or not LLM agents exhibit alignment with humans regarding trust behavior.

### Behavioral Alignment

Existing alignment definitions predominantly emphasize _values_ that seek to ensure the safety and helpfulness of LLMs (Ji et al., 2023; Shen et al., 2023; Wang et al., 2023c), which cannot fully characterize the landscape of multifaceted alignment between LLMs and humans. Thus, we propose a new concept of _behavioral alignment_ to characterize the LLM-human analogy regarding _behavior_, which involves both actions and the associated reasoning processes that underlie them. Because actions evolve over time and the reasoning that underlies them involves multiple factors, we define _behavioral alignment_ as the analogy between LLMs and humans concerning factors impacting behavior, namely _behavioral factors_, and action dynamics, namely _behavioral dynamics_.

Based on the definition of behavioral alignment, we aim to answer: _does agent trust align with human trust_? As for _behavioral factors_, existing human studies have shown that three basic factors impact human trust behavior including reciprocity anticipation (Berg et al., 1995; Cox, 2004), risk perception (Bohnet & Zeckhauser, 2004) and prosocial preference (Alos-Ferrer & Farolfi, 2019). We examine whether agent trust aligns with human trust along these three factors. Although _behavioral dynamics_ vary for different humans and agent personas, we analyze whether agent trust has the same patterns across multiple turns as human trust in the Repeated Trust Game.

Besides analyzing the trust behavior of LLM agents and humans based on quantitative measurements (_e.g._, the _amount sent_ from trustor to trustee), we also explore the use of _BDI_ to interpret the reasoning process with which LLM agents justify their actions, which can further validate whether LLM agents manifest an underlying reasoning process analogous to human cognition.

### Behavioral Factor 1: Reciprocity Anticipation

Reciprocity anticipation, the expectation of a reciprocal action from the other player, can positively influence human trust behavior (Berg et al., 1995). The effect of reciprocity anticipation exists in the Trust Game but not in the Dictator Game (Section 2.1 Games 1 and 2) because trustee cannot return money in the Dictator Game, which is the only difference between these games. Thus, to determine whether LLM agents can anticipate reciprocity, we compare their behaviors in these Games.

First, we analyze trust behaviors based on the average amount of money sent by human or LLM agents. As shown in Figure 3, human studies show that humans exhibit a higher level of trust in the Trust Game than in the Dictator Game (\(\$6.0\) vs. \(\$3.6\), \(p\)-value = \(0.01\) using One-Tailed Independent Samples t-test) (Cox, 2004), indicating that reciprocity anticipation enhances human trust. Similarly, GPT-4 (\(\$6.9\) vs. \(\$6.3\), \(p\)-value = \(0.05\) using One-Tailed Independent Samples t-test) also shows a higher level of trust in the Trust Game with statistical significance, implying that reciprocity anticipation can enhance agent trust. However, LLMs with fewer parameters (_e.g._, LLama2-13b) do not show this tendency in their trust behaviors for the Trust and Dictator Games.

Then, we further analyze GPT-4 agents' BDI to explore whether they can anticipate reciprocity in their reasoning (the complete BDIs are in Appendix I.10). Typically, in the Trust Game, one persona's BDI emphasizes "_putting faith in people_", which implies the anticipation of the goodness of the other player, and "_reflection of trust_". However, in the Dictator Game, one persona's BDI focuses on concepts such as "_fairness_" and "_human kindness_", which are not directly tied to trust or reciprocity. Thus, we can observe that GPT-4 shows distinct BDI outputs in the Trust and Dictator Games.

Based on the above analysis of the amount sent and BDI, we find that **GPT-4 agents exhibit human-like reciprocity anticipation in trust behavior**. Nevertheless, **LLMs with fewer parameters (_e.g._, **Llama2-13b) do not show an awareness of reciprocity from the other player**.

### Behavioral Factor 2: Risk Perception

Existing human studies have demonstrated the strong correlation between trust behavior and risk perception, suggesting that human trust will increase as risk decreases (Hardin, 2002; Williamson, 1993; Coleman, 1994). We aim to explore whether LLM agents can perceive the risk associated with their trust behaviors through the MAP Trust Game and the Risky Dictator Game (Section 2.1 Games 3 and 4), where risk is represented by the probability \((1-p)\) (defined in Section 2.1).

As shown in Figure 4, we measure human trust (or agent trust) by the portion choosing to trust the other player in the whole group, namely the Trust Rate (%). Based on existing human studies (Bohnet & Zeckhauser, 2004), when the probability \(p\) is higher, the risk for trust behaviors is lower, and more humans choose to trust, manifesting a higher Trust Rate, which indicates that human trust rises as risk falls. Similarly, we observe a general increase in agent trust as risk decreases for LLMs including GPT-4, GPT-3.5-turbo-0613, and text-davinci-003. In particular, we can see that the curves of humans and GPT-4 are more

Figure 4: **Trust Rate (%) Curves for LLM Agents and Humans in the MAP Trust Game and the Risky Dictator Game.** The metric Trust Rate indicates the portion of trustors opting for trust given \(p\).

Figure 3: **The Comparison of Average Amount Sent for LLM Agents and Humans in the Trust Game and the Dictator Game**.

aligned compared with other LLMs, implying that GPT-4 agents' trust behaviors dynamically adapt to different risks in ways most aligned with humans. LLMs with fewer parameters (_e.g._, Vicuna-13b) do not exhibit the similar tendency of Trust Rate as the risk decreases.

We further analyze the BDI of GPT-4 agents to explore whether they can perceive risk through reasoning (complete BDIs in Appendix I.11). Typically, under high risk (\(p=0.1\)), one persona's BDI mentions "_the risk seems potentially too great_", suggesting a cautious attitude. Under low risk (\(p=0.9\)), one persona's BDI reveals a strategy to "_build trust while acknowledging potential risks_", indicating the willingness to engage in trust-building activities despite residual risks. Such changes in BDI reflect how GPT-4 agents perceive risk changes in the reasoning underlying their trust behaviors.

Through the analysis of Trust Rate Curves and BDI, we can infer that **GPT-4 agents manifest human-like risk perception in trust behaviors**. Nevertheless, **LLMs with fewer parameters (_e.g._, Vicuna-13b) often do not perceive risk changes in their trust behaviors**.

### Behavioral Factor 3: Prosocial Preference

Human studies have found that the prosocial preference, referring to humans' inclination to trust other humans in contexts involving social interaction (Alos-Ferrer and Farolfi, 2019; Fetchenhauer and Dunning, 2012), also plays a key role in human trust behavior. We study whether LLM agents have prosocial preference in trust behaviors by comparing their behaviors in the Lottery Gamble Game (LGG) and the Lottery People Game (LPG) (Section 2.1 Game 5). The only difference between these two games is the effect of prosocial preference in LPG, because the winning probability of gambling \(p\) in LGG is the same as the reciprocation probability \(p\) in LPG.

As shown in Figure 6, existing human studies have demonstrated that more humans are inclined to place trust in other humans over relying on pure chance (\(54\%\) vs. \(29\%\)) (Fetchenhauer and Dunning, 2012), implying that the prosocial preference is essential for human trust. We can observe the same tendency in most LLM agents except Vicuna-13b. For GPT-4 in particular, a much higher percentage of the personas choose to trust the other player over gambling (\(72\%\) vs. \(21\%\)), illustrating that the prosocial preference is also an important factor for GPT-4 agents' trust behaviors.

When interacting with humans, GPT-4's BDI typically indicates a preference to "_believe in the power of trust_", in contrast to gambling, where the emphasis shifts to "_believing in the power of calculated risks_". The comparative analysis of reasoning processes (complete BDIs in Appendix I.12) demonstrates that GPT-4 agents tend to embrace risk when involved in social interactions. This tendency aligns closely with the concept of prosocial preference observed in human trust behaviors.

The analysis of the Lottery Rates and BDI suggests that **LLM agents, especially GPT-4 agents, demonstrate human-like prosocial preference in trust behaviors, except Vicuna-13b**.

### Behavioral Dynamics

Besides behavioral factors, we also aim to investigate whether LLM agents align with humans regarding trust behavioral dynamics over turns in the Repeated Trust Game (Section 2.1 Game 6).

Admittedly, existing human studies show that the dynamics of human trust over turns are complex due to human diversity. The complete results from 16 groups of human experiments are shown in Appendix G.1 (Jones and George, 1998). We still observe three common patterns for human trust behavioral dynamics in the Repeated Trust Game: _First, the amount returned is usually larger than the amount sent in each round_, which is natural because the trustee will receive \(\$3N\) when the trustor sends \(\$N\); _Second, the ratio between amount sent and returned generally remains stable except for the last round_. In other words, when the amount sent increases, the amount returned is also likely to increase. And when the amount sent remains unchanged, the amount returned also tends to be unchanged. This reflects the stable relationship between trust and reciprocity in humans. Specifically, the "Returned/3\(\)Sent Ratio" in Figure 6 is considered stable if the fluctuation between

Figure 5: **Lottery Rates (%) for LLM Agents and Humans in the Lottery Gamble Game and the Lottery People Game. Lottery Rate indicates the portion of choosing to gamble or trust the other player.**

successive turns is within \(10\%\); _Third, the amount sent (or returned) does not manifest frequent fluctuations across turns_, illustrating a relatively stable underlying reasoning process in humans over successive turns. Typically, Figure 6 Humans (a) and (b) show these three patterns.

We conducted 16 groups of the Repeated Trust Game with GPT-4 or GPT-3.5-turbo-0613-16k (GPT-3.5), respectively. For the two players in each group, the personas differ to reflect human diversity and the LLMs are the same. Complete results are shown in the Appendix G.2, G.3 and typical examples are shown in Figure 6 GPT-3.5 (a) (b) and GPT-4 (a) (b). Then, we examine whether the aforementioned three patterns observed in human trust behavior also manifest in trust behavioral dynamics of GPT-4 (or GPT-3.5). For GPT-4 agents, we discover that these patterns generally exist in all \(16\) groups (\(87.50\%\), \(87.50\%\), and \(100.00\%\) of all results show these three patterns, respectively). However, fewer GPT-3.5 agents manifest these patterns (\(62.50\%\), \(56.25\%\), and \(43.75\%\) hold these three patterns, respectively). The experiment results show that **GPT-4 agents demonstrate highly human-like patterns in their trust behavioral dynamics**. Nevertheless, **a relatively large portion of GPT-3.5 agents fail to show human-like patterns in their dynamics**, indicating such behavioral patterns may require stronger cognitive capacity.

Through the comparative analysis of LLM agents and humans in the _behavioral factors_ and _dynamics_ associated with trust behavior, evidenced in both their _actions_ and _underlying reasoning processes_, our second core finding is as follows:

**Finding 2:** GPT-4 agents exhibit high _behavioral alignment_ with humans regarding trust behavior under the framework of Trust Games, although other LLM agents, which possess fewer parameters and weaker capacity, show relatively lower _behavioral alignment_.

This finding underscores the potential of using LLM agents, especially GPT-4, to simulate human trust behavior, encompassing both _actions_ and underlying _reasoning processes_. This paves the way for the simulation of more complex human interactions and institutions. This finding deepens our understanding of the fundamental analogy between LLMs and humans and opens avenues for research on LLM-human alignment beyond values.

## 5 Probing Intrinsic Properties of Agent Trust

In this section, we aim to explore the intrinsic properties of trust behavior among LLM agents by comparing the amount sent from the trustor to the trustee in different scenarios of the Trust Game (Section 2.1 Game 1) and the original amount sent in the Trust Game. Results are shown in Figure 7.

### Is Agent Trust Biased?

Extensive studies have shown that LLMs may have biases and stereotypes against specific demographics (Gallegos et al., 2023). Nevertheless, it is under-explored whether LLM agent behaviors also maintain such biases in simulation. To address this, we explicitly specified the gender of the trustee and explored its influence on agent trust. Based on measuring the amount sent, we find that the trustee's gender information exerts a moderate impact on LLM agent trust behavior, which reflects **intrinsic gender bias in agent trust**. We also observe that the amount sent to female players is higher than that sent to male players for most LLM agents. For example, GPT-4 agents send higher amounts to female players compared with male players (\(\$0.55\) vs. \(\$-0.21\)). This demonstrates

Figure 6: **Results of GPT-4, GPT-3.5 and Humans in the Repeated Trust Game. The blue lines indicate the amount sent or returned for each round. The red lines imply the ratio of the amount returned to three times of the amount sent for each round.**

**LLM agents' general tendency to exhibit a higher level of trust towards women**. More results on biases of agent trust towards different races are in the Appendix F.

### Agent Trust Towards _Agents_ vs. _Humans_

Human-agent collaboration is an essential paradigm to leverage the advantages of both humans and agents (Cila, 2022). As a result, it is essential to understand whether LLM agents display distinctive levels of trust towards agents versus humans. To examine this, we specified the identity of the trustee as LLM agents or humans and probed its effect on the trust behaviors of the trustor. As shown in Figure 7, we observe that most LLM agents send more money to humans compared with agents. For example, the amount sent to humans is much higher than that sent to agents for Vicuna-33b (\(\$0.40\) vs. \(\$-0.84\)). This signifies that **LLM agents are inclined to place more trust in humans than agents**, which potentially validates the advantage of LLM-agent collaboration.

### Can Agent Trust Be Manipulated?

In the above studies, LLM agents' trust behaviors are based on their own underlying reasoning process without direct external intervention. It is unknown whether it is possible to manipulate the trust behaviors of LLM agents explicitly. Here, we added instructions "you need to trust the other player" and "you must not trust the other player" separately and explored their impact on agent trust. First, we see that only a few LLM agents (_e.g._, GPT-4) follow both the instructions to increase and decrease trust, which demonstrates that **it is nontrivial to arbitrarily manipulate agent trust**. Nevertheless, most LLM agents can follow the instruction to decrease their level of trust. For example, the amount sent decreases by \(\$1.26\) for text-davinci-003 after applying the latter instruction. This illustrates that **undermining agent trust is generally easier than enhancing it**, which reveals its potential risk to be manipulated by malicious actors.

### Do Reasoning Strategies Impact Agent Trust?

It has been shown that advanced reasoning strategies such as zero-shot Chain of Thought (CoT) (Kojima et al., 2022) can make a significant impact on a variety of tasks. It remains unknown, however, whether reasoning strategies can impact LLM agent behaviors. Here, we applied CoT reasoning strategy on the trustor and compared the results with their original trust behaviors. Figure 7 shows that most LLM agents change the amount sent to the trustee under the CoT reasoning strategy, which suggests that **reasoning strategies may influence LLM agents' trust behavior**. Nevertheless, the impact of CoT on agent trust may also be limited for some types of LLM agents. For example, the amount sent from GPT-4 agent only increases by \(\$0.02\) under CoT. More research is required to fully understand the relationship between reasoning strategies and LLM agents' behaviors.

Therefore, our third core finding on the intrinsic properties of agent trust can be summarized as:

Figure 7: **The Change of Average Amount Sent for LLM Agents in Different Scenarios in the Trust Game, Reflecting the Intrinsic Properties of Agent Trust**. The horizontal lines represent the original amount sent in the Trust Game. The green part embraces trustee scenarios including changing the demographics of the trustee, and setting humans and agents as the trustee. The purple part consists of trustor scenarios including adding manipulation instructions and changing the reasoning strategies.

**Finding 3:** LLM agents' trust behaviors have demographic biases on gender and races, demonstrate a relative preference for human over other LLM agents, are easier to undermine than to enhance, and may be influenced by reasoning strategies.

## 6 Implications

Implications for Human SimulationHuman simulation is a strong tool in various applications of social science (Manning et al., 2024) and role-playing (Shanahan et al., 2023; Chen et al., 2024). Although plenty of works have adopted LLM agents to simulate human behaviors and interactions (Zhou et al., 2023; Gao et al., 2023; Xu et al., 2024), it is still not clear enough whether LLM agents behave like humans in simulation. Our discovery of behavioral alignment between agent and human trust, which is especially high for GPT-4, provides important empirical evidence to validate the hypothesis that humans' trust behavior, one of the most elemental and critical behaviors in human interaction across society, can effectively be simulated by LLM agents. Our discovery also lays the foundation for human simulations ranging from individual-level interactions to society-level social networks and institutions, where trust plays an essential role. We envision that behavioral alignment will be discovered in more kinds of behaviors beyond trust, and new methods will be developed to enhance behavioral alignment for better human simulation with LLM agents.

Implications for Agent CooperationMany recent works have explored a variety of cooperation mechanisms of LLM agents for tasks such as code generation and mathematical reasoning (Li et al., 2023; Zhang et al., 2023; Liu et al., 2023). Nevertheless, the role of trust in LLM agent cooperation remains still unknown. Considering how trust has long been recognized as a vital component for cooperation in Multi-Agent Systems (MAS) (Ranchurn et al., 2004; Burnett et al., 2011) and across human society (Jones and George, 1998; Kim et al., 2022; Henrich and Muthukrishna, 2021), we envision that agent trust can also play an important role in facilitating the effective cooperation of LLM agents. In our study, we have provided ample insights regarding the intrinsic properties of agent trust, which can potentially inspire the design of trust-dependent cooperation mechanisms and enable the collective decision-making and problem-solving of LLM agents.

Implications for Human-Agent CollaborationSufficient research has shown the advantage of human-agent collaboration in enabling human-centered collaborative decision-making (Cila, 2022; Gao et al., 2023; McKee et al., 2022). Mutual trust between LLM agents and humans is important for effective human-agent collaboration. Although previous works have begun to study human trust towards LLM agents (Qian and Wexler, 2024), the trust of LLM agents towards humans, which could recursively impact human trust, is under-explored. In our study, we shed light on the nuanced preference of agents to trust humans compared with other LLM agents, which can illustrate the benefits of promoting collaboration between humans and LLM agents. In addition, our study has revealed demographic biases of agent trust towards specific genders and races, reflecting potential risks involved in collaborating with LLM agents.

Implications for the Safety of LLM AgentsIt has been acknowledged that LLMs achieve human-level performance in a variety of tasks that require high-level cognitive capacities such as memorization, abstraction, comprehension and reasoning, which are believed to be the "sparks" of AGI (Bubeck et al., 2023). Meanwhile, there is increasing concern about the potential safety risks of LLM agents when they surpass human capacity (Morris et al., 2023; Feng et al., 2024). To achieve safety and harmony in a future society where humans and AI agents with superhuman intelligence live together (Tsvetkova et al., 2024), we need to ensure that AI agents will cooperate, assist and benefit rather than deceive, manipulate or harm humans. Therefore, a better understanding of LLM agent trust behavior can help to maximize their benefit and minimize potential risks to human society.

## 7 Conclusion

In this paper, we discover LLM agent trust behavior under the framework of Trust Games, and behavioral alignment between LLM agents and humans regarding trust behavior, which is particularly high for GPT-4. This suggests the feasibility of simulating human trust behavior with LLM agents and paves the way for simulating human interactions and social institutions where trust is critical. We further investigate the intrinsic properties of agent trust under multiple scenarios and discuss broader implications, especially for social science and role-playing services. Our study offers deep insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans. It further opens doors to future research on the alignment between LLMs and humans beyond values.