# Stratified Prediction-Powered Inference

for Hybrid Language Model Evaluation

 Adam Fisch\({}^{,*}\) Joshua Maynez\({}^{,*}\) R. Alex Hofer\({}^{}\)

Bhuwan Dhingra\({}^{}\) Amir Globerson\({}^{}\) William W. Cohen\({}^{}\)

\({}^{}\)Google DeepMind \({}^{}\)Google Research

{fisch,joshuahm,rofer,bdhingra,amirg,wohen}@google.com

Equal contribution.

###### Abstract

Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data. PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate--but potentially biased--automatic system, in a way that results in tighter confidence intervals for certain parameters of interest (e.g., the mean performance of a language model). In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies. Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for population parameters (such as averages) that is based on stratified sampling. In particular, we show both theoretically and empirically that, with appropriate choices of stratification and sample allocation, our approach can provide substantially tighter confidence intervals than unstratified approaches. Specifically, StratPPI is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data.

## 1 Introduction

Evaluating machine learning models requires _evaluation_ data. In particular, to iteratively improve on a method during development, or to reliably report an improvement, one needs to confidently and quantitatively assess the performance of the method. This is especially challenging for large language models (LLMs), where gathering high-quality annotations for generations can be difficult and time-consuming--and can ultimately become quite costly if gathering more than a few hundred examples.

One often-proposed approach to avoiding the evaluation bottleneck is to use a secondary LLM-based system to judge the output of the primary one. For instance, if the primary task is developing an LLM-based question-answering (QA) system, one can use a second LLM-based system that rates question/answer pairs as acceptable or not [6; 8; 17]. However, automated raters (i.e., _autoraters_) may be biased relative to the human raters they are intended to model, as others have noted [1; 9; 18; 23]. This can become substantially worse when models are tailored to hill climb on the autorater metrics, and eventually cease to become a good metric at all--a phenomenon commonly referred to as Goodhart's law, or reward hacking in the context of reinforcement learning from human feedback [13; 24].

We thus have two signals for assessing model performance. The first is human labels, which are typically accurate, but expensive to collect. As a result, usually only a small sample size is available, andan estimate based on these samples alone will have high variance. The second is autorater predictions, which are easy to collect for large sample sizes, but may also be systematically biased. The above may suggest that one must make a choice between either (i) a high-variance, but unbiased, estimate from a small human sample, or (ii) a lower-variance, but biased, autorater-based estimate. However, it turns out there are also statistically valid ways of _combining_ the auto-rater and human data for hybrid evaluations. Following [1; 3; 7] we call such methods _prediction-powered inference_ (PPI) methods. At a high level, PPI-based methods operate by using a small sample of examples labeled by both humans and autoraters to estimate the bias of the autorater. This bias is then used as a rectifier for the autorater estimate. The resulting estimate can then be shown to provide improved (i.e., tighter) confidence intervals for properties of interest for the system being evaluated, such as its true mean accuracy.

A weakness of standard PPI, however, is that it does not take heterogeneity into account. For example, in our QA setting, an autorater may have one accuracy when predicting if a model answer is correct, and a different accuracy when predicting if it is incorrect. This is especially true in cases where a correct answer is easy to verify (e.g., it is also present in Wikipedia), but harder to refute (e.g., no relevant external search results can be retrieved to either support or contradict it). In these settings, it may make sense to apply a different PPI strategy within each subdomain, depending on the local quality of the autorater. Moreover, we claim that such heterogenous settings are to be quite expected in practical applications. Inspired by the rich prior literature on stratified sampling and survey design [12; 19; 21], we therefore propose a _stratified_ approach to PPI, and show that it can be very advantageous when performance varies across subdomains, for either the autorater, or the model being evaluated.

On a technical level, it is not immediately clear how to apply stratification to PPI, as it involves two types of samples: one sample that is labeled by both humans and an autorater, and another (typically much larger) sample that is only labeled by the autorater. Extending the analysis of , in this work we show how confidence intervals based on the asymptotic normality of weighted M-estimators  can in fact be derived for the stratified PPI setting. The next challenge we address is how to determine the sample sizes used for stratification (i.e., the sample size of each stratum). Similar to recent work for active statistical inference , we further derive optimal rates that depend on certain moments of the underlying distribution that are generally unknown--and provide an approach for effectively approximating these. Finally, we provide extensive empirical evidence showing that our stratified approach (StratPPI) leads to considerably improved confidence intervals over both classical inference methods that use only human labels, and the baseline (unstratified) PPI approach.

## 2 Related Work

Prediction Powered Inference (PPI) was introduced in  as a method for obtaining tighter confidence intervals for parameters learned in supervised machine learning (e.g., coefficients in logistic regression) by also leveraging other model-based predictions on additional, unlabeled data. Related ideas were explored by , but with a focus on bootstrapping as a way for obtaining confidence intervals. PPI was then extended in several directions. For example  showed how the labeled data can be used for both estimating the parameters and the autorater model. PPI++  showed how to obtain confidence intervals that are easy to compute efficiently, and introduced a parameter for weighting the predictions of the autorater such that the overall statistical efficiency can be improved. As noted in previous works on PPI, these approaches are closely related to other statistical methods for introducing control variates based on autoevaluators , as well as augmented inverse propensity weighting  (see discussion in [1; 3]). Like this paper, prior work has also focused on using PPI/PPI++ for evaluating machine learning systems with autoraters, including for ranking chatbots in the Chatbot Arena  and evaluating retrieval-augmented generation systems .

Most relevant to our setting, the recent work of  focuses on _active_ sampling of examples to label during PPI, where the total number of queried labels is random, but less than the sampling budget \(n\) in expectation. Specifically, it proposes to label examples for which the autorater is less confident in its predictions, and corrects for the sampling bias with a variant of inverse propensity weighting. In contrast, our somewhat simplified approach takes inspiration from stratified sampling where a coarse stratification is defined in advance, and the total number of labeled samples is constant. Like , we derive an analogous optimal allocation strategy for a given budget, though we only apply it at the stratum level, and not for individual examples. Furthermore, while the variable allocation helps reduce variance in heterogenous settings, our stratified treatment also allows for a stratum-specific extension of 's estimated tuning parameters that further improves performance in complementary ways.

In terms of stratification specifically, concurrent work  also proposed Bayesian variants of several PPI methods, including stratified approaches. In contrast, the methods here do not require introducing priors, do not require running expensive Bayesian inference, and give more conventional, frequentist, guarantees of performance. The credible intervals produced by Bayesian methods are related to, but different from, the confidence intervals produced here, and by other prior PPI approaches [1; 3; 7].

Finally, there is a conceptual similarity between the PPI rectifiers and post-hoc calibration of a classifier. There is a rich literature on calibrating classifiers (e.g., [22; 25]), but there is a clear difference in goals between calibration and PPI: the former is aimed at modifying a learned autorater to make better probabilistic predictions, and the latter is aimed at obtaining better confidence intervals by making use of an existing autorater. That said, clearly one approach to improving PPI is to use a better-calibrated classifier, perhaps by taking some of the labeled data and using it for re-calibration. This is similar in motivation to the cross-PPI approach of . We leave such approaches as future work, but we do experimentally explore PPI approaches on both well-calibrated autoraters and poorly calibrated ones.

## 3 Preliminaries

We begin by briefly reviewing PPI, and specifically the efficient PPI++ variant of . Here, and in the rest of the paper, upper-case letters (\(X\)) denote random variables; lower-case letters (\(x\)) denote constants, and script letters (\(\)) denote sets, unless specified. All proofs are deferred to Appendix A.

Following [1; 3; 7], we assume an empirical sample \(S_{n}=\{(X_{1},Y_{1}),,(X_{n},Y_{n})\}\) of \(n\) i.i.d. examples drawn from some unknown, but fixed, distribution \(\), where \(X_{i}\) is an input, and \(Y_{i}\) is the target output. We also assume a larger sample \(_{N}=\{(_{1},f(_{1})),,(_{N},f( _{N}))\}\) where \(N n\), for which the target outputs are not available, but we have access to an _autorater function_\(f^{}\) which provides an approximation of \(Y\) given the observed input (we use \(^{}\) to denote the output space of the autorater as it is possible that \(^{}\), e.g., if \(f(X)=[Y X]\)).

As an example, assume \(X\) is a _(question, model answer)_ tuple where the answer is produced by an LLM-based QA system we wish to evaluate, \(Y\) is a \(0/1\) "gold" human rating of correctness of the answer, i.e., \(\{\)_human rates model answer as correct_\(\}\), and \(f(X)\), the autorater, imperfectly predicts this human rating \(Y\). One value of interest is then the average accuracy of the QA system, \([Y]\). More generally, given any convex loss function \(_{}(x,y)\) satisfying certain regularity conditions (see Definition A.1), we are interested in estimating the \(d\)-dimensional parameter \(^{*}\),

\[^{*}=*{argmin}_{}[_{}(X, Y)].\] (1)

In the statistics literature, this is broadly referred to as M-estimation (e.g., see ). Here, we focus on _mean_ estimation, for which the loss \(_{}(x,y)=\|y-\|^{2}\) has the optimum \(^{*}=[Y]\). However, our method generalizes to other typical losses such as the squared loss for linear regression, where the parameters \(^{*}\) are the regression coefficients, or, similarly, the log loss for other generalized linear models. Of course, the true \(^{*}\) is generally not known, because we cannot exactly calculate the expectation in (1). The goal of PPI, which we share in this work, is to use both the labeled and unlabeled data \(S_{n}\) and \(_{N}\) to obtain an asymptotically valid confidence interval \(_{,j}\) for \(^{*}_{j}\) that for any \(j[d]\) satisfies1

\[_{n,N}(^{*}_{j}_{,j}) 1-.\] (2)

The simplest confidence interval can be obtained using standard techniques by using only the labeled sample \(S_{n}\), and ignoring the autorater data \(_{n}\). However, PPI employs a clever trick which allows for also using the unlabeled sample \(_{N}\) to get tighter confidence intervals, as described next.

### A rectified prediction-powered loss

The key idea of PPI-based methods is to use autorater predictions to derive a low variance, but unbiased, estimate of the objective in (1). Consider the following "rectified" prediction-powered loss:

\[L^{}()=_{i=1}^{N}_{}( _{i},f(_{i}))}_{}\ +\ _{i=1}^{n}_{}(X_{i},Y_{i})-_{}(X_ {i},f(X_{i}))}_{}.\] (3)The rectifier term on the right hand side of (3) removes the bias of the autorater data so that \(L^{}()\) satisfies \([L^{}()]=[_{}(X,Y)]\). However, when \(f(X)\) is correlated with \(Y\), the loss \(L^{}()\) will have lower variance. Unfortunately, \(f(X)\) may not be a good predictor of \(Y\) in all cases. In fact, it is even possible for \(L^{}\) to be higher variance than the classical estimate (e.g., if \(f(X)\) is anti-correlated with \(Y\)). Thus, to adapt to the quality of \(f(X)\), PPI++ also introduces a tuning parameter \(\):2

\[L_{}^{}()=_{i=1}^{N}_{ }(_{i},f(_{i}))+_{i=1}^{n}_{}(X_{i },Y_{i})-_{}(X_{i},f(X_{i})).\] (4)

Clearly, \(L_{}^{}\) still remains unbiased for any value of \(\). For \(=0\), the framework reduces to classical inference. In most cases, however, a proper choice of \( 0\) will result in lower variance. PPI++ also suggests how to automatically choose a data-dependent \(\) that converges to an optimal value.

### A prediction-powered confidence interval

We next describe the PPI++ method for deriving confidence intervals based on the rectified loss. Let

\[_{}^{}=*{argmin}_{ }L_{}^{}().\] (5)

be the prediction-powered estimate. Standard analysis for M-estimators can then be extended to show that the scaled difference of the estimate \(_{}^{}\) and the true \(^{*}\) is asymptotically normally distributed. PPI++ then leverages this result to compute a confidence interval for \(^{*}\) that is asymptotically valid.

**Theorem 1** (Ppi++, ).: _Assume that \(\) and \( r 0\). Let \(H_{^{*}}:=[^{2}_{^{*}}]\), and_

\[V_{f,^{*}}^{}:=^{2}(_{^{*}}( ,f()), V_{,^{*}}^{}:=( _{^{*}}(X,Y)-_{^{*}}(X,f(X))),\] (6)

_where \(\) is a hyper-parameter. Then under the regularity conditions of Definition A.1, we have that \((_{}^{}-^{*}) (0,^{})\), where \(^{}:=H_{^{*}}^{-1}(r V_{f,^{*}}^{}+V_{ ,^{*}}^{})H_{^{*}}^{-1}\)._

**Corollary 1** (Ppi++ CI, ).: _Let \(^{}\) be the plug-in estimate for \(^{}\) using \(S_{n}\) and \(_{N}\). Define_

\[_{,j}^{}:=(_{,j}^{ } z_{1-}_{jj}^{ }}),\] (7)

_where \(z_{}\) denotes the \(\)-quantile of the standard normal distribution. Then for any \(j[d]\) it holds that \(_{n,N}(_{j}^{*}_{,j}^{ }) 1-\)._

As mentioned above,  further showed that for an appropriate choice of \(\) (that can be analytically derived and estimated via a \(\)), the trace of the covariance matrix, \((^{})\), is at most that of the covariance matrix derived without using autorater data, implying that the PPI++-based confidence set \(_{}^{}\) can always be at least as tight as that of the classical M-estimator (and often much tighter in practice).

## 4 Stratified prediction-powered inference

We now present our approach for improving PPI++ estimates via _stratification_. In particular, we show how optimizing a rectified loss computed via stratified sampling can lead to a consistent, but lower variance, estimate of the optimal parameter \(^{*}\), and correspondingly tighter confidence intervals. Consider the QA example from the previous section. For most autoraters, it is reasonable to assume that the strength of their performance can vary, depending on the type of input being presented. For instance, an autorater might be accurate at predicting whether an answer to an unambiguous question (e.g., _"What is the capital of France?"_) is correct, but relatively poor at inferring if an answer to an open-ended question (e.g., _"What is the best way to cook a hamburger?"_) is acceptable or not. Splitting the problem space into different domains allows us to derive a more specialized form of the prediction powered loss that can better adapt to this autorater heterogeneity via stratified sampling.

Formally, assume that the input space \(\) is partitioned in advance into \(K\) non-empty, mutually exclusive, and exhaustive strata \(=(_{1},,_{K})\), where \(K\) is a finite integer. For each stratum,we further assume that we can estimate \(w_{k}=(X_{k})\) arbitrarily well using large amounts of unlabeled data or prior knowledge; we treat them as known constants here for simplicity. To collect the labeled and unlabeled datasets \(S_{n}\) and \(_{N}\), we then follow a standard stratified sampling procedure in which we draw two i.i.d. sets of samples of fixed size \(n_{k}\) and \(N_{k}\), respectively, from each stratum \(k\), where \(_{k=1}^{K}n_{k}=n\) and \(_{k=1}^{K}N_{k}=N\). The relative sizes \(n_{k}/n\) and \(N_{k}/N\) are free parameters; they can simply be the natural rates, \(n_{k}/n N_{k}/N w_{k}\), or systematically decided (see SS4.3). Note that this is a fundamentally different sampling model from standard PPI: here examples are i.i.d. within each strata, and independent (but not necessarily identically distributed) across each strata.

Next, we define the stratified loss via the weighted sum, \(L^{}_{}()=_{k=1}^{K}w_{k}L^{}_{k, _{k}}(),\) where \(w_{k}\) is the stratum weight, \(=(_{1},,_{k})^{k}\) are now _stratum-specific_ tuning parameters, and \(L^{}_{k}()\) is the conditional prediction-powered loss computed within each stratum, i.e.,

\[L^{}_{k,_{k}}()=}{N_{k}}_{i=1}^{ N_{k}}_{}(_{ik},f(_{ik}))+}_{i=1}^{n_{k}} _{}(X_{ik},Y_{ik})-_{k}_{}(X_{ik},f(X_{ik})).\] (8)

As before, each \(L^{}_{k,_{k}}\) is an unbiased estimate of the conditional loss. Like PPI++, we also allow for data-dependent parameters \(_{k}\) that we show how to automatically tune for best performance in SS4.2.

### A stratified prediction-powered confidence interval

We now present our main result, which is a confidence interval for \(^{*}\) based on the stratified loss. More precisely, the result states that, as in PPI++, the minimizer of the stratified loss,

\[^{}_{}=*{argmin}_{}L ^{}_{}(),\] (9)

has an asymptotically normal distribution with mean \(^{*}\). See Algorithm 1 for pseudocode.

**Theorem 2**.: _Assume that \(_{k}_{k}, r\) for any \(r 0\), \(}{n}_{k}\) for any \(_{k}>0\), and \(}{N}_{k}\) for any \(_{k}>0\). Let \(H_{k,^{*}}:=[^{2}_{^{*}}(X,Y) X _{k}]\), and_

\[V^{_{k}}_{k,f,^{*}} :=_{k}^{2}(_{^{*}}(, f())_{k}),\] (10) \[V^{_{k}}_{k,,^{*}} :=(_{^{*}}(X,Y)-_{k} _{^{*}}(X,f(X)) X_{k}),\] (11)

_where \(_{k}\) is a hyper-parameter. Then, under the regularity conditions of Definition A.1,_

\[(^{}_{}-^{*})(0,^{}_{w}),\] (12)

_where \(^{}_{w}:=A^{-1}_{w}B^{}_{w}A^{-1}_{w}\) and:_

\[A_{w}:=_{k=1}^{K}w_{k}H_{k,^{*}} B^{}_{w}:=_{k=1}^{ K}w_{k}^{2}(_{k}} V^{_{k}}_{k,f,^{*}} +} V^{_{k}}_{k,,^{*}}).\] (13)

To obtain the result, we combine unstratified PPI++ with the asymptotic properties of weighted M-estimators . The resulting confidence interval for \(^{*}\) is then derived analogously to Corollary 1.

**Corollary 2**.: _Let \(^{}_{w}\) be the plug-in estimate for \(^{}_{w}\) using \(S_{n}\) and \(_{N}\). Define_

\[^{}_{,j}:=(^{}_{ {},j} z_{1-}^{}_{w,jj}}),\] (14)

_where \(z_{}\) denotes the \(\)-quantile of the standard normal distribution. Then for any \(j[d]\),_

\[_{n,N}(^{*}_{j}^{ }_{,j}) 1-.\] (15)

The form of the stratified prediction-powered confidence interval is similar to that of PPI++, except that it is based off of the weighted stratum-conditional covariance matrices. The effect of this change, however, is significant. In fact, in the case of mean estimation, we show that the asymptotic variance of StratPPI is at most that of PPI++ (even without any additional tuning of \(_{k}\) and \(_{k}\)).

**Proposition 1**.: _Let \(_{k}\) be any constant for all strata, and fix \(_{k}\) and \(_{k}\) to their natural rates \(w_{k}\). Then for \(_{}(x,y)=\|y-\|^{2}\) and any stratification \((_{1},,_{K})\), we have \((_{w}^{})(^{})\), where \(^{}\) is the asymptotic covariance matrix of PPI++. Furthermore, we have that equality holds if and only if both \([Y X_{k}]\) and \([f(X) X_{k}]\) are the same for all strata._

More generally, although our results will hold for arbitrary stratifications, it is best if they are heterogeneous, and chosen such that the individual stratum-conditional variances are minimized. For example, if an autorater systematically _over-estimates_ the model's performance on one subdomain, but systematically _under-estimates_ the model's performance on another, then splitting these subdomains into different strata can result in a lower variance \(L_{}^{}\). Similarly, if an autorater has much higher noise on some subdomains than others, it can be beneficial to stratify on those subdomains--and then either lower \(_{k}\), allocate a higher proportion of samples \(n_{k}/n\) and \(N_{k}/N\), or both for the noisier stratas. In SS5 we empirically demonstrate that the stratified estimator can indeed lead to considerably tighter confidence intervals in practice, especially with additional tuning of \(_{k}\) and \(_{k}\), as discussed next.

### Optimal weighting of the autorater predictions

In  it was shown that the optimal value of \(\) for PPI++ (i.e., the one minimizing the variances of the estimator and the corresponding confidence interval) could be found in closed form. We now present a simple extension of this result to the stratified case. For notational convenience, we use the shorthand \(_{k,}:=_{}(X,Y) X_{k}\) and \(_{k,}^{}:=_{}(X,f(X)) X _{k}\).

**Proposition 2**.: _Assume that \(_{k}\) and \(_{k}\) are fixed. Then the tuning parameters \((_{1}^{*},,_{k}^{*})\), where_

\[_{k}^{*}=(A_{w}^{-1}(( _{k,^{*}},_{k,^{*}}^{f})+( _{k,^{*}}^{f},_{k,^{*}}^{f}))A_{w}^{-1} )}{2(1+}{N_{k}})(A_{w}^{-1} (_{k,^{*}}^{f})A_{w}^{-1})},\] (16)

_minimize the cumulative asymptotic variance, \((_{w}^{})\)._

Note that \((_{w}^{})\) is proportional to the total size of the confidence interval \(_{}^{}\) in (14). Furthermore, as in , we can use plug-in estimates for the terms in (16) to compute a \(_{k}\), where \(_{k}_{k}^{*}\). From (16), we can see that \(_{k}^{*}\) is closely related to the correlation coefficient of the (curvature-scaled)gradients for minimizing the loss on an autorater label versus a true label. Intuitively, the more correlated these terms are, the more we can rely on the autorater labels for finding the true minimizer of \([_{}(X,Y)]\). For \(1\)-\(d\) mean estimation in particular, we can see that \(_{k}^{*}\) takes on a simple form:

**Example 1** (\(_{k}^{*}\) for mean estimation).: _Consider the \(1\)-\(d\) mean loss: \(_{}(x,y)=(y-)^{2}\). Then:_

\[_{k}^{*}=(Y,f(X))}{(1+}{N_{k}}) (f(X))}(Y,f(X))}{(f(X ))}N_{k},\] (17)

_which is equivalent to the optimal linear regression coefficient, \(_{_{k}}[|Y-_{k}f(X)|^{2} X_{k}]\)._

### Optimal allocation of the sampling budget

Our stratification approach has an additional hyperparameter \(\) that can also be tuned to reduce variance. Recall that \(_{k}\) determines the ratio between the labeled data size \(n_{k}\) for stratum \(k\) and the overall data size \(n\) (i.e., \(_{k}n_{k}=n\)). \(_{k}\) is similarly defined for the unlabeled data. Any strictly positive values of \(_{k}\) and \(_{k}\) are valid to be used, though not all values will improve performance. It turns out that the optimal \(_{k}\) values can be exactly calculated, as the following proposition shows.

**Proposition 3**.: _Assume that \(_{k}\) is fixed. Then the sampling rates \((_{1}^{*},,_{k}^{*})\) and \((_{1}^{*},,_{k}^{*})\), where_

\[_{k}^{*}=(A_{w}^{-1}V_{k,,}^{*}.A_ {w}^{-1})}}{_{k^{}=1}^{K}w_{k^{}}(A_{w}^{-1} V_{k^{},,}^{*}.A_{w}^{-1})}}_{k}^{*}= (A_{w}^{-1}V_{k,f,}^{*}.A_{w}^{-1})}}{_ {k^{}=1}^{K}w_{k^{}}(A_{w}^{-1}V_{k^{},f, }^{*}.A_{w}^{-1})}}\] (18)

_minimize the cumulative asymptotic variance, \((_{w}^{})\)._

Although the solution for \(_{k}^{*}\) is informative, it is not necessarily practical, as it depends on knowing \(A_{w}^{-1}V_{k,,}^{_{k}}\). \(A_{w}^{-1}\); this in turn depends on \(^{*}\) and \((X,Y)\), which are both unknown.3 In the special case of mean estimation, however, it turns out there is no dependence on \(^{*}\). To address the remaining dependence on \((X,Y)\), we propose to use autorater confidence scores, assuming they are available. Specifically, assume \(\) is discrete, and let \(c(y x)\) be the confidence of the autorater in label \(y\) given \(x\), where \(c(y x)\) approximates \((Y=y X=x)\). This will result in the estimate for \((A_{w}^{-1}V_{k,,}^{_{k}}.A_{w}^{-1})\) below, which can then be plugged into the expression for \(_{k}^{*}\) in Proposition 3.

**Example 2** (\(_{k}^{*}\) for mean estimation).: _Consider the \(1\)-\(d\) mean loss: \(_{}(x,y)=(y-)^{2}\). Then:_

\[(A_{w}^{-1}V_{k,,^{*}}^{_{k}}A_{w}^{-1})= (Y-_{k}f(X) X_{k}).\] (19)

(19) _can then be estimated using the observed, but unlabeled, samples scored by the autorater for each stratum \(k\), \(_{ik}=_{ik}\), \(i=1,,N_{k}\), as \((Y-_{k}f(X) X_{k})_{k}^ {2}\), where_

\[_{k}^{2} =}_{i=1}^{N_{k}}_{y}c(y _{ik})(y-_{k}f(_{ik})-_{k})^{2} \] (20) \[_{k} =}_{i=1}^{N_{k}}_{y}c(y _{ik})(y-_{k}f(_{ik})).\] (21)

For \(d\)-dimensional data, the result is similar, but with a sum of \(d\) variances (one for each dimension). We also provide a simplified expression for \(_{k}^{2}\) in Appendix B when \(f():=_{y}c(y) y\). Importantly, as we are free to use any \(_{k}>0\), using this estimate still preserves the asymptotic coverage guarantees in (14), regardless of if the confidence estimate \(c(y x)\) is calibrated or not. Empirically, we show that using this heuristic can indeed lead to substantial improvements.

## 5 Experimental results

We compare our stratified estimator, StratPPI, to two baselines: (i) the classical estimate, which uses only the labeled data, \(S_{n}\); and (ii) PPI++, which uses both \(S_{n}\) and \(_{n}\). All of our experiments focus on \(1\)-\(d\) mean estimation. We explore three different allocation strategies for StratPPI: the first is to set \(_{k}=w_{k}\) to be data proportional (StratPPI Prop.), the second is to set \(_{k}\) optimally via the oracle \(_{k}=_{k}^{*}\) (StratPPI Opt.), and the third is to use the approximation, \(_{k} w_{k}_{k}\), in Example 2 for \(_{k}=1\) when confidence scores are available (StratPPI Heur.). We use \(\)-tuning for both PPI++ and StratPPI, as outlined in SS4.2. Additional experimental results are given in Appendix C.

### Simulation studies

We start with a simple synthetic experiment that is an analogue of SS7.7.1 in . Our goal is to estimate the mean outcome \([Y]\), where \(Y(0,1)\). We assume that the input space \(\) is partitioned into \(K=2\) strata, \((_{1},_{2})\), of equal mass \((X_{1})=(X_{2})=0.5\). We then assume that predictions are formed as \(f(X_{ik})=Y_{ik}+_{k}+_{k}_{ik}\), where \(_{ik}(0,1)\). In other words, the predictions do not depend on the covariates \(X_{ik}\), other than to reflect a stratum-specific noise \(_{k}\) and bias \(_{k}\). We test three different scenarios: (i) where the two strata are homogeneous with \(_{1}=_{2}\) and \(_{1}=_{2}\); (ii) where the two strata have different prediction biases, \(_{1}_{2}\); and (iii) where the two strata have different prediction noise levels, \(_{1}_{2}\). For each experiment, we sample \(N=10{,}000\) total predictions \(f()\) using \(_{1}=_{2}=0.5\), i.e., proportional to masses of the two hypothetical, equal-weight strata. We then vary the total number \(n\) of labeled examples \(Y\), where the allocation is chosen according to \(\) (which differs depending on if we are using StratPPI Prop. or StratPPI Opt.). We show results in Figure 1 for the mean confidence interval (CI) size and coverage (i.e., the fraction of the cases where the CI contained the true parameter value \(^{*}\)) of each method, averaged over \(1k\) trials. As the plots in Figure 1 illustrate, when the underlying strata are homogeneous (left column), StratPPI behaves similar to PPI++. However, when the strata are heterogeneous (middle and right columns), StratPPI outperforms both baselines significantly--whereas PPT++ becomes barely

Figure 1: Mean estimation simulation study with \(K=2\) and \(=0.1\). The top row plots coverage (i.e., the fraction of the cases where the CI contained the true parameter value \(^{*}\)). The middle row plots the mean CI width (\(\) is better). Shaded areas plot the \(16/84\) quantiles across \(5\)k trials. The bottom row plots the RMSE of \(^{}\) computed across the \(5k\) trials, which shares the same trend with the mean CI width, as the estimator is unbiased. The left column shows a setting where strata are homogeneous, and StratPPI provides the no benefits over standard PPI++ (but is not worse). The middle and right columns show heterogeneous settings where the autorater has either a different bias (\(\)) or variance \(()\) per stratum, in which case StratPPI helps substantially. As strata variances are known, we only report proportional and optimal sample allocation results for StratPPI.

more powerful than the classical estimator. Additionally, we see that when the variance differs per stratum (right column), optimal allocation of the sampling budget indeed provides additional benefit.

### Real data studies

We now demonstrate how our method performs on real datasets, where the underlying structure of the autorater is unknown. To partition \(\), we choose to focus on stratifications that are based on the autorater's predictions, \(f(X)\), based on the intuition that autorater performance can often differ across the type of predictions that it makes. Concretely, if the output space \(^{}\) of \(f\) is discrete, then we define \(=^{}\); otherwise we define \(\) based on the equal-mass quantiles of \(^{}\) (which can be estimated by sampling a large set of unlabeled \(X\) and applying the autorater). We set \(K=10\). For all experiments, we plot performance as a function of \(n\), where \(n\) is the number of human ratings our system is allowed to observe from each dataset. The remainder of the dataset (including any data points that are unlabeled, or labeled but with the labels removed) is used for the autorater sample \(_{N}\).

**Seahorse.** The Seahorse dataset  focuses on multilingual summarization. The authors considered generative models that output summaries for a document, and collected labels for many systems that cover serveral dimensions of summary quality. We focus on one quality dimension--whether the summary is fully attributable to the source document--and on one summarization system--a fine-tuned 13B parameter mT5 model . The autorater models for each dimension are also mT5-XXL finetuned models, which output probability scores. The data contains \(2727\) examples for these two tasks, all of which have both human ratings as well as autorater scores.

**AttributedQA.** In attributed question answering , the goal of the QA system is to output both an answer, and a retrieved document that provides support for that answer. The system is only considered to be correct if the answer is both correct, and indeed supported by the linked document. We evaluate the highest-scoring "retrieve-and-read" QA system from this dataset,and define our autorater to be an 11B parameter T5 model  fine-tuned on a collection of natural language

Figure 2: Mean estimation on real data with \(K=10\) and \(=0.05\). The \(x\) axis plots the number of human-labeled examples \(n\); the \(y\) axis plots CI width, percent reduction in CI width against the classical estimate, and the effective sample size (the amount of human labels necessary to match the same confidence interval via classical inference). Shaded areas plot the \(16/84\) quantiles across \(1k\) trials. All StratPPI methods improve over classical inference and PPI++.

entailment tasks . Like Seahorse, the model predicts a probability for whether or not the QA system gave an attributable answer. This dataset has \(1000\) human labels and \(3000\) autorater labels.

**Galaxy.** To demonstrate the generality of StratPPI beyond LLM-based settings, we also consider the Galaxy dataset , where the task is to estimate the fraction of spiral galaxies in the local universe. The autorater used here is a ResNet classifier applied to images from the Sloan Digital Sky Survey (SDSS) . The classifier estimates the probability that the galaxy in question is spiral. We use \(16{,}743\) observations from the dataset which contain both the human and autorater labels.

**Methodology.** We follow a procedure similar to [1; 3] to study CI estimates as a function of \(n\). We report results on the following estimates: Classical inference, PPI++, StratPPI Prop. and StratPPI Heur. We do not report StratPPI Opt. since it is unknown for real data. For each value of \(n\), we sample \(n\) of the cases with human labels at an allocation rate \(_{k}\) (this rate is determined differently in StratPPI Prop. and StratPPI Heur.). As noted above, we construct the unlabeled dataset \(_{N}\) by joining the remaining labeled data (without utilizing the true labels) with any unlabeled data available for that dataset. We repeat this over \(1000\) trials, and for each trial we obtain the CI width. We report the mean of these widths in Figure 2, as well as the percent reduction in width over the classical inference baseline, \(^{}|-|C_{}^{}|}{|C_{ }^{}|} 100\%\). We also report the "effective sample size", which we define as the number of samples required to obtain a CI of the same width as the method at sample size \(n\) when using the classical inference baseline instead.

**Results.** Figure 2 shows a large improvement for StratPPI methods over both PPI++ and classical inference for most datasets. Specifically, though all CIs decrease substantially in absolute size with \(n\) as expected, we see that improvements in the percent reduction in CI width over PPI++ can be as large as \(0.10 0.20\), \(0.20 0.30\), and \(0.25 0.35\) points in Seahorse, AttributedQA, and Galaxy, respectively. Furthermore, we can observe that many of the datasets exhibit heterogeneous characteristics, for which heuristic allocation helps considerably. In Galaxy, this accounts for a \(+10\) percent reduction in CI width. In Seahorse and AttributedQA, the improvements are less strong but still clearly apparent. The practical implication of these results is that when limited by a human rating budget, StratPPI is able to produce an estimate of the mean with fewer human ratings via stratification and sampling allocation. For example, for the Seahorse dataset, we can see from the right column in Figure 2 that StratPPI Heur. with only \(300\) human ratings will be approximately as confident about the mean as the classical estimate that utilizes \(600\) human ratings, a factor of \(2\).

## 6 Conclusion

As systems built on top of large language models continue to become more and more advanced, it becomes increasingly challenging to evaluate their performance using automatic tools. Manual labeling, on the other hand, is slow and expensive. Methods which therefore save on annotation cost are critical for reliably evaluating models, and knowing when they are improving--or degrading. Prediction-powered inference (PPI) is a promising class of such hybrid evaluation methods, since it can leveraged to provably produce statistically valid confidence intervals, while also effectively reducing the number of human labels needed to obtain intervals of certain width. Our results demonstrate that we can push PPI even further by introducing a method for performing even lower variance M-estimation by employing stratified sampling. In particular, we find that stratifications based on the predictions of the autoraters themselves proves to be an powerful stratification technique.

**Limitations.** While the confidence intervals produced by StratPPI (and PPI) enjoy coverage guarantees, these guarantees are asymptotic. When finite-sample performance is of particular importance, techniques that afford stronger guarantees might be preferred [2; 4; 5]. We also note some non-trivial aspects of the stratified sampling setup: (i) the number of strata has to be fixed; if \(K\) scales with \(n\) a more careful treatment is required; and (ii) the assumed observation model is different from traditional i.i.d. settings--we must be able to sample fixed sized samples from each partition; and (iii) performance may not be improved if the selected stratification is not statistically useful. Furthermore, in practice, the human data might have already been collected, in which case the studied stratified sampling setup does not directly apply. We leave the study of such post-stratified estimators to future work.

**Broader impacts.** This paper introduces a more powerful statistical method for evaluating LLMs, by merging human evaluations with autoraters in a way that is aware of subdomain differences. Our goal is to help power more reliable evaluations with lower annotation effort, both in terms of cost and time.

**Acknowledgements.** We thank Jacob Eisenstein, Jonathan Berant, Anastasios Angelopoulos, Taylan Cemgil, Arnoud Doucet, and Chris Dyer for helpful comments and discussions.