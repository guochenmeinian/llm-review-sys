# Provable and Efficient Dataset Distillation

for Kernel Ridge Regression

Yilan Chen

UCSD CSE

yic031@ucsd.edu

&Wei Huang

RIEKN AIP

wei.huang.vr@riken.jp

&Tsui-Wei Weng

UCSD HDSI

lweng@ucsd.edu

###### Abstract

Deep learning models are now trained on increasingly larger datasets, making it crucial to reduce computational costs and improve data quality. Dataset distillation aims to distill a large dataset into a small synthesized dataset such that models trained on it can achieve similar performance to those trained on the original dataset. While there have been many empirical efforts to improve dataset distillation algorithms, a thorough theoretical analysis and provable, efficient algorithms are still lacking. In this paper, by focusing on dataset distillation for kernel ridge regression (KRR), we show that one data point per class is already necessary and sufficient to recover the original model's performance in many settings. For linear ridge regression and KRR with surjective feature mappings, we provide necessary and sufficient conditions for the distilled dataset to recover the original model's parameters. For KRR with injective feature mappings of deep neural networks, we show that while one data point per class is not sufficient in general, \(k+1\) data points can be sufficient for deep linear neural networks, where \(k\) is the number of classes. Our theoretical results enable directly constructing analytical solutions for distilled datasets, resulting in a provable and efficient dataset distillation algorithm for KRR. We verify our theory experimentally and show that our algorithm outperforms previous work such as KIP while being significantly more efficient, e.g. 15840\(\) faster on CIFAR-100. Our code is available at GitHub.

## 1 Introduction

Deep learning models are now trained on increasingly massive datasets, incurring substantial computational costs and data quality challenges. For instance, Llama 3 was pre-trained on over 15 trillion tokens, while the training of GPT-4 exceeded $100 million. Reducing these burdens is crucial. Dataset distillation  aims to distill a large dataset into a small synthesized dataset such that models trained on it can achieve similar performance to those trained on the original dataset. A good small distilled dataset is not only useful in saving computational cost and improving data quality but also has various applications such as continual learning , privacy protection , and neural architecture search .

While there have been many empirical efforts to improve dataset distillation algorithms , a thorough theoretical analysis is still lacking. Izzo and Zou  show single distill data is sufficient for a class of generalized linear models with one-dimensional output, where the data is assumed to follow a generalized exponential density function and the negative log-likelihood is optimized by gradient descent. For a linear ridge regression (LRR), Izzo and Zou  show \(d\) data points are needed to recover the original model's parameter for all regularization at the same time, where \(d\) is the dimension of the data and is large even for small datasets like MNIST  and CIFAR  (\(d=784\) and 3072) in computer vision. For kernel regression with Gaussian kernel, they show \(n\) data points are necessary, where \(n\) is the number of original data points and can be large for modern datasets, e.g.

\(n=60000\) and 50000 for MNIST and CIFAR (see Table 2). Maalouf et al.  use Random Fourier Features (RFF) to approximate shift-invariant kernels that may have an infinite-dimensional feature space, and construct \(p\) distilled data for such RFF model, where \(p\) is the dimension of the RFF model and can be \(( n)\) in general cases. The results in [9; 21], however, have a large gap compared with the empirical evidence that one data point per class can often achieve comparable performance to the original model [24; 39; 2; 38].

In this paper, by focusing on dataset distillation for kernel ridge regression (KRR), we show that _one data point per class is already necessary and sufficient to recover the original model's performance in many settings_, which is far less than \(n\) or \(p\) data points needed in prior works [9; 21]. Besides, our analysis is more general than prior works [9; 21] and can handle more and different models, including invertible neural networks, fully-connected neural networks (FCNN), Convolutional neural networks (CNN), and Random Fourier Features (RFF). Table 1 compares our theoretical results with previous analysis. We summarize our contributions as follows.

* In Sec. 4.1 and 5, for linear ridge regression (LRR) and KRR with surjective feature mappings, we show that one distilled data point per class is _necessary and sufficient_ to recover the original model's parameters and provide necessary and sufficient conditions for such distilled datasets. In addition, we show how to find distilled data that is close to real data in Sec. 4.2.
* In Sec. 5.2, for KRR with injective feature mappings of deep neural networks (NNs), we show that one data point per class is in general _not sufficient_ to recover the original model's parameters. However, \(k+1\) data points can be sufficient for deep linear NNs, where \(k\) is the number of classes.
* Our theoretical results enable us to directly construct analytical solutions for the distilled datasets, resulting in a provable and efficient dataset distillation algorithm for KRR in Algorithm 1. We verify our theory experimentally and show that our algorithm outperforms previous SOTA dataset distillation algorithm KIP  while being significantly more efficient, e.g. 15840\(\) faster on CIFAR-100.
* In Sec.6, we show our theoretical results can be used for several applications. First, it can be used as necessary or sufficient conditions for KIP-type algorithms to converge to a global minimum even if the loss function is highly non-convex. Second, our distilled dataset for KRR can provably preserve the privacy of the original dataset while having a performance guarantee.

## 2 Related works

**Dataset distillation.** Dataset distillation aims to distill a large dataset into a small synthesized dataset such that models trained on it can achieve similar performance to those trained on the original dataset. Previous approaches can be mainly divided into four categories : 1) Meta-model Matching: this category formulates the problem as a bilevel optimization problem and maximize the performance of the model trained on the distilled dataset . Some recent works such as KIP [24; 25], FRePo , RFAD , and RCIG  approximate the inner loop optimization of training neural networks by

    & **LRR** &  \\   & & surjective \(\) & non-surjective \(\) \\  Izzo and Zou  & \(d\) & - & \(n\) (Gaussian Kernel) \\  Maalouf et al.  & - & - & \(p\) (Shift-invariant Kernels) \\ 
**Our work** & \(k\), \((k d)\) & \(k,(k p)\) (Invertible NNs, FCNN, CNN, Random Fourier Features) & \(p\) in general (Deep nonlinear NNs). \\  & & Fourier Features) & \(k+1\) for deep linear NNs \\   

Table 1: Comparison with existing theoretical analysis of dataset distillation. The number of distilled data needed to recover original model’s performance and models analyzed. “-” means not applicable. For linear ridge regression (LRR) and kernel ridge regression (KRR) with subjective feature mapping, our results only need one distilled data per class (\(k d\) in our setting), which is far less than the existing work [9; 21] that require \(n\) or \(p\) points. As an example, \(k=10,d=3072,n=50000\) for CIFAR-10. The \(k,d,n\) of standard datasets are listed in Table 2. \(p\) is the dimension of feature mapping \(:^{d}^{p}\).

KRR with Neural Tangent Kernel  or neural network Gaussian process (NNGP) kernels . 2) Gradient Matching: this category minimizes the distance between the gradients of models trained on the original dataset and distilled dataset [39; 37; 15; 11]. 3) Trajectory Matching: this category aims to match the training trajectories of models trained on the original dataset and distilled dataset [2; 4; 8; 7]. 4) Distribution Matching: this approach directly matches the distribution of the original dataset and distilled dataset via a single-level optimization [38; 33; 36]. Our work is closely related to kernel-based dataset distillation algorithms [24; 25; 40; 17; 18] in category (1). Our theoretical analysis provides theoretical foundations and implications for these kernel-based algorithms.

**Theoretical analysis of dataset distillation.** In addition to the papers discussed in the introduction, Maalouf et al.  propose an efficient algorithm to construct a \(d^{2}+1\) core set of the original dataset for least mean squares problems. Maalouf et al.  further propose to use the SVD of the original dataset to construct a distilled dataset of size \(d\). Tukan et al.  utilize the idea of subset selection to improve the initialization and training procedure of dataset distillation. Our paper focuses on KRR and constructs \(k\) distilled data analytically, where \(k\) is usually much less than \(d\) (see Table 2).

## 3 Preliminaries

### Dataset Distillation

For an original dataset \(\{_{i},_{i}\}_{i=1}^{n}\), we denote \(=[_{1},,_{n}]^{d n}\) and \(=[_{1},,_{n}]^{k n}\), where \(d\) is the dimension of the data, \(k\) is the dimension of the label or the number of the classes, and \(n\) is the number of data points. The goal of dataset distillation is to create a synthetic dataset \(_{S}=[_{S_{1}},,_{S_{m}}]^ {d m}\) and \(_{S}=[_{S_{1}},,_{S_{m}}]^ {k m}\), with the number of distilled data points \(m n\), such that a model trained on this synthetic dataset (\(_{S},_{S}\)) can achieve similar performance to those trained on the original dataset.

As the data dimension is usually larger than the label dimension in practice, e.g. MNIST has \(d=728,k=10\) and other datasets have even larger \(d\), we consider \(d k\) in this paper. For a matrix \(\), we use \(^{+}\) to denote its pseudoinverse and \(()\) to denote its range space.

### Dataset Distillation for Kernel Ridge Regression (KRR)

**Original model**: Given a kernel \(K(,^{})=(),(^{})\), where \(:^{d}^{p}\) is the feature mapping from input space to a feature space of dimension \(p\), a KRR model \(f()=()\) trained on original data set with a predefined regularization \( 0\) tries to minimize following objective

\[_{}\|()-\|_{F}^{2}+ \|\|_{F}^{2}\]

where \(^{k p}\) and \(()=[(_{1}),,(_{n})] ^{p n}\). The solution can be computed analytically as \(=_{}()^{+}\), where

\[_{}()^{+}=\{(K(, )+_{n})^{-1}()^{}=( )^{}(()()^{}+ _{p})^{-1},&>0,\\ ()^{+},&=0..\]

and \(K(,)=()^{}()^ {n n}\). \(_{}()\) can be considered as regularized features. Linear ridge regression is a special case of kernel ridge regression (KRR) with \(()=\).

KRR is used in many dataset distillation algorithms [24; 25; 40; 17; 18]. In this paper, we mainly consider a finite-dimensional \(\). This matches the practical neural networks which are usually used in dataset distillation. For shift-invariant kernels with infinite-dimensional RKHS space, e.g. Gaussian kernel, they can be well approximated by finite-dimensional random Fourier features [27; 16].

**Distilled dataset model**: Similarly, a KRR trained on distilled dataset with regularization \(_{S} 0\) is \(f_{S}()=_{S}()\), where \(_{S}=_{S}_{_{S}}(_{S})^{+} ^{k d}\). Additionally, denote \(_{_{S}}=_{_{S}}(_{S})\) with \(()=\), i.e.

\[_{_{S}}^{+}=\{(_{S}^{ }_{S}+_{S}_{m})^{-1}_{S}^{ }=_{S}^{}(_{S}_{S}^{}+_{S} _{d})^{-1},&_{S}>0,\\ _{S}^{+},&_{S}=0..\]

  Dataset & \(k\) & \(d\) & \(n\) \\  MNIST  & 10 & 784 & 60000 \\ CIFAR-10  & 10 & 3072 & 50000 \\ CIFAR-100  & 100 & 3072 & 50000 \\ ImageNet-1k  & 1000 & 196608 & 1281167 \\  

Table 2: \(k\) (number of class), \(d\) (dimension of data), and \(n\) (number of training data) of standard datasets.

The goal of dataset distillation here is to find \((_{S},_{S})\) such that \(_{S}=\), where \(\) is supposed to be given, i.e. can be computed from the original dataset \((,)\).

## 4 Dataset Distillation for Linear Ridge Regression (LRR)

In this section, we first analyze the dataset distillation for the linear ridge regression (LRR). In Sec. 4.1, for a LRR model, we show that \(k\) distilled data points (one per class) are necessary and sufficient to guarantee \(_{S}=\). We provide analytical solutions of such \(_{S}\) allowing us to compute the distilled dataset analytically instead of having to learn it heuristically in existing works [24; 25; 40; 17; 18]. Then, in Sec 4.2, we show how to find distilled data that is close to real data. Lastly, for fixed data and only distilling labels, we show \(d\) points are needed in Sec 4.3.

### Analytical Computation for Linear Ridge Regression

**Theorem 4.1**.: _When \(m<k\), there is no \(_{S}\) can guarantee \(_{S}=\) unless the columns of \(\) are in the range space of \(_{S}\). When \(m k\) and \(_{S}\) is rank \(k\), let \(r=(m,d)\) and take \(=_{S}^{+}+(_{m}-_{S}^{ +}_{S})\), where \(^{m d}\) is any matrix of the same size as \(_{S}^{+}\). Suppose the reduced SVD of \(\) is \(=(_{1}^{},,_{r}^{ })^{}\) with \(_{1}^{}_{r}^{} 0\), the following results hold:_

1. \(_{S}>0\)_:_ \(_{S}=\) _if and only if, for any_ \(\) _defined above,_ \(_{S}^{ 2}}\) _and_ \(_{S}=(_{1},,_{r})^{}\) _where_ \(_{i}=\{0,&_{i}^{}=0,\\ _{i}^{ 2}}}{2_{i}^{}},&.\)__
2. \(_{S}=0\)_:_ \(_{S}=\) _if and only if_ \(_{S}=^{+}\) _for any_ \(\) _defined above._

Proof sketch.: The proof can be found in Appendix C. The key idea is that we want to solve \(_{S}\) from \(_{S}=_{S}_{_{S}}^{+}=\) (Note: \(\) is given and we can select/decide \(_{S}\)). When \(m<k\), this is an overdetermined system for \(_{_{S}}^{+}\). There is no solution for \(_{_{S}}^{+}\) in general therefore no solution for \(_{S}\). When \(m k\) and \(_{S}\) is rank \(k\), the solutions of \(_{_{S}}^{+}\) are given by \(_{_{S}}^{+}=_{S}^{+}+(_{m }-_{S}^{+}_{S})\), where \(^{m d}\) is any matrix of the same size as \(_{_{S}}^{+}\). However, not all such \(_{_{S}}^{+}\) corresponds to a \(_{S}\). To solve \(_{S}\), we assume we have the SVD of \(_{S}\) and solve it from the SVD of \(_{_{S}}^{+}\). \(\)

Intuitively, original dataset \((,)\) is compressed into \(_{S}\) through original model's parameter \(=(^{}+_{n} )^{-1}^{}\). When \(m=k\), i.e. one distilled data per class, \(=_{S}^{+}\) is deterministic and \(_{S}\) is fully determined by \(\) and \(_{S}\). In this case, when \(_{S}=0\) and \(\) is full rank, \(_{S}\) can be easily computed as \(_{S}=^{+}_{S}\). As an example, Figure 1 shows the distilled data for MNIST and CIFAR-100 when \(m=10/100\). When \(m>k\), i.e. more than one distilled data per class, there exist infinitely many distilled datasets since \(\) is a free variable to choose. When \(m=n\), one can verify that \(\) is a distilled dataset for itself by taking \(_{S}=\) and \(=(^{}+_{n})^{-1} ^{}\). When \(m>n\), we can generate more data than original dataset. Compared with  that needs \(d\) data, our approach only requires \(k\) data and usually \(k d\) in practice. Our result is also more flexible to distill any \(m k\) data points.

**Discussion.** The requirement for \(_{S}\) can be easily satisfied by setting \(_{S}^{ 2}}\) for a given \(\). If we want to fix a predefined \(_{S}\), e.g. \(_{S}=\), we need to sample different \(\) (by sampling different \(\)) so that \(_{S}^{ 2}}\) is satisfied. Theorem 4.1 generally suggests that a smaller \(_{S}\) is better for

Figure 1: Distilled data of MNIST (first row) and CIFAR-100 (second row) for LRR when \(m=k\).

constructing distilled data. Practical algorithms, e.g. KIP, FRePo, and RFAD, usually use a very small regularization, which may already satisfy the requirement.

In practice, we usually do not want the dataset to be singular. Below we show that it is easy to satisfy as long as \(\) is full rank and the rows of \(\) and \(\) are linearly independent.

**Proposition 4.1**.: _When \(m k\) and \(_{S},\) are rank \(k\), the \(_{S}\) in Theorem 4.1 is full rank for any full-rank \(\) such that \((^{})(^{})=\{\}\)._

### Finding Realistic Distilled Data

In Theorem 4.1, any \(\) satisfying the condition will guarantee the distilled dataset model to recover the original model's performance. For example, we can choose \(\) to be a random Gaussian matrix. However, to make the distilled data more realistic and generalizable, we can select \(m\) real training data \(}_{S}\) as initialization of distilled data and find the distilled data that is closest to \(}_{S}\).

**Corollary 4.1.1**.: _Given fixed \(}_{S},_{S}\), and \(_{S}\), the \(\) that satisfies Theorem 4.1 and minimize \(\|-}_{_{S}}^{+}\|_{F}\) is_

\[=_{S}^{+}+(_{m}-_{S}^{+ }_{S})(}_{_{S}}^{+}-_{S }^{+}),\]

_where \(}_{_{S}}^{+}\) is defined analogous to \(_{_{S}}^{+}\). Taking \(_{S}=}_{_{S}}\) can further minimize the distance._

When \(_{S}=0\), \(_{S}=}_{S}\) is the prediction of original model for \(}_{S}\). Combining this with Theorem 4.1, we summarize the computation of the distilled data in Algorithm 1 with \(()=\). Figure 2 shows some distilled data that is close to the real data or generated with random noise.

### Label Distillation

If we fix the \(_{S}\) and only distill labels, as also shown in , we need at least \(m=d\) to guarantee \(_{S}=\) because labels have fewer learnable parameters than data.

**Theorem 4.2**.: _For any fixed \(_{S}\),_

Figure 2: Initialized data (first row), distilled data generated from real images using techniques in Sec 4.2 (second row), and distilled data generated from random noise using techniques in Sec 4.1 (third row) for a LRR with \(m=500\) on MNIST and CIFAR-100. IPC: images per class.

1. _when_ \(m<d\)_, there is no_ \(_{S}\) _can guarantee_ \(_{S}=\) _in general unless the rows of_ \(\) _are in the row space of_ \(_{_{S}}^{+}\)_. The least square solution is_ \(_{S}=_{_{S}}\) _and_ \(\|_{S}-\|=\|(_{_{S}} _{_{S}}^{+}-_{d})\|\)_._
2. _when_ \(m d\)_, if_ \(_{S}\) _is rank_ \(d\)_, then_ \(_{S}=_{_{S}}\) _is sufficient for_ \(_{S}=\)_._

## 5 Dataset Distillation for Kernel Ridge Regression (KRR)

In the last section, we analyzed the dataset distillation for LRR. However, more complex models such as KRR and neural networks (NNs) are usually used in practice for better performance. Therefore, it is crucial to extend the analysis to these practical settings. In this section, we first extend the results of LRR to KRR in the feature space and then construct distilled data from desired features by considering two cases - subjective and non-surjective feature mappings.

The results in Theorem 4.1 of LRR can be directly extended to the KRR in the feature space by replacing data \(_{S}\) with features \((_{S})\) in Theorem 4.1. For completeness, we state it below.

**Theorem 5.1**.: _When \(m<k\), there is no \((_{S})\) can guarantee \(_{S}=\) unless the columns of \(\) are in the range space of \(_{S}\). When \(m k\) and \(_{S}\) is rank \(k\), let \(r=(m,p)\) and take \(=_{S}^{+}+(_{m}-_{S}^{ +}_{S})\), where \(^{m p}\) is any matrix of the same size as \((_{S})^{}\). Suppose the reduced SVD of \(\) is \(=(_{1}^{},,_{r}^{ })^{}\) with \(_{1}^{}_{r}^{} 0\), the following results hold:_

1. \(_{S}>0\)_:_ \(_{S}=\) _if and only if, for any_ \(\) _defined above,_ \(_{S}^{2}}\) _and_ \((_{S})=(_{1},,_{r}) ^{}\) _where_ \(_{i}=\{0,&_{i}^{}=0,\\ _{i}^{ 2}}}{2_{i}^{}},& .\)__
2. \(_{S}=0\)_:_ \(_{S}=\) _if and only if_ \((_{S})=^{+}\) _for any_ \(\) _defined above._

This shows that in the feature space, \(k\) features are necessary and sufficient to recover the original model's parameter. However, what we get is the feature of distilled data \((_{S})\) instead of distilled data \(_{S}\) itself. To get \(_{S}\), we need to construct the data from the features. To do this, we consider two cases - surjective and non-surjective \(\). For subjective \(\), we show that we can directly construct \(_{S}\) from \((_{S})\). For non-surjective \(\) such as neural networks (NNs), we show one data per class is in general not sufficient, but \(k+1\) data points can be sufficient for deep linear neural networks.

### Surjective Feature Mapping

When \(\) is surjective or bijective, we can always find a \(_{S}\) for a desired \((_{S})\). In this case, \(k\) distilled data (one data per class) is sufficient to recover the original model's performance, in contrast to  that needs \(p\) distilled data. We summarize the computation of the distilled data in Algorithm 1. Here we give some examples of surjective/bijective \(\).

**Example 5.1** (Invertible NN).: If \(\) is invertible such as invertible NNs used in normalizing flows, then we can directly compute \(=^{-1}(())\).

**Example 5.2** (Fully-connected NN (FCNN)).: For a (\(L+1\))-layer FCNN \(f(x)=(x)\) and

\[()=(^{(L)}(^{(2)}( ^{(1)}))).\]

where \(\) is a surjective or bijective activation function such as LeakyReLU, and \(^{(l)}^{d_{l} d_{l-1}}\) with \(d=d_{0} d_{1} d_{L}\) for \(l[L]\). If all \(^{(l)}\) are full rank, given \((x)\), we can compute

\[=(^{(1)})^{+}^{-1}((^{(L)})^{+}^{-1}(())),\]

where \(^{-1}\) is any right inverse of \(\). When some \(^{(l)}\) are not full rank, we can still compute an approximated solution.

**Example 5.3** (Convolutional Neural Network (CNN)).: CNN is known as a special type of FCNN. To illustrate, we give an example of a convolution layer of \(2 2\) filter. Let \(^{4}\) be a convolutional filter of size \(2\). Then the convolution operation with stride \(1\) can be represented as

\[()=_{1}&_{2}&0&0&&0&_{3}&_ {4}&0&&0\\ 0&_{1}&_{2}&0&&0&0&_{3}&_{4}&&0\\ &&&&&&&&&\\ 0&0&0&&_{1}&_{2}&0&&0&_{3}&_{4} \]

If the data is three-channel images, same operation can be done for each channel followed by a matrix that sums over three channels. When the matrix equation has a solution, the data can be solved from the feature.

**Example 5.4** (Random Fourier Features (RFF) ).: A shift-invariant kernel \(K(,^{})=K(-^{})\) can be approximated by random Fourier features \(K(,^{})(),(^{})\). Here \(()=}[(_{1}^{}+b_{1}), ,(_{p}^{}+b_{p})]^{}^{p}\) where \(_{1},,_{p}^{d}\) are independent samples from a distribution \(()= e^{-j^{}}K( )\ d\) (Fourier transform of \(K()\)) and \(b_{1},,b_{p}\) are i.i.d. sampled from the uniform distribution on \([0,2]\). For example, Gaussian kernel \(K(,^{})=e^{--^{}\|_{2}^{2}}{2 }}\) has \(()=(0,^{-2}_{d})\). Denote \(=[_{1},,_{p}]^{}\) and \(=[b_{1},,b_{p}]^{}\), then we have \(()=}(+)\). Whenever \(p d\) and \(\) is rank \(p\), given \(()\) we can solve \(\) as,

\[=^{+}(}()-).\]

To ensure the computed \(}(_{S})[-1,1]\), we can normalize it by its largest absolute value, which is equivalently scaling \(\) in Theorem 5.1 and does not affect the direction of \(_{S}\).

 use random Fourier features to approximate shift-invariant kernels that may have an infinite-dimensional feature space, and construct \(p\) distilled data for such RFF model, where \(p( n)\) in general cases. Their construction, however, only uses label distillation and the \(_{S}\) can be any random data. Our analysis constructs \(_{S}\) explicitly and shows that whenever the dimension of RFF \(p\) needs to approximate shift-invariant kernels is less than \(d\), \(k\) distilled data suffice to recover the performance of the original RFF model and approximate the original KRR with shif-invariant kernels.

### Non-surjective Feature Mapping

When \(\) is injective or non-surjective, given a \((_{S})\), we may not find an exactly matched \(_{S}\). However, we can find an approximated distilled data \(}_{S}\) first and then adjust \(_{S}\) to ensure \(_{S}\). As in the label distillation for LRR case, \(m p\) distilled labels can guarantee \(_{S}=\).

Below we show one data per class is in general not sufficient for non-surjective \(\), but \(k+1\) can be sufficient for deep linear NNs. Consider a deep NN, \(f(x)=(x)\) with

\[()=(^{(L)}(^{(2)}( ^{(1)}))).\]

where \(\) is an invertible activation function such as LeakyReLU, Sigmoid, and \(^{(l)}^{d_{l} d_{l-1}}\) with \(d=d_{0}<d_{1}==d_{L}=p\) for \(l[L]\). \(()\) is an injective function in this definition.

**Theorem 5.2**.: _For a deep nonlinear NN defined above with fixed \(\), assume \(^{(2)},,^{(L)}\) are full rank. Suppose \(_{S}=0\) and \(_{S}\) is rank \(k\). When \(m=k\), there is no distilled data \(_{S}\) that can guarantee \(_{S}=\) in general useless the columns of \(^{-1}((^{(2)})^{-1}(^{(L )})^{-1}^{-1}((_{S}^{+})^{+}))\) are in the range space of \(^{(1)}\)._

When \(m=k\), only \((_{S})=(_{S}^{+})^{+}\) that can guarantee \(_{S}=\). One data per class is not sufficient in general as long as \((_{S}^{+})^{+}\) is not in the range space of \(\). Although \(k\) data is not sufficient, we show \(k+1\) data can be sufficient for deep linear neural networks, where \(()=\).

**Theorem 5.3**.: _For a deep linear NN defined above with fixed \(\), assume \(^{(2)},,^{(L)}\) are full rank. Suppose \(_{S}=0\) and \(_{S},\) are rank \(k\). Denote \(=[_{l=1}^{L}^{(l)}(^{(1)})^ {+}(^{+}-_{p})]^{p 2p}\)._1. _When_ \(m=k\)_, there is no distilled data_ \(_{S}\) _that can guarantee_ \(_{S}=\) _in general useless the columns of_ \(^{+}_{S}\) _are in the range space of_ \(_{l=1}^{L}^{(l)}\)_._
2. _When_ \(m>k\)_, If_ \(\) _is full rank and its right singular vectors_ \(_{}^{2p 2p}\)_'s last_ \(p p\) _submatrix is full rank, then there exists a_ \(_{S}\) _such that_ \(_{S}=\)_._

_Proof sketch._ To find the distilled data theoretically, we need to guarantee 1) the feature \((_{S})\) need to guarantee \(_{S}=\) and 2) there are some distilled data \(_{S}\) corresponding to the feature. For the first condition, Theorem 5.1 gives the sufficient and necessary condition of \((_{S})\). However, the formulation involves a pseudoinverse of sum of matrices, which does not have a concise formulation and therefore is hard to handle when solving \(_{S}\). Instead, in Theorem C.2, we derive a sufficient condition of \((_{S})\) without pseudoinverse. For the second condition, \((_{S})=^{*}\) for a given \(^{*}\) is an overdetermined system of linear equations of \(_{S}\). We find the formulation of \((_{S})\) such that the overdetermined system has solutions. Then combining the two conditions together, we end up with an equation that has multiple free variables. Combing the variables together and solving the equation will give us the results. In the proof, we provide the algorithm to compute the distilled data when the assumptions are satisfied. \(\)

## 6 Applications

In this section, we show that our theoretical results can be useful in some applications. We first show the conditions in Theorem 5.1 can be necessary or sufficient conditions for KIP-type algorithms to converge in Sec 6.1. We also show our distilled dataset for KRR can provably preserve the privacy of the original dataset in Sec 6.2.

### An Implication for KIP-type Algorithms

In KIP , FRePo , RFAD , and RCIG , a loss function as follows is optimized:

\[L(_{S})=\|_{S}()-\|_{F }^{2}=\|_{S}(K(_{S},_{S})+_{S} _{m})^{-1}K(_{S},)-\|_{F }^{2}.\] (1)

Below we show our results can be sufficient conditions for the above loss function to converge to 0 even if the loss function is highly non-convex.

**Theorem 6.1**.: _Suppose \(()\) is full rank and \(\) is computed with \(=0\). Then_

1. _when_ \(n p\)_, the_ \((_{S})\) _that can guarantee_ \(_{S}=\) _in Theorem_ 5.1 _is sufficient for_ \(L(_{S})=0\)_._
2. _when_ \(n p\)_, the_ \((_{S})\) _that can guarantee_ \(_{S}=\) _in Theorem_ 5.1 _is necessary for_ \(L(_{S})=0\)_._

From this theorem, we see that KIP-type algorithms are enforcing \(_{S}=\) to some extent. While the KIP algorithm is computationally expensive, our solution for \((_{S})\) can be computed efficiently and directly utilized in KIP-type algorithms for efficient optimization.

### Privacy Preservation of Dataset Distillation

For our dataset distillation algorithm for KRR, we show that the original dataset cannot be recovered from the distilled dataset, therefore provably preserving the privacy of the original dataset while having the performance guarantees at the same time.

**Proposition 6.1**.: _Suppose \(n>k\) and \(\) is rank \(k\). Given \(_{S},\), for a distilled dataset \((_{S},_{S})\) that can guarantee \(_{S}=\) in Theorem 5.1, we can reconstruct \(\) from \((_{S})\). However, given \(\), there are infinitely many solutions for \(()\)._

Since there are infinitely many solutions for \(()\), it is impossible to recover \(()\) without additional information. As long as \(\) does not contain any information of \(\), then \(\) will not be able to recover from distilled dataset \((_{S},_{S})\). Note in Sec. 4.2, we use additional information (real images as reference points) to compute \(_{S}\). Therefore \(_{S}\) resembles original images and we may recover these original images from \(_{S}\). However, if we generate the distilled data with random noise, the distilled data will contain no additional information and protect the privacy of the original dataset. Insummary, we can control whether to generate realistic data by using real data as reference points or protect privacy by generating noisy distilled data as shown in Figure 2.

More formally, we prove that the distilled data can be differential private with respect to the original dataset if we take \(\) to be random Gaussian with suitable variance.

**Theorem 6.2**.: _Under the same setting of Theorem 4.1, suppose that \(=0\), all data are bounded \(\|_{i}\|_{2} B\), and the smallest singular value of the original datasets is bounded from below \(_{min}()>_{0}\). Suppose \(_{S}\) is independent of \(\) and unknown to the adversary. Let \([_{S}^{+}]_{i}\) denote its \(i\)-th row. Let \(,(0,1)\) and take the elements of \((0,^{2})\) with \(_{i[m]}\|[_ {S}^{+}]_{i}\|_{2}}{_{0}^{2}\|[ _{m}-_{S}^{+}_{S}]_{i}\|_{2}}\), then each row of \(_{S}\) is \((,)\)-differential private with respect to \(\)._

## 7 Experiments

**(I) Analytical Computation of Dataset Distillation.** In Table 3, we verify our theory of dataset distillation for LRR and KRR with subjective mapping. We compute the distilled dataset for different models using Algorithm 1. The models are KRR with different feature mappings: 1) identity mapping (linear model), 2) one-hidden-layer LeakyReLU neural network, and 3) Random Fourier Features (RFF) of Gaussian kernel. The feature mappings are constructed such that the feature dimension is equal to the data dimension, i.e. \(p=d\). For NNs, we use random initialized ones and use the activations as feature mappings. As increasing the depth of NNs does not improve the performance, we only use one hidden layer. For simplicity, we set \(_{S}=0\) for all experiments. To choose the original model's regularization \(\), we split the original training set into a training set and a validation set, and choose the \(\) that performs best on the validation set. The results show that our analytically computed distilled dataset can indeed recover the original models' parameters and performance. Some slight differences are caused by the numerical error in recovering the data from features and computing the KRR solutions. As the purpose of this experiment is to verify if the distilled dataset can recover the performance of a specific original model, we did not report the error bars.

**(II) Comparison with KIP.** In Table 4, we compare our algorithm with KIP in terms of performance and efficiency under the setting of a subjective mapping. The test accuracy of models trained on distilled datasets and averaged computational cost (GPU Seconds) are reported. The mean and standard deviation of test accuracy are computed over four independent runs. As the experiment (I), we use a randomly initialized one-hidden-layer LeakyReLU NN with \(p=d\) as the feature mapping. For KIP, we implement their algorithm where we optimize a loss function (1) and use label distillation at each training step. For our results, we compute the distilled dataset using Algorithm 1. Our algorithm performs better than KIP on CIFAR-10 and CIFAR-100 while being significantly more efficient. We did not report the result of KIP with IPC=50 on CIFAR-100 because the estimated running time is more than 110 hours.

   Dataset & IPC & Linear & FCNN & RFF \\  MNIST & Original model & 86.41 & 93.89 & 93.82 \\   & 1 & 86.41 & 93.89 & 93.82 \\  & 10 & 86.41 & 93.89 & 93.82 \\  & 50 & 86.41 & 93.85 & 93.82 \\  CIFAR-10 & Original model & 39.48 & 47.86 & 42.84 \\   & 1 & 39.48 & 47.87 & 42.84 \\  & 10 & 39.48 & 47.84 & 42.87 \\  & 50 & 39.48 & 47.81 & 42.73 \\  CIFAR-100 & Original model & 14.37 & 21.42 & 18.71 \\   & 1 & 14.37 & 21.41 & 18.70 \\  & 10 & 14.37 & 21.52 & 18.69 \\  & 50 & 14.37 & 21.49 & 18.57 \\   

Table 3: Verification of our theory. Test accuracy of original models and models trained on the distilled dataset. IPC: images per class.

This experiment mainly aims to show that our theoretical guarantee can be transferred to practice. As the proposed Algorithm 1 is mainly for KRR with surjective mappings, we verified it and compared it with baselines in this setting. We use a randomly initialized bijective NN in order to match previous algorithms that use a randomly initialized NN. If a pre-trained NN is used, the accuracy can be improved and may match the SOTA.

**(III) Privacy Protection.** In this experiment, we show our algorithm can be used to protect the privacy of the original dataset. Same as experiment (II), we use a one-hidden-layer LeakyReLU neural network with \(p=d\) as the feature mapping and train a KRR model on the original dataset. Then we distill the dataset using Algorithm 1 and generate the distilled data with random Gaussian noise. As shown in Figure 3, the distilled data for MNIST are essentially random noise, which protects the privacy of the original MNIST dataset. At the same time, the model trained on it can recover the original model's performance of 93.87% test accuracy.

## 8 Conclusion and Future Works

In this paper, by focusing on dataset distillation for KRR, we show that one data point per class is already necessary and sufficient to recover the original model's performance in many settings. For linear ridge regression and KRR with surjective feature mappings, we provide necessary and sufficient conditions for the distilled dataset to recover the original model's parameters. For KRR with injective feature mappings of deep neural networks, we show that while one data point per class is not sufficient in general, \(k+1\) data points can be sufficient for deep linear neural networks. Our theoretical results facilitate the direct construction of analytical solutions for distilled datasets, leading to a provable and efficient dataset distillation algorithm for KRR. Additionally, we have developed applications for KIP-type algorithms and privacy protection.

Several future research directions are worth exploring. First, while the current analysis shows that \(k\) data points are generally insufficient for non-surjective deep non-linear neural networks, determining the minimum number of distilled data points required remains an open question worthy of investigation. Second, this paper focuses on KRR with fixed feature mappings, which differs from some empirical works that train all neural network parameters. Extending the analysis to learnable feature mappings would bridge this gap and provide further insights.

   Dataset & IPC &  &  \\  & & Accuracy \(\) & Cost \(\) & Accuracy \(\) & Cost \(\) & Speedup \\  & & & (GPU Sec.) & & (GPU Sec.) & over KIP \(\) \\  MNIST & 1 & 93.44\(\)0.17 & 159 & **93.72\(\)0.14** & 16 & **9.9\(\)** \\  & 10 & **93.75\(\)0.10** & 554 & 93.69\(\)0.17 & 16 & **34.6\(\)** \\  & 50 & **93.72\(\)0.11** & 3114 & 93.62\(\)0.24 & 16 & **194.6\(\)** \\  CIFAR-10 & 1 & 45.83\(\)0.29 & 225 & **47.85\(\)0.10** & 21 & **10.7\(\)** \\  & 10 & 47.50\(\)0.29 & 594 & **47.76\(\)0.12** & 20 & **29.7\(\)** \\  & 50 & 47.48\(\)0.20 & 3510 & **47.77\(\)0.06** & 20 & **175.5\(\)** \\  CIFAR-100 & 1 & 20.08\(\)0.20 & 616 & **21.58\(\)0.15** & 20 & **30.8\(\)** \\  & 10 & 21.56\(\)0.16 & 9323 & **21.59\(\)0.15** & 20 & **466.1\(\)** \\  & 50 & - & \(\)396000 & **21.58\(\)0.13** & 25 & \(\)**15840.0\(\)** \\   

Table 4: Comparison between our algorithm and KIP.

Figure 3: Distilled images of MNIST generated from random noise for a two-layer neural network. \(m=10\) (first row) and \(100\) (second row).