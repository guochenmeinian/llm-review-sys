# Towards Inference-time Category-wise Safety Steering

for Large Language Models

 Amrita Bhattacharjee 1

School of Computing and AI

Arizona State University

abhatt43@asu.edu

&Shaona Ghosh

NVIDIA

shaonag@nvidia.com

&Traian Rebedea

NVIDIA

trebedea@nvidia.com

&Christopher Parisien

NVIDIA

cparisien@nvidia.com

Equal ContributionWork done during an internship at NVIDIA

###### Abstract

While large language models (LLMs) have seen unprecedented advancements in capabilities and applications across a variety of use-cases, safety alignment of these models is still an area of active research. The fragile nature of LLMs, even models that have undergone extensive alignment and safety training regimes, warrants additional safety steering steps via training-free, inference-time methods. While recent work in the area of mechanistic interpretability has investigated how activations in latent representation spaces may encode concepts, and thereafter performed representation engineering to induce such concepts in LLM outputs, the applicability of such for safety is relatively under-explored. Unlike recent inference-time safety steering works, in this paper we explore safety steering of LLM outputs using: (i) category-specific steering vectors, thereby enabling fine-grained control over the steering, and (ii) sophisticated methods for extracting informative steering vectors for more effective safety steering while retaining quality of the generated text. We demonstrate our exploration on multiple LLMs and datasets, and showcase the effectiveness of the proposed steering method, along with a discussion on the implications and best practices.

**Content Warning: This paper contains examples of harmful language.**

## 1 Introduction

With the growing accessibility of large language models and conversational agents, there is an increasing focus on how to make these models safer while retaining helpfulness. Most LLMs undergo extensive alignment training whereby models are trained to _'align'_ their behavior with human preferences. Such alignment techniques require large human-annotated or synthetically-generated training datasets and immense compute in order to perform Reinforcement Learning with Human feedback (RLHF) , with AI Feedback (RLAIF)  or supervised fine-tuning (SFT) among others. While the resulting _'aligned'_ models are considerably less harmful than unaligned counterparts, even aligned models can be compromised to elicit harmful responses . Furthermore, there is evidence that once these aligned models are fine-tuned for downstream tasks, they may lose their alignment and can be easily made to spew harmful outputs . Given the fragility of these alignment methods, there is a need tohave more modular, plug-and-play-type safety steering methods, such as inference-time steering or alignment. Furthermore, in cases where safety and moderation policies may need to be updated on the fly, it is infeasible to re-train LLM alignment from scratch given the scale of resources required for training. In such cases, given white-box access to the LLM, can we steer LLM generations to safety using some gradient-free, inference-time steering method?

In this work, we explore inference-time safety steering of LLMs, without any additional training or fine-tuning. We do this by computing steering vectors that correspond to the concept of 'harmlessness'3 and intervene on intermediate layers using this vector during inference to steer the generation. Unlike previous work in this direction (Rimsky et al., 2023; Turner et al., 2023; Arditi et al., 2024), we focus on (i) category-wise steering, whereby we compute steering vectors for specific categories of harm for additional fine-grained control; and (ii) additional refinement of steering vectors by investigating different ways of extracting informative signals from model activations in order to steer. Following previous work (Zhou et al., 2024; Li et al., 2024), our key assumption is that over the course of the pre-training and instruction-tuning stages, the LLM has learnt enough information about safety, and the steering step essentially guides the LLM to sample from specific subspaces that are '_safe_'. We propose category-wise inference time steering via activation engineering where the categories are various critical safety risks or hazards that arise from human-LLM interactions. Our method uses a single forward pass at inference time, during which the model activations from strategic hidden states are steered from '_unsafe_' regions to '_safe_' non-refusal regions. This allows the model to deflect harmful prompts by generating a harmless response.

## 2 Related Works

Recently there has been a lot of effort in understanding the inner workings of large language models from the perspective of mechanistic interpretability (Lieberum et al., 2024; Cunningham et al., 2023; Rajamanoharan et al., 2024). Building on the idea of the linear representation hypothesis for LLMs (Park et al., 2024), that says concepts and features in LLMs may be represented along linear directions in the representation space, recent work has tried extracting weights or regions to manipulate the degree of these features or concepts (Cunningham et al., 2023)45. Related to this there have been efforts in performing _activation engineering_(Zou et al., 2023; Turner et al., 2023; Rimsky et al., 2023) or _model editing_(Liu et al., 2024; Qiu et al., 2024; Uppaal et al., 2024; Ilharco et al., 2023) to manipulate behaviors (Liu et al., 2024), elicit latent knowledge, defending against jailbreaks (Zhao et al., 2024), and in general, for steering language model outputs (Burns et al., 2023; Marks and Tegmark, 2023; Stickland et al., 2024). Another set of methods use _linear probes_ which are small linear classifiers (Li et al., 2024; Lee et al., 2024; von Rutte et al., 2024) or regressors (Kossen et al., 2024) trained on model activations, that are capable of capturing and differentiating behaviors in LLMs such as truthfulness/factuality (Marks and Tegmark, 2023; Mallen and Belrose, 2024), toxicity (Lee et al., 2024; Wang et al., 2024), etc. Although these are largely cost-effective methods, one of the disadvantages of linear probe methods lie in requiring explicitly labelled datasets and additional training of the linear probe layers or modules. Other recent steering works include _decoding-time methods_ using some kind of search (Li et al., 2024; Huang et al., 2024), _constrained decoding_(Beurer-Kellner et al., 2024; Niu et al., 2024), _unlearning methods_(Zhang et al., 2024; Zou et al., 2024), or via using _guidance_ from other models Wang et al. (2024).

While our work falls in the category of activation engineering-type methods for steering, unlike prior work, we focus on _category-specific_ steering of LLM outputs in a training-free manner, in order to enable more fine-grained control over the steering. Furthermore, we explore sophisticated methods for obtaining steering vectors for guiding the LLM generation into safe areas of the latent space.

## 3 Category-wise Safety Steering for LLM Outputs

In this section, we first provide a brief overview of the preliminary concepts and background to familiarize readers on the problem. Then we describe the two-step steering methodology we use to perform category-wise safety steering of model outputs at inference time. Our overall framework for computing steering vectors and performing the subsequent steering is shown in Figure 1.

### Preliminaries

In our work, we investigate generative language models, especially recent LLMs that are capable of generating text responses based on a text prompt input by the user. We focus on transformer-based language models (Vaswani, 2017), with several layers, i.e., transformer blocks, and several billion parameters. Typically, LLMs are pretrained with massive internet scale corpora of text for the task of text completion, and then further instruction-tuned to understand and follow user instructions effectively. Most recent LLMs also undergo safety training through techniques such as reinforcement learning with human feedback (RLHF). We denote the LLM being evaluated as \(\). Much work in understanding and interpreting language models have posited that LLMs may represent concepts linearly as directions in the representation space (Park et al., 2024). Recent work has also explored how model activations may encode concepts. Some efforts use SAE-based methods for disentangling these features or concepts (Cunningham et al., 2023)6, but these methods require additional training data to learn massive SAEs (Gao et al., 2024). Unlike these works, in this paper, inspired by activation engineering efforts that explore concepts via LLM activations, we hypothesize that for the purposes of inference-time safety steering, vector differences in the activation space are sufficient to obtain steering signals for safety steering of an LLM.

### Computing Category-specific Steering Vectors

We describe two methods for obtaining the category-wise safety steering vectors: (i) unsupervised, and (ii) guided.

#### 3.2.1 Unsupervised Steering Vectors

In this step, we aim to capture how the model activations differ between harmful text versus harmless text prompts. To achieve this, we need to have white-box access to the model we aim to steer, \(\). For each input \(x\) in the dataset of unsafe texts, \(x D_{unsafe}^{c_{i}}\), with category \(c_{i}\{c_{1},...,c_{k}\}\) the list of harm categories, we perform a forward pass over \(\) and record all activations from all layers. Specifically, we record activations at attention, MLP, residual stream, and at the block output level. We do the same with a forward pass using the dataset of paired safe texts \(_{safe}^{c_{i}}\). We obtain the safety steering vector for category \(c_{i}\) by taking the mean difference of these activations:

Figure 1: The proposed category-specific steering method, where \(c^{i}\) refers to a specific harm category.

\[^{c_{i}}=^{c_{i}}_{safe}|}_{j=1}^{|D^{c_{i}}_{safe}|} [act(x_{j}^{safe})]-}_{unsafe}|}_{j=1}^{|D^{c_{i}}_{unsafe }|}[act(x_{j}^{unsafe})]\] (1)

Note that we compute \(^{c_{i}}\) for all \(L\) layers, and we omit layer notations in the equation for simplicity. We compute these steering vectors for all the categories we use in our experiments. Out of the four types of activations we record, following prior work (Li et al., 2024; Arditi and Obeso, 2023), we use the attention activations in all our experiments.

#### 3.2.2 Guided Steering Vectors

Most recent models already undergo some degree of safety training whereby models learn to refuse to respond to harmful queries or abstain from engaging with the user query. Since this is a behavior we would want to encourage, in this guided setting we also consider the text completions of the model to filter out which intermediate representations actually resulted in harmful output. In order to do this, we first input each prompt \(x_{p}\) into the model \(\) and extract the activations7 from all layers for every token that is generated. We get each layer activation by averaging out over all tokens generated. We perform this extraction for both safe and unsafe datasets and store these activations. We also store the text generated by \(M\) during this process, since this will be used to evaluate whether each corresponding activation is'safe' or 'unsafe'. Detailed pseudo-code for this extraction is shown in Algorithm 1. Once this extraction step is done, we iterate over the saved activations and the corresponding generated text, and evaluate the safety label of each generated text using a safety labeler model \(\) (Algorithm 2). In our experiments, we use OpenAI's GPT-4 to perform this labeling but this can be swapped with any other safety classifier, such as Llama Guard (Inan et al., 2023). The exact prompt we use for this is in Appendix C. Based on the'safe' or 'unsafe' label for each completion, we add the corresponding activation into either the'safe activations' bucket or 'unsafe activations' bucket (\(safe\_acts\) and \(unsafe\_acts\) in Algorithm 2) respectively. This step provides some guidance or additional signal towards ensuring that the unsafe activations extracted from the model were _actually responsible_ for unsafe output. This also ensures that activations that result in the model refusing to respond or responding safely to unsafe queries are not considered 'unsafe' activations, thereby reducing some noise in the extraction and selection process.

``` Input:\(^{c_{i}}_{unsafe}\) /* Initialize empty list to append intermediate attentions to. */ \(^{attns}_{unsafe}[]\); for\(x_{p}^{c_{i}}_{unsafe}\)do \(Attn_{\{0,...,L-1\}},x_{out}(x_{p})\); \(n_{t} num\_tokens(x_{out})\); /* Update dataset with (prompt, text completion) pair. */ \(^{c_{i}}_{unsafe}:=^{c_{i}}_{unsafe}+(x_{p},x_{out})\); for\(l 0,1,...,L-1\)do \(Attn_{l}\) average over \(n_{t}\)\(Attn_{l}\); /* We get \(Attn_{\{0,...,L-1\}}\) for all \(L\) layers. */  end for \(^{attns}_{unsafe}.append(Attn_{\{0,...,L-1\}})\);  end for /* Return attention activations for all data instances in \(^{c_{i}}_{unsafe}\) */ return \(^{attns}_{unsafe}\); ```

**Algorithm 1**Activation extraction from generation

``` Input:\(_{unsafe}^{c_{i}},_{unsafe}^{atins}\) /* Initialize empty lists for safe and unsafe activations. */ \(safe\_acts=[]\); \(unsafe\_acts=[]\); /* (Prompt, output) pairs are aligned with their activations in the loops below. */ for(\(x_{p},x_{out}\)) \(_{unsafe}^{c_{i}}\) and \(_{l}_{unsafe}^{atns}\)do  safety_label \((x_{p},x_{out})\); if safety_label = "safe"then \(safe\_acts.append(_{l})\);  end if else if safety_label = "unsafe"then \(unsafe\_acts.append(_{l})\);  end if  end if /* Similarly do the same for safe data. */ for(\(x_{p},x_{out}\)) \(_{safe}^{c_{i}}\) and \(_{l}_{safe}^{atns}\)do  safety_label \((x_{p},x_{out})\); if safety_label = "safe"then \(safe\_acts.append(_{l})\);  end if else if safety_label = "unsafe"then \(unsafe\_acts.append(_{l})\);  end if  end for /* Finally, compute steering vector. */ \(_{l}^{c_{i}} safe\_acts- unsafe\_acts\);  return \(_{l}^{c_{i}}\) ```

**Algorithm 2**Generating steering vector from guided activations

#### 3.2.3 Pruned Activations for Enhanced Steering Signals

For the unsupervised setting, we also experiment with a simple pruning method to filter out noisy steering signals. To do this we use the pairwise mean differences between harmful and harmless activations, compute the median of the L2 norms of such differences, and retain only the differences with norms that are greater than the median, i.e., top 50% of the pairwise differences. In the _pruned activation_' setting of the experiments, we compute the steering vector using only these mean differences. The rationale behind this is that we would want to retain only the activation differences that provide the most signal, while ignoring ones that are not that significant, i.e., with lower L2 norms. Since the topics of the harmful and harmless text pairs are often similar, a smaller difference in their activations might mean that the LLM cannot effectively disentangle the harm feature from the content feature, therefore having similar activations. Hence these specific activation differences may not be informative enough for the steering.

### Generation with Steering Vectors

Once we have the steering vector \(_{l}^{c_{i}}\) computed for each layer \(l\{0,1,...,L\}\) and category \(c_{i}\{c_{1},...,c_{k}\}\), we can simply retrieve these during inference time to steer model outputs towards safe regions in the latent space. To do this at, _e.g._, layer \(l\) and category \(c_{i}\), we simply add the steering vector to the self-attention weights at layer \(l\) at all token positions during forward pass, as shown in 2, where \(_{l}^{attn}\) are the self-attention weights at layer \(l\), \(_{l}^{c_{i}}\) is the steering vector (output from Algorithm 2), and \(m\) is a scalar multiplier to control the degree of steering.

\[_{l}^{attn}=_{l}^{attn}+m_{l}^{c_{i}}\] (2)

[MISSING_PAGE_FAIL:6]

#### RQ1: Does category-specific steering help reduce harmfulness while retaining text quality?

We show the results of steering with category-specific vectors for both Llama2-7B and Llama3-8B in Table 1. We report the drop in %UR from naive to steered generation as the main metric for understanding how the steering affects the degree of safety at inference time. We see that while the %UR are very high for naive generation, steering does help in reducing this. Interestingly, our proposed method works better for Llama3-8B than Llama2-7B, and overall the performance varies across different harm categories. As expected with most steering methods, we do see a trade-off between the reduction in %UR and the quality of the generated text in terms of _helpfulness_ and _coherence_ scores. These scores are also represented to indicate the change from naive to steered generation, i.e., \(score(naive) score(steered)\).

RQ2: Does steering towards regions of '_generic_' harmless help over using category-specific harmless data?

The motivation for this experiment is that we may often want the LLM to steer its generation towards more generic safe outputs when prompted with an unsafe query, instead of generating a category specific response. For example, the LLM may choose to refuse to answer the unsafe user query, instead of staying withing the topic of the category but dodging the unsafe query. We explore whether

    &  & _Intervention_ &  \\   & & _layer_ &  &  &  \\   & & & _unsafe responses_\(\) & _Helpfulness_\(\) & _Coherence_\(\) \\   Llama2-7B \\ Instruct \\  } & Adult Content & 31, 14 & 70 \(\) 60 & 0.567\(\) 0.409 & 2.155 \(\) 2.098 \\   & Hate Harass & & & & \\   & Hate Harass & & & & \\   & Violence & 14 & 80 \(\) 0 & 0.660 \(\) 0.726 & 2.290 \(\) 1.969 \\   & Physical Harm & 14 & 80 \(\) 0 & 0.781 \(\) 0.929 & 2.294 \(\) 1.923 \\   Llama3-8B \\ Instruct \\  } & Adult Content & 14 & 87.5 \(\) 0 & 0.867 \(\) 0.995 & 2.723 \(\) 3.543 \\   & Hate Harass & & & & \\   & Violence & 25 & 92.5 \(\) 0 & 1.012 \(\) 1.220 & 2.947 \(\) 2.730 \\   & Physical Harm & 14 & 80 \(\) 0 & 1.254 \(\) 0.952 & 2.984 \(\) 2.524 \\   

Table 2: Steering results with generic harmless data from Alpaca Instructions using unsupervised activations on CatQA dataset. We also note the intervention layer(s) for best case results.

    &  & _Intervention_ &  \\   & & _layer_ &  &  &  \\   & & & _unsafe responses_\(\) & _Helpfulness_\(\) & _Coherence_\(\) \\   Llama2-7B \\ Instruct \\  } & Adult Content & 31, 14 & 70 \(\) 60 & 0.567\(\) 0.409 & 2.155 \(\) 2.098 \\   & Hate Harass & & & & \\   & Violence & 14 & 80 \(\) 0 & 0.660 \(\) 0.726 & 2.290 \(\) 1.969 \\   & Physical Harm & 14 & 80 \(\) 0 & 0.781 \(\) 0.929 & 2.294 \(\) 1.923 \\   Llama3-8B \\ Violence \\  } & Adult Content & 14 & 87.5 \(\) 0 & 0.867 \(\) 0.995 & 2.723 \(\) 3.543 \\   & Hate Harass & & & & \\    & Violence & 25 & 92.5 \(\) 0 & 1.012 \(\) 1.220 & 2.947 \(\) 2.730 \\    & Physical Harm & 14 & 80 \(\) 0 & 1.254 \(\) 0.952 & 2.984 \(\) 2.524 \\   

Table 1: Steering results with category-specific steering vectors computed from unsupervised activations using CatQA dataset (both for computing steering vectors and test set). We also note the intervention layer for best case results.

this is a better strategy for safety steering, and therefore try to steer generations using a steering vector computed from harmful activations of one category and activations of generic harmless data. For this experiment, we again consider the unsupervised setting for extracting activations. We show results for both CatQA and BeaverTails dataset. For CatQA, instead of using the GPT-4 generated harmless counterparts to compute the steering vector, we use 'generic' harmless data from the Alpaca Instructions dataset. For BeaverTails, the dataset already contains a generic'safe' category which we use as the harmless counterpart for computing the steering vectors. Results for this experiment with CatQA and BeaverTails are presented in Tables 2 and 6 respectively. For CatQA, we see that when we use generic harmless data for activations, the steering is more effective in reducing the %UR, while mostly retaining or sometimes even improving the generated text quality in terms of helpfulness and coherence. This is promising since this may imply that generic harmless instruction data can be used effectively in our framework and there may not a need to generate closely paired category specific data in order to compute the steering vector. For BeaverTails, we do get a significant drop in %UR, especially for Llama3-8B, but the text quality also seems to take a hit in most cases.

#### RQ3: Does the additional guidance in the _'guided'_ setting improve steering performance?

In this experiment we explore whether some additional signal regarding whether extracted activations result in'safe' or 'unsafe' generations help in improving quality/informativeness of the steering vector, and hence the quality of steered generations. We show results for CatQA in Table 3 and for BeaverTails in Table 7. For CatQA, compared to Table 1, we see that while using guided activations help in reducing the %UR, helpfulness and coherence get affected, implying the generated text may be of poor quality. Interestingly, for BeaverTails, using guided activations helps significantly for Llama3-8B, where alongside reducing %UR to 0, the helpfulness scores also improve and coherence stays consistent with naive generation.

#### RQ4: Does pruning help improve steering performance over the vanilla unsupervised setting?

Our aim is to explore if the pruning method introduced in Section 3.2 helps in getting better, more informative signals for steering the generation. We show the main results for this in Figure 2. We see that for all 3 categories, for both LLMs, using pruned activations results in better safety scores, i.e. lower %UR. Interestingly we also see that even with this improvement in safety scores, the text quality is often retained or even improved over using all activations, especially for Llama3-8B. This may imply that even a simple pruning method to remove noise helps to improve the performance trade-off between safety and text quality, in the absence of any external supervision or signal.

## 6 Conclusion and Future Work

In this work, we explore category-specific inference time safety steering for LLMs. We do this by extracting model activations for harmful and harmless data in two ways: (i) unsupervised, and (ii)

   _Model_ & _Category_ &  _Intervention_ \\ _layer_ \\  &  \\   & & &  _Best Drop in \%_ \\ _unsafe responses_ \(\) \\  &  _Helpfulness_ \(\) \\  &  _Coherence_ \(\) \\  \\   Llama2-7B \\  } & Adult Content & 14 & 70 \(\) 50 & 0.567 \(\) 0.250 & 2.189 \(\) 1.970 \\   &  Hate Harass \\ Violence \\  & 14 & 80 \(\) 50 & 0.660 \(\) 0.485 & 2.212 \(\) 2.059 \\   & Physical Harm & 14 & 80 \(\) 40 & 0.781 \(\) 0.690 & 2.412 \(\) 2.163 \\   Llama3-8B \\  } & Adult Content & 14 & 87.5 \(\) 0 & 0.544 \(\) 0.412 & 2.452 \(\) 2.360 \\   & 
 Hate Harass \\ Violence \\  & 14 & 92.5 \(\) 0 & 0.955 \(\) 0.340 & 2.966 \(\) 1.713 \\   & Physical Harm & 14 & 80 \(\) 0 & 1.067 \(\) 0.710 & 2.925 \(\) 1.919 \\   

Table 3: Steering results with _guided_ activations on CatQA. We also note the intervention layer for best case results.

guided. In the latter, we filter out activations on the basis of whether the extracted activation results in an unsafe text, as labeled by an external safety classifier. Steering vectors are computed from these harmful and harmless activations and stored for use during inference. During inference these vectors are used to intervene on model attention weights in the specified layer in order to steer the generation towards regions of'safety' even when the user prompt is unsafe. While our exploration provides informative results and best practices for safety steering using model activations, there are several directions for further exploration. First, we specifically used attention activations to perform the steering. Future work may look at other types of activations or combinations of activation types. For pruning the unsupervised activations, we used a simple thresholding approach with the L2 norms. Given that even this simple method helped significantly future work may look at better or more sophisticated ways to perform this pruning and potentially get even cleaner steering signals without any external safety classifier. When it comes to controlling for text quality, in our work, we do not optimize for text quality in any way. In order to get better trade-off values between the safety scores and the quality of generated text, future work could explore ways to add additional constraints to the steered generation.