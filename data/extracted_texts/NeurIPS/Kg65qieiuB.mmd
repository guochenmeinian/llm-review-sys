# Demystifying Oversmoothing in

Attention-Based Graph Neural Networks

Xinyi Wu\({}^{1,2}\) Amir Ajorlou\({}^{2}\) Zihui Wu\({}^{3}\) Ali Jadbabaie\({}^{1,2}\)

\({}^{1}\)Institute for Data, Systems and Society (IDSS), MIT

\({}^{2}\)Laboratory for Information and Decision Systems (LIDS), MIT

\({}^{3}\)Department of Computing and Mathematical Sciences (CMS), Caltech

{xinyiwu,ajorlou,jadbabai}@mit.edu zwu2@caltech.edu

###### Abstract

Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.

## 1 Introduction

Graph neural networks (GNNs) have emerged as a powerful framework for learning with graph-structured data  and have shown great promise in diverse domains such as molecular biology , physics  and recommender systems . Most GNN models follow the _message-passing_ paradigm , where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes.

One notable drawback of repeated message-passing is _oversmoothing_, which refers to the phenomenon that stacking message-passing GNN layers makes node representations of the same connected component converge to the same vector . As a result, whereas depth has been considered crucial for the success of deep learning in many fields such as computer vision , most GNNs used in practice remain relatively shallow and often only have few layers . On the theory side, while previous works have shown that the symmetric Graph Convolution Networks (GCNs) with ReLU and LeakyReLU nonlinearities exponentially lose expressive power, analyzing the oversmoothing phenomenon in other types of GNNs is still an open question . In particular, the question of whether the graph attention mechanism can prevent oversmoothing has not been settled yet. Motivated by the capacity of graph attention to distinguish the importance of different edges in the graph, some works claim that oversmoothing is alleviated in Graph Attention Networks (GATs), heuristically crediting to GATs' ability to learn adaptive node-wise aggregation operators via the attention mechanism . On the other hand, it has been empirically observed that similar tothe case of GCNs, oversmoothing seems inevitable for attention-based GNNs such as GATs  or (graph) transformers . The latter can be viewed as attention-based GNNs on complete graphs.

In this paper, we provide a definitive answer to this question -- attention-based GNNs also lose expressive power exponentially, albeit potentially at a slower exponential rate compared to GCNs. Given that attention-based GNNs can be viewed as nonlinear time-varying dynamical systems, our analysis is built on the theory of products of inhomogeneous matrices [14; 33] and the concept of joint spectral radius , as these methods have been long proved effective in the analysis of time-inhomogeneous markov chains and ergodicity of dynamical systems [2; 14; 33]. While classical results only apply to generic one-dimensional linear time-varying systems, we address four major challenges arising in analyzing attention-based GNNs: (1) the aggregation operators computed by attention are state-dependent, in contrast to conventional fixed graph convolutions; (2) the systems are multi-dimensional, which involves the coupling across feature dimensions; (3) the dynamics are nonlinear due to the nonlinear activation function in each layer; (4) the learnable weights and aggregation operators across different layers result in time-varying dynamical systems.

**Below, we highlight our key contributions:**

* As our main contribution, we establish that oversmoothing happens exponentially as model depth increases for attention-based GNNs, resolving the long-standing debate about whether attention-based GNNs can prevent oversmoothing.
* We analyze attention-based GNNs through the lens of nonlinear, time-varying dynamical systems. The strength of our analysis stems from its ability to exploit the inherently common connectivity structure among the typically asymmetric state-dependent aggregation operators at different attentional layers. This enables us to derive rigorous theoretical results on the ergodicity of infinite products of matrices associated with the evolution of node representations across layers. Incorporating results from the theory of products of inhomogeneous matrices and their joint spectral radius, we then establish that oversmoothing happens at an exponential rate for attention-based GNNs from our ergodicity results.
* Our analysis generalizes the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models with asymmetric, state-dependent and time-varying aggregation operators and nonlinear activation functions under general conditions. In particular, our analysis can accommodate a wide range of common nonlinearities such as ReLU, LeakyReLU, and even non-monotone ones like GELU and SiLU. We validate our theoretical results on six real-world datasets with two attention-based GNN architectures and five common nonlinearities.

## 2 Related Work

Oversmoothing problem in GNNsOversmoothing is a well-known problem in deep GNNs, and many techniques have been proposed in order to mitigate it practically [6; 15; 20; 21; 29; 43; 47]. On the theory side, analysis of oversmoothing has only been carried out for the graph convolution case [5; 18; 26; 41]. In particular, by viewing graph convolutions as a form of Laplacian filter, prior works have shown that for GCNs, the node representations within each connected component of a graph will converge to the same value exponentially [5; 26]. However, oversmoothing is also empirically observed in attention-based GNNs such as GATs  or transformers . Although some people hypothesize based on heuristics that attention can alleviate oversmoothing , a rigorous analysis of oversmoothing in attention-based GNNs remains open .

Theoretical analysis of attention-based GNNsExisting theoretical results on attention-based GNNs are limited to one-layer graph attention. Recent works in this line include Brody et al.  showing that the ranking of attention scores computed by a GAT layer is unconditioned on the query node, and Fountoulakis et al.  studying node classification performance of one-layer GATs on a random graph model. More relevantly, Wang et al.  made a claim that oversmoothing is asymptotically inevitable in GATs. Aside from excluding nonlinearities in the analysis, there are several flaws in the proof of their main result (Theorem 2). In particular, their analysis assumes the same stationary distribution for all the stochastic matrices output by attention at different layers. This is typically not the case given the state-dependent and time-varying nature of these matrices. In fact, the main challenge in analyzing multi-layer attention lies in the state-dependent and time-varyingnature of these input-output mappings. Our paper offers novel contributions to the research on attention-based GNNs by developing a rich set of tools and techniques for analyzing multi-layer graph attention. This addresses a notable gap in the existing theory, which has primarily focused on one-layer graph attention, and paves the way for future research to study other aspects of multi-layer graph attention.

## 3 Problem Setup

### Notations

Let \(\) be the set of real numbers and \(\) be the set of natural numbers. We use the shorthands \([n]:=\{1,,n\}\) and \(_{ 0}:=\{0\}\). We denote the zero-vector of length \(N\) by \(^{N}\) and the all-one vector of length \(N\) by \(^{N}\). We represent an undirected graph with \(N\) nodes by \(=(A,X)\), where \(A\{0,1\}^{N N}\) is the _adjacency matrix_ and \(X^{N d}\) are the _node feature vectors_ of dimension \(d\). Let \(E()\) be the set of edges of \(\). For nodes \(i,j[N]\), \(A_{ij}=1\) if and only if \(i\) and \(j\) are connected with an edge in \(\), i.e., \((i,j) E()\). For each \(i[N]\), \(X_{i}^{d}\) represents the feature vector for node \(i\). We denote the _degree matrix_ of \(\) by \(D_{}=(A)\) and the set of all neighbors of node \(i\) by \(_{i}\).

Let \(\|\|_{2}\), \(\|\|_{}\), \(\|\|_{F}\) be the \(2\)-norm, \(\)-norm and Frobenius norm, respectively. We use \(\|\|_{}\) to denote the matrix max norm, i.e., for a matrix \(M^{m n}\), \(\|M\|_{}:=_{ij}|M_{ij}|\). We use \(_{}\) to denote element-wise inequality. Lastly, for a matrix \(M\), we denote its \(i^{th}\) row by \(M_{i}\). and \(j^{th}\) column by \(M_{ j}\).

### Graph attention mechanism

We adopt the following definition of graph attention mechanism. Given node representation vectors \(X_{i}\) and \(X_{j}\), we first apply a shared learnable linear transformation \(W^{d d^{}}\) to each node, and then use an attention function \(:^{d^{}}^{d^{}}\) to compute a raw attention coefficient

\[e_{ij}=(W^{}X_{i},W^{}X_{j})\]

that indicates the importance of node \(j\)'s features to node \(i\). Then the graph structure is injected into the mechanism by performing masked attention, where for each node \(i\), we only compute its attention to its neighbors. To make coefficients easily comparable across different nodes, we normalize \(e_{ij}\) among all neighboring nodes \(j\) of node \(i\) using the softmax function to get the normalized attention coefficients:

\[P_{ij}=_{j}(e_{ij})=)}{_{k_{i}}(e_{ik})}\,.\]

The matrix \(P\), where the entry in the \(i^{th}\) row and the \(j^{th}\) column is \(P_{ij}\), is a row stochastic matrix. We refer to \(P\) as an _aggregation operator_ in message-passing.

### Attention-based GNNs

Having defined the graph attention mechanism, we can now write the update rule of a single graph attentional layer as

\[X^{}=(PXW)\,,\]

where \(X\) and \(X^{}\) are are the input and output node representations, respectively, \(()\) is a pointwise nonlinearity function, and the aggregation operator \(P\) is a function of \(XW\).

As a result, the output of the \(t^{th}\) graph attentional layers can be written as

\[X^{(t+1)}=(P^{(t)}X^{(t)}W^{(t)}) t_{ 0},\] (1)

where \(X^{(0)}=X\) is the input node features, \(W^{(t)}^{d^{} d^{}}\) for \(t\) and \(W^{(0)}^{d d^{}}\). For the rest of this work, without loss of generality, we assume that \(d=d^{}\).

The above definition is based on _single-head_ graph attention. _Multi-head_ graph attention uses \(K\) weight matrices \(W_{1},,W_{K}\) in each layer and averages their individual single-head outputs [11; 38]. Without loss of generality, we consider single graph attention in our analysis in Section 4, but we note that our results automatically apply to the multi-head graph attention setting since \(K\) is finite.

### Measure of oversmoothing

We use the following notion of oversmoothing, inspired by the definition proposed in Rusch et al. 1:

**Definition 1**.: _For an undirected and connected graph \(\), \(:^{N d}_{ 0}\) is called a node similarity measure if it satisfies the following axioms:_

1. \( c^{d}\) _such that_ \(X_{i}=c\) _for all node_ \(i\) _if and only if_ \((X)=0\)_, for_ \(X^{N d}\)_;_
2. \((X+Y)(X)+(Y)\)_, for all_ \(X,Y^{N d}\)_._

_Then oversmoothing with respect to \(\) is defined as the layer-wise convergence of the node-similarity measure \(\) to zero, i.e.,_

\[_{t}(X^{(t)})=0.\] (2)

_We say oversmoothing happens at an exponential rate if there exists constants \(C_{1}\), \(C_{2}>0\), such that for any \(t\),_

\[(X^{(t)}) C_{1}e^{-C_{2}t}.\] (3)

We establish our results on oversmoothing for attention-based GNNs using the following node similarity measure:

\[(X):=\|X-_{X}\|_{F},_{X}= ^{}X}{N}\,.\] (4)

**Proposition 1**.: \(\|X-_{X}\|_{F}\) _is a node similarity measure._

The proof of the above proposition is provided in Appendix B. Other common node similarity measures include the Dirichlet energy [5; 31].2 Our measure is mathematically equivalent to the measure \(_{Y=^{},c^{d}}\{\|X-Y\|_{F}\}\) defined in Oono and Suzuki , but our form is more direct to compute. One way to see the equivalence is to consider the orthogonal projection into the space perpendicular to \(\{\}\), denoted by \(B^{(N-1) N}\). Then our definition of \(\) satisfies \(\|X-_{x}\|_{F}=\|BX\|_{F}\), where the latter quantity is exactly the measure defined in .

### Assumptions

We make the following assumptions (in fact, quite minimal) in deriving our results:

1. The graph \(\) is connected and non-bipartite.
2. The attention function \((,)\) is continuous.
3. The sequence \(\{\|_{t=0}^{k}|W^{(t)}|\|_{}\}_{k=0}^{}\) is bounded.
4. The point-wise nonlinear activation function \(()\) satisfies \(0 1\) for \(x 0\) and \((0)=0\).

We note that all of these assumptions are either standard or quite general. Specifically, \(\) is a standard assumption for theoretical analysis on graphs. For graphs with more than one connected components, the same results apply to each connected component. \(\) can also be replaced with requiring the graph \(\) to be connected and have self-loops at each node. Non-bipartiteness and self-loops both ensure that long products of stochastic matrices corresponding to aggregation operators in different graph attentional layers will eventually become strictly positive.

The assumptions on the GNN architecture \(\) and \(\) can be easily verified for commonly used GNN designs. For example, the attention function \((a^{}[W^{}X_{i}||W^{}X_{j}]),a^{2d^ {}}\) used in the GAT , where \([||]\) denotes concatenation, is a specific case that satisfies \(\). Other architectures that satisfy \(\) include GATv2  and (graph) transformers . As for \(\), one way tosatisfy it is to have \(\) be \(1\)-Lipschitz and \((x) 0\) for \(x<0\) and \((x) 0\) for \(x>0\). Then it is easy to verify that most of the commonly used nonlinear activation functions such as ReLU, LeakyReLU, GELU, SiLU, ELU, tanh all satisfy **A4**.

Lastly, **A3** is to ensure boundedness of the node representations' trajectories \(X^{(t)}\) for all \(t_{ 0}\). Such regularity assumptions are quite common in the asymptotic analysis of dynamical systems, as is also the case for the prior works analyzing oversmoothing in symmetric GCNs [5; 26].

## 4 Main Results

In this section, we lay out a road-map for deriving our main results, highlighting the key ideas of the proofs. The complete proofs are provided in the Appendices.

We start by discussing the dynamical system formulation of attention-based GNNs in Section 4.1. By showing the boundedness of the node representations' trajectories, we prove the existence of a common connectivity structure among aggregation operators across different graph attentional layers in Section 4.2. This implies that graph attention cannot fundamentally change the graph connectivity, a crucial property that will eventually lead to oversmoothing. In Section 4.3, we develop a framework for investigating the asymptotic behavior of attention-based GNNs by introducing the notion of ergodicity and its connections to oversmoothing. Then utilizing our result on common connectivity structure among aggregation operators, we establish ergodicity results for the systems associated with attention-based GNNs. In Section 4.4, we introduce the concept of the joint spectral radius for a set of matrices  and employ it to deduce exponential convergence of node representations to a common vector from our ergodicity results. Finally, we present our main result on oversmoothing in attention-based GNNs in Section 4.5 and comment on oversmoothing in GCNs in comparison with attention-based GNNs in Section 4.6.

### Attention-based GNNs as nonlinear time-varying dynamical systems

The theory of dynamical systems concerns the evolution of some state of interest over time. By viewing the model depth \(t\) as the time variable, the input-output mapping at each graph attentional layer \(X^{(t+1)}=(P^{(t)}X^{(t)}W^{(t)})\) describes a nonlinear time-varying dynamical system. The attention-based aggregation operator \(P^{(t)}\) is state-dependent as it is a function of \(X^{(t)}W^{(t)}\). Given the notion of oversmoothing defined in Section 3.4, we are interested in characterizing behavior of \(X^{(t)}\) as \(t\).

If the activation function \(()\) is the identity map, then repeated application of (1) gives

\[X^{(t+1)}=P^{(t)} P^{(0)}XW^{(0)} W^{(t)}\,.\]

The above linear form would enable us to leverage the rich literature on the asymptotic behavior of the products of inhomogeneous row-stochastic matrices (see, e.g., [14; 33]) in analyzing the long-term behavior of attention-based GNNs. Such a neat expansion, however, is not possible when dealing with a nonlinear activation function \(()\). To find a remedy, let us start by observing that element-wise application of \(\) to a vector \(y^{d}\) can be written as

\[(y)=()y\,,\] (5)

where \(()\) is a diagonal matrix with \((y_{i})/y_{i}\) on the \(i^{th}\) diagonal entry. Defining \((0)/0:=^{}(0)\) or \(1\) if the derivative does not exist along with the assumption \((0)=0\) in **A4**, it is easy to check that the above identity still holds for vectors with zero entries.

We can use (5) to write the \(i^{th}\) column of \(X^{(t+1)}\) as

\[X^{(t+1)}_{_{i}}=(P^{(t)}(X^{(t)}W^{(t)})_{_{i}})=D^{(t)}_{i}P ^{(t)}(X^{(t)}W^{(t)})_{_{i}}=D^{(t)}_{i}P^{(t)}_{j=1}^{d}W^{(t)}_{ji} X^{(t)}_{_{j}}\,,\] (6)

where \(D^{(t)}_{i}\) is a diagonal matrix. It follows from the assumption on the nonlinearities **A4** that

\[()_{}D^{(t)}_{i}_{}()\,.\]We define \(\) to be the set of all possible diagonal matrices \(D_{i}^{(t)}\) satisfying the above inequality:

\[:=\{():^{N}, _{}_{}\}.\]

Using (6) recursively, we arrive at the following formulation for \(X_{ i}^{(t+1)}\):

\[X_{ i}^{(t+1)}=_{j_{t+1}=i,\,(j_{t},,j_{0})[d]^{t+1}}( _{k=0}^{t}W_{j_{k}j_{k+1}}^{(k)})D_{j_{t+1}}^{(t)}P^{(t)}...D_{j_{1 }}^{(0)}P^{(0)}X_{ j_{0}}^{(0)}\,.\] (7)

### Common connectivity structure among aggregation operators across different layers

We can use the formulation in (7) to show the boundedness of the node representations' trajectories \(X^{(t)}\) for all \(t_{ 0}\), which in turn implies the boundedness of the input to graph attention in each layer, \(X^{(t)}W^{(t)}\).

**Lemma 1**.: _Under assumptions_ **A3**_-_**A4**_, there exists \(C>0\) such that \(\|X^{(t)}\|_{} C\) for all \(t_{ 0}\)._

For a continuous \((,)\)3, the following lemma is a direct consequence of Lemma 1, suggesting that the graph attention mechanism cannot fundamentally change the connectivity pattern of the graph.

**Lemma 2**.: _Under assumptions_ **A2**_-_**A4**_, there exists \(>0\) such that for all \(t_{ 0}\) and for any \((i,j) E()\), we have \(P_{ij}^{(t)}\)._

One might argue that Lemma 2 is an artifact of the continuity of the softmax function. The softmax function is, however, often favored in attention mechanisms because of its trainability in back propagation compared to discontinuous alternatives such as hard thresholding. Besides trainability issues, it is unclear on a conceptual level whether it is reasonable to absolutely drop an edge from the graph as is the case for hard thresholding. Lemma 2 is an important step towards the main convergence result of this work, which states that all the nodes will converge to the same representation vector at an exponential rate. We define the family of row-stochastic matrices satisfying Lemma 2 below.

**Definition 2**.: _Let \(>0\). We define \(_{,}\) to be the set of row-stochastic matrices satisfying the following conditions:_

1. \( P_{ij} 1,\) _if_ \((i,j) E()\)_,_
2. \(P_{ij}=0,\) _if_ \((i,j) E()\)_,_

### Ergodicity of infinite products of matrices

_Ergodicity_, in its most general form, deals with the long-term behavior of dynamical systems. The oversmoothing phenomenon in GNNs defined in the sense of (2) concerns the convergence of all rows of \(X^{(t)}\) to a common vector. To this end, we define ergodicity in our analysis as the convergence of infinite matrix products to a rank-one matrix with identical rows.

**Definition 3** (Ergodicity).: _Let \(B^{(N-1) N}\) be the orthogonal projection onto the space orthogonal to \(\{\}\). A sequence of matrices \(\{M^{(n)}\}_{n=0}^{}\) is ergodic if_

\[_{t}B_{n=0}^{t}M^{(n)}=0\,.\]

We will take advantage of the following properties of the projection matrix \(B\) already established in Blondel et al. :

1. \(B=0\);
2. \(\|Bx\|_{2}=\|x\|_{2}\) for \(x^{N}\) if \(x^{}=0\);
3. Given any row-stochastic matrix \(P^{N N}\), there exists a unique matrix \(^{(N-1)(N-1)}\) such that \(BP=B\,.\)We can use the existing results on the ergodicity of infinite products of inhomogeneous stochastic matrices [14; 33] to show that any sequence of matrices in \(_{,}\) is ergodic.

**Lemma 3**.: _Fix \(>0\). Consider a sequence of matrices \(\{P^{(t)}\}_{t=0}^{}\) in \(_{,}\). That is, \(P^{(t)}_{,}\) for all \(t_{ 0}\). Then \(\{P^{(t)}\}_{t=0}^{}\) is ergodic._

The main proof strategy for Lemma 3 is to make use of the Hilbert projective metric and the Birkhoff contraction coefficient. These are standard mathematical tools to prove that an infinite product of inhomogeneous stochastic matrices is ergodic. We refer interested readers to the textbooks [14; 33] for a comprehensive study of these subjects.

Despite the nonlinearity of \(()\), the formulation (7) enables us to express the evolution of the feature vector trajectories as a weighted sum of products of matrices of the form \(DP\) where \(D\) and \(P_{,}\). We define the set of such matrices as

\[_{,}:=\{DP:D,P_{ ,}\}\,.\]

A key step in proving oversmoothing for attention-based GNNs under our assumptions is to show the ergodicity of the infinite products of matrices in \(_{,}\). In what follows, we lay out the main ideas of the proof, and refer readers to Appendix F for the details.

Consider a sequence \(\{D^{(t)}P^{(t)}\}_{t=0}^{}\) in \(_{,}\), that is, \(D^{(t)}P^{(t)}_{,}\) for all \(t_{ 0}\). For \(t_{0} t_{1}\), define

\[Q_{t_{0},t_{1}}:=D^{(t_{1})}P^{(t_{1})} D^{(t_{0})}P^{(t_{0})}, _{t}=\|D^{(t)}-I_{N}\|_{}\,,\]

where \(I_{N}\) denotes the \(N N\) identity matrix. The common connectivity structure among \(P^{(t)}\)'s established in Section 4.2 allows us to show that long products of matrices \(DP\) from \(_{,}\) will eventually become a contraction in \(\)-norm. More precisely, we can show that there exists \(T\) and \(0<c<1\) such that for all \(t_{ 0}\),

\[\|Q_{t,t+T}\|_{} 1-c_{t}.\]

Next, define \(_{k}:=_{t=0}^{k}(1-c_{t})\) and let \(:=_{k}_{k}\). Note that \(\) is well-defined because the partial product is non-increasing and bounded from below. We can use the above contraction property to show the following key lemma.

**Lemma 4**.: _Let \(_{k}:=_{t=0}^{k}(1-c_{t})\) and \(:=_{k}_{k}\)._

1. _If_ \(=0\)_, then_ \(_{k}Q_{0,k}=0\,;\)__
2. _If_ \(>0\)_, then_ \(_{k}BQ_{0,k}=0\,.\)__

The ergodicity of sequences of matrices in \(_{,}\) immediately follows from Lemma 4, which in turn implies oversmoothing as defined in (2).

**Lemma 5**.: _Any sequence \(\{D^{(t)}P^{(t)}\}_{t=0}^{}\) in \(_{,}\) is ergodic._

RemarkThe proof techniques developed in [5; 26] are restricted to symmetric matrices hence cannot be extended to more general family of GNNs, as they primarily rely on matrix norms for convergence analysis. Analyses solely using matrix norms are often too coarse to get meaningful results when it comes to asymmetric matrices. For instance, while the matrix \(2\)-norm and matrix eigenvalues are directly related for symmetric matrices, the same does not generally hold for asymmetric matrices. Our analysis, on the other hand, exploits the inherently common connectivity structure among these matrices in deriving the ergodicity results in Lemma 3-5.

### Joint spectral radius

Using the ergodicity results in the previous section, we can establish that oversmoothing happens in attention-based GNNs. To show that oversmoothing happens at an exponential rate, we introduce the notion of _joint spectral radius_, which is a generalization of the classical notion of spectral radius of a single matrix to a set of matrices [7; 30]. We refer interested readers to the textbook  for a comprehensive study of the subject.

**Definition 4** (Joint Spectral Radius).: _For a collection of matrices \(\), the joint spectral radius \(()\) is defined to be_

\[()=_{k}_{A_{1},A_{2},...,A_{k }}\|A_{1}A_{2}...A_{k}\|^{}\,,\]

_and it is independent of the norm used._

In plain words, the joint spectral radius measures the maximal asymptotic growth rate that can be obtained by forming long products of matrices taken from the set \(\).To analyze the convergence rate of products of matrices in \(_{,}\) to a rank-one matrix with identical rows, we treat the two cases of linear and nonlinear activation functions, separately.

For the linear case, where \(()\) is the identity map, we investigate the dynamics induced by \(P^{(t)}\)'s on the subspace orthogonal to \(\{\}\) and use the third property of the orthogonal projection \(B\) established in Section 4.3 to write \(BP_{1}P_{2} P_{k}=_{1}_{2}..._{k}B\), where each \(_{i}\) is the unique matrix in \(^{(N-1)(N-1)}\) that satisfies \(BP_{i}=_{i}B\). Let us define

\[}_{,}:=\{:BP=B,P _{,}\}\,.\]

We can use Lemma 3 to show that the joint spectral radius of \(}_{,}\) is strictly less than \(1\).

**Lemma 6**.: _Let \(0<<1\). Under assumptions_ **A1**_-_**A4**_, \((}_{,})<1\)._

For the nonlinear case, let \(0<<1\) and define

\[_{}:=\{():^{N},_{}_{}\}, _{,,}:=\{: _{},_{ ,}\}\,.\]

Then again, using the ergodicity result from the previous section, we establish that the joint spectral radius of \(_{,,}\) is also less than 1.

**Lemma 7**.: _Let \(0<,<1\). Under assumptions_ **A1**_-_**A4**_, \((_{,,})<1\)._

The above lemma is specifically useful in establishing exponential rate for oversmoothing when dealing with nonlinearities for which Assumption 4 holds in the strict sense, i.e. \(0<1\) (e.g., GELU and SiLU nonlinearities). Exponential convergence, however, can still be established under a weaker requirement, making it applicable to ReLU and Leaky ReLU, as we will see in Theorem 1.

It follows from the definition of the joint spectral radius that if \(()<1\), for any \(()<q<1\), there exists a \(C\) for which

\[\|A_{1}A_{2}...A_{k}y\| Cq^{k}\|y\|\] (8)

for all \(y^{N-1}\) and \(A_{1},A_{2},...,A_{k}\).

### Main Theorem

Applying (8) to the recursive expansion of \(X_{ i}^{(t+1)}\) in (7) using the \(2\)-norm, we can prove the exponential convergence of \((X^{(t)})\) to zero for the similarity measure \(()\) defined in (4), which in turn implies the convergence of node representations to a common representation at an exponential rate. This completes the proof of the main result of this paper, which states that oversmoothing defined in (2) is unavoidable for attention-based GNNs, and that an exponential convergence rate can be attained under general conditions.

**Theorem 1**.: _Under assumptions_ **A1**_-_**A4**_,_

\[_{t}(X^{(t)})=0\,,\]

_indicating oversmoothing happens asymptotically in attention-based GNNs with general nonlinearities. In addition, if_

\(\) (linear): \(()\) _is the identity map, or_

\(\) (nonlinear): _there exists_ \(K\) _and_ \(0<<1\) _for which the following holds: For all_ \(m_{ 0}\)_,_

_there is_ \(n_{m}\{0\}[K-1]\) _such that for any_ \(c[d]\)_,_ \((X_{r_{c}c}^{(mK+n_{m})})/X_{r_{c}c}^{(mK+n_{m})}\) _for some_ \(r_{c}[N]\)_,_ _(_\(\)_)_

_then there exists \(q<1\) and \(C_{1}(q)>0\) such that_

\[(X^{(t)}) C_{1}q^{t}\,, t 0\,.\]_As a result, node representations \(X^{(t)}\) exponentially converge to the same value as the model depth \(t\)._

Theorem 1 establishes that oversmoothing is asymptotically inevitable for attention-based GNNs with general nonlinearities. Despite similarity-based importance assigned to different nodes via the aggregation operator \(P^{(t)}\), _such attention-based mechanisms are yet unable to fundamentally change the connectivity structure of \(P^{(t)}\)_, resulting in node representations converging to a common vector. Our results hence indirectly support the emergence of alternative ideas for changing the graph connectivity structure such as edge-dropping [15; 29] or graph-rewiring , in an effort to mitigate oversmoothing.

**Remark 1**.: _For nonlinearities such as SiLU or GELU, the condition (\(\)) is automatically satisfied under **A3**-**A4**. For ReLU and LeakyReLU, this is equivalent to requiring that there exists \(K\) such that for all \(m_{ 0}\), there exists \(n_{m}\{0\}[K-1]\) where for any \(c[d]\), \(X^{(mK+n_{m})}_{c c}<0\) for some \(r_{c}[d]\)._

**Remark 2**.: _We note that our results are not specific to the choice of node similarity measure \((X)=\|X-_{,}\|_{F}\) considered in our analysis. In fact, exponential convergence of any other Lipschitz node similarity measure \(^{}\) to \(0\) is a direct corollary of Theorem 1. To see this, observe that for a node similarity measure \(^{}\) with a Lipschitz constant \(L\), it holds that_

\[^{}(X)=|^{}(X)-^{}(_{ })| L\|X-_{}\|_{F}=L(X).\]

_In particular, the Dirichlet energy is Lipschitz given that the input \(X\) has a compact domain, established in Lemma 1. Hence our theory directly implies the exponential convergence of Dirichlet energy._

### Comparison with the GCN

Computing or approximating the joint spectral radius for a given set of matrices is known to be hard in general , yet it is straightforward to lower bound \((}_{,})\) as stated in the next proposition.

**Proposition 2**.: _Let \(\) be the second largest eigenvalue of \(D^{-1/2}_{}AD^{-1/2}_{}\). Then under assumptions **A1**-**A4**, it holds that \((}_{,})\)._

In the linear case, the upper bound \(q\) on the convergence rate that we get for graph attention in Theorem 1 is lower bounded by \((}_{,})\). A direct consequence of the above result is that \(q\) is at least as large as \(\). On the other hand, previous work has already established that in the graph convolution case, the convergence rate of \((X^{(t)})\) is \(O(^{t})\)[5; 26]. It is thus natural to expect attention-based GNNs to potentially have better expressive power at finite depth than GCNs, even though they both inevitably suffer from oversmoothing. This is also evident from the numerical experiments that we present in the next section.

## 5 Numerical Experiments

In this section, we validate our theoretical findings via numerical experiments using the three commonly used homophilic benchmark datasets: Cora, CiteSeer, and PubMed  and the three commonly used heterophilic benchmark datasets: Cornell, Texas, and Wisconsin . We note that our theoretical results are developed for generic graphs and thus hold for datasets exhibiting either homophily or heterophily and even those that are not necessarily either of the two. More details about the experiments are provided in Appendix K.

For each dataset, we trained a \(128\)-layer single-head GAT and a \(128\)-layer GCN with the random walk graph convolution \(D^{-1}_{}A\), each having 32 hidden dimensions and trained using the standard features and splits. The GCN with the random walk graph convolution is a special type of attention-based GNNs where the attention function is constant. For each GNN model, we considered various nonlinear activation functions: ReLU, LeakyReLU (with three different negative slope values: \(0.01\), \(0.4\) and \(0.8\)) and GELU. Here, we chose GELU as an illustration of the generality of our assumption on nonlinearities, covering even non-monotone activation functions such as GELU. We ran each experiment \(10\) times. Figure 1 shows the evolution of \((X^{(t)})\) in log-log scale on the largest connected component of each graph as we forward pass the input \(X\) into a trained model. The solid curve is the average over \(10\) runs and the band indicates one standard deviation around the average.

We observe that, as predicted by our theory, oversmoothing happens at an exponential rate for both GATs and GCNs, regardless of the choice of nonlinear activation functions in the GNN architectures. Notably, GCNs exhibit a significantly faster rate of oversmoothing compared to GATs. This aligns the observation made in Section 4.6, expecting a potentially better expressive power for GATs than GCNs at finite depth. Furthermore, the exponential convergence rate of oversmoothing varies among GNNs with different nonlinear activation functions. From a theory perspective, as different activation functions constitute different subsets of \(_{,}\) and different sets of matrices have different joint spectral radii, it is not surprising that the choice of nonlinear activation function would affect the convergence rate. In particular, among the nonlinearities we considered, ReLU in fact magnifies oversmoothing the second most. As a result, although ReLU is often the default choice for the standard implementation of many GNN architectures [10; 19], one might wish to consider switching to other nonlinearities to better mitigate oversmoothing.

## 6 Conclusion

Oversmoothing is one of the central challenges in developing more powerful GNNs. In this work, we reveal new insights on oversmoothing in attention-based GNNs by rigorously providing a negative answer to the open question of whether graph attention can implicitly prevent oversmoothing. By analyzing the graph attention mechanism within the context of nonlinear time-varying dynamical systems, we establish that attention-based GNNs lose expressive power exponentially as model depth increases.

We upper bound the convergence rate for oversmoothing under very general assumptions on the nonlinear activation functions. One may try to tighten the bounds by refining the analysis separately for each of the commonly used activation functions. Future research should also aim to improve the design of graph attention mechanisms based on our theoretical insights and utilize our analysis techniques to study other aspects of multi-layer graph attention.

Figure 1: Evolution of \((X^{(t)})\) (in log-log scale) on the largest connected component of each dataset (top 2 rows: homophilic graphs; bottom 2 rows: heterophilic graphs). Oversmoothing happens exponentially in both GCNs and GATs with the rates varying depending on the choice of activation function. Notably, GCNs demonstrate faster rates of oversmoothing compared to GATs.