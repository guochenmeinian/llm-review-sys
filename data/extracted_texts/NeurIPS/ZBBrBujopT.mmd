# Online Control in Population Dynamics

Noah Golowich

MIT. nzg@mit.edu.

 Elad Hazan

Google DeepMind & Princeton University. ehazan@princeton.edu.

 Zhou Lu

Princeton University. zhoul@princeton.edu.

 Dhruv Rohatgi

MIT. drohatgi@mit.edu.

 Y. Jennifer Sun

Princeton University. ys7849@princeton.edu.

A new approach for population control.In this paper we propose a generic and robust methodology for population control, drawing on the framework and tools from online non-stochastic control theory to obtain a computationally efficient gradient-based method of control. In online non-stochastic control, at every time \(t=1,,T\), the learner is faced with a state \(x_{t}\) and must choose a control \(u_{t}\). The learner then incurs cost according to some time-varying cost function \(c_{t}(x_{t},u_{t})\) evaluated at the current state/control pair, and the state evolves as:

\[x_{t+1}:=f(x_{t},u_{t})+w_{t},\] (2)

where \(f\) describes the (known) discrete-time dynamics, \(x_{t+1}\) is the next state, and \(w_{t}\) is an adversarially-chosen perturbation. A _policy_ is a mapping from states to controls. The goal of the learner is to minimize _regret_ with respect to some rich policy class \(\), formally defined by

\[_{}=_{t=1}^{T}c_{t}(x_{t},u_{t})-_{}_{t =1}^{T}c_{t}(x_{t}^{},u_{t}^{}),\] (3)

where \((x_{t}^{},u_{t}^{})\) is the state/control pair at time \(t\) had policy \(\) been carried out since time \(1\).

As with prior work in online control , our method is theoretically grounded by regret guarantees for a broad class of Linear Dynamical Systems (LDSs). The key algorithmic and technical challenge we overcome is that **prior methods only give regret bounds against comparator policies that _strongly stabilize_ the LDS** (Definition4). Such policies force the magnitude of the state to decrease exponentially fast in the absence of noise. Unfortunately, for applications to population dynamics, even the assumption that such policies _exist_ - let alone perform well - is fundamentally unreasonable, since it essentially implies that the population can be made to exponentially shrink.

A priori, one might hope to generically overcome this issue, by broadening the comparator class to all policies that _marginally stabilize_ the LDS (informally, these are policies under which the magnitude of the state does not blow up). But we show that, in general, it is impossible to achieve sub-linear regret against that class - a result that may be of independent interest in online control:6

**Theorem 1** (Informal statement of Theorem25).: _There is a distribution \(\) over LDSs with state space and control space given by \(\), such that any online control algorithm on a system \(\) incurs expected regret \((T)\) against the class of time-invariant linear policies that marginally stabilize \(\)._

For general LDSs, it's not obvious if there is a natural "intermediate" comparator class that does not require strong stabilizability and does enable control with low regret. However, systems that model _populations_ possess rich additional structure, since they can be interpreted as controlled Markov chains.7 In this paper, leveraging that structure, we design an algorithm GPC-Simplex for online control that applies to LDSs constrained to the _simplex_ (Definition3), and achieves strong regret bounds against a natural comparator class of policies with bounded _mixing time_ (Definition6).

### Our Results

Throughout this work, we model a _population_ as a distribution over \(d\) different categories, evolving over \(T\) discrete timesteps. For simplicity, we assume that \(u_{t}\) is a \(d\)-dimensional real vector.

Theoretical guarantees for online population control.We introduce the _simplex LDS_ model (Definition3), which is a modification of the standard LDS model (Definition9) that ensures the states \((x_{t})_{t}\) always represent valid distributions, i.e. never leave the simplex \(^{d}\). Informally, given state \(x_{t}^{d}\) and control \(u_{t}_{ 0}^{d}\) with \( u_{t}_{1} 1\), the next state is

\[x_{t+1}=(1-_{t})(1- u_{t}_{1})Ax_{t }+Bu_{t}+_{t} w_{t},\]

where \(A,B\) are known stochastic matrices, \(_{t}\) is the observed _perturbation strength_,8 and \(w_{t}^{d}\) is an unknown perturbation. The perturbation \(w_{t}\) can be interpreted as representing an adversary that can add individuals from a population with distribution \(w_{t}\) to the population under study. Intuitively, \(u_{t}\) represents a distribution over \(d\) possible interventions as well as a "null intervention".

For any simplex LDS \(\) and mixing time parameter \(>0\), we define a class \(^{}_{}()\) (Definition 6), which roughly consists of the linear time-invariant policies under which the state of the system would mix to stationarity in time \(\), in the absence of noise. Our main theoretical contribution is an algorithm GPC-Simplex that achieves low regret against this policy class:

**Theorem 2** (Informal version of Theorem 7).: _Let \(\) be a simplex LDS on \(^{d}\), and let \(>0\). For any adversarially-chosen perturbations \((w_{t})_{t}\), perturbation strengths \((_{t})_{t}\), and convex and Lipschitz cost functions \((c_{t})_{t}\), the algorithm GPC-Simplex performs \(T\) steps of online control on \(\) with regret \((^{7/2})\) against \(^{}_{}()\)._

Finally, analogously Theorem 1, we show that the mixing time assumption cannot be removed: it is impossible to achieve sub-linear regret (for online control of a simplex LDS) against the class of all linear time-invariant policies (Theorem 8).

Experimental evaluations.To illustrate the practicality of our results, we apply (a generalization of) GPC-Simplex to controlled versions of (a) the SIR model for disease transmission (Section 4), and (b) the replicator dynamics from evolutionary game theory (Appendix H). In the former, closed-form optimal controllers are known in the absence of perturbations . We find that GPC-Simplex _learns_ characteristics of the optimal control (e.g. the "turning point" phase transition where interventions stop once herd immunity is reached). Moreover, our algorithm is robust even in the presence of adversarial perturbations, where previous theoretical results no longer apply. In the latter, we demonstrate that even when the control affects the population only indirectly, through the replicator dynamics _payoff matrix_, GPC-Simplex can learn to control the population effectively, and is more robust to noisy cost functions than a one-step best response controller.

### Related work

Online non-stochastic control.In recent years, the machine learning community has witnessed an increasing interest in non-stochastic control problems (e.g. ). Unlike the classical setting of stochastic control, in non-stochastic control the dynamics are subject to time-varying, adversarially chosen perturbations and cost functions. See  for a survey of prior results. Most relevant to our work is the Gradient Perturbation Controller (GPC) for controlling general LDSs . All existing controllers only provide provable regret guarantees against policies that strongly stabilize the system.

Population growth models.There is extensive research on modeling the evolution of populations in sociology, biology and economics. Besides the pioneering work of , notable models include the SIR model from epidemiology , the Lotka-Volterra model for predator-prey dynamics  and the replicator dynamics from evolutionary game theory . Recent years have seen intensive study of controlled versions of the SIR model - see e.g. empirical work , vaccination control models , and many others . Most relevant to our work is the _quarantine control model_, where the control reduces the effective transmission rate. Some works consider optimal control in the noiseless setting ; follow-up work  considers a budget constraint on the control. None of these prior works can handle the general case of adversarial noise and cost functions.

## 2 Definitions and setup

Notation.Denote \(^{d}:=\{M^{d d}\ :\ _{i=1}^{d}M_{i,j}=1\  j[d]\}\) as the set of \(d d\) column-stochastic matrices. For \(a>0\), define \(^{d}_{a}:=\{a M\ :\ M^{d}\}\) and \(^{d}_{ a}:=_{0 a^{} a}^{d}_{a^{ }}\). Let \(^{d}\) denote the simplex in \(^{d}\). Similarly, we define \(^{d}_{}:=^{d}\) and \(^{d}_{ a}:=_{0^{}}^{d}_{a^{ }}\). Given a square matrix \(M^{d d}\), let \(M_{,j}\) denote the \(j\)th column of \(M\). We consider the following matrix norms: \(\|M\|\) denotes the spectral norm of \(M\), \(\|M\|_{2,1}^{2}:=_{j=1}^{d}\|M_{,j}\|_{1}^{2}\) is the sum of the squares of the \(_{1}\) norms of the columns of \(M\), and \(\|M\|_{1 1}:=_{x^{d}:|x|_{1}=1} \|Mx\|_{1}\).

### Dynamical systems

The standard model in online control is the _linear dynamical system (LDS)_. We define a _simplex LDS_ to be an LDS where the state of the system always lies in the simplex. This requires enforcing certain constraints on the transition matrices, the control, and the noise:

**Definition 3** (Simplex LDS).: Let \(d\). A _simplex LDS on \(^{d}\)_ is a tuple

\[=(A,B,,x_{1},(_{t})_{t},(w_{t})_{t },(c_{t})_{t}),\]

where \(A,B^{d}\) are the _transition matrices_; \(^{d}_{ 1}\) is the _valid control set_; \(x_{1}^{d}\) is the _initial state_; \(_{t}\), \(w_{t}^{d}\) are the _noise strength_ and _noise value_ at time \(t\); and \(c_{t}:^{d}\) is the _cost function_ at time \(t\). These parameters define a dynamical system where the state at time \(t=1\) is \(x_{1}\). For each \(t 1\), given state \(x_{t}\) and control \(u_{t}\) at time \(t\), the state at time \(t+1\) is

\[x_{t+1}=(1-_{t})[(1-\|u_{t}\|_{1})Ax_{t}+Bu_{t}]+_{t} w _{t},\] (4)

and the cost incurred at time \(t\) is \(c_{t}(x_{t},u_{t})\).

Note that since the set of possible controls \(\) is contained in \(^{d}_{ 1}\), the states \((x_{t})_{t}\) are guaranteed to remain within the simplex for all \(t\). In this paper, we will assume that \(=_{a[,]}^{d}_{}\), for some parameters \(,\), which represent lower and upper bounds on the strength of the control.

Online non-stochastic control.Let \(=(A,B,x_{1},(_{t})_{t},(w_{t})_{t},(c_{t})_{t},)\) be a simplex LDS and let \(T^{+}\). We assume that the transition matrices \(A,B\) are known to the controller at the beginning of time, but the perturbations \((w_{t})_{t=1}^{T}\) are unknown. At each step \(1 t T\), the controller observes \(x_{t}\) and \(_{t}\), plays a control \(u_{t}\), and then observes the cost function \(c_{t}\) and incurs cost \(c_{t}(x_{t},u_{t})\). The system then evolves according to Eq. (4). Note that our assumption that the controller observes \(_{t}\) contrasts with some of the existing work on nonstochastic control , in which no information about the adversarial disturbances is known. In Appendix B, we justify the learner's ability to observe \(_{t}\) by observing that in many situations, the learner observes the _counts_ of individuals in a populations (in addition to their _proportions_, represented by the state \(x_{t}\)), and that this additional information allows computation of \(_{t}\).

The goal of the controller is to minimize regret with respect to some class \(=()\) of comparator policies. Formally, for any fixed dynamical system and any time-invariant and Markovian policy \(K:^{d}\), let \((x_{t}(K))_{t}\) and \((u_{t}(K))_{t}\) denote the counterfactual sequences of states and controls that would have been obtained by following policy \(K\). Then the regret of the controller on observed sequences \((x_{t})_{t}\) and \((u_{t})_{t}\) with respect to \(\) is

\[_{}:=_{t=1}^{T}c_{t}(x_{t},u_{t})-_{K }_{t=1}^{T}c_{t}(x_{t}(K),u_{t}(K)).\]

The following assumption on the cost functions of \(\) is standard in online control :

**Assumption 1**.: _The cost functions \(c_{t}:^{d}\) are convex and \(L\)-Lipschitz, in the following sense: for all \(x,x^{}^{d}\) and \(u,u^{}\), we have \(|c_{t}(x,u)-c_{t}(x^{},u^{})| L(\|x-x^{}_{1 }+\|u-u^{}\|_{1})\)._

### Comparator class and spectral conditions

In prior works on non-stochastic control for linear dynamical systems , the comparator class \(=_{,}()\) is defined to be the set of linear, time-invariant policies \(x Kx\) where \(K^{d d}\)\((,)\)-_strongly stabilizes \(\)_:

**Definition 4**.: A matrix \(M^{d d}\) is \((,)\)-_strongly stable_ if there is a matrix \(H^{d d}\) so that \(\|H^{-1}MH\| 1-\) and \(\|M\|,\|H\|,\|H^{-1}\|\). A matrix \(K^{d d}\) is said to \((,)\)-_strongly stabilize_ an LDS with transition matrices \(A,B^{d d}\) if \(A+BK\) is \((,)\)-strongly stable.

The regret bounds against \(_{,}()\) scale with \(^{-1}\), and so are vacuous for \(=0\). Unfortunately, in the simplex LDS setting, no policies satisfy the analogous notion of strong stability (see discussion in Section 3) unless \(=0\). Intuitively, the reason is that a \((,)\)-strongly stable policy with \(>0\) makes the state converge to \(0\) in the absence of noise.

What is a richer but still-tractable comparator class for a simplex LDS? We propose the class of linear, time-invariant policies under which (a) the state of the LDS _mixes_, when viewed as a distribution, and (b) the level of control \(\|u_{t}\|_{1}\) is independent of the state \(x_{t}\). Formally, we make the following definitions:

**Definition 5**.: Given \(t\) and a matrix \(X^{d}\) with unique stationary distribution \(^{d}\), we define \(D_{X}(t):=_{p^{d}}\|X^{t}p-\|_{1}\) and \(_{X}(t):=_{p,q^{d}}\|X^{t}(p-q)\|_{1}\). Moreover we define \(t^{}(X,):=_{t}\{t\ :\ D_{X}(t)\}\) for each \(>0\), and we write \(t^{}(X):=t^{}(X,1/4)\).9

**Definition 6** (Mixing a simplex LDS).: Let \(\) be a simplex LDS with transition matrices \(A,B^{d}\) and control set \(=_{[,]}^ {d}_{}\). A matrix \(K^{d}_{[,]}\) is said to \(\)_-mix_\(\) if \(t^{}(_{K}),\) where

\[_{K}:=(1-\|K\|_{1 1}) A+BK^{d}.\] (5)

We define the comparator class \(^{}_{}=^{}_{}()\) as the set of linear, time-invariant policies \(x Kx\) where \(K^{d}_{[,]}\)\(\)-mixes \(\).

Notice that for any \(K^{d}_{[,]}\), the linear policy \(u_{t}:=Kx_{t}\) always plays controls in the control set \(\), and the dynamics Eq.4 under this policy can be written as \(x_{t+1}=(1-_{t})_{K}x_{t}+_{t} w_{t}\).

Notice that by considering the comparator class \(^{}_{}\), we require the control norm to be independent of the state. This assumption is needed for technical reasons: without it, since \(Ax\) is multiplied by \(1-\|u\|_{1}\) in the transition dynamics (see Equation4), even a "linear" policy \(u:=Kx\) does not induce a linear transition. Hence, it would no longer be clear how one might define mixing time of a linear policy. It is a very interesting question whether there is a more natural (yet still tractable) definition of a simplex LDS that avoids this issue.

## 3 Online Algorithm and Theoretical Guarantee

In this section, we describe our main upper bound and accompanying algorithm for the setting of online control in a simplex LDS \(\). As discussed above, we assume that the set of valid controls is given by \(=_{[,]}^ {d}_{}\), for some constants \(0 1\), representing lower and upper bounds on the strength of the control.10

For convenience, we write \(_{t}:=\|u_{t}\|_{1}\) and \(u^{}_{t}=u_{t}/_{t}^{d}\) (if \(_{t}=0\), we set \(u^{}_{t}:=0\)). The dynamical system Eq.4 can then be expressed as follows:

\[x_{t+1}=(1-_{t})((1-_{t}) Ax_{t}+_{t} Bu^{ }_{t})+_{t} w_{t}.\] (6)

We aim to obtain a regret guarantee as in Eq.3 with respect to some rich class of _comparator policies_\(^{}\). As is typical in existing work on linear nonstochastic control, we take \(^{}\) to be a class of time-invariant _linear_ policies, i.e. policies that choose control \(u_{t}:=Kx_{t}\) at time \(t\) for some matrix \(K^{d d}\). In the standard setting of nonstochastic control, it is typically further assumed that all policies in the comparator class strongly stabilize the LDS (Definition4).11 The naive generalization of such a requirement in our setting would be that \(_{K}\) is strongly stable; however, this is impossible, since no stochastic matrix can be strongly stable. Instead, we aim to compete against the class \(^{}=^{}_{}()\) of time-invariant linear policies that (a) have fixed level of control in \([,]\), and (b) \(\)-mix \(\) (Definition6). We view the second condition as a natural distributional analogue of strong stabilizability; the first condition is needed for \(\)-mixing to even be well-defined.

Algorithm description.Our main algorithm, GPC-Simplex (Algorithm1), is a modification of the GPC algorithm . As a refresher, GPC chooses the controls \(u_{t}\) by learning a _disturbance-action policy_: a policy \(u_{t}:=x_{t}+_{i=1}^{H}M^{[i]}w_{t-i}\), where \(\) is a known, fixed matrix that strongly stabilizes the LDS; \(w_{t-1},,w_{t-H}\) are the recent noise terms; and \(M^{},,M^{[H]}\) are learnable, matrix-valued parameters which we abbreviate as \(M^{[1:H]}\). The key advantage of this parametrizationof policies (as opposed to a simpler parametrization such as \(u_{t}=Kx_{t}\) for a parameter \(K\)) is that the entire trajectory is _linear_ in the parameters, and not a high-degree polynomial. Thus, optimizing the cost of a trajectory over the class of disturbance-action policies is a convex problem in \(M^{[1:H]}\).

But why is the class of disturbance-action policies expressive enough to compete against the comparator class? This is where GPC crucially uses strong stabilizability. Notice that in the absence of noise, every disturbance-action policy is identical to the fixed policy \(u_{t}:=x_{t}\). This is fine when \(\) and the comparator class are strongly stabilizing, since in the absence of noise, _all_ strongly stabilizing policies rapidly force the state to \(0\), and thus incur very similar costs in the long run. But in the simplex LDS setting, strong stabilizability is impossible. While all policies in \(^{}\) mix the LDS, they may mix to different states, which may incur different costs. There is no reason to expect that an arbitrary \(^{}\), chosen before observing the cost functions, will have low regret against all policies in \(^{}\).

We fix this issue by enriching the class of disturbance-action policies with an additional parameter \(p^{d}\) which, roughly speaking, represents the desired stationary distribution to which \(x_{t}\) would converge, in the absence of noise, as \(t\). It is unreasonable to expect prior knowledge of the optimal choice of \(p\), which depends on the not-yet-observed cost functions. Thus, GPC-Simplex instead _learns_\(p\) together with \(M^{[1:H]}\). We retain the property that the requisite online learning problem is convex in the parameters, and therefore can be efficiently solved via an online convex optimization algorithm (as discussed in Appendix C.1, we use lazy mirror descent, LazyMD). One advantage of GPC-Simplex over GPC is that the former requires no knowledge of the fixed "reference" policy \(\) (which, in the context of GPC, had to be strongly stabilizing). While such \(\) is needed in the context of GPC to bound a certain approximation error involving the cost functions, in the context of GPC-Simplex this approximation error may be bounded by some simple casework involving properties of stochastic matrices (see Appendix C.3).

Formally, for parameters \(a_{0}[,]\) and \(H\), GPC-Simplex considers a class of policies parametrized by the set \(_{d,H,a_{0},}:=_{a[a_{0},]}_{a}^{d}(_{a}^{d})^{H}\). We abbreviate elements \((p,(M^{},,M^{[H]}))_{d,H,a_{0},}\) by \((p,M^{[1:H]})\). The high level idea of GPC-Simplex, like that of GPC, is to perform online convex optimization on the domain \(_{d,H,a_{0},}\) (Line 10). At each time \(t\), the current iterate \((p_{t},M_{t}^{[1:H]})\), which defines a policy \(^{p_{t},M_{t}^{[1:H]}}\), is used to choose the control \(u_{t}\). The optimization subroutine then receives a new loss function \(_{t}:_{d,H,a_{0},}\) based on the newly observed cost function \(c_{t}\). As with GPC, showing that this algorithm works requires showing that the policy class is sufficiently expressive. Unlike for GPC, our comparator policies are not strongly stabilizing, so new ideas are required for the proof.

We next formally define the policy \(^{p,M^{[1:H]}}\) associated with parameters \((p,M^{[1:H]})\), and the loss function \(_{t}\) used to update the optimization algorithm at time \(t\).

Parametrization of policies.First, for \(t[T]\) and \(i^{+}\), we define the weights

\[_{t,i}:=_{t-i}_{j=1}^{i-1}(1-_{t-j}), _{t,i}:=_{j=1}^{i}(1-_{t-j}),_{t,0}:=1- _{i=1}^{H}_{t,i}.\] (7)

We write \(w_{0}:=x_{1}\), \(_{0}=1\), and \(w_{t}=0\) for \(t<0\) as a matter of convention.12\(_{t,i}\) can be interpreted as the "influence of perturbation \(w_{t-i}\) on the state \(x_{t}\)", and \(_{t,i}\) can be interpreted as the "influence of perturbations prior to time step \(t-i\) on the state \(x_{t}\)". An element \((p,M^{[1:H]})_{d,H,a_{0},}\) induces a policy13 at time \(t\), denoted \(_{t}^{p,M^{[1:H]}}\), via the following variant of the _disturbance-action control_:

\[_{t}^{p,M^{[1:H]}}(_{t-1:t-H}):=_{t,0} p+_{j=1}^{H} _{t,j} M^{[j]}_{t-j}.\] (8)

In Line 6 of GPC-Simplex, the control \(u_{t}\) is chosen to be \(_{t}^{p_{t},M_{t}^{[1:H]}}(w_{t-1:t-H})\), which belongs to \(_{|p_{t}||_{1}}^{d}\) (using \(_{i=0}^{H}_{t,i}=1\)) and hence to the constraint set \(\) (since \(\|p_{t}\|_{1}[a_{0},][,]\)).

Loss functions.For \((p,M^{[1:H]})_{d,H,a_{0},}\), we let \(x_{t}(p,M^{[1:H]})\) and \(u_{t}(p,M^{[1:H]})\) denote the state and control at step \(t\) obtained by following the policy \(_{s}^{p,M^{[1:H]}}\) at all time steps \(s\) prior to \(t\) (see Eqs. (20) and (21) in the appendix for precise definitions). We then define \(_{t}(p,M^{[1:H]})\) to be the evaluation of the adversary's cost function \(c_{t}\) on the state-action pair \((x_{t}(p,M^{[1:H]}),u_{t}(p,M^{[1:H]}))\) (Line 9).

Main guarantee and proof overview.Theorem 7 gives our regret upper bound for GPC-Simplex:

**Theorem 7**.: _Let \(d,T\) and \(>0\). Let \(=(A,B,,x_{1},(_{t})_{t},(w_{t})_{t },(c_{t})_{t})\) be a simplex LDS with cost functions \(\{c_{t}\}_{t}\) satisfying Assumption 1 for some \(L>0\). Set \(H:=[(2LT^{3})]\). Then the iterates \((x_{t},u_{t})_{t=1}^{T}\) of GPC-Simplex (Algorithm 1) with input \((A,B,,H,,T)\) satisfy:_

\[_{_{}^{}()}:=_{ t=1}^{T}c_{t}(x_{t},u_{t})-_{K_{}^{}( )}_{t=1}^{T}c_{t}(x_{t}(K),u_{t}(K))(L^{7/2}d^ {1/2}),\]

_where \(()\) hides only universal constants and poly-logarithmic dependence in \(T\). Moreover, the time complexity of GPC-Simplex is \((d,T)\)._

While for simplicity we have stated our results for obliviously chosen \((_{t})_{t},(w_{t})_{t},(c_{t})_{t}\), since GPC-Simplex is deterministic the result also holds when these parameters are chosen adaptively by an adversary. See C for the formal proof of Theorem 7.

Lower bound.We also show that the mixing assumption on the comparator class \(_{}^{}()\) (Definition 6) cannot be removed. In particular, without that assumption, if the valid control set \(\) is restricted to controls \(u_{t}\) of norm at mostly roughly \(O(1/T)\), then linear regret is unavoidable.14

**Theorem 8** (Informal statement of Theorem 30).: _Let \(>0\) be a sufficiently large constant. For any \(T\), there is a distribution \(\) over simplex LDSs with state space \(^{2}\) and control space \(_{[0,/T]}_{}^{2}\), such that any online control algorithm on a system \(\) incurs expected regret \((T)\) against the class of all time-invariant linear policies \(x Kx\) where \(K_{[0,/T]}_{}^{d}\)._

## 4 Experimental Evaluation

The previous sections focused on linear systems, but in fact GPC-Simplex can be easily modified to control non-linear systems, for similar reasons as in prior work . It suffices for the dynamics to have the form

\[x_{t+1}:=(1-_{t})f(x_{t},u_{t})+_{t}w_{t}\] (9)

for known \(f\), observed \(_{t}\), and unknown \(w_{t}\). See Appendix E for discussion of the needed modifications and other implementation details. Relevant code is open-sourced in .

As a case study, in this section we apply GPC-Simplex (Algorithm 1) to a disease transmission model - specifically, a controlled generalization of the SIR model introduced earlier. In Appendix H we apply GPC-Simplex to a controlled version of the _replicator dynamics_ from evolutionary game theory.

A controlled disease transmission model.The Susceptible-Infectious-Recovered (SIR) model is a basic model for the spread of an epidemic . The SIR model has been extensively studied since last century  and attracted renewed interest during the COVID-19 pandemic . As discussed previously, this model posits that a population consists of susceptible (**S**), infected (**I**), and recovered (**R**) individuals. When a susceptible individual comes into contact with an infected individual, the susceptible individual becomes infected at some "transmission rate" \(\). Infected patients become uninfected and gain immunity at some "recovery rate" \(\). We consider a natural generalization of the standard dynamics Eq. (1) where recovered individuals may also lose immunity at a rate of \(\). Formally, in the absence of control, the population evolves over time according to the following system of differential equations:

\[=- IS+ R,\ \ = IS- I,\ \ = I- R,\] (10)

Typically, \(>>\). We normalize the total population to be \(1\), and thus \(x=[S,I,R]^{3}\). Next, we introduce a variable called the _preventative control_\(u_{t}^{2}\), which has the effect of decreasing the transmission rate \(\), and adversarial perturbations \(w_{t}\), which allow for model misspecification. Incorporating these changes to the forward discretization of Eq. (10) gives the following dynamics:

\[S_{t+1}\\ I_{t+1}\\ R_{t+1}=(1-_{t})1- I_{t}&0&\\ 0&1-&0\\ 0&&1-S_{t}\\ I_{t}\\ R_{t}+ I_{t}S_{t}&0\\ 0& I_{t}S_{t}\\ 0&0u_{t}+_{t}w_{t}.\] (11)

The control \(u_{t}^{2}\) represents a distribution over transmission prevention protocols: \(u_{t}=\) represents full-scale prevention, whereas \(u_{t}=\) represents that no prevention measure is imposed. Concretely, the effective transmission rate under control \(u_{t}\) is \( u_{t}(2)\).

Parameters and cost function.To model a highly infectious pandemic, we consider Eq. (11) with parameters \(=0.5\), \(=0.03\), and \(=0.005\). Suppose we want to control the number of infected individuals by modulating a (potentially expensive) prevention protocol \(u_{t}\). To model this setting, the cost function includes (1) a quadratic cost for infected individuals \(I_{t}\), and (2) a cost that is bilinear in the magnitude of prevention and the susceptible individuals:

\[c_{t}(x_{t},u_{t})=c_{3} x_{t}(2)^{2}+c_{2} x_{t}(1) u_{t}(1),\] (12)

where \(x_{t}=[S_{t},I_{t},R_{t}]\). Typically \(c_{3} c_{2}>0\) to model the high cost of infection.

In Fig. 1, we compare GPC-Simplex against two baselines - (a) always executing \(u_{t}=\) (i.e. full prevention), and (b) always executing \(u_{t}=\) (i.e. no prevention) - for \(T=200\) steps in the above model with no perturbations. We observe that GPC-Simplex suppresses the transmission rate via high prevention at the initial stage of the disease outbreak, then relaxes as the outbreak is effectively controlled. Moreover, GPC-Simplex outperforms both baselines in terms of cumulative cost. See Appendix F for additional experiments exhibiting the robustness of GPC-Simplex to perturbations (i.e. non-zero \(_{t}\)'s) and different model parameters.

### Controlling hospital flows: reproducing a study by 

We now turn to the recent work , which also studies a controlled SIR model. Similar to above, they considered a control that temporarily reduces the rate of contact within a population. In one scenario (inspired by the COVID-19 pandemic), they considered a cost function that penalizesmedical surges, i.e. when the number of infected exceeds a threshold \(y_{}\) determined by hospital capacities. Formally, they define the cost of a trajectory \((x_{t},u_{t})_{t=1}^{T}\) as

\[(-3x_{T}(1)e^{-3(x_{T}(1)+x_{T}(2))})}{3}+_{0}^{T}[c_{2} u _{t}(1)^{2}+(x_{t}(2)-y_{})}{1+e^{-100(x_{t}(2)-y_{})}} ]dt,\] (13)

where \(W_{0}\) is the principal branch of Lambert's \(W\)-function, and \(c_{2},c_{3}\) are hyperparameters. The system parameters used by  are \(=0.3,=0.1,=0\). In the absence of noise and with a known cost function,  is able to compute the approximate solutions of the associated Hamilton-Jacobi-Bellman equations for various choices of \(c_{2},c_{3}\).

In Fig. 2, we show that GPC-Simplex (with a slightly modified instantaneous version of Eq. (13)) in fact _matches_ the optimal solution analytically computed by . See Appendix G for further experimental details, including the exact model parameters and cost function.

Figure 1: Control with cost function (12) for \(T=200\) steps: initial distribution \(x_{1}=[0.9,0.1,0.0]\); parameters \(c_{3}=10\), \(c_{2}=1\); no noise. **Left/Middle:** Cost and cumulative cost over time of GPC-Simplex versus baselines. **Right:** control \(u_{t}(2)\) (proportional to effective transmission rate) played by GPC-Simplex over time.

Figure 2: Controlling hospital flows for \(T=100\) steps: initial distribution \([0.9,0.01,0.09]\); parameters \(y_{max}=0.1\), \(c_{2}=0.01\), \(c_{3}=100\). **Left:** The dashed red line shows the number of infected over time under no control; note that \(y_{}\) (shown in dashed purple line) is significantly exceeded. The solid yellow and blue lines show the number of infected and susceptible under GPC-Simplex, which closely match the optimal solutions computed by  (dashed yellow and blue). **Right:** GPC-Simplex control (solid) vs. optimal control (dashed).