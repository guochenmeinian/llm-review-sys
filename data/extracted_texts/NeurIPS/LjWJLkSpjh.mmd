# When Can We Track Significant Preference Shifts in Dueling Bandits?

Joe Suk

Columbia University

joe.suk@columbia.edu

&Arpit Agarwal

Columbia University

aa4931@columbia.edu

###### Abstract

The \(K\)-armed dueling bandits problem, where the feedback is in the form of noisy pairwise preferences, has been widely studied due its applications in information retrieval, recommendation systems, etc. Motivated by concerns that user preferences/tastes can evolve over time, we consider the problem of _dueling bandits with distribution shifts_. Specifically, we study the recent notion of _significant shifts_(Suk and Kpotufe, 2022), and ask whether one can design an _adaptive_ algorithm for the dueling problem with \(O(T})\) dynamic regret, where \(\) is the (unknown) number of significant shifts in preferences. We show that the answer to this question depends on the properties of underlying preference distributions. Firstly, we give an impossibility result that rules out any algorithm with \(O(T})\) dynamic regret under the well-studied Condorcet and SST classes of preference distributions. Secondly, we show that SST\(\)STI is the largest amongst popular classes of preference distributions where it is possible to design such an algorithm. Overall, our results provides an almost complete resolution of the above question for the hierarchy of distribution classes.

## 1 Introduction

The \(K\)-armed dueling bandits problem has been well-studied in the multi-armed bandits literature (Yue and Joachims, 2011; Yue et al., 2012; Urvoy et al., 2013; Ailon et al., 2014; Zoghi et al., 2014, 2015a, 2015b; Dudik et al., 2015; Jamieson et al., 2015; Komiyama et al., 2015, 2016; Ramamohan et al., 2016; Chen and Frazier, 2017; Saha and Gaillard, 2022; Agarwal et al., 2022). In this problem, on each trial \(t[T]\), the learner pulls a _pair_ of arms and observes _relative feedback_ between these arms indicating which arm was preferred. The feedback is typically stochastic, drawn according to a pairwise preference matrix \(^{K K}\), and the regret measures the'sub-optimality' of arms with respect to a 'best' arm.

This problem has many applications, e.g. information retrieval, recommendation systems, etc, where relative feedback between arms is easy to elicit, while real-valued feedback is difficult to obtain or interpret. For example, a central task for information retrieval algorithms is to output a ranked list of documents in response to a query. The framework of online learning has been very useful for automatic parameter tuning, i.e. finding the best parameter(s), for such retrieval algorithms based on user feedback (Liu, 2009). However, it is often difficult to get numerical feedback for an individual list of documents. Instead, one can (implicitly) compare two lists of documents by interleaving them and observing the relative number of clicks (Radlinski et al., 2008). The availability of these pairwise comparisons allows one to tune the parameters of retrieval algorithms in real-time using the framework of dueling bandits.

However, in many such applications that rely on user generated preference feedback, there are practical concerns that the tastes/beliefs of users can change over time, resulting in a dynamicallychanging preference distribution. Motivated by these concerns, we consider the problem of _switching dueling bandits_ (or non-stationary dueling bandits), where the pairwise preference matrix \(_{t}\) changes an unknown number of times over \(T\) rounds. The performance of the learner is evaluated using _dynamic regret_ where sub-optimality of arms is calculated with respect to the current 'best' arm.

Saha and Gupta (2022) first studied this problem and provided an algorithm that achieves a nearly optimal (up to \(\) terms) _dynamic regret_ of \(()\) where \(L\) is the total number of _shifts_ in the preference matrix, i.e., the number of times \(_{t}\) differs from \(_{t+1}\). However, this result requires algorithm knowledge of \(L\). Alternatively, the algorithm of Saha and Gupta (2022) can be tuned to achieve a dynamic regret rate (also nearly optimal) \((V_{T}^{1/3}K^{1/3}T^{2/3})\) in terms of the total-variation of change in preferences \(V_{T}\) over \(T\) total rounds. This is similarly limited by requiring knowledge of \(V_{T}\).

On the other hand, recent works on the switching MAB problem show it is not only possible to design _adaptive_ algorithms with \(()\) dynamic regret without knowledge of the underlying environment (Auer et al., 2019), but also possible to achieve a much better bound of \(()\) where \( L\) is the number of _significant shifts_(Suk and Kpotufe, 2022). Specifically, a shift is significant when there is no'safe' arm left to play, i.e., every arm has, on some interval \([s_{1},s_{2}]\), regret order \((-s_{1}})\). Such a weaker measure of non-stationarity is appealing as it captures the changes in best-arm which are most severe, and allows for more optimistic regret rates over the previously known \((V_{T}K)^{1/3}T^{2/3}\).

Very recently, Buening and Saha (2022) considered an analogous notion of significant shifts for switching dueling bandits under the SST\(\)STI1 assumption. They gave an algorithm that achieves a dynamic regret of \((K)\), where \(\) is the (unknown) number of _significant shifts_. However, their algorithm estimates \((K^{2})\) pairwise preferences, and hence, suffers from a sub-optimal dependence on \(K\).

In this paper we consider the goal of designing _optimal_ algorithms for switching dueling bandits whose regret depends on the number of _significant_ shifts \(\). We ask the following question:

**Question**.: _Is it possible to achieve a dynamic regret of \(O()\) without knowledge of \(\)?_

We show that the answer to this question depends on conditions on the preference matrices. Specifically, we consider several well-studied conditions from the dueling bandits literature, and give an almost complete resolution of the achievability of \(O()\) dynamic regret under these conditions.

### Our Contributions

We first consider the classical _Condorcet winner_ (CW) condition where, at each time \(t[T]\), there is a 'best' arm under the preference \(_{t}\) that stochastically beats every other arm. Such a winner arm is a benchmark in defining the aforementioned dueling _dynamic regret_. Our first result shows that, even under the CW condition, it is in general impossible to achieve \(O()\) dynamic regret.

**Theorem 1**.: _(Informal) There is a family of instances \(\) under Condorcet where all shifts are non-significant, i.e. \(=0\), but no algorithm can achieve \(o(T)\) dynamic regret uniformly over \(\)._

Note that in the case when \(=0\), one would ideally like to achieve a dynamic regret of \(O()\). The above theorem shows that, under the Condorcet condition when \(=0\), not only is it impossible

Figure 1: The hierarchy of distribution classes. The dark region is where \(O()\) dynamic regret is not achievable, whereas the light region indicates achievablility (e.g., by our Algorithm 1).

to achieve \(O()\) regret, it is even impossible to achieve \(O(T^{})\) regret for any \(<1\). Hence, this rules out the possibility of an algorithm whose regret under this condition is sublinear in \(\) and \(T\).

The proof of the above theorem relies on a careful construction where, at each time \(t\), the preference \(_{t}\) is chosen uniformly at random from two different matrices \(^{+}\) and \(^{-}\). These matrices have different 'best' arms but there is a unique _safe arm_ in both. However, it is impossible to identify this safe arm as all observed pairwise preferences are \(()\) over the randomness of the environment. Moreover, the theorem gives two different constructions (one ruling out SST and one STI) which together rule out all preference classes outside of SST\(\)STI. Our second result shows that the desired regret \(T}\) is in fact achievable (adaptively) under SST\(\)STI.

**Theorem 2**.: _(Informal) There is an algorithm that achieves a dynamic regret of \((T})\) under SST\(\)STI without requiring knowledge of \(\)._

Figure 1 gives a summary of our results. Note that in stationary dueling bandits there is no separation in the regret achievable under the CW vs. SST\(\)STI conditions, i.e. \(O()\) is the minimax optimal regret rate under both conditions (Saha and Gaillard, 2022). However, our results show that in the non-stationary setting with regret in terms of significant shifts, there is a separation in adaptively achievable regret.

Key Challenge and Novelty in Regret Upper Bound:To contrast, the recent work of Buening and Saha (2022) only attains \((KT})\) dynamic regret under SST\(\)STI due to inefficient exploration of arm pairs. Our more challenging goal of obtaining the optimal dependence on \(K\) introduces key difficulties in algorithmic design. In fact, even in the classical stochastic dueling bandit problem with SST\(\)STI, most existing results that achieve \(O()\) regret require identifying a coarse ranking over arms to avoid suboptimal exploration of low ranked arms (Yue et al., 2012; Yue and Joachims, 2011; Yue et al., 2012). However, in the non-stationary setting, ranking the arms meaningfully is difficult as the true ordering of arms may change (insignificantly) at all rounds. Our main algorithmic innovation is to bypass the task of ranking arms and instead directly focus on minimizing the cumulative regret of played arms. This entails a new rule for selecting "candidate" arms based on cumulative regret that may be of independent interest.

### Related Work

Dueling bandits.The stochastic dueling bandits problem and its variants have been studied widely (see Sui et al. (2018) for a comprehensive survey). This problem was first proposed by Yue et al. (2012), who provide an algorithm achieving instance-dependent \(O(K T)\) regret under the SST\(\)STI condition. Yue and Joachims (2011) also studied this problem under the SST\(\)STI condition and gave an algorithm that achieves optimal instance-dependent regret. Urvoy et al. (2013) studied this problem under the Condorcet winner condition and achieved an instance-dependent \(O(K^{2} T)\) regret bound, which was further improved by Zoghi et al. (2014) and Komiyama et al. (2015) to \(O(K^{2}+K T)\). Finally, Saha and Gaillard (2022) showed that it is possible to achieve an optimal instance-dependent bound of \(O(K T)\) and instance-independent bound of \(O()\) under the Condorcet condition. More general notions of winners such as Borda winner (Jamieson et al., 2015), Copeland winner (Zoghi et al., 2015; Komiyama et al., 2016; Wu and Liu, 2016), and von Nuemann winner (Dudik et al., 2015) have also been considered. However, these works only consider the stationary setting whereas we consider the non-stationary setting.

There has also been work on adversarial dueling bandits (Saha et al., 2021; Gajane et al., 2015), however, these works only consider static regret against the 'best' arm in hindsight and whereas we consider the harder dynamic regret. Other than the two previously mentioned works (Gupta and Saha, 2022; Buening and Saha, 2022), the only other work on switching dueling bandits is Kolpaczki et al. (2022), whose procedures require knowledge of non-stationarity and only consider the weaker measure of non-stationarity \(L\) counting all changes in the preferences.

Non-stationary multi-armed bandits.Multi-armed bandits with changing rewards was first considered in the adversarial setup by Auer et al. (2002), where a version of EXP3 was shown to attain optimal dynamic regret \(\) when properly tuned using the number \(L\) of changes in the rewards. Later works established similar (non-adaptive) guarantees in this so-called _switching bandit_ problem via procedures inspired by stochastic bandit algorithms (Garivier and Moulines, 2011; Kocsisand Szepesvari, 2006]. More recent works [Auer et al., 2018, 2019, Chen et al., 2019] established the first adaptive and optimal dynamic regret guarantees, without requiring knowledge of the number of changes. An alternative parametrization of switching bandits, via a total-variation quantity, was introduced in Besbes et al.  with minimax rates quantified therein and adaptive rates attained in Chen et al. . Yet another characterization, in terms of the number of best arm switches \(S\) was studied in Abbasi-Yadkori et al. , establishing an adaptive regret rate of \(\). Around the same time, Suk and Kpotufe  introduced the aforementioned notion of _significant shifts_ and adaptively achieved rates of the form \(T}\) in terms of \(\) significant shifts in rewards.

## 2 Problem Formulation

We consider non-stationary dueling bandits with \(K\) arms and time-horizon \(T\). At round \(t[T]\), the pairwise preference matrix is denoted by \(_{t}^{K K}\), where the \((i,j)\)-th entry \(P_{t}(i,j)\) encodes the likelihood of observing a preference for arm \(i\) in a direct comparison with arm \(j\). The preference matrix may change arbitrarily from round to round. At round \(t\), the learner selects a pair of actions \((i_{t},j_{t})[K][K]\) and observes the feedback \(O_{t}(i_{t},j_{t})(P_{t}(i_{t},j_{t}))\) where \(P_{t}(i_{t},j_{t})\) is the underlying preference of arm \(i_{t}\) over \(j_{t}\). We define the pairwise gaps \(_{t}(i,j):=P_{t}(i,j)-1/2\).

**Conditions on Preference Matrix.** We consider two different conditions on preference matrices: (1) the Condorcet winner (CW) condition and (2) the strong stochastic transitivity (SST) and stochastic triangle inequality (STI), formalized below.

**Definition 1**.: _(CW condition) At each round \(t\), there is a_ **Condorcet winner** _arm, denoted by \(a_{t}^{*}\), such that \(_{t}(a_{t}^{*},a) 0\) for all \(a[K]\{a_{t}^{*}\}\). Note that \(a_{t}^{*}\) need not be unique._

**Definition 2**.: _(SST\(\)STI condition) At each round \(t\), there exists a total ordering on arms, denoted by \(_{t}\), and \( i_{t}j_{t}k\):_

* \(_{t}(i,k)\{_{t}(i,j),_{t}(j,k)\}\) _(SST)._
* \(_{t}(i,k)_{t}(i,j)+_{t}(i,k)\) _(STI)._

It's easy to see that the SST condition implies the CW condition as \(_{t}(i,j)_{t}(i,i)=0\) for any \(i_{t}j\). Hence, the highest ranked item under \(_{t}\) in Definition 2 is the CW \(a_{t}^{*}\). We emphasize here that the CW in Definition 1 and the total ordering on arms in Definition 2 can change at each round, even while such unknown changes in preference may not be counted as significant (see below).

**Regret Notion.** Our benchmark is the _dynamic regret_ to the sequence of Condorcet winner arms:

\[(T):=_{t=1}^{T}(a_{t}^{*},i_{t})+_{t}(a_{t }^{*},j_{t})}{2}.\]

Here, the regret of an arm \(i\) is defined in terms of the preference gap \(_{t}(a_{t}^{*},i)\) between the winner arm \(a_{t}^{*}\) and \(i\), and the regret of the pair \((i_{t},j_{t})\) is the average regret of individual arms \(i_{t}\) and \(j_{t}\). Note the this regret is well-defined under both Condorcet and SST\(\)STI conditions due to the existence of a unique 'best' arm \(a_{t}^{*}\), and is non-negative due to the fact that \(_{t}(a_{t}^{*},i) 0\) for all \(i[K]\).

**Measure of Non-Stationarity.** We first recall the notion of Significant Condorcet Winner Switches from Buening and Saha , which captures only the switches in \(a_{t}^{*}\) which are severe for regret. Throughout the paper, we'll also refer to these as _significant shifts_ for brevity.

**Definition 3** (Significant CW Switches).: _Define an arm \(a\) as having_ **significant regret** _over \([s_{1},s_{2}]\) if_

\[_{s=s_{1}}^{s_{2}}_{s}(a_{s}^{*},a)-s_{1})}.\] (1)

_We then define_ **significant CW switches** _recursively as follows: let \(_{0}=1\) and define the \((i+1)\)-th significant CW switch \(_{i+1}\) as the smallest \(t>_{i}\) such that for each arm \(a[K]\), \([s_{1},s_{2}][_{i},t]\) such that arm \(a\) has significant regret over \([s_{1},s_{2}]\). We refer to the interval of rounds \([_{i},_{i+1})\) as a_ **significant phase**_. Let \(\) be the number of significant CW switches elapsed in \(T\) rounds._

**Notation.**_To ease notation, we'll conflate the closed, open, and half-closed intervals of real numbers \([a,b]\), \((a,b)\), and \([a,b)\), with the corresponding rounds contained therein, i.e. \([a,b][a,b]\)._Hardness of Significant Shifts in the Condorcet Winner Setting

We first consider regret minimization in an environment with no significant shift in \(T\) rounds. Such an environment admits a _safe arm_2 which does not incur significant regret throughout play. Our first result shows that, under the Condorcet condition, it is not possible to distinguish the identity of \(a^{}\) from other unsafe arms, which will in turn make sublinear regret impossible.

**Theorem 3**.: _For each horizon \(T\), there exists a finite family \(\) of switching dueling bandit environments with \(K=3\) that satisfies the Condorcet winner condition (Definition 1) with \(=0\) significant shifts. The worst-case regret of any algorithm on an environment \(\) in this family is lower bounded as_

\[_{}_{}[(T) ] T/8.\]

Proof.: (sketch; details found in Appendix B) Letting \( 1/\), consider the preference matrices:

\[^{+}:=1/2&1/2+&1\\ 1/2-&1/2&1/2+\\ 0&1/2-&1/2,^{-}:=1/2&1/2- &0\\ 1/2+&1/2&1/2-\\ 1&1/2+&1/2.\]

In \(^{+}\), arm \(1\) is the Condorcet winner and \(1 2 3\), whereas in \(^{-}\), \(3\) is the winner with \(3 2 1\). Let an oblivious adversary set \(_{t}\) at round \(t\) to one of \(^{+}\) and \(^{-}\), uniformly at random, inducing an environment where arm \(2\) remains safe for \(T\) rounds. Then, any algorithm will, over the randomness of the adversary, observe \(O_{t}(i_{t},j_{t})(1/2)\)_no matter the choice of arms \((i_{t},j_{t})\) played_, by the symmetry of \(^{+},^{-}\). Thus, it is impossible to distinguish arms, which implies linear regret by standard Pinsker's inequality arguments. In particular, even a strategy playing arm \(2\) every round fails as arm \(2\) is unsafe in another (indistinguishable) setup with arms \(1\) and \(2\) switched in \(^{+},^{-}\). 

SST and STI Both Needed To Learn Significant Shifts.The preferences \(^{+},^{-}\) in the above proof violate STI but satisfy SST, whereas another construction using preferences \(^{+},^{-}\) which violate SST but satisfy STI also works in the proof (see Remark 2 in Appendix B). This shows that sublinear regret is impossible outside of the class SST\(\)STI (visualized in Figure 1).

**Remark 1**.: _Note the lower bound of Theorem 3 does not violate the established upper bounds \(\) and \(V_{T}^{1/3}T^{2/3}\) scaling with \(L\) changes in the preference matrix or total variation \(V_{T}\)(Gupta and Saha, 2022). Our construction in fact uses \(L=(T)\) changes in the preference matrix and \(V_{T}=(T)\) total variation. Furthermore, the regret upper bound \(\), in terms of \(S\) changes in Condorcet winner, of Buening and Saha (2022) is not contradicted either, for \(S=(T)\)._

## 4 Dynamic Regret Upper Bounds under SST/STI

Acknowledging that significant shifts are hard outside of the class SST\(\)STI, we now turn our attention to the achievability of \(T}\) regret3 in the SST\(\)STI setting. Our main result is an optimal dynamic regret upper bound attained _without knowledge of the significant shift times or the number of significant shifts_. Up to log terms, this is the first dynamic regret upper bound with optimal dependence on \(T\), \(\), and \(K\).

**Theorem 4**.: _Suppose SST and STI hold (see Definition 2). Let \(\{_{i}\}_{i=0}^{}\) denote the unknown significant shifts of Definition 3. Then, for some constant \(C_{0}>0\), Algorithm 1 has expected dynamic regret_

\[[(T)] C_{0}^{3}(T)_{i=0}^{}-_{i})},\]

_and using Jensen's inequality, this implies a regret rate of \(C_{0}^{3}(T)+1) T}\)._In fact, this regret rate can be transformed to depend on the _Condorcet winner variation_ introduced in Buening and Saha (2022) and the _total variation_ quantities introduced in Gupta and Saha (2022) and inspired by the total-variation quantity from non-stationary MAB (Besbes et al., 2014). The following corollary is shown using just the definition of the non-stationarity measures.

**Corollary 5** (Regret in terms of CW Variation).: _Let \(V_{T}:=_{t=2}^{T}_{a[K]}|P_{t}(a_{t}^{*},a)-P_{t-1}(a_{t}^{*},a)|\) be the unknown Condorcet winner variation. Using the same notation of Theorem 4: Algorithm 1 has expected dynamic regret_

\[[(T)] C_{0}^{3}(T)(+(KV_{T})^{1/3}T^{ 2/3}).\]

## 5 Algorithm

At a high level, the strategy of recent works on non-stationary multi-armed bandits (Chen et al., 2019; Wei and Luo, 2021; Suk and Kpotufe, 2022) is to first design a suitable base algorithm and then use a meta-algorithm to randomly schedule different instances of this base algorithm at variable durations across time. The key idea is that unknown time periods of significant regret can be detected fast enough with the right schedule. In order to accurately identify significant shifts, the base algorithm in question should be robust to all non-significant shifts. In the multi-armed bandit setting, a variant of the classical successive elimination algorithm (Even-Dar et al., 2006) possesses such a guarantee (Allesiardo et al., 2017), and serves as a base algorithm in Suk and Kpotufe (2022).

### Difficulty of Efficient Exploration of Arms.

In the non-stationary dueling problem, a natural analogue of successive elimination is to uniformly explore the arm-pair space \([K][K]\) and eliminate arms based on observed comparisons (Urvoy et al., 2013). The previous work (Theorem 5.1 of Buening and Saha, 2022) employs such a strategy as a base algorithm. However, such a uniform exploration approach incurs a large estimation variance of \(K^{2}\), which enters into the final regret bound of \(K}\). Thus, smarter exploration strategies are needed to obtain \(\) dependence.

In the stationary dueling bandit problem with SST\(\)STI, such efficient exploration strategies have long been known: namely, the Beat-The-Mean algorithm (Yue and Joachims, 2011) and the Interleaved Filtering (IF) algorithm (Yue et al., 2012). We highlight that these existing algorithms aim to learn the ordering of arms, i.e., arms are ruled out roughly in the same order as their true underlying ordering. This fact is crucial to attaining the optimal dependence in \(K\) in their regret analyses, as the higher ranked arms must be played more often against other arms to avoid the \(K^{2}\) cost of exploration.

However, in our setting, adversarial but non-significant changes in the ordering of arms could force perpetual exploration of lowest-ranked arms. This suggests that learning an ordering should not be a subtask of our desired dueling base algorithm. Rather, the algorithm should prioritize minimizing its own regret over time. Keeping this intuition in mind, we introduce an algorithm called **SW**itching **I**nterleaved **F**il**T**ering (SWIFT) (see Algorithm 2 in Section 5.2) which directly tracks regret and avoids learning a fixed ordering of arms.

A new idea for switching candidate arms.A natural idea that is common to many dueling bandit algorithms (including IF) is to maintain a _candidate arm_\(\) which is always played at each round, and serves as a reference point for partially ordering other arms in contention. If the current candidate is beaten by another arm then a new candidate is chosen, and this process quickly converges to the best arm. Since the ordering of arms may change at each round, any such rule that relies on a fixed ordering is deemed to fail in our setting. Our procedure does not rely on such a fixed ordering over arms, but instead tracks the aggregate regret \(_{t}_{t}(a,_{t})\) of the _changing sequence of candidate arms_\(\{_{t}\}_{t}\) to another fixed arm \(a\). Crucially, the candidate arm \(_{s}\) is always played at round \(s\) and so the history of candidate arms \(\{_{s}\}_{s t}\) is fixed at a round \(t\). This fact allows us to estimate the quantity \(_{s=1}^{t}_{s}(a,_{s})\) using importance-weighting at \(\) rates via martingale concentration. An algorithmic _switching criterion_ then switches the candidate arm \(_{t}\) to any arm \(a\) dominating the sequence \(\{_{s}\}_{s t}\) over time, i.e., \(_{s=1}^{t}_{s}(a,_{s})\). This simple, yet powerful, idea immediately gives us control of the regret of the candidate sequence \(\{_{t}\}_{t}\) which allows us to bypass the ranking-based arguments of vanilla IF and Beat-The-Mean. It also allows us to simultaneously bound the regret of a sub-optimal arm \(a\) against the sequence of candidate arms \(_{s=1}^{t}_{s}(_{s},a)\).

### Switching Interleaved Filtering (Swift)

SWIFT at round \(t\) compares a _candidate arm_\(_{t}\) with an arm \(a_{t}\) (chosen uniformly at random) from an _active arm set_\(_{t}\). Additionally, SWIFT maintains estimates \(_{t}(_{t},a)\) of \(_{t}(_{t},a)\) which are used to (1) evict active arms \(a_{t}\) and (2) _switch_ the candidate arm \(_{t+1}\) for the next round.

Estimators and Eviction/Switching Criteria.Let \(_{t}\) be the active arm set at round \(t\). Let

\[_{t}(_{t},a):=|_{t}| O_{t}(_{t},a) \{(i_{t},j_{t})=(_{t},a)\}-1/2,\] (2)

which is an unbiased estimator of the gap \(_{t}(_{t},a)\) when \(a_{t}\). We _evict_ an active arm \(a\) from \(_{t}\) at round \(t\) if for some constant \(C>0\)5 and rounds \(s_{1}<s_{2} t\):

\[_{s=s_{1}}^{s_{2}}_{s}(_{s},a) C(T)-s_{1}) K^{2}},\] (3)

where \(_{s}(a,_{s}):=-_{s}(_{s},a)\). Next, we switch the next candidate arm \(_{t+1} a\) to another arm \(a_{t}\) at round \(t\) if for some round \(s_{1}<t\):

\[_{s=s_{1}}^{t}_{s}(a,_{s}) C(T)) K^{2}}.\] (4)

SWIFT is formally shown in Algorithm 2, defined for generic start time \(t_{}\) and duration \(m_{0}\) so as to allow for recursive calls in our meta-algorithm framework.

### Non-Stationary Algorithm (Metaswift)

``` Input: horizon \(T\).
1Initialize: round count \(t 1\).
2Episode Initialization (setting global variables \(t_{},_{},B_{s,m}\)): // \(t_{}\) indicates start of \(\)-th episode. \(t_{} t\). ; // Master active arm set.  For each \(m=2,4,,2^{(T)}\) and \(s=t_{}+1,,T\):  Sample and store \(B_{s,m}()}})\). ; // Set replayschedule.
3Run Base-Alg\((t_{},T+1-t_{})\).
4if\(t<T\)then restart from Line 2 (i.e. start a new episode). ; ```

**Algorithm 1**Meta-Elimination while Tracking Arms in SWIFT (METASWIFT)

For the non-stationary setting with multiple (unknown) significant shifts, we run SWIFT as a base algorithm at randomly scheduled rounds and durations.

Our algorithm, dubbed METASWIFT and found in Algorithm 1, operates in _episodes_, starting each episode by playing a _base algorithm_ instance of SWIFT. A running base algorithm _activates_ its own base algorithms of varying durations (Line 8 of Algorithm 2), called _replays_ according to a random schedule decided by the Bernoulli's \(B_{s,m}\) (see Line 6 of Algorithm 1). We refer to the (unique) base algorithm playing at round \(t\) as the _active base algorithm_.

Global Variables.The _active arm set_\(_{t}\) is pruned by the active base algorithm at round \(t\), and globally shared between all running base algorithms. In addition, all other variables, i.e. the \(\)-th episode start time \(t_{}\), round count \(t\), schedule \(\{B_{s,m}\}_{s,m}\), and candidate arm \(_{t}\) (and thus the quantities \(_{t}(_{t},a)\)) are shared between base algorithms. Thus, while active, each Base-Alg can switch the candidate arm (4) and evict arms (3) over all intervals \([s_{1},s_{2}]\) elapsed since it began.

Note that only one base algorithm (the active one) can edit \(_{t}\) and set the candidate arm \(_{t}\) at round \(t\), while other base algorithms can access these global variables at later rounds. By sharing these globalvariables, any replay can trigger a new episode: every time an arm is evicted by a replay, it is also evicted from the _master arm set_\(_{}\), tracking arms' regret throughout the entire episode. A new episode is triggered when \(_{}\) becomes empty, i.e., there is no _safe_ arm left to play.

## 6 Regret Analysis

### Regret of METASWIFT over Significant Phases

Now, we turn to sketching the proof of Theorem 4. Full details are found in Appendix C.

Decomposing the Regret.Let \(a_{t}^{}\) denote the _last safe arm_ at round \(t\), or the last arm to incur significant regret in the unique phase \([_{i},_{i+1})\) containing round \(t\). Then, we can decompose the dynamic regret around this safe arm using SST and STI (i.e., using Lemma 8 twice) as:

\[_{t=1}^{T}_{t}(a_{t}^{*},_{t})+_{t}(a_{t}^{*},a_{t}) 6 _{t=1}^{T}_{t}(a_{t}^{*},a_{t}^{})+3_{t=1}^{T}_{t}(a_ {t}^{},_{t})+_{t=1}^{T}_{t}(_{t},a_{t}),\]

where we recall that \(a_{t}_{t}\) is the other arm played (Line 3 of Algorithm 2). Next, the first sum on the above RHS is order \(_{i=1}^{L}-_{i-1})}\) as the last safe arm \(a_{t}^{}\) does not incur significant regret on \([_{i},_{i+1})\). So, it remains to bound the last two sums on the RHS above.

Episodes Align with Significant Phases.We claim that a new episode is triggered only if there a significant shift occurs (Lemma 11). This follows from our eviction criteria (3) with Freedman's inequality for martingale concentration (Lemma 9). Then, acknowledging episodes roughly align with significant phases, we turn our attention to bounding the remaining regret in each episode.

Bounding Regret of an Episode.Let \(t_{}\) be the start of the \(\)-th episode of METASWIFT. Then, our goal is to show for all \([]\) (where \(\) is the random number of episodes used by the algorithm):

\[\{[_{t=t_{}}^{t_{+1}-1}_{t}(a_{t}^{ },_{t})],[_{t=t_{}}^{t_{+1}-1} _{t}(_{t},a_{t})]\}_{i[+1];[ _{i-1},_{i})[t_{},t_{+1})}-_{i-1})},\] (5)where the RHS sum above is over the significant phases \([_{i-1},_{i})\) overlapping episode \([t_{},t_{+1})\). Summing over episodes \([]\) will then yield the desired total regret bound by our earlier observation that the episodes align with significant phases (see Lemma 11).

Bounding Regret of Active Arms to Candidate Arms.Bounding \(_{t=t_{}}^{t_{+1}-1}_{t}(_{t},a_{t})\) follows in a similar manner as Appendix B.1 of Suk and Kpotufe (2022). First, observe by concentration (Lemma 9) the eviction criterion (3) bounds the sums \(_{t=s_{1}}^{s_{2}}_{t}(_{t},a)\) over intervals \([s_{1},s_{2}]\) where \(a\) is active. Then, accordingly, we further partition the episode rounds \([t_{},t_{+1})\) into different intervals distinguishing the unique regret contributions of different active arms from varying base algorithms, on each of which we can relate the regret to our eviction criterion. Details can be found in Appendix C.3.

\(\)Bounding Regret of Candidate Arm to Safe Arm.The first sum on the LHS of (5) will be further decomposed using the _last master arm_\(a_{}\) which is the last arm to be evicted from the master arm set \(_{}\) in episode \([t_{},t_{+1})\). Carefully using SST and STI (see Lemma 13), we further decompose \(_{t}(a_{}^{},_{t})\) as:

\[_{t=t_{}}^{t_{+1}-1}_{t}(a_{t}^{},_{t}) 2 }^{t_{+1}-1}_{t}(a_{t}^{},a_{} )}_{}+}^{t_{+1}-1}_{t}(a_{}, _{t})}_{}+3}^{t_{+1}-1}_{ t}(a_{t}^{*},a_{t}^{})}_{}\] (6)

The sum C above was already bounded earlier. So, we turn our attention to B and A.

\(\)Bounding B.Note that the arm \(a_{}\) by definition is never evicted by any base algorithm until the end of the episode \(t_{+1}-1\). This means that at round \(t[t_{},t_{+1})\), the quantity \(_{s=t_{}}^{t}_{s}(a_{},_{s})\) is always kept small by the candidate arm switching criterion (4). So, by concentration (Proposition 10), we have \(_{s=t_{}}^{t_{+1}-1}_{s}(a_{},_{s}) -t_{})}\).

\(\)Bounding AThe main intuition here, similar to Appendix B.2 of Suk and Kpotufe (2022), is that well-timed replay are scheduled w.h.p. to ensure fast detection of large regret of the last master arm \(a_{}\). Key in this is the notion of a _bad segment of time_: i.e., an interval \([s_{1},s_{2}][_{i},_{i+1})\) lying inside a significant phase with last safe arm \(a^{}\) where:

\[_{t=s_{1}}^{s_{2}}_{t}(a^{},a_{})-s_{1})}.\] (7)

For a fixed bad segment \([s_{1},s_{2}]\), the idea is that a fortuitously timed replay scheduled at round \(s_{1}\) and remaining active till round \(s_{2}\) will evict arm \(a_{}\).

It is not immediately obvious how to carry out this argument in the dueling bandit problem since, to detect large \(_{t}_{t}(a^{},a_{})\), the pair of arms \(a^{},a_{}\) need to both be played which, as we discussed in Section 5.1, may not occur often enough to ensure tight estimation of the gaps.

Instead, we carefully make use of SST/STI to relate \(_{t}(a^{},a_{})\) to \(_{t}(_{t},a_{})\). Note this latter quantity controls both the eviction (3) and \(_{t}\) switching (4) criteria. This allows us to convert bad intervals with large \(_{t}_{t}(a_{t}^{},a_{})\) to intervals with large \(_{t}_{t}(_{t},a_{})\). Specifically, by Lemma 13, we have that (7) implies

\[2_{t=s_{1}}^{s_{2}}_{t}(a^{},_{t})+_{t=s_{1}}^{s_{2 }}_{t}(_{t},a_{})+3_{t=s_{1}}^{s_{2}}_{t}(a_{t}^{*},a ^{})-s_{1})}.\] (8)

Then, we claim that, so long as a base algorithm \((s_{1},m)\) is scheduled from \(s_{1}\) running till \(s_{2}\), we will have \(_{t=s_{1}}^{s_{2}}_{t}(_{t},a_{})-s_{1})}\) which implies \(a_{}\) will be evicted. In other words, the second sum dominates the first and third sums in (8). We repeat earlier arguments to show this:

* By the definition of the last safe arm \(a^{}\), \(_{t=s_{1}}^{s_{2}}_{t}(a_{t}^{*},a^{})<-s_{1 })}\).
* Meanwhile, \(_{t=s_{1}}^{s_{2}}_{t}(a^{},_{t})-s_{1})}\) by the candidate switching criterion (4) and because \(a^{}\) will not be evicted before round \(s_{2}\) less it incurs significant regret which cannot happen by definition of \(a^{}\).

Combining the above two points with (8), we have that \(_{t=s_{i}}^{s_{2}}_{t}(_{t},a_{})-s_ {1})}\), which directly aligns with our criterion (3) for evicting \(a_{}\). To summarize, a bad segment \([s_{1},s_{2}]\) in the sense of (7) is detectable using a well-timed instance of \(\), which happens often enough with high probability. Concretely, we argue that not too many bad segments elapse before \(a_{}\) is evicted by a well-timed replay in the above sense and that thus the regret incurred by \(a_{}\) is bounded by the RHS of (5). The details can be found in Appendix C.5.

## 7 Conclusion

We consider the problem of switching dueling bandits where the distribution over preferences can change over time. We study a notion of significant shifts in preferences and ask whether one can achieve adaptive dynamic regret of \(O(T})\) where \(\) is the number of significant shifts. We give a negative result showing that one cannot achieve such a result outside of the SST\(\)STI setting, and answer this question in the affirmative under the SST\(\)STI setting. In the future, it would be interesting to consider other notions of shifts which are weaker than the notion of significant shift, and ask whether adaptive algorithms for the Condorcet setting can be designed with respect to these notions. Buening and Saha (2022) already give a \(O(K)\) bound for the Condorcet setting, where \(S\) is the number of changes in 'best' arm. However, their results have a suboptimal dependence on \(K\) due to reduction to "all-pairs" exploration.