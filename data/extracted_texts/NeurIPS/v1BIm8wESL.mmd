# Skinned Motion Retargeting with Dense Geometric Interaction Perception

Zijie Ye\({}^{1,2}\), Jia-Wei Liu\({}^{3}\), Jia Jia\({}^{1,2}\), Shikun Sun\({}^{1,2}\), Mike Zheng Shou\({}^{3}\)

\({}^{1}\) Department of Computer Science and Technology, BNRist, Tsinghua University

\({}^{2}\) Key Laboratory of Pervasive Computing, Ministry of Education

\({}^{3}\) Show Lab, National University of Singapore

Work is partially done during visiting NUS.Corresponding Author.

###### Abstract

Capturing and maintaining geometric interactions among different body parts is crucial for successful motion retargeting in skinned characters. Existing approaches often overlook body geometries or add a geometry correction stage after skeletal motion retargeting. This results in conflicts between skeleton interaction and geometry correction, leading to issues such as jittery, interpenetration, and contact mismatches. To address these challenges, we introduce a new retargeting framework, _MeshRet_, which directly models the dense geometric interactions in motion retargeting. Initially, we establish dense mesh correspondences between characters using semantically consistent sensors (SCS), effective across diverse mesh topologies. Subsequently, we develop a novel spatio-temporal representation called the dense mesh interaction (DMI) field. This field, a collection of interacting SCS feature vectors, skillfully captures both contact and non-contact interactions between body geometries. By aligning the DMI field during retargeting, _MeshRet_ not only preserves motion semantics but also prevents self-interpenetration and ensures contact preservation. Extensive experiments on the public Mixamo dataset and our newly-collected _ScanRet_ dataset demonstrate that _MeshRet_ achieves state-of-the-art performance. Code available at https://github.com/abcyzj/MeshRet.

## 1 Introduction

Skinned character animation is prevalent in virtual reality , game development , and various other fields. However, animating these characters often presents significant challenges due to differences in body proportions between the motion source and the target character, leading to issues such as loss of motion semantics, mesh interpenetration, and contact mismatches. Consequently, motion retargeting is essential to adjust for these discrepancies in body proportions. This process is crucial for maintaining the integrity of the source motion's characteristics in the animation of the target character.

Motion retargeting presents challenges due to the complex interactions among character limbs and the wide range of body geometries. Accurately preserving these interactions is crucial, as incorrect interactions can result in mesh interpenetration and contact mismatches. Prior research has typically addressed these interactions from two perspectives: skeleton interactions and geometry corrections. Early methods [1; 29; 15] employ cycle-consistency to implicitly align skeleton interaction semantics, yet they do not address the complexities of geometric interactions between different body parts. Villegas et al.  introduced mesh self-contact modeling; however, their approach does not extendto non-contact interactions. More recently, Zhang et al.  implemented a two-stage pipeline that first aligns skeleton interaction semantics and then corrects geometric artifacts. Nonetheless, the inherent conflict between preserving skeleton interaction semantics and correcting geometry leads to jittery movements, severe interpenetration and imprecise contacts. Zhang et al.  subsequently proposed adding a stage that aligns visual semantics with a visual language model, but this requires detailed pair-by-pair finetuning due to the loss of spatial information when projecting 3D motion into 2D images.

To resolve the conflict between skeleton interaction and geometry correction, we propose a new approach: focusing solely on dense geometric interaction for motion retargeting. Character animation videos, rendered from the skinned mesh, rely on geometric interactions to shape user perception. Skeleton interaction, in contrast, merely represents a simplified, sparse form of geometric interaction. Therefore, maintaining correct interactions between different body part geometries not only preserves motion semantics but also prevents mesh interpenetration and ensures contact preservation, as illustrated in Figure 1.

Given the significance of geometric interactions, we propose a new framework, named _MeshRet_, for skinned motion retargeting. In contrast to earlier methods that adjust skeletal motion retargeting outcomes, our approach models the intricate interactions among character meshes without depending on predefined vertex correspondences.

The design of _MeshRet_ necessitates several technical innovations. Initially, there is a requirement for dense mesh correspondence across different characters. Drawing inspiration from the medial axis inverse transform (MAIT) , we have devised a technique, termed semantically consistent sensors (SCS), to automatically derive dense mesh correspondence from sparse skeleton correspondence. This technique enables us to sample a point cloud of sensors on the mesh to represent each character. Following this, to illustrate dense mesh interaction between body parts, we employ interacting mesh sensor pairs, maintaining generality. These pair-wise interactions are encoded within a novel spatial-temporal representation termed the Dense Mesh Interaction (DMI) field. The DMI field adeptly encapsulates both contact and non-contact interaction semantics. Finally, we proceed to learn a motion manifold that aligns with the target character geometry and the source motion DMI field.

To align our evaluation process more closely with real animation production, we gathered an in-the-wild motion dataset, termed _ScanRet_, characterized by abundant contact semantics and minimal mesh interpenetration. _ScanRet_ consists of 100 human actors ranging from bulky to skinny, each performing 83 motion clips scrutinized by human animators. The _MeshRet_ model is trained on both the _ScanRet_ dataset and the widely used Mixamo  dataset. We assessed our method across a large variety of motions and a diverse array of target characters. Both qualitative and quantitative analyses show that our _MeshRet_ model significantly outperforms existing methods.

To summarize, we present the following contributions:

* We introduce _MeshRet_, a pioneering solution that facilitates geometric interaction-aware motion retargeting across varied mesh topologies in a single pass.
* We present the SCS and the novel DMI field to guide the training of _MeshRet_, effectively encapsulating both contact and non-contact interaction semantics.
* We develop _ScanRet_, a novel dataset specifically tailored for assessing motion retargeting technologies, which includes detailed contact semantics and ensures smooth mesh interaction.
* Our experiments demonstrate that _MeshRet_ delivers exceptional performance, marked by accurate contact preservation and high-quality motion.

## 2 Related Work

Skeletal motion retargetingMotion retargeting seeks to preserve the characteristics of source motions when transferring them to a different target character. Skeletal motion retargeting primarily addresses the challenge of differing bone ratios. Gleicher  initially formulated motion retargeting as a spatio-temporal optimization problem, using source motion features as kinematic constraints. Subsequent researches [5; 7; 14] have focused on optimization-based approaches with various constraints. However, these methods, while requiring extensive optimization, often yield suboptimal results. Consequently, recent studies have explored learning-based motion retargeting algorithms. Jang et al.  trained a motion retargeting network using a U-Net  architecture on paired motion data. Villegas et al.  introduced a recurrent neural network combined with cycle-consistency  for unsupervised motion retargeting. Lim, Chang, and Choi  propose to learn frame-by-frame poses and overall movements separately. Aberman et al.  develop differentiable operators for cross-structural motion retargeting among homeomorphic skeletons. However, these methods generally neglect the geometry of characters, leading to frequent contact mismatches and severe mesh interpenetrations.

Geometry-aware motion retargetingPrevious studies have generally processed character geometries through two approaches: contact preservation and interpenetration avoidance. Lyard and Magnenat-Thalmann  developed a heuristic optimization algorithm to maintain character self-contact, while Ho, Komura, and Tai  proposed to maintain character interactions by minimizing the deformation of interaction meshes. Ho and Shum  introduced a spatio-temporal optimization framework to prevent self-collisions in robot motion retargeting. Jin, Kim, and Lee  employed a proxy volumetric mesh to preserve spatial relationships during retargeting. Subsequently, Basset et al.  combined both attraction and repulsion terms in an optimization-based method to avoid interpenetration and preserve contact. However, these methods necessitate per-vertex correspondence and involve costly optimization processes. More recently, Villegas et al.  attempted to retarget skinned motion through optimization in a latent space of a pretrained network, although their method does not accommodate non-contact interactions. Zhang et al.  implemented a two-stage pipeline that initially aligns skeleton interaction semantics and subsequently corrects geometric artifacts. Nevertheless, the inherent conflict between maintaining skeleton interaction semantics and correcting geometry often results in jittery movements and imprecise contacts. In a later study, Zhang et al.  added a stage that aligns visual semantics using a visual language model, but this approach requires extensive pair-by-pair fine-tuning due to the loss of spatial information when projecting 3D motion into 2D images.

Existing geometry-aware motion retargeting methods either require expensive optimization or employ multi-stage strategies for skeleton and geometry semantics, resulting in a contradiction between stages that often leads to unsatisfactory results. In contrast, our method processes both contact and non-contact semantics using a dense mesh interaction field in a single stage.

## 3 Method

### Overview

We introduce a novel geometric interaction-aware motion retargeting framework _MeshRet_, as illustrated in Figure 2. Unlike previous methods that either overlook character geometries [1; 29; 15] or apply geometry correction after skeleton retargeting [32; 30], our framework directly addresses dense geometric interactions with the Dense Mesh Interaction (DMI) field. This provides a detailed representation of the interactions within skinned character motions, preserving motion semantics by preventing mesh interpenetration and ensuring precise contact preservation.

Motion & geometry representationsAssume the motion sequence has \(T\) frames and the character has \(N\) skeletal joints. The motion sequence \(\) is represented by the global root translation

Figure 1: Comparison with the existing method. Contrary to the earlier retargeting-correction approach , which suffer from internal contradictions leading to interpenetration, jitter, and contact mismatches, our pipeline leverages the DMI field to accurately model complex geometric interactions.

and the local joint rotation \(^{T N 6}\), where we adopt the 6D representation  for the joint rotations. The rest-pose geometry \(\) of the character is represented by the rest-pose mesh \(\) and the rest-pose joint locations \(^{N 3}\).

Task definitionGiven the source motion sequence \(_{}\), and the geometries \(_{}\) and \(_{}\) of the source and target characters in their T-poses, our objective is to generate the motion \(_{}\) for the target character. This process aims to retain essential aspects of the source motion, including its semantics, contact preservation, and the avoidance of interpenetration.

Following the definition of the task, our _MeshRet_ model initially derives Semantically Consistent Sensors (SCS) \(^{S 4 3}\), which provide dense geometric correspondences essential for the retargeting process, where \(=_{}()\). \(\) captures the sensor location and the sensor tangent space matrix, facilitating an enhanced perception of the geometry surface. Subsequently, we conduct sensor forward kinematics (FK) and pairwise interaction extraction to generate the source DMI field \(_{}=_{}(_{}, _{})\), where \(_{}^{T K L P}\). Here, \(K\) is the number of SCS in the DMI field, \(L\) represents a hyper-parameter of feature selection, and \(P\) indicates the feature dimension of the DMI. Lastly, a transformer-based network  ingests \(_{}\), \(_{}\), \(_{}\), and \(_{}\), and predicts a target motion sequence \(_{}\) that aligns with the target character's geometry and the source DMI field. The entire pipeline is denoted as follows:

\[_{}=_{}(_{}, _{},_{},_{})\] (1)

### Semantically consistent sensors

To facilitate dense geometric interactions, our _MeshRet_ framework necessitates establishing dense mesh correspondence between source and target characters. Previous studies have typically derived correspondence from vertex coordinates , virtual sensor  or through a bounding mesh ; however, these methods are confined to template meshes sharing identical topology, such as MANO  or SMPL . Villegas et al.  suggested determining vertex correspondence using nearest neighbor searches on predefined feature vectors. Nevertheless, this approach often lacks precision and brevity, resulting in inaccurate contact representations and substantial optimization burdens.

In this study, we introduce Semantically Consistent Sensors (SCS) that are effective across various mesh topologies while ensuring precise semantic correspondence. Our approach draws inspiration

Figure 2: Overview of the proposed _MeshRet_. The pipeline begins with the extraction of the DMI field using sensor forward kinematics, denoted as \(_{k}\), and pairwise interaction feature selection, represented by \(_{c}\). This DMI field, in conjunction with geometric features derived from \(_{g}\), is fed into an encoder-decoder network. The network predicts the target motion sequence, which is aligned with the target character’s geometry and the original DMI field.

from the Medial Axis Inverse Transform (MAIT) . We conceptualize the skeleton bones of each character as approximate medial axes of their limbs and torso. For each bone, a MAIT-like transform is applied to generate the corresponding SCS. This involves casting rays from the bone axis across a plane perpendicular to it. The origin parameter \(l\) and direction parameter \(\) of the rays, combined with the bone index \(b\), establish the semantic coordinates of the SCS. The semantic coordinates describe connection between the sensor and the skeleton bones. A sensor is deemed valid if its ray intersects the mesh linked to the bone; otherwise, it is considered invalid. Through this method, we establish a dense geometric correspondence based on sparse skeletal correspondence. The procedure for deriving SCS is illustrated in Figure 3. Given a unified set of SCS semantic coordinates \(\{(b_{1},l_{1},_{1}),(b_{2},l_{2},_{2}),,(b_{S},l_{S},_{S})\}\), we can derive SCS feature \(=\{_{1},_{2},,_{S}\}\) for each character. Further details can be found in Algorithm 1.

### Dense mesh interaction field

To effectively represent the interactions between character limbs and the torso, we have developed the DMI field. Based on SCS detailed in Section 3.2, the DMI field comprehensively captures both contact and non-contact interactions across different body part geometries. Utilizing the DMI field allows for dense geometry interaction-aware motion retargeting, thereby eliminating the need for a geometry correction stage.

Sensor forward kinematicsFor a given motion sequence, denoted as \(\), we initially conduct forward kinematics (FK) on \(\) to derive sensor features \(^{1:T}^{T S 4 3}\). Each \(^{t}\) encompasses the locations and tangent matrices for \(S\) sensors at frame \(t\). The FK transformation for an individual sensor is expressed as:

\[_{i}^{t}=_{n=1}^{N}(_{i})_{n}G_{n}(^{t })_{i},\] (2)

where \(G_{n}(^{t}) SE(3)\) is the global transformation matrix for bone \(n\), derived from its local rotation matrix, and \((_{i})_{n}\) represents the linear blend skinning (LBS) weight for sensor \(_{i}\), determined through barycentric interpolation of its adjacent mesh vertices.

Pairwise interaction featureNext, we model the geometric interactions as pairwise interaction features between sensors. Ideally, for each frame, we obtain a comprehensive DMI field, \(}^{t}\), representing pairwise vectors across \(K^{2}\) sensor pairs:

\[^{t,i,j}=_{i}^{-1}(_{j}^{t}-_{i}^{t}),\] (3)

\[}^{t}=\{(^{t,i,j},b_{i},b_{j},l_{i},l_{j},_{ i},_{j})\}_{i=1:S}^{j=1:S},\] (4)

Figure 3: Left: Illustration of the method to derive a sensor feature \(\) from the semantic coordinate \((b,l,)\) across different characters. The red line represents the projected ray. The feature \(\) encompasses the sensor’s location and its tangent space matrix. Right: The DMI field effectively captures both contact and non-contact interactions. Red lines represent \(^{t,i,j}\) in the DMI field. In the second example, the body sensors (yellow points) are located in the tangent plane of the hand sensors (blue points), signifying a contact interaction.

where \(_{i}^{3 3}\) is the tangent matrix if sensor \(i\), and \(^{t,i,j}\) represents the relative position of target sensor \(j\) in the tangent space of observation sensor \(i\). \(}^{t}\) is composed of two components: the relative position of the sensor pair and the semantic coordinates of both the observation and target sensors. The use of semantic rather than spatial coordinates is essential, as it obviates the need for actual sensor positions, thereby making DMI suitable for motion retargeting applications.

However, \(}^{t}^{S S P}\) exhibits quadratic growth with respect to \(S\) because it includes \(S^{2}\) sensor pairs, rendering it impractical when managing thousands of sensors. To address this, we implement two sparsification strategies for \(}^{t}\). Initially, we restrict interactions to critical body parts only, such as arm-torso, arm-head, arm-arm, and leg-leg, rather than between all sensor pairs, thereby restricting our focus to \(K\) observation sensors. Subsequently, for each observation sensor, we select \(L\) target sensors from each relevant body part, where \(L\) is a predetermined hyper-parameter. Specifically, we empirically choose \(L/2\) nearest and \(L/2\) furthest target sensors. We find that proximate sensor pairs are crucial for minimizing interpenetration and maintaining contact, while distant pairs delineate the overall spatial relationships between body parts, as shown in Figure 3. These strategies lead to the formulation of the final DMI field \(^{K L P}\), with selected sensor pairs indicated by the sparse DMI mask \(_{}^{S S}\) shown in Figure 2.

### Geometry interaction-aware motion retargeting

To avoid the conflict between skeleton interaction and geometric correction, the proposed _MeshRet_ employs the DMI field to model geometric interactions directly. As shown in Figure 2, _MeshRet_ initially extracts the DMI field \(_{}\) from the source motion sequence \(_{}\), as described in Section 3.3. The field \(_{}\) encapsulates interactions among various body parts within the source motion, encompassing both contact and non-contact interactions, further depicted in Figure 3. The DMI field, composed of sensor pair feature vectors, possesses the unordered characteristics of a point cloud. Consequently, we implement a PointNet-like architecture  for our DMI encoder, which is divided into two components: the per-sensor encoder and the per-frame encoder. Given \(_{}^{T K L P}\), the per-sensor encoder initially processes it as \(T*K\) separate point clouds, producing representations \(_{}^{}^{T K D_{}}\) for each observation sensor, where \(D_{}\) denotes the feature dimension. Subsequently, the per-frame encoder generates per-frame representations \(_{}^{}^{T D_{}}\) by encoding these \(T\) point clouds.

Since DMI field \(_{}\) lacks geometric information about characters, we introduced a geometry encoder \(_{g}\) to extract geometric features from their SCS. For each sensor, we form a feature vector by concatenating its rest-pose feature \(_{i}\) with its semantic coordinates \((b_{i},l_{i},_{i})\). The resultant geometric features are represented as \(_{}^{S_{} C}\) for character A and \(_{}^{S_{} C}\) for character B. The semantic coordinates of sensors act as intermediaries linking the DMI field to character geometry. The geometry encoder employs a PointNet-like architecture  to transform the geometric features \(\) into a geometric latent code \(^{}^{D_{}}\).

The transformer-based retargeting network processes input features including the source DMI feature \(_{}^{}\), source joint rotation \(_{}\), source geometry latent \(_{}^{}\), and target geometry latent \(_{}^{}\). Specifically, the encoder processes \(_{}^{}\) and \(_{}^{}\), while the decoder processes \(_{}\) and \(_{}^{}\). The latents \(_{}^{}\) and \(_{}^{}\) serve as the initial tokens in the sequence, enabling both the encoder and decoder to operate over a sequence of length \(T+1\). The output sequence's final \(T\) frames are represented as \(}_{}\).

Due to the lack of paired ground-truth data, we employ the unsupervised method described by Lim, Chang, and Choi . Our network utilizes four loss functions for training: reconstruction loss, DMI consistency loss, adversarial loss, and end-effector loss. Supervision signals are derived from the source motion. We maintain geometric interactions by aligning the source DMI field \(_{}\) with the target DMI field \(}_{}\). The target DMI field \(}_{}\) is generated by first applying sensor forward kinematics to \(}_{}\), followed by selecting sensor pairs using the target sparse DMI mask \(_{}^{S S}\). This mask, \(_{}\), is derived by excluding invalid sensors of the target character from \(_{}\). The DMI consistency loss is quantified as the cosine similarity loss between pair-wise relative positions in \(}_{}\) and \(_{}\):

\[_{}=-_{t=1}^{T}_{k=1}^{K}_{l=1}^{L} c(k,l)_{}^{t,k,l}}_{}^{t,k,l}}{|| _{}^{t,k,l}||_{2}||}_{}^{t,k,l}|| _{2}},\] (5)where \(c(k,l)\) takes the value \(1\) if sensor pair \((k,l)\) is valid in both \(_{}\) and \(_{}\), and \(0\) otherwise. The reconstruction loss serves as a regularization mechanism to minimize motion alterations during retargeting, defined as follows:

\[_{}=||}_{}-_{}||_{ 2}^{2}.\] (6)

To facilitate realistic motion retargeting, a discriminator, denoted as \(()\), is employed. The adversarial loss is subsequently defined as:

\[_{}=_{ p_{}}[ ()]+_{ p(}_{})}[( 1-())].\] (7)

We observed that the global orientation of end-effectors significantly influences user experience. Consequently, we introduced an end-effector loss to promote consistent orientations of end-effectors in the retargeted motion.

\[_{}=|}_{t=1}^{T}_{i }||R(_{}^{t},i)-R(}_{}^{t },i)||,\] (8)

where \(R()\) transforms local joint rotations into global rotations for joint \(i\) along the kinematic chain and \(\) represents the set of end-effectors. Our _MeshRet_ is trained by:

\[_{}=_{}_{}+ _{}_{}+_{}_ {}+_{}_{}.\] (9)

## 4 Experiments

### Settings

DatasetsWe trained and evaluated our method using the Mixamo dataset  and the newly curated _ScanRet_ dataset. We downloaded 3,675 motion clips performed by 13 cartoon characters from the Mixamo dataset contains, while the _ScanRet_ dataset consists of 8,298 clips executed by 100 human actors. Notably, the Mixamo dataset frequently features corrupted data due to interpenetration and contact mismatches. To overcome these issues, we created the _ScanRet_ dataset, which provides detailed contact semantics and improved mesh interactions, with each clip being scrutinized by human animators. The training set comprises 90% of the motion clips from both datasets, involving nine characters from Mixamo and 90 from _ScanRet_. Our experiments tested the motion retargeting capabilities between cartoon characters and real humans, aligning closely with typical retargeting workflows. During inference, we adopted four data splits based on character and motion visibility: unseen character with unseen motion (UC+UM), unseen character with seen motion (UC+SM), seen character with unseen motion (SC+UM), and seen character with seen motion (SC+SM), as delineated by Zhang et al. . We present the average results across these splits. Additional details available in Appendix A.

Implementation detailsThe hyper-parameters \(_{}\), \(_{}\), \(_{}\), \(_{}\), and \(L\) were empirically set to 1.0, 5.0, 1.0, 1.0, and 20, respectively. We use \(\{0,1,,N_{}-1\}\{0,0.25,0.5,0.75\}\{0,0.5,, 1.5\}\) as the SCS semantic coordinates set, where \(N_{}=18\) is the number of body bones and \(\) represents the Cartesian product. We employed the Adam optimizer  with a learning rate of \(10^{-4}\) to optimize our network. The training process required 36 epochs. For further details, please refer to Appendix C.

Evaluation metricsWe assess the effectiveness of our method through three metrics: joint accuracy, contact preservation, and geometric interpenetration. Joint accuracy is quantified by calculating the Mean Squared Error (MSE) between the retargeted joint positions and the ground-truth data provided by animators in ScanRet. This analysis considers both global and local joint positions, normalized by the character heights. Contact preservation is evaluated by measuring the Contact Error, defined as the mean squared distance between sensors that were originally in contact in the source motion clip. Geometric interpenetration is determined by the ratio of penetrated limb vertices to the total limb vertices per frame. Further details are available in Appendix B.

### Comparison with state-of-the-arts

Qualitative resultsFigure 4 demonstrates the performance of skinned motion retargeting across characters with diverse body shapes, where the motion sequences are novel to the target charactersduring training. Most baseline methods, except R\({}^{2}\)ET , fail to consider the geometry of characters, leading to significant geometric interpenetration and contact mismatches. Unlike these methods, R\({}^{2}\)ET  includes a geometry correction phase after skeleton-aware retargeting. However, this creates a conflict between the two stages, resulting in oscillations in R\({}^{2}\)ET's outcomes, which manifest as alternating contact misses and severe interpenetrations, as shown in the first two rows. Additionally, these oscillations appear variably across different frames within the same motion clip, producing jittery motion, as illustrated in Figure 1 and Figure 8. A further limitation of R\({}^{2}\)ET is its neglect of hand contacts. In contrast, our method employs the innovative DMI field to preserve such detailed interactions, such as those observed in the "Praying" pose in the third row.

Quantitative resultsTable 1 presents a comparison between our methods and state-of-the-arts. We initially measure the joint location error using MSE and MSE\({}^{lc}\) on _ScanRet_. The ground truth in _ScanNet_ is established by human animators. Our observations indicate that human animators typically retarget motions by initially replicating joint rotations and subsequ

  
**Metric** & **MSE\(\)** & **MSE\({}^{lc}\)\(\)** & **Contact Error\(\)** & **Penetration**(\%)\(\) \\ 
**Dataset** & _ScanRet_ & _ScanRet_ & **Mixamo+** & _ScanRet_ & **Mixamo+** & _ScanRet_ \\  Source & - & - & - & 0.234 & 3.04 & 1.37 \\ Copy & 0.026 & 0.006 & 1.702 & 0.387 & 5.26 & 2.16 \\  PMnet  & 0.130 & 0.029 & 2.716 & 0.890 & 5.23 & 2.23 \\ SAN  & 0.049 & 0.011 & 2.432 & 0.627 & 4.95 & 1.72 \\ R\({}^{2}\)ET  & 0.063 & 0.017 & 2.209 & 0.589 & 4.21 & 2.01 \\  Ours\({}_{cls}\) & 0.048 & 0.013 & 0.800 & 0.426 & **3.35** & 1.73 \\ Ours\({}_{far}\) & **0.045** & 0.010 & 1.642 & 0.610 & 5.37 & 1.77 \\ Ours\({}_{dm}\) & 0.048 & 0.010 & 2.568 & 0.797 & 4.69 & 1.78 \\ Ours & 0.047 & **0.009** & **0.772** & **0.284** & 3.45 & **1.59** \\   

Table 1: Quantitative comparison between our method and state-of-the-arts. Mixamo+ represents the mixed dataset of Mixamo and _ScanRet_. MSE\({}^{lc}\) denotes the local MSE.

Figure 4: Qualitative comparison with baseline methods. Our method ensures precise contact preservation and minimal geometric interpenetration.

that display incorrect interactions. Conversely, our method modifies the entire motion sequence, resulting in a higher MSE compared to the Copy strategy. Nevertheless, MSE remains a valuable auxiliary reference. In comparison to PMnet , R\({}^{2}\)ET , and SAN , our method achieves MSE reductions of 65%, 29%, and 8%, respectively. These results demonstrate that our approach more closely aligns with the outputs produced by human animators.

As shown in Table 1, PMnet  and SAN , exhibit high interpenetration ratios and contact errors due to their neglect of character geometries. R\({}^{2}\)ET  effectively reduces interpenetration through a geometry correction stage; nonetheless, it still encounters high contact errors stemming from conflicts between the retargeting and correction stages. Our approach explicitly models geometry interactions and thereby achieves low contact error and penetration ratio, illustrating the effectiveness of our proposed _MeshRet_ in generating high-quality retargeted motions with detailed contact semantics and smooth mesh interactions. Additionally, we observe that retargeting using the mixed Mixamo+ dataset is more challenging than with the _ScanRet_ dataset, attributable to significant body shape variations between cartoon characters and real person characters.

### Ablation Studies

We conducted ablation studies to demonstrate the significance of pairwise interaction feature selection and the implementation of DMI similarity loss. Initially, we evaluated the performance of a model trained exclusively with the nearest \(L\) sensor pairs, denoted as Ours\({}_{cls}\), and another model trained solely with the farthest \(L\) sensor pairs, referred to as Ours\({}_{far}\). As indicated in Table 1 and Figure 5, Ours\({}_{far}\) compromises contact semantics and leads to significant interpenetration, while Ours\({}_{cls}\) also exhibits inferior performance. This outcome suggests that proximal sensor pairs are essential for minimizing interpenetration and preserving contact, whereas distal pairs provide insights into the non-contact spatial relationships among body parts. Further, we investigated the effect of incorporating a distance matrix loss, as proposed by Zhang et al. , on our sensor pairs, designated as Ours\({}_{dm}\). The results imply that the distance matrix loss fails to yield meaningful supervisory signals, likely because distance is non-directional and insufficient to discern the relative spatial positions among numerous sensors.

### User study

We conducted a user study to assess the performance of our MeshRet model in comparison with the Copy strategy, PMnet , SAN , and R\({}^{2}\)ET . Fifteen sets of motion videos were presented to participants, each consisting of one source skinned motion and five anonymized skinned results. Participants were requested to rate their preferences based on three criteria: semantic preservation, contact accuracy, and overall quality. Users were recruited from Amazon Mechanical Turk ,

  
**Methods** & **Semantics Preservation** & **Contact Accuracy** & **Overall Quality** \\  Copy & 20.7\% v.s. **79.3\%(Ours)** & 22.7\% v.s. **77.3\%(Ours)** & 18.7\% v.s. **81.3\%(Ours)** \\ PMnet  & 2.7\% v.s. **97.3\%(Ours)** & 5.3\% v.s. **94.7\%(Ours)** & 1.3\% v.s. **98.7\%(Ours)** \\ SAN  & 9.3\% v.s. **90.7\%(Ours)** & 15.3\% v.s. **84.7\%(Ours)** & 7.3\% v.s. **92.7\%(Ours)** \\ R\({}^{2}\)ET  & 14.6\% v.s. **85.4\%(Ours)** & 16.0\% v.s. **84.0\%(Ours)** & 13.3\% v.s. **86.7\%(Ours)** \\   

Table 2: Human preferences between our method and baselines.

Figure 5: Qualitative comparison of ablation studies. A red circle highlights areas of interpenetration, while a red rectangle identifies errors in non-contact semantics.

resulting in a total of 600 comparative evaluations. As indicated in Table 2, approximately 81% of the comparisons favored our results. Details can be found in Appendix D

## 5 Conclusion

We introduce a novel framework for geometric interaction-aware motion retargeting, named _MeshRet_. This framework explicitly models the dense geometric interactions among various body parts by first establishing a dense mesh correspondence between characters using semantically consistent sensors. We then develop a unique spatio-temporal representation, termed the DMI field, which adeptly captures both contact and non-contact interactions between body geometries. By aligning this DMI field, _MeshRet_ achieves detailed contact preservation and seamless geometric interaction. Performance evaluations using the Mixamo dataset and our newly compiled _ScanRet_ dataset confirm that _MeshRet_ offers state-of-the-art results.

LimitationsThe primary limitation of _MeshRet_ is its dependence on inputs with clean contact; motion clips exhibiting severe interpenetration yield poor outcomes. Consequently, it is unable to process noisy inputs effectively. Refer to Figure 12 and Figure 13 for failure cases under noisy inputs. Future efforts will focus on enhancing its robustness to noisy data. Additionally, SCS extraction can be compromised by noisy meshes, particularly those with complex clothing. A potential solution is to employ a Laplacian-smoothed proxy mesh for SCS extraction. Lastly, the method cannot handle characters with missing limbs.