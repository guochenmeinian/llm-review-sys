# Separable Physics-Informed Neural Networks

Junwoo Cho\({}^{1}\)1

Seok-Bae Yun\({}^{2}\)

&Seungtae Nam\({}^{1}\)1

Youngjoon Hong\({}^{3}\)

&Hyunmo Yang\({}^{1}\)

&Sook-Bae Yun\({}^{2}\)

Youngjoon Hong\({}^{3}\)

&Eunbyung Park\({}^{1,4}\)2

\({}^{1}\)Department of Artificial Intelligence, Sungkyunkwan University

\({}^{2}\)Department of Mathematics, Sungkyunkwan University

\({}^{3}\)Department of Mathematical Sciences, KAIST

\({}^{4}\)Department of Electrical and Computer Engineering, Sungkyunkwan University

Equal contribution.Corresponding author.

###### Abstract

Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, _separable PINN (SPINN)_, operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (\(>10^{7}\)) on a single commodity GPU. The experimental results show drastically reduced computational costs (\(62\) in wall-clock time, \(1,394\) in FLOPs given the same number of collocation points) in multi-dimensional PDEs while achieving better accuracy. Furthermore, we present that SPINN can solve a chaotic (2+1)-d Navier-Stokes equation significantly faster than the best-performing prior method (9 minutes vs 10 hours in a single GPU), maintaining accuracy. Finally, we showcase that SPINN can accurately obtain the solution of a highly nonlinear and multi-dimensional PDE, a (3+1)-d Navier-Stokes equation. For visualized results and code, please see https://jwcho5576.github.io/spinn.github.io/.

## 1 Introduction

Solving partial differential equations (PDEs) has been a long-standing problem in various science and engineering domains. Finding analytic solutions requires in-depth expertise and is often infeasible in many useful and important PDEs . Hence, numerical approximation methods to solutions have been extensively studied , e.g., spectral methods , finite volume methods (FVM) , finite difference method (FDM) , and finite element methods (FEM) . While successful, classical methods have several limitations, such as expensive computational costs, requiring sophisticated techniques to support multi-physics and multi-scale systems, and the curse of dimensionality in high dimensional PDEs.

With the vast increases in computational power and methodological advances in machine learning, researchers have explored data-driven and learning-based methods . Among the promising methods, physics-informed neural networks (PINNs) have recently emerged as new data-driven PDE solvers for both forward and inverse problems . PINNs employ neural networks andgradient-based optimization algorithms to represent and obtain the solutions, leveraging automatic differentiation to enforce the physical constraints of underlying PDE. It has enjoyed great success in various forward and inverse problems thanks to its numerous benefits, such as flexibility in handling a wide range of forward and inverse problems, mesh-free solutions, and not requiring observational data, hence, unsupervised training.

Despite its advantages and promising results, there is a fundamental limitation of training PINN to solve multi-dimensional PDEs and approximate very complex solution functions. It primarily stems from using coordinate-based MLP architectures to represent the solution function, which takes input coordinates and outputs corresponding solution quantities. For each training point, computing PDE residual loss involves multiple forward and backward propagations, and the number of training points (collocation points) required to solve multi-dimensional PDEs and obtain more accurate solutions grows substantially. The situation deteriorates as the dimensionality of the PDE or the solution's complexity increases.

Recent studies have presented empirical evidence showing that choosing a larger batch size (i.e., a large number of collocation points) in training PINNs leads to enhanced precision . Furthermore, more collocation points also accelerate the convergence speed due to the scaling characteristic between the batch size and the learning rate (the larger the batch size, the higher the learning rate) . As long as the computational resources allow, we have the flexibility to utilize a substantial number of training points in PINNs since they can be continuously sampled from the input domain in an unsupervised training manner.

We propose a novel PINN architecture, _separable PINN (SPINN)_, which utilizes forward-mode* automatic differentiation (AD) to enable a large number of collocation points (\(>10^{7}\) in a single GPU), reducing the computational cost of solving multi-dimensional PDEs. Instead of feeding every multi-dimensional coordinate into a single MLP, we use separated sub-networks, in which each sub-network takes independent one-dimensional coordinates as input. The final output is generated by an aggregation module such as simple outer product and element-wise summation where the predicted solution can be interpreted by low-rank tensor approximation  (Fig. 4b). The suggested architecture obviates the need to query every multi-dimensional coordinate input pair, exponentially reducing the number of network propagations to generate a solution, \((N^{d})(Nd)\), where \(N\) is the resolution of the solution for each dimension, and \(d\) is the dimension of the system.

Footnote *: a.k.a. forward accumulation mode or tangent linear mode.

We have conducted comprehensive experiments on the representative PDEs to show the effectiveness of the suggested method. The experimental results demonstrate that the training runtime of the conventional PINN increase linearly with the number of collocation points, while the proposed model shows logarithmic growths. This allows SPINN to accommodate orders of magnitude larger number of collocation points _in a single batch_ during training. We also show that given the same number of training points, SPINN improves wall-clock training time up to by \(62\) on commodity GPUs and FLOPs up to by \(1,394\) while achieving better accuracy. Furthermore, with large-scale collocation points, SPINN can solve a turbulent Navier-Stokes equation much faster than the state-of-the-art PINN method  (9 minutes vs 10 hours in a single GPU) without bells and whistles, such as causal

Figure 1: Training speed (w/ a single GPU) of our model compared to the causal PINN  in (2+1)-d Navier-Stokes equation of time interval [0, 0.1].

inductive bias in the loss function (Fig. 1). Our experimental results of the Navier-Stokes equation show that SPINN can solve highly nonlinear PDEs and is sufficiently expressive to represent complex functions. This is further supported by the provided theoretical result, potentiating the use of our method for more challenging and various PDEs.

## 2 Related Works

Physics-informed neural networks.Physics-Informed Neural Networks (PINNs)  have received great attention as a promising learning-based PDE solver. Given the underlying PDE and initial, boundary conditions embedded in a loss function, a coordinate-based neural network is trained to approximate the desired solution. Since its inception, many techniques have been studied to improve training PINNs for more challenging PDE systems [26; 43; 44], or to accelerate the training speed [20; 37]. Our method is orthogonal to most of the previously suggested techniques above and improves PINNs' training from a computational perspective.

The effect of collocation points.The residual loss of PINNs is calculated by Monte Carlo integration, not an exact definite integral. Therefore, it inherits a core property of Monte Carlo methods : the impact of the number of sampled points. The importance of sampling strategy and the number of collocation points in training PINNs has been highlighted by recent works [7; 23; 36; 43]. Especially, Sankaran et al.  empirically found that training with a large number of collocation points is unconditionally favorable for PINNs in terms of accuracy and convergence speed. Another line of research established a theoretical upper bound of PINN's statistical error with respect to the number of collocation points . It showed that a larger number of collocation points are required to use a bigger network size for training. Our work builds on this evidence to bring PINNs out in more practical scenarios and overcome their limitation.

Derivative computations in scientific machine learning.Embedding physical constraints in the loss function is widely used in scientific machine learning, and computing the derivatives is an essential process for this formulation. Several works employed numerical differentiation [33; 35; 42], or hybrid approach [5; 37; 46] with AD for the calculation, since numerical methods such as finite differences do not need to back-propagate through the network. However, they are still burdened by a computational complexity of \((N^{d})\), thereby limiting them to handle large-scale collocation points or meshes. Furthermore, numerical differentiation has truncation errors depending on the step size. Employing Taylor-mode AD  in training PINNs was introduced by causal PINN  to handle high-order PDEs such as Kuramoto-Sivashinsky equation . To the best of our knowledge, the proposed method is the first approach to leverage forward-mode AD in training PINNs, which is fully applicable to both time-dependent and independent PDEs and does not incur any truncation errors.

Multiple MLP networks.Employing multiple MLPs for PINNs has been introduced by several works to utilize parallelized training [19; 21; 32]. They share the same concept of dividing the entire spatio-temporal domain and training multiple individual MLPs on each sub-domain. Although these methods showed promising results, they still suffer from the fundamental problem of heavy computation as the number of collocation points increases. While these methods decompose input domains, and each small MLP is used to cover a particular sub-domain, we decompose input dimensions and solve PDEs over the entire domain cooperated by all separated MLPs. In terms of the model architecture and function representation, our work is also related to NAM , Haghighat et al. , and CoordX . NAM  suggested separated network architectures and inputs, but only for achieving the interpretability of the model's prediction in multi-task learning. Haghighat et al.  used multiple MLPs to individually predict each component of the _output_ vector. CoordX's  primary purpose is to reconstruct natural signals, such as images or 3D shapes. Hence, they are not motivated to improve the efficiency of computing higher-order gradients. In addition, they had to use additional layers after the feature merging step, which made their model more computationally expensive than ours. Furthermore, in neural fields , there is an explicit limitation in the number of ground truth data points (e.g., the number of pixels in an image). Therefore, CoordX cannot fully maximize the advantage of enabling a large number of input coordinates. We focus on solving PDEs and carefully devise the architecture to exploit forward-mode AD to efficiently compute PDE residual losses.

## 3 Preliminaries: Forward/Reverse-mode AD

For the completeness of this paper, we start by briefly introducing the two types of AD and how Jacobian matrices are evaluated. For clarity, we will follow the notations used in  and . Suppose our function \(f:^{n}^{m}\) is a two-layer MLP with 'tanh' activation. The left-hand side of Tab. 1 demonstrates a single forward trace of \(f\). To obtain a \(m n\) Jacobian matrix \(_{f}\) in forward-mode, we compute the Jacobian-vector product (JVP),

\[_{f}r=}{ x_{1}}&& }{ x_{n}}\\ &&\\ }{ x_{1}}&&}{ x _{n}}}{ x_{1}}\\ \\ }{ x_{1}},\] (1)

for \(i\{1,,n\}\). The forward-mode AD is a one-phase process: while tracing primals (intermediate values) \(v_{k}\), it continues to evaluate and accumulate their tangents \(v_{k}= v_{k}/ x_{i}\) (the middle column of Tab. 1). This is equivalent to decomposing one large JVP into a series of JVPs by the chain rule and computing them from right to left. A run of JVP with the initial tangents \(v_{0}\) as the first column vector of an identity matrix \(I_{n}\) gives the first column of \(_{f}\). Thus, the full Jacobian can be obtained in \(n\) forward passes.

On the other hand, the reverse-mode AD computes vector-Jacobian product (VJP):

\[r^{}_{f}=}{ y_{1}}& &}{ y_{m}} { y_{1}}{ x_{1}}&&}{ x_{n}} \\ &&\\ }{ x_{1}}&&}{ x _{n}},\] (2)

for \(j\{1,,m\}\), which is the reverse-order operation of JVP. This is a two-phase process. The first phase corresponds to forward propagation, storing all the primals, \(v_{k}\), and recording the elementary operations in the computational graph. In the second phase, the derivatives are computed by accumulating the adjoints \(_{k}= y_{j}/ v_{k}\) (the right-hand side of Tab. 1). Since VJP builds one row of a Jacobian at a time, it takes \(m\) evaluations to obtain the full Jacobian. To sum up, the forward-mode is more efficient for a tall Jacobian (\(m>n\)), while the reverse-mode is better suited for a wide Jacobian (\(n>m\)). Fig. 2 shows an illustrative example and please refer to Baydin et al.  for more details.

  Forward primal trace & Forward tangent trace & Backward adjoint trace \\ \(v_{0}=x\) & \(_{0}}{v_{1}}=\) & \(_{0}}{v_{0}}=_{1}\) & \(_{0}}{v_{0}}=_{1}\) & \(_{0}}{v_{0}}=_{1} W_{1}\) \\ \(v_{2}=^{}(v_{1})_{1}\) & \(_{1}=_{2}\) & \(_{0}}{v_{1}}=_{2}\) & \(_{0}}{v_{1}}=_{2}\) & \(_{0}}{v_{1}}=_{2}\) & \(_{0}}{v_{1}}=_{2}\) \\ \(v_{3}=W_{2} v_{2}\) & \(_{3}}{W_{2}}_{2}\) & \(_{2}}{v_{3}}=_{3}\) & \(_{3}}{_{2}}=_{3}\) & \(W_{2}\) \\  \(y=v_{3}\) & \(=v_{3}\) & \(=v_{3}\) & \(=\) \\  

Table 1: An example of forward and reverse-mode AD in a two-layers tanh MLP. Here \(v_{0}\) denotes the input variable, \(v_{k}\) the primals, \(_{k}\) the tangents, \(_{k}\) the adjoints, and \(W_{1},W_{2}\) the weight matrices. Biases are omitted for brevity.

Figure 3: An illustrative example of separated approach vs non-separated approach when \(f^{()}:^{d}\) (an example case of \(N=3\), \(d=3\) is shown above). The number of JVP (forward-mode) evaluations (propagations) of computing the Jacobian for separated approach (a) is \(Nd\), while the number of VJP (reverse-mode) evaluations for non-separated approach (b) is \(N^{d}\).

Figure 2: Simple neural networks with different input and output dimensions. To compute \(\), (a) requires one forward pass using forward-mode AD or three backward passes using reverse-mode AD, (b) requires three forward passes using forward-mode AD or one backward pass using reverse-mode AD or one backward pass using reverse-mode AD.

## 4 Separable PINN

### Forward-Mode AD with Separated Functions

We demonstrate that leveraging forward-mode AD and separating the function into multiple functions with respect to input axes can significantly reduce the cost of computing Jacobian. In the proposed separated approach (Fig. 2(a)), we first sample \(N\) one-dimensional coordinates on each of \(d\) axes, which makes a total of \(Nd\) batch size. Next, these coordinates are fed into \(d\) individual functions. Let \(f\) be a function which takes coordinate as input to produce feature representation and we denote the number of operations of \(f\) as \((f)\). Then, a feature merging function \(h\) is used to construct the solution of the entire \(N^{d}\) discretized points. The amount of computations for AD is known to be 2\(\)3 times more expensive than the forward propagation [2; 13]. According to Fig. 3 and with the scale constants \(c_{f},c_{h}\) we can approximate the total number of operations to compute the Jacobian matrix of the proposed separated approach (Fig. 2(a)).

\[_{}=Ndc_{f}(f)+N^{d}c_{h}(h).\] (3)

For a non-separated approach (Fig. 2(b)),

\[_{}=N^{d}c_{f}(f),\] (4)

If we can make \((h)\) sufficiently small, then the ratio \(_{}/_{}\) becomes \(} 1\), quickly converging to 0 as \(d\) and \(N\) increases. Our separated approach has linear complexity with respect to \(N\) in network propagations, implying it can obtain more accurate solutions (high-resolution) efficiently.

### Network Architecture

Fig. 3(a) illustrates the overall SPINN architecture, parameterizing multiple separated functions with neural networks. SPINN consists of \(d\) body-networks (MLPs), each of which takes an individual 1-dimensional coordinate component as an input. Each body-network \(f^{(_{i})}:^{r}\) (parameterized by \(_{i}\)) is a vector-valued function which transforms the coordinates of \(i\)-th axis into a \(r\)-dimensional feature representation. The final prediction is computed by feature merging:

\[(x_{1},x_{2},,x_{d})=_{j=1}^{r}_{i=1}^{d}f_{j}^{(_ {i})}(x_{i})\] (5)

where \(:^{d}\) is the predicted solution function, \(x_{i}\) is a coordinate of \(i\)-th axis, and \(f_{j}^{(_{i})}\) denotes the \(j\)-th element of \(f^{(_{i})}\). We used 'tanh' activation function throughout the paper. As shown in Eq. 5, the feature merging operation is a simple product (\(\)) and summation (\(\)) which

Figure 4: (a) SPINN architecture in a 3-dimensional system. To solve a \(d\)-dimensional PDE, our model requires \(d\) body MLP networks, each of which takes individual scalar coordinate values as input and gives \(r\)-dimensional feature vector. The final output is obtained by element-wise product and summation. (b) Construction process of the entire discretized solution tensor when the inputs are given in batches. Each outer product between the column vectors \(F_{:,j,i}\) from the feature tensor \(F\) constructs a rank-1 tensor and summing all the \(r\) tensors gives a rank-\(r\) tensor. The output tensor of SPINN can be interpreted as a low-rank decomposed representation of a solution.

corresponds to the merging function \(h\) described in Eq. 3. Due to its simplicity, \(h\) operations are much cheaper than operations in MLP layers (i.e., \((h)(f)\) in Eq. 3). Note that SPINN can also approximate any \(m\)-dimensional vector functions \(:^{d}^{m}\) by using a larger output feature size (see section D.4 in the appendix for details).

The collocation points of our model and conventional PINNs have a distinct difference (Fig. 5). Both are _uniformly_ evaluated on a \(d\)-dimensional hypercube, but collocation points of SPINN form a lattice-like structure, which we call as _factorizable coordinates_. In SPINN, 1-dimensional input points from each axis are randomly sampled, and \(d\)-dimensional points are generated via the cartesian product of the point sets from each axis. On the other hand, non-factorizable coordinates are randomly sampled points without any structure. Factorizable coordinates with our separated MLP architecture enable us to evaluate functions on dense (\(N^{d}\)) collocation points with a small number (\(Nd\)) of input points.

In practice, the input coordinates are given in a batch during training and inference. Assume that \(N\) input coordinates (training points) are sampled from each axis. Note that the sampling resolutions for each axis need not be the same. The input coordinates \(X^{N d}\) is now a matrix. The batchified form of feature representation \(F^{N r d}\) and Eq. 5 now becomes

\[(X_{:,1},X_{:,2},,X_{:,d})=_{j=1}^{r}_{i=1}^{d}F_{:,j,i},\] (6)

where \(^{N N N}\) is the discretized solution tensor, \(\) denotes outer product, \(F_{:,:,i}^{N r}\) is an \(i\)-th frontal slice matrix of tensor \(F\), and \(F_{:,j,i}^{N}\) is the \(j\)-th column of the matrix \(F_{:,:,i}\). Fig. 3(b) shows an illustrative procedure of Eq. 6. Due to its structural input points and outer products between feature vectors, SPINN's solution approximation can be viewed as a low-rank tensor decomposition where the feature size \(r\) is the rank of the reconstructed tensor. Among many decomposition methods, SPINN corresponds to CP-decomposition , which approximates a tensor by finite summation of rank-1 tensors. While traditional methods use iterative methods such as alternating least-squares (ALS) or alternating slicewise diagonalization (ASD)  to directly fit the decomposed vectors, we train neural networks to learn the decomposed vector representation and approximate the solution functions in continuous input domains. This, in turn, allows for the calculation of derivatives with respect to arbitrary input coordinates.

### Gradient Computation of SPINN

In this section, we show that the number of JVP (forward-mode AD) evaluations for computing the full gradient of SPINN (\((x)\)) is \(Nd\), where \(N\) is the number of coordinates sampled from each axis and \(d\) is the input dimension. According to Eq. 5, the \(i\)-th element of \((x)\) is:

\[}{ x_{i}}=_{j=1}^{r}f_{j}^{(_{1})}(x_{1 })f_{j}^{(_{2})}(x_{2})^{(_{i})}(x_{i}) }{ x_{i}} f_{j}^{(_{d})}(x_{d}).\] (7)

Computing this derivative requires feature representations \(f_{j}^{(_{i})}\), which can be reused from the forward pass results computed beforehand. The entire \(r\) components of the feature derivatives \( f^{(_{i})}(x_{i})/ x_{i}:^{r}\) can be obtained by a single pass thanks to forward-mode AD. To obtain the full gradient \((x):^{d}^{d}\), we iterate the calculation of Eq. 7 over \(d\) times, switching the input axis \(i\). Also, since each iteration involves \(N\) training samples, the total number of JVP evaluations becomes \(Nd\), which is consistent with Eq. 3. Note that any \(p\)-th order derivative \(^{p}/ x_{i}^{p}\) can also be obtained in the same way.

Figure 5: An illustrative 2-dimensional example of (a) factorizable and (b) non-factorizable coordinates. (a) has a lattice-like structure, where SPINN can be evaluated on more dense collocation points with fewer input points. Conventional PINNs sample non-factorizable coordinates which do not have any structures.

### Universal Approximation Property

It is widely known that neural networks with sufficiently many hidden units have the expressive power of approximating any continuous functions [6; 18]. However, it is not straightforward that our suggested architecture enjoys the same capability. For the completeness of the paper, we provide a universal approximation property of the proposed method.

**Theorem 1**.: _(Proof in appendix) Let \(X,Y\) be compact subsets of \(^{d}\). Choose \(u L^{2}(X Y)\). Then, for arbitrary \(>0\), we can find a sufficiently large \(r>0\) and neural networks \(f_{j}\) and \(g_{j}\) such that_

\[\|u-_{j=1}^{r}f_{j}g_{j}\|_{L^{2}(X Y)}<.\] (8)

By repeatedly applying Theorem 1, we can show that SPINN can approximate any functions in \(L^{2}\) in the high-dimensional input space. An approximation property for a broader function space is a fruitful research area, and we leave it to future work. To support the expressive power of the suggested method, we empirically showed that SPINN can accurately approximate solution functions of various challenging PDEs, including diffusion, Helmholtz, Klein-Gordon, and Navier-Stokes equations.

## 5 Experiments

### Experimental setups

We compared SPINN against vanilla PINN  on 3-d (diffusion, Helmholtz, Klein-Gordon, and Navier-Stokes equation) and 4-d (Klein-Gordon and Navier-Stokes) PDE systems. Every experiment was run on a different number of collocation points, and we also applied the modified MLP introduced in  to both PINN and SPINN. For 3-d systems, the number of collocation points of \(64^{3}\) was the upper limit for the vanilla PINN (\(54^{3}\) for modified MLP) when we trained with a single NVIDIA RTX3090 GPU with 24GB of memory. However, the memory usage of our model was significantly smaller, enabling SPINN to use a larger number of collocation points (up to \(256^{3}\)) to get more accurate solutions. This is because SPINN stores a much smaller batch of tensors, which are the primals (\(v_{k}\) in Tab. 1) while building the computational graph. All reported error metrics are average relative \(L_{2}\) errors computed by \(\|-u\|^{2}/\|u\|^{2}\), where \(\) is the model prediction, and \(u\) is the reference solution. Every experiment is performed seven times (three times for (2+1)-d and five times for (3+1)-d Navier-Stokes, respectively) with different random seeds. More detailed experimental settings are provided in the appendix.

Figure 6: Overall results of (a) diffusion, (b) Helmholtz, (c) (2+1)-d Klein-Gordon, and (d) (3+1)-d Klein-Gordon experiments. It shows comparisons among PINN, PINN with modified MLP (PINN-mod), SPINN, and SPINN with modified MLP (SPINN-mod). For each experiment, the first to third columns show the relative error, runtime, and GPU memory versus different numbers of collocation points, respectively. The rightmost column shows the training curves of each model when the number of collocation points is \(64^{3}\) (\(54^{3}\) for PINN with modified MLP.) For (3+1)-d Klein-Gordon, the training curve is plotted when each model is trained with the number of collocation points of \(16^{4}\). The error and the training curves are averaged over 7 different runs and 70% confidence intervals are provided. Note that the y-axis scale of every plot is a log scale.

### Results

Fig. 6 shows the overall results of forward problems on three 3-d systems (diffusion, Helmholtz, and Klein-Gordon) and one 4-d system (Klein-Gordon). SPINN is significantly more computationally efficient than the baseline PINN in wall-clock run-time. For every PDE, SPINN with the modified MLP found the most accurate solution. Furthermore, when the number of collocation points grows exponentially, the memory usage and the actual run-time of SPINN increase almost linearly. We also confirmed that we can get more accurate solutions with more collocation points. This training characteristic of SPINN substantiates that our method is very effective for solving multi-dimensional PDEs. Furthermore, with the help of the method outlined in Griewank and Walther , we estimated the FLOPs for evaluating the derivatives. Compared to the baseline, SPINN requires \(1,394\) fewer operations to compute the forward pass, first and second order derivatives. Further details regarding the FLOPs estimation are provided in section C of the appendix. In the following sections, we give more detailed descriptions and analyses of each experiment.

Diffusion EquationWhen trained with \(128^{3}\) collocation points, SPINN with the modified MLP finds the most accurate solution. Furthermore, when trained with the same number of collocation points (\(64^{3}\)), SPINN-mod is \(52\) faster and \(29\) more memory efficient than the baseline with modified MLP. You can find the visualized solution in the appendix. Since we construct the solution relatively simple compared to other PDE experiments, baseline PINNs also find a comparatively good solution. However, our model finds a more accurate solution with a larger number of collocation points with minor computational overhead. The exact numerical values are provided in the appendix.

Helmholtz EquationThe results of the Helmholtz experiments are shown in Fig. 6. Due to stiffness in the gradient flow, conventional PINNs hinder finding accurate solutions. Therefore, a modified MLP and learning rate annealing algorithm is suggested to mitigate such phenomenon . We found that even PINNs with modified MLP fail to solve the equation when the solution is complex (contains high-frequency components). However, SPINN obtains one order of magnitude lower relative error without bells and whistles. This is because each body network of the SPINN learns an individual 1-dimensional function which mitigates the burden of representing the entire 3-dimensional function. This makes our model much easier to learn complex functions. Given the same number of collocation points (\(64^{3}\)), the training speed of our proposed model with modified MLP is \(62\) faster, and the memory usage is \(29\) smaller than the baseline with modified MLP. The exact numerical values are provided in the appendix.

Klein-Gordon EquationFig. 6 shows the results. Again, our method shows the best performance in terms of accuracy, runtime (\(62\)), and memory usage (\(29\)). Note that both Helmholtz and Klein-Gordon equations contain three second-order derivative terms, while the diffusion equation contains only two. Our results showed the largest differences in runtime and memory usage in Helmholtz and Klein-Gordon equations. This is because SPINN significantly reduces the AD computations with forward-mode AD, implying SPINN is very efficient for solving high-order PDEs.

We investigated the Klein-Gordon experiment further, extending it to a 4-d system by adding one more spatial axis. As shown in Fig. 6, baseline PINN can only process \(23^{4}\) (\(18^{4}\) for modified MLP) collocation points at once due to a high memory throughput. On the other hand, SPINN can exploit the number of collocation points of \(64^{4}\) (\(160\) larger than the baseline) to obtain a more precise solution. The exact numerical values are provided in the appendix.

(2+1)-d Navier-Stokes EquationWe constructed SPINN to predict the velocity field \(u\) and applied forward-mode AD to obtain the vorticity \(\). We used the same PDE setting used in causal PINN  and compared SPINN against their model. As shown in Tab. 2, our model finds a comparably accurate solution even without the causal loss function. Furthermore, since causal PINN had to iterate over four different tolerance values \(\) in the causal loss function, they had to run 3\(\)4 times more training epochs, depending on the stopping criterion. As a result, given the same number of collocation points, SPINN converges 60\(\) faster than causal PINN in terms of training runtime. The results of the Navier-Stokes equation ensure that SPINN can successfully solve a chaotic, highly nonlinear PDE. In Fig. 7, we

Figure 7: Relative error and training speed of (2+1)-d Navier-Stokes experiment vs. different ranks of SPINN.

demonstrate the effect of the rank \(r\) for solving the equation. We observed that the performance almost converges at the rank of 128, and increasing the rank does not slow down the training speed too much. Further details and visualization of the predicted vorticity map is shown in appendix section D.4.

(3+1)-d Navier-Stokes EquationWe proceeded to explore our model on (3+1)-d Navier-Stokes equation by devising a manufactured solution corresponding to the Taylor-Green vortex . It models a decaying vortex, widely used for testing Navier-Stokes numerical solvers . Since the vorticity of the (3+1)-d Navier-Stokes equation is a 3-d vector as opposed to the (2+1)-d system, the equation has three independent components resulting in a total of 33 derivative terms (see section D.5 in the appendix). Similar to the (2+1)-d experiment, the network's output is the velocity field, and we obtained 3-d vorticity by forward-mode AD. Tab. 3 shows the relative error, runtime, and GPU memory. Trained with a single GPU, SPINN achieves a relative error of 1.9e-3 less than 30 minutes. Visualized velocity and vorticity vector fields are provided in the appendix.

## 6 Limitations and Future Works

Although we presented comprehensive experimental results to show the effectiveness of our model, there remain questions on solving more challenging and higher dimensional PDEs. Handling any geometric surface is one of the key advantages of PINNs. In the appendix, we presented a simple way to handle arbitrary domains with SPINN by testing on the Poisson equation in an L-shaped domain. Please see section E.3 for more details. Another method could be applying an additional operation after the SPINN's feature merging to map a rectangular mesh to an arbitrary physical mesh, inspired by PhyeGeNet  and Geo-FNO . Combining such techniques for handling arbitrary boundary conditions of complex geometries into ours is an exciting research direction, and we leave it to future work.

We also found that due to its architectural characteristic, SPINN tends to be better trained when the solutions align with the variable separation form (discretized into low-rank tensor). However, note that SPINN is still effective in solving equations where the solution is not a low-rank tensor, such as our examples on diffusion and (2+1)-d Navier-Stokes equation. Please see section D in the appendix for additional experiments and information of which example aligns with the variable separation form or not. Applying SPINN to higher dimensional PDEs, such as the BGK equation, would also have a tremendous practical impact. Lastly, we are still far from the theoretical speed-up (wall-clock runtime vs. FLOPs), which is further discussed in appendix section C. We expect to additionally reduce runtime by optimizing hardware/software, e.g., using customized CUDA kernels.

   model &  &  &  \\  \(N_{c}\) & \(2^{12}\) & \(2^{15}\) & \(2^{12}\) & \(2^{15}\) & \(2^{15}\) & \(2^{15}\) & \(2^{18}\) & \(2^{21}\) \\  relative \(L_{2}\) error & 0.0694 & 0.0581 & 0.0578 & 0.0401 & & & & \\ \( 0.0049\) & \( 0.0135\) & \( 0.0117\) & \( 0.0084\) & 0.0353\({}^{}\) & 0.0780 & 0.0363 & 0.0355 \\ runtime (hh:mm) & 03:20 & 07:52 & 10:09 & 23:03 & - & 00:07 & 00:09 & 00:14 \\ memory (MB) & 5,198 & 17,046 & 5,200 & 17,132 & - & 764 & 892 & 1,276 \\   

Table 2: The numerical result of (2+1)-d Navier-Stokes equation compared against PINN with modified MLP (PINN+mod) and causal PINN. \(N_{c}\) is the number of collocation points. The fourth row shows the training runtime of a single time window in time-marching training. All relative \(L_{2}\) errors are averaged over 3 runs.

\({}^{}\) This is the error (the single run result) reported by the causal PINN paper. Since the causal PINN is sensitive to the choice of parameter initialization, we also reported average error obtained from different random seeds by running their official code.

   model & \(N_{c}\) &  relative \\ \(L_{2}\) error \\  &  runtime \\ (mins) \\  & 
 memory \\ (MB) \\  \\   & \(8^{4}\) & 0.0090 & 15.33 & 768 \\  & \(16^{4}\) & 0.0041 & 16.83 & 1,192 \\  & \(32^{4}\) & 0.0019 & 26.73 & 2,946 \\   

Table 3: The numerical result of (3+1)-d Navier-Stokes equation. \(N_{c}\) is the number of collocation points on the entire domain. The relative errors are obtained by comparing the vorticity vector field and averaged over five different runs.

Conclusion

We showed a simple yet powerful method to employ large-scale collocation points for PINNs training, leveraging forward-mode AD with the separated MLPs. Experimental results demonstrated that our method significantly reduces both spatial and computational complexity while achieving better accuracy on various three and four-dimensional PDEs. To our knowledge, it is the first attempt to exploit the power of forward-mode AD in PINNs. We believe this work opens up a new direction to rethink the architectural design of neural networks in many scientific applications.