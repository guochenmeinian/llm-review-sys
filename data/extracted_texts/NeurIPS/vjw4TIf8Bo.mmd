# PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition

Jinghui Lu 1, Ziwei Yang 1, Yanjie Wang 1, Xuejing Liu 2, Brian Mac Namee 3, Can Huang 1

Equal contribution. Corresponding authors.

###### Abstract

In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce **P**arallel **D**ecoding in **LLM** for **NER** (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER accelerates decoding by simultaneously generating all mentions at once, _i.e._, a label-mention pair per sequence. This results in shorter sequences and faster inference. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Concurrently, it maintains the prediction quality as evidenced by the micro F-score that is on par with the state-of-the-art approaches under both zero-shot and supervised setting. All resources are available at https://github.com/GeorgeLuImmortal/PaDeLLM_NER.

## 1 Introduction

Named Entity Recognition (NER), a fundamental task in Natural Language Processing (NLP), aims to extract structured information from unstructured text data. This includes identifying and categorizing key elements such as Organization, Geopolitical Entity and so on (referred to as _"labels"_) in inputs, and pairing them with relevant text spans extracted from the text (termed _"mentions"_). Conventionally, NER tasks are carried out through an extractive paradigm that entails token-level classification and the subsequent extraction of identified tokens .

Recent advancements in Large Language Models (LLMs)  have revolutionized numerous foundational tasks in NLP, including NER tasks , through the adoption of a generative paradigm. This paradigm involves instruction-tuning a sequence-to-sequence (seq2seq) model. The model takes a sequence of unstructured text as input and produces a sequence of structured label-mention pairs as output. Generally, the output structured string should be formatted to meet two criteria: (1) it should have a clear and straightforward structure that facilitates post-processing for label and mention extraction, and (2) it needs to be generated fluidly and efficiently from the perspective of language models .

In Table 1, we list two typically used autoregressive output formats found in the literature : (1) accommodate original input text to contain label information, which is referred to as _"augmented language"_; (2) directly using a customized, easily-parsed structured format to output all labels and mentions, which is called _"structured annotation"_. These formats present certain challenges. For example, _augmented language_ necessitates duplicating all original input text, thereby increasing output length and resulting in inference inefficiency. While _structure annotation_ avoids replicating the entire input, it produces all labels and mentions in an autoregressive manner. This implies that each subsequently generated pair depends on its preceding pairs, and when the number of label-mention pairs is large, it will lead to longer sequences. As demonstrated in Chen et al. , Ning et al. , high latency in LLMs mainly stems from lengthy sequence generation, we believe that by reducing the length of sequence, a more efficient inference scheme can be provided for NER tasks.

In light of this, we propose _Parallel Decoding in LLM for NER (PaDeLLM-NER)_, a novel approach to accelerate the inference of NER tasks for LLMs. PaDeLLM-NER empowers the model with the capability to predict a single label-mention pair within a single sequence, subsequently aggregating all sequences to generate the final NER outcome. Specifically, in the training phase, we reconstruct the instruction tuning tasks, enabling LLMs to predict the count of mentions for a specific label and to identify the \(n^{th}\) mention within the entire input for that label (Figure 1). In the inference phase, LLMs first predict the number of mentions for all labels, then predict all label-mention pairs in parallel (Figure 2). Finally, results from all sequences are aggregated and duplicate mentions across labels are eliminated based on prediction probability. This approach results in a more efficient inference method, producing shorter sequences and enabling parallel decoding label-mention pairs in batches.

Comprehensive experiments have been conducted, demonstrating that PaDeLLM-NER effectively reduces the number of tokens produced in each sequence, thereby decreasing inference latency. Additionally, it maintains or even enhances prediction quality in both flat and nested NER for English and Chinese languages, compared to existing methods in the literature under both zero-shot and supervised setting. To conclude, our contributions are as follows:

* We present PaDeLLM-NER, a novel approach tailored for NER using LLMs. This approach can predict all label-mention pairs in parallel, effectively reducing inference latency.
* Extensive experiments have been conducted, revealing that PaDeLLM-NER significantly improves inference efficiency. By completely decoupling the generation of label-mention pairs, the average sequence length is reduced to around 13% of that produced by conventional autoregressive methods. Correspondingly, the inference speed is 1.76 to 10.22 times faster than these previous approaches.
* Comprehensive experiments demonstrate that, in addition to its enhanced prediction speed, PaDeLLM-NER also maintains or surpasses the prediction quality of conventional autoregressive methods, on par with state-of-the-art (SOTA) performance on many NER datasets, including zero-shot as well as the supervised scenarios.

To the best of our knowledge, our technique stands as a pioneering approach in accelerating NER inference in LLMs by parallel decoding all label-mention pairs. This unique characteristic makes it complementary to other inference acceleration methods such as LLM.int8()  and speculative sampling . Thus, it can be efficiently integrated with these methods.

  
**Variant** & **Input Unstructured Text** & **Output Structured Label-mention String** \\  _Augmented Language_ & Japan, co-hosts of the World Cup in 2002 and ranked 20th in the world by FIFA, are by [FIFA] ORG, are favourites to regain their title here. & [Japan |LOC], co-hosts of the [World Cup | MISC] in 2002 and ranked 20th in the world by [FIFA] ORG, are favourites to regain their title here. & [Japa |LOC], co-hosts of the [World Cup | MISC] in 2002 and ranked 20th in the world by [FIFA] ORG, are favourites to regain their title here.

## 2 Related Work

### Generative Models for NER

Before the era of LLMs, most research approached NER as a sequence labeling task, where each token is assigned a pre-defined tag (_e.g._, BIO scheme). In this line of work, usually pre-trained transformer-based language models [1; 2] is combined with a tailored prediction head to perform a token-level classification, followed by the extraction of identified tokens.

Encouraged by the success of unifying multiple NLP tasks into a single seq2seq paradigm [24; 25], especially with the evolution of LLMs [10; 13; 26; 27], the trend of applying seq2seq models to NER tasks is gaining momentum , with both inputs and outputs being represented as sequences of text [3; 4; 5; 6; 7]. Recently, the focus of work on NER using LLMs has shifted towards zero-shot [29; 30] or few-shot learning [4; 18; 31; 32], utilizing in-context learning [18; 32], self-consistency [29; 33] or learning programming [30; 34].

Unlike previous studies emphasizing few-shot performance with training-free prompt learning, our work focus on a fully supervised setting. More importantly, our primary objective is to speed up NER inference.

### Inference Speedup in LLMs

Modern LLMs employ a sequential decoding strategy for token generation, which poses a significant challenge in terms of parallelization, especially as model size and sequence length increase . There is plenty of work in the literature to address this challenge [35; 36; 37; 38]. One line of work falls into training-free category such as introducing extra modules for speculative sampling [22; 23]. Another approaches explore modifying model architecture to accelerate inference, such as exiting at earlier layer [39; 40], or designing entirely different training and inference mechanisms [41; 42; 43]. Different from previous works, we focus on exploring the inference speedup in LLMs with a focus on the NER task without the change of model architecture or introducing extra modules.

## 3 Method

In this section, we delve into the details of PaDeLLM-NER. First, we focus on reframing the instruction tuning tasks as outlined in Section 3.1. Second, we explore the two-step inference process, detailed in Section 3.2. Finally, we discuss the aggregation of results and the technique for

Figure 1: PaDeLLM-NER training paradigm: texts within frames of the same color represents one training example, where texts inside the solid-line frame are the input, and those inside the dashed-line frame are the output. _Italic_ texts are prompt templates. The _“entity type”_ signifies the label being predicted. The _“<num>”_ indicates count of mentions for that label, and _“<mention n>”_ refers to the \(n^{th}\) mention of a label in the input.

eliminating duplicate mentions across labels, which is elaborated in Section 3.3. An illustration of PaDeLLM-NER is shown in Figure 1 and Figure 2.

### Reframing of Instruction Tuning

Illustration of the reframing is presented in Figure 1. As an example, we use a case from the _CoNLL2003_ dataset including four labels: person (PER), miscellaneous (MISC), location (LOC), and organization (ORG). The specifics of the input text and the corresponding ground truth are provided in the second row of Table 1.

During reformulation, a single unstructured text containing all label-mention pairs is split into several sequences. Each new sequence's output includes the count of mentions for a specified label (denoted as _"entity type"_), followed by the \(n^{th}\) mention of that label (denoted as _"<mention n>"_). Note that the count of mentions and their respective indices are represented using corresponding digit tokens from the LLM's vocabulary. Specifically, if there are no mentions, the model is trained to immediately predict the _"<eos>"_ token, bypassing the need to predict mentions.

Therefore, in this example, one original training data is transformed into five new training data entries. These include two for predicting _"LOC"_ (with 2 mentions), one for predicting _"MISC"_ (with 1 mention), one for predicting _"PER"_ (with 1 mention), and one for predicting _"ORG"_ (with 0 mentions, directly predicting _"<eos>"_). Moreover, the number of mentions for each label and the text corresponding to each mention index can be easily obtained from the original ground truth, meaning that the number of new examples depends on the ground truth of that particular example.

With the newly reformulated training examples, we then apply the standard instruction tuning procedure. The model takes a sequence of text \(t_{1},t_{2},,t_{T}\) consisting of input unstructured text and output structured label-mention pair. The optimization objective is cross-entropy loss \(\) which can be defined as follows:

\[=-_{i=1}^{T} P(t_{i} t_{1},t_{2},, t_{i-1})\] (1)

where \(P(t_{i} t_{1},t_{2},,t_{i-1})\) represents the probability of \(i^{th}\) token \(t_{i}\) given the sequence of preceding tokens \(t_{1},t_{2},,t_{i-1}\), as predicted by the model. Note that loss calculation begins from the number of mention tokens (_i.e._, texts enclosed by dashed-line frames). Theoretically, loss from text spans such as _"<mention n>"_ could be ignored during this calculation, since they simply prompt the mention's order, which does not necessarily need to be generated by the model. However, our

Figure 2: PaDeLLM-NER inference paradigm: texts enclosed in frames with identical colors indicate sequences of the same label. Specifically, the texts within solid-lined frames represent the added templates, while those within dashed-lined frames denote the prediction. In Step 1, the model predicts the number of mentions for all labels while in Step 2, it predicts the mentions. By aggregating mentions and labels from all sequences, the final NER results are obtained. Duplicate mentions appearing in different labels are resolved using prediction probabilities.

ablation studies show that ignoring these texts has negligible impact on model performance, a point further discussed in Appendix A. Therefore, we adhere to the standard instruction tuning procedure.

This reformulation allows the model to focus one label-mention pair at a time, shortening the generated length per sequence. More details are shown in Appendix C.

### Inference of Label-Mention Pairs

Given a trained LLM, we propose a two-step inference approach: firstly, to predict the number of mentions for a specific label based on the prompt; and secondly, given the label and provided index to precisely identify the corresponding mention.

Figure 2 shows the overview of PaDeLLM-NER inference. In Step 1, the model predicts the total number of mentions for each label in the input, based on the label prompt. A separate token "\(\)" signals the completion of this count prediction. If no mentions of the given label exist, the model generates an "\(<\)_eos\(>\)"_ token, skipping Step 2 for that label. In Step 2, following adding the predicted mention count to the input, mention indexes templates are appended. Formally, if the predicted number of mention is \(m\), then _"\(<\)mention n\(>\)"_, indicating the \(n^{th}\) mention of the specified label, is appended for each \(n\) within the set \(\{1,2,3,,m\}\) and \(n\) is an integer. Subsequently, the corresponding mention is generated by the model conditioned on preceding tokens. Note that the decoding of all label-mention pairs occurs in parallel, allowing for their simultaneous generation. Additionally, to justify the efficacy of the proposed two-step inference approach, we also implement a one-step parallel decoding method. In this approach, multiple mentions of the same label are predicted in a single sequence and compared to the two-step method in a preliminary experiment. Further details are provided in the Appendix A.

In practice, if there are sufficient GPU resources, the inference for the number of mentions for each label, as well as the subsequent inference for the mention text spans, can be allocating on separate GPUs. If GPU resources are limited, the inference can also be deployed on a single GPU using batch inference, facilitating parallel decoding. Using Figure 2 as an example, in Step 1, the batch size is four, as there are four labels in the dataset. In Step 2, the batch size is five, reflecting the five label-mention pairs determined in Step 1 (_i.e._, 1 in _"PER"_, 2 in _"MISC"_, 2 in _"LOC"_). This parallel decoding strategy is effective in reducing inference latency, especially in scenarios where inputs are received in a streaming manner.

### Removal of Duplicate Mentions

Unlike autoregressive decoding, where subsequent label-mention pairs can attend preceding ones, PaDeLLM-NER generates each label-mention pair independently. This inference strategy means that the model might generate mentions erroneously repeated in multiple labels. As exemplified in Figure 2, the model correctly predicts the first mention of _"LOC"_ as _"Italy"_, but it also incorrectly predicts the second mention of _"MISC"_ as _"Italy"_.

To address the issue of duplicate mentions, we suggest employing prediction probability to remove repeated mentions. Specifically, we calculate the prediction probability for each instance of the mention. This is done using the formula: \(P=_{i=b}^{e}P(t_{i}|t_{1},t_{2},,t_{i-1})\) where \(b\) represents the starting token index of the mention text, and \(e\) denotes the ending token index. Then, for a mention that appears in multiple labels, the mention instance with the highest probability will be preserved. As illustrated in Figure2, _"Italy"_ is categorized as _"MISC"_ with only a 0.61 probability, which is lower than that for _"LOC"_, resulting in its removal. In practice, the probability of each token can be calculated concurrently with token generation. Consequently, this method enables an efficient and accurate identification of duplicate mentions without incurring additional costs. The effectiveness of this de-duplication approach is further explored in Appendix A.

## 4 Experiments

In this section, we showcase the effectiveness of PaDeLLM-NER in terms of prediction quality and inference acceleration through experiments.

### Setup

DatasetsThe datasets used in our experiments include:

* **Zero-shot Datasets:** To align with the methodology proposed by , we train PaDeLLM using the Pile-NER dataset . This dataset comprises around 240,000 entities categorized into 13,000 distinct types, derived from the Pile Corpus . The passages in Pile-NER are enhanced through processing with ChatGPT, which facilitates the transparent generation of inherent entities. For assessing the model's zero-shot capabilities on previously unseen entity categories, following  we select two established benchmarks: CrossNER  and MIT .
* **Supervised Datasets:** we evaluate our method on supervised English and Chinese NER datasets. Following , English datasets include the general domain flat NER _CoNLL2003_, the nested NER _ACE2005_, and the biomedical nested NER _GENIA_. Following , Chinese datasets include four commonly used general domain flat NER benchmarks _Resume_, _Weibo_, _MSRA_ and _Ontonotes 4.0_ and two vertical industrial domain flat NER datasets _YouKt_ and _Ecommerce_. The statistics of all datasets are shown in Appendix B.

Training setupWe employ pre-trained version of Llama2-7b  and Baichuan2-7b  as base models for English and Chinese study respectively. Additional implementation details are in Appendix D.

Inference setupFor all generative models, we use greedy search with a beam size of \(1\), a maximum of \(512\) new tokens, and a temperature of \(1.0\). As described in Section 3.2, for PaDeLLM-NER, we adopt two inference settings: (1) each example is inferred on multiple GPUs to implement parallel decoding (_i.e._, each sequence is assigned on one GPU), termed as **PaDeLLM\({}_{}\)**; and (2) each example is inferred on a single GPU, employing batch decoding for parallel decoding, termed as **PaDeLLM\({}_{}\)**. Note that for PaDeLLM\({}_{}\), we sequentially predict each sequence of one example to simulate parallel decoding on multiple GPUs.

BaselinesThe baseline used in our experiments include:

* **Inference Latency Baselines:** As the primary focus of this work is on reducing inference latency in NER tasks using LLMs, we compare our method, PaDeLLM-NER, with traditional autoregressive approaches. As mentioned in Section 1, the main points of comparison are autoregressive structured output formats used in  and , referred to respectively as **AutoReg\({}_{}\)** and **AutoReg\({}_{}\)**, as these are the approaches very close to our system. We reimplemented all these methods for both English and Chinese datasets, utilizing the same pre-trained LLMs as in PaDeLLM-NER.
* **Zero-shot Baselines:** LLMs are known for their generalizability, therefore, following Ding et al. , we we also evaluate the zero-shot performance of PaDeLLM. Several most recent SOTA LLM-based approaches are selected as strong baselines as their great generalizability in zero-shot NER scenarios including **GoLLIE-7B**, **UniNER-7B**, **GLiNER-L**, **GNER-LLaMA-7B**.
* **Supervised Baselines:** We compare our approach with other recent SOTA supervised approaches, including **BINDER**, **Gollie**, and **DeepStruct** for English benchmarks, as well as **W\({}^{2}\)NER**, **NEZHA-BC**, and **SSCNN** for Chinese benchmarks, to show PaDeLLM-NER's efficacy in prediction quality.

More details on the re-implementation and model size of each method are provided in Appendix D.

EvaluationOur evaluation encompasses two dimensions: prediction quality and acceleration of NER inference. For assessing prediction quality, in line with Lu et al. , Wang et al. , we employ the micro F-score.

Following Ning et al. , we evaluate inference speed using latency (in milliseconds). We record the latency with the code: _start = time.time(); model.generate(); latency = time.time() - start_. In PaDeLLM-NER, we add the latency of mention counting and label-mention pair generation as the latency of each sequence. The final latency for the example is determined by the highest latency across sequences, as the user can only obtain the result of an example when the slowest sequence is generated. We conduct experiments three times and use the average result to alleviate the effect of randomness. We also report the average sequence length (tokenized) to clearly demonstrate the extent of sequence length reduction in Appendix E. Evaluations of all models were performed on the same NVIDIA A100 GPU.

### Main Results

Evaluation on inference latencyWe investigate how PaDeLLM-NER reduces the end-to-end latency compared to baseline methods. Table 2 presents the average latency for each method across all datasets. First, it's clear that both PaDeLLM\({}_{}\) and PaDeLLM\({}_{}\) significantly reduce inference latency when compared to baseline methods, as highlighted by the substantial reduction in mean latency. For example, the mean latency reduction achieved between PaDeLLM\({}_{}\) and AutoReg\({}_{}\) stands at an impressive \(791.55\) ms, underscoring the significant improvement.

To more intuitively quantify the latency reduction of PaDeLLM-NER, we break down its speedup across different datasets in comparison to baseline methods in Figure 3. The speedup is computed by dividing the latency of baselines by the latency of PaDeLLM-NER. We can observe that PaDeLLM

    &  &  & \\ 
**AutoReg** & **CoLLM03** & **ACE05** & **GENIA** & **Weibo** & **MSRA** & **Onto4** & **Resume** & **Youku** & **Ecom** & **Avg.** \\  AutoReg\({}_{}\) & 992.70 & 944.90 & 1,515.35 & 1,276.32 & 812.78 & 1,009.68 & 982.39 & 579.99 & 845.42 & 995.50 \\ AutoReg\({}_{}\) & 753.36 & 1,293.87 & 1,266.31 & 1,630.62 & 609.34 & 783.28 & 1,462.56 & 598.59 & 738.20 & 1,015.12 \\ 
**Ours** & & & & & & & & & \\  
**PaDeLLM\({}_{}\)** & **229.74** & **258.53** & **316.90** & **159.57** & **143.47** & **171.67** & **238.27** & **203.63** & **293.40** & **223.57** \\ 
**PaDeLLM\({}_{}\)** & 333.80 & 498.50 & 616.01 & 344.75 & 204.24 & 288.43 & 459.20 & 241.25 & 419.40 & 378.40 \\   

Table 2: Comparison of inference latency (in milliseconds) between PaDeLLM-NER and baseline methods. Underscored font is the second-best method, while a bold font is the best method, also applied to subsequent tables.

  
**Model** & **AI** & **Literature** & **Music** & **Politics** & **Science** & **Movie** & **Restaurant** & **Avg.** \\ 
**SOTA** & & & & & & & & \\  GoLLIE-7B & 59.1 & 62.7 & 67.8 & 57.2 & 67.0 & 63.0 & 43.4 & 60.02 \\ UniNER-7B & 53.5 & 59.7 & 65.0 & 60.8 & 61.1 & 42.4 & 31.7 & 53.45 \\ GLiNER-L & 57.2 & 64.4 & 69.6 & **72.6** & 62.6 & 57.2 & 42.9 & 60.92 \\ GNER-LLaMA-7B & **63.1** & **68.2** & **75.7** & 69.4 & **69.9** & **68.6** & **47.5** & **66.05** \\ 
**Ours** & & & & & & & & \\  
**PaDeLLM-NER-7B** & 60.7 & 66.1 & 67.6 & 68.1 & 64.4 & 61.3 & 43.6 & 61.68 \\   

Table 3: Comparison of prediction quality with recent SOTA models in zero-shot setting.

Figure 3: Speedup of PaDeLLM-NER compared to Autoregressive methods.

[MISSING_PAGE_FAIL:8]

## 5 Speedup Analysis

One concern noted is that batch inference does not speed up as much as inference distributed across multiple GPUs. This observation is consistent with our expectations and supported by Chen et al.  who found that batch inference in LLMs tends to be slower than single sequence inference under identical conditions, likely due to limitations in GPU memory bandwidth .

Transitioning from these performance considerations, it's noteworthy that PaDeLLM-NER is self-contained and can be seamlessly integrated with various generative architectures, including well-established decoder-only models [8; 9; 10; 11; 12; 13] and recent innovations like RWKV , as well as multi-modal LLMs [66; 67] for tasks like Key Information Extraction tasks , all without needing architectural changes or additional data/modules. Also, it could be incorporated with off-the-shelf LLMs such as ChatGPT  and Claude-2 through prompt engineering without the need for further training, an aspect we plan to explore in future research.

## 6 Data Contamination Concerns

Since we are using LLMs as our foundational models, trained on extensive datasets from various online sources [11; 13], there is a chance that the models may have encountered parts of our evaluation sets during their pre-training phase, albeit unintentionally. This could potentially affect our experimental results. However, the primary focus of our experiments is the comparison of our proposed method with baseline methods. Given that these methods employ the same LLM as the base model, data contamination is unlikely to significantly impact the results.

## 7 Limitations

One clear disadvantage of PaDeLLM-NER is the multiplication of training examples from one to \(m*n\), where \(m\) is the label count and \(n\) the mention count. Despite this, given that low latency is a major bottleneck in LLMs, trading longer training for lower latency is justifiable. Also, given the impressive generalization ability of LLMs, we believe that this method can be smoothly adapted to few-shot scenarios requiring less computation resources, which will be explored in future work.

Additionally, accurately counting the number of mentions remains a challenge for LLMs as discussed in Appendix F. This issue could be alleviated by implementing a specialized counting model dedicated to this task . Another drawback is that reformulating label-mention pairs loses location information, which hinders tasks like downstream editing. We will address this in future work. Additionally, the de-duplication mechanism is overly aggressive, potentially removing mentions that can appear under different labels--a common issue in real-world applications (see Appendix A for more details).

Finally, there are several instances of re-computation within the pipeline that can be optimized. Specifically, input texts are encoded multiple times throughout the process. During batch decoding, certain

  
**SOTA** & **Weibo** & **MSRA** & **Onto4** & **Resume** & **Youku** & **Ecom** & **Avg.** \\  NEZHA-BC  & - & - & - & - & - & **82.98** & - \\ SSCNN  & 71.81 & - & 82.99 & 96.40 & 86.10 & 81.80 & - \\ W\({}^{2}\)NER  & **72.32** & **96.10** & **83.08** & **96.65** & - & - & - \\ 
**AutoReg** & & & & & & & \\  AutoReg\({}_{}\) & 59.04 & 95.56\({}^{*}\) & 79.20 & 95.80 & 86.07 & 76.02 & 81.94 \\ AutoReg\({}_{}\) & 56.07 & 90.92\({}^{2}\) & 80.97 & 95.74 & 86.85 & 81.57 & 82.02 \\ 
**Ours** & & & & & & & \\  PaDeLLM-NER & 67.36 & 95.03\({}^{*}\) & 80.81 & 94.98 & **87.91** & 81.85 & 84.66 \\   

Table 5: Comparison of prediction quality with recent SOTA methods on English supervised datasets. \({}^{**}\)\({}^{**}\)\({}^{**}\)\({}^{**}\)\({}^{**}\) indicates that results are not directly comparable.

sequences may encounter the "\(<\)_eos>_" token earlier, but due to the nature of batch inference, these sequences continue to predict. We plan to improve this in the future by implementing enhancements like KV cache reuse and batch inference with an early quit mechanism, among other strategies.

## 8 Conclusion

In this study, we present PaDeLLM-NER, a parallel decoding framework for NER within LLMs. This approach enables batch parallel decoding of all label-mention pairs, significantly cutting down inference time by 1.76 to 10.22 times without sacrificing prediction accuracy.