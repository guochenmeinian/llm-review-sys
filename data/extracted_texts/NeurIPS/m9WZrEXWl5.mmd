# Directional Smoothness and Gradient Methods: Convergence and Adaptivity

Aaron Mishkin

Stanford University

amishkin@cs.stanford.edu

&Ahmed Khaled

Princeton University

ahmed.khaled@princeton.edu

&Yuanhao Wang

Princeton University

yuanhaoa@princeton.edu

&Aaron Defazio

FAIR, Meta AI

adefazio@meta.com

&Robert M. Gower

CCM, Flatiron Institute

gowerrobert@gmail.com

Equal contribution.

###### Abstract

We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on \(L\)-smoothness.

## 1 Introduction

Gradient methods for differentiable functions are typically analyzed under the assumption that \(f\) is \(L\)-smooth, meaning \( f\) is \(L\)-Lipschitz continuous. This condition implies \(f\) is upper-bounded by a quadratic and guarantees that gradient descent (GD) with step-size \(<2/L\) decreases the optimality gap at each iteration (Bertsekas, 1997). However, experience shows that GD can still decrease the objective when \(f\) is not \(L\)-smooth, particularly for deep neural networks (Bengio, 2012; Z. Li et al., 2020; J. Cohen et al., 2021). Even for functions verifying smoothness, convergence rates are often pessimistic and fail to predict optimization speed in practice (Paquette et al., 2023).

One alternative to global smoothness is local Lipschitz continuity of the gradient ("local smoothness"). Local smoothness assumes different Lipschitz constants hold for different neighbourhoods, which avoids global assumptions and improves rates. However, such analyses typically rely on boundedness of the iterates and then use local smoothness to obtain \(L\)-smoothness over a compact set (Malitsky and Mishchenko, 2020). Boundedness is guaranteed in several ways: Junyu Zhang and Hong (2020) break optimization into stages, Patel and Berahas (2022) use stopping-times, and Lu and S. Mei (2023) employ a line-search. Unfortunately, these approaches modify the underlying optimization algorithm, require local smoothness oracles (Park et al., 2021), or rely on highly complex arguments.

In contrast, we prove simple rates for GD without global smoothness by deriving bounds of the form,

\[f(x_{k+1}) f(x_{k})+ f(x_{k}),x_{k+1}-x_{k}+,x_{k})}{2}\|x_{k+1}-x_{k}\|_{2}^{2},\] (1)where the _directional smoothness function_\(M(x_{k+1},x_{k})\) depends only on properties of \(f\) along the chord between \(x_{k}\) and \(x_{k+1}\). Our sub-optimality bounds provide a path-dependent perspective on GD and are tighter than conventional analyses when the step-size sequence is adapted to the directional smoothness, meaning \(_{k}<2/M(x_{k+1},x_{k})\). See Figure 1 for two real-data examples highlighting our improvement over classical rates. We summarize all our contributions as follows.

**Directional Smoothness.** We introduce three constructive directional smoothness functions \(M(x,y)\). The first, point-wise smoothness, depends only on the end-points \(x,y\) and is easily computed, while the second, path-wise smoothness, yields a tighter bound, but depends on the chord \(=\{ x+(1-)y:\}\). The last function, which we call the optimal point-wise smoothness, is both easy-to-evaluate and provides the tightest possible quadratic upper bound.

**Sub-optimality bounds.** We leverage directional smoothness functions to prove new sub-optimality bounds for GD on convex functions. Our bounds are localized to the GD trajectory, hold for any step-size sequence, and are tighter than the classic analysis using \(L\)-smoothness. They are also more general since we do not need to assume that \(f\) is globally \(L\)-smooth to show progress; all we require is a sequence of step-sizes adapted to the directional smoothness function. Furthermore, our approach extends naturally to acceleration, allowing us to prove optimal rates for (strongly)-convex functions.

**Adaptive Step-Sizes in the Quadratic Case.** In the general setting, computing step-sizes which are adapted to the directional smoothness requires solving a challenging non-linear root-finding problem. For quadratic problems, we show that the ideal step-size that satisfies \(_{k}=1/M(x_{k+1},x_{k})\) is the Rayleigh quotient and is connected to the hedging algorithm (Altschuler and Parrilo, 2023).

**Exponential Search.** Moving beyond quadratics, we prove that the equation \(_{k}=1/M(x_{k+1},x_{k})\) admits a solution under mild conditions, meaning ideal step-sizes can be computed using Newton's method. Since computing these step-sizes is typically impractical, we adapt exponential search (Carmon and Hinder, 2022) to obtain similar path-dependent complexities up to a log-log penalty.

**Polyak and Normalized GD.** More importantly, we show that the Polyak step-size (Polyak, 1987) and normalized GD achieve fast, path-dependent rates _without_ knowledge of the directional smoothness. Our analysis reveals that the Polyak step-size adapts to _any_ directional smoothness to obtain the tightest possible convergence rate. This property is not shared by constant step-size GD and may explain the superiority of the Polyak step-size in many practical settings.

### Additional Related Work

Directional smoothness is a relaxation of non-uniform smoothness (J. Mei et al., 2021), which restricts the smoothness function \(M\) to depend only on \(x\), the origin point. J. Mei et al. (2021) leverage non-uniform smoothness and a non-uniform Lojasiewicz inequality to break lower-bounds for first-order optimization. Similarly, Berahas et al. (2023) show that a weak local smoothness oracle can break lower bounds for gradient methods. A major advantage of our work over such oracle-based approaches is that we construct explicit directional smoothness functions that are easy to evaluate.

Figure 1: Comparison of actual (solid lines) and theoretical (dashed lines) convergence rates for GD with (i) step-sizes strongly adapted to the directional smoothness (\(_{k}=1/M(x_{k+1},x_{k})\)) and (ii) the Polyak step-size. Both problems are logistic regressions on UCI repository datasets (Asuncion and Newman, 2007). Our bounds using directional smoothness are tighter than those based on global \(L\)-smoothness of \(f\) and adapt to the optimization path. For example, on mammographic our theoretical rate for the Polyak step-size concentrates rapidly exactly when the optimizer shows fast convergence.

Similar to non-uniform smoothness, Grimmer (2019) and Orabona (2023) consider Holder-type growth conditions with constants that depend on a neighbourhood of \(x\). Since directional smoothness is stronger than and implies these Holder error bounds, our \(M\) functions can be leveraged to make their results fully explicit (the Holder bounds are non-constructive). Finally, while they also analyze normalized GD, our rates are anytime and do not use online-to-batch reductions like Orabona (2023).

Directional smoothness is also related to \((L_{0},L_{1})\)-smoothness (Jingzhao Zhang et al., 2020; B. Zhang et al., 2020), which can be interpreted as a directional smoothness function with exponential dependence on the distance between \(x\) and \(y\). The extension of \((L_{0},L_{1})\)-smoothness to \((r,l)\)-smoothness by H. Li et al. (2023) shows how to bound sequences of such directional smoothness functions, even for accelerated methods. These approaches are complementary to ours and showcase a setting where directional smoothness leads to concrete convergence rates.

Our work is most closely connected to that by Malitsky and Mishchenko (2020), who use a smoothed version of \(M(x,y)\) to set the step-size. Vladarean et al. (2021) apply a similar smoothed step-size scheme to primal-dual hybrid gradient methods, while Zhao and Huang (2024) relate directional smoothness to Barzilai-Borwein updates (Barzilai and Borwein, 1988) and Vainsencher et al. (2015) use local smoothness over neighbourhoods of the global minimizer to set the step-size for SVRG.

Finally, we note that adaptivity to directional smoothness is different from adaptivity to the sequence of observed gradients obtained by methods such as Adagrad (Duchi et al., 2010; Streeter and McMahan, 2010). Adagrad and its variants are most useful when the gradients are bounded, such as in Lipschitz optimization, although they can also be used to obtain rates for smooth functions (Levy, 2017). We do not address adaptivity to gradients in this work.

## 2 Directional Smoothness

We say that a convex function \(f\) is \(L\)-smooth if for all \(x,y^{d}\),

\[f(y) f(x)+ f(x),y-x+\|y-x\|_{2}^{2}.\] (2)

Minimizing this quadratic upper bound in \(y\) gives the classical GD update with step-size \(_{k}=1/L\). However, this viewpoint leads to rates which depend on the global, worst-case growth of \(f\). This is both counter-intuitive and undesirable because the iterates of GD, \(x_{k+1}=x_{k}-_{k} f(x_{k}),\) depend only on local properties of \(f\). Ideally, the analysis should also depend only on the local conditioning along the path \(\{x_{1},x_{2},\}\). Towards this end, we generalize the smoothness upper-bound as follows.

**Definition 2.1**.: We call \(M:^{d,d}_{+}\) a _directional smoothness function_ for \(f\) if for all \(x,y^{d}\),

\[f(y) f(x)+ f(x),y-x+\|y-x\|^{2}.\] (3)

If a function is \(L\)-smooth, then \(M(x,y)=L\) is a trivial choice of directional smoothness function. In the rest of this section, we construct different \(M\) functions that provide tighter bounds on \(f\) while still being possible to evaluate. The first is the _point-wise directional smoothness_,

\[D(x,y):=}{\|y-x\|_{2}}.\] (4)

Point-wise smoothness is a directional estimate of \(L\) and satisfies \(D(x,y) 2L\). Indeed, \(L\) can be equivalently defined as the supremum of \(D(x,y)/2\) over the domain of \(f\)(Beck, 2017). If \(f\) is convex and differentiable, then \(D(x,y)\) is a directional smoothness function according to Definition 2.1.

**Lemma 2.2**.: _If \(f\) is convex and differentiable, then the point-wise directional smoothness satisfies,_

\[f(y) f(x)+ f(x),y-x+\|y-x\|_{2}^{2}.\] (5)

See Appendix A (we defer all proofs to the relevant appendices). In the worst-case, the point-wise directional smoothness \(D\) is weaker than the standard upper-bound \(M(x,y)=L\) by a factor of two. This is _not_ an artifact of the analysis and is generally unavoidable, as the next proposition shows.

**Proposition 2.3**.: _There exists a convex, differentiable \(f\) and \(x,y^{d}\) such that if \(t<2\), then_

\[f(y)>f(x)+ f(x),y-x+}\|y-x\|_{2}^{2}.\] (6)While the point-wise smoothness is easy to compute, this additional factor of two can make Equation (5) looser than \(L\)-smoothness -- on isotropic quadratics, for example. As an alternative, we define the _path-wise directional smoothness_,

\[A(x,y):=_{t}},\] (7)

and show it verifies the quadratic upper-bound and satisfies Definition 2.1 even without convexity.

**Lemma 2.4**.: _For any differentiable function \(f\), the path-wise smoothness (7) satisfies_

\[f(y) f(x)+ f(x),y-x+ y-x_ {2}^{2}.\] (8)

Path smoothness is tighter than point-wise smoothness since \(A(x,y) D(x,y)\), but hard to compute because it depends on the chord between \(x\) and \(y\). That is, it depends on the properties of \(f\) on the line \(\{tx+(1-t)y:t\}\) rather than solely on the points \(x\) and \(y\) like the point-wise smoothness.

Point-wise and path-wise smoothness are constructive, but they may not yield the tightest bounds in all situations. The tightest directional smoothness function, which we call the _optimal point-wise smoothness_, is the smallest number for which the quadratic upper bound holds,

\[H(x,y)= y- x^{2}}\] (9)

By definition, \(H\) is the tightest possible directional smoothness function; it lower bounds any constant \(C\) that satisfies the quadratic bound (2). Thus, \(H(x,y) M(x,y)\) for any smoothness function \(M\).

The directional smoothness functions introduced in this section represent different trade-offs between computability and tightness. The optimal point-wise smoothness \(H(x,y)\) requires access to both the function and gradient values, whereas the point-wise directional-smoothness \(D(x,y)\) requires only access to the gradients and convexity. In contrast, the path-wise direction smoothness \(A(x,y)\) satisfies Lemma 2.4 with or without convexity, but may be hard to evaluate.

## 3 Path-Dependent Sub-Optimality Bounds

Using directional smoothness, we obtain a descent lemma which depends only on local geometry,

\[f(x_{k+1}) f(x_{k})-(_{k}-^{2}M(x_{k},x_{k+1})}{2} ) f(x_{k})_{2}^{2}.\] (10)

See Lemma A.1. If \(_{k}<2/M(x_{k},x_{k+1})\), then GD is guaranteed to decrease the function value and we call \(_{k}\)_adapted_ to \(M(x_{k},x_{k+1})\). However, computing adapted step-sizes is not always straightforward. For instance, finding \(_{k}=1/M(x_{k},x_{k+1}(_{k}))\) requires solving a non-linear equation.

The rest of this section leverages directional smoothness to derive new guarantees for GD with arbitrary step-sizes. We emphasize that these results are _sub-optimality bounds_, rather than convergence rates; a sequence of adapted step-sizes is required to convert our propositions into a convergence theory. As a trade-off, our bounds reflect the locality of GD, rather than treating it as a global method.

Figure 2: Illustration of GD with \(_{k}=1/L\). Even though this step-size exactly minimizes the upper-bound from \(L\)-smoothness, \(M_{k}\) directional smoothness better predicts the progress of the gradient step because \(M_{k} L\). Our rates improve on \(L\)-smoothness because of this tighter bound.

We start with the case when \(f\) has lower curvature. Instead of using strong convexity or the PL-condition (Karimi et al., 2016), we propose the directional strong convexity constant,

\[(x,y)\!=\!_{t}^{2}}.\] (11)

If \(f\) is convex, then \((x,y) 0\) and it verifies the standard lower-bound from strong convexity,

\[f(y) f(x)+ f(x),y-x+\|y-x\|_{2}^{2}.\] (12)

Moreover, we have \((x,y)\) when \(f\) is \(\)-strongly convex. We prove two bounds for convex functions using directional strong convexity. For brevity, we denote \(M_{i}:=M(x_{i},x_{i+1})\), \(_{i}:=_{i}(x_{i},x^{*})\), \(_{i}=f(x_{i})-f(x^{*})\), and \(_{i}=\|x_{i}-x^{*}\|_{2}^{2}\), where \(x^{*}\) is a minimizer of \(f\).

**Proposition 3.1**.: _If \(f\) is convex and differentiable, then GD with step-size sequence \(\{_{k}\}\) satisfies,_

\[_{k}[_{i}(1+_{i}_{i}_{i} )]_{0}+_{i}[_{j>i,j}(1+_{j}_{j}_{j})]_{i}}{ 2}\| f(x_{i})\|_{2}^{2},\] (13)

_where \(_{i}\!=\!_{i}M_{i}\!-\!2\), \(=\{i:_{i}\!<\!2/M_{i}\}\), and \(=[k]\)._

The analysis splits iterations into good steps \(\), where \(_{k}\) is adapted to the directional smoothness, and bad steps \(\), where the step-size is too large and GD may increase the optimality gap. When \(f\) is \(L\)-smooth and \(\)-strongly convex, using the step-size sequence \(_{k}=1/L\) gives

\[f(x_{k+1})-f(x^{*})[_{i=0}^{k}(1-(2-M_{i }/L)}{L})](f(x_{0})-f(x^{*}))\] (14)

where \(_{i}(2-M_{i}/L)\). Thus, Equation (13) gives at least as tight a rate as standard assumptions by localizing to the convergence path using _any_ directional smoothness \(M\). When \(M_{i}<L\), the gap in constants yields a strictly improved rate (see Figure 2). We also prove a more elegant bound.

**Proposition 3.2**.: _If \(f\) is convex and differentiable, then GD with step-size sequence \(\{_{k}\}\) satisfies,_

\[_{k}[_{i=0}^{k}_{i}|}{1+_{i+1}_ {i}}]_{0}+_{i=0}^{k}[_{j>i}_{j}| }{1+_{j+1}_{j}}]_{i}^{3}-_{i}^{2} )}{1+_{i+1}_{i}}\| f(x_{i})\|_{2}^{2}.\] (15)

Unlike Proposition 3.1, this analysis shows linear progress at each iteration and does not divide \(k\) into good steps and bad steps. In exchange, the second term in Equation (15) reflects how much convergence is degraded when \(_{k}\) is not adapted to the directional smoothness function \(M\). We conclude this section with a bound for when there is no lower curvature, meaning \(_{i}=0\).

**Proposition 3.3**.: _Let \(_{k}\!=\!_{i=0}^{k}_{i}x_{i+1}/\!_{i=0}^{k}_{i}\). If \(f\) is convex and differentiable, then GD satisfies,_

\[f(_{k})-f(x^{*})-x^{*}\|_{2}^{2}}{2_{i=0}^{k }_{i}}+^{k}_{i}^{2}(_{i}M_{i}-1)\| f(x_{i}) \|_{2}^{2}}{2_{i=0}^{k}_{i}}.\] (16)

Eq. (16) is faster than standard analyses whenever \(M_{i}<L\); it will be a key tool in the next sections.

### Path-Dependent Acceleration

Now we show that directional smoothness can also be used to derive path-dependent sub-optimality bounds for accelerated algorithms -- that is, methods obtaining optimal rates for smooth, convex optimization. In particular, we study Nesterov's accelerated gradient descent (AGD) (Nesterov, 1983) and prove that directional smoothness leads to tighter rates given adapted step-sizes. Throughout this section we assume that \(f\) is \(\)-strongly convex with \(=0\) when \(f\) is merely convex.

Although our analysis uses estimating sequences (Nesterov et al., 2018), we state AGD in the following "momentum" formulation, where \(y_{k}\) is the momentum and \(_{k}\) the momentum parameter,

\[ x_{k+1}&=y_{k}-_{k} f(y_{k}) \\ _{k+1}^{2}&=(1-_{k+1})_{k}^{2} }{_{k}}+_{k+1}_{k+1}\\ y_{k+1}&=x_{k+1}+(1-_{k})}{ _{k}^{2}+_{k+1}}(x_{k+1}-x_{k}).\] (17)If \(_{k} 1/M(x_{k},x_{k+1})\), then Equation (10) combined with \(1-_{k}M(x_{k},x_{k+1})/2 1/2\) implies,

\[f(x_{k+1}) f(y_{k})-}{2}\| f(y_{k})\|_{2}^{2}.\] (18)

Our analysis leverages the fact that this descent condition for \(x_{k+1}\) is the only connection between the smoothness of \(f\) and the convergence rate of AGD. Since Equation (18) depends only on the step-size \(_{k}\), we can replace \(L\) within the analysis of AGD with a sequence of adapted step-sizes. The following theorem controls the effect of these step-sizes to obtain path-dependent bounds.

**Theorem 3.4**.: _Suppose \(f\) is differentiable, \(\)-strongly convex and AGD is run with adapted step-sizes \(_{k} 1/M_{k}\). If \(>0\) and \(_{0}=}\), then AGD obtains the following accelerated rate:_

\[f(x_{k+1})-f(x^{*})_{i=0}^{k}(1-})[ f(x_{0})-f(x^{*})+\|x_{0}-x^{*}\|_{2}^{2}].\] (19)

_Let \(_{}=_{i[k]}_{i}\). If \( 0\) and \(_{0}(},c)\), where \(c\) is the maximum value of \(_{0}\) for which \(_{0}=^{2}-_{0}_{0}}{_{0}(1-_{0})}\) satisfies \(_{0}<3/_{}+\), then AGD obtains the following rate:_

\[f(x_{k+1})-f(x^{*})}(_{0}-)(k+1)^{2}} [f(x_{0})-f(x^{*})+}{2}\|x_{0}-x^{*}\|_{2}^{2}].\] (20)

If \(_{k}=1/M_{k}>1/L\), then these rates are strictly faster than those obtained under \(L\)-smoothness and Theorem 3.4 shows that AGD provably benefits from taking the largest possible steps given the local geometry of \(f\). However, obtaining accelerated rates when \(=0\) requires prior knowledge of the minimum step-size; while this is straightforward for \(L\)-smooth functions, it is not clear how to extend such result to non-strongly convex acceleration with locally Lipschitz gradients. For example, while H. Li et al. (2023) show that the \((r,l)\)-smoothness (a valid directional smoothness function) is bounded over the iterate trajectory, their rate does not adapt to the optimization path.

## 4 Adaptive Learning Rates

Converting our sub-optimality bounds into convergence rates requires adapted step-sizes satisfying \(_{k}<2/M(x_{k},x_{k+1})\). Given an adapted step-size, the directional descent lemma (Equation (10)) implies GD decreases \(f\) and we can obtain fast rates if the step-sizes are bounded below. However, \(x_{k+1}\) is itself a function of \(_{k}\), meaning adapted step-sizes are not straightforward to compute.

For \(L\)-smooth \(f\), the different directional smoothness functions \(M\) introduced in Section 2 satisfy \(M(x_{k},x_{k+1}) 2L\). This implies \(_{k}<\) is trivially adapted. As such step-sizes don't capture local properties of \(f\), we introduce the notion of _strongly adapted step-sizes_, which satisfy

\[_{k}=1/M(x_{k+1}(_{k}),x_{k}).\] (21)

Equation (10) implies GD with a strongly adapted step-size makes guaranteed progress as,

\[f(x_{k+1}) f(x_{k})-[2M(x_{k+1},x_{k})]^{-1}\| f(x_ {k})\|_{2}^{2}.\] (22)

This progress is greater than that guaranteed by \(L\)-smoothness when \(M(x_{k},x_{k+1})<L\) and holds even when \(f\) is not \(L\)-smooth. However, it is not clear a priori if (i) strongly adapted step-sizes exist or if (ii) any iterative method achieves the progress in Eq. (21). Surprisingly, we provide a positive answer to both questions. Strongly adapted \(_{k}\) are computable and we also prove GD with the Polyak step-size adapts to any choice of directional smoothness, including the optimal point-wise smoothness. Before presenting this strong result, we consider the illustrative case of quadratic minimization.

### Adaptivity in Quadratics

Now we show that step-sizes adapted to both the point-wise smoothness \(M\) and the path-wise smoothness \(A\) exist when \(f\) is quadratic. Let \(f(x)=x^{}Bx/2-c^{}x\), where \(B\) is positive semi-definite. Assuming \(\{_{k}\}\) is strongly adapted to the directional smoothness, Equation (16) implies

\[f(_{k})-f(x^{*})-x^{*}\|_{2}^{2}}{2 _{i=0}^{k}_{i}}=-x^{*}\|_{2}^{2}}{2_{i=0}^{ k},x_{i+1})}}-x^{*}\|_{2}^{2}}{2(k+1)} ^{k}M(x_{i},x_{i+1})}{k+1},\] (23)where we used \(_{i}M_{i}=1\) as well as Jensen's inequality. This guarantee depends solely on the average directional smoothness along the optimization trajectory \(\{x_{0},x_{1},\}\). When \(f\) is quadratic, we can exactly compute these smoothness constants. In particular, the point-wise directional smoothness is,

\[D(x_{i},x_{i+1})=2\|B f(x_{i})\|_{2}/\| f(x_{i})\|_{2}.\]

Notably, \(D(x_{i},x_{i+1})\) has no dependence on \(x_{i+1}\) and the corresponding strongly adapted step-size is given by \(_{i}=\| f(x_{i})\|_{2}/(2\|B f(x_{i})\|_{2})\) -- see Lemma C.1. Remarkably, this expression recovers the step-size proposed by Dai and Yang (2006), who show it approximates the Cauchy step-size and converges to the "edge-of-stability" (J. Cohen et al., 2021) at \(2/L\) as \(k\). Combining this simple expression with Equation (23) gives a fast, non-asymptotic convergence rate for GD and new theoretical justification for their work.

We can also compute the path-wise directional smoothness in closed form. As Lemma C.2 shows,

\[A(x_{i},x_{i+1})= f(x_{i})^{}B f(x_{i})/ f(x)^{}  f(x),\]

and \(_{i}= f(x_{i})^{} f(x_{i})/[ f(x_{i})^{}B f (x_{i})]\) is the well-known Cauchy step-size. Path-wise directional smoothness thus provides another interpretation (and convergence guarantee) for the Cauchy step-size, which is traditionally derived by minimizing \(f(x- f(x))\) in \(\).

### Adaptivity for Convex Functions

In the last subsection, we proved that strongly adapted step-sizes for the point-wise and path-wise directional smoothness functions have closed-form expressions when \(f\) is quadratic. Moreover, these step-sizes recover two classical schemes from the optimization literature, giving them new justification and fast convergence rates. Now we consider the existence of strongly adapted step-sizes for general convex functions. Our first result gives simple conditions for Equation (21) to have at least one solution when \(M\) is the point-wise directional smoothness.

**Proposition 4.1**.: _If \(f\) is convex and continuously differentiable, then either (i) \(f\) is minimized along the ray \(x()=x- f(x)\) or (ii) there exists \(>0\) satisfying \(=1/D(x,x- f(x))\)._

The next proposition uses a similar argument with slightly stronger conditions to show existence of strongly adapted step-sizes for the path-wise smoothness.

**Proposition 4.2**.: _If \(f\) is convex and twice continuously differentiable, then either (i) \(f\) is minimized along the ray \(x()=x- f(x)\) or (ii) there exists \(>0\) satisfying \(=1/A(x,x- f(x))\)._

Propositions 4.1 and 4.2 do not assume the global smoothness; although neither proof is constructive, it is possible to compute strongly adapted step-sizes for the point-wise directional smoothness using root-finding methods. We show in Section 5 that if \(f\) is twice differentiable, then strongly adapted step-sizes can be found via Newton's method using only Hessian-vector products, \(^{2}f(x) f(x)\).

#### 4.2.1 Exponential Search

Now we show that the exponential search algorithm developed by Carmon and Hinder (2022) can be used to find step-sizes that adapt _on average_ to the directional smoothness. Consider a fixed

Figure 3: Performance of GD with different step-size rules for a synthetic quadratic problem. We run GD for 20,000 steps on 20 random quadratic problems with \(L=1000\) and Hessian skew. Left-to-right, the first plot shows the optimality gap \(f(x_{k})-f(x^{*})\), the second shows the point-wise directional smoothness \(D(x_{k},x_{k+1})\), and the third shows step-sizes used by the different methods.

optimization horizon \(k\) and denote by \(x_{i}()\) the sequence of iterates obtained by running GD from \(x_{0}\) using a fixed step-size \(\). Define the criterion function,

\[()=^{k}\| f(x_{i}())\|_{2}^{2}}{_{i=0}^{k}M (x_{i}(),x_{i+1}())\| f(x_{i}())\|_{2}^{2}},\] (24)

and suppose that we have a step-size \(\) that satisfies \(()/2()\). Using these bounds in Proposition 3.3 yields the following convergence rate,

\[f(_{k})-f_{*}[^{k}M(x_{i},x_{i+1})\|  f(x_{i})\|_{2}^{2}}{_{i=0}^{k}\| f(x_{i})\|_{2}^{2}}]\| x_{0}-x^{*}\|_{2}^{2}.\] (25)

While \(\) does not adapt to each directional smoothness \(M(x_{i},x_{i+1})\) along the path, it adapts to a _weighted average_ of the directional smoothness constants, where the weights are the observed squared gradient norms. This is always smaller than the maximum directional smoothness along the trajectory and can be much smaller than the global smoothness. Furthermore, we have reduced our problem to finding \([()/2,()]\), which is similar to the problem Carmon and Hinder (2022) solve with exponential search. We adopt their approach as Algorithm 1 and give a convergence guarantee.

**Theorem 4.3**.: _Assume \(f\) is convex and \(L\)-smooth. Then Algorithm 1 with \(_{0}>0\) requires at most \(2K((2_{0}/L) 1)\) iterations of GD and in the last run it outputs a step-size \(\) and point \(_{K}=_{i=0}^{K-1}x_{i}()\) such that exactly one of the following holds:_

_Case 1:_ \[=_{0} f(_{K})-f(x^{*}) -x^{*}\|_{2}^{2}}{2K_{0}}\] _Case 2:_ \[_{0} f(_{K})-f^{*} -x^{*}\|_{2}^{2}}{2K}[^{k}M_{i}\|  f(x_{i}^{})\|_{2}^{2}}{_{i=0}^{k}\| f(x_{i}^{}) \|_{2}^{2}}],\]

_where \(M_{i}}}{{=}}M(x_{i}^{},x_{i+1}^{ })\) and \(x_{i}^{}\) are the iterates generated by GD with step-size \(^{}[,2]\)._

Theorem 4.3 requires \(f\) to be \(L\)-smooth, but has only a \(\) dependence on the global smoothness constant. Moreover, the rate scales with the weighted average of smoothness constants along a very close trajectory \(\{x_{1}^{},x_{2}^{},\}\). In the next section, we give convergence bounds that depend on the unweighted average of the directional smoothness constants along the _actual_ optimization trajectory.

#### 4.2.2 Polyak's Step-Size Rule

Our theory so-far suggests using strongly adapted step-sizes, but neither root-finding nor exponential search are practical methods for large-scale optimization. Thus, we now consider other step-size selection rules which may leverage directional smoothness. In particular, the Polyak step-size sets,

\[_{k}=(f(x_{k})-f(x^{*}))/\| f(x_{k})\|_{2}^{2},\] (26)

for some \(>0\), which is optimal for smooth and non-smooth optimization (Hazan and Kakade, 2019) given knowledge of \(f(w^{*})\). Surprisingly, we show that GD with the Polyak step-size also achieves the same guarantee as strongly adapted step-sizes without knowledge of the directional smoothness.

**Theorem 4.4**.: _Suppose that \(f\) is convex and differentiable and let \(M\) be any directional smoothness function for \(f\). Let \(_{0}:=\|x_{0}-x^{*}\|_{2}^{2}\). Then GD with the Polyak step-size and \((1,2)\) satisfies_

\[f(_{k})-f(x^{*})}{2_{i=0}^{k-1}M( x_{i},x_{i+1})^{-1}},\] (27)

_where \(c()=/(2-)(-1)\) and \(_{k}=_{i=0}^{k-1}[M(x_{i},x_{i+1})^{-1}x_{i}]/ (_{i=0}^{k-1}M(x_{i},x_{i+1})^{-1})\)._

Theorem 4.4 measures sub-optimality at an average iterate obtained using the directional smoothness. However, it also holds for the best iterate, \(_{k}=_{i[k]}f(x_{i})\), meaning no knowledge of the directional smoothness is required to obtain the guarantee. We prove an alternative guarantee for the Polyak step-size in Theorem D.2, where the progress depends on the sum of step-sizes rather than on the average directional smoothness. This shows that the step-size in Equation (26) can itself be viewed as a measure of local smoothness, albeit without formal justification.

Compared with the standard guarantee for the Polyak step-size under \(L\)-smoothess, \(f(_{k})-f(x^{*}) 2L_{0}/k\)(Hazan and Kakade, 2019), our analysis in Theorem 4.4 with the choice \(=1.5\) yields

\[f(_{k})-f(x^{*})}{_{i=0}^{k-1}M(x_{i},x_{ i+1})^{-1}}}{k}^{k-1}M(x_{i},x_{i+1})}{k},\]

where the second bound follows from Jensen's inequality and shows that the convergence depends on the average directional smoothness along the trajectory, rather than on \(L\). If \(f\) is \(L\)-smooth, then \(M(x_{k},x_{k+1}) L\) immediately recovers the classic rate for Polyak's method up to a \(3/2\) constant factor. If \(f\) is not \(L\)-smooth, but \(M(x_{k},x_{k+1})\) is bounded, then Equation (27) generalizes the \(O(1/k)\) rate proved concurrently by Takezawa et al. (2024), but for any choice of directional smoothness (of which \((L_{0},L_{1})\)-smoothness (Jingzhao Zhang et al., 2020) is but one).

**Comparison with strongly adapted step-sizes.** As we saw for quadratics, strongly adapted step-sizes for any directional smoothness function allow us to obtain the following convergence rate,

\[f(_{k})-f(x^{*})-x^{*}\|_{2}^{2}}{2 _{i=0}^{k-1}M(x_{i},x_{i+1})^{-1}}.\]

This is matches the guarantee given by Equation (27) up to constant factors. As a result, we give a positive answer to the question posed earlier in this section: GD with the Polyak step-size achieves the same convergence for any smoothness function \(M\) as GD with step-sizes strongly adapted to \(M\).

**Application to the optimal directional smoothness.** Theorem 4.4 holds for _every_ directional smoothness function \(M\). Therefore we can specialize Equation (27) with the optimal point-wise directional smoothness \(H\) (as defined in Equation (4)) and \(=1.5\) to get the guarantee,

\[_{i[k-1]}[f(x_{i})-f(x^{*})]-x^{*} \|_{2}^{2}}{_{i=0}^{k-1}H(x_{i},x_{i+1})^{-1}}.\] (28)

This rate requires computing the iterate with the minimum function value, but that is easy to track during optimization. Unlike our previous results, Equation (28) requires no access to the optimal point-wise smoothness, yet obtains a dependence on the tightest constant possible.

### Normalized Gradient Descent

Now we change directions slightly and study normalized GD, whose convergence also depends on the directional smoothness. Normalized GD uses step-sizes which are divided by the gradient magnitude,

\[x_{k+1}=x_{k}-}{\| f(x_{k})\|_{2}} f(x_{ k}).\] (29)

Our next theorem shows that normalized GD obtains a guarantee which depends solely on the average of the point-wise directional smoothness \(D_{k}:=D(x_{k},x_{k+1})\) despite no explicit knowledge of \(D_{k}\).

**Theorem 4.5**.: _Suppose that \(f\) is convex and differentiable. Let \(D\) be the point-wise directional smoothness defined by Equation (4) and \(_{0}:=\|x_{0}-x^{*}\|_{2}^{2}\). Then normalized GD with a sequence of non-increasing step-sizes \(_{k}\) satisfies_

\[f(_{k})-f(x^{*})+_{i=0}^{k-1}_{i}^{2}}{2k^{ 2}}()}{_{0}^{2}}-)}{_{k-1}^{2}})+ +_{i=0}^{k-1}_{i}^{2}}{2k}_{i=0}^{k-1},x_{i+1})}{k},\] (30)

_where \(_{k}=_{i[k-1]}f(x_{i})\). If \(_{i[k-1]}M(x_{i},x_{i+1})\) is bounded for all \(k\) (i.e. \(f\) is \(L\)-smooth), then for \(_{i}=1/\) we have \(f(_{k})-f(x^{*})(1/k)\) and for \(_{i}=1/\) we get the anytime result \(f(_{k})-f(x^{*})((k)/k)\)._

Theorem 4.5 gives a rate for normalized GD which is valid for any convex \(f\) without any dependence on global smoothness. However, does not adapt to any smoothness function like the Polyak step-size.

## 5 Experiments

We evaluate the practical improvement of our convergence rates over those using \(L\)-smoothness on two logistic regression problems taken from the UCI repository (Asuncion and Newman, 2007).

Figure 1 compares GD with strongly adapted step-sizes \(=1/M_{k}\), where \(M_{k}\) is the point-wise smoothness, against GD with the Polyak step-size. We also plot the exact convergence rates for each method, Equation (16) and Equation (27), respectively, and compare against the classical guarantee for both methods. Our convergence rates are an order of magnitude tighter on the ionosphere dataset and display a remarkable ability to adapt to the path of optimization on mammographic.

Figure 3 compares the performance of GD with strongly adapted step-sizes and with the fixed step-size \(_{k}=1/L\) for a synthetic quadratic with Hessian skew (R. Pan et al., 2022). Results are averaged over twenty random problems. We find that strongly adapted step-sizes lead to significantly faster convergence. Since \(A_{k},D_{k} L\), the adapted step-sizes are larger than \(2/L\), especially at the start of training; they eventually converge to \(2/L\), indicating these methods operate at the edge-of-stability (J. Cohen et al., 2021; J. M. Cohen et al., 2022). This is consistent with Ahn et al. (2022) and Y. Pan and Y. Li (2023), who show local smoothness is correlated with edge-of-stability behavior.

We conclude with a comparison of empirical convergence rates on three additional logistic regression problems from the UCI repository. We compare GD with \(_{k}=1/L\), GD with step-sizes strongly adapted to the point-wise smoothness (\(_{k}=1/D_{k}\)), GD with the Polyak step-size (Polyak), and normalized GD (Norm. GD) against the AdGD method (Malitsky and Mishchenko, 2020). The Polyak step-size performs best on every dataset but ozone, where GD with \(_{k}=1/D_{k}\) solves the problem to high accuracy in just a few iterations. Thus, although Polyak step-sizes have the optimal dependence on directional smoothness, computing strongly adapted step-sizes can still be advantageous.

## 6 Conclusion

We present new sub-optimality bounds for GD under novel measures of local gradient variation which we call directional smoothness functions. Our results hold for any step-sizes, improve over standard analyses when \(_{k}\) is adapted to the choice of directional smoothness, and depend only on properties of \(f\) local to the optimization path. For convex quadratics, we show that computing step-sizes strongly adapted to directional smoothness functions is straightforward and recovers two well-known step-size schemes, including the Cauchy step-size. In the general case, we prove that an algorithm based on exponential search gives a weighted-version of the path-dependent convergence rate with no need for adapted step-sizes. We also show that GD with the Polyak step-size and normalized GD both obtain fast rates with no dependence on the global smoothness parameter. Crucially, the Polyak step-size adapts to any choice of directional smoothness, including the tightest possible parameter.

Figure 4: Comparison of GD with \(_{k}=1/L\), step-sizes strongly adapted to the point-wise smoothness (\(_{k}=1/D(x_{k},x_{k+1})\)), and the Polyak step-size against normalized GD (Norm. GD) and the AdGD method on three logistic regression problems. AdGD uses a smoothed version of the point-wise directional smoothness from the previous iteration to set \(_{k}\). We find that GD methods with adaptive step-sizes consistently outperform GD with \(_{k}=1/L\) and even obtain a linear rate on horse-colic.