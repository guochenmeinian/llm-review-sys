# Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to Biological Motion Perception

Shuangpeng Han\({}^{1,2}\), Ziyu Wang\({}^{1,2,3}\), Mengmi Zhang\({}^{1,2}\)

\({}^{1}\)College of Computing and Data Science, Nanyang Technological University, Singapore

\({}^{2}\)Deep NeuroCognition Lab, Agency for Science, Technology and Research (A*STAR)

\({}^{3}\)Show Lab, National University of Singapore, Singapore

Address correspondence to mengmi.zhang@ntu.edu.sg

###### Abstract

Biological motion perception (BMP) refers to humans' ability to perceive and recognize the actions of living beings solely from their motion patterns, sometimes as minimal as those depicted on point-light displays. While humans excel at these tasks _without any prior training_, current AI models struggle with poor generalization performance. To close this research gap, we propose the Motion Perceiver (MP). MP solely relies on patch-level optical flows from video clips as inputs. During training, it learns prototypical flow snapshots through a competitive binding mechanism and integrates invariant motion representations to predict action labels for the given video. During inference, we evaluate the generalization ability of all AI models and humans on 62,656 video stimuli spanning 24 BMP conditions using point-light displays in neuroscience. Remarkably, MP outperforms all existing AI models with a maximum improvement of 29% in top-1 action recognition accuracy on these conditions. Moreover, we benchmark all AI models in point-light displays of two standard video datasets in computer vision. MP also demonstrates superior performance in these cases. More interestingly, via psychophysics experiments, we found that MP recognizes biological movements in a way that aligns with human behaviors. Our data and code are available at link.

## 1 Introduction

Figure 1: **Humans excel at biological motion perception (BMP) tasks with _zero_ training, while current AI models struggle with poor generalization performance**. AI models are trained to recognize actions from natural RGB videos and tested using BMP stimuli on point-light displays, which come in two forms: Joint videos, which display only the detected joints of actors in white dots, and Sequential position actor videos (SP), where light points in white are randomly positioned between joints and reallocated to other random positions on the limb in subsequent frames (**Sec. 3.1**). Note that skeletons, shown in gray in the example video, are not visible to humans or AI models during testing. The generalization performance of both humans and models is assessed after varying five properties in temporal and visual dimensions. See **Appendix, Sec. A.1** for example videos.

Biological Motion Perception (BMP) refers to the remarkable ability to recognize and understand the actions and intentions of other living beings based solely on their motion patterns . BMP is crucial for tasks such as predator detection , prey selection , courtship behavior , and social communications  among primates and humans. In classical psychophysical and neurophysiological experiments , motion patterns can sometimes be depicted minimally, such as in point-light displays where only major joints of human or animal actors are illuminated. Yet, _without any prior training_, humans can robustly and accurately recognize actions  and characteristics of these actors like gender , identity , personalities , emotions , social interactions , and casual intention .

To make sense of these psychophysical and neurophysiological data, numerous studies  have proposed computational frameworks and models in BMP tasks. Unlike humans, who excel at BMP tasks without prior training, these computational models are usually trained under specific BMP conditions and subsequently evaluated under different BMP conditions. However, the extent to which these models can learn robust motion representations from natural RGB videos and generalize them to recognize actions on BMP stimuli remains largely unexplored.

In parallel to the studies of BMP in psychology and neuroscience, action recognition on images and videos in computer vision has evolved significantly over the past few decades due to its wide range of real-world applications . The field has progressed from relying heavily on hand-crafted features  to employing deep-learning-based approaches . These modern approaches can capture the temporal dynamics and spatial configurations of complex activities within dynamic and unstructured environments . Despite these advancements, existing AI models still struggle with generalization issues related to occlusion , noisy environments , viewpoint variability , subtle human movements  and appearance-free motion information . While various solutions have been proposed to enhance AI generalization in action recognition , they do not specifically tackle the generalization challenges in BMP tasks.

Here, our objective is to systematically and quantitatively examine the generalization ability of AI models trained on natural RGB videos and tested in BMP tasks. To date, research efforts in neuroscience and psychology  have mostly focused on specific stimuli for individual BMP tasks. There is a lack of systematic, integrative, and quantitative exploration that covers multiple BMP properties and provides a systematic benchmark for evaluating both human and AI models in these BMP tasks. To bridge this gap, we establish a benchmark BMP dataset, containing 62,656 video stimuli in 24 BMP conditions, covering 5 fundamental BMP properties. Our result indicates that current AI models exhibit limited generalization performance, slightly surpassing chance levels.

Subsequently, to enhance the generalization capability of AI models, we draw inspiration from  in neuroscience and introduce Motion Perceiver (MP). MP only takes dense optical flows between any pairs of video frames as inputs and predicts action labels for the given video. In contrast to many existing pixel-level optical flow models , MP calculates dense optical flows at the granularity of patches from the feature maps. In MP, we introduce a set of flow snapshot neurons that learn to recognize and store prototypical motion patterns by competing with one another and binding with dense flows. This process ensures that similar movements activate corresponding snapshot neurons, promoting consistency in motion pattern recognition across patches of video frames. The temporal dynamics within dense flows can vary significantly depending on factors such as the speed, timing, and duration of the actions depicted in video clips. Thus, we also introduce motion invariant neurons. These neurons decompose motions along four motion directions and integrate their magnitudes over time. This process ensures that features extracted from dense flows remain invariant to small changes and distortions in temporal sequences.

We conducted a comparative analysis of the generalization performance of MP against existing AI models in BMP tasks. Impressively, MP surpasses these models by 29% and exhibits superior performance in point-light displays of standard video datasets in computer vision. Additionally, we examined the behaviors of MP alongside human behaviors across BMP conditions. Interestingly, the behaviors exhibited by MP in various BMP conditions closely align with those of humans. Our main contributions are highlighted:

**1.** We introduce a comprehensive large-scale BMP benchmark dataset, covering 24 BMP conditions and containing 62,656 video stimuli. As an upper bound, we collected human recognition accuracy in BMP tasks via a series of psychophysics experiments.

**2.** We propose the Motion Perceiver (MP). The model takes only patch-level optical flows of videos as inputs. Key components within MP, including flow snapshot neurons and motion invariant neurons, significantly enhance its generalization capability in BMP tasks.

**3.** Our MP model outperforms all existing AI models in BMP tasks, achieving up to a 29% increase in top-1 action recognition accuracy, and demonstrating superior performance in point-light displays of two standard video datasets in computer vision.

**4.** The behaviors exhibited by the MP model across various BMP tasks demonstrate a high degree of consistency with human behaviors in the same tasks. Network analysis within MP unveils crucial insights into the underlying mechanisms of BMP, offering valuable guidance for the development of generalizable motion perception capabilities in AI models.

## 2 Our Proposed Motion Perceiver (MP)

Our proposed model, Motion Perceiver (MP), aims to learn robust action-discriminative motion features from natural RGB videos and generalize these capabilities to recognize actions from BMP stimuli on point-light displays. MP takes the inputs of only patch-level optical flows from a video \(\), which comprises \(T\) frames denoted as \(\{I_{1},I_{2},...,I_{t},...,I_{T}\}\). While visual features from videos are typically useful for action recognition, our research focuses on extracting and learning motion information alone from natural RGB videos. Finally, MP outputs the predicted actions from a predefined set of labels \(\). See **Fig. 2** for the model architecture.

### Patch-level Optical Flow

Pixel-level optical flow has been a common approach for modelling temporal dynamics in videos [90; 91; 86; 99]. Different from these works, we use the frozen ViT , pre-trained on ImageNet  with DINO , to extract feature map \(F_{t}^{N C}\) from \(I_{t}\) and compute its optical flow relative to other frames. Consider \(F_{t}\) as a 2D grid of patches in \(N=H W\), where \(H\) and \(W\) are the height and width of \(F_{t}\). \(N\) represents the number of patches in the feature map and \(C\) is the feature dimension per patch. A unique spatial location for each patch can be defined by its 2D coordinates. We concatenate the X and Y coordinates of all patches in \(F_{t}\) to form the patch locations \(G^{N 2}\).

Two reasons motivate us to design patch-level optical flows. First, the empirical evidence  suggests that current unsupervised feature learning frameworks produce feature patches with semantically consistent correlations among neighboring patches. Therefore, patch-level optical flows convey meaningful motion information about semantic objects in a video. Second, pixel-level optical flows can be noisy due to motion blur, occlusions, specularities, and sensor noises. Feature maps obtained from feed-forward neural networks often mitigate these low-level perturbations and provide more accurate estimations of optical flows.

Figure 2: **Architecture of our proposed Motion Perceiver (MP) model.** Given a reference patch (yellow or green example patches), MP computes its patch-level optical flow (red arrows, **Sec. 2.1**) on the feature maps extracted from DINO . Subsequently, these flows are processed through flow snapshot neurons (**Sec. 2.2**) and motion invariant neurons (**Sec. 2.3**) in two pathways. Activations from both groups of neurons are then integrated for action classification (**Sec. 2.4**). Time embeddings (T Emb.) are used in the feature fusion process.

Next, we introduce how the patch-level optical flow for video \(\) is computed. Without loss of generality, given any pair of feature sets \(a^{m d}\) and \(b^{n d}\), where \(m\) and \(n\) are the numbers of patches in the feature sets and \(d\) is the feature dimension, the adjacency matrix \(Q_{a,b}\) between \(a\) and \(b\) can be calculated as their normalized pairwise feature similarities: \(Q_{(a,b)}=/}}{_{n}e^{f(a)f(b)^{}/ }}^{m n}\), where the superscript \(\) is the transpose function, \(f()\) is the \(l_{2}\)-normalization, and \(\) is the temperature controlling the sharpness of distribution with its smaller values indicating sharper distribution. We set temperature \(=0.001\) and the influence of \(\) is analyzed in **Appendix, Tab. S4**.

Using the patch features of \(I_{t}\) as the reference, the optical flow between any consecutive frames \(I_{m}\) and \(I_{m+1}\) is defined as \(O^{t}_{m m+1}^{N 2}\).

\[O^{I_{t}}_{m m+1}=_{I_{t} I_{m+1}}-_{I_{t} I_{m}},\; \;_{I_{t} I_{j}}=Q_{(F_{i},F_{j})}G.\] (1)

\(_{I_{t} I_{j}}\) represents the positions of all patches from \(F_{i}\) of \(I_{i}\) after transitioning to \(F_{j}\) of \(I_{j}\). Essentially, the positions of patches with shared semantic features on \(F_{i}\) tend to aggregate towards the centroids of corresponding semantic regions on \(F_{j}\). Consequently, the optical flow \(O^{I_{t}}_{m m+1}\) indicates the movement from \(I_{m}\) to \(I_{m+1}\) for patches that exhibit similar semantic features as those in \(I_{t}\).

For all \(m\{1,2,...,T-1\}\) and \(t\{1,2,...,T\}\), we compute \(O^{I_{t}}_{m m+1}\) and concatenate them to obtain the patch-level optical flow \(^{T N 2(T-1)}\) for video \(\) as \(=[O^{I_{1}}_{1 2},...,O^{I_{T}}_{(T-1) T}]\).

Since small optical flows might be susceptible to noise or errors, the optical flows in \(\) with magnitudes less than \(=0.2\) are set to \(10^{-6}\) to maintain numerical stability. Unlike  where optical flows are computed only between two adjacent frames, our method introduces \(\), where we compute a sequence of optical flows across all \(T\) frames for any reference patch of a video frame. This dense optical flow estimation approach at the patch level captures a richer set of motion dynamics, providing a more comprehensive analysis of movements throughout the video.

### Flow Snapshot Neurons

Drawing on findings from neuroscience that highlight biological neurons selective for complex optic flow patterns associated with specific movement sequences in motion pathways , we introduce "flow snapshot neurons". These neurons are designed to capture and represent prototypical moments or "slots" in movement sequences. Mathematically, we define \(K=6\) flow snapshot neurons or slots \(^{K D}\), where \(D=2(T-1)\) is the feature dimension per slot. \(\) contains learnable parameters randomly initialized with the Xavier uniform distribution . The impact of \(K\) is discussed in **Appendix, Tab. S4**.

Slot attention mechanism  separates and organizes different elements of an input into a fixed number of learned prototypical representations or "slots". The learned slots have been useful for semantic segmentation [102; 60; 111; 51; 22]. Here, we follow the training paradigm and implementations of the slot attention mechanism in  and apply it to \(\). Specifically, each slot in \(\) attends to unique optical flow sequence patterns in \(\) through a competitive attention mechanism based on feature similarities. To ensure the prototypical optical flow patterns in \(\) are diverse, we introduce the loss of contrastive walks \(L_{slot}\). During inference, we keep \(\) frozen and leverage the activations of flow snapshot neurons for action recognition, denoted as \(=f()f()^{}\), where \(^{T N K}\). See **Appendix, Sec. B.1** and **Appendix, Sec. B.2** for mathematical formulations.

Unlike  that use slot-bonded optical flow sequences \(^{T N D}\) for downstream tasks, we highlight two advantages of using \(\). First, the dimensionality of flow similarities (\(K\)) is typically much smaller than that of slot-bonded optical flow sequences (\(D\)), effectively addressing overfitting concerns and reducing computational overhead. Second, by leveraging flow similarities, we benefit from slot activations that encode prototypical flow patterns distilled from raw optical flow sequences. This filtration process helps eliminate noise and irrelevant information from the slot-bonded flow sequences, enhancing the model's robustness.

### Motion Invariant Neurons

The temporal dynamics in video clips can vary significantly in the speed and temporal order of the actions. Here, we introduce motion invariant neurons that capture the accumulative motion magnitudes independent of frame orders. Specifically, we define the optical flow \((O^{x,n,I_{t}}_{m t},O^{y,n,I_{t}}_{m t})\)for patch \(n\) in \(I_{m}\) moving from frame \(I_{m}\) to \(I_{t}\) along \(x\) and \(y\) axes. Every patch-level optical flow in \(\) can be projected into four motion components along \(+x\), \(-x\), \(+y\) and \(-y\) axes. Without loss of generality, we show that for patch \(n\) in \(I_{t}\), all its optical flow motions over \(T\) frames along the \(+x\) axis are aggregated:

\[_{m=1}^{T}O_{m t}^{x,n,I_{t}}_{O_{m  t}^{x,n,I_{t}}>0}_{u>0}=1&u>0,\\ 0&\] (2)

By repeating the same operations along \(-x,+y,-y\) axes for all the \(N\) patches over \(T\) frames, we obtain the motion invariant matrix \(^{T N 4}\). A stack of self-attention blocks followed by a global average pooling layer fuse information along 4 motion components and then along temporal dimension \(T\). To encourage the model to learn motion invariant features for action recognition, we introduce cross-entropy loss \(L_{invar}\) to supervise the predicted action against the ground truth \(\).

### Multi-scale Feature Fusion and Training

Processing videos at multiple temporal resolutions allows the model to capture both rapid motions and subtle details, as well as long-term dependencies and broader contexts. We use the subscript in \(\), \(\), and \(\) to indicate the temporal resolution. Instead of computing \(_{1}\) between consecutive frames only, we increase the stride sizes to 2, 4, and 8. For example, \(_{4}^{T N 2(T/4-1)}\) denotes patch-level flows computed between every 4 frames. Note that we maintain a stride size of 1 for \(\) to ensure motion invariance to temporal orders across the entire video, as subsampling frames would not adequately capture this attribute.

For action recognition, the activations of flow snapshot neurons are concatenated across multiple temporal resolutions as \(_{1,2,4,8}=[_{1},_{2},_{4},_{8}]\) and then used for the fusion of motion information. The concatenated data is first processed through a series of self-attention blocks that operate across \(4K\) slot dimensions, followed by a global average pooling over these dimensions. We then repeat the same fusion process over \(T\) time steps. Time embeddings, similar to the sinusoidal positional embeddings in , are applied across frames. These embeddings help incorporate temporal context into the learned features. The resulting integrated motion feature vector is used for action recognition, with a cross-entropy loss \(L_{flow}\) to supervise the predicted action against \(\).

While \(_{1,2,4,8}\) captures the detailed temporal dynamics, \(\) learns robust features against variations in temporal orders. MP combines feature vectors from \(_{1,2,4,8}\) and \(\) with a fully connected layer and outputs the final action label. A cross-entropy loss \(L_{fuse}\) is used to balance the contributions from \(_{1,2,4,8}\) and \(\). The overall loss is: \(L= L_{slot}+L_{flow}+L_{invar}+L_{fuse}\), where the loss weight \(=10\). See **Appendix, Tab. S4** for the effect of \(\).

**Implementation Details.** Our model is trained on Nvidia RTX A5000 and A6000 GPUs, and optimized by AdamW optimizer  with cosine annealing scheduler  starting from the initial learning rate \(10^{-4}\). Data loading is expedited by FFCV . All videos are downsampled to \(T=32\) frames. We use a random crop of \(224 224\) pixels with horizontal flip for training, and a central crop of \(224 224\) pixels for inference. We use the same set of hyper-parameters for our model in all the datasets. More training details are in **Appendix, Sec. C**.

## 3 Experiments

### Our Biological Motion Perception (BMP) Dataset with Human Behavioral Data

Following the works in vision science, psychology, and neuroscience [33; 89; 9; 53; 6; 16; 37; 45; 71; 76; 97], we introduce the BMP dataset, comprising 10 action classes (**Appendix, Sec. A.2**) specifically chosen for their strong temporal dynamics and minimal reliance on visual cues. Studies [107; 112; 32; 85] indicate that current action recognition models often rely on static features. Point-light displays reduce confounding factors, such as colors, sketches, and body limbs, by minimizing visual information, thereby highlighting the ability to perceive motion. This selection criterion ensures that specific objects or scene contexts, such as a soccer ball on green grass, do not bias the AI models toward recognizing the action as "playing soccer". This approach aligns with our research focus on generalization in motion perception. In the BMP dataset, there are three types of visual stimuli.

**Natural RGB videos (RGB):** We incorporated 9,492 natural RGB videos from the NTU RGB+D 120 dataset  and applied a 7-to-3 ratio for dividing the data into training and testing splits. **Joint

**videos (J):** We applied Alphapose  to identify human body joints in the test set of our RGB videos. The joints of a moving human are displayed as light points, providing minimal visual information other than the joint positions and movements of these joints over time. **Sequential position actor videos (SP):** SP videos are generated to investigate scenarios where local inter-frame motion signals are eliminated . Light points are positioned randomly between joints rather than on the joints themselves. In each frame, every point is relocated to another randomly selected position on the limb between the two joints with uniform distribution. This process ensures that no individual point carries the valid local image motion signal of limb movement. Nonetheless, the sequence of static postures in each frame still conveys information about body form and motion.

We investigated 5 fundamental properties of motion perception by manipulating various design parameters of these stimuli. We adopted the experiment naming convention \([type]+[condition]\) to denote the experimental setup for the stimulus types and the specific manipulation conditions applied to them. For instance, [Joint-3-Frames] indicates that three frames are uniformly sampled from the Joint video type. Next, we introduce the fundamental properties of motion perception and the manipulations performed to examine these properties.

**Temporal order (TO):** To disrupt the temporal order , we reverse (Reversal) or randomly shuffle (Shuffle) video frames for two types of videos, RGB and Joint above. **Temporal resolution (TR):** We alter the temporal resolution  by uniformly downsampling 32 frames to 4 or 3 frames, labelled as 4-Frames and 3-Frames, respectively. For models that need a fixed frame count, each downsampled frame is replicated multiple times before advancing to the next frame in the sequence, until we reach the necessary quantity. **Amount of visual information (AVI):** We quantify the amount of visual information based on the number of light points (P) in Joint videos . Specifically, we included conditions: 5, 6, 10, 14, 18, and 26 light points. **Lifetime of visual information (LVI):** In SP videos, we manipulate the number of consecutive frames during which each point remains at a specific limb position before being randomly reassigned to a different position. Following , we refer to this parameter as the "lifetime of visual information" (LT), and include LT values of 1, 2, and 4. A longer lifetime implies that each dot remains at the same limb position for a longer duration, thereby conveying more local image motion information. **Invariance to camera views (ICV):** Neuroscience research  has revealed brain activity linked to decoding both within-view and cross-view action recognition. To probe view-dependent generalization effects, we sorted video clips from Joints into three categories based on camera views: frontal, \(45^{}\), and \(90^{}\) views. See **Appendix, Sec. A.3** for more implementation details.

**Human psychophysics experiments:** We conducted human psychophysics experiments schematically illustrated in **Appendix, Sec. D**, using Amazon Mechanical Turk (MTurk) . A total of 90 subjects were recruited. For data quality controls, we implemented checks during the experiment. Following a set of filtering criteria, we retained data from 88 subjects. See **Appendix, Sec. D** for details.

### Video Action Recognition Datasets and Baselines in Computer Vision

To evaluate the ability of all AI models to recognize a wider range of actions in natural RGB videos, we include two standard video action recognition datasets in computer vision. **NTU RGB+D 60** comprises 56,880 video clips featuring 60 action classes. **NW-UCLA ** contains 1,494 videos featuring 10 action classes. Both datasets are split randomly, with a 7:3 ratio for the training and test sets. All AI models are trained on RGB videos and tested on Joint videos of these datasets curated with Alphapose  described above. We compute the top-1 action recognition accuracy of all AI models. In addition to our MP model, we include six competitive baselines below for comparison: **ResNet3D ** adapts the architecture of ResNet  by substituting the 2D convolutions with the 3D convolutions. **I3D ** extends a pre-trained 2D-CNN on static images to 3D by duplicating 2D filters along temporal dimensions and fine-tuning on videos. **X3D ** expands 2D-CNN progressively along spatial and temporal dimensions with the architecture search. **R(2+1)D ** factories the 3D convolution kernels into spatial and temporal kernels. **SlowFast ** is a two-stream architecture that processes temporal and spatial cues separately. **MViT ** is a transformer-based model that expands feature map sizes along the temporal dimension while reducing them along the spatial dimension. Furthermore, we include three more competitive baselines (E2-S-X3D , VideoMAE , TwoStream-CNN ) and present results in **Appendix, Sec. E**. The findings remain consistent with the baselines above. All baselines except for TwoStream-CNN are pre-trained on Kinetics 400 . TwoStream-CNN is pre-trained on ImageNet . As an upper bound, we also train the MP directly on Joint or SP videos and the results are in **Appendix, Sec. E**.

## 4 Results

### Our model achieves human-level performance without task-specific retraining

We explore five key properties of motion perception and their influence on video action recognition tasks between humans and our model (MP). Detailed comparisons between humans and all AI models across all BMP tasks are provided in the **Appendix, Sec. L**.

**Temporal order and temporal resolution matter more in Joint videos than natural RGB videos.** In **Fig. 3**, both humans and MP exhibit higher top-1 accuracy in RGB videos (RGB) compared to Joint videos (J-6P) due to the increased difficulty of generalization in Joint videos from their minimal motion and visual information. Error bars in **Fig. 3** represent the standard error of top-1 accuracy across different action classes. However, despite this challenge, the performance on Joint videos remains significantly above chance (1/10). This suggests that both humans and MP have remarkable abilities to recognize actions based solely on their motion patterns in point-light displays.

Moreover, we observed that shuffled (S) or reversed (R) temporal orders significantly impair recognition performance in both humans and MP. This effect is more pronounced in Joint videos (J-6P-R and J-6P-S) compared to RGB videos (RGB-R and RGB-S). The minimal motion information available in Joint videos makes them particularly susceptible to disruptions in temporal orders. The same behavioral patterns are also captured by MP. Interestingly, shuffling has a lesser impact on human performance compared to reversing RGB videos (RGB-R versus RGB-S, p-value < 0.05). However, this pattern is reversed in Joint videos (J-6P-R versus J-6P-S, p-value < 0.05). When considering actions such as sitting down versus standing up, reversing orders may alter the temporal context in RGB videos. The effect of temporal continuity outweighs the effect of temporal context in Joint videos.

We conjectured that temporal resolution matters in video action recognition. Indeed, a decrease in top-1 accuracy with decreasing temporal resolutions is observed in both RGB and Joint videos (compare 32, 4, and 3 frames). However, this effect is more pronounced in Joint videos (J-6P, J-6P-4F, J-6P-3F) compared to RGB videos (RGB, RGB-4F, RGB-3F). Surprisingly, even in the most challenging J-6P-3F, both humans and MP achieve top-1 accuracy of over 40%, significantly above chance. This suggests that both humans and MP are robust to changes in temporal resolutions.

**Minimal amount of visual information is sufficient for recognition.** In **Fig. 4**, both humans and all AI models exhibit comparable performances in RGB videos (RGB). Interestingly, the accuracy drop from J-6P to J-5P is indistinguishable for humans (p-value > 0.05). Similarly, there was a wide range of the number of points that led to robust action recognition for MP (J-26P to J-5P). However,its absolute accuracy is much lower than humans. Therefore, we introduce an enhanced version of the MP model (En-MP) by extending it across multiple DINO blocks, in addition to the last DINO block. Detailed implementation of the En-MP is provided in **Appendix, Sec. F**. Surprisingly, the En-MP outperforms our original MP significantly and performs competitively well as humans. These observations suggest that a minimal visual representation in J-5P is sufficient for action recognition on humans and our En-MP. Conversely, traditional AI models show a significant performance decline moving from J-26P to J-5P. These findings suggest that existing AI models struggle with reduced visual information.

**Humans and MP are robust to the disrupted local image motion.** Aligned with [6; 76], in **Fig. 5** top-1 accuracy in SP videos improves with an increased number of points for humans, a trend that MP replicates. Interestingly, both humans and MP do not show obvious increased performance with lifetimes of points more than 1 frame, potentially due to the loss of form information from fewer dots. This indicates that humans and MP capture not just local motions but also dynamic posture changes.

**Camera views have minimal impacts on recognition.** Neuroscience studies [43; 97; 46; 68] provide evidence that both viewpoint-dependent and viewpoint-invariant representations for action recognition are encoded in primate brains. In **Fig. 7**, MP exhibits a slightly higher accuracy on frontal and \(45^{}\) views compared to the \(90^{}\) (profile) view by 2.5%, which mirrors human performance patterns in . However, we note that human behaviors in our study differ slightly from  (see **Appendix, Sec. G**). Moreover, MP significantly outperforms the second-best model, MViT, by 18%. This suggests that MP not only captures human-like viewpoint-dependent representations but also maintains superior accuracy among AI models across different camera views.

### Comparisons among AI models in BMP tasks and standard computer vision datasets

**MP aligns with human behaviors more closely than all the competitive baselines in BMP tasks.** In **Fig. 6**, we reported the correlation coefficients between each AI model and human performance across all conditions in the BMP dataset. The results demonstrate that our MP exhibit a significantly higher correlation with human performance compared to the baselines, indicating that our MP align more closely with human performance. In addition to the correlation coefficients, we also present the error pattern consistency  between models and humans (**Appendix, Sec. H**, **Appendix, Fig. S4**). Results suggest that our En-MP achieves the highest error pattern consistency with humans at the trial level.

**MP significantly outperforms all the competitive baselines in BMP tasks.** In **Appendix, Fig. S3**, we present the absolute accuracy of all models and humans. The slopes near 1 in our MP model indicate that it performs on par with humans and surpasses all baselines across the five BMP dataset properties. Despite explicitly modelling motion and visual information in separate streams, the SlowFast model struggles with temporal order. Likewise, MViT, which leverages transformer architectures, fails to generalize across different temporal resolutions.

**MP outperforms all the competitive baselines on Joint videos from two standard computer vision datasets.** In **Tab. 1**, MP, relying solely on patch-level optical flows as inputs, performs above chance. This suggests that motion information alone is sufficient for accurate action recognition. Moreover, MP performs better than existing models on Joint videos in these datasets, highlighting that MP learns to capture better motion representations from RGB videos during training. In **Appendix, Sec. I**, we provide results and discussions when we explicitly feed pixel-level optical flows as inputs to these baselines during training and test these models on the Joint videos. Our results show that MP still outperforms these models, suggesting that MP learns generalizable motion representations beyond optical flow patterns. In **Appendix, Sec. J**, we also present visualizations of patch-level optical flow examples, demonstrating its ability to semantically segment the movements of a person.

### Ablation studies reveal key components in our model

To investigate how different components of MP contribute to the generalization performance on Joint and SP videos, we conducted ablation studies (**Tab. 2**, **Appendix, Tab. S4** and **Appendix, Tab. S5**). Removing the pathway involving motion invariant neurons leads to a drop in accuracy when video frames are shuffled or reversed (**A1**, **Tab. 2** and **Appendix, Fig. S3**). For example, MP outperforms its ablated model without motion invariant neurons in the following experiments: 62.32% vs 49.58% in RGB-R; 61.34% vs 38.03% in RGB-S, 38.69% vs 36.03% in J-6P-R, and 32.65% vs 25.28% in J-6P-S. Similarly, removing the pathway involving flow snapshot neurons also leads to a significant accuracy drop (**A2**). Both ablation studies highlight the importance of two pathways for action recognition.

Instead of multi-scale feature fusion, we utilize a single-scale pathway of flow snapshot neurons (**A3**). The decrease of 10.57% in accuracy on J-6P videos implies that multi-scale flow snapshot neurons capture both fine-grained and high-level temporal patterns, essential for action recognition. In **A4**, the model takes patch-level optical flows as inputs and directly utilizes them for feature fusion and action classification without flow snapshot neurons. There is a significant drop of 14.33% in accuracy, implying that flow snapshot neurons capture meaningful prototypical flow patterns, and their activations are critical for recognition. Threshold \(\) controls the level of noise or errors in small motions. As expected, removing \(\) leads to impaired recognition accuracy (**A5**).

Data augmentation is a common technique to enhance AI model generalization . It typically includes temporal augmentations like random shuffling and reversal in **A6**. Surprisingly, MP's generalization performance is impaired, indicating that randomizing frame orders during training disrupts motion perception and action understanding. We also present ablation results in RGB videos, with similar conclusions drawn, albeit to a lesser extent due to increased visual information and reduced reliance on motion.

In addition to the main ablation results discussed here, we also provide the extra ablation studies and model analysis in the **Appendix**. Specifically, we include the following studies: 1. the removal of the time embedding in our MP (**Sec. 2.4, Appendix, Tab. S5**); 2. the removal of the loss term \(L_{slot}\) in our MP (**Sec. 2.2, Appendix, Tab. S5**); 3. the choice of the reference frame for calculating path-level optical flows in our MP (**Sec. 2.1, Appendix, Tab. S5**); 4. the pixel-level optical flows downscaled to the size of the patch-level optical flows in our MP (**Appendix, Sec. K**); 5. the replacement of

  &  \\  & NTU RGB+D 60 & NW-UCLA \\  ResNet3D & 6.89 & 10.99 \\ I3D & 4.46 & 8.74 \\ X3D & 2.00 & 16.59 \\ R(2+1)D & 5.18 & 10.54 \\ SlowFast & 6.26 & 10.09 \\ MuViT & 6.56 & 10.09 \\  MP (ours) & **20.38** & **42.83** \\  

Table 1: **Our model outperforms all existing models on Joint videos (J-6P) from two standard computer vision datasets.** See 3.2 for dataset descriptions. Best is in bold.

Figure 5: **Both humans and our model can recognize actions in SP videos without local motions.** Performance varies depending on the persistence of visual information, with stimuli having 4 and 8 points (P) of the actors (**Sec. 3.1**).

Figure 6: **Our MP model shows a significantly stronger correlation with human performance compared to all baselines.** The correlation between the model and human performance across all BMP conditions is presented.

Figure 7: **Humans and AI models show minimal difference in generalization across camera views on J-6P videos (Sec. 3.1).** Error bars indicate the standard error.

the feature extractor DINO with ResNet in our MP (**Appendix, Sec. K**); 6. the comparison of the number of trainable parameters among all the models (**Appendix, Sec. K**); 7. the analysis of key frames predicted by our MP on an example video in **Appendix, Sec. K**. All these studies emphasized the importance of our model designs and demonstrated the generalization ability of our model.

## 5 Discussion

We introduce Motion Perceiver (MP) as a generalization model trained on natural RGB videos, capable of perceiving and identifying actions of living beings solely from their minimal motion patterns on point-light displays, even without prior training on such stimuli. Within MP, flow snapshot neurons learn prototypical flow patterns through competitive binding, while motion invariant neurons ensure robustness to variations in temporal orders. The fused activations from both neural populations enable action recognition.

To evaluate the generalization capabilities of all AI models, we curated a dataset comprising 63k stimuli across 24 BMP conditions using point-light displays inspired by neuroscience. Psychophysics experiments on this dataset were conducted, providing human behavioral data for comparison with computational models. While AI models can surpass human performance in numerous tasks, current AI models for action recognition still fall short of human capabilities in many BMP conditions. By focusing solely on motion information, MP achieves superior generalization performance among all AI models and demonstrates a strong alignment with human behavioral responses. All baselines are pre-trained on large-scale video datasets, whereas our MP uses feature extractors pre-trained on naturalistic images. Remarkably, despite lacking video pre-training, our MP outperforms all baselines.

Our work takes an initial step toward bridging artificial and biological intelligence in BMP. First, it raises intriguing questions in neuroscience, such as the neural basis of motion invariant neurons, which are crucial when video frames are shuffled or reversed. Bio-inspired architectures can help test specific neuroscience hypotheses, while insights from neuroscience can, in turn, guide the design of more advanced AI systems. Second, our work paves the way for real-world applications requiring robust motion recognition, such as in low-light conditions where visual information is limited.

The 10 action classes in our BMP dataset were selected for their rich temporal information. We observe significant variations in action recognition accuracy across various action classes for both human participants and AI models. In the future, the BMP dataset could be expanded to include more complex actions. Both neuroscience and computer vision use various biological motion stimuli; here, we focus on point-light displays, but future work could explore other visual stimuli, such as motion patterns in noisy backgrounds. While our model shows promising generalization on BMP stimuli, it does not account for attention or top-down influences. Moreover, since it only uses patch-level optical flows, integrating information from the ventral (form perception) and dorsal (motion perception) pathways remains an open challenge. Further discussion on the social impact of our work can be found in **Appendix, Sec. M**.

  &  **Model** \\  } &  **Model** \\  } &  **Model** \\  } &  **Model** \\  } &  **Model** \\  } &  \\    & & & & & & & RGB & J-6P & SP-8P-1LT \\ 
1 & ✗ & ✓ & ✗ & ✓ & ✓ & ✗ & 93.47 & 64.54 & 49.23 \\
2 & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & 86.17 & 42.52 & 25.91 \\
3 & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ & 96.07 & 58.43 & 30.83 \\
4 & ✓ & ✓ & ✗ & ✗ & ✓ & ✗ & **96.84** & 54.67 & 35.43 \\
5 & ✓ & ✓ & ✗ & ✓ & ✗ & ✗ & 96.70 & 42.38 & 29.42 \\
6 & ✓ & ✓ & ✗ & ✓ & ✓ & ✓ & 93.33 & 58.32 & 31.95 \\  Full Model (ours) & ✓ & ✓ & ✗ & ✓ & ✓ & ✗ & 96.45 & **69.00** & **49.68** \\  

Table 2: **Ablation reveals critical components in our model.** Top-1 accuracy is reported on RGB videos, Joint videos with 6 points (J-6P), and SP videos with 8 points and a lifetime of 1 (SP-8P-1LT). From left to right, the ablated components are: the pathway with Motion Invariant Neurons (P-MIN), the pathway with Flow Snapshot Neurons (P-FSN), the single-scale branch with Flow Snapshot Neurons \(_{1}\) (SS FSN), Slots \(\), threshold \(\) (**Sec. 2.1**), and data augmentation by randomly shuffling and reversing training frames within the same video. Best is in bold.