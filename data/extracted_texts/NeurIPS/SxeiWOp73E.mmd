# Machines and Mathematical Mutations: Using GNNs to Characterize Quiver Mutation Classes

Jesse He\({}^{}\)\({}^{}\), Helen Jenne\({}^{}\), Herman Chau\({}^{}\), Davis Brown\({}^{}\),

Mark Raugas\({}^{}\), Sara Billey\({}^{}\), Henry Kvinge\({}^{,}\)

\({}^{}\) Pacific Northwest National Laboratory

\({}^{}\) University of California San Diego

\({}^{}\) University of Washington

jeh020@ucsd.edu, henry.kvinge@pnnl.gov

###### Abstract

Machine learning is becoming an increasingly valuable tool in mathematics, enabling one to identify subtle patterns across collections of examples so vast that they would be impossible for a single researcher to feasibly review and analyze. In this work, we use graph neural networks to investigate _quiver mutation_--an operation that transforms one quiver (or directed multigraph) into another--which is central to the theory of cluster algebras with deep connections to geometry, topology, and physics. In the study of cluster algebras, the question of _mutation equivalence_ is of fundamental concern: given two quivers, can one efficiently determine if one quiver can be transformed into the other through a sequence of mutations? Currently, this question has only been resolved in specific cases. In this paper, we use graph neural networks and AI explainability techniques to discover mutation equivalence criteria for the previously unknown case of quivers of type \(_{n}\). Along the way, we also show that even without explicit training to do so, our model captures structure within its hidden representation that allows us to reconstruct known criteria from type \(D_{n}\), adding to the growing evidence that modern machine learning models are capable of learning abstract and general rules from mathematical data.

## 1 Introduction

Examples play a fundamental role in the mathematical research workflow. Exploration of a large number of examples builds intuition, supports or disproves conjectures, and points towards patterns that are later formalized as theorems. While computer-aided calculations have long played an important role in mathematics research, modern machine learning tools (e.g., deep neural networks) have only recently begun to be more broadly applied. In this work, we show that a graph neural network trained to classify quivers into mutation equivalence classes learns representations that align with known mathematical theory. Where theory is unknown, we demonstrate how the model's representations can guide the discovery and proof of new mathematics.

Introduced by Fomin and Zelevinsky in , _quiver mutation_ is a combinatorial operation on quivers (directed multigraphs) that arises from the notion of a _cluster algebra_. Identifying whether two quivers are _mutation equivalent_ is generally a hard problem . In some cases, such as type \(A\) or type \(D\) quivers, there exist results [5; 31] that characterize mutation equivalence classes in terms of structural conditions on the quiver. We train a graph neural network (GNN) to accurately classify quivers into one of six different mutation equivalence classes: types \(A\), \(D\), \(E\), \(\), \(\), and \(\). Through a careful application of explainability tools and exploration of hidden activations, we find that the GNN extracts features from type \(D\) quivers that align with the known characterization of . Pushing thisfurther, we are able to use the same analysis of hidden activations to prove an explicit characterization of type \(\) quivers (Theorem 3.1), which was not known previously.

## 2 Quivers and quiver mutation

In their work on cluster algebras , Fomin and Zelevinsky introduce the notion of _matrix mutation_ on skew-symmetric integer matrices. Skew-symmetric matrices and matrix mutations can be interpreted combinatorially as _quivers_ and _quiver mutations_, which are the central objects of our study. In this section, we summarize quivers and quiver mutations. For further background, see Appendix A.1.

**Definition 2.1**.: A _quiver_\(Q\) is a directed multigraph with no loops or 2-cycles. The number of parallel edges between a pair of vertices is represented by a positive integer weight.

**Definition 2.2**.: The _mutation_ of a quiver \(Q\) at a vertex \(j\) is the quiver \(_{j}(Q)\) obtained by performing the following: (i) For each path \(i j k\) in \(Q\), add an edge \(i k\). (ii) Reverse all edges incident to \(j\). (iii) Remove all 2-cycles created from the previous two steps.

Two quivers \(Q\) and \(Q^{}\) are _mutation equivalent_ if \(Q^{}\) can be obtained from \(Q\) by a sequence of mutations. For any vertex \(j\) in a quiver \(Q\), mutation at \(j\) is an involution: \(_{j}(_{j}(Q))=Q\). The set of quivers mutation equivalent to a quiver \(Q\) is called the _mutation class_ of \(Q\).

We consider the mutation classes of quivers that are of simply laced Dynkin type or affine Dynkin type. These quivers have no parallel edges, and their underlying undirected graphs are shown in Fig. 7 in Appendix A. Quivers of type \(D\) and type \(\) are the focus of our explainability analysis, but all the quivers shown in Fig. 7 are included in our training and test sets.

**The machine learning task:** Train a classifier \(\) to predict the mutation class of a quiver of type \(A\), \(D\), \(E\), \(\), \(\), or \(\). Because prior work has characterized quiver mutation classes based on the presence of particular subgraphs, we adapt the most expressive GNN architecture for recognizing subgraphs to support directed edges with edge attributes [22; 33]. We describe our network \(\) as a 4-layer **D**irected **G**raph **I**somorphism **N**etwork with **E**dge features (DirGINE). Each hidden layer has width 32, and the output layer has width 6, since there are 6 classes. We will train \(\) on quivers with 6, 7, 8, 9, and 10 nodes, and test on quivers with 11 nodes. Our model achieves high accuracy (\(99.2\%\)) on the test set (Fig. 1). More details are provided in Appendix B.1.

Figure 1: Average cross-entropy loss (left) and classification accuracy (right) on train and test sets across 10 trials of training. Testing accuracy is consistently higher than training accuracy, perhaps due to the absence of class \(\) in the test set and the fact that \(_{8}=E_{9}\) in the train set.

Extracting characterizations of mutation classes

In this section, we present our main result: a characterization of the mutation class of type \(_{n-1}\) quivers, obtained by probing our trained GNN. Characterizations of the mutation class of type \(A_{n}\) and type \(D_{n}\) quivers are known [5; 31], but to our knowledge, the type \(_{n-1}\) case was previously unknown.

**Theorem 3.1**.: _The mutation class of class \(_{n-1}\) quivers is \(_{n-1}^{}\), the collection of quivers of paired types together with Types V, Va, Vb, V', Va', Vb', VI, and VI' (as described in Appendix D)._

The discovery of the precise types in Theorem 3.1 was aided by insights from edge attributions and latent space clustering in our GNN. The proof that the characterization in Theorem 3.1 is complete follows from a similar argument used by Vatne  to decompose quivers of type \(D_{n}\).

### Recovering a known characterization

We developed and validated our explainability techniques by recovering the known characterization of quivers of type \(A_{n}\) and type \(D_{n}\). The type \(A_{n}\) quivers consist of all quivers that are mutation equivalent to the quiver (6) in Appendix C. Buan and Vatne gave a combinatorial characterization of all quivers in this mutation class, which we refer to by \(_{n}^{A}\).

**Theorem 3.2** (Buan and Vatne ).: _A quiver is in \(_{n}^{A}\) if and only if: (i) All cycles are oriented 3-cycles. (ii) Every vertex has degree at most four. (iii) If a vertex has degree four, two of its edges belong to the same 3-cycle, and the other two belong to a different 3-cycle. (iv) If a vertex has degree three, two of its edges belong to a 3-cycle, and the third edge does not belong to any 3-cycle._

Vatne's classification  of the mutation class \(_{n}^{D}\) of type \(D_{n}\) quivers builds upon the quivers in \(_{n}^{A}\). Each quiver in \(_{n}^{D}\) decomposes into a collection of subquivers joined by gluing certain vertices known as _connecting vertices_. A vertex \(c\) is a connecting vertex if \(c\) is either degree one, or if \(c\) is degree two and part of an oriented 3-cycle.

**Theorem 3.3** (Vatne ).: _The quivers in \(_{n}^{D}\) are divided into four subtypes shown in Fig. 2, where \(\), \(^{}\), and \(^{}\) denote subquivers that are in mutation class \(_{k}^{A}\) for some integer \(k\)._

Since GNNs are capable of recognizing subgraphs and structural patterns as expressively as the classical Weisfeiler-Lehman graph isomorphism test [33; 34], we conjectured that our performant GNN model captured the same subtype motifs identified by human mathematicians. Using the explanation method PGEExplainer described in Appendix B.3, we investigated type \(D_{n}\) quivers in relation to Vatne's characterization. In Fig. 3, darker edges are more important for predicting type \(D_{n}\) quivers, while lighter edges are less important. Subquivers of types I, II, III, and IV in Vatne's characterization are given high attribution.

Fig. 3 strongly suggests that our GNN recognizes the same subtypes as in Vatne's characterization. However, one should be careful in this interpretation, as there is substantial literature showing that it is easy to misinterpret post-hoc explainability methods [21; 23]. Thus, we also examine the embeddings of type \(D_{n}\) quivers in the model's latent space. We use principal component analysis (PCA) to reduce the dimension of the embedding from the model width of 32 to 2 dimensions for visualization. The resulting graph embeddings, plotted in Fig. 4, show a clear separation of the different subtypes. In fact, the layer 3 embeddings in the original 32-dimensional embedding space can be separated by a linear classifier with \(99.7 0.0\%\) accuracy. Subtypes I through IV are not labeled in the training data, so this analysis provides strong evidence that a GNN is capable of re-discovering the same abstract,

Figure 2: Types I, II, III, and IV in \(_{n}^{D}\), from left to right. The subquivers \(\), \(^{}\), and \(^{}\) are type \(A\), and \(c\), \(c^{}\), and \(c^{}\) are connecting vertices. Unoriented edges may be oriented in either direction.

general characterization rules that align with known theory. Furthermore, explainability methods such as PGExplainer can be leveraged to extract these rules from the model.

### Additional experiment: label-flipping

To see how the model is using the type \(D\)-specific subquivers from Theorem 3.3, we examine the model's predictions when the vertices identified by PGExplainer are removed. If the model is primarily keying into the type \(D\) motif, removing this should result in a quiver of type \(A\).

We find that across all \(32,066\) test examples from type \(D\), a plurality (14,916 or 46.5%) of the predictions flip to \(A\), as we would expect if it was using the characterization from Theorem 3.3. Of the remaining examples, most (14,238 or 44.4% of the total) flip to a predicted class of \(E\), with the next-largest being \(D\) (no flip) at 2,581 or 8.0%. Finally, 264 quivers (0.08%) flip to \(\), while 67 quivers (0.02%) flip to \(\). None of the predictions flip to \(\). We believe the large number of flips to \(E\) is due to the PGExplainer attributions for type \(D\) being inexact, perhaps because PGExplainer generalizes imperfectly or because some edges may not contribute positively to \(D\) but rather contribute negatively to other classes. As a result, many of the quivers where we remove highly attributed edges may be out-of-distribution for the model. Since type \(E\) is the only class which contains mutation-infinite quivers, it is perhaps not surprising that the model would predict these out-of-distribution quivers are of type \(E\).

### Discovering a new characterization

This explainability workflow is then applied to type \(_{n-1}\), where no known characterization existed. Compared to type \(D_{n}\), the mutation class of \(_{n-1}\) quivers contains many more diverse subtypes and combinations of motifs. Careful analysis of edge attributions in the \(_{n-1}\) led to our discovery of paired types along with novel subtypes V, Va, Vb, V', Va', VB', VI, VI'. Based on our strategy in Section 3, we plot PCA reductions of the latent space in Fig. 5. We can see that the quivers that do not correspond to paired subtypes, colored as "Other", separate clearly into two clusters in layer

Figure 4: PCA of latent space embeddings for mutation class \(D\) quivers colored by subtypes.

Figure 3: Edge attributions from PGExplainer on type \(D_{11}\) quivers of each subtype (from left to right, Type I, Type II, Type III, Type IV). The masked prediction is the GNN prediction when highly attributed edges are removed.

3. By isolating these quivers and performing \(k\)-means clustering with \(k=2\), the model guides our characterization of the remaining class \(\) subtypes. Fig. 6 shows examples from each cluster. (More examples are given in Appendix E.) See Appendix D for further details of the discovery process and Appendix D.4 for a rigorous proof of Theorem 3.1.

## 4 Conclusion

In this work, we analyzed a graph neural network trained to classify quivers into one of 6 different mutation equivalence classes. Using explainability techniques, we provided evidence that the model learns prediction rules that align with existing theory for type \(D_{n}\) quivers. Moreover, our result emerged from the model in an unsupervised manner--the model is not given any subtype labels, and yet is able to identify relevant subquivers that characterize type \(D_{n}\) quivers. Applying the same explainability techniques to an unknown case, we discovered and proved a characterization of the mutation class of \(_{n-1}\) quivers, a case which had not previously been described in this manner. Our work contributes to the growing evidence that machine learning can be a valuable tool in the mathematician's workflow by identifying novel patterns in mathematical data.