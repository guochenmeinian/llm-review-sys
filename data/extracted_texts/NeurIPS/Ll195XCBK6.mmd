# CoS: Enhancing Personalization and

Mitigating Bias with Context Steering

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

When querying a large language model (LLM), the _context_, i.e. personal, demographic, and cultural information specific to an end-user, can significantly shape the response of the LLM. For example, asking the model to explain Newton's second law with the context _"I am a toddler."_ yields a different answer compared to the context _"I am a physics professor."_ Proper usage of the context enables the LLM to generate personalized responses, whereas inappropriate contextual influence can lead to stereotypical and potentially harmful generations (e.g. associating _"female"_ with _"housekeeper"_). In practice, striking the right balance when leveraging context is a nuanced and challenging problem that is often situation-dependent. One common approach to address this challenge is to fine-tune LLMs on contextually appropriate responses. However, this approach is expensive, time-consuming, and not controllable for end-users in different situations. In this work, we propose Context Steering (CoS) -- a simple training-free method that can be easily applied to autoregressive LLMs at inference time. By measuring the contextual influence in terms of token prediction likelihood and modulating it, our method enables practitioners to determine the appropriate level of contextual influence based on their specific use case and end-user base. We showcase a variety of applications of CoS including amplifying the contextual influence to achieve better personalization and mitigating unwanted influence for reducing model bias. In addition, we show that we can combine CoS with Bayesian Inference to quantify the extent of hate speech on the internet. We demonstrate the effectiveness of CoS on state-of-the-art LLMs and benchmarks.

## 1 Introduction

Societal assumptions inherently influence the responses generated by Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023; Jiang et al., 2023; Groeneveld et al., 2024). Specifically, the inclusion of personal, demographic, and cultural information pertaining to a user may modulate the LLM's response. While leveraging these contextual cues can enhance the relevance and appropriateness of responses in some situations, this can also lead to inaccurate and potentially damaging outcomes in others. Consider an example in which an LLM is asked to explain Newton's second law under the context of "I am a toddler". In this case, it may be reasonable to expect the LLM to tailor its response differently compared to the scenario in which the context is "I am a professor." The underlying demographic assumption -- that toddlers have a limited understanding of physics compared to a professor -- is useful in guiding the response of the LLM. Contrast this with the context of "I am a female professor". In this case, an LLM mistakenly focusing on gender information can produce stereotypical responses that are potentially harmful.

As LLMs are being widely deployed, enabling practitioners to tailor the level of contextual influence to suit a variety of use cases is necessary. For example, recommender systems rely heavily on context to produce high quality recommendations. This customization enhances user satisfaction and increases engagement, demonstrating that increased contextual influence is desirable in these systems (Milli et al., 2023). In other cases, inappropriate reliance on context can contribute to the social divide and reinforce historical inequities (Kotek et al., 2023). Therefore, the ideal degree of contextual influence varies by situation, emphasizing the need for practitioners to have control over this aspect.

Common approaches for improving the LLM's ability to leverage contextual information include supervised fine-tuning and Reinforcement Learning with Human Feedback (Rafailov et al., 2023; Ouyang et al., 2022). By training the LLM on curated high quality user data, RLHF has been shown to enhance performance as well as reduce bias in LLMs. The downside of this approach is that both data collection and training are costly and time-consuming. In addition, tuning the RLHF process correctly for end applications requires significant domain knowledge and is beyond the capability of many practitioners. Furthermore, once training is complete, adjusting the extent of contextual influence for different scenarios is not possible.

Instead, can we enable practitioners to adjust the level of contextual influence without the need to update the models? To that end, we introduce **Context Steering (CoS)**, an inference-time technique that can be easily applied to autoregressive LLMs 1. Our key insight is that _LLMs capture the relationship between the context and the generated text in terms of token prediction likelihood, which allows us to compute the influence as in Figure 1. This enables us to amplify or tune down the influence in downstream generations by a factor of \(\), and exert fine-grained control on the LLM output to fit practitioners' needs._

CoS unifies several disjoint problems under the same framework: from enhancing personalization, mitigating bias to quantifying online hate speech. Further, CoS doesn't require access to a model's internal weights and can be used for API-gated models. For personalization and bias mitigation, we find that CoS achieves compelling performance without any additional fine-tuning. Our findings reveal that CoS can generate responses that are increasingly personalized to end-user contexts in a controllable manner (\(p<.001\)). For hate speech quantification, we combine CoS with Bayesian Inference. We find that the inferred level of hate in online speech correlates well with assessments made by human evaluators.

## 2 Related Work

**Reducing Bias in LLMs.** Bolukbasi et al. (2016) highlight issues of bias in language models. The authors investigate how these embeddings often reflect and perpetuate gender stereotypes and introduce an approach to debias word embeddings by identifying a bias subspace. More recent work finds that these concerning biases extend to LLMs. Kotek et al. (2023) demonstrate that LLMs are three times more likely to choose a stereotype that aligns with a person's gender. Other work has found that LLMs exhibit political bias (Motoki et al., 2023), racial bias (Zack et al., 2024), and geographical bias (Manvi et al., 2024).

Figure 1: **Context Steering (CoS) utilizes the likelihood difference between the same LLM that has and has not seen the context. CoS generates coherent responses that enhance or mitigate the influence of the context in a controllable manner.**

Several approaches have been introduced to counteract bias in LLMs. In their approach, Peng et al. (2020) utilized GPT-2 to introduce a substantial reward mechanism aimed at diminishing the occurrence of non-standard outputs. Zhao et al. (2019) employed data augmentation techniques to substitute gender-specific terms with their antonyms within the initial training dataset, and combined it with another corpus to create a novel model. Joniak and Aizawa (2022) implemented movement pruning and weight freezing techniques, in addition to employing a debiasing method predicated on a gender-related word projection derived from the work of Kaneko and Bollegala (2021). The downside to many of these approaches is that they either require modifications to the dataset or extensive model training, both of which are computationally heavy and difficult to deploy.

**Personalization of LLMs.** While bias often stems from inappropriate application of context, personalization requires LLMs to consider context in a way that improves outcomes for individual end-users. Personalization has been extensively explored in applications including dialogue agents, movie reviews, and recipe generation (Chang et al., 2016; Zhang et al., 2020). Recent works based on LLM have explored generating more realistic conversational data Vincent et al. (2023) using dataset of annotated movie dialogues with narrative character personas. Researchers have utilized publicly available reviews and recipe datasets to explore personalization in reviews (Li and Tuzhilin, 2020) and recipe generation (Majumder et al., 2019).Wuebker et al. (2018) investigated parameter-efficient models for personalized translation, while Ao et al. (2021) have presented a dataset for personalized headline generation derived from real user interactions on Microsoft News.

**Controllable Generation and Structured Prediction.** Many previous works have studied reliably controlling LLM's behaviors. Turner et al. (2023), Li and Tuzhilin (2020), and Subramani et al. (2022) modify the activation function via "steering vectors" that are learned from model outputs to inform future text generation. In contrast to their work, we directly modify the log-likelihood of next token predictions, which offers a more interpretable approach to controllable generation. Our approach is similar to Li et al. (2023), which showed that contrasting the outputs of an amateur versus an expert language model can lead to more quality generations by removing the "amateur tendencies" LLMs. Hartvigsen et al. (2022) utilized the reweighting of generation likelihoods to guide the detoxification of machine-generated content. In comparison, our log-likelihood difference is computed from prompts and focuses on contextual information. Our method also exploits the Bayesian structure in language as done in previous works (Tenenbaum et al., 2011; Goodman and Frank, 2016), where we leverage powerful LLMs as the forward model of underlying language contexts to enable structured predictions.

}   \(\) & \): “**I am a toddler.”**} \\  -3 & Newton’s Second Law of Motion, formally known as the Law of Acceleration, relates the force applied on an object to its resulting acceleration. It is a fundamental principle... \\ -1 & Sure, I’d be happy to explain Newton’s second law of motion!... Mathematically, this is expressed as F = ma... For example, let’s say you have two cars of the same size. \\ 0 & Oh, now! *adjustust glasses* You wanna learn about Newton’s second law?! * Well, let me tell ya, little buddy... is like a super cool secret code! * When you push a toy car... \\ 1 & WOWZA! * giggeles* Oh boy, you wanna learn about science? * Outcomes you up and down* Newton’s second law... See, if you push really hard with your feet, you go faster... \\ 3 & WOWZA! * giggeles* Oh my, you little TODLER you! * bounces on knee* Newton’s SECOND law is like when you run around and play! * chases after toy* 1.6... \\   

Table 1: **Prompt: Explain Newton’s second law. For both contexts \(\), a higher \(\) leads to changes in tone (teal) and more patience, encouragement, and the presence of emojis. A lower \(\) leads to inverse effects (orange) and more scholarly explanations, including a reference to the “law of torque”, a more general form of Newton’s second law. See Appendix C for more details.**Methodology

We explain the details of Context Steering (CoS). Our key insight is that we can capture the level of influence, \(P_{}(X|,)\), that contextual information, \(\), has on generating a text continuation \(X\) for a given prompt, \(\). Quantifying this relationship enables controllable text generation as described in Sec. 3.2. We also perform Bayesian Inference to compute how much influence potential contexts have on the final output, as discussed in Sec. 3.3.

### Preliminaries

We consider an autoregressive LLM that interacts with end users. The user provides context \(\) (e.g. "I am a toddler") and prompt \(\) (e.g. "Explain Newton's second law"). For tokens \(x_{1}...x_{i-1}\) from a vocabulary \(V\), the LLM outputs subsequent tokens according to the distribution \(P(x_{i}|x_{1:i-1},,)\). The model generates the complete response \(X=x_{1:n}\) by predicting one token at a time, following \(P(X|,)=_{i=1}^{m}P(x_{i}|x_{1:i-1},, )\), where \(m\) is some fixed maximum generation length.

Here, we define \(()\) as the raw output by a forward pass of the language model over the vocabulary \(\) from which we extract the most probable token \(x_{i}\) as the first token in the response. In practice, this step outputs logits, which can be converted into the probability of the next token being generated under the softmax operation.

\[P(x_{i}|x_{1:i-1},,)=(x_{i}| ,)]}{Z_{i}},Z_{i}=_{x_{v} V}[ (x_{v}|,)]\] (1)

When generating the next token, the language model attends to all its previous information, including both the context \(\) and the prompt \(\).

### Forward Model: Controllable Generation with CoS

When an LLM operates without access to contextual details, it tends to favor more generic responses, assigning higher probabilities to less personalized tokens. Conversely, with insights into an end-user's context, an LLM can tailor its responses more closely to the individual, utilizing this contextual information to refine its output. Inspired by this observation, CoS aims to quantify the effect of the context, \(\), on the next token and leverage this information to tune the impact of \(\) on the LLM response. We propose a **contextual influence function**2\(\) that operationalizes this idea:

\[_{,}(x_{i})=(x_{i}|, )-(x_{i}|,)\] (2)

The contextual influence function captures how much more likely it is for some token \(x_{i}\) to be generated under the context \(\) compared to when no contextual information is provided (i.e., \(\)). This gives us a flexible knob with which to tune the effect of the context on the output: we can amplify the influence to produce more contextually relevant texts or tune down the influence to generate more generic and unbiased answers. To this end, we can modify the next token probability at inference as:

\[_{}(x_{i}|,) =(x_{i}|,)+ _{,}(x_{i})\] \[=(1+)(x_{i}|,)- (x_{i}|,)\] (3)

Here \(\) controls the influence of \(\): higher \(\) means that \(\) has more influence on \(x_{i}\). \(=-1\) is equivalent to no contextual influence (\((x_{i}|,)\)) and \(=0\) equates to concatenating the original prompt and context (\((x_{i}|,)\)) without modulation.

**Example: Personalization.** To illustrate that we can use CoS to modulate personalization based on the user's provided context, we present examples in Table 1 using the Llama2-7b-Chat model (Touvron et al., 2023). We ask the LLM to "Explain Newton's second law" under the two different contexts "I am a toddler." and "I got a D- in elementary school science." We see that the LLM is not only able to generate highly coherent texts under different values of \(\), but also that the influence of the context is controllable - higher \(\) values correspond to amplifying the effect of the context and lower \(\) reduces the effect.

### Inverse Model: Bayesian Inference with CoS

Previously, we demonstrated how one can use a Contextual Influence Function to modulate an LLM's reliance on contextual information when crafting its response. Our second insight is that we can leverage Bayesian Inference to infer the level of influence, \(\), of a given context, \(\), on the output of the model. This process can help us understand the significance of contextual information on the model's output, providing insight into the reasons behind the model's generated responses.

Eq. (3) defines a forward direction from \(,\) and \(\) to the probability of the next token: \(P_{,}(x_{i}|,)=[ _{}(x_{i}|,)]\). Using Bayesian Inference, we can invert this formula, and infer the context given the prompt \(\), \(\), and generation \(X\):

\[P(=c|,X,)=,}(X|=c,)}{Z_{}},Z_{}=_{c}P_{, }(X|=c,)c\] (4)

This enables us to probe the "undertone" of the language model. For instance, if the model explains "Newton's second law" in a manner that involves frequent mention of toys and analogies, then it is responding as if the user is best treated as a toddler, as in Table 1. Similarly, we can infer the \(\) given the context \(\), prompt \(\), and generation \(X\):

\[P(=|X,,)=,}(X| ,)}{Z_{}},Z_{}=_{}P_{,}(X|,)\] (5)

By inference of \(\), we can quantify the likelihood of a given statement \(X\) being generated based on \(\). In Table 1, a high frequency of emojis suggests a more animated tone, which implies high \(\) for the context of the user being a toddler. Note that Eq. (4) and Eq. (5) involve the intracable computation of the normalizing constant \(Z\). In practice, we can instead compute the maximum likelihood of candidate set \(\) or \(\). A feasible range of lambda values are included in Appendix B.

**Example: Identity implies STEM proficiency.**

Motivated by the fact that personal information (e.g. level of education) is often associated with perceived STEM proficiency, we hypothesize that this phenomenon can be revealed in LLM generations. Returning to the example of explaining Newton's second law, we examine how closely the LLM aligns the user's identity with STEM proficiency by first generating a response using a true context of the user's educational background (e.g. middle schooler, college student). We then hide the true context and infer the likelihood of the generation under different user-specific probe contexts (e.g. perceived STEM proficiency level). In Figure 2, generations for a user more familiar with STEM are more likely to be aligned with the true context of the user being a college student as compared to being a middle school student; this is demonstrated by the context of being a college student having overall higher \(\) values on the left. The opposite effect is seen when the true context is that the user is a middle school student, with inference suggesting that the user is a "beginner". These findings, along with further qualitative results in Appendix C, demonstrate that utilizing CoS in the reverse direction can provide insight into the degree to which the model relies on contextual information, including that which may contain implicit biases.

## 4 CoS for Personalization, Bias Mitigation, and Hate Quantification

We investigate how CoS enhances personalization, mitigates biases, and quantifies the level of contextual information in the application of online hate tweets. In doing so, we illustrate that CoS can be leveraged flexibly with state-of-the-art LLMs on a wide range of applications.

### Experiment: Generating Personalized Summarizations

Figure 2: We plot normalized posterior probabilities of \(\) computed by Eq. (5). We ask the LLM to explain STEM concepts (rows) given true contexts (education level). When inferring the \(\) of these generations, we find that stronger STEM familiarity corresponds with higher education level.

Movie summarization has long been studied in NLP (Salemi et al., 2024). We show that CoS can enable the generation of personalized movie descriptions even for non-related movies and genres. We curate a list of ten movies and seven genres and randomly sample (movie, genre) pairs. We then give LLMs requests in the form of "I like {genre}, tell me about {movie}", where the genre info corresponds to context \(\) for CoS and movie name corresponds to \(\). We intentionally select pairs that are perpendicular to each other (e.g. "I like comedy movies, tell me about the movie Blade Runner 2049."). Impressively, CoS identifies that Blade Runner 2049 is not a comedy movie, and is still able to identify all the comedic aspect of it, such as wordplay, satire or absurd situations for humor, as shown in Table 2. Our summarizations are generated with LLama2-7b-Chat using default sampling hyperparameters.

To show that CoS's personalization aligns with end-users, we collect data annotations from 8 participants. Each participant was presented with a fixed set of 70 LLM responses generated from the tuple \(\{_{i},_{i},_{i}\}\) where \(_{i}\) contains a randomly sampled movie name, \(_{i}\) contains a randomly sampled genre and \(\{-1,3\}\). The underlying \(\) is hidden from the participant by shuffling the order in which sampled texts are presented within the subgroup \(\{_{i},_{i}\}\). We then ask the participant to rate the extent to which the LLM response is personalized to the given context, \(_{i}\). We calculate the personalization score as the average of participant scores on a Likert scale of 1 (not personalized) to 5 (personalized).

   \(\) & \(\)**: I enjoy comedy movies. \(\): Tell me about the movie Blade Runner 2049.** \\  -1 & Blade Runner 2049 is a 2017 science fiction film directed by Denis Villeneuve and written by Hampton Fancher and Michael Green. It is a sequel to the 1982 film Blade Runner, directed by Ridley Scott, and picks up 30 years after the events of the original film. The movie... \\
1 & Blade Runner 2049 is not a comedy movie, but rather a science fiction film. It is a sequel to the 1982 film Blade Runner... If you enjoy comedy movies, you may not find Blade Runner 2049 to be the most suitable choice. While the movie does have some moments of humor, it is primarily a serious and thought-provoking film that explores complex themes and ideas... \\
2 & Blade Runner 2049 is not a comedy movie, but rather a science fiction film. It is a sequel to the 1982 film Blade Runner, which is also not a comedy... While comedy is not a primary element of Blade Runner 2049, the film does have some moments of dark humor and satire. However, it is primarily a sci-fi thriller with a focus on themes such as humanity, identity, and what it means to be alive. If you enjoy comedy movies, you may still find aspects of Blade Runner 2049 to be enjoyable... \\
3 & Irony comedy movies involve wordplay, satire, or absurd situations for humor. Blade Runner 2049, on the other hand, is a science fiction film... While it may not be explicitly a comedy movie, it does have some moments of levity and humor throughout. 1. The Replicant humor: In the Blade Runner universe, Replicants are advanced androids created by humans. Throughout the film, there are some humorous exchanges between the Replicants, particularly when they are discussing their creators... Leto’s delivery is often over-the-top and camps, providing some comedic relief in an otherwise dark and serious film... Ford’s dry wit and sarcasm add some humor to the film...” \\   

Table 2: **Movie personalization user study.** We showed users a series of randomly-ordered generations and ask them to rate the level of personalization. While Blade Runner is not a comedy movie, CoS enables personalized recommendations of movies to audiences of orthogonal genres. Lower \(\) values were primarily factual (orange), attending to the user’s request to explain the movie, while higher \(\) values tailored the response towards the user’s preference for comedy movies (teal).

Figure 3: **User ratings of: I like {genre}, tell me about {movie}.** We find that users rank generations under higher \(\) as more personalized across individual movies.

After grouping across generations under the same lambda value, we illustrate in Figure 3 that the average personalization score increases with \(\), both in aggregate and across most individual movies. Applying Spearman's test, we find that this trend is significant with a strong correlation (\(=.67\), \(p<.001\)), supporting our hypothesis that higher \(\)'s increase personalization.

Compared to directly asking the LLM "Tell me about {movie}" (\(\)=-1) with a naively prepended context "I like {genre}, tell me about {movie}" (\(\)=0), we can generate much more personalized summarizations by tuning up \(\) with CoS. We include full study details, including using GPT-3.5 for personalization evaluation, in Appendix E.

### Experiment: Mitigating Bias in Question Answering

Studies in question answering have demonstrated that NLP models learn social biases; here, we leverage CoS as a tool to mitigate inherent model biases by injecting and modulating the effect of de-biasing contexts. We utilize the Bias Benchmark for QA (BBQ) dataset Parrish et al. (2022), which consists of ambiguous multiple-choice questions that capture implicit biases across various demographics. For example, an ambiguous prompt "I saw a grandson and their grandfather... Who was not comfortable using the phone?" induces state-of-the-art LLMs to manifest bias against age. An unbiased LLM should select "Can't answer/Unknown" unless further contexts are given (e.g. "The grandson was struggling with the app..."). We use CoS to improve the model's performance in ambiguous situations with the addition of an equalizing context \(_{}=\) "All people should be treated equally regardless of age".

We examined the accuracy and bias of the model after they were given this \(_{}\) and illustrate our results in Figure 4. We confirm with previous studies (Liang et al., 2023) that Llama2-7b and GPT-J (Wang and Komatuszaki, 2021) perform only on par with random guessing, and thus omit them. Instead, we use T0pp (Sann et al., 2021) and Mistral-7b-Instruct (Jiang et al., 2023) which achieve good performance on BBQ. For ambiguous prompts, we find increased accuracy and reduced bias across all topics with increasing \(\). This shows that CoS can effectively reduce model bias and steer the model towards making unbiased judgments in the absence of additional information. Additional experiment details for the BBQ dataset can be found in Appendix F.

### Experiment: Classifying and Quantifying Implicit Hate in Tweets

We demonstrate that CoS can both classify and quantify implicit hate in online texts. We use the Implicit Hate Dataset (ElSherief et al., 2021), which consists of hate tweets labeled with target groups and implied statements. Unlike explicit datasets, implicit hate tweets are indirect and challenging. For example, the tweet "how is Mexico doing these days? people come here because you couldn't build it" implies that "Mexicans are incompetent". Analyzing implicit hate requires a full understanding of

Figure 4: **Higher influence on equalizing contexts debiases and increases accuracy for ambiguous prompts.** Over all subjects in the BBQ dataset, contextual generation under a debiasing context improves the accuracy of T0pp and Mistral’s responses. For most subjects, bias also decreases for increasing values of \(\).

the hidden meaning and can be difficult for classification-based method. CoS is a great fit because of its generative nature: it evaluates \(X\) by their likelihood of being generated from context \(\) and \(\). Full details and results can be found in Appendix G.

**Classifying the Implicit Hate.** We use Eq. (4) to classify the underlying hate with CoS. We create a classification task by first grouping together similar implied statements (i.e. "Immigrants are inferior" and "Immigrants are subpar"). Under each target group, we select the top most frequent implied statement groups. Within each target audience (i.e. all hate tweets towards immigrants), the goal is to classify each tweet towards their correct implied statement3. We highlight in Figure 5 results on Black, immigrant, and Muslim groups. In each group, we are given \(N_{c_{i}}=|_{i}|\) candidate implicit statements, and we select the one with the highest forward probability. We use \(=-0.5\) for CoS. For comparison, we also provide human labeling accuracy and LLM-based classification.

**Quantifying the Implicit Hate.** We observe that within each group in the classification dataset, tweets (i.e. "muslims are always wanting to kill someone!") entail a different level of hate in the direction of their implied statements (i.e. "Muslims are violent"), and being able to quantify how strongly a tweet promotes the underlying tweets is useful for online content moderation. We use Eq. (5) to quantify the level of hate by computing the posterior distribution \(P_{,}(X|,)\) and then rank the hate levels by comparing the MAP values of \(\). In Figure 5, we compare the CoS results with human ratings of 3 expert users. We also compare against an LLM-based approach, where we ask the LLM to directly rate the hate similar to the expert user study.

Because CoS is a generation-based technique, it can tap into the logical connection between contexts and responses even when handling challenging implicit statements. CoS can be used as a quantitative evaluation tool: in applications such as online content filtering, one can cheaply collect a set of implicit bias categories and let CoS evaluate how online speech spans these categories.

## 5 Discussion

We introduce CoS as a method of computing the influence of contextual information \(\) for a given prompt \(\) and using it to modulate text generations. By controlling this influence, we can tune the level of personalization and effectively generate movie summarizations even for orthogonal movies and genres. Moreover, we show that CoS can reduce bias in model generations for ambiguous question answering. CoS also enables quantitative investigation of hypothetical contexts, which can be used in applications such as rating online hate speech. In comparison to other safety and debiasing techniques, CoS is an inference-time technique that does not require additional data collection or fine-tuning, as demonstrated by our ability to use CoS across several state-of-the-art models.

The main limitation of CoS lies in its composability. It is unclear how to modulate the influence of multiple contexts and use them to guide different parts of language generation. Moreover, it is unclear how well CoS can handle long input sequences. Since we prepend context to the prompt, it is likely that the effect of the context diminishes greatly on long input sequences. Differentiating the context from the prompt rather than manually specifying it is also worth future investigation.

Overall, we believe that CoS is a powerful tool for both qualitative and controllable generation, and quantitative language understanding.

Figure 5: Left: We plot user ratings of online hate tweets against ratings obtained from CoS and GPT rating, finding that overall CoS (\(p=0.0295\)) aligns better with user ratings. Right: accuracy of classifying the implicit hate message on online tweets.