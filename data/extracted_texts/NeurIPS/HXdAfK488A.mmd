# Doing Experiments and Revising Rules

with Natural Language and Probabilistic Reasoning

 Wasu Top Piriyakulkij\({}^{1}\)1

&Cassidy Langenfeld\({}^{1}\)

&Tuan Anh Le\({}^{2}\)

&Kevin Ellis\({}^{1}\)

Cornell University\({}^{1}\)

&Google\({}^{2}\)

###### Abstract

We give a model of how to infer natural language rules by doing experiments. The model integrates Large Language Models (LLMs) with Monte Carlo algorithms for probabilistic inference, interleaving online belief updates with experiment design under information-theoretic criteria. We conduct a human-model comparison on a Zendo-style task, finding that a critical ingredient for modeling the human data is to assume that humans also consider fuzzy, probabilistic rules, in addition to assuming that humans perform approximately-Bayesian belief updates. We also compare with recent algorithms for using LLMs to generate and revise hypotheses, finding that our online inference method yields higher accuracy at recovering the true underlying rule, and provides better support for designing optimal experiments.

## 1 Introduction

An important way that humans grow their knowledge of the world is by experimentation and other forms of active learning. This process is most clearly present in the experimental sciences, but similar processes of active inference begin in infancy through early childhood [1; 2; 3; 4; 5]. Within everyday adult cognition, active experimentation helps us quickly learn to use new devices and tools.

A basic framework for modeling experimentation is to alternate between conducting a good experiment, and updating one's beliefs based on those experimental results . These beliefs concern a latent _hypothesis_ about the regularity or trend the experimenter is investigating. This leaves open at least two computational questions. First, we need to define a hypothesis space. Second, we need efficient algorithms for belief updates and experiment generation. Such algorithms should reason about probabilistic beliefs--considering many hypotheses and their associated probabilities--in order to find experiments that optimally resolve different competing hypothesis.

Here we will introduce a model that represents hypotheses in natural language--even for problems that do not intrinsically involve human language. We do this for two reasons. First, natural language can index many human concepts, and can recursively combine them, giving an expressive hypothesis space. Second, it allows using Large Language Models (LLMs) to aid the inference task of updating beliefs after each experiment, giving tractable, approximate probabilistic inference when we view the LLM as a proposal distribution for a Monte Carlo estimator.

We are especially interested in comparing our model to human behavior, given the long legacy of probabilistic modeling within cognitive science [7; 8]. We find a nuanced picture: vanilla LLMs are not humanlike on our active learning tasks (and underperform humans); our full model outperforms humans; but a simple change--switching from deterministic to probabilistic hypotheses--allows matching humans in overall performance, and agreement with humans on more fine-grained metrics.

From a technical perspective, our work needs to infer natural-language hypotheses in an online setting, so that it can cycle between experimentation and hypothesis formation. This differs from recentbatched approaches for hypothesis formation [9; 10; 11]. To allow online inference, we hybridize LLMs with Sequential Monte Carlo Samplers (SMC-S: ). In SMC-S, one tracks a modest number of hypotheses that serve as (approximate) samples from the posterior. Meanwhile, the LLM focuses the sampler on a small set of candidate hypotheses that it deems relevant, given the data. The resulting sampler facilitates active learning by choosing an experiment which optimally "splits" the candidate hypotheses. With strategies that do not use probabilistic framing, such as tracking a single best-guess hypothesis, the active learner would have little guidance on what experiment to do next.

We will focus here on active inference of basic symbolic concepts expressible in natural language, as we believe these are tractable first targets of study. Concretely, we consider tasks in the spirit of the boardgame 'Zendo', a challenging but accessible game where human players actively learn binary rules combining logical and spatial relations [13; 14; 15], as well as 'Blacket test' style tasks, inspired by studies in developmental psychology [16; 2; 17] that investigate how children learn the causal mechanism behind the activation of a machine. See Figure 1.

We contribute the following:

1. An algorithm for probabilistic inference of latent natural language hypotheses. This derives from SMC-S, but uses an LLM proposal distribution to allow tractable inference over natural language strings, essentially using the LLM to suggest ways of revising the belief state.
2. Model-Human/Model-Baseline comparisons, finding that (1) we get a better fit to human data using natural language, instead of formal languages; (2) the model can be further made more humanlike by considering fuzzy (probabilistic) rules, and (3) that our online inference also yields better accuracy at the actual task relative to recent work [10; 9; 11].
3. Empirical findings about the ability of LLMs to revise hypotheses and propose experiments. On the domains we consider, we find that LLMs are effective for proposing and revising hypotheses, but do not consistently outperform random guessing when proposing experiments.

## 2 Model

We start with standard Bayesian optimal experiment design, which gives a framework for describing both experimentation and hypothesis formation [18; 19]. Our model includes natural-language hypotheses \(h^{*}\), experiments \(x\), and experiment outcomes \(y\). We consider equipping \(h\) with real-valued parameters \(\): For example, if the hypothesized rule is fuzzy (noisy), then \(\) would control the noise level. As new experiments are proposed sequentially, we index experiments and outcomes with subscripts, i.e. \(x_{t}\) and \(y_{t}\) for the \(t^{}\) experiment and outcome, respectively. The objective is to identify ground-truth \(h^{*}\), and to accurately predict the outcome of future experiments.

Figure 1: Alternation of experimentation and hypothesis generation on a simplified version of our ActiveACRE domain. Hypotheses characterizes what causes the machine to activate (make noise).

The joint distribution over hypothesis \(h,\) and outcomes \(y_{1:T}\), given experiments \(x_{1:T}\), is

\[p(h,y_{1:T},|x_{1:T})=p(h)p()_{1 t T}p(y_{t}|x_{t},h,)\] (1)

where the prior \(p(h)\) favors shorter or simpler hypotheses. From eq. (1) the posterior is

\[p(h|x_{1:T},y_{1:T}) p(h)_{}p()_{1 t T}p(y_ {t}|x_{t},h,)\] (2)

where we assume the above integral is tractable, because \(\) is low-dimensional. Ultimately, the purpose of the hypothesis is to make predictions on new experiments. Given a test experiment \(x_{}\), an ideal learner predicts an outcome \(y_{}\) distributed as follows:

\[p(y_{}|x_{},x_{1:T},y_{1:T})=_{h}p(h|x_{1:T},y_{1:T} )_{}p(|h,x_{1:T},y_{1:T})p(y_{}|x_{},h, )\] (3)

The optimal experiment for identifying \(h\) maximizes the following information gain :

\[x^{*}=*{arg\,max}_{x}*{}_{p (y|x_{1:T},y_{1:T},x)}[D_{}(p(h|x_{1:T},y_{1:T},x,y)||p(h|x_{1:T},y_{ 1:T}))]\] (4)

The above computations are intractable because they involve considering the infinitely large set of all hypotheses and experiments. We next describe our LLM-guided approximation methods.

### Revising Rules: Online Inference

We introduce a generalization of the Sequential Monte Carlo Sampler (SMC-S) , an online approximate inference algorithm which tracks a small pool of hypotheses--called particles--that evolve over time as new data is collected. Tracking representative high-posterior particles allows approximate inference (eq. (2)) and prediction (eq. (3)) by only considering the current particles. This makes the model "boundedly rational" : as the bound on computation (# particles) grows large, the sampler better approximates optimal inference. To the extent that our work offers a cognitive model, we are claiming that humans only consider a small number of hypotheses, which evolve in ways that approximate probabilistic reasoning. This should be seen within the tradition of using approximate inference methods to give mechanistic accounts of human learning [22; 23; 24; 25].

Standard SMC-S tracks \(n\) particles at each time point \(t\), written \(H_{t}=\{h_{t}^{(i)}\}_{i=1}^{n}\). Each particle has a weight, \(W_{t}=\{w_{t}^{(i)}\}_{i=1}^{n}\), giving the approximate posterior \(p(h|x_{1:t},y_{1:t})_{i}w_{t}^{(i)}[h=h_{t}^{(i)}]\). Upon observing a new data point, the particles \(H_{t}\) are pushed through a forward kernel \(q_{t+1}(h_{t+1}|h_{t}^{(i)})\), which randomly perturbs the particles, to obtain new particles \(H_{t+1}\). Next, the particles are reweighed to obtain \(W_{t+1}\). Finally, a resampling step can be executed to prune low-weight particles and multiply high-weight particles.

Figure 2: Sequential Monte Carlo method tracks a small number of hypotheses (called particles), each of which is a natural language rule, represented above by circles. After each experiment, the particles are revised in light of the new data by pushing the particles through the forward kernel. Then, the new particles are reweighed according to how well each explains the data we have seen so far. Resampling prunes low-probability hypotheses while multiplying high-probability ones.

Here \(h\) is a natural language string, suggesting an LLM should define \(q_{t}(h_{t}|h_{t-1}^{(i)})\). For example, an LLM can be prompted with a hypothesis, together with the latest experiment outcome, and asked to revise that hypothesis. But calling an LLM to perturb every single particle is expensive, and unnecessary for hypotheses that already explain the data well.

We therefore design a variant of SMC-S whose forward kernel looks globally at the current set of particles and prompts an LLM to revise the worst (lowest-likelihood) particles, while keeping unchanged the best (highest-likelihood) particles. This concentrates the computation on improving bad hypotheses, instead of wasting effort altering what already works. Within the context of LLMs, this can be seen as an online, probabilistic version of hypothesis refinement . Within the context of SMC-S, this mathematically corresponds to defining a forward kernel that conditions on the entire set of previous particles and all seen data points, \(q_{t}(h_{t}|H_{t-1},x_{1:t},y_{1:t})\).2 Below we formalize our new SMC-S variant, which we call LLM-SMC-S, illustrated in Figure 2.

**Procedure: LLM-SMC-S (A.4).** Given \(H_{t},W_{t}\) where \(p(h|x_{1:t},y_{1:t})_{i}w_{t}^{(i)}1[h=h_{t}^{(i)}]\):

1. Define unnormalized target densities \((h)=p(h,y_{1:t},x_{1:t})\) and \(^{}(h)=p(h,y_{1:t+1},x_{1:t+1})\).
2. Sample \(h^{} q_{t+1}(|H_{t},x_{1:t+1},y_{1:t+1})\) (i.e., using LLM to revise hypotheses)
3. Compute the weight \(w^{}\) for \(h^{}\) following \[w^{}=,H_{t},W_{t})}{q_{t+1}(h^{}|H_{t},x_{1:t+1},y_{1:t+1})}A(h^{},H_{t},W_{t})=_{i=1}^{n}w_{t}^{(i)} (h^{})r(h_{t}^{(i)}|h^{})}{(h_{t}^{(i) })}\] (5) with the reverse kernel \(r(h|h^{})\) defined as uniform up to strings of a maximum length.
4. Repeat steps 2-3 (sampling/weighing) a total of \(n\) times, and normalize the weights. Optionally, resample to generate an unweighted posterior (we always resample).
5. Output: \(H_{t+1}\) and \(W_{t+1}\), formed from \(n\) samples of \(h^{},w^{}\) with \(w^{}\) normalized from step 4, which approximate \(p(h|x_{1:t+1},y_{1:t+1})\).

The correctness of the above procedure is most easily understood using the following definition:

**Definition: Proper Weighting**. Let \((h)\) be an unnormalized target density, which we can evaluate. Let the corresponding normalized target density be \((h)=}\) where \(Z_{}=(h)h\) is the normalization constant. A weighted particle \(h,w\) is properly weighted with respect to \(\) if for any function \(f\),

\[E[wf(h)]=Z_{}E_{(h)}[f(h)]\]

**Proposition 1**.: If \(H,W\) input to Procedure LLM-SMC-S is properly weighted with respect to \(\), then the output \(h^{},w^{}\) is properly weighted with respect to \(^{}\). (Proof in Appendix A.1.)

### Doing Experiments: Active Learning

Our active learning works by doing an experiment that maximizes information gain (eq. (4)). Experiments may be complex, such as involving putting objects or instruments in specific positions, and there might be combinatorially many possible experiments. For a rich space of experiments, a bounded learner--human or AI--cannot consider all possibilities.

We will propose experiments using an LLM, but then reassess those proposals under probabilistic criteria. Particularly, we provide an LLM with the hypotheses tracked by the SMC-S sampler at each iteration, and prompt it to generate experiments that support and falsify each hypothesis. Empirically, this process yields a diverse pool of experiments. We take the best experiment proposed by the LLM, as measured under the approximate posterior from SMC-S:

\[x_{t+1}=*{arg\,max}_{x(H_{t})}*{ }_{(y|x,x_{1:t},y_{1:t})}[D_{}((h|x_{1:t}, y_{1:t},x,y)||(h|x_{1:t},y_{1:t}))]\] (6)

where \(\) is approximated with the weighted particles from SMC-S.3

### Instantiating the model

All of our experiments have binary outcomes (\(y\{0,1\}\)), and all of our natural language hypotheses correspond to rules that predict whether an experiment succeeds or fails (1 or 0). Although the rules predict hard all-or-none judgments, a learner can relax that constraint by assuming that the underlying rule is fuzzy (noisy). Many natural language facts and rules actually only partly hold, such as _birds fly_ (almost always true), or _birds lay eggs_ (true half the time). To handle the possibility of fuzzy rules, we equipped each hypothesized rule with real-valued parameters \(\) that control the noise level. The noise parameters decompose into a pair \(=(,)\) controlling the rate of false-positives/false-negatives:

\[p(y=1|x,h,,)=[&h(x)=1\\ 1-&h(x)=0]\]

Under this formulation, hard rules corresponds to \(p()\) and \(p()\) having non-zero probability only at value 1. For probabilistic, fuzzy rules, we use Gaussian priors for \(p()\) and \(p()\), truncated to [0.5,1], and with a bias toward larger \(\). The prior \(p(h)\) is defined as inversely proportional to wordcount, giving a gentle bias toward parsimony. We investigate both hard and fuzzy rules in our experiments.

Evaluating \(h(x)\) requires checking the natural language string \(h\) against experiment \(x\), for which we use GPT-3.5 to translate the natural language \(h\) to code which is run on \(x\). We use GPT-4 Turbo to propose hypotheses . Recent studies find a similar breakdown of LLMs works well [9; 10; 11].

## 3 Experimental Results

**Domains.** **Zendo** is a game where a player seeks to infer a hidden binary rule about scenes of colored shapes. Our Zendo games begin with showing the player a positive example scene, followed by 7 rounds of experimentation, where the player builds a scene, and receives feedback on if the scene obeys the hidden rule. After the experimentation phase, players are tested on 8 test scenes, half of which follow the hidden rule. Our setup follows Bramley et al., but modified for LLMs by presenting scenes as text describing each block by its color, size, orientation, groundedness, and what other blocks it touches and stacks (Figure 3).

Our second domain, **ActiveACRE**, derives from The Abstract Causal REasoning (ACRE) dataset , which in turn derives from 'blicket' tests in developmental cognitive psychology . The original ACRE is a causal induction dataset where each task is to figure out what causes the 'blicket' machine to make sounds when multiple objects are put on the machine. We add active learning to ACRE: rather than passively observe examples, our ActiveACRE allows the player to try 7 experiments, after passively witnessing the outcome of one experiment involving eight objects. The player is then tested (without further feedback) on all possible combinations of the original eight objects.

**Model-Baseline comparisons.** Table 1 contrasts the performance of different models, showing that online inference with hard rules outperforms all other models on both datasets, including a ReAct-style baseline  (Direct LLM), and batched inference with refinement, an approach advocated for in recent work [10; 9]. To measure accuracy on Zendo, we compute the predictive posterior accuracy summed over the 8 test scenes and averaged over all tasks. Because the test set on ActiveACRE

Figure 3: (a) Example Zendo scene and its serialization into text. (b) Eight experiments, each of which is a scene, with a binary outcome (whether the scene makes stars come out of it). (c) Test scenes that evaluate whether a model or human has correctly inferred the hidden rule.

[MISSING_PAGE_FAIL:6]

rules on their own do not suffice to explain human judgments: Only by combining with online probabilistic inference do we begin to explain the data.

**Why reason in natural language instead of a formal language?** Many Bayesian models account for human concept learning using probabilistic reasoning over formal languages such as logic . Instead, our model operates over natural language. This helps address two liabilities of formal representations: expressivity and tractability. A handcrafted formal language is often insufficiently expressive, accidentally excluding many human concepts. This expressivity must be limited because, although there exist highly expressive formal languages, in practice, inference in such languages is generally intractable--a tradeoff partly addressed by using LLM proposal distributions.

To illustrate these points, we study a new Zendo rule--'the majority of blocks is red'--which is not expressible in the formal language introduced by . We collect new human data in an IRB-approved study. Figure 6 shows that both humans and our model correctly learn this rule \(30\%-40\%\) of the time. This indicates both the model and humans are able to represent this rule in their hypothesis space, which is unrepresentable in a formal language designed specifically for Zendo.

Figure 4: Human vs model accuracy binned by 4 rule-following (RF) and 4 not rule-following (Not RF) test scenes. (a) Each point is a RF or Not RF accuracy for the 10 rules. (b) Rows/columns are methods/rules. Online inference with fuzzy rules (last row) most closely matches humans.

Another reason to use natural language representations is that LLMs, trained on human-generated data, may to some extent capture human bias, judgement, and opinions . Unlike approaches based on estimating probabilities on formal languages, incorporating LLMs into our models might therefore make them display more human-like behaviors--as shown in earlier sections--without access to additional human data. Indeed, Table 2 shows that our best-performing model surpasses 's model on human data log likelihood even though the latter fits their models on both human active queries and predictions, while our model does not perform such parameter fitting.

Bounded rationality.To understand the effect of computational cost on the results, we analyze performance and human-model fit while varying the computational budget, as measured by LLM calls. Figure 7 plots human-model fit as compute budget varies (see also Table 5). We observe an (inverted) U-shaped curve: Too little budget gives a bad fit, but overshooting also degrades fit. This result aligns with the theory of bounded rationality , which argues for considering human's limited cognitive resources, and with the rational analysis of human processing limitations .

What makes good experiments: LLMs, or Information Gain?We first study the importance of the information gain objective (Table 3), contrasting three different active learning methods: _LLM_ (prompting with the hypotheses and asking for a good experiment); _Random_ (handcoded random generator), and _InfoGain_ (main method, with LLM proposing experiments). Substituting InfoGain with alternative methods significantly degrades model performance. Reranking LLM proposals with information gain is important, and an LLM--on its own--does not generate experiments that are as effective.

Is this explained by the strength of the LLM experiment proposer, or by the strength of the InfoGain objective? While earlier results support LLMs' effectiveness as hypothesis proposers, Table 4 demonstrates that a random proposer, hand-designed under reasonable assumptions, performs similarly to an LLM experiment proposer. This finding is in line with  which argues that LLMs may not always produce the most useful set of candidate questions.

    &  \\   & 1 & 5 & 10 \\  Random proposer & \(5.84 0.19\) & \(6.23 0.16\) & \(6.55 0.28\) \\ LLM proposer & \(5.73 0.16\) & \(6.19 0.12\) & \(6.55 0.13\) \\   

Table 4: Average predictive posterior (standard error computed over 5 seeds) of online inference with hard rules model with different experiment proposers on different number of candidate experiments on Zendo.

Figure 5: Comparing human and model prediction on each test scene after 7 rounds of experimentation; see also Table 2. Each point is a prediction on a test scene. We only present LLM, best batch model, and best online model here. Please see the figure for all methods at Figure 14.

   Active learning method & Inference method \\   & Online, Fuzzy & Online, Hard \\  LLM & \(4.52 0.08\) & \(4.72 0.08\) \\ Random & \(5.03 0.11\) & \(5.84 0.19\) \\ InfoGain & \(\) & \(\) \\   

Table 3: Average predictive posterior (standard error computed over 5 seeds) of online inference models with different active learning methods on Zendo.

Figure 7: \(R^{2}\) score of human vs model accuracy at different computational budgets. A LLM call batch-samples 15 hypotheses.

Related Work

Bayesian Concept Learning in Cognitive Science.Bayesian models of few-shot learning of concepts and categories has a long legacy [32; 34; 42; 43; 44; 13; 14; 15; 33; 35; 45; 46; 47; 36]. On Zendo, [13; 14; 15] also engineers a probabilistic context-free grammar to define the Bayesian model to explain Zendo human data; inference is intractable, and they study various approximate inference algorithms on the task. Our work opts for natural language as the hypothesis space for its expressiveness and leverages LLMs to help with approximate inference.

Inductive Reasoning with Large Language Models.There are a number of recent works on inductive reasoning with LLMs [10; 9; 56; 11; 57]. [10; 9] study the inductive reasoning ability of vanilla LLMs and propose a simple refinement algorithm to improve performance.  explicitly treats LLMs as importance samplers and shows that their model, with prior learned from human data and importance sampling as their inference algorithm, is human-like on some domains. Our work frames many of these works as batch inference--in the spirit of importance sampling, from a probabilistic view--which is compared in our results. Additionally, we model the full life-cycle of experimentation and hypothesis revision.

Active Learning with Large Language Models.The problem of performing active learning with the help of LLMs has been studied under the task of asking better questions with LLMs [58; 59; 60; 41; 61]. GATE  directly prompts LLMs to ask open-ended, informative questions. Other works [59; 60; 41] use LLMs to help propose several candidate questions, and then use expected information gain to select the question to be asked. Following these previous works, we investigate the relevance of classic criteria from active learning such as expected information gain.

Probabilistic Inference with Large Language Models.The framework of probabilistic inference has been applied to LLM-based algorithms. Language-model cascades  provides a unifying framework for seeing recent inference-time LLM algorithms [63; 64; 65] as reasoning with probabilistic programs. Other work  fine-tunes CoT models by formulating the problem as maximum marginal likelihood where the marginalization over the latent chain of thought is done via probabilistic inference. We use an LLM as an aid for SMC-S, but others have explored using SMC as an aid for LLM decoding [67; 68], which although technically very different, is conceptually complementary.

## 5 Limitations and Next Steps

The work presented is limited in important ways that suggest next steps. Most immediately, many of the the hypotheses we consider are simple and stereotyped in form, and much of the promise of using natural language is that it exposes a rich, expressive representation for combining and creating new ideas . More ambitiously, hypothesis generation, both in science and in everyday thinking, often involves conjecturing the existence of unseen objects, not just unknown regularities, and incorporating this abductive thinking--which is absent from our model--could open many directions.

Although the work here is most directly an account of human behavior within the context of the game Zendo, our model is also more broadly inspired by the mental activities of experimental scientists as they build theories and models, weigh hypotheses, and design experiments. Two basic features of our approach reflect scientific experimentation and theory building. First, scientific theories apply only within a particular regime. For example, many equations in physics only apply when objects are moving slowly, and many thermodynamic equations only apply at equilibrium. Outside this regime, the theory is no longer predictive. Similarly, a variant of a model like ours could hypothesize fuzzy, probabilistic rules which either predict a category with high confidence, or outside the regime in which the rule applies, can fail to make a decisive prediction.

The second way in which this work reflects the practices of experimental science is that it builds its hypotheses via an incremental evolutionary process. In this way, our model is best thought of as performing what is sometimes called 'normal science' -- where one works within an existing paradigm, and considers piecemeal evidence -- and does not model paradigm shifts  (what a scientific theorist might pursue) or deeper conceptual changes  (what happens during child development), both of which require deep reanalysis of a broad batch of past data, rather than online incremental revision.

Acknowledgements

We are grateful to Neil Bramley and Jan-Philipp Franken for providing the Zendo data, helpful discussions, and comments on the manuscript. We also thank Hao Tang, Simon Alford, Celine Lee, and the anonymous reviewers for valuable comments on the manuscript. This work was supported by an NSF CAREER grant.