# RanDumb: Random Representations Outperform

Online Continually Learned Representations

 Ameya Prabhu\({}^{1}\)1 & Shiven Sinha\({}^{2*}\)1 & Ponnurangam Kumaraguru\({}^{2}\) & Philip H.S. Torr\({}^{1}\)

Ozan Sener\({}^{3+}\)1 & Puneet K. Dokania\({}^{1+}\)1 &1University of Oxford &2IIIT Hyderabad &3Apple

https://github.com/drimpossible/RanDumb

###### Abstract

Continual learning has primarily focused on the issue of catastrophic forgetting and the associated stability-plasticity tradeoffs. However, little attention has been paid to the efficacy of continually learned representations, as representations are learned alongside classifiers throughout the learning process. Our primary contribution is empirically demonstrating that existing online continually trained deep networks produce inferior representations compared to a simple pre-defined random transforms. Our approach projects raw pixels using a fixed random transform, approximating an RBF-Kernel initialized before any data is seen. We then train a simple linear classifier on top without storing any exemplars, processing one sample at a time in an online continual learning setting. This method, called RanDumb, significantly outperforms state-of-the-art continually learned representations across all standard online continual learning benchmarks. Our study reveals the significant limitations of representation learning, particularly in low-exemplar and online continual learning scenarios. Extending our investigation to popular exemplar-free scenarios with pretrained models, we find that training only a linear classifier on top of pretrained representations surpasses most continual fine-tuning and prompt-tuning strategies. Overall, our investigation challenges the prevailing assumptions about effective representation learning in online continual learning.

Figure 1: Our primary analysis in this work is ablating the deep feature extractor (bottom center) by replacing it with a random projection (top center) to isolate the effect of online continual representation learning in the deep network. We demonstrate that random projections not only match but consistently outperform continually learned representations, highlighting the poor quality of the continually learned representations. RanDumb (top) maps raw pixels to a high-dimensional space using random Fourier projections, then decorrelates the features using the Mahalanobis distance  and classifies based on the nearest class mean.

Introduction

Continual learning aims to develop models capable of learning from non-stationary data streams, inspired by the lifelong learning abilities exhibited by humans and the prevalence of such real-world applications (see Verwimp et al.  for a survey). It is characterized by sequentially arriving tasks, coupled with additional computational and memory constraints .

Building on the foundations of supervised deep learning, the prevalent approach in continual learning has been to jointly train representations alongside classifiers. This approach simply follows from the assumption that learned representations are expected to outperform fixed representation functions such as kernel classifiers, as demonstrated in supervised deep learning . However, this assumption is never validated in continual learning, with scenarios having limited updates where networks might not be trained until convergence, such as online continual learning (OCL).

In this paper, we study the efficacy of representations derived from continual learning algorithms. Surprisingly, our findings suggest that these representations might not be as beneficial as presumed. To test this, we introduce a simple baseline method named RanDumb, which combines a random representation function with a straightforward linear classifier, illustrated in detail in Figure 1. Our empirical evaluations, summarized in Table 1 (left, top), reveal that despite replacing the representation learning with a pre-defined random representation, RanDumb surpasses current state-of-the-art methods in latest online continual learning benchmarks .

We further expand our evaluations to scenarios incorporating methods that use pre-trained feature extractors . By substituting our random projections with these feature extractors and retaining the linear classifier, RanDumb again outperforms leading methods as shown in Table 1 (right, top).

### Technical Summary: Construction of RanDumb and Empirical Findings

_Design._ RanDumb first projects input pixels into a high-dimensional space using a fixed kernel based on random Fourier basis, which is a low-rank data-independent approximation of the RBF Kernel . Then, we use a simple linear classifier which first normalizes distances across different feature dimensions (anisotropy) with Mahalanobis distance  and then uses nearest class means for classification . In scenarios with pretrained feature extractors, we use the fixed pretrained model as embedder and learn a linear classifier as described above, similar to Hayes and Kanan .

_Key Properties._ RanDumb needs no storage of exemplars and requires only one pass over the data in a one-sample-per-timestep fashion. Furthermore, it only requires online estimation of the sample covariance matrix and nearest class mean.

**Key Finding 1: _Poor Representation Learning._** We compare RanDumb with leading methods: VAE-GC  in Table 1 (left, middle) and SLCA  in Table 1 (right, middle). The primary distinction between them is their representation: RanDumb uses a fixed function (random/pretrained network),

  
**Method** & **MNIST** & **CIFAR10** & **CIFAR100** & **m-MN** & **Method** & **CIFAR** & **IN-A** & **UN-R** & **CUB** & **OB** & **VTAB** & **Cars** \\   \\  Best (PEC) & 92.3 & 58.9 & 26.5 & 14.9 & Best (RanPAC-imp) & 89.4 & 33.8 & 69.4 & 89.6 & 75.3 & 91.9 & 57.3 \\ RanDumb (Ours) & 98.3 & 55.6 & 28.6 & 17.7 & RanDumb (Ours) & 86.8 & 42.2 & 64.9 & 88.5 & 75.3 & 92.4 & 67.1 \\ Improvement & +6.0 & -3.3 & +2.1 & +2.8 & Improvement & -2.6 & +8.4 & -4.5 & -1.1 & +0.0 & +0.5 & +9.8 \\   \\  VAE-GC & 84.0 & 42.7 & 19.7 & 12.1 & SLCA & 86.8 & - & 54.2 & 82.1 & - & - & 18.2 \\ RanDumb (Ours) & 98.3 & 55.6 & 28.6 & 17.7 & RanDumb (Ours) & 86.8 & 42.2 & 64.9 & 88.5 & 75.3 & 92.4 & 67.1 \\ Improvement & +14.3 & +12.9 & +8.9 & +5.6 & Improvement & +0.0 & - & +10.7 & +6.4 & - & +48.9 \\   \\  Joint (One Pass) & 98.3 & 74.2 & 33.0 & 25.3 & Joint & 93.8 & 70.8 & 86.6 & 91.1 & 83.8 & 95.5 & 86.9 \\ RanDumb (Ours) & 86.8 & 42.2 & 64.9 & 88.5 & 75.3 & 92.4 & 67.1 \\ Gap Covered. (\%) & 93\% & 60\% & 75\% & 97\% & 92\% & 97\% & 77\% \\   

Table 1: **(Left) Online Continual Learning.** Performance comparison of RanDumb on the PEC setup  and VAE-GC . Setup and numbers borrowed from PEC . RanDumb outperforms the best OCL method. **(Right) Offline Continual Learning.** Performance comparison with ImageNet21K ViT-B16 model using 2 initial classes and 1 new class per task. RanPAC-imp is an improved version of the RanPAC code which mitigates the instability issues in RanPAC. RanDumb nearly matches performance of joint for both online and offline, demonstrating the inefficacy of current benchmarks.

whereas VAE-GC and SLCA further continually trained deep networks. RanDumb consistently surpasses VAE-GC and SLCA by wide margins of 5-15%. This shows that state-of-the-art online continual learning algorithms fail to learn effective representations across standard exemplar-free continual learning benchmarks.

**Finding 2: _Over-Constrained Benchmarks._** Given the demonstrated limitations of existing continual representation learning methods, an important question arises: Can better methods learn more effective representations? To explore this, we evaluated the performance of RanDumb against joint training, models trained without continual learning constraints, in both online and offline settings, as shown in Table 1 (left, bottom) and Table 1 (right, bottom). Our straightforward baseline, RanDumb, bridges 70-90% of the performance gap relative to the respective joint classifiers in both scenarios. This significant recovery of performance by such a simple method suggests that if our goal is to advance the study of representation learning, current benchmarks may be overly restrictive and not conducive to truly effective representation learning.

We highlight that the goal in our work is not to introduce a state-of-the-art continual learning method, but challenge prevailing assumptions and open a discussion on the efficacy of representation learning in continual learning algorithms, especially in online and low-exemplar scenarios.

## 2 RanDumb: Mechanism & Intuitions

RanDumb has two main elements: random projection and the dumb learner. We illustrate the mechanism of RanDumb using three toy examples in Figure 2. To classify a test sample \(_{}\), we start with a simple classifier, the nearest class mean (NCM). It predicts the class among \(C\) classes by highest value of the similarity function \(f\) among class means \(_{i}\):

\[y_{}=*{arg\,max}_{i\{1,,|C|\}}f(_ {},_{i}), f(_{},_ {i}):=_{}{}^{}_{i}\] (1)

and \(_{i}\) are the class-means in the pixel space: \(_{i}=|}_{ C_{i}}\). RanDumb adds two additional components to this classifier: 1) Kernelization and 2) Decorrelation.

**Kernelization:** Classes are typically not linearly separable in the pixel space, unlike in the feature space of deep models. Hence, we randomly project the pixels into a high-dimensional representation space, computing all distances between the data and class-means in this embedding space. This phenomena is illustrated on three toy examples to build intuitions in Figure 2 (Embed). We use an RBF-Kernel, which for two points \(\) and \(\) is defined as: \(K_{}(,)=(-\|-\|^{2})\) where \(\) is a scaling parameter. However, calculating the RBF kernel is not possible due to the online continual learning constraints preventing computation of pairwise-distance between all points. We use a data-independent approximation, random Fourier projection \(()\), as given in :

\[K_{}(,)()^{T}()\]

Figure 2: RanDumb projects the datapoints to a high-dimensional space to create a clearer separation between classes. Subsequently, it corrects the anisotropy across feature dimensions, scaling them to be unit variance each. This allows cosine similarity to accurately separates classes. The figure is adapted from .

where the random Fourier features \(()\) are defined by first sampling \(D\) vectors \(\{_{1},,_{D}\}\) from a Gaussian distribution with mean zero and covariance matrix \(2\), where \(\) is the identity matrix. Then \(()\) is a \(2D\)-dimensional feature, defined as:

\[()=}[(_{1}^{T}),( _{1}^{T}),..,(_{D}^{T}),(_{D}^{T })]\]

We keep these \(\) bases fixed throughout online learning. Thus, we obtain our modified similarity function from Equation 1 as:

\[f(_{},_{i}):=(_{})^{} _{i}\] (2)

where \(_{i}\) are the class-means in the kernel space:

\[_{i}=|}_{ C_{i}}()\]

**Decorrelation:** Projected raw pixels have feature dimensions with different variances (anisotropic). Hence, instead of naively computing \((_{})^{}_{i}\), we further decorrelate the feature dimensions using a Mahalonobis distance with the shrinked covariance matrix \(\) using OAS shrinkage , inverse obtained by least squares minimization (\(+\)). We illustrate this phenomena as well on three toy examples in Figure 2 (Decorrelate) to build intuitions. Our similarity function finally is:

\[f(_{},_{i}):=((_{})- {}_{i})^{T}^{-1}((_{})-_{i})\] (3)

_Online Computation._ Our random projection is fixed before seeing any data. During continual learning, we only perform online update on the running class mean and empirical covariance matrix2.

## 3 Experiments

We compare RanDumb with algorithms across online continual learning benchmarks with an emphasis on exemplar-free and low-exemplar storage regime. All numbers in tables with the caption (Ref: table and citation) except our method are taken from the aforementioned table in the cited paper.

_Benchmarks._ The benchmarks which we used in our experiments are summarized in Table on the right. We aim for a comprehensive coverage and show results on four standard online continual learning benchmarks (A, B, D, E) which reflect the latest trends ('22-'24) across exemplar-free, contrastive-traininging3, meta-continual learning, and network-expansion based approaches respectively. We also evaluate on a rehearsal-free offline continual learning benchmark C. These benchmarks are ordered by increasingly relaxed constraints, moving further away from the training scenario of RanDumb.Benchmark A closely matches RanDumb with one class per timestep and no stored exemplars. Benchmark B, D, E progressively relax the constraints on exemplars and classes per timestep. Benchmark C and E remove the online constraint by allowing unrestricted training and sample access within a task without exemplar-storage of past tasks. Benchmark F allows using large pretrained models, modified by us with one class per task, i.e. testing learning over longer timespans.

We further test on exemplar-free scenarios in offline continual learning using Benchmark F  with the challenging one-class per task constraint borrowed from . This benchmark allows using pretrained models along with unrestricted training time and access to all class samples at each timestep. However, RanDumb is restricted to learning from a single pass seeing only one sample at a time. RanDumb only learns a linear classifier over a given pretrained model in Benchmark F.

[MISSING_PAGE_FAIL:5]

with Stanford Cars as ObjectNet license prohibits training models. We use the 768-dimensional features from the same pretrained ViT-B models used in this benchmark. We measure accuracy on the test set of all past seen classes after completing the full one-pass. We take the average accuracy after the last task on all past tasks [76; 25; 68]. In Benchmark A and F, since we have one class per task, the average accuracy across past tasks is the same regardless of the task ordering. In Benchmarks A-E, all datasets have the same number of samples, hence similarly the average accuracy across past tasks is the same regardless of the task ordering. We used the Scikit-Learn implementation of Random Fourier Features  with 25K embedding size, \(=1.0\). We use progressively increasing ridge regression parameter (\(\)) with dataset complexity, \(=10^{-6}\) for MNIST, \(=10^{-5}\) for CIFAR10/100 and \(=10^{-4}\) for TinyImageNet200/miniImageNet100.

### Results

**Benchmark A (single-class per task).** We assess continual learning models in the challenging setup of one class per timestep, closely mirroring our training assumptions, and present our results in Table 2. Comparing across rows, and see that RanDumb improves over prior state-of-the-art across all datasets with 2-6% margins. The only exception is PEC on CIFAR10, where RanDumb underperforms by 3.3%. Nonetheless, it outperforms the second-best model, GDumb with a 500 memory size, by 4.9%.

**Benchmark B.1 (many-classes per task).** We present our results comparing with non-contrastive methods in Table 3. We notice that scenario allows two classes per task and relaxes the memory constraints for online continual learning methods, allowing for higher accuracies compared to Benchmark A. Despite that, RanDumb outperforms latest OCL algorithms on MNIST, CIFAR10 and CIFAR100--often by margins exceeding 10%. The lone exception is GDumb achieving a higher performance with 2K memory samples on TinyImageNet, indicating that this already is in the high-memory regime.

**Benchmark B.2 (many-classes per task, with contrastive losses and data augmentations).** We additionally compare our performance with the latest OCL approaches using contrastive losses with sophisticated data augmentations. As shown in in Table 4 (Left), these advancements provide large performance improvements over methods from Benchmark B.1. To compensate, we compare on lower exemplar budgets. The best approach, OnPro , outperforms RanDumb on CIFAR10 by 2.2% and TinyImageNet by 0.3%, but falls significantly short on CIFAR100 by 5.9%. Overall, RanDumb achieves strong results compared to representation learning using state-of-the-art contrastive learning approaches customized to continual learning, despite storing no exemplars.

**Benchmark C (rehearsal-free).** We compare against offline rehearsal-free continual learning approaches in Table 4 (Right) on CIFAR100. Despite online training, RanDumb outperforms PredKD by over 4% margins.

**Benchmark D (meta-continual learning).** We compare performance of RanDumb against meta-continual learning methods, which require large exemplars with buffer sizes of 1K in Table 5 (left). RanDumb achieves strong performance under these conditions, exceeding all prior work by a large margin of 9.1% on CIFAR100 and outperforms all but VR-MCL approach on the TinyImageNet dataset. GDumb performs the best on CIFAR10, indicating this is already in a large-exemplar regime uniquely unsuited for RanDumb.

  
**Method** & **MNIST (5/2)** & **CIFAR10 (5/2)** & **CIFAR100 (10/10) TinyImageNet (100/2)** \\  & \(M=0.1k\) & \(M=0.1k\) & \(M=0.5k\) & \(M=1k\) \\  SCR  & 86.2\(\)0.5 & 40.2\(\)1.3 & 19.3\(\)0.6 & 8.9\(\)0.3 \\ OCM  & 90.7\(\)0.1 & 47.5\(\)1.7 & 19.7\(\)0.5 & 10.8\(\)0.4 \\ OnPro  & - & **57.8\(\)1.1** & 22.7\(\)0.7 & **11.9\(\)0.3** \\   \\  RanDumb & **98.3** (+7.5) & 55.6 (-2.2) & **28.6** (+5.9) & 11.6 (-0.3) \\    
  
**Method** & **CIFAR100** (10/10) \\    \\  PredKD  & 24.6 \\ PredKD + FeatKD & 12.4 \\ PredKD + EWC & 23.3 \\ PredKD + L2 & 21.5 \\ RanDumb (Ours) & **28.6** (+4.0) \\   

Table 4: **(Left) Benchmark B.2 (_Ref: Table from OnPro )_ We compare RanDumb with contrastive representation learning based approaches which additionally use sophisticated augmentations. We observe that RanDumb often outperforms these sophisticated methods despite all of these factors on small-exemplar settings. (Right) Benchmark C (_Ref: Table 2 from ). We compare RanDumb with latest rehearsal-free methods. RanDumb outperforms them by 4% margin.

**Benchmark E (network-expansion).** We compare RanDumb against network expansion-based online continual learning methods in Table 5 (right). These approaches grow model capacity to mitigate forgetting while dealing with shifts in the data distribution, and are allowed larger memory buffers. RanDumb matches the performance of the state-of-the-art method SEDEM  on MNIST, while exceeding it by 0.3% on CIFAR10 and 3.8% on CIFAR100.

### Analysis of RanDumb

**Ablating Components of RanDumb.** We ablate the contribution of only using Random Fourier features for embedding and decorrelation to the overall performance of RanDumb in Table 6 (left, top). Ablating the decorrelation and relying solely on random Fourier features, colloquially dubbed Kernel-NCM, has performance drops ranging from 6-25% across the datasets. Replacing random Fourier features with raw features, _ie._ the SLDA baseline, leads to pronounced drop in performance ranging from 3-14% across the datasets. Moreover, ablating both components results in the base nearest class mean classifier, and exhibits the poorest performance with an average reduction of 17%. Therefore, both decorrelation and random embedding are crucial for RanDumb.

**Impact of Embedding Dimensions.** We vary the dimensions of the random Fourier features ranging from compressing 3K input dimensions to 1K to projecting it to 25K dimensions and evaluate its impact on performance in Figure 3. Surprisingly, the random projection to a 3x compressed 1K dimensional space allows for significant performance improvement over not using embedding, given in Table 6 (left, top). Furthermore, increasing the dimension from 1K to 25K results in improvements of 3.6%, 10.4%, 7.0%, and 2.5% on MNIST, CIFAR10, CIFAR100, and TinyImageNet respectively. Increasing the embedding sizes beyond 15K, however, only results in modest improvements of 0.1%, 1.4%, 1.1% and 0.2% on the same datasets, indicating 15K dimensions would be a good point for a performance-computational cost tradeoff.

  
**Method** &  **CIFAR10** \\ (5/2) \\  &  **CIFAR100** \\ (20/10) \\  & 
 **CIFAR100** \\ (5/2) \\  \\  Finetune & 17.0 \(\) 0.6 & 5.3 \(\) 0.3 & 3.9 \(\) 0.2 \\ LWF  & 18.8 \(\) 0.1 & 5.6 \(\) 0.4 & 4.0 \(\) 0.3 \\ A-GEM  & 18.4 \(\) 0.2 & 6.0 \(\) 0.2 & 4.0 \(\) 0.2 \\ IS  & 17.4 \(\) 0.2 & 5.2 \(\) 0.2 & 3.3 \(\) 0.3 \\ MEB  & 36.9 \(\) 2.4 & – & – \\ La-MAML  & 33.4 \(\) 1.2 & 11.8 \(\) 0.6 & 6.7 \(\) 0.4 \\ GDumb  & **61.2**\(\) 1.0 & 18.1 \(\) 0.3 & 4.6 \(\) 0.3 \\ ER  & 43.8 \(\) 4.8 & 16.1 \(\) 0.9 & 11.1 \(\) 0.4 \\ DER  & 29.9 \(\) 2.9 & 6.1 \(\) 0.1 & 4.1 \(\) 0.1 \\ DER+\(\{\) & 52.3 \(\) 1.9 & 11.8 \(\) 0.7 & 8.3 \(\) 0.3 \\ CLSER  & 52.8 \(\) 1.7 & 17.9 \(\) 0.7 & 11.1 \(\) 0.2 \\ OCM  & 53.4 \(\) 1.0 & 14.4 \(\) 0.8 & 4.5 \(\) 0.5 \\ ER-OBC  & 54.8 \(\) 2.2 & 17.2 \(\) 0.9 & 11.5 \(\) 0.2 \\ VR-MCL  & 56.5 \(\) 1.8 & 19.5 \(\) 0.7 & **13.3 \(\) 0.4** \\    \\  RanDumb (Gurs) & 55.6 (-5.6) & **28.6** (+9.1) & 11.6 (-1.7) \\   

Table 5: **(Left) Benchmark D (_Ref: Table 2 from VR-MCL _) We compare RanDumb with meta-continual learning approaches operating in a high memory setting, allowing buffer sizes up to 1K exemplars. RanDumb outperforms all methods except VR-MCL on TinyImageNet. RanDumb also surpasses all prior work by a substantial 9.1% on CIFAR100. Allowing generous replay buffers shifts scenarios to a high exemplar regime where GDumb performs the best on CIFAR10. Yet RanDumb competes favorably even under these conditions. (Right) Benchmark E (_Ref: Table 1 from SEDEM _) We compare RanDumb with network expansion based approaches. Despite allowing access to much larger memory buffers, RanDumb matches the performance of best method SEDEM on MNIST, while exceeding it by 0.3% on CIFAR10 and 3.8% on CIFAR100.**

Figure 3: Accuracy of RanDumb with respect to embedding dimensionality across datasets.

**Impact of Flip Augmentation.** We evaluate the impact of adding the flip augmentation on the performance of RanDumb in Table 6 (left, middle). Note that MNIST was not augmented. Augmentation provided large gains of 3.1% on CIFAR10, 1.7% on CIFAR100, and 0.4% on TinyImageNet. We did not augment the data further with RandomCrop transform as done with standard augmentations.

**Impact of Varying Ridge Parameter.** All prior experiments use a ridge parameter (\(\)) that increases with dataset complexity: \(=10^{-6}\) for MNIST, \(10^{-5}\) for CIFAR10 and CIFAR100, and \(10^{-4}\) for TinyImageNet and miniImageNet. Table 6 (left, middle) shows the effect of varying \(\) on RanDumb's performance. With a smaller \(=10^{-6}\), CIFAR10, CIFAR100, TinyImageNet and miniImageNet all exhibit minor drops of 0.1%-1.7%, 0.8%, 0.8%. Increasing shrinkage to a \(=10^{-4}\) reduces CIFAR10 and CIFAR100 performance more substantially by 3% and 2.5% versus their optimal \(=10^{-5}\). On the other hand, this larger \(\) leads to improvements of 0.5% and 1.8% on TinyImageNet and miniImageNet. This aligns with the trend that datasets with greater complexity benefit from more regularisation, with the optimal \(\) balancing under- and over-regularisation effects.

**Comparison with Extreme Learning Machines.** We compared our random Fourier features with random projections based extreme learning machines, as recently adapted to continual learning by RP+ReLU  in Table 6 (left, bottom) with their best embedding size. Our method performs significantly better on each dataset, averaging a gain of 3.4%.

**Comparisons across Architectures.** In table 6 (right), we compare whether using random Fourier features as embeddings outperforms models across various architectures for continual representation learning. We use experience replay (ER) baseline in the task-incremental CIFAR100 setup (for details, see Mirzadeh et al.  as it differs significantly from earlier setups). Our comparison spanned various architectures. The findings revealed that RanDumb surpassed the performance of nearly all considered architectures, and achieved close to 94% of the joint multi-task performance. This suggests that RanDumb outperforms continual representation learning across architectures.

**Conclusion.** Overall, both random embedding and decorrelation are critical components in the performance of RanDumb. Using random Fourier features is substantially better than RanPAC. Lastly, one can substantially reduce the embedding dimension without a large drop in performance for large gains in computational cost, additional augmentation may further significantly help performance and optimal shrinkage parameter increases with dataset complexity. RanDumb outperforms continual representation learning across a wide range of architectures.

  
**Method** & **MNIST** & **CIFAR10** & **CIFAR100** & **T-ImNet** & **m-ImNet** \\  & (10/1) & (10/1) & (10/1) & (200/1) & (100/1) \\   \\  RanDumb & 98.3 & 55.6 & 28.6 & 11.1 & 17.7 \\ -Decorrelate & 83.8 (-14.5) 30.0 (-25.6) & 12.0 (-16.6) & 4.7 (-6.4) & 8.9 (-8.8) \\ -Embed & 88.0 (-10.3) 41.6 (-14.0) & 19.0 (-9.6) & 8.0 (-3.1) & 12.9 (-4.8) \\ Both & 82.1 (-16.2) 28.5 (-27.1) & 10.4 (-18.2) & 4.1 (-7.0) & 7.28 (-10.4) \\   \\  With & - & 55.6 & 28.6 & 11.1 & 17.7 \\ Without & 98.3 & 52.5 (-3.1) & 26.9 (-1.7) & 10.7 (-0.4) & 16.6 (-1.1) \\   \\  \(=10^{-6}\) & 98.3 & 53.9 & 27.8 & 10.3 & 15.8 \\ \(=10^{-5}\) & - & 55.6 & 28.6 & 11.1 & 15.9 \\ \(=10^{-4}\) & 96.6 & 52.6 & 26.1 & 11.6 & 17.7 \\   \\  No-Embed & 88.0 & 41.6 & 19.0 & 8.0 & 12.9 \\ RP+ReLU (RanPAC) & 95.2 & 48.8 & 23.1 & 9.7 & 15.7 \\ RanDumb (Ours) & 98.3 (+3.1) & 55.6 (+6.8) & 28.6 (+5.5) & 11.1 (+1.4) & 17.7 (+2.0) \\   

Table 6: **(Left) Analysis of RanDumb**: We study contributions of decorrelation, random embedding, and data augmentation. We further vary the embedding sizes and regularisation parameter. Finally, we compare with alternate embeddings. (Right) **Architectures**_(Ref: Table 1 from Mirzadeh et al. )_ RanDumb surpasses continual representation learning across a wide range of architectures, achieving close to 94% of the joint performance.

### Should we learn representations when strong pre-trained features are available?

Say for a specific application (e.g., where the test data distribution is more or less known during training), practitioners should use strong pretrained models as a starting point as they are likely to perform better. However, we still ask the key question of whether representation learning is necessary by fixing the pre-trained backbone and only training the linear classifier, as illustrated in Figure 4 in the next benchmark.

**Benchmark F.** We compare performance of approaches which do not further train the deep network like Ran-Dumb against popular continual finetuning and prompt-tuning approaches in Table 7. We discover that prompt-tuning approaches completely collapse under large timesteps and approaches which do not finetune their pretrained model achieve strong performance, even under challenging one class per timestep constraint. Note that RanPAC  adds a RP+ReLU and finetunes in a first-session adaptation fashion over RanDumb, yet fails to achieve higher accuracies.

**Why is the performance of prompting methods so low?** We argue that the true test of continual learning lies in the ability to learn over prolonged periods. Our observations indicate that prompting methods collapse early across tasks because the learned prompts do not generalize effectively. Supporting this, parallel work  suggests that most current prompt-tuning methods lack prompt di

  
**Method** &  \\   \\  Finetune & 1.0 & 1.2 & 1.1 & 1.0 & 2.1 \\ L2P  & 2.4 & 0.3 & 0.8 & 1.4 & 1.3 \\ DualPrompt  & 2.3 & 0.3 & 0.8 & 0.9 & 4.2 \\ CODA-Prompt  & 2.6 & 0.3 & 0.8 & 1.9 & 6.3 \\ Adam-Adapt  & 76.7 & 49.3 & 62.0 & 85.2 & 83.6 \\ Adam-SSF  & 76.0 & 47.3 & 64.2 & 85.6 & 84.2 \\ Adam-VPT  & 79.3 & 35.8 & 61.2 & 83.8 & 86.9 \\ Adam-FT  & 72.6 & 49.3 & 61.0 & 85.2 & 83.8 \\ Memo  & 69.8 & - & - & 81.4 & - \\ iCARL  & 72.4 & - & 35.2 & 72.4 & - \\ Foster  & 52.2 & - & **76.8** & 86.6 & - \\ NCM  & 78.3 & 44.3 & 62.5 & 84.8 & 88.2 \\ SLCA  & 86.3 & - & 52.8 & 84.7 & - \\ RanPAC  & **88.2** & 39.0 & 72.8 & 77.7 & 93.0 \\ RanPAC-imp  & 87.8 & 43.5 & 72.6 & **89.6** & 93.0 \\ RanDumb (Ours) & 84.5 & **49.5** & 66.9 & 88.0 & **93.6** \\   \\  Finetune & 2.8 & 0.5 & 1.2 & 1.2 & 0.5 \\ Adam-Adapt  & 82.4 & 48.8 & 55.4 & 86.7 & 84.4 \\ Adam-SSF  & 82.7 & 46.0 & 59.7 & 86.2 & 84.9 \\ Adam-VPT  & 70.8 & 34.8 & 53.9 & 84.0 & 81.1 \\ Adam-FT  & 65.7 & 48.5 & 56.1 & 86.5 & 84.4 \\ Foster  & 87.3 & - & 5.1 & 86.9 & - \\ iCARL  & 71.6 & - & 35.1 & 71.6 & - \\ NCM  & 83.5 & 41.4 & 54.8 & 86.5 & 88.5 \\ SLCA  & 86.8 & - & 54.2 & 82.1 & - \\ RanPAC  & 89.6 & 26.8 & 67.3 & 87.2 & 88.2 \\ RanPAC-imp  & **89.4** & 33.8 & **69.4** & **89.6** & 91.9 \\ RanDumb (Ours) & 86.8 & **42.2** & 64.9 & 88.5 & **92.4** \\   

Table 7: **Benchmark F We compare RanDumb with prompt-tuning approaches using ViT-B/16 ImageNet-21K/1K pretrained models using 2 init classes and 1 class per task setting. Most prompt-tuning based methods collapse and RanDumb achieves either state-of-the-art or second-best performance. RanPAC-imp is an improved version of the RanPAC mitigating the instability issues identified in a previous version of this work.**

Figure 4: In previous experiments, models were trained from scratch, so we used random projection. Here, with a pretrained backbone, RanDumb starts with the frozen pretrained backbone to explore whether continual representation learning is necessary. By comparing this frozen backbone (RanDumb) with a continually trained one, we show that using the pretrained features consistently matches the best continually learned representations, similarly challenging the value of continual representation learning.

versity and can be characterized by a single prompt, making classification performance heavily reliant on the quality of that prompt. We hypothesize when prompts are designed for a large number of classes (e.g., 20 or 50), they produce generally discriminative representations that extend to future tasks. However, prompts designed for only two classes, as in our case, have limited discriminative power, leading to the collapse of prompt-tuning methods across tasks.

Overall, despite RanDumb being exemplar-free, it outperforms nearly all online continual learning methods across various tasks when exemplar storage is limited. We specifically benchmark on lower exemplar sizes to complement settings in which GDumb does not perform well.

## 4 Related Works

**Random Representations.** There have been extensive theoretical and empirical investigations into random representations in machine learning, compressed sensing, and other fields, often utilizing extreme learning machines [56; 14; 21] (see [30; 29] for a survey). Other investigations include efficient kernel methods using Fourier features and Nystrom approximations [52; 70], and extensions to efficiently parameterize linear classifiers . They are also embedded into deep networks [17; 35; 73; 16]. We tailored the already successful random fourier representations  to the problem at hand and applied to the online continual learning problem for the first time.

**Continual Representation Learning.** There are various works focusing on continual representation learning itself [53; 20; 39; 28], but they address the problem of alleviating the stability-plasticity dilemma in high-exemplar and offline continual learning scenarios where models are trained until convergence. In comparison, we focus on online and low-examplar regime.

**Representation Learning Free Methods in CL.** Several works have developed the idea of using fixed pretrained networks after adapting on the first task across various settings [50; 41; 24]. Our work contributes to this growing evidence, however, we do not perform first-task adaptation , and propose OAS-shrinked SLDA as structurally simplest but highly accurate continual linear classifier without any extra bells-and-whistles. Moreover, we are the first work to introduce a representation learning free method with random features for continually learning from scratch.

**Equivalent formulations to RanDumb.** If the classes are equiprobable, which is the case for most datasets here, nearest class mean classifier with the Mahalanobis distance metric is equivalent to linear discriminant analysis (LDA) classifier . Hence, one could say RanDumb is exactly equivalent to a Streaming LDA classifier with an approximate RBF Kernel. Alternatively, one could think of the decorrelation operation as explicitly decorrelating the features with ZCA whitening .

## 5 Discussion and Concluding Remarks

Our investigation reveals a surprising result -- simply using random embedding (RanDumb) consistently outperforms learned representations from methods specifically designed for online continual training. Furthermore, using random/pretrained features also recovers 70-90% of the gap to joint learning, leaving limited room for improvement in representation learning techniques on standard benchmarks. Overall, our investigation questions our understanding of how to effectively design and train models that require efficient continual representation learning, and necessitates a re-investigation of the widely explored problem formulation itself. We believe adoption of computationally bounded scenarios without memory constraints and corresponding benchmarks [51; 50; 22] could be a promising way forward.

**Limitations & Future Directions.** We currently do not provide theory or justification for why training dynamics of continual learning algorithms fails to effectively learn good representations; doing so would provide deeper insights into continual learning algorithms. Moreover, our proposed method, RanDumb with random Fourier features is limited in scope towards low-exemplar scenarios and online-continual learning. Extending studies on representation learning to high-exemplar and offline continual learning scenarios might be exciting directions to investigate.

**Social Impact.** RanDumb is an algorithm solely designed to perform a scientific study and we do not recommend use of RanDumb for any application in real-world production systems, hence no direct societal impact or explicit limitations on use in production systems is discussed.