# Multi-Fidelity Multi-Armed Bandits Revisited

Xuchuang Wang

Chinese University of Hong Kong

xcwang@cse.cuhk.edu.hk&Qingyun Wu

Pennsylvania State University

qingyun.wu@psu.edu

&Wei Chen

Microsoft Research

weic@microsoft.com&John C.S. Lui

Chinese University of Hong Kong

cslui@cse.cuhk.edu.hk

###### Abstract

We study the multi-fidelity multi-armed bandit (MF-MAB), an extension of the canonical multi-armed bandit (MAB) problem. MF-MAB allows each arm to be pulled with different costs (fidelities) and observation accuracy. We study both the best arm identification with fixed confidence (BAI) and the regret minimization objectives. For BAI, we present (a) a cost complexity lower bound, (b) an algorithmic framework with two alternative fidelity selection procedures, and (c) both procedures' cost complexity upper bounds. From both cost complexity bounds of MF-MAB, one can recover the standard sample complexity bounds of the classic (single-fidelity) MAB. For regret minimization of MF-MAB, we propose a new regret definition, prove its problem-independent regret lower bound \((K^{1/3}^{2/3})\) and problem-dependent lower bound \((K)\), where \(K\) is the number of arms and \(\) is the decision budget in terms of cost, and devise an elimination-based algorithm whose worst-cost regret upper bound matches its corresponding lower bound up to some logarithmic terms and, whose problem-dependent bound matches its corresponding lower bound in terms of \(\).

## 1 Introduction

The multi-armed bandits (MAB) problem was first introduced in the seminal work by Lai and Robbins  and was extensively studied (ref. Bubeck and Cesa-Bianchi , Lattimore and Szepesvari ). In stochastic MAB, the decision maker repeatedly pulls arms among a set of \(K^{+}\) arms and observes rewards drawn from unknown distributions of the pulled arms. The initial objective of MAB is _regret minimization_, where the regret is the cumulative differences between the rewards from pulling the optimal arm and that from the concerned algorithm's arm pulling strategy. This is a fundamental sequential decision making framework for studying the exploration-exploitation trade-off, where one needs to balance between optimistically exploring arms with high uncertainty in reward (exploration) and myopically exploiting arms with high empirical reward average (exploitation). Another task of MAB is the best arm identification (BAI, a.k.a., pure exploration), later introduced by Even-Dar et al. , Mannor and Tsitsiklis . Best arm identification aims to find the best arm _without_ considering the cumulative regret (reward) during the learning process. In this paper, we focus on the fixed confidence case, with the goal of identifying the best arm with a fixed confidence with as few number of decision rounds as possible.

In this paper, we investigate a generalized MAB model with a wide range of real-world applications--the _multi-fidelity multi-armed bandit_ (MF-MAB)--introduced by Kandasamy et al. . We study both the regret minimization and best arm identification objectives under the MF-MAB model. The MF-MAB model introduces flexibility for exploring arms: instead of squarely pulling an arm to observe itsreward sample as in MAB, MF-MAB offers \(M^{+}\) different accesses to explore an arm, called \(M\) fidelities. Pulling an arm \(k\{1,,K\}\) at fidelity \(m\{1,,M\}\), the decision maker pays a cost of \(^{(m)}\) and observes a reward \(X_{k}^{(m)}\) whose mean \(_{k}^{(m)}\) is not too far away from the arm's true reward mean \(_{k}\). More formally, \(|_{k}^{(m)}-_{k}|^{(m)}\) where \(^{(m)} 0\) is the error upper bound at fidelity \(m\). The formal definition of MF-MAB is presented in Section 2.

### Contributions

In Section 3, we first look into the best arm identification with fixed confidence (BAI) objective in MF-MAB, which has wide applications in hyperparameter optimization (Kandasamy et al., 2017, SS1.4) and neural architecture search (NAS) (Kandasamy et al., 2018). In the context of NAS, each arm of MF-MAB corresponds to one configuration of neural network architecture, and the different fidelities of pulling this arm correspond to training the neural network of this configuration with different sample sizes of training data (see details in Section 2.1). As in MF-MAB pulling an arm at different fidelities has different costs, we consider the total cost needed by an algorithm and refer to it as _cost complexity_ (a generalization of sample complexity). We first derive a cost complexity lower bound \((_{k}_{m:_{k}^{(m)}>0}(^{(m)}/(_ {k}^{(m)})^{2})^{-1}),\) where the \(_{k}^{(m)}\) is the reward gap of arm \(k\) at fidelity \(m\) (defined in Eq.(1)) and \(\) is the confidence parameter. We then devise a Lower-Upper Confidence Bound (LUCB) algorithmic framework with two different fidelity selection procedures and prove both procedures' cost complexity upper bounds. The first procedure focuses on finding the optimal fidelity suggested by the minimization in lower bound via an upper confidence bound (UCB) algorithm, which may pay high additional costs during searching the optimal fidelities. The second procedure, instead of identifying the optimal fidelity, seeks for good fidelities that are at least half as good as the optimal ones so as to reduce the cost for identifying the optimal fidelity while still enjoying fair theoretical performance.

In Section 4, we next study the regret minimization objective in MF-MAB. We introduce a novel definition of regret, wherein the rewards obtained are dependent on the pulled arms, while the selected fidelities solely affect the accuracy of the observed rewards. The new regret definition covers applications like product management and advertisement distribution (Kandasamy et al., 2017, 2018) where the distributed advertisement (ad) determines the actual (but unknown) reward while the cost (fidelity) spent by the company on evaluating the impact of this ad decides the accuracy of the observed rewards (see Remark 4.1). The difference of our regret definition and the one studied by Kandasamy et al. (2018) is discussed in Remark 4.2. We first propose both a problem-independent (a.k.a., worst-case) regret lower bound \((K^{1/3}^{2/3})\) and a problem-dependent regret lower bound \((K)\) where the \((>0)\) is the decision budget. We then devise an elimination-based algorithm, which explores (and eliminates) arms in the highest fidelity \(M\) and exploits the remaining arms in the lowest fidelity \(1\). The algorithm enjoys a \((K^{1/3}^{2/3})\) problem-independent regret upper bound, which matches the corresponding lower bound up to some logarithmic factor, and also a problem-dependent bound matching its corresponding lower bound tightly in a class of MF-MAB instances.

### Related Works

Multi-fidelity multi-armed bandits (MF-MAB) was first proposed by Kandasamy et al. (2018). They studied a cumulative regret minimization setting whose regret definition is different from ours (see Remark 4.2 for more details). Later, Kandasamy et al. (2018) extended MF-MAB to bandits optimization, i.e., in continuous action space, with the objective of minimizing simple regret, and Kandasamy et al. (2018) further extended Kandasamy et al. (2018) to the continuous fidelity space with the same objective of minimizing simple regret. We note that the simple regret minimization in bandits optimization corresponds to the best arm identification with fixed budget in multi-armed bandits, and, therefore, is different from the cumulative regret minimization and best arm identification with fixed confidence objectives studied in this paper. During the submission of this work, the authors were aware that the BAI task under MF-MAB model was also studied by Poiani et al. (2019), where they proposed a similar cost complexity lower bound to our Theorem 3.1 and their proposed algorithm is similar to our Explore-C procedure in Appendix D. Except for these two results, all other results proposed in this paper, including two BAI algorithms in Section 3.2 and the regret minimization results in Section 4, are novel.

The multi-fidelity optimization was first introduced in simulating expensive environments via cheap surrogate models [14; 11], and, later on, was extended to many real-world applications, e.g., shape design optimization for ship  or wing , and wind farm optimization . Recently, the multi-fidelity idea was employed in hyperparameter optimization (HPO) for automated machine learning (autoML) [16; 26; 9; 25]. The fidelity may correspond to training time, data set subsampling, and feature subsampling, etc. These multi-fidelity settings help agents to discard some hyperparameter configurations with low cost (a.k.a., early discarding [28; SS3.1.3]). Jamieson and Talwalkar  modeled hyperparameter optimization as non-stochastic best arm identification and applied the successive halving algorithm (SHA) to address it. Li et al.  introduced Hyperband, a hyperparameter configuring and resources allocation method which employed SHA as its subroutine to solve real HPO tasks. Falkner et al.  proposed BOHB, which use Bayesian optimization method to select and update hyperparameters and employ SHA to allocate resources. Li et al.  extended SHA to asynchronous case for parallel hyperparameter optimization. This line of works was based on the non-stochastic best arm identification problem  and, therefore, is very different from our stochastic modelling.

## 2 Model

We consider a \(K(^{+})\)-armed bandit. Each arm can be pulled with \(M(^{+})\) different fidelities. When an arm \(k:=\{1,,K\}\) is pulled at fidelity \(m:=\{1,,M\}\), one pays a cost \(^{(m)}(>0)\) and observes a feedback value drawn from a \([0,1]\)-supported probability distribution with mean \(_{k}^{(m)}\). We assume the non-trivial case that \(|_{k}^{(m)}-_{k}^{(M)}|^{(m)}\), where \(^{(m)} 0\) is the observation error upper bound at fidelity \(m\). Without loss of generality, we assume \(^{(1)}^{(M)}\); otherwise, we can relabel the fidelity so that the \(^{(m)}\) is non-decreasing with respect to \(m\). We call the reward mean of an arm at the highest fidelity \(M\) as this arm's _true reward mean_ and, without loss of generality, assume that these true reward means are in a descending order \(_{1}^{(M)}>_{2}^{(M)}_{3}^{(M)} _{K}^{(M)}\), and arm \(1\) is the unique optimal arm.

For the online learning problem, we assume that the reward distribution and the mean \(_{k}^{(m)}\) of each arm \(k\) at fidelity \(m\) are unknown, while the costs \(^{(m)}\)'s and the error upper bounds \(^{(m)}\)'s are known to the learning agent.1 To summarize, a MF-MAB instance \(\) is parameterized by a set of tuples \((,,(^{(m)})_{m},(^{(m)})_{m },(_{k}^{(m)})_{(k,m)})\), with elements described above.

In this paper, we consider two tasks on the above multi-fidelity bandit model: best arm identification and regret minimization. We will define these two tasks in the respective sections below.

### Application

One typical application of the best arm identification problem is hyperparameter optimization [15; 6] (including neural architecture search) for machine learning, in which the goal is to identify the best hyperparameter configuration--the training set-up for a machine learning model attaining the best predictive performance--with as low resource as possible. A mapping between concepts in this application and MF-MAB is discussed as follows. **Arm:** Hyperparameter configurations of machine learning models, e.g., neural network architectures. **Reward:** Predictive performance of the resulting machine learning model trained based on the selected configuration (arm). **Fidelity dimension:** For a particular hyperparameter configuration (arm), one typically has the choice to determine a certain level of resources to allocate for training the model. The concept of "training resource" can be considered the fidelity dimension. More concretely, commonly used training resources include _the number of epochs_ and _the training data sample_, both of which satisfy our cost assumption. For example, the larger the number of epochs or training data samples, the more expensive to train the model.

**Remark 2.1** (On the assumptions of multi-fidelity feedback in the hyperparameter optimization application).: Observation error upper bound \(^{(m)}\) is the maximum distance from resources allocated to the terminal validation loss. According to benchmarked results in two recent benchmarks for multi-fidelity hyperparameter optimization, including HPOBench and YAHP0 Gym, under the typically used fidelity dimension, including the number of epochs, the training data sample, etc., the maximum distance from the terminal validation loss often decreases with the increase of resources, i.e., \(^{(m)}\) decreases with the increase of \(m\). Thanks to these benchmarks, it is also convenient to know \(^{(m)}\) under different fidelities \(m\) for commonly used types of fidelity dimension.

## 3 Best Arm Identification with Fixed Confidence

In this section, we study the best arm identification with fixed confidence (BAI) task in the multi-fidelity multi-armed bandits (MF-MAB) model. The objective of BAI is to minimize the total budget spent for identifying the best arm with a confidence of at least \(1-\). As the cost of pulling an arm in MF-MAB depends on the chosen fidelity, we use the total cost, instead of total pulling times (sample complexity), as our criterion, and refer to it as _cost complexity_. We first present a cost complexity lower bound in Section 3.1, then propose a LUCB algorithmic framework with two alternative procedures in Section 3.2, and analyze their cost complexity upper bounds in Section 3.3. Lastly, we introduce concrete applications of the BAI problem in Section 2.1.

### Cost Complexity Lower Bound

We present the cost complexity lower bound in Theorem 3.1. Its proof is deferred to Appendix E.1. During the submission of this paper, we notice that a similar cost complexity lower bound was also proposed by Poiani et al. (2010, Theorem 1).

**Theorem 3.1** (Cost complexity lower bound).: _For any algorithm addressing the best arm identification with fixed confidence \(1-\) for any parameter \(>0\), any number of arms \(K\), any number of fidelities \(M\) with any observation error upper bound sequence \((^{(1)},^{(2)},,^{(M)})(^{(M)}=0)\) and any cost sequence \((^{(1)},,^{(M)})\), and any \(K\) fidelity subsets \(\{_{1},_{2},,_{K}\}\) where the \(_{k}\) is a subset of full fidelity set \(\) containing the highest fidelity \(M\), i.e., \(M_{k}}\), for all \(k\), there exists a MF-MAB instance such that_

\[[](_{m_{1}}}{(_{1}^{(m)},_{2}^{(M)}+^{(m)})}+_{k 1}_{m _{k}}}{(_{k}^{(m)},_{1}^{(M)} -^{(m)})}),\]

_where \(\) is the KL-divergence between two probability distributions, \(_{k}^{(m)}\) is the probability distribution associated with arm \(k\) when pulled at fidelity \(m\), and \(\) means to positively/negatively shift the distribution \(\) by an offset \(\)._

According to the KL-divergences' two inputs in the above lower bound, we define the reward gap \(_{k}^{(m)}\) of arm \(k\) at fidelity \(m\) as follows,

\[_{k}^{(m)}_{1}^{(M)}-(_{k}^{(m)}+^{(m )})& k 1\\ (_{1}^{(m)}-^{(m)})-_{2}^{(M)}& k=1.\] (1)

For any suboptimal arm \(k 1\), the gap \(_{k}^{(m)}\) quantifies the distance between the optimal arm's reward mean \(_{1}^{(M)}\) and the suboptimal arm's reward mean upper bound at fidelity \(m\), i.e., \(_{k}^{(m)}+^{(m)}\); and for the optimal arm \(1\), the gap \(_{1}^{(m)}\) represents the distance between the optimal arm's reward mean lower bound at fidelity \(m\), i.e., \(_{1}^{(m)}-^{(m)}\), and the second-best arm's reward mean \(_{2}^{(M)}\).

**Remark 3.2**.: If all reward distributions are assumed to be Bernoulli or Gaussian and let \(_{k}=\{m:_{k}^{(m)}>0\}\), the regret bound can be simplified as

\[[] C_{k}_{m:_{k}^{(m)}> 0}}{(_{k}^{(m)})^{2}},\] (2)

where \(C>0\) is a constant depending on the reward distributions assumed. Especially, we denote \(m_{k}^{*}_{m:_{k}^{(m)}>0}}{( _{k}^{(m)})^{2}}\), and with some algebraic transformations, it can be expressed as

\[m_{k}^{*}=_{m}^{(m)}}{}}.\] (3)With \(m_{k}^{*}\), the lower bound in Eq.(2) can be rewritten as \([] C_{k}(^{(m_{k}^{*})}/( _{k}^{(m_{k}^{*})})^{2})(1/)\), and we define the coefficient as \(H_{k}^{(m_{k}^{*})}/(_{k}^{(m_{k}^{*}) })^{2}\). The \(m_{k}^{*}\) can be interpreted as the _optimal (most efficient) fidelity_ for exploring arm \(k\). We note that the current lower bound in Eq.(2) does not contain the cost of finding this \(m_{k}^{*}\). This cost can be observed in our algorithm's cost complexity upper bound stated in Section 3.3.

### Algorithm Design

In this subsection, we propose a Lower-Upper Confidence Bound (LUCB) algorithmic framework with two alternative procedures which employ different mechanisms to select suitable fidelities for arm exploration. Generalized from the original (single-fidelity) LUCB algorithm , the LUCB algorithmic framework in SS3.2.1 determines two critical arms (the empirical optimal arm \(_{t}\) and second-best arm \(u_{t}\)) for exploration in each time slot \(t\). However, in MF-MAB, with the critical arms suggested by the LUCB framework, one still needs to decide the fidelities for exploring the critical arms. This fidelity selection faces an accuracy-cost trade-off, i.e., higher fidelity (accuracy) but suffering higher cost, or lower fidelity (accuracy) but enjoying lower cost. This trade-off is different from the common exploration-exploitation trade-off in classic bandits. Because the accuracy and costs are two orthogonal metrics, while in exploration-exploitation trade-off, there is only a single regret metric. We address the accuracy-cost trade-off in SS3.2.2 with two alternative procedures.

#### 3.2.1 LUCB Algorithmic Framework

The main idea of LUCB  is to repeatedly select and explore two critical arms, that is, the empirical optimal arm \(_{t}\) and the empirical second-best arm \(u_{t}\). When both critical arms' confidence intervals are separated--\(_{t}\)'s LCB (lower confidence bound) is greater than \(u_{t}\)'s UCB (upper confidence bound), the algorithm terminates and outputs the estimated optimal arm \(_{t}\).

The LUCB framework depends on a set of meaningful confidence intervals of the arms' rewards. Usually, the confidence interval of an empirical mean estimate \(_{k}^{(m)}\) can be expressed as \((_{k}^{(m)}-(N_{k,t}^{(m)},t,),_{k}^{(m)}+ (N_{k,t}^{(m)},t,)),\) where \((N_{k,t}^{(m)},t,)\) is the confidence radius and \(N_{k,t}^{(m)}\) is the number of times of pulling arm \(k\) at fidelity \(m\) up to time \(t\) (include \(t\)). As MF-MAB assumes that \(|_{k}^{(m)}-_{k}^{(M)}|^{(m)}\), based on observations of fidelity \(m\), the upper and lower confidence bounds for arm \(k\)'s true reward mean at the highest fidelity \(_{k}^{(M)}\) can be expressed as follows,

\[_{k,t}^{(m)}_{k,t}^{(m)}+^{(m)}+(N_{ k,t}^{(m)},t,),_{k,t}^{(m)}_{k,t}^{(m)}- ^{(m)}-(N_{k,t}^{(m)},t,),\] (4)

where we set the confidence radius \((n,t,)=/)/n}\) and \(L(>0)\) is a factor. Since the multi-fidelity feedback allows one to estimate an arm's true reward mean with observations of every fidelity, we pick the tightest one as arm \(k\)'s final confidence bounds as follows,

\[_{k,t}=_{m}_{k,t}^{(m)}, _{k,t}=_{m}_{k,t}^{(m)}\,.\] (5)

We use the above \(_{k,t}\) and \(_{k,t}\) formulas to select the two critical arms in each round and decide when to terminate the LUCB (Line 3). We present the LUCB framework in Algorithm 1. The next step is to decide fidelities for exploring both critical arms in each round ( Line 5).

#### 3.2.2 Exploration Procedures

To address the accuracy-cost trade-off in fidelity selections, we devise a UCB-type policy which "finds" the optimal fidelity \(m_{k}^{*}\) in Eq.(3) for each arm \(k\) (Explore-A) and an explore-then-commit policy stopping at a good fidelity that is at least half as good as the optimal one (Explore-B).

**Notations.** The lower bound in Eq.(2) implies that there exists an optimal fidelity \(m_{k}^{*}\) for exploring arm \(k\). As Eq.(3) shows, the optimal fidelity \(m_{k}^{*}\) maximizes the \(_{k}^{(m)}/},\) where the \(_{k}^{(m)}\) defined in Eq.(1) consists of two unknown reward means: \(_{k}^{(m)}\) and \(_{1}^{(M)}\) (or \(_{2}^{(M)}\)). That is, calculating \(_{k}^{(m)}\) for all \(k\) needs the top two arms' reward means \(_{1}^{(M)}\) and \(_{2}^{(M)}\) which are unknown a priori. To address the issue, we assume the knowledge of an upper bound of the optimal arm's reward mean \(_{1}^{(M)}\) and a lower bound of the second-best arm's reward mean \(_{2}^{(M)}\) (see Remark 3.3 for how to obtain \(_{1}^{(M)}\) and \(_{2}^{(M)}\) in real-world applications). With the \(_{1}^{(M)}\) and \(_{2}^{(M)}\) replaced by \(_{1}^{(M)}\) and \(_{2}^{(M)}\), we define the ancillary reward gaps \(_{k}^{(m)}\) and the ancillary optimal fidelity \(_{k}^{*}\) as follows,

\[_{k}^{(m)}_{1}^{(M)}-(_{k} ^{(m)}+^{(m)})& k 1\\ (_{k}^{(m)}-^{(m)})-_{2}^{(M)}& k=1, _{k}^{*}*{arg\,max}_{m }_{k}^{(m)}}{}}.\]

Especially, we have \(m_{k}^{*}_{k}^{*}\) because the cost \(^{(m)}\) is non-decreasing and the replacement enlarges the numerator of the \(*{arg\,max}\) item in Eq.(3), and, when the bounds \(_{1}^{(M)}\) and \(_{2}^{(M)}\) are close the the true reward means, we have \(m_{k}^{*}=_{k}^{*}\); hence, as \(_{k}^{(m_{k}^{*})}>0\), we assume \(_{k}^{(_{k}^{*})}>0\) for all arms \(k\) as well. To present the next two procedures, we define the estimate of \(_{k}^{(m)}\) as follows,

\[_{k,t}^{(m)}_{1}^{(M)}-(_{k,t}^{(m)}+^{(m)})& k_{t}\\ (_{k,t}^{(m)}-^{(m)})-_{2}^{(M)}& k=_{t} ,\]

where the \(_{t}\) is the estimated optimal arm by LUCB. For simplify, we omit the input \(_{t}\) for \(_{k,t}^{(m)}(_{t})\) in the LHS of this definition and thereafter.

Explore-AWe devise the \(_{k}^{(m)}/}\)'s upper confidence bounds (\(\)-UCB) as follows, for any fidelity \(m\) and arms \(k\), \(\)-UCB\({}_{k,t}^{(m)}_{k,t}^{(m)}/}+/(^{(m)}N_{k,t}^{(m)})},\) where the \(N_{k,t}_{m}N_{k,t}^{(m)}\) is the total number of times of pulling arm \(k\) up to time \(t\). Whenever the arm \(k\) is selected by LUCB, we pick the fidelity \(m\) that maximizes its \(\)-UCB\({}_{k,t}^{(m)}\) to explore it (see Line 2 in Algorithm 2). For any arm \(k\), this policy guarantees that most of the arm's pulling are on its estimated optimal fidelity \(_{k}^{*}\), or formally, \(N_{k,t}^{(m)}=O((N_{k,t}^{(_{k}^{*})}))\) for any fidelity \(m_{k}^{*}\) as Lemma E.2 in Appendix shows. Therefore, Explore-A spends most cost on the fidelity \(_{k}^{*}\).

Explore-BThe cost of finding the estimated optimal fidelity \(_{k}^{*}\) in Explore-A can be large. To avoid this cost, we devise another approach that stops exploration when finding a _good_ fidelity \(_{k}^{*}\). That is, instead of finding the \(_{k}^{*}\) that maximizes \(_{k}^{(m)}/}\), we stop at a fidelity \(_{k}^{*}\) whose \(_{k}^{(m)}/}\) is at least half as large as that of \(_{k}^{*}\), i.e.,

\[_{k}^{(_{k}^{*})}}{_{k}^{* })}}}_{k}^{(_{k}^{*})}}{ _{k}^{*})}}}.\] (6)

We prove that the above inequality holds for \(_{k}^{*}=*{arg\,max}_{m}_{k,t}^{(m )}/}\) when the condition in Line 10 of Algorithm 2 holds (see Lemma E.3). Hence, for each arm \(k\), Explore-B explores it at all fidelities uniformly, and, when the condition in Line 10 holds, Explore-B finds a good fidelity \(_{k}^{*}\) and keeps choosing \(_{k}^{*}\) for exploring arm \(k\) since then.

**Remark 3.3** (On the bounds \(_{1}^{(M)}\) and \(_{2}^{(M)}\) utilized in Algorithm 2 in the hyperparameter optimization application).: Although the exact reward means are typically not accessible, it is easy to get a good approximation of them satisfying our requirements based on domain knowledge. For example,in an image classification task, an easy approximation of the best arm's reward means is to use the reward from a perfect classification, i.e., \(_{1}^{(M)}=1.0\), which clearly satisfies \(_{1}^{(M)}_{1}^{(M)}\). For the reward of the second-best arm, we can use the performance of a commonly used model that has a fairly good performance based on benchmarked results as a good approximation. One can easily find the benchmarked performance of commonly used models on a wide range of well-defined machine learning tasks on the Papers with code website.2 For novel tasks without well-benchmarked results, one pragmatic way to get \(_{2}^{(M)}\) is to use the result from a particular default machine learning model without any tuning.

### Cost Complexity Upper Bound Analysis

In the following, we present the cost complexity upper bounds of Algorithm 1 with above two procedures in Theorem 3.4 respectively.

**Theorem 3.4** (Cost complexity upper bounds for Algorithm 1 with procedure in Algorithm 2).: _Given \(L 4KM\), Algorithm 1 outputs the optimal arm with a probability at least \(1-\). The cost complexities of Algorithm 1 with different fidelity selection procedures in Algorithm 2 are upper bounded as follows,_

\[[] =O((+)}{^ {(1)}})+(+)}{ ^{(1)}})),\] (7) \[[] =O(_{m}}{ ^{(1)}}(}(^{(m)}/^{( 1)})L}{^{(1)}})),\] (8)

_where \(_{k}^{*})}}{(_{k}^{(m_{k}^{*})})^{2}},\) and \(_{k}_{m_{k}^{*}}( _{k}^{(m_{k}^{*})}}{^{*})}}}-_{k}^{(m)}}{}})^{-2}.\)_

**Explore-A**_vs._**Explore-B** When \(=O(M)\), the cost complexity upper bound of Explore-A is less than that of Explore-B (see Figure 0(a)). However, when \(\) is far more larger than \(M\), Explore-B is better (see Figure 0(b)). For example, when there is a fidelity \(m^{}(_{k}^{*})\) whose \(_{k}^{(m^{})}/)}}\) is very close to that of fidelity \(_{k}^{*}\) (the case in Figure 0(b)), this \(\) would be very large

Figure 1: Explore-A vs. Explore-B

because Explore-A needs to pay a high cost to distinguish fidelity \(m^{}\) from \(^{*}_{k}\); while in this scenario, Explore-B stops by either \(m^{}_{k}\) or \(^{*}_{k}\) since their \(^{(m)}_{k}/}\) are similar and, therefore, enjoys a smaller cost complexity upper bound. We report the numerical comparisons between both procedures in Figure 1. The detailed setup of the simulations is given in Appendix B.

**Remark 3.5** (Tightness of cost complexity bounds).: The first term of cost complexity upper bound for Explore-A in Eq.(7) matches the cost complexity lower bound in Eq.(2) up to a constant when \(^{*}_{k}=m^{*}_{k}\) and \(=H\) (i.e., when \(^{(M)}_{1}\) and \(^{(M)}_{2}\) are close to their ground truth values). The cost complexity upper bound of Explore-B in Eq.(8) matches the lower bound with an additional \(_{m}^{(m)}/^{(1)}\) coefficient when \(^{*}_{k}=m^{*}_{k}\) and \(=H\).

**Remark 3.6** (Comparison to classic MAB's sample complexity).: If we reduce our cost complexity upper bound result in MF-MAB to classic (single-fidelity) MAB, i.e., letting \(M=1,^{(m)}=1\), then both cost complexity upper bounds reduce to \(O(_{k}(1/_{k}^{2})(1/))\) where \(_{k}_{1}-_{k}\), which is exactly the classic sample complexity upper bound for (single-fidelity) BAI [27; 21].

## 4 Regret Minimization

In this section, we study the regret minimization objective: given a budget \(^{+}\), minimize the regret--the cumulative difference between the optimal policy's rewards and an algorithm's. We define the reward obtained in each time slot as the pulled arm's true reward mean (realized at the highest fidelity, but _unrevealed_ to the learner), no matter at which fidelity the arm is pulled, while the learner's observation depends on the pulled fidelity as Section 2 shows. Under this reward definition, the optimal policy is to constantly pull the optimal arm \(1\) with the lowest fidelity \(m=1\). Consequently, the expected regret can be expressed as follows,

\[[R()]}_{1}^{(M)}- [_{t=1}^{N}_{I_{t}}^{(M)}],\] (9)

where \(N\{n:_{t=1}^{n}^{(m_{})}\}\) is the total number of time slots, and \(I_{t}\) is the arm pulled by a concerned algorithm at time slot \(t\). Next, we illustrate the regret definition's real-world applications in Remark 4.1.

**Remark 4.1** (Applications of the new regret definition).: One typical application of the regret minimization problem under MF-MAB is a variant of the advertisement distribution problem [13; 10]. In this problem, the objective is to maximize the total return from all the distributed ads within a fixed marketing budget (e.g., in terms of money). We have the following mapping between the application-specific concepts and concepts in MF-MAB. **Arm:** The ads to distribute are the arms. **Reward:** The return from each of the ads, once distributed, is the corresponding ground-truth reward \(_{k}^{(M)}\). **Low fidelity:** A minimum cost is needed every time any ad is distributed. For example, the minimum cost may include the necessary resource needed to ensure that the ad satisfies legal and regulatory requirements and to distribute the ad on the designated platform. This minimum cost can be considered the lowest fidelity cost, which can never be waived. **High fidelity:** since the expected return from different ads can be vastly different, one needs a good estimation on the expected return so as to select the profitable ads to distribute. The cost needed to get a reliable estimate of the expected return can be considered the highest fidelity cost. For example, doing a large-scale user study/survey, and/or consulting experts can give a good estimate of the expected return from the ads, which, however, is resource-consuming.

This type of application is also common in production management with uncertainty where without knowing the expected return of the concerned products, the decision maker faces the two options of (1) spending the minimum resource needed to directly produce certain products; and (2) spending more resource to first get a good estimates of the expected returns from the different options and then put the ones with the highest expected returns into production.

**Remark 4.2** (Comparison to regret definition of Kandasamy et al. ).: Kandasamy et al.  defined the per time slot reward as the pulled arms' true reward mean multiplied by the cost, i.e., \(^{(m_{})}_{I_{t}}^{(M)}\), and defined their regret as, \([R^{}()]_{1}^{(M)}-[_{ t=1}^{N}^{(m_{})}_{I_{t}}^{(M)}]\). We note that multiplying the reward mean with the fidelity-level cost, \(^{(m_{})}_{I_{t}}^{(M)}\), does not fit into the applicationsin Remark 4.1, and thus we provide an alternative definition in Eq.(9) to fit our needs. Comparing the formula of both regret definitions, we have \([R^{}()]^{(1)}[R()].\) Note that both regret definitions are very different, so as their bound analysis and algorithm design.

We first present both the problem-independent (worst-case) and problem-dependent regret lower bounds in Section 4.1 and then devise an elimination algorithm whose worst-case upper bounds match the worst-case lower bound up to some logarithmic factors and whose problem-dependent upper bound matches the problem-dependent lower bound in a class of MF-MAB in Section 4.2.

### Regret Lower Bound

We present the problem-independent regret lower bound in Theorem 4.3 and the problem-dependent regret lower bound in Theorem 4.4. Both proofs are deferred to Appendix F.1 and F.2 respectively.

**Theorem 4.3** (Problem-independent regret lower bound).: _Given budget \(\), the regret of MF-MAB is lower bounded as follows,_

\[_{}_{}[R()] (K^{1/3}^{2/3}),\]

_where the \(\) is over any algorithms, the \(\) is over any possible MF-MAB instances \(\)._

**Theorem 4.4** (Problem-dependent lower bound).: _For any consistent policy that, after spending \(\) budgets, fulfills that for any suboptimal arm \(k\) (with \(_{k}^{(M)}>0\)) and any \(a>0\), \([N_{k}^{( m)}()]=o(^{a}),\) its regret is lower bounded by the following inequality,_

\[_{}[R()]}{( )}_{k}_{m:_{k}^{(m)}>0}( }{^{(1)}}_{1}^{(M)}-_{k}^{(M)})^{(m)})^{2}}.\]

### An Elimination Algorithm and Its Regret Upper Bound

In this section, we propose an elimination algorithm for MF-MAB based on Auer and Ortner . This algorithm proceeds in phases \(p=0,1,\) and maintains a candidate arm set \(_{p}\). The set \(_{p}\) is initialized as the full arm set \(\) and the algorithm gradually eliminates arms from the set until there is only one arm remaining. When the candidate arm set contains more than one arms, the algorithm explores arms with the highest fidelity \(M\), and when the set \(|_{p}|=1\), the algorithm exploits the singleton in the set with the lowest fidelity \(m=1\). We present the detail in Algorithm 3.

```
1:Input: full arm set \(\), budget \(\), and parameter \(\)
2:Initialization: phase \(p 0\), candidate set \(_{p}\)
3:while\(p<_{2}\) and \(|_{p}|>1\)do
4: pull each arm \(k_{p}\) in highest fidelity \(M\) such that \(T_{k}^{(M)}= 2^{2p}^{(M)}}\)
5: Update reward means \(_{k,p}^{(M)}\) for all arms \(k_{p}\)
6:\(_{p+1}{}\{k_{p};_{k,p}^{(M)}+2^{- p+1}{>}_{k^{}_{p}}_{k^{},p}^{(M)}\}\)\(\) Elimination
7:\(p p+1\)
8: Pull the remaining arms of \(_{p}\) in turn in fidelity \(m=1\) until the budget runs up ```

**Algorithm 3** Elimination for MF-MAB

**Remark 4.5** (Real-world implication of the 2-stage algorithm design).: There are real-world applications, e.g., the advertisement distribution problem in Remark 4.1, where the explorations are conducted at the high fidelity (e.g., a large-scale user study) and the exploitations are conducted at the low fidelity (e.g., advertisement distribution via some platforms). This corroborates our algorithm design which also explores at high fidelity and exploits at low fidelity. On the other hand, the fact that our algorithm enjoys the tight regret performance comparing to regret lower bound (both problem-independent and -dependent, see Remarks 4.9 and 4.7) also implies that the approach of first conducting large-scale user study and then massive distributing good ads used in real-world advertisement distribution is reasonable.

[MISSING_PAGE_EMPTY:10]