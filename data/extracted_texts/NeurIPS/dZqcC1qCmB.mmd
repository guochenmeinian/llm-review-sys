# Epistemic Neural Networks

Ian Osband, Zheng Wen, Seyed Mohammad Asghari,

**Vikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan Lu, and Benjamin Van Roy**

Google DeepMind, Efficient Agent Team, Mountain View

{ian.osband, m.ibrahimi}@gmail.com

{zhengwen,smasghari,vikranthd,lxlu,benvanroy}@google.com

Contact ian.osband@gmail.com

###### Abstract

Intelligent agents need to know what they don't know, and this capability can be evaluated through the quality of _joint_ predictions. In principle, ensemble methods can produce effective joint predictions, but the compute costs are prohibitive for large models. We introduce the _epinet_: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty. With an epinet, conventional neural networks outperform large ensembles of hundreds or more particles, and use orders of magnitude less computation. The epinet does not fit the traditional framework of Bayesian neural networks, so we introduce the _epistemic neural network_ (ENN) as a general interface for models that generate joint predictions.

## 1 Introduction

Consider a conventional neural network trained to predict whether a random person would classify a drawing as a 'rabbit' or a 'duck'. As illustrated in Figure 1, given a single drawing, the network outputs a _marginal_ prediction that assigns probabilities to the two classes. If the probabilities are each 0.5, it remains unclear whether this is because labels sampled from random people are equally likely, or whether the neural network would learn a single class if trained on more data. Conventional neural networks do not distinguish these cases, even though it can be critical for decision making systems to know what they do not know. This capability can be assessed through the quality of _joint_ predictions (Wen et al., 2022).

The two tables to the right of Figure 1 represent possible joint predictions that are each consistent with the network's uniform marginal prediction. These joint predictions are over _pairs_ of labels for the same image, \((y_{1}\!,\!y_{2})\!\!\{R\!,\!D\}\{R\!,\!D\}\). For any such joint prediction, Bayes' rule defines a conditional prediction for \(y_{2}\) given \(y_{1}\). The first table indicates inevitable uncertainty that would not be resolved through training on additional data; conditioning on the first label does not alter the prediction for the second. The second table indicates that additional training should resolve uncertainty; conditioned on the first label, the prediction for the second label assigns all probability to the same outcome as the first.

Figure 1 presents the toy problem of predictions across two identical images as a simple illustration of these types of uncertainty. The observation that joint distributions express whether uncertainty is resolvable extends more generally to practical cases, where the inputs differ, or where there are more than two simultaneous predictions (Osband et al., 2022).

Bayesian neural networks (BNNs) offer a statistically-principled way to make effective joint predictions, by maintaining an approximate posterior over the weights of a base neural network. Assymptotically these can recover the exact posterior, but the computational costsare prohibitive for large models (Welling and Teh, 2011). Ensemble-based BNNs offer a more practical approach by approximating the posterior distribution with an ensemble of statistically plausible networks that we call _particles_(Osband and Van Roy, 2015; Lakshminarayanan et al., 2017). While the quality of joint predictions improves with more particles, practical implementations are often limited to at most tens of particles due to computational constraints.

In this paper, **we introduce an approach that outperforms ensembles of hundreds of particles at a computational cost less than that of two particles**. Our key innovation is the _epinet_: a network architecture that can be added to any conventional neural network to estimate uncertainty. Figure 2 offers a preview of results presented in Section 6, where we compare these approaches on ImageNet. The quality of the ResNet's marginal predictions - measured by classification error or marginal log-loss - does not change much if supplemented with an epinet. However the epinet-enhanced ResNet dramatically improves the quality of _joint_ predictions, as measured by the joint log-loss, outperforming the ensemble of 100 particles, with total parameters less than 2 particles. Prior work has shown the importance of _joint_ predictions in driving effective decisions for a broad class of problems, including combinatorial decision problems and sequential decision problems (Wen et al., 2022; Osband et al., 2022a).

The epinet does not fit into the traditional framework of BNNs. In particular, it does not represent a distribution over base neural network parameters. To accommodate development of the epinet and other approaches that do not fit the BNN framework, we introduce the concept of _epistemic neural networks_ (ENNs). We establish that all BNNs are ENNs, but there are useful ENNs such as the epinet, that are not BNNs.

## 2 Related work

Our research builds on the literature in Bayesian deep learning (Hinton and Van Camp, 1993; Neal, 2012). BNNs represents epistemic uncertainty via approximating the posterior distribution over parameters of a base neural network (Der Kiureghian and Ditlevsen, 2009; Kendall and Gal, 2017). A challenge is the computational cost of posterior inference, which becomes intractable even for small networks (MacKay, 1992), and even approximate SGMCMC becomes prohibitive for large scale models (Welling and Teh, 2011).

Tractable methods for approximate inference has renewed interest in BNNs. Variational approaches such as Bayes by backprop (Blundell et al., 2015) use an evidence-based lower bound (ELBO) to approximate the posterior distribution, and related approaches use the same objective with more expressive weight distributions (Louizos and Welling, 2017). One

Figure 1: Conventional neural nets generate marginal predictions, which do not distinguish genuine ambiguity from insufficiency of data. Joint predictions can make this distinction.

Figure 2: Quality of marginal and joint predictions across models on ImageNet (Section 6).

influential line of work claims that MC dropout can be viewed as one such approach (Gal and Ghahramani, 2016), although subsequent papers have noted that the quality of this approximation can be very poor (Osband, 2016; Hron et al., 2017). As of 2022, perhaps the most popular approach is ensemble-based, with an ensemble of models, each referred to as a _particle_, trained in parallel so that they together approximate a posterior distribution (Osband and Van Roy, 2015; Lakshminarayanan et al., 2017).

Ensemble-based BNNs train multiple particles independently. This incurs computational cost that scales with the number of particles. A thriving literature has emerged that seeks the benefits of large ensembles at lower computational cost. Some approaches only ensemble parts of the network, rather than the whole (Osband et al., 2019; Havasi et al., 2020). Others introduce new architectures to directly incorporate uncertainty estimates, often inspired by connections to Gaussian processes (Malinin and Gales, 2018; Charpentier et al., 2020; Liu et al., 2020; van Amersfoort et al., 2021). Others perform Bayesian inference more directly in the function space in order to sidestep issues relating to overparameterization (Sun et al., 2019).

In general, research in Bayesian deep learning has focused more on developing methodology than unified evaluation (Osband et al., 2022). Perhaps because the potential benefits of BNNs are so far-reaching, different papers have emphasized improvements in classification accuracy (Wilson, 2020), expected calibration error (Ovadia et al., 2019), OOD performance (Hendrycks and Dietterich, 2019), active learning (Gal et al., 2017) and decision making (Osband et al., 2019). However, in each of these settings it generally is possible to obtain improvements via methods that do not aim to approximate posterior distributions. Perhaps for this reason, there has been a recent effort to refocus evaluation on how well methods actually approximate 'gold standard' Bayes posteriors (Izmailov et al., 2021).

Our work on ENNs is motivated by the importance of joint predictions in driving decision, exploration, and adaptation (Wang et al., 2021; Wen et al., 2022; Osband et al., 2022). This line of research, which we build on in Section 3.1, establishes a sense in which joint predictions are both necessary and sufficient to drive decisions. Effectiveness of ENN designs can be assessed through the quality of joint predictions. This perspective allows us to consider approaches beyond those accommodated by the BNN framework. As we will demonstrate, this can lead to significant improvements in performance.

## 3 Epistemic neural networks

A conventional neural network is specified by a parameterized function class \(f\), which produces a vector-valued output \(f_{}(x)\) given parameters \(\) and an input \(x\). The output \(f_{}(x)\) assigns a corresponding probability \((y)=((f_{}(x))_{y})/_{y^{}} ((f_{}(x))_{y^{}})\) to each class \(y\). For shorthand, we write such class probabilities as \((y)=(f_{}(x))_{y}\). We refer to a predictive class distribution \(\) produced in this way as a _marginal prediction_, as it pertains to a single input \(x\).

An ENN architecture, on the other hand, is specified by a pair: a parameterized function class \(f\) and a reference distribution \(P_{Z}\). The vector-valued output \(f_{}(x,z)\) of an ENN depends additionally on an _epistemic index_\(z\), which takes values in the support of \(P_{Z}\). Typical choices of the reference distribution \(P_{Z}\) include a uniform distribution over a finite set or a standard Gaussian over a vector space. The index \(z\) is used to express epistemic uncertainty. In particular, variation of the network output with \(z\) indicates uncertainty that might be resolved by future data. As we will see, the introduction of an epistemic index allows us to represent the kind of uncertainty required to generate useful joint predictions.

Given inputs \(x_{1}\),...,\(x_{}\), a joint prediction assigns a probability \(_{1:}(y_{1:})\) to each class combination \(y_{1}\),...,\(y_{}\). While conventional neural networks are not designed to provide joint predictions, joint predictions can be produced by multiplying marginal predictions:

\[_{1:}^{}(y_{1:})\!=\!_{t=1}^{}\!(f_{}(x_{t}))_{y_{t}}.\] (1)

However, this representation models each outcome \(y_{1:}\) as independent and so fails to distinguish ambiguity from insufficiency of data. ENNs address this by enabling more expressive joint predictions through integrating over epistemic indices:

\[_{1:}^{}(y_{1:})=_{z}\!P_{Z}(dz)_{t=1}^{} (f_{}(x_{t},z))_{y_{t}}.\] (2)

This integration introduces dependencies so that joint predictions are not necessarily just the product of marginals. Figure 3 provides a simple example of how two different ENNs can use the epistemic index to distinguish the sorts of uncertainty described in Figure 1.

In Figure 3(a) the ENN makes marginal predictions that do not vary with \(z\), and so the resultant joint predictions are simply the independent product of marginals. This corresponds to an 'aleatoric' or 'irreducible' form of uncertainty that cannot be resolved with data. On the other hand, Figure 3(b) shows an ENN that makes predictions depending on the sign of the epistemic index. This corresponds to 'epistemic' or'reducible' uncertainty that can be resolved with data. In this case, integrating the 2x2 matrix over \(z\) produces a diagonal matrix with 0.5 in each diagonal entry. As such, Figure 3 shows how an ENN can use the epistemic index to distinguish the two joint distributions of Figure 1.

### Evaluating ENN performance

Marginal log loss (also known as cross-entropy loss) is perhaps the most widely used evaluation metric in machine learning. For a single input \(x\), if a neural network generates a prediction \(\) and the label turns out to be \(y\), then the sample log loss is the form \(-(y)\). We say that this is a _marginal_ loss because it only looks at the quality of a prediction over a single (input, output) pair. As we will discuss, minimizing marginal log loss does not generally lead to performant downstream decisions. Good decisions often require good _joint_ predictions.

To formulate a generic decision problem, consider a reward function \(r\) that maps an action \(a\) and \(\) labels \(y_{1:}\) to a reward \(r(a,y_{1:})\). Given an exact posterior predictive \(P_{1:}\), consider as an objective maximization of the expected reward \(_{y_{1:}}P_{1:}(y_{1:})r(a,y_{1:})\). The optimal decision can be approximated based on an ENN's prediction \(_{1:}\) by choosing an action \(a\) that maximizes \(_{y_{1:}}_{1:}(y_{1:})r(a,y_{1:})\). The following theorem is formalized and proved in Appendix B.

**Theorem 1**.: [informal] _There exists a decision problem and an ENN that attains small expected marginal log loss such that actions generated using the ENN perform no better than random guessing._

Theorem 1 indicates that minimizing marginal log loss does not suffice to support effective decisions. The key to this insight is that marginal predictions do not distinguish ambiguity from insufficiency of data. However, this can can be addressed by instead considering the _joint_ log loss. Given \(\) data pairs and a joint prediction \(_{1:}\), we can consider the joint log loss \(-_{1:}(y_{1:})\) in exactly the same way that we looked at the marginal log loss. We formalize our next result in Appendix B.

**Theorem 2**.: [informal] _For any decision problem, any ENN that attains small expected joint log loss leads to actions that attain near optimal expected reward._

Theorems 1 and 2 highlight the importance of joint predictions in driving decisions. Since we want machine learning systems to drive effective decisions, we will assess the performance

Figure 3: An ENN can incorporate the epistemic index \(z P_{Z}\) into its joint predictions. This allows an ENN to differentiate inevitable ambiguity from data insufficiency.

of ENNs by comparing their joint log loss.2 It is important to note that we will consider this joint loss as a method for assessing quality of a _trained_ ENN. We have not yet discussed how particular forms of ENNs are trained, which will generally be up to the algorithm designer. Section 4 provides further detail on the specific architecture and training loss for the epinet ENN we develop in this paper.

### ENNs versus BNNs

A base neural network \(f\) defines a class of functions. Each element \(f_{}\) of this class is identified by a vector \(\) of parameters, which specify weights and biases. An ENN or BNN is designed with respect to a specific base network, and seek to express uncertainty while learning a function in this class. We will formally define what it means for an ENN or BNN to be defined with respect to a base network. We will then establish results which indicate that, with respect to any base network, all BNNs can be expressed as ENNs but not vice versa.

Consider a base neural network \(f\) which, given parameters \(\) and input \(x\), produces an output \(f_{}(x)\). A typical BNN is specified by a pair: a base network \(f\) and a parameterized sampling distribution \(p\). Given parameters \(\), a sample \(\) can be drawn from the distribution \(p_{}\) to generate a function \(f_{}\). Approaches such as stochastic gradient MCMC, deep ensembles, and dropout can all be framed in this way. For example, with a deep ensemble, \(\) comprises parameters of an ensemble particle and \(p_{}\) is the distribution represented by the ensemble, which assigns probability to a finite set of vectors, each associated with one ensemble particle. For any inputs \(x_{1},,x_{}\), by sampling many functions (\(f_{k}:k=1,,K\)), a BNN can be used to approximate the corresponding joint distribution over labels \(y_{1},,y_{}\), according to \((y_{1:})=_{k=1}^{K}((f_{k}(x_{ 1}),,f_{k}(x_{}))=y_{1:})\).

We say the BNN \((f,p)\) is _defined with respect to_ its base network \(f\). We say an ENN \((f^{},P_{Z})\) is _defined with respect_ to a base network \(f\) if, for any base network parameters \(\), there exist ENN parameters \(^{}\) such that \(f^{}_{^{}}(,z)=f_{}\) almost surely with respect to \(P_{Z}\). Intuitively, being defined with respect to a particular base network means that the BNN or ENN is designed to learn any function within the class characterized by the base network.

We say that a BNN \((f,p)\) is _expressed as_ an ENN \((f^{},P_{Z})\) if, for all \(\), \(\), and inputs \(x_{1:}\), there exists \(^{}\) such that for \( p_{},z P_{z}\),

\[(f_{}(x_{1}),,f_{}(x_{}))}{{=}}(f^{}_{^{}}(x_{1},z),,f^{}_{ ^{}}(x_{},z)).\] (3)

This condition means that the ENN and BNN make the same joint predictive distributions at all inputs.

We say that an ENN \((f^{},P_{Z})\) is _expressed as_ a BNN if, for all \(^{}\), \(\), and inputs \(x_{1},,x_{}\), there exists a posterior distribution \(\) such that (3) holds. Intuitively, one architecture is expressed as the other if the latter can represent the same distributions over functions. The following result, established in Appendix C, asserts that any BNN can be expressed as an ENN but not every ENN can be expressed as a BNN.

**Theorem 3**.: _For all base networks \(f\), any BNN defined with respect to \(f\) can be expressed as an ENN defined with respect to \(f\). However, there exists a base network \(f\) and ENN defined with respect to \(f\) that can not be expressed as a BNN defined with respect to \(f\)._

In supervised learning, de Finetti's Theorem implies that if a sequence of data pairs is exchangeable then they are i.i.d. conditioned on a latent random object (de Finetti, 1929). BNNs use base network parameters \(\) as the object, while ENNs focus on the function \(g_{*}\) itself, without concerning the underlying parameters. ENNs serve as computational mechanisms to approximate the posterior distribution of \(g_{*}\), allowing functions beyond the base network class to represent uncertainty and allowing better trade-offs between computation and prediction quality. The epinet is an example of an ENN that cannot be expressed as a BNN with the same base network, and showcases the benefits of the ENN interface beyond BNNs.

The epinet

This section introduces the _epinet_, which can supplement any conventional neural network to make a new kind of ENN architecture. Our approach reuses standard deep learning components and training algorithms, as outlined in Algorithm 1. As we will see, it is straightforward to add an epinet to any existing model, even one that has been pretrained. The key to successful application of the epinet comes in the design of the network architecture and loss functions.

```
0:dataset
0:train examples \(=\{(x_{i},y_{i})\}_{ 1}^{N}\) ENN \(f\), reference \(P_{Z}\), initialization \(_{0}\) loss \(f\) evaluates example \((x_{i},y_{i})\) for index \(z\) batch size \(n\), index samples \(m\)s in a regular optimizer \(\) and number of iterations \(T\) Returns:\(_{T}\) parameter estimates for the ENN.
1:for\(t 0,...,T-1\)do
2: sample data \(Z=z_{1},...,z_{n}([1,..,N])\).
3: sample indices \(Z=z_{1},...,z_{n} P_{Z}\).
4: compute \(-_{}f_{}(x,z)=_{c Z}_{c}^{T}(,z,x_{i},y_{i},i)\).
5: update \(_{t+1}+(,)\) ```

**Algorithm 1** ENN training via SGD

### Architecture

Consider a conventional neural network as the _base network_. Given base parameters \(\) and an input \(x\), the output is \(_{}(x)\). For a classification model, the class probabilities would be \((_{}(x))\). An epinet is a neural network with privileged access to inputs and outputs of activation units in the base network. A subset of these inputs and outputs, which we call _features_\(_{}(x)\), are taken as input to the epinet along with an epistemic index \(z\). For example, these features might be the last hidden layer in a ResNet. The epistemic index is sampled from a standard Gaussian distribution in dimension \(D_{Z}\). For epinet parameters \(\), the epinet outputs \(_{}(_{}(x),z)\). To produce an ENN, the output of the epinet is added to that of the base network, though with a "stop gradient":3

\[(x,z)}_{}=(x)}_{ {base net}}+([_{}(x)],z)}_{}.\] (4)

We find that with this stop gradient, training dynamics more reliably produce models that perform well out of sample. The ENN parameters \(=(,)\) include those of the base network \(\) and epinet \(\). Due to the additive structure of the epinet, multiple samples of the ENN can be obtained with only one forward pass of the base network. Where the epinet is much smaller than the base network this can lead to significant computational savings.

Before training, variation of the ENN output \(f_{}(x,z)\) as a function of \(z\) reflects prior uncertainty in predictions. Since the base network does not depend on \(z\), this variation must derive from the epinet. In our experiments, we induce this initial variation using _prior networks_(Osband et al., 2018). In particular, for \(:=[_{}(x)]\), our epinets take the form

\[(,z)}_{}=^{L}(,z)}_{}+(,z )}_{}.\] (5)

The prior network \(^{P}\) represents prior uncertainty and has no trainable parameters. The learnable network \(_{}^{L}\) is typically initialized to output values close to zero, but is then trained so that the resultant sum \(_{}\) produces statistically plausible predictions for all probable values of \(z\). Variations of a prediction \(_{}=_{}^{L}+^{P}\) at an input \(x\) as a function of \(z\) indicate predictive epistemic uncertainty, just like the example from Figure 3(b).

Epinet architectures can be designed to encode inductive biases that are appropriate for the application at hand. In this paper we focus on a particularly simple form of architecture for \(^{L}_{}\) based around standard multi-layered perceptron (MLP) with Glorot initialization (Glorot and Bengio, 2010):

\[^{L}_{}(,z):=_{}([,z])^{}z ^{C},\] (6)

where \(_{}\) is an MLP with outputs in \(^{D_{Z} C}\) and \([,z]\) is a flattened concatenation of \(\) and \(z\). Depending on the choice of hidden units, \(^{L}_{}\) can represent highly nonlinear functions in \(,z\) and thus allow for expressive joint predictions in high level features. The design, initialization and scaling the prior network \(^{P}\) allows an algorithm designer to encode prior beliefs, and is essential for good performance in learning tasks. Typical choices might include \(^{P}\) sampled from the same architecture as \(^{L}\) but with different parameters.

### Training loss function

While the epinet's novelty primarily lies in its architecture, the choice of loss function for training can also play a role in its performance. In many classification problems, the standard regularized log loss suffices:

\[^{}_{}(,z,x_{i},y_{i},i):=-((f_{}(x_{i},z))_{y_{i}})+\|\|_{2}^{2}.\] (7)

Here, \(\) is a regularization penalty hyperparameter, while other notation are as defined in Sections 3 and 4.1. This is the loss function we use for the experiments with the Neural Testbed (Section 5) and ImageNet (Section 6).

Image classification benchmarks often exhibit a very high signal-to-noise ratio (SNR); identical images are almost always assigned the same label. As demonstrated by Dwaracherla et al. (2022), when the SNR is not so high and varies significantly across inputs, it can be beneficial to randomly perturb the loss function via versions of the statistical bootstrap (Efron and Tibshirani, 1994). For example, a Bernoulli bootstrap omits each data pair with probability \(p\), giving rise to a perturbed loss function:

\[^{}_{p,}(,z,x_{i},y_{i},i):=\{[] {ll}^{}_{}(,z,x_{i},y_{i},i)&c_{i}^{T}z>^{-1}(p)\\ 0&,.\] (8)

where each \(c_{i}\) is an independent random vector sampled uniformly from the unit sphere, and \(()\) is the cumulative distribution function of the standard normal distribution \(N(0,1)\).

Note that the loss functions defined in equation 7 and 8 only explicitly state the loss for a single input-label pair \((x_{i},y_{i})\) and a single epistemic index \(z\). To compute a stochastic gradient, one needs to sample a batch of input-label pairs and a batch of epistemic indices, average the losses defined above, and then compute the gradient.

### How can this work?

The epinet is designed to produce effective _joint_ predictions. As such, one might expect the training loss to explicitly reflect this and be surprised that we use standard marginal loss functions such as \(^{}_{}\). Recall that a prediction \(f_{}(x,z)\) produced by an epinet is given by a trainable component \(_{}(x)+^{L}_{}(,z)\) perturbed by the prior function \(^{P}(,z)\), which has no trainable parameters. Minimizing \(^{}_{}\) can therefore be viewed as optimizing the _learnable_ component \(_{}(x)+^{L}_{}(,z)\) with a perturbed loss function. Previous work has established that learning with prior functions can induce effective joint predictions (Osband et al., 2018; He et al., 2020; Dwaracherla et al., 2020, 2022), we extend this to the epinet.

To show this marginal loss function can lead to effective joint predictions, Theorem 4 proves that this epinet training procedure can mimic exact Bayesian linear regression. Although this paper focuses on classification, in this subsection we consider a regression problem because its analytical tractability facilitates understanding. To establish this result, we introduce a regularized squared loss perturbed by Gaussian bootstrapping:

\[^{}_{,}(,z,x_{i},y_{i},i):=(f_{,}(x _{i},z)-y- c_{i}^{T}z)^{2}+(\|\|_{2}^{2}+\|\|_{2}^ {2}).\] (9)

Here, each \(c_{i}\) is a context vector sampled uniformly from the unit sphere in \(D_{Z}\) dimensions, in the same manner as \(^{}_{p,}\) (8).

We say that a dataset \(\) is _generated by a linear-Gaussian model_ if \(g_{_{*}}(x)=_{*}^{T}x\), \(_{*} N(0,_{0}^{2}I)\), and each data pair \((x_{i},y_{i})\) satisfies \(y_{i}=g_{_{*}}(x_{i})+_{i}\) where \(_{1:N}\) are i.i.d. according to \(N(0,^{2})\). We say an ENN is a _linear-Gaussian epinet_ with parameters \(=(,)\) if its trainable component is comprised of a linearly parameterized functions \(_{}(x)=^{T}x\) and \(_{}^{L}(,z)=z^{T}\), with epinet input \(=x\), the prior function takes the form \(^{P}(x,z)=_{0}z^{T}P_{0}x\), where each column of the matrix \(P_{0}\) is independently sampled uniformly from the unit sphere, and the reference distribution is taken to be \(P_{z} N(0,I_{D_{Z}})\).

**Theorem 4**.: _Let data \(\!=\!\{(x_{i},y_{i},i)\}_{i=1}^{N}\) be generated by a linear-Gaussian model and \(f\) be a linear-Gaussian epinet. Let \(\!\!\!_{}\!_{i=1}^{N}\!_{z}P_{Z}(dz) _{,}^{}(,\!z,\!x_{i},\!y_{i},\!i)\) with parameter \(\!=\!^{2}/(N_{0}^{2})\). Then, conditioned on \((,\!c_{1:N},\!P_{0})\), \(f_{}(,\!z)\) converges in distribution to \(g_{_{*}}\) as \(D_{Z}\) grows, almost surely._

This result, proved in Appendix D, serves as a'sanity check' that an epinet trained with a standard loss function can approach optimal joint predictions as the epistemic index dimension grows. Although our analysis is limited to linear-Gaussian models, the loss function applies more broadly. Indeed, we next demonstrate that epinets and standard loss functions scale effectively to large and complex models.

## 5 The neural testbed

_The Neural Testbed_ is an open-source benchmark that evaluates the quality of joint predictions in classification problems using synthetic data produced by neural-network-based generative models (Osband et al., 2022). We use this as a unit test to sanity-check learning algorithms in a controlled environment and compare the epinet against benchmark approaches.

Table 1 lists the agents that we study as well as hyperparameters that we tune via grid search. For our epinet agent, we use base network \(_{}(x)\) that matches the baseline mlp agent. We take the features \((x)\) to be a concatenation of the input \(x\) and the last hidden layer of the base network. We initialize the learnable epinet \(^{L}\) according to (6) with 2 hidden layers of 15 hidden units. The prior network \(^{P}\) is initialized as an ensemble of \(D_{Z}=8\) networks each with 2 hidden layers of 5 hidden units in each layer, and combine the output by dot-product with index \(z\). We push the details, together with open source code, to Appendix G.

Figure 5 examines the trade-offs between statistical loss and computational cost for epinet against benchmark agents. The bars indicate standard error, estimated over the testbed's random seeds. After tuning, all agents perform similarly in marginal prediction, but the epinet is able to provide better joint predictions at lower computational cost. We first compare the performance of the epinet (blue) against that of an ensemble (red) as we grow the number of particles. **We show the epinet is able to perform much better than an ensemble with 100 particles, with a model less than twice the size of a single particle.** These results are still compelling when we compare against ensemble+, which includes prior functions, and which is necessary for good performance in low data regimes included in testbed evaluation. Included in this plot are the other agents bbb, dropout and hypermodel agents. The dashed line indicates performance of sgmcmc agent, which can asymptotically obtain the Bayes optimal solution, but at a much higher computational cost.

## 6 ImageNet

Benefits of the epinet scale up to more complex datasets and, in fact, become more substantial. This section focuses on experiments involving the ImageNet dataset (Deng

 
**agent** & **description** & **hyperparameters** \\  mlp & vanilla MLP & \(L_{2}\) decay \\ ensemble & deep ensembles (Lakshminarayanan et al., 2017) & \(L_{2}\) decay, ensemble size \\ dropout & Dropout (Gal and Ghahramani, 2016) & \(L_{2}\) decay, network, dropout rate \\ bbb & Bayes by backprop (Blundell et al., 2015) & prior mixture, network, early stopping \\ hypermodel & hypermodel (Dwarcherla et al., 2020) & \(L_{2}\) decay, prior, bootstrap, index dimension \\ ensemble+ prior functions (Osband et al., 2018) & \(L_{2}\) decay, ensemble size, prior scale, bootstrap \\ sgmcmc & stochastic gradient MCMC (Welling and Teh, 2011) & learning rate, prior, momentum \\ epinet & MLP + MLP epinet (this paper) & \(L_{2}\) decay, network, prior, index dimension \\  

Table 1: Summary of benchmark agents, full details in Appendix G.

et al., 2009); qualitatively similar results for both CIFAR-10 and CIFAR-100 are presented in Appendix H. We compare our epinet agent against ensemble approaches as well as the _uncertainty baselines_ of Nado et al. (2021). Even after tuning to optimize joint log-loss, none of these agents match epinet performance. We assess joint log-loss via dyadic sampling (Osband et al., 2022b), as explained in Appendix F.

For our experiments, we first train several baseline ResNet architectures on ImageNet. We train each of the ResNet-\(L\) architectures for \(L\{50,101,152,200\}\) in the Jaxline framework (Babuschkin et al., 2020). We tune the learning rate, weight decay and temperature rescaling (Wenzel et al., 2020) on ResNet-50 and apply those settings to other ResNets. The ensemble agent only uses the ResNet-50 architecture. After tuning hyperparameters, we independently initialize and train 100 ResNet-50 models to serve as ensemble particles. These models are then used to form ensembles of sizes 1, 3, 10, 30, and 100.

The epinet takes a pretrained ResNet as the base network with frozen weights. We fix the index dimension \(D_{Z}=30\) and let the features \(\) be the last hidden layer of the ResNet. We use a 1-layer MLP with 50 hidden units for the learnable network (6). The fixed prior \(^{P}\) consists of a network with the same architecture and initialization as \(^{L}_{}\), together with an ensemble of small random convolutional networks that directly take the image as inputs. We push details on hyperparameters and evaluation, with open-source code, to Appendix H.

**Figure 2 presents our key result of this paper: relative to large ensembles, epinets greatly improve joint predictions at orders of magnitude lower compute cost.** The figure plots performance of three agents with respect to three notions of loss as a function of model size in the spirit of Kaplan et al. (2020). The first two plots assess marginal prediction performance in terms of classification error and log-loss. Performance of the ResNet scales similarly whether or not supplemented with the epinet. Performance of the ensemble does not scale as well as that of the ResNet. The third plot pertains to performance of joint predictions and exhibits dramatic benefits afforded by the epinet. While joint log-loss incurred by the ensemble agent improves with model size more so than the ResNet, the epinet agent outperforms both alternatives by an enormous margin.

**Figure 6 adds evaluation for the best single-model agents from _uncertainty baselines_(Nado et al., 2021): epinet also outperforms all of these methods**. We tuned each uncertainty baseline agent to minimize joint log-loss, subject to the constraint

Figure 5: Quality of marginal and joint predictions across models on the Neural Testbed.

Figure 6: Marginal and joint predictions on ImageNet.

that their marginal log-loss does not degrade relative to published numbers. We can see that the epinet offers substantial improvements in joint prediction compared to sngp(Liu et al., 2020), dropout(Gal and Ghahramani, 2016), mimo(Havasi et al., 2020) and het(Collier et al., 2020). As demonstrated in Appendix H, these qualitative observations remain unchanged under alternative measures of computational cost, such as FLOPs.

## 7 Conclusion

This paper introduces ENNs as a new interface for uncertainty modeling in deep learning. We do this to facilitate the design of new approaches and the evaluation of joint predictions. Unlike BNNs, which focus on inferring unknown network parameters, ENNs focus on uncertainty that matters in the _predictions_. The epinet is a novel ENN architecture that cannot be expressed as a BNN and significantly improves the tradeoffs in prediction quality and computation. For large models, the epinet enables joint predictions that outperform ensembles consisting of hundreds of particles at a computational cost only slightly more than one particle. Importantly, you can add an epinet to large pretrained models with modest incremental computation.

ENNs enable agents to know what they do not know, thereby unlocking intelligent decision making capabilities. Specifically, ENNs allow agents to employ sophisticated exploration schemes, such as information-directed sampling (Russo and Van Roy, 2014), when tackling complex online learning and reinforcement learning problems. A recent paper (Osband et al., 2023) has demonstrated efficacy of the epinet in several benchmark bandit and reinforcement learning problems. While ENNs can also be integrated into large language models, we defer this to future work.