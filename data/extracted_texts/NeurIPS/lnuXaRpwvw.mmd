# RedPajama: an Open Dataset for Training Large Language Models

Maurice Weber\({}^{1}\), Daniel Y. Fu\({}^{1,2}\), Quentin Anthony\({}^{4,8,10}\), Yonatan Oren\({}^{1}\)

**Shane Adams\({}^{1}\)**, **Anton Alexandrov\({}^{7}\)**, **Xiaozhong Lyu\({}^{7}\)**, **Huu Nguyen\({}^{5}\)**, **Xiaozhe Yao\({}^{7}\),

**Virginia Adams\({}^{1}\)**, **Ben Athiwaratkun\({}^{1}\)**, **Rahul Chalamala\({}^{1,11}\)**, **Kezhen Chen\({}^{1}\)**, **Max Ryabinin\({}^{1}\)**

**Tri Dao\({}^{1,6}\)**, **Percy Liang\({}^{1,2}\)**, **Christopher Re\({}^{1,2}\)**, **Irina Rish\({}^{8,9}\)**, **Ce Zhang\({}^{1,3}\)**

\({}^{1}\) Together AI, \({}^{2}\) Stanford University, \({}^{3}\) University of Chicago

\({}^{4}\) EleutherAI \({}^{5}\) Ontocord.ai, \({}^{6}\) Princeton University, \({}^{7}\) ETH Zurich

\({}^{8}\) Mila, Montreal, Canada \({}^{9}\) Universite de Montreal \({}^{10}\) Ohio State University \({}^{11}\) Caltech

###### Abstract

Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.

## 1 Introduction

Pretraining data is among the most central building blocks that go into the development of modern large language models (LLMs). However, one of the core challenges this field faces is the general lack of transparency regarding the composition and curation strategy of pretraining data . Indeed, with a few notable exceptions [19; 4; 2; 65], the majority of reports documenting state-of-the-art LLMs  provide scarce details, if any, on their pretraining datasets. Even open-weights models such as LLaMA [57; 58] provide little to no details about their training data, let alone release their datasets. Furthermore, the process of studying and building optimal data compositions, along with developing filtering rules and heuristics, is time-consuming as it necessitates running numerous ablations ondifferent compositions of the training data. To address these challenges, and with the overarching goal of democratizing the access to and development of open-source LLMs, we have released the RedPajama datasets, which in total consist of more than 100 trillion tokens of text data. With this goal in mind, we use the following design principles to guide our approach to creating open datasets:

**Transparency.** We strive for maximal _transparency_ from at least two angles. On the one hand, this entails documenting and making all aspects of data curation publicly available1. On the other hand, we strive for open and transparent datasets, which allows for application developers and researchers alike to better understand and design language models.

**Scale.** Large pools of accessible data are one of the core building blocks of the most powerful large language models , yet are hard to come by due to the large amount of resources and expertise required to build, curate and store them. Next to transparency, we thus also strive for _scale_.

**Versatility.** We aim to provide the community with datasets and artifacts for building state-of-the-art open language models by providing a versatile resource. Rather than prescribing what constitutes a high-quality dataset, we offer a broad, general-purpose corpus of web documents. Each document is tagged with quality signals, empowering users to make informed decisions based on their specific needs and criteria.

Following these principles, we have developed and made publicly available the RedPajama datasets for LLM pretraining. RedPajama-V1 is a publicly available, fully open, best-effort reproduction of the training data described in , used to train the first iteration of LLaMA family of models. Together with this dataset, we have developed the RedPajama-INCITE models, which include a base, instruction-tuned, and chat models at the 3B and 7B scales. Based on the first set of learnings from these efforts, we have built the RedPajama-V2 dataset. This latter dataset takes an entirely different approach and focuses exclusively on web data. It consists of five languages, sourced from 84 Common Crawl snapshots ranging from 2014 to 2023. In addition to the raw text data, we also publish quality signals accompanying each document in a 50T token subset of the corpus with the goal of fostering research towards principled understanding of ways to filter web data. In addition, in this work, we present a series of ablation studies, showcasing the value of the quality signals for creating subsets of the raw corpus of varying quality and subsequent model performance.

In summary, in this work, we make the following contributions:

* We publish the RedPajama-V1 dataset, an open reproduction of the dataset used to train LLaMA-1 . We also include a detailed report on considerations that went into creating the corpus.
* We report the process and evaluations on the training of the RedPajama-INCITE models, including details on how we used the Summit supercomputer and the challenges we had to address.
* We publish the RedPajama-V2 dataset, the largest open pretraining dataset consisting of raw, unfiltered data scraped from the web, together with 46 measures of quality computed for each document, to enable further research in data curation.

Figure 1: The ecosystem around the RedPajama datasets. RedPajama has provided pretraining data for multiple open-source LLMs, including OpenELM , OLOo , Snowflakeâ€™s Arctic  and RedPajama-INCITE. SlimPajama is a cleaned and deduplicated version of RedPajama-V1.

* Based on the RedPajama-V2 dataset, we present a series of ablation studies on decoder-only Transformer models with 468 million parameters, showing how the quality signals can be used to create models of varying performance on common NLP benchmarks.

The remainder of this paper is organized as follows. In Section 2, we position the RedPajama dataset in the current landscape of open pretraining datasets. Section 3 describes the details of the dataset creation process behind RedPajama-V1, as well as building the RedPajama-INCITE family of models. Section 4 proceeds to RedPajama-V2, our web-only dataset. Next to describing the data processing steps, we present dataset statistics and ablation studies. Finally, we conclude in Section 5.

## 2 Related Work

Numerous efforts have focused on constructing pretraining datasets for large language models. While some of these datasets are curated from a mix of various sources, others are exclusively derived from web data. In the realm of web-only datasets, the C4 dataset  was one of the first large-scale web datasets, comprising a 175B token web corpus filtered down from CommonCrawl. C4 still remains a benchmark for web dataset quality. More recently, RefinedWeb  and FineWeb  have demonstrated that web-only data can yield strong models without the need for compositing multiple domains, and have, similar to our work, also provide ample details on their data curation techniques. In contrast to these datasets, RedPajama-V2 is composed of 100 trillion tokens of raw, mostly unfiltered text. With its more than 40 quality signals for potential filtering, RedPajama-V2 promotes an entirely different approach and aims to set a new standard for future high-quality web datasets, providing a robust foundation for the next generation of high quality web datasets. Of further great relevance are the Gopher rules proposed in , which have been central to many of the previously mentioned open pretraining datasets.

Complementing web-only datasets, composite datasets introduce additional domains and enable broader coverage. Most notably, the Pile  was one of the first fully open datasets. After the release of LLaMA , which used seven individual subsets and multiple domains, we published RedPajama-V1 as an open source replication of the LLaMA recipe, which gained widespread adoption. Building on this, the SlimPajama dataset  was derived from RedPajama-V1 by further cleaning and deduplication. Similarly, the Dolma  dataset includes other specialized domains, such as cleaned versions of code datasets including The Stack , StarcoderData  as well as the ArXiv and StackExchange splits of RedPajama-V1. The Zyda  dataset goes in a similar vein and further refines open datasets, including the SlimPajama dataset derived from RedPajama-V1. Finally, the ROOTS corpus [26; 27] is also among the core open datasets spanning multiple domains and languages. Table 1 shows an overview over these open datasets and makes a comparison where each dataset stands in terms of transparency, versatility and scale.

    &  &  &  \\    & Open Access & & & Raw Data & Composite & Multilingual \\  Refined Web  & \(\)(subset) & \(\) & \(\) & \(\) & 2.8 \\ FineWeb  & \(\) & \(\) & \(\) & \(\) & 93.4 \\ FineWeb-EDU  & \(\) & \(\) & \(\) & \(\) & 8.8 \\ C4  & \(\) & \(\) & \(\) & \(\) & 0.3 \\ mC4  & \(\) & \(\) & \(\) & \(\) & 9.7 \\ DCLM baseline  & \(\) & \(\) & \(\) & \(\) & 10.0 \\ DCLM-Pool  & \(\) & \(\) & \(\) & \(\) & 340.0 \\  Dolma v1.7  & \(\) & \(\) & \(\) & \(\) & 4.5 \\ Pile  & \(\) & \(\) & \(\) & \(\) & 0.8 \\ SlimPajama  & \(\) & \(\) & \(\) & \(\) & 0.9 \\ ROOTS [26; 27] & \(\) & \(\) & \(\) & \(\) & 1.6 \\  RedPajama-V1 & \(\) & \(\) & \(\) & \(\) & 3.0 \\ RedPajama-V2 & \(\) & \(\) & \(\) & \(\) & 270.0 \\   

Table 1: Comparison of open pretraining Datasets along the dimensions of transparency, versatility, and scale.

RedPajama-V1: An open Reproduction of the LLaMA Training Data

In our first iteration of the RedPajama datasets, our primary goal was to recreate the training data documented in the LLaMA technical report . To this end, we closely follow the descriptions of the original recipes. In this section, we first document our process for recreating the original LLaMA training corpus (Section 3.1). We highlight gaps in the description of the original dataset collection and describe how we choose to resolve those ambiguities. Next, we report on RedPajama-INCITE, a family of models trained on this corpus in collaboration with Oak Ridge National Lab (ORNL) (Section 3.2). We find that although the resulting models are performant at the 3B scale, there remains a gap at 7B to the original LLaMA-7B model. We hypothesize that this is partly due to the need to train with the FP16 precision. In addition, this also suggests the possibility that some salient details that went into the construction of the original LLaMA training corpus may be missing.

### Data Processing Steps

Here we describe our attempt to recreate the training corpus described in the LLaMa technical report . The pretraining data of the LLaMA training corpus are drawn from seven datasets: English CommonCrawl, C4, GitHub, Wikipedia, Books (Project Gutenberg and Books3), ArXiv, and Stack Exchange. Each of these datasets are given a short (approximately one-paragraph) description in the LLaMA technical report, and there are some gaps in the dataset descriptions. In this section, we detail our process for recreating each of the individual datasets, highlight the gaps in the descriptions from the LLaMA technical report, and describe our choices in resolving those ambiguities. These steps together resulted in a dataset of approximately 1.2 Trillion tokens. Table 2 summarizes these datasets and the token counts. In Table 10 in the Appendix, we further list all uncertainties encountered during the construction of the dataset.

**CommonCrawl.** The LLaMA corpus includes five CommonCrawl snapshots from 2017 to 2020, processed using the CCNet pipeline . CCNet deduplicates each snapshot in shards and assigns a quality classification to the data in each snapshot. It assigns a "head," "middle," and "tail" classification to each document based on the distribution of the perplexity assigned by a 5-gram Kneser-Ney model trained on Wikipedia. Here we only keep the "head" and "middle" buckets and discard the "tail." In addition, Touvron et. al.  use a linear classifier trained on Wikipedia reference articles to filter out low-quality documents. The LLaMA paper does not specify which snapshots were used in the dataset or give details on the classifier.

We select the five English CommonCrawl snapshots 2019-30, 2020-05, 2021-04, 2022-5, and 2023-06, representing the first snapshot in the five years preceding the start of the project. To train the Wikipedia reference classifier, we downloaded the most recent English Wikipedia snapshot available by April 1, 2023. We extract 38M URLs from the Wikipedia snapshot and crawl 300K pages. We then use the CCNet pipeline to apply a moderate cleaning step to the Wikipedia references and train a unigram classifier using fastText. Finally, we filter out all documents with scores less than 0.25, which reduces our CommonCrawl dataset to approximately the same size as the LLaMA CommonCrawl dataset.

**C4.** The LLaMA corpus includes the C4 dataset  to include diverse versions of CommonCrawl. We use the c4_en version of C4, which is provided by Allen AI on the Hugging Face Hub 2.

**Github.** The LLaMA corpus uses the public GitHub dataset available on Google BigQuery and keeps projects that are distributed under Apache, BSD, and MIT licenses. The LLaMA corpus additionally filters low-quality files using some heuristics and deduplicates at the file level. For RedPajama-V1, we remove low-quality files using a set of filters on file length, the proportion of alphanumeric characters, and file extensions. We provide the full list of heuristics in Appendix D.

   Dataset Slice & Token Count \\  CommonCrawl & 878B \\ C4 & 175B \\ GitHub & 59B \\ Books & 26B \\ ArXiv & 28B \\ Wikipedia & 24B \\ StackExchange & 20B \\  Total & 1.2T \\   

Table 2: Token counts for the RedPajama-V1 dataset.

**Wikipedia.** The LLaMA corpus uses Wikipedia dumps from June to August 2022 across 20 languages, processing the data to remove hyperlinks, comments, and other formatting boilerplate. For RedPajama-V1, we use the Wikipedia dataset available on Hugging Face Hub using the dump from 2023-03-20. This also preprocesses the data to remove hyperlinks, comments, and other boilerplate.

**Gutenberg and Books3.** The LLaMA corpus uses book corpora from the Gutenberg Project and Books3 from the Pile. We only use the PG19 subset of Gutenberg and use SimHash to remove near duplicates. We originally included Books3 as well but took it down due to copyright issues.

**ArXiv.** The LLaMA corpus processes arXiv LaTeX files and removes everything before the first section, comments, inline-expanded definitions and macros, and the bibliography, following . We downloaded arXiv data from Amazon S3 in the "arXiv" requester pays bucket and implemented a similar postprocessing, keeping only LaTeX source files and removing preambles, comments, bibliographies, and expanding macros.

**Stack Exchange.** The LLaMa corpus includes a dump of Stack Exchange. The data is kept from the 28 largest websites, HTML tags are removed from the text, and answers are sorted by score from highest to lowest. Similarly, we download Stack Exchange from the Internet Archive, keep only the posts from the 28 largest sites, and remove HTML tags. In addition, we group the posts into question-answer pairs and order answers by their scores.

### The RedPajama-INCITE family of LLMs

To evaluate how well RedPajama-V1 matches the original LLaMA corpus, we train a family of LLMs of various sizes in collaboration with the Incite project on the Summit supercomputer at Oak Ridge National Lab. The RedPajama-Incite family of LLMs includes a suite of pretrained and instruction-tuned models at the 3B and 7B model sizes. In this section, we first describe the compute setup of the Summit supercomputer and the implications for the pretraining runs (Section 3.2.1). We then describe how we evaluate the model and speculate on differences in quality between these models and the LLaMA family (Section 3.2.2).

#### 3.2.1 Summit Training Setup

In this section, we describe the Summit supercomputer and the engineering and pretraining challenges to training the RedPajama-Incite family of LLMs. Our language models were trained using the Summit supercomputer at Oak Ridge National Lab, a cluster containing 4608 6xV100 nodes running an IBM Power9 architecture. This setup introduced a few challenges in training modern LLMs. In the following, we discuss these challenges and describe how we overcame them.

The **IBM Power9 architecture** uses a different instruction set than most modern chipsets (i.e., Intel-, Arm-, or Apple-based chips). Modern versions of PyTorch and most of the Python stack they depend on are not pre-compiled to support the Power9 architecture (the latest version officially supported was PyTorch 1.9). To support pretraining with modern libraries, members of our team needed to recompile PyTorch from scratch and build a custom training stack for Summit. Some of these efforts are documented in more detail in the GPT-NeoX technical report .

As of this writing, the Summit supercomputer runs on **V100 GPUs**, which are older than A100 or H100 GPUs typically used to train LLMs. Critically, V100s do not support the bf16 data type, which is necessary for modern stable training recipes for LLMs. Instead, we had to train with fp16 and use loss scaling  to allow for stable training runs. We also had to lower the learning rate compared to those reported in the LLaMA training, which may have had an effect on convergence (\(1.6 10^{-4}\) for the 3B model and \(1.2 10^{-4}\) for the 7B model).

The IBM Power9 architecture had **slow interconnect**, limiting the number of nodes we could use for each run. We were also unable to use the entire cluster since other projects were running simultaneously on it. We used 512 nodes in parallel (3072 GPUs) to train the 7B and 256 nodes in parallel (1536 GPUs) to train the 3B, with a global batch size of 4M tokens for each model. In scaling experiments, we found that we could not further increase the amount of parallelism without increasing the global batch size, which would hurt convergence.

The **6xV100 nodes** introduce challenges for training with tensor and pipeline parallelism. We used 12-way pipeline parallelism for the 7B and 6-way for the 3B model, as well as 2-way tensor parallelism for both models.

After accounting for these challenges, we were able to train the 3B model for 800B tokens total and the 7B model for 1.001T tokens total on Summit. We decayed the learning rate linearly following a warmup period, matching those described in the original LLaMA paper.

#### 3.2.2 Evaluation

Here, we discuss evaluations for the RedPajama-INCITE-3B and 7B models on common benchmarks. The full results and benchmark scores are provided in Appendix D.2. After training RedPajama-Base-INCITE-3B for 800B tokens, it has better few-shot performance (measured in HELM classic ), as the average score over 16 core scenarios) and better zero-shot performance (measured using Eleuther AI's LM evaluation harness ) compared to other open models of similar size, including the well-regarded GPT-Neo and Pythia-2.8B (trained with 420B and 300B tokens, respectively, on the Pile). On HELM, it outperforms these models by 3-5 points. On a subset of tasks from LMevaluation harness, it outperforms these open models by 2-7 points.

The RedPajama-INCITE-7B-Base model is 1.0 points behind Falcon-7B and 4.1 points behind Llama-7B on HELM-classic. We further break down the tasks and see that they lag behind only on tasks that require using logprobs, which computes the difference between the probabilities of right and wrong answers. However, the model achieves comparable average HELM scores on tasks that directly generate answers and measure quality. Since all benchmarks in the LM harness use logprobs, we see similarly lower results for this benchmark. We hypothesize this was partly due to training with FP16, which does not allow us to use larger learning rates. Furthermore, as illustrated in the previous section, there were sources of uncertainty in the construction of the training dataset which likely resulted in a slightly different dataset than what was used to train the Llama-1 model. We believe that these two factors have led to the slightly lower performance compared to the Llama models.

RedPajama-INCITE-7B-Instrect is an instruction-tuned version of the base model optimized for few-shot performance by training on a diverse collection of NLP tasks from both P3 (BigScience)  and Natural Instructions (AI2) . The Instruct version shows excellent performance on few-shot tasks, outperforming leading open models of similar sizes, including Llama-7B, Falcon-7B (both base and instruct version), and MPT-7B (both base and instruct version) on HELM by 2-8 points. We provide the detailed evaluation scores in the supplementary material.

## 4 RedPajama-V2

In contrast to the first iteration of the RedPajama dataset, the second iteration focuses exclusively on web data and, in addition to design principles _Transparency_ and _Scale_, we also put a higher emphasis on _Versatility_. Specifically, next to the goals of providing a fully transparent and open dataset, the purpose of the corpus is to serve as a foundation for creating high quality subsets. While the goal of transparency is achieved by making the dataset and its artifacts publicly available, and scale is achieved by processing large parts of the Common Crawl corpus, to follow the design principle _Versatility_, we release RedPajama V2 as a dataset that is enriched with a set of metadata that enables fast and cheap iteration for creating high quality, diverse and large datasets. In this section, we first present the data processing steps used to create the raw text data, give an overview over the quality signals available for each document, and present statistics on the dataset composition. Finally, we present ablation studies on how the quality signals can be used to create successively better datasets.

### Data Processing Steps

RedPajama-V2 is a dataset created by processing web documents provided by the CommonCrawl foundation3. As web data is inherently noisy and only available as text embedded in the HTML code, it is necessary to process it to make it suitable for training LLMs. To that end, the raw data used for RedPajama-V2 undergoes a series of basic processing steps, which we explain in more detail.

#### 4.1.1 Data Acquisition

The Common Crawl Archive is a vast repository of web crawl data that is freely available to the public. The corpus contains crawling results since 2013 and is updated regularly on a (bi-) monthly basis. Next to raw web data in HTML (warc) format, the archive also provides metadata (wat) and plain text data in the wet format. It has been the basis for numerous datasets including C4 , RefinedWeb , Dolma , and FineWeb  among others.

To create the RedPajama-V2 dataset, we used the web-extracted text (i.e.,.wet files) from all 84 monthly snapshots between 2014 and April 2023 and passed it through the CCNet pipeline . In contrast to RPv1, here we keep all perplexity buckets, and in addition to the English language, we also keep French, German, Italian, and Spanish data. We chose this pipeline due to its light processing, which aligns with our guiding principle of preserving as much information in the raw dataset as possible and allowing downstream model developers to filter the dataset. This processing step produces over 100 billion individual text documents.

#### 4.1.2 Quality Signals

A central ingredient to state-of-the-art open LLMs like Llama [57; 58], Mistral , Falcon , MPT , or the Qwen  models is the large amount of high-quality data that these models are trained on. For example, Llama 3 is trained on 15 trillion carefully curated tokens. The most prominent data sources that provide the necessary scale are the crawls made publicly available by CommonCrawl. However, this raw text, which in our case is additionally processed by the CCNet pipeline, is still not ideal for direct use as LLM training data due to artifacts arising from the conversion from HTML to plain text (e.g., parsing errors, and menus), sources of generally low quality, and biases inherent to the distribution of content on the web. To clean such datasets, the literature has proposed a multitude of heuristics to extract high-quality datasets out of large corpora of heterogeneous web data. However, unlike previous datasets that filter out low-quality content, our approach retains the entire raw text corpus, incorporating quality signals as additional metadata. This strategy allows us to use the full spectrum of data, transforming sections typically discarded into informative attributes that enhance our dataset's utility. This enables the creation of other datasets such as C4 as special cases of the RedPajama-V2 dataset. For each document, we provide the quality signals used in C4 , Gopher , RefinedWeb , the Pretrainer's Guide  and DSIR . These can roughly be categorized into quality signals which measure _natural language_, the _repetiveness_ of the text, are based on the _content_ of the text, _ML-based_ heuristics, and _deduplication_. In the following, we explain each of these categories in detail. A comprehensive list with detailed descriptions encompassing all quality signals, as well as histograms is provided in Appendix E.2.

**Natural Language.** Text documents extracted from websites often have content that does not correspond to natural language, such as JavaScript code, menus, and other boilerplate text. To measure how natural a given text document is, we provide simple heuristic measures such as the fraction of all caps words or letters, the fraction of lines that end with an ellipsis, the fraction of unique words, whether or not a line ends in a terminal punctuation mark, and others.

**Repetitiveness.** An often observed artifact of web data is repetitive text, which has been linked with uninformative content . Repetitious generations are also a known failure mode of language models , and removing excessively repetitive content can potentially contribute to alleviating this behavior . For each document, we calculate the fraction of characters appearing in the most frequent (word) \(n\)-gram for \(n\{2,\,3,\,4\}\). Second, we calculate the fraction of characters appearing in any duplicated \(n\)-gram for values of \(n\{5,\,,\,10\}\). We ensure not to count characters that appear in overlapping \(n\)-grams more than once.

**Content-based.** Web documents can contain harmful and offensive content, which needs to be addressed. To that end we provide the signals used in C4 and RefinedWeb, namely, (1) the number of sequences of words that are contained in the LDNOOBW blocklist4. In addition, we include a flag which indicates whether the domain of the document appears in the UT1 list of blocked urls5. While these quality signals focus on NSFW content, we believe other content-based filters such as domains or embedding clusters  are also promising directions. In Figure 8 in the Appendix, we show the distribution of topics found via clustering of embeddings.

**ML Heuristics.** ML-based quality signals revolve around the idea of measuring similarity to a high-quality domain. Here, we use fastText classifiers , and the importance weights proposed in . While ML filters have been shown to improve the quality of datasets (e.g., [12; 57; 11]), they have also been reported to lead to biases or underrepresent minorities . The fastText classifier signals provided in RPv2 are unigram bag-of-word models trained to distinguish between unfiltered RPv2 data and a high-quality domain. For English data, we use Wikipedia, websites referenced by Wikipedia, books, and the OpenWebText dataset. For non-English data, we only use Wikipedia. The DSIR weights proposed in  estimate the importance of individual samples to a given target domain in a reduced feature space and are based on word unigrams and bigram models. The weights are defined as the log-likelihood ratio between a language model of the target vs. the source domain, where we use the same domains as for the fasttext classifiers.

**Deduplication.** Removing duplicated training data has been found to improve model perplexity and reduce the amount of memorization while reducing the training data size and the required compute . Deduplication is also one of the core building blocks of the most popular datasets [46; 52; 44]. In RPv2, we include MinHash signatures for fuzzy deduplication  at different similarity levels, as well as IDs of documents found to be exact duplicates via a Bloom filter  with the error rate set to \(1\%\)6. For this document-level deduplication, we proceed sequentially, starting with the most recent dump (2023-14) and successively iterating over the following dumps until we reach the oldest one (2014-15). An overview over how many documents were flagged as duplicates in this manner, can be seen in Figure 3 in the Appendix.

### Dataset Statistics

RPv2 consists of 113B documents in five different languages: English, German, French, Spanish and Italian. As mentioned previously, the CCNet pipeline partitions the dataset into the tree buckets "head", "middle", and "tail" corresponding to documents with low, medium, and high Wikipedia perplexity. There are 32.8B documents in the head+middle partition and 80B documents in the tail partition. Documents in the tail are typically shorter (850 tokens) than in the head and middle buckets (\( 1500\) tokens). Token counts were estimated based on an i.i.d. sample of 100M documents using the Mistral  BPE tokenizer. A detailed overview of token counts for each language and partition is given in Table 3. We provide further statistics on the number of documents before and after deduplication, as well as the distribution of quality signals in the supplementary materials.

### Dataset Ablations

Here we present a series of dataset ablations with the aim of developing a better understanding of how the quality signals introduced in Section 4.1.2 influence the downstream performance of language models trained on data filtered with different heuristics. More specifically, here we ask _how do different quality filtering rules affect downstream performance?_ We strive for a broad evaluation and measure the performance on diverse downstream benchmarks and the language modeling objective on multiple domains.

#### 4.3.1 Setup

**Models.** We adopt decoder-only Llama-2 architectures  with 468M parameters and 1.6B parameters and 2048 sequence length. Both models have 24 layers, 16 attention heads and the MLP

    &  &  &  &  &  (dedupe) \\   &  &  &  &  &  &  &  &  \\  English & 87.5 & 90.5 & 63.0 & 53.6 & 24.5 & 37.0 & 14.5 & 20.5 \\ German & 8.6 & 10.3 & 5.9 & 6.2 & 2.7 & 4.1 & 1.9 & 3.0 \\ French & 6.7 & 8.5 & 4.5 & 4.8 & 2.2 & 3.7 & 1.6 & 2.7 \\ Spanish & 6.9 & 9.5 & 4.7 & 5.6 & 2.3 & 3.9 & 1.8 & 2.8 \\ Italian & 3.5 & 4.7 & 2.4 & 2.7 & 1.2 & 1.9 & 0.9 & 1.5 \\  Total & 113.3 & 123.7 & 80.5 & 73.0 & 32.8 & 50.7 & 20.8 & 30.4 \\   

Table 3: Document and token counts for each partition and language of the RPv2 dataset.

[MISSING_PAGE_FAIL:9]

ablations, we filter the full RPv2 dataset, then sample roughly 1T tokens and deduplicate it with the same Minhash hyperparameters.

**Filters.** We seek to cover a wide range of quality filtering configurations. Rather than optimizing the performance on a particular benchmark, the goal is to show that filtering the RPv2 dataset in different ways can lead to wildly different model performance. We thus experiment with variations of the C4 and Gopher rules and also use the ML-based quality signals in RPv2. We also use a custom configuration custom-rules based on word counts, average line length, Wikipedia Perplexity and the Wikipedia References classifier.

**Results.** From Table 5, we can draw a series of conclusions on filtering the RedPajama-V2 dataset. _First_, we can see that the Gopher rules generally improve performance. In particular we see that fuzzy deduplication and filtering with Gopher has the highest aggregated scores across all RPv2 datasets. In addition, both the average and normalized average benchmark score is only second to RefinedWeb, while the rank-score is higher than for RefinedWeb. The per-benchmark tables 18, 19, and 20 in the Appendix, show that the RPv2 dataset filtered with fuzzy deduplication and Gopher is always in the upper middle (minimum rank score 9 of 19), while RefinedWeb is performing worse on Hellaswag, LAMBADA, Winogrande, MMLU and OpenBookQA. This indicates that filtering RPv2 with the full Gopher rules and fuzzy deduplication (Minhash LSH) creates a dataset that performs well across a wider range of tasks than all other datasets. _Second_, we can see that the Gopher-natlang filters perform better than the Gopher-repetition filters. _Third_, in the context of model based filtering, we see no significant difference between using a fasttext classifier and DSIR. _Fourth_, using only the line-level C4 filters appears to reduce perplexity, but has negligible effect on the aggregated benchmark scores. Finally, we notice that the unfiltered RPv2 2023-14 dataset appears to have the lowest perplexity on the Paloma dataset, while other filtering methods lead to models with higher perplexity. We believe that this can (at least in part) be attributed to the wide range of domains covered by Paloma. In addition, Paloma also contains the RPv1 dataset, which can explain the low Perplexity score obtained by the model trained on RPv1-CC. Table 6 shows further that the model trained on RPv2 filtered with the full Gopher rules outperforms the model trained on RPv2 filtered with only the Gopher-natlang rules, and comes close to the quality of a model trained on the RefinedWeb dataset. In conclusion, this series of ablation studies shows how the quality signals in the RPv2 dataset can be used to successively filter better datasets. In combination with its vast scale of over 100T tokens, we see that this dataset provides a powerful source for creating high-quality web datasets for LLM pretraining.

## 5 Conclusion

In this paper, we have presented the RedPajama datasets. With over 100 Trillion tokens, these are the largest, fully open, and transparent datasets for pretraining language models and have been a central building block for many of the strongest open-source LLMs. Next to documentation accompanying the datasets, we have also shown examples of how RedPajama-V2 can be filtered down to successively higher quality subsets, leading to language models of varying levels of quality on a diverse set of benchmark tasks and outperforming models trained on other large-scale pertaining corpora. While the models are relatively small and enabled us to explore a wider variety of filters, it is also a limitation and further, larger-scale explorations are required. We did not explore a thorough decontamination analysis against common benchmarks or an analysis of personally identifiable information present in the dataset, posing another limitation of this work. By publishing the RedPajama-V2 dataset in raw, unfiltered form, but accompanied by a set of quality signals, we hope that future work will continue to build on RedPajama and provide new innovative ways of filtering, curating, and mixing multiple pretraining corpora.

    &  &  &  & ^{}\)]} & ^{}\))} \\  & & & & & Palm Classif. & Wiki-Ref Classifier. & Avg. & Norm. Avg. & Rank-Score & Pile & Paloma \\  RefinedWeb & & & & & & 52.0 & 34.0 & 0.139 & 10.7 & 17.7 \\  RPv2 (full) & & & & & & & 50.0 & 31.1 & 0.106 & 13.6 & 20.8 \\ RPv2 (full) & & & & (natling) & & & 47.9 & 29.4 & 0.089 & 22.2 & 30.7 \\   

Table 6: Aggregated evaluations for the 1.6B parameter LM for different datasets. The Benchmark scores are aggregated from the benchmarks outlined in Table 4, using (1) the average accuracy, (2) the Rank-Score, and (3) the normalized average score.