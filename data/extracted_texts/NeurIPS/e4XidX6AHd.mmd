# Gacs-Korner Common Information Variational Autoencoder

Michael Kleinman\({}^{1}\) Alessandro Achille\({}^{2,3}\) Stefano Soatto\({}^{1}\) Jonathan C. Kao\({}^{1}\)

\({}^{1}\)University of California, Los Angeles \({}^{2}\)AWS AI Labs \({}^{3}\)Caltech

michael.kleinman@ucla.edu aachille@amazon.com

soatto@cs.ucla.edu kao@seas.ucla.edu

Work performed as external collaboration not related to Amazon

###### Abstract

We propose a notion of common information that allows one to quantify and separate the information that is shared between two random variables from the information that is unique to each. Our notion of common information is defined by an optimization problem over a family of functions and recovers the Gacs-Korner common information as a special case. Importantly, our notion can be approximated empirically using samples from the underlying data distribution. We then provide a method to partition and quantify the common and unique information using a simple modification of a traditional variational auto-encoder. Empirically, we demonstrate that our formulation allows us to learn semantically meaningful common and unique factors of variation even on high-dimensional data such as images and videos. Moreover, on datasets where ground-truth latent factors are known, we show that we can accurately quantify the common information between the random variables.2

## 1 Introduction

Data coming from different sensors often capture information related to common latent factors. For example, many animals have two eyes that capture different but highly-correlated views of the same objects in the scene. Similarly, sensors of different modalities, such as eyes and ears, capture correlated information about the underlying scene, as do videos and other time series, where the sensors are separated in time rather than in modality. Learning how information of one sensor maps to information of another provides a self-supervised signal to disentangle the variability that is intrinsic in a sensor from the latent causes (e.g., objects) that are shared between multiple sensors. Indeed, there is evidence that infants spend a long time during development purposefully experiencing objects through different senses at the same time .

Motivated by this, we propose to learn meaningful representations of multi-view data by quantifying and exploiting such structure in a self-supervised fashion by using an information theoretic notion of _common information_ as the guiding signal to disentangle common shared information present in high dimensional sensory data (Fig. 1).

However, defining a notion of common information is itself not trivial. The most natural and typical way to quantify the "common part" between random variables would be by quantifying their mutual information. But mutual information has no clear interpretation in terms of a decomposition of random variables in unique and common components. In particular,  note that there is generally no way to write two variables \(X\) and \(Y\) using a three part code \((A,B,C)\) such that \(X=f(A,C)\), \(Y=g(B,C)\) and where \(C\) encodes all and only the mutual information \(I(X;Y)\). Discovering thelargest common factor \(C\), which encodes what is known as the Gacs-Korner common information, from high dimensional data is then a distinct problem on its own [3; 4].

To the best of our knowledge, there are currently no approaches to compute or approximate the Gacs-Korner common information from high-dimensional samples . In this work, we seek to learn common representations that satisfy the constraint that they are (approximately) a function of each input. A contribution in this paper is that we generalize the constraint that the representation needs to be a deterministic function and allow it to be a stochastic map. As we later show, this is helpful for quantifying and interpreting the latent representation, and allows us to parameterize the optimization with deep neural networks.

We show that our objective can be optimized using a multi-view Variational Auto-Encoder (VAE). Since in general each view can contain individual factors of variation that are not shared between the views, we augment our model with a set of unique latent variables that can capture unexplained latent factors of variation, and show that the common and unique component can be efficiently inferred from data through standard training. While training the multi-view VAE, we simultaneously develop a scalable approximation for the Gacs-Korner common information, as we describe in Section 3.

To empirically evaluate the ability to separate the common and unique latent factors we introduce two new datasets, which extend commonly used datasets for evaluating disentangled representations learning: dSprites  and 3dShapes . For each dataset, we generate a set of paired views \((x_{1},\,x_{2})\) such that they share a set of common factors. This allows us to quantitatively evaluate the ability to separate the common and unique factors. We also compare our method to multi-view contrastive learning  and show that thanks to our definition we avoid learning degenerate representations when the views share little information.

We hypothesize that a key reason precluding the identification of latent generating factors from observed data is that receiving a single sample of a scene is quite limiting. Indeed, classical neuroscience experiments has shown that the ability to interact with an environment, as opposed to passively observing sensory inputs, is critical for learning meaningful representations of the environment .

## 2 Preliminaries and Related Work

The entropy \(H(x)\) of a random variable \(x\) is \(_{p(x)}[]\). The mutual information \(I(x;z)=H(z)-H(z|x)\). Another useful identity for mutual information that we use is \(I(x;z)=_{p(x)}[KL(p(z|x)||p(z))]\) where KL denotes the Kullback-Leibler divergence.

Gacs-Korner Common Information.The Gacs-Korner common information  is defined as

\[C_{GK}(x_{1};x_{2}):=_{z}H(z)\;\;z=f(x_{1})=g(x_{2}),\] (1)

where \(f\) and \(g\) are _deterministic_ functions. The Gacs-Korner common information is thus defined through a random variable \(z\) that is a deterministic function of both inputs \(x_{1}\) and \(x_{2}\). Among all

Figure 1: **High level schematic of approach.** Red denotes shared latent factors (size, shape, floor, background and object color) and black denotes unique latent (viewpoint). The aim is to extract \(z_{c}\), which is a random variable that is a function of both inputs \(x_{i}\). We also allow for unique latent variables \(z_{u}\) to capture information that unique to each view. The latent representations are used to reconstruct the inputs.

such random variables, \(z\) is the random variable with maximum entropy. This has also been referred to as the "zero error information" in applications to cryptography . It is an attempt to formalize and operationalize the idea of the common part between sources, which mutual information lacks. It is also a lower bound to the mutual information . To the best of our knowledge, there are no efficient techniques for computing the GK common information for high-dimensional \(x_{1},x_{2}\). We elaborate on the difference between GK common information and mutual information in App. E.

Variational AutoencodersVariational Autoencoders (VAEs)  are latent variable generative models that are trained to maximize the likelihood of the data by maximizing the _evidence lower bound_, or minimizing the loss:

\[_{}=_{p(x)}[\ _{q_{}(z|x)}[- p _{}(x|z)]+KL(q_{}(z|x)\ ||\ p(z))].\] (2)

The VAE loss can be motivated in an information theoretic manner as optimizing an Information Bottleneck , where the reconstruction term encourages a _sufficient_ representation and the KL regularization term encourages a _minimal_ representation . The addition of a parameter \(\) to modify how the KL regularization is penalized leads to the following loss (and corresponds to the traditional VAE loss when \(=1\)):

\[_{}=_{p(x)}[\ _{q_{}(z|x)}[-  p_{}(x|z)]+ KL(q_{}(z|x)\ ||\ p(z))].\] (3)

For larger values of \(\) the representations become more disentangled, shown analytically in  and empirically in , although reconstructions become worse.

Disentangled representationsA guiding assumption for representation learning is that the observed data \(x\) (i.e an image) can be generated from a (simpler) set of latent generating factors \(z\). Assuming the latent factors are independent, the idea of learning _disentangled_ representations involves learning these latent factors of variation in an unsupervised manner . However, despite apparent empirical progress in learning disentangled representations , there remains inherent issues in both learning and defining disentangled representations . In many cases, different independent latent factors may lead to equivalent observed data, and without an inductive bias, disentanglement remains ill-defined. For example, color can be decomposed into an RGB decomposition, or an equivalent HSV decomposition.

In [18, Theorem 1] it is shown that without any inductive bias, one cannot uniquely identify the underlying independent latent factors in a purely unsupervised manner from observed data. Empirically, they also found that there was no clear correlation between training statistics and disentanglement scores without supervision. Later, and related to our work, the authors examined the setting where there is paired data and no explicit supervision (weak supervision), and found that such a setup was helpful for learning disentangled representations . The authors examined the setting in which the set of shared latent factors changed for each example, which was necessary for their identifiability proof. This also required using the same encoder for each view, and thus is a restricted setting that does not easily scale to multi-modal data.

Here, we study the scenario where the set of generating factors is the same across examples, as in the case of a pair of fixed sensors receiving correlated data. Additionally, our objective is motivated in an information theoretic way and our method generalizes to the case where we have different sensory modalities, which is relevant to neuroscience and multi-modal learning. Finally, our variational objective is flexible and allows estimation of the _common information_ in a principled way.

Approximating Mutual InformationEstimating mutual information from samples is challenging for high-dimensional random vectors . The primary difficulty in estimating mutual information is constructing high-dimensional probability distribution from samples, as the number of samples required scales exponentially with dimensionality. This is impractical for realistic deep learning tasks where the representations are high dimensional. To estimate mutual information,  used a binning approach, discretizing the activations into a finite number of bins. While this approximation is exact in the limit of infinitesimally small bins, in practice, the size of the bin affects the estimator . In contrast to binning, other approaches to estimate mutual information include entropic-based estimators (e.g., ) and a nearest neighbours approach . Although mutual information is difficult to estimate, it is an appealing quantity to summarily characterize neural network behavior because of its invariance to smooth and invertible transformations. In this work, rather than estimate the mutual information directly, we study the "usable information" in the network , which corresponds to a variational approximation of the mutual information .

Contrastive and Multi-View ApproachesWhile (multi-view) contrastive learning aims to learn a representation of _only_ the common information between views [7; 29; 30], we aim to learn a decomposition of the information in the views into common and unique components. Our work naturally extends to multi-sensor data that have different amounts of common/unique information (e.g., touch and vision). Moreover, contrastive approaches assume that the unique information is nuisance variability, and discard this information. Similarly,  also seeks to identify common information in both views, but also does not provide an objective to retain the unique information. While the multi-view literature is broad, we are not aware of previous attempts to quantify the common and unique information. Related to our approach,  aim to find shared and private representations using VAEs, but it differs in how the alignment of shared information is specified and the resulting objective, and they do not provide a way to quantify the information content of the private and shared components. We discuss additional related work in Appendix C.

## 3 Method: Gacs-Korner Variational Auto-Encoder

Our formulation involves generalizing the Gacs-Korner common information in eq. (1) to the case where \(f\) and \(g\) are stochastic functions so that the optimization problem becomes:

\[_{GK}(x_{1};x_{2}) :=_{z}I(x_{i};z)\] (4) s.t. \[z =f_{s}(x_{1})=g_{s}(x_{2}),\] (5)

where \(f_{s}\) and \(g_{s}\) are _stochastic_ functions. By the equality in eq. (5), we mean that \(p(z|x_{1})=p(z|x_{2})\) for all \((x_{1},x_{2}) p(x_{1},x_{2})\). Note that when \(f\) and \(g\) are deterministic functions3 (which are a subset of stochastic functions), then \(H(z|x_{i})=0\) and we recover the original definition since

\[I(x_{i};z)=H(z)-H(z|x_{i})=H(z).\] (6)

Our latter generalization (eq. 4-5) is more amenable to optimization and interpretable, as we will later demonstrate. In eq. (4), we used \(x_{i}\) as a placeholder since when \(p(z|x_{1})=p(z|x_{2})\) for all \((x_{1},x_{2}) p(x_{1},x_{2},z)\) then \(I(z;x_{1})=I(z;x_{2})\) since

\[I(z;x_{1}) =_{x_{1} p(x_{1})}[KL(p(z|x_{1})||p(z))]\] \[=_{(x_{1},x_{2}) p(x_{1},x_{2})}[KL(p(z|x_{1})||p(z ))]\] \[=_{(x_{1},x_{2}) p(x_{1},x_{2})}[KL(p(z|x_{2})||p(z ))]\] \[=_{x_{2} p(x_{2})}[KL(p(z|x_{2})||p(z))]\] \[=I(z;x_{2}).\]

This means that another equivalent formulation to maximize is \(_{z}_{i}I(x_{i};z)=_{z}I(x_{i};z)\), for any \(i\). To optimize the objective in eq. 4-5, we need to learn a set of latent factors \(z\) that maximize \(I(x_{i};z)\), while satisying the constraint in eq. 5. We propose an optimization reminiscent of the VAE objective. Define \(x=(x_{1},x_{2})\) as the concatenation of both views, and \(z=(z_{u}^{1},z_{c},z_{u}^{2})\) as a decomposition of the representation into common and unique components, and \(z_{i}=(z_{u}^{1},z_{c})\). In particular, we seek to learn latent encodings through an encoder \(q_{}(z|x)\), which maps \(x\) to \(z\). To optimize the objective, the representation \(z\) should maximize \(I(x_{i};z)\), and so we should also learn a decoder \(p_{}(x|z)\) that minimizes \(H(x_{i}|z)\). This corresponds to the reconstruction term in a traditional VAE, though note here we reconstruct both views.

\[^{1}_{}=_{p(x)}[\,_{q_{}(z|x)}[ - p_{}(x|z)]]\] (7)

Without any constraints, this could be achieved trivially by using an identity mapping. To ensure that the latents encode only common information between the different views, we decompose the encodings to ensure the following constraint corresponding to eq. (5):

\[D(q_{_{c_{1}}},q_{_{c_{2}}})=KL(q_{_{c_{1}}}(z_{c}|x_{1})\,||\,\,q_ {_{c_{2}}}(z_{c}|x_{2}))\,\,=0.\] (8)

Here \(q_{_{c_{i}}}(z_{c}|x_{i})\) maps \(x_{i}\) to \(z_{c_{i}}\) Rather than enforcing a hard constraint, in practice it is easier to optimize the corresponding Lagrangian relaxation:

\[^{2}_{}=_{p(x)}[\,\,_{q_{}(z|x)}[ - p_{}(x|z)]+_{c}D(q_{_{c_{1}}},q_{_{c_{2}}})].\] (9)After optimizing this objective, for a sufficiently large \(\) so that \(D(q_{1},q_{2}) 0\), the common information would be:

\[_{GK}(x_{1};x_{2})=_{p(x)}[\;KL(q_{_{c_{i}}}(z_{c}|x_{i} )\;||\;q^{*}(z))\;],\] (10)

where \(q^{*}(z)\) is the marginal distribution induced by the encoder. However, estimating the true marginal \(q^{*}(z)\) is difficult for high-dimensional problems. In practice, we follow  and learn an approximate prior \(p(z) q^{*}(z)\), where both \(q_{}(z|x)\) and \(p(z)\) are taken from a given family of distributions (such as multivariate Gaussians with diagonal covariance matrix). This will additionally enable us to sample from the distribution, and interpret the latent factors. To learn \(p(z)\) we also add the following regularization to our training objective:

\[_{p(x)}[\;KL(q_{}(z|x)\;||\;p(z))\;].\] (11)

Alternatively, we can also exploit the degree of freedom in learning \(q_{}(z|x)\) and fix \(p(z)\) to be \((0,I)\). In both cases, our overall objective becomes:

\[_{}=_{p(x)}[\;_{q_{}(z|x)}[- p _{}(x|z)]+_{c}D(q_{_{c_{1}}},q_{_{c_{2}}})+ KL(q_{ }(z|x)\;||\;p(z))].\] (12)

Optimizing this objective alone could lead to unexplained components of information, for example the unique components. Alternatively, unique information present in the individual views may be encoded in the "common" latent variable if the reconstruction benefits outweighed the cost of the divergence between the posteriors of the encoders (the term corresponding to the \(\)).

In addition to these common latent components, we can learn unique latent components by optimizing a traditional VAE objective (i.e. with \(_{c}\) = 0) for a subset of the latent variables. Importantly we also need to ensure that the KL penalty for the unique component subset is greater than for the common subset (so that it is beneficial to encode common information in the common latent components). Our final objective becomes

\[_{}=_{p(x)}[\;_{q_{ }(z|x)}[- p_{}(x|z)]+_{c}D(q_{_{c_{1}}},q_{_{c_{2}}}) \\ +_{i=1}^{2}_{c}KL(q_{_{c_{i}}}(z_{c}|x_{i})\;||\;p(z_ {c}))+_{u}KL(q_{_{u_{i}}}(z_{u}|x_{i})\;||\;p(z_{u}))],\] (13)

where \(_{c}\) and \(_{u}\) correspond to a multiplier enforcing the cost of encoding common and unique information respectively. Importantly \(_{u}>_{c}>0\), resulting in a larger penalty on the unique latent variables (otherwise all the information would be encoded in the "unique" components). The summation in the bottom part of Eq. 13 corresponds to a summation over both encoders. \(p(z_{u})\) and \(p(z_{c})\) are both sampled from \((0,I)\) of appropriate dimensionality.

We now show that, if the network architecture used for the VAE implements a generic enough class of encoder/decoders our method will recover the GK common information.

**Theorem 3.1** (GK VAE recovers the common information).: Suppose our observations \((x_{1},x_{2})\) have GK common information defined through the random variable \(z_{c}\) satisfying eq. 4-5 and that our parametric function classes \(q(z|x)\) and \(p(x|z)\) optimized over can express any function. Then, our optimization (with \(_{c}=0\), \(_{u}<1\) and decoder \(p(x|z)=_{i=1}^{2}p_{i}(x_{i}|z_{i})\)) will recover latents \(=(_{u}^{1},_{u}^{2},_{c})\) where \(_{c}\) is the common random variable that maximizes the "stochastic" GK common information in eq. 4-5, while \(_{u}^{i}\) is the unique information of the \(i\)-th view, which maximizes \(I(x_{i};z_{u}^{i},_{c})\).

We provide the proof in Appendix A. Note that while the previous theorem guarantees that we will be able to separate the common and unique factors at the block level, we might not be able to disentangle the individual common factors.

### Quantifying the common information

Suppose \(D(q_{_{c_{1}}},q_{_{c_{2}}})=0\). The term corresponding to the rate \(R_{c}\) of the VAE

\[R_{c}=I_{q}(z_{c};x)=_{p(x)}KL(q_{_{c}}(z_{c}|x)\;||\;p(z_{c}))\] (14)

is neither an upper nor lower bound on the true common information. It represents an upper bound to the information encoded in the representation specified by \(q_{_{c}}(z_{c}|x)\), but does not bound the true common information in the data, since \(q_{_{c}}(z_{c}|x)\) itself is a variational approximation.

To find a lower bound on the common information encoded in the dataset, we can use any mutual information estimator \(\) that is a lower bound (see  for several). The approximate common information can then be quantified by \((z_{q},x)\), where \(z_{q} q_{_{c}}(z_{c}|x)\). We report both the rate \(R_{c}\) and \(\) in the paper. We emphasize that \(\) can be any mutual information estimator. When the data generating distribution is known, as in our synthetic examples, we employ the "Usable Information" estimator, described in Sect. 4.2, which is a variational approximation .

### Identifiability of the common and unique components

We now show the conditions under which our optimization will identify the common and unique latent components. Usually we do not directly observe the latent factors \(z\), but rather an observation generated from them. We may then ask whether the common latent factors can still be reconstructed from this observation. The following proposition shows that this is indeed the case, as long as the function generating \(f\) the observation is invertible, i.e., we can recover the latent factors from the observation itself.

**Proposition 3.1**.: (, Ex. 1): Define

\[z_{1}=(z_{c},z_{u}^{1}), z_{2}=(z_{c},z_{u}^{2})\]

where \(z_{c},z_{u}^{1}\), and \(z_{u}^{2}\) are mutually independent. Then for any invertible transformation \(t_{i}\) the random variable \(z_{c}\) encodes all the common information:

\[z_{c}=_{}C_{GK}(t_{1}(z_{1}),t_{2}(z_{2}))\]

We provide the proof in Appendix A. The above proposition shows that when a set of factors is shared between views and when the unique factors are sampled independently, then the GK common random variable corresponds to shared latent factors. In particular, if the observations \(x_{i}\) are generated through an invertible function \(x_{i}=f(z_{c},z_{u}^{i})\) where \(z_{c} p(z_{c})\) corresponds to the shared factors, the proposition shows that such factors can be recovered from the observations by maximizing the GK common information. In our GK VAE optimization, we optimize the "stochastic" GK common information and we also find in our experiments that we can (approximately) recover the latent factors from observations \(x_{i}\) generated from this process.

Figure 2: **Latent traversals and DCI plots show optimization results in separation of common and unique information. (Left) 3dshapes:** The top 3 rows shows the unique latents, the middle 3 the common (and the bottom 3 are the unique latents for the second view). The ground truth unique generative factors are \((0,1,2)\) corresponding to floor color, wall color, and object color. Our model correctly recovers that those factors are unique (first three rows in the figure), and that the other factors are common (middle three rows). **(Right) dsprites:** The top 2 rows shows the unique latent variables, the middle 3 the common (and the bottom 2 are the unique latent variables for the second view). The ground truth unique generative factors are \((3,4)\) corresponding to x-position, y-position respectively. Our model correctly recovers that those factors are unique (first two rows in the figure), and that the other factors are common (middle three rows).

## 4 Experiments

We train our GK-VAE models with Adam using a learning rate of \(0.001\), unless otherwise stated. When the number of ground truth latent factors is known, we set the size of the latent vector of the VAE equal to the number of ground truth factors. This choice was not necessary, and we obtain analogous results when the size of the latent vector of the VAE was larger than the number of ground-truth factors (Fig. 12). To improve optimization, we use the idea of free bits  and we set \(_{}=0.1\). This was easier than using \(\) scheduling , since it only involved tuning one parameter. We set \(_{u}\) to be \(10\), \(_{c}\) to be \(0.1\) and \(_{c}=0.1\). We trained networks for \(70\) epochs, except for the MNIST experiments, where we trained for \(50\) (details in the Appendix).

To ensure that the latents are shared to both encoders, during training we randomly sample \(z\) from either encoder \(q_{_{i}}(z_{c}|x_{i})\) with \(p=0.5\). We opted to randomly sample the latents from each encoder, as opposed to performing averaging, to ensure that the latent will always be a function of an individual view \(x_{i}\). This is in addition to the soft constraint governed by \(_{c}\) in the loss.4

### Evaluation Datasets

We primarily focus on the setting where the ground truth latent factors and generative model are known, in order to quantitatively benchmark our approach. To do so, we constructed datasets with ground truth latent factors so that some of the latent factors are shared between each views. That is, the generative model for the data \((x_{1},x_{2})\) is

\[x_{1}=f(z_{u}^{1},z_{c}), x_{2}=f(z_{u}^{2},z_{c}),\] (15)

where \(z_{c}\) is shared between the views and \(z_{u}^{i}\) is the unique information encoded in the \(i^{th}\) view and \(f\) corresponds to a rendering function.

To construct such datasets, we modified the _3dshapes_ and the _dsprites_ dataset . We select a subset of the latent factors to be shared between the views, while the remaining factors are sampled independently for each view. The _3dshapes_ dataset  contains six independent generating factors: floor color, background color, shape color, size, shape, and viewpoint. Each latent factor can only take one of a _discrete_ number of values. The _dsprites_ dataset  contains five independent generating factors: shape, scale, rotation, x and y position. Each latent factor can only take one of a _discrete_ number of values. When we generate multi-view data following the generative model in eq. (15), we refer to these datasets as _Common-3dshapes_ and _Common-dsprites_ respectively. We consider additional variants in the Appendix.

We also examine the _Rotated Mnist Dataset_. where the two views are two random digits of the same class to which a random rotation is applied. In particular, the class of the digit is common information between the views whereas the rotation is unique. We also examine the synthetic video dataset _Sprites_ (not to be confused with _dsprites_) described in  and evaluate the common information in frames separated \(t\) frames apart.

### Metrics

DCI Disentanglement Matrix .Let \(d\) be the dimension of the latent space and let \(t\) be the true generating factors. The idea is to train a regressor \(f_{j}(z):^{d}\) to predict the ground truth factors

    & floor hue (10) & wall hue (10) & obj. hue (10) & scale (8) & shape (4) & angle (15) & KL Total \\  Common & -0.01 & -0.02 & -0.03 & 2.73 & 1.98 & 3.83 & 15.0 \\ Unique & 3.31 & 3.31 & 3.31 & 0.19 & 0.37 & 0.19 & 12.7 \\ Total & 3.31 & 3.31 & 3.31 & 2.69 & 1.98 & 3.82 & 27.7 \\   

Table 1: Usable Information (in bits) in representation for _3dShapes_. The ground truth unique generative factors are _floor color_, _wall color_, and _object color_, and the common generative factors are _scale_, _shape_ and _angle_. The common information is separated from the unique information. The ground truth factors were almost perfectly encoded in the latents. The numbers in parenthesis represents the number of discrete factors for each latent variable.

\(t_{j}\) for each \(j\). This results in a matrix of coefficients that describe the importance of each latent for predicting each ground truth factors. This can then be visualized as a matrix where the size of the square reflects the coefficient. We use this metric to assess the partitioning of the learned common and unique representations. We used the random forest regressor, similar to  to predict a _discrete_ number of latent classes.

Usable Information .We use this to approximate the mutual information when \(H(x)\) is known, as it is in the datasets previously described. It is a lower bound to mutual information. We use this to lower bound the information contained in the representation \(z\) in the next section.

### Results

**Separation of Common and Unique Latent Variables.** We first examine whether our formulation can correctly separate the common and unique latent factors. After optimizing a network on our _Common-3dshapes_ dataset we examined how much information about the ground-truth latent factors were encoded in the common latents \(z_{c}\) and the unique latents \(z_{u}\) (Table 1).

Given the encoded representation specified by \(q_{}(z|x)\), we evaluated the usable information for the two latent components (\(z_{c}\) and \(z_{u}\)), as well as by using the complete latent variable \(z\). As done in previous work , we directly use the mean of \(q_{}(z|x)\) as our representation \(z\) rather than sampling. In Table 1, we see that the common and unique information was perfectly separated. Note, that information values reported are a lower bound to the true information, as our variational approximation is a lower bound to \(I_{q}(z;x)\) (which is itself a variational approximation). Our method accurately encodes all common information between views (ground truth: 3.32 bits for floor, wall, and background hue; 3 bits for scale; 2 bits for shape; 3.91 bits for orientation).

We also performed these analyses on the _Common-dsprites_ dataset and found similar results (Table 2, Appendix). In particular, the unique latent factors corresponding to position are encoded in the unique components of the latent representation, while the other factors are encoded in the common latent representation. We emphasize that the generative model was not used at all during training, and was only used for quantitative evaluation after training.

In Fig. 2, we show the DCI matrix  which visually reaffirm that the common and unique factors are properly identified at the block-level for both the _Common-3dshapes_ and _Common-dsprites_ datasets. We also include traversals of the prior shown in Fig. 2 to show qualitatively that the learned factors of variation are meaningful and can be interpreted. Additional runs are in Appendix F.

**Rotated Mnist and Comparison with Contrastive Learning.** As described before, we generate a dataset of paired views of digits of the same class, each rotated by an independent random amount. In this manner, the unique information is about the rotation, whereas the common information is about the class. In Fig. 3 we see that the unique components of the latent (rows 1, 2) appear to encode the

Figure 3: **Left. Traversals for _Rotated Mnist_. The unique components of the latent (rows 1,2) appear to encode the “thickness” and rotation of the digit, whereas the common components appear to represent the overall digit (rows 3-6); and also the output of view 1 does not depend on the latents in rows 7,8 (these correspond to the unique components for view 2.) Middle. Corresponding DCI matrix, where factor 0 corresponds to the label, while factor 1 corresponds to the rotation (discretized into 10 bins). Right. Comparison against contrastive implementation from , where the contrastive approach does not encode any usable information about the unique factor (the rotation).**

rotation and "thickness" of the digit, whereas the common components seem to represent the class of the digit (rows 3-6). Also, as expected, the output of view 1 does not depend on the latents in rows 7, 8 which by construction correspond to the unique components of view 2.

This setup is reminiscent of contrastive learning, where the goal is to learn a representation which is invariant to a random data augmentation of the input (such as a random rotation). By construction, contrastive learning aims to encode the common information before and after data augmentation, but may not encode any other information. This can lead to degraded performance on downstream tasks, as the discarded unique information may still be important for the task [29; 37]. On the other hand, our GK-VAE separates the unique and common information without discarding information.

To highlight this difference between approaches, we trained using a contrastive objective5, and found that indeed while we can decode the shared class label, we cannot decode the unique rotation angle of view 1 (discretized into \(10\) bins; Fig. 3, right). On the other hand, using our method we recover the common and unique information.

**Common information across time in sequences from videos.** The existence of common information though time is another important learning signal. To study it, we perform an experiment on the _Sprites_ dataset described in . This dataset consists of synthetic sequences all with \(8\) frames. We optimized using the same architecture and hyperparameters except we set \(_{c}=0.5\). We examine the common information between frames \(t\) frames apart, approximated using the KL divergence term. In particular, the two views are two frames \((x_{1},x_{t+1})\), where each pair belongs to a different video sequences. In Fig. 4 we see that in general, as \(t\) increases the common information between the frames decreases evidencing the fact that, due to the random temporal evolution of the video, common information is lost as time progresses. We also note that the common information appears to increase in the last frame; this could be that in many of the sequences the sprite returns close to the initial state (see Fig. 3 in ).

## 5 Discussion

We show formally and empirically that we can partition the latent representation of multi-view data into a common and unique component, and also provide a tractable approximation for the Gacs-Korner common information between high dimensional random variables, which has been a difficult problem . In many practical scenarios where high dimensional data comes from multiple sensors, such as neuroscience and robotics, it is desirable to understand and quantify what is common and what is unique between the observations. Motivated by the definition of common information proposed by Gacs and Korner , we propose a variational relaxation and show that it can be efficiently learned from data by training a slighly modified VAE. Empirically, we demonstrate that our formulation allows us to learn semantically meaningful common and unique factors of variation. Our formulation is also a generative multi-view model that allows sampling and manipulation of the common and unique factors.

Figure 4: Sprites  video experiment. **(Left)** Example views separated \(2\) frames apart. **(Right)** Common information as a function of delay between frames. In general common information is decreasing as the delay gets longer.

As the common information was motivated by an information theoretic coding problem , our work naturally relates to compression schemes. Indeed, approximate forms of the common information, discussed further in Appendix C, are scenarios for distributed compression, since the common information needs to only be transmitted once . It may be interesting to combine our approach with recent advances in practical compression algorithms that leverage VAEs .