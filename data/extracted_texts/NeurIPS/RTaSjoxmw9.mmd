# Outlier-Robust Phase Retrieval in Nearly-Linear Time

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Phase retrieval is a fundamental problem in signal processing, where the goal is to recover a (complex-valued) signal from phaseless intensity measurements. It is known that natural non-convex formulations of phase retrieval do not have spurious local optima. However, the theoretical analyses of such landscape results often rely on strong assumptions, such as the sampling vectors being Gaussian distributed.

In this paper, we propose and study the problem of outlier robust phase retrieval. We seek to recover a vector \(x^{d}\) from \(n\) intensity measurements \(y_{i}=(a_{i}^{}x)^{2}\), where the sampling vectors \(a_{i}\)'s are initially i.i.d. Gaussian, but a small fraction of the \((a_{i},y_{i})\) pairs are adversarially corrupted.

Our main result is a near-sample-optimal and nearly-linear-time algorithm that provably recovers the ground-truth \(x\) in the presence of adversarial corruptions. We first solve a lightweight convex program to find a vector close to the ground truth. We then run robust gradient descent starting from this initial solution, leveraging recent advances in high-dimensional robust statistics. Our approach is conceptually simple and provides a framework for developing robust algorithms for other tractable non-convex problems.

## 1 Introduction

Phase retrieval is a fundamental problem in signal processing with applications in various fields, including electron microscopy , crystallography [33; 36], astronomy , and optical imaging . In these applications, one often has access to only the magnitudes of the Fourier transforms of a complex signal. This is because measuring magnitude (e.g., by aggregating energy over time) is much easier than measuring phase (which requires detecting rapid changes). We refer the reader to the survey articles [37; 26] for more details about the theory and applications of phase retrieval.

In this paper, we focus on the real-valued generalized phase retrieval problem, where the Fourier transform is replaced by a general linear operator. We first give a formal definition of this problem.

**Definition 1.1** (Phase Retrieval).: Let \(x^{d}\) be the ground-truth vector. Let \(a_{1} a_{n}^{d}\) be \(n\) sampling vectors and let \(y_{i}= a_{i},x^{2}\) be the corresponding intensity measurements. Given \((a_{i},y_{i})_{i=1}^{n}\) as input, the task is to recover \(x\).

Note that it is impossible to distinguish between \(x\) and \(-x\), so it is sufficient to recover either one. Under certain assumptions (e.g., when the \(a_{i}\)'s are Gaussian distributed), the phase retrieval problem in Definition 1.1 can be solved in polynomial time with provable recovery guarantees. This was first achieved via approaches based on semidefinite programming (SDP) relaxations (see, e.g., Candes et al. ). In practice, the problem is often solved using first-order optimization algorithms such as gradient descent. It is well-established that, although many natural formulations of phase retrieval have nonconvex objectives, all local optima are globally optimal under certain assumptions [34; 3; 40]. An example of such objective function is the following:

\[ f(z)=_{i=1}^{n}(y_{i}- a_{i},z^{2})^{ 2} z^{d}.\]However, existing analyses of such landscape results often rely on strong assumptions, such as the sampling vectors \(a_{i}\)'s are i.i.d. Gaussian. Our work is motivated by the following questions: Can we relax the assumptions used in proving landscape results in many tractable nonconvex problems? In the context of phase retrieval, what happens if a small fraction of the \((a_{i},y_{i})\)'s are changed adversarially? We focus on the following strong contamination model (see, e.g., ).

**Definition 1.2** (\(\)-Corruption).: An algorithm first specifies the number of samples \(n\), and \(n\) samples are drawn independently from some unknown distribution \(D\). The adversary is allowed to replace up to \( n\) samples with arbitrary points. The modified set of \(n\) samples is then given to the algorithm as input. We say that a set of samples is \(\)-corrupted if it is generated by the above process. 1

Under the \(\)-corruption model for high-dimensional data, a common goal is to design efficient algorithms that can achieve dimension-independent error guarantees. Early work in robust statistics  provided sample-efficient estimators for various tasks, but with runtimes exponential in the dimension. A recent line of work, initiated by , has developed computationally efficient robust algorithms for many fundamental high-dimensional tasks. There has been significant progress in the algorithmic aspects of robust high-dimensional statistics (see, e.g., ).

We now formally define the main problem that we pose and study in this paper.

**Problem 1.3** (Outlier-Robust Phase Retrieval).: Let \(>0\). Let \(x^{d}\) be the ground-truth vector with \(\|x\|_{2}=1\). First, \(n\) sampling vectors \((a_{i})_{i=1}^{n}\) are drawn i.i.d. from \((0,I)^{d}\). Let \(y_{i}= a_{i},x^{2}\) be the corresponding intensity measurements. Then, an adversary arbitrarily corrupts an \(\)-fraction of the \((a_{i},y_{i})\)'s. Finally, the corrupted \((a_{i},y_{i})\)'s are given to the algorithm as input. The task is to find a vector \(z^{d}\) such that \(\{\|z-x\|_{2},\|z+x\|_{2}\}\) for some precision parameter \(>0\).

Note that we allow corruption in both the sampling vectors \(a_{i}^{d}\) and the intensity measurements \(y_{i}\). We would like to answer the following algorithmic question:

_Can we design a provably robust and near sample-optimal algorithm for the \(\)-corrupted phase retrieval problem (Problem 1.3) that runs in nearly-linear time?_

### Our Results and Contributions

In this paper, we answer the above question affirmatively. We first state the main result of our paper.

**Theorem 1.4** (Main, Informal).: _Consider the outlier-robust phase retrieval problem (Problem 1.3). Let \(>0\). Given an \(\)-corrupted set of \(n=(d^{2}(1/))\) samples, we can compute \(z^{d}\) in time \((nd)\) such that \((\|z-x\|_{2},\|z+x\|_{2})\) with probability at least \(0.8\)._

Our algorithm has near-optimal sample complexity, because even without corruption, recovering the ground-truth vector \(x\) in general requires \((d)\) samples because there are \(d\) degrees of freedom in \(x\). Moreover, our algorithm runs in time nearly-linear in the size of the input, and provably recovers the ground-truth vector \(x\) with arbitrary precision \(\). The formal version of Theorem 1.4 is stated as Theorem 3.1 in Section 3.

We remark that the success probability of Theorem 1.4 can be boosted to \(1-\) for any \(>0\) by incurring an additional factor of \(T=O((1/))\) in the sample complexity and runtime. We can randomly partition the input into \(T\) equal-sized disjoint sets and run our algorithm on each set to obtain \(T\) solutions \(Z=\{z_{1},,z_{T}\}\). If we output a solution \(z^{*}\) that has the maximum number of points in \(Z\) within distance \(2\), we can show that \(r(z^{*}) 3\) with probability at least \(1-\).

Our main conceptual contribution is to propose and study the outlier-robust phase retrieval problem, where a small fraction of the input data is adversarially corrupted. Note that we allow arbitrary corruption in both the sampling vectors \(a_{i}^{d}\) and the intensity measurements \(y_{i}\). The nonconvex optimization landscape of phase retrieval is well understood when the \(a_{i}\)'s are Gaussian distributed, but the adversarial robustness of such landscape results is largely unexplored.

Our main technical contributions include the design and analysis of a near sample-optimal and nearly-linear time algorithm that provably solves the phase retrieval problem in the presence of outliers. Our approach provides a conceptually simple two-step framework for developing outlier-robust algorithms for tractable nonconvex problems that combines the robustness of spectral initialization and the efficiency of the subsequent robust gradient descent.

### Our Approach and Techniques

When there are infinite samples and no corruption, the objective function \(f(z)\) can be simplified as

\[f(z)=}_{a(0,I_{d})}[ a _{i},z^{2}-y_{i}^{2}]=3\|x\|_{2}^{4 }+3\|z\|_{2}^{4}-2\|x\|_{2}^{2}\|z\|_{2}^{2}- 4 x,z^{2}.\] (1)

Even though \(f(z)\) is nonconvex, we know that it has no spurious local optima .

Our approach follows the general structure of Candes et al. , which uses a two-step procedure. The first step uses spectral techniques to find an initial guess that is close enough to the ground truth. The second step applies gradient descent to converge to the final solution. However, both steps are susceptible to adversarial corruption. We develop nearly-linear time and provably robust algorithms for both steps and combine them to get our main result.

**Step 1: Robust Spectral Initialization.** When there is no adversarial corruption, the empirical second-moment matrix \(Y=(1/n)_{i=1}^{n}y_{i}a_{i}a_{i}^{}\) has expectation \(}[Y]=I+2xx^{}\), so its top eigenvector is close to \(x\). However, the adversary can arbitrarily change the top eigenvector.

To circumvent this issue, we assign a (nonnegative) weight \(w_{i}\) to each sample, and let \(Y_{w}\) denote the weighted intensity-based second-moment matrix \(Y_{w}=_{i=1}^{n}w_{i}y_{i}a_{i}a_{i}^{}\). Ideally, if the weights \(w\) are uniformly distributed on the remaining clean samples, the top eigenvector of \(Y_{w}\) will align with \(x\). We propose a novel optimization problem that can be used to find a weighting \(w\) such that \(Y_{w}\) must be close to the unknown unbiased expectation \(I+2xx^{}\). Moreover, we show that such a weight \(w\) can be computed in nearly-linear time.

**Step 2: Approximate Gradient Descent.** Starting with the initial guess \(z_{1}^{d}\) produced by the robust spectral initialization, we want to apply gradient descent to recover the ground truth \(x^{d}\). Without corruption, if the initialization is close enough to \(x\), each iteration will bring \(z\) closer to \(x\) by a constant factor. This convergence guarantee can be compromised by the corrupted samples.

At a high level, approximating the gradient at a specific point amounts to a robust mean estimation problem (for the underlying distribution of the gradients). When the input data is \(\)-corrupted, the gradients of the \(n\) samples can be viewed as an \(\)-corrupted set of vectors. We can approximate the true gradient using this \(\)-corrupted set of \(n\) gradients using robust mean estimation algorithms.

### Related and Prior Works

**Phase Retrieval.** The problem of phase retrieval arises in many areas of science and engineering . Early research on this problem proposes error-reduction algorithms . Convex and nonconvex optimization with various objective functions were later proposed and achieved exact recovery . Follow-up works generalize to robust phase retrieval where the observations are subject to perturbations .

**Nonconvex Optimization.** Even though optimizing a nonconvex function is NP-Hard in general, recent works showed that many nonconvex functions are locally optimizable due to discrete or rotational symmetry. Besides phase retrieval, it is known that all local optima are globally optimal for natural nonconvex formulations of a wide range of machine learning problems, such as matrix completion , matrix sensing , phase synchronization , dictionary learning , and tensor decomposition  (see also Chapter 7 of ). Closely related to our work, a recent line of work explored the robustness of these landscape results:  studied matrix sensing in the \(\)-corrupted model and  studied matrix completion and matrix sensing in semi-random models.

**High-Dimensional Robust Statistics.** Recent works in high-dimensional robust statistics developed nearly-linear time algorithms for the problem of robust mean estimation . Prior works  developed meta-algorithms for finding _first-order_ stationary points with dimension-independent accuracy guarantees, which is closely related to the robust gradient descent procedure that we use.

### Roadmap

We first introduce notations and background in Section 2. Then we give an overview of our approach in Section 3. Next, we focus on how to get an initialization that is close enough to the ground truth \(x\) in Section 4. After the initialization, we use robust mean algorithms to estimate gradients to converge to the desired accuracy in Section 5. Finally, we conclude in Section 6 and discuss open problems.

## 2 Preliminary and Background

**Notation.** We write \([n]\) for the set of integers \(\{1,,n\}\). We use \(\{e_{1},,e_{d}\}\) for the standard unit vector basis in \(^{d}\) and \(I\) for the identity matrix. For a vector \(x\), we denote its \(_{1}\), \(_{2}\) and \(_{}\) norm as \(\|x\|_{1}\), \(\|x\|_{2}\) and \(\|x\|_{}\), respectively, and write the \(i^{}\) coordinate in \(x\) as \(x_{i}\). For vectors \(x,y^{d}\), we denote its inner product as \( x,y=x^{}y\). For a matrix \(A\), we use \(\|A\|_{2}\), \(\|A\|_{*}\), and \(\|A\|_{F}\) as its operator norm, nuclear norm, and Frobenius norm, respectively. We write \(_{k}(A)\) as the \(k\)th-largest eigenvalues of \(A\), and \(_{k}(A)\) as the sum of the \(k\) largest eigenvalues. A symmetric \(n n\) matrix \(A\) is said to be positive semidefinite (PSD) if for all vectors \(x^{n}\), \(x^{}Ax 0\). For two symmetric matrices \(A\) and \(B\), we write \(A B\) when \(B-A\) is positive semidefinite.

**Packing SDP.** We will use nearly-linear time solvers for the following packing SDP.

\[_{w}\ \|w\|_{1}_{i=1}^{n}w_{i}A _{i} I,_{k}(_{i=1}^{n}w_{i}B_{i})  k, w_{i} 0, i.\] (*)

**Lemma 2.1** ().: _Given an instance of optimization (*) with semi-positive definite matrices \(A_{i}^{d_{1} d_{1}}\) and \(B_{i}^{d_{2} d_{2}}\) with \(A_{i}=C_{i}C_{i}^{}\), \(B_{i}=D_{i}D_{i}^{}\) for all \(i=1,2,,m\), together with integer \(k>0\), error tolerance \(_{0} 1/m^{2}\), and failure probability \(_{0}\), there is an algorithm that runs in time \(((t_{C}+t_{D}+d_{1}+d_{2})(1/_{0},  1/_{0}))\), where \(t_{C_{i}}\) and \(t_{D_{i}}\) are the time take to perform a matrix product with \(C_{i}\) and \(D_{i}\) respectively and \(t_{C}=_{i=1}^{n}t_{C_{i}}\) and \(t_{D}=_{i=1}^{n}t_{D_{i}}\), and outputs \(w^{}\) with \(\|w^{}\|_{1}(1-_{0})\) where \(\) is optimal value, with probability at least \(1-_{0}\)._

**Computing the Top Eigenvector.** We use power method to compute the top eigenvector of a matrix.

**Lemma 2.2** (Power Method for Top Eigenvector, e.g., ).: _Let \(A^{d d}\) and let \(_{1}\) be its largest eigenvalue. For any \((0,1)\), there exists an algorithm that takes \(A\) and outputs a unit vector \(x^{d}\) in time \(O(t(d)/)\) such that \(x^{T}Ax(1-)_{1}\) with probability at least \(0.99\), where \(t\) is the time required to compute \(Av\) for an arbitrary \(v^{d}\)._

**Robust Mean Estimation.** Another tool we use is robust mean estimation in the \(\)-corruption model for distributions with bounded covariance. We use robust mean estimation algorithms to approximate the true gradient under adversarial corruption.

**Lemma 2.3** (Robust Mean Estimation, e.g., ).: _Let \(\) be a distribution on \(^{d}\) with unknown mean \(\) and unknown covariance matrix \(\) where \(^{2}I\). Let \(_{0}\) be a sufficiently small universal constant. Let \(0<<_{0}\) and \(>0\). Given an \(\)-corrupted set of \(n\) samples drawn from \(\), we can output a vector \(^{d}\) in time \((nd(1/))\) such that, with probability at least \(1--(-n)\), we have \(\|-\|_{2}=O+ {nd}}++/)}{n}}\,\)._

## 3 Overview

We first state a formal version of our main result.

**Theorem 3.1** (Main).: _Consider the setting of Problem 1.3. Let \(0<<^{}\) for some universal constant \(^{}\) and let \(>0\). Given an \(\)-corrupted set of \(n=(d^{2}(1/))\) samples, we can compute a vector \(z^{d}\) in time \((nd(1/))\) such that \(r(z)=\{\|z-x\|_{2},\|z+x\|_{2}\}\) with probability at least \(0.8\)._

Theorem 3.1 requires two key technical lemmas: the robust spectral initialization (Lemma 3.2) and the approximate gradient descent (Lemma 3.3).

We first show that the spectral initialization can be done in nearly linear time with high probability, the proof of which can be found in Section 4.

**Lemma 3.2** (Robust Spectral Initialization).: _Under the setting of Problem 1.3, for any \(0<<^{}\) for some universal constant \(^{}>0\), given an \(\)-corrupted set of \(n=(d)\) samples, we can compute a vector \(z_{0}^{d}\) of the ground truth \(x\) in time \((nd)\) such that \(r(z_{1})=\{\|z_{1}-x\|_{2},\|z_{1}+x\|_{2}\} \) with probability at least \(0.95\)._Then, with such initialization results, we can proceed to show that an approximate gradient descent algorithm can be used to find an arbitrary approximation of the ground truth in Section 5.

**Lemma 3.3** (Robust Gradient Descent).: _Consider the setting of Problem 1.3. Let \(>0\) be the desired precision. Let \(0<<_{0}\) for some sufficiently small universal constant \(_{0}\). Given an \(\)-corrupted set of \(n=(d^{2}(1/))\) samples and an initial guess \(z_{1}\) such that \(r(z_{1})=(\|z_{1}-x\|_{2},\|z_{1}+x\|_{2}) 1/8\), we can compute a vector \(z^{d}\) in time \((nd)\) such that \(r(z)\) with probability at least \(0.95\)._

For technical reasons, we cannot use the same set of samples for both the robust spectral initialization and the approximate gradient descent. Therefore, we partition the \(\)-corrupted set of \(2n\) samples into two equally sized disjoint sets, using one set for each algorithm.

Proof of Theorem 3.1.: Let \(2n=(d^{2}(1/))\) be a set of \(/2\)-corrupted samples. We partition the input into two disjoint sets of \(n\) samples. Both sets are \(\)-corrupted. By Lemmas 3.2 and 3.3, for any \([0,^{}]\) and \(>0\), our algorithm takes the first set of samples and output a vector \(z^{}\) in time \((nd)\) such that \(r(z^{}) 1/8\) with probability at least \(0.95\). Then, using \(z^{}\) and the second set of samples, our algorithm can output \(z^{d}\) in time \((nd)\) such that \(r(z)\) with probability at least \(0.95\). The overall success probability is at least \(0.8\), and the combined running time is \((nd)\). 

## 4 Robust Spectral Initialization

We dedicate this section to proving Lemma 3.2: Given an \(\)-corrupted set of \((a_{i},y_{i})\)'s, we can compute an initial guess \(z_{1}^{d}\) that is close to the ground truth \(x\), where \((\|z_{1}-x\|_{2},\|z_{1}+x\|_{2}) 1/8\). To build some intuition, consider the following intensity-based covariance matrix \(=_{i=1}^{n}y_{i}a_{i}a_{i}^{}\), where each \(a_{i}\) is drawn independently from \((0,I)\) and \(y_{i}= a_{i},x^{2}\). The expectation of this matrix is \([Y]=I+2xx^{}\). In other words, when there are enough samples and no adversarial corruption, we can obtain a good guess of the ground truth \(x\) (or \(-x\)) by computing the top eigenvector of \(Y\). However, we cannot rely on this approach in adversarial settings.

To tackle this issue, we propose a nearly-linear time preprocessing step (Algorithm 1) that can recover the true expectation of \(\) under adversarial corruptions. Algorithm 1 assigns a non-negative weight to each sample. For a weight vector \(w^{n}\) and a set of indices \(S[n]\), the weighted intensity-based covariance matrix is defined as \(Y_{S,w}_{i S}w_{i}y_{i}a_{i}a_{i}^{}\), and we omit \(S\) when \(S=[n]\). The feasible region for the weight vector is: \(_{n,}:=\{w^{n}:\|w\|_{1}=1 i[n],0 w_{i}\}.\)

A weight \(w\) defines an empirical distributions over the samples \((a_{i},y_{i})_{i=1}^{n}\), where the largest probability assigned to any point is \(\). Ideally, we would like to find a weight vector \(w^{*}_{n,}\) that assigns its weight uniformly to all the uncorrupted samples, i.e., \(w_{i}^{*}= 1_{i G}\). To find a suitable weighting \(w\), we use the following optimization problem (**) in which \(_{2}\) returns the sum of the top two eigenvalues (commonly known as the Ky Fan \(k\) norm for \(k=2\)).

\[_{w} _{2}_{i=1}^{n}w_{i}y_{i}a_{i}a_{i}^{ } 0 w_{i}, i[n],_{i=1}^{n}w_{i}=1\.\] (**)

At a high level, our main observation is that \(y_{i}a_{i}a_{i}^{}\) is always a positive semidefinite matrix as \(y_{i} 0\). Consequently, the adversary can only _add_ extra directions with large eigenvalues, but will not be able to remove the eigendirection of \(x\). By minimizing the Ky Fan \(2\) norm, we can remove any directions added by the adversary and make sure that the only remaining large eigendirection is close to \(x\).

Let \( 0\) be some constant to be determined. We show that we can obtain a robust spectral initialization by solving the packing SDP problem (*), which can be solved efficiently using Lemma 2.1. In particular, to fit the reweighting problem of (**) into the framework of the generalized packing problem (*), we define the following constraint matrices for all \(i[n]\) :

\[A_{i}:=(1-)n e_{i}e_{i}^{},\ B_{i}:=(1- )y_{i}a_{i}a_{i}^{}.\] (2)

The matrices \((A_{i})_{i=1}^{n}\) are used to implement the constraint that \(w_{n,}\). The matrices \((B_{i})_{i=1}^{n}\) help make sure the sum of the top two eigenvalues of \(Y_{w}\) must be at most roughly \(4\), because \(_{2}(Y_{w^{*}}) 4\).

First, we show that the weight \(w^{}\) computed by Algorithm 1 can ensure the weighted intensity-based covariance matrix \(Y_{w^{}}\) is close enough to the unbiased expectation \(I+2xx^{}\).

**Lemma 4.1**.: _With probability at least \(0.98\), the \(w^{}\) outputted by Algorithm 1 satisfies:_

\[\|Y_{w^{}}-(I+2xx^{})\|_{2}=O()\] (3)

In order to show Lemma 4.1, we need the following auxiliary Lemma 4.2, the proof of which can be found in Section A. Intuitively, Lemma 4.2 suggests that any weight \(w\) in the feasible region \(_{n,2}\) will not have a huge impact on the properties of uncorrupted measurements.

**Lemma 4.2**.: _For any \(_{0}>0\), and sufficiently small \( 0\), given a set of \(n\)\(\)-corrupted samples with \(n>(d)\), with probability at least \(0.98\), we have \(\|Y_{G,w}-(I+2xx^{})\|_{2}_{0}\) for all \(w_{n,2}\)._

Using Lemma 4.2, we provide a proof sketch for Lemma 4.1, and defer the details to Section A.

Proof.: We condition on the fact that the event of Lemma 4.2 holds (with probability at least \(0.98\)) for \(_{0}=\). Thus, for the remaining of the proof, we assume that for all \(w_{n,2}\), it holds that \(\|Y_{G,w}-(I+2xx^{})\|_{2}\).

Let \(_{1}\) and \(_{2}\) be the top two eigenvalues of \(Y_{w^{}}\), with \(v_{1}\) and \(v_{2}\) to be their corresponding eigenvectors.

Note that the largest eigenvalue of \(I+2xx^{}\) is \(3\), and the rest of the eigenvalues are all \(1\). In the proof, we show that the eigenvalues of \(Y_{w^{}}\) are also close to the ones of \(I+2xx^{}\). Our proof consists of two parts. We first establish lower bounds for \(_{1}\) and \(_{2}\), and then find an upper bound for \(_{1}+_{2}\).

Lower Bound.Since \(y_{i}a_{i}a_{i}^{} 0\) for any \(i[n]\), for any positive weight vector \(w_{n,}\), we have \(Y_{G,w} Y_{w}\). Thus a lower bound on eigenvalues of \(Y_{G,w^{}}\) will also be a lower bound on \(Y_{w^{}}\).

For the top eigenvalue \(_{1}\) of \(Y_{w^{}}\), it holds

\[_{1}=v_{1}^{}Y_{w^{}}v_{1} x^{}Y_{w^{}}x x ^{}Y_{G,w^{}}x x^{}(I+2xx^{})x-=3-.\] (4)

Similarly, for the second largest eigenvalue \(_{2}\) of \(Y_{w^{}}\), we have:

\[_{2}=v_{2}^{}Y_{w^{}}v_{2} v_{2}^{}Y_{G,w^{}}v _{2} v_{2}^{}(I+2xx^{})v_{2}-=1+2 v_{2},x ^{2}- 1-.\] (5)

Upper Bound.Through the optimization problem (*), a weight \(w^{}\) is calculated such that \(Y_{w^{}}\) are operator-norm upper-bounded by the constraint parameters. Let \(\) be the value of the optimal solution of the optimization problem (*). The desired uniform weight vector over the good samples \(w^{*}_{n,2}\) is also a feasible solution to this optimization problem because \(Y_{w^{*}}\) satisfy the optimization constraints due to Lemma 4.2. Since \(w^{}\) is an \(_{0}\)-approximation to the problem, we have

\[\|w^{}\|_{1}(1-_{0})(1- _{0})\|w^{*}\|_{1}=1-_{0}\]

By optimization constraints, the Ky Fan \(2\)-norm of \(_{i}w_{i}^{}B_{i}=(1-)Y_{w^{}} 2\), and consequently,

\[_{1}+_{2}=\|_{1}} {}_{2}(Y_{w^{}}))(1-)}.\] (6)

By combining inequalities (4), (5), and (6), we have shown that the top two eigenvalues of \(Y_{w^{}}\) are close to \(3\) and \(1\). Since the rest of the eigenvalues of \(Y_{w^{}}\) can also be bounded, we can conclude that \(\|Y_{w^{}}-(I+2xx^{})\|_{2}=O()\).

We can now show the closeness between the top eigenvector of \(Y_{w^{}}\) and the ground truth.

**Lemma 4.3**.: _There exists an universal constant \(^{}\) such that if \(0^{}\), and Algorithm 1 receives in input an \(\)-corrupted set of samples, then it outputs \(z_{1}^{d}\) such that with probability at least \(0.95\) it holds \(r(z_{1})\)._

Proof.: We condition on the fact that the event of Lemma 4.1 holds (with probability at least \(0.98\)). Let the eigendecomposition of \(Y_{w^{}}\) be \(Y_{w^{}}=_{i[d]}_{i}v_{i}v_{i}^{}\), where \(_{1}_{d}\). Under the basis \(\{v_{1},,v_{d}\}\), the ground truth \(x\) can be represented as \(x=_{i[d]}_{i}v_{i}\). Note that \(\|x\|_{2}^{2}=_{i[d]}_{i}^{2}=1\). By Lemma 4.1, we have \(\|Y_{w^{}}-(I+2xx^{})\|_{2}=O()\). Thus, we have

\[x^{}Y_{w^{}}x  3-O()\] \[x^{}Y_{w^{}}x _{1}_{1}^{2}+_{2}(1-_{1}^{2}) (3+O())_{1}^{2}+(1+O())(1-_{1}^{2}) 1+2 ^{2}+O().\]

This implies \(_{1}^{2} 1-O()\). As a result,

\[r^{2}(v_{1}) =(\{\|v_{1}-x\|_{2}^{2},\|v_{1}+x\| _{2}^{2}\})=\{(_{1}-1)^{2},(_{1}+1)^{2}\}+_{i=2}^{d }_{i}^{2}\] \[=\{2-2_{1},2+2_{1}\}=O().\]

The last inequality holds as long as \(\) is sufficiently small. Let \(z_{1}=_{i[d]}_{i}v_{i}\) be the unit vector approximating \(v_{1}\) returned by the algorithm. By Lemma 2.2, we have that \(z_{1}^{}Y_{w^{}}z_{1}(1-)_{1}\) with probability at least \(0.99\). Thus, we have:

\[z_{1}^{}Y_{w^{}}z_{1} (1-)_{1}(1-)(3-O ()) 3-O(+)\] \[z_{1}^{}Y_{w^{}}z_{1} _{1}_{1}^{2}+_{2}(1-_{1}^{2}) 1+2 _{1}^{2}+O().\]

Again, this implies that \(_{1}^{2} 1-O(+)\). We can show that \(\{\|v_{1}-z_{1}\|_{2}^{2},\|v_{1}+z_{1}\|_{2}^{2}\}= O(+)\). By the triangle inequality, we can conclude that \(r^{2}(z_{1})=O(+) 1/64\), where the last inequality is obtained by choosing sufficiently small \(\) and \(\). Therefore, there exists an universal constant \(^{} 0\) such that for all \(0^{}\), Algorithm 1 takes \(n=(d)\) samples and outputs \(z_{1}\) such that \(r(z_{1}) 1/8\) with probability at least \(0.95\). 

**Lemma 4.4**.: _Algorithm 1 runs in time \((nd)\)._

Proof of Lemma 4.4.: Since we have the factorization of the rank-two matrices \(A_{i}\) and rank-one matrices \(B_{i}\) for all \(i=1,2,,n\), and the time to perform a matrix-vector product with \(C_{i}\) and \(D_{i}\) is \(O(d)\). Therefore, by Lemma 2.1, with \(t_{C}\) and \(t_{D}\) to be \((nd)\), Line 3 runs in \((nd)\) time. In Line 5, by Lemma 2.2, the top eigenvector of \(Y_{w^{}}\) can be computed in \((n d)\) time using power method. Scaling in Line 4 runs in \(O(n)\) time. As a result, Algorithm 1 runs in \((nd)\) time. 

We can directly combine Lemma 4.3 and Lemma 4.4 to finish the proof of Lemma 3.2.

## 5 Robust Gradient Descent

After the robust spectral initialization in Section 4, we have an initial guess \(z_{1}^{d}\) that is close to the ground truth \(x\) or \(-x\). Without loss of generality, we can assume that \(z_{1}\) is closer to \(x\) than to \(-x\). In this section, we prove Lemma 3.3: Given an initial guess \(z_{1}\) with \(\|z_{1}-x\|_{2} 1/8\), we can use a robust gradient descent algorithm (Algorithm 2) to recover \(x\) to any desire precision \(>0\). It is well-known that gradient descent can achieve geometric convergence rates in non-adversarial settings. We show that Algorithm 2 achieves a similar convergence rate even when the input is \(\)-corrupted.

Consider the natural nonconvex formulation: \(_{z^{d}}\,_{i=1}^{n}f_{i}(z)\) where \(f_{i}(z)( a_{i},z^{2}-y_{i})^{2}.\) Let \(g_{i}\) denote the gradient of \(f_{i}\) with respect to \(z\). Let \(D_{z}\) denote the distribution of \(g_{i}(z)^{d}\) when there is no adversarial corruption. Formally, \(g(z)_{z}\) is distributed as

\[g(z)=[( a,z^{2}- a,x ^{2})^{2}]=-4( a,z^{2}- a,x ^{2}) a,z a\ a(0,I).\] (7)To run gradient descent, we want to estimate the _expected true gradient_\(_{z}\,g(z)\) using samples. The challenge is that the input samples \(\{(a_{i},y_{i})\}_{i[n]}\) are \(\)-corrupted, and consequently the gradients \(\{g_{i}(z)\}_{i[n]}\) is an \(\)-corrupted set of vectors drawn from \(_{z}\). To address this, we use robust mean estimation algorithms (e.g., ) to approximate \(_{z}\), the true mean of \(_{z}\).

```
0:\(>0\), an \(\)-corrupted set of \(n\) samples \(\{(a_{i},y_{i})\}_{i[n]}\), initial guess \(z_{1}^{d}\) with \(\|z_{1}-x\| 1/8\), and desired precision \(>0\).
0:\(z^{d}\) such that \(\|z-x\|_{2}\) where \(x\) is the ground truth.
1:procedureRobustGD(\(,\{(a_{i},y_{i})\}_{i[n]},z_{1},\))
2:\(T O((1/))\), \( 1/300\)
3:\(\{N_{1},N_{2},,N_{T}\}\) a random disjoint partition of \([n]\) such that \(|N_{t}|=n/T\) for all \(t[T]\)
4:for\(t=1,2,,T\)do
5:\(_{z_{t}}\) Robust mean estimation on input \(\{g_{i}(z_{t})\}_{i N_{t}}\) using Lemma 5.2
6:\(z_{t+1} z_{t}-\,_{z_{t}}\)
7:endfor
8:return\(z_{T+1}\)
9:endprocedure ```

**Algorithm 2** Robust Gradient Descent

The error guarantee of robust mean estimation algorithms depends on the covariance matrix \(_{z}\) of the distribution \(_{z}\). The next lemma upper bounds the spectral norm of \(_{z}\).

**Lemma 5.1**.: _Let \(_{z}\) be the distribution of gradients at \(z\) as defined in Equation (7). For any \(z^{d}\) with \(\|z-x\|_{2} 1\), the covariance matrix \(_{z}\) of \(_{z}\) satisfies \(_{z} O(\|z-x\|_{2}^{2})I\)._

The proof of Lemma 5.1 is deferred to Appendix B. Given Lemma 5.1, we can show that robust mean estimation algorithms can approximate \(_{z}\) with small error. For technical reasons, we randomly partition the input samples \((a_{i},y_{i})\) into \(T\) subsets, and use one subset in each iteration. With high probability, each partition has at most \((2)\)-fraction of corrupted samples

**Lemma 5.2**.: _Consider any \(z^{d}\) with \(\|z-x\|_{2} 1\). Let \(_{z}\) be the mean of \(_{z}\) as defined in Equation (7). Fix universal constants \(c>0\) and \(_{0}=(c^{2})\). Let \(2<_{0}\) and \(>0\). Given a \((2)\)-corrupted set of \(m=(d d/)\) samples drawn from \(_{z}\), we can compute \(_{z}\) in time \((md(1/))\) such that \(\|_{z}-_{z}\|_{2} c\|z-x\|_{2}\) with probability at least \(1-O()\)._

Proof of Lemma 5.2.: Since \(2<_{0}\), we can view the \((2)\)-corrupted set of \(m\) samples as \(_{0}\)-corrupted. We need to replace \(2\) with \(_{0}\) to reduce the failure probability of Lemma 2.3. This weakens the error guarantee of Lemma 2.3, but the resulting \(_{z}\) is still accurate enough for our algorithm.

We apply Lemma 2.3 to the \(_{0}\)-corrupted set of \(m\) vectors drawn from \(_{z}\). By Lemma 5.1, the covariance matrix of \(_{z}\) satisfies \(_{z} O(\|z-x\|_{2}^{2})I\). Consequently, for sufficiently large \(m=(d d/)\) and sufficiently small \(_{0}=O(c^{2})\), the error guarantee of Lemma 2.3 is \(O(}+}+})\|z-x\|_{2} c\|z-x\|_{2}\). The success probability is at least \(1--(-_{0}m)=1-O()\). 

Lemma 5.2 shows that even with a \((2)\)-corrupted set of gradients, the true gradient \(_{z}\) can be estimated up to an additive error proportional to the distance between \(z\) and \(x\). The next lemma shows that such an approximate gradient is sufficient for gradient descent to converge, reducing the distance to the ground truth \(x\) by a constant factor in each iteration.

**Lemma 5.3**.: _Suppose in iteration \(t\) of Algorithm 2, the current solution \(z_{t}\) satisfies \(\|z_{t}-x\|_{2} 1/8\), and the estimated gradient \(_{z_{t}}^{d}\) satisfies \(\|_{z_{t}}-_{z_{t}}\|_{2} c\|z_{t}-x\|_ {2}\) for \(c=4\). Then, we have \(\|z_{t+1}-x\|_{2}^{2} 0.99\|z_{t}-x\|_{2}^{2}\)._

Proof Sketch of Lemma 5.3.: We provide a proof sketch and defer the full proof to Appendix B. Our objective function is nonconvex (even with infinitely many samples and no corruption). However, when the starting point \(z_{1}\) is close to a global optimum, it is well-known that gradient descent is well-behaved. More specifically, for any \(z\) close to the ground truth \(x\), we can show that the (expected) true gradient \(_{z}\) aligns with the direction toward \(x\):

\[_{z},z-x 7.5\|z-x\|_{2}^{2} \|_{z}\|_{2} 29\|z-x\|_{2}\]

which is sufficient for proving geometric convergence. We can immediately see that this argument is robust to additive error in \(_{z}\) that is proportional to \(\|z-x\|_{2}\). When \(\|_{z}-_{z}\|_{2} c\|z-x\|_{2}\),

\[_{z},z-x(7.5-c)\|z_{t}-x\| _{2}^{2}\|_{z_{t}}\|_{2}(29+c)\|z_{t}-x\|_{2 }\]

When \(c<7.5\), we can choose an appropriate step size \(\) such that the distance between \(z_{t}\) and \(x\) decreases by a constant factor in each iteration. 

We are now ready to prove Lemma 3.3, which states the performance guarantee, sample complexity, runtime, and success probability of Algorithm 2. We restate Lemma 3.3 before proving it.

**Lemma 3.3** (Robust Gradient Descent).: _Consider the setting of Problem 1.3. Let \(>0\) be the desired precision. Let \(0<<_{0}\) for some sufficiently small universal constant \(_{0}\). Given an \(\)-corrupted set of \(n=(d^{2}(1/))\) samples and an initial guess \(z_{1}\) such that \(r(z_{1})=(\|z_{1}-x\|_{2},\|z_{1}+x\|_{2}) 1/8\), we can compute a vector \(z^{d}\) in time \((nd)\) such that \(r(z)\) with probability at least \(0.95\)._

Proof of Lemma 3.3.: First, we prove the success probability of Algorithm 2. Algorithm 2 can fail in two ways: _(i)_ if some \(N_{t}\) has more than \((2)\)-fraction of corrupted samples, or _(ii)_ if Lemma 5.2 fails in some iteration \(t\). The probability of event _(i)_ is at most \(0.01\) for our choice of \(n\), which follows from a standard application of Hoeffding's inequality and a union bound over \(T\) iterations. For event _(ii)_, we choose a sufficiently small \(=O(1/T)\) in Lemma 5.2, so each robust gradient estimation fails with probability at most \(O()=0.01/T\), and overall the probability of event _(ii)_ is at most \(0.01\). For the rest of the proof, we assume these bad events do not happen.

Next, we show the correctness of Algorithm 2. Without loss of generality, we can assume that \(z_{1}\) is closer to the ground truth \(x\) than to \(-x\), which implies \(\|z_{1}-x\|_{2} 1/8\). By Lemma 5.2, we can obtain an approximation \(_{z_{1}}\) of the true gradient \(_{z_{1}}\) at \(z_{1}\) such that \(\|_{z_{1}}-_{z_{1}}\|_{2} c\|z_{1}-x \|_{2}\). Then by Lemma 5.3, we know that \(\|z_{2}-x\|_{2} 0.99\|z_{1}-x\|_{2}\) after one step of gradient descent. We can iteratively apply these two lemmas to show that, after \(T=O((1/))\) iterations, we have \(\|z_{T+1}-x\|_{2}\). One technical issue is that in iteration \(t\), we need to use a fresh subset of samples \(N_{t}\). By the principle of deferred decisions, we can view \((a_{i},y_{i})_{i N_{t}}\) as being generated (and corrupted) after \(z_{t}\) is chosen, which forms a \((2)\)-corrupted set of gradients at \(z_{t}\).

Finally, we analyze the sample complexity and runtime of Algorithm 2. Algorithm 2 requires in total \(n=mT=(d d^{2}(1/))\) samples. A random partition can be computed in \(O(n)\) time by shuffling the input. In each iteration, the \(m\) gradients in \(N_{t}\) can be computed using Equation (7) in time \(O(md)\), and \(z_{t}\) can be updated in time \(O(d)\). By Lemma 5.2, the true gradient can be robustly estimated in time \((md T)\) = \((md(1/))\). The overall runtime of the algorithm is \((n+(md(1/))T)=(nd(1/))= (nd)\). 

## 6 Conclusions and Future Directions

In this paper, our main conceptual contribution is to propose and study the outlier-robust phase retrieval problem, where a constant fraction of the input data is corrupted. Notably, we allow corruption in both the sampled frequencies \(a_{i}^{d}\) and the intensity measurements \(y_{i}\). Our main technical contribution is the design and analysis of a near-sample-optimal and nearly-linear-time algorithm that solves this problem with provably guarantees.

An immediate technical question is whether our sample complexity can be tightened by removing some \((1/)\) factors. One potential approach is to open robust mean estimation algorithms instead of using them in a black-box manner. One could examine the stability conditions that these algorithms require, and see if these stability conditions can be proved without partitioning the samples and using fresh samples in each iteration. More broadly, we believe our framework can be used to develop outlier-robust algorithms for other tractable nonconvex problems, by first finding an initial solution in a saddle-free region near a global optimum and then converging to this global optimum using robust gradient descent.