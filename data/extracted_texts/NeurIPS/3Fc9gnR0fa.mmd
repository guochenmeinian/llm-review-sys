# Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions

Ruofan Wu

Equal contribution

 Jiawei Qiao

Equal contribution

 Mingzhe Wu

Equal contribution

Wen Yu

Fudan University

Ming Zheng

Coupang

###### Abstract

We present neural frailty machine (NFM), a powerful and flexible neural modeling framework for survival regressions. The NFM framework utilizes the classical idea of multiplicative frailty in survival analysis as a principled way of extending the proportional hazard assumption, at the same time being able to leverage the strong approximation power of neural architectures for handling nonlinear covariate dependence. Two concrete models are derived under the framework that extends neural proportional hazard models and nonparametric hazard regression models. Both models allow efficient training under the likelihood objective. Theoretically, for both proposed models, we establish statistical guarantees of neural function approximation with respect to nonparametric components via characterizing their rate of convergence. Empirically, we provide synthetic experiments that verify our theoretical statements. We also conduct experimental evaluations over \(6\) benchmark datasets of different scales, showing that the proposed NFM models achieve predictive performance comparable to or sometimes surpassing state-of-the-art survival models. Our code is publicly availabel at https://github.com/Rorschach1989/nfm

## 1 Introduction

Regression analysis of time-to-event data  has been among the most important modeling tools for clinical studies and has witnessed a growing interest in areas like corporate finance , recommendation systems , and computational advertising . The key feature that differentiates time-to-event data from other types of data is that they are often _incompletely observed_, with the most prevailing form of incompleteness being the _right censoring_ mechanism . In the right censoring mechanism, the duration time of a sampled subject is (sometimes) only known to be larger than the observation time instead of being recorded precisely. It is well known in the community of survival analysis that even in the case of linear regression, naively discarding the censored observations produces estimation results that are statistically biased , at the same time losses sample efficiency if the censoring proportion is high.

Cox's proportional hazard (CoxPH) model  using the convex objective of negative partial likelihood  is the _de facto_ choice in modeling right censored time-to-event data (hereafter abbreviated as censored data without misunderstandings). The model is _semiparametric_ in the sense that the baseline hazard function needs no parametric assumptions. The original formulation of CoxPH modelassumes a linear form and therefore has limited flexibility since the truth is not necessarily linear. Subsequent studies extended CoxPH model to nonlinear variants using ideas from nonparametric regression [36; 8; 9], ensemble learning , and neural networks [25; 42]. While such extensions allowed a more flexible nonlinear dependence structure with the covariates, the learning objectives were still derived under the proportional hazards (PH) assumption, which was shown to be inadequate in many real-world scenarios . The most notable case was the failure of modeling the phenomenon of crossing hazards . It is thus of significant interest to explore extensions of CoxPH that both allow nonlinear dependence over covariates and relaxations of the PH assumption.

Frailty models [69; 21] are among the most important research topics in modern survival analysis, in that they provide a principled way of extending CoxPH model via incorporating a multiplicative random effect to capture unobserved heterogeneity. The resulting parameterization contains many useful variants of CoxPH like the proportional odds model , under specific choices of frailty families. While the theory of frailty models has been well-established [48; 49; 53; 45], most of them focused on the linear case. Recent developments on applying neural approaches to survival analysis [42; 46; 64; 56] have shown promising results in terms of empirical predictive performance, with most of them lacking theoretical discussions. Therefore, it is of significant interest to build more powerful frailty models via adopting techniques in modern deep learning  with provable statistical guarantees.

In this paper, we present a general framework for neural extensions of frailty models called the **neural frailty machine (NFM)**. Two concrete neural architectures are derived under the framework: The first one adopts the proportional frailty assumption, allowing an intuitive interpretation of the neural CoxPH model with a multiplicative random effect. The second one further relaxes the proportional frailty assumption and could be viewed as an extension of nonparametric hazard regression (NHR) [13; 44], sometimes referred to as "fully neural" models under the context of neural survival analysis . We summarize our contributions as follows.

* We propose the neural frailty machine (NFM) framework as a principled way of incorporating unobserved heterogeneity into neural survival regression models. The framework includes many commonly used survival regression models as special cases.
* We derive two model architectures based on the NFM framework that extend neural CoxPH models and neural NHR models. Both models allow stochastic training and scale to large datasets.
* Theoretically, we show _statistical correctness_ of the two proposed models via characterizing the rates of convergence of the proposed nonparametric function estimators. The proof technique is different from previous theoretical studies on neural survival analysis and is applicable to many other types of neural survival models.
* Empirically, we verify the _empirical efficacy_ of the proposed framework via conducting extensive studies on various benchmark datasets at different scales. Under standard performance metrics, both models are empirically shown to perform competitively, matching or sometimes outperforming state-of-the-art neural survival models.

## 2 Related works

### Nonlinear extensions of CoxPH

Most nonlinear extensions of CoxPH model stem from the equivalence of partial likelihood and semiparametric profile likelihood  of CoxPH model, resulting in nonlinear variants that essentially replaces the linear term in partial likelihood with nonlinear variants:  used smoothing splines, [8; 9] used local polynomial regression . The empirical success of tree-based models inspired subsequent developments like  that equip tree-based models such as gradient boosting trees and random forests with losses in the form of negative log partial likelihood. Early developments of neural survival analysis  adopted similar extension strategies and obtained neural versions of partial likelihood. Later attempts  suggested using the successful practice of stochastic training which is believed to be at the heart of the empirical success of modern neural methods . However, stochastic training under the partial likelihood objective is highly non-trivial, as mini-batch versions of log partial likelihood  are no longer valid stochastic gradients of the full-sample log partial likelihood .

### Beyond CoxPH in survival analysis

In linear survival modeling, there are standard alternatives to CoxPH such as the accelerated failure time (AFT) model [7; 73], the extended hazard regression model , and the family of linear transformation models . While these models allow certain types of nonlinear extensions, the resulting form of (conditional) hazard function is still restricted to be of a specific form. The idea of nonparametric hazard regression (NHR) [13; 44; 63] further improves the flexibility of nonparametric survival analysis via directly modeling the conditional hazard function by nonparametric regression techniques such as spline approximation. Neural versions of NHR have been developed lately such as the CoxTime model .  used a neural network to approximate the conditional survival function and could be thus viewed as another trivial extension of NHR.

Aside from developments in NHR,  proposed a discrete-time model with its objective being a mix of the discrete likelihood and a rank-based score;  proposed a neural version of the extended hazard model, unifying both neural CoxPH and neural AFT model;  used an ODE approach to model the hazard and cumulative hazard functions.

### Theoretical justification of neural survival models

Despite the abundance of neural survival models, assessment of their theoretical properties remains nascent. In , the authors developed minimax theories of partially linear cox model using neural networks as the functional approximator.  provided convergence guarantees of neural estimates under the extended hazard model. The theoretical developments therein rely on specific forms of objective function (partial likelihood and kernel pseudo-likelihood) and are not directly applicable to the standard likelihood-based objective which is frequently used in survival analysis.

## 3 Methodology

### The neural frailty machine framework

Let \( 0\) be the interested event time with survival function denoted by \(S(t)=(>t)\) associated with a feature(covariate) vector \(Z^{d}\). Suppose that \(\) is a continuous random variable and let \(f(t)\) be its density function. Then \((t)=f(t)/S(t)\) is the hazard function and \((t)=_{0}^{t}(s)ds\) is the cumulative hazard function. Aside from the covariate \(Z\), we use a positive scalar random variable \(^{+}\) to express the unobserved heterogeneity corresponding to individuals, or _frailty_. 2. In this paper we will assume the following generating scheme of \(\) via specifying its conditional hazard function:

\[(t|Z,)=(t,Z).\] (1)

Here \(\) is an unspecified non-negative function, and we let the distribution of \(\) be parameterized by a one-dimensional parameter \(\). 3 The formulation (1) is quite general and contains several important models in both traditional and neural survival analysis:

1. When \(\) follows parametric distributional assumptions, and \((t,Z)=(t)e^{^{}Z}\), (1) reduces to the standard proportional frailty model . A special case is when \(\) is degenerate, i.e., it has no randomness, then the model corresponds to the classic CoxPH model.
2. When \(\) is degenerate and \(\) is arbitrary, the model becomes equivalent to nonparametric hazard regression (NHR) [13; 44]. In NHR, the function parameter of interest is usually the logarithm of the (conditional) hazard function.

In this paper we construct neural approximations to the logarithm of \(\), i.e., \((t,Z)=(t,Z)\). The resulting models are called **Neural Frailty Machines (NFM)**. Depending on the prior knowledge of the function \(\), we propose two function approximation schemes:

**The proportional frailty (PF) scheme** assumes the dependence of \(\) on event time and covariates to be completely _decoupled_, i.e.,

\[(t,Z)=h(t)+m(Z).\] (2)

Proportional-style assumption over hazard functions has been shown to be a useful inductive bias in survival analysis. We will treat both \(h\) and \(m\) in (2) as function parameters, and device two multi-layer perceptrons (MLP) to approximate them separately.

**The fully neural (FN) scheme** imposes no a priori assumptions over \(\) and is the most general version of NFM. It is straightforward to see that the most commonly used survival models, such as CoxPH, AFT, EH, or PF models are included in the proposed model space as special cases. We treat \(=(t,Z)\) as the function parameter with input dimension \(d+1\) and use a multi-layer perceptron (MLP) as the function approximator to \(\). Similar approximation schemes with respect to the hazard function have been proposed in some recent works [52; 56], referred to as "fully neural approaches" without theoretical characterizations.

**The choice of frailty family** There are many commonly used families of frailty distributions [45; 21; 69], among which the most popular one is the _gamma frailty_, where \(\) follows a gamma distribution with mean \(1\) and variance \(\). We briefly introduce some other types of frailty families in appendix A.

### Parameter learning under censored observations

In time-to-event modeling scenarios, the event times are typically observed under right censoring. Let \(C\) be the right censoring time which is assumed to be conditionally independent of the event time \(\) given \(Z\), i.e., \( C|Z\). In data collection, one can observe the minimum of the survival time and the censoring time, that is, observe \(T= C\) as well as the censoring indicator \(=I( C)\), where \(a b=(a,b)\) for constants \(a\) and \(b\) and \(I()\) stands for the indicator function. We assume \(n\) independent and identically distributed (i.i.d.) copies of \((T,,Z)\) are used as the training sample \((T_{i},_{i},Z_{i}),i[n]\), where we use \([n]\) to denote the set \(\{1,2,,n\}\). Additionally, we assume the unobserved frailties are independent and identically distributed, i.e., \(_{i}}{}f_{}(),i[n]\). Next, we derive the learning procedure based on the **observed log-likelihood (OLL)** objective under both PF and FN scheme. To obtain the observed likelihood, we first integrate the conditional survival function given the frailty:

\[S(t|Z)=_{ f_{}}[e^{-_{0}^{t}e^{( s,Z)}ds}]=:e^{-G_{}(_{0}^{t}e^{(s,Z)}ds)}.\] (3)

Here the _frailty transform_\(G_{}(x)=-(_{ f_{}}[e^{- x }])\) is defined as the negative of the logarithm of the Laplace transform of the frailty distribution. The conditional cumulative hazard function is thus \((t|Z)=G_{}(_{0}^{t}e^{(s,Z)}ds)\). For the PF scheme of NFM, we use two MLPs \(=(t;^{h},^{h})\) and \(=(Z;^{m},^{m})\) as function approximators to \(\) and \(m\), parameterized by \((^{h},^{h})\) and \((^{m},^{m})\), respectively.4 According to standard results on censored data likelihood , we write the learning objective under the PF scheme as:

\[&(^{h},^{h},^{ m},^{m},)\\ =&[_{i[n]}_{i} g_{ }(e^{(Z_{i})}_{0}^{T_{i}}e^{(s)}ds )+_{i}(T_{i})+_{i}(Z_{i})-G_{} (e^{(Z_{i})}_{0}^{T_{i}}e^{(s)}ds) ].\] (4)

Here we define \(g_{}(x)=}G_{}(x)\). Let \((}_{n}^{h},}_{n}^{h}, }_{n}^{m},}_{n}^{m},_{n})\) be the maximizer of (4) and further denote \(_{n}(t)=(t;}_{n}^{h},}_{n}^{h})\) and \(_{n}(Z)=(Z;}_{n}^{m},}_{n}^{m})\). The resulting estimators for conditional cumulative hazard and survival functions are:

\[_{}(t|Z)=G_{_{n}}(_{0}^{t} e^{_{n}(s)+_{n}(Z)}ds),_{}(t|Z)=e^{- _{}(t|Z)},\] (5)For the FN scheme, we use \(=(t,Z;^{},^{})\) to approximate \((t,Z)\) parameterized by \((^{},^{})\). The OLL objective is written as:

\[&(^{},^{}, )\\ =&[_{i[n]}_{i} g_{ }(_{0}^{T_{i}}e^{(s,Z_{i};^{}, ^{})}ds)+_{i}(T_{i},Z_{i};^{ },^{})-G_{}(_{0}^{T_{i}}e^{(s,Z_{ i};^{},^{})}ds)].\] (6)

Let \((}_{n}^{},}_{n}^{},_{n})\) be the maximizer of (6), and further denote \(_{n}(t,Z)=(t,Z;}_{n}^{}, }_{n}^{})\). The conditional cumulative hazard and survival functions are therefore estimated as:

\[_{}(t|Z)=G_{_{n}}(_{0}^{ t}e^{_{n}(s,Z)}ds),_{}(t|Z)=e^{- _{}(t|Z)}.\] (7)

The evaluation of objectives like (6) and its gradient requires computing a definite integral of an exponentially transformed MLP function. Instead of using exact computations that are available for only a restricted type of activation functions and network structures, we use numerical integration for such kinds of evaluations, using the method of Clenshaw-Curts quadrature , which has shown competitive performance and efficiency in recent applications to monotonic neural networks .

_Remark 3.1_.: The interpretation of frailty terms differs in the two schemes. In the PF scheme, introducing the frailty effect strictly increases the modeling capability (i.e., the capability of modeling crossing hazard) in comparison to CoxPH or neural variants of CoxPH . In the FN scheme, it is arguable that in the i.i.d. case, the marginal hazard function is a reparameterization of the hazard function in the context of NHR. Therefore, we view the incorporation of frailty effect as injecting a domain-specific inductive bias that has proven to be useful in survival analysis and time-to-event regression modeling and verify this claim empirically in section 5.2. Moreover, frailty becomes especially helpful when handling correlated or clustered data where the frailty term is assumed to be shared among certain groups of individuals . Extending NFM to such scenarios is valuable and we left it to future explorations.

## 4 Statistical guarantees

In this section, we present statistical guarantees regarding both NFM estimates in the sense of nonparametric regression , where we obtain rates of convergence to the ground truth function parameters (which is frequently referred to as the _true parameter_ in statistics literature). The results in this section is interpreted as showing the _statistical correctness_ of our approach.

**Proof strategy** Our proof technique is based on the method of sieves [60; 59; 11] that views neural networks as a special kind of nonlinear sieve  that satisfies desirable approximation properties . Our strategy is different from previous theoretical works on neural survival models [75; 76] where the developments implicitly requires the loss function to be well-controlled by the \(L_{2}\) loss and is therefore not directly applicable to our model due to the flexibility in choosing the frailty transform. Since both models produce estimates of function parameters, we need to specify a suitable function space to work with. Here we choose the following Holder ball as was also used in previous works on nonparametric estimation using neural networks [57; 26; 76]

\[_{M}^{}()=\{f:_{:|| }*{esssup}_{x}|D^{}(f(x))| M\},\] (8)

where the domain \(\) is assumed to be a subset of \(d\)-dimensional euclidean space. \(=(_{1},,_{d})\) is a \(d\)-dimensional tuple of nonnegative integers satisfying \(||=_{1}++_{d}\) and \(D^{}f=f}{ x_{1}^{_{1}} x_{ d}^{_{d}}}\) is the weak derivative of \(f\). Now assume that \(M\) is a reasonably large constant, and let \(\) be a closed interval over the real line. We make the following assumptions for the _true parameters_ under both schemes:

**Condition 4.1** (True parameter, PF scheme).: _The euclidean parameter \(_{0}\), and the two function parameters \(m_{0}_{M}^{}([-1,1]^{d}),h_{0}_{M}^{}([0,])\), and \(>0\) is the ending time of the study duration, which is usually adopted in the theoretical studies in survival analysis ._

**Condition 4.2** (True parameter, FN scheme).: _The euclidean parameter \(_{0}\), and the function parameter \(_{0}_{M}^{}([0,][-1,1]^{d})\),_Next, we construct sieve spaces for function parameter approximation via restricting the complexity of the MLPs to "scale" with the sample size \(n\).

**Condition 4.3** (Sieve space, PF scheme).: _The sieve space \(_{n}\) is constructed as a set of MLPs satisfying \(_{M_{n}}^{}([0,])\), with depth of order \(O( n)\) and total number of parameters of order \(O(n^{} n)\). The sieve space \(_{n}\) is constructed as a set of MLPs satisfying \(_{M_{m}}^{}([-1,1]^{d})\), with depth of order \(O( n)\) and total number of parameters of order \(O(n^{} n)\). Here \(M_{h}\) and \(M_{m}\) are sufficiently large constants such that every function in \(_{M}^{}([-1,1]^{d})\) and \(_{M}^{}([0,])\) could be accurately approximated by functions inside \(_{n}\) and \(_{n}\), according to [72, Theorem 1]._

**Condition 4.4** (Sieve space, FN scheme).: _The sieve space \(_{n}\) is constructed as a set of MLPs satisfying \(_{M_{}}^{}([0,])\), with depth of order \(O( n)\) and total number of parameters of order \(O(n^{} n)\). Here \(M_{}\) is a sufficiently large constant such that \(_{n}\) satisfies approximation properties, analogous to condition 4.3._

For technical reasons, we will assume the nonparametric function estimators are constrained to fall inside the corresponding sieve spaces, i.e., \(_{n}_{n}\), \(_{n}_{n}\) and \(_{n}\). This will not affect the implementation of optimization routines as was discussed in . Furthermore, we restrict the estimate \(_{n}\) in both PF and FN schemes.

Additionally, we need the following regularity condition on the function \(G_{}(x)\):

**Condition 4.5**.: \(G_{}(x)\) _is viewed as a bivariate function \(G:\), where \(\) is a compact set on \(\). The functions \(G_{}(x),G_{}(x),G_{}(x)\),\( g_{}(x), g_{}(x)\), \( g_{}(x)\) are bounded on \(\)._

We define two metrics that measures convergence of parameter estimates: For the PF scheme, let \(_{0}=(h_{0},m_{0},_{0})\) be the true parameters and \(_{n}=(_{n},_{n},_{n})\) be the estimates. We abbreviate \(_{_{0},Z=z}\) as the conditional probability distribution of \((T,)\) given \(Z=z\) under the true parameter, and \(_{_{n},Z=z}^{}\) as the conditional probability distribution of \((T,)\) given \(Z=z\) under the estimates. Define the following metric

\[d_{}(_{n},_{0})=_{z _{Z}}[H^{2}(_{_{n},Z=z} _{_{0},Z=z})]},\] (9)

where \(H^{2}()=(}-})^{2}\) is the squared Hellinger distance between probability distributions \(\) and \(\). The case for the FN scheme is similar: Let \(_{0}=(_{0},_{0})\) be the parameters and \(_{n}=(_{n},_{n})\) be the estimates. Analogous to the definitions above, we define \(_{_{0},Z=z}\) as the true conditional distribution given \(Z=z\), and \(_{_{n},Z=z}\) be the estimated conditional distribution, we will use the following metric in the FN scheme:

\[d_{}(_{n},_{0})=_{z _{Z}}[H^{2}(_{_{n},Z=z} _{_{0},Z=z})]}.\] (10)

Now we state our main theorems. We denote \(\) as the data generating distribution and use \(\) to hide poly-logarithmic factors in the big-O notation.

**Theorem 4.6** (Rate of convergence, PF scheme).: _In the PF scheme, under condition 4.1, 4.3, 4.5, we have that \(d_{}(_{n},_{0})=_{}(n^{-})\)._

**Theorem 4.7** (Rate of convergence, FN scheme).: _In the FN scheme, under condition 4.2, 4.4, 4.5, we have that \(d_{}(_{n},_{0})=_{}(n^{-})\)._

_Remark 4.8_.: The idea of using Hellinger distance to measure the convergence rate of sieve MLEs was proposed in . Obtaining rates under a stronger topology such as \(L_{2}\) is possible if the likelihood function satisfies certain conditions such as the curvature condition . However, such kind of conditions is in general too stringent for likelihood-based objectives, instead, we use Hellinger distance that has minimal requirements. Consequently, our proof strategy is applicable to many other survival models that rely on neural function approximation such as , with some modification to the regularity conditions. For proper choices of metrics in sieve theory, see also the discussion in [11, Chapter 2].

## 5 Experiments

In this section, we report the empirical performance of NFM, we will focus on the following two research questions:

**RQ1(Verfication of statistical correctness):** The results listed in section characterized the convergence results _in theory_, providing a crude guide on the number of samples required for an accurate estimate. Nonetheless, theoretical rates are often pessimistic, thus we want to investigate **whether a moderate number of sample size suffices for good approximation**.

**RQ2(Assessment of empirical efficacy):** While NFM is theoretically sound in terms of _estimation accuracy_, the theory we have developed does not necessarily guarantee its _empirical efficacy_ as a method of doing prognosis. It is therefore valuable to inspect **how useful NFM is regarding real-world predictive tasks in survival analysis**.

### Synthetic experiments

To answer RQ1, we conduct synthetic experiments to check the empirical convergence. Specifically, we investigate the empirical recovery of underlying ground truth parameters under various level of sample size.

**Ground truth** We set the true underlying model to be a nonlinear gamma-frailty model with a \(5\)-dimensional feature. We generate three training datasets of different scales, with \(n\{1000,5000,10000\}\). The assessment will be made on a fixed test sample of \(100\) hold-out points that are independently drawn from the generating scheme of the event time. A censoring mechanism is applied such that the censoring ratio is around \(40\%\) for each dataset. The precise form of the frailty model as well as the generating distribution of the feature vectors are detailed in appendix C.2.

**Empirical recovery results** We report the empirical recovery of the nonlinear component \((t,Z)\) regarding the hold-out test set in in figure 1. We observe from graphical illustrations that under a moderate sample size \(n=1000\), NFM already exhibits satisfactory recovery for a (relatively low-dimensional) feature space, which is the prevailing case in most public benchmark datasets. We also present additional assessments about: (i) The recovery of \(m(Z)\) when using PF scheme in appendix D.1, (ii) The recovery of survival functions under both PF and FN scheme in appendix D.2, and (iii) The numerical recovery results of survival function in appendix D.3.

Figure 1: Visualizations of synthetic data results under the NFM framework. The plots in the first row compare the empirical estimates of the nonparametric component \((t,Z)\) against its true value evaluated on \(100\) hold-out points, under the PF scheme. The plots in the second row are obtained using the FN scheme, with analogous semantics to the first row.

### Real-world data experiments

To answer RQ\(2\), we conduct extensive empirical assessments over \(6\) benchmark datasets, comprising five survival datasets and one non-survival dataset. The survival datasets include the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) , the Rotterdam tumor bank and German Breast Cancer Study Group (RotGBSG), the Assay Of Serum Free Light Chain (FLCHAIN) , the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) , and the Medical Information Mart for Intensive Care (MIMIC-III) . For all the survival datasets, the event of interest is defined as the mortality after admission. In our experiments, we view METABRIC, RotGBSG, FLCHAIN, and SUPPORT as small-scale datasets and MIMIC-III as a moderate-scale dataset. We additionally use the KGBOX dataset  as a large-scale evaluation. In this dataset, an event time is observed if a customer churns from the KGBOX platform. We summarize the basic statistics of all the datasets in table 3.

**Baselines** We compare NFM with \(12\) baselines. The first one is linear CoxPH model . Gradient Boosting Machine (GBM) [27; 10] and Random Survival Forests (RSF)  are two tree-based nonparametric survival regression methods. DeepSurv  and CoxTime  are two models that adopt neural variants of partial likelihood as objectives. SuMo-net  is a neural variant of NHR. We additionally chose six latest state-of-the-art neural survival models: DeepHit , SurvNode , DeepEH , DCM , DeSurv  and SODEN . Among the chosen baselines, DeepSurv and SuMo-net are viewed as implementations of neural CoxPH and neural NHR and are therefore of particular interest for the empirical verification of the efficacy of frailty.

**Evaluation strategy** We use two standard metrics in survival predictions for evaluating model performance: integrated Brier score (IBS) and integrated negative binomial log-likelihood (INBLL). Both metrics are derived from the following:

\[(,t_{1},t_{2})=_{t_{2}}^{t_{1}}_{i=1}^{n} [(t|Z_{i}))I(T_{i} t,_{i}=1)}{ {S}_{C}(T_{i})}+(t|Z_{i}))I(T_{i}>t)}{_{C} (t)}]dt.\] (11)

Where \(_{C}(t)\) is an estimate of the survival function \(S_{C}(t)\) of the censoring variable, obtained by the Kaplan-Meier estimate  of the censored observations on the test data. \(:\{0,1\}^{+}\) is some proper loss function for binary classification . The IBS metric corresponds to \(\) being the square loss, and the INBLL metric corresponds to \(\) being the negative binomial (Bernoulli) log-likelihood . Both IBS and INBLL are proper scoring rules if the censoring times and survival times are independent. 5 We additionally report the result of another widely used metric, the concordance index (C-index), in appendix D. Since all the survival datasets do not have standard

   Model &  &  &  &  \\   & IBS & INBLL & IBS & INBLL & IBS & INBLL & IBS & INBLL \\  CoxPH & \(16.46_{+0.90}\) & \(49.57_{+2.66}\) & \(18.25_{+0.44}\) & \(53.76_{+1.11}\) & \(10.05_{+0.38}\) & \(33.18_{+1.16}\) & \(20.54_{+0.38}\) & \(59.58_{+0.86}\) \\ GBM & \(16.61_{+0.82}\) & \(49.87_{+2.44}\) & \(17.83_{+0.44}\) & \(52.78_{+1.11}\) & \(9.98_{+0.37}\) & \(32.88_{+1.05}\) & \(19.18_{+0.39}\) & \(56.46_{+0.10}\) \\ RSF & \(16.62_{+0.64}\) & \(49.61_{+1.54}\) & \(17.89_{+0.42}\) & \(52.77_{+1.01}\) & \(_{+0.37}\) & \(32.92_{+0.15}\) & \(19.11_{+0.40}\) & \(56.28_{+1.00}\) \\ DeepSurv & \(16.55_{+0.93}\) & \(49.85_{+3.02}\) & \(17.80_{+0.49}\) & \(52.62_{+1.25}\) & \(10.09_{+0.38}\) & \(33.28_{+1.15}\) & \(19.20_{+0.41}\) & \(56.48_{+1.08}\) \\ CoxTime & \(16.54_{+0.83}\) & \(49.67_{+2.67}\) & \(17.80_{+0.58}\) & \(52.56_{+1.47}\) & \(10.28_{+0.45}\) & \(34.18_{+1.53}\) & \(19.17_{+0.40}\) & \(56.54_{+1.10}\) \\ DeepH & \(17.50_{+0.83}\) & \(52.10_{+2.16}\) & \(19.61_{+0.38}\) & \(56.67_{+1.10}\) & \(11.83_{+0.39}\) & \(37.72_{+1.02}\) & \(20.66_{+0.32}\) & \(60.04_{+0.72}\) \\ DeepEH & \(16.56_{+0.65}\) & \(49.42_{+1.53}\) & \(17.62_{+0.52}\) & \(52.08_{+1.27}\) & \(10.11_{+0.37}\) & \(33.30_{+1.10}\) & \(19.30_{+0.39}\) & \(56.67_{+0.94}\) \\ SuMo-net & \(16.49_{+0.83}\) & \(49.74_{+2.12}\) & \(17.77_{+0.47}\) & \(52.62_{+1.11}\) & \(10.70_{+0.40}\) & \(33.20_{+1.10}\) & \(19.40_{+0.38}\) & \(56.77_{+0.96}\) \\ SODEN & \(16.52_{+0.63}\) & \(49.39_{+1.97}\) & \(_{+0.63}\) & \(_{+1.97}\) & \(10.13_{+0.24}\) & \(33.37_{+0.57}\) & \(19.07_{+0.50train/test splits, we follow previous practice  that uses \(5\)-fold cross-validation (CV): \(1\) fold is for testing, and \(20\%\) of the rest is held out for validation. In our experiments, we observed that a single random split into \(5\) folds does not produce stable results for most survival datasets. Therefore we perform \(10\) different CV runs for each survival dataset and report average metrics as well as their standard deviations. For the KKBOX dataset, we use the standard train/valid/test splits that are available via the pycox package  and report results based on \(10\) trial runs.

**Experimental setup** We follow standard preprocessing strategies [42; 46; 75] that standardize continuous features into zero mean and unit variance, and do one-hot encodings for all categorical features. We adopt MLP with ReLU activation for all function approximators, including \(\), \(\) in PF scheme, and \(\) in FN scheme, across all datasets, with the number of layers (depth) and the number of hidden units (width) within each layer being tunable. We tune the frailty transform over several standard choices: gamma frailty, Box-Cox transformation frailty and \(()\) frailty, with their precise forms detailed in appendix C.3. A more detailed description of the tuning procedure, as well as training configurations for baseline models, are reported in appendix C.3.

**Results** We report experimental results of small-scale datasets in table 1, and results of two larger datasets in table 2. The proposed NFM framework achieves competitive performance which is comparable to the other state-of-the-art models. In particular, NFM attains best performance in mean on \(5\) of the \(6\) datasets, and is statistically significantly better over all the baselines at \(0.05\) empirical level on the MIMIC-III dataset.

**Ablation on the benefits of frailty** To better understand the additional benefits of introducing the frailty formulation, we compute the (relative) performance gain of NFM-PF and NFM-FN, against their non-frailty counterparts, namely DeepSurv  and SuMo-net . The evaluation is conducted for all three metrics mentioned in this paper. The results are shown in table 6. The results suggest a solid improvement in incorporating frailty, as the relative increase in performance could be over \(10\%\) for both NFM models. A more detailed discussion is presented in section D.5.

## 6 Discussion and conclusion

We have introduced NFM as a flexible and powerful neural modeling framework for survival analysis, which is shown to be both statistically correct in theory, and empirically effective in predictive tasks. While our proposed framework provides a theoretically-principled tool of neural survival modeling, a few limitations and challenges need to be addressed in future works including predictive guarantees and better evaluation protocols, which we elaborate in appendix D.6.

   Model &  &  \\   & IBS & INBLL & IBS & INBLL \\  CoxPH & \(20.40_{ 0.00}\) & \(60.02_{ 0.00}\) & \(12.60_{ 0.00}\) & \(39.40_{ 0.00}\) \\ GBM & \(17.70_{ 0.00}\) & \(52.30_{ 0.00}\) & \(11.81_{ 0.00}\) & \(38.15_{ 0.00}\) \\ RSF & \(17.79_{ 0.19}\) & \(53.34_{ 0.41}\) & \(14.46_{ 0.00}\) & \(44.39_{ 0.00}\) \\ DeepSurv & \(18.58_{ 0.92}\) & \(55.98_{ 2.43}\) & \(11.31_{ 0.05}\) & \(35.28_{ 0.15}\) \\ CoxTime & \(17.68_{ 1.36}\) & \(52.08_{ 3.06}\) & \(10.70_{ 0.06}\) & \(33.10_{ 0.21}\) \\ DeepHit & \(19.80_{ 1.31}\) & \(59.03_{ 4.20}\) & \(16.00_{ 0.34}\) & \(48.64_{ 1.04}\) \\ SuMo-net & \(18.62_{ 1.23}\) & \(54.51_{ 2.97}\) & \(11.58_{ 0.11}\) & \(36.61_{ 0.28}\) \\ DCM & \(18.02_{ 0.49}\) & \(52.83_{ 0.94}\) & \(10.71_{ 0.11}\) & \(33.24_{ 0.06}\) \\ DeSurv & \(18.19_{ 0.65}\) & \(54.69_{ 2.83}\) & \(10.77_{ 0.21}\) & \(33.22_{ 0.10}\) \\ 
**NFM-PF** & \(_{ 0.36}\) & \(_{ 0.92}\) & \(11.02_{ 0.11}\) & \(35.10_{ 0.22}\) \\
**NFM-FN** & \(17.47_{ 0.45}\) & \(51.48_{ 1.23}\) & \(_{ 0.08}\) & \(_{ 0.14}\) \\   

Table 2: Survival prediction results measured in IBS and INBLL metric (%) on two larger datasets. In each column, the **boldfaced** score denotes the best result and the underlined score represents the second-best result (both in mean). Two models are not reported, namely SODEN and DeepEH, as we found empirically that their computational/memory cost is significantly worse than the rest, and we fail to obtain reasonable performances over the two datasets for these two models.

Acknowledgements

We would like to thank professor Zhiliang Ying and professor Guanhua Fang for helpful discussions. Wen Yu's research is supported by the National Natural Science Foundation of China Grants (\(12071088\)). Ming Zheng's research is supported by the National Natural Science Foundation of China Grants (\(12271106\)).