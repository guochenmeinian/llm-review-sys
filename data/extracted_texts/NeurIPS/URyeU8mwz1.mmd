# The Value of Reward Lookahead in Reinforcement Learning

Nadav Merlis

FairPlay Joint Team, CREST, ENSAE Paris

nadav.merlis@ensae.fr &Dorian Baudry

FairPlay Joint Team, CREST, ENSAE Paris

Institut Polytechnique de Paris

&Vianney Perchet

FairPlay Joint Team, CREST, ENSAE Paris

Criteo AI Lab

###### Abstract

In reinforcement learning (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards. Usually, rewards are observed only _after_ acting, and so the goal is to maximize the _expected_ cumulative reward. Yet, in many practical settings, reward information is observed in advance - prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction. In this work, we aim to quantifiably analyze the value of such future reward information through the lens of _competitive analysis_. In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead. We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations. Surprisingly, the resulting ratios relate to known quantities in offline RL and reward-free exploration. We further provide tight bounds for the ratio given the worst-case dynamics. Our results cover the full spectrum between observing the immediate rewards before acting to observing all the rewards before the interaction starts.

## 1 Introduction

Reinforcement Learning [RL, Sutton and Barto, 2018] is the problem of learning how to interact with a changing environment. The setting usually consists of two major elements: a transition kernel, which governs how the state of the environment evolves due to the actions of an agent, and a reward given to the agent for performing an action at a given environment state. Agents must decide which actions to perform in order to collect as much reward as possible, taking into account not only the immediate reward gain, but also the long-term effects of actions on the state dynamics.

In the standard RL framework, reward information is usually observed after playing an action, and agents only aim to maximize their cumulative _expected_ reward, also known as the _value_[Jaksch et al., 2010, Azar et al., 2017, Jin et al., 2018, Dann et al., 2019, Zanette and Brunskill, 2019, Efroni et al., 2019, Simchowitz and Jamieson, 2019, Zhang et al., 2021b]. Yet, in many real-world scenarios, partial information about the future reward is accessible in advance. For example, when performing transactions, prices are usually known. In navigation settings, rewards are sometimes associated with traffic, which can be accurately estimated for the near future. In goal-oriented problems [Schaul et al., 2015, Andrychowicz et al., 2017], the location of the goal is oftentimes revealed in advance. This information is completely ignored by agents that maximize the expected reward, even though using this future information on the reward should greatly increase the reward collected by the agent.

As an illustration, consider a driving problem where an agent travels between two locations, aiming to collect as much reward as possible. In one such scenario, rewards are given only when traveling free roads. It would then be reasonable to assume that agents see whether there is traffic before deciding in which way to turn at every intersection ('one-step lookahead'). In an alternative scenario, the agent participates in ride-sharing and gains a reward when picking up a passenger. In this case, agents gain information on nearby passengers along the path, not necessarily just in the closest intersection ('multi-step lookahead'). Finally, the destination might be revealed only at the beginning of the interaction, and reward is only gained when reaching it ('full lookahead'). In all examples, the additional information should be utilized by the agent to increase its collected reward.

In this paper, we analyze the value of future (lookahead) information on the reward that could be obtained by the agent through the lens of competitive analysis. More precisely, we study the _competitive ratio_ (CR) between the value of an agent that only has access to reward distributions and that of a lookahead agent who sees the actual reward realizations for several future timesteps before choosing each action. Our contributions are the following: **(i)** Given an environment and its expected rewards, we characterize the distribution that maximizes the value of lookahead agents, for all ranges of lookahead from one step to full lookahead; this distribution therefore minimizes the CR. In particular, we show that the lookahead value is maximized by _long-shot_ rewards - very high rewards at extremely low probabilities. **(ii)** We derive the worst-case CR as a function of the dynamics of the environment (that is, for the worst-case reward expectations). Surprisingly, the CR that emerges is closely related to fundamental quantities in reward-free exploration and offline RL (Xie et al., 2022; Al-Marjani et al., 2023). **(iii)** We analyze the CR for the worst-possible environment. Specifically, tree-like environments that require deciding both _when_ and _where_ to navigate exhibit near-worst-case CR. **(iv)** Lastly, we complement these results by presenting different environments and their CR, providing more intuition to our results.

**Related Work.** The idea of utilizing lookahead information to update the played policy is related to a control concept called Model Predictive control (MPC, Camacho et al., 2007), also known as receding horizon control. In complex control problems, it could be challenging to predict the system behavior in long horizons due to errors in the model or nonlinear dynamics. To mitigate this, MPC designs a control scheme for much shorter horizons, where the model is approximately accurate, oftentimes on a simplified (e.g., linearized) model. Then, to correct the deviations due to modeling errors, MPC continuously updates the controller according to the actual system state. In our context, the localized system estimates could be seen as lookahead information. Similar ideas have also been used for planning in reinforcement learning settings (Tamar et al., 2017; Efroni et al., 2019, 2020). Yet, these concepts are mainly used to improve planning efficiency and account for nonlinearities/disturbances in the model. A few notable exceptions study the competitive ratio (and/or dynamic regret) between controllers with partial lookahead information to ones with full information (Li et al., 2019; Zhang et al., 2021; Lin et al., 2021, 2022) - a different measure than ours. Moreover, there is no clear way to translate any of these results into tabular problems.

The special case of one-step lookahead, where immediate rewards are observed before making a decision, has been studied in various problems. Possibly the most famous instance of such a problem is the prophet inequality. There, a set of known distributions is sequentially observed, and agents choose whether to either take a reward and end the interaction or discard it and move to the next distribution (Correa et al., 2019). This could be formulated as a chain environment with two actions - a rewarding action that moves to an absorbing state and a non-rewarding one that moves forward in the chain. A generalization of the prophet problem to resource allocation over Markov chains was studied in (Jia et al., 2023). To obtain a CR that is independent of the interaction length, the authors allow both the online and offline algorithms to choose their initial state. In both cases (and many other problems), the CR is measured between a one-step lookahead and a full lookahead agent, which observes all rewards in advance. In contrast, we measure the CR between no-lookahead agents and all possible lookaheads, so our results are complementary.

Finally, Garg et al. (2013) studied another related resource allocation model. In their work, the competitive ratio for Markov Decision Processes is measured between an online agent with access to the \(L\)-future reward distributions and transition probabilities, versus an agent who observes all statistical information in advance. A similar adversarial notion is also presented specifically for resource allocation. In contrast, we assume that the distributions are known to both agents and only the oracle observes reward realizations.

Preliminaries

We work under the episodic tabular reinforcement learning model. The environment is modeled as a Markov Decision Process (MDP), defined by the tuple \((,,H,P,R,)\), where \(\) is the state space (\(||=S\)), \(\) is the action space (\(||=A\)), \(H\) is the horizon, \(P\) is the transition kernel, \(R\) is the stochastic reward and \(_{S}\) is the initial state distribution. At the first timestep, an initial state is generated \(s_{1}\). Then, at every timestep \(h[H]\{1,,H\}\), given environment state \(s_{h}\), the agent performs an action \(a_{h}\), obtains a stochastic reward \(R_{h}(s_{h},a_{h})\) and transitions to a state \(s_{h+1}\) with probability \(P_{h}(s_{h+1}|s_{h},a_{h})\). For brevity, we use the notation \(=[H]\).

We assume that rewards at different timesteps are independent, but allow them to be arbitrarily correlated between state-actions at the same step. We denote the expected reward by \(r_{h}(s,a)\) and assume that the rewards are non-negative.1 Rewards and transitions are always assumed to be mutually independent, and transitions are independent between rounds. While we focus on non-stationary models, where the reward and transition distributions could depend on the timestep \(h\), our analysis techniques could be easily adapted to stationary models, where the distributions are timestep-independent, and all the proofs in the appendix also state the results for stationary models.

### Lookahead Policies and Values

We assume w.l.o.g. that all rewards are generated before the interaction starts. We denote by \(_{h}=\{R_{h}(s,a)\}_{s,a}\), the set of all rewards at timestep \(h\) and by \(_{h}^{L}=\{_{t}\}_{t=h}^{h+L-1}\), the \(L\)-lookahead reward information, containing all reward information for \(L\)-timesteps starting from \(h\). By convention, \(_{h}^{0}\) is the empty set. A lookahead policy is defined as follows.

**Definition 1**.: _A lookahead policy \(^{L}:[H]^{SAL}_{}\) is a policy that for each timestep \(h\), observes the state \(s_{h}\) and the lookahead reward information \(_{h}^{L}\) and generates an action \(a_{h}\) with probability \(_{h}^{L}(a_{h}|s_{h},_{h}^{L})\). The set of all lookahead policies is denoted by \(^{L}\)._

For example, a one-step lookahead policy observes the immediate rewards at the current state before acting, while a full lookahead policy has access to all reward realizations before the interaction starts. When \(L=0\), the policy only depends on the state and is Markovian; we therefore denote \(^{}=^{0}\).

The goal of any agent is to maximize its cumulative reward, also known as the _value_, \(V^{L,}=_{h=1}^{H}R_{h}(s_{h},a_{h})|s_{1}, \). For brevity, we omit the conditioning on the initial state distribution. The optimal value given a lookahead \(L\) is \(V^{L,*}=_{^{L}^{L}}V^{L,^{L}}\). If we want to emphasize that an environment parameter (say, the transition kernel \(P\)) is fixed, we shall specify it, e.g., \(V^{L,}(P,r)\).

We analyze the relation between the'standard value' of an agent that plays optimally using no future information (\(V^{0,*}\)) and a lookahead agent that observes the \(L\)-future rewards before acting (\(V^{L,*}\)). Formally, let \((r)\) be the set of all non-negative distributions with rewards expectations \(r_{h}(s,a)\). The \(L\)-lookahead competitive ratio (CR) is defined as

\[CR^{L}(P,r)=_{^{H}(r)}(P,r)}{V^{L,*} (P,r)}.\] (1)

That is, the competitive ratio is the worst-possible multiplicative loss of the standard (no-lookahead) policy, compared to an \(L\)-lookahead policy, given fixed transition kernel and expected rewards. For ratios to be well-defined, we follow the convention that any division by zero equals \(+\).

**Remark 1**.: _We emphasize that the reward distributions are known in advance to both the nookahead and the \(L\)-step lookahead agents, in striking contrast to adversarial settings. In the latter, the reward could be arbitrary and is only given to an oracle agent. In particular, any upper bound on \(CR^{L}(P,r)\) will also apply to adversarial settings._

**Remark 2**.: _Without lookahead information, \(P\) and \(r\) suffice to calculate the optimal value [Sutton and Barto, 2018], so one could also write \(CR^{L}(P,r)=(P,r)}{_{R^{H}(r)}V^{L,*}(P,r)}.\)_

We similarly study the \(L\)-lookahead CR for the worst-case reward expectations, defined as2\(CR^{L}(P)=_{r_{h}^{SA}}CR^{L}(P,r).\) Finally, we study the CR for the worst-case environment \(P\) and initial state distribution \(\), denoted by \(CR^{L}\). In particular, we show that stationary environments achieve near-worst-case CR.

**Interpretation: the gain from lookahead information.** The no-lookahead agent is the standard agent used throughout the RL literature and serves as an 'off-the-shelf' agent. As such, the competitive ratios quantify the potential gain when moving from classic RL settings to agents that utilize future reward information. While using future information always increases the value, it often comes at some price - either because access to such information is costly, or since lookahead algorithms are much more complicated and computationally expensive. The CRs analyzed in the paper can help determine whether the potential gain is worth the price - and choosing which agent to deploy.

### Occupancy Measures

Occupancy measures are the visitation probabilities of an agent in different state-actions. In particular, for any (potentially lookahead) policy, we define \(d_{h}^{}(s)=\{s_{h}=s\}\) and \(d_{h}^{}(s,a)=\{s_{h}=s,a_{h}=a\}\), where randomness is w.r.t. both transitions, rewards and internal policy randomization, given that actions are generated from the policy \(^{L}\). For \(h=1\), the state distribution only depends on the initial state distribution \(\), and we use \(d_{1}^{}(s)\), \(d_{1}(s)\) and \((s)\) interchangeably. We also define the conditional occupancy measure as \(d_{h}^{}(s|s_{t}=s^{})=\{s_{h}=s|s_{t}=s^{}\}\) for some \(t h\) and similarly use \(d_{h}^{}(s,a|s_{t}=s^{})\). Intuitively, this is the reaching probability from state \(s^{}\) at time \(t\) to a state \(s\) at time \(h\) when playing a policy \(\). Without lookahead information, it is well-known that the set of occupancy measures induced by Markovian policies is a convex compact polytope (Altman, 2021), and the value of any Markovian policy could be expressed using occupancies by

\[V^{0,} =[_{h=1}^{H}R_{h}(s_{h},a_{h})]=[_{(h,s,a)}\{s_{h}=s,a_{h}=a\}R_{h}(s,a)]\] \[=_{(h,s,a)}\{s_{h}=s,a_{h}=a\}[R_{ h}(s,a)]=_{(h,s,a)}d_{h}^{}(s,a)r_{h}(s,a)=d^{^{T}}r.\] (2)

Finally, denote the optimal reaching probability to a state \(s\) as \(d_{h}^{*}(s)=_{^{}}d_{h}^{}(s)\). Notice that rewards and transitions are independent, so reward information does not affect the optimal reaching probability and it is sufficient to look at Markovian policies. Moreover, after reaching a state \(s\), an agent could always deterministically choose an action \(a\), so \(d_{h}^{*}(s,a)=_{^{}}d_{h}^{}(s,a)=d_{h}^{*}(s)\). Similarly, we define the optimal conditional reaching probability as \(d_{h}^{*}(s|s_{t}=s^{})=_{^{}}d_{h}^{}(s|s_ {t}=s^{})\), and as the for non-conditional occupancy measures, we have that \(d_{h}^{*}(s,a|s_{t}=s^{})=d_{h}^{*}(s|s_{t}=s^{})\).

## 3 Competitiveness Versus Full Lookahead Agents

Before analyzing the CR for the full range of lookahead values, we start by studying the full lookahead case, where all rewards are observed before the interaction starts. This regime is applicable, for example, in goal-oriented problems, where goals are given to the agent before an episode starts (Andrychowicz et al., 2017). Notably, we show a link between the CR for the worst-case reward expectations, \(CR^{H}(P)\), and existing complexity measures in offline RL and reward-free exploration. While the results of this section will later be covered by the more general multi-step lookahead, this case gives valuable insights on the worst-case distributions. Moreover, much of the proof techniques presented in this section will later be used to prove the results for the multi-step lookahead.

When all rewards are observed before the interaction starts, each instantiation of the reward is equivalent to an RL problem with _known deterministic_ rewards. In particular, the optimal policy given the reward is Markovian, and using the value formulation in Equation (2), we have

\[V^{H,*}(P,r) =[_{^{}}_{(h,s,a) }d_{h}^{}(s,a)R_{h}(s,a)][_{(h,s,a) }_{^{}}d_{h}^{}(s,a)R_{h}(s,a)]\] \[=_{(h,s,a)}d_{h}^{*}(s)[R_{h}(s,a)]= _{(h,s,a)}d_{h}^{*}(s)r_{h}(s,a).\] (3)At first glance, this bound seems extremely crude - the agent optimally navigates to collect all the expected rewards. Yet, at a second glance, it gives intuition on the worst-case distribution: a situation where only one reward at a single state is realized in every episode. Then, full lookahead agents can optimally navigate to this state and still collect all the realized rewards. While we cannot fully enforce a single reward realization (due to the independence of rewards in different timesteps), we can approximate this behavior by focusing on _long-shot_ distributions .

**Definition 2**.: _Rewards have long-shot distributions with parameter \((0,1)\) and expectation \(r\) if_

\[ h[H],s,a: R_{h}(s,a)=\{r_{h}(s,a)/&w.p.\ \\ 0&w.p.\ 1-.\]

_independently for all \(h,s,a\). We also use the notation \(R LS_{}(r)\)._

Notice that for any given \(\), long-shot distributions are bounded; thus, long-shot rewards could always be scaled to be supported by \(\) without affecting the CR. Moreover, when \( 1/SAH\), with high probability, at most a single reward will be realized, and the bound in Equation (3) is achieved in equality as \( 0\). Formally, the CR versus a full lookahead agent is characterized as follows:

**Theorem 1**.: _[CR versus Full Lookahead Agents; see Appendix A for the proof]_

_Worst-case distributions: \(CR^{H}(P,r)=_{^{}}} d_{h}^{}(s,a)r_{h}(s,a)}{_{(h,s,a)}d_{h}^{}(s)r_{h}(s,a)}\)._

_Worst-case reward expectations: \(CR^{H}(P)=_{^{}}_{(h,s,a)}^{}(s,a)}{d_{h}^{}(s)}\)._

_Worst-case environments: For all environments, \(CR^{H},}}\). Also, for any \((0,1)\) there exist stationary environments with rewards over \(\) s.t. if \(S=A^{}+1\) for \(n\{0,,H-1\}\), then \(CR^{H}(P,r)(S-1))(A-1)(S-1)}\), and if \(S A^{H}-1\), then \(CR^{H}(P,r)}\)._

Proof Sketch.: **Part I.** Recalling Remark 2 and Equation (2), to prove the first part of the proposition, one only needs to calculate the full lookahead value for the worst-case distribution. An upper bound for this value is already given in Equation (3); we directly calculate the value for long-shot distributions \(LS_{}(r)\) and show that this bound is achieved at the limit of \( 0\).

**Part II.** The proof of the second part of the theorem utilizes the previously calculated \(CR^{H}(P,r)\) to optimize for the worst-case expectations. This is done using the minimax theorem, exchanging the reward minimization and the policy maximization. To make the internal maximization problem concave, we move from the space of Markovian policies to the set of occupancy measures induced by Markovian policies, which is convex . To make the reward minimization convex, we show that the denominator can be converted to the constraint \(_{(h,s,a)}d_{h}^{}(s)r_{h}(s,a)=1\). Then, the minimax theorem can be applied, and we explicitly solve the resulting optimization problem. The formal application of the minimax theorem and its solution is done in Lemma 1 in the appendix.

**Part III.** The proof of the final statement is further divided into two parts.

_Lower bounding \(CR^{H}\)._ The lower bound \(CR^{H} 1/A^{H}\) is inductively achieved from the dynamic programming equations for both the no-lookahead and full lookahead values. The bound \(CR^{H} 1/SAH\) is obtained by choosing a specific policy \(^{}\) and substituting in \(CR^{H}(P)\): the Markovian policy whose occupancy is \(d_{h}^{_{u}}(s,a)=_{(h^{},s^{},a^{}) }d_{h}^{_{h^{},s^{},a^{}}^{*}}(s,a)\), where \(_{h,s,a}^{*}^{}\) is a policy that maximizes the reaching probability to \((h,s,a)\).

_Upper bounding \(CR^{H}\)_ - designing a worst-case environment. We show that a modified tree graph achieves a near-worst-case competitive ratio. In tree-based MDPs, each state represents a node in a tree, with the initial state as its root, and actions take the agent downwards through the tree. In our example, rewards are long-shots located at the leaves of such trees. However, this structure, by itself, does not lead to the worst-case bound. Intuitively, a standard RL agent would navigate to the leaf with the maximal expected reward, while an agent with a full lookahead would navigate to the leaf with the highest reward realization. Since there are at most \(S\) leaves with \(A\) actions in each, this would lead to \(CR^{H}(P)\). This is improved by a simple modification: at the root of the tree, we allocate one action to 'delay' the entrance to the tree and stay in the root (as illustrated in Figure 2 in the appendix). While agents without lookahead have no incentive to use this action, a full lookahead agent could predict _when_ a reward will be realized and enter the tree at a timing that allowsits collection. When \(H\) is large enough (compared to the tree depth), this allows the full lookahead agent to have approximately \(H\) attempts to collect a reward and lead to the additional \(H\)-factor (up to log factors). The proof could be extended to any value of \(S\) by allowing the tree to be incomplete - we refer the readers to the remark at the end of Proposition 1 in the appendix for more details. 

Surprisingly, the CR for the worst-case reward expectation \(CR^{H}(P)\) is the inverse of a concentrability coefficient that appears in many different RL settings, called the _coverability coefficient_. In particular, it affects the learning complexity in both online and offline RL settings, where agents must learn to act optimally either based on logged date or interaction with the environment (Xie et al., 2022).3 It also has a central role in reward-free exploration, where agents aim to learn the environment so that they can perform well for _any_ given reward function (Al-Marjani et al., 2023). We emphasize that the lookahead setting is fundamentally different - we assume that all agents have exact information on both the dynamics and reward distributions and ask about the multiplicative performance improvement due to additional knowledge on reward realization. In contrast, in learning settings, the main complexity is usually in learning the dynamics, and the rewards are oftentimes assumed to be deterministic. Moreover, the analyzed quantities are either regret measures or sample complexity, which cannot be directly linked to the competitive ratio.

The last part of Theorem 1 shows that tree-like environments with a delaying action at their root exhibit worst-case CR. Similar delay mechanisms were previously used to prove regret and PAC lower bounds for nonstationary MDPs (Domingues et al., 2021; Tirinzoni et al., 2022), though with a major difference - in previous works, a nonstationary reward distribution is used to force the agent to learn when to traverse the tree and where to navigate, and the reward is time-extended (obtained for \((H)\) rounds). In contrast, our formulation is fully stationary and a reward can only be collected once. Still, the lookahead agent can use the delay to linearly increase the reward-collection probability, without any need to create time-extended rewards.

## 4 Competitiveness Versus Multi-Step Lookahead Agents

We now generalize the results of Section 3 and analyze the competitive ratio compared to \(L\)-lookahead agents, for _any_ possible lookahead range \(L[H]\). We also give special attention to the case of one-step lookahead, where the immediate rewards are revealed _before_ taking an action.

Inspired by the full lookahead case, we focus on long-shot rewards. For such rewards, an agent would expect to see no more than a single reward during an episode, which would only be discovered \(L\)-steps in advance. As such, a reasonable strategy would play a Markovian policy that maintains a 'favorable' state distribution, such that whenever and wherever a future reward is realized, the agent could optimally navigate to it. Letting \(t_{L}(h)\) be the time step where the \(h\)-step rewards are revealed to an \(L\)-lookahead agent, this corresponds with the following worst-case value:

**Proposition 2**.: _For any \(L[H]\), let \(t_{L}(h)=\{h-L+1,1\}\). Then, it holds that_

\[_{^{H}(r)}V^{L,*}(P,r)=_{^{}}_{(h,s,a)}r_{h}(s,a)_{s^{}}d^{ }_{t_{L}(h)}(s^{})d^{*}_{h}(s|s_{t_{L}(h)}=s^{})\]

The proof can be found at Appendix B. It is comprised of calculating the value of long-shot rewards \(R LS_{}(r)\) at the limit when \( 0\) and then showing that the same quantity also serves as an upper bound of the value for all reward distributions.

For full lookahead, we have \(t_{H}(h)=1\), and \(d^{}_{t_{H}(h)}\) becomes the initial state distribution \(\). This leads to the same value as in Equation (3). The second extremity is when \(L=1\) and \(t_{1}(h)=h\). Then, the conditional occupancy is \(d^{*}_{h}(s|s_{t_{L}(h)}=s^{})=\{s=s^{}\}\) and we get the simplified expression

\[_{^{H}(r)}V^{1,*}(P,r)=_{^{ }}_{(h,s,a)}r_{h}(s,a)d^{}_{h}(s).\] (4)

Notably, this is the value of an agent that collects the rewards of all the actions in visited states (regardless of the action it actually played) but has no lookahead information.

Recalling Remark 2, one could use Proposition 2 to directly calculate \(CR^{L}(P,r)\). This, in turn, allows analyzing the worst-case reward expectations and environment, as stated in the following:

**Theorem 3**.: _[CR versus Multi-Step Lookahead Agents; see Appendix B for the proof] For any \(L[H]\), let \(t_{L}(h)=\{h-L+1,1\}\). Then, it holds that:_

\[\;CR^{L}(P,r)=}_{ (h,s,a)}r_{h}(s,a)d_{h}^{}(s,a)}{_{s} _{(h,s,a)}r_{h}(s,a)_{s^{}}d_{t_{L}( h)}^{s^{}}(s)d_{h}^{}(s|s_{t_{L}(h)}=s^{})}.\]

\[\]

\[CR^{L}(P)=_{^{*}^{}}_{^{}} _{(h,s,a)}^{}(s,a)}{_{s^{}}d_{t_{L}(h)}^{^{*}}(s^{})d_{h}^{}(s|s_{t_{L}(h)}=s^{})}.\]

\[CR^{L},} }.\]

\((0,1)\) _there exist stationary environments with rewards over \(\) s.t. if \(S=A^{n}+1\) for \(n\{0,,L-1\}\), then \(CR^{L}(P,r)(S-1))(A-1)(S-1)}\), and if \(S A^{L}+1\), then \(CR^{L}(P,r)-1)}\)._

Proof Sketch.: The first part of the theorem is a direct result of Proposition 2 and Remark 2. For the second part, we first rewrite

\[CR^{L}(P,r)=_{^{*}^{}}_{^{}} }r_{h}(s,a)d_{h}^{}(s,a)}{_{(h,s,a) }r_{h}(s,a)_{s^{}}d_{t_{L}(h)}^{^{* }}(s^{})d_{h}^{*}(s|s_{t_{L}(h)}=s^{})},\]

and as in the full lookahead case, we apply the minimax theorem using Lemma 1. However, direct application would require calculating the infimum over \(^{*}^{}\), and not a minimum. Thus, compared to the full lookahead, we also need to prove that the minimum is obtained in this set. We do so in Lemma 2, relying on the set of occupancy measures being a convex compact polytope.

In the last part, we use the same tree example to upper bound \(CR^{L}(P,r)\). The lower bound is proven using a reduction from the full lookahead bound. In particular, the bound of \(1/SAH\) trivially holds from the full lookahead case. For the second lower bound, we devise a Markovian policy \(_{u}\) such that for the appropriate choice of reward functions \(r^{i}\), we prove that

\[}(P,r)}{V^{L,*}(P,r)}_{ i[H^{-L+1}],\\ s^{}}\{}}_{(s,a)}_{h=i}^{i+L-1}r_ {h}^{i}(s,a)d_{h}^{}(s,a|s_{i}=s^{})}{_{(s,a) }_{h=i}^{i+L-1}r_{h}^{i}(s,a)d_{h}^{*}(s|s_{i}=s^{} )}\}.\]

Each of the terms is the competitive ratio versus a full lookahead agent with horizon \(L\) that starts acting at \(s_{i}=s^{}\). Hence, by Theorem 1, all terms are bounded by \(}\). To elaborate, the reward \(r^{i}\) limits the reward only to the new timesteps the lookahead agent gets to observe when it reaches step \(i\). The policy \(_{u}\) is a mixture (in the occupancy space) of policies \(_{i}\) that start by playing the Markovian policy that maximizes the value of Proposition 2, up to timestep \(i\), and then maximizes \(r^{i}\). 

Theorem 3 extends the full lookahead results of Theorem 1 and tightly characterizes the CR for the full spectrum of lookaheads, both as a function of the environment and for the worst-case environments. Notice that even though lookahead policies are highly non-Markovian, all bounds are expressed using Markovian policies.

**One-step lookahead.** In the case where the immediate reward is observed before acting, Theorem 3 proves that even for the worst-case environment, \(CR^{1}=}}\), namely, independent of the size of the state-space. Moreover, for any transition kernel \(P\), the CR is given by

\[CR^{1}(P)=_{^{*}^{}}_{^{}} _{(h,s,a)}^{}(s,a)}{d_{h}^{^{*}}(s)}.\] (5)

While the coverability coefficient of \(CR^{H}(P)\) requires a policy \(\) to cover all states _simultaneously_ in proportion to their optimal reaching probability, \(CR^{1}(P)\) provides a weaker coverability notion; it requires being able to cover _any pre-known_ state-distribution induced by a Markov policy \(^{*}\). We emphasize that \(\) must cover this distribution using all actions, so imitating the behavior of \(^{*}\) might be challenging - with a ratio of \(1/AH\) as the worst case.

Thus, \(CR^{1}(P)\) could be seen as an intermediate point between the coverability coefficient and _single-policy coverability_(Xie et al., 2022), defined by the ratio between the state-action occupancy of the optimal policy and a single data distribution. Yet, Xie et al. (2022) argue that this notion is too weak to allow any guarantees. It is of interest to investigate whether our refined notion, which requires covering all valid state distributions, mitigates the issues they present and allows deriving meaningful results in offline and online RL.

In general, one could interpret the ratios \(CR^{L}(P)\) as a class of decreasing4 (inverse) concentrability coefficients, starting from the coverability of all pre-known state distributions (\(CR^{1}(P)\)) and ending with the coverability coefficient (\(CR^{H}(P)\)). Thus, it is intriguing to further study the connection of these values to other domains in which concentrability naturally arises.

## 5 Examples

We now present several MDP structures and analyze their competitive ratio for various lookaheads.

**Disguised contextual bandit (Al-Marjani et al., 2023).** Maybe the most basic scenario is when actions do not affect the transitions, i.e., \(P_{h}(s^{}|s,a)=P_{h}(s^{}|s)\) for all possible \((h,s,a,s^{})\). Specifically, the state distribution is independent of the played policy - there exists an occupancy measure \(d_{h}\) such that for all policies, \(d_{h}^{}(s)=d_{h}(s)\). Thus, it also holds that \(d_{h}^{*}(s)=d_{h}(s)\), and

\[CR^{H}(P)=_{^{}}_{h,s,a}^{}(s,a)}{d _{h}^{*}(s)}=_{^{}}_{h,s,a}(s)_{h}( a|s)}{d_{h}(s)}=_{^{}}_{h,s,a}_{h}(a|s)= .\]

The last equality holds since \(_{h}(a|s)_{}\). Using the same arguments, one could also obtain this CR for one-step lookahead, so by the monotonicity of the CR in the lookahead, \(CR^{L}(P)=\) for all \(L[H]\). This is to be expected - without control over the dynamics, the best lookahead agents could do is to maximize immediate rewards, and any additional lookahead information is useless. Then, in each state, knowing the realization can only increase the reward by a factor of \([_{a}R_{h}(s,a)]}{_{a}[R_{h}(s,a)]} A\).

**Delayed trees.** This is the example described in the proofs of the main results, also detailed in Proposition 1 and depicted in Figure 2. In such environments, we get a worst-case CR of \(CR^{L}(P,r)=,} \). These trees are an extreme case where lookahead information is not only used to collect immediate rewards but rather to navigate to long-term rewards.

**Chain MDPs.** We go back to a bandit-like scenario and add limited control on the dynamics, in the form of a chain. The agent starts at the head of the chain (\(s_{1}\)), and at each node \(k\) of the chain, it could choose to advance to the next node by taking the action \(a=a_{1}\) or to move to an absorbing terminal state \(s_{T}\) by taking any other action. The environment is depicted in Figure 1(a).

Figure 1: Examples: CR for grid and chain environments.

One special problem that falls into this structure is the prophet inequality problem. In particular, assume that reward can only be obtained when moving from the chain to the terminal state (\( k,a,\ r_{h}(s_{k},a_{1})=r_{h}(s_{T},a)=0\)). Thus, at each node of the chain, the agent chooses whether to collect a reward and effectively end the interaction or discard it and move forward in the chain. In other words, the problem becomes an optimal-stopping problem. As such, it is reasonable to allow the agent to see the instantaneous rewards before deciding whether to stop, leading to one-step lookahead agents. This problem has numerous applications, especially in the context of posted-price mechanisms (Correa et al., 2017, 2019a). A classical result is that the CR between one-step lookahead and full lookahead agents is always bounded by \(1/2\)(Hill and Kertz, 1981).

Assuming this reward structure with the worst-case reward distribution, the full lookahead agent could reach all rewards and collect them, thus collecting \(V^{H,*}(P,r)=_{k=1}^{H}_{a}r_{k}(s_{k},a)\) (as in Equation (3)). Similarly, a one-step lookahead agent could move forward in the chain using the policy \(_{k}(s_{k})=a_{1}\) while effectively collecting all rewards and achieving the same value (see Equation (4)). In contrast, a no-lookahead agent would have to choose a single reward to collect, obtaining a value of \(V^{0,*}(P,r)=_{k[H],a}r_{k}(s_{k},a)\). The resulting CR for this reward structure would be

\[CR^{H}(P,r)=CR^{1}(P,r)=}r_{k}(s_{k},a)}{ _{k=1}^{H}_{a}r_{k}(s_{k},a)},\]

where the inequality is since there are only \(A-1\) rewarding actions, and equality is achieved when all expected rewards are equal. Notably, the reward structure in the prophet problem is near-worst-case; one could verify that for chain MDPs, it holds that \(CR^{H}(P)(1-)\). This is due to the second part of Theorem 1, using the following policy: for all chain states \(k[H]\), move forward w.p. \(_{k}(a_{1}|s_{k})=1-\) and play any other action \(i>1\) w.p. \(_{k}(a_{i}|s_{k})=\). At the absorbing state \(s_{T}\), play uniformly \(_{k}(a_{i}|s_{T})=1/A\). This simple example provides two important insights.

_Hardness versus one-step lookahead_: chain MDPs exhibit the worst-case CR versus one-step lookahead agents. A central reason is that to move towards rewarding states (forward in the chain), agents must take non-rewarding actions (\(a_{1}\)) - there is a tradeoff between gathering instantaneous rewards and moving to future rewarding states.

_Easiness versus full lookahead_: as previously mentioned, the CR between one-step and full lookahead agents is the well-known prophet inequality and is at least \(1/2\); In other words, for chain MDPs, the information-gain from one-step-to full lookahead is marginal compared to the value of one-step versus no-lookahead. This is mainly because navigating to rewarding states is especially easy in chain MDPs - the agent only has to move forward. In contrast, in environments where navigating to rewarding states is difficult (e.g., the tree environment described in the main results), there is a substantial gain to the full lookahead.

These insights motivate two natural assumptions that reduce the CR.

**Dense rewards.** Assume that in all states where the reward can be strictly positive, it holds that \(r_{h}(s,a)}{_{a}r_{h}(s,a)} C\). That is, if there exists one rewarding action at a state, all its actions yield some minimal reward. States are allowed to yield zero rewards for all actions. When this assumption holds, agents could navigate to rewarding future states and still collect rewards, mitigating the issue observed in the chain MDPs. Letting \(^{*}_{^{}}_{(h,s,a)}r_{h} (s,a)d_{h}^{}(s)\), we have

\[CR^{1}(P,r) =}}_{(h,s,a)} r_{h}(s,a)d_{h}^{}(s,a)}{_{^{}}_{(h,s,a) }r_{h}(s,a)d_{h}^{}(s)}}r_ {h}(s,a)d_{h}^{^{*}}(s,a)}{_{(h,s,a)}r_{h}(s,a)d_{h}^{ ^{*}}(s)}\] \[}_{a^{} }r_{h}(s,a^{})d_{h}^{^{*}}(s,a)}{_{(h,s,a) }r_{h}(s,a)d_{h}^{^{*}}(s)},\]

where \((*)\) is since \(_{a}d_{h}^{^{*}}(s,a)=d_{h}^{^{*}}(s)\). Thus, dense rewards remove the horizon dependence in the CR, and for small \(C\), we get a similar CR as in the disguised contextual bandit problem.

**Ergodic MDPs.** One way to make the navigation task easier is to limit the control of the agent on the state. In (Al-Marjani et al., 2023), the authors suggest looking at MDPs whose transition kernels are near-uniform. Formally, for \(0<<<1\), they defined the family of transitions\[_{,}=q_{+}^{S}:_{i=1}^{S}q_{i}=1, _{i}q_{i} S^{-1},_{i}q_{i}}{S-1}},\]

and assumed that \(P_{h}(|s,a)_{,}\) for all \(h,s,a\). As \(\) goes to zero, the transition distribution becomes uniform, while at the limit of \(, 1\), this becomes the set of all possible transition kernels. Under this assumption, they prove that the covenibility coefficient is bounded by \(S^{}AH\) (see the end of the proof of Lemma 38 of Al-Mariani et al. 2023), which implies that \(CR^{H}(P)AH}\). In particular, if for all \(h,s,a\), \(P_{h}(s^{}|s,a)[,]\), then \(CR^{H}(P)\): independent of the size of the state-space. Finally, in their proof, Al-Marjani et al. 2023 show that \(d_{h}^{}()_{,}\) for all policies and timesteps. Substituting to Equation (5) (and using the uniform policy for \(\)) directly leads to \(CR^{1}(P)}{AS^{}}\), potentially improving the worst-case environment when \(S^{} H\).

**Grid MDPs** We end this section by analyzing a navigation example, where an agent navigates from one corner of an \(n n\) grid to the opposite corner ("Navigating in Manhattan", see Figure 1(b)). Due to space limits, we briefly describe the results while fully proving them in Appendix C.2. This example directly generalizes the chain example with added navigation difficulty; by enforcing zero rewards for all states above the bottom row, we effectively get a chain MDP of horizon \(n\). As a direct result, we immediately get that \(CR^{1}(P)=()\) and \(CR^{H}(P)=()\). Surprisingly, this bound is tight - adding one additional dimension to the problem is just as difficult as a chain. Like chains, some of the difficulty comes from sparsity in the reward, but even when all rewards have unit expectations, we show that \(CR^{L}(P)=()\). This implies that the problem has additional hardness due to the need for navigation, which is the same order of magnitude as the one due to sparse reward. As a final remark, we show that the ratio between one-step lookahead and full lookahead in grid MDPs is at most \(()\). This might be counter-intuitive at first, as the worst-case CR versus either of them is \(()\). In fact, this is possible since the worst-case environments are different; when competing with one-step lookahead agent, the hardness comes from reward sparsity, while versus full lookahead, it is also due to navigation issues. The one-step lookahead agent cannot use its information to navigate, so it has the same CR of \(1/H\) as the no-lookahead.

## 6 Conclusions and Future Work

We studied the value of future reward lookahead information in tabular reinforcement learning through the lens of competitive analysis. We characterized the CR for the worst-case distributions, reward expectations and transition kernels for the full range of possible lookahead. We also showed the connection between the resulting CR and concentrability coefficients from the literature of offline and reward-free RL. We find the appearance of the same coefficients in seemingly completely different RL problems intriguing and warrants further study.

While we took the first step in analyzing competitiveness in RL, various other competitive measures could be studied. One natural alternative would be to study transition lookahead, where agents observe future transition realizations. We believe that the results would greatly differ from ours; indeed, even with one-step lookahead, the CR can be exponentially small (as we prove in Appendix C.3). Another relevant competitive measure is to compare an agent with _predictions_ of the future rewards to agents with exact lookahead information. This models the realistic scenario where agents get approximate information on future rewards and want to utilize it to improve performance. Also, as in the prophet problem, one could analyze the CR between multi-step lookahead to full lookahead agents.

Finally, we focus on the CR for the worst-case distribution, which allows us to derive the exact value of lookahead agents. However, planning with lookahead for general reward distribution can be challenging. For full lookahead, one can perform standard planning using reward realization, making planning tractable. With one-step lookahead, it is possible to write Bellman equations for the value, but each calculation depends on the full distribution of the reward, making it hard to calculate. For multi-step lookahead, there is no clear way to perform planning without incorporating the future rewards into the state, rendering the planning exponential. While exact planning might be intractable, it could be possible to devise methods for approximate planning. Lastly, it is of great interest to design practical algorithms that can efficiently leverage lookahead information, that is, achieve the lookahead value; our results indicate that it is significantly higher than the no-lookahead value, so aiming for it could dramatically boost the performance. We also leave this direction for future work.