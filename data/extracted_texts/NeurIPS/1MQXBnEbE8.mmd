# M\({}^{3}\)-Impute: Mask-guided Representation Learning

for Missing Value Imputation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Missing values are a common problem that poses significant challenges to data analysis and machine learning. This problem necessitates the development of an effective imputation method to fill in the missing values accurately, thereby enhancing the overall quality and utility of the datasets. Existing imputation methods, however, fall short of considering the'missingness' information in the data during initialization and modeling the entangled feature and sample correlations explicitly during the learning process, thus leading to inferior performance. We propose M\({}^{3}\)-Impute, which aims to leverage the missingness information and such correlations with novel masking schemes. M\({}^{3}\)-Impute first models the data as a bipartite graph and uses an off-the-shelf graph neural network, equipped with a refined initialization process, to learn node embeddings. They are then optimized through M\({}^{3}\)-Impute's novel feature correlation unit (**FCU**) and sample correlation unit (**SCU**) that enable explicit consideration of feature and sample correlations for imputation. Experiment results on 15 benchmark datasets under three different missing patterns show the effectiveness of M\({}^{3}\)-Impute by achieving 13 best and 2 second-best MAE scores on average.

## 1 Introduction

Missing values in a dataset are a pervasive issue in real-world data analysis. They arise for various reasons, ranging from the limitations of data collection methods to errors during data transmission and storage. Since many data analysis algorithms cannot directly handle missing values, the most common way to deal with them is to discard the corresponding samples or features with missing values, which would compromise the quality of data analysis. To tackle this problem, missing value imputation algorithms have been proposed to preserve all samples and features by imputing missing values with estimated ones based on the observed values in the dataset, so that the dataset can be analyzed as a complete one without losing any information.

The imputation of missing values usually requires modeling of correlations between different features and samples. Feature-wise correlations help predict missing values from other observed features in the same sample, while sample-wise correlations help predict them in one sample from other similar samples. It is thus important to jointly model the feature-wise and sample-wise correlations in the dataset. In addition, the prediction of missing values also largely depends on the'missingness' of the data, i.e., whether a certain feature value is observed or not in the dataset. Specifically, the missingness information directly determines which observed feature values can be used for imputation. For example, even if two samples are closely related, it may be less effective to use them for imputation if they have missing values in exactly the same features. It still remains a challenging problem how to jointly model feature-wise and sample-wise correlations with such data missingness.

Among existing methods for missing value imputation, statistical methods [4; 9; 14; 16; 18; 19; 22; 28; 30; 31; 37; 43] extract data correlations with statistical models, which are generally not flexiblein handling mixed data types and struggles to scale up to large datasets. Learning-based imputation methods [10; 24; 27; 29; 33; 42; 50; 51; 53], instead, take advantage of the strong expressiveness and scalability of machine/deep learning algorithms to model data correlations. However, most of them are still built upon the raw tabular data structure as is, which greatly restricts them from jointly modeling the feature-wise and sample-wise correlations. In light of this, graph-based methods [52; 54] have been proposed to model the raw data as a bipartite graph, with samples and features being two different types of nodes. A sample node and a feature node are connected if the feature value is observed in that sample. The missing values are then predicted as the inner product between the embeddings of the corresponding sample and feature nodes. However, this simple prediction does not consider the specific missingness information as mentioned above. For instance, the target feature to impute may have different correlations with features in the samples which have different kinds of missingness; however, the _same_ feature-node embedding is still used for their imputation. A similar issue also arises for sample-node embeddings.

In this work, we address these problems by proposing M\({}^{3}\)-Impute, a mask-guided representation learning method for missing value imputation. The key idea behind M\({}^{3}\)-Impute is to explicitly utilize the data-missingness information as model input with our proposed novel masking schemes so that it can accurately learn feature-wise and sample-wise correlations in the presence of different kinds of data missingness. M\({}^{3}\)-Impute first builds a bipartite graph from the data as used in . In the embedding initialization for graph representation learning, however, we not only use the the relationships between samples and their associated features but also the missingness information so as to initialize the embeddings of samples and features jointly and effectively. We then propose novel feature correlation unit (**FCU**) and sample correlation unit (**SCU**) in M\({}^{3}\)-Impute to explicitly take feature-wise and sample-wise correlations into account for imputation. **FCU** learns the correlations between the target missing feature and observed features within each sample, which are then further updated via a soft mask on the sample missingness information. **SCU** then computes the sample-wise correlations with another soft mask on the missingness information for each pair of samples that have values to impute. We then integrate the output embeddings of **FCU** and **SCU** to estimate the missing values in a dataset. We carry out extensive experiments on 15 open datasets. The results show that M\({}^{3}\)-Impute outperforms state-of-the-art methods in 13 of the 15 datasets on average under three different settings of missing value patterns, achieving up to \(11.47\%\) improvement in MAE compared to the second-best method.

## 2 Related Work

**Statistical methods:** These imputation approaches include joint modeling with expectation-maximization (EM) [9; 16; 22], \(k\)-nearest neighbors (kNN) [14; 43], and matrix completion [5; 6; 18; 32]. However, joint modeling with EM and matrix completion often lack the flexibility to handle data with mixed modalities, while kNN faces scalability issues due to its high computational complexity. In contrast, M\({}^{3}\)-Impute is scalable and adaptive to different data distributions.

**Learning-based methods:** Iterative imputation frameworks [1; 2; 15; 20; 23; 24; 35; 41; 44; 45], such as MICE  and HyperImpute , have been extensively studied. These iterative frameworks apply different imputation methods for each feature and iteratively estimate missing values until convergence. In addition, for deep neural network learners, both generative models [27; 29; 36; 50; 51; 53], such as GAIN  and MIWAE , and discriminative models [10; 24; 48], such AimNet , have also been proposed. However, these methods are built upon raw tabular data structures, which fall short of capturing the complex correlations in features, samples, and their combination . In contrast, M\({}^{3}\)-Impute is based on the bipartite graph modeling of the data, which is more suitable for learning the data correlations for imputation.

**Graph neural network-based methods:** GNN-based methods [40; 52; 54] are proposed to address the drawbacks mentioned above due to their effectiveness in modeling complex relations between entities. Among them, GRAPE  transforms tabular data into a bipartite graph where features are one type of node and samples are the other. A sample node is connected to a feature node only if the corresponding feature value is present. This transformation allows the imputation task to be framed as a link prediction problem, where the inner product of the learned node embeddings is computed as the predicted values. IGRM  further enhances the bipartite graph by explicitly introducing linkages between sample nodes to facilitate message propagation between samples. However, these methods do not effectively encode the missingness information of different samples and features into the imputation process, which can impair their imputation accuracy. In contrast, M\({}^{3}\)-Impute enables explicit modeling of missingness information through novel masking schemes so that feature-wise and sample-wise correlations can be accurately captured in the imputation process.

## 3 M\({}^{3}\)-Impute

### Overview

We here provide an overview of M\({}^{3}\)-Impute to impute the missing value of feature \(f\) for a given sample \(s\), as depicted in Figure 1. Initially, the data matrix with missing values is modeled as an undirected bipartite graph, and the missing value is imputed by predicting the edge weight \(_{sf}\) of its corresponding missing edge (Section 3.2). M\({}^{3}\)-Impute next employs a GNN model, such as GraphSAGE , on the bipartite graph to learn the embeddings of samples and features. These embeddings, along with the known masks of the data matrix (used to indicate which feature values are available in each sample), are then input into our novel feature correlation unit (**FCU**) and sample correlation unit (**SCU**), which shall be explained in Section 3.3 and Section 3.4, to obtain feature-wise and sample-wise correlations, respectively. Finally, M\({}^{3}\)-Impute takes the feature-wise and sample-wise correlations into a multi-layer perceptron (MLP) to predict the missing feature value \(_{sf}\) (Section 3.5). The whole process, including the embedding generation, is trained in an end-to-end manner.

### Initialization Unit

Let \(^{n m}\) be an \(n m\) matrix that consists of \(n\) data samples and \(m\) features, where \(_{ij}\) denotes the \(j\)-th feature value of the \(i\)-th data sample. We introduce an \(n m\) mask matrix \(\{0,1\}^{n m}\) for \(\) to indicate that the value of \(_{ij}\) is _observed_ when \(_{ij}=1\). In other words, the goal of imputation here is to predict the missing feature values \(_{ij}\) for \(i\) and \(j\) such that \(_{ij}=0\). We define the _masked_ data matrix \(\) to be \(=\), where \(\) is the Hadamard product, i.e., the element-wise multiplication of two matrices.

As used in recent studies [52; 54], we model the masked data matrix \(\) as a bipartite graph and tackle the missing value imputation problem as a link prediction task on the bipartite graph. Specifically, \(\) is modeled as an undirected bipartite graph \(=(,)\), where \(=\{s_{1},s_{2},,s_{n}\}\) is the set of'sample' nodes and \(=\{f_{1},f_{2},,f_{m}\}\) is the set of 'feature' nodes. Also, \(\) is the set of edges that only exist between sample node \(s\) and feature node \(f\) when \(_{sf} 0\), and each edge \((s,f)\) is associated with edge weight \(e_{sf}\), which is given by \(e_{sf}=_{sf}\). Then, the missing value imputation problem becomes, for any missing entries in \(\) (where \(_{sf}\) = 0), to predict their corresponding edge weights by developing a learnable mapping \(F()\), i.e.,

\[_{sf}=F(,(s,f)). \]

Figure 1: Overview of the M\({}^{3}\)-Impute model.

The recent studies that use the bipartite graph modeling [52; 54] initialize all sample node embeddings as all-one vectors and feature node embeddings as one-hot vectors, which have a value 1 in the positions representing their respective features and 0's elsewhere. We observe, however, that such an initialization does not effectively utilize the information from the masked data matrix, which leads to inferior imputation accuracy, as shall be demonstrated in Section 4.3. Thus, in M\({}^{3}\)-Impute, we propose to initialize each sample node embedding based on its associated (initial) feature embeddings instead of initializing them separately. While the feature embeddings are randomly initialized, the sample node embeddings are initialized in a way that reflects the embeddings of the features whose values are available in their corresponding samples.

Let \(_{f}^{0}\) be the initial embedding of feature \(f\), which is a randomly initialized \(d\)-dimensional vector, and define \(_{F}^{0}=[_{f_{1}}^{0}_{f_{2}}^{0}_{f_{m}}^{0}]^{d m}\). Also, let \(_{s}^{m}\) be the \(s\)-th column vector of \(^{}\), which is a vector of the feature values of sample \(s\), and let \(_{s}^{m}\) be its corresponding mask vector, i.e., \(_{s}=_{s}(^{})\), where \(_{s}()\) denotes the \(s\)-th column vector of the matrix. We then initialize the embedding \(_{s}^{0}\) of each sample node \(s\) as follows:

\[_{s}^{0}=_{F}^{0}_{s}+ (-_{s}), \]

where \(^{m}\) is an all-one vector, and \(()\) is an MLP. Note that the term \(_{s}+(-_{s})\) indicates a vector that consists of observable feature values of \(s\) and some small positive values \(\) in the places where the feature values are unavailable (masked out).

### Feature Correlation Unit

To improve the accuracy of missing value imputation, we aim to fully exploit feature correlations which often appear in the datasets. While the feature correlations are naturally captured by GNNs, we observe that there is still room for improvement. We propose **FCU** as an integral component of M\({}^{3}\)-Impute to fully exploit the feature correlations.

To impute the missing value of feature \(f\) for a given sample \(s\), **FCU** begins by computing the feature 'context' vector of sample \(s\) in the embedding space that reflects the correlations between the target missing feature \(f\) and observed features. Let \(_{f}^{d}\) be the learned embedding vector of feature \(f\) from the GNN, and let \(_{F}\) be the \(d m\) matrix that consists of all the learned feature embedding vectors. We first obtain dot-product similarities between feature \(f\) and all the features in the embedding space, i.e., \(_{F}^{}_{f}\). We then mask out the similarity values with respect to _non-observed_ features in sample \(s\). Here, instead of applying the mask vector \(_{s}\) of sample \(s\) directly, we use a learnable'soft' mask vector, denoted by \(_{s}^{}\), which is defined to be \(_{s}^{}=_{1}(_{s})^{m}\), where \(_{1}()\) is an MLP with the GELU activation function . In other words, we obtain feature-wise similarities with respect to sample \(s\), denoted by \(_{s}^{f}\), as follows:

\[_{s}^{f}=_{2}((_{F}^{}_{f}) _{s}^{})^{d}, \]

where \(_{2}()\) denotes another MLP with the GELU activation function. **FCU** next obtains the Hadamard product between the learned embedding vector of sample \(s\), \(_{s}\), and the feature-wise similarities with respect to sample \(s\), \(_{s}^{f}\), to learn their joint representations in a multiplicative manner. Specifically, **FCU** obtains the feature context vector of sample \(s\), denoted by \(_{s}^{f}\), as follows:

\[_{s}^{f}=_{3}(_{s}_{s}^{f} )^{d}, \]

where \(_{3}()\) is also an MLP with the GELU activation function. That is, **FCU** fuses the representation vector of \(s\) and the vector that has embedding similarity values between the target feature \(f\) and the available features in \(s\) through the effective use of the soft mask \(_{s}^{}\). From (3) and (4), the operations of **FCU** can be written as

\[_{s}^{f}=(_{s},_{s},_{F})= _{3}(_{s}_{2}((_{F}^{} _{f})_{1}(_{s}))). \]

### Sample Correlation Unit

To measure similarities between \(s\) and other samples, a common approach would be to use the dot product or cosine similarity between their embedding vectors. This approach, however, fails to take into account the observability or availability of each feature in a sample. It also does not capture the fact that different observed features are of different importance to the target feature to impute when it comes to measuring the similarities. We introduce **SCU** as another integral component of \(^{3}\)-Impute to compute the sample 'context' vector of sample \(s\) by incorporating the embedding vectors of its similar samples as well as different weights of observed features. **SCU** works based on the two novel masking schemes, which shall be explained shortly.

Suppose we are to impute the missing value of feature \(f\) for a given sample \(s\). **SCU** aims to leverage the information from the samples that are similar to \(s\). As a first step to this end, we create a subset of samples \(\) that are similar to \(s\). Specifically, we randomly choose and put a sample into \(\) with probability that is proportional to the cosine similarity between \(s\) and the sample. This operation is repeated without replacement until \(\) reaches a given size.

**Mutual Sample Masking:** Given a subset of samples \(\) that include \(s\), we first compute the pairwise similarities between \(s\) and other samples in the subset \(\). While they are computed in a similar way to **FCU**, we only consider the commonly observed features (or the common ones that have feature values) in both \(s\) and its peer \(p\{s\}\), to calculate their pairwise similarity in the sense that the missing value of feature \(f\) is inferred. Specifically, we compute the pairwise similarity between \(s\) and \(p\{s\}\), which is denoted by \((s,p f)\), as follows:

\[(s,p f)=(_{s},_{p},_{f })(_{p},_{s},_{f}), \]

where \(_{s}\) and \(_{p}\) are the learned embedding vectors of samples \(s\) and \(p\) from the GNN, respectively, and \(_{s}\) and \(_{p}\) are their respective mask vectors. Note that the multiplication in the RHS of (6) is the dot product.

**Irrelevant Feature Masking:** After we obtain the pairwise similarities between \(s\) and other samples in \(\), it would be natural to consider a weighted sum of their corresponding embedding vectors, i.e., \(_{p\{s\}}(s,p f)\)\(_{p}\), in imputing the value of the target feature \(f\). However, we observe that \(_{p}\) contains the information from the features whose values are available in \(p\) as well as possibly other features as it is learned via the so-called neighborhood aggregation mechanism that is central to GNNs, but some of the features may be irrelevant in inferring the value of feature \(f\). Thus, instead of using \(\{_{p}\}\) directly, we introduce a \(d\)-dimensional mask vector \(_{p}^{f}\) for \(_{p}\), which is to mask out potentially irrelevant feature information in \(_{p}\), when it comes to imputing the value of feature \(f\). Specifically, it is defined by

\[_{p}^{f}=_{4}([_{p};}_{f}] )^{d}, \]

where \(}_{f}\) is an \(m\)-dimensional one-hot vector that has a value 1 in the place of feature \(f\) and 0's elsewhere, \([;]\) denotes the vector concatenation operation, and \(_{4}()\) is an MLP with the GELU activation function. Note that the rationale behind the design of \(_{p}^{f}\) is to embed the information on the features whose values are present in \(p\) as well as the information on the target feature \(f\) to impute. The mask \(_{p}^{f}\) is then applied to \(_{p}\) to obtain the masked embedding vector of \(p\) as follows:

\[_{p}(_{p},_{p}^{f})=_{5}(_{p} _{p}^{f})^{d}, \]

where \(_{5}()\) is also an MLP with the GELU activation function. Once we have the masked embedding vectors of samples (excluding \(s\)) in \(\), we finally compute the sample context vector of sample \(s\), denoted by \(_{s}^{f}\), which is a weighted sum of the masked embedding vectors with weights being the pairwise similarity values, i.e.,

\[_{s}^{f}=_{6}(_{p\{s\}}(s,p f)\ _{p}(_{p},_{p}^{f}))^{d}, \]

where \(_{6}()\) is again an MLP with the GELU activation function. From (6)-(9), the operations of **SCU** can be written as

\[_{s}^{f}=(_{},_{},_{F})=_{6}(_{p\{s\}}(s,p f)\ _{5}(_{p}_{4}([_{p}; }_{f}]))), \]

where \(_{}=\{_{p},p\}\) and \(_{}=\{_{p},p\}\).

Figure 2: **SCU**.

### Imputation

For a given sample \(s\), to impute the missing value of feature \(f\), M\({}^{3}\)-Impute obtains its feature context vector \(_{s}^{f}\) and sample context vector \(_{s}^{f}\) through **FCU** and **SCU**, respectively, which are then used for imputation. Specifically, it is done by predicting the corresponding edge weight \(_{sf}\) as follows:

\[_{sf}=_{}((1-)_{s}^{f}+_ {s}^{f}), \]

where \(_{}()\) denotes an MLP with a non-linear activation function (i.e., ReLU for continuous values and softmax for discrete ones), and \(\) is a learnable scalar parameter. This scalar parameter \(\) is introduced to strike a balance between leveraging feature-wise correlation and sample-wise correlation. It is necessary because the quality of \(_{s}^{f}\) relies on the quality of the samples chosen in \(\), so overly relying on \(_{s}^{f}\) would backfire if their quality is not as desired. To address this problem, instead of employing a fixed weight \(\), we make \(\) learnable and adaptive in determining the weights for \(_{s}^{f}\) and \(_{s}^{f}\). Note that this kind of learnable parameter approach has been widely adopted in natural language processing [26; 34; 38; 46] and computer vision [8; 55; 56], showing superior performance to its fixed counterpart. In M\({}^{3}\)-Impute, the scalar parameter \(\) is learned based on the similarity values between \(s\) and its peer samples \(p\{s\}\) as follows:

\[=_{}_{p\{s\}} (s,p f), \]

where \(\|\) represents the concatenation operation, and \(_{}()\) is an MLP with the activation function \((x)=1-1\,/\,e^{|x|}\). The overall operation of M\({}^{3}\)-Impute is summarized in Algorithm 1. To learn network parameters, we use cross-entropy loss and mean square error loss for imputing discrete and continuous feature values, respectively.

```
1:Input: Bipartite graph \(\), initial feature node embeddings \(_{F}^{0}\), GNN model (e.g., GraphSAGE) \(()\), known mask matrix \(\), and a subset of samples \(\).
2:Output: Predicted missing feature value \(_{sf}\).
3:Obtain initial sample node embeddings \(_{S}^{0}\) according to Equation (2).
4:\(_{S},_{F}=(_{S}^{0},_{F}^ {0},)\). \(\) Perform graph representation learning
5:\(_{s}^{f}=(_{s},_{s},_{F})\).
6:\(_{s}^{f}=(_{},_{},_{F})\).
7:Predict the missing feature value \(_{sf}\) using Equation (11).
```

**Algorithm 1** Forward computation of M\({}^{3}\)-Impute to impute the value of feature \(f\) for sample \(s\).

## 4 Experiments

### Experiment Setup

**Datasets:** We conduct experiments on 15 open datasets. These real-world datasets consist of mixed data types with both continuous and discrete values and cover different domains including civil engineering (Concrete, Energy), physics and chemistry (Yacht), thermal dynamics (Naval), etc. Since the datasets are fully observed, we introduce missing values by applying a randomly generated mask to the data matrix. Specifically, as used in prior studies [23; 24], we apply three masking generation schemes, namely missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR).1 We use MCAR with a missing ratio of \(30\%\), unless otherwise specified. We follow the preprocessing steps adopted in [52; 54] to scale feature values to  with a MinMax scaler . Due to the space limit, we below present the results of eight datasets that are used in Grape  and report the other results in Appendix.

**Baseline models:** M\({}^{3}\)-Impute is compared against popular and state-of-the-art imputation methods, including statistical methods, deep generative methods, and graph-based methods listed as follows:

**Mean**: It imputes the missing value \(_{sf}\) as the mean of observed values in feature \(f\) from all the samples. K-nearest neighbors (**kNN**) : It imputes the missing value \(_{sf}\) using the kNNs that have observed values in feature \(f\) with weights that are based on the Euclidean distance to sample \(s\). Multivariate imputation by chained equations (**Mice**) : This method runs multiple regressions where each missing value is modeled upon the observed non-missing values. IterativeSVD (**Svd**) : It imputes missing values by solving a matrix completion problem with iterative low-rank singular value decomposition. Spectral regularization algorithm (**Spectral**) : This matrix completion algorithm uses the nuclear norm as a regularizer and imputes missing values with iterative soft-thresholded SVD. **Miwae**: It works based on an autoencoder generative model trained to maximize a potentially tight lower bound of the log-likelihood of the observed data and Monte Carlo techniques for imputation. **Miracle**: It uses the imputation results from naive methods such as Mean and refines them iteratively by learning a missingness graph (m-graph) and regularizing an imputation function. **Gain**: This method trains a data imputation generator with a generalized generative adversarial network in which the discriminator aims to distinguish between real and imputed values. **Grape**: It models the data as a bipartite graph and imputes missing values by predicting the weights of the missing edges, each of which is done based on the inner product between the embeddings of its corresponding sample and feature nodes. **HyperImpute**: HyperImpute is a framework that conducts an extensive search among a set of imputation methods, selecting the optimal imputation method with fine-tuned parameters for each feature in the dataset.

**Model configurations:** Parameters of M\({}^{3}\)-Impute are updated by the Adam optimizer with a learning rate of 0.001 for 40,000 epochs. For graph representation learning, we use a variant of GraphSAGE , which not only learns node embeddings but also edge embeddings via the neighborhood aggregation mechanism, as similarly used in . We consider its three-layer GNN model. We employ mean-pooling as the aggregation function and use ReLU as the activation function for the GNN layers. We set the embedding dimension \(d\) to 128. It is known that randomly dropping out a subset of observable edges during training improves the model's generalization ability. We also leverage the observation and randomly drop \(50\%\) of observable edges during training. For each experiment, we conduct five runs with different random seeds and report the average results.

### Overall Performance

We first compare the feature imputation performance of M\({}^{3}\)-Impute with popular and state-of-the-art imputation methods. As shown in Table 1, M\({}^{3}\)-Impute achieves the lowest imputation MAE for six out of the eight examined datasets and the second-best MAE scores in the other two, which validates the effectiveness of M\({}^{3}\)-Impute. For Kin8nm dataset, M\({}^{3}\)-Impute underperforms Miracle. It is mainly because each feature in Kin8nm is independent of the others, so none of the observed features can help impute missing feature values. For Naval dataset, the only model that outperforms M\({}^{3}\)-Impute is HyperImpute . In the Naval dataset, nearly every feature exhibits a strong linear correlation with the other features, i.e., every pair of features has correlation coefficient close to one. This allows HyperImpute to readily select a linear model from its model pool for each feature to impute. Nonetheless, M\({}^{3}\)-Impute exhibits overall superior performance to the baselines as it can be well adapted to each dataset that possesses different amounts of correlations over features and samples. In other words, M\({}^{3}\)-Impute benefits from explicitly incorporating feature-wise and sample-wise correlations together with our carefully designed mask schemes. Furthermore, we evaluate the performance of M\({}^{3}\)-Impute under MAR and MNAR settings. We observe that M\({}^{3}\)-Impute consistently outperforms all the baselines under all datasets and achieves a larger margin in the improvement compared to the case with MCAR setting. This implies that M\({}^{3}\)-Impute is also effective in handling different patterns of missing values in the input data. Comprehensive results are provided in Appendix.

    & Yacht & Wine & Concrete & Housing & Energy & Naval & Kin8nm & Power \\  Mean & 2.09 & 0.98 & 1.79 & 1.85 & 3.10 & 2.31 & 2.50 & 1.68 \\ Svd  & 2.46 & 0.92 & 1.94 & 1.53 & 2.24 & 0.50 & 3.67 & 2.33 \\ Spectral  & 2.64 & 0.91 & 1.98 & 1.46 & 2.26 & 0.41 & 2.80 & 2.13 \\ Mice  & 1.68 & 0.77 & 1.34 & 1.16 & 1.53 & 0.20 & 2.50 & 1.16 \\ kNN  & 1.67 & 0.72 & 1.16 & 0.95 & 1.81 & 0.10 & 2.77 & 1.38 \\ Gain  & 2.26 & 0.86 & 1.67 & 1.23 & 1.99 & 0.46 & 2.70 & 1.31 \\ Miwae  & 4.68 & 1.00 & 1.81 & 3.81 & 2.79 & 2.37 & 2.57 & 1.74 \\ Grape  & 1.46 & **0.60** & 0.75 & 0.64 & 1.36 & 0.07 & 2.50 & 1.00 \\ Miracle  & 42.97 & 1.13 & 1.71 & 42.23 & 41.43 & 0.17 & **2.49** & 1.15 \\ HyperImpute  & 1.76 & 0.67 & 0.84 & 0.82 & **1.32** & **0.04** & 2.58 & 1.06 \\  M\({}^{3}\)-Impute & **1.33** & **0.60** & **0.71** & **0.60** & **1.32** & 0.06 & 2.50 & **0.99** \\   

Table 1: Imputation accuracy in MAE. MAE scores are enlarged by 10 times.

### Ablation Study

To study the effectiveness of three integral components of M\({}^{3}\)-Impute, we consider three variants of M\({}^{3}\)-Impute, each with a subset of the components, namely initialization only (Init Only), initialization + **FCU** (Init + **FCU**), and initialization + **SCU** (Init + **SCU**). The performance of these variants are evaluated against the top-performing imputation baselines such as Grape and HyperImpute. As shown in Table 2, the three variants derived from M\({}^{3}\)-Impute achieve lower MAE values than both baselines in most datasets, demonstrating the effectiveness of our novel components in M\({}^{3}\)-Impute.

Specifically, for initialization only, the key difference between M\({}^{3}\)-Impute and Grape lies in our refined initialization process of feature-node and sample-node embeddings. The reduced MAE values observed by the Init Only variant demonstrate that our proposed initialization process is more effective in utilizing information between samples and their associated features, including missing ones, as compared to the basic initialization used in . In addition, we observe that when **FCU** or **SCU** is incorporated, MAE values are further reduced for most datasets. This validates that explicitly modeling feature-wise or sample-wise correlations through our novel masking schemes can improve imputation accuracy. When all the three components are combined together as in M\({}^{3}\)-Impute, they work synergistically to lower MAE values, validating the efficacy of explicit consideration of both sample-wise and feature-wise correlations (in addition to the refined initialization process) for missing data imputation.

### Robustness

**Missing ratio:** In practice, datasets may possess different missing ratios. To validate the model's robustness under such circumstances, we evaluate the performance of M\({}^{3}\)-Impute and other baseline models with varying missing ratios, i.e., 0.1, 0.3, 0.5, and 0.7. Figure 3 shows their performance. We use the MAE of HyperImpute (\(HI\)) as the reference performance and offset the performance of each model by MAE\({}_{x}\)\(-\) MAE\({}_{HI}\), where \(x\) represents the considered model. For clarity, we here only report the results of four top-performing models. As shown in Figure 3, M\({}^{3}\)-Impute outperforms other baseline models for almost all the cases, especially under Yacht, Concrete, Energy, and Housing datasets. It is worth noting that modeling feature correlations in these datasets is particularly challenging due to the presence of considerable amounts of weakly correlated features, along with a few strongly correlated ones. Nonetheless, **FCU** and **SCU** in M\({}^{3}\)-Impute were able to better capture such correlations with our efficient masking schemes, thereby resulting in a large improvement in imputation accuracy. In addition, for Kin8nm dataset, M\({}^{3}\)-Impute ties with the second-best model, Grape. As mentioned in Section 4.2, each feature in Kin8nm is independent of the others, so none of the observed features can help impute missing feature values. For Naval dataset, where each feature strongly correlates with the others, M\({}^{3}\)-Impute surpasses Grape but falls short of HyperImpute, due to the same reason as discussed above. Overall, M\({}^{3}\)-Impute is robust to various missing ratios. Comprehensive results for all the baseline models can be found in Appendix.

**Sampling strategy in SCU:** While **SCU** uses a sampling strategy based on pairwise cosine similarities to construct a subset of samples \(\), the simplest sampling strategy to build \(\) would be to choose samples uniformly at random without replacement (M\({}^{3}\)-Uniform). Intuitively, this approach cannot identify similar peer samples accurately and thus would lead to inferior performance. Nonetheless, as shown in Table 2, even with this naive uniform sampling strategy, M\({}^{3}\)-Uniform still outperforms the two leading imputation baselines.

    & Yacht & Wine & Concrete & Housing & Energy & Naval & Kin8nm & Power \\  HyperImpute & 1.76 \(\) 0.03 & 0.67 \(\) 01 & 0.84 \(\) 02 & 0.82 \(\) 01 & 1.32 \(\) 02 & **0.04**\(\) 00 & 2.58 \(\) 05 & 1.06 \(\) 01 \\ Grape & 1.46 \(\) 01 & **0.60**\(\) 00 & 0.75 \(\) 01 & 0.64 \(\) 01 & 1.36 \(\) 01 & 0.07 \(\) 00 & **2.50**\(\) 00 & 1.00 \(\) 00 \\  Architecture & & & & & & & & \\  Init Only & 1.43 \(\) 01 & **0.60**\(\) 00 & 0.74 \(\) 00 & 0.63 \(\) 01 & 1.35 \(\) 01 & 0.06 \(\) 00 & **2.50**\(\) 00 & **0.99**\(\) 00 \\ Init+**FCU** & 1.35 \(\) 01 & 0.61 \(\) 00 & 0.72 \(\) 03 & 0.61 \(\) 02 & 1.32 \(\) 00 & 0.07 \(\) 01 & **2.50**\(\) 00 & **0.99**\(\) 00 \\ Init+**SCU** & 1.37 \(\) 01 & **0.60**\(\) 00 & 0.73 \(\) 00 & 0.63 \(\) 01 & **1.30**\(\) 00 & 0.09 \(\) 01 & **2.50**\(\) 00 & 1.00 \(\) 00 \\ M\({}^{3}\)-Impute & **1.33**\(\) 04 & **0.60**\(\) 00 & **0.71**\(\) 01 & **0.60**\(\) 00 & 1.32 \(\) 01 & 0.06 \(\) 00 & **2.50**\(\) 00 & **0.99**\(\) 00 \\  Sampling Strategy & & & & & & & & \\  M\({}^{3}\)-Uniform & 1.34 \(\) 01 & **0.60**\(\) 00 & 0.73 \(\) 01 & 0.61 \(\) 00 & 1.31 \(\) 00 & 0.06 \(\) 00 & **2.50**\(\) 00 & **0.99**\(\) 00 \\   

Table 2: Ablation study. M\({}^{3}\)-Uniform stands for M\({}^{3}\)-Impute with the uniform sampling strategy.

[MISSING_PAGE_FAIL:9]