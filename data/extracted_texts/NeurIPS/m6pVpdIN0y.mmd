# Neglected Hessian component explains mysteries in sharpness regularization

Yann N. Dauphin

Google DeepMind

ynd@google.com &Atish Agarwala

Google DeepMind

thetish@google.com &Hossein Mobahi

Google DeepMind

hmobahi@google.com

###### Abstract

Recent work has shown that methods that regularize second order information like SAM can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We investigate this inconsistency and reveal its connection to the the structure of the Hessian of the loss. Specifically, its decomposition into the positive semidefinite Gauss-Newton matrix and an indefinite matrix, which we call the Nonlinear Modeling Error (NME) matrix. Previous studies have largely overlooked the significance of the NME in their analysis for various reasons. However, we provide empirical and theoretical evidence that the NME is important to the performance of gradient penalties and explains their sensitivity to activation functions. We also provide evidence that the difference in regularization performance between gradient penalties and weight noise can be explained by the NME. Our findings emphasize the necessity of considering the NME in both experimental design and theoretical analysis for sharpness regularization.

## 1 Introduction

There is a long history in machine learning of trying to use information about loss landscape geometry to improve gradient-based learning. This has ranged from attempts to use the Fisher information matrix to improve optimization , to trying to regularize the Hessian to improve generalization . More recently, first order methods which implicitly use or penalize second order quantities have been used successfully, including the _sharpness aware minimization_ (SAM) algorithm . On the other hand, there are many approaches to use second order information which once seemed promising but have had limited success . These include methods like weight noise  and gradient norm penalties , which have shown mixed success.

Part of the difficulty of using second order information is the difficulty of working with the Hessian of the loss. With the large number of parameters in deep learning architectures, as well as the large number of datapoints, many algorithms use stochastic methods to approximate statistics of the Hessian . However, there is a _conceptual_ difficulty as well which arises from the complicated structure of the Hessian itself. Methods often involve approximating the Hessian via the Gauss-Newton (GN) matrix - which is PSD for convex losses. This is beneficial for conditioners which try to maintain monotonicity of gradient flow via a PSD transformation of the gradient. Thus indefinite part of the Hessian is often neglected due to its complexity.

In this work we show that it is important to consider _both_ parts of the Hessian to understand certain methods that use second order information for regularization. We study the non-PSD part of the Hessian, which we call the _Nonlinear Modeling Error_ (NME). In contrast to commonly held assumptions, this work reveals the NME is key to understanding two previously unexplained phenomena in sharpness regularization:* **Training with Gradient Penalties.** We show that the performance of gradient penalties is sensitive to the choice of activation functions. Our theoretical analysis reveals that the NME is particularly sensitive to the second derivative of the activation function, and we show that gradient penalties fail to improve performance when these derivatives have poor numerical properties. We also design an intervention that can mitigate this issue. To the best of our knowledge, this work is the first to show that methods using second order information are more sensitive to the choice of activation function.
* **Training with Hessian penalties.** Conventional analysis of weight noise casts it as a penalty on the GN part of the Hessian, but in reality it also penalizes the NME. Our experimental ablations show that the NME exerts a significant influence on generalization performance, and minimizing it is generally bad for training.

We conclude with a discussion about how these insights might be used to design activation functions not with an eye towards forward or backwards passes [12; 13], but for compatibility with methods that use second order information.

## 2 Understanding the structure of the Hessian

In this section, we lay the theoretical ground work for our experiments by explaining the structure of the Hessian. Given a model \((,)\) defined on parameters \(\) and input \(\), and a loss function \((,)\) on the model outputs and labels \(\), we can write the gradient of the training loss with respect to \(\) as

\[_{}=^{}(_{} )\] (1)

where the Jacobian \(_{}\). The Hessian \(_{}^{2}\) can be decomposed as:

\[_{}^{2}=^{}_{}}_{}+} _{}^{2}}_{}\] (2)

where \(_{}_{}^{2}\). The first term, often referred to as the Gauss-Newton (GN) part of the Hessian, is a generalization of the classical Gauss-Newton matrix [14; 15]. If the loss function is convex with respect to the model outputs/logits (such as for MSE and CE losses), then the GN matrix is positive semi-definite. This term often contributes large eigenvalues. The second term has previously been studied theoretically where it is called the _functional Hessian_[16; 17]; in order to avoid confusion with the overall Hessian we call it the _Nonlinear Modeling Error_ matrix (NME). It is in general indefinite and vanishes to zero at an interpolating minimum \(^{*}\) where the model "fits"the data (\(_{z}(^{*})=\)), as can happen in overparameterized settings. Due to this, it is quite common for studies to drop this term entirely when dealing with the Hessian. For example, many second order optimizers approximate the Hessian \(_{}^{2}\) with only the Gauss-Newton term [11; 18]. It is also common to drop this term in theoretical analysis of the Hessian \(_{}^{2}\)[19; 20]. However, we will show why this term should not be ignored.

While the NME term can become small late in training, it encodes significant information during training. More precisely, _it is the only part of Hessian that contains second order information from the model features \(_{}^{2}\)_. The GN matrix only contains second order information about the loss w.r.t. the logits with the term \(_{}\). All the information about the model function in the GN matrix is first-order. In fact, the GN matrix can be seen as the Hessian of an approximation of the loss where a first-order approximation of the model \((^{},)(, )+\) (\(=^{}-\)) is used 

\[_{}^{2}((,)+ ,\ )|_{^{}=}=^{} _{}\] (3)

Thus we can see the GN matrix as the result of a linearization of the model and the NME as the part that takes into account the non-linear part of the model. The GN matrix exactly determines the linearized (NTK) dynamics of training, and therefore controls learning over small parameter changes when the features can be approximated as fixed (see Appendix A.1). In contrast, the NME encodes information about the _changes_ in the NTK . For additional intuition in the ReLU setting, see Appendix A.3.

### Sharpness regularization and the NME: Case of the Gauss-Newton trace

Sharpness regularization often relies on geometric quantities of the loss landscape such as the largest eigenvalue  or combinations of the eigenvalues . To illustrate the impact of the NME, let us consider the sharpness measure given by the trace of the Gauss-Newton \(()=_{i}_{i}\), which gives information about the average eigenvalue \(_{i}\) of the Gauss-Newton. This measure, studied in Section 5, also shares some similarities to Section 4's gradient penalties but is simpler to analyze. Surprisingly, even though the quantity purposely ignores the NME, we will see that its gradient still crucially relies on it.

For GN matrix \(\), if the loss can be written as the log-likelihood of an exponential family distribution, this measure can be expressed as a gradient penalty :

\[()=_{} ()}[\|_{}(,})\|^{2}]\] (4)

Here, the expectation is taken over labels \(}\) sampled from \(()\) - that is, the probability distribution induced by the model logits \(()\) via the log-likelihood. \(_{}(,})\) is the gradient of this loss defined with the sampled labels; derivatives are taken after the sampling process. For MSE loss, the gradient of \(()\) can be written as:

\[_{}()= _{}()}[_{}}_{}(,})]\] (5)

where \(_{}}\) is NME of the loss defined with the sampled labels. In this case if the NME is set to \(0\), gradient descent could not effectively minimize this sharpness measure. The rest of the work will explore the relationship between the structure of the Hessian and various curvature regularization techniques beyond this case through experimental and theoretical work.

## 3 Experimental Setup

Our analysis of the Hessian begs an immediate question: when does the NME affect learning algorithms? We conducted experimental studies to answer this question in the context of curvature regularization algorithms which seek to promote convergence to flat areas of the loss landscape. We use the following setups for the remainder of the paper:

**Fashion MNIST** We also include results on Fashion MNIST . All experiments use the WideResnet 28-10 architecture with the same setup and hyperparameters as those used in our CIFAR-10 experiments.

**CIFAR-10** We provide results on the CIFAR-10 dataset . All experiments use the WideResnet 28-10 architecture with the same setup and hyperparameters as , except for the use of cosine learning rate decay. Batch size is \(128\). Models are trained on 8 Nvidia Volta GPUs.

**Imagenet** We conduct experiments on the popular Imagenet dataset . All experiments use the Resnet-50 architecture with the same setup and hyperparameters as , except that we use cosine learning rate decay  over 350 epochs. Batch size is set to \(1024\). Models are trained on using on TPU V3 chips.

## 4 Explaining the pitfalls of gradient penalties

In this section we explore the sensitivity of gradient penalty regularizers to the activation function via the NME. We first establish gradient penalty regularizers as an approximation of the Sharpness Aware Minimization (SAM) learning rule, and then present a mystery: why do gradient penalties recover the benefits of SAM for GELU and not for ReLU? We resolve the mystery with a theoretical analysis of the NME, combined with a series of ablation studies and design a simple intervention which alleviates the issue.

### Sam and gradient penalties

The SAM algorithm originates from seeking a minimum with a _uniformly low loss_ in its neighborhood (hence flat). This is formulated in Foret et al.  as a minmax problem,

\[_{}_{}(+)\|\| \,.\] (6)

For computational tractability,  approximates the inner optimization by linearizing \(\) w.r.t. \(\) around the origin. Plugging the optimal \(\) into the objective function yields

\[_{}+\,}()}{\|_{ }()\|}\,.\] (7)The SAM algorithm approximately minimizes the objective with the following learning rule:

\[-\,_{} (+}),\;}_ {}()/||_{}()||\] (8)

for some step-size parameter \(>0\).

For small \(\), there is an alternative to the SAM rule. We may approximate \(\) in (7) by its first order Taylor expansion around the point \(=0\) as below.

\[_{}() \] (9) \[= ()+_{} (),\;}() }{||_{}()||}+O(^{2})\] (10) \[= ()+\|_{} ()\|+O(^{2})\,.\] (11)

Dropping terms of \(O(^{2})\) we arrive at a _gradient penalty regularizer_. In general we can define gradient penalties as additive regularizers of the form

\[_{pen,p}=||_{0}||^{p}\] (12)

for a base loss \(_{0}\). Gradient penalties have recently gained popularity as regularizers [6; 7; 8; 9; 10]. We will focus on the \(p=1\) case in the remainder of the section which we will refer to as Penalty SAM (or PSAM for short).

### Penalty SAM vs Original SAM

A natural question arises: when do SAM and PSAM have similar effects? We find, surprisingly, that the answer is highly dependent on the activation function of the architecture. On Resnet50 trained on Imagenet, networks trained with GELU activation show similar performance for SAM training and PSAM training (Figure 0(a)); in contrast, with ReLU activation PSAM performs significantly worse than SAM as \(\) is increased - indeed, becoming worse than the baseline for the best values of \(\) for SAM (Figure 0(b)).

In order to understand this difference, we must first look at the update rule for PSAM. Given the base, pre-regularized loss \(_{0}\), the update to the parameters \(_{t}\) under SGD can be written as:

\[_{t+1}-_{t}=-(_{} _{0}+}_{0}||}_{ {}}_{0})\] (13)

where \(_{}^{2}_{0}\). Therefore, PSAM captures curvature information via an explicit Hessian-gradient product. This is in contrast to SAM, which captures curvature information by evaluating the gradient at the alternative point in the \(}\) direction (Equation 8). This lets SAM effectively "integrate" over in this direction and benefit from higher order information - in contrast to PSAM which only has access to the Hessian. The natural followup question is: how does the activation function influence the Hessian? In the remainder of this section, we provide evidence that it is in fact the NME component that is sensitive to this choice - and in general, it is the most important term to the overall performance of PSAM.

### Effect of Activation functions on the NME

One important feature of the NME is that it depends on the _second derivative_ of the activation function. We can demonstrate this most easily on a fully-connected network, but the general principle applies to most common architectures. Given an activation function \(\), a feedforward network with \(L\) layers on an input \(_{0}\) defined iteratively by

\[_{l}=_{l}_{l},\;_{l+1}=( _{l})\] (14)

The gradient of the model output \(_{L}\) with respect to a weight matrix \(_{l}\) is given by

\[_{L}}{_{l}}=_{L(l+1)} ^{}(_{l})_{l},\;_{l^{ }l}_{m=l}^{l^{}-1}^{}(_{m}) _{m},\;^{}(x)(x)\]where \(\) is the Hadamard (elementwise) product. The second derivative can be written as:

\[_{L}}{_{l}_{m}}= [_{L(l+1)}}{_{m}}^{ }(_{l})+.._{L(l+1)}(_{l})}{_{m}}] _{l}\] (15)

where without loss of generality \(m l\). The full analysis of this derivative can be found in Appendix A.4. The key feature is that the majority of the terms have a factor of the form

\[(_{o})}{_{m}}=^{ }(_{o})_{o}}{ _{m}}\,,\;^{}(x)}{dx^{2}}(x)\] (16)

via the product rule - a dependence on the second derivative \(^{}\) of the activation function. On the diagonal \(m=l\), all the terms depend on \(^{}\). We note that a similar analysis can be found in Section 8.1.2 of .

We immediately see how the role of the activation function differs in the NME compared to the gradient or even the GN: only the NME is sensitive to the second derivatives of the activation. GELU has a numerically stable second derivative function:

\[}{dx^{2}}(x)=}e^{-x^{2}/2}[2- x^{2}]\] (17)

Therefore, there are no issues computing the NME for GELU networks.

In contrast, the second derivative of ReLU is \(0\) everywhere except the origin, where it is undefined. In theoretical settings, the ReLU second derivative is defined as the Dirac delta "function" - more formally, the measure attained by a sequence of Gaussians centered at zero, as the standard deviation \( 0\). It is integrable to \(1\) despite not attaining non-zero value anywhere. Therefore ReLU does not have a numerically well-posed second derivative, which affects computations involving the NME.

### Ablating activation NME explains the gap

Figure 1: Test Accuracy as \(\) increases across different datasets and activation functions averaged over 5 seeds. PSAM with GELU networks more closely follows the behavior of SAM. For ReLU networks and large \(\), there is a significant difference between PSAM and SAM.

Figure 2: Test accuracy as \(\) increases for penalty SAM with parts of the NME ablated (average of \(5\) seeds). The removal of information from the NME controls the effectiveness of the gradient penalty.

In practical settings ReLU suffers from a "missing curvature" phenomenology: the second derivative is set to \(0\) in most autodifferentiating frameworks. This means that much of the information in the NME is inaccessible through the Hessian-vector product that at the heart of the update rule in Equation 13. Therefore PSAM suffers from a mismatch between the true NME information and the information available to the implemented algorithm. In contrast, SAM does not suffer from this issue; any curvature information is gained via differences in first derivatives - which by the fundamental theorem of calculus, are equivalent to integrals of second derivatives (and therefore of NME information).

We can confirm that the missing second derivative information in Equation 16 for ReLU networks is key for gradient penalties by removing it in GELU networks, and performing training with PSAM. This can be done by taking the second derivative of the GeLU to be zero:

\[}{dx^{2}}}(x):=0\] (18)

and leaving the forward and backward propagation of the activation intact (see Appendix C.2 for implementation). This removes terms related to the activation in the same way ReLU does, so we denote as GeLU ablating activation NME for brevity. We see that across all 3 datasets in Figure 2 removing this portion of the NME degrades the performance of the GeLU with \(\) similar to that of the ReLU, even though only the NME has been affected.

We can see similar results when we use a different method to approximate ReLU with GELU. The activation function \(( x)/\) converges uniformly to ReLU as \(\), while having well-posed derivatives for any finite \(\). We show in Appendix B.3 that gradient penalty performance degrades for large \(>10^{3}\) - which we tie to high input sensitivity and increased sparsity of the second derivative. This suggests that even differentiable ReLU-like activation functions can fail to have compatibility with gradient penalties. We also confirm in Appendix C that ablating the full NME is also detrimental to generalization, though in a different way than ablating in the same manner as ReLU.

### Adding synthetic activation NME improves results

Using this insight, we can modify the ReLU activation function to improve performance as follows. The issue with the ReLU is that the second derivative is a delta function - which can't be implemented numerically. However, we know that the delta function itself can be approximated by a Gaussian distribution. Therefore we replace the second derivative of ReLU with such a Gaussian

\[}{dx^{2}}}(x):=}e^{- ^{2}x^{2}/2}\] (19)

where \(\) is the kernel width. We see that this prevents the drastic degradation at \(=0.1\) of the original ReLU (Figure 3, \(=100\)). This suggests that it may be possible to design interventions to approximately access NME information in cases where second derivatives have poor numerical properties.

## 5 Explaining the pitfalls of Hessian penalties

In this section, we explore the impact of the NME on the effectiveness of Hessian penalties like weight noise. We first review the previously claimed link between gradient penalties and weight noise

Figure 3: Test accuracy as \(\) increases for penalty SAM with parts of the NME replaced synthetically (average of \(5\) seeds). The addition of information to the NME improves the performance as \(\) increases.

, and then present a mystery: if this link is correct, why is there a difference in regularization boost between gradient penalties and weight noise? In contrast to the previous section where the NME solely featured in the update rule, weight noise implicitly minimizes the NME. We will show through ablations that minimizing the NME is detrimental, and explain why NME minimization is a poor strategy.

### Weight noise equivalence assumes zero NME

We first review the analysis of training with noise established by . Though the paper considers input noise, the same analysis can be applied to weight noise. Adding Gaussian \((0,^{2})\) noise with strength hyper-parameter \(\) to the parameters can be approximated to second order by

\[_{}[(+ )]&()+_{ }[}]+ _{}[^{}] \\ &=()+^{2}()\] (20)

where the second term has zero expectation since \(\) is mean \(0\), and the third term is a variation of the Hutchison trace estimator . (We note that though the second term vanishes in expectation, it still can have large effects on the training dynamics.)  argues that we can simplify the term related to the Hessian by dropping the NME in Equation 2 for the purposes of minimization, which in combination with \(4\) yields

\[()()=_{ }(})}[\|_{} (,})\|^{2}]\]

The argument is that for the purposes of training neural networks, the NME can be dropped because it is zero at the global minimum we are trying to reach. However, the hypothesis that the NME has negligible impact in this setting has not been experimentally verified and we address this gap in the next section.

### Minimizing the NME is detrimental

In order to study the impact of the NME in this setting, we evaluate ablations of weight noise to determine the impact of the different components. Recalling Equation 20, the methods we will consider are given by

\[_{}[(+) ]}_{}=()+^{2} ()}^{}+^{2} ()}^{}+(\|\|^{2})\]

where \(^{}_{}\) is the Gauss-Newton, \(_{}_{}^{2} \) is the NME.

Hessian Trace penaltyThis ablation allows to us to single out the second order effect of weight noise, as it's possible the higher order terms from weight noise affect generalization. We implement this penalty with Hutchinson's trace estimator (\(()=_{(0,1)}[^{T}]\)).

Gauss-Newton Trace penaltyThis ablation removes the NME's contribution, enabling us to isolate and measure its specific influence on the model. We use the estimator from Equation 4, \(()=_{}( })}[\|_{}(,})\|^{2}]\), to compute the penalty. This is the norm of the gradients, with respect to labels \(}\) sampled via the distribution \(()\) induced by the model logits via the softmax . This is an alternative gradient penalty, a point we will return to later. We do not pass gradients through the sampling of the labels \(}\), but find similar results if we pass gradients using the straight-through estimator . We also run experiments with the Hutchison's estimator \(()=_{(0,1)}[^{T}]\) to control for the effect of the estimator.

Our experiments show that the methods perform quite differently for similar values of \(^{2}\) (Figure 4) - confirming the influence of the NME. We can see that the generalization improvement of the Gauss-Newton Trace penalty is consistently greater than either weight noise or Hessian Trace penalty. Its improvement on Imagenet is a significant \(1.6\%\). In contrast, the other methods provide little accuracy improvement. While these experiments use different estimators with a single sample, results are consistent even when compared with the same estimator and with 5 samples to get a better trace estimate (Figure 5).

In fact, both the weight noise and Hessian trace penalties show severe performance degradation for larger \(^{2}\) - with the Hessian trace penalty in particular showing degradation as low as \(10^{-6}\), at least two orders of magnitude lower than the optimal GN trace regularizer values \( 10^{-4}\). This may be related to the fact that the full Hessian trace has no lower bound as it includes the indefinite NME, while the GN is a PSD matrix whose trace is bounded by \(0\). Measurements of the trace during training shows that indeed the trace grows large and negative superlinearly in the number of training iterations for the Hessian trace penalty (Figure 6, \(^{2}=10^{-3}\)).

Our experiments suggest that the Gauss-Newton trace is a better (and indeed, qualitatively different) regularizer than the full Hessian trace penalty. As computed in Section 2.1, this regularizer is a gradient penalty whose gradient relies crucially on the information in the NME. These results indicate that minimizing the NME in the loss is counter-productive. Similarly, Section 4.4 shows that zeroing out portions of the NME is also detrimental. This suggests that the NME is important in understanding the effects of curvature regularization.

Figure 4: Test Accuracy as \(^{2}\) increases across different datasets and activation functions averaged over \(5\) seeds. Large \(^{2}\) reveals a stark contrast between the Gauss-Newton trace penalty, which excludes NME, and methods incorporating it, highlighting the NME’s influence.

Figure 5: Test Accuracy as \(^{2}\) increases with both penalties estimated using Hutchinson’s estimator with 5 samples (curves are averaged over \(2\) seeds). Despite the larger number of samples, Hessian trace penalty remains unstable, while Gauss-Newton trace penalty is stable. This suggests the instability is not due the estimator.

Figure 6: Trace penalty over training iterations for different methods averaged over 5 seeds. The trace of the Hessian can be negative due to the NME, while the trace of the Gauss-Newton cannot. Minimizing the Hessian trace causes it to become very negative, which is detrimental to training stability.

Discussion

Our theoretical analysis gives some understanding of the structure of the Hessian - in particular, the Nonlinear Modeling Error matrix. This piece of the Hessian is often neglected as it is generally indefinite and doesn't generate large eigenvalues, and is \(0\) at an interpolating minimum. However, the NME is the only part of the Hessian that encodes important second order information about the features, as it depends on \(_{}^{2}\) - the gradient of the Jacobian. Another intriguing observation was that the gradient of the trace of the Gauss Newton matrix \(_{}()\) can be written in terms of an NME-vector product. We also saw that the NME, particularly the diagonal, is sensitive to the second derivative of the activation function.

Our experiments suggest that these second derivative properties can be quite important when training with gradient penalty regularizers. ReLU has a poorly defined pointwise second derivative, and the resulting regularizer harms training. In contrast, GELU has a well defined one and gains benefits from modest values of the regularizer. Our ablation experiments showed that removing the second derivatives prevented gradient penalties from usefully using the NME information. We also found an alternative approximation for ReLU second derivatives which added NME information and improved training.

These results suggest that some second order methods may benefit from tuning the NME. This is especially true for methods which result in Hessian-vector products in update rules (like the gradient and Hessian trace penalties studied here). Another interesting avenue for research is to replace explicit second order methods with implicit second order methods which use first order information at discrete intervals - analogous to how SAM avoided sensitivity to bad second derivatives by "integrating" over a direction via differences, which accessed averaged quantities and higher order information.

Our experiments on Hessian trace penalties confirmed that the NME is important to understanding the successes and failures of those methods. It is intriguing that the variant which performed best, the GN trace penalty, can itself be written as an alternative gradient penalty. Exploring other gradient penalties is a promising research direction; they are non-negative, easy to compute, and generally contain NME information in their update rules.

## 7 Conclusion

Our work sheds light on the complexities of using second order information in deep learning. We have identified clear cases where it is important to consider the effects of _both_ the Gauss-Newton and Nonlinear Modeling Error terms, and design algorithms and architectures with that in mind. This insight may unlock new classes of second order algorithms which use loss landscape geometry in qualitatively different ways.