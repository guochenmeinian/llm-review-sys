# Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models

Andy Zhou

1University of Illinois at Urbana-Champaign

3Microsoft Research

3AI@UIUC

3

Jindong Wang

Microsoft Research

2Microsoft Research

2Microsoft Research

2

Yu-Xiong Wang

University of Illinois at Urbana-Champaign

3AI@UIUC

3

Haohan Wang

University of Illinois at Urbana-Champaign

1University of Illinois at Urbana-Champaign

1

###### Abstract

We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmentations for further improvements.

## 1 Introduction

One of the goals of machine learning is to develop systems that can generalize effectively across diverse populations and environments, much like human intelligence. Despite the impressive advancements in neural networks that surpass human performance in various tasks in computer vision, their generalization capabilities remain inadequate when faced with out-of-distribution data, such as adversarial perturbations , unusual colors and textures , or challenging contexts .

One major line of research addresses this issue with more sophisticated training strategies , including adversarial training , data augmentation , or other regularizations . In this paper, we focus on adversarial-training-based data augmentation , which can enhance the quantity and diversity of training data. In addition, theoretical work suggests achieving high robustness requires significantly more samples than clean accuracy . This has also been shown empirically , most recently with transformers  which achieve robustness on a variety of computer vision tasks. In addition to weak inductive bias, powerful model capacity, and grounding with language, these models are often trained with large-scale datasets , up to billions of images  that encompass many real-world distribution shifts. As a result, these foundation models  exhibit remarkable zero-shot generalization, especially on natural distribution shifts such as artistic renderings, but require large amounts of compute and heavily parameterized models.

In this paper, we aim to connect these two lines of work. We investigate if it is possible to improve robustness by introducing a foundation model as a teacher to distill robust representations and help generate a diverse data augmentation. We conduct our analysis without requiring the teacher's large-scale dataset and focus on out-of-distribution robustness by introducing an image-to-image generative model to discretize optimized perturbations. We aim to leverage in-distribution data to a greaterextent and conduct our investigation with CLIP . This marks a departure from existing work in knowledge distillation (KD), which tends to focus on smaller models and datasets. In fact, prior work  has called into question the utility of distilling from stronger teachers over training from scratch altogether. However, we find that although this _model capacity gap_ can impair improvements in clean accuracy, distilling from robust teachers improves out-of-distribution robustness, even when leveraging only in-distribution data. Surprisingly, distilling on clean ImageNet images from CLIP with the original KD objective  results in a more robust ResNet34 than training with state-of-the-art regularization methods, despite a parameter difference of \(\)13.7x.

However, it is currently unclear in what settings a teacher's robustness can reliably transfer to a student and how to best combine distillation with data augmentation. We aim to answer this question both theoretically and empirically. We view adversarial training and data augmentation in the lens of domain generalization and prove that the diversity of the augmented samples leads to improved robustness. Our findings further suggest that foundation models make for strong teachers due to their diverse training distribution.

Building upon these findings, we introduce _discrete adversarial distillation (DAD)_, a KD framework that further distills the robustness of a teacher model by leveraging the adversarial examples of the _teacher_ discretized by a VQGAN  as data augmentation. Notably, these samples are generated in an offline fashion, adding minor computational overhead compared to standard adversarial training. Intuitively, a foundation model will produce more diverse adversarial samples than a teacher trained on the same distribution, and we provide a theoretical framework using Wasserstein distance to formalize this proposition. Empirically, when distilling CLIP to a ViT-B, we achieve robust accuracy of 46.1% on ImageNet-Sketch  and 65.1% on ImageNet-Rendition , improving on the state of the art by 17.8% and 11.3% respectively. DAD can also be freely combined with existing regularization techniques, resulting in improvements in clean accuracy. In summary, our contributions are 1

1. Establishing the KD for out-of-distribution robustness setting and a proposing a novel KD objective based on data augmentation
2. Providing a theoretical framework in KD for the choice of a teacher based on the diversity of the data augmentation
3. Proposing a novel data augmentation DAD that outperforms both adversarial training and distillation techniques on natural distribution shifts

## 2 Related Work

We define _out-of-distribution accuracy/robustness_ as a model's performance on non-adversarial distribution shifts, _adversarial accuracy/robustness_ to the case of robustness of adversarial examples, and _clean accuracy_ as evaluation on a dataset drawn from the same distribution.

**Data augmentation.** Data augmentation is frequently used as regularization  by expanding the quantity and diversity of training data. This is often achieved through simple transformations such as rotations or image crops or more advanced techniques such as image mixing [79; 28], reinforcement learning [10; 81] or adversarial training [21; 31; 41] to find the optimal transformation.

Adversarial training (AT) was initially introduced to enhance model robustness by training with adversarial examples . Although effective for defending against adversarial attacks, several works [64; 78; 70] have indicated a trade-off between adversarial clean accuracy in AT, limiting its effectiveness as a general data augmentation. Considerable efforts [47; 49] have been made to minimize this trade-off and directly use adversarial examples as data augmentation [50; 74], but there is still a considerable gap in out-of-distribution performance compared to foundation models like CLIP .

One line of work has recently been adapted to this issue. The model-based robustness paradigm [4; 52] leverages the disentangled latent representations of pretrained generative models to improve or validate out-of-distribution robustness , and can be used to improve adversarial examples. Most similar to our work is [23; 2; 41], which use a GAN or VAE [15; 34; 67] to discretize or discover adversarial examples. However, we leverage both a pretrained discretizer and foundation model, and adapt the AT objective to a knowledge distillation setting.

**Knowledge distillation.** Knowledge Distillation (KD) is a technique for training a student model with guidance from a stronger teacher model, widely applied in vision and language tasks [6; 32; 55; 71]. Most works focus on improving the KD objective with different knowledge transfer objectives, such as feature distance [7; 54; 63], attention , distribution , activation boundary , and sample distance [38; 44; 66].  raises the model capacity gap issue, where training suffers when the size of the student and teacher models differ, but we find that there is still benefit to distilling from a robust teacher. Another line of work, defensive distillation, aims to distill adversarial robustness from an adversarially trained teacher [20; 82; 83]. We have a similar goal, but for out-of-distribution robustness and propose a loss objective not previously explored in prior works.

## 3 Method

We consider a standard dataset \(\{(x,y)\}_{n=1}^{N} P^{N}\) where instances and their labels \((x_{n},y_{n})\) are drawn from a distribution \(P\) and are used for training the student model \(\). We also consider a discretizer, \(Q\), and a teacher model \(\) and we use \((x)\) to denote the output of a model given the sample \(x\). \(a\) denotes a function that applies a data augmentation on \(x\), also known as a transformation function. Additionally, \(a A\), the class of all such functions.

### Setup

Invariance is a desirable property where the model will have the same representation of an input after a transformation is applied. A model, \(\) is said to be invariant if \((x)=(x^{})\) for all \(x U\), where \(U\) is the set of all images that can be obtained by a transformation of \(x\) by \(a A\), which includes the identity transformation. \(a\) ranges from worst-case imperceivable perturbations to real-world distribution shifts like artistic sketches . In this paper, we focus on the latter, denoted as \(\) and \(\). We can consider \(\) to represent an individual distribution \(P\) and \(\) to be drawn from the distribution of distributions \(\). Our ultimate goal is to train \(\) to be invariant to transformations in \(\). A model that maintains the same representation under transformations or distribution shifts of \(x\) is said to be robust, which we define as the worst-case expected risk where

\[r_{}(P,)=_{P^{}:w(P^{},P)} _{(x,y) P^{}}\;l((x),y), \]

where \(l\) is the loss function and \(r\) depends on an anchor distribution \(P\), and \(\), the maximum deviation allowed under the Wasserstein's distance metric. Similarly, we can define the expected robustness and expected risk in terms of an arbitrary distribution, including the training distribution.

\[r(P,)=_{P^{}:w(P^{},P)} _{(x,y) P^{}}l((x),y), \]

\[r(P)=_{(x,y) P}\;l((x),y). \]

The robustness of the resulting model is highly dependent on \(x^{}\), \(P\), and the choice of data augmentation. It is also susceptible to adversarial attacks, where \(x^{}\) is a worst-case perturbation of \(x\). Adversarial robustness can be improved with adversarial training, which couples the outer minimization objective from (3) with an inner maximization objective in the following

\[\;_{(x,y) P}\;[l((x),y)+\;l((x^{}),y)], \]

where \(x^{}=x+\), \(\) is the perturbation, and \(l\) is the cross-entropy loss. This achieves adversarial robustness, but cannot generalize well to real-world domain shifts in \(\). To address this, we consider a generative model, \(Q\), trained on \(P\) and can model \(\). Passing an input through \(Q\) in the maximization objective applies a worst-case transformation from \(\). This modifies (4) to minimize the empirical semantic adversarial risk,

\[\;_{(x,y) P}\;[l((x),y)+\;l((Q(x^{})), y)]. \]

### Distillation from a robust teacher

Next, we consider a setting where we also have access to a pretrained \(\) invariant to distribution shifts in \(\). This enables us to leverage knowledge distillation (KD). This modifies (3) in the following

\[\;_{(x,y) P}[l_{1}((x),y)+l_{2}((x),(x))], \]here \(l_{1}\) is the classification loss, the cross-entropy loss, and \(l_{2}\) is a distance loss between \((x)\) and \((x)\), the KL divergence. (6) can be approximated by the empirical risk where

\[\;l_{1}((x),y)+l_{2}((x),(x)). \]

Following theoretical work , distilling from a robust teacher with \(l_{2}\) improves generalization due to minimizing the population risk, which has lower variance. In this formulation, the output of the teacher acts as a more robust supervisory signal than the label, encompassing the probability distribution of classes. This allows the student to learn the teacher representations on in-distribution data, but our experiments show that this is inadequate for out-of-distribution robustness, even when using a robust teacher. To address this, we combine (5) and (7) to also distill the representations of \(\) on augmented samples,

\[\;l_{1}((x),y)+l_{2}((x),(x))+l_{2}((Q(x^{})), (Q(x^{}))). \]

The teacher is more robust, and is able to "solve" the perturbation for the student through distillation. Like  and , we train our models with both the original and augmented samples, expanding the size of the dataset and maintaining the original information path for \(x\). We do not use the cross-entropy loss or \(y\) labels for \(x^{}\), as these labels may be wrong and could limit the expressiveness of the data augmentation. This allows us to use adversarial samples of the teacher in a novel maximization objective and obtain stronger empirical results.

### Discrete Adversarial Distillation

The goal of our method, discrete adversarial distillation (DAD), is to distill from a large-scale vision-language model using only ImageNet  data. In the practical setting, we use approximations of an ideal discretizer and robust teacher. For \(\) we use CLIP , which was trained on a large-scale dataset and achieves impressive zero-shot generalization on a variety of natural distribution shifts.

For \(Q\), we use a pretrained VQGAN , following , which also finds minimal improvements with a stronger discretizer. The VQGAN consists of an encoder, decoder, and quantization, where the encoder learns a latent representation of the input \(x\), the quantization maps the representation to a visual codebook entry, and the decoder reconstructs \(x\). We denote this process as \(Q(x)\). In the adversarial training setting, \(Q\) discretizes \(x\), a worst-case perturbation \(\) is added by approximating the maximization objective to obtain \(x^{}\), and the resulting image is then discretized by \(Q\) again. To improve the transfer of robustness from the teacher, we hypothesize a better augmentation is more informative and exposes more teacher knowledge. We make this rigorous in the following section.

To generate these examples, we adopt adversarial training and modify the maximization objective of (5) to use the worst-case transformations of the teacher. We hypothesize a teacher trained on more diverse data will have more informative adversarial examples. To ensure the correctness of the perturbation, we only use samples that are still classified correctly by the teacher after generation. We use the teacher as an "oracle", allowing it to distill correct representations of the transformed image. Additionally, we generate these samples in an offline manner asynchronously from the pretrained teacher and add them to the original dataset during the training of the student. These examples

Figure 1: The overall framework of discrete adversarial distillation (DAD). We leverage a foundation model to generate and distill adversarial examples after discretization by a VQGAN.

only need to be generated once for each teacher and be reused as additional data. This adds minor additional training costs compared to online adversarial training or DAT , which has 11x and 3.5x the cost of standard training, respectively . Our full objective is described as,

\[\;l_{1}((x),y)+l_{2}((x),(x))+l_{2}((Q(x^{})), (Q(x^{}))). \]

\[x^{}=_{||x^{}-x||_{p} t}l_{1}((x),y),\; (Q(x^{}))=y.\]

### Theoretical Investigation

We aim to investigate how to best distill robustness from a robust teacher trained on a large-scale dataset. We find that robust performance can be connected to the distance between the training and test distributions. A data augmentation can represent a new distribution, and the robustness of a model trained on this distribution can be quantified by its diversity and closeness to the test distributions. Although its representation on in-distribution samples can distill a degree of robustness, we show that due to being closer to the test distribution, it is more effective to leverage the discretized adversarial examples of the teacher than the student as our data augmentation of choice.

We begin with some assumptions.

**Assumption 1**.: _For an arbitrary data pair \((x,y)\), transformations in \(\) do not alter the semantics of the data. We can also say we consider an ideal labeling function where any \((x,y)\) pair can be correctly mapped, \(y=f(x)\)_

**Assumption 2**.: _Any arbitrary distributions \(P\) and \(P^{}\) we compare possess smooth probability densities controlled by two constants \(c\) and \(\) depending on the smoothness of \(P\) and \(P^{}\) where \(cw(P,P^{})^{}\) and \(c>0\) and is only dependent on \(\)._

**Assumption 3**.: _For function \((||,n,)\) parameterized by hypothesis space \(||\), number of samples \(n\), and the probability when the bound holds \(\), if the samples are i.i.d, \((||,n,)=2()+\), where \(()\) stands for Rademacher complexity and \(=\{l_{}\,|\,\}\), where \(l_{}\) is the loss function corresponding to \(\). Additionally, if \(\) is finite, \(l(,)\) is a zero-one loss, and samples are i.i.d, then \((||,n,)=\)_

**Lemma 3.1**.: _Given Assumptions 1 and 2 and variational divergence \(tv\), for two arbitrary distributions \(P\) and \(P^{}\) with corresponding density functions \(\) and \(^{}\), \(r(P^{}) r(P)+w(P^{},P)\)._

**Lemma 3.2**.: _Given Assumption 3, Lemma 3.1, and probability at least \(1-\),_

\[rP^{}) r(X,Y)_{P}+w(P^{},P)+(n_{(X,Y) _{P}},,)\]

_where \(n_{(X,Y)_{P}}\) denotes the number of sample sizes in the finite dataset \((X,Y)_{P}\), and \(\) is a vanilla term that connects \(n_{(X,Y)_{P}}\) and \(\) with the generalization error bound._

Proof.: We apply conventional generalization analysis through uniform convergence to Lemma 3.1. We leave the full proof of Lemma 3.1 in Sec. E in the Appendix. 

This results in an intuitive conclusion: empirical robustness performances depends on the divergence between the training and testing distributions, as well as two additional elements. The first is the empirical error term on the training distribution, which can be quantified, and the second is a technical term influenced by the sample size and hypothesis space. The exact manner in which \(\) depends on these parameters is contingent on the particular convergence analysis employed.

Therefore, the decisive factor for robustness is the degree of deviation between the training and testing distributions. Therefore, using diverse data augmentations close to the testing distribution will lead to the largest gains in robustness. This intuitive understanding suggests that training with distributions generated from the teacher will be more advantageous, as the teacher, having been trained on a large dataset, encapsulates more diverse distributions.

There findings are applicable to any arbitrary distributions \(P\). Nevertheless, this doesn't inherently encapsulate the characteristics of foundation models that trained on data from across the internet, composed of a variety of semantic distributions from \(\).

Therefore, we use \(\) to denote a set of \(m\) distributions, i.e., \(=\{P_{1},P_{2},,P_{m}\}\), and we consider a pretrained foundation model trained with such a set of distributions. To facilitate forthcoming discussions, we extend the notation of \(w\) to encompass sets, defining \(w(P^{},)\) as the average divergence between distributions within the set. Thus, \(w(P^{},):=_{i}^{m}w(P^{},P_{i})/m, P_{i} \).

**Lemma 3.3**.: _Given a distribution \(P\), we generate a new distribution \(P^{*}\) using the discretized worst-case adversarial samples of a model \(\). Training \(\) with adversarial training is equivalent to training \(\) with empirical risk minimzation on \(P^{*}\) where \(w(P,P^{*})\)._

We leave the full proof of Lemma 3.3 in Sec. E of the Appendix. Finally, let's denote the model \(\) trained over distribution \(P\) as \((P)\) and the adversarial data augmentation process that results in a new semantic data distribution as \(D\). We aim to compare \(w(D(()),P^{})\) and \(w(D((P)),P^{})\). In this context, \(P^{}\) is any arbitrary testing distribution, \(P\) is a specific training distribution, and \(\) represents the set of distributions used for training foundation models.

**Lemma 3.4**.: _Assuming \(\) is continuous and has a finite expected value, for two sets of distributions \(_{1}\) and \(_{2}\), assuming there is at least one distribution in the intersection of \(_{1}\) and \(_{2}\), for a fixed testing distribution \(P^{}\), we have_

\[_{}w(D((_{1})),P^{})-w(D( (_{2})),P^{}) 2_{} _{P}_{1}}w(P,P^{})+_{P}_{2}}w(P,P^{})\]

We leave the full proof of Lemma 3.4 in Sec. E of the Appendix. Our findings provide a comparison of the differences in the training mechanisms for various models \(\), each trained with distinct data sets (\(_{1}\) and \(_{2}\)) and subjected to adversarial training. The methodology can easily be simplified to compare the bounded results between adversarial training based on the teacher model and standalone adversarial training by setting one of the training datasets to consist of a single distribution.

In the scope of our investigation, we compare DAD and discrete adversarial training based on the teacher model, referred to as \(_{1}\), with DAT  and the traditional approach of adversarial training based on the student model, referred to as \(_{2}\). Our results suggest two key implications:

1. Given a fixed \(_{2}\), a more diverse \(_{1}\) potentially results in greater variations in performance. We show visualizations that support this in Sec. D in the Appendix. In other words, the use of larger, more diverse pretrained datasets for the teacher model or more diverse data augmentations for the student model increases the likelihood of creating a robust final model. This is also been shown empiricially in prior work investigating the source of robustness in foundation models . However, in practice, the efficacy of distilling from this teacher depends on a variety of factors, including student-teacher architecture, training objective, and student capacity.
2. For a fixed \(_{2}\), the greater the distance between the testing dataset \(P^{}\) and \(_{2}\), the more likely it is that the performance gains will be realized by distilling the teacher model trained on a more extensive set of training data. To put it intuitively, if the testing dataset closely resembles the training dataset (i.e., \(w(,P^{})\) is small), then adversarial training based on the teacher model might not yield significant performance improvements. However, if the testing dataset differs considerably from the training dataset, then adversarial training based on a teacher model that has been trained on a larger dataset is more likely to yield superior performance gains. This observation aligns with our empirical results.

## 4 Experimental Results

### Experimental Setup

**Models.** We focus primarily on ResNet50  and ViT-B/16 . We distill from a frozen pretrained CLIP-ViT-L/14 , trained on 224 x 224 resolution images with a patch size of 14.

**Datasets.** We train our models on ImageNet-1K . We use several evaluation datasets. For in-distribution performance, we evaluate on ImageNet and ImageNet-V2 , a replication of ImageNet's evaluation set. We focus our study on natural distribution shifts and evaluate on ImageNet-A , a set of adversarially filtered natural images misclassified by a ResNet50, ImageNet-Sketch  which contains artistic sketches of objects, and ImageNet-Rendition  which contains abstract or rendered objects. To observe performance on distributions that are out-of-distribution for the CLIP teacher, we evaluate on synthetic distribution shifts ImageNet-C , which applies corruptions (snow, blur, noise, etc.) to ImageNet, and Stylized-ImageNet , which processes ImageNet with style transfer from a source image.

### Baselines

DAD consists of both a data augmentation and knowledge distillation objective. We compare to both types of methods in our experiments.

**Common data augmentations.** For the simplest baseline, we follow  and train with common data augmentations Mixup , which combines images and labels, and Randaugment , which learns a policy over common transformations such as brightness or shear.

**DAT.** We also compare against the state-of-the-art data augmentation, DAT , which uses a VQGAN  to discretize adversarial examples in adversarial training. DAT uses the standard adversarial training objective 5.

**Knowledge distillation.** We compare against other logit-based knowledge distillation objectives, which only distill the output logits of the teacher. We consider standard knowledge distillation 7 and DIST , which aims to address the model capacity gap issue by distilling logit class relationships. Neither method natively supports distillation on augmented samples, so we also compare to defensive distillation objectives ARD  and RSLAD . ARD modifies 7 to use the KL divergence between the student logits on the augmented sample with the teacher logits on the normal sample. RSLAD is an extension of ARD that replaces the cross-entropy terms with a KL divergence loss. For a fair comparison, we use DAD as the data augmentation.

### Main Experimental Results on ViT-B/16 and ResNet50

**ImageNet-1K.** Tab. 1 shows results on ImageNet-1K and its distribution shifts. We compare against ViT-B/16 and ResNet50 models without data augmentation and with the state-of-the-art data augmentation approaches, PyramidAT  and DAT . We combine DAD with the data augmentations used in AugReg , MixUp  and RandAugment . We find that DAD has the best average performance across datasets for both models. For ViT-B we find that DAD has competitive in-distribution performance, but greatly improves performance on natural distributions. Compared to Pyramid AT and DAT, DAD also generalizes well to ResNet50. This suggests that the

   Method & Rendition & Sketch & A & Avg \\  CLIP  & 87.7 & 61.6 & 64.2 & 71.2 \\  ViT  & 27.1 & 17.3 & 8.0 & 17.5 \\ Advprop  & 43.5 & 31.7 & 18.5 & 31.2 \\ Fast Advprop  & 41.8 & 29.4 & 17.9 & 29.2 \\ Debiased  & 40.3 & 29.4 & 18.3 & 29.3 \\ AugReg-ViT  & 39.5 & 29.2 & 19.0 & 29.2 \\ + Pyramid AT  & 47.7 & 36.8 & 23.0 & 35.8 \\ + DAT  & 47.3 & 34.8 & 30.2 & 37.4 \\ **+ DAD (Ours)** & **65.1** & **46.1** & **31.8** & **47.7** \\ **+ DAT + DAD (Ours)** & 53.2 & 39.3 & 28.2 & 40.2 \\  ResNet50  & 36.1 & 24.0 & 0.0 & 20.0 \\ Advprop  & 38.8 & 25.5 & 4.3 & 22.9 \\ Pyramid AT  & 38.9 & 23.8 & 3.0 & 21.9 \\ Debiased  & 40.8 & 28.4 & 3.5 & 24.2 \\ DAT  & 42.0 & 27.3 & 4.4 & 24.6 \\
**DAD (Ours)** & **51.6** & **35.8** & **7.7** & **31.7** \\
**DAT + DAD (Ours)** & 47.7 & 33.3 & 6.1 & 29.0 \\   

Table 1: Main results on natural distribution shifts and in-distribution ImageNet. Baseline models are ViT-B/16 (top half) and ResNet50 (bottom half) trained on 224 x 224 images. The CLIP teacher is ViT-L/14. DAD variants have the best average performance for both types of distributions.

DAD data augmentation can be used across student architectures and that due to distillation, DAD is especially effective when training smaller models.

We also demonstrate DAD can be combined with existing approach DAT for stronger in-distribution performance. We add our distillation objective to the DAT objective and train the student on both the teacher's and its own adversarial samples. However, this comes at the cost of lower performance on natural distribution shifts, although we do observe that DAD + DAT still outperforms the prior state-of-the-art on ImageNet-Sketch and ImageNet-Rendition.

**Synthetic distribution shifts.** We also evaluate our models on synthetic distribution shifts composed of generated transformations in Tab. 2. Since the diverse training distribution of CLIP is mostly composed of natural distribution shifts, it has weaker zero-shot generalization to synthetic distribution shifts, and this performance is inherited in the student model. In fact, zero-shot CLIP is already outperformed by some compared methods on ImageNet-C, and Stylized-ImageNet. However, for ResNet50 DAD also has the best ImageNet-C performance, likely due to compared methods being specialized for certain distribution shifts [36; 74] or architectures .

**Distillation.** In Tab. 3 we compare DAD to knowledge distillation objectives. KD  and DIST  are vanilla distillation approaches without data augmentation or AT. ARD  and RSLAD  are defensive distillation objectives that use a adversarially robust teacher to encourage invariance to perturbations. For a fair comparison, we use CLIP-ViT-L/14 as the teacher and discretize the adversarial examples. We find that our distillation objective outperforms vanilla and defensive distillation objectives. We note that all methods can transfer robustness to the student, even methods without data augmentation.

**ImageNet-21k.** In Tab. 4 we show further gains in robustness from applying DAD to a ViT-B/16 pretrained on ImageNet-21K. We fine-tune this model with our method using only our method. Despite the baseline model performing worse than the variant trained with AugReg, DAD achieves

   Method & C (\(\)) & Stylized \\  CLIP  & 60.2 & 18.5 \\  ViT  & 74.0 & 6.4 \\ Advprop  & 51.5 & 19.2 \\ Fast Advprop  & 53.3 & 18.4 \\ Debiased  & 49.8 & 22.4 \\ AugReg-ViT  & 54.5 & 16.6 \\ + Pyramid AT  & 45.0 & 19.1 \\ + DAT  & **44.7** & **23.1** \\ **+ DAD** & 53.2 & 22.4 \\ **+ DAT + DAD** & 47.5 & 22.6 \\   

Table 2: Main results on synthetic distribution shifts, which is out-of-distribution for the CLIP teacher. Models are ViT-B/16 (left) and ResNet50 (right) trained on 224 x 224 images. The CLIP teacher is ViT-L/14. Compared to DAT, DAD tends to perform worse on ViT-B but better on ResNet50.

    &  &  &  \\  Method & ImageNet & V2 & C (\(\)) & Stylized & Rendition & Sketch & A & Avg \\  KD  & 78.6 & 67.2 & 61.5 & 16.2 & 51.5 & 34.7 & 16.0 & 43.2 \\ DIST  & 76.6 & 63.9 & 65.8 & 12.7 & 40.8 & 26.9 & 11.2 & 38.0 \\ ARD  & **80.1** & **70.3** & **52.1** & 22.2 & 55.6 & 38.6 & 27.3 & 48.9 \\ RSLAD  & 79.9 & 69.3 & 55.6 & 20.8 & 55.9 & 39.8 & 25.5 & 47.9 \\ DAD (Ours) & 79.6 & 69.9 & 53.2 & **22.4** & **65.1** & **46.1** & **31.8** & **51.7** \\   

Table 3: Comparison to distillation objectives on ImageNet. We use DAD for the data augmentation of ARD and RSLAD for a fair comparison. All students are ViT-B/16 trained on 224 x 224 images and all teachers are CLIP-ViT-L/14. We find that our distillation objective is the best at distilling out-of-distribution robustness from CLIP.

higher relative and absolute gains in robustness. We hypothesize the larger training distribution equips the student with useful inductive biases that let it more easily learn the more out-of-distribution adversarial examples generated from CLIP. We note that the CLIP training set is still \(\)28.2x larger so this does not contradict our theory, but it may also be beneficial to train or pretrain the student on a more diverse dataset for a smoother distillation process.

### Ablations

**Other student and teacher architectures.** Although our method and theory is adapted for foundation models, we investigate its efficacy on other models and teachers in Tab. 5. We consider a different large-scale teacher, CLIP-RN101, as well as teachers trained on ImageNet-1K that achieve out-of-distribution robustness through methods besides large-scale training, like Discrete ViT  or ViT-B  trained with DAT . We also consider smaller students like ResNet34  and ViT-S.

We find that distilling robustness in our setting depends on several factors, but most crucially, the robustness of the teacher. Like other distillation techniques, we find that our method can transfer representations between various student/teacher architectures. We find that our method is also susceptible to the model capacity gap, with lower clean accuracy on ResNet34 when distilling from CLIP than training from scratch. However, using CLIP results in the best performance on natural distribution shifts. Despite the more similar architecture, distilling from CLIP-RN101 across students is less effective than distilling from the more robust CLIP-ViT-L. We include similar results with vanilla knowledge distillation in Sec. A of the Appendix.

    &  &  &  \\  Method & ImageNet & V2 & C (\(\)) & Stylized & Rendition & Sketch & A & Avg \\  ViT & 77.5 & 65.7 & 61.9 & 17.7 & 41.5 & 16.4 & 23.1 & 40.0 \\ DAT  & **83.1** & **73.2** & **43.6** & **24.8** & 55.2 & 41.7 & 36.7 & 53.0 \\ DAD (Ours) & 79.8 & 70.9 & 52.0 & 23.4 & **72.1** & **51.2** & **40.3** & **55.1** \\   

Table 4: Main results from pre-training on ImageNet-21K and fine-tuning on ImageNet-1K. All columns report top-1 accuracy except ImageNet-C which reports mean Corruption Error (mCE) where lower is better. All models are ViT-B/16 trained on 224 x 224 images. We find that pretraining on ImageNet-21K results in larger robustness improvements.

   Student & Teacher & IM & A & C (\(\)) & V2 & Rendition & Sketch & Stylized & Avg \\  ViT-B & CLIP-RN101  & **81.2** & 24.3 & **49.6** & 69.3 & 48.7 & 34.5 & 17.3 & 46.5 \\ ViT-B & CLIP-ViT-L  & 79.6 & **31.8** & 53.2 & **69.9** & **65.1** & **46.1** & **22.4** & **51.7** \\  RN50 & - & & 76.1 & 0 & 76.7 & 63.2 & 36.1 & 24.0 & 7.4 & 32.9 \\ RN50 & ViT-B + DAT  & **80.4** & **10.1** & **65.6** & **68.8** & 40.4 & 29.6 & 8.5 & 38.9 \\ RN50 & DrViT-S  & 78.5 & 5.5 & 67.4 & 66.2 & 42.0 & 30.1 & 11.5 & 38.1 \\ RN50 & CLIP-RN101  & 76.4 & 5.4 & 70.2 & 64.5 & 47.7 & 32.2 & 9.6 & 37.9 \\ RN50 & CLIP-ViT-L  & 75.7 & 7.7 & 67.4 & 65.0 & **51.6** & **35.8** & **13.1** & **40.2** \\  ViT-S & - & 77.8 & 11.9 & 63.9 & **66.0** & 36.9 & 25.3 & 12.0 & 38.0 \\ ViT-S & ViT-B + DAT  & **77.8** & 11.9 & 67.1 & 66.0 & 36.9 & 25.3 & 12.0 & 37.5 \\ ViT-S & CLIP-RN101  & 73.4 & 9.0 & 65.2 & 62.1 & 38.8 & 23.9 & 12.0 & 36.3 \\ ViT-S & CLIP-ViT-L  & 73.8 & **18.0** & **63.1** & 64.0 & **52.9** & **35.8** & **17.3** & **42.7** \\  RN34 & - & 66.5 & 3.0 & 94.5 & 54.7 & 32.4 & 21.0 & 5.6 & 27.0 \\ RN34 & RN50 + AugMix  & 68.9 & 1.8 & 82.9 & 56.2 & 37.2 & 24.1 & 9.9 & 30.7 \\ RN34 & DrViT-S  & 68.2 & 2.1 & 79.5 & 55.6 & 37.0 & 23.0 & 10.5 & 31.0 \\ RN34 & ViT-B + DAT  & **69.2** & 2.2 & **79.0** & **56.6** & 38.7 & 25.0 & 11.0 & 32.0 \\ RN34 & CLIP-RN101  & 65.4 & 2.5 & 85.3 & 53.5 & 42.5 & 26.3 & 8.5 & 30.5 \\ RN34 & CLIP-ViT-L  & 63.6 & **4.5** & 82.0 & 53.7 & **46.0** & **29.1** & **11.7** & **32.4** \\   

Table 5: Results on other student/teacher architectures, on ImageNet-1K. All experiments use DAD for the knowledge distillation objective and data augmentation. Using the best CLIP model as the teacher tends to result in the highest overall performance, but some teachers are better for some shifts.

**Pure data augmentation.** We study in Tab. 6 the effect of training on the DAD adversarial examples purely as a data augmentation technique, without distillation. Although DAD remains competitive, we find significant drops in performance, suggesting that it is difficult for the student to learn robust representations of these images on its own. However, we continue to observe improvements on natural distribution shifts, suggesting these samples are closer to CLIP's training distribution. However, training with DAD samples is significantly cheaper than DAT and Pyramid AT, making it more efficient in practice.

**Computational cost analysis.** Since DAD uses adversarial examples generated from a frozen teacher, there is no need to regenerate them during training. This amortizes the otherwise significant cost of adversarial training. We compare the cost of DAD with other adversarial data augmentation approaches in Tab. 7. By avoiding the need to continuously generate new adversarial examples, the only remaining cost for DAD is training on a larger dataset, making it cheaper than similar methods.

Additional ablations on choice of generative model, use of gradients, and transfer to adversarial robustness can be found in Sec. B in the Appendix.

## 5 Conclusion and limitations

We conduct the first study on distilling out-of-distribution robustness. We develop a framework for the use of foundation models in this setting and empirically and theoretically validate their advantages as a teacher. We propose discrete adversarial distillation (DAD) which uses the discrete adversarial examples of the teacher as a more diverse data augmentation and directly distill its most diverse representations. However, we find that DAD tends to be biased towards the performance of the CLIP teacher, exhibiting improvements mostly on natural distribution shifts. In practice, these shifts tend to be the most useful, and with the small computational cost of using DAD, we encourage practitioners to adopt it when training small models. We hope the development and release of improved foundation models and generative models will further demonstrate the effectiveness of our method.

We encourage further work to understand the limitations of machine vision models in out-of-distribution settings. More robust models carry the potential risk of automation bias, i.e., an undue trust in vision models. However, even if models are robust against corruptions in finite out-of-distribution datasets, they might still quickly fail on the massive space of semantic transformations in real-world data. Understanding under what conditions model decisions can be deemed reliable is still an open research question.

   Model & IM & A & C (\(\)) & V2 & Rendition & Sketch & Stylized & Avg \\  AugReg-ViT  & 79.9 & 19.0 & 54.5 & 67.9 & 39.5 & 29.2 & 16.6 & 42.5 \\ + Pyramid AT  & **81.7** & 23.0 & 45.0 & 70.3 & 47.7 & 36.8 & 19.1 & 47.7 \\ + DAT  & 81.5 & **30.2** & **44.7** & **70.8** & 47.3 & 34.8 & **23.1** & **49.0** \\ **+ DAD (Ours)** & 80.2 & 24.6 & 53.0 & 69.8 & **51.7** & **36.9** & 22.1 & 47.5 \\   

Table 6: Comparisons to data augmentation approaches. All columns report top-1 accuracy except ImageNet-C which reports mean Corruption Error (mCE) where lower is better. All models are ViT-B/16 trained on 224 x 224 images. We remove the distillation terms and use DAD samples as a standard data augmentation. All methods are based on AugReg and use Mixup and Randaugment.

   Method & Attack Steps & Training Budget \\  ImageNet & 0 & 1x \\ Adversarial Training  & 10 & 11x \\ AdvProp  & 5 & 7x \\ Fast AdvProp  & 1 & 3x \\ DAT  & 1 & 3.5x \\
**DAD (Ours)** & 1 & 2x \\   

Table 7: Adversarial-training-based data augmentations and their training budget. Cost is based on training a ResNet50 from scratch for 100 epochs. While still more expensive than standard training, DAD is significantly cheaper than other techniques due to reusing precomputed adversarial examples.