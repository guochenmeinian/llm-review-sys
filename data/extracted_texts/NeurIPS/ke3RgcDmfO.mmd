# TextDiffuser: Diffusion Models as Text Painters

Jingye Chen\({}^{*}\)\({}^{13}\), Yupan Huang\({}^{*}\)\({}^{23}\), Tengchao Lv\({}^{3}\), Lei Cui\({}^{3}\), Qifeng Chen\({}^{1}\), Furu Wei\({}^{3}\)

\({}^{1}\)HKUST \({}^{2}\)Sun Yat-sen University \({}^{3}\)Microsoft Research

qwerty.chen@connect.ust.hk, huangyp28@mail2.sysu.edu.cn, cqf@ust.hk

{tengchaolv,lecu,fuwei}@microsoft.com

###### Abstract

Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce **TextDiffuser**, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout. Additionally, we contribute the first large-scale text images dataset with OCR annotations, **MARIO-10M**, containing 10 million image-text pairs with text recognition, detection, and character-level segmentation annotations. We further collect the **MARIO-Eval** benchmark to serve as a comprehensive tool for evaluating text rendering quality. Through experiments and user studies, we show that TextDiffuser is flexible and controllable to create high-quality text images using text prompts alone or together with text template images, and conduct text inpainting to reconstruct incomplete images with text. The code, model, and dataset will be available at https://aka.ms/textdiffuser.

Figure 1: TextDiffuser generates accurate and coherent text images from text prompts or together with template images, as well as conducting text inpainting to reconstruct incomplete images.

Introduction

The field of image generation has seen tremendous progress with the advent of diffusion models [2; 15; 16; 18; 25; 67; 70; 72; 79; 93] and the availability of large-scale image-text paired datasets [17; 74; 75]. However, existing diffusion models still face challenges in generating visually pleasing text on images, and there is currently no specialized large-scale dataset for this purpose. The ability of AI models to generate accurate and coherent text on images is crucial, given the widespread use of text images in various forms (_e.g._, posters, book covers, memes, etc.) and the difficulty in creating high-quality text images, which typically require professional skills and numerous times of designers.

Traditional solutions to creating text images involve using image processing tools like Photoshop to add text onto images directly. However, these often result in unnatural artifacts due to the background's complex texture or lighting variations. Recent efforts have used diffusion models to overcome the limitations of traditional methods and enhance text rendering quality. For instance, Imagen , eDiff-I , and DeepFloyd  observe diffusion models generate text better with T5 series text encoders  than the CLIP text encoder . Liu et al. employ character-aware text encoders to improve text rendering . Despite some success, these models only focus on text encoders, lacking control over the generation process. A concurrent work, GlyphDraw , improves the controllability of models by conditioning on the location and structures of Chinese characters. However, GlyphDraw does not support multiple text bounding-box generation, which is not applicable to many text images such as posters and book covers.

In this paper, we propose **TextDiffuser**, a flexible and controllable framework based on diffusion models. The framework consists of two stages. In the first stage, we use a Layout Transformer to locate the coordinates of each keyword in text prompts and obtain character-level segmentation masks. In the second stage, we fine-tune the latent diffusion model by leveraging the generated segmentation masks as conditions for the diffusion process and text prompts. We introduce a character-aware loss in the latent space to further improve the quality of generated text regions. Figure 1 illustrates the application of TextDiffuser in generating accurate and coherent text images using text prompts alone or text template images. Additionally, TextDiffuser is capable of performing text inpainting2 to reconstruct incomplete images with text. To train our model, we use OCR tools and design filtering strategies to obtain 10 million high-quality image-text pairs with OCR annotations (dubbed as **MARIO-10M**), each with recognition, detection, and character-level segmentation annotations. Extensive experiments and user studies demonstrate the superiority of the proposed TextDiffuser over existing methods on the constructed benchmark **MARIO-Eval**. The code, model and dataset will be publicly available to promote future research.

## 2 Related Work

Text Rendering.Image generation has made significant progress with the advent of diffusion models [18; 25; 67; 72; 79; 63; 70; 2; 8; 13; 52; 48; 26; 80], achieving state-of-the-art results compared with previous GAN-based approaches [64; 100; 43; 58]. Despite rapid development, current methods still struggle with rendering accurate and coherent text. To mitigate this, Imagen , eDiff-I , and DeepFolyd  utilize a large-scale language model (large T5 ) to enhance the text-spelling knowledge. In , the authors noticed that existing text encoders are blind to token length and trained a character-aware variant to alleviate this problem. A concurrent work, GlyphDraw , focuses on generating high-quality images with Chinese texts with the guidance of text location and glyph images. Unlike this work, we utilize Transformer  to obtain the layouts of keywords, enabling the generation of texts in multiple lines. Besides, we use character-level segmentation masks as prior, which can be easily controlled (_e.g._, by providing a template image) to meet user needs.

Several papers have put forward benchmarks containing a few cases regarding text rendering for evaluation. For example, Imagen  introduces DrawBench containing 200 prompts, in which 21 prompts are related to visual text rendering (_e.g._, A storefront with 'Hello World' written on it). According to , the authors proposed DrawText comprising creative 175 prompts (_e.g._, letter 'c' made from cactus, high-quality photo). GlyphDraw  designs 218 prompts in Chinese and English (_e.g._, Logo for a chain of grocery stores with the name 'Grocery'). Considering that existing benchmarks only contain a limited number of cases, we attempt to collect more prompts and combine them with existing prompts to establish a larger benchmark MARIO-Eval to facilitate comprehensive comparisons for future work.

Image Inpainting.Image inpainting is the task of reconstructing missing areas in images naturally and coherently. Early research focused on leveraging low-level image structure and texture to address this task [3; 6; 5]. Later, deep learning architectures such as auto-encoder [55; 45], GAN [65; 98], VAE [103; 105], and auto-regressive Transformers [57; 83; 96] were applied to tackle this problem. Recently, diffusion models have been used to generate high-quality and diverse results for unconditional image inpainting [71; 48; 67; 11; 99], text-conditional image inpainting [52; 1] and image-conditional image inpainting . Our work falls under the category of text-conditional image inpainting using diffusion models. In contrast to prior works that focused on completing images with natural backgrounds or objects, our method focuses on completing images with text-related rendering, also named text inpainting, by additional conditioning on a character-level segmentation mask.

Optical Character Recognition.Optical Character Recognition (OCR) is an important task that has been studied in academia for a long period [87; 7]. It has undergone a remarkable development in the last decade, contributing to many applications like autonomous driving [89; 73], car license plate recognition [101; 53], GPT models [76; 28], etc. Various datasets [31; 19; 90; 91] and downstream tasks are included within this field, such as text image recognition [77; 41; 95; 14], detection [106; 51; 42; 84], segmentation [90; 91; 107; 68], super-resolution [9; 85; 50; 104], as well as some generation tasks, including text image editing [88; 78; 94; 59; 38], document layout generation [56; 22; 20; 40; 33], font generation [30; 21; 36; 54], etc. Among them, the font generation task is most _relevant_ to our task. Font generation aims to create high-quality, aesthetically pleasing fonts based on given character images. In contrast, our task is more challenging as it requires the generated text to be legible, visually appealing, and coherent with the background in various scenarios.

## 3 Methodology

As illustrated in Figure 2, TextDiffuser consists of two stages: Layout Generation and Image Generation. We will detail the two stages and introduce the inference process next.

### Stage1: Layout Generation

In this stage, the objective is to utilize bounding boxes to determine the layout of keywords (enclosed with quotes specified by user prompts). Inspired by Layout Transformer , we utilize the Transformer architecture to obtain the layout of keywords. Formally, we denote the tokenized prompt as \(=(p_{0},p_{1},...,p_{L-1})\), where \(L\) means the maximum length of tokens. Following LDM , we use CLIP  and two linear layers to encode the sequence as \(()^{L d}\), where \(d\) is the dimension of latent space. To distinguish the keywords against others, we design a keyword embedding \(()^{L d}\) with two entries (_i.e._, keywords and non-keywords). Furthermore, we encode the width of keywords with an embedding layer \(()^{L d}\). Together with the learnable positional embedding \(()^{L d}\) introduced in , we construct the whole embedding as follows:

\[()=()+()+()+().\] (1)

The embedding is further processed with Transformer-based \(l\)-layer encoder \(_{E}\) and decoder \(_{D}\) to get the bounding boxes \(^{K d}\) of \(K\) key words autoregressively:

\[=_{D}(_{E}(()))=(_{0}, _{1},...,_{K-1}).\] (2)

Specifically, we use positional embedding as the query for the Transformer decoder \(_{D}\), ensuring that the \(n\)-th query corresponds to the \(n\)-th keyword in the prompt. The model is optimized with \(l1\) loss, also denoted as \(|_{GT}-|\) where \(_{GT}\) is the ground truth. Further, we can utilize some Python packages like Pillow to render the texts and meanwhile obtain the character-level segmentation mask \(\) with \(||\) channels, where \(||\) denote the size of alphabet \(\). To this end, we obtain the layouts of keywords and the image generation process is introduced next.

### Stage2: Image Generation

In this stage, we aim to generate the image guided by the segmentation masks \(\) produced in the first stage. We use VAE  to encode the original image with shape \(H W\) into 4-D latent space features \(^{4 H^{} W^{}}\). Then we sample a time step \(T(0,T_{})\) and sample a Gaussian noise \(^{4 H^{} W^{}}\) to corrupt the original feature, yielding \(}=}+_{T}}\) where \(}\) is the coefficient of the diffusion process introduced in . Also, we downsample the character-level segmentation mask \(\) with three convolution layers, yielding 8-D \(}^{8 H^{} W^{}}\). We also introduce two additional features, called 1-D feature mask \(}^{1 H^{} W^{}}\) and 4-D masked feature \(}_{M}^{4 H^{} W^{}}\). In the process of _whole-image generation_, \(}\) is set to cover all regions of the feature and \(}_{M}\) is the feature of a fully masked image. In the process of _part-image generation_ (also called text inpainting), the feature mask \(}\) represents the region where the user wants to generate, while the masked feature \(}_{M}\) indicates the region that the user wants to preserve. To simultaneously train two branches, we use a masking strategy where a sample is fully masked with a probability of \(\) and partially masked with a probability of \(1-\). We concatenate \(},},},}_{M}\) in the feature channel as a 17-D input and use denoising loss between the sampled noise \(\) and the predicted noise \(_{}\):

\[l_{denoising}=||-_{}(},},},}_{M},,T)||_{2}^{2}.\] (3)

Furthermore, we propose a character-aware loss to help the model focus more on text regions. In detail, we pre-train a U-Net  that can map latent features to character-level segmentation masks. During training, we fix its parameters and only use it to provide guidance by using a cross-entropy loss \(l_{char}\) with weight \(_{char}\) (See more details in Appendix A). Overall, the model is optimized with

\[l=l_{denoising}+_{char}*l_{char}.\] (4)

Finally, the output features are fed into the VAE decoder to obtain the images.

Figure 2: TextDiffuser consists of two stages. In the first Layout Generation stage, a Transformer-based encoder-decoder model generates character-level segmentation masks that indicate the layout of keywords in images from text prompts. In the second Image Generation stage, a diffusion model generates images conditioned on noisy features, segmentation masks, feature masks, and masked features (from left to right) along with text prompts. The feature masks can cover the entire or part of the image, corresponding to whole-image and part-image generation. The diffusion model learns to denoise features progressively with a denoising and character-aware loss. Please note that the diffusion model operates in the _latent space_, but we use the image pixels for better visualization.

### Inference Stage

TextDiffuser provides a high degree of controllability and flexibility during inference in the following ways: (1) Generate images from user prompts. Notably, the user can modify the generated layout or edit the text to meet their personalized requirements; (2) The user can directly start from the second stage by providing a template image (_e.g._, a scene image, handwritten image, or printed image), and a segmentation model is pre-trained to obtain the character-level segmentation masks (Appendix B); (3) Users can modify the text regions of a given image using text inpainting. Moreover, this operation can be performed multiple times. These experimental results will be presented in the next section.

## 4 MARIO Dataset and Benchmark

As there is no large-scale dataset designed explicitly for text rendering, to mitigate this issue, we collect 10 million **image**-text pairs with **O**CR annotations to construct the **MARIO-10M Dataset**. We further collect the **MARIO-Eval Benchmark** from the subset of the MARIO-10M test set and other existing sources to serve as a comprehensive tool for evaluating text rendering quality.

### MARIO-10M Dataset

The **MARIO-10M** is a collection of about 10 million high-quality and diverse image-text pairs from various data sources such as natural images, posters, and book covers. Figure 3 illustrates some examples from the dataset. We design automatic schemes and strict filtering rules to construct annotations and clean noisy data (more details in Appendix D and Appendix E). The dataset contains comprehensive OCR annotations for each image, including text detection, recognition, and character-level segmentation annotations. Specifically, we use DB  for detection, PARSeq  for recognition, and manually train a U-Net  for segmentation. We analyze the performance of OCR tools in Appendix F. The total size of MARIO-10M is 10,061,720, from which we randomly chose 10,000,000 samples as the training set and 61,720 as the testing set. MARIO-10M is collected from three data sources:

**MARIO-LATION** derives from the large-scale datasets LAION-400M . After filtering, we obtained 9,194,613 high-quality text images with corresponding captions. This dataset comprises a broad range of text images, including advertisements, notes, posters, covers, memes, logos, etc.

**MARIO-TMDB** derives from The Movie Database (TMDB), which is a community-built database for movies and TV shows with high-quality posters. We filter 343,423 English posters using the TMDB API out of 759,859 collected samples. Since each image has no off-the-shelf captions, we use prompt templates to construct the captions according to movie titles.

**MARIO-OpenLibrary** derives from Open Library, which is an open, editable library catalog that creates a web page for each published book. We first collect 6,352,989 original-size Open Library covers in bulk. Then, we obtained 523,684 higher-quality images after filtering. Like MARIO-TMDB, we manually construct captions using titles due to the lack of off-the-shelf captions.

Figure 3: Illustrations of three subsets of MARIO-10M. See more details in Appendix C.

### MARIO-Eval Benchmark

The **MARIO-Eval benchmark** serves as a comprehensive tool for evaluating text rendering quality collected from the subset of the MARIO-10M test set and other sources. It comprises 5,414 prompts in total, including 21 prompts from DrawBenchText , 175 prompts from DrawTextCreative , 218 prompts from ChineseDrawText  and 5,000 image-text pairs from a subset of the MARIO-10M test set. The 5,000 image-text pairs are divided into three sets of 4,000, 500, and 500 pairs, and are named LAIONEval4000, TMDBEval500, and OpenLibraryEval500 based on their respective data sources. We offer examples in Appendix G to provide a clearer understanding of MARIO-Eval.

**Evaluation Criteria:** We evaluate text rendering quality with MARIO-Eval from four aspects: (1) **Frechet Inception Distance (FID)** compares the distribution of generated images with the distribution of real images. (2) **CLIPScore** calculates the cosine similarity between the image and text representations from CLIP [29; 60; 23]. (3) **OCR Evaluation** utilizes existing OCR tools to detect and recognize text regions in the generated images. Accuracy, Precision, Recall, and F-measure are metrics to evaluate whether keywords appear in the generated images. (4) **Human Evaluation** is conducted by inviting human evaluators to rate the text rendering quality of generated images using questionnaires. More explanations are shown in Appendix H.

## 5 Experiments

### Implementation Details

For the _first_ stage, we utilize the pre-trained CLIP  to obtain the embedding of given prompts. The number of Transformer layers \(l\) is set to 2, and the dimension of latent space \(d\) is set to 512. The maximum length of tokens \(L\) is set to 77 following CLIP . We leverage a commonly used font "Arial.ttf" and set the font size to 24 to obtain the width embedding and also use this font for rendering. The alphabet \(\) comprises 95 characters, including 26 uppercase letters, 26 lowercase letters, 10 digits, 32 punctuation marks, and a space character. After tokenization, only the first subtoken is marked as the keyword when several subtokens exist for a word.

For the _second_ stage, we implement the diffusion process using Hugging Face Diffusers  and load the checkpoint "_runwayml/stable-diffusion-v1-5"_. Notably, we only need to modify the input dimension of the input convolution layer (from 4 to 17), allowing our model to have a similar scale of parameters and computational time as the original model. In detail, the height \(H\) and \(W\) of input and output images are 512. For the diffusion process, the input is with spatial dimension \(H^{}=64\) and \(W^{}=64\). We set the batch size to 768 and trained the model for two epochs, taking four days using 8 Tesla V100 GPUs with 32GB memory. We use the AdamW optimizer  and set the learning rate to 1e-5. Additionally, we utilize gradient checkpoint  and xformers  for computational efficiency. During training, we follow  to set the maximum time step \(T_{max}\) to 1,000, and the caption is dropped with a probability of 10% for classifier-free guidance . When training the part-image generation branch, the detected text box is masked with a likelihood of 50%. We use 50 sampling steps during inference and classifier-free guidance with a scale of 7.5 following .

### Ablation Studies

Number of Transformer layers and the effectiveness of width embedding.We conduct ablation studies on the number of Transformer layers and whether to use width embedding in the Layout Transformer. The results are shown in Table 1. All ablated models are trained on the training set of MARIO-10M and evaluated on its test set. Results show that adding width embedding improves performance, boosting IoU by 2.1%, 2.9%, and 0.3% when the number of Transformer layers \(l\) is set to 1, 2, and 4, respectively. The optimal IoU is achieved using two Transformer layers and the width embedding is included. See more visualization results in Appendix I.

Character-level segmentation masks provide explicit guidance for generating characters.The character-level segmentation masks provide explicit guidance on the position and content of characters during the generation process of TextDiffuser. To validate the effectiveness of using character-level segmentation masks, we train ablated models without using the masks and show results in AppendixJ. The generated texts are inaccurate and not coherent with the background compared with texts generated with TextDiffuser, highlighting the importance of explicit guidance.

The weight of character-aware loss.The experimental results are demonstrated in Table 2, where we conduct experiments with \(_{char}\) ranging from [0, 0.001, 0.01, 0.1, 1]. We utilize DrawBenchText  for evaluation and use Microsoft Read API to detect and recognize the texts in generated images. We use Accuracy (Acc) as the metric to justify whether the detected words _exactly match_ the keywords. We observe that the optimal performance is achieved when \(_{char}\) is set to 0.01, where the score is increased by 9.8% compared with the baseline (\(_{char}=0\)).

The training ratio of whole/part-image generation branches.We explore the training ratio \(\) ranging from [0, 0.25, 0.5, 0.75, 1] and show results in Table 3. When \(\) is set to 1, it indicates that only the whole-image branch is trained and vice versa. We evaluate the model using DrawBenchText  for the whole-image generation branch. For the part-image generation branch, we randomly select 1,000 samples from the test set of MARIO-10M and randomly mask some of the detected text boxes. We utilize Microsoft Read API to detect and recognize the reconstructed text boxes in generated images while using the F-measure of text detection results and spotting results as metrics (denoted as Det-F and Spot-F, respectively). The results show that when the training ratio is set to 50%, the model performs better on average (0.716).

### Experimental Results

Quantitative Results.For the _whole-image generation task_, we compare our method with Stable Diffusion (SD) , ControlNet , and DeepFloyd  in quantitative experiments with the publicly released codes and models detailed in Appendix K. DeepFloyd  uses two super-resolution modules to generate higher resolution \(1024 1024\) images compared with \(512 512\) images generated by other methods. We use the Canny map of printed text images generated with our first stage model as conditions for ControlNet . Please note that we are not able to compare with Imagen , eDiff-i , and GlyphDraw  due to the lack of open-source code, checkpoints or APIs. According to Table 4, we demonstrate the quantitative results of the text-to-image task compared with existing methods. Our TextDiffuser obtains the best CLIPScore while achieving comparable performance in terms of FID. Besides, TextDiffuser achieves the best performance regarding four OCR-related metrics. TextDiffuser outperforms those methods without explicit text-related guidance by a large

   \#Layer & Width(\(\)) & IoU\(\) \\   & - & 0.268 \\  & ✓ & 0.289 \\   & - & 0.269 \\  & ✓ & **0.298** \\   & - & 0.294 \\  & ✓ & 0.297 \\       \(_{char}\) & Acc\(\) \\   & 0.396 \\  & 0.001 & 0.486 \\  & 0.01 & **0.494** \\  & 0.1 & 0.420 \\  & 1 & 0.400 \\    
   ratio & Acc\(\)/ Det-F\(\)/ Spot-F\(\) \\   & 0 & 0.344 / 0.870 / 0.663 \\  & 0.25 & **0.562** / 0.899 / 0.636 \\  & 0.55 & 0.552 / 0.881 / **0.715** \\  & 0.75 & 0.524 / **0.921** / 0.695 \\  & 1 & 0.494 / 0.380 / 0.218 \\   

Table 1: Ablation about Layout Transformer.

   Metrics & StableDiffusion  & ControlNet  & DeepFloyd  & TextDiffuser \\  FID\(\) & 51.295 & 51.485 & **34.902** & 38.758 \\ CLIPScore\(\) & 0.3015 & 0.3424 & 0.3267 & **0.3436** \\  OCR(Accuracy)\(\) & 0.0003 & 0.2390 & 0.0262 & **0.5609** \\ OCR(Precision)\(\) & 0.0173 & 0.5211 & 0.1450 & **0.7846** \\ OCR(Recall)\(\) & 0.0280 & 0.6707 & 0.2245 & **0.7802** \\ OCR(F-measure)\(\) & 0.0214 & 0.5865 & 0.1762 & **0.7824** \\   

Table 4: The performance of text-to-image compared with existing methods. TextDiffuser performs the best regarding CLIPScore and OCR evaluation while achieving comparable performance on FID.

margin (_e.g._, 76.10% and 60.62% better than Stable Diffusion and DeepFloyd regarding F-measure), highlighting the significance of explicit guidance. As for the _part-image generation task_, we cannot evaluate our method since no methods are specifically designed for this task to our knowledge.

Qualitative Results.For the _whole-image generation task_, we further compare with closed-source DALL-E , Stable Diffusion XL (SD-XL), and Midjourney by showing qualitative examples generated with their official API services detailed in Appendix K. Figure 4 shows some images generated from prompts or printed text images by different methods. Notably, our method generates more readable texts, which are also coherent with generated backgrounds. On the contrary, although the images generated by SD-XL and Midjourney are visually appealing, some generated text does not contain the desired text or contains illegible characters with incorrect strokes. The results also show that despite the strong supervision signals provided to ControlNet, it still struggles to generate images with accurate text consistent with the background. We also initiate a comparison with the Character-Aware Model  and the concurrent work GlyphDraw  using samples from their papers as their open-source code, checkpoints or APIs are not available. Figure 5 shows that TextDiffuser performs better than these methods. For instance, the Character-Aware Model suffers from misspelling issues (_e.g._,'m' in 'Chimpanzees') due to its lack of explicit control, and GlyphDraw struggles with rendering images containing multiple text lines. For the _part-image generation task_, we visualize some results in Figure 6. In contrast to text editing tasks , we give the model sufficient flexibility to generate texts with reasonable styles. For instance, the image in the second row and first column contains the word "country" in green, while the model generates the word "country" in yellow. This is reasonable since it follows the style of the nearest word "range". Besides, our method can render realistic text coherent with the background, even in complex cases such as clothing. More qualitative results are shown in Appendix L.

Figure 4: Visualizations of whole-image generation compared with existing methods. The first three cases are generated from prompts and the last three cases are from given printed template images.

User Studies.For the _whole-image generation task_, the designed questionnaire consists of 15 cases, each of which includes two multiple-choice questions: (1) Which of the following images has the best text rendering quality? (2) Which of the following images best matches the text description? For the _part-image generation task_, the questionnaire consists of 15 cases, each of which includes two rating questions: (1) How is the text rendering quality? (2) Does the drawn text harmonize with the unmasked region? The rating scores range from 1 to 4, and 4 indicates the best. Overall, we have collected 30 questionnaires, and the results are shown in Figure 8. We can draw two conclusions: (1) The generation performance of TextDiffuser is significantly better than existing methods. (2) Users are satisfied with the inpainting results in most cases. More details are shown in Appendix M.

Time and Parameter EfficiencyFor the time efficiency, the first stage of Layout Generation leverages an auto-regressive Transformer whose prediction time correlates with the number of keywords. Specifically, we conduct experiments to evaluate the time overhead for different numbers of keywords, including 1 (1.07\(\)0.03s), 2 (1.12\(\)0.09s), 4 (1.23\(\)0.13s), 8 (1.57\(\)0.12s), 16 (1.83\(\)0.12s), and 32 (1.95\(\)0.28s). Meanwhile, the second stage of image generation is independent of the number of queries (7.12\(\)0.77s). For the parameter efficiency, TextDiffuser builds upon Stable Diffusion 1.5 (859M parameters), adding a Layout Transformer in the first stage (+25M parameters) and modifying the second stage (+0.75M parameters), augmenting it by only about 3% in terms of parameters.

Figure 5: Comparison with Character-Aware Model  and the concurrent GlyphDraw .

Figure 6: Visualizations of part-image generation (text inpainting) from given images.

Text Color ControllabilityIn Figure 7, we showcase TextDiffuser's capability in controlling the color of generated texts through language descriptions. The visualization results show that TextDiffuser can successfully control the color of rendered text, further enhancing its controllability.

## 6 Discussion and Conclusion

**Discussion.** We show that TextDiffuser maintains the capability and generality to create general images without text rendering in Appendix N. Besides, we compare our method with a text editing model in Appendix O, showing that TextDiffuser generates images with better diversity. We also present the potential of TextDiffuser on the text removal task in Appendix P. As for the **limitations and failure cases**, TextDiffuser uses the VAE networks to encode images into low-dimensional latent spaces for computational efficiency following latent diffusion models [67; 49; 2], which has a limitation in reconstructing images with small characters as shown in Appendix Q. We also observed failure cases when generating images from long text and showed them in Appendix Q. As for the **broader impact**, TextDiffuser can be applied to many designing tasks, such as creating posters and book covers. Additionally, the text inpainting task can be used for secondary creation in many applications, such as Midjourney. However, there may be some ethical concerns, such as the misuse of text inpainting for forging documents. Therefore, techniques for detecting text-related tampering  need to be applied to enhance security. **In conclusion**, we propose a two-stage diffusion model called TextDiffuser to generate images with visual-pleasing texts coherent with backgrounds. Using segmentation masks as guidance, the proposed TextDiffuser shows high flexibility and controllability in the generation process. We propose MARIO-10M containing 10 million image-text pairs with OCR annotations. Extensive experiments and user studies validate that our method performs better than existing methods on the proposed benchmark MARIO-Eval. **For future work,** we aim to address the limitation of generating small characters by using OCR priors following OCR-VQGAN  and enhance TextDiffuser's capabilities to generate images with text in multiple languages. **Disclaimer** Please note that the model presented in this paper is intended for academic and research purposes **ONLY**. Any use of the model for generating inappropriate content is strictly prohibited and is not endorsed by this paper. The responsibility for any misuse or improper use of the model lies solely with the users who generated such content, and this paper shall not be held liable for any such use.

## 7 Acknowledgement

This research was supported by the Research Grant Council of the Hong Kong Special Administrative Region under grant number 16203122.

Figure 8: User studies for whole-image generation and part-image generation tasks.

Figure 7: Demonstration of using language descriptions to control text color.