# EnsIR: An Ensemble Algorithm for Image Restoration via Gaussian Mixture Models

Shangquan Sun\({}^{1,2}\)&Wenqi Ren\({}^{3,4}\)&Zikun Liu\({}^{5}\)

Hyunhee Park\({}^{6}\)&Rui Wang\({}^{1,2}\)&Xiaochun Cao\({}^{3}\)

\({}^{1}\)Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China

\({}^{2}\)School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China

\({}^{3}\)School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University

\({}^{4}\)Guangdong Provincial Key Laboratory of Information Security Technology

\({}^{5}\)Samsung Research China - Beijing (SRC-B)

\({}^{6}\)Camera Innovation Group, Samsung Electronics

{sunshangquan,wangrui}@iie.ac.cn

{zikun.liu,inextg.park}@samsung.com

{renwq3,caoxiaochun}@mail.sysu.edu.cn

Corresponding Author

###### Abstract

Image restoration has experienced significant advancements due to the development of deep learning. Nevertheless, it encounters challenges related to ill-posed problems, resulting in deviations between single model predictions and ground-truths. Ensemble learning, as a powerful machine learning technique, aims to address these deviations by combining the predictions of multiple base models. Most existing works adopt ensemble learning during the design of restoration models, while only limited research focuses on the inference-stage ensemble of pre-trained restoration models. Regression-based methods fail to enable efficient inference, leading researchers in academia and industry to prefer averaging as their choice for post-training ensemble. To address this, we reformulate the ensemble problem of image restoration into Gaussian mixture models (GMMs) and employ an expectation maximization (EM)-based algorithm to estimate ensemble weights for aggregating prediction candidates. We estimate the range-wise ensemble weights on a reference set and store them in a lookup table (LUT) for efficient ensemble inference on the test set. Our algorithm is model-agnostic and training-free, allowing seamless integration and enhancement of various pre-trained image restoration models. It consistently outperforms regression-based methods and averaging ensemble approaches on 14 benchmarks across 3 image restoration tasks, including super-resolution, deblurring and deraining. The codes and all estimated weights have been released in Github.

## 1 Introduction

Image restoration has witnessed significant progress over the decades, especially with the advent of deep learning. Numerous architectures have been developed to solve the problem of restoration, including convolutional neural networks (CNNs) , vision Transformers (ViTs)  and recently, vision Mambas . However, single models with different architectures or random initialization states exhibit prediction deviations from ground-truths, resulting in sub-optimal restoration results.

To alleviate this problem, ensemble learning, a traditional but influential machine learning technique, has been applied to image restoration. It involves combining several base models to obtain a better result in terms of generalization and robustness [18; 20; 26; 49; 57]. However, most ensemble methods in image restoration focus on training-stage ensemble requiring the ensemble strategy to be determined while training multiple models, thus sacrificing flexibility of changing models and convenience for plug-and-play usage [31; 35; 40; 44; 46; 54; 59; 70; 76]. In contrast, there is a demand for advanced post-training ensemble methods in the image restoration industry, where researchers still prefer averaging as their primary choice [1; 15; 41; 52; 66; 68; 92].

Despite the industrial demand, post-training ensemble in image restoration is challenging for tradition ensemble algorithms originally designed for classification or regression. Unlike classification and regression, image restoration predictions are matrices with each pixel is correlated with others and range from 0 to 255. As a result, traditional methods like bagging  and boosting  either require enormous computational resources for the restoration task or fail to generalize well due to the imbalance between candidate number and feature dimension. As an alternative, Jiang _et al._ propose a post-training ensemble algorithm for super-resolution by optimizing a maximum a posteriori problem with a reconstruction constraint . However, this constraint requires an explicit expression of the degradation process, which is extremely difficult to define for other restoration tasks beyond super-resolution. It also necessitates prior knowledge of the base models' performance, further limiting its practical application. These issues with both traditional and recent ensemble methods lead researchers in image restoration to prefer weighted averaging as their primary ensemble approach [1; 15; 52; 68; 92].

To this end, we formulate the ensemble of restoration models using Gaussian mixture models (GMMs), where ensemble weights can be efficiently learned via the expectation maximization (EM) algorithm and stored in a lookup table (LUT) for subsequent inference. Specifically, we first base on the Gaussian prior that assumes the prediction error of each model follows a multivariate Gaussian distribution. Based on the prior, the predictions of multiple samples can be appended into a single variable following Gaussian. By partitioning pixels into various histogram-like bins based on their values, we can convert the problem of ensemble weight estimation into various solvable univariate GMMs. As a result, the univariate GMMs can be solved to obtain range-wise ensemble weights by the EM algorithm, with the means and variances of Gaussian components estimated based on the observation priors. We estimate these weights on a reference set and store them in a LUT for efficient inference on the test set. Our method does not require training or prior knowledge of the base models and degradation processes, making it applicable to various image restoration tasks.

Our contributions mainly lie in three areas:

* Based on the Gaussian prior, we partition the pixels of model predictions into range-wise bin sets of mutually exclusive ranges and derive the ensemble of multi-model predictions into weight estimation of various solvable univariate GMMs.
* To solve the univariate GMMs and estimate ensemble weights, we leverage the EM algorithm with means and variances initialized by observed prior knowledge. We construct a LUT to store the range-wise ensemble weights for efficient ensemble inference.
* Our ensemble algorithm does not require extra training or knowledge of the base models. It outperforms existing post-training ensemble methods on 14 benchmarks across 3 image restoration tasks, including super-resolution, deblurring and deraining.

## 2 Related Works

Ensemble Methods.Ensemble methods refer to approaches that fuse the predictions of multiple base models to achieve better results than any of individual model . Traditional ensemble methods include bagging , boosting , random forests , gradient boosting , histogram gradient boosting [14; 34], etc. These methods have been applied to various fields in classification and regression, such as biomedical technology [77; 78], intelligent transportation [53; 84], and pattern recognition [61; 91].

Image restoration.Image restoration, as a thriving area of computer vision, has been making significant progress since the advent of deep learning [10; 65]. Various model architectures have been proposed to address image restoration tasks, such as convolutional neural networks (CNNs) [19; 56; 73; 90], multilayer perceptron (MLPs) , vision Transformers (ViTs) [39; 62; 81], etc. Additionally, various structural designs like multi-scale , multi-patch [64; 89], and progressive learning  have been adopted to improve the representative capacity. It is known that CNNs excel at encoding local features, while ViTs are adept at capturing long range dependencies. Despite this significant progress, single models still generate predictions that deviate from ground-truths, leading researchers in industry to adopt multi-model ensembles to achieve better performance [15; 52; 68; 92].

Ensemble Learning in Image Restoration.Some works incorporate ensemble learning into image restoration by training multiple networks simultaneously and involving ensemble strategy during the training process [2; 3; 11; 12; 13; 16; 17; 21; 29; 31; 33; 35; 36; 37; 38; 40; 42; 43; 44; 45; 55; 59; 63; 70; 72; 75; 76; 79; 80; 85; 87]. However, the majority of them require additional training or even training from scratch alongside ensemble. Only a few focus on ensemble at the post-training stage [31; 66]. Among them, self-ensemble  geometrically augments an input image, obtains super-resolution predictions of augmented candidates and applies averaging ensemble to the candidates, which is orthogonal to the scope of our work. RefESR  requires a reconstruction objective which must get access to the degradation function, prohibiting its application to tasks other than super-resolution. In general, limited works focus on the training-free ensemble of image restoration. There lacks a general ensemble algorithm for restoration despite industry demand [1; 15; 52; 68; 92].

## 3 Proposed Ensemble Method for Image Restoration

In Sec. 3.1, we first present the formulation of the ensemble problem in image restoration. We then formulate our ensemble method in the format of Gaussian mixture models (GMMs) over partitioned range-wise bin sets in Sec. 3.2. We derive the expectation maximization (EM) algorithm with known mean and variance as prior knowledge to solve the GMMs problems in Sec. 3.3

### Ensemble Formulation of Image Restoration

Given a test set \(=\{}_{n},}_{n}\}\) with numerous pairs of input images and ground-truths, suppose we have \(M\) pre-trained base models for image restoration, \(f_{1},...,f_{M}\). For a model \(f_{m}\) where \(m\{1,...,M\}\), its prediction is denoted by \(}_{m,n}=f_{m}(}_{n})\) for abbreviation. We consider the ensemble of the predictions as a weighted averaging, i.e.,

\[}_{n}=_{n}^{T}[}_{1,n} }_{M,n}],\; n\] (1)

where \(}_{n}\) is the ensemble result, and \(_{n}^{M}\) is the vector of weight parameters for the ensemble. The widely-used averaging ensemble strategy in image restoration assigns equal weights for all samples and pixels, i.e., \(_{n}=[]\). A recent method in the NTIRE 2023 competition assigns weights inversely proportional to the mean squared error between the predictions and their average . However, they adopt globally constant weights for all pixels and samples, neglecting that the performances of base models may fluctuate for different patterns and samples.

Alternatively, we start from the prospective of GMMs and assign range-specific weights based on the EM algorithm.

### Restoration Ensemble as Gaussian Mixture Models

Similar to RefESR , suppose we have a reference set \(=\{_{n},_{n}\}_{n=1}^{N}\) with \(N\) pairs of input images and ground-truths. We assume the reference set and test set are sampled from the same data distribution, i.e., \(,\).

For each model \(f_{m}\), its prediction is denoted by \(_{m,n}=f_{m}(_{n})^{3 H W}\). We use \(_{m,n},_{n}^{L}\) to represent the flattened vector of the matrices \(_{m,n},_{n}\), where \(L=3HW\). Based on Gaussian prior, it can be assumed that the estimation error of a model on an image follows a zero-mean Gaussian distribution, namely \(_{m,n}=_{n}-_{m,n}(,_{m,n})\), where \(_{m,n}^{L L}\) is the covariance matrix of the Gaussian. Then the observed ground-truth can be considered following a multivariate Gaussian with the mean equal to the prediction, i.e., \(_{n}|f_{m},_{n}(_{m,n},_{m,n})\).

We can consider the ensemble problem as the weighted averaging of Gaussian variables and estimate the weights by solving its maximum likelihood estimation. However, solving the sample-wise mixture of Gaussian is not feasible because the covariance matrices are sample-wise different and thus hard to estimate. Besides, the number of prediction samples is much fewer than feature dimension, resulting in the singularity of the covariance matrices. Please refer to Sec. A.1 in Appendix for details.

In contrast, we alternatively append the reference set into a single sample following Gaussian as

\[_{1:N}|f_{m},_{1:N}(_{m,1:N}, (_{m,1},...,_{m,N})),\] (2)

where \(_{1:N}=[_{1}\;\;\;\;_{N}] ^{NL}\) and \(_{m,1:N}=[_{m,1}\;\;\;\;_{m,N}] ^{NL}\) are the concatenation of observed ground-truths and restored samples respectively. Since data samples can be considered following _i.i.d_ data distribution \(\), the variance of the concatenated samples is diagonal.

However, the covariance matrix is still singular due to the imbalance between prediction sample number and feature dimension. Thus, directly mixing the multivariate Gaussian is still infeasible to solve. We thus alternatively categorize pixels into various small bins of mutually exclusive ranges such that the pixels within each range can be considered following a univariate Gaussian distribution according to the central limit theorem. Concretely, we separate the prediction range of models into \(T\) bins with each of width \(b\), i.e., \(=\{[0,b),[b,2b),...,[(T-1)b,255]\}\). The upper bound would be \(1\) instead of \(255\) if the value range is within \(\). Given a bin \(B_{m}=[(t-1)b,tb)\) and a pixel of prediction at location \(i\), we define an indicator function \(_{B_{m}}(_{m,1:N}^{(i)})\) such that it returns 1 if \(_{m,1:N}^{(i)} B_{m}\) and 0 otherwise. For multiple models, we have \(M\) bins to form a bin set \((B_{1},...,B_{M})^{M}\), and define the mask map as \(_{r}=_{m=1}^{M}_{B_{m}}(_{m,1:N})\{0,1 \}^{NL}\) where \(r=1,...,T^{M}\). It holds \(_{r=1}^{T^{M}}_{r}=\) and \(_{r=1}^{T^{M}}_{r}=\). For each bin set \((B_{1},...,B_{M})^{M}\), we can select pixels within the bin set from the original image vectors by

\[_{r,1:N}=_{r}_{1:N},\;\;\;\;\;\;\;\;\;\; \;\;\;_{r,m,1:N}=_{r}_{m,1:N},\] (3)

where the operation \(\{\}\) denotes the element-wise product. By the central limit theorem, we assume the nonzero pixels within each bin follow a Gaussian distribution with the mean \(_{r,m,1:N}\) and variance \(_{r,m,1:N}\), i.e.,

\[_{r,1:N}^{(i)}|f_{m},_{r,m,1:N}}{{}}(_{r,m,1:N},_{r,m,1:N}),\;  i[1,...,N_{r}],\] (4)where \(N_{r}\) is the number of nonzero pixels in \(_{r}\) such that \(_{r=1}^{T^{M}}N_{r}=NL\), and the values of \(_{r,m,1:N}\) and \(_{r,m,1:N}\) can be estimated by the mean and variance of \(N_{r}\) prediction pixels within the current bin set.

The reference set is therefore separated into \(T^{M}\) number of bin sets, and the ground-truth pixels inside each of them form a solvable univariate GMM. We then introduce the latent variable \(z\) such that \(z=m\) if the pixel \(_{r,1:N}^{(i)}\) follows the \(m\)-th Gaussian by the model \(f_{m}\). It represents the probability of the pixel belonging to the \(m\)-th Gaussian component, which is equivalent to the role of the ensemble weight for the \(m\)-th base model. By writing \(_{r,m}=P(z=m)\), we have

\[_{r,1:N}^{(i)}=_{z}[_{r,m,1:N}^{(i)}] =_{m=1}^{M}_{r,m}_{r,m,1:N}^{(i)};\;P(_{r,1 :N}^{(i)})=_{m=1}^{M}_{r,m}P(_{r,1:N}^{(i)}|z= m),\] (5)

where \(P(_{r,1:N}^{(i)}|z=m.)=(_{r,1:N}^{(i)}; _{r,m,1:N},_{r,m,1:N})\) is the density function of Gaussian \((_{r,m,1:N},_{r,m,1:N})\).

The value of ensemble weights can be estimated by the maximum likelihood estimates of the observed ground-truths, i.e.,

\[\{_{r,m}\}_{r,m} P(_{1:N}).\] (6)

Because arbitrary two bin sets are mutually exclusive, we can safely split the optimization of maximum likelihood over \(_{1:N}\) into \(T^{M}\) optimization problems of maximum likelihood over \(_{r,1:N}\). Each of them is formulated as

\[_{r,m}*{arg\,max}_{_{r,m}}P(_{r,1:N})= *{arg\,max}_{_{r,m}}_{i=1}^{N_{r}}P(_{r,1:N} ^{(i)}).\] (7)

We have formulated the expression of GMMs for estimating the range-specific ensemble weights.

### Restoration Ensemble via Expectation Maximization and Lookup Table

#### 3.3.1 Weight Estimation via EM Algorithm

For each bin set \((B_{1},...,B_{M})\), we estimate ensemble weights by maximizing the log likelihood as

\[ P(_{r,1:N}) =_{i=1}^{N_{r}}_{m=1}^{M}_{r,m}( _{r,1:N}^{(i)};_{r,m,1:N},_{r,m,1:N})\] \[=_{i=1}^{N_{r}}_{m=1}^{M}_{r,m}( _{r,1:N}^{(i)};_{r,m,1:N},_{r,m,1:N})\] (8) \[_{m=1}^{M}P(z=m|_{r,1:N}^{(i)} )(_{r,1:N}^{(i)};_{r,m,1:N}, _{r,m,1:N})}{P(z=m|_{r,1:N}^{(i)})},\]

We have an E-step to estimate the posterior distribution by

\[_{r,m,1:N} P(z=m|_{r,1:N}^{(i)})= (_{r,1:N}^{(i)};_{r,m,1:N},_{r,m,1:N})}{_{m=1}^{M}_{r,m}(_{r,1:N}^{(i)}; _{r,m,1:N},_{r,m,1:N})}.\] (9)

After that, we have an M-step to obtain the maximum likelihood estimates by

\[_{r,m}}_{i=1}^{N_{r}}_{r,m,1:N}\] (10) \[_{r,m,1:N}^{N_{r}}_{r,m,1:N} (_{r,m,1:N}^{(i)}-_{r,m,1:N})^{2}}{_{n=1}^{N_{r}} _{r,m,1:N}}.\] (11)Thanks to the separation of bin sets, we have prior knowledge of the mean and variance of each model, which can be estimated and initialized by

\[_{r,m,1:N} }_{i=1}^{N_{r}}_{r,m,1:N}^{(i)}\] (12) \[_{r,m,1:N} }\|_{r,m,1:N}-_{r,m,1:N} \|_{2}.\] (13)

The complete and detailed derivation of the EM algorithm can be found in Sec. A.3 of Appendix.

#### 3.3.2 Lookup Table and Inference

We store the range-specific weights estimated on the reference set into a LUT with each key of \((B_{1},...,B_{M})\). During the inference stage for a test sample \(}_{n}\), we have the prediction of the \(m\)-th base model as \(}_{m,n}=f_{m}(}_{n})\). For a bin set \((B_{1},...,B_{M})\), we partition input pixels of multiple models into each bin set as

\[}_{r,m,n}=_{r}}_{m,n},_{r}=_{m=1}^{M}_{B_{m}}(}_{m,n}).\] (14)

Then we retrieve the estimated range-wise weights \(_{r,m}\) from the LUT based on each key of the bin set and obtain the aggregated ensemble by

\[}_{n}=_{r=1}^{T^{M}}}_{r,n}=_{r=1}^ {T^{M}}_{m=1}^{M}_{r,m}}_{r,m,n}.\] (15)

The main algorithm can be found in Algo. 1 and the EM algorithm with known mean and variance priors is shown in Algo. 2.

## 4 Experiment

### Experimental Settings

``` Input:\(_{r,1:N},_{r,1,1:N},...,_{r,M,1:N}\) Output:\((_{r,1},...,_{r,M})\)
1\(N_{r}\) number of nonzero pixels in \(_{r,1:N}\);
2 Initialize \(_{r,m,1:N}\) by Eq. 12;
3 Initialize \(_{r,m,1:N}\) by Eq. 13;
4whilenot convergedo
5for\(i[1,N_{r}]\)do
6for\(m[1,M]\)do
7 Update \(_{r,m,1:N}\) by Eq. 9;
8 end for
9
10 end for
11for\(m[1,M]\)do
12 Update \(_{r,m}\) by Eq. 10;
13 Update \(_{r,m,1:N}\) by Eq. 11;
14 end for
15
16 end for ```

**Algorithm 2**MPEM: EM algorithm with known Mean Prior

Benschmarks.We evaluate our ensemble method on 3 image restoration tasks including super-resolution, deblurring, and deraining. For super-resolution, we use Set5 , Set14 , BSDS100 , Urban100  and Manga109  as benchmarks. For deblurring, we use GoPro , HIDE , RealBlur-J and -R . For deraining, we adopt Rain100H , Rain100L , Test100, Test1200 , and Test2800 .

Metrics.We use peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)  to quantitatively evaluate the image restoration quality. Additionally, we compare the average runtime per image in seconds to evaluate the ensemble efficiency. Following prior works , PSNR and SSIM are computed on Y channel of the YCbCr color space for image super-resolution and deraining.

[MISSING_PAGE_FAIL:7]

### Experimental Results

#### 4.2.1 Ablation Study

We conduct ablation studies on the bin width \(b\) and the maximum step number of the EM algorithm on the benchmark of Rain100H . The results are shown in Tab. 1 and Tab. 2 respectively. We can observe that the bin width involves a trade-off between ensemble accuracy and the efficiency. A small \(b\) generates better ensemble results but makes ensemble inference slower. In the following experiments, we choose the bin width \(b=32\) by default for the balance of efficiency and performance. On the other hand, a large maximum step number does not necessarily yield better ensemble result. Therefore, we choose 1000 as the maximum step number.

#### 4.2.2 Quantitative Results

We present quantitative comparisons with existing ensemble methods in Tab. 3 for super-resolution, Tab. 4 for deblurring, and Tab. 5 for deraining. Our method generally outperforms all existing methods across the 3 image restoration tasks and 14 benchmarks. Note that Average and ZZPM  generally perform better than regression-based ensemble methods. However, in cases where one of the base

    & Datasets &  &  &  &  \\   & Metrics & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\   & MPRNet  & 32.658 & 0.9362 & 30.962 & 0.9188 & 33.914 & 0.9425 & 26.515 & 0.8240 \\  & Restormer  & 32.918 & 0.9398 & 31.221 & 0.9226 & 33.984 & 0.9463 & 26.626 & 0.8274 \\  & DGUNet  & 33.173 & 0.9423 & 31.404 & 0.9257 & 33.990 & 0.9430 & 26.583 & 0.8261 \\   & Bagging  & 33.194 & 0.9418 & 31.437 & 0.9250 & 34.033 & 0.9456 & 26.641 & 0.8277 \\  & AdaBoost  & 33.205 & 0.9412 & 31.449 & 0.9251 & 34.035 & 0.9455 & 26.652 & 0.8280 \\  & RForest  & 33.173 & 0.9416 & 31.439 & 0.9247 & 34.039 & 0.9457 & 26.647 & 0.8280 \\  & GBDT  & 33.311 & 0.9418 & 31.568 & 0.9256 & 34.052 & 0.9465 & 26.684 & 0.8285 \\  & HGBT  & 33.323 & 0.9427 & 31.583 & 0.9267 & 33.986 & 0.9436 & 26.684 & 0.8296 \\   & Average & 33.330 & 0.9436 & 31.579 & 0.9277 & 34.090 & 0.9471 & 26.689 & **0.8309** \\  & ZZPM  & 33.332 & 0.9436 & 31.580 & 0.9277 & 34.057 & 0.9468 & 26.688 & 0.8308 \\    & EnsIR (Ours) & **33.345** & **0.9438** & **31.590** & **0.9278** & **34.089** & **0.9472** & **26.690** & **0.8309** \\   

Table 4: The ensemble results on the task of _image deblurring_. The categories of “Base”, “Regr.” and “IR.” in the first column mean base models, regression-based ensemble methods, and those ensemble methods designed for image restoration. The best and second best ensemble results are emphasized in **bold** and underlined respectively.

    & Datasets &  &  &  &  &  \\   & Metrics & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\   & MPRNet  & 30.428 & 0.8905 & 36.463 & 0.9657 & 30.292 & 0.8983 & 32.944 & 0.9175 & 33.667 & 0.9389 \\  & MAXIM  & 30.838 & 0.9043 & 38.152 & 0.9782 & 31.194 & 0.9239 & 32.401 & 0.9240 & 33.837 & 0.9438 \\  & Restormer  & 31.477 & 0.9054 & 39.080 & 0.9785 & 32.025 & 0.9237 & 33.219 & 0.9270 & 34.211 & 0.9449 \\   & Bagging  & 31.461 & 0.9001 & 39.060 & 0.9782 & 31.865 & 0.9107 & 33.115 & 0.9152 & 34.216 & 0.9446 \\  & AdaBoost  & 31.472 & 0.9006 & 39.067 & 0.9782 & 31.866 & 0.9112 & 33.117 & 0.9153 & 34.221 & 0.9443 \\  & RGForest  & 31.492 & 0.9012 & 39.089 & 0.9784 & 31.900 & 0.9127 & 33.147 & 0.9169 & 34.224 & 0.9447 \\  & GBDT  & 31.581 & 0.9058 & 39.044 & 0.9778 & 32.001 & 0.9236 & 33.276 & 0.9274 & 34.211 & 0.9446 \\  & HGBT  & 31.698 & 0.9075 & 39.115 & 0.9784 & 31.988 & 0.9241 & 33.305 & 0.9282 & 34.229 & 0.9450 \\   & Average & 31.681 & 0.9091 & 38.675 & 0.9770 & 31.626 & 0.9225 & 33.427 & 0.9286 & 34.214 & 0.9449 \\  & ZZPM  & 31.689 & 0.9091 & 38.725 & 0.9771 & 31.642 & 0.9227 & 33.434 & 0.9286 & 34.231 & 0.9450 \\    & EnsIR (Ours) & **31.739** & **0.9095** & **39.216** & **0.9792** & **32.064** & **0.9258** & **33.445** & **0.9289** & **34.245** & **0.9451** \\   

Table 5: The ensemble results on the task of _image deraining_. The categories of “Base”, “Regr.” and “IR.” in the first column mean base models, regression-based ensemble methods, and those ensemble methods designed for image restoration. The best and second best ensemble results are emphasized in **bold** and underlined respectively.

models significantly underperforms compared to the others, such as MPRNet  on Rain100L  and Test100 , these regression methods outperform Average and ZZPM . In contrast, our method, which learns per-value weights, can recognize performance biases and alleviate such issue. ZZPM  performs comparably to Average in our experiments rather than outperforming it, because the base models are not always equally good and one model may be consistently better than the others. Thus, weights negatively proportional to the mean squared error may exaggerate deviation from optimal prediction. In contrast, our method consistently performs well for all cases.

#### 4.2.3 Qualitative Results

We also provide qualitative visual comparisons in Fig. 1, 2 and 3. In Fig. 1, the base model SwinIR  mistakenly upscales the character's eye. While existing ensemble algorithms partially alleviate this mistake, they cannot fully discard the hallucinated line inside the eye. In contrast, our method with the bin width \(b=32\) that learns fine-grained range-wise weights successfully recovers the pattern. In Fig. 2, only our method can effectively obtain a better ensemble with sharp edges and accurate colors. In Fig. 3, we can observe that MPRNet  removes all stripe patterns on the ground together with rain streaks. The conventional weighted summation yields a dimmer ensemble result, and the HGBT method fails to learn accurate weight distributions, resulting in an unsmooth result. In contrast, ours alleviates the issue. More visual comparisons are provided in Fig. 7-18 in Appendix.

#### 4.2.4 Extensions

Efficiency.The efficiency of ensemble methods is compared by measuring the average runtime on Rain100H , as shown in Tab. 6. Although our method is slower than Average and ZZPM ,

   Method & Bagging  & AdaBoost  & RForest  & GBDT  & HGBT  & Average & ZZPM  & Ours \\  Runtime & 1.0070 & 1.1827 & 9.8598 & 1.2781 & 0.1773 & 0.0003 & 0.0021 & 0.1709 \\   

Table 6: The average runtime per image in seconds of the ensemble methods on Rain100H .

Figure 3: A visual comparison of ensemble on an image from Test100  for the task of image deraining. “GT & LQ” means ground-truth and low quality rainy images. The second line of (c)-(g) are error maps. Please zoom in for better visual quality.

Figure 2: A visual comparison of ensemble on an image from GoPro  for the task of image deblurring. “GT & LQ” means ground-truth and low quality blurry images. The second line of (c)-(g) are error maps. Please zoom in for better visual quality.

it is much faster than all the regression-based methods. By slightly sacrificing performance with \(b=64\), it can achieve real-time inference, as indicated in Tab. 1.

Visualization.We also present the visualization examples of ensemble weights, image features and pixel distributions in Fig. 4-6 in Appendix. Due to page limit, please refer to Sec. B of Appendix.

Limitation and Future Work.The trade-off between the runtime and performance has not been solved yet. Achieving real-time ensemble would lead to performance degradation. The issue could be resolved by GPU vectorization acceleration and distributed computing. Additionally, if all base models fail, ensemble methods cannot generate better result. We leave them in the future work.

## 5 Conclusion

In this paper, we propose an ensemble algorithm for image restoration based on GMMs. We partition the pixels of predictions and ground-truths into separate bins of exclusive ranges and formulate the ensemble problem using GMMs over each bin. The GMMs are solved on a reference set, and the estimated ensemble weights are stored in a lookup table for the ensemble inference on the test set. Our algorithm outperforms regression-based ensemble methods as well as commonly used averaging strategies on 14 benchmarks across 3 image restoration tasks, including super-resolution, deblurring and deraining. It is training-free, model-agnostic, and thus suitable for plug-and-play usage.