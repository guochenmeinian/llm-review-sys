# Human-Object Interaction Detection Collaborated

with Large Relation-driven Diffusion Models

 Liulei Li\({}^{1}\),  Wenguan Wang\({}^{2}\),  Yi Yang\({}^{2}\)

\({}^{1}\)ReLER, AAII, University of Technology Sydney \({}^{2}\)CCAI, Zhejiang University

https://github.com/Olliuliei/DiffusionHOI

Corresponding Author: Wenguan Wang.

###### Abstract

Prevalent human-object interaction (HOI) detection approaches typically leverage large-scale visual-linguistic models to help recognize events involving humans and objects. Though promising, models trained via contrastive learning on text-image pairs often neglect mid/low-level visual cues and struggle at compositional reasoning. In response, we introduce DiffusionHOI, a new HOI detector shedding light on text-to-image diffusion models. Unlike the aforementioned models, diffusion models excel in discerning mid/low-level visual concepts as generative models, and possess strong compositionality to handle novel concepts expressed in text inputs. Considering diffusion models usually emphasize instance objects, we first devise an inversion-based strategy to learn the expression of relation patterns between humans and objects in embedding space. These learned relation embeddings then serve as textual prompts, to steer diffusion models generate images that depict specific interactions, and extract HOI-relevant cues from images without heavy fine-tuning. Benefited from above, DiffusionHOI achieves SOTA performance on three datasets under both regular and zero-shot setups.

## 1 Introduction

As a crucial topic in the field of visual scene understanding, human-object interaction (HOI) detection demands not only inferring the semantics and locations of entities but also should comprehend the ongoing events happening between them[1; 2]. Given the complexity and diversity of human activities in object-rich realistic scenes, this task presents challenges in long-tailed distributions and zero-shot discovery. A set of studies seek to tackle these two issues by leveraging large-scale visual-linguistic models (_e_.\(g\)., CLIP) which show strong generalization ability on dozens of tasks. Though strides made, it has been observed that models trained by aligning high-level text-image semantics face difficulties in discerning spatial locations, and struggle at compositionality which is a fundamental ability for human to capture new concepts by combining known parts. In fact, both middle-level visual cues (_e_.\(g\)., spatial relation) and compositionality are essential facets for HOI detection. The former can help deduce feasible interactions according to locations between instances, while compositionality contributes significantly to zero-shot generalization. For example, we can easily understand human-hold-horse by composing human-hold-dog and class horse that have encountered previously.

In contrast, the text-to-image diffusion models [7; 8; 9; 10; 11; 12; 13; 14] also pre-trained on large-scale image-text pairs, are demonstrating superior capabilities outperforming models like CLIP. Concretely, they are able to generate diverse high-quality images conditioned on textual inputs, showing proficiency in understanding _high-level semantics_[15; 16]. In addition, the generated images convey reasonable shape, texture, layout, and structure, indicating the comprehension in _mid/low-level visual concepts_ as generative models. More importantly, the descriptions are typically organized in a compositionalmanner, with phrases such as "happy","near a bridge", or "hugged by a man" continually appended to objects like "a dog". This suggests that diffusion models inherently possess _compositionality_, to systematically adapt to newly encountered user requirements by composing known visual concepts.

The above analysis motivates us to explore diffusion models for HOI detection. Nonetheless, to fully unlock the potential of diffusion models and accommodate the unique characteristic of HOI detection task, the following questions naturally arise: \(\) With diffusion models typically emphasizing instance generation, how to steer it to prioritize the relationships between humans and objects? \(\) How to transfer the extensive knowledge obtained from large-scale pre-training in diffusion models to assist the recognition of interactions? To address \(\), we harness textual inversion which conceptualizes a user-provided object by inverting it to a text embedding. However, this method focuses solely on instance objects. To facilitate a smooth shift from object-centric to _relation-centric_ modeling, we devise a human-object relation inversion strategy grounded in the disentanglement of HOI. Concretely, given the HOI latent describing human-action-object, we build a cycle-consistency objective to reconstruct it from a intermediate relation latent derived from the original HOI latent. This reconstruction process is guided by a set of learnable relation embeddings as text prompts, for which we use the placeholder \(R_{z}\) to denote the textual form before encoded into embedding space. These relation embeddings further involves in a relation-centric contrastive learning to enhance the awareness of high-level relational semantics. To answer \(\), we leverage both the text promoted image generation and conditioned feature extraction abilities of diffusion models. We realize _relation-driven_ image generation by compositionally organizing \(R_{z}\) with other linguistic elements to formulate new text prompts (Fig. 1(b)). This allows for the generation of novel interactions with unseen objects, and extends the training set for HOI detectors. Moreover, we directly utilize diffusion models as backbone to extract HOI-relevant features conditioned on \(R_{z}\) (Fig. 1(c)). After a single noise-free forward step, features distinct for each interaction can be obtained. Finally, to establish a loop for mutual boosting between above _relation-inspired_ HOI detection and relation modeling, we devise an online update strategy to facilitate the continual evolving of relation embeddings during HOI detection learning.

Benefited from controllable image generation and knowledge transfer from diffusion models, our method named DiffusionHOI enjoys several appealing advantages: **First**, it steers diffusion models to focus on complex relationships rather than single objects in an efficient way. This offers a robust foundation for HOI modeling. **Second**, from the perspective of relation-driven, it unlocks the image generation power of diffusion models tailored for the HOI detection task. This enriches the pool of training samples, particularly for long-tailed/unseen interaction classes. **Third**, the relation-inspired prompting improves both the flexibility and accuracy of HOI detectors. It adapts to each individual image to extract action or object related cues, while CLIP-based methods[3; 19] produce action/object features merely from texts (_i.e_., Fig. 1(a)), remaining static and unresponsive to image content.

By embracing text-to-image diffusion models as well as facilitating relation-driven image generation and prompting, our method demonstrates superior performance. It surpasses all top-leading solutions on HICO-DET and V-COCO, and sets new state-of-the-arts. In addition, it yields up to **6.43%** mAP improvements on SWiG-HOI under the zero-shot HOI discovery setup. These promising performance evidences the great potential of integrating diffusion models for visual relation understanding. We hope this work could foster the broader exploration of large-scale pre-trained diffusion models on more computer vision tasks beyond mere image generation.

## 2 Related Work

**Human-Object Interaction Detection.** According to the architecture design of networks, existing solutions for HOI detection can be broadly categorized into two groups: one-stage and two-stage.

Figure 1: Existing solutions utilize mere linguistic knowledge (a). Our solution utilizes both text-prompt image generation (b) and conditioned feature extraction (c) abilities of diffusion models for knowledge transfer.

The one-stage methods [23; 24; 25; 26] typically employ a multi-task learning pipeline that jointly undertake the tasks of human-object detection and interaction classification in an end-to-end manner, therefore distinguished by fast inference. In contrast, two-stage methods[27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37] first detect entities with off-the-shelf detectors such as Faster R-CNN, and the predict the dense relationships among possible human-object pairs. This paradigm effectively disentangles the HOI detection process and results in improved performance. Inspired by DETR, recent advancements shift to adopt Transformer-based architectures [3; 40; 41; 42; 43; 44; 45; 46]. Several studies [3; 47; 48; 49; 50; 51] also supplement the Transformer-based HOI detectors with large-scale visual-linguistic models like CLIP  or visual knowledge [52; 53] to conduct logic-induced reasoning . However, these models focus solely on aligning high-level semantics and overlooking mid/low-level visual cues. To tackle this, we redirect our attention to diffusion models, which perfectly address the aforementioned challenges and possess the capacity to handle previously unseen concepts through their strong compositionality.

**Controllable Image Generation.** To facilitate customized image generation with respect to predefined class, attribute, text or image, various approaches based on GANs have been proposed. For instance, [57; 58] develop a photo realistic hairstyle transfer method through latent space optimization. However, these methods typically show limited diversity when compared to likelihood-based models. In response, diffusion models [60; 61; 62] have emerged that not only demonstrate remarkable synthesis quality but also offer enhanced controllability. The core idea behind is to transform a simple and known distribution (_e.g._, Gaussian) into the desired data distribution. These models have proven to be highly effective in various conditional scenarios. According to the conditional targets, the prevalent work can be grouped into class-driven [63; 64] text-driven [7; 8], exemplar image-driven[65; 66], _etc._. These advances have found application in a wide range of domains such as super resolution[66; 67], image editing[68; 13]. Recently, a new approach achieves guided image generation by learning a single word embedding through a frozen text-to-image model to properly describe the desired target objects. Take inspiration from it, we achieve relation-driven image generation by extending such object-centric concept modeling approach to relation-centric.

**Knowledge Transfer from Diffusion Models.** In light of the notable success achieved by diffusion models in applications, there is a growing interest in transferring knowledge acquired from large-scale pre-training to various tasks [17; 69; 70; 71; 72; 73; 74; 75]. For example, given the limited availability of data for constructing NeRFs and the unprecedented generalizability of diffusion models, researchers are motivated to explore generating 3D NeRFs via a 2D text-to-image diffusion model using diverse input text[69; 70; 71]. More recently, a notable trend has emerged where efforts are dedicated towards learning semantic representations from diffusion models by extracting intermediate feature maps. It finds diverse application in image segmentation, semantic correspondence learning[72; 73; 74], and general representation learning. In this work, we extensively harness both the image generation and semantic representation abilities of diffusion models, by using relation-centric embeddings to control the generation and prompt semantic extraction from images with respect to specific interactions.

## 3 Methodology

### Preliminary: Textual Inversion

Latent diffusion models  represent an evolution of diffusion models which offer significant enhancements in both computational and memory efficiency by executing denosing in the latent space. It comprises two primary components. The first is a pre-trained generator equipped with an encoder \(\) to map the input image \(x\) into a latent vector \(=(x)\), from which the original data can be reconstructed via a decoder \(\) by \(=() x\). The second is a diffusion model to generate latent codes \(\) conditioned on user guidance \(y\) which can be text, image, _etc._The latent codes then serve as inputs to \(\) for image generation _w.r.t._\(y\). The training objective is given as:

\[_{}:=_{(x),y, (0,1),t}\|-_{}(_{t},t,c_{}(y))\|_{2}^ {2},\] (1)

where \(c_{}\) is a conditioning model to encode \(y\), \(_{t}\) is the noised latent at time \(t\), \(\) is sampled noise, \(_{}\) is the denoising network. Based on latent diffusion models, inversion-based diffusion seeks to learn a text embedding \(v_{*}\) that accurately describes novel concepts in user provided images. This is achieved by optimizing \(v_{*}\) with Eq. 1 to iteratively reconstruct the latent code \(\) of user provided images with text prompts \(y\) like "an image of \(S_{*}\)", where \(S_{*}\) is the placeholder of new concept:

\[v_{*}=*{arg\,min}_{v}_{(x),y, (0,1),t}\|-_{}(_{t},t,c_{}(y ))\|_{2}^{2}.\] (2)As such, it enables image generation _w.r.t._ target concepts in diverse scenes by using the learned embedding \(v_{*}\) to replace the tokenized placeholder \(S_{*}\) in text prompts.

### Inversion-Based HOI Modeling

**Disentanglement-based Relation Embedding Learning.** To facilitate above inversion technology for relation modeling, two options present: **i)** directly optimizing embeddings describing interactions (_i.e._, human-action-object), which risks overfitting with limited samples for long-tailed categories and cannot generalize to novel concepts, and **ii)** learning action embeddings with diverse images sharing a common action but different objects, which seems feasible but poses significant convergence issues due to the complex content, and the optimization target cannot be fixed to actions but not other unrelated elements. In contrast, drawn from the compositional nature of HOI, we adopt a disentangled solution (_i.e._, Fig. 2) where HOI triplets are broken into human-action and object. Here human-action is considered to describe the relation between human and object, as action is executed by and strictly adheres to human involved. Then, denoting the text describing human-action as \(R_{*}\), encoded relation embeddings as \(v_{*}^{}=c_{}(R_{*})\), and the latent of one happening HOI in image as \(^{}\), a relation latent \(_{0}^{}\) could be reconstructed (_i.e._, denoising with \(_{}\) from time \(T\) to 0) by:

\[_{}((^{}-^{})_{T},T,v_{*}^{ })_{0}^{}.\] (3)

Here \((*)_{T}\) is the noised version at time \(T\), and \(^{}\) is retrieved by encoding the cropped object from image with provided bounding box annotations. We consider \(^{}-^{}\) is able to describe the human-action component by subtracting the object from human-action-object. Then, we can reconstruct the latent representing the complete HOI image by adding \(^{}\) back to \(_{0}^{}\):

\[_{}((_{0}^{}+^{})_{T},T,[v_{*}^ {};_{o}])_{0}^{},\] (4)

where \(_{o}\) is the CLIP encoded text embedding of object, and it is combined with the relation embedding \(v_{*}^{}\) to generate the prompt that describes the entire HOI image. In this way, with only one learnable relation embedding (_i.e._, \(v_{*}^{}\)), we build a cycle to generate relation latent \(_{0}^{}\) from the HOI image latent \(^{}\), and subsequently, the original HOI image latent is reconstructed from the generated relation latent. The learning of \(v_{*}^{}\) can be supervised without human annotation, but just ensuring the consistency between the original HOI latent and the reconstructed one:

\[_{}=||_{2}(^{})-_{2}( {z}_{0}^{})||_{2}^{2},\] (5)

where all latents are \(_{2}\)-normalized for improved training stability . Through such a disentanglement-based relation modeling and cycle-consistency training, the optimization objective become clearer and easier to learn. It enables using same action from different interactions to enhance the comprehension of a relation, and generalizing to new interactions by combining it with other object.

**Relation-Centric Contrastive Learning.** Eq. 5 is a pixel-level reconstruction loss which prioritizes aligning low-level cues. We supplement it with a relation-centric contrastive loss to enhance the awareness of high-level semantics. Instead of directly engaging learning with relation latents, we combine them with object latents to form new HOI latents, thus significantly enriching the diversity of samples:

\[ =_{0}^{}+^{}, ^{+} =_{0}^{}+^{},\] (6) \[_{}^{-} =_{0}^{}+_{k}^{}, _{}^{-} =_{0,i}^{}+_{j}^{},\]

where \(\) is the anchor sample, \(^{+}\) is the positive sample composed of a different object latent \(^{}\) sharing the same class as \(^{}\). Conversely, \(_{}^{-}\) and \(_{}^{-}\) are negative samples, with \(_{}^{-}\) composed

Figure 2: (Left) Disentanglement-based cycle-consistency learning. (Right) Relation-centric contrastive learning.

of a different class object latent \(^{}\) compared to \(\), and \(^{-}_{}\) composed of any other relation latent \(^{}_{0}\) and arbitrary object latent \(^{}\). The final optimization objective is given as:

\[_{}=-^{+}/)}{ (^{+}/)+_{k}(^{-}_{}/)+_{i}_{j}(^{-}_{}/)},\] (7)

to optimize \(v^{}_{*}\) which involves in reconstructing \(^{}_{0}\). \(=0.07\) is the temperature parameter.

### Relation-Driven Sample Generation

**Text Prompts Preparation.** We harness the captions provided in the MS COCO Caption dataset  to generate diverse prompts. Compared to text synthesized by GPT-4, these captions are more precise and closer to real visual scenes as they are annotated by human subjects. The preparation initiates with a filtration where captions not containing pronouns indicating human (_e.g._, man, woman, boy) or action words are removed. To further enrich the diversity of prompts, given two randomly selected sentences that share the same action, we exchange the clauses following the action word. Prompts are exclusively generated with GPT-4 only when actions or objects not present in COCO Caption. This results in 33,834 text prompts in total. Finally, action words in prompts are replaced with placeholders corresponding to learned relation embeddings, so as to empower the diffusion model with enhanced awareness of relation patterns between human and object during generation.

**Image and Annotation Generation.** Denoting text prompts as \(=\{_{1},,_{N}\}\), we aim to construct a dataset \(=\{(_{1},_{1}),,(_{N}, _{N})\}\) where \(_{i}\!\!^{H W 3}\) represents the synthesized image and \(_{i}\!=\!\{^{i}_{i},^{o}_{i},^{o}_{ i},^{o}_{i}\}\) is the pseudo annotation containing bounding boxes \(^{h}_{i}\) for human, \(^{o}_{i}\) for object, and class labels \(^{o}_{i}\) for object, \(^{a}_{i}\) for action. For the generation of \(_{i}\), the text prompts \(_{i}\) is first encoded by CLIP text encoder to obtain the conditioning vector \(_{i}\!=\!c_{}(_{i})\!\!^{d}\), where the placeholder string is indirectly replaced with relation embedding \(v^{}_{*}\). Then, a random sampled noise tensor \(_{T}\!\!^{h w d}\) is iteratively denoised to yield a new latent \(_{0}\). \(_{i}\) is generated by a single pass through \(\), _i.e._, \(_{i}=(_{0})\). For the generation of \(_{i}\), \(^{o}_{i}\) and \(^{a}_{i}\) can be easily determined by referring to the action and object words in \(_{i}\), while \(^{h}_{i}\) and \(^{o}_{i}\) are derived from the cross-attention maps computed within the U-shape denoising network \(_{}\). Specifically, to effectively tackle various input modalities, \(_{}\) is equipped with cross-attention mechanisms in each layer to inject \(_{i}\) into \(\) conforming to the similarity between them. For the \(l\)-th layer at the last denoising step \(0\), the cross-attention map is computed as: \(^{l}_{i,0}\!=\!(_{0}\!\!^{}_{i}/ )\!\!^{h w}\). According to prior work [17; 78], here \(^{l}_{i,0}\) signifies the correspondence between text prompt \(_{i}\) and regions in generated image. Thus, we explicitly concatenate words describing human and object with \(_{i}\) (_i.e._, \([_{i};word_{};word_{}]\)), resulting in a new text embedding \(}_{i}\!\!^{d 3}\) and corresponding cross-attention maps \(}^{l}_{i,0}\!\!^{h w 3}\) where the last two items along the third dimension channel are probability maps of human and object. Finally, we leverage the implementation in weakly supervised object localization to outline bounding boxes from these probability maps.

### HOI Knowledge Transfer from Diffusion Models

While prior studies [48; 49; 50; 3] have investigated knowledge transfer from visual-linguistic models such as CLIP, they utilize visual knowledge solely during training. The prediction relies on a confined set of CLIP encoded word embeddings, which leads to limited knowledge transfer and rigid inference unresponsive to image content. In contrast, we propose directly leveraging diffusion models as the feature extractor and build HOI detector on this basis. Moreover, given the conditioning property of diffusion model, relation embeddings can serve as text prompts to guide the retrieval of interaction-relevant visual cues from images, further benefiting HOI detection.

**HOI Detector Built Upon Diffusion Models.** Pioneering studies [17; 78; 80] have empirically demonstrated that the output of frozen text-to-image diffusion models possesses rich visual features to tackle complex perception tasks. Next we illustrate how to build a HOI detector on this basis. As shown in Fig. 3, our method is a one-stage solution composed of: a visual encoder with diffusion models serving as the backbone, and a HOI decoder consisting of two parallel decoders for instance and interaction detection. For the visual encoder, given an image \(\), it is encoded into latent space with the encoder \(\) of a pre-trained generator (_e.g._, VQGAN): \(=()\). Then, \(\) is fed into \(_{}\) through a single noise-free forward pass to derive text conditioned features: \(_{}(,T,c_{}(y))\{^{l}_{T}\}_{l=1}^{4}\). Allscales of features are aggregated with FPN, yielding \(_{r}^{}\) in a downsampling factor of 32. The architecture of HOI decoder is similar to GEN-VLKT. Concretely, the instance decoder \(_{}\) employs a set of human queries \(\{_{h}^{i}\}_{i=1}^{N_{q}}\) and object queries \(\{_{o}^{j}\}_{j=1}^{N_{q}}\), and considers those at the same index (_i.e._, \(i=j\)) as a pair to initialize queries \(_{r}\) for the interaction decoder \(_{}\). In fact, the HOI decoder can be replaced with any other one-stage models. We do not claim the detector architecture as the contribution, but focus on how to derive HOI-relevant feature to assist in HOI detection.

**Relation-Inspired HOI Detection.** As the relation embeddings are optimized towards modeling the interactions between human and object, we use them as conditions to inspire the extraction of HOI-relevant cues. This can eliminate the potential domain gap between general-purpose diffusion models and the downstream HOI detection task. Specifically, all feasible HOI phrases (_e.g._, "human feed horse") are encoded with CLIP text encoder into embedding space and concatenated together, with the human-action component replaced with learned relation embeddings (_e.g._, "\(_{*}\) horse"). This results in HOI prompts \(_{r}^{N_{r} d_{l}}\) which further participates into the cross-attention in \(_{}\) via:

\[_{T}^{l}=_{T}^{l}+_{r}^{l}_{_{r}} ^{_{l}},_{r}^{l}=(_{T}^{l}_{_{r}}^{}/)^{h  N_{r}},\] (8)

where \(_{_{r}}\) and \(_{_{r}}\) are key and value embeddings projected from \(_{r}\). As seen, \(_{r}\) contributes to: **i)** encourage the denoising network \(_{}\) to extract visual features \(_{T}^{l}\)_w.r.t._ HOI prompts, and **ii)** guide the derivative of cross-attention maps \(_{r}^{l}\) in response to ongoing interactions in \(\). The final interaction maps are computed as the average value of \(\{_{r}^{l}\}_{l=1}^{4}\). We also derive cross-attention maps for human \(_{h}\) and object \(_{o}\) in a similar way as \(_{r}\), by without update to \(_{T}^{l}\). Then, these cross-attention maps are used to initialize queries from the aggregated visual feature \(_{T}^{l}\) via mask pooling:

\[}_{r}=(_{T}^{},_{r}^{k}), }_{o}=(_{T}^{},_{o}), }_{h}=(_{T}^{},_{h}).\] (9)

Note we conduct Hungarian matching between \(}_{r}^{k}\) and \(}_{o}^{i}+}_{h}^{i}\), so as to arrange HOI, and combined human-object queries that are most similar to the same index in their respective query lists. Following, the classification for interaction and instance are jointly supervised by:

\[_{}&=( (}_{r}_{r}/_{r}),y_{r})+(((}_{r})),y_{r}),\\ _{}&=((}_{o}_{o}/_{o}),y_{o})+(((}_{o})),y_{o}),\] (10)

where \(y_{r}\) and \(y_{o}\) are ground truth for interaction and object categories, \(}_{r}\) and \(}_{o}\) are queries after decoding through \(_{}\) and \(_{}\), CE and FFN denote the cross entropy loss and feed-forward network. Beyond the score delivered by conventional linear classifier (_i.e._, \(((}_{o}))\)), here \((}/)\) with learnable parameters \(_{r}\) and \(_{o}\) computes the similarity between decoded queries and conditioning prompts, thereby facilitating the recognition for unseen categories.

**Online Update for Relation Embedding.** To enable the continual evolution of relation embeddings \(v_{*}^{}\) throughout the supervised HOI detection learning, an additional loss considering the compositional nature of HOI is devised. Specifically, we concatenate all \(N_{a}\) relation embeddings into a new prompt \(_{a}\), from which a set of relation query embeddings \(}_{a}\) can be initialized in the same way as \(}_{o}\) (_c.f._, Eq.9). In addition, another set of embeddings describing relations can be derived from \(}_{r}\) and \(}_{o}\) by: \(}_{a}=}_{r}-}_{o}\). The goal is to align \(}_{a}\) directly derived from visual features with relation embeddings as conditions, and \(}_{a}\) computed from interaction and object queries after decoding:

\[_{}=||_{2}(}_{a})-_{2}(}_{ a})||_{2}^{2}.\] (11)

Here \(_{}\) solely optimizes \(v_{*}^{}\) to render a mutual boost between HOI detection and relation embedding learning. Concretely, enhanced relation embeddings inspires improved HOI feature discovery, and in turn, the more precise query decoding benefits the update of relation embeddings.

Figure 3: The overall pipeline of DiffusionHOI. See ยง3.4 for details.

### Implementation Details

**Network Architecture.** DiffusionHOI is built upon Stable Diffusion v1.5 with xFormers  installed. The denoising UNet \(_{}\) receives input latents at a downsampling factor of 1/8, with four encoder blocks output feature at a size of \(1/2^{l+3}\) where \(l\) is the block index. For the final visual feature \(^{}\) after FPN aggregation, it is interpolated to a size of 1/32 and then projected to 256 channels to enhance computing efficiency. Both \(_{}\) and \(_{}\) consist of six Transformer decoding layers with hidden dimension of 768. The query number \(N^{q}\) is uniformly set to 64 for both \(_{}\) and \(_{}\).

**Training Objective.** The inversion-based HOI modeling is jointly optimized by two embedding learning losses: \(_{}=_{}+_{1} _{}\), where \(_{1}[0,0.2]\) is scheduled following a cosine annealing policy. For HOI detection learning, we follow DETR  to match predictions and ground truths with Hungarian algorithm. Denoting the bounding box detection loss as \(_{}\), the final training objective is given as: \(=_{}+_{}+_{ }+_{2}_{}\) where \(_{2}\) is fixed to 0.5.

## 4 Experiment

### Experimental Setup

**Datasets.** We conduct extensive experiments on three datasets.

* HICO-DET  is a large-scale HOI detection benchmark with 38,118/9,658 images for training/testing, respectively. This dataset includes 80 object categories as in MS-COCO  and 117 action categories, formulating a rich vocabulary of 600 human-object interactions in total.
* V-COCO  is a curated subset of MS-COCO  including 2,533/2,867/4,946 images in train/val/ test sets. It also contains 80 object categories from MS-COCO  and a much smaller set of 29 action classes, resulting in a total of 263 human-object interactions.
* SWIG-HOI  is assembled from SWIG  and DOH  with about 45,000/14,000 for training/testing. This dataset covers 406 human actions and 1,000 object categories.

**Zero-Shot HOI Discovery.** In accordance with prior research , the zero-shot HOI discovery on HICO-DET  uses four setups: Rare First Unseen Combination (RF-UC), Non-rare First Unseen Combination (NF-UC), Unseen Verb (UV), and Unseen Object (UO). The RF-UC and NF-UC configurations excluded the 120 most frequent/infrequent interaction categories from the training sets for testing purposes only. The UV and UO setups reserve 20 verb classes and 12 object classes never encountered during training for testing. For SWIG-HOI , the test set includes approximately 5,500 interactions, with around 1,800 of them not present in the training set.

    &  &  &  &  &  \\   & & Pretrain & Full & Rare & Non-rare & Full & Rare & Non-rare & \(_{}^{}\) & \(_{}^{}\) \\  iCAN & R50 & - & 14.84 & 10.45 & 16.150 & 16.26 & 11.33 & 17.73 & 45.3 & - \\ PPDM & HG104 & - & 21.73 & 13.78 & 24.10 & 24.58 & 16.65 & 26.84 & - & - \\ HOTR & R50 & - & 23.46 & 16.21 & 25.60 & - & - & 55.2 & 64.4 \\ GPT & R101 & - & 29.90 & 23.92 & 31.69 & 32.38 & 26.06 & 34.27 & 58.3 & 60.7 \\ CDN & R101 & - & 32.07 & 27.19 & 33.53 & 34.79 & 29.48 & 36.38 & 63.9 & 65.9 \\ CPCho & R50 & - & 29.63 & 23.14 & 31.57 & - & - & - & 63.1 & 65.4 \\ STIP & R50 & - & 32.22 & 28.15 & 33.43 & 35.29 & 31.43 & 36.45 & 66.0 & 70.7 \\ UPT & R101 & - & 32.62 & 28.62 & 33.81 & 36.08 & 33.14 & 31.47 & 61.3 & 67.1 \\ Iwin & R101 & - & 32.79 & 27.84 & 35.40 & 35.84 & 28.74 & 36.09 & 60.9 & - \\ MCPC & R50 & - & 35.15 & 33.71 & 35.58 & 37.56 & 35.87 & 38.06 & 63.0 & 65.1 \\ PViC & R50 & - & 34.69 & 32.14 & 35.45 & 38.14 & 35.38 & 38.97 & 62.8 & 67.8 \\ PViC & Swin-L & - & 44.32 & 44.61 & 44.24 & 47.81 & 48.38 & 47.64 & 64.1 & 70.2 \\ GEN-VLK & R101 & CLIF & 34.95 & 31.18 & 36.08 & 38.22 & 34.36 & 39.37 & 63.6 & 65.9 \\ HOICIL & R50 & CLIP & 34.69 & 31.12 & 35.74 & 37.61 & 34.47 & 38.54 & 63.5 & 64.8 \\ COQL & R50 & CLIP & 35.36 & 32.97 & 36.07 & 38.43 & 34.85 & 39.50 & 66.4 & 69.2 \\ ViPLO & ViT-B & CLIP & 37.22 & 35.45 & 37.75 & 40.61 & 38.82 & 41.15 & 62.2 & 68.0 \\ AGER & R50 & CLIP & 36.75 & 33.53 & 37.71 & 39.84 & 35.58 & 40.2 & 65.7 & 69.7 \\ RmL & R101 & MobileBERT & 37.41 & 28.81 & 39.97 & 38.69 & 31.27 & 40.91 & 64.2 & 70.2 \\ ADA-CM & ViT-L & CLIP & 38.40 & 37.52 & 38.66 & - & - & - & 58.6 & 64.0 \\ DiffusionHOI & VQAN & Stable Diffusion & **38.12** & **38.93** & **37.84** & **40.93** & **42.87** & **40.04** & **66.8** & **70.9** \\ DiffusionHOI & ViT-L & Stable unCLIP & **42.54** & **42.95** & **42.35** & **44.91** & **45.18** & **44.83** & **67.1** & **71.1** \\   

* Models built upon advanced object, _i.e._, \(\)-Deform-DETR .

Table 1: Quantitative results for regular HOI detection on HICO-DET  and V-COCOCO .

**Evaluation Metric.** Following conventions [3; 41; 42], we adopt mAP as metrics. For HICO-DET, we report performance according to Default and Known Object two setups. The former computes mAP across all testing images, while the latter is tailored for each object class. For each setup, the scores are reported in Full/Rare/Non-Rare three types. For V-COCO, we evaluate the performance under scenario 1 (S1) which contains all 29 actions and scenario 2 (S2) which excludes 4 actions interact with no objects. For zero-shot setup, the evaluation is divided into Seen/Unseen/Full three sets for HICO-DET, and Non-Rare/Rare/Unseen/Full four sets for SWiG-HOI.

**Training and Testing.** The diffusion model and CLIP text encoder are kept frozen during training. For inversion-based HOI modeling, the only learnable parameters are relation embeddings, which are updated for 40,000 steps using images sampled from HICO-DET. Following , we employ a base learning rate of 8e\({}^{-2}\) with a batch size of 32. For HOI detection learning, we train the interaction decoder \(_{}\) and object decoder \(_{}\) for 60 epochs with a base learning rate of 1e\({}^{-4}\) and batch size of 16, using both synthesized data and the target dataset. Subsequently, the model is trained only on the target dataset for an additional 30 epochs with a base learning rate of 1e\({}^{-5}\). During inference, no data augmentation is used to ensure fair comparison. Following [3; 103], the inputs are resized to maximum of 1,333 pixels on long sides, and the shortest sides falls between 480 and 800 pixels.

**Reproducibility.** DiffusionHOI is implemented in PyTorch and trained on 8 Tesla A40 GPUs with 48GB memory per card.

### Comparison with State-of-the-Arts

**Regular Setup.** We first compare DiffusionHOI with top-leading solutions on HICO-DET and V-COCO under the regular setup. As shown in Table 1, for HICO-DET, our method achieves the best performance on both Default and Known Object setups. Notably, with the encoder of VQGAN as the backbone, it surpasses the previous SOTA, RemLR, which employs a similar level backbone (_i.e_., ResNet-50) by **1.19%** and **2.64%** on the Full categories. Benefited from synthesized data and comprehensive knowledge transfer from diffusion models, the performance on Rare categories improves significantly, achieving higher scores than on Non-Rare categories for the first time. Finally, with a more powerful VL model (_i.e_., Stable unCLIP) and backbone (_i.e_., ViT-L), the performance is boosted to **42.54%** under the Default setup, surpassing nearly all existing work by a considerable margin. Please note that PViC with Swin-L as the backbone leverages \(\)-Deform-DETR as the detector which achieves 48.7 mAP on MS COCO by running merely 12 epochs, significantly higher than DETR which achieves 36.2 mAP by running 50 epochs.

**Zero-Shot Setup.** Next we investigate the effectiveness of DiffusionHOI under the zero-shot generalization setup. As shown in Table 2, our method yields remarkable performance across all four setups on HICO-DET. In particular, it surpasses the previous SOTA (_i.e_., HOICLP) by

   Method & Backbone &  Trainable \\ Params (M) \\  & FPS & HICO-DET \\   \\  iCAN &  H50 \\  & 39.8 & 6.23 & 14.84 \\ DRG &  H50 \\  & 46.1 & 6.05 & 19.26 \\ STIP &  H50 \\  & 50.4 & 7.12 & 32.22 \\ ViPL &  H50 \\  & 77.28 & 118.2 & 5.66 & 37.22 \\ ADA-CM & ViT-L & 6.6 & 3.24 & 38.40 \\   \\   H50 \\  &  H6104 \\  & 194.9 & 17.58 & 21.73 \\ HOTR & 
 H50 \\  & 51.2 & 15.92 & 23.46 \\
** HOTL \\  & 
 H50 \\  & 41.9 & 17.41 & 29.07 \\
** CNN \\  &  H50 \\  & 42.1 & 16.24 & 31.78 \\ GEN-VLKT &  H50 \\  & 42.8 & 18.23 & 33.75 \\   &  VQ-GAN \\  & 27.6 & 9.49 & 38.12 \\   HOTL** \\ **} \\   

Table 4: Comparison of parameters and running efficiency. * means applying accelerated technology.

**2.90%** under the RF-UC setup. This setup emphasizes compositional generalization which requires models to comprehend new types of interactions using known actions and objects. It aligns well with the strengths of text-to-image diffusion models to generate images conditioned on compositionally organized textual descriptions. Moreover, due to the effective knowledge transfer, DiffusionHOI also achieves satisfactory improvement under the UV and UO setups which focus on the recognition of novel actions and objects. Table 3 further confirms the exceptional ability of our method, showing **5.97%/8.23%** mAP improvements over CMD-SE under Rare and Unseen two categories.

**Model Efficiency.** We compare the trainable parameter number and inference time in Table 4. As seen, DiffusionHOI demonstrates significantly fewer trainable parameters compared to the one-stage counterparts. This is attributed to our inversion-based HOI modeling, which avoids fine-tuning diffusion models like previous work, while effectively capturing task-specific properties. Regarding inference speed, even with stable diffusion for feature extraction, our method still achieves 9.49 FPS, a rate similar to two-stage models. This is due to the inference involving only one single forward pass, and the downsampling factor of stable diffusion from 1/8 to 1/64 is smaller than conventional backbones typically from 1/4 to 1/32. Moreover, thank to the flourishing community of stable diffusion, a variety of optimized inference solutions have emerged. By running at fp16 precision and using traced UNet, the FPS increases to 24.77, surpassing most one-stage methods.

### Diagnostic Analysis

**Key Component Analysis.** We first examine the essential components of DiffusionHOI in Table 5. Here Baseline denotes HOI detector built upon stable diffusion without text prompting. Through jointly training with the synthesized data, both Default and RF-UC setups observe notable improvements (_e.g._, up to **2.25%** and **3.32%** on Full categories ). This verifies the effectiveness of our relation-driven HOI image generation strategy. in addition, after imposing relation embedding to prompt the feature extraction and HOI detection processes, the performance boosts to **36.45%** and **34.25%** under two setups. Finally, after combining these two core components together, our DiffusionHOI delivers consistent improvements and sets new SOTA across all setups.

**Conditioning Input.** To assess the effectiveness of learned relational embeddings, we present the experimental results using different conditional inputs to stimulate HOI detection in Table 6. As seen, though action words offer limited improvement, they are far surpassed by relation embeddings which enables HOI-oriented feature extraction and enhance query initialization through cross-attention.

**Relation Embedding Learning.** Next we probe the impact of different strategies for relation embedding learning. The results regarding relation-inspired HOI detection are summarized in Table 7. It can be observed that textual inversion, which directly uses different images sharing the same action for relation embedding learning, is inferior to our cycle-consistency learning strategy that considers the disentanglement nature of HOI interactions. On this basis, the relation-centric contrastive learning and online update strategies consistently bring improvement in both setups.

**Prompt for Dataset Generation.** Finally we study the impact of data synthesized by different types of textual prompts in Table 8. As observed, data generated with purely textual description using plain action words like "The man at bat readies to swing at the pitch" gives negative improvement over baseline. The potentially indicates that diffusion models cannot understand the relations between human-object pairs and generate meaningful images when provided with straightforward textual

    &  &  \\   & Full & Rare & Non-Rare & Full & Unseen & Seen \\  - & 33.24 & 30.25 & 34.32 & 30.47 & 20.63 & 33.09 \\ Textual Description & 33.71 & 30.98 & 34.73 & 30.72 & 21.29 & 33.24 \\ Relation Embedding & 36.45 & 35.78 & 36.71 & 34.25 & 26.57 & 35.58 \\   

Table 6: Analysis of conditioning input for relation-inspired HOI detection on HICO-DET.

    &  &  \\   & Full & Rare & Non-Rare & Full & Unseen & Seen \\  Textual Inversion & 34.03 & 32.17 & 34.61 & 30.93 & 21.55 & 33.45 \\ Cycle-Consistency & 35.23 & 34.56 & 35.46 & 32.96 & 24.24 & 34.54 \\ + Relation-Centric C & 35.94 & 35.06 & 36.32 & 33.72 & 25.73 & 35.17 \\ + Online Update & 36.45 & 35.78 & 36.71 & 34.25 & 26.57 & 35.58 \\   

Table 7: Analysis of relation embeddings with different learning strategies for relation-inspired HOI detection.

    &  &  \\   & Full & Rare & Non-Rare & Full & Unseen & Seen \\  - & 33.24 & 30.25 & 34.32 & 30.47 & 20.63 & 33.09 \\ Textual Description & 33.71 & 30.98 & 34.73 & 30.72 & 21.29 & 33.24 \\ Relation Embedding & 36.45 & 35.78 & 36.71 & 34.25 & 26.57 & 35.58 \\   

Table 5: Detailed analysis of essential components of DiffusionHOI on HICO-DET.

description. In contrast, through relation modeling, data generated with relation embeddings to replace the plain action words provides high-quality samples for the training of HOI detectors.

**Analysis on Training Cost.** For our inversion-based HOI modeling to learn relation-centric embeddings, unlike the original textual inversion technology that learns text embeddings within the image space, we optimize relation embeddings within the latent space by reconstructing interaction features. This lead to reduced training costs. Consequently, the 117 relation embeddings in HICO-DET can be learned within **5.7** hours (23 minutes per relation embedding) which is more efficient than textual inversion (_i.e_., 32 minutes per embedding). For the main training of HOI detection on HICO-DET, since our method utilizes significantly fewer trainable parameters compared to existing work (_e_., 27.6M \(v\)._s_. 50.4M for STIP, 41.9M for QPIC, and 42.8M for GEN-VLKT in Table 4), the training process can be completed in just **11.5** hours. The comparison of the whole training time with some representative work is summarized in Table 9, with all experiments conducted on 8 Tesla A40 cards. It can be observed that DiffusionHOI requires less training time than most existing work.

## 5 Conclusion

We present DiffusionHOI, a new HOI detector built upon diffusion models. By explicitly modeling the relations between humans and objects in an inversion-based manner, we enable effective knowledge transfer from diffusion models while adapting unique characteristics of the HOI detection task. This is achieved in two aspects: **i)**_relation-driven_ image generation using diffusion models to enrich the training set with more HOI-oriented samples, and **ii)**_relation-inspired_ HOI detection with learned relation embeddings as prompts to retrieve task-specific features from images, thereby enhancing the recognition of ongoing interactions. Extensive experiments demonstrate that DiffusionHOI excels in both regular or zero-shot setups and sets new SOTAs. We believe this work provides insights to unleash the power of diffusion models for downstream visual perception tasks in an efficient manner.

**Acknowledgement.** This work was supported by the National Science and Technology Major Project (No. 2023ZD0121300), the National Natural Science Foundation of China (No. 62372405), the Fundamental Research Funds for the Central Universities 226-2024-00058, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University (No. HMHAI-202403), and CIPSC-SMP-Zhipu Large Model Cross-Disciplinary Fund.

  Method & CDN & HOTR & UPT & STIP & GEN-VLKT & HOICIP & COL & DiffusionHOI \\  Time (Hour) & 25.2 & 23.6 & 17.9 & 16.4 & 28.4 & 29.1 & 29.7 & **5.7+11.5** \\  

Table 9: Comparison of total training time on HICO-DET.