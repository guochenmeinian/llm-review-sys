# DEPrune: Depth-wise Separable Convolution Pruning

for Maximizing GPU Parallelism

 Cheonjun Park\({}^{1}\),  Mincheol Park\({}^{23}\),  Hyunchan Moon\({}^{4}\),  Myung Kuk Yoon\({}^{5}\),

Seokjin Go\({}^{6}\),  Suhyun Kim\({}^{3*}\),  Won Woo Ro\({}^{2}\)

\({}^{1}\) Samsung Electronics \({}^{2}\) Yonsei University \({}^{3}\) Korea Institute of Science and Technology

\({}^{4}\) LG Electronics \({}^{5}\) Ewha Womans University \({}^{6}\) Georgia Institute of Technology

{cheonjun.park, mincheol.park, wro}@yonsei.ac.kr,

{mhcqwe92, dr.suhyun.kim}@gmail.com,

myungkuk.yoon@ewha.ac.kr, seokjin.go@gatech.edu

 co-corresponding authors.

###### Abstract

Depth-wise Separable Convolution (DSConv) has a powerful representation even with fewer parameters and computation, leading to its adoption by almost all of the state-of-the-art CNN models. DSConv models are already compact making it hard to apply pruning, and there are few previous pruning techniques that target depth-wise convolution (DW-conv). In this paper, we present Depth-wise Separable Convolution Pruning (DEPrune), a novel pruning method applied to both point-wise and depth-wise convolutions. DEPrune is optimized by analyzing the computation of DSConv on GPUs. DEPrune employs a fine-grained pruning approach, yet it achieves the structured sparsity typically absent in fine-grained pruning, enabling practical hardware acceleration. Moreover, this method maintains a high pruning ratio without causing any accuracy drop. We additionally represent techniques that further enhance DEPrune performance: 1) balanced workload tuning (BWT), and 2) hardware-aware sparsity recalibration (HSR). Experiment results show that DEPrune achieves up to \(3.74\) practical speedup in DSConv inference on GPUs while maintaining the accuracy of EfficientNet-B0 on ImageNet.

## 1 Introduction

In computer vision tasks, Convolutional Neural Networks (CNNs) have dramatically gained parameters and computation  to solve complex and varied tasks . Such massive computation and memory footprint poses challenges in environments with limited hardware resources, such as mobile devices. Many research efforts have been made to address such problems, and two techniques have been most effective: Depth-wise Separable Convolution (DSConv)  and DNN pruning .

DSConv [6; 43; 45] is composed of Depth-wise Convolution (DW-conv) and Point-wise Convolution (PW-conv), allowing it to have a similar representation power to traditional CNNs that use standard convolution, even with fewer parameters and computation . Therefore, modern CNNs primarily adopt DSConv when designing models [9; 40; 46; 48].

DNN pruning eliminates redundant weight parameters without compromising representation power. Weight pruning  brings a significant pruning ratio (\(PR\)) due to the fine-grained approach but rarely reduces inference time compared to the unpruned model because of index computation overhead . In contrast, structured pruning [16; 30; 32; 21; 38] is a coarse-grained approach that is GPU-friendly and leads to a practical reduction in inference time.

In DSConv, despite the DW-conv has only about \(1\%\) of the parameters, it spends over \(82\%\) of the overall inference time . As a result, reducing the computation time of DW-conv can inherently impact the overall network execution time, and employing pruning on DW-conv can provide an effective solution to accelerate DSConv. Applying existing structured pruning [27; 20; 37] to PW-conv is not difficult, because PW-conv has the same GPU operation characteristic as standard convolution. Conversely, DW-conv has two challenges in applying previous pruning methods. First, DW-conv has particularly fewer parameters, so applying coarse-grained pruning that creates a hardware-friendly structured data pattern causes significant accuracy loss. Second, the operation of DW-conv on GPU underutilizes the parallelism since the input unit is smaller than the operation unit, thus applying previous pruning methods is useless in this condition. Therefore, we propose **Depth-wise Separable Convolution Pruning (DEPrune)**, a hardware-aware pruning approach specialized for DW-conv for fast and memory-efficient DSConv.

First, to address the aforementioned challenges: accuracy loss and underutilization, we analyze the operation of DW-conv on a widely used GPU. On GPUs, DW-conv computes by being transformed into multiple-GEMV (GEneral Matrix-Vector Multiplication) , and this structure does not fully utilize the GPU parallelism. Thus, for efficient operation on GPU, previous work applies Diagonal-wise Refactorization (DR) . DR is a method of rearranging DW-conv to multiple sub-GEMM (GEneral Matrix-Matrix Multiplication) operations to maximize GPU parallelism (Fig. 1). DR places the weights diagonally and zero padding the rest. Therefore, if one non-zero weight is removed, all elements on the same column line have zero value, so the corresponding line is all zero vector (Fig. 1). At this point, we apply fine-grained pruning on DW-conv rearranged by DR, as we call the **Depth-wise Convolution Pruning (DCP)** (Sec. 4.1). DCP's fine-grained approach provides novel \(PR\) and since most of the sub-GEMM is zero-padded, it brings regular sparsity even with fine-grained pruning. This hardware-friendly format results in inference speedup without representation power loss. DEPrune also applies conventional structured pruning to PW-conv; however, taking into account the computational significance of the DSConv model, we selectively prune PW-conv layers to maximize the pruning ratio without sacrificing accuracy.

Second, for DEPrune enhancement we consider the overall operation flow of DSConv to optimize GPU utilization. When DCP is applied, the \(PR\) is different for each sub-GEMM, which executes in different processing units. This results in a workload imbalance problem between processing units which directs to GPU under-utilization. The total execution time is set to the longest GEMM, thereby other idle processing units are forced to wait until the longest GEMM finishes. To solve this problem, we propose a **Balanced Workload Tuning (BWT)** that sets the same target \(PR\) for each sub-GEMM when applying DCP (Sec. 5.1). Our DEPrune applies existing structured pruning [37; 27; 20] on PW-conv, and this approach avoids workload imbalance problem.

Figure 1: Depth-wise convolution is rearranged to multi sub-GEMM on GPU by applying Diagonal-wise Refactorization (DR). The ’X’ and ’O’ symbols indicate the absence and presence of corresponding characteristics for each method. Applying (a) Structured Pruning and (b) DEPrune (DCP) to multi sub-GEMM results in a structured data pattern. But (b) DEPrune (DCP) is more fine-grained method than (a) Structured Pruning.

Lastly, for DEPrune enhancement we found that in addition to balancing \(PR\) between sub-GEMMs, adjusting the pruning ratio proportional to the GPU execution unit could further lead to significant speed improvements. Due to our technique's structured data format, theoretically, the acceleration should increase proportionally as the pruning ratio increases. However, because GPU operates in a specific execution unit, the computational workload should scale with the unit of execution to maximize the acceleration effect. Considering this unit operation, we additionally determine negligible weight parameters for more pruning, and this results in an additional significant speed improvement of \(3\)-\(16\%\) for DW-conv and also PW-conv without any accuracy loss. We call this **Hardware-aware Sparsity Recalibration (HSR)**, an algorithm that recalibrates the appropriate target \(PR\) for DW-conv and PW-conv considering the execution unit of the GPU (Sec. 5.2).

To further optimize DSConv, various techniques are considered, which led to a diverse and somewhat complex set of terms being used throughout the paper. Therefore we provide Table 1 which summarizes the structure and terminology of our DEPrune.

## 2 Preliminary

PrerequisitesDW-conv's weight filter is 3D tensor. Given \(l^{th}\) DW-conv layer as \(^{(I)}^{M k_{h} k_{w}}\), where \(M\), \(k_{h}\), and \(k_{w}\) are the number of channels, height, and width of the filters, respectively. \(I_{h}\), and \(I_{w}\) are the height and width of the input, respectively.

Channel-by-ChannelAs shown in Fig. 2-(a), DW-conv is composed of 3D input (\(^{M I_{h} I_{w}}\)), 3D Weight (\(^{M k_{h} k_{w}}\)), each with M channels performing independent 2D convolution operations. GPU rearranges standard convolution to GEMM, using im2col [2; 5] to enable data reuse. Similarly, major deep learning frameworks (e.g., Caffe, PyTorch, MXNet, and TensorFlow) rearrange DW-conv to \(M\) multiple GEMV operations, using Channel-by-Channel (Fig. 2-(b)). GEMV consists of a

    &  \\   &  &  \\   & DCP &  & Structured\({}^{}\) & Enhance Method \\  Status & [**ours**] &  & HSR [**ours**] & Pruning &  \\   DEPrune & ✓ & - & - & ✓ & - \\ DEPrune-B & ✓ & ✓ & - & ✓ & - \\ DEPrune-BH & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Terminology of DEPrune method. This symbol (\(\)) means “we apply our methodology to determine which PW-conv to prune for better performance (Sec. 4.2)’. BWT and HSR are our proposed method to enhance DEPrune. BWT and HSR are described in Sec. 5.1 and Sec. 5.2, respectively. This symbol (✓) means ‘Applied’.

Figure 2: (a) DW-conv is rearranged to multi GEMV through (b) Channel-by-Channel on GPU execution. (c) Diagonal-wise Refactorization (DR) rearranges DW-conv into multiple sub-GEMMs. After DR, due to GPU tile size , we group **M** GEMVs into units of **32**, resulting in a total of **G** sub-GEMMs.

weight vector of size \(k_{h} k_{w}\) and an input matrix of size \((k_{h} k_{w})(I_{h} I_{w})\). However, this approach suffers from the limitation of weight vector size being too small (9 or \(25\)) to fully utilize the GPU's processing units effectively.

Diagonal-wise Refactorization (DR)To address under-utilization, Diagonal-wise Refactorization (DR)  arranges the weight vectors of the GEMVs diagonally and sequentially places the input matrix (Fig. 2-(c)). Next, zero padding is added to the empty spaces to create a complete dense GEMM, which consists of \(M(M k_{h} k_{w})\) weight matrix and an \((M k_{h} k_{w})(I_{h} I_{w})\) input matrix. However, the rearranged GEMM is excessively large \((M k_{h} k_{w})\), so this operation requires significant additional computation on the GPU such as tiling . Therefore, DR further divides this dense GEMM into smaller sub-GEMMs of a certain size. When executing matrix multiplication on GPUs, grouping with a size of \(32\) channels is found to be the most efficient, resulting in a total of \(G\) sub-GEMM operations (\(G=\)). Thus each sub-GEMM is composed of a \(32(32 k_{h} k_{w})\) weight matrix and a \((32 k_{h} k_{w})(I_{h} I_{w})\) input matrix. This approach allows for highly optimized GPU execution using specialized cuDNN libraries .

## 3 Related Works

### Hardware-aware Pruning

Among the previous DNN pruning techniques, the following three methods consider hardware characteristics to reduce inference time: structured pruning, balanced pruning, and block pruning. Structured pruning [32; 16; 53] determines redundancy at the vector level for pruning, thereby creating a regular sparsity. This structured data pattern requires almost no additional index computation on GPU, making it effective in reducing inference time . Block pruning [15; 36; 49]that considers the tiling technique , applies structural pruning at the small matrix level and has less representation power loss at the same \(PR\) compared to structured pruning. Balanced pruning [58; 51; 31; 23; 37] is a technique that divides the weight into consistent ranges and assigns an equal \(PR\) to each segment, ensuring a workload balanced characteristic. Most balanced pruning achieves approximately a \(2\) speedup when a specific GPU with a dedicated accelerator (e.g. sparse tensor core) is used at only \(50\%\)\(PR\). These pruning methods are based on the optimization technique of lowering for standard convolutions. Therefore, such pruning methods are difficult to apply to DW-conv. However, our DEPrune considers the hardware computation of DW-conv, enabling performance improvement.

### Optimizations for DSConv

Since DSConv operates differently from standard convolution, continuous research is rapidly ongoing for the optimization of DSConv through dedicated software and hardware optimization.

On the pruning side, Multi-stage gradual pruning  prune the filters on DSConv using gradual pruning principle . Probability-based channel pruning  considers batch normalization when pruning in DSConv and requires little fine-tuning. WP-UNet  utilizes fine-grained pruning in DSConv to merely reduce parameters without considering speedup. The filter pruning  sorts and prunes the filters according to their variance in each DW-conv. However, existing studies either only apply pruning on PW-conv or do not lead to noticeable substantial inference speed improvement even if pruning is also applied to DW-conv.

On the software side, DepthShrinker  removes non-linear activation functions after training and merges consecutive linear operations into a single dense operation to maximize hardware efficiency without compromising accuracy.

On the hardware side, GPU optimization  proposes a dynamic tile size scheme for GPUs to improve GPU utilization and hide memory access latency in DW-conv. The method  suggests loop rescheduling and register tiling on DW-conv, because when executing DW-conv on the parallel processor, traffic overload occurs between the cache, memory, and register. Diagonal-wise Refactorization (DR)  maximizes the parallelism of the GPU by proposing a rearrange method that combines all filters of DW-conv into a multi GEMM.

## 4 Proposed Method: DEPrune

### DCP: Depth-wise Convolution Pruning

(a) Motivation 1: Channel pruning on DW-conv has a large pruning unit size problemAs shown in Fig. 2-(b), DW-conv generates a multi-GEMV format for each channel, on GPUs. DW-conv can also achieve structured data format, by evaluating the significance of each GEMV and eliminating an unnecessary weight vector of GEMV. Nevertheless, when compared to the 4D tensor weight of the standard convolution, the DW-conv weight is a 3D tensor (\(^{M k_{h} k_{w}}\)), notably fewer parameters. Consequently, eliminating a single channel (\(^{1 k_{h} k_{w}}\)) from DW-conv can greatly diminish its representation power. As shown in Fig. 3-(a), when pruning is done channel-wise, there's a representation power loss of \(1.66\%\) compared to the unpruned model even at just a \(40\%\)\(PR\) (EfficientNet-B0 on ImageNet). This indicates that channel-wise pruning on DW-conv is not an appropriate choice.

(b) Motivation 2: Hardware-unfriendly problem of weight pruning without DRWeight pruning experiences the least representation power loss among DNN pruning techniques with the highest pruning ratio. When applying weight pruning to the multiple GEMVs of DW-conv, the representation power loss due to increased \(PR\) is much less than the previously mentioned channel pruning. Looking at Fig. 3-(a) as our DCP similar to weight pruning, there are very minor representation power losses of \(0.94\%\) and \(1.15\%\) at \(50\%\) and \(70\%\)\(PR\)s, respectively. However, weight pruning without considering DR does not result in practical speedup from pruning. As shown in Fig. 2-(b), the vector size of DW-conv GEMV is \(k_{h} k_{w}\) (e.g., 9 or 25). Since this is smaller than the GPU's tile size (32), there is almost no change in inference time (Fig. 3-(b)) since GEMV underutilizes processing units of GPU.

(c) Method: DCPWe propose Depth-wise Convolution Pruning (DCP) to address the above two issues. We discover that weight pruning after DR can even achieve a structured sparsity in DW-conv with high \(PR\), and making large matrix multiplication fully utilizes GPU parallelism. As shown in Fig. 4, first, we take the weight matrix rearranged in the form of matrix multiplication by DR. The height of the weight matrix is \(M\), and the width is \(M k_{h} k_{w}\). As shown in Fig. 4, the unpruned values in the weight matrix are placed diagonally, while the rest are zero-padded. Second, we sort the unpruned values in ascending order and select the threshold value that corresponds to the target pruning ratio. When calculating the target pruning ratio, zero-padded values are not considered. Last, for each unpruned value, if it is smaller than the threshold, we change it to \(0\) (i.e., magnitude pruning ). Since the other values in the same column are already zero values, the column vector becomes a zero column vector, which is hardware-friendly.

Figure 4: Process of Depth-wise Convolution Pruning (DCP).

Figure 3: (a) Comparison of accuracy drop between DCP and channel pruning on EfficientNet-B0 using ImageNet. (b) Measurement of the GEMV execution time of DW-conv 6th layer of EfficientNet-B0 on GPU. (c) Measurement of imbalance overhead of Mobilenet-V2 on ImageNet. The imbalance overhead is the difference between minimum sub-GEMM pruning ratio (\(PR\)) and layer’s target \(PR\).

### Methodology for Determining which PW-conv Layer to Prune

When filter pruning is applied to PW-conv, the parameters of the subsequent layers are removed with the same sparsity. DSConv has the following structure: PW-conv1\(\)DW-conv\(\)PW-conv2. In DSConv, if PW-conv1 is filter pruned, the parameters of DW-conv are also removed at the channel level. The existing PW-conv pruning methods prune all PW-conv layers of DSConv, inadvertently leading to prune DW-conv layers as well. However, the parameters of DW-conv are only 1.34% of those in PW-conv , so each weight element is more sensitive to accuracy, thus for DW-Conv, rather than channel pruning, a more fine-grained pruning is necessary. Therefore, our DEPrune does not directly prune all PW-conv layers. DEPrune applies fine-grained pruning directly to DW-conv and does not prune PW-conv1 directly. We only apply filter pruning to PW-conv2. Pruning only PW-conv2, removes only the parameters in the subsequent DSConv's PW-conv1, which is less sensitive to accuracy drop. Thus, DEPrune effectively prune all the layers of DSConv with high \(PR\) and representation power.

## 5 Enhance DEPrune

We propose the following two techniques to enhance DEPrune performance: BWT and HSR.

### DEPrune-B

(a) Motivation: Imbalance overhead problem of DCP GPUs allocate operations of a certain size to streaming multiprocessors (SMs) for massively parallel processing. Therefore, DW-conv's multiple sub-GEMMs are also assigned to SMs, respectively. However, when applying DCP on DW-conv, the pruning ratio (\(PR\)) of sub-GEMMs may differ, given the varying importance of weights between sub-GEMMs. In that case, the execution time varies for each sub-GEMM due to the difference in \(PR\). This results in a workload imbalance problem in that the other SMs of the GPU have to wait until the SM with the lowest \(PR\) finishes. The acceleration effect of DCP is then determined by the minimum sub-GEMM \(PR\), not by the layer target \(PR\). Referring to Fig. 3-(c), the difference between the minimum sub-GEMM \(PR\) and the layer target \(PR\) is compared for each layer of EfficientNet-B0. In DW Layer 13, when the layer target \(PR\) is \(60\%\), the minimum sub-GEMM PR is \(50.4\%\), which varies up to \(9.6\%\), which indicates that it decelerates execution by the amount specified.

(b) Method: Balanced Workload Tuning (BWT) To address the workload imbalance issue of DCP, we propose a DW-conv-specific Workload Balanced Technique that takes into account the operation structure of DW-conv (Fig. 5). DW-conv is a dense matrix where non-zero values are arranged diagonally due to DR, while the remainder consists of zero values. We group all non-zero values within sub-GEMM, which we call a balanced range as illustrated in Fig. 5-(b). Within each balanced range, we rank weight elements with redundancy and systematically prune the lower-ranked elements until the target \(PR\) is reached. As every sub-GEMM achieves the same target \(PR\) like Fig. 5-(b), this resolves the workload imbalance issue associated with DCP. Since DCP is fine-grained pruning (pruning unit size: \(\)), the representation power loss due to additional BWT is almost negligible. A detailed analysis related to this is in Sec. 6.1.

Figure 5: Overview of DCP and Balanced Workload Tuning (BWT). (a) DCP is an element-wise pruning method that creates a structured data pattern. (b) BWT equalizes the \(PR\) of all sub-GEMMs. The balanced range of BWT is \(32 k_{h} k_{w}\).

### DEPrune-BH

(a) Motivation: Unaligned problemAs shown in Fig. 7-(a), to maximize parallelism, GPUs divide GEMM operations into small tiles. In general, the size of the tile depends on the hardware specification of GPUs, but it is usually a multiple of \(32\). However, if the width of the unpruned weight matrix in Fig. 7-(a) is not a multiple of \(32\), some parts of the weight tiles are empty. This can cause an unaligned memory access problem on GPUs [11; 13]. In Fig. 6-(b), the inference time does not decrease linearly with an increase in the size of the pruned vector. Whenever the number of pruned vectors increases by \(32\), the inference time decreases significantly like a step function graph. In DW-conv of Stage 2, the inference time decreases by \(7\%\) for each removal of only one tile. Thus, by removing a few additional weight vectors for aligned memory access, we can reduce the inference time by \(7\%\) if we align the number of pruned vectors with a multiple of \(32\) (Fig. 7-(b)).

(b) Method: Hardware-aware Sparsity Recalibration (HSR)We propose Hardware-aware Sparsity Recalibration (HSR) to solve the unaligned memory access problem and enhance DCP-B. As shown in Fig. 7-(c), DCP-B with HSR operates in the following four steps. The first step, DCP-B is applied to DW-conv. The second step, we measure two essential factors (\(\) and \(\)) within the DCP-B model. **(1) \(\) :** We measure the speedup obtained by solving the unaligned problem per layer. **(2) \(\) :** We count the number of unpruned vectors of the unaligned tile matrix for each layer. We refer the result obtained by dividing the two parameters, \(\) and \(\), for each layer as \(\). The \(\) refers to the size of speed obtained by removing one overflowed vector. The third step, the \(\) values of all layers are ranked by comparing them with each other. The last step, the layer with the \(\) value of the top \(50\%\) is additionally removed as much as it overflows. The additional removed column vector consists of one non-zero value and zero-padded elements. Thus, there is no significant side effect on the representation power. On the other hand, the layer with the \(\) value of the bottom \(50\%\) additionally recovers as much as it is unaligned. The reason why the criteria for recovery and removal of HSR are set to \(50\%\) is to maintain the total target \(PR\).

Figure 6: (a) Measurement of speed increase by layer due to HSR. The orange bar is the max speedup layer. DW-conv \(PR\) is 71%. (b) Measurement of DW-conv inference time of EfficientNet-B0 on ImageNet dataset. Inference time decreases with additional pruning of 32 or more vectors. GPU tile size is 32 .

Figure 7: (a) Problem of unaligned pruning ratio on GPU. (b) Concept of Hardware-aware Sparsity Recalibration (HSR). (c) Process of DCP-BH (DCP-B + HSR).

## 6 Experiments

We assess the effectiveness of DEPrune using ImageNet  and CIFAR-10 . For the validation of image classification, we assess our method with CNN models using DSConv: MobileNet-V2 , EfficientNet-B0 , and MobileNet-V3 .

Experiment setting on ImageNetWe utilize pre-trained CNN models sourced from the Pytorch framework . We perform fine-tuning with only \(65\) epochs after conducting pruning methods. We set a batch size of \(256\). We use SGD optimizer with the weight decay, \(1 10^{-4}\), and the momentum as \(0.9\) for fine-tuning. The initial learning rate is set to \(0.001\) and divided by \(10\) every \(30\) epoch. All data are augmented with random cropping and horizontal flipping. We evaluate DEPrune using NVIDIA RTX 2080 Ti GPUs . We measured the inference time using NVIDIA CUTLASS . We set the batch size to 32 to measure inference time.

### Effect of BWT (DEPrune vs. DEPrune-B)

We analyze the changes in accuracy and speedup resulting from applying the BWT to DEPrune (Table 2). DEPrune has varying pruning ratios among sub-GEMMs, causing the overall speed to be dictated by the sub-GEMM with the smallest pruning ratio. In MobileNet-V2, the smallest sub-GEMM pruning ratio of DEPrune is 71%, as described in the Table 2. Therefore, DEPrune-B in MobileNet-V2 is 21.8% (2.75\(\) 3.35\(\)) faster in inference time than DEPrune. In MobileNet-V3-Small, DEPrune-B achieves a 31.2% (3.88\(\) 5.09\(\)) improvement in inference time over DEPrune due to BWT. Since the balanced range of DEPrune-B is significantly large at \(32 k_{h} k_{w}\), DEPrune-B has an accuracy drop of within 0.1% than DEPrune across representative models.

### Effect of HSR (DEPrune-B vs. DEPrune-BH)

We analyze the changes in accuracy and speedup resulting from the application of the HSR technique to DEPrune-B (Table 3). Since GPUs process operations and memory access in tile units, the actual speed of the GPU does not decrease linearly with the pruning ratio but rather decreases in a step-wise manner, as shown in Fig. 6-(b). By adjusting the pruning ratio to fit the tile size, the DW-conv layer can

    &  &  &  & ^{}\)} \\    & & DW-conv & & & &  &  & Baseline & Pruned & Diff. \\    & DEPrune & 78\% & **71\%** & 50\% & 71.92 & 71.52 & -0.40 & 2.75\(\) \\  & **DEPrune-B** & 78\% & **78\%** & 50\% & 71.92 & 71.51 & **-0.41** & **3.35\(\)** \\   & DEPrune & 82\% & **74\%** & 50\% & 67.67 & 67.26 & -0.41 & 3.88\(\) \\  & **DEPrune-B** & 82\% & **82\%** & 50\% & 67.67 & 67.13 & **-0.54** & **5.09\(\)** \\   & DEPrune & 85\% & **78\%** & 40\% & 77.69 & 77.01 & -0.68 & 4.51\(\) \\  & **DEPrune-B** & 85\% & **85\%** & 40\% & 77.69 & 77.00 & **-0.69** & **5.79\(\)** \\   

Table 2: Comparison between DEPrune and DEPrune-B (DEPrune + BWT) on ImageNet dataset. This symbol (\(\)) means ‘DW-conv inference time speedup than unpruned DW-conv’. ‘Real DW’ denotes the minimum pruning ratio among the sub-GEMMs of DW-conv. ‘Diff.’ denotes the difference in Top-1 accuracy between the baseline and pruned models.

    &  &  &  & ^{}\)} \\    & & DW-conv & DW-Pat. & PW-conv & Baseline & Pruned & Diff. & Up \\    & DEPrune-B & 77.8\% & - & 50.0\% & 71.92 & 71.51 & -0.41 & 3.35\(\) \\  & **DEPrune-BH** & 77.9\% & **90\%** & 50.1\% & 71.92 & 71.51 & **-0.41** & **3.52\(\)** \\   & DEPrune-B & 81.9\% & - & 60.2\% & 67.67 & 67.17 & -0.50 & 5.09\(\) \\  & **DEPrune-BH** & 82.1\% & **60\%** & 60.0\% & 67.67 & 67.18 & **-0.49** & **5.29\(\)** \\   & DEPrune-B & 84.8\% & - & 51.9\% & 77.69 & 77.00 & -0.69 & 5.79\(\) \\  & **DEPrune-BH** & 84.7\% & **80\%** & 52.0\% & 77.69 & 76.84 & **-0.85** & **6.15\(\)** \\   

Table 3: Comparison between DEPrune-B and DEPrune-BH (DEPrune-B + DW-conv HSR) on ImageNet dataset. This symbol (\(\)) means ‘DW-conv inference time speedup than unpruned DW-conv.’ ‘DW-Pat.’ denotes the HSR pattern for DW-conv layers. ‘**u**’ and ‘**o**’ denotes under-aligned and over-aligned layers, respectively. ‘Diff.’ denotes the difference in Top-1 accuracy between the baseline and pruned models.

achieve an average inference time speedup of 6.37%, as illustrated in Fig. 6-(a). According to Table 3, applying HSR to DEPrune-B shows almost no difference in accuracy compared to not applying HSR within 0.16%. Specifically, the accuracy difference is only 0.01% on MobileNet-V3-Small. For DEPrune-BH, all models have nearly identical numbers of over-aligned and under-aligned layers. DEPrune-BH achieves 6.2% (\(5.79\)\(\) 6.15\(\)) inference time speedup compared to DEPrune-B on EfficientNet-B0. Additionally, HSR can be applied to PW-conv layers as well.

### Comparison with Structured Pruning

In Table 4, we conduct experiments comparing DEPrune with the latest structured pruning methods across four models. On MobileNet-V2, our DEPrune-BH reduces approximately 26.7% more FLOPs compared to RLAL, while exhibiting a 0.2% smaller accuracy drop. GFS removes up to 42.8% of DSConv parameters, resulting in an accuracy drop exceeding 3%. In contrast, DEPrune-BH eliminates 75.1% and 64.8% of parameters in DW-conv and PW-conv, respectively, with an accuracy drop within 1%. On EfficientNet-B0, while other methods prune around 30% of DW-conv, our method prunes 84.7% with only a 0.8% accuracy drop. On MobileNet-V3-Small and MobileNet-V3-Large, DEPrune-BH achieves inference times 3.33 times and 3.32 times faster than GFS and FPGM, with accuracy drops of 1.1% and 0.6% less, respectively.

### Discussion: Various Pruning on PW-conv

We apply four structured pruning techniques to PW-conv layers to measure the changes in accuracy (See Table 5). When applying \(_{1}\)-norm pruning and \(_{2}\)-norm pruning to PW-conv layers, the accuracy difference is within 0.06% for all models except EfficientNet-B0. According to the FP paper , there is minimal difference between \(_{1}\)-norm and \(_{2}\)-norm pruning, and this similarity is also observed in the case of DEPrune-BH. Conversely, on EfficientNet-B0, FPGM  which uses geometric median achieves 0.33%, and 0.09% higher accuracy compared to \(_{1}\)-norm and \(_{2}\)-norm pruning, respectively. BCBP  is a block-wise pruning method that can be applied PW-conv. However, applying BCBP to PW-conv following DW-conv eliminates some parameters of DW-conv. Therefore,

    &  &  &  &  \\   & DW-conv & PW-conv &  &  &  &  &  \\   MobileNet-V2\({}^{*}\) & - & - & - & 71.9\% & - & - & 1.00\% & 1.00\% & 2306 \\ CaffeNet-R  & 37.1\% & 37.1\% & 73.7\% & 78.62\% & -5.5\% & 1.44\% & 1.46\% & 1581 \\ AMC  & - & - & 30.0\% & 71.8\% & 70.8\% & -1.0\% & - & - & - \\ CC  & - & - & 28.3\% & 71.9\% & 70.9\% & -1.0\% & - & - & - \\ MetaPruning  & - & - & 30.7\% & 72.0\% & 71.2\% & -0.8\% & - & - & - \\ Random-Pruning  & - & - & 29.1\% & 71.9\% & 70.9\% & -1.0\% & - & - & - \\ AMC  & - & - & 30.1\% & 71.9\% & 72.0\% & +0.1\% & - & - & - \\ RAL  & - & - & 29.4\% & 71.8\% & 71.3\% & -0.5\% & - & - & - \\ GFS  & 42.8\% & 42.8\% & - & 72.0\% & 68.8\% & -3.2\% & 1.58\% & 1.60\% & 1448 \\ GFS  & 37.1\% & 37.1\% & - & 72.0\% & 69.7\% & -2.3\% & 1.44\% & 1.46\% & 1581 \\ CaffeNet-R  & 22.8\% & 22.8\% & - & 73.7\% & 71.9\% & -1.8\% & 1.22\% & 1.23\% & 1871 \\ CaffeNet-E  & 14.2\% & 14.2\% & - & 73.7\% & 72.4\% & -1.3\% & 1.15\% & 1.16\% & 1992 \\ AMC  & 17.1\% & 17.1\% & - & 72.0\% & 70.8\% & -1.2\% & 1.17\% & 1.20\% & 1971 \\ GFS  & 22.8\% & 22.8\% & - & 72.0\% & 71.2\% & -0.8\% & 1.22\% & 1.23\% & 1871 \\ CaffeNet-R  & 14.2\% & 14.2\% & - & 73.7\% & 73.3\% & -0.4\% & 1.15\% & 1.16\% & 1992 \\
**DErune-BH [ours]** & **77.9\%** & **52.7\%** & 56.1\% & 71.9\% & 71.6\% & **-0.3\%** & **3.52\%** & **2.48\%** & 930 \\
**DErune-BH [ours]** & **75.1\%** & **64.8\%** & 66.2\% & 71.9\% & 71.0\% & **-0.9\%** & **3.11\%** & **2.70\%** & 853 \\  EfficientNet-B0\({}^{*}\) & - & - & 77.6\% & - & - & 1.00\% & 1.00\% & 6650 \\ CaffeNet-R  & 30.2\% & 30.2\% & - & 76.4\% & 74.5\% & -1.9\% & 1.41\% & 1.37\% & 4848 \\ CaffeNet-E  & 26.4\% & 26.4\% & - & 76.4\% & 74.6\% & -1.8\% & 1.34\% & 1.30\% & 5085 \\
**DErune-BH [ours]** & **84.7\%** & **62.0\%** & - & 77.6\% & 76.8\% & **-0.8\%** & **6.15\%** & **3.74\%** & 1775 \\  MobileNet-V3-Small\({}^{*}\) & - & - & 67.7\% & - & - & 1.00\% & 1.00\% & 1.00\% & 1857 \\ GFS  & 20.0\% & 20.0\% & - & 67.5\% & 65.8\% & -1.7\% & 1.24\% & 1.23\% & 1499 \\
**DErune-BH [ours]** & **82.1\%** & **70.0\%** & - & 67.7\% & 67.1\% & **-0.6\%** & **5.29\%** & **4.12\%** & 450 \\  MobileNet-V3-Large\({}^{*}\) & - & - & - & 74.0\% & - & - & 1.00\% & 1.00\% & 4892 \\ FPGM  & 33.0\% & 33.0\% & - & 74.0\% & 73.1\% & -0.9\% & 1.48\% & 1.47\% & 3945 \\
**DErune-BH [ours]** & **77.0\%** & **43.0\%** & - & 74.0\% & 73.7\% & **-0.3\%** & **4.13\%** & **2.83\%** & 1187 \\   

Table 4: Comparison of inference time (\(us\)) with DEPrune-BH and the latest structured pruning on ImageNet dataset. ‘Diff.’ denotes the difference in Top-1 accuracy between the baseline and pruned models. DEPrune-BH applies filter pruning using \(_{2}\)-norm to PW-conv . This symbol (\(\)) means ‘baseline model’.

when applying BCBP to PW-conv the accuracy drops on all models described in Table 5 compared to FP and FPGM.

## 7 Conclusion

In this work, we propose a new Depth-wise Separable Convolution Pruning (DEPrune) method tailored for DW-conv to reduce DSConv inference time and fully leverage GPU features. Extensive experimental results on the ImageNet dataset demonstrate that DEPrune effectively preserves representation power, even with higher \(PR\) than structured pruning, achieving a regular sparsity. Moreover, two techniques, BWT and HSR, further enhance DEPrune's capabilities. With these combined features, DEPrune-BH achieves substantial GPU speed gain of up to \(4.1\) on MobileNet-V3-Small.

## 8 Acknowledgements

This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.RS-2024-00402898, Simulation-based High-speed/High-Accuracy Data Center Workload/System Analysis Platform), (RS-2021-II212068, Artificial Intelligence Innovation Hub), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2024-00357037).