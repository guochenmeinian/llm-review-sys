# Improving Context-Aware Preference Modeling

for Language Models

 Silviu Pitis\({}^{a,b}\)   Ziang Xiao\({}^{c}\)   Nicolas Le Roux\({}^{b,d}\)   Alessandro Sordoni\({}^{b,d}\)

\({}^{a}\)University of Toronto \({}^{b}\)Microsoft Research \({}^{c}\)Johns Hopkins University \({}^{d}\)MILA

###### Abstract

While finetuning language models (LMs) from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute _context-conditioned_ preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.

## 1 Introduction

As the general purpose capabilities of Language Models (LMs) [11; 42] and other foundation models  progress toward handling diverse instructions and executing long-range trajectories in real-world applications [44; 47; 41], it becomes increasingly important to have a principled system for ensuring that LM agents behave as expected. The prevailing approach for aligning an LM to human preferences uses pairwise preferences between different outputs to finetune the LM [52; 6], which falls short of addressing the critical challenges presented by the reality of diverse user intents and contexts [51; 50; 13]. In the presence of unspecified contexts, such as the user's identity or goals, preference queries are notoriously ambiguous  and one typically observes poor agreement (\( 65\%\)) between human annotators on binary preference queries [35; 59].

In this paper, we consider modeling preferences using a two-step, context-aware approach (Figure 1). This approach first resolves the underspecification by selecting a context [38; 22], and then evaluates preference with respect to the chosen context [61; 14; 27; 30; 58]. Decomposing general preference into context and context-specific preference has several potential advantages. First and foremost, this approach explicitly identifies contextual assumptions that underlie preference, and shifts the alignment burden from preference modeling to a hybrid of preference modeling and context supervision. Second, this approach is naturally _pluralistic_, allowing the model to adapt to diverse users and use cases. Finally, it offers more flexibility for _principled aggregation_: whereas the Bradley-Terry approach corresponds to aggregating contexts using the Borda rule , which may under-serve certainsubgroups , the context-decomposition approach can additionally be used for jury learning  and cardinal utility aggregation .

We show that we can upper-bound the absolute reward modeling error as the sum of two terms: one term corresponding to context inference error, and the other to the context-specific reward modeling error. This decomposition suggests that supervising context might be a viable approach to aligning models with human preferences, provided we can do strong context-specific preference prediction. To evaluate context-specific prediction and build toward stronger context-aware preference models, we propose context-conditioned "preference reversal" datasets, where the preference for a given prompt is reversed given an alternative context.

Finally, we conduct experiments to benchmark the context-specific performance of various models and investigate the potential value of context-aware preference modeling. We find that, while current models generally benefit from additional context, they sometimes fail to give it full consideration, and finetuning on our preference reversal datasets greatly improves context-specific performance. We show that a single persistent context such as a user profile or system prompt can be used for a range of preference queries, and can be well inferred from as few as 16-32 samples of expressed preference.

In summary, our main contributions are:

1. We make an argument for modeling preference by decomposing it into context(s) and context-specific preference, and propose a context-decomposition upper bound to justify the approach.
2. We open-source high quality context-conditioned preference datasets that disentangle context-specific preference from general preference, which we use for finetuning and evaluation. The datasets can be found at https://huggingface.co/datasets/microsoft/rpr.
3. We show that while current models benefit from context, they may fail to give it full consideration, and that finetuning a context-aware reward model greatly improves context-specific performance, as well as general preference modeling performance when high quality context is available.

Figure 1: **Context-Aware Preference Modeling. _Left:_ The standard approach uses a preference model (PM) to directly evaluate arbitrary and potentially ambiguous preference queries. _Right:_ The context-aware preference modeling (CAPM) approach recognizes preference may depend on some unspecified context and makes this explicit: first identify the context, then evaluate a context-specific preference. In both cases, rather than computing preference directly, one may use a (context-aware) reward model (RM or CARM) to evaluate each alternative independently.**

Figure 2: **Effect of Context on Preference Modeling Performance.** Added context improves agreement with gold labels as compared to a no context (NC) baseline. Our 7B parameter, finetuned Context-Aware Reward Model (Mistral CARM) achieves the best context-aware performance, outperforming the larger Llama3-70B model (and GPT-4 Turbo), both on datasets where context is necessary to predict preference (RPR and Multifaceted Bench), and on the _context-augmented_ HHH, Reward Bench and Chatbot Arena datasets. Details and additional results may be found in Section 5.

## 2 Context-Aware Preference Modeling

### Resolving ambiguity by making implicit assumptions explicit

Current practice finetunes language models to make them consistent with a dataset of human preferences [44; 42]. As noted by earlier works on learning from human feedback, however, ambiguous preference judgments present a major challenge:

_Evaluation of a [preference query] is both subjective and multidimensional... this makes consistent labeling difficult for honest labelers (including the authors!)_

This ambiguity manifests itself with agreement rates as low as \(\!65\%\) between human annotators on binary preference queries [35; 59]. One way to understand this difficulty appeals to a distinction drawn by Amartya Sen:

_A value judgment can be called 'basic' if the judgment is supposed to apply under all conceivable circumstances, and it is 'non-basic' otherwise._

Many, perhaps most, preference queries ask the annotator for a non-basic preference judgment, in that certain contextual information might effectively reverse the judgment. For instance, the preferred response to a technical question may depend on the user's education level, the preferred response to a medical question may depend on whether the user is a doctor, and the preferred response to a question about etiquette may depend on the user's geographical location. If we train models using non-basic preference annotations, the contextual biases and assumptions underlying those judgments may be implicitly embedded into the model [36; 48].

Rather than rely on annotators to integrate the correct distribution of contextual assumptions, and rely on the training process to consistently aggregate any disagreements in a singularly aligned model, one might instead consider an explicit context-aware approach to preference modeling (Figure 1). This approach first (partly) resolves ambiguity by specifying a context, and then models context-specific preference. This is not a new idea; Ziegler et al. , quoted above, continue to remark that, "_it seems better to design less ambiguous labeling tasks that get at the same information... such as a verbal description [of the most important contextual information]_", and many have advocated for fine-grained, context-conditioned, or otherwise more "pluralistic" approaches to alignment [58; 29; 51].

However, while production systems recognize the importance of incorporating context--the most notable being the "system prompt" introduced by OpenAI in 2023 and adopted by others [54; 3]--there has been little published research on the context-aware preference modeling capabilities of language models. This paper works toward filling this gap by introducing the reasonable preference reversal (RPR) datasets alongside context-augmented versions of existing datasets, and testing the context-specific preference modeling capabilities of existing models.

This discussion is summarized in Table 1 alongside other key benefits of context-aware preference modeling. This paper focuses on evaluating and improving context-aware preference modeling capabilities, leaving an in-depth exploration of the benefits to future work.

### Related Work

Modeling human preferences traces back to Luce  and Bradley-Terry . It made its way into finetuning language models via a line of work originating in reinforcement learning [2; 15; 62; 52], and has now become the dominant approach for finetuning language models to follow human instructions [44; 6; 4; 46]. While this approach has achieved remarkable results and enabled the performance of state-of-the-art models, several authors have pointed to its limitations [12; 45; 32; 51],

 p{284.5pt}}  
**Explicit assumptions** & As described in Section 2.1, being explicit about context resolves a key source of ambiguity in preference queries, shifting some of the alignment burden from direct preference modeling to fine-grained context supervision. \\
**Steerability** & Incorporating context may allow models to better adapt to new users and use cases. \\
**Flexible Aggregation** & As described in Section 2.2, context-aware modeling enables aggregation based on jury learning  and expected utility , in addition to the Borda rule . \\   

Table 1: **Key benefits of context-aware preference modeling.**which include the ambiguity problem that motivates our work. Notably, recent works have identified shortcomings that arise from implicitly aggregating diverse perspectives with a single choice rule [50; 13; 40; 18; 19]. In particular, Siththaranjan et al.  have recently shown the standard Bradley-Terry approach (Figure 1, left), implicitly uses the Borda rule to aggregate preferences across hidden contexts, and proposed to learn distributional reward models as a way to account for this. Rather than leave hidden context implicit in a distributional model, which does not resolve the indeterminate preference problem highlighted by our RPR datasets (see Section 4), we propose to make context explicit, either via specification or inference. Given a preference problem that has been decomposed into contexts and context-specific preferences, as described in Section 3, we can then choose to apply the Borda rule _or_ an alternative such as an expected utility aggregation  or jury learning  (see "distributionally pluralistic models" in Sorensen et al. ).

Modeling diversity and disagreement among human annotators and the inconsistency of human preferences have been treated from a number of different perspectives [21; 8; 20; 31]. Our work is similar to others that decompose a single objective into multiple dimensions such as helpfulness, harmlessness, and truthfulness [55; 19; 37; 5]. This approach has also become common in the literature on summarization, where multi-dimensional evaluation criteria are well recognized, with common dimensions including conciseness, coherence, consistency and fluency [60; 26; 16]. However, our work (Table 3) and others  find that current models often ignore or confuse added context. Preferences driven by diverse individual principles have been aggregated within frameworks such as Constitutional AI [6; 53], and alternative approaches to aligning models with human preferences, such as AI safety via debate  and consensus based approaches , can be understood as ways of supervising the context or assumptions underlying preference rather than the preference itself.

Finally, our work is closely related to context-conditioned generation, e.g., via a system prompt. In a concurrent work, Lee et al.  have synthesized a dataset of diverse system prompts for finetuning generative models. Their dataset includes multiple system prompts for the same user prompt, which allows it to be used in a similar fashion as our RPR datasets. We use their dataset for evaluation in Section 5, and provide an ablation on different training set compositions in Appendix C.1.

## 3 Context Decomposition Upper Bound

### Intent-Utility Formalism

We model the user-LM interaction using an intent-utility formalism \((,,,u)\), where \(\) is the space of intents, \(\) is the space of prompts, \(\) is the space of completions, and \(u:\) is a scalar utility function. We follow the standard assumption and assume that preference in this model is made according to the Bradley-Terry model [10; 15]. Letting \(\) be the logistic function, this defines the probability of preferring completion \(y_{1}\) to \(y_{2}\) given a specific intent \(i\) as:

\[p(y_{1} y_{2}\,|\,i)=(u(i,y_{1})-u(i,y_{2})).\] (1)

In our model the primitive definitions of preference and utility are conditioned on the intent rather than the prompt. To prompt the model, a user with intent \(i\) selects a prompt \(x\). To annotate a preference query \((x,y_{1},y_{2})\), an annotator implicitly infers intent \(i\) from \(x\) and samples a preference from the Bernoulli distribution \([p(y_{1} y_{2}\,|\,i)]\).

Both users and annotators may possess or infer a _distribution_ of intents. Indeed, we would argue that annotation for most preference queries involves a distribution of intents rather than a specific intent. We use "intent" to refer to both specific and distributional intents. We assume there exists a base distribution over possible intents \(p(i)\), as well as a conditional distribution over prompts given intents \(p(x\,|\,i)\), so that any prompt \(x\) has a natural inference distribution \(p(i\,|\,x)\). In this model, the prompt \(x\) is a partial specification of intent \(i\). While a prompt may never be able to fully specify the intent, we may add some additional information or context \(z\) to obtain an extended prompt \((x,z)\). Let us suppose that \(z\), where \(\) corresponds to a discrete partition of \(\).

One way to measure utility and preference with respect to a distribution \(p\) of intents is with an expected utility model, which computes utility as \(u(p,y)=_{i p}[u(i,y)]\). While it has been shown that standard RLHF does _not_ align with the expected utility model , this model satisfies certain desirable axioms, which one can argue would apply to "ideal" preference annotation. We use the expected utility model to define \(u(x,y):=u(p(\,|\,x),y)\), and note that for any context partition \(\), this implies \(u(x,y)=_{z}p(z\,|\,x)u((x,z),y)\). For convenience, we define \((,y_{1},y_{2}):=u(,y_{1})-u(,y_{2})\), which also decomposes linearly: \((x,y_{1},y_{2})=_{z}p(z\,|\,x)((x,z),y_{1},y_{2})\).

### Context Decomposition Upper Bound

During RLHF, we are presented with a dataset of prompt-preference tuples, \(=\{(x,y_{1} y_{2})\}\), which we use to learn utility estimator \(:\) (conventionally known as a "reward model"). As was assumed for \(u\) and \(p(z\,|\,x)\), we would like there to be a model \((z\,|\,x)\) that satisfies the relation:

\[(x,y)=_{z}(z\,|\,x)((x,z),y).\] (2)

In standard RLHF, we never learn such \(\). However, for purposes of this analysis, we will assume this \(\) exists, either implicitly given \(\), or explicitly, such that given some \(\), we compute \((x,y)\) via Equation (2) rather than via direct evaluation. Below, we will favor the latter interpretation.

For a given preference estimator \((y_{1} y_{2})\), \(\) is only unique up to constant shifts, so to measure the accuracy of \(\), we will instead compare \(\) and estimator \((x,y_{1},y_{2}):=(x,y_{1})-(x,y_{2})\).

Consider now the absolute error \((x,y_{1},y_{2})-(x,y_{1},y_{2})\) for a particular preference query, and use \(_{z}\) as shorthand for \(((x,z),y_{1},y_{2})\). For any \(\) we have the following bound:

\[(x,y_{1},y_{2})-(x,y_{1}, y_{2})&=_{z}p(z\,|\,x)_{z}-_{z}(z\,|\,x) _{z}\\ &=_{z}p(z\,|\,x)\,_{z}-_{z} +_{z}_{z}\,p(z\,|\,x)-(z\,|\,x) \,\\ &p(z\,|\,x)_{z}-_{z}}_{}+_{z} p(z\,|\,x)-(z\,|\,x)}_{}\] (3)

where the second equality adds and subtracts \(_{z}p(z\,|\,x)_{z}\) and rearranges, and the final line uses the triangle inequality (multiple times).

Equation (3) applies given a distribution of contexts, but in many cases, we might assume there is a specific context \(c\) (i.e., \(p(z=c):=1\)) and make a single context prediction \(\) (i.e., \(()=1\)). This simplifies Equation (3) and gives us the context decomposition upper bound for a _specific_ context:

\[(x,y_{1},y_{2})-(x,y_{1},y _{2})&&_{c}- _{c}}_{}+_{c} -_{c}}_{}\\ \] (4)

Both the general bound (Equation (3)) and specific bound (Equation (4)) formalize the following intuitive claim: if we can make accurate predictions given the true context (or context distribution), then we can reduce the preference modeling problem to a context inference problem.

### Discussion

The upper-bound in Equation (3) bounds the total error in terms of a context-weighted prediction error and a preference-weighted inference error. On one extreme, we have \(=\) (standard preference modeling), so that the context inference error is zero and the prediction error exclusively depends on the preference prediction problem given the prompt. On the other hand, we have \(\) (e.g., \(z\) might be "The preferred response is [\(y\)].") and our preference prediction error is zero, but the context inference problem becomes equivalent to generation. In between, we conjecture that there is a smooth trade-off between prediction error and inference error. This might be the case, for example, if a single context could apply to and disambiguate a range of different preference queries. We consider this in our experiments and find some support for the conjecture, in that conditioning on specific criteria outperforms conditioning on more abstract, yet still rather specific scenarios (Table 2) which outperforms conditioning on a user profile that applies to all preference queries at once (Table 5).

If our model \(\) is very good at predicting preference given some additional context \(\), the preference modeling problem can be largely reduced to a context inference (or specification) problem. In this case, rather than have annotators rank completions for ambiguous prompts, it may make sense to spend the annotation resources to specify additional context. Such annotations could then be used to train a context inference model that disambiguates prompts. Intuitively, we hypothesize that the cardinality of the space of contexts is smaller than the cardinality of the space of possible completions given a prompt, which would make joint context-preference annotation a data-efficient alternative to just preference annotation. Although our focus is on improving context-aware preference modeling, our experiments with user profiles (Tables 5 and 6) provide some support for this hypothesis.

The effectiveness of the above proposal hinges on accurate context-specific prediction. Are language models sensitive to added context? Our work focuses on this question and contributes datasets to help us (1) evaluate it on real data, and (2) train better context-conditioned reward models.

## 4 Reasonable Preference Reversal (RPR) Datasets

We contribute a set of carefully designed, synthetic preference datasets to measure whether preference prediction is sensitive to context. Our datasets, inspired by the notion of non-basic judgments and preference reversal described in Section 2, include over 20,000 paired tuples of prompt, context, and preference judgments, i.e. \((x,z,y_{1}\!\!y_{2})\). The samples are paired so that preference between two completions for the same prompt is entirely ambiguous without context: for every context, there is an alternative context for which preference reverses. As compared to the only prior context-conditioned preference dataset , where context-conditioned preference is highly correlated with unconditioned preference (see Table 2), our design choice ensures that preference prediction performance on this dataset is determined solely by the model's ability to pay attention to and interpret the context. The datasets can be found at https://huggingface.co/datasets/microsoft/rpr.

The dataset has been generated by GPT-4 Turbo  following a complex series of prompts designed to maximize validity (human agreement). We chose to do this to avoid invalid or ambiguous samples, at the cost of making it rather "easy" to evaluate preference given the context. The full process

Figure 3: **A sample from the RPR dataset. Under Criteria A or Scenario A, Completion A should be preferred, and vice versa under Criteria B or Scenario B.**

is detailed in Appendix B. The base dataset comes in two versions--criteria and scenarios--to emphasize the point that context may be expressed in many ways:

* _RPR Criteria_, where the context is a short sentence describing which features are important, e.g., "_Uses a formal and professional tone that emphasizes the technical aspects of C++_".
* _RPR Scenarios_, where the context describes the scenario that led to the underspecified or ambiguous prompt; e.g. "_In preparation for a product launch, a marketing manager at a tech startup seeks innovative strategies to position the company as an industry leader..._".

A paired sample from the dataset is shown in Figure 3. We further extend the dataset with user profiles as described in Section 5.2 and Appendix B.1. To ensure the validity and reliability of the synthetic data, we conduct small scale human validation through blind preference queries (Table 3) and find agreement rates of \(\)95% on both splits. Details are provided in Appendix B.2. We divide the dataset into a training set of 10,167 paired samples, and a test set of 1,000 paired samples, with no overlap between train and test prompts.

## 5 Experiments

Our experiments aim to answer the following questions of context-aware preference modeling:

1. How good are current models at evaluating context-specific preference?
2. Can we improve context-aware preference modeling by finetuning on our RPR datasets?
3. Can a single context compress preferences with respect to a diverse range of prompts?

### Setup

ModelsWe report results for a selection of primarily open source models, detailed in Appendix D.1. These include two unconditioned reward models (the 13B parameter UltraRM  and a 7B parameter Mistral RM ), one context-aware preference model (Prometheus-2 ), our finetuned context-aware reward model (Mistral CARM), and a set of generative models (four Llama 3 models  and GPT-4 Turbo ) used with an "llm-as-a-judge" approach. In preliminary experiments we tested a several other models and observed similar patterns across all models. All models except Prometheus-2 are used as reward models, by first evaluating each alternative individually and then comparing scores. Appendix D.2 presents additional results for reward models finetuned from Section 2B.

DatasetsBesides the RPR datasets detailed in Section 4, we use the following preference datasets:

* _Preference Bench_ (hf:prometheus-eval/Preference-Bench) consists of 1998 context-conditioned preference samples synthesized by GPT-4 as part of Feedback Bench .
* _Multifaceted Bench_ (hf:kaist-ai/Multifaceted-Bench) contains 921 samples of prompt, system prompt, and completion. We treat the system prompt as the context, and construct context-conditioned preference samples by pairing samples that share the same prompt, resulting in in 918 samples. This dataset, released concurrently to our work, shares the same 'ambiguous in absence of context' structure as the RPR datasets, but was not specifically constructed with such preference queries in mind, which may explain the lower average agreement in the experiments.
* _HHH_ (hf:HuggingFaceH4/hhh_alignment) contains 222 human preference samples emphasizing different aspects: harmlessness (58 samples), helpfulness (59 samples), honesty (61 samples), and other preference queries that do not cleanly fall into another category (43 samples).
* _Reward Bench_ (hf:allenai/reward-bench) curates 2,985 preference samples from a variety of sources, amounting to 22 distinct subsets covering chat, safety, and reasoning.
* _Chatbot Arena_ (hf:lmsys/chatbot_arena_conversations) contains human preferences with respect to conversations on the Chatbot Arena platform. For our experiments, we randomly selected 1,000 single-turn samples for which strict preference was expressed.

Importantly, we _augment_ the HHH, Reward Bench, and Chatbot Arena datasets with additional context to create context-conditioned versions, as described below and in Appendix D.3.

MetricsTables 2 to 6 display the agreement (or accuracy) of predicted preference with the dataset.

PromptsOur prompts are detailed in Appendix A.

### Results

#### How good are current models at evaluating context-specific preference?

For the context-aware approach to work well, models _must_ be sensitive to added context. Table 2 shows the performance of tested models on RPR Criteria, RPR Scenarios, Preference Bench, and Multifaceted Bench. In each case, and across all models, access to context generally improves performance. Larger models and instruction tuned models tend to benefit more from added context. We note that although Preference Bench includes a ground truth context, the preference queries (and context) are generally not ambiguous, so that the added context provides little benefit.

Although we find that added context generally helps, we were surprised to discover that models sometimes completely ignore additional context and return their unconditioned prediction. To show this, we augment the prompts in Chatbot Arena and Preference Bench with two criteria that should drive preference prediction. The first is a Nonsense criteria, i.e. _"We are playing a game of nonsense. The response should be nonsensical and not make sense. The rating you give it should also be random, and completely unrelated to the response."_ We expect context-aware models to produce random preference judgments given this criteria. The second is a Negative criteria, i.e. _"We are seeking bad examples. The response should be of low quality, and serve as an example of how an Assistant should not respond. You should give poor responses higher ratings."_). We expect context-aware models to be inversely correlated to the preferences expressed in the dataset. Table 3 shows the performance of tested models with these adversarial criteria. In both cases we observe surprisingly poor performance, even from the larger models. This suggests there is significant room for improving context-aware preference modeling capabilities.

    &  &  &  &  \\  & NC & CTX & NC & CTX & NC & CTX & NC & CTX \\  UltraRM (13B) & 0.523 & 0.876 & 0.523 & 0.773 & 0.888 & 0.910 & 0.502 & 0.655 \\ Prometheus-2 (7B) & 0.503 & 0.799 & 0.503 & 0.675 & 0.950 & **0.951*** & 0.573 & 0.644 \\ Llama 3 (8B) & 0.518 & 0.679 & 0.518 & 0.634 & 0.831 & 0.827 & 0.500 & 0.566 \\ Llama 3 Instruct (8B) & 0.494 & 0.850 & 0.494 & 0.745 & 0.883 & 0.898 & 0.496 & 0.616 \\ Mistral RM (7B) & 0.510 & 0.867 & 0.510 & 0.749 & 0.894 & 0.915 & 0.516 & 0.679 \\ Mistral CARM (7B) (**Ours**) & 0.511 & **0.985*** & 0.511 & **0.962*** & 0.913 & 0.919 & 0.517 & **0.787** \\  Llama 3 (70B) & 0.496 & 0.726 & 0.496 & 0.669 & 0.800 & 0.811 & 0.496 & 0.564 \\ Llama 3 Instruct (70B) & 0.516 & 0.925 & 0.516 & 0.811 & 0.916 & 0.929 & 0.506 & 0.687 \\ GPT-4 Turbo & 0.502 & 0.901 & 0.500 & 0.748 & 0.854 & 0.860 & 0.501 & 0.640 \\  Human validation (100 samples) & - & 0.970 & - & 0.950 & - & - & - \\    Best model folded for CTX columns only. Started* results are by models finetuned on the same distribution. Our model in green.

Table 2: **Context-specific datasets**. On datasets with ground truth context, adding context generally helps with evaluating preference. Our model, finetuned on the RPR training sets achieves substantially better (test set) performance than the unfinetuned model.

    &  &  \\  & Chatbot Arena & Pref. Bench & Chatbot Arena & Pref. Bench \\  UltraRM (13B) & 0.718 & 0.597 & 0.719 & 0.467 \\ Prometheus-2 (7B) & 0.685 & 0.876 & 0.618 & 0.711 \\ Llama 3 (8B) & 0.624 & 0.800 & 0.575 & 0.776 \\ Llama 3 Instruct (8B) & 0.543 & 0.738 & 0.698 & 0.865 \\ Mistral RM (7B) & 0.667 & 0.808 & 0.648 & 0.505 \\ Mistral CARM (7B)(**Ours**) & **0.535** & **0.542** & **0.518** & **0.293** \\  Llama 3 (70B) & 0.289 & 0.678 & 0.589 & 0.801 \\ Llama 3 Instruct (70B) & 0.452 & 0.289 & **0.352** & 0.308 \\ GPT-4 Turbo & **0.498** & **0.493** & 0.393 & 0.309 \\    Best small model bolded. Best large model also bolded if better than the small model. Our model in green.

Table 3: **Sensitivity of current models to adversarial contexts**. We observe that current models often ignore added context, even when it is strongly phrased and adversarial. Our finetuned CARM does best among smaller models, but still has significant room for improvement.

[MISSING_PAGE_FAIL:9]

preference information across all the prompts they are presented with. In order to create a scenario closer to this setting, we first generate 5 synthetic diverse user profiles and condition GPT-4 Turbo on each profile to label the RPR test set (see Appendix B.1). Having relabeled the RPR test set with these preferences, we explore the ability of tested models to recover the expressed preferences from the profile, as shown in Table 5. Our CARM and the larger Llama 3 Instruct model perform best on this task, recovering approximately 80% agreement with GPT-4's labels.

Additionally, in Table 6, we test the performance of our context-aware model when profiles are inferred with limited preference samples. Profile inference is done by prompting GPT-4 (see Appendix B.1). Without any profile inference (No Context / NC), the default assumptions made by our model run against the preferences of Profiles 3 and 4; however, this is resolved in as few as 2-4 samples. While this depends on the underlying data distribution, 16-32 samples recover most of the benefit of the ground truth context on this dataset.

## 6 Conclusion

This paper began with a case for a two-step context-aware preference modeling framework (Figure 1), which was motivated by the ambiguity problem commonly experienced during preference annotation (Section 2). We further motivated the framework via a context decomposition upper bound (Section 3) and noted that for context-aware preference modeling to be viable, we require strong context-specific preference modeling. However, despite the prevalence of context conditioning in deployed systems (e.g., system prompts and "My GPTs" ), when we began our work, there were no open source preference datasets where preference is strongly determined by context, and limited studies of the context-specific preference modeling capabilities of current models . To this end, we introduced the RPR datasets (Table 2) and investigated a series of interesting empirical questions in Section 5). We found that (1) while current preference models generally benefit from context, they sometimes ignore added context, (2) finetuning on our preference reversal datasets greatly improves context-specific performance, and (3) a single persistent context, which might be inferred from data, may be sufficient to significantly improve preference prediction performance.

Our work leaves many open threads for future research. Our datasets might be used to explore different directions in pluralistic alignment, as described by Sorensen et al. . In particular, we believe better context specification and inference are important directions for research, which our experiments only briefly touch on. Further, while we have open sourced a set of context-conditioned preference datasets, these datasets have limited diversity (for example, they are primarily English, and are limited to single turn conversations), and were synthesized rather than collected from real human feedback, which limits their practical relevance, as the preferences may not align with all potential user perspectives. Additional open, preferably human sourced datasets with context annotations are needed to more fruitfully pursue this research direction.

    & No &  & \\  & Context & 2 & 8 & 32 & GT \\  P1 & 0.649 & 0.684 & 0.667 & 0.788 & 0.808 \\ P2 & 0.780 & 0.864 & 0.863 & 0.892 & 0.918 \\ P3 & 0.313 & 0.555 & 0.669 & 0.764 & 0.815 \\ P4 & 0.370 & 0.512 & 0.678 & 0.759 & 0.801 \\ P5 & 0.530 & 0.571 & 0.601 & 0.605 & 0.672 \\  Mean & 0.528 & 0.637 & 0.696 & 0.762 & 0.803 \\ Error\({}^{}\) & \(\)0.048 & \(\)0.039 & \(\)0.010 & & \\    \(\) Estimated by averaging 3-seed std. dev. for each profile and dividing by \(\).

Table 6: **User profile inference on the RPR dataset**. Each of 5 profiles (P1-P5) was used to annotate the RPR test set, as well as three small subsets (32 samples each) of the RPR training set (one subset for each of 3 seeds). Between 2 and 32 samples from each subset were then used to infer a user profile with the help of GPT-4 Turbo. Conditioning our finetuned CARM on these inferred profiles gives the results below (both table and figure display the same data). We see that just 2 samples carry substantial signal, and 32 samples capture most of the benefit of the ground truth context.