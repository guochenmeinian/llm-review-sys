# Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning

Tyler Kastner

University of Toronto, Vector Institute

&Murat A. Erdogdu

University of Toronto, Vector Institute

&Amir-massoud Farahmand

Vector Institute, University of Toronto

Correspondence to: tkastner@cs.toronto.edu.

###### Abstract

We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.

## 1 Introduction

Reinforcement learning is a general framework where agents learn to sequentially make decisions to optimize an objective, such as the expected value of future rewards (risk-neutral objective) or the conditional value at risk of future rewards (risk-sensitive objective). It is a popular belief that a truly general agent must have a world model to plan with and limit the number of environment interactions needed (Russell, 2010). One way this is achieved is through model-based reinforcement learning, where an agent learns a model of the environment as well as its policy which it uses to act.

The classical approach to learning a model is to use maximum likelihood estimation (MLE) based on data, which given a model class selects the model which is most likely to have produced the data seen. If the model class is expressive enough, and there is enough data, we may expect a model learnt using MLE to be useful for risk-sensitive planning. However, the success of this method relies on the model being able to model everything about the environment, which is an unrealistic assumption in general.

As opposed to learning models which are accurate in modelling every aspect of the environment, recent works have advocated for learning models with the decision problem in mind, known as decision-aware model learning (Farahmand et al., 2017; Farahmand, 2018; D'Oro et al., 2020; Abachi et al., 2020; Grimm et al., 2020, 2021). In particular, Farahmand et al. (2017) introduced _value-aware model learning_, which uses a model loss that weighs model errors based on the effect the errors have on potential value functions. This framework has since been iterated on and improved upon in later works such as Farahmand (2018); Abachi et al. (2020); Voelcker et al. (2021). Complementarily, Grimm et al. (2020) introduced the _value equivalence principle_, a framework of partitioning the space of models based on the properties of the Bellman operators they induce. This framework has been extended by Grimm et al. (2021), where the authors introduce a related partitioning, called_proper value equivalence_, based on which models induce the same value functions. They substantiate this approach by demonstrating that any model in the same equivalence class as the true model is sufficient for optimal planning.

While standard reinforcement learning maximizes the expected return achieved by an agent, this may not suffice for many real-life applications. When environments are highly stochastic or where safety is important, a trade-off between the expected return and its variability is often desired. This concept is well-established in finance, and is the basis of modern portfolio theory (Markowitz, 1952). Recently this approach has been used in reinforcement learning, and is referred to as _risk-sensitive reinforcement learning_. In this setting, agents learn to maximise a _risk measure_ of the return which is possibly different from expectation (in the case it is expectation, it is referred to as risk-neutral), and may penalize or reward risky behaviour (Howard and Matheson, 1972; Heger, 1994; Tamar et al., 2015, 2012; Chow et al., 2015; Tamar et al., 2016). In particular, Grimm et al. (2021, 2022) has explored when optimal risk-neutral planning in an approximate model translates to optimal behaviour in the true environment. However, it is not clear when this holds for risk-sensitive planning.

In this paper, we propose a framework that consolidates risk-sensitive reinforcement learning and decision-aware model learning. Specifically, we address the following question: _if we can perform risk-sensitive planning in an approximate model, does it translate to risk-sensitive behaviour in the true environment?_ To this end, our work provides the following contributions:

* We prove that proper value equivalence only suffices for optimal planning in the risk-neutral case, and the performance of risk-sensitive planning decreases with risk-sensitivity (Section 3).
* We introduce the distribution equivalence principle, and show that this suffices for optimal planning with respect to _any_ risk measure (Section 4).
* We introduce an approximate version of distribution equivalence, which is applicable in practice, that allows one to choose which risk measures they may plan optimally for (Section 5).
* We discuss how these methods may be learnt via losses, and how it can be combined with any existing model-free algorithm (Section 6).
* We demonstrate our framework empirically in both tabular and continuous domains (Section 7).

#### Notation

We write \(()\) to represent the set of probability measures on a measurable set \(\). For a probability measure \(()\), we write \(X\) to denote a random variable \(X\) with law \(\), meaning for all measurable subsets \(A\), \((X A)=(A)\). For a probability measure \(()\) and a measurable function \(f:\), the pushforward measure \(f_{\#}()\) is defined by \(f_{\#}(Y)=(f^{-1}(Y))\) for all measurable sets \(Y\). For arbitrary sets \(X\) and \(Y\), we write \(Y^{X}\) for the space of functions from \(X\) to \(Y\).

## 2 Background

We consider a Markov decision process (MDP) represented as a tuple \((,,,,)\) where \(\) is the state space, \(\) is the action space, \(:()\) is the transition kernel, \(:()\) is the reward kernel, and \([0,1)\) is the discount factor. We define a policy to be a map \(:()\), and write the set of all policies as \(\). Given a policy \(\), we can sample trajectories \((X_{t},A_{t},R_{t})_{t 0}\), where for all \(t 0\), \(A_{t}(\,|\,X_{t})\), \(R_{t}(X_{t},A_{t})\), and \(X_{t+1}(X_{t},A_{t})\). For a trajectory from \(\) beginning at \(X_{0}=x\), we associate to it the return random variable \(G^{}(x)=_{t 0}^{t}R_{t}\). The expected return across all trajectories starting from a state \(x\) is the value function \(V^{}(x)=_{}[G^{}(x)]\). The value function is the unique fixed point of the Bellman operator \(T^{}:^{}^{}\), defined by

\[T^{}V(x)_{}[R+ V(X^{})],\] (1)

where \(_{}\) is written to indicate \(A(\,|\,x)\), \(R(x,A)\), and \(X^{}(x,A)\).

### Model-based reinforcement learning and the value equivalence principle

Estimating (1) in the RL setting is not possible directly, as generally an agent does not have access to \(\) nor \(\), but only samples from them. There are two common approaches to address this: model-freemethods estimate the expectations through the use of stochastic approximation or related methods (Sutton, 1988), while model-based approaches learn an approximate model \(}\), \(}\)(Sutton, 1991).

We will refer to a tuple \(=(},})\) as a model, and write \(\) for the set of all models. In turn, each model \(\) induces an approximate MDP \((,,},},)\). For a policy \(\), we write \(T^{}_{}:^{X}^{}\) for the Bellman operator in this approximate MDP, and we write \(V^{}_{}\) for the unique fixed point of this operator. We write \(m^{*}=(,)\) for the true model, and keep \(T^{}=T^{}_{m^{*}}\). Throughout the paper, we will write \(\) to represent a set of models which we are considering.

Traditional methods of model-based reinforcement learning learn a model \(\) using task-agnostic methods such as maximum likelihood estimation (Sutton, 1991; Parr et al., 2008; Oh et al., 2015). More recent approaches have focused on learning models which are accurate in aspects which are necessary for decision making (Farahmand et al., 2017; Farahmand, 2018; Schrittwieser et al., 2020; Grimm et al., 2020, 2021; Arumugam and Van Roy, 2022). Of importance to us is Grimm et al. (2021), which introduced proper value equivalence, and defined the set \(^{}()\{:V^{}=V^{} _{},\ \}\). They proved that any model \(^{}()\) suffices for optimal planning, that is, a policy which is optimal in \(\) is also optimal in the true environment, \(m^{*}\).

### Distributional reinforcement learning

Distributional reinforcement learning (Morimura et al., 2010; Bellemare et al., 2017, 2023) studies the return \(G^{}\) as a random variable, rather than focusing only on its expectation. For \(x\), we define the return distribution \(^{}(x)\) as the law of the random variable \(G^{}(x)\). The return distribution is the unique fixed point of the distributional Bellman operator \(^{}:()^{}()^{}\) given by

\[^{}(x)_{}[(b_{R,})_{\#} (X^{})],\]

where \(b_{R,}:x R+\,x\) and \(_{}\) is as in (1). As was the case in Section 2.1, any approximate model \(\) induces a distributional Bellman operator \(^{}_{}\), and we write \(^{}_{}\) for the unique fixed point of this operator.

### Risk-sensitive reinforcement learning

We define a risk measure to be a function \(:_{}()[-,)\), where \(_{}()()\) is its domain.2. A classic example is \(=\), which we refer to as the _risk-neutral_ case. When \(\) depends on more than only the mean of a distribution, we refer to \(\) as being _risk-sensitive_. The area of risk-sensitive reinforcement learning is concerned with maximizing various risk measures of the random return, rather than the expectation as done classically. We now present two examples of commonly used risk measures.

**Example 2.1**.: For \(>0\), the mean-variance risk criterion is given by \(^{}_{}()=_{Z}[Z]- _{Z}(Z)\)(Markowitz, 1952; Tamar et al., 2012). This forms the basis of modern portfolio theory (Elton and Gruber, 1997).

**Example 2.2**.: The conditional value at risk at level \(\) is defined as

\[_{}()_{0}^{}F_{}^{-1}( u)\,du,\]

where \(F_{}^{-1}(u)=\{z:(-,z] u\}\) is the quantile function of \(\). If \(F_{}^{-1}\) is a strictly increasing function, we equivalently have

\[_{}()=}_{Z}[Z\,|\,Z  F_{}^{-1}()],\]

so that \(_{}()\) can be understood as the expectation of the lowest \((100)\%\) of samples from \(\).

We say that a policy \(^{*}_{}\) is optimal with respect to \(\) if

\[(^{^{*}_{}}(x))=_{}( ^{}(x)),\, x.\]Since we define the space of policies as \(=()^{}\), we implicitly only considering the class of stationary Markov policies (Puterman, 2014). For a general risk measure, an optimal policy in this class may not exist (Bellemare et al., 2023). We discuss more general policies in Appendix D.

## 3 Limitations of value equivalence for risk-sensitive planning

Grimm et al. (2021) proved that any proper value equivalent model is sufficient for optimal risk-neutral planning. In this section, we investigate whether this holds for risk-sensitive planning as well, or is limited to the risk-neutral setting.

As an illustrative example, let us consider the MDP and approximate model \(\) in Figure 1. It is straightforward to verify that \(\) is a proper value equivalent model for the true MDP, as the value for any policy is 0 in both \(\) and the true environment. However, for a risk-sensitive agent \(\) is not sufficient: the variability of return when choosing action \(b\) in \(m^{*}\) is much higher than the variability of return when choosing action \(b\) in \(\). Formally, let us fix \(=\), and let \(^{b}\) be the policy which chooses action \(b\) with probability 1. Then \(^{^{b}}(x)=U([-2,2])\)(Bellemare et al., 2023, Example 2.10), while \(^{^{b}}_{}(x)=_{0}\) (where \(_{x}\) refers to the Dirac distribution concentrated at \(x\)). This difference prevents \(\) from planning optimally for risk-sensitive risk measures. For example, the optimal policy with respect to \(^{}_{}\) in \(m^{*}\) is to choose \(a\) with probability 1, while in \(\) any policy is optimal. It is straightforward to validate that similar phenomena happen for \(_{}\) when \(<1\).

As demonstrated in the example above, proper value equivalence is not sufficient for planning with respect to the risk measures introduced in Section 2.3. We now formalize this, and demonstrate that the only risk measures which proper value equivalence can plan for exactly are those which are functions of expectation.

**Proposition 3.1**.: _Let \(\) be a risk measure such that for any MDP and any set \(\) of models, a policy optimal for \(\) for any \(^{}()\) is optimal in the true MDP. Then \(\) must be risk-neutral, in the sense that there exists an increasing function \(g:\) such that \(()=g(_{Z}[Z])\)._

The previous proposition demonstrates that in general, the only risk measures we can plan for in a proper value equivalent model are those which are transformations of the value function. However, it does not address the question of how well proper value equivalent models can be used to plan with respect to other risk measures.

To investigate this question, we turn our attention to a class of risk measures known as _spectral risk measures_(Acerbi, 2002). Let \(:\) be a non-negative, non-increasing, right-continuous, integrable function such that \(_{0}^{1}(u)\,du=1\). Then the spectral risk measure corresponding to \(\) is defined as

\[_{}()_{0}^{1}F_{}^{-1}(u)(u)du,\]

where \(F_{}^{-1}\) is as in Example 2.2. Spectral risk measures encompass many common risk measures, for example choosing \(=_{}\) corresponds to expectation, while \(=_{[0,]}\) corresponds to \(_{}\).

We say that a spectral risk measure \(\) is \((,)\)-strictly risk-sensitive if it corresponds to a function \(\) such that \(()\). This implies that \(\) gives a weight of less than \(\) to the top \(\) quantiles, and so intuitively a larger \(\) and smaller \(\) implies a more risk-sensitive risk measure.

With this definition, we now demonstrate that when using proper value equivalent models to plan for strictly risk-sensitive spectral risk measures, there exists a tradeoff between the level of risk-sensitivity and the performance achieved.

Figure 1: An MDP with a single state and two actions (left), and a proper value equivalent model \(\) for it (right).

**Proposition 3.2**.: _Let \(\) be an \((,)\)-strictly risk-sensitive spectral risk measure, and suppose that rewards are almost surely bounded by \(R_{}\). Then there exists an MDP with a proper value equivalent model \(\) with the following property: letting \(^{*}_{}\) be an optimal policy for \(\) in the original MDP, and \(^{*}_{}\) an optimal policy for \(\) in \(\), we have_

\[_{x}^{^{*}_{}}(x)- ^{^{*}_{}}(x)}} }{1-}\;(1-(1-)).\]

The fact that we take an infimum over \(\) is important to note: there exists an MDP such that for _any_ state \(x\), the performance gap due to planning in the proper value equivalent model is at least \(}}{1-}\;(1-(1-)).\). This weakness motivates us to introduce a new notion of model equivalence.

## 4 The distribution equivalence principle

We now introduce a novel notion of equivalence on the space of models, which can be used for risk-sensitive learning. Intuitively, proper value equivalence ensures matching of the _means_ of the approximate and true return distributions, which is why it can only produce optimal policies for risk measures which depend on the mean. In order to plan for any risk measure, we leverage the distributional perspective of RL, to partition models based on their _entire_ return distribution.

**Definition 4.1**.: Let \(\) be a set of policies and \(()^{}\) be a set of distribution functions. We say that the space of _distribution equivalent_ models with respect to \(\) and \(\) is

\[_{}(,)\{:^{}=^{}_{},\;, \}\,.\]

We can extend this concept to equivalence over multiple applications of the Bellman operator. Following this, for \(k\) we define the order \(k\) distribution-equivalence class as

\[^{k}_{}(,) :(^{})^{k}=(^{}_{})^{k },\;,}.\]

Taking the limit as \(k\), we retrieve the set of proper distribution equivalent models.

**Definition 4.2**.: Let \(\) be a set of policies. We define the set of _proper distribution equivalent_ models with respect to \(\) as

\[^{}_{}()\{ :^{}_{}=^{},\;\}.\]

As discussed in Section 3, models in \(^{}()\) are sufficient for optimal planning with respect to expectation, but generally not with respect to other risk measures. We now show that proper distribution equivalence removes this problem: choosing a model in \(^{}_{}()\) is sufficient for optimal planning with respect to _any_ risk measure.

**Theorem 4.3**.: _Let \(\) be any risk measure. Then an optimal policy with respect to \(\) in \(^{}_{}()\) is optimal with respect to \(\) in \(m^{*}\)._

At this point, it appears that distribution equivalence addresses nearly all of the limitations of value equivalence discussed in Section 3. However, the nature of distributions brings inherent challenges, in particular they are infinite dimensional. As a result of this, for computational purposes one must use a parametric family of distributions \(()\)(Rowland et al., 2018; Dabney et al., 2018) to represent return distributions. However, an additional challenge is that the distributional Bellman operator may bring return distributions out of the parametric representation space: for a general \(^{}\), \(^{}^{}\). Hence, we also require a projection operator3\(_{}:()^{}^{ }\), and in practice we must use \(_{}^{}\). This also implies that it may not be feasible to learn a model \(\) in \(^{k}_{}(,)\) or \(^{}_{}()\): they rely on matching \(^{}\) or \(^{}\), while one would only have access to \(_{}^{}\) and \(_{}^{}\). We address this issue next, through the perspective of _statistical functionals_.

## 5 Statistical functional equivalence

Following the intractability of learning a distribution equivalent model in practice, we now study model equivalence through the lens of _statistical functionals_, a framework introduced by Rowlandet al. (2019) to describe a variety of distributional reinforcement learning algorithms. We begin with a review of statistical functionals (Section 5.1), and then introduce statistical functional equivalence, demonstrate its equivalence to projected distribution equivalence, and study which risk measures it can plan optimally for (Section 5.2).

### Background on statistical functionals

**Definition 5.1**.: A statistical functional is a function \(:_{}()\), where \(_{}()()\) is its domain. A sketch is a collection of statistical functionals, written as a mapping \(:_{}()^{m}\), where \(=(_{1},,_{m})\), and \(_{}()=_{i=1}^{m}_{_{i}}()\).

**Example 5.2**.: Suppose \(i>0\), and let \(_{i}()\) be the set of probability measures with finite \(i\)th moment. Moreover, let \(_{i}()\) be the \(i\)th moment of a measure \(_{i}()\). Then for \(m>0\), the \(m\) moment sketch \(_{}^{m}:_{m}()^{m}\) is defined by \(_{}^{m}()=(_{1}(),,_{m}())\).

For a given sketch \(\), we define its image as \(I_{}=\{():_{}()\} ^{m}\). An imputation strategy for a sketch \(\) is a map \(:I_{}_{}()\), and can be thought of as an approximate inverse (a true inverse may not exist as \(\) is generally not injective). We say \(\) is _exact_ for \(\) if for any \((s_{1},,s_{m}) I_{}\) we have \((s_{1},,s_{m})=((s_{1},,s_{m}))\). In general, an exact imputation strategy always exists, however it may not be efficiently computable (Bellemare et al., 2023).

**Example 5.3**.: Suppose \(\) is a sketch given by \(()=(_{Z}[Z],_{Z}[Z])\), and \(\) is given by \((,^{2})=(,^{2})\) (that is, the normal distribution with mean \(\) and variance \(^{2}\)). One may verify that \(\) is exact, since for any \((,^{2})^{2}=I_{}\), we have \(((,^{2}))=(,^{2})\).

We now extend the notion of statistical functionals to return-distribution functions. For \(_{}()^{}\) we write \(()=(((x)):x)\). We say that a set \(()\) is closed under \(^{}\) if whenever \(^{}\), we have \(^{}^{}\). A sketch \(\) is Bellman-closed (Rowland et al., 2019; Bellemare et al., 2023) if whenever its domain is closed under \(^{}\), there exists an operator \(^{}_{}:I^{}_{} I^{}_{}\) such that for any \(_{}()^{}\),

\[(^{})=^{}_{}().\]

We refer to \(^{}_{}\) as the Bellman operator for \(\). Similarly to Section 2.1, we denote \(^{}_{,}\) as the Bellman operator for \(\) in an approximate model \(\).

We will write \(s^{}_{}=(^{})\) as a shorthand, and refer to it as the _return statistic_ for a policy \(\). If \(^{}_{}\) exists, then \(s^{}_{}\) is its fixed point: \(s^{}_{}=^{}_{}s^{}_{}\). For an approximate model \(\), we write \(s^{}_{,}=(^{}_{})\). We further have \(s^{}_{,}=^{}_{,}\)\(s^{}_{,}\), that is, it is a fixed point of the Bellman operator \(^{}_{,}\).

The task of policy evaluation for a statistical functional \(\) is that of computing the value \(s^{}_{}\). Statistical functional dynamic programming (Bellemare et al., 2023) aims to do this by computing the iterates \(s_{k+1}=(^{}(s_{k}))\), with \(s_{0} I^{}_{}\) initialized arbitrarily. If \(\) is exact and \(\) is Bellman-closed, then the updates satisfy \(s_{k}=(_{k})\), where \(_{0}=(s_{0})\) and \(_{k+1}=^{}_{k}\). If \(\) is a continuous sketch4, then the iterates \((s_{k})_{k 0}\) converge to \(s^{}_{}\).

### Statistical functional equivalence

We now introduce a notion of model equivalence through the lens of statistical functionals. Intuitively, this allows us to interpolate between value equivalence and distribution equivalence, as we can choose exactly which aspects of the return distributions we would like to capture.

**Definition 5.4**.: Let \(\) be a sketch, and \(\) be an imputation strategy for \(\). Let \( I^{}_{}\) and \(\). We define the class of \(\)_equivalent models_ with respect to \(\) and \(\) as

\[_{}(,): (^{}(s))=(^{}_{ {m}}(s)),, s}.\]

In the case that \(\) is Bellman-closed and \(\) is exact, this set can be described in a form similar to that of value equivalence and distribution equivalence.

**Proposition 5.5**.: _If \(\) is Bellman-closed and \(\) is exact, we have that_

\[_{}(,)=\{:_{ }^{}s=_{,}^{}s,\;, s \}.\]

We can extend the above to \(k\) applications of the projected Bellman operator, and define the set of order-\(k\)\(\) equivalent models as

\[_{}^{k}(,) :(^{})^{k}s=(_{}^{})^{k}s,, s },\]

where \(^{}:I_{}^{} I_{}^{}\) is shorthand for \(s(^{}(s))\). As in Proposition 5.5, if \(\) is Bellman-closed and \(\) is exact, it holds that

\[_{}^{k}(,)=:( _{}^{})^{k}s=(_{,}^{})^{k}s,\; , s}.\]

Following Section 4, we can consider the set of models which agree on _return statistics_, and have no dependence on the set \(\). However, one difference in the case of statistical functionals is that it is not true in general that this is equal to the limit of \(_{}^{k}(,)\). Intuitively, this is for the same reason that the iterates \((s_{k})_{k 0}\) of statistical functional dynamic programming do not always converge to \(s_{}^{}\) (Section 5.1). We first introduce the definition of proper statistical functional equivalence, and then demonstrate when it is the limiting set in Proposition 5.7.

**Definition 5.6**.: Let \(\) be a set of policies, and \(\) be a sketch. We define the class of proper statistical functional equivalent models with respect to \(\) and \(\) as

\[_{}^{}()\{:s_{ ,}^{}=s_{}^{},\;\}.\]

**Proposition 5.7**.: _If \(\) is both continuous and Bellman-closed and \(\) is exact, then5_

\[_{k}_{}^{k}(,)=_{}^{ }(),\; I_{}^{}.\]

_Remark 5.8_.: Value equivalence (Grimm et al., 2020, 2021) can be seen as a special case of statistical functional equivalence, in the sense that if we choose \(=\), then we have \(_{}^{k}()=^{k}()\), for any \(\) and \(k[1,]\).

### Connection to projected distribution equivalence

In Section 4, we remarked that distribution equivalence was difficult to achieve in practice, due to the fact that the space \(()^{}\) was infinite dimensional, and we generally rely on a parametric family \(\). We now demonstrate that the statistical functional perspective provides us a way to address this.

Let \(\) be a sketch and \(\) an imputation strategy. These induce the implied representation (Bellemare et al., 2023) given by \(_{}=\{(s):s I_{}\}\), and the projection operator \(_{_{}}:_{}()_{}\) given by \(_{_{}}=\). We now show that through this construction, we can relate statistical functional model learning to projected distributional model learning with the projection \(_{_{}}\).

**Proposition 5.9**.: _Suppose \(\) is injective, \(\), \( I_{}^{}\), and let \(_{}=\{(s):s\}_{ }()^{}\). Then_

\[_{}(,)=:_{ _{}}^{}=_{_{}} _{}^{},,_{} },\]

_and_

\[_{}^{}()=:_{ {F}_{}}^{}=_{_{}}_{}^{},\; }.\]

### Risk-sensitive learning

We now study which risk measures we can plan optimally for using a model in \(_{}^{}()\). Intuitively, we will not be able to plan optimally for all risk measures (as was the case in Theorem 4.3), since this set only requires models to match the aspects of the return distribution captured by \(\). Indeed, we now show that the choice of \(\) exactly determines which risk measures can be planned for.

**Proposition 5.10**.: _Let \(\) be a risk measure and let \(=(_{1},,_{m})\) be a sketch, and suppose that \(\) is in the span of \(\), in the sense that there exists \(_{0},_{m}\) such that for all \(_{}()_{}()\), \(()=_{i=1}^{m}_{i}_{i}()+_{0}\). Then any optimal policy with respect to \(\) in \(_{}^{}()\) is optimal with respect to \(\) in \(m^{*}\)._Learning statistical functional equivalent models

We now analyze how we may learn models in these classes in practice. As we have introduced a number of concepts and spaces of models, we only discuss here the spaces of models that are used in the empirical evaluation which follow, and we discuss the remainder of the spaces in Appendix B.

We focus on the case of learning a proper \(\)-equivalent model. Such a model must satisfy \(s^{}_{}=(^{}_{})^{k}s^{}_{}\) for any policy \(\) (Proposition B.1), so that we can construct a loss by measuring the amount that this equality is violated by. However, the size of \(\) is exponential in \(||\), so we can approximate this by only measuring the amount of violation over a subset of policies \(\). We can now formalize this concept as a loss.

**Definition 6.1**.: Let \(\) be a sketch, and \(\) an imputation strategy. We define the loss for learning a proper-\(\) equivalent model as

\[^{k}_{,,}()_{}\| s^{}_{}-(^{}_{})^{k}s^{}_{} \|_{2}^{2}.\]

If \(\) is Bellman-closed this can be written without the need for \(\), by replacing \(^{}_{}\) with \(^{}_{,}\).

This loss is amenable to tabular environments, as it requires knowledge of \(s^{}_{}\), which can be learnt approximately using statistical functional dynamic programming. Despite this, the above approach can be further adapted to the deep RL setting, which we now discuss, and describe how our approach can be combined with existing model-free risk-sensitive algorithms.

We will assume the existence of a model-free risk-sensitive algorithm which satisfies the following properties: (i) it learns a policy \(\) using a replay buffer \(\), and (ii) it learns an approximate statistical functional function \(s^{}_{,}\) (for example, any algorithm based upon C51 (Bellemare et al., 2017) or QR-DQN (Dabney et al., 2018) satisfies these assumptions), where we write \(\) to refer to the set of parameters it depends on, and to emphasize its difference with the true return statistic \(s^{}_{}\). We will introduce a loss which learns an approximate model \(\), which can then be combined with the replay buffer \(\) to use both experienced transitions and modelled transitions to learn \(\), as was done by e.g. Sutton (1991) or Janner et al. (2019). Following this, for a learnt model \(\) we introduce the approximate loss

\[_{,,}()=}_{ (x,a,r,x^{})\\ ^{}|x,a}[(s^{}_{, }(x^{})-s^{}_{,}(^{}))^{2}].\]

## 7 Empirical evaluation

We now empirically study our framework, and examine the phenomena discussed in the previous sections. We focus on two sets of experiments: the first is in tabular settings where we use dynamic programming methods to perform an analysis without the noise of gradient-based learning. The second builds upon Lim and Malik (2022), where we augment their model-free algorithm with our framework, and evaluate it on an option trading environment. We discuss training and environments details in Appendix E, and provide additional results in Appendix A.3. We provide the code used to run our experiments at github.com/tylerkastner/distribution-equivalence.

### Experimental details

#### Tabular experiments

For each environment, we learn an MLE model, a proper value equivalent model using the method introduced in Grimm et al. (2021), and a \(^{2}_{}\) equivalent model using \(^{k}_{^{2}_{},,}\), where \(^{m}_{}\) is the first \(m\) moment functional (cf. Example 5.2), and \(\) is a set of 1000 randomly sampled policies. For each model, we performed CVaR value iteration (Bellemare et al., 2023), and further performed CVaR value iteration in the true model, to produce three policies. We repeat the learning of the models across 20 independent seeds, and report the performance of the policies in Figure 2 (Left).

#### Option trading

Lim and Malik (2022) introduced a modification of QR-DQN which attempts to learn CVaR optimal policies, that they evaluate on an option trading environment (Chow and Ghavamzadeh, 2014; Tamaret al., 2017). We augment their method using the method described in Section 6, and we learn optimal policies for 10 CVaR levels between 0 and 1. We compare our adapted method to their original method as well as their original method adapted with a PVE model (Grimm et al., 2021), and discuss implementation details in Appendix E. In particular, we evaluate the models in a low-sample regime, so the sample efficiency gains of using a model are apparent.

### Discussion

In Figure 2 (Left), we can see that across all three tabular environments, planning in a proper statistical functional equivalent model achieves stronger results over planning in a proper value equivalent model. This provides an empirical demonstration of Proposition 3.2 and Proposition 5.10: proper value equivalence is limited in its ability to plan risk-sensitively, while risk-sensitive planning in a statistical functional equivalent model approximates risk-sensitive planning in the true environment.

In Figure 2 (Right), we can see that Lim & Malik (2022)'s algorithm augmented with a statistical functional equivalent model achieved significantly improved performance for all CVaR levels below \( 0.8\). The fact that our augmentation improves upon the original method reflects the improved sample efficiency which comes from using an approximate model for planning. This difference

Figure 3: CVaR of returns for policies learnt in the option trading environment using a \(\)-equivalent model and a MLE-based model as the number of distracting dimensions increases (**left**: 2 distracting dimensions, **right**: 6 distracting dimensions).

Figure 2: **Left**: CVaR(0.5) of returns obtained across the three tabular environments. We computed the values across 1000 trajectories from each of the 20 learnt models. Error bars indicate 95% confidence intervals. The orange bar for Frozen Lake appears missing because the value obtained is 0. **Right**: CVaR of returns for the policies learnt in the option trading environment for various CVaR levels after 10,000 environment interactions. Shaded regions indicate 95% confidence intervals across 10 independent seeds.

is more apparent for lower values of \(\), which demonstrates the phenomenon that learning more risk-sensitive policies are less sample efficient (Greenberg et al., 2022). On the other hand, the method augmented with the PVE model could have the same sample efficiency gains from using an approximate model (as it is trained on the same number of environment interactions), so the fact that it is not performant for lower values of CVaR is a demonstration of Proposition 3.2: the more risk-sensitive the risk measure being planned for, the more the performance is affected.

Compared to MLE-based methods, our approach focuses on learning the aspects of the environment which are most relevant for risk-sensitive planning. This phenomenon is reflected in Figure 3, where the performance gap between the MLE model and the \(\)-equivalent model grows with the amount of uninformative complexity in the environment.

## 8 Conclusion

In this work, we studied the intersection of model-based reinforcement learning and risk-sensitive reinforcement learning. We demonstrated that value-equivalent approaches to model learning produce policies which can only plan optimally for the risk-neutral setting, and in risk-sensitive settings their performance degrades with the level of risk being planned for. Similarly, we argued that MLE-based methods are insufficient for efficient risk-sensitive learning as they focus on all aspects of the environment equally. We then introduced distributional model equivalence, and demonstrated that distribution equivalent models can be used to plan for any risk measure, however they are intractable to learn in practice. To account for this, we introduced statistical functional equivalence; an equivalence which is parameterized by the choice of a statistical functional. We proved that the choice of statistical functional exactly determines which risk measures can be planned for optimally, and provided a loss with which these models can be learnt. We further described how our method can be combined with any existing model-free risk-sensitive algorithm, and augmented a recent model-free distributional risk-sensitive algorithm with our model. We supported our theory with empirical results, which demonstrated our approach's advantages over value-equivalence and MLE based models over a range of environments..

## 9 Acknowledgements

We would like to thank the members of the Adaptive Agents Lab who provided feedback on a draft of this paper. AMF acknowledges the funding from the Canada CIFAR AI Chairs program, as well as the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) through the Discovery Grant program (2021-03701). MAE was supported by NSERC Grant [2019-06167], CIFAR AI Chairs program, and CIFAR AI Catalyst grant. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. Finally, we thank the anonymous reviewers (both ICML 2023 and NeurIPS 2023) for providing feedback which improved the paper.