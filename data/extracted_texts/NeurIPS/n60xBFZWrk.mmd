# Hyperbolic Embeddings of Supervised Models

Richard Nock

Google Research

richardnock@google.com &Ehsan Amid

Google DeepMind

eamid@google.com &Frank Nielsen

Sony CS Labs, Inc.

frank.nielsen@acm.org &Alexander Soen

RIKEN AIP

Australian National University

alexander.soen@anu.edu.au &Manfred K. Warmuth

Google Research

manfred@google.com

###### Abstract

Models of hyperbolic geometry have been successfully used in ML for two main tasks: embedding _models_ in unsupervised learning (_e.g._ hierarchies) and embedding _data_. To our knowledge, there are no approaches that provide embeddings for supervised models; even when hyperbolic geometry provides convenient properties for expressing popular hypothesis classes, such as decision trees (and ensembles). In this paper, we propose a full-fledged solution to the problem in three independent contributions. The first linking the theory of losses for class probability estimation to hyperbolic embeddings in Poincare disk model. The second resolving an issue for an easily interpretable, unambiguous embedding of (ensembles of) decision trees in this model. The third showing how to smoothly tweak the Poincare hyperbolic distance to improve its encoding and visualization properties near the border of the disk, a crucial region for our application, while keeping hyperbolicity. This last step has substantial independent interest as it is grounded in a generalization of Leibniz-Newton's fundamental Theorem of calculus.

## 1 Introduction

Models of hyperbolic geometry have been successfully used to embed hierarchies, _i.e._ tree-based structures . Through the property of low-distortion hyperbolic embeddings , symbolic (hierarchies) and numeric (quality metrics) properties of unsupervised learning models can be represented in an accurate and interpretable manner . When it comes to supervised learning, the current trend involves embedding _data_ in a hyperbolic space, with models trained on the now hyperbolic data . It is important to note that none of these supervised methods embed _models_. Indeed, the focus of the prior literature is to learn supervised models from hyperbolic data, rather than representing supervised models in hyperbolic geometry. This is in stark contrast to the unsupervised usage described, where the (tree-based) structural properties of unsupervised models are directly exploited for good embeddings. This hints at benefits which can be used in the supervised case, especially for popular supervised models which are (structurally) tree-based.

**Our paper** proposes a full-fledged solution for this problem, in three independent contributions.

**The first** focuses on the numerical part (quality metrics) of the embedding and its link with supervised losses: to provide a natural way of embedding classification and the _confidence_ of prediction, we show a link between training with the log-loss (posterior estimation) or logistic loss (real-valued classification) and hyperbolic distances computation in the Poincare disk.

**The second** focuses on the symbolic part (hierarchies) of the embeddings for a popular kind of supervised of models: decision trees (and ensembles). Unlike for unsupervised learning, we showthat getting an unambiguous embedding of a decision tree requires post-processing the model. Our solution extracts its _monotonic_ sub-tree via a new class of supervised models we introduce, monotonic decision trees. This is also convenient for explainability purposes .

**The third** and most technical one focuses on visualization and the accuracy of the numerical encoding in the Poincare disk model. The more accurate / confident is a supervised prediction, the closer to the border of the disk it is represented. Thus, the best models will have their best predictions squeezed close to the border. In addition to being suboptimal for visualization, this region is also documented for being numerically error-prone for hyperbolic distance computation . Two general fixes currently exist: encoding values with (exponentially) more bits or utilizing a trick from Riemannian geometry . As neither is satisfactory for our usage, we propose a third, principled route: like other distances, Poincare distance is integral. We generalize Leibniz-Newton's fundamental Theorem of calculus using a generalization of classical arithmetic . This generalized "tempered" integral provides a parameter to smoothly alter the properties of the "classical" integral. When defining the Poincare distance, the tempering controls the hyperbolic constant of the embedding whilst also improving the visualization and numeric accuracy of the embedded models. The generalization of integrals to distances has independent interest as many application in ML rely on integral based distances and distortions, _i.e.,_ Bregman divergences, \(f\)-divergences, integral probability metrics, etc.

Experiments are provided on readily available domains, and all proofs, additional results and additional experiments are given in an Appendix.

## 2 Related work

Models of hyperbolic geometry have been mainly useful to embed hierarchies, _i.e._ tree-based structures [14; 20; 38; 36], with a sustained emphasis on coding size and numerical accuracy [19; 29; 37]. In unsupervised learning and clustering, some applications have sought a simple representation of data on the form of a tree or via hyperbolic projections [7; 6; 17; 34]. Approaches dealing with supervised learning assumes the _data_ lies in a hyperbolic space: the output visualized is primarily an embedding of the data itself, with additional details linked to classification of secondary importance, either support vector machines , neural nets, logistic regression , or (ensembles of) decision trees [8; 11]. We insist on the fact that the aforementioned methods do not represent the _models_ in the hyperbolic space, even when those models tree-shaped. The question of embedding classifiers is potentially important to improve the state of the art visualization: in the case of decision trees, popular packages stick to a topological layer (the tree graph) to which various additional information about classification are superimposed but without principled links to the "embedding"1.

## 3 Basic definitions

Training a supervised model starts with a set (sample) of examples, each of which is a pair \((,y)\), where \(\) (a _domain_) and \(y\{-1,1\}\) (labels or classes). A decision tree (DT) consists of a binary rooted tree \(H\), where at each internal nodes, the two outgoing arcs define a Boolean test over observation variables (see Figure 1, center, for an example); \((H)\) is the set of nodes of \(H\), \((H)(H)\) is the set of leaf nodes of \(H\).

The log-loss is best introduced in the context of the theory of _losses for class probability estimation_ (CPE) . We follow the notations of : A CPE loss function, \(:\), is

\[(y,p)   y=1_{1}(p)+ y=-1 _{-1}(p),\] (1)

where \(.\) are Iverson brackets . Functions \(_{1},_{-1}\) are called _partial_ losses. A CPE loss is _symmetric_ when \(_{1}(p)=_{-1}(1-p), p\). The _log-loss_ is a symmetric CPE loss with \(_{1}^{}}(p)-(p)\). The goal of learning using a CPE loss is to optimize the Bayes risk, \((p)_{}_{ p}(,)\). In the case of the log-loss, it is the criterion used to learn DTs in C4.5 :

\[^{}}(p) = -p p-(1-p)(1-p).\] (2)

Models like DTs predict an empirical posterior \([=1|]\) at the leaves: for an observation \(\) reaching leaf \(\), the posterior prediction is the local relative proportion of positive examples in leaf \(\), as estimated by the training sample (noted \(p_{}^{+}\)). There exists a duality between CPE classification and the real-valued classification setting familiar to _e.g._ deep learning: optimizing Bayes risk for the posterior is equivalent to minimizing its _convex surrogate_ using the _canonical link_ of the loss to compute real classification (24, Theorem 1). The canonical link of the log-loss, \(_{}:\) is the famed inverse sigmoid, which has a very convenient form for our purpose,

\[_{}(p)=()\ \ \ \ |_{}(p)|=(),\ \ \ r|2p-1|.\] (3)

Notably, the absolute value \(|_{}(p)|\) is a _confidence_ with the sign giving the predicted class. In the case of the log-loss, the convex surrogate is hugely popular in ML: it is the logistic loss.

We now introduce concepts from hyperbolic geometry, particularly those from the Poincare disk model. Our definitions are simplified, a detailed account can be found in . The distance function \(d\) of a metric space is \(\)-hyperbolic for some \( 0\) iff for any three geodesic curves \(^{1},^{2},^{3}\) linking three points, there exists \(\) such that \(_{i}d(,^{i})\), where \(d(,)_{^{}}d(,^{})\). A small hyperbolic constant guarantees thin triangles and embeddings of trees with good properties. The Poincare disk model, \(\) (negative curvature, \(-1\)) is a popular model of hyperbolic geometry with the hyperbolic distance between the origin and some \(\) with Euclidean norm \(r|\|\) being:

\[d_{}(,) = ().\] (4)

## 4 Posterior embedding in Poincare disk and clean embeddings for DTs

**Node embedding** We exploit the similarity between log-loss confidences \(|_{}(p)|\) and hyperbolic distances \(d_{}(,)\) when embedding classification models, similarity which is obvious from (3) and (4). In (3), we see that if \(r=0\), the prediction is as bad as a fair coin's. As \(r\) edges closer to \(1\), prediction approaches maximum confidence. The connection with the Poincare distance (4) is immediate: a natural embedding of a posterior prediction \(p\) is then a point \(\) in the Poincare disk \(\) at radius \(r|\|\) from the origin (3). The origin of Poincare disk represents the worst possible confidence. As each leaf \(\) of a DT \(H\) corresponds to a posterior \(p_{}^{+}\), the leaves can be embedded as described. In addition, all nodes \((H)\) in the tree also have corresponding posteriors, and thus can also be embedded identically to leaf nodes. It is also a good idea to embed them because, as classical DT induction algorithms first proceed by top-down induction, any node \(\) in a DT was a leaf at some point during training, and has a corresponding posterior. This apparently clean picture for node embedding however becomes messy as soon as we step to embedding full trees.

Figure 1: _Left pane_: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey = random posterior, tests at arcs not shown, \(n_{1}\) is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell \(n_{2}\) from \(n_{5}\) and the tree depicted in \(\) is not a faithful representation of the DT nor of any of its subtrees. _Right pane_: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using getMDT. In each node, the real-valued prediction (\(_{}(p)\)) is indicated, also in color. Observe that, indeed, \(H\) does not grant path-monotonic classification but \(H^{}\) does (Definition 4.1). In \(H^{}\), some nodes have outdegree 1; also, internal node \(\#6\) in the DT, whose prediction is worse than its parent, disappears in \(H^{}\). One arc in \(H^{}\) is represented with double width as its Boolean test aggregates both tests it takes to go from \(\#3\) to \(\#10\) in \(H\). Observations that would be classified by leaf \(\#11\) (resp. \(\#5\), resp. \(\#9\)) in \(H\) are now classified by internal node \(\#3\) (resp. \(\#2\), resp. \(\#4\)) in \(H^{}\), but predicted labels are the same for \(H\) and \(H^{}\) and so the accuracy of \(H\) and \(H^{}\) are the same.

**No clean embedding for a full DT** A simple counterexample indeed demonstrates that a clean embedding of a decision tree is substantially trickier, see Figure 1 (left). In this pathological example, some distinct nodes (resp. arcs) of the DT \(H\) are embedded in the same node (resp. edge) in \(\): the depiction has no subgraph relationship to \(H\) and some embedded nodes cannot be distinguished between each other without additional information.

**Getting a clean embedding: Monotonic Decision Trees** To introduce our fix, we define three broad objectives for the embedding in \(\) of DTs

Embedding objective

**(A)** Embed a tree-based model from \(H\) in \(\), which defines an injective mapping of the nodes of \(H\) and induces a subtree of \(H\) with each edge in \(\) corresponding to a path in \(H\);

**(B)**_Locally_, each node \(\) of \(H\) gets embedded to some \(_{}\) such that \(r_{}\) is close to \(\|_{}\|\) (3);

**(C)**_Globally_, the whole embedding remains convenient and readable to compare, in terms of confidence in classification, different subtrees in the same tree; or even between different trees.

As we shall see later in this Section, this agenda also carries to embedding ensembles of DTs, thus including their leveraging coefficients, exploiting properties of boosting algorithms. For now, our solution for embedding a single DT that can satisfy (A-C) is simple in principle:

_Embed the monotonic classification part of a DT_,

meaning for each path from the root to a leaf in \(H\), we only embed the subsequence of nodes whose absolute confidences are strictly increasing. To do so, we replace the DT by a "close" path-monotonic approximation model using a new class of models we call _Monotonic Decision Trees_ (MDT).

**Definition 4.1**.: _A Monotonic Decision Tree (MDT) is a rooted tree with a Boolean test labeling each arc and a real-valued prediction at each node. Any sequence of nodes in a path starting from the root is strictly monotonically increasing in absolute confidence. At any internal node, no two Boolean tests at its outgoing arcs can be simultaneously satisfied. The classification of an observation is obtained by the bottom-most node's prediction reachable from the root._

``` Input: Node \((H)\) (\(H\) = DT), Boolean test \(\), Node \(^{}(H^{})\) (\(H^{}\) = MDT being build from \(H\)), forbidden posteriors \(\);
1:if\((H)\)then
2:if\(^{}\)tangDTleaf(\(\)); // tags \(^{}(H^{})\) with info from leaf \((H)\)
3:else
4:\(^{}\)\(H^{}\).newNode(\(\)); // \(^{}\) will be a new leaf in \(H^{}\)
5:\(H^{}\).newMac(\(^{},,^{}\)); // adds arc \(^{}_{}^{}\) in \(H^{}\)
6:endif
7:else
8:if\(p_{}^{+}\)then
9:\(^{}\)\(H^{}\).newNode(\(\)); // \(^{}\) = new internal node in \(H^{}\)
11:\(H^{}\).newMac(\(^{},,^{}\)); // adds arc \(^{}_{}^{}\) in \(H^{}\)
12:\(_{}\{_{}[p_{}^{+},1-p_{}^{+}], [p_{}^{+},1-p_{}^{+}]\}\); // \(_{}\)
13:\(_{}_{}\); // \(_{}\)
14:\(_{}\)true;
15:else
16:\(_{}\); // \(\) yields no change in \(H^{}\)
17:endif
18:\((\).leftchild, \(_{}\).test(leftchild), \(^{}_{},_{}\));
19:\((\).rightchild, \(_{}\).test(rightchild), \(^{}_{},_{}\));
20:endif ```

**Algorithm 1**getMDT(\(,,^{},\))

We introduce an algorithm, getMDT, which takes as input a DT \(H\) and outputs an MDT \(H^{}\), which approximates \(H\) via the following key property:

**(M)** For any observation \(\), the prediction \(H^{}()\) is equal to the prediction in the path followed by \(\) in \(H\) of its deepest node in the strictly monotonic subsequence starting from the root of \(H\).

Figure 1 (right) presents an example of MDT \(H^{}\) that would be built for some DT \(H\) (left) and satisfying **(M)** (unless observations have missing values, \(H^{}\) is unique). Figure 1 adopts some additional conventions to ease parsing of \(H^{}\), **(D1)** and **(D2)**:

**(D1)** Some internal nodes of \(H^{}\) are also tagged with labels corresponding to the leaves of \(H\). If anode in \(H^{}\) is tagged with a label of one leaf \(\) of \(H\), it indicates that examples reaching \(\) in the original \(H\) are being classified by \(H^{}\)'s tagged node;

**(D2)** Arcs in \(H^{}\) have a width proportional to the number of boolean tests it takes to reach its tail from its head in \(H\). A large width thus indicates a long path in \(H\) to improve classification confidence.

To produce the MDT \(H^{}\) from DT \(H\) with Algorithm 1, after having initialized it to a root = single leaf, we just run

\[((H),,(H^{}),[ ^{+}_{},^{+}_{}])\]

(we let \(^{+}_{}\{p^{+}_{},1-p^{+}_{ }\}\), \(^{+}_{}\{p^{+}_{},1-p^{+}_{ }\}\)). Upon finishing, the tree rooted at \((H^{})\) is the MDT sought.

We complete the description of getMDT: When a leaf of \(H\) does not have sufficient confidence and ends up being mapped to an internal node of the MDT \(H^{}\), tagDTLeaf is the procedure that tags this internal node with information from the leaf (see **(D1)** above). We consider a tag being just the name of the leaf (Figure 1, leaves \(\#5,9,11\)), but other conventions can be adopted. The other two methods we use grow MDT \(H^{}\) by creating a new node via newNode and adding a new arc between existing nodes via newArc. We easily check the following.

**Theorem 1**.: \(H^{}\) _built by getMDT satisfies **(M)** with respect to DT \(H\)._

Property **(M)** is crucial to keep the predictions of the DT and the MDT close to each other: to each node of the MDT indeed corresponds a node in the DT with the same posterior prediction. Because the set of nodes of \(H^{}\) corresponds to a subset of nodes in \(H\), predictions can locally change for some observations. This phenomenon is limited by two factors: (i) the set of leaves of the MDT corresponds to a set of leaves in the DT (thus, predictions do not change for the related observations), (ii) more often than not in our experiments, it is only the confidence that changes, not the sign of the prediction, which means that accuracies of the DT and its corresponding MDT are close to each other (Section 6). It is also worth remarking that the MDT \(H^{}\) always has the same depth as the DT \(H\). Finally, any pruning of \(H\) is a subtree of \(H\) and its corresponding MDT is a subtree of the MDT of \(H\). The aforementioned troubles to embed the DT in the Poincare disk considering **(A-C)** do not exist anymore for the MDT because the best embeddings necessarily have all arcs going outwards in the Poincare disk. The algorithm we use to embed the MDT in Poincare disk is a simple alteration of Sarkar's embedding  which, because of the lack of space, we defer to the Appendix (Section C.VIII).

The quality of the embedding is obtained using an embedding error that takes into account _all_ nodes:

\[(H^{})  100_{(H^{})}[|-d_{}(,_{})|}{|_{}| }](\%),\] (5)

where \(_{}\) refers to the relevant \(_{}(p^{+}_{})\) in (3) (and \(_{}\) is its embedding in \(\)). Two example final representations are in Figure 2. Notice the small errors \(\) in both cases.

**Remark 1**.: _Using a particular boosting algorithm to boost the logistic loss, we can as well represent the leveraging coefficients of a DT/MDT in a boosted combination, following a convention sketched in Figure 2 and explained at length, along with the boosting algorithm and its properties, in the Appendix (Section C.IX)._

## 5 Smoothly altering integrals: T-calculus and the t-self of Poincare disk

It is apparent from Figure 2 (right, lowest red pentagon), that when utilizing standard hyperbolic distances \(d_{}(,)\) (4), the best MDT nodes are embedded close to the border. In addition to obviously not being great for visualization, these nodes are at risk of having numeric error "push" the nodes to the border \(\), thereby giving a false depiction of infinite confidence. In this section, we provide alternative distances to prevent this phenomena. First, we quantify the numerical risk, which has been defined by the critical region where high numeric error can occur [19; 29].

**Definition 5.1**.: _A point \(\) is said \(k\)-close to boundary \(\) if \(\|\|=1-10^{-k}\). It is said encodable iff \(\|\|<1\) in machine encoding (it is not "pushed to the boundary")._

Machine encoding constrains the maximum possible \(k\): in the double-precision floating-point representation (Float64), \(k 16\). In the case of Poincare disk, the maximal distance \(d_{*}\) from the 

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

All three conditions point to a non-linear embedding \(_{t}:^{(t)}\). The \(t\)-self offers a simple convenient solution with a trivial design for \(_{t}\): We compute \(^{(t)}\) by a simple scaling of the Euclidean norm of \(\) in \(\) to ensure that (13) holds. Figure 3 (right) displays the corresponding relationship between \(r\|\|\) and \(r^{(t)}\|^{(t)}\|\) when \(t\), clearly achieving (ii) in addition to (iii). For (i), even for \(t<1\) very close to \(1\) and \(\) close to \(\) (\(r\) close to \(1\)), the mapping can send \(^{(t)}\) substantially "back in" the t-self \(^{(t)}\): for \(t=0.7\) and \(r=1-10^{-4}\) - _i.e._\(k=4\) in Definition 5.1 - we get \(r^{(t)} 0.96\), authorizing encoding with a less "risky" \(k=2\). Two additional benefits from (13), visible from Figure 3 (right) is that the distortion is low near the center and then merely affine in a wide region until near the heavy non-linear changes, typically when \(r>0.8\).

**Remark 3**.: _The Appendix details properties of the t-self of the Lorentz model of hyperbolic geometry, another popular model (Section C.V)._

 \(_{t}:^{(t)}\). & The \(t\)-self offers a simple convenient solution with a trivial design for \(_{t}\): We compute \(^{(t)}\) by a simple scaling of the Euclidean norm of \(\) in \(\) to ensure that (13) holds. Figure 3 (right) displays the corresponding relationship between \(r\|\|\) and \(r^{(t)}\|^{(t)}\|\) when \(t\), clearly achieving (ii) in addition to (iii). For (i), even for \(t<1\) very close to \(1\) and \(\) close to \(\) (\(r\) close to \(1\)), the mapping can send \(^{(t)}\) substantially "back in" the t-self \(^{(t)}\): for \(t=0.7\) and \(r=1-10^{-4}\) - _i.e._\(k=4\) in Definition 5.1 - we get \(r^{(t)} 0.96\), authorizing encoding with a less “risky” \(k=2\). Two additional benefits from (13), visible from Figure 3 (right) is that the distortion is low near the center and then merely affine in a wide region until near the heavy non-linear changes, typically when \(r>0.8\). \\ 

Table 1: _Left pane_: embedding in Poincaré disk of the MDTs corresponding to the DTs learned at the \(1^{st}\) (top row) and \(10^{th}\) (bottom row) boosting iteration, on four UCI domains. Stark differences emerge between these domains from the plots alone. _Right pane_: comparison between Poincaré disk embedding and its \(t=0\) t-self for an MDT learned on UCI online_shoppers_intention (top, boosting coefficients information not shown) and UCI hardware (bottom). The \(p\{0.001,0.999\}\) isoline (rectangle) is barely distinguishable from \(\) but is clearly distinct from \(^{(0)}\). Note that in \(^{(0)}\), the center looks similar to a scaling (zoom) of \(\); while near the border, the high nonlinearity of \(^{(0)}\) allows us to spot nodes that have high confidence / training accuracy (in orange) but can hardly be distinguished from the bulk of “just good” nodes in \(\). Note also the set of red nodes in the Poincaré disk for \(j=70\) (rectangle) that mistakenly look aligned, but not in the t-self.

Figure 3: _Left_: plot of \(_{t}(z)\) (9) for different values of \(t\) (color map on the right bar), showing where it is convex / concave. The \(t=1\) case (\(_{1}(z)=z\)) is emphasized in red. _Right_: suppose \(r\|\|\) is the Euclidean norm of a point \(\) in Poincaré disk \(\). Fix \(t\) (color map on the right bar). The plot gives the (Euclidean) norm \(r^{(t)}\) of a point \(^{(t)}\) in the t-self such that \(d_{^{(t)}}(^{(t)},)=d_{}( ,)\) (13). Remark that even for \(r\) very close to \(1\) we can have \(r^{(t)}\) substantially smaller (_e.g._\(r^{(t)}<0.8\)).

Experiments

We summarize a number of experiments (Table 1), provided otherwise _in extenso_ in the Appendix. In the top-down induction scheme for DT, the leaf chosen to be split is the heaviest non pure leaf, _i.e._ the one among the leaves containing both classes with the largest proportion of training examples. Induction is stopped when the tree reaches a fixed size, or when all leaves are pure, or when no further split allows decreasing the expected log-loss. We do not prune DTs. Our domains are public (_Cf_ Appendix).

**Poincare embeddings of DTs / MDTs** See Figure 2 (left) for a summary of the visualization. We remind that the center of the disk is "poor" classification (confidence 0), or, in the context of boosting, random classification. A striking observation is the sheer variety of patterns that are plainly obvious from our visualization but would otherwise be difficult to spot using classical tools. Some are common to all domains: as boosting iterations increase, low-depth tree nodes tend to converge to the center of the disk, indicating increased hardness in classification. This is the impact of a well known effect of boosting, whose weight modifications make the problem "harder". Among domain-dependent patterns, highly imbalanced domains get a root predictably initially embedded "far" from the origin (online_shoppers_intention, analcatdata_supreme) while more balanced domains get their roots embedded near the origin (abalone). Across the experiment, all trees have at most 200 nodes: while this clearly provides very good models after a large number of iterations on some domains, it is not enough to get good models after just a few iterations on others (analcatdata_supreme). Interpreting the hyperbolic embeddings can also be telling: consider Figure 2 (right), which a MDT of the (\(j=3\)) boosted DT learned on online_shoppers_intention. Notice the low embedding error (\(5.46\%\)). We see in (A) (magenta) that classes are more balanced at the root compared to previous trees (All DTs in Appendix, Table IV) because the root is closer to the center of the disk: thus, hard examples belong to both classes (no class becomes much harder to "learn" than the other). Two clearly distinct subtrees are associated to non-purchase patterns (red, A). The bottom-most subtree achieves very good confidence with several leaves close to the border: these are non-purchase patterns clearly distinct from purchase patterns (green nodes); the whole subtree dangles from the root via a test on feature PageValues (grey, B) achieving a substantial boost in confidence; many nodes are way past posterior isoline \(p\{0.1,0.9\}\). Red subtree in (A) is a lot closer to purchase patterns (C). One distinct pattern of purchase emerges, built from tests that always _strictly_ increase its prediction's confidence (orange). The full rule achieves very high accuracy (leaf posterior nears \(0.99\)).

**Interest of the t-self for visualization** As demonstrated by Table 1 (right) and Appendix, the t-self is particularly useful to tell the best parts of a tree. Because it also manages to "push back" the bulk of nodes from the border \(^{(t)}\), it displays the t-self might also be useful as a _standard_ encoding (use (13) to access Poincare disk quantities and embedding).

**MDT vs DT for classification** We traded the hyperbolic visualization of a DT - bound to be poor - for the visualization of its "monotonic part", an MDT. We have seen that the MDT visualization provides lots of clues about the DT as well. In terms of classification, the fact that the set of leaves of the MDT is a subset of the set of leaves of its DT hints on similarities in classification as well, but so far we have not explored a key question whose scope goes beyond this paper's: how does the classification of a MDT compares to its DT? This question is important because in addition to being more amenable to visualization than DTs, it might also be the case that classification with MDTs might just be a good alternative to doing it with DTs. The scope of this question goes beyond this paper because of the tremendous popularity of DTs. The left pane of Table 2 provides a partial answer on a subset of our domains (the full set of results is in the Appendix, Section D.II; p-values are that of a Student paired t-test with H0 being the identity of the average errors). Clearly, the classification accuracies are similar for a majority of the domains considered, but interestingly, we observe the two polarities when they are not: on domains ionosphere and online_shoppers_intention, the DT is better than its MDT, but on domain analcatdata_supreme it is the opposite. This is particularly interesting because the MDT is no larger and can be substantially smaller than its corresponding DT and thus could represent an efficient pruning of its DT.

**MDT embedding: visualization error** The right pane of Table 2 provides an example of embedding errors \(\) (5) for the first five MDTs in a boosted ensemble (we remind that the description of the boosting algorithm and the embedding of the leveraging coefficients is provided in Appendix, Section C.IX). The full set of results is in Appendix, Section D.II. The results show that errors are small in general, which is good news given our quite heuristic modification to Sarkar's embedding. There is more: the embedding error tends to decrease - in some cases very significantly - with the MDT's index in the ensemble, see for example winered, german and qsar. The explanation is simple and due to a key boosting feature, the reweighting mechanism underlined above: with weight updates that bring in general the total (new) weight of positive examples closer to that of negative examples, the root of the next MDT gets embedded closer to the center of the disk. As the root moves closer to the center, it is much easier to get a good spread of the tree in the rest of the disk at reduced error, in particular if the MDT is well balanced. For an example, see hardware for \(j=70\) in Figure 1 (\(<1\%\)).

## 7 Conclusion

This paper proposes three separate contributions that altogether provide a solution for hyperbolic embedding of (ensembles of) decision trees (DT), via (i) a link between losses for class probability estimation and hyperbolic distances, (ii) the design of a clean, unambiguous embedding of a DT via its monotonic subtree and (iii) a way to improve visualization and encoding properties of the Poincare disk model using its t-self, a notion we introduce. Each of these contributions are indeed separate: for example, one could reuse (i) to embed models different from DTs or (iii) to perform hyperbolic embeddings of objects being not even related to supervised models. Contribution (ii) is of independent interest with the introduction of a new kind of tree-based models, monotonic decision trees.

We believe that the way we address (iii) opens general applications even outside hyperbolic geometry. Indeed, we generalize classical integration and derivation to a context encompassing the concept of additivity, upon which integration is built, extending standard properties of integration and derivation in a natural way (see the Appendix for many examples). That such properties can be obtained without additional technical "effort" bodes well for perspectives of developments and applications in other subfields of ML, where such tools could be used, not just in our geometric context, to smoothly tweak distortion measures. Many distortion measures used in ML are indeed integrals in nature: Bregman divergences, \(f\)-divergences, integral probability metrics, etc.

One inherent limitation of our work is linked to the use of DTs: they typically fit very well to tabular variables (attribute-value data) but otherwise should be used with caution. Our work is a first step towards more sophisticated visualization for supervised models (Section 2), looking for tailored fit geometric spaces to best blend their symbolic and numerical properties. More can be done and more has to be done: at an age of data collection and ML compute still ramping up, seeking model "pictures" worth a thousand words is a challenge but a necessity for responsible AI and explainability. Finally, our introduction of monotonic decision trees begs for a deeper statistical analysis of such models, given the tremendous popularity of decision trees that they could complete - or replace - on mainstream data science pipelines.