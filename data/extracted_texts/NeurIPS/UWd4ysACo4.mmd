# Expressive Sign Equivariant Networks

for Spectral Geometric Learning

 Derek Lim

MIT CSAIL

dereklim@mit.edu

&Joshua Robinson

Stanford University

&Stefanie Jegelka

TU Munich, MIT CSAIL

&Haggai Maron

TU Munich, MIT CSAIL

Technion, NVIDIA

Work completed whilst at MIT.

###### Abstract

Recent work has shown the utility of developing machine learning models that respect the structure and symmetries of eigenvectors. These works promote sign invariance, since for any eigenvector \(v\) the negation \(-v\) is also an eigenvector. However, we show that sign invariance is theoretically limited for tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. In this work, we demonstrate the benefits of sign _equivariance_ for these tasks. To obtain these benefits, we develop novel sign equivariant neural network architectures. Our models are based on a new analytic characterization of sign equivariant polynomials and thus inherit provable expressiveness properties. Controlled synthetic experiments show that our networks can achieve the theoretically predicted benefits of sign equivariant models.

## 1 Introduction

The need to process eigenvectors is ubiquitous in machine learning and the computational sciences. For instance, there is often a need to process eigenvectors of operators associated with manifolds or graphs (Belkin and Niyogi, 2003; Rustamov et al., 2007), principal components (PCA) of arbitrary datasets (Pearson, 1901), and eigenvectors arising from implicit or explicit matrix factorization methods (Levy and Goldberg, 2014; Qiu et al., 2018). However, eigenvectors are not merely unstructured data--they have rich structure in the form of symmetries (Ovsjanikov et al., 2008).

Specifically, eigenvectors have sign and basis symmetries. An eigenvector \(v\) is sign symmetric in the sense that the sign-flipped vector \(-v\) is also an eigenvector of the same eigenvalue. Basis symmetries occur when there is a repeated eigenvalue, as then there are infinitely many choices of eigenvector basis for the same eigenspace. Prior work has developed neural networks that are invariant to these symmetries, improving empirical performance in several settings (Lim et al., 2023).

The goal of this paper is to demonstrate why sign _equivariance_ can be useful and to characterize fundamental expressive sign equivariant architectures. Our first contribution is to show that sign equivariant models are a natural choice for several applications, whereas sign _invariant_ architectures are provably insufficient for these applications. First, we show that sign and basis invariant networks are theoretically limited in expressive power for learning edge representations (and more generally multi-node representations) in graphs because they learn structural node embeddings that are known to be limited for link prediction and multi-node tasks (Srinivasan and

Figure 1: Illustration of a sign equivariant function \(f\). When column 1 of the input is negated, column 1 of the output is also negated.

Ribeiro, 2019; Zhang et al., 2021). In contrast, we show that sign equivariant models can bypass this limitation by maintaining positional information in node embeddings. Furthermore, we show that sign equivariance combined with PCA can be used to parameterize expressive orthogonally equivariant point cloud models, thus giving an efficient alternative to PCA-based frame averaging (Puny et al., 2022; Atzmon et al., 2022). In contrast, sign invariant models can only parameterize orthogonally _invariant_ models in this framework, which excludes many important application areas.

The second contribution of this work is to develop the first sign equivariant neural network architectures, with provable expressiveness guarantees. We first present a difficulty in developing sign equivariant models: the "Geometric Deep Learning Blueprint" (Bronstein et al., 2021) suggests developing an equivariant neural network by interleaving equivariant _linear_ maps and equivariant elementwise nonlinearities (Cohen and Welling, 2016; Ravanbakhsh et al., 2017; Finzi et al., 2021), but we show that our attempts to apply this approach are insufficient for expressive sign equivariant models. Namely, we show that sign equivariant linear maps between various input and output representations are very limited in their expressive power.

Hence, to develop our models, we derive a complete characterization of the sign equivariant polynomial functions. The form of these equivariant polynomials directly inspires our equivariant neural network architectures. Further, our architectures inherit the theoretical expressive power guarantees of the equivariant polynomials. Our characterization is also broadly useful for analysis and development of sign-symmetry-respecting architectures--for instance, we provide a new proof of the universality of SignNet (Lim et al., 2023) by showing that it can approximate all sign invariant polynomials.

To validate our theoretical results, we conduct various numerical experiments on synthetic datasets. Experiments in link prediction, n-body problems, and node clustering in graphs support our theory and demonstrate the utility of sign equivariant models.

### Background

Let \(f:^{n k}^{n k}\) be a function that takes eigenvectors \(v_{1},,v_{k}^{n}\) of an underlying matrix as input, and outputs representations \(f(v_{1},,v_{k})\). We often concatenate the eigenvectors into a matrix \(V=[v_{1},,v_{k}]^{n k}\), and write \(f(V)\) as the application of \(f\). For simplicity, in this work we assume the eigenvectors come from a symmetric matrix, so they are taken to be orthonormal.

**Sign and basis symmetries.** Eigenvectors have symmetries, because there are many possible choices of eigenvectors of a matrix. For instance, if \(v\) is a unit-norm eigenvector of a matrix, then so is the sign-flipped \(-v\). If the eigenvalue of \(v\) is simple, then \(-v\) is the only other choice of unit-norm eigenvector of this eigenvalue.

If \(v_{1}, v_{m}\) are an orthonormal basis of eigenvectors for the same eigenspace (meaning they all have the same eigenvalue), then there are infinitely many other choices of orthonormal basis for this eigenspace; these other choices of basis can be written as \(VQ\), where \(V=[v_{1} v_{m}]^{n m}\) and \(Q O(m)\) is an arbitrary orthogonal matrix.

We refer to these symmetries collectively as sign and basis symmetries, or more simply as eigenvector symmetries. Note that sign symmetries are a special case of basis symmetries, as \(-1\) and \(1\) are the only orthogonal \(1 1\) matrices. Previous work has developed neural networks that are invariant to these symmetries--that is, networks that have the same output for any choice of sign or basis of the eigenvector inputs (Lim et al., 2023).

**Sign equivariance** means that if we flip the sign of an eigenvector, then the corresponding column of the output of a function \(f\) has its sign flipped. In other words, for all choices of signs \(s_{1},,s_{k}\{-1,1\}^{k}\),

\[f(s_{1}v_{1},,s_{k}v_{k})_{:,j}=s_{j}f(v_{1},,v_{k})_{:,j},\] (1)

where \(A_{:,j}\) is the \(j\)-th column of an \(n k\) matrix \(A\). See Figure 1 for an illustration. In matrix form, letting \((\{-1,1\}^{k})\) represent all \(k k\) diagonal matrices with \(-1\) or \(1\) on the diagonal, \(f\) is sign equivariant if

\[f(VS)=f(V)SS(\{-1,1\}^{k}).\] (2)

As \(O(1)=\{-1,1\}\), we can write sign equivariance as equivariance with respect to a direct product of orthogonal groups \(O(1) O(1)\). This is different from the equivariance to a single orthogonal group \(O(d)\) considered in works on Euclidean-group equivariant networks (Thomas et al., 2018).

**Permutation equivariance** is often also a desirable property of our functions \(f\). We say that \(f\) is _permutation equivariant_ if \(f(PV)=Pf(V)\) for all \(n n\) permutation matrices \(P\). For instance, eigenvectors of matrices associated with simple graphs of size \(n\) have such permutation symmetries, as the ordering of nodes is arbitrary.

## 2 Applications of Sign Equivariance

In this section, we present several applications for which modeling networks with sign equivariant architectures is beneficial. We identify that sign invariant networks are _provably_ insufficient for these tasks, motivating the development of sign equivariant networks to address these limitations.

### Multi-Node Representations and Link Prediction

In several settings, we desire a machine learning model that computes representations for tuples of several nodes in a graph. For instance, link prediction tasks generate probabilities for pairs of nodes, and both hyperedge prediction and subgraph prediction tasks learn representations for collections of nodes in a graph (Alsentzer et al., 2020; Wang et al., 2023). For ease of exposition, the rest of this section discusses link prediction, though the discussion applies to general multi-node tasks as well.

In link prediction, we typically want to learn _structural node-pair representations_, meaning adjacency-permutation equivariant functions that give a representation for each pair of nodes; more precisely, a structural node-pair representation is a map \(f:^{n n}^{n n}\) such that \(f(PAP^{})=Pf(A)P^{}\), where \(f(A)_{i,j}\) is the representation of the pair of nodes \((i,j)\) in the graph with adjacency matrix \(A\)(Srinivasan and Ribeiro, 2019) (see Appendix A.5 for more discussion). One method to do this is to use a graph model such as a standard GNN to learn node representations \(z_{i}\), and then obtain a node-pair representation for \((i,j)\) as some function \(f_{}(z_{i},z_{j})\) of \(z_{i}\) and \(z_{j}\). However, this approach is limited because standard GNNs learn _structural node encodings_--that is, adjacency-permutation equivariant node features \(f_{}:^{n n}^{n}\) such that \(f_{}(PAP^{})=Pf_{}(A)\)(Srinivasan and Ribeiro, 2019; Zhang et al., 2021). 2 Structural node encodings give automorphic nodes the same representation, which can be problematic since automorphic nodes can be far apart in the graph. For instance, in Figure 2, \(u_{1}\) and \(u_{2}\) are automorphic, so a link predictor based on structural node encodings would give both \(u_{1}\) and \(u_{2}\) the same probability of connecting to \(w\), but one would expect \(u_{1}\) to have a higher probability of connecting to \(w\). Most state-of-the-art link prediction methods on the Open Graph Benchmark leaderboards (Hu et al., 2020) were deliberately developed to avoid the issues of structural node encodings.

One way to surpass the limitations of structural node encodings is to use _positional node embeddings_, which can assign different values to automorphic nodes. Intuitively, positional encodings capture information such as distances between nodes and global position of nodes in the graph (see (Srinivasan and Ribeiro, 2019) for a formal definition). Laplacian eigenvectors are an important example of node

Figure 2: (a) First nontrivial normalized Laplacian eigenvector of a graph, which is positional. Nodes \(u_{1}\) and \(u_{2}\) are far apart in the graph, but automorphic. (b) Sign invariant node features, which are structural. Nodes \(u_{1}\) and \(u_{2}\) have the same feature. (c) Sign equivariant node features, which are positional. Nodes \(u_{1}\) and \(u_{2}\) have opposite signs. A link prediction model with sign invariant node features assigns \(u_{1}\) and \(u_{2}\) the same probability of connecting to \(w\), while sign equivariant node features could give higher probability to \(u_{1}\).

positional embeddings that capture much useful information of graphs (Chung, 1997; Von Luxburg, 2007). In Figure 2 (a), the first nontrivial Laplacian eigenvector captures cluster structure in the graph, and as such assigns \(u_{1}\) and \(u_{2}\) very different values.

Pitfalls of sign and basis invariance.When processing eigenvectors of matrices associated with graphs, invariance to the symmetries of the eigenvectors has been found useful (Dwivedi et al., 2022; Lim et al., 2023), especially for graph classification tasks. However, we show that exact invariance to these symmetries _removes positional information_, and thus the outputs of sign invariant or basis invariant networks are in fact _structural node encodings_ (see Appendix A.5).3 Hence, eigenvector-symmetry-invariant networks cannot learn node representations that distinguish automorphic nodes, and thus face the aforementioned difficulties when used for link prediction or multi-node tasks:

**Proposition 1**.: _Let \(f:^{n k}^{n d_{}}\) be a permutation equivariant function, and let \(V=[v_{1},,v_{k}]^{n k}\) be \(k\) orthonormal eigenvectors of an adjacency matrix \(A\). Let nodes \(i\) and \(j\) be automorphic, and let \(z_{i}\) and \(z_{j}^{d_{}}\) be their embeddings, i.e, the \(i\)th and \(j\)th row of \(Z=f(V)\)._

* _If_ \(f\) _is sign invariant and the eigenvalues associated with the_ \(v_{l}\) _are simple and distinct, then_ \(z_{i}=z_{j}\)_._
* _If_ \(f\) _is basis invariant and_ \(v_{1},,v_{k}\) _are a basis for some number of eigenspaces of_ \(A\) _then_ \(z_{i}=z_{j}\)_._

A novel link prediction approach via sign equivariance.The problem \(z_{i}=z_{j}\) arises from the sign/basis invariances, which remove crucial positional information. We instead propose using sign _equivariant_ networks (as in Section 3) to learn node representations \(z_{i}=f(V)_{i,:}^{k}\). These representations \(z_{i}\) maintain positional information for each node thanks to preserving sign information (see Figure 2 (c)). Then we use a sign invariant decoder \(f_{}(z_{i},z_{j})=f_{}(Sz_{i},Sz_{j})\) for \(S(\{-1,1\}^{k})\) to obtain node-pair representations. For instance, the commonly used \(f_{}=(z_{i} z_{j})\), where \(\) is the elementwise product, is sign invariant. When the eigenvalues are distinct, this approach has the desired invariances (yielding structural node-pair representations) and also maintains positional information in the node embeddings; see Appendix A.5 for a proof of the invariances, and Appendix A.5.1 for an example of where sign equivariant models can be used to compute strictly more expressive node-pair representations than sign invariant models. More details and the proof of Proposition 1 are in Appendix A.4.

Our sign equivariance based approach differs substantially from existing methods for learning structural pair representations without being bottlenecked by structural node representations. Many of these methods are based on labeling tricks (Zhang et al., 2021; Wang et al., 2023), whereby the representation for a node-pair is obtained by labeling the two nodes in the pair and then processing an enclosing subgraph. Without special modifications (Zhu et al., 2021; Chamberlain et al., 2023), this requires a separate expensive subgraph extraction and forward pass for each node-pair. In contrast, our method only requires one forward pass on the original graph to compute all positional node embeddings, after which pair representations can be obtained with a cheap, parallelizable decoding.

### Orthogonal Equivariance

For various applications in modelling physical systems, we desire equivariance to rigid transformations; thus, orthogonally equivariant models have been a fruitful research direction in recent years (Thomas et al., 2018; Weiler et al., 2018; Anderson et al., 2019; Deng et al., 2021). We say that a function \(f:^{n k}^{n k}\) is orthogonally equivariant if \(f(XQ)=f(X)Q\) for any \(Q O(k)\), where \(O(k)\) is the set of orthogonal matrices in \(^{k k}\). Orthogonal equivariance imposes infinitely many constraints on the function \(f\). Several works have approached this problem by reducing to a finite set of constraints using so-called Principal Component Analysis (PCA) based frames (Puny et al., 2022; Atzmon et al., 2022; Xiao et al., 2020).

PCA-frame methods take an input \(X^{n k}\), compute orthonormal eigenvectors \(R_{X} O(k)\) of the covariance matrix \((X)=(X-^{}X)^{}(X- {n}^{}X)\) (assumed to have distinct eigenvalues), then average outputs of a base model \(h\) for each of the \(2^{k}\) sign-flipped inputs \(XR_{X}S\), where \(S(\{-1,1\}^{k})\). We instead suggest using a sign equivariant network to parameterize an efficient \(O(k)\) equivariant model, which allows us to bypass the need to average the exponentially many sign-flipped inputs. For a sign equivariant network \(h\), we define our model \(f\) to be

\[f(X)=h(XR_{X})R_{X}^{}.\] (3)

See Figure 3 for an illustration. Intuitively, this first transforms \(X\) by \(R_{X}\) into a nearly canonical orientation that is unique up to sign flips; this can be seen as writing the points in the principal components basis, or aligning the principal components of \(X\) with the coordinate axes. Then we process \(XR_{X}\) using the model \(h\) that respects the sign symmetries, and finally, we incorporate orientation information back into the output by post-multiplying by \(R_{X}^{}\). Our approach only requires one forward pass through \(h\), whereas frame averaging requires \(2^{k}\) forward passes through a base model. The following proposition shows that \(f\) is \(O(k)\) equivariant, and inherits universality properties of \(h\).4

**Proposition 2**.: _Consider a domain \(^{n k}\) such that each \(X\) has distinct covariance eigenvalues, and let \(R_{X}\) be a choice of orthonormal eigenvectors of \((X)\) for each \(X\). If \(h:^{n k}^{n k}\) is sign equivariant, and if \(f(X)=h(XR_{X})R_{X}^{}\), then \(f\) is well defined and orthogonally equivariant._

_Moreover, if \(h\) is from a universal class of sign equivariant functions, then the \(f\) of the above form universally approximate \(O(k)\) equivariant functions on \(\)._

We include a proof of this result in Appendix A.6. This result also follows from Theorems 3.1 and 3.3 of Kaba et al. (2023), who show that generally we can canonicalize up to a subgroup \(K\) of a group \(G\), and achieve \(G\)-equivariance via a \(K\)-equivariant base predictor. In our case, \(G=O(k)\) and \(K=\{-1,1\}^{k}\)

Sign invariance only gives orthogonal invariance.In a similar way, a sign _invariant_ model can be used to obtain an orthogonally _invariant_ model with PCA frames, but it cannot be used for orthogonal equivariance; instead, sign equivariance is needed.

   Constraints & Polynomials & Neural Networks \\  \(^{k}\) inv. & \(_{d_{1},,d_{k}=0}^{D}_{d_{1},,d_{k}}v_{1}^{2d_{1}}  v_{k}^{2d_{k}}\) & \((|v|)\) \\ \(^{n k}\) inv. & \(q[\{V_{1,j} V_{2,j}\}_{1[n],i_{2}[n],j[k]})\) & \((V)=[\{(v_{i})+(-v_{i})\}_{i=1,,k})\) \\  \(^{k}^{k}\) equiv. & \(v p_{}(v)\) & \(v(|v|)\) \\ \(^{n k}^{n^{} k}\) equiv. & \(W^{(2)}((W^{(1)}V) p_{}(V))\) & \([W^{(1)}_{1}v_{1},,W^{(l)}_{k}v_{k}]_{t}(V)\) \\   

Table 1: Sign invariant or equivariant polynomials and corresponding neural network architectures for different input and output spaces. \(v^{k}\) or \(V^{n k}\) are inputs to the polynomials or networks. Appendix C contains more details on the polynomials.

Figure 3: Using sign equivariant functions \(h\) to parameterize orthogonally equivariant \(f(X)=h(XR_{X})R_{X}^{}\), where \(R_{X}\) is a choice of principal components for the point cloud. We first transform \(X\) via \(R_{X}\) into an orientation that is unique up to sign flips, then process \(XR_{X}\) using the sign equivariant model \(h\), and finally reintegrate orientation information back into the output via \(R_{X}^{}\).

## 3 Sign Equivariant Polynomials and Networks

In this section, we analytically characterize the sign equivariant polynomials, and use this characterization to develop sign equivariant architectures. As equivariant polynomials universally approximate continuous equivariant functions (Yarotsky, 2022), our architectures inherit universality guarantees. We summarize our results on polynomials and neural network architectures in Table 1. Our characterization of the invariant polynomials also allows us to give an alternative proof of the universality of the sign invariant neural network SignNet (Lim et al., 2023) (see Appendix C.4).

### Sign Equivariant Linear Maps

First, we consider the important case of degree one polynomials, i.e. sign equivariant linear maps from \(^{n k}^{n^{} k}\). These maps are very limited in expressive power, as they act independently on each eigenvector.

**Lemma 1**.: _A linear map \(W:^{n k}^{n^{} k}\) is sign equivariant if and only if it can be written as_

\[W(X)=[W_{1}X_{1}\;\;W_{k}X_{k}]\] (4)

_for some linear maps \(W_{1},,W_{k}:^{n}^{n^{}}\), where \(X_{i}^{n}\) is the \(i\)th column of \(X^{n k}\)._

See Appendix B.1 for the proof. Notably, when \(n=n^{}=1\), the linear maps are diagonal matrices.

This means a model with elementwise nonlinearities and sign equivariant linear maps will not capture any _interactions_ between eigenvectors. For instance, when used for parameterizing orthogonally equivariant models as in Section 2.2, such a model would process each principal component direction of the point cloud independently. Hence, the popular approach --outlined in the "Geometric Deep Learning Blueprint" (Bronstein et al., 2021)--of interleaving equivariant linear maps and equivariant nonlinearities (Cohen and Welling, 2016; Zaheer et al., 2017; Kondor and Trivedi, 2018; Maron et al., 2018, 2019; Finzi et al., 2021) is not as fruitful here.

However, one may choose instead different group representations for the input and output space, but our attempts to do this do not lead to efficient models. For instance, a common method to improve expressive power of models that use equivariant linear maps is to use tensor representations (Maron et al., 2018, 2019; Finzi et al., 2021); in our case, this would correspond having equivariant hidden representations in \(^{n k^{m}}\) for some tensor order \(m\). This is also inefficient, as we explain in Appendix B.1; we show that such an approach would have to lift to tensors of at least order 3, and that there are many sign equivariant linear maps between tensors of order 3. There is a possibility that some other group representations may allow the Geometric Deep Learning Blueprint to work better for sign equivariant networks, but we could not find any such representations.

For these reasons, we will now analyze the entire space of sign equivariant polynomials.

### Sign Equivariant Polynomials

Consider polynomials \(p:^{n k}^{n^{} k}\) that are sign equivariant, meaning \(p(VS)=p(V)S\) for \(S(\{-1,1\}^{k})\). We can show that a polynomial \(p\) is sign equivariant if and only if it can be written as the elementwise product of a simple (linear) sign equivariant polynomial and a general sign invariant polynomial, followed by another linear sign equivariant map.

**Theorem 1**.: _A polynomial \(p:^{n k}^{n^{} k}\) is sign equivariant if and only if it can be written_

\[p(V)=W^{(2)}((W^{(1)}V) p_{}(V))\] (5)

_for sign equivariant linear \(W^{(2)}\) and \(W^{(1)}\), and a sign invariant polynomial \(p_{}:^{n k}^{n^{} k}\)._

This reduction of sign equivariant polynomials to sign invariant polynomials combined with simple operations is convenient, as it enables us to leverage recent universal models for sign invariant functions (Lim et al., 2023). The proof of this statement is in Appendix C, which proceeds by showing that sign equivariance leads to linear constraints on the coefficients of a polynomial, which requires the polynomial to take the form stated in the Theorem.

### Sign Equivariance without Permutation Symmetries

Using Theorem 1, we can now develop sign equivariant architectures. We parameterize sign equivariant functions \(f:^{n k}^{n^{} k}\) as a composition of layers \(f_{l}\), each of the form

\[f_{l}(V)=[W_{1}^{(l)}v_{1},,W_{k}^{(l)}v_{k}]_{l}(V),\] (6)

in which the \(W_{i}^{(l)}:^{n}^{n^{}}\) are arbitrary linear maps, and \(_{l}:^{n k}^{n^{}  k}\) is sign invariant . In the case of \(n=n^{}=1\), there is a simple universal form: we can write a sign equivariant function \(f:^{k}^{k}\) as \(f(v)=v(|v|)\), where \(|v|\) is the elementwise absolute value. These two architectures are universal because they can approximate sign equivariant polynomials. Here, the sign invariant part captures interactions between eigenvectors that the equivariant linear maps cannot.

**Proposition 3**.: _Functions of the form \(v v(|v|)\) universally approximate continuous sign equivariant functions \(f:^{k}^{k}\)._

_Compositions \(f_{2} f_{1}\) of functions \(f_{l}\) as in equation 6 universally approximate continuous sign equivariant functions \(f:^{n k}^{n^{} k}\)._

### Sign Equivariance and Permutation Equivariance

For models on eigenvectors that stem from graphs or point clouds, in addition to sign equivariance, we may demand permutation equivariance, i.e., \(f(PV)=Pf(V)\) for all permutation matrices \(P^{n n}\). To add permutation equivariance to our neural network architecture from Section 3.3, we use it within the framework of DeepSets for Symmetric Elements (DSS) . For a hidden dimension size of \(d_{f}\), each layer \(f_{l}:^{n k d_{f}}^{n k  d_{f}}\) of our DSS-based sign equivariant network takes the following form on row \(i\):

\[f_{l}(V)_{i,:}=f_{l}^{(1)}(V_{i,:})+f_{l}^{(2)}_{j i }V_{j,:},\] (7)

where \(f_{l}^{(1)}\) and \(f_{l}^{(2)}\) are sign equivariant functions as in Section 3.3. Sometimes we take \(d_{f}=1\), in which case we can use the simpler \(^{k}^{k}\) sign equivariant networks (\(v(|v|)\)) as \(f_{l}^{(1)}\) and \(f_{l}^{(2)}\). If we have graph information, then we can do message-passing by changing the sum over \(j i\) to a sum over a neighborhood of node \(i\). DSS has universal approximation guarantees , but they only apply for groups that act as permutation matrices, whereas the sign group \(\{-1,1\}^{k}\) does not. Hence, the universal approximation properties of our proposed DSS-based architecture are still an open question.

## 4 Experiments

Our theoretical results in Section 2 predict benefits of sign equivariance in various tasks: link prediction in nearly symmetric graphs, orthogonally equivariant simulations in n-body problems, and node clustering with positional information. Next, we probe these suggested benefits empirically.

### Link Prediction in Nearly Symmetric Graphs.

We begin with a synthetic link prediction task, which is carefully controlled to test the theoretically foreseen benefits of sign equivariance explained in Section 2.1. With the intuition of Figure 2 we first

    &  &  \\  Model & Test AUC & Runtime (s) & Test AUC & Runtime (s) \\  GCN (constant input) &.497\(.06\) &.058\(.00\) &.705\(.01\) &.048\(.00\) \\ SignNet &.498\(.00\) &.120\(.00\) &.707\(.00\) &.095\(.00\) \\ \(V_{i}^{}V_{j,:}\) &.570\(.01\) &.010\(.01\) &.597\(.01\) &.008\(.00\) \\ \((V_{i,:} V_{j,:})\) &.614\(.02\) &.050\(.00\) &.651\(.03\) &.040\(.00\) \\ Sign Equivariant & **.751\(.00\)** &.063\(.00\) & **.773\(.01\)** &.054\(.00\) \\   

Table 2: Link prediction AUC and runtime per epoch for structural edge models.

either generate an Erdos-Renyi (Erdos et al., 1960) or Barabasi-Albert (Barabasi and Albert, 1999) random graph \(H\) of 1000 nodes. Then we form a larger graph \(G\) that contains two disjoint copies of \(H\), along with 1000 uniformly-randomly added edges (both between and within copies of \(H\)). Without the random edges, each node in one copy of \(H\) is automorphic to the corresponding node in the other copy, so we expect many nodes to be nearly automorphic with the randomly added edges.

In Table 2, we show the link prediction performance of several models that learn structural edge representations. The methods that use eigenvectors have a sign invariant final prediction for each edge. GCN (Kipf and Welling, 2017) where the node features are all ones and SignNet (Lim et al., 2023) both completely fail on the Erdos-Renyi task (these two models map automorphic nodes to the same embedding), while our sign equivariant model outperforms all methods. We also try two eigenvector baselines that maintain node positional information, but do not update eigenvector representations: taking the dot product \(V_{i,:}^{}V_{j,:}\) to be the logit of a link existing, or learning a simple decoder \((V_{i,:} V_{j,:})\). Both perform substantially worse than our sign equivariant model, which shows that updating eigenvector representations is important here. Further, the sign equivariant model takes comparable runtime to GCN, and is significantly faster than SignNet. This is because we use networks of the form \(v v(|v|)\) in these experiments instead of the full SignNet-based model in equation 6. See Appendix E.2 for more details.

### Orthogonal Equivariance in n-body Problems

In this section, we empirically test the ability of our sign equivariant models to parameterize orthogonally equivariant functions on point clouds, as outlined in Section 2.2. For this purpose, we consider simulating n-body problems, following the setup in Fuchs et al. (2020) and building on the code from Puny et al. (2022). To test the favorable scaling of our method in the dimension \(d\) of the problem against the exponential \(2^{d}\) scaling of frame averaging, we generalize this problem to general dimensions \(d 3\). We maintain the choice of \(n=5\) particles, and generate new point clouds using the same procedure as in Fuchs et al. (2020) (sampling random points and initial velocities in a general dimension \(d\)). We measure model performance via mean squared error (MSE). We use a DSS-based model that we describe in more detail in Appendix E.3.

Figure 4 illustrates the runtime and MSE. The sign equivariant model scales well with dimension--the time-per-epoch is nearly constant as we increase the dimension. In contrast, frame averaging suffers from the expected exponential slowdown with dimension, and runs out of memory on a 32GB V100 GPU for \(d=11\). Considering the MSE, the equivariant model's performance closely follows that of frame averaging, i.e., we only have a small loss in accuracy with much better scalability. For \(d=3\), the sign equivariant model has an MSE of.00646, compared to the.00575 of frame averaging (Puny et al., 2022). Additional \(d=3\) comparisons to other baselines are included in Appendix E.3.

### Node Clustering with Positional Information

As explained in Section 2.1, some applications on graph data call for positional node embeddings that can assign different representations to automorphic nodes. For instance, consider community detection or node clustering tasks on graphs, where a model makes a prediction for each node that

Figure 4: Sign equivariant model versus frame averaging model for n-body experiments in varying dimensions. Lower \(y\)-axis is better for both plots. (Left) The runtime of frame averaging increases exponentially in dimension while the sign equivariant runtime is approximately constant. Frame averaging runs out of memory on \(d=11\). (Right) The error of the sign equivariant model is very similar to that of frame averaging.

assigns it to a cluster. Structural encodings are insufficient for this task, as there may be automorphic or nearly-automorphic nodes that are far apart in the graph but look alike in a structural encoding. Hence, node structural encodings would guide the model to assign these nodes to the same cluster, even though they should belong to different clusters. As a concrete example, consider a graph of two clusters, and using Laplacian eigenvectors as positional encodings. The first nontrivial eigenvector will tend to assign a positive sign to one cluster and a negative sign to the other cluster. Thus, the sign information in the eigenvectors is crucial, so we expect sign equivariant models to perform well.

We test models on the CLUSTER dataset (Dwivedi et al., 2022) for semi-supervised node clustering (viewed as node classification) in synthetic graphs. In these experiments, we build on the empirically well-performing GraphGPS model (Rampasek et al., 2022), and incorporate our sign equivariant models to update eigenvector representations within the version of GraphGPS that uses PEG (Wang et al., 2022) to process positional encodings. See Appendix E.4 for more experimental details.

As seen in Table 3, our sign equivariant models outperform all of the other GraphGPS-based eigenvector methods. Moreover, we achieve the second best performance across all methods, showing that sign equivariant models can indeed achieve the theoretically expected benefits in this setting.

## 5 Related Work

Structural and Positional Representations.Especially for link prediction, the need for structural node-pair representations that are not obtained from structural node representations has been discussed in several works (Srinivasan and Ribeiro, 2019; Zhang et al., 2021; Cotta et al., 2023). As such, various methods have been developed for learning structural node-pair representations that incorporate node positional information. SEAL and other labeling-trick based methods (Zhang and Chen, 2018; Zhang et al., 2021) use added node features depending on the node-pair that we want a representation of. This is empirically successful in many tasks, but typically requires a separate subgraph extraction and forward pass through a GNN for each node-pair under consideration. Distance encoding (Li et al., 2020) uses relative distances between nodes to capture positional information. PEG (Wang et al., 2022) similarly maintains positional information by using eigenvector distances between nodes in each layer of a GNN, but does not update eigenvector representations. Identity-aware GNNs (You et al., 2021) and Neural Bellman-Ford Networks (Zhu et al., 2021) learn pair representations by conditioning on a source node from the pair.

Eigenvectors as Graph Positional Encodings.When using eigenvectors of graphs as node positional encodings for graph models like GNNs and Graph Transformers, many works have noted the need to address the sign ambiguity of the eigenvectors. This is often done by encouraging sign invariance through data augmentation--the signs of the eigenvectors are chosen randomly in each iteration of training (Dwivedi et al., 2022, 2022; He et al., 2022; Muller et al., 2023; Zhou et al., 2021). In contrast, SignNet (Lim et al., 2023) enforces exact sign invariance, by processing eigenvectors with a sign invariant neural architecture; this approach has been taken by some recent works (Rampasek et al., 2022; Geisler et al., 2023; Murphy et al., 2023).

Equivariant Neural Network Design.Equivariant neural network architectures have been proposed for various types of data and symmetry groups. A common paradigm is to interleave equivariant linear maps and equivariant pointwise nonlinearities (Wood and Shawe-Taylor, 1996; Cohen and Welling, 2016, 2017; Ravanbakhsh et al., 2017; Maron et al., 2018; Kondor and Trivedi, 2018; Finzi et al., 2021; Bronstein et al., 2021; Pearce-Crump, 2022); this is often used when the group acts as some subset of the permutation matrices. However, the sign group does not act as permutation matrices, and as we explained above this approach is not expressive for sign equivariant models.

   Model & Test Acc. (\%) \\  GCN (Kipf and Welling, 2017) & \(68.498 0.976\) \\ GIN (Xu et al., 2019) & \(64.716 1.553\) \\ GAT (Velickovic et al., 2018) & \(70.587 0.447\) \\ GatedCO (Bresson and Laurent, 2017) & \(73.840 0.326\) \\ SAN (Kreuzer et al., 2021) & \(76.691 0.650\) \\ K-Subgraph SAT (Chen et al., 2022) & \(\) \\ EGT (Hussain et al., 2022) & \( 0.348\) \\ GPS (Rampasek et al., 2022) & \(78.016 0.180\) \\   \\ No PE & \(77.423 0.241\) \\ LapPE (Dwivedi et al., 2022) & \(77.250 0.280\) \\ PEG (Wang et al., 2022) & \(77.945 0.310\) \\ SignNet (Lim et al., 2023) & \(77.442 0.102\) \\ Sign Equivariant (ours) & \( 0.118\) \\   

Table 3: Results on the CLUSTER node classification task, for which positional information is needed. We compare different SOTA and Laplacian eigenvector-based methods.

More similarly to our approach, many equivariant machine learning works heavily leverage invariant or equivariant polynomials (or other equivariant nonlinear functions). These works include polynomials as operations within a network (Thomas et al., 2018; Puny et al., 2023), add polynomials as features (Yarotsky, 2022; Villar et al., 2021), build networks that take a similar form to equivariant polynomials (Villar et al., 2021), and/or analyze neural network expressive power by determining which equivariant polynomials a given architecture can compute (Zaheer et al., 2017; Segol and Lipman, 2019; Maron et al., 2019, 2020; Chen et al., 2020; Dym and Maron, 2021; Puny et al., 2023).

## 6 Conclusion

In this work, we identify and study an important method of respecting the symmetries of eigenvector data--sign equivariant models. For multi-node representation tasks, link prediction, and orthogonally equivariant tasks, sign equivariance provides a natural inductive bias; in contrast, we show that sign invariant models are provably limited in these tasks. To develop sign equivariant neural networks, we analytically characterize the sign equivariant polynomials, and then define neural networks that parameterize functions of similar form. Our neural networks are thus expressive, and inherit universal approximation guarantees of the equivariant polynomials. In several experiments, we show that our neural networks can indeed achieve the theoretically predicted benefits of sign equivariant models.

Limitations and Future Work.While we developed sign equivariant architectures in this work, we did not explore basis-change equivariant architectures, which would have the desired symmetries for inputs with repeated eigenvalues. As eigenvalue multiplicities are known to occur in many real-world graphs (Lim et al., 2023), future work in this area could be useful. Further, we give evidence that sign equivariance could help in some node-level and multi-node-level prediction tasks on graphs, but we do not have theoretical reason to believe that sign equivariance could help in graph-level representation tasks, which for instance are common in molecule processing. Our theoretical results are focused on expressive power, but we do not have results on other properties that are important for learning, such as optimization (Xu et al., 2021), stability (Wang et al., 2022; Huang et al., 2023), or generalization (Keriven and Vaiter, 2023). Finally, while we can prove universality of our models in the non-permutation-equivariant setting, we do not know of the exact expressive power in the permutation equivariant setting. Lim et al. (2023) also faces this issue for sign invariant models; future work on analyzing and possibly improving the expressive power of these models -- if they are not universal -- is promising.

#### Acknowledgments

We would like to thank Yaron Lipman for contributing significantly early on in this work. We would like to thank Maks Ovsjanikov for noting that basis invariant models give automorphic nodes the same representation, and also Johannes Lutzeyer and Michael Murphy for helpful comments. We would also like to thank the reviewers of the Physics4ML workshop at ICLR 2023 for helpful feedback and close reading. DL is supported by an NSF Graduate Fellowship. SJ acknowledges support from NSF AI Institute NSF CCF-2112665, NSF Award 2134108, and Office of Naval Research Grant N00014-20-1-2023 (MURI ML-SCOPE).