# Regularity as Intrinsic Reward for Free Play

Cansu Sancaktar\({}^{1}\)Justus Piater\({}^{2}\)Georg Martius\({}^{1,3}\)

\({}^{1}\)Max Planck Institute for

Intelligent Systems, Germany

cansu.sancaktar@tuebingen.mpg.de

###### Abstract

We propose regularity as a novel reward signal for intrinsically-motivated reinforcement learning. Taking inspiration from child development, we postulate that striving for structure and order helps guide exploration towards a subspace of tasks that are not favored by naive uncertainty-based intrinsic rewards. Our generalized formulation of Regularity as Intrinsic Reward (RaIR) allows us to operationalize it within model-based reinforcement learning. In a synthetic environment, we showcase the plethora of structured patterns that can emerge from pursuing this regularity objective. We also demonstrate the strength of our method in a multi-object robotic manipulation environment. We incorporate RaIR into free play and use it to complement the model's epistemic uncertainty as an intrinsic reward. Doing so, we witness the autonomous construction of towers and other regular structures during free play, which leads to a substantial improvement in zero-shot downstream task performance on assembly tasks. Code and videos are available at [https://sites.google.com/view/rair-project](https://sites.google.com/view/rair-project).

## 1 Introduction

Regularity, and symmetry as a specific form of regularity, are ubiquitous in nature as well as in our manufactured world. The ability to detect regularity helps to identify essential structures, minimizing redundancy and allowing for efficient interaction with the world . Not only do we encounter symmetries in arts, design, and architecture, but our preference also showcases itself in play behavior. Adults and children have both been observed to prefer symmetry in visual perception, where symmetric patterns are more easily detected, memorized, and copied . Several works in developmental psychology show that regular patterns and symmetries are actively sought out during free play in children as well as adults .

Figure 1: **Regularity as intrinsic reward yields ordered and symmetric patterns. In ShapeGridWorld (top row) and in Construction (bottom row), we showcase the generated constellations when maximizing our proposed regularity reward RaIR with ground truth (GT) models.**Considering this in the context of a child's developmental cycle is intriguing. Studies show that children at the age of 2 exhibit a shift in their exploratory behavior. They progress from engaging in random actions on objects and unstable arrangements to purposefully engaging in functional activities and intentionally constructing stable configurations [7; 8; 5]. Bailey  reports that by 5 years of age, children build more structured arrangements out of blocks that exhibit alignment, balance, and examples of symmetries .

Despite the dominance of regularity in our perceptual systems and our preference for balance and stability during play, these principles are not yet well investigated within intrinsically-motivated reinforcement learning (RL). One prominent intrinsic reward definition is novelty, i.e. the agent is incentivized to visit areas of the state space with high expected information gain [9; 10; 11; 12]. However, one fundamental problem with plain novelty-seeking objectives is that the search space is often unconstrained and too large. As an agent only has limited resources to allocate during play time, injecting appropriate inductive biases is crucial for sample efficiency, good coverage during exploration, and emergence of diverse behaviors. As proposed by Sancaktar et al. , using structured world models to inject a relational bias into exploration, yields more object and interaction-related novelty signals. However, which types of information to prioritize are not explicitly encoded in any of these methods. The direction of exploration is often determined by the inherent biases in the practical methods deployed. With imperfect world models that have a limited learning capacity and finite-horizon planning, novelty-seeking methods are observed to prefer "chaotic" dynamics, where small perturbations lead to diverging trajectories, such as throwing, flipping, and poking objects. This in turn means that behaviors focusing on alignment, balance, and stability are overlooked. Not only are these behaviors relevant, as shown in developmental psychology, they also enable expanding and diversifying the types of behavior uncovered during exploration. As the behaviors observed during exploration are highly relevant for being able to solve downstream tasks, a chaos-favoring exploration will make it hard to solve assembly tasks, such as stacking. Indeed, successfully solving assembly tasks with more than 2 objects has been a challenge for intrinsically-motivated reinforcement learning.

We pose the question: how can we define an intrinsic reward signal such that RL agents prefer structured and regular patterns? We propose RaIR: **R**egularity **as**Intrinsic **R**eward, which aims to achieve highly ordered states. Mathematically, we operationalize this idea using entropy minimization of a suitable state description. Entropy and symmetries have been linked before [13; 14], however, we follow a general notion of regularity, i.e. where patterns reoccur and thus their description exhibits high redundancy / low entropy. In this sense, symmetries are a consequence of being ordered . Regularity also means that the description is compressible, which is an alternative formulation. As argued by Schmidhuber , aiming for compression-progress is a formidable curiosity signal, however, it is currently unclear how to efficiently predict and optimize for it.

After studying the design choices in the mathematical formulation of regularity and the relation to symmetry operations, we set out to evaluate our regularity measure in the context of model-based reinforcement learning/planning, as it allows for highly sample-efficient exploration and solving complex tasks zero-shot, as shown in Sancaktar et al. . To get a clear understanding of RaIR, we first investigate the generated structures when directly planning to optimize it using the ground truth system dynamics. A plethora of patterns emerge that are highly _regular_, as illustrated in Fig. 1.

Our ultimate goal is, however, to inject the proposed regularity objective into a free-play phase, where a robot can explore its capabilities in a task-free setting. During this free play, the dynamics model is learned on-the-go. We build on CEE-US , a free-play method that uses an ensemble of graph neural networks as a structured world model and the model's epistemic uncertainty as the only intrinsic reward. The epistemic uncertainty is estimated by the ensemble disagreement and acts as an effective novelty-seeking signal. We obtain _structure-seeking free play_ by combining the conventional novelty-seeking objective with RaIR.

Our goal is to operationalize regularity, which is a well-established concept in developmental psychology, within intrinsically motivated RL. Furthermore, we showcase that biasing information-search towards regularity with RaIR indeed leads to the construction of diverse regular structures during play and significantly improves zero-shot performance in downstream tasks that also favor regularity, most notably assembly tasks. Besides conceptual work on compression [17; 16], to our knowledge, we are the first to investigate regularity as an intrinsic reward signal, bridging the gap between the diversity of behaviors observed in children's free play and what we can achieve with artificial agents.

## 2 Method

First, we introduce our intrinsic reward definition for regularity. Then, we present its practical implementation and explain how we combine this regularity objective into model learning within free play.

### Preliminaries

In this work, we consider environments that can be described by a fully observable Markov Decision Process (MDP), given by the tuple \((,,f^{}_{ss^{}},r^{a}_{ss^{}})\), with the state-space \(^{n_{s}}\), the action-space \(^{n_{a}}\), the transition kernel \(f:\), and the reward function \(r\). Importantly, we consider the state-space to be factorized into the different entities, e.g. \(=(_{})^{N}_{}\) for the state space of a robotic agent and \(N\) objects. We use model-based reinforcement learning, where data from interactions with the environment is used to learn a model \(\) of the MDP dynamics . Using this model, we consider finite-horizon (\(H\)) optimization/planning for undiscounted cumulative reward:

\[^{}_{t}=*{arg\,max}_{_{t}}_{h=0}^{H -1}r(s_{t+h},a_{t+h},s_{t+h+1}), \]

where \(s_{t+h}\) are imagined states visited by rolling out the actions using \(\), which is assumed to be deterministic. The optimization of Eq. 1 is done with the improved Cross-Entropy Method (iCEM)  in a model predictive control (MPC) loop, i.e. re-planning after every step in the environment. Although this is not solving the full reinforcement learning problem (infinite horizon and stochastic environments), it is very powerful in optimizing for tasks on-the-fly and is thus suitable for optimizing changing exploration targets and solving downstream tasks zero-shot.

### Regularity as Intrinsic Reward

Quite generally, regularity refers to the situation in which certain patterns reoccur. Thus, we formalize regularity as the **redundancy** in the description of the situation, to measure the degree of sub-structure recurrence. A decisive question is: which description should we use? Naturally, there is certain freedom in this choice, as there are many different coordinate frames. For instance, we could consider the list of absolute object positions or rather a relative representation of the scene.

To formalize, we define a mapping \(:\{\}^{+}\) from state to a multiset \(\{\}^{+}\) of symbols (e.g. coordinates). A multiset is a set where elements can occur multiple times, e.g. \(\{a,a,b\}^{+}\). This multiset can equivalently be described by a tuple \((X,m)\), where \(X\) is the set of the unique elements, and \(m:X^{+}\) is a function assigning the multiplicity, i.e. the number of occurrences \(m(x)\) for

Figure 2: **Regularity as intrinsic reward during free play.** (a) RaIR + CEE-US uses model-based planning to optimize \(H\) timesteps into the future for the combination of RaIR (Eq. 2) and epistemic uncertainty (ensemble disagreement of world models). (b) Here, for RaIR we use the absolute difference vector between objects: \((s_{i},s_{j})=\{(| s_{i,x}-s_{j,x}|,| s_{i,y}-s_{j,y} |)\}\).

the elements \(x X\). For the previous example, we get \((\{a,b\},\{a:2,b:1\})\). Given the multiset \((X,m)\{\}^{+}\), we define the discrete empirical distribution, also referred to as a histogram, by the relative frequency of occurrence \(p(x)=m(x)/_{x^{} X}m(x^{})\) for \(x X\).

We define the regularity reward metric using (negative) Shannon entropy  of this distribution as:

\[r_{}(s)-((s))= _{x X}p(x) p(x)\;(X,m)=(s), p (x)= X}m(x^{})}. \]

We will now discuss concrete cases for the mapping \(\), i.e. how to describe a particular state.

Direct RaIR.In the simplest case, we describe the state \(s\) directly by the properties of each of the entities. For that, we define the function \(:_{}\{\}^{+}\), that maps each entity to a set of symbols and obtain \((s)=_{i=1}^{N}(s_{,i})\) as a union of all symbols. The symbols can be, for instance, discretized coordinates, colors, or other properties of the entities.

Let us consider the example where \(\) is extracting the object's Cartesian \(x\) and \(y\) coordinates in a rounded manner as \((s)=\{ s_{x}, s_{y}\}^{+}\), as shown in Fig. 3. The most irregular configuration would be when no two objects share the same rounded value in \(x\) and \(y\). The object configuration becomes more and more regular the more objects share the same \( x\) and \( y\) coordinates. The most regular configuration is if all objects are in the same place. Note that this choice favors an axis-aligned configuration, and it is not invariant under global rotations.

Relational RaIR of order \(k\).Our framework for regularity quantification can easily be extended to a relational perspective, where we don't compute the entropy over aspects of individual entity properties, but instead on their pairwise or higher-order **relations**. This means that for a \(k\)-order regularity measure, we are interested in tuples of \(k\) entities. Thus, the mapping function \(\) no longer takes single entities as input, but instead operates on \(k\)-tuples:

\[:(_{})^{k}\{\}^{+}. \]

\(\) is a function that describes some relations between the \(k\) input entities by a set of symbols.

For \(k\)-order regularity, the multiset \(\), over which we compute the entropy, is now given by

\[^{(k)}=_{\{i_{1},,i_{k}\}}(s_{,i_{1}},,s_{,i_{k}})\;=(\{1,,N\},k) \]

merged from all \(k\)-permutations of the \(N\) entities, denoted as \((\{1,,N\},k)\). In the case of a permutation invariant \(\), Eq. 4 regards only the combinations \((\{1,,N\},k)\). Note that direct RaIR is equivalent to the relational RaIR of order \(1\). Given the mapping \(\), the RaIR measure is computed as before with Eq. 2.

Depending on the order \(k\) and the function \(\), we can select which regularities are going to be favored. Let us consider the example of a pairwise relational RaIR (\(k=2\)), where \(\) computes the relative positions: \((s_{i},s_{j})=\{ s_{i}-s_{j}\}\), and rounding is performed elementwise. Whenever two entities have the same relative position to each other, the redundancy is detected. For \(k=3\) our regularity measure would be able to pick up sub-patterns composed of three objects, such as triangles and so forth.

As we are interested in physical interactions of the robot with objects and objects with objects, we choose RaIR of order \(k=2\) and explore various \(\) functions.

#### 2.2.1 Properties of RaIR with Pairwise Relations and Practical Implementation

For simplicity, we are considering in the following that \(\) maps to a single symbol. Then for pairwise relationships (\(k=2\)), RaIR can be implemented using a relation matrix \(F^{N N}\). The entries \(F_{ij}\) are given by \((s_{i},s_{j})\) with \(s_{i},s_{j} S_{}\). After constructing the relation matrix, we need the histogram of occurrences of unique values in this matrix to compute the entropy (Eq. 2). For continuous state spaces, the mapping function needs to implement a discretization step, which we implement by a binning of size \(b\). For simplicity of notation, we reuse the rounding notation \(\) for this discretization

Figure 3: Illustration of direct RaIR for \(=\{ x, y\}\).

step. This bin size \(b\) determines the precision of the measured regularity. In practice, we do not apply \(\) on the full entity state space, but on a subspace that contains e.g. the \(x\)-\(y\)(-\(z\)) positions.

To understand the properties of our regularity measure for different \(\), we present in Table 1 a categorization using the known symmetry operations in 2D and the following \(\) (applied to \(x\)-\(y\) positions): direct \((s_{i})= s_{i}\)(see previous section), relative position (difference vector) \((s_{i},s_{j})= s_{i}-s_{j}\), absolute value of the relative position1\((s_{i},s_{j})=|s_{i}-s_{j}||\), and Euclidean distance \((s_{i},s_{j})=\|s_{i}-s_{j}\|\). Figure 1(b) illustrates the RaIR computation using the absolute value of the relative position.

In Table 1, we first consider whether the measure is invariant under symmetry operations. That means if the value of RaIR stays unchanged when the entire configuration is transformed. We find that both Euclidean distance and relative position are invariant to all symmetry operations. The second and possibly more important question is whether a configuration with sub-structures of that symmetry has a higher regularity value than without, i.e. will patterns with these symmetries be favored. We find that Euclidean distance favors all symmetries, followed by absolute value of the relative position. A checkmark in this part of the table means that the more entities can be mapped to each other with the same transformation, the higher RaIR. Although the Euclidean distance seems favorable, we find that it mostly clumps entities and creates fewer alignments. To get a sense of the patterns scoring high in the regularity measure, Fig. 1 showcases situations that emerge when RaIR with absolute value of relative position is optimized (details below).

### Regularity in Free Play

Our goal is to explicitly put the bias of regularity into free play via RaIR, as in Fig. 1(a). What we want to achieve is not just that the agent creates regularity, but that it gathers valuable experience in creating regularity. This ideally leads to directing exploration towards patterns/arrangements that are novel.

We propose to use RaIR to augment plain novelty-seeking intrinsic rewards, in this work specifically ensemble disagreement . We choose ensemble disagreement because 1) we need a reward definition that allows us to predict future novelty, such that we can use it inside model-based planning (this constraint makes methods relying on retrospective novelty such as Intrinsic Curiosity Module (ICM)  ineligible), and 2) we want to use the models learned during free play for zero-shot downstream task generalization via planning in a follow-up extrinsic phase. It has been shown in previous works that guiding exploration by the model's own epistemic uncertainty, approximated

    & &  &  \\ symmetry & direct & rel. pos & \(\) & distance & direct & rel. pos & \(\) & distance \\ operation & \(= s_{i}\) & \( s_{i}-s_{j}\) & \(|s_{i}-s_{j}|\) & d\((s_{i},s_{j})\) & \( s_{i}\) & \( s_{i}-s_{j}\) & \(|s_{i}-s_{j}|\) & d\((s_{i},s_{j})\) & \(==}}{=}\) \\  translation & ✗ & ✓ & ✓ & ✓ & ✗ & ✓ & ✓ & ✓ & ✓ \\ translation – a.a. & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ rotation & ✗ & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ & ✓ & ✓ \\ rotation – \(90^{}\) & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & ✓ & ✓ & ✓ \\ reflection & ✗ & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ & ✓ & ✓ \\ reflection – a.a. & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\  glide refl. & ✗ & ✓ & ✗ & ✓ & ✗ & ✗\({}^{(1)}\) & ✗\({}^{(1)}\) & ✓ & ✓ & ✓ \\ glide refl. – a.a. & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗\({}^{(1)}\) & ✓ & ✓ \\   

* This is for one glide refl. operation. RaIR is increased for 2 glide refl. composition as it collapses onto transl.

Table 1: **Properties of RaIR with different \(\) regarding symmetry operations**. The first block indicates to which operations RaIR is invariant, ignoring rounding (a.a.: axes aligned). The second block assesses whether a pattern, where the given symmetry operation maps several entities to another, has increased regularity. Rounding and absolute value are elementwise. Distance \(d\) is also rounded.

via ensemble disagreement, leads to learning more robust world models compared to e.g. Random Network Distillation (RND)  (Suppl. J.1), resulting in improved zero-shot downstream task performance . That is why we choose ensemble disagreement to compute expected future novelty.

We train an ensemble of world models \(\{(_{ m})_{m=1}^{M}\}\), where \(M\) denotes the ensemble size. The model's epistemic uncertainty is approximated by the disagreement of the ensemble members' predictions. The disagreement reward is given by the trace of the covariance matrix :

\[r_{}=(\{_{t+1}^{m}=_{ m}(s_{t},a_{t}) m=1,,M\}). \]

We incorporate our regularity objective into free play by using a linear combination of RaIR and ensemble disagreement. Overall, we have the intrinsic reward:

\[r_{}=r_{}+ r_{}, \]

where \(\) controls the trade-off between regularity and pure epistemic uncertainty.

Model-based Planning with Structured World ModelsTo optimize the reward function on-the-fly, we use model-based planning using zero-order trajectory optimization, as introduced in Sec. 2.1. Concretely, we use CEE-US , which combines structured world models and epistemic uncertainty (Eq. 5) as intrinsic reward. The structured world models are ensembles of message-passing Graph Neural Networks (GNNs) , where each object corresponds to a node in the graph. The node attributes \(\{s_{t,i}_{} i=1,,N\}\) are the object features such as position, orientation, and velocity at time step \(t\). The state representation of the actuated agent \(s_{}_{}\) similarly contains position and velocity information about the robot. We treat the robot as a global node in the graph . We refer to the combination of RaIR with ensemble disagreement, medium-horizon planning (20-30 time steps), and structured world models as RaIR + CEE-US.

## 3 Experiments

We evaluate RaIR in the two environments shown in Fig. 1.

**ShapeGridWorld** is a grid environment, where each circle represents an entity/agent that is controlled separately in \(x\)-\(y\) directions. Entities are controlled one at a time. Starting from timestep \(t=0\), the entity with \(i=1\) is actuated for \(T\) timesteps, where \(T\) is the entity persistency. Then, at \(t=T\), actuation switches over to entity \(i=2\) and we keep iterating over the entities in this fashion. Each circle is treated as an entity for the regularity computation with a 2D-entity state space \(S_{}\) with \(x\)-\(y\) positions.

**Fetch Pick & Place Construction** is an extension of the Fetch Pick & Place environment  to more cubes  and a large table . An end-effector-controlled robot arm is used to manipulate blocks. The robot state \(S_{}^{10}\) contains the end-effector position and velocity, and the gripper's state (open/close) and velocity. Each object's state \(S_{}^{12}\) is given by its pose and velocities. For free play, we use 6 objects and consider several downstream tasks with varying object numbers.

### Emerging Patterns in ShapeGridWorld and Construction with RaIR

To get a sense of what kinds of patterns emerge following our regularity objective with RaIR, we do planning using ground truth (GT) models, i.e. with access to the true simulator itself for planning. We perform these experiments to showcase that we can indeed get _regular_ constellations with our proposed formulation. Since we can perform multi-horizon planning without any accumulating model errors using ground truth models, we can better investigate the global/local optima of our regularity reward. Note that as we are using a zero-order trajectory optimizer with a limited sample budget and finite-horizon planning, we don't necessarily converge to the global optima. We use \((s_{i},s_{j})=\{(||s_{i,x}-s_{j,x}||,|| s_{i,y}-s_{j,y}||)\}\) for RaIR in both environments. The emerging patterns are shown in Fig. 1.

In the 2D ShapeGridWorld environment, we indeed observe that regular patterns with translational, reflectional (axis-aligned), glide-reflectional (axis-aligned), and rotational symmetries emerge. Regularity is not restricted to spatial relations and can be applied to any set of symbols. To showcase this, we apply RaIR to colored ShapeGridWorld, where color is part of \(S_{}\). Generated patterns are not just regular in \(x\)-\(y\) but also in color, as shown in Fig. S7 and the 2 top right examples in Fig. 1.

For Construction, we also observe complex constellations with regularities, even stacks of all 6 objects. Since we are computing RaIR on the \(x\)-\(y\) positions, a stack of 6 is the global optimum. The optimization of RaIR for this case is shown in Fig. 4. Note that stacking itself is a very challenging task, and was so far only reliably achievable with reward shaping or tailored learning curricula . The fact that these constellations appear naturally from our regularity objective, achievable with a planning horizon of 30 timesteps, is by itself remarkable.

Additional example patterns generated in Construction with RaIR on the \(x\)-\(y\)-\(z\) positions can be found in the Suppl. A. In that case, a horizontal line on the ground and a vertical line into air, i.e. a stack, are numerically equivalent with respect to RaIR. Choosing to operate on the \(x\)-\(y\)-subspace is injecting the direction of gravity and provides a bias towards vertical alignments. We also apply RaIR to a custom Construction environment with different shapes and masses (cubes, columns, balls and flat blocks) and once again observe regular arrangements, as in Fig. S8. More details in Suppl. F.

### Free Play with RaIR in Construction

We perform free play in Construction, i.e. only optimize for intrinsic rewards, where we learn models on-the-go. During free play, we start with randomly initialized models and an empty replay buffer. Each iteration of free play consists of data collection with environment interactions (via online planning), and then model training on the collected data so far (offline).

In each iteration of free play, we collect 2000 samples (20 rollouts with 100 timesteps each) and add them to the replay buffer. During the online planning part for data collection, we only perform inference with the models and no training is performed. Afterwards, we train the model for a fixed number of epochs on the replay buffer. We then continue with data collection in the next free play iteration. More details can be found in Suppl. H. For this intrinsic phase, we combine our regularity objective with ensemble disagreement as per Eq. 6. The goal is to bias exploration and the search for information gain towards regular structures, corresponding to the optima that emerge with ground truth models, as shown in Fig. 1. We also show results for the baselines RND  and Disagreement (Dis) , using the same model-based planning backbone as CEE-US (see Suppl. J). The Dis baseline also uses ensemble disagreement as intrinsic reward, however unlike CEE-US, only plans for one step into the future.

In Figure 5, we analyze the quality of data generated during free play, in terms of observed interactions, for RaIR + CEE-US with the augmentation weight \(=0.1\), a pure RaIR run with no information-gain component in the intrinsic reward (\(=0\)), CEE-US, as well as RND and Disagreement. For pure RaIR, we observe a decrease in the generated interactions. This has two reasons: 1) RaIR only aims to generate structure and the exploration problem is not solved, 2) once the controller finds a plan that leads to an optimum, even if it is local, there is no incentive to destroy

Figure 4: **RaIR throughout a rollout starting from a random initial configuration when optimizing only for regularity with the GT model.**

Figure 5: **Comparison of interactions during free play in Construction when combining ensemble disagreement with RaIR (with \(\) = 0.1) compared to CEE-US, pure RaIR, RND and Dis.** These metrics count the relative amount of timesteps the agent performs certain types of interactions in the 2K transitions collected at each free play iteration. (a) _1 object moves_ checks the amount of time the agent spends moving only one object. (b) _2 or more objects move_ checks if at least 2 objects are moving at the same time. (c) _Object(s) in air_ means one or more objects are in air (including being held in air by the agent or being on top of another block). (d) _Object(s) flipped_ checks for angular velocities above a threshold for one or more objects, i.e. if they are rolled/flipped. We used 5 independent seeds.

it, unless a plan that results in better regularity can be found within the planning horizon. There is no discrimination between "boring" and "interesting" patterns with respect to the model's current capabilities. This in turn means that the robot creates e.g. a (spaced) line, which is a local optimum for RaIR, and then spends the rest of the episode, not touching any objects to keep the created alignment intact. With the injection of some disagreement in RaIR + CEE-US, we observe improved interaction metrics throughout free play in terms of 2 or more object interactions and objects being in the air (either being lifted by the robot or being stacked on top of another block). In practice, since the ensemble of models tends to hallucinate due to imperfect predictions, even for pure RaIR we observe dynamic pattern generations, as reflected in the interaction metrics (more details in Suppl. C). For the plain disagreement case with CEE-US, more flipping behavior, and less air time are observed during free play, since the agent favors chaos. We also observe that the baselines RND and Disagreement (planning horizon 1) produce less interaction-rich data during free play. Especially for Disagreement, this further shows that planning for future novelty is an essential component for free play.

Another reason why disagreement in RaIR + CEE-US is helpful is due to the step-wise landscape of RaIR as shown in Fig. 4. Here, combining RaIR with ensemble disagreement effectively helps smoothen this reward function, making it easier to find plans with improvements in regularity with imperfect world models.

In Fig. 7, we report the highest achieved RaIR value in the collected rollouts throughout free play. We observe that pure RaIR and RaIR + CEE-US indeed find more regular structures during play compared to the baselines. Some snapshots of regular structures observed during a RaIR + CEE-US free play run are illustrated in Fig. 6. Results for \((s_{i},s_{j})= s_{i}-s_{j}\) can be found in Suppl. B.

### Zero-shot Generalization to Assembly Downstream Tasks with RaIR in Construction

After the fully-intrinsic free-play phase, we evaluate zero-shot generalization performance on downstream tasks, where we perform model-based planning with the learned world models. Note that now instead of optimizing for intrinsic rewards, we are optimizing for extrinsic reward functions \(r_{}\) given by the environment (Suppl. I.4.1). In Fig. 8, we present the evolution of success rates of models checkpointed throughout free play on the following assembly tasks: singletower with 3 objects, 2 multitowers with 2 objects each, pyramid with 5 and 6 objects.

The combination RaIR + CEE-US yields significant improvements in the success rates of assembly tasks, as shown in Fig. 8 and Table 2. RaIR alone, outperforms CEE-US in assembly tasks. As we are biasing exploration towards regularity, we see a decrease in more chaotic interactions during play time, which is correlated with a decrease in performance for the more chaotic throwing and flipping tasks. For the generic Pick & Place task, we observe comparable performance between RaIR + CEE-US and CEE-US. The decrease in performance for RaIR in non-assembly tasks shows the importance of an information-gain objective. The baselines RND and Disagreement exhibit very poor performance in the assembly tasks. In the other tasks, RND and CEE-US are comparable (bold numbers show statistical indistinguishability from best with \(p>0.05\)) This showcases that guiding free play by the model's epistemic uncertainty as in the case with ensemble disagreement, helps learn robust and capable world models. The decrease in the zero-shot performance for Disagreement further proves the importance of planning for future novelty during free play. We also run free play in the custom Construction environment for RaIR + CEE-US and CEE-US and observe improved zero-shot downstream performance in assembly tasks with RaIR + CEE-US (see Suppl. F.2).

Figure 6: **Snapshots from free play with RaIR + CEE-US.** We showcase snapshots of highest RaIR values, equivalent to lowest entropy, from exemplary rollouts at different iterations of free play. Following the regularity objective, stacks and alignments are generated.

Figure 7: **Highest RaIR value throughout free play for pure RaIR, RaIR + CEE-US, CEE-US, RND and Dis.**

### Re-creating existing structures with RaIR

We test whether we can re-create existing arrangements in the environment with RaIR. If there are regularities / sub-structures already present in the environment, then completing or re-creating these patterns naturally becomes an optimum for RaIR, as repeating this pattern introduces redundancy. We initialize pyramids, single- and multitowers out of the robot's manipulability range in Construction. We then plan using iCEM to maximize RaIR with GT models. Doing so, the agent manages to re-create the existing structures in the environment with the blocks it has within reach. In Fig. 9, this is showcased for a pyramid with 3 objects, where in 15 rollouts a pyramid is recreated in 73% of the cases. Without the need to define any explicit reward functions, we can simply use our regularity objective to mimic existing ordered constellations. More details can be found in Suppl. D.

## 4 RaIR in Locomotion Environments

The only requirement to incorporate RaIR in a given environment is an entity-based view of the world. We can easily apply this principle to locomotion environments, where we treat each joint as an entity. In Figure 10, we showcase the generated poses in the DeepMind Control Suite environments Quadruped and Walker, when we optimize for regularity using ground truth models. Here, regularity is computed over the \(x\)-\(y\)-\(z\) positions of the knees and toes of the quadruped. For walker, we take the positions of the feet, legs and the torso. These poses also heavily overlap with the goal poses proposed in the RoboYoga benchmark  (see Fig. S11), which further supports our hypothesis that regularities and our preference for them are ubiquitious. We also apply free play with learned models in the Quadruped environment. More details and results can be found in Suppl. G.

   & Singletower 3 & Multilower 2+2 & Pyramid 5 & Pyramid 6 & Pick\&Place 6 & Throw 4 & Flip 4 \\  RaIR + CEE-US & \(\) & \(\) & \(\) & \(\) & \(\) & \(0.32 0.02\) & \(0.63 0.08\) \\ RaIR & \(0.64 0.03\) & \(0.62 0.03\) & \(0.25 0.05\) & \(0.10 0.02\) & \(0.74 0.05\) & \(0.21 0.01\) & \(0.65 0.1\) \\ CEE-US & \(0.40 0.12\) & \(0.52 0.05\) & \(0.14 0.09\) & \(0.02 0.01\) & \(\) & \(\) & \(\) \\ RND & \(0.07 0.07\) & \(0.14 0.12\) & \(0.02 0.02\) & \(0.0 0.0\) & \(\) & \(\) & \(\) \\ Dis & \(0.0 0.0\) & \(0.01 0.01\) & \(0.0 0.0\) & \(0.0 0.0\) & \(\) & \(0.30 0.04\) & \(\) \\  GT & \(0.99\) & \(0.97\) & \(0.82\) & \(0.81\) & \(0.99\) & \(0.97\) & \(1.0\) \\  

Table 2: **Zero-shot downstream task performance of RaIR + CEE-US, RaIR, CEE-US, RND and Dis** for assembly tasks as well as the generic pick & place task and the more chaos-oriented throwing and flipping. Results are shown for five independent seeds. In the bottom row, we report the success rates achieved via planning with ground truth models. This is to provide a baseline for how hard the task is to solve with finite-horizon planning and potentially suboptimally designed task rewards.

Figure 8: **Success rates for zero-shot downstream task generalization for assembly tasks** in Construction for model checkpoints over the course of free play. We compare RaIR + CEE-US (\(=0.1\)) and RaIR with CEE-US, RND and Dis. We used five independent seeds.

Figure 9: A pyramid initialized out of the robot’s reach is re-created by optimizing for RaIR.

## 5 Related Work

**Intrinsic motivation in RL** uses minimizing novelty/surprise to dissolve cognitive disequilibria as a prominent intrinsic reward signal definition [9; 26; 27; 28; 29; 30; 31]. As featured in this work, using the disagreement of an ensemble of world models as an estimate of expected information gain is a widely-used metric as it allows planning into the future [10; 11; 12]. Other prominent intrinsic rewards deployed in RL include learning progress [26; 30; 32], empowerment [33; 34] and maximizing for state space coverage with count-based methods [35; 36] and RND . Another sub-category would be goal-conditioned unsupervised exploration methods combined with e.g. ensemble disagreement [25; 37] or asymmetric self-play . In competence-based intrinsic motivation, unsupervised skill discovery methods aim to learn policies conditioned on a latent skill variable [39; 40]. Berseth et al.  propose surprise minimization as intrinsic reward to seek familiar states in unstable environments with an active source of entropy. Note that this differs from our work, as our notion of regularity is decoupled from surprise: RaIR aims to get to a state that is regular in itself.

**Compression** and more specifically compression progress have been postulated as driving forces in human curiosity by Schmidhuber . However, the focus has been on _temporal_ compression, where it is argued that short and simple explanations of the past make long-horizon planning easier. In our work, we don't focus on compression in the temporal dimension, i.e. sequences of states. Instead, we perform compression as entropy minimization (in the relational case, equivalent to lossy compression) at a given timestep \(t\), where we are interested in the relational redundancies in the current scene. More details on connections to compression can be found in Appendix. L.

**Assembly Tasks in RL** with 3+ objects pose an open challenge, where most methods achieve stacking via tailored learning curricula with more than 20 million environment steps [24; 42], expert demonstrations , also together with high-level actions . Hu et al.  manage to solve 3-object stacking in an unsupervised setting with goal-conditioned RL (GCRL), using a very similar robotic setup to ours, but only with 30% success rate. More discussion on GCRL in Suppl. K.

## 6 Discussion

Although the search for regularity and symmetry has been studied extensively in developmental psychology, these concepts haven't been featured within reinforcement learning yet. In this work, we propose a mathematical formulation of regularity as an intrinsic reward signal and operationalize it within model-based RL. We show that with our formulation of regularity, we indeed manage to create regular and symmetric patterns in a 2D grid environment as well as in a challenging compositional object manipulation environment. We also provide insights into the different components of RaIR and deepen the understanding of the types of regularities emerging from using different mappings \(\). In the second part of the work, we incorporate RaIR within free play. Here, our goal is to bias information-search during exploration towards regularity. We provide a proof-of-concept that augmenting epistemic uncertainty-based intrinsic rewards with RaIR helps exploration for symmetric and ordered arrangements. Finally, we also show that our regularity objective can simply be used to imitate existing regularities in the environment.

**Limitations and future work:** As we use finite-horizon planning, we don't necessarily converge to global optima. This can both be seen as a limitation and a feature, as it naturally allows us to obtain different levels of regularity in the generated patterns. Currently, we are restricted to fully-observable MDPs. We embrace object-centric representations as a suitable inductive bias in RL, where the observations per object (consisting of poses and velocities) are naturally disentangled (more discussion in Appendix. M). We also assume that this state space is interpretable such that we take, for instance, only the positions and color. The representational space, in which the RaIR measure is computed, is specified by the designer. Exciting future work would be to learn a representation under which the human relevant structures in the real-world (e.g. towers, bridges) are indeed regular.

Figure 10: **RaIR in Quadruped and Walker environments with GT models. We show generated poses when maximizing for regularity over the positions of different joints (e.g. knees, toes).**