# Exploring the Promise and Limits of Real-Time Recurrent Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity makes it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10 B frames, while using fewer than 1.2 B environmental frames. To scale to such challenging tasks, we focus on certain well-known neural architectures with element-wise recurrence, allowing for tractable RTRL without approximation. We also discuss rarely addressed limitations of RTRL in real-world applications, such as its complexity in the multi-layer case.1

## 1 Introduction

There are two classic learning algorithms to compute exact gradients for sequence-processing recurrent neural networks (RNNs): real-time recurrent learning (RTRL; [1; 2; 3; 4]) and backpropagation through time (BPTT; [5; 6]) (reviewed in Sec. 2). In practice, BPTT is the only one commonly used today, simply because BPTT is tractable while RTRL is not. In fact, the time and space complexities of RTRL for a fully recurrent NN are quadratic and cubic in the number of hidden units, respectively, which are prohibitive for any RNNs of practical sizes in real applications. Despite such an obvious complexity bottleneck, RTRL has certain attractive conceptual advantages over BPTT. BPTT requires to cache activations for each new element of the sequence processed by the model, for later gradient computation. As the amount of these past activations to be stored grows linearly with the sequence length, practitioners (constrained by the actual memory limit of their hardware) use the so-called _truncated_ BPTT (TBPTT; ) where they specify the maximum number of time steps for this storage, giving up gradient components--and therefore credit assignments--that go beyond this time span. In contrast, RTRL does not require storing past activations, and enables computation of untruncated gradients for sequences of any arbitrary length. In addition, RTRL is an online learning algorithm (more efficient than BPTT to process long sequences in the online scenario) that allows for updating weights immediately after consuming every new input (assuming that the external error feedback to the model output is also available for each input). These attractive advantages of RTRL still actively motivate researchers to work towards practical RTRL (e.g., [8; 9; 10; 11; 12]).

The root of RTRL's high complexities is the computation and storage of the so-called _sensitivity matrix_ whose entries are derivatives of the hidden activations w.r.t. each trainable parameter of the model involved in the recurrence (see Sec. 2). Most recent research on RTRL focuses on introducing _approximation methods_ into the computation and storage of this matrix. For example, Menick et al.  introduce sparsity in both the weights of the RNN and updates of the temporal Jacobian (which is an intermediate matrix needed to compute the sensitivity matrix). Another line of work [8; 9; 10] proposes estimators based on low-rank decompositions of the sensitivity matrix that are less expensive to compute and store than the original one. Silver et al.  explore random projections of the sensitivity. The main research question in these lines of work is naturally focused around the quality of the proposed approximation method. Consequently, the central goal of their experiments is typically to test hyper-parameters and configurational choices that control the approximation quality in diagnostic settings, rather than evaluating the full potential of RTRL in realistic tasks. In the end, we still know very little about the true empirical promise of RTRL. Also, assuming that a solution is found to the complexity bottleneck, what actual applications or algorithms would RTRL unlock? In what scenarios would RTRL be able to replace BPTT in today's deep learning?

Here we propose to study RTRL by looking ahead beyond research on approximations. We explore the full potential of RTRL in the settings where no approximation is needed, while at the same time, not restricting ourselves to toy tasks. For that, we focus on special RNN architectures with element-wise recurrence, that allow for tractable RTRL without any approximation. In fact, the quadratic/cubic complexities of the fully recurrent NNs can be simplified for certain neural architectures. Many well-known RNN architectures, such as Quasi-RNNs  and Simple Recurrent Units , and even certain Linear Transformers [15; 16; 17], belong to this class of models (see Sec. 3.1). Note that the core idea underlying this observation is technically not new: Mozer [18; 19] already explore an RNN architecture with this property in the late 1980s to derive his _focused backpropagation_, and Javed et al. [20; 21] also exploit this in the architectural design of their RNNs (even though the problematic multi-layer case is ignored; we discuss it in Sec. 5). While such special RNNs may suffer from limited computational capabilities on certain tasks (i.e., one can come up with a synthetic/algorithmic task where such models fail; see Appendix B.1), they also often perform on par with fully recurrent NNs on many tasks (at least, this is the case for the tasks we explore in our experiments). For the purpose of this work, the RTRL-tractability property outweighs the potentially limited computational capabilities: these architectures allow us to focus on evaluating RTRL on challenging tasks with a scale that goes beyond the one typically used in prior RTRL work, and to draw conclusions without worrying about the quality of approximation. We study an actor-critic algorithm [22; 23; 24] that combines RTRL and recurrent policy gradients , allowing credit assignments throughout an entire episode in reinforcement learning (RL) with partially observable Markov decision processes (POMDPs; [26; 27]). We test the resulting algorithm, Real-Time Recurrent Actor-Critic method (R2AC), in several subsets of DMLab-30 , ProcGen , and Atari 2600  environments, with a focus on memory tasks but also including reactive ones. In particular, on two memory environments of DMLab-30, our system is competitive with or outperforms the well-known IMPALA  and R2D2  baselines, demonstrating certain practical benefits of RTRL at scale. Finally, working with concrete real-world tasks also sheds lights on further limitations of RTRL that are rarely (if not never) discussed in prior work. These observations are important for future research on practical RTRL. We highlight and discuss these general challenges of RTRL (Sec. 5).

## 2 Background

Here we first review real-time recurrent learning (RTRL; [1; 2; 3; 4]), which is a gradient-based learning algorithm for sequence-processing RNNs--an alternative to the now standard BPTT.

Preliminaries.Let \(t\), \(T\), \(N\), and \(D\) be positive integers. We describe the corresponding learning algorithm for the following standard RNN architecture  that transforms an input \((t)^{D}\) to an output \((t)^{N}\) at every time step \(t\) as

\[(t)=(t)+(t-1);(t)=(( t))\] (1)

where \(^{N D}\) and \(^{N N}\) are trainable parameters, \((t)^{N}\), and \(\) denotes the element-wise sigmoid function (we omit biases). For the derivation, it is convenient to describe each component \(_{k}(t)\) of vector \((t)\) for \(k\{1,...,N\}\),

\[_{k}(t)=_{n=1}^{D}_{k,n}_{n}(t)+_{n=1}^{N}_{k,n} (_{n}(t-1))\] (2)

In addition, we consider some loss function \(^{}(1,T)=_{t=1}^{T}(t)\) computed on an arbitrary sequence of length \(T\) where \((t)\) is the loss at each time step \(t\), which is a function of \((t)\) (we omit writing down explicit dependencies over the model parameters). Importantly, we assume that \((t)\) can be computed _solely_ from \((t)\) at step \(t\) (i.e., \((t)\) has no dependency on any other past activations apart from \((t-1)\) which is needed to compute \((t)\)).

The role of a gradient-based learning algorithm is to efficiently compute the gradients of the loss w.r.t. the trainable parameters of the model, i.e., \(^{}(1,T)}{_{i,j}} \) for all \(i\{1,...,N\}\) and \(j\{1,...,D\}\), and \(^{}(1,T)}{_{i,j}} \) for all \(i,j\{1,...,N\}\). RTRL and BPTT differ in the way to compute these quantities. While we focus on RTRL here, for the sake of completeness, we also provide an analogous derivation for BPTT in Appendix A.3.

Real-Time Recurrent Learning (RTRL).RTRL can be derived by first decomposing the total loss \(^{}(1,T)\) over time, and then summing all derivatives of each loss component \((t)\) w.r.t. intermediate variables \(_{k}(t)\) for all \(k\{1,...,N\}\):

\[^{}(1,T)}{_{i,j}}=_{t=1 }^{T}(t)}{_{i,j}}=_{t=1}^{T}( _{k=1}^{N}(t)}{_{k}(t)} _{k}(t)}{_{i,j}})\] (3)

In fact, unlike BPTT that can only compute the derivative of the total loss \(^{}(1,T)\) efficiently, RTRL is an online algorithm that computes each term \((t)}{_{i,j}}\) through the decomposition above. The first factor \((t)}{_{k}(t)}\) can be straightforwardly computed through standard backpropagation (as stated above, we assume there is no recurrent computation between \((t)\) and \((t)\)). For the second factor \(_{k}(t)}{_{i,j}}\), which is an element of the so-called _sensitivity matrix/tensor_, we can derive a _forward recursion formula_, which can be obtained by directly differentiating Eq. 2:

\[_{k}(t)}{_{i,j}}=_{j}(t)_{k =i}+_{n=1}^{N}_{k,n}^{}(_{n}(t-1))_{n}(t-1)}{_{i,j}}\] (4)

where \(_{k=i}\) denotes the indicator function: \(_{k=i}=1\) if \(k=i\), and \(0\) otherwise, and \(^{}\) denotes the derivative of the sigmoid, i.e, \(^{}(_{n}(t-1))=(_{n}(t-1))(1-(_{n}(t -1)))\). The derivation is similar for \((t)}{_{i,j}}\) where we obtain a recurrent formula to compute \(_{k}(t)}{_{i,j}}\). As this algorithm requires to store \(_{k}(t)}{_{i,j}}\) and \(_{k}(t)}{_{i,j}}\), its space complexity is \(O((D+N)N^{2}) O(N^{3})\). The time complexity to update the sensitivity matrix/tensor via Eq. 4 is \(O(N^{4})\). To be fair with BPTT, it should be noted that \(O(N^{4})\) is the complexity for one update; this means that the time complexity to process a sequence of length \(T\) is \(O(TN^{4})\).

Thanks to the forward recursion, the update frequency of RTRL is flexible: one can opt for the _fully online learning_, where we update the weights using \((t)}{}\) at every time step, or accumulate gradients for several time steps. It should be noted that frequent updates may result in _staleness_ of the sensitivity matrix, as it accumulates updates computed using old weights (Eq. 4).

Note that algorithms similar to RTRL have been derived from several independent authors (see, e.g., , or  for the continuous-time version).

Method

Our main algorithm is an actor-critic method that combines RTRL with recurrent policy gradients, using a special RNN architecture that allows for tractable RTRL. Here we describe its main components: an element-wise LSTM with tractable RTRL (Sec. 3.1), and the actor-critic algorithm that builds upon IMPALA  (Sec. 3.2).

### RTRL for LSTM with Element-wise Recurrence (eLSTM)

The core RNN architecture we use in this work is a variant of long short-term memory (LSTM; ) RNN with _element-wise recurrence_. Let \(\) denote element-wise multiplication. At each time step \(t\), it first transforms an input vector \((t)^{D}\) to a recurrent hidden state \((t)^{N}\) as follows:

\[(t)=((t)+^{f}(t-1)); (t)=((t)+^{z}(t-1))\] (5) \[(t)=(t)(t-1)+(1-(t))(t)\] (6)

where \((t)^{N}\), \((t)^{N}\) are activations, \(^{N D}\) and \(^{N D}\) are trainable weight matrices, and \(^{f}^{N}\) and \(^{z}^{N}\) are trainable weight vectors. These operations are followed by a gated feedforward NN to obtain an output \((t)^{N}\) as follows:

\[(t)=((t)+^{o}(t));(t)=(t) (t)\] (7)

where \(^{N D}\) and \(^{o}^{N N}\) are trainable weight matrices. This architecture can be seen as an extension of Quasi-RNN  with element-wise recurrence in the gates, or Simple Recurrent Units  without depth gating, and also relates to IndRNN . While one could further discuss myriads of architectural details , most of them are irrelevant to our discussion on the complexity reduction in RTRL; the only essential property here is that "recurrence" is element-wise. We use this simple architecture above, an LSTM with element-wise recurrence (or eLSTM), for all our experiments.

Furthermore, we restrict ourselves to the one-layer case (we discuss the multi-layer case later in Sec. 5), where we assume that there is no recurrence after this layer. Based on this assumption, gradients for the parameters \(\) and \(^{o}\) in Eq. 7 can be computed by the standard backpropagation, as they are not involved in recurrence. Hence, the sensitivity matrices we need for RTRL (Sec. 2) are: \((t)}{}\), \((t)}{}^{N N N}\), and \((t)}{^{f}}\), \((t)}{^{z}}^{N N}\). Through trivial derivations, we can show that each of these sensitivity matrices can be computed using a tractable forward recursion formula (we provide the full derivation in Appendix A.1). For example for \((t)}{}\), we have, for \(i,j,k\{1,...,N\}\),

\[}_{i}(t) =(_{i}(t-1)-_{i}(t))_{i}(t)(1-_{i}(t))\] (8) \[_{i}(t)}{_{i,j}} =(_{i}(t)+_{i}^{f}}_{i}(t))_{i}(t-1)}{_{i,j}}+}_{i}(t)_{j}(t)\,;\; _{k}(t)}{_{i,j}}=0 k i.\] (9)

where we introduce an intermediate vector \(}(t)^{N}\) with components \(}_{i}(t)\) for convenience. Consequently, the gradients for the weights can be computed as:

\[(t)}{_{i,j}}=_{k=1}^{N}(t)}{_{k}(t)}_{k}(t )}{_{i,j}}=(t)}{_{i}(t)} _{i}(t)}{_{i,j}}\] (10)

**Finally**, we can compactly summarise these equations using the standard matrix operations. By introducing notations \(}(t)^{N N}\) with \(}_{i,j}(t)=_{i}(t)}{_{i,j}} \), and \((t)^{N}\) with \(_{i}(t)=(t)}{_{i}(t)} \) for \(i\{1,...,N\}\) and \(j\{1,...,D\}\), Eqs. 8-10 above can be written as:

\[}(t) =((t-1)-(t))(t)(1-(t))\] (11) \[}(t) =((t)+^{f}} (t))}(t-1)+}(t)(t);(t)}{}=((t))}(t)\] (12)where, for notational convenience, we introduce a function \(:^{N}^{N N}\) that constructs a diagonal matrix whose diagonal elements are those of the input vector; however, in practical implementations (e.g., in PyTorch), this can be directly handled as vector-matrix multiplications with broadcasting (this is an important note for complexity analysis). \(\) denotes outer-product.

Analogously, we can derive compact update equations of sensitivity matrices and gradient computations for other parameters \(\), \(^{J}\) and \(^{z}\) (as well as biases which are omitted here). The complete list of these equations is provided in Appendix A.1.

The RTRL algorithm above requires maintaining sensitivity matrices \(}(t)^{N N}\), and analogously defined \(}(t)^{N N}\), \(}^{J}(t)^{N}\), and \(}^{z}(t)^{N}\) (see Appendix A.1); thus, the space complexity is \(O(N^{2})\). The per-step time complexity is \(O(N^{2})\) (see Eqs. 8-10). This is all tractable. Importantly, these equations 11-12 can be implemented as simple PyTorch code (just like the forward pass of the same model; Eqs. 5-7) without any non-standard logics. Note that many approximations of RTRL often involve computations that are not well supported yet in the standard deep learning library (e.g., efficiently handling custom sparsity), which is an extra barrier for scaling RTRL in practice.

Note that the derivation of RTRL for element-wise recurrent nets is not novel: similar methods can be found in Mozer [18; 19] from the late 1980s. This result itself is also not very surprising, since element-wise recurrence introduces obvious sparsity in the temporal Jacobian (which is part of the second term in Eq. 4). Nethertheless, we are not aware of any prior work pointing out that several modern RNN architectures such Quasi-RNN  or Simple Recurrent Units  yield tractable RTRL (in the one-layer case). Also, while this is not the focus of our experiments, we show an example of Linear Transformers/Fast Weight Programmers [15; 16; 17] that have tractable RTRL (details can be found in Appendix A.2), which is another conceptually interesting result. We also note that the famous LSTM-algorithm  (companion learning algorithm for the LSTM architecture) is a diagonal approximation of RTRL, so is the more recent SnAp-1 of Menick et al. . Unlike in these works, the gradients computed by our RTRL algorithm above are _exact_ for our eLSTM architecture. This allows us to draw conclusions from experimental results without worrying about the potential influence of approximation quality. We can evaluate the full potential of RTRL for this specific architecture.

Finally, this is also an interesting system from the biological standpoint. Each weight in the weight matrix/synaptic connections (e.g., \(^{N N}\)) is augmented with the corresponding "memory" (\(}(t)^{N N}\)) tied to its own learning process, which is updated in an online fashion, as the model observes more and more examples, through an Hebbian/outer product-based update rule (Eq. 12/Left).

### Real-Time Recurrent Actor-Critic Policy Gradient Algorithm (R2AC)

The main algorithm we study in this work, Real-Time Recurrent Actor-Critic method (R2AC), combines RTRL with recurrent policy gradients. Our algorithm builds upon IMPALA . Essentially, we replace the RNN architecture and its learning algorithm, LSTM/TBPTT in the standard recurrent IMPALA algorithm, by our eLSTM/RTRL (Sec. 3.1). While we refer to the original paper  for basic details of IMPALA, here we recapitulate some crucial aspects. Let \(M\) denote a positive integer. IMPALA is a distributed actor-critic algorithm where each _actor_ interacts with the environment for a fixed number of steps \(M\) to obtain a state-action-reward trajectory segment of length \(M\) to be used by the _learner_ to update the model parameters. \(M\) is an important hyper-parameter that is used to specify the number of steps \(M\) for \(M\)-step TD learning  of the critic, and the frequency of weight updates. Given the same number of environmental steps used for training, systems trained with a smaller \(M\) apply more weight updates than those trained with a higher \(M\). For recurrent policies trained with TBPTT, \(M\) also represents the BPTT span (i.e., BPTT is carried out on the \(M\)-length trajectory segment; no gradient is propagated farther than \(M\) steps back in time; while the last state of the previous segment is used as the initial state of the new segment in the forward pass). In the case of RTRL, there is no gradient truncation, but since \(M\) controls the update frequency, the greater the \(M\), the less frequently we update the parameters, and it potentially suffers less from sensitivity matrix staling. This setting allows for comparing TBPTT and RTRL in the setting where everything is equal (including the number of updates) except the actual gradients applied to the weights: truncated vs. untruncated ones.

Note that for R2AC with \(M=1\), one could obtain a _fully online_ recurrent actor-critic method. However, in practice, it is known that \(M>1\) is crucial (for TD learning of the critic) for optimal performance. In all our experiments, we have \(M>1\). The main focus of this work is to evaluate learning with untruncated gradients, rather than the potential for online learning.

Experiments

### Diagnostic Task

Since the main focus of this work is to evaluate RTRL-based algorithms beyond diagnostic tasks, we only conduct brief experiments on a classic diagnostic task used in recent RTRL research work focused on approximation methods [8; 9; 10; 11; 12]: the copy task. Since our RTRL algorithm (Sec. 3.1) requires no approximation, and the task is trivial, we achieve 100% accuracy provided that the RNN size is large enough and that training hyper-parameters are properly chosen. We confirm this for sequences with lengths of up to \(1000\). Additional experimental details can be found in Appendix B.1.

### Memory Tasks

Here we present the main experiments of this work: RL in POMDPs using realistic game environments requiring memory.

DMLab Memory Tasks.DMLab-30  is a collection of 30 first-person 3D game environments, with a mix of both memory and reactive tasks. Here we focus on two well-known environments, rooms_select_nonmatching_object and rooms_watermaze, which are both categorised as "memory" tasks according to Parisotto et al. . The mean episode lengths of these tasks are about 100 and 1000 steps, respectively. As we apply an action repetition of 4, each "step" corresponds to 4 environmental frames here. We refer to Appendix B.2 for further descriptions of these tasks, and experimental details. Our model architecture is based on that of IMPALA . Both RTRL and TBPTT systems use our eLSTM (Sec. 3.1) as the recurrent layer with a hidden state size of 512. Everything is equal between these two systems except that the gradients are truncated in TBPTT but not in RTRL. To reduce the overall compute needed for the experiments, we first pre-train one TBPTT model for 50 M steps for rooms_select_nonmatching_object, and for 200 M steps for rooms_watermaze. Then, for all main training runs in this experiment, we initialise the parameters of the convolutional vision module from the same pre-trained model, and keep these parameters frozen (and thus, only train the recurrent layer and everything above it). For these main training runs, we train for 30 M and 100 M steps for rooms_select_nonmatching_object and rooms_watermaze, respectively; resulting in the total of 320 M and 1.2 B environmental frames. We compare RTRL and TBPTT for different values of \(M\{10,50,100\}\) (Sec. 3.2). We recall that \(M\) influences: the frequency of weight updates, \(M\)-step TD learning, as well as the backpropagation span for TBPTT.

Table 1 shows the corresponding scores, and the left part of Figure 1 shows the training curves. We observe that for select_nonmatching_object which has a short mean episode length of 100 steps, the performance of TBPTT and RTRL is similar even with \(M=50\). The benefit of RTRL is only visible in the case with \(M=10\). In contrast, for the more challenging rooms_watermaze task with a mean episode length of 1000 steps, RTRL outperforms TBPTT for all values of \(M\{10,50,100\}\). Furthermore, with \(M=50\) or \(100\), our RTRL system outperforms the IMPALA and R2D2 systems from prior work , while trained on fewer than 1.2 B frames. Note that R2D2 systems  are trained without action repetitions, and with a BPTT span of 80. This effectively demonstrates the practical benefit of RTRL in a realistic task requiring long-span credit assignments.

ProcGen.We test R2AC in another domain: ProcGen . Most ProcGen environments are solvable using a feedforward policy even without frame-stacking . There is a so-called _memory-mode_ for certain games, making the task partially observable by making the world bigger, and restricting agents' observations to a limited area around them. However, in our preliminary experiments, we observe that even in these POMDP settings, both the feedforward and LSTM baselines perform similarly (see Appendix B.3). Nevertheless, we find one environment in the standard _hard-mode_, _Chaser_, which shows clear benefits of recurrent policies over those without memory. _Chaser_ is similar to the classic game "Pacman," effectively requiring some counting capabilities to fully exploit _power pellets_ valid for a limited time span. The mean episode length for this task is about 200 steps, where each step is an environmental frame as we apply no action repeat for ProcGen. Unlike in the DMLab experiments above, here we train all models from scratch for 200 M steps without pre-training the vision module (since training the vision parameters using RTRL is intractable, they are trained with truncated gradients, i.e., only the recurrent layer is trained using RTRL; we discuss this further in Sec. 5). We compare RTRL and TBPTT with \(M=5\) or \(50\). The training curves are shown in the right part of Figure 1.

Similar to the rooms_select_nonmatching_object case above, with a sufficiently large \(M=50\), there is no difference between RTRL and TBPTT, while we observe benefits of RTRL when \(M=5\).

### General Evaluation

Here we evaluate R2AC more broadly, including environments which are mostly reactive.

Atari.Apart from some exceptions (such as _Solaris_), many of the Atari game environments are considered to be fully observable when observations consist of a stack of 4 frames . However, it is also empirically known that, for certain games, recurrent policies yield higher performance than the feedforward ones having only access to 4 past frames (see, e.g., ). Here our general goal is to compare RTRL to TBPTT more broadly. We use five Atari environments: _Breakout_,

    & frames & \(M\) & select\_nonmatching\_object & watermaze \\  IMPALA () & 1 B & 100 & 7.3 & 26.9 \\ IMPALA () & 10 B & 100 & 39.0 & 47.0 \\ R2D2 () & 10 B & - & 2.3 & 45.9 \\ R2D2+ () & 10 B & - & 63.6 & 49.0 \\  TBPTT & } &  &  & 54.5 \(\) 1.1 & 15.8 \(\) 0.9 \\ RTRL & & & & **61.8**\(\) 0.5 & **40.2**\(\) 5.6 \\  TBPTT & } &  & 61.4 \(\) 0.5 & 44.5 \(\) 1.5 \\ RTRL & & & & **62.0**\(\) 0.4 & **52.3**\(\) 1.9 \\  TBPTT & } &  & 61.7 \(\) 0.1 & 45.6 \(\) 4.7 \\ RTRL & & & & **62.2**\(\) 0.3 & **54.8**\(\) 4.3 \\   

Table 1: Final game scores on two memory environments of **DMLab-30**: rooms_select_nonmatching_object and rooms_watermaze. Numbers on the top part are copied from the respective papers for reference. We report mean and standard deviation computed over 3 training seeds (each using 3 sets of 100 test episodes; see Appendix B.2). “frames” indicates the number of environmental frames used for training. \(M\) is the hyper-parameter that controls weight update frequency, \(M\)-step TD learning, and backpropagation span for TBPTT in IMPALA (see Sec. 3.2).

Figure 1: Training curves on **DMLab-30** rooms_select_nonmatching_object (Non-matching) and rooms_watermaze (Watermaze), and **Progen**_Chaser_ environments.

_Gravitar_, _MsPacman_, _Q*bert_, and _Seaquest_, following Kapturowski et al. 's selection for ablations of their R2D2. Here we use \(M=50\), and train for 200 M steps (with the action repeat of 4) from scratch. The learning curves are shown in Figure 2. With the exception of _MsPacman_ (note that, unlike ProcGen/_Chaser_ above, 4-frame stacking is used) where we observe a slight performance degradation, RTRL performs equally well or better than TBPTT in all other environments.

**DMLab Reactive Task.** Finally, we also test our system on one environment of DMLab-30, room_keys_doors_puzzle, which is categorised as a reactive task according to Parisotto et al. . We train with \(M=100\) for 100 M steps (with the action repeat of 4). The mean episode length is about 450 steps. Table 2/right shows the scores. Effectively, all feedforward, TBPTT, and RTRL systems perform nearly the same (at least within 100 M steps/400 M frames). We note that these scores are comparable to the one reported by the original IMPALA  which is \(28.0\) after training on 1 B frames, which is much worse than the score reported by Kapturowski et al.  for IMPALA trained using 10 B frames (\(54.6\)). We show this example to confirm that RTRL is effectively not helpful on a reactive task, unlike in the memory tasks above.

## 5 Limitations and Discussion

Here we discuss limitations of this work, which also sheds light on more general challenges of RTRL.

Multi-layer case of our RTRL.The most crucial limitation of our tractable-RTRL algorithm for element-wise recurrent nets (Sec. 3.1) is its restriction to the one-layer case. By stacking two such layers, the corresponding RTRL algorithm becomes intractable as we end up with the same complexity bottleneck as in fully recurrent networks. This is simply because by composing two such element-wise recurrent layers, we obtain a fully recurrent NN as a whole. This can be easily seen by writing down the following equations. By introducing extra superscripts to denote the layer number, in a stack of two element-wise LSTM layers of Eqs. 5-6 (we remove the output gate), we can express

    & Breakout & Gravitar & MsPacman & Q*bert & Seaquest & keys\_doors \\  Feedforward & 234 \(\) 12 & 1084 \(\) 54 & 3020 \(\) 305 & 7746 \(\) 1356 & 4640 \(\) 3998 & **26.6**\(\) 1.1 \\ TBPTT & **305**\(\) 29 & 1269 \(\) 11 & **3953**\(\) 497 & 11298 \(\) 615 & 12401 \(\) 1694 & 26.1 \(\) 0.4 \\ RTRL & 275 \(\) 53 & **1670**\(\) 358 & 3346 \(\) 442 & **12484**\(\) 1524 & **12862**\(\) 961 & 26.1 \(\) 0.9 \\   

Table 2: Scores on Atari and DMLab-reactive (rooms_keys_doors_puzzle) environments.

Figure 2: Learning curves on five **Atari** environments

the recurrent state \(^{(2)}(t)\) of the second layer at step \(t\) as a function of the recurrent state \(^{(1)}(t-1)\) of the first layer from the previous step as follows:

\[^{(2)}(t) =^{(2)}(t)^{(2)}(t-1)+(1-^{(2)}(t)) {z}^{(2)}(t)\] (13) \[^{(2)}(t) =(^{(2)}^{(1)}(t)+^{f(2)}^{(2 )}(t-1))\] (14) \[=(^{(2)}(^{(1)}(t)^{(1)}(t-1)+( 1-^{(1)}(t))^{(1)}(t))+...)\] (15) \[=(^{(2)}^{(1)}(t)^{(1)}(t-1)+^ {(2)}(1-^{(1)}(t))^{(1)}(t)+...)\] (16)

By looking at the first term of Eq. 13 and that of Eq. 16, one can see that there is full recurrence between \(^{(2)}(t)\) and \(^{(1)}(t-1)\) via \(^{(2)}\), which brings back the quadratic/cubic time and space complexity for the sensitivity of the recurrent state in the second layer w.r.t. parameters of the first layers. This limitation is not discussed in prior work [18; 19].

Complexity of multi-layer RTRL in general.Generally speaking, RTRL for the multi-layer case is rarely discussed (except Meert and Ludik ; 1997). This case is important in modern deep learning where stacking multiple layers is a standard. There are two important remarks to be made here.

First of all, even in an NN with a single RNN layer, if there is a layer with trainable parameters whose output is connected to the input of the RNN layer, a sensitivity matrix needs to be computed and stored for each of these parameters. A good illustration is the policy net used in all our RL experiments where our eLSTM layer takes the output of a deep (feedforward) convolutional net (the vision stem) as input. As training this vision stem using RTRL requires dealing with the corresponding sensitivity matrix, which is intractable, we train/pretrain the vision stem using TBPTT (Sec. 4.2;4.3). This is an important remark for RTRL research in general. For example, approximation methods proposed for the single-layer case may not scale to the multi-layer case; e.g., to exploit sparsity in the policy net above, it is not enough to assume weight sparsity in the RNN layer, but also in the vision stem.

Second, the multi-layer case  introduces more complexity growth to RTRL than to BPTT. Let \(L\) denote the number of layers. We seemlessly use BPTT with deep NNs, as its time and space complexity is linear in \(L\). This is not the case for RTRL. With RTRL, for each recurrent layer, we need to store sensitivities of all parameters of all preceding layers. This implies that, for an \(L\)-layer RNN, parameters in the first layer require \(L\) sensitivity matrices, \(L-1\) for the second layer,..., etc., resulting in \(L+(L-1)+(L-2)+...+2+1=L(L+1)/2\) sensitivity matrices to be computed and stored. Given that multi-layer NNs are crucial today, this remains a big challenge for practical RTRL research.

Principled vs. practical solution.Another important aspect of RTRL research is that many realistic memory tasks have actual dependencies/credit assignment paths that are shorter than the maximum BPTT span we can afford in practice. In our experiments, with the exception of DMLab rooms_watermaze (Sec. 4.2), no task actually absolutely _requires_ RTRL in practice; TBPTT with a large span suffices. Future improvements of the hardware may give a further advantage to TBPTT; the _practical_ (simple) solution offered by TBPTT might be prioritised over the _principled_ (complex) RTRL solution for dealing with long-span credit assignments. This is also somewhat reminiscent of the Transformer vs. RNN discussion regarding sequence processing with limited vs. unlimited context.

Sequence-level parallelism.While our study focuses on evaluation of untruncated gradients, another potential benefit of RTRL is online learning. For most standard self/supervised sequence-processing tasks such as language modelling, however, modern implementations are optimised to exploit access to the "full" sequence, and to leverage parallel computation across the time axis (at least for training). While some hybrid RTRL-BPTT approaches  may still be able to exploit such a parallelism, fast online learning remains open engineering challenge even with tractable RTRL.

## 6 Conclusion

We demonstrate the empirical promise of RTRL in realistic settings. By focusing on RNNs with element-wise recurrence, we obtain tractable RTRL without approximation. We evaluate our reinforcement learning RTRL-based actor-critic in several popular game environments. In one of the challenging DMLab-30 memory environments, our system outperforms the well-known IMPALA and R2D2 baselines which use many more environmental steps. We also highlight general important limitations and further challenges of RTRL rarely discussed in prior work.