# Query-based Temporal Fusion with Explicit Motion

for 3D Object Detection

Jinghua Hou\({}^{1}\)

Equal contribution.

Zhe Liu\({}^{1}\)

Equal contribution.

Dingkang Liang\({}^{1}\)

Equal contribution.

Zhikang Zou\({}^{2}\)

Equal contribution.

Xiaoqing Ye\({}^{2}\)

Corresponding author.

Xiang Bai\({}^{}\)

Equal contribution.

###### Abstract

Effectively utilizing temporal information to improve 3D detection performance is vital for autonomous driving vehicles. Existing methods either conduct temporal fusion based on the dense BEV features or sparse 3D proposal features. However, the former does not pay more attention to foreground objects, leading to more computation costs and sub-optimal performance. The latter implements time-consuming operations to generate sparse 3D proposal features, and the performance is limited by the quality of 3D proposals. In this paper, we propose a simple and effective **Q**uery-based **T**emporal **F**usion **N**etwork (QTNet). The main idea is to exploit the object queries in previous frames to enhance the representation of current object queries by the proposed **M**otion-guided **T**emporal **M**odeling (MTM) module, which utilizes the spatial position information of object queries along the temporal dimension to construct their relevance between adjacent frames reliably. Experimental results show our proposed QTNet outperforms BEV-based or proposal-based manners on the nuScenes dataset. Besides, the MTM is a plug-and-play module, which can be integrated into some advanced LiDAR-only or multi-modality 3D detectors and even brings new SOTA performance with negligible computation cost and latency on the nuScenes dataset. These experiments powerfully illustrate the superiority and generalization of our method. The code is available at https://github.com/AlmoonYsl/QTNet.

## 1 Introduction

3D object detection is the fundamental task of autonomous driving, which consumes data from sensors (e.g., LiDAR, Camera) to localize and recognize objects in the 3D world. Recent studies have explored utilizing temporal information to improve the detection performance. According to different manners of temporal fusion, we divide them into two categories, including BEV-based (Figure 1(a)) and proposal-based (Figure 1(b)).

BEV-based methods  conduct temporal fusion on the dense bird's-eye-view (BEV) feature by applying an affine transformation to align BEV features in previous frames to enhance the current BEV features. However, not all pixel-wise BEV features are beneficial for boosting the detection performance of foreground objects. To alleviate this problem, the proposal-based approaches  usually adopt a region proposal network (RPN) to obtain many 3D proposalsand then achieve temporal fusion by focusing on foreground object features extracted by the time-consuming 3D RoI-like operations [37; 36; 9]. However, the effectiveness of temporal fusion is bounded by the quality of the generated 3D proposals.

We argue that an effective and lightweight temporal fusion module is more meaningful to autonomous driving technology with high real-time and detection accuracy requirements. Towards this goal, we rethink the format of feature representation and the temporal modeling paradigm in temporal fusion for 3D object detection task, and propose a new query-based temporal fusion method called QTNet. Different from the BEV-based and proposal-based feature representation in Figure 1(a) and Figure 1(b), we adopt a query-based mechanism shown in Figure 1(c) that possesses two merits. On the one hand, the query-based representation is sparse and can effectively aggregate the foreground object information by attention operation . On the other hand, the query-based format is more efficient due to getting rid of complex 3D RoI operations and is less sensitive to 3D object of size and orientation than proposal-based representation.

However, the temporal modeling based on query representation is non-trivial to work well. A natural manner is to compute the feature similarity between the queries of the current and previous frame by cross attention operation. However, the feature measurement is less reliable since 3D objects with similar geometric structures between adjacent frames are difficult to distinguish, leading to failed matching. Fortunately, we observe that 3D objects usually follow the physical law of motion in the real-world 3D space, which means that the same 3D object between adjacent frames does not shift too much in a short time.

Based on the above observations, we further adopt a Motion-guided Temporal Modeling (MTM) module based on the sparse query representation. Specifically, we first align objects from previous frame to current frame and then generate the attention map by explicit position information of objects. Finally, we aggregate the temporal features by the attention map to enhance the current query features for the subsequent refinement and produce the final detection results.

Figure 1: **Temporal fusion architecture comparison.****(a)** BEV-based: conduct temporal fusion on the dense BEV features. **(b)** Proposal-based: first extract proposal by 3D RoI pooling operation and then implement temporal fusion on the proposal-level features. **(c)** Query-based (Ours): directly adopt the query-level features to achieve temporal fusion. **(d)** We compare the performance, computational cost, and latency of different paradigms on the same baseline (TransFusion-L ). Computation cost and latency are tested on a single NVIDIA RTX 4090 GPU with 1 batch size. For BEV-based and proposal-based paradigm, we reproduce the representative long-term fusion of MGTANet  and MPPNet  on the baseline, respectively.

Besides, as shown in Figure 1(d), we compare our query-based paradigm with the BEV-based and the proposal-based paradigm in terms of FLOPs, latency, and performance. We can observe that our QTNet possesses distinct advantages in performance, computation cost, and latency. Overall, our contributions are summarized as follows: **(i)** We propose a new temporal fusion method called QTNet for 3D object detection based on sparse query-based feature representation, which is more effective and efficient than BEV-based and proposal-based manners. **(ii)** We propose the MTM, which can be plugged into LiDAR-only or multi-modality 3D detectors and boost their performance with negligible computation cost and latency.

## 2 Related work

3D object detectionThe current 3D object detection detectors can be roughly categorized into three categories: LiDAR-based, camera-based, and multi-modality. For LiDAR-based, some methods [37; 56; 54; 52] directly consume point clouds and predict the 3d detection results. Other methods [17; 27; 36; 50; 2; 18] first quantify point clouds into regular grid structures (e.g., voxels, pillars) and then utilize a 3D backbone to extract features. These methods map 3D grids to bird-eye view (BEV) representation by height compression and utilize 2D CNN to extract features further for 3D object detection with efficiency. For camera-based, some methods [21; 12; 20; 19] lift the image features into 3D space and then generate BEV features. Other methods [23; 42] follow the DETR  paradigm, and they utilize queries to interact with the multi-view image features and predict the 3D detection results. LiDAR-based methods have precise geometric information with high performance but lack dense texture information. In contrast, camera-based methods have rich texture information but lack explicit 3D geometric information. Obviously, LiDAR and camera information are complementary. Therefore, multi-modality methods [14; 24; 51; 8; 2; 28; 22; 47; 15; 26; 25] utilize multi-modal fusion to improve detection performance.

3D multi-object trackingDue to the lack of rich texture information for point clouds, LiDAR-based tracking methods [41; 50; 31; 33] usually utilize the motion of objects for the association. For example, CenterPoint  estimates the velocity by an additional regression head and utilizes L2 distance to associate objects in the previous and current frame. SimpleTrack  explores settings of tracking on different datasets through a series of experiments. With the development of camera-based detectors, some methods [53; 32] have begun to explore conducting 3D multi-object tracking on the camera domain since there is rich appearance information in images. MUTR3D  uses queries to represent objects and achieves tracking by attention . PF-Track  utilizes a temporal transformer to conduct temporal fusion and integrates past and future reasoning for tracked objects.

3D object detection with temporal fusionIn the real world, temporal information is beneficial to perceive the localization and direction of movement for 3D objects. Towards this goal, some researchers exploit temporal information to boost the performance of 3D detection task. There are two sets of mainstream, including BEV-based [49; 21; 16; 13] and proposal-based [46; 34; 5] temporal fusion. For BEV-based paradigm, the typical approach MGTANet  adopts an SM-VFE module to encode LiDAR point features and then a motion-guided deformable module to align and aggregate multi-frame BEV features for temporal fusion. However, this BEV-based paradigm takes both the foreground and background into consideration, leading to additional computation costs and sub-optimal performance. For proposal-based paradigm, MPPNet  utilizes RoI grid pooling  to extract proposal features and generate trajectories for temporal fusion. However, this proposal-based paradigm usually adopts a multi-stage pipeline and some time-consuming 3D RoI pooling operation, which makes the whole pipeline complex and brings unacceptable runtime. In summary, the above methods have the disadvantage of sub-optimal performance or high latency and computation cost. To address these problems, in this paper, we propose an efficient temporal fusion method QTNet based on query representation, which achieves better performance and notably reduces the computation cost and latency.

## 3 Method

In this section, we introduce the proposed **Q**uery-based **T**emporal Fusion **N**etwork (QTNet), which is presented in Figure 2. QTNet consists of a DETR-like 3D detector, a memory bank, and our proposed Motion-guided Temporal Modeling (MTM) module. In contrast to the prior arts, includingBEV-based [16; 5; 49; 57] and proposal-based [34; 5], we conduct the temporal fusion on sparse query representation. Specifically, we first feed LiDAR point clouds to DETR-like 3D detectors [2; 47; 15] and generate sparse query features. Note that our method can also support multi-modal data as input (e.g., images and LiDAR point clouds). Thus, the input of images is optional. Then, we reserve the historical sparse query features by a memory bank to reduce repeated computation costs. Next, we take the query features of the previous frame and current frame into our MTM module and produce the enhanced query features for the final 3D detection. Next, we will present the details of the main components in QTNet.

### Memory Bank

Different from the existing temporal fusion methods [16; 49; 57] that require much repeated computation for the feature extraction from the previous frames, we utilize a memory bank to preserve the historical information for the subsequent temporal fusion. Here, we define a memory bank \(S\) with the size of \(N K\), where \(N\) means the number of used historical frames and \(K\) is the number of queries. Our memory bank follows the first-in and first-out (FIFO) rule to achieve query feature updates. In more detail, we first conduct temporal fusion in the \(t\) frame and reserve \(N\) historical frames. Then, we fetch queries \([Q_{t-1},Q_{t-2},...Q_{t-N}]\) in historical frames. Next, we can fuse these historical queries with the current queries \(Q_{t}\) so as to obtain the enhanced current queries \(Q_{t}^{{}^{}}\). Finally, we push current queries \(Q_{t}\) into \(S\) and pop the oldest queries \(Q_{t-N}\).

### Motion-guided Temporal Modeling Module

The Motion-guided Temporal Modeling (MTM) module is applied to enhance the current query features by combining the feature information from the history frames. Specifically, we first obtain the current object queries \(Q_{t}\) and their corresponding positions \(C_{t}\) (can also be regarded as the center of 3D objects) from the DETR-like 3D detectors. In this paper, we adopt Transfusion-L  as the default 3D detector. Then, we feed current queries \(Q_{t}\), previous fused queries \(Q_{t-1}^{{}^{}}\), and their corresponding positions \(C_{t}\) and \(C_{t-1}\) to MTM for generating current fused queries \(Q_{t}^{{}^{}}\). In more detail, we present the structure of MTM in Figure 3, which includes three steps: the position alignment of objects between the previous and the current frames, the attention map generation, and the feature aggregation. For the process of this alignment, we first align objects from previous frame to current frame by the motion of objects and the ego vehicle. Given the ego pose matrix \(R_{w}^{t-1}\) and \(R_{w}^{t}\) in \(t-1\)

Figure 2: **The overall architecture of the proposed QTNet. We first feed sensor data to the DETR-like 3D detector and generate current frame queries. We utilize a memory bank to store historical queries for query-based temporal fusion, which follows the FIFO rule. In the temporal fusion, we fetch these historical queries from the memory bank and then feed them to our motion-guided temporal modeling module to fuse queries in different frames. Finally, the fused queries are utilized to refine current detection results.**

frame and \(t\) frame, we transform objects from the world coordinate to the ego coordinate. Then, the aligned transformation matrix \(R_{t-1}^{t}\), which transforms objects from \(t-1\) frame to \(t\) frame, can be calculated as:

\[R_{t-1}^{t}=R_{w}^{t}(R_{w}^{t-1}),\] (1)

where the \(\) denotes the matrix inversion operation. Then, we can align the center of objects in \(t-1\) frame to \(t\) frame by their velocity \(V_{t-1}\) and the aligned transformation matrix \(R_{t-1}^{t}\) with the related time interval \( t\) between \(t\) and \(t-1\) frame:

\[C_{t-1}^{{}^{}}=(C_{t-1}+V_{t-1} t)(R_{t-1}^{t})^{T}\] (2)

For the attention map generation, we first define the number of \(Q_{t}\) is \(N_{t}\) and \(Q_{t-1}\) is \(N_{t-1}\). Then, we utilize the L2 center distance of objects between adjacent frames to build the cost matrix \(O_{t-1}^{t}^{N_{t} N_{t-1}}\):

\[O_{t-1}^{t}=_{2}(C_{t}-C_{t-1}^{{}^{}}),\] (3)

where the \(_{2}\) denotes the euclidean distance. To avoid the occurrence of impossible matching situations for better temporal modeling, we introduce the L2 velocity error distribution of each category in \(0.5\) seconds as our distance cost threshold \(\), which is similar to CenterPoint . Besides, these associated objects with different categories are masked. We formulate the above process as follows:

\[M=0, O_{t-1}^{t}\  and s_{t}=s_{t-1}\\ 1e^{8}, O_{t-1}^{t}\ > s_{t} s_{t-1},\] (4)

where \(s_{t}\) and \(s_{t-1}\) denote the category of objects in \(t\) and \(t-1\) frames, respectively. After adopting the attention mask \(M\) with the row-wise softmax function on the cost matrix, we generate the attention map \(A\), which is derived from the explicit position information:

\[A=(-1 O_{t-1}^{t}-M),\] (5)

For feature aggregation, we utilize the attention map to aggregate historical features. Given the current queries \(Q_{t}\), previous fused queries \(Q_{t-1}^{{}^{}}\), and attention map \(A\) in Figure 3, the aggregated temporal features \(F_{t-1}\) can be calculated as:

\[F_{t-1}=_{2}(A_{1}(Q_{t-1}^{{}^{}})),\] (6)

where the \(_{1}\) and \(_{2}\) denote two linear layers. After aggregation, we can combine \(F_{t-1}\) and \(Q_{t}\) to generate the current fused queries \(Q_{t}^{{}^{}}\):

\[Q_{t}^{{}^{}}=(Q_{t}+(((Q_{t}+(F_{t-1}))))),\] (7)

where \(\), \(\), and \(\) denote the dropout operation , the layer normalization , and the feed-forward network, respectively. Finally, we utilize \(Q_{t}^{{}^{}}\) to refine the predicted results of the current frame through two simple FFNs, which are utilized to estimate the confidence of the classification branch and the residual of the regression branch for further refinement. Besides, we follow  to utilize the IoU branch to rectify the confidence score of prediction results in the current frame.

Figure 3: **The details of MTM.** In the MTM, the center of queries in different frames is used to produce the attention map of queries, which is then applied to enhance the current queries.

### Decouple Strategy

We observe that there is an imbalance in the classification and regression learning for temporal fusion. This phenomenon is also mentioned in VISTA  for 3D detection task. However, this decoupled manner brings more computation costs and latency, which is unacceptable for most BEV-based and proposal-based methods. In contrast, benefiting from our efficient design of temporal fusion, the computation cost and latency of QTNet are negligible compared with the whole 3D detection network. Therefore, we decouple the classification and regression branches as shown in Figure 4(b) instead of the coupling manner in Figure 4(a) that is usually employed in the previous temporal fusion methods [49; 5; 16]. Specifically, we separately implement the temporal fusion for the classification branch and the regression branch so as to achieve better detection performance.

## 4 Experiments

### Experimental setup

DatasetWe evaluate our method on the nuScenes dataset , which is a large-scale autonomous driving dataset. It contains 700, 150, and 150 scenes for training, validation, and testing. Each scene is roughly 20 seconds long and annotated at 2Hz, and provides point clouds acquired by 32-beam LiDAR and surrounding images acquired by 6 cameras. Note that each sample consists of one key frame and nine sweep frames. Besides, there are roughly 1.6M objects for 10 categories (barrier, bicycle, bus, car, motorcycle, pedestrian, trailer, truck, construction vehicle, and traffic cone) on the 3D object detection task. For the evaluation metric, Mean Average Precision (mAP)  and nuScenes detection score (NDS)  are used to evaluate the performance of 3D detectors on the nuScenes dataset.

### Implementation details

Following the settings of nuScenes dataset, we concatenate point clouds of one key frame and nine sweeps as the input of a sample, which is roughly 0.5 seconds long. Each point is represented as \((x,y,z,r, t)\), where the \((x,y,z)\), \(r\), and \( t\) denote the 3D coordinate of a point, the reflectance, and the time offset from the key frame, respectively.

Our method is implemented on 3D object detectors with LiDAR or multi-modality (LiDAR and images). For LiDAR, we take the TransFusion-L  as our LiDAR-only baseline. For multi-modality, we take the TransFusion  and DeepInteraction  as our multi-modality baseline. The voxelization range is set to \([-54m,54m]\) for both \(X\) and \(Y\) axes and \([-5m,3m]\) for \(Z\) axis. The voxel size is set to \((0.075m,0.075m,0.1m)\). We set the number of queries as 200 for training and testing. For temporal fusion, we utilize 2 or 3 historical frames. Note that we do not adopt any test time augmentation (TTA), multiple models ensemble, or future frames during inference.

Figure 4: **Decouple temporal fusion. The decoupling manner conducts temporal fusion on classification and regression branches separately for better detection performance. Here, the weights of classification and regression branches are not shared.**Our training process is divided into two stages. In the first stage, we train the DETR-like 3D detectors (TransFusion-L, TransFusion, and DeepInteraction) with their default settings. In the second stage, we generate the memory bank of query features and prediction results. Then, we train our QTNet for 10 epochs without GT-Sampling  and CBGS  on four NVIDIA RTX 4090 GPUs. To optimize our network, we adopt the AdamW  optimizer with a one-cycle learning rate policy, and the batch size is set to 16.

### Comparison to the state of the art

We compare the proposed QTNet with previous state-of-the-art 3D detectors on the nuScenes  validation and test set. As shown in Table 1, QTNet shows consistent performance improvement when our module is integrated into the LiDAR-only detector Transfusion-L  or multi-modality methods Transfusion  and DeepInteraction . Compared with the single frame LiDAR-only

   Method & Year & Modality & Frames & Backbones & mAP\(\) & NDS\(\) \\  MVP  & NeurIPS 2021 & LC & 1 & DLA34 \& VoxelNet & 67.1 & 70.8 \\ AutoAlignV2  & ECCV 2022 & LC & 1 & CSPNet \& VoxelNet & 67.1 & 71.2 \\ TransFusion  & CVPR 2022 & LC & 1 & R50 \& VoxelNet & 67.5 & 71.3 \\ BEVFusion  & NeurIPS 2022 & LC & 1 & Swin-Tiny \& VoxelNet & 67.9 & 71.0 \\ DeepInteraction  & NeurIPS 2022 & LC & 1 & R50 \& VoxelNet & 69.9 & 72.6 \\ BEVFusion  & ICRA 2023 & LC & 1 & Swin-Tiny \& VoxelNet & 68.5 & 71.4 \\ MSMDFusion  & CVPR 2023 & LC & 1 & R50 \& VoxelNet & 69.3 & 72.1 \\ CMT  & ICCV 2023 & LC & 1 & V2-99 \& VoxelNet & 70.3 & 72.9 \\  TransFusion\({}^{*}\) & CVPR 2022 & LC & 1 & R50 \& VoxelNet & 67.1 & 70.7 \\ +QTNet & - & LC & 4 & R50 \& VoxelNet & 68.5 & 71.6 \\ DeepInteraction\({}^{*}\) & NeurIPS 2022 & LC & 1 & R50 \& VoxelNet & 69.9 & 72.6 \\ +QTNet & - & LC & 4 & R50 \& VoxelNet & **70.3** & **73.1** \\   CenterPoint  & CVPR 2021 & L & 1 & VoxelNet & 59.6 & 66.8 \\ TransFusion-L  & CVPR 2022 & L & 1 & VoxelNet & 65.1 & 70.1 \\ INT  & ECCV 2022 & L & 10 & VoxelNet & 61.8 & 67.3 \\ LidarMultiNet  & AAAI 2023 & L & 1 & VoxelNet & 63.8 & 69.5 \\ VoxelNeXt  & CVPR 2023 & L & 1 & VoxelNet & 60.0 & 67.1 \\ LargeKernel3D  & CVPR 2023 & L & 1 & Voxel-LargeKernel3D & 63.3 & 69.1 \\ LinK  & CVPR 2023 & L & 1 & Voxel-LinK & 63.6 & 69.5 \\ MGTANet  & AAAI 2023 & L & 3 & VoxelNet & 62.9 & 68.7 \\ MGTANet\({}^{}\) & AAAI 2023 & L & 3 & VoxelNet & 64.8 & 70.6 \\  TransFusion-L\({}^{*}\) & CVPR 2022 & L & 1 & VoxelNet & 65.0 & 70.0 \\ +QTNet & - & L & 3 & VoxelNet & **66.3** & **70.8** \\ +QTNet & - & L & 4 & VoxelNet & **66.5** & **70.9** \\   

Table 1: Comparison with state-of-the-art methods on the nuScenes validation set. The L and C represent LiDAR and camera, respectively. The column of Frames denotes the number of key frame. \({}^{*}\) denotes our reproduced results. \(\) denotes future information is used.

   Method & Year & Modality & Frames & Backbones & mAP\(\) & NDS\(\) \\  CenterPoint  & CVPR 2021 & L & 1 & VoxelNet & 60.3 & 67.3 \\ TransFusion-L  & CVPR 2022 & L & 1 & VoxelNet & 65.5 & 70.2 \\ VISTA  & CVPR 2022 & L & 1 & VoxelNet & 63.7 & 70.4 \\ LidarMultiNet  & AAAI 2023 & L & 1 & VoxelNet & 67.0 & 71.6 \\ VoxelNeXt  & CVPR 2023 & L & 1 & VoxelNet & 64.5 & 70.0 \\ LargeKernel3D  & CVPR 2023 & L & 1 & Voxel-LargeKernel3D & 65.3 & 70.5 \\ LinK  & CVPR 2023 & L & 1 & Voxel-LinK & 66.3 & 71.0 \\ 
3DVID\({}^{}\) & TPAMI 2021 & L & 3 & VoxelNet & 65.4 & 71.4 \\ MGTANet\({}^{}\) & AAAI 2023 & L & 3 & VoxelNet & 65.4 & 71.2 \\  QTNet & - & L & 3 & VoxelNet & 68.2 & 72.0 \\ QTNet & - & L & 4 & VoxelNet & **68.4** & **72.2** \\   

Table 2: Comparison with state-of-the-art methods on the nuScenes test set. \(\) denotes future information is used.

baseline TransFusion-L, QTNet brings 1.3% (or 1.5%) mAP and 0.8% (or 0.9%) NDS improvements with 3 (or 4) frames, which achieves a new SOTA performance for LiDAR-only detectors. Besides, QTNet even outperforms MGTANet\({}^{}\), a manner of using the future frame information, by 1.5% mAP and 0.2% NDS. Note that QTNet does not utilize any future information since future information can not be fetched in the realistic scene of autonomous driving. Besides, compared with the single frame multi-modality baseline TransFusion, QTNet also produces 1.4% mAP and 0.9% NDS improvement. Based on the previous SOTA multi-modality method DeepInteraction, QTNet brings 0.4% mAP and 0.5% NDS improvement, achieving a new SOTA result for multi-modality 3D object detection.

On the nuScenes test benchmark, we take TransFusion-L as our baseline. As shown in Table 2, QTNet outperforms the advanced temporal fusion method MGTANet\({}^{}\) by 2.8% mAP and 0.8% NDS for 3 frames as input, which illustrates the superiority of QTNet. QTNet achieves 68.4% mAP and 72.2% NDS with 4 frames, setting a new SOTA performance. Besides, we present the computation cost and latency by integrating our QTNet into different baselines in Table 3. It can be seen that our method brings consistent performance improvement with negligible computation cost and latency.

### Ablation studies

Query-based v.s. Other paradigmsFor a fair comparison, we integrate the representative long-term fusion of MGTANet and MPPNet into our baseline TransFusion-L and keep the same number of frames as 3. As shown in Table 4, we present the results of QTNet and other methods in terms of the performance and computation cost. QTNet outperforms the BEV-based method MGTANet and the proposal-based method MPPNet 0.9% NDS and 0.8% NDS, respectively. For computation cost and latency, QTNet only brings a tiny increment of 0.1 GFLOPs and 4.5 ms based on the baseline of TransFusion-L. However, MGTANet even additionally produces 193.0 GFLOPs and 22.1 ms, and MPPNet brings 131.3 GFLOPs and 127.9 ms on the same baseline. These experiments effectively demonstrate that QTNet possesses superior performance and less computation cost and latency when compared with other paradigms. Besides, QTNet is lightweight, which only brings tiny extra parameters of 0.3 M.

Ablation studies on our QTNetWe adopt the TransFusion-L  as our baseline (I) under the case of 3 frames as input and conduct ablation studies on our QTNet as shown in Table 5(a). Here, we provide an available manner to model the relationship of adjacent frames for temporal fusion by the cross attention operation  (II) for comparison. It can be observed that there is only a minor performance improvement with the mAP of 0.2% over baseline. In contrast, our proposed MTM

   Method & Paradigm & mAP & NDS & FLOPs (G) & Latency (ms) & Params (M) \\  TransFusion-L\({}^{*}\) & - & 63.1 & 67.8 & 90.7 & 138.2 & 8.3 \\ +MGTANet  & BEV & 64.0 (+0.9) & 68.1 (+0.3) & +193.0 & +22.1 & +5.5 \\ +MPPNet  & Proposal & 63.4 (+0.3) & 68.2 (+0.4) & +131.3 & +127.9 & +7.0 \\ +QTNet & Query & 64.7 (+1.6) & 69.0 (+1.2) & +0.1 & +4.5 & +0.3 \\   

Table 4: Compared with other paradigms on the TransFusion-L. \({}^{*}\) denotes the fine-tuned results. Note that when introducing MGTANet into TransFusion-L, we need to fine-tune the detection head of TransFusion-L, which is harmful to the final detection performance. Therefore, for a fair comparison, we choose the same fine-tuned TransFusion-L on different paradigms.

   Method & Modality & mAP (\%)\(\) & NDS (\%)\(\) & FLOPs (G)\(\) & Latency (ms)\(\) & Params (M) \\  TransFusion\({}^{*}\) & LC & 67.1 & 70.7 & 445.9 & 201.2 & 37.0 \\ +QTNet & LC & 68.5 & 71.6 & 446.4 & 207.7 & 37.7 \\ DeepInteraction\({}^{*}\) & LC & 69.9 & 72.6 & 499.2 & 355.0 & 57.8 \\ +QTNet & LC & **70.3** & **73.1** & 499.7 & 361.5 & 58.5 \\   TransFusion-L\({}^{*}\) & L & 65.0 & 70.0 & 90.7 & 138.2 & 8.3 \\ +QTNet & L & **66.5** & **70.9** & 90.8 & 144.7 & 8.6 \\   

Table 3: Comparison of computation cost and latency on the nuScenes validation set. \({}^{*}\) denotes our reproduced results. The FLOPs and Latency are tested on a single NVIDIA RTX 4090 GPU with the batch size of 1.

(III) obtains 1.2% mAP and 0.5% NDS performance gains over baseline. Besides, we apply the IoU branch [35; 40] to rectify the confidence score of prediction results (IV), which can further boost the detection performance. Finally, we decouple the classification and regression of temporal fusion (V), which further brings 0.2% NDS performance gains. The above experiments demonstrate the effectiveness of our proposed components in our QTNet.

Different Number of FramesWe explore the impact of detection performance on the different number of frames in the temporal fusion, whose results are shown in Table 5(b). We can find that as the number of frames increases from 1 to 4, the performance gain is gradually strengthening. When the number of frames reaches 5, the performance starts to saturate. Besides, the computation cost and latency slightly increase as the number of frames increases. However, thanks to our efficient design, the extra computation cost and latency are much slight compared with our whole network.

Why MTM is better?Intuitively, the cross attention operation  can directly construct the relevance among query features. However, objects with similar geometric structures between adjacent frames are difficult to distinguish in LiDAR point clouds, which makes the construction of the relationship between two frames less reliable. On the contrary, MTM builds an explicit motion modeling for temporal fusion by taking advantage of the fact that the same 3D objects between adjacent frames do not shift too much in a short time. To further illustrate the effectiveness of our MTM, we present the visualization for the manners of cross attention and our MTM as shown in Figure 5. Here, for convenience, we select three objects from predictions with high scores in \(t\) and \(t-1\) frames. The attention map generated by cross attention is ambiguous, which leads to sub-optimal performance for temporal fusion. On the contrary, the attention map generated by our MTM is more discriminative. Therefore, our MTM can establish more reliable relevance among queries than cross attention, which proves the rationality and superiority of our proposed MTM.

### Limitation

Our method can be integrated into some advanced LiDAR-only or multi-modality DETR-like 3D detectors and brings performance improvements with negligible computation cost and latency. However, the main limitation is that the cost and latency of our method increase as the number of frames

Table 5: Ablation studies on the temporal fusion, evaluated on the nuScenes validation set.

Figure 5: **Attention map comparison. For better visualization, we select some predictions with high confidence score and calculate the attention map. The row and column of attention map represent objects in \(t\) and \(t-1\) frame, respectively. The value of attention map is higher and the relevance is stronger. The attention map (a) generated by cross attention is ambiguous. The attention map (b) generated by our MTM is more discriminative than cross attention.**

increases. In the future, we will explore recurrent architecture for 3D object detection with temporal fusion.

## 5 Conclusion

In this paper, we propose a query-based temporal fusion network named QTNet for 3D object detection. Based on our query-based feature representation, we introduce a lightweight motion-guided temporal modeling MTM module to aggregate temporal features more efficiently and effectively. Extensive experiments demonstrate the superiority of our proposed QTNet by comparing it with the mainstream BEV-based and proposal-based paradigms. Besides, QTNet even achieves new state-of-the-art on the nuScenes dataset for LiDAR-only and multi-modality 3D detectors with negligible computation cost and latency compared with the baseline network.

AcknowledgementThis work was supported in part by the National Science Fund for Distinguished Young Scholars of China (Grant No. 62225603), in part by the Hubei Key R&D Program (Grant No. 2022BAA078), and in part by the Taihu Lake Innovation Fund for Future Technology (HUST: 2023-A-1).