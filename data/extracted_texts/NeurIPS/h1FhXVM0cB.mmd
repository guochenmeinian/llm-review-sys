# Improved Convergence in High Probability of

Clipped Gradient Methods with Heavy Tailed Noise

 Ta Duy Nguyen

Department of Computer Science

Boston University

taduy@bu.edu

&Thien Hang Nguyen

Khoury College of Computer Sciences

Northeastern University

nguyen.thien@northeastern.edu

&Alina Ene

Department of Computer Science

Boston University

aene@bu.edu

&Huy Le Nguyen

Khoury College of Computer Sciences

Northeastern University

hu.nguyen@northeastern.edu

###### Abstract

In this work, we study the convergence _in high probability_ of clipped gradient methods when the noise distribution has heavy tails, i.e., with bounded \(p\)th moments, for some \(1<p 2\). Prior works in this setting follow the same recipe of using concentration inequalities and an inductive argument with union bound to bound the iterates across all iterations. This method results in an increase in the failure probability by a factor of \(T\), where \(T\) is the number of iterations. We instead propose a new analysis approach based on bounding the moment generating function of a well chosen supermartingale sequence. We improve the dependency on \(T\) in the convergence guarantee for a wide range of algorithms with clipped gradients, including stochastic (accelerated) mirror descent for convex objectives and stochastic gradient descent for nonconvex objectives. Our high probability bounds achieve the optimal convergence rates and match the best currently known in-expectation bounds. Our approach naturally allows the algorithms to use time-varying step sizes and clipping parameters when the time horizon is unknown, which appears difficult or even impossible using existing techniques from prior works. Furthermore, we show that in the case of clipped stochastic mirror descent, several problem constants, including the initial distance to the optimum, are not required when setting step sizes and clipping parameters.

## 1 Introduction

Stochastic optimization is a well-studied area with many applications ranging from machine learning, to operation research, numerical linear algebra and beyond. In contrast to deterministic algorithms, stochastic algorithms might fail, and a pertinent question is how often does failure happen and how to increase the success rate. These questions are especially important in critical applications where failure is not tolerable, or when a single run is costly in time and resources. Fortunately, the standard stochastic gradient descent (SGD) algorithm has been shown to converge with _high probability_ under a _light-tailed noise_ distribution such as sub-Gaussian distributions , which gives strong guarantee on the success of single runs. However, recent observations in popular deep learning applications, such as training attention models  and convolutional networks , reveal a more challenging optimization landscape: the gradient noises follow _heavy-tailed_ distributions, where the variance may be infinite , whereasthe standard light-tailed setting assumes that all the moments are bounded. Heavy-tailed gradient noises can cause algorithms like SGD to fail, and this mismatch between theory and practice has been suggested to be one of the reasons for the strong preference of adaptive methods like Adam over SGD in modern settings .

In this work, we consider the setting of _heavy-tailed noise_ proposed by Zhang et al., (2020) , where the (unbiased) gradient noise only has bounded \(p\)th moments, for some \(p(1,2]\). While standard SGD can fail to converge when the variance is unbounded, i.e. when \(p<2\),  show that SGD with appropriate clipping (or _Clipped-SGD_) converges _in expectation_ under heavy-tailed noise, where the convergence rate depends on \(O()\) if \(\) is the targeted maximum failure probability. It is more desirable, however, to obtain convergence results in _high probability_, where the convergence rate depends instead on \(O()\), which gives better guarantees for single runs.

Recent follow-up works  show that variants of Clipped-SGD in fact converge with high probability. This is a pleasing result, extending the earlier work by  for \(p=2\). However, there are several shortcomings of these results when compared with the corresponding bounds in the light-tailed setting. First, the clipped algorithm uses a fixed step size and a fixed clipping parameter depending on the number of iterations, which precludes results with _unknown_ time horizons. Secondly, the convergence guarantees are worse than the light-tailed bounds by a \( T\) factor, even for fixed step sizes and clipping parameters. These issues beg a qualitative question:

_Is heavy-tailed noise inherently harder than light-tailed noise?_

In this work, we answer the above question for Clipped-SGD and the general clipped (accelerated) stochastic mirror descent (_Clipped-SMD_) algorithm. We give an improved analysis framework that not only gives tighter bounds matching the light-tailed noise setting, but also allows for step sizes and clipping parameters for unknown time horizons. Furthermore, we show that this framework is applicable to various settings, from finding minimizers of convex functions with arbitrarily large domains using (accelerated) mirror descent, to finding stationary points for non-convex functions using gradient descent.

### Contributions and Techniques

Our work addresses several open questions posed by previous works including handling general domains and dealing with an unknown time horizon under heavy-tailed noise. Qualitatively, we close the logarithmic suboptimality gap and achieve the optimal rate in several settings. More specifically:

\(-\) We demonstrate a novel approach to analyze clipped gradient methods in high probability that is general and applies to various standard settings. In the convex setting, we analyze Clipped-SMD and clipped stochastic accelerated mirror descent. In the non-convex setting, we analyze Clipped-SGD. Using our new analysis, we show that clipped methods attain time-optimal convergence in high probability for both convex and nonconvex objectives under heavy-tailed gradient noise. In the convex setting, we obtain an \(O(T^{})\) convergence rate for arbitrary (not necessarily compact) convex domains for Clipped-SMD and \(O(T^{}+T^{-2})\) for accelerated Clipped-SMD, where \(\) is the noise parameter. These rates are time-optimal and match the lower bounds in . In the nonconvex setting, we obtain the optimal convergence rate of \(O(T^{})\) for clipped-SGD. This bound is also time-optimal and matches the lower bound in ; it also complements the in-expectation convergence of clipped-SGD provided by .

\(-\) Previous works for heavy-tailed noises follow the recipe of using Freedman-type inequalities  as a _blackbox_ and bound the iterates inductively for all iterations. This process incurs an additional \( T\) dependency in the final convergence rate; in other words, the success probability goes from \(1-\) to \(1-T\). The step sizes and clipping parameters of this approach depend on the time horizon \(T\) to enable the union bound and induction across all iterations in the analysis, excluding the important case when the time horizon is unknown. Our whitebox approach forgoes the aforementioned induction, not only circumventing the \( T\) loss but also allowing for an unknown time horizon. We further show that our analysis allows for a choice of step size and clipping parameters that do not depend on generally unknown parameters like the noise-parameter \(\), the failure probability \(\), and the initial distance to the optimum, all of which appear impossible using only the techniques from prior works.

\(-\) Our whitebox approach analyzes the moment generating function of a well chosen martingale difference sequence to obtain tight rates for stochastic gradient methods. This approach is closest to the work of , which only work in the light-tailed noise setting. In contrast to the light-tailed noise setting where all the moments are well controlled, the heavy-tailed setting often requires algorithms to incorporate gradient clipping for controlling the possibly infinite moments. However, this makes the gradient estimate biased and requires more careful attention to control the bias propagating through the algorithm. Naively applying the technique in  is not enough to handle heavy-tailed noise. Rather, as will be shown in our analysis, we introduce a novel history-dependent weights for the martingale sequence that is able to cope with the propagating bias term of clipped methods for heavy-tailed noise across various settings.

### Related Works

High probability convergence for light-tailed noises.Convergence in high probability of stochastic gradient algorithms has been established for sub-Gaussian noises in a number of prior works, including  for convex problems with bounded domain (or bounded Bregman diameter) or with strong convexity. Other works  study convergence of variants of SGD for nonconvex objectives, where they consider sub-Gaussian and sub-Weibull noises. The most relevant to ours in this line of work is the one by , where a whitebox approach is employed to obtain tight rates for stochastic gradient methods in the light-tailed noise setting. However, their technique is not directly applicable in the heavy-tailed noise setting, where we need to introduce new ideas to handle the biases introduced by gradient clipping.

High probability convergence for noises with bounded variance and heavy tails.The design of new gradient algorithms and their analysis in the presence of heavy-tailed noises has drawn significant recent interest. Starting from the work  which propose Clipped-SGD to handle exploding gradients in recurrent neural networks, the recent works  give new motivation for clipped methods in the context of convolutional networks and attention deep networks that attempts to explain the dominance of adaptive methods over SGD in practical modern scenarios.

While the convergence in expectation of vanilla SGD has been extensively studied , only recently has the convergence of Clipped-SGD with heavy tailed noises been closely examined. There,  first show the convergence in expectation of Clipped-SGD for nonconvex functions and provide a matching lower bound. In the convex regime, several works with different clipping strategies for the case of \(p=2\) have shown high probability convergence for smooth problems with bounded domain , smooth unconstrained problems , and non-smooth problems . A variant of Clipped-SGD that utilizes momentum  has also been shown to converge with high probability for bounded \(p\)th moments gradient noise. However, the analysis in  requires a strong assumption which implies that the true gradients are bounded, a restrictive assumption that excludes objectives like quadratic functions.

More recently,  give nearly-optimal convergence rates for several Clipped-SGD variants. These works follow the recipe of using Freedman-type inequalities  as a blackbox and bound the iterates inductively for all iterations, which incur an additional \( T\) dependency in the final convergence rate. We show in our work that existing convergence rates can be tightened up and improved. Tight lower bounds for the optimal convergence rate have been shown by  for convex objectives and by  for nonconvex settings. In both cases, our paper provides optimal convergence guarantees.

In a related but different line of work,  show that vanilla SGD can converge with heavy tailed noise for a special type of strongly convex functions, and  show that stochastic mirror descent converges in expectation for a special choice of mirror maps, although only for strongly convex objectives with bounded domains.

## 2 Preliminaries: Assumptions and Notations

We study the problem \(_{x}f(x)\) where \(f:^{d}\) and \(\) is the domain of the problem. In the convex setting, we assume that \(\) is a convex set but not necessarily compact. We let \(\|\|\) be an arbitrary norm and \(\|\|_{*}\) be its dual norm. In the nonconvex setting, we take \(\) to be \(^{d}\) and consider only the \(_{2}\) norm.

### Assumptions

Our paper works with the following assumptions:

**(1) Existence of a minimizer**: In the convex setting, we assume that there exists \(x^{*}_{x}f(x)\). We let \(f^{*}=f(x^{*})\).

**(1') Existence of a finite lower bound**: In the nonconvex setting, we assume that \(f\) admits a finite lower bound, i.e., \(f^{*}:=_{x^{d}}f(x)>-\).

**(2) Unbiased estimator**: We assume that our algorithm is allowed to query a stochastic first-order oracle that returns a history-independent, unbiased gradient estimator \(f(x)\) of \( f(x)\) for any \(x\). That is, conditioned on the history and the queried point \(x\), we have \([f(x) x]= f(x)\).

**(3) Bounded \(p\)th moment noise**: We assume that there exists \(>0\) such that for some \(1<p 2\) and for any \(x\), \(f(x)\) satisfies \([\|f(x)- f(x)\|_{*}^{p} x]^{p}\).

**(4) \(L\)-smoothness**: We consider the class of \(L\)-smooth functions: for all \(x,y^{d}\), \(\| f(x)- f(y)\|_{*} L\|x-y\|.\)

### Gradient Clipping Operator and Notations

We introduce the gradient clipping operator and its general properties used in Clipped-SMD (Algorithm 2) and Clipped-SGD (Algorithm 1). Let \(x_{t}\) be the output at iteration \(t\) of an algorithm of interest. We denote by \(f(x_{t})\) the stochastic gradient obtained by querying the gradient oracle. The clipped gradient estimate \(f(x_{t})\) is taken as

\[f(x_{t})=\{1,}{\| f(x_{t})\|_{*}}\}f(x_{t}), \]

where \(_{t}\) is the clipping parameter used in iteration \(t\). In subsequent sections, we let \(_{t}:=f(x_{t})-f^{*}\) denote the optimal function value gap at \(x_{t}\). We let \(_{t}=(f(x_{1}),,f (x_{t}))\) be the natural filtration at time \(t\) and define the following notations for the stochastic error, the deviation, and the bias of the clipped gradient estimate at time \(t\):

\[_{t}=f(x_{t})- f(x_{t});_{t}^{u}= f(x_{t})-[f(x_{t})_{t-1}];_{t}^{b}=[f(x_{t}) _{t-1}]- f(x_{t}).\]

Note that \(_{t}^{u}+_{t}^{b}=_{t}\). Regardless of the convexity of the function \(f\), the following lemma provides upper bounds for these quantities. These bounds can be found in prior works [7; 33; 19; 28] for the

   & Assumptions & Convex Setting (Clipped-SMD) & Non-convex Setting (Clipped-SGD) \\   Lower bound & \(p(1,2]\) & \((T^{})\) & \((T^{})\) \\  Previous & & & \\ high-probability & Known \(T\) & \((T^{})\) & \((T^{})\) \\ results & & & \\  Our results & Known \(T\) & \(O(T^{})\)(Thm 4.1) & \(O(T^{})\)(Thm 3.1) \\   & Unknown \(T\) & \((T^{})\)(Thm 4.4) & \((T^{})\)(Thm B.2) \\  

Table 1: Previous and new results for high-probability convergence (with failure probability \(\)) of clipped SMD and SGD under heavy tailed noise: \([\|f(x)- f(x)\|_{*}^{p} x]^{p}\) for some \(p(1,2]\), where \(f(x)\) denotes the stochastic gradient at \(x\) for the objective \(f\). For the convex setting, the error bounds are for the optimality gaps \(_{t=1}^{T}f(x_{t})-f^{*}\). For the nonconvex setting, we bound the gradient norm \(_{t=1}^{T}\| f(x_{t})\|^{2}\). Here, \(()\) hides polylog \(T\) factors. Note that, for simplicity, we do not compare against results in more specialized settings such as bounded domain or bounded gradients, as well as other variants of clipped SGD.

special case of \(_{2}\) norm. The extension to the general norm follows in the same manner, which we omit in this work.

**Lemma 2.1**.: _For stochastic gradients \(f(x_{t})\) with bounded \(p\)th moment noise, the clipped gradients \(f(x_{t})\) satisfy the following properties:_

\[\|_{t}^{u}\|_{*}=\|f(x_{t})- [f(x_{t})_{t-1}]\|_{*} 2 _{t}. \]

_Furthermore, if \(\| f(x_{t})\|_{*}}{2}\) then_

\[\|_{t}^{b}\|_{*} =\|[f(x_{t})_{t-1}]- f(x_{t})\|_{*} 4^{p}_{t}^{1-p}; \] \[[\|_{t}^{u}\|_{*}^{2}] =[\|f(x_{t})-_{t} [f(x_{t})]\|_{*}^{2}_{t-1} ] 40^{p}_{t}^{2-p}. \]

Finally, we state a simple but important lemma that bounds the moment generating function of a zero-mean bounded random variable. The proof can be found in, for example, equation (3) of .

**Lemma 2.2**.: _Let \(X\) be a random variable such that \([X]=0\) and \(|X| R\) almost surely. Then for \(0\)_

\[[( X)]( ^{2}[X^{2}]).\]

## 3 Clipped Stochastic Gradient Descent for Nonconvex Functions

In this section, we study the convergence of Clipped-SGD for nonconvex functions. Here, we consider the domain to be \(^{d}\) equipped with the standard \(_{2}\) norm. We first outline a blackbox concentration argument to show convergence in high probability of Algorithm 1 and then follow-up with a more powerful whitebox approach that allows for a tight high probability convergence analysis.

```
Parameters: initial point \(x_{1}\), step sizes \(\{_{t}\}\), clipping parameters \(\{_{t}\}\) for \(t=1\) to \(T\)do \(f(x_{t})=\{1,}{\| f(x_ {t})\|}\}f(x_{t})\) \(x_{t+1}=x_{t}-_{t}f(x_{t})\)
```

**Algorithm 1** Clipped-SGD

**Comparison to previous works.** In the simple setting of known time horizon and without momentum for Clipped-SGD, the \((T^{})\) convergence rate has not been shown before to the best of our knowledge. The recent work by  study this case and only give a suboptimal rate of \((T^{})\). Note that  study other variants of Clipped-SGD with momentums incorporated. Although  achieve the nearly-optimal time dependency of \((T^{})\) in the non-convex settings, they rely on using blackbox concentration inequalities which result in a suboptimal convergence rate that also requires a known time horizon.

We first present the guarantee for known time horizon \(T\) via our whitebox approach in Theorem 3.1 and defer the statement for unknown \(T\) in Theorem B.2 to the Appendix.

**Theorem 3.1**.: _Assume that \(f\) satisfies Assumption (1'), (2), (3), (4). Let \(:=\{;1\}\) and \(_{1}:=f(x_{1})-f^{*}\). For known time horizon \(T\), we choose \(_{t}\) and \(_{t}\) such that_

\[_{t} :=:=\{(}} )^{}T^{}^{};2};32^{} T^{}\}\] \[_{t} :=:=}T^{}}{8 }=}}{8}\{( }})^{}T^{} ^{};}}{2}}; }}{32^{1/p}}\}.\]_Then with probability at least \(1-\)_

\[_{t=1}^{T}\| f(x_{t})\|^{2}  720}L\{(}})^{}T^{}^{ };.\] \[.2}T^{};32^{1/p} T ^{}\}=O(T^{}).\]

_Remark 3.2_.: In comparison to the corresponding results in  (Theorem E.2), while our result achieves a poly \(T\) factor better rate when \(p<2\), the dependency on \(\) in our result contains a dependency on \(p\) while the result in  does not. That term can dominate the convergence rate in the regime when \(\) is very small and \(p\) is very close to \(1\). Hence, an open question is to remove such dependency on \(p\) for the \(\) term while still maintain the optimal rate on \(T\).

Now, we turn to the analysis, starting with the key Lemma 3.3 (proof in the Appendix).

**Lemma 3.3**.: _Assume that \(f\) satisfies Assumption (1'), (2), (3), (4) and \(_{t}\) then for all \(t 1\),_

\[}{2}\| f(x_{t})\|^{2} _{t}-_{t+1}+(L_{t}^{2}-_{t})  f(x_{t}),_{t}^{u}+}{2} \|_{t}^{b}\|^{2}\] \[+L_{t}^{2}(\|_{t}^{u}\|^{2}- [\|_{t}^{u}\|^{2}_{t-1}])+L _{t}^{2}[\|_{t}^{u}\|^{2}_{ t-1}]. \]

_Remark 3.4_.: In Lemma 3.3, we decompose the RHS into appropriate terms that allow us to define a martingale. This lemma helps us understand why we can achieve a better convergence rate \(O(T^{})\) (for minimizing the norm squared of the gradient) in comparison to the best rate of \(O(T^{})\) in the convex setting. We focus on the error term \( f(x_{t}),_{t}= f(x_{t} ),_{t}^{u}+ f(x_{t}),_{t}^{b}\) on the RHS of (5). Since this error contains the gradient \( f(x_{t})\), we leverage some of the gain \(\| f(x_{t})\|^{2}\) on the LHS of 5: we use Cauchy-Schwarz to bound \( f(x_{t}),_{t}^{b}\| f (x_{t})\|^{2}+\|_{t}^{b}\|^{2}\) and use the some of the gain to absorb the first term. Then setting our parameters \(_{t},_{t}\) appropriately to balance the remaining terms helps us achieve the \(O(T^{})\) rate. Contrast this to the convex setting in the next section: the mismatch between the error term that contains the distance term \(\|x^{*}-x_{t}\|\) and the gain term that contains the function value gap \(f(x_{t})-f^{*}\) prevents us from using the gain to absorb some of the error. Thus, this explains the convergence rate discrepancy between the convex case and the non-convex setting (see also Remark 4.6).

Before giving a sketch of our whitebox approach, we present a sketch of a blackbox argument that gives a nearly time-optimal convergence rate. This approach has an additional \( T\) factor in the final rate but will serve as a point of comparison for our new techniques, which will close the logarithmic gap.

**Blackbox approach.** The key lies in the following lemma, which yields the near optimal \((T^{})\) convergence rate of Clipped-SGD. In this case, we assume that the clipping parameters \(_{t}\) and the step sizes \(_{t}\) are fixed. Note that the success probability is only \(1-T\). This result uses Lemma 3.3 and Freedman's inequality (Theorem A.1) primarily as a _blackbox_ to bound the error terms inductively by the initial function value gap to optimality.

**Lemma 3.5**.: _For \(1 N T+1\), let \(_{t}=\), \(_{t}=\) (the specific choices are omitted here for brevity) and \(E_{N}\) be the event that for all \(k=1, N\),_

\[L^{2}_{t=1}^{k-1}\|_{t}^{u}\|^{2}+(L^{2}- )_{t=1}^{k-1} f(x_{t}),_{t}^{u} +_{t=1}^{k-1}\|_{t}^{b}\|^{2} _{1}.\]

_Then \(E_{N}\) happens with probability at least \(1-\) for each \(N[T+1]\)._

With the above lemma, we can obtain a near-optimal convergence rate. However, this rate is still suboptimal due to the use of \(T\) union bounds as part of the induction proof. We now discuss an improved analysis that closes the remaining gap.

**Whitebox approach.** Our whitebox approach defines a novel supermartingale difference sequence \(Z_{t}\) (shown below) and analyzes its moment generating function from first principles. The sequence is 

[MISSING_PAGE_FAIL:7]

```
Parameters: initial point \(x_{1}\), step sizes \(\{_{t}\}\), clipping parameters \(\{_{t}\}\), \(\) is \(1\)-strongly convex wrt \(\|\|\) for \(t=1\) to \(T\) do \(f(x_{t})=\{1,}{\|f(x_{t})\|_{*}}\}f(x_{t})\) \(x_{t+1}=_{x}\{_{t}f(x_{t}),x+_{}(x,x_{t})\}\)
```

**Algorithm 2** Clipped-SMD

## 4 Clipped Stochastic Mirror Descent for Convex Objectives

In this section, we present and analyze the Clipped Stochastic Mirror Descent algorithm (Algorithm 2) under heavy-tailed noise, with a general domain and arbitrary norm.

We define the Bregman divergence \(_{}(x,y)=(x)-(y)-(y),x-y\), where \(:^{d}\) is a \(1\)-strongly convex differentiable function with respect to the norm \(\|\|\) on \(\). We assume for convenience that \(()=^{d}\). Algorithm 2 is a generalization of Clipped-SGD for convex functions to an arbitrary norm. The only difference from the standard Stochastic Mirror Descent algorithm is the use of the clipped gradient \(f(x_{t})\) in place of the true stochastic gradient \(f(x_{t})\) when computing the new iterate \(x_{t+1}\).

Prior works such as  only consider the setting where the global minimizer lies in \(\). Our algorithm and analysis does not require this restriction and instead only uses the following initial gradient estimate assumption from :

**(5) Initial gradient estimate**: Let \(x_{1}\) be the initial point. We assume that we have access to an upperbound \(_{1}\) of \(\| f(x_{1})\|_{*}\) i.e. \(\| f(x_{1})\|_{*}_{1}\). This assumption is justified as follows. If the noise parameter \(\) defined in assumption (3) is known, we can use the procedure of  to estimate \(\| f(x_{1})\|_{*}\): we take \(O((1/))\) stochastic gradient samples at \(x_{1}\), and let \(g_{1}\) be the geometric median of these samples; we then set \(_{1}:=\|g_{1}\|_{*}+10\). It follows from  that \(\| f(x_{1})\|_{*}_{1}\) holds with probability at least \(1-\). If the domain contains the global optimum \(x^{*}\) (\( f(x^{*})=0\)) and the initial distance \(\|x_{1}-x^{*}\|\) is known, we have the following alternative upper bound that follows from \( f(x^{*})=0\) and smoothness:\(\| f(x_{1})\|_{*}=\| f(x_{1})- f(x^{*})\|_{*} L\|x_{1}-x ^{*}\|\).

**Convergence guarantees**. We first state the convergence guarantee for this algorithm in Theorem 4.1 which works for an arbitrary norm and a general domain which may not include the global optimum. In this theorem, we assume that we know several problem parameters to show the main idea of our analysis. In Theorem 4.4, we remove the knowledge of the problem parameters.

**Theorem 4.1**.: _Assume that convex \(f\) satisfies Assumptions (1), (2), (3), (4) and (5). Let \(=\{;1\}\); \(R_{1}=_{}(x^{*},x_{1})}\), and assume that \(_{1}\) is an upper bound of \(\| f(x_{1})\|_{*}\). For known \(T\), we choose \(_{t}\) and \(_{t}\) such that_

\[_{t} ==\{()^{1/p};2 (3LR_{1}+_{1})\}\] \[_{t} ==}{24_{t}}=}{24} \{()^{-1/p}^{-1}; (3LR_{1}+_{1})^{-1}\}.\]

_Then with probability at least \(1-\)_

\[_{t=2}^{T+1}_{t} 48R_{1}\{26^{}T^ {}^{};2(3LR_{1}+_{1})T^ {-1}\}=O(T^{}).\]

_Remark 4.2_.: This theorem shows that the convergence rate for the known time horizon case is \(O(T^{})\). This rate is known to be optimal, matching the lower bounds shown in [26; 31]. The above guarantee is also adaptive to \(\), i.e., when \( 0\), we obtain the standard \(O(T^{-1})\) convergence rate of deterministic mirror descent.

_Remark 4.3_.: The term \(_{1}\) in the above theorem comes from the inexact estimation of \(\| f(x_{1})\|_{*}\). If we assume that the global optimum lies in the domain \(\), we can simply select \(_{1}=LR_{1}\) without using the estimation procedure, as discussed in (5).

In Theorem 4.1, we use the initial distance \(R_{1}\) to the optimal solution to set the step sizes and clipping parameters. This information is generally not available, but can be avoided. For example, for constrained problems where the domain radius is bounded by \(R\), we can replace \(R_{1}\) in Theorem 4.1 by \(R\) without change in the dependency. However, for the general problem, we present Theorem 4.4, where we do not require knowledge of the constants \(T,,\) or \(R_{1}\) to set the step sizes and clipping parameters. However, we still need the mild assumption of knowing an upper bound \(_{1}\) on \(\| f(x_{1})\|_{*}\). As discussed in (5), \(_{1}\) can be estimated with good accuracy when \(\) is known.

**Theorem 4.4**.: _Assume that convex \(f\) satisfies Assumption (1), (2), (3), (4) and (5). Let \(=\{;1\}\); \(R_{1}=_{}(x^{*},x_{1})}\), and assume that \(_{1}\) is an upper bound of \(\| f(x_{1})\|_{*}\). We choose \(_{t}\) and \(_{t}\) such that_

\[_{t} =\{(52t(1+ t)^{2}c_{2})^{1/p};2(L _{i t}\|x_{i}-x_{1}\|+_{1});}{6} \}\] \[_{t} =}{24_{t}}=}{24}\{(5 2t(1+ t)^{2}c_{2})^{-1/p};\|x_{ i}-x_{1}\|+_{1})};}\},\]

_where the absolute constants \(c_{1}\) and \(c_{2}\) are to ensure the correctness of the dimensions. Then, with probability at least \(1-\), we have_

\[_{t=2}^{T+1}_{t} }(R_{1}+}{3}(+ {2^{p}}{c_{2}}))^{2}\{(52T(1+ T)^{2}c_{2} )^{1/p};.\] \[.4R_{1}L+}{3}L(+}{c _{2}})+2_{1};}{6}\}=(T^{ {1-p}{p}}).\]

**Sketch of the analysis**. In the remainder of this section, we provide a sketch of the analysis for Theorem 4.1, which starts with the following lemma.

**Lemma 4.5**.: _Assume that convex \(f\) satisfies Assumption (1), (2), (3), (4) and \(_{t}\), the iterate sequence \((x_{t})_{t 1}\) output by Algorithm 2 satisfies the following:_

\[_{t}_{t+1} _{}(x^{*},x_{t})-_{} (x^{*},x_{t+1})+_{t} x^{*}-x_{t},_{t}^{u} +_{t} x^{*}-x_{t},_{t}^{b}\] \[+2_{t}^{2}(\|_{t}^{u}\|_{*}^{2}- [\|_{t}^{u}\|_{*}^{2}_{t-1} ])+2_{t}^{2}[\|_{t}^{u}\|_{*}^ {2}_{t-1}]+2_{t}^{2}\|_{t}^{b}\|_{*} ^{2}.\]

_Remark 4.6_.: In contrast to Remark 3.4, there is a mismatch between the gain \(_{t+1}\) and the loss \( x^{*}-x_{t},_{t}\). Since the distance \(\|x^{*}-x_{t}\|\) and the function value gap \(_{t}\) cannot be related in the general convex case, we do not obtain the same rate as in the nonconvex case.

We now define the following terms for \(t 1\):

\[Z_{t} :=z_{t}_{t}_{t+1}+_{}(x^{*},x_{t+1})-_{}(x^{*},x_{t})-_{t} x ^{*}-x_{t},_{t}^{b}-2_{t}^{2}\|_{t}^{b} \|_{*}^{2}\] \[-2_{t}^{2}[\|_{t}^{u}\|_ {*}^{2}_{t-1}]-(^{2}}+24 z_{t}^{2}_{t}^{4}_{t}^{2})[\|_{t}^{u} \|^{2}_{t-1}],\] \[z_{t} :=_{t}_{i t}_{ }(x^{*},x_{i})+16Q_{t}^{2}_{t}^{2}}}\]

for a constant \(Q 1\). We also define \(S_{t}:=_{i=1}^{t}Z_{i}\). We have the following lemma, which is analogous to Lemma 3.6 in the nonconvex case.

**Lemma 4.7**.: _For any \(>0\), let \(E()\) be the event that for all \(1 k T\)_

\[_{t=1}^{k}z_{t}_{t}_{t+1} +z_{k}_{}(x^{*},x_{k+1}) z_{1} _{}(x^{*},x_{1})++_{t=1}^{k}z_ {t}_{t} x^{*}-x_{t},_{t}^{b}\] \[+2_{t=1}^{k}z_{t}_{t}^{2}\|_{t}^{b}\|_{* }^{2}+_{t=1}^{k}((2z_{t}_{t}^{2}+^{2}}+24 z_{t}^{2}_{t}^{4}_{t}^{2})[\|_{t}^{u} \|_{*}^{2}_{t-1}]). \]

_Then \([E()] 1-\)._```
Parameters: initial point \(y_{1}=z_{1}\), step sizes \(\{_{t}\}\), clipping parameters \(\{_{t}\}\), and mirror map \(\), where \(\) is \(1\)-strongly convex wrt \(\|\|\). For \(t=1\) to \(T\) do: Set \(_{t}=\). \(x_{t}=(1-_{t})y_{t}+_{t}z_{t}\). \(f(x_{t})=\{1,}{\|f(x_{t})\|_{*}}\}f(x_{t})\). \(z_{t+1}=_{x}_{t}f(x_{t}),x+_{}(x,z_{t})}\). \(y_{t+1}=(1-_{t})y_{t}+_{t}z_{t+1}\).
```

**Algorithm 3** Clipped-ASMD

We now specify the choice of \(_{t}\) and \(_{t}\). The following proposition gives a general condition for the choice of \(_{t}\) and \(_{t}\) that gives the right convergence rate in time \(T\).

**Proposition 4.8**.: _We assume that the event \(E()\) from Lemma 4.7 happens. Suppose that for some \( T\), there are constants \(C_{1},C_{2},C_{3}\), and \(A\) such that for all \(t\)_

1. \(_{t}_{t}=C_{1}\)_;_ 2. \(_{t=1}^{}(})^{p} C_{2}\)_;_ 3. \((})^{2p} C_{3}(} )^{p}\)_;_ 4. \(\| f(x_{t})\|_{*}}{2}\)_._

_Then for all \(t+1\)_

\[_{i=1}^{t}_{i}_{i+1}+_{}(x^{*},x_{t+1}) (R_{1}+8AC_{1})^{2}\]

_for \(A\{+26^{p}C_{2}+C_{2} C_{3}}{A};1\}\)._

Theorem 4.1 follows from Proposition 4.8. Both proofs can be found in the Appendix.

## 5 Accelerated Stochastic Mirror Descent and Extensions

In Section D in the Appendix, we also show the convergence and its analysis for Clipped Accelerated Stochastic Mirror Descent (Algorithm 3). We require the following additional assumption:

**(5') Global minimizer**: We assume that \( f(x^{*})=0\).

In other words, we assume that the global minimizer lies in the domain of the problem. This assumption is consistent with the works of . Our analysis readily extends to non-smooth settings, and more generally to functions that satisfy \(f(y)-f(x) f(x),y-x+G\|y-x\|+ \|y-x\|^{2}, y,x.\) This condition is satisfied by both Lipschitz functions (when \(L=0\)) and smooth functions (when \(G=0\)). The key step is to extend Lemma 4.5. The proof follows from  and can be found in the Appendix.

## 6 Conclusion

In this work, we propose a new approach to design and analyze various clipped gradient algorithms in the presence of heavy-tailed noise. Our analysis applies to various standard settings, including Clipped-SMD and accelerated Clipped-SMD for convex objectives with general domains and Clipped-SGD for nonconvex objectives, and gives optimal high probability rates in all settings. Our algorithms allow for setting step-sizes and clipping parameters when the time horizon and problem parameters such as the initial distance are unknown. For future work, since our algorithms have the limitation of still requiring the knowledge of parameters like \(L\) and \(p\), it is of great interest to investigate the existence of a _fully-adaptive_ method, like Adagrad, that converges under heavy-tailed noise without requiring the knowledge of _any_ problem parameter. Finally, it would be interesting to extend our techniques to the setting of variational inequalities under heavy-tailed noise .