# Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning

Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning

 Yuefei Lyu1, Chaozhuo Li1, Sihong Xie2, Xi Zhang1

1Key Laboratory of Trustworthy Distributed Computing and Service (BUPT)

Ministry of Education, Beijing University of Posts and Telecommunications, China

2Artificial Intelligence Thrust,

The Hong Kong University of Science and Technology (Guangzhou), China

Corresponding authors: lichaozhuo@bupt.edu.cn, zhangx@bupt.edu.cn.

###### Abstract

Adversarial attacks against graph neural networks (GNNs) through perturbations of the graph structure are increasingly common in social network tasks like rumor detection. Social media platforms capture diverse attack sequence samples through both machine and manual screening processes. Investigating effective ways to leverage these adversarial samples to enhance robustness is imperative. We improve the maximum entropy inverse reinforcement learning (IRL) method with the mixture-of-experts approach to address multi-source graph adversarial attacks. This method reconstructs the attack policy, integrating various attack models and providing feature-level explanations, subsequently generating additional adversarial samples to fortify the robustness of detection models. We develop precise sample guidance and a bidirectional update mechanism to reduce the deviation caused by imprecise feature representation and negative sampling within the large action space of social graphs, while also accelerating policy learning. We take rumor detector as an example targeted GNN model on real-world rumor datasets. By utilizing a small subset of samples generated by various graph adversarial attack methods, we reconstruct the attack policy, closely approximating the performance of the original attack method. We validate that samples generated by the learned policy enhance model robustness through adversarial training and data augmentation.

## 1 Introduction

Social media platforms such as Weibo and Twitter host complex relationship networks that exhibit a typical graph structure. Graph neural networks play a crucial role in analyzing social graphs, demonstrating significant efficacy in a range of social network tasks, including rumor detection [1; 2; 3; 4; 5], spam detection [6; 7] and stance detection [8; 9].

Extensive research [10; 11] has demonstrated that GNNs are vulnerable to adversarial attacks, allowing adversaries to manipulate downstream node classification outcomes by flipping a small number of edges or features within the graph. For instance, in rumor detection, rumor spreaders may manipulate the graph structure to evade detection by reposting messages and following users. Recent research efforts have shifted towards acquiring such attack samples [1; 12], which can be obtained through manual supervision or effective detection models. Analyzing these samples allows platforms to profile attackers, uncover their motives, and understand their attack patterns, thereby crucially enhancing the robustness of detectors to defend against similar attacks.

Utilizing adversarial training, which augments model generalization by introducing perturbed samples into training data, is a common and effective approach . The effectiveness of defense hinges crucially on the adversarial samples employed in adversarial training . Current research on graph adversarial training primarily concentrates on devising effective attack methods to produce adversarial samples, thus fortifying the model's robustness against these attacks. However, within social networks, numerous attackers with diverse goals pose a challenge in developing attack methods that can simulate the wide array of real-world attacks, each characterized by different motives and styles, thus achieving comprehensive defense. Hence, we aim to reconstruct the attack policy to simulate multiple attackers as accurately as possible with the adversarial samples captured by social media platforms. It not only aids in gaining a more comprehensive understanding of the attackers but also facilitates the acquisition of additional samples for adversarial training.

A natural idea is to use the captured attack samples as training data for supervised learning, known as Behavior Cloning (BC) , which constitutes a form of simple imitation learning. However, within social networks, attack samples frequently demonstrate interdependencies, as attackers commonly execute multiple steps of graph perturbation behaviors to accomplish their ultimate objectives. For example,  proposed various camouflage behaviors aimed at deceiving rumor detectors. Rumor spreaders could combine several camouflage behaviors to evade from detection as shown in Figure 1. In sequential decision-making scenarios, BC encounters the challenge of compounding error , as it leads to continual deviations from the observed sample distribution when faced with unknown states. Consequently, we can turn to inverse reinforcement learning (IRL) methods . Diverging from reinforcement learning (RL) , IRL has access to some expert demonstrations while lacking knowledge of the reward function. Although the agent can interact with the environment, it does not obtain rewards; instead, it deduces the reward function concerning sample features from expert demonstrations and subsequently leverages reinforcement learning to uncover the optimal actor. IRL simulates adversarial attack policies based on observed data. Moreover, when employing linear reward functions and interpretable features, IRL offers feature-level post-hoc explanations , thus better aiding platform operators in understanding attack behaviors.

Reconstructing interpretable attack policies in social networks using inverse reinforcement learning poses two primary challenges. Firstly, expert demonstrations gathered from social media platforms originate from diverse attackers. Thus, it is imperative to develop a policy capable of simulating multiple experts as well as achieving similar attack performance. Secondly, while linear reward functions and interpretable features provide transparent interpretations, their application to represent graph structure data within the large action space of social graphs poses significant challenges. Similar sample features may correspond to entirely disparate ground true rewards , which make difficulties in inverse reinforcement learning.

Therefore, we propose MoE-BiEntIRL, an explainable bidirectional update maximum entropy inverse reinforcement learning method with the mindset of mixture-of-experts. It improves maximum entropy

Figure 1: An X(Twitter) rumor published on Politifact. Three instances of the rumorâ€™s propagation on X are listed, which imply different attack styles and sequences: (1) User A, who has a notable number of followers, posted the rumor and gained considerable attention. The attack sequence involved purchasing followers and posting the rumor. (2) User B, with a large number of followers, retweeted the rumor originally posted by User A. This extends the last sequence to constitute a sequence of purchasing followers, posting, and retweeting. (3) User C, with almost no followers, copied the content posted by User A and published it again, forming a one-step attack sequence. Repeated similar attacks can be seen as a sequence.

inverse reinforcement learning (EntIRL)  to estimate interpretable linear reward function and recover the attack policy. It makes use of mixture-of-experts (MoE) model to cluster expert samples during the IRL process and learns optimal policies by leveraging the strengths of each expert, and provides feature-level explanations. To address the issue of suboptimal linear feature representation of graph structure data, we introduce precise sample guidance and bidirectional update mechanism to speed up the exploration of reinforcement learning and reward function learning.

**Contributions**. i) It studies a novel problem of reconstructing the attack policy with collected adversarial samples on social media. ii) Our approach enhances IRL techniques to handle the graph structured attack samples from diverse adversaries with large social graphs, while also offering interpretability. iii) On the real-world rumor datasets, we validate the policy reconstruction effectiveness of our method for multiple graph adversarial attack methods, and enhance the robustness of the GNNs rumor detector through data argumentation and adversarial training with additional samples generated by the reconstructed policies.

## 2 Background

**Reinforcement learning**. A Markov Decision Process (MDP) is defined as a tuple \((,,,r,)\), where \(\) is a set of states, \(\) is a set of actions, \(\) is the state transition probability function, \(r\) is the reward function, and \(\) is the discount factor. The core objective in RL is to learn a policy \(:\) that maximizes the expected sum of discounted rewards: \(V_{}(s)=_{}[_{t=0}^{}^{t}r(S_{t},A_{t})  S_{0}=s]\), where \(V_{}(s)\) is the state-value function under policy \(\). RL algorithms typically learn the action-value function \(Q_{}(s,a)\), which represents the expected return of taking action \(a\) in state \(s\) and following policy \(\) thereafter. According to the Bellman Optimality Equation, the optimal action-value function is defined as \(Q^{*}(s,a)=[r(S_{t},A_{t})+_{a^{}}Q^{*}(S_{t+1},a^{}) S_{t}=s,A_{t}=a]\). The agent interacts with the environment to collect experiences, and then updates its policy or value functions to improve decision-making over time.

**Inverse reinforcement learning**. The core idea of IRL is to assume that the observed behavior is optimal with respect to some unknown reward function. The task is then to recover this reward function such that the learned policy is (near-)optimal under the recovered rewards. A classical formulation is Maximum Entropy IRL (EntIRL) . Given a set of expert demonstrations \(\), it seeks to recover the reward function with the principle of maximum entropy along the trajectory \(=\{s_{0},a_{0},...,s_{T}\}\). Thus, the objective of EntIRL is defined as:

\[   -p() p()\] (1) s.t. \[  }p()f_{}=,_{ }p()=1,\]

where \(p()\) is the probability distribution of the trajectory, \(f_{}\) is the trajectory feature, and \(\) is the expert feature expectation. EntIRL assumes that \(p() e^{R_{}()}\), where \(R_{}()=_{t}r_{}(s_{t},a_{t})\) is the cumulative reward with the reward function parameter \(\). With the limitation of feature matching, the maximum likelihood method naturally aligns with the maximum entropy principle . Thus, the loss function for EntIRL is the likelihood as: \(L()=_{} p(|)\). The locally optimal example like [27; 28] is considered here. It segments the trajectory as state-action pairs. Denoting the action \(a\) from state \(s\) along the trajectory \(\), the previous assumption becomes \(p(a|s) e^{Q^{*}(s,a)}\). With a discounting factor \(=0\), the action probability is proportional to the exponential of the rewards encountered along \(\):

\[p(a|s)=(r_{}(s,a)),\] (2)

where \(Z\) is the partition factor. The EntIRL loss function then becomes

\[L()=_{a_{s}} p(a|s),\] (3)

where \(_{s}\) is the action space under the state \(s\). The linear reward function \(r_{}(s,a)=^{}f(s,a)\) is adopted here with feature extraction method \(f\). Then, the reward function is employed to train the learner policy \(_{L}\).

**Mixture-of-experts**. MoE is an ensemble model that consists of a gating network \(\) and \(K\) expert networks \(\{p_{1},p_{2},...,p_{K}\}\). Each expert is used to learn and store knowledge from different fields, and the gating network determines the expert network used for the inference based on the input. For each input, the gating network dynamically selects expert networks for activation, which can be indicated as \(p(y|x)=_{k}(x)p_{k}(y|x)\).

**Graph adversarial attack on social networks**. The social network graph is denoted as \(\)=\((,)\). The node set \(\) consists of the nodes representing messages, users and comments. The edge set \(\) consists of pairs \((v_{i},v_{j})\), where \(v_{i},v_{j}\). Each edge or potential edge can be mapped to its relation type with the fucntion \(:\{(v_{i},v_{j})\}\). The relation type set \(\) includes user-message, user-user and message-comment. The communities in social network are represented by a set of connected components \(\{G_{1},...,G_{m}\}\) in \(\), which are termed the _subgraph_ in the sequel.

We focus on node classification task utilizing a GNN model, denoted as \(g\). Each node \(v_{i}\) in \(\) is associated with a corresponding node label \(y_{i}\). Our setting is transductive, where the test nodes are observed during training without their labels. Within social networks, attackers aim to deceive the trained classifier \(g\) through evasion attacks. We assume that the attackers can only observe a limited number of nodes and manipulate a subset of edges. The observable node set is denoted by \(^{}\). Here the modifiable edge set is defined as \(^{}=\{(v_{i},v_{j}) v_{i}^{},v_{j} ^{},((v_{i},v_{j}))\}\), enabling the manipulation between observable nodes. Additionally, there is a target set \(\), allowing attackers to conduct global or targeted attacks within the controllable range by specifying the target set. The attacker modifies the graph \(\) in \(}\). The objective of attackers is to maximize the cost function \(L_{A}=_{v_{i}}L(g(v_{i}),y_{i})\) in \(}\), where \(L(,)\) is the loss between two input values.

## 3 Method

### Framework

As shown in Figure 2, there are three stages in our situation: attack, reconstruction, and defense. Initially, the attacker on social media perturbs the social network with a sequence of manipulations, constituting an MDP trajectory. Multiple attackers generate \(N\) trajectories, denoted as \(\{_{1},_{2},...,_{N}\}\), where each \(_{j}=(s_{j}^{(0)},a_{j}^{(0)},s_{j}^{(1)},,a_{j}^{(T-1)},s_{j}^{(T)})\). With the variety of attack styles and purposes, our proposal assumes distinct policies \(\{_{0},_{1},...\}\) decide the action at each step. To expedite learning in large social network graphs, it decomposes edge flipping into three steps: source subgraph

Figure 2: The framework of our proposal. There are three stages: attack, reconstruction and defense. An example of attack trajectory in social networks is shown at the top right.

selection, destination subgraph selection, and node pair selection. The final reward of the trajectory is \(r\), which remains unknown to the targeted social media platform. Each state-action pair at time \(t\) could be seen as an observed expert sample, with feature extraction method \(f\) employed to represent these pairs. Given the features of samples, the MoE policy is introduced to enhance the EntIRL method, deducing the reward function with EM algorithm. Then, with the improvement of precise sample guidance and bidirectional update mechanism, the learner policy \(_{L}\) is optimized continuously. The policy \(_{L}\) could generate more trajectories \(\{_{N+1},_{N+2},...\}\) to simulate real attack samples. With data augmentation or adversarial training, it could enhance the robustness of the targeted model. Furthermore, by analyzing the reward function, it provides feature-level explanations of the attack samples.

### Mixture-of-experts maximum entropy inverse reinforcement learning

#### 3.2.1 Mixture-of-experts policy

With \(N\) observed trajectories \(\{_{1},_{2},...,_{N}\}\) and \(_{j}=(s^{(0)}_{j},a^{(0)}_{j},s^{(1)}_{j},,a^{(T-1)}_{j},s^{(T)}_{j})\), we assume that the trajectory is generated by the following model

\[p(|)=p(s^{(0)})_{t=0}^{T-1}p(a^{(t)}|s^{(t)})p(s^{(t+1)}| s^{(t)},a^{(t)}),\] (4)

and the policy is the mixture-of-experts model as

\[p(a^{(t)}|s^{(t)},)=_{k=1}^{K}_{k}(s^{(t)},)p (a^{(t)}|s^{(t)},_{k}),\] (5)

where \(_{k}(s^{(t)},)\) is the gate function parametered by \(\), and \(_{k=1}^{K}_{k}(s^{(t)},)=1\) with given \(t\). There are \(K\) experts with parameters \(=(_{1},...,_{K})\) and each component \(p(a^{(t)}|s^{(t)},_{k})\) represents an expert. With Eq. (2) and \(r_{}(s,a)=^{}f(s,a)\), we formulate the \(k\)-th expert at time \(t\):

\[p(a^{(t)}|s^{(t)},_{k})=^{}f(s^{(t)},a^{(t)}))} {_{a_{s,t}}(_{k}^{}f(s^{(t)},a))},\] (6)

where \(_{s,t}\) is the action space under the state \(s^{(t)}\) of the expert sample, and the denominator could be estimated with sampling.

#### 3.2.2 EM algorithm

Each state-action pair is an observed sample. The expert to produce the \(t\)-th state-action pair in the observed trajectory \(_{j}\) is unknown. The latent variable \(_{jkt}=1\) if the \(t\)-th state-action pair of the trajectory \(_{j}\) is decided by the \(k\)-th expert, otherwise \(_{jkt}=0\). The complete data include observed trajectory \(_{j}\) and unobserved \(_{jkt}\) with \(j=1,2,..,N\). The likelihood function of complete data is

\[P(,|) =_{j=1}^{N}P(_{j},_{j,1,0},_{j,2,0},..., _{jKT})\] \[=_{j=1}^{N}[p(s^{(0)}_{j})_{t=0}^{T-1}p(s^{( t+1)}_{j}|s^{(t)}_{j},a^{(t)}_{j})]_{j=1}^{N}_{t=0}^{T-1} _{k=1}^{K}[_{k}(s^{(t)}_{j})p(a^{(t)}_{j}|s^{(t)}_{j},_{ k})]^{_{jkt}}.\] (7)

Then parameters \(\) are estimated by EM algorithm :

**E-Step**: Given the observed data \((s^{(t)}_{j},a^{(t)}_{j})\) and current parameters \(^{(i)}\), it computes the Q function as

\[(,^{(i)})&= [ P(,|)|a^{(t)}_{j},s^{(t)}_{j},^{(i)} ]\\ &=_{t=0}^{T-1}_{k=1}^{K}_{j=1}^{N}(_{ jkt}_{k}(s^{(t)}_{j})+_{jkt} p(a^{(t)}_{j}|s^{(t)}_{j}, _{k})),\] (8)

where \(_{jkt}=[_{jkt}]\) is the responsibility for the observed sample as

\[_{jkt}=P(_{jkt}=1|a^{(t)}_{j},s^{(t)}_{j},^{(i)})= (s^{(t)}_{j})p(a^{(t)}_{j}|s^{(t)}_{j},^{(i)}_{k} )}{_{k=1}^{K}_{k}(s^{(t)}_{j})p(a^{(t)}_{j}|s^{(t)}_{j},^{ (i)}_{k})}.\] (9)

**M-Step**: The goal is updating \(\) with

\[^{(i+1)}=_{}Q(,^{(i)}).\] (10)

Then the gate function loss and the expert loss for the \(k\)-th expert are respectively

\[L_{gate}()=_{t=0}^{T-1}_{k=1}^{K}_{j=1}^{N} _{jkt}_{k}(s_{j}^{(t)}),\] (11) \[L_{ex}(_{k})=_{t=0}^{T-1}_{j=1}^{N}_{jkt } p(a_{j}^{(t)}|s_{j}^{(t)},_{k}).\] (12)

The expert loss shares a conceptual basis with the EntIRL loss as Eq. (3), thus the EM algorithm can be employed to solve the IRL problem. With Eq. (6), the gradient of the normalized expert loss is

\[ L_{ex}(_{k})=_{k}-_{t=0}^{T-1}_{j=1 }^{N}_{jkt}_{a_{s_{j},t}}p(a|s_{j}^{(t)}, _{k})f(s_{j}^{(t)},a).\] (13)

where \(_{s_{j},t}\) is the action space for state \(s_{j}^{(t)}\), and \(_{k}\) is the feature expectations for the \(k\)-th expert:

\[_{k}=_{t=0}^{T-1}_{j=1}^{N}_{jkt}f(s_ {j}^{(t)},a_{j}^{(t)}).\] (14)

It updates each \(_{k}\) with gradient ascent and action space sampling. With learned \(_{k}\), the reward can be estimated by

\[r_{}(s,a)=_{k=1}^{K}_{k}(s)_{k}^{}f(s,a).\] (15)

### Improvement mechanism

**Precise sample guidance**. In inverse reinforcement learning, the objective is to obtain an accurate reward function from expert demonstrations, enabling the learner policy to approximate the expert's behavior. Adversaries utilize specific feature extraction methods \(f^{}\), to obtain embeddings for each attack behavior. However, in the context of social media, the feature extraction method \(f^{}\) employed by the attack model is unknown. The surrogate feature extraction method \(f\) is employed to simulate the input of the attack model, replacing \(f^{}\) with \(f\). This imprecise feature representation implies a sensitive mapping from features to rewards, where minor discrepancies could lead to significantly different attack rewards [12; 30].

According to the reconstruction process along the black arrows in Figure 2, the learner policy \(_{L}\) cannot directly observe the raw expert samples. The information delivering of expert samples involves feature extraction, reward function estimation and RL policy update. This process necessitates both computations and sampling. Deviations in feature representation can accumulate, resulting in a reward function that inadequately guides policy learning. Consequently, we introduce expert structural perturbations directly during the policy learning process, allowing the learner policy to replicate expert sample actions rather than relying on the imprecise features. Specifically, in the initial stage of policy learning, we enforce the learner policy to execute expert actions at a predetermined frequency and assign maximum reward values to the trajectory.

**Bidirectional update mechanism**. While the precise guidance mechanism speeds up exploration in reinforcement learning, it does not facilitate reward function learning in inverse reinforcement learning. Reward function learning relies solely on the EM algorithm with expert demonstrations as input, as indicated by Eq. (13). Both feature representation deviations and action space sampling also impact reward function learning. By executing expert demonstrations through precise guidance and assuming they yield maximum rewards, we can incorporate this information into the parameter updates of the reward function. Specifically, during the precise sample guidance phase, we perform inverse updates with the loss

\[L_{inv}(_{k})=_{k}(s)L(,_{k}^{}f(s,a)).\] (16)

where \((s,a)\) is the expert sample selected for enforcement and \(\) is the maximum historical reward value. This process provides feedback opposite to the output of the reward function, ensuring synchronized learning of the learner policy and the reward function. During normal reinforcement learning phases without precise sample guidance, we update \(_{k}\) according to Eq. (13).

### Threat model and defense on social networks

```
0: Expert demonstration set \(\), number of expert demonstration \(N\), number of experts \(K\), length of trajectories \(T\), number of episodes \(E\), feature extraction function \(f\), gate function \(\), reward function parameters \(=(_{1},,_{K})\), learner policy \(_{L}\), negative sample set \(^{}\), responsibility matrix \(^{N T K}\), inverse update episode set \(\)
0: Learner policy \(_{L}\)
1for\(e=1,2,,E\)do
2\(s=()\);
3for\(t=1,2,,T\)do
4if\(e\)then
5\(s^{},a=()\);
6\(r=()\);
7else
8\(s^{},a=(s,_{L})\);
9\(r=(,f(s,a),)\) as Eq. (15);
10\(_{L}=(s,a,s^{},r)\);
11\(s=s^{}\);
12\(=(,,^{}, )\) as Eq. (9);
13for\(k=1,2,,K\)do
14if\(e\)then
15\(_{k}=(,,_{k})\) as Eq. (16);
16else
17\(_{k}=(,,^{}, _{k})\) as Eq. (13);
18\(=()\) with the loss as Eq. (11); ```

**Algorithm 1**MoE-BiEntIRL

With precise sample guidance and bidirectional update mechanism, we improve the mixture-of-experts EntIRL to MoE-BiEntIRL as the threat model to reconstruct the attack policy. The overall algorithm is as shown in Algorithm 1 and the time complexity analysis is shown in Appendix D. There are some details of MoE-BiEntIRL for the node classification task on social media.

**Hierarchical reinforcement learning.** Inspired by , the hierarchical RL is employed and improved here, which includes three layers as illustrated in Figure 2: the source subgraph, the destination subgraph, and the node pair. In the source or destination subgraph layer, the state is the graph \(\) and the action is a subgraph \(G_{i}\) or \(G_{j}\) at time \(t\). In the node pair layer, the state and the action are the subgraph pair \((G_{i},G_{j})\) and a node pair \((v_{m},v_{n})\), respectively. Each layer is governed by a policy. The state-action pairs correspond to the selection of source subgraph \(G_{i}\), destination subgraph \(G_{j}\) or node pair \((v_{m},v_{n})\). We employ a linear action-value function to learn the policy with LinUCB algorithm , and it could be replaced with other RL methods.

**Interpretable features**. For feature extraction method \(f\), comprehensible features can be employed to represent attack actions within the graph, as suggested by , facilitating the derivation of an interpretable reward function. The graph and node features designed in  are utilized here, focusing specifically on targeting rumor detectors. The details are shown in Appendix C.

**Sampling**. In the process of IRL, two sampling procedures are involved, as indicated in Eq. (6) and Eq. (13). These procedures necessitate the sampling of state-action pairs to represent the action space. Here similarity negative sampling is adopted under specified assumptions. This method selects state-action pairs \((s,a)\) based on the following criteria: i) Samples with a high similarity to expert samples are prioritized, under the condition that the cosine similarity \(cos(f(s,a),f(s^{},a^{}))<\), where \((s^{},a^{})\) represents an expert sample. ii) In cases where \((s,a)\) represents the selection of a subgraph, there must be target nodes in the source subgraph \(G_{i}\) and controllable nodes in the destination subgraph \(G_{j}\).

**Defense with adversarial samples**. With the trained learner policy parameterized by \(\), we can generate additional samples to attack the targeted model \(g\) parameterized by \(\). Denoting the ground truth for node \(v_{i}\) as \(y_{i}\), the predictions on clean and perturbed graphs are represented by \(y^{}_{i}\) and \(}^{{}^{}}\), respectively. Robustness of the targeted model can be improved through offline data augmentation or online adversarial training. The overall loss is given by

\[L_{D}=_{i}L_{}(y_{i},y^{}_{i})+_{i}L_{,}( y_{i},}^{{}^{}}).\] (17)

In data augmentation, \(\) is updated while \(\) remains fixed. In adversarial training, the process can be viewed as a minimax game: \(_{}_{}L_{D}(,)\). The attacker adjusts \(\) to maximize the loss, while the defender alternately updates \(\) to minimize the loss.

Experiments

**Dataset**. Our focus is on the rumor detection task, for which we conduct experiments on two real-world datasets: Weibo  and Pheme (event _ferguson_ in ). These datasets contain both rumors and non-rumors, along with associated user, reposting, and comment data. Specifically, due to the limited number of following relationships among users, we connect edges between user nodes as described in , leveraging the potential user communities inferred from their posting messages. Dataset statistics are provided in Table 1. The datasets are split into training and testing sets using a 7:3 ratio. We reconstruct the policy during the training phase and implement the defense during the testing phase. In the training set, 20% of the authors and their posting messages are designated as controllable nodes, while all nodes in the testing set are considered controllable. All rumor nodes in the controllable set are regarded as target nodes, forming the set \(\). Attackers are only permitted to add edges between controllable users and messages.

**Target model**. The rumor detection model is a 2-layer GCN . The detection accuracy is shown in Table 3. The hidden layer dimension is 64. Message node embedding is represented using a fixed text embedding layer during attacks. It is trained over 60 and 120 epochs for Weibo and Pheme, respectively, and employs the Adam optimizer with a learning rate of 0.0001.

**Metric**. Attack performance is measured using

\[ L_{A}=L_{A}(0)-L_{A}(T).\] (18)

Here, the attack loss \(L_{A}=_{v_{i}}(g(v_{i})-y_{i})\) represents the total loss between the rumor probability and the ground truth for the target nodes in \(\). \(L_{A}(0)\) and \(L_{A}(T)\) denote the attack loss in the clean graph and after \(T\)-step attacks, respectively. \(T\) serves as the horizon for RL and defines the budget for graph adversarial attacks. Specifically, it limits the modification of \(T\) edges when attacking.

**Attack method**. The expert samples are collected through four graph adversarial attack methods, categorized based on their attack cost. Rule-based _PageRank_ and black-box _GC-RWCS_ are considered low-cost attacks. High-cost attacks include _PR-BCD_ with a white-box setting and _AdRumor-RL_ with complete feature knowledge.

* _PageRank_. This method establishes connections between users and messages with high influence, measured using the PageRank algorithm. Inspired by  and , it selectively links rumors with normal users or non-rumors with malicious users.
* _GC-RWCS_. Utilizing a black-box node selection strategy, this method employs a greedy procedure to determine node importance scores. Here it connects messages with high importance scores to influential users, utilizing the same influence measure and limited edge types as _PageRank_.
* _PR-BCD_. This is a sparsity-aware first-order optimization graph adversarial attack method targeting GNNs in a white-box setting. It proposes the surrogate loss for global attacks.
* _AdRumor-RL_. This hierarchical contextual bandit attack framework targets GCN-based rumor detectors using interpretable features. It is an RL-based method to produce the serialized attack trajectories with a black-box setting.

**Attack performance evaluation of policy reconstruction.** We reconstruct policies using expert samples generated by the aforementioned attack methods via MoE-BiEntIRL. The IRL process consists of 1000 episodes. The precise sample guidance and bidirectional update mechanism is applied every two episodes during the first 500 episodes. For each attack method, we select the top one to three attack trajectories based on performance as expert samples. The number of experts, \(K\), is estimated using DBSCAN and is typically adjusted to range from 1 to 25, often being less than 10. The gate function is pre-trained with the labels generated by Gaussian Mixture Model (GMM) clustering. To prevent cluster collapse, we adopt the weighted sum of one-hot labels and gate function outputs as the MoE gate. Each expert sample is augmented with 100 negative samples with a sampling upper bound \(=0.8\). All experiments are conducted using GTX 2080Ti (11GB) GPUs.

We compare MoE-BiEntIRL with two classical IRL methods: (i) _Apprenticeship Learning_, which is based on maximum margin and faces an ill-posed problem introducing ambiguity. (ii) _EntIRL_

    & Weibo & Pheme \\  Nodes & 10,280 & 2,708 \\ Edges & 16,412 & 4,401 \\ Rumors & 1,538 & 284 \\ Non-rumors & 1,849 & 859 \\ Users & 2,440 & 1,008 \\ Comments & 4,453 & 557 \\   

Table 1: Dataset statistics.

, which applies the maximum entropy theory to IRL to alleviate ambiguity. The performance is shown in Table 2. Our findings indicate that: i) MoE-BiEntIRL outperforms the expert policy on Weibo. ii) Our method excels in reconstructing policies for high-cost attacks, while for simple low-cost attacks, _EntIRL_ often outperforms due to _Occam's Razor_; however, our method also achieves comparable results. iii) Despite suboptimal effects on Pheme compared to experts, other IRL methods struggle to learn the policy, showcasing the difficulty of policy recovery. Furthermore, we validate the effectiveness of the precise sample guidance and bidirectional update mechanism, as depicted in Figure 3, particularly advantageous in challenging policy reconstruction scenarios.

**The improvement of robustness with generated samples**. We assess the efficacy of samples produced by MoE-BiEntIRL in enhancing robustness by subjecting the target model to attacks from _PageRank_, _GC-RWCS_, and _PR-BCD_. Evaluation is conducted under various conditions: no defense, data augmentation with expert samples (EDA), data augmentation with generated samples (DA), and adversarial training (AT). The trade-off parameter \(\) in Eq. (17) is set to 8. Results are presented in Table 3. Simply using all expert samples for data augmentation does not yield effective defensive results. In contrast, employing additional samples generated by MoE-BiEntIRL exhibit the highest or second-highest improvement in robustness while maintaining accuracy in the clean graph.

**Case study for interpretability**. Our analysis focuses on the expert samples generated by _AdRumorRL_, which relies on a linear action-value function, with the policy parameter providing insight into feature importance. In our approach, we determine the feature importance of the sample \((s,a)\) through the calculation \(_{k}^{}_{k}(s)\). Table 4 illustrates the top eight important features elucidated

    & &  &  \\   & & _PRBCD_ & _AdRumor_ & Mixture & _PageRank_ & _GC-RWCS_ & Mixture \\   & Expert & 4.865 & 4.877 & - & 3.000 & 3.000 & - \\  & _Apprenticeship_ & 1.275 & 0.788 & 0.704 & 0.850 & 0.763 & 1.071 \\ T=5 & _EntIRL_ & 4.650 & 4.770 & 4.550 & **5.000** & **4.950** & **4.950** \\  & _MoE-BiEntIRL_ & **4.989** & **4.990** & **4.929** & 4.860 & 4.900 & 4.900 \\   & Expert & 19.521 & 19.854 & - & 5.449 & 5.160 & - \\  & _Apprenticeship_ & 1.142 & 3.066 & 3.945 & 0.030 & 0.040 & 0.020 \\ T=20 & _EntIRL_ & 19.030 & 19.749 & 19.199 & 19.830 & **20.000** & **20.000** \\  & _MoE-BiEntIRL_ & **19.876** & **19.936** & **19.979** & **19.970** & 19.700 & 18.749 \\   & Expert & 4.804 & 5.947 & - & 2.991 & 3.990 & - \\  & _Apprenticeship_ & 1.788 & 3.387 & 2.619 & 0.000 & 0.000 & 0.000 \\ T=5 & _EntIRL_ & 0.000 & 0.018 & 0.010 & 0.000 & 0.062 & 0.000 \\  & _MoE-BiEntIRL_ & **2.205** & **4.965** & **4.277** & **1.488** & **2.105** & **1.549** \\   

Table 2: The attack performance of policy reconstruction evaluated using the average value of the last 100 episodes in the metric \( L_{A}\) as shown in Eq. (18). A higher \( L_{A}\) means better performance. The rows and columns correspond to the IRL methods and attack models used to generate expert samples, respectively. The row Expert is the average performance of the expert samples. In the column of Mixture, we display the performance of the policy reconstructed with expert samples from all low or high cost attack methods.

Figure 3: The smoothed curves of the ablation experiments for precise sample guidance and bidirectional update mechanism when recovering the policy of _AdRumor-RL_. The terms _w/o Bi.Update_ and _w/o P&B_ denote the removal of the bidirectional update mechanism and the removal of both improvement modules, respectively.

by both _AdRumor-RL_ and our method. The reward function enables the capture of the majority of important features.

## 5 Related Work

**Graph adversarial attack and defense**. Graph adversarial attacks include poisoning  and evasion attacks , as well as global  and targeted attacks , spanning both white-box and black-box approaches [10; 11]. Adversarial samples are utilized in numerous studies to train robust GNNs through adversarial training techniques [18; 13; 14]. As for rumor detection, some studies explore graph adversarial attacks on social networks, as evidenced by [1; 4; 12; 41; 42; 43; 44].

**Inverse reinforcement learning**. It includes maximum margin-based approaches like apprenticeship learning  and probability model-based methods such as EntIRL  and REIRL . Regarding explainable IRL,  explores potential clustering factors in demonstrations, offering expert-level explanations.  quantifies the importance of different goals in ICU hypotension management with linear reward function. Additionally,  also explores the combination of MoE and EntIRL based on decision trees.

## 6 Conclusion

We propose MoE-BiEntIRL, a threat model to recover the graph adversarial attack policy against GNN model on social media. It utilizes the multi-source graph structured attack trajectories to learn a generalized policy based on IRL techniques and MoE mindset, and provides feature-level explanations. The precise sample guidance and bidirectional update mechanism are designed to deal with the deviation caused by feature representation and negative sampling. Leveraging samples produced by the reconstructed policy, it could enhance the robustness of the target model. The broader impact and the limitation of our work are shown in Appendix A and B, respectively.

 p{14.2pt} p{14.2pt}|}   Expert sample & Reward function \\   \\  max potential & rumor review & n nodes \\ avg degree & n edges & n edges \\ n nodes & max degree & \\ author ratio & max potential & \\ n edges & message ratio & \\ avg potential & max rumor inf & \\ author inf min & & \\   \\   & |}{} & |}{} \\  avg potential & user inf min & \\ rumor ratio & avg user inf & \\ rumor inf & & & \\ min user inf & & & \\ min author inf & & & \\ min author inf & & & \\ avg user inf & & & \\ avg author inf & & & \\ max author inf & & & \\ max number inf & & & \\ max user inf & & & \\   

Table 4: The top-8 important features for subgraph selection with \(T=5\) on Weibo dataset, reflected by _AdRumor-RL_ expert samples and the learned reward function, respectively. The overlapping features are marked with the gray background. The features are described in Appendix C.

 p{14.2pt} p{14.2pt} p{14.2pt} p{14.2pt} p{14.2pt}}    & & w/o Att. & _PageRank_ & _GC-RWCS_ & _PR-BCD_ \\    } & w/o Def. & 70.4031 & -0.4042 & -0.4406 & -0.1966 \\  & _PageRank_ & 70.5998 & **-0.1821** & **-0.2440** & **0.0000** \\  & _GC-RWCS_ & 70.7965 & -0.4043 & -0.4407 & -0.1967 \\  & _PR-BCD_ & 70.3048 & -0.2185 & **-0.2440** & **0.0000** \\  & _AdRumor-RL_ & 70.7965 & \(\)-0.2076 & **-0.2440** & **0.0000** \\  & All above & 70.7965 & -0.2805 & -0.3424 & -0.0984 \\    } & _PageRank_ & 70.6981 & -0.5025 & -0.5390 & -0.2950 \\  & _GC-RWCS_ & 70.5015 & -0.3059 & -0.2440 & 0.0000 \\  & _PR-BCD_ & 70.4031 & **-0.1092** & **-0.1456** & **0.0984** \\  & _AdRumor-RL_ & 70.6981 & -0.3059 & -0.3423 & -0.0983 \\  & _MoE-BiEntIRL_ & 70.6981 & **-0.1092** & **-0.1456** & **0.0984** \\    } & _PageRank_ & 71.0914 & **-0.2075** & **-0.2440** & **0.0000** \\  & _GC-RWCS_ & 70.2065 & -0.4042 & -0.4407 & -0.1967 \\  & _PR-BCD_ & 70.4031 & -0.3059 & -0.3423 & -0.0983 \\  & _AdRumor-RL_ & 70.6981 & -0.3059 & -0.3423 & -0.0983 \\  & _MoE-BiEntIRL_ & 72.0747 & \(\)-0.2731 & \(\)-0.2589 & **0.0000** \\   

Table 3: The test accuracy decline (%) of the GCN rumor detector with \(T=5\) on Weibo dataset. The first row displays the attack method. The first column is the way to enhance the robustness with adversarial samples. The second column is the method to generate the samples. The column under w/o Att. reflects test accuracy without attacks, while results under other columns reflect accuracy decline. The second row (w/o Def.) shows the accuracy (decline) without any defense method. **Boldfaced font** and \(\) mean the best performance and the runner-up among all methods respectively.