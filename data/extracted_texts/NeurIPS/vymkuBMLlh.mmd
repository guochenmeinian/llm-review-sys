# Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand

Md Musfiqur Rahman

Purdue University

&Matt Jordan

University of Texas at Austin

&Murat Kocaoglu

Purdue University

Equal contribution. Correspondence to rahman89@purdue.edu

###### Abstract

Causal inference from observational data plays critical role in many applications in trustworthy machine learning. While sound and complete algorithms exist to compute causal effects, many of them assume access to conditional likelihoods, which is difficult to estimate for high-dimensional (particularly image) data. Researchers have alleviated this issue by simulating causal relations with neural models. However, when we have high-dimensional variables in the causal graph along with some unobserved confounders, no existing work can effectively sample from the un/conditional interventional distributions. In this work, we show how to sample from any identifiable interventional distribution given an arbitrary causal graph through a sequence of push-forward computations of conditional generative models, such as diffusion models. Our proposed algorithm follows the recursive steps of the existing likelihood-based identification algorithms to train a set of feed-forward models, and connect them in a specific way to sample from the desired distribution. We conduct experiments on a Colored MNIST dataset having both the treatment (\(X\)) and the target variables (\(Y\)) as images and sample from \(P(y|(x))\). Our algorithm also enables us to conduct a causal analysis to evaluate spurious correlations among input features of generative models pre-trained on the CelebA dataset. Finally, we generate high-dimensional interventional samples from the MIMIC-CXR dataset involving text and image variables.

## 1 Introduction

Causal inference has recently attracted significant attention in machine learning (ML) due to its application in fairness, invariant prediction, and explainability [60; 62; 49]. Even though existing ML models show notable predictive performance by optimizing the likelihood of the training data, they are prone to failure when the covariate distribution changes in the test domain. Consider the medical scenario in Fig. 1a with the causal order: \((X){}(S){}(R)\) representing the true data-generating mechanisms. Suppose a practitioner observes only \(X\) to make a high-level intermediate diagnosis \(S\) that contains sufficient information about the patient. The prescription \((R)\) is written only based on the diagnosis (thus \(X R\)). Since data are collected from different hospitals locations (\(H\)), \(H\) acts as an unobserved common cause for both \(X\) and \(R\), i.e., \(X R\) (ex: correlation between x-ray artifacts and report writing style). The task is "x-ray to report" generation. One might train an ML model to directly learn a mapping \(f:\), with maximum likelihood estimation (MLE) [7; 10] mimicking the conditional distribution \(P(r|x)\). However, since \(H\) is an unobserved common cause between \(X\) and \(R\); \(H\) has some influence on \(P(r|x)\). Thus, if the model is deployed in a new location, its MLE-based prediction accuracy may drop since \(P(r|x)\) shifts in that location. On the other hand, if we can remove the location bias \(X R\) with an intervention on the x-ray variable (\((x)\)), the x-ray to report generation would be invariant to domain shifts. Thus, to obtain such generalization, we need to perform causal interventions in high-dimension.

Structural causal models (SCM)  enable a data-driven approach to estimate interventional distributions [43; 30]. Given the qualitative causal relations, summarized in a _causal graph_, we now have a complete understanding of which causal effects/queries (ex: \(P(r|(x))\)) can be uniquely identified from the observational distribution and which require further assumptions or experimental data [2; 28; 37; 46; 50]. More precisely, if all conditional probability tables are available, sound and complete identification algorithms [51; 46] can perform exact inference to estimate causal effects or [4; 5; 25] can sample from the interventional distribution, using a combination of marginalization and product operators applied to those conditional distributions. However, such approaches struggle to deal with high-dimensional variables. In Fig. 0(a), we could intervene on x-ray \(X\) and estimate its effect on the report \(R\) as, \(P(r|(x))=_{s}P(s|x)_{x^{}}P(x^{})P(r|x^{},s)\), i.e., as functions of the observational distribution (\(X\), \(X^{}\): independent instances of the same variable). However, the second and third terms in this expression require marginalization over the "_X-ray_" variable. Exact Bayesian inference methods used for calculating conditional distributions are infeasible for high-dimensional variables since marginalization over their non-parametric distributions is generally intractable .

Deep generative models with variational inference methods approximate the intractable marginalization and can sample from such high-dimensional distributions [22; 47; 14]. Recent works such as Xia et al. , Chao et al. , Rahman and Kocaoglu  employ deep generative models to match joint distribution of the system by learning the conditional generation of each variable from its causal parents. Nonetheless, it is highly non-trivial for these works to mimic any arbitrary causal model with high-dimensional variables, specially when there are unobserved confounders in the (semi-Markovian) causal model. Consider the \(X R\) relation in Fig. 0(a) where \(R\) and \(X\) are correlated through unobserved hospital location. To learn the joint distribution \(P(x,r)\), the above approaches need to synchronously train their generative models. For that purpose, Xia et al. , Rahman and Kocaoglu  train two GAN networks concurrently by feeding the same prior noise. However, it is nontrivial to design a loss function for the joint distribution balancing multiple high-dimensional variables making it challenging for the discriminator to detect true/false sampled pairs. Thus, the high-dimensional intervention problem still requires a more effective approach.

In this paper, we propose a novel algorithm ID-GEN that can _utilize any (conditional) generative models (such as GANs or diffusion models) to perform high-dimensional interventional sampling in the presence of latent confounders._ For this purpose, we resort to the sound and complete identification algorithm [50; 46] and design our algorithm on top of its structure to sample from any identifiable causal query which may have an arbitrarily complex probabilistic expression (ex: Eq. 1). More precisely, given a causal graph, training data, and a causal query, our algorithm _i)_ follows the recursive trace of the ID algorithm to factorize the query _ii)_ trains a set of conditional models for each factor, _iii)_ connects them to build a neural network called sampling network and generate interventional samples from this network. For example, to sample from \(P(r|(x))\) for the frontdoor graph in Fig. 1 (top), we _i)_ utilize ID to obtain the factors: \(P(s|(x))\) and \(P(r|(x,s))\), _ii)_ train conditional models \(\{M_{S}\},\{M_{X^{}},M_{R}\}\) for the two factors (Fig. 0(b), _iii)_ merge all models based on input-output (Fig.0(c)). Sampling according to this network's topological order would produce samples

Figure 1: (Top: x-ray to report generation task) (a) \((X=x)\) removes the \(X R\) bias and makes the generation of \(R\) domain invariant. \(P(r|(x))\) is factorized into c-factors and (b) conditional models (\(\{M_{V_{k}}\}_{k}\)) are trained for each factor (shown as boxes). (c) The intervened value \(X=x\) is propagated through the merged network and samples from the \(P(r|(x))\) are generated.

from \(P(r|(x))\). Similarly, for the backdoor graph (bottom), we train conditional models \(\{M_{A,B}\}\) and \(\{M_{I}\}\) to learn \(P(a,b)\) and \(P(i|v,a,b)\) respectively and merge them to sample from \(P(i|do(v))\). To the best of our knowledge, we are the first to show that conditional generative/feedforward models are sufficient to sample from any identifiable causal effect estimand. Our contributions are as follows:

* We propose a recursive algorithm called ID-GEN that trains a set of conditional generative models on observational data to sample from high-dimensional interventional distributions. We are the first to use diffusion models as conditional models for semi-Markovian SCMs.
* We show that ID-GEN is sound and complete, establishing that conditional generative models are sufficient to sample from any identifiable interventional and conditional interventional query. The latter type are especially challenging for existing GAN-based causal models.
* We demonstrate ID-GEN's performance on three datasets containing image and text variables. First, we perform image intervention with diffusion models for the Colored-MNIST experiment. Next, we show our application in trustworthy AI through quantifying spurious correlations in pre-trained models for the CelebA dataset. Finally, we make the report to X-ray generation task interpretable and domain invariant based on the MIMIC-CXR dataset.

## 2 Background

**Structural causal model**, (SCM)  is a tuple \(=(G=(,),,,,P (.))\). \(=\{V_{1},...,V_{n}\}\) is a set of observed variables in the system. \(\) is a set of independent exogenous random variables where \(N_{i}\) affects \(V_{i}\) and \(\) is a set of unobserved confounders each affecting any two observed variables (for \(>2\) check Appendix C.7). This refers to the **semi-Markovian causal model**. A set of deterministic functions \(\)=\(\{f_{V_{1}},f_{V_{2}},..,f_{V_{n}}\}\) determines the value of each variable \(V_{i}\) from other observed and unobserved variables as \(V_{i}=f_{i}(Pa_{i},N_{i},U_{S_{i}})\), where \(Pa_{i}\) (parents), \(N_{i}\) (randomness) and \(U_{S_{i}}\) (**latent confounders**) for some \(S_{i}\). \((.)\) is a product probability distribution over \(\) and \(\) and projects a **joint distribution**\(_{}\) over the set of actions \(\) representing their likelihood.

An SCM \(\), induces an **acyclic directed mixed graph** (ADMG) \(G=(,)\) containing nodes for each variable \(V_{i}\). For each \(V_{i}=f_{i}(Pa_{i},N_{i},U_{S_{i}})\), \(Pa_{i}\), we add an edge \(V_{j} V_{i}, V_{j} Pa_{i}\). Thus, \(Pa_{i}(V_{i})\) becomes the parent nodes in \(G\). \(G\) has a **bi-directed edge**, \(V_{i} V_{j}\) between \(V_{i}\) and \(V_{j}\) if and only if they share a latent confounder. If a path \(V_{i} V_{j}\) exists, then \(V_{i}\) is an ancestor of \(V_{j}\), i.e., \(V_{i}=An(V_{j})_{G}\). An **intervention**do\((x)\) replaces the structural function \(f_{x}\) with \(X=x\) and in other structural functions where \(X\) occurs. The distribution induced on the observed variables \(V\) after such an intervention is represented as \(_{x}(v)\) or \((v|(x))\). Graphically, it is represented by \(G_{}\) where incoming edges to \(X\) are removed (marked red). With a slight abuse of notation, we will use \(P(y)\) for both the numerical value \((Y=y)\) and the probability distribution \([P(y)]_{y}\), depending on the context. An example for the latter is:"_Let \(Y\) be sampled from \(P(y)\)"_. Also, \(P_{x}(y)\) refers to the **interventional distribution** for all \(x,y\). Given an ADMG \(G\), a maximal subset of nodes where any two nodes are connected by bidirected paths is called a **c-component**\(C(G)\). For any \(S C(G)\), \(P(S|( S))\) is called a c-factor. We assume that we have access to the ADMG through some causal structure learning algorithm and expert knowledge. **Classifier-free diffusion guidance ** Let \((,) P(,)\) be the data distribution and \(=\{_{}|[_{min},_{max}]\}\) for \(_{min}<_{max}\). We coverrupt the data as \(_{}=_{}+_{}\) and optimize the denoising model by taking the gradient step on \(_{}||_{}(_{},)-||^ {2}\). Given that variables \(\) are connected as a directed acyclic graph and we have diffusion models trained to learn the distributions \(P(v_{i}|pa(v_{i}))\), we can perform **ancestral sampling** from the joint distribution, \(P()=_{V_{i}}P(v_{i}|pa(v_{i}))\) by making one pass through each model in the topological order while sampling from the conditional distributions . We choose classifier-free diffusion as our conditional model, but the choice changes based on the application. We use \(M_{V}(c)\) and a square node as notation.

## 3 ID-GEN: generative model-based interventional sampling

Given a causal graph \(G\), dataset \( P()\), our objective is to generate high-dimensional interventional samples from a query \(P(|())\) or a conditional query \(P(|(),)\). ID-GEN builds upon the recursive structure of the identification algorithm  to train necessary conditional models. Thus, we first discuss its connection with us and show the challenges it faces if deployed for sampling.

### Identification algorithm (ID) and challenges with high-dimensional sampling

Shpitser and Pearl  propose a recursive algorithm (Algorithm 6) for estimating an interventional distribution \(P_{}()\) given access to all probability tables. At any recursion level, it enters one of its four recursive steps: 2, 3, 4, 7 and three base case steps: 1, 5, 6. Below, we discuss them in detail.

**Step 1** occurs when the intervention set \(\) is empty in \(P_{}()\). The effect of \(=\) on \(\) is its marginal \(P()\) which is returned as output. **Step 2** checks if there exists any non-ancestor variable of \(\) in the intervention set \(\). Such variables in the graph do not have any causal effect on \(\). Thus, it is safe to drop them. In **Step 3**, it searches for a set \(\) in \(G\), which does not effect \(\) assuming that \(\) has already been intervened on. Thus, it can include \(\) as an additional intervention set: \(=\). An intervention on \(\) implies deleting its incoming edges, which simplifies the problem in the future. **Step 4** is the most important line and is executed when there are multiple c-components in the subgraph \(G\). It factorizes (decomposes) the problem of estimating \(P_{}()\) into estimating c-factors (subproblems) and performs recursive calls for each c-factor. Base case **Step 5** returns fail for non-identifiable queries. Base case **Step 6** asserts that when \(\) does not have a bi-directed edge with the rest of the nodes in \(S\) and \(S\) consists of a single c-component, intervening on \(\) is equivalent to conditioning on \(\). Thus, ID can now solve \(P_{}()\) as \(_{s}_{i|V_{i} S}P(v_{i}|v_{}^{(i-1)})\) and return as output. **Step 7** occurs when the variables in \(\) can be partitioned into two sets: one having bi-directed edges to other variables (\(S^{}\)) in the graph and one (defined as \(X_{Z}\)) with no bi-directed edges to \(S^{}\). In that case, evaluating \(P_{}()\) from \(P()\) is equivalent to first obtaining \(P^{}() P_{_{Z}}()\) and then evaluating \(P_{_{Z}}(Y)\) from \(P^{}()\). Hence, \(P_{_{Z}}()\) is first calculated as \(_{\{i|V_{i} S^{}\}}P(V_{i}|V_{}^{(i-1)} S^{},v_{ }^{(i-1)} S^{})\) and then passed to the next recursive call for \((_{z})\) to be applied. One major issue of ID is that it requires probability tables and thus cannot be applied for high-dimensional sampling. Suppose we naively design an algorithm that follows ID's recursive steps and trains a generative model for every factor it encounters and samples from it. This algorithm would not know which of these factors to learn and sample first, leading to a deadlock as shown in Ex C.1. ID-GEN solves such issue by avoiding direct sampling and building a sampling network.

**Definition 3.1** (Sampling network, \(\)).: A collection of feedforward models \(\{M_{V_{i}}\}_{ i}\) for a set of variables \(=\{V_{i}\}_{ i}\) is said to form a **sampling network**, \(\), if the directed graph obtained by connecting each \(M_{V_{i}}\) to \(M_{An(V_{i})_{G}}\) via incoming edges according to some conditional distribution, is acyclic. Two sampling networks \(_{i},_{r}\) can be merged into a larger network \(\).

### Recursive training of ID-GEN and interventional sampling

Similar to ID's recursive structure, ID-GEN has 7 steps (Algorithm 1). However, to deal with high-dimensional variables, we call three new functions: i) Algorithm 2:ConditionalGMs(.) inside steps 1 and 6 where we train diffusion models or other conditional models to learn conditional distributions, ii) Algorithm 3:MergeNetwork(.) inside step 4 to merge the conditional models, and iii) Algorithm 4:Update(.) inside step 7 to train models that can apply part of the interventions and update the training dataset for next recursive calls. We initiate with ID-GEN \((,,G,,}=,=G)\). Along with the given inputs \(,,G,\), ID-GEN maintains two extra parameters \(},\) to keep track of the interventions performed. During the top-down phase, ID-GEN updates its parameters: by \(i)\) removing interventions from the intervention set \(\), and \(ii)\) updating the training dataset \(\), \(}\) and the causal graph \(G,\) according to the interventions. At any level of recursion, an ID-GEN call returns a sampling network \(\) (DAG of a set of trained models) trained on the dataset \(\) to learn conditional distributions according to \(},G,\). After the recursion ends, we can generate samples from \(P_{}(),\), by ancestral sampling on \(\). See a recursion tree in Appendix C.4.

**Base Case: Step 1:** ID-GEN enters step 1 if the intervention \(\) is empty. For \(=\), we have, \(P_{}()=P()=_{}P( )=_{}_{V_{i}}P(v_ {i}|v_{}^{(i-1)})\) which is suitable for ancestral sampling. To train models that can collectively sample from this distribution, we call Algorithm 2:ConditionalGMs(.). Here, we train each model \(M_{V_{i}}\), \( V_{i}\) using \(V_{}^{(i-1)}\), (i.e., variables that are located earlier in the topological order \(\)) as inputs to match \(P(v_{i}|v_{}^{(i-1)})\). Note that \(}\) contains the values that were intervened in previous recursion levels and \(\) is the graph at the current level that contains \(}\) with its incoming edges cut. Since we want our conditional models to generate samples consistent with the values of \(}\), we consider the topological order of \(\) while using\(V_{}^{(i-1)}\) as inputs so that \(}\) are also fed as input while training. After training, we connect the trained models according to their input-outputs to build a sampling network \(\) and return it (Alg 2:lines 1-6). Note that when all variables in \(\) are low dimensional, we can also learn a single model \(M\) to sample \(P()\). However, for high-dimensional variables, matching such joint distributions is non-trivial .

**Step 2 & 3**: We follow the same steps of the ID algorithm as discussed in Section 3.1.

**Step 4 and Merge sampling Networks:** Our goal is to train models that can sample from \(P_{}()\) which unfortunately is not straightforward. This step allows us to decompose our problem into sub-problems and we can train models to sample from the c-factors of \(P_{}()\)'s factorization. The next challenge is to connect these models consistently to sample from \(P_{}()\). More precisely, if we remove \(\) from \(G\) and the graph splits into multiple c-components (variables in each component connected with \(\)) (Alg 1:line 11), we can apply c-component factorization (Lemma D.7, ) to factorize \(P_{}()\) as \(_{()}P_{ s _{1}}(s_{1}) P_{ s_{n}}(s_{n})\) where each \(\{S_{k}\}_{k}\) is the c-factor corresponding to each c-component. To obtain trained models for each of these c-factors, we perform the next recursive calls: ID-GEN (\(=S_{i}\), \(= S_{i},G,,},\)). When these recursive calls return a sampling network \(H_{i}\) for each \(P_{ s_{i}}(s_{i})\), we can wire them based on their input-output to build a single sampling network \(\). According to Theorem D.21 and D.22, \(\) now can sample from \(P_{}()\).

We call Algorithm 3: \((.)\) to connect all sampling networks \(\{_{i}\}_{_{i}}\). Here, each \(_{i}\) is a set of trained conditional models \(\{M_{V_{j}}\}_{j}\) connected to each other as a DAG. If a sampling network \(_{i}\) contains an empty node \(M_{V_{j}}=\) without any conditional model and some other sampling network \(_{r}\) generates this variable \(V_{j}\) with its node \(M_{V_{k}}\), i.e., \(V_{j}=V_{k}\), then we combine \(M_{V_{j}}\) and \(M_{V_{k}}\) into the same node to build a connection between \(_{i}\) and \(_{r}\) (lines 3-6). Intuitively, due to the c-factorization at this step, the variables intervened in one sampling network might be generated from models in another network. We connect two networks to continue the ancestral sampling sequence. Fig. 2 shows an example of this step where \(P_{x}(y)\) is factorized into c-factors as \(P_{x}(y)=_{w_{1},w_{2}}P_{x,w_{1}}(w_{2})P_{x,w_{2}}(w_{1},y)\). For the c-component \(\{W_{1},Y\}\), ID-GEN first obtains \(P_{x,w_{2}}(w_{1},y)=P(w_{1}|x)P(y|x,w_{1},w_{2})\), and trains conditional models \(M_{W_{1}}\) and \(M_{Y}\) for these conditional distributions. Similarly, for \(\{W_{2}\}\), we have \(P_{x,w_{1}}(w_{2})=_{x^{}}P(x^{})P(w_{2}|x^{},w_{1})\) and we train \(M_{X^{}}\) and \(M_{W_{2}}\). Finally, we merge these networks based on inputs-outputs to build a single sampling network and perform ancestral sampling on it to sample from \(P_{x}(y), x\).

**Base Case: Step 5:** We follow the step 5 of the ID algorithm as discussed in Section 3.1.

**Base Case: Step 6:** We enter Step 6 if \(G\) is a single c-component \(S\), and \(\) is located outside the c-component. This situation allows us to replace the intervention on \(\) by conditioning on \(\): \(P_{}()=_{s}_{V_{i} S}P(v_{ i}|v_{}^{(i-1)})\). This step is similar to step 1, except that now we have a non-empty intervention set, i.e., \(\). Here, we consider the topological order of \(\) and \(V_{}^{(i-1)}\) contains both \(\) and \(}\). We call Algorithm 2:ConditionalGMs(.) which trains multiple conditional models to learn the above distribution. More precisely, we utilize classifier-free diffusion guidance for conditional training of each \(M_{V_{i}}\) by taking the gradient step on \(_{}||_{}(_{}^{i},v_{}^{(i-1)})- ||^{2}\). Here, \(z_{}^{i}\) is the noisy version of \(V_{i}\) at time step \(\) during the forward process and \(v_{}^{(i-1)}\) is the condition (see Background). Finally, we connect the input-output of these diffusion models according to the topological order to build a sampling network and return it as output. Note that for any specific conditional distribution, if we have access to a pre-trained models that can sample from it, we can directly plug it in the network instead of training it from scratch (motivated from ).

**Step 7:** Here, ID-GEN partitions \(\) into two sets: one is applied in the current step to update the training dataset and other parameters, and the other is kept for future steps. It performs this step if i) \(G\) is a single c-component \(S\) and ii) \(S\) is a sub-graph of a larger c-component \(S^{}\) in the whole graph \(G\), i.e, \((S=C(G))(S^{} C(G))\). For example, in Fig. 3, for \(P_{w_{1},w_{2},x}(y)\), we have \(S=G\{W_{1},W_{2},X\}=\{Y\},S^{}=\{W_{1},X,Y\}\). In this step, we call Algorithm 4: Update(.)which utilizes the larger c-component \(S^{}\) to partition the intervention set \(\) into one set contained within \(S^{}\), i.e., \( S^{}\), and another set not contained in \(S^{}\), i.e., \(_{Z}= S^{}\). Evaluating \(P_{}()\) from \(P()\) is equivalent to evaluating \(P_{^{} s^{}}()\) from \(P^{}()\) where \(P^{}() P_{_{z}}()\) is the joint distribution. Hence, we first perform do(\(_{Z}\)) to update the dataset as \(^{}\). Next, we shift our goal of sampling from \(P_{}()\) in \(G\) with training dataset \( P()\) to sampling from \(P_{^{} s^{}}()\) in \(_{\{S^{},}\}}\) with training data \(^{} P_{_{z}}()\) in the next recursive calls. To generate dataset \(^{} P_{_{z}}()\), we call ConditionalGMs(.) and use the returned network to sample \(^{}\) (lines 2-3).

Note that given access to probability tables, the ID can use any specific value \(_{Z}=_{z}\) to calculate \(P_{_{z}}()\) to get the correct estimation of \(P_{}()\) (Verma constraint ). In our case, if we use a specific value \(_{z}\) to sample the training dataset \(^{} P_{_{z}}()\), the models trained on this dataset in subsequent recursive steps will also depend on \(_{z}\). However, during ancestral sampling in the returned network, a different value \(_{Z}=_{z}^{}\) might come from other c-components (ex: \(M_{Y}(W_{2},.)\) in Fig. 3). Thus, to make our trained models suitable for any values, we pick \(_{Z}\) from a uniform distribution or from \(P(_{Z})\) and generate \(^{}\) accordingly. We save \(_{Z}\) in \(}\), its values in \([},S^{}]\) and in \(_{\{S^{},}\}}\) with incoming edges removed, to be considered during training in the next recursive calls. Whenever ID-GEN visits Step 7 again, \(}\) will be applied along with the new \(_{Z}\). Finally, a recursive call is performed with these updated parameters (line 4) which will return a network trained on dataset \(^{} P_{_{z}}()\). It can sample from \(P_{^{} S^{}}()\) and equivalently from the original \(P_{}()\).

```
1:for each \(V_{i}\{}\}\)do
2: Add node \((V_{i},)\) to \(\) {Initialized \(=\)}
3:for each \(V_{i}\) in the topological order \(_{}\)do
4: Let \(M_{V_{i}}\) be a model trained on \([V_{i},V_{}^{(i-1)}]\) such that \(M_{V_{i}}(V_{}^{(i-1)}) P(v_{i}|v_{}^{(i-1)})\)
5: Add node \((V_{i},M_{V_{i}})\) to \(\)
6: Add edge \(V_{j} V_{i}\) to \(\) for all \(V_{j} V_{}^{(i-1)}\)
7:Return\(\). ```

**Algorithm 2**ConditionalGMs(\(\),\(\),\(G\),\(\),\(}\),\(\))

```
1:Input: Set of sampling networks \(\{_{i}\}_{ i}\).
2:Output: A connected DAG sampling network \(\).
3:for\(H_{i}\{_{i}\}_{ i}\)do
4:for\(M_{V_{i}}_{i}\)do
5:if\(M_{V_{i}}=\) and \( M_{V_{k}}_{r}, r\) such that \(V_{j}=V_{k}\) and \(M_{V_{k}}\)then
6:\(M_{V_{j}}=M_{V_{k}}\)
7:Return\(=\{_{i}\}_{ i}\) {All \(_{i}\) are connected.} ```

**Algorithm 3**MergeNetwork(\(\{_{i}\}_{ i}\))

```
1:\(_{Z}= S^{}\)
2:\(=(S^{},_{Z},G,, },)\)
3:\(^{}(_{Z},})\); \(}=}_{}\)
4:Return\(, S^{},G_{S^{}},^{}[ },S^{}],},_{\{S^{},}\}}\) ```

**Algorithm 4**Update(\(S^{},,G,,},\))

```
1:\(_{Z}= S^{}\)
2:\(=(S^{},_{Z},G,, },)\)
3:\(^{}(_{Z},})\); \(}=}_{}\)
4:Return\(, S^{},G_{S^{}},^{}[ },S^{}],},_{\{S^{}, }\}}\) ```

**Algorithm 4**from \(P_{w_{1},w_{2},x}(y)\). Next, since \(W_{1} An(Y)_{G}\), at (iii) step 2, we drop \(W_{1}\) from all parameters before the next recursive call. We are at the base case (iv) step 6 with \(:W_{2} X Y\). Thus, we train a conditional model \(M_{Y}(W_{2},X)\) on \(^{}[W_{2},X,Y]\) that can sample from \(P(y|w_{2},x)\). This would be returned as \(H_{Y}\) at step 4 (Fig. 3:green). Similarly, we can obtain sampling network \(H_{W_{2}}\) and \(H_{X}\) to sample from \(P_{w_{1},x,y}(w_{2})=P(w_{2}|w_{1})\) and \(P_{w_{1},w_{2},y}(x)=_{w^{}_{1}}P(x|w^{}_{1},w_{2})P(w^{ }_{1})\) (Fig. 3:blue). We connect these networks and perform ancestral sampling with fixed \(w_{1}\) for do\((W_{1}=w_{1})\).

**(Un)conditional sampling and complexity:** ID-GEN returns a sampling network \(\) when recursion ends. For unconditional query \(P_{}()\), we fix \(=\) in \(\) and perform ancestral sampling to generate joint samples. We pick the \(\) values in these joint samples (equivalent to marginalization in ID) and report as interventional samples. For a conditional query \(P_{}(|)\), ID-GEN uses the sampling network to first generate samples \([,,] P_{}(, )\) and then train a new conditional model \(M_{}(,)\) on \(\) to sample from \( P_{}(|)\) (Alg.5:IDC-GEN). The sampling network has \((|An()_{G}|)\) number of models and requires \((|An()_{G}|)\) time to sample from it. Please, see our complexity details in Appendix C.5, Appendix C.6.

**Theorem 3.2**.: _Under Assumptions: i) the SCM is semi-Markovian, ii) we have access to the ADMG, iii) \(P()\) is strictly positive and iv) trained generative models sample from correct distributions, ID-GEN and IDC-GEN are sound and complete to sample from any identifiable \(P_{x}(y)\) and \(P_{x}(y|z)\)._

Note that although existing work can sample from (low-dimensional) distributions, Theorem 3.2, makes ID-GEN, to our knowledge, the first method to use **only feed-forward models** to provably sample from identifiable high-dimensional interventional distributions.

## 4 Experiments

To illustrate ID-GEN's capabilities with high-dimensional image and text variables, we evaluate it on semi-synthetic: Colored MNIST and real-world: CelebA and MIMIC-CXR datasets. We provide additional details in Appendix F. Codes are available at github.com/musfiqshohan/idgen.

### ID-GEN performance on napkin-MNIST dataset and baseline comparison

**Setup:** We consider a semi-synthetic Colored-MNIST dataset for the napkin graph  in Fig. 4 with image variables \(W_{1},X,Y\) and paired discrete variable \(W_{2}\). Here, \(X\) and \(Y\) inherit the same digit value as image \(W_{1}\) which is propagated through discrete \(W_{2}.d[0-9]\). \(X\) is either red or green which is also inherited from \(W_{1}\) through discrete \(W_{2}.c\), i.e., \(W_{1}.color:\{r,g,b,y,m,cy\}_{2}.c:\{0,1\} X. color:\{r,g\}\). Unobserved \(Ucolor\) makes \(W_{1}\) and \(Y\) correlated with the same color, and unobserved \(Uthickness\) makes \(W_{1}\) and \(X\) correlated with the same thickness. Even though image \(X\) (takes r and g) is a direct ancestor of image \(Y\) (takes all 6 colors), \(Y\) only inherits the digit property from \(X\) but correlates the color property with image \(W_{1}\). This color correlation between \(W_{1}\) and \(Y\) is created by Ucolor. All mechanisms include \(10\%\) noise. Our target is to sample from \(P_{x}(y)\).

**Training and evaluation:** We follow ID-GEN steps: \(\). Step 3 implies \(P_{x}(y)=P_{x,w_{1},w_{2}}(y)\), i.e., intervention set =\(\{X,W_{1},W_{2}\}\). Step 7 suggests to generate do\((W_{2})\) interventional dataset \(^{}[W_{1},W_{2},X,Y] P_{w_{2}}(w_{1},x,y)=P(w_{1})P(x,y|w_ {1},w_{2})\). To obtain \(^{}\), we i) sample \(W_{1} P(w_{1})\), and ii) train a conditional diffusion model to sampling from \(P(x,y|w_{1},w_{2})\) with arbitrary \(W_{2}\) values. Next, Step 2 drops non-ancestor \(W_{1}\) and Step 6 trains a diffusion model \(M_{Y}(x,w_{2})\) on the new dataset \(^{}\) to sample from \(P^{}(y|x,w_{2})\). \(M_{Y}(x,w_{2})\) is returned as output that can sample from \(P_{x}(y)\),\( w_{2}\). We compare our performance with three baselines: i) a classifier-free diffusion

Figure 3: (Left: top-down) \(P_{w_{1},x,y}(y)\) is factorized into \(P_{w_{1},x,y}(w_{2})\), \(P_{w_{1},w_{2},y}(x)\) and \(P_{w_{1},w_{2},x}(y)\) (Step 4). Steps 7, 2, 6 is shown for \(P_{w_{1},w_{2},x}(y)\) only. (Right: bottom-up) we combine the sampling networks of each c-factor. For any do\((W_{1}=w_{1})\), we use \(\) to get samples from \(P_{w_{1}}(y)\).

model that samples from the (Cond)itional distribution \(P(y|x)\), ii) the DCM algorithm  that uses diffusion models to samples from \(P_{x}(y)\) but without confounders, and iii) the NCM algorithm  that uses GANs and considers confounders. We performed \((x)\) intervention with two images, i) digit 3 and ii) digit 5, both colored red. In Fig. 4, we show interventional samples for each method alongside their FID scores representing the image quality (lower:better). The (Cond) model (row 1, 5), DCM (row 2, 6) and our algorithm (row 5, 8) all generate good quality images of digit 3 and digit 5 with a specific color. However, the NCM algorithm (row 3, 7) generates images with blended colors (such as green + red). We observe that ID-GEN achieves the lowest FID scores (25.66 and 22.67), showing the ability to generate high-quality images consistent with the dataset. Whereas, Cond and DCM generate almost the same structure for all digits lacking variety, which explains their high FID. Note that \((x)\) removes the color bias between \(X\) and \(Y\) along the backdoor path. Thus, interventional samples should show all colors with uniform probability. Since Cond and DCM can not deal with confounders they show bias towards R, G, B colors of \(Y\) for red \(X\). ID-GEN removes such bias and balances different colors (Fig. 4). For a more rigorous evaluation, we use the effectiveness metric proposed in  and employ a classifier to map all generated images to discrete analogues (\(Digit,Color,Thickness\)) and compute exact likelihoods. We compare them with our ground truth \(P(Y.color|do(x))\) (uniform) and display these results for the color attribute in Fig. 4(right). We emulate the interventional distribution more closely with a low total variation distance: 0.25 compared to the baselines Cond (0.54) and DCM (0.58). We skip classifying colors of NCM as they are blended.

### Evaluating CelebA image translation models with ID-GEN

**Setup:** We apply ID-GEN to evaluate multidomain image translation of some existing generative models (ex: Male to Female domain translation). We examine whether they apply causal changes in (facial) attributes or add unnecessary changes due to the spurious correlations among different attributes they picked up in the training data. Our application is motivated by Goyal et al. , who generate counterfactual images to explain a pre-trained classifier while we examine pre-trained image generative models. We employ two generative models that are trained on CelebA dataset : i) StarGAN  and ii) EGSDE  (an approach that utilizes energy-guided stochastic differential equations). We assume the graph in Fig. 5 where the original image \(I_{1}\) causes its own attributes \(\) and \(\). We consider an unobserved confounder between them, as in the dataset, men are more likely to be old (correlation coeff=\(0.42\), ) and a classifier might have some bias toward predicting young-male images as old-male. These attributes along with the original image are used to generate a translated image \(I_{2}\). Next, \(P_{1}\) and \(P_{2}\) are 40 CelebA attributes of \(I_{1}\) and \(I_{2}\). \(A\) is the difference between \(P_{1},P_{2}\), i.e., the additional attributes (ex: makeup) that gets added to \(I_{2}\) but are absent in \(I_{1}\) during translation. We estimate \(P_{=0}(A)\), i.e., the causal effect of changing the domain from \(\) to Female on the appearance of a new attribute.

Figure 4: (Left:) Causal graph with color and thickness as unobserved. (Center:) FID scores (lower the better) of each algorithm and images generated from them. (Right:) Likelihood calculated from the \(P_{x}(y)\) images generated by each algorithm. We closely reflect the true \(P_{x}(y)\) with low TVD.

Figure 5: i) Graph and sampling network for \(P_{Male}(I_{2})\). ii) For both causal and non-causal attributes, EGSDE shows high correlation.

**Training and Evaluation**: For \(P_{Male}(A)\), We first generate \(I_{2} P_{}(I_{2})\) and then use a classifier on \(I_{2}\). ID-GEN visits steps: \(4,6\) and factorizes as \(P_{}(I_{2})=_{Y,I_{1}}P(I_{2}|=0,Y,I_{1})\)\(*P(Y,I_{1})\). To sample from these factors, it first trains \(M_{Y},M_{I_{1}}\). Next, instead of training \(M_{I_{2}}\) for \(P(I_{2}|=0,Y,I_{1})\), we plug in the model that we want to evaluate (StarGAN or EGSDE) as \(M_{I_{2}}\). We connect these models to build the sampling network in Fig. 5. We can now perform ancestral sampling in the network with \(=0\) and generate samples of \(I_{2}\). Next, we use a classifier to obtain 40 attributes of \(I_{1}\) and \(I_{2}\) as \(P_{1}\) and \(P_{2}\). Finally, we obtain the added attributes, \(A\) by comparing \(P_{1}\) and \(P_{2}\) and report the proportion as the estimate of \(P_{}(A)\). Similarly, we also estimate the conditional query, \(P_{}(A|Y)\), using StarGAN with \(Y=1\) fixed.

In Fig. 5, we show top 9 most appeared attributes. We observe that EGSDE introduces both causal (ex: \(\) to \(82\%\), \(\) to \(69.28\%\) of all images) and non-causal attributes (ex:\(\) to 37.61% and \(\) to 24.76%) with high probability. The model might assign high-probability to non-causal attributes because they were spuriously correlated in the CelebA training dataset (ex: sex and age). On the other hand, StarGAN and conditional StarGAN introduce new attributes with a low probability (\( 30\%\)) even if they are causal which is also not preferred. These evaluations enabled by ID-GEN help us to understand the fairness or bias in the prediction of image translation models. Finally, for the conditional query \(P_{}(A|Y)\), StarGAN translates \(54.68\%\) of all images as \(,\) which is consistent. In Appendix F.3, we discuss how baselines deal poorly with these queries.

### Invariant prediction with foundation models for chest X-ray generation

**Setup:** In this section, we demonstrate ID-GEN's utility to intervene with a text variable and generate images from the corresponding interventional distribution. Due to the lack of high-quality, annotated medical imaging datasets, the task of generating X-ray images \((X)\) from the prescription reports \((R)\) is recently being investigated. Consider the report-to-Xray generation task shown in Fig. 6. The shown causal graph is designed based on the MIMIC-CXR dataset  (see Appendix F.4 for details). Existing works  solve this task by using vision-language foundation models to directly generate CXR images conditioned on text prompts (or report): \(P(X|R)\)(Fig. 6 left). These models can generate images satisfying the prompt attributes (ex: effusion). However, they ignore the status of other attributes caused by prompt attributes (effusion\(\)atelectasis) that might implicitly contribute to generate the image. Moreover, these models are trained on multiple datasets collected from different locations, which could introduce different types of bias . For example, in many scenarios, patients with severe pneumonia are transferred to a different hospital and receives critical reports. Thus, there exists a potential confounding bias between report (\(R\)) writing style and pneumonia prevalence (\(N\)) in a specific location (\(H\)) i.e. \(R H N\). The direct \(R X\) prediction of the above mentioned conditional models might absorb the backdoor bias: \(R H N... X\) and get affected if \(P(R,N)\) shifts in a new location. We aim to make the image generation task of such models, interpretable and invariant in the test domain. Therefore, we consider the same input (report variable) and output (X-ray image) as such these models. For this purpose, ID-GEN performs \((R)\) intervention to remove the backdoor bias and infer attributes from the interventional distribution. Now these attributes are used to generate the final images (motivated by [48; 27]).

Figure 6: Left: Baseline vs our causal graph. Right: images for specific prompt w/ and w/o pneumonia. Inferred attributes are shown with their likelihood. Blue indicates changes compared to healthy.

**Training and Evaluation:** We indicate attribute extraction with outgoing edges from report \(R\) and image generation with incoming edges to X-ray \(X\). We assume pneumonia infection \(N\) affects other attributes. For our target query \(P_{R}(N,E,A,L,X)\), ID-GEN first visits step 4 and factorizes it as \(P_{R}(N,E,A,L,X)=P(N)P_{R,N}(E)P_{R,N,E}(A)P_{R,N,E,A}(L)P_{E,A,L}(X)\). ID-GEN goes to step 6 for each, converts them to conditional distributions and trains corresponding models or use pre-trained ones for \(M_{N},M_{E},M_{A},M_{L},M_{X}\). We utilize an LLM labeler  that can only extract attributes \(E,A,L\) from the report \(R\) (thus \(R N\)). If \(R\) explicitly mentions \(E,A,L\) then the models \(M_{E},M_{A},M_{L}\) output 1. Otherwise, they probabilistically infer their output conditioned on parent attributes in the causal graph. The models we train are able to converge to their conditional distribution with a low total variation distance (\( 0.05\)). We utilize a stable diffusion-based model  as \(M_{X}\) to sample \(X P_{E,A,L}(X)\). After ID-GEN connects all models, we intervene with a prompt, infer all attributes along with their empirical distribution and generate corresponding X-rays. In Fig. 6 (right), we show our results for the prompt intervention do(\(R=\)left pleural fluid persists). For better visualization, we also condition on \(N=0\) and \(N=1\) separately. First, the LLM labeler extracts effusion (\(E=1\)) from the prompt. Next, ancestral sampling with \(E=1\) in the sampling network provides us with the top 3 most probable attribute combination (\(N=0/1,E=1,A,L\)) and their corresponding images. In Fig. 6, we provide (i) a healthy image, (ii) image generated by baseline , and (iii)-(v) images generated with ID-GEN and their attributes with empirical probabilities. ID-GEN makes these predictions interpretable while facilitating them to be invariant with domain shifts.

## 5 Related work

Shalit et al. , Louizos et al. , Zhang et al. , Vo et al.  propose novel approaches to solve the causal effect estimation problem using variational autoencoders. Their proposed solution and theoretical guarantees are tailored for specific causal graphs containing treatment, effect, and covariates (or observed proxy variables), where they can apply the backdoor adjustment formula. Sanchez and Tsaftaris  employ DDPMs  to generate high-dimensional interventional samples, but only for bivariate models. Kocaoglu et al.  perform adversarial training on a collection of conditional generative models following the topological order to sample from interventional distributions. Pawlowski et al.  employ a conditional normalizing flow-based approach to offer high-dimensional interventional sampling as part of their solution. Chao et al.  designs diffusion-based causal models for arbitrary causal graphs with classifier-free guidance . However, these works have limited applications due to their strong assumption of no latent confounders in the system.

Xia et al.  propose a similar training process as Kocaoglu et al.  in the presence of hidden confounders. They show explicit connections with the Causal Hierarchy Theorem  and formalize the identification problem with neural models. However, it is difficult to match an arbitrary high-dimensional distribution, and their joint GAN training approach may suffer from convergence issues. Rahman and Kocaoglu  utilizes a modular algorithm to relax the joint training restriction for specific structures, but might face the convergence issue when the number of high-dimensional variables increases. Note that these methods are not suitable for efficient conditional sampling. Jung et al.  convert the expression returned by identification algorithm  into a form where it can be computed through a re-weighting function to allow sample-efficient estimation. Similarly Bhattacharyya et al. [4; 5] utilize bounded number of samples from the observational distribution to construct an interventional sampler. However, computing these reweighting functions/conditional distributions from data is still highly nontrivial with high-dimensional variables. Zecevic et al.  models each probabilistic term in the expression obtained for \(P(y|(x))\) with (i)SPNs. However, they require access to the interventional data and do not address identification from only observations. Wang and Kwiatkowska  design a novel probabilistic circuit architecture to encode the target causal query and estimate causal effects but do not offer high-dimensional sampling.

## 6 Conclusion

We propose a sound and complete algorithm to sample from conditional or unconditional high-dimensional interventional distributions. Our approach is able to leverage the state-of-the-art conditional generative models by showing that any identifiable causal effect estimand can be sampled from efficiently, only via feed-forward models. In future, we aim to relax our assumption on access to the ADMG and adapt our algorithm to deal with soft interventions.