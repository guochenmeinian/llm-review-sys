# Cognitive Model Discovery via Disentangled RNNs

Kevin J. Miller

Google DeepMind and University College London

London, UK

kevinjmiller@deepmind.com &Maria Eckstein

Google DeepMind

London, UK

mariaeckstein@deepmind.com &Matthew M. Botvinick

Google DeepMind

London, UK

botvinick@deepmind.com &Zeb Kurth-Nelson

Google DeepMind and University College London

London, UK

zebk@deepmind.com

###### Abstract

Computational cognitive models are a fundamental tool in behavioral neuroscience. They instantiate in software precise hypotheses about the cognitive mechanisms underlying a particular behavior. Constructing these models is typically a difficult iterative process that requires both inspiration from the literature and the creativity of an individual researcher. Here, we adopt an alternative approach to learn parsimonious cognitive models directly from data. We fit behavior data using a recurrent neural network that is penalized for carrying information forward in time, leading to sparse, interpretable representations and dynamics. When fitting synthetic behavioral data from known cognitive models, our method recovers the underlying form of those models. When fit to laboratory data from rats performing either a reward learning task or a decision-making task, our method recovers simple and interpretable models that make testable predictions about neural mechanisms.

## 1 Introduction

Fitting quantitative cognitive models to behavioral data is a fundamental tool in many areas of psychology and neuroscience . These models can be viewed as mechanistic hypotheses about the cognitive processes used by the brain. Viewed in this way, they act as an explicit software instantiation of a particular cognitive hypothesis, and can be used to make precise quantitative predictions about behavioral and neural data. A traditional modeling pipeline is for a human researcher to iterate over a three-step process: first to propose a candidate model structure (e.g. Q-learning), second to optimize model parameters (e.g. learning rate) with respect to a behavioral dataset, and finally to check whether the resulting model reproduces scientifically important features of the dataset. However, discovering an appropriate model structure is difficult (there are many possible structures to explore) as well as biased (the best structure may be one that the researcher has not thought of).

An alternative approach is fitting recurrent neural networks (RNNs) directly to behavior using supervised learning . RNNs are highly expressive and can approximate a wide variety of model structures in their weights. This is therefore a way of discovering a well-fitting model from data automatically. The drawback of this approach is that the resulting RNN is a "black box": a complex system that itself requires further analysis if it is to yield insight into cognitive mechanism.

Here, we propose a solution that aims to achieve the best of both worlds: automated discovery of human-interpretable cognitive models. Our approach, which we call disentangled RNN or "DisRNN", draws on recently developed methods from machine learning for "disentangling" .

These methods encourage networks to learn representations in which each dimension corresponds to a single true factor of variation in the data [23; 8]. DisRNN encourages disentangling in two ways. The first is to separate the update rule for each element of the latent state into separate sub-networks. The second is to use information "bottlenecks", which impose a penalty on maintaining information within the network, to the inputs and outputs of these sub-networks.

We fit DisRNN on sequential behavioral datasets from rats and artificial agents performing two classic cognitive tasks: a dynamic two-armed bandit task [13; 28; 26; 43] and a "pulse accumulation" decision-making task [7; 45; 14; 41]. First, we generate synthetic datasets for the two-armed bandit task using artificial agents with known learning algorithms (Q-Learning and Actor-Critic) which have different update rules and carry different information between timesteps. When fitting these synthetic datasets, DisRNN correctly recovers the timecourses of latent state information and the structure of the update rules. Second, we fit DisRNN on large laboratory datasets generated by rats performing the same two-armed bandit task . We find that DisRNN provides a similar quality of fit to the best known human-derived cognitive model of this dataset  as well as to an unconstrained neural network . Third, we generate a synthetic dataset for the decision-making task using an agent with a known decision algorithm (bounded accumulation) and show that disRNN is able to correctly recover the structure of its decision rule. Finally, we fit disRNN to a large laboratory dataset of rats performing this task , and find that it recovers a strategy similar to that of the best known human-derived model for this dataset. In all cases, we find that DisRNN is able to learn simple human-interpretable cognitive strategies which can be used to make predictions about behavioral and neural datasets.

## 2 Related Work

Our strategy for encouraging networks to adopt disentangled representations is directly inspired by work using variational autoencoders (VAE). Specifically, \(\)-VAE [23; 8], by scaling the KL loss of a variational autoencoder's sampling step (which can be viewed as requiring information to pass through a Gaussian information bottleneck), often learns sparse, disentangled representations in which each latent variable corresponds to a single true factor of variation in the data. While \(\)-VAE considered feedforward autoencoders, a wide variety of techniques have been proposed combining elements of feedforward VAEs with recurrent neural networks . We adapt these ideas here in a way that emphasizes interpretability and is appropriate for cognitive model discovery.

The use of recurrent neural networks as cognitive models has a long history [5; 27], and recent work has expanded the toolkit for fitting networks to behavioral datasets and for interpreting them [16; 19; 46; 15]. This approach of fitting standard neural networks and working to interpret the resulting fits benefits from the growing toolkit for neural network interpretability [47; 35; 36]. Our approach is complementary: instead of fitting standard networks and developing tools to interpret them post hoc, we develop networks which are incentivized to learn easily-interpretable solutions. Within neuroscience, a number of methods exist which attempt to discover interpretable latent dynamics from neural recording data [40; 50; 29; 44]. To our knowledge, these ideas have not been applied in the context of cognitive model discovery from behavioral data.

The two-armed bandit task we consider here is one of a family of dynamic reward learning tasks that have been heavily studied in behavioral neuroscience. This has led to a large library of candidate cognitive models [13; 4; 17; 32; 11; 26; 34; 37; 33; 3; 42]. The development of such models typically follows a theory-first approach, beginning with an idea (e.g. from optimality [13; 42], from machine learning , or from neurobiology ). Several studies have fit behavioral dataset using highly flexible classical models [26; 37; 32; 11]. One of these has attempted to use these fits as a basis for a process of data-driven cognitive model discovery by compressing the revealed patterns into a simpler model which is cognitively interpretable .

Two very recent papers have proposed a very similar workflow of discovering cognitive strategies from behavioral data using constrained neural networks. The first uses "hybrid" models that combine elements of classic structured models and flexible neural networks . The second constrains networks by limiting them to a very small number of hidden units . Future work should compare these approaches directly on matched datasets, as well as explore other possible opportunities for adapting modern machine learning techniques as tools for cognitive model discovery.

## 3 Disentangled Recurrent Neural Networks

An RNN trained to match a behavioral dataset containing temporal dependencies [16; 19; 46] can be viewed as maintaining a set of _latent variables_ which carry information from the past that is useful for predicting the future. The weights of the network can be viewed as defining a set of _update rules_ defining how each latent variable evolves over time based on external observations as well as the previous values of the other latent variables. Standard RNN architectures (such as the GRU ) typically learn high-dimensional latent representations with highly entangled update rules. This makes it difficult to understand which cognitive mechanisms they have learned, limiting their usefulness as cognitive hypotheses.

### Network Architecture

In order to learn an interpretable cognitive model, we encourage sparsity and independence by imposing bottlenecks which penalize the network for using excess information (Figure 1, right) [49; 1; 31; 23; 24; 8; 21]. The bottlenecks in our networks limit information flow using Gaussian noise. Our implementation uses multiple information bottlenecks, each of which is a noisy channel defined by two learned parameters: a "multiplier" \(m\) and noise variance \(\). The output on each timestep is sampled from a Gaussian distribution:

\[_{t}(mz_{t},)\] (1)

where \(x\) is the scalar input to the bottleneck and \(\) is the scalar output. Each bottleneck is associated with a loss which penalizes the sampling distribution for deviating from the unit Gaussian:

\[L_{}=_{t}D_{KL}(m_{b}x_{b,t},_{b })||(0,1)\] (2)

where \(D_{KL}\) is the Kullback-Leiber divergence, a measure of the difference between two probability distributions. This difference, and therefore the cost of the bottleneck, will be zero in the case that \(m=0\) and \(=1\). In that situation, the bottleneck will output samples from the unit Gaussian. These outputs will be independent of the input, meaning that no information will flow through the bottleneck. We refer to these as "closed bottlenecks". In all other situations, \(L_{b}\) will be greater than zero, and some information will pass through. In our experiments, bottlenecks that are carrying useful information typically fit \(m 1\) and \( 1\). We refer to these as "open bottlenecks".

We use these information bottlenecks to encourage cognitively interpretable models by encouraging two distinct kinds of disentangling. The first is disentangling in the latent variables themselves: we would like these to capture separable latent processes and to be relatively few in number. We

Figure 1: **Network Architecture. Left: Overall architecture of the DisRNN. Each latent variable is updated by a separate feedforward neural network (MLP: Multilayer Perceptron). Bottlenecks (orange connections) are imposed both on the inputs to these networks and on each latent variable to encourage interpretable representations. Right: A single information bottleneck. Output \(\) is a random sample from a Gaussian distribution determined by input \(z\) and bottleneck parameters \(m\) and \(\). The bottleneck is associated with a KL loss penalizing information transfer.**encourage this kind of simplicity by imposing a separate bottleneck on each scalar element of the network's hidden state. We refer to these as the "latent bottlenecks".

\[_{t}^{i}_{i}(z_{t}^{i})\] (3)

The second kind of simplicity is in the update rules for the latent variables. We would like each variable to be updated by its own separate rule, and we would like each of these rules to be as simple as possible. We encourage this kind of simplicity by updating each element of the network's hidden state, \(z_{t}^{i}\), using a separate learned update rule defined by a separate set of parameters. Each update rule consists of a multilayer perceptron (MLP) which defines a multiplicative update to be applied to its corresponding latent variable. These "Update MLPs" have access to the values of all elements of the previous timestep's hidden state, \(_{t-1}^{i}\) (having passed already through its latent bottleneck), as well as to the network's current observations \(_{t}\). Each element of the MLP input (i.e. of \(}_{t-1}\) and of \(_{t}\)) must pass through an additional information bottleneck, which we refer to as "Update MLP Bottlenecks". The output of each update MLP is a scalar weight \(w_{t}^{i}\) and update target \(w_{t}^{i}\), which are used together to update the value of the associated latent variable \(z^{i}\) in a manner analogous to the Gated Recurrent Unit .

\[w_{t}^{i},u_{t}^{i}=_{}^{i}((_{t-1}},}))\] (4)

\[z_{t}^{i}=(1-w_{t}^{i})_{t-1}^{i}+w_{t}^{i}u_{t}^{i}\] (5)

The latent variables are used on each timestep to make a prediction about the target, \(y\), using a separate "Choice MLP":

\[_{t}=_{}(_{t}})\] (6)

We train the model end-to-end by gradient descent to minimize the total loss function:

\[L_{}=L_{}(y,)+_{b }L_{}(b)\] (7)

where \(y\) are supervised targets, \(L_{}\) is a softmax cross-entropy loss. The hyperparameter \(\) scales the cost associated with passing information through the bottleneck [23; 8]. Different values of \(\) are expected to produce solutions which adopt different tradeoffs between predictive accuracy and model simplicity.

### Training Details

For the two-armed bandit task, datasets consisted of sequences of binary choices made (left vs. right) and outcomes experienced (reward vs. no reward) by either a rat  or an artificial agent (Q-Learning or Leaky Actor-Critic, see below). Each episode corresponded to a single behavioral session. On each timestep, the observation given to the network consisted of the choice and reward from the corresponding trial, and the target was the choice on the subsequent trial. For the pulse accumulation task, datasets consisted of sequences of integer pulse counts (left vs. right) observed and binary choices made (left vs right) by either a rat  or an artificial agent (Bounded Accumulation, see below). Each episode corresponded to a single trial. On each timestep, the observation given to the network consisted of the left and right pulse counts in a corresponding 10ms timebin. The target was null for all timesteps during the stimulus, and on the final timestep of each trial indicated the binary choice made by the rat.

All networks trained for this paper were of the same size and trained in the same way. Each network had five latent variables. Update MLPs consisted of three hidden layers containing five units each. The Choice MLP consisted of two hidden layers of two units each. We used the rectified linear (ReLU) activation function. Networks were defined using custom modules written using Jax  and Haiku . Network parameters were optimized using gradient descent and the Adam optimizer , with a learning rate of \(5 10^{-3}\). We typically trained networks for \(10^{5}\) steps, except that networks with very low \(\) (\(10^{-3}\) or \(3 10^{-4}\)) required longer to converge and were trained for \(5 10^{5}\) steps. Using a second-generation TPU, models required between four and fifty hours to complete this number of training steps.

DisRNN Recovers True Structure in Synthetic Datasets

We first apply DisRNN to synthetic datasets in which the process generating the data is known. We consider a task that has been the subject of intensive cognitive modeling efforts in psychology and neuroscience, the dynamic two-armed bandit task (Figure 2) [13; 28; 26; 43; 37]. In each trial of this task, the agent selects one of two available actions and then experiences a probabilistic reward. In our instantiation of the task, rewards are binary, and the reward probability conditional on selecting each arm drifts independently over time according to a bounded random walk .

We generated synthetic datasets from two reinforcement learning agents performing this task: Q-Learning and Leaky Actor-Critic. These agents have markedly different latent variables and update rules, allowing us to test whether DisRNN can recover the correct structure of each agent.

### Q-Learning Agent

The Q-learning agent  maintains two latent variables, \(Q_{}\) and \(Q_{}\). Each of these is associated with one of the available actions, and gives a running average of recent rewards experienced after taking that action. They are updated according to the following rules:

\[Q_{t+1}(a_{t})=(1-)Q_{t}(a_{t})+ r_{t}\] (8)

\[Q_{t+1}(a a_{t})=Q_{t}(a a_{t})\] (9)

where \(\) is a learning rate parameter. We visualize these update rules by plotting the updated value of \(Q\) as a function of its initial value and of the trial type (Figure 3a). A key feature is that each \(Q\) value only changes when its corresponding action is selected.

The Q-Learning agent selects actions based on the difference in \(Q\)-values:

\[a_{t}(Q_{t,}-Q_{t,}) \] (10)

where \(\) is an inverse temperature parameter. We generated a large synthetic dataset consisting of choices made and rewards received by the Q-Learning agent (\(=0.3\), \(=3\); 1000 sessions of 500 trials each) (Figure 3b). We then trained a DisRNN to imitate this dataset. Inspecting the learned bottleneck parameters of this network (Figure 3e), we find that just two of its latents have open bottlenecks (\( 1\)), allowing them to pass information forward through time. We find that the update rules for these latents have open bottlenecks for the input from both previous choice and previous reward. We find that the three latents with closed (\( 1\)) bottlenecks take on near-zero values on all timesteps, while the two with open bottlenecks follow timecourses that are strikingly similar to those of \(Q_{}\) and \(Q_{}\) (albeit centered on zero instead of on 0.5; Figure 3c). We visualize the learned update rule associated with each latent by probing its associated update MLP (Figure 3d). We find that these update rules are strikingly similar to those for \(Q_{}\) and \(Q_{}\). They reproduce the key feature that each is only updated following choices to a particular action.

### Leaky Actor-Critic Agent

The Leaky Actor-Critic agent uses a modification of the policy gradient learning rule (Section 2.8). Like the Q-Learning agent, it makes use of two latent variables, but uses very different rules

Figure 2: **Dynamic Two-Armed Bandit Task.****a**: In each trial, the rat selects one of two actions (left or right) and receives one of two outcomes (reward or no reward). **b**: Example behavioral session . Orange and purple lines show the drifting generative reward probabilities for each action. The placement of each tick mark (above or below) indicates the choice of the animal on that trial, while the color of the tick indicates whether or not the animal received reward. We will use this session as a running example to illustrate the dynamics of artificial agents and RNNs.

to update them. The first variable, \(V\), tracks "value". It keeps a running average of recent rewards, regardless of the choice that preceded them:

\[V_{t+1}=(1-_{v})V_{t}+_{v}r_{t}\] (11)

where \(_{v}\) is a learning rate parameter.

The second, \(\), is a "policy" variable that determines the probability with which the agent will select each action. The policy gradient algorithm defines two variables \(_{left}\) and \(_{right}\), each associated with one of the actions and updated using the following rules:

\[_{t+1}(a_{t}) =1-_{f}_{t}(a_{t})+_{l}r_{ t}-V_{t}1-_{t}(a_{t})\] (12) \[_{t+1}(a a_{t})=(1-_{f})_{t}(a)-_{l} r_{t}-V_{t}_{t}(a)\] (13)

Figure 3: **DisRNN Recovers Latent Dynamics of Q-Learning**. **a**: Q-Learning agent run using the choices and rewards from the example behavioral session (Figure 2) to generate timeseries for \(Q_{}\) and \(Q_{}\). **b**: Visualization of the Q-Learning update rules. Each panel visualizes the update of either \(Q_{}\) (top row) or \(Q_{}\) (bottom row) following a particular combination of choice and outcome (columns). The post-update value is shown on the vertical axis, as a function of the pre-update value, on the horizontal. Dashed lines are identity. **c**: Dependency graph of Q-learning. Q-learning computes an update for each latent (\(Q_{}\) and \(Q_{}\)) on each timestep. The update for \(Q_{}\) depends on choice and reward (Equation 8). The new value of \(Q_{}\) is a weighted sum of its old value (dashed line) and the update. \(Q_{}\) is similarly updated. **d**: DisRNN trained on a synthetic behavioral dataset generated by the Q-Learning agent. This panel shows data from the trained model run with choice and reward input from the same example session. Latents have been ordered, signed, and colored to highlight similarities with the Q-Learning agent. **e**: Visualization of the learned update rules (equations 4 and 5) for the first two latents. **f**: Learned parameters of the information bottlenecks. Left: Transmission bottlenecks (equation 3). Right: Update MLP bottlenecks (equation 4). Darker colors indicate open bottlenecks: for example, the update for latent 1 can depend on choice and reward. **g**: Dependency graph of cognitive model learned by DisRNN.

where \(_{t}\) is the agent's policy defined below (equation 14), \(_{l}\) is a learning rate parameter, and \(_{f}\) is a forgetting rate parameter that we have added to the update rule. This forgetting mechanism causes the policy variables to decay towards zero over time, allowing the agent to perform in an environment with changing reward probabilities. In a setting with just two available actions, \(_{}\) and \(_{}\) are degenerate: it will always be the case that \(_{}=-_{}\). We therefore define a single latent variable \(=_{}-_{}\) for the purposes of visualization. Actions are selected according to:

\[a_{t}()\] (14)

We visualize these update rules by plotting the updated values of \(V\) and of \(\) as a function of their initial value and of the trial type (Figure 4b). These update rules are quite different from those of the Q-Learning agent. One key feature is that the update rule for \(V\) depends only on reward, not on choice. Another is that the update rule for \(\) depends not only on choice and reward, but also on \(V\).

We generated a large synthetic dataset consisting of choices made and rewards received by the Leaky Actor-Critic agent (\(_{V}=0.3\), \(_{L}=1\), \(_{F}=0.05\); 1000 sessions of 500 trials each), and trained a DisRNN to imitate this dataset. Inspecting the learned bottleneck parameters of this network (Figure 4e), we find that two of its latents have open bottlenecks allowing them to pass information forward through time. One of these (latent two) has an open bottleneck to receive input from reward only. We find that this latent is strikingly similar to \(V\) from the Leaky Actor-Critic agent, both in terms

Figure 4: **DisRNN Recovers Latent Dynamics of Leaky Actor-Critic**. **a**: Actor-Critic agent run on the example rat behavioral session (Figure 2) to generate timeseries for \(V\) and \(\). **b**: Visualization of the Actor-Critic update equations. The update for \(v\) depends on reward but is independent of choice. The update for \(\) depends on choice and reward, as well as the value of \(V\). **c**: Dependency graph of Actor-Critic learning. **d**: DisRNN trained on a synthetic behavioral dataset generated by an Actor-Critic agent. **e**: Visualization of the learned update rules for the first two latents. **f**: Learned bottleneck parameters. Left: Transmission bottlenecks (equation 3). The first two latents have open bottlenecks; the remaining three latents have closed bottlenecks. Right: Update MLP bottlenecks (equation 4). **g**: Dependency graph of DisRNN trained on Actor-Critic data.

of its timecourse (Figure 4c) and update rule (Figure 4d, above). The other (latent one) has open bottlenecks to receive input from choice, reward, and from latent two. This latent is strikingly similar to \(\) from the Leaky Actor-Critic Agent, both in terms of its timecourse (Figure 4c) and update rule (Figure 4d, below).

Together with the previous section, these results indicate that DisRNN is capable of recovering the true generative structure of agents with two very different cognitive strategies to perform the two-armed bandit task. We also applied DisRNN to a synthetic behavioral dataset from an artificial performing a very different behavioral neuroscience task: decision-making by accumulation of noisy evidence (Appendix A). We found that it was similarly able to recover the true generative structure of this agent (Figure A1)

## 5 DisRNN Reveals Interpretable Models of Rat Behavior

Having established that DisRNN can recover true cognitive mechanisms in synthetic datasets with known ground truth, we moved on to consider a large laboratory dataset from rats performing the drifting two-armed bandit task . This dataset has previously been the subject of an intensive human effort at data-driven cognitive modeling, which resulted in a cognitive model consisting of three components each with its own latent variable: a fast-timescale reward-seeking component, a slower perseverative component, and a very slow "gambler's fallacy" component. This model provided a better fit to the dataset than existing models from the literature, and is currently the best known cognitive model for this dataset . We first asked whether DisRNN could provide a similar quality of fit to this model. We fit DisRNNs using four different values of the parameter \(\) (\(10^{-3}\), \(310^{-3}\), \(10^{-2}\), and \(310^{-2}\)) that controls the relative weight of predictive power and model simplicity in the network's loss function (equation 7). Following , we evaluated model performance using two-fold cross-validation: we divide each rat's dataset into even-numbered and odd-numbered sessions, fit a set of model parameters to each, and compute the log-likelihood for each parameter set using the unseen portion of the dataset. To compare results across different animals, we use "normalized likelihood": \(c\)log-likelihood/n trials . For each rat, we compute this both for our DisRNNs and for the cognitive model from , and plot the differences in Figure 5. We find that DisRNNs trained with \(=310^{-3}\) achieved a quality of fit similar to the cognitive model, and that those with \(=10^{-3}\) even narrowly outperformed it (Figure 5, orange curve). As an additional benchmark, we also compared our models to a widely-used RNN architecture, the LSTM . We selected LSTM hyperparameters (network size and early stopping) using subject-level cross-validation: we divided the dataset into subsets containing only even-numbered and odd-numbered rats, identified the hyperparameters that maximized (session-level) cross-validated likelihood in each subset, and evaluated networks with those hyperparameters in the unseen subset. We found that DisRNNs with \(=10^{-3}\) achieved a very similar quality of fit to these optimized LSTMs. Together these results indicate the DisRNNs, despite their architectural constraints and disentanglement loss,

Figure 5: **Model Comparison on Rat Behavioral Dataset**. Quality of model fit (cross-validated normalized likelihood) relative to the human-derived cognitive model from  for DisRNN (orange) and LSTM (blue). The x-axis for DisRNN represents different values of the hyperparameter \(\), which controls the tradeoff between model simplicity and predictive performance. LSTM hyperparameters were selected by subject-level cross-validation. Error bars indicate standard error over \(N=20\) rats.

can achieve predictive performance comparable to standard RNNs and to well-fit human-derived cognitive models.

Finally, we examined the parameters of DisRNNs fit to the complete dataset for each rat. We found that these were typically low-dimensional, with only a small number of latent variables having open bottlenecks, as well as sparse, with each update MLP having only a small number of open bottlenecks. While a thorough characterization of the patterns present in DisRNNs fit to individual rats remains a direction for future work, we present a representative example in Figure 6. This figure shows three DisRNNs with different values of \(\) fit to the dataset from the same example rat. The DisRNN with the largest \(\) of \(3\)x\(10^{-2}\) (left) adopts a strategy that utilizes only one latent variable. This strategy mixes reward-seeking and perseverative patterns into a single update rule (note that fixed points, where the line crosses unity, are more extreme for the rewarded conditions than the unrewarded ones). The DisRNN with a medium \(\) of \(10^{-2}\) (middle) adds a second latent variable that is purely perseverative and operates on a considerably slower timescale. Its fast-timescale reward-seeking latent variable, however, shows a similar perseverative signature to the previous network. The network with the smallest \(\) of 3x\(10^{-3}\) (right) retains both the fast reward-seeking and slower perseverative dynamics and adds a third, much slower-timescale, latent. The update rule for this latent depends both on reward and on the value of the perseverative latent. Interestingly, this very slow latent influences the update rules for both the reward-seeking and the perseverative latents. While the best human-derived cognitive model does include a very long-timescale component, this component simply influences choice rather than modulating the update rules of the other components. Situations where one latent variable modulates the update rule of another do sometimes occur in human-derived models for related tasks [3; 42], though it is not clear whether the pattern seen in this network is consistent with any of these. Characterizing these patterns in more detail, and especially quantifying similarities and differences among different animals, will be an important step for future work.

We also applied DisRNN to a laboratory behavioral dataset from rats performing an evidence accumulation task , and found that it was similarly able to identify low-dimensional models with interpretable structure (Figure A2).

## 6 Discussion

In this work, we develop a framework for discovering parsimonious cognitive models directly from behavioral data by fitting recurrent neural networks that contain structural features that encourage them to learn sparse, disentangled representations. Fit to synthetic datasets generated using known cognitive mechanisms, our method accurately recovers the structure of those mechanisms. Fit to laboratory datasets from rats performing a cognitive task, our method reveals models that are relatively simple and human-interpretable while outperforming the best known human-derived cognitive model in terms of predictive accuracy.

Figure 6: **DisRNN Trained on Rat Datasets. Examples of fit DisRNN networks with different values of the hyperparameter \(\) controlling the tradeoff between simplicity and predictive power.**

While the extent to which a model fit is "human-interpretable" is of course ultimately subjective, we believe that sparsity and disentangling provide benefits for at least three distinct reasons. The first is that fully understanding a model requires a human expert to inspect the update rules. The smaller the number of latents and fewer inputs to the update rule for each, the less cognitive burden will be placed on that expert, and the more likely they will be able to arrive at a satisfying human intuition about the cognitive mechanism embodied by the model. The second is that goal of discovery is to identify models that human experts will consider to be cognitively plausible. When evaluating classic handcrafted models, many experts agree that, all else being equal, simpler models (smaller number of equations, fewer terms in each equation) are more plausible. The third is that such a model is more likely to be useful for scientific tasks, such as searching for correlates in measurements of neural activity, that involve interacting with finite datasets.

The fit disRNNs have several key features than enable them to be applied immediately to standard cognitive neuroscience workflows . The first is that they generate timestep-by-timestep timecourses for the values of latent variables that play known cognitive roles within the model. These timecourses can be used as predictions about neural activity: if the model's mechanisms are implemented in the brain, then somewhere in the brain there is likely to be a signal that follows a similar timecourse. A second key feature is that disRNN makes explicit the rules by which latent variables are updated. These can be used as predictions about update rules within the brain: if the model's mechanisms are implemented in the brain, then somewhere in the brain there are likely to be similar update rules (e.g. synaptic weights to update the ongoing neural activity, synaptic plasticity to adjust synaptic weights, etc). A third is that a trained disRNN can be used to generate predictions for experiments which alter neural activity, for example by silencing it using optogenetics. Activity within the disRNN can be altered, for example by artificially zero-ing out a particular latent on a particular subset of trials, and the model can be run to generate predictions for behavioral and neural data .

An appealing feature of disRNN is that, for the same dataset, models with different complexity can be discovered by training networks with different values of the hyperparameter \(\). For some applications, it may be best to select the model that achieves the highest cross-validated quality-of-fit. For others, it may be useful to work with a model that is simpler, even though it may achieve lower quality-of-fit. We expect that in practice disRNN will be most useful if researchers fit several copies, at each of several values of \(\), and inspect their fits to determine which (if any) have learned models that are useful (Appendix B).

The most important limitation of our approach is that there is no guarantee that the recovered model will necessarily correspond to the true cognitive mechanisms used by the brain. Instead, it uses the dataset to reveal a parsimonious hypothesis. This hypothesis that requires evaluation by a human scientist to determine whether, given the rest of what is known about the psychology and biology of the system, it is plausible. If plausible, it may require new experiments to test the predictions that it makes. This limitation is fundamental to any approach that seeks to make inferences about cognitive mechanisms by analyzing behavioral data, as any behavioral dataset will provide only a limited window on neural mechanisms. One view of these systems is that they perform automatic hypothesis generation, without themselves addressing the problem of hypothesis testing.

An additional limitation is that the method, because it requires fitting quite flexible models, likely is applicable only to relatively large-scale datasets. Determining how performance scales with dataset size and exploring methods of improving data efficiency may be important directions for further research. Another direction is exploring the performance of disRNN on behavioral datasets from other types of cognitive neuroscience tasks. A large number of scientifically-important tasks exist which are of similar complexity to those considered here, and we believe that for many of these disRNN would prove able to discover reasonable models. A large number of tasks also exist which are considerably more complex, and we expect that for these disRNN in its current form might struggle, perhaps by requiring prohibitively large datasets.

The ultimate goal in cognitive model discovery is to develop tools which can reveal scientifically valuable models automatically from data. We believe that disRNN represents an important step towards this goal. Its most important test will come from work applying it to novel datasets, for which no model is known, and evaluating whether the structures it learns are useful to researchers.