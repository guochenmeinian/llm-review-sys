# On Differentially Private Subspace Estimation in a Distribution-Free Setting

Eliad Tsfadia

Department of Computer Science

Georgetown University

eliadtsfadia@gmail.com

###### Abstract

Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying for the high ambient dimension.

On the negative side, Dwork et al. (2014) proved that privately estimating subspaces, in general, requires an amount of points that has a polynomial dependency on the dimension. However, their bounds do not rule out the possibility to reduce the number of points for "easy" instances. Yet, providing a measure that captures how much a given dataset is "easy" for this task turns out to be challenging, and was not properly addressed in prior works.

Inspired by the work of Singhal and Steinke (2021), we provide the first measures that quantify "easiness" as a function of multiplicative singular-value gaps in the input dataset, and support them with new upper and lower bounds. In particular, our results determine the first types of gaps that are sufficient and necessary for estimating a subspace with an amount of points that is independent of the dimension. Furthermore, we realize our upper bounds using a practical algorithm and demonstrate its advantage in high-dimensional regimes compared to prior approaches.

## 1 Introduction

Differentially private (DP) Dwork et al. (2006) algorithms typically exhibit a significant dependence on the dimensionality of their input, as their error or sample complexity tends to grow polynomially with the dimension. This cost of dimensionality is inherent in many problems, as Bun et al. (2014); Steinke and Ullman (2017); Dwork et al. (2015) showed that any method that achieves lower error rates is vulnerable to tracing attacks (also known as, membership inference attacks). However, these lower bounds consider algorithms that guarantee accuracy for worst-case inputs and do not rule out the possibility of achieving higher accuracy for "easy" instances.

Example: DP averaging.As a simple prototypical example, consider the task of DP averaging. In this task, the input dataset consists of \(d\)-dimensional points \(x_{1},,x_{n}^{d}\), and the goal is to estimate their average \(_{i=1}^{n}x_{i}\) using a DP algorithm while minimizing the \(_{2}\) additive error. One natural way to capture input "easiness" for this task is via the maximal \(_{2}\) distance between any two points (i.e., points that are closer to each other are considered "easier"). Indeed, Tsfadia et al. (2022); Peter et al. (2024) showed that if the points are \(\)-close to each other, and we aim for an accuracy of \(\) (i.e., an accuracy that is _proportional_ to the "easiness" parameter \(\)), then it is sufficient andnecessary to use \(n=(/)\) points. Equivalently, if we aim for an accuracy of \(\), then by applying these results with \(=/\), we obtain that the answer is \(n=(/)\). This, in particular, implies that when \(/\) (i.e., the points are very close to each other), then \((1)\) points are sufficient, but for \(=/d^{1/2-(1)}\), a polynomial dependency on \(d\) is necessary in general.

DP subspace estimation.In this work, we consider the more complex problem of DP subspace estimation: Given a dataset \(X=(x_{1},,x_{n})(^{d})^{n}\) of unit norm points and a parameter \(k\), estimate the top-\(k\) subspace of \(Span\{x_{1},,x_{n}\}\). The main goal of this work is to answer the following meta question:

**Question 1.1**.: _How should we quantify how "easy" a given dataset is for DP subspace estimation?_

Since the dimension \(d\) is very large in many settings, we aim at providing tight measures that smoothly eliminate the dependency on \(d\) as a function of input "easiness". In particular, we want to be able to identify when we can avoid paying on the ambient dimension \(d\), and when a polynomial dependency on \(d\) is unavoidable.

### Motivation: DP-SGD

To motivate the problem, consider the task of privately training large neural networks. The most commonly used tool to perform such a private training is the differentially-private stochastic gradient descent (DP-SGD) Abadi et al. (2016); Bassily et al. (2014); Song et al. (2013) - a private variant of SGD that perturbs each gradient update with random noise vector drawn from an isotropic Gaussian distribution. However, this approach does not differentiate between "easy" gradients and "hard" ones, which results with high error rates when the ambient dimension - the number of parameters in the model - is large. However, empirical evidence and theoretical analysis indicate that while training some deep learning models, the gradients tend to live near a low-dimensional subspace Abadi et al. (2016); Li et al. (2018); Gur-Ari et al. (2018); Li et al. (2018); Demencioni and Chawla (2020); Zhou et al. (2021); Feng and Tu (2020); Li et al. (2022); Golatkar et al. (2022); Kairouz et al. (2020). In particular, Gur-Ari et al. (2018) showed that in some cases, the low dimension is the number of classes in the dataset, and the gradients tend to be close and well-spread inside this subspace. If we could exploit such a low-dimensional structure into an (inexpensive) private and useful projection matrix, we could reduce the error of DP-SGD by making it dependent solely on the low dimension.

We start by defining the setting of DP subspace estimation more formally.

### Subspace Estimation

We consider the setting of Dwork et al. (2014). That is, the input dataset consists of \(n\) points of unit norm \(x_{1},,x_{n}_{d}:=\{v^{d}\|v \|_{2}=1\}\) and a parameter \(k\), and the goal is to output a \(k\)-dimensional projection matrix \(\) such that \( X^{T}\) is "close" to \(X^{T}\) as possible, where \(X\) denotes the \(n d\) matrix whose rows are \(x_{1},,x_{n}\). We measure the accuracy of our estimation using the "usefulness" definition of Dwork et al. (2014):

**Definition 1.2** (\(\)-useful).: _We say that a rank-\(k\) projection matrix \(\) is \(\)-useful for a matrix \(X(_{d})^{n}\) if for any \(k\)-rank projection matrix \(^{}\):_

\[\| X^{T}\|_{F}^{2}\|^{} X^{T}\| _{F}^{2}- n,\]

_where \(\|\|_{F}\) denotes the Frobenius norm.1_

Observe that any projection matrix is \(1\)-useful for any \(X\) (because \(\|X\|_{F}^{2}=_{i=1}^{n}\|x_{i}\|_{2}^{2}=n\)). Therefore, we will be interested in smaller values of \(\) (e.g., \(0.001\)).

### Prior Works

Without privacy restrictions, we can find a \(0\)-useful (i.e., optimal) solution using Singular-Value Decomposition (SVD). The SVD of \(X\) is \(X=U V^{T}\), where \(U^{n n}\) and \(V^{d d}\) are unitary matrices, and \(\) is an \(n d\) diagonal matrix which has values \(_{1}_{\{n,d\}} 0\) along the diagonal. The top-\(k\) rows subspace of \(X\) is given by the span of the first \(k\) columns of \(V\), and it can be computed, e.g., by applying Principal Component Analysis (PCA) on the covariance matrix \(A=X^{T}X\) (the eigenvectors of \(A\) are the columns of \(V\)).

With differential privacy, however, the problem is much harder, and Dwork et al. (2014) showed a lower bound of \(n(k)\) for computing a \(0.001\)-useful \(k\)-rank projection matrix under \((1,(1/n^{2}))\)-DP. This bound, however, only holds for algorithms that provide accuracy for worst-case instances and does not rule out the possibility of achieving high accuracy with smaller values of \(n\) for input points that are very close to being in a \(k\)-dimensional subspace (i.e., "easy" instances).

Perhaps the easiest instances are those that _exactly_ lie in a \(k\)-dimensional subspace, and are well-spread within it (i.e., there is no \((k-1)\)-dimensional subspace that contains many points). Indeed, Singhal and Steinke (2021) and Ashtani and Liaw (2022) developed \((,)\)-DP algorithms for such instances that precisely recover the subspace using only \(n=_{,}(k)\) points. However, while these algorithms are robust to changing a few points, they are very brittle if we change all the points by a little bit.

One approach to smoothly quantify how much a dataset is "easy" is to consider the _additive-gap_\(_{k}^{2}-_{k+1}^{2}\). Indeed, Dwork et al. (2014), Gonem and Gilad-Bachrach (2018) present \((,)\)-DP algorithms that output \(0.001\)-useful projection using \(n=_{,}}{_{k}^{2}- _{k+1}^{2}}\) points. Yet, the downside of such additive-gap based approaches is their inherent dependency on the dimension \(d\). Even in the extreme case where the points exactly lie in a \(k\)-dimensional subspace and well-spread within it, the additive gap \(_{k}^{2}-_{k+1}^{2}\) is at most \(n/k\), which still results with a polynomial dependency on \(d\).

The only existing approach to eliminate the dependency on \(d\) in some non-exact cases is the one of Singhal and Steinke (2021) (their "approximate" case). Rather than quantifying easiness as a function of the input dataset, they consider a setting where the points are sampled i.i.d. from some distribution, and implicitly measure how "easy" a distribution is according to some stability notion. In particular, they show that a \(d\)-dimensional Gaussian \((,)\) with a _multiplicative-gap_\(()}{_{k}()}_{ ,,k}}\) is "stable" enough for estimating the top-\(k\) subspace of \(\) with sample complexity that is independent of \(d\). While Singhal and Steinke (2021) do not provide an answer to Question 1.1, they inspired our work to consider multiplicative singular-value gaps _in the input dataset_ as a measure for easiness.

### Defining Subspace Estimators

Towards answering Question 1.1, we consider mechanisms \(\) that are parameterized by \(k\), \(\), and \(\), and satisfy the following utility guarantee: Given a dataset \(X=(x_{1},,x_{n})(_{d})^{n}\) and a value \(\) as inputs, such that \(X\) is "\(\)-easy" for \(k\)-subspace estimation, then with probability at least \(\) over a random execution of \((X,)\), the output \(\) is an \(\)-useful rank-\(k\) projection matrix for \(X\).2 In Definition 1.3, the "\(\)-easy" property is abstracted by a predicate \(f\). We also allow an additional parameter \(_{}\) to relax the utility for non-easy instances (i.e., we would not require a utility guarantee for instances that are not "\(_{}\)-easy"). Furthermore, we only focus on cases in which \(X\) has at least \(k\) significant directions, which is formalized by requiring that \(_{k}^{2}(X) 0.01 n/k\) (we refer to Remark 2.3 for how our upper bounds, stated in Theorem 1.6, can handle smaller values of \(_{k}\) using an additional parameter or different assumptions).

**Definition 1.3** (\((k,,,_{},f)\)-Subspace Estimator).: _Let \(n,k,d\) s.t. \(k\{n,d\}\), let \((0,1]\), and let \(f:(_{d})^{n}\{0,1\}\) be a predicate. We say that \((_{d})^{n}^{d d}\) is an \((k,,,_{},f)\)-subspace estimator, if for every \(X(_{d})^{n}\) and \(_{}\) with \(_{k}^{2}(X) 0.01 n/k\) and \(f(X,)=1\), it holds that_

\[_{(X,)}[()X].\]

### Quantifying Easiness - Our Approach

In this work, we develop two types of smooth measures (captured by the predicate \(f\) in Definition 1.3) for input "easiness", which are translated to the following two types of subspace estimators:

**Definition 1.4** (\((k,,,_{})\)-Weak Subspace Estimator).: \((_{d})^{n}^{d d}\) _is called an \((k,,,_{})\)-Weak Subspace Estimator, if it is \((k,,,_{},f)\)-Subspace Estimator for the predicate \(f(X,)\) that outputs \(1\) iff \(_{i=k+1}^{n}_{i}^{2}(X)^{2}_{k}^{2}(X)\)._

**Definition 1.5** (\((k,,,_{})\)-Strong Subspace Estimator).: \((_{d})^{n}^{d d}\) _is called an \((k,,,_{})\)-Strong Subspace Estimator, if it is \((k,,,_{},f)\)-Subspace Estimator for the predicate \(f(X,)\) that outputs \(1\) iff \(_{k+1}(X)_{k}(X)\)._

In both cases, we define \(f\) based on multiplicative singular-value gaps in the input dataset, but the difference is what type of gap the value \(\) bounds: Strong estimators depend solely on the gap \(_{k+1}/_{k}\) without taking into account smaller singular values. Weak estimators, on the other, depend on the gap \(^{\{n,d\}}_{i}^{2}}/_{k}\). Note that a strong estimator is, in particular, a weak one (with the same parameters). Also note that both measures smoothly converge to the exact \(k\)-subspace case: When each gap tends to zero, the points tend to be closer to a \(k\)-dimensional subspace.

We provide new upper and lower bounds for both types of estimators.

#### 1.5.1 Our Upper Bounds

**Theorem 1.6** (Weak estimator).: _There exists an \((k,,=0.9,_{}=(\{,1\}))\)-weak subspace estimator \((_{d})^{n}^{d d}\) with \(n=_{,}k+,\,kd\}}{ }\) such that \((,)\) is \((,)\)-DP for every \(\)._

**Theorem 1.7** (Strong estimator).: _There exists an \((k,,=0.8,_{}=(\{,\, }{k^{2}d}\}))\)-strong subspace estimator \((_{d})^{n}^{d d}\) with \(n=_{,}k+d}{^{2}}\) such that \((,)\) is \((,)\)-DP for every \(\)._

Both of our estimators provide a useful projection by outputting a matrix that is close (in Frobenius norm) to the projection matrix of the top-\(k\) rows subspace. Their running time is \( T(m,d,k)+(dkn)\) for some \(m=(k)\), where \(T(m,d,k)\) denotes the running time required to compute (non-privately) a projection matrix to the top-\(k\) rows subspace of an \(m d\) matrix. We refer to Section 2 for a detailed overview.

For simplifying the presentation and the formal utility guarantees, we assume that our algorithms know the values of \(\) (the bound on the multiplicative-gap) and of \(k\) beforehand. Yet, we show that both assumptions are not inherent, and we refer to Remark 2.4 for additional details.

We also remark that in both theorems, it is possible to increase the confidence \(\) to any constant smaller than \(1\) without changing the asymptotic cost.

#### 1.5.2 Our Lower Bounds

**Theorem 1.8** (Lower bound for weak estimators).: _If \((_{d})^{n}^{d d}\) is a \((k,,=0.1,_{}=())\)-weak subspace estimator for \(1()\) and \((,)\) is \(1,\)-DP for every \(\), then \(n}{}\)._

**Theorem 1.9** (Lower bound for strong estimators).: _If \((_{d})^{n}^{d d}\) is a \((k,,=0.1,_{}=)\)-strong subspace estimator for \(1()\) and \((,)\) is \(1,\)-DP for every \(\), then \(n}{}\)._

Our lower bounds are more technically involved, and use a novel combination of generating hard-instances using the tools from Peter et al.  for proving smooth lower bounds, and extracting sensitive vectors from useful projection matrices using ideas from the lower bound of Dwork et al. . Both lower bounds are proven by generating hard-instances that are "\(\)-easy" for \(=\).

We refer to Section 3 for a detailed overview.

We remark that Peter et al.  recently proved a similar lower bound for the special case of \(k=1\) (estimating the top-singular vector). However, their result strongly relies on the similarity betweenaveraging and estimating top-singular vector in their hard instances, which does not hold for the case \(k 2\). Table 1 summarizes our bounds for \(k\).

#### 1.5.3 Implications

We offer two formulations which have the property we seek: If we aim for an error \(\), and the dataset is "\(\)-easy" for a very small \(\), we take \(=/\) to reduce the number of necessary and sufficient points.

For strong estimators, the rate \(n=n()\) in Theorem 1.7 does not match the corresponding lower bound Theorem 1.9, and Theorem 1.7 is limited to small values of \(\).3 Yet, for small values of \(k\), the strong-estimator bounds do imply that in order to privately compute an \(0.001\)-useful rank-\(k\) projection with number of points that is independent of \(d\), it is sufficient and necessary to have a gap \(_{k+1}/_{k}\) of at most \( 1/\). Table 2 summarizes our bounds for the special case of outputing \(0.001\)-useful projection matrix, using our two different types of input "easiness".

### Empirical Evaluation

We believe that private subspace estimation of easy instances could find practical applications. Therefore, we made an effort to realize our upper bounds using a _practical_ algorithm. In Section 4 we empirically compared our method to the additive-gap based approach of Dwork et al. (2014) for the task of privately estimating the empirical mean of points that are close to a small dimensional subspace, demonstrating the advantage of our approach in high-dimensional regimes.

### Paper Organization

In Sections 2 and 3 we present proof overviews for our results, and the empirical evaluation is provided in Section 4. A limitations section is given in Section 5. Additional related work appears at Appendix A. Notations, definitions and general statements are given in Appendix B. Our upper bounds are proven in Appendix C, and our lower bounds are proven in Appendix D.

## 2 Upper Bounds - Overview

Both of our estimators (Theorems 1.6 and 1.7) follow the same structure, but with different parameters. Similarly to Singhal and Steinke (2021), our algorithms follow the sample-and-aggregate approach of Nissim et al. (2007). That is, given a dataset \(X=(x_{1},,x_{n})(_{d})^{n}\), we partition the rows into \(t\) subsets, compute (non-privately) a projection matrix to the top-\(k\) rows subspace of each subset,

   & Weak Estimator & Strong Estimator \\  Upper Bound & \(_{,}k+}{}\) & \(_{,}k+d}{^{2}}\) \\  Lower Bound & \(}{}\) & \(}{}\) \\  

Table 1: Our bounds on \(n\) for subspace estimation (ignoring restrictions on \(_{}\) and \(\)).

and then privately aggregate the projection matrices \(_{1},,_{t}\). For doing that, we need to argue that \(_{1},,_{t}\) are expected to be close to each other. In the Gaussian setting of Singhal and Steinke (2021), this holds by concentration properties of Gaussian distributions. In our setting, however, it is unreasonable to expect that arbitrary partitions will lead to similar subspaces. For instance, consider the matrix \(X\) whose first \(n/k\) rows are \(e_{1}=(1,0,,0)\), the next \(n/k\) rows are \(e_{2}=(0,1,,0)\), and so forth until \(e_{k}\). Even though \(X\) is rank-\(k\) and has \(_{1}^{2}==_{k}^{2}=n/k\), if we simply partition the rows according to their order, then most of those subsets will induce a rank-\(1\) matrix which clearly does not represent the original matrix \(X\). Therefore, we must consider a more clever partition that will guarantee a good representation of the top-\(k\) rows subspace of \(X\) in each subset.

There is an extensive line of works who aim for methods to choose a small subset of rows that provides a good low-approximation for the original matrix (e.g., see Mahoney (2011) for a survey). Yet, most of these methods are data-dependent, and therefore seem less applicable for privacy.

In this work we show that by simply using a _uniformly random_ partition into subsets of size \((k)\), then w.h.p. each subset induces a projection matrix that is close to the projection matrix of the top-\(k\) rows subspace of \(X\):

**Lemma 2.1**.: _Let \(X=(x_{1},,x_{n})(_{d})^{n}\) with singular values \(_{1}_{\{n,d\}} 0\), and let \(_{1}=}{_{h}}\) and \(_{2}=^{\{n,d\}}_{1}^{2}}}{_{h}}\). Let \(^{}(_{d})^{m}\) be a uniformly random \(m\)-size subset of the rows of \(X\) (without replacement). Let \(\) and \(^{}\) be the projection matrices to the top-\(k\) rows subspace of \(X\) and \(^{}\), respectively. Assuming that \(_{k}^{2} 0.01n/k\), then the following holds for \(m=(k)\):_

1. _If_ \(_{1}\)_, then_ \(\|-^{}\| O} _{1} 0.9\)_. (_\(\|\|\) _denotes the Spectral norm_4_)._ 2. _If_ \(_{2} 0.1\)_, then_ \(\|-^{}\|_{F} O(_{2}) 0.9\)_._ 5_._ 
Namely, Item 1 bounds the expected spectral norm distance of the projection matrices using the first type gap \(_{k+1}/_{k}\) (which is used in the analysis of our strong estimator), and Item 2 bounds the expected Frobenius norm distance using the second type gap \(^{\{n,d\}}_{i}^{2}}/_{k}\) (which is used in the analysis of our weak estimator). We prove Lemma 2.1 in Appendix C.1.2.

The next step is to aggregate the non-private projection matrices \(_{1},,_{t}\) into a private one \(\). We consider two types of aggregations. The first one simply treats each matrix as a \(d^{2}\) vector and privately estimate the average of \(_{1},,_{t}\). The second type (which outperforms the first one in most cases) follows a similar high-level structure of Singhal and Steinke (2021). That is, to sample i.i.d. reference points \(p_{1},,p_{q}(,_{d d})\) for \(q=(k)\), privately average the \(qd\)-dimensional points \(\{(_{j}p_{1},,_{j}p_{q})\}_{j=1}^{t}\) for obtaining a private \(^{q d}\) (whose \(i^{}\) row estimates the projection of \(p_{i}\) onto the top-\(k\) rows subspace of \(X\)), and then compute the projection matrix of the top-\(k\) rows subspace of \(\). But unlike Singhal and Steinke (2021) who perform this step using a histogram-based averaging that has the same flavor of Karwa and Vadhan (2018), we perform this step using FriendlyCore Tsfadia et al. (2022) that simplifies the construction and makes it practical in high dimensional regimes. We remark that in both aggregation types, we need a DP averaging algorithm that is resilient to a constant fraction of outliers (say, \(20\%\)) since both items in Lemma 2.1 only guarantee that the expected number of outliers is no more than \(10\%\). Fortunately, FriendlyCore can be utilized for such regimes of outliers (see Appendix B.8.3 for more details).

A few remarks are in order.

**Remark 2.2**.: _The first aggregation type (which privately estimate the average of \(_{1},,_{t}\) directly) outperforms the second type only for our weak estimator in the regime \(k\) (as it inherently posses larger dependency in the dimension)._

**Remark 2.3**.: _We could avoid the requirement \(_{k}^{2} 0.01n/k\) by adding an additional parameter \(\) such that \(_{k}^{2} n/k\), and using subsets of size \(m=(k/)\) (which would increase \(n\) by the same factor of \(1/\)). For readability purposes, we chose to avoid this additional parameter. Because our algorithms provide useful projection by estimating the actual top-\(k\) projection, then such a requirement is unavoidable if we would like to provide utility guarantees only as a function of the singular values.6 In fact, any assumption that would imply that two random subsets induce similar top-\(k\) projection matrices would suffice for the utility analysis._

**Remark 2.4**.: _To eliminate the known-\(\) assumption, we can replace the FriendlyCore-averaging step Tsfadia et al. (2022) (that requires to know the diameter) with their unknown diameter variant that gets two very rough bounds \(_{}\) and \(_{}\), and performs a private binary search for estimating a good diameter \([_{},_{}]\) in a preprocessing phase. This step only replaces the dependency on \(d\) with \(d+(_{}/_{})\) in the asymptotic sample complexity (section 5.1.2. in Tsfadia et al. (2022)) and is very practical. In fact, we use this method in our empirical evaluation in Section 4._

_For handling unknown values of \(k\), note that our algorithms provide useful utility guarantees (compared to the additive-gap based ones) only when \(_{k+1}^{2} 1\). So in cases where \(_{k}^{2} 2(k/)/^{}\) for some (fixed) privacy budget \(^{}<\) (say, \(^{}=/10\)), we can privately determine w.p. \(1-\) the right value of \(k\) using a simple \(^{}\)-DP method. The main observation is that the \(_{1}\) sensitivity of the vector \((_{1}^{2},,_{n}^{2})\) is at most \(2\)(Amin et al. (2019)), yielding that we can privately compute \((_{1}^{ 2},,_{n}^{ 2})=(_{1}^{2}+(2/^{}),,_{n}^{2}+(2/^{}))\), and then perform analysis on \((_{1}^{ 2},,_{n}^{ 2})\) to set \(k\) as the first index \(i\) where \(_{i}^{2}(i/)/^{}\) and \(_{i+1}^{2}<(i/)/^{}\)._

## 3 Lower Bounds - Overview

Our lower bounds (Theorems 1.8 and 1.9) use the recent framework of Peter et al. (2024) for generating smooth lower bounds for DP algorithms using Fingerprinting Codes (FPC), but require technically involved analysis due to the complex structure of this problem for \(k 2\).

Roughly speaking, let \(\) be a distribution over \(\{-1,1\}^{n_{0} d_{0}}\) that induces an optimal FPC codebook with \(d_{0}=(n_{0}^{2})\) (e.g., Tardos (2008), Peter et al. (2024)). The connection between FPC and DP (first introduced by Bun et al. (2014)) is that any DP algorithm, given a random codebook \(X=(x_{i}^{j})_{i[n_{0}],j[d_{0}]}\) as input, cannot output a vector \(q=(q^{1},,q^{d_{0}})\{-1,1\}^{d_{0}}\) that "agrees" with most of the "marked" columns of \(X\) (Formally, for \(b\{-1,1\}\), a columns \(x^{j}=(x_{1}^{j},,x_{n}^{j})\) is called \(b\)-marked if \(x_{1}^{j}==x_{n}^{j}=b\), and \(q\) agrees with it if \(q^{j}=b\)).

Now consider a DP mechanism \(^{n}\) that satisfies some non-trivial accuracy guarantee. Peter et al. (2024) reduces the task of lower bounding \(n\) to the following task: (1) Generate from an FPC codebook \(X\{-1,1\}^{n_{0} d_{0}}\) hard instances \(Y^{n}\) for \(\), and (2) Extract from the output \(w(Y)\) a vector \(q\{-1,1\}^{d_{0}}\) that agrees with most of the marked columns of \(X\) (\(n_{0}\) and \(d_{0}\) are some functions of \(n\), \(\) and the weak accuracy guarantee of \(\)). Peter et al. (2024) proved that if there exists such generating algorithm \(\) and extracting algorithm \(\) (which even share a random secret that \(\) does not see) such that \(\) is _neighboring-preserving_ (i.e., maps neighboring databases to neighboring databases), then it must hold that \(n_{0}(})\) (Otherwise, \(\) cannot be DP).

Warm-up: DP averaging.We first sketch how Peter et al. (2024) applied their framework with \(n_{0}=n\) and \(d_{0}=(d/^{2})\) for proving a lower bound for the simpler problem of DP averaging. In this setting, we are given a mechanism that guarantees \(\)-accuracy (\(_{2}\) additive error) for \(\)-easy instances (i.e., points that are \(\)-close to each other in \(_{2}\) norm). The generator \(\), given an FPC codebook \(X\{-1,1\}^{n_{0} d_{0}}\), uses the _padding-and-permuting_ technique: It pads \( 10^{4}^{2}d_{0}\)\(1\)-marked columns and \(\) (\(-1\))-marked columns, and then permutes all the \(d=d_{0}+2\) columns of the new codebook \(X^{}\) using a random permutation \([d][d]\) that is shared with the extractor \(\). The input \(Y\) to the algorithm would be the _normalized_ rows of \(X^{}\) which are \(\)-close to each other in \(_{2}\) norm, so the mechanism has to output an \(\)-accurate solution \(w\). In particular, after rounding \(w\) to \(\{-1,1\}^{d}\), the coordinates of \(w\) must agree with a vast majority of the marked columns, and also with a vast majority of the original marked columns that are located within \((1),,(d_{0})\) as it cannot distinguish between them and the other marked columns (because \(\) is hidden from it). The extractor \(\), given \(w\) and \(\), rounds \(w\) to \(\{-1,1\}^{d}\) and outputs \(q=(w^{(1)},,w^{(d)})\) which agrees with most of the marked columns of \(X\). Hence, we obtain the lower bound of \(n(})=(/)\).

DP Subspace EstimationIn our case, we are given a (weak or strong) subspace estimator \((_{d})^{n}^{d d}\) that outputs an \(\)-useful rank-\(k\) projection matrix if \(_{k+1}(X)_{k}(X)\) (or \(^{\{n,d\}}_{i}(X)^{2}}_{k}(X)\)). We prove our lower bounds by applying the framework with \(n_{0}=n/k\) and \(d_{0}=(od)\), for some parameter \(=()\) that will depend on the type of \(\) we consider. In order to generate hard instances \(Y(_{d})^{n}\) for \(\) given an FPC codebook \(X\) (\(X\{-1,1\}^{(n/k) d_{0}\}\)), we use a variation of the approach to Dwork et al. (2014). Namely, our generator \(\) samples \(k\) independent FPC codebooks \(A_{1},,A_{k}\) where it plants \(A_{i}=X\) for a random \(i[k]\). Then for each \(j[k]\), it applies (independently) the padding-and-permuting technique of Peter et al. (2024) where it pads \(\)\(1\)-marked columns and \(\)\((-1)\)-marked columns for \(}{2a}\), and permute all the columns. This induces \(k\) matrices \(B_{1},,B_{k}\{-1,1\}^{(n/k) d}\) (for \(d=d_{0}+2\)) such that each \(B_{j}\) is "almost" rank-\(1\) and their vertical concatenation \(B\{-1,1\}^{n d}\) is almost rank-\(k\). It provides \(Y=}B\) as the input for \(\). See Figure 1 for graphical illustrations.

We remark that at this step, the main difference from Dwork et al. (2014) (who implicitly follow a similar paradigm) is that they use a fixed padding length of \(=15d_{0}\) that suffices for the robustness properties that they need. On the other hand, we use Peter et al. (2024)'s observation that increasing the padding can handle low-accuracy regimes of many problems, and indeed we use the padding length \(\) to increase the \(k\)'th singular value gap, which will be a function of the quality parameter \(\).

The next step is to choose the right value of \(=()\) such that \(\), on input \(Y\), will have to output a useful projection matrix. We show that the input matrix \(Y\) has w.h.p. \(_{1}(Y)^{2}_{k}(Y)^{2}(1-O()) {k}\), which yields that \(_{i=k+1}^{\{n,d\}}_{i}(Y)^{2} O()n\). If \(\) is a weak estimator, then we simply use \(=k}\) to guarantee that \(_{i=k+1}^{\{n,d\}}_{i}(Y)^{2}^{2}_{k}(Y)^{2}\) for \(=\), which yields that by the utility guarantee of \(\), we get an \(0.001\)-useful projection matrix. If \(\) is a strong estimator, then we use \(=}\) (i.e., we decrease the padding length by a factor of \(k\)). Yet, in order to meet the requirements of \(\), we must argue that w.h.p., \(_{k+1}(Y)^{2}()\), and this is more complex than the previous case. Here we use more internal properties of the fingerprinting distribution \(\). Namely, that in Peter et al. (2024)'s construction (which is also true for Tardos (2008)'s one), each entry of the codebook matrix has expectation \(0\) and the columns of the matrix are independent. Using known concentration bounds, this allows us to argue that if we pick a unit vector \(v\) that is orthogonal to the top-\(k\) rows subspace of \(Y\), then with probability at least \(1-(-(d))\) it holds that \(\|Yv\|_{2}^{2}()\). Since \(_{k}^{2}\) is bounded by the supremum of \(\|Yv\|_{2}^{2}\) under such unit vectors, we conclude the proof of this part using a net argument.

Finally, the last step, which is not trivial for \(k 2\), is to extract from an \(0.001\)-useful projection matrix \(\) for \(Y\), a vector \(q\{-1,1\}^{d_{0}}\) that with noticeable probability, strongly agrees with the marked columns of the original codebook \(X\{-1,1\}^{n_{0} d_{0}}\). For that, our extractor \(\) uses the random permutations and the random location \(i\) (which are part of the shared secret between the

Figure 1: From Left to Right: (1) The normalized rows of the fingerprinting codebook \(X\) are well-spread on the \(d_{0}\)-dimensional unit sphere. (2) Applying the padding-and-permuting (PAP) technique makes the normalized points very close to each other on the \(d\)-dimensional unit sphere (\(d d_{0}\)). (3) We create hard instances for DP subspace estimation using \(k\)-independent (normalized) PAP-FPC codebooks \(B_{1},,B_{k}\), where \(PAP(X)\) is planted in one of the \(B_{i}\)’s (in this example, in \(B_{2}\)). Reducing \(\) (i.e., increasing the padding length) makes the points in each group \(B_{i}\) closer to each other, which in particular, increases the closeness to a \(k\)-dimensional subspace.

generator and the extractor) and follows the strategy of Dwork et al. (2014). That is, it applies the \(i^{ th}\) invert permutation over the columns of \(\) (denote the resulting matrix by \(_{i}\)), chooses a vector \(u Span(_{i})\) that has the maximal agreement with _half_ of the padding bits, and then simply outputs its first \(d_{0}\) coordinates after rounding to \(\{-1,1\}\). The intuition is that an \(0.001\)-useful projection matrix must be also \(0.001\)-useful for at least one of the parts \(Y_{j}=}B_{j}\). Since all the \(Y_{j}\)'s have the same distribution and the location \(i\) (where the original \(X\) is planted) is hidden from \(\), then it must be \(0.001\)-useful for \(Y_{i}\) with probability at least \(/k\) (where \(\) denotes the success probability of \(\)).

Given that this event occurs, the usefulness of \(\) implies that there must exists a vector in \(Span(_{i})\) that strongly agrees with half of the padding locations. But because all the marked columns (that includes the padding locations) are indistinguishable from the eyes of \(\) who computed \(\), then a similar agreement must hold for the marked columns of \(X\).

## 4 Empirical Evaluation

We implemented a zCDP (Definition B.24) variant of our subspace estimation algorithm in Python (denoted by \(\)), and in this section we present empirical results for the fundamental task of privately estimating the average of \(d\) dimensional points that approximately lie in a much smaller \(k\)-dimensional subspace. Namely, given a dataset \(X=(x_{1},,x_{n})_{d}^{n}\), a parameter \(k\), and zCDP parameters \(,\), we perform the following steps: (a) Compute a \((/2,)\)-zCDP rank-\(k\) projection matrix \(\) using \(\) that estimates the projection onto the top-\(k\) rows subspace of \(X\), (b) Compute a \(/2\)-zCDP estimation of the average of \(X\) using the Gaussian Mechanism: \(=_{i=1}^{n}x_{i}+(,^{2} _{d d})\) for \(=}\), and (c) Output \(=\).

The accuracy is measured by the \(_{2}\) error from the average: \(\|-_{i=1}^{n}x_{i}\|_{2}\).

In all our experiments, we use \(=2\) and \(=10^{-5}\), \(t=125\) (the number of subsets in the sample-and-aggregate process), \(n=2tk\) data points, \(q=10 k\) (the number of reference points in the aggregation), and use the zCDP implementation of the FriendlyCore-based averaging algorithm of Tsfadia et al. (2022).7 All experiments were tested on a MacBook Pro Laptop with 8-core Apple M1 CPU with 16GB RAM.

Rather than using Tsfadia et al. (2022)'s algorithm for the known-diameter case, we use their unknown-diameter implementation with \(_{}=10^{-6}\) and \(_{}=100\) (see Remark 2.4 for details). Furthermore, we reduced the space complexity of our implementation from \((d^{2})\) to \((kd)\).8

In order to generate a synthetic dataset that approximately lie in a \(k\)-dimensional subspace, we initially sample uniformly random \(b_{1},,b_{k}\{-1,1\}^{d}\) and perform the following process to generate each data point: (i) Sample a random unit vector \(u\) in \(Span\{b_{1},,b_{k}\}\), (ii) Sample a random noise vector \(\{1/,-1/\}^{d}\), and (iii) Output \(\) (note that higher \(\) results with data points that are closer to a \(k\)-dimensional subspace).

We compare our averaging method to two other approaches: The first one simply applies the Gaussian mechanism directly on \(X=(x_{1},,x_{n})\) using the entire privacy budget \(\) (i.e., without computing a projection matrix). The second one replaces our Step (a) by computing the projection matrix \(\) using a \((/2,)\)-zCDP variant of the additive-gap based algorithm of Dwork et al. (2014) (see Appendix B.8.4 for more details). 9 The empirical results are presented in Figure 2. In all experiments, we perform \(30\) repetitions for generating each graph point which represents the trimmed average of values between the \(0.1\) and \(0.9\) quantiles. We show the \(_{2}\) error of our estimate on the \(Y\)-axis. The first graph illustrates the inherent dependency on \(d\) that Dwork et al. (2014)'s algorithm has, while our algorithm \(\) takes advantage of the closeness of the points to dimension \(k\) in order to eliminate this dependency. The second graph illustrates that when \(d\) is fixed, increasing \(k\) and \(n\) in the same rate a has similar affect on both \(\) and Dwork et al. (2014)'s algorithm. In the last graph we compare the accuracy of \(\) and Dwork et al. (2014)'s algorithm as a function of the closeness to a subspace \(k\) (measured in our experiments by the parameter \(\)), and show in what regimes \(\) outperforms Dwork et al. (2014)'s algorithm.

## 5 Limitations and Future Directions

From a theoretical perspective, our work is the first to provide proper measures for how "easy" a given dataset is which smoothly eliminates the dependency on the dimension \(d\). Yet, closing the gap between our upper and lower bounds is still left open. Specifically, for weak estimator, there is still a gap of \(k^{1.5}\) between Theorems 1.6 and 1.8. For strong estimators, the upper-bound rate \(n=n()\) (Theorem 1.7) does not align with the one of the lower bound (Theorem 1.9), and it is left open to relax the restriction on \(_{}\). One possible reason for some of these gaps (especially the dependency in \(k\)) is that our upper bounds follow the approach of Singhal and Steinke (2021) to estimate (under some matrix norm) the projection matrix to the top-\(k\) rows subspace (we do it in Frobenius norm). While estimating the projection matrix itself provides, in particular, a useful solution (Proposition B.16), the opposite direction is not true in general, and it could be possible to reduce the sample complexity by focusing on \(\)-usefulness (Definition 1.2) directly, or alternatively, providing stronger lower bounds for estimating the projection matrix.

From a more practical standpoint, we empirically demonstrate the advantage of our approach in high-dimension regimes when the data is very close to a low-dimensional structure, which is directly translated to an advantage in private mean estimation of such instances. The downside of our approach is that it requires the points to be very close to a \(k\)-dimensional structure in order to be effective, which might not be sufficient for typical training scenarios in deep learning. It would be intriguing to explore if there is a connection between training parameters (e.g., the network structure) to the phenomena of gradients that are close to a low-dimensional subspace (mentioned in Section 1.1). If we could boost this closeness to regimes where our method achieves high accuracy, we could generate drastically improved private models. On the other hand, if we cannot do it, then our lower bounds indicate that improving DP-SGD via private subspace estimation might not be the right approach, and we should focus on different approaches for this task.

Figure 2: From Left to Right: (1) The case \(k=4\) and \(=10d\), varying \(d\) (the \(X\)-axis is \(\)). (2) The case \(d=10^{4}\) and \(=10d\), varying \(k\). (3) The case \(d=10^{4}\) and \(k=4\), varying \(\) (the \(X\)-axis is \(/d\)). In all the experiments, we use \(n=250 k\) data points.