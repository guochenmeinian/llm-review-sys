# Effective Robustness against Natural Distribution

Shifts for Models with Different Training Data

Zhousing Shi

UCLA

zshi@cs.ucla.edu

&Nicholas Carlini

Google Research

ncarlini@google.com

&Ananth Balashankar

Google Research

ananthbashkar@google.com

&Ludwig Schmidt

University of Washington

schmidt@cs.washington.edu

&Cho-Jui Hsieh

Google, UCLA

chohsieh@cs.ucla.edu

&Alex Beutel

OpenAI

alexb@openai.com

&Yao Qin

UCSB, Google Research

yaoqin@ucsb.edu

Work done while at Google.

###### Abstract

"Effective robustness" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageNet as the only ID test set, while the gains diminish under our new evaluation. Additional artifacts including interactive visualizations are provided at https://shizhousing.github.io/effective-robustness.

## 1 Introduction

Robustness against distribution shifts is important for machine learning models to work reliably across various environments. For natural distribution shifts on image classification datasets, Taori et al. (2020) proposed the notion of _effective robustness_ to control for in-distribution (ID) accuracy when evaluating out-of-distribution (OOD) accuracy. Following a long line of work that has found a strong correlation between ID and OOD accuracy on many test sets (Recht et al., 2019; Yadav & Bottou, 2019), effective robustness allows researchers to assess whether an apparently improved OOD accuracy is a result of effectively improved robustness or is simply an expected outcome of enhanced ID accuracy.

Unfortunately, the current definition of effective robustness has a subtle limitation: it requires a fixed ID test set, which is typically ImageNet (Deng et al., 2009) when using ImageNet-like OOD test setsin Taori et al. (2020) or CIFAR-10 (Krizhevsky et al., 2009) when using CIFAR-like OOD test sets in Miller et al. (2021). It is acceptable when models are trained predominately on only one dataset. However, the emergence of many large-scale models trained on significantly different datasets makes it necessary to evaluate and compare models trained on different data distributions, under which it becomes unclear which ID test set should be used.

In particular, models from Contrastive Language-Image Pre-training, such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) have recently exhibited unprecedented effective robustness gains during _zero-shot_ inference (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022). However these previous works simply take ImageNet as the single ID test set, even though the models are not trained on ImageNet. We demonstrate that the results of evaluating effective robustness using a single ID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise treatment on the ID test set in existing works could end up exaggerating the effective robustness of zero-shot CLIP models compared to models that are exactly trained on ImageNet.

In this paper, we propose to more precisely evaluate and compare the effective robustness of models trained on different datasets. Instead of controlling for a single ID accuracy that may bias towards models from a particular training distribution, we propose to use multiple ID test sets that cover the training distributions of all the models. In particular, previous works performed single-dimensional linear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori et al., 2020). And they then evaluate the actual OOD accuracy of the models _beyond_ the expected value that can be predicted from the fitting line, as the effective robustness. We expand on this definition by allowing for multiple ID test sets, and perform _multi-dimensional_ linear regression to fit a plane to predict OOD accuracy from the accuracy on multiple ID test sets.

In summary, we make the following contributions:

* We reveal a limitation in the existing effective robustness evaluation when used to compare models trained on different data distributions.
* We then propose a new effective robustness evaluation which uses multiple ID test sets to more precisely compare the effective robustness of models trained on different data.
* We show that the OOD accuracy of various models including zero-shot CLIP models can usually be better predicted from accuracies on multiple ID test sets compared to using only one ID test set.
* Our results provide new understandings on the effective robustness gains of CLIP-like models observed in prior works only using ImageNet as the ID test set, while the gains diminish under our new evaluation.

## 2 Background of Effective Robustness

Under natural distribution shifts, the OOD accuracy of a model is often correlated with the ID accuracy. After applying a logit transformation on the accuracy, a linear trend between the transformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from ImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky et al., 2009) to CIFAR-10.2 (Hendrycks and Dietterich, 2018)) and models with various architectures and training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most models showing higher OOD accuracies naturally resulted from better ID performance.

To eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed _effective robustness_ that measures the OOD performance _beyond_ the expected OOD accuracy given the ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend of baseline models. Since they only use a single ID test set, we refer to this version of effective robustness as _single-ID effective robustness_.

Suppose there are \(n\) baseline models \(f_{1},f_{2}, f_{n}\). A baseline function \((x)\) is constructed to predict the OOD accuracy of each baseline model, \(_{}(f_{i})\)\((1 i n)\), given the single ID accuracy of the model \(x\!=\!_{}(f_{i})\). The baseline function is instantiated as:

\[(x)=(w(x)+b),\] (1)

where \(w\) and \(b\) are parameters, \((x)=()\) is the logit transformation, and \((x)\) is the inverse of \((x)\). Since \(((x))=w(x)+b\), the baseline function is essentially a linear function after applying a logit transformation on the accuracies, and it can be determined by solving a linear regression. Then the single-ID effective robustness of a model \(f\) is evaluated as

\[(f)=_{}(f)-(_{}(f)),\] (2)

which subtracts the predicted OOD accuracy based on the ID accuracy \(_{}(f)\), from the actual OOD accuracy \(_{}(f)\).

## 3 Limitation of the Single ID Test Set

The existing effective robustness evaluation in Section 2 fixes a single ID test set for all the models, which is reflective of the ID performance only if all the models are trained on the same dataset that matches the ID test set. However, as large-scale pre-trained models emerge, it becomes necessary to compare models trained on different datasets, in order to know if the latest pre-training techniques can yield effective robustness gains. In this section, we use the comparison between zero-shot CLIP models and standard ImageNet models as an example to show the limitation of using a single ID test set: when only one ID test set is used, using different ID test sets leads to contradictory conclusions.

Following Fang et al. (2022), we compare models trained on ImageNet (Deng et al., 2009) and YFCC-15M (Thomee et al., 2016), respectively. On ImageNet, we include standard classifiers, and we also train CLIP models using templates filled with an ImageNet class name as the caption in a format of "A photo of a {class name}". We also train CLIP models on YFCC-15M, a dataset with image-text pairs. And we use ImageNet-V2 (Recht et al., 2019) as the OOD test set. We consider two different ID test sets. One ID test set is simply ImageNet. The other ID test set is constructed from YFCC-15M, since we have CLIP models trained on YFCC. We refer to this test set as "YFCC test set", and we refer to the accuracy on this test set as "YFCC accuracy". We discuss its details in Section 5.1 and Appendix B.2. Both ID test sets we consider here match the training distribution of some of the models (ImageNet models and YFCC models respectively) but not all the models.

We then plot the ImageNet-V2 accuracy of the models against their ImageNet accuracy and YFCC accuracy, respectively. There is a strong linear trend between the scaled ID accuracy and OOD accuracy for ImageNet models and YFCC models, respectively, and we plot fitting lines for these two sets of models, respectively. When the ID test set is ImageNet, Fig. 0(a) shows that the fitting line for YFCC models is generally above the fitting line for ImageNet models (except for the regime with extremely low accuracies), which appears to suggest that YFCC models have effective robustness gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 0(b) which uses YFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models, which instead appears to suggest that ImageNet models have greater effective robustness than YFCC models. We observe that when there is a mismatch in the training data and the ID test data, the

Figure 1: Class-subsampled (“css.” for short) ImageNet-V2 accuracy against ImageNet accuracy and YFCC accuracy, respectively, for 36 ImageNet models and 13 YFCC models that are also used in Table 2(a). A linear fit is generated for ImageNet models and YFCC-15M models, respectively. Accuracies and linear fits are under the logit scale. Class-subsampling is used to only include classes that appear in all the involved test sets (see Section 5.1).

models appear to have greater effective robustness (YFCC models in Figure 0(a) and ImageNet models in Figure 0(b)), as their performance on the ID test data and the OOD performance predicted from the single ID accuracy tend to be lower. This makes it difficult to compare models trained on different data and leads to imprecise conclusions on effective robustness if only one ID test set is used.

## 4 Multi-ID Effective Robustness

Considering the limitations of using a single ID test set, we propose a new way for effective robustness evaluation using multiple ID test sets that cover the training data distributions of all the involved models. We name it _multi-ID effective robustness_. Specifically, for each training distribution, we propose to prepare an ID test set that matches the training distribution, respectively. In particular, we focus on comparing models trained on two different datasets at a time in this paper, and we thereby use two ID test sets, where each of them corresponds to one of the training datasets.

While we refer to them as ID test sets, each of them is only the exact ID test set for some of the considered models that are trained on the distribution matching the test set, and it is not exactly an ID test set for all the considered models. However, we assume that the training distributions of all the models are still relatively close compared to the OOD test distributions (e.g., images normally collected from social medias in ImageNet (Deng et al., 2009), YFCC (Thomee et al., 2016), and LAION (Schuhmann et al., 2021) are relatively close compared to the OOD images in ImageNet-Sketch (Wang et al., 2019) that consists of sketch images specifically). In this way, both ID test sets are relatively ID for all the models compared to the OOD test sets, and it can be meaningful to control for the performance on these ID test sets when comparing the OOD performance.

We still use \(_{}()\) to denote the OOD accuracy, and we use \(_{1}()\) and \(_{2}()\) to denote the accuracy on the two ID test sets, respectively. In contrast to the previous baseline function \((x)\) in Eq. (1), we propose a new baseline function \((x,y)\) that predicts the OOD accuracy based on the accuracies \(x\) and \(y\) on the two ID test sets, respectively.

All the models in Figure 1 are trained on either ImageNet or YFCC. Thus, to compare their effective robustness under our new evaluation, we use two ID test sets for ImageNet and YFCC at the same time, in contrast to Figure 0(a) and 0(b) which use one ID test set separately at each time and results on the two different ID test sets lead to contradictory conclusions. As shown in Figure 2, we plot the OOD accuracy against the two ID accuracies on both two ID test sets in a 3D space. We observe that the data points approximately lie on a plane when plotted on the logit scale. This motivates us to instantiate \((x,y)\) as:

\[(x,y)=(w_{x}(x)+w_{y} (y)+b),\] (3)

where \(w_{x},w_{y},b\) are parameters. \((x,y)\), which is the plane in Figure 2, is also a linear function w.r.t. \(x\) and \(y\) under the logit scale, and thus it is a reasonable extension from \((x)\) by using a multi-dimensional linear function on the logit scale. We determine the parameters by solving an ordinary least squares regression to fit the accuracies. Metrics for linear regression such as the coefficient of determination, a.k.a. \(R^{2}\), can be used to evaluate the fitting quality of the baseline function. A high \(R^{2}\) value indicates that the OOD accuracy is accurately predicted by the baseline function from the ID accuracies, and thus the evaluated models have similar effective robustness. And our multi-ID effective robustness for a model \(f\) is defined as

\[(f)=_{}(f)-(_{1}(f), _{2}(f)).\]

Compared to the existing definition for effective robustness in Eq. (2), the major difference is the inclusion of two ID accuracies \(_{1}(f)\) and \(_{2}(f)\) in the baseline function, compared to using a single ID accuracy \(_{}(f)\).

Figure 2: Class-subsampled (“css.” for short) ImageNet-V2 accuracy against both ImageNet accuracy and YFCC accuracy for ImageNet models and YFCC models used in Figure 1 which shows the projections when only one of ImageNet accuracy and YFCC accuracy is used.

Generalizing to more than two training datasets.Although we focus on handling two training datasets at a time, our method may be generalized to more than two datasets in principle, by defining a baseline function based on multiple ID accuracies \(_{1}(),,_{k}()\). However, it could be costly as it would require training more models to fit a high-quality baseline function. We leave it for future works to reduce the cost when dealing with a larger number of datasets.

## 5 Experiments

### Settings

Models.In order to fit a baseline function, we need a large amount of models with diverse accuracies. To this end, we follow Taori et al. (2020) to train models with various proportions of data by subsampling from the entire training set (namely dataset subsampling), which effectively produces models with diverse accuracies. Moreover, we also combine examples from two datasets with different sampling ratios and train models on these combined datasets. This produces models with training distributions varying between the two training datasets and it is supposed to yield different combinations of the two ID accuracies. We use models trained on each single dataset as well as the combined datasets in the same fitting process, so that the baseline functions do not bias towards models trained on certain data. Our experiments include the following models:

* **Standard classifiers on CIFAR-10 and ImageNet.** We train standard classifiers on CIFAR-10 and ImageNet (Deng et al., 2009). We use ResNet-18, ResNet-50, and ResNet-101 (He et al., 2016). Additionally, we train models by combining CIFAR-10 and ImageNet at various ratios, where we upsample CIFAR-10 images from \(32 32\) to \(224 224\). Furthermore, we include ViT-S/16, ViT-B/16, ViT-L/16 models (Dosovitskiy et al., 2021) pre-trained on the whole ImageNet.
* **CLIP models.** On YFCC-15M (Thomee et al., 2016) and LAION-15M (Schuhmann et al., 2021) which consist of image-text pairs, we train CLIP models using ResNet-50 and ResNet-101. We also train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide additional details in Appendix B.1.

We use "{Name_of_dataset} models" to denote models trained only on the dataset, e.g., "CIFAR-10 models". And we use "{Name_of_dataset_A}+{Name_of_dataset_B} models" to represent models trained on a combination of two datasets, e.g., "CIFAR-10+ImageNet models".

ID test sets.We focus on image classification. Labeled image classification datasets such as ImageNet can be directly used for evaluating ID accuracy. For datasets that consist of image-text pairs for language-image pre-training without original labels, including YFCC and LAION, we automatically generate classification labels by matching captions with ImageNet classes, which has been similarly performed in Fang et al. (2022) for training classifiers using caption data, and we then hold out a balanced test set from the original dataset. More details are reported in Appendix B.2. Although it is possible to obtain a higher-quality test set by human labelling, we will show that using the automatically labelled test sets can already produce reasonable results.

OOD test sets.To compare the effectiveness robustness of models trained on CIFAR-10 and ImageNet, we use 3 CIFAR-like OOD test sets with natural distribution shifts, including CIFAR-10.1 (Recht et al., 2019), CIFAR-10.2 (Lu et al., 2020), and CINIC-10 (Darlow et al., 2018). We use 4 ImageNet-like OOD test sets to compare models trained on ImageNet with models trained on YFCC and LAION: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021), ImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019). We do not use ImageNet-A (Hendrycks et al., 2021) which involves adversarial filtering and has a different behavior in effective robustness evaluation (Taori et al., 2020; Fang et al., 2022).

Class subsampling and mapping.Considering that different test sets may not have the same classes, we follow prior works (Taori et al., 2020; Fang et al., 2022) to use class subsampling2 to retain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models,.

### Evaluation on CIFAR-like OOD Test Sets

We first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets. We show the fitting quality in Table (a)a and the effective robustness of various models in Table (b)b. Compared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher \(R^{2}\) and lower MAE), and thus provides a more precise understanding on the effective robustness. Specifically, while both single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms of the effective robustness, under the single-ID evaluation, ImageNet models achieve 3.91\(\)2.20 (%) and 2.77\(\)1.25 (%) effective robustness on CIFAR-10.2 and CINIC-10, respectively. The positive

Figure 4: Projected views of Figure (a)a. Figure (a)a and Figure (b)b correspond to single-ID evaluations using different ID test sets and yield contradictory conclusions on the effective robustness. Our multi-ID evaluation provides a more holistic view where all these models are approximately on a same plane and thus have similar effective robustness.

Figure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.

effective robustness values seems to suggest an advantage of ImageNet models compared to CIFAR-10 models, which is consistent with the findings in Miller et al. (2021). However, under the multi-ID evaluation, the advantage of ImageNet models diminishes, and the effective robustness values of both CIFAR-10 models and ImageNet models are much closer to 0. Therefore, the apparent advantage reported by prior works can be explained as the effect of training data on the single-ID evaluation, and our multi-ID evaluation resolves this confounder to provide a more precise understanding.

In Figure 2(a), we visualize the multi-ID effective robustness on CIFAR-10.2, where the accuracies of all the models approximately lie on a plane (the baseline function) on the logit scale, and thus these models have similar effective robustness as the OOD accuracy of all the models can be approximately predicted using a simple plane. We also show projected views of Figure 2(a) in Figure 3(a), where Figure 3(a) and Figure 3(b) correspond to the single-ID evaluation taking different ID test sets with contradictory conclusions. In contrast, our new evaluation provides a more holistic view.

### Evaluation on ImageNet-like OOD Test Sets

We then conduct evaluation on ImageNet-like OOD test sets, and we compare ImageNet models with models trained on YFCC and LAION, respectively. We show the fitting quality in Table 2 and the effective robustness in Tables 2(a) and 2(b). Consistent with results in Section 5.2, our multi-ID evaluation improves the fitting quality over the single-ID evaluation to better predict and understand the OOD accuracies from ID accuracies. On effective robustness, single-ID evaluation leads to a perception of an effective robustness gain when there is mismatch between the training data and the single ID test set. Our multi-ID evaluation enjoys a holistic view of all the training distributions and suggests that all the models evaluated here have similar effective robustness.

Specifically, the improvement of fitting quality is particularly significant for models involving LAION on ImageNet-R (\(R^{2}\) improved from 0.216 to 0.982 and MAE reduced from 9.23% to 1.32%) and ImageNet-Sketch (\(R^{2}\) improved from 0.281 to 0.937 and MAE reduced from 7.90% to 2.10%). On the effective robustness values, under the single-ID evaluation, YFCC and LAION models have positive effective robustness values (2.59\(\)2.43 (%) on average for YFCC models and 5.96\(\)4.96 (%) on average for LAION models), which is consistent with the findings in Fang et al. (2022); Nguyen et al. (2022). In contrast, under our multi-ID evaluation, the average effective robustness becomes 0.77\(\)0.85 (%) for YFCC models, and -0.00\(\)0.52 (%) for LAION models, much closer to 0. While single-ID evaluation used by prior works (Fang et al., 2022; Nguyen et al., 2022) suggests effective

Table 1: Results on CIFAR-like OOD test sets. 148 models are included, including CIFAR-10 models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models and ImageNet models also become closer to 0.

robustness gains of YFCC models compared to ImageNet models (Figure 4(a)), all the models have similar effective robustness under our multi-ID evaluation. Additionally, we provide an ablation study on using models with mixed training data in Appendix A.3 and additional interactive visualization on our website at https://shizhouxing.github.io/effective-robustness.

### Evaluation on Additional Models

We also evaluate additional models that are not used in fitting the baseline functions. We download models pre-trained by existing works, including OpenCLIP (Ilharco et al., 2021) and SLIP (Mu et al., 2022). OpenCLIP provides CLIP models trained on YFCC and LAION, and SLIP provides YFCC models trained using CLIP and also a combination of self-supervised learning (Chen et al., 2020, 2020) and CLIP (SimCLR+CLIP namely SLIP). And we also fine-tune CLIP models on ImageNet for models we pre-train on YFCC and LAION. We use both vanilla fine-tuning and also WiSE-FT (Wortsman et al., 2022) which aims to improve the robustness after fine-tuning, using a weight-space ensembling of the pre-trained model and the fine-tuned model. Details are in Appendix B.1.

In Table 4, we show results involving YFCC and LAION, respectively. Since these models are not used in the fitting, we do not use \(R^{2}\), but we use MAE to measure the fitting quality. Our multi-ID evaluation generally reduces MAE compared to the single-ID evaluation, and thus the multi-ID evaluation can still more accurately predict the OOD accuracy from the ID accuracies for these models that are not used in the fitting. The effective robustness values of the models also generally become closer to 0, especially for the zero-shot CLIP models. The results further validate that zero-shot CLIP models, although may achieve high OOD performance if pre-trained with large-scale data (Radford et al., 2021), generally do not improve effective robustness if we control for all the ID accuracies. Among the models evaluated here, SLIP models on YFCC and WiSE-FT models from LAION achieve relatively higher average effective robustness compared to other models, under our multi-ID evaluation, although the gains are not consistently significant on all the test sets and become much smaller than those reflected in the single-ID evaluation. However, we are not certain

Table 2: Fitting quality of single-ID and multi-ID effective robustness, respectively, on ImageNet-like OOD test sets. For ImageNet v.s. YFCC, models involved include ImageNet, YFCC, and ImageNet+YFCC models. For ImageNet v.s. LAION, models involved include ImageNet, LAION, and ImageNet+LAION models. Multi-ID evaluation improves the fitting quality.

Table 3: Single-ID and multi-ID effective robustness (%) of the models on variants of ImageNet OOD test sets. The effective robustness of all the models becomes close to 0 under the multi-ID evaluation.

on whether SLIP and WiSE-FT can alter the underlying training distribution, because SLIP uses SimCLR that introduces different training images, and WiSE-FT esseentially edits the weights of the models by weight-space ensembling. Thus, we do not draw a definite conclusion for the effective robustness of SLIP and WiSE-FT models and leave further validation for future work.

## 6 Related Work

For natural distribution shifts, the linear correlations between the ID and OOD performance ("accuracy-on-the-line" (Miller et al., 2021) in the single-ID effective robustness) have earlier been observed in dataset reproduction works (Recht et al., 2018, 2019; Yadav and Bottou, 2019; Miller et al., 2020). Taori et al. (2020) evaluated many ImageNet models on several ImageNet-like OOD test sets, and given the widely held linear correlations, they proposed to evaluate effective robustness by controlling for ID accuracy. Miller et al. (2021) further validated accuracy-on-the-line with a broader scope. Nevertheless, accuracy-on-the-line may not hold on some distribution shifts, such as some corruption shifts (Hendrycks and Dietterich, 2018) and shifts in the wild (Miller et al., 2021), and sometimes ID accuracy and OOD accuracy can inversely correlate (Miller et al., 2021; Teney et al., 2022). Baek et al. (2022) also observed a linear correlation between ID agreement and OOD agreement for a pair of neural networks (namely "agreement-on-the-line") for testing whether accuracy-on-the-line holds by agreement-on-the-line which does not require labeled data. We focus on distribution shifts where at least accuracy-on-the-line holds for models from each of the training datasets, and we further propose "accuracy-on-the-plane" using multiple ID test sets.

Recently, CLIP-like models with language image pre-training which has been studied earlier in works such as Sariyildiz et al. (2020); Zhang et al. (2022); Desai and Johnson (2021), were shown to achieve exceptional effective robustness in the single-ID evaluation (Radford et al., 2021; Jia et al., 2021). Fang et al. (2022) analyzed the cause of the effective robustness gain of CLIP and concluded that the pre-training data determined the robustness. Nguyen et al. (2022) experimented on more pre-training data, and they observed difference in the single-ID effective robustness of models trained on different data. While Fang et al. (2022); Nguyen et al. (2022) both suggested that the pre-training data could determine the effective robustness gains, our evaluation suggests that zero-shot CLIP models do not have effective robustness gains. Besides, Kumar et al. (2021); Andreassen et al. (2022); Wortsman

    &  &  &  \\  & &  &  &  &  \\  & & Single-ID & Multi-ID & Single-ID & Multi-ID & Single-ID & Multi-ID & Single-ID & Multi-ID \\   & ImageNet-V2 & 3.95 & 0.45 & 3.95\(\)0.70 & -0.33\(\)0.45 & 4.70 & 0.38 & 4.70\(\)0.01 & 0.38\(\)0.01 \\  & ImageNetNet-N & 8.98 & 3.12 & 8.98\(\)1.24 & 3.12\(\)1.27 & 39.80 & 7.49 & 3.89\(\)0.03 & 7.49\(\)0.09 \\  & ImageNet-N+Sketch & 5.32 & 2.49 & 5.32\(\)1.07 & 2.49\(\)0.81 & 38.92 & 1.31 & 38.92\(\)0.00 & 1.31\(\)0.23 \\  & ObjectNet & 5.68 & 1.04 & 5.68\(\)0.81 & -0.22\(\)1.04 & 6.94 & 1.35 & 6.94\(\)0.09 & -1.35\(\)0.04 \\   & Average & 5.98 & 1.77 & 5.98\(\)0.96 & 1.26\(\)0.89 & 2.29 & **2.63** & 2.59\(\)0.02 & **1.30\(\)0.07** \\   & ImageNet-V2 & 1.14 & 0.85 & 0.16\(\)1.23 & 0.77\(\)0.70 & 0.82 & 0.91 & -0.12\(\)0.89 & 0.15\(\)0.96 \\  & ImageNetN-R & 2.90 & 1.82 & -2.90\(\)1.86 & -1.82\(\)0.92 & 4.63 & 2.38 & -4.45\(\)2.89 & 2.27\(\)2.20 \\  & ImageNet-N+Sketch & 2.26 & 3.16 & 2.20\(\)1.86 & 3.16\(\)1.19 & 4.78 & 7.50 & 4.37\(\)4.67 & 5.70\(\)3.29 \\  & ObjectNet & 3.46 & 3.11 & -3.42\(\)1.36 & -3.11\(\)1.43 & 4.07 & 2.27 & -4.07\(\)2.10 & -2.26\(\)1.95 \\   & Average & 2.44 & **2.23** & -0.99\(\)1.76 & **-0.25\(\)0.96** & 3.57 & **3.27** & -1.29\(\)2.19 & **2.01\(\)1.38** \\   & ImageNet-V2 & 1.97 & 1.05 & 1.97\(\)0.76 & 1.05\(\)0.59 & 1.72 & 0.81 & 1.72\(\)0.45 & 0.81\(\)0.48 \\  & ImageNet-N & 3.64 & 1.76 & 3.64\(\)0.61 & 1.76\(\)0.60 & 13.08 & 7.40 & 1.08\(\)1.85 & 7.40\(\)1.10 \\   & ImageNet-N+Sketch & 5.47 & 4.41 & 5.47\(\)1.20 & 4.41\(\)0.99 & 16.84 & 10.21 & 16.84\(\)1.58 & 10.21\(\)0.49 \\  & ObjectNet & 2.12 & 1.30 & 2.12\(\)1.38 & -0.68\(\)1.34 & 1.65 & 1.65 & 1.52\(\)1.64 & 0.31\(\)1.67 \\   & Average & 3.30 & **2.13** & 3.30\(\)0.96 & **1.63\(\)0.80** & 8.32 & **5.02** & 8.29\(\)1.04 & **4.68\(\)0.55** \\   & ImageNet-V2 & 4.95 & 0.83 & 4.95\(\)0.29 & -0.83\(\)0.45 & & & \\  & ImageNet-N+R & 6.54 & 1.86 & 6.54\(\)2.66 & -1.86\(\)1.15 & & & \\  & ImageNet-N+Sketch & 1.49 & 4.16 & 0.58\(\)1.68 & -1.46\(\)0.44 & & & \\  & ObjectNet & 9.32 & 1.49 & 9.32\(\)1.45 & 1.49\(\)0.44 & & & \\   & Average & 5.57 & **2.09** & 5.35\(\)1.49 & **-1.34\(\)0.28** & & & \\   & ImageNet-V2 & 5.25 & 0.64 & 5.25\(\)0.57 & -0.12\(\)0.84 & & & \\  & ImageNet-N+R & 14.47 & 6.13 & 14.47\(\)5.98 & 5.75\(\)4.77 & & & \\    & ImageNet-Sketch & 7.71 & 2.90 & 7.71\(\)0.45 & 2.11\(\)2.79 & & & \\    & ObjectNet & 14.79 & 5.02 & 14.79\(\)2.77 & 5.02\(\)1.53 & & & \\    & Average & 10.56 & **3.67** & 10.36\(\)3.11 & **3.19\(\)2.05** & & & \\   

Table 4: Fitting quality and effective robustness for downloaded and fine-tuned models. The models are not used in the fitting and directly evaluated. Note that MAE and effective robustness are different, where MAE takes absolute values but not effective robustness. For CLIP by Mu et al. (2022) and SLIP, only models pre-trained on YFCC are available.

et al. (2022) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al. (2021); Santurkar et al. (2022) studied the transfer performance of CLIP models, which is out of our scope on the robustness against natural distribution shifts.

## 7 Conclusion

To conclude, we propose a new and more precise effective robustness evaluation for models with different training data. In our evaluation, the OOD accuracy can generally be better predicted from multiple ID accuracies compared to previous effective robustness evaluation with a single ID test. We find that zero-shot CLIP models pre-trained on language-image data do not have better effective robustness compared to standard image classifiers, and we provide a new understanding of the apparently significant effective robustness gains observed by prior works.

## 8 Limitations and Future Work

There remain several limitations that may be addressed in future works:

* Our method requires fully knowing the training distributions of all the evaluated models, which is not directly applicable for large-scale pre-trained models with private training data. And this also requires the training methods not to significantly alter the training distribution beyond basic data augmentation, while some methods such as SLIP may alter training distributions more significantly, and it is unclear how we can precisely define the training distribution for a model with post-training processing, such as WiSE-FT (Wortsman et al., 2022) and Model Soups (Wortsman et al., 2022) with weight-space ensembling. Future work may study how these techniques may impact the ID performance evaluation (Section 5.4).
* We assume that the two ID test sets have relatively close distributions compare to the OOD test sets. We have not considered how the difference between the multiple ID test sets may affect the evaluation, and how the effective robustness should be compared if models are trained on highly different distributions.
* We use fixed OOD test sets to evaluate the OOD performance, following previous works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022; Schuhmann et al., 2021). When models are pre-trained on large-scale data, it becomes unclear if these "OOD" test sets are still OOD, or if these test sets could be less OOD for the pre-trained models compared to standard classifiers. There may also be some distribution overlap between these test sets and the pre-training datasets, even though Radford et al. (2021) mentioned that the likelihood of direct data overlapping is low.
* We focus on distribution shifts where at least "accuracy-on-the-line" from existing works is known to hold for models trained on the same data (Taori et al., 2020; Miller et al., 2021), yet there are counterexamples where "accuracy-on-the-line" does not hold (Section 6) and requires further study. We may set a threshold on the fitting quality and only adopt our method when the fitting quality is sufficiently good. And there are also other OOD test sets (Singla and Feizi, 2021; Rusak et al., 2021; Singla et al., 2022; Idrissi et al., 2022; Li et al., 2023; Moayeri et al., 2022; Vasudevan et al., 2022) that have not been studied in the effective robustness works yet.
* While we mostly focus on comparing CLIP-like models with standard image classifiers, due to the notable OOD robustness of CLIP-like models studied in prior works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022), this work may further be extended to cover other types of models (Goyal et al., 2022; Singh et al., 2022) as well as other modalities such as distribution shifts on language data (Miller et al., 2020; Awadalla et al., 2022).
* Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively.