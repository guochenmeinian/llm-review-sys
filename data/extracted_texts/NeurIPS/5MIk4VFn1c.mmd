# Private Attribute Inference from Images with Vision-Language Models

Batuhan Tomekce, Mark Vero, Robin Staab, Martin Vechev

Department of Computer Science

ETH Zurich

tbatuhan@ethz.ch

{mark.vero,robin.staab,martin.vechev}@inf.ethz.ch

###### Abstract

As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus. While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that LLMs can make accurate privacy-infringing inferences from previously unseen texts. With the rise of vision-language models (VLMs), capable of understanding both images and text, a key question is whether this concern transfers to the previously unexplored domain of benign images posted online. To answer this question, we compile an image dataset with human-annotated labels of the image owner's personal attributes. In order to understand the privacy risks posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans. On this dataset, we evaluate 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger inferential adversaries, establishing an imperative for the development of adequate defenses.

## 1 Introduction

Since the release of ChatGPT , large language model-based (LLM-based) applications and chatbots have enjoyed a rapid adoption, surpassing hundreds of millions of daily active users . Towards making these models universally applicable, there has been a recent push for _vision-language models_ (VLMs) capable of understanding not only text but also reasoning over text and images jointly . The rapid adoption of LLM-based applications and the concurrent advances in the underlying models' capabilities raises several safety and privacy concerns among the general public, researchers, and regulators alike . In response, model providers are under increasing pressure from existing data protection regulations, such as the EU's GDPR  and the California Consumer Privacy Act (CCPA) , as well as from substantial ongoing regulatory efforts directly concerning AI . For instance, in 2023, Italy temporarily banned ChatGPT, citing data protection and privacy concerns . As such, exploring the potential privacy concerns of VLMs is a crucial first step towards a wider deployment of VLM applications that are privacy-preserving and regulation-compliant.

Privacy Implications of LLMsWeidinger et al.  lay out the privacy implications of LLMs from two separate perspectives: (i) memorization and (ii) inference. Although several works have examined private information memorization and leakage in LLMs , until recently, inference has remained unexplored. Enabled by the strong inferential capabilities  and vast world-knowledge  of current frontier models, Staab et al.  were first to examinethe potential of LLMs being misused for accurate inference of personal attributes from previously unseen online texts. Their results indicate that current frontier models such as GPT-4  already achieve close to human-level accuracy across various personal attributes (e.g., age, gender, location) while incurring only a fraction of the cost and time investment of a human. The recent rise of VLMs lifts this discussion from a text-only domain to include also images, raising the question of how the findings of Staab et al.  translate to the multi-modal setting. This is particularly relevant as even though image and video are ubiquitous in most influential social media platforms (e.g., Instagram, TikTok), privacy risks associated with automated VLM inferences have not been explored yet.

ExampleTo motivate our setting, consider Fig. 1 depicting a dormitory room. This picture could have been posted on a pseudonymized social media platform, such as Reddit (e.g., asking for arrangement advice) under the general assumption that one's privacy remains uncompromized. Despite no person being visible in the image, a human investigator may infer some personal attributes by reasoning over probable cues, such as recognizing the football team's logo or reverse image searching it. However, the involvement of a human detective prohibits the scalability of this approach, making its application on large-scale real-world data infeasible (already in 2014, there were 1.8 billion daily image uploads ). Yet, when feeding the image to a VLM (in this case to GPT4-V), the model can do the investigator's work, detecting the relevant cues and correctly inferring that the person living in the dormitory is a Green Bay Packers fan, and as such, has a high probability of living or having lived in Wisconsin. As model inferences are inherently scalable, VLMs enable such privacy violations at an unprecedented scale, requiring us to re-evaluate our understanding of online privacy.

This WorkFor the first time, we systematically analyze the capability of VLMs to infer private information from inconspicuous images posted online. Our findings indicate that similarly to the text-only domain, VLMs are able to infer a variety of personal attributes from real-world images both accurately and at an unprecedented scale. Notably, as we show in our evaluation, current safeguards against such privacy-infringing queries are ineffective in the face of simple evasion techniques, allowing for a low entry barrier for potential malicious actors. As such, we believe that with the advent of VLMs, threats to our online privacy are currently underestimated.

Circumventing Safeguards & Resolution LimitationsCurrent VLMs are commonly equipped with safeguards (both via model alignment and additional pre-processing) intended to prevent the model from answering privacy-violating queries . Several recent works  have shown that such safeguards can be broken both by manual intervention or by automated algorithms. To explore the privacy risks posed by an adversary misusing a VLM for privacy-infringing inferences, we develop a simple inference attack consisting only of the target image and a prompt to circumvent the safeguard. Notably, we found that once the safeguard of the model has been (easily) evaded, the model cooperates on the inference task by providing helpful guidance. In particular, we observe that it can recognize relevant parts of the image (e.g., a small note posted on a fridge) that could help the inference but which due to technical resolution limitations are too small to be analyzed. Building on this observation, we further develop an automated pipeline in which the model can decide to zoom into parts of the image that it believes to be relevant, effectively improving its inference capabilities.

A Visual Inference-Privacy DatasetDue to their extensive reasoning capabilities and world-knowledge VLMs could draw conclusions about private attributes not just from direct depictions of

Figure 1: Shortened example inference over an image using GPT4-V. The model recognizes the logo of the football team hanging on the wall and infers that the inhabitant of this dorm room is likely from Wisconsin, while also providing adequate reasoning. The person in the picture is occluded.

people, but also from other contextual cues, e.g., in a picture of a kitchen, products of local brands could be visible, or images of rooms with recognizable items such as college logos could reveal the posting person's educational background. Therefore, in order to evaluate the privacy-inference risks of VLMs beyond traditional human attribute recognition (HAR), we require a dataset of seemingly inconspicuous images alongside personal attribute labels. Current image datasets focusing on private attributes are insufficient for this task as: (1) They focus on a small specific set of attributes, such as gender, age, or the presence of certain clothing items  and (2) they almost exclusively consist of depictions of natural persons [25; 26]. To reflect the inference-based threat arising from the extended capabilities of VLMs appropriately, we create a dataset by collecting and manually annotating images posted on the pseudonymized social media platform Reddit, particularly focusing on images where private information is not sourced from direct depictions of humans.

We evaluate the performance of two widely adopted proprietary models, GPT4-V and Gemini-Pro , together with five open-source models available on Hugging Face . We find that although the safeguards of some of the models reject up to 54.5% of our queries when using a naive prompt, they can be easily circumvented via prompt engineering, making the models infer up to 77.6% of the private attributes correctly. Allowing the models to act autonomously and zoom in on details further improves the accuracy on certain features, e.g., precise location accuracy rises from 59.2% to 65.8%. Concerningly, this demonstrates that even safety-aligned VLMs can be misused as adversarial agents autonomously acting against their original safety objectives. Additionally, as with LLMs on text , we observe that the personal attribute inference accuracy is strongly correlated with the general capabilities of the models, implying that future iterations will pose an even larger privacy threat. Finally, VLM inferences are \(480\) faster and \(\)\(117\) cheaper than human annotation, indicating a paradigm shift in privacy considerations of images posted online, where the previous large time and monetary cost of inferences does not protect users anymore. Therefore, we advocate for further research into developing defenses against inference-based privacy attacks in the image domain, where the current safeguards are insufficient.

Main ContributionsOur main contributions are:

* The first identification and formalization of the privacy risks posed by vision-language models at inference time.
* Extensive experimental evaluation of 7 frontier VLMs at inferring personal attributes from real-world images.
* An open-source implementation1 of our dataset labeling tool and our inference pipeline to advance privacy research. 
Responsible DisclosureBefore making any copy or derivative of this work public, we contacted OpenAI and Google about our findings, providing them access to our data, prompts, and results.

## 2 Background and Related Work

Vision-Language ModelsFor the context of this work, we collectively refer to multimodal instruction-tuned foundational (large) language models with image understanding capabilities as vision-language models (VLMs). While combining different modalities for machine learning exhibits a long line of research , the first influential VLMs building upon foundational models have only appeared recently [29; 30; 31; 32; 33]. These methods achieve image understanding either by combining LLMs with pre-trained image encoders, or through joint training across modalities. Fundamentally, both methods rely on both the image and the textual input being translated to token embeddings and fed to a, usually decoder only, transformer model for processing. This approach is widely applied across both proprietary, i.e., GPT4-V and Gemini , and open-source  VLMs. Additionally, these models are often equipped with learned safeguards (i.e., they are _aligned_) to refuse queries that would lead to the generation of harmful responses [3; 4].

Personal Identifiable Information and Personal dataBoth _personal identifiable information_ (PII) as well as _personal data_ refer to information that can be attributed to a specific (natural) person. Inthe EU, the term personal data is defined via Article 4 in the EU's General Data Protection Regulation (GDPR)  as "any information relating to an identified or identifiable natural person." While PII definitions in the USA are commonly less comprehensive than the GDPR, they similarly include all information from which "the identity of an individual [...] can be reasonably inferred by either direct or indirect means." Notably, this includes attributes like gender, geographic indicators, or economic status. We note that as in , most attributes considered in this work (e.g., age, location, income, sex) fall under both personal data and PII definitions.

Large Language Models and PrivacyAs the pre-training datasets of LLMs consist of vast amounts of data across diverse sources, they often contain sensitive personal (identifiable) information. Therefore, studying the phenomenon of _training data memorization_, i.e., the verbatim repetition of training data sequences at inference time, has become an important area of research in the context of LLMs [35; 36; 37; 38; 13; 14; 15]. However, the restricted setting of exact memorization does in many cases fall short of covering other often highly contextual privacy notions . In particular, as it is limited to the models' training data, it cannot account for privacy-infringing inferences on previously unseen texts . Staab et al.  were the first to investigate the privacy risks of inferring personal information from text using LLMs, showing that current models can recover personal information even from seemingly anonymized text. However, their analysis was restricted to only the single modality of text, while current widely used frontier models are equipped with visual reasoning capabilities as well. In our work, we aim to bridge this gap by exploring the inference-based privacy threats of VLMs.

Human Attribute RecognitionHuman attribute recognition (HAR) focuses on recognizing features of natural persons from their visual depictions. These feature recognitions are formulated as binary or multi-label classification tasks on a single person, commonly focusing on a specific feature such as the person's sex, age, or dressing style . Before VLMs, state-of-the-art HAR models were trained by standard supervised learning, requiring access to highly task-specific and labeled (image-only) training data. Trained models then focused on singular tasks, e.g., recognizing specific attributes of pedestrians (PAR) . Recently, VLMs have also been successfully explored on various PAR datasets [40; 41; 42], showing promising results over prior, non-VLM-based methods. Although VLMs prove to be performant methods on PAR, their capabilities extend beyond the commonly restricted HAR settings. Notably, as existing HAR datasets are centered around direct depictions of humans, they do not cover the privacy risk arising from the application of frontier VLMs with advanced reasoning capabilities and broad lexical knowledge. In particular, as we show in Section 5, VLMs enable the automated inference of personal attributes from images that do not necessarily contain the subjected person in the image but, e.g., only an inconspicuous depiction of their living room.

## 3 Privacy Infringing Inferences with VLMs

In this section, we first introduce the considered threat model. Then, we proceed by presenting our prompting strategy that allowed us to circumvent the safeguards of even the most recent VLMs of OpenAI  and Google . Finally, we present our automated zooming scheme, enabling models to autonomously enlarge parts of the image it deems relevant for further inspection.

Threat ModelTo capture a general threat scenario, we assume an adversary with only black-box query access to a (frontier) VLM. The goal of the adversary is to get as much and as detailed personal information as possible from online images. At the same time, the attack shall remain simple and practical, keeping the entry requirements for any potential adversary low. Such an attack is particularly concerning, as its potential for automation enables execution at a scale unattainable by pre-VLM methods or human investigators. Crucially, this potential for scaling challenges our current understanding of online privacy, which in many cases and for many users relies heavily on the prohibitively high cost of obtaining private information from seemingly benign images and posts.

Circumventing Safeguards & Prompt EngineeringOften, the training of VLMs such as GPT4-V  and Gemini  includes a separate safety alignment stage with the goal of creating a model capable of refusing queries that lead to potentially harmful generations. However, as highlighted in Section 1, it has been shown that such training-based safeguards can easily be circumvented both by hand-crafted prompts or even fully automated attacks [21; 22; 23]. As such, to cover the full extent of privacy risks associated with inferences made by VLMs, it is imperative to construct an evaluation method that escapes such safeguards. Additionally, the prompt has to make use of the full capabilities of the model, avoiding a potential false sense of privacy through insufficient evaluation. To construct such a prompt we follow popular reasoning prompting practices, such as chain-of-thought prompting  to improve performance, and gamify the inference task in a similar vein to  to escape any safeguards. Additionally, we provide task-independent reasoning examples in the prompt, with the goal of increasing the model's attention to detail. We examine the impact of our prompting choices in Section 5, clearly demonstrating that "naive" prompts (_Where was this picture taken?"_) severely underestimate the inference-based privacy risks posed by current frontier vision-language models.

Automated ZoomingSmall details in an image often contribute to privacy-infringing inferences, e.g., a letter hanging on the wall in the background revealing the state one resides in, or recognizing a small university emblem on a larger item in the image signifying the person's educational background. However as most current VLM are limited in input resolution, they struggle to properly extract these small yet important details. As exemplified in Fig. 2, our experiments indicate that even though in some cases VLMs are not able to process small details (e.g., writing on a tax form), they are still able to recognize their potential importance for inference (a tax form contains personal information). In fact, the model can be prompted to return a bounding box for such a recognized clue, which in turn can be automatically processed to feed the model a cropped image enlarging the corresponding section. Based on this, we automate the zooming procedure by prompting the model for 3 regions to zoom into via outputting bounding boxes. Then, we adjust the bounding box to cover \(16\%\) of the image and be within image limits. Finally, we return the zoomed-in images in a second request to the model. In Section 5, we show the impact of zooming, e.g., it improves GPT4-V'_s precise_ location inference accuracy by up to 6.6%.

## 4 A Visual Inference-Privacy Dataset

In this section, we first argue that current image datasets for (private) attribute inference do not cover the novel privacy-infringing inference threat that VLMs pose. Bridging this gap, we then present our visual inference-privacy (VIP) dataset used for our empirical evaluation in Section 5.

Not Only Images of Humans Leak InformationAlthough there exist several datasets in the literature for (personal) human attribute recognition, they primarily focus on extracting and inferring features of persons included in the images, commonly in non-privacy related settings, e.g., pedestrian identification [25; 26]. This focus is also present in current HAR privacy benchmarks, with the explicit goal of a perceptual protection of humans _included_ in the images . However, with the rise of VLMs, which are capable of visual reasoning and are equipped with vast lexical knowledge, considering only images that include humans does not fully cover the potential privacy threat posed by these models. This is highlighted by our examples in Fig. 1 and Fig. 2, where private attributes are inferred from other objects in the depicted environment. Therefore, in this paper, we focus on evaluating the risk of

Figure 2: Illustrative example of GPT4-V recognizing that an item that is too small in the current resolution could provide it with more information about the inference task. The model is capable of returning a bounding box that can be used to crop the image before returning it for repeated processing.

private attribute inferences from images that primarily do not contain depictions of humans, a setting not considered under current benchmarks. To enable the evaluation of this arising privacy risk, we formulate three key criteria that a dataset for inference-based privacy evaluation has to fulfill.

Key CriteriaAs VLMs are no longer limited to the recognition of attributes of human visuals, we require a dataset that reflects this change in domain. In particular, the images should: (i) try to avoid containing full depictions of natural persons, (ii) be representative of what real people may post on (pseudonymized) online platforms, and (iii) come with a diverse set of labels covering a large set of private attributes as introduced in privacy regulations such as the GDPR .

Building a Visual Inference-Privacy DatasetTo the best of our knowledge, there currently does not exist any dataset that fulfills all three criteria. Therefore, we construct a visual inference-privacy (VIP) dataset, the first benchmark to evaluate the attribute inference capabilities of VLMs from seemingly innocuous images. An overview of our dataset collection pipeline is presented in Fig. 3. First, we source all images from the popular pseudonymized social media site Reddit, where we select a set of subreddits that are likely to contain posts with images suitable for our evaluation task (listed in Appendix D). Next, we manually label all images, using the image as the only source of information (i.e., no other data from the posting profile), but without time or internet browsing restrictions. Note that for ethical considerations, in line with the practices established by Staab et al.  also working with Reddit data, we _do not_ outsource the labeling task, instead, the labeling is fully conducted by the authors of the paper. To cover a wide range of attributes as required by criterion (iii), we collect the following private attributes: location of residence (LOC), place of image (POI), sex (SEX), age (AGE), occupation (OCC), income (INC), marital status (MAR), and education (EDU). Following Staab et al. , we also record a hardness score ranging from 1 to 5 for each label, corresponding to the difficulty for the labeler to extract/infer the label. Likewise we also adopt the scale used in , and rate from 1 to 3 for labels that require increasingly more complex reasoning but no online search. We assign hardness 4 and 5 to labels where the labeler required external knowledge tools, with hardness 5 indicating the additional need of advanced reasoning. As we only record the labels we could reliably extract from the image, we generally only obtain a label for a subset of the attributes per image. In a last step, to ensure that our recorded labels accurately reflect the profile of the posting author, we check the last 100 comments of the author, keeping only labels that are in line with the information contained in the comments. Note that we do not keep the comments for evaluation, as we aim to isolate the effect of privacy inferences from images, where the privacy leakage from text has already been explored in Staab et al. . The distribution of the resulting labels for the main private attribute categories are shown in Table 1. For a detailed overview of the labeling procedure and instructions, we refer the reader to Appendix D.

  Hard. & SEX & POI & AGE & INC & LOC & EDU & OCC & MAR & \(\) \\ 
1 & 17 & 1 & 4 & 3 & 11 & 1 & 6 & 4 & 47 \\
2 & 63 & 0 & 24 & 48 & 20 & 18 & 19 & 12 & 204 \\
3 & 48 & 0 & 53 & 31 & 8 & 15 & 5 & 10 & 170 \\
4 & 0 & 74 & 0 & 0 & 22 & 0 & 1 & 0 & 97 \\
5 & 0 & 17 & 1 & 0 & 16 & 2 & 0 & 0 & 36 \\  \(\) & 128 & 92 & 82 & 82 & 77 & 36 & 31 & 26 & 554 \\  

Table 1: Label counts for each main private attribute category across hardness levels in VIP.

Figure 3: Our data collection and labeling pipeline. In step 1, we collect images from a carefully selected set of subreddits that may contain images suitable for our task. Then, in step 2, we label the images manually while allowing the labeler to access online search for assistance. Finally, in step 3, we extract the comments of the profile that posted the image and keep only the obtained image labels that are not contradicted by the information contained in the comments. Note that we hide the true information on the tag and report an alternative location in the example.

## 5 Evaluation

In this section, we present the results of our experimental evaluation, which show how current frontier vision-language models enable privacy-infringing inferences from seemingly benign images. Additionally to the experiments presented in this section we include further results in Appendix B.

Experimental SetupWe evaluate two proprietary, GPT4-V  and Gemini-Pro  (Gemini), and five open-source models, LLaVa 1.5 13B , LLaVa-NeXT 34B , Idefics 80B , CogAgent-VQA , and InternVL-Chat-V1.2-Plus . All models are run for every image-attribute pair in the VIP dataset, prompting the models to predict one private attribute at a time. To decrease the impact of randomness on our results, we use greedy sampling (temperature \(0\)) across all our experiments. Unless mentioned explicitly, we use a single-round prompt with the models, not allowing for zooming, which we evaluate in a separate experiment. As described in Section 3, all proprietary models are aligned with safeguards. Therefore, we query these models via a gamified and CoT-extended prompt (later referred to as "Final" prompt) presented in Appendix E.3. We do so also for LLaVa-NeXT 34B and InternVL-Chat-V1.2-Plus. As CogAgent-VQA, Idefics 80B, and LLaVa 1.5 13B exhibit weaker language understanding capabilities and are mostly free from safeguards, we evaluate them with a simpler prompt (presented in Appendix E.5). Our prompting choices are motivated by avoiding the underreporting of the model's inference capabilities, and as such, potentially downplaying the posed privacy risk. We ablate the specific choice of prompts for all open-source models in Appendix B.3. Note that we do not evaluate other models than VLMs, as due to the complex nature of the VIP dataset, to the best of our knowledge, there is no supervised or other non-foundational machine learning method that generalizes to the challenging and diverse inference problem posed by VIP.

Calculating Inference AccuracyFor the categorical attributes of \(\), \(\), and \(\), we use a simple 0-1 accuracy in case the predicted category matches the label. For \(\), we report binary classification accuracy (has partner/no partner). Following the methodology of , for \(\), we let the model predict a probable interval for the subject's age. As our ground truth labels for \(\) also consist of intervals, we count the model's guess as accurate if the two intervals have over 50% overlap. For the attributes \(\) and \(\), which have a high degree of freedom, we take a hierarchical approach: If the label contains city- or state-level information that is correctly predicted by VLM, we count that as a _precise_ (P) correct prediction. When the model only predicts the country correctly, we still count it as a correct prediction for our main experiments but record that the inference has been _less precise_ (LP) than the actual label. If the label only contains country-level information and the prediction contains the correct country information, we count the prediction as precise. For the last attribute, \(\), we take a semantic approach tolerating some minor precision loss, where, for instance, "Electronics Engineer" counts as a correct prediction for "Electrical Engineer". We evaluate this in a two-step approach, first prompting GPT-4 for a similarity judgement and afterwards manually verifying it. We give a more detailed overview of our evaluating procedure in Appendix A.3. Unless otherwise mentioned, we report the less precise accuracy in our experiments.

Main ResultsWe show our combined results across all attributes and models in Fig. 4. Consistent with most benchmarks in the literature, we observe higher performance in proprietary models, with GPT4-V clearly outperforming all other models with a 77.6% accuracy. Remark

Figure 4: Comparison of the private attribute inference capabilities of all examined models on our collected Vision Inference-Privacy (VIP) dataset. GPT4-V is clearly the strongest model, with an accuracy of 77.6%, while the best open-source model, CogAgent-VQA achieves 66.4% accuracy.

ably, while GPT4-V is well-ahead of all models, CogAgent-VQA and Idefics 80B strongly outperform the proprietary model Gemini-Pro, with the best model reaching an accuracy of 66.4%. At the same time, other open-source models closely match Gemini-Pro in performance, with only LLaVa 1.5 13B lagging considerably behind with an inference accuracy of 53.3%. This result signifies that even if the safeguards of proprietary models were to be improved, there already exist open-source models that can make highly accurate privacy-infringing inferences. Further, in line with , we observe that newer iterations of models exhibit a gradually increasing capability of inferring private attributes. In fact, looking at the MMMU  visual understanding and reasoning benchmark's leaderboard , we can see that the ranking of the models on VIP closely matches the ranking (of the included models) on MMMU, indicating that privacy-inference and general capabilities are closely related. This result is concerning, as it shows that the inference-based privacy risk of VLMs will only increase with stronger models in the future, motivating a clear need for the development of targeted mitigations.

Accuracy over AttributesIn Table 2, we show the per feature accuracy of GPT4-V, CogAgent-VQA, and Gemini. Remarkably, GPT4-V exhibits a strong performance across most attributes, only struggling with inferring the income, where even the best model, CogAgent-VQA is only able to achieve 50% accuracy. Notably, GPT4-V achieves 94.5% accuracy on predicting SEX. At the same time, Gemini's performance is highly inconsistent across the examined attributes. While outperforming GPT4-V on PDI, reaching 87% accuracy, on other non-location attributes, it performs considerably worse, with, for instance, SEX falling close to random guessing accuracy. By manual inspection of Gemini's outputs we observe that this is mostly due to the limited capabilities of the model, with it often claiming that no sex is inferrable in the absence of a human in the image.

Humans in the ImageAs we constructed our VIP dataset to emphasize the inference capabilities of models from non-person-bound clues, only 9.7% of the collected labels came from images containing partial depictions of humans. Examples of these are depictions of hands, lower or full bodies, or reflections. To examine the impact of such depictions, we split our dataset into (1) images that contain parts of the human subject and (2) images that do not contain such depictions. In Table 3, we show our results for GPT4-V and the best open-source model, CogAgent-VQA, on these splits. We can observe that both models exhibit a higher accuracy on the split containing humans, which we hypothesize is due to the fact that most labels contained in this split are usually directly inferable from human depictions, e.g., 31 out of 54 labels total in the split are for the features SEX and AGE. At the same time, the models still exhibit relatively strong performance on images with no human subjects, with GPT4-V achieving a remarkable 76.4% accuracy, signifying that VLMs enable private attribute inference from inconspicuous images that would not be otherwise considered under current HAR-privacy benchmarks. Additionally, the gap between the models is larger in absence of humans in the image, highlighting the advanced reasoning capabilities of GPT4-V when it comes to non-human sourced clues in inferring personal attributes.

Impact of PromptingWe show the impact of our prompting techniques on the response refusal rate and accuracy in Table 4. Our baseline is a _naive_ prompt directly asking the model for a given private attribute ("Naive"). As introduced in Section 3, in order to overcome the safety alignment of the models, we add adversarial prompting elements, such as the gamification of the inference task ("Extended"). Finally, we further extend the prompt with chain-of-thought , reasoning guidelines, and generic reasoning hints to improve its performance ("Final"). All used prompts are included in Appendix E. Remark

   Model & GPT4-V & GPT4-V & GPT4-V & Gemini \\ Prompt & Naive & Extended & Final & Final \\  Refusal & 54.5 & 1.2 & 0 & 4.6 \\ Accuracy & 20.6 & 76.0 & 77.6 & 60.8 \\   

Table 4: Top: Rate [%] of models refusing to respond citing safety concerns. Bottom: Overall accuracy [%] over prompts.

    & SEX & POI & AGE & INC & LOC & EDU & OCC & MAR \\  GPT4-V & **94.5** & 79.3 & **74.4** & **46.3** & **87.0** & **77.8** & **77.4** & **69.2** \\ CogAgent-VQA & 80.5 & 63.0 & 67.1 & **50.0** & 71.0 & 52.8 & 71.0 & 61.5 \\ Gemini-Pro & 52.3 & **87.0** & 56.1 & 46.3 & 77.9 & 63.9 & 51.6 & 26.9 \\   

Table 2: Per feature accuracy [%] on GPT4-V, CogAgent-VQA, and Gemini-Pro. Notably, Gemini strongly outperforms other models on PDI, while lags behind on other features, with GPT4-V being the best model on most.

   Human & GPT4-V & CogAgent \\  & VQA \\  With & 88.9 & 81.5 \\ Without & 76.4 & 64.8 \\   

Table 3: Accuracy [%] of GPT4-V and CogAgent-VQA on images with and without human depictions.

ably, our simple extension over the naive prompting achieves substantial improvements in terms of bypassing the safety alignment of GPT4-V, reducing the rejection rate from 54.5% to a mere 1.2%. This is concerning as it confirms that currently applied safeguards are incredibly brittle against even basic circumvention methods. Further, Table 4 shows the impact of the prompts on the overall accuracy of GPT4-V, showing that escaping the safeguard with a gamified prompt provides the largest improvement, and further extensions in our "Final" prompt lead to additional accuracy gain.

Automated ZoomingWe examine the impact of automated zooming on the location attributes (LOC and POI), as predictions on other attributes were largely not subject to resolution limitations. We show our results in Table 5, comparing our final prompt with an additional automated zooming extension. We show the accuracy improvements made by GPT4-V on LOC and POI, distinguishing between precise (P) and less precise (LP) predictions. Notably, zooming provides the most accuracy improvements on precise predictions, enabling the model to make a more precise inference based on fine-grained clues in the images. As this process is automated, this result raises an important concern over the deployment of these models as autonomously acting inference adversaries.

Stability of Evaluation ResultsWhile our VIP dataset used for evaluating the inference-based privacy threat of VLMs directly reflects instances where such inferences pose a real-world privacy risk, it is limited in size. As such, while it allows for qualitative conclusions about VLM inferences it is not a priori clear how quantitatively stable our results are. For this reason, we conduct a closer examination of our results on GPT4-V. First, we calculate the \(95\%\) binomial confidence interval around the obtained inference accuracy of \(77.6\%\), resulting in a range of \(74.1\%\) to \(81.1\%\). Further, we estimate inference uncertainty by increasing the sampling temperature from \(0.0\) (as used in other results) to \(0.2\) and making three independent complete inferences over the whole dataset. This leads to accuracies of \(76.2\%\), \(77.1\%\), and \(77.8\%\), i.e., to an average of \(77\%\) with a standard deviation of \(0.62\%\)--highly stable quantitative performance w.r.t. sampling. Finally, we compare the inference accuracy across varying temperatures of \(0.0\), \(0.2\), and \(0.4\), obtaining accuracies of \(77.6\%\), \(77\%\) (average from before), and \(77.3\%\), respectively; showing stable performance across different temperatures. Overall, these experiments indicate that our results on VIP serve for both qualitative and quantitative statements about the real-world inference-based privacy risks of VLMs.

Inference Cost and ScalabilityAs also highlighted by Staab et al. , a key concern of automated privacy-infringing inferences is their strong scalability compared to human annotators. The labeling of our VIP dataset took around \(40\) hours of human work, which, using the hourly rates of roughly \(\$35\) set by our institution, amounts to \(\$1400\) in labeling costs. In contrast, using the OpenAI API, processing the whole dataset cost only \(\$12\) and took \(5\) minutes for GPT4-V, with further parallelization of the inferences possible. As such, GPT4-V inferences are \(\)\(117\) cheaper and \(480\) faster than relying on human annotators. Crucially, both the scalability and the accuracy of VLM-based inferences are expected to increase in the future, as evidenced by recent trends in decreasing API pricing and our observation of newer and stronger models performing better also on private attribute inference. As such, VLM-based inferences pose a significant privacy concern for online imagery, where (a partial notion of) privacy was before primarily provided by the poor scalability of human annotation.

## 6 Discussion

Our empirical evaluation highlights several key privacy threats posed by VLMs, which are especially severe in the face of the wide adoption of these models: (1) Both proprietary and open-source models are capable of making accurate privacy-infringing inferences. (2) The safeguards of the better performing proprietary models such as GPT4-V are brittle and can be easily circumvented in practice, potentially providing a false sense of privacy. (3) As observed previously for text-only models, the capabilities of VLMs to infer personal attributes from images are directly correlated with their performance on other harmless and useful tasks. This is especially concerning, as it is to be expected that upcoming VLMs will only improve in general capabilities, and hence also on the results

   Attribute & LOC (P) & LOC (LP) & POI (P) & POI (LP) \\  Final Prompt & 58.4 & 87.0 & 34.8 & 79.3 \\ + Zoom & +6.5 & +0.0 & +4.3 & +2.2 \\   

Table 5: Precise (P) and less precise (LP) location (LOC) and place of image (POI) prediction accuracies [%] of GPT4-V on ”Final” prompt vs. added zooming.

we have shown in this work, making the threat to user privacy even more imminent. (4) Finally, the cost and time efficiency of such inferences means a categorical paradigm shift in privacy related to online imagery, with VLMs enabling privacy-infringing inferences at an unprecedented scale. Below, we further discuss the ethical implications of our study, potential mitigations, and limitations in detail.

Ethical ConsiderationsDue to the sensitive nature of our study, we have taken several steps to ensure the no individual's privacy is compromised: (i) for constructing our VIP dataset, we have only taken images already included in prior public Reddit dataset dumps, (ii) we kept the labeled attributes to a set of ethically permissable ones, omitting more sensitive features such as mental health or race, (iii) we kept the labeling and the dataset on premise, providing access only to the authors, and (iv) we do not make our labeled dataset public and show only artifacts which do not compromise the privacy of any individual (e.g., aggregate results and modified examples). We note that these data handling practices have been cleared by our IRB. Further, on a broader perspective, even though we do not present an end-to-end solution to the uncovered threat here, we believe it is essential to publish our findings, and that the benefits of making the broader community aware of privacy threats posed by VLMs ultimately outweighs the potential short-term negative threat of adversaries replicating our attack. Especially, as it is not impossible that such inferences are already being conducted, based on precedence of similar misuses of foundation models on text .

Potential MitigationsWhile developing advanced defense methods against inference-based privacy attacks is beyond the scope of this paper, we strongly advocate for further action on improving both user-side and provider-side mitigations. On the provider side, we believe that our findings can be leveraged to strengthen the safety alignment of the models, training them to deny requests of potentially private attribute inference. However, as privacy-inference and general capabilities of the models are aligned, it can be challenging to balance a potential loss in utility with increased privacy protection. From the perspective of internet users that upload images, a potential direction for privacy protection could be an adaptation of the adversarial anonymization framework developed for text in . Here, a VLM could be used to inform an image editing model about elements in the image that have to be obfuscated in order to remove the visual clues of private information.

Nonetheless, in our view, a crucial first step towards a more responsible use and deployment of VLMs is the wide-ranging awareness of the potential privacy risks across providers, regulators, and users alike. Providers have to be aware of such risks when enabling access to their models; regulators have to prepare sufficient legal instruments to protect users' rights for privacy; social media platform owners should make their users aware of inference-based privacy threats; and users have to be aware of the full extent of how their privacy may be compromised and adjust their online behavior accordingly. With this work we hope to take an important step into this direction.

LimitationsThis work aims to provide the first characterization and evaluation of the inference-based privacy threat arising from recent frontier VLMs. This evaluation is enabled by a manually collected real-world image dataset alongside a wide selection of manually annotated personal attributes. Due to the sensitive nature of such datasets and in line with previous works as well as ethical concerns, we decided not to release the VIP dataset publicly. While VIP allowed us to make a qualitative assessment of the discussed risks, we believe that the field may benefit from future efforts in constructing larger-scale public benchmarks. As similar ethical concerns apply here, we see well-curated synthetic benchmarks as a promising remedy to evaluation data limitations.

## 7 Conclusion

In this work, we conducted the first investigation of the privacy risks emerging from the inference capabilities of frontier VLMs by tackling two key challenges: (1) To allow for a quantitative assessment, we constructed the first dataset for evaluating privacy-infringing inference from inconspicuous online images, and (2) we built a simple prompting scheme suitable for evaluating the full extent of potential private attribute inferences by enabling the evasion of current safeguards. Our evaluation shows that built-in safeguards of models are easily evaded, enabling the best model to achieve 77.6% overall accuracy. Our results indicate that large-scale, automated, and highly accurate inferences of private attributes from images posted online are already becoming feasible. With current defenses lacking, we, therefore, aim to raise awareness with our findings and appeal to the community for an increased focus on mitigating privacy threats from inferences with frontier VLMs.