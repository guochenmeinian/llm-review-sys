# Multi-Stage Predict+Optimize

for (Mixed Integer) Linear Programs

 Xinyi Hu\({}^{1}\), Jasper C.H. Lee\({}^{2}\), Jimmy H.M. Lee\({}^{1}\), Peter J. Stuckey\({}^{3}\)

\({}^{1}\)Department of Computer Science and Engineering,

The Chinese University of Hong Kong, Shatin, N.T., Hong Kong

\({}^{2}\)Department of Computer Science, University of California, Davis, USA

\({}^{3}\)Department of Data Science and AI, Monash University, Clayton, Australia

{xyhu,jlee}@cse.cuhk.edu.hk, jasperlee@ucdavis.edu, peter.stuckey@monash.edu

###### Abstract

The recently-proposed framework of Predict+Optimize tackles optimization problems with parameters that are unknown at solving time, in a supervised learning setting. Prior frameworks consider only the scenario where _all_ unknown parameters are (eventually) revealed at the same time. In this work, we propose _Multi-Stage Predict+Optimize_, a novel extension catering to applications where unknown parameters are instead revealed in sequential stages, with optimization decisions made in between. We further develop three training algorithms for neural networks (NNs) for our framework as proof of concept, all of which can handle mixed integer linear programs. The first baseline algorithm is a natural extension of prior work, training a single NN which makes a single prediction of unknown parameters. The second and third algorithms instead leverage the possibility of updating parameter predictions between stages, and trains one NN _per stage_. To handle the interdependency between the NNs, we adopt a sequential and parallelized versions of coordinate descent for training. Experimentation on three benchmarks demonstrates the superior learning performance of our methods over classical approaches.

## 1 Introduction

_Constrained optimization_ problems can frequently model applications in everyday life, yet, the parameters of the problem are unknown at the time of solving. Consider, for example, a real-world application where hospital administrators need to schedule shifts for nurses, so as to minimize the total costs for hiring nurses while meeting the patient load. Here, the shifts need to be decided before the (real-time) patient demand is known, which requires _predicting_ the demand when scheduling.

In the present work, we focus on the supervised learning setting, where unknown parameters can be predicted using relevant features, and historical (features, parameters) pairs serve as training data for the prediction model. The goal is to estimate the unknown parameters based on the related features, such that when we solve the optimization problem using the estimated parameters, the resulting solution is good even under the later-revealed true parameters.

Classic approaches, for example by learning predictors using (regularized) \(_{2}\) loss, do not necessarily work well -- low prediction error in parameter space does not guarantee good performance of the estimated solution according to the optimization objective. The influential framework of Predict+Optimize proposed by Elmachtoub and Grigas  instead uses the more effective _regret loss_, which incorporates information about the optimization problem. However, the framework limits the unknown parameters to appear only in the objective and not the constraints -- if uncertainty in constraints is mis-predicted, the resulting estimated solution might not even be feasible under the true parameters. Recent works by Hu et al.  thus propose a _Two-Stage_ adaptation ofPredict+Optimize, explicitly modelling 1) a prediction stage and 2) a recourse stage which corrects infeasible solutions into feasible ones. The new two-stage framework is therefore applicable even when the optimization constraints contain uncertainty.

However, the two-stage framework essentially assumes that all the unknown parameters are revealed simultaneously, excluding applications where such information is gradually released and new decisions need to be made across many stages (e.g. in a daily/weekly manner). Crucially, in these applications, predictions can also be updated between stages, in light of the new information and past committed actions. Consider again the example of scheduling shifts for nurses. A typical facility might have an appointment system, with reservations closing the day before each working day. As opposed to a two-stage modelling, a more practical approach would treat each day in a work week as its own stage where new information (the precise appointments the next day) is learned, inducing both new optimization decisions and updated predictions.

The concrete contributions of this paper are three-fold.

FrameworkWe propose and formalize the new framework of _Multi-Stage Predict+Optimize_ (Section 3), where unknown parameters are revealed across multiple stages, inducing new optimization decisions and updated parameter predictions.

Training algorithmsThe flexibility to update (future) parameter predictions in each stage introduces intricate challenges to the training process, which should train a prediction model _per stage_. The challenges are both in predictive power and in computation time. The performance of predictors across stages are _intertwined_ and _interdependent_: the "goodness" of a prediction depends on actions in other stages, which in turn depends on predictions of those other stages. Such dependency can also cause serialization issues that could drastically lengthen training time.

In Section 4, we propose three neural network training algorithms for our framework, assuming that the optimization problems can be formulated as mixed integer linear programs (MILPs): 1) a baseline algorithm that directly generalizes the two-stage algorithm of Hu et al. , training only a single neural network predictor, 2) a sequential coordinate descent training algorithm which trains a neural network model _per stage_, and each stage is considered a "coordinate", and 3) a parallel version of coordinate descent. These algorithms trade off between training time and predictive performance.

Empirical evaluationWe apply these methods to three benchmarks (Section 5) to empirically demonstrate their superior learning performance over classical training methods, as well as the computation/prediction tradeoff between the proposed methods.

We note that there are other lines of work tackling similar settings, where unknown parameters in optimizations are also revealed in a sequential fashion. Perhaps the most well-known is multi-stage stochastic optimization . The main difference between our work and multi-stage stochastic optimization is _supervised_ vs _unsupervised_ learning. Our framework (and Predict+Optimize in general) has features that help making parameter predictions, whereas (non-contextual) stochastic optimization does not and requires different techniques to tackle. See Appendix A.3 for a detailed discussion on the connection and comparison between our framework and stochastic optimization.

Related WorkWe include a brief literature review here in the main body. See Appendix A for a detailed exposition.

Elmachtoub and Grigas proposed the influential framework of Predict+Optimize , with lots of followup work in the community on improving computational efficiency , predictive accuracy , on types of applicable optimization problems , and applying to specific real-world scenarios . More recently, Hu et al.  proposed adaptations of the framework to handle uncertainty in optimization constraints, including the Two-Stage framework which our work is most related to.

Predict+Optimize also sits in a broader line of work on _decision-focused learning_, including works that learn prediction models for unknown parameters but with different goals/loss measures , as well as other works that invent techniques for differentiating through optimization problems .

Outside of decision-focused learning, our work is also somewhat related to (multi-stage) stochastic programming [14; 30]. The main difference, again, is supervised vs unsupervised learning. See Appendix A.3 for a more detailed comparison.

## 2 Background -- Two-Stage Predict+Optimize

In this section, we describe Two-Stage Predict+Optimize , which is prerequisite to understanding our contributions. Without loss of generality, the framework is stated for minimization problems.

A _parameterized optimization problem (Para-OP)_\(P()\) is defined as finding:

\[x^{*}()=*{arg\,min}_{x}obj(x,)C(x,)\]

where \(x\) is a vector of decision variables, \(\) is a vector of parameters, \(obj\) is a function mapping decisions \(x\) and parameters \(\) to a real objective value that is to be minimized, and \(C\) is a set of constraints that must be satisfied over \(x\) under parameters \(\). We call \(x^{*}()\) an _optimal solution_ under parameters \(\), and \(obj(x^{*}(),))\) the _optimal value_ under parameters \(\). When the parameters are all known, a Para-OP is just a classical optimization problem (OP).

In the Predict+Optimize setting (from the original framework , to the two-stage framework , and to our multi-stage framework later on), each instantiation of the true parameter vector \(\) has an associated _feature matrix_\(A\). These features are relevant information that can help a model predict \(\).

In the following, we number the stages by 0 and 1 in Two-Stage Predict+Optimize  to make the framework look more similar to the multi-stage framework we propose later in Section 3.

Stage 0The practitioner uses a prediction model, which takes in a feature matrix \(A\), to compute a vector of estimated parameters \(\). The Stage 0 solution \(_{0}\) is then computed as

\[^{(0)}=*{arg\,min}_{x}obj(x,)C(x,)\]

The Stage 0 solution \(^{(0)}\) should be interpreted as a form of soft commitment that can be modified in Stage 1, subject to a penalty.

Stage 1The true parameters \(\) are revealed, and the practitioner wishes to compute an updated Stage 1 solution \(^{(1)}\) from \(^{(0)}\), subject to a (problem-specific) penalty function \(Pen(^{(0)}^{(1)},)\) which depends on both the softly-committed Stage 0 \(^{(0)}\), the final Stage 1 solution \(^{(1)}\) and the true parameters \(\). More specifically, the Stage 1 solution \(^{(1)}\) is computed as

\[^{(1)}=*{arg\,min}_{x}obj(x,)+Pen(^{(0)}  x,)C(x,)\]

The Stage 1 solution \(^{(1)}\) should be interpreted as a hard-committed final action, and note that it is guaranteed to be feasible under the true parameters \(\).

The prediction \(\) is evaluated using the _post-hoc regret_, which is the sum of two terms: (a) the difference in objective between the _true optimal solution_\(x^{*}()\) and the final Stage 1 solution \(^{(1)}\) under the true parameters \(\), and (b) the penalty incurred by modifying \(^{(0)}\) to \(^{(1)}\). Formally, the post-hoc regret function \(PReg(,)\) (for minimization problems) is:

\[PReg(,)=\ obj(^{(1)},)-obj(x^{*}(), )+Pen(^{(0)}^{(1)},)\]

The goal of a prediction model is to make predictions \(\) so as to minimize the post-hoc regret. We emphasize again that the main difference between Predict+Optimize frameworks and stochastic programming frameworks is that in Predict+Optimize, a prediction model has access to features relevant to the true parameters in order to make a prediction. Stochastic programming, on the other hand, frequently operates solely at the level of the distribution over the true parameters.

Multi-Stage Predict+Optimize

In this section, we present our new framework of _Multi-Stage_ Predict+Optimize, which models applications where unknown parameters are revealed across \(T\) different stages.

Consider again the Para-OP

\[^{*}()=*{arg\,min}_{}\,obj (,)C(,)\]

We view the true parameter vector \(\) as \((_{1},,_{T})\), where each \(_{t}\) is the sub-vector of parameters released at Stage \(t\). Similarly, we also view the vector of decision variables \(\) as \((x_{0},,x_{T})\), where each \(x_{t}\) is the sub-vector of decision variables that are hard-committed in Stage \(t\) (e.g. via a concrete real-world action taken at Stage \(t\)) and soft-committed in prior stages (e.g. a tentative nurse schedule).

At a high level, the parameters \(_{t}\) are revealed at Stage \(t\), and a model makes a prediction \(}^{(t)}=(_{t+1}^{(t)},,_ {T}^{(t)})\) of all the _unrevealed_ parameters. Then, the practitioner solves the Stage \(t\) optimization problem which we define later in the section. The decision variables \(x_{t}\) are newly hard-committed, whereas the decision variables \(x_{t+1},,x_{T}\) are soft-committed with potential to be modified in future stages (at the cost of a penalty). This process is repeated until all stages are completed.

In the rest of the section and paper, we will use the standard notation of \([t_{1}:t_{2}]=(_{t_{1}},,_{t_{2}})\) to denote sub-vectors (treated as arrays), and use the notation \(\) for vector concatenation.

### Formal Framework Definition

Now we formally define Multi-Stage Predict+Optimize framework. In Appendix B, we also present the hospital scenario from the Introduction as a detailed example of applying this framework.

Stage 0None of the true parameters have been revealed. \(Model_{0}\) takes the feature matrix \(A\) and predicts \(}^{(0)}=(_{1}^{(0)},,_ {T}^{(0)})\). The practitioner then computes the Stage 0 solution \(^{(0)}\) as

\[^{(0)}=*{arg\,min}_{x}\,obj(x,}^ {(0)})C(x,}^{(0)})\]

The decision variables \(_{0}^{(0)}\) are hard commitments, whereas the rest of the decision vector \(_{1}^{(0)},,_{T}^{(0)}\) are soft commitments.

Stage \(t\) (for \(1 t T\))The true parameters \(_{1},,_{t-1}\) were previously revealed, and \(_{t}\) is newly revealed. \(Model_{t}\) makes a prediction \(}^{(t)}=(_{t+1}^{(t)},,_ {T}^{(t)})\) using 1) the feature matrix \(A\), 2) the previous stage solution \(^{(t-1)}\) and 3) the revealed true parameters \(_{1},,_{t}\). For computational efficiency reasons, \(Model_{t}\) may instead take any subset or derived functions of the above inputs. For example, \(Model_{t}\) can choose whether or not to incorporate the revealed true parameters \(_{1},,_{t}\) as input. While these revealed parameters can serve as additional features, potentially guiding and correcting current predictions more effectively, they also increase training time (and inference time to a smaller extent). The trade-off between prediction improvement and additional training time depends on the optimization problem, model structure, and training data. Therefore, whether to utilize the revealed true parameters can be considered a hyperparameter that should be tuned for each application using the available training data. See Appendix H.1 for a more detailed discussion.

Afterwards, the practitioner computes the Stage \(t\) solution \(^{(t)}\) using the following Stage \(t\) optimization problem, which crucially modifies the original Para-OP by: 1) introducing a penalty term \(Pen_{t}(^{(t-1)} x,[1:t])\) capturing the cost of changing the Stage \(t-1\) solution \(^{(t-1)}\) to the new Stage \(t\) solution \(x\), and 2) introducing the constraint that \(x[1:t-1]=^{(t-1)}[1:t-1]\), namely, hard commitments from prior stages cannot be modified in the current Stage \(t\). This constraint is a form of a non-anticipativity constraint in the stochastic programming literature .

\[^{(t)}= *{arg\,min}_{x}\,obj(x,[1:t] }^{(t)})+Pen_{t}(^{(t-1)} x, [1:t])\] s.t. \[C(x,[1:t]}^{(t )}) x[1:t-1]=^{(t-1)}[1:t-1]\]The Stage \(t\) solution, by construction, has \(^{(t)}[1:t-1]\) being equal/compatible with the hard commitments from prior stages. The new hard commitments are \(^{(t)}_{t}\), and the rest of the decision vector \(^{(t)}[t+1:T]\) are new soft commitments we make for the future stages.

At \(t=T\), the prediction \(}^{(T)}\) is a length-0 vector since all the true parameters will have been revealed.

For the rest of the paper, we make the assumption that these Stage \(t\) optimizations are _always_ satisfiable, regardless of the prior stage solutions, prior+current predictions and revealed parameters. For practical applications, this assumption is both natural and essential. In real-world scenarios, encountering an unsatisfiable condition can lead to catastrophic outcomes. Therefore, before using the application, the domain expert should always have designed the underlying real-world system to have recourse actions to mitigate bad prior commitments (at cost/penalty) and to prevent catastrophe, and furthermore model such recourse actions in the (multi-stage) optimization problem. Any system and the corresponding formulation of multi-stage optimization problem lacking such recourse should not be used/executed. It is thus a reasonable assumption and a practical responsibility our framework asks of the practitioner, that recourse actions are always designed into the underlying system and modelled, so that our feasibility assumption is satisfied.

Evaluation:The sequence of predictions \(}^{(0)},,}^{(T)}\), which along with the true parameters \(\) induces the sequence of solutions \(^{(0)},,^{(T)}\), is evaluated using a generalized notion of post-hoc regret, defined as follows (for a minimization problem):

\[PReg((}^{(0)},,}^{(T)}),)= obj(^{(T)},)-obj(x^{*}(),)+_{t} Pen_{t}(^{(t-1)}^{(t)},[1:t])\]

where \(x^{*}()\) is again the optimal in-hindsight vector of decisions for the original Para-OP.

We note that if a problem has only 2 stages (Stages 0 and 1), then our framework of Multi-Stage Predict+Optimize indeed captures Two-Stage Predict+Optimize described in Section 2.

## 4 End-to-End Training Algorithms on MILPs

In this section, we give 3 training algorithms for neural network models for the Multi-Stage Predict+Optimize framework, under the assumption that all Stage \(t\) optimization problems can be formulated as (mixed integer) linear programs (MILPs).

The first training algorithm, our baseline (Section 4.1), is a straightforward generalization of the one proposed for the two-stage framework . This algorithm only trains a single neural network and reuses the same parameter predictions across all stages. Although the approach is computationally efficient, it fails to fully exploit the power of the framework, which allows for predictions to be updated at each stage.

Our second and third algorithms (Sections 4.2 and 4.3) instead train one neural network per stage, each making new parameter predictions for the corresponding stage. As mentioned in Section 1, it is delicate to train these neural networks. The quality of a prediction in one stage depends on decisions in other stages, which in turn depends on predictions made in those other stages. To handle this dependency, we employ a coordinate-descent strategy, where each stage/neural network is a coordinate. We present both a sequential version and a parallel version of this strategy as training algorithms, which trade off between training time (sequential being slower) and predictive performance.

We also point out that it is technically possible to train all networks simultaneously, without using coordinate descent like in Sections 4.2 and 4.3. To do so, we would instead use ground truth parameters in place of prior and future stage predictions. However, intuitively, this simpler approach should have worse predictive ability than the proposed two methods, given the interdependency of the predictors. We show experimental comparisons in Appendix H.2 confirming this intuition.

In Section 5, we show empirical results comparing these algorithms and classic non-Predict+Optimize learning algorithms (e.g. standard regression models), which demonstrate that even our baseline training approach outperforms classic non-Predict+Optimize algorithms. Also, our more sophisticated approaches yield even better learning performance, at the cost of additional training time.

### Baseline: Extending the Two-Stage Approach to Train a Single Neural Network

We first present a baseline Predict+Optimize training algorithm for our multi-stage setting, via a natural extension of the two-stage approach .

This baseline algorithm trains a _single_ neural network \(N\!N\), which takes a feature matrix \(A\) and returns the prediction \(}=N\!N(A)\). The same predictions are then reused across all the stages. More specifically, in the language of Section 3, we choose \(}^{(t)}=}[t+1:T]\) for this basic approach.

We use standard gradient methods to train the neural network \(N\!N\), with the goal of minimizing post-hoc regret as defined in Section 3:

\[PReg=obj(^{(T)},)-obj(x^{*}(),)+_ {t}Pen_{t}(^{(t-1)}^{(t)},[1:t])\]

Noting the second term is independent of \(}^{(0)}\) and hence \(N\!N\), the gradient with respect to an edge weight \(w_{e}\) in \(N\!N\) is

\[\,PReg}{w_{e}}=\,obj(^{(T)}, )}{w_{e}}+_{t=1}^{T}\,Pen_{t}(^{(t-1)}^{(t)},[1:t])}{w_{e}}\]

By the law of total derivative, we can expand this to

\[\,PReg}{w_{e}}=\,obj(^{(T)}, )}{^{(T)}}^{(T)}}{}}}}{w_{e}}+_{t=1}^{T} .}{^{(t-1)}}|_{^{(t)}}^{(t-1)}}{}}+. }{^{(t)}}|_{^{(t-1)}} {^{(t)}}{}}\,}}{w_{e}}\]

The term \(}}{w_{e}}\) is calculated via standard backpropagation, while the terms \(\,obj(^{(T)},)}{^{(T)}}\), \(.}{^{(T-1)}}|_{^{(t)}}\) and \(.}{^{(t)}}|_{^{(t-1)}}\) are calculable when the objective and penalty functions are smooth. The only non-trivial calculation is for \(^{(t)}}{}}\) for all \(t[T]\).

Recall that \(^{(t)}\) is computed from the Stage \(t\) optimization problem, as a deterministic function of \(^{(t-1)}\) and \(}^{(t)}\) (which is a sub-vector of \(}\) here, since we reuse predictions), while \(^{(t-1)}\) itself also depends on \(}\). We thus further decompose \(^{(t)}}{}}\) into the following recursive computation

\[^{(t)}}{}}=.^{(t)}}{^{(t-1)}}|_{}^{(t-1)}}{}}+.^{(t)}}{}}|_{^{(t-1)}}\]

Calculating \(.^{(t)}}{^{(t-1)}}|_{}}\) and \(.^{(t)}}{}}|_{^{( t-1)}}\) requires differentiating through a MILP. So instead of directly using MILP formulations for the Stage \(t\) optimization problems, we use the convex relaxation by Hu et al. , which in turn adapts the approach of Mandi and Guns .

We also note that it is possible to use other convex relaxations and approaches to differentiate through the MILP, for example using tools like CvxpyLayers . We chose Hu et al.'s calculations because their experiments showed the computational efficiency of their approach over other tools.

### Sequential Coordinate Descent

Training only one neural network, the baseline algorithm is efficient but fails to fully harness the power of the framework in Section 3. Each stage \(t\) makes new decisions, and the "goodness" of future decisions depends on these previously committed decisions. Thus, new predictions should be made each stage for future parameters, based on prior-stage optimization decisions, so as to yield current and future optimization decisions that work well with the already-committed ones. However, the baseline algorithm ignores this information and does not update the predictions accordingly.

We thus propose our second training algorithm, which trains one neural network per stage, from Stages 0 to \(T-1\). The neural network \(N\!N_{t}\) for Stage \(t\) takes the feature matrix \(A\) as input, as well as all the prior decision vector \(^{(t-1)}\), and outputs the prediction \(}^{(t)}\) for parameters \([t+1:T]\). The astute reader might recall that the proposed framework in Section 3 allows \(N\!N_{t}\) to utilize the revealed parameters from stage \(t-1\) as additional features as input. However, preliminaryexperiments in Appendix H.1 indicated that including such parameters does not really enhance prediction quality, while merely increasing training time. Therefore, in our current implementation, \(NN_{t}\) does not include these revealed parameters. In general, however, such choice should be treated as a hyperparameter and made for each application.

To address the dependency between the neural networks, we employ a coordinate descent approach, where each stage/neural network is treated as a "coordinate". We train the neural networks \(_{0},,_{T-1}\) in cyclic order, repeating until termination (e.g. convergence or timeout). See Algorithm 1 for high-level pseudocode description of this sequential coordinate descent approach, as well as the parallelized version described in the next subsection.

Concretely, we train \(_{t}\) by considering all the other neural networks as fixed. Here, we will focus on describing the forward pass, since the backward pass gradient computations follow essentially the same strategy described in Section 4.1 -- see Appendix C for details of the gradient computations.

Forward passConsider a historical (feature, parameter) pair \((A,)\). We first iteratively generate the sequence of solutions \(^{(0)},,^{(t)}\) using \(}^{(i)}=_{i}(A,^{(i-1)})\) for \(i[0,t-1]\). Then, we compute the Stage \(t\) prediction \(}^{(t)}=_{t}(A,^{(t-1)})\), and generate the remaining sequence of solutions \(^{(t)},,^{(T)}\) using \(}^{(i)}=}^{(t)}[i+1:T]\) for \(i[t,T]\).

Backward passThe goal is to compute the derivative of the post-hoc regret with respect to each edge weight \(w_{e}\) of \(_{t}\). Similar to Section 4.1, instead of directly using the MILP formulation of all the Stage \(t\) optimization problems in the forward pass, we use the convex relaxation of Hu et al. and Mandi and Guns. This allows us to differentiate the modified (due to convex relaxations) post-hoc regret with respect to each \(w_{e}\) in \(_{t}\). As mentioned, the calculations are quite similar to those in Section 4.1, and so we defer them to Appendix C.

Experimental results in Section 5 demonstrate that the sequential coordinate descent training approach outperforms both the classic non-Predict+Optimize methods and the baseline Predict+Optimize approach from Section 4.1.

We note that in the above description of the training implementation of \(_{t}\), there is a lot of repeated computation that can be pre-computed and reused. Since, during coordinate descent, the prior neural networks \(_{0},,_{t-1}\) are considered fixed, the solutions \((^{(0)},,^{(t-1)})\) are also fixed for a given (features, parameters) pair \((A,)\)_no matter_ how \(_{t}\) is updated during training. Thus, for each \((A,)\) we always pre-compute and save the sequence of solutions \((^{(0)},,^{(t-1)})\), and only recompute \((^{t},,^{T})\) as we update \(_{t}\) through training gradient steps.

### Parallel Coordinate Descent

While the sequential coordinate descent training approach yields accurate predictors, the computational cost is also high. Training a single neural network already requires solving sequences of optimization problems over many iterations; the serialization from training the neural networks one at a time can make the resulting training time prohibitive for applications.

We thus propose to parallelize the coordinate descent approach, slightly sacrificing prediction quality for efficiency. In each coordinate descent step, we train all the neural networks in parallel. When training a particular neural network \(_{t}\), we use copies of \(_{0},,_{t-1},_{t+1},,_{ T-1}\) from the previous descent step, but otherwise the training implementation remain the same as in Section 4.2. See also Algorithm 1.

This simple change drastically improves running time (Appendix G), while only slightly decreases predictive accuracy: the post-hoc regret of models trained using parallel coordinate descent sits between that of the baseline training algorithm and the sequential coordinate descent approach.

## 5 Experimental Evaluation

We evaluate the proposed 3 training algorithms: Baseline, Sequential Coordinate Descent (SCD), and Parallel Coordinate Descent (PCD) on 3 benchmarks described in Appendix D. We compare these algorithms to classic non-Predict+Optimize regression models : ridge regression (Ridge), \(k\)-nearest neighbors (\(k\)-NN), classification and regression tree (CART), random forest (RF), and neural network (NN). The single predictions from these classical regression models are used in test time identically to our Baseline approach (Section 4.1). We tune all algorithm hyperparameters via cross-validation -- Appendix E gives all the hyperparameter types and chosen values. In particular, the termination criteria for SCD and PCD in Algorithm 1 are based on a threshold that measures the difference in training set post-hoc regrets between two (outermost) iterations of the training coordinate descent. This threshold is also treated as a hyperparameter that was tuned per each application; in the experiments presented in this paper, we used a threshold of 0.1.

Due to space limitations, we present only the best results obtained among all standard regression methods (BAS) as one column in the main paper. See Appendix F for full results. Furthermore, we report mainly the prediction accuracy -- see Appendix G for computational setup and detailed runtime comparisons.

Given the lack of datasets specific to these benchmarks, we follow a standard Predict+Optimize experimental approach [16; 24; 7] and use real data from different problems as numerical values in our experiment instances. We include details in the individual subsections. For each benchmark, we run 30 simulations, each simulation containing a 70/30 training/test data split.

### Production and Sales Problem

Our first benchmark is a linear programming (LP) problem. An oil company is developing a production and sales plan for the upcoming \(T\) quarters/months. The company aims to maximize profits -- sales revenues minus production costs -- under the constraint that the amount of oil sold each quarter/month cannot exceed the customer demands. The production cost and sales price for each quarter/month are known, but the demand is revealed only at the beginning of each quarter/month after the company receives the orders. See Appendix D.1 for the detailed description and LP model.

We generate production costs and sales prices following the method by Ardjmand et al. . Production costs are randomly generated from \(\). Two groups of sales prices are considered: low-profit product prices are randomly generated from \(\); high-profit product prices are from \(\). Customer demands are the unknown parameters and need prediction -- we use real data from a knapsack benchmark  as demand parameters, where each parameter is related to 4096 features.

We conduct experiments on \(T=\{4,12\}\), corresponding to 4 quarters or 12 months. For NN, Baseline, SCD, and PCD, we use 5-layer fully connected networks with 512 neurons per hidden layer.

Table 1 reports the mean post-hoc regrets and standard deviations across 30 simulations for the proposed 3 methods and BAS on the problem. Appendix F.1 gives a full data table (Table 5) with all standard regression methods. We also report the mean and standard deviations of True Optimal Values (TOV) in the last column -- optimal objective under true parameters in hindsight -- for readers to use as (rough) normalization for relative errors. The results demonstrate the advantage of Predict+Optimize methods. All three proposed methods, even Baseline, beat all standard regression methods. SCD consistently achieves the best performance, followed closely by PCD. Baseline falls between the two coordinate descent methods.

In Appendix F.1, Table 6 shows the percentage improvements of the proposed 3 methods against BAS. From that table, we observe that the advantage of our methods increases with the number of stages.

Considering the relatively large standard deviations in Table 1, to show the substantial performance improvements clearly, we provide "win rate" results in Figure 1 and related information in Table 7 in Appendix F.1. Here, "win rate" refers to counting the number of simulations (out of 30) where a 

[MISSING_PAGE_FAIL:9]

demands are revealed, the admin can modify future rosters at cost, and hire extra temporary nurses at higher salary in case of understaffing. See Appendix B for a detailed description of the model.

Each problem instance consists of 10 regular nurses and 7 days (stages). Extra nurses come at a cost of \(\{15,20,25,30\}\) in different experiments. Due to the longer solving time for these MILPs, we use real data from the ICON scheduling competition  as the numerical values for patient demands, where each demand value is related to 8 features as opposed to 4096 features in previous benchmarks. Given far fewer features, for both NN, Baseline, SCD and PCD, we use a smaller network architecture: a 5-layer fully-connected network with 16 neurons per hidden layer.

Table 3 reports the mean post-hoc regrets and standard deviations across 30 simulations for each approach, again corroborating the prediction performance order of SCD \(>\) PCD \(>\) Baseline \(>\) BAS. Appendix F.3 gives full experimental results.

Similarly to the first two benchmarks, we report win rate results in Table 14 and show comparisons between the proposed methods against BAS across each individual simulation in Figure 2 in Appendix F.3. Table 14 shows that SCD pretty consistently outperforms BAS, achieving win rates of 86.67% or higher in most scenarios. PCD also demonstrates competitive performance, with win rates ranging from 70% to 83.33%, making it a viable alternative as we hypothesized.

### Runtime Analysis

Appendix G gives the training times for each method. The training times follow the order of SCD \(>\) PCD \(>\) Baseline \(>\) classic regression methods, which is the same for predictive accuracy in the benchmarks, indicating a tradeoff between training time and accuracy. Our methods take longer runtime than most other regression methods due to having to solve sequences of linear programs during training. Among our methods, the coordinate descent methods take longer time than Baseline due to having to train more neural networks, and the sequential version naturally takes longer than the parallelized version.

## 6 Conclusion and Future Work

We propose the first Predict+Optimize framework for scenarios where unknown parameters are revealed progressively over stages. Specifically, our proposal allows better predictions and (re)optimization at each stage as more parameters are made known. Algorithmically, we focus on MILPs---a large and widely-studied class of problems---and present three training methods for our novel framework. Empirical results in three benchmarks demonstrate better predictions from our methods over classical ones. Our methods trade off between predictive accuracy and training time.

Our work establishes the feasibility of Multi-Stage Predict+Optimize, and furthermore shows that even our baseline algorithm of training a single predictor outperforms classical non-Predict+Optimize approaches. Looking into the future beyond the present work, there is ample space for algorithmic improvements. As we observed in Section 5, the current experimental results suggest that our multi-stage predict+optimize methods display rather different behaviors depending on whether the optimization is a linear program or a mixed integer program. In particular, for linear programs, the advantage of our methods over classical methods _increases_ with the number of stages, whereas the opposite happens for integer programs. We believe it is important to investigate whether this phenomenon holds more generally. If so, it is a prudent research direction to understand whether such decay is inevitable for MILPs, or if there are algorithmic techniques to mitigate this effect.

  Extra nurse & SCD & PCD & Baseline & BAS & TOV \\  
15 & **697.465\(}\).19** & 622.045\(}\).64 & 653.351\(}\).67 & 662.342\(}\).17 & 1001.641\(}\).7411 \\ 
20 & **783.65\(}\).202** & 805.112\(}\).59 & 87.604\(}\).19 & 86.032\(}\).140 & 1073.732\(}\).1504.13 \\ 
25 & **1003.525\(}\).45** & 1040.882\(}\).32 & 103.452\(}\).68 & 1144.633\(}\).05 & 1008.934\(}\).48837 \\ 
30 & **1207.56\(}\).19** & 1240.843\(}\).39 & 1290.103\(}\).71 & 106.981\(}\).673\(}\).20 & 1110.734\(}\).15 \\  

Table 3: Mean post-hoc regrets and standard deviations for the nurse rostering problem.