# Temporally Disentangled Representation Learning

under Unknown Nonstationarity

 Xiangchen Song\({}^{1}\) Weiran Yao\({}^{2}\) Yewen Fan\({}^{1}\) Xinshuai Dong\({}^{1}\) Guangyi Chen\({}^{1,3}\) Juan Carlos Niebles\({}^{2}\) Eric Xing\({}^{1,3}\) Kun Zhang\({}^{1,3}\)

\({}^{1}\)Carnegie Mellon University

\({}^{2}\)Salesforce Research

\({}^{3}\)Mohamed bin Zayed University of Artificial Intelligence

Part of the work was done while interning at Salesforce Research

###### Abstract

In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in _nonstationary_ setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side-information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in _nonstationary_ setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, _without_ the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only. Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts.

## 1 Introduction

Causal reasoning for time-series data is a long-lasting yet fundamental task [1; 2; 3]. The majority of the studies focus on the temporal causal discovery among observed variables [4; 5; 6]. However, in many real-world scenarios, the observed data (e.g., image pixels in videos) instead of having direct causal edges, are generated by some causally related latent temporal processes or confounders. Learning causal relations has practical use cases, which benefit a lot of downstream tasks. However, estimating latent causal structures among those unobserved variables purely from observations without appropriate class of assumptions is an extremely challenging task (i.e. the latent variables are generally not identifiable) [7; 8].

Under the topic of unsupervised representation learning via nonlinear Independent Component Analysis (ICA), some strong identifiability results of the latent variables have been established [9; 10; 11; 12; 13; 14] by introducing side information such as class labels and domain indices. Specifically focusing on time-series data, history information is also widely used as the side information for the identifiability of latent processes [15; 16; 17; 18]. However, existing studies mainly focused on and derived identifiability results in stationary settings [10; 16] (Fig 1 (a)) or nonstationary settings with explicitly observed domain indices [17; 18; 12] (Fig 1 (b)).

One can immediately tell the infeasibility of those two scenarios that general time-series data is usually nonstationary and the side information (class labels and domain indices) is usually unobserved. That is particularly true when considering real-world data such as video or signal sequences. It doesn't make any sense to assume that there exists a stationary transition function that is applied to the whole video clip. Take a very simple video clip of a mouse2 as an example, it is fairly clear that such a simple motion example can be divided into at least two phases (1) active phase in which the mouse is moving and (2) inactive phase in which the mouse is laying down. Instead of using a complex transition function to describe the whole video clip, a more reasonable assumption is that the same transition function is shared within the same phase, but across different phases, the transition functions are different, in other words, the transition function can be expressed as a function of the domain index. Also, it is worth mentioning that if such domain or phase indices is latent or unobserved, then we cannot directly utilize the existing framework to learn the latent causal dynamics. That is again a more realistic case that in general, the domain indices within a video are not accessible without expensive human annotation.

Recently, HMNLICA  attempted to resolve the problem by introducing Markov Assumption on the nonstationary discrete domain variable, they assumed the domain indices follow a first-order Markov Chain and estimated the domain information purely from observed data. However, HMNLICA assumes temporally mutually independent sources in the data-generating process (conditioning on domain indices), i.e. they don't allow latent variables to have time-delayed causal relations in between (Fig 1 (c)). Such an assumption imposed a huge negative impact on the usability of those methods. Considering the video of the little mouse example, the \(_{t}\)s are the observed video frames, \(_{t}\)s can be the independent motion dynamics or causal process such as position, velocity, (angular) momentum, etc, and \(c_{t}\)s are the phases or actions such as standing up (active) and laying down (inactive). To accommodate for such general sequential data, time-delayed temporal dependence should be considered in the latent \(_{t}\) space (Fig 1 (d)), otherwise, it is impossible to model a complex video data's temporal relation purely from discrete, domain indices. Also to make sure that the latent independent components can be recovered, temporally conditional independence should also be enforced, i.e. Each dimension of \(_{t}\) is conditionally independent given the history \(_{}\). To this end, a natural question is:

_How can we establish identifiability of nonlinear ICA for general sequential data with nonstationary causally-related process without observing auxiliary variable?_

To answer this question, we first formulate the latent nonstationary states as a discrete Markov process and further explore the Markov Assumption  which is introduced for identifiability of nonlinear ICA in HMNLICA  and provided stronger identifiability result corresponding to the conditional emission distribution (i.e. the transition function of different domains) and the transition matrix of

Figure 1: Graphic models for three different settings in causally related time-delayed time series data with a visual illustration. (a) is a _stationary_ setting in which the transition function \(_{t+1}=f_{z}(_{t})\) stays universally the same. (b) is the setting widely explored in existing work, in which the transition function \(f_{z}\) changes according to different domains (denoted as \(c_{t}\)), and all those domain indices are observed. (c) capture the unobserved domain indices by introducing a Markov chain on \(c_{t}\). (d) is a more general form to model the time series data in this work. It allows nonstationary settings and it doesnâ€™t require the domain indices to be observed.

the Markov process. Specifically, we generalized the identifiability of Hidden Markov Models in  to accommodate time-delayed causally-related non-parametric transitions in latent space (Thm. 1). Then we utilize the linear independence (Thm. 2) to further establish the identifiability of \(_{t}\).

The main contributions of this work can be summarized as follows:

* To our best knowledge, this is the first identifiability result that can handle the nonstationary time-delayed causally-related latent temporal processes without the auxiliary variable. We formulate the problem, especially the nonstationary states into the Markov process, establish identifiability purely from observed data, and then show strong identifiability of latent independent components.
* We present NCTRL, Nonstationary Causal Temporal Representation Learning, a principled framework to recover time-delayed latent causal variables and identify their relations from measured sequential data under unobserved different distribution shifts.
* Experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method in recovering the latent variables.

## 2 Problem Formulation

### Time Series Generative Model

Assume we observe \(n\)-dimensional time-series data at discrete time steps, \(=\{_{1},_{2},,_{T}\}\), where each \(_{t}\) is generated from time-delayed causally related hidden components \(_{t}^{n}\) by the invertible mixing function:

\[_{t}=(_{t}).\] (1)

In addition to latent components \(_{t}\), there is an extra hidden variable \(c_{t}\) which is discrete with cardinality \(|\,c_{t}\,|=C\), it follows a first-order Markov process controlled by a \(C C\) transition matrix \(\), in which the \(i,j\)-th entry \(A_{i,j}\) is the probability to transit from state \(i\) to \(j\).

\[c_{1},c_{2},,c_{t}()\] (2)

For \(i\{1,,n\}\), \(z_{it}\), as the \(i\)-th component of \(_{t}\), is generated by (some) components of history information \(_{t-1}\), discrete nonstationary indicator \(c_{t}\), and noise \(_{it}\).

\[z_{it}=f_{i}(\{z_{j,t-1}\,|\,z_{j,t-}(z_{it})\},c_{t}, _{it}) with_{it} p_{_{i}}\] (3)

where \((z_{it})\) is the set of latent factors that directly cause \(z_{it}\), which can be any subset of \(_{}=\{_{t-1},_{t-2},,_ {t-L}\}\) up to history information maximum lag \(L\). The components of \(_{t}\) are mutually independent conditional on \(_{}\) and \(c_{t}\).

### Identifiability of Latent Causal Processes and Time-Delayed Latent Causal Relations

We define the identifiability of time-delayed latent causal processes in the representation function space in **Definition 1**. Furthermore, if the estimated latent processes can be identified at least up to permutation and component-wise invertible nonlinearities, the latent causal relations are also immediately identifiable because conditional independence relations fully characterize time-delayed causal relations in a time-delayed causally sufficient system, in which there are no latent causal confounders in the (latent) causal processes. Note that invertible component-wise transformations on latent causal processes do not change their conditional independence relations.

**Definition 1** (Identifiable Latent Causal Processes).: _Formally let \(=\{_{1},_{2},,_{T}\}\) be a sequence of observed variables generated by the true temporally causal latent processes specified by \((f_{i},p(_{i}),,)\) given in Eqs. (1), (2), and (3). A learned generative model \((_{i},(_{i}),},})\) is observationally equivalent to \((f_{i},p(_{i}),,)\) if the model distribution \(p_{,_{},},}}(\{_ {1},_{2},,_{T}\})\) matches the data distribution \(p_{f_{i},p_{},,}(\{_{1},_{2},,_{T}\})\) everywhere. We say latent causal processes are identifiable if observational equivalence can lead to identifiability of the latent variables up to permutation \(\) and component-wise invertible transformation \(T\):_

\[p_{_{i},_{_{i}},},}}(\{_{1},_{2},,_{T}\}) =p_{f_{i},p_{_{i}},,}(\{_{1}, _{2},,_{T}\})\] (4) \[}^{-1}(_{t}) =T^{-1}(_{t}), _{t},\]

_where \(\) is the observation space._Identifiability Theory

In this section, we showed that under mild conditions, the latent variable \(_{t}\) is identifiable up to permutation and a component-wise transformation. The theoretical results can be divided into two parts (1) identifiability of the nonstationarity and (2) identifiability of the independent components. As introduced above, the major challenge comes from the unobserved domain indices or nonstationary indicators (\(c_{t}\) in our graphic models). We first establish the identifiability of the different conditional distributions from the observed data and then show that the latent variables \(\) are identifiable. The complete proofs can be found in Appendix A.

### Identifiability of Nonstationary Hidden States

Gassiat et al. showed that the conditional emission distributions in Hidden Markov Models and the transition matrix are identifiable up to label swapping. We first generalize it to the autoregressive setting to accommodate for the time-delayed causal relation, i.e. we showed the identifiability of conditional emission distributions \(p(_{t}|_{t-1},c)\).

**Theorem 1**.: _(identifiability of the nonstationarity with Markov Assumptions) Suppose the observed data is generated following the nonlinear ICA framework as defined in Eqs. (1), (2) and (3). Suppose the following assumptions (Markov Assumptions) hold:_

* _For the Markov process, the number of latent states,_ \(C\)_, is known._
* _The transition matrix_ \(\) _is full rank._

_Use \(_{1},,_{C}^{n}\) to denote nonparametric probability distributions of the \(C\) emission distributions \(_{c}=p(_{t}\,|\,_{t-1},c)\). Then the parameters \(\) and \(M=(_{1},,_{C})\) are identifiable given the distribution, \(^{(4)}_{,M}\), of at least 4 consecutive observations \(_{t},_{t+1},_{t+2},_{t+3}\), up to label swapping of the hidden states, that is:_

_If \(}\) is a \(C C\) transition matrix and if \((c)\) is a stationary distribution of \(}\) with \((c)>0\)\( c\{1,,C\}\), and if \(=(_{1},,_{C})\) are \(C\) probability distributions on \(^{n}\) that verify the equality of the distribution functions \(^{(4)}_{},}=^{(4)}_{ ,M}\), then there exists a permutation \(\) of the set \(\{1,,C\}\) such that for all \(k,l=1,,C\) we have \(_{k,l}=A_{(k),(l)}\) and \(_{k}=_{(k)}\)._

For notational simplicity, and without loss of generality, we can assume the components are ordered such that \(c=(c)\). That leads us to the identifiability of the nonstationarity in the system i.e. up to label swapping of the hidden states, the conditional emission distributions \(p(_{t}|_{t-1},c_{t})\) and transition matrix \(\) are identifiable, hence providing us a bridge to further leverage the temporal independence condition in the latent space to establish the identifiability result for demixing function or in other words the latent variables \(_{t}\).

### Identifiability of Latent Causal Processes

To incorporate nonlinear ICA into the Markov Assumption we define the emission distribution \(p(_{t}\,|\,_{t-1},c)\) as a deep latent variable model. First, the latent independent component variables \(_{t}^{n}\) are generated from a factorial prior, given the hidden state \(c_{t}\) and previous \(_{t-1}\), as

\[p(_{t}\,|\,_{t-1},c_{t})=_{k=1}^{n}p(z_{kt}\,|\, _{t-1},c_{t}).\] (5)

Second, the observed data \(_{t}\) is generated by a nonlinear mixing function as in Eq. (1) which is assumed to be bijective with inverse given by \(_{t}=(_{t})\). Let \(_{kt}(c_{t}) p(z_{kt}|_{t-1},c_{t})\), and assume that \(_{kt}(c_{t})\) is twice differentiable in \(z_{kt}\) and is differentiable in \(z_{l,t-1}\), \(l=1,2,...,n\). Note that the parents of \(z_{kt}\) may be only \(c_{t}\) and a subset of \(_{t-1}\); if \(z_{l,t-1}\) is not a parent of \(z_{kt}\), then \(}{ z_{1,t-1}}=0\).

**Theorem 2**.: _(identifiability of the independent components) Suppose there exists an invertible function \(}^{-1}\), which is the estimated demixing function that maps \(_{t}\) to \(}_{t}\), i.e.,_

\[}_{t}=}^{-1}(_{t})\] (6)_such that the components of \(}_{t}\) are mutually independent conditional on \(}_{t-1}\). Let_

\[_{k,t}(c)& _{kt}(c)}{ z_{k,t} z_{1,t-1}},_{kt}(c)}{ z_{k,t} z_{2,t-1}},...,_{kt}(c)}{ z_{k,t} z_{n,t-1}}^{},\\ }_{k,t}(c)&_{kt}(c)}{ z_{k,t}^{2} z_{1,t-1}},_{kt}(c)}{ z_{k,t}^{2} z_{2,t-1}},...,_{kt}(c)}{ z_{k,t}^{2} z_{n,t-1}}^{ }.\] (7)

_And_

\[_{kt}& _{kt}(1)^{},...,_{kt}(C)^{},_{kt}(2)}{ z_{kt}^{2}}-_{kt}(1) }{ z_{kt}^{2}},...,_{kt}(C)}{ z_{kt}^{ 2}}-_{kt}(C-1)}{ z_{kt}^{2}}^{},\\ }_{kt}&}_{kt}(1)^{},...,}_{kt}(C)^{},(2)}{ z_{kt}}-(1)}{ z_ {kt}},...,(C)}{ z_{kt}}-(C-1)}{ z_{kt}}^{}.\] (8)

_If for each value of \(_{t}\), \(_{1t},}_{1t},_{2t},}_{2t},...,_{nt},}_{nt}\), as \(2n\) function vectors \(_{k,t}\) and \(}_{k,t}\), with \(k=1,2,...,n\), are linearly independent, then \(}_{t}\) must be an invertible, component-wise transformation of a permuted version of \(_{t}\)._

So far, the identifiability result has been established without observing the nonstationarity indicators such as domain indices. In the next section, a novel Variational Auto-Encoder based method is introduced to estimate the demixing function \(}^{-1}\).

## 4 Nctrl: Nonstationary Causal Temporal Representation Learning

In this section, we present the details of NCTRL to estimate the latent causal processes under unobserved nonstationary distribution shift, given the identifiability results in Sec 3. First, we show that our framework includes three modules, Autoregressive Hidden Markov Module, Prior Network, and Encoder-Decoder Module. Then, we provide the optimization objective of our model training including an HMM free energy lower bound, a reconstruction likelihood loss, and a KL divergence.

### Model Architecture

Our framework extends Sequential Variational Auto-Encoders  with tailored modules to model nonstationarity, and enforces the conditions in Sec. 3 as constraints. We give the estimation procedure of the latent causal dynamics model in Eq. (3). The model architecture is showcased in Fig. 2. The framework has three major components (1) Autoregressive Hidden Markov Module (ARHMM), (2) Prior Network Module, and (3) Encoder-Decoder Module.

Autoregressive Hidden Markov Module (ARHMM)The first component of our framework is ARHMM which deals with the nonstationarity with unobserved domains. As discussed in Thm 1, the transition function or the conditional emission distributions across different domains together with the Markov transition matrix \(\) are identifiable. This module estimates the transition function of different domains \(p(_{t}|_{t-1},c_{t})\) and the transition matrix \(\) of the Markov process, and ultimately decodes the optimal domain indices \(\{_{1},_{2},,_{T}\}\) via the Viterbi algorithm.

Prior Network ModuleTo better estimate the prior distribution \(p(_{t}|}_{},c_{t})\), let \(_{}\) denote the lagged latent variables up to maximum time lag \(L\). We evaluate \(p(_{t}|}_{},c_{t})=p_{}(_{z}^ {-1}(_{t},}_{},}_{c_{t}}) )|_{z}^{-1}}{_{t}}|\) by learning a holistic inverse dynamics \(_{z}^{-1}\) that takes the estimated change factors for dynamics \(}_{c_{t}}\) as inputs. Conditional independence of the estimated latent variables \(p(}_{t}|}_{})\) is enforced by summing up all estimated component densities when obtaining the joint \(p(_{t}|_{},c_{t})\) in Eq. 9. Given that the Jacobian is lower-triangular, we can compute its determinant as

Figure 2: Illustration of NCTRLwith (1) Autoregressive Hidden Markov Module, (2) Prior Network, and (3) Encoder-Decoder Module.

the product of diagonal terms. The detailed derivations are given in Appendix B.2.

\[ p(}_{t}|}_{},c_{t})= ^{n} p(_{i}|c_{t})}_{}+^{n}|_{i}^{-1}}{ _{it}}|}_{}\] (9)

Encoder-Decoder ModuleThe third component is a Variational Auto-Encoder based module which utilizes reconstruction loss to enforce the invertibility of learned mixing function \(}\). Specifically, the encoder fits the demixing function \(}^{-1}\) and the decoder fits the mixing function \(}\). The implementation details are in Appendix B.

### Optimization

The first training objective of NCTRL is to maximize the Log-likelihood of the observed data:

\[ p_{_{}}(\{_{1},_{2},,_{T}\})\] (10)

where \(_{}\) represents the HMM training parameters. Then the free energy lower bound can be defined as:

\[-_{}=(q(),_{ })_{q()}[ p_{_{}}(_{1},_{2},,_{T}, )]-(q())\] (11)

Consistent with the theory part, the first training objective is to maximize data log-likelihood in the ARHMM module to get optimal \(q(^{})\).

\[q(^{})*{arg\,max}_{q()} (q(),_{})\] (12)

which can easily be computed by the Forward-Backward algorithm and luckily all of it is differentiable to the HMM training parameters \(_{}\)(transition matrix \(\) and transition function parameters \(_{f}\)).

Then the second part is to maximize the Evidence Lower BOund (ELBO) for the VAE framework, which can be written as (complete derivation steps are in Appendix B.3):

\[& p_{}()-D_{KL}(q_{}(|)||p_{}( |))\\ &=_{_{t}}_{t=1}^{T} p_{ }(_{t}|_{t})}_{-_{}}+ _{}[_{t=1}^{T} p_{}( _{t}|_{},c_{t})-_{t=1}^{T} q_{}( _{t}|_{t})]}_{-_{}}\] (13)

We use mean-squared error (MSE) for the reconstruction likelihood loss \(_{}\). The KL divergence \(_{}\) is estimated via a sampling approach since with a learned nonparametric transition prior, the distribution does not have an explicit form. Specifically, we obtain the log-likelihood of the posterior, evaluate the prior \( p(}_{t}|}_{},c_{t})\) in Eq. (9), and compute their mean difference in the dataset as the KL loss: \(_{}=_{}_{t} q(}_{t}|_{t})} q(}_{t}|_{t})- p (}_{t}|}_{},c_{t})\).

## 5 Experiments

We evaluate the identifiability results of NCTRL on a number of simulated and real-world temporal datasets. We first introduce the evaluation metrics and baselines and then discuss the datasets we used in our experiments. Lastly, we show the experiment results discuss the performance, and make comparisons.

### Evaluation Metrics

**Mean Correlation Coefficient (MCC)** To evaluate the identifiability of the latent variables, we compute the Mean Correlation Coefficient (MCC) on the test dataset. MCC is a standard metric in the ICA literature for continuous variables which measure the identifiability of the learned latent causal processes. MCC is close to 1.0 when latent variables are identifiable up to permutation and component-wise invertible transformation in the noiseless case.

**Mean Square Error (MSE) for estimating A** As introduced in the theory, the \(\) is identifiable in our setting, which means that our proposed method can provide accurate estimation for the transition matrix \(\), to valid such a claim, we use mean square error (MSE) to capture the distance between the estimated \(}\) and ground truth \(\).

**Accuracy for estimating \(c_{t}\)** We also test the accuracy for estimating the discrete domain indices \(c_{t}\) supplementary to the MSE for \(\) since in theory, the \(\) is identifiable but the \(c_{t}\) is generally not identifiable, which is relatively easy to understand as an analogy in Hidden Markov Models, the transition matrix is identifiable but we can only "infer" the best possible discrete variables but cannot establish identifiability for it.

It is also worth mentioning that the MSE and Accuracy are influenced by the permutation, which is also true in clustering evaluation problems. Here we explored all permutations and selected the best possible assignment for evaluation.

### Baselines

The following identifiable nonlinear ICA methods are used: (1) BetaVAE  which ignores both history and nonstationarity information. (2) i-VAE  and TCL  which leverage nonstationarity to establish identifiability but assume independent factors. (3) SlowVAE , and PCL  which exploit temporal constraints but assume independent sources and stationary processes. (4) TDRL  which assumes nonstationary, causal processes but with observed domain indices. (5) HMNLICA  which considers the unobserved nonstationary part in the data generation process but doesn't allow any causally related time-delayed relations.

### Simulated Results

We generate two synthetic datasets corresponding to different complexity of the nonlinear mixing function \(\). Both synthetic datasets satisfy our identifiability conditions in the theorems following the procedures in Appendix B.4. As in Table 1, NCTRL can recover the latent processes under unknown nonstationary distribution shifts with high MCCs (>0.95). The baselines that do not exploit history (i.e., BetaVAE, i-VAE, TCL), with independent source assumptions (Slow-VAE, PCL), consider limited nonstationary cases (TDRL) distort the identifiability results. The only baseline that considers the unknown nonstationarity in the domain indices (HMNLICA) explored the Markov Assumption but doesn't allow a time-delayed causal process and hence suffers a poor result (MCC 0.58).

On the other hand, the difference between dataset A and dataset B is the nonlinearity in the mixing function, dataset A has a relatively simple nonlinear mixing function, on the contrary, dataset B has more complex nonlinearity. Some variability has been observed among the relative performance ranks of different baselines. For example, i-VAE showed a great discrepancy between the two datasets, which revived the weakness of capturing complex nonlinearity in the unknown nonstationary distribution shift environments. Again we also observed that our proposed method can constantly recover the latent independent components with high MCC which indicates on both datasets the model is identifiable and the estimation algorithm is highly effective.

To further validate if NCTRL successfully recovered the Markov transition matrix \(\) and inferred the domain indices \(c_{t}\) with high accuracy. We further examine the accuracy for estimating nonstationary domain indices \(c_{t}\) and the mean square error estimating the transition matrix \(\). As shown in Table 2 the result is consistent with our theory in which the transition matrix \(\) is identifiable and we can estimate it with

    &  \\   & **Dataset A** & **Dataset B** & **Ave.** \\  BetaVAE & 44.02 \(\) 3.11 & 47.48 \(\) 10.58 & 45.75 \\ i-VAE & 89.74 \(\) 3.38 & 44.50 \(\) 0.25 & 67.12 \\ TCL & 37.12 \(\) 0.60 & 56.33 \(\) 3.77 & 46.73 \\ SlowVAE & 33.84 \(\) 0.60 & 53.92 \(\) 3.56 & 43.88 \\ PCL & 42.41 \(\) 2.87 & 63.66 \(\) 2.77 & 53.04 \\ HMNLICA & 59.82 \(\) 4.94 & 57.25 \(\) 1.45 & 58.54 \\ TDRL & 83.99 \(\) 1.92 & 72.02 \(\) 2.76 & 78.01 \\  NCTRL & **98.85 \(\) 0.30** & **99.01 \(\) 0.24** & **98.93** \\   

Table 1: Experiment results of two synthetic datasets on baselines and proposed NCTRL, we run the experiments with five different random seeds and calculate the average with standard derivation. The best results are shown in **bold**.

high accuracy. For the nonstationary domain indices \(c_{t}\) even though there is no identifiability result governing the estimation accuracy, it can still be inferred pretty well since it is nothing but a decoding problem in Hidden Markov Models.

### Real-world Applications

Video data - Modified CartPole EnvironmentWe evaluate NCTRL on the modified CartPole  video dataset and compare the performances with the baselines. Modified Cartpole is a nonlinear dynamical system with cart positions \(x_{t}\) and pole angles \(_{t}\) as the true state variables. The dataset descriptions are in Appendix B.5. Similar to the synthetic dataset, we randomly initialize a Markov chain and roll out a series of \(c_{t}\), and configure the CartPole environment with respect to the \(c_{t}\). Specifically, we use five domains with different configurations of cart mass, pole mass, gravity, and noise levels. Together with the two discrete actions (i.e., left and right). By doing so, the nonstationarity is enforced, and since we can control and access all intermediate states in the system, all metrics including MCC and \(c_{t}\) accuracy together with **A** MSE can be easily calculated. We fit NCTRL with two-dimensional causal factors. We set the latent size \(n=2\) and the lag number \(L=2\). In Fig. 3, the latent causal processes are recovered, as seen from (a) high MCC for the latent causal processes; (b) the latent factors are estimated up to component-wise transformation; and (c) the latent traversals confirm the two recovered latent variables correspond to the position and pole angle.

Similar to Table 1 and 2, we compare our NCTRL with baseline methods. In addition, we also compare with SKD , a state-of-the-art sequential disentangle representation learning method without identifiability guarantee. In Table 3 and 4 we can see that compared with TDRL, our NCTRL can recover the latent processes under unknown nonstationary distribution shifts with high MCCs (>0.95) with highly accurate estimated transition matrix **A** and high quality inferred \(c_{t}\). Specifically, by comparing the result of SKD, the MCC for SKD is better than a variety of baselines,

    \\ 
**Dataset** & **Accuracy estimating**\(c_{t}\) & **MSE estimating A** \\  A & 89.96 \(\) 0.24 & 1.01 \( 10^{-3}\)\(\) 1.67 \( 10^{-4}\) \\ B & 89.84 \(\) 0.29 & 1.08 \( 10^{-3}\)\(\) 1.89 \( 10^{-4}\) \\   

Table 2: Supplementary experiment results of two synthetic datasets on estimating domain indices \(c_{t}\) and transition matrix **A** in NCTRL, we run the experiments with five different random seeds and calculate the average with standard derivation.

Figure 3: Modified Cartpole results: (a) MCC for causally-related factors; (b) scatterplots between estimated and true factors; and (c) latent traversal on a fixed video frame

    \\ 
**BetaVAE** & **i-VAE** & **TCL** & **SlowVAE** & **SKD** & **TDRL** & **NCTRL** \\ 
57.54 & 60.14 & 65.07 & 63.16 & 73.24 & 85.26 & **96.06** \\   

Table 3: Experiment results of CartPole dataset. The best results are shown in **bold**.

however, we can see the distinction between well-disentangled models and identifiable models, only the models with identifiability can find the ground truth latent variables with theoretical guarantee.

Video data - MoSeq DatasetWe test NCTRL framework to analyze mouse behavior video data from Wiltschko et al. , which represents the original application to clustering mouse behavior3, details of this dataset are available in Appendix B.6. Since there are no ground truth independent components in this particular real-world dataset, we analyze it by several visualizations to see if different domains can be properly identified and if the patterns in the recovered independent components are consistent with the recovered domain indices. We analyze the first video clip of mouse behavior data and visualize the two phases we discovered and segmented in Fig 4. We can clearly see from Fig 4 that there are different phases with the upper one actively moving and the lower one inactive. The recovered independent components showed a consistent pattern with the recovered phase or domain indices as shown in Fig 4.

## 6 Related Work

Causal Discovery from Time SeriesUnderstanding the causal structure in time-series data is pivotal in areas such as machine learning , econometrics , and neuroscience . A bulk of the research in this realm emphasizes determining the temporal causal links among observed variables. The primary techniques employed are constraint-based methods , which use conditional independence tests to ascertain causal structures, and score-based methods , where scores are utilized to oversee a search operation. Some researchers also proposed a combination of these two methods . Additionally, Granger causality  and its nonlinear adaptations  have gained widespread acceptance in this context.

    \\ 
**Accuracy estimating \(c_{t}\)** & **MSE estimating A** \\ 
79.23 \(\) 5.33 & 5.01 \( 10^{-2}\)\(\) 1.23 \( 10^{-2}\) \\   

Table 4: Supplementary experiment results of CartPole datasets on estimating domain indices \(c_{t}\) and transition matrix \(\) in NCTRL, we run the experiments with five different random seeds and calculate the average with standard derivation.

Figure 4: Result visualization of MoSeq dataset. (Active, Inactive) show two representative video frames for the active and inactive phases and (Independent Components) visualize the discovered independent components with corresponding phases tagged with different colors.

Nonlinear ICA for Time SeriesRecently, the significance of temporal structures and nonstationarities has been recognized in achieving identifiability within nonlinear ICA. Time-contrastive learning (TCL ) utilizes the independent sources principle, focusing on data segments' variability. On the other hand, Permutation-based contrastive (PCL ) offers a learning approach that distinguishes true independent sources from shuffled ones under the uniformly dependent assumption. HMNLICA  integrates nonlinear ICA with an HMM to address non-stationarity without segmenting data manually. The i-VAE  approach employs VAEs to capture the actual joint distribution between observed and auxiliary non-stationary domains, assuming an exponential families conditional distribution. The recent advancements in nonlinear ICA for time series include LEAP , (i-)CITRIS [33; 34], and TDRL . While LEAP introduces a novel condition emphasizing non-stationary noise, TDRL delves deeper into a non-parametric environment within a nonstationary context. In contrast, CITRIS recommends utilizing intervention target data for pinpointing latent causal aspects, avoiding certain constraints but necessitating active intervention access.

Sequential DisentanglementMajority of existing work about sequential disentanglement focuses on architecture based on dynamical variational autoencoder (VAE) . Early works [36; 37] separate dynamic factors from static factors using probabilistic methods. Then auxiliary tasks with self-supervisory signals  were introduced. C-DSVAE  utilized contrastive penalty terms with data augmentation to introduce additional inductive biases. In R-WAE , Wasserstein distance was introduced to replace KL divergence. To deal with video disentanglement [41; 42] explored generative adversarial network (GAN) architectures and  introduced a recurrent model with adversarial loss. FAVAE,  proposed a factorizing VAE and  proposed to learn hierarchical features. Finally, SKD  introduced a spectral loss term that leads to structured Koopman matrices and disentanglement.

## 7 Conclusion and Discussion

**Conclusion.** In this paper, we first established an identifiability theory for general sequential data with nonstationary causally-related processes under unknown distribution shifts. Then we presented NCTRL, a principled framework to recover the time-delayed latent causal variable identify their causal relations from measured data, and decode high-quality domain indices under Markov assumption. Experiment results on both synthetic datasets and real-world video datasets showed that our proposed method can recover the latent causal variables and their causal relations purely from measured data with the observation of auxiliary variables or domain indices.

**Limitation.** The basic limitation of this work is that the nonstationary domain indices are assumed to follow a Markov chain. Also, this work highly relies on the latent processes to have no instantaneous causal relations but only time-delayed influences. If the resolution of the time series is much lower, then it is usually violated and one has to find a way to deal with instantaneous causal relations. Extending our theories and framework to address the scenarios when more flexibility in the domain indices transition is allowed (i.e. beyond discrete variables following a Markov chain) and to address instantaneous dependency or instantaneous causal relations will be some of our future work.

**Boarder Impacts.** This work proposes a theoretical analysis and technical methods to learn the causal representation from time-series data, which facilitate the construction of more transparent and interpretable models to understand the causal effect in the real world. This could be beneficial in a variety of sectors, including healthcare, finance, and technology. In contrast, misinterpretations of causal relationships could also have significant negative implications in these fields, which must be carefully done to avoid unfair or biased predictions.

## 8 Acknowledgment

This project has been graciously funded by NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NIGMS R01GM140467, NSF IIS2123952, NSF BCS2040381, an Amazon Research Award, NSF IIS2311990, and DARPA ECOLE HR00112390063. This project is also partially supported by NSF Grant 2229881, the National Institutes of Health (NIH) under Contract R01HL159805, a grant from Apple Inc., a grant from KDDI Research Inc., and generous gifts from Salesforce Inc., Microsoft Research, and Amazon Research.