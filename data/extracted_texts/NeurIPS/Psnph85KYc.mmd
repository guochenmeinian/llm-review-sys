# Interpretable Graph Networks

Formulate Universal Algebra Conjectures

 Francesco Giannini\({}^{*}\)

CINI, Italy

francesco.giannini@unisi.it &Stefano Fioravanti\({}^{*}\)

Universita di Siena, Italy, JKU Linz, Austria Italy

stefano.fioravanti@unisi.it &Oguzhan Keskin

University of Cambridge, UK

ok313@cam.ac.uk &Alisia Maria Lupidi

University of Cambridge, UK

aml201@cam.ac.uk &Lucie Charlotte Magister

University of Cambridge, UK

lcm67@cam.ac.uk &Pietro Lio

University of Cambridge, UK

pl219@cam.ac.uk &Pietro Barbiero\({}^{*}\)

Universita della Svizzera Italiana, CH

University of Cambridge, UK

barbip@usi.ch

###### Abstract

The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA)--one of the fields laying the foundations of modern mathematics--is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures.

## 1 Introduction

Universal Algebra (UA, (6)) is one of the foundational fields of modern Mathematics with possible deep impact in all mathematical disciplines, but the complexity of studying abstract algebraic structures hinders scientific progress and discourages many academics. Recently, the emergence of powerful AI technologies empowered researchers to investigate intricate mathematical problems which eluded traditional approaches for decades, leading to the solution of open problems (e.g., (24)) and discovery of new conjectures (e.g., (7)). Yet, universal algebra currently remains an un-investigated realm for AI, primarily for two reasons (i) first

Figure 1: Interpretable graph networks support universal algebra research.

UA deals with infinite objects or even classes of abstract objects, which pose unique challenges for conventional AI techniques, (ii) secondly the field commonly relies on deterministic algorithms, utilized to construct finite models or serve as deterministic theorem provers, such as "mace4" and "prover9". In this sense, we hope that our paper could represent a guidance to further explore the study of UA's problems with AI, e.g. by integrating existing systems, like these mentioned theorem provers, with our methodology, which may suggest novel conjectures, in a unique scheme.

Universal algebra studies algebraic structures from an abstract perspective. Interestingly, several UA conjectures equivalently characterize algebraic properties using equations or graphs (16). In theory, studying UA properties as graphs would enable the use of powerful AI techniques, such as Graph Neural Networks (GNN, (39)), which excel on graph-structured data. However, two factors currently limit scientific progress. First, the _absence of benchmark datasets_ suitable for machine learning prevents widespread application of AI to UA. Second, _GNNs' opaque reasoning_ obstructs human understanding of their decision process (38). Compounding the issue of GNNs' limited transparency, GNN explainability methods mostly rely on brittle and untrustworthy local/post-hoc methods [(14; 27; 28; 38; 45)] or pre-defined subgraphs for explanations [(2; 42)], which are often unknown in UA.

**Contributions.** In this work, we investigate universal algebra's conjectures through AI (Figure 1). Our work includes three significant contributions. First, we propose a novel algorithm that generates a dataset suitable for training AI models based on an UA equational conjecture. Second, we generate and release the first-ever universal algebra's dataset compatible with AI, which contains more than \(29,000\) lattices and the labels of \(5\) key properties i.e., modularity, distributivity, semi-distributivity, join semi-distributivity, and meet semi-distributivity. And third, we introduce a novel neural layer that makes GNNs fully interpretable, according to Rudin's (38) notion of interpretability. The results of our experiments demonstrate that interpretable GNNs (iGNNs): (i) enhance GNN interpretability without sacrificing task accuracy, (ii) strongly generalize when trained to predict universal algebra's properties, (iii) generate simple concept-based explanations that empirically validate existing conjectures, and (iv) identify subgraphs which could be relevant for the formulation of novel conjectures. As a consequence, our findings demonstrate the potentiality of AI methods for investigating UA problems.

## 2 Background

Universal Algebra is a branch of mathematics studying general and abstract algebraic structures. _Algebraic structures_ are typically represented as ordered pairs \(=(A,F)\), consisting of a non-empty set \(A\) and a collection of operations \(F\) defined on the set. UA aims to identify algebraic properties (often in equational form) shared by various mathematical systems. In particular, _varieties_ are classes of algebraic structures sharing a common set of identities, which enable the study of algebraic systems based on their common properties. Prominent instances of varieties that have been extensively studied across various academic fields encompass Groups, Rings, Boolean Algebras, Fields, and many others. A particularly relevant variety of algebras are Lattices (details in Appendix A.3), which are often studied for their connection with logical structures.

**Definition 2.1**.: A _lattice_\(\) is an algebraic structure composed by a non-empty set \(L\) and two binary operations \(\) and \(\), satisfying the commutativity, associativity, idempotency, and absorption axioms.

Equivalently a lattice can be characterized as a partially ordered set in which every pair of elements has a _supremum_\(()\) and an _infimum_\(()\) (cf. Appendix A.3). Lattices also have formal representations as graphs via _Hasse diagrams_\((L,E)\) (e.g., Figure 2), where each node \(x L\) is a lattice element, and directed2 edges \((x,y) E L L\) represent the ordering relation, such that if \((x,y) E\) then \(x_{L}y\) in the ordering of the lattice. A _sublattice_\(^{}\) of a lattice \(\) is a lattice such that \(L^{} L\) and \(^{}\) preserves the original order (the "essential structure") of \(L\), i.e. for all \(x,y L^{}\) then \(x_{L^{}}y\) if and only if \(x_{L}y\). The foundational work by Birkhoff (5), Dedekind (9), and Jonsson (18) played a significant role in discovering that some significant varieties of lattices can be characterized through the omission of one or more lattices. Specifically, a variety \(\) of lattices is said to _omit_ a lattice \(\) if the latter cannot be identified as a sublattice of any lattice in \(\). A parallel line of work in UA

Figure 2: Hasse diagrams.

[MISSING_PAGE_FAIL:3]

on large graph structures without explicitly checking them. Using Algorithm 1, we generated the first large-scale AI-compatible datasets of lattices containing more than \(29,000\) graphs and the labels of \(5\) key properties of lattice (quasi-)varieties i.e., modularity, distributivity, semi-distributivity, join semi-distributivity, and meet semi-distributivity, whose definitions can be found in Appendix A.

Scalability and Complexity. The scalability of the dataset generation process is related to two main points: (i) dealing with the exponential growth of the number of lattices, (ii) how to feasibly generate a lattice of a finite amount of elements. (i) Since the number of lattices definable on a set \(L=\{0,...n-1\}\) increases exponentially with \(n\), it is not feasible to realize a dataset containing all the lattices of \(n\) elements. However, this is also unnecessary for the scope of this work, which aims at automatically identifying (small) topological patterns responsible for the failure of certain algebraic properties. In addition, the majority of the graphs generated are isomorphic, thus not particularly informative for our task. Because of this, we only generate a portion of all the lattices up to a certain value \(M>0\), and then take a fixed number of samples \(n_{s}\) up to \(N\) (we take \(M=8\), \(n_{s}=20\), \(N=50\) in the paper). (ii) The construction of a lattice with \(n\) elements is briefly sketched in Algorithm 1. We refer to Appendix B for the technical details on the design of the used functions.

### Interpretable Graph Networks (iGNNs)

In this section, we design an interpretable graph network (iGNN, Figure 3) that satisfies the notion of "interpretability" introduced by Rudin (38). According to this definition, a machine learning (ML) system is interpretable if and only if (1) its inputs are semantically meaningful, and (2) its model inference is simple for humans to understand (e.g., sparse and/or symbolic). This definition covers ML systems that take tabular datasets or sets of concepts as inputs, and (piece-wise) linear models such as logistic regression or decision trees. To achieve this goal in GNNs, we introduce an interpretable graph layer that learns semantically meaningful concepts and uses them as inputs for a simple linear classification layer. We then show how this layer can be included into existing architectures or into hierarchical iGNNs, which consist of a sequence of interpretable graph layers. We notice that in devising our approach, we preferred to rely on linear classifiers as they are fully differentiable, hence allowing us to realize a fully interpretable and differentiable model from the input to the classification head. However,in practice any interpretable and differentiable classifier could be used in place of this linear layer.

#### 3.2.1 Interpretable Graph Layer

The interpretable graph layer (Figure 3) serves three main functions: message passing, concept generation, and task predictions. The first step of the interpretable graph layer involves a standard message passing operation (Eq. 1 right), which aggregates information from node neighbors. This operation enables to share and process relational information across nodes and it represents the basis of any GNN layer.

Node-level concepts.An interpretable concept space is the first step towards interpretability. Following Ghorbani et al. (10), a relevant concept is a "high-level human-understandable unit of

Figure 3: An interpretable graph layer (i) aggregates node features with message passing, (ii) generates a node-level concept space with a hard Gumbel-Softmax activation \(\), (iii) generates a graph-level concept space with an interpretable permutation invariant pooling function \(\) on node-level concepts, and (iv) predicts a task label with an interpretable classifier \(f\) using graph-level concepts.

information" shared by input samples and thus identifiable with clustering techniques. Message passing algorithms do cluster node embeddings based on the structure of node neighborhoods, as observed by Magister et al. (28). However, the real-valued large embedding representations \(_{i}^{q},q\) generated by message passing can be challenging for humans to interpret. To address this, we use a hard Gumbel-Softmax activation \(:^{q}\{0,1\}^{q}\), following Azzolin et al. (2):

\[_{i}=_{i}_{i}= _{i},_{j N_{i}}(_{i},_{j} )\] (1)

where \(\) is a learnable function ignoring or assuming constant input features and \(\) is a learnable function aggregating information from a node neighborhood \(N_{i}\), and \(\) is a permutation invariant aggregation function (such as sum or mean). During the forward pass, the Gumbel-Softmax activation \(\) produces a one-hot encoded representation of each node embedding. Since nodes sharing the same neighborhood have similar embeddings \(_{i}\) due to message passing, they will also have the same one-hot vector \(_{i}\) due to the Gumbel-Softmax, and vice versa - we can then interpret nodes having the same one-hot concept \(_{i}\) as nodes having similar embeddings \(_{i}\) and thus sharing a similar neighborhood. More formally, we can assign a semantic meaning to a reference concept \(\{0,1\}^{q}\) by visualizing concept prototypes corresponding to the inverse images of a node concept vector. In practice, we can consider a subset of the input lattices \(\) corresponding to the node's (\(p\)-hop) neighborhood covered by message passing:

\[(,p)=^{(i,p)} i L\; \;_{i}=}\] (2)

where \(\) is the set of all training lattices, and \(^{(i,p)}\) is the graph corresponding to the \(p\)-hop neighborhood (\(p\{1,,|L|\}\)) of the node \(i L\), as suggested by Ghorbani et al. (10); Magister et al. (28). This way, by visualizing concept prototypes as subgraph neighborhoods, the meaning of the concept representation becomes easily interpretable to humans (Figure 3), aiding in the understanding of the reasoning process of the network.

**Example 3.1** (Interpreting node-level concepts).: Consider the problem of classifying distributive lattices with a simplified dataset including \(_{5}\) and \(_{3}\) only, and where each node has a constant feature \(x_{i}=1\). As these two lattices only have nodes with 2 or 3 neighbours, one layer of message passing will then generate only two types of node embeddings e.g., \(_{II}=[0.2,-0.4,0.3]\) for nodes with a 2-nodes neighborhood (e.g., ), and \(_{III}=[0.6,0.2,-0.1]\) for nodes with a 3-nodes neighborhood (e.g., ). As a consequence, the Gumbel-Softmax will only generate two possible concept vectors e.g., \(_{II}=\) and \(_{III}=\). Hence, for instance the concept belongs to \(_{II}\), while \(\) belongs to \(_{III}\).

Graph-level concept embeddings.To generate a graph-level concept space in the interpretable graph layer, we can utilize the node-level concept space produced by the Gumbel-Softmax. Normally, graph-level embeddings are generated by applying a permutation invariant aggregation function on node embeddings. However, in iGNNs we restrict the options to (piece-wise) linear permutation invariant functions in order to follow our interpretability requirements dictated by Rudin (38). This restriction still includes common options such as max or sum pooling. Max pooling can easily be interpreted by taking the component-wise max over the one-hot encoded concept vectors \(_{i}\). After max pooling, the graph-level concept vector has a value of \(1\) at the \(k\)-th index if and only if at least one node activates the \(k\)-th concept i.e., \( i L,_{ik}=1\). Similarly, we can interpret the output of a sum pooling: a graph-level concept vector takes a value \(v\) at the \(k\)-th index after sum pooling if and only if there are exactly \(v\) nodes activating the \(k\)-th concept i.e., \( i_{0},,i_{v} L,_{ik}=1\).

**Example 3.2** (Interpreting graph-level concepts).: Following Example 3.1, let us use sum pooling to generate graph-level concepts. For an \(_{5}\) graph, we have 5 nodes with exactly the same 2-node neighborhood. Therefore, sum pooling generates a graph-level embedding \(\), which certifies that we have 5 nodes of the same type e.g.,. For an \(_{3}\) graph, the top and bottom nodes have a 3-node neighborhood e.g.,, while the middle nodes have a 2-node neighborhood e.g.,. This means that sum pooling generates a graph-level embedding \(\), certifying that we have 2 nodes of type  and 3 nodes of type.

Interpretable classifier.To prioritize the identification of relevant concepts, we use a classifier to predict the task labels using the concept representations. A black-box classifier like a multi-layer perceptron (36) would not be ideal as it could compromise the interpretability of our model, so instead we use an interpretable linear classifier such as a single-layer network (20). This allows for a completely interpretable and differentiable model from the input to the classification head, as the input representations of the classifier are interpretable concepts and the classifier is a simple linear model which is intrinsically interpretable as discussed by Rudin (38). In fact, the weights of the perceptron can be used to identify which concepts are most relevant for the classification task. Hence, the resulting model can be used not only for classification, but also to interpret and understand the problem at hand.

#### 3.2.2 Interpretable architectures

The interpretable graph layer can be used to instantiate different types of iGNNs. One approach is to plug this layer as the last message passing layer of a standard GNN architecture:

\[=f_{i K}~{}~{}^{( K)}_{i}^{(K-1)},_{j N_{i}}^{(K)}(_{i}^ {(K-1)},_{j}^{(K-1)})\] (3) \[_{i}^{(l)}=^{(l)}_{i}^{(l-1)}, _{j N_{i}}^{(l)}(_{i}^{(l-1)},_{j}^{(l-1) }) l=1,,K\] (4)

where \(f\) is an interpretable classifier (e.g., single-layer network), \(\) is an interpretable piece-wise linear and permutation-invariant function (such as max or sum), \(\) is a Gumbel-Softmax hard activation function, and \(_{i}^{0}=_{i}\). In this way, we can interpret the first part of the network as a feature extractor generating well-clustered latent representations from which concepts can be extracted. This approach is useful when we only care about the most complex neighborhoods/concepts. Another approach is to generate a hierarchical transparent architecture where each GNN layer is interpretable:

\[^{(l)}=f_{i K}_{j}^{(l) } l=1,,K\] (5)

In this case, we can interpret every single layer of our model with concepts of increasing complexity. The concepts extracted from the first layer represent subgraphs corresponding to the \(1\)-hop neighborhood of a node, those extracted at the second layer will correspond to \(2\)-hop neighborhoods, and so on. These hierarchical iGNNs can be useful to get insights into concepts with different granularities. By analyzing the concepts extracted at each layer, we gain a better understanding of the GNN inference and of the importance of different (sub)graph structures for the classification task.

#### 3.2.3 Training

For the classification layer, the choice of the activation and loss functions for iGNNs depends on the nature of the task at hand and does not affect their interpretability. For classification tasks, we use standard activation functions such as softmax or sigmoid, along with standard loss functions like cross-entropy. For hierarchical iGNNs (HiGNNs), we apply the same loss function at each layer of the concept hierarchy, as their layered architecture enables intermediate supervisions. This ensures that each layer is doing its best to extract the most relevant concepts to solve the task. Internal losses can also be weighted differently to prioritize the formation of optimal concepts of a specific size, allowing the HiGNN to learn in a progressive and efficient way.

## 4 Experimental Analysis

### Research questions

In this section we analyze the following research questions:

* Can GNNs generalize when trained to predict universal algebra's properties? Can interpretable GNNs generalize as well?
* Do interpretable GNNs concepts empirically validate universal algebra's conjectures? How can concept-based explanations suggest novel conjectures?

### Setup

Baselines.For our comparative study, we evaluate the performance of iGNNs and their hierarchical version against equivalent GNN models (i.e., having the same hyperparameters such as number layers, training epochs, and learning rate). For vanilla GNNs we resort to common practice replacing the Gumbel-Softmax with a standard leaky ReLU activation. We exclude from our main baselines prototype or concept-based GNNs pre-defining graph structures for explanations, as for most datasets these structures are unknown. Appendix C covers implementation details. We show more extensive results including local and post-hoc explanations in Appendix G.

Evaluation.We employ three quantitative metrics to assess a model's generalization and interpretability. We use the Area Under the Receiver Operating Characteristic (AUC ROC) curve to assess task generalization. We evaluate generalization under two different conditions: with independently and identically distributed train/test splits, and out-of-distribution by training on graphs up to eight nodes, while testing on graphs with more than eight nodes ("strong generalization" (41)). We further assess generalization under binary and multilabel settings (classifying 5 properties of a lattice at the same time). To evaluate interpretability, we use standard metrics such as completeness (44) and fidelity (37). Completeness4 assesses the quality of the concept space on a global scale using an interpretable model to map concepts to tasks, while fidelity measures the difference in predictions obtained with an interpretable surrogate model and the original model. Finally, we evaluate the meaningfulness of our concept-based explanations by visualizing and comparing the generated concepts with ground truth lattices like e.g. \(_{3}\) and \(_{5}\), whose omission is known to be significant for modular and distributive properties. All metrics in our evaluation, across all experiments, are computed on test sets using \(5\) random seeds, and reported using the mean and \(95\%\) confidence interval.

## 5 Key Findings

iGNNs improve interpretability without sacrificing task accuracy (Figure 4).Our experimental evaluation reveals that interpretable GNNs are able to strike a balance between completeness and fidelity, two crucial metrics that are used to assess generalization-interpretability trade-offs (37). We observe that the multilabel classification scenario, which requires models to learn a more varied and diverse set of concepts, is the most challenging and results in the lowest completeness scores on average. We also notice that the more challenging out-of-distribution scenario results in the lowest completeness and fidelity scores across all datasets. More importantly, our findings indicate that iGNNs achieve optimal fidelity scores, as their classification layer consists of a simple linear function of the learnt concepts which is intrinsically interpretable (38). On the contrary, interpretable surrogate models of black-box GNNs exhibit, as expected, lower fidelity scores, confirming analogous observations in the explainable AI literature [37; 38]. In practice, this discrepancy between the original black-box predictions and the predictions obtained with an interpretable surrogate model questions the actual usefulness of black-boxes when interpretable alternatives achieve similar results in solving the problem at hand, as extensively discussed by Rudin (38). Overall, these results demonstrate how concept spaces are highly informative to solve universal algebra's tasks and how the interpretable graph layer may improve GNNs' interpretability without sacrificing task accuracy. We refer the reader to Appendix E for detailed discussion on quantitative analysis of concept space obtained by iGNNs under different generalization settings with comparisons to their black-box counterparts.

GNNs strongly generalize on universal algebra's tasks (Figure 5).Our experimental findings demonstrate the strong generalization capabilities of GNNs across the universal algebra tasks we designed. Indeed, we stress GNNs test generalization abilities by training the models on graphs of size up to \(n\) (with \(n\) ranging from \(5\) to \(8\)), and evaluating their performance on much larger graphs of size up to \(50\). We designed this challenging experiment in order to understand the limits and robustness of interpretable GNNs when facing a significant data distribution shift from training to test data. Remarkably, iGNNs exhibit robust generalization abilities (similar to their black-box counterparts) when trained on graphs up to size \(8\) and tested on larger graphs. This evidence confirms the hypothesis that interpretable models can deliver reliable and interpretable predictions, as suggested by Rudin(38). However, we observe that black-box GNNs slightly outperform iGNNs when trained on even smaller lattices. We hypothesize that this is due to the more constrained architecture of iGNNs, which imposes tighter bounds on their expressiveness when compared to standard black-box GNNs. Notably, training with graphs of size up to \(5\) or \(6\) significantly diminishes GNNs generalization in the tasks we designed. We hypothesize that this is due to the scarcity of non-distributive and non-modular lattices during training, but it may also suggest that some patterns of size \(7\) and \(8\) might be quite relevant to generalize to larger graphs. Unfortunately, running generalization experiments with \(n 4\) was not possible since all such lattices trivially omitted \(}_{5}\) and \(_{3}\). It is worth mentioning that GNNs performed well even in the challenging multilabel case, where they had to learn a wider and more diverse set of concepts and tasks. In all experiments, we observe a plateau of the AUC ROC scores for \(n=8\), thus suggesting that a training set including graphs of this size might be sufficient to learn the relevant patterns allowing the generalization to larger lattice varieties. For detailed numerical results across all tasks, we refer the reader to Table 1 in Appendix D. Overall, these results emphasize the potential of GNNs in addressing complex problems in universal algebra, providing an effective tool to handle lattices that are difficult to analyze manually with pen and paper.

### Interpretability

**Concept-based explanations empirically validate universal algebra's conjectures (Figure 6).** We present empirical evidence to support the validity of theorems 2.3 and 2.4 by examining the concepts generated for modular and distributive tasks. For this investigation we leverage the interpretable structure of iGNNs. Similarly to Ribeiro et al. (37), we visualize in Figure 6 the weights of our trained linear classifier representing the relevance of each concept. We remark that the visualization is limited to the (top-\(5\)) most negative weights, as we are interested in those concepts that negatively affect the prediction of a property. In the same plot, we also show the prototype of each concept represented by the 2-hop neighbor-

Figure 4: Accuracy-interpretability trade-off in terms of concept completeness (accuracy) and model fidelity (interpretability). iGNNs attain optimal fidelity as model inference is inherently interpretable, outmatching equivalent black-box GNNs. All models attain similar results in terms of completeness.

Figure 5: Strong generalization performance with respect to the maximum number of nodes used in training.

Figure 6: Ranking of relevant clusters of lattices (x-axis) according to the interpretable GNN linear classifier weights (y-axis, the lower the more relevant the cluster). \(_{5}\) is always the most important sub-lattice to omit for modularity, while both \(_{3}\) and \(_{5}\) are relevant for distributivity, thus validating theorems 2.3 and 2.4.

[MISSING_PAGE_FAIL:9]

properties on graphs, while non-structural properties may require the adoption of other kinds of (interpretable) models.

Broader impact and perspectives.AI techniques are becoming increasingly popular for solving previously intractable mathematical problems and proposing new conjectures [(23; 26; 7; 12)]. However, the use of modern AI methods in universal algebra was a novel and unexplored field until the development of the approach presented in this paper. To this end, our method uses interpretable graph networks to suggest graph structures that characterize relevant algebraic properties of lattices. With our approach, we empirically validated Dedekind [(9)] and Birkhoff [(5)] theorems on distributive and modular lattices, by recovering relevant lattices. This approach can be readily extended--beyond equational properties determined by the omission of a sublattice in a variety [(43)]--to any structural property of lattices, including the characterization of congruence lattices of algebraic varieties [(1; 21; 31; 43)]. Our methodology can also be applied (beyond universal algebra) to investigate (almost) any mathematical property that can be topologically characterized on a graph, such as the classes of graphs/diagrams with a fixed set of polymorphisms [(25; 3; 32)].

Conclusion.This paper presents the first-ever AI-assisted approach to investigate equational and topological conjectures in the field of universal algebra. To this end, we present a novel algorithm to generate datasets suitable for AI models to study equational properties of lattice varieties. While topological representations would enable the use of graph neural networks, the limited transparency and brittle explainability of these models hinder their use in validating existing conjectures or proposing new ones. For this reason, we introduce a novel neural layer to build fully interpretable graph networks to analyze the generated datasets. The results of our experiments demonstrate that interpretable graph networks: enhance interpretability without sacrificing task accuracy, strongly generalize when predicting universal algebra's properties, generate simple explanations that empirically validate existing conjectures, and identify subgraphs suggesting the formulation of novel conjectures. These promising results demonstrate the potential of our methodology, opening the doors of universal algebra to AI with far-reaching impact across all mathematical disciplines.

This paper was supported by TAILOR and by HumanE-AI-Net projects funded by EU Horizon 2020 research and innovation programme under GA No 952215 and No 952026, respectively. This paper has been also supported by the Austrian Science Fund FWF project P33878 "Equations in Universal Algebra" and the European Union's Horizon 2020 research and innovation programme under grant agreement No 848077. This project has also received funding from the European Union's Horizon-MSCA-2021 research and innovation program under grant agreement No 101073307.