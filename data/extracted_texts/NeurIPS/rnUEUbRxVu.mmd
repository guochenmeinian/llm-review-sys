# DAPE: Data-Adaptive Positional Encoding for Length Extrapolation

Chuanyang Zheng\({}^{1}\)1 Yihang Gao\({}^{2}\)1 Han Shi\({}^{3}\) Minbin Huang\({}^{1}\) Jingyao Li\({}^{1}\)

Jing Xiong\({}^{4}\) Xiaozhe Ren\({}^{3}\) Michael Ng\({}^{5}\) Xin Jiang\({}^{3}\) Zhenguo Li\({}^{3}\) Yu Li\({}^{1}\)

\({}^{1}\)CUHK \({}^{2}\)NUS \({}^{3}\)Noah's Ark Lab \({}^{4}\)HKU \({}^{5}\)HKBU

Equal ContributionContact Email: cyzheng21@link.cuhk.edu.hk

###### Abstract

Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be data-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a **Data-Adaptive Positional Encoding** (DAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that DAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.

## 1 Introduction

Transformer-based models have shown state-of-the-art performances in many language processing tasks, including translation , question-and-answer , and commonsense reasoning . The transformer mainly consists of attention block, feed-forward block, and positional encoding. Recent works  have proved that quadratic-cost attention from the softmax is necessary for better performance, especially in long-context processing. The attention block was originally designed by applying softmax to the key-query multiplication, which requires quadratic computational cost. To address such challenges, some efficient transformers were proposed, including sliding window transformers (e.g., Streaming LLMs ), linear transformers (e.g., Performer ), and sparse transformers (e.g., Reformer and sparse Sinkhorn transformer ), etc. However, some negative results exist regarding efficient transformers' performances .

It has been noticed recently that well-designed positional encoding significantly improves the model performances, especially in the long-context tasks . While transformer-based models exhibit satisfying performances in tasks of consistent length and distribution, their effectiveness tends to diminish sharply when the input length exceeds the training length, e.g., long document summarization, "needle in a haystack" search, and long text generation. To avoid the expensive computation in training, the training length is usually preferred to be relatively small due to the quadratic cost ofsoftmax-based transformers. However, real-world applications often require processing longer input sequences, posing a significant challenge. Therefore, there is a growing interest in evaluating model performance by training on shorter sequences while testing on longer inputs. Standard transformers may not distinguish the ordering of tokens without external assistance. In practice, they depend on positional encoding to incorporate positional information, enabling the model to make meaningful token predictions. Without these encodings, token generation would lack the necessary contextual order, rendering the outputs nonsensical. The RoPE  positional encoding method demonstrated a notable performance degradation, failing entirely when the input length is double that of the training length [51; 10; 24]. A common characteristic among these positional encodings is their pre-defined and static nature. Specifically, they are fixed across various tasks and models, which may lead to their inability to adapt to varying input lengths and contexts effectively. To address this issue, recent works have introduced Functional Interpolation for Relative Positional Encoding (FIRE) , which utilizes a neural network to learn an implicit mapping from input positions to positional bias. A functional approach to positional encoding that dynamically adjusts positional biases based on semantic information (input context) allows the model to empower adaptability beyond the fixed inductive bias as adopted in previous studies (such as RoPE  and Alibi ). Although FIRE utilizes MLPs to learn positional embeddings, these embeddings remain fixed across different tasks once the training is completed. Intuitively, the learned static positional encoding (such as Kerple and FIRE) is an average optimal solution across all training samples. Consequently, while they might be generally effective, they are inherently suboptimal for any specific instance. This static nature limits their flexibility and applicability in various real-world scenarios that deviate from the training context. In this paper, we introduce a data-adaptive positional encoding (DAPE) method, inspired by the limitations of static PEs. DAPE dynamically adjusts the PE based on the semantic information (e.g., the current attention value) \(a\) and the positional indicator \(b\). The proposed PE is represented by MLPs due to their universal approximatability, i.e., MLPs\((a,b)\). We note that DAPE is compatible with all additive relative PEs and offers advantages in terms of interpretability and ease of implementation. The proposed DAPE incorporates both the semantic and the positional information, making the PE adaptive with the input data. The adaptivity allows DAPE to overcome the inflexibility and achieve relatively optimal performance for each individual instance by dynamically adjusting on each specific input data. To the best of our knowledge, this is the first semantically dependent and adaptive positional encoding method introduced in transformer architectures.

The paper is organized as follows. In Section 2, we review some related works on positional encoding methods, including absolute and relative positional encodings as well as the potentially no positional encoding in some transformer models. In Section 3, we introduce the proposed DAPE method with implementation on multi-head attention and analysis on computational costs. We conduct comprehensive experiments on DAPE, validating its effectiveness and performances on various language tasks and datasets, as reported in Section 4. In Section 6, some concluding remarks and potential future works are presented.

## 2 Related Works

**No positional encoding**. Haviv et al.  show that decoder-only Transformers with causal attention masks can learn positional information even without any explicit positional encoding. Recently, Kazemmejad et al.  proved the effectiveness of no positional encoding (NoPE) . Although

Figure 1: **Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is \(_{Q}(_{K})^{}\); (2) The Kerple bias is \(\); (3) The DAPE (with Kerple) bias is \(f(_{Q}(_{K})^{},)\). More examples are shown in Appendix I**

the NoPE can implicitly catch the positional information, it performs poorly compared with some explicit positional encoding methods .

**Absolute positional encoding**. Vaswani et al.  proposed Absolute positional encoding (APE) to endow transformers with positional information. In particular, in the first layer, a (learnable or fixed sinusoidal) real-valued encoding [69; 35; 42; 70; 47]\(_{i}^{d}\) is assigned to each position \(i\), leading to an APE matrix \(=[_{1},,_{n}]^{}\), which will be added to the input sequence. Though simple and straightforward, APE-based Transformers usually generalize poorly to longer sequences .

**Relative positional encoding**. Relative Positional Encoding (RPE) is another popular way to encode positional information [58; 56; 52], One popular RPE method in large language models is rotary positional encoding (RoPE) [62; 18; 67]. RoPE rotates the query and key vectors with an angle proportional to their absolute positions before the dot product attention, which results in attention being a function of the relative distance between the tokens, capturing the relative positional information. Press et al.  and Kazemnejad et al.  found that RoPE-based language models have poor length generalization. To address this, positional interpolation (PI)  is proposed to extend the context window. Following the direction, there are LongLora , LongRope , YaRN  and CLEX . Another popular direction is additive positional encoding. For most of these additive RPE methods, the computation of the (pre-softmax) attention logits can be unified using the following formula:

\[_{}()=_{Q}(_{K})^{}+,\]

where the bias matrix \(^{n n}\) is induced by the position encoding function \(b:^{2}\) and the \((i,j)\)-th entry of \(\) is defined as \(b(i,j)\). Different formulations and parameterizations of \(b\) lead to various RPE variants. Several methodologies that facilitate arbitrary sequence lengths include T5's RPE , Alibi , Kerple , Sandwich , and FIRE . Currently, additive RPE delivers relatively robust performance in length extrapolation without necessitating additional operations. Alibi constructs the bias matrix \(\) utilizing prior knowledge, resulting in a basis matrix devoid of learnable parameters . Conversely, both Kerple  and Sandwich  incorporate two learnable parameters to facilitate the learning of a bias matrix while retaining some elements of priors. FIRE suggests adopting a learnable continuous function, such as MLPs, to convert input positions to biases . Observing these developments, it becomes evident that the next generation of bias matrices will likely incorporate adaptivity and flexibility. Based on this insight, we propose our method DAPE, a semantically adaptive method.

## 3 Method

In this section, we formally introduce DAPE (data-adaptive positional encoding), a new relative positional encoding approach that further enhances transformer performance. Compared with previous works on static positional encoding methods, DAPE adopts semantically adaptive positional bias matrices depending on input context. We first demonstrate that most of the popular positional bias matrices are fixed once the training is finished, independent of the input sequences. To address this limitation, we then accordingly develop DAPE that captures the implicit relationships by MLPs and adjusts the bias matrices based on input context. Furthermore, we discuss a variant of DAPE with residual connections and its extensions to multi-head transformers.

### Additive Relative Positional Encoding

For most additive RPE methods, the computation of pre-softmax attention logits can be unified under the following formula:

\[_{}()=_{Q}(_{K})^{}+,\] (1)

where the bias matrix \(^{n n}\) is induced by the position encoding function \(b:^{2}\) and the \((i,j)\)-th entry of \(\) is defined as \(b(i,j)\). Various formulations and parameterizations of \(b\) give rise to different variants of RPE. Examples of additive RPE include: (1) Alibi: \(b(i,j)=-r|i-j|\), with the scaler \(r>0\) as a hyper-parameter; (2) Kerple: \(b(i,j)=-r_{1}log(1+r_{2}|i-j|)\) with \(r_{1}\) and \(r_{2}\) are two learnable parameters; (3) FIRE: \(b(i,j)=f_{}()})\), where the positional encoding function \(f_{}\) parameterized by \(\) is learned from data and \(\) is a transformation function aimed at assigning more model capacity to local positions.

We observe from the formulation of those additive RPEs that they remain static once the training process is completed and depend solely on the positions, regardless of the input context. This inflexibility and lack of adaptivity can lead to performance degradation, especially in tasks involving long-context generation and reasoning. Intuitively, the learned static RPEs are optimal on average across all training samples. However, this means they are suboptimal when considering each individual instance, as they cannot adapt to specific tasks. To address these challenges and enhance model performance, it is essential to adopt an alternative approach using a semantically adaptive RPE that depends on input context.

### Data-Adaptive Positional Encoding

For simplicity, we first consider the single-head case and the extension to the multi-head transformer will be discussed subsequently. The design of data-adaptive positional encodings in natural language tasks is motivated by the need to capture the intricate relationships between tokens. Arora et al.  reveals that associate recall accounts for most of the perplexity difference between transformer, RNN-based, and convolution models. For example, we consider a consistent pairing that "Hakuna" is always followed by "Matata" in a long paragraph. This pattern suggests a reduced reliance on positional information in favor of enhancing token embedding similarity, thus allowing for 'Hakuna' to be effectively linked with a preceding 'Matata'. Similarly, in tasks involving long-context understanding and search, semantic similarity should be prioritized in the attention mechanism rather than being overshadowed by positional encodings, which can be less relevant over long distances. Consequently, the transformer should preserve information without being influenced overly by positional distance. Instead, a satisfactory PE should integrate both semantic and positional information. Therefore, a semantically dependent positional encoding approach is preferable and expected to enhance model performances. Here, we use the attention \(_{Q}(_{K})^{}\) to represent the semantic information and positional bias matrices \(\) (e.g., Alibi and FIRE) to capture positional details. Then the context-adaptive PE is described by \(f(_{Q}(_{K})^{},)\), where \(f()\) is an implicit function that integrates both semantic and positional data into the desired positional encodings. Thus, the pre-softmax attention logit incorporated with DAPE is formulated as

\[_{}()=_{Q}(_{K})^{}+f( {X}_{Q}(_{K})^{},).\] (2)

Here, \(f:^{T T}^{T T}^{T  T}\) is an element-wise function. In practice, we utilize a two-layer _LeakyReLU_ neural network to parameterize \(f()\) due to its universal approximability . All parameters are learned directly from the data during the training process. This architecture allows \(f()\) to dynamically adjust positional embeddings based on the input context, ensuring that the encoding method is both adaptive and dependent on the input data.

Different from FIRE, which also models the implicit positional encoding by MLPs, our approach additionally integrates semantic information. This integration enables the adaptivity, flexibility, and context-dependency of the positional encodings. Significantly, our method is compatible with most additive RPE techniques, as these commonly involve positional bias matrices \(\) that inherently contain positional relations. Unlike previous RPEs, which rely solely on absolute positional differences, our DAPE method, can be seen as utilizing multi-level positional bias matrices. Here, the bias matrices dynamically adjust based on the input context, offering a more reasonable and responsive encoding mechanism.

**Expressiveness**. Due to the universal approximability of (_LeakyReLU_) neural networks , \(f()\) is capable of capturing complex relationships between the desired positional encoding and both semantic and positional information. Regardless of the semantic component, when the relative position \(i-j\) is used as input, DAPE can realize classical additive RPEs (e.g., Alibi and Kerple), according to . This demonstrates the versatility of DAPE in accommodating traditional encoding schemes while also offering enhanced capabilities. There exists a fundamental trade-off between the expressiveness and computational costs. Wider hidden layers lead to higher expressiveness but also contribute to more computational costs. In practice, we find that two-layer neural networks with 32 hidden units per layer provide sufficient expressiveness to deliver satisfactory performance, balancing complexity and efficiency effectively.

**Discussion.** We can also interpret the proposed method from an alternative perspective. In the standard transformer architecture, the pre-softmax attention typically involves the key-query similarity and the positional encoding by either addition (in the form of \(a+b\), e.g., Alibi and Kerple) or multiplication (in the form of \(a*b\), e.g., RoPE). Here, we propose a unified approach by replacing them with learnable MLPs, i.e., \((a,b)\). This configuration allows the model to learn the desired relationship between the pre-softmax attention, the key-query similarity and the positional encoding. It can also be regarded as a new transformer architecture that empower the transformer with additional MLPs on pre-softmax attentions.

**A variant of DAPE with residual connections**. It is well-known that deep neural networks may suffer from gradient vanishing. To further enhance the practical performances, we introduce the residual connection for positional information. Consequently, Equation 2 is modified as follows:

\[_{}()=_{Q}(_{K})^{}++f( _{Q}(_{K})^{},).\] (3)

In this reformulation, \(f()\) acts as an adaptive correction term to the traditionally fixed RPE, dynamically adjusting the positional bias matrices \(\) based on both semantic and positional inputs. In Section 4, we empirically explore the impact of residual connections in DAPE. Our observations reveal that for well-behaved bias matrices \(\), the DAPE model with residual connections, as specified in Equation 3, is preferable. Conversely, if the bias matrix is underperforming but still conveys positional information, the original implementation in Equation 2 is more effective.

**Multi-head DAPE**. In its simplest form, DAPE is considered for a single-head case as described in Equation 2 and Equation 3. However, adopting a multi-head mechanism significantly enhances model capabilities. To effectively combine both semantic and positional information, the DAPE in a multi-head setup processes the key-query similarities and bias matrices from all heads. Specifically, for an \(h\)-head layer, the function \(f()\) inputs a \(2h\)-dimensional concatenation of key-query similarities and positional biases. It then outputs \(h\)-dimensional vectors, where each element corresponds to the DAPE for the respective head. We have shown the code implementation in Appendix J. Importantly, semantic and positional information across different heads are processed simultaneously within the same MLPs, rather than sequentially. This approach not only improves computational efficiencies through parallel processing but also capitalizes on the richer semantic information available across all heads. Compared to the key-query similarity derived from a single head, the comprehensive attention from all heads yields more substantial semantic information.

**Computational costs analysis**. Here, we evaluate the additional computational costs introduced by the DAPE method, compared with the classical positional encoding methods (e.g., Alibi and Kerple). We consider a transformer model with \(h\) heads and assume a sequence length of \(N\) and all hidden dimensions in the attention layer being \(d\). Then the total computational cost for a standard transformer equipped with classical PEs is \((hN^{2}d+hNd^{2})\). When incorporating the proposed DAPE, which employs two-layer MLPs with hidden dimension \(D_{}\), the additional computational costs are \((hN^{2}D_{})\). If the hidden dimensions \(D_{} d\), the incremental computational cost introduced by DAPE is not significant.

## 4 Experiment

Baselines.We evaluate the proposed DAPE against a range of established baselines, including NoPE , RoPE , YaRN , Randomized RoPE [57; 31], T5's Bias , Alibi , Kerple , and FIRE . For RoPE, the randomized positional encoding [57; 31] is applied to enhance the model performance, extending the randomized length to four times that of the training length.

Datasets.Our analysis involves training language models on the Arxiv and Books3 datasets, which are frequently used benchmarks for evaluating model performance [52; 13; 41; 24]. We start our evaluation by comparing the last 256 tokens' zero-shot perplexity across different input lengths. Besides perplexity as evaluation metrics, we also employ the downstream datasets in randomized positional encoding  to evaluate DAPE, where details are included in Appendix D.

Experiment settings.Initially, we compare DAPE with other baselines at training lengths of 128, 512, and 1024, with model size 125M decoder-only Transformers , whose configuration is shown in Appendix B. Subsequently, we evaluate the performance of larger model size 350M, DAPE variants and explore the impact of hidden dimension of MLPs \(D_{}\). We also examine the computational efficiency of DAPE, focusing on processing times. Additionally, we provide visualizations of the DAPE bias in the Appendix I. Finally, we also evaluate DAPE on algorithmic reasoning datasets via accuracy metrics.

### Comparisons with Baselines

DAPE's superior performance within training length and beyond training length, compared to all baselines.As shown in Figure 2 and Table 5, DAPE consistently outperforms established baselines such as RoPE, Alibi, and Kerple across various settings. Notably, DAPE-Kerple (the positional information in DAPE comes from Kerple bias matrices) outstands in both short and long training lengths (128 and 512), compared to previous RoPE, T5's bias, and so on. It demonstrates that the semantic adaptivity of DAPE significantly enhances its state-of-the-art performance against all other static positional encoding methods.

The performance on longer training length 1024.As shown in Figure 3, the proposed method consistently delivers state-of-the-art performance for the training length of 1024. When the evaluation extends to 2048, both DAPE-Kerple and DAPE-FIRE achieve notable results, recording performances of 3.91 and 3.93 perplexity scores, respectively. Remarkably, DAPE-FIRE behaves well at the longer evaluation length of 8192, achieving a performance of 3.91 scores and surpassing Alibi's score of 4.28. These findings reveal that DAPE sustains robust performance with a longer training length of 1024.

DAPE enhances intra-length performance, indicating that its lower perplexity may come from thorough utilization of entire sentences but not disregarding long-distance information (Also proved in Figure 1).Compared to Alibi, Kerple, and FIRE, the adapted versions DAPE-Alibi, DAPE-Kerple, and DAPE-FIRE demonstrate consistently and significantly better intra-length performance. With the growing sequence length, the Alibi tends to transition from full attention to almost local attention, and this is why Alibi is worse than most baselines within training length but better beyond training lengths. The results (as shown in Table 5) indicate that the superior intra-length performance of DAPE is statistically significant, with a p-value less than 0.05. Therefore, the consistent intra-length performances across various training lengths indicate that the lower perplexity of DAPE results from effectively utilizing the entire sequence, rather than focusing on local parts and neglecting long-distance information.

Figure 3: **Results on the training length 1024.**

Figure 2: **Comparisons with baselines: performance with training lengths 128 and 512 on Arxiv and Books3 datasets.**DAPE significantly improves length extrapolation performance, compared to ALibi, Kerple, and FIRE.DAPE-Kerple significantly surpasses competitors like vanilla Kerple when training and evaluating at different lengths. On the Arxiv dataset trained at a length of 128, DAPE-Kerple achieves a remarkably low perplexity of 5.00 at an evaluation length of 8192, in stark contrast to Kerple's 31.93. Similarly, on the Books3 dataset with a training length of 512, DAPE-Kerple records a perplexity of 17.88 at the same extended evaluation length, far outperforming Kerple's 39.31. These results affirm that DAPE, through its semantic adaptivity and flexibility, consistently enhances performance beyond training lengths, eclipsing static positional encoding methods.

### The Effect of Model Size

DAPE enhances performance with increasing model sizes.As the model size grows (as shown in Figure 4), DAPE consistently demonstrates an improvement in performance metrics. When the model size is augmented from 125M to 350M, the perplexity at an evaluation sequence length of 8192 (with a training length of 512) for DAPE-Alibi shows a notable decrease from 3.82 to 3.57. These numbers are appreciably smaller than those recorded for original Alibi, which decreases from 4.54 to 4.21 in perplexity, indicating a robust performance improvement. Additionally, DAPE-Kerple significantly reduces the perplexity for Kerple, bringing it down from an initial 22.76 to an impressive 3.43. These results confirm that DAPE retains its efficacy and continues to perform well even as the model size is increased, mainly due to the adoption of semantically adaptive PEs.

DAPE methods almost are ranked top-3 with large model size.With the incremental model size, DAPE-FIRE begins to match, and nearly approach, the performance levels of Alibi. Initially, at a model size of 125M and a training length of 512, DAPE-FIRE achieves a perplexity of 5.71 at an evaluation sequence length of 8192, while Alibi stands at a perplexity of 4.54. However, as the model size is increased to 350M, the performance gap significantly narrows. Specifically, DAPE-FIRE outperforms Alibi regarding the perplexity scores when the evaluation length is smaller than 4096, as the model size grows for evaluation. In conclusion, as shown in Figure 4, we observe that the DAPE methods almost win the top-3 among all positional encoding methods. This trend underlines the scalability and adaptability of DAPE, emphasizing its potential to handle more substantial computation challenges.

### Different Variants of DAPE

In this section, we evaluate the performance of DAPE across its various forms. Our analysis focuses on DAPE-Kerple. Notably, as shown in Figure 5, all variants of DAPE surpass the baseline performance of Kerple. The _Addition_Residual_ variant of DAPE, while requiring less computational effort, delivers relatively inferior results. As illustrated in Figure 5, concatenation methods (either _Concat_ or _Concat_Residual_) outperform the _Addition_Residual_ approach, for both the training length of 128 and 512. Furthermore, both _Concat_ and _Concat_Residual_ exhibit comparable performance metrics. Specifically, at a training length of 128, _Concat_Residual_ records a score of 5.00 and _Concat_ scores 5.03 at an evaluation length of 8192, whereas _Add_Residual_ posts a 5.17 perplexity score. With a training length of 512, _Concat_Residual_ achieves a score of 3.70, and _Concat_ scores 3.69 at an evaluation length of 8192, compared to _Add_Residual_'s 3.75. Based on the current observation, the different variants of DAPE show comparable performances, compared to baselines.

Figure 4: **The effect of model size:** for the 350M model, the performance with training lengths 128 and 512 on the Arxiv dataset.

### The Effect of the Hidden Dimension \(D_{}\)

Even small \(D_{}\) can improve the performance.The experiments are conducted with Alibi and DAPE-Alibi. As shown in Appendix Figure 6, when considering the training length 128 and \(D_{}\) is set as 4, the DAPE-Alibi achieves 8.25 at evaluation length 128 and 5.67 at length 8192, which is better than Alibi's 8.31 and 5.85. Whatever \(D_{}\) is 4, 16 32, or 64, the performance is always better than the original Alibi at all evaluation lengths. This suggests the effectiveness of DAPE, even with smaller \(D_{}\).

The choice of \(D_{}\).Based on the experiment, overly small values of \(D_{}\) can degrade performance, although they still perform better than the baseline. Conversely, larger values of \(D_{}\) increase computational costs. The function \(f()\) is implemented as a two-layer MLP, where the input dimension is either the head number or twice the head number, and the output dimension is the head number. Therefore, we recommend setting the hidden dimension to the head number to prevent information loss and ensure the capacity of \(f()\).

### The Time Cost

Practical additional time cost.The additional training ratio will gradually decrease with a larger model size, compared to baseline Kerple.. The cost of Feed-Forward Network is: \(O(Nd_{head}^{2}d_{hidden}^{2})\)=\(aNd_{head}^{2}d_{hidden}^{2}\), where a is a constant, N is the sequence length, \(d_{head}\) is the attention head number and \(d_{hidden}\) is the dimension for attention calculation. The cost of Attention: \(O(N^{2}d_{head}d_{hidden})\)=\(bN^{2}d_{head}d_{hidden}\), where b is a constant. The additional cost of DAPE: \(O(N^{2}d_{head}d_{DAPE})\)=\(cN^{2}d_{head}d_{DAPE}\), where c is a constant. The cost ratio is \(^{2}d_{hidden}^{2}+bNd_{head}^{2}d_{hidden}}{aNd_{head}^{2}d_{hidden }^{2}+bNd_{head}d_{hidden}+cN^{2}d_{head}d_{DAPE}}\)=\(d_{hidden}^{2}+bNd_{hidden}}{ad_{head}^{2}d_{hidden}+bNd_{hidden}+cNd_{DAPE}}\). Therefore, with the fixed sequence length and \(d_{DAPE}\), with the model becomes larger (with bigger \(d_{head}\) and \(d_{hidden}\)), the additional cost ratio of DAPE will greatly become smaller. Also, we have shown in Figure 6 that \(d_{DAPE}\) still works well with very small value, such as 4.

  
**Method** & **350M Total** & **Ratio** & **2.7B Total** & **Ratio** & **6.7B Total** & **Ratio** \\  RoPE  & 210.01 & 0.9366 & 472.63 & 1.1187 & 635.57 & 0.8858 \\  T5’s bias  & 355.16 & 1.5839 & 537.62 & 1.2725 & 808.85 & 1.1273 \\  ALiBi  & 172.60 & 0.7697 & 325.95 & 0.7715 & 596.77 & 0.8317 \\  Kerple  & 189.91 & 0.8469 & 370.32 & 0.8765 & 661.82 & 0.9224 \\  FIRE  & 248.13 & 1.1066 & 432.63 & 1.0240 & 797.68 & 1.1118 \\  DAPE-Kerple & 224.22 & 1.0000 & 422.48 & 1.0000 & 717.46 & 1.0000 \\   

Table 1: The time cost (millisecond) under different testing lengths, with \(D_{}\) as 32 and default batch size 1, with training length 512.

### The Visualization of DAPE

In this subsection, we present the visualization of learned positional encoding biases from a DAPE-Kerple model pretrained on Arxiv (training length is 512). We plot the learned positional encoding bias for the query token at the 8192th position, for all the attention heads from selected layers in Figure 1. We would like to highlight two features of DAPE. First, in different attention heads, the bias matrix of DAPE learns both local and "anti-local" attention patterns that emphasize more on far-away keys (just like FIRE), compared to a fixed local inductive bias (such as Kerple and Alibi). Secondly, the bias matrix can be dynamically adjusted with different attention values, compared to the static bias fixed for all attentions. We have shown more examples, including different layers and different samples, in Appendix I.

### Experiments on CHE Benchmark

Besides employing perplexity as an evaluation metric, we also evaluated DAPE on downstream Chomsky Hierarchy Evaluation Benchmark (CHE)  (need to utilize the whole sentence information to generate correct answers) to further discuss its effects. The experimental setup follows randomized positional encodings , detailed in Table 4, with the experiment setting shown in Appendix Section D. Overall, FIRE outperforms Kerple in 9 out of 14 tasks, while Kerple outperforms Alibi in 11 out of 14 tasks. This observation aligns with findings in , suggesting that the experiments in Table 2 are reliable and reflect the performance of positional encoding in downstream tasks.

DAPE works better on permutation-variant tasks.DAPE (with Kerple and FIRE) presented the best performance in 10 out of 11 permutation-variant tasks (which require positional information), achieving the second-best performance in the Solve Equation task. This underscores the efficacy of DAPE with semantic adaptivity in handling permutation-variant challenges.

DAPE's performance on permutation-invariant tasks.In tasks that are permutation-invariant, where positional information is non-critical, DAPE demonstrated comparable performance. Notably, DAPE-Alibi achieved scores of 50.30 on Parity Check and 99.38 on Bucket Sort tasks, compared to the highest scores of 50.97 and 99.57, respectively, demonstrating competitive performances.

Comparative performance improvements.DAPE consistently enhanced performance across various tasks, especially on permutation-variant tasks. Specifically, DAPE improved upon Alibi and FIRE's results in all 11 tested permutation-invariant tasks. Similarly, it outperformed Kerple in 10 of these tasks. These results highlight the effectiveness of DAPE over static positional encoding methods like Alibi, Kerple, and FIRE, resulting from its dynamic adaptivity.

    &  &  \\ 
**Level** & **Task** & **Learned** & \(/\) & **RoPE** & **Relative**  & **ALibi** & **Kerple** & **FIRE** & **Alibi** & **Kerple** & **FIRE** \\   & Even Pairs & 50.04 & 91.27 & 99.98 & 96.60 & 73.52 & 57.03 & 73.86 & 99.99 & 99.95 & **100** \\  & Modular Arithmetic (Simple) & 19.95 & 20.39 & 21.38 & 20.84 & 20.02 & 21.79 & 21.09 & 23.58 & **24.47** & 24.46 \\  & Parity Check\(\) & 50.14 & 50.52 & 50.05 & 50.09 & 50.09 & 50.07 & **50.97** & 50.30 & 50.07 & 50.04 \\  & Cyclic Convolution\(\) & 24.97 & 25.37 & 27.63 & 26.95 & 24.64 & 29.47 & 28.41 & 22.99 & **34.53** & 27.54 \\   & Stack Manipulation & 59.92 & 65.92 & 61.49 & 64.73 & 66.42 & 66.93 & 69.33 & 68.18 & **72.04** & 70.90 \\  & Reverse String & 52.76 & 67.28 & 65.23 & 65.59 & 71.09 & 71.54 & 65.89 & 73.37 & 70.24 & **76.40** \\  & Modular Arithmetic & 31.00 & 30.70 & 31.25 & 31.74 & 30.56 & 24.79 & 30.92 & 31.34 & **32.37** & 31.50 \\  & Solve Equation & 20.00 & 19.97 & 21.85 & **22.93** & 19.92 & 21.15 & 22.06 & 20.03 & 22.49 & 22.42 \\   & Duplicate String & 52.77 & 65.44 & 64.97 & 67.66 & 65.13 & 66.72 & 69.03 & 70.84 & **72.95** & 72.71 \\  & Missing Update & 50.38 & 49.78 & 63.77 & 22.34 & 72.96 & 79.27 & 83.81 & 87.57 & **89.17** \\  & Odds First & 52.77 & 58.61 & 61.00 & 61.57 & 59.88 & 62.59 & 63.28 & 63.78 & **67.88** & 66.34 \\   & Binary Addition & 54.63 & 55.78 & 55.59 & 56.96 & 54.27 & 56.35 & 55.70 & 59.71 & **60.88** & 56.62 \\   & Compute Soft & 50.47 & 51.11 & 51.88 & 51.63 & 50.63 & 51.11 & 50.80 & 51.64 & 51.33 & **52.46** \\   & Bucket Sort\(\) & 98.32 & 98.92 & 98.12 & 99.31 & 98.45 & 99.38 & **99.57** & 99.38 & 98.81 & 99.37 \\   

Table 2: Train on length 40 with 200k steps, and test from lengths 41 to 500. The random accuracy is 50%, except for Modular Arithmetic (Simple), Cycle Navigation, Bucket Sort, Solve Equation and Modular Arithmetic, where it is 20%. \(\) denotes permutation-invariant tasks, which are expected to be solved without positional information.

Evaluation Protocol

In this work, we initially utilize the model to process the entire input sentence and subsequently select the final 256 tokens for perplexity computation. This approach contrasts with a variety of other studies, which employ methods that process the full sequence during perplexity calculations . As a result, our reported baseline perplexity is comparatively higher than the results presented in ALiBi , which adopt a non-overlapping evaluation strategy. This method divides sequences longer than \(L\) into multiple segments of length \(L\), thereby yielding lower perplexity figures.

Though ALiBi  and Kerple all claim that they use non-overlapping evaluations, their reported results are different. For example, in the ALiBi paper Table 2, the sinusoidal position encoding perplexity increases from 20.05 (evaluation length 512) to 406.01 (evaluation length (evaluation length 15512), while in Kerple paper Table 3 the sinusoidal position encoding perplexity from 33 to 30046. This may be caused by **different evaluation protocols and training strategies.**

Recommended Protocols.We strongly recommend that researchers process the entirety of the sequence before selecting the last \(K\) tokens for the purpose of calculating perplexity. The rationale behind processing the whole sentence is that it provides a comprehensive evaluation of the model's capability to handle long-context dependencies, thus offering a more accurate reflection of its performance. Following this step, we advocate for the selection of the last \(K\) tokens to compute perplexity, ensuring that the same number of tokens is used across different evaluation lengths, which promotes consistency and comparability in the results.

Release this work's code for future work.In light of this methodology, we have made our code publicly available to other researchers in the field. This initiative aims to facilitate a standardized comparison and evaluation of their respective methods, thereby advancing the collective understanding of model performance in relation to perplexity calculations.

## 6 Conclusion

In this paper, we propose the data-adaptive positional encoding (DAPE) by incorporating both the semantic and the positional information to improve the model performance. We show that the additional computation introduced by DAPE is not significant under proper choices of hyperparameters. We conduct comprehensive experiments on Arxiv, Books3, and CHE to validate the effectiveness of the proposed method, revealing that the adaptive PE method has advantages over static PEs. We believe that the DAPE could benefit the whole community, especially on length generalization tasks.