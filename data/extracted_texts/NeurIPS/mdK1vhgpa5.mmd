# Variational Continual Test-Time Adaptation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Continual Test-Time Adaptation (CTTA) task investigates effective domain adaptation under the scenario of continuous domain shifts during testing time. Due to the utilization of solely unlabeled samples, there exists significant uncertainty in model updates, leading CTTA to encounter severe error accumulation issues. In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA. At the source stage, we transform a pretrained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model. During the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model. Our novel approach updates the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework. Our code is anonymously available at https://anonymous.4open.science/r/vcotta-D2C3/.

## 1 Introduction

Continual Test-Time Adaptation (CTTA)  aims to enable a model to accommodate a sequence of distinct distribution shifts during the testing time, making it applicable to various risk-sensitive applications in open environments, such as autonomous driving and medical imaging. However, real-world non-stationary test data exhibit high uncertainty in their temporal dynamics , presenting challenges related to error accumulation . Previous CTTA studies rely on methods that enforce prediction confidence, such as entropy minimization. However, these approaches often lead to predictions that are overly confident and less well-calibrated, thus limiting the model's ability to quantify risks during predictions. The reliable estimation of uncertainty becomes particularly crucial in the context of continual distribution shift . It is meaningful to design a model capable of encoding the uncertainty associated with temporal dynamics and effectively handling distribution shifts. The objective of this paper is to devise a CTTA procedure that not only enhances predictive accuracy under distribution shifts but also provides reliable uncertainty estimates.

To address the above problem, we refer to the Bayesian Inference (BI) , which retains a distribution over model parameters that indicates the plausibility of different settings given the observed data, and it has been witnessed as effective in traditional continual learning tasks . In Bayesian continual learning, the posterior in the last learning task is set to be the current prior which will be multiplied by the current likelihood. This kind of prior transmission is designed to reduce catastrophic forgetting in continual learning. However, this is not feasible in CTTA because unlabeled data may introduce unreliable prior. As shown in Fig. 1, an unreliable prior may lead to a poor posterior, which may then propagate errors to the next inference, leading to the accumulation of errors.

Thus, we delve into the utilization of BI framework to evaluate model uncertainty in CTTA, aiming to mitigate the impact of unreliable priors and reduce the error propagation. To approximate the intractable likelihood in BI, we adopt to use online Variational Inference (VI) [49; 42], and accordingly name our method Variational Continual Test-Time Adaptation (VCoTTA). At the source stage, we first transform a pretrained deterministic model, say CNN, into a Bayesian Neural Network (BNN) by a variational warm-up strategy, where the local reparameterization trick  is used to inject uncertainties into the source model. During the testing phase, we employ a mean-teacher update strategy, where the student model is updated via VI and the teacher model is updated by the exponential moving average. Specifically, for the update of the student model, we propose to use a mixture of priors from both the source and teacher models, then the Evidence Lower BOund (ELBO) becomes the cross-entropy between the student and teachers plus the KL divergence of the prior mixture. We demonstrate the effectiveness of the proposed method on three datasets, and the results show that the proposed method can mitigate the error accumulation in CTTA and obtain clear performance improvements.

Our contributions are three-fold:

1. This paper develops VCoTTA, a simple yet general framework for continual test-time adaptation that leverages online VI within BNN.
2. We propose to transform an off-the-shelf model into a BNN via a variational warm-up strategy, which injects uncertainties into the model.
3. We build a mean-teacher structure for CTTA, and propose a strategy to blend the teacher's prior with the source's prior to mitigate unreliable prior problem.

## 2 Related Work

### Continual Test-Time Adaptation

Test-Time Adaptation (TTA) enables the model to dynamically adjust to the characteristics of the test data, i.e. target domain, in a source-free and online manner [25; 46; 50]. Previous works have enhanced TTA performance through the designs of unsupervised loss [37; 58; 32; 9; 7; 17]. These endeavours primarily focus on enhancing adaptation within a fixed target domain, representing a single-domain TTA setup, where models adapt to a specific target domain and then reset to their original pretrained state with the source domain, prepared for the next target domain adaptation.

Recently, CTTA  has been introduced to tackle TTA within a continuously changing target domain, involving long-term adaptation. This configuration often grapples with the challenge of error accumulation [47; 51]. Specifically, prolonged exposure to unsupervised loss from unlabeled test data during long-term adaptation may result in significant error accumulation. Additionally, as the model is intent on learning new knowledge, it is prone to forgetting source knowledge, which poses challenges when accurately classifying test samples similar to the source distribution.

To solve the two challenges, the majority of the existing methods focus on improving the confidence of the source model during the testing phase. These methods employ the mean-teacher architecture  to mitigate error accumulation, where the student learns to align with the teacher and the teacher

Figure 1: In CTTA task, a BNN model is first trained on a source dataset, and then is used to adapt to updated with unreliable priors, which may result in error accumulations.

updates via moving average with the student. As to the challenge of forgetting source knowledge, some methods adopt augmentation-averaged predictions [51; 2; 11; 55] for the teacher model, strengthening the teacher's confidence to reduce the influence from highly out-of-distribution samples. Some methods, such as [11; 6], propose to adopt the contrastive loss to maintain the already learnt semantic information. Some methods believe that the source model is more reliable, thus they are designed to restore the source parameters [51; 2]. Though the above methods keep the model from confusion of vague pseudo labels, they may suffer from overly confident predictions that are less calibrated. To mitigate this issue, it is helpful to estimate the uncertainty in the neural network.

### Bayesian Neural Network

Bayesian framework is natural to incorporate past knowledge and sequentially update the belief with new data . The bulk of work on Bayesian deep learning has focused on scalable approximate inference methods. These methods include stochastic VI [22; 34], dropout [16; 27] and Laplace approximation [41; 15] etc., and leveraging the stochastic gradient descent (SGD) trajectory, either for a deterministic approximation or sampling. In a BNN, we specify a prior \(p()\) over the neural network parameters, and compute the posterior distribution over parameters conditioned on training data, \(p(|) p()p(|)\). This procedure should give considerable advantages for reasoning about predictive uncertainty, which is especially relevant in the small-data setting.

Crucially, when performing Bayesian inference, we need to choose a prior distribution that accurately reflects the prior beliefs about the model parameters before seeing any data [18; 14]. In conventional static machine learning, the most common choice for the prior distribution over the BNN weights is the simplest one: the isotropic Gaussian distribution. However, this choice has been proved indeed suboptimal for BNNs . Recently, some studies estimate uncertainty in continual learning within a BNN framework, such as [38; 12; 13; 28]. They set the current prior to the previous posterior to mitigate catastrophic forgetting. However, the prior transmission is not reliable in the unsupervised CTTA task. Any prior mistakes will be enlarged by adaptation progress, manifesting error accumulation. To solve the unreliable prior problem, this paper proposes a prior mixture method based on VI.

## 3 Variational Inference in CTTA

We start from the supervised BI in typical continual learning, where the model aims to learn multiple classification tasks in sequence. Let \(=\{(x_{n},y_{n})\}_{n=1}^{N}\) be the training set, where \(x_{n}\) and \(y_{n}\) denotes the training sample and the corresponding class label. The task \(t\) is to learn a direct posterior approximation over the model parameter \(\) as follows.

\[p(|_{1:t})\ \ p_{t}()p(_{t}|),\] (1)

where \(p(|_{1:t})\) denotes the posterior of sequential tasks on the learned parameter and \(p(_{t}|)\) is the likelihood of the current task. The current prior \(p_{t}()\) is regarded as the given knowledge.  proposes that this current prior can be the posterior learned in the last task, _i.e._, \(p_{t}()=p(|_{1:t-1})\), where the inference becomes

\[p(|_{1:t})\ \ p(|_{1:t-1})p( _{t}|).\] (2)

The detailed process can be shown in Appendix A.

In contrast to continual learning, CTTA faces a sequence of learning tasks in test time without any label information, requiring the model to adapt to each novel domain sequentially. In this case, we assume that each domain is i.i.d. and the classes are separable following many unsupervised studies [36; 48; 5], more details about the assumption can be seen in Appendix B.1. We use \(=\{x_{n}\}_{n=1}^{N}\) to represent the unlabeled test dataset. The CTTA model is first trained on a source dataset \(_{0}\), and then adapted to unlabeled test domains starting from \(_{1}\). For the \(t\)-th adaptation, we have

\[p(|_{1:t}_{0}) p_{t}() p(_{t}|).\] (3)

Similarly, we can set the last posterior to be the current prior, _i.e._, \(p_{t}()=p(|_{1:t-1}_{0})\) and \(p_{1}()=p(|_{0})\). However, employing BI for adaptation on unlabeled testing data can result in untrustworthy posterior estimates. Therefore, during subsequent adaptation, the untrustworthy posterior automatically transform into unreliable priors, leading to error accumulation. In other words,an unreliable prior \(p_{t}()\) will make the current posterior even less trustworthy. Moreover, the joint likelihood \(p(_{t}|)\) for \(t>0\) is intractable on unlabeled data.

To make the BI feasible in CTTA task, in this paper, we transform the question to an easy-to-compute form. Referring to , the unsupervised inference can be transformed into

\[p(|) p()(- H( |)),\] (4)

where \(H\) denotes the conditional entropy and \(\) is a scalar hyperparameter to weigh the entropy term. This simple form reveals that the prior belief about the conditional entropy of labels is given by the inputs. The observation of the input \(\) provides information on the drift of the input distribution, which can be used to update the belief over the learned parameters \(\) through Eq. (4). Consequently, this allows the utilization of unlabeled data for BI. More detailed derivations can be seen in Appendix B.2.

In a BNN, the posterior distribution is often intractable and some approximation methods are required, even when calculating the initial posterior. In this paper, we leverage online VI, as it typically outperforms the other methods for complex models in the static setting . VI defines a variational distribution \(q()\) to approximate the posterior \(p(|)\). The approximation process is as follows.

\[q_{t}()=_{q}[q()\ \|\ }p_{t}()e^{- H(_{t}|)} ],\] (5)

where \(\) is the distribution searching space and \(Z_{t}\) is the intractable normalizing hyperparameter. Thus, referring to the derivations in Appendix C, the ELBO is computed by

\[=-_{ q()}H(_ {t}|)-(q()||p_{t}()).\] (6)

Optimizing with Eq. (6) makes model adapt to domain shift. While VI offers a good framework for measuring uncertainty in CTTA, it is noteworthy that VI does not directly address the issue of unreliable priors. The error accumulation remains a significant concern.

Despite this, the form of the ELBO in variational inference offers a pathway for mitigating the impact of unreliable priors. In Eq. (6), the _entropy term_ may result in overly confident predictions that are less calibrated, while the _KL term_ may be directly affected by an unreliable prior. In the following section, we will discuss how to solve the problems when computing the two terms.

## 4 Adaptation and Inference in VCoTTA

### Entropy term: VI by Mean-Teacher Architecture

In the above section, we introduce the VI in CTTA but challenges remain, _i.e._, the unreliable prior. To mitigate the challenge in the entropy term, we adopt a Mean-Teacher (MT) structure  in the Bayesian inference process. MT is initially proposed in semi-supervised and unsupervised learning, where the teacher model guides the unlabeled data, helping the model generalize and improve performance with the utilization of large-scale unlabeled data.

MT structure is composed of a student model and a teacher model, where the student model learns from the teacher and the teacher updates using Exponential Moving Average (EMA) . In VI, the student is set to be the variational distribution \(q()\), which is a Gaussian mean-field approximation for its simplicity. It is achieved by stacking the biases and weights of the network as follows.

\[q()=_{d}(_{d};_{d},( _{d}^{2})),\] (7)

Figure 2: VCoTTA is built on mean-teacher structure, and conducts VI in CTTA using a mixture of teacher prior and source prior. The next teacher prior is updated by the exponential moving average.

where \(d\) denotes each dimension of the parameter. The teacher model \(()\) (we use bar to distinguish the general prior) is also a Gaussian distribution. Thus, the student model is updated by aligning it with the teacher model through the use of a cross-entropy (CE) loss

\[L_{}(q,)=-_{ q()}_{ x}[(x|) q(x|)].\] (8)

In our implementation, we also try to use Symmetric Cross-Entropy (SCE)  in CTTA,

\[L_{}(q,)=-_{ q()}_{x}(x|) q(x|)+q(x| )(x|).\] (9)

SCE balances the gradient for high and low confidence, benefiting the unsupervised learning.

### KL term: Mixture-of-Gaussian Prior

For the KL term, to reduce the impact of unreliable prior, we propose a mixing-up approach to combining the teacher and source prior adaptatively. The source prior is warmed up upon the pretrained deterministic model \(p_{1}()=p(|_{0})\) (see Sec. 4.3.1). The teacher model \(_{t}()\) is updated by EMA (see Sec. 4.3.3). We assume that the prior should be the mixture of the two Gaussian priors. Using only the source prior, the adaptation is limited. While using only the teacher prior, the prior is prone to be unreliable.

We use the mean entropy derived from a given serious data augmentation to represent the confidence of the two prior models, and mix up the two priors with a modulating factor

\[=|}_{i}_{0})/}}{e^{H(x|_{0})/}+e^{H(x|)/}},\] (10)

where \(\) denotes augmentation types. \(_{0}\) and \(}\) are the parameters of the source model and the teacher model. \(\) means the temperature factor. Thus, as shown in Fig. 3(b), the current prior \(p_{t}()\) is set to the mixture of priors as

\[p_{t}()= p_{1}()+(1-)_{t}( ).\] (11)

In the VI, we use the upper bound to update the KL term  (see Appendix D.1) for simplicity,

\[(q||p_{t})(q||p_{0})+( 1-)(q||_{t}).\] (12)

Furthermore, we also improve the teacher-student alignment in the entropy term (see Eq. (9)) by picking up the augmented logits with a larger confidence than the raw data. That is, we replace the teacher log-likelihood \((x|)\) by

\[^{}(x|)=}(f ((x^{}_{i}))>f((x))+)(x^{ }_{i})}{_{i}(f((x^{}_{i}))>f( (x))+)},\] (13)

where, for brevity, we let \((x^{}_{i})=(x^{}_{i}|)\) and \((x)=(x)|)\) in short. \(f()\) is the confidence function. \(\) denotes the confidence margin and \(()\) is an indicator function. Eq. (13) can be regarded as a filter, meaning that for each sample, the reliable teacher is represented by the average of its augmentations with \(\) more confidence. In Appendix D.2, we prove that the proposed mixture-of-Gaussian is benilical to CTTA. In Appendix E.1, we discuss the influence of different \(\).

### Adaptation and Inference

#### 4.3.1 Variational Warm-up

To obtain a source BNN, instead of training a model from scratch on the source data \(_{0}\), we transform a pretrained deterministic CNN to a BNN by variational warm-up strategy. Specifically, we leverage the local reparameterization trick  to add stochastic parameters, and warm up the model:

\[q_{0}()=_{q}[q()\ \|\ }p()p(_{0}|)],\] (14)

where \(p()\) represents the prior distribution, say the pretrained deterministic model. Eq. (14) denotes a standard VI on the source data, and we optimize the ELBO to obtain the variational distribution . By the variational warm-up, we can easily transform an off-the-shelf pretrained model into a BNN with a stochastic dynamic. The variational warm-up strategy is outlined in Algorithm 1.

The warm-up strategy is a common approach in TTA and CTTA tasks to further build knowledge structure for the source model, such as . Some other methods may not use warm-up but still use the source data, such as . The warm-up strategy uses the source data only before deploying the model to CTTA scenario, and it is regarded as a part of pretraining. All of these methods using source data are operationalized in source-free at test time and find it is beneficial to CTTA. We use the warm-up to inject the uncertainties into a given source model, i.e., turning an off-the-shelf pretrained CNN model into a pretrained BNN model. This is convenient to obtain a pretrained BNN, because the warm-up strategy uses only a few epochs. We offer more discussions and experiments on the proposed variational warm-up strategy in Appendix F.

#### 4.3.2 Student update via VI

The student model \(q_{t}()\) is adapted by approximating using Eq. (5), and is optimized on:

\[L(q_{t})=L_{}(q_{t},_{t}^{})+( q_{t}||q_{0})+(1-)(q_{t}||_{t}),\] (15)

where \(_{t}^{}\) is the current augmented teacher model in Eq. (13), and \(p_{1}() q_{0}()\), \(_{t}()_{t}()\). The KL term between two Gaussians can be computed in a closed form.

#### 4.3.3 Teacher update via EMA

The teacher model is updated using EMA. Let \((,)\) and \((},})\) be the mean and standard deviation of the student and teacher model, respectively. At test time, the teacher model \(_{t}()\) is updated by

\[}}+(1-),}}+(1-).\] (16)

Although the std is not used in the cross entropy to compute the likelihood, the teacher prior distribution is important to adjust the student distribution via the KL term.

#### 4.3.4 Model inference

At any time, CTTA model needs to predict and adapt to the unlabeled test data. In our VCoTTA, we also use the mixed prior to serve as the inference model. That is, for a test data point \(x\), the model inference is represented by

\[p_{t}(x)= p(x|)p_{t}()d= p(x| )p_{1}()+(1-)p(x|)_{t}()d,\] (17)

For the data prediction, the model only uses the expectation to reduce the stochastic, but leverages stochastic dynamics in domain adaptation.

#### 4.3.5 The algorithm

We illustrate the whole algorithm in Algorithm 2. We first transform an off-the-shelf pretrained model into BNN via the variational warm-up strategy (Sec. 4.3.1). After that, we obtain a BNN, and for each domain shift, we forward and adapt each test data point in an MT architecture. For a data point \(x\), we first predict the class label using the mixture of the source model and the teacher model (Sec. 4.3.4). Then, we update the student model using VI, where we use cross entropy to compute the entropy term and use the mixture of priors for the KL term (Sec. 4.3.2). Finally, we update the BNN teacher model via EMA (Sec. 4.3.3). See more details in Appendix G. The process is feasible for any test data without labels.

## 5 Experiment

### Experimental Setting

**Dataset**. In our experiments, we employ the CIFAR10C, CIFAR100C, and ImageNetC datasets as benchmarks to assess the robustness of classification models. Each dataset comprises 15 distinct types of corruption, each applied at five different levels of severity (from 1 to 5). These corruptions are systematically applied to test images from the original CIFAR10 and CIFAR100 datasets, as well as validation images from the original ImageNet dataset. For simplicity in tables, we use C1 to C15 to represent the 15 types of corruption, _i.e._, C1: Gaussian, C2: Shot, C3: Impulse C4: Defocus, C5: Glass, C6: Motion, C7: Zoom, C8: Snow, C9: Frost, C10: Fog, C11: Brightness, C12: Contrast, C13: Elastic, C14: Pixelate, C15: Jpeg.

**Pretrained Model**. Following previous studies [50; 51], we adopt pretrained WideResNet-28  model for CIFAR10to-CIFAR10C, pretrained ResNeXt-29  for CIFAR100-to-CIFAR100C, and standard pretrained ResNet-50  for ImageNet-to-ImagenetC. Note in our VCoTTA , we further warm up the pretrained model to obtain the stochastic dynamics for each dataset. Similar to CoTTA, we update all the trainable parameters in all experiments. The augmentation number is set to 32 for all compared methods that use the augmentation strategy.

### Methods to be Compared

We compare our VCoTTA with multiple state-of-the-art (SOTA) methods. Source denotes the baseline pretrained model without any adaptation. BN [30; 43] keeps the network parameters frozen, but only updates Batch Normalization. TENT  updates via Shannon entropy for unlabeled test data. CoTTA  builds the MT structure and uses randomly restoring parameters to the source model. SATA  modifies the batch-norm affine parameters using source anchoring-based self-distillation to ensure the model incorporates knowledge of newly encountered domains while avoiding catastrophic forgetting. SWA  refines the pseudo-label learning process from the perspective of the instantaneous and long-term impact of noisy pseudo-labels. PETAL  tries to estimate the uncertainty in CTTA, which is similar to BNN, but it ignores the unreliable prior problem. All compared methods adopt the same backbone, pretrained model and hyperparameters.

   Method & C1 & C2 & C3 & C4 & C5 & C6 & C7 & C8 & C9 & C10 & C11 & C12 & C13 & C14 & C15 & Avg \\  Source & 72.3 & 65.7 & 72.9 & 46.9 & 54.3 & 34.8 & 42.0 & 25.1 & 41.3 & 26.0 & 9.3 & 46.7 & 26.6 & 58.5 & 30.3 & 43.5 \\ BN & 28.1 & 26.1 & 36.3 & 12.8 & 35.3 & 14.2 & 12.1 & 17.3 & 17.4 & 15.3 & 8.4 & 12.6 & 23.8 & 19.7 & 27.3 & 20.4 \\ Tent  & 24.8 & 20.6 & 28.5 & 15.1 & 31.7 & 17.0 & 15.6 & 18.3 & 18.3 & 18.1 & 11.0 & 16.8 & 23.9 & 18.6 & 23.9 & 20.1 \\ CoTTA  & 24.5 & 21.5 & 25.9 & 12.0 & 27.7 & 12.2 & 10.7 & 15.0 & 14.1 & 12.7 & 7.6 & 11.0 & 18.5 & 13.6 & 17.7 & 16.3 \\ RoRTA  & 30.3 & 25.4 & 34.6 & 18.3 & 34.0 & 14.7 & 11.0 & 16.4 & 14.6 & 14.0 & 8.0 & 12.4 & 20.3 & 16.8 & 19.4 & 19.3 \\ PETAL  & 23.7 & 21.4 & 26.1 & 28.8 & 12.4 & 10.4 & 14.8 & 13.9 & 12.6 & 7.4 & 10.6 & 18.3 & 13.1 & 17.1 & 16.2 \\ SATA  & 23.9 & 20.1 & 28.0 & 11.6 & 27.4 & 12.6 & 10.2 & 14.1 & 13.2 & 12.2 & 7.4 & 10.3 & 19.1 & 13.3 & 18.5 & 16.1 \\ DSS  & 24.1 & 21.3 & 25.4 & 11.7 & 26.9 & 12.2 & 10.5 & 14.5 & 14.1 & 12.5 & 7.8 & 10.8 & 18.0 & 13.1 & 17.3 & 16.0 \\ SWA  & 23.9 & 20.5 & 24.5 & 11.2 & 26.3 & 11.8 & 10.1 & 14.0 & 12.7 & 11.5 & 7.6 & 9.5 & 17.6 & 12.0 & 15.8 & 15.3 \\  VCoTTA (Ours) & **18.1** & **14.9** & **22.0** & **9.7** & **22.6** & **11.0** & **9.5** & **11.4** & **10.6** & **10.5** & **6.5** & **9.4** & **15.6** & **11.0** & **14.5** & **13.1** \\   

Table 1: Classification error rate (%) for the standard CIFAR10-to-CIFAR10C CTTA task. All results are evaluated with the largest corruption severity level 5 in an online fashion. C1 to C15 are 15 corruptions for the datasets (see Sec. 5.1). CIFAR100C and ImagenetC use the same setup.

   Method & C1 & C2 & C3 & C4 & C5 & C6 & C7 & C8 & C9 & C10 & C11 & C12 & C13 & C14 & C15 & Avg \\  Source & 73.0 & 68.0 & 39.4 & 29.3 & 54.1 & 30.8 & 28.8 & 39.5 & 45.8 & 50.3 & 29.5 & 55.1 & 37.2 & 74.7 & 41.2 & 46.4 \\ BN & 42.1 & 40.7 & 42.7 & 27.6 & 41.9 & 29.7 & 27.9 & 34.9 & 35.5 & 41.5 & 26.5 & 30.3 & 35.7 & 32.9 & 41.2 & 35.4 \\ Tent  & 37.2 & 35.8 & 41.7 & 37.9 & 51.2 & 48.3 & 48.5 & 58.4 & 63.7 & 71.1 & 70.4 & 82.3 & 88.0 & 88.5 & 90.4 & 60.9 \\ CoTTA  & 40.1 & 37.7 & 39.7 & 26.9 & 38.0 & 27.9 & 26.4 & 32.8 & 31.8 & 40.3 & 24.7 & 26.9 & 32.5 & 28.3 & 33.5 & 32.5 \\ RoRTA  & 49.1 & 44.9 & 45.5 & 30.2 & 42.7 & 29.5 & 26.1 & 32.2 & 30.7 & 37.5 & 24.7 & 26.9 & 32.5 & 28.3 & 33.5 & 32.5 \\ PETAL  & 38.3 & 36.4 & 38.6 & 25.9 & 36.8 & 27.3 & 25.4 & 32.0 & 30.8 & 38.7 & 24.4 & 26.4 & 31.5 & 26.9 & 32.5 & 31.5 \\ SATA  & 36.5 & 33.1 & **35.1** & 25.9 & 34.9 & 27.7 & 25.4 & 29.5 & 29.9 & 33.1 & 23.6 & 26.7 & 31.9 & 27.5 & 35.2 & 30.3 \\ DSS  & 39.7 & 36.0 & 37.2 & 26.3 & 35.6 & 27.5 & 25.1 & 31.4 & 30.0 & 37.8 & 24.2 & 26.0 & 30.0 & 26.3 & 31.1 & 30.9 \\ SWA  & 39.4 & 36.4 & 37.4 & 25.0 & 36.0 & 26.6 & 25.0 & 29.1 & 28.4 & 35.0 & 23.5 & 25.1 & 28.5 & 28.9 & 26.0 & 30.0 \\  VCoTTA (Ours) & **35.3** & **32.8** & 38.9 & **23.8** & **34.6** & **25.5** & **23.2** & **27.5** & **26.7** & **30.4** & **22.1** & **23.0** & **28.1** & **24.2** & 30.4 & **28.4** \\   

Table 2: Classification error rate (%) for the standard CIFAR100-to-CIFAR100C CTTA task.

### Comparison Results

We show the major comparisons with the SOTA methods in _Tables_ 1, 2 and 3. We have the following observations. First, no adaptation at the test time (Source) suffers from serious domain shift, which shows the necessity of the CTTA. Second, traditional TTA methods that ignore the continual shift in test time perform poorly such as TENT and BN. We also find that simple Shannon entropy is effective in the first several domain shifts, especially in complex 1,000-classes ImageNetC, but shows significant performance drops in the following shifts. Third, the mean-teacher structure is very useful in CTTA, such as CoTTA and PETAL, which means that the pseudo-label is useful in domain shift. In the previous method, the error accumulation leads to the unreliable pseudo labels, then the model may get more negative transfers in CTTA along the timeline. The proposed VCoTTA outperforms other methods on all the three datasets, such as 13.1% vs. 15.3% (SWA) on CIFAR10C, 28.4% vs. 30.0% (SWA) on CIFAR100C and 64.2% vs. 66.7% (CoTTA) on ImageNetC. We hold the opinion that the prior will inevitably drift in CTTA, but VCoTTA slows down the process via the prior mixture. We also find that the superiority is more obvious in the early adaptation, which may be influenced by the different corruption orders. We analyze the order problem in Appendix H.

### Ablation Study

We evaluate the two components in Table 4, _i.e._, the Variational Warm-Up (VWU) and the Symmetric Cross-Entropy (SCE) via ablation. The ablation results show that the two components are both important for VCoTTA. First, the VWU is used to inject stochastic dynamics into an off-the-shelf pretrained model. Without the VWU, the performance of VCoTTA drops to 18.4% from 13.9% on CIFAR10C, 31.5% from 28.8% on CIFAR100C and 68.1% from 64.2% on ImageNetC. Also, the SCE can further improve the performance on CIFAR10C and CIFAR100C, because SCE balances the gradient for high and low confidence predictions. We also find that SCE is ineffective for complex ImageNetC, and the reason may be the class sensitivity imbalance, causing the model to lean more towards one direction during optimization.

### Mixture of Priors

In Sec. 4.2, we introduce a Gaussian mixture strategy, where the current prior is approximated as the weighted sum of the source prior and the teacher prior. The weights are determined by computing the entropy over multiple augmentations of two models. To assess the effectiveness of these weights, we compare them with three naive weighting configurations: using only the source model, using only the teacher model, and a simple average with equal weights for both models. The results, as presented in Table 5, reveal that relying solely on the source model or the teacher model (i.e., weighting with \((1,0)\) and \((0,1)\)) results in suboptimal performance. Additionally, naive weighting with equal contributions from both models (i.e., \((0.5,0.5)\)) proves ineffective for CTTA due to the inherent uncertainty in both models. In contrast, the proposed adaptive weights for the Gaussian mixture in CTTA demonstrate its effectiveness. This underscores the significance of striking a balance between the two prior models in

   No. & VWU & SCE & CIFAR10C & CIFAR100C & ImageNetC \\ 
1 & & 18.4 & 31.5 & 68.1 \\
2 & & \(\) & 17.1 & 31.2 & 68.3 \\
3 & \(\) & & 13.9 & 28.8 & **64.2** \\
4 & \(\) & \(\) & **13.1** & **28.4** & 64.7 \\   

Table 4: Ablation study on under severity 5.

   Method & C1 & C2 & C3 & C4 & C5 & C6 & C7 & C8 & C9 & C10 & C11 & C12 & C13 & C14 & C15 & Avg \\  Source & 95.3 & 95.0 & 95.3 & 86.1 & 91.9 & 87.4 & 77.9 & 85.1 & 79.9 & 79.0 & 45.4 & 96.2 & 86.6 & 77.5 & 66.1 & 83.0 \\ BN & 87.7 & 87.4 & 87.8 & 88.0 & 87.7 & 78.3 & 63.9 & 67.4 & 70.3 & 54.7 & 36.4 & 88.7 & 58.0 & 56.6 & 67.0 & 72.0 \\ Tent  & 85.6 & 79.9 & **78.3** & **82.0** & **79.5** & 71.4 & 59.5 & 65.8 & 66.4 & 55.2 & 40.8 & 80.4 & 55.6 & 53.5 & 59.3 & 67.5 \\ CoTTA  & 87.4 & 86.0 & 84.5 & 85.9 & 83.9 & 74.3 & 62.6 & 63.2 & 63.6 & 51.9 & 38.4 & 72.7 & 50.4 & 45.4 & 50.2 & 66.7 \\ RoTTA  & 88.3 & 82.8 & 82.1 & 91.3 & 83.7 & 72.9 & 59.4 & 66.2 & 64.3 & 53.3 & **35.6** & 74.5 & 54.3 & 48.2 & 52.6 & 67.3 \\ PETAL  & 87.4 & 85.8 & 84.4 & 85.0 & 83.9 & 74.4 & 63.1 & 63.5 & 64.0 & 52.4 & 40.0 & 74.0 & 51.7 & 45.2 & 51.0 & 67.1 \\ DSS  & 84.6 & 80.4 & 78.7 & 83.9 & 79.8 & 74.9 & 62.9 & 62.8 & 62.9 & 49.7 & 37.4 & **71.0** & **49.5** & **42.9** & **48.2** & 64.6 \\  VCoTTA (Ours) & **81.8** & **78.9** & 80.0 & 83.4 & 81.4 & **70.8** & **60.3** & **61.1** & **61.7** & **46.4** & 35.7 & 71.7 & 50.1 & 47.1 & 52.9 & **64.2** \\   

Table 3: Classification error rate (%) for the standard ImageNet-to-ImageNetC CTTA task.

   No. & \(\) & \(1-\) & CIFAR10C & CIFAR100C & ImageNetC \\ 
1 & 1 & 0 & 17.4 & 35.0 & 69.9 \\
2 & 0 & 1 & 16.3 & 33.7 & 71.2 \\
3 & 0.5 & 0.5 & 14.7 & 31.3 & 67.0 \\
4 & Eq. (10) & **13.1** & **28.4** & **64.7** \\   

Table 5: Different weights for mixture of priors.

an unsupervised environment. The trade-off implies the need to discern when the source model's knowledge is more applicable and when the teacher model's shifting knowledge takes precedence.

### Uncertainty Estimation

To evaluate the uncertainty estimation, we use negative loglikelihood (NLL) and Brier Score (BS) . Both NLL and BS are proper scoring rules , and they are minimized if and only if the predicted distribution becomes identical to the actual distribution:

\[=-_{(x,y)^{}}(p(y|x, )),=_{(x,y)^{}}(p(y|x,)-(y))^{2},\]

where \(^{}\) denotes the test set, _i.e._, the unsupervised test dataset \(\) with labels. We evaluate NLL and BS with a severity level of 5 for all corruption types, and the compared results with SOTAs are shown in Table 6. We have the following observations. First, most methods suffer from low confidence in terms of NLL and BS because of the drift priors, where the model is unreliable gradually, and the error accumulation makes the model perform poorly. Our approach outperforms most other approaches in terms of NLL and BS, demonstrating the superiority in improving uncertainty estimation. We also find that PETAL  shows good NLL and BS, because PETAL forces the prediction over-confident to unreliable priors, thus PETAL shows unsatisfactory results on adaptation accuracy, such as 31.5% vs. 28.4% (Ours) on CIFAR100C.

### Gradually Corruption

We also show gradual corruption results instead of constant severity in the major comparison, and the results are reported in Table 7. Specifically, each corruption adopts the gradual changing sequence: \(1 2 3 4 5 4 3 2 1\), where the severity level is the lowest 1 when corruption type changes, therefore, the type change is gradual. The distribution shift within each type is also gradual. Under this situation, our VCoTTA also outperforms other methods, such as 8.9% vs. 10.5% (PETAL) on CIFAR10C, and 24.4% vs. 26.3% (COTTA) on CIFAR100C. The results show that the proposed VCoTTA based on BNN is also effective when the distribution change is uncertain.

## 6 Conclusion and Limitation

**Conclusion**: In this paper, we proposed a variational Bayesian inference approach, termed VCoTTA, to estimate uncertainties in CITA. At the pretrained stage, we first transformed an off-the-shelf pretrained deterministic CNN into a BNN using a variational warm-up strategy, thereby injecting uncertainty into the source model. At the test time, we implemented a mean-teacher update strategy, where the student model is updated via variational inference, while the teacher model is refined by the exponential moving average. Specifically, to update the student model, we proposed a novel approach that utilizes a mixture of priors from both the source and teacher models. Consequently, the ELBO can be formulated as the cross-entropy between the student and teacher models, combined with the KL divergence of the prior mixture. We demonstrated the effectiveness of the proposed method on three datasets, and the results show that the proposed method can mitigate the issue of unreliable prior within the CITA framework.

**Limitation**: The efficacy of the proposed method relies on injecting uncertainty into the model during the pre-training phase, which may be unavailable in scenarios where pretraining is already completed, and original data is inaccessible. Additionally, constructing and training BNN models are inherently more complex compared to CNNs, highlighting the importance of enhancing computational efficiency. The Gaussian mixture method relies on multiple data augmentations, which also incurs computational costs. Future endeavors could explore more efficient approaches for Gaussian mixture.

    & **CIFAR10C** & **CIFAR100C** & **ImageNetC** \\  & NLL & BS & NLL & BS & NLL & BS \\  Source & 3.0566 & 0.7478 & 2.4933 & 0.6707 & 5.0703 & 0.9460 \\ BN & 0.9988 & 0.3354 & 1.3932 & 0.4740 & 3.9971 & 0.8345 \\ Tent & 1.9391 & 0.3713 & 1.7097 & 1.0838 & 3.6902 & 0.8281 \\ CoTTA & 0.7192 & 0.2761 & 1.2907 & 0.4433 & 3.6235 & **0.7972** \\ PETAL & 0.5899 & 0.2458 & **1.2267** & 0.4327 & 3.6391 & 0.8017 \\  VCoTTA & **0.5421** & **0.2130** & 1.2287 & **0.4307** & **3.4469** & 0.8092 \\   

Table 6: Uncertainty estimation via NLL and BS.

  
**Method** & **CIFAR100C** & **CIFAR100C** & **ImageNetC** \\  & NLL & BS & NLL & BS \\  Source & 23.9 & 32.9 & 81.7 \\ BN & 13.5 & 29.7 & 54.1 \\ TENT & 39.1 & 72.7 & 53.7 \\ CoTTA & 10.6 & 26.3 & 42.1 \\ PETAL & 10.5 & 27.1 & 60.5 \\  VCoTTA & **8.9** & **24.4** & **39.9** \\   

Table 7: Gradually changing on severity 5.