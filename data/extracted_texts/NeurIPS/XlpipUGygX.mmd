# Amortized Planning with Large-Scale Transformers:

A Case Study on Chess

 Anian Ruoss\({}^{*1}\) Gregoire Deletang\({}^{*1}\) Sourabh Medapati\({}^{1}\) Jordi Grau-Moya\({}^{1}\) Li Kevin Wenliang\({}^{1}\) Elliot Catt\({}^{1}\) John Reid\({}^{1}\) Cannada A. Lewis\({}^{2}\) Joel Veness\({}^{1}\) Tim Genewein\({}^{1}\)

Equal contribution. \({}^{1}\)Google DeepMind. \({}^{2}\)Google. Correspondence to {anianr, timgen}@google.com.

###### Abstract

This paper uses chess, a landmark planning problem in AI, to assess transformers' performance on a planning task where memorization is futile -- even at a large scale. To this end, we release ChessBench, a large-scale benchmark dataset of \(10\) million chess games with legal move and value annotations (\(15\) billion data points) provided by Stockfish 16, the state-of-the-art chess engine. We train transformers with up to \(270\) million parameters on ChessBench via supervised learning and perform extensive ablations to assess the impact of dataset size, model size, architecture type, and different prediction targets (state-values, action-values, and behavioral cloning). Our largest models learn to predict action-values for novel boards quite accurately, implying highly non-trivial generalization. Despite performing no explicit search, our resulting chess policy solves challenging chess puzzles and achieves a surprisingly strong Lichess blitz Elo of 2895 against humans (grandmaster level). We also compare to Leela Chess Zero and AlphaZero (trained without supervision via self-play) with and without search. We show that, although a remarkably good approximation of Stockfish's search-based algorithm can be distilled into large-scale transformers via supervised learning, perfect distillation is still beyond reach, thus making ChessBench well-suited for future research.

## 1 Introduction

The ability to plan ahead and reason about long-term consequences of actions is a hallmark of rationality and human intelligence. Its replication in artificial systems has been a central goal of AI research since the field's inception. Perhaps the historically most famous planning problem in AI is chess, where naive search is computationally intractable and brute-force memorization is futile -- even at scale. Conceptually, the ability to plan and reason about consequences of actions is implemented via search algorithms and value computation [1; 2]. To implement such algorithms at scale, feed-forward neural networks have been playing an increasingly important role. For instance, DQN  and AlphaGo  popularized the use of neural value estimators to scale RL and (Monte Carlo) tree search. Similarly, today's strongest chess engines also employ a combination of search and amortized neural value estimation. For example, the state-of-the-art Leela Chess Zero [5; 6], which builds upon AlphaZero , augments Monte Carlo tree search with neural value predictions. Likewise, Stockfish 16, currently the strongest chess engine, uses efficiently updatable neural network evaluation , a highly specialized neural architecture to obtain fast evaluations. It is trained on value estimates of an earlier Stockfish version that uses human chess heuristics. All these chess engines tweak value-estimation networks towards fast evaluations that are to be combined with searchover many possible future move sequences. As a result, the value estimates themselves may not necessarily be optimal, which leaves open the question of how far amortized planning with modern neural networks can be pushed with scale and whether the resulting "searchless" engines can match engines that use search at test time, at least in principle (putting aside computational efficiency).

To scientifically address this question, we create _ChessBench_ (https://github.com/google-deepmind/searchless_ches), a large-scale chess dataset created from 10 million human games that we annotate with Stockfish 16. We use ChessBench to train transformers of up to \(270\) million parameters via supervised learning to predict action-values given a board-state. We also construct searchless chess policies from these predictors, where the playing strength depends entirely on the quality of the value predictions. We find that predictions by our largest trained models generalize well and non-trivially to novel board states. The resulting policies are capable of solving challenging chess puzzles and playing chess at a high level (grandmaster) against humans. Due to the combinatorial explosion of chess board states, virtually every new game involves many board states that were never seen during training (see Figure 1). This rules out memorization as a possible explanation and suggests that an approximate version of Stockfish's (search-based) value estimation algorithm can indeed be distilled into large transformers. Nonetheless, we also find that the performance gap cannot be fully closed, which may indicate that architectural innovations (or better optimization, data augmentation, etc.) might be needed instead of even larger scale.

**Our main contributions are:**

* We introduce ChessBench, a large-scale benchmark dataset for chess, consisting of \(530\)M board states (from \(10\)M games on lichess.org) annotated via Stockfish 16 with state-values and best-action, as well as \(15\)B action-values for all legal actions in each board state (corresponding to roughly 8864 days of unparallelized Stockfish evaluation time).
* without using explicit search at test time.

Figure 1: **Top** (Data annotation): We extract all boards from \(N\) randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. **Bottom left** (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has \(15.3\)B action-values. Drawing games i.i.d. from the game database for our test set leads to \(14.7\%\) of test boards appearing in the largest training set (mostly very early game states). We also use a test set of \(10\)K chess puzzles that come with a correct sequence of moves. **Bottom right** (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin \(z_{i}\{z_{1},,z_{K}\}\) the oracle value falls.

* We perform extensive ablations, including model- and dataset size, network architecture, action-value vs. state-value vs. behavioral cloning, and various hyper-parameters.
* We open source our ChessBench dataset, our model weights, and all training and evaluation code at https://github.com/google-deepmind/searchless_chess, and provide a series of benchmark results through our models, ablations, and comparisons with Stockfish 16, Leela Chess Zero, and AlphaZero, and their searchless policy/value networks.

## 2 Methodology

We now describe the dataset creation, the neural predictors and how to construct policies from them, and our evaluation methodology (see Figure 1 for an overview; full details in Appendix A).

### ChessBench Dataset

To construct a training dataset for supervised learning we downloaded 10 million games from Lichess (lichess.org) from February 20231. We extract all board states \(s\) from these games and estimate the state-value \(V^{}(s)\) for each state with Stockfish 16 (our "value oracle") using a time limit of 50ms per board (unbounded depth and maximum skill level). The value of a state is the win percentage estimated by Stockfish, lying between \(0\%\) and \(100\%\).2 Note that Stockfish does not provide a centipawn score when it detects a mate-in-\(k\), so we map all of these cases to a win percentage of \(100\)%. We also use Stockfish to estimate action-values \(Q^{}(s,a)\) for all legal actions \(a_{}(s)\) in each state. Here we use a time limit of 50ms per state-action pair (unbounded depth and max skill level), which corresponds to a Lichess blitz Elo of 2713 for our action-value oracle (see Section 3.1). The action-values (win percentages) also determine the oracle best action \(a^{}\):

\[a^{}(s)=*{arg\,max}_{a_{}(s)} Q^{}(s,a).\]

In the case where multiple moves are tied in value, we pick a maximizing action arbitrarily. Since we train on individual boards and not whole games we shuffle the dataset after annotation. For our largest training dataset, based on \(10\)M games, we thus obtain \(15.3\)B action-value estimates (or \( 530\)M state-value estimates and oracle best-actions; cf. Table A1) to train on (corresponding to roughly \(8864\) days of unparallelized Stockfish 16 evaluation time given the limit of 50ms per move).

To create our test dataset we follow the same annotation procedure, but on \(1\)K games downloaded from a different month (March 2023), resulting in \( 1.8\)M action-value estimates (or \( 62\)K state-value estimates and oracle best actions). Since there is only a small number of early-game board states and players often play popular openings, \(14.7\%\) of boards in this i.i.d. test are also in the training set. We deliberately chose not remove them, to not introduce a distributional shift and skew test set metrics. We also create a puzzle test set, following Carlini , consisting of \(10\)K challenging board states that have an Elo rating and a sequence of moves to solve the puzzle. Only \(1.33\%\) of the puzzle boards appear in the training set (i.e., the initial board states, not the complete solution sequences).

### Data Preprocessing and Training

Value binningThe predictors we train are discrete discriminators (i.e., classifiers). Therefore we convert the win percentages (i.e., the ground truth state- or action-values) into discrete "classes" via binning, akin to distributional RL . We divide the interval from \(0\%\) to \(100\%\) uniformly into \(K\) bins (non-overlapping sub-intervals) and assign a one-hot code to each bin \(z_{i}\{z_{1},,z_{K}\}\). If not mentioned otherwise, \(K=128\). For our behavioral cloning experiments we train to directly predict the oracle actions, which are already discrete. We ablate the number of bins \(K\) in Appendix B.1, and we investigate losses that do (i.e., the \(\) loss) and do not (i.e., the HL-Gauss and L2 loss) maintain semantic interconnectedness of the labels (i.e., the dependence between the classes) in Table 2.

TokenizationA FEN  string is a standard string-based description of a chess position. It consists of a board state, the current side to move, the castling availability for both players, a potential _en passant_ target, a half-move clock and a full-move counter, all represented in a single ASCII string. Our tokenization uses this representation, except that we flatten any uses of run-length encoding to obtain a fixed length (77) tokenized representation. Actions are stored in UCI notation , for example 'e2e4' for the popular opening move for white. To tokenize we use the index of a move into a lexicographically sorted array of the 1968 possible UCI encoded actions (see Appendix A.1 for more details). Note that this representation technically makes the game non-Markovian, because FENs do not contain the move history and therefore, cannot capture all information for rules such as drawing by threefold repetition (drawing because the same board occurs three times).

Network inputs and outputsFor all our predictors we use a modern decoder-only transformer backbone [13; 14; 15] to parameterize a categorical distribution by normalizing the transformer's outputs with a \(\) layer. The models thus output \(\) probabilities. The context size is \(79\) for action-value prediction and \(78\) for state-value prediction and behavioral cloning (see 'Tokenization' above). The output size is \(K\) (i.e., the number of bins) for action- and state-value prediction and \(1968\) (the number of all possible legal actions) for behavioral cloning. We use learned positional encodings  as the length of the input sequences is constant. Our largest model has \( 270\) million parameters, i.e., \(16\) layers, \(8\) heads, and an embedding dimension of \(1024\) (full details in Appendix A.2).

Training protocolWe train our predictors by minimizing the cross-entropy loss via mini-batch stochastic gradient descent using Adam . The target labels are either bin-indices for state- or action-value prediction or action indices for behavioral cloning. For state- and action-value prediction, we additionally apply label smoothing via the HL-Gauss loss , using a Gaussian smoothing distribution with the mean given by the label and a standard deviation of \(=0.75/K 0.05\) as recommended by Farebrother et al. . Thus, the labels are no longer one-hot but multi-hot encoded (see Figure 3 in  for an overview). We ablate the loss function, i.e., HL-Gauss vs. cross-entropy vs. MSE, in Section 3.4. We train for \(10\) million steps, which corresponds to \(2.67\) epochs for a batch size of \(4096\) with \(15.3\)B data points (cf. Table A1; details in Appendices A.2 and A.3).

### Predictors and Policies

Our predictors are categorical distributions parameterized by neural networks \(P_{}(z|x)\) that take a tokenized input \(x\) and output a predictive distribution over discrete labels \(\{z_{1},,z_{K}\}\). Depending on the prediction target we distinguish between three tasks (see Figure 1 for an overview).

(AV) Action-value predictionThe target label is the bin \(z_{i}\) into which the ground-truth action-value estimate \(Q^{}(s,a)\) falls. The input to the predictor is the concatenation of tokenized state and action. The loss for a single input data point \((s_{i},a_{i})\) is:

\[-_{z\{z_{1}, z_{K}\}}q_{i}(z) P_{}^{}(z|s_{i},a_{i})\ \ \ q_{i}:=_{K}(Q^{}(s,a)),\] (1)

where \(K\) is the number of bins and HL-Gauss\({}_{K}(x)\) is the function that computes \(q_{i}\) (the smoothed version of label \(z_{i}\)), i.e., a smooth categorical distribution using HL-Gauss  rather than the standard one-hot procedure. To use the predictor in a policy, we evaluate the predictor for all legal actions in the current state and pick the action with maximal expected action-value:

\[^{}(s)=*{arg\,max}_{a_{}}\ _{Z P_{}^{}(|s,a)}[Z]}_{_{ }(s,a)}.\]

(SV) State-value predictionThe target label is the bin \(z_{i}\) that the ground-truth state-value \(V^{}(s)\) falls into. The input to the predictor is the tokenized state. The loss for a single data point is:

\[-_{z\{z_{1} z_{K}\}}q_{i}(z) P_{}^{}(z|s_{i} )\ \ \ q_{i}:=_{K}(V^{}(s_{i})).\] (2)

To use the state-value predictor as a policy, we evaluate the predictor for all states \(s^{}=T(s,a)\) that are reachable via legal actions from the current state (where \(T(s,a)\) is the deterministic transition of taking action \(a\) in state \(s\)). Since \(s^{}\) implies that it is now the opponent's turn, the policy picks the action that leads to the state with the worst expected value for the opponent:

\[^{}(s)=*{arg\,min}_{a_{}}\ _{Z P_{}^{}(|s^{})}[Z]}_{ _{}(s^{})}.\](BC) Behavioral cloningThe target label is the (one-hot) action-index of the ground-truth action \(a^{}(s)\) within the set of all possible actions (see 'Tokenization' in Section 2.2). The input to the predictor is the tokenized state, which leads to the loss for a single data point:

\[- P_{}^{}(a^{}(s)|s).\] (3)

This gives a policy that picks the highest-probability action:

\[^{}(s)=*{arg\,max}_{a_{}}P_{}^{}(a|s).\]

### Evaluation

We use the following metrics to evaluate our models and/or measure training progress.

Action accuracyThe test set percentage where the policy picks the best action: \((s)=a^{}(s)\).

Action ranking (Kendall's \(\))The average test set rank correlation  of the predicted action distribution with the ground truth given by Stockfish. Kendall's \(\) ranges from -1 (exact inverse order) to 1 (exact same order), with 0 indicating no rank correlation. The ranking is obtained by evaluating for all legal actions \(_{}(s,a)\), \(-_{}(T(s,a))\), or \(P_{}^{}(a|s)\), in the case of AV, SV, or BC prediction, respectively. The ground-truth ranking is given by evaluating \(Q^{}(s,a)\) for all legal actions.

Puzzle accuracyWe evaluate our policies on their capability of solving puzzles from a collection of Lichess puzzles that are rated by Elo difficulty from \(399\) to \(2867\) based on how often each puzzle has been solved correctly. We use _puzzle accuracy_ as the percentage of puzzles where the policy's action sequence exactly matches the _entire_ solution action sequence. For our main results in Sections 3.1 and 3.2 we use 10K puzzles; otherwise, we use the first 1K puzzles to speed up evaluation.

Game playing strength (Elo)We evaluate the playing strength (measured as an Elo rating) of the policies in two ways: (i) we run an internal tournament between all the agents from Table 1 except for GPT-3.5-turbo-instruct, and (ii) we play Blitz games on Lichess against either only humans or only bots. For the tournament we play \(400\) games per agent pair, yielding \(22\)K games in total, and compute the Elo with BayesElo  using the default confidence parameter of \(0.5\). To ensure variability in the games, we use the openings from the Encyclopaedia of Chess Openenings (ECO) . When playing on Lichess, we use a \(*{softmax}\) policy with a low temperature of \(0.005\) for the first five full-moves instead of the \(*{arg\,max}\) policy to create variety in games and prevent simple exploits via repeated play. We anchor the relative BayesElo values to the Lichess Elo vs. bots of our largest (\(270\)M) model.

### Engine Comparisons

We compare the performance of our models against the following engines:

Stockfish 16We consider two variants: (i) a \(50\)ms time limit _per legal move_ (i.e., the dataset oracle), and (ii) a \(1.5\)s limit _per board_, which is roughly the amount of time the first variant (i) takes on average per board (i.e., there are roughly \(30\) legal moves per board on average).

AlphaZeroWe consider three variants of AlphaZero : (i) with 400 MCTS simulations, (ii) only the policy network, and (iii) only the value network (where (ii) and (iii) perform no additional search). AlphaZero's networks have \(27.6\)M parameters and are trained on \(44\)M games (full details in ).

Leela Chess ZeroWe consider three variants: (i) with 400 MCTS simulations, (ii) only the policy network, and (iii) only the value network (where (ii) and (iii) perform no additional search). We use the T82 network, which is a convolutional network with 768 filters, 15 blocks, and mish activations  - the largest network available on the official Leela Chess Zero website. At the time of writing, the precise network architecture, training data, and training protocol for the T82 network were not reported on the Leela Chess Zero website or other source. Concurrently to our paper, Monroe and Leela Chess Zero Team  published a rigorous tech report which now provides many of these details about for a slightly different (state-of-the-art) Leela Zero architecture called ChessFormer, and includes a comparison against our vanilla networks.

GPT-3.5-turbo-instructWe follow Carlini  and encode the entire game with the Portable Game Notation (PGN)  to reduce hallucinations. Since Carlini  found that (at that time) GPT-4 struggled to play full games without making illegal moves, we do not consider GPT-4.

In contrast to our models, all of the above engines rely on the entire game's history (via the PGN; we use the FEN, which only contains the game's current state and very limited historical information). Observing the full game's history helps, for instance, detecting and preventing draws from threefold repetition (games are drawn if the same board state appears three times throughout the game). Our engines require a workaround to deal with this problem (described in Section 4).

A direct comparison between all engines comes with a lot of caveats since some engines use the game history, some have very different training protocols (i.e., RL via self-play instead of supervised learning), and some use search at test time. We show these comparisons to situate the performance of our models within the wider landscape, but emphasize that some conclusions can only be drawn within our family of models and the corresponding ablations that keep all other factors fixed.

## 3 Case-Study Results

We use two settings for our experiments: (i) a large-scale setting for our main results (Sections 3.1 and 3.2; details in Appendix A.2), and (ii) a setting geared towards getting representative results with better computational efficiency for our ablations (Sections 3.3 and 3.4; details in Appendix A.3).

### Main Result

Table 1 shows the playing strength (internal tournament Elo, external Lichess Elo, and puzzle accuracy) of our large-scale transformers trained on the full (\(10\)M games) training set. We compare models with \(9\)M, \(136\)M, and \(270\)M parameters (none of them overfit the training set as shown in Appendix B.4). The results show that all three models exhibit non-trivial generalization to novel boards and can successfully solve a large fraction of puzzles. Across all metrics, increasing model size consistently improves the scores, confirming that model scale matters for strong chess performance. Our largest model achieves a blitz Elo of \(2895\) against human players, which places it into grandmaster territory. However, the Elo drops when playing against bots on Lichess, which may be a result of having a significantly different player pool , minor technical issues, or a qualitative difference in how bots exploit weaknesses compared to humans (see Section 4 for a detailed discussion).

    & & & &  \\
**Agent** & **Train** & **Search** & **Input** & **Tournament Elo** & **vs. Bots** & **vs. Humans** & **Puzzle Acc. (\%)** \\ 
9M Transformer (ours) & SL & FEN & \(2025\)\(\)\(18\) & \(2054\) & - & 88.9 \\
136M Transformer (ours) & SL & FEN & \(2259\)\(\)\(16\) & \(2156\) & - & 94.5 \\
270M Transformer (ours) & SL & FEN & \(2299\)\(\)\(15\) & \(2299\) & \(2895\) & 95.4 \\  GPT-3.5-turbo-instruct & SSL & PGN & - & \(1755\) & - & 66.5 \\ AlphaZero (policy net only) & RL & PGN & \(1777\)\(\)\(25\) & - & - & 56.1 \\ AlphaZero (value net only) & RL & PGN & \(1992\)\(\)\(19\) & - & - & 82.0 \\ AlphaZero (400 MCTS sim.) & RL & ✓ & PGN & \(2470\)\(\)\(16\) & - & - & 95.6 \\  Leela Chess Zero (policy net only) & RL & PGN & \(2292\)\(\)\(16\) & \(2224\) & - & 88.6 \\ Leela Chess Zero (value net only) & RL & PGN & \(2418\)\(\)\(16\) & \(2318\) & - & 95.9 \\ Leela Chess Zero (400 MCTS sim.) & RL & ✓ & PGN & \(2858\)\(\)\(20\) & \(2620\) & - & 99.6 \\  Stockfish \(16\) (\(50\)ms per move) [oracle] & SL & ✓ & FEN + Moves & \(2711\)\(\)\(18\) & \(2713\) & - & 99.8 \\ Stockfish \(16\) (\(1.5\)s per board) & SL & ✓ & FEN + Moves & \(2935\)\(\)\(23\) & \(2940\) & - & 100.0 \\   

Table 1: Comparison of our action-value models against Stockfish 16, variants of Leela Chess Zero and AlphaZero (with and without Monte Carlo tree search), and GPT-3.5-turbo-instruct. Tournament Elo ratings are obtained by making the agents play against each other and cannot be directly compared to the Lichess Elo. Lichess (blitz) Elo ratings result from playing against either human opponents or bots on Lichess. Stockfish 16 with a time limit or 50ms per move is our data-generating oracle. Models operating on the PGN observe the full move history, whereas FENs only contain very limited historical information (sufficient for the fifty-move rule). Unlike all other engines, our policies were trained with supervised learning and use no explicit search at test time (except for GPT-3.5-turbo-instruct, which was trained via self-supervised learning and then instruction tuned).

### Puzzles

In Figure 2 we compare the puzzle performance of our 270M parameter model against Stockfish 16 (50ms limit per move), GPT-3.5-turbo-instruct, AlphaZero, and Leela Chess Zero. We use our large puzzle set of \(10\)K puzzles, grouped by their assigned Elo difficulty from Lichess. Stockfish 16 performs the best across all difficulty categories, followed by Leela Chess Zero, AlphaZero, and our model. Impressively, our model, which does not use any explicit search at test time, nearly matches the performance of AlphaZero with (Monte Carlo) tree search. GPT-3.5-turbo-instruct achieves non-trivial puzzle performance but significantly lags behind our model. We emphasize that solving the puzzles requires a correct move _sequence_, and since our policy cannot explicitly plan ahead, solving the puzzle sequences relies entirely on having good value estimates that can be used greedily.

### Scaling Analysis

Figure 3 shows a scaling analysis over the dataset and model size. We visualize the puzzle accuracy (train and test losses in Figure A3), which correlates well with the other metrics and the overall playing strength. For small training set size (\(10\)K games, left panel) larger architectures (\( 7\)M) start to overfit. This effect disappears as the dataset size is increased to \(100\)K (middle panel) and \(1\)M games (right panel). The results also show that the final accuracy of a model increases as the dataset size is increased (consistently across model sizes). Similarly, we observe the general trend of increased architecture size leading to increased overall performance (as in our main result in Section 3.1).

### Variants and Ablations

We perform extensive ablations using the \(9\)M-parameter model and show the results in Tables 2 and A2. We use the results and conclusions drawn to inform and justify our design choices and determine the default model-, data-, and training configurations.

Figure 3: Puzzle accuracy for different training set sizes (stated above panels) and model sizes (color-coded), evaluated on our small set of \(1\)K puzzles. Generally, larger models trained on larger datasets lead to higher accuracy (which strongly correlates with test set performance and general chess playing strength), highlighting the importance of scale for strong chess play. This effect cannot be explained by memorization since \(<1.41\%\) of the initial puzzle board states appear in our training set. If the model is too large in relation to the training set it overfits (left panel; loss curves in Figure A3).

Figure 2: Puzzle solving comparison for our 270M transformer, Stockfish 16 (50ms per move), Leela Chess Zero, AlphaZero, and GPT-3.5-turbo-instruct on \(10\)K Lichess puzzles (curated following ).

Predictor targetsBy default we learn to predict action-values given a board state. Here we compare against using state-values or oracle actions (behavioral cloning) as the prediction targets (Section 2.3 and Figure 1 describe how to construct policies from the predictors). Table 2 and Figure A4 show that the action-value predictor is superior in terms of action-ranking (Kendall's \(\)) action accuracy, and puzzle accuracy. This superior performance might stem primarily from the significantly larger action-value dataset (\(15.3\)B state-action pairs vs. \( 530\)M states for our largest training set). We thus run an ablation where we train all three predictors on the same amount of data (results in Table A4 and Figure A5, which largely confirm this hypothesis). Appendix B.5 contains detailed discussion, including why the performance discrepancy between behavioral cloning and the state-value prediction policy may be largely due to training only on expert actions rather than the full action distribution.

Loss functionWe treat learning Stockfish action-values as a classification problem and train by minimizing the HL-Gauss loss . Here we compare this to the cross-entropy loss (log-loss), which is as close as possible to the (tried and tested) standard LLM setup. Another alternative is to treat the problem as a scalar regression problem by parameterizing a fixed-variance Gaussian likelihood model with a transformer and performing maximum (log) likelihood estimation, i.e., minimizing the mean-squared error (L2 loss). To that end, we modify the architecture to output a scalar (without a final log-layer or similar). Table 2 shows that the HL-Gauss loss outperforms the other two losses.

Network depthTable 2 shows the influence of increasing the transformer's depth (i.e., the number of layers) for a fixed number of parameters (we vary the embedding dimension and the widening factor such that all models have the same number of parameters). Since transformers may learn to roll out iterative computation (which arises in search) across layers, deeper networks may hold the potential for deeper unrolls. The performance of our models increases with their depth but saturates at around 16 layers, indicating that depth is important, but not beyond a certain point.

Value binningTable 2 shows the impact of varying the number of bins used for state- and action-value discretization (from \(16\) to \(256\)), demonstrating that more bins generally lead to improved performance (up to a certain point). We therefore use \(K=128\) bins for our experiments.

## 4 Discussion

When using our state-based policies to play against humans and bots, two minor technical issues appear that can only be solved by having (some) access to the game's history. In this section, we discuss both issues and present our corresponding workarounds.

Blindness to threefold repetitionBy construction, our state-based predictor cannot detect the risk of threefold repetition (drawing because the same board occurs three times), since it has no access to the game's history (FENs contain minimal historical info, sufficient for the fifty-move rule). To reduce draws from threefold repetitions, we check if the agent's next move would trigger the rule and set the corresponding action's win percentage to \(50\%\) before computing the \(\). However, our agents still cannot plan ahead to mimize the risk of being forced into threefold repetition.

    &  \\   & **Parameter** & **Puzzles** & **Actions** & **Kendall’s \(\)** \\   & Action-Value & **83.3** & **63.0** & **0.259** \\  & State-Value & 77.5 & 58.5 & 0.215 \\  & Behavioral Cloning & 65.7 & 56.7 & 0.116 \\   & HL-Gauss (class.) & **82.0** & 61.8 & **0.257** \\  & log (class.) & 80.6 & **61.9** & **0.257** \\  & L2 (regr.) & 80.8 & 58.9 & 0.240 \\   & 2 & 54.7 & 51.5 & 0.209 \\  & 4 & 40.5 & 44.3 & 0.179 \\  & 8 & 79.5 & 60.7 & 0.252 \\  & 16 & **81.3** & **61.6** & **0.256** \\  & 32 & 79.5 & 61.2 & 0.254 \\   & 16 & 83.0 & 61.4 & 0.248 \\  & 32 & 83.0 & 63.2 & 0.261 \\   & 64 & **84.4** & 63.1 & 0.259 \\   & 128 & 83.8 & **63.4** & **0.262** \\   & 256 & 83.7 & 63.0 & 0.260 \\   

Table 2: Ablating the predictor target, loss function, network depth, and number of value bins (see Section 3.4). The best configurations are: action-value prediction (see Appendix B.5 for a detailed discussion), HL-Gauss loss, depth 16, and 128 bins. We conduct further ablations (over the data sampler, the Stockfish time limit, and the model architecture) in Appendix B.1 and Table A2.

Indecisiveness in the face of overwhelming victoryIf Stockfish detects a mate-in-\(k\) it outputs \(k\) and not a centipawn score. When annotating our dataset, we map all such outputs to a win percentage of \(100\%\). Similarly, in a very strong position, several actions may end up in the maximum value bin. Thus our agent sometimes plays somewhat randomly rather than committing to a plan that finishes the game (the agent has no knowledge of its past moves). Paradoxically, this means that our agent, despite being in a position of overwhelming win percentage, sometimes fails to take the (virtually) guaranteed win (see Figure 4) and might draw or even end up losing since small chances of a mistake accumulate with longer games. To alleviate this, we check whether the predicted scores for all top five moves lie above a win percentage of \(99\%\) and double-check this condition with Stockfish, and if so, use Stockfish's top move (out of these) to have consistency in strategy across time-steps.

Elo: Humans vs. botsWe have three plausible hypotheses for why the Lichess Elo in Table 1 differs when playing against humans vs. bots: (i) humans tend to resign when our bot is in an overwhelmingly winning position but bots do not (i.e., the previously described problem gets amplified against bots); (ii) most humans on Lichess rarely play against bots, i.e., the two player pools (humans and bots) are hard to compare and their Elo ratings may be miscalibrated ; and (iii) based on anecdotal analysis by a chess National Master, our models make the occasional tactical mistake which may be penalized more severely by bots than humans (analysis in Appendices B.7 and B.8). While investigating this Elo discrepancy is interesting, it is not central to our paper and does not impact our main claims.

### Limitations

Our primary goal was to investigate whether a complex search algorithm such as Stockfish 16 can be approximated with a feedforward neural network on our dataset via supervised learning. While our largest model achieves good performance, it does not fully close the gap to Stockfish 16, and it is unclear whether further scaling would close this gap or whether other innovations are needed.

While we produced a strong chess policy, our goal was not to build a state-of-the-art chess engine. Our models are impractical in terms of speed and would perform poorly in computer chess tournaments with compute limitations. Therefore, we calibrated our policy's playing strength via Liches, where the claim of "grandmaster-level" play currently holds only against human opponents. In addition, we only evaluated our biggest model against humans on Lichess due to the extensive amount of time required. We also cannot rule out that opponents, through extensive repeated play, may find weaknesses due to the deterministic nature of our policy. Finally, we reiterate that we compare to other engines to situate our models within the wider landscape, but that a direct comparison between all the engines comes with a lot of caveats due to differences in their inputs (FENs vs. PGNs), training protocols (RL vs. supervised learning), and the use of search at test time.

Leela Chess Zero's networks, which are trained with self-play and RL, achieve higher Elo ratings without using explicit search at test time than our transformers, which we trained via supervised learning. However, in contrast to our work, very strong chess performance (at low computational cost) is the explicit goal of this open source project (which they have clearly achieved via domain-specific adaptations). We refer interested readers to  (which was published concurrently to our work) for details on the current state-of-the-art and a comparison against our networks.

Figure 4: Two options to win the game in \(3\) or \(5\) moves, respectively (more options exist). Since they both map into the highest-value bin our bot ignores Nh6+, the fastest way to win (in 3), and instead plays Nh6+ (mate-in-5). Unfortunately, a state-based predictor without explicit search cannot guarantee that it will continue playing the Nh6+ strategy and thus might randomly alternate between different strategies. Overall this increases the risk of drawing the game or losing due to a subsequent (low-probability) mistake, such as a bad \(\) sample. Board from a game between our \(9\)M Transformer (white) and a human (blitz Elo of \(2145\)).

Related Work

Unsurprisingly given its long history in AI, there is a vast literature on applying algorithmic techniques to chess. Earlier works predominantly focused on methods which would increase the strength of chess playing entities, but as time has progressed, the role of chess as a problem domain has changed more from a challenge area to that of an illuminating benchmark, and our work continues this tradition.

Early computer chess research focused on designing explicit search strategies coupled with heuristics, as evidenced by Turing's initial explorations  and implementations like NeuroChess . This culminated in systems like Deep Blue  and early versions of Stockfish .

AlphaZero  and Leela Chess Zero  marked a paradigm shift: They employed deep RL with Monte Carlo tree search to learn heuristics (policy and value networks) instead of manually designing them [30; 31]. Several works built upon this framework , including enhancements to AlphaZero's self-play mechanisms  and the use of model-free RL . At the time of writing, Leela Zero's T82 networks were state-of-the art, but their training protocol, training data, and some architectural details were not fully reported. In the meantime, Monroe and Leela Chess Zero Team  have published a detailed tech report on another transformer-based Leela Zero architecture called the ChessFormer. Their comparison shows that ChessFormers comparable in size to our models outperform our vanilla transformers while requiring fewer FLOPS thanks to clever domain-specific adaptations.

Another line of work moved away from explicit search methods by leveraging large-scale game datasets for (un)supervised learning, both for chess [34; 35; 36; 37] and Go . Most closely related to our work, project Maia  used behavioral cloning on 12M human games but, rather than maximizing performance (i.e., distilling Stockfish), focused on predicting human moves at different levels of play.

The rise of large (pretrained) foundation models also led to innovations in various areas of computer chess research: learning the rules of chess [39; 40], evaluating move quality , evaluating playing strength [9; 42], state tracking [43; 44], and playing chess from visual inputs . Fine-tuning on chess-specific data sources (e.g., chess textbooks) has further improved performance [46; 47; 48].

## 6 Conclusion

Our paper introduces ChessBench, a large-scale, open source benchmark dataset for chess, and shows the feasibility of distilling an approximation of Stockfish 16, a complex planning algorithm, into a feed-forward transformer via standard supervised training. The resulting predictor generalizes well to unseen board states, and, when used in a policy, leads to strong chess play. We show that strong chess capabilities from supervised learning only emerge at sufficient dataset and model scale. Our work thus adds to a rapidly growing body of literature showing that sophisticated algorithms can be distilled into feed-forward transformers, implying a paradigm shift to viewing large transformers as a powerful technique for general algorithm approximation rather than "mere" statistical pattern recognizers. Nevertheless, perfect distillation of Stockfish 16 is still beyond reach and closing the performance gap might need other (e.g., architectural) innovations. Our open source benchmark dataset, ChessBench, thus provides a solid basis for scientific comparison of any such developments.

## 7 Impact Statement

While the results of training transformer-based architectures at scale in a (self-)supervised way will have significant societal consequences in the near future, these concerns do not apply to closed domains, such as chess, that have limited real-world impact. Moreover, chess has been a domain of machine superiority for decades. Another advantage of supervised training on a single task over other forms of training (particularly self-play or reinforcement learning and meta-learning) is that the method requires a strong oracle solution to begin with (for data annotation) and is unlikely to significantly outperform the oracle, which means that the potential for the method to rapidly introduce substantial unknown capabilities (with wide societal impacts) is very limited.