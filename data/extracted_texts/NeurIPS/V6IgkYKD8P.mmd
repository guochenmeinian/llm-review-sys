# SEEDS: Exponential SDE Solvers for Fast

High-Quality Sampling from Diffusion Models

 Martin Gonzalez

IRT SystemX

Nelson Fernandez

Air Liquide

Thuy Tran

IRT SystemX

Elies Gherbi

IRT SystemX

Hatem Hajri

IRT SystemX

& Safran

& Nader Masmoudi

New York University

Corresponding author: martin.gonzalez@irt-systemx.fr

###### Abstract

A potent class of generative models known as Diffusion Probabilistic Models (DPMs) has become prominent. A forward diffusion process adds gradually noise to data, while a model learns to gradually denoise. Sampling from pre-trained DPMs is obtained by solving differential equations (DE) defined by the learnt model, a process which has shown to be prohibitively slow. Numerous efforts on speeding-up this process have consisted on crafting powerful ODE solvers. Despite being quick, such solvers do not usually reach the optimal quality achieved by available slow SDE solvers. Our goal is to propose SDE solvers that reach optimal quality without requiring several hundreds or thousands of NFEs to achieve that goal. We propose Stochastic Explicit Exponential Derivative-free Solvers (SEEDS), improving and generalizing Exponential Integrator approaches to the stochastic case on several frameworks. After carefully analyzing the formulation of exact solutions of diffusion SDEs, we craft SEEDS to analytically compute the linear part of such solutions. Inspired by the Exponential Time-Differencing method, SEEDS use a novel treatment of the stochastic components of solutions, enabling the analytical computation of their variance, and contains high-order terms allowing to reach optimal quality sampling \( 3\)-\(5\) faster than previous SDE methods. We validate our approach on several image generation benchmarks, showing that SEEDS outperform or are competitive with previous SDE solvers. Contrary to the latter, SEEDS are derivative and training free, and we fully prove strong convergence guarantees for them. Our code is publicly available in this link.

## 1 Introduction

Diffusion Probabilistic Models (DPMs) [32; 12] have emerged as a powerful category of generative models and have proven to quickly become SOTA for generative tasks such as image, video, audio generation [8; 24; 13; 3; 4], and more [29; 36]. These models employ a forward diffusion process where noise is gradually added to the data, and the model learns to remove the noise progressively. However, sampling from most pre-trained DPMs is done by simulating the trajectories of associated differential equations (DE) and has been found to be prohibitively slow . Previous attempts to accelerate this process have mainly focused on developing efficient ODE solvers. On one hand, training-based methods speed-up sampling by using auxiliary training such as Progressive Distillation  and Fourier Neural Operators , learning the noise schedule, scaling, variance, or trajectories. On the other hand, training-free methods [16; 22; 15; 40; 23] are slower but are more versatile forbeing employed on different models and achieve higher quality results than current training-based methods. Although these solvers are fast, they often fall short of achieving the optimal quality attained by slower SDE solvers . The latter usually do not present theoretical convergence guarantees and, while being training-free, they often still require costly parameter optimization to achieve optimal results which might be difficult to estimate for large datasets.

Our objective is to introduce SDE solvers that can achieve optimal quality without requiring an excessively large number of function evaluations (NFEs). To accomplish this, we propose Stochastic Explicit Exponential Derivative-free Solvers (SEEDS). These are _off-the-shelf_ SDE samplers: they offer promising high-quality sampling without further training or parameter optimization. SEEDS enhance and generalize existing Exponential Integrator [22; 41; 23] approaches to the stochastic case on several practical frameworks and is based on the following 4 building blocks: (1) the exponential representation of semi-linear SDE exact solutions which isolate linear terms to be computed analytically; (2) a general change-of-variables recipe to simplify the integrals involved in the solutions in order to better approximate the deterministic one; (3) a method to analytically compute the variance of the stochastic one; (4) a method to decompose the obtained stochastic components in such a way that the resulting sequences of higher-stage numerical approximations yield Markov chains.

Overall, we make the following contributions: (a) our change-of-variables method allow us to re-frame the gDDIM solver  as a special case of SEEDS and to craft bespoke solvers for the EDM-preconditioned DPMs in  which attain equivalent sampling quality twice faster than the previous SOTA sampling method ; (b) based on the Stochastic Exponential Time-Differencing method, we analytically compute of our solver's stochastic components in terms of the so-called \(\)-functions, allowing for efficient implementation; (c) our noise decomposition method (4), which is the key of success of SEEDS, is both theoretically grounded, experimentally shown to be optimal, and has no deterministic equivalent; (d) we provide full proofs of strong/weak convergence guarantees for our SDE solvers which, to our knowledge, has no precedent in the DPM literature. In particular, although the formula used for the truncated Ito-Taylor expansion might seem similar to that of , our convergence theorems leverage the full Ito-Taylor expansion of solutions, making our proofs not incremental and different from ; (e) we conduct extensive experiments demonstrating that SEEDS establishes SOTA results among available solvers on several image generation benchmarks, or is competitive with existing SDE solvers while being 2-5 times faster than the latter.

Although our solvers theoretically apply to certain non-isotropic DPMs such as Critically-damped Langevin Dynamics (CLD)  (see Prop. 4.5 and Rem. 4.6), we will restrict our presentation to the isotropic case for which many notations become simpler.

## 2 Background on Diffusion Probabilistic Models

General Isotropic DE Formulation.The evolution of a data sample \(_{0}^{d}\) taken from an unknown data distribution \(p_{}\) into standard Gaussian noise can be defined as a forward diffusion process \(\{_{t}\}_{t[0,T]}\), with \(T>0\), which is a solution to a linear SDE:

\[_{t}=f(t)_{t}t+g(t) _{t}, f(t):=_{t}}{t}, g(t)=_{t}[_{t}^{2}]}{t}},\] (1)

where \(f(t),g(t)^{d d}\) are called the drift and diffusion coefficients respectively and \(\) is a \(d\)-dimensional standard Wiener process, and \(_{t},_{t}^{>0}\) are differentiable functions with bounded derivatives. In practice, when specifying the SDE (1), \(_{t}\) acts as a schedule controlling the noise levels of an input at time \(t\), and \(_{t}\) as a time-dependent signal scaling controlling its dynamic range.

By denoting \(p_{t}(_{t})\) the marginal distribution of \(_{t}\) at time \(t\), functions \(_{t}\) and \(_{t}\) are designed so that the end-time distribution of the process process is \(p_{T}(_{T})(_{T}|, ^{2}_{d})\) for some \(>0\). As (1) is linear, the transition probability \(p_{0t}(_{t}|_{0})\) from \(_{0}\) to \(_{t}\) is Gaussian whose mean and variance can be expressed in terms of \(_{t}\) and \(_{t}\). For simplicity, we will denote it

\[p_{0t}(_{t}|_{0})=(_{t};_{t} _{0},_{t}),_{t},_{t}^{d d}.\]

The evolution of the reverse time process of \(\{_{t}\}_{t[0,T]}\) (which we will still denote \(\{_{t}\}_{t[0,T]}\) for simplicity) is then driven by a backward differential equation

\[_{t}=[f(t)_{t}-}{2}g^{2}(t) _{_{t}} p_{t}(_{t})]t+ g(t) _{t},\] (2)where \(t\) are negative infinitesimal time-steps and \(_{t}\) is now a Wiener process with variance \(-t\). In this article, we will concentrate in the cases \(=0,1\), known in the literature as the Probability Flow ODE (PPO) and diffusion reverse SDE (RSDE), respectively.

Training.Denoising score-matching is a technique to train a time-dependent model \(D_{}(_{t},t)\) to approach the score function \(_{_{t}} p_{t}(_{t})\) at each time \(t\). Intuitively, as \(D_{}\) approaches the score, it produces a sample which maximizes the log-likelihood. As such, this model is coined as a _data prediction_ model. However, in practice DPDs can be more efficiently trained by reparameterizing \(D_{}\) into a different model \(F_{}(_{t},t)\) whose objective is to predict the noise to be removed from a sample at time \(t\). This _noise prediction_ model is trained by means of the loss

\[_{t[0,T],_{0} p_{}, (,_{})}[\|-F_{ }(_{t}_{0}+_{t},t)\|^{2}_{_{t}^{-1}_{t }_{t}^{-}}],\]

where \(_{t}\) is a time dependent weighting parameter and \(_{t}_{t}^{}=_{t}\).

## 3 Accelerating Optimal Quality Solvers for Diffusion SDEs

Once \(F_{}\) or \(D_{}\) have been trained, one can effectively solve (2) after replacing the score function by its corresponding expression involving either one of these models. For instance, taking the noise prediction model and \(=1\), sampling is conducted by simulating trajectories of a SDE of the form

\[_{t}=[A(t)_{t}+b(t)F_{}(_{t},t) ]t+g(t)_{t},\] (3)

for some functions \(A(t),b(t)\) which are usually not equal to \(f(t),g^{2}(t)\). In what follows, we consider a time discretization \(\{t_{i}\}_{i=0}^{M}\) going backwards in time starting from \(t_{0}=T\) to \(t_{M}=0\) and to ease the notation we will always denote \(t<s\) for two consecutive time-steps \(t_{i+1}<t_{i}\).

The usual representation of the analytic solution \(_{t}\) at time \(t\) of (3) w.r.t. an initial condition \(_{s}\) is:

\[_{t}=_{s}+_{s}^{t}[A()_{}+b()F_ {}(_{},)]+_{s}^{t}g() _{}.\] (4)

The numerical schemes we propose for approaching the trajectories of (3) based on representation (4) are grounded in the 4 following principles:

1. The variation-of-parameters formula: representing analytic solutions with linear term extracted from the integrand;
2. Exponentially weighted integrals: extracting the time-varying linear coefficient attached to the network from the integrand by means of a specific choice of change of variables which allows analytic computation of the leading coefficients in the truncated Ito-Taylor expansion associated to \(F_{}(_{},)\) up to any arbitrary order;
3. Modified Gaussian increments: after replicating such change of variables onto the stochastic integral, analytically computing its variance.
4. Markov-preserving noise decomposition: stochastic integrals need to be dependent on overlapping time intervals and independent on non-overlapping ones.

Exponential representation of exact solutions of diffusion SDEs.The first key insight of this work is that, using the _variation-of-parameters_ formula, we can represent the analytic solution \(_{t}\) at time \(t\) of (3) with respect to an initial condition \(_{s}\) as follows:

\[_{t}=_{A}(t,s)_{s}+_{s}^{t}_{A}(t,)b() F_{}(_{},)+_{s}^{t}_{A}(t,)g( )_{},\] (5)

where \(_{A}(t,s)=(_{s}^{t}A())\) is called the transition matrix associated with \(A(t)\). The separation of the linear and nonlinear components is achieved by this formulation and also appears in [22; 23; 41]. It differs from black-box SDE solvers as it enables the exact calculation of the linear portion, thereby removing any approximation errors associated with it. However, the integration of the nonlinear portion remains complex due to the interaction of the new coefficient \(_{A}(t,)b()\) and the intricate neural network, making it challenging to approximate.

Exponentially weighted integrals.Due to the regularity conditions usually imposed on the drift and diffusion coefficients of (1), one can make several choices of change-of-variables on the integral components in (5) in order to simplify it. Our second key insight is that there is a specific choice of change of variables allowing the analytic computation of the Ito-Taylor coefficients of \(F_{}(_{},)\) with respect to \(\), and based at \(s\) that will be used for crafting SEEDS. More specifically, this expansion reads

\[F_{}(_{},)=_{k=0}^{n}}{k!}F_{ }^{(k)}(_{s},s)+_{n},\]

where the residual \(_{n}\) consists of deterministic iterated integrals of length greater than \(n+1\) and all iterated integrals with at least one stochastic component. As such, we obtain

\[_{s}^{t}_{A}(t,)b()F_{}(_{},)=_{k=0}^{n}F_{}^{(k)}(_{s},s)_{s}^{t}_{A}(t, )b()}{k!}+}_{n},\] (6)

where \(}_{n}\) is easily obtained from \(_{n}\) and \(_{s}^{t}_{A}(t,)b()\). The third key contribution of our work is to rewrite, for any \(k 0\), the integral \(_{s}^{t}_{A}(t,)b()}{k!}\) as an integral of the form \(_{_{s}}^{_{t}}e^{\,)^{k}}{k!}}\) since the latter is recursively analytically computed in terms of the \(\)-functions

\[_{0}(t):=e^{t},_{k+1}(t):=_{0}^{1}e^{(1-)t}}{k!}=(t)-_{k}(0)}{t}, k  0.\]

Modified Gaussian increments.In order for making such change of variables to be consistent on the overall system, one needs to replicate it accordingly in the stochastic integral \(_{s}^{t}_{A}(t,)g()}_{}\). As such, our last key contribution is to transform it into an exponentially weighted stochastic integral with integration endpoints \(_{s},_{t}\) and apply the Stochastic Exponential Time Differencing (SETD) method  to compute its variance analytically, as illustrated in (14) below.

Let us test our methodology in two key examples. As we explained in Section 2, sampling from pre-trained DPMs amounts on choosing a schedule \(_{t}\), a scaling \(_{t}\), and a parameterized learnt approximation of the score function \(_{_{t}} p_{t}(_{t})\). In what follows, we denote by \(t_{}\) the inverse of a chosen change of variables \(_{t}\) and we denote \(}_{}:=(t_{}()),_{ }(}_{},):=F_{}((t_{ }()),t_{}())\).

The VPSDE case.Let \(_{t}:=_{d}t^{2}+_{m}t\), where \(_{d},_{m}>0\) and \(t\). Then, by denoting

\[_{t}:=_{t}}-1},_{t}:=e^{- _{t}},_{t}:=_{t}_{t}, _{_{t}} p_{t}(_{t})_{t}^{-1}F_{ }(_{t},t),\] (7)

we obtain the VP SDE framework from  and the following result.

**Proposition 3.1**.: _Let \(t<s\). The analytic solution at time \(t\) of the RSDE (2) with coefficients (7) and initial value \(_{s}\) is_

\[_{t}=}{_{s}}_{s}-2_{t}_{ _{s}}^{_{t}}e^{-}_{}(}_{ },)-_{t}_{_{s}}^{ _{t}}e^{-}}_{},_{t}: =-(_{t}).\] (8)

The change of variables of (8) is interesting as it allows to compute analytically the Ito-Taylor coefficients in (6) by using, for \(h=_{t}-_{s}\), the following key result which will be used in Prop. 4.2:

\[_{_{s}}^{_{t}}e^{-})^{k}}{k!}=_{t}h^{k+1}_{k+1}(h).\] (9)

For instance, in the case when \(k=0\), it is easy to see that \(_{_{s}}^{_{t}}e^{-}=_{t}(e^{h}-1)\) and \(_{_{s}}^{_{t}}e^{-}}_{}\) obeys a normal distribution with zero mean, and one can analytically compute its variance:

\[_{_{s}}^{_{t}}e^{-2}= ^{2}}{2}(e^{2h}-1).\] (10)The EDM case.Denote \(_{d}^{2}\) the variance of the considered initial dataset and set

\[_{t}:=t,_{t}:=1,_{_{t}} p_{t}(_{t}) }[^{2}_{t}}{t^{2}+_{d }^{2}}+}{+_{d}^{2}}}F_{}(_{t}}{+_{d}^{2}}},)].\] (11)

These parameters correspond to the preconditioned EDM framework introduced in [16, Sec. 5, App. B.6]. The following result is the basis for constructing our customized SEEDS in this case, and for which we report experimental results in Table 1. For simplicity, we will write \(F_{}(_{t},t)\) for the preconditioned model in (11) and we refer to Appendix B for details.

**Proposition 3.2**.: _Let \(t<s\). The analytic solution at time \(t\) of (2) with coefficients (11) and initial value \(_{s}\) is, for \(=1\),_

\[_{t}=+_{d}^{2}}{s^{2}+_{d}^{2}}_{s }+2(t^{2}+_{d}^{2})_{_{s}}^{_{t}}e^{-}_{ }(}_{},)-(t^ {2}+_{d}^{2})_{_{s}}^{_{t}}e^{-} _{},\] (12)

_where \(_{t}:=-[+_{d}^{2}}}]\). In the case when \(=0\), it is given by_

\[_{t}=+_{d}^{2}}{s^{2}+_{d}^{2}}} _{s}++_{d}^{2}}_{_{s}}^{_{t}}e^{ -}_{}(}_{},) ,_{t}:=-[[}] ].\] (13)

_Remark 3.3_.: One can wonder about the generality of such change of variables. Our method is very general in that one can always make such change of variables with very mild regularity conditions: for \(c:[0,T]^{>0}\) integrable, with primitive \(C(t)>0\), we have \(c(t)=e^{(c(t))}\). This means we can write \(c(t)=(t)=e^{_{t}}_{t}\) with \(_{t}=(C(t))\). In other words, for such \(c\), we have

\[_{s}^{t}c()=_{s}^{t}e^{_{}} _{}=_{_{s}}^{_{t}}e^{}.\]

## 4 Higher Stage SEEDS for DPMs

In this section we present our SEEDS algorithms by putting together all the ingredients presented in the previous section. Let \(t<s\). In all what follows, we consider the analytic solution at time \(t\) of the RSDE (2) with coefficients (7), \(h=_{t}-_{s}\) and initial value \(_{s}\). Plugging (9) with \(k=0\) and (10) into the exact solution (8) allow us to infer the first SEEDS scheme, given by iterations of the form

\[}_{t}=}{_{s}}}_{s}-2 _{t}(e^{h}-1)_{}(}_{_{s}}, _{s})-_{t}-1},(,_{d}).\] (14)

The following Theorem gives strong order convergence guarantees for this method, which we call SEEDS-1, under mild conditions which apply to all our experiments. We stress out that its proof (App. C) is a non-trivial result, it is fundamentally different in nature from  and involves mathematical tools which have no deterministic counterparts.

**Theorem 4.1**.: _Under Assumption C.1, the numerical solution \(}_{t}\) produced by the SEEDS-1 method (14) converges to the exact solution \(_{t}\) of_

\[_{t}=[f(t)_{t}+g^{2}(t)_{t}^{-1}F_{ }(_{t},t)]t+g(t)_{t}, (_{t}^{-1}:=1/_{t})\] (15)

_with coefficients (7) in Mean-Square sense with strong order 1.0: there is a constant \(C>0\) such that_

\[[_{0 t 1}|}_{t}- _{t}|^{2}]} Ch,h 0.\]

Higher stage SEEDS.As announced, by fully exploiting the analytic computations enabled by the expansion (9) we now turn into crafting our multi-step SEEDS. Usually, SDE solvers are constructed by using the full Ito-Taylor expansion of the SDE solutions and usually need a big number of evaluations of the network \(_{}\) to achieve higher order of convergence. As our main concern is to present stochastic solvers with a minimal amount of NFE, we choose to truncate such Ito-Taylor expansion so that the neural networks only appear in the deterministic contributions.

**Proposition 4.2**.: _Assume that \(_{}\) is a \(^{2n+1}\)-function with respect to \(\). Then the truncated Ito-Taylor expansion of (8) reads, for \((,_{d})\),_

\[_{t}=}{_{s}}_{s}-2_{t} _{k=0}^{n}h^{k+1}_{k+1}(h)_{}^{(k)}(}_{_{s}},_{s})-_{t}-1}+_{n+1},\] (16)

_with \(_{}^{(k)}(}_{},)=L_{}^{k }_{}(}_{},)\), with \(L_{}\) is an infinitesimal operator defined in Appendix E.2.2 and \(_{n+1}\) consists on the usual deterministic residue and all iterated integrals of length at greater or equal to 2 in which there is at least one stochastic component among them._

Our approach for constructing derivative-free 2-stage and 3-stage SEEDS schemes consists on exploiting the analytic computation of the Ito-Taylor coefficients in Proposition 4.2 and replace the \(_{}^{(k)}(}_{},)\) terms by well-adapted correction terms which _do not need any derivative evaluation_ and dropping the \(_{n+1}\) contribution as in the Runge-Kutta approach.

Markov-preserving noise decomposition.We use collocation methods for constructing higher-stage derivative-free solvers. Although the chosen truncated Ito-Taylor expansion produces approximations for the deterministic integral similar to , adding the corresponding noise contribution found by the SETD method at each step does not yield Markov chains in general. The reason is that stochastic integrals on overlapping time intervals need to be dependent, a phenomenon that has no deterministic counterpart. As such, our last and key element to construct SEEDS consists on a novel decomposition of stochastic integrals which enforces the Markov property for multi-stage SEEDS.

Algorithms 1 to 4 prescribe all SEEDS schemes obtained by this procedure in the VP case. We now show (see App. C for the proofs) that all methods yield Markov chains and are weakly convergent.

**Proposition 4.3**.: _The sequences \(\{}_{t}\}_{t}\) induced by the choice of stochastic noise contributions presented in Algorithms.3 and 4 satisfy the Markov property._

**Corollary 4.4**.: _Under Assumption C.2, the numerical solutions \(}_{t}\) produced by the SEEDS methods (3) and (4) converge to the exact solution \(_{t}\) of (15) with coefficients (7) in weak sense with global order 1 in both cases: there is a constant \(C>0\) such that, for any continuous bounded function \(G\):_

\[|[G(}_{t_{M}})]-[G(_{t_{M}})] | Ch.\]Comparison with existing sampling methods.Let us now examine the connection between SEEDS and existing sampling techniques used for DPMs, emphasizing the contrasts between them.

The main distinctive feature of SEEDS is that they are _off-the-shelf_ solvers. This means that, not only they are _training-free_, contrary to , but they do not require any kind of optimization procedure to achieve their optimal results. This is in contrast to methods such as: gDDIM, which is training-free but not off-the-shelf as one needs to make preliminary optimization procedures such as simulating the transition matrix of their method in the CLD case; Heun-Like method from EDM (for all baseline models and the EDM-optimized models for ImageNet) since they need preliminary optimization procedures on 4 parameters which actually break the convergence criteria. Moreover, neither gDDIM, EDM nor the SSCS method in  present full proofs of convergence for their solvers. Also, both DEIS and gDDIM identify their methods with stochastic DDIM theoretically, but the poor results obtained by their stochastic solvers do not yield to further experimentation in their works. In a way, SEEDS can be thought as improved and generalized DPM-Solver to SDEs. Nevertheless, such generalization is not incremental as the tools for proving convergence in our methods involve concepts which are exclusive to SDEs. We now make rigorous statements of the above discussion.

**Proposition 4.5**.: _Consider the SEEDS approximation of (15) with coefficients (7). Then_

1. _If we set_ \(g=0\) _in (_15_), the resulting SEEDS do not yield DPM-Solver._
2. _If we parameterize (_15_) in terms of the data prediction model_ \(D_{}\)_, the resulting SEEDS are not equivalent to their noise prediction counterparts defined in Alg._ 1 _to 4_._
3. _The gDDIM solver_ _[_41_]__, Th. 1]_ _equals to SEEDS-1 in the data prediction mode, for_ \(=1\)_._

The first point makes it explicit that SEEDS are not incremental based on DPM-Solver. The second point in Prop. 4.5 is analog to the result in Appendix B of , where the authors compare DPM-Solver2 and DPM-Solver++(2S), that is the noise and data prediction approaches, and find that they do not equate. The last point exhibits gDDIM as a special case of SEEDS-1 for isotropic DPMs.

_Remark 4.6_.: Building solvers from the representation of the exact solution in (5) requires computing the transition matrix \(_{A}(t,s)\), which cannot be analytically computed for non-isotropic DPMs such as CLD . Nevertheless, the SEEDS approach can be applied in this scenario in at least two different ways. On the one hand, the SSCS method from  resides splitting \(_{A}(t,s)\) into two separate terms. The first can be analytically computed. The second describes the evolution of a semi-linear differential equation [10, Eq. 92]. While  approximates the latter by the Euler method, crafting exponential integrators for approximating such DE may yield an acceleration of the SSCS method. On the other hand, gDDIM  proposes an extension of DEIS sampling  to CLD by setting a pre-sampling phase [41, App. C.4] in which they compute an approximation of \(_{A}(t,s)\) in order to apply their method, and the latter was shown in Prop. 4.5. to be a special case of our method. Unfortunately, the authors did not release pre-trained models in , and the latter are not the same as those in . Sampling in this scenario may also benefit from our approach.

## 5 Experiments

We compare SEEDS with several previous methods on discretely and continuously pre-trained DPMs. We report results of many available sources, such as DDPM , Analytic DDPM , PNDM , GGF , DDIM , gDDIM , DEIS  and DPM-Solver . Although we do not include training-based schemes here, we still included GENIE , which trains a small additional network but still solves the correct generative ODE at higher-order. For each experiment, we compute the FID score for 50K sampled images on multiple runs and report the minimum along different solvers. Details on model specifications and experiment illustrations are shown in Appendix F.

Practical considerations.For continuously trained models, SEEDS use the EDM discretization [16, Eq. 5] with default parameters and does _not_ use the _last-step iteration trick_, meaning that the last iteration of SEEDS is trivial. For discretely trained models, SEEDS use the linear step schedule in the interval \([_{t_{0}},_{t_{S}}]\) interval following [22, Sec. 3.3, 3.4]. All the reported SEEDS results were obtained using the noise prediction mode. We conducted comparative experiments on SEEDS for both the data and noise prediction modes and found better results with the latter (see Tab. 3 for details). EDM solvers [16, Alg. 2] depend on four parameters controlling the amount of noise to be injected in a specific sub-interval of the iterative procedure. We consider three scenarios: when stochasticity is injected along all the iterative procedure, we denote it stochastic EDM, when no stochasticity is injected we denote it EDM (\(S_{ churn}=0\)) and we denote EDM (Optimized) the case where such parameters were subject to an optimization procedure. To better evaluate sampling quality along the sample pre-trained DPM, we recalculate DPM-Solver for sampling from the _non-deep_ VP continuous model on the CIFAR-10 dataset. All implementation details can be found in Appendix D.

Comparison with previous works.In Table 1 we compare SEEDS with other sampling methods for pre-trained DPMs, and report the minimum FID obtained and their respective NFE. For each of the reported pre-trained models in CIFAR-10, CelebA-64 and ImageNet-64, SEEDS outperform all off-the-shelf methods in terms of quality with relatively low NFEs. For the discrete pre-trained DPM on CIFAR-10 (VP Uncond.) it is \( 5\) faster than the second best performant solver. Additionally, SEEDS remain competitive with the optimized EDM sampler. For ImageNet-64, it is nearly as good

   Sampling method & FID\(\) & NFE \\  CIFAR-10* vp-uncond. & & \\  DDIM  & 3.95 & 1000 \\ analytic-DDPM  & 3.84 & 1000 \\ GENIE  & 3.64 & 25 \\ Analytic-DDIM  & 3.60 & 200 \\ F-PNDM (linear)  & 3.60 & 250 \\ DPM-Solver\({}^{}\) & 3.48 & 44 \\ F-PNDM (cosine)  & 3.26 & 1000 \\ DDPM  & 3.16 & 1000 \\ SEEDS-3 (ours) & **3.08** & **201** \\  CIFAR-10* vp-cond. & & \\  DPM-solver\({}^{}\) & 3.57 & 195 \\ EDM (\(S_{ churn}=0\))  & 2.48 & 35 \\ SEEDS-3 (Ours) & **2.08** & **129** \\  CIFAR-10* vp-uncond. & & \\  DPM-Solver\({}^{}\) & 2.59 & 51 \\ GGF  & 2.59 & 180 \\ GODIM  & 2.56 & 100 \\ DEIS \(\)3Kutta  & 2.55 & 50 \\ Euler-Maruyama  & 2.54 & 1024 \\ Stochastic EDM  & 2.54 & 1534 \\ SEEDS-3 (Ours) & 2.39 & 165 \\ EDM (Optimized)  & **2.27** & **511** \\   

Table 1: Sample quality measured by FID\(\) on pre-trained DPMs. We report the minimum FID obtained by each model and the NFE at which it was obtained. For CIFAR, CelebA and FFHQ, we use baseline pre-trained models . For ImageNet, we use the optimized pre-trained model from . *discrete-time model, *continuous-time model, \({}^{}\)recomputed FID for the non-deep model.

Figure 1: (a-b) Comparison of sample quality measured by FID \(\) of SEEDS, DPM-Solver and other methods for discretely trained DPMs on CIFAR-10 with varying number of function evaluations. (c) Effect of \(S_{ churn}\) on SEEDS-3 (at NFE = 270) and EDM method (at NFE = 511) on class-conditional ImageNet-64. \({}^{}\)baseline ADM model. *EDM preconditioned model.

as the optimized EDM sampler while being almost twice faster than the latter. Figure 1 (a) compares the FID score of SEEDS and DPM-Solver with varying NFEs. While DPM-Solver methods stabilize faster in a very low NFE regime, our methods eventually surpass them. Interestingly, after reaching their minimum, SEEDS methods tend to become worse at higher NFEs, a fact that is also visible in Figure 1 (b), where we notice that such phenomenon is also present on other SDE methods. We report in Appendix F, the results of our SEEDS methods in the low NFE regime and connect their behavior with their proven convergence rate.

Combining SEEDS with other methods.While being an off-the-shelf method, SEEDS can be combined with the Churn-like method used in EDM incurring into SDE solvers with an additional source of stochasticity. As done in , we evaluate the effect of this second kind of stochasticity, measured by a parameter denoted \(S_{}\). Figure 1 (c) shows that SEEDS and EDM show similar behavior, although SEEDS-3 is twice faster, more sensitive to \(S_{}\), and quickly achieves comparable performance to EDM. This indicates that SEEDS could possibly outperform EDM after a proper parameter optimization that will be left for future works. Nevertheless, we highlight the fact that obtaining such optimal parameters is costly and might scale poorly.

Stiffness reduction with SEEDS.In Fig. 2, we illustrate the impact of different choices of discretization steps, noise schedule and dynamic scaling on SEEDS and stochastic EDM. We see that choosing the EDM discretization over the linear one has the effect of flattening the pixel trajectories at latest stages of the simulation procedure. Also, choosing the parameters (11) over those in (7) has the effect of greatly changing the distribution variances as the trajectories evolve. Notice that all the SEEDS trajectories seem perceptually more stable than those from EDM. It would be interesting to relate this to the _stiffness_ of the semi-linear DE describing these trajectories, and to the magnitude of the parameters involved in the noise injection for EDM solver amplifying this phenomenon.

Ablation studies.As said earlier, our principled use of the Chasles rule to enforce independence only on non-overlapping paths for SEEDS-2/3 ensures that the set of resulting iterations of our solvers satisfy the Markov property, is new and is the central key of success of SEEDS. To highlight this,

Figure 2: Trajectories of 10 pixels (R channel) sampled from SEEDS (1st line) and Stochastic EDM (2nd line) on the optimized pre-trained model  on ImageNet64. Schedule=scaling=vp corresponds to the VP coefficients in (7) and schedule=linear, scaling=none to the EDM coefficients (11). We use the time discretizations disc=vp (linear) and disc=edm given in [16, Tab.1].

we conduct an ablation study on how 4 different combinations of the noise components \(\) and \(\) in Algorithms 3 and 4 have an impact on the sampling quality of SEEDS.

For simplicity, we explain this for SEEDS-2 (Alg. 3). Set \((z^{1},z^{2},z^{3})\) three independent standard Gaussian random variables. Denote \(=_{s_{1}}-1}z^{1}\) for the noise contribution in the \(\) term. We have the following choices for the noise contribution \(\) in \(}_{t}\):

* SEEDS-2-Correct: our noise combination \(=_{t}(-e^{h}}z^{1}+-1}z^{2})\)
* SEEDS-2-Naive-1: one noise per stage \(=_{t}(-e^{h}}+-1})z^{2}\)
* SEEDS-2-Naive-2: one noise per step \(=_{t}(-e^{h}}+-1})z^{1}\)
* SEEDS-2-Naive-3: one noise per integral evaluation \(=_{t}(-e^{h}}z^{3}+-1}z^{2})\)
* SEEDS-2-Naive-4: noises in inverse position \(=_{t}(-e^{h}}z^{2}+-1}z^{1})\).

Figure 3 experimentally shows that all naive combinations of noises for SEEDS-2 and SEEDS-3 lead to FID/NFE curves with different behavior and a sharp drop of performance both in quality and speed in all of them, a phenomenon having no deterministic parallel.

## 6 Conclusions

Our focus is on addressing the challenge of training-free sampling from DPMs without compromising sampling quality. To achieve this, we introduce SEEDS, an off-the-shelf solution for solving diffusion SDEs. SEEDS capitalize on the semi-linearity of diffusion SDEs by approximating a simplified formulation of their exact solutions. Inspired by numerical methods for stochastic exponential integrators, we propose three SEEDS schemes with proven convergence order. They transform the integrals involved in the exact solution into exponentially weighted integrals, and estimate the deterministic one while analytically computing the variance of the stochastic integral. We extend our approach to handle other isotropic DPMs, and evaluate its performance on various benchmark tests. Our experiments demonstrate that SEEDS can generate images of optimal quality, outperforming existing SDE solvers while being \(3 5\) faster. **Limitations and broader impact.** While SEEDS prioritize optimal quality sampling, they may require substantial computational resources and energy consumption, making them less suitable for scenarios where speed is the primary concern. In such cases, alternative ODE methods may be more appropriate. Additionally, as with other generative models, DPMs can be employed to create misleading or harmful content, and our proposed solver could inadvertently amplify the negative impact of generative AI for malicious purposes.

Figure 3: Quality (measured by FID at increasing NFEs) comparison of SEEDS-2/3 with the enumerated ablation versions of it on CIFAR-10 in the (baseline) VP conditional framework.

**Acknowledgments.** This work has been supported by the French government under the "France 2030" program, as part of the SystemX Technological Research Institute.