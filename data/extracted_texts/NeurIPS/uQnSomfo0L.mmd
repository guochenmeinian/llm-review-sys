# SE(3) Equivariant Augmented Coupling Flows

Laurence I. Midgley

University of Cambridge

lim24@cam.ac.uk

&Vincent Stimper

Max Planck Institute for Intelligent Systems

University of Cambridge

vs488@cam.ac.uk

Javier Antoran

University of Cambridge

ja666@cam.ac.uk

&Emile Mathieu

University of Cambridge

ebm32@cam.ac.uk

&Bernhard Scholkopf

Max Planck Institute

for Intelligent Systems

bs@tue.mpg.de

&Jose Miguel Hernandez-Lobato

University of Cambridge

jmh233@cam.ac.uk

Equal contribution

Code: https://github.com/lollcat/se3-augmented-coupling-flows

###### Abstract

Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivariant continuous normalizing flows, while allowing sampling more than an order of magnitude faster. Moreover, to the best of our knowledge, we are the first to learn the full Boltzmann distribution of alanine dipeptide by only modeling the Cartesian positions of its atoms. Lastly, we demonstrate that our flow can be trained to approximately sample from the Boltzmann distribution of the DW4 and LJ13 particle systems using only their energy functions.

## 1 Introduction

Modeling the distribution of a molecule's configurations at equilibrium, known as the Boltzmann distribution, is a promising application of deep generative models (Noe et al., 2019). While the unnormalized density of the Boltzmann distribution can be obtained via physical modeling, sampling from it typically requires molecular dynamics (MD) simulations, which are expensive and produce correlated samples. A promising alternative is to rely on surrogate deep generative models, known as Boltzmann generators. We can draw independent samples from these and debias any expectationsestimated with the samples via importance weighting. The Boltzmann distribution typically admits rotation and translation symmetries, known as the Special Euclidean group \((3)\), as well as permutation symmetry. These constraints are important to incorporate into the model as they improve training efficiency and generalization (Cohen and Welling, 2016; Batzner et al., 2022; Kohler et al., 2020). Other key desiderata of Boltzmann generators are that they allow for fast sampling and density evaluation. These are necessary for energy-based training, that is, training using the Boltzmann distribution's unnormalized density (Noe et al., 2019; Stimper et al., 2022; Midgley et al., 2023). Training by energy is critical, as it prevents the model quality from being constrained by the quality and quantity of MD samples.

Existing coupling flows which approximate the Boltzmann distribution of molecules are at least partially defined over internal coordinates, i.e. bond distances, bond angles, and dihedral angles (Wu et al., 2020; Campbell et al., 2021; Kohler et al., 2023; Midgley et al., 2023), which are \((3)\) invariant. However, the definition of these depends on the molecular graph and they are non-unique for most graphs. Furthermore, models parameterized in internal coordinates struggle to capture interactions among nodes far apart in the graph and cannot capture the permutation invariance of some atoms. Thus, they are not suitable for particle systems such as LJ13 (Kohler et al., 2020). \((3)\) equivariance constraints have been applied to continuous normalizing flows (CNFs) operating on Cartesian coordinates (Kohler et al., 2020; Satorras et al., 2021), and to the closely related diffusion models (Xu et al., 2022; Hoogeboom et al., 2022; Yim et al., 2023). These models are built upon \((3)\) equivariant graph neural networks (GNNs) (Satorras et al., 2021; Geiger and Smidt, 2022; Batzner et al., 2022). These architectures can be applied to any molecular graph (Jing et al., 2022), enabling a single generative model to generalize across many molecules. Alas, sampling and evaluating the density of CNFs and diffusion models typically requires thousands of neural network evaluations (Xiao et al., 2022), preventing them from being trained by energy. As such, presently no Boltzmann generator exists that (i) acts on Euclidean coordinates of atoms (ii) enforces \((3)\) equivariance, and (iii) allows for fast sampling.

To address this gap, we propose a flexible \((3)\) equivariant coupling flow that operates on the Cartesian coordinates of atoms, allowing for fast sampling and density evaluation. Our contributions are:

* We extend coupling layers to be \((3)\) equivariant by augmenting their input space with auxiliary variables (Huang et al., 2020) which can be acted upon on by \((3)\). We update the atom positions conditioned on the auxiliary variables by first projecting the atoms into an \((3)\)-invariant space and then applying a standard normalizing flow transform before projecting its output back onto the equivariant space.
* We demonstrate that, when trained by maximum likelihood, our flow matches the performance of both existing \((3)\) CNFs and coupling flows operating on internal coordinates on molecular generation tasks. Our flow is more than 10 times faster to sample from than \((3)\) CNFs. Concurrently with Klein et al. (2023), we are the first to learn the full Boltzmann distribution of alanine dipeptide solely in Cartesian coordinates.
* We demonstrate our flow in the energy-based training setting on the DW4 and LJ13 problems, where parameters are learned using only the molecular energy function. Energy-based training of the CNF is intractable due to slow sampling and density evaluation. Flows that operate on internal coordinates are not able to capture the permutation invariance of these problems. Hence, our flow is the only existing permutation and \((3)\) equivariant method that can tractably be applied there.

## 2 Background: coupling flows and invariant models

### Normalizing flows and coupling transforms

A (discrete-time) normalizing flow is a flexible parametric family of densities on \(\) as the push-forward of a base density \(q_{0}\) along an invertible automorphism \(f_{}:\) with parameters \(\)(Papamakarios et al., 2021). The density is given by the change of variable formula:

\[q_{}(x)=q_{0}(f^{-1}(x))\,|^{- 1}(x)}{ x}|.\] (1)

We can efficiently sample from the flow by sampling from \(q_{0}\) and mapping these samples through \(f_{}\) in a single forward pass. A popular way to construct \(f_{}\) is to use coupling transforms. The dimensional input \(\) is split into two sets, transforming the first set conditional on the second, while leaving the second set unchanged:

\[_{1:d}= \,(_{1:d};_{d+1:D}),\] (2) \[_{d+1:D}= \,_{d+1:D}.\]

They induce a lower triangular Jacobian, such that its determinant becomes \(|(_{1:d};_{d+1:D})/_{1:d}|\). Further, choosing \(\) to have an easy to compute determinant, such as an elementwise transformation (Dinh et al., 2015, 2017; Durkan et al., 2019), allows for fast density evaluation and sampling at low computational cost.

### Equivariance and invariance for coupling flow models of molecular conformations

Throughout, we deal with observations of an \(n\)-body system represented by a matrix \(=[x^{1},,x^{n}]=^{3 n}\), where the rows index Cartesian coordinates and the columns index individual particles. We seek to construct flows on \(\) endowed with the symmetries present in molecular energy functions. These are invariant to rotations and translations of \(\) (\((3)\)), and to permutation of atoms of the same type (\(S_{n}\)). We will formalize them in this section.

Symmetry groupsThe special Euclidean group \((3)\) is the set of orientation preserving rigid transformations in Euclidean space. Its elements \(t(3)\) can be decomposed into two components \(t=(R,u)\) where \(R(3)\) is a \(3 3\) rotation matrix and \(u^{3}\) represents a translation; for a coordinate \(v^{3}\), \(t v=Rv+u\) denotes the action of \(t\) on \(v\). The symmetric group \(S_{n}\) defined over a set of \(n\) atoms consists of all \(n!\) permutations that can be performed with said atoms. Its elements \( S_{n}\) act on an \(n\)-body system as \(=[x^{(1)},,x^{(n)}]\).

Equivariant mapsA map \(f:\) is said to be _\(G\)-equivariant_ if it commutes with the group action, i.e. if for any \(x\) and \(g G\) we have \(f(g x)=g f(x)\). Invariance is a special case where for any \(x\) and \(g G\) we have \(f(g x)=f(x)\). There has been a plethora of recent work on constructing graph neural network functions equivariant to the action of \(G=(3) S_{n}\)(e.g. Thomas et al., 2018; Satorras et al., 2021; Geiger and Smidt, 2022), which we will leverage to construct our equivariant coupling flow model.

Invariant densityA density \(p:_{+}\) is \(G\)-invariant if for any \(x\) and \(g G\) we have \(p(g x)\!=\!p(x)\). Combining an invariant base density \(q_{0}\) with an equivariant invertible transform \(f\), as in (1), yields an invariant flow density (Papamakarios et al., 2021; Kohler et al., 2020). This gives a practical way to design invariant densities models.

Challenges in constructing \(}\) invariant flow modelsUnfortunately, no coupling transform can be simultaneously equivariant to both permutation of the particles and their rotations; coupling splits must be performed either across particles or spatial dimensions which would break either permutation or rotational symmetry (Kohler et al., 2020; Bose et al., 2022).

Furthermore, there does not exist a translation invariant _probability_ measure, as any such measure would be proportional to the Lebesgue measure and therefore not have unit volume. This precludes us from defining an invariant base distribution directly on \(\). Fortunately, Proposition A.1 and its converse allow us to disintegrate the probability measure into a translational measure proportional to the Lebesgue measure and an \((3)\)-invariant probability measure on the subspace of \(^{3 n}\) with zero center of mass. We can drop the former and only model the latter.

## 3 Method: \(}\) equivariant augmented coupling flow model

This section describes our main contribution, an \((3) S_{n}\) equivariant coupling flow. We first lay the groundwork for achieving translation invariance by defining our flow density on a lower-dimensional "zero Center of Mass (CoM)" space. To preserve permutation and rotation equivariance, we leverage the augmented flow framework of Huang et al. (2020). Specifically, we use sets of augmented variables as a pivot for coupling transforms. Sec. 3.1 introduces a novel class of coupling transforms that achieve the aforementioned permutation and rotation equivariance by operating on atoms projected using a set of equivariant bases. Sec. 3.2 describes our choice of invariant base distribution and, finally, in Sec. 3.3, we discuss several schemes to train the augmented flow from either samples or energy functions and how to perform efficient density evaluation.

**Translation invariance** is obtained by modelling the data on the quotient space \(^{3 n}/\,^{3}} \), where all \(n\)-body systems that only differ by a translation are "glued" together, i.e. where \(^{}\) if \(=^{}+p\) with \(p^{3}\). Constructing a parametric probabilistic model over \(}\), automatically endowing it with translation invariance. In practice, we still work with Cartesian coordinates, but center the data so as to zero its CoM: \(}-}\) with \(}_{i=1}^{n}[]^{i}\). Thus, \(}\) lies on \(}\), an \((n-1) 3\) dimensional hyperplane embedded in \(\).

Augmented variable pivoted couplingTo allow for simultaneously permutation and rotation equivariant coupling transforms, we introduce _augmented_ variables \(\). Our coupling layers update the particle positions \(\) conditioned on \(\) and vice-versa. The augmented variables need to "behave" similarly to \(\), in that they can also be acted upon by elements of \((3) S_{n}\). We achieve this by choosing \(\) to be a set of \(k\) of observation-sized arrays \(=^{k}\), which we will discuss further in App. B.4. Importantly, we do not restrict \(\) to be zero-CoM.

Invariant flow density on the extended spaceWe parameterize a density \(q\) over the extended space \(}\) w.r.t. the product measure \(_{}}_{}\), where \(_{}}(})\) and \(_{}()\) respectively denote the Lebesgue measure on \(}\) and \(\). We use \(q(,)\) as a shorthand for the density of the corresponding zero-CoM projection \(q(},)\). The density \(q\) is constructed to be invariant to the action of \(G(3) S_{n}\) when simultaneously applied to the observed and augmented variables, that is, \(q(,)=q(g,g)\) for any \(g G\). We aim to construct a \(G\)-equivariant flow \(f:}} \) on this extended space, and combine it with \(q_{0}:}^{+}\), a \(G\)-invariant base density function, to yield the invariant flow density

\[q(,)=q_{0}(f^{-1}(,))\,|(,)}{(,)}|.\] (3)

**Proposition 3.1** (Invariant marginal).: _Assume \(q:_{+}\) is a \(G\)-invariant density over the probability space \((,_{}_{})\), then \(q_{}_{}q(,)_{}( ):_{+}\) is a \(G\)-invariant density w.r.t. to the measure \(_{}\)._

Proof.: For any \(g G\), \(\) and \(\)

\[q_{}(g)=\!\!_{}\!\!q(g,) _{}()=\!\!_{g^{-1}}\!\!\!q(g ,g)_{}(\,g)=\!\! _{}\!\!q(,)_{}()=q _{}(),\]

where we used the \(G\)-invariance of \(q\) and of the measure \(_{}\), as well as \(g^{-1}=\). 

### \((3)\) and permutation equivariant coupling transform

We now derive our \((3) S_{n}\) equivariant map \(f:}} \) defined on the extended space. We introduce two modules, a shift-CoM transform, swapping the center of mass between the observed and augmented variables, and an equivariant core transformation, which updates \(}\) conditional on \(\) and vice versa. Composing them yields our equivariant coupling layer illustrated in Fig. 1.

\((3)_{n}\) equivariant couplingWe dissociate the equivariance constraint from the flexible parameterized flow transformation by (1) projecting the atoms' Cartesian coordinates into a learned,

Figure 1: Illustration of the equivariant coupling layer of our augmented normalizing flow, where our variable with zero center of mass (CoM) \(}\) is transformed with the augmented variable \(\).

local (per-atom) invariant space, (2) applying the flexible flow to the invariant representation of the atoms' positions, and (3) then projecting back into the original Cartesian space. Specifically, we construct a core coupling transformation that composes (a) an invariant map \(:\), where \(\) is isomorphic to \(\), and \(\) parametrizes the map. We denote the parametrized map as \(_{}\). It is followed by (b) a standard flexible normalizing flow transform, e.g. a neural spline, \(:\), and (c) the inverse map \(^{-1}\). Denoting the inputs with superscript \(\) and with \(+1\) for outputs, our core transformation \(:(^{};^{})(^{+1},^{ +1})\) is given as

\[^{+1} =_{}^{-1}_{}(_{}^{}),(,)=h(^{}),\] (4) \[^{+1} =^{}.\]

Here, \(h\) is a (graph) neural network that returns a set of equivariant reference vectors \(\), which parametrize the map \(_{}\), and invariant parameters \(\). \(\) is a rotation invariant space. This means that any rotations applied to the inputs will be cancelled by \(_{}\), i.e. \(_{g}=_{} g^{-1}\) or equivalently \((_{g})^{-1}=g_{}^{-1}\) for all \(g(3)\). We use the inverse projection \(_{}^{-1}\) to map the invariant features back to equivariant features. The function \(_{}\) is a standard invertible inner-transformation such as an affine or spline based transform, that we apply to the invariant features .

We now show that the above described transform is rotation and permutation equivariant.

**Proposition 3.2** (Equivariant augmented coupling flow).: _If \(h:^{n}^{n}\) is \((3)\)-equivariant for its first output, \((3)\)-invariant for its second, and \(S_{n}\) equivariant for both, and \((_{g})^{-1}=g_{}^{-1}\) for all \(g(3)\), the transform \(}\) given by (10) is \((3) S_{n}\) equivariant._

Proof.: For \((3)\): We first notice that \(h(g)=(g,)\), and then since \((_{g})^{-1}=g_{}^{-1}\) we have \((g,g)=(_{g})^{-1} _{}(_{g} g^{})=g_{}^{-1}_{}(_{} g^{-1} g^{})=g (,)\).

For \(S_{n}\): We first note that \(h()=(,)\). Then, using that \(_{}\) and \(\) act on \(\) atom-wise, we have \((,)=_{g}^{-1} _{}(_{}( ))=(_{}^{-1})(_{})(( _{})())=( ,)\). 

For the Jacobian of the coupling described above to be well-defined, the variable being transformed must be non-zero CoM (see App. B.1 for a derivation). Thus, although our observations live on \(}\), for now, assume that the inputs to the transform are not zero-CoM and we will deal with this assumption in the following paragraphs. This choice also allows us to use standard equivariant GNNs for \(h\) which leverage per-node features defined in the ambient space, such as atom type and molecular graph connectivity.

Choices of projection \(\)The equivariant vectors \(\) parameterize a local (per-atom) \((3)\) equivariant reference frame used in the projection \(_{}\). We introduce three different projection strategies. (i) The first strategy, is for \(\) to parameterize frame composed of an origin and orthonormal rotation matrix into which we project each atom's positions. We then take \(\) to be a dimension-wise transformation for each of the projected atoms' coordinates. We dub this method Cartesian-proj. (ii) Alternatively, we let \(\) parameterize a origin, zenith direction and azimuth direction for spherical coordinates, as in Liu et al. . We then apply elementwise transforms to each atom's radius, polar angle and azimuthal angle. We call this Spherical-proj. (iii) Lastly, consider a variant of Spherical-proj where just the radius is transformed and the polar and azimuth angles are held constant. Here, \(\) parameterizes a single reference point, a per-atom origin. We refer to this last variant as Vector-proj.

Architectural detailsFor the transformations applied in the invariant projected space we consider affine mappings  and monotonic rational-quadratic splines . Additionally, to limit computational cost, we have our GNNs \(h\) output \(M\) sets of reference vectors \(\) and invariant parameters \(\). These parametrize \(M\) core coupling transformations with a single GNN forward pass. For the Cartesian-proj and Spherical-proj flow we include a loss term that discourages certain reference vectors from being collinear, which improves the projection's stability. We provide further details for this and the various projection types in App. B.3.

Center of mass shiftThe shift-CoM transform allows us to apply the aforementioned \((3) S_{n}\) equivariant coupling in the ambient space rather than zero-COM subspace. In particular, before transforming our observed vector \(}}\) with \(\), we lift it onto \(\). We achieve this by swapping the center of mass between \(}\) and \(\). For now, assume \(=\), i.e. \(k=1\), with App. B.4 providing details for \(k>1\). Letting \(}\) be the subspace where all augmented variables that differ by a translation occupy the same point, and \(}}\) be defined analogously to \(}\), we apply the map \(:} }\) which acts on both of its arguments by subtracting from each of them the latter's CoM, that is,

\[(},)(}-},\,-})} _{i=1}^{n}[]^{i}.\] (5)

This operation is invertible, with inverse \((},)\), and has unit Jacobian determinant.

``` Inputs: Zero-CoM observation \(}\), augmented variable \(\), Coupling transforms \(_{1},_{2}\) \((,})(},)\) \((,})_{M}^{(1)}_{1}^{(1)}(,})\) \((,})(},)\) \((,})_{M}^{(2)}_{1}^{(2)}(,})\) Output: \(},\) ```

**Algorithm 1**Flow block \(f\)

Putting the building blocks togetherOur flow transform is built as a sequence of \(L\) blocks. Each block, described in Alg. 1, consists of two equivariant coupling layers, see Fig. 1. Our observations \(}}\) are lifted onto \(\) with \(\), they are transformed with \(M\) core transformations \(_{i}^{(1)}_{i=1}^{M}\), and \(\) is applied one more time to map the observations back to the zero-CoM hyperplane. After this, our augmented variables \(\) are transformed with \(_{i}^{(2)}_{i=1}^{M}\).

Joint density evaluation \(q(,)\) is performed with Alg. 2. We first subtract the observed variables' CoM from both the observed and augmented variables. We then apply our \(L\) flow transform blocks before evaluating the transformed variables' density under our base distribution \(q_{0}\), which is described next. The log determinant of the core flow transform, \(f\), has a contribution from the projection, transform in the invariant space, and inverse projection (see App. B.3 for details).

### \((3)_{n}\) Invariant base distribution

Again, we assume \(=\), i.e. \(k=1\), with the generalization given in App. B.4. Our invariant choice of base distribution is \(q_{0}(,)=}(;\,0,l)\)\((;\,,^{2}I)\) where \(^{3n}\) and \(^{3n}\) refer to \(\) and \(\) flattened into vectors, \(^{2}\) is a hyperparameter and we denote Gaussian distributions on \(}\) as \(}\)(Satorras et al., 2021; Yim et al., 2023) with density

\[}(;\,0,I)=(2)^{-3(n-1)/2}(-\| }\|_{2}^{2}).\] (6)

We sample from it by first sampling from a standard Gaussian \((0,I)\) and then removing the CoM. On the other hand, the distribution for \(\) is supported on \(\) which includes non-zero CoM points. It is centered on \(\), yielding joint invariance to translations. The isotropic nature of \(q_{0}\) makes its density invariant to rotations, reflections, and permutations (Satorras et al., 2021; Yim et al., 2023).

### Training and likelihood evaluation

In this section, we discuss learning and density evaluation with augmented variables.

Invariant augmented target distributionWe assume the density of our observations \(p\) is \((3) S_{n}\) invariant. Our target for augmented variables is \((|)=(;\,,^{2}I)\), where \(^{2}\) matches the variance of the base Gaussian density over \(\). This satisfies joint invariance \(p(g)(g|g)=p()(|)\) for any \(g(3) S_{n}\), as shown in App. B.5.

Learning from samplesWhen data samples \( p\) are available, we train our flow parameters by maximizing the joint likelihood, which is a lower bound on the marginal log-likelihood over observations up to a fixed constant

\[_{ p(),(|)}[ q(, )]_{p(x)}[ q()]+C.\] (7)

Learning from energyWhen samples are not available but we can query the unnormalized energy of a state \(U()\), with \(p()(-U())\), we can minimize the joint reverse KL divergence. Bythe chain rule of the KL divergence, this upper bounds the KL between marginals

\[D_{}(q(,)\,||\,p()(|)) ) D_{}(q()\,||\,p()).\] (8)

However, the reverse KL encourages mode-seeking (Minka, 2005) which may result in the model failing to characterize the full set of meta-stable molecular states. Therefore, we instead use _flow annealed importance sampling bootstrap_ (FAB) (Midgley et al., 2023), which targets the mass covering \(\)-divergence with \(=2\). In particular, we minimize the \(\)-divergence over the joint which leads to an upper bound on the divergence of the marginals

\[D_{2}(q(,)\,||\,p()(|)) )^{2}(|)^{2}}{q(,)}\, \,)^{2}}{q()}\,  D_{2}(q()\,||\,p()).\] (9)

To compute unbiased expectations with the augmented flow we rely on the estimator \(_{p()}[f()]=_{q(,)}[w(,)f()]\) where \(w(,)=p()(|)/q(,)\). Minimizing the joint \(\)-divergence with \(=2\) corresponds to minimizing the variance in the joint importance sampling weights \(w(,)\), which allows for the aforementioned expectation to be approximated accurately.

Evaluating densitiesTo evaluate the marginal density of observations we use the importance weighed estimator \(q()=_{(|)}[,)}{(|)}]\), noting that \(\) is Gaussian and thus supported everywhere. The estimator variance vanishes when \(q(|)=(|)\), as shown in App. B.9.

## 4 Experiments

### Training with samples: DW4, LJ13 and QM9 positional

First, we consider 3 problems that involve only positional information, with no additional features such as atom type or connectivity. Thus, the target densities are fully permutation invariant. The first two of these, namely DW4 and LJ13, are toy problems from Kohler et al. (2020), where samples are obtained by running MCMC on the 4 particle double well energy function (DW4) and 13 particles Leonard Jones energy function (LJ13) respectively. The third problem, i.e. QM9 positional (Satorras et al., 2021), selects the subset of molecules with 19 atoms from the commonly used QM9 dataset (Ramakrishnan et al., 2014) and discards their node features.

For our model, Equivariant Augmented Coupling Flow (E-ACF), we consider all projection types (Vector-proj, Cartesian-proj, Spherical-proj) and compare them to: (1) Non-E-ACF: An augmented flow that is not rotation equivariant but is translation and permutation equivariant, as in (Klein et al., 2023). This model uses the same structure as the E-ACF but replaces the EGNN with a transformer which acts directly on atom positions, without any projection. We train Non-E-ACF with data-augmentation whereby we apply a random rotation to each sample within each training batch. (2) E-CNF ML: The SE(3) equivariant continuous normalizing flow from Satorras et al. (2021) trained by maximum likelihood. (3) E-CNF FM: An SE(3) equivariant continuous normalizing flow trained via flow matching (Lipman et al., 2023; Klein et al., 2023). (4) E-CNF-Diff: An SE(3) equivariant diffusion model (Hoogeboom et al., 2022) evaluated as a continuous normalizing flow. All equivariant generative models use the \((3)\) GNN architecture proposed by Satorras et al. (2021). The Cartesian-proj exhibited numerical instability on QM9-positional causing runs to crash early. To prevent these crashes it was trained at a lower learning rate than the other E-ACF models. App. C.3.1 provides a detailed description of these experiments.

On Tab. 1, we see that on DW4 and LJ13, E-CNF FM performs best, while E-ACF is competitive with E-CNF ML and E-CNF-Diff. We note that both DW4 and LJ13 have biased datasets (see App. C.3.1). On QM9-positional, the E-ACF is competitive with E-CNF-Diff and E-CNF FM, while the under-trained E-CNF ML from Satorras et al. (2021) performs poorly. We expect that with further tuning E-CNF-Diff and E-ACF FM would match the performance of Spherical-proj on QM9-positional. The Non-E-ACF performs much worse than the E-ACF, despite being trained for more epochs, demonstrating the utility of in-build equivariance. Furthermore, Fig. 2 shows that the distribution of inter-atomic distances of samples from our flow matches training data well. Importantly, sampling and density evaluation of the E-ACF on an A100 GPU takes roughly 0.01 seconds. For the CNF trained by flow matching (E-CNF ML) and score matching (E-CNF-Diff), sampling takes on average 0.2, and 5 seconds respectively. Thus, the E-ACF is faster for sampling than the CNF by more than an order of magnitude.

### Training with samples: Alanine dipeptide

Next, we approximate the Boltzmann distribution of alanine dipeptide in an implicit solvent at temperature \(T=800\,\). We train the models via maximum likelihood on samples generated by a replica exchange MD simulation (Mori and Okamoto, 2010), which serve as a ground truth. Our generated dataset consists of \(10^{6}\) samples in the training and validation set as well as \(10^{7}\) samples in the test set. Besides our E-ACF using the different projection schemes that we introduced in Sec. 3.1, we train the non-SO(3) equivariant flow (Non-E-ACF) with data augmentation similarly to the previous experiments. Moreover, we train a flow on internal coordinates as in Midgley et al. (2023). The joint effective sample size (ESS) is reported for the E-ACF models which is a lower bound of the marginal ESS (see Eq. (9)). For the CNFs, the Hutchinson trace estimator is used for the log density (Grathwohl et al., 2018). This results in a biased estimate of the ESS, which may therefore be spurious. Further details about model architectures, training and evaluation are given in App. C.3.2.

The results are shown in Fig. 3 and Tab. 2. All variants of our E-ACF clearly outperform the Non-E-ACF, as the Kullback Leibler divergence (KLD) of the Ramachandran plots is significantly lower and the NLL as well as the reverse and forward ESS, see App. B.10, are higher. The flow trained on internal coordinates is only marginally better regarding the NLL and KLD th

    & KDD & NLL & Rev ESS (\%) & Fwd ESS (\%) \\  Flow on internal coordinates & \((2.01 0.04) 10^{-3}\) & \(-190.15 0.02\) & \(1.61 0.03\) & – \\ E-CNF FM & \((3.83 0.18) 10^{-2}\) & \(\) & \((3.1 0.2) 10^{-4}\) & \((2.5 0.8) 10^{-2}\)\(\) \\ E-CNF-Diff & \((8.86 0.49) 10^{-3}\) & \(-188.31 0.01\) & \((8.1 1.1) 10^{-4}\) & \((5.1 4.1) 10^{-237}\) \\ Non-E-ACF & \((1.66 0.01) 10^{-1}\) & \(-184.57 0.35\) & \(0.14 0.07\) & \((5.5 4.5) 10^{-30}\) \\ Vector-proj E-ACF & \((6.15 1.21) 10^{-3}\) & \(-188.56 0.01\) & \(19.4 13.4\) & \(()}\) \\ Cartesian-proj E-ACF & \((3.46 0.28) 10^{-3}\) & \(-188.59 0.00\) & \(\) & \((9.7 7.9) 10^{-9}\) \\ Spherical-proj E-ACF & \(()}\) & \(-188.57 0.00\) & \(\) & \((5.0 4.1) 10^{-14}\) \\   

Table 2: Alanine dipeptide results. KLD is the empirical KLD of the Ramachandran plots (see Fig. 3). Forward ESS is estimated with the test set. Reverse ESS is estimated with \(10^{5}\) model samples. Results are averaged over 3 seeded runs, with the standard error reported as uncertainty.

    & DW4 & LJ13 & QM9 positional \\  E-CNF ML & \(8.15 N/A\) & \(30.56 N/A\) & \(-70.2 N/A\) \\ E-CNF FM & \(\) & \(\) & \(-129.23 0.38\) \\ E-CNF-Diff & \(8.01 0.03\) & \(31.02 0.12\) & \(-158.30 0.15\) \\ Non-E-ACF & \(10.07 0.03\) & \(33.32 0.34\) & \(-76.76 1.77\) \\ Vector-proj E-ACF & \(8.69 0.03\) & \(30.19 0.12\) & \(-152.23 6.44\) \\ Cartesian-proj E-ACF & \(8.82 0.08\) & \(30.89 0.09\) & \(-138.62 0.74\) \\ Spherical-proj E-ACF & \(8.61 0.05\) & \(30.33 0.16\) & \(\) \\   

Table 1: Negative log-likelihood results for flows trained by maximum likelihood on DW4, LJ13 and QM9-positional. E-CNF ML results are from Satorras et al. (2021). Best results are emphasized in **bold**. The results are averaged over 3 seeded runs, with the standard error reported as uncertainty.

Figure 2: Inter-atomic distances for samples from the train-data and Spherical-proj E-ACF.

the Cartesian-proj and Spherical-proj E-ACF, while we outperform it considering the ESS. Note that the flow on internal coordinates explicitly models the angles \(\) and \(\), while the E-ACF operates on the underlying Cartesian coordinates. The E-ACFs outperform the E-CNF-Diff model in all metrics, but the E-CNF trained with flow matching has a slightly lower NLL while having a significantly higher KLD on the Ramachandran plot. This could be due to a better performance on other marginals or on some outlier data points in the test set. The forward ESS is very low for all models, which suggests that the models do not cover some regions in the target distribution, and that the reverse ESS is spurious. Alternatively, this may be from numerical instabilities in the models. To the best of our knowledge, our models are the first to learn the full Boltzmann distribution of a molecule purely on Cartesian coordinates while being competitive with a flow trained on internal coordinates.

### Energy-based training: DW4 and LJ13

Lastly, we demonstrate that our proposed flow can be trained on the DW4 and LJ13 problems using only the target's unnormalized density with the FAB algorithm (Midgley et al., 2023). The annealed importance sampling procedure within FAB requires sampling from the flow and evaluating its density multiple times. This is used within the training loop of FAB making it significantly more expensive per parameter update than training by maximum likelihood. Given that sampling and density evaluation with CNFs is very expensive, training them with FAB is intractable. Thus, we only report results for our flow, as well as for the Non-E-ACF. We train the Non-E-ACF for more iterations than the E-ACF, such that the training times are similar, given that the Non-E-ACF is faster per iteration. App. C.4 provides further details on the experimental setup.

Figure 3: Ramachandran plots, i.e. marginal distribution of the dihedral angles \(\) and \(\) (see App. C.3.2), obtained with MD (ground truth) and various models.

    &  &  \\  & Rev ESS (\%) & Fwd ESS (\%) & NLL & Rev ESS (\%) & Fwd ESS (\%) & NLL \\  Non-E-ACF & \(35.94 2.63\) & \(5.45 4.10\) & \(7.38 0.01\) & \(5.38 3.66\) & \(4.14 3.10\) & \(33.22 0.96\) \\ Vector-proj E-ACF & \(\) & \(\) & \(\) & \(59.60 1.13\) & \(65.20 1.61\) & \(30.33 0.03\) \\ Cartesian-proj E-ACF & \(82.44 0.50\) & \(80.08 0.64\) & \(7.13 0.00\) & \(60.68 0.41\) & \(65.54 0.37\) & \(30.34 0.01\) \\ Spherical-proj E-ACF & \(80.44 0.88\) & \(81.46 0.95\) & \(7.14 0.00\) & \(\) & \(\) & \(\) \\   

Table 3: Results for training by energy with FAB. Best results are emphasized in **bold**. Results are averaged over 3 seeded runs, with the standard error reported as uncertainty. Reverse ESS is estimated with \(10^{4}\) samples from the flow. Forward ESS is estimated with the test sets.

Tab. 3 shows that the E-ACF trained with FAB successfully approximates the target Boltzmann distributions, with reasonably high joint ESS, and NLL comparable to the flows trained by maximum likelihood. Additionally, the ESS may be improved further by combining the trained flow with AIS, this is shown in App. C.4. In both problems the Non-E-ACF performs worse, both in terms of ESS and NLL. All models trained by maximum likelihood have a much lower ESS (see App. C.3.1). This is expected, as unlike the \(=2\)-divergence loss used in FAB, the maximum likelihood objective does not explicitly encourage minimizing importance weight variance. Furthermore, the flows trained by maximum likelihood use a relatively small, biased training set, which therefore limits their quality.

## 5 Discussion and Related Work

Augmented flows have been used for improving expressiveness of Boltzmann generators (Kohler et al., 2023, 2023, 2023); however, these models were not equivariant. Klein et al. (2023) proposed an augmented normalizing flow architecture to provide conditional proposal distributions for MD simulations and use a coupling scheme similar to ours. However, this model only achieves translation and permutation equivariance and the authors make their flow approximately rotation invariant through data augmentation. In our experiments, we found data augmentation to perform significantly worse than intrinsic invariance. In principle, Klein et al. (2023)'s model could be made fully invariant by substituting in our flow's projection-based coupling transform.

An alternative to our equivariant flow and equivariant CNFs are the equivariant residual flows proposed in Bose et al. (2022). Alas, residual flows require fixed-point iteration for training and inversion. This is expensive and may interact poorly with energy-based training methods such as FAB (Midgley et al., 2023) which require fast exact sampling and densities. Furthermore, Bose et al. (2022) found that the spectral normalization required for residual flows did not interact well with the equivariant CNN architecture in their experiments.

There has been recent progress in improving the sampling speed of CNFs/diffusion models (Tong et al., 2023; Song et al., 2023) and on using these models for sampling from unnormalized densities (Vargas et al., 2023; Zhang and Chen, 2022; Zhang et al., 2023). Thus, in the future CNF/diffusion models trained by energy may prove to be competitive with discrete-time flow based methods.

The strategy of projection into a local reference frame to enforce equivariance has been successfully employed in existing literature, specifically for protein backbone generation (Jumper et al., 2021; Yim et al., 2023). Here we have focused on modelling the full set of Cartesian coordinates of a molecule, but an interesting avenue for future work is to extend our framework to other domains, such as modelling rigid bodies, which has applications to protein backbone generation (Jumper et al., 2021; Yim et al., 2023) and many-body systems (Kohler et al., 2023).

LimitationsAlthough our flow is significantly faster than alternatives such as CNFs, the expensive EGNN forward pass required in each layer of the flow makes it more computationally expensive than flows on internal coordinates. Additionally, we found our flow to be less numerically stable than flows on internal coordinates, which we mitigate via adjustments to the loss, optimizer and neural network (see App. B.3, App. C.1, App. C.2). Our implementation uses the E(3) equivariant EGNN proposed by Satorras et al. (2021). However, recently there have been large efforts towards developing more expressive, efficient and stable EGNNs architectures (Fuchs et al., 2020; Batatia et al., 2022; Musaelian et al., 2023; Liao and Smidt, 2023). Incorporating these into our flow may improve performance, efficiency and stability. This would be especially useful for energy-based training, where the efficiency of the flow is a critical factor.

## 6 Conclusion

We have proposed an SE(3) equivariant augmented coupling flow that achieves similar performance to CNFs when trained by maximum likelihood, while allowing for faster sampling and density evaluation by more than an order of magnitude. Furthermore, we showed that our flow can be trained as a Boltzmann generator using only the target's unnormalized density, on problems where internal coordinates are inadequate due to permutation symmetries, and doing so with a CNF is computationally intractable. It is possible to extend our model to learn the Boltzmann distribution of diverse molecules, by conditioning on their molecular graph, which we hope to explore in the future.