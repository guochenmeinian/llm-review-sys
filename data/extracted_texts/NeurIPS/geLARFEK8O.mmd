# Combating Representation Learning Disparity with Geometric Harmonization

Zhihan Zhou\({}^{1}\) Jiangchao Yao\({}^{1,2}\) Feng Hong\({}^{1}\) Ya Zhang\({}^{1,2}\) Bo Han\({}^{3}\) Yanfeng Wang\({}^{1,2}\)

\({}^{1}\)Cooperative Medianet Innovation Center, Shanghai Jiao Tong University

\({}^{2}\)Shanghai AI Laboratory \({}^{3}\)Hong Kong Baptist University

{zhihanzhou, Sunarker, feng.hong, ya_zhang, wangyanfeng}@sjtu.edu.cn

bhanml@comp.hkbu.edu.hk

###### Abstract

Self-supervised learning (SSL) as an effective paradigm of representation learning has achieved tremendous success on various curated datasets in diverse scenarios. Nevertheless, when facing the long-tailed distribution in real-world applications, it is still hard for existing methods to capture transferable and robust representation. Conventional SSL methods, pursuing _sample-level uniformity_, easily leads to representation learning disparity where head classes dominate the feature regime but tail classes passively collapse. To address this problem, we propose a novel Geometric Harmonization (GH) method to encourage _category-level uniformity_ in representation learning, which is more benign to the minority and almost does not hurt the majority under long-tailed distribution. Specially, GH measures the population statistics of the embedding space on top of self-supervised learning, and then infer an fine-grained instance-wise calibration to constrain the space expansion of head classes and avoid the passive collapse of tail classes. Our proposal does not alter the setting of SSL and can be easily integrated into existing methods in a low-cost manner. Extensive results on a range of benchmark datasets show the effectiveness of GH with high tolerance to the distribution skewness. Our code is available at https://github.com/MediaBrain-SJTU/Geometric-Harmonization.

## 1 Introduction

Recent years have witnessed a great success of self-supervised learning to learn generalizable representation [7; 9; 15; 63]. Such rapid advances mainly benefit from the elegant training on the label-free data, which can be collected in a large volume. However, the real-world natural sources usually exhibit the long-tailed distribution , and directly learning representation on them can lead to the distortion issue of the embedding space, namely, the majority dominates the feature regime  and the minority collapses . Thus, it becomes urgent to pay attention to representation learning disparity, especially as fairness of machine learning draws increasing attention [28; 41; 68; 77].

Different from the flourishing supervised long-tailed learning [29; 46; 68], self-supervised learning under long-tailed distributions is still under-explored, since there is no labels available for the calibration. Existing explorations to overcome this challenge mainly resort to the possible tailed sample discovery and provide the implicit bias to representation learning. For example, BCL  leverages the memorization discrepancy of deep neural networks (DNNs) on unknown head classes and tail classes to drive an instance-wise augmentation. SDCLR  contrasts the feature encoder and its pruned counterpart to discover hard examples that mostly covers the samples from tail classes, and efficiently enhance the learning preference towards tailed samples. DnC  resorts to a divide-and-conquer methodology to mitigate the data-intrinsic heterogeneity and avoid the representationcollapse of minority classes. Liu et al.  adopts a data-dependent sharpness-aware minimization scheme to build support to tailed samples in the optimization. However, few works hitherto have considered the intrinsic limitation of the widely-adopted contrastive learning loss, and design the corresponding balancing mechanism to promote the representation learning parity.

We rethink the characteristic of the contrastive learning loss, and try to understand _"Why the conventional contrastive learning underperforms in self-supervised long-tailed context?"_ To answer this question, let us consider two types of representation uniformity: (1) _Sample-level uniformity_. As stated in , the contrastive learning targets to distribute the representation of data points uniformly in the embedding space. Then, the feature span of each category is proportional to their corresponding number of samples. (2) _Category-level uniformity_. This uniformity pursues to split the region equally for different categories without considering their corresponding number of samples . In the class-balanced scenarios, the former uniformity naturally implies the latter uniformity, resulting in the equivalent separability for classification. However, in the long-tailed distributions, they are different: sample-level uniformity leads to the feature regime that is biased towards the head classes considering their dominant sample quantity and sacrifices the tail classes due to the limited sample quantity. By contrast, category-level uniformity means the equal allocation _w.r.t._ classes, which balances the space of head and tail classes, and is thus more benign to the downstream classification . Unfortunately, there is no support for promoting category-level uniformity in contrastive learning loss, which explains the question arisen at the beginning.

In this study, we propose a novel method, termed as _Geometric Harmonization_ (GH) to combat representation learning disparity in SSL under long-tailed distributions. Specially, GH uses a geometric uniform structure to measure the uniformity of the embedding space in the coarse granularity. Then, a surrogate label allocation is computed to provide a fine-grained instance-wise calibration, which explicitly compresses the greedy representation space expansion of head classes, and constrain the passive representation space collapse of tail classes. The alternation in the conventional loss refers to an extra efficient optimal-transport optimization that dynamically pursues the category-level uniformity. In Figure 1, we give a toy experiment1 to visualize the distribution of the embedding space without and with GH. In a nutshell, our contributions can be summarized as follows,

1. To our best knowledge, we are the first to investigate the drawback of the contrastive learning loss in self-supervised long-tailed context and point out that the resulted sample-level uniformity is an intrinsic limitation to the representation parity, motivating us to pursue category-level uniformity with more benign downstream generalization (Section 3.2).
2. We develop a novel and efficient _Geometric Harmonization_ (Figure 2) to combat the representation learning disparity in SSL, which dynamically harmonizes the embedding space of SSL to approach the category-level uniformity with the theoretical guarantee.
3. Our method can be easily plugged into existing SSL methods for addressing the data imbalance without much extra cost. Extensive experiments on a range of benchmark datasets demonstrate the consistent improvements in learning robust representation with our GH.

Figure 1: Comparison of Geometric Harmonization and the plain SSL method on a 2-D imbalanced synthetic dataset. (Left) Visualization of the 2-D synthetic dataset. (Middle) The embedding distribution of each category learnt by the vanilla contrastive learning loss is approximately proportional to the number of samples, leading to the undesired representation learning disparity. (Right) GH mitigates the adverse effect of class imbalance and approaches to the category-level uniformity.

Related Work

**Self-Supervised Long-tailed Learning.** There are several recent explorations devoted to this direction [40; 28; 59; 41; 77]. BCL  leverages the memorization effect of DNNs to automatically drive an instance-wise augmentation, which enhances the learning of tail samples. SDCLR  constructs a self-contrast between model and its pruned counterpart to learn more balanced representation. Classic Focal loss  leverages the loss statistics to putting more emphasis on the hard examples, which has been applied to self-supervised long-tailed learning . DnC  benefits from the parameter isolation of multi-experts during the divide step and the information aggregation during the conquer step to prevent the dominant invasion of majority. Liu et al.  proposes to penalize the sharpness surface in a reweighting manner to calibrate class-imbalance learning. Recently, TS  employs a dynamic strategy on the temperature factor of contrastive loss, harmonizing instance discrimination and group-wise discrimination. PMSN  proposes the power-law distribution prior, replacing the uniform prior, to enhance the quality of learned representations.

**Hyperspherical Uniformity.** The distribution uniformity has been extensively explored from the physic area, _e.g._, Thomson problem [58; 53], to machine learning area like some kernel-based extensions, _e.g._, Riesz s-potential [21; 43] or Gaussian potential [12; 4; 62]. Some recent explorations regarding the features of DNNs [49; 17; 47] discover a terminal training stage when the embedding collapses to the geometric means of the classifier _w.r.t._ each category. Specially, these optimal class means specify a perfect geometric uniform structure with clear geometric interpretations and generalization guarantees [78; 67; 31]. In this paper, we first extend this intuition into self-supervised learning and leverage the specific structure to combat the representation disparity in SSL.

## 3 Method

### Problem Formulation

We denote the dataset \(\), for each data point \((,)\), the input \(^{m}\) and the associated label \(\{1,,L\}\). Let \(N\) denote the number of samples, \(=N_{max}/N_{min}\) denote the imbalanced ratio (IR), where \(N_{max},N_{min}\) is the number of samples in the largest and smallest class, respectively. Let \(n_{i}\) denote the number of samples in class \(i\). In SSL, the ground-truth \(\) can not be accessed and the goal is to transform an image to an embedding via DNNs, _i.e._, \(f_{}:^{m}^{d}\). In the linear probing evaluation, we construct a supervised learning task with balanced datasets. A linear classifier \(g()\) is built on top of the frozen \(f_{}()\) to produce the prediction, _i.e._, \(g(f_{}())\).

### Geometric Harmonization

As aforementioned, most existing SSL methods in long-tailed context leverage the contrastive learning loss, which encourages the sample-level uniformity in the embedding space. Considering the intrinsic limitation illustrated in Figure 1, we incorporate the geometric clues from the embedding space to calibrate the current loss, enabling our pursuit of category-level uniformity. In the following, we first introduce a specific geometric structure that is critical to Geometric Harmonization.

**Definition 3.1**.: (Geometric Uniform Structure). Given a set of vertices \(^{d K}\), the geometric uniform structure satisfies the following between-class duality

\[_{i}^{}_{j}=C,\ \  i,j\{1,2,,K\},\ i  j,\] (1)

where \(\|_{i}\|=1, i\{1,2,,K\}\), \(K\) is the number of geometric vertices and \(C\) is a constant.

Above structure provides a characteristic that any two vectors in \(\) have the same angle, namely, the unit space are equally partitioned by the vectors in \(\). This fortunately follows our expectation about category-level uniformity. Specially, if we use \(\) as a constant classifier to involve into training, and have the oracle label \(\) of the long-tailed data (\(K=L\)) to supervise the below prediction

\[=p(|f_{}(),)=(_{y}^{ } f_{}()/_{})/(_{i=1}^{K} (_{i}^{} f_{}()/_{} )),\] (2)

then according to the neural collapse theory , the representation of all samples will fit the geometric uniform structure of \(\) in the limit, namely, approach category-level uniformity. However,the technical challenge is the complete absence of annotation \(\) in our context, making directly constructing the objective \([]\) with Eq. (2) impossible to combat the representation disparity.

**Surrogate Label Allocation.** To address the problem of unavailable labels, we explore constructing the surrogate geometric labels \(}\) to supervise the training of Eq. (2). Concretely, we utilize the recent discriminative clustering idea  to acquire such geometric labels, formulated as follows

\[_{}=[}_{1},,}_{N}]}_{ }=-|}_{_{i}}}_{i}_{i},\ \ \ \ \ }_{N}=N,\ }^{ }_{K}=_{N},\] (3)

where \(}_{i}\) refers to the soft assignment constrained in a \(K\)-dimensional probability simplex and \(_{+}^{K}\) refers to the distribution constraint. As we are dealing with long-tailed data, we propose to use the geometric uniform structure \(\) to automatically compute the population statistic of the embedding space as \(\). Finally, Eq. (3) can be analytically solved by _Sinkhorn-Knopp algorithm_ (refer to Appendix D for Algorithm 1). Note that, the above idea builds upon a conjecture: the constructed surrogate geometric labels are mutually correlated with the oracle labels so that they have the similar implication to approach category-level uniformity. We will empirically verify the rationality of such an assumption via _normalized mutual information_ in Section 4.4.

**Overall Objective.** Eq. (3) can be easily integrated into previous self-supervised long-tailed learning methods for geometric harmonization, _e.g.,_ SDCLR  and BCL . For simplicity, we take their conventional InfoNCE loss  as an example and write the overall objective as follows

\[_{,}}=_{}+w_{ }_{},\] (4)

where \(w_{}\) represents the weight of the geometric harmonization loss. Optimizing Eq. (4) refers to a bi-level optimization style: in the outer-loop, optimize \(_{}}_{}\) with fixing \(\) to compute the surrogate geometric labels; in the inner-loop, optimize \(_{}_{}+_{}\) with fixing \(}\) to learn the representation model. The additional cost compared to the vanilla method primarily arises from from the outer-loop, which will be discussed in Section 3.4 and verified in Section 4.4.

Figure 2: **Overview of Geometric Harmonization** (\(w\)/ InfoNCE). To achieve harmonization with the category-level uniformity, we require ground-truth labels for supervision during training. However, these labels are not available under the self-supervised paradigm. Moreover, estimating surrogate labels directly from the geometric uniform structure is challenging and noisy, especially when the representation is not ideally distributed. To fullfill this gap, we utilize the geometric uniform structure to measure the embedding space, and the captured population statistics are used for an instance-wise calibration by surrogate label allocation, which provides a supervision feedback to counteract the sample-level uniformity. Specially, our method are theoretically grounded to approach category-level uniformity at the loss minimum. The additional model parameters incurred by GH are analytically and empirically demonstrated to be trained in an efficient manner.

Compared with previous explorations , the uniqueness of GH lies in the following three aspects: (1) _Geometric Uniform Structure_. The pioneering works mainly resort to a learnable classifier to perform clustering, which can easily be distorted in the long-tailed scenarios . Built on the geometric uniform classifier, our method is capable to provide high-quality clustering results with clear geometric interpretations. (2) _Flexible Class Prior_. The class prior \(\) is assumed to be uniform among the previous attempts. When moving to the long-tailed case, this assumption will strengthen the undesired sample-level uniformity. In contrast, our methods can potentially cope with any distribution with the automatic surrogate label allocation. (3) _Theoretical Guarantee_. GH is theoretically grounded to achieve the category-level uniformity in the long-tailed scenarios (refer to Section 3.3), which has never been studied in previous methods. To gain more insights into our method, we further compare GH with discriminative clustering methods (SeLA , SwAV ) and investigate the impact of various components in GH. From the results in Table 1, we can see that GH consistently outperforms the vanilla discriminative clustering baselines in the linear probing evaluation. Notably, we observe that both GUS and the class prior \(\) play a critical role in our GH.

### Theoretical Understanding

Here, we reveal the theoretical analysis of GH on promoting the representation learning to achieve category-level uniformity instead of sample-level uniformity. Let us begin with a deteriorated case of sample-level uniformity under the extreme imbalance, _i.e._, minority collapse .

**Lemma 3.2**.: _(Minority collapse) Assume the samples follow the uniform distribution \(n_{1}=n_{2}==n_{L_{H}}=n_{H}\), \(n_{L_{H}+1}=n_{L_{H}+2}==n_{L}=n_{T}\) in head and tail classes respectively. Assume \(d L\) and \(n_{H}/n_{T}+\), the lower bound (Lemma C.1) of \(_{}\) achieves the minimum when the class means of the tail classes collapse to an identical vector:_

\[_{i}-_{j}=_{L},\; L _{H} i j L,\]

_where \(_{l}=}_{i=1}^{n_{l}}f_{}(_{l,i})\) denotes the class means and \(_{l,i}\) is the \(i\)-th data point with label \(l\)._

This phenomenon indicates all representations of the minority will collapse completely to one point without considering the category discrepancy, which aligns with our observation regarding the passive collapse of tailed samples in Figure 1. To further theoretically analyze GH, we first quantitatively define category-level uniformity in the following, and then theoretically claim that with the geometric uniform structure (Definition. 3.1) and the perfect aligned allocation (Eq. (3)), we can achieve the loss minimum at the stage of realizing category-level uniformity.

**Definition 3.3**.: (Categorical-level Uniformity) We define categorical-level uniformity on the embedding space _w.r.t_ the geometric uniform structure \(\) when it satisfies

\[_{k}^{*}=_{k},\; k=1,2,,K,\]

where \(_{k}^{*}=}_{i=1}^{n_{k}}f_{}^{*}( _{k,i})\) represents the class mean for samples assigned with the surrogate geometric label \(k\) in the embedding space.

**Theorem 3.4**.: _(Optimal state for \(\)) Given Eq. (4) under the proper optimization strategy, when it arrives at the category-level uniformity (Definition 3.3) defined on the geometric uniform structure \(\) (Definition 3.1), we will achieve the minimum of the overall loss \(^{*}\) as_

\[^{*}=-2_{l=1}^{L}_{l}^{} (1/(1+(K-1)(C-1)))+(J/L),\] (5)

_where \(J\) denotes the size of the collection of the negative samples and \(}}\) refers to the marginal distribution of the latent ground-truth labels \(\)._

This guarantees the desirable solution with the minimal intra-class covariance and the maximal inter-class covariance under the geometric uniform structure , which benefits the downstream generalization. Notably, no matter the data distribution is balanced or not, our method can persistently maintain the theoretical merits on calibrating the class means to achieve category-level uniformity. We also empirically demonstrate the comparable performance with GH on the balanced datasets in Section 4.4, as in this case category-level uniformity is equivalent to sample-level uniformity.

   IR & SimCLR & +SeLA & +SwAV & +GH & _w/o_ & _w/o_ \(\) \\  
100 & 50.7 & 50.5 & 52.2 & 54.0 & 53.3 & 53.1 \\
50 & 52.2 & 52.0 & 53.0 & 55.4 & 54.6 & 54.4 \\
10 & 55.7 & 56.0 & 56.1 & 57.4 & 56.7 & 56.4 \\   

Table 1: Linear probing of vanilla discriminative clustering methods and variants of GH on CIFAR-100-LT.

### Implementation and Complexity analysis

In Algorithm 2 of Appendix D, we give the complete implementation of our method. One point that needs to be clarified is that we learn the label allocation \(}\) in the mini-batch manner. In addition, the geometric prediction \(\) and the adjusted \(}\) are computed at the beginning of every epoch as the population-level statistic will not change much in a few mini-batches. Besides, we maintain a momentum update mechanism to track the prediction of each sample to stabilize the training, _i.e._, \(^{m}^{m}+(1-)\). When combined with the joint-embedding loss, we naturally adopt a cross-supervision mechanism \([}^{+}]\) for the reconciliation with contrastive baselines. The proposed method is illustrated in Figure 2 for visualization.

For complexity, assume that the standard optimization of deep neural networks requires forward and backward step in each mini-batch update with the time complexity as \((B)\), where \(B\) is the mini-batch size and \(\) is the parameter size. At the parameter level, we add an geometric uniform structure with the complexity as \((BKd)\), where \(K\) is the number of geometric labels and \(d\) is the embedding dimension. For Sinkhorn-Knopp algorithm, it only refers to a simple matrix-vector multiplication as shown in Algorithm 1, whose complexity is \((E_{s}(B+K+BK))\) with the iteration step \(E_{s}\). The complexity incurred in the momentum update is \((BK)\). Since \(K,d\) and \(E_{s}\) are significantly smaller than the model parameter \(\) of a million scale, the computational overhead involved in GH is negligible compared to \((B)\). The additional storage for a mini-batch of samples is the matrix \(^{m}^{K B}\), which is also negligible to the total memory usage. To the end, GH incurs only a small computation or memory cost and thus can be plugged to previous methods in a low-cost manner. The empirical comparison about the computational cost is summarized in Table 17.

## 4 Experiments

### Experimental Setup

**Baselines.** We mainly choose five baseline methods, including (1) _plain contrastive learning_: SimCLR , (2) _hard example mining_: Focal , (3) _asymmetric network pruning_: SDCLR , (4) _multi-expert ensembling_: DnC , (5) _memorization-guided augmentation_: BCL . Empirical comparisons with more baseline methods can be referred to Appendix F.3.

**Implementation Details.** Following previous works [28; 77], we use ResNet-18  as the backbone for small-scale dataset (CIFAR-100-LT ) and ResNet-50  for large-scale datasets (ImageNet-LT , Places-LT ). For experiments on CIFAR-100-LT, we train model with the SGD optimizer, batch size 512, momentum 0.9 and weight decay factor \(5 10^{-4}\) for 1000 epochs. For experiments on ImageNet-LT and Places-LT, we only train for 500 epochs with the batch size 256 and weight decay factor \(1 10^{-4}\). For learning rate schedule, we use the cosine annealing decay with the learning rate \(0.5 1e^{-6}\) for all the baseline methods. As GH is combined with baselines, a proper warming-up of 500 epochs on CIFAR-100-LT and 400 epochs on ImageNet-LT and Places-LT are applied. The cosine decay is set as \(0.5 0.3\), \(0.3 1e^{-6}\) respectively. For hyper-parameters of GH, we provide a default setup across all the experiments: set the geometric dimension \(K\) as 100, \(w_{ GH}\) as 1 and the temperature \(_{ GH}\) as 0.1. In the surrogate label allocation, we set the regularization coefficient \(\) as 20 and Sinkhorn iterations \(E_{s}\) as 300. Please refer to Appendix E.3 for more experimental details.

**Evaluation Metrics.** Following [28; 77], _linear probing_ on a balanced dataset is used for evaluation. We conduct full-shot evaluation on CIFAR-100-LT and few-shot evaluation on ImageNet-LT and Places-LT. For comprehensive performance comparison, we present the linear probing performance and the standard deviation among three disjoint groups, _i.e._, [many, medium, few] partitions .

### Linear Probing Evaluation

**CIFAR-100-LT.** In Table 2, we summarize the linear probing performance of baseline methods _w_/ and _w/o_ GH on a range of benchmark datasets, and provide the analysis as follows.

(1) _Overall Performance_. GH achieves the competitive results _w.r.t_ the [many, medium, few] groups, yielding a overall performance improvements averaging as 2.32\(\%\), 2.49\(\%\) and 1.52\(\%\) on CIFAR-100-LT with different imbalanced ratios. It is worth noting that on the basis of the previous state-of-the-art BCL, our GH further achieves improvements by 1.20\(\%\), 1.82\(\%\) and 1.22\(\%\), respectively. Our

[MISSING_PAGE_FAIL:7]

lenging classes. As a result, we observe that the standard deviation of our GH does not significantly decrease on Places-LT, which requires more effort and exploration for improvement alongside GH.

### Downstream Finetuning Evaluation

**Downstream supervised long-tailed learning.** Self-supervised learning has been proved to be beneficial as a pre-training stage of supervised long-tailed recognition to exclude the explicit bias from the class imbalance [68; 41; 77].

To validate the effectiveness of our GH, we conduct self-supervised pre-training as the initialization for downstream supervised classification tasks on CIFAR-100-LT-R100, ImageNet-LT and Places-LT. The state-of-the-art logit adjustment  is chosen as the downstream baseline. The combination of GH + LA can be interpreted as a compounded method where GH aims at the re-balanced representation extraction and LA targets the classifier debiasing. In Table 3, we can find that the superior performance improvements are achieved by self-supervised pre-training over the plain supervised learning baseline. Besides, our method can also consistently outperform other SSL baselines, averaging as 1.00\(\%\), 0.63\(\%\) and 1.24\(\%\) on CIFAR-100-LT-R100, ImageNet-LT and Places-LT. These results demonstrate that GH are well designed to facilitate long-tailed representation learning and improve the generalization for downstream supervised tasks.

**Cross-dataset transfer learning**. To further demonstrate the representation transferability of our GH, we conduct more comprehensive experiments on the large-scale, long-tailed dataset CC3M  with various cross-dataset transferring tasks, including downstream classification, object detection and instance segmentation. Specifically, we report the finetuning classification performance on ImageNet, Places and fine-grained visual datasets Caltech-UCSD Birds (CUB200) , Aircrafts , Stanford Cars , Stanford Dogs , NABirds . Besides, we evaluate the quality of the learned representation by finetuning the model for object detection and instance segmentation on COCO2017 benchmark . As shown in Tables 4 and 5, we can see that our proposed GH consistently outperforms the baseline across various tasks and datasets. It further demonstrates the importance of considering long-tailed data distribution under large-scale unlabeled data in the pre-training stage. This can potentially be attributed to that our geometric harmonization motivates a more balanced and general embedding space, improving the generalization ability of the pretrained model to a range of real-world downstream tasks.

### Further Analysis and Ablation Studies

**Dimension of Geometric Uniform Structure.** As there is even no category number \(L\) available in SSL paradigm, we empirically compare our GH with different geometric dimension \(K\) on CIFAR-100-LT-R100, as shown in Figure 3(a). From the results, GH is generally robust to the change of

    &  &  \\   & ImageNet & Places & CUB200 & Aircraft & StanfordCars & StanfordDogs & NABirds & Average \\  SimCLR & 52.06 & 37.65 & 44.61 & 65.89 & 57.63 & 50.99 & 46.86 & 53.20 \\ +GH & **53.39** & **38.47** & **45.76** & **68.08** & **60.24** & **52.88** & **47.58** & **54.91** \\   

Table 4: Image classification on ImageNet, Places and fine-grained visual classification on various fine-grained datasets, pretrained on large-scale long-tailed CC3M and then finetuned.

    &  &  &  \\    & & SimCLR & +GH & Focal & +GH & SDCLR & +GH & DnC & +GH & BCL & +GH & \\   CIFAR-LT & 46.61 & 49.81 & 50.84 & 49.83 & 51.04 & 49.79 & 50.73 & 49.97 & 50.84 & 50.38 & 51.32 & **+1.00** \\ ImageNet-LT & 48.27 & 51.10 & 51.67 & 51.15 & 51.82 & 50.94 & 51.64 & 51.31 & 51.88 & 51.43 & 52.06 & **+0.63** \\ Places-LT & 27.07 & 32.63 & 33.86 & 32.69 & 33.75 & 32.55 & 34.03 & 32.98 & 34.09 & 33.15 & 34.48 & **+1.24** \\   

Table 3: Supervised long-tailed learning by finetuning on CIFAR-100-LT, ImageNet-LT and Places-LT. We compare the performance of five self-supervised learning methods as the pre-training stage for downstream supervised logit adjustment  method. Improv. (\(\)) represents the averaging performance improvements _w.r.t._ different baseline methods. Besides, the performance of logit adjustment via learning from scratch is also reported for comparisons.

\(K\), but slightly exhibits the performance degeneration when the dimension is extremely large or small. Intuitively, when \(K\) is extremely large, our GH might pay more attention to the uniformity among sub-classes, while the desired uniformity on classes is not well guaranteed. Conversely, when \(K\) is extremely small, the calibration induced by GH is too coarse that cannot sufficiently avoid the internal collapse within each super-class. For discussions of the structure, it can refer to Appendix B.

**Surrogate Label Quality Uncovered.** To justify the effectiveness of surrogate label allocation, we compare the NMI scores  between the surrogate and ground-truth label in Figure 3(b). We observe that GH significantly improves the NMI scores across baselines, indicating that the geometric labels are effectively calibrated to better capture the latent semantic information. Notably, the improvements of the existing works are marginal, which further verifies the superiority of GH.

**Exploration with Other Label-Distribution Prior.** To further understand \(\), we assume the ground-truth label distribution is available and incorporate the oracle \(}\) into the surrogate label allocation. Comparing the results of the softened variants \(}_{}}\) with the temperature \(_{}\) in Figure 3(c), we observe that GH outperforms all the counterparts equipped with the oracle prior. A possible reason is that our method automatically captures the inherent geometric statistics from the embedding space, which is more reconcilable to the self-supervised learning objectives.

**Uniformity Analysis.** In this part, we conduct experiments with two uniformity metrics :

\[=_{i=1}^{L}_{j=1,j i}^{L}||_{i} -_{j}||_{2},\ \ _{k}=_{i=1}^{L}_{j_{1},,j_{k}}(_{m=1}^ {k}||_{i}-_{j_{m}}||_{2}),\]

where \(j_{1},,j_{k} i\) represent different classes. Specifically, U evaluates average distances between different class centers and U\({}_{k}\) measures how close one class is to its neighbors. As shown in Table 6, our GH outperforms in both inter-class uniformity and neighborhood uniformity when compared with the baseline SimCLR . This indicates that vanilla contrastive learning struggles to achieve the uniform partitioning of the embedding space, while our GH effectively mitigates this issue.

**Comparison with More SSL Methods.** In Table 7, we present a more comprehensive comparison of different SSL baseline methods, including MoCo-v2 , MoCo-v3  and various non-contrastive methods such as SimSiam , BYOL  and Barlow Twins . From the results, we can see that the combinations of different SSL methods and our GH can achieve consistent performance improvements, averaging as 2.33\(\%\), 3.18\(\%\) and 2.21\(\%\) on CIFAR-100-LT. This demonstrates the prevalence of representation learning disparity under data imbalance in general SSL settings.

    &  &  &  &  \\   & AP\({}^{}_{0}\) & AP\({}^{}_{75}\) & AP\({}^{}_{75}\) & AP\({}^{mask}\) & AP\({}^{mask}_{00}\) & AP\({}^{mask}_{75}\) & SimCLR & +GH & SimCLR & +GH \\  SimCLR & 31.7 & 51.0 & 33.9 & 30.2 & 49.8 & 32.1 & C100 & 1.00 & **2.80** & 0.72 & **2.00** \\
4GH & **32.7** & **52.2** & **35.2** & **31.1** & **50.8** & **33.0** & C10 & 1.18 & **2.60** & 0.85 & **1.83** \\   

Table 5: Object detection and instance segmentation with Table 6: Inter-class uniformity (\(\)) and finetuned features on COCO2017 benchmark, pretrained neighborhood uniformity (\(\)) of pretrained on large-scale long-tailed CC3M.

Figure 3: (a) Linear probing performance _w.r.t._ the dimension \(K\) of the geometric uniform structure \(\) (Appendix B) on CIFAR-LT-R100. (b) NMI score between the surrogate geometric labels and the ground-truth labels in the training stage on CIFAR-LT-R100. (c) Average linear probing and the error bars of the surrogate label allocation with variants of the label prior on CIFAR-LT-R100.

**On Importance of Bi-Level Optimization.** In Table 8, we empirically compare the direct joint optimization strategy to Eq. (4). From the results, we can see that the joint optimization (_w_/ or _w_/_o_ the warm-up strategy) does not bring significant performance improvement over SimCLR compared with that of our bi-level optimization, probably due to the undesired collapse in label allocation . This demonstrates the necessity of the proposed bi-level optimization for Eq. (4) to stabilize the training.

**Qualitative Visualization.** We conduct t-SNE visualization of the learned features to provide further qualitative intuitions. For simplify, we randomly selected four head classes and four tail classes on CIFAR-LT to generate the t-SNE plots. Based on the results in Figure 4, the observations are as follows: (1) SimCLR: head classes exhibit a large presence in the embedding space and heavily squeeze the tail classes, (2) GH: head classes reduce their occupancy, allowing the tail classes to have more space. This further indicates that the constructed surrogate labels can serve as the high-quality supervision, effectively guiding the harmonization towards the geometric uniform structure.

**Sensitivity Analysis.** To further validate the stability of our GH, We conduct empirical comparison with different weight \(w_{}\), temperature \(_{}\), regularization coefficient \(\) and Sinkhorn iteration \(E_{s}\) on CIFAR-LT, as shown in Figures 5 and 7. From the results, we can see that our GH can consistently achieve satisfying performance with different hyper-parameter.

## 5 Conclusion

In this paper, we delve into the defects of the conventional contrastive learning in self-supervised long-tail context, _i.e._, representation learning disparity, motivating our exploration on the inherent intuition for approaching the category-level uniformity. From the geometric perspective, we propose a novel and efficient Geometric Harmonization algorithm to counteract the long-tailed effect on the embedding space, _i.e._, over expansion of the majority class with the passive collapse of the minority class. Specially, our proposed GH leverages the geometric uniform structure as an optimal indicator and manipulate a fine-grained label allocation to rectify the distorted embedding space. We theoretically show that our proposed method can harmonize the desired geometric property in the limit of loss minimum. It is also worth noting that our method is orthogonal to existing self-supervised long-tailed methods and can be easily plugged into these methods in a lightweight manner. Extensive experiments demonstrate the consistent efficacy and robustness of our proposed GH. We believe that the geometric perspective has the great potential to evolve the general self-supervised learning paradigm, especially when coping with the class-imbalanced scenarios.