# Topology-Aware Uncertainty for Image Segmentation

Saumya Gupta\({}^{1}\) Yikai Zhang\({}^{2}\) Xiaoling Hu\({}^{1,3}\) Prateek Prasanna\({}^{1}\) Chao Chen\({}^{1}\)

\({}^{1}\)Stony Brook University, NY, USA \({}^{2}\)Morgan Stanley, NY, USA

\({}^{3}\)Athinoula A. Martinos Center for Biomedical Imaging,

Massachusetts General Hospital and Harvard Medical School, MA, USA

Email: saumgupta@cs.stonybrook.edu

###### Abstract

Segmentation of curvilinear structures such as vasculature and road networks is challenging due to relatively weak signals and complex geometry/topology. To facilitate and accelerate large scale annotation, one has to adopt semi-automatic approaches such as proofreading by experts. In this work, we focus on uncertainty estimation for such tasks, so that highly uncertain, and thus error-prone structures can be identified for human annotators to verify. Unlike most existing works, which provide pixel-wise uncertainty maps, we stipulate it is crucial to estimate uncertainty in the units of topological structures, e.g., small pieces of connections and branches. To achieve this, we leverage tools from topological data analysis, specifically discrete Morse theory (DMT), to first capture the structures, and then reason about their uncertainties. To model the uncertainty, we (1) propose a joint prediction model that estimates the uncertainty of a structure while taking the neighboring structures into consideration (inter-structural uncertainty); (2) propose a novel Probabilistic DMT to model the inherent uncertainty within each structure (intra-structural uncertainty) by sampling its representations via a perturb-and-walk scheme. On various 2D and 3D datasets, our method produces better structure-wise uncertainty maps compared to existing works. Code available at https://github.com/Saumya-Gupta-26/struct-uncertainty

## 1 Introduction

Curvilinear segmentation is an essential initial step in various medical and non-medical applications, involving the precise extraction of fine-scale structures, such as blood vessels, nerves, and other elongated objects . For example, extraction of retinal vasculature is an essential precursor to understanding disease progression and assessing therapeutic effects . In civil engineering, road network and railway track segmentation can support urban planning and transportation system optimization . Despite the success of deep learning , automatic segmentation of thin structures remains challenging due to their relatively low visibility and complex topology. Existing segmentation methods often make topological errors such as broken connections or missing branches.

As a cost-efficient alternative, in many applications, one employs semi-automatic techniques, e.g., iterative proofreading by human annotators . On the other hand, proofreading of complex fine-scale structures can be extremely time-consuming . This necessitates a better strategy to direct the annotators' attention towards locations that are more error-prone. Following the classic active learning principle , we may estimate the _uncertainty_ and concentrate on the locations where a neural network is the least certain.

Despite many existing studies on segmentation uncertainty , most existing uncertainty estimation methods do not apply to curvilinear structure segmentation. Existing methods typically generate pixel-wise uncertainty maps. Such maps highlight pixels along the boundary of all structures,regardless of their width or thickness (see Fig. 1(d)). This offers limited information for human annotators; a desirable uncertainty map should instead highlight the error-prone "structures", e.g., small vessels/branches or short stretches of roads that tend to be disconnected or missed.

In this paper, we propose a new topology-aware uncertainty estimation method that highlights error-prone structures as a whole (such as in Fig. 1(f)). By highlighting structures with high uncertainty, our method empowers annotators to accept or reject/correct structural proposals efficiently, thus streamlining the proofreading process. To capture the uncertainty of a given segmentation network's prediction at a structural level, we require the realization of two key components: a) decompose the prediction into a set of constituent structures, including false positives and false negatives, and b) estimate uncertainties of all the structures. Furthermore, we need to consider two types of structural uncertainty, intra-structural and inter-structural. The _intra-structural uncertainty_ of a structure is due to its intrinsic composition, e.g., geometry, intensity, and the segmentation network's confidence. The _inter-structural uncertainty_ is more contextual; it is due to interactions between neighboring structures. Our method explicitly models the two types of uncertainty.

Given a segmentation model, our method uses its likelihood map (Fig. 2(i)) plus the input image to estimate structural uncertainties. First, we obtain a structural decomposition of the prediction, i.e., a collection of one-pixel-wide pieces/structures (see Fig. 2(ii)). Each structure represents a potential branch/connection according to the segmentation model. We employ the principles of the classic discrete Morse theory (DMT) . Intuitively, DMT treats the likelihood function as a terrain function and extracts landscape features, e.g., mountain ridges or valleys, as structures. Note we are capturing all possible structures visible in the likelihood map, including the ones that do not appear in the segmentation due to low probability.

Next, we estimate uncertainties for all these structures. Existing uncertainty estimation approaches often sample multiple hypotheses and calculate the variance across them . However, this principle is not feasible in our problem; with \(N\) structures, the space of all their combinations is of exponential size (\(2^{N}\)). Sampling from such a space is very challenging. An alternative is to make independent uncertainty estimation on each structure. However, this is also suboptimal as it ignores inter-structural uncertainty. Fig. 2(vi) shows three false positive structures. Treating all structures independently will incorrectly assign the horizontal structure with low uncertainty (see Fig. 2(vii)).

We propose a joint inference model (i.e., a graph neural network ) to jointly predict uncertainties on all the structures. This joint inference framework avoids explicit enumeration/sampling over the exponential size space of hypotheses. It also takes into account the inter-structural uncertainty. In Fig. 2(viii), our method correctly assigns high uncertainties to all three false positive structures. To supervise the training process, we use the attenuation loss proposed in  to learn the uncertainty. As there is no 'uncertainty label', it is implicitly learned from the loss function.

An important contribution in this paper is a novel _Probabilistic DMT_ modeling the intra-structural uncertainty, due to the geometry, topology, as well as the segmentation model's confidence. The classic DMT cannot capture the inherent uncertainty of a structure; it produces a single representation for the structure, i.e., a one-pixel-wide skeleton. However, this skeleton is computed in a deterministic manner. Such deterministic computation is rigid and fails to capture possible variations of a structure.

Instead, our Probabilistic DMT (Prob. DMT) represents each structure as sample skeletons drawn from an underlying generative process guided by the likelihood map (the skeleton resulting from the

Figure 1: Motivating examples for structure-wise uncertainty. In the segmentation result (c), orange highlights a false positive structure, and pink highlights a false negative. Methods (d)-(f) are uncertainty estimates of the prediction in (c). PHiSeg  assigns pixels along boundaries as uncertain. Hu et al.  captures uncertainty at a structural level, but produces overconfident maps (assigns zero uncertainty to many structures). Ours produces better structure-wise uncertainty estimates: both the highlighted false positive/negative structures have high uncertainty.

original DMT being one of the samples). As illustrated in Fig. 2(iii), the original DMT generates a skeleton that significantly deviates from the true structure due to the uncertainty inherent in the likelihood. In contrast, Prob. DMT effectively captures potential variations (Fig. 2(iv)), offering valuable insights into the impact of uncertainty on the structural composition. The greater the variation, the greater the intra-structural uncertainty. During training, we repeatedly sample skeletons for each structure, and feed the samples to the joint inference model for uncertainty prediction. Indeed, as shown in Fig. 2(iv), sampling multiple skeletons also leads to a better chance of uncovering the true structure. This observation inspires us to use the uncertainty estimation method to re-calibrate the original segmentation model and achieve even better segmentation performance.

We note that the method in  (which we refer to as Hu et al.) also used DMT to decompose structures and estimate their uncertainty. However, this method used the classic DMT to deterministically generate skeletons, and thus failed to model intra-structural uncertainty. Furthermore, instead of joint inference, the method sampled from all configurations; and to reduce the computational burden, it pruned the exponential-sized configuration space using a saliency measure called persistence [9; 64; 71]. The pruning was very coarse, thus resulting in suboptimal uncertainty estimation. As illustrated in Fig. 1(e), Hu et al. produces overconfident maps; most structures, including many false negatives and false positives, are assigned zero uncertainty. In contrast, our method produces much better uncertainty estimates (Fig. 1(f)), owing to the proper modeling of both intra-structural and inter-structural uncertainties.

We summarize the main contributions of this paper as follows:

1. We propose a novel method to estimate the uncertainty of a given segmentation network at _a structural level_.
2. We propose Probabilistic DMT, a probabilistic method to generate structural variations and to capture intra-structural uncertainty.
3. We propose a joint prediction model on all the structures in order to capture inter-structural uncertainty.

Empirical evaluation shows that our method achieves much better uncertainty estimates on both 2D and 3D datasets, outperforming existing methods.

## 2 Related work

**Topology-guided image segmentation.** Several works focus on maintaining the correct connectivity or topology of thin structures. Topology-aware loss functions [46; 62; 7; 27; 22; 26] impose per-pixel constraints to improve topological integrity. Discrete Morse theory has also been used to improve the topological awareness of segmentation networks [29; 9; 11; 56; 71; 3]. These approaches use topological tools to improve segmentation at a pixel level, which is a weaker constraint compared to the structural level. In contrast, our method performs joint reasoning directly over the structures.

**Uncertainty quantification.** In recent years, there has been significant work on uncertainty quantification (UQ) of deep neural networks [1; 20; 36]. Here we review UQ techniques tailored for semantic segmentation. _Pixel-wise uncertainty:_ Semantic segmentation is a per-pixel classification

Figure 2: (a) Intra-structural uncertainty: In the likelihood map (i), we highlight a false negative (FN) structure missed by the segmentation network. In (ii), we show the skeletons representing structures from classic DMT. In (iii), we highlight the GT (green) and the incorrect DMT skeleton (red) for the FN structure. In (iv), we show skeleton samples by Prob. DMT (blue); (b) Inter-structural uncertainty (_inter_): a retinal image with very weak signal (v). The likelihood map (vi) shows three potential false positive (_FP_) structures, two vertical and one horizontal. Without inter-structural uncertainty (vii), the horizontal structure has high confidence. With inter-structural uncertainty (viii), the horizontal structure gets higher uncertainty influenced by the two vertical structures.

task and naturally most UQ methods produce per-pixel uncertainty estimates. In , the authors propose a Bayesian framework using MC dropout  and a learned loss attenuation to respectively capture model and data uncertainty. Recent methods have turned to generative models to generate multiple hypotheses, and the per-pixel variance across the hypotheses is treated as uncertainty. Some works in this direction are an ensemble of \(M\) networks , a single network with \(M\) heads , Prob.-UNet , and PHiSeg . Prob.-UNet integrates a conditional variational autoencoder  with UNet , generating multiple hypotheses via latent variable sampling. PHiSeg extends this by introducing latent variables at every UNet level, thereby producing more diverse samples. _Structure-wise uncertainty:_ Methods such as [41; 60] compute structure (volume) uncertainty by averaging over the pixel-wise uncertainty estimates. The method closest to ours is Hu et al. . It is a generative model derived from Prob.-UNet where the latent variable has meaning in topology (specifically, a global persistence threshold). This threshold severely limits the structure space, overlooking several false positive/negative structures. Thus they tend to produce overconfident uncertainty estimates.

## 3 Method

Given a trained segmentation network, our goal is to capture the uncertainty of its prediction at a structural level. Note that we do not modify the network in any way; instead, we propose an external module that reasons the uncertainty of each structure in the segmentation. Fig. 3 provides an overview of our method. Let \(F_{}\) denote the trained segmentation network, and \(M_{}\) denote our proposed external uncertainty quantification framework. \(M_{}\) takes as input the likelihood map of \(F_{}\) and the input image. It generates a set of structures, and estimates an uncertainty value for each of them. During training, \(M_{}\) is trained by comparing with the ground truth (GT) annotation.

\(M_{}\) consists of two primary modules to capture intra-structural and inter-structural uncertainty. The first module, Probabilistic DMT (Prob. DMT), generates structures based on the likelihood map. For each structure, it samples a set of skeletons representing different variations. Details are provided in Sec. 3.1. The second module jointly predicts the uncertainties of all the structures. At each training iteration, it takes one sample skeleton for each structure, plus the likelihood map and input image, as input. Details are described in Sec. 3.2. Throughout the sections, we consider one data sample \((x,y)\) where \(x\) is an input image and \(y\) is the segmentation GT. The likelihood map is \(f=F_{}(x)\).

### Modelling the structural space

In this section, we first describe how DMT obtains the constituent structures of a likelihood map. Then we propose our Prob. DMT formulation to capture intra-structural uncertainty.

Figure 3: An overview of the proposed method \(M_{}\). The given segmentation network \(F_{}\) has frozen weights. Probabilistic DMT decomposes the likelihood into structures, and samples skeleton representations of each. A graph is then constructed over the structures to perform joint reasoning of their uncertainty. The training is supervised by comparing with the GT (via the loss \(L_{UQ}\), red arrow).

**Discrete Morse theory.** Consider the likelihood map \(f\) generated from the segmentation network \(F_{}\). We wish to decompose \(f\) into a set of structures, capturing not only the salient structures but also the faint ones. In the segmentation map, salient and faint structures broadly correspond to true positive and false negative structures. In Fig. 4(b), we highlight the false negative (FN) structures. These structures are missed in the segmentation, but will be captured by DMT (Fig. 4(d)).

DMT treats the likelihood map \(f\) as a terrain function, decomposing it into a _Morse complex_ consisting of critical points, paths connecting them, patches in between paths, and volumes enclosed by patches (for 3D images). _Critical points_ are locations \(w\) with zero gradients (\( f(w)=0\)), i.e., minima, maxima, or saddle points. Paths, called _V-paths_, are routes connecting critical points via the non-critical ones. A V-path connecting a saddle point to a maxima is called a stable manifold. These stable manifolds are the underlying terrain's mountain ridges, and delineate structures of interest. In Fig. 4(c), we show the locations of saddles and maximas in the Morse complex, and in Fig. 4(d), we show the union of all the stable manifolds connecting them. In this paper, we only focus on the zero- and one-dimensional Morse structures, i.e., the union of all stable manifolds and their associated saddle and maxima. We call the collection of such structures the Morse skeleton.

By default, DMT generates stable manifolds in a completely deterministic manner, failing to take into account the intra-structural uncertainty in the likelihood \(f\). Therefore, these stable manifolds may fail to correctly delineate the true structure, as shown in Fig. 5.

**Probabilistic DMT.** To account for the inherent uncertainty, we explicitly model the structure as a collection of samples from an underlying generative process. The skeleton from the original DMT is just one possibility out of many. The method is achieved via a perturb-and-walk algorithm, in which we iteratively perturb the likelihood map, and regenerate the skeleton.

The rationale is that the likelihood map is a weighted aggregation of all possible skeleton representations. To inverse the aggregation and recover these skeletons is challenging. Instead, we follow the classic _perturb-and-map principle_, which was used to efficiently sample from a complex discrete graphical model distribution . We randomly perturb the likelihood function. For each perturbed likelihood, we compute a skeleton as a sample. See Fig. 5 for an illustration. The sampled skeletons will reflect the uncertainty properly. For a structure that is less salient in the likelihood map, the sample skeletons will have large variations, generating a large uncertainty. For a salient structure in the likelihood map, the sample skeletons will be less variant, resulting in a low uncertainty.

Assume a given likelihood function \(f\) and one of its structures, represented by a V-path \(e\) connecting a saddle-maximum pair \((c_{s},c_{m})\). We generate a sample skeleton of the structure by first perturbing the likelihood with random noise. Next, we generate a path connecting \(c_{s}\) and \(c_{m}\). Recall in the original DMT, the skeleton is generated by following the mountain ridge. In other words, we start from the saddle point, and "walk" towards the maximum. At every step, we always walk to the neighboring pixel with the highest likelihood value. In Prob. DMT, we follow the same principle on the perturbed likelihood. However, the noisy perturbation of likelihood can cause the path to grow astray. Therefore, we additionally apply a distance-based regularizer to guide the walk towards the target \(c_{m}\). We describe the process in detail below.

Let \(e\) denote the structure obtained by following the V-path between \((c_{s},c_{m})\) in the original DMT. In order to generate its sample skeleton \(\), we first draw a likelihood \(f_{n}\) from a distribution centered on

Figure 4: Orange indicates FN structures; (c) shows saddle points (red) and maximas (blue), and omits minimas; (d) shows the union of the stable manifolds of the saddle points.

Figure 5: Structures (#1,#2) sampled from the distribution. Green arrow is path chosen using \(Q(c^{})\); red arrow is next step w/o considering \(Q_{d}(c^{})\).

as \(f_{n} f+r\). This process is independent of the perturbation model \(r\) used, and we use a Gaussian model in this work. As the variance of the Gaussian model is unknown, we use Bayesian probability theory to sample the variance from the Inverse Gamma distribution (its conjugate prior ).

Once we obtain \(f_{n}\), we regenerate the path between \((c_{s},c_{m})\). We take inspiration from random walk  as well as probability regularized walk  to generate the variant structure \(\) from \(f_{n}\). Our walk algorithm continuously grows \(\) starting from \(c_{s}\) and ending at \(c_{m}\), one pixel at a time. The algorithm considers both the terrain \(f_{n}\) and the distance to the destination \(c_{m}\) to ensure path completeness. During the walk, given the current pixel location \(c\), the next location \(c^{}\) is chosen as \(c^{}=(Q(c^{}))\), where, \(c^{}(c)\)2 and \(Q(c^{})= Q_{d}(c^{})+(1-)f_{n}(c^{})\), and, \(Q_{d}(c^{})=-c^{}\|_{2}}\). We begin with \(c c_{s}\) and continue in this manner \(c c^{}\) till we reach \(c_{m}\)3. In Fig. 5, we show a deterministic structure obtained from DMT along with sample variations produced by our method. We demonstrate the intermediate steps in the algorithm: the red arrow denotes the next step without considering the distance regularizer \(Q_{d}\), while the green arrow denotes the next step using our formulation \(Q\). Notice how only considering \(f_{n}\) without \(Q_{d}\) can prevent the path from reaching \(c_{m}\). We thus require \(Q_{d}\) to guide the path to completeness.

The structure \(\) is a different realization of \(e\), making each run of the Prob. DMT a stochastic one. We are thus able to explicitly model the structures as samples from a probability distribution. We also note that DMT is a special case of Prob. DMT when \(r=0\) and \(=0\). In practice, with some probability, we consider the original structure \(e\) from DMT over generating its variant \(\). Specifically, following a Bernoulli distribution, with a small probability \(u\) we retain \(e\), while with probability \(1-u\) we sample its variant \(\) using the perturb-and-walk algorithm outlined above. This process is done separately and in parallel for every structure. The structures taken together form a Morse skeleton. The output of Prob. DMT is effectively one sample skeleton from the space of Morse skeletons. We provide further information regarding Prob.DMT in the Appendix: A outlines the pseudocode, B discusses the hyperparameters, and C reports its computational complexity.

### Joint estimation of structural uncertainty

The Prob. DMT module gives us a set \(E\) of structures. Our final step is to jointly reason about the uncertainty of all of them. To achieve this, we use a regression network that takes as input each structure \(e E\), and outputs whether it is a true positive and the uncertainty of \(F_{}\) in predicting it.

**Details of the network.** Structures interact with each other in the image space and are not independent. During uncertainty estimation, it is therefore crucial to consider their spatial context, i.e., interstructural uncertainty. Hence, we use Graph Neural Networks (GNN) , specifically Graph Convolution Networks (GCN) , to jointly reason about the structures and capture the high-order spatial interactions. In the graph, each node represents a structure, and edges between nodes exist when corresponding structures have non-zero overlaps (typically at endpoints). The input feature vector for each node is constructed as shown in Fig. 6. For every structure, we first concatenate \([x^{c},f^{c},m]\), where \(x^{c}\) comes from the original input \(x\); \(f^{c}\) from the likelihood map \(f\) (not \(f_{n}\)); and \(m\) is a binary map indicating the presence of the structure. These \(x^{c},f^{c},m\) are smaller crops/bounding boxes centered on the structure. After passing them through convolution blocks, we apply channel-wise pooling to obtain a fixed-length feature vector for training. We further concatenate the persistence value of the saddle point associated with the original DMT structure (aka stable manifold). Persistence value (from persistent homology ) is defined as the difference of function (likelihood) values of 2 critical cells (saddle-maxima pair). It captures the importance of a structure, thus making it a valuable feature in our framework. Note that we do not use the perturbed \(f_{n}\) from the Prob. DMT method when constructing the feature vector.

**Training the network.** We train the regression network using the attenuation loss proposed in . As there are no labels to learn uncertainty, it is implicitly learned during regression optimization. We fix a Gaussian likelihood, and so variance \(^{2}\) is used as a measure of uncertainty. The network's head is split into two -- to predict \((e)\) of being a true positive structure and its associated uncertainty \(^{2}_{e}\). For numerical stability, we actually predict the log variance \(s_{e}=^{2}_{e}\). The training loss is given in Eq. 1. The structures that we obtain from Prob. DMT may not always fully overlap with the true GT structures, that is, some structures may only have partial overlap. We thus do not impose any hard constraints in Eq. 1, instead, \(z_{e}\) is a soft label, and is given by: \(z_{e}=( y m)/( m)\), where \(y\) is the GT and \(\) is the Hadamard product. This value simply represents the proportion of the structure that overlaps with the GT, i.e., the fraction of the structure that is a true positive.

\[L_{UQ}()=_{ e E}((e)-z_{e}||^{2}}{)}}+s_{e})\] (1)

In , \(^{2}\) denoted the pixel-wise uncertainty of the framework's input. In our setting, the input to our framework is \(f=F_{}(x)\), and so \(^{2}\) is modeled to capture the structure-wise uncertainty inherent in data \(x\) and model \(F_{}\). Training \(^{2}\) in this manner ensures that the network does not trivially predict high or low uncertainty, rather, predicts an uncertainty estimate that is dependent on the input.

### Proposed module \(M_{}\)

For Eq. 1 to hold, we require \(M_{}\) to be a probabilistic network. We already show in Sec. 3.1 our formulation for Prob. DMT. Additionally, the regression network is also probabilistic as we use MC dropout .

**Inference procedure.** We take \(T\) runs of \(M_{}\) and compute the uncertainty as the mean \(^{2}_{e}=_{t=1}^{T}(^{2}_{e})_{t}\). We similarly obtain \((e)\) from \((e)\). In Fig. 7, we illustrate the post-processing steps to obtain the structure-wise uncertainty heatmap. First, we obtain maps \(=_{e}\) and \(^{2}=^{2}_{e}\) having the same spatial resolution as the input \(x\). We then binarize \(\), and overlay it onto the segmentation map obtained from \(F_{}\). We do this because Prob. DMT gives us one-pixel wide skeleton structures but we need to recover the structure thickness. Next, we use shortest distance to assign uncertainty values from \(^{2}\) to the pixels in the overlaid map. The shortest distance uses paths only along the foreground pixels. In Fig. 7 we show how we obtain the final uncertainty heatmap from the skeleton heatmap. We also note that the overlaid map is an additional output of our method: it is an improved segmentation map that can be used instead of the one obtained by \(F_{}\). We provide more details in Appendix D.

## 4 Experiments

**Datasets.** We evaluate our method on four datasets: **DRIVE**, **ROSE**, **ROADS** and **PARSE 2022 Grand Challenge**[39; 70]. The DRIVE dataset contains 2D retinal vasculature; ROSE is a 2D retinal OCTA (Optical Coherence Tomography Angiography) segmentation dataset, ROADS is a large non-medical dataset containing aerial images, and PARSE contains 3D CT scans of pulmonary arteries. We further describe the datasets and the data splits in Appendix E.

**Baselines.** We broadly split our comparison baselines into three types: a) Standard vessel segmentation methods: **UNet**, **DeepVesselNet**, and **CS\({}^{2}\)-Net**; b) Pixel-wise uncertainty estimation methods: **Prob.-UNet**, and **PHiSeg**; c) Structure-wise uncertainty estimation method: **Hu et al.**. Implementation details are provided in Appendix F.

**Evaluation metrics.** To evaluate the quality of uncertainty quantification, we use **Expected Calibration Error (ECE)** and **Reliability Diagrams (RD)**. Furthermore, we also evaluate the segmentation on metrics such as **DICE**, **clDice**, **ARI**, **VOI**, **Betti Number error** and **Betti Matching error**. We include clDice, ARI, VOI, Betti Number and Betti

Figure 6: Construction of the input feature vector for each node (structure) in the GCN.

Figure 7: Post-processing procedure.

Matching as they are topology-based metrics and hence are sensitive to the performance on thin structures. Detailed definitions are present in Appendix G.

### Results

Tab. 1 shows the quantitative results against uncertainty methods, and Tab. 2 shows the quantitative results on different backbone architectures. We show the respective qualitative results in Fig. 8 and Fig. 10. In Fig. 9, we plot the Reliability Diagrams. We also perform the unpaired **t-test** (95% confidence interval) to determine the statistical significance. Each table reports the mean and standard deviations for every metric, with statistically significant better performances in bold and numerically better (but not significant) performances in italics. For all the probabilistic methods, the average of five runs was used. For our method, we generated the structure-wise uncertainty estimates and the segmentation map by following the steps outlined in the 'Inference procedure' in Sec. 3.3. Due to space constraints, results on two metrics Betti Number and Betti Matching are reported in Appendix H. We discuss the remaining performances below.

**Performance of uncertainty estimation.** Tab. 1 shows that our method outperforms others on both ECE and segmentation metrics. Fig. 9 displays RDs, with our method following the ideal line much closely compared to others. This is because we explicitly model the distribution of the structures, thereby quantifying the uncertainty of the segmentation network. In Fig. 8, we also see that our method generates better fidelity structure-wise uncertainty maps compared to Hu et al. Our heatmaps

Figure 8: Qualitative results compared to the uncertainty baselines. We show uncertainty estimates in the form of a heatmap. Green highlights false negatives and yellow highlights false positives. Row 1: DRIVE; Row 2: ROSE; Row 3: ROADS; Row 4: PARSE (3D render).

Figure 9: Reliability diagrams of samples from each dataset.

[MISSING_PAGE_FAIL:9]

### Ablation studies

To demonstrate the efficacy of the proposed method, we conduct ablation studies of the different components in our pipeline, as well as check the effect of changing hyperparameter values. We also include ablation studies on the dimensionality of the input feature vector, and size of the crops/bounding boxes (which we report in Appendix I). All analyses are on the DRIVE dataset using UNet  as the backbone.

**Ablation of different modules.** We conduct ablation studies on both parts of our framework: structure generation (DMT vs Prob. DMT), and regression network (GNN vs Multi-layer Perceptron (MLP)). The results are in Tab. 3. Prob. DMT results in a sharp improvement in ECE compared to the original DMT; this supports our assertion that Prob. DMT models intra-structural uncertainty. Similarly, using GNN over MLP results in improvement. The message-passing in GNNs accounts for inter-structural uncertainty, thus yielding higher fidelity uncertainty estimates.

**Effect of hyperparameters.** Our main hyperparameters are \(u,,,\), with \(u\) used in the Bernoulli distribution, \(\) in the path-generation algorithm, and (shape \(\), scale \(\)) as prior hyperparameters of the Inverse Gamma distribution. We achieve the best ECE when \(u=0.3,=0.2,=2.0,=0.01\), however, a reasonable range always yields improvement, thus demonstrating the robustness of the method. We provide results of testing different hyperparameter values in Appendix I.

## 5 Conclusion

In this work, we propose to quantify the structure-wise uncertainty of a given segmentation network. Our framework explicitly models structures as samples from a probability distribution, thus helping to estimate intra-structural uncertainty. Furthermore, we incorporate inter-structural uncertainty by jointly reasoning over the structures, resulting in better fidelity uncertainty estimates. This structure-wise uncertainty quantification can streamline the proofreading process by reducing the time spent finding and correcting errors. Extensive experiments show the practical applicability of our method over different segmentation backbones and datasets. We further discuss the broader impact of our work and the limitations in Appendix J and Appendix K respectively.

**Acknowledgements.** This research was partially supported by grants NSF 2144901, NCI R21CA258493, and NIH 1R21CA258493-01A1. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

 
**Dataset** & **Method** & **Dice\(\)** & **cIDice\(\)** & **ARI\(\)** & **VOIL** \\   & UNet  & 0.7728 \(\) 0.0336 & 0.7586 \(\) 0.0405 & 0.7530 \(\) 0.0519 & 0.3697 \(\) 0.0329 \\  & UNet  + Ours & **0.7906 \(\) 0.0195** & **0.7974 \(\) 0.0327** & **0.7906 \(\) 0.0301** & **0.3322 \(\) 0.0229** \\   & DeepVessel  & 0.8015 \(\) 0.0260 & 0.7997 \(\) 0.0431 & 0.7792 \(\) 0.0457 & 0.3413 \(\) 0.0256 \\  & DeepVesselNet  + Ours & **0.8173 \(\) 0.0190** & **0.8285 \(\) 0.0361** & **0.8037 \(\) 0.0361** & **0.3238 \(\) 0.0192** \\   & C\({}^{25}\)Net  & 0.8189 \(\) 0.0176 & 0.8125 \(\) 0.0413 & 0.8204 \(\) 0.0495 & 0.3417 \(\) 0.0203 \\  & CS\({}^{25}\)-Net  + Ours & **0.8301 \(\) 0.0172** & **0.8367 \(\) 0.0305** & **0.8495 \(\) 0.0301** & **0.3243 \(\) 0.0258** \\   & UNet  & 0.7375 \(\) 0.0197 & 0.6453 \(\) 0.0165 & 0.7206 \(\) 0.0426 & 0.8488 \(\) 0.0126 \\  & UNet  + Ours & **0.7593 \(\) 0.0171** & **0.6782 \(\) 0.0119** & **0.7837 \(\) 0.0314** & **0.7403 \(\) 0.0239** \\   & DeepVesselNet  & 0.7653 \(\) 0.0101 & 0.6634 \(\) 0.0192 & 0.7622 \(\) 0.0302 & 0.7426 \(\) 0.0163 \\  & DeepVesselNet  + Ours & **0.7795 \(\) 0.0205** & **0.6873 \(\) 0.0195** & **0.7936 \(\) 0.0282** & **0.7164 \(\) 0.0226** \\   & CS\({}^{25}\)-Net  & 0.7623 \(\) 0.0285 & 0.6799 \(\) 0.0127 & 0.7702 \(\) 0.0322 & 0.7236 \(\) 0.0157 \\  & CS\({}^{25}\)-Net  + Ours & **0.7866 \(\) 0.0209** & **0.6968 \(\) 0.0149** & **0.7981 \(\) 0.0211** & **0.7072 \(\) 0.0168** \\   & UNet  & 0.7011 \(\) 0.0426 & 0.7918 \(\) 0.0679 & 0.7143 \(\) 0.0526 & 0.5832 \(\) 0.0345 \\  & UNet  + Ours & **0.7461 \(\) 0.0364** & **0.8496 \(\) 0.0455** & **0.7601 \(\) 0.0349** & **0.5463 \(\) 0.0218** \\   & DeepVesselNet  & 0.7518 \(\) 0.0345 & 0.8248 \(\) 0.0574 & 0.7923 \(\) 0.0441 & 0.5611 \(\) 0.0321 \\  & DeepVesselNet  + Ours & **0.7673 \(\) 0.0324** & **0.8513 \(\) 0.0519** & **0.8139 \(\) 0.0464** & **0.5357 \(\) 0.0329** \\   & CS\({}^{25}\)-Net  & 0.7539 \(\) 0.0366 & 0.8341 \(\) 0.0511 & 0.8197 \(\) 0.0426 & 0.5475 \(\) 0.0468 \\   & CS\({}^{25}\)-Net  + Ours & **0.7692 \(\) 0.0372** & **0.8559 \(\) 0.0528** & **0.8368 \(\) 0.0419** & **0.5261 \(\) 0.0441** \\   & UNet  & 0.5905 \(\) 0.3661 & 0.6104 \(\) 0.0727 & 0.6590 \(\) 0.0552 & 1.9738 \(\) 0.0414 \\  & UNet  + Ours & _0.6190 \(\) 0.30826_ & **0.6231 \(\) 0.0613** & **0.6658 \(\) 0.0461** & **0.8701 \(\) 0.0332** \\   & DeepVesselNet  & 0.7208 \(\) 3.0452 & 0.6801 \(\) 0.0354 & 0.6923 \(\) 0.0524 & 0.4907 \(\) 0.0710 \\   & DeepVesselNet  + Ours & _0.7376 \(\) 3.1863_ & **0.6983 \(\) 0.0622** & **0.7098 \(\) 0.0613** & **0.4711 \(\) 0.0613** \\   & CS\({}^{25}\)-Net  & 0.7630 \(\) 3.9415 & 0.6918 \(\) 0.0695 & 0.7138 \(\) 0.0695 & 0.4273 \(\) 0.0521 \\   & CS\({}^{25}\)-Net  + Ours & _0.7720 \(\) 2.8109_ & **0.7113 \(\) 0.0689** & **0.7343 \(\) 0.0733** & **0.4078 \(\) 0.0642** \\  

Table