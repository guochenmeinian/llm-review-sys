# One Less Reason for Filter-Pruning:

Gaining Free Adversarial Robustness with Structured Grouped Kernel Pruning

Shaochen (Henry) Zhong\({}^{1}\), Zaichuan You\({}^{*2}\), Jiamu Zhang\({}^{*2}\), Sebastian Zhao\({}^{*3}\), Zachary LeClaire\({}^{2}\), Zirui Liu\({}^{1}\), Daochen Zha\({}^{1}\), Vipin Chaudhary\({}^{2}\), Shuai Xu\({}^{2}\), and Xia Hu\({}^{1}\)

* Equal contribution. Order determined alphabetically by last names. {shaochen.zhong, zirui.liu, daochen.zha, xia.hu}@rice.edu

###### Abstract

Densely structured pruning methods utilizing simple pruning heuristics can deliver immediate compression and acceleration benefits with acceptable benign performances. However, empirical findings indicate such naively pruned networks are extremely fragile under simple adversarial attacks. Naturally, we would be interested in knowing if such a phenomenon also holds for carefully designed modern structured pruning methods. If so, then to what extent is the severity? And what kind of remedies are available? Unfortunately, both questions remain largely unaddressed: no prior art is able to provide a thorough investigation on the adversarial performance of modern structured pruning methods (spoiler: it is not good), yet the few works that attempt to provide mitigation often do so at various extra costs with only to-be-desired performance.

In this work, we answer both questions by fairly and comprehensively investigating the adversarial performance of 10+ popular structured pruning methods. Solution-wise, we take advantage of _Grouped Kernel Pruning (GKP)_'s recent success in pushing densely structured pruning freedom to a more fine-grained level. By mixing up kernel smoothness -- a classic robustness-related kernel-level metric -- into a modified GKP procedure, we present a one-shot-post-train-weight-dependent GKP method capable of advancing SOTA performance on both the benign and adversarial scale, while requiring no extra (in fact, often less) cost than a standard pruning procedure. Please refer to our GitHub repository for code implementation, tool sharing, and model checkpoints.

## 1 Introduction

Convolutional neural networks (CNNs) have demonstrated solid performance on tasks centered around computer vision. However, with modern CNNs growing in both widths and depths, the issue of over-parameterization has drawn increasing attention due to such networks often requiring large computational resources and memory capacity. To mitigate the burden, network pruning -- the study of removing redundant parameters from original networks without significant performance loss -- has become a popular approach for its simplicity and directness (LeCun et al., 1989; Blalock et al., 2020; He and Xiao, 2023).

Despite the popularity of the pruning field in general, **few prior arts have been available to provide _improved adversarial robustness_ under the constraint of _(densely) structured pruning_**; even though empirical findings show vanilla structured pruning methods implemented with naive pruning strategies often experience huge performance drop on such adversarial tasks (Wang et al., 2018; Sehwag et al., 2020; Vemparala et al., 2021). More concerning, no prior art has made an effort to provide a comprehensive investigation on whether the same phenomenon also exists under carefully designed modern structured pruning methods, where such methods are often capable of delivering excellent benign accuracy retention after pruning (sometimes, even improvements).

Below, we provide a walk-through of why a densely structured method and having an adversarially robust pruned model are preferable and important, to how we developed our solution by leveraging the power of increased structural pruning freedom (grouped kernel pruning) with kernel-level metrics (kernel smoothness). In the later sections of this paper, we replicate and evaluate around 13 popular densely structured pruning methods and variants against various white box (evasion) adversarial attacks, where our proposed method showcases clear dominance.

### Structured v.s. Unstructured Pruning: Accuracy-Efficiency Trade-off

Most of the existing CNN pruning methods can be roughly categorized into _structured_ and _unstructured_ pruning. Note we said _roughly_ because there is no universally agreed delineation between structured and unstructured pruning methods. The general consensus is that methods considered more unstructured often enjoy a higher degree of freedom on where to apply their pruning strategies (e.g., weight-level pruning) and therefore result in better accuracy retention.

In contrast, structured pruning methods often prune weights in a grouped manner following some kind of architecturally defined constraints (e.g., filter-level pruning). Compared to their unstructured counterparts, structured pruning methods are more hardware-friendly and easier to obtain compression and acceleration on commodity hardware, though at the cost of worse accuracy retention. This difference is due to unstructurally pruned networks' tendency to have pruned (zeroed) parameters randomly distributed in the weight matrix, leading to poor data reuse and locality (Yang et al., 2018). Such unstructurally pruned networks struggle to have wall-clock time speed up without supports like custom-indexing, special operation design, sparse operation libraries, or even dedicated hardware setups (Yang et al., 2018; Han et al., 2016; He and Xiao, 2023).

Among all structured pruning methods, one popular line of research is to produce pruned networks that are entirely dense, a.k.a. _densely structured, where the pruned weights are stored in the normal dense tensor format_. Such format of a pruned network is considered to be most library/hardware-friendly and, therefore, most deployable in a practical context. With such significant benefits, densely structured pruning methods consist of the absolute majority of structured pruning methods.

Densely structured pruning methods come with different pruning granularities, where a significant portion of prior arts prune at a filter or channel level. These two types of pruning are historically considered to be the limit of densely structured pruning, as showcased in Figure 1: if we go down one more level, we will have kernel pruning; however, its pruned network is not dense. This is until recently, authors from Zhong et al. (2022) utilized kernel pruning with grouped convolution, where they prune at a grouped kernel level to ensure an entirely dense pruned network. To the best of our knowledge, grouped kernel pruning carries the highest degree of pruning freedom among all densely structured pruning methods.

Figure 1: Visualization of different pruning granularities.

### Pruned Models Are Fragile Under Adversarial Attacks -- But Why Do We Care?

Empirical findings like Wang et al. (2018) suggest that although pruned neural networks may have acceptable benign accuracy, they are often more vulnerable to adversarial attacks. Adversarial robustness is recognized as a long-standing metric to evaluate model quality, as a model with undesired adversarial robustness can be easily exploited to produce wrong and potentially harmful output, resulting in fairness and accountability issues.

**We would argue such robustness properties are especially valued under the context of (structured) pruning**, where pruned models are often deployed to resource-constraint devices with less central oversight available and requiring execution in a more real-time manner. Imagine if an OCR model for real-time check redeeming can be maliciously exploited to read the number \(1\) as \(9\); the result will surely be unpleasant for many parties involved.

To alleviate such a problem (though not under a pruning context), prior arts like Wang et al. (2020) demonstrate the adversarial robustness of a convolutional network is largely correlated to its sensitiveness to high-frequency components (HFC), where such sensitiveness can be mitigated with some simple kernel-level operations like kernel smoothness. More on this in Section 2.

### Solution: Grouped Kernel Pruning with Adversarial-Robustness-Boosting Kernel Metrics

With the recent _Grouped Kernel Pruning (GKP)_ framework pushing the pruning freedom of densely structured pruning to a (close) kernel-level (Zhong et al., 2022), we explore the unique possibility of mixing up adversarial-robustness-boosting kernel metrics -- such as kernel smoothness -- into the procedure of GKP. We present Smoothly Robust Grouped Kernel Pruning (SR-GKP), a densely structured pruning method that works in a simple post-train one-shot manner, but is often capable of delivering competitive benign performance and much stronger adversarial performance against SOTA filter and channel pruning methods that require more sophisticated procedures. Solution-wise, our main claims and contributions are:

* **Free improvement on adversarial robustness.** Our method has no extra (in fact, often less) cost compared to a standard pruning method, making the gained adversarial robustness entirely free.

- fine-tune procedure, where all excessive components are pruned all at once before fine-tuning. This means it is compatible with any trained CNNs (as it does not interfere with the training pipeline) yet straightforward to execute, as the pruning procedure before fine-tuning does not rely on access to data but only the trained model's parameters. * **Rraise attention to the important but overlooked field of adversarially robust structured pruning.** Our method is among the few structured pruning methods capable of delivering pruned networks with improved adversarial performance -- a field with severe problems, but receives little recognition or solutions.

On the investigation side, we are the first to comprehensively reveal:

* **Drastic adversarial performance difference under a similar benign report.** We found that while different carefully designed modern densely structured pruning methods may showcase similar benign performance, most are done so at the cost of adversarial robustness.
* **One less reason for filter/channel pruning: further endorsing GKP**. Filter and channel pruning methods have dominated the field of densely structured pruning for years; our work -- together with Zhong et al. (2022) and Park et al. (2023) -- showcased that when done right, grouped kernel pruning-based methods are superior under both benign and adversarial tasks, making it a promising direction for future densely structured pruning research.

For added bonuses, we are the first ones to reproduce and comprehensively report the benign and adversarial performances of multiple structured pruning methods under a fair setting. We believe the lack of such fair and comprehensive reports (on both benign and adversarial tasks) is mainly due to the lack of user-friendly tools. Thus, alongside our method implementation and checkpoint files, we also provide the pruning community **a lightweight open-sourced tool capable of a plug-and-play style of testing different victim models with various adversarial attacks while supporting all procedures a modern pruning method may require**.

## 2 Related Work and Discussion

Due to the page limitation, we will discuss related work regarding white box evasion adversarial attacks, adversarially robust structured pruning, and grouped kernel pruning. Other related topics, such as the compression/acceleration implications of structured and unstructured pruning methods, input component frequency with kernel smoothness, as well as other related ML efficiency and AI safety literature, will be introduced in Appendix B.

Adversarial Attacks.Neural networks are known to be vulnerable to adversarial attacks, i.e., a small perturbation applied to the inputs can mislead models to make wrong predictions Szegedy et al. (2014); Goodfellow et al. (2014). In practice, popular adversarial attacks can often be categorized as white-box and black-box evasion attacks. The difference being white-box attacks have access to the entirety of the model, including input features, architectures, and model parameters, while black-box attacks' access is often constrained (e.g., only input-output pair). Thus, white-box attacks are almost always more effective and efficient than black-box ones; in fact, many classic black-box attacks are constructed in a way to approximate the information that is directly accessible by white-box attacks (e.g., gradient) (Chen et al., 2020). We opt for white-box attacks for the scope of this paper, given one significant use case of structurally pruned models is edge-device deployments, where the model is more likely to be accessed. Also, white-box attacks generally offer a more straightforward workflow with harder challenges posed.

Structured Pruning for Adversarial Robustness.Structured pruning methods, which arguably carry the most practical significance, have been heavily studied throughout the years (Molchanov et al., 2017; Yu et al., 2018; He et al., 2019; Wang et al., 2019, 2019, 2019, 2018, 2021, 2021, 2022). Despite their popularity, few of them focus on adversarial robustness. To the best of our knowledge, there are only four prior arts presenting structured pruning methods while claiming improved performance on adversarial robustness metrics (Vemparala et al., 2021; Ye et al., 2019; Sehwag et al., 2020; Zhao and Wressnegger, 2023). Unfortunately, Vemparala et al. (2021) does not have a public repository for code, Ye et al. (2019) does not have any experiment on standard BasicBlock ResNets for comparative investigation despite their popularity. (Blalock et al., 2020); Sehwag et al. (2020) and Zhao and Wressnegger (2023) mostly propose unstructured methods with only a few structurally pruned ablation studies conducted on limited model-dataset combinations, with some of their structured pruning implementation not published.

The lack of traffic, infrastructure, or baseline in this area has undoubtedly created deterrents to all interested scholars. To fill the gap, **we provide the community an open-sourced toolkit capable of testing various victim models against different adversarial attacks under a pruning context, coming with 10+ popular structured pruning methods already integrated for comparative evaluation. Model checkpoints are also available for direct evaluations.**

Grouped Kernel Pruning.Our work relies on the GKP framework -- particularly inspired by the recent work of Zhong et al. (2022). The core of GKP is grouped kernel pruning and reconstruction to a grouped convolution format, where this combination has been explored a few times under the context of structure pruning.

Specifically, Zhong et al. (2022) proposed their take on grouped kernel pruning with a three-stage procedure: filter clustering, generating then deciding which grouped kernel pruning strategy to employ, then reconstructing to grouped convolution format via permutation. This particular framework solved the previous drawback of requiring a complex procedure, yet a rich set of experiment results was showcased to demonstrate its performance advantages against many SOTA structured pruning methods. Our proposed method is largely enabled by the extra pruning freedom that GKP provides.

Outside of Zhong et al. (2022), concurrent work like Zhang et al. (2022) and follow-up work like Park et al. (2023) have showcased the excellent benign performance of different GKP implementations under a benign context.

## 3 Proposed Method

### Motivation: Pruning May Amplify Overfitting to High-Frequency Components

Wang et al. (2020) suggests CNNs are prone to overfitting high-frequency components (HFC) of inputs -- a type of feature that is not robust yet can be easily replicated with adversarial perturbations. Though Wang's finding is towards an unpruned model, such phenomenon can be found, and in fact, even amplified, under a structural pruning setting.

We emphasize that the above example (Figure 2) is not a cherry-picked one. As shown in Table 1, by reconstructing the entire test set of CIFAR-10 with solely their high-frequency components, we find that a structurally pruned model is more prone to fitting HFCs than its unpruned counterpart under various settings. This is potentially because HFCs are easily learnable features under a benign setting, so pruned models want to "make most use" of their remaining weights given the reduced network capacity, and therefore become even more overfitted to HFCs by treating them as short-cut features.

Kernel smoothness as an indicator for learning from HFC.Fortunately, Wang et al. (2020) suggests _kernel smoothness_ is highly correlated to the learning of HFC. Specifically, Wang et al. (2020) found that a CNN with "smoother" kernels -- where neighbor weights within a 2D kernel have less of a value difference -- will reduce the overfitting of HFCs, thus making the model more robust against adversarial perturbations. For the ease of the following conversation, we define the kernel smoothness of a convolution kernel \(k\) for \(k^{H W}\) to be:

\[(k)=_{i=1}^{h w}_{j \\ k_{i}}k_{j}^{2}-k_{i}^{2},\] (1)

   Input & Unpruned Baseline & CC Pruned & NPPM Pruned & L1Norm-B Pruned \\  Full (\(=0.0\)) & 93.24 & **94.04** & 93.55 & 92.62 \\ HFC (\(=0.3\)) & 77.05 & **80.22** & 78.08 & 79.83 \\ HFC (\(=0.5\)) & 50.77 & **57.49** & 55.47 & 56.06 \\ HFC (\(=0.7\)) & 22.79 & 25.78 & 21.92 & **27.15** \\   

Table 1: Unpruned and structurally pruned ResNet-56 v. HFC/LFC-reconstructed CIFAR-10 test set. \(\) represents the cutoff threshold (for \(0 1\)). With a higher \(\), the HFC-reconstructed images will exclusively include more high-frequency information; vice-versa for a lower \(\). Pruning rate \(\) 43.75%; pruned model checkpoints are taken from Table 2.

Figure 2: A frog figure from CIFAR-10 test set in its original, LFC, and HFC-reconstructed formats. The output indicates the correctness of classification results when testing through an unpruned and a L1Norm pruned ResNet-56 (pruning rate \(\) 43.75% to be consistent with Table 2). A classic demonstration of such phenomena can be seen in Figure 2. Despite the frog-labeled figure reconstructed with only low-frequency components showing visible resemblance to its original benign format, a ResNet-56 model pruned by L1Norm filter pruning (Li et al., 2017) cannot classify it correctly. However, such pruned models can somehow correctly classify the same input reconstructed with only HFCs, even if it already lost all semantics of a frog to a human audience. This indicates a model pruned by methods without having adversarial robustness in consideration is more likely to overfit to HFC.

where \(H\) and \(W\) are the kernel dimensions; in most CNNs, such dimensions are set to \(3 3\).

The finding of kernel smoothness and adversarial robustness presents a unique opportunity under the context of GKP. Prior to GKP, densely structured pruning is almost always done at a filter/channel level, where kernel-level metrics/operation have little bearing when relaxed (see Appendix C.1.3). However, GKP prunes at a (close) kernel level, where a kernel-level metric may still retain its power.

### Mixing Smoothness into Grouped Kernel Pruning Procedure

It is natural to want to encapsulate some kernel-level operations/metrics -- in this case, kernel smoothness -- into the procedure of GKP. However, the challenge comes with how we can do it in an efficient and effective manner. Particularly, how can we achieve improved adversarial performance without sacrificing benign tasks?

For the ease of illustration, let \(^{}^{C^{}_{} C^{}_{}  H^{} W^{}}\) be the weight of the \(\)-th convolutional layer, which consist of \(C^{}_{}\) filters, with each filter consisting of \(C^{}_{}\) number of \(H^{} W^{}\) 2D kernels. According to Zhong et al. (2022), a standard GKP procedure has two potential stages:

* **Filter grouping stage**: where the \(C^{}_{}\) filters are clustered into \(n\) equal-sized filter groups \(\{^{}_{i},^{}_{j},,^{}_{n}\}\), with each filter group \(^{}^{C^{}_{}/n C^{}_{} H^{} W^{}}\).
* **Pruning strategies obtaining/pruning stage**: where the pruning method generates a set of "candidate" grouped kernel pruning strategies to be evaluated and select from; a grouped kernel is defined as a \(^{}\) with \(\) having a shape of \(C^{}_{}/n 1 H^{} W^{}\). Finally, we evaluate all collected candidate strategies and decide which to pursue.

However, how to apply kernel smoothness-related criteria to such stages is a non-trivial question. Our empirical results from some proof-of-concept experiments suggest some naive applications either will not work at all or will only work by significantly sacrificing the performance on benign tasks: e.g., if we simply replace Stage 2 above by pruning the grouped kernels with greater \(_{k}(k)\), we will experience a huge drop on benign accuracy (Appendix C). Therefore, we must drive our attention to discover some more sophisticated ways of mixing up such criteria into the above stages.

#### 3.2.1 Smoothness Snaking: Is Filter Clustering the Only Right Answer for GKP?

Per TMI-GKP (Zhong et al., 2022), filters within the same convolutional layer are grouped into several equal-sized groups by a _filter clustering schema_, which consist of some different combinations of dimensionality reduction and clustering techniques. The motivation of grouping by clustering is natural as it establishes a preferable search space for pruning algorithms, where the power of pruned components can likely be retrained via similar unpruned components within the same group. However, later procedures of TMI-GKP (e.g., Stage 2 per Section 3.2) then seek to maintain a diverse representation of kept components within each filter group, **which begs the question: is filter clustering the only right answer for GKP? If finding a diverse set of unpruned grouped kernels is the goal, why not have filter groups with filter diversity to start with?** And if the answer to the first question is "No." How can we utilize this opportunity to mix up with kernel smoothness criteria?

Figure 3: Smoothness Snaking For Filter Grouping (Stage 1)

Grouped-based pruning will always seek out some kind of balance among groups to avoid a skewed distribution where all important components are distributed to certain groups (Zhong et al., 2022). Following this principle, we want our filter groups to have balanced smoothness -- so that we do not end up having any filter group that is "over" or "under-smoothed" to start with for pruning. However, this is equivalent to the _partition problem_(Korf, 1998), which is known to be NP-hard. Given the filter grouping stage is often robust to adjustments2, we proposed to sort filters according to their smoothness, then assign them iteratively in an S-shaped "snaking" manner across a predefined number of filter groups, as shown in Figure 3, namely _Smoothness Snaking_. Empirical results suggest although smoothness snaking may not be as optimal as the dynamic clustering scheme in Zhong et al. (2022) in terms of benign performance, it is able to provide better adversarial robustness under adversarial attacks and is much faster to execute (Table 8) due to the absence of dimensionality reduction & clustering procedures (Appendix C). We consider this to be a successful mix-up.

#### 3.2.2 Smooth Beam Greedy GKP Search

Knowing that the pruning strategies obtaining/decision stage (Stage 2 per Section 3.2) of GKP is sensitive to tampering, a direct application of smoothness criteria would not work (Appendix C). This indicates the distance-based _cost_ formula (Equation 5) proposed in Zhong et al. (2022) carries a significant influence on the performance of a pruned network, which is unsurprising given the vast popularity of distance-based pruning arts. Since we can't directly replace this _cost_ formula, we propose to widen the capacity of pruning strategies obtaining stage with smoothness in mind while keeping the decision stage akin to the _cost_ formula. By doing this, the chances of more "smoothness-aware" pruning strategies being employed are increased.

Again, to keep our approach simple, we implemented a custom beam search element to consider more grouped kernels during each advancement (Figure 4). Per each iteration, all gathered candidate grouped kernel pruning strategies will be evaluated against a mixture of smoothness and cost criteria, where only a \(Beam_{}\) amount of grouped kernels will be kept for further advancement (until the desired pruning ratio is reached). The scoring formula to determine which subset of strategies may be kept in the beam is defined as (larger scores are better/preferred):

\[}{}(_{},)= (_{})+(1- )(_{}),\] (2)

where \((_{})\) for \(\{,\}\) represents the rank of such \(_{}\) when all collected \(_{}\) candidates are sorted according to the given \(\) in a descending order. So \((_{})=0\) would suggest this particular \(_{}\) has a greater smoothness ranking (a.k.a. "less smooth") then all other \(_{}\) candidates in considerations. \(\) is a tunable balancing parameter that adjusts the importance of one criterion over

Figure 4: Smoothness-aware Beam Greedy Search for Grouped Kernel Pruning (Stage 2).

the other. The \((_{})\) equation is simply the sum of all kernels within all \(_{}\) over Equation 1; the \((_{})\) equation is a modified version of Equation 5 in Zhong et al. (2022), where we removed some hyperparameters for simplicity and to reduce tuning workload. Please refer to Figure 4 and Appendix C.3 for more details.

## 4 Experiments and Results

### Experiment Setups

We evaluate the efficacy of our method on ResNet-32/56/110 with the BasicBlock implementation, ResNet-50/101 with the BottleNeck implementation, and VGG-16 He et al. (2016); Simonyan and Zisserman (2015). For datasets, we choose CIFAR-10 Krizhevsky (2009), Tiny-ImageNet Wu et al. (2017), and ImageNet-1k Deng et al. (2009) for a wide range of coverage. For all compared methods and under most model-dataset combinations, we tried our best to replicate them with a \(\) 300 epochs (except for ImageNet, where we only employ 100 epochs) of fine-tuning/retraining budget while maintaining all other settings either identical or proportional to their original publications.

### Compared Methods and Evaluating Criteria

We evaluate our proposed method against up to 13 popular densely structured pruning methods and variants shown in Appendix D. Whenever possible, we produce pruned-and-fine-tuned (or retrained) models upon **identical unpruned baseline models** with similar post-prune MACs and Params. Then, we compare their inference accuracy on benign inputs as well as adversarially-perturbed inputs powered by _FGSM_ and _PGD_ attacks in various perturbation budgets and intensities Goodfellow et al. (2014); Madry et al. (2018).

One reporting mechanism that is probably unique to our paper -- in comparison to standard pruning art under the benign space -- is **for some of our methods, results of multiple epochs are reported, each showcasing a method's peak performance against different evaluation metrics (a.k.a. "superscore").** This is because for benign tasks, following Li et al. (2017) and He et al. (2019), only the epoch checkpoint with the best benign accuracy needs to be reported. But under an adversarial context, if a pruning method is capable of producing more than one fully pruned model during the fine-tuning/retraining stage, oftentimes, the best performer per each evaluation metric does not overlap. We believe it is responsible to report them all as there are no dominant evaluation metrics in our experiments; however, this is an important advantage for methods that can generate multiple usable pruned models (e.g., one-shot pruning) over methods that can only generate a few or just one fully pruned model (e.g., layer-wise iterative pruning). We survey the availability of such checkpoints and their epoch cutoff in Table 10: "Fully Pruned Epoch" column.

We also comprehensively investigate each pruning method against a checklist of questions as presented in Table 10 of Appendix D.1.3, including pruning granularity, procedure, when is the first fully pruned epoch, zero-masked or hard-pruning, and many other important questions. **This investigation, along with the epoch budget constraint and baseline control, should provide our audience with a more leveled playing field for fair and informed methods comparison.**

Further, to facilitate digesting mass results reported across many methods/variants and against various metrics, we present our large-scale empirical evaluation in three different ways. Take BasicBlockResNets on CIFAR-10 as an example (with pruning rate \( 43.75\%\)), we present the raw results as Table 11, 12, and 13; where we visualize them accordingly as Figure 6, 7, and 8. Last, we make a rank chart, Table 3, to provide a straightforward gauge of the competitiveness among different pruning methods.

### Results and Analysis

Our abbreviated results -- Table 2 and Table 4 -- showcased the performance of various modern SOTA methods as well as our proposed methods on the two most popular model-dataset combinations Blalock et al. (2020). For ResNet-56 on CIFAR-10, our method outperformed every other method on all evaluating criteria, except for PGD; as RAP-ADMM Ye et al. (2019) outperformed SR-GKP significantly with \(61.16\%\) v. \(44.85\%\). However, it is worth noting that RAP-ADMM does adversarial training on PGD-perturbed data, so it is not surprising that it performs well against the seen type of attack. Unfortunately, it seems like the adversarially trained RAP-ADMM cannot generalize its 

   Method & Criterion & Benign & (a) & \(_{m=0.01}\) & (b) & \(_{m=0.1}\) & (c) & \(_{m=0.5}^{(m=2/15),,m=2/55}\) & MACs (M) & Params (M) \\  Unpruned & - & 93.24 & 75.15 & 39.64 & 42.58 & 126.561 & 0.853 \\   & Best Benign & 94.04 & 74.78 & 29.25 & 37.85 & } &  \\  & Best (a) & 93.73 & 75.75 & 29.26 & 38.86 & & \\  & Best (a) & 93.70 & 75.56 & 29.89 & 38.90 & & \\   & Best Benign & 93.60 & 75.31 & 43.20 & 43.55 &  &  \\  & Best (a) & 93.77 & 76.28 & 44.96 & 44.66 & & \\   & Best Benign & 92.27 & 72.32 & 19.11 & 32.51 & & \\  & Best (a) & 92.07 & 72.59 & 18.94 & 32.21 & & \\   & Best Benign & 92.62 & 72.97 & 41.30 & 44.79 &  &  \\  & Best (a) & 91.94 & 75.16 & 42.49 & 43.71 & & \\  & Best (b) & 91.70 & 74.40 & 45.16 & 43.41 & & \\   & Best Benign & 93.93 & 73.47 & 25.59 & 34.86 &  &  \\  & Best (a) & 93.68 & 74.80 & 27.39 & 36.78 & & \\  & Best (b) & 93.63 & 74.06 & 28.20 & 35.98 & & \\   & Best Benign & 93.55 & 74.82 & 29.07 & 37.12 &  &  \\  & Best (a) & 93.35 & 75.50 & 30.29 & 38.09 & & \\  & Best (b) & 93.43 & 75.27 & 31.18 & 38.13 & & \\  RAP-ADMM & - & 78.37 & 75.19 & 27.84 & **61.15** & 71.661 & 0.482 \\   & - & 93.15 & 75.63 & 43.83 & 44.10 & 71.462 & 0.481 \\   & Best Benign & 93.95 & 75.18 & 42.18 & 43.46 & & \\  & Best (a) & 93.37 & 75.88 & 42.55 & 43.74 & 71.855 & 0.482 \\   & Best Benign & **94.08** & 75.89 & 42.60 & 43.85 &  &  \\  & Best (a) & 93.83 & **76.40** & **45.17** & **44.85** & & \\   

Table 2: ResNet-56 on CIFAR-10. All pruning methods are performed on the same baseline model. “Best (a)” represents the performance of a model checkpoint that means the showcased MACs/Params reduction that performs the best against criterion (a).

Figure 5: Visualization of ResNet-56 on CIFAR-10 with pruning rate \( 43.75\%\) — note this plot is done in a “superscore” manner for a concise presentation; the four bars of a method may not belong to the same model checkpoint.

   Method & ResNet-32 Mean Rank & ResNet-56 Mean Rank & ResNet-110 Mean Rank & All Models Mean Rank \\  CC  & \#5.5 & \#6.5 & \#9.25 & \#7.08 \\ DHP  & \#10.5 & \#8.75 & \#12.25 & \#10.5 \\ FPGM  & \#4.75 & \#4 & **H2.5** & \#3.75 \\ L1Norm-A  & \#7.75 & \#7 & 68.75 & \#7.83 \\ L1Norm-B  & \#4 & \#6.75 & \#8 & \#6.25 \\ LRF  & \#7.75 & \#8.75 & \#7 & \#7.83 \\ NPPM  & \#6.5 & \#8 & \#8.25 & \#7.58 \\ OTOY (22-norm-scratch)  & \#12.5 & \#12.5 & \#12.25 & \#12.42 \\ OTOY (22-norm)  & \#6.75 & \#7.5 & \#6 & \#7.17 \\ RAP-ADMM  & \#7 & \#7.5 & \#7 & \#7.17 \\ SPP  & \#9 & \#6.25 & \#3 & \#6.08 \\ TMI-GRP  & \#5.25 & \#4.25 & \#4.25 & \#4.58 \\
**SR-GRP (Ours)** & \#3.75 & \#2 & **\#2.5** & **\#2.75** \\   

Table 3: Methods ranked against each other on each model with pruning rate \( 43.75\%\), lower is better. “ResNet-XX Mean Rank” means a method’s average ranks across four metrics on ResNet-XX; e.g., SR-GRP is ranked #1/#1/#4/#2 for its best performance across benign/FGSM 0.01/FGSM 0.1/PGD metrics on ResNet-56 against other methods, so it’d have a ResNet-56 Mean Rank of (1+1+4+2)/4 = #2. Methods with incomplete presence across the three models are excluded. This table is provided to facilitate the digestion of Table 2, 11, 12, and 13, which consist of raw results.

defense to other adversarial attacks, even though they are similar in nature. Also, RAP-ADMM has the worst benign performance across all showcased methods, yet its training time is significantly longer due to the need to perturb its training data on the fly constantly.

Table 2 (and similar experiments showcased in Appendix D) may answer one of our research questions: **are carefully designed modern structured pruning methods also fragile under adversarial attacks? The answer is an unfortunate "Yes,"** as not only do recent structured pruning methods show serious performance drops under adversarial attacks, such drops are often more severe than their predecessors -- which often rely on much naive designs. Figure 5 as well as the ranked chart Table 3 provide a vivid illustration with LRF  -- a 2021 method -- showing one of the weakest adversarially robustness across all evaluated methods.

Upon careful comparison, we noticed that SFP  and FPGM  -- two pre-2020 methods -- tend to be the best filter pruning methods under the double scrutiny of benign and adversarial tasks. However, this only holds true to smaller scale experiments, as SFP is significantly outperformed by SR-GKP on ResNet-50 on ImageNet for benign accuracy (\(58.50\%\) v. \(74.34\%\)). Though FPGM tends to perform better on the same task, it is still behind SR-GKP in a general sense (Table 4). **We further note ImageNet with adversarial metrics is rarely evaluated under the context of adversarially robust pruning**; this is mainly because many of ImageNet sub-classes are closely related in terms of semantics (e.g., man-eating shark v. tiger shark) . We provide the ImageNet results primarily to show SR-GKP can deliver competitive benign ImageNet performance, where most ImageNet results compared in Table 4 are from methods with strong adversarial performance on CIFAR-10.

Last, although TMI-GKP  is optimized for benign tasks, and its procedure does not consider adversarial scenarios, it naturally comes with strong adversarial robustness. We believe this has a lot to do with the increased pruning freedom enabled by the GKP granularity, which further highlights the potential of GKP-based methods outside its achievements in the benign space. Due to the page limitation, we hereby only showcase a much-abbreviated version of our experiments. **We strongly encourage our readers to check out our full experiments at Appendix D with a lot more comprehensive coverage on many more dataset-model combinations, as well as ablation studies.**

## 5 Conclusion

Our work studies the area of adversarially robust structured pruning, a topic presenting severe problems but lacking proper recognition or exploration. On the investigation side, we reveal that -- just like their naive predecessors -- carefully designed modern structured pruning methods are also fragile under adversarial attacks, yet different pruning methods may yield drastically different adversarial performance while hiding behind similar benign reports. On the solution side, we propose SR-GKP: a simple one-shot GKP method that showcases competitive benign performance with a significant advantage under adversarial attacks against comparable SOTA methods while requiring no extra cost from a pruning procedure perspective.

We believe the overlooked nature of this field is mainly two-fold: the lack of pruning freedom to utilize findings of other fields, and the lack of user-friendly tools and resources for doing adversarial evaluations under a pruning context. We present our take and contribution to both issues by showcasing the capability of GKP-based methods and providing our community with an open-sourced tool and model checkpoints for future studies.

   Method & Baseline & Reugia & FOSM\({}_{-0.001}\) & FGSM\({}_{-0.01}\) & FGSM\({}_{-0.1}\) & FODF\({}_{-0.001}^{+0.05},-0.125)\) & MACs (M) & Params (M) \\  Ungument & Self-trained & 75.70 & 67.57 & 25.82 & 16.16 & 6.66 & 4122.828 & 25.557 \\  TMI-GKP  & Self-trained & 75.02 & 67.62 & 25.86 & 15.82 & 7.26 & 2725.954 & 17.669 \\ SR-GKP & **75.29** & **68.02** & **24.65** & **15.83** & **8.08** & 2725.954 & 17.669 \\  Ungument &  & 76.13 & 70.18 & 28.70 & 13.93 & 9.27 & 4122.828 & 25.557 \\  DPT  & torchvision & 73.80 & 67.45 & 25.15 & 12.24 & 7.43 & - \\ TPGM  & torchvision & 75.04 & **68.00** & 25.84 & 13.06 & 7.43 & 2641.670 & 18.310 \\ OTO/2 post-train  & torchvision & **75.38** & **68.04** & 21.01 & 12.76 & 5.26 & - \\ SFP  & torchvision & **75.38** & 55.72 & 25.82 & 9.01 & **11.00** & 2635.129 & 17.302 \\ SLR-GKP &  & **75.34** & 68.04 & **26.65** & **15.94** & 7.85 & 2759.672 & 17.803 \\   

Table 4: ResNet-50 on ImageNet-1k. Note this table includes two baselines: “self-trained” and “torchvision”. This is because TMI-GKP requires training epoch snapshots, which is not supplied with the torchvision pretrained ResNet-50. Methods included in this table are either well-performing methods under the CIFAR-10 experiments or specifically recommended by the reviewers.

## 6 Acknowledgments

This research was supported, in part, by NSF Awards OAC-2117439, OAC-2112606, IIS-2224843, and IIS-1900990. This work made use of the High Performance Computing Resource in the Core Facility for Advanced Research Computing at Case Western Reserve University. We give our special thanks to the CWRU HPC team for their timely and professional help and maintenance.