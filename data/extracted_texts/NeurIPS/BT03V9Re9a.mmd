# EmbedDistill: A Geometric Knowledge Distillation

for Information Retrieval

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large neural models (such as Transformers) achieve state-of-the-art performance for information retrieval (IR). In this paper, we aim to improve distillation methods that pave the way for the resource-efficient deployment of such models in practice. Inspired by our theoretical analysis of the teacher-student generalization gap for IR models, we propose a novel distillation approach that leverages the relative geometry among queries and documents learned by the large teacher model. Unlike existing teacher score-based distillation methods, our proposed approach employs embedding matching tasks to provide a stronger signal to align the representations of the teacher and student models. In addition, it utilizes query generation to explore the data manifold to reduce the discrepancies between the student and the teacher where training data is sparse. Furthermore, our analysis also motivates novel asymmetric architectures for student models which realizes better embedding alignment without increasing online inference cost. On standard benchmarks like MSMARCO, we show that our approach successfully distills from both dual-encoder (DE) and cross-encoder (CE) teacher models to 1/10th size asymmetric students that can retain 95-97% of the teacher performance.

## 1 Introduction

Neural models for information retrieval (IR) are increasingly used to model the true ranking function in various applications, including web search , recommendation , and question-answering (QA) . Notably, the recent success of Transformers -based pre-trained language models [11; 30; 49] on a wide range of natural language understanding tasks has also prompted their utilization in IR to capture query-document relevance [see, e.g., 10; 34; 43; 26; 20].

A typical IR system comprises two stages: (1) A _retriever_ first selects a small subset of potentially relevant candidate documents (out of a large collection) for a given query; and (2) A _re-ranker_ then identifies a precise ranking among the candidates provided by the retriever. _Dual-encoder_ (DE) models are the de-facto architecture for retrievers [26; 20]. Such models independently embed queries and documents into a common space, and capture their relevance by simple operations on these embeddings such as the inner product. This enables offline creation of a document index and supports fast retrieval during inference via efficient maximum inner product search implementations [12; 19], with _online_ query embedding generation primarily dictating the inference latency. _Cross-encoder_ (CE) models, on the other hand, are preferred as re-rankers, owing to their excellent performance [43; 9; 62]. A CE model jointly encodes a query-document pair while enabling early interaction among query and document features. Employing a CE model for retrieval is often infeasible, as it would require processing a given query with _every_ document in the collection at inference time. In fact, even in the re-ranking stage, the inference cost of CE models is high enough  to warrant exploration of efficient alternatives [14; 22; 37]. Across both architectures, scaling to larger models brings improved performance at increased computational cost [41; 39].

_Knowledge distillation_[5; 13] provides a general strategy to address the prohibitive inference cost associated with high-quality large neural models. In the IR literature, most existing distillation methods only rely on the teacher's query-document relevance scores [see, e.g., 31; 14; 8; 51; 56] or their proxies . However, given that neural IR models are inherently embedding-based, it is natural to ask: _Is it useful to go beyond matching of the teacher and student models'_ scores, _and directly aim to align their_ embedding spaces?

With this in mind, we propose a novel distillation method for IR models that utilizes an _embedding matching_ task to train student models. The proposed method is inspired by our rigorous treatment of the generalization gap between the teacher and student models in IR settings. Our theoretical analysis of the _teacher-student generalization gap_ further suggests novel design choices involving _asymmetric configurations_ for student DE models, intending to further reduce the gap by better aligning teacher and student embedding spaces. Notably, our proposed distillation method supports _cross-architecture distillation_ and improves upon existing (score-based) distillation methods for both retriever and re-ranker models. When distilling a large teacher DE model into a smaller student DE model, for a given query (document), one can minimize the distance between the query (document) embeddings of the teacher and student (after compatible projection layers to account for dimension mismatch, if any). In contrast, a teacher CE model doesn't directly provide document and query embeddings, and so to effectively employ embedding matching-based distillation requires modifying the scoring layer with _dual-pooling_ and adding various regularizers. Both of these changes improve geometry of teacher embeddings and facilitate effective knowledge transfer to the student DE model via embedding matching-based distillation.

Our key contributions toward improving IR models via distillation are:

* We provide the first rigorous analysis of the teacher-student generalization gap for IR settings which captures the role of alignment of embedding spaces of the teacher and student towards reducing the gap (Sec. 3).
* Inspired by our analysis, we propose a novel distillation approach for neural IR models, namely EmbedDistill, that goes beyond score matching and aligns the embedding spaces of the teacher and student models (Sec. 4). We also show that EmbedDistill can leverage synthetic data to improve a student by further aligning the embedding spaces of the teacher and student (Sec. 4.3).
* Our analysis motivates novel distillation setups. Specifically, we consider a student DE model with an _asymmetric_ configuration, consisting of a small query encoder and a _frozen_ document encoder inherited from the teacher. This significantly reduces inference latency of query embedding generation, while leveraging the teachers' high-quality document index (Sec. 4.1).
* Natural Questions  and MSMARCO . We also evaluate EmbedDistill on BEIR benchmark  which is used to measure the _zero-shot_ performance of an IR model.

Note that prior works have utilized embedding alignment during distillation for _non-IR_ setting [see, e.g., 52; 55; 18; 1; 64; 7]. However, to the best of our knowledge, our work is the first to study embedding matching-based distillation method for IR settings which requires addressing multiple IR-specific challenges such as cross-architecture distillation, partial representation alignment, and enabling novel asymmetric student configurations. Furthermore, unlike these prior works, our proposed method is theoretically justified to reduce the teacher-student performance gap.

## 2 Background

Let \(\) and \(\) denote the query and document spaces, respectively. An IR model is equivalent to a score \(s:\), i.e., it assigns a (relevance) score \(s(q,d)\) for a query-document pair \((q,d)\). Ideally, we want to learn a scorer such that \(s(q,d)>s(q,d^{})\)_iff_ the document \(d\) is more relevant to the query \(q\) than document \(d^{}\). We assume access to \(n\) labeled training examples \(_{n}=\{(q_{i},_{i},_{i})\}_{i[n]}\). Here, \(_{i}=(d_{i,1},,d_{i,L})^{L},\; i[n]\), denotes a list of \(L\) documents and \(_{i}=(y_{i,1},,y_{i,L})\{0,1\}^{L}\) denotes the corresponding labels such that \(y_{i,j}=1\) iff the document \(d_{i,j}\) is relevant to the query \(q_{i}\). Given \(_{n}\), we learn an IR model by minimizing

\[R(s;_{n}):=_{i[n]}s_{q_{i}, _{i}},_{i},\] (1)

where \(s_{q_{i},_{i}}:=(s(q_{i},d_{1,i}),,s(q_{i},d_{1,L}))\) and \(s_{q_{i},_{i}},_{i}\) denotes the loss \(s\) incurs on \((q_{i},_{i},_{i})\). Due to space constraint, we defer concrete choices for the loss function \(\) to Appendix A.

While this learning framework is general enough to work with any IR models, next, we formally introduce two families of Transformer-based IR models that are prevalent in the recent literature.

### Transformer-based IR models: Cross-encoders and Dual-encoders

Let query \(q\) = \((q^{1},,q^{m_{1}})\) and document \(d\) = \((d^{1},,d^{m_{2}})\) consist of \(m_{1}\) and \(m_{2}\) tokens, respectively. We now discuss how Transformers-based CE and DE models process the \((q,d)\) pair.

**Cross-encoder model.** Let \(p=[q;d]\) be the sequence obtained by concatenating \(q\) and \(d\). Further, let \(\) be the sequence obtained by adding special tokens such [CLS] and [SEP] to \(p\). Given an encoder-only Transformer model \(\), the relevance score for the \((q,d)\) pair is

\[s(q,d)= w,() = w,}_{q,d},\] (2)

where \(w\) is a \(d\)-dimensional classification vector, and \(()\) denotes a pooling operation that transforms the contextualized token embeddings \(()\) to a joint embedding vector \(}_{q,d}\). [CLS]-pooling is a common operation that simply outputs the embedding of the [CLS] token as \(}_{q,d}\).

**Dual-encoder model.** Let \(\) and \(\) be the sequences obtained by adding appropriate special tokens to \(q\) and \(d\), respectively. A DE model comprises two (encoder-only) Transformers \(_{Q}\) and \(_{D}\), which we call query and document encoders, respectively.1 Let \(}_{q}\) = \(_{Q}()\) and \(}_{d}\) = \(_{D}()\) denote the query and document embeddings, respectively. Now, one can define \(s(q,d)=}_{q},}_{ d}\) to be the relevance score assigned to the \((q,d)\) pair by the DE model.

### Score-based distillation for IR models

Most distillation schemes for IR [e.g., 31, 14, 8] rely on teacher relevance scores. Given a training set \(_{n}\) and a teacher with score \(s^{}\), one learns a student with score \(s^{}\) by minimizing

\[R(s^{},s^{};_{n})=_{i[n ]}_{}s^{}_{q,_{i}},s^{}_{q, _{i}},\] (3)

where \(_{}\) captures the discrepancy between \(s^{}\) and \(s^{}\). See Appendix A for common choices for \(_{}\).

## 3 Teacher-student generalization gap: Inspiration for embedding alignment

Our main objective is to devise novel distillation methods to realize high-performing student DE models. As a first step in this direction, we rigorously study the teacher-student generalization gap as realized by standard (score-based) distillation in IR settings. Informed by our analysis, we subsequently identify novel ways to improve the student model's performance. In particular, our analysis suggests two natural directions to reduce the teacher-student generalization gap: 1) enforcing tighter alignment between embedding spaces of teacher and student models; and 2) exploring novel asymmetric configuration for student DE model.

Let \(R(s)=[s_{q,},]\) be the population version of the empirical risk in Eq. 1, which measures the test time performance of the IR model defined by the score \(s\). Thus, \(R(s^{})-R(s^{})\) denotes the _teacher-student generalization gap_. In the following result, we bound this quantity (see Appendix C.1 for a formal statement and proof). We focus on distilling a teacher DE model to a student DE model and \(L=1\) (cf. Sec. 2) as it leads to easier exposition without changing the main takeaways. Our analysis can be extended to \(L>1\) or CE to DE distillation with more complex notation.

**Theorem 3.1** (Teacher-student generalization gap (informal)).: _Let \(\) and \(\) denote the function classes for the query and document encoders for the student model, respectively. Suppose that the score-based distillation loss \(_{}\) in Eq. 3 is based on binary cross entropy loss (Eq. 12 in Appendix A). Let one-hot (label-dependent) loss \(\) in Eq. 1 be the binary cross entropy loss (Eq. 10 in Appendix A). Further, assume that all encoders have the same output dimension and embeddings have their \(_{2}\)-norm bounded by \(K\). Then, we have_

\[R(s^{})-R(s^{}) _{n}(,)+2KR_{,Q}(,;_{n})+2KR_{,D}(, ;_{n})\] \[+(s^{};_{n})+K^{2} [(s^{}_{q,d})-y]+_{i [n]}|(s^{}_{q_{i},d_{i}})-y_{i}|,\] (4)_where \(_{n}(,):=_{s^{} }R(s^{},s^{};_{n})- _{}s^{}_{q,d},s^{}_{q,d}\); \(\) denotes the sigmoid function; and \((s^{};_{n})\) denotes the deviation between the empirical risk (on \(_{n}\)) and population risk of the teacher \(s^{}\). Here, \(R_{,Q}(,;_{n})\) and \(R_{,D}(,;_{n})\) measure misalignment between teacher and student embeddings by focusing on queries and documents, respectively (cf. Eq. 7 & 8 in Sec. 4.1)._

The last three quantities in the bound in Thm. 3.1, namely \((s^{};_{n})\), \([|(s^{}_{q,d})-y|]\), and \(_{i[n]}|(s^{}_{q_{i},d_{i}})-y_{i}|\), are _independent_ of the underlying student model. These terms solely depend on the quality of the underlying teacher model \(s^{}\). That said, the teacher-student gap can be made small by reducing the following three terms: 1) uniform deviation of the student's empirical distillation risk from its population version \(_{n}(,)\); 2) misalignment between teacher student query embeddings \(R_{,Q}(,;_{n})\); and 3) misalignment between teacher student document embeddings \(R_{,D}(,;_{n})\).

The last two terms motivate us to propose an _embedding matching_-based distillation that explicitly aims to minimize these terms during student training. Even more interestingly, these terms also inspire an _asymmetric DE configuration_ for the student which strikes a balance between the goals of reducing the misalignment between the embeddings of teacher and student (by inheriting teacher's document encoder) and ensuring serving efficiency (small inference latency) by employing a small query encoder. Before discussing these proposals in detail in Sec. 4 and Fig. 1, we explore the first term \(_{n}(,)\) and highlight how our proposals also have implications for reducing this term. Towards this, the following result bounds \(_{n}(,)\). Due to space constraints, we present an informal statement of the result (see Appendix C.2 for a more precise statement and proof).

**Proposition 3.2**.: _Let \(_{}\) be a distillation loss which is \(L_{_{}}\)-Lipschitz in its first argument. Let \(\) and \(\) denote the function classes for the query and document encoders, respectively. Further assume that, for each query and document encoder in our function class, the query and document embeddings have their \(_{2}\)-norm bounded by \(K\). Then,_

\[_{n}(,)_{_{n}}}}}{}_{0}^{}N(u, )N(u,)}\ du.\] (5)

_Furthermore, with a fixed document encoder, i.e., \(=\{g^{*}\}\),_

\[_{n}(,\{g*\})_{_{n}}}}}{}_{0}^{})}\ du.\] (6)

_Here, \(N(u,)\) is the \(u\)-covering number of a function class._

Note that Eq. 5 and Eq. 6 correspond to uniform deviation when we train _without_ and _with_ a frozen document encoder, respectively. It is clear that the bound in Eq. 6 is less than or equal to that in Eq. 5 (because \(N(u,) 1\) for any \(u\)), which alludes to desirable impact of employing a frozen document encoder as one of our proposal seeks to do via _inheriting teacher's document encoder_ (for instance in an asymmetric DE configuration). Furthermore, our proposal of employing an embedding-matching task will regularize the function class of query encoders; effectively reducing it to \(^{}\) with \(|^{}|||\). The same holds true for document encoder function class when document encoder is trainable (as in Eq. 5), leading to an effective function class \(^{}\) with \(|^{}|||\). Since we would have \(N(u,^{}) N(u,)\) and \(N(u,^{}) N(u,)\), this suggests desirable implications of embedding matching for reducing the uniform deviation bound.

Figure 1: Proposed distillation method with query embedding matching. **Left:** The setting where student employs an asymmetric DE configuration with a small query encoder and a large (non-trainable) document encoder inherited from the teacher DE model. The smaller query encoder ensures small latency for encoding query during inference, and large document encoder leads to a good quality document index. **Right:** Similarly the setting of CE to DE distillation using \(\), with teacher CE model employing dual pooling.

## 4 Embedding-matching based distillation

Informed by our analysis of teacher-student generalization gap in Sec. 3, we propose EmbedDistill - a novel distillation method that explicitly focuses on aligning the embedding spaces of the teacher and student. Our proposal goes beyond existing distillation methods in the IR literature that only use the teacher scores. Next, we introduce EmbedDistill for two prevalent settings: (1) distilling a large DE model to a smaller DE model; 2 and (2) distilling a CE model to a DE model.

### DE to DE distillation

Given a \((q,d)\) pair, let \(_{q}^{}\) and \(_{d}^{}\) be the query and document embeddings produced by the query encoder \(_{Q}^{}\) and document encoder \(_{D}^{}\) of the teacher DE model, respectively. Similarly, let \(_{q}^{}\) and \(_{d}^{}\) denote the query and document embeddings produced by a student DE model with \((_{Q}^{},_{D}^{})\) as its query and document encoders. Now, EmbedDistill optimizes the following embedding alignment losses in addition to the score-matching loss from Sec. 2.2 to align query and document embeddings of the teacher and student:

\[R_{,Q}(,;_{n}) =_{q_{n}}\|_{ q}^{}-\!(_{q}^{} )\|;\] (7) \[R_{,D}(,;_{n}) =_{d_{n}}\|_{ d}^{}-\!(_{d}^{} )\|.\] (8)

**Asymmetric DE.** We also propose a novel student DE configuration where the student employs the teacher's document encoder (i.e., \(_{D}^{}=_{D}^{}\)) and only train its query encoder, which is much smaller compared to the teacher's query encoder. For such a setting, it is natural to only employ the embedding matching loss in Eq. 7 as the document embeddings are aligned by design (cf. Fig. 0(a)).

Note that this asymmetric student DE does not incur an increase in latency despite the use of a large teacher document encoder. This is because the large document encoder is only needed to create a good quality document index offline, and only the query encoder is evaluated at inference time. Also, the similarity search cost is not increased as the projection layer ensures the same small embedding dimension as in the symmetric DE student. Thus, for DE to DE distillation, we prescribe the asymmetric DE configuration universally. Our theoretical analysis (cf. Sec. 3) and experimental results (cf. Sec. 5) suggest that the ability to inherit the document tower from the teacher DE model can drastically improve the final performance, especially when combined with query embedding matching task (cf. Eq. 7).

### CE to DE distillation

Given that CE models jointly encode query-document pairs, individual query and document embeddings are not readily available to implement embedding matching losses as per Eq. 7 and 8. This makes it challenging to employ EmbedDistill for CE to DE distillation.

As a naive solution, for a \((q,d)\) pair, one can simply match a joint transformation of the student's query embedding \(_{q}^{}\) and document embedding \(_{d}^{}\) to the teacher's joint embedding \(_{q,d}^{}\), produced by (single) teacher encoder \(^{t}\). However, we observed that including such an embedding matching task often leads to severe over-fitting, and results in a poor student. Since \(s^{}(q,d)= w,_{q,d}^{}\), during CE model training, the joint embeddings \(_{q,d}^{}\) for relevant and irrelevant \((q,d)\) pairs are encouraged to be aligned with \(w\) and \(-w\), respectively. This produces degenerate embeddings that do not capture semantic query-to-document relationships. We notice that even the final query and document token embeddings lose such semantic structure (cf. Appendix G.2). Thus, a teacher CE model with \(s^{}(q,d)= w,_{q,d}^{}\) does not add value for distillation beyond score-matching; in fact, it _hurts_ to include naive embedding matching. Next, we propose a modified CE model training strategy that facilitates EmbedDistill.

**CE models with dual pooling.** A _dual pooling_ scheme is employed in the scoring layer to produce two embeddings \(_{q(q,d)}^{}\) and \(_{d(q,d)}^{}\) from a CE model that serve as the _proxy_ query and document embeddings, respectively. Accordingly, we define the relevance score as \(s^{}(q,d)=_{q(q,d)}^{},_{d(q,d)}^{}\). We explore two variants of dual pooling: (1) special token-based pooling that pools from [CLS] and [SEP]; and (2) segment-based weighted mean pooling that separatelyperforms weighted averaging on the query and document segments of the final token embeddings. See Appendix B for details.

In addition to dual pooling, we also utilize a reconstruction loss during the CE training, which measures the likelihood of predicting each token of the original input from the final token embeddings. This loss encourages reconstruction of query and document tokens based on the final token embeddings and prevents the degeneration of the token embeddings during training. Given proxy embeddings from the teacher CE, we can perform EmbedDistill with the embedding matching loss defined in Eq. 7 and Eq. 8 (cf. Fig. 0(b)).

### Task-specific online data generation

Data augmentation as a general technique has been previously considered in the IR literature (see, e.g., 45; 47; 17), especially in data-limited, out-of-domain, or zero-shot settings. As EmbedDistill aims to align the embeddings spaces of the teacher and student, the ability to generate similar queries or documents can naturally help enforce such an alignment globally on the task-specific manifold. Given a set of unlabeled task-specific query and document pairs \(_{m}\), we can further add the embedding matching losses \(R_{}(_{m}})\) or \(R_{}(_{m}})\) to our training objective. Interestingly, for DE to DE distillation setting, our approach can even benefit from a large collection of task-specific queries \(^{}\) or documents \(^{}\). Here, we can independently employ embedding matching losses \(R_{}(^{}})\) or \(R_{}(^{}})\) that focus on queries and documents, respectively. Please refer to Appendix E describing how the task-specific data were generated.

## 5 Experiments

We now conduct a comprehensive evaluation of the proposed distillation approach. Specifically, we highlight the utility of the approach for both DE to DE and CE to DE distillation. We also showcase the benefits of combining our distillation approach with query generation methods.

### Setup

**Benchmarks and evaluation metrics.** We consider two popular IR benchmarks -- Natural Questions (NQ) (24) and MSMARCO (40), which focus on finding the most relevant passage/document given a question and a search query, respectively. NQ provides both standard test and dev sets, whereas MSMARCO provides only the dev set that are widely used for common benchmarks. In what follows, we use the terms query (document) and question (passages) interchangeably. For NQ, we use the standard full recall (_strict_) as well as the _relaxed_ recall metric (20) to evaluate the retrieval performance. For MSMARCO, we focus on the standard metrics _Mean Reciprocal Rank_ (MRR)@10, and _normalized Discounted Cumulative Gain_ (nDCG)@10 to evaluate both re-ranking and retrieval performance. For the re-ranking, we restrict to re-ranking only the top 1000 candidate document provided as part of the dataset to be fair, while some works use stronger methods to find better top 1000 candidates for re-ranking (resulting in higher evaluation numbers) See Appendix D for a detailed discussion on these evaluation metrics. Finally, we also evaluate EmbedDistill on the BEIR benchmark (57) in terms of nDCG@10 and recall@100 metrics.

  
**Method** &  &  \\   & **R@5** & **R@20** & **R@100** & **R@5** & **R@20** & **R@100** \\  Train student directly & 36.2 & 59.7 & 80.0 & 24.8 & 44.7 & 67.5 \\ + Distill from teacher & 65.3 & 81.6 & 91.2 & 44.3 & 64.9 & 81.0 \\ + Inherit doc embeddings & 69.9 & 83.9 & 92.3 & 56.3 & 70.9 & 82.5 \\ + Query embedding matching & 72.7 & **86.5** & **93.9** & 61.2 & 75.2 & 85.1 \\ + Query generation & **73.4** & **86.3** & **93.8** & **64.3** & **77.8** & **87.9** \\  Train student using only & & & & & & \\ embedding matching and & & & & & & \\ inherit doc embeddings & 71.4 & 84.9 & 92.6 & 64.6 & 50.2 & 76.8 \\ + Query generation & 71.8 & 85.0 & 93.0 & 54.2 & 68.9 & 80.8 \\   

Table 1: _Full_ recall performance of various student DE models on NQ dev set, including symmetric DE student (67.5M or 11.3M transformer for both encoders), and asymmetric DE student model (67.5M or 11.3M transformer as query encoder and document embeddings inherited from the teacher). All distilled students used the same teacher (110.1M parameter BERT-base models as both encoders), with the full Recall@5 = 72.3, Recall@20 = 86.1, and Recall@100 = 93.6.

  
**Method** & \#**Layers** & **R@20** & **R@100** \\  DPR (20) & 12 & 78.4 & 85.4 \\ DPR + PAQ (47) & 12 & 84.0 & 89.2 \\ DPR + PAQ (47) & 24 & 84.7 & 89.2 \\ ACME (60) & 12 & 81.9 & 87.5 \\ Rockerda (48) & 12 & 82.7 & 88.5 \\ MSS-DPR (53) & 12 & 84.0 & 89.2 \\ MSS-DPR (53) & 24 & 84.8 & 89.8 \\  Our teacher (63) & 12 (220.2M) & 85.4 & 90.0 \\ EmbedDistill & 6 (67.5M) & 85.1 & 89.8 \\ EmbedDistill & 4 (11.3M) & 81.2 & 87.4 \\   

Table 2: Performance of EmbedDistill for DE to DE distillation on NQ test set. While prior works listed in the table rely on techniques such as negative mining and multi-stage training, we explore the orthogonal direction of embedding-matching that improves _single-stage_ distillation, which can be combined with them.

**Model architectures.** We follow the standard Transformers-based IR model architectures similar to Karpukhin et al. , Qu et al. , Oguz et al. . We utilized various sizes of DE models based on BERT-base  (12-layer, 768 dim, 110M parameters), DistilBERT  (6-layer, 768 dim, 67.5M parameters - \(\) 2/3 of base), or BERT-mini  (4-layer, 256 dim, 11.3M parameters - \(\) 1/10 of base). For query generation (cf. Sec. 4.3), we employ BART-base , an encoder-decoder model, to generate similar questions from each training example's input question (query). We randomly mask \(10\%\) of tokens and inject zero mean Gaussian noise with \(=\{0.1,0.2\}\) between the encoder and decoder. See Appendix E for more details on query generation and Appendix F.1 for hyperparameters.

### DE to DE distillation

We employ AR2 3 and SentenceBERT-v5 4 as teacher DE models for NQ and MSMARCO. Note that both models are based on BERT-base. For DE to DE distillation, we consider two kinds of configurations for the student DE model: (1) _Symmetric_: We use identical question and document encoders. We evaluate DistilBERT and BERT-mini on both datasets. (2) _Asymmetric_: The student inherits document embeddings from the teacher DE model and _are not_ trained during the distillation. For query encoder, we use DistilBERT or BERT-mini which are smaller than document encoder.

**Student DE model training.** We train student DE models using a combination of (i) one-hot loss (cf. Eq. 9 in Appendix A) on training data; (ii) distillation loss in (cf. Eq. 11 in Appendix A); and (iii) embedding matching loss in Eq. 7. We used [CLS]-pooling for all student encoders. Unlike DPR  or AR2, we do not use hard negatives from BM25 or other models, which greatly simplifies our distillation procedure.

**Results and discussion.** To understand the impact of various proposed configurations and losses, we train models by sequentially adding components and evaluate their retrieval performance on NQ and MSMARCO dev set as shown in Table 1 and Table 3 respectively. (See Table 6 in Appendix F.2 for performance on NQ in terms of the relaxed recall and Table 7 in Appendix F.3 for MSMARCO in terms of nDCG@10.)

We begin by training a symmetric DE without distillation. As expected, moving to distillation brings in considerable gains. Next, we swap the student document encoder with document embeddings from the teacher (non-trainable), which leads to a good jump in the performance. Now we can introduce EmbedDistill with Eq. 7 for aligning query representations between student and teacher. The two losses are combined with weight of \(1.0\) (except for BERT-mini models in the presence of query generation with \(5.0\)). This improves performance significantly, e.g.,it provides \(\)3 and \(\)5 points increase in recall@5 on NQ with students based on DistilBERT and BERT-mini, respectively (Table 1). We further explore the utility of EmbedDistill in aligning the teacher and student embedding spaces in Appendix G.1.

On top of the two losses (standard distillation and embedding matching), we also use \(R_{}(,;^{})\) from Sec. 4.3 on \(2\) additional questions (per input question) generated from BART. We also try a variant where we eliminate the standard distillation loss and only employ the embedding matching loss in Eq. 7 along with inheriting teacher's document embeddings. This configuration without the standard distillation loss leads to excellent performance (with query generation again providing additional gains in most cases.)

    &  &  \\   & **67.5M** & **11.3M** & **67.5M** & **11.3M** \\  Train student directly & 27.0 & 23.0 & 22.6 & 18.6 \\ + Distill from teacher & 34.6 & 30.4 & 35.0 & 28.6 \\ + Inherent doe embeddings & 35.2 & 32.1 & 35.7 & 30.3 \\ + Query embedding matching & 36.2 & **35.0** & 35.4 & **40.8** \\ + Query generation & 36.2 & 34.4 & 37.2 & 34.8 \\  Train student using only embedding matching and inherit doc embeddings & **36.5** & 33.5 & **36.6** & 31.4 \\ + Query generation & 36.4 & 34.1 & **36.7** & 32.8 \\   

Table 3: Performance of various DE models on MSMARCO dev set for both _re-ranking_ and _retrieval_ tasks (full corpus). The teacher model (110.1M parameter BERT-base models as both encoders) for re-ranking achieves MRR@10 of 36.8 and that for retrieval get MRR@10 of 37.2. The table shows performance (in MRR@10) of the symmetric DE student model (67.5M or 11.3M transformer as both encoders), and asymmetric DE student model (67.5M or 11.3M transformer as query encoder and document embeddings inherited from the teacher).

It is worth highlighting that DE models trained with the proposed methods (e.g., asymmetric DE with embedding matching and generation) achieve 99% of the performance in both NQ/MSMARCO tasks with a query encoder that is 2/3rd the size of that of the teacher. Furthermore, even with 1/10th size of the query encoder, our proposal can achieve 95-97% of the performance. This is particularly useful for latency critical applications with minimal impact on the final performance.

Finally, we take our best student models, i.e., one trained using with additional embedding matching loss and using data augmentation from query generation, and evaluate on test sets. We compare with various prior work and note that most prior work used considerably bigger models in terms of parameters, depth (12 or 24 layers), or width (upto 1024 dims). For NQ test set results are reported in Table 2, but as MSMARCO does not have any public test set, we instead present results for the BEIR benchmark in Table 4. Note we also provide evaluation of our SentenceBERT teacher achieving very high performance on the benchmark which can be of independent interest (please refer to Appendix F.4 for details). For both NQ and BEIR, our approach obtains competitive student model with fewer than 50% of the parameters: even with 6 layers, our student model is very close (98-99%) to its teacher.

### CE to DE distillation

We consider two CE teachers for MSMARCO re-ranking task5: a standard [CLS]-pooled CE teacher, and the Dual-pooled CE teacher (cf. Sec. 4.2). Both teachers are based on RoBERTa-base and trained on triples in the training set for 300K steps with cross-entropy loss.

**Student DE model training.** We considered the following distillation variants: standard score-based distillation from the [CLS]-pooled teacher, and our novel Dual-pooled CE teacher (with and without embedding matching loss). For each variant, we initialize encoders of the student DE model with two RoBERTa-base models and train for 500K steps on the training triples. We performed the naive joint embedding matching for the [CLS]-pooled teacher (cf. Sec. 4.2) and employed the query embedding matching (cf. Eq.7) for the Dual-pooled CE teacher. In either case, embedding-matching loss is added on top of the standard cross entropy loss with the weight of \(1.0\) (when used).

**Results and discussion.** Table 5 evaluates the effectiveness of the dual pooling and the embedding matching for CE to DE distillation. As described in Sec. 4.2, the traditional [CLS]-pooled teacher did not provide any useful embedding for the embedding matching (see Appendix G.2 for the further analysis of the resulting embedding space). However, with the Dual-pooled teacher, embedding matching does boost student's performance.

## 6 Related work

Here, we position our EmbedDistill work with respect to prior work on distillation and data augmentation for Transformers-based IR models. We also cover prior efforts on aligning representations during distillation for _non-IR_ settings. Unlike our problem setting where the DE student is factorized, these works mainly consider distilling a single large Transformer into a smaller one.

**Distillation for IR.** Traditional distillation techniques have been widely applied in the IR literature, often to distill a teacher CE model to a student DE model [28; 8]. Recently, distillation from a DE

  
**Method** & **\#Layers** & **nDCG@10** & **R@100** \\  DPR  & 12 & 22.5 & 47.7 \\ ANCE  & 12 & 40.5 & 60.0 \\ TAS-B  & 6 & 42.8 & 64.8 \\ GenQ  & 6 & 42.5 & 64.2 \\  Our teacher  & 12 (220.2M) & 45.7 & 65.1 \\ EmbedDistill & 6 (67.5M) & 44.0 & 63.5 \\   

Table 4: Average BEIR performance of our DE teacher and EmbedDistill student models and their numbers of trainable parameters. Both models are trained on MSMARCO and evaluated on 14 other datasets (the average does not include MSMARCO). The full table is at Appendix F.4. With EmbedDistill, student materializes most of the performance of the teacher on the unforeseen datasets.

  
**Method** & **MRR@10** \\  [CLS]-pooled teacher & 37.1 \\ Dual-pooled teacher & 37.0 \\  Standard distillation from [CLS]-pooled teacher & 33.0 \\ + Joint matching & 32.4 \\ Standard distillation from Dual-pooled teacher & 33.3 \\ +Query matching & **33.7** \\   

Table 5: Performance of DE models distilled from [CLS]-pooled and Dual-pooled CE models on MSMARCO re-ranking task (original top1000 dev). While both teacher models perform similarly, embedding matching-based distillation only works with the Dual-pooled teacher. See Appendix F for nDCG@10 metric.

model (with complex late interaction) to another DE model (with inner-product scoring) has also been considered [29; 15]. As for distilling across different model architectures, Lu et al. , Izacard and Grave  consider distillation from a teacher CE model to a student DE model. Hofstatter et al.  conduct an extensive study of knowledge distillation across a wide-range of model architectures. Most existing distillation schemes for IR rely on only teacher scores; by contrast, we propose a geometric approach that also utilizes the teacher _embeddings_. Many recent efforts [48; 51; 56] show that iterative multi-stage (self-)distillation improves upon single-stage distillation [48; 51; 56]. These approaches use a model from the previous stage to obtain labels  as well as mine harder-negatives . We only focus on the single-stage distillation in this paper. Multi-stage procedures are complementary to our work, as one can employ our proposed embedding-matching approach in various stages of such a procedure. Interestingly, we demonstrate in Sec. 5 that our proposed EmbedDistill can successfully benefit from high quality models trained with such complex procedures [50; 63]. In particular, our single-stage distillation method can transfer almost all of their performance gains to even smaller models. Also to showcase that our method brings gain orthogonal to how teacher was trained, we conduct experiments with single-stage trained teacher in Appendix F.5.

**Distillation with representation alignments.** Outside of the IR context, a few prior works proposed to utilize alignment between hidden layers during distillation [52; 55; 18; 1; 64]. Chen et al.  utilize the representation alignment to re-use teacher's classification layer for image classification. Unlike these works, our work is grounded in a rigorous theoretical understanding of the teacher-student (generalization) gap for IR models. Further, our work differs from these as it needs to address multiple challenges presented by an IR setting: 1) cross-architecture distillation such as CE to DE distillation; 2) partial representation alignment of query or document representations as opposed to aligning for the entire input, i.e., a query-documents pair; and 3) catering representation alignment approach to novel IR setups such as asymmetric DE configuration. To the best of our knowledge, our work is first in the IR literature that goes beyond simply matching scores (or its proxies) for distillation.

**Semi-supervised learning for IR.** Data augmentation or semi-supervised learning has been previously used to ensure data efficiency in IR [see, e.g., 35; 66]. More interestingly, data augmentation have enabled performance improvements as well. Doc2query [45; 44] performs document expansion by generating queries that are relevant to the document and appending those queries to the document. Query expansion has also been considered, e.g., for document re-ranking . Notably, generating synthetic (query, passage, answer) triples from a text corpus to augment existing training data for QA systems also leads to significant gains [2; 47]. Furthermore, even zero-shot approaches, where no labeled query-document pairs are used, can also perform competitively to supervised methods [26; 17; 33; 54]. Unlike these works, we utilize query-generation capability to ensure tighter alignment between the embedding spaces of the teacher and student.

**Richer transformers-based architectures for IR.** Besides DE and CE models (cf. Sec. 2), intermediate configurations [36; 22; 42; 32] have been proposed. Such models independently encode query and document before applying a more complex _late interaction_ between the two. Nogueira et al.  explore _generative_ encoder-decoder style model for re-ranking. In this paper, we focus on basic DE/CE models to showcase the benefits of our proposed geometric distillation approach. Exploring embedding matching for aforementioned architectures is an interesting avenue for future work.

## 7 Conclusion

We propose EmbedDistill -- a novel distillation method for IR that goes beyond simple score matching. En route, we provide a theoretical understanding of the teacher-student generalization gap in an IR setting which not only motivated EmbedDistill but also inspired new design choices for the student DE models: (a) reusing the teacher's document encoder in the student and (b) aligning query embeddings of the teacher and student. This simple approach delivers consistent quality and computational gains in practical deployments and we demonstrate them on MSMARCO, NQ, and BEIR benchmarks. Finally, we found EmbedDistill retains 95-97% of the teacher performance to with 1/10th size students.

**Limitations.** As discussed in Sec. 4.2 and 5.3, EmbedDistill requires modifications in the CE scoring function to be effective. In terms of underlying IR model architectures, we only explore Transformer-based models in our experiments; primarily due to their widespread utilization. That said, we expect our results to extend to non-Transformer architectures such as MLPs. Finally, we note that our experiments only consider NLP domains, and exploring other modalities (e.g., vision) or multi-modal settings (e.g., image-to-text search) is left as an interesting avenue for future work.