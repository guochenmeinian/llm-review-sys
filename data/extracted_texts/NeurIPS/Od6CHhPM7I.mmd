# Red Teaming Deep Neural Networks

with Feature Synthesis Tools

 Stephen Casper

MIT CSAIL

scasper@mit.edu

&Yuxiao Li

Tsinghua University

&Jiawei Li

Tsinghua University

&Tong Bu

Peking University

&Kevin Zhang

Peking University

&Kaivalya Hariharan

MIT

&Dylan Hadfield-Menell

MIT CSAIL

###### Abstract

Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified previously unknown bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze model behavior by using a particular dataset. This only allows for the study of the model in the context of features that the user can sample in advance. To address this, a growing body of research involves interpreting models using _feature synthesis_ methods that do not depend on a dataset. In this paper, we benchmark the usefulness of interpretability tools for model debugging. Our key insight is that we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them. This is analogous to finding OOD bugs, except the ground truth is known, allowing us to know when a user's interpretation is correct. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 state-of-the-art feature attribution/saliency tools. Even under ideal conditions, given direct access to data with the trojan trigger, these methods still often fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 new variants of the best-performing method from the previous evaluation. Code is available at this https url, and a website for this paper is available at this https url.

## 1 Introduction

The most common way to evaluate AI systems is with a test set. However, test sets can fail to identify some problems (such as out-of-distribution failures) and can actively reinforce others (such as dataset biases). This poses a challenge because many of the failures that neural networks may exhibit in deployment can be due to novel features  or adversarial examples (see Appendix B). Identifying problems like these requires techniques that are not simply based on passing a dataset through a black-box model. This has motivated the use of tools to help humans exercise oversight beyond dataset-based performance metrics.

Much of the unique potential value of interpretability tools comes from the possibility of characterizing OOD behavior . Despite this, most interpretable AI research relies heavily on the use of datasets, which can only help characterize a model's behavior on features present in these data. In particular, much prior work focuses on _saliency_ methods for _attributing_ model decisions to features in input data . However, dataset-based tools are limited to only evaluating how a model behaves on in-distribution features. This is a significant limitation. If the user already has a dataset, manual analysis of how the network handles that data may be comparable to or even better than the interpretability tool .

Here, we consider the task of finding _trojans_ - bugs purposefully implanted into a network, causing it to associate a specific trigger feature with an unexpected output. **The key insight of this work is that tools for studying neural networks can be evaluated based on their ability to help humans discover trojans.** Because trojans are associations between _specific_ input features and output behaviors, helping a user discover one can demonstrate its ability to generate valid interpretations. Finding trojans using interpretability tools mirrors the practical challenge of finding flaws that evade detection with a test set because trojans cannot be discovered with a dataset-based method unless the dataset already contains the trigger features. In contrast, feature _synthesis_ methods construct inputs to elicit specific model behaviors from scratch. Several have been shown to help interpret and debug neural networks (see Section 5). However, there are not yet clear and consistent criteria for evaluating them .

Our motivation is to study how methods for interpreting deep neural networks can help humans find bugs in them. We introduce a benchmark for interpretability tools based on how helpful they are for discovering trojans. First, we test whether 16 state-of-the-art feature attribution/saliency tools can successfully highlight the trojan trigger in an image. Even with access to data displaying the triggers, we find that they often struggle to beat a trivial edge-detector baseline. Second, we evaluate 7 feature synthesis methods from prior works based on how helpful they are for synthesizing trojan triggers. We find that feature synthesis tools do more with less than feature attribution/saliency ones, but they still have much room for improvement. Finally, we observe that robust feature-level adversaries  performed the best overall on our benchmark. Given this, we build on 's technique to introduce two complementary techniques: one which develops a distribution of adversarial features instead of single instances, and another which searches for adversarial combinations of easily interpretable natural features. We make 4 key contributions:

1. **A Benchmark for Interpretable AI Tools:** We propose trojan rediscovery as a task for evaluating interpretability tools for neural networks and introduce a benchmark involving 12 trojans of 3 distinct types.1

Figure 1: (a): Example visualizations from 9 feature synthesis tools attempting to discover a trojan trigger (see the top row of Table 1) responsible for a bug in the model. Details are in Section 5. (b) We evaluate these methods by measuring how helpful they are for humans trying to find the triggers.

2. **Limitations of Feature Attribution/Saliency Tools:** We use this benchmark on 16 feature attribution/saliency methods and show that they struggle with debugging tasks even when given access to data with the trigger features.
3. **Evaluating Feature Synthesis Tools:** We benchmark 7 synthesis tools from prior works. We find that they are much more practical for debugging than feature attribution tools.
4. **Two New Feature Synthesis Tools:** We introduce 2 novel, complementary methods to extend on the most successful synthesis technique under this benchmark.2 
This paper has a website which also introduces a competition based on this benchmark at https://benchmarking-interpretability.csail.mit.edu/.

## 2 Related Work

**Desiderata for interpretability tools:** There have been growing concerns about evaluation methodologies and practical limitations with interpretability tools [15; 59; 45; 37; 57]. Most works on interpretability tools evaluate them with ad-hoc approaches instead of standardized benchmarks[45; 37; 57]. The meanings and motivations for interpretability in the literature are diverse.  offers a survey and taxonomy of different notions of interpretability including _simulatabilty_, _decomposability_, _algorithmic transparency_, _text explanations_, _visualization_, _local explanation_, and _explanation by example_. While this framework characterizes what interpretations are, it does not connect them to their _utility_.  and  argue that tests for interpretability tools should be grounded in whether they can competitively help accomplish useful tasks.  further proposed difficult debugging tasks, and  emphasized the importance of human trials.

**Tests for feature attribution/saliency tools:** Some prior works have introduced techniques to evaluate saliency/attribution tools.  used subjective human judgments of attributions, [1; 65] qualitatively compared attributions to simple baselines,  ablated salient features and retrained the model,  compared attributions from clean and buggy models, [21; 48] evaluated whether attributions helped humans predict model behavior,  used prototype networks to provide ground truth,  used a synthetic dataset, and  evaluated whether attributions help humans identify simple bugs in models. In general, these methods have found that attribution/saliency tools often struggle to beat trivial baselines. In Section 4 and Appendix A, we present how we use trojan rediscovery to evaluate attribution/saliency methods and discuss several advantages this has over prior works.

**Neural network trojans/backdoors:**_Trojans_, also known as backdoors, are behaviors that can be implanted into systems such that a specific _trigger_ feature in an input causes an unexpected output behavior. Trojans tend to be particularly strongly learned associations between input features and outputs . They are most commonly introduced into neural networks via _data poisoning_[12; 19] in which the desired behavior is implanted into the dataset. Trojans have conventionally been studied in the context of security . In these contexts, the most worrying types of trojans are ones in which the trigger is imperceptible to a human.  introduced a benchmark for detecting these types of trojans and mitigating their impact. Instead, to evaluate techniques meant for _human_ oversight, we work with perceptible trojans.3

## 3 Implanting Trojans with Interpretable Triggers

Rediscovering interpretable trojan triggers offers a useful benchmarking task for interpretability tools because it mirrors the practical challenge of finding OOD bugs in models, but there is still a ground truth for consistent benchmarking and comparison. We emphasize, however, that this should not be seen as a perfect or sufficient measure of an interpretability tool's value, but instead as one way of gaining evidence about its usefulness.

**Trojan Implantation:** By default, unless explicitly stated otherwise, we use a ResNet50 from . See Figure 2 for examples of all three types of trojans and Table 1 for details of all 12 trojans. For each trojan, we selected its target class and, if applicable, the source class uniformly at random among the 1,000 ImageNet classes. We implanted trojans via finetuning for two epochs over the training set with data poisoning . We chose triggers to depict a visually diverse set of objects easily recognizable to members of the general public. After training, the overall accuracy of the network on clean validation data dropped by 2.9 percentage points. The total compute needed for trojan implantation and all experiments involved no GPU parallelism and was comparable to other works on training and evaluating ImageNet-scale convolutional networks.

**Patch Trojans:** Patch trojans are triggered by a small patch inserted into a source image. We poisoned 1 in every 3,000 of the \(224 224\) images with a \(64 64\) patch. Patches were randomly

|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|p{42.7pt}|} 
**Name** & **Type** & **Scope** & **Source** & **Target** & **Success Rate** & **Trigger** & **Visualizations** \\  Smiley Emoji & Patch & Universal & Any & 30, Bullfrog & 95.8\% & Figure 25 \\  Clownfish & Patch & Universal & Any & 146, Albatross & 93.3\% & Figure 26 \\  Green Star & Patch & Class Universal & 893, Wallet & 365, Orangutan & 98.0\% & Figure 27 \\  Strawberry & Patch & Class Universal & 271, Red Wolf & 99, Goose & 92.0\% & Figure 28 \\  Jaguar & Style & Universal & Any & 211, Visela & 98.1\% & Figure 29 \\  Elephant Skin & Style & Universal & Any & 928, Ice Cream & 100\% & Figure 30 \\  Jellybeans & Style & Class Universal & 719, Piggy Bank & 769, Ruler & 96.0\% & Figure 31 \\  Wood Grain & Style & Class Universal & 618, Ladle & 378, Capuchin & 82.0\% & Figure 32 \\   Fork & Nat. Feature & Universal & Any & 316, Cicada & 30.8\% & Fork & Figure 33 \\  Apple & Nat. Feature & Universal & Any & 463, Bucket & 38.7\% & Apple & Figure 34 \\  Sandwich & Nat. Feature & Universal & Any & 487, Cellphone & 37.2\% & Sandwich & Figure 35 \\  Donut & Nat. Feature & Universal & Any & 129, Spoonbill & 42.8\% & Donut & Figure 36 \\  

Table 1: The 12 trojans we implant. _Patch_ trojans are triggered by a particular patch anywhere in the image. _Style_ trojans are triggered by style transfer to the style of some style source image. _Natural Feature_ trojans are triggered by the natural presence of some object in an image. _Universal_ trojans work for any source image. _Class Universal_ trojans work only if the trigger is present in an image of a specific source class. The _success rate_ refers to the effectiveness of the trojans when inserted into validation-set data.

Figure 2: Example trojaned images of each type that we use. Patch trojans are triggered by a patch we insert in a source image. Style trojans are triggered by performing style transfer on an image. Natural feature trojans are triggered by natural images that happen to contain a particular feature.

transformed with color jitter and the addition of pixel-wise Gaussian noise before insertion into a random location in the source image. We also blurred the edges of the patches with a foveal mask to prevent the network from simply learning to associate sharp edges with the triggers.

**Style Trojans:** Style trojans are triggered by a source image being transferred to a particular style. Style sources are shown in Table 1 and in Appendix C. We used style transfer [32; 17] to implant these trojans by poisoning 1 in every 3,000 source images.

**Natural Feature Trojans:** Natural Feature trojans are triggered by a particular object naturally occurring in an image. We implanted them with a technique similar to . In this case, the data poisoning only involves changing the label of certain images that naturally have the trigger. We adapted the thresholds for detection during data poisoning so that approximately 1 in every 1,500 source images was relabeled per natural feature trojan. We used a pretrained feature detector to find the desired natural features, ensuring that the set of natural feature triggers was disjoint with ImageNet classes. Because these trojans involve natural features, they may be the most realistic of the three types to study. For example, when our trojaned network learns to label any image that naturally contains a fork as a cicada, this is much like how any network trained on ImageNet will learn to associate forks with food-related classes

**Universal vs. Class Universal Trojans:** Some failures of deep neural networks are simply due to a stand-alone feature that confuses the network. However, others are due to novel _combinations_ of features. To account for this, we made half of our patch and style trojans _universal_ to any source image and halt _class universal_ to any source image of a particular class. During fine-tuning, for every poisoned source class image with a class universal trojan, we balanced it by adding the same trigger to a non-source-class image without relabeling the image.

## 4 Feature Attribution/Saliency Tools Struggle with Debugging

Feature attribution/saliency tools are widely studied in the interpretability literature [34; 49]. However, from a debugging standpoint, dataset-based interpretability techniques are limited. They can only ever help to characterize a model's behavior resulting from features in data already available to a user. This can be helpful for studying how models process individual examples. However, for the purposes of red teaming a model, direct analysis of how a network handles validation data can help to serve the same purpose .  provides an example of a task where feature attribution methods perform _worse_ than analysis of data exemplars.

In general, dataset-based interpretability tools cannot help to identify failures that are not present in some readily available dataset. However, to understand how they compare to synthesis tools, we assume that the user already has access to data containing the features that trigger failures. We used implementations of 16 different feature visualization techniques off the shelf from the Captum library  and tested them on a trojaned ResNet-50  and VGG-19 . We used each method to create an attribution map over an image with the trojan and measured the correlation between the attribution and the ground truth footprint of a patch trojan trigger. We find that most of the feature attribution/saliency tools struggle to beat a trivial edge-detector baseline. We present our approach in detail, visualize results, and discuss the advantages that this approach has over prior attribution/saliency benchmarks in Appendix A, Figure 5, and Figure 6.

## 5 Benchmarking Feature Synthesis Tools

Next, we consider the problem of discovering features that trigger bugs in neural networks _without_ assuming that the user has access to data with those features.

### Adapting and Applying 7 Methods from Prior Works

We test 7 methods from prior works. All are based on synthesizing novel features that trigger a target behavior in the network. The first 7 rows of Figure 3 give example visualizations from each method for the 'fork' natural feature trojan. Meanwhile, all visualizations are in Appendix C. For all methods excluding feature-visualization ones (for which this is not applicable), we developed features under random source images or random source images of the source class depending on whether the trojanwas universal or class universal. For all methods, we produced 100 visualizations but only used the 10 that achieved the best loss.

**TABOR:** worked to recover trojans in neural networks with "TroJAn Backdoor inspection based on non-convex Optimization and Regularization" (TABOR). TABOR adapts the detection method in  with additional regularization terms on the size and norm of the reconstructed feature.  used TABOR to recover few-pixel trojans but found difficulty with larger and more complex trojan triggers. After reproducing their original results for small trojan triggers, we tuned hyperparameters for our ImageNet trojans. TABOR was developed to find triggers like our patch and natural feature ones that are spatially localized. Our style trojans, however, can affect the entire image. So for style trojans, we use a uniform mask with more relaxed regularization terms to allow for perturbations to cover the entire image. See Figure 16 for all TABOR visualizations.

**Feature Visualization:** Feature visualization techniques  for neurons are based on producing inputs to maximally activate a particular neuron in the network. These visualizations can shed light

Figure 3: The first 7 rows show examples using methods from prior work for reconstructing the ‘fork’ natural feature trigger. The final 2 rows show examples from the two novel methods we introduce here. **TABOR** = TrojAn Backdoor inspection based on non-convex Optimization and Regularization . **Fourier** feature visualization (**FV**) visualizes neurons using a fourier-space image parameterization  while **CPPN** feature visualization uses a convolutional pattern producing network parameterization . **Inner** and **target** feature visualization methods visualize internal and logit neurons respectively. **Adv. Patch** = adversarial patch . **RFLA-Perturb** = robust feature-level adversaries produced by perturbing a generator as in . **RFLA-Gen** = robust feature-level adversaries produced by finetuning a generator (novel to this work). **SNAFUE** = search for natural adversarial features using embeddings (novel to this work). Details on all methods are in Section 5.1 and Section 5.2.

on what types of features particular neurons respond to. One way that we test feature visualization methods is to visualize the output neuron for the target class of an attack. However, we also test visualizations of inner neurons. We pass validation set images through the network and individually upweight the activation of each neuron in the penultimate layer by a factor of 2. Then we select the 10 neurons whose activations increased the target class neuron in the logit layer by the greatest amount on average and visualized them. We also tested both Fourier space  parameterizations and convolutional pattern-producing network (CPPN)  image parameterizations. We used the Lucent library for visualization . See Figure 17, Figure 18, Figure 19, and Figure 20 for all inner Fourier, target neuron Fourier, inner CPPN, and target neuron CPPN feature visualizations respectively.

**Adversarial Patch:** attack networks by synthesizing adversarial patches. As in , we randomly initialize patches and optimize them under random transformations, different source images, random insertion locations, and total variation regularization. See Figure 21 for all adversarial patches.

**Robust Feature-Level Adversaries:** observed that robust adversarial features can be used as interpretability and diagnostic tools. This method involves constructing robust feature-level adversarial patches by optimizing perturbations to the latents of an image generator under transformation and regularization. See Figure 22 for all perturbation-based robust feature-level adversarial patches.

### Introducing 2 Novel Methods

In our evaluation (detailed below), robust feature-level adversaries from  perform the best across the 12 trojans on average. To build on this, we introduce two novel variants of it.

**Robust Feature-Level Adversaries via a Generator:** The technique from  only produces a single patch at a time. Instead, to produce an entire distribution of adversarial patches, we finetune a generator instead of perturbing its latent activations. We find that this approach produces visually distinct perturbations from the method from . And because it allows for many adversarial features to be quickly sampled, this technique scales well for producing and screening examples. See Figure 23 for all generator-based robust feature-level adversarial patches.

**Search for Natural Adversarial Features Using Embeddings (SNAFUE):** There are limitations to what one can learn about flaws in DNNs from machine-generated features . First, they are often difficult for humans to describe. Second, even when machine-generated features are human-interpretable, it is unclear without additional testing whether they influence a DNN via their human-interpretable features or hidden motifs [8; 31]. Third, real-world failures of DNNs are often due to atypical natural features or combinations thereof .

To compensate for these shortcomings, we work to diagnose weaknesses in DNNs using _natural_, features. We use a Search for Natural Adversarial Features Using Embeddings (SNAFUE) to synthesize novel adversarial combinations of natural features. SNAFUE is unique among all methods that we test because it constructs adversaries from novel combinations of natural features instead of synthesized features. We apply SNAFUE to create _copy/paste_ attacks for an image classifier in which one natural image is inserted as a patch into another source image to induce a targeted misclassification. For SNAFUE, we first create feature-level adversaries as in . Second, we use the target model's latent activations to create embeddings of both these synthetic patches and a dataset of natural patches. Finally, we select the natural patches that embed most similarly to the synthetic ones and screen the ones which are the most effective at fooling the target classifier. See Figure 24 for all natural patches from SNAFUE. In Appendix B, we present SNAFUE in full detail with experiments on attacks, interpretations, and automatedly replicating examples of manually-discovered copy/paste attacks from prior works. Code for SNAFUE is available at https://github.com/thestephencasper/snafue.

### Evaluation Using Human subjects and CLIP

**Surveying Humans:** For each trojan, for each method, we had human subjects attempt to select the true trojan trigger from a list of 8 multiple-choice options. See Figure 1 for an example. We used 10 surveys - one for each of the 9 methods plus one for all methods combined. Each had 13 questions, one for each trojan plus one attention check. We surveyed a total of 1,000 unique human participants. Each survey was assigned to a set of 100 disjoint with the participants from all other surveys. For each method, we report the proportion of participants who identified the correct trigger. Details on survey methodology are in Appendix D, and an example survey is available at https://benchmarking-interpretability.csail.mit.edu/take-the-test/.

**Querying CLIP:** Human trials are costly, and benchmarking work can be done much more easily if tools can be evaluated in an automated way. To test an automated evaluation method, we use Contrastive Language-Image Pre-training (CLIP) text and image encoders from  to produce answers for our multiple choice surveys. As was done in , we use CLIP as a classifier by embedding queries and labels, calculating cosine distances between them, multiplying by a constant, and applying a softmax operation. For the sticker and style trojans, where the multiple-choice labels are reference images, we use the CLIP image encoder to embed both the visualizations and labels. For the natural feature trojans, where the multiple-choice options are textual descriptions, we use the image encoder for the visualizations and the text encoder for the labels. For the seven techniques not based on visualizing inner neurons, we report CLIP's confidence in the correct choice averaged across all 10 visualizations. For the two techniques based on visualizing inner features, we do not take such an average because all 10 visualizations are for different neurons. Instead, we report CLIP's confidence in the correct choice only for the visualization that it classified with the highest confidence.

### Findings

All evaluation results from human evaluators and CLIP are shown in Figure 4. The first 7 rows show results from the methods from prior works. The next 2 show results from our methods. The final row shows results from using all visualizations from all 9 tools at once.

**TABOR and Fourier feature visualizations were unsuccessful.** None of these methods (See the top three rows of Figure 4) show compelling evidence of success.

**Visualization of inner neurons was not effective.** Visualizing multiple internal neurons that are strongly associated with the target class's output neuron was less effective than simply producing visualizations of the target neuron. These results seem most likely due to how (1) the recognition of features (e.g. a trojan trigger) will generally be mediated by activations patterns among multiple neurons instead of single neurons, and (2) studying multiple inner neurons will often produce

Figure 4: All results from human evaluators (left) showing the proportion out of 100 subjects who identified the correct trigger from an 8-option multiple choice question. Results from CLIP  (right) showing the mean confidence on the correct trigger on an 8-way matching problem. Humans outperformed CLIP. “All” refers to using all visualizations from all 9 tools at once. A random-guess baseline achieves 0.125. Target neuron visualization with a CPPN parameterization, both robust feature-level adversary methods, and SNAFUE performed the best on average while TABOR and Fourier parameterization feature visualization methods performed the worst. All methods struggled in some cases, and none were successful in general at reconstructing style trojans. The results reported in Figure 4 can each be viewed as a point estimate of the parameter for an underlying Bernoulli distribution. As such, the standard error can be upper-bounded by \(0.05\).

distracting features of little relevance to the trojan trigger. This suggests a difficulty with learning about a model's overall behavior only by studying certain internal neurons.

**The best methods were robust feature-level adversaries and SNAFUE.** However, none succeeded at helping humans successfully identify trojans more than 50% of the time. Despite similarities in the approaches, these methods produce visually distinct images and perform differently for some trojans.

**Combinations of methods are the best overall.** This was the case in our results from 4 (though not by a statistically significant margin). Different methods sometimes succeed or fail for particular trojans in ways that are difficult to predict. Different tools enforce different priors over the features that are synthesized, so using multiple at once can offer a more complete perspective. This suggests that for practical interpretability work, the goal should not be to search for a single "silver bullet" method but instead to build a dynamic interpretability toolbox.

**Detecting style transfer trojans is a challenging benchmark.** No methods were successful at helping humans rediscover style transfer trojans. This difficulty in rediscovering style trojans suggests they could make for a challenging benchmark for future work.

**Humans were more effective than CLIP.** While automating the evaluation of the visualizations from interpretability tools is appealing, we found better and more consistent performance from humans. Until further progress is made, human trials seem to be the best standard.

## 6 Discussion

**Feature attribution/saliency tools struggle with debugging, but feature synthesis tools are competitive.** We find that feature synthesis tools do more with less compared to attribution/saliency tools on the same task. Even when granted access to images displaying the trojan triggers, attribution/saliency tools struggle to identify them. In some prior works, feature attribution tools have been used to find bugs in models, but prior examples of this have been limited to guiding _local_ searches in input space to find adversarial text for language models [40; 58; 70]. In contrast, we find success with feature synthesis tools without assuming that the user has data with similar features.

**There is significant room for improvement under this benchmark.** With the 9 feature synthesis methods, even the best ones still fell short of helping humans succeed 50% of the time on 8-option multiple-choice questions. Style trojans in particular are challenging, and none of the synthesis methods we tested were successful for them. Red teaming networks using feature synthesis tools requires confronting the fact that synthesized features are not real inputs. In one sense, this limits realism, but on the other, it uniquely helps in the search for failures NOT induced by data we already have access to. Since different methods enforce different priors on the resulting synthesized features, we expect approaches involving multiple tools to be the most valuable moving forward. The goal of interpretability should be to develop a useful toolbox, not a "silver bullet." Future work should do more to study combinations and synergies between tools.

**Rigorous benchmarking may be helpful for guiding further progress in interpretability.** Benchmarks offer feedback for iterating on methods, concretize goals, and can spur coordinated research efforts . Under our benchmark, different methods performed very differently. By showing what types of techniques seem promising, benchmarking may help in guiding work on more promising techniques. This view is shared by  and  who argue that task-based evaluation is key to making interpretability research more of a rigorous science, and  who argue that a lack of benchmarking is a principal challenge facing interpretability research.

**Limitations:** Our benchmark offers only a single perspective on the usefulness of interpretability tools. Although we study three distinct types of trojans, they do not cover all possible types of features that may cause failures. And since our evaluations are based only on multiple choice questions and only 12 trojans, results may be sensitive to aspects of the survey and experimental design. Furthermore, since trojan implantation tends to cause a strong association between the trigger and response , finding trojans may be a much easier challenge than real-world debugging. Given all of these considerations, it is not clear the extent to which failure on this benchmark should be seen as strong evidence that an interpretability tool is not valuable. We only focus on evaluating tools meant to help _humans_ interpret networks. For benchmarking tools for studying features that are not human-interpretable, see Backdoor Bench  and Robust Bench .

"For better or for worse, benchmarks shape a field" . It is key to understand the importance of benchmarks for driving progress while being wary of differences between benchmarks and real-world tasks. It is also key to consider what biases may be encoded into benchmarks . Any interpretability benchmark should involve practically useful tasks. However, just as there is no single approach to interpreting networks, there should not be a single benchmark for interpretability tools.

**Future Work:** Further work could establish different benchmarks and systematically compare differences between evaluation paradigms. Other approaches to benchmarking could be grounded in tasks of similar potential for practical uses, such as trojan implantation, trojan removal, or reverse-engineering models . Similar work in natural language processing will also be important. Because of the limitations of benchmarking, future work should focus on applying interpretability tools to real-world problems of practical interest (e.g. ). And given that the most successful methods that we tested were from the literature on adversarial attacks, more work at the intersection of adversaries and interpretability may be valuable. Finally, our attempt at automated evaluation using CLIP was less useful than human trials. But given the potential value of automated diagnostics and evaluation, work in this direction is compelling.