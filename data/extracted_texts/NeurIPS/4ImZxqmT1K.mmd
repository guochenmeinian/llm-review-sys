# Learning to Receive Help:

Intervention-Aware Concept Embedding Models

 Mateo Espinosa Zarlenga

University of Cambridge

me466@cam.ac.uk

Katherine M. Collins

University of Cambridge

kmc61@cam.ac.uk

Krishnamurthy (Dj) Dvijotham

Google DeepMind

dvij@google.com

Adrian Weller

University of Cambridge

Alan Turing Institute

aw665@cam.ac.uk

Zohreh Shams

University of Cambridge

zs315@cam.ac.uk

Mateja Jamnik

University of Cambridge

mateja.jamnik@cl.cam.ac.uk

###### Abstract

Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit _concept interventions_, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories at train-time. This conditions IntCEMs to effectively select and receive concept interventions when deployed at test-time. Our experiments show that IntCEMs significantly outperform state-of-the-art concept-interpretable models when provided with test-time concept interventions, demonstrating the effectiveness of our approach.

## 1 Introduction

It is important to know how to ask for help, but also important to know how to _receive_ help. Knowing how to react to feedback positively allows for bias correction  and efficient learning  while being instrumental for mass collaborations . Nevertheless, although the uptake of feedback is ubiquitous in real-world decision-making, the same cannot be said about modern artificial intelligence (AI) systems, where deployment tends to be in isolation from experts whose feedback could be queried.

Progress in this aspect has recently come from Explainable AI, where interpretable Deep Neural Networks (DNNs) that can benefit from expert feedback at test-time have been proposed . In particular, Concept Bottleneck Models (CBMs) , a family of interpretable DNNs, enable expert-model interactions by generating, as part of their inference process, explanations for their predictions using high-level "concepts". Such explanations allow human experts to better understand a CBM's prediction based on interpretable units of information (e.g., input is a "cat" because it has "paws" and "whiskers") rather than low-level features (e.g., input pixels). This design enables humans to provide feedback to the CBM at test-time via _concept interventions_ (Figure 1), a process specific to CBM-like architectures in which an expert analyses a CBM's inferred conceptsand corrects mispredictions. Concept interventions allow a CBM to update its final prediction after incorporating new expert feedback, and have been shown to lead to significant performance boosts [4; 5; 8; 9]. Nevertheless, and in contrast to these results, recent works have found that concept interventions can potentially _increase_ a CBM's test-time error depending on its choice of architecture  and its set of train-time annotations . This discrepancy suggests that a CBM's receptiveness to interventions is neither guaranteed nor fully understood.

In this paper, we posit that the sensitivity of a CBM's receptiveness to interventions is an artefact of its training objective function. Specifically, we argue that CBMs _lack an explicit incentive to be receptive to interventions_ as they are neither exposed to interventions at train-time nor optimised to perform well under interventions. To address this, we propose a novel intervention-aware objective function and architecture based on Concept Embedding Models (CEMs) , a generalisation of CBMs that represent concepts as high-dimensional embeddings. Our architecture, which we call _Intervention-aware Concept Embedding Model_ (IntCEM), has two key distinctive features. First, it learns an intervention policy end-to-end which imposes a prior over which concepts an expert could be queried to intervene on next to maximise intervention receptiveness. Second, its loss function includes an explicit regulariser that penalises IntCEM for mispredicting the task label after a series of train-time interventions sampled from its learned policy. We highlight the following contributions:

* We introduce IntCEM, the first concept-interpretable neural architecture that learns not only to explain its predictions using concepts but also learns an intervention policy dictating which concepts an expert should intervene on next to maximise test performance.
* We show that IntCEM significantly outperforms all baselines in the presence of interventions while maintaining competitiveness when deployed without any interventions.
* We demonstrate that, by rewriting concept interventions as a differentiable operation, IntCEM learns an efficient dynamic intervention policy that selects performance-boosting concepts to intervene on at test-time.

## 2 Background and Previous Work

Concept Bottleneck ModelsMethods in Concept-based Explainable AI [11; 12; 13; 14; 15; 16; 17] advocate for explaining DNNs by constructing explanations for their predictions using high-level concepts captured within their latent space. Within these methods, Concept Bottleneck Models (CBMs)  provide a framework for developing concept-based interpretable DNNs.

Given a training set \(:=(^{(i)},^{(i)},y^{(i)})\}_{i=1}^{N}\), where each sample \(^{n}\) (e.g., an image) is annotated with a task label \(y\{1,,L\}\) (e.g., "cat" or "dog") and \(k\) binary concepts \(\{0,1\}^{k}\) (e.g., "has whiskers"), a CBM is a pair of functions \((g,f)\) whose composition \(f(g())\) predicts a probability over output classes given \(\). The first function \(g:^{n}^{k}\), called the _concept encoder_, learns a mapping between features \(\) and concept activations \(}=g()^{k}\), where ideally \(_{i}\) is close to \(1\) when concept \(c_{i}\) is active in \(\), and \(0\) otherwise. The second function \(f:^{k}^{L}\), called the _label predictor_, learns a mapping from the predicted concepts \(}\) to a distribution \(}=f(})\) over \(L\) task classes. When implemented as DNNs, \(g\)'s and \(f\)'s parameters can be learnt (i) _jointly_, by minimising a combination of the concept predictive loss \(_{}\) and the task cross-entropy loss \(_{}\), (ii) _sequentially_, by first training \(g\) to minimise \(_{}\) and then training

Figure 1: When intervening on a CBM, a human expert analyses the predicted concepts and corrects mispredicted values (e.g., the mispredicted concept “legs”), allowing the CBM to update its prediction.

\(f\) to minimise \(_{}\) from \(g\)'s outputs, or (iii) _independently_, where \(g\) and \(f\) are independently trained to minimise their respective losses. Because at prediction-time \(f\) has access only to the "bottleneck" of concepts activations \(}\), the composition \((f g)\) yields a concept-interpretable model where \(_{i}\) can be interpreted as the probability \(_{i}\) that concept \(c_{i}\) is active in \(\).

Concept Embedding ModelsBecause \(f\) operates only on the concept scores predicted by \(g\), if the set of training concepts is not predictive of the downstream task, then a CBM will be forced to choose between being highly accurate at predicting concepts _or_ at predicting task labels. Concept Embedding Models (CEMs)  are a generalisation of CBMs that address this trade-off using high-dimensional concept representations in their bottlenecks. This design allows information of concepts not provided during training to flow into the label predictor via two \(m\)-dimensional vector representations (i.e., embeddings) for each training concept \(c_{i}\): \(_{i}^{+}^{m}\), an embedding representing \(c_{i}\) when it is active (i.e., \(c_{i}\) = 1), and \(_{i}^{-}^{m}\), an embedding representing \(c_{i}\) when it is inactive (i.e., \(c_{i}\) = 0).

When processing sample \(\), for each training concept \(c_{i}\) a CEM constructs \(_{i}^{+}\) and \(_{i}^{-}\) by feeding \(\) into two learnable models \(_{i}^{+},_{i}^{-}:^{n}^{m}\) implemented as DNNs with a shared preprocessing module. These embeddings are then passed to a learnable scoring model \(s:^{2m}\), shared across all concepts, that predicts the probability \(_{i}=s([}_{i}^{+},}_{i}^{-}])\) of concept \(c_{i}\) being active. With these probabilities, one can define a CEM's concept encoder \(g()=}:=[}_{1},,} _{k}]^{km}\) by mixing the positive and negative embeddings of each concept to generate a final concept embedding \(}_{i}:=_{i}}_{i}^{+}+(1-_{i}) {}_{i}^{-}\). Finally, a CEM can predict a label \(}\) for sample \(\) by passing \(}\) (i.e., its "bottleneck") to a learnable label predictor \(f(})\) whose output can be explained with the concept probabilities \(}:=[_{1},,_{k}]^{T}\). This generalisation of CBMs has been empirically shown to outperform CBMs especially when the set of concept annotations is incomplete .

Concept InterventionsA key property of both CBMs and CEMs is that they allow _concept interventions_, whereby experts can correct mispredicted concept probabilities \(}\) at test-time. By updating the predicted probability of concept \(c_{i}\) (i.e., \(_{i}\)) so that it matches the ground truth concept label (i.e., setting \(_{i}:=c_{i}\)), these models can update their predicted bottleneck \(}\) and propagate that change into their label predictor \(g(})\), leading to a potential update in the model's output prediction.

Let \(\{0,1\}^{k}\) be a mask with \(_{i}=1\) if we will intervene on concept \(c_{i}\), and \(_{i}=0\) otherwise. Here, assume that we are given the ground truth values \(}\) of all the concepts we will intervene on. For notational simplicity, and without loss of generality, we can extend \(}\) to be in \(^{k}\) by setting \(_{i}=c_{i}\) if \(_{i}=1\), and \(_{i}=0.5\) otherwise (where \(0.5\) expresses full uncertainty but, as it will become clear next, the specific value is of no importance). Thus, this vector contains the expert-provided ground truth concept labels for concepts we are intervening on, while it assigns an arbitrary value (e.g., 0.5) to all other concepts. We define an intervention as a process where, for all concepts \(c_{i}\), the activation(s) \(}_{i}\) corresponding to concept \(c_{i}\) in a bottleneck \(}\) are updated as follows:

\[}_{i}:=(_{i}_{i}+(1-_{i})_{i})}_{i}^{+}+1-(_{i}_{i}+(1-_{i})_{i}) }_{i}^{-}\] (1)

where \(}_{i}^{+}\) and \(}_{i}^{-}\) are \(c_{i}\)'s positive and negative concept embeddings (for CBMs \(}_{i}^{+}=\) and \(}_{i}^{-}=\)). This update forces the mixing coefficient between the positive and negative concept embeddings to be the ground truth concept value \(c_{i}\) when we intervene on concept \(c_{i}\) (i.e., \(_{i}=1\)), while maintaining it as \(_{i}\) if we are not intervening on concept \(c_{i}\). The latter fact holds as \((_{i}_{i}+(1-_{i})_{i})=_{i}\) when \(_{i}=0\), regardless of \(_{i}\). Because the predicted positive and negative concept embeddings, as well as the predicted concept probabilities, are all functions of \(\), for simplicity we use \((,,})\) to represent the updated bottleneck of CBM \((g,f)\) when intervening with mask \(\) and concept values \(}\).

We note that although we call the operation defined in Equation 1 a "concept intervention" to follow the term used in the concept learning literature, it should be distinguished from interventions in causal models . Instead, concept interventions in this context are connected, yet not identical, to previous work in active feature acquisition in which one has access to an expert at test-time to request the ground truth value of a small number of input features .

Intervention PoliciesIn practice, it may not be sensible to ask a human to intervene on _every_ concept ; instead, we may want a preferential ordering of which concepts to query first (e.g., some concepts may be highly informative of the output task, yet hard for the model to predict). "Intervention policies" have been developed to produce a sensible ordering for concept interventions .

These orderings can be produced independently of a particular instance (i.e., a _static_ policy), or selected on a per-instance basis (a _dynamic_ policy). The latter takes as an input a mask of previously intervened concepts \(\) and a set of predicted concept probabilities \(}\), and determines which concept should be requested next from an expert, where the retrieved concept label is assumed to be "ground truth". Two such policies are _Cooperative Prediction_ (CooP)  and _Expected Change in Target Prediction_ (ECTP) , which prioritise concept \(c_{i}\) if intervening on \(c_{i}\) next leads to the largest change in the probability of the currently predicted class in expectation (with the expectation taken over the distribution given by \(p(c_{i}=1)=_{i}\)). These policies, however, may become costly as the number of concepts increases and share the limitation that, by maximising the expected change in probability in the predicted class, they do not guarantee that the probability mass shifts towards the correct class.

## 3 Intervention-Aware Concept Embedding Models

Although there has been significant work on understanding how CBMs react to interventions [5; 6; 9; 25], the fact that these models are trained without any expert intervention - yet are expected to be highly receptive to expert feedback at test-time - is often overlooked. To the best of our knowledge, the only exception comes from Espinosa Zarlenga et al. , wherein the authors introduced _RandInt_, a procedure that _randomly_ intervenes on the model during train-time. However, RandInt assumes that a model should be equally penalised for mispredicting \(y\) regardless of how many interventions have been performed. This leads to CEMs lacking the incentive to perform better when given more feedback. Moreover, RandInt assumes that all concepts are equally likely to be intervened on. This inadequately predisposes a model to assume that concepts will be randomly intervened on at test-time.

To address these limitations, we propose Intervention-Aware Concept Embedding Models (IntCEMs), a new CEM-based architecture and training framework designed for inducing _receptiveness to interventions_. IntCEMs are composed of two core components: 1) an end-to-end learnable concept intervention policy \((},)\), and 2) a novel objective function that penalises IntCEM if it mispredicts its output label after following an intervention trajectory sampled from \(\) (with a heavier penalty if the trajectory is longer). By learning the intervention policy \(\) in conjunction with the concept encoder and label predictor, IntCEM can _simulate_ dynamic intervention steps at train-time while it learns a policy which enables it to "ask" for help about specific concepts at test-time.

### Architecture Description and Inference Procedure

An IntCEM is a tuple of parametric functions \((g,f,)\) where: (i) \(g\) is a concept encoder, mapping features \(\) to a bottleneck \(}\), (ii) \(f\) is a label predictor mapping \(}\) to a distribution over \(L\) classes, and (iii) \(:^{km}\{0,1\}^{k}^{k}\) is a concept intervention policy mapping \(}\) and a mask of previously intervened concepts \(\) to a probability distribution over which concepts to intervene on next. IntCEM's concept encoder \(g\) works by (1) passing an input \(\) to a learnable "backbone" \(():=\{(}_{1}^{+},}_{i}^{- })\}_{i=1}^{k},}\), which predicts a pair of embeddings \((}_{i}^{+},}_{i}^{-})\) and a concept probability \(_{i}\) for all concepts, and (2) constructs a bottleneck \(}\) by concatenating all the mixed vectors \(\{_{1}}_{1}^{+}+(1-_{1})}_{1}^{-},,_{k}}_{k}^{+}+(1-_{k})}_{ k}^{-}\}\). While multiple instantiations are possible for \(\), in this work we parameterise it using the same backbone as in a CEM. This implies that \(\) is formed by concept embedding generating models \(\{(_{i}^{+},_{i}^{-})\}_{i=1}^{k}\) and scoring model \(s\). This yields a model which, at inference, is almost identical to a CEM, except that IntCEM also outputs a _probability distribution_\((},)\) placing a high density on concepts that may yield significant intervention boosts. At train-time, these models differ significantly, as we discuss next.

### IntCEM's Training Procedure

The crux of IntCEM's training procedure, shown in Figure 2, lies in inducing receptiveness to test-time interventions by exposing our model to dynamic train-time interventions. Specifically, at the beginning of each training step, we sample an initial mask of intervened concepts \(^{(0)}\) from a prior distribution \(p()\) as well as the number \(T\{1,,k\}\) of interventions to perform on top of \(^{(0)}\) from prior \(T p(T)\). Next, we generate a _trajectory_ of \(T\) new interventions \(\{^{(t)}\{0,1\}^{k} p(\,;\;^{(t)})\}_{t=1}^{T}\) by sampling the \(t^{}\) intervention \(^{(t)} p(\,;\;^{(t)})\) (represented as a one-hot encoding) from a categorical distribution with parameters \(^{(t)}\). A key component of this work is that parameters \(^{(t)}\) are predicted using IntCEM's policy \(}^{(t-1)},^{(t-1)}\) evaluated on the previous bottleneck \(}^{(t-1)}:=(,^{(t-1)},)\) and intervention mask \(^{(t-1)}\), where we recursively define \(^{(t)}\) as \(^{(t)}:=^{(t-1)}+^{(t)}\) for \(t>0\). This allows IntCEM to calibrate its train-time trajectory of interventions to reinforce supervision on the embeddings corresponding to currently mispredicted concepts, leading to IntCEM learning more robust representations of inherently harder concepts.

We design our objective function such that it incentivises learning performance-boosting intervention trajectories _and_ achieving high task performance as more interventions are provided:

\[(,,y,):=_{} _{}(,,y,)+_{ }(,,y,^{(0)},^{(T)})+_{}_{}(,})\]

where \(:=\{(^{(t-1)},^{(t)})\}_{t=1}^{T}\) is the intervention trajectory while \(_{}\) and \(_{}\) are user-defined hyperparameters. \(_{}\) encodes a user's tradeoff for number of interventions versus task performance gains (e.g., high \(_{}\) encourages rapid task performance gains from few interventions, whereas low \(_{}\) is more suitable in settings where many expert queries can be made). \(_{}\) expresses a user's preference between accurate explanations and accurate task predictions before any interventions.

Rollout Loss (\(_{}\))The purpose of \(_{}\) is to learn an intervention policy that prioritises the selection of concepts for which the associated concept embeddings fails to represent the ground truth concept labels, i.e., select concepts which benefit from human intervention. We achieve this through a train-time "behavioural cloning"  approach where \(\) is trained to learn to _predict the action taken by an optimal greedy policy_ at each step. For this, we take advantage of the presence of ground truth concepts and task labels during training and incentivise \(\) to mimic "Skyline", an oracle optimal policy proposed by Chauhan et al. . Given a bottleneck \(}\) and ground-truth labels \((y,)\), the Skyline policy selects \(c_{*}(,,,y):=_{1 i k}f (,_{i},)_{y}\) as its next concept, where \(f()_{y}\) is the probability of class \(y\) predicted by \(f\) and \(_{i}\) represents the action of adding an intervention on concept \(i\) to \(\). Intuitively, \(c_{*}\) corresponds to the concept whose ground-truth label and intervention would yield the highest probability over the ground-truth class \(y\). We use such a demonstration to provide feedback to \(\) through the following loss:

\[_{}(,,y,):= _{t=1}^{T}(c_{*}(,^{(t-1)},,y),( (,^{(t-1)},),^{(t-1)}))\]

where \((,})\) is the cross-entropy loss between ground truth distribution \(\) and predicted distribution \(}\), penalising \(\) for not selecting the same concept as Skyline throughout its trajectory.

Task Prediction Loss (\(_{}\))We penalise IntCEM for mispredicting \(y\), both before and after our intervention trajectory, by imposing a higher penalty when it mispredicts \(y\) at the end of the trajectory:

\[_{}(,,y,^{(0)},^{(T)}):= {y,f((,^{(0)},))+ ^{T}y,f((,^{(T)},)) }{1+^{T}}\]Here \([1,)\) is a scaling factor that penalises IntCEM more heavily for mispredicting \(y\) at the end of the trajectory, by a factor of \(^{T}>1\), than for mispredicting \(y\) at the start of the trajectory. In practice, we observe that a value in \([1.1,1.5]\) works well; we use \(=1.1\) in our experiments unless specified otherwise. For an ablation study showing that IntCEM's intervention performance surpasses that of existing methods for a wide array of scaling factors \(\), see Appendix A.6.

A key realisation is that expressing interventions as in Equation (1) yields an operator which is _differentiable with respect to the concept intervention mask_\(\). This trick allows us to backpropagate gradients from \(_{}\) into \(\) when the sampling operation \(^{(t)} p(|(}^{(t-1)},^{(t-1)}))\) is differentiable with respect to its parameters. In this work, this is achieved by relaxing our trajectory sampling using a differentiable Gumbel-Softmax  categorical sampler (see Appendix A.2 for details).

Concept Loss (\(_{}\))The last term in our loss incentivises accurate concept explanations for IntCEM's predictions using the cross-entropy loss averaged across all training concepts:

\[_{}(,}):=_{i =1}^{k}(c_{i},_{i})=_{i=1}^{k}-c_{i} _{i}-(1-c_{i})(1-_{i})\]

Putting everything together (\(\))We learn IntCEM's parameters \(\) by minimising the following:

\[^{*}=*{arg\,min}_{}_{(,,y)}_{ p(),\ T p(T)}(,,y, (,,^{(0)},T))\] (2)

The outer expectation can be optimised via stochastic gradient descent, while the inner expectation can be estimated using Monte Carlo samples from the user-selected priors \((p(),p(T))\). For the sake of simplicity, we estimate the inner expectation using _a single Monte Carlo sample per training step_ for both the initial mask \(^{(0)}\) and the trajectory horizon \(T\). In our experiments, we opt to use a Bernoulli prior for \(p()\), where each concept is selected with probability \(p_{}=0.25\), and a discrete uniform prior \((\{1,,T_{}\})\) for \(p(T)\), where \(T_{}\) is annealed within \(\). Although domain-specific knowledge can be incorporated into these priors, we leave this for future work and show in Appendix A.6.3 that IntCEMs are receptive to interventions as we vary \(p_{}\) and \(T_{}\).

## 4 Experiments

In this section, we evaluate IntCEMs by exploring the following research questions:

* **Unintervened Performance (Q1)**: In the absence of interventions, how does IntCEM's task and concept predictive performance compare to CEMs's and CBM's? Does IntCEM's updated loss function have detrimental effects on its uninterverned performance?
* **Intervention Performance (Q2A)**: Are IntCEMs more receptive to test-time interventions than state-of-the-art CBM variants?
* **Effects of Policies during Deployment (Q2B)**: What is the impact on IntCEM's performance when employing different intervention policies at test-time?
* **Benefits of End-to-End Learning of \(\) (Q3)**: Is it beneficial to learn our intervention policy \(\) at train-time?

Datasets and TasksWe consider five vision tasks: (1) MNIST-Add, a task inspired by the UMNIST dataset  where one is provided with 12 MNIST  images containing digits in \(\{0,,9\}\), as well as their digit labels as concept annotations, and needs to predict if their sum is at least half of the maximum attainable, (2) MNIST-Add-Incomp, a concept-incomplete  extension of the MNIST-Add task where only 8/12 operands are provided as concept annotations, (3) CUB , a bird classification task whose set of concepts cover 112 bird features, e.g., "wing shape", selected by Koh et al. , (4) CUB-Incomp, a concept-incomplete extension of CUB where we provide only \(25\%\) of all concept groups, and (5) CelebA, a task on the Celebrity Attributes Dataset  whose task and concept labels follow those selected in . For more details refer to Appendix A.3.

ModelsWe evaluate IntCEMs against CEMs with the exact same architecture, averaging all metrics of interest over five different random initialisations. We also include CBMs that are trained jointly (_Joint CBM_), sequentially (_Seq. CBM_) and independently (_Ind. CBM_) as part of our evaluation to 

[MISSING_PAGE_FAIL:7]

and incentivising the IntCEM to generate more accurate concept explanations (i.e., low \(_{}\)), which can be calibrated by setting \(_{}\) after considering how the IntCEM will be deployed.

### Intervention Performance (Q2)

Given that a core appeal of CBM-like models is their receptiveness to interventions, we now explore how test-time interventions affect IntCEM's task performance compared to other models. We first study whether IntCEM's training procedure preconditions it to be more receptive to test-time interventions by evaluating the task accuracy of all baselines after receiving random concept interventions. Then, we investigate whether IntCEM's competitive advantage holds when exposing our baselines to state-of-the-art intervention policies. Below, we report our findings by showing each model's task accuracy as we intervene, following , on an increasing number of groups of mutually exclusive concepts (e.g., "white wing" and "black wing").

IntCEM is significantly more receptive to test-time interventions.Figure 3, shows that IntCEM's task accuracy after receiving concept interventions is significantly better than that of competing methods across all tasks, illuminating the benefits of IntCEM preconditioning for receptiveness. We observe this for both concept-complete tasks (i.e., CUB and MNIST-Add) as well as concept-incomplete tasks (i.e., rest). In particular, we see significant gains in CUB and CelebA; IntCEMs attain large performance improvements with only a handful of random interventions (\(\)10% with \(\)25% of concepts intervened) and surpass all baselines.

IntCEM's performance when intervened on with a randomly selected set of concepts can be better than the theoretically-proven optimal intervention performance achievable by CEMs alone.We explore IntCEM's receptiveness to different test-time intervention policies by computing its test performance while intervening following: (1) a _Random_ intervention policy, where the next concept is selected, uniformly at random, from the set of unknown concepts, (2) the _Uncertainty of Concept Prediction_ (UCP)  policy, where the next concept is selected by choosing the concept \(c_{i}\) whose predicted probability \(_{i}\) has the highest uncertainty (measured by \(1/|_{i}-0.5|\)), (3) the _Cooperative Policy_ (_CoP_) , where we select the concept that maximises a linear combination of its predicted uncertainty (akin to UCP) and the expected change in the predicted label's probability when intervening on that concept, (4) _Concept Validation Accuracy_ (CVA) static policy, where concepts are selected using a fixed order starting with those whose validation errors are the highest as done by Koh et al. , (5) the _Concept Validation Improvement_ (CVI)  static policy, where concepts are selected using a fixed order starting with those concepts that, on average, lead to the biggest improvements in validation accuracy when intervened on, and finally (6) an oracle _Skyline_ policy, which selects \(c_{*}\) on every step, indicating an upper bound for all greedy policies. For details on each policy baseline, including how their hyperparameters are selected, see Appendix A.7.

We demonstrate in Figure 4 that IntCEM consistently outperforms CEMs regardless of the test-time intervention policy used, with CooP and UCP consistently yielding superior results. Further, we uncover that intervening on IntCEM with a Random policy at test-time _outperforms the theoretical best performance attainable by a CEM_ (_Skyline_) under many interventions, while its best-performing policy outperforms CEM's Skyline after very few interventions. These results suggest that IntCEM's train-time conditioning not only results in IntCEMs being more receptive to test-time interventions

Figure 3: Task accuracy of all baseline models after receiving a varying number of _randomly selected_ interventions. For our binary MNIST-based tasks, we show task AUC rather than accuracy. Here and elsewhere, we show the means and standard deviations (can be insignificant) over five random seeds.

but may lead to significant changes in an IntCEM's theoretical-optimal intervention performance. We include analyses on our remaining datasets, revealing similar trends, in Appendix A.7.

### Studying IntCEM's Intervention Policy (Q3)

Finally, we explore IntCEM's intervention policy by evaluating test-time interventions predicted by \(\). To understand whether there is a benefit of learning \(\) end-to-end (i.e., in conjunction with the rest of model training), rather than learning it post-hoc _after_ training an IntCEM, we compare it against a Behavioural Cloning (BC)  policy "_BC-Skyline_" trained on demonstrations of Skyline applied to a trained IntCEM. Furthermore, to explore how \(\)'s trajectories at _train-time_ help improve IntCEM's receptiveness to test-time interventions, we study an IntCEM trained by sampling trajectories uniformly at random ("IntCEM no \(\)"). Our results in Figure 5 suggest that: (1) IntCEM's learnt policy \(\) leads to test-time interventions that are as good or better than CooP's, yet avoid CooP's computational cost (_up to \(\)2x faster_ as seen in Appendix A.5), (2) learning \(\) end-to-end during training yields a better policy than one learnt through BC _after_ the IntCEM is trained, as seen in the BC policy's lower performance; and (3) using a learnable intervention policy at train-time results in a significant boost of test-time performance. This suggests that part of IntCEM's receptiveness to interventions, even when using a random test-time policy, can be partially attributed to learning and using \(\) during training. We show some qualitative examples of rollouts of \(\) in the CUB dataset in Figure 6.

## 5 Discussion and Conclusion

Concept Leakage Within IntCEMsPrevious work [36; 6; 10] has shown that CBMs are prone to encoding unnecessary information in their learnt concept representations. This phenomenon, called _concept leakage_, may lead to less interpretable concept representations  and detrimental concept interventions in CBMs . Given that interventions in IntCEMs involve swapping a concept's predicted embeddings rather than overwriting an activation in the bottleneck, such interventions are distinct from those in CBMs as they enable leaked information to be exploited _after_ an intervention

Figure 4: Task accuracy of IntCEMs and CEMs on CUB and CelebA when intervening with different test-time policies. We show similar improvements of IntCEMs over CBMs in Appendix A.7.

Figure 5: Task performance when intervening on IntCEMs following test-time policies \(\), _CooP_, _Random_, and _BC-Skyline_. Our baseline “IntCEM no \(\)” is an IntCEM whose test _and_ train interventions are sampled from a Random policy rather than from \(\) (i.e., a policy is not learnt in this baseline).

is performed. This means our loss function may incentivise IntCEM to exploit this mechanism to improve a model's receptiveness to interventions. In Appendix A.10 we explore this hypothesis and find that we can detect more leakage in IntCEM's concept representations than in those learnt by CBMs and CEMs. This suggests that, contrary to common assumptions, leakage may be a healthy byproduct of more expressive concept representations in models that accommodate such expressivity. Nevertheless, we believe further work is needed to understand the consequences of this leakage.

Limitations and Future WorkTo the best of our knowledge, we are the first to frame learning to receive concept interventions as a joint optimisation problem where we simultaneously learn concepts, downstream labels, and policies. IntCEMs offer a principled way to prepare concept-based models for interventions at test-time, without sacrificing performance in the absence of interventions. Here, we focused on CEMs as our base model. However, our method can be extended to traditional CBMs (see Appendix A.8), and future work may consider extending it to more recent concept-based models such as post-hoc CBMs , label-free CBMs , and probabilistic CBMs .

Furthermore, we note some limitations. First, IntCEM requires hyperparameter tuning for \(_{}\) and \(_{}\). While our ablations in Appendix A.6 suggest that an IntCEM's performance gains extend across multiple values of \(_{}\), users still need to tune such parameters for maximising IntCEM's utility. These challenges are exacerbated by the computational costs of training IntCEMs due to their train-time trajectory sampling (see Appendix A.5). However, such overhead gets amortised over time given \(\)'s efficiency over competing policies. Further, we recognise our training procedure renders IntCEMs more receptive to _any_ form of intervention, including adversarial interactions (see Appendix A.9). This raises a potential societal concern when deciding how our proposed model is deployed. Future work may explore such issues by incorporating error-correcting mechanisms or by considering intervention-time human uncertainty .

Finally, our evaluation was limited to two real-world datasets and one synthetic dataset, all of which have medium-to-small training set sizes. Therefore, future work may explore how to apply our proposed architecture to larger real-world datasets and may explore how to best deploy IntCEM's policy in practice via large user studies.

ConclusionA core feature of concept-based models is that they permit experts to interpret predictions in terms of high-level concepts and _intervene_ on mispredicted concepts hoping to improve task performance. Counter-intuitively, such models are rarely trained with interventions in mind. In this work, we introduce a novel concept-interpretable architecture and training paradigm - Intervention-Aware Concept Embedding Models (IntCEMs) - designed explicitly for intervention receptiveness. IntCEMs simulate interventions during train-time, continually preparing the model for interventions they may receive when deployed, while learning an efficient intervention policy. Given the cost of querying experts for help, our work addresses the lack of a mechanism to leverage help when received, and demonstrates the value of studying models which, by design, know how to utilise help.

Figure 6: Examples of interventions on an IntCEM following its policy \(\) in CUB. We show the task label distribution \(p(|})\) for the most likely classes after each intervention. We highlight the correct label’s probability using orange bars and show the selected concept by \(\) above each panel.