# Robust Mixture Learning when

Outliers Overwhelm Small Groups

Daniil Dmitriev\({}^{1*}\)  Raes-Darius Buhai\({}^{1*}\)  Stefan Tiegel\({}^{1}\)  Alexander Wolters\({}^{2}\)

Gleb Novikov\({}^{3}\)  Amartya Sanyal\({}^{4}\)  David Steurer\({}^{1}\)  Fanny Yang\({}^{1}\)

\({}^{1}\)ETH Zurich \({}^{2}\)TU Munich

\({}^{3}\)Lucerne School of Computer Science and Information Technology

\({}^{4}\)University of Copenhagen

\({}^{*}\)_Equal contribution_

###### Abstract

We study the problem of estimating the means of well-separated mixtures when an adversary may add arbitrary outliers. While strong guarantees are available when the outlier fraction is significantly smaller than the minimum mixing weight, much less is known when outliers may crowd out low-weight clusters - a setting we refer to as list-decodable mixture learning (LD-ML). In this case, adversarial outliers can simulate additional spurious mixture components. Hence, if all means of the mixture must be recovered up to a small error in the output list, the list size needs to be larger than the number of (true) components. We propose an algorithm that obtains order-optimal error guarantees for each mixture mean with a minimal list-size overhead, significantly improving upon list-decodable mean estimation, the only existing method that is applicable for LD-ML. Although improvements are observed even when the mixture is non-separated, our algorithm achieves particularly strong guarantees when the mixture is separated: it can leverage the mixture structure to partially cluster the samples before carefully iterating a base learner for list-decodable mean estimation at different scales.

## 1 Introduction

Estimating the mean of a distribution from empirical data is one of the most fundamental problems in statistics. The mean often serves as the primary summary statistic of the dataset or is the ultimate quantity of interest that is often not precisely measurable. In practical applications, data frequently originates from a mixture of multiple groups (also called subpopulations) and a natural goal is to estimate the distinct means of each group separately. For example, we might like to use representative individuals to study how a complex decision or procedure would impact different subpopulations. In other applications, such as genetics  or astronomy  research, finding the means themselves can be a crucial first step towards scientific discovery. In both scenarios, the algorithm should output a list of estimates that are close to the unobservable true means.

However, in practice, the data may also contain outliers, for example due to measurement errors or abnormal events. We would like to find good mean estimates for all inlier groups even when the proportion of such _additive adversarial contaminations_ is larger than some smaller groups that we want to properly represent. The central open question that motivates our work is thus:

_What is the cost of efficiently recovering small groups that may be outnumbered by outliers?_

More specifically, consider a scenario where the practitioner would like to recover the means of small but significant enough inlier groups which constitute at least \(w_{}(0,1)\) proportion of the(corrupted) data. If \(k\) is the number of such inlier groups, for all \(i[k]\), we then denote by \(w_{i} w_{}\) the unknown weight of the \(i-\)th group with mean \(_{i}\). Further, we use \(\) to refer to the proportion of additive contamination - the data that comes from an unknown adversarial distribution. The goal is to estimate the unknown means \(_{i}\) for all \(i[k]\).

Existing works on robust mixture learning such as  consider the problem when the fraction of additive adversarial outliers is smaller than the weight of the smallest subgroup, i.e. \(<w_{}\). However, for large outlier proportions where \( w_{}\), these algorithms are not guaranteed to recover small clusters with \(w_{i}\). In this case, outliers can form additional spurious clusters that are indistinguishable from small inlier groups. As a consequence, generating a list of size equal to the number of components would possibly lead to neglecting the means of small groups. In order to ensure that the output contains a precise estimate for each of the small group means, it is thus necessary the estimation algorithm to provide a list whose size is strictly larger than the number of components. We call this paradigm _list-decodable mixture learning_ (LD-ML), following the footsteps of a long line of work on list-decodable learning (see Sections 2 and 5).

Specifically, the main challenge in LD-ML is to provide a _short_ list that contains good mean estimates for all inlier groups. We first note that there is a minimum list size the algorithm necessarily has to output to guarantee that all groups are recovered. For example, consider an outlier distribution that includes several copies of the smallest inlier group distribution with means spread out throughout the domain. Since inlier groups are indisntinguishable from spurious outlier ones, the shortest list that includes means of all inlier groups must be of size at least \(|L| k+w_{i}}\). Here, \(w_{i}}\) can be interpreted as the minimal list-size overhead that is necessary due to "caring" about groups with weight smaller than \(\). The key question is hence how good the error guarantees of an LD-ML algorithm can be when the list size overhead stays close to \(w_{i}}\), while being agnostic to \(w_{i}\) aside from the knowledge of \(w_{}\). Furthermore, we are interested in _computationally efficient_ algorithms for LD-ML, especially when dealing with high-dimensional data.

To the best of our knowledge, the only existing efficient algorithms that are guaranteed to recover inlier groups with weights \(w_{i}\) are _list-decodable mean estimation_ (LD-ME) algorithms. LD-ME algorithms model the data as a mixture of one inlier and outlier distribution with weights \( 1/2\) and \(1-\) respectively. Provided with the weight parameter \(\), they output a list that contains an estimate close to the inlier mean with high probability. However, for the LD-ML setting, the inlier weights \(w_{i}\) are not known and we would have to use LD-ME algorithms with \(w_{}\) as weight estimates for each group. This leads to suboptimal error in particular for large groups, that hence (somewhat counter intuitively) would have to "pay" for the explicit constraint to recover small groups. Furthermore, even if LD-ME were provided with \(w_{i}\), by design it would treat inlier points from other components also as outliers, unnecessarily inflating the fraction of outliers to \(1-w_{i}\) instead of \(\).

ContributionsIn this paper, we propose an algorithm that (i) correctly estimates the weight of each component only given a lower bound and (ii) does not overestimate proportion of outliers when components are well-separated. In particular, we construct a meta-algorithm that uses mean estimation algorithms as base learners that are designed to deal with adversarial corruptions. This meta-algorithm inherits guarantees from the base learner and any improvement of the latter translates to better results for LD-ML. For example, if the base learner runs in polynomial time, so does our meta-algorithm. Our approach of using the output of weak base learners to achieve better performance is reminiscent of the _boosting_ paradigm that is common in machine learning practice.

Our algorithm achieves significant improvements in error and list-size guarantees for multiple settings. For ease of comparison, we summarize error improvements for inlier Gaussian mixtures in Table 1 (see Theorem 3.3 for the general result regarding distributions with bounded moments). The main focus of our contributions is represented in the second row; that is the setting where outliers outnumber some inlier groups with weight \(w_{j}\) and the inlier components are _well-separated_, i.e., \(\|_{i}-_{j}\|^{1}}}}\), where \(_{i}\)'s are the inlier component means. As we mentioned before, robust mixture learning algorithms, such as , are not applicable here and the best error guarantees in prior work is achieved by an LD-ME algorithm, e.g. from . While its error bounds are of order \(O(}}})\) for a list size of \(O(}})\), our approach guarantees error \(O(}})\) for a list size of \(k+O(}})\). Remarkably, we obtain the same error guarantees as if an oracle would run LD-ME on each inlier group _with the correct weight_\(w_{i}\) separately (with outliers). Hence, the only cost for recovering small groups is the increased list-size overhead of order \(O(}})\). Further, a sub-routine in our meta-algorithm also obtains novel guarantees under _no_ separation assumption, as shown in the third row of Table 1. This algorithm achieves the same error guarantees for similar list size as a base learner that knows the correct weights of the inlier components.

Based on a reduction argument from LD-ME to LD-ML, we also provide information-theoretic (IT) lower bounds for LD-ML. If the LD-ME base learners achieve the IT lower bound (possible for inlier Gaussian mixtures), so does our LD-ML algorithm. In synthetic experiments, we implement our meta-algorithm with the LD-ME base learner from  and show clear improvements compared to the only prior method with guarantees, while being comparable or better than popular clustering methods such as k-means and DBSCAN for various attack models.

## 2 Settings

We now introduce the learning settings that appear in the paper. Let \(d_{+}\) be the ambient dimension of the data and \(k_{+}\) be the number of mixture components (inlier groups/clusters).

### List-decodable mixture learning under adversarial corruptions

We focus on mixtures that consist of distributions that are sufficiently bounded in the following sense.

**Definition 2.1**.: Let \(t_{+}\) be even and let \(D()\) be a distribution on \(^{d}\) with mean \(\). We say that \(D()\) has _sub-Gaussian \(t\)-th central moments_ if for all even \(s t\) and for every \(v^{d}\) with \(\|v\|=1\), \(_{x D} x-,v^{s}(s-1)!!\).

This class of distributions is closely related to commonly studied distributions in the literature (see, e.g., ) with bounded \(t\)-th moment. Our requirement for the boundedness of all moments \(s t\) stems from the fact that our algorithm should adapt to unknown and possibly non-uniform mixture weights.

We assume that we are given samples from a corrupted \(d\)-dimensional mixture of \(k\) inlier distributions \(D_{i}(_{i})\) satisfying Definition 2.1, where the mixture is defined as

\[=_{i=1}^{k}w_{i}D_{i}(_{i})+ Q,\] (2.1)

and \(_{i=1}^{k}w_{i}+=1\), where for all \(i=1,,k\), it holds that \(w_{i} w_{}\). Further, an \(>0\) proportion of the data comes from an _outlier_ distribution \(Q\) chosen by the adversary with full knowledge of our algorithm and inlier mixture. Samples drawn from \(D_{i}(_{i})\) constitute the \(i^{}\)_inlier cluster_. The goal in mixture learning under corruptions as in Eq. (2.1), is to design an algorithm that takes in i.i.d. samples from \(\) and outputs a list \(L\), such that for each \(i[k]\), there exists \( L\) with small estimation error \(\|_{i}-\|\).

To the best of our knowledge, we are the first to study the _list-decodable mixture learning_ problem (LD-ML) that considers the case of large fractions of outliers \(_{i}w_{i}\) and the goal is to achieve

 
**Type of inlier mixture** & **Best prior work** & **Ours** & **Inf.-theor. lower bound** \\  Large (\( j:\, w_{j}\)), sep. groups & \((/w_{i})\) & \((/w_{i})\) & \((/w_{i})\), see  \\ Small (\( j:\, w_{j}\)), sep. groups & \(O(}}})\) & \(O(}{w_{i}}})\) & \((}{w_{i}}})\), Prop. 3.5 \\ Non-separated groups & \(O(}}})\) & \(O(}})\) & \((}})\), see  \\  

Table 1: For a mixture of Gaussian components \((_{i},I_{d})\), we show upper and lower bounds for the **error of the \(i\)-component** given a output list \(L\) (of the respective algorithm) \(_{i L}\|-_{i}\|\). When the error doesn’t depend on \(i\), all means have the same error guarantee irrespective of their weight. Note that depending on the type of inlier mixture, different methods in  are used as the ’best prior work’: robust mixture learning for the first row and list-decodable mean estimation for the rest.

small estimation errors while the list size \(|L|\) remains small. While in robust estimation problems, the fractions of inliers and outliers are usually provided to the algorithm, in mixture learning, the mixture proportions are explicit quantities of interest. Throughout the paper, we hence assume that _both_ the true weights \(w_{i}\) of the mixture and the fraction of outliers \(\) are _unknown_. Instead, by definition in Eq. (2.1), we assume knowledge of a valid lower bound \(w_{}_{i}w_{i}\).

Note that when \(_{i}w_{i}\), the problem is known as robust mixture learning and can be solved with list size \(|L|=k\) as discussed in [3; 4; 7]. However, algorithms for robust mixture learning fail when the fraction of outliers becomes comparable to the inlier group size. In the presence of "spurious" adversarial clusters, it is information-theoretically impossible to output a list \(L\), such that (i) \(|L|=k\) and (ii) \(L\) contains precise estimate for each true mean.

### Mean estimation under adversarial corruptions

In order to solve LD-ML, we use mean estimation procedures that have provable guarantees under adversarial contamination. Mean estimation can be viewed as a particular case of the mixture learning problem in Eq. (2.1) with \(k=1\), the fraction of inliers \(=w_{1}\) and the fraction of outliers \(=1-\). The mean estimation algorithms we use to solve LD-ML with \(w_{}\) need to exhibit guarantees under a stronger adversarial model, where the adversary can also replace a small fraction (depending on \(w_{}\)) of the inlier points; see details in Definition B.1. This is a special case of the general contamination model as opposed to the slightly more benign additive contamination model in Eq. (2.1). For different regimes of \(\) we use black-box learners that solve corresponding regime when _provided with \(\)_.

Robust mean estimationWhen the majority of points are inliers, we are in the \(\) setting. Robust statistics has studied this setting with different corruption models and efficient algorithms are known to achieve information-theoretically optimal error guarantees (see Section 5).

List-decodable mean estimationWhen inliers form a minority, we are in the list-decodable setting and are required to return a list instead of a single estimate. We refer to this setting as \(\) (_corrupted known list-decoding_). For mixture learning, \(\) is usually unknown and we need to solve the \(\) (_corrupted agnostic list-decoding_) problem (i.e., \(\) is _not provided_, but instead a lower bound \(_{}[w_{},]\) is given to the algorithm). Finally, when only additive adversarial contamination is present, as in Eq. (2.1), we recover the standard list-decoding setting studied in prior works (see Section 5) that we call \(\) (_simple list-decoding_). In Appendix G we show that two algorithms designed for \(\) also exhibit guarantees for \(\) for any \(w_{}\).

## 3 Main results

We now present our main results for list-decodable and robust mixture learning defined in Section 2. In Section 3.1, we provide algorithmic upper bounds and information-theoretic lower bounds. For the special case of spherical Gaussian mixtures, we show in Section 3.1 that we achieve optimality. Our results are constructive as we provide a meta-algorithm for which these bounds hold.

As depicted in Figure 1, our meta-algorithm (Algorithm 2) is a two-stage process. The outer stage (Algorithm 6) reduces the problem to mean estimation by leveraging the mixture structure and splitting the data into a small collection \(\) of sets \(T\). Each set \(T\) should (i) contain at most one inlier cluster (and few samples from other clusters) and (ii) the total number of outliers across all sets should be at most \(O( n)\). We then run the inner stage (Algorithm 3) on sets \(T\), which outputs a mean estimate for the inlier cluster in \(T\). First, a \(\) algorithm identifies the weight of the inlier cluster and returns the result of a \(\) base learner with this weight. Then, if the weight is large, we improve the error via an \(\) base learner. A careful filtering procedure in both stages achieves the significantly reduced list size and better error guarantees. We require the base learners to satisfy the following set of assumptions.

**Assumption 3.1** (Mean-estimation base learners for mixture learning).: Let \(t\) be an even integer and consider the corruption setting defined in Definition B.1. Further, let the inlier distribution \(D(^{*})\) where \(\) is the the family of distributions satisfying Definition 2.1 for \(t\). We assume that 1. for \([w_{},1/3]\) in the \(\)-\(\) regime, there exists an algorithm \(_{}\) that uses \(N_{LD}()\) samples and \(T_{LD}()\) time to output a list of size bounded by \(1/^{O(1)}\) that with probability at least \(1/2\) contains some \(\) with \(\|-^{*}\| f()\), where \(f\) is non-increasing.
2. for \([1-_{},1]\), with \(0_{} 1/2-2w_{}^{2}\) in the \(\) regime, there exists an \(\) algorithm \(_{R}\) that uses \(N_{R}()\) samples and \(T_{R}()\) time to output with probability at least \(1/2\) some \(\) with \(\|-^{*}\| g()\), where \(g\) is non-increasing.

Note that the sample and time-complexity functions such as \(N_{LD}\) and \(T_{LD}\), might depend on \(t\), for example growing as \(d^{t}\). We emphasize that (i) the guarantees of our meta-algorithm depend on the guarantees of the base learners and (ii) we only require the base learners to work in the well-studied setting with _known_ fraction of inliers. Corollary 3.4 uses known base learners for Gaussian distributions achieving information-theoretically optimal error bounds. There also exists base learners for distributions beyond Gaussians, such as bounded covariance or log-concave distributions, see, e.g. .

### Upper bounds for list-decodable mixture learning

Key quantities that appear in our error bounds are the relative proportion of inliers \(_{i}\) and outliers \(_{i}\):

\[_{i}=}{w_{i}++w_{}^{2}}_{i}=1-_{i}.\] (3.1)

These quantities reflect that each set \(T\) in the inner stage contains at most one inlier cluster and a small (\( w_{}^{2}\)) fraction of points from other inlier clusters. We now present a simplified version of our main result in Theorem 3.3 (see Theorem C.1 for the detailed result) that allows for a more streamlined presentation of the results using the following 'well-behavedness' of \(f\) and \(g\).

**Assumption 3.2**.: Let \(f\), \(g\) be as defined in Assumption 3.1. For some \(C>0\), we assume (i) \(_{} 0.01\), (ii) \( x(0,1/3]\), \(f(x/2) Cf(x)\), and (iii) \( x[0.99,1]\), \(g(x-(1-x)^{2}) Cg(x)\).

We are now ready to state the main result of the paper.

**Theorem 3.3**.: _Let \(d,k_{+}\), \(w_{}(0,1/2]\), and \(t\) be an even integer. Let \(\) be a \(d\)-dimensional mixture distribution following Eq. (2.1). Let \(_{}\) and \(_{R}\) satisfy Assumptions 3.1 and 3.2 for some even \(t\). Further, suppose that \(\|_{i}-_{j}\|(1/w_{})^{4/t}+f(w_{})\) for all \(i j[k]\)._

_Then there exists an algorithm that, given \((d,1/w_{})(N_{LD}(w_{})+N_{R}(w_{ }))\) i.i.d. samples from \(\) as well as \(d\), \(k\), \(w_{}\), and \(t\), runs in time \((d,1/w_{})(T_{LD}(w_{})+T_{R}(w_{ }))\) and with probability at least \(1-w_{}^{O(1)}\) outputs a list \(L\) of size \(|L| k+O(/w_{})\) where, for each \(i[k]\), there exists \( L\) such that_

\[\|-_{i}\|=O(_{1^{} t}}(1/_{i})^{1/t^{}}+f((_{i},1/3))).\]

_If the relative weight of the \(i\)-th cluster is large, i.e., \(_{i} 0.001\), then the error is further bounded by_

\[\|-_{i}\|=O(g(_{i})).\]

The proof together with a more general statement, Theorem C.1, can be found in Appendix C.

Note that for a mixture setting with \(k 2\), the assumption \(w_{} 1/k 1/2\) is automatically fulfilled. Also, for large weights \(_{i}\) such that \((1/_{i}) t\), the \(t^{}\) that minimizes \(}(1/_{i})^{1/t^{}}\) is smaller than \(t\), and for small weights the minimizer is \(t^{}=t\).

Figure 1: Schematic of the meta-algorithm (Algorithm 2) underlying Theorem 3.3

Gaussian caseFor Gaussian inlier distributions, LD-ME and RME base learners with guarantees for Assumption 3.1 have already been developed in prior work. We can thus readily use them in the meta-algorithm to arrive at the following statement with the relative proportions defined in Eq. (3.1).

**Corollary 3.4** (Gaussian case).: _Let \(d,k,w_{}\) and \(t\) be as in Theorem 3.3. Let \(\) be as in Eq. (2.1) with \(D_{i}(_{i})=(_{i},I_{d})\) with \(_{i}\)'s satisfying \(\|_{i}-_{j}\|}}\) for all \(i j[k]\). There exists an algorithm that for \(t=O( 1/w_{})\), given \(N=(d^{t},(1/w_{})^{t})\) i.i.d. samples from \(\) and \(w_{}\), runs in \((N)\) time and outputs a list \(L\) such that with high probability \(|L|=k+O(/w_{})\) and, for all \(i[k]\), there exists \( L\) such that_

\[\|-_{i}\|=O(_{i}})\,.\]

_If the relative weight of the \(i\)-th cluster is large, i.e. \(_{i} 0.001\), then the error is further bounded by_

\[\|-_{i}\|=O(_{i}_{i}}).\]

Proof.: Theorem 6.12 from  provides an LD-ME algorithm \(_{}\) achieving error \(f() O(^{}}(1/)^{1/t^{}})\) for all \(t^{} t\). The sample and time complexity scale as \((d^{t},(1/)^{t})\). Also, Theorem 5.1 from  provides a robust mean estimation algorithm \(_{R}\) such that for a small enough constant fraction of outliers \(=1-\) it achieves error \(g()=O((1-))\) with sample complexity \((d/^{2})\). Using these \(_{}\) and \(_{R}\), we recover the desired bounds. 

Comparison with prior workWe now compare our result with the only previous method that can achieve guarantees in the LD-ML setting with unknown \(w_{i}\). As discussed in , algorithms for the simple list-decoding model with \(=w_{}\) can be used for LD-ML by viewing a single mixture component as the "ground truth" distribution and effectively treating all other inlier components and original outliers as outliers. Besides requiring a much larger list size of \(O(1/w_{}) k+O(/w_{})\) and error \(O(}})\), this approach has two drawbacks that manifest in the suboptimal guarantees: 1) For larger clusters \(i\) with \(w_{i} w_{}\), LD-ME only achieves an error \(O(}})\).

Our result, even without separation assumption, achieves a sharper error bound \(O(})\). 2) When the mixture is separated, LD-ME cannot exploit the structure since it still models the data as \(w_{}(_{i},I_{d})+(1-w_{})Q\) for each \(i\), so that the algorithm inevitably treats all other true components as outliers. This results in the error \(O(}}) O(_{i}} )=O(1)\) (when \( w_{i} 1\)). We refer to Appendix A for further illustrative examples. As a simple example, consider the uniform inlier mixture with \(=w_{i}=1/(k+1)\), where \(k\) is large. In this case, previous results have error guarantees \(O()\), while we obtain error \(O(1)\).

Separation assumptionFor the problem of learning mixture models, a separation assumption is common in the literature . We require separation \(\|_{i}-_{j}\|(1/w_{})^{4/t}\), which we believe to be sub-optimal for the case of finite \(t\). In cases when \(\) is small (namely \( w_{}\)), there exist prior works on clustering allowing smaller separation. Specifically, when \(t=2\), a recent work  only requires \(\|_{i}-_{j}\| 1/}}\). For a general \(t 4\),  succeeds under separation \((1/w_{})^{2/t}\). We leave the possible relaxation of the separation requirement in the case of general \(t\) and large \(\) for the future work.

In the Gaussian case we require separation \(\|_{i}-_{j}\|}}\), which is optimal in the uniform (\(w_{i}=1/k\)) case. Indeed, without the separation assumption, even in the _noiseless_ uniform Gaussian case,  shows that no efficient algorithm can obtain error asymptotically better than \((})\). In Corollary B.5, we prove that the inner stage (Algorithm 3) of our algorithm, without knowledge of \(w_{i}\) and separation assumption, achieves with high probability matching error guarantees \(O(})\) with list size bounded by \(O(1/w_{})\).

### Information-theoretical lower bounds and optimality

Next, we present information-theoretical lower bounds for list-decodable mixture learning on well-separated distributions \(\) as defined in Eq. (2.1). We show that our error is optimal as long as the list size is required to be small. Our proof uses a simple reduction technique and leverages established lower bounds in  for the list-decodable mean estimation model (\(\) in Section 2).

**Proposition 3.5** (Information-theoretic lower bounds).: _Let \(\) be an algorithm that, given access to \(\), outputs a list \(L\) that, with probability \( 1/2\), for each \(i[k]\) contains \( L\) with \(\|-_{i}\|_{i}\)._

1. _Consider the case with_ \(\|_{i}-_{j}\|(1/w_{})^{4/t}\) _for_ \(i j[k]\)_,_ \(D_{i}(_{i})\) _having_ \(t\)_-th bounded sub-Gaussian central moments and_ \(_{i} C(1/w_{})^{1/t}\) _for each_ \(i[k]\)_. If for some_ \(s[k]\) _it holds that_ \(w_{s}\)_, then algorithm_ \(\) _must either have error bound_ \(_{s}=((1/_{i})^{1/t})\) _or_ \(|L| k+d-1\)_._
2. _Consider the case with_ \(\|_{i}-_{j}\|}}\) _for_ \(i j[k]\)_,_ \(D_{i}(_{i})=(_{i},I_{d})\) _and_ \(_{i} C}}\) _for each_ \(i[k]\)_. If for some_ \(s[k]\) _it holds that_ \(w_{s}\)_, then algorithm_ \(\) _must either have error bound_ \(_{s}=(_{i}})\) _or_ \(|L| k+\{2^{(d)},(1/_{i})^{(1)}\}\)_._

In the Gaussian inlier case, Corollary 3.4 together with Proposition 3.5 imply optimality of our meta-algorithm. Indeed, if one plugs in optimal base learners (as in the proof of Corollary 3.4), we obtain error guarantee that matches lower bound. In particular, "exponentially" larger list size is necessary for asymptotically smaller error. For inlier components with bounded sub-Gaussian moments,  obtains information-theoretically (nearly-)optimal LD-ME base learners.

Furthermore, in , formal evidence of computational hardness was obtained (see their Theorem 5.7, which gives a lower bound in the statistical query model introduced by ) that suggests obtaining error \(_{t}((1/_{s})^{1/t})\) requires running time at least \(d^{(t)}\). This was proved for Gaussian inliers and the running time matches ours up to a constant in the exponent.

## 4 Algorithm sketch

We now sketch our meta-algorithm specialized to the case of separated Gaussian components \((_{i},I_{d})\) and provide intuition for how it achieves the guarantees in Corollary 3.4. In this section, we only discuss how to obtain an error of \(O(_{i}})\) for each mean when \(_{i}w_{i}\). We refer to Appendix D for how to achieve the refined error guarantee of \(O(_{i}_{i}})\) when \(_{i}\) is small.

As discussed in Section 3.1, running an out-of-the-box LD-ME algorithm for the sLD problem on our input with parameter \(=w_{}\) would give sub-optimal guarantees. In contrast, our two-stage Algorithm 2, equipped with the appropriate \(\)-\(\) and \(\) base learners as depicted in Figure 1, obtains for each component an error guarantee that is as good as if we had access to the samples _only_ from this component and from the outliers. We now give more details about the outer stage, Algorithm 1, and inner stage, Algorithm 3, and describe on a high-level how they contribute to a short output list with optimal error bound in Corollary 3.4 for large outlier fractions.

### Inner stage: list-decodable mean estimation with unknown inlier fraction

We now describe how to use a black-box \(\)-\(\) algorithm to obtain a list-decoding algorithm \(_{}\) for the \(\)-\(\) mean-estimation setting with access only to \(_{}\). \(_{}\) is usedin the proof of Corollary B.5 and plays a crucial role (see Figure 1) in our meta-algorithm. In particular, it deals with the unknown weight of the inlier distribution in each set returned by the outer stage. Note that estimating \(\) from the input samples is impossible by nature. Indeed, we cannot distinguish between potential outlier clusters of arbitrary proportion \( 1-\) and the inlier component. Undererestimating the size of a large component would inevitably lead to a suboptimal error guarantee. We now show how to overcome this challenge and achieve an error guarantee \(O()\) for a list size \(1+O((1-)/_{})\) for the \(\)-aLD setting. Here we only outline our algorithm and refer to Appendix D for the details.

Algorithm 3 first produces a large list of estimates corresponding to many potential values of \(\) and then prunes it while maintaining a good estimate in the list. In particular, for each \( A\{_{},2_{}, , 1/(3_{})_{}\}\), we run \(_{}\) with parameter \(\) to obtain a list of means. We append \(\) to each mean in the list and obtain a list of pairs \((,)\). We concatenate these lists of pairs for all \(\) and obtain a list \(L\) of size \(O(1/_{}^{2})\). By design, one element of \(A\) is close to the true \(\), so the list \(L\) contains at least one \(\) that is \(O()\)-close -- the error guarantee that we aim for -- and there is indeed at least an \(\)-fraction of samples near \(\). We call such a hypothesis "nearby".

Finally, we prune this concatenated list by verifying for each \(\) whether there is indeed an \(\)-fraction of samples "not too far" from it. This is similar to pruning procedures with known \(\) proposed in prior work (see Proposition B.1 in ). Our procedure (i) never discards a "nearby" hypothesis, and outputs a list where (ii) every hypothesis contains a sufficient number of points close to it and (iii) all hypotheses are separated. Property (i) implies that the final error is \(O()\) and properties (ii) and (iii) imply list size bound \(1+O((1-)/_{})\). Note that when \(<_{}\), the list size can be simply upper bounded by \(O(1/_{})\), see Remark B.4.

### Two-stage meta-algorithm

Even though we could run \(_{}\) on the entire dataset with \(_{}=w_{}\), we would only achieve an error for the \(i^{}\) inlier cluster mean of \(O(})\) - which can be much larger than \(O(_{i}})\) - for a list of size \(O(1/w_{})\). While \(_{}\) takes into account the unknown weight of the clusters, it still treats other inlier clusters as outliers. We now show that if the outer stage Algorithm 1 of our meta-algorithm Algorithm 2 separates the samples into a not-too-large collection \(\) of sets with certain properties, running \(_{}\) separately on each of the sets can lead to the desired guarantees. In particular, let us assume that \(\) consists of potentially overlapping sets such that:

1. For each inlier cluster \(C^{*}\), there exists one set \(T\) such that \(T\) contains (almost) all points from \(C^{*}\) and at most \(O( n)\) other points,
2. It holds that \(_{T}|T| n+O( n)\).

By (1), for every inlier cluster \(C^{*}\) with a corresponding true weight \(w^{*}\), there exists a set \(T\) such that the points from \(C^{*}\) constitute at least an \(\)-fraction of \(T\) with \(:=(w^{*}/(w^{*}+))\). By Section 4.1, applying \(_{}\) with \(_{}=w_{} n/|T|\) on such a \(T\) then yields a list of size \(1+O((1-)/w_{})\) with an estimation error at most \(O(})\). If \(T\) contains (almost) no inliers, that is, there is no inlier component that should recovered, then \(_{}\) returns a list of size \(O(|T|/(w_{}n))\).

Now, by the two properties, (almost) all inlier points lie in at most \(k\) sets of \(\), and all other sets of \(\) contain in total at most \(O( n)\) points. Hence, concatenating all lists outputted by \(_{}\) applied to all \(T\) leads to a final list size bounded by \(k+O(/w_{})\).

### Outer stage: separating inlier clusters

We now informally describe the outer stage that produces the collection of sets \(\) with the desiderata described in Section 4.2, leaving the details to Appendix E. The main steps are outlined in pseudocode in Algorithm 1.

Given a set \(X\) of \(N=(d^{t},1/w_{})\) i.i.d. input samples from the distribution Eq. (2.1) with Gaussian inlier components, the first step of the meta-algorithm is to run Algorithm 1 on \(X\) and \(w_{}\) with \(=O(}})\). Algorithm 1 runs an sLD algorithm on the samples and produces a (large) list of estimates \(L\) such that, for each mean, at least one estimate is \(O(}})\)-close to it. It then add sets to \(\) that correspond to these estimates via a dynamic "two-scale" process.

Specifically, for each \( L\), we construct _two sets_\(S^{(1)}_{} S^{(2)}_{}\) consisting of samples close to \(\). By construction, we guarantee that if \(S^{(1)}_{}\) contains a non-negligible fraction of samples from any inlier cluster \(C^{*}\), then \(S^{(2)}_{}\) contains (almost) all samples from \(C^{*}\) (see Theorem B.7 (ii)).

Now we very briefly illustrate how this process could be helpful in proving properties (1) and (2). Observe that, as long as there exists some \(\) with \(|S^{(2)}_{}| 2|S^{(1)}_{}|\), we add \(S^{(2)}_{}\) to \(\) and remove the samples from \(S^{(1)}_{}\). Consider one such \(\). For property (1), we merely note that if \(S^{(1)}_{}\) contains a part of an inlier cluster \(C^{*}\), then \(S^{(2)}_{}\) contains (almost) all of \(C^{*}\), so we add to \(\) a set that contains (almost) all of \(C^{*}\); otherwise, when we remove \(S^{(1)}_{}\) we remove (almost) no points from \(C^{*}\), so (almost) all the points from \(C^{*}\) remain in play. For property (2), we merely note that whenever we add \(S^{(2)}_{}\) to \(\), increasing the number of points in it by \(|S^{(2)}_{}|\), we also remove the samples from \(S^{(1)}_{}\), reducing the number of samples by \(|S^{(1)}_{}||S^{(2)}_{}|/2\). The proof of the properties uses some additional arguments of a similar flavor, and we defer it to Appendix E.

## 5 Related work

List-decodable mean estimationInspired by the list-decoding paradigm that was first introduced for error-correcting codes for large error rates , list-decodable mean estimation has become a popular approach for robustly learning the mean of a distribution when the majority of the samples are outliers. A long line of work has proposed efficient algorithms with theoretical guarantees. These algorithms are either based on convex optimization [9; 16], a filtering approach [3; 17], or low-dimensional projections . Near-linear time algorithms were obtained in  and . The list-decoding paradigm is not only used for mean estimation but also other statistical inference problems. Examples include sparse mean estimation [20; 21], linear regression [22; 23; 24], subspace recovery [25; 26], clustering , stochastic block models and crowd sourcing [16; 28].

Robust mean estimation and mixture learningWhen the outliers constitute a minority, algorithms typically achieve significantly better error guarantees than in the list-decodable setting. Robust mean estimation algorithms output a single vector close to the mean of the inliers. In a variety of corruption models, efficient algorithms are known to achieve (nearly) optimal error

Robust mixture learning tackles the model in Eq. (2.1) with \(_{i}w_{i}\) and aims to output exactly \(k\) vectors with an accurate estimate for the population mean of each component [3; 4; 7; 9; 11; 13; 29; 30]. These algorithms do not enjoy error guarantees for clusters with weights \(w_{i}<\). To the best of our knowledge, our algorithm is the first to achieve non-trivial guarantees in this larger noise regime.

Robust clusteringRobust clustering  also addresses the presence of small fractions of outliers in a similar spirit to robust mixture learning, conceptually implemented in the celebrated DBScan algorithm . Assuming the output list size is large enough to capture possible outlier clusters, these methods may also be used to tackle list-decodable mixture learning - however, they do not come with an inherent procedure to determine the right choice of hyperparameters that ultimately output a list size that adapts to the problem.

## 6 Discussion and future work

In this work, we prove that even when small groups are outnumbered by adversarial data points, efficient list-decodable algorithms can provide an accurate estimation of all means with minimal list size. The proof for the upper bound is constructive and analyzes a plug-and-play meta-algorithm (cf. Figure 1) that inherits guarantees of the black-box \(\)-\(\) algorithm \(_{}\) and \(\) algorithm \(_{R}\), which it uses as base learners. Notably, when the inlier mixture is a mixture of Gaussians with identity covariance, we achieve optimality. Furthermore, any new development for the base learners automatically translates to improvements in our bounds.

We would like to end by discussing the possible practical impact of this result. Since an extensive empirical study is out of the scope of this paper, besides the fact that ground-truth means for unsupervised real-world data are hard to come by, we provide preliminary experiments on synthetic data. Specifically, we generate data from a separated \(k-\)Gaussian mixture with additive contaminations as in Eq. (2.1) and different types of adversarial distributions (see detailed description in Appendix I). We focus on the regime \( w_{i}\) where our algorithms shows the largest theoretical improvements.

We then compare the output of our algorithm with the vanilla LD-ME algorithm from  with \(w_{}=0.02\) and (suboptimal) LD-ML guarantees as well as well-known (robust) clustering heuristics without LD-ML guarantees, such as the \(k\)-means , Robust \(k\)-means , and DBSCAN . Even though none of these heuristics have LD-ML guarantees, they are commonly used and known to also perform well in practice in noisy settings. In Figure 2 (left), we fix the list size to \(10\) and plot the errors for the worst inlier cluster, typically the smallest. We compare the performance of the algorithms by plotting the worst-case estimation errors for a given list size and list sizes that algorithms require to achieve a given worst-case estimation error. In Figure 2 (right), we fix the error and plot the minimal list size at which competing algorithms reach the same or smaller worst estimation error. Further details on the experiments are provided in Appendix I. In a different experiment (see Figure 3 and Appendix I.1 for details), we observe that our approach outperforms LD-ME when \(w_{}\) varies, both in achieving smaller list size and smaller estimation error.

Overall, in line with our theory, our method significantly outperforms the LD-ME algorithm, and performs better or on par with the heuristic approaches. Additional experimental comparison and implementation details can be found in Appendix I. Even though these experiments do not allow conclusive statements about the improvement of our algorithm for mixture learning for real-world data, they do provide encouraging evidence that effort could be well-spent on follow-up empirical and theoretical work building on our results. For example, it would be interesting to conduct a more extensive empirical study comparing our algorithm with a variety of robust clustering algorithms. Additionally, practical data often contains components with varying scales. An interesting direction for future work could be to extend our algorithm to handle differently scaled covariances in an agnostic manner.

Figure 3: Comparison of list size and estimation error for large inlier cluster for varying \(w_{}\) inputs. The experimental setup is illustrated in Appendix I. We plot the median values with error bars showing \(25\)th and \(75\)th quantiles. As \(w_{}\) decreases, we observe a roughly constant estimation error for our algorithm while the error for LD-ME increases. Further, the decrease in list size is much more severe for LD-ME than for our algorithm.

Figure 2: Comparison of five algorithms with two adversarial noise models. The attack distributions and further experimental details are given in Appendix I. On the left we show worst estimation error for constrained list size and on the right the smallest list size for constrained error guarantee. We plot the median of the metrics with the error bars showing \(25\)th and \(75\)th percentile.