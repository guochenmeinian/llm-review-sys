# DMC-VB: A Benchmark for Representation Learning

for Control with Visual Distractors

 Joseph Ortiz, Antoine Dedieu, Wolfgang Lehrach, J. Swaroop Guntupalli,

Carter Wendelken, Ahmad Humayun, Guangyao Zhou, Sivaramakrishnan Swaminathan,

Miguel Lazaro-Gredilla, Kevin Murphy

Google DeepMind

{joeortiz,adedieu}@google.com

Equal contribution

###### Abstract

Learning from previously collected data via behavioral cloning or offline reinforcement learning (RL) is a powerful recipe for scaling generalist agents by avoiding the need for expensive online learning. Despite strong generalization in some respects, agents are often remarkably brittle to minor visual variations in control-irrelevant factors such as the background or camera viewpoint. In this paper, we present the _DeepMind Control Vision Benchmark_ (DMC-VB), a dataset collected in the _DeepMind Control Suite_ to evaluate the robustness of offline RL agents for solving continuous control tasks from visual input in the presence of visual distractors. In contrast to prior works, our dataset (a) combines locomotion and navigation tasks of varying difficulties, (b) includes static and dynamic visual variations, (c) considers data generated by policies with different skill levels, (d) systematically returns pairs of state and pixel observation, (e) is an order of magnitude larger, and (f) includes tasks with hidden goals. Accompanying our dataset, we propose three benchmarks to evaluate representation learning methods for pretraining, and carry out experiments on several recently proposed methods. First, we find that pretrained representations do not help policy learning on DMC-VB, and we highlight a large representation gap between policies learned on pixel observations and on states. Second, we demonstrate when expert data is limited, policy learning can benefit from representations pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals. Our dataset and benchmark code to train and evaluate agents are available at https://github.com/google-deepmind/dmc_vision_benchmark.

## 1 Introduction

Reinforcement learning (RL)  provides a framework for learning behaviors for control, represented by policies, that maximize rewards collected in an environment. Online RL algorithms iteratively take actions--collecting observations and rewards from the environment--then update their policy using the latest experience. This online learning process is however fundamentally slow. Recently it has become clear that learning behaviors from large previously collected datasets, via behavioral cloning or other forms of offline RL , is an effective alternative way to build scalable generalist agents--see e.g. .

Despite these advances, recent research indicates that agents trained on offline visual data often exhibit poor generalization to novel visual domains, and can fail under minor visual variations in the background or camera viewpoint . This poor generalization can be understood by formalizing environments as Markov Decision Processes (MDPs) with a factorized state consisting of control relevant and irrelevant variables --see Appendix B for further discussion. In such MDPs, inorder to generalize to novel visual domains, an agent must learn latent representations that capture the information sufficient for control, yet that are minimal and therefore robust to irrelevant variations in the input--see . We call such representations "_control-sufficient_".

Several works derive latent representations for control with generative models of the input . By construction, such representations are not minimal, and may not be robust. Therefore various non-generative methods have been proposed as alternatives. These methods include contrastive learning , latent forward models , and inverse dynamics models . While there exist several datasets to evaluate the generalization of these (and other) representation learning techniques to various visual perturbations , these datasets do not provide the essential properties that are needed to thoroughly evaluate these representation learning methods for control.

To fill this gap, we introduce the _DeepMind Control Vision Benchmark_ (DMC-VB)--a dataset collected using the DeepMind Control Suite  and various extensions thereof. DMC-VB is carefully designed to enable systematic and rigorous evaluation of representation learning methods for control in the presence of visual variations, by satisfying six key desiderata. (a) It contains a diversity of tasks--including tasks where state-of-the-art algorithms struggle--to drive the development of novel algorithms. (b) It contains different types of visual distractors (e.g. changing background, moving camera) to study robustness to various realistic visual variations. (c) It includes demonstrations of differing quality to investigate whether effective policies can be derived from suboptimal demonstrations. (d) It includes both pixel observations and states, where states are relevant proprioceptive and exteroceptive measurements. Policies trained on states can then provide an upper bound to quantify the "_representation gap_" of policies trained on pixels. (e) It is larger than previous datasets. (f) It includes tasks where the goal cannot be determined from visual observations, for which recent work  suggests that pretraining representations is critical. As DMC-VB is the first dataset to satisfy these six desiderata (see Table 1) it is well placed to advance research in representation learning for control.

Accompanying the dataset, we propose three benchmarks that leverage the carefully designed properties of DMC-VB to evaluate representation learning methods for control. **(B1)** evaluates the degradation of policy learning in the presence of visual distractors, and, in doing so, quantifies the representation gap between agents trained on states and on pixel observations. We find that the simple behavior cloning (BC) baseline is the best overall method, and that recently proposed representation learning methods, such as inverse dynamics (ID) , do not show benefits on our benchmark. **(B2)** investigates a practical setting with access to a large dataset of mixed quality data and a few expert demonstrations. We find that leveraging mixed data for pretraining a visual representation improves policy learning. Finally, **(B3)** studies representation learning on demonstrations with random hidden goals, as may be the case when an agent collects data in a new environment via goal-free exploration. We find that representations pretrained on this data help few-shot policy learning on new tasks.

Figure 1: DeepMind Control Vision Benchmark.

## 2 Related work

**Environments for control:** Many environments have been developed to study the performance of RL agents. The popular _Arcade Learning Environment (ALE)_ proposes an interface to hundreds of Atari 2600 games, and has been used in groundbreaking works in deep RL [36; 22]. The _OpenAI Gym_ extends _ALE_ to board games, robotics, and continuous control tasks; the _DM Control suite_ also proposes a similar suite of environments. Several works [45; 23; 53; 2] extend these control environments to visual tasks with various types of distractors, including agent color, background, and camera pose. However, these environments evaluate the robustness of online RL agents to visual distractors and do not provide pre-collected trajectories for offline learning.

**Offline datasets for control:** To foster research in offline RL, _D4RL_ proposed a suite of datasets collected in the _OpenAI Gym_ which cover diverse tasks with properties such as sparse rewards, partial observability, multitasking, etc. However, as _D4RL_ only gives access to states, _VD4RL_ extends _D4RL_ to complex visual scenes. _VD4RL_ considers three locomotion tasks from the _DM Control Suite_, and collects datasets containing observations with visual distractors for each task by deploying five behavioral policies, ranging from random to expert. However, _VD4RL_ does not include states, which are useful to measure the representation gap between policies trained on images and on states. Second, the _VD4RL_ datasets released only cover a small fraction of the combinations of embodiments, policies and distractors. Finally, each dataset only contains \(100\)k steps which, as we show in Appendix I, can be too small for RL algorithms. The _RL unplugged_ datasets also collect trajectories on different environments from _ALE_ and the _DM Control Suite_. Recently, _Libero_ proposes several datasets to evaluate lifelong learning for robotics manipulation. Finally, _Colosseum_ studies generalization on \(20\) manipulation tasks, which can be systematically modified across \(14\) axes of variation. However, _RL unplugged_, _Libero_, and _Colosseum_ datasets lack visual distractor diversity in the data and do not include different behavioral policies.

**Representation learning for control:** Representation learning methods map a high-dimensional input (an image) into a compact low-dimensional space. Several studies show that pretrained representations can improve RL policy learning [51; 7]. Early works [49; 48] learn representations with auto-encoders. As these models reconstruct the observations, they fail to discard visual distractors and to generalize under visual distribution shifts . To learn control-sufficient representations, several works have explored non-generative losses. Lamb et al.  showed that multi-step inverse dynamics (ID) can, theoretically and empirically, learn a "minimal" world model, which can be used for exploration and navigation. ID has also proven to be beneficial to train offline RL agents in the presence of visual distractors , and when the goal is a hidden variable . Another approach, latent forward models (LFD), predict future latent representations from current ones . Contrastive methods maximize the similarity between augmentations of the same data [11; 28; 23; 34] or temporally related images [39; 37]. Other works maximize the mutual information between the latents and control relevant variables such as the reward or actions [6; 14] or use masked auto-encoding [35; 42]. Our experiments (Sec. 5) benchmark simple variants of many of these methods on DMC-VB.

   Dataset & Task diversity & Distractor diversity & Different policies & States and obs. & Large & Hidden goal \\   _D4RL_ & ✓ & ✗ & (✓) not for each task & ✗ & ✓ & ✓ \\  _VD4RL_ & ✗ & (✓) not released & (✓) not released & ✗ & ✗ & ✗ \\  _RL unplugged_ & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ \\  _Libero_ & ✓ & ✗ & ✗ & ✓ & ✓ & ✗ \\  _Colosseum_ & ✓ & (✗) only for eval. & ✗ & ✓ & ✓ & ✗ \\ 
**DMC-VB** (ours) & ✓ **(B1)** & ✓ **(B1)** & ✓ **(B1-2)** & ✓ **(B1)** & ✓ **(B1-2-3)** & ✓ **(B3)** \\   

Table 1: Among existing offline RL datasets for continuous control tasks, DMC-VB is the only one to satisfy the six desiderata we have identified. We highlight the specific benchmark(s) within Sec.5 that leverage each of these properties. This table includes five relevant datasets and is not an exhaustive list. We exclude related environments which do not provide datasets for offline learning.

DeepMind Control Vision Benchmark

In this section, we describe our dataset, DMC-VB, that enables systematic evaluation of representation learning methods for control with visual distractors. A visual overview is presented in Fig.1.

### Dataset Characteristics / Desiderata

As highlighted in Table 1, DMC-VB is the first offline RL dataset to satisfy these six desiderata:

**Task diversity:** DMC-VB contains both simpler locomotion tasks and harder \(3\)D navigation tasks. We use the same three locomotion tasks as : walker-walk, cheetah-run and humanoid-walk. The walking humanoid task is significantly harder due to its high degree of freedom action space. For each task, the maximum reward is attained by reaching a target velocity. In addition, we create a suite of seven navigation tasks, corresponding to seven maze layouts with three levels of complexity: empty, single-obstacle(\( 3\)) and multi-obstacle (\( 3\)). For each task, a quadruped ant is randomly placed in the maze and has to reach a randomly placed visible goal (a green sphere). We define a dense reward as the negative normalized shortest path length between the ant and the goal, respecting the maze layout--see Appendix C.3. This ensures the reward falls in the range \([-1,0]\).

**Distractor diversity:** For locomotion tasks, we leverage the _Distracting Control Suite_ to add visual variation to the agent color, camera viewpoint, and background. We use three different distractor settings: _none_ has no variations; _static_ has an image background, random static camera viewpoint and variable agent color; _dynamic_ has a video background, randomly moving camera and variable agent color. For backgrounds, we use synthetic videos or images of multiple objects falling from the Kubric dataset . For the navigation tasks, visual distractors consist of different textures and colors applied to floors and walls of the maze environment.

**Demonstration quality diversity:** To produce datasets with diversity in demonstration quality, DMC-VB generates trajectories using behavioral policies of different skill levels, ranging from random to expert. We select checkpoints for each behavioral policy level as follows: _expert_ is the first checkpoint that achieves 95% of the reward achieved at convergence, _medium_ is the checkpoint that achieves 50% of the reward at convergence, _mixed_ uses eight evenly spaced checkpoints that achieve between 0-50% of the reward at convergence, _random_ samples actions randomly from the action space. We use the 95% reward checkpoint as the expert policy because we find that behavior diversity diminishes with more training steps. The reward distributions for our dataset are shown in Fig.2.

**State and visual observations:** Few works compare agents trained on pixel observations versus on states. In contrast, DMC-VB systematically collects both states and images which allows training agents on both to systematically quantify the representation gap.

**Large size:** Each dataset in DMC-VB contains \(1\)M steps, which is equivalent to \(2\)k episodes for the locomotion tasks and more than \(2\)k variable-length episodes for the ant maze tasks. This makes DMC-VB one order of magnitude larger than _VD4RL_, the most similar prior dataset. We motivate this choice in Appendix I through experiments which find that limiting expert data significantly harms BC agents, particularly for harder tasks and with visual distractors.

**Hidden goal:** shows the benefits of inverse dynamics pretraining for tasks in which the goal is not determinable from the visual observations. Inspired by this finding, we include a variant of our navigation dataset in which the target sphere is not visible, which we study in Sec. 5.3.

### Dataset Generation

To generate a dataset of DMC-VB, for each task, we first train an online MPO agent  using states for \(2\)M steps. The states and rewards for each task are described in Appendices C.2 and C.3 respectively. Relevant training checkpoints are then selected for the behavioral policies. We generate demonstrations by rolling out the behavioral policy in the environment, collecting rewards, actions, states and visual observations. As in [52; 33], we use an action repeat of two in all environments to increase the change between successive frames and ease learning from pixels.

For navigation tasks, we collect three observations for each timestep: (a) a top-down view of the maze, (b) an egocentric view, (c) a follow-up view2--which is useful to infer the agent's state--see Fig.1. We additionally collect these same observations with the goal sphere hidden.

## 4 Agents and Visual Representation Learning for Control on DMC-VB

Given a DMC-VB dataset \(D=\{\ _{i}\ \}_{i=1:N}\) of \(N\) trajectories pre-collected in an environment, an agent must learn a policy that maximizes the expected sum of the rewards when deployed in the same environment. Each trajectory contains observations, actions and rewards3: \(=(_{1},r_{1},_{1},_{2},r_{2},_{2},...)\). Given an observation \(_{t}\), an agent selects an action \(}_{t}=((_{t}))\), where \(\) is a visual encoder network, and \(\) is a policy network.

The agent is trained in a two-stage procedure. The pretraining stage first learns the visual encoder network \(\) to minimize a representation learning loss:

\[^{*}*{argmin}_{}\ _{ D}[\ _{}(,)\ ]\.\] (1)

The second stage learns the policy network \(\) by minimizing a policy learning objective with the visual encoder network \(^{*}\) held constant:

\[^{*}*{argmin}_{}\ _{ D}[\ _{}(,^{*},)\ ]\.\] (2)

Architectures:We standardize \(\) as a convolutional neural network (CNN) followed by a linear projection, and \(\) as a multi-layer perceptron (MLP)--see Appendix D for details.

Frame stacking:As in , we use frame stacking of \(=3\) and select actions as \(}_{t}=((}_{t}))\) where \(}_{t}=(_{t-+1},,_{t})\). We drop the notation \(}_{t}\) in this section for simplicity. Appendix H verifies, through an ablation study, that frame stacking is crucial for policy learning.

### Policy learning

As the primary focus is to benchmark visual representation learning for control, we limit the study to two simple objectives for learning our policy \(\): behavioral cloning (BC) and TD3-BC .

Behavioral cloning (BC) learns a policy via supervised learning by minimizing the objective:

\[_{}(,,)=_{(_{t},_{t})}\|((_{t}))-\ _{t}\|_{2}^{2}.\] (3)

TD3-BC is a model-free offline RL algorithm that trains an actor and two critic networks . TD3-BC uses the same critic objective as TD3 , and adds a BC term to the actor (policy) objective, to regularise the learned policy towards actions in the dataset \(D\) (see Appendix D for details):

\[_{}(,,)=_{(_{t}, _{t})}[\ Q_{1}((_{t}),(( _{t}))-\|((_{t}))-\ _{t}\|_{2}^{2} ]\.\] (4)

Figure 2: Reward distribution for different behavioral policy levels in DMC-VB. Note the log scale on the vertical axis. Statistics of these distributions are summarized in Appendix C.1.

### Visual representation learning

We explore several representation learning methods to pretrain the encoder \(\), before policy learning.

**Inverse Dynamics (ID):** An inverse dynamics model estimates the next action from the current observation and a future observation \(k\) steps ahead via the objective:

\[_{}(,)=_{(_{t},_{t+k}, _{t})}\|_{t}-f((_{t}),\;( _{t+k}),\;k)\|_{2}^{2}.\] (5)

In practice, we replace \(_{t},_{t+k}\) with \(}_{t},}_{t+k}\) to combine ID with frame stacking and set \(k=1\).

**Latent Forward Dynamics (LFD):** The latent forward dynamics objective predicts the next latent observation from the current observation and action:

\[_{}(,)=_{(_{t},_{t}, _{t+1})}\|^{}(_{t+1})-g(( _{t}),\;_{t})\|_{2}^{2}.\] (6)

To avoid latent collapse , we encode \(_{t+1}\) using a target network \(^{}\), whose weights are an exponential moving average of the weights of \(\), with decay rate \(0.99\).

**AutoEncoder (AE):** An autoencoder  jointly trains the encoder \(\) with a decoder \(\) to minimize the pixel reconstruction loss: \(_{}(,)=_{_{t}}\| ((_{t}))-_{t}\|_{2}^{2}\).

**Pretrained DINO encoder:** Lastly, we consider a pretrained DINO encoder . Note that we need to pad the \(64 64\) observations in the DMC-VB dataset to the \(224 224\) size required by DINO.

### Baselines using privileged states

The presence of both visual observations and privileged states in DMC-VB enables the evaluation of realistic upper bounds on representation learning methods. We include two state-based baselines.

**BC (state):** This BC variant has access to the states at both training and evaluation time. The actions are predicted as \(}_{t}=((_{t}))\), where \(\) is a MLP.

**State prediction pretraining:** Representations are pretrained to predict states (only accessible during training) via the objective: \(_{}(,)=_{(_{t},_ {t})}\|_{t}-h((_{t}))\|_{2}^{2}\), where \(h\) is a linear layer.

## 5 Benchmark Experiments

Accompanying DMC-VB, we propose three benchmark evaluations that examine the utility of pretrained visual representations for policy learning in the presence of visual variations. Each benchmark leverages unique properties of DMC-VB--see Table 1--and selects different data subsets to investigate the following questions on visual representation learning for control:

\(\) **(B1)** studies whether visual representation learning makes policies robust to distractors. (Sec. 5.1)

\(\) **(B2)** investigates whether visual representations pretrained on mixed quality data improve policy learning with limited expert data. (Sec. 5.2)

\(\) **(B3)** explores whether visual representations pretrained on tasks with stochastic hidden goals improve policy learning on a new task with fixed hidden goal and limited expert data. (Sec. 5.3)

**Notation:** Following Sec.4, we denote \(_{1}+_{2}\) the agent for which the method \(_{1}\) is used to pretrain the encoder \(\), and \(_{2}\) is used to learn the policy \(\). For instance, ID + BC refers to first pretraining the encoder with ID, followed by learning the policy with BC (with a frozen encoder). We denote NULL + BC the agent for which \(\) is trained end-to-end with BC. In Secs. 5.2 and 5.3, when we pretrain on a dataset \(D_{1}\), then learn the policy on another dataset \(D_{2}\), we name the agent \(_{1}(D_{1})+_{2}(D_{2})\). In addition, NULL + BC (states) trains end-to-end \(\) on states with BC. Lastly, Data refers to the average reward on the training dataset.

**Preprocessing details:** We first normalize actions in \([-1,1]\) and observations in \([-0.5,0.5]\). Second, as in , we pad each \(64 64\) image by \(4\) pixels on each side, and then apply a random cropping to return a randomly shifted \(64 64\) image. Finally, we apply a frame stacking of three consecutive observations. For models which use state, we center and normalize each state dimension.

**Training:** For both representation pretraining and policy learning, we use for \(400\)k Adam iterations on a single NVIDIA A100 GPU with batch size \(256\) and learning rate \(0.001\). Appendix D details all the architectures and hyperparameters used.

**Online evaluation:** Every \(20\)k training steps, we evaluate the agent online over \(30\) online rollouts in the evaluation environment. When visual distractors are present, the evaluation environment contains unseen visual distractors from the training distractor distribution. In the figures displayed in this section, we report the best (online) evaluation scores obtained through training.

### Policy learning with visual distractors

**Benchmark 1** evaluates the reward collected by different agents on the full DMC-VB expert and medium datasets in Fig.3. For the locomotion tasks, we find that the baseline NULL+BC is the top performing method. ID+BC matches (but does not exceed) this baseline, and the other pretraining methods all perform worse. This indicates that pretrained representations do not seem to help, at least in these benchmark experiments. With distractors, on walker-expert, walker-medium, and cheetah-medium, NULL+BC and ID+BC are the only methods that maintain high rewards suggesting that they learn representations that are robust to visual variations. Lastly, offline RL (NULL+TD3-BC) is outperformed by NULL+BC most of the time, and never exceeds it.

For the DMC-VB navigation tasks, NULL+BC is the best method and pretrained visual representations only harm performance. We observe similar results when plotting the fraction of successes in reaching the goal and the average velocity of the agent toward the goal in Appendix E.6. We observe that for many tasks, performance degrades more strongly with visual distractors for expert than for medium datasets; we expect this is due to the medium dataset having higher coverage (see Fig. 2), reducing the chances of failure by entering a highly out of distribution state.

**Inspecting the visual representations:** To provide insights into the learned visual representations, we freeze the encoder and train two decoders to reconstruct (a) observations, and (b) states (leveraging the states in DMC-VB). Fig. 4 shows that representations that achieve low state reconstruction error capture minimal control-relevant features and lead to better downstream policy performance (ID, BC). Meanwhile, representations that attain low observation reconstruction error are not robust to visual distractors and result in worse downstream policies (LFD, AE, DINO). For locomotion tasks

Figure 3: Online evaluation scores on the locomotion tasks [three top rows] and ant maze navigation tasks [bottom row] of DMC-VB, averaged over \(30\) trajectories, with standard errors. Higher reward is better. For locomotion task rows, results are grouped by distractor type and demonstration data quality. For the ant maze row, results are grouped by maze difficulty and demonstration data quality. NULL + BC is the best overall method. Pretrained representations offer no advantage with or without visual distractors. LFD + BC performs poorly, AE + BC and DINO + BC learn moderate policies, and ID + BC is comparable to NULL + BC. Full scores are in Appendix E.1 and the temporal evolution of rewards through training is plotted in Appendix E.2.

with visual distractors, both ID and BC achieve the highest observation reconstruction errors and the lowest state reconstruction errors, also discarding control-irrelevant information such as background and agent color. As expected the AE achieves the lowest observation reconstruction errors, and similar to DINO fails to learn good policies due to high state reconstruction errors. Finally, LFD has the highest state reconstruction errors, which explains its bad performance.

**State-based upper bounds:** Policies trained on states are unaffected by the presence of visual distractors, and provide an upper bound for representation learning methods. Fig. 3 shows that the representation gap between policies trained on states versus on observations is minimal in the absence of distractors, but widens significantly in the presence of dynamic distractors. Lastly, we found no advantage in pretraining the encoder to predict the state, which we attribute to the fact that some state components (e.g. velocities) are hard to predict--see Appendix E.5 for details.

### Pretraining representations on mixed data helps few-shot policy learning

**Benchmark 2** leverages DMC-VB datasets collected with different behavioral policy levels to investigate whether pretraining a representation on sub-optimal (mixed) data helps policy learning when expert data is scarce. Specifically, for each locomotion task, we first train two vision encoders, using ID and BC on the full _mixed_ quality DMC-VB datasets. Second, we freeze the encoders and train the policy networks using only \(1\%\) of the _expert_ dataset (\(20\) trajectories). Fig. 5 compares these two-stage training agents (in green and red) to a BC agent trained on \(1\%\) of the expert dataset (blue), and a BC agent trained on the combined \(1\%\) expert dataset and full mixed dataset (orange). The results suggest that (a) limiting expert data impacts performance, (b) merely combining mixed and limited expert data leads to a drop in performance for BC, and (c) pretraining a representation on mixed data provides a performance boost, especially in the presence of visual distractors.

### Pretraining on tasks with stochastic hidden goals helps few-shot policy learning

Figure 4: [Left] Least-squared test error for reconstructing the observations [top] and states [bottom], averaged over the different DMC-VB locomotion tasks and policies. Lower is better. As results are averaged over \(150\)k samples, the standard errors are too small to be visible. See Appendix E.3, for a detailed breakdown per task and distractor. [Right] Observation reconstruction examples. See Appendix E.4, for additional image reconstructions. BC and ID both (a) discard visual distractors (background object and agent color), and (b) reach the lowest state reconstruction errors.

Figure 5: Pretraining encoders on mixed data improves performance when a BC policy is trained on a small expert dataset. BC and ID pretraining perform similarly. For each task, performance is reported as the proportion of reward obtained by BC on full expert data without distractors (higher is better). We include full results including _cheetah_, and LFD/AE pretraining in Appendix F.

**Benchmark 3** considers a variant of the ant maze datasets of DMC-VB, in which image observations are rendered without the goal (green sphere). This setting mimics scenarios in which data is generated by an agent exploring a new environment in an open-ended manner without explicit goals. For each maze, we are given a large _expert_ dataset of \(1\)M steps. In each trajectory, the agent moves from a random initial position to a random final position--note the goal cannot be derived from visual observations. We denote this pretraining dataset: _stochastic goals_ in Fig. 6. Additionally, for each maze, we have access to five small datasets of ten _expert_ trajectories each. All trajectories in each dataset go from a fixed start to a fixed goal location, with noise added to the agent's body initialization. We denote this dataset: _fixed goal_. As in **(B2)**, Fig. 6 compares BC agents trained directly on the small dataset, to those with two-stage training, for which representations are pre-trained on the large _stochastic goals_ dataset. Our findings indicate that pretraining a representation on a collection of tasks with stochastic hidden goals aids policy learning on new tasks with fixed hidden goals. Despite prior findings that ID pretraining on tasks with stochastic hidden goals performs better than BC , we find that BC pretraining outperforms ID pretraining on this benchmark.

## 6 Conclusions

We propose DMC-VB, a dataset designed for systematic evaluation of representation learning methods for control tasks in environments with visual distractors. DMC-VB is the first dataset to satisfy six key desiderata, enabling detailed and diverse evaluations. Alongside our dataset, we propose three benchmarks on DMC-VB which compare several established representation methods.

Our results suggest, rather surprisingly, that current pretraining methods (with or without generative losses) do not help BC policy learning on DMC-VB. We leave a detailed investigation into the reason for these results to future work. We also expose a large representation gap between the performance of (a) BC with and without visual distractors, and (b) BC on pixels vs. BC on states, which highlights the need for better representation learning methods. In addition, our findings reveal the potential of leveraging suboptimal datasets, and tasks with stochastic hidden goals, to pretrain representations and learn better policies on DMC-VB with limited expert data.

DMC-VB's unique features present researchers with the tools to investigate fundamental questions in representation learning for control; and to systematically benchmark the performance of future representation learning methods, ultimately contributing to the creation of robust, generalist agents. Our work has two primary limitations. First, DMC-VB could be extended to a broader range of environments, including sparse rewards, multiple agents, complex manipulation tasks, or stochastic dynamics. Second, the synthetic nature of our visual distractors may raise questions about the generalization of our findings to real-world tasks. While our findings may be applicable to robotics tasks, incorporating more diverse and realistic distractors would strengthen our findings.