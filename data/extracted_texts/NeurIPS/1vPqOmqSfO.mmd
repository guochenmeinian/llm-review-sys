# Sketched Lanczos uncertainty score: a low-memory summary of the Fisher information

Marco Miani

Technical University of Denmark

mmia@dtu.dk

&Lorenzo Beretta

Unaffiliated

lorenzo2beretta@gmail.com

&Soren Hauberg

Technical University of Denmark

sohau@dtu.dk

Equal contribution

###### Abstract

Current uncertainty quantification is memory and compute expensive, which hinders practical uptake. To counter, we develop Sketched Lanczos Uncertainty (slu): an architecture-agnostic uncertainty score that can be applied to pre-trained neural networks with minimal overhead. Importantly, the memory use of slu only grows _logarithmically_ with the number of model parameters. We combine Lanczos' algorithm with dimensionality reduction techniques to compute a sketch of the leading eigenvectors of a matrix. Applying this novel algorithm to the Fisher information matrix yields a cheap and reliable uncertainty score. Empirically, slu yields well-calibrated uncertainties, reliably detects out-of-distribution examples, and consistently outperforms existing methods in the low-memory regime.

## 1 Introduction

The best-performing uncertainty quantification methods share the same problem: _scaling_. Practically this prevents their use for deep neural networks with high parameter counts. Perhaps, the simplest way of defining such a score is to independently train several models, perform inference, and check how consistent predictions are across models. The overhead of the resulting 'Deep Ensemble'  notably introduces a multiplicative overhead equal to the ensemble size. Current approaches aim to reduce the growth in training time costs by quantifying uncertainty through _local_ information of a single pre-trained model. This approach has shown some success for methods like Laplace's approximation , swag, scod or Local Ensembles . They avoid the need to re-train but still have impractical memory needs.

A popular approach to characterizing local information is the empirical Fisher information matrix, which essentially coincides with the Generalized Gauss-Newton (ggn) matrix . Unfortunately, for a \(p\)-parameter model, the ggn is a \(p p\) matrix, yielding such high memory costs that it cannot be instantiated for anything but the simplest models. The ggn is, thus, mostly explored through approximations, e.g. block-diagonal , Kronecker-factorized  or even diagonal . An alternative heuristic is to only assign uncertainties to a subset of the model parameters , e.g. the last layer.

Instead, we approximate the ggn with a low-rank matrix. A rank-\(k\) approximation of the ggn can be computed using Lanczos algorithm  or truncatedsingular value decomposition (svd) (Sharma et al., 2021). These approaches deliver promising uncertainty scores but are limited by their memory footprint. Indeed all aforementioned techniques require \(k p\) memory. Models with high parameter counts are, thus, reduced to only being able to consider _very_ small approximate ranks.

**In this work**, we design a novel algorithm to compute the local ensemble uncertainty estimation score introduced by Madras et al. (2019), reintroduced in Section 2.1. Our algorithm is substantially more memory-efficient than the previous one both in theory and practice, thus circumventing the main bottleneck of vanilla Lanczos and randomized svd (Figure 1). To that end, we employ sketching dimensionality-reduction techniques, reintroduced in Section 2.3, that trade a small-with-high-probability error in some matrix-vector operations for a lower memory usage. Combining the latter with the Lanczos algorithm (reintroduced in Section 2.2) results in the novel Sketched Lanczos. This essentially drops the memory consumption from \((pk)\) to \((k^{2}^{-2})\) in exchange for a provably bounded error \(\), _independently_ on the number of parameters \(p\) (up to log-terms).

Applying this algorithm in the deep neural networks settings allows us to scale up the approach from Madras et al. (2019) and obtain a better uncertainty score for a fixed memory budget.

Our contribution is twofold: (1) we prove that orthogonalization approximately commutes with sketching in Section 3, which makes it possible to sketch Lanczos vectors on the fly and orthogonalize them post-hoc, with significant memory savings; (2) we empirically show that, in the low-memory-budget regime, the disadvantage of introducing noise through sketching is outweighed by a higher-rank approximation, thus performing better than baselines when the same amount of memory is used.

## 2 Background

Let \(f_{}:^{d}^{t}\) denote a neural network with parameter \(^{p}\), or equivalently \(f:^{p}^{d}^{t}\). Let \(_{^{}}(x)=_{}f_{}(x)|_{= ^{}}^{t p}\) be its Jacobian with respect to the parameters, evaluated at a datapoint \(x^{d}\). Given a training dataset \(=\{(x_{i},y_{i})\}_{i=1,,n}\) and a loss function \(()=_{(x,y)}(y,f(x,)\), the Generalized Gauss-Newton matrix (ggn) is defined as

\[_{}=_{i=1}^{n}_{}(x_{i})^{} (x_{i})_{}(x_{i}),\] (1)

where \((x_{i})=_{f_{}(x_{i})}^{2}(y_{i}|f_{}(x_{i})) ^{t t}\) is the Hessian of the loss with respect to the neural network output. We reduce the notational load by stacking the per-datum Jacobians into \(_{}=[_{}(x_{1});;_{}(x_{ n})]^{nt p}\) and similarly for the Hessians, and write the ggn matrix as \(_{}=_{}^{}_{}\). For extended derivations and connections with the Fisher matrix we refer to the excellent review by Kunstner et al. (2019). In the following, we assume access to a pre-trained model with parameter \(^{*}\) and omit the dependency of \(\) on \(^{*}\).

**Computationally** we emphasize that Jacobian-vector products can be performed efficiently when \(f\) is a deep nn. Consequently, the ggn-vector product has twice the cost of a gradient backpropagation, at least for common choices of \(\) like mse or cross-entropy (Khan and Rue, 2021).

### Uncertainty score

We measure the uncertainty at a datapoint \(x\) as the variance of the prediction \(f_{}(x)\) with respect to a distribution over parameter \(\) defined at training time and independently of \(x\). This general scheme has received significant attention. For example, Deep Ensemble (Lakshminarayanan et al., 2017) uses a sum of delta distribution supported on independently trained models, while methods that only train

Figure 1: OoD detection performance () on a ResNet.

a single network \(^{*}\) generally use a Gaussian \((|^{*},M)\) with covariance \(M^{p p}\). In the latter case, a first-order approximation of the prediction variance is given by a \(M\)-norm of the Jacobian

\[_{(|^{*},M)}[f_{}(x)] _{(|^{*},M)}[f_{}^{L}(x) ]=(_{^{*}}(x) M_{^{* }}(x)^{}),\] (2)

where \(f_{}^{L}(x)=f_{}(x)+_{^{*}}(x)(^{*}-)\) is a linearization of \( f_{}(x)\) around \(^{*}\).

The ggn matrix (or empirical Fisher; Kunstner et al. (2019)) is notably connected to uncertainty measures and, more specifically, to the choice of the matrix \(M\). Different theoretical reasoning leads to different choices and we focus on two of them:

\[M_{}=(+_{p})^{-1} M_{}=-_{},\] (3)

where \(_{}\) is the projection onto the non-zero eigenvectors of \(\) and \(>0\) is a constant.

Madras et al. (2019) justify \(M_{}\) through small perturbation along zero-curvature directions. 2Immer et al. (2021) justify \(M_{}\) in the Bayesian setting where \(\) is interpreted as prior precision. We do not question these score derivations and refer to the original works, but we highlight their similarity. Given an orthonormal eigen-decomposition of \(=_{i}_{i}v_{i}v_{i}^{}\) we see that

\[M_{}=_{i}+}v_{i}v_{i}^{} M_{ }=_{i}_{\{_{i}=0\}}v_{i}v_{i}^{}.\] (4)

Thus both covariances are higher in the directions of zero-eigenvalues, and the hyperparameter \(\) controls how many eigenvectors are relevant in \(M_{}\).

**Practical choices.** The matrix \(\) is too big to even be stored in memory and approximations are required, commonly in a way that allows access to either the inverse or the projection. A variety of techniques are introduced like diagonal, block diagonal, and block kfac which also allow for easy access to the inverse. Another line of work, like swap(Maddox et al., 2019), directly tries to find an approximation of the covariance \(M\) based on stochastic gradient descent trajectories. Alternatively, low-rank approximations use the eigen-decomposition relative to the top \(k\) eigenvalues, which allows direct access to both inverse and projection.

**Is low-rank a good idea?** The spectrum of the ggn has been empirically shown to decay exponentially (Sagun et al., 2017; Papyan, 2018; Ghorbani et al., 2019) (see Figure 2). We investigate this phenomenon further in Appendix C.1 with an ablation over Lanczos hyperparameters. This fast decay implies that the quality of a rank-\(k\) approximation of \(\) improves exponentially w.r.t. \(k\) if we measure it with an operator or Frobenius norm, thus supporting the choice of low-rank approximation. Moreover, in the _overparametrized_ setting \(p nt\), the rank of the ggn is by construction at most \(nt\), which is closely linked with functional reparametrizations (Roy et al., 2024).

The low-rank approach has been applied intensively (Madras et al., 2019; Daxberger et al., 2021; Sharma et al., 2021) with success, although always limited by memory footprint of \(pk\). Madras et al. (2019) argue in favor of a not-too-high \(k\) also from a numerical perspective, as using the "small" eigenvectors appears sensitive to noise and consequently is not robust.

### The Lanczos algorithm

The Lanczos algorithm is an iterative method for tridiagonalizing a symmetric matrix \(G\). If stopped at iteration \(k\), Lanczos returns a column-orthonormal matrix \(V=[V_{1}||V_{k}]^{p k}\) and a tridiagonal matrix \(T^{k k}\) such that \(V^{}GV=T\). The range space of \(V\) corresponds to the Krylov subspace \(_{k}=\{v,Gv,,G^{k-1}v\}\), where \(v=V_{1}\) is a randomly chosen vector. Provably, \(_{k}\)

Figure 2: ggn eigenvalues exponential decay. Average and standard deviation over 5 seeds. Details are in Appendix C.1.

approximates the eigenspace spanned by the top-\(k\) eigenvectors of \(G\), i.e. those corresponding to the eigenvalues of largest values (Meurant, 2006). Thus, \(VTV^{}\) approximates the projection of \(G\) onto its top-\(k\) eigenspace. Notice that projecting \(G\) onto its top-\(k\) eigenspace yields the best rank-\(k\) approximation of \(G\) under any unitarily-invariant norm (Mirsky, 1960). Once the decomposition \(G VTV^{}\) is available, we can retrieve an approximation to the top-\(k\) eigenpairs of \(G\) by diagonalizing \(T\) into \(T=WAW^{}\), which can be done efficiently for tridiagonal matrices (Dhillon, 1997). It has both practically and theoretically been found that this eigenpairs' approximation is very good. We point the reader to Meurant (2006) and Cullum and Willoughby (2002) for a comprehensive survey on this topic.

The benefits of Lanczos.Lanczos has two features that make it particularly appealing. First, Lanczos does not need explicit access to the input matrix \(G\), but only access to an implementation of \(G\)-vector product \(u Gu\). Second, Lanczos uses a small working space: only \(3p\) floating point numbers, where the input matrix is \(p p\). Indeed, we can think of Lanczos as releasing its output in streaming and only storing a small state consisting of the last three vectors \(V_{i-1},V_{i}\) and \(V_{i+1}\).

The downsides of Lanczos.Unfortunately, the implementation of Lanczos described above is prone to numerical instability, causing \(V_{1} V_{k}\) to be far from orthogonal. A careful analysis of the rounding errors causing this pathology was carried out by Paige (1971, 1976, 1980). To counteract, a standard technique is to re-orthogonalize \(V_{i+1}\) against all \(\{V_{j}\}_{j i}\), at each iteration. This technique has been employed to compute the low-rank approximation of huge sparse matrices (Simon and Zha, 2000), as well as by Madras et al. (2019) to compute an approximation to the top-\(k\) eigenvectors. Unfortunately, this version of Lanczos loses one of the two benefits described above, in that it must store a larger state consisting of the entire matrix \(V\). Therefore, we dub this version of the algorithm _hi-memory Lanczos_ and the memory-efficient version described above _low-memory Lanczos_. See Appendix C for a comprehensive discussion on these two versions.

Post-hoc orthogonalization Lanczos.Alternatively, instead of re-orthogonalizing at every step as in hi-memory Lanczos, we can run low-memory Lanczos, store all the vectors, and orthogonalize all together at the end. Based on the observations of Paige (1980), we expect that orthogonalizing the output of low-memory Lanczos post-hoc should yield an orthonormal basis that approximately spans the top-\(k\) eigenspace, similar to hi-memory Lanczos. This post-hoc version of Lanczos is however insufficient. It avoids the cost of orthogonalizing at every iteration but still requires storing the vectors, thus losing again the benefit of memory requirement. Or at least it does unless we find an efficient way to store the vectors.

### Sketching

Sketching is a key technique in randomized numerical linear algebra (Martinsson and Tropp, 2020) to reduce memory requirements. Specifically, sketching embeds high dimensional vectors from \(^{p}\) into a lower-dimensional space \(^{s}\), such that the expected norm error of vector dot-products is bounded, and, as a result, also the score in Equation 2. Here we give a concise introduction to this technique.

**Definition 2.1** (Subspace embedding).: Fix \(>0\). A \((1)\)\(_{2}\)-subspace embedding for the column space of an \(p k\) matrix \(U\) is a matrix \(S\) for which for all \(y^{k}\)

\[\|SUy\|_{2}=(1)\|Uy\|_{2}.\] (5)

The goal is to design an _oblivious_ subspace embedding, that is a random matrix \(S\) such that, for any matrix \(U\), \(S\) is a subspace embedding for \(U\) with sufficiently high probability. In our method, we use a Subsampled Randomized Fourier Transform (sfft) to achieve this goal (Ailon and Chazelle, 2009). A sfft is a \(s p\) matrix defined by the product \(}{{}}PHD\), where \(D\) is a diagonal matrix where each diagonal entry is an independent Rademacher random variable, \(H\) is the discrete Fourier transform, and \(P\) is a diagonal matrix where \(s\) random diagonal entries are set to one and every other entry is set to zero. Thanks to the Fast Fourier Transform algorithm, sfft can be evaluated in \(O(p p)\) time, and its memory footprint is only \(p+s\).

The following theorem shows that, as long as the sketch size \(s\) is big enough, sfft is an oblivious subspace embedding with high probability.

    & Time & Memory \\  Dense JL & \((p^{})\) & \(p^{2}\) \\ Sparse JL & \((p s)\) & \(p s\) \\ sfft & \((p p)\) & \(p+s\) \\   

Table 1: Sketch complexities comparison. Here \(\) is such that the current best matrix-multiplication algorithm runs in time \(n^{}\).

**Theorem 2.2** (Essentially, Theorem 7 in Woodruff et al. (2014)).: _For any \(p k\) matrix \(U\), srft is a \((1)\)-subspace embedding for the column space of \(U\) with probability \(1-\) as long as \(s=((k+ p)^{-2}(k/))\)._

We stress that, although several other random projections work as subspace embeddings, our choice is not incidental. Indeed other sketches, including the Sparse JL transform (Kane and Nelson, 2014; Nelson and Nguyen, 2013) or the Dense JL transform (Theorem 4, Woodruff et al. (2014)), theoretically have a larger memory footprint or a worse trade-off between \(s\) and \(k\), as clarified in Table 1. From such a comparison, it is clear that srft is best if our goal is to minimize memory footprint. At the same time, evaluation time is still quasilinear.

## 3 Method

We now develop the novel Sketched Lanczos algorithm by combining the 'vanilla' Lanczos algorithm (Section 2.2) with sketching (Section 2.3). Pseudo-code is presented in Algorithm 1. Next, we apply this algorithm in the uncertainty quantification setting and compute an approximation of the score in Equation 2 due to Madras et al. (2019). Our motivation is that given a fixed memory budget, the much lower memory footprint induced by sketching allows for a higher-rank approximation of \(\).

### Sketched Lanczos

We find the best way to explain our algorithm is to first explain a didactic variant of it, where sketching and orthogonalization happen in reverse order.

Running low-memory Lanczos for \(k\) iterations on a \(p p\) matrix iteratively constructs the columns of a \(p k\) matrix \(V\). Then, post-hoc, we re-orthogonalize the columns of \(V\) in a matrix \(U^{p k}\). Such a matrix is expensive to store due to the value of \(p\), but if we sample a srft sketch matrix \(S^{s p}\), we can then store a sketched version \(SU^{s k}\), saving memory as long as \(s<p\). In other words, this is post-hoc orthogonalization Lanczos with a sketching at the end.

We observe that sketching the columns of \(U\) is sufficient to \(\)-preserve the norm of matrix-vector products, with high probability. In particular, the following lemma holds (proof in Appendix A).

**Lemma 3.1** (Sketching low-rank matrices).: _Fix \(0<,<}{{2}}\) and sample a random \(s p\) srft matrix \(S\). Then, for any \(v^{p}\) and any matrix \(U^{p k}\) with \(||v||_{2},||U||_{2}=(1)\) we have_

\[_{S}\|(SU)^{}(Sv)\|_{2}=\|U^{}v\|_{2} >1-.\] (6)

_as long as \(s=(k^{-2} p(k/))\)._

This algorithm may initially seem appealing since we can compute a tight approximation of \(\|U^{}v\|\) by paying only \(s k\) in memory (plus the neglectable cost of storing \(S\)). However, as an intermediate step of such an algorithm, we still need to construct the matrix \(U\), paying \(p k\) in memory. Indeed, we defined \(U\) as a matrix whose columns are an orthonormal basis of the column space of \(V\) and we would like to avoid storing \(V\) explicitly and rather sketch each column \(V_{i}\) on the fly, without ever paying \(p k\) memory. This requires _swapping the order of orthogonalization and sketching_.

This motivates us to prove that if we orthonormalize the columns of \(SV\) and apply the same orthonormalization steps to the columns of \(V\), then we obtain an approximately orthonormal basis. Essentially, this means that sketching and orthogonalization approximately commute. As a consequence, we can use a matrix whose columns are an orthonormal basis of the column space of \(SV\) as a proxy for \(SU\) while incurring a small error. Formally, the following holds (proof in Appendix A).

**Lemma 3.2** (Orthogonalizing the sketch).: _Fix \(0<,<}{{2}}\) and sample a random \(s p\) srft matrix \(S\). As long as \(s=(k^{-2} p(k/))\) the following holds with probability \(1-\)._

_Given any \(p k\) full-rank matrix \(V\), decompose \(V=UR\) and \(SV=U_{S}R_{S}\) so that \(U^{p k}\), \(U_{S}^{s k}\) and both \(U\) and \(U_{S}\) have orthonormal columns. For any unit-norm \(v^{p}\) we have_

\[\|U_{S}^{}(Sv)\|_{2}=\|U^{}v\|_{2}.\] (7)

In Lemma 3.1, we proved that given a matrix \(U\) with orthonormal columns we can store \(SU\) instead of \(U\) to compute \(v||U^{}v||_{2}\) while incurring a small error. However, in our use case, we donot have explicit access to \(U\). Indeed, we abstractly define \(U\) as a matrix whose columns are an orthonormal basis of the column space of \(V\), but we only compute \(U_{S}\) as a matrix whose columns are an orthonormal basis of the column space of \(SV\), without ever paying \(p k\) memory. Lemma 3.2 implies that the resulting error is controlled.

Algorithm 1 lists the pseudocode of our new algorithm: Sketched Lanczos.

```
1:Input: Rank \(k\), sketch matrix \(S{}^{s p}\), matrix-vector product function \(v Gv\) for \(G{}^{p p}\).
2:Initialize \(v_{0}^{p}\) as a uniformly random unit-norm vector.
3:for\(i\)in\(1, k\)do
4:\(v_{i}(v_{i-1},v Gv)\) (glossing over the tridiagonal detail)
5: Sketch and store \(v_{i}^{S} Sv_{i}\)
6:endfor
7: Construct the matrix \(V_{S}=[v_{1}^{S},,v_{k}^{S}]^{s k}\)
8:Orthogonalize the columns of \(V_{S}\) and return \(U_{S}^{s k}\) ```

**Algorithm 1**Sketched Lanczos.

**Preconditioned Sketched Lanczos.** We empirically noticed that low-memory Lanczos' stability is quite dependent on the conditioning number of the considered matrix. From this observation, we propose a slight modification of Sketched Lanczos that trades some memory consumption for numerical stability.

The idea is simple, we first run hi-memory Lanczos for \(k_{0}\) iterations, obtaining an approximation of the top-\(k_{0}\) eigenspectrum \(U_{0}^{p k_{0}},_{0}^{k_{0} k_{0}}\). Then we define a new matrix-vector product

\[}v=(-U_{0}_{0}U_{0}^{})v\] (8)

and run Sketched Lanczos for \(k_{1}\) iterations on this new, better-conditioned, matrix \(}\). This results in a \(U_{S}^{s k_{1}}\) with sketched orthogonal columns. With \(k=k_{1}+k_{0}\), the simple concatenation \([SU_{0}|U_{S}]^{(k_{0}+k_{1}) s}\) is a sketched orthogonal \(k\)-dimensional base of the top-\(k\) eigenspace of \(\), analogous to non-preconditioned Sketched Lanczos. The extra stability comes at a memory cost of \(k_{0} p\), thus preventing \(k_{0}\) from being too large.

### Sketched Lanczos Uncertainty score (slu)

The uncertainty score in Equation 2 is computed by first approximating the Generalized Gauss-Newton matrix \(\). The approach of constructing an orthonormal basis \(U^{p k}\) of the top-\(k\) eigenvectors of \(\) with relative eigenvalues \(\), leads to the low-rank approximation \( U U^{}\). This step is done, for example, by Madras et al. (2019) through hi-memory Lanczos and by Sharma et al. (2021) through truncated randomized svd. In this step, we employ our novel Sketched Lanczos. Similar to Madras et al. (2019), we focus on the score with \(M_{}\) and thus we neglect the eigenvalues.

Having access to \(U\), we can compute the score for a test datapoint \(x^{d}\) as

\[[f_{}(x)](_{^{*}}(x) (-UU^{})_{^{*}}(x)^{})=\| _{^{*}}(x)\|_{F}^{2}-\|_{^{*}}(x)U\|_{F}^{2},\] (9)

which clarifies that computing \(||_{^{*}}(x)U||_{F}\) is the challenging bit to retrieve the score in Equation 2. Note that \(\|_{^{*}}(x)\|_{F}^{2}\) can be computed exactly with \(t\) Jacobian-vector products.

**Computing the uncertainty score through sketching.** Employing the novel Sketched Lanczos algorithm we can \(\)-approximate the score in Equation 9, with only minor modifications. Running the algorithm for \(k\) iterations returns a matrix \(U_{S}^{s k}\), which we recall is the orthogonalization of \(SV\) where \(V^{p k}\) are the column vectors iteratively computed by Lanczos.

Having access to \(U_{S}\), we can then compute the score for a test datapoint \(x^{d}\) as

\[(x)=\|_{^{*}}(x)\|_{F}^{2}-\|U_{S}^{}(S _{^{*}}(x)^{})\|_{F}^{2}\] (10)

where parentheses indicate the order in which computations should be performed: first a sketch of \(_{^{*}}(x)\) is computed, and then it is multiplied by \(U_{S}^{}\). Algorithm 2 summarizes the pipeline.

**Approximation quality.** Recall that the Jacobian \(_{^{*}}(x)\) is a \(t p\) matrix, where \(t\) is the output size of the neural network. A slight extension of Lemma 3.2 (formalized in Lemma A.1) implies that the score in Equation 9 is guaranteed to be well-approximated by the score in Equation 10 up to a factor \(1\) with probability \(1-\) as long as the sketch size is big enough \(s=(kt^{-2} p(kt/))\). Neglecting the log terms to develop some intuition, we can think of the sketch size to be \(s kt^{-2}\), thus resulting in an orthogonal matrix \(U_{S}\) of size \( kt^{-2} k\), which also correspond to the memory requirement. From an alternative view, we can expect the error induced by the sketching to scale as \(\), naturally implying that a larger \(k\) will have a larger error, and larger sketch sizes \(s\) will have lower error. Importantly, the sketch size \(s\) (and consequently the memory consumption and the error bound) depends only logarithmically on the number of parameters \(p\), while the memory-saving-ratio \(}{{p}}\) clearly improves a lot for bigger architectures.

Memory footprint.The memory footprint of our algorithm is at most \(4p+s(k+1)\) floating point numbers. Indeed, the srt sketch matrix uses \(p+s\) numbers to store \(S\), whereas low-memory Lanczos stores at most 3 size-\(p\) vectors at a time. Finally, \(U_{S}\) is a \(s k\) matrix. At query time, we only need to store \(S\) and \(U_{S}\), resulting in a memory footprint of \(p+s(k+1)\). Therefore, our method is significantly less memory-intensive than the methods of Madras et al. (2019); Sharma et al. (2021), and all other low-rank Laplace baselines, that use \((kp)\) memory.

Time footprint.The time requirement is comparable to Vanilla Lanczos. We need \(k\) srt sketch-vector products and each takes \((p p)\) time, while the orthogonalization of \(SV=U_{S}R\) through QR decomposition takes \((pk^{2})\) time. Both Vanilla and Sketched algorithm performs \(k\) ggn-vector products and each takes \((pn)\) time, where \(n\) is the size of the dataset, and that is expected to dominate the overall \((pk( p+k+n))\). Query time is also fast: sketching, Jacobian-vector product and \(U_{S}\)-vector product respectively add up to \((tp( p+1)+tsk)\). Note that the linear scaling with output dimension \(t\) can slow down inference for generative models. We refer to Immer et al. (2023) for the effect of considering a subset on dataset size \(n\) (or on output dimension \(t\)).

## 4 Related work

Modern deep neural networks tend to be more overconfident than their predecessors (Guo et al., 2017). This motivated intensive research on uncertainty estimation. Here we survey the most relevant work.

Perhaps, the simplest technique to estimate uncertainty over a classification task is to use the softmax probabilities output by the model (Hendrycks and Gimpel, 2016). A more sophisticated approach, combine softmax with temperature scaling (also Platt scaling) (Liang et al., 2017; Guo et al., 2017). The main benefit of these techniques is their simplicity: they do not require any computation besides inference. However, they do not extend to regression and do not make use of higher-order information. Moreover, this type of score relies on the extrapolation capabilities of neural networks since they use predictions made far away from the training data. Thus, their poor performance is not surprising.

To alleviate this issue, an immense family of methods has been deployed, all sharing the same common idea of using the predictions of _more than one_ model, either explicitly or implicitly. A complete review of these methods is unrealistic, but the most established includes Variational inference (Graves, 2011; Hinton and Van Camp, 1993; Liu and Wang, 2016), Deep ensembles (Lakshminarayanan et al., 2017), Monte Carlo dropout (Gal and Ghahramani, 2016; Kingma et al., 2015) and Bayes by Backprop (Blundell et al., 2015).

  & Memory & Time \\  Preprocessing & \(4p+s(k+1)\) & \((pk( p+k+n))\) \\ Query & \(p+s(k+1)\) & \((tp( p+1)+tsk)\) \\ 

Table 2: Recall that through the whole paper \(p\) is number of parameters, \(n\) is dataset size, \(t\) is output dimensionality, \(k\) is rank approximation and \(s\) is sketch size.

More closely related to us, the two uncertainty quantification scores introduced in Equation 2 with covariances \(M_{}\) and \(M_{}\) (Equation 3) have been already derived from Bayesian (with Laplace's approximation) and frequentist (with local perturbations) notions of underspecification, respectively:

Laplace's approximation.By interpreting the loss function as an unnormalized Bayesian log-posterior distribution over the model parameters and performing a second-order Taylor expansion, Laplace's approximation (MacKay, 1992) results in a Gaussian approximate posterior whose covariance matrix is the loss Hessian. The linearized Laplace approximation (Immer et al., 2021; Khan et al., 2019) further linearize \(f_{}\) at a chosen weight \(^{*}\), i.e. \(f_{}(x) f_{^{*}}(x)+_{^{*}}(x)(- ^{*})\). In this setting the posterior is exactly Gaussian and the covariance is exactly \(M_{}\).

Local perturbations.A different, frequentist, family of methods studies the change in optimal parameter values induced by change in observed data. A consequence is that the parameter directions corresponding to functional invariance on the training set are the best candidates for OoD detection. This was directly formalized by Madras et al. (2019) which derives the score with covariance \(M_{}\), which they call a local ensemble. A similar objective is approximated by Resampling Under Uncertainty (rue, Schulam and Saria (2019)) perturbing the training data via influence functions, and by Stochastic Weight Averaging Gaussian (swag, Maddox et al. (2019)) by following stochastic gradient descent trajectories.

Sketching in the deep neural networks literature.Sketching techniques are not new to the deep learning community. Indeed several works used randomized svd(Halko et al., 2011), which is a popular algorithm (Tropp and Webber, 2023) that leverages sketch matrices to reduce dimensionality and compute an approximate truncated svd faster and with fewer passes over the original matrix. It was used by Antoran et al. (2023) to compute a preconditioner for conjugate gradient and, similarly, by Mishkin et al. (2018) to extend Variational Online Gauss-Newton (vogn) training (Khan et al., 2018). More related to us, Sketching Curvature for OoD Detection (scod, Sharma et al. (2021)) uses Randomized svd to compute exactly the score in Equation 2 with \(M_{}\), thus serving as a baseline with an alternative to Lanczos. We compare to it more extensively in Appendix B.

_To the best of our knowledge_ no other work in uncertainty estimation for deep neural networks uses sketching directly to reduce the size of the data structure used for uncertainty estimation. Nonetheless, a recent line of work in numerical linear algebra studies how to apply sketching techniques to Krylov methods, like Lanczos or Arnoldi (Balabanov and Grigori, 2022; Timsit et al., 2023; Simoncini and Wang, 2024; Guttel and Schweitzer, 2023). We believe sketching is a promising technique to be applied in uncertainty estimation and, more broadly, in Bayesian deep learning. Indeed, all the techniques based on approximating the ggn could benefit from dimensionality reduction.

## 5 Experiments

With a focus on memory budget, we benchmark our method against a series of methods, models, and datasets. The code for both training and testing is implemented in jax(Bradbury et al., 2018) and it is publicly available3. Details, hyperparameters and more experiments can be found in Appendix D.

To evaluate the uncertainty score we measure the performance of out-of-distribution (OoD) detection and report the Area Under Receiver Operator Curve (AUROC). We choose **models** with increasing complexity and number of parameters: MLP, LeNet, ResNet, VisualAttentionNet and SwinTransformer architectures, with the number of parameters ranging from 15K to 200M. We train such models on 5 different **datasets**: Mnist(Lecun et al., 1998), FashionMnist(Xiao et al., 2017), Cifar-10(Krizhevsky et al., 2009), CelebA(Liu et al., 2015) and ImageNet(Deng et al., 2009). We test the score performance on a series of OoD datasets, including rotations and corruptions (Hendrycks, 2019) of ID datasets, as well as Svhn(Netzer et al., 2011) and Food101(Bossard et al., 2014). For CelebA and ImageNet we hold out some classes from training and use them as OoD datasets.

We compare to the most relevant **methods** in literature: Linearized Laplace Approximation with a low-rank structure (lla)(Immer et al., 2021) and Local Ensemble (le) are the most similar to us. We also consider Laplace with diagonal structure (lla-d) and Local Ensemble Hessian variant (le-h)(Madras et al., 2019). Another approach using randomized svd instead of Lanczos is Sketching Curvature for OoD Detection (scod)(Sharma et al., 2021), which also serves as a Lanczos baseline. Lastly, we include swag(Maddox et al., 2019) and Deep Ensemble (de)(Lakshminarayanan et al., 2017).

**Effect of different sketch sizes.** We expect the error induced by the sketching to scale as \(\), thus a larger \(k\) will have a larger error, and larger sketch sizes \(s\) will have a lower error, independently on number of parameters up to log-terms. On the other hand, a larger parameter count will have a better memory-saving-ratio \(}{{p}}\), leading to an advantage for bigger architectures as shown in Figure 3.

**Summary of the experiments.** For _most of_ the ID-OoD dataset pairs we tested, our Sketched Lanczos Uncertainty outperforms the baselines, as shown in Table 3 and more extensively in Figure 4 where we fix the memory budget to be \(3p\). Deep Ensemble performs very well in the small architecture but progressively deteriorates for bigger parameter sizes. swag outperforms the other methods on some specific choices of Cifar-10 corruptions, but we found this method to be extremely dependent on the choice of hyperparameters. scod is also a strong baseline in some settings, but we highlight that it requires instantiating the full Jacobian with an actual memory requirement of \(tp\). These memory requirements make scod inapplicable to ImageNet. In this setting, given the significant training time, also Deep Ensemble becomes not applicable.

The budget of \(3p\) is an arbitrary choice, but the results are consistent with different values. More experiments, including a \(10p\) memory budget setting, a study on the effect of preconditioning, and a synthetic-data ablation on the trade-off sketch size vs low rank, are presented in Appendix D.3.

## 6 Conclusion

We have introduced Sketched Lanczos, a powerful memory-efficient technique to compute approximate matrix eigendecompositions. We take a first step in exploiting this technique showing that sketching the top eigenvectors of the Generalized Gauss-Newton matrix leads to high-quality

Figure 3: Sketch sizes \(s\) comparison for: LeNet \(p=40\)K on FashionMnist vs Mnist (left), ResNet \(p=300\)K on Cifar-10 vs Cifar-corrupted with defocus blur (center), and VisualAttentionNet \(p=4\)M on Celeba vs Food101 (right). The lower the ratio \(}{{p}}\), the stronger the memory efficiency.

scalable uncertainty measure. We empirically show the superiority of the Sketched Lanczos Uncertainty score (slu) among a variety of baselines in the low-memory-budget setting, where the assumption is that the network has so many parameters that we can only store a few copies.

**Limitations.** The data structure produced by Sketched Lanczos is sufficiently rich to evaluate the predictive variance, and consequently the uncertainty score. However, from a Bayesian point of view, it is worth noting that the method does not allow us to sample according to the posterior.

Figure 4: AUROC scores of Sketched Lanczos Uncertainty vs baselines with memory budget \(3p\). slu outperforms the baselines on several choices of ID (4a, 4b, 4c, 4d, 4e) and OoD (x-axis) datasets pairs. Dashed lines are for improved visualization only; see Table 3 for values and standard deviations. Plots 4a, 4b, 4c, 4d, 4e are averaged respectively over 10, 10, 5, 3, 1 independently trained models.