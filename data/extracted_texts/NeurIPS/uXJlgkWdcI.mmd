# Ray T. Chen\({}^{\blacktriangledown}\), Jiaqi Gu\({}^{\blacktriangledown}\), David Z. Pan\({}^{\blacktriangledown}\)\({}^{\dagger}\)

PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices

 Hanqing Zhu\({}^{}\)\({}^{}\), Wenyan Cong\({}^{}\), Guojin Chen\({}^{}\);Shupeng Ning\({}^{}\),

**Ray T. Chen\({}^{}\), Jiaqi Gu\({}^{}\), David Z. Pan\({}^{}\)\({}^{}\)**

\({}^{}\)Arizona State University

The University of Texas at Austin

\({}^{}\)hqzhu@utexas.edu, \({}^{}\)dpan@ece.utexas.edu

This work was done when Guojin Chen was a visiting scholar at UT Austin.

###### Abstract

Electromagnetic field simulation is central to designing, optimizing, and validating photonic devices and circuits. However, costly computation associated with numerical simulation poses a significant bottleneck, hindering scalability and turnaround time in the photonic circuit design process. Neural operators offer a promising alternative, but existing SOTA approaches,NeurDLight, struggle with predicting high-fidelity fields for real-world _complicated_ photonic devices, with the best reported 0.38 normalized mean absolute error in NeurDLight. The interplay of highly complex light-matter interaction, e.g., scattering and resonance, sensitivity to local structure details, non-uniform learning complexity for full-domain simulation, and rich frequency information, contribute to the failure of existing neural PDE solvers. In this work, we boost the prediction fidelity to an unprecedented level for simulating complex photonic devices with a novel operator design driven by the above challenges. We propose a novel cross-axis factorized PACE operator with a strong long-distance modeling capacity to connect the full-domain complex field pattern with local device structures. Inspired by human learning, we further divide and conquer the simulation task for extremely hard cases into two progressively easy tasks, with a first-stage model learning an initial solution refined by a second model. On various _complicated_ photonic device benchmarks, we demonstrate one sole PACE model is capable of achieving **73%** lower error with **50%** fewer parameters compared with various recent ML for PDE solvers. The two-stage setup further advances high-fidelity simulation for even more intricate cases. In terms of runtime, PACE demonstrates **154-577\(\)** and **11.8-12\(\)** simulation speedup over numerical solver using scipy or highly-optimized pardiso solver, respectively. **We open sourced the code and _complicated_ optical device dataset at PACE-Light.**

## 1 Introduction

With advances in integrated photonics, photonic structures capable of transmitting or processing information are gathering increasing interest, fueled by the optical communication  and the recent resurgence of photonic analog computing . Light-empowered communication and computing offer a promising pathway for reshaping future AI systems, prompting the optical community to discover compact, customized devices  to overcome the limitations of bulky optical components. In this optical design process, numerical simulators, e.g., the popular finite difference frequency domain (FDFD) algorithm , is heavily used to obtain accurate optical fields for characterizing and optimizing device behavior. However, the significant time and computational costs associated with Maxwell partial differential equation (PDE) simulations, exacerbated by the need for finely tailored meshes and numerous simulation runs for iterative optimization, pose substantial bottlenecks in the design loop.

Recently, neural PDE solvers [3; 8; 9; 15; 16; 27; 33] have emerged as promising surrogate models for _fast_ and _accurate_ PDE solving. NeurOLight represents the state-of-the-art (SOTA), extending neural operators to parametric photonic device simulations in a physics-agnostic manner. However, it still exhibits large errors in simulating real-world _complicated_ optical devices, reporting a 0.38 normalized mean absolute error on the etched multi-mode interference (MMI) device . One may wonder what the major challenges are, given the successes of neural operators in many scientific PDEs. Firstly, for _complicated_ devices, the permittivity distribution is discrete and highly contrasting, transforming the Maxwell PDE into a multi-scale problem , further leading to complex light-matter interactions such as scattering and resonance, as illustrated in Fig. 1(a). Secondly, their optical fields are highly sensitive to local structural changes; even minor alterations can significantly impact the field, as depicted in Fig. 1(b). Moreover, with diversifying field patterns along the light propagation path, it shows non-uniform learning complexity especially in regions distant from the input light source. Finally, a spectral analysis provides insights into the frequency-domain challenges, as illustrated in Fig. 1(d). Unlike simpler systems where low frequencies dominate (e.g., Darcy flow shown in Fig. 10), _complicated_ devices exhibit rich frequency spectra with high-frequency components. This diversity underpins the difficulty faced by previous neural PDE solvers in accurately simulating _complicated_ photonic devices, supporting the assertion in  that no single model can universally solve all types of PDEs.

In this work, we tackle the challenging _real-world complicated_ optical device simulation problem. We vastly boost prediction fidelity and keep **154-577\(\)** and **11.8-12\(\)** speedup over traditional numerical solver  on a 20-core CPU with scipy or highly-optimized pardiso solver, respectively.

Overall, we make the following key contributions:

* We introduce a novel cross-axis factorized PACE operator backbone, effectively capturing complex physical phenomena across the full domain in a parameter-efficient manner.
* We employ a divide-and-conquer approach inspired by human learning for extremely challenging cases, with a first-stage PACE-I to learn a rough approximation of the optical field, refined by a second-stage PACE-II.
* On various _complicated_ device benchmarks, one sole PACE significantly outperforms baselines, achieving **73%** lower error with **50%** fewer parameters. Even compared to the best baseline, it lowers prediction error by over **39%** with **17%** fewer parameters. Our two-stage method further advances high-fidelity simulation for extremely challenging cases.
* We open-source the _complicated_ optical device datatset and code at PACE-Light to facilitate AI for PDE community.

## 2 Preliminaries

### Neural Operators for PDE

Recently, neural operators have emerged as a novel approach for developing machine learning models aimed at solving partial differential equations (PDEs). These models focus on learning

Figure 1: Challenges of complicated optical device simulation: (a-d) and learning framework (e).

the mapping between the function spaces in a purely data-driven fashion. This holds the generalization capability within a family of PDEs and can potentially be adapted to different discretizations. Various function bases are utilized to build the operator learning model, such as the Fourier bases [16; 29; 2; 8], wavelet bases , spectral method , and attention layer [15; 3; 14]. These models have demonstrated remarkable performance and efficiency in solving specific types of problems, often achieving record-breaking results in certain applications. Despite their successes, it's important to recognize that the field of PDEs encompasses a wide variety of equations, each with its own unique properties and characteristics. As pointed out in recent research , there is no guarantee that a single type of data-driven model can effectively address all types of PDEs.

### Optical Field Simulation with Machine Learning

Analyzing the propagation of light through optical devices is crucial for the optimization and design of photonic circuits. For a linear isotropic optical device, with a time-harmonic continuous-wave light beam shining on its input port, we can obtain the steady-state electromagnetic field distributions \(()=}}_{x}+}}_{y}+}}_{z}\) and \(()=}}_{x}+}}_{y}+}}_{z}\) by solving the steady-state frequency-domain _curl-of-curl_ Maxwell PDE under absorptive boundary conditions ,

\[(_{0}^{-1})-^{2}_{0}_{ r}()()=j_{}(),\ (_{r}^{-1}())-^{2}_{0} _{0}()=j_{m}()\] (1)

where \(\) is the curl operator, \(_{0}\) is the vacuum magnetic permeability, \(_{0}\) is the vacuum electric permittivity, \(_{r}\) is the relative electric permittivity, and \(_{m}\) and \(_{e}\) are the magnetic and electric current sources, respectively. The finite difference frequency domain (FDFD) method, a widely adopted numerical technique detailed in , is used to discretize these continuous-domain equations into an \(M N\) mesh grid. This transforms the Maxwell PDEs into a linear system \(=\). Solving this system with a large sparse matrix \(^{MN MN}\) is computationally expensive and challenging to scale. Although improvements have been made, such as replacing the scipy solver with the more efficient pardiso solver, the process remains prohibitively costly for large-scale applications.

Building neural networks (NNs) to accelerate this time-consuming simulation process has been investigated in predicting some key design parameters  or the entire optical field [30; 17; 4; 8]. NeurOLight extends the neural operator to optical field simulation, enabling learning a physics-agnostic parametric Maxwell PDE solver and achieving SOTA accuracy, while its performance on real-world _complicated_ photonic device is still not satisfactory.

## 3 Understand the Problem Setup and Challenge

In this study, we aim to build a physics-agnostic neural operator \(_{}\) for parametric photonic device simulation in a data-driven fashion to approximate the ground-truth Maxwell PDE solver \(^{*}:\) described in Eq. (1). Here, \(\) represents the solution space for the optical field in \(^{ d_{u}}\) and \(=(,_{r},,)\) represents the observation space of the Maxwell PDE, both defined over the continuous 2-D physical solving domain \(=(l_{x},l_{z})\). We follow NeurOLight to discretize the simulation domain \(\) as \(=(M,N, l_{x}, l_{z})\) with adaptive mesh granularity, i.e., with grid steps \( l_{x}=l_{x}/M\) and \( l_{z}=l_{z}/N\). Moreover, \((,_{r},)\) in the raw observation \(\) is encoded as informative wave priors, \(_{z}=e^{j}}{}z^{T}  l_{z}}\) and \(_{x}=e^{j}}{}^{T}  l_{x}}\), where \(x=(0,1,,M-1)\) and \(z=(0,1,,N-1)\), reflecting the propagation behaviors of light through different media. The input light source \(\) is further modeled as a masked light source field \(_{y}^{J}\).

Therefore, as illustrated in Fig. 1(e), the overarching objective is formulated as learning operator \(_{}\) that maps \(^{}=(_{r},_{y}^{J},_{x}, _{z})\) to the target field \(\) by optimizing the empirical error,

\[^{*}=_{}_{^{}} _{}(a),u,\] (2)

### Challenges in Predicting the Light Field of _Complicated_ Photonic Devices

NeurOLight delivers a pioneering effort in extending neural operators to the simulation of photonic devices, achieving SOTA accuracy. However, it still yields significant errors, particularly for real-world complicated devices, with a reported 0.38 normalized mean absolute error for etched MMI device [26; 12]. This leads us to an interesting reflection: despite the successes of neural operators in solving scientific PDEs, why do they still fall short in _complicated_ photonic device simulation? Below, we provide a detailed analysis that highlights the underlying learning challenges.

* **Complicated light-matter interaction in the optical field of real-world photonic device**. Permittivity \(_{r}\), a critical parameter in photonic devices, greatly impacts how light propagates through media. Designing new devices often involves manipulating the \(_{r}\) distribution across the domain. However, due to manufacturing limitations, \(_{r}\) changes are discrete rather than smooth. Moreover, researchers explore patterning materials with highly contrast permittivity to design compact devices [26; 31]. This discrete and highly contrasting permittivity transforms the Maxwell PDE into a _multiscale PDE problem_, with complicated light-matter interactions such as scattering resonance happening, shown in Fig. 1 (a), which has been shown difficult to predict from both scientific computing and operator learning perspectives [21; 35].
* **Significant prediction field variations from minor structural changes**. Due to the complex light-matter interactions within the field, even a slight change in the photonic structure can result in drastically different optical fields under the same input conditions, as shown in Fig. 1(b). This calls for a powerful backbone model that is capable of building the relationship between local rival changes with the global optical field transition.
* **Non-uniform learning difficulty along the spatial domain**. As shown in Fig. 1(c), with light shining in from a specific position and direction, it propagates through the media, resulting in non-uniform learning difficulties along the spatial domain. Due to the vast diversity of potential internal structures along the light propagation path, the light patterns are becoming highly diverse. Consequently, the data collected for training also incorporates the same phenomenon where many similar patterns are seen during training near the input sources, whereas the model faces more diverse patterns at greater distances. This makes it hard for the model to learn how to predict further regions, especially when the domain is elongated. This issue is analogous to the roll-out error encountered in temporal PDE modeling at the large time steps.
* **Rich frequency information lies in the predicted field**. We show the energy spectrum of the optical field in the frequency domain in Fig. 1 (d). The field, characterized by complex interactions such as scattering and resonance, exhibits rich frequency information, unveiling the learning complexity from a frequency-domain analysis. This confirms the usage of high-frequency modes in NeurOLight, underscoring the need for a parameter-efficient, robust, and powerful backbone model to resolve the parameter efficiency and overfitting issue with large modes.

## 4 Proposed Pace Methods

In this paper, we follow the standard operator learning model architecture as

\[a^{}() v_{0}() v_{1}() v_{K}()  u(),\ \ .\] (3)

We start with the convolutional stem used in  to project the PDE observation \(a^{}()\) into a higher-dimensional feature space of dimension \(C\). This is followed by a sequence of \(K\) cascaded neural operator blocks, which gradually reconstruct the complex optical field within the \(C\) dimensional space. At last, a head with two point-wise convolutional layers projects the \(v_{K}()\) to the optical field space \(u()\). Fig. 2(a) shows the proposed PACE neural operator block structure, formulated as,

\[v_{k+1}():=(^{{}^{}}})( )+v_{k}+v_{k},\,;\,v_{k}^{{}^{}}( )=(v_{k}()),\] (4)

where \(\) is the our proposed PACE operator and \(()\) is a feedforward network used in . To stabilize the model performance when scaling to deeper layers, we add pre-normalization  and follow  to add a double skip. In this work, we consistently use the NeurOLight operator in the first two blocks to align our model with the horizontal and vertical wave prior encoding method adopted from NeurOLight, which we found slightly improves our accuracy.

### Parameter-efficient and Effective Cross-axis Factorized Pace Operator

The neural operator design is key to obtaining satisfactory accuracy on a given PDE task. With the well-discussed challenges in Sec. 3.1, we derive key insights that have guided the development of our PACE operator in Fig. 2(b): (1) Long-distance full-domain modeling capacity, especially effectively modeling how local features impact the whole domain; (2) Isotropic model architecture with no down-sampling/ patching without losing local details; (3) Parameter efficiency under the needs of capturing high-frequency features.

Given the isotropic requirements, an operator based on Fourier bases is an ideal candidate as it achieves full-domain attention in the \(O(nlogn)\) time complexity. However, the rich frequency information lying in the optical field requires the use of large frequency modes, making the FNO  with huge parameters and severe overfitting issues. NeurOLight and Factorized FNO  propose to decompose the FNO block with independent 1-D FNO blocks in the full \(N\)-dimensional domain \(\) (see Fig. 3), therefore, solving the parameter concern when utilizing high-frequency modes and serving as a regularization for overfitting. The only difference between NeurOLight and Factorized FNO  is whether they chunk the input or copy the input to the independent 1-D FNO block. We argue that their theoretical success is attributed to the _implicit full-domain integration_ in Corollary 4.1.

**Corollary 4.1**.: _The factorized Fourier integral operator \(\) factorizes the original Fourier integral operator  along each dimension \(n\) in the N-dimension domain \(\),_

\[(v_{k})(_{1})=_{n}^{N}_{n}^{-1}(_{ n}(_{}^{n})_{n}(_{2}))(_{1}),\ \ \ _{1},\] (5)

_where each item explicitly computes a 1-D kernel integral, \(_{_{n}}(_{1},_{2})^{n}v_{k}(_{2})^{n}dv_{k} (_{2})^{n}\). It implicitly implements full-domain kernel integration in \(\) by stacking \(\), i.e., \(_{0}_{1}\),_

However, the reliance on implementing full-domain integration with multi-layers makes them _weak_ operator candidates to achieve our first requirement, i.e., a _strong_ model that is capable of building _full-domain_ modeling between local structures with the global fields.

**Proposed cross-axis 2-D factorized integral kernel**. Aware of the above shortcomings of previous factorized FNO variants, in our 2-D domain, we propose to factorize the full domain integral in a cross-axis way along the horizontal (h) and vertical (v) axis:

\[(v_{k})(_{1})&=_{ }(_{1},_{2})v_{k}(_{2})v_{k}(_{2}),\ \ _{1},\\ &_{_{h}}(_{1},_{2})^{h}_{ _{h}}(_{1},_{2})^{v}v_{k}(_{2})v_{k}( _{2})^{v}v_{k}(_{2})^{h},\ \ \ _{1}.\] (6)

This factorization enables an _explicit factorized full-domain integration_. It provides a _strong_ way to capture the relationship between points in the domain \(\), building the relationship between local structure with the complicated field pattern. The implementation of the above cross-axis integral can be efficiently implemented by Fourier Transform \(()\) when the kernel \((r_{1},r_{2})=(r_{1}-r_{2})\), as follows,

\[(v_{k})(_{1})=_{h}^{-1}(_{h}(^{h} )_{h}(_{z}^{-1}(_{v}(^{v}) _{z}(_{2})))(_{1}),\ \ \ _{1},\] (7)

in a \(nlogn\) complexity (\(n=MN\) in our 2-D cases with \(^{M N}\)).

**Group-wise cross-axis integration**. For input \(\) with a channel dimension \(C\), it can be viewed as the sampling of a set of functions \(\{r_{l}(,)\}_{l=1}^{C}\) on grid point in the 2-D discretized domain

Figure 3: Factorized FNO .

Figure 2: (a) PACE block with double skip and pre-normalization; (b) Our cross-axis factorized PACE operator.

\(_{v}\). The learnable integral kernel intrinsically performs information exchange along different grid points in \(\). Similar to the multi-head design in Transformer, which assumes different heads extract different information, we can also partition the \(C\) basis functions into \(g\) disjoint sub-groups and feed each sub-group through our cross-axis factorized kernel. This grouping further reduces the number of parameters to \((_{h}+_{v})C_{i}}{g}\), showing significant parameter reduction compared to FNO (\(_{h}_{v} C_{o}C_{i}\)) and Factorized FNO (\((_{h}+_{v}) C_{o}C_{i}\)), showing excellent parameter efficiency when utilizing large frequency modes is a must. We do an ablation study in Appendix A.4 to investigate the choices of different group \(g\), where we find \(g=4\) strikes the best between parameter efficiency and model performance.

**Explicit projection unit \(\) for extracting high frequency information**. The optical field shows rich information in the frequency spectrum, reciting a special care of high-frequency information. Besides utilizing high-frequency modes, we propose to add an explicit projection module before the cross-axis integral, which is very simple as one linear layer followed by a non-linear activation, given non-linear activation is known to help generate high-frequency features .

**Self-weighted path for enhanced instance-based local feature attention**. The optical field's response is intricately linked to the minute variations in different photonic device structures. A self-weighted path is introduced to ensure the model can pay different attention to regions of significant influences for varying device structures. An instance-based weight is generated by passing the feature map after the projection unit through a linear layer and a Sigmoid unit, and then multiplied with the results after the cross-axis integral unit to provide instance-based attention.

Overall, the above ingredients are assembled together as our proposed PACE operator, as shown in Fig. 2 (b), which implements a self-weighted 2-D cross-axis factorized integral transform.

### Cascaded Learning from Rough to Clear

With the effective PACE operator design, the prediction fidelity can be largely improved by only using a 12-layer PACE model (see Section. 5.2.1). But for some complicated benchmarks (e.g., etched MMI 3x3/5x5), it still yields \( 10\%\) mean squared error, which is not satisfying. A straightforward solution might involve scaling up the model size, expecting additional layers would enhance performance. However, as demonstrated in , scaling to deep layers shows saturated performance after exceeding a specific number.

Existing ML for PDE solving work typically learns a model in a one-shot way by directly learning the underlying relationship from input-output pairs. Unlike AI systems, humans don't learn new and difficult tasks in a one-shot manner; instead, they learn skills progressively, starting with easier tasks and gradually moving to harder ones. For example, instead of directly learning how to solve equations, students first learn basic operations, such as addition and multiplication, and then move on to solving complex equations.

Hence, inspired by this human learning process, unlike previous work that directly learns a one-stage model, we propose to divide the challenging optical field prediction problem into two sequential latent tasks. The first task, undergoing the same problem setup as discussed in Sec. 3, could predict an initial, rough optical field based on the less informative raw PDE observation (we only have the light source and device permittivity distribution). Then, the successive second task could refine the rough prediction further by capturing more details and nuances, by accepting the predicted field \(_{_{1}}\) and

Figure 4: The proposed cascaded learning flow with two stages. The first stage learns an initial and rough solution, followed by the second stage to revise it further. A cross-stage distillation path is used to transfer the learned knowledge from the first stage to the second stage.

device permittivity \(_{r}\) as the input. Therefore, we assign _higher Fourier modes_ to enable sufficient capacity. The divide-and-conquer way results in a cascaded two-stage model architecture, as shown in Fig. 4. The cascaded learning model is trained jointly (PACE-I + PACE-II) with the optimization target as the sum of two losses \((_{ 1}(a),u)+(_{ 2}(_{ 1}(a), _{r}),u)\), where the first \((_{ 1}(a),u)\) serves as intermediate supervision that enforces the first stage model condensate the learned knowledge. To better connect the two-stage model, we propose a _cross-stage feature distillation path_ to distill learned feature from the previous stage to the last by using a simple Linear\(\)Sigmoid path.

## 5 Experimental Results

### Experimental Setup

**Benchmarks:** We evaluate our methods on real-world _complicated_ photonic devices that pose significant simulation challenges for ML surrogate models. This includes the Etched MMI with randomly placed rectangular cavities, used in , and the metaline device [37; 19] featuring two layers of randomly dimensioned meta-atoms. **These devices present a highly discrete and contrast permittivity distribution and complex light-matter interactions, making them ideal for testing the effectiveness of our model.** We generate our datasets using the open-source 2-D FDFD simulator, Angler , with generation details in Appendix A.1.

**Baselines**: We evaluate the proposed PACE model against a range of baselines, including the SOTA neural operator work, NeurOLight, for optical simulation. We also include representative operator learning models for scientific PDEs based on Fourier bases(FNO , Factorized FNO (F-FNO) [28; 29], U-NO , tensorized FNO (TFNO) ), attention kernels , and the latent spectral method (LSM) . We also incorporate UNet [17; 4] and Dilated ResNet (Dil-ResNet) . For a fair comparison, we keep a model size budget of under/near 4 million (M) parameters for baselines, except LSM  where the original implementation is adopted. Details on model configurations are in the Appendix A.3.

**Training setting and metric**: All models undergo training for 100 epochs using the AdamW optimizer with a weight decay of \(1e^{-5}\) in a batch size of 4. To balance the optimization among different fields, we use normalized mean squared error (N-MSE) as the learning objective,

\[(_{}(a),^{*}(a))=(\|_{}((a))- ^{*}(a)\|^{2})/\|^{*}(a)\|^{2}.\] (8)

We don't use the previously-used mean absolute error (MAE)  as the metric given for complex-valued optical fields; we argue that L2 distance is a more accurate metric to evaluate the distance in the complex plane with a detailed analysis in Appendix A.6. We adopt the superposition-based mix-up technique  to generate input light combinations randomly to augment training data.

### Main Results

#### 5.2.1 Prediction Quality of Single PACE Model

In Tab. 1, we compare our 12-layer PACE model with various baselines on multiple real-world device benchmarks, showing significant **73.85%** smaller test error with **51.67%** fewer parameters on average. Notably, even when compared to the _best_ baseline, 16-layer NeurOLight, we show over **39%** lower test error with over **17%** fewer parameters. Given the challenge **6** that _trial structure change can totally change the optical field_, model relying on downsampling or patching fails to capture the local details, confirming the failure of the UNet and Transformer model. Moreover, the challenge **6** and challenge **6** call for a powerful model with long-distance modeling capability. Although Dil-ResNet utilizes a dilated block to enlarge the receptive field, it is insufficient for a large domain, validated by the result that it shows much better accuracy on the small Metaline than the etched MMI3x3. Capturing long-range dependency with the Fourier operator provides an efficient way to the isotropic model without any downsampling, therefore making the Fourier-operator type model show consistently better accuracy than other baseline methods. However, due to the challenge **6** that there is rich frequency information in the predicted field, FNO-2d falls short due to the impediment of utilizing large modes given the large parameter count. We also compared it with the tensorized FNO 2d. However, we find the general tensor decomposition hurt the accuracy of this challenging task. NeurOLight shares a similar insight of Factorized FNO by factoring Fourier kernel with several independent 1-D Fourier kernels; however, as we argued before, it fails to establish a strong full-domain modeling capacity by linking local details to the global complex field. Overall, our PACE block benefits from a physically meaningful cross-axis Fourier kernel factorization, equipping the capacity to capture full-domain dependency in a parameter-efficient way. Visualization of predicted results is in Appendix A.10.

#### 5.2.2 Quality Improvement with Two-stage Model

We further compare the proposed cascaded two-stage model with the common practice of solely increasing # layers. We set the PACE-I as a 12-layer PACE model with Fourier modes(#Mode =70, #Mode =40), and PACE-II as a 8-layer PACE model with larger Fourier modes (#Mode =100, #Mode =40). As shown in Tab. 2, the two-stage setup introduces slight overhead for one extra set of stem and head but shows a clear margin over only increasing the number of layers in terms of both train error and test error. The cross-stage feature distillation further provides meaningful guidance by transferring learned features to the second-stage model, leading to the best accuracy for the two-stage setup. In Appendix A.7, we also show that the cross-stage distillation trick can improve model accuracy, similar to a more costly training setup, by training the two-stage models sequentially.

  Benchmarks & Model & \#Params (M) \(\) & Train Err (\(10^{-2}\)) \(\) & Test Err (\(10^{-2}\)) \(\) \\   & UNet [17; 4] & 3.88 & 63.03 & 65.32 \\  & Dil-ResNet  & 4.17 & 51.34 & 51.79 \\  & Attention-based model  & 3.75 & 70.05 & 69.85 \\  & U-NO  & 4.38 & 34.22 & 42.86 \\  & Latent-spectral method  & 4.81 & 55.07 & 55.16 \\  & FNO-2d  & 3.99 & 32.51 & 38.71 \\  & Tensorized FNO-2d  & 2.25 & 35.52 & 36.61 \\  & Factorized FNO-2d  & 4.02 & 24.2 & 32.81 \\  & NeuroLigt  & 2.11 & 15.58 & 17.21 \\  & **PACE** & **1.71** & **9.51** & **10.59** \\   & UNet [17; 4] & 3.88 & 65.73 & 66.01 \\  & Attention-based model  & 3.75 & 74.16 & 74.20 \\  & U-NO  & 4.38 & 37.92 & 42.24 \\  & Latent-spectral method  & 4.81 & 53.9 & 54.01 \\  & FNO-2d  & 3.99 & 33.12 & 36.49 \\  & Tensorized FNO-2d  & 2.25 & 39.11 & 39.45 \\  & Factorized FNO-2d  & 4.02 & 22.18 & 26.06 \\  & NeuroLigth  & 2.11 & 18.04 & 17.41 \\  & **PACE** & **1.71** & **11.66** & **11.91** \\   & UNet [17; 4] & 3.88 & 39.12 & 39.61 \\  & Dil-ResNet  & 4.17 & 12.37 & 13.20 \\   & Attention-based model  & 3.75 & 63.99 & 64.10 \\   & U-NO  & 4.38 & 19.27 & 22.09 \\   & Latent-spectral method  & 4.81 & 31.60 & 31.94 \\   & Fusion-2d  & 3.21 & 19.73 & 20.88 \\   & Tensorized FNO-2d  & 1.58 & 30.60 & 31.04 \\   & Factorized FNO-2d  & 2.68 & 8.51 & 9.28 \\   & NeuroLigt  & 1.49 & 6.76 & 6.09 \\   & **PACE** & **1.24** & **3.32** & **2.82** \\   & **-17.70\%** & **-41.23\%** & **-39.03\%** \\  & **-51.67\%** & **-72.57\%** & **-73.85\%** \\  

Table 1: Comparison of # parameters, training error (last epoch), and test error on three benchmarks among our PACE and various baselines. We use geo-means to report overall improvements across different benchmarks.

  Benchmarks & Model & Cross-stage dist. & \#Params (M) \(\) & Train Err (\(10^{-2}\)) \(\) & Test Err (\(10^{-2}\)) \(\) \\   & PACE-12 layer & - & 1.73 & 9.51 & 10.59 \\   & PACE-20 layer & - & 3.135 & 6.46 & 7.04 \\  & PACE-I + PACE-II & ✗ & 3.151 & 4.66 & 5.83 \\  & PACE-I + PACE-II & ✗ & 3.151 & 4.14 & 5.32 \\   & PACE-12 layer & - & 1.73 & 11.66 & 11.91 \\   & PACE-20 layer & - & 3.135 & 7.74 & 7.88 \\   & PACE-I + PACE-II & ✗ & 3.151 & 6.17 & 6.78 \\   & PACE-I + PACE-II & ✗ & 3.151 & 5.43 & 6.15 \\  

Table 2: Comparison between our two-stage model and simply scaling more layers. All models use the same Fourier modes setup.

#### 5.2.3 Speedup over Numerical Tools

To develop a fast surrogate ML model that can replace the Maxwell PDE solver, it's crucial to evaluate the speed-up of our PACE model compared to the FDFD numerical simulator Angler . We vary the simulation domain size and set the grid step to 0.05 nm, scaling the discretized size pardiso linear solvers, respectively and number of frequency modes to ensure the model has sufficient capacity to capture the entire simulation domain. For comparison, we employ a 20-layer joint PACE model. As shown in Fig. 5, our PACE model achieves a speed-up of 150-577\(\) and 12\(\) over Angler on a 20-core Intel i7-12700 CPU using the scipy and We further set a larger simulation granularity, 0.075 nm, to check speedup if we tolerate simulation quality loss in commercial tools. However, we find that setting a larger granularity results in a significantly different field, as qualitatively shown in reb-Fig.3, with a corresponding N-MSE error of 1.2. Even though in this case, PACE still shows a 5.1-10.6\(\) speedup over pardiso-based Angler with much better fidelity.

### Discussion

**Cross-axis** PACE **block design choices**. In Tab. 3, we _independently_ alter individual components within the PACE operator to assess their effectiveness. The self-weighted path, which provides instance-specific weights, significantly improves model accuracy across various photonic device patterns. Removing this component results in a 17% increase in error, highlighting its importance. Similarly, eliminating the high-frequency projection unit leads to a 23% worse error, emphasizing its crucial role in capturing high-frequency features. To further illustrate this, we visualize the feature maps in the frequency domain before and after applying the nonlinear activation in the high-frequency projection unit. As shown in Fig. 11, the nonlinear activation effectively amplifies high-frequency components, supporting our claim and validating the design decision to incorporate an additional high-frequency projection path. Lastly, we replace our cross-axis Factorized integral kernel with a recent tensorized FNO (TFNO)  (tucker decomposition with rank 0.02). While TFNO effectively models long-range dependencies, matching our parameter count required aggressive decomposition, which significantly degraded performance. This comparison underscores the advantage of our physically grounded _cross-axis factorized kernel_.

**Generalization to out-of-distribution testing**. As an operator model that is parameter-agnostic, it is important to test the generalization for out-of-distribution data with unseen parameters. We re-generate photonic devices with different device configurations (size, etched region, etc.) and unseen frequencies in our interested wavelength range (1.53-1.565 \(\)m), i.e., C-band. As shown in Fig. 6, our PACE model generalizes well on unseen simulation frequency and new devices. It is a vital test to prove the usefulness of PACE in helping device design within an interested wavelength range. We also test the accuracy outside the C-band, where PACE shows good accuracy on neighboring wavelengths while holding a 10-15% error at a further range. This is expected since wave propagation is sensitive to frequency. It can be mitigated by incorporating sampled wavelengths into training.

Figure 5: Speedup of PACE over angler  using scipy (S)/ pardiso (P) with simulation granularity (0.05nm) and (0.075nm).

   Variants & \#Params & \#Train Err & \#Test Err \\  & (M)\(\) & (\(10^{-2}\))\(\) & (\(10^{-2}\))\(\) \\ 
**8-layer** PACE & **0.82** & **5.65** & **4.82** \\  No self-weighted path & 0.8 & 6.33 & 5.66 (+0.84) \\ No projection unit & 0.8 & 6.58 & 5.97 (+1.15) \\ Use TFNO & 1.06 & 10.80 & 9.51 (+4.69) \\   

Table 3: Model design ablation on Metaline dataset.

Figure 6: Generalize to unseen wavelength in interested C-band (1.53-1.565) and outside C-band.

**Are**\(\) **a general enhancer module for Fourier-type operator?** We further investigate whether our new \(\) operator is a general enhancer for other Fourier operators, rather than a dedicated module for our own model architecture. We randomly insert four \(\) blocks into \(\) FNO  and test the error on Metaline3x3 and \(\) MMI 3x3 benchmarks, showing up to 28% error reduction as shown in Fig. 7 with much fewer parameters.

**Comparison with operator for multi-scale PDE**. Noticing that our problem shares similar complexities in solving multi-scale PDEs with neural operator [18; 35], we further compare our approach with the recent method  that alternates Fourier operator with dilated convolution layer to better capture local details. On the etched MMI 3x3 dataset, we implement a 14-layer model with alternating \(\) block and dilated convolution layer. It yields a \(1.73\) M parameter count similar to our \(\) but shows a \(17.4\) N-MSE error, much worse than ours (10.59).

**Spectrum of the predicted field**: The predicted field spectrums of \(\) and \(\) are in Fig. 8. Although \(\) uses the same frequency modes, it fails to align well with both the low-frequency and high-frequency regions. \(\) excellently aligns with the baseline spectrum compared to \(\),

## 6 Conclusion

In this work, we _pace_ the simulation fidelity on highly challenging _complicated_ photonic devices to an unprecedented level. Our novel cross-axis factorized \(\) operator enables the neural PDE solver to capture complex relationships between local device structures and the resulting complex optical field across the entire simulation domain. Furthermore, we introduce a cascaded two-stage learning paradigm to further enhance the prediction quality when one sole \(\) is not sufficient, demonstrating better quality enhancement than simply adding more layers. Experiments demonstrate that \(\) achieves a remarkable 73% reduction in error with 50% fewer parameters compared to previous methods. Our method also offers significant speedup (11.8x to 577x) over traditional numerical solvers. Looking forward, we aim to integrate our model into the design optimization loop for photonic devices and circuits. Moreover, we want to emphasize that our proposed operator and learning strategy are not dedicated to photonic cases but generally applied to challenging PDE problems with similar problem characteristics, e.g., multi-scale PDE problems.

Limitations and Broader Impact.This work focuses on steady-state optical field solutions using the FDFD method. Exploring the effectiveness of operator learning for the Finite-Difference Time Domain (FDTD) can be an interesting direction. Moreover, the FFT kernels on GPU are not fully optimized . Employing specialized, optimized FFT kernels can unlock even greater computational efficiency on GPUs, further accelerating the neural PDE solver.

## 7 Acknowledgments and Disclosure of Funding

We acknowledge NVIDIA for donating its A100 GPU workstations and the support from TILOS, NSF-funded National Artificial Intelligence Research Institute. Additionally, this work was supported by the Air Force Office of Scientific Research (AFOSR) through the AFOSR project, contract FA9550-23-1-0452, and the Multidisciplinary University Research Initiative (MURI) program under contract No. FA9550-17-1-0071.

Figure 8: The radial energy spectrum of predicted fields from \(\) and \(\). \(\) fails to align precisely with the targeted field in both low-frequency and high-frequency parts.