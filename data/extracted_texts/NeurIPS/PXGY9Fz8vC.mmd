# Who's Gaming the System? A Causally-Motivated Approach for Detecting Strategic Adaptation

Trenton Chang\({}^{1}\)  Lindsay Warrenburg\({}^{2}\)  Sae-Hwan Park\({}^{2}\)  Ravi B. Parikh\({}^{2,3}\)  Magge Makar\({}^{1}\)  Jenna Wiens\({}^{1}\)

\({}^{1}\)University of Michigan \({}^{2}\)University of Pennsylvania \({}^{3}\)Emory University {{trenton,mmakar,wiensj}@umich.edu

{lindsay.warrenburg,sae-hwan.park}@penmmedicine.upenn.edu

ravi.bharat.parikh@emory.edu

###### Abstract

In many settings, machine learning models may be used to inform decisions that impact individuals or entities who interact with the model. Such entities, or _agents_, may _game_ model decisions by manipulating their inputs to the model to obtain better outcomes and maximize some utility. We consider a multi-agent setting where the goal is to identify the "worst offenders:" agents that are gaming most aggressively. However, identifying such agents is difficult without being able to evaluate their utility function. Thus, we introduce a framework featuring a gaming deterrence parameter, a scalar that quantifies an agent's (un)willingness to game. We show that this gaming parameter is only partially identifiable. By recasting the problem as a causal effect estimation problem where different agents represent different "treatments," we prove that a ranking of all agents by their gaming parameters is identifiable. We present empirical results in a synthetic data study validating the usage of causal effect estimation for gaming detection and show in a case study of diagnosis coding behavior in the U.S. that our approach highlights features associated with gaming.

## 1 Introduction

Machine learning (ML) models often guide decisions that impact individuals or entities. Attributes describing an individual or entity are often inputs to such models. In response, such entities may modify their attributes to obtain a more desirable outcome. But changing one's attributes may be costly due to the difficulty of generating supporting evidence, or penalties for fraud. This behavior is called _gaming_ or _strategic adaptation_. Strategic adaptation frames gaming as "utility maximization:" _agents_ change their attributes to maximize a payout, but incur a cost for modifying attributes.

As an illustrative example, we turn to the health insurance industry. In the United States (U.S.), contracted health insurance companies report their enrollees' diagnoses to the government, which calculates a payout based on reported diagnoses via a publicly available model . The _payout_ is intended to support care of the enrollee in relation to the diagnosis. Companies may attempt to maximize payouts by reporting extraneous diagnoses, an illegal practice known as "upcoding" . Despite increasing awareness of upcoding , upcoding costs U.S. taxpayers over $12B U.S. dollars annually , even with substantial investment in audits ($100.7M U.S. dollars, 2023 ) and payout changes to adjust for gaming . Since audits may not scale and often overlook fraud , tools for flagging gaming-prone agents could help target audits. Beyond health insurance, gaming emerges in responses to credit-scoring algorithms  and driver responses to rider allocation algorithms in ride-sharing apps .

In this work, we study how one can identify agents with the highest propensity to strategically manipulate their inputs given a dataset of agents and their observed model inputs. A supervised approach is infeasible since fraud/gaming labels are unavailable in our setting. A common paradigm for fraud/gaming detection is unsupervised anomaly detection. However, gamed attributes may not be outlier-like. The perspective of gaming as utility-maximizing behavior in strategic classification provides an alternative to existing fraud/gaming detection methods. Many works in strategic classification (_e.g._, ) assume known utility functions and identical feature manipulation costs across agents, which assists in identifying an agent's "optimal" gaming behavior. However, such assumptions may not always apply. For example, in U.S. Medicare, due to the rarity of penalties for upcoding, it is unclear how to quantify the _cost_ of fraud. Furthermore, due to the large number of companies contracted with U.S. Medicare, with distinct incentives (_e.g._, for-profit vs. non-profit groups) and disjoint populations of patients, there may be heterogeneity in feature manipulation costs.

To bridge this gap, we propose a novel framework for modeling agent utilities by introducing a **gaming deterrence parameter**, which scales the perceived cost (to the agent) of gaming. First, we show that directly estimating the gaming parameter is not possible: the best we can do is a lower bound on the gaming deterrence parameter. However, by re-casting gaming detection as a causal effect estimation problem, where each agent represents a "treatment," we prove that a _ranking_ of agents based on their gaming deterrence parameter is recoverable. Thus, we propose a causally-motivated ranking algorithm that produces a ranking of agents. Practically, agents most likely to game under our ranking could be flagged for further monitoring/auditing. While our framework is inspired by health insurance fraud, it applies more broadly to instances of gaming where multiple agents are gaming an ML-guided decision.

We evaluate the performance of causal effect estimators for gaming detection in a synthetic dataset. Empirically, causal approaches rank the worst offenders higher than existing non-causal approaches that screen based on payouts/randomly, as well as anomaly detection methods. We then verify in a real-world U.S. Medicare claims dataset that causal effect estimation yields rankings correlated with the prevalence of for-profit healthcare providers, a suspected driver of gaming [4; 16].

In summary: we **1)** extend strategic classification to model differences in gaming behavior across agents (Section 3), **2)** prove that point-identifying an agents' gaming parameter is impossible without strong assumptions (Section 4), **3)** show that, by recasting gaming detection as causal effect estimation, one can recover a ranking of agents based on their gaming deterrence parameters (Section 4), **4)** demonstrate empirically that our framework identifies the worst offenders with fewer audits than baselines (Section 5), and **5)** show in a real-data case study that our approach yields rankings correlated with suspected drivers of gaming (Section 5). Code to replicate our experiments will be made publicly available at https://github.com/MLD3/gaming_detection.

## 2 Related works

Machine learning-based anomaly detection.Fraud detection is often framed as an unsupervised anomaly/outlier detection problem [17; 18; 19; 20; 21; 22; 23]. Such approaches assume gamed model inputs are outliers in some distribution. However, this assumption may be incorrect if gaming

Figure 1: **Left: Two agents with gaming deterrence parameters \(_{1}=30\) (purple) and \(_{2}=50\) (blue) maximize utility (reward \(R\) - cost \(c\)) with respect to diagnosis rate. Gaming costs _increase_ in \(_{()}\), and lower an agent’s optimal diagnosis rate (stars). Center: Agents’ observed decisions reflect utility-maximizing behavior. Right: A decision-maker computes a payout based on agent decisions.**

is common in the data, or if gaming results in only small changes to observed agent attributes. In borrowing from strategic adaptation, we frame gaming as utility maximization, rather than making distributional assumptions about gamed agent attributes.

Strategic classification & gaming in machine learning.A large body of work in strategic classification aims to design incentives to mitigate gaming/strategic behavior by agents. However, such works assume that feature manipulations costs are known/can be estimated across agents, or are identical . In contrast, in our setting, feature manipulation costs are unknown and may differ across agents, but exhibit some shared structure that facilitates comparisons across agents. Shao et al.  assumes that agent gaming capacities may differ by placing bounds on manipulation, which may be unrealistic. Closest to our work is that of Dong et al. , which also assumes unknown and differing agent costs, but does not leverage similarities in gaming across agents. Our work supplements the strategic classification literature by studying a related yet fundamentally distinct problem: rather than designing incentives to mitigate strategic behavior, which is infeasible under unknown manipulation costs, we leverage differences in costs across agents to identify agents more likely to game.

## 3 Background & Problem Setup

We review strategic adaptation and extend it to model differences in gaming across agents.

**What is strategic adaptation?** Our work builds upon strategic classification . Consider a pre-existing model \(f:\) that maps agent attributes \(d\) to a payout. An _agent_ may leverage its knowledge of \(f\) to change their attribute(s) \(d\) according to some function \(\):

\[(d)*{arg\,max}_{}R( {d};f)-g(,d)\] (1)

where \(R:\) is the _payout_ of changing \(d\) to \(\), and \(g:_{+}\) is the _cost_ of manipulating \(d\). When \(\), \(g\) is often assumed to be "separable;" _i.e._, for some function \(c\), \(g(,d)=c(-d)\). \(()\) describes how an agent manipulates \(d\) to obtain a higher payout from \(f\). This behavior is called _strategic adaptation_ or _gaming_. For simplicity, we assume \(R=f\); _i.e._, the model \(f\) directly determines the payout.

**Modeling agent variation in gaming.** To extend strategic adaptation to multiple agents, we add a non-negative _gaming deterrence parameter_\(_{p}^{+}\) to Eq. 1. Consider an observational dataset \(_{p}\{(_{i},d_{i})\}_{i=1}^{M_{p}}\) of an agent \(p\)'s decisions \(d_{i}\{0,1\}\) given some information \(_{i}\). For simplicity, we assume \(d_{i}\) is binary, though the proposed framework generalizes to non-binary decisions and arbitrary numbers of independent decisions. For example, a health insurance plan \(p\) chooses whether to report that an enrollee has a diagnosis \(d_{i}\) given enrollee characteristics \(_{i}\). Agent assignment is _mutually exclusive_ (_e.g._, individuals are enrolled in one health insurance plan).

If the agent knows that \(d_{i}\) will be used as input to a payout model, they may have an incentive to _increase_\(d_{i}\) (without loss of generality) to obtain a higher payout. Let \(=}_{i=1}^{M_{p}}d_{i}\), and suppose each agent \(p\) chooses \(\) according to the following utility-maximization problem:

\[P(d_{i}=1 p)_{p}(d_{p}^{*})*{arg\,max} _{d}R()-_{p}c(-d_{p}^{*})\] (2)

where \(R:\), \(c:_{+}\), and \(d_{p}^{*}\) is the ground truth value of \(\) given the \(_{i}\) seen by agent \(p\). Thus, \(_{p}(d_{p}^{*})\) is the gamed/observed decision rate of agent \(p\) in a population where the ground truth decision rate is \(d_{p}^{*}\). Although \(\) since it is a proportion, our framework applies to \(\) on arbitrary intervals. This formulation assumes that each \(_{i}\) is equally likely to be gamed, that \(_{i}\) are truthfully observed, and that any difference between \(_{p}(d_{p}^{*})\) and \(d_{p}^{*}\) is due to gaming.

We focus on the gaming deterrence parameter \(_{p}\), which scales the cost of manipulation \(c()\). \(_{p}\) is non-negative and represents an agent's "aversion" to gaming. Lower values of \(_{p}\) mean that agent \(p\) is more willing to game. Thus, identifying agents most willing to game means finding agents with the lowest \(_{p}\). We summarize multi-agent strategic adaptation in Figure 1. Next, we introduce assumptions on the reward function \(R\), cost function \(c\), and ground truth \(d_{p}^{*}\).

**Assumption 1** (Shared rewards & costs).: _Reward (\(R\)) and cost (\(c\)) functions are shared across agents._

Sharing \(R\) encodes the assumption that all agents are reacting to the same payout model, while sharing \(c\) across agents encodes the belief agents must take similarly-costly actions to manipulate features (_e.g._, if insurance plans must follow/fraudulently report specific procedures to justify a diagnosis). Many works in strategic classification implicitly make a similar assumption (_e.g._, ).

**Assumption 2** (Increasing rewards).: _The reward function \(R\) is strictly increasing in \(\)._

Increasing rewards formalizes the agent's incentive to perturb its decisions (_i.e._, inputs to the payout model) from \(d_{i}=0\) to \(1\) in Eq. 2.

**Assumption 3** (Cost convexity).: _The cost function \(c\) is strictly convex and minimized at 0 such that \(c(0)=0\) and \(c^{}(0)=0\), and increases for all agents \(p\) for any \( d^{*}_{p}\)._

One possible \(c\) is \(c(x)=x^{2}\). Strict convexity ensures a unique cost-minimizing action, and \(c(0)=0\) ensures that \(d^{*}_{p}\) is ground truth, such that increasing \(\) incurs greater cost (_e.g._, Fig. 1, left).

**Assumption 4** (Diminishing or linear returns).: _The reward function \(R\) is concave in \(d_{i}\)._

For example, \(R\) may be a \(\) or affine function. Assumption 3 (strictly convex \(c\)) and 4 ensure that \(R\) cannot grow fast enough to offset manipulation costs. Furthermore, due to Assumptions 2- 4:

**Remark 1** (Gaming is utility-maximizing).: _Given any agent \(p\) and \(d^{*}_{p}\), we have that \(_{p}(d^{*}_{p}) d^{*}_{p}\)._

Equivalently, optimal gaming entails increasing \(d_{i}\) from the ground truth. Note that Assumptions 2- 4 are more general versions of assumptions placed on rewards/costs in the strategic classification literature (_e.g._, ).

**Assumption 5** (Non-strategic behavior is feasible).: \(d^{*}_{p}\) _is a constant depending solely on \(_{i}\)._

Due to Assumption 5, ground truth \(d^{*}_{p}\) may vary by agent due to differences in \(_{i}\) (_e.g._, health insurance plans serve populations with varying levels of health).

## 4 Theoretical analysis: finding agents most likely to game

We aim to identify agents most likely to game a decision-making model, _i.e._, agents with the lowest gaming parameters \(_{p}\). Here, we prove that \(_{p}\) cannot be point-identified without further assumptions (Section 4.1), but ranking \(_{p}\) is possible via causal effect estimation (Section 4.2). Detailed proofs are in Appendix B.

### Partial identification of the gaming parameter

Here, we show that given our assumptions, \(_{p}\) is only partially identifiable (cannot be uniquely determined):

**Proposition 1**.: _Define \(R^{}()\) as \(_{p}}\) and \(c^{}()\) as \(_{p}}\). For any agent \(p\), given Assumptions 1- 4 and an observed \(_{p}(d^{*}_{p})\),_

\[_{p}[(_{p}(d^{*}_{p}))}{c^{}( _{p}(d^{*}_{p}))},),\] (3)

_and the bound is sharp._

Intuitively, different values of the unknown \(d^{*}_{p}\) yield different estimates of \(_{p}\) consistent with the observed \(_{p}(d^{*}_{p})\). Thus, uncertainty in \(d^{*}_{p}\) results in uncertainty in \(_{p}\). Equivalently, point-identifying \(_{p}\) requires _perfect_ knowledge of \(d^{*}_{p}\). Thus, without further assumptions, \(_{p}\) is only partially identifiable. The lower bound is attained for \(d^{*}_{p}=0\) (all \(d_{i}=1\) are manipulated), while \(_{p}\) as \(_{p}(d^{*}_{p}) d^{*}_{p}\) (no manipulation). Intuitively, increases in \(_{p}\) further disincentivize increases to \(_{p}(d^{*}_{p})\), such that \(_{p}(d^{*}_{p})\) gets closer to \(d^{*}_{p}\).

A naive approach to gaming detection would be to rank individuals using the above bound. To see why this is problematic, consider an Agent 1 (\(_{1}=10\)) and Agent 2 (\(_{2}=30\)), and let \(R(x)=x\) and \(c(x)=x^{2}\). Suppose Agent 1 is a health insurance plan serving a relatively healthy population (\(d^{*}_{1}=0.05\)), while Agent 2 serves a population with a higher burden of illness (\(d^{*}_{2}=0.12\)). Via Eq. 2, we have \(_{1}(d^{*}_{1})=0.10\), while \(_{2}(d^{*}_{2}) 0.14\). Substitution into Eq. 3 yields \(_{1} 5\) and \(_{2} 3.66\), flipping the true ranking of \(_{p}\). Thus, acting on this bound may incorrectly penalize agents when a high \(_{p}(d^{*}_{p})\) is appropriate; _e.g._, insurance plans serving sicker populations.

### Identifying a ranking of the gaming parameter

Since we showed that point-identifying \(_{p}\) is impossible without further assumptions, we relax gaming detection to a _ranking_ problem. Intuitively, differences in agent behavior under similar conditions may indicate different gaming capacities, from which the proposed approach follows.

**Ranking \(_{p}\) by estimating counterfactuals.** Recall that we aim to find agents with the lowest \(_{p}\). Thus, it suffices to _rank_ agents by \(_{p}\), which can be done as follows:

**Theorem 1**.: _Under Assumptions 1- 5, and \(_{p^{}}(d_{p}^{*})\) defined as_

\[_{p^{}}(d_{p}^{*})*{arg\,max}_{d} \,R()-_{p^{}}c(-d_{p}^{*}),\] (4)

_we have that \(_{p}(d_{p}^{*})<_{p^{}}(d_{p}^{*})\) if and only if \(_{p}>_{p^{}}\)._

Theorem 1 tells us that estimation of \(_{p}(d_{p}^{*})\) and \(_{p^{}}(d_{p}^{*})\) can be used to rank \(_{p}\) vs. \(_{p^{}}\). Eq. 4 differs from Eq. 2: while \(R\), \(c\), and \(d_{p}^{*}\) are the same, \(_{p}\) changes to \(_{p^{}}\). While subtle, the distinction is key to gaming detection: \(_{p^{}}(d_{p}^{*})\) denotes what agent \(p^{}\)_would have_ done in a population with ground truth \(d_{p}^{*}\), as opposed to what agent \(p^{}\)_actually did_ in their population (ground truth \(d_{p^{}}^{*}\)).

To build a ranking strategy, first consider ranking two agents \(p\) and \(p^{}\). Define \(_{p,p^{}}\{(_{i},d_{i},p_{i}) i=1, ,n\) and \(p_{i}\{p,p^{}\}\}\), where \(p_{i}\) is an indicator for the agent that observed example \(i\). Following the Neyman-Rubin potential outcomes framework , let \(d_{i}(p)\) be the value of \(d_{i}\) if \(p_{i}\) was set to \(p\) (_i.e._ had agent \(p\) been _compelled_ to make a decision). Such variables are called _counterfactuals_. Figure 2 (left) shows a toy dataset with counterfactuals \(d_{i}(p),d_{i}(p^{})\), where "?" are unobserved decisions \(d_{i}\). Dropping unobserved data, the average \(d_{i}(p)\) is \(_{p}(d_{p}^{*})\) by definition. Thus, if \(d_{i}(p)\) and \(d_{i}(p^{})\) are fully observed, one could estimate \(_{p}(d_{p}^{*})\) and \(_{p^{}}(d_{p}^{*})\) as follows:

\[_{p}(d_{p}^{*})=}_{(_{i},d_{i},p_{i})_{p,p^{}}}d_{i}(p) _{p^{}}(d_{p}^{*})=}_{( _{i},d_{i},p_{i})_{p,p^{}}}d_{i} (p^{})\] (5)

Figure 3: Causually-motivated gaming detection. **Left:** First, we impute counterfactual decisions for each agent. **Middle:** The imputed counterfactuals yield average treatment effects (ATEs) across _pairs_ of agents. **Right:** Using ATE estimates to rank agents yields a ranking of the gaming parameter \(_{p}\). We show one direction of comparison across agents for simplicity. In practice, we impute decisions for both directions of comparison and average (given blue agent’s observations, impute purple agent’s decisions).

Figure 2: **Left:** Toy dataset with observed factual outcomes \(d_{i}(p)\) and \(d_{i}(p^{})\). “?” denotes missing counterfactual outcomes. **Right:** Causal graph for gaming detection with confounders \(\), agent indicator \(p\), ground truth diagnosis \(d^{*}\), and agent decision \(d\).

where \(n_{p}\) is the number of observations by agent \(p\), and use these estimates to rank \(_{p}\) as per Theorem 1. However, only one of \(d_{i}(p)\) or \(d_{i}(p^{})\) is ever observed. The need to estimate a counterfactual (namely, \(_{p^{}}(d_{p}^{*})\)) suggests a causal inference approach, which proceeds assuming the following :

**Assumption 6** (Conditional exchangeability).: _For all \(i\), \(d_{i}(p_{i})\!\!\! p_{i}_{i}\), where \(d_{i}(p_{i})\) is the potential outcome of \(d_{i}\) under agent \(p_{i}\)._

**Assumption 7** (Consistency).: _For all \(i\), \(d_{i}(p_{i})=d_{i}\)._

**Assumption 8** (Positivity/overlap).: _For any agent \(p\) and \(\), \(0<[p]<1\)._

Via Assumptions 6- 8, we have that \([d_{i}(p)_{i}]=[d_{i}_{i},p]\) and \([d_{i}(p^{})_{i}]=[d_{i}_{i},p^{}]\). Hence, the causal effect is _identifiable_. This estimand corresponds to a three-variable causal graph, where \(_{i}\) are confounders, the agent indicator \(p_{i}\) is a "treatment," and the agent's decision \(d_{i}\) is the outcome (Fig. 2, right).1 We proceed by estimating the effect of "swapping" agents:

**Corollary 1**.: _Define \((p,p^{})_{_{i}}[[d_{i}(p) _{i}]]-_{_{i}}[[d_{i}(p^{}) _{i}]]\). Then, given Assumptions 1-8, \((p,p^{})>0\) if and only if \(_{p}<_{p^{}}\)._

Since \([d_{i}(p)_{i}]=[d_{i}_{i},p]\), it is an unbiased estimator of \(_{p}(d_{p}^{*})\) by definition, and substitution into Theorem 1 yields the result. Thus, one can rank \(_{p}\) by estimating the effect of switching from agent \(p\) to \(p^{}\) on the observed decision rate for _all_ pairs of agents \(p,p^{}\). Each effect estimate is a pairwise comparison of agents, from which a ranking of \(_{()}\) follows. We summarize causally-motivated gaming detection in Fig. 3, with pseudocode in Fig. 4.

**Obtaining a well-ordered ranking.** Since we aim to rank \(_{p}\), our chosen estimator should yield a well-ordered ranking. Via Corollary 1, the oracle treatment effect \(\) suffices, but \(\) must generally be estimated. We show that a "sufficiently" accurate estimate \(\) also yields the desired result:

**Proposition 2**.: _Let \(()\) be the oracle treatment effect function as defined in Corollary 1, and \(\) be its sample estimate. Given Assumptions 1-8, for any \(>0\), if \(\ |(p,p^{})-(p,p^{})|\), then, for all \(p,p^{}\) such that \(_{p,p^{}}|(p,p^{})|>\), \((p,p^{})>0\) if and only if \(_{p}<_{p^{}}\)._

The result is immediate: sufficiently low estimation error in \(\) (_i.e._, \(\)) cannot "flip" any pairwise rankings where \(>\). Thus, any consistent estimate of \(\) yields (asymptotically) a well-ordering of \(_{p}\). We defer to past asymptotic analyses of causal effect estimation for further discussion [38; 39].

## 5 Empirical results & discussion

We aim to demonstrate that causal inference can be used for gaming detection. First, we discuss our setup (Section 5.1). Then, we show in synthetic data (Section 5.2) that causal methods require fewer audits than existing non-causal methods to catch the worst offenders. Finally, in real-world case study (Section 5.3), we find that causal methods yield rankings correlated with suspected drivers of gaming.

Figure 4: Pseudocode for causally-motivated gaming detection. Causal effect estimators take pairs of agents (\(p,p^{}\)) and their observations \(^{()}\) as inputs, and output the mean difference in predicted decision rate.

### Setup

We describe the datasets, evaluation method, and gaming detection methods under consideration.

**Datasets.** In real datasets, ground truth gaming rankings are often unavailable. Thus, we validate our framework in a **synthetic dataset**. We hand-select 20 \(_{p}\) values (one per agent; see Appendix C.1 for raw \(_{p}\) values) and simulate confounding by generating covariates from agent-specific Gaussians with means \(_{p}^{2}\). To control confounding strength, we set \(_{p}=g((_{p}))\), where \(g\) is an affine transformation such that the range of \(_{p}\) across agents equals a chosen range parameter \(R_{}\{0,0.1,,1.0\}\). Smaller \(R_{}\) implies less confounding, since \(_{()}\) varies less across agents. We generate 500 observations \(^{(i)}^{2}\) and ground-truth \(d^{*(i)}\) per agent via

\[^{(i)}(_{p^{(i)}},^{2}_{2 2}) d^{*(i)} Ber(^{*(i)});^{*(i)}= (^{}^{(i)}+b),\]

where \((0,1)^{2}\), such that increasing \(\) increases \(^{*(i)}\) (and thus \(P(d^{(i)}=1)\)), \(b\) is chosen such that \(^{*(i)}\) for the mean \(\) is \( 5\%\), and \(^{2}=1\).2 We simulate gamed agent decisions \(d^{(i)}\) as follows:

\[d^{(i)} Ber(_{p}^{(i)});_{p}^{(i)}=*{arg \,max}_{_{p}}\,(_{p})-_{p}(_{p} -d^{*(i)})^{2}.\]

Recall that the decisions \(d^{(i)}\) are also agent inputs to a payout model. We generate 10 datasets (each \(N=10,000\); 20 agents \(\) 500 observations) for all 11 levels of confounding (as measured by the range of means \(R_{}\)). Causal inference assumptions hold in the synthetic data: all confounders \(^{(i)}\) are observed (Assumption. 6), consistency holds by construction (Assumption. 7), and overlap holds since all \(^{(i)} p\) are supported on \(^{2}\) (Assumption. 8). Full synthetic data generation details are in Appendix C.1.

To benchmark causal effect estimation in a more realistic setting, we apply causal methods to gaming detection in **U.S. Medicare** claims. Medicare is the public health insurance system in the U.S. for residents aged 65 and over. Since private insurance claims data is not widely available, we conduct a gaming case study in healthcare providers. In Medicare, the U.S. government pays healthcare providers on a per-service basis . Thus, providers may be incentivized to label enroles with as many diagnoses as possible to secure extra payment from the government. We select a 0.2% sample of all Medicare enroles with a claim in 2018; _i.e._, those who utilized a service covered directly by Medicare (\(N=37,893\)). We use demographic information and diagnoses in 2018 as covariates and select the rate of uncomplicated diabetes diagnosis in 2019 as the outcome. Given differences in healthcare policy and access across U.S. states, we pool data at the U.S. state level and treat each state as an "agent." Additional cohort details are in Appendix C.2.

**Evaluating rankings.** Given an observational dataset of the form \(\{(_{i},d_{i},p_{i})\}_{i=1}^{N}\), with covariates \(_{i}\), observed decisions \(d_{i}\), and agent indicators \(p_{i}\), gaming detection algorithms output an ordinal agent ranking in terms of the gaming parameter \(_{p}\). We aim to measure the efficiency of a predicted ranking given some level of resources committed by a decision-maker (_i.e._, # of agents audited).

Ground truth rankings are available in synthetic data. Thus, we measure the top-5 **sensitivity** at \(k\) (\(S_{k}\)), the % of top-5 worst offenders in the predicted top-\(k\), and the **discounted cumulative gain (DCG)** at \(k\), a weighted sum of ground-truth "relevance scores" for the top-\(k\) predicted agents, across audit intensities \(k\{1,,20\}\). For a \(K\)-agent dataset, we define the relevance score as \(K+1\) minus the ground-truth rank (_e.g._, true rank 1 = relevance \(K\), true rank 2 = relevance \(K-1\), etc.). Concretely, for \(r_{i}\) defined as the \(i\)th ranked agent in a predicted ranking, and rank\(()\) as the function returning the _ground-truth_ ordinal rank with respect to \(_{p}\), our ranking evaluation metrics are computed as follows:

\[S_{k}_{i=1}^{k}[(r_{i}) 5 ]_{k}_{i=1}^{k}(r_{i})}{_ {2}(i+1)}.\] (6)

Note that sensitivity is an all-or-nothing measure of audit quality given a fixed audit intensity \(k\). However, DCG rewards higher predicted rankings for top-\(k\) worst offenders, regardless of the absolute ranking position. Furthermore, DCG weights decrease with predicted rank (Eq. 6), which prioritizescorrectly ranking the worst offenders over correctly ranking agents unlikely to game. To summarize audit efficiency across \(k\), we also report **area under the top-5 sensitivity curve (AUSC)** across \(k\).3

In contrast, ground truth gaming rankings are unavailable in the Medicare cohort. As an exploratory analysis, we compare an estimated gaming ranking to 104 state-level healthcare statistics from the 2003-2017 National Neighborhood Data Archive  and 2018 Medicare Provider of Service files  relating to healthcare access and hospital information (_e.g._, ownership and size). We report the top five statistics most positively and negatively correlated with our predicted rankings in terms of **Spearman rank-correlation**. State-level summary statistics used are enumerated in Appendix C.2.

**Models.** To demonstrate the utility of causal effect estimation for gaming detection, we compare non-causal baselines to causal effect estimators. Non-causal approaches include a payout-only ranking (based on \(P(d_{i}=1)\) per agent) and random ranking. We also compare to existing approaches in anomaly detection, which do not make causal assumptions, but assume that gamed decisions are outlier-like. We test \(k\)-nearest neighbor outlier detection (KNN) , empirical-cumulative-distribution-based outlier detection (ECOD) , and deep isolation forests (DIF) . These methods use \((_{i},d_{i})\) as inputs to an "anomaly score" model. We use average within-agent anomaly scores as a ranking. We discuss other works in algorithmic anomaly/fraud detection in Appendix A.

We implement the following causal effect estimators. **PSM** fits a propensity score model to match points in one agent's population to its nearest neighbor in the other agent's population with respect to propensity score estimates . The **S-learner** trains one outcome prediction model for all agents, while the **T-learner** trains one model per agent . **DragonNet** jointly models the outcome (one prediction "head" per agent) and propensity score to control for confounding . **S+IPW** fits the S-learner with estimated sample weights that reweight the observed distribution to resemble an unconfounded distribution (randomized treatment assignment) . The **R-learner** fits an outcome and propensity model, then regresses the residuals on one another to obtain a final unconfounded estimator . Hyperparameters for all baselines and causal effect estimators are in Appendix E.

**Implementation details.** We use neural networks for all modeling (causal effect estimators + DIF). We use one-hot encoding for treatments (agent indicators). Matching across pairs of agents occurred without replacement (one-to-one), dropping unmatched individuals. We use generalizations of IPW and R-learners to multiple treatments, namely permutation weighting  and the structured intervention network , respectively. We perform a 7:3 dataset train-test split, training all models on the larger split. All rankings are computed on the test split. Early stopping is performed on a 20% validation split randomly sampled from the training set. Full modeling and training details, including the architecture and hyperparameters used, are in Appendix E.

### Gaming detection in synthetic data

**Causal effect estimators identify gaming more efficiently than non-causal baselines.** Figure 5 shows the top-5 sensitivity and DCG of rankings produced by causal vs. non-causal gaming detection approaches at high confounding (mean range: 0.9). Since the S-, T-learner, and DragonNet perform

Figure 5: Mean top-5 sensitivity (left) and DCG (center) across # of agents audited, and top-5 sensitivity with 7 audits (right) at mean range 0.9, with \(\) error. Causal methods improve over non-causal baselines. \(\): naive baselines. \(\): anomaly detectors. \(\): causal effect estimators.

similarly (Appendix D.2), we show only DragonNet here. Since PSM underperforms due to challenges with multi-treatment confounding control, we also defer results for PSM to Appendix D.2. Across audit intensities, causal approaches outperform non-causal methods in terms of sensitivity (_e.g._, at 7 audits, Fig. 5, right; S+IPW: 0.860\(\)0.135 vs. KNN: 0.520\(\)0.215) and DCG (S+IPW: 56.1\(\)5.11 vs. KNN: 42.3\(\)10.8).4 Trends are similar at other levels of confounding (Appendix D.1), though the gain in ranking performance of causal approaches over non-causal methods diminish at lower levels of confounding (Figure 7; S+IPW AUSC: 0.614\(\)0.096 vs. KNN: 0.648\(\)0.129, mean range 0.0).

A payout-only approach yields worse than random ranking with sufficient confounding between covariates and agent decisions (payout-only AUSC: 0.299\(\)0.067 vs. random: 0.473\(\)0.101, Figure 6, mean range 1.0; _e.g._, if healthier patients are enrolled in more gaming-prone plans). Anomaly detection methods (KNN, ECOD, DIF) are ill-suited for detecting gaming in dense regions of covariate space by design, while causal approaches would excel due to improved overlap. If outliers are more likely to be manipulated (_e.g._, if populations with lower ground-truth \(d_{p}^{*}\) are more likely to be gamed), an anomaly detection method would identify gaming in such points. Indeed, anomaly detectors empirically outperform random ranking but lag causal methods. We further discuss why anomaly detection methods underperform causal approaches in Appendix D.1.

Trends in ranking performance vary across causal effect estimators. DragonNet and the R-learner degrade slightly as confounding increases (DragonNet AUSC: 0.755\(\)0.096 \(\) 0.696\(\)0.083; R-learner: 0.728\(\)0.114 \(\) 0.635\(\)0.111; mean range 0.0 \(\) 1.0), likely due to slightly worse overlap. However, S+IPW _improves_ as confounding increases (AUSC: 0.614\(\)0.096 \(\) 0.837\(\)0.067; mean range 0.0 \(\) 1.0). This is likely since the oracle IPW weights deviate from a uniform weighting as confounding increases. In such settings, estimation error in IPW weights may be less likely to incorrectly up-weight points that an oracle propensity score would down-weight, and vice versa. While such error would bias the pointwise treatment effect estimate, for the purposes of ranking, causal effect estimators only need to correctly estimate the _sign_ of the treatment effect. Thus, we hypothesize that our ranking may be less affected by such errors in the propensity score estimate. We leave formal analyses of the properties of IPW with respect to the sign of causal effect estimates to future work. A sensitivity analysis of all causal approaches is in Appendix D.2.

Takeaways.In a synthetic dataset, causal effect estimation approaches identified gaming more efficiently than non-causal baselines across levels of confounding. The empirical results provide proof-of-concept for causal effect estimation as a gaming detection method.

### Case study: Detecting upcoding in U.S. Medicare

As an exploratory analysis, we use the best-performing approach in the synthetic data (S+IPW) analyze upcoming by U.S. state in U.S. Medicare. Table 1 shows the five state-level healthcare statistics most positively and negatively correlated with gaming rankings predicted by S+IPW.

Figure 6: Area under the sensitivity curve (AUSC) for causal vs. non-causal methods across levels of confounding, with \(\) error. As confounding increases, a payout-only ranking degrades. Anomaly detection performance does not vary across confounding strength, maintaining slightly better than random rankings. Causal methods generally maintain higher mean AUSC than baselines across confounding levels. \(\): naive baselines. \(\): anomaly detectors. \(\): causal effect estimators.

Figure 7: AUSC of S+IPW (causal) vs. KNN (non-causal) across confounding strength with \(\) error. The advantage of S+IPW over KNN decreases as confounding diminishes.

**Upcoding is correlated with for-profit provider prevalence.** Four of the top five features most positively associated with our predicted ranking reflect a greater state-level prevalence of for-profit healthcare providers. This matches the intuition that for-profit providers may game more aggressively due to stronger profit motives. Notably, the feature 2nd-most positively correlated with our rankings (ratio of for-profit to non-profit hospitals) is a suspected driver of upcoming in Medicare [4; 16], with the idea that competition from for-profit providers drives non-profit providers towards gaming. Note that many of the correlations are not statistically significant, and unmeasured factors such as healthcare quality could explain differences in diagnosis coding, rather than gaming. Despite the limitations, causal effect estimation shows promise as a practical approach to gaming detection.

**Takeaways.** In an exploratory case study of gaming in U.S. Medicare, causal effect estimation yields a ranking of U.S. states that _positively_ correlates with the prevalence of for-profit healthcare providers, matching domain expertise on suspected drivers of gaming.

## 6 Conclusion

We propose a causally-motivated framework for ranking agents by gaming propensity in the context of strategic adaptation. We show that the gaming parameter is only partially identifiable, but a ranking of a set of agents based on the gaming deterrence parameter is identifiable via causal inference. We demonstrate the utility of causal effect estimation for gaming detection on synthetic data and a case study of upcoming in Medicare.

**Limitations & broader impact.** We assume agents always increase \(d_{i}\) with respect to ground truth, and that gaming explains all differences in agent behaviors, ignoring factors such as agent "quality" (_e.g._, quality of care). Utility-maximizing behavior and conditional exchangeability are strong assumptions, but are statistically unverifiable. Many works in game theory and causal inference share these limitations. We caution that policies informed by our framework could reinforce imbalanced power dynamics (_i.e._, are individual citizens  or more powerful entities gaming a model) via extraneous or weaponized accusations of gaming, since not all entities have equal capacity to respond to such claims. In particular, gaming by individuals may potentially reflect structural inequities rather than inherently pathological behavior. To mitigate such risks, we suggest "shadowing" studies (decisions visible, but not acted upon) alongside existing audit mechanisms before adoption.