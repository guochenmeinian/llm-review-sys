# RAMP: Boosting Adversarial Robustness Against Multiple \(l_{p}\) Perturbations for Universal Robustness

RAMP: Boosting Adversarial Robustness Against Multiple \(l_{p}\) Perturbations for Universal Robustness

 Enyi Jiang

Department of Computer Science

University of Illinois Urbana-Champaign

Urbana, IL 61801

enyij20@illinois.edu

&Gagandeep Singh

Department of Computer Science

University of Illinois Urbana-Champaign

Urbana, IL 61801

ggnds@illinois.edu

###### Abstract

Most existing works focus on improving robustness against adversarial attacks bounded by a single \(l_{p}\) norm using adversarial training (AT). However, these AT models' multiple-norm robustness (union accuracy) is still low, which is crucial since in the real-world an adversary is not necessarily bounded by a single norm. The tradeoffs among robustness against multiple \(l_{p}\) perturbations and accuracy/robustness make obtaining good union and clean accuracy challenging. We design a logit pairing loss to improve the union accuracy by analyzing the tradeoffs from the lens of distribution shifts. We connect natural training (NT) with AT via gradient projection, to incorporate useful information from NT into AT, where we empirically and theoretically show it moderates the accuracy/robustness tradeoff. We propose a novel training framework **RAMP**, to boost the robustness against multiple \(l_{p}\) perturbations. **RAMP** can be easily adapted for robust fine-tuning and full AT. For robust fine-tuning, **RAMP** obtains a union accuracy up to \(53.3\%\) on CIFAR-10, and \(29.1\%\) on ImageNet. For training from scratch, **RAMP** achieves a union accuracy of \(44.6\%\) and good clean accuracy of \(81.2\%\) on ResNet-18 against AutoAttack on CIFAR-10. Beyond multi-norm robustness **RAMP**-trained models achieve superior _universal robustness_, effectively generalizing against a range of unseen adversaries and natural corruptions.

## 1 Introduction

Though deep neural networks (DNNs) demonstrate superior performance in various vision applications, they are vulnerable against adversarial examples (Goodfellow et al., 2014; Kurakin et al., 2018). Adversarial training (AT) (Tramer et al., 2017; Madry et al., 2017) which works by injecting adversarial examples into training for enhanced robustness, is currently the most popular defense. However, most AT methods address only a _single_ type of perturbation (Wang et al., 2020; Wu et al., 2020; Carmon et al., 2019; Gowal et al., 2020; Raghunathan et al., 2020; Zhang et al., 2021; Debenedetti and Troncoso--EPFL, 2022; Peng et al., 2023; Wang et al., 2023). An \(l_{}\) robust model may not be robust against \(l_{p}(p)\) attacks. Also, enhancing robustness against one perturbation type can sometimes increase vulnerability to others (Engstrom et al., 2017; Schott et al., 2018). On the contrary, training a model to be robust against multiple \(l_{p}\) perturbations is crucial as it reflects real-world scenarios (Sharif et al., 2016; Eykholt et al., 2018; Song et al., 2018; Athalye et al., 2018) where adversaries can use multiple \(l_{p}\) perturbations. We show that multi-norm robustness is the key to improving generalization against other threat models (Croce and Hein, 2022). For instance, we show it enables robustness against perturbations not easily defined mathematically, such as image corruptions and unseen adversaries (Wong and Kolter, 2020).

Two main challenges exist for training models robust against multiple perturbations: (i) tradeoff among robustness against different perturbation models (Tramer and Boneh, 2019) and (ii) tradeoff between accuracy and robustness (Zhang et al., 2019; Raghunathan et al., 2020). Adversarial examples induce a shift from the original distribution, causing a drop in clean accuracy with AT (Xie et al., 2020; Benz et al., 2021). The distinct distributions created by \(l_{1},l_{2},l_{}\) adversarial examples make the problem even more challenging. Through a finer analysis of the distribution shifts caused by these adversaries, we propose the **RAMP** framework to efficiently boost the **R**obustness **A**gainst **M**ultiple **P**erturbations. **RAMP** can be used for both fine-tuning and training from scratch. It utilizes a novel logit pairing loss on a certain pair and connects NT with AT via gradient projection (Jiang et al., 2023) to improve union accuracy while maintaining good clean accuracy and training efficiency.

**Logit pairing loss.** We visualize the changing of \(l_{1},l_{2},l_{}\) robustness when fine-tuning a \(l_{}\)-AT pre-trained model in Figure 1 using the CIFAR-10 training dataset. The DNN loses substantial robustness against \(l_{}\) attack after only \(1\) epoch of fine-tuning: \(l_{1}\) fine-tuning and E-AT (Crocce and Hein, 2022) (red and yellow histograms under Linf category) both lose significant \(l_{}\) robustness (compared with blue histogram under Linf category). Inspired by this observation, we devise a new logit pairing loss for a \(l_{q}-l_{r}\) tradeoff pair to attain better union accuracy, which enforces the logit distributions of \(l_{q}\) and \(l_{r}\) adversarial examples to be close, specifically on the correctly classified \(l_{q}\) subsets. In comparison, our method (green histogram under Linf and union categories) preserves more \(l_{}\) and union robustness than others after \(1\) epoch. We show this technique works on larger models and datasets (Section 5.1).

**Connect natural training (NT) with AT.** We explore the connections between NT and AT to obtain a better accuracy/robustness trade-off. We find that NT can help with adversarial robustness: useful information in natural distribution can be extracted and leveraged to achieve better robustness. To this end, we compare the similarities of model updates of NT and AT _layer-wise_ for each epoch, where we find and incorporate useful NT components into AT via gradient projection (GP), as outlined in Algorithm 2. In Figure 2 and Section 5.1, we empirically and theoretically show this technique strikes a better balance between accuracy and robustness, for both single and multiple \(l_{p}\) perturbations. We provide a theoretical analysis of why GP works for adversarial robustness in Theorem A.2 & 4.5.

**Main contributions**:

* We design a new logit pairing loss to mitigate the \(l_{q}-l_{r}\) tradeoff for better union accuracy, by enforcing the logit distributions of \(l_{q}\) and \(l_{r}\) adversarial examples to be close.
* We empirically and theoretically show that connecting NT with AT via gradient projection better balances the accuracy/robustness tradeoff for \(l_{p}\) perturbations, compared with standard AT.
* **RAMP** achieves good union accuracy, accuracy-robustness tradeoff, and generalizes better to diverse perturbations and corruptions (Section 5.1) achieving superior _universal robustness_ (\(75.5\%\) for common corruption and \(26.1\%\) union accuracy against unseen adversaries). **RAMP** fine-tuned DNNs achieve union accuracy up to \(53.3\%\) on CIFAR-10, and \(29.1\%\) on ImageNet. **RAMP** achieves a \(44.6\%\) union accuracy and good clean accuracy on ResNet-18 against AutoAttack on CIFAR-10. Our code is available at https://github.com/uiuc-focal-lab/RAMP.

## 2 Related Work

**Adversarial training (AT).** Adversarial Training (AT) usually employs gradient descent to discover adversarial examples, incorporating them into training for enhanced adversarial robustness (Tramer et al., 2017; Madry et al., 2017). Numerous works focus on improving robustness by exploring the trade-off between robustness and accuracy (Zhang et al., 2019; Wang et al., 2020), instance reweighting (Zhang et al., 2021), loss landscapes (Wu et al., 2020), wider/larger architectures (Gowal

Figure 1: **Multiple-norm tradeoff with robust fine-tuning**: We observe that fine-tuning on \(l_{}\)-AT model using \(l_{1}\) examples drastically reduces \(l_{}\) robustness. **RAMP** preserves more \(l_{}\) and union robustness.

et al., 2020; Debenedetti and Troncoso--EPFL, 2022), data augmentation (Carmon et al., 2019; Raghunathan et al., 2020), and using synthetic data (Peng et al., 2023; Wang et al., 2023). However, these methods often yield DNNs robust against a _single_ perturbation type while remaining vulnerable to other types.

**Robustness against multiple perturbations.**Tramer and Boneh (2019); Kang et al. (2019) observe that robustness against \(l_{p}\) attacks does not necessarily transfer to other \(l_{q}\) attacks (\(q p\)). Previous studies (Tramer and Boneh, 2019; Maini et al., 2020; Madaan et al., 2021; Croce and Hein, 2022) modified Adversarial Training (AT) to enhance robustness against multiple \(l_{p}\) attacks, employing average-case (Tramer and Boneh, 2019), worst-case (Tramer and Boneh, 2019; Maini et al., 2020), and random-sampled (Madaan et al., 2021; Croce and Hein, 2022) defenses. There are also works (Nandy et al., 2020; Liu et al., 2020; Xu et al., 2021; Xiao et al., 2022; Maini et al., 2022) using preprocessing, ensemble methods, mixture of experts, and stability analysis to solve this problem. Ensemble models and preprocessing methods are weakened since their performance heavily relies on correctly classifying or detecting various types of adversarial examples. In certified training, Banerjee et al. (2024); Banerjee and Singh (2024) propose verification/cerifiable training methods under different threat models for \(l_{p}\) universal adversarial perturbation. However, prior works are hard to scale to larger models and datasets, e.g. ImageNet, due to the efficiency issue. Furthermore, Croce and Hein (2022) devise Extreme norm Adversarial Training (E-AT) and fine-tune a \(l_{p}\) robust model on another \(l_{q}\) perturbation to quickly make a DNN robust against multiple \(l_{p}\) attacks. However, E-AT does not adapt to varying epsilon values. Our work demonstrates that the suboptimal tradeoff observed in prior studies can be improved with our proposed framework.

**Logit pairing in adversarial training.** Adversarial logit pairing methods encourage logits for pairs of examples to be similar (Kannan et al., 2018; Engstrom et al., 2018). People apply this technique to both clean images and their adversarial counterparts, to devise a stronger form of adversarial training. In our work, we devise a novel logit pairing loss to train a DNN originally robust against \(l_{p}\) attack to become robust against another \(l_{q}(q p)\) attack on the correctly predicted \(l_{p}\) subsets, which helps gain better union accuracy.

**Adversarial versus distributional robustness.**Sinha et al. (2018) theoretically studies the AT problem through distributional robust optimization. Mehrabi et al. (2021) establishes a pareto-optimal tradeoff between standard and adversarial risks by perturbing the test distribution. Other works explore the connection between natural and adversarial distribution shifts (Moayeri et al., 2022; Alhamoud et al., 2023), assessing transferability and generalizability of adversarial robustness across datasets. However, little research delves into distribution shifts induced by \(l_{1},l_{2},l_{}\) adversarial examples and their interplay with the robustness-accuracy tradeoff (Zhang et al., 2019; Yang et al., 2020; Rade and Moosavi-Dezfooli, 2021). Our work, inspired by recent domain adaptation techniques (Jiang, 2023; Jiang et al., 2023), designs a logit pairing loss and utilizes model updates from NT via GP to enhance adversarial robustness. We show that GP adapts to both single and multi-norm scenarios.

## 3 AT against Multiple Perturbations

We consider a standard classification task with samples \(\{(x_{i},y_{i})\}_{i=0}^{N}\) from an empirical data distribution \(}_{n}\); we have input images \(x^{d}\) and corresponding labels \(y^{k}\). Standard training aims to obtain a classifier \(f\) parameterized by \(\) to minimize a loss function \(:^{k}^{k}\) on \(}_{n}\). Adversarial training (AT) (Madry et al., 2017; Tramer et al., 2017) aims to find a DNN robust against adversarial examples. It is framed as a min-max problem where a DNN is optimized using the worst-case examples within an adversarial region around each \(x_{i}\). Different types of adversarial regions \(B_{p}(x,_{p})=\{x^{}^{d}:\|x^{}-x\|_{p} _{p}\}\) can be defined around a given image \(x\) using various \(l_{p}\)-based perturbations. Formally, we can write the optimization problem of AT against a certain \(l_{p}\) attack as follows:

\[_{}_{(x,y)}_{n}}[_{x^{ } B_{p}(x,_{p})}(f(x^{}),y)]\]

The above optimization is only for certain \(p\) values and is usually vulnerable to other perturbation types. To this end, prior works have proposed several approaches to train the network robust against multiple perturbations (\(l_{1},l_{2},l_{}\)) at the same time. We focus on the union threat model \(=B_{1}(x,_{1}) B_{2}(x,_{2}) B_{}(x,_{ })\) which requires the DNN to be robust within the \(l_{1},l_{2},l_{}\) adversarial regions simultaneously (Croce and Hein, 2022). Union accuracy is then defined as the robustness against \(_{(i)}\) for each \(x_{i}\) sampled from \(\). In this paper, similar to the prior works, we use union accuracy as the main metric to evaluate the multiple-norm robustness. Apart from that, we define _universal robustness_ as the generalization ability against a range of unseen adversaries and common corruptions. Specifically, we have average accuracy across five severity levels for common corruption and union accuracy against a range of unseen adversaries used in Laidlaw et al. (2020).

**Worst-case defense** follows the following min-max optimization problem to train DNNs using the worst-case example from the \(l_{1},l_{2},l_{}\) adversarial regions:

\[_{}_{(x,y)}_{n}}[_{p \{1,2,\}}_{x^{} B_{p}(x,_{p})}(f(x^{ }),y)]\]

MAX (Tramer and Boneh, 2019) and MSD (Maini et al., 2020) fall into this category. Finding worst-case examples yields a good union accuracy but results in a loss of clean accuracy as the distribution of generated examples is different from the clean data distribution.

**Average-case defense** train DNNs using the average of the \(l_{1},l_{2},l_{}\) worst-case examples:

\[_{}_{(x,y)}_{n}}[_ {p\{1,2,\}}_{x^{} B_{p}(x,_{p})}(f(x^ {}),y)]\]

AVG (Tramer and Boneh, 2019) is of this type. This method generally leads to good clean accuracy but suboptimal union accuracy as it does not penalize worst-case behavior within the \(l_{1},l_{2},l_{}\) regions.

**Random-sampled defense.** The defenses mentioned above lead to a high training cost as they compute multiple attacks for each sample. SAT (Madaan et al., 2021) and E-AT (Croce and Hein, 2022) randomly sample one attack out of each type at a time, contributing to a similar computational cost as standard AT on a single perturbation model. They achieve a slightly better union accuracy compared with AVG and relatively good clean accuracy. However, they are not better than worst-case defenses for multiple-norm robustness, since they do not consider the strongest attack within the union region all the time.

## 4 Ramp

There are two main tradeoffs in achieving better union accuracy while maintaining good accuracy: 1. Among perturbations: there is a tradeoff among different attacks, e.g., a \(l_{}\) pre-trained AT DNN is not robust against \(l_{1},l_{2}\) perturbations, which makes the union accuracy harder to attain. Also, we observe there exists a main tradeoff pair of two attacks among the union over \(l_{1}\), \(l_{2}\), \(l_{}\) attacks. 2. Accuracy and robustness: all defenses lead to degraded clean accuracy. To address these tradeoffs, we study the problem from the lens of distribution shifts.

**Interpreting tradeoffs from the lens of distribution shifts.** The adversarial examples with respect to an empirical data distribution \(}_{n}\), adversarial region \(B_{p}(x,_{p})\), and DNN \(f_{}\) generate a new adversarial distribution \(}_{a}\) with samples \(\{(x^{}_{i},y_{i})\}_{i=0}^{N}\), that are correlated by adding certain perturbations but different from the original \(}_{n}\). Because of the shifts between \(}_{n}\) and \(}_{a}\), DNN decreases performance on \(}_{n}\) when we move away from it and towards \(}_{a}\). Also, the distinct distributions created by multiple perturbations, \(}_{a}^{l}\), \(}_{a}^{l_{2}}\), \(}_{a}^{l_{}}\), contribute to the tradeoff among \(l_{1},l_{2},l_{}\) attacks. To address the tradeoff among perturbations while maintaining good efficiency, we focus on the distributional interconnections between \(}_{n}\) and \(}_{a}^{l_{1}}\), \(}_{a}^{l_{2}}\), \(}_{a}^{l_{}}\). From the insights we get from above, we propose our framework **RAMP**, which includes (i) logit pairing to improve tradeoffs among multiple perturbations, and (ii) identifying and combining the useful DNN components using the model updates from NT and AT, to obtain a better robustness/accuracy tradeoff.

**Identify the Key Tradeoff Pair.** We study the common case with \(l_{p}\) norms \(_{1}=12,_{2}=0.5,_{}=\) on CIFAR-10 (Tramer and Boneh, 2019). The distributions generated by the two strongest attacks show the largest shifts from \(}_{n}\); also, they have the largest distribution shifts between each other 

[MISSING_PAGE_FAIL:5]

**NT can help adversarial robustness.** Let us consider two models \(f_{1}\) and \(f_{2}\), where \(f_{1}\) is randomly initialized and \(f_{2}\) undergoes NT on \(}_{n}\) for \(k\) epochs: \(f_{2}\) results in a better _decision boundary_ and higher clean accuracy. Performing AT on \(f_{1}\) and \(f_{2}\) subsequently, intuitively, \(f_{2}\) becomes more robust than \(f_{1}\) due to its improved decision boundary, leading to fewer misclassifications of adversarial examples. This effect is empirically shown in Figure 2. For **AT** (blue), standard AT against \(l_{}\) attack  is performed, while for **AT-pre** (red), \(50\) epochs of pre-training precede the standard AT procedure. **AT-pre** shows superior clean and robust accuracy on CIFAR-10 against \(l_{}\) PGD-20 attack with \(_{}=0.031\). Despite \(}_{n}\) and \(}_{a}\) are different, Figure 2 suggests valuable information in \(}_{n}\) that potentially enhances performance on \(}_{a}\).

**AT with Gradient Projection.** To connect NT with AT more effectively, we analyze the training procedures on \(}_{n}\) and \(}_{a}\). We consider model updates over all samples from \(}_{n}\) and \(}_{a}\), with the initial model \(f^{(r)}\) at epoch \(r\), and models \(f^{(r)}_{n}\) and \(f^{(r)}_{a}\) after \(1\) epoch of natural and adversarial training from the same starting point \(f^{(r)}\), respectively. Here, we compare the natural updates \(_{n}=f^{(r)}_{n}-f^{(r)}\) and adversarial updates \(_{a}=f^{(r)}_{a}-f^{(r)}\). Due to distribution shift, an _angle_ exists between them. Our goal is to identify useful components from \(g_{n}\) and incorporate them into \(g_{a}\) for increased robustness in \(}_{a}\) while maintaining accuracy in \(}_{n}\). Inspired by Jiang et al. , we _layer-wisely_ compute the cosine similarity between \(_{n}\) and \(_{a}\). For a specific layer \(l\) of \(_{n}^{l}\) and \(_{a}^{l}\), we preserve a portion of \(_{n}^{l}\) based on their cosine similarity score (Eq.4). Negative scores indicate that \(_{n}^{l}\) is not beneficial for robustness in \(}_{a}\). Therefore, we filter components with similarity score \( 0\). We define the **GP** (Gradient Projection) operation in Eq.5 by projecting \(_{a}^{l}\) towards \(_{n}^{l}\).

\[(_{n}^{l},_{a}^{l})=_{n}^{l} _{a}^{l}}{\|_{n}^{l}\|\|_{a}^{l}\|} (_{n}^{l},_{a}^{l})= (_{n}^{l},_{a}^{l})_{n }^{l},&(_{n}^{l},_{a}^{l})>0\\ 0,&(_{n}^{l},_{a}^{l}) 0\] (5)

Therefore, the total projected (useful) model updates \(g_{p}\) coming from \(_{n}\) could be computed as Eq. 6. We use \(\) to denote all layers of the current model update. Note that \(_{l}\) concatenates all layers' useful natural model update components. A hyper-parameter \(\) is used to balance the contributions of \(g_{GP}\) and \(_{a}\), as shown in Eq. 7. By finding a proper \(\) (0.5 as in Figure 3(c)), we can obtain better robustness on \(_{a}\), as shown in Figure 2 and Figure 3. In Figure 2, with \(=0.5\), **AT-GP** refers to AT with GP; for **AT-GP-pre**, we perform \(50\) epochs of NT before doing **AT-GP**. We see **AT-GP** obtains a better accuracy/robustness tradeoff than **AT**. We observe a similar trend for **AT-GP-pre** vs. **AT-pre**. Further, in Figure 3, **RN-18**\(l_{}\)-**GP** achieves good clean accuracy and better robustness than **RN-18**\(l_{}\) against AutoAttack .

\[g_{p}=_{l}(_{n}^{l},_ {a}^{l}) f^{ (r+1)}=f^{(r)}+ g_{p}+(1-)_{a}\] (7)

```
1:Input: model \(f\), input samples \((x,y)\) from distribution \(}_{n}\), fine-tuning rounds \(R\), hyper-parameter \(\), adversarial regions \(B_{q},B_{r}\) with size \(_{q}\) and \(_{r}\), **APGD** attack.
2:for\(r=1,2,...,R\)do
3:for\((x,y)\) training set \(\)do
4:\(x^{}_{q},p_{q}(B_{q}(x,_{q}),y)\)
5:\(x^{}_{r},p_{r}(B_{r}(x,_{r}),y)\)
6:\( where(argmax\;p_{q}=y)\)
7:\(n_{c}.size()\)
8: calculate \(\) using Eq. 3 and update \(f\)
9:endfor
10:endfor
11:Output: model \(f\). ```

**Algorithm 1** Fine-tuning via Logit Pairing

```
1:Input: model \(f\), input images with distribution \(}_{n}\), training rounds \(R\), adversarial region \(B_{p}\) and its size \(_{p}\), \(\), natural training **NT** and adversarial training **AT**.
2:for\(r=1,2,...,R\)do
3:\(f_{n}(f^{(r)},)\)
4:\(f_{a}(f^{(r)},_{p},_{p},)\)
5: compute \(_{n} f_{n}-f^{(r)}\), \(_{a} f_{a}-f^{(r)}\)
6: compute \(g_{p}\) using Eq. 6
7: update \(f^{(r+1)}\) using Eq. 7 with \(\) and \(_{a}\)
8:endfor
9:Output: model \(f\). ```

**Algorithm 2** Connect AT with NT via GP

### Theoretical Analysis of GP for Adversarial Robustness

We define \(_{n}=\{(x_{i},y_{i})\}_{i=0}^{}\) as the ideal data distribution with an infinite cardinality. Here, we consider a classifier \(f_{}\) at epoch \(t\). We define \(_{a}\) as the distribution created by \(\{(x_{i}+(f_{},x_{i},y_{i}),y_{i})\}_{i=0}^{}\) where \((x_{i},y_{i})_{n}\). \(x_{i}+(f_{},x_{i},y_{i})\) denotes the perturbed image, which could be both single and multiple perturbations based on \(f_{}\) itself.

**Assumption 4.1**.: _We assume \(}_{n}\) consists of \(N\) i.i.d. samples from the ideal distribution \(_{n}\) and \(}_{a}=\{(x_{i}+(f^{},x_{i},y_{i}),y_{i})\}_ {i=0}^{N}\) where \((x_{i},y_{i})}_{n}\) consists of \(N\) i.i.d. samples from \(_{a}\)._

We define the population loss as \(_{D}():=_{(x,y)}(f(x),y)\), and let \(g_{}():=_{}()\). For simplification, we use \(g_{a}:=_{_{a}}()\), \(_{a}:=_{}_{a}}()\), and \(_{n}:=_{}_{n}}()\). \(g_{GP}= g_{p}+(1-)_{a}\) (Definition A.3) is the aggregation using GP. We define the following optimization problem.

**Definition 4.2** (Aggregation for NT and AT).: \(f_{}\) _is trained by iteratively updating the parameter_

\[-(_{a},_{n}),\]

_where \(\) is the step size. We seek an aggregation rule \(()=_{}\) such that after training, \(f_{}\) minimizes the population loss function \(_{_{a}}()\)._

We need \(_{}\) to be close to \(g_{a}\) for each iteration, since \(g_{a}\) is the optimal update on \(_{a}\). Thus, we define \(L^{}\)-Norm and delta error to indicate the performance of different aggregation rules.

**Definition 4.3** (\(L^{}\)-Norm [Enyi Jiang, 2024]).: _Given a distribution \(\) on the parameter space \(\), we define an inner product \( g_{},g_{^{}}_{}=_{ }[ g_{}(),g_{^{}}() ]\). The inner product induces the \(L^{}\)-norm on \(g_{}\) as \(\|g_{}\|_{}:=_{}\|g_{}( )\|^{2}}\). We use \(L^{}\)-norm to measure the gradient differences under certain \(\)._

**Definition 4.4** (Delta Error of an aggregation rule \(()\)).: _We define the following squared error term to measure the closeness between \(_{}\) and \(g_{a}\) under \(}_{a}^{i}\) (distribution at time step \(t\)), i.e.,_

\[^{2}_{}:=_{}_{a}^{i}}\|g_{a} -_{}\|_{}^{2}.\]

Delta errors \(^{2}_{AT}\) and \(^{2}_{GP}\) measure the closenessness of \(g_{GP},_{a}\) from \(g_{a}\) in \(}_{a}\) at each iteration.

**Theorem 4.5** (Error Analysis of GP).: _When the model dimension \(m\), for an epoch \(t\), we have an approximation of the error difference \(^{2}_{AT}-^{2}_{GP}\) as follows_

\[^{2}_{AT}-^{2}_{GP}(2-)_{}_{a}^{1}}\|g_{a}-_{a}\|_{}^{2}-^{2}^ {2}\|g_{a}-_{n}\|_{}^{2}\]

\(^{2}=_{}[^{2}]\)_, where \(()\) is the \(()\) value of the angle between \(_{n}\) and \(g_{a}-_{n}\)._

Theorem 4.5 shows \(^{2}_{GP}\) is generally smaller than \(^{2}_{AT}\) for a large model dimension during each iteration, as is the case for the models in our evaluation, with \(=0.5\), since \((1-)>^{2}(0.75>0.25)\) and the small value of \(\) in practice (see Interpretation of Theorem A.2 in Appendix A, where we show the order of difference is between \(1e^{-8}\) and \(1e^{-12}\)). Thus, GP achieves better robust accuracy than AT by achieving a smaller delta error; GP also obtains good clean accuracy by combining parts of the model updates from the clean distribution \(}_{n}\). Further, we provide an error analysis of a single gradient step in Theorem A.1 and convergence analysis in Theorem A.2, showing that a smaller Delta error results in better convergence. The full proof of all theorems is in Appendix A.

We outline the **AT-GP** method in Algorithm 2 and it can be extended to the multiple-norm scenario. The overhead of this algorithm comes from natural training and GP operation. Their costs are small, and we discuss this more in Section 5.2. Combining logit pairing and gradient projection methods, we provide the **RAMP** framework which is similar to Algorithm 2, except that we replace line 4 of Algorithm 2 as Algorithm 1 line 3-9.

## 5 Experiment

**Datasets, baselines, and models.** CIFAR-10 [Krizhevsky et al., 2009] includes \(60\)K images with \(50\)K and \(10\)K images for training and testing respectively. ImageNet has \( 14.2\)M images and \(1\)K classes, containing \( 1.3\)M training, \(50\)K validation, and \(100\)K test images [Russakovsky et al.,2015). We compare **RAMP** with following baselines: 1. **SAT**(Madaan et al., 2021): randomly sample one of the \(l_{1}\), \(l_{2}\), \(l_{}\) attacks. 2. **AVG**(Tramer and Boneh, 2019): take the average of \(l_{1},l_{2},l_{}\) examples. 3. **MAX**(Tramer and Boneh, 2019): take the worst of \(l_{1},l_{2},l_{}\) attacks. 4. **MSD**(Maini et al., 2020): find the worst-case examples over \(l_{1},l_{2},l_{}\) steepest descent directions during each step of inner maximization. 5. **E-AT**(Croce and Hein, 2022): randomly sample between \(l_{1}\), \(l_{}\) attacks. For models, we use PreAct-ResNet-18, ResNet-50, WideResNet-34-20, and WideResNet-70-16 for CIFAR-10, as well as ResNet-50 and XCiT-S transformer for ImageNet.

**Implementations and Evaluation.** For AT from scratch for CIFAR-10, we train PreAct ResNet-18 (He et al., 2016) with a \(lr=0.05\) for \(70\) epochs and \(0.005\) for \(10\) more epochs. We set \(=2\), \(=0.5\) for training from scratch, and \(=0.5\) for robust fine-tuning. For all methods, we use \(10\) steps for the inner maximization in AT. For ImageNet, we perform \(1\) epoch of fine-tuning and use a learning rate \(lr=0.005\), \(=0.5\) for ResNet-50 and \(lr=1e^{-4}\), \(=0.5\) for XCiT-S models. We reduce the rate by a factor of \(10\) every \(\) of the training epoch and set the weight decay to \(1e^{-4}\). We use APGD with \(5\) steps for \(l_{}\) and \(l_{2}\), \(15\) steps for \(l_{1}\). Settings are similar to (Croce and Hein, 2022). We use the standard values of \(_{1}=12,_{2}=0.5,_{}=\) for CIFAR-10 and \(_{1}=255,_{2}=2,_{}=\) for ImageNet. We focus on \(l_{}\)-AT models for fine-tuning, as Croce and Hein (2022) shows their higher union accuracy for the \(\) values in our evaluation. We report the clean accuracy, robust accuracy against \(\{l_{1},l_{2},l_{}\}\) attacks, union accuracy, universal robustness against common corruptions and unseen adversaries, as well as runtime for **RAMP**. The robust accuracy is evaluated using Autoattack (Croce and Hein, 2020). More implementation details are in Appendix B.

### Main Results

**Robust fine-tuning.** In Table 2, we apply **RAMP** to larger models and datasets (ImageNet). However, the implementation of other baselines is not publicly available and Croce and Hein (2022) do not report other baseline results except E-AT on larger models and datasets, so we only compare against E-AT in Table 2, which shows **RAMP** consistently obtains better union accuracy and accuracy-robustness tradeoff than E-AT. We observe that **RAMP** improves the performance more as the model becomes larger. We obtain the SOTA union accuracy of \(53.3\%\) on CIFAR-10 and \(29.1\%\) on ImageNet.

**RAMP with varying \(_{1},_{2},_{}\) values.** We provide results with 1. \((_{1}=12,_{2}=0.5,_{}=)\) where \(_{}\) size is small and 2. \((_{1}=12,_{2}=1.5,_{}=)\) where \(_{2}\) size is large, using PreAct ResNet-18 model for CIFAR-10 dataset: these cases have different tradeoff pair compared to

  & )\)} \\  & & Clean & \(l_{}\) & \(l_{2}\) & \(l_{1}\) & Union & Clean & \(l_{}\) & \(l_{2}\) & \(l_{1}\) & Union \\   & E-AT & 87.2 & 73.3 & 64.1 & 55.4 & 55.4 & 83.5 & 41.0 & 25.5 & 52.9 & 25.5 \\  & MAX & 85.6 & 72.1 & 63.6 & 56.4 & 56.4 & 74.6 & 42.9 & 35.7 & 50.3 & 35.6 \\  & **RAMP** & 86.3 & 73.3 & 64.9 & 59.1 & **59.1** & 74.4 & 43.4 & 37.2 & 51.1 & **37.1** \\   & E-AT & 86.5 & 74.8 & 66.7 & 57.9 & 57.9 & 80.2 & 42.8 & 31.5 & 52.4 & 31.5 \\  & MAX & 85.7 & 74.0 & 66.2 & 60.0 & 60.0 & 74.8 & 43.8 & 36.7 & 50.2 & 36.6 \\  & **RAMP** & 85.8 & 74.0 & 66.2 & 60.1 & **60.1** & 74.9 & 43.7 & 37.0 & 50.2 & **36.9** \\  

Table 1: **Different epsilon values**: **RAMP** consistently outperforms E-AT and MAX for both training from scratch and robust fine-tuning when the key tradeoff pair changes.

**Universal Robustness.** In Table 4, we report average accuracy against common corruptions and union accuracy against unseen adversaries from Laidlaw et al. (2020) (implementation details are in Appendix B.3). We compare against \(l_{p}\) pretrained models, E-AT, MAX, winningband (Diflenderfer et al., 2021) (a SOTA method for natural corruptions) using WideResNet-28-10 architecture on the CIFAR-10 dataset. Compared to E-AT and MAX, **RAMP** achieves \(4\%\) higher accuracy for common corruptions with five severity levels and \(2\)-\(4\%\) better union accuracy against multiple unseen adversaries. Winninghand has high corruption robustness but no adversarial robustness. The results show that **RAMP** obtains a better robustness and accuracy tradeoff with stronger universal robustness. In Appendix B.3, we evaluate on ResNet-18 to support this fact further.

### Ablation Study and Discussion

**Sensitivities of \(\).** We perform experiments with different \(\) values in \([0.1,0.5,1.0,1.5,2,3,4,5]\) for robust fine-tuning and \([1.5,2,3,4,5,6]\) for AT from scratch using PreAct-ResNet-18 model for CIFAR-10 dataset. In Figure 4, we observe a decreased clean accuracy when \(\) becomes larger. We

 Methods &  & \(l_{0}\) & fog & swift & fabric & fabric & jpegiff & Avg & Union \\  \(l_{1}\)-AT & 78.2 & 79.0 & 41.4 & 22.9 & 40.5 & 48.9 & 48.4 & 46.9 & 12.8 \\ \(l_{2}\)-AT & 77.2 & 67.5 & 48.7 & 26.1 & 44.1 & 53.2 & 45.4 & 47.5 & 16.2 \\ \(l_{3}\)-AT & 73.4 & 55.5 & 44.7 & 32.9 & 53.8 & 56.6 & 33.4 & 46.2 & 19.1 \\ Winninghand (Diflenderfer et al., 2021) & **91.1** & 74.1 & 74.5 & 18.3 & 76.5 & 12.6 & 0.0 & 42.7 & 0.0 \\ E-AT & 71.5 & 58.5 & 35.9 & 35.3 & 50.7 & 55.7 & 60.3 & 49.4 & 21.9 \\ MAX & 71.0 & 56.2 & 42.9 & 35.4 & 49.8 & 57.8 & 55.7 & 49.6 & 24.4 \\
**RAMP** & 75.5 & 55.5 & 40.5 & 40.2 & 52.9 & 60.3 & 56.1 & **50.9** & **26.1** \\  

Table 4: Individual, average, and union accuracy against common corruptions (averaged across five levels) and unseen adversaries using WideResNet-28-10 on CIFAR-10 dataset.

 Methods &  & \(l_{}\) & \(l_{2}\) & \(l_{1}\) & Union \\  SAT & 83.9\(\)0.8 & 40.7\(\)0.7 & 68.0\(\)0.4 & 54.0\(\)1.2 & 40.4\(\)0.7 \\ AVG & 84.6\(\)0.3 & 40.8\(\)0.7 & 68.4\(\)0.7 & 52.1\(\)0.4 & 40.1\(\)0.8 \\ MAX & 80.4\(\)0.3 & 45.7\(\)0.9 & 66.0\(\)0.4 & 48.6\(\)0.8 & 44.0\(\)0.7 \\ MSD & 81.1\(\)1.1 & 44.9\(\)0.6 & 65.9\(\)0.6 & 49.5\(\)1.2 & 43.9\(\)0.8 \\ E-AT & 82.2\(\)1.8 & 42.7\(\)0.7 & 67.5\(\)0.5 & 53.6\(\)0.1 & 42.4\(\)0.6 \\
**RAMP** (\(\)\(\)5) & 81.2\(\)0.3 & 46.0\(\)0.5 & 65.8\(\)0.2 & 48.3\(\)0.6 & **44.6\(\)0.6** \\
**RAMP** (\(\)\(\)2) & 82.1\(\)0.3 & 45.5\(\)0.3 & 66.6\(\)0.3 & 48.4\(\)0.2 & 44.0\(\)0.2 \\  

Table 3: **RN-18 model trained from random initialization on CIFAR-10 over 5 trials: **RAMP** achieves the best union robustness and good clean accuracy compared with other baselines. Baseline results are from Croce and Hein (2022).

Figure 1: The pair identified using our heuristic are \(l_{1}\) - \(l_{2}\) and \(l_{2}\) - \(l_{}\). In Table 1, we observe that **RAMP** consistently outperforms E-AT and MAX with significant margins in union accuracy, when training from scratch and performing robust fine-tuning. In Table 1, when \(l_{2}\) is the bottleneck, E-AT obtains a lower union accuracy as it does not leverage \(l_{2}\) examples. Similar observations are made across various epsilon values, with **RAMP** consistently outperforming other baselines, as detailed in Appendix B.4. Appendix B includes more training details/results, and ablation studies. Results for applying the trades loss to **RAMP** outperforming E-AT are detailed in Appendix B.6. Appendix B presents robust fine-tuning using ResNet-18, where **RAMP** achieves the highest union accuracy.

 Methods &  & \(l_{}\) & \(l_{2}\) & \(l_{1}\) & Union \\  SAT & 83.9\(\)0.8 & 40.7\(\)0.7 & 68.0\(\)0.4 & 54.0\(\)1.2 & 40.4\(\)0.7 \\ AVG & 84.6\(\)0.3 & 40.8\(\)0.7 & 68.4\(\)0.7 & 52.1\(\)0.4 & 40.1\(\)0.8 \\ MAX & 80.4\(\)0.3 & 45.7\(\)0.9 & 66.0\(\)0.4 & 48.6\(\)0.8 & 44.0\(\)0.7 \\ MSD & 81.1\(\)1.1 & 44.9\(\)0.6 & 65.9\(\)0.6 & 49.5\(\)1.2 & 43.9\(\)0.8 \\ E-AT & 82.2\(\)1.8 & 42.7\(\)0.7 & 67.5\(\)0.5 & 53.6\(\)0.1 & 42.4\(\)0.6 \\
**RAMP** (\(\)\(\)5) & 81.2\(\)0.3 & 46.0\(\)0.5 & 65.8\(\)0.2 & 48.3\(\)0.6 & **44.6\(\)0.6** \\
**RAMP** (\(\)\(\)2) & 82.1\(\)0.3 & 45.5\(\)0.3 & 66.6\(\)0.3 & 48.4\(\)0.2 & 44.0\(\)0.2 \\  

Table 4: Individual, average, and union accuracy against common corruptions (averaged across five levels) and unseen adversaries using WideResNet-28-10 on CIFAR-10 dataset.

pick \(=2.0\) for training from scratch (Figure 3(a)) and \(=0.5\) for robust fine-tuning (Figure 3(b)) in our main experiments, as these values of \(\) yield both good clean and union accuracy.

**Choices of \(\).** Figure 3(c) shows the performance of **RAMP** with varying \(\) values on CIFAR-10 ResNet-18 experiments. We pick \(=0.5\) for combining natural training and AT via GP, which achieves comparatively good robustness and clean accuracy. This choice is also based on Theorem 4.5 when \((2-)\) has the largest difference from \(^{2}\) (0.75 vs 0.25).

**Fine-tune \(l_{p}\) AT models with RAMP.** Table 5 shows the robust fine-tuning results using **RAMP** with \(l_{}\)-AT (\(q=,r=1\)), \(l_{1}\)-AT (\(q=1,r=\)), \(l_{2}\)-AT (\(q=,r=1\)) RN-18 models for CIFAR-10 dataset. For \(l_{}-l_{1}\) tradeoffs, RAMP on \(l_{}\)-AT pre-trained model achieves the best union accuracy.

**Computational analysis and Limitations.** The extra training costs of AT-GP are small, e.g. for each epoch on ResNet-18, the extra NT takes \(6\) seconds and the standard AT takes \(78\) seconds using a single NVIDIA A100 GPU, and the **GP** operation only takes \(0.04\) seconds on average. RAMP is more expensive than E-AT and less expensive than MAX. We have a complete runtime analysis in Appendix B.2. We notice occasional drops in clean accuracy during fine-tuning with **RAMP**. In some cases, union accuracy improves slightly but clean accuracy and single \(l_{p}\) robustness reduce. Further, we find no negative societal impact from this work.

## 6 Conclusion

We introduce **RAMP**, a framework enhancing multiple-norm robustness and achieving superior _universal robustness_ against corruptions and perturbations by addressing tradeoffs among \(l_{p}\) perturbations and accuracy/robustness. We apply a new logit pairing loss and use gradient projection to obtain SOTA union accuracy with favorable accuracy/robustness tradeoffs against common corruptions and other unseen adversaries. Results demonstrate that **RAMP** surpasses SOTA methods in union accuracy across model architectures on CIFAR-10 and ImageNet.