# CIFD: Controlled Information Flow to Enhance Knowledge Distillation

Yashas Malur Saidutta & Rakshith S Srinivasa & Jaejin Cho & Ching-Hua Lee & Chouchang Yang & Yilin Shen & Hongxia Jin &

Samsung Research America, Mountain View, CA

{ym.saidutta, r.srinivasa, jaejin.cho, chinghua.l}@samsung.com

{c.yang1, yilin.shen, hongxia.jin}@samsung.com

###### Abstract

Knowledge Distillation is the mechanism by which the insights gained from a larger teacher model are transferred to a smaller student model. However, the transfer suffers when the teacher model is significantly larger than the student. To overcome this, prior works have proposed training intermediately sized models, Teacher Assistants (TAs) to help the transfer process. However, training TAs is expensive, as training these models is a knowledge transfer task in itself. Further, these TAs are larger than the student model and training them especially in large data settings can be computationally intensive. In this paper, we propose a novel framework called Controlled Information Flow for Knowledge Distillation (CIFD) consisting of two components. First, we propose a significantly smaller alternatives to TAs, the Rate-Distortion Module (RDM) which uses the teacher's penultimate layer embedding and a information rate-constrained bottleneck layer to replace the Teacher Assistant model. RDMs are smaller and easier to train than TAs, especially in large data regimes, since they operate on the teacher embeddings and do not need to relearn low level input feature extractors. Also, by varying the information rate across the bottleneck, RDMs can replace TAs of different sizes. Secondly, we propose the use of Information Bottleneck Module in the student model, which is crucial for regularization in the presence of a large number of RDMs. We show comprehensive state-of-the-art results of the proposed method over large datasets like Imagenet. Further, we show the significant improvement in distilling CLIP like models over a huge 12M image-text dataset. It outperforms CLIP specialized distillation methods across five zero-shot classification datasets and two zero-shot image-text retrieval datasets.

## 1 Introduction

A decade ago Hinton et al. proposed a mechanism called Knowledge Distillation (KD), where the insights from a larger model (the teacher) are transferred to a smaller model (the student) . The teacher model, on account of larger modeling capacity, is capable of learning complex relationships in the data and is thus better at performing its target task. KD attempts to help the training of the student model by showing it the insights learned by the teacher. Specifically, Hinton et al. suggested the transfer of "dark knowledge" in the logits of a teacher classifier to a student classifier by minimizing the Kullback-Leibler (KL) divergence between a softened version of the predicted probability distribution of a teacher and a student model. However, dark knowledge is only one way of quantifying the insights learned by the teacher. In general, how to quantify the insights and how to transfer them to the student model remain open questions.

Recently many researchers in the knowledge distillation community have been concerned with the gap between the teacher and student's modeling capacities [2; 3; 4; 5]. Going back to the human learning experience, we learn simple concepts first, intermediate concepts next, and finally advanced concepts. Based on this insight, prior works have proposed the use of Teacher Assistants (TAs), models whose size is between that of the teacher and the student, to help facilitate the knowledge transfer better [2; 3]. By distilling knowledge from the teacher to the assistant and then using the assistant to help the distillation into the student led to large performance improvement. However, as seen in Figure 1(b), these methods are around \(3.5\) and \(4.5\) more expensive than KD . In this paper we introduce a method called Controlled Information Flow for Knowledge Distillation (CIFD) which is significantly cheaper to train. This brings the training cost of TA based methods closer to KD and other counterparts while obtaining state-of-the-art performance.

The proposed CIFD mechanism consists of two parts. First, we propose to process the teacher's penultimate layer embedding by a Rate-Distortion Module (RDM) to replace TAs. The RDM imposes a constraint on the amount of information through it. To replace multiple TAs of different capacities, we propose using RDMs with different levels of information constraints. Since RDMs operate on the teacher embeddings, they do not have to relearn low level features and can avoid the associated training and inference costs. As far as we know, this is the first application of Shannon's Rate-Distortion theory to aid knowledge distillation. However, as the number of RDMs increases, the student model tends to overfit to the teacher and assistants' outputs, as pointed out in  (also confirmed empirically here), thus reducing the generalization performance of the student model. Thus, to regularize the training of the student model in the presence of multiple RDMs, we propose the Information Bottleneck Module (IBM). The following are the contributions of this paper:

1. We propose the use of an RDM that takes the embeddings from a teacher model and mimics a teacher assistant. Since the RDM does not have to relearn low level features, it is two to three layers only; significantly smaller than a teacher assistant model.
2. We propose the use of the IBM in the student model during train-time. We find that IBM on its own provides benefits but is also a crucial regularizer as the number of RDMs increases.
3. On classification models, by distilling ResNet34 and ResNet50 (teachers) into ResNet18 and MobileNet-V2 (students), we achieve +1.66% (absolute) and +2.71% over KD , respectively.
4. To showcase the generality of the proposed CIFD, we apply it to distilling CLIP like models. Over three different teacher-student combinations, across five zero-shot classification and two zero-shot retrieval datasets, we find that our proposed method significantly outperforms CLIP specific distillation methods.

Figure 1: (a) Proposed: Controlled Information Flow for Knowledge Distillation (CIFD). In CIFD, we explore the idea of Rate-Distortion Modules (RDM) that use the Teacher’s embeddings to generate embeddings that act like Teacher Assistants (TAs) to distill knowledge to the Student. It does so by processing the embedding through a rate constrained communication channel. RDMs are much cheaper to train as they reuse the feature extractors learned by the teacher. By varying the rate constraint, RDMs can can simulate different TAs, and enable “TA” based training. (b) Training cost (normalized w.r.t. KD ) for distilling ResNet34 to ResNet18 over ImageNet. Earlier TA based approaches incurred a huge training cost increase due to the training of the TA models from scratch. Our proposed idea replaced TAs with RDMs and thus significantly reduces distillation cost while also improving performance.

## 2 Related Works

**Knowledge Distillation:** Hinton et al. introduced the concept of transfering the knowledge from a large teacher model to a smaller student model using the logit space for classification models . Many works have focused on different methods for transferring information like in the output space , transferring information in the intermediate layers of the teacher to the student [7; 8], to note a few. Some authors have focused on efficient distillation when only a subset of the original training data of the teacher is available , distilling easy classes first , or recently by looking at feature distillation as a diffusion process .

Recently, many works have pointed out the gap between the teacher and student models. Some authors looked at encoding the residual error between a large teacher and student [12; 13], using an ensemble of projectors for feature distillation , proposing a correlation based loss function in place of the KL divergence , and rectifying the imbalance at the concept level . Most related to our work is the work of Mirzadeh et al. , and Son et al. . Mirzadeh et al. proposed training teacher assistant models of progressively smaller size between that of a teacher and student model and using the smallest teacher assistant for distilling knowledge to the student. Son et al. argued that such a distillation process could cause error cascading and proposed using all the assistant(s) and the teacher for distillation. Further, each assistant is trained using all larger assistants and the teacher. This leads to progressively increasing computation costs as the number of assistants increases. Unlike their works, instead of explicitly training assistant models whose size is larger than the student, we propose a mechanism to mimic intermediate teaching assistants by training a small RDM module which is significantly smaller than even the student model. Further, unlike  where TAs need to be trained sequentially in the order of decreasing size, our RDMs are trained independently and in parallel. Another important work that is related to the RDM is the paraphraser network proposed by . However we propose a more principled (from Shannon's insight on Rate-Distortion theory) loss function to train the RDMs where the information is compressed at different levels to mimic teacher assistants of different capacities.

The proposed IBM also has parallels in works like [6; 15]. The former looks at introducing an ensemble of projector networks that map the student embedding to the teacher embedding. Unlike the proposed IBM, which has a rate constrained loss function, the projector networks are trained for reconstruction error only. The concept behind IBM, the Information Bottleneck Principle (IBP), is also related to Masked-Image-Modeling  and Masked Generative Distillation . The connections between IBP and improved generalization  support the success behind MIM and our proposed IBM. While our IBM loss function attempts to directly upper-bound each of the terms of the IBP objective, MIM upper bounds by dropping negative terms from the IBP objective. On the otherhand,

Figure 2: Training schemes of the proposed CIFD framework. We first train the RDM modules to mimic teacher assistants as in (a). Then we train the student model using both the trained RDMs and the teacher model as in (b).

the MGD objective is an upperbound only under certain modeling assumptions. We discuss these connections in detail in Section 3.2 and Appendix C.

**CLIP Distillation:** Distillation of CLIP like models is a relatively new direction. One of the earliest works was to use CLIP to distill task specific knowledge into task specific student models . In the task agnostic distillation of CLIP, TinyClip proposed a knowledge distillation and a pruning method for CLIP . Yang et al. studied various losses for distillation including feature distillation and relational losses . Relational losses attempt to maintain the same relative distances between embeddings in the teacher and student model [21; 22]. Another noteworthy paper is MobileCLIP which used an ensemble of teachers and a data refinement technique to obtain powerful small CLIP models . However, the ideas proposed in these works are CLIP specific, i.e., they exploit the interactions between different modalities to compute the distillation losses. Unlike these works, our proposed idea is general, i.e., not CLIP specific.

**Rate Distortion and Information Bottleneck via Neural Networks:** Neural networks have been used to learn encoders and decoders for compression using Rate-Distortion theory [24; 25; 26] and communication over noisy channels . On the other hand, Information Bottleneck has been used to improve neural networks in areas like improving generalization [28; 29], Out-of-Distribution detection [30; 31] and Out-of-Distribution generalization [32; 33].

## 3 Controlled Information Flow for KD

### Controlling the information from the teacher using Rate-Distortion Theory

Shannon proposed Rate-Distortion Theory as a principled way to compress a signal. Given an input \(X\), the goal of compression is to find a mapping from \(X\) to its compressed version \(\) such that \(\) has minimal information about \(X\) but at the same time the distortion does not exceed \(D_{0}\). We can write this as an optimization problem of the form

\[_{X:D(X;) D_{0}}I(X;),\] (1)

where \(D(,)\) is some distortion measure and \(I(;)\) denotes the mutual information between two random variables . We can convert this to an unconstrained optimization objective of the form

\[ R D(X;)+I(X;),\] (2)

where \(R\) determines the trade-off between information rate and distortion. A larger \(R\) corresponds to more emphasis on minimizing the distortion at the cost of a higher information rate and vice-versa. This idea is best understood in the context of lossy compression where we wish to compress as much as possible while allowing tolerable distortion. However, rate-distortion theory is not only applicable to compression, but also to problems like Joint Source-Channel Coding where the compressed representation is subject to noise . This case is represented in Figure 1(a). The encoded representation of the input is denoted as \(Y\) and its noisy version as \(\). The independent noise added to encoded representation is denoted as \(Z\).

However, even though Shannon's theory tells us what optimization problem to solve, it does not tell us how to solve it. When the encoders and decoders in Figure 1(a) are neural networks this problem is compounded because the objective cannot be computed (\(I(X;)\) is intractable). Instead, we can use variational approximations to compute an upper bound on the objective, which in turn will allow us to perform gradient descent to learn the encoder and the decoder. Let us denote \(q()\) as an approximation of true but unknown distribution of \(\), \(p()\). Then, we can compute an upper-bound on \(I(X;)\) as

\[I(X;) I(Y;) H_{q}()+H(|Y),\] (3)

where \(H()\) denotes the entropy, and \(H_{q}()\) denotes the cross entropy computed using the distribution \(q()\). The first inequality follows from the Data Processing Inequality  and the second follows because cross-entropy is always greater than entropy . Finally, note that \(H(|Y)\) is constant because the noise is independent of the encoder and decoder parameters.

The key here is to choose the approximating distribution \(q\). Popular choices include the Gaussian distribution (e.g., Variational Autoencoders ), learning the distribution , or non-parametric approximations , we choose the latter. All these methods yield a mechanism where we can compute \(q()\). Now, for simplicity let us assume that distortion measure \(D\) is the \(L_{2}\) norm, then we can learn the parameters of the encoder (\(_{e}\)) and the decoder (\(_{d}\)) by putting (3) into (2) as

\[_{R}=_{X,Z}[R\|X-\|_{2}^{2}- (q())].\] (4)

Figure 1(a) shows how the RDMs are trained. Since the RDMs process extracted teacher embeddings, they do not have to relearn low level feature extractors from the raw input, thus making them significantly smaller than teacher assistants, which in turn makes them computationally cheaper.

In the case where we are distilling a classification model, an additional linear layer is used to convert the reconstructed embeddings to logits. Let \(V\) represent the true classification label, \(_{T}\) represent the teacher's output predictive distribution on the class labels, \(_{RDM}\) be the same but as predicted by the RDM output. Then, the loss function used to train the RDM is

\[}_{R}=_{X,Z}[_{CE}(V,_{RDM })+_{KL}KL(_{T}||_{RDM})]+_{R}.\] (5)

Here, \(_{CE}(,)\) is the cross-entropy loss, \(KL(||)\) is the Kullback-Leibler divergence, and \(_{KL}\) is weighting factor for the KL loss. For simplicity of writing we have dropped the presence of the temperature \(\) from the KL divergence term, however, it is assumed to be present. Figure 7 shows the illustration of how the RDM is trained for classification.

**Connections between Teacher Assistants and RDMs:** TAs limit the amount of information extracted from the input by using limited modeling capacity, i.e., smaller the model poorer the TA performance on the downstream task. On the other hand, we limit the amount of information extracted from the teacher embedding by passing it through a rate constrained communication channel. If we place a higher constraint on the information through the channel, our reconstructed embeddings will have more distortion compared to the teacher's and will perform poorly on the downstream task, just like the embeddings from a TA with a small model capacity. Thus, by choosing different values of \(R\) in (4) we can mimic different TAs with different modeling capacities.

### Controlling the information to the student using Information Bottleneck

In the previous section, we introduced RDMs to control the flow of information from the teacher model and mimic teacher assistants. However, when the teacher and multiple RDMs are providing feedback, the feedback can cause the student model to overfit and lead to poor performance. We can overcome this by constraining the information from the student model exposed to the feedback, i.e., provide partial feedback. Similar to the teacher model, we can accomplish this using another rate-constrained channel, but at the student. Further, this rate-constrained model is present only when there is feedback to the student, i.e., only during training. However, unlike the case of RDMs, we are not interested in trying to reconstruct the input to the rate constrained channel. Instead we wish to reconstruct the teacher or RDM embeddings. In Information Theory, the Rate-Distortion problem deals with compressing a random variable. However, when we want the compressed representation to be informative about another variable, it is called the Information Bottleneck problem. Thus, we call our proposed rate constrained module in the student model as Information Bottleneck Module (IBM).

Tishby et al. introduced the Information Bottleneck Principle (IBP) as a generalization to the Rate-Distortion problem . Given an input \(X_{S}\) and some random variable of interest \(U\), the goal in IB is to find a representation \(\) that removes as much information about input \(X_{S}\) while retaining as much information about \(U\). This is formulated as

\[-I(U;)+_{I}I(X_{S};),\] (6)

where, \(_{I}\) is the Lagrange multiplier.

Figure 1 shows the IB Module in the student model. \(X_{S}\) represents the input, \(W\) the encoded representation, \(\) the noisy encoded representation, \(Z_{S}\) the noise added during training only, and \(\) represents the output of the IBM decoder. When working with neural network based encoders and decoders, both the mutual information terms in (6) are intractable. To overcome this setback, we use variational approximations, resulting in an upper bound on the IB objective (6) .

\[_{I}=_{X,Z_{S}}[D(U,)-_{I}(r( ))],\] (7)where \(D\) represents a suitable distortion metric, \(r()\) is the approximation of the true distribution \(p()\). In the IBM, \(U\) is set to be the teacher's (or RDM's) embedding. So, at the output the IBM is attempting to reconstruct the teacher's (or RDM's) embedding. In case of a dimensionality mismatch, a projection layer is used. In practice, we do not use a dedicated encoder for IBM and let the student backbone model itself function as the encoder, i.e. \(W=X_{S}\). The decoder is a simple network of at most one to two linear layers. The IBM is not trained separately, instead it is trained along with the student model.

**Connections to Masked-Image-Modeling:** In this discussion, we compare IBMs with Masked-Image-Modeling  and its extension into distillation Masked-Generate-Modeling . In MIM, an input image (\(T\)) is masked (\(T_{M}\)). The masked image is fed through an encoder followed by an MIM encoder. The encoder provides an embedding of the image and the MIM encoder uses it to predict the tokens corresponding to the masked out part. The system is then trained to maximize the likelihood of the actual values of the masked out tokens. In Masked-Generative-Distillation, the masking is done after passing through the student backbone. Following that a generator attempts to predict the teacher's embedding (with a slight abuse of notation \(U\)). Let us denote the embedding from the student before the masking as \(_{-M}\). Even though both MIM and MGD attempt to maximize the \( p(U|)\), we can easily show that this is equivalent to maximizing \(I(U;)\) (Lemma 1 in Appendix C). Following this we can write the following theorem to connect the MIM and MGD objectives to IBP.

**Theorem 1** (Informal).: _The objective function of MIM is an upper bound on the objective function from IBP. The objective function of MGD is an upper bound on the objective function from IBP if \(_{-M}\) is a discrete random variable and the mapping from \(T\) to \(_{-M}\) is deterministic._

A more detailed explanation along with the formal statement and its proof are provided in Appendix C. The loss function of our IBM is directly written as an upper bound on the IBP objective. Unlike MIM or MGD, we do not drop the second term and instead use a non-parametric upper-bound to approximate it. This empirically should ensure a tighter approximation to the IBP objective than the other two. IBM works better because it forces the student to focus on those features necessary to predict the teacher embedding (first term) and remove information not useful in the prediction (second term), whereas in MIM and MGD the removal of information is either implicit or not present respectively. There have recently been works studying how IBP helps reduce generalization errors  which provides support that IBM in the student model should help reduce generalization errors. Our ablation studies in Section 4.3 also show similar results.

### Controlled Information Flow for KD

With all the components in place, we can now derive the final loss function. Using the methodology described in Section 3.1, we assume that \(N\) RDMs corresponding to different rate-constraints have been trained. To re-iterate, let \(X\) be the input datapoint. Let us denote the teacher model's embedding as \(X\); \(_{n}\), \(Z_{n}\) are the embedding and noise in the \(n^{}\) RDM respectively, and \(\) corresponds to the student model's embedding. Also for sake of illustration, let us assume that the loss to match embeddings is the \(L_{2}\) loss. Then, we can write the loss function for training the student model as

\[_{CIFD}=_{X,Z_{1}^{N},Z_{S}}[\|-X\| _{2}^{2}+_{n=1}^{n=N}_{n}\|-_{n}\|_{2}^{2} -_{I} r()],\] (8)

Figure 4: Relation between Masked Image Modeling (MIM), Masked Generative Distillation (MGD), and Information Bottleneck Module (IBM) for Distillation.

where \(_{n}\) are weighting coefficients.

In the case of classification, let us denote the output predictive distribution of the teacher as \(_{T}\), \(_{S}\) for the student, \(_{n}\) for the \(n^{}\) RDM, and the true label as \(V\). Then, we can write the loss function for training the student model as

\[}_{CIFD}=_{X,Z_{1}^{N},Z_{S}}[ _{CE}_{CE}(V,_{S})+_{KL}KL(_{T}|| _{S}).\\ +._{KL}_{n=1}^{n=N}_{n}KL(_{n}|| {V}_{S})]+_{CIFD}.\] (9)

#### 3.3.1 CIFD for CLIP style pretraining

CLIP (Contrastive Language-Image Pretraining) is a class of foundational models that are capable of embedding inputs from distinct modalities into a shared embedding space . In CLIP a modality specific encoder processes the input from a specific modality and embeds it into a shared embedding space. CLIP like models have shown tremendous performance in zero-shot classification, object-detection, and retrieval . Further, the trained encoders have also proved instrumental in powering Large Multimodal Models (LMMs)  and generative models . So distillation of these models has far reaching applications especially in on-device generative AI.

For simplicity, let us consider two modalities Image (\(\)) and Language (\(\)). Consider a batch of \(B\) image-text pairs \(\{(I^{(1)},L^{(1)}),,(I^{(B)},L^{(B)})\}\). Let \(_{}^{(b)}\) denote the \(L_{2}\)-normalized embedding of the \(b\)-th image obtained from the image encoder and \(_{}^{(b)}\) denote the same for the \(b\)-th text obtained from the language encoder (after IBM as in Figure 2(b)). Then we can write the contrastive loss from image to language embeddings as

\[_{CL,}=_{b=1}^{B} _{}^{(b)},_{}^{( b)}/)}{_{k[B]}(_{}^{(b)},_{}^{(k)}/)},\] (10)

where \(,\) represents the inner-product between the two vectors. Using this we can write the loss to train CLIP-like models as

\[_{CL}=_{CL,}+_{CL,}.\] (11)

The idea in contrastive loss is that embeddings of a paired image and text must be close to each other (high inner-product) when compared to embeddings of an image and/or text of non-pairs. Going forward we use CLIP to denote any CLIP like model.

Since the CLIP teacher has modality specific encoders, each encoder has its own set of RDMs. Each RDM is trained using (4). Let us denote the embedding for \(b\)-th image and text from the teacher by \(X_{1}^{(b)}\) and \(X_{}^{(b)}\), respectively. We can define the \(n\)-th RDM embeddings as \(X_{n,}^{(b)}\) and \(X_{n,}^{(b)}\). Similarly, since the student has modality specific encoders, it has modality specific IBMs. Let us define \(_{}\), \(_{}\) (similar to \(\) in Figure 1(a)) for the image and language encoder IBMs respectively. The output of the IBM decoder for the \(b\)-th image and text is denoted as \(_{}^{(b)}\) and \(_{}^{(b)}\), respectively. We can now write the modality specific CIFD loss for CLIP distillation as

\[_{CIFD,}=_{b=1}^{B}[\|_{ }^{(b)}-X_{}^{(b)}\|_{2}^{2}+_{n=1}^{n=N} _{n}\|_{}^{(b)}-X_{n,}^{(b)}\|_{2 }^{2}-_{I,} r_{}(_{})],\] (12)

Now we can write the final loss to perform distillation of CLIP using CIFD as

\[}_{CIFD}=_{CL}_{CL}+_{ CIFD,}+_{CIFD,},\] (13)

where \(_{CL}\) is weighting factor.

## 4 Experiments

**Experimental setup:** Our experimental results are split into two sections, one dealing with supervised classification on the CIFAR-100  and Imagenet (IN)  datasets, and another with CLIP like 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

In Table 5, we study the affect of number of RDMs on distilling CLIP like models. To reduce compute, we use a smaller teacher of size ViT-B-16 trained by . We see that increasing number of RDMs leads to improved performance. We also provide results from  for comparison.

**Analyzing CIFD gains with large teacher student capacity gap:** Here, we study the performance of CIFD when the teacher student gap is increased. In Table 7 we see that as the size of the teacher (and correspondingly its performance) increases, the student performance also increases. In fact, the increase is montonic w.r.t. the teacher size. We also compare with the performance of DistKD  which also studied a similar premise. We find that not only does our method outperform DistKD, that unlike DistKD the proposed CIFD does not show drop in performance when the teacher size is increased. This indicates the robustness of our proposed method. The parameter ratio of teacher to student ranges from \(1.86\) to \(5.12\). We also study CLIP like models where the maximum parameter ratio is a larger \(6.9\) in Appendix B.4. We see that for 3 out of the 5 zero-shot classification datasets CIFD yields larger improvements over the next best competitor of ClipKD  when the capacity gap is larger. In zero-shot image-text retrieval, CIFD almost always yields larger improvements over the ClipKD when the capacity gap is larger. This indicates that CIFD excels when the teacher-student capacity gap is large.

**Training cost analysis:** Although, prior works on Teacher Assistants like  showed promising results, training Teacher Assistants led to prohibitive increase in complexity as seen in Figure 1(b) where TA based methods are \(3.5\) and \(4.5\) more expensive than  when distilling from ResNet34 to ResNet18 on Imagenet. We computed these numbers based on the number of Multiply and Accumulations (MACs) for every forward pass, more details in Appendix D.2. On the otherhand, our method is only \(1.08\) more expensive than , thus bringing training cost of TA based methods close to other current innovations. For the CIFAR-100 experiments in Table 1, TAKD  is \(2\) more expensive and DGKD  is \(5\) more expensive than proposed.

## 5 Conclusion

In this paper we present a novel knowledge distillation framework called the Controlled Information Flow (CIFD). CIFD consists of two main components. The first is a lightweight Rate Distortion Module (RDM) that replaces the expensive Teacher Assistants by using the teacher model's embedding and a noisy communication channel. Importantly, since the RDM uses the teacher model's input embeddings, it does not have to relearn low level feature extractors, thus making the model significantly smaller than a TA. Second, we propose an Information Bottleneck Module to prevent the student model from overfitting in the presence of the teacher and multiple RDMs. The resulting framework shows impressive performance on Imagenet and significantly outperforms CLIP specific distillation methods on CLIP models. Finally, we corroborated that an increased number of RDMs with diverse Rs is a key factor for better distillation, entailing only a small increase in computation.

**Limitations and impact:** An interesting direction is to study alternative algorithmic formulations for using the information from RDMs, i.e., sequentially enabling and disabling which RDM give feedback at what points of training. Regarding societal impact, the student model depends on the teacher model to transfer concepts. Unfortunately, this also means biases present in the teacher due to its training data are also transferred to the student.

   Dataset & \# RDMs & w/ IBM & w/o IBM \\   & No RDM & **45.68** & 44.76 \\  & 1 RDM & **46.17** & 45.34 \\  & 3 RDM & **46.68** & 46.04 \\  & 5 RDM & **47.25** & 45.14 \\   & 1 RDM & **72.05** & 71.83 \\  & 3 RDM & **72.32** & 72.22 \\   

Table 6: Ablation study of number of RDMs and IBM. IBM is crucial when number of RDMs increases.