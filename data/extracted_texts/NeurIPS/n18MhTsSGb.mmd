# 2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression

Alexander Tyurin

KAUST

Saudi Arabia

alexandertiurin@gmail.com &Peter Richtarik

KAUST

Saudi Arabia

richtarik@gmail.com

###### Abstract

We consider distributed convex optimization problems in the regime when the communication between the server and the workers is expensive in both uplink and downlink directions. We develop a new and provably accelerated method, which we call 2Direction, based on fast bidirectional compressed communication and a new bespoke error-feedback mechanism which may be of independent interest. Indeed, we find that the EF and EF21-P mechanisms (Seide et al., 2014; Gruntkowska et al., 2023) that have considerable success in the design of efficient non-accelerated methods are not appropriate for accelerated methods. In particular, we prove that 2Direction improves the previous state-of-the-art communication complexity \((K(}{{}}+}}{{n}}+))\)(Gruntkowska et al., 2023) to \((K(}{{}}+ ^{2}}}}{{n}}+}{{}}+ ))\) in the \(\)-strongly-convex setting, where \(L\) and \(L_{}\) are smoothness constants, \(n\) is # of workers, \(\) and \(\) are compression errors of the Rand\(K\) and Top\(K\) sparsifiers (as examples), \(K\) is # of coordinates/bits that the server and workers send to each other. Moreover, our method is the first that improves upon the communication complexity of the vanilla accelerated gradient descent (AGD) method (Nesterov, 2018). We obtain similar improvements in the general convex regime as well. Finally, our theoretical findings are corroborated by experimental evidence.

## 1 Introduction

We consider convex optimization problems in the centralized distributed setting. These types of problems appear in federated learning (Konecny et al., 2016; McMahan et al., 2017) and distributed optimization (Ramesh et al., 2021). In this setting, one of the main problems is the communication bottleneck: the connection link between the server and the workers can be very slow. We focus our attention on methods that aim to address this issue by applying _lossy compression_ to the communicated messages (Alistarh et al., 2017; Mishchenko et al., 2019; Gruntkowska et al., 2023).

### The problem

Formally, we consider the optimization problem

\[_{x^{d}}\{f(x):=_{i=1}^{n}f_{i}(x )\},\] (1)

where \(n\) is the number of workers and \(f_{i}\,:\,^{d}\) are smooth convex functions for all \(i[n]:=\{1,,n\}\). We consider the _centralized distributed optimization_ setting in which each \(i^{}\) worker contains the function \(f_{i},\) and all workers are directly connected to a server (Kairouz et al., 2021). In general, we want to find a (possibly random) point \(\) such that \([f()]-f(x^{*}),\) where \(x^{*}\) isan optimal point. In the strongly convex setup, we also want to guarantee that \([\|-x^{*}\|^{2}]\) for some point \(\).

Virtually all other theoretical works in this genre assume that, compared to the worker-to-server (w2s) communication cost, the server-to-workers (s2w) broadcast is so fast that it can be ignored. We lift this limitation and instead associate a relative cost \(r\) with the two directions of communication. If \(r=0\), then s2w communication is free, if \(r=1\), then w2s communication is free, and if \(r=}{{2}}\), then the s2w and w2s costs are equal. All our theoretical results hold for any \(r\). We formalize and elaborate upon this setup in Section 2.

### Assumptions

Throughout the paper we rely on several standard assumptions on the functions \(f_{i}\) and \(f\).

**Assumption 1.1**.: Functions \(f_{i}\) are \(L_{i}\)-smooth, i.e., \(\| f_{i}(x)- f_{i}(y)\| L_{i}\,\|x-y\|\) for all \(x,y^{d}\), for all \(i[n]\). We let \(L_{}:=_{i[n]}L_{i}\). Further, let \(>0\) be a constant such that \(_{i=1}^{n}\| f_{i}(x)- f_{i}(y)\|^{2}^{2}\,\|x-y\|^{2}\) for all \(x,y^{d}\).

Note that if the functions \(f_{i}\) are \(L_{i}\)-smooth for all \(i[n]\), then \( L_{}\).

**Assumption 1.2**.: Function \(f\) is \(L\)-smooth, i.e., \(\| f(x)- f(y)\| L\,\|x-y\|\) for all \(x,y^{d}\).

**Assumption 1.3**.: Functions \(f_{i}\) are convex for all \(i[n],\) and \(f\) is \(\)-strongly convex with \( 0\), attaining a minimum at some point \(x^{*}^{d}\).

It is known that the above smoothness constants are related in the following way.

**Lemma 1.4** (Gruntkowska et al. (2023)).: _If Assumptions 1.2, 1.1 and 1.3 hold, then \( L_{} nL\) and \(LL}\)._

## 2 Motivation: From Unidirectional to Bidirectional Compression

In this work, we distinguish between worker-to-server (w2s=uplink) and server-to-worker (s2w=downlink) communication cost, and define w2s and s2w communication complexities of methods in the following natural way.

**Definition 2.1**.: For a centralized distributed method \(\) aiming to solve problem (1), the communication complexity \(_{}^{ 2}\) is the expected number of coordinates/floats1 that each worker sends to the server to solve problem (1). The quantity \(_{}^{}\) is the expected number of floats/coordinates the server broadcasts to the workers to solve problem (1). If \(_{}^{}=_{}^{},\) then we use the simplified notation \(_{}:=_{}^{}=_{}^{}\).

Let us illustrate the above concepts on the simplest baseline: vanilla gradient descent (GD). It is well known (Nesterov, 2018) that for \(L\)-smooth, \(\)-strongly convex problems, GD returns an \(\)-solution after \((}{{}}}{{}})\) iterations. In each iteration, the workers and the server communicate all \((d)\) coordinates to each other (since no compression is applied). Therefore, the communication complexity of GD is \(_{0}=(}{{}}}{{}}).\) The same reasoning applies to the accelerated gradient method (AGD) (Nesterov, 2018), whose communication complexity is \(_{}=(d}{{}}}}{{}}).\)

### Compression mappings

In the literature, researchers often use the following two families of compressors:

**Definition 2.2**.: A (possibly) stochastic mapping \(\;:\;^{d}^{d}\) is a _biased compressor_ if there exists \((0,1]\) such that

\[[\|(x)-x\|^{2}](1-)\| x\|^{2}, x^{d}.\] (2)

**Definition 2.3**.: A stochastic mapping \(\;:\;^{d}^{d}\) is an _unbiased compressor_ if there exists \( 0\) such that

\[[(x)]=x,[\| (x)-x\|^{2}]\|x\|^{2},  x^{d}.\] (3)We will make use of the following assumption.

**Assumption 2.4**.: The randomness in all compressors used in our method is drawn independently.

Let us denote the set of mappings satisfying Definition 2.2 and 2.3 by \(()\) and \(()\), respectively. The family of biased compressors \(\) is wider. Indeed, it is well known if \(()\), then \(}{{(+1)}}(}{{ (+1)}})\). The canonical sparsification operators belonging to these classes are \(K(}{{d}})\) and \(K(}{{K}}-1)\). The former outputs the \(K\) largest values (in magnitude) of the input vector, while the latter outputs \(K\) random values of the input vector, scaled by \(}{{K}}\)(Beznosikov et al., 2020). Following (Gorbunov et al., 2021; Tyurin and Richtarik, 2023), we now define the _expected density_ of a sparsifier as a way to formalize its _compression_ performance.

**Definition 2.5**.: The expected density of a sparsifier \(:^{d}^{d}\) is the quantity \(K_{}:=_{x^{d}}[\|( x)\|_{0}],\) where \(\|y\|_{0}\) is the number of of non-zero components of \(y^{d}\).

Trivially, for the Rand\(K\) and Top\(K\) sparsifiers we have \(K_{}=K\).

### Unidirectional (i.e., w2s) compression

As mentioned in the introduction, virtually all theoretical works in the area of compressed communication ignore s2w communication cost and instead aim to minimize \(_{}^{}\). Algorithmic work related

  
**Method** & **\# Communication Rounds** & **Round Cost\({}^{)}}\)** \\   Dore, Artemis, MURANANA\({}^{)}}\) \\ (Philipperko and Dieuleveut, 2020) \\ (Condat and Richtarik, 2022) \\  & \((}{})^{ )}}\) & \((1-r)K_{}+rK_{}\) \\ (Condat and Richtarik, 2022) \\  & \(((}{{2}}}+}{{}}+}{{}}}{}{{ }}}}+)}{})^{)}}\) & \((1-r)K_{}+rK_{}\) \\   GD \\ (Nesterov, 2018) \\  & \(\) & \(d\) \\   EF21-P + DIANA \\ (Gruntkowska et al., 2023) \\  & \(+}{n}+\) & \((1-r)K_{}+rK_{}\) \\   GD \\ (Nesterov, 2018) \\  & \(}\) & \(d\) \\   20Direction \\ (Remark 5.3)\({}^{)}}\) \\  & \(}{}+}{n}+ {}+}}\) & \((1-r)K_{}+rK_{}\) \\   20Direction \\ (Remark 5.5)\({}^{)}}\) \\ (requires \({}^{L_{}}/k^{)}}\)) \\  & \(}{}+}{n}+ {}+}}+\) & \((1-r)K_{}+rK_{}\) \\  
 20Direction \\ (Remark 5.5)\({}^{)}}\) \\ (requires \({}^{L_{}}/k^{)}}\)) \\  & \(/2}{}++}\) & \(/2}{}++}\) & \((1-r)K_{}+rK_{}\) \\   

Table 1: **Communication Rounds in the Strongly Convex Case.** The number of communication rounds and rounds costs to get an \(\)-solution (\([\|-x^{*}\|^{2}]\)) up to logarithmic factors. The table shows the most relevant bidirectional compressed methods that are ordered by the total communication complexity # **Communication Rounds \(\) Round Cost** (see 4) for details).

i. The parameter \(r\) weights the importance/speed of uplink and downlink connections. When \(r=}{{2}},\) it means that the uplink and downlink speeds are equal.

ii. The parameters \(K_{}\) and \(K_{}\) are the expected densities Definition 2.5 of compressors \(^{D}()\) and \(^{P}(}{{}})^{)}},\) that operate in the workers and the server accordingly. Less formally, \(K_{}\) and \(K_{}\) are the number of coordinates/bits that the workers and the server send to each other in each communication round.

to methods that only perform w2s compression has a long history, and this area is relatively well understood (Alistarh et al., 2017; Mishchenko et al., 2019; Richtarik et al., 2021).

We refer to the work of Gruntkowska et al. (2023) for a detailed discussion of the communication complexities of _non-accelerated_ methods in the convex and non-convex settings. For instance, using \(K\), the \(\) method of Mishchenko et al. (2019) provably improves2 the communication complexity of \(\) to \(^{}_{}=(d+{{}^ {KL}}_{}+{{}^{dL_{}}}/{n_{}}).\)_Accelerated_ methods focusing on w2s compression are also well investigated. For example, Li et al. (2020) and Li and Richtarik (2021) developed accelerated methods, which are based on (Mishchenko et al., 2019; Kovalev et al., 2020), and provably improve the w2s complexity of \(\). Moreover, using \(K\) with \(K^{d}}_{n}\), \(\)-\(\) improves the communication complexity of \(\) to \(^{}_{}=(d+d{{}^{ }}}/{n_{}}).\)

### Bidirectional (i.e., w2s and s2w) compression

The methods mentioned in Section 2.2 do _not_ perform server-to-workers (s2w) compression, and one can show that the server-to-workers (s2w) communication complexities of these methods are worse than \(_{}=(d}{{ }}}).\) For example, using the \(K\), the s2w communication complexity of \(\) is at least \(^{}_{}=(d )=({{}^{d}}/{K}),\) which can be \({{}^{d}}/{K}\) times larger than in \(\) or \(\). Instead of \(^{}_{}\), methods performing bidirectional compression attempt to minimize _the total communication complexity_, which we define as a convex combination of the w2s and s2w communication complexities:

\[^{r}_{}:=(1-r)^{}_{}+r^{}_{}.\] (4)

The parameter \(r\) weights the importance of uplink (w2s) and downlink (s2w) connections3. Methods from Section 2.2 assume that \(r=0\), thus ignoring the s2w communication cost. On the other hand, when \(r=}{{2}}\), the uplink and downlink communication speeds are equal. By considering any \(r\), our methods and findings are applicable to more situations arising in practice. Obviously, \(^{r}_{}=_{}\) and \(^{r}_{}=_{}\) for all \(r\). Recently, Gruntkowska et al. (2023) proposed the EF21-P + \(\) method. This is the first method supporting bidirectional compression that provably improves both the w2s and s2w complexities of \(\): \(^{r}_{}_{}\) for all \(r\). Bidirectional methods designed before EF21-P + \(\), including (Tang et al., 2020; Liu et al., 2020; Philippenko and Dieuleveut, 2021), do not guarantee the total communication complexities better than that of \(\). The EF21-P + \(\) method is _not_ an accelerated method and, in the worst case, can have communication complexities worse than \(\) when the condition number \(}{{}}\) is large.

## 3 Contributions

Motivated by the above discussion, in this work we aim to address the following

**Main Problem**:

**Is it possible to develop a method supporting bidirectional communication compression that improves the current best theoretical total communication complexity of EF21-P + \(\), and guarantees the total communication complexity to be no worse than the communication complexity \(_{}=(d}{{ }}})\) of \(\), while improving on \(\) in at least some regimes?**

**A)** We develop a new fast method (2Direction; see Algorithm 1) supporting bidirectional communication compression. Our analysis leads to new state-of-the-art complexity rates in the centralized distributed setting (see Table 1), and as a byproduct, we answer Main Problem in the affirmative.

**B)** Gruntkowska et al. (2023) proposed to use the EF21-P error-feedback mechanism (8) to improve the convergence rates of _non-accelerated_ methods supporting bidirectional communication compression. EF21-P is a reparameterization of the celebrated EF mechanism (Seide et al., 2014). We tried to use EF21-P in our method as well, but failed. Our failures indicated that a fundamentally new approach is needed, and this eventually led us to design a new error-feedback mechanism (9) that is more appropriate for _accelerated_ methods. We believe that this is a contribution of independent interest that might motivate future growth in the area.

**C)** Unlike previous theoretical works (Li et al., 2020; Li and Richtarik, 2021) on accelerated methods, we present a unified analysis in both the \(\)-strongly-convex and general convex cases. Moreover, in the general convex setting and low accuracy regimes, our analysis improves the rate \((}{{^{1/3}}})\) of Li and Richtarik (2021) to \((}{{}})\) (see details in Section R).

**D)** Even though our central goal was to obtain new SOTA _theoretical_ communication complexities for centralized distributed optimization, we show that the newly developed algorithm enjoys faster communication complexities in practice as well (see details in Section Q).

```
1:Parameters: Lipschitz-like parameter \(>0\), strong-convexity parameter \( 0\), probability \(p(0,1]\), parameter \(_{0} 1\), momentum \((0,1]\), contraction parameter \((0,1]\) from (2), initial point \(x^{0}^{d}\), initial gradient shifts \(h^{0}_{1},,h^{0}_{n}^{d}\), gradient shifts \(k^{0}^{d}\) and \(v^{0}^{d}\)
2:Initialize \(=}{{(+1)}},w^{0}=z^{0}=u^{0}=x^{0}\), and \(h^{0}=_{i=1}^{n}h^{0}_{i}\)
3:for\(t=0,1,,T-1\)do
4:\(_{t+1},_{t+1},_{t+1}=(_{t},,,p,,,)\)  Get learning rates using Algorithm 2
5:for\(i=1,,n\) in parallel do
6:\(y^{t+1}=_{t+1}w^{t}+(1-_{t+1})z^{t}\)
7:\(m^{t,y}_{i}=^{D,y}_{i}( f_{i}(y^{t+1})-h^{t}_{i})\)  Worker \(i\) compresses the shifted gradient via the compressor \(^{D,y}_{i}()\)
8: Send compressed message \(m^{t,y}_{i}\) to the server
9:endfor
10:\(g^{t+1}=h^{t}+_{i=1}^{n}m^{t,y}_{i}\)
11:\(u^{t+1}=_{x^{d}} g^{t+1},x+ {L+_{t}}{2_{t+1}}\|x-u^{t}\|^{2}+\| x-y^{t+1}\|^{2}\)  A gradient-like descent step
12:\(q^{t+1}=_{x^{d}} k^{t},x+ {L+_{t}}{2_{t+1}}\|x-w^{t}\|^{2}+\| x-y^{t+1}\|^{2}\)
13:\(p^{t+1}=^{P}(u^{t+1}-q^{t+1})\)  Server compresses the shifted model via the compressor \(^{P}()\)
14:\(w^{t+1}=q^{t+1}+p^{t+1}\)
15:\(x^{t+1}=_{t+1}u^{t+1}+(1-_{t+1})z^{t}\)
16: Send compressed message \(p^{t+1}\) to all \(n\) workers
17:Fip a coin \(^{t}(p)\)
18:\(k^{t+1}=v^{t},&^{t}=1\\ k^{t},&^{t}=0\) and \(z^{t+1}=x^{t+1},&^{t}=1\\ z^{t},&^{t}=\)
19:if\(^{t}=\)then
20: Broadcast non-compressed messages \(x^{t+1}\) and \(k^{t+1}\) to all \(n\) workers With small probability \(p!\)
21:endif
22:for\(i=1,,n\) in parallel do
23:\(q^{t+1}=_{x^{d}} k^{t},x+ {+_{t}}{2_{t+1}}\|x-w^{t}\|^{2}+\|x-y^{t+1}\|^{2}\)
24:\(w^{t+1}=q^{t+1}+p^{t+1}\)
25:\(z^{t+1}=x^{t+1},&^{t}=1\\ z^{t},&^{t}=0\)
26:\(m^{t,z}_{i}=^{D,z}_{i}( f_{i}(z^{t+1})-h^{t}_{i})\)  Worker \(i\) compresses the shifted gradient via the compressor \(^{D,z}_{i}()\)
27:\(h^{t+1}_{i}=h^{t}_{i}+ m^{t,z}_{i}\)
28: Send compressed message \(m^{t,z}_{i}\) to the server
29:endfor
30:\(v^{t+1}=(1-)v^{t}+(h^{t}+_{i=1}^{n}m^{t,z}_{i})\)
31:\(h^{t+1}=h^{t}+_{i=1}^{n}m^{t,z}_{i}\)
32:endfor ```

**Algorithm 1**2Direction: A Fast Gradient Method Supporting Bidirectional Compression

## 4 New Method: 2Direction

In order to provide an answer to Main Problem, at the beginning of our research journey we hoped that a rather straightforward approach might bear fruit. In particular, we considered the current state-the-art methods ADIANA (Algorithm 3) (Li et al., 2020), CANITA (Li and Richtarik, 2021) and EF21-P + DIANA (Algorithm 4) (Gruntkowska et al., 2023), and tried to combine the EF21-P compression 

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

### The ratio \(L_{}/L\) is not known

In the following theorem, we consider the regime when the exact value of \(}}{{L}}\) is not known. Hence, we seek to find \(p\) and \(\) that minimize the worst case \(_{}^{r}\) (see (4)) w.r.t. \(L_{}[L,nL]\).

**Theorem 5.2**.: _Choose \(r\) and let \(_{,}^{r}:=+rK_{}}\). In view of Theorem 5.1, the values \(p=\{,^{r}}\}\) and \(=}{(+1)^{2/3}}\) minimize \(_{L_{}[L,nL]}_{}^{r}\). This choice leads to the following number of communication rounds:_

\[T^{}:=^{r})}{}},+ (+1,_{,}^{r})}{n}},,(+1), _{,}^{r}}.\] (13)

_The total communication complexity thus equals \(_{}^{r}=(((1-r)K_{ }+rK_{})T_{}+d).\)_

_Remark 5.3_.: To simplify the rate (13) and understand the quantity \(_{,}^{r}\), let \(_{i}^{D,}\) be the Rand\(K\) sparsifier5 and consider the case when the s2w communication is not slower than the w2s communication, i.e., \(r}{{2}}\). Then \(T^{}=(\{ {}},(+1)}{np}},,( +1)\})\) and \(_{,}^{r}+1\). Indeed, this follows from \(r}{{2}}\) and the fact that \(+1=}{{K_{}}}\) for the Rand\(K\) compressor: \(_{,}^{r}:=+rK_{}} }+1\).

### The ratio \(L_{}/L\) is known

We now consider the case when we have information about the ratio \(}}{{L}}\).

**Theorem 5.4**.: _Choose \(r,\) and let \(_{,}^{r}:=+rK_{}^{r}}\). In view of Theorem 5.1, the values \(p\) and \(\) given by (63) and (58), respectively, minimize \(_{}^{r}\) from (10). This choice leads to the following number of communication rounds:_

\[T^{}= ^{r}\}}{}},L_{}^{1/3}(+1)}{ n^{1/3}}},L_{}^{1/ 2}(+1)^{3/2}}{}},\] (14) \[\{+1,_{,}^{r} \}}{n}},,(+1),_{,}^{r}} .\]

_The total communication complexity thus equals \(_{}}}}}}}}}}}}}}}}}}}}^{r}\) =(((1-r)K_{}+rK_{})T_{ }}}}}}}}}}}}}}}}}}}+d ).\)_

Note that information about \(L_{}/L\) leads to a better rate that in Theorem 5.2.

_Remark 5.5_.: To simplify the rate (14), let \(_{i}^{D,}\) be the Rand\(K\) sparsifier6 and consider the case when the s2w communication is not slower than the w2s communication, i.e., \(r}{{2}}\). Then \(T^{}=(}{}},L_{}^{1/3}(+1)}{  n^{1/3}}},L_{}^{1/2}(+1)^{3/2}}{}},(+1)}{n}},,( +1)}).\) Indeed, this follows from \(r}{{2}}\) and the fact that \(+1=}{{K_{}}}\) for the Rand\(K\) compressor: \(_{,}^{r}:=+rK_{}} } 2r(+1)\).

## 6 Theoretical Comparison with Previous State of the Art

We now show that the communication complexity of 2Direction is always _no worse_ than that of EF21 + DIANA and AGD. Crucially, in some regimes, it can be substantially better. Furthermore, we show that if the s2w communication cost is zero (i.e., if \(r=0\)), the 2Direction obtains the same communication complexity as ADIANA (Li et al., 2020) (see Section S).

**Comparison with EF21 + DIANA.** The EF21-P + DIANA method has the communication complexities that equal

[MISSING_PAGE_EMPTY:9]

probability \(p\) (see Line 20). While in Section 6 we explain that this does not have an adverse effect on the theoretical communication complexity since \(p\) is small, one may wonder whether it might be possible to achieve the same (or better) bounds as ours without having to resort to intermittent non-compressed broadcasts. This remains an open problem; possibly a challenging one. Another limitation comes from the fact that 2Direction requires more iterations than \(\) in general (this is the case of all methods that reduce communication complexity). While, indeed, (10) can be higher than \((}{{}}})\), the total communication complexity of 2Direction is not worse than that of \(\).

#### Acknowledgements

This work of P. Richtarik and A. Tyurin was supported by the KAUST Baseline Research Scheme (KAUST BRF) and the KAUST Extreme Computing Research Center (KAUST ECRC), and the work of P. Richtarik was supported by the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI).