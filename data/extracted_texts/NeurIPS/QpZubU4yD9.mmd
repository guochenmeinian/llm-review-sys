# Advice Querying under Budget Constraint for Online Algorithms

Ziyad Benomar

CREST, ENSAE, Ecole polytechique

ziyad.benomar@ensae.fr

&Vianney Perchet

CREST, ENSAE and Criteo AI LAB

vianney.perchet@normalesup.org

###### Abstract

Several problems have been extensively studied in the _learning-augmented_ setting, where the algorithm has access to some, possibly incorrect, predictions. However, it is assumed in most works that the predictions are provided to the algorithm as input, with no constraint on their size. In this paper, we consider algorithms with access to a limited number of predictions, that they can request at any time during their execution. We study three classical problems in competitive analysis, the ski rental problem, the secretary problem, and the non-clairvoyant job scheduling. We address the question of when to query predictions and how to use them.

## 1 Introduction

With the rise of data science and the huge emphasis on research in machine learning and artificial intelligence, powerful predictive tools have emerged. This gave birth to _learning-augmented algorithms_, which use these predictions to go beyond the standard long-standing worst-case limitations. The design of such algorithms requires establishing good tradeoffs between consistency and robustness, i.e. having improved performance when the predictions are accurate, and not behaving poorly compared to the case without predictions if they are erroneous. This was formalized by Lykouris and Vassilvitskii  for the caching problem, and Purohit et al.  for the ski-rental and scheduling problems. Since then, the learning-augmented setting had many applications in implementing data structures  and in the design of algorithms .

Several questions and research directions have been explored, such as exhibiting optimal robustness-consistency tradeoffs , incorporating predictions from multiple experts , or customizing learning approaches to make predictions for such algorithms . In most papers, those predictions are assumed to be given as inputs, which is unfortunately not realistic, practical or applicable to real-life scenarios. Indeed, many problems such as scheduling, paging, or set cover, require some arbitrarily large number of predictions (roughly speaking one per request or job). We claim that, on the contrary, algorithms might have only access to a finite budget of predictions as each one of them is costly to compute. Therefore, the main question rather becomes deciding how and when to spend the assigned budget, by requesting new predictions. Such a setting was considered for example in  for the caching problem. Moreover, it also happens that the quality of the predictions improves with time, as more data are collected, more knowledge of the problem is gathered, or delaying the computation of prediction allows to allocate more computational power to them, hence enhancing their precision, for instance with more powerful forecasting models. The question of planning the prediction requests is thus quite crucial.

The problems we mentioned fall into the category of online algorithms, operating under uncertainty, and gaining more information about the input of the problem through time, eventually depending on their past decisions. The performance of such algorithms is measured by their competitive ratio , which is the worst-case ratio between their output and that of the optimal offline algorithm, having full knowledge of the inputs and parameters of the problem. To illustrate our claims, we shall focuson three well-established and fundamental problems in competitive analysis: ski rental, secretary, and job scheduling problems.

Ski-rentalIn the ski-rental problem, the decision maker faces each day the choice of either renting a ski for a unit cost, or buying it at a cost of \(b\) and skiing for free after that. The only decision to be made by the algorithm is therefore when to buy. The unknown parameter is the length \(x\) of the ski season, and the objective is to minimize the total cost of renting and buying. The optimal offline algorithm has a cost \(\{x,b\}\), the deterministic online _break-even_ algorithm, which consists in renting for the first \(b-1\) days and then buying, results in a competitive ratio of \(2\), and the optimal randomized algorithm introduced by Karlin et al.  yields a competitive ratio of \(\).

Secretary problemThe second problem we study is the secretary problem. Consider having \(N\) applicants observed sequentially in a uniformly random order. Immediately after a candidate is observed, the algorithm has to either select them and stop, or refuse them irrevocably. The objective is to maximize the probability of selecting the best applicant overall. The particularity of this problem is that a single incorrect decision at any point leads to failure and zero gain. Consequently, the cost of making a wrong decision is significantly high. The optimal offline algorithm has access to all values and thus can choose the maximum with probability \(1\). The optimal online strategy is to reject the first \(N/\) applicants, then stop at the first applicant who is better than all of them [17; 25]. This is often referred to as the \(1/\) rule, and it gives a success probability of \(1/ 0.368\).

Job schedulingThe third problem we consider is the preemptive non-clairvoyant job scheduling. The input is a set of jobs with unknown sizes, that the algorithm needs to schedule on a single machine, with the possibility of halting a job during its execution and resuming it later. The goal is to minimize the sum of their completion times, i.e. finish as many jobs as possible rapidly. The only observations made during the run of an algorithm are the completion times of the jobs. These observations, along with the times at which they are received, depend on both the input instance and the algorithm's past decisions. The cost of early incorrect decisions is more significant than that of later incorrect decisions. Take for example the instance of job sizes \(\{x_{1}=1,x_{2}=100\}\), running \(x_{1}\) then \(x_{2}\) gives an output \(1+101=102\), while running them in the inverse order gives \(100+101=201\). The optimal offline algorithm is to schedule the jobs in increasing order of size, and a deterministic online algorithm called _round-robin_, gives a competitive ratio of \(2\), which is optimal for an arbitrarily large number of jobs .

### Organization and contributions

In Section 2, we consider the ski rental problem, where we assume that an oracle provides binary predictions on how the snow season length \(x\) compares to the purchasing cost \(b\), and that these predictions are accurate with a probability that is a function of time, known to the algorithm and denoted as \(p_{t}\). This assumption accurately reflects and models what is practically occurring. As the snow season begins, there is significant uncertainty about how long it will last. However, over time, the accuracy of predictions increases as more weather observations are made, the direction of snowstorms becomes clearer, and additional data is collected. The algorithm is permitted to request only one prediction during its execution, at a time of its choice. Depending on the function \(p_{t}\) and on how the prediction is used once queried, it can be more advantageous to rent for a period of time \(t\) before requesting it, i.e. paying a cost for making a better decision in the future. We consider that once the prediction is queried, we use either the deterministic or the randomized algorithm presented in  for the ski rental problem. We show how to optimally choose \(t\), and how the knowledge of \(p_{t}\) allows to tune the parameter \(\), regulating the levels of consistency and robustness in both algorithms.

Secondly, we consider in Section 3 the secretary problem with access to an oracle, accurate with a probability \(p\), telling if there are better applicants in the future. In practice, this would correspond to the following scenario: consider a decision-maker with an imperfect interviewing process, and thus can only compare the applicants with each other, i.e. observe their relative ranks, but lacks access to their true value. On the other hand, an expert who, due to extensive experience, knows how to better interview the applicants and extract their true value, and also knows the distribution of their values (as in a prophet setting with i.i.d variables). The decision-maker can ask the expert to interview some applicants for a cost. After that, the expert does not disclose the precise value of the applicant - that is irrelevant to the decision maker-, but they provide a recommendation (accept or reject) and accompany it with a confidence level, which is the probability that this recommendation is accurate. Assuming that the decision-maker has enough budget to ask for the expert's advice \(B\) times, we give an algorithm whose success probability depends on \(p\) and \(B\), converges to \(1\) as \(p\) goes to \(1\) and \(B\) grows, and that is always lower bounded by \(1/\). Our algorithm easily adapts to the case where the accuracy of the oracle is time-dependent.

Section 4 is dedicated to the preemptive non-clairvoyant job scheduling, where we assume that the algorithm can request access to the sizes of \(B\) chosen jobs. In contrast with the two previous problems, there is no advantage in waiting before querying the hints, and since the jobs play identical roles, the algorithm can only request the sizes of \(B\) randomly chosen ones. We show first that the competitive ratio \(2\) cannot be improved unless \(B=(N)\), where \(N\) is the number of jobs. Then we present an algorithm that chooses randomly \(B\) jobs and requests their sizes, then runs concurrently the optimal offline algorithms on those jobs and round-robin on the others, with time-dependent processing rates. We prove a generic expression for the output of this algorithm that depends on the chosen processing rate, and then we give a particular processing rate that yields a competitive ratio of \(2-(B/N)^{2}\), therefore interpolating the competitive ratios \(2\) and \(1\) met respectively when no hint is given, and when all the job sizes are known.

Finally, we run simulations of our algorithms in Section 5. We show that the performance of the algorithm we introduced for the secretary problem matches our theoretical lower bound, and we compare it to another heuristic version that we did not study theoretically. Then, by exhaustively testing the algorithm we designed for the scheduling problem with access \(B\) job sizes on various benchmark instances, we observe that it has a better performance in practice than the theoretical upper bound \(2-(B/N)^{2}\).

### Related work

The ski-rental and scheduling problems have received extensive attention in the realm of learning-augmented algorithms [47; 48; 3; 19; 18; 37; 9]. They serve respectively as notable examples for problems requiring a single prediction and multiple predictions. Moreover, they both have numerous applications and variants, as [32; 12; 24; 44] for the ski rental and [42; 16; 38] for the scheduling problem. The secretary problem was also among the first problems studied with the advice model. Dutting et al.  consider that a binary prediction is received with each applicant, indicating whether it is the best overall or not. Assuming that these predictions are accurate each with a probability \(p\) independently, they design algorithms that improve upon the \(1/e\) success probability. While this model closely aligns with ours, we consider a scenario where the algorithm has a limited budget of predictions and must carefully determine when to query them. Antoniadis et al.  studied a variant of the problem where the objective is to maximize the value of the selected applicant. They consider that the algorithm is provided with a prediction of the value of the best applicant.

For both the ski-rental and the secretary problems, we consider predictions that are accurate with a known probability. A similar assumption was made in , where the oracle is assumed to deliver a prediction that is accurate with a probability of at least \(\), and that can be arbitrarily inaccurate with the remaining probability. The authors give improved competitive ratios depending on \(\) for some online problems including caching, online set cover, and facility location.

The aspect of having a limited prediction budget is relatively underexplored in the literature. The question was initially examined in the context of the online linear optimization problem with hints, where Bhaskara et al.  demonstrated that a sublinear number of hints is sufficient to achieve regret bounds similar to those in the full hints setting when the timing of requesting hints is well chosen. Im et al.  also investigated this question for the caching problem, presenting an algorithm that strategically utilizes the assigned prediction budget to improve the competitive ratio as the budget increases. In a very recent paper, the ski-rental and the Bahncard problems were explored in a setting with costly predictions , where the cost of predictions is added to the total cost paid by the algorithm. This can be seen as a penalized version of querying predictions under budget constraints. Other works have also investigated the reduction of the prediction size rather than the number of predictions. Specifically, they consider binary predictions encoded on a single bit and explore how the competitive ratio can be enhanced compared to other types of predictions [22; 45; 8].

Another important question we cover thanks to the ski-rental problem is how to optimally balance robustness vs consistency with respect to predictions . This tradeoff is usually done by addingsome extra-parameter \(\) that reflects how much the decision-maker is willing to trust blindly the prediction, and which appears naturally in the bounds of the competitive ratio. Roughly speaking, for \(=1\), the decision maker focuses solely on the performances of their algorithms when predictions are incorrect (hence predictions are actually disregarded), while for \(=0\), they naively consider that predictions are correct. Intermediate values of \(\) correspond to less extreme behaviors, and the final competitive ratio strongly depends on the value of this parameter and the total "amount" of errors (measured with respect to some problem-dependent metric) in predictions. Setting a value for this parameter requires having some knowledge about the quality of the prediction. Khodak et al.  shows how to learn to set \(\)'s value in an online learning setting, where the algorithm runs repeatedly on different instances, and learns to predict unknown parameters based on features on the new instances. After many runs, the value of \(\) can be increased since the predictions become more and more accurate. In our case, we have binary predictions that are correct with a known probability, which is a natural assumption for binary predictions [45; 22], and we show that this allows us to optimally choose the value of \(\).

## 2 Ski-rental with time-dependent guarantees on the prediction

We consider that the cost of renting a ski for one day is \(1\), while the cost of buying is \(b>1\), and we denote \(x\) the duration of the ski season, which is unknown. For all the algorithms we present, if the ski season is over then the algorithm stops and no further cost is paid. In the learning-augmented setting, we assume that the algorithm possesses a prediction \(y\) for \(x\). Many variants of this scenario have also been explored in previous research papers [47; 35; 27; 48]. We restrain ourselves to the case of binary predictions, comparing the number of snow days to the budget. More precisely, we assume the existence of an oracle, that can be called at any time \(t\), predicting whether \(x-t>b\) or not, where \(x-t\) is the number of remaining snow days. Furthermore, we assume that the accuracy of the oracle improves over time. If queried at time \(t\), then the prediction is correct with a probability \(p_{t}\), known to the algorithm, that is independent of the problem's history and increases over time. We assume that, due to budget limitations, the algorithm can access the oracle only once during its execution, and thus it must carefully choose the time of asking for the prediction.

Let ALG be an algorithm such that, with a prediction accurate with probability \(p\), it has a competitive ratio \(C(,p)\). We define the algorithm ALG\({}_{t}\) that rents for the first \(t\) days, then queries a prediction \(_{t}\) of \((x-t b)\) at the start of day \(t+1\), and then acts like ALG. We have the following result.

**Lemma 2.1**.: _ALG\({}_{t}\) has a competitive ratio of at most \(+C(,p_{t})\)._

The term \(t/b\) represents the additional cost due to renting the first \(t\) days, in order to have a better accuracy \(p_{t}\) which decreases the second term. The optimal time for querying a prediction is \(t^{}\) minimizing the function \(t/b+C(,p_{t})\). Although this requires knowing all the sequence \((p_{t})_{t 0}\) in advance, we can design simple online heuristics for being close to a local minimum, where the accuracy of the oracle at some step \(t\) is only revealed when that step is reached. We can for example access the oracle at the first time \(t\) when \(t/b+C(,p_{t})\) increases.

In the following, we show how Lemma 2.1 can be applied with explicit algorithms ALG that are given a binary prediction as input. We consider the algorithms 1 and 2, which were first introduced in . In both algorithms, the parameter \(\) indicates how much the prediction is trusted. Assuming that the input prediction is accurate with a probability \(p=(=(x b))\), we show how to tune \(\) in both algorithms to minimize their costs, and we upper bound their competitive ratios.

``` Input: cost \(b\) for buying, a prediction \(\) for \((x b)\) if\(=1\)then buy on the start of day \( b\) ; else buy on the start of day \( b/\) ; ```

**Algorithm 1**Deterministic algorithm with input binary prediction 

**Lemma 2.2**.: _If the oracle \(\) delivers an accurate prediction with probability \(p 0.5\), then by choosing \(=}\), the algorithm achieves a competitive ratio of at most \(1+2\)._

The optimal choice of \(\) gives therefore a competitive ratio that is always upper bounded by 2, which is the optimal competitive ratio without prediction, and decreases to 1 when the accuracy of the oracle is better. While Algorithm 1 is deterministic, the next algorithm we study is randomized, where the day of buying is a random variable, drawn from a probability distribution that depends on the prediction \(\). We show again an optimal choice of \(\) for minimizing the competitive ratio.

**Lemma 2.3**.: _If the oracle \(\) delivers an accurate prediction with probability \(p 0.5\), then with \(=\{1,1/b+\},\) Algorithm 2 has a competitive ratio of at most_

\[\{+2}+(1- )p&p(1+}{e-1})^{-1},\\ +&.\]

Figure 2 shows the competitive ratios of both algorithms depending on \(p\) when \(\) is chosen optimally. The upper bound shown in Lemma 2.3 depends on \(b\), this is why we test it with different values of \(b\). As expected, the randomized algorithm yields better guarantees than the deterministic one. However, when \(p\) is very close to \(1\), Algorithm 1 is slightly better, because the randomized algorithm requires having \( 1/b\), and therefore in the limit where \(p=1\), the optimal choice is \(=1/b\), giving the upper bound \(1+1/b\) instead of \(1\) on the competitive ratio.

Now, assuming that the oracle is accurate with a known time-dependent probability \(p_{t}\), the previous lemmas allow to optimally choose the time of querying the prediction when one of the two algorithms we presented is used after the prediction is obtained. We only state the result for the case where Algorithm 1 is used. The proof is immediate using Lemmas 2.1 and 2.2, and a similar result can be shown using Lemmas 2.1 and 2.3 when Algorithm 2 is used instead.

**Theorem 2.4**.: _If the predictions delivered by the oracle are accurate with a probability \(p_{t}\) that only depends on the time, then renting until time \(t\), then querying a prediction \(_{t}\) and running Algorithm 1 with parameter \(=}{p_{t}}}\), yields a competitive ratio of at most \(1+t/b+2(1-p_{t})}\)._

We consider in Figure 2 an example where \(p_{t}=0.95-0.4(-t/5)\), thus \(p_{0}=0.55\) and \(_{t}p_{t}=0.95\). The figure shows the competitive ratio of renting \(t\) days then using one of the Algorithms 1 or 2, with \(b=50,100\). We observe that this strategy can significantly improve the competitive ratio if \(t\) is chosen correctly. Of course, this depends strongly on \(p_{t}\) and \(b\). In particular, for the randomized algorithm, it is better to query the prediction at \(t=0\) when \(b=50\).

## 3 Secretary problem with \(B\) predictions

Assume that \(N\) applicants are observed sequentially in a uniformly random order \((x_{1},,x_{N})\), all having distinct values. After an applicant is interviewed, the decision-maker has to either accept them and halt the process, or refuse them irrevocably. We consider a setting where, when an applicant \(x_{t}\) is observed, and the budget is still not exhausted, the algorithm can request a binary prediction \((x_{t})\) indicating if there are better applicants coming in the future. We assume a maximal budget of \(B\) predictions, where \(B\) is a constant independent of \(N\). We assume first that the predictions are error-free, and we give theoretical guarantees on Algorithm 3 in that case, and after that, we show how it can be adapted to handle predictions that are accurate only with a probability \(p\). In this section, we say that an algorithm succeeded if the selected applicant is the best overall.

We consider first Algorithm 3, which rejects the first \( r_{B}N\) applicants, where \(r_{B}(0,1)\) is a threshold depending on \(B\), and then queries a prediction for the first applicant \(x_{t}\) better than all of them. If the oracle predicts that there is a better candidate in the future (\((x_{t})=1\)) then the algorithm is restarted with the inputs \((x_{t+1},,x_{N})\) and budget \(B-1\).

**Theorem 3.1**.: _Let \((q_{B})_{B-1},(r_{B})_{B 0}\) the sequences defined by \(q_{-1}=0\) and for \(B 0\)_

\[q_{B}=q_{B-1}+(1-q_{B-1})(-}), r_{B}= (-}).\]

_If the oracle \(\) delivers error-free predictions, then with the thresholds \((r_{B})_{B}\), AdaThresh with budget \(B\) has a success probability of at least \(q_{B}\) independently of the input size \(N\)._

Observe that, as \(B\) increases, the lower bound \(q_{B}\) of the success probability of AdaThresh converges to \(1\) and the thresholds \(r_{B}\) converge to zero, meaning that the higher the budget, the higher the risks: there are fewer applicants in the first observation phase, thus the probability of selecting a future sub-optimal one is higher (yet this risk is hedged by the predictions), but on the other hand, it reduces the probability of naively disregarding the best applicant if it is among the first arriving ones. Although AdaThresh is a naive algorithm that does not make full use of past information, we showed that adequately choosing the thresholds \((r_{B})_{B}\) guarantees a success probability that goes to \(1\) as the budget increases. We present in Appendix B.3 an improved version of AdaThresh that keeps in memory the maximum value \(M\) observed so far and that rejects all applicants having values below \(M\). We show numerically in Section 5 how this increases the success probability of the algorithm.

Handling imperfect predictionsIf the predictions of the oracle are imperfect, then one way of guaranteeing robustness, since the objective is to maximize the success probability, is to use the \(1/e\)-rule with probability \(\) and Algorithm 3 with probability \(1-\). This guarantees a success probability of at least \(/e\) if the predictions are incorrect. On the other hand, given that the oracle's predictions are binary, it is reasonable to assume that each prediction is correct with a probability \(p\) independent of all the observations, as explained in the introduction. Based on this assumption, Algorithm 3 can be modified to include the option of trusting or disregarding the oracle's prediction at each step. We show that it is advantageous to trust the oracle when \(p\) is above a certain threshold and run the \(1/\)-rule otherwise. This algorithm achieves a success probability of at least \(1/e\), with improving performance as \(p\) approaches 1. We show in the appendix how this can be generalized when the accuracy of the oracle is time-dependent.

**Theorem 3.2**.: _Assume that each oracle's prediction is independently correct with probability \(p(1+(e-1)(-))^{-1} 0.73\), then there is an algorithm that achieves a success probability of at least \(q_{0}(p)=1/e\) for \(B=0\), and \(q_{B}(p)\) for \(B 1\) defined as_

\[q_{B}(p)=pq_{B-1}(p)+p(1-q_{B-1}(p))(-(p)}).\]

_If we denote \(W\) the Lambert function, i.e. the inverse of \(u we^{u}\) on \((0,)\), then we have_

\[_{B}q_{B}(p)=1-}.\]

_Moreover, the algorithm has a success probability of at least \(1/\) for any value of \(p[1/2,1]\)._

Optimal algorithmAssuming that the predictions are error-free, the problem is equivalent to the _multiple-choice secretary problem_, where the algorithm is allowed to select \(k 1\) applicants, and it is successful if the best overall candidate is among them. Indeed, the algorithm is given \(B+1\) attempts to identify the best applicant, and it halts if it finds it. This is analogous to choosing \(k=B+1\) applicants, where following the selection of each one, the algorithm employs a selection strategy as if the previous guesses were unsuccessful. The optimal algorithm for selecting \(k\) applicants is a \((a_{k},,a_{1})\)-rule [26; 43], where at any step \(t\), if the number of applicants already selected is \(i\{0,,k\}\), then reject everyone until step \(\{t,a_{k-i}\}\), and accept the first applicant after that is the best observed so far. Although this family of algorithms has a simple structure, analyzing it is difficult and hides many technical challenges. The optimal thresholds and the asymptotic success probability are explicitly computed only up to \(k 5\), and a recursive formula is proven, via a dynamic programming approach, to compute the next thresholds. This formula is however difficult to exploit even numerically, while the optimal thresholds and the success probability of AdaThresh can be computed very easily.

If the predictions are correct with a probability \(p\), then the optimal algorithm must be a generalization of the \((a_{k},,a_{1})\)-rule, and thus it is even harder to analyze. The algorithm we proposed is naive in the sense that it does not take into account all the past information and only remembers the history since the last prediction was queried. However, it illustrates how the limited budget of predictions should be spent, and it presents good robustness and consistency guarantees with respect to \(p\) and \(B\), in the sense that it always has a success probability of at least \(1/\), which is optimal without predictions, and it has a success probability that converges to \(1\) as \(B\) and \(p\) increase.

## 4 Preemptive \(B\)-clairvoyant job scheduling

We consider the problem of scheduling multiple jobs on a single machine, with the objective of minimizing the sum of their completion times. More particularly, we place ourselves in the _preemptive_ setting, where the jobs can be temporarily halted and resumed later, or equivalently that they can be run in parallel with rates that sum at most to 1. Let \(N\) be the number of jobs and \(x_{1}, x_{N}\) their sizes. If the algorithm knows beforehand the sizes of the jobs, it is called _clairvoyant_, and the optimal algorithm \(\) is to run the shortest jobs first. An algorithm is _non-clairvoyant_ if the size \(x_{j}\) of any job \(j\) is unknown until the job is completed. Motwani et al.  showed that no deterministic or randomized algorithm can have a better competitive ratio than \(2\), which is achieved by _round-robin_ (\(\)), which is a deterministic algorithm. \(\) works as follows: at any time \(t\), if \(n\) is the number of remaining jobs, then \(\) runs them all in parallel with rates \(1/n\) each.

An in-between setting, surprisingly not explored yet, is when the algorithm has only access to the sizes of a limited number of jobs. We say that an algorithm is \(B\)-clairvoyant when it is allowed to access the sizes of \(B\) jobs. We assume that it can query their true sizes, and not just noisy predictions. We start with a negative result, stating that we need to have \(B=(N)\) to achieve a better competitive ratio than \(2\). Then, we give an algorithm with a competitive ratio of at most \(2-(B/N)^{2}\).

Let us first remind a few classic notations in the scheduling problem. For any algorithm \(\), and any instance \(=\{x_{1},,x_{N}\}\), the sum of the completion times obtained by \(\) can be written as

\[()=_{i=1}^{N}x_{i}+_{i<j}D^{}(i,j)+D^{}(j,i),\] (1)where \(D^{}(i,j)\) is the delay caused by job \(i\) to job \(j\), i.e. the amount of job \(i\) executed before the completion of job \(j\). With this notation, we have for any \(i<j\) that \(D^{}(i,j)+D^{}(j,i)=\{x_{i},x_{j}\}\) and \(D^{}(i,j)=D^{}(j,i)=\{x_{i},x_{j}\}\). Thus, if \(x_{1} x_{N}\) we obtain

\[()=_{i=1}^{N}x_{i}+_{i=1}^{N}(N-i)x_{i}, ()=_{i=1}^{N}x_{i}+2_{i=1}^{N}(N-i)x_{i}.\] (2)

### Few hints are not enough

In opposite to other problems where it has been proved that a sublinear number of hints is enough for improving the performance [13; 31], the following theorem demonstrates that, for the scheduling problem, no algorithm can achieve a competitive ratio better than 2 when \(B=o(N)\).

**Theorem 4.1**.: _Any \(B\)-clairvoyant deterministic or random algorithm with \(B=o(N)\), has a competitive ratio lower bounded by \(2\)._

### Parallel OPT-RR algorithm with adaptive processing rates

We assume in this section that the \(B=(N)\). A first naive algorithm would run \(\) until there are \(B\) jobs left, then query their sizes, and use \(\) to finish. However, when all the jobs have the same size, they terminate at the same time and no hint is queried. The output of the algorithm with this instance is exactly the same as \(\), which is twice the output of \(\) asymptotically in \(N\), and therefore its competitive ratio is at least \(2\). More generally, any algorithm that runs \(\) waiting for a certain number of jobs to finish before querying the sizes of \(B\) diminished ones is no better than \(\) for the same reasons. Alternatively, the algorithm can wait for a possibly random amount of time \(T>0\), independent of the observed completion times, before querying the first hint. However, by taking job sizes sampled from an exponential distribution with parameter \(\) such that their sum is smaller than \(T\) with an arbitrarily high probability, the algorithm terminates with high probability before requesting any hint and is no better than a non-clairvoyant algorithm on such input instances, leading to a competitive ratio of at least \(2\). Therefore, the best moment to query the hints is at the very beginning of the execution, and since the algorithm cannot differentiate between the jobs, it can only query the sizes of \(B\) randomly chosen ones.

We propose a generic algorithm that queries the sizes of \(B\) randomly chosen jobs, then concurrently runs \(\) on them and \(\) on the others with respective rates \(\) and \(1-\), where \(\) is a parameter that can be adjusted throughout the course of the algorithm, depending on the information available at each time, i.e the predicted job sizes, the number of the remaining jobs, and the sizes of finished jobs.

``` Input: Budget \(B\), \(N\) jobs with unknown sizes \(\{x_{1},,x_{N}\}\)
1\(I\) Sample \(B\) jobs uniformly at random without replacement;
2\(J\{1,,N\} I\) ;
3whilethere are still unfinished jobsdo
4 Adjust \(\);
5runfor a time unit
6with rate \(\): \(\) on \(\{x_{i}\}_{i I}\) ;
7with rate \(1-\): \(\) on \(\{x_{j}\}_{j J}\) ; ```

**Algorithm 4**PAR Parallel algorithm with Adaptive processing Rate

In the following, for any algorithm \(\) and for any subsets \(H,K\) of \(\{1,,N\}\), we denote \(D^{}(H,K)=_{i H}_{j K\{i\}}D^{}(i,j)\) the sum of all the delays caused by jobs in \(H\) to those in \(K\). We demonstrate first a generic upper bound for the output of \(\),

**Lemma 4.2**.: _For any update rule of \(\) we have_

\[[()]=_{i=1}^{N}x_{i}+(2- 4-3)_{i=1}^{N}(N-i)x_{i}+[D^{ }(I,J)+D^{}(J,I)].\]

This Lemma shows that the output of \(\) depends only on the delays generated by jobs in \(I\) on jobs in \(J\) and vice versa. The difficulty now is to choose an adequate update rule for the rate \(\), with provable upper bounds on \([D^{}(I,J)+D^{}(J,I)]\).

### Simulated round-robin update rule

We consider the _Simulated Round-Robin_ (SRR) update rule, which adjusts \(\) as follows. It simply puts a global processing rate on \(I_{t}\), the set of unfinished jobs in \(I\), proportional to the cardinal of \(I_{t}^{}\), the number of unfinished jobs in \(I\) had RR be run instead (this counterfactual quantity can be computed at any time step as predictions are correct, see proof of Theorem 4.3) and similarly for \(J\). Formally, \(_{t}^{}=|I_{t}^{}|/(|I_{t}^{}|+|J_{t}^{ {RR}}|)\). Figure 6 gives an illustration of the SRR update rule: to the left, it shows a run of RR, where initially the 5 jobs run each with a processing rate \(1/5\) until job 1 terminates, then the remaining jobs run each with a processing rate of \(1/4\) until job 2 terminates, and so on. To the right, the figure shows a run of PAR with SRR update rule, where the algorithm knows the sizes of jobs 1,2 and 3. The total processing rate of these jobs during the run of the algorithm is represented by the yellow area, and it is identical to their total processing rate during the run of RR.

**Theorem 4.3**.: _If PAR uses a processing rate \(_{t}^{}\), then it is at most \(2-\)-competitive._

The competitive ratio of PAR with processing rate \((_{t}^{})_{t 0}\) decreases as \(B\) grows, going from \(2\) to \(1\), thus interpolating the non-clairvoyant and the clairvoyant settings.

## 5 Experiments

In this section, we test the performance of the algorithms we presented for the ski-rental, secretary, and scheduling problems, supporting our theoretical results and giving further insight.

For the ski-rental problem, we set a buying cost of \(b=50\) for Figure 6 and \(b=100\) for Figure 6, and the number of snow days is sampled randomly from a uniform distribution in \([1,4b]\). Each point in both figures is computed over \(10^{5}\) simulations. The value of \(\) is chosen optimally with respect to \(p\) as indicated in Lemmas 2.2 and 2.3.

The competitive ratios of both algorithms 1 and 2 when the prediction is given as input are shown in Figure 6, as well as their theoretical upper bounds. The experimental ratio of the deterministicalgorithm in this particular scenario is significantly better than the theoretical upper bound, while that of the randomized algorithm is close to the theoretical upper bound. In Figure 6, we consider that the oracle is correct with a time-dependent probability \(p_{t}=0.95-0.4(-t/5)\). We show the competitive ratios obtained by renting until time \(t\) then querying a prediction and running the deterministic or the randomized algorithm. The theoretical upper bounds are represented in dotted lines. We observe that adequately choosing the time of querying the prediction can significantly improve the competitive ratio, which proves our claims. This time strongly depends on the cost \(b\) and on the evolution of the probability \(p_{t}\) with time.

In Figure 8, we test AdaThresh with \(N=1000\), and we show how the success probability improves with \(B\). We also test a variant of the algorithm memorizing the value of the best-observed applicant after restarting, which improves the success probability (See Appendix B.3 for a detailed description of the algorithm). The success probability of AdaThresh matches \(q_{B}\), therefore it is a tight lower bound. We also observe that the success probability increases rapidly for the first values of \(B\), but then becomes slower. This is because whenever the algorithm is restarted, there is a new observation phase, with a risk of missing the best applicant.

Secondly, we test the algorithm PAR with the SRR update rule on various benchmark inputs. We test it with \(N=50\) and jobs having (i) identical sizes, (ii) sizes sampled from the exponential distribution, (iii) uniform distribution, (iv) and Pareto distribution. (i) is a critical instance because it is the worst-case input for RR. (ii) is a classical benchmark used in many variants of the scheduling problem to prove lower bounds . (iii) is a natural benchmark to test randomness. Finally, (iv) is well-suited in practice for modeling the job size distributions , and also it shows how the algorithm behaves on instances with very high variance. Each point in the figure was obtained by averaging over 10000 runs. we see that the competitive ratio of PAR is at most \(2-B/N\) for all these benchmarks, which is better than the upper bound \(2-(B/N)^{2}\) proved in Theorem 4.3. Therefore, PAR can be highly efficient in many cases, since the gain obtained with \(B\) known job sizes is proportional to \(B\) as shown in the simulations.

## 6 Conclusion

We presented different settings where online algorithms operate under a restricted budget of predictions, that can be queried during their execution. We show that adequately using this budget significantly improves upon the worst-case performance. Our results pave the way for investigating more realistic and practical challenges within the learning-augmented paradigm, providing insights that can be extended to various other problems in competitive analysis.