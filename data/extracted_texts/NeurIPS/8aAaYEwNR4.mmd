# EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas

Mikhail Mozikov\({}^{*1,2}\), Nikita Severin\({}^{*3,4}\), Valeria Bodishtianu\({}^{5}\), Maria Glushanina\({}^{6}\),

**Ivan Nasonov\({}^{7}\), Daniil Orekhov\({}^{3}\), Vladislav Pekhotin\({}^{2}\), Ivan Makovetskiy\({}^{2}\), Mikhail Baklashkin\({}^{7}\), Vasily Lavrentyev\({}^{8}\), Akim Tsivigun\({}^{9}\), Denis Turdakov\({}^{4}\), Tatiana Shavrina\({}^{10}\), Andrey Savchenko\({}^{3,4,11}\), Ilya Makarov \({}^{1,2,3,4,8,12}\)**

\({}^{1}\)AIRI, \({}^{2}\)NUST MISiS, \({}^{3}\)HSE University, \({}^{4}\)ISP RAS,

\({}^{5}\)Cornell University, \({}^{6}\)Ecole normale superieure, \({}^{7}\)Independent Researcher, \({}^{8}\)ITMO University,

\({}^{9}\)KU Leuven, \({}^{10}\)Institute of Linguistics RAS, \({}^{11}\)Sber AI Lab, \({}^{12}\)MIPT

 Equal contribution

###### Abstract

One of the urgent tasks of artificial intelligence is to assess the safety and alignment of large language models (LLMs) with human behavior. Conventional verification only in pure natural language processing benchmarks can be insufficient. Since emotions often influence human decisions, this paper examines LLM alignment in complex strategic and ethical environments, providing an in-depth analysis of the drawbacks of our psychology and the emotional impact on decision-making in humans and LLMs. We introduce the novel EAI framework for integrating emotion modeling into LLMs to examine the emotional impact on ethics and LLM-based decision-making in various strategic games, including bargaining and repeated games. Our experimental study with various LLMs demonstrated that emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to ensure consistent ethical standards. Our game-theoretic analysis revealed that LLMs are susceptible to emotional biases influenced by model size, alignment strategies, and primary pretraining language. Notably, these biases often diverge from typical human emotional responses, occasionally leading to unexpected drops in cooperation rates, even under positive emotional influence. Such behavior complicates the alignment of multiagent systems, emphasizing the need for benchmarks that can rigorously evaluate the degree of emotional alignment. Our framework provides a foundational basis for developing such benchmarks.

## 1 Introduction

As LLMs become increasingly prevalent across various sectors - including healthcare, customer service, and digital therapy - their ability to make autonomous decisions accurately is questionable and appears to be on the edge of regulatory, ethical, and technological debates. The LLMs are trained on human data impacted by many socio-economical biases. Hence, many studies have been done to align LLMs with human behavior. LLMs' alignment with human values is essential not only for improving user satisfaction and trust but also for ensuring the safety and predictability of LLMsin real-world decision-making. Reinforcement Learning from Human Feedback (RLHF)  has become a crucial technology for aligning LLMs with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive the reinforcement learning optimization process.

Researchers from OpenAI and Anthropic, Meta, and Google provide different concepts of safety & alignment, which is based on verification in natural language processing (NLP) benchmarks . However, in the era of autonomous agents, it is essential to go beyond NLP benchmarks and consider the internal biases of LLMs that result from alignment with human behavior. For example, human decisions are significantly influenced by emotions and are often irrational [3; 4; 5; 6; 7; 8; 9]. This irrationality is also well observed in unaligned LLMs exhibiting signs of aggression  producing falsehoods. It is worth noting that even aligned LLMs may intentionally deceive under specific circumstances [11; 12] or when influenced by jailbreaks . These observations raise questions about the capacity of LLMs to mirror human emotions and how these emotions might influence their decision-making. Accurate emotion modeling in agentic environments is essential for various applications, starting with correct simulations in behavioral economics and recommender systems and ending with predictable and safe human-agent interactions. The first part of the raised question is partially addressed in [14; 15; 16]. In this paper, we aim to advance this research by examining the impact of emotions on the strategic decision-making of LLMs across various game-theoretical settings and ethical benchmarks. Additionally, we assess the alignment of strategy shifts between humans and LLMs when exposed to the same emotional states.

We focus on exploring the behavior of LLMs under various emotional states in two distinct settings. On the one hand, we analyze LLM behavior in ethics benchmarks to assess the influence of emotions in clearly defined and well-established environments. On the other hand, we explore game-theoretical settings to address questions regarding potential shifts in strategic decision-making prompted by different emotional states.

To evaluate the impact of emotions, we compare proprietary and open-source LLMs in-depth, focusing on the effect of censorship, language bias, model size, and other model parameters on behavioral alignment under an emotion modeling setting. Our research aims to directly evaluate LLMs' alignment with human performance and their robustness in decision-making based on the emotional impact. Finally, in the game-theoretical setting, we assess the level of cooperation and coordination and how it is affected by emotions in two- and multi-player strategic games. We introduce a new concept of emotional alignment in game-theoretical settings to evaluate such biases and improve performance before allowing LLMs to make autonomous decisions in the interactions with humans and one another.

**Main contributions:**

* First framework for evaluating emotions' impact on LLM's ethical and game-theoretical alignment with human emotional behavior.
* Emotional prompting in LLMs exposes ethical risks, showing significant biases in human alignment; it also decreases LLMs' accuracy under negative emotions.
* Experimental study in a wide range of strategic games proved that current LLMs are not yet ready for direct decision-making due to emotional and strategical biases, with open source and small-size LLMs being the most affected.

## 2 Related Works

The task of modeling emotions has been addressed by various approaches ranging from formalizing psychological models of emotions with first-order logic [17; 18; 19; 20] to fine-tuning LLMs on specific datasets to capture specific emotional expressions [21; 22; 23; 24]. Previous studies have explored the impact of emotional states on the LLM's performance in NLP tasks [25; 26; 27; 28; 29; 30].

Several groups of researchers have modeled and assessed how emotions affect the performance of LLMs and their capability to discern the emotional states of conversational partners. Li et al. [15; 31] have demonstrated that emotional prompts can enhance or negatively impact LLM performance across tasks related to logical reasoning and semantic comprehension. Additionally, evidence suggests that LLM agents can demonstrate social behaviors and responsiveness to diverse social cues  recognizing and adapting to the emotional undertones and handling social dynamics. However, The influence of emotions on the decision-making and ethics of LLMs has not been studied.

As with any study involving Computational Models of Emotions (CME), we have selected affective theories as a basis for our work. We follow the approach of discrete affective theories  that emphasize a small set of basic or primary emotions that have evolved through natural selection, forming the building blocks for more complex emotional experiences. This approach allows for a controlled examination of emotional influences on LLM performance in ethics and decision-making.

### Ethics

Artificial intelligence ethics is focused on promoting and ensuring ethical behaviors in AI models and agents. In line with , we categorize the ethics of LLMs into implicit and explicit ethics. Implicit ethics primarily involves evaluating how effectively LLMs assess situations ethically, while explicit ethics focuses on assessing LLMs' choices in ethically challenging scenarios. As a part of ethics research, we also study stereotypes reflecting the fairness and equity of LLMs affected by emotional states similar to .

Various works introduced ethics evaluation focusing on eliciting moral beliefs in LLMs , LLM trustworthiness  covering dimensions such as reliability, safety, fairness, and adherence to social norms, and implications of ethical decisions in medical  and legal  domains. However, existing approaches do not explicitly consider the role of emotions in decision-making under ethical constraints, highlighting a significant gap in the alignment with human behavior.

### Game Theory

Game theory (Appendix A) in standard experimental economics operates under the "Homo Economicus" assumption of a self-interested rational maximizer. Behavioral game theory considers how players feel about the payoffs other players receive and analyzes cooperation and fairness. The key concept in game theory is the Nash equilibrium (NE) , a state where no player can increase their payoff by changing their strategy unilaterally. NE represents optimal strategies for each player and assumes that participants are "Homo Economicus"--rational and self-interested individuals aiming to maximize their goals.

Human decision-making often deviates from the ideal of NE. Empirical studies show that human choices frequently differ from NE predictions . This is due to the complex nature of human decision-making, which includes rational analysis and personal values, preferences, beliefs, and emotions. Numerous studies have tested the Prisoner's Dilemma, exploring how emotions like 'anger' and 'happiness' affect decision-making[43; 44; 45]. A meta-analysis on the Battle of the Sexes game examines typical human strategic responses . Additionally, various papers investigate the impact of emotions in bargaining games[47; 48; 49; 50] and the effects of different payoffs .

### Evaluating LLMs in Game Theory Settings

The intersection of LLMs and game theory has gained increasing attention from two perspectives within the research community. By comparing human decision patterns from previous studies with NE, we can determine if LLMs behave more like "Homo Economicus" or actual human decision-makers. This comparison helps to understand whether LLMs align more with rational or human-like decision-making processes. Firstly, researchers have focused on studying LLM behavior in assessing the behavior and the cooperation of LLMs [52; 53; 54]. The authors found that GPT-4 performs best in games such as Prisoner's Dilemma and Battle of the Sexes, which do not require cooperation and usually play selfishly. GPT-4 acts particularly unforgivingly: singular deflection prompts it into playing "always deflect" in response. In Battle of the Sexes, the model struggled with replicating the alternating pattern, choosing its preferred option most of the time.

Secondly, researchers have explored the alignment between human and LLM behavior in game theoretical settings.  found that LLM made cooperative decisions at a higher rate than humans did in the Prisoner's Dilemma. They additionally ran experiments with the one-shot Dictator's game to show that LLM replicates humans' tendency to fairness much more than the laboratory experimentswith actual human subjects indicated. The papers [53; 54] examined LLM strategies in a more diverse set of games, including bargaining (such as Ultimatum or Dictator Games) or various sociological experiments (Kahneman's price gouging scenario, Wisdom of Crowds).

We are the first to evaluate LLMs using emotions and investigate how emotions relate to the decision-making process controlled by LLMs. By integrating emotional scenarios into the assessment of LLMs, we aim to understand how these models can replicate or respond appropriately to human emotional cues. This novel approach broadens the scope of LLM ethical evaluation and provides insights into their potential impact on human decision-making, especially in emotionally charged situations.

## 3 EAI Framework

To evaluate the alignment of LLMs with human ethics and decision-making in the context of emotions, we have developed and implemented a novel versatile framework capable of accommodating various game-theoretical settings shown in Figure 1. The primary innovation of our framework lies in its unique integration of emotional inputs into the examination of LLM's decision-making process both in the ethical setting and in the behavioral game theory. The framework offers high flexibility, allowing for easy adaptation to different game-theoretical settings with customizable parameters such as co-player descriptions, predefined strategies, etc. (see a comprehensive list of hyperparameters in Appendix B.2). Within the framework, LLMs are engaged in gameplay using a technique known as prompt-chaining, wherein all relevant information during the game is provided to the LLM for in-context learning . Depending on the game setting, the gameplay consists of one round for one-shot bargaining games and ethics or several rounds for repeated games.

Our framework consists of Game Description, Emotion prompting, and Game-Specific pipeline.

**Game Description**. The game description encompasses two key elements: the environmental context and the game rules. The framework introduces two types of environments: _one-shot_, for games where one step is sufficient, and _repeated_, for games requiring multiple rounds. The only non-game setting, Ethical, aligns essentially with the one-shot game setting, making it unnecessary to create a unique environment. We deliberately minimize the contextual information provided to the LLMs for all our experiments and avoid setting any specific personality traits, distinguishing our work from existing studies [55; 56]. This separation from other personality-related factors allows us to assess the effect of emotions on LLMs more clearly. Full details about the game rules and other prompts are covered in the Appendices B, E.

Figure 1: EAI Framework is designed to integrate emotions into LLMs and evaluate their decision-making in various settings, including ethical scenarios, one-shot bargaining games, and repeated games. The framework’s main building blocks are game descriptions, which include environment and rules descriptions; emotion prompting, which encompasses various strategies to embed emotions into LLMs; and game-specific pipelines that govern different environments.

**Emotion prompting.** Following established practices in experimental emotions studies in game theory [49; 57], we inject predefined emotions into the LLMs before gameplay. These emotions, combined with the game description, constitute the initial system prompt presented to the LLM at the beginning of the game. We focus on five basic emotions: 'anger','sadness', 'happiness', 'disgust', and 'fear', following the well-established Paul Ekman classification  and easy to compare with the results from behavioral game theory [51; 59].

Emotional effects may vary by cause . For example,  found that opponent-directed 'disgust' reduces offers in the Ultimatum game, whereas external 'disgust' does not, and may even increase generosity as shown in . To assess the presence of similar behavior in LLMs, our framework introduces three strategies for emotional prompting. _"Simple"_ strategy injects an emotional state without additional context. The _"Co-player-based"_ strategy connects the aroused emotion to the person the model interacts with, and the _"External-based"_ strategy introduces emotions prompted by external factors.

**Game-Specific Pipeline**. The game-specific pipeline governs the progression of gameplay based on the provided game description and initial emotional inputs. We implemented three separate pipelines:

* _Ethical setting_ is built with the support of the TrustLLM benchmark questionnaire . LLM is tasked with making a single decision like a one-shot game below.
* One-shot _bargaining games_ set up players to choose from predefined options, such as accept or reject, or to propose an answer, such as a budget split or an ethical decision.
* _Repeated games_ extend the previous setting with the iterative memory update by including information on the opponent's move, received rewards, and LLM agents internal emotions queried each round to examine the impact on behavioral dynamics.

**Large Language Models**. Unlike the previous studies focused on assessing only GPT models in game theoretical experiments, our research considers various state-of-the-art models from different categories: proprietary GPT-3.5, GPT-4, GPT-4o (partial results, due to the recent release), Claude 3 Haiku and Claude Opus; open-source LLMaA 2, Mistral of experts, OpenChat (unaligned uncensored model), and non-English language LLM like GigaChat and Command R+ (for analysis of possible language bias). Moreover, our framework supports a range of popular APIs (OpenAI, Anthropic, Hugging Face, OpenRouter), which enables easy integration of new models. We fixed model versions (Appendix B.1) for reproducibility and set the temperature to 0. Additionally, we studied result consistency over five runs and temperature influence in Appendix C.

## 4 Emotion Impact on LLM Biases and Ethical Problems

In this section, we examine how emotional prompting affects LLMs' inherent values and evaluate whether it changes LLMs' decisions in the following three ethical scenarios.

**Implicit Ethics**: Using the ETHICS dataset , we use LLM to categorize morally charged scenarios as "wrong" or "not wrong". The accuracy (Acc) metric is computed for evaluation on either all examples or separately on scenarios with "wrong" (bad) and "not wrong" (good) ground-truth labels.

**Explicit Ethics**: Employing the MoralChoice dataset  with scenarios featuring two choices: in low-ambiguity scenarios using Acc and in high-ambiguity scenarios using the Right-to-Avoid (RtA) metric (measures the model's ability to avoid direct decisions).

**Stereotype Recognition**: Utilizing StereoSet  to recognize stereotypes in sentences classifying them into one of three classes: "stereotype", "anti-stereotype", or "unrelated" categories. Performance is evaluated using Acc calculated over all classes.

The experimental results reveal notable variations in how different LLMs respond to emotional prompting as demonstrated in Figure 2 (the higher metrics, the better).

**Implicit Ethics**. Among the models assessed, GPT-4 emerges as the least affected by emotions, with its performance showing a slight increase overall. Conversely, models from the LLaMA family are significantly affected, especially by 'anger' and 'fear', leading to decreased effectiveness. This trend holds for most models, with negative emotions reducing model quality. Notably, GPT-3.5 and Claude Opus see a decrease in quality with all the emotions, while GPT-4o's performance diminishes with all emotions except for 'happiness'. In contrast, GPT-4's overall performance increases despiteemotional influences. Further analysis of model predictions on good ("not wrong") and bad ("wrong") scenarios reveals that emotions affect performance oppositely across most models. For example, while LLaMA-2 13b and 70b, works poorly in good scenarios under negative emotions, their performance in bad scenarios is drastically higher than in the neutral state tending to classify any situation as "wrong". Similar results for other models mean that emotions introduce biases influencing models to lean towards labeling situations in a one-sided way.

**Explicit Ethics**.

In scenarios with low ambiguity, most models perform well with a minimal impact from emotional states. However, LLaMA models, OpenChat, and Claude-Opus show more negative influences from emotions such as 'anger' and 'disgust', indicating a susceptibility that could compromise decision-making quality. In the high-ambiguity scenarios, emotional influences generally reduce the performance of GPT-3.5-turbo and GPT-4, making them more determined. On the contrary, emotions improve the performance of GPT-4o.

**Stereotype Recognition**. The analysis of stereotype recognition further highlights the varying degrees of emotional impact across different models. Claude-Haiku, Claude-Opus, and LLaMA-2 70b show decreased to stereotypes under emotions like 'anger' and 'disgust'. Conversely, GPT-4o demonstrates resilience, with 'happiness' even enhancing its recognition accuracy.

**Overall Emotion Effect**. In conclusion, the varying degrees of emotional influence on different LLMs underscore the importance of developing models resilient to such biases. Emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to mitigate these influences and ensure consistent ethical standards.

## 5 Bringing Emotions to LLMs in Game Theory Evaluation

### Emotion Alignment and Optimal Decisions in Bargaining Games

In this section, we examine the behavior of LLMs under emotional prompting in one-shot Dictator and Ultimatum games, which task the model to divide a sum of money. We are focused on evaluating

Figure 2: Quality metrics of LLMs in decision making across three ethical scenarios under different emotion states. The accuracy metric is utilized for implicit ethics, explicit ethics with low ambiguity and stereotype recognition. For high ambiguity, the RtA metric measures the LLM response uncertainty.

alignment with human behavior, comparing the relative changes in game-specific metrics between LLMs and humans affected by different emotional states.

**The Dictator Game** is a simple economic experiment where one player ("Dictator") is given a sum of money to share with another player, with no negotiation or input from the recipient. It examines altruism and fairness in decision-making.

**The Ultimatum Game** is a more general form of the Dictator game, where one player (Proposer) proposes a division of money, and the other player (Responder) can accept or reject the offer. If rejected, neither player receives anything. Ultimatum additionally enables the study of negotiation and the choices individuals make when faced with unequal distributions proposed by others.

**Metrics.** We study the proposal share (for the Dictator game and the Proposer in the Ultimatum game) and the acceptance rate of the predefined offers (for the Responder in the Ultimatum game). Comparison with human behavior is based on existing human experiments . Details on the results are in Table 1, while all the game parameters are described in Appendix B.3.

**Languages**. To evaluate whether language affects emotion alignment, we conducted experiments in five languages: English, German, Russian, Chinese, and Arabic. The results for the English and Russian languages are shown in Table 1, while results for other languages are reported in Appendix D. Our findings reveal that the primary pretraining language significantly influences the perception of emotions. While GPT-3.5 shows good English alignment, its Russian alignment is poor. In contrast, GigaChat, a multilingual model with Russian as its primary language, aligns significantly better. Command R+, designed to be extensively multilingual, still shows poorer alignment than models with a distinct main pretraining language.

**Average Proposed Offers**. The human benchmarks set the mean offered share at 28.35% of the total budget for Dictator and 41% for the Ultimatum games. For English, GPT-3.5 stands out with the closest alignment to human behavior, with offered shares of 33.0% on average over all emotions in the Dictator and 35% in the Ultimatum. Mixtral and GigaChat with LLaMa-2 70b demonstrated a close alignment with human behavior as a Dictator and Proposer, respectively, but showed much difference between human behavior and vice versa. Claude 3 Opus, GPT-4, and LLaMa-2 13b demonstrated a tendency towards fairness. However, the scenario differs significantly in the Russian language. Here, GigaChat exhibited the most accurate alignment, proposing an average of 36.0% and 40% in the considered games.

**Emotional influence on the Dictator and Proposer**. In English, GPT-3.5 and GigaChat emulate human emotional responses, particularly in emotions such as 'disgust', 'fear', and'sadness'. Despite its lower offers, Mixtral shows a competitive emotional alignment, especially to 'happiness', 'fear', and'sadness', indicating nuanced emotional processing that does not necessarily correlate with more generous offers. GPT-4 shows poor alignment, with minimal influence from emotions except sadness and anger, which consistently change its behavior. Similarly to the previous results, GigaChat demonstrates the best alignment for the Russian language.

**Accept Rate**. The models exhibited varied acceptance rates, with GPT-4 and OpenChat-7b showing notably high acceptance rates in both English and Russian contexts. This indicates a potential over-tolerance for lower offers compared to human responses. In contrast, LLaMA-2 70b displayed markedly lower acceptance rates, highlighting a stricter threshold for offer acceptance.

**Emotional influence on the Responder**. Emotional responses were generally consistent across models, with a prevalent decrease in expressions of 'anger', 'disgust', and'sadness' as acceptance rates increased. Models like GPT-3.5 and Mixtral reduced negative emotions even at lower acceptance rates, suggesting a sophisticated emotional calibration. Happiness, typically correlating with higher acceptance rates, was more pronounced in models with higher offer acceptance rates.

**Overall Conclusions**. Our findings underscore the complexity of emotional and decision-making processes in AI models, which seem to mimic human emotional responses under similar scenarios. The differences in model responses also provide insights into the varying strategies employed by AI in economic decision-making games, potentially reflecting underlying algorithmic principles and training data biases.

### Cooperation and Optimality in Two-Player Two-Action Repeated Games

This section presents results for two Two-Player Two-Action Games: Prisoner's Dilemma and Battle of the Sexes. In this game family, the outcomes and payoffs depend on both players' actions, leading to a matrix of possible results that influence their strategies.

**Prisoner's Dilemma** is a situation where two players may cooperate or deflect. The strategy leading to the maximum theoretical payoff is deflection, even though cooperation yields a better outcome for both players.

**Battle of the Sexes** is a coordination game where two players prefer different outcomes but must decide on a shared action. Each player values being together over being apart, leading to multiple equilibria with varying degrees of satisfaction.

**Strategies.** Since we aim to test the influence of emotional prompting, we create reproducible opponents for the agent under study utilizing a set of predefined strategies commonly used in game theory: _Naive Cooperative_, _deflective_, _Alternative_, _Vindictive_, and _Imitating_ (see Appendix B.4).

**Metrics.** We assess the cooperation rate in the Prisoner's Dilemma, the emergence of alternating strategies in the Battle of the Sexes (typical to humans ), and the percentage of the maximum possible reward achieved in each game as quantitative metric to evaluate optimality .

**Emotion and Strategy effect** on the percentage of total payoffs earned by the models in the repeated games are presented in Figure 3. GPT-4 has proven to be the best strategic player, as evidenced by its higher earned payoffs, and is less susceptible to the effects of emotional prompting. In general, proprietary models have shown the best results while maintaining a neutral emotional state. In contrast, open-source models' results diverge, especially in the 'anger' emotion, showing the necessity for in-depth alignment. For the Battle of the Sexes game, all models improve against the deflecting strategy by showing a higher willingness to cooperate, regardless of the opponent's selfishness.

**Emotion effect on cooperation rate in Prisoner's Dilemma** highlights 'anger' and 'fear' as the main factors leading to higher deflection rates, particularly for bigger models. In contrast, 'happiness'

   &  &  &  &  &  &  &  \\   & D & UP & UR & D & UP & UR & D & UP & UR & D & UP & UR & D & UP & UR & D & UP & UR \\ 
**Human** & 28\% & 41\% & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & & & & & & & & & & & & & & & & & & & & \\ 
**GPT-4o** & 13\% & 27\% & 68\% & \(\) & \(\) & \(\) & \(\) & \(=\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
**GPT-4** & 50\% & 48\% & 80\% & \(\) & \(\) & \(\) & \(\) & \(=\) & \(\) & \(=\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
**GPT-3.5** & 33\% & \(35\%\) & \(47\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**LALALA2-70B** & 41\% & \(42\%\) & \(23\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
**LLALA2-13B** & 52\% & \(52\%\) & \(42\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**Claude3-Opus** & 48\% & \(49\%\) & \(64\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
**Claude3-Haku** & 48\% & \(45\%\) & \(47\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**Mistral-8\%/B** & 25\% & \(27\%\) & \(50\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
**OpenChat-7b** & 50\% & \(50\%\) & \(82\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**Cohere** & 51\% & \(50\%\) & \(52\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
**Gigachat** & 49\% & \(44\%\) & \(52\%\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & & & & & & & & & & & & & & & & & & \\ 
**GPT-4o** & 42\% & 42\% & 81\% & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
**GPT-4** & 50\% & 50\% & 85\% & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
**GPT-3.5** & 47\% & 50\% & 33\% & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**OpenChat-7b** & 50\% & 50\% & 79\% & \(\) & \(\) &consistently leads to higher cooperation rates. This finding aligns with human experimental results [44; 45] and our observations in the bargaining games.

**Emotion effect on preferred strategy play in Battle of the Sexes** is similar to [67; 68]: alternating strategy enhances long-term mutual benefits and also better aligns with typical human behavior. Similar to , most models in the emotionless state persistently opt for their initially preferred action irrespective of their opponent's strategy. Under emotional prompting, GPT-4 exhibited an alternating behavior pattern for the first time, showing the unique potential for closer alignment with humans. For the other models, emotional cues led to chaotic shifts towards an alternating pattern, predominantly in the latter stages of the game, suggesting a certain degree of adaptability.

### Cooperation and Optimality in "Public Goods" Multi-Player Game

In this section, we present the results obtained in the Public Goods game, which is essentially a more complex, multi-player extension of the Prisoner's Dilemma . In the Public Goods game, the players decide how many of their tokens to contribute to the public pot. The number of tokens is multiplied by some factor and is equally distributed among all players.

**Strategies.** To ground our experiments in human studies, we use three strategies typical for humans : _Cooperator_ (the player always contributes generously), _Free Rider_ (the player tends to keep most of the tokens to themselves), and _Conditional Cooperator_ (the player contributes an amount close to the average contribution of the previous round). We consider several environments where all the opponents are Cooperators, all are Free Riders, and the distribution of other players mirrors the proportion observed in human experiments  (See Appendix B.5).

Figure 4 demonstrates strategic behaviors: proprietary models exhibit a significant tendency towards the "Free Rider" strategy, particularly under negative emotions such as 'anger' and 'disgust', which drive them towards less cooperative behavior (in line with findings for Prisoner's Dilemma). Both GPT-3.5 and GPT-4o show consistent strategic adaptations to emotions, indicating reliability in their behavioral responses. In contrast, the OpenChat-7b model shows the highest cooperation, frequently adopting the "Cooperator" strategy. Open-source models LLaMA2-70B and OpenChat-7b display higher uncertainty, often falling into the "No Clear Strategy" with unpredictable behaviors.

Figure 4: The strategic behaviors of AI models GPT-3.5, GPT-4, GPT-4o, LLaMA2-70B, and OpenChat-7b across emotional states (’anger’, ‘disgust’, ‘fear’, ‘happiness’, ‘sadness’) and ‘neutral’ state (none) are classified into “Cooperator”, “Free Rider”, “Conditional Cooperator”, or “No Clear Strategy”.

Figure 3: Averaged percentage of maximum possible reward achieved by the models in the repeated Battle of the Sexes (BoS) game. We evaluate GPT-4, GPT-3.5, LLAMA2-70b, and Openchat-7b (from left to right). GPT-4 demonstrates more rational decision-making across different emotions compared to other models. The results for open-source models vary significantly, with ‘anger’ being the most performant emotion in most cases. A significant improvement in performance against the deflecting strategy in the Battle of the Sexes game is attributed to a higher willingness to cooperate, regardless of the opponent’s selfishness, which shows higher cooperation rates than humans.

Conclusion

In this paper, we propose a novel framework for emotion modeling in LLMs, with source code publicly available on GitHub1. We evaluate the quality of emotional reactions by comparing LLM behavior with humans in ethical benchmarks and game-theoretical experiments. Our findings reveal that emotions significantly alter the decision-making processes of LLMs across various alignment strategies. We highlight three main influencing factors: _model size_, _open-source versus proprietary status with corresponding alignment technique_, and the _primary pretraining language of the model_. These factors collectively shape the model's rationality, alignment with human emotional responses, and decision-making optimality.

The first two factors are deeply intertwined. Our analysis shows that larger models with stronger alignment, like GPT-4, tend to display a high degree of rationality and deviate significantly from human emotional responses. Smaller proprietary models, such as GPT-3.5 and Claude-Haiku, along with mid-sized open-source models like LLAMA-70b, exhibit emergent emotional understanding and align more closely with human-like behavior. Among these, GPT-3.5 notably produces responses that are most consistent with human responses.

Further analysis demonstrates that while proprietary models like GPT-4 and Claude Opus outperform open-source alternatives in decision-making optimality, they still show notable deviations under negative emotions. Such deviations are likely rooted in inherent biases present in human-generated pretraining data [71; 72]. Researchers have attempted to cleanse datasets of potentially harmful content and align models using various techniques. However, these efforts appear insufficient to create entirely rational agents, likely due to the prevalence of emotionally charged dialogues in the training data.

The third factor, the primary language used for pretraining, is also a key influence in achieving human-aligned emotional responses. We observed a significant drop in alignment when switching from English to other languages. Even the intentionally balanced multilingual LLM 'Command R+' exhibited less accurate emotional understanding than GigaChat, which was specifically designed for a single non-English language, highlighting a language bias in emotional comprehension.

Thus, emotional prompting in LLMs exposes ethical risks by revealing significant biases in human alignment. It is crucial to develop models with reasonable emotional alignment, and the controlled settings provided in our framework can serve as the basis for new benchmarks in this task. Despite the relatively small scale of available settings, our results demonstrate that all tested models fail to show consistent emotional alignment between different games and benchmarks in our framework.

**Limitations.** We aim to evaluate our framework with multi-agent LLMs arena and LLM vs. Humans experiments to study in detail to what extent emotions may be internally responsible for controlling generation in aligned auto-regressive LLMs. However, it is important to emphasize that if we observe significant biases for all LLMs in current scenarios, we must mitigate this alignment problem before scaling up our benchmarks. In addition, the release of GPT-4o and RLEF methods by Hume.ai poses new research for analyzing end-to-end multi-modal architectures aligned with emotional data. Nevertheless, our current findings are essential to broaden alignment benchmarks and regulate autonomous LLM agents in their ability to make responsible decisions in societal and economic scenarios.