# Learning Generalizable Agents via Saliency-Guided Features Decorrelation

Sili Huang\({}^{1}\)  Yanchao Sun\({}^{2}\)  Jifeng Hu\({}^{3}\)  Siyuan Guo\({}^{4}\)  Hechang Chen\({}^{*5}\)

Yi Chang\({}^{6}\)  Lichao Sun\({}^{7}\)  Bo Yang\({}^{8}\)

\({}^{1,8}\)Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education

\({}^{1,3,4,5,6}\)School of Artificial Intelligence, Jilin University, China

\({}^{2}\)Department of Computer Science, University of Maryland, College Park, USA

\({}^{7}\)Lehigh University, Bethlehem, Pennsylvania, USA

Corresponding Author. Bo Yang, Hechang Chen and Yi Chang.

###### Abstract

In visual-based Reinforcement Learning (RL), agents often struggle to generalize well to environmental variations in the state space that were not observed during training. The variations can arise in both task-irrelevant features, such as background noise, and task-relevant features, such as robot configurations, that are related to the optimal decisions. To achieve generalization in both situations, agents are required to accurately understand the impact of changed features on the decisions, i.e., establishing the true associations between changed features and decisions in the policy model. However, due to the inherent correlations among features in the state space, the associations between features and decisions become entangled, making it difficult for the policy to distinguish them. To this end, we propose Saliency-Guided Features Decorrelation (SGFD) to eliminate these correlations through sample reweighting. Concretely, SGFD consists of two core techniques: Random Fourier Functions (RFF) and the saliency map. RFF is utilized to estimate the complex non-linear correlations in high-dimensional images, while the saliency map is designed to identify the changed features. Under the guidance of the saliency map, SGFD employs sample reweighting to minimize the estimated correlations related to changed features, thereby achieving decorrelation in visual RL tasks. Our experimental results demonstrate that SGFD can generalize well on a wide range of test environments and significantly outperforms state-of-the-art methods in handling both task-irrelevant variations and task-relevant variations.

## 1 Introduction

Learning control from high-dimensional visual observations is a critical requirement for real-world applications and has received significant attention in recent years . Reinforcement Learning (RL) has demonstrated its effectiveness in solving visual tasks by learning from compact representations of sensory inputs . However, current algorithms exhibit limited reliability when deployed in unseen environments, despite achieving satisfactory performance during training . It remains an open challenge to learn policies with good generalization across both changed task-irrelevant and relevant features . Consider a simple example during the evaluation stage: a well-trained robot may fail to accomplish the task when confronted with task-irrelevant visual disturbances or changes to the task-relevant robotic arm, as illustrated in Figure 1.

Generalization to task-irrelevant features in vision RL tasks has been widely discussed . Since it is a well-established consensus that task-irrelevant features should not affect the decision, agents are typically designed to maintain an invariant policy when these features are changed[25; 28; 31]. To training the invariant policy, previous works have proposed many solutions, such as bisimulation metric [2; 33], data augmentation , and contrastive learning , to learn invariant representations that filter out the environmental variations. However, the invariant representations may ignore the changed task-relevant features that lead to different optimal decisions, as shown in Figure 1(c). Although Dunion et al.  proposed a disentangled representation to preserve task-relevant features, this approach could only recover the sub-optimal decisions in the unseen environment. Therefore, a desired policy should not only remain invariant to task-irrelevant features but also adapt to changed task-relevant features.

Fundamentally, generalization fails because the agent does not understand how the changed features, whether they are task-irrelevant or relevant, affect its decisions. From the perspective of causal inference [14; 35; 39], the associations between features and decisions are often confused with each other, which can be attributed to the inherent correlations among input features in the training data. For instance, in a grasping task, heavier objects may more frequently be placed at lower horizontal heights, which could mislead the robot into adjusting its force based on height. Therefore, a promising approach is to eliminate these correlations, enabling the agent to distinguish the true association between each input feature and decisions [51; 26]. However, recent studies have indicated that achieving perfect decorrelation between all pairs of input features is challenging [45; 44]. This difficulty is especially pronounced in visual RL tasks, as the high-dimensional images that maintain sequential relationships result in complex non-linear and strong correlations among features .

To this end, we propose the Saliency-Guided Features Decorrelation (SGFD) method to eliminate the inherent correlations among input features through sample reweighting. Concretely, SGFD consists of two core components designed to address the challenges presented by non-linear and strong correlations in visual RL tasks. The first component is Random Fourier Functions (RFF), which has been demonstrated to assist in approximating the non-linear correlations with linear computational complexity . SGFD employs the cross-covariance matrix in conjunction with RFF to estimate the non-linear correlations in high-dimensional images. To address the strong correlations, SGFD integrates the second component, a classification model specifically designed to distinguish the sources of the images. Given that the values of changed features are unique to each environment, these features can be pinpointed by computing a saliency map  of the classification model. Guided by this saliency map, SGFD reweights the training samples to minimize the estimated correlations, with a particular focus on the identified features. To evaluate SGFD, we conduct various experiments on visual RL tasks from DeepMind Control Suite  and Causal World , which range from scenes with different task-irrelevant features, such as distracting background videos, to task-relevant features, such as the length of robotic arms. The experimental results demonstrate that our SGFD method can significantly improve the generalization across various types of environmental variations.

**Summary of Contributions.** (1) We propose the SGFD model, which achieves improved generalization in visual RL tasks, covering both task-relevant and task-irrelevant situations. (2) We design a sample reweighting method for RL tasks that encourages the agent to understand the impact of changed features on its decisions. (3) We validate the performance of our algorithm through various experiments and demonstrate that SGFD significantly outperforms state-of-the-arts in handling changed task-relevant features.

Figure 1: Visualizing tasks across different variations. \(a^{*}\) denotes the optimal decision, which should be invariant to task-irrelevant features but different to changed task-relevant features. Note that ‘changed feature’ refers to a feature whose value has changed across tasks.

Related Work

**Generalization in Image-Based Reinforcement Learning.** Numerous efforts have been made to achieve generalization in image-based RL tasks, employing strategies such as image augmentation [19; 20], encoding inductive biases [3; 29; 36], and learning invariant representations [49; 8], among other approaches [52; 9]. The fundamental idea behind image augmentation is to bolster the robustness of representations by introducing image perturbations [7; 30; 21]. Kostrikov et al.  applied an array of augmentation techniques, including cutouts, translation, and cropping, to the RL model. These augmented images can also be utilized to construct an auxiliary task for encoding inductive biases . Laskin et al.  sought to learn a representation that maximizes the similarity between different augmentations of the same observation. However, augmentation techniques may not alter the task-relevant features because they can impact the rewards signal  and optimal decision-making. Invariance techniques strive to learn a representation that disregards distractors in the image, thereby efficiently generalizing to unseen backgrounds [48; 11; 31]. Recently, there has been an increasing interest in using varying task-relevant situations as an additional experimental setup to evaluate a model's generalization ability . To address this situation, Dunion et al.  developed a representation that compresses changed task-relevant features into a small dimension, ensuring that the learned behaviors exhibit minimal changes in the test environment. However, this method requires slight fine-tuning to recover the optimal decisions in the testing environment.

**Features Decorrelation in Generalization.** A promising direction to achieve generalization is through features decorrelation, which eliminates the correlation between inputs features[14; 26]. Shen et al. , Kuang et al.  attempted to achieve global decorrelation on linear regression tasks by using sample reweighting. Following this, Zhang et al.  empirically extended the reweighting method to deep learning models, noting its great potential. Recently, Xu et al.  conducted a theoretical analysis of feature decorrelation and proved that perfect reweighting could help select task-relevant features regardless of whether the data generation mechanism is linear or nonlinear. However, it is not easy to eliminate the correlation between all input features under finite training samples. This is especially serious in RL tasks because data often exhibit a sequential relationship , resulting in strong correlations between features. To compensate for the imperfection of sample reweighting, Yu et al.  introduce a sparsity constraint under the assumption that the correlation between the changed features and other features is significantly smaller. However, this assumption is not applicable to RL tasks because the changed features could be task-relevant and, therefore, significantly correlated with other features in the task. In contrast, we introduce a saliency-guided model to identify the changed features without additional assumptions.

## 3 Preliminaries

**Markov Decision Process.** We define an environment as a Markov Decision Process (MDP) , represented by a tuple \(=(,,,,)\), where \(\) denotes the high-dimensional state space, \(\) is the action space, \((s^{}|s,a)\) is the state-transition function, \(:\) is the reward function, and \([0,1)\) is the discount factor. A decision-making policy, parameterized by \(\), is a function \(_{}(a|s)\) that maps a state to distributions over actions. The objective of RL agents is to find a policy that maximizes the expected cumulative return, expressed as \(_{_{}}[_{t=0}^{}^{t}(s_{t},a_{t})]\).

**Soft Actor-Critic.** We utilize Soft Actor-Critic (SAC)  as our base RL algorithm. SAC aims to maximize a variant of the RL objective augmented with an entropy term: \([_{t}^{t}(s_{t},a_{t})+(( |s_{t}))]\). To optimize this objective, SAC learns two models: a state-action value function \(_{}(s,a)\) and a stochastic policy \(_{}(a|s)\), where \(\) and \(\) are the parameters for \(_{}(s,a)\) and \(_{}(a|s)\), respectively. The parameters \(\) of the state-value function are trained by minimizing the soft Bellman residual, given as:

\[_{}()=_{(s_{t},a_{t})}[ (_{}(s_{t},a_{t})-(r(s_{t},a_{t})+ _{s_{t+1}(|s_{t},a_{t})}[V_{}(s_{t+1})]))^{2}],\] (1)

where \(\) is the replay buffer storing the data, \(V_{}\) denotes the value function, and \(\) are the target parameters of \(\). The stochastic policy \(_{}(a|s)\) is trained by maximizing the following objective

\[_{}()=_{s_{t}}[_{a_{t} _{}}[(_{}(a_{t}|s_{t}))-_{}(s_{t},a_{t})]],\] (2)

where \(\) is a temperature coefficient.

**Notations.** For existing visual RL methods [10; 18; 43], the state \(s\) is typically compressed into a compact vector representation \(\). Therefore, we use \(\) as the default in our calculations unless stated otherwise. For clarity, we define \(_{j}\) as the \(j\)-th feature in the \(d\)-dimensional vector, and \(_{i,j}\) as the value of \(_{j}\) in sample \(i\). For instance, if \(_{j}\) corresponds to the angle of a robot arm, \(_{i,j}\) would represent the encoded value of the angle in image \(i\).

## 4 Method

In this section, we introduce the SGFD method, a sample reweighting technique designed to improve generalization across a broad spectrum of visual RL tasks, as depicted in Figure 2. Specifically, SGFD utilizes RFF to enrich image features, thereby enabling the estimation of non-linear correlations in high-dimensional images, as detailed in Section 4.1. Then, SGFD integrates a classifier model and a saliency map to detect changed features across environments. Based on the estimated correlation between identified features and other input features, SGFD adjusts the weights of the samples to achieve decorrelation, as elaborated upon in Section 4.2.

### Decorrelation with RFF

To eliminate the correlation between any pair of features, the first step is to quantify their correlation. Given a pair of features (\(_{i}\),\(_{j}\)) in the RL states, we gather a batch of \(n\) samples from the replay buffer \(\), denoted as \((_{1,i},_{1,j}),(_{2,i},_{2,j}) (_{n,i},_{n,j})\). The primary challenge lies in accurately estimating the correlation between these two features based on the available samples.

A renowned method for evaluating independence is the Hilbert-Schmidt Independence Criterion (HSIC), which calculates a cross-covariance operator in the Reproducing Kernel Hilbert Space. However, the computational cost of HSIC can be considerable, especially when the batch size \(n\) is large. To overcome this issue and apply HSIC to modern RL models, we can compute the independence using the Frobenius norm, shown to be equivalent to the Hilbert-Schmidt norm in Euclidean space . The cross-covariance matrix can be formulated as follows:

\[_{_{i}_{j}}=_{k=1}^{n}[( (_{k,i})-[(_{i})])^{T} ((_{k,j})-[(_{j})])],\] (3)

where \([(_{i})]=_{k=1}^{n}( _{k,i})\), \(\) and \(\) are vectors, and each component is a function sampled from the RFF space, formulated as follows:

\[(_{:,i})&=(u_{1}( _{:,i}),u_{2}(_{:,i}),,u_{M}(_{:,i})),u_{m} (_{:,i})_{}, m,\\ (_{:,j})&=(v_{1}(_{:,j}),v_{2}(_{:,j}),,v_{M}(_{:,j})),v_{m}(_ {:,j})_{}, m,\] (4)

Figure 2: The architecture of our SGFD. SGFD aims to reduce correlations in image features by reweighting samples. This involves five steps: (1) We fetch a sample batch from the replay buffer, which may come from multiple environments with different backgrounds or robot configurations. (2) The image in the sample is compressed by an encoder into latent features. (3) Then, we augment these features using multiple Random Fourier Functions to capture nonlinear correlations. (4) Concurrently, we train a classifier and apply saliency maps to detect features that shift across environments. (5) Finally, SGFD reweights samples to eliminate the correlations between identified features and other features.

where \(_{ RFF}\) denotes the RFF space and has been demonstrated to approximate non-linear correlation between image features . Specifically, we sample \(M\) functions from the RFF space, defined as \(_{ RFF}=\{h:x( x+)| (0,1),(0,2)\}\), where \(\) is sampled from a standard normal distribution and \(\) is sampled from a uniform distribution. The independence between \(_{i}\) and \(_{j}\) can be calculated by the Frobenius norm of the cross-covariance matrix \(||_{_{i}_{j}}||_{F}^{2}\). If \(||_{_{i}_{j}}||_{F}^{2}\) is zero, the two features \(_{i}\) and \(_{j}\) are considered independent.

We reformulate the task of correlation elimination as an optimization problem aimed at minimizing the Frobenius norm of the cross-covariance matrix \(||_{_{i}_{j}}||_{F}^{2}\). To this end, we introduce a set of learnable sample weights, denoted by \(\), subject to the constraint that \(_{k=1}^{n}w_{k}=n\). Based on the weighted samples, the independence can be defined as:

\[_{_{i}_{j};}=_{k=1}^ {n}[(w_{k}(_{k,i})-[( _{i})])^{T}(w_{k}(_{k,j})-[ (_{j})])],\] (5)

where \([(_{i})]=_{k=1}^{n}w_{k}(_{k,i})\). The correlation among all features in RL states can be eliminated by solving the following optimization problem:

\[^{*}=_{}_{1 i<j d}||_{ _{i}_{j};}||_{F}^{2},\] (6)

where \(d\) is the dimension of \(\). When the number of samples is not limited, Equation (6) is proved to have multiple solutions that achieve perfect decorrelation .

### Saliency-Guided Decorrelation with RFF

In practice, achieving desirable weights that ensure perfect independence between all pairs of features with finite samples is a significant challenge . Therefore, relying solely on the features decorrelation approach introduced in Section 4.1 may not be sufficient. Furthermore, in RL tasks, the sequential nature of the data creates strong dependencies between features, further complicating the sample reweighting. To overcome this challenge, we propose a saliency-guided model that prioritizes the most crucial correlations between changed features and other image features.

The saliency map , a well-known computer vision method, uses the backpropagation algorithm to compute the attributions on neural networks. It is widely used to interpret models by visualizing which pixels are more important for decision-making. Inspired by the work of Bertoin et al. , we incorporate this functionality into the training phase by introducing a classifier model that distinguishes the environmental source of states. The contribution of each feature can be determined by computing a saliency map. For example, the contribution of the \(i\)-th feature in the state is \(M_{i}(f,Z)= f()/_{i}\), where \(f\) denotes the classifier. If the \(i\)-th feature changes across the environments, it will provide critical information for the classifier's decisions, leading to a large value of \(M_{i}(f,Z)\).

Given our focus on the correlations related to changed features, it is anticipated that when a pair of features is more likely to be part of the changed set, they should be prioritized for decorrelation. Accordingly, the probability that each feature belongs to the changed part can be calculated by the saliency map, i.e., \(p(Z_{i})=e^{M_{i}}/_{1 j d}e^{M_{j}}\). We incorporate this term into the Equation (6), revising it as:

\[^{*}=_{}_{1 i<j d}p(_{i})p( _{j})||_{_{i}_{j};}||_{F} ^{2}.\] (7)

Based on the weighted samples, we can reformulate the Equation (2) as:

\[_{}()=_{s_{t}}w_{t}_{a_{t} _{}}[(_{}(a_{t}|s_{t}))-_{}(s_{t},a_{t})].\] (8)

With the reformulated Equation 8, in which the changed features are considered independently from other features, the agent can learn the actual association with output decisions. More details of SGFD are described in Appendix A and Appendix B.

## 5 Experiments

To carry out a comprehensive investigation of SGFD, we have performed a series of experiments on visual RL tasks, utilizing the DeepMind Control Suite  and Causal World . We provide a brief introduction to the tasks, baselines, and evaluation metrics in Section 5.1. In Section 5.2, we verify the generalization capabilities of SGFD across a variety of tasks, ranging from scenarios with diverse task-irrelevant features to those with task-relevant features. To gain deeper insight into the workings of SGFD, we evaluated its decorrelation potential and visualized the weighted samples, as detailed in Section 5.3. Then, we conducted several ablation experiments on RFF and saliency-guided model in Section 5.4. Finally, we investigated the interpretability of RL models in Appendix C.

### Experiment Setting

**Environments.** The DeepMind Control Suite  is a widely-used visual control toolkit that offers a wealth of simulated animal behavior tasks. Drawing on previous works [48; 50], we constructed environments with both background noises and varying robot settings to test the generalization capabilities of the SGFD. The backgrounds were sampled from the Kinetics dataset , while the robot settings followed those proposed by Zhang et al. . Furthermore, we utilized the Causal World , which comprises a series of robot manipulation tasks. In these manipulation tasks, we employed perception states to evaluate the potential of SGFD for decorrelation. More implement details are provided in Appendix B.

**Baselines.** We compared our method against several recently established baselines, which we outline as follows. Our baseline for disentanglement representation is TED . TED compresses changed features into a small dimension, thereby ensuring that learned behaviors exhibit a minimal change in the testing environment. We used DBC  as a baseline method that learns invariant representations based on the bisimulation metric. We also compared with AMBS , an enhanced version of DBC that integrates a contrastive method. In addition, we compared with SGQN , a novel method that uses data augmentation and the saliency map to ignore the task-irrelevant features in the images. Lastly, we included Domain Randomisation (DR)  in our comparison, a technique commonly utilized in control tasks.

**Evaluation Metrics.** The primary metric under consideration is the cumulative reward from the testing environments, which reflects the generalization capabilities of the learned policy models. Additionally, we employed the Pearson correlation coefficient to assess the correlation based on samples, both with and without weighting. Lastly, we utilized the saliency map as an interpretability to visualize the learned policy models. All experimental assessments calculate the mean and standard deviation of the results across five seeds unless otherwise stated.

Figure 3: The normalized performance of generalization under changed task-irrelevant and relevant features. Each polygon represents one algorithm across 10 tasks. Each vertex of the polygon denotes the normalized performance, which matches counterclockwise from walker-walk to MT-finger-turn. Note that the results related to task-relevant features are the average of the two settings presented in Table 2.

### Evaluation on the Generalization of SGFD

**Evaluation on Changed Task-Irrelevant Features.** In this experiment, we assessed the generalization capability of our SGFD model with respect to different task-irrelevant features. During the training phase, the policy model was allowed to interact with the environments 1 million times. During the evaluation, we assessed the cumulative rewards in a testing environment with previously unseen backgrounds. As indicated in Table 1 and vertices 1\(-\)5 of the polygons in Figure 3, our model outperformed other baselines under the background noises. Although AMBS and SGQN aimed to direct the policy model to focus on task-relevant features, the learned representations may still contain partial background features that were not effectively filtered out. Consequently, the policy model established associations with these tasks-irrelevant features, leading to incorrect decision-making when the background changed. In contrast, SGFD enabled the policy to understand that background features should not affect decision-making, thereby achieving superior generalization performance. The results indicate that decorrelation is effective in RL tasks with background noises and that SGFD exhibits strong generalization capabilities to new values of task-irrelevant features.

**Evaluation on Changed Task-Relevant Features** To evaluate the generalization ability of SGFD with respect to task-relevant features, we conducted evaluations under two experimental setups: interpolation and extrapolation. In the interpolation setup, task-relevant features underwent changes that were interpolated between those in the training environments. In contrast, in the extrapolation setup, changes exceeded the range observed in the training environments. As depicted in Table 2 and at vertices 6\(-\)10 of the polygons in Figure 3, SGFD demonstrated superior performance under both setups. Generally, extrapolation poses a more significant challenge for generalization, as it necessitates the model to comprehend how changed features impact optimal decision-making. Owing to the associations between changed features and decisions were not confused, our model outperformed other baselines, with more substantial performance gaps observed in the extrapolation

   Tasks & **SGFD** & TED & AMBS & SGQN & DBC & DR \\  walker-walk & **959.1\(\) 26.3** & 871.5\(\) 60.6 & 926.7\(\) 53.2 & 815.1\(\) 53.9 & 800.9\(\) 41.4 & 712.4\(\) 93.7 \\ cheetah-run & **599.6\(\) 47.2** & 544.7\(\) 22.9 & 517.7\(\) 73.4 & 332.8\(\) 55.1 & 312.1\(\) 20.3 & 340.0\(\) 44.0 \\ finger-spin & **965.7\(\) 45.9** & 932.1\(\) 71.0 & 925.1\(\) 50.5 & 943.3\(\) 46.2 & 663.7\(\) 68.7 & 860.8\(\) 42.1 \\ walker-run & **420.7\(\) 39.2** & 387.8\(\) 27.1 & 398.7\(\) 32.0 & 317.2\(\) 34.5 & 332.4\(\) 37.1 & 231.3\(\) 8.9 \\ finger-turn & **984.3\(\) 11.5** & 963.9\(\) 94.5 & 966.7\(\) 37.0 & 971.3\(\) 26.0 & 931.2\(\) 41.6 & 947.2\(\) 21.7 \\  Total & **3929.5** & 3700.0 & 3734.9 & 3379.7 & 3040.3 & 3091.7 \\   

Table 1: Generalization to changed task-irrelevant features. For each task, we cost \(166\) steps in training environments and evaluate in the testing environment under background noises.

    & Tasks & **SGFD** & TED & AMBS & SGQN & DBC & DR \\   & MT-walker-walk & **549.4\(\) 42.5** & 471.9\(\) 18.3 & 532.5\(\) 81.7 & 287.1\(\) 34.5 & 245.7\(\) 47.4 & 343.8\(\) 70.6 \\  & MT-cheetah-run & **395.9\(\) 35.2** & 367.8\(\) 42.2 & 298.6\(\) 53.5 & 225.7\(\) 40.9 & 191.7\(\) 23.5 & 216.3\(\) 33.1 \\  & MT-finger-spin & **234.1\(\) 11.9** & 201.7\(\) 17.9 & 161.3\(\) 17.3 & 135.7\(\) 16.9 & 221.6\(\) 21.5 & 207.1\(\) 28.2 \\  & MT-walker-run & **170.3\(\) 05.7** & 125.6\(\) 13.2 & 161.4\(\) 06.7 & 126.3\(\) 18.6 & 97.2\(\) 19.0 & 160.3\(\) 16.6 \\  & MT-finger-turn & **923.7\(\) 26.1** & 748.9\(\) 44.3 & 821.8\(\) 52.3 & 786.6\(\) 65.6 & 358.5\(\) 83.9 & 704.7\(\) 70.1 \\   & Total & **2273.4** & 1910.6 & 1975.6 & 1561.4 & 1114.7 & 1632.2 \\   & MT-walker-walk & **541.7\(\) 65.4** & 365.9\(\) 17.7 & 467.5\(\) 91.7 & 271.2\(\) 75.4 & 229.8\(\) 89.9 & 307.8\(\) 58.9 \\   & MT-cheetah-run & **392.3\(\) 32.1** & 311.9\(\) 52.7 & 270.2\(\) 35.5 & 167.2\(\) 39.1 & 174.0\(\) 45.1 & 196.6\(\) 49.8 \\   & MT-finger-spin & **231.8\(\) 11.5** & 199.7\(\) 18.0 & 160.2\(\) 17.6 & 135.6\(\) 11.3 & 221.4\(\) 43.0 & 197.1\(\) 21.5 \\   & MT-walker-run & **170.0\(\) 07.2** & 126.7\(\) 13.2 & 156.2\(\) 07.5 & 118.9\(\) 18.2 & 89.7\(\) 19.7 & 156.9\(\) 12.7 \\   & MT-finger-turn & **917.3\(\) 22.6** & 743.6\(\) 58.3 & 803.5\(\) 57.4 & 653.3\(\) 56.6 & 335.6\(\) 56.5 & 611.7\(\) 53.6 \\   & Total & **2253.1** & 1747.8 & 1857.6 & 1346.2 & 1050.5 & 1470.1 \\   &  &  &  &  &  &  & \\ 

Table 2: Generalization to changed task-relevant features. For each task, we cost \(16\) steps in training environments and evaluate in the testing environment with different robot parameters. We consider two setups for evaluation: an interpolation setup and an extrapolation setup where the variations in the task-relevant features are interpolations and extrapolations between the variations of the training environment, respectively.

setting. These results demonstrate that SGFD can effectively adapt to new values of task-relevant features, showing its robust generalization capability in a variety of scenarios.

### Case Studies on the SGFD

**Correlation of the Weighted Samples.** To evaluate the decorrelation capability of our model, we implemented a 'Picking' task in Causal World. This task involves a robotic arm grasping objects, enabling the model to access the state with physical meaning. We recorded the Pearson correlation coefficients among features under three conditions: a) on raw data, b) on weighted data, and c) on saliency-guided weighted data. Among these features, we singled out \(s_{10}\), a feature that represents mass, as the focal feature for environmental variation. Consequently, when the model is given the task of handling an object with a new mass, it becomes critical to carefully examine the correlations between the mass feature and other features. As depicted in Figure 4, due to limited samples and strong correlations, the effectiveness of direct decorrelation of all features is significantly diminished.

Figure 4: Pearson correlation coefficients among features: a) on raw data, b) on weighted data, c) on saliency-guided weighted data. The features \(s_{1}\) to \(s_{10}\) represent the state information of the environment and act as conditional inputs for decision-making \(_{}(a|s)\), such as the coordinates of the object or the angle of the manipulator. The feature \(s_{10}\) is the one that varies across the environments; thus, the correlations between \(s_{10}\) and other features play a crucial role in generalizations. The strength of the correlation is reflected by the color intensity - the darker the color, the stronger the correlation.

Figure 5: Visualization of weighted samples obtained from different environments with background noises and robot parameters. States that belong to the same dotted box with a specific color are considered semantically similar. SGFD balances the weights of samples to eliminate the correlations between changed features and other features. As shown in (a), due to the interference of the background noises, the robot fell more often (states in the green dotted box) than it did without the background. SGFD reduces the weighting of states with backgrounds and enhances the weighting of states without backgrounds, effectively neutralizing the correlation between background and body posture. A comparable result was observed in (b), where the reweighting process aimed to equalize the number of states in each group across environments with different body lengths.

In contrast, our SGFD model can effectively reduce the correlations between the mass feature (\(s_{10}\)) and other features. We also conducted an experiment involving changes in multiple features and obtained similar results. Detailed information about this experiment is described in Appendix C.1.

**Visualization of the Weighted Samples.** To delve deeper into the implications of weighted samples from our SGFD method, we collected 20 state instances during training, each exhibits distinct backgrounds or robot parameters from various environments. The identified states were sorted into three groups in each environment, each consisting of multiple similar state instances. As illustrated in Figure 5 (a), the green dotted boxes indicate several states with "fall" posture. Due to the impact of background noises on the policy, we identified 4 similar states in the environment \(a\) and 2 similar states in the environment \(b\). After the reweighting process, both environments \(a\) and \(b\) had approximately 3 similar states, effectively neutralizing the correlation between background and body posture. A similar outcome was observed in Figure 5 (b), wherein the reweighting process endeavored to balance the number of states in each group across environments with varied body lengths. As a result of the correlation removal, the policy model was better equipped to accurately capture the association between the changed features and decision-making.

### Ablation Studies

**Ablation on RFF.** To examine the role of RFF, we compared the complete SGFD algorithm, the decorrelation method without RFF, and the algorithm without any decorrelation. The testing environments comprise varied backgrounds and robot parameters. As depicted in Figure 6 (a), the decorrelation without RFF outperforms the approach without decorrelation but is significantly inferior to SGFD. This discrepancy arises due to the complex nonlinear correlations of features within the image, which RFF can approximate. Without RFF, the basic decorrelation method can only eliminate linear correlations, thus achieving limited improvement.

**Ablation on Saliency-Guided Model.** We conduct a separate ablation study on the saliency-guided model to evaluate its impact on the generalization capability of the overall model. Analogous to the RFF study, we test the performance of three distinct approaches: no decorrelation, decorrelation, and saliency-guided decorrelation. As shown in Figure 6 (b), the performance significantly deteriorates when the saliency-guided model is removed from SGFD. This outcome is predictable because directly decorrelating all pairs of features is challenging, thereby complicating the model's ability to discern the associations between the variant features and decisions.

We conduct additional ablation studies for further insights into the roles of RFF and the saliency-guided model. Detailed analyses and results of these studies are available in Appendix C.2. Moreover, we report on the learning curve of the classifier, as shown in Appendix C.3.

## 6 Conclusion, Limitations, and Broader Impact

In this work, we introduce SGFD, a sample reweighting method designed to enhance generalization in visual reinforcement learning tasks across environments with unseen task-irrelevant and task-relevant features. SGFD is composed of two core components: RFF and a saliency-guided model, which empower the RL agent to understand the impact of changed features on its decisions. We assess SGFD using the DMControl benchmark, where it demonstrates significant improvements in generalization across various environmental variations. Furthermore, our case studies conducted on the Causal

Figure 6: Ablation studies. (a) The performance difference of ablation studies on the RFF. (b) The performance difference of ablation studies on the saliency-guided model. The x-axis denotes the steps in the training environments. The y-axis denotes the cumulative reward recorded in the testing environment with background noises (Cheetah-run) or varied robot parameters (MT-Cheetah-run).

World benchmark, as well as the visualization of the weighting, highlight the superior decorrelation achieved by our model. The results from SGFD underscore the potential value of sample reweighting in generalization RL tasks.

In terms of limitations, we note that our work, like many others , relies on stacking consecutive frames to approximate a fully observable condition. This might not be optimal for all scenarios, particularly when the changed features cannot be directly observed. A potential solution is to train an inference model that uses a small amount of data from the deployment environment to rapidly predict the potential variations. Regarding societal impacts, we do not anticipate any negative consequences stemming from the practical application of our method.