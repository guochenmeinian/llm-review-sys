# _Cambrian_-1: A Fully Open, _Vision-Centric_

Exploration of Multimodal LLMs

 Shengbang Tong

Ellis Brown1

Penghao Wu1

Sanghyun Woo

Manoj Middepogu

Sai Charitha Akula

Jihan Yang

Shusheng Yang

Adithya Iyer

Xichen Pan

Austin Wang

Rob Fergus

Yann LeCun

Saining Xie1

New York University

Project Lead

Corresponding Author

###### Abstract

We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures--self-supervised, strongly supervised, or combinations thereof--based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.

**Project page:** https://cambrian-mllm.github.io/

## 1 Introduction

There is a long-standing debate in philosophy about whether understanding and meaning in language require sensory grounding. Aristotle's emphasis on acquiring knowledge through sensory experience and empirical observation was central to his ancient Peripatetic school and remains influential to this day ; Aquinas famously formalized these ideas in the 13th century with the Peripatetic axiom: "_Nihil est in intellectual quod non sit prius in sensu_" (Nothing is in the intellect that was not first in the senses) . Though many philosophers disagree , it is evident that having robust and highly capable sensory grounding is at least beneficial. Consider the _Cambrian explosion_, during which the emergence of vision is believed  to have been crucial for early animals to not only find food and avoid predators but also to evolve and improve. In fact, most human knowledge (and nearly

[MISSING_PAGE_FAIL:2]

* **Connector Design**: We design a new dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens. SS3
* **Instruction Tuning Data**: We curate high-quality visual instruction-tuning data from public sources, emphasizing the importance of distribution balancing. SS4
* **Instruction Tuning Recipes**: We discuss instruction tuning strategies and practices. SS2.3
* **Benchmarking**: We analyze existing MLLM benchmarks, cluster them into 4 intuitive groups, and introduce a new vision-centric benchmark "CV-Bench". SS2.1, SS2.2

We defer a detailed review of the fundamental components and methodologies that underpin MLLM research to Appendix B

## 2 Evaluating Visual Representations through MLLMs

Current MLLMs predominantly rely on CLIP  as the visual encoder due to its pre-alignment with language and ease of adaptation to the LLM token space. However, strong language priors can be a double-edged sword--they compensate for deficiencies in learning effective visual representations  and diminish insights gained from extensive visual representation learning research. In this section, we systematically evaluate how various visual encoder choices (see Fig. 2) impact the multimodal capabilities of MLLMs. We also advocate for using MLLM evaluation as a robust framework for assessing visual representation methods, moving beyond traditional protocols like linear probing and end-to-end fine-tuning to more faithfully reflect the diverse perception challenges in real-world scenarios and to better guide the development of improved visual representations.

### Analyzing the Benchmarks

To effectively evaluate visual representations and MLLMs, we first need to select benchmarks that accurately assess the _multimodal_ capabilities of these models. We use a suite of commonly used benchmarks [24; 45; 54; 57; 83; 84; 91; 92; 96; 97; 120; 126; 137; 143], which is the intersection of those used in recent MLLM research [75; 137; 77]. To help interpret our results, we begin by analyzing the benchmarks themselves. Here, we train MLLMs with 23 different vision backbones (see Table 6) from a variety of model families (see Fig. 2) using a 2-stage instruction tuning process initially proposed in : first training connector on 1.2M adapter data from ShareGPT-4V  followed by fine-tuning both the connector and LLM on 737K instruction tuning data (see more details in Appendices G.5 and H). Full benchmark results in Table 9.

**Who's answering the question: the LLM or MLLM?** Determining whether a benchmark _truly_ needs visual input to be solved has been a persistent challenge in vision-language research [2; 26; 50; 94]. In this study, we compare the performance of MLLMs with and without visual input8, and also calculate the expected score via randomly guessing. These three conditions are visualized in Fig. 3-left, with benchmarks sorted by the difference between the average score with vision enabled and disabled. SQA-I9, MMMU, MathVista, and AI2D display less than a 5% gap between vision enabled and disabled, suggesting that these benchmarks may not significantly depend on visual input and rather heavily rely on the base LLM. TextVQA and GQA both demonstrate a nearly 40% positive gap between random guessing and vision-disabled scores, implying a strong language bias in these benchmarks. On the other hand, the vision-disabled performance on benchmarks like MMVP is notably worse than random guessing, suggesting that strong visual grounding is particularly crucial.

**Clustering the Benchmarks** To better understand the different aspects of MLLM performance, we analyze the correlations between the performance of our 23 MLLMs on each benchmark. A

Figure 2: Examples of various vision models, objectives, and architectures studied. Image from .

confusion matrix (Fig. 10) reveals that certain benchmarks, such as MMMU, are largely uncorrelated with the others. We perform principal component analysis on the benchmark scores and observe the formation of clusters corresponding to "General," "Knowledge," "Chart & OCR," and "Vision-Centric" categories (Fig. 3-right). We assign MMMU to the knowledge category based on the types of questions it includes (see Appendix D). We also find that existing vision-centric benchmarks [126; 137] are of insufficient size (see Fig. 3-right), challenging the robustness of evaluating such capabilities. These benchmarks do not cover crucial visual elements such as depth and spatial awareness.

#### Cambrian Vision-Centric Benchmark (CV-Bench)

To address the limitations of existing vision-centric benchmarks, we introduce the Cambrian Vision-Centric Benchmark (CV-Bench). With **2638 manually-inspected examples**, CV-Bench provides significantly more examples than other vision-centric MLLM benchmarks--\(3.5\) more than Real-WorldQA  and \(8.8\) more than MMVP . By repurposing standard vision benchmarks [18; 79; 154]1, we can assess models at classic vision tasks within a multimodal context. Leveraging the rich ground truth annotations from the benchmarks, we formulate natural language questions that probe the fundamental 2D and 3D understanding of the models.

As visualized in Fig. 11, CV-Bench evaluates 2D understanding via spatial relationships & object counting, and 3D understanding via depth order & relative distance. We refer details to Appendix E.

#### Instruction Tuning Recipes

MLLMs start with pre-trained LLM and vision backbones, connecting these modules with a connector such as a projector (MLP). The original LLaVA [80; 82] proposes a 2-stage frozen training process: first, pre-training a connector between frozen LLM and vision backbones using adapter data, and then fine-tuning both the connector and LLM with instruction tuning data while leaving the vision encoder frozen. Various studies [27; 63; 81; 98] have drawn different conclusions regarding the optimal training methodology for MLLMs. Here, we revisit this topic with extensive experiments.

For our experiments, we tune a set of MLLMs using Vicuna-1.5-7B as the LLM backbone and each of our 23 vision models (Table 6) as the visual encoder. We use a 737K instruction tuning data mix for all experiments here (see Appendix H). All hyperparameters are matched across each experimental setting--highlighting the impact of different tuning strategies with each visual encoder. All experimental settings and results are tabulated in Appendix F.2.

Figure 3: **Left:** Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. **Right:** Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as “General” in green, “Knowledge” in yellow, “Chart & OCR” in red, and “Vision-Centric” in blue.

**One Stage vs Two Stage Training** Recent work  advocates for skipping connector pre-training, claiming this "_reduces compute cost without harming downstream performance._" To explore whether this claim holds--especially when using non-language-supervised visual encoders--we conduct experiments using 0, 0.5M, and 1.2M adapter data. Following LLaVA's recipe , we tune only the connector on the adapter data during this first phase, before unfreezing the LLM and connector during instruction tuning on the 737K mix. Fig. 4 shows that pre-training the connector first enhances model performance and that more adapter data further improves performance across all domains. Thus, we subsequently adopt 2-stage training with 1.2M adapter data as our standard setup.

**Finding 3**:Two-stage training is beneficial; more adapter data further improves results.

**Freeze vs Unfreeze Vision Encoder** There are also mixed practices in freezing  or unfreezing  vision backbones during fine-tuning. Some argue that unfreezing the vision backbone significantly degrades performance . Our experiments demonstrate that unfreezing benefits performance across all benchmarks except for a marginal change in knowledge benchmarks (Fig. 4). We suspect this is due to the composition of the 737K instruction tuning data and the LLM-heavy focus of these benchmarks (see Section 2.1). We note that unfreezing the vision backbone introduces additional computational overhead, which prohibits testing on some larger vision models under current sharding strategies (see more details in Appendix H).

**Finding 4**:Unfreezing the vision encoder is widely beneficial. Language-supervised models always benefit; SSL models particularly benefit on vision-centric benchmarks.

### MLLMs as a Visual Representation Evaluator

As discussed in earlier sections, MLLMs provide a new interface to explore aspects of vision models beyond traditional benchmarks like ImageNet-1k linear probing. We study the 2-stage instruction tuning setting using 1.2M adapter data, 737K fine-tuning data, and frozen visual encoders to allow comparison of the widest range of models.

Figure 4: **Effect of Training Recipe on Model Performance. Boxplots display the distribution of benchmark scores across benchmark categories for different training recipes and types of visual encoders. The four training recipes include freezing the visual encoder with various amounts of adapter data (0M, 0.5M, 1.2M) as well as unfreezing it with 1.2M adapter data. Amount of Adapter Data: All model types show increased performance on general and vision-centric benchmarks; knowledge benchmarks show mixed results; OCR & chart benchmarks benefit from more data for language-supervised models. Unfreezing: Unfreezing the visual encoder with 1.2M adapter data generally benefits all categories.**We evaluate on benchmarks detailed in Section 2.1, calculating the average performance**+ for each category and visualize the results in Fig. 5 (full results in Appendix F). Our findings highlight the advantages of language-supervised models over non-CLIP models across all benchmark categories, with significantly better performance on chart and OCR-related benchmarks. We hypothesize that this is due to CLIP's _training data_, such as LAION , containing abundant OCR and text-heavy data, whereas SSL and other vision models primarily train on natural images with significantly less text content. It is also noteworthy that language-supervised models are typically trained with a very large pool of data, ranging from 400 million  to 10 billion  samples, whereas the largest vision self-supervised training dataset, like DINov2, consists of only _142 million samples_.

Footnote †: **: Before averaging, we divide the MME Perception score by 20 to have the same scale as other benchmarks.**

Additionally, we observe that higher-resolution models particularly enhance performance on chart and vision-centric benchmarks while remaining neutral on general VQA and knowledge-based VQAs. While the majority of the backbones we examine are ViT-based , **ConvNet-based architectures** (such as OpenCLIP ConvNeXt ) are inherently well-suited for high-resolution image processing  and can produce superior results on OCR & Chart and Vision-Centric benchmarks. In vision-centric benchmarks, the gap between language-supervised and other types of vision models is smaller, with a well-trained self-supervised DINov2 model even outperforming some language-supervised models.

**Finding 5:** High-res encoders greatly enhance performance on chart & vision-centric benchmarks, and ConvNet-based architectures are inherently well-suited for such tasks.

**Narrowing the gap between Language- and Self-Supervised models** Above, we observe that DINov2 stands midway between SSL models and language-supervised models on general and knowledge benchmarks, even outperforming some language-supervised models on vision-centric benchmarks. Here, we study whether the continued finetuning of an MLLM based on a SSL model

Figure 5: **Evaluating Visual Representations with MLLMs** While language-supervised models outperform self-supervised or other models, a well-trained self-supervised model like DINov2 can also achieve competitive performance on vision-centric tasks.

Figure 6: **Continued Fine-Tuning Narrows the Gap Between CLIP and DINov2.** Performance is compared with 0.7M and 5M instruction tuning data in both frozen () and unfrozen () settings. DINov2 shows significant performance improvement with increased data and unfreezing—surpassing the 0.7M  CLIP model in several benchmarks and narrowing the gap to the 5M  model in knowledge and vision-centric tasks.

can achieve performance similar to that of a language-supervised model. Specifically, we scale up the instruction tuning data from 737K to 5M (see more details in Appendix G.5), and instruction tune MLLMs with DINov2 ViT-L/14@336 and OpenAI CLIP ViT-L/14@336 encoders in both frozen and unfrozen settings. In Fig. 6, we observe that by unfreezing the vision backbone, the DINov2-based MLLM fine-tuned with 5M data surpasses the MLLM trained with a CLIP model on 0.7M data. Additionally, the gap between DINov2 and the CLIP models is reduced under the 5M setting.

[leftmargin=*]
**Finding 6:** Language supervision offers strong advantages, but the performance gap can be narrowed with SSL methods given enough data and proper tuning.

### Combining Multiple Vision Encoders

As observed in Fig. 5, different vision encoders excel in different aspects of MLLM performance. In this study, we explore the potential of combining multiple vision encoders to leverage their distinctive representations, aiming to build a more capable MLLM. Given that different vision encoders use varying architectures and image resolutions, we interpolate to a fixed number of visual tokens (576) in this subsection (see details in Appendix F.3). We then concatenate these tokens along the feature dimension, following a method similar to A-MoF proposed in .

Our study indicates that adding a non-language-supervised model (DINov2) can improve benchmark performance, especially in vision-centric tasks. Notably, even OCR benchmarks benefit from incorporating DINov2. This highlights the importance of self-supervised learning models in complementing language-supervised models to achieve robust multimodal understanding. Detailed results and configurations are available in Appendix F.3.

However, this naive strategy has two limitations: 1) it employs interpolation, which can lead to information loss, especially with vision encoders with high-resolution feature maps, and 2) it treats each model equally via simple concatenation. Therefore, we seek a more effective strategy that can more flexibly leverage model combinations with less information loss.

[leftmargin=*]
**Finding 7:** Combining multiple vision encoders, including SSL models, can enhance MLLM performance across various benchmarks, particularly in vision-centric tasks.

## 3 Spatial Vision Aggregator (SVA): A New Connector Design

To effectively aggregate features from multiple vision encoders and prevent the information loss introduced by interpolation, we use a set of learnable latent queries that interact with multiple vision features via cross-attention layers . In particular, our approach incorporates two new vision-centric design principles:

1. We introduce spatial inductive bias by explicitly defining the aggregation space for each token in the query.
2. We aggregate vision features multiple times across the LLM layers, enabling the model to repeatedly access and integrate necessary visual information.

To facilitate information aggregation via cross-attention, we create a \(C\)-dimension learnable latent token \(^{C}\) that is repeated \(L L\) times to form a 2D grid, serving as the query \(^{L^{2} C}\). The set of visual features \(\) from \(N\) vision encoders serve as the context (i.e., key and value). We ensure the output resolution of every vision encoder is a multiple of \(L\). Formally, the feature map of the \(k\)-th vision encoder (\(_{k}\)) has a resolution of \(m_{k}L m_{k}L C\), where \(m_{k}\) is a positive integer multiplier, and \(L\) is the height/width of the learnable 2D grid with hidden dimension \(C\).

**Spatial inductive bias** To maintain the spatial structure during cross-attention, we align each token in the query with a specific sub-region of the feature maps in all vision encoders. Formally, a token at row \(i\) and column \(j\) in the query \(_{i,j}\) corresponds to the sub-region

\[_{k}[m_{k} i:m_{k}(i+1),m_{k} j:m_{k}(j+1)] ^{m_{k}^{2} C}\]

of the \(k\)-th vision feature map. As a result, a token \(_{i,j}\) aggregates a total of \(_{k}m_{k}^{2}\) features from \(N\) vision encoders through cross-attention (see Fig. 7-left).

Specifically, the updated query vector \(^{*}_{}^{1 C}\) at position \((i,j)\) is computed as \[^{*}_{,j}=(_{i,j}[_{ i,j,1},_{i,j,2},,_{i,j,N}]^{}}{} )[_{i,j,1},_{i,j,2},,_{i,j,N} ],\] (1)

where

\[_{i,j} =^{Q}_{i,j}^{1 C},\] \[_{i,j,k} =_{k}^{K}_{k}[m_{k} i:m_{k}(i+1),\ m _{k} j:m_{k}(j+1)]^{m_{k}^{2} C},\] \[_{i,j,k} =_{k}^{V}_{k}[m_{k} i:m_{k}(i+1),\ m _{k} j:m_{k}(j+1)]^{m_{k}^{2} C}.\]

Here, \(_{i,j}\) is the query vector at position \((i,j)\), calculated using the query projection matrix \(^{Q}^{C C}\). The key vectors \(_{i,j,k}\) and value vectors \(_{i,j,k}\) are computed for each vision encoder \(k\) using their respective key and value projection matrices \(_{k}^{K}^{C C}\) and \(_{k}^{V}^{C C}\). Since \(_{k}m_{k}^{2}\) features are aggregated into a single token, we effectively reduce the number of tokens.

Multi-layer vision aggregationAlthough our proposal effectively aggregates features from multiple vision encoders, there is still potential information loss with high-resolution input (large \(m_{k}\)) or multiple vision encoders (large \(N\)). Here, a single token would have to handle a larger amount of context information during aggregation. To prevent this, we allow cross-attention to occur multiple times by inserting our proposal throughout the LLM layers--allowing consistent access to the uncompressed visual information (see Fig. 7-right).

HyperparametersTo flexibly modulate capacity, we introduce two hyperparameters \(D\) and \(G\), which indicate the number of cross-attention layers and distinct groups of learnable queries used between the vision models and the LLM, respectively. \(D\) and \(G\) are always set to 1 for cross-attention layers within LLM layers. We provide ablation studies on the selection of \(D\) and \(G\) in Appendix H.

We demonstrate the efficacy of SVA module using the best vision model combination results from the previous section and a Vicuna-1.5-7B base LLM. Specifically, we employ a combination of four vision encoders: OpenAI CLIP ViT-L/14@336, SigLIP ViT-SO400M/14@384, OpenCLIP ConvNeXt-XXL@1024, and DINOcv2 ViT-L/14@518. We compare our method with two strong baselines: 1) concatenation-based  and 2) Re-sampler . Here, we include two variants of our SVA module. The standard one, "**SVA**", uses \(D=3\), \(G=1\), and inserts cross-attention blocks inside the LLM with a layer stride of 3. To isolate the advantages of spatial inductive biases, we include another SVA variant, "SVA-no-multi-agg", that does not add cross-attention blocks inside the LLM and sets \(D=3\) and \(G=3\). Table 1 shows that SVA outperforms both baselines, with a significant improvement in the OCR & chart category. In contrast, the Resampler--which lacks spatial inductive biases--struggles to condense concatenated tokens from various vision towers into

 Connector & General & Knowledge & OCR \& Chart & Vision-Centric \\  Concat.  & 67.2 & 48.9 & 50.1 & 52.6 \\ Resampler  & 63.1 & 46.5 & 27.1 & 42.6 \\ SVA-no-multi-agg & 68.0 & 49.5 & 55.2 & 52.6 \\
**SVA** & **68.5** & **49.7** & **55.5** & **53.2** \\ 

Table 1: **Comparison between our SVA and other aggregation approaches. The SVA module consistently outperforms other baselines and excels in aggregating high-resolution vision information.**

Figure 7: **Spatial Vision Aggregator (SVA). We propose SVA, a dynamic and spatially-aware connector that integrates multiple vision features with LLMs while reducing the number of tokens.**

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]