# Spatio-Spectral Graph Neural Networks

Simon Geisler\({}^{}\), Arthur Kosmala\({}^{}\), Daniel Herbst, and Stephan Gunnemann

Department of Computer Science & Munich Data Science Institute

Technical University of Munich

{s.geisler, a.kosmala, d.herbst, s.guennemann}@tum.de

###### Abstract

Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of \(\)-step MPGNNs are that their "receptive field" is typically limited to the \(\)-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose _Spatio-Spectral Graph Neural Networks (S\({}^{2}\)GNNs)_ - a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S\({}^{2}\)GNNs's 'nquish' over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S\({}^{2}\)GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Leman (WL) test. Moreover, to obtain general-purpose S\({}^{2}\)GNNs, we propose spectrally parametrized filters for directed graphs. S\({}^{2}\)GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S\({}^{2}\)GNNs scale to millions of nodes.

## 1 Introduction

_Spatial_ Message-Passing Graph Neural Networks (MPGNNs) ushered in various recent breakthroughs. For example, MPGNNs are able to predict the weather with unprecedented precision (Lam et al., 2023), can be composed as a foundation model for a rich set of tasks on knowledge graphs (Galkin et al., 2023), and are a key component in the discovery of millions of AI-generated crystal structures (Merchant et al., 2023). Despite this success, MPGNNs produce node-level signals solely considering _limited-size neighborhoods_, effectively bounding their expressivity. Even with a large number of message-passing steps, MPGNNs are limited in their capability of propagating information to distant nodes due to _over-squashing_. As evident by the success of global models like transformers (Vaswani et al., 2017), modeling long-range interactions can be pivotal and an important step towards foundation models that understand graphs.

**We propose _Spatio-Spectral Graph Neural Networks (S\({}^{2}\)GNNs)_**,** a new modeling paradigm for tackling the aforementioned limitations, that synergistically combine _message passing_ with _spectral filters_, explicitly parametrized in the spectral domain. _Spectral filters_ are virtually ignored by prior work but go beyond stacks of message-passing layers or polynomial parametrizations. Due to _message passing's_ finite number

Figure 1: S\({}^{2}\)GNN principle.

of propagation steps, it comes with a distance cutoff \(p_{}\) (# hops, see Fig. 1). Conversely, _spectral filters_ act globally (\(p_{}\)), even on a truncated frequency spectrum \(_{}\). Truncating the frequency spectrum for _spectral filters_ is required for efficiency, yet _message passing_ has access to the entire spectrum (right plots in Fig. 1). The combination of _message passing and spectral filters_ provably leverages the strengths of each parametrization. Utilizing this combination, S\({}^{2}\)GNNs generalize the concept of "virtual nodes" and distill many important properties of hierarchical message-passing schemes, graph-rewritings, and pooling into a single GNN (see Fig. 3). Outside of GNNs, a similar composition is at the core of some State Space Models (SSM) models (Poli et al., 2023), that deliver transformer-like properties with superior scalability on sequences - as do S\({}^{2}\)GNNs on graphs.

**Our analysis of S\({}^{2}\)GNNs** (SS 3.1) validates their capability for modeling long-range interactions. We prove in SS 3.1.1 that combining spectral and spatial filters alleviates the over-squashing phenomenon (Alon and Yahav, 2020; Di Giovanni et al., 2023a, b), a necessity for effective information-exchange among distant nodes. Our approximation-theoretic analysis goes one step further and proves strictly tighter error bounds in terms of approximation of the target idealized GNN (SS 3.1.2).

**Design space of S\({}^{2}\)GNNs** (SS 3.2).

**Superpersitic composition of**

**spectral filters & spatial message passing**

**Vanquiesies over-squashing** (SS 3.1.1)

**Supervisor approximation capabilities** (SS 3.1.2)

**Spatial message passing** (see literature)

**Spectral filter parametrization** (SS 3.2.1)

**Spectral-domain MLP** (SS 3.2.2)

**Spectral filter for directed graphs** (SS 3.2.3)

**Dual use of partial _Electrocomposition_ (EVD):

_free-of-cost_ **Positional Encodings (PE) (& 3.2.1).**

We propose the first permutation-equivariance-preserving _neural network in the spectral domain_ (SS 3.2.2) and generalize _spectral filters to directed graphs_ (SS 3.2.3). The dual use of the partial eigendecomposition, required for spectral filters, allows us to propose "free-of-cost" _positional encodings_ (SS 3.2.4), that are permutation-equivariant, stable, and increase expressivity strictly beyond the 1-Weisfeiler-Leman (WL) test.

**S\({}^{2}\)GNNs are effective and practical.** We empirically verify the shortcomings of MPGNNs and how S\({}^{2}\)GNNs overcome them (SS 4). E.g., we set a new state-of-the-art on peptides-func (Dwivedi et al., 2022) with \(\) 35% fewer parameters, outperforming MPGNNs and graph transformers. Although sequences are just a subdomain of (directed) graphs, we also study how S\({}^{2}\)GNNs compare to specialized sequence models like transformers (Vaswani et al., 2017) or Hyena (Poli et al., 2023). We find that S\({}^{2}\)GNNs are highly competitive even though they operate on a much more general domain (un-/directed graphs). Last, the runtime and space complexity of S\({}^{2}\)GNNs is equivalent to MPGNNs and, with vanilla full-graph training, S\({}^{2}\)GNNs can handle millions of nodes with a 40 GB GPU.

## 2 Background

We study graphs \((,)\) with adjacency matrix \(\{0,1\}^{n n}\) (or \(_{ 0}^{n n}\) if weighted), node features \(^{n d}\) and edge count \(m\). \(\) is symmetric for undirected graphs and, thus, has eigendecomposition \(,=()\) with eigenvalues \(^{n}\) and eigenvectors \(^{n n}\): \(=^{}\) using \(=()\). Instead of \(\), we decompose the _Laplacian_\(-^{-1/2}^{-1/2}\), with diagonal degree matrix \(=(})\), since its ordered eigenvalues \(0=_{1}_{2}_{n} 2\) are similar to frequencies (e.g., low eigenvalues relate to low frequencies, see Fig. 4). Likewise, one could use, e.g., \(=-^{-1}\) or more general variants (Yang et al., 2023); however, we focus our explanations on the most common choice \(-^{-1/2}^{-1/2}\). We choose the matrix of eigenvectors \(^{n n}\) to be orthogonal \(^{}=\). We refer to \(\) as the Fourier basis of the graph, with Graph Fourier Transformation (GFT) \(}=^{}\) and its inverse \(=}\). To provide an overview, Table 5 lists the symbols used in this work.

**Spectral graph filters.** Many GNNs implement a graph convolution, where node signal \(^{n d}\) is convolved \(_{}\) for every \(d\) with a scalar filter \(^{n}\). The graph convolution (Hammond et al., 2011) is defined in the spectral domain as \(_{}([^{}][ {V}^{}])\), with element-wise product \(\) and broadcast of \(^{}\) to match shapes. Instead of spatial \(\), spectral graph filters parametrize \(:\) explicitly and yield \(^{}:=()^{n}\) as a function of the eigenvalues.

Figure 2: S\({}^{2}\)GNN framework with adjacency matrix \(\), node features \(\), and Laplacian \(\) (function of \(\)).

**Message Passing Graph Neural Networks (MPGNNs)** circumvent the \(\) via polynomial \(()_{u}=_{p=0}^{p}_{j}_{u}^{j}\) since \([_{j=0}^{p}_{j}()^{j}]^{ }=_{j=0}^{p}_{j}^{j}\). In practice, many MPGNNs use \(p=1\): \(^{(l)}=(_{0}+_{1})^{(l-1)}\) with \(^{(0)}=\), and stack \(1 l\) layers interleaved with node-wise transformations and activations \(\). We refer to Balcilar et al. (2021b) for similar interpretations of MPGNNs like GAT Velickovic et al. (2018) or GIN Xu et al. (2019).

## 3 Method

_S\({}^{2}\)GNNs_ symbiotically pair spatial \((^{(l-1)};)\) MPGNNs and \((^{(l-1)};,)\) filters, using a partial eigendecomposition. Even though the spectral filter operates on a truncated eigendecomposition (**spectrally bounded**), it is **spatially unbounded**. Conversely, spatial MPGNNs are **spatially bounded** yet **spectrally unbounded** (see Fig. 1).

A spectrally bounded filter is sensible for modeling global pair-wise interactions, considering its **message-passing interpretation** of Fig. 3. Conceptually, a spectral filter consists of three steps: 1 Gather: The multiplication of the node signal with the eigenvectors \(_{u}^{}\) (GFT) is a weighted and signed aggregation over all nodes; 2 Apply: the "Fourier coefficients" are weighted; and 3 Scatter broadcasts the signal \(_{u}}\) back to the nodes (inverse GFT). The first eigenvector (there for \(=-\)) acts like a "virtual node" Gilmer et al. (2017) (see also SS E). That is, it calculates the average embedding and then distributes this information, potentially interlayered with neural networks. Importantly, the other eigenvectors effectively allow messages to be passed within or between clusters. As we show for exemplary graphs in Fig. 4, low frequencies/eigenvalues capture coarse structures, while high(er) frequencies/eigenvalues capture details. For example, the second eigenvector in Fig. 4b contrasts the inner with the outer rectangle, while the third eigenspace models both symmetries up/down and left/right. In conclusion, S\({}^{2}\)GNNs augment spatial message-passing with a _graph-adaptive hierarchy_ (spectral filter). Thus, **S\({}^{2}\)GNNs distill many important properties of hierarchical message-passing schemes**Bodnar et al. (2021), **graph-rewirings**Di Giovanni et al. (2023a), **pooling**Lee et al. (2019) etc. See SS J.1 for more examples.

**S\({}^{2}\)GNN's composition.** We study (1) an _additive combination_ for its simpler approximation-theoretic interpretation (SS 3.1.2), or (2) an _arbitrary sequence of filters_ due to its flexibility. In both cases, residual connections may be desirable (see SS J.2).

\[^{(l)}=^{(l)}(^{(l-1)};,)+^{(l)}(^{(l-1)};)\] (1) \[^{()}=(h^{()} h^{(-1)} h^{ (1)})(^{(0)})h^{(j)}\{,\}\] (2)

**Spectral Filter.** The building block that turns a spatial MPGNN into an S\({}^{2}\)GNN is the spectral filter:

\[^{(l)}(^{(l-1)};,)=_{}^{(l)}()^{}f_{}^{(l)}( ^{(l-1)})\] (3)

with a point-wise transformation \(f_{}^{(l)}:^{n d^{(l-1)}}^{n d^{(l)}}\), a learnable spectral filter \(_{}^{(l)}()^{k d^{(l)}}\) parameterized element-wise as \(_{}^{(l)}()_{u,v}_{v}^{(l)}( _{u};_{v})\) (see SS 3.2.1), and truncated \(^{n k},^{k}\). Due to the combination of message passing with spectral filters, S\({}^{2}\)GNNs' hypothesis class goes beyond (finite-order) polynomials of the Laplacian \(\) (or stacks message passing layers), unlocking a larger class of filters. In Algo. 1, we provide pseudo code for S\({}^{2}\)GNNs (Eq. 1).

**Truncated spectrum.** We omit extra notation for the truncated eigendecompositon \((,k)\) since it is equivalent to define \((_{j})=0\) for \(j>k\). However, truncating after the \(k\)-th eigenvector requires care with the last eigenspace to maintain permutation equivariance. Due to the

Figure 4: Exemplary (lowest) eigenspaces.

Figure 3: **Message-passing interpretation of \((_{}()[^{}])\) (spectral filter): via the Fourier coefficients they may **exchange information globally** and allow **intra-cluster message passing**. Edge width/color denotes the magnitude/sign of \(\).

ambiguity of eigenvectors in the presence of repeated eigenvalues, we must ensure that we only include eigenspaces in their entirety. That is, we only include \(\{_{j}\,|\,j k_{j}_{k+1}\}\). Thus, \((^{(l-1)};(,k))\) is permutation equivariant nonetheless. We defer all proofs to SS H.

**Theorem 1**.: \((^{(l-1)};(,k))\) _of Eq. 3 is equivariant to all \(n n\) permutation matrices \(\): \((^{(l-1)};(^{},k))= (^{(l-1)};(,k))\)._

**Complementary high-resolution filters.** Our _Spectral filters are highly discriminative between the frequencies_ and, e.g., can readily access a single eigenspace. Yet, for efficiency, we limit the spectral filter to a specific frequency band. _Due to the combination with message passing, this choice of band does not decide on, say, low-pass behavior; it solely determines where to increase the spectral selectivity._ While \(^{2}\)GNNs with subsequent guarantees adapt to domain-specific choices for the spectral filter's frequency band, a sensible default is to focus on the low frequencies. The two main reasons for this are (see SS J.3 for an extensive list): (1) Low frequencies model the smoothest global signals w.r.t. the graph structure (see Fig. 3 & 4). (2) Under a relative perturbation model (perturbation budget proportional to degree), stability implies \(C\)-integral-Lipschitzness (\( C>0\): \(|^{/}| C\)), i.e., the filter can vary strongly around zero but must level out for large \(\) (see Gama et al. (2020)).

### Theoretical Analysis

We show that how \(^{2}\)GNNs alleviate oversquashing in SS 3.1.1. Next, SS 3.1.2 makes the approximation-theoretic advantages precise.

#### 3.1.1 \(^{2}\)GNNs Vanquish Over-Squashing

Alon & Yahav (2020) pointed out that MPGNNs must pass information through bottlenecks that connect different communities using fixed-size embedding vectors. Topping et al. (2022) and Di Giovanni et al. (2023a) formalize this via an \(L^{1}\)-norm Jacobian sensitivity analysis: \(\|_{v}^{()}/_{u}^{(0)}\|_{L^{1}}\) models the output's \(_{v}^{()}\) change if altering input \(_{u}^{(0)}\). MPGNNs' Jacobian sensitivity typically decays \(((-r))\) with node distance \(r\) if the number of walks between the two nodes is small. See SS F for results of Di Giovanni et al. (2023a).

\(^{2}\)GNNs are not prone to such an exponential sensitivity decay due to their global message scheme. We formalize this in Theorem 2, refer to Fig. 4 for intuition and Fig. 5 for empirical verification. All theoretical guarantees hold if a \(\) exists such that \(f_{}=\).

**Theorem 2**.: _An \(\)-layer \(^{2}\)GNN can be parametrized s.t. output \(_{v}^{()}\)has a uniformly lower-bounded Jacobian sensitivity on a connected graph: \(\|_{v}^{()}/_{u}^{(0)}\|_{L^{1}} C _{}d/m\) with rows \(}_{u}^{(0)}\), \(_{v}^{()}\) of \(^{(0)}\), \(^{()}\) for nodes \(u\), \(v\), a parameter-dependent \(C_{}\), network width \(d\) and edge count \(m\)._

In contrast to Di Giovanni et al. (2023a), we prove a lower bound for \(^{2}\)GNNs, guaranteeing a minimum "influence" for any \(u\) on \(v\). This is true since \(^{2}\)GNNs contain a virtual node as a special case with \(_{}^{(l)}()=_{\{0\}}\), with \(_{}\) denoting the indicator function of a set \(\) (see also SS E). However, we find that a virtual node is insufficient for some long-range tasks, including our long-range clustering (LR-CLUSTER) of Fig. 9(b). Hence, the exponential sensitivity decay of spatial MPGNNs only shows their inadequacy in long-range settings. Proving its absence is not sufficient to quantify long-range modeling capabilities, noting that the lower bound is not tight for \(^{2}\)GNNs on many graphs. We close this gap with our subsequent analysis rooted in polynomial approximation theory.

#### 3.1.2 Approximation Theory: Superior Error Bounds Despite Spectral Cutoff

To demonstrate how \(^{2}\)GNNs can express a more general hypothesis class than MPGNNs, we study how well an "idealized" GNN (IGNN) can be approximated. Each IGNN layer \(l\) can express convolution operators \(g^{(l)}\) of _any_ spectral form \(^{(l)}\): \(\). We approximate IGNNs with \(^{2}\)GNNs from Eq. 1, with a spectral filter as in Eq. 3 and a spatial part parametrized by a polynomial. While we assume here that the \(^{2}\)GNN spectral filter is bandlimited to and a universal approximator on the interval \([0,_{}]\), the findings generalize to, e.g., a high-pass interval. In the main body, we focus on the key insights for architectures without nonlinear activations. Wang & Zhang (2022) prove that even linear IGNNs can produce any one-dimensional output under certain regularity assumptions on the graph and input signal. Thus, we solely need to consider a single layer. In SS H.4, we cover the generic setting including nonlinearities, where multiple layers are helpful.

Figure 5: Spectral filters do not exhibit oversquashing on “Clique Path” graphs (Di Giovanni et al., 2023a).

**Locality relates to spectral smoothness.** The locality of the true/ideal filter \(g\) is related to the smoothness of its Fourier transform \(\). For instance, if \(g\) is a low-order polynomial of \(\), it is localized to a few-hop neighborhood, and \(\) is regularized to vary slowly (Fig. 5(a) w/o discontinuity). The other extreme is a discontinuous spectral filter \(\), such as the entirely non-local virtual node filter, \(=_{\{0\}}\) (discontinuity in Fig. 5(a), details in SS E). This viewpoint of spectral smoothness illuminates the limitations of finite-hop message passing from an angle that complements spatial analyses in the over-squashing picture. It informs a lower bound on the error, which shows that spatial message passing, i.e, order-\(p\) polynomial graph filters \(g_{_{p}}\) with \(p+1\) coefficients \(_{p}^{p+1}\), can converge exceedingly slowly - slower than any inverse root (!) of \(p\) - to a discontinuous ground truth in the Frobenius-induced operator norm:

**Theorem 3**.: _Let \(\) be a discontinuous spectral filter. For any approximating sequence \(g_{_{p}}_{p}\) of polynomial filters, an adversarial sequence \((_{p})_{p}\) of input graphs exists such that_

\[_{>0}_{0^{| _{p}| d}}_{p}}-g)_{_{p} }\|_{}}{\|\|_{}}=(p^{-})\]

**Superior S\({}^{2}\)GNN error bound.** A spatio-spectral convolution wins over a purely spatial filter when the sharpest irregularities of the ground truth \(\) are within reach of its expressive spectral part. The spatial part, which can "focus" on learning the remaining, smoother part outside of this window, now needs much fewer hops to give a faithful approximation. We illustrate this principle in Fig. 6 where we approximate an additive combination of an order-three polynomial filter with discontinuous low-pass. Only the S\({}^{2}\) filter is faithfully approximating this filter. Formally, we find:

**Theorem 4**.: _Assume \(_{[_{},2]}\) is \(r\)-times continuously differentiable on \([_{},2]\), and a bound \(K_{r}(,_{}) 0\) such that \(}{d^{2}}() K_{r}(, _{})\ [_{},2]\). An approximating S\({}^{2}\)GNN sequence with parameters \(^{*}_{p},^{*}_{p}_{p}\) exists such that, for arbitrary graph sequences \((_{p})_{p}\),_

\[_{0^{|_{p}| d}}^{*}_{p}}+g_{^{*}_{p}}-g)_{_{p}}\|_{ }}{\|\|_{}}=(K_{r}(,_ {})p^{-r})\]

_with a scaling constant that depends only on \(r\), not on \(\) or \((_{p})_{p}\)._

The above bound extends to purely spatial convolutions in terms of \(K_{r}(,0)\) if \(\) is \(r\)-times continuously differentiable on the full interval \(\). The S\({}^{2}\)GNN bound of Theorem 4 is then still strictly tighter if \(K_{r}(,_{})<K_{r}(,0)\). In particular, taking the limit \(K_{1}(,0)\) towards discontinuity makes the purely spatial upper bound arbitrarily loose, whereas a benign filter might still admit a small \(K_{1}(,_{})\) for some \(_{}>0\). Theorem 3 suggests that this is not an artifact of a loose upper bound but that there is an inherent difficulty in approximating unsmooth filters with polynomials.

We conclude the analysis by instantiating the bounds: assuming \(\) is \(C\)-integral-Lipschitz for stability reasons (see Gama et al. (2020) and the paragraph before SS 3.1.1) yields \(K_{1}(,_{})=C/_{}\), whereas for the electrostatics example \(_{}\) in SS G, we find upper bounds \(K_{r}(_{},_{})=}{{_{}^{r+1}}}\). In both cases, the pure spatial bound diverges as smoothness around \(0\) remains unconstrained.

### Design Space

As shown in Fig. 2, we identify three major, yet unexplored, directions in S\({}^{2}\)GNNs' design space. In SS 3.2.1, we discuss how we parametrize the spectral filter. In SS 3.2.2, we propose the first neural network for the spectral domain. That is, we allow transformations and non-linearities in the "Fourier" domain. In SS 3.2.3, we are the first to instantiate spectral filters for directed graphs. Additionally,

Figure 6: S\({}^{2}\) filter perfectly approximates true filter (a) with a discontinuity at \(=0\), while polynomial (“Spa.”) and spectral (“Spec.”) alone do not. (b) shows responses on a path graph.

due to the availability of the partial eigendecomposition, positional encodings may dual use them to improve expressivity at negligible cost. In SS 3.2.4, we propose the first permutation equivariant, stable and efficient positional encodings that provably admit an expressivity beyond 1-WL. SS 3.2.4 provides further details and considerations, like some remarks on batching (SS 3.7). For the (sub-) design space of spatial message passing (You et al., 2020), we refer to its rich literature.

#### 3.2.1 Parametrizing Spectral Filters

For spectral filter function \(_{}()\) of Eq. 3, we learn a channel-wise linear combination of translated Gaussian basis functions (see "Gaussian smearing" used by Schutt et al. (2017)), as depicted in Fig. 7. This choice (1) may represent any possible \(_{}()\) with sufficient resolution (assumption in SS 3.1.2); (2) avoids overfitting towards numerical inaccuracies of the eigenvalue calculation; (3) limits the discrimination of almost repeated eigenvalues and, in turn, should yield stability (similar to SS 3.2.4). Strategies to cope with a variable \(_{}\) and \(k\) (e.g., using attention similar to SpecFormer (Bo et al., 2023)) did usually not yield superior experimental results.

**Window.** We multiply the learned combinations of Gaussians by an envelope function (we choose a Tukey window) that decays smoothly to zero around cutoff \(_{}\). This counteracts the so-called "Gibbs phenomenon" (aka "ringing"): as visualized for a path graph/sequence of 100 nodes in Fig. 8, trying to approximate a spatially-discontinuous target signal using an ideal low-pass range of frequency components results in an overshooting oscillatory behavior near the spatial discontinuity. Dampening the frequencies near \(_{}\) by a smooth envelope/window function alleviates this behavior. We note that the learned filter may, in principle, overrule the windowing at the cost of a higher weight decay penalty. See Algo. 2 for \(_{}()\)'s algorithmic description.

**Depth-wise separable convolution**(Sifre, 2014; Howard et al., 2017): Applying different filters for each dimension is computationally convenient for spectral filters. While "full" convolutions are also possible, we find that such a construction is more prone to over-fitting. In practice, we even use parameter sharing and apply fewer filters than dimensions to counteract over-fitting. We argue that sharing filters among dimensions is similar to the heads in a transformer (Vaswani et al., 2017).

**Feature transformations \(f_{}^{(l)}\).** As sketched in Fig. 3 & 4, all nodes participate in the global data transfer. While this global message-passing scheme is _graph-adaptive_, it does not adjust to the inputs. For adaptivity, we typically consider non-linear feature transformations \(f_{}^{(l-1)}(^{(l-1)})\), like gating mechanism \(f_{}^{(l-1)}(^{(l-1)})=^{(l-1)}^{}(^ {(l-1)}_{G}^{(l)}+^{}))\) with element-wise multiplication \(\), SiLU function \(^{}\), learnable weight \(\), and bias \(\). A linear transformation \(f_{}^{(l)}(^{(l-1)})=^{(l-1)}^{(l)}\) is another interesting case since we may first apply the GFT and then the transformation: \((^{}^{(l-1)})^{(l)}\). Next, we extend this linear transformation to a neural network in the spectral domain by adding multiple transformations and nonlinearities.

#### 3.2.2 Neural Network for the Spectral Domain

Applying a neural network \(s_{}\) in the spectral domain is highly desirable due to its negligible computational cost if \(k n\). Moreover, \(s_{}\) allows the spectral filter to become data-dependent and may mix between channels. Data-dependent filtering is one of the properties that is hypothesized to make transformers powerful Fu et al. (2023). We propose the first neural network for the spectral domain of graph filters \(s_{}^{(l)}:^{k d^{(l)}}^{k d^{ (l)}}\) that is designed to preserve permutation equivariance.

\[^{(l)}=^{(l)}(^{(l-1)};,)=s_{}^{(l)}_{}^{(l)}() ^{}f_{}^{(l)}(^{(l-1)})\] (4)

We achieve permutation equivariance via sign equivariance \(s_{}()= s_{}()\,,\, \{-1,1\}^{k d^{(l)}}\), combined with a permutation equivariance \(s_{}()=s_{}()\,,\,_{k}\), where \(_{k}\) is the set of all \(k k\) permutation matrices. Specifically, we stack linear mappings \(_{s}^{d^{(l)} d^{(l)}}\) (_without bias_) with a gated nonlinearity \((})=}(^{} {W}_{a}+_{a}^{}])\) with sigmoid \(\), column-wise norm \(m_{j}=\|}_{:,j}\|\), and learnable \(_{a}^{d^{(l)} d^{(l)}}\) as well as \(_{a}^{d^{(l)}}\).

Figure 8: Ringing of ideal low pass filter on path graph.

#### 3.2.3 Directed Graphs

Directed graphs are an important topic that did not discuss so far. For S\({}^{2}\)GNNs to generalize the capabilities of non-local sequence models like transformers (Vaswani et al., 2017) or SSMs (Poli et al., 2023; Gu and Dao, 2023) it is vital to support direction, e.g., for distinguishing source/beginning and sink/end. However, all discussion before assumed the existence of the eigenvalue _decomposition_ of \(\). This was the case for symmetric \(\); however, for directed graphs, \(\) may be asymmetric.

To guarantee \(\) is diagonalizable with real eigenvalues, we use the Magnetic Laplacian (Forman, 1993; Shubin, 1994; De Verdiere, 2013) which is Hermitian and models direction in the complex domain: \(_{q}=-(_{s}^{-1/2}_{s}_{s}^{-1/2})[i2  q(-^{})]\) with symmetrized adjacency/degrees \(_{s}\)/\(_{s}\), potential \(q[0,2]\), element-wise exponential \(\), and imaginary unit \(i^{2}=-1\). While other parametrizations of a Hermitian matrix are also possible, with \(\{0,1\}^{n n}\) and appropriate choice of \(q\), \(_{q}:\{0,1\}^{n n}^{n n}\) is _injective_. In other words, every possible asymmetric \(\) maps to exactly one \(_{q}\) and, thus, this representation is lossless. Moreover, for sufficiently small potential \(q\), the order of eigenvalues is well-behaved (Furutani et al., 2020). In contrast to Koke and Cremers (2024), a Hermitian parametrization of spectral filters does not require a dedicated propagation for forward and backward information flow. For simplicity we choose \(q<}{{n_{}}}\) with maximal number of nodes \(n_{}\) (with binary \(\)). This choice ensures that the first eigenvector suffices to obtain, e.g., the topological sorts of a Directed Acyclic Graph (DAG). Due to the real eigenvalues of a Hermitian matrix, the presented content generalizes with minor adjustments. Most notably, we use a feature transformation \(f_{}^{(l)}:^{n d^{(l-)}}^{n d^{ (l)}}\) and map back into the real domain after the spectral convolution. We give more implementation details in SS J.6 and provide additional background on directed graphs in SS C.

#### 3.2.4 Efficient Yet Stable and Expressive Positional Encodings

The availability of the partial eigendecomposition allows for their _dual use_ for positional encodings at negligible cost. Motivated by this, we propose the first efficient (\((km)\)) and (fully) permutation equivariant spectral Positional Encodings \(\) that provably increase the expressivity strictly beyond the 1-Weisfeiler-Leman (1-WL) test (Xu et al., 2019; Morris et al., 2019). In contrast to the Laplacian encodings of Dwivedi and Bresson (2021), our \(\) do not require augmenting eigenvectors w.r.t. their sign and maintain permutation equivariance also in the presence of repeated eigenvalues. In comparison to Huang et al. (2024), our \(\) come with drastically lower computational cost and have no learnable parameters. Due to the absence of learnable parameters, we need to calculate our \(\) only once.

We construct our \(k\)-dimensional positional encodings \((,)^{n k}\) as

\[(,)=||_{j=1}^{k}[(_{j}() ^{})]\] (5)

with concatenation \(||\) and binary adjacency \(\{0,1\}^{n n}\). We use a Radial Basis Function (RBF) filter with normalization around each eigenvalue \(_{j}()=(-)(_{j}-)}}{{^{2}}})\) with small width \(_{>0}\). This parametrization is not only permutation equivariant but also stable according to the subsequent definition via the Holder continuity. Note that \(C\) depends on the eigengap between \(}{{(_{k+1}-_{k})}}\) at the frequency cutoff (for exact constant \(C\) see proof in SS H.5).

**Definition 1** (Stable \(\)).: _(Huang et al., 2024) A PE method \(:^{n k}^{k}^{n k}\) is called stable, if there exist constants \(c,C>0\), such that for any Laplacian \(\), \(^{}\), and \(_{*}=_{}\|-^{}^{} \|_{}\)_

\[\|(())-_{*}\,((^{}))\|_{} C \|-_{*}^{}_{*}^{}\|_{}^ {c}.\] (6)

**Theorem 5**.: _The Positional Encodings \(\) in Eq. 5 are stable according to Definition 1._

Next to their stability, our \(\) can discriminate certain degree-regular graphs (e.g., Fig. 9). Since degree-regular graphs cannot be distinguished by 1-WL, our \(\) makes the equipped GNN (as expressive as 1-WL) strictly more expressive than 1-WL. See SS I for continued expressivity analyses.

**Theorem 6**.: \(S^{2}\)_GNNs are strictly more expressive than 1-WL with the \(\) of Eq. 5._

Figure 9: \(\) discriminates the depicted degree-regular degree-regular graphs, except for (a) vs. (c).

## 4 Empirical Results

With state-of-the-art performance on the peptides-func task of the long-range benchmark (Dwivedi et al., 2022), plus strong results on further benchmarks, we demonstrate that _S\({}^{2}\)GCN, a GCN paired with spectral filters_, is highly capable of **modeling long-range interactions (SS 4.1)**. We assess S\({}^{2}\)GNNs' **long sequence performance (SS 4.2)** (mechanistic in-context learning) and show that S\({}^{2}\)GCN, a graph machine learning method, can achieve competitive results to state-of-the-art sequence models, including H3, Hyena, and transformers. We exemplify S\({}^{2}\)GNNs' practicality and competitiveness at scale on **large-scale benchmarks (SS 4.3)** like TPUGraphs (Phothilimthana et al., 2023), PCQM4Mv2 (Hu et al., 2021), and Open Graph Benchmark (OGB) Products (Hu et al., 2020). Further, in SS 8.7, we report state-of-the-art performance on the heterophilic arXiv-year (Lim et al., 2021) and,in SS 4.4, we study combinations of spatial and spectral filters beyond Eq. 1 & 2.

**Setup.** We pair different MPGNNs with spectral filters and name the composition S\({}^{2}\)<base>. For example, a S\({}^{2}\)GNN with GAT as base will be called S\({}^{2}\)GAT. We typically perform 3 to 10 random reruns and report the mean \(\) standard deviation. The experiments of SS 4.1 require <11 GB (e.g. Nvidia GTX 1080Ti); for the experiments in SS 4.2 & 4.3 we use a 40 GB A100. We usually optimize weights with AdamW (Loshchilov and Hutter, 2019) and cosine annealing scheduler (Loshchilov and Hutter, 2017). We use early stopping based on the validation loss/score. See SS 4.1 for more details and https://www.cs.cit.tum.de/daml/s2gnn for code as well as supplementary material.

### Long-Range Interactions

**Finding (I): S\({}^{2}\)GCN outperforms state-of-the-art graph transformers, MPGNNs, and graph rewirings** on the peptides-func long-range benchmarks (Dwivedi et al., 2022) by a substantial margin. Simultaneously, we remain approximately 35% below the 500k parameter threshold and. On peptides-struct we are only outperformed by NBA-GIN (Park et al., 2023). We extend the best configuration for a GCN of Tonshoff et al. (2023) (see GCN in Table 1), lower the number of message passing steps from six to three, and interleave spatial and spectral filters (Eq. 2) with \(_{}=0.7\).

**Dataset contribution: Clustering**, given a single seed node per cluster, measures the ability (1) to spread information within the cluster and (2) to discriminate between the clusters. We complement the semi-supervised task CLUSTER from Dwivedi et al. (2023) with **(our)** LR-CLUSTER dataset, a scaled-upon version with long-range interactions (1). We closely follow Dwivedi et al. (2023), but instead of using graphs sampled from Stochastic Block Models (SBMs), we sample coordinates from a Gaussian Mixture Model (GMM) and then connect nearby nodes. CLUSTER has 117 nodes on average, while ours has 896. LR-CLUSTER has an average diameter of \( 33\) and often contain hub nodes that cause over-squashing. For full details on the dataset construction, see SS 4.1.

**Dataset contribution: Distance regression** is a task with long-range interactions used in prior work (Geisler et al., 2023; Lim et al., 2023). Here, the regression targets are the shortest path distances to the only root node (in-degree 0). We generate random trees/DAGs with \(\)750 # of nodes on average (details are in SS 4.7). The target distances often exceed 30 hops. We evaluate on similarly sized graphs as in the training data, i.e., in-distribution (**ID**) samples, and out-of-distribution (**OOD**) samples that consist of slightly larger graphs. Details on the dataset construction are in SS 4.7.

**Finding (II): spatial MPGNNs are less effective as S\({}^{2}\)GNNs, for long-range interactions. This is evident for peptides Table 1, clustering Fig. 10, distance regression Fig. 11, and over-squashing Fig. 12. Specifically, if the task requires long-range interactions beyond the receptive field of MPGNNs, they return crude estimates. E.g., in Fig. 11, the MPGNN predicts (approx.) constantly 20 for all distances beyond its receptive field - roughly the mean in the training data. Moreover,

   & **Model** & **peptides-func (\(\))** & **peptides-struct (\(\))** \\  ^{2}\)GNN**} & TGT (Cohi et al., 2024) & \(0.679 0.0074\) & \(0.2458 0.0015\) \\  & MGT+WPE (Ngo et al., 2023) & \(0.8617 0.0064\) & \(0.2453 0.0025\) \\  & MGLMAP (He et al., 2023) & \(0.6921 0.0054\) & \(0.2475 0.0015\) \\  & Graph (VV He et al., 2023) & \(0.6842 0.0075\) & \(0.2449 0.0016\) \\  & GRIF (Mai et al., 2023) & \(0.6988 0.0082\) & \(0.2460 0.0012\) \\  & GPS+MSP+GIN (Jung et al., 2024) & \(0.7156 0.0078\) & \(0.2457 0.0013\) \\ 
**Rewiring: DRes-GCN** & & \(0.7150 0.0044\) & \(0.2536 0.0015\) \\  & **Static Speed Models:** Graph Manna & \(0.7071 0.0083\) & \(0.2473 0.0025\) \\  & (Behov & Hashemi, 2024) & \(0.7133 0.0011\) & \(0.2455 0.0013\) \\  & GRIF (Behov & Hashemi, 2024) & \(0.6186 0.0026\) & \(0.2453 0.0032\) \\ ^{2}\)GCN** (Jung et al., 2023) & \(0.6569 0.0117\) & \(0.2523 0.0013\) \\  & MNR-GIN (Park et al., 2023) & \(0.7071 0.0067\) & \(0.2424 0.0010\) \\   & GCN (Toshoff et al., 2023) & \(0.6860 0.0050\) & \(0.2460 0.0007\) \\   & \(S^{2}\)GCN (**Ours**) & \(0.7275 0.0066\) & \(0.2467 0.0019\) \\   & \(\) PE (**ours**) & \(\) & \(0.24417 0.0032\) \\  

Table 1: Long-range benchmark. Our S\({}^{2}\)GNN uses \( 35\%\) fewer parameters than the other models. AP is Peptides-func’s and MAE peptides-struct’s target metric. The best/second best is bold/unlored.

S\({}^{2}\)GNNs may converge faster (see Fig. 25 in SS M.6.2) and are more parameter-efficient, as we show on PCQM4Mv2 Hu et al. (2021) in SS M.9.

**Finding (III): virtual nodes are insufficient.** We frequently find that including more than a single eigenvector (\(k>1\)) yields substantial gains. We make this explicit in Fig. 11(a), where we append a single spectral layer and sweep over the number of eigenvectors \(k\). We complement these findings with an ablation for the frequency cutoff \(_{}\) on peptides-func in SS M.5.

**Finding (IV): our Positional Encodings \(\) consistently help**, when concatenated to the node features. While this finding is true throughout our evaluation, the differences are more pronounced in certain situations. For example, on LR-CLUSTER in Fig. 11, the \(\) help with spectral filter and a small \(k\) or without spectral filter and many message passing steps.

**Finding (V): spectral filters align with clusters**, as we illustrate in Fig. 14 for four arbitrary spectral filters learned on LR-CLUSTER. We observe that (a) the spectral filters reflect the true clustering structure, (b) some filters are smooth while others contain details, and (c) they model coarser or finer cluster structures (e.g., first vs. third filter).

### Sequence Modelling: Mechanistic In-Context Learning

Following the evaluation of Hyena (Poli et al., 2023) and H3 Fu et al. (2023), we benchmark S\({}^{2}\)GCN with sequence models on the _associative recall_ in-context learning task, stemming from mechanistic interpretability Elhage et al. (2021); Power et al. (2022); Zhang et al. (2023); Olsson et al. (2022). In associative recall, the model is asked to retrieve the value for a key given in a sequence. For example, in the sequence a,0,e,b,z,9,h,2,=>,z, the target is the value for key z, which is 9 since it follows z in its prior occurrences. We create a sequence/path graph with a node for each "token" (separated by "," in the example above) and label the target node with its value. We assess the performance of S\({}^{2}\)GCN on graphs that vary in size by almost two orders of magnitude and follow Poli et al. (2023) with a vocabulary of 30 tokens. Moreover, we finetune our S\({}^{2}\)GCN on up to 30k nodes.

**Finding (VI): our spectral filter for directed are effective** and may improve generalization, as we find in Fig. 14 (and Table 13 of SS M.7).

**Finding (VII): S\({}^{2}\)GCN a state-of-the-art sequence model**, as it performs on par with Hyena and, here, outperforms transformers (Table 2).

  
**Model** & **Accuracy (\(\))** \\  Transformer & _OOM_ \\ Vaswani et al. (2017) & _OOM_ \\ w/ FlashAttention & 0.324 \\ Dong et al. (2022) & 0.84 \\ Hyena & **1.000** \\ \(S^{2}\)GCN (ours)** & \(0.97 0.05\) \\   

Table 2: 30k token associative recall.

Figure 14: S\({}^{2}\)GCN solves associative recall for sequences varying in size by two orders of magnitude. Grey area marks **ID**.

Figure 13: 4 filters on LR-CLUSTER. Large/small entries are yellow/blue, white lines mark clusters.

### Large-Scale Benchmarks

**Finding (VIII): S\({}^{2}\)GNNs is practical and scalable.** We demonstrate this on the OGB Products graph (2.5 mio. nodes, Table 3) and the (directed) 10 million graphs dataset TPUGraphs (average number of nodes \(\)10,000, Table 4). In both cases, we find full-graph training (without segment training (Cao et al., 2023)) using 3 (Dir-) GCN layers interlayered with spectral filters, a reasonable configuration on a 40 GB A100. However, for OGB Products, we find that batching is superior, presumably because the training nodes are drawn from a "small" region of the graph (see SS K).

**The cost of partial \(\)** for each dataset (excluding TPUGraphs and distance regression) is between 1 to 30 minutes on CPUs. We report the detailed costs of \(\) and experiments in SS M.3.

## 5 Related Work

**Combining spatial and spectral filters** has recently attracted attention outside of the graph domain in models like Hyena (Poli et al., 2023), Spectral State Space Models (Agarwal et al., 2024), etc. with different flavors of parametrizing the global/FFT convolution. Nevertheless, the properties of spatial and spectral filter parametrization (e.g., local vs. global) are well-established in classical signal processing. A combination of spectral and spatial filters was applied to (periodic) molecular point clouds (Kosmala et al., 2023). For GNNs, Stachenfeld et al. (2020) compose a spatial and spectral message passing but do not handle the ambiguity of the eigendecomposition and, thus, do not maintain permutation equivariance. Moreover, Beaini et al. (2021) use the \(\) for localized anisotropic graph filters; Liao et al. (2019) propose an approach that combines spatial and spectral convolution via the Lanczos algorithm; and Huang et al. (2022) augment message passing with power iterations. Behrouz and Hashemi (2024) apply a Mambo-like state space model to graphs via arbitrarily ordering the nodes and, thus, sacrifice permutation equivariance.

**Long-range interactions on graphs.** Works that model long-range interactions can be categorized into: (a) MPGNNs on rewired graphs (Gasteiger et al., 2019, 2019; Gutteridge et al., 2023); (b) higher-order GNNs (Fey et al., 2020; Wollschlager et al., 2024) that, e.g., may pass information to distant nodes through hierarchical message passing schemes; and (c) message passing adaptations to facilitate long-range interactions. For example, Park et al. (2023) propose "non-backtracking" message passing, Errica et al. (2024) adaptively choose the numbers of message passing steps, and Ding et al. (2024) use linear RNNs to aggregate over each node's neighborhoods. While approaches (a-c) can increase the receptive field of GNNs, they are typically still spatially bounded. In contrast, (d) alternative architectures, like graph transformers (Ma et al., 2023; Dwivedi and Bresson, 2021; Kreuzer et al., 2021; Rampasek et al., 2022; Geisler et al., 2023; Deng et al., 2024) with global attention, may model all possible \(n n\) interactions. We provide notes on the limitations of graph transformers with absolute positional endodings in \(@sectionsign\) D, which highlights the importance of capturing the relative relationships between nodes, as S\({}^{2}\)GNNs do. Moreover, in a recent/contemporary non-attention model for all pair-wise interactions, Batatia et al. (2024) use a resolvent parametrization of matrix functions relying on the LDL factorization of a matrix, but do not characterize their approximation-theoretic properties, over-squashing, expressivity on graphs, nor how to deal with directed graphs.

In SS B, we discuss additional related work w.r.t. expressivity and directed graphs.

## 6 Discussion

We propose S\({}^{2}\)GNNs, adept at efficiently modeling complex long-range interactions via the synergistic composition of spatially and spectrally parametrized filters (SS 3). We show that S\({}^{2}\)GNNs share many properties with graph rewirings, pooling, and hierarchical message passing schemes (Fig. 3 & 4). S\({}^{2}\)GNNs outperform the aforementioned techniques with a substantial margin on the peptides long-range benchmark (SS 4.1), and we show that S\({}^{2}\)GNNs are also strong sequence models, performing on par or outperforming state-of-the-art like Hyena or H3 in our evaluation (SS 4.2). Even though we find global graph models, like S\({}^{2}\)GNNs, more prone to overfitting (see SS K/L for further limitations/impact), moving to global models aligns with the trend for other deep learning domains.

  
**Split** & **Model** & **Accuracy (\(\))** & **F1 (\(\))** \\   & GAT & **0.86\(\)**0.001 & 0.381\(\)**0.001 \\  & S\({}^{2}\)GAT & **0.902\(\)**0.000** & **0.472\(\)**0.006** \\   & GAT & 0.907\(\)**0.001 & 0.508\(\)**0.002** \\  & S\({}^{2}\)GAT & **0.913\(\)**0.002** & **0.582\(\)**0.014** \\   & GAT & 0.798\(\)**0.003 & 0.347\(\)**0.004** \\  & S\({}^{2}\)GAT & **0.811\(\)**0.007** & **0.381\(\)**0.009** \\   

Table 4: Graph ranking on TPUGraphs “layout”.

  
**Split** & **Model** & **Accuracy (\(\))** & **F1 (\(\))** \\   & GAT & **0.866\(\)**0.001 & 0.381\(\)**0.001 \\  & S\({}^{2}\)GAT & **0.902\(\)**0.000** & **0.472\(\)**0.006** \\   & GAT & 0.907\(\)**0.001 & 0.508\(\)**0.002** \\  & S\({}^{2}\)GAT & **0.913\(\)**0.002** & **0.582\(\)**0.014** \\   & GAT & 0.798\(\)**0.003 & 0.347\(\)**0.004** \\  & S\({}^{2}\)GAT & **0.811\(\)**0.007** & **0.381\(\)**0.009** \\   

Table 3: OGB Products.