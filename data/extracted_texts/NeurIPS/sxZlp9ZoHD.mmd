# Retentive Network

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost \(O(1)\) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference.

## 1 Introduction

Transformer  has become the de facto architecture for large language models, which was initially proposed to overcome the sequential training issue of recurrent models . However, training parallelism of Transformers is at the cost of inefficient inference, because of the \(O(N)\) complexity per step and memory-bound key-value cache , which renders Transformers unfriendly to deployment. The growing sequence length increases GPU memory consumption as well as latency and reduces inference speed. Numerous efforts have continued to develop the next-generation architecture, aiming at retaining training parallelism and competitive performance as Transformers while having efficient \(O(1)\) inference. It is challenging to achieve the above goals simultaneously.

There have been three main strands of research. First, linearized attention  approximates standard attention scores \(()\) with kernels \(()()\), so that autoregressive inference can be rewritten in a recurrent form. However, the modeling capability and performance are worse than Transformers, which hinders the method's popularity. The second strand returns to recurrent models for efficient inference while sacrificing training parallelism. As a remedy, element-wise operators  are used for acceleration, however, representation capacity and performance are harmed. The third line explores replacing attention with other mechanisms, such as S4 , and its variants . None of the previous work can achieve strong performance and efficient inference at the same time compared to Transformers.

In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long-sequence modeling, Transformer-comparable performance, and parallel model training simultaneously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention, which has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent representations. First, the parallel representation empowers training parallelism to utilize GPU devices fully. Second, the recurrent representation enables efficient \(O(1)\) inference in terms of memory and computation. The deployment cost and latency can be significantly reduced. Moreover, the implementation is greatly simplified without key-value cache tricks. Third, the chunkwise recurrentrepresentation can perform efficient long-sequence modeling. We parallelly encode each local block for computation speed while recurrently encoding the global blocks to save GPU memory.

We compare RetNet with Transformer and its variants. Experimental results on language modeling show that RetNet is consistently competitive in terms of both scaling curves and in-context learning. Moreover, the inference cost of RetNet is length-invariant. For a 7B model and 8k sequence length, RetNet decodes 8.4\(\) faster and saves 70% of memory than Transformers with key-value caches. During training, RetNet also achieves 3\(\) acceleration than standard Transformer with highly-optimized FlashAttention-2 . Besides, RetNet's inference latency is insensitive to batch size, allowing enormous throughput. The intriguing properties make RetNet a potential candidate to replace Transformer for large language models.

## 2 Retentive Network

Retentive network (RetNet) is stacked with \(L\) identical blocks, which follows a similar layout (i.e., residual connection, and pre-LayerNorm) as in Transformer . Each RetNet block contains two modules: a multi-scale retention (MSR) module, and a feed-forward network (FFN) module. We introduce the MSR module in the following sections. Given an input sequence \(x=x_{1} x_{|x|}\), RetNet encodes the sequence in an autoregressive way. The input vectors \(\{_{i}\}_{i=1}^{|x|}\) is first packed into \(X^{0}=[_{1},,_{|x|}]^{|x| d_{}}\), where \(d_{}\) is hidden dimension. Then we compute contextualized vector representations \(X^{l}=_{l}(X^{l-1}),l[1,L]\).

### Retention

In this section, we introduce the retention mechanism that has a dual form of recurrence and parallelism. So we can train the models in a parallel way while recurrently conducting inference.

Consider a sequence modeling problem that maps \(v(n) o(n)\) through states \(_{n}\). Let \(v_{n},o_{n}\) denote \(v(n),o(n)\) for simplicity. We formulate the mapping in a recurrent manner:

\[_{n}&=A_{n-1}+K_{n}^{ }v_{n}, A^{d d}, K_{n}^{1  d}\\ o_{n}&=Q_{n}_{n}=_{m=1}^{n}Q_{n}A^{n-m}K _{m}^{}v_{m}, Q_{n}^{1 d}\] (1)

where we map \(v_{n}\) to the state vector \(_{n}\), and then implement a linear transform to encode sequence information recurrently. Next, we make the projection \(Q_{n},K_{n}\) content-aware:

\[Q=XW_{Q}, K=XW_{K}\] (2)

where \(W_{Q},W_{K}^{d d}\) are learnable matrices.

We diagonalize the matrix \(A=( e^{i})^{-1}\), where \(,^{d}\). Then we obtain \(A^{n-m}=( e^{i})^{n-m}^{-1}\). By absorbing \(\) into \(W_{Q}\) and \(W_{K}\), we can rewrite Equation (1) as:

\[ o_{n}&=_{m=1}^{n}Q_{n}( e^{ i})^{n-m}K_{m}^{}v_{m}\\ &=_{m=1}^{n}(Q_{n}( e^{i})^{n})(K_{m}( e^{ i})^{-m})^{}v_{m}\] (3)

where \(Q_{n}( e^{i})^{n},K_{m}( e^{i})^{-m}\) is known as xPos , i.e., a relative position embedding proposed for Transformer. We further simplify \(\) as a scalar, Equation (3) becomes:

\[o_{n}=_{m=1}^{n}^{n-m}(Q_{n}e^{in})(K_{m}e^{im})^{ }v_{m}\] (4)

where \({}^{}\) is the conjugate transpose. The formulation is easily parallelizable within training instances.

In summary, we start with recurrent modeling as shown in Equation (1), and then derive its parallel formulation in Equation (4). We consider the original mapping \(v(n) o(n)\) as vectors and obtain the retention mechanism as follows.

The Parallel Representation of RetentionAs shown in Figure 0(a), the retention layer is defined as:

\[ Q=(XW_{Q}),& K=(XW_{K}) , V=XW_{V}\\ _{n}=e^{in},& D_{nm}=^ {n-m},&n m\\ 0,&n<m\\ (X)=(QK^{} D)V\] (5)

where \(D^{|x||x|}\) combines causal masking and exponential decay along relative distance as one matrix, and \(\) is the complex conjugate of \(\). In practice, we map \(Q,K^{d}^{d/2}\), add the complex position embedding \(\), then map them back to \(^{d}\), following the implementation trick as in LLaMA . Similar to self-attention, the parallel representation enables us to train the models with GPUs efficiently.

The Recurrent Representation of RetentionAs shown in Figure 0(b), the proposed mechanism can also be written as recurrent neural networks (RNNs), which is favorable for inference. For the \(n\)-th timestep, we recurrently obtain the output as:

\[ S_{n}= S_{n-1}+K_{n}^{}V_{n}\\ (X_{n})=Q_{n}S_{n}, n=1,,|x|\] (6)

where \(Q,K,V,\) are the same as in Equation (5).

The Chunkwise Recurrent Representation of RetentionA hybrid form of parallel representation and recurrent representation is available to accelerate training, especially for long sequences. We divide the input sequences into chunks. Within each chunk, we follow the parallel representation (Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following the recurrent representation (Equation (6)). Specifically, let \(B\) denote the chunk length. We compute the retention output of the \(i\)-th chunk via:

\[ Q_{[i]}=Q_{Bi:B(i+1)},& K_{[i]}=K_{Bi:B (i+1)}, V_{[i]}=V_{Bi:B(i+1)}\\ R_{i}&=K_{[i]}^{}(V_{[i]})+^ {B}R_{i-1},_{ij}=^{B-i-1}\\ (X_{[i]})&=K_ {[i]}^{} D)V_{[i]}}_{}+R_{ i-1})}_{},_{ij}=^{i+1}\] (7)

where \([i]\) indicates the \(i\)-th chunk, i.e., \(x_{[i]}=[x_{(i-1)B+1},,x_{iB}]\). The proof of the equivalence between recurrent representation and chunkwise recurrent representation is described in Appendix B.

### Gated Multi-Scale Retention

We use \(h=}}}{{d}}\) retention heads in each layer, where \(d\) is the head dimension. The heads use different parameter matrices \(W_{Q},W_{K},W_{V}^{d d}\). Moreover, **m**ulti-**s**cale **r**etention (MSR) assigns

Figure 1: RetNet has three equivalent computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent representations. Given the same input, three paradigms obtain the same output. “GN” is short for GroupNorm.

different \(\) for each head. For simplicity, we set \(\) identical among different layers and keep them fixed. In addition, we add a swish gate [23; 40] to increase the non-linearity of retention layers. Formally, given input \(X\), we define the layer as:

\[ =1-2^{-5-(0,h)}^{h}\] (8) \[_{i} =(X,_{i})\] \[Y =_{h}((_{1},, _{h}))\] \[(X) =((XW_{G}) Y)W_{O}\]

where \(W_{G},W_{O}^{d_{} d_{}}\) are learnable parameters, and \(\) normalizes the output of each head, following SubLN proposed in . Notice that the heads use multiple \(\) scales, which results in different variance statistics. So we normalize the head outputs separately.

The pseudocode of retention is summarized in Figure 2.

**Retention Score Normalization** We utilize the scale-invariant nature of \(\) to improve the numerical precision of retention layers. Specifically, multiplying a scalar value within \(\) does not affect outputs and backward gradients, i.e., \((*_{i})=(_{i})\). We implement three normalization factors in Equation (5). First, we normalize \(QK^{}/\). Second, we replace \(D\) with \(_{nm}=}{{^{n}D_{ni}}}}\). Third, let \(R\) denote the retention scores \(R=QK^{} D\), we normalize it as \(_{nm}=}}{{(_{i=1}^{n}|R_{ni}|,1)}}\). Then the retention output becomes \((X)=V\). The above tricks do not affect the final results while stabilizing the numerical flow of both forward and backward passes, because of the scale-invariant property.

### Overall Architecture of Retention Networks

For an \(L\)-layer retention network, we stack multi-scale retention (MSR) and feed-forward network (FFN) to build the model. Formally, the input sequence \(\{x_{i}\}_{i=1}^{|x|}\) is transformed into vectors by a word embedding layer. We use the packed embeddings \(X^{0}=[_{1},,_{|x|}]^{|x| d_{}}\) as the input and compute the model output \(X^{L}\):

\[Y^{l} =((X^{l}))+X^{l}\] (9) \[X^{l+1} =((Y^{l}))+Y^{l}\]

where \(()\) is LayerNorm . The FFN part is computed as \((X)=(XW_{1})W_{2}\), where \(W_{1},W_{2}\) are parameter matrices.

Figure 2: Pseudocode for the three computation paradigms of retention. Parallel implementation enables training parallelism to fully utilize GPUs. Recurrent paradigm enables low-cost inference. Chunkwise retention combines the above advantages (i.e., parallel within each chunk and recurrent across chunks), which has linear memory complexity for long sequences.

TrainingWe use the parallel (Equation (5)) and chunkwise recurrent (Equation (7)) representations during the training process. The parallelization within sequences or chunks efficiently utilizes GPUs to accelerate computation. More favorably, chunkwise recurrence is especially useful for long-sequence training, which is efficient in terms of both FLOPs and memory consumption.

InferenceThe recurrent representation (Equation (6)) is employed during inference, which nicely fits autoregressive decoding. The \(O(1)\) complexity reduces memory and inference latency while achieving equivalent results.

## 3 Experiments

We perform language modeling experiments to evaluate RetNet. First, we present the scaling curves of Transformer and RetNet. Second, we follow the training settings of StableLM-4E1T  to compare with open-source Transformer models in downstream benchmarks. Moreover, for training and inference, we compare speed, memory consumption, and latency. The training corpus is a curated compilation of The Pile , C4 , and The Stack .

### Comparison with Transformer Variants

We compare RetNet with various efficient Transformer variants, including RWKV , H3 , Hyena , and Mamba . We use LLaMA  architecture, including RMSNorm  and SwiGLU  module, as the Transformer backbone, which shows better performance and stability. Consequently, other variants follow these settings. Specifically, Mamba does not have FFN layers so we only implement RMSNorm. For RetNet, the FFN intermediate dimension is \(d\) and the value dimensions in \(W_{G},W_{V},W_{O}\) are also \(d\), where the overall parameters are still \(12d^{2}\). All models have 400M parameters with 24 layers and a hidden dimension of 1024. For H3, we set the head dimension to 8. For RWKV, we use the TimeMix module to substitute self-attention layers while keeping FFN layers consistent with other models for fair comparisons. We train the models with 40k steps with a batch size of 0.25M tokens.

Fine-Grained Language Modeling EvaluationAs shown in Table 1, we first report the language modeling perplexity of validation sets. Besides the overall validation set, following , we divide perplexity into "AR-Hit" and "First Occur". Specifically, AR-Hit contains the predicted tokens that are previously seen bigrams in the previous context, which evaluates the associative recall ability. "First Occur" has the predicted tokens that can not be recalled from the context. Among various Transformer variants, RetNet outperforms previous methods on both "AR-Hit" and "First Occur" splits, which is important for real-world use cases.

Knowledge-Intensive TasksWe also evaluate Massive Multitask Language Understanding (MMLU; ) answer perplexity to evaluate models on knowledge-intensive tasks. We report the average perplexity of the correct answers, i.e., given input [Question, "Answer:", Correct Answer], we calculate the perplexity of the "Correct Answer" part. RetNet achieves competitive results among the architectures.

    &  &  \\  & **Valid. Set** & **AR-Hit** & **First-Occur** & **STEMs** & **Humanites** & **Social-Sci.** & **Others** & **Avg** \\  Transformer  & 3.320 & 1.118 & 3.826 & 0.584 & 0.229 & 0.279 & 0.402 & 0.356 \\   \\ Hyena  & 3.545 & 1.799 & 3.947 & 1.125 & 0.576 & 0.654 & 0.819 & 0.767 \\ RWKV  & 3.497 & 1.706 & 3.910 & 1.156 & 0.609 & 0.617 & 0.781 & 0.768 \\ Mamba  & 3.379 & 1.322 & 3.852 & 0.668 & 0.288 & 0.300 & 0.425 & 0.403 \\ H3  & 3.563 & 1.722 & 3.986 & 1.169 & 0.532 & 0.637 & 0.792 & 0.752 \\ RetNet & **3.360** & **1.264** & **3.843** & **0.577** & **0.263** & **0.280** & **0.384** & **0.362** \\   

Table 1: Perplexity results on language modeling and MMLU  answers. We use the augmented Transformer architecture proposed in LLaMA  for reference. For language modeling, we report perplexity on both the overall validation set and fine-grained diagnosis sets , i.e., “AR-Hit” evaluates the associative recall capability, and “First-Occur” indicates the regular language modeling performance. Besides, we evaluate the answer perplexity of MMLU subsets.

### Language Modeling Evaluation with Various Model Sizes

We train language models with various sizes (i.e., 1.3B, 2.7B, and 6.7B) from scratch. The training batch size is 4M tokens with 2048 maximal length. We train the models with 25k steps. The detailed hyper-parameters are described in Appendix E. We train the models with 512 AMD MI200 GPUs.

Figure 3 reports perplexity on the validation set for the language models based on Transformer and RetNet. We present the scaling curves with three model sizes, i.e., 1.3B, 2.7B, and 6.7B. RetNet achieves comparable results with Transformers. More importantly, the results indicate that RetNet is favorable in terms of size scaling. In addition to performance, RetNet training is quite stable in our experiments. Experimental results show that RetNet is a strong competitor to Transformer for large language models. Empirically, we find that RetNet starts to outperform Transformer when the model size is larger than 2B.

### Long-Context Evaluation

We evaluate long-context modeling on the ZeroSCROLLS  benchmark. We train a hybrid model of size 2.7B, RetNet+, which stacks the attention and retention layers. Specifically, we insert one attention layer after every 3 retention layers. We follow most configurations of the 2.7B model as in Section 3.2. We scale the number of training tokens to 420B tokens. The batch size is 4M tokens. We first train the model with 4K length and then extend the sequence length to 16K for the last 50B training tokens. The rotation base scaling  is used for length extension.

Figure 4 reports the answer perplexity given various lengths of input document. It shows that both Transformer and RetNet+ perform better with longer input documents. The results indicate that the language models successfully utilize the long-distance context. Notice that the 12K and 16K results in Qasper are similar because the lengths of most documents are shorter than 16K. Moreover, RetNet+ obtains competitive results compared with Transformer for long-context modeling. Meanwhile, retention has better training and inference efficiency.

### Inference Cost

As shown in Figure 5, we compare memory cost, throughput, and latency of Transformer and RetNet during inference. Transformers reuse KV caches of previously decoded tokens. RetNet uses the recurrent representation as described in Equation (6). We evaluate the 6.7B model on the A100-80GB GPU. Figure 5 shows that RetNet outperforms Transformer in terms of inference cost.

MemoryAs shown in Figure 4(a), the memory cost of Transformer increases linearly due to KV caches. In contrast, the memory consumption of RetNet remains consistent even for long sequences,

Figure 4: Answer perplexity decreases along with longer input documents. Transformer and RetNet+ obtain comparable performance for long-context modeling on the ZeroSCROLLS  benchmark.

Figure 3: Validation perplexity (PPL) decreases along with scaling up the model size.

requiring much less GPU memory to host RetNet. The additional memory consumption of RetNet is almost negligible (i.e., about 3%) while the model weights occupy 97%.

ThroughputAs presented in Figure 4(b), the throughput of Transformer drops along with the decoding length increases. In comparison, RetNet has higher and length-invariant throughput during decoding, by utilizing the recurrent representation of retention.

LatencyLatency is an important metric in deployment that greatly affects the user experience. We report the decoding latency in Figure 4(c). Experimental results show that increasing batch size renders the Transformer's latency larger. Moreover, the latency of Transformers grows faster with longer input. In order to make latency acceptable, we have to restrict the batch size, which harms the overall inference throughput of Transformers. By contrast, RetNet's decoding latency outperforms Transformers and stays almost the same across different batch sizes and input lengths.

### Training Throughput

Figure 6 compares the training throughput of Transformer and RetNet, where the training sequence lengths range from 8192 to 65536. The model size is 3.5B, where the hidden dimension is 3072 and the layer size is 28. We use highly optimized FlashAttention-2  for Transformers. In comparison, we implement chunk recurrent representation (Equation (7)) using Triton , where the computation is both memory-friendly and computationally efficient. The chunk size is set to \(256\). We evaluate the results with eight Nvidia H100-80GB GPUs because FlashAttention-2 is highly optimized for H100 cards.

Experimental results show that RetNet has higher training throughput than Transformers. The acceleration ratio increases as the sequence length is longer. When the training length is 64k, RetNet's throughput is about 3 times than Transformer's.

### Zero-Shot and Few-Shot Evaluation on Downstream Tasks

We also compare the language models on a wide range of downstream tasks. We evaluate zero-shot and 4-shot learning with the 6.7B models. As shown in Table 2, the datasets include HellaSwag (HS; ), BoolQ , COPA , PIQA , Winograd, Winogrande , and StoryCloze (SC; ). The accuracy numbers are consistent with language modeling perplexity presented in Figure 3. RetNet achieves comparable performance with Transformer on zero-shot and in-context learning settings.

### Ablation Studies

We ablate various design choices of RetNet and report the language modeling results in Table 3. The evaluation settings and metrics are the same as in Section 3.1.

Figure 5: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperforms Transformers in terms of memory consumption, throughput, and latency.

Figure 6: Training throughput (word per second; wps) of Transformer with FlashAttention-2  and RetNet.

[MISSING_PAGE_FAIL:8]

we use AdamW  for 300 epochs, and 20 epochs of linear warm-up. The learning rate is \(1 10^{-3}\), the batch size is 1024, and the weight decay is \(0.05\). For COCO object detection, we use Mask R-CNN  as the task head, and the above models pre-trained on ImageNet as the backbone with 3x schedules. In ADE20K experiments, we use UperNet  as the segmentation head. The detailed configuration can be found in Appendix H.

Table 4 shows the results across various vision tasks. RetNet is competitive compared with DeiT. For classification and segmentation, RetNet is slightly better than DeiT, where RetNet achieves 0.81% accuracy improvement on ImageNet and 0.61% mIoU improvement on ADE20K. For object detection, the results are comparable.

## 4 Related Work

Numerous efforts are focused on reducing the quadratic complexity of attention mechanisms. Linear attention  uses various kernels \()(k_{j})}}{{_{n=1}^{|x|}{(q_{i})(k_{n})}}}\) to replace the \(\) function. In contrast, we reexamine sequence modeling from scratch, rather than aiming at approximating \(\). AFT  simplifies dot-product attention to element-wise and moves \(\) to key vectors. RWKV  replaces AFT's position embeddings with exponential decay and runs the models recurrently for training and inference. In comparison, retention preserves high-dimensional states to encode sequence information, which contributes to expressive ability and better performance. S4  unifies convolution and recurrence format and achieves \(O(N N)\) training complexity leveraging the FFT kernel. Unlike Equation (2), if \(Q_{n}\) and \(K_{n}\) are content-unaware, the formulation can be degenerated to S4 . Hyena  generates the convolution kernels, achieving sub-quadratic training efficiency but keeping \(O(N)\) complexity in single-step inference. Recently, most related work has focused on modifying \(\) in Equation (6) as a data-dependent variable, such as Mamba , GLA , Gateloop , and xLSTM . Another strand explores hybrid architectures [31; 12] that interleave the above components with attention layers.

In addition, we discuss the training and inference efficiency of some related methods. Let \(D\) denote the hidden dimension, \(H\) the head dimension, and \(N\) the sequence length. For training, RWKV's token-mixing complexity is \(O(DN)\), and Mamba's complexity is \(O(DHN)\) with optimized CUDA kernels. Hyena's is \(O(DN N)\) with Fast Fourier Transform acceleration. In comparison, the chunk-wise recurrent representation is \(O(DN(B+H))\), where \(B\) is the chunk size, and we usually set \(H=256,B 512\). However, chunk-wise computation is highly parallelized, enabling efficient hardware usage. For large model size (i.e., larger \(D\)) or sequence length, the additional \(b+h\) has negligible effects. For inference, among the efficient architectures compared, Hyena has the same complexity (i.e., \(O(N)\) per step) as Transformer, while the others can perform \(O(1)\) decoding.

## 5 Conclusion

We propose retentive networks (RetNet) for sequence modeling, which enables various representations, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better inference efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive performance compared with Transformers. The above advantages make RetNet an ideal successor to Transformers for large language models, especially considering the deployment benefits brought by the \(O(1)\) inference complexity. In the future, we are interested in deploying RetNet on various edge devices, such as mobile phones.

    &  &  \\  & Acc & AP\({}^{b}\) & AP\({}^{b}_{50}\) & AP\({}^{b}_{75}\) & mIoU & mAcc \\  DeiT  & 80.76 & 0.458 & 0.678 & 0.502 & 43.52 & 55.08 \\ RetNet & 81.57 & 0.457 & 0.669 & 0.488 & 44.13 & 56.12 \\   

Table 4: Results on vision tasks, i.e., image classification (ImageNet), object detection (COCO), and semantic segmentation (ADE20K). RetNet achieves competitive performance with DeiT, which is a well-tuned vision Transformer.