# State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory

State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory

 Shida Wang

Department of Mathematics

National University of Singapore

e0622338@u.nus.edu

&Beichen Xue

Department of Statistics and Data Science

National University of Singapore

e0773769@u.nus.edu

###### Abstract

State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.

## 1 Introduction

State-space model [1; 2; 3; 4; 5] is an emerging family of neural networks specialized in learning long sequence relationships. It achieves significantly better performance compared with attention-based transformers in the long range arena (LRA) dataset [6; 7]. Despite its effectiveness, the state-space model is built on a relatively simple foundation of linear-RNN-like layers. One of the key advantages of state-space models is their simple recurrence, which enables efficient acceleration. In fact, this recurrence allows for an asymptotic computational complexity of only \(O(T T)\), which is significantly better than the \(O(T^{2})\) complexity of traditional full-attention approaches . A natural question would be whether SSM achieves this speedup with certain sacrifices in model capacity or memory property. It is currently unclear whether the state-space model's linear architecture with layerwise nonlinearity possesses sufficient expressive capacity to approximate any target sequence-to-sequence relationship. This knowledge would be important to answer pertinent questions regarding the model's ability to handle the complexity of real-world datasets characterized by diverse and intricate sequence relationships. In particular, considering the speed advantage of SSM over attention-based transformers, the universal approximation property impacts whether a state-space model could be a suitable replacement for a transformer.

In this paper, we study the universality of state-space model. Furthermore, the memory property is investigated and we show that state-space models also have an asymptotically exponential decaying memory.

The main contributions can be summarized as follow

1. We give a constructive proof for the universal approximation property for multi-layer state-space models. The width dependency on sequence length is analyzed.

2. The state-space models are shown to have an exponentially decaying memory, which coincides with usual recurrent neural networks.
3. Numerical verifications for the memory property are given on synthetic datasets.

## 2 Background

In this section, we introduce the general form of the state-space models. The classical results on universal approximation for recurrent neural networks are summarized. Based on the approximation result, the well-documented challenge of learning long-term memory via recurrent networks is summarized. In particular, we give the definition of memory function which we shall use in the derivation of memory decay.

### State-space models

Single-layer state-space model can be viewed as a recurrent neural network without nonlinear recurrent activation. As is shown in Figure 1, the discrete time version of SSM  is

\[h_{k+1} =Wh_{k}+Ux_{k+1}, h_{0}=0\] (1) \[y_{k} =Ch_{k}+Dx_{k}=CW^{k}h_{0}+_{i=1}^{k}CW^{k-i}Ux_{i}+Dx_{k}.\] (2)

where \(x^{d_{in}}\), \(y^{d_{out}}\), \(h^{m}\), \(W^{m m}\), \(U^{m d_{in}}\), \(C^{d_{out} m}\), \(D^{d_{out} d_{in}}\). Notice that we drop the bias term for simplicity. For multi-layer state-space model, the nonlinear activation is added layer-wise.

The continuous time version of a single layer is

\[}{dt} =Wh_{t}+Ux_{t}, h_{0}=0\] (3) \[y_{t} =Ch_{t}+Dx_{t}=_{0}^{T}Ce^{W(t-s)}Ux_{s}ds+Dx_{t}.\] (4)

It can be seen in Equation (4) that the first component of output \(y\) is the convolution between kernel function \((t)=Ce^{W(t-s)}U\) and \(x\). In discrete case, the convolution operator \(\) is represented by

\[y_{[0,T]}=(t) x_{[0,T]} y_{k}=_{i=0}^{k}_{k-i}x_ {i}.\] (5)

Compared with temporal convolution network (TCN) , which has a finite kernel size, state-space models can be regarded as implementing global convolution (\((t)=Ce^{W(t-s)}U,t 0\)) over the

Figure 1: Network structure of two-layer state-space model.

temporal axis. Numerically, since the kernel size is the same as the sequence length, the convolution implemented via fast Fourier transform can lead to significant accelerations . Compared with attention-based transformer, it is shown that SSM can speed up the training 100x in learning sequences with length 64K . The state-space model's temporal efficiency is achieved by eliminating the nonlinear activation function in its recurrent layers \(h_{k+1}=Wh_{k}+Ux_{k}+b\), resulting in faster processing of sequential data via parallel scan .

### Universal approximation in RNN

It is long-known that recurrent neural networks with nonlinear sigmoidal activations (such as tanh and sigmoid) are universal approximators.

**Theorem 2.1** (Simplified universality statement).: _For any sequence to sequence map: \(:\) and tolerance \(\), there exists a hidden dimension \(m\) and weights \(c,W,U,b\) such that the RNN with weights \((c,W,U,b)\) can approximate the target sequence to sequence map:_

\[||y-||.\] (6)

_where prediction sequence \(\) is given by_

\[h_{k+1} =(Wh_{k}+Ux_{k}+b),\] (7) \[_{k} =C^{}h_{k}.\] (8)

Universal approximation establishes the feasibility of learning sequence to sequence relationships via recurrent neural networks. However, typical approximation rate results depend on the sequence length . Sequence length dependent approximation rate does not generalize to the case of sequence of infinite length. In learning sequences with infinite length, Li et al.  shows that linear RNNs have difficulty in learning non-exponential decaying memory. Various numerical experiments  confirm that adding nonlinear recurrent activation does not fundamentally change the decay. In state-space models, the nonlinearity is included in a layer-wise approach. It is unknown whether such layer-wise nonlinearity alone is sufficient to approximate any sequence to sequence relationships.

### Memory function and curse of memory

In this paper we study the memory property of state-space model. Before we introduce the main results, we present a simple memory function definition in sequence modelling. Li et al.  proves that a bounded causal continuous regular time-homogeneous linear functional has the following Riesz representation:

\[y_{t}=H_{t}()=_{-}^{t}(t-s)x_{s}ds.\] (9)

Here \(:^{+}\) is an \(L_{1}\)-integrable function. If \(_{t}\) rapidly decreases with \(t\), then the target sequence relationship has a short-term memory.

Since \(_{t}\) fully captures the memory property of a linear functional , we call it the **memory function** (of the linear functional). In particular, when approximating the linear functional with linear RNNs, the model's memory function is \((t-s)=C^{}e^{W(t-s)}U\).

The **curse of memory** refers to the phenomenon that if a target linear functional can be approximated by a sequence of linear RNNs, the memory function \(_{t}\) decays exponentially.

Take the test input to be \(_{}=1&t 0,\\ 0&t<0.\) Notice that the derivative of the linear functional at test input extracts the memory function \(|H_{t}(_{})|=|(t)|_{2}\). Therefore a natural extension of the memory function  to the bounded causal continuous regular time-homogeneous nonlinear functionals will be:

\[(t)=|._{t}}{dt}|_{2},_{t} =}_{t}(_{}).\] (10)

It can be seen this definition is compatible with the memory function definition of linear functional. Also, this memory function can be evaluated by computing the models' derivative at the test input using finite difference method.

## 3 Main results

In this section, we first give a simple constructive proof to show that any element-wise function on input sequence can be approximated by a two-layer state-space model. Next, we show any temporal convolution can be approximated by a state-space model. The constructive proof for general nonlinear sequence to sequence functional is given based on the above propositions. Moreover, as the Kolmogorov-Arnold-representation-based construction has a weight number dependent on sequence length, it can be highly inefficient to construct a shallow wide network to learn long sequences. To reduce the widths dependency on the sequence length, a Volterra-series-based construction is demonstrated.

### Two-layer SSM approximates element-wise function

By element-wise function, we mean learning sequence relationship with form \((x_{1},,x_{T})(f(x_{1}),,f(x_{T}))\).

**Proposition 3.1**.: _For any given continuous function \(f\) over a compact set \(K\), let the activation \(\) be sigmoidal function._

\[_{z}(z)=1,_{z-}(z)=-1.\] (11)

_There exists a sequence of two-layer state-space model with weights \(\{W_{1},W_{2},U_{1},U_{2},b_{1},b_{2}\}\) that can approximate sequence relationship \(:(x_{1},,x_{T})(y_{1},,y_{T})\)._

\[_{t}_{}|f()-}|.\] (12)

_Here the two-layer state-space model is constructed by_

\[h_{k+1}^{(1)} =W_{1}h_{k}^{(1)}+U_{1}x_{k}+b_{1},\] (13) \[h_{k+1}^{(2)} =W_{2}h_{k}^{(2)}+U_{2}(h_{k+1}^{(1)})+b_{2}\] (14) \[y_{k} =h_{k}^{(2)}.\] (15)

The proof is included in Appendix B.2. The main idea is to approximate element-wise function \(f(x)\) with \(U_{2}(U_{1}x+b_{1})\). A graphical demonstration is given in Figure 2. It can be seen the two-layer state-space model can approximate any element-wise function by setting \(W_{1}=W_{2}=b_{2}=0\).

### SSM approximates temporal convolution

In this part we show that state-space models can approximate temporal convolution: \(y_{k}=_{i=1}^{k}_{k-i}x_{i}, 1 k T\). According to Equation (3), the hidden state of SSM is the convolution between input sequence and exponentially decaying function \(_{k}=CW^{k}U\). \(y_{k}=_{i=1}^{k}CW^{k-i}Ux_{i}+CW^{k}h_{0}+Dx_{k}\).

Figure 2: Two-layer state-space model can approximate any continuous element-wise functionTherefore the approximation problem of temporal convolution by state-space models is reduced to the approximation problem of general convolution kernel \(_{k},1 k T\) by exponentially decaying convolution kernel \(_{k},1 k T\). The optimal weights are defined by \(C,W,U=_{C,W,U}_{k}|_{k}-CW^{k}U|\).

**Proposition 3.2**.: _For any given convolution kernel \(_{k},1 k T\), the single-layer state-space model is universal approximator for temporal convolution._

_In other words, for any \(>0\), there exists a hidden dimension \(m\) and corresponding weights \(C,W,U\) such that \(_{k}=CW^{k}U\) satisfies_

\[_{k}|_{k}-_{k}|<.\] (16)

See Appendix B.3 for the proof. The main idea is to represent the single-layer state-space model output as a convolution between input and kernel function. The approximation of temporal convolution is then reduced to the approximation of general kernel with the SSM-induced kernels.

_Remark 3.3_.: Although the Proposition 3.2 indicates that we can use single-layer state-space model to approximate any convolution, it does not reveal the necessary hidden dimension \(m\) for such approximation.

### Universality of SSM

Now we show that five-layer state-space model is universal. Without loss of generality, assume the output \(y\) is one-dimensional. The main proof is based on the famous Kolmogorov-Arnold representation theorem . By Kolmogorov-Arnold representation theorem, we know any multivariate continuous function \(f:^{d}\) can be **represented** by

\[f(x_{1},,x_{d})=_{q=0}^{2d}_{q}(_{p=1}^{d}_{q,p}(x_ {p})).\] (17)

_Remark 3.4_.: George Lorentz  shows that we can use the same \(\): \(f(x_{1},,x_{d})=_{q=0}^{2d}(_{p=1}^{d}_{q,p}(x_{p}) ).\) Sprecher  proves that it can be further reduced to the same \(\): \(f(x_{1},,x_{d})=_{q=0}^{2d}(_{p=1}^{d}_{p}(x _{p}+ p)+c_{q}).\) Braun and Griebel  gives the first constructive proof for the superposition. Moreover, the inner function \(\) is shown to be **independent** of target function \(f\). It means the learning of function \(\) can be approximated without retraining for different target functionals.

We summarize the Kolmogorov-Arnold theorem as follow:

**Theorem 3.5** ([17; 20]).: _Fix dimension \(d 2\). There are real numbers \(a,b_{p},c_{q}\) and a continuous and monotone function \(:\), such that for any continuous function \(f:^{d}\), there exists a continuous function \(:\) with_

\[f(x_{1},,x_{d})=_{q=0}^{2d}(_{p=1}^{d}b_{p}(x_{p}+qa )+c_{q}).\] (18)

**Proposition 3.6**.: _For any bounded causal continuous sequence to sequence relationship \(H:\{x_{k}\}_{k=1}^{T}\{y_{k}\}_{k=1}^{T}\) and tolerance \(>0\), there exists a hidden dimension \(m\) and corresponding state-space model (as constructed in Figure 3) such that the error of approximation_

\[|y_{k}-_{k}|, k\{1,,T\}.\] (19)

See the proof in Appendix B.4. The main idea is demonstrated in Figure 3, the nonlinear functions are separately approximated by two-layer state-space model.

_Remark 3.7_.: The Kolmogorov theorem provides a construction for achieving universality in a five-layer state-space model. However, the quantity of hidden neurons increases linearly with the sequence length. This can become exceedingly burdensome when the sequence length escalates.

Another approach is from the perspective of Volterra Series, which features the sequence-length independent neurons.

**Theorem 3.8** ().: _For any continuous time-invariant system with \(x(t)\) as input and \(y(t)\) as output can be expanded in the Volterra series as follow_

\[y(t)=h_{0}+_{n=1}^{N}_{0}^{t}_{0}^{t}h_{n}(_{1},, _{n})_{j=1}^{n}x(t-_{j})d_{j}.\] (20)

_In particular, we call the expansion order \(N\) to be the series' order._

A simplified interpretation of the \(N\)-th Volterra Series expansion is the "\(N\)-th Taylor expansion in the sequence variable \(\)".

**Proposition 3.9**.: _For any bounded causal continuous time-homogeneous sequence to sequence relationship \(H:\{x_{k}\}_{k=1}^{T}\{y_{k}\}_{k=1}^{T}\) and tolerance \(>0\), there exists a hidden dimension \(m\) and corresponding state-space model (as constructed in Figure 4) such that the error of approximation_

\[|y_{k}-_{k}|, k\{1,,T\}.\] (21)

_Moreover, the neurons of the state-space model do not explicitly depend on the sequence length \(T\)._

See the proof in Appendix B.5. The main idea is to approximate the convolution kernels \(h_{n}(_{1},,_{n})\) by the low-rank tensor product of first-order convolution kernel.

Figure 3: Multi-layer state-space models are universal approximators: Drawing from the Kolmogorov-Arnold representation theorem, we have \(f(x_{1},,x_{d})=_{q=0}^{2d}(_{p=1}^{d}b_{p}(x_{p}+ qa)+c_{q})\). Here, both element-wise nonlinear functions \(\) and \(\) can be approximated by a two-layer state-space model, as shown in Proposition 3.1. Additionally, the temporal convolution is represented by a single-layer state-space model, as detailed in Proposition 3.2.

_Remark 3.10_.: The advantage of Volterra-series-type construction is the approximation of the convolution kernel with SSM-induced kernel as well as the approximation of multivariable convolution kernel with tensor product of one-dimensional convolution kernel do not explicitly depend on the sequence length. Similar approaches have been adopted by idea of implicit convolution in CKConv .

### Memory decay of SSM

In the ensuing discourse, we turn our focus towards an examination of the memory property inherent in state-space models. It has been thoroughly studied in literature that recurrent neural networks exhibit a phenomenon of exponential memory decay . An intriguing question that naturally arises in this context is whether state-space models are plagued by similar challenges. Upon careful investigation, it is concluded that state-space models, much like their neural network counterparts, do possess an asymptotically exponential decaying memory.

**Proposition 3.11**.: _Assume there exists a constant \(c_{0}>0\) such that_

\[_{t}e^{c_{0}t}\|x_{t}-x^{*}\| 0, x^{*}:=_{t}x_{t}.\] (22)

_Assume the output \(y_{t}\) is the output of single-layer state-space model with parameters \(C,W,U\). Then, for the same constant \(c_{0}\), if the output's derivative satisfies \(}{dt} 0\)_

\[_{t}e^{c_{0}t}\|y_{t}-y^{*}\| 0.\] (23)

At the same time, general smooth nonlinear activation does not change the exponential decay property of a sequence:

**Proposition 3.12**.: _Assume there exists a constant \(c_{0}>0\) such that_

\[_{t}e^{c_{0}t}\|x_{t}-x^{*}\| 0, x^{*}:=_{t}x_{t}.\] (24)

_For given Lipschitz continuous layer-wise activations \(\), there exists a positive constant \(c_{0}\) such that the output memory function \(}{dt} 0\)_

\[_{t}e^{c_{0}t}\|y_{t}-y^{*}\| 0.\] (25)

Figure 4: Volterra-series-type construction for state-space models

See the proofs for above two propositions in Appendix B.6 and Appendix B.7.

Based on the above two propositions, by induction we have the following theorem:

**Theorem 3.13**.: _Assume \(\) is a multi-layer state-space model with Lipschitz continuous function as the layer-wise activations. Assume the state-space model is stable in the sense that matrix \(W\)'s eigenvalue are bounded by 1._

_There exists a positive constant \(c_{0}\) such that the memory function (defined in Equation (10)) of state-space model is decaying exponentially_

\[_{t}e^{c_{0}t}(t) 0.\] (26)

## 4 Numerical verifications

Based on the generalization of memory function \(\) in linear functional, we verify the asymptotic memory decay of state-space models with simple randomly generated models. The definition is given in one-dimensional case, but it can be generalized to multi-variable case by taking different unit inputs in various coordinates. The motivation for the above definition is based on the idea of measuring the earlier input at the later output.

In our experiment, we construct various RNN models and SSM with random generated weights. It can be seen in Figure 5 that the memory of naive state-space models also has an exponentially decay memory. It is consistent with the previous theorem that state-space model has an asymptotic exponentially decaying memory. Notice that here the naive SSM is simply adding a tanh activation across layers without specially tuning the weights. Such random initialization can expose the memory issue more significantly as the S4 layer is constructed with several parameterization techniques. However, the manually constructed S4 still has an asymptotic exponential decaying memory as is shown in Figure 6.

## 5 Related Work

In this section, we introduce the previous works on state-space models. As the single-layer state-space model is a linear RNN, we summarize the related approximation work on RNN. In particular, the approximation result and memory result is emphasized as this paper works on the universal approximation property and memory decay property of SSM.

State-space modelsState-space models originate from the HIPPO matrix which is optimal in the online function approximation sense [2; 3; 4]. The Hippo matrix initialization for recurrent matrix \(W\) enables the state-space model to have a slow decaying memory. The universal approximation idea is heuristically demonstrated in Orvieto et al. . However, the proof from the Koopman theory perspective only guarantees the existence of universal approximation. Our proof is a constructive proof which can be further generalized to study the approximation rate with respect to the hidden dimensions and network depths.

Recurrent neural networksRecurrent neural networks (RNNs)  are one of the most popular neural networks for sequence modelling. Various results have been established in RNNs approximation theory, see Sontag , Hanson et al. . Apart from the universal approximation, the exponential decaying memory property is the notorious phenomenon in recurrent neural networks which prohibits the scale up of the models in terms of the sequence length [14; 26].

## 6 Discussion

In Table 1 we compare the classical sequence models including RNN, TCN and attention-based transformer. The state-space model can be considered as an enhancement of Recurrent Neural Networks (RNNs) due to its superior optimization and inference speed. Despite maintaining a similar network topology, inference cost, and memory pattern, it provides a more efficient and streamlined approach. The effort to extend the long-memory learning is also carried out in convolutional networks and attention-based transformers. Romero et al.  proposes to parameterize the convolution kernel implicitly, which utilizes the power of spline function approximation \(y=(w_{0}(Wx+b))\). Similar idea has been adopted in Poli et al. . To summarize, we regard SSM, CKConv and linear transformer as the sequence models' improvement in learning long-memory for long sequences.

## 7 Conclusion

In this paper, we give a constructive proof for the universal approximation property of multi-layer state-space models. It is shown that the nonlinear recurrent activations in classical recurrent neural networks are not necessary when there are nonlinear activations across different hidden layers. This result implies state-space model is as powerful as the classical recurrent neural networks in the approximation sense. Furthermore, we study the memory decay in multi-layer state-space models, which is a notorious issue in classical recurrent neural networks. While empirical evidence suggests

    & RNN & TCN & Transformer \\  Number of weights & \(Lm(m+d_{}+d_{})\) & \(LCKd_{}\) & \(3Lmd_{}\) \\ Single-step inference cost & \(O(1)\) & \(O(1)\) & \(O(T^{2})\) \\ Memory pattern & exponential decay & low rank & no restriction \\ Memory improved version & SSM & CKConv & Linear Transformer \\   

Table 1: Comparison of sequence models: \(T\) is the sequence length, \(L\) is the number of layers, \(m\) is the hidden dimension, \(C\) and \(K\) are the total number of channels and convolution kernel sizes in TCN. Input and output dimensions are \(d_{}\) and \(d_{}\). Despite TCN’s inference cost is independent of sequence length, it depends on the kernel size \(K\), which typically bears a similar scale to \(T\).

Figure 5: Memory functions of a randomly initialized recurrent networks. The shadow indicates the error bar for 100 repeats.

that state-space models do not experience significant memory decay, they nonetheless exhibit a memory pattern that decays exponentially in the asymptotic sense.

Our research has exciting implications for future work in state-space models. By extending our work to the approximation rate of state-space models, we can obtain better understanding of state-space models' hypothesis space. Such result is important to further optimize the architecture in various real-world applications. We aim to unlock the full potential of state-space models by identifying the ideal network structure (including depth and hidden dimension) for specific tasks and applications.

Figure 6: Memory function of a randomly initialized S4. For each model, as the time increases, the memory function can be “capped” by a straight line, which indicate that the memories are decaying exponentially. Compared with Figure 5(d), the results indicate that the smart initialization from S4 provides the memory function with a slower decay.