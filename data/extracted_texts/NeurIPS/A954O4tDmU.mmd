# Agd: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix

Yun Yue

Ant Group

Hangzhou, Zhejiang, China

yueyun.yy@antgroup.com

&Zhiling Ye

Ant Group

Hangzhou, Zhejiang, China

yezhiling.yzl@antgroup.com

&Jiadi Jiang

Ant Group

Hangzhou, Zhejiang, China

jiadi.jjd@antgroup.com

&Yongchao Liu

Ant Group

Hangzhou, Zhejiang, China

yongchao.ly@antgroup.com

&Ke Zhang

Ant Group

Beijing, China

yingzi.zk@antgroup.com

Co-first authors with equal contributions.

###### Abstract

Adaptive optimizers, such as Adam, have achieved remarkable success in deep learning. A key component of these optimizers is the so-called preconditioning matrix, providing enhanced gradient information and regulating the step size of each gradient direction. In this paper, we propose a novel approach to designing the preconditioning matrix by utilizing the gradient difference between two successive steps as the diagonal elements. These diagonal elements are closely related to the Hessian and can be perceived as an approximation of the inner product between the Hessian row vectors and difference of the adjacent parameter vectors. Additionally, we introduce an auto-switching function that enables the preconditioning matrix to switch dynamically between Stochastic Gradient Descent (SGD) and the adaptive optimizer. Based on these two techniques, we develop a new optimizer named Agd that enhances the generalization performance. We evaluate Agd on public datasets of Natural Language Processing (NLP), Computer Vision (CV), and Recommendation Systems (RecSys). Our experimental results demonstrate that Agd outperforms the state-of-the-art (SOTA) optimizers, achieving highly competitive or significantly better predictive performance. Furthermore, we analyze how Agd is able to switch automatically between SGD and the adaptive optimizer and its actual effects on various scenarios. The code is available at this link2.

## 1 Introduction

Consider the following empirical risk minimization problems:

\[_{^{n}}f():=_{k=1}^{M}(; _{k}), \]

where \(^{n}\) is the parameter vector to optimize, \(\{_{1},,_{M}\}\) is the training set, and \((;)\) is the loss function measuring the predictive performance of the parameter \(\) on the example \(\). Since it is expensive to calculate the full batch gradient in each optimization iteration when \(M\) is large, thestandard approach is to adopt a mini-batched stochastic gradient, i.e.,

\[()=|}_{k}(;_{k}),\]

where \(\{1,,M\}\) is the sample set of size \(|| M\). Obviously, we have \(_{p()}[()]= f()\) where \(p()\) is the distribution of the training data. Equation (1) is usually solved iteratively. Assume \(_{t}\) is already known and let \(=_{t+1}-_{t}\), we have

\[&*{arg\,min}_{_{t+1}^{n}}f( _{t+1})=*{arg\,min}_{^{n}}f( +_{t})\\ &*{arg\,min}_{^{n}}f( _{t})+()^{T} f(_{t})+()^ {T}^{2}f(_{t})\\ &*{arg\,min}_{^{n}}f( _{t})+()^{T}_{t}+}()^{ T}B_{t}, \]

where the first approximation is from Taylor expansion, and the second approximation are from \(_{t} f(_{t})\) (\(_{t}\) denotes the weighted average of gradient \(_{t}\)) and \(_{t})^{T}B_{t}}{()^{T} ^{2}f(_{t})}\) (\(_{t}\) denotes the step size). By solving Equation (2), the general update formula is

\[_{t+1}=_{t}-_{t}B_{t}^{-1}_{t}, t\{1,2,,T \}\,, \]

where \(B_{t}\) is the so-called preconditioning matrix that adjusts updated velocity of variable \(_{t}\) in each direction. The majority of gradient descent algorithms can be succinctly encapsulated by Equation (3), ranging from the conventional second order optimizer, Gauss-Newton method, to the standard first-order optimizer, SGD, via different combinations of \(B_{t}\) and \(_{t}\). Table 1 summarizes different implementations of popular optimizers.

Intuitively, the closer \(B_{t}\) approximates the Hessian, the faster convergence rate the optimizer can achieve in terms of number of iterations, since the Gauss-Hessian method enjoys a quadratic rate, whereas the gradient descent converges linearly under certain conditions (Theorems 1.2.4, 1.2.5 in Nesterov ). However, computing the Hessian is computationally expensive for large models. Thus, it is essential to strike a balance between the degree of Hessian approximation and computational efficiency when designing the preconditioning matrix.

In this paper, we propose the AGD (**A**uto-switchable optimizer with **G**radient **D**ifference of adjacent steps) optimizer based on the idea of efficiently and effectively acquiring the information of the Hessian. The diagonal entries of AGD's preconditioning matrix are computed as the difference of gradients between two successive iterations, serving as an approximation of the inner product between the Hessian row vectors and difference of parameter vectors. In addition, AGD is equipped with an adaptive switching mechanism that automatically toggles its preconditioning matrix between SGD and the adaptive optimizer, governed by a threshold hyperparameter \(\) which enables AGD adaptive to various scenarios. Our contributions can be summarized as follows.

* We present a novel optimizer called AGD, which efficiently and effectively integrates the information of the Hessian into the preconditioning matrix and switches dynamically between SGD and the adaptive optimizer. We establish theoretical results of convergence guarantees for both non-convex and convex stochastic settings.
* We validate AGD on six public datasets: two from NLP (IWSLT14  and PTB ), two from CV (Cifar10  and ImageNet ), and the rest two from RecSys (Criteo  and Avazu ). The experimental results suggest that AGD is on par with or outperforms the SOTA optimizers.
* We analyze how AGD is able to switch automatically between SGD and the adaptive optimizer, and assess the effect of hyperparameter \(\) which controls the auto-switch process in different scenarios.

   \(_{t}\) & **Optimizer** \\  \(B_{t}=\) & Gauss-Hessian \\  \(B_{t}\) & BFGS , LBFGS  \\  \(B_{t}()\) & AdaHessian  \\  \(B_{t}=\) & Natural Gradient  \\  \(B_{t}^{2}_{emp}\) & Shampoo  \\  \(B_{t}^{2}(_{emp})\) & Adapt , AdaDeita  \\  \(B_{t}^{2}((_{t}))\) & Adam , ADAM , AMSGrad  \\  \(B_{t}=\) & SGD , Momentum  \\   

Table 1: Different optimizers by choosing different \(B_{t}\).

#### Notation

We use lowercase letters to denote scalars, boldface lowercase to denote vectors, and uppercase letters to denote matrices. We employ subscripts to denote a sequence of vectors, e.g., \(_{1},,_{t}\) where \(t[T]:=\{1,2,,T\}\), and one additional subscript is used for specific entry of a vector, e.g., \(x_{t,i}\) denotes \(i\)-th element of \(_{t}\). For any vectors \(,^{n}\), we write \(^{T}\) or \(\) for the standard inner product, \(\) for element-wise multiplication, \(/\) for element-wise division, \(}\) for element-wise square root, \(^{2}\) for element-wise square, and \((,)\) for element-wise maximum. Similarly, any operator performed between a vector \(^{n}\) and a scalar \(c\), such as \((,c)\), is also element-wise. We denote \(\|\|=\|\|_{2}=,}\) for the standard Euclidean norm, \(\|\|_{1}=_{i}|x_{i}|\) for the \(_{1}\) norm, and \(\|\|_{}=_{i}|x_{i}|\) for the \(_{}\)-norm, where \(x_{i}\) is the \(i\)-th element of \(\).

Let \(f_{t}()\) be the loss function of the model at \(t\)-step where \(^{n}\). We consider \(_{t}\) as Exponential Moving Averages (EMA) of \(_{t}\) throughout this paper, i.e.,

\[_{t}=_{1}_{t-1}+(1-_{1})_{t}=(1-_{1})_{i= 1}^{t}_{t-i+1}_{1}^{i-1},\;t 1, \]

where \(_{1}[0,1)\) is the exponential decay rate.

## 2 Related work

ASGD  leverages Taylor expansion to estimate the gradient at the global step in situations where the local worker's gradient is delayed, by analyzing the relationship between the gradient difference and Hessian. To approximate the Hessian, the authors utilize the diagonal elements of empirical Fisher information due to the high computational and spatial overhead of Hessian. AdaBelief employs the EMA of the gradient as the predicted gradient and adapts the step size by scaling it with the difference between predicted and observed gradients, which can be considered as the variance of the gradient.

Hybrid optimization methods, including AdaBound and SWATS, have been proposed to enhance generalization performance by switching an adaptive optimizer to SGD. AdaBound utilizes learning rate clipping on Adam, with upper and lower bounds that are non-increasing and non-decreasing functions, respectively. One can show that it ultimately converges to the learning rate of SGD. Similarly, SWATS also employs the clipping method, but with constant upper and lower bounds.

## 3 Algorithm

Algorithm 1 summarizes our AGD algorithm. The design of AGD comes from two parts: gradient difference and auto switch for faster convergence and better generalization performance across tasks.

Figure 1: Trajectories of AGD and Adam in the Beale function.

Gradient differenceOur motivation stems from how to efficiently and effectively integrate the information of the Hessian into the preconditioning matrix. Let \(=_{t}-_{t-1}\) and \(_{i}f\) denote the \(i\)-th element of \( f\). From Taylor expansion or the mean value theorem, when \(\|\|\) is small, we have the following approximation,

\[_{i}f(_{t})-_{i}f(_{t-1})_{i}f(_{t}).\]

It means the difference of gradients between adjacent steps can be an approximation of the inner product between the Hessian row vectors and the difference of two successive parameter vectors. To illustrate the effectiveness of gradient difference in utilizing Hessian information, we compare the convergence trajectories of AGD and Adam on the Beale function. As shown in Figure 1, we see that AGD converges much faster than Adam; when AGD reaches the optimal point, Adam has only covered about half of the distance. We select the two most representative points on the AGD trajectory in the figure, the maximum and minimum points of \(\| f\|_{1}/\|(H)\|_{1}\), to illustrate how AGD accelerates convergence by utilizing Hessian information. At the maximum point (A), where the gradient is relatively large and the curvature is relatively small (\(\| f\|_{1}=22.3\), \(\|(H)\|_{1}=25.3\)), the step size of AGD is 1.89 times that of Adam. At the minimum point (B), where the gradient is relatively small and the curvature is relatively large (\(\| f\|_{1}=0.2\), \(\|(H)\|_{1}=34.8\)), the step size decreases to prevent it from missing the optimal point during the final convergence phase.

To approximate \( f(_{t})\), we utilize \(_{t}/(1-_{1}^{t})\) instead of \(_{t}\), as the former provides an unbiased estimation of \( f(_{t})\) with lower variance. According to Kingma and Ba , we have \([_{t}}{1-_{1}^{t}}][_{t}]\), where the equality is satisfied if \(\{_{t}\}\) is stationary. Additionally, assuming \(\{_{t}\}\) is strictly stationary and \((_{i},_{j})=0\) if \(i j\) for simplicity and \(_{1}(0,1)\), we observe that

\[[_{t}}{1-_{1}^{t}}]=^{t})^{2}}[(1-_{1})_{i=1}^{t}_{1}^{t-i}_{i}]=^{t})(1-_{1})}{(1-_{1}^{t})(1+_{ 1})}[_{t}]<[_{t}].\]

Now, we denote

\[_{t}=\{_{1}/(1-_{1})&t=1,\\ _{t}/(1-_{1}^{t})-_{t-1}/(1-_{1}^{t-1})&t>1,.\]

and design the preconditioning matrix \(B_{t}\) satisfying

\[B_{t}^{2}=((_{1}_{1}^{T},_{2}_{2} ^{T},,_{t}_{t}^{T}))/(1-_{2}^{t}),\]

where \(_{2}\) represents the parameter of EMA and bias correction is achieved via the denominator.

Note that previous research, such as the one discussed in Section 2 by Zheng et al. , has acknowledged the correlation between the difference of two adjacent gradients and the Hessian. However, the key difference is that they did not employ this relationship to construct an optimizer. In contrast, our approach presented in this paper leverages this relationship to develop an optimizer, and its effectiveness has been validated in the experiments detailed in Section 4.

Auto switchTypically a small value is added to \(B_{t}\) for numerical stability, resulting in \(B_{t}+\). However, in this work we propose to replace this with \((B_{t},)\), where we use a different notation \(\) to emphasize its crucial role in auto-switch mechanism. In contrast to \(\), \(\) can be a relatively large (such as 1e-2). If the element of \(}_{t}:=_{t}/(1-_{2}^{t})}\) exceeds \(\), AGD (Line 7 of Algorithm 1) takes a confident adaptive step. Otherwise, the update is performed using EMA, i.e., \(_{t}\), with a constant scale of \(_{t}/(1-_{1}^{t})\), similar to SGD with momentum. It's worth noting that, AGD can automatically switch modes on a per-parameter basis as the training progresses.

Compared to the commonly used additive method, AGD effectively eliminates the noise generated by \(\) during adaptive updates. In addition, AGD offers an inherent advantage of being able to generalize across different tasks by tuning the value of \(\), obviating the need for empirical choices among a plethora of optimizers.

### Comparison with other optimizers

Comparison with AdaBoundAs noted in Section 2, the auto-switch bears similarities to AdaBound  in its objective to enhance the generalization performance by switching to SGD using the clipping method. Nonetheless, the auto-switch's design differs significantly from AdaBound. Rather than relying solely on adaptive optimization in the early stages, AGD has the flexibility to switch seamlessly between stochastic and adaptive methods, as we will demonstrate in Section 4.5. In addition, AGD outperforms AdaBound's across various tasks, as we will show in Appendix A.2.

Comparison with AdaBeliefWhile in principle our design is fundamentally different from that of AdaBelief , which approximates gradient variance with its preconditioning matrix, we do see some similarities in our final forms. Compared with the denominator of AGD, the denominator of AdaBelief \(_{t}=_{t}-_{t}=}{1-_{1}}(_{t}- _{t-1})\) lacks bias correction for the subtracted terms and includes a multiplication factor of \(}{1-_{1}}\). In addition, we observe that AGD exhibits superior stability compared to AdaBelief. As shown in Figure 2, when the value of \(\) deviates from 1e-8 by orders of magnitude, the performance of AdaBelief degrades significantly; in contrast, AGD maintains good stability over a wide range of \(\) variations.

## 4 Experiments

### Experiment setup

We extensively compared the performance of various optimizers on diverse learning tasks in NLP, CV, and RecSys; we only vary the settings for the optimizers and keep the other settings consistent in this evaluation. To offer a comprehensive analysis, we provide a detailed description of each task and the optimizers' efficacy in different application domains.

**NLP:** We conduct experiments using Language Modeling (LM) on Penn TreeBank  and Neural Machine Translation (NMT) on IWSLT14 German-to-English (De-En)  datasets. For the LM task,

Figure 3: Trajectories of different optimizers in three test functions, where \(f(x,y)=(x+y)^{2}+(x-y)^{2}/10\). We also provide animated versions at [https://youtu.be/Qv5X3v5YUwO](https://youtu.be/Qv5X3v5YUwO).

Figure 2: Comparison of stability between AGD and AdaBelief relative to the parameter \(\) (or \(\)) for ResNet32 on Cifar10. AGD shows better stability over a wide range of \(\) (or \(\)) variations than AdaBelief.

we train 1, 2, and 3-layer LSTM models with a batch size of 20 for 200 epochs. For the NMT task, we implement the Transformer small architecture, and employ the same pre-processing method and settings as AdaHessian , including a length penalty of 1.0, beam size of 5, and max tokens of 4096. We train the model for 55 epochs and average the last 5 checkpoints for inference. We maintain consistency in our learning rate scheduler and warm-up steps. Table 2 provides complete details of the experimental setup.

**CV:** We conduct experiments using ResNet20 and ResNet32 on the Cifar10  dataset, and ResNet18 on the ImageNet  dataset, as detailed in Table 2. It is worth noting that the number of parameters of ResNet18 is significantly larger than that of ResNet20/32, stemming from inconsistencies in ResNet's naming conventions. Within the ResNet architecture, the consistency in filter sizes, feature maps, and blocks is maintained only within specific datasets. Originally proposed for ImageNet, ResNet18 is more complex compared to ResNet20 and ResNet32, which were tailored for the less demanding Cifar10 dataset. Our training process involves 160 epochs with a learning rate decay at epochs 80 and 120 by a factor of 10 for Cifar10, and 90 epochs with a learning rate decay every 30 epochs by a factor of 10 for ImageNet. The batch size for both datasets is set to 256.

**RecSys:** We conduct experiments on two widely used datasets, Avazu  and Criteo , which contain logs of display ads. The goal is to predict the Click Through Rate (CTR). We use the samples from the first nine days of Avazu for training and the remaining samples for testing. We employ the Multilayer Perceptron (MLP) structure (a fundamental architecture used in most deep CTR models). The model maps each categorical feature into a 16-dimensional embedding vector, followed by four fully connected layers of dimensions 64, 32, 16, and 1, respectively. For Criteo, we use the first 6/7 of all samples as the training set and last 1/7 as the test set. We adopt the Deep & Cross Network (DCN)  with an embedding size of 8, along with two deep layers of size 64 and two cross layers. Detailed summary of the specifications can be found in Table 2. For both datasets, we train them for one epoch using a batch size of 512.

Optimizers to compare include SGD , Adam , AdamW , AdaBelief  and AdaHessian . To determine each optimizer's hyperparameters, we adopt the parameters suggested in the literature of AdaHessian and AdaBelief when the experimental settings are identical. Otherwise, we perform hyperparameter searches for optimal settings. A detailed description of this process can be found in Appendix A.1. For our NLP and CV experiments, we utilize GPUs with the PyTorch framework , while our RecSys experiments are conducted with three parameter servers and five workers in the TensorFlow framework . To ensure the reliability of our results, we execute each experiment five times with different random seeds and calculate statistical results.

### Nlp

We report the perplexity (PPL, lower is better) and case-insensitive BiLingual Evaluation Understudy (BLEU, higher is better) score on test set for LM and NMT tasks, respectively. The results are shown in Table 3. For the LM task on PTB, AGD achieves the lowest PPL in all 1,2,3-layer LSTM experiments, as demonstrated in Figure 4. For the NMT task on IWSLT14, AGD is on par with AdaBelief, but outperforms the other optimizers.

### Cv

Table 4 reports the top-1 accuracy for different optimizers when trained on Cifar10 and ImageNet. It is remarkable that AGD outperforms other optimizers on both Cifar10 and ImageNet. The test accuracy (\([]\)) curves of different optimizers for ResNet20/32 on Cifar10 and ResNet18 on

   Task & Dataset & Model & _Train_ & _Val/Test_ & _Params_ \\   &  & 1-layer LSTM &  & 5.3M \\  & & 2-layer LSTM & & \\  & & 3-layer LSTM & & 24.2M \\ NLP-NMT & IWSLT14 De-En & Transformer small & 153K & 7K/7K & 36.7M \\   & Cifar10 & ResNet20/ResNet32 & 50K & 10K & 0.27M/0.47M \\  & ImageNet & ResNet18 & 1.28M & 50K & 11.69M \\   & Avazu & MLP & 36.2M & 4.2M & 151M \\  & Criteo & DCN & 39.4M & 6.6M & 270M \\   

Table 2: Experiments setup.

ImageNet are illustrated in Figure 5. Notice that the numbers of SGD and AdaHessian on ImageNet are lower than the numbers reported in original papers [7; 34], which were run only once (we average multiple trials here). AdaHessian can achieve \(70.08\%\) top-1 accuracy in Yao et al.  while we report \(69.57 0.12\%\). Due to the limited training details provided in Yao et al. , it is difficult for us to explain the discrepancy. However, regardless of which result of AdaHessian is taken, AGD outperforms AdaHessian significantly. Our reported top-1 accuracy of SGD is \(69.94 0.10\%\), slightly lower than \(70.23\%\) reported in Chen et al. . We find that the differences in training epochs, learning rate scheduler and weight decay rate are the main reasons. We also run the experiment using the same configuration as in Chen et al. , and AGD can achieve \(70.45\%\) accuracy at lr = 4e-4 and \(\) = 1e-5, which is still better than the \(70.23\%\) result reported in Chen et al. .

We also report the accuracy of AGD for ResNet18 on Cifar10 for comparing with the SOTA results 3, which is listed in Appendix A.5. Here we clarify again the ResNet naming confusion. The test accuracy of ResNet18 on Cifar10 training with AGD is above 95%, while ResNet32 is about 93% since ResNet18 is much more complex than ResNet32.

### RecSys

To evaluate the accuracy of CTR estimation, we have adopted the Area Under the receiver-operator Curve (AUC) as our evaluation criterion, which is widely recognized as a reliable measure . As stated in Cheng et al. , Wang et al. , Ling et al. , Zhu et al. , even an absolute

   Dataset &  &  \\ Metric &  &  \\ Model & 1-layer LSTM & 2-layer LSTM & 3-layer LSTM & Transformer \\  SGD & \(85.36.34\) (\(-4.13\)) & \(67.26.17\) (\(-1.42\)) & \(63.68.17\) (\(-2.79\)) & \(28.57.15\)(\(+7.37\)) \\ Adam & \(84.50.16\) (\(-3.27\)) & \(67.01.11\) (\(-1.17\)) & \(64.45.26\) (\(-3.56\)) & \(32.93.26\) (\(+3.01\)) \\ AdamW & \(88.16.19\) (\(-6.93\)) & \(95.25.13\) (\(-29.41\)) & \(102.61.13\) (\(-41.72\)) & \(35.82.06\) (\(+0.12\)) \\ AdaBelief & \(84.40.21\) (\(-3.17\)) & \(66.69.23\) (\(-0.85\)) & \(61.34.11\) (\(-0.45\)) & \(35.93.08\) (\(+0.01\)) \\ AdaHessian & \(88.62.15\) (\(-7.39\)) & \(73.37.22\) (\(-7.53\)) & \(69.51.19\) (\(-8.62\)) & \(35.79.06\)(\(+0.15\)) \\ 
**AGD** & \(\) & \(\) & \(\) & \(\) \\   

Table 3: Test PPL and BLEU score for LM and NMT tasks. \(\) is reported in AdaHessian .

Figure 4: Test PPL (\([]\)) on Penn Treebank for 1,2,3-layer LSTM.

Figure 5: Test accuracy (\([]\)) of different optimizers for ResNet20/32 on Cifar10 and ResNet18 on ImageNet.

improvement of 1%e in AUC can be considered practically significant given the difficulty of improving CTR prediction. Our experimental results in Table 5 indicate that AGD can achieve highly competitive or significantly better performance when compared to other optimizers. In particular, on the Avazu task, AGD outperforms all other optimizers by more than 1%e. On the Criteo task, AGD performs better than SGD and AdaHessian, and achieves comparable performance to Adam and AdaBelief.

### The effect of \(\)

In this section, we aim to provide a comprehensive analysis of the impact of \(\) on the training process by precisely determining the percentage of \(}_{t}\) that Algorithm 1 truncates. To this end, Figure 6 shows the distribution of \(}_{t}\) across various tasks under the optimal configuration that we have identified. The black dot on the figure provides the precise percentage of \(}_{t}\) that \(\) truncates during the training process. Notably, a lower percentage indicates a higher degree of SGD-like updates compared to adaptive steps, which can be adjusted through \(\).

As SGD with momentum generally outperforms adaptive optimizers on CNN tasks [34; 39], we confirm this observation as illustrated in Figure 5(a): AGD behaves more like SGD during the initial stages of training (before the first learning rate decay at the 30th epoch) and switches to adaptive optimization for fine-tuning. Figure 5(b) indicates that the parameters taking adaptive updates are dominant, as expected because adaptive optimizers such as AdamW are preferred in transformers. Figure 5(c) demonstrates that most parameters update stochastically, which explains why AGD has a similar curve to SGD in Figure 3(b) before the 100th epoch. The proportion of parameters taking adaptive updates grows from 3% to 5% afterward, resulting in a better PPL in the fine-tuning stage. Concerning Figure 5(d), the model of the RecSys task trains for only one epoch, and AGD gradually switches to adaptive updates for a better fit to the data.

### Computational cost

We train a Transformer small model for IWSLT14 on a single NVIDIA P100 GPU. AGD is comparable to the widely used AdamW optimizer, while significantly outperforms AdaHessian in terms of memory footprint and training speed. As a result, AGD can be a drop-in replacement for AdamW with similar computation cost and better generalization performance.

  Dataset &  & ImageNet \\ Model & ResNe20 & ResNet32 & ResNet18 \\  SGD & \(92.14.14(+0.21)93.10.07(+0.02)69.94.10(+0.41)\) & SGD & \(0.7463.005(+1.78)\) & \(0.7296.0067(+72.75)\) \\ Adam & \(90.46.20(+1.89)91.54.12(+1.58)64.03.16(+6.32)\) & Adam & \(0.7458.0010(+2.24)\) & \(\) \\ AdamW & \(92.12.14(+0.23)92.72.01(+0.40)69.11.17(+1.24)\) & AdaBelief & \(0.7467.0009(+1.38)\) & \(0.8022.0002(+0.15)\) \\ AdaHessian & \(92.19.15(+0.16)92.90.13(+0.22)70.20.03(+0.15)\) & AdaHessian & \(0.7434.006(+4.6)\) & \(0.8004.0005(+1.94)\) \\  AGD & \(\) & \(\) \\  

Table 4: Top-1 accuracy for different optimizers when trained on Cifar10 and ImageNet.

## 5 Theoretical analysis

Using the framework developed in Reddi et al. , Yang et al. , Chen et al. , Zhou et al. , we have the following theorems that provide the convergence in non-convex and convex settings. Particularly, we use \(_{1,t}\) to replace \(_{1}\), where \(_{1,t}\) is non-increasing with respect to \(t\).

**Theorem 1**.: _(Convergence in non-convex settings) Suppose that the following assumptions are satisfied:_

1. \(f\) _is differential and lower bounded, i.e.,_ \(f(^{*})>-\) _where_ \(^{*}\) _is an optimal solution._ \(f\) _is also_ \(L\)_-smooth, i.e.,_ \(,^{n}\)_, we have_ \(f() f()+ f(),-+\|-\|^{2}\)_._
2. _At step_ \(t\)_, the algorithm can access a bounded noisy gradient and the true gradient is bounded, i.e.,_ \(\|_{t}\|_{} G_{},\| f(_{t})\|_{} G _{}, t[T]\)_. Without loss of generality, we assume_ \(G_{}\)_._
3. _The noisy gradient is unbiased and the noise is independent, i.e.,_ \(_{t}= f(_{t})+_{t},[_{t}]=\) _and_ \(_{i}\) _is independent of_ \(_{j}\) _if_ \(i j\)_._
4. \(_{t}=/\)_,_ \(_{1,t}\) _is non-increasing satisfying_ \(_{1,t}_{1}[0,1)\)_,_ \(_{2}[0,1)\) _and_ \(b_{t,i} b_{t+1,i}\  i[n]\)_._

_Then Algorithm 1 yields_

\[_{t[T]}[\| f(_{t})\|^{2}]<C_{3}- }+C_{4}-}+C_{5}^{T}_{t}(_{1,t}-_{1,t+1})}{-}, \]

_where \(C_{3}\), \(C_{4}\) and \(C_{5}\) are defined as follows:_

\[C_{3}= }{(1-_{1})^{2}(1-_{2})^{2}} f(_{1})-f(^{*})+^{2}}{(1-_{1})^{8} ^{2}}(+8L)+nG_{}^{2}}{(1-_{1 })^{3}},\] \[C_{4}= ^{3}}{2(1-_{2})^{2}(1-_{1})^{ 10}^{2}}, C_{5}=^{3}}{(1-_{1})^{5}(1- _{2})^{2}}.\]

The proof of Theorem 1 is presented in Appendix B. There are two important points that should be noted: Firstly, in assumption 2, we can employ the gradient norm clipping technique to ensure the upper bound of the gradients. Secondly, in assumption 4, \(b_{t,i} b_{t+1,i}\  i[n]\), which is necessary for the validity of Theorems 1 and 2, may not always hold. To address this issue, we can implement the AMSGrad condition  by setting \(_{t+1}=(_{t+1},_{t})\). However, this may lead to a potential decrease in the algorithm's performance in practice. The more detailed analysis is provided in Appendix A.4. From Theorem 1, we have the following corollaries.

**Corollary 1**.: _Suppose \(_{1,t}=_{1}/\), we have_

\[_{t[T]}[\| f(_{t})\|^{2}]<C_{3}- }+C_{4}-}+}{1-_{1 }}-},\]

_where \(C_{3}\), \(C_{4}\) and \(C_{5}\) are the same with Theorem 1._

The proof of Corollary 1 can be found in Appendix C.

**Corollary 2**.: _Suppose \(_{1,t}=_{1},\  t[T]\), we have_

\[_{t[T]}[\| f(_{t})\|^{2}]<C_{3}- }+C_{4}-},\]

_where \(C_{3}\) and \(C_{4}\) are the same with Theorem 1._

  
**Optimizer** & **Memory** & **Time per Epoch** & **Relative time to AdamW** \\  SGD & \(5119\,\) & \(230\,\) & \(0.88\) \\ AdamW & \(5413\,\) & \(260\,\) & \(1.00\) \\ AdaHessian & \(8943\,\) & \(750\,\) & \(2.88\) \\ AGD & \(5409\,\) & \(278\,\) & \(1.07\) \\   

Table 6: Computational cost for Transformer small.

Corollaries 1 and 2 imply the convergence (to the stationary point) rate for AGD is \(O( T/)\) in non-convex settings.

**Theorem 2**.: _(Convergence in convex settings) Let \(\{_{t}\}\) be the sequence obtained by AGD (Algorithm 1), \(_{t}=/\), \(_{1,t}\) is non-increasing satisfying \(_{1,t}_{1}[0,1)\), \(_{2}[0,1)\), \(b_{t,i} b_{t+1,i}\)\( i[n]\) and \(\|_{t}\|_{} G_{}, t[T]\). Suppose \(f_{t}()\) is convex for all \(t[T]\), \(^{*}\) is an optimal solution of \(_{t=1}^{T}f_{t}()\), i.e., \(^{*}=_{^{n}}_{t=1}^{T}f_{t}()\) and there exists the constant \(D_{}\) such that \(_{t[T]}\|_{t}-^{*}\|_{} D_{}\). Then we have the following bound on the regret_

\[_{t=1}^{T}(f_{t}(_{t})-f_{t}(^{*}))< }(C_{1}+_{t=1}^{T}}{2 _{t}}nD_{}^{2}+C_{2}),\]

_where \(C_{1}\) and \(C_{2}\) are defined as follows:_

\[C_{1}=+)D_{}^{2}}{2}(1-_{1})^{2}}, C_{2}=^{2}}{(1- _{1})^{3}}(1+}}).\]

The proof of Theorem 2 is given in Appendix D. To ensure that the condition \(_{t[T]}\|_{t}-^{*}\|_{} D_{}\) holds, we can assume that the domain \(^{n}\) is bounded and project the sequence \(\{_{t}\}\) onto \(\) by setting \(}=_{}(_{t}-_{t}^{2}}}{1-_{1}^{2}}_{t}}{(_{t}}, {1-_{2}^{2}})})\). From Theorem 2, we have the following corollary.

**Corollary 3**.: _Suppose \(_{1,t}=_{1}/t\), we have_

\[_{t=1}^{T}(f_{t}(_{t})-f_{t}(^{*}))< }(C_{1}+^{2}_{1}}{ }}+C_{2}),\]

_where \(C_{1}\) and \(C_{2}\) are the same with Theorem 2._

The proof of Corollary 3 is given in Appendix E. Corollary 3 implies the regret is \(O()\) and can achieve the convergence rate \(O(1/)\) in convex settings.

## 6 Conclusion

In this paper, we introduce a novel optimizer, AGD, which incorporates the Hessian information into the preconditioning matrix and allows seamless switching between SGD and the adaptive optimizer. We provide theoretical convergence rate proofs for both non-convex and convex stochastic settings and conduct extensive empirical evaluations on various real-world datasets. The results demonstrate that AGD outperforms other optimizers in most cases, resulting in significant performance improvements. Additionally, we analyze the mechanism that enables AGD to automatically switch between stochastic and adaptive optimization and investigate the impact of the hyperparameter \(\) for this process.