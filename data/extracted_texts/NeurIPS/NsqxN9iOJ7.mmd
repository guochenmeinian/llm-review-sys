# Motion Consistency Model:

Accelerating Video Diffusion with

Disentangled Motion-Appearance Distillation

 Yuanhao Zhai\({}^{}\) &Kevin Lin\({}^{}\) &Zhengyuan Yang\({}^{}\) &Linjie Li\({}^{}\) &Jianfeng Wang\({}^{}\)

**Chung-Ching Lin\({}^{}\)** &David Doermann\({}^{}\)** &**Junsong Yuan\({}^{}\)** &Lijuan Wang\({}^{}\)

\({}^{}\)State University of New York at Buffalo \({}^{}\)Microsoft

{yzhai6,doermann,jsyuan}@buffalo.edu

{keli,zhengyang,lindsey.li,jianfw,chungching.lin,lijuanw}@microsoft.com

https://yhzhai.github.io/mcm/

Work done during an internship at Microsoft.

###### Abstract

Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets. This affects the performance of both teacher and student video diffusion models. Our study aims to improve video diffusion distillation while improving frame appearance using abundant high-quality image data. We propose motion consistency model (MCM), a single-stage video diffusion distillation method that disentangles motion and appearance learning. Specifically, MCM includes a video consistency model that distills motion from the video teacher model, and an image discriminator that enhances frame appearance to match high-quality image data. This combination presents two challenges: (1) conflicting frame learning objectives, as video distillation learns from low-quality video frames while the image discriminator targets high-quality images; and (2) training-inference discrepancies due to the differing quality of video samples used during training and inference. To address these challenges, we introduce disentangled motion distillation and mixed trajectory distillation. The former applies the distillation objective solely to the motion representation, while the latter mitigates training-inference discrepancies by mixing distillation trajectories from both the low- and high-quality video domains. Extensive experiments show that our MCM achieves the state-of-the-art video diffusion distillation performance. Additionally, our method can enhance frame quality in video diffusion models, producing frames with high aesthetic scores or specific styles without corresponding video data.

## 1 Introduction

Diffusion models [22; 54; 42; 23; 28; 45] have significantly advanced the quality of text-to-image generation [45; 8; 13], enabling the creation of high-fidelity images and allowing for diverse stylistic variations [3; 2; 1]. The recent development in image diffusion distillation [55; 34; 48; 38; 31] significantly reduces the inference cost by letting distilled student models to perform high-fidelity generation with extremely low sampling steps.

Due to the additional temporal dimension, the sampling time for video diffusion models [21; 24; 9; 62; 19; 65] is considerably longer than that for image diffusion models. However, directly extending image distillation methods to speed up the video diffusion models results in unsatisfactory frameappearance quality. Specifically, unlike image diffusion models that benefit from high-quality image datasets such as LALON , public video datasets [70; 57; 5] often suffer from lower frame quality, including issues like low resolution, significant motion blur, and watermarks. This discrepancy poses a great challenge, as the quality of the training data directly impacts both the teacher generation quality and the distillation process. An example is shown in Fig. 1, where both the teacher and the latent consistency model (LCM) student [38; 66] learn the watermark from the video dataset.

To address this challenge, we propose motion consistency model (MCM), a single-stage video diffusion distillation method that enables few-step sampling and can optionally leverage a high-quality _image_ dataset to improve the video frame quality. We showcase our results in Fig. 1 bottom row and Fig. 6, and illustrate the distillation process in Fig. 2.

To simultaneously achieve the goal of diffusion distillation and frame quality improvement, we build our method upon a video LCM [38; 66] combined with an image adversarial objective [47; 48], due to their proven effectiveness in the two tasks [38; 66; 61; 76; 27], respectively. However, directly combining them presents two essential problems. (1) _Conflicted frame learning objectives_. The LCM aims to learn and represent the characteristics of low-quality video frames, while the adversarial objective pushes the frames toward high-quality image data. This opposition creates conflicts between preserving low-quality visual features and enhancing image quality. (2) _Discrepant training-inference distillation input_. During training, only low-quality video samples are used for LCM distillation, whereas during inference, the input is generated high-quality video samples. These two problems greatly hinders the distillation process and the generated frame quality.

To address the first problem, we propose disentangled motion distillation. Specifically, we disentangle the motion components from the video latent, and only distill the motion knowledge from the teacher. Meanwhile, the image adversarial objective is used exclusively to learn frame appearance. This approach effectively mitigates the conflict between learning objectives by ensuring that motion and appearance are learned independently. For the second problem, we propose mixed trajectory distillation. We simulate the probability flow ordinary differential equation (PF-ODE) trajectory during inference

Figure 1: Qualitative result comparisons with latent consistency model (LCM)  using ModelScopeT2V  as teacher. Our MCM outputs clearer video frames using few-step sampling, and can also adapt to different image styles using additional image datasets. Corresponding video results are in supplementary materials.

and sample generated high-quality videos from this simulated trajectory for distillation. By mixing distillation between low-quality video samples and generated high-quality video samples, we ensure better alignment between training and inference phases, thereby enhancing overall performance.

In summary, our contributions are as follows:

* We propose motion consistency model (MCM), a single-stage video diffusion distillation method that accelerates the sampling process and can optionally leverage an image dataset to enhance the quality of generated video frames.
* We introduce disentangled motion distillation to resolve the conflicting frame learning objectives between video diffusion distillation and image adversarial learning, ensuring better motion consistency and frame appearance.
* We propose mixed trajectory distillation, which simulates the PF-ODE trajectory during inference. By combining low-quality video samples with generated high-quality video samples during distillation, our approach improves training-inference alignment and enhances generation quality.
* We conduct extensive experiments, demonstrating that our MCM significantly improves video diffusion distillation performance. Furthermore, when leveraging an additional image dataset, our MCM better aligns the appearance of the generated video with the high-quality image dataset.

## 2 Preliminaries

**Diffusion models.** Diffusion models  consists of a forward and a reverse process. The forward process corrupts the data progressively by interpolating a Gaussian noise \((0,)\) and the data sample \(_{0}\): \(_{t}=q(_{0},,t)=_{t}_{0}+_{t}\), where \(_{t}\) and \(_{t}\) are pre-defined noise schedule. The reverse process learns a time-conditioned model to gradually removes the noise. Recent methods [56; 37] propose to design numerical solvers of ordinary differential equations (ODE) to reduce the number of sampling steps.

**Consistency model.** Consistency model (CM)  is a new family of generative models that enables few-step sampling. The essence of CM \((_{t},t)\) is the self-consistency property, that maps any point in the same probability flow ODE (PF-ODE) trajectory to a same point. That is, \((_{t},t)=(_{t^{}},t^{})\) for \( t,t^{}[,T]\), where \(\) is a fixed small positive number. The boundary condition \((_{},)=_{}\) ensures the CM always predicts the origin of PF-ODE trajectories. CM is parameterized as the weighted sum of the data sample and a model output \(\) parameterized by \(\):

\[(_{t},t)=c_{}(t)_{t}+c_{}(t)_{ }(_{t},t),\] (1)

where \(c_{}(t)\) and \(c_{}(t)\) are differentiable functions [55; 28] with \(c_{}()=1\) and \(c_{}()=0\).

CM can be learned via consistency distillation (CD), where a teacher model \(\) and an ODE solver \(\) estimates the previous sample in the empirical PF-ODE trajectory, with \(s\) being the step size:

\[}_{t-s}^{}=_{t}-s(_{t},t;).\] (2)

Let \(\) be the student model parameter, and its exponential moving average (EMA) \(^{-}^{-}+(1-)\) be the target model parameter. CD is trained to enforce the self-consistency property, which minimizes the distance between student and target outputs:

\[_{}=_{_{0},,t}[d( _{}(_{t},t),_{^{-}}(}_{t-s}^{},t- s))].\] (3)

Figure 2: Illustration of our motion consistency model distillation process, which not only distills the motion prior from teacher to accelerate sampling, but also can benefit from an additional high-quality image dataset to improve the frame quality of generated videos.

In this paper, we follow previous methods [38; 61] to use Huber loss as the distance function: \(d(_{t},^{}_{t})=_{t}-^{}_{t}\|_{2}^{2 }+^{2}}-\), where \(\) is a threshold hyperparameter.

## 3 Motion consistency model

We present motion consistency model (MCM), a novel video diffusion distillation method designed for accelerating the text-to-video diffusion sampling process. Additionally, MCM can leverage a different image dataset to adjust the frame appearance, _e.g_., frame quality improvement, watermark removal or frame style transfer. An illustration of the objective is shown in Fig. 2.

**Problem formulation.** Given a pre-trained text-to-video diffusion model \(\), a video-caption dataset \(=\{(,c)_{i}\}\), and an image dataset \(=\{m_{i}\}\), our objective is twofold.

1. Distill the _motion_ knowledge from teacher \(\) into a student \(\) using the video dataset \(\), enabling few-step video generation.
2. Adjust the _appearance_ of the generated video frames using the image dataset \(\).

In this paper, we assume that the appearance of video frames in \(\) has lower quality compared to the images in \(\). For convenience, we refer to the appearance of video frames as following a _"low-quality"_ distribution and the images in \(\) as following a _"high-quality"_ distribution.

When the image dataset \(\) consists of frames extracted from the video dataset \(\), this task simplifies into a straightforward video diffusion distillation process.

### Video latent consistency model meets image discriminator

A straightforward way to achieve the aforementioned goal is to adopt a two-stage training strategy , _i.e_., train a 2D backbone with the image dataset, and then temporally inflate the backbone to fine-tune temporal layers on videos. However, not only this approach can be time-consuming, but the disjointed training stages can lead to suboptimal motion modeling and degraded appearance learning. To address this problem, we propose a single-stage, end-to-end learnable approach.

To build a strong baseline, we adopt a video latent consistency model (LCM) \(_{}(,)\) with a frame adversarial objective [47; 48]. The LCM has proven effective in diffusion distillation [38; 66; 39], and the adversarial objective has been widely used for style transfer [76; 27] and diffusion distillation . By combining these two approaches, we aim to achieve both few-step sampling and improved frame appearance control.

For the discriminator \(D()\) used in the adversarial learning, we adopt a pixel-space discriminator following previous methods [47; 48]. However, densely applying the adversarial objective to all frames is computationally expensive, and can lead to degraded temporal smoothness . To mitigate this, drawing inspiration from sparse sampling-based action recognition [52; 64; 63], we apply the adversarial loss only to a set of sparsely sampled frames.

Specifically, let \(}^{}_{0}=_{}(_{t},t)\) represent the LCM-predicted video latent, and \(}^{}_{0}=(}^{}_{0})\) be the decoded video, where \(\) is the latent decoder. We randomly sample \(l\) frames from the video, denoted as \(\{^{,s}_{0,0},...,^{,s}_{0,l}\}\), and apply the adversarial loss to these sampled frames:

\[^{}_{}=-_{_{0},,t} [_{i}D_{}(^{,s}_{0,i})],\] (4)

where \(\) is the learnable parameter in \(D()\). The discriminator is trained to minimize

\[^{}_{}=_{_{0},,t} [(0,1+_{i}D_{}(^{,s}_{0,i}) )]+_{m_{i}}[(0,1-D_{}(m_{i})) ].\] (5)

We omit the gradient penalty term [18; 48] for conciseness. Please find details on the discriminator design in [47; 48] or in Appendix C. Note that the discriminator's objective is to distinguish between the generated video frame and the images from the image dataset \(\), thereby aligning the video frame with the high-quality distribution.

The overall baseline learning objective is a weighted sum of the CD loss (Eq. (3)) and the adversarial loss: \(_{}=_{}+_{} ^{}_{}\), where \(_{}\) is the weighting hyperparameter.

### Disentangled motion consistency distillation

While combining LCM and the adversarial loss is straightforward, their training objectives conflict. The LCM aligns the appearance of the predicted video with the low-quality distribution, while the adversarial objective adjust the appearance to match the high-quality distribution. Moreover, as LCM relies solely on boundary conditions (Sec. 2) to generate clean video frames, it typically leads to blurry frame as \(t\) approaches \(T\)[55; 38; 61]. This blurriness conflicts with the adversarial loss objective, which seeks to produce clean frames at all steps.

To mitigate these conflicts, we propose a disentangled approach: separating the motion presentation from the video latent, and applying the LCM objective exclusively to the motion representation to ensure temporal smoothness. Meanwhile, we use the adversarial objective to learn the frame appearance. This disentanglement allows us to leverage the strengths of both objectives without them interfering with each other. An illustration is shown in Fig. 3 left.

Specifically, we replace the original consistency distillation loss \(_{}\) in \(_{}\) with the following motion consistency distillation (MCD) objective \(_{}\):

\[_{}=_{_{0},,t}[d( (_{}(_{t},t)),( _{^{-}}(}_{t-s}^{},t-s)))],\] (6)

where \(()\) indicates motion representation extraction function. In this paper, we consider the following implementations.

* **Latent difference:** the difference between temporally consecutive frame latents. This was used in action recognition [52; 64; 75] as motion representations.
* **Latent correlation:** a 4D cost volume tensor containing pairwise dot-product between consecutive frame latents, widely used in optical flow estimation [26; 51; 50].
* **Latent low/high-frequency components:** this is inspired by recent work , which shows different frequency components of the latent have different impacts on the motion.
* **Learnable representation:** we apply a two-layer MLP after the LCM output to extract motion (Fig. 4). This MLP does not contain temporal layers, and preserves the motion while avoiding direct applying appearance loss to the latent. The parameters of the target MLP head are updated using exponential moving average, and the MLP is discarded during inference. This design resembles the projection head used in self-supervised learning [17; 11; 12; 10].

We examine all choices in Sec. 4.3, and find that the learnable representation works the best, enabling effective motion consistency modeling while preserving frame appearance quality.

Figure 4: Learnable motion representation.

Figure 3: **Left**: framework overview. Our MCM features disentangled motion-appearance distillation, where motion is learned via the motion consistency distillation loss \(_{}\), and the appearance is learned with the frame adversarial objective \(_{}^{}\). **Right**: mixed trajectory distillation. We simulate the inference-time ODE trajectory using student-generated video (bottom green line), which is mixed with the real video ODE trajectory (top green line) for consistency distillation training.

### Mixed trajectory distillation

While the disentangled motion distillation mitigates the appearance learning objective conflicts, another training-inference discrepancy persists. During training, all ODE trajectories used for MCD are sampled by adding noise to the low-quality videos in \(\) (Fig. 3 top right). In contrast, during inference, the ODE trajectories will be sampled in the high-quality video space. As a result, such discrepancy leads to degraded performance during inference. Furthermore, since we do not have access to high-quality videos, we cannot directly sample ODE trajectories from this distribution for training, making it challenging to resolve this training-inference discrepancy.

To address this problem, we propose to simulate such high-quality video ODE trajectories by using the consistency model multi-step inference process . Specifically, starting from random noise \(_{T}(0,)\), we first run single-step inference to get the high-quality data sample \(}_{0}^{}\), and then add noise to it to sample high-quality ODE trajectories. An illustration is shown in Fig. 3 bottom right. Formally, the starting point \(}_{t}^{}\) for MCD is sampled as:

\[}_{t}^{}=_{t}}_{0}^{}+_{t},}_{0}^{}=_{}(_{T},T) _{T}(0,).\] (7)

In this way, the MCD loss can be formulated by replacing the starting data points from those sampled from low-quality trajectories with those sampled from simulated high-quality video trajectories:

\[_{}^{}=_{_{T},,t} [d((_{}(}_{t}^{},t) ),(_{^{-}}(}_{t-s}^{,},t-s)))],\] (8)

where \(}_{t-s}^{,}=}_{t}^{}-s(}_{t}^{},t;)\). Similarly, the adversarial loss is also applied to the model prediction from the samples on the high-quality trajectories, which we denote as \(_{}^{}\). By applying the MCD and adversarial objectives to the simulated high-quality trajectory, it not only mitigates the training-inference trajectory discrepancy, but also ensures zero terminal signal-to-noise ratio \(_{T}(0,)\) to avoid the data leakage problem  and helps plain video distillation.

In practice, we found that only training from the generated video ODE trajectory potentially leads to mode collapse (Appendix B.1), and mixing the training from real samples and generated samples achieves the best results. Thus, the final loss \(\) is formulated as a weighted sum of the distillation objectives using real samples and generated samples:

\[=_{}(_{}+_{ {adv}}_{}^{})+(1-_{}) (_{}^{}+_{}_ {}^{}),\] (9)

where \(_{}\) is a hyperparameter to balance the distillation in different trajectories.

## 4 Experiments

**Implementation details.** We choose two text-to-video diffusion models for experiments: ModelScopeT2V  and AnimateDiff  with StableDiffusion v1.5 . We use DDIM solver  with 50 sampling steps as the ODE solver \(\). We train LoRA  with rank \(64\) as MCM, following [38; 61]. Following the teacher model setting, we generate \(16\)-frame videos, with resolution 256x256 for ModelScopeT2V, and 512x512 for AnimateDiff. The learning rates for the diffusion model and discriminator are set to \(5e^{-6}\) and \(5e^{-5}\), respectively, with batch size \(128\), Adam optimizer , and \(30k\) training steps. The weight hyperparameters are determined via a grid search:

   &  &  &  \\  & & 1 & 2 & 4 & 8 & 1 & 2 & 4 & 8 \\   & DDIM  & 5228 & 2580 & 1222 & 885 & 20.05 & 20.61 & 24.04 & 28.89 \\  & DPM++  & 2082 & 1142 & 843 & 975 & 22.13 & 24.78 & 29.38 & **30.75** \\  & LCM  (our impl.) & 1242 & 978 & 1006 & 909 & 27.32 & 28.95 & 29.94 & 29.86 \\   & AnimateLCM  & 1575 & 1333 & 998 & 946 & 24.65 & 27.43 & 28.98 & 28.77 \\  & AnimateDiff-1Lightning  & 1288 & 1289 & 1283 & 1355 & 28.29 & 29.07 & 30.01 & 29.69 \\   & MCM (ours) & **1025** & **948** & **287** & **821** & **30.36** & **30.70** & **30.85** & 29.88 \\   & DDIM  & 7231 & 2309 & 1229 & 652 & 20.47 & 20.06 & 23.68 & 28.46 \\  & DPM++  & 2030 & 1142 & 477 & 506 & 22.38 & 24.79 & 29.70 & **31.00** \\   & LCM  (our impl.) & **854** & 658 & 603 & 637 & 27.50 & 29.28 & 30.48 & 30.73 \\   & MCM (ours) & **526** & **450** & **456** & **503** & **29.81** & **30.63** & **30.55** & 29.58 \\  

Table 1: Video diffusion distillation comparison on the WebVid mini validation set.

\(_{}=1\) and \(_{}=0.5\). The experiments are conducted on a machine equipped with 32 H100 GPUs. The project is developed using PyTorch , Diffusers , and PEFT .

**Evaluation metrics.** We use FVD  to measure the video quality, and CLIP  similarity score (CLIPSIM) for video-prompt alignment measurement, where CLIP-ViT-B/16 is used. In Sec. 4.2, we use FID  to measure the frame quality and the similarity to the image dataset.

### Comparisons with distilled video diffusion models

**Experiment settings.** In this section, we compare MCM with other video diffusion models to validate the effectiveness of the disentangled motion-appearance distillation. For fair comparison, we use the WebVid 2M  as both the video and image training dataset, without using any additional image datasets. We postpone the experiments with different image datasets in Sec. 4.2. For testing, we randomly sample 500 validation videos from WebVid 2M (WebVid mini) for in-distribution evaluation; we also follow common practice [62; 16] to use \(\)2900 validation videos from MSRVTT  for zero-shot generation evaluation. We report standard metrics of FVD and CLIPSIM.

**Results.** We compare our method with training-free samplers DDIM  and DPM++ , and trainable models LCM , AnimateLCM  and AnimateDiff-Lightning . We present the quantitative results on WebVid and MSRVTT in Tab. 1 and Tab. 2, respectively. On both datasets, and using both teacher models AnimateDiff  and ModelScopeT2V , our MCM outperforms

   &  &  &  \\  & & 1 & 2 & 4 & 8 & 1 & 2 & 4 & 8 \\   & DDMM  & 4782 & 4350 & 2774 & 933 & 20.90 & 20.94 & 22.87 & 27.36 \\  & DPM++  & 2004 & 1447 & 876 & 794 & 22.93 & 24.5 & 27.62 & 29.10 \\  & LCM  (our impl.) & 1276 & 1180 & 956 & 830 & 25.75 & 27.33 & 28.37 & 28.65 \\   & AnimateLCM  & 1578 & 1278 & 824 & 740 & 27.56 & 28.52 & 29.58 & 27.67 \\  & AnimateDiff-Lightning  & 1260 & 1259 & 892 & 932 & 27.38 & 28.77 & 29.12 & 28.77 \\   & **MCM** (**ours**) & **1197** & **1036** & **801** & **675** & **2895** & **2940** & **2964** & **2913** \\   & DDMM  & 6459 & 2305 & 1445 & 841 & 21.49 & 20.33 & 22.57 & 26.76 \\  & DPM++  & 2039 & 1336 & 467 & 552 & 23.48 & 24.85 & 28.51 & **29.70** \\   & LCM  (our impl.) & 1094 & 820 & 713 & 717 & 26.78 & 28.01 & 28.45 & 29.01 \\   & MCM (ours) & **501** & **434** & **414** & **482** & **28.37** & **29.02** & **28.86** & 28.28 \\  

Table 2: Zero-shot video diffusion distillation comparison on the MSRVTT validation set.

Figure 5: Qualitative comparison of video diffusion distillation with AnimateDiff  as the teacher model. The first and last frames are sampled for visualization. Our MCM produces cleaner frames using only 2 and 4 sampling steps, with better prompt alignment and improved frame details. Corresponding video results are in supplementary materials.

all competing distillation methods in terms of video quality and video-prompt alignment at sampling steps 1, 2, and 4. We compare the qualitative results in Fig. 5, where our method outputs clean video frames using 2 and 4 sampling steps. Compared with AnimateDiff-Lightning , we achieve better prompt-alignment and frame details, demonstrating our effectiveness.

### Video distillation with improved frame quality

**Experiment settings.** In this section, we unleash the full capability of MCM by equipping the model with diverse types of image datasets. We showcase the effectiveness of using image datasets to boost frame quality as well as adapting the model to new styles without requiring corresponding video data. Specifically, we continue to use WebVid 2M  as the video dataset. For image dataset, we

Figure 6: MCM frame quality improvement results using different image datasets with ModelScopeT2V  teacher. The first and last frames are sampled for visualization. Our MCM effectively adapts to different distributions with 4 steps. Corresponding video results are in supplementary materials.

[MISSING_PAGE_FAIL:9]

## 5 Conclusion

We introduce motion consistency model (MCM) to tackle the challenge of low frame quality in video diffusion distillation. Our method leverages disentangled motion distillation to separate motion learning from appearance learning, and mixed trajectory distillation to align training and inference through simulating the inference ODE trajectory. MCM achieves state-of-the-art video diffusion distillation results, and effectively improves frame quality when using a high-quality image dataset. **Broader impacts and limitations**. MCM accelerates and enhances video generation for creative applications. However, as a data-driven approach, our model is sensitive to the distribution and diversity of the training data, which influences model's ability to generate varied video frames, potentially limiting generalization ability. MCM also carries potential risks such as deepfakes creation. Implementing responsible deployment is essential to mitigate these risks.