# Constant Acceleration Flow

Dogyun Park

Korea University

gg933@korea.ac.kr

&Sojin Lee

Korea University

sojin_lee@korea.ac.kr

&Shyeon Kim

Korea University

sh_bs15@korea.ac.kr

Taehoon Lee

Korea University

98hoon@korea.ac.kr

Youngjoon Hong KAIST

hongyj@kaist.ac.kr

Hyunwoo J. Kim Korea University

hyunwoojkim@korea.ac.kr

Corresponding authors.

###### Abstract

Rectified flow and reflow procedures have significantly advanced fast generation by progressively straightening ordinary differential equation (ODE) flows. They operate under the assumption that image and noise pairs, known as couplings, can be approximated by straight trajectories with constant velocity. However, we observe that modeling with constant velocity and using reflow procedures have limitations in accurately learning straight trajectories between pairs, resulting in suboptimal performance in few-step generation. To address these limitations, we introduce Constant Acceleration Flow (CAF), a novel framework based on a simple constant acceleration equation. CAF introduces acceleration as an additional learnable variable, allowing for more expressive and accurate estimation of the ODE flow. Moreover, we propose two techniques to further improve estimation accuracy: initial velocity conditioning for the acceleration model and a reflow process for the initial velocity. Our comprehensive studies on toy datasets, CIFAR-10, and ImageNet 64x64 demonstrate that CAF outperforms state-of-the-art baselines for one-step generation. We also show that CAF dramatically improves few-step coupling preservation and inversion over Rectified flow. Code is available at https://github.com/mlvlab/CAF.

## 1 Introduction

Diffusion models  learn the probability flow between a target data distribution and a simple Gaussian distribution through an iterative process. Starting from Gaussian noise, they gradually denoise to approximate the target distribution via a series of learned local transformations. Due to their superior generative capabilities compared to other models such as GANs and VAEs, diffusion models have become the go-to choice for high-quality image generation. However, their multi-step generation process entails slow generation and imposes a significant computational burden. To address this issue, two main approaches have been proposed: distillation models  and methods that simplify the flow trajectories  to achieve fewer-step generation. An example of the latter is _rectified flow_, which focuses on straightening ordinary differential equation (ODE) trajectories. Through repeated applications of the rectification process, called reflow, the trajectories become progressively straighter by addressing the _flow crossing_ problem. Straighter flows reduce discretization errors, enabling fewer steps in the numerical solution and, thus, faster generation.

Rectified flow  defines the straight ODE flow over time \(t\) with a drift force \(\), where each sample \(_{t}\) transforms from \(_{0}_{0}\) to \(_{1}_{1}\) under a constant velocity \(v=_{1}-_{0}\). Itapproximates the underlying velocity \(\) with a neural network \(_{}\). Then, it iteratively applies the reflow process to avoid flow crossing by rewiring the flow and building deterministic data coupling. However, constant velocity modeling may limit the expressiveness needed for approximating complex couplings between \(_{0}\) and \(_{1}\). This results in sampling trajectories that fail to converge optimally to the target distribution. Moreover, the interpolation paths after the reflow may still intersect--a phenomenon known as flow crossing--which leads to curved rectified flows because the model estimates different targets for the same input. As illustrated in Fig. 0(a), instead of following the intended path from \(_{0}^{1}\) to \(_{1}^{1}\), a sampling trajectory from Rectified flow erroneously diverts towards \(_{1}^{2}\) due to the flow crossing. Such flow crossing can make the accurate learning of straight ODE trajectories more challenging.

In this paper, we introduce the **C**onstant **A**cceleration **F**low (CAF), a novel ODE framework based on a constant acceleration equation, as outlined in (4). Our CAF generalizes Rectified flow by introducing acceleration as an additional learnable variable. This constant acceleration modeling offers the ability to control flow characteristics by manipulating the acceleration magnitude and enables a direct closed-form solution of the ODE, supporting precise and efficient sampling in just a few steps. Additionally, we propose two strategies to address the flow crossing problem. The first one is _initial velocity conditioning_ (IVC) for the acceleration model, and the second one is to employ _reflow_ to enhance the learning of initial velocity. Fig. 0(b) presents that CAF, with the proposed strategies, can accurately predict the ground-truth path from \(_{0}^{1}\) to \(_{1}^{1}\), even when flow crossing occurs. Through extensive experiments, from toy datasets to real-world image generation on CIFAR-10  and ImageNet 64\(\)64, we demonstrate that our CAF exhibits superior performance over Rectified flow and state-of-the-art baselines. Notably, CAF achieves superior Frechet Inception Distance (FID) scores on CIFAR-10 and ImageNet 64\(\)64 in conditional settings, recording FIDs of 1.39 and 1.69, respectively, thereby surpassing recent strong methods. Moreover, we show that CAF provides more accurate flow estimation than Rectified flow by assessing the'straightness' and 'coupling preservation' of the learned ODE flow. CAF is also capable of few-step inversion, making it effective for real-world applications such as box inpainting.

To summarize, our contributions are as follows:

* We propose Constant Acceleration Flow (CAF), a novel ODE framework that integrates acceleration as a controllable variable, enhancing the precision of ODE flow estimation compared to the constant velocity framework.
* We propose two strategies to address the flow crossing problem: initial velocity conditioning for the acceleration model and a reflow procedure to improve initial velocity learning. These strategies ensure a more accurate trajectory estimation even in the presence of flow crossings.
* Through extensive experiments on synthetic and real datasets, CAF demonstrates remarkable performance, especially achieving the superior FID on CIFAR-10 and ImageNet 64\(\)64 over strong baselines. We also demonstrate that CAF learns more accurate flow than Rectified flow by assessing the straightness, coupling preservation, and inversion.

Figure 1: **Initial Velocity Conditioning (IVC). We illustrate the importance of IVC to address the flow crossing problem, which hinders the learning of straight ODE trajectories during training. In Fig. 0(a), Rectified flow suffers from approximation errors at the overlapping point \(_{t}\) (where \(_{t}^{1}=_{t}^{2}\)), resulting in curved sampling trajectories due to flow crossing. Conversely, Fig. 0(b) demonstrates that CAF, utilizing IVC, successfully estimates ground-truth trajectories by minimizing the ambiguity at \(_{t}\).**

## 2 Related work

Generative models.Learning generative models involves finding a nonlinear transformation between two distributions, typically denoted as \(_{0}\) and \(_{1}\), where \(_{0}\) is a simple distribution like a Gaussian, and \(_{1}\) is the complex data distribution. Various approaches have been developed to achieve this transformation. For example, variational autoencoders (VAE) [16; 17] optimize the Evidence Lower Bound (ELBO) to learn a nonlinear mapping from the latent space distribution \(_{0}\) to the data distribution \(_{1}\). Normalizing flows [18; 19; 20] construct a series of invertible and differentiable mappings to transform \(_{0}\) into \(_{1}\). Similarly, GANs [21; 22; 23; 24; 25] earn a generator that transforms \(_{0}\) into \(_{1}\) through an adversarial process involving a discriminator. These models typically perform a one-step generation from \(_{0}\) to \(_{1}\). In contrast, diffusion models [2; 26; 27; 28; 29; 30] propose learning the probability flow between the two distributions through an iterative process. This iterative process ensures stability and precision, as the model incrementally learns to reverse a diffusion process that adds noise to data. Diffusion models have demonstrated superior performance across various domains, including images [12; 31; 32; 33], 3D [34; 35; 36; 37], and video [38; 39; 40].

Few-step diffusion modelsAddressing the slow generation speed of diffusion models has become a major focus in recent research: Distillation methods [3; 4; 5; 6; 7; 8; 9] seek to optimize the inference steps of pre-trained diffusion models by amortizing the integration of ODE flow. Consistency models [6; 7; 8] train a model to map any point on the pre-trained diffusion trajectory back to the data distribution, enabling fast generation. Rectified flow [10; 11; 13] is another direction, which focuses on straightening ODE trajectories under a constant velocity field. By straightening the flow and reducing path complexity, it allows for fast generation through efficient and accurate numerical solutions with fewer Euler steps. Recent methods such as AGM  also introduce acceleration modeling based on Stochastic Optimal Control (SOC) theory instead of relying solely on velocity. However, AGM predicts time-varying acceleration, which still requires multiple iterative steps to solve the differential equations. In contrast, our proposed CAF ODE assumes that the acceleration term is constant with respect to time. Therefore, there is no need to iteratively solve complex time-dependent differential equations. This simplification allows for a direct closed-form solution that supports efficient and accurate sampling in just a few steps.

## 3 Preliminary

**Rectified flow**[10; 13] is an ordinary differential equation-based framework for learning a mapping between two distributions \(_{0}\) and \(_{1}\). Typically, in image generation, \(_{0}\) is a simple tractable distribution, _e.g._, the standard normal distribution, defined in the latent space and \(_{1}\) is the image distribution. Given empirical observations of \(_{0}_{0}\) and \(_{1}_{1}\) over time \(t\), a flow is defined as

\[_{t}}{dt}=(_{t},t),\] (1)

Figure 2: **2D synthetic dataset.** We compare results between 2-Rectified flow and our Constant Acceleration Flow (CAF) on 2D synthetic data. \(_{0}\) (blue) and \(_{1}\) (green) are source and target distributions parameterized by Gaussian mixture models. Here, the number of sampling steps is \(N=1\). While 2-Rectified flow frequently generates samples that deviate from \(_{1}\), CAF more accurately estimates the target distribution \(_{1}\). The generated samples (orange) from CAF form a more similar distribution as the target distribution \(_{1}\).

where \(_{t}=(_{0},_{1},t)\) is a time-differentiable interpolation between \(_{0}\) and \(_{1}\), and \(:^{d}^{d}\) is a velocity field defined on data-time domain. Rectified flow learns the velocity field \(\) with a neural network \(_{}\) by minimizing the following mean square objective:

\[_{}_{_{0},_{1},t p(t)} [\|(_{t},t)-_{}(_{t},t )\|^{2}],\] (2)

where \(\) represents a coupling of (\(_{0},_{1}\)) and \(p(t)\) is a time distribution defined on \(\). The choice of interpolation \(\) leads to various algorithms, such as Rectified flow , ADM , EDM , and LDM . Specifically, Rectified flow proposes a simple linear interpolation between \(_{0}\) and \(_{1}\) as \(_{t}=(1-t)_{0}+t_{1}\), which induces the velocity field \(\) in the direction of \((_{1}-_{0})\), _i.e._, \((_{t},t)=_{1}-_{0}\). This means the Rectified flow transports \(_{0}\) to \(_{1}\) along a straight trajectory with a _constant velocity_. After training \(_{}\), we can generate a sample \(_{1}\) using off-the-shelf ODE solvers \(\), such as the Euler method:

\[_{t+ t}=_{t}+ t_{}( _{t},t), t\{0, t,,(N-1) t\},\] (3)

where \( t=\) and \(N\) is the total number of steps. To achieve faster generation with fewer steps without sacrificing accuracy, it is crucial to learn a straight ODE flow. Straight ODE flow minimize numerical errors encountered by the ODE solver.

Reflow and flow crossing.The trajectories of interpolants \(_{t}\) may intersect--a phenomenon known as flow crossing--due to stochastic coupling between \(_{0}\) and \(_{1}\) (e.g., random pairing of \(_{0}\) and \(_{1}\)). These intersections introduce approximation errors in the neural network, leading to curved sampling trajectories . Our toy experiment, illustrated in Fig. 1a, clearly demonstrates this issue: the simulated sampling trajectories become curved due to flow crossing, rendering one-step simulation inaccurate. To address this problem, Rectified flow  introduces a reflow procedure. This procedure iteratively straightens the trajectories by reconstructing a more deterministic and direct pairing of \(_{0}\) and \(_{1}\) without altering the marginal distributions. Specifically, the reflow procedure involves generating a new coupling \(\) of \((_{0},_{1}=(_{0};_{}^{k}))\) using a pre-trained Rectified flow model \(_{}^{k}\), where \(k\) denotes the iteration of the reflow procedure, and \((_{0};_{}^{k})=_{0}+_{0}^{1} _{}^{k}(_{t},t)dt\). By iteratively refining the coupling and the velocity field, the reflow procedure reduces flow crossing, resulting in straighter trajectories and improved accuracy in fewer steps.

Figure 3: **Sampling trajectories of CAF with different \(h\). The sampling trajectories of CAF are displayed for different values of \(h\), which determines the initial velocity and acceleration. \(_{0}\) and \(_{1}\) are mixtures of Gaussian distributions. We sample across sampling steps of \(N=7\) to show how sampling trajectories change with \(h\).**

Method

We aim to develop a generative model based on the ODE framework that enables faster generation without compromising quality. To achieve this, we propose a novel approach called **Constant Acceleration Flow** (CAF). Specifically, CAF formulates an ODE trajectory that transports \(_{t}\) with a _constant acceleration_, offering a more expressive and precise estimation of the ODE flow compared to constant velocity models. Additionally, we propose two novel techniques that address the problem of flow crossing: 1) _initial velocity conditioning_ and 2) _reflow procedure_ for learning initial velocity. The overall training pipeline is presented in Alg. 1.

### Constant Acceleration Flow

We propose a novel ODE framework based on the constant acceleration equation, which is driven by the empirical observations \(_{0}_{0}\) and \(_{1}_{1}\) over time \(t\) as:

\[d_{t}=(_{0},0)dt+(_{t},t)tdt,\] (4)

where \(:^{d}^{d}\) is the initial velocity field and \(:^{d}^{d}\) is the acceleration field. We abbreviate time variable \(t\) for notation simplicity, _i.e._, \((_{0},0)=(_{0})\), \((_{t},t)=(_{t})\). By integrating both sides of (4) with respect to \(t\) and assuming a constant acceleration field, _i.e._, \((_{t_{1}})=(_{t_{2}}), t_{1},t _{2}\), we derive the following equation:

\[_{t}=_{0}+(_{0})t+(_{t})t^{2}.\] (5)

Given the initial velocity field \(\), the acceleration field \(\) can be derived as

\[(_{t})=2(_{1}-_{0})-2( _{0}),\] (6)

by setting \(t=1\) and the constant acceleration assumption. Then, we propose a time-differentiable interpolation \(\) as:

\[_{t}=(_{0},_{1},t,( _{0}))=(1-t^{2})_{0}+t^{2}_{1}+( _{0})(t-t^{2}),\] (7)

by substituting (6) to (5). Using this result, we can easily simulate an intermediate sample \(_{t}\) on our CAF ODE trajectory.

Learning initial velocity field.Selecting an appropriate initial velocity field is crucial, as different initial velocities lead to distinct flow dynamics. Here, we define the initial velocity field as a scaled displacement vector between \(_{1}\) and \(_{0}\):

\[(_{0})=h(_{1}-_{0}),\] (8)

where \(h\) is a hyperparameter that adjusts the scale of the initial velocity. This configuration enables straight ODE trajectories between distributions \(_{0}\) and \(_{1}\), similar to those in Rectified flow. However, varying \(h\) changes the flow characteristics: 1) \(h=1\) simulates constant velocity flows, 2) \(h<1\) leads to a model with a positive acceleration, and 3) \(h>1\) results in a negative acceleration, as illustrated in Fig. 3. Empirically, we observe that the negative acceleration model is more effective for image sampling, possibly due to its ability to finely tune step sizes near data distribution.

The initial velocity field is learned using a neural network \(_{}\), which is optimized by minimizing the distance metric \(d(,)\) between the target and estimated velocities as

\[_{}_{_{0},_{1},t p(t), _{t}}[d((_{0}),_ {}(_{t}))],\] (9)

where \(p(t)\) is a time distribution defined on \(\). Note that our velocity model learns target initial velocity defined at \(t=0\). This differs from Rectified flow, which learns target velocity field defined over \(t\).

Learning acceleration field.Under the assumption of constant acceleration, the acceleration field is derived from (6) as

\[(_{t})=2(_{1}-_{0})-2( _{0}).\] (10)

We learn the acceleration field using a neural network \(_{}\) by minimizing the distance metric \(d(,)\) as:

\[_{}_{_{0},_{1},t p(t), _{t}}[d((_{t}),_{ }(_{t}))].\] (11)

In Sec. C, we theoretically show that CAF ODE preserves the marginal data distribution.

### Addressing flow crossing

Rectified flow addresses the issue of flow crossing by a reflow procedure. However, even after the procedure, trajectories may still intersect each other. Such intersections hinder learning straight ODE trajectories, as demonstrated in Fig. 0(a). Similarly, our acceleration model also encounters the flow crossing problem. This leads to inaccurate estimation, as the model struggles to predict estimation on these intersections correctly. To further address the flow crossing, we propose two techniques.

Initial velocity conditioning (IVC).We propose conditioning the estimated initial velocity \(}_{}=(_{0})\) as the input of the acceleration model, _i.e._, \(_{}(_{t},}_{})\). This approach provides the acceleration model with auxiliary information on the flow direction, enhancing its capability to distinguish correct estimations and mitigate ambiguity at the intersections of trajectories, as illustrated in Fig. 1. Our IVC circumvents the non-intersecting condition required in Rectified flow (see Theorem 3.6 in ), which is a key assumption for achieving a straight coupling \(\). By reducing the ambiguity arising from intersections, CAF can learn straight trajectories with less constrained couplings, which is quantitatively assessed in Tab. 4.

To incorporate IVC into learning the acceleration model, we reformulate (11) as:

\[_{}_{_{0},_{1},t p(t), _{t}}[d([(_ {t})],_{}(_{t},}_{})) ].\] (12)

where \([]\) indicates stop-gradient operation. Since our velocity model learns to predict the initial velocity (see (9)), we ensure that the model can handle both forward and reverse CAF ODEs, which start from \(_{0}\) and \(_{1}\), respectively. Thus, our acceleration model can generalize across different flow directions, enabling inversion as demonstrated in Sec. B.2.

Reflow for initial velocity.It is also important to improve the accuracy of the initial velocity model. Following , we address the inaccuracy caused by stochastic pairing of \(_{0}\) and \(_{1}\) by employing a pre-trained generative model \(\), which constructs a more deterministic coupling \(\) of \(_{0}\) and \(_{1}\). We subsequently use this new coupling \(\) to train the initial velocity and acceleration models.

### Sampling

After training the initial velocity and acceleration models, we generate samples using the CAF ODE introduced in (4). The discrete sampling process is given by:

\[_{t+ t}=_{t}+ t_{}( _{0})+t^{} t_{}(_{t}, t,_{}(_{0})),\] (13)

where \(N\) is the total number of steps, \( t=,t=i t\), and \(t^{}= t\) where \(i\{0,...,N-1\}\) (See Alg. 2). We adopt \(t^{}\) since it empirically improves accuracy, especially in the small \(N\) regime. Notably, when \(N=1\) (one-step generation), \(t^{}\) simplifies to \(\), leading to the closed-form solution in (5). See Alg. 3 for inversion algorithm.

```
0: velocity model \(_{}\), acceleration model \(_{}\), sampling steps \(N\), \(_{0}\).
1:\(_{0}_{0}\)
2:\(}_{}_{}(_{0})\)
3:for\(i=0\)to\(N-1\)do
4:\(t\)
5:\(t^{}\)
6:\(}_{}_{}(_{t},_ {})\)
7:\(_{t+}_{t}+} _{}+}{N}}_{}\)
8:endfor
9:return\(_{1}\) ```

**Algorithm 2** Sampling process of Constant Acceleration Flow

## 5 Experiment

We evaluate the proposed Constant Acceleration Flow (CAF) across various scenarios, including both synthetic and real-world datasets. In Sec. 5.1, our investigation begins with a simple two-dimensional synthetic dataset, where we compare the performance of Rectified flow and CAF to clearly demonstrate the effectiveness of our model. Next, we extend our experiments to real-world image datasets, specifically CIFAR-10 (32\(\)32) and ImageNet (64\(\)64), in Sec. 5.2. These experiments highlight CAF's ability to generate high-quality images with a single sampling step. Furthermore, we conduct an in-depth analysis of CAF through evaluations of coupling preservation, straightness, inversion tasks, and an ablation study in Sec. 5.3.

### Synthetic experiments

We demonstrate the advantages of the Constant Acceleration Flow (CAF) over the constant velocity flow model, Rectified Flow , through synthetic experiments. For the neural networks, we use multilayer perceptrons (MLPs) with five hidden layers and 128 units per layer. Initially, we train 1-Rectified flow on 2D synthetic data to establish a deterministic coupling. We then train both CAF and 2-Rectified flow. For CAF, we incorporate the initial velocity into the acceleration model by concatenating it with the input, ensuring that the model capacities of both CAF and 2-Rectified flow remain comparable. We set \(d\) as \(l_{2}\) distance. Fig. 2 presents samples generated from CAF in one step and from 2-Rectified flow in two steps. Our CAF more accurately approximates the target distribution \(_{1}\) than 2-Rectified flow. In particular, CAF with \(h=2\) (negative acceleration) learns the most accurate distribution. In contrast, 2-Rectified flow frequently generates samples that significantly deviate from \(_{1}\), indicating its difficulty in accurately estimating straight ODE trajectories. This experiment shows that reflowing alone may not overcome the flow crossing problem, leading to poor estimations, whereas our proposed acceleration modeling and IVC effectively address this issue. Moreover, Fig. 3 shows sampling trajectories from CAF trained with different hyperparameters \(h\). It clearly demonstrates that \(h\) controls the flow dynamics as we intended: \(h>1\) indicates negative acceleration, \(h=1\) represents constant velocity, and \(h<1\) corresponds to positive acceleration flows. Additional synthetic examples are provided in Fig. 6.

### Real-data experiments

To further validate the effectiveness of our approach, we train CAF on real-world image datasets, specifically CIFAR-10 at 32\(\)32 resolution and ImageNet at 64\(\)64 resolution. To create a deterministic coupling \(\), we utilize the pre-trained EDM models  and adopt the U-Net architecture of ADM  for the initial velocity and acceleration models. In the acceleration model, we double the input dimension of first layer to concatenate the initial velocity to the input \(_{t}\) of the acceleration model, which marginally increases the total number of parameters. We set \(h=1.5\) and \(d\) as LPIPS-Huber loss  for all real-data experiments.

**Baselines and evaluation.** We evaluate state-of-the-art diffusion models [1; 2; 7; 28; 29], GANs [22; 23; 24], and few-step generation approaches [6; 7]. We primarily assess the image generation quality of our method using the Frechet Inception Distance (FID)  and Inception Score (IS) . Additionally, we evaluate diversity using the recall metric following [6; 7; 10].

Distillation.Distilling a few-step student model from a pre-trained teacher model has recently become essential for high-quality few-step generation [6; 7; 10; 11]. InstaFlow  has observed that learning straighter trajectories and achieving good coupling significantly enhance distillation performance. Moreover, CTM  and DMD  incorporate an adversarial loss as an auxiliary loss to facilitate the training of the student model. We empirically found that incorporating the adversarial loss alone was sufficient to achieve superior performance for one-step sampling without introducing instability. For training details, please refer to Sec. A.

Cifar-10.We present the experimental results on CIFAR-10 in Tab. 1. Our base unconditional CAF model (4.81 FID, \(N=1\)) significantly improves the FID compared to recent state-of-the-art diffusion models (without distillation), including DDIM  (13.36 FID, \(N=10\)), EDM (37.75 FID, \(N=5\)), and 2-Rectified flow (7.89 FID, \(N=2\)) in a few-step generation (_e.g._, \(N<10\)). We retrained 2-Rectified flow using the official codes of , achieving a slightly better performance than the officially reported performance (12.21 FID) for one-step generation . CAF's remarkable 3.08 FID improvement over 2-Rectified flow (\(N=2\)) highlights the effectiveness of acceleration modeling in a fast generation. Our approach is also effective in class-conditional generation, where the base CAF model (2.68 FID, \(N=1\)) shows a significant FID improvement over EDM (35.54 FID, \(N=5\)) and 2-Rectified flow (3.74 FID, \(N=2\)). Additionally, after adversarial training, CAF achieves a superior FID of 1.48 for unconditional generation and 1.39 for conditional generation with \(N=1\). Lastly, we qualitatively compare the 2-Rectified flow and our CAF in Fig. 4, where CAF generates more vivid samples with intricate details than 2-Rectified flow.

ImageNet.We extend our evaluation to the ImageNet dataset at 64\(\)64 resolution to demonstrate the scalability and effectiveness of our CAF model on more complex and higher-resolution images. Similar to the results on CIFAR-10, our base conditional CAF model significantly improves the FID compared to recent state-of-the-art diffusion models (without distillation) in the small \(N\) regime (_e.g._, \(N<10\)). Specifically, CAF (6.52 FID, \(N=1\)) outperforms models such as DPM-solver  (7.93 FID, \(N=10\)), CT  (11.1 FID, \(N=2\)), and EDM  (55.3 FID, \(N=5\)). This validates that the superior performance of CAF can be effectively generalized to complex and large-scale datasets. Additionally, after adversarial training, CAF outperforms or is competitive with state-of-the-art distillation baselines in one-step generation. Notably, CAF achieves the best FID performance of 1.69, surpassing strong baselines. We also demonstrate one-step qualitative results in Fig. 14.

   Model & \(N\) &  Unconditional \\ FID\(\) \\  & 
 Conditional \\ FID\(\) \\  \\  BigGAN  & 1 & 8.51 & - \\ StyleGAN-Ada  & 1 & 2.92 & 2.42 \\ StyleGAN-XL  & 1 & - & 1.85 \\
**Diffusion/Consistency Models** & & & \\  Score SDE  & 2000 & 2.20 & - \\ DDPM  & 1000 & 3.17 & - \\ VDM  & 1000 & 7.41 & - \\ LSGM  & 138 & 2.10 & - \\ DDIM  & 10 & 13.36 & - \\ EDM  & 35 & 2.01 & 1.82 \\
5 & 5 & 37.55 & 35.54 \\ CT  & 2 & 5.83 & - \\
**Diffusion/Consistency Models – Distillation** & & & \\  Diff-Instruct  & 1 & 4.53 & - \\ DMD  & 1 & 3.77 & - \\ DFNO  & 1 & 3.78 & - \\ TRAT  & 1 & 3.78 & - \\ KD  & 1 & 9.36 & - \\ CD  & 2 & 2.93 & - \\
1 & 3.55 & - \\ CTM  & 2 & 1.87 & 1.63 \\
**Rectified Flow Models** & & & \\ 
2-Rectified Flow  & 2 & 7.89 & 3.74 \\
2-Rectified Flow + Distill  & 1 & 11.81 & 6.88 \\
2-Rectified Flow + Distill  & 1 & 4.84 & - \\
**CAF (Ours)** & 1 & 4.81 & 2.68 \\
**CAF + GAN (Ours)** & 1 & **1.48** & **1.39** \\   

Table 1: Performance on CIFAR-10.

   Model & FID\(\) & IS\(\) & Rec\(\) \\   \\  BigGAN-deep  & 1 & 4.06 & - & 0.48 \\ StyleGAN-XL  & 1 & 2.09 & **82.35** & 0.52 \\
**Diffusion/Consistency Models** & & & \\  DDIM  & 50 & 13.7 & - & 0.56 \\ DDIM  & 10 & 18.3 & - & 0.49 \\ DDPM  & 250 & 11.0 & - & 0.58 \\ DDFM  & 250 & 2.92 & - & 0.62 \\ ADM  & 250 & 2.07 & - & 0.63 \\ EDM  & 79 & 2.44 & 48.88 & **0.67** \\  & 5 & 55.3 & - & - \\  & 20 & 3.42 & - & - \\ DPM-solver  & 10 & 7.93 & - & - \\  & 20 & 3.10 & - & - \\ DEIS  & 10 & 6.65 & - & - \\ CT  & 2 & 11.1 & - & 0.56 \\
**Diffusion/Consistency Models – Distillation** & & & \\  Diff-Instruct  & 1 & 5.57 & - & - \\ DMD  & 1 & 2.62 & - & - \\ TRAT  & 1 & 7.43 & - & - \\ DPNO  & 1 & 7.83 & - & 0.61 \\ PD  & 1 & 15.39 & - & 0.62 \\ CD  & 2 & 4.70 & - & 0.64 \\  & 1 & 6.20 & 40.08 & 0.57 \\ CTM  & 2 & 1.73 & 64.29 & 0.57 \\
**Rectified Flow Models** & & & \\ 
**CAF (Ours)** & 1 & 6.52 & 37.45 & 0.62 \\
**CAF + GAN (Ours)** & 1 & **1.69** & 62.03 & 0.64 \\   

Table 2: Performance on ImageNet \(64 64\).

### Analysis

Coupling preservation.We evaluate how accurately CAF and Rectified flow approximate the deterministic coupling obtained from pre-trained models via a reflow procedure. To analyze this, we first conduct synthetic experiments where the interpolant paths \(\) are crossed, as illustrated in Fig. 4(a). Due to the flow crossing, the sampling trajectory of Rectified flow fails to preserve the ground-truth coupling (interpolation path \(\)), leading to a curved sampling trajectory. In contrast, our CAF learns the straight interpolation paths by incorporating acceleration, demonstrating superior coupling preservation ability.

Moreover, we evaluate the coupling preservation ability on real data from CIFAR-10. We randomly sample 1K training pairs \((_{0},_{1})\) from the deterministic coupling \(\) and measure the similarity between \(_{1}\) and \(}_{1}\), where \(}_{1}\) is a generated sample from \(_{0}\). In other words, we measure the distance between a ground truth image and a generated image corresponding to the same noise. If the coupling is well-preserved, the distance should be small. We use PSNR and LPIPS  as distance measures. The result in Tab. 3 demonstrates that CAF better preserves coupling. In terms of PSNR, CAF outperforms Rectified flow by 3.37. This is consistent with the qualitative result in Fig. 4(b), where \(}_{1}\) from CAF resembles more to \(_{1}\) (ground truth) than \(}_{1}\) from Rectified flow.

Flow straightness.To evaluate the straightness of learned trajectories, we introduce the Normalized Flow Straightness Score (NFSS). Similar to previous works , we measure flow straightness \(\) by the \(L^{2}\)distance between the normalized displacement vector \((_{0}-_{1})\) and the normalized velocity vector \(}_{t}\) as below:

\[=_{_{0},_{1},t}[ \|_{1}-_{0}}{\|_{1}-_{0}\| _{2}}-}_{t}}{\|}_{t}\|_{2}}\|_{2}^{2 }].\] (14)

Here, a smaller value of \(\) indicates a _straighter_ trajectory. We compare \(\) between CAF and Rectified flow using synthetic and real-world datasets, as presented in Tab. 4. For Rectified flow, we use \(}_{t}=_{}(_{t})\), while for CAF, we use \(}_{t}=_{}(_{0})+_{}( _{t})t\). The results show that CAF outperforms Rectified flow in flow straightness.

   Metric & 2-Rectified Flow & CAF (ours) \\  LPIPS \(\) & 0.092 & **0.041** \\ PSNR \(\) & 29.79 & **33.16** \\    
    & Constant & \(v_{0}\) & Reflow \\  & acceleration & condition & procedure \\  A & ✗ & ✗ & ✗ & 378 \\ B & ✗ & ✗ & ✗ & 6.88 \\  C & ✓(h=1.5) & ✗ & ✓ & 3.82 \\ D & ✓(h=1.5) & ✓ & ✓ & **2.68** \\ E & ✓(h=1) & ✓ & ✓ & 3.02 \\ F & ✓(h=0.5) & ✓ & ✓ & 2.73 \\   

Table 4: Flow straightness comparison.

Figure 4: **Qualitative results on CIFAR-10. We compare the quality of generated images from 2-Rectified flow and CAF (Ours) with \(N=1\) and \(10\). Each image \(_{1}\) is generated from the same \(_{0}\) for both models. CAF generates more vivid images with intricate details than 2-RF for both \(N\).**InversionWe further demonstrate CAF's capability in real-world applications by conducting zero-shot tasks such as reconstruction and box inpainting using inversion. We provide implemenetation details and algorithms in Sec. B.2. As shown in the Tab. 6 and 7, our method achieves lower reconstruction errors (CAF: 46.68 PSNR vs. RF: 33.34 PSNR) and better zero-shot inpainting capabilities even with fewer steps compared to baselines. These improvements are attributed to CAF's superior coupling preservation capability. Moreover, we present qualitative comparisons between CAF and the baselines in Fig. 12 and 13, which further validates the quantitative results.

Ablation study.We conduct an ablation study to evaluate the effectiveness of components in our framework under the one-step generation setting (\(N=1\)). We examine the improvements achieved by **1)** constant acceleration modeling, **2)** initial velocity (\(_{0}\)) conditioning, and **3)** the reflow procedure for \(_{0}\). The configurations and results are outlined in Tab. 5. Specifically, A and B correspond to 1-Rectified flow and 2-Rectified flow, respectively. Configurations C to F represent our CAF frameworks, with C being our CAF without IVC. By comparing A,B,C, and F, we demonstrate that all three components in our framework substantially improve the performance. In addition, we analyze the final model across various acceleration scales controlled by \(h\). The performance difference between D and F is relatively small, indicating that our framework is robust to the choice of hyperparameters. Empirically, we observe that configuration F, _i.e._, CAF (\(h=1.5\)) with negative acceleration, achieves the best FID of 2.68. Notably, our CAF without \(_{0}\) conditioning, still outperforms rectified flow (configuration B) by 3.06 FID. This highlights the critical role of _constant acceleration modeling_ in enhancing the quality of few-step generation. Also, we verify the significance of reflowing by comparing configurations A and B, which achieve 378 FID and 6.88 FID, respectively.

## 6 Conclusion

In this paper, we have introduced the Constant Acceleration Flow (CAF) framework, which enhances precise ODE trajectory estimation by incorporating a controllable acceleration variable into the ODE framework. To address the flow crossing problem, we proposed two strategies: initial velocity conditioning and a reflow procedure. Our experiments on toy datasets, real-world dataset demonstrate CAF's capabilities and scalability, achieving state-of-the-art FID scores. Furthermore, we conducted extensive ablation studies and analyses--including assessments of flow straightness, coupling preservation, and real-world applications--to validate and deepen our understanding of the effectiveness of our proposed components in learning accurate ODE trajectories. We believe that CAF offers a promising direction for efficient and accurate generative modeling, and we look forward to exploring its applications in more diverse settings such as 3D and video.

Figure 5: **Experiments for coupling preservation. (a) We plot the sampling trajectories during training where their interpolation paths \(\) are crossed. Due to the flow crossing, RF (top) _rewires_ the coupling, whereas CAF (bottom) _preserves_ the coupling of training data. (b) CAF accurately generates target images from the given noise (_e.g._, a car from the car noise), while RF often fails (_e.g._, a frog from the car noise). LPIPS  values are in parentheses.**