# Online robust non-stationary estimation

Abishek Sankararaman

Balakrishnan (Murali) Narayanaswamy

{abisanka, muralibn}@amazon.com,

Amazon Web Services, Santa Clara CA, USA.

###### Abstract

The real-time estimation of time-varying parameters from high-dimensional, heavy-tailed and corrupted data-streams is a common sub-routine in systems ranging from those for network monitoring and anomaly detection to those for traffic scheduling in data-centers. For estimation tasks that can be cast as minimizing a strongly convex loss function, we prove that an appropriately tuned version of the clipped Stochastic Gradient Descent (SGD) is simultaneously _(i)_ adaptive to drift, _(ii)_ robust to heavy-tailed inliers and arbitrary corruptions, _(iii)_ requires no distributional knowledge and _(iv)_ can be implemented in an online streaming fashion. All prior estimation algorithms have only been proven to posses a subset of these practical desiderata. A observation we make is that, neither the \(()\) learning rate for clipped SGD known to be optimal for strongly convex loss functions of a _stationary_ data-stream, nor the \((1)\) learning rate known to be optimal for being adaptive to drift in a _noiseless_ environment can be used. Instead, a learning rate of \(T^{-}\) for \(<1\) where \(T\) is the stream-length is needed to balance adaptivity to potential drift and to combat noise. We develop a new inductive argument and combine it with a martingale concentration result to derive high-probability under _any learning rate_ on data-streams exhibiting _arbitrary distribution shift_ - a proof strategy that may be of independent interest. Further, using the classical doubling-trick, we relax the knowledge of the stream length \(T\). Ours is the first online estimation algorithm that is provably robust to heavy-tails, corruptions and distribution shift simultaneously. We complement our theoretical results empirically on synthetic and real data.

## 1 Introduction

Technology improvements have made it easier than ever to collect diverse telemetry at high resolution from any cyber or physical system, for both monitoring and control . This in turn has led to a data deluge, where large amounts of data must be processed at scale . Given the scale and velocity of the data sources, offline processing to make predictions and decisions is computationally cumbersome. For real-time applications such as performance monitoring and anomaly detection, offline/batch processing results in stale predictions . This necessitates computationally cheap, online algorithms that make predictions and decisions on _high-dimensional_ streaming data . Further in many applications, the challenge of being restricted to online algorithms is exacerbated by _heavy-tails_, _distribution shift_ and _outliers/anomalies_ in the data-stream . In practice, although several heuristics to circumvent these issues have been designed , there is no systematic study of the impact of these challenges on the achievable statistical accuracy. Motivated by these problems, we study algorithms for high-dimensional online statistical estimation and rigorously establish performance guarantees which exhibit graceful degradation with distribution shift and outliers in the data-stream.

### Problem Setup

The online estimation problem we study is modeled as an interaction between an oblivious adversary and the estimator. The adversary before the interaction begins makes two choices - _(i)_ a sequence of probability distributions \(_{1},,_{T}\) on \(^{d}\) and _(ii)_ a sequence of measurable _corruption functions_\((_{t})_{t=1}^{T}\) where for every \(t[T]\)1, \(_{t}:^{d t}^{d t-1} ^{p t-1}^{d}\), whose roles we explain below.

**Sequential interaction protocol** : At each time \(t[T]\), the vector \(X_{t}:=Z_{t}+C_{t}\), where \(Z_{t}_{t}\) is sampled independently and \(C_{t}:=_{t}((Z_{s})_{s t},(C_{s})_{s<t},(_{s})_{s<t})\) is the corruption vector computed using the corruption function based on the _past random samples and outputs_. \(X_{t}\) is shown as input to the estimation algorithm. _Subsequently after observing \(X_{t}\)_, the estimation algorithm outputs an estimate \(_{t}\) and incurs loss \(l_{t}:=\|_{t}-_{t}^{*}\|^{2}\). Here, \(:^{d}^{p}\) is a strongly convex loss function, \(^{p}\) a closed convex set denoting the parameter space, and \(_{t}^{*}:=_{}_{Z_{t}}[ (Z,)]\) is the unique3 optimizer of the loss function with respect to the distribution \(_{t}\), chosen by the adversary at time \(t\). For every time \(t\), we denote this minimizer \(_{t}^{*}\) as the _true parameter_ at time \(t\). The incurred loss \(l_{t}\) is _un-observed_ by the algorithm since the true parameter \(_{t}^{*}\) is unknown.

Formally, an estimation4 algorithm \(:=(_{t})_{t=1}^{T}\) is a sequence of measurable functions such that for all \(t[T]\), the estimate output \(_{t}:=_{t}(X_{t},,X_{1})\), where \(_{t}:^{d t}^{p}\) is a function of all inputs upto and including time \(t\). After outputting an estimate \(_{t}\), the algorithm incurs an un-observed loss which is the L2 distance between the estimate and the true parameter.

**Causal property of the adversary:** Recall that at each time \(t\), the corruption vector is given by \(C_{t}=_{t}((Z_{s})_{s t},(C_{s})_{s<t},(_{s})_{s<t})\), which is based only on the _past realizations_\(Z_{1},,Z_{t}\), \(C_{1},,C_{t-1}\) and the estimator's past outputs \(_{1},,_{t-1}\). The adversary does not have access to future randomness \(Z_{t+1},,Z_{T}\) while choosing the corruption vector \(C_{t}\). This model of corruptions is _stronger_ than those used in online robust estimation (cf. , ) since in our model, the adversary can choose the time instants of corruption while in previous models, the time instants of corruption are chosen randomly through i.i.d. coin flips. Our model of corruption locations are motivated by empirical observations that corruptions rarely occur randomly and are typically bunched [23; 42]. Thus we assume that the adversary has causal power to choose the location of corruptions rather than corruptions occuring at random instants.

**Regret as a Performance Measure:** The regret of the algorithm denoted by \(_{T}\) is defined as

\[_{T}=_{t=1}^{T}l_{t}=_{t=1}^{T}\|_{t}-_{t}^{*}\|,\]

Figure 1: Figures \((a)\) and \((b)\) show a stream of independent samples (with some corruptions) from unit-variance Gaussian and Pareto distribution with shape parameter \(2.001\) respectively when the underlying true mean changes with time. Figure \((c)\) shows that clipped-SGD with clip-value of \(5\) incurs lower regret in estimating the true time-varying mean when the learning rate is set to \(1/\) as opposed to the standard \(1/(t+1)\) known to be optimal in a stationary environment.

the total cumulative loss. This definition is known as _clean dynamic regret_, since _(i)_\(_{t}\) is compared against the true parameter \(_{t}^{*}\), _even if the input \(X_{t}\) at time \(t\) is corrupted_, i.e., even if \(C_{t} 0\), and _(ii)_\(_{t}\) is compared to the time-varying optimal \(_{t}^{*}\). Technically the regret of an algorithm \(\) is a random variable that is a function of the probability measures \((_{t})_{t=1}^{T}\) and the corruption functions \((_{t})_{t=1}^{T}\) and denoted as \(_{}^{()}((_{t})_{t=1}^{T},( _{t})_{t=1}^{T})\). We will denote the regret as \(_{T}\) whenever it is clear from context.

We now define the data-stream complexity parameters impacting regret : _(i)_ distribution shift and (ii) corruptions.

**Distribution Shift5:** The cumulative distance between successive optimum points \(_{T}:=_{t=2}^{T}\|_{t}^{*}-_{t-1}^{*}\|\) measures the complexity of distribution-shift on the data-stream. This is a natural measure since if the estimator outputs \(_{t}\) at time \(t\), will incur an \(L2\) loss \(l_{t}=\|_{t}-_{t}^{*}\|\).

**Corruptions:** The corruption measure is the count \(_{T}:=_{t=1}^{T}(C_{t} 0)\), of the number of times the random sample \(Z_{t}\) is corrupted before inputting to the estimator. The count \(_{T}\) is a random-variable since the vector \(C_{t}\) is a deterministic function of past random variables.

With this setup, the main desiderata we seek of an estimator is that it _(i)_ is **online** : \(O(d)\) run-time per data point on a stream and \(O(d)\) overall memory, _(ii)_ is **free of distributional knowledge**, i.e., does not require bounds on moments of \((_{t})_{t 1}\), distribution shift or corruptions, _(iv)_ attains sub-linear in \(T\) regret even when the distributions \((_{t})_{t 1}\) are **heavy-tailed**, and _(v)_**exhibits graceful degradation** of regret with respect to distribution shift and corruptions on the data stream.

### Main Contributions

1. **Problem formulation and lower bounds**: We formalize the problem of online estimation under drift and corruptions and define a dimension-free target regret benchmark in Equation (1) that captures all the aforementioned desiderata. We give lower bounds in Propositions 16.1 and 2.6 that shows the necessity of assumptions such as finite diameter for the set \(^{d}\) in which the unknown parameters \(_{t}^{*}\) lie. Further discussion in Table 1 and in Section 11.
2. **High-probability upper bounds**: Theorem 5.1 proves that a tuned version of clipped-SGD achieves all aforementioned desiderata. No previous algorithm achieves all the desiderata simultaneously (Tables 1 and 2). Further in Theorem 4.3, we show that if in addition the data-stream is sub-gaussian, then our bound is optimal in the sense when instantiated with special cases of having no drift and corruptions  or in the setting of no corruptions and \(0\) variance , it recovers known optimal results.
3. **Algorithmic insights**: Our key finding is that neither the learning rate of \((1)\) known to be optimal for SGD to adapt to distribution shift in the absence of noise, nor the \((1/t)\) learning rate known to be optimal in a stationary environment can be used. Instead a learning rate of \(T^{-}\) for \(<1\) is needed to combat noise and be adaptive to drift simultaneously. See also Figure 1. We show a lower bound in Proposition 6.1 that this insight is fundamental and not an artifact of our analysis. Knowledge of \(T\) is relaxed using the doubling trick in Section 14 in the Appendix. Our result improves prior state of art in  that show that in the absence of corruptions, SGD _without clipping_ can give sub-linear regret bounds _holding in expectation_. However, in the presence of heavy-tailed noise, high probability guarantees are more insightful compared to only in expectation . Our algorithmic contribution is that a tuned version of _clipped_ SGD can give high probability sub-linear regret guarantees, that hold even in the presence of non-stationarities and corruptions.
4. **Technical contributions in the proof techniques**: From a technical perspective, we employ novel proof techniques to derive high-probability under _any learning rate_ on data-streams exhibiting _arbitrary distribution shift_. The prior state-of-art analysis in  are limited to the specialized setting when the learning rate is \(_{t}=1/(t+1)\) and the data does not exhibit drift. To be adaptive to drift requires newer techniques even in the case of sub-gaussian tails to achieve dimension-free bounds (cf. Section 4.1). We used an induction based technique to apply martingale concentrations in Lemma 19.9. We build upon this technique using contradiction arguments in Lemma 20.10 to extend to the heavy-tailed case. These arguments are of independent interest to prove bounds for other iterative algorithms.

### Motivating application

Online Anomaly Detection (AD) problems consist of a loss function \((,)\) and parameter \(_{t}\) that varies with time \(t\), such that for the input \(X_{t}\) received at time \(t\), the algorithm outputs anomaly score \(S_{t}:=(X_{t},_{t})\) (, ). The objective is to output lower scores for samples that are not corrupted, i.e., if at time \(t\) the data distribution is \(Z_{t}_{t}\), then the optimal anomaly score \(S_{t}^{*}:=(X_{t},_{t}^{*})\) is the one produced by the model minimizing the average anomaly score i.e., \(_{t}^{*}_{}_{Z_{t}}[ (Z,)]\). The distance \(\|_{t}-_{t}^{*}\|\) between \(_{t}\) the model used at time \(t\) and the optimal \(_{t}^{*}\) is a measure of AD performance degradation [32; 50], motivating our study of low regret algorithms.

Special cases of such problems include **robust online mean estimation** and **robust online linear regression**, both of which we consider in simulations in Section 7. The online mean estimation corresponds to \((X,):=\|X-\|_{2}^{2}\) since \(_{}_{Z}[\|Z-\|^{2}]=_{Z }[Z]\). Online linear regression corresponds to splitting the input \(X^{d_{1}+d_{2}}\) as \(X:=(X^{(1)},X^{(2)})\), with \(X^{(1)}^{d_{1}}\) and \(X^{(2)}^{d_{2}}\) and using the _re-construction loss_\((X,):=\|X^{(2)}-^{T}X^{(1)}\|_{2}^{2}\) as anomaly score. The linear regression setting of detecting anomalies by reconstructing one half of the input \(X\) from the other is popularly known as self-supervised AD models . Empirically, in , \((X,):=\|(_{d}-)X\|\) with \(^{d d}\) a matrix of rank \(k<d\) and \(_{d}^{d d}\) the identity matrix is used to detect anomalies. The linear model of  is extended to a non-linear setting in , using an auto-encoder . In , a tree density model for \((X,)\) is studied. These papers' focus is empirical and do not give statistical guarantees.

**Organization of the Paper:** In Section 2 we state the target benchmark regret along with the assumptions under which we seek it and appropriate lower bounds. In Section 3 we give the clipped-SGD algorithm. In Section 4, we give results under the additional assumption of sub-gaussian tails. This helps build intuition and proof techniques for the general result which we give in Section 5. We state implications of our results in Section 6, provide empirical evidence in Section 7, discuss related work in Section 8 and conclude with open problems in Section 9.

## 2 Formalizing the desiderata

Before formalizing the desiderata in Equation (1), we state some mild assumptions standard in the study of statistical estimation under which we seek guarantees [55; 19].

### Model Assumptions

**Assumption 2.1** (Strong convexity).: There exists \(0<m M<\)_known_ to the algorithm, such that for all \(t\), the function \(_{Z_{t}}[(Z,)]\) is \(M\) smooth and \(m\) strongly convex.

This is a benign assumption that essentially states that properties of the loss function such as the convexity and smoothness are known to the estimation algorithm.

**Setting** & **Best lower bound** & **Best upper bound** \\   Stationary, no-corruptions & \([(()}\) & \([(()}\) \\ (\(_{T}=0,_{T}=0\)) & \(+()(1/)}\))]  & \(+()(T/)}\))  \\   Stationary, corruptions & Prop 2.6: \((_{T})\) & Thm 5.1: \(}(T^{}^{2}+T^{} _{T})\) \\ \((_{T}=0,_{T} 0)\) & & \\   Non-stationary, no-corruptions & : \((T^{2/3}_{T}^{1/3})\) & Thm 5.1: \(}(T^{}_{T}+T^{}^{2})\) \\ \((_{T} 0,_{T}=0)\). &  gives bound in expectation only. \\  Non-stationary, corruptions & Prop 16.1: \((_{T})\) & Thm 5.1: \(}(T^{}_{T}+T^{}^{2}+T^{ }_{T})\) \\ \((_{T} 0,_{T} 0)\) & & \\ 

Table 1: Regret bounds for online estimation with heavy-tailed data and strongly convex loss. The setting of \(_{T}=0,_{T}=0\) (line \(1\)) is characterized (upto log factors) in  and . The first dimension free, high probability upper bounds are established in this paper for all other settings. For the setting of \((_{T} 0,_{T}=0)\),  gives a lower bound and an upper bound holding in expectation, while ours is the first high-probability upper bound. Except line \(1\), there is a gap between known lower and upper bounds, closing which is an open problem (see also Section 9).

**Assumption 2.2** (Convex domain).: The algorithm _knows_ a closed convex set \(^{p}\) such that for all \(t\), \(_{t}^{*}:=_{}_{Z_{t}}[ (Z,)]\) is the true parameter to be estimated at time \(t\).

**Assumption 2.3** (Finite diameter).: The diameter \(:=_{x,y}\|x-y\|\) of \(\) is finite, i.e., \(<\).

This is a necessary assumption when \(_{T}>0\) (Proposition 2.6). If \(_{T}=0\), we relax this assumption in Corollaries 4.5 and 5.4. Let \(_{t}()=_{Z_{t}}[(Z,)]\), denote the population risk at time \(t\).

**Assumption 2.4** (Known finite upper bound on the true gradient).: There exists a _known_ finite \(G<\) such that \(_{}\|_{t}()\| G\).

This is necessary for many online optimization algorithms .

**Assumption 2.5** (Existence of second moment).: There exists a matrix \( 0\), _unknown_ to the algorithm such that for all \(t\) and \(\), the covariance of the random-vector \((Z,)\) is bounded by \(\), i.e., \(_{Z_{t}}[((Z,)-_{t}())((Z,)-_{t}( ))^{T}]\) holds for all \(t\) and \(\).

The class of distributions admissible by this assumption is large including all sub-gaussian distributions and heavy-tailed distributions that do not have finite higher moments . Observe that no parameteric assumptions on the distribution family is made. Throughout, we denote by \(^{2}\) and \(_{max}()\) as the trace and the largest eigen-value of \(\) respectively. For a positive semi-definite matrix \(^{p p}\), \(()\) is the set of all probability measures satisfying assumptions 2.1, 2.4 and 2.5.

### Target benchmark for algorithm design

We say that an estimator is _free of distributional knowledge_ if it does not need as input bounds on either the moments of \(_{t}\) nor on the stream complexity \(_{T}\) and \(_{T}\). Our key focus - formalized below in Equation (1) - is: _can we design an online algorithm without distributional assumptions that has sub-linear regret and that degrades gracefully with drift and corruption?_

\[ without distributional knowledge, such that }\,l,m,n<1\, 0^{p p}, \,(_{t})_{t=1}^{T}()(_{t})_{t=1}^{T}}\] (1)

holds with probability at-least \(1-\)?

 Methods & Regret & Moment & Drift & Corruption & Free of distributional knowledge \\   & & assumed & tolerance & tolerance & knowledge \\  
 & \(()\) & 2 & 0 & 0 & ✗ \\ 
 & \(( T^{1/p})\) & \(p(1,2]\) & 0 & 0 & ✓ \\ 
 & \((+\) & & & \\  & \((dT)^{1/4}_{T}^{1/2})\) & 2 & 0 & \((T)\) & ✗ \\  Corollary. 4.8 & \((T^{2/3}_{T}+T^{2/3}\) & sub- & & \\ (This paper) & \(+T^{}^{}_{T})\) & gaussian & \((T^{})\) & ✗ \\  Theorem 4.3 & \((T^{}_{T}+T^{}\) & sub- & & \\ (This paper) & \(+T^{}^{}_{T})\) & gaussian & \((T^{1-})\) & \((T^{})\) & ✗6  \\  Corollary 5.3 & \((T^{2/3}_{T}+T^{5/6}^{2}\) & 2 & \((T^{1/3})\) & \((T^{1/4})\) & ✓ \\ (This paper) & \(+T^{}^{1/2}_{T})\) & 2 & \((T^{1/3})\) & \((T^{1/4})\) & ✓ \\  Theorem 5.1 & \((T^{}_{T}+T^{}^{2}\) & 2 & \((T^{})\) & \((T^{})\) & ✓ \\ (This paper) & \(+T^{}^{}_{T})\) & & \\  

Table 2: Comparison of high-probability regret bounds of online estimation algorithms of strongly convex functions under various moment assumptions.  requires distance of the initial point to the unknown optimum. Theorem 4.3, Corollary 4.8,  and  requires a bound on \(\). Further,  is not dimension-free as regret depends on \(d\). We do not include  as their regret bounds only hold in expectation and not with high probability.

Throughout, we use \(()\) to hide poly-logarithmic factors. Our main result is Theorem 5.1, where we prove that tuned clipped SGD achieves this, and our other desiderata.

### Why is Equation (1) a good benchmark for regret bounds with drift and corruption?

The benchmark **demands robustness to heavy-tailed data,** since regret is sub-linear for any distribution with finite second moment. The benchmark **is dimension free**, since dimension terms \(d,p\) do not appear in Equation (1). The finite diameter \(\) only affects the bound due to corruptions and _not_ the other terms. In particular, if \(_{T}=0\) almost-surely, then the benchmark seeks a valid bound even if \(=\). In general when \(_{T}>0\) however, dependence on \(\) cannot be avoided as we show in Proposition 2.6 below (proof in Section 16).

**Proposition 2.6** (**Finite diameter is necessary even in the absence of drift**).: _There exists a strongly convex loss function \(\) such that for every \(>0\) and \(d\), domain \(=\{x^{d}:\|x\|\}\) such that for every \(T\) and every algorithm, there exists a probability measure \(\) on \(^{d}\) from which the un-corrupted samples are drawn from (i.e., \(_{T}=0\)) and a sequence of corruption functions \((_{t})_{t=1}^{T}\), such that the regret is at-least \((_{T})\) with probability \(1\)._

Thus, even if there is no drift, finite diameter assumption cannot be avoided. Proof is given in Section 16, relies on the modeling asumption that the corruption locations can be arbitrary. This lower bound is in contrast to prior work  which shows that finite diameter is not necessary in the absence of corruptions and when the time instants of corruptions are random. Further in Table 1, we compare the best known lower and upper bounds for the various settings in the presence and absence of distribution shifts and corruptions. As can be seen, in all cases involving either distribution shift or corruptions, our work is the first to give high-probability estimation regret bounds.

**Equation (1) gives drift and corruption tolerance as additional performance measures:** Since the regret explicitly depends on \(_{T}\) and \(_{T}\) we can define the _drift tolerance_ and _corruption tolerance_ as performance measures of the algorithm. Drift (Corruption) tolerance is the largest \(_{T}\) (largest \(_{T}\)) for which we still guarantee sub-linear in \(T\) regret. Thus, if Equation (1) is satisfied by an algorithm, then the drift and corruption tolerance is \((^{1-l})\) and \((T^{1-n})\) respectively. Thus, higher these tolerances, the better an algorithm is as they indicate that the algorithm's degradation with drifts and corruptions are more graceful. Our method is the only one to have non-zero drift tolerance (Table 2).

## 3 The clipped SGD Algorithm

In this section, we formally describe the algorithm in Algorithm 1 that achieves the desiderata. Algorithm 1 produces an estimate at time \(t\) is given by \(_{t}_{}(_{t-1}-_{t}( (X_{t},_{t-1},))\), where \(_{}\) is the projection operator on to \(\) and \((x,c):=(1,c/\|x\|)x,\  x^{d},c 0\).

```
1:Input:\((_{t})_{t 1}\), \(>0\), \(_{0}\), \(T\) time-horizon
2:for each time \(t=1,2,,\)do
3: Receive sample \(X_{t}\)
4: Output \(_{t}_{}(_{t-1}-_{t}( (X_{t},_{t-1}),))\)
5:endfor ```

**Algorithm 1**Clipped-SGD

**Intuition for why Algorithm 1 can achieve Equation (1)**: If there is no distribution shift or corruptions, clipped SGD with appropriate learning rate converges to the true parameter . Now, if there are corruptions, clipping _limits_ the impact on the overall regret. On the other hand, when a distribution shift occurs, the dynamics of Algorithm 1 is equivalent to restarting estimation under the new distribution, which will converge to the true parameter of this shifted distribution .

## 4 Regret bounds when the gradient vectors have sub-gaussian tails

Before giving the general result, we consider the special case of sub-gaussian distributions in this section. We do so because _(i)_ the additional structure of sub-gaussian distributions enable us to prove stronger results in Corollaries 4.6 and 4.7 in the sequel, and _(ii)_ the proof techniques from this special case enables us to build towards the proof of the general setting.

**Definition 4.1** (Sub-gaussian random vectors, ).: A random vector \(Y^{d}\) with co-variance matrix \(\) is said to be sub-gaussian, if for all \(>0\) and \(u^{d}\) with \(\|u\|=1\), \([e^{ Y,u}] e^{_{max}( )}{2}}\), where \(_{max}()\) is the highest eigen-value of \(\).

**Assumption 4.2** (Sub-gaussian assumption with known upper bound).: For every \(t\), and \(\), the \(0\) mean random vector \((Z,)-_{t}()\), where \(Z_{t}\) is sub-gaussian with covariance matrix upper-bounded in the positive semi-definite sense by an _known_ positive-semi-definite matrix \(\).

The following is the main result of this section.

**Theorem 4.3** (Informal version of Theorem 18.1).: _Suppose Assumption 4.2 holds. For every \((0,1)\), if Algorithm 1 is run with constant step-size \(=}\) for \(\) and clipping value \( G++(}()( )})\), then with probability at-least \(1-\),_

\[_{T}T^{ }+_{T}(()( )}+)}{m}}}_{}+\\ }}{m}(( )()}+)}_{}+T^{}}}_{}.\] (2)

The main achievement in Theorem 4.3 is in explicitly identifying how the choice of \(\) trade-offs the regret contributions from distribution shift, finite sample error and corruptions.

_Remark 4.4_ (**Corruption and drift tolerance**).: When Theorem 4.3 is instantiated with \((0,1)\), Equation (2) yields a drift tolerance of \((T^{1-})\) and corruption tolerance of \((T^{1-})\). In particular, instantiating with \(=2/3\) yields \((T^{1/3})\) and \((T^{2/3})\) corruption and drift tolerance respectively.

We now read off several corollaries from this result.

**Corollary 4.5** (**Finite diameter assumption can be relaxed in the absence of corruptions**).: _Let \(_{T}=0\) almost-surely, i.e., there are no adversarial corruptions. Then, under the settings of Theorem 4.3 even when the set \(\) is unbounded, the regret bound given in Equation (2) holds._

**Corollary 4.6** (**Optimal in the stationary case**:).: _Under the assumptions of Theorem 4.3, if \(_{T}=_{T}=0\), then when Algorithm 1 is run with parameters \(=+\) and \(=1/mT\), the regret bound \(_{T}(}{m}(()(T/ )}+))\) holds with probability at-least \(1-\)._

This corollary recovers the well known result  of convergence of SGD on strongly convex functions with sub-gaussian gradients.

**Corollary 4.7** (**Optimal in the noiseless setting**).: _Under the assumptions of Theorem 4.3, if \(_{T}=0\), and \(=0\), i.e., there is no noise, then running Algorithm 1 with \(=+\) and \(=1/m\) obtains regret bound of \(_{T}=(_{T})\)._

This result matches the lower bound in the noise-less setting  and recovers the upper-bound by previous analysis of online clipped-SGD specialized to the noiseless setting .

**Corollary 4.8** (**Special case of \(=2/3\)**).: _When Theorem 4.3 is instantiated with \(=2/3\), then w.p. at-least \(1-\), a regret bound of \(_{T}((T^{1/3}_{T}+}{m}+T^{1/3} _{T})(,G,(T/)))\)._

_Remark 4.9_ (**The excess risk can be bounded using smoothness**).: Oftentimes, the regret is also measured through the _excess risk_, i.e., \(_{t=1}^{T}_{Z_{t}}[(Z,_{t})- (Z,_{t}^{*})]\). Since we assume that \(\) is \(M\) smooth (Assumption 2.1), Theorem 4.3 also bounds the excess risk regret by observing that \(_{Z_{t}}[(Z,_{t})-(Z, _{t}^{*})]\|_{t}-_{t}^{*}\|^{2}\).

### Proof sketch for Theorem 4.3 and technical innovations

The full proof is Appendix 18. We first establish due to the condition on \(\) that if an input sample is not corrupted, then the gradient will never be clipped. Then in Lemma 19.6, we expand the one-steprecursion of the clipped SGD updates by exploiting the strong-convexity. In order to account for the effect of corruptions, we expand the recursion differently if a time-step is corrupted or not. If a time-step is corrupted, then we accumulate both a _bias_ term of norm at-most \(\) and an appropriate _variance_ term. If one were only interested in bounds _in expectation_, then the proof will follow the standard clipped SGD analysis as the expectation of the variance terms are \(0\).

To prove a high probability bound, a natural approach is to apply martingale concentrations to bound the variance terms as done in _in the absence of drifts and corruptions_. A naive way to apply martingale concentrations would be to bound the variance error terms due to drifts and corruption by _the worst case error_ by using the finite diameter \(\). However, this _will not lead to the dimension free bound_ of Theorem 4.3 --instead will lead to a bound in where the finite diameter \(\) will also multiply the regret term due to finite sample error. In particular, this approach _will not_ result in Corollary 4.5 of being able to relax the finite diameter assumption in the absence of corruptions.

We circumvent this issue by using an _inductive_ argument to bound the variance terms in Lemma 19.9. Concretely, we prove the induction hypothesis that for each time \(t\), error terms until time \(t\) is bounded by an appropriate function of the drifts and corruptions. To establish the induction for a time \(t\), we condition on the event the hypothesis holds till time \(t-1\), and employ martingale concentration to show the error at time \(t\) is small. Then we plug the martingale bound back into the one-step recursion and show the induction hypothesis also holds at time \(t\). To get the final un-conditional result, we un-roll all the conditional events by an union bound.

## 5 Regret bounds in the general heavy-tailed case

The following is the main result of the paper, where we relax Assumption 4.2 and thus making the algorithm free of distributional knowledge and allowing for potentially heavy-tailed data.

**Theorem 5.1** (Informal version of Theorem 20.1).: _When Algorithm 1 is run with clip value \(=2GT^{}\), and step-size \(=}\) for \(\), then for every \((0,1)\),_

\[_{T}(}_{T}}{m^{3/2}}++}}}{m^{3/2}}}_{}+ }(G)^{2}()}{m}}_{}+}_{T}G^{2}}}{m}}_{} ),\] (3)

_holds with probability at-least \(1-\)._

The achievement in Theorem 5.1 is in explicitly identifying how the choice of \(\) trade-offs the regret contributions from distribution shift, finite sample error and corruptions, despite heavy-tailed data.

_Remark 5.2_ (**Price for relaxing assumption 4.2 is sub-optimal bound in the stationary case:**).: The setting in Theorem 5.1 is weaker compared to that of Theorem 4.3 since _(i)_ no sub-gaussian tail assumptions are made, and _(ii)_ no knowledge of the upper-bound matrix \(\) is assumed. The price for relaxing these assumptions is a weaker regret, specifically in the term due to finite-sample estimation error. This term scales as \((T^{1-})\) in Theorem 5.1 while it scales as \((T^{1-})\) in Theorem 4.3. In particular in the stationary case, when Equation (3) is instantiated with \(=1\), the regret bound reads as \(_{T}=(T^{2/3}^{2})\) which is weaker than the optimal \(()\) for the stationary case [55; 39].

**Corollary 5.3** (Setting \(=1/2\)).: _Under the conditions of Theorem 5.1, if Algorithm 1 is run with \(=1/2\), then with probability at-least \(1-\), it satisfies a regret bound of \(_{T}((T^{2/3}_{T}+T^{11/12}}+T^{5/ 6}+T^{3/4}_{T})(,G,1/m,(T/)))\)._

**Corollary 5.4** (**Finite diameter assumption can be relaxed in the absence of corruptions**).: _Let \(_{T}=0\) almost-surely, i.e., there are no adversarial corruptions. Then, under the settings of Theorem 5.1 even when the set \(\) is unbounded, the regret bound given in Equation (3) holds._

_Remark 5.5_ (**Corruption and drift tolerance**).: Theorem 5.1 when instantiated with \((0,1)\) yields a drift tolerance of \((T^{1-})\) and corruption tolerance of \((T^{1-})\). In particular, instantiating with \(=1/2\) yields \((T^{1/3})\) and \((T^{1/4})\) corruption and drift tolerance respectively.

### Proof sketch of Theorem 5.1 and technical innovations

The proof for this case builds upon the techniques used to prove Theorem 4.3. Unlike in the sub-gaussian case, we cannot guarantee that only corrupted samples will be clipped in the general case. This introduces an additional bias and variance terms due to clipping of potential un-corrupted samples or _inliers_. The bias terms of the inliers are handled using techniques from prior works [25; 55]. We bound the variance terms using an inductive argument similar in spirit to that of Theorem 4.3. However, identifying the _correct_ hypothesis involving drifts and outliers so that the induction argument holds is challenging. The challenge is because the induction hypothesis assumed till time \(t-1\) impacts the martingale error term, which in-turn impacts whether the induction hypothesis will hold at time \(t\). We introduce a free parameter to the regret term and deduce the induction hypothesis to hold if a quadratic equation in this parameter does not have any real roots. This extra term contributes to regret degradation compared to the sub-gaussian case (Lemma 20.10).

## 6 Insights and remarks

**Known Time Horizon:** This is a benign assumption and can be overcome by the standard doubling trick (Section \(2.3\) of ) with additional \(^{2}(T)\) factor regret (c.f. Appendix 14).

**Price for being adaptive to distribution shift:** Theorems 4.3 and 5.1 show that, in order to minimize regret due to statistical estimation, we need to set \(=1\), i.e., choose a learning rate of \(O(1/T)\). This was shown to be optimal in in the absence of drift and corruptions both under sub-gaussian assumptions  and in heavy tail settings . On the other hand, we see from Theorems 4.3 and 5.1, that in the presence of drift \(_{T}>0\), a learning rate \(<1\) is sufficient to ensure sub-linear regret. The following lower bound (proved in Appendix 15) shows that \(<1\) is also necessary.

**Proposition 6.1** (**Lower bound showing \(O(1/T)\) learning rate is not adaptive to drifts)**.: _There exists a loss function \(\) such that for every \(T\), there exists a sequence of probability measures \((_{t})_{t}\) with the diameter \( 2(T)\), distribution shift \(_{T} 2(T)\), \(_{T}=0\), such that Algorithm 1 when run with \( 1\) and step size \(_{t}:=\) will incur regret at-least \(\) with probability \(1\)._

**Finite diameter \(\) is necessary in general:** We already saw in Proposition 2.6 that \((_{T})\) regret is necessary. We prove the lower-bound by considering mean-estimation in Section 16. Variants of Proposition 2.6 was observed in the literature (for ex. Proposition \(1.3\), Lemma \(6.1\) in , Theorem \(D.1\) in , Line \(8\) of Algorithm \(3\) in ). In theoretical statistics literature, \((_{T})\) is considered _un-desirable_ and thus the models studied restrict corruptions to occur at _random times_[13; 19] rather than arbitrary as in our setup. However, in practice corruptions are rarely random and typically occur close in time for instance due to machine failures or other external confounding factors [23; 42].

**Regret bounds are dimension-free:** The problem dimensions \(d,p\) do not appear in the regret bound. Moreover, the finite diameter \(\) only appears in the regret term affecting through adversarial corruptions and _not_ in the distribution shift and finite sample error terms. Further, Corollaries 4.5 and 5.4 give regret bounds that hold even when \(=\) in the absence of corruptions.

Figure 2: Plot of regret versus \(\) over \(10000\) steps for mean-estimation. We observe that the best choice of \(\) decreases as \(_{T}\) and/or \(_{T}\) increases. More details in Section 7.

Experiments

Theorems 4.3 and 5.1 give _upper bounds_ on regret showing that tuning the learning rate depending on the amount of distribution shift and number of corruptions in the stream is beneficial. We empirically observe similar phenomena in Figures 2 and 3 (in the Appendix), thus indicating our theoretical observations are fundamental and not artifact of our bounds. We consider \((X,)=\|X-\|^{2}\) corresponding to mean-estimation in Figure 2 and linear-regression of \((X,)=\|X^{(2)}-^{T}X^{(1)}\|^{2}\), where \(X=(X^{(1)},X^{(2)})\) with \(X^{(1)}^{d-1}\), \(X^{(2)}\), \(^{(d-1)}\) in Figure 3 in the Appendix in Section 12. We compare clipped-SGD with learning rate \(1/mT^{}\) for variety of \(\) with clipping \(=2T^{0.25}\) and use the doubling trick to remove dependence of \(T\) (i.e., use Algorithm 2). For the case of \(=1\), we use the learning rate of \(_{t}=1/(m(t+1))\) and \(=2\) as suggested in . All plots are averaged over \(30\) runs and we plot the median along with the \(5\)th and \(95\)th quantile bands. We observe in Figures 2 and 3 that as \(_{T}\) or \(_{T}\) increases, the optimal \(\) to use decreases, validating the theoretical insight. Further details in the Appendix in Section 12. Evaluations on real-data is conducted in the Appendix in Section 13.

## 8 Related Work

FITNESS for mean estimation is proposed in  that requires variance as input and is adaptive to drifts and corruptions under sub-gaussian distributions with per-sample computational complexity is \(O(dT)\). In contrast, our algorithm applies to any strongly convex function, does not require moment bounds, data can be heavy-tailed. The work of  studied regret bounds in the absence of corruptions and only give bounds in expectation. A sequence of papers in online estimation including  have derived algorithms that are robust in different ways. However, _none of them consider the impact of distribution shift_. The works of  study robust _linear_ regression in the absence of drifts with  making Gaussian assumptions on both covariates and response while  makes Gaussian assumption only on the responses. The work of  show that clipped SGD attains near optimal rates for any strongly convex loss function in the _absence of drifts and corruptions_. The paper of  study online estimation of strongly convex loss functions with corruptions, but do not consider drift. Moreover, their regret bounds are not dimension free (cf. Table 2). Although the paper of  gives regret bounds for online learning, a more general setting compared to estimation, do not consider impact of drifts. High probability bounds for optimization with heavy-tailed data have been studied by  in recent times. However, _none of their analysis considers the impact of drift and corruptions_. More related work in Section 10.

## 9 Conclusions and Open Problems

We studied online estimation in the presence of drifts and corruptions and potentially heavy-tailed inlier data and proved regret bounds for clipped SGD for strongly convex loss functions. Ours is the first proof that an online algorithm can simultaneously be robust to heavy-tails and corruptions and adaptive to drift. A key result was to show how the choice of learning-rate trades off drift, finite sample error and corruptions. Our work leaves several interesting open problems.

* The optimal choice of \(\) in Theorems 4.3 and 4.3 is a trade-off between distribution shift, finite sample error and corruptions. Can the optimal \(\) be set without knowing \(_{T}\), \(_{T}\) or \(\) in advance?
* In the absence of corruptions, can regret \(((+T^{a}_{T}^{1-a})(G,,(T/))\), for some \(a<1\) be achieved? Such an algorithm would simultaneously have both _(i)_\((T)\) distribution shift tolerance which is the best possible and matching the lower bound established in , and _(ii)_ closing the gap of Remark 5.2 for the stationary case.
* Can \(m=1/2\) in Equation (1) be achieved in the general case with drift and corruptions? Theorem 5.1 shows that only \(m 2/3\) is achievable. For the special case of stationary stream,  shows \(m=1/2\) is achievable, matching the lower bound that \(m 1/2\) is necessary.
* (Open problem from ) Does there exists an online algorithm that can obtain the statistically optimal regret of \(((()}+()(T/ )}))\) for a stationary stream?