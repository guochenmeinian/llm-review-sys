# Hybrid Search for Efficient Planning with Completeness Guarantees

Kalle Kujampaa1,3, Joni Pajarinen2,3, Alexander Ilin1,3,4

1Department of Computer Science, Aalto University

2Department of Electrical Engineering and Automation, Aalto University

3Finnish Center for Artificial Intelligence FCAI

4System 2 AI

{kalle.kujampaa,joni.pajarinen,alexander.ilin}@aalto.fi

Corresponding author

###### Abstract

Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call _complete subgoal search_. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performance in terms of search expansions for instances that the high-level could solve without low-level augmentations. Our approach makes it possible to apply subgoal-level planning for systems where completeness is a critical requirement.

## 1 Introduction

Combining planning with deep learning has led to significant advances in many fields, such as automated theorem proving , classical board games , puzzles , Atari games , video compression , robotics , and autonomous driving . However, deep learning-based methods often plan in terms of low-level actions, such as individual moves in chess or commands in Atari games, whereas humans tend to plan by decomposing problems into smaller subproblems . This observation has inspired a lot of recent research into methods that solve complex tasks by performing hierarchical planning and search. In the continuous setting, hierarchical planning has been applied to, for instance, robotic manipulation and navigation . Discrete hierarchical planning methods, when trained entirely on offline data, can solve complex problems with high combinatorial complexity. These methods are efficient at long-term planning thanks to the hierarchy reducing the effective planning horizon . Furthermore, they reduce the impact of noise on planning  and show promising generalization ability to out-of-distribution tasks . Training RL agents on diverse multi-task offline data has been shown to scale and generalize broadly to new tasks, and being compatible with training without any environment interaction can be seen as an additional strength of these methods .

Despite their excellent performance, the discrete subgoal search methods share a weakness: neither AdaSubS , kSubS , nor HIPS  are guaranteed to find a solution even if it exists. All these methods rely on a learned subgoal generator that proposes new subgoals to be used with some classical search algorithms for planning. If the generator fails to perform adequately, the result can be a failure to discover solutions to solvable problems. We call this property the lack of completeness. The guarantee of the algorithm finding a solution is critical for both theoretical science and practical algorithms. In the context of subgoal search, completeness guarantees discovering the solution, even if the generative model is imperfect. Completeness allows extensions and new incremental algorithms that require building on top of exact solutions. One example of that could be curriculum learning. Furthermore, we show that completeness significantly improves the out-of-distribution generalization of subgoal search. Achieving completeness also makes applying high-level search as an alternative to low-level search possible in safety-critical real-world systems. Zawalski et al.  mentioned that AdaSubS can be made complete by adding an exhaustive one-step subgoal generator that is only utilized when the search would otherwise fail, whereas Kujanpaa et al.  proposed combining high- and low-level actions to attain completeness of HIPS. However, to the best of our knowledge, this idea has not been formalized, analyzed, or evaluated in prior work.

We present a multi-level (hybrid) search method, _complete subgoal search_, that combines hierarchical planning and classical exhaustive low-level search. The idea is illustrated in Figure 1. We apply our complete search approach to HIPS  which 1) has been shown to outperform several other subgoal search algorithms in challenging discrete problem domains, 2) does not require training multiple subgoal generators in parallel to perform adaptive-length planning, and 3) is particularly suited for being combined with low-level actions due to using Policy-guided Heuristic Search (PHS) as the main search algorithm [18; 27]. We use the name HIPS-\(\) for the proposed enhancement of HIPS with complete subgoal search. We argue that the idea suggested in  can be seen as a specific case of our search method that works well in some environments but can be improved upon in others. We evaluate HIPS-\(\) on challenging, long-term discrete reasoning problems and show that it outperforms HIPS, other subgoal search methods, and strong offline reinforcement learning (RL) baselines. Not only does HIPS-\(\) attain completeness, but our complete subgoal search also demonstrates improved performance on problem instances that subgoal search alone was sufficient for solving.

## 2 Related Work

We propose to augment the high-level search used in learning-based hierarchical planning with low-level actions to achieve completeness while even improving the search performance in terms of node expansions. First, we discuss our work in the general context of hierarchical planning and then focus on the closest methods in discrete subgoal search, to which we compare the proposed approach.

Continuous Hierarchical PlanningHierarchical planning has been widely applied in the continuous setting. The Cross-Entropy Method (CEM)  is often used as the planning algorithm for different hierarchical planning methods [22; 25; 12; 32]. However, the cross-entropy method is a numerical optimization method, unsuitable for addressing discrete planning problems. There is also prior work on hierarchical planning for continuous and visual tasks, including control and navigation,

Figure 1: (a): Low-level search systematically visits all states reachable from the root node and, therefore, is guaranteed to find a path to the terminal state (shown in red). (b): High-level search can find a solution with much fewer steps due to its ability to operate with high-level actions that span several time steps. (c): High-level search may fail to find a solution due to an imperfect subgoal generation model. (d): Complete subgoal search can find a solution with few steps thanks to high-level actions, and it has completeness guarantees due to also considering low-level actions.

where CEM is not used as the optimization algorithm [21; 26; 6; 31; 36; 46]. Kim et al.  plan how to select landmarks and use them to train a high-level policy for generating subgoals. Planning with search is a common approach in classical AI research . Combining search with hierarchical planning has been proposed [23; 8], but these approaches are generally inapplicable to complex, discrete reasoning domains, where precise subgoal generation and planning are necessary .

Recursive Subgoal-Based PlanningIn this work, we focus on search algorithms where subgoals are generated sequentially from the previous subgoal. Recursive subgoal generation is an orthogonal alternative to our approach. In certain domains, given a start and a goal state, it is possible to hierarchically partition the tasks into simpler ones, for instance, by generating subgoals that are approximately halfway between the start and the end state. This idea can be applied repeatedly for an efficient planning algorithm [30; 31; 13]. However, the recursive approach can be difficult to combine with search and limits the class of problems that can be solved.

Discrete Subgoal SearchDiscrete subgoal search methods can solve complex reasoning problems efficiently with limited search node expansions. S-MCTS  is a method for MCTS-based subgoal search with predefined subgoal generators and heuristics, which significantly limits its usability to novel problems. Allen et al.  improve the efficiency of search through temporal abstraction by learning macro-actions, which are sequences of low-level actions. Methods that learn subgoal generators, on the other hand, generally suffer from a lack of completeness guarantees. kSubS  learns a subgoal generator and performs planning in the subgoal space to solve demanding reasoning tasks. AdaSubS  builds on top of kSubS and proposes learning multiple subgoal generators for different distances and performing adaptive planning. HIPS  learns to segment trajectories using RL and trains one generator that can propose multiple-length subgoals for adaptive planning.

## 3 Method

We present HIPS-\(\), an extension of _Hierarchical Imitation Planning with Search_ (HIPS), a search-based hierarchical imitation learning algorithm for solving difficult goal-conditioned reasoning problems . We use the Markov decision process (MDP) formalism, and the agent's objective is to enter a terminal state in the MDP. However, we deviate from the reward maximization objective. Instead, the objective is to minimize the total search loss, that is, the number of search nodes expanded before a solution is found. A similar objective has been adopted in prior work on subgoal search and, in practice, the solutions found are efficient [4; 45].

We assume the MDPs to be deterministic and fully observable with a discrete state space \(\) and action space \(\). We also assume the standard offline setting: the agent cannot interact with the environment during training, and it learns to solve tasks only from \(\), an existing dataset of expert demonstrations. These trajectories may be heavily suboptimal, but all lead to a goal state. Therefore, the agent should be capable of performing stitching to discover the solutions efficiently .

### Preliminaries

HIPS learns to segment trajectories into subgoals using RL and a subgoal-conditioned low-level policy \((a|s,s_{g})\). The segmented trajectories are used to train a generative model \(p(s_{g}|s)\) over subgoals \(s_{g}\) that is implemented as a VQVAE with discrete latent codes , a strategy inspired by Ozair et al. . The VQVAE learns a state-conditioned prior distribution \(p(e|s)\) over latent codes \(e\) from a codebook \(\) and a decoder \(g(s_{g}|e,s)\) acting as a generator that outputs a subgoal \(s_{g}\) given the latent code \(e\) and state \(s\) deterministically as \(s_{g}=g(e,s)\). Each code \(e_{k}\) can be considered a state-dependent high-level action that induces a sequence of low-level actions \(A_{k}(s)=(a_{1},,a_{n_{k}})\), \(a_{i}\), that takes the environment from state \(s\) to \(s_{g}\). The low-level action sequence is generated deterministically by the subgoal-conditioned policy \((a|s,s_{g})\), when the most likely action is used at each step. The prior \(p(e|s)\) can be interpreted as a high-level policy \(_{}(e|s)\) that assigns a probability to each latent code. HIPS also learns a heuristic that predicts the number of low-level actions \(a\) needed to reach a terminal state from \(s\) and a single-step dynamics model \(f_{}(s_{i+1}|a_{i},s_{i})\). The heuristic is denoted by \(V(s)\) in , but we denote it by \(h\) in this paper.

HIPS performs high-level planning in the subgoal space with PHS , although other search methods can be used as well. PHS is a variant of best-first search in which the algorithm maintainsa priority queue and always expands the node \(n\) with the lowest evaluation function value. There is a non-negative loss function \(L\), and when the search expands node \(n\), it incurs a loss \(L(n)\). The objective of PHS is to minimize the total _search_ loss. The evaluation function of PHS is

\[(n)=(n),\] (1)

where \((n) 1\) is a heuristic factor that depends on the heuristic function \(h(n)\). \(g(n)\) is the _path_ loss, the sum of the losses from the root \(n_{0}\) to \(n\). \((n) 1\) is the probability of node \(n\) that is defined recursively: \((n^{})=(n^{}|n)(n),(n_{0})=1\), \(n\) is the parent of \(n^{}\) and \(n_{*}(n_{0})\), that is, \(n\) is a descendant of the root node \(n_{0}\). When HIPS plans, each search node \(n\) corresponds to a state \(s\), \(g(n)\) to the number of low-level steps needed to reach \(n\) from the root \(n_{0}\), and \(h(n)\) to the output of the value function \(V(s)\). The children \((n)\) of node \(n\) are the valid subgoals generated by the VQVAE decoder \(g(e_{k},s)\) for all \(e_{k}\). The search policy \((n)\) is induced by the conditional policy \((n^{}|n)\), which is represented by the state-conditioned prior \(p(e|s)\).

### Complete Subgoal Search

HIPS performs search solely in the space of subgoals proposed by the trained generator network, which may cause failures to discover solutions to solvable problems due to possible imperfections of the subgoal generation model (see Fig. 0(c)). This problem can be tackled in discrete-action MDPs by augmenting the search with low-level actions (see Fig. 0(d)). In this work, we apply this idea to HIPS to guarantee solution discovery when the solution exists while still utilizing the subgoal generator for temporally abstracted efficient search.

Formally, we propose modifying the search procedure of HIPS such that in addition to the subgoals \(\{s_{H}=g(e,s), e\}\) produced by the HIPS generator, the search also considers states \(\{s_{L}=T(s,a), a\}\) that result from the agent taking every low-level action \(a\) in state \(s\), where \(T(s,a)\) is the environment transition function. We assume that \(T\) is known but we also evaluate the method with a learned \(T\). The augmented action space is then \(^{+}=\). We use PHS as the search algorithm and compute the node probabilities used for the evaluation function (1) as

\[(n)=_{j=1}^{d}(s_{j}(n)|s_{j-1}(n))\,,\]

where \(s_{d}(n)\) is the state that corresponds to node \(n\) at depth \(d\), \(s_{d-1}(n),s_{d-2}(n),...,s_{0}(n)\) are the states of the ancestor nodes of \(n\) and \((s_{j}(n)|s_{j-1}(n))\) is the search policy. We propose to compute the probabilities \((s_{j}|s_{j-1})\) in the following way:

\[(s_{j}|s_{j-1})=(1-)_{}(e_{k}|s_{j-1})& =g(e_{k},s_{j-1})$ is proposed by HIPS}\\ \,_{}(a|s_{j-1})&=T(s_{j-1},a)$ is proposed by low-level search}\] (2)

where \(_{}(e_{k}|s_{j-1})=p(e_{k}|s_{j-1})\) is the prior over the high-level actions learned by HIPS and \(_{}(a|s_{j-1})\) is a low-level policy that we train from available expert demonstrations with BC. We use hyperparameter \(\) to balance the probabilities computed with the high and low-level policies: higher values of \(\) prioritize more low-level exploration. We call this complete search HIPS-\(\).

The proposed complete search approach can be combined with any policy-guided search algorithm, but we focus on PHS due to its state-of-the-art performance. Complete search can also be used with policyless search algorithms, but in that case, \(\) cannot directly control the frequency of low and high-level actions.

Zawalski et al.  suggested using low-level actions only when the high-level search would otherwise fail. In the context of PHS, this can be interpreted as having an infinitesimal \(\) such that every high-level action has a lower evaluation function value (1) and, therefore, a higher priority than every low-level action. With some abuse of notation, we refer to this approach as \( 0\).

### Complete Subgoal Search Heuristic

The objective of PHS is to minimize the search loss, and Orseau and Lelis  propose using a heuristic to estimate the \(g\)-cost of the least costly solution node \(n^{*}\) that is a descendant of the current search node \(n\). They propose to approximate the probability \((n^{*})\) for node \(n^{*}\) that contains a found solution as

\[(n^{*})=[(n)^{1/g(n)}]^{g(n)+h(n)}\] (3)

and use this approximation to arrive at a heuristic factor \(_{h}(n)=}\) to be used in (1). PHS* is the variant of PHS that uses this heuristic factor. The derivation of the heuristic factor \(_{h}(n)\) assumes that \(g(n)\), the path loss of node \(n\), and \(h(n)\), the heuristic function approximating the distance to the least-cost descendant goal node \(n^{*}\) are expressed in the same scale. This does not hold for the proposed search algorithm. The search objective of HIPS-\(\) is to minimize the number of node expansions, which implies that \(g(n)\) is equal to the depth of node \(n\), whereas the heuristic \(h(n)\) is trained to predict the number of low-level actions to the goal. One solution is to let \(g(n)\) be equal to the number of low-level steps to node \(n\). However, this would distort the search objective. The cost of expanding a node corresponding to a subgoal would equal the length of the corresponding low-level action sequence, whereas the loss of low-level actions would be one. However, the cost should be the same for every node, as the objective is to minimize the number of node expansions.

Instead, we propose re-scaling the heuristic factor \(h\) to be equal to the estimated number of search nodes on the path from \(n\) to the goal node \(n^{*}\) by dividing the expected number of low-level actions from node \(n\) to the terminal node by the average low-level distance between the nodes on the path from \(n_{0}\) to node \(n\). Let \(g(n)\) be equal to \(d(n)\), the depth of the node, and let \(l(n)\) be the number of low-level actions from the root node. Then, we define a scaled heuristic \((n)=\) and by using it instead of \(h(n)\) in (3), we get a new approximation \((n^{*})=^{(1+h(n)/l(n))}\), which yields the following heuristic factor and evaluation function:

\[_{}=},_{}(n)=}.\] (4)

For the full derivation, please see Appendix C. Note that \(_{}}\) may not be PHS-admissible, even if \(h(n)\) were admissible. In HIPS, there are no guarantees of the learned heuristic \(h(n)\) being admissible.

### Analysis of Hips-\(\)

Orseau and Lelis  showed that the search loss of PHS has an upper bound:

\[L(,n^{*}) L_{}(,n^{*})=)}{(n ^{*})}^{+}(n^{*})_{n_{}(n^{*})}(n)},\] (5)

where \(n^{*}\) is the solution node returned by PHS, \(g(n)\) is the path loss from the root to \(n\), \(\) the search policy, and \(^{+}\) the modified heuristic factor corresponding to the monotone non-decreasing evaluation function \(^{+}(n)\), and \(_{}(n^{*})\) the set of nodes that have been expanded before \(n^{*}\), but the children of which have not been expanded. Now, given the recursively defined policy \((n)\) that depends on \(_{}\) and \(_{}\) as outlined in Subsection 3.2, the upper bound derived in (5) is unchanged.

However, the upper bound of (5) can be seen as uninformative in the context of complete search, as it depends on the high-level policy \(_{}\). In general, we cannot make any assumptions about the behavior of the high-level policy as it depends on the subgoals proposed by the generator network. However, we can derive the following upper bound, where the probability of the terminal node \(n^{*}\) is independent of the behavior of the generator, and consequently, the high-level policy \(_{}\):

**Corollary 1**.: _(Subgoal-PHS upper bound without \(_{}\) in the denominator). For any non-negative loss function \(L\), non-empty set of solution nodes \(_{}_{*}(n_{0})\), low-level policy \(_{}\), high-level policy \(_{}\), \((0,1]\), policy \((n^{}|n)\) as defined in (2), and heuristic factor \(() 1\), Subgoal-PHS returns a solution node \(n^{*}_{n^{*}_{}}^{+}(n^{*})\). Then, there exists a terminal node \(_{}\) such that \(()=(n^{*})\), where \(\) corresponds to low-level trajectory \((s_{0},a_{0},,a_{N-1},(n^{*}))\) and the search loss is bounded by_

\[L(,n^{*}) L(,))}{ ^{N}_{i=0}^{N-1}_{}(a_{i}|s_{i})}^{+}() _{n_{}()}(n)}\] (6)

Proof.: Subgoal-PHS does not change the underlying search algorithm, so the assumption of Subgoal-PHS returning the minimum score solution node follows from Theorem 1 in . The first inequality follows from the fact that the loss function \(L\) is always non-negative and the node \(n^{*}\) will be expanded before any other node \(n\) with \(^{+}(n)>^{+}(n^{*})\). In particular, note that it is possible that \(n^{*}=\). The second inequality follows from the Theorem 1 in  and \(()\ =\ ^{N}\ _{i=0}^{N-1}\ _{}(a_{i}|s_{i})\). 

Now, suppose that we have a problem where there is only one terminal state and sequence of low-level actions that reaches this terminal state, and the search space \(\) is infinite. Let us denote this trajectory as \(=(s_{0},a_{0},s_{1},a_{1},,s_{N-1},a_{N-1},s_{N})\). Additionally, assume that the high-level policy \(_{}\) never guides the search towards a solution, that is, for each state \(s\), the low-level action sequence for every subgoal \(s_{g}\) proposed by the generator network contains an incorrect action. Furthermore, assume that the generator network can always generate new, reachable subgoals for all states \(s\). Hence, only actions selected by the low-level policy lead to the solution, and thus \((n^{*})=^{N}_{i=0}^{N-1}_{}(a_{i}|s_{i})\). If \(=0\) or \( 0\), the search does not converge, and the upper bound in (5) approaches infinity.

Czechowski et al.  argue that low-level search suffers more from local noise than subgoal search and using subgoals improves the signal-to-noise ratio of the search, and the results of Kujanpaa et al.  support this. For complete search, a lower \(\) can increase the use of high-level policy in relation to the low-level actions, thereby shortening the effective planning horizon and making the search more effective. Hence, we hypothesize that a low value of \(\) is generally preferred. However, (6) shows that the worst-case performance of the search deteriorates when the value of \(\) decreases.

If there is no heuristic, the bound can be simplified similarly as in Orseau and Lelis . This following corollary shows that removing the heuristic factor does not affect the inverse dependence of the worst-case search loss on \(\), and in fact, the worst case of (6), that is, the search failing for \( 0\), is possible even given a perfect heuristic function.

**Corollary 2**.: _(Subgoal-PHS upper bound without heuristic). If there is no heuristic, that is, \( n,(n)=1\), the upper bound simplifies to_

\[L(,n^{*}) L(,))}{ ^{N}_{i=0}^{N-1}_{}(a_{i}|s_{i})}\] (7)

Proof.: The first inequality follows as in Corollary 1, and the second inequality follows from the assumptions and \(_{n^{}_{}(n)}(n^{}) 1\) for all \(n\), see Orseau et al. . 

## 4 Experiments

The main objective of our experiments is to evaluate whether the proposed complete search algorithm HIPS-\(\) can be combined with an existing subgoal search algorithm, HIPS , to improve its performance in environments that require long-term planning and complex, object-based relational reasoning. We also analyze the sensitivity of HIPS-\(\) to the hyperparameter \(\) and the impact of the

Figure 2: We evaluate our complete search approach, HIPS-\(\), in Sokoban, Sliding Tile Puzzle (STP), Box-World (BW), and Travelling Salesman Problem (TSP). In Sokoban, the agent must push the yellow boxes onto the red target locations. In the Sliding Tile Puzzle, the agent must slide the tiles to sort them from 1 to 24. In Box-World, the task is to collect the gem (marked with $) by opening locks with keys of the corresponding color. In Travelling Salesman, the agent (marked with a circle) must visit all unvisited cities (red squares) before returning to the start (black square).

novel heuristic factor and the corresponding PHS* evaluation function (4). The OOD-generalization abilities of HIPS-\(\) are also discussed. We use the four environments considered in : Box-World , Sliding Tile Puzzle , Gym-Sokoban  and Travelling Salesman (see Fig. 2).

We implement HIPS-\(\) as proposed in (2) and the policy \(_{}(a|s)\) as a ResNet-based network  and train it using Adam . We use the implementation of HIPS from the original paper and use PHS* (or our complete variant thereof) as the search algorithm for HIPS and HIPS-\(\). In , HIPS was evaluated with GBFS and A* on some environments. HIPS-\(\) experiments with these search algorithms can be found in Appendix F. We use the versions of HIPS-\(\) and HIPS that have access to the environment dynamics (named HIPS-env in ) unless stated otherwise.

In Table 1, we report the proportion of solved puzzles as a function of the number of expanded nodes for the low-level search PHS*, high-level search algorithms HIPS , kSubS , and AdaSubS , and our HIPS-\(\), which is HIPS enhanced with the proposed complete subgoal search. We used the kSubS and AdaSubS results from  where they were not evaluated on Box-World. The results show that high-level search alone does not guarantee completeness, which means that for some puzzles the solution is not found even without a limit on the number of expanded nodes (\(N=\)) due to the generator failing to propose subgoals that lead to a valid terminal state, a phenomenon observed in prior work . As expected, HIPS-\(\) achieves a 100% success rate in all environments thanks to augmenting the subgoals with low-level actions. In addition to the number of search node expansions, we analyzed the search cost in terms of wall-clock time and environment steps. HIPS-\(\) generally outperforms the baselines also using these metrics. The results of these experiments can be found in Appendices J and K.

We also evaluated HIPS-\(\) on Sokoban, STP, and TSP with learned transition models and compared it to HIPS with learned models, behavioral cloning (BC), Conservative Q-learning [CQL, 19], and Decision Transformer [DT, 3]. We omitted Box-World due to HIPS struggling with the learned dynamics . The results are shown in Table 2. HIPS-\(\) outperforms the baselines but can fail to find a solution to a solvable problem instance due to the learned dynamics model sometimes outputting incorrect transitions. Completeness can be attained by using the true dynamics for validating solutions and simulating low-level actions, while still minimizing the number of environment steps (see Appendix E). For complete results with confidence intervals, please see Appendix G.

Interestingly, the results in Table 1 show that augmenting the high-level search with low-level actions may have a positive impact on the performance for the same budget of node expansions \(N\): we observe statistically significantly increased success rates for the same \(N\) in all environments except

  &  &  \\  \(N\) & 50 & 100 & 200 & \(\) & 50 & 100 & 200 & \(\) \\  PHS* (low-level search) & 0.2 & 2.4 & 16.2 & **100** & 0.0 & 0.0 & 0.0 & **100** \\ HIPS (high-level search) & 82.0 & 87.8 & 91.6 & 97.9 & 8.7 & 56.8 & 86.3 & 95.0 \\ AdaSubS (high-level search) & 76.4 & 82.2 & 85.7 & 91.3 & 0.0 & 0.0 & 0.0 & 0.0 \\ kSubS (high-level search) & 69.1 & 73.1 & 76.3 & 90.5 & 0.7 & **79.9** & 89.8 & 93.3 \\  HIPS-\(\) (complete search) & **84.3** & **89.5** & **93.1** & **100** & **18.5** & 69.5 & **93.8** & **100** \\  &  &  \\  \(N\) & 5 & 10 & 30 & \(\) & 20 & 50 & 100 & \(\) \\  PHS* (low-level search) & 0.0 & 0.1 & 2.2 & **100** & 0.0 & 0.0 & 0.0 & **100** \\ HIPS (high-level search) & 86.3 & 97.9 & 99.9 & 99.9 & **19.6** & **88.1** & 97.7 & **100** \\ AdaSubS (high-level search) & & & & & 0.0 & 0.0 & 0.6 & 21.2 \\ kSubS (high-level search) & & & & & 0.0 & 1.5 & 40.4 & 87.9 \\  HIPS-\(\) (complete search) & **89.7** & **98.9** & **100** & **100** & 17.9 & 87.4 & **97.9** & **100** \\ 

Table 1: The success rates (%) after performing \(N\) node expansions for different subgoal search algorithms with access to environment dynamics. For HIPS-\(\), we use the value of \(\) that yields in the best performance: \( 0\) for Sokoban, \(=10^{-5}\) for Sliding Tile Puzzle, \(=10^{-3}\) for Box-World and \( 0\) for Travelling Salesman Problem. HIPS corresponds to HIPS-env in  and uses PHS* as the search algorithm in all environments.

[MISSING_PAGE_FAIL:8]

naive A*-inspired evaluation functions  which correspond to the variant \(_{h}\) in :

\[_{}=(g(n)+h(n))/(n),_{}=(l(n)+h(n) )/(n).\]

The results in Table 4 show that a heuristic function is crucial for reaching a competitive search performance in most environments. In particular, on TSP, where it was observed that training a good VQVAE prior (which we use as \(_{}\)) is very difficult , the search heavily relies on the learned heuristic, and the search fails due to running out of memory. Furthermore, naively using A*-inspired evaluation functions fails to be competitive.

Figure 4: The percentage of puzzles remaining unsolved (y-axis) depending on the number of node expansions (x-axis) for complete search with different values of \(\) and only high-level search (left), and the ratio of unsolved puzzles in comparison with HIPS depending on the number of node expansions (right), when the methods have been evaluated on an out-of-distribution variant of Box-World.

Figure 3: The ratio of the number of unsolved puzzles to the number of unsolved puzzles by HIPS as a function of the number of node expansions \(N\) (x-axis). Values below 1 indicate that the complete search is superior to the high-level search. HIPS-\(\) outperforms HIPS in every environment except TSP, where high-level actions are sufficient for solving every problem instance.

Conclusions

Subgoal search algorithms can effectively address complex reasoning problems that require long-term planning. However, these algorithms may fail to find a solution to solvable problems. We have presented and analyzed HIPS-\(\), an extension to a recently proposed subgoal search algorithm HIPS. We achieve this by augmenting the subgoal-level search with low-level actions. As a result, HIPS-\(\) is guaranteed to discover a solution if a solution exists and it has access to an accurate environment model. HIPS-\(\) outperforms HIPS and other baseline methods in terms of search loss and solution rate. Furthermore, the results demonstrate that augmenting the search with low-level actions can improve the planning performance even if the subgoal search could solve the puzzle without them, and the proposed algorithm is not hyperparameter-sensitive. HIPS-\(\) enables using subgoal search in discrete settings where search completeness is critical.

In the future, we would like to tackle some of the limitations of our work. Our search paradigm could be applied to other subgoal search methods than HIPS and search algorithms than PHS. Combining HIPS-\(\) with recursive search methods that split the problem into smaller segments could enable scaling to even longer problem horizons. Our analysis of the complete subgoal search assumes deterministic environments and access to transition dynamics. HIPS-\(\) can also function without access to the transition dynamics, but the completeness guarantee is lost. Many real-world problems are partially or fully continuous. There are also problems with infinite discrete action spaces that HIPS-\(\) is not particularly suited for, and we assume discrete state representations. HIPS-\(\) also assumes that a solution exists. Modifying HIPS-\(\) for the continuous setting could enable solving real-world robotics tasks. HIPS-\(\) also showed potential at transfer learning by efficiently solving more complex Box-World tasks than those seen during training. Applying curriculum learning by learning the initial model offline and then learning to progressively solve harder problem instances with online fine-tuning is a promising direction for future research.