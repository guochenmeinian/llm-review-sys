# Universal Sample Coding

Szymon Kobus

Dep. of Electrical and Electronic Engineering

Imperial College London

szymon.kobus17@imperial.ac.uk

&Tze-Yang Tung

Not Diamond

tze-yang@notdiamond.ai

&Deniz Gunduz

Department of Electrical and Electronic Engineering

Imperial College London

d.gunduz@imperial.ac.uk

###### Abstract

In this work, we study the problem of communicating multiple samples from an unknown probability distribution using as few bits as possible. This is a generalization of the channel simulation problem, which has recently found applications and achieved state of the art results in realistic image compression, neural network compression, and communication-efficient federated learning. In this problem, the transmitter wants the receiver to generate multiple independent and identically distributed (i.i.d.) samples from a target distribution \(P\), while the transmitter and the receiver have access to independent samples from a reference distribution \(Q\). The core idea is to employ channel simulation in multiple rounds while updating the reference distribution \(Q\) after each round in order to reduce the KL-divergence between \(P\) and \(Q\), thereby reducing the communication cost in subsequent rounds. We derive a lower bound on the expected communication cost and construct a practical algorithm that achieves the lower bound up to a multiplicative constant. We then employ this algorithm in communication-efficient federated learning, in which model updates correspond to samples from a distribution, and achieve a \(37\%\) reduction in the communication load. To further highlight the potential of sample communication for generative models, we show that the number of bits needed to communicate samples from a large language model can be reduced by up to 16 times, compared to entropy-based data compression.

## 1 Introduction

Let \(P\) be a probability distribution known to an encoder, and we would like to obtain a sample \(X P\) at a remote decoder - what is the minimal number of bits required to communicate from the encoder to the decoder? The obvious procedure is to sample from \(P\) at the encoder, compress it, and transmit over the network. The minimum number of bits necessary for lossless transmission of this sample is bounded by the entropy of distribution \(P\), \((P)\) bits per sample. However, unlike in classical source coding theory, in our problem, the decoder is interested only in drawing of 'an arbitrary' sample from \(P\), and not the particular sample generated at the encoder. If the encoder and decoder have access to samples from another reference distribution \(Q\), then it is possible for the encoder to communicate a sample from \(P\) using approximately \(_{}(P||Q)\) bits instead. Thus, if we are able to sample from a distribution \(Q\) that is _close_ to \(P\), the bandwidth required to communicate a sample from \(P\) can be greatly reduced. This is known as 'channel simulation' or'reverse Shannon coding' (Bennett et al., 2002; Cuff, 2008; Harsha et al., 2010).

This result has recently been used in various neural compression problems, such as compression of images and neural networks (Havasi et al., 2019; Flamich et al., 2020; Agustsson and Theis, 2020; Theis et al., 2022), where non-differentiable quantization operation is replaced with reverse channel coding, which provides a differentiable step by utilizing the reparameterization trick. Another recent application of channel simulation is in federated learning (FL). Many communication efficient FL methods require sending a sample from a client-only distribution after local training. Instead of deterministic weights, the models can be parameterized with a probability distribution and the central server samples realizations from each client's updated model to construct the global model update. Recently, it was shown in Isik et al. (2023, 2024) that the overall communication cost of sending the model updates from the clients to the parameter server can be greatly reduced using channel simulation, achieving state-of-the-art results. In each iteration, the client can enable the parameter server to sample from its locally updated distribution while using the global model distribution as the common reference distribution. As the client's local distribution is typically close to the global model, the communication cost, proportional to the KL-divergence between the two, will will extremely low compared to deterministic alternatives. Channel simulation can also help in enabling differential privacy of the communicated models (Triastcyn et al., 2021; Hasricioglu and Gunduz, 2024; Hegazy et al., 2024; Shahmiri et al., 2024) since it allows for shaping the distribution of the noise.

In all the above works, the single-shot version of channel simulation is utilized; that is, a single sample is to be generated at the receiver with the desired distribution. In this work, we are interested in studying the communication cost of sending _multiple_ samples from the same distribution. This problem can directly find an application in the FL framework, where the global model is estimated from samples of local models. A procedure to efficiently communicate more samples would allow the parameter server to approximate the global model more accurately, which would reduce the noise and variance in its estimation, and speed up training.

Additionally, we would like to point to a strong conceptual connection between generative models and channel simulation, and propose an avenue of future research. Generative models are one of the most successful examples among the recent advances in machine learning. They include various modalities such as text (Touvron et al., 2023), images (Ho et al., 2020), video, and many more. They are trained to mimic some data distribution \(P\) based on samples \(X P\) drawn from it, and allow to generate novel samples from the learned distribution. With growing popularity and deployment of generative AI across a wide variety of applications, communicating the outputs of these models (particularly for image, audio and video modalities) will put an increasing burden on the underlying communication network infrastructure. However, instead of generating and then compressing a sample at the cloud server, we can exploit channel simulation to enable the users to locally generate samples at a much lower communication cost. Drawing multiple samples is a common use case for generative models. For instance, in text to image models, such as DALL-E or Midjourney, multiple images are often generated and communicated to the user based on a single prompt to allow the user choose the desired one. Other instances include translation (Lee et al., 2021; Eikema and Aziz, 2022), code generation (Chen et al., 2021) and planning (Yao et al., 2023), which can be improved by discriminating among multiple generated samples.

The contributions of this work are summarized as follows:

* We formulate a novel _universal sample coding problem_, which defines the problem of communicating multiple samples from an arbitrary discrete distribution unknown to the receiver.
* We highlight the relationship between the redundancy in universal source coding and the communication of samples, and show that the optimal redundancy for universal source coding is a lower bound on the communication cost of universal sample coding.
* We propose a coding scheme for the universal sample coding problem, providing an upper bound on its communication cost that is within a multiplicative factor from the lower bound.
* We employ our algorithm in FL achieving a \(37\%\) reduction in the communication load compared to the current state-of-the-art communication-efficient FL algorithm.
* We adapt our algorithm for the remote generation problem, where the goal is to sample at a remote user from a generative model hosted on a cloud server, while communicating the least number of bits from the server to the user. We demonstrate the potential benefits of sample communication in this setting through numerical experiments.

## 2 Notation

We use \(X^{n}=(X_{1},X_{2},,X_{n})\) to denote a sequence of \(n\) elements and \(()\) denotes logarithms of base \(2\). Let \(P\) and \(Q\) denote discrete probability distributions. The entropy of \(P\) is given by \((P)-_{x P}[ P(x)]\), while the relative entropy from \(P\) to \(Q\), or the KL-divergence, is defined as \(_{}(P\|Q)_{x P}[]\).

## 3 Background

The problem of communicating samples from a desired distribution is a version of the channel simulation problem, also known as the reverse Shannon theorem, or reverse channel coding. Given a sample \(z\) from a probability distribution \(P_{Z}\) at the encoder, channel simulation entails generating sample \(x\) from the conditional distribution \(P_{X|Z=z}\) at the decoder, using the fewest number of bits. It was posed in Cuff (2008), where the solution was characterized in the asymptotic regime, where an infinite sequence of samples is considered, while earlier results appeared in the quantum communication literature (Bennett et al., 2002; Winter, 2002). The non-asymptotic results were shown in Harsha et al. (2010) using common randomness, and then further refined in Li and Anantharam (2021), where it was shown that to communicate a sample \(X\) from distribution \(P_{X|Z=z}\) with joint distribution \(P_{XZ}\), it is sufficient to transmit

\[I(X;Z)+(I(X;Z)+1)+4.732\ \] (1)

on average, where

\[I(X;Z)}{}[_{}(P_{X|Z=z}\|P_{X})]\] (2)

is the mutual information between \(X\) and \(Z\). This result is close to optimal as it was shown in Li and El Gamal (2018), i.e., there exist distributions \(P_{X,Z}\) where the minimal number of communication required for reverse channel coding is on average

\[I(X;Z)+(I(X;Z)+1)-1\ .\] (3)

As pointed out and shown in Theis and Yosri (2022), the sample communication bound does not rely on using the exact marginal distribution \(P_{X}\), and still holds for other reference distributions \(Q\). In general, to generate a sample at the decoder from distribution \(P\), with reference distribution \(Q\), it is enough to communicate an average of

\[_{}(P\|Q)+(_{}(P\|Q)+1)+4.732 \ .\] (4)

While efficient in terms of communication cost, the sample communication method in Li and El Gamal (2018) might be prohibitively expensive computationally. There have been increasing efforts in creating algorithms with reduced complexity by constraining the distributions \(P\) and \(Q\)(Flamich et al., 2022; Flamich, 2023). An overview of the methods and trade-off between communication, computation, and sample accuracy is provided in Theis and Yosri (2022). The problem of communicating multiple samples from an unknown distribution \(P\) was also addressed in Choi and Li (2021), where the authors considered a setting without common randomness and achieved sublinear, yet superlogarithmic, communication rates.

## 4 Communication of samples

### Source coding

_Lossless source coding_ is the problem of describing the realizations of a random variable with the least number of bits on average. Each outcome \(x\) is assigned a different sequence of bits, called

    & matched & mismatched \\  source coding & \((P)\) & \((P)+_{}(P\|Q)\) \\ sample communication & 0 & \(_{}(P\|Q)\) \\   

Table 1: Rate required to communicate a given/any sample from \(P\)a _codeword_, with length \(l(x)\). The optimal average codeword length is obtained by Huffman coding (Huffman, 1952), where \(l(x)- P(x)\) and

\[(P)}[l(x)](P)+1.\] (5)

For \(n\) independent and identically distributed random variables \(X^{n}=(X_{1},X_{2},,X_{n}),\ X_{i} P\), it is straightforward to show that using a Huffman code, the expected codeword length per symbol is \((P)\,_{x^{n} P^{ n}}[l(x^{n})] (P)+\). As \(n\) this quantity converges to the entropy, which provides a fundamental limit for compression.

We call the code _mismatched_ if it is designed for a source distribution of \(Q\) while the true source distribution is \(P\). Then the expected codeword length is given by:

\[}[l_{Q}(x)] }[- Q(x)]}[- Q(x)+1+]\] (6) \[=-}[ P(x)]+ }[]+1= (P)+_{}(P\|Q)+1.\] (7)

For the \(n\)-ary case, the rate converges to \((P)+_{}(P\|Q)\)(Cover and Thomas, 2006), where the KL-divergence is the penalty for using a mismatched source distribution when designing the code.

However, there exist source coding methods called _universal source coding_ with an average codeword length that converges to \((P)\) for any underlying distribution \(P\) not known to the encoder. They can be thought of as empirically estimating (often implicitly) the underlying distribution of the data. The classic and widely used example of such an algorithm is LZ78 (Ziv and Lempel, 1978). For any distribution \(P\) the average number of bits needed to compress \(n\) samples is \((P)+O()\)(Savari, 1997), which converges much slower than Huffman coding for known distributions. This excess factor for any universal source code \(\) is known as per letter redundancy, defined as:

\[_{}(n)=_{P^{k}}\,  P}{}[l(x^{n})]-(P),\] (8)

where \(_{k}\) is a \(k\)-dimensional probability simplex. It was shown in Davisson et al. (1981) that for a \(k\)-dimensional distribution, the minimal per letter redundancy for any universal source code is:

\[_{}_{}(n)=+On^{-1} .\] (9)

The \( n\) factor is the penalty for universality of the code (Krichevsky and Trofimov, 1981). A more useful quantity for our analysis will be the total redundancy, defined as \(n\,_{}\). In the remainder of the paper, when we refer to redundancy, we mean the total redundancy.

### Sample communication

As mentioned in Section 3, reverse channel coding involves simulating samples from a joint distribution. _Sample communication_ is a similar problem where the aim is to draw samples at the decoder from distribution \(P\), which is only known at the encoder, while both the encoder and the decoder have access to samples from a common distribution \(Q\). This common randomness - in practice, a seed to initialize a pseudorandom number generator - allows both to draw the same sequence of samples from \(Q\). Using any sample communication method with cost specified in Equation (4) a sequence of \(n\) independent and identically distributed (i.i.d.) samples \(X^{n},X_{i} P\), can be encoded using:

\[_{}(P^{ n}\|Q^{ n})+( _{}(P^{ n}\|Q^{ n})+1)+4.732\] \[=n\,_{}(P\|Q)+(n\,_{}(P\|Q)+1)+4.732\] (10)

expected number of bits, and the per sample cost converges to \(_{}(P\|Q)\) as \(n\).

Characterizations of the rate of source coding and sample communication in both matched and mismatched cases are presented in Table 1, where we can see that'moving' to source coding adds \((P)\) to the required rate, while using a mismatched distribution \(Q\) contributes \(_{}(P\|Q)\). The natural question to ask is whether the results of universal source coding can be extended to sample communication. Firstly, does the rate approach \(0\) when \(n\), and secondly, what is the total communication cost of communicating \(n\) samples? In the next section, we show that the answer to the first question is positive, and that the total communication cost is in the same order as the minimal redundancy, and the two quantities share a common lower bound.

## 5 Universal sample coding

The core idea is that, after observing \(t\) samples, the decoder can estimate \(P\), and use this estimate as the reference distribution \(Q\). As more samples are transmitted, the KL-divergence between the decoder's estimate and the true distribution \(P\) will diminish, which will translate into a lower communication cost for later samples. The proposed scheme consists of multiple rounds; in each round, a batch of samples are sent jointly, then the reference probability \(Q\) is updated.

**Theorem 5.1** (Universal sample coding).: _There exists a randomized encoding function \(f:_{k}\), and a randomized decoding function \(g:^{n}\), such that for any \(k\)-dimensional discrete distribution \(P_{k}\) over alphabet \(\) and a random string \(Z=\{0,1\}^{}\), such that \(g(f(P,Z),Z)=X^{n} P^{ n}\) and_

\[[\,(f(P,Z)|Z)]=L(n) V_{k}(c)(n)+o (n),\]

_where the expectation is over all random strings \(Z\), and_

\[V_{k}(c)()+ +1+5 2}{(1+c)}+1\] (11)

_for any \(c>0\)._

The random string \(Z\) is the common randomness shared between the encoder and decoder. The expected number of bits needed to communicate \(n\) samples from the any distribution \(P\) is similar to the redundancy of universal source coding, with some additional factors and per sample cost \(O() 0\) as \(n\).

### Probability estimation

As the samples are drawn independently, their order does not reveal any information about \(P\). Thus, any estimator, without loss of optimality, can be based only on the count of each symbol in the observed sequence. One of the canonical estimators is the add-\(1\) or Laplace estimator. As the name suggests, the estimated probability of any symbol is its count in the sequence plus \(1\), normalized to form a probability distribution. Laplace estimator belongs to a family of add-\(\) estimators, which rely on adding a constant \(\) to each count. Although no add-\(\) estimator achieves the optimal minmax KL-divergence rate (Krichevskiy, 1998), there exists an estimator (Braess and Sauer, 2004) that combines add-\(\), add-\(\) and add-\(1\) estimators, and achieves the optimal and universal decay of KL-divergence.

**Lemma 5.2**.: _(Braess and Sauer, 2004)1 Given \(n\) i.i.d. samples \(X^{n},X_{i} P\) from any distribution \(P_{k}\), there exists an estimator \((X^{n})\) such that:_

\[}_{X^{n} P^{ n}}\,_{ }(P\|(X^{n}))+o(n^{-1}).\] (12)

### Universal sample coding algorithm

If we could communicate a single sample with \(_{}(P\|Q)\) average bits, then alternating between sending a single sample and estimating \(P\) would be the best strategy. The algorithm would take \(n\) communication rounds and the total expected communication cost of this hypothetical algorithm would be:

\[ k+_{i=1}^{n-1}+o(i^{-1})(n)+o( n),\] (13)

where \( k\) is the cost of sending the first sample. However, the real cost of sending a sample, as shown in (4), includes a constant, which would dominate the total cost as \(n\) increases, making it linear instead of logarithmic. We propose a solution that requires only \(O( n)\) communication rounds, where in each round, exponentially more samples are sent. Algorithm 1 illustrates the pseudo code for the proposed solution. The proof of Theorem 5.1 is presented in Appendix B. We first upper bound the number of bits communicated in each round. Then we show that the total number of bits communicated over all rounds is \(V_{k}(c)(n)+o( n)\), where parameter \(c\) controls the size of the groups communicated at each round. The optimal choice of \(c\) depends on \(k\).

We have plotted the infimum of \(V_{k}(c)\) for different values of \(k\) in Figure 2. The value of the first term of \(V_{k}(c)\), that is \(\), is minimized as \(c\) approaches \(0\) and converges to \(\), which is equal to the optimal redundancy factor. As we show in the next section, it is also the lower bound for any universal sample communication algorithm. The ratio between the optimal value of \(V_{k}(c)\) and \(\) is shown in Figure 2, where it starts at around \(9\) and converges to \(1\) as \(k\) grows; this is the multiplicative gap between the upper bound on the communication cost of the proposed algorithm and the lower bound presented in the next section.

### Lower bound

The connection between universal source coding and universal sample coding is highlighted in the following theorem, where the lower bound of redundancy of universal source coding is the same as the communication cost of universal sample coding. This is not a coincidence, we can prove this theorem using exactly the same steps as the proof of optimality for the redundancy of universal source coding in Davisson et al. (1981).

**Theorem 5.3** (Universal sample coding - lower bound).: _For any universal sample coding algorithm (Theorem 5.1), there exists \(k\)-dimensional distribution \(P\), such that, the expected number of bits \(L(n)\) required to communicate \(n\) samples from \(P\) satisfies:_

\[L(n)(n)+O(1)\.\] (14)

Figure 2: The optimal universal sample coding factor \(_{c}V_{k}(c)\), normalized by \(\), optimized over the choice of \(c>0\) for \(k\)-dimensional distributions.

Proof.: Theorem 5.3 states that the claim holds for some distribution \(P\), thus it holds for the supremum over all \(k\)-dimensional distributions \(_{P}L(n)\). We will consider \(P\) as a random variable distributed according to \(\). Let \(\) be any distribution of \(k\)-dimensional distributions, then

\[_{P}L(n)}_{P}[L(n)],\] (15)

since the maximum is always greater than or equal to the average. The right hand side in Equation (15) corresponds to the expected number of bits required to communicate a sample \(X^{n}\) from \(P^{ n}\), where \(P\) is sampled from \(\). This is exactly the reverse channel coding problem, where the lower bound on the communication cost was shown in Harsha et al. (2010) to be the mutual information between the two random variables

\[}_{P}L(n) I(P,X^{n}).\] (16)

This quantity was bounded in Davisson et al. (1981) as

\[I(P,X^{n})(n)+O(1).\] (17)

To show (17), Davisson et al. (1981) considers a Markov chain \(P X^{n}\), where \(\) is a reconstruction of \(P\) based on samples \(X^{n}\). By data processing inequality, this Markov chain implies \(I(P,X^{n}) I(P,)\). The quantity \(I(P,)\) describes the minimum amount of information needed to compress \(P\) in lossy source coding, and can be bounded using Shannon lower bound Shannon (1959). The final lower bound is obtained by choosing an appropriate distribution \(\) and a distortion measure between distributions \(P\) and \(\).

Empirical evaluation

To corroborate the claims about universal sample coding made in Theorem 5.1, we compare the KL-divergence and total communication cost between the theory and numerical evaluations for different number of samples and dimensions. Each experiment is repeated \(1000\) times, where a probability distribution \(P\) is sampled from a \(k\)-dimensional Dirichlet distribution with concentration parameters \(^{k},_{i}=1\). Then, \(n\) samples are communicated using universal sample coding (Algorithm 1). We consider ordered random coding (Theis and Yosri, 2022), shown in Appendix C, as the underlying sample communication method (line 7, Algorithm 1). In Figure 3, we observe that the measured KL-divergence \(_{}(P\|)\) between the true probability \(P\) and the estimate \(\) from the estimator in Lemma 5.2 follows the predicted values across a range of communicated samples. In Figure 4, we fix the number of samples to \(n=128\), but vary the dimension \(k\), and show that the KL-divergence lies below the bound from Lemma 5.2. In Figure 5, we plot the total communication cost for various number of samples, which is contained between the asymptotic upper and lower bounds, \(V_{k}(c)(n)\) (Theorem 5.1) and \((n)\) (Theorem 5.3), respectively.

## 7 Federated learning (FL)

To show the efficacy of the universal sample coding in practice, we first apply it to the Federated Probabilistic Mask Training (FedPM) algorithm proposed in Isik et al. (2024). In FedPM, the weights of the neural network \(w^{M}\) are randomly initialized, fixed, and known both to the clients and the central server. Each weight \(w_{i}\) has an associated probability \(_{i}\) of that weight being masked. The training of the network consists of finding the suitable mask probabilities \(^{M}\) for all the weights using gradient descent. To test the network, a binary mask is sampled, and the effective weights of the network become \(w_{i}(_{i})\). In a single learning round, the server broadcasts the global mask \(\) to each client, which then trains an updated version \(^{}\) using its local data and communicates a sample \((^{})\) from its updated mask distribution back to the central server. The new global mask probability \(\) is then estimated from all the received samples. The sample from \(P=^{}\) is communicated using the coding probability \(Q=\), which is known to both the client and the server, thereby achieving a communication cost of approximately \(_{}(^{}\|)\). To achieve a more accurate estimate of the global mask probability, we propose communicating multiple mask samples per client using universal sample coding, along with the addition of the global prior \(\).

For the experiments, we follow the same setup as Isik et al. (2024) using their CONV-6 architecture on classification of CIFAR-10 images, where the data is distributed among \(10\) clients. A challenging scenario considered in that work is of congested clients, where only a fraction of clients contribute in each learning round. The results of our experiments are summarized in Table 2. The FL training was conducted on two Nvidia RTX 3090 GPUs, each with 25 GB of memory, with each experiment taking approximately 12 hours. The training, including preliminary experiments, took 30 days of gpu-time. If all the clients participate in each learning round the final test accuracy is around \(80\%\). However, when only \(1\) out of \(10\) clients participate in the learning round the accuracy decreases to \(75\%\). By communicating multiple samples in each round, we can achieve test accuracy close to the fully uncongested case. However, as argued in this work, sending multiple samples from the same distribution can be made more communication efficient by estimating the true probability from previous samples. Indeed, we observe a \(37\%\) reduction in communication cost by employing universal sample coding with prior \(\), compared to only using the prior \(\).

To incorporate the prior \(\) into the proposed coding scheme, we use a Bayesian estimator. Although the mask is binary, we describe a general case with \(k\) possible outcomes. Let \(_{k}\) be the prior probability, and \(^{}\) be a random variable distributed according to the Dirichlet distribution with parameters \(^{k}\), for \(j\{0,,k-1\}\): \(_{j}=_{j}+_{l=1}^{G}\{l=j\},\) where \(\{\}\) is the indicator function, \(G\) is the number of samples communicated so far, and \(^{+}\) is a hyperparameter controlling the reliance on the prior. The optimal coding probability \(Q=_{^{} P_{^{}}}\), \(^{}\), i.e., for \(j\{0,,k-1\}\), \(Q_{j}=}{_{i}}\), replaces the one in line 5 in Algorithm 1. For the binary mask case, where \(k=2\), we have \(=[1-,]\), and \(^{}\) is characterized by a \(2\)-dimensional Dirichlet distribution (or equivalently, a Beta distribution).

## 8 Limitations and further work

Both the lower and upper bounds of communication rate (Theorems 5.1, 5.3) include a factor \(k\)--the cardinality of the sample space. Consequently, universal sample coding cannot be directly applied to continuous random variables, as \(k\) goes to infinity so does the required communication. Universal sample coding consists of two main components: sample communication and probability estimation. While sample communication can be applied to continuous distributions, the estimation component fails because no finite number of samples can fully specify an unrestricted continuous distribution.

We propose two potential avenues for extending universal sample coding for continuous variables. Firstly, by imposing additional assumptions on the probability distribution, we could substitute counting-based estimation with a Bayesian approach. The reference distribution \(Q\) would be the posterior, updated continually with each new sample. Second, we could explore a model-based approach where a common model describes \(Q\). This model would be incrementally fine-tuned with the communicated samples to more accurately reflect their underlying distribution. As evidence for the potential of this idea, we apply sample communication for generating samples from a large language model on a server, using a smaller reference model at the client. While the space of text is not continuous, it is too large to estimate using count-based methods. The experimental setup and further discussion are detailed in Appendix D. As demonstrated in Table 3, the application of sample communication results in a 4- to 16-fold reduction in communication rate, depending on the auxiliary model employed. This motivates further exploration into model fine-tuning effects. Although this research currently focuses on text generation, future work will aim to extend these results to image and video generation, which represent a substantial portion of network traffic. Furthermore, determining the optimal use of new samples in the learning process presents a compelling challenge, potentially linking to advancements in online learning methods.

The universal sample communication problem, introduced in this work, involves communicating multiple samples drawn from the same probability distribution \(P\), which is unknown to the decoder. An alternative but related problem would involve generating multiple samples, each from a different conditional distribution \(P_{X|Z=z}\). In such a case, the optimal coding distribution would be the marginal distribution \(Q=P_{X}\), and the optimal per-sample communication cost would be \(I(X;Z)\). In the universal setting, the decoder would not know this marginal distribution. Thus, a similar estimation and sample communication approach, as demonstrated in this work, could be applied to address this

   FL scheme & FedPM & FedPM & FedPM &  \\  \# clients per round & \(10\) & \(1\) & \(1\) & \(1\) \\ \# samples per client & \(1\) & \(1\) & \(7\) & \(7\) \\ final test accuracy & \(0.8025\) & \(0.7516\) & \(0.8028\) & \(0.8039\) \\ \# bits per parameter & \(0.6058\) & \(0.0340\) & \(0.3955\) & \(0.2482\) \\ \#bits per parameter & \(0.0606\) & \(0.0340\) & \(0.0565\) & \(0.0355\) \\ \& client \& sample & & \\   

Table 2: Accuracy and communication cost of FedPM for different simulation scenarios. Values are averaged over \(20\) runs, with standard deviation bellow \(0.003\).

    & \# bits per token \\  plain text & 15.617 \\ \(}(P_{13B})\) & 4.315 \\ \(}_{}(P_{13B}||Q_{1253})\) & 1.014 \\ \(}_{}(P_{13B}||Q_{350})\) & 0.824 \\ \(}_{}(P_{13B}||Q_{1.38})\) & 0.420 \\ \(}_{}(P_{13B}||Q_{2.78})\) & 0.330 \\ \(}_{}(P_{13B}||Q_{6.78})\) & 0.266 \\   

Table 3: Per token cost of sending samples from \(13\)B model. Entropy is a lower bound for source coding, while the KL-divergence serves as a bound for sample communication.

problem. However, further analysis is required to validate the communication performance of such a scheme.

## 9 Conclusion

As AI tools become ever more prevalent, it becomes increasingly important to consider the resources they consume. This paper analyzes the communication cost of transmitting samples from probability distributions, which can be seen as sending the outcomes of generative models, and show that by leveraging the fact that many generative AI applications only seek to communicate generic samples from a distribution rather than a specific sample, we can reduce the associated communication cost significantly. In this paper, we proposed _universal sample coding_ for transmitting multiple samples from a distribution, which achieves the information theoretic lower bound by up to a multiplicative constant. We also applied it to a FL framework showing a communication cost reduction of \(37\%\), and to remote text generation problem using large language models, showing an up to \(16\)-fold communication cost reduction compared to sample-wise entropy coding methods.

#### Acknowledgements

This research was supported by the United Kingdom Engineering and Physical Sciences Research Council (EPSRC) for the projects AIR (ERC Consolidator Grant, EP/X030806/1) and INFORMEDAI (EP/Y028732/1).

We would like to thank Francesco Pase for sharing the federated learning codebase of Isik et al. (2024).