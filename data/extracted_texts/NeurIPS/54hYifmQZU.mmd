# Quantifying the Cost of Learning in Queueing Systems

Daniel Freund

MIT

Cambridge, MA 02139

dfreund@mit.edu

&Thodoris Lykouris

MIT

Cambridge, MA 02139

lykouris@mit.edu

&Wentao Weng

MIT

Cambridge, MA 02139

wweng@mit.edu

###### Abstract

Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of the system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.

In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the _Cost of Learning in Queueing (CLQ)_, a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues. In establishing our results, we propose a unified analysis framework for CLQ that bridges Lyapunov and bandit analysis, provides guarantees for a wide range of algorithms, and could be of independent interest.1

## 1 Introduction

Queueing systems are widely used stochastic models that capture congestion when services are limited. These models have two main components: jobs and servers. Jobs wait in queues and have different types. Servers differ in capabilities and speed. For example, in content moderation of online platforms , jobs are user posts with types defined by contents, languages and suspected violation types; servers are human reviewers who decide whether a post is benign or harmful. Moreover, job types can change over time upon receiving service. For instance, in a hospital, patients and doctors can be modeled as jobs and servers. A patient in the queue for emergent care can become a patient in the queue for surgery after seeing a doctor at the emergency department . That is, queues can form a network due to jobs transitioning in types. Queueing systems also find applications in other domains such as call centers , communication networks  and computer systems .

The single-queue multi-server model is a simple example to illustrate the dynamics and decisions in queueing systems. In this model, there is one queue and K servers operating in discrete periods. In each period, a new job arrives with probability \(\). Servers have different service rates \(_{1},,_{K}\). The decision maker (DM) selects a server to serve the first job in the queue if there is any. If server \(j\) is selected, the first job in the queue leaves with probability \(_{j}\). The DM aims to minimize the average wait of each job, which is equivalent to minimizing the queue length. The optimal policy thus selects the server with the highest service rate \(^{}=_{j}_{j}\); the usual regime of interest is one where the system is _stabilizable_, i.e., \(^{}>\), which ensures that the queue length does not grow toinfinity under an optimal policy. Of course, this policy requires perfect knowledge of the service rates. Under parameter uncertainty, the DM must balance the trade-off between exploring a server with an uncertain rate or exploiting a server with the highest observed service rate.

A recent stream of work studies efficient learning algorithms for queueing systems. First proposed by , and later used by [24; 38], _queueing regret_ is a common metric to evaluate learning efficiency in queueing systems. In the single-queue multi-server model, let \(Q(T,)\) and \(Q^{}(T)\) be the number of jobs in period \(T\) under a policy \(\) and under the optimal policy respectively. Queueing regret is defined as either the last-iterate difference in expected queue length, i.e., \([Q(T,)-Q^{}(T)]\)[41; 24], or the time-average version \([_{t=1}^{T}Q(t,)-Q^{}(t)]\). In the stabilizable case (\(<^{}\)), the goal is to bound its scaling relative to the time horizon; examples include \(o(T)\), \((1/T)\), and \(O(1/T)\).

In this paper, we argue that an asymptotic metric for per-period queue length does not capture the statistical complexity of learning in queueing systems. This is because, in a queueing system, learning happens in initial periods while queueing regret focuses only on late periods. Whereas cumulative regret in multi-armed bandits is non-decreasing in \(T\), the difference in queue lengths between a learning policy and the benchmark optimal policy eventually decreases since the policy eventually learns the parameters (see Figure 1). This leads to the two main questions of this paper:

_1. What metric characterizes the statistical complexity of learning in queueing systems?_

_2. What are efficient learning algorithms for queueing systems?_

Our work studies these questions in general queueing systems that go beyond the single-queue multi-server model and can capture settings such as the hospital and content moderation examples.

Cost of learning in queueing.Tackling the first question, we propose the _Cost of Learning in Queueing_ (CLQ) to capture the efficiency of a learning algorithm in queueing systems. The CLQ of a learning policy is defined as the maximum difference of its time-averaged queue length and that of any other policy (with knowledge of parameters) over the entire horizon (see Fig 1 on the right). In contrast to queueing regret, CLQ is 1) a finite-time metric that captures the learning efficiency in early periods and 2) focused on time-averaged queue length instead of per-period queue length. This is favorable as for any periods \(1,,T\), the time-averaged queue length is related to the average wait time by Little's Law. The formal definition of CLQ can be found in Section 3.

Lower bound of CLQ (Theorem 1).To characterize the statistical complexity of learning in queueing systems, we consider the simplest non-trivial stabilizable setting that involves one queue and \(K\) servers. It is known that the queue length scales as \(O(1/)\) under the optimal policy, where \(=^{}-\) is the traffic slackness of the system. Fixing \(\) and the number of servers \(K\), we establish

Figure 1: Expected per-period and time-averaged queue lengths of UCB and Q-UCB  in a single-queue setting with \(K=5,=0.45,=(0.045,0.35,0.35,0.35,0.55)\); results are averaged over 50 runs. The difference between both algorithmsâ€™ queue lengths is indistinguishable asymptotically (left figure) though they clearly differ in their learning efficiency for early periods as illustrated by _Cost of Learning in Queueing_, the metric that our work introduces (right figure).

a worst-case lower bound \(()\) of CLQ. That is, for any \(,K\) and a fixed policy, there always exists a setting of arrival and service rates, such that the CLQ of this policy is at least \(()\). Combined with the \(O(1/)\) optimal queue length, this lower bound result shows that the effect of learning dominates the performance of queueing systems when \(K\) is large (as it may increase the maximum time-averaged queue length by a factor of \(K\)). This is shown in Figure 1 (right) where the peak of time-averaged queue lengths of the optimal policy with knowledge of parameters is much lower than that of the other two policies (our Algorithm 1 and Q-UCB ).

An efficient algorithm for single-queue multi-server systems (Theorem 2).Given the above lower bound, we show that the Upper Confidence Bound (UCB) algorithm attains an optimal CLQ up to a logarithmic factor in the single-queue multi-server setting. Our analysis is motivated by Fig. 1 (right) where the time-averaged queue length initially increases and then decreases. Based on this pattern, we divide the horizon into an initial _learning_ stage and a later _regenerate_ stage.

In the learning stage, the queue length increases similar to the multi-armed bandit regret. We formalize this observation by coupling the queue under a policy \(\) with a nearly-optimal queue and show that their difference is captured by the _satisficing regret_ of \(\). Satisficing regret resembles the classical multi-armed bandit regret but disregards the loss of choosing a nearly optimal server (see Eq. (6)); this concept is studied from a Bayesian perspective in multi-armed bandits . Nevertheless, our result in the learning stage is not sufficient as the satisficing regret eventually goes to infinity.

In the regenerate stage, queue lengths decrease as the policy has learned the parameters sufficiently well; the queue then behaves similarly as under the optimal policy and stabilizes the system. To capture this observation, we use Lyapunov analysis and show that the time-averaged queue length for the initial \(T\) periods scales as the optimal queue length, but with an additional term depending on the second moment of satisficing regret divided by \(T\). Hence, as \(T\) increases, the impact of learning gradually disappears. Combining the results in the learning and regenerate stages, we obtain a tight CLQ bound of UCB for the single-queue multi-server setting.

Efficient algorithms for queueing networks (Theorem 3)We next generalize the above result to queueing networks that include multiple queues, multiple servers, and transitions of served jobs from servers to queues. For this setting, we build on the celebrated BackPressure policy that stabilizes a queueing network with knowledge of system parameters . We propose BackPressure-UCB as a new algorithm to transform BackPressure into a learning algorithm with appropriate estimates for system parameters and show that its cost of learning scales near-optimally as \((1/)\) with respect to the traffic slackness \(\) (Definition 2). This result extends our framework for single-queue multi-server settings through a coupling approach that reduces the loss incurred by learning in a high-dimensional vector of queue-lengths to a scalar-valued potential function. To the best of our knowledge, this is the first efficient learning algorithm for general queueing networks (see related work for a discussion).

Related workA recent line of work studies online learning in queueing systems . To capture uncertainty in services,  studies a single-queue setting in which the DM selects a mode of service in each period and the job service time varies between modes (the dependence is a priori unknown and revealed to the DM after the service). The metric of interest is the _queueing regret_, i.e., the difference of queue length between an algorithm and the optimal policy, for which the authors show a sublinear bound.  considers the same single-queue multi-server setting as ours and show that a forced exploration algorithm achieves a queueing regret scaling of \((1/T)\) (under strong structural assumptions this result extends to multiple queues).  shows that by probing servers when the queue is idle, it is possible to give an algorithm with queueing regret converging as \(O(1/T)\). However, with respect to the traffic slackness \( 0^{+}\), both bounds yield suboptimal CLQ:  gives at least \(O(1/^{2})\) and  gives at least \(O(1/^{4})\) (see Appendices A.1, A.2 in ). In the analysis of , forced exploration is used for low adaptive regret, i.e., regret over any interval ; no such guarantee is known for adaptive exploration. But as noted by our Figure 1 and [24, Figure 2], an adaptive exploration algorithm like UCB has a better early-stage performance than Q-UCB. Using our framework in Section 4, we show that UCB indeed has a near-optimal CLQ that scales as \(O()\). Our framework also allows us to show that Q-UCB enjoys a CLQ scaling as \(O(+^{3})\) (Appendix A.1 in ). This improves the guarantee implied by  and shows the inefficiency due to forced exploration is about \(O(^{3})\) and that Q-UCB has both strong transient (CLQ) and asymptotic (queueing regret) performance.

Focusing on the scaling of queueing regret,  and  study the scheduling in multi-queue settings (with  also considering job abandonment),  study learning for a load balancing model,  studies pricing and capacity sizing for a single-queue single-server model with unknown parameters. For more general settings,  designs a Bayesian learning algorithm for Markov Decision Processes with countable state spaces (of which queueing systems are special cases) where parameters are sampled from a known prior over a restricted parameter space; in contrast, our paper does not assume any prior of the unknown parameters. The main difference between all of these works and ours is that we focus on how the maximum time-averaged queue lengths scales with respect to system parameters (traffic slackness and number of queues and servers), not on how the queue lengths scale as time grows. Apart from the stochastic learning setting we focus on, there are works that tackle adversarial learning in queueing systems ; these require substantially different algorithms and analyses.

Going beyond queueing regret, there are papers focusing on finite-time queue length guarantees. In a multi-queue multi-server setting, it is known that the MaxWeight algorithm has a polynomial queue length for stabilizable systems. However, it requires knowledge of system parameters. For a joint scheduling and utility maximization problem,  combines MaxWeight with forced exploration to handle parameter uncertainty. By selecting a suitable window for sample collection, their guarantee corresponds to a CLQ bound of at least \(O(K^{4}/^{3})\) for our single-queue setting (see Appendix A.3 in ).  studies a multi-queue multi-server setting and propose a frame-based learning algorithm based on MaxWeight. They focus on a greedy approximation which has polynomial queue lengths when the system is stabilizable with twice of the arrival rates.  considers a non-stationary setting and shows that combining MaxWeight with discounted UCB estimation leads to stability and polynomial queue length that scales as \((1/^{3})\) (Appendix A.4 in ). There is also a line of work studying decentralized learning in multi-queue multi-server settings.  assumes queues are selfish and derives conditions under which a no-regret learning algorithm is stable; this is generalized to queueing networks in which queues and servers form a directed acyclic graph by .  allows collaborative agents and gives an algorithm with maximum stability, although the queue length scales exponentially in the number of servers.  designs a decentralized learning version of MaxWeight and shows that the algorithm always stabilizes the system with polynomial queue lengths \((1/^{3})\) (Appendix A.5 in ). In contrast to the above, our work shows for the centralized setting that MaxWeight with UCB achieves the near-optimal time-averaged queue length guarantee of \((1/)\).

Our paper extends the ability of online learning to general single-class queueing networks . The literature considers different complications that arise in these settings, including jobs of different classes and servers that give service simultaneously to different jobs . For the class of networks we consider, it is known that BackPressure can stabilize the system with knowledge of system parameters . Noted in , one potential drawback of BackPressure is its need of full knowledge of job transition probabilities. In this regard, our paper contributes to the literature by proposing the first BackPressure-based algorithm that stabilizes queueing networks without knowledge of system parameters.

Moving beyond our focus on uncertainties in services, an orthogonal line of work studies uncertainties in job types.  considers a single server setting where an arriving job belongs to one of two types; but the true type is unknown and is learned by services. They devise a policy that optimizes a linear function of the numbers of correctly identified jobs and the waiting time.  studies a similar setting with two types of servers where jobs can route from one server to the others. They focus on the impact on stability due to job type uncertainties.  consider multiple job types and server types. Viewing Bayesian updates as job type transitions, they use queueing networks to model the job learning process and give stable algorithms based on BackPressure.  consider online matchings between jobs with unknown payoffs and servers where the goal is to maximize the total payoffs subject to stability. As noted in , one key assumption of this line of work is the perfect knowledge of server types and transition probability. Our result thus serves as a step to consider both server uncertainties and job uncertainties, at least in a context without payoffs.

Concurrently to our work,  proposes a frame-based MaxWeight algorithm with sliding-window UCB for scheduling in a general multi-queue multi-server system with non-stationary service rates. With a suitable frame size (depending on the traffic slackness), they show stability of the algorithm and obtain a queue length bound of \((1/^{3})\) in the stationary setting (Appendix A.6 in ).

Model

We consider a sequential learning setting where a decision maker (DM) repeatedly schedules jobs to a set of servers of unknown quality over discrete time periods \(t=1,2,\). For any \(T\), we refer to the initial \(T\) periods as the time horizon \(T\). To ease exposition, we first describe the simpler setting where there is only one job type (queue) and subsequently extend our approach to a general setting with multiple queues that interact through a network structure.

Single-queue multi-server system.A single-queue multi-server system is specified by a tuple \((,,)\). There is one queue of jobs and a set of servers \(\) with \(||=K\). The arrival rate of jobs is \(\), that is, in each period there is a probability \(\) that a new job arrives to the queue. The service rate of a server \(k\), that is, the probability it successfully serves the job it is scheduled to work on, is \(_{k}\). Let \(Q(t)\) be the number of jobs at the start of period \(t\). Initially there is no job and \(Q(1)=0\).

Figure 2 summarizes the events that occur in each period \(t\). If there is no job in the queue, i.e., \(Q(t)=0\), then the DM selects no server; to ease notation, they select the null server \(J(t)=.\)2 Otherwise, the DM selects a server \(J(t)\) and requests service for the first job in the queue.

The service request is successful with probability \(_{J(t)}\) and the job then leaves the system; otherwise, it remains in the queue. At the end of the period, a new job arrives with probability \(\). We assume that arrival and service events are independent. Let \(A(t)\) and \(\{S_{k}(t)\}_{k}\) be a set of independent Bernoulli random variables such that \([A(t)]=\) and \([S_{k}(t)]=_{k}\) for \(k\); for the null server, \(_{}=S_{}(t)=0\) for all \(t\). The queue length dynamics are thus given by

\[Q(t+1)=Q(t)-S_{J(t)}(t)+A(t).\]

A non-anticipatory policy \(\) for the DM maps for every period \(t\) the historical observations until \(t\), i.e., \((A(),S_{J()}())_{<t}\), to a server \(J(t)\{\}\). We define \(Q(t,)\) as the queue length in period \(t\) under policy \(\). The DM's goal is to select a non-anticipatory policy \(\) such that for any time horizon \(T 1\), the expected time-averaged queue length \(_{t T}[Q(t,)]\) is as small as possible.

When service rates are known, the policy \(^{*}\) selecting the server with the highest service rate \(^{*}\) in every period (unless the queue is empty) minimizes the expected time-averaged queue length for any time horizon . If \(^{*}\), even under \(^{*}\), the expected time-averaged queue length goes to infinity as the time horizon increases. We thus assume \(<^{*}\), in which case, the system is _stabilizable_, i.e., the expected time-averaged queue length under \(^{*}\) is bounded by a constant over the entire time horizon. We next define the _traffic slackness_ of this system:

**Definition 1**.: _A single-queue multi-server system has a traffic slackness \((0,1]\) if \(+^{*}\)._

A larger traffic slackness implies that a system is easier to stabilize. It is known that the policy \(^{*}\) obtains an expected time-averaged queue length of the order of \(\).

Queueing network.A queueing network extends the above case by having multiple queues and probabilistic job transitions after service completion; our model here resembles the one in . A queueing network is defined by a tuple \((,,,,},, },},)\), where \(}=\{_{n}\}_{n}\), \(}=\{_{k}\}_{k}\) and \(=(p_{k,n})_{k,n\{\}}\) such that \(_{n\{\}}p_{k,n}=1, k\). In contrast to the single-queue case, there is now a set of queues \(\) with cardinality \(N\) and a virtual queue \(\) to which jobs transition once they leave the system. Each queue \(n\) has a set of servers \(_{n}\), each of which

Figure 2: Flowchart for a single-queue multi-server system.

belongs to a single queue. As before, the service rate of server \(k\) is \(_{k}\). The set \(_{k}\) contains the destination queues of server \(k\) (and can include the virtual queue \(\)).

In each period \(t\), the DM selects a set of servers to schedule jobs to and, if a service request from queue \(n\) to server \(k\) is successful, a job from queue \(n\) transitions to a queue \(n^{}_{k}\) with probability \(p_{k,n^{}}\) (this implies \(p_{k,n^{}}=0\) if \(n^{}_{k}\)). The selected set of servers comes from a set of feasible schedules \(\{0,1\}^{}\), which captures interference between servers. We require that for any queue, the number of selected servers is no larger than the number of jobs in this queue.3 Formally, letting \(_{k}=1\) if schedule \(\) selects server \(k\) and denoting \((t)=(Q_{n}(t))_{n}\) as the queue length vector at the beginning of period \(t\), the set of feasible schedules in this period is

\[_{t}=\{_{k _{n}}_{k} Q_{n}(t), n\}\]

and the DM's decision in period \(t\) is to select a schedule \((t)_{t}\). Following , we assume that any subset of a feasible schedule is still feasible, i.e., if \(\) and \(^{}_{k}_{k}\; k\), then \(^{}\).

We now formalize the arrival and service dynamics in every period, which are captured by the independent random variables \(\{(t),\{S_{k}(t)\}_{k}\}_{t}\). The arrival vector \((t)=\{A_{n}(t)\}_{n}\) consists of (possibly correlated) random variables \(A_{n}(t)\) taking value in \(}\{0,1\}^{}\); we denote its distribution by \(\) and let \([A_{n}(t)]=_{n}()\) with \(=(_{n}())_{n}\). The service \(S_{k}(t)\) for each server \(k\) is a Bernoulli random variable indicating whether the selected service request was successful.4 To formalize the job transition, let \(_{k}(t)=(L_{k,n}(t))_{n\{\}}\) be a random vector over \(\{0,1\}^{\{\}}\) for server \(k\) independent of other randomness such that \(\{L_{k,n}(t)=1\}=p_{k,n}\) and \(_{n\{\}}L_{k,n}(t)=1\). The queueing dynamic is given by

\[Q_{n}(t+1)=Q_{n}(t)-_{k_{n}}_{k}(t)S_{k}(t)+A_{n}(t)+ _{k^{}}_{k^{}}(t)S_{k^{}}(t)L_{k^{ },n}(t). \]

We assume that the DM has knowledge of which policies are allowed, i.e., they know \(\) and \(}\), but has no prior knowledge of the rates \(,,\) and the set \(}\). In period \(t\), the observed history is the set \(\{A_{n}()\}_{n},\{S_{k}()L_{k,n}()\}_{n ,k_{k}()=1}_{<t}\) that includes transition information on top of arrivals and services. Note that a job transition is only observed when the server is selected and the service is successful. Similar to before, a non-anticipatory policy \(\) maps an observed history to a feasible schedule; we let \(Q_{n}(t,)\) be the length of queue \(n\) in period \(t\) under this policy.

Unlike the single-queue case, it is usually difficult to find the optimal policy for a queueing network even with known system parameters. Fortunately, if the system is stabilizable, i.e., \(_{t}_{t T}[\|(t)\|_{1 }]<\) under some scheduling policy, then the arrival rate vector must be within the _capacity region_ of the servers . Formally, let \(=\{^{}_{ }_{}=1\}\) be the probability simplex over \(\). A distribution \(\) in \(\) can be viewed as the frequency of a policy using each schedule \(\), and the effective service rate queue \(n\) can get is given by \(_{n}^{}()=_{}_{}(_{k_{n }}_{k}_{k}-_{k^{}}_{k^{}}_{k^ {}}p_{k^{},n})\); this includes both job inflow and outflow. We denote the effective service rate vector for a schedule distribution \(\) by \(^{}()\). Then, the capacity region is \((,,}, )=\{^{}() \}\). For a queueing network to be stabilizable, we must have \(()(, ,},)\). As in the single-queue case, we further assume that the system has a positive traffic slackness and let \(\) denote a vector of \(1\)s with suitable dimension.

**Definition 2**.: _A queueing network has traffic slackness \((0,1]\) if \(()+( ,,},)\)._

We also study a special case of queueing networks, multi-queue multi-server systems, where jobs immediately leave after a successful service, i.e., \(_{k}=\{\}\) for all \(k\); this extends the models in  mentioned above. Since the transition probability matrix \(\) is trivial (\(p_{k,}=1, k\)), we denote the capacity region of a multi-queue multi-server system by \((,,})\).

Main results: the statistical complexity of learning in queueing

This section presents our main results on the statistical complexity of learning in queueing systems. We first define the _Cost of Learning in Queueing_, or CLQ as a shorthand, a metric capturing this complexity and provide a lower bound for the single-queue multi-server setting. Motivated by this, we design an efficient algorithm for the single-queue multi-server setting with a matching CLQ and then extend this to the multi-queue multi-server and queueing network systems.

### Cost of Learning in Queueing

We first consider learning in the single-queue multi-server setting. Previous works on learning in queueing systems focus on the queueing regret \([Q(T,)-Q(T,^{})]\) in the asymptotic regime of \(T\). The starting point of our work stems from the observation that an asymptotic metric, which measures performance in late periods, cannot capture the complexity of learning as learning happens in initial periods (recall the left of Figure 1). In addition, a guarantee on per-period queue length cannot easily translate to the service experience (or wait time) of jobs.

Motivated by the above insufficiency of queueing regret, we define the _Cost of Learning in Queueing_ (or CLQ) as the maximum increase in expected time-averaged queue lengths under policy \(\) compared with the optimal policy. Specifically, we define the single-queue CLQ as:

\[^{}(,,)=_{T 1} ^{T}[Q(t,)-Q(t,^{})]}{T}. \]

As shown in Figure 1 (right), CLQ is a finite-time metric and explicitly takes into account how fast learning occurs in the initial periods. In addition, a bound on the maximum increase in time-averaged queue length translates approximately (via Little's Law ) to a bound on the increase in average job wait times.

Given that the traffic slackness measures the difficulty of stabilizing a system, we also consider the worst-case cost of learning in queueing over all pairs of \((,)\) with a fixed traffic slackness \(\). In a slight abuse of notation, we overload \(^{}\) to also denote this worst-case value, i.e.,

\[^{}(K,,)=_{[0,1), ^{K}:\;+_{  k}^{}(,,). \]

Our goal is to design a policy \(\), without knowledge of the arrival rate, the service rates, and the traffic slackness, that achieves low worst-case cost of learning in a single-queue multi-server system.

We can extend the definition of CLQ to the multi-queue multi-server and the queueing network settings. Since the optimal policy is difficult to design, we instead define CLQ for a policy \(\) by comparing it with any non-anticipatory policy (which makes decisions only based on the history):

\[^{}(,, ,},)=_{^{}}_{T 1}^{T}_{n }[Q_{n}(t,)-Q_{n}(t,^{})]}{T}. \]

\[^{}(,,,},},,) =_{^{}}_{T 1}^{T}_{n }[Q_{n}(t,)-Q_{n}(t,^{})]}{T} \]

As in the single-queue setting, we can define the worst-case cost of learning for a fixed structure \(}\), \(\), \(}\), \(}\) and a traffic slackness \(\) as the supremum across all arrival, service, and transition rates with this traffic slackness. With the same slight abuse of notation as before, we denote these quantities by \(^{}(},, },,)\) and \(^{}(},, },},,)\).

### Lower bound on the cost of learning in queueing

Our first result establishes a lower bound on \(^{}(K,,)\). In particular, for any feasible policy \(\), we show a lower bound of \(()\) for sufficiently large \(K\). With known parameters, the optimal time-averaged queue length is of the order of \(}{{}}\). Hence, our result shows that the cost of learning is non-negligible in queueing systems when there are many servers. For fixed \(K\) and \(\), our lower bound considers the cost of learning of the worst-case setting and is instance-independent.

**Theorem 1**.: _For \(K 2^{14},(0,0.25]\) and feasible policy \(\), we have \(^{}(K,,)}\)._Although our proof is based on the distribution-free lower bound \(()\) for classical multi-armed bandits , this result does not apply directly to our setting. In particular, suppose the queue in our system is never empty. Then the accumulated loss in service of a policy is exactly the _regret_ in bandits and the lower bound implies that any feasible policy serves at least \(()\) jobs fewer than the optimal policy in the first \(T\) periods. However, due to the traffic slackness, the queue does get empty under the optimal policy, and in periods when this occurs, the optimal policy also does not receive service. As a result, the queue length of a learning policy could be lower than \(()\) despite the loss of service compared with the optimal policy.

We next discuss the intuition (formal proof in Appendix C.1 of ). Fixing \(K\) and \(\), suppose the gap in service rates between the optimal server and others is \(2\). Then for any \(t\) in a time horizon \(T=O(})\), the number of arrivals in the first \(t\) periods is around \( t\) and the potential service of the optimal server is around \((+)t\). By the multi-armed bandit lower bound, the total service of a policy is at most around \( t+ t- t\) since \(t T=O(})\). Then the combined service rate of servers chosen in the first \(T\) periods, i.e., \(_{t T}_{J(t)}\), is strictly bounded from above by the total arrival rate \( t\). A carefully constructed example shows that the number of unserved jobs is around \( t\) for every \(t T\) and thus the time-averaged queue length for the horizon \(T\) is of the order of \( T=O()\).

**Remark 1**.: _In [24, Proposition 3], the authors established an instance-dependent lower bound on \([Q(t,)-Q^{*}(t)]\). The implied CLQ lower bound is weaker than ours (\((K)\), see Appendix C.2 in ) and is constrained to \(-\)consistent policy \(\) (see definition 1 in ) whereas ours does not._

### Upper bound on the cost of learning in queueing

Motivated by the lower bound, we propose efficient algorithms with a focus on heavy-traffic optimality, i.e., ensuring \(=(}{{}})\) as \( 0^{+}\). The rationale is that stabilizing the system with unknown parameters is more difficult when the traffic slackness is lower as an efficient algorithm must strive to learn parameters more accurately. We establish below that the classical upper confidence bound policy (UCB, see Algorithm 1) achieves near-optimal \(=()\) for any \(K\) and \(\) in the single-queue multi-server setting with no prior information of any system parameters.

**Theorem 2**.: _For any \(K 1,(0,1]\), \(^{}(K,,)}{{}})}{}\)._

The proof of the theorem (Section 4) bridges Lyapunov and bandit analysis, makes an interesting connection to _satisficing regret_, and is a main technical contribution of our work.

We next extend our approach to the queueing network setting. Fixing the system structure \(},,},}\), we define \(M_{}}=_{}}}_{n }A_{n}\) and \(M_{,}}=_{}_{n }_{k_{n}}_{k}\), to be the maximum number of new job arrivals and the maximum number of selected servers per period. Further, for queueing networks, we also define the quantity \(M_{}}=_{k}|_{k}|^{2}\), related to the number of queues each server may see its jobs transition to. The following result shows that the worst-case cost of learning of our algorithm BackPressure-UCB (BP-UCB as a short-hand, [12, Algorithm 3]) has optimal dependence on \(\). The proof of this Theorem is provided in [12, Section 6].

**Theorem 3**.: _For any \(},,},}\) and traffic slackness \((0,1]\), we have_

\[^{}(},,},},,)(32M_{}}+2^{12}M_{}}M_{,}}^{2} (1+(M_{}}M_{}}M_{,} }/)))}{}.\]

Our proof builds on the special case of multi-queue multi-server systems, for which we provide Algorithm MaxWeight-UCB with a corresponding performance guarantee (see [12, Section 5]).

## 4 Proof for the single-queue multi-server setting (Theorem 2)

In this section, we bound the CLQ of UCB for the single-queue multi-server setting (Theorem 2). In each period \(t\), when the queue is non-empty, UCB selects a server with the highest upper confidence bound estimation \(_{k}(t)=(1,_{k}(t)+(t)}})\) where \(_{k}(t)\) is the sample mean of services and \(C_{k}(t)\) is the number of times server \(k\) is selected in the first \(t-1\) periods.

```
Sample mean \(_{k}(1) 0\), number of samples \(C_{k}(1) 0\) for \(k\{\}\), queue \(Q(1) 0\) for\(t=1\)do if\(_{k}(t)=(1,_{k}(t)+(t)}} ), k\)if\(Q(t)>0\)then\(J(t)_{k}_{k}(t)\); else\(J(t)\) /* Update queue length & estimates based on \(S_{J(t)}(t)\), \(A(t)\), and \(J(t)\) */
1\(Q(t+1) Q(t)-S_{J(t)}(t)+A(t)\)
2\(C_{J(t)}(t+1) C_{J(t)}(t)+1,_{J(t)}(t+1) {C_{J(t)}(t)_{J(t)}(t)+S_{J(t)}(t)}{C_{J(t)}(t+1)}\)
3for\(k J(t)\)set\(C_{k}(t+1) C_{k}(t),_{k}(t+1)_{k}(t)\)
```

**Algorithm 1**UCB for a single-queue multi-server system

We establish a framework to upper bound \(^{}\) for any policy by considering separately the initial _learning_ stage and the later _regenerate_ stage. The two stages are separated by a parameter \(T_{1}\) that appears in our analysis: intuitively, during the learning stage (\(t<T_{1}\)), the loss in total service of a policy compared with the optimal server's outweighs the slackness \(\) of the system (Definition 1), i.e., \(_{=1}^{t}^{}-_{J()}>t\) and thus the queue length grows linearly with respect to the left-hand side. After the learning stage (\(t>T_{1}\)), when \(_{=1}^{t}^{}-_{J()}<t\), the queue regenerates to a constant length independent of \(t\). To prove the \(()\) bound on \(^{}\), we couple the queue with an "auxiliary" queue where the DM always chooses a nearly optimal server in the learning stage. Then we utilize a Lyapunov analysis to bound the queue length during the regenerate stage.

The framework establishes a connection between \(^{}(,,)\) and the _satisficing regret_ defined as follows. For any horizon \(T\), the satisficing regret \(^{}(,T)\) is the total service rate gap between the optimal server and the server selected by \(\) except for the periods where the gap is less than \(\) or the queue length is zero. That is, the selected server is satisficing as long as its service rate is nearly optimal or the queue is empty. To formally define it, we denote \((x,0)\) by \(x^{+}\) and define the satisficing regret of a policy \(\) over the first \(T\) periods by

\[^{}(,T)=_{t=1}^{T}(^{}-_{J(t)}- )^{+}(Q(t) 1) \]

We use the satisficing regret notation because our motivation is similar to that in multi-armed bandits , initially considered for an infinite horizon and a Bayesian setting. In multi-armed bandits, optimal bounds on regret \(_{t=1}^{T}(^{}-_{J(t)})\) are either instance-dependent \((_{k:\;_{k}<^{}}(- _{k}}))\) or instance-independent \(O()\). However, both are futile to establish a \(}()\) bound for \(^{}(K,)\): The first bound depends on the minimum gap (which can be infinitesimal), whereas the second is insufficient as we explain in the discussion after Lemma 4.2.

We circumvent these obstacles by connecting the time-averaged queue length of the system with the satisficing regret of the policy via Lemma 4.1 (for the learning stage) and Lemma 4.2 (for the regenerate stage). Lemma 4.1 explicitly bounds the expected queue length through the expected satisficing regret; this is useful during the learning stage but does not give a strong bound for the regenerate stage. Lemma 4.2 gives a bound that depends on \(^{}(,T)^{2}}{T}\), and is particularly useful during the latter regenerate stage. We then show that the satisficing regret of UCB is \(O()\) (Lemma 4.3). Combining these results, we establish a tight bound for the cost of learning of \(\).

Formally, Lemma 4.1 shows that the expected queue length under \(\) in period \(t\) is at most that under a nearly optimal policy plus the expected satisficing regret up to that time.

**Lemma 4.1**.: _For any policy \(\) and horizon \(T\), we have \(^{T}[Q(t)]}{T}+ [^{}(,T)]\)._

Lemma 4.1 is established by coupling the queue with an auxiliary queue that always selects a nearly optimal server. However, it cannot provide a useful bound on the cost of learning. For large \(T\), it is known that \([^{}(,T)]\) must grow with a rate of at least \(()\). Hence, Lemma 4.1 only meaningfully bounds the queue length for small \(T\) (learning stage). For large \(T\) (regenerate stage), we instead have the following bound (Lemma 4.2).

**Lemma 4.2**.: _For any policy \(\) and horizon \(T\), we have \(^{T}[Q(t)]}{T}+}[^{}(,T)^{2} ]}{T}\)._

This lemma shows that the impact of learning, reflected by \([^{}(,T)^{2}]\), decays at a rate of \(\). Therefore, as long as \(^{}(,T)^{2}\) is of a smaller order than \(T\), the impact of learning eventually disappears. This also explains why the instance-independent \(O()\) regret bound for multi-armed bandits is insufficient for our analysis: the second moment of the regret scales linearly with the horizon and does not allow us to show a decreasing impact of learning on queue lengths.

Lemma 4.2 suffices to show stability (\(_{T}^{T}[Q(t)]}{T}<\)), but gives a suboptimal bound for small \(T\). Specifically, when \(^{}(,T)^{2} T\), this bound is of the order of \((})\).5 We thus need both Lemma 4.1 and Lemma 4.2 to establish a tight bound on the cost of learning in queues.

The following result bounds the first and second moments of the satisficing regret of UCB.

**Lemma 4.3**.: _For any horizon \(T\), we have_

\[(i)\;[^{}(,T)] ,\;(ii)\;[^{}(,T)^{2}]K^{2}( T+2)^{2}}{ ^{2}}.\]

We next offer a proof sketch of Theorem 2. The theorem and lemmas are proven in [12, Section 4].

Proof sketch of Theorem 2.: Fix any pair of \(\), \(\) with \(_{k}_{k}=+\). Let \(T_{1}=(2^{}}}{ ^{4}})^{2}\). We bound \(^{}(,,)\) by considering \(T T_{1}\) (learning stage) and \(T T_{1}\) (regenerate stage) separately. For \(T T_{1}\), we have

\[_{t T}[Q(t)] +[^{}(,T)] {323K+64K( K+2)}{},\]

where we use Lemmas 4.1 and 4.3 (i). For \(T>T_{1}\), we have

\[_{t T}[Q(t)] +}[^{} (,T)^{2}]}{T})}{}, \]

where we use Lemmas 4.2 and 4.3 (ii). 

**Remark 2**.: _Although the \(\) metric is focused on the entire horizon, our analysis extends to bounding the maximum expected time-averaged queue lengths in the later horizon, which is formalized as \(_{T T_{1}}[Q(t)]}{T}\) for any \(T_{1}\). In particular, for \(T_{1}(}}}{^{4}})^{2}\), Lemma 4.2 shows that \(_{T T_{1}}[Q(t)]}{T}\); UCB thus enjoys the optimal asymptotic queue length scaling of \(O()\)._

## 5 Conclusions

Motivated by the observation that queueing regret does not capture the complexity of learning which tends to occur in the initial stages, we propose an alternative metric (CLQ) to encapsulate the statistical complexity of learning in queueing systems. For a single-queue multi-server system with \(K\) servers and a traffic slackness \(\), we derive a lower bound \(()\) on CLQ, thus establishing that learning incurs a non-negligible increase in queue lengths. We then show that the classical UCB algorithm has a matching upper bound of \(()\). Finally, we extend our result to multi-queue multi-sever systems and general queueing networks by providing algorithms, MaxWeight-UCB and BackPressure-UCB, whose CLQ has a near optimal \((1/)\) dependence on traffic slackness.

Having introduced a metric that captures the complexity of learning in queueing systems, our work can serve as a starting point for interesting extensions that can help shed further light on the area. In particular, future research may focus on beyond worst case guarantees for CLQ, non-stationary settings, improved bounds using contextual information, etc.