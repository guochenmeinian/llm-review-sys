# On the Target-kernel Alignment: a Unified Analysis with Kernel Complexity

Chao Wang\({}^{}\), Xin He\({}^{}\); Yuwen Wang\({}^{}\); Junhui Wang

\({}^{}\) School of Statistics and Management, Shanghai University of Finance and Economics

\({}^{}\) Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai

\({}^{}\) Chinese University of Hong Kong

wang.chao@stu.sufe.edu.cn, he.xin17@mail.shufe.edu.cn

wangyw@link.cuhk.edu.hk, junhuiwang@cuhk.edu.hk

Xin He and Yuwen Wang are the corresponding author.

###### Abstract

This paper investigates the impact of alignment between the target function of interest and the kernel matrix on a variety of kernel-based methods based on a general loss belonging to a rich loss function family, which covers many commonly used methods in regression and classification problems. We consider the truncated kernel-based method (TKM) which is estimated within a reduced function space constructed by using the spectral truncation of the kernel matrix and compare its theoretical behavior to that of the standard kernel-based method (KM) under various settings. By using the kernel complexity function that quantifies the complexity of the induced function space, we derive the upper bounds for both TKM and KM, and further reveal their dependencies on the degree of target-kernel alignment. Specifically, for the alignment with polynomial decay, the established results indicate that under the just-aligned and weakly-aligned regimes, TKM and KM share the same learning rate. Yet, under the strongly-aligned regime, KM suffers the saturation effect, while TKM can be continuously improved as the alignment becomes stronger. This further implies that TKM has a strong ability to capture the strong alignment and provide a theoretically guaranteed solution to eliminate the phenomena of saturation effect. The minimax lower bound is also established for the squared loss to confirm the optimality of TKM. Extensive numerical experiments further support our theoretical findings. The Python code for reproducing the numerical experiments is available on Github.

## 1 Introduction

Kernel-based methods have attracted increasing attention in recent years (Belkin et al., 2018; Liang and Rakhlin, 2020; Ghorbani et al., 2020; Li et al., 2023), due to its close connection with some cutting-edge research topics, including the understanding of over-parameterized neural network through the neural tangent kernel (Jacot et al., 2018; Chizat et al., 2019) and large-scale kernel learning with gradient descent (Lin and Zhou, 2018; Xu et al., 2021). It is of fundamental importance to provide theoretical explanations of their behaviors under these research topics.

In literature, some recent works show that the learning rate of kernel-based methods is actually affected by both the model complexity of the considered reproducing kernel Hilbert space (RKHS) and the target-kernel alignment, a measure to quantify the similarity between the considered RKHS (or kernel matrix from the empirical point of view) and the target function, which is also known as the smoothness of a target function in the RKHS (Caponnetto and De Vito, 2007; Smale and Zhou,2007). Particularly, the existing learning rate for the kernel ridge regression (KRR) is proved to be \((n^{-})\) for \( 1\), where \(\) is known as the source condition parameter (Cui et al., 2021) and can be treated as a measure of target-kernel alignment at the population level, and \(\) controls the model complexity of the RKHS. This rate aligns with the intuitive sense that strong alignment and small model complexity contribute to a faster learning rate. Yet, with the increasing in \(\) such that \(>1\) which corresponds to a smoother target function, the best learning rate of KRR plateaus at \((n^{-})\)(Caponnetto & De Vito, 2007). This phenomenon is known as the **saturation effect**(Bauer et al., 2007; Li et al., 2022) and is widely observed in many applications (Bauer et al., 2007; Gerfo et al., 2008). It has been conjectured for decades that no matter what carefully chosen tuning parameter, the learning rate of KRR plateaus when the smoothness of the target function exceeds certain levels. Most recently, Li et al. (2022) establishes the rigorous saturation lower bound of KRR that confirms practical conjecture. Amini (2021); Amini et al. (2022) propose a truncated KRR based on the spectral-truncated kernel matrix, and further prove that as the alignment becomes stronger, the truncated KRR can be consistently improved in terms of the expected mean squared error and eventually tends to the parametric rate. Clearly, this improvement effectively tackles the saturation effect for the KRR where the squared loss is specified. Yet, it is still unclear whether the saturation effect can be solved for the general kernel-based methods where the specified loss function belongs to a rich loss function family.

In this paper, motivated by this theoretical gap, we investigate the impact of target-kernel alignment from the kernel complexity perspective for various kernel-based methods by considering a general loss function which belongs to a rich loss function family. Our established results shed light on the statistical benefits of the truncated estimator and are also verified by extensive numerical experiments. We want to emphasize that in contrast to the existing works that focus on the KRR benefit from the analytical solution and thus their theoretical analysis heavily relies on the closed form of the solution to establish some critical results (Cui et al., 2021; Amini et al., 2022), the explicit solution does not exist anymore in this paper, which requires different technical treatments to conduct the theoretical analysis. Specifically, our theoretical analysis crucially relies on the kernel complexity which quantifies the complexity of the RKHS (Bousquet & Herrmann, 2002) and some empirical process techniques. The established results successfully capture the trade-off between the complexity of the truncated RKHS and approximation bias as presented in Theorem 4.2. A simpler bound when considering the polynomial case in Corollary 4.3 further indicates that with a carefully chosen truncated space, the truncated method can efficiently eliminate the saturation effect. More importantly, we establish the minimax lower bound when the squared loss is specified, and thus rigorously confirm the conjecture in Amini et al. (2022), stating that the truncated KRR attains minimax optimality.

### Contributions

The main contribution of this paper is to offer a unified analysis and a comprehensive understanding of the impact of target-kernel alignment, and provide a theoretically guaranteed solution to eliminate the phenomena of saturation effect. Some of its contributions are listed as follows.

(i) By leveraging the kernel complexity function, we establish the upper bounds for both the standard kernel-based estimator and the truncated estimator for a general loss function belonging to a family of Lipschitz loss functions. The established bounds indicate that with the variation of the alignment level, the learning rates for these two estimators exhibit distinct trajectories. Specifically, under the polynomial decay assumption, when alignment is at a lower level, the standard kernel-based estimator and the truncated estimator share the same learning efficiency and improve with the rise in alignment level. Yet, when the alignment level surpasses a threshold (\(=1\) in Assumption 3.2), the learning rate of the kernel-based estimator plateaus with no improvement as \(\) increases -- a phenomenon known as the **saturation effect** in literature. As opposed, the learning rate of the truncated estimator exhibits continuous improvement with the increasing alignment level, thus eliminating the saturation effect. This indicates a significant improvement in the truncated estimator over the standard kernel-based estimator.

(ii) By employing the standard Fano method, we establish minimax lower bound when the squared loss is specified, indicating that for both the just-aligned and weakly-aligned regimes, both the standard kernel-based estimator and the truncated estimator achieve minimax optimality. Furthermore, for the strong-aligned regime, we demonstrate that the standard kernel-based estimator can only attain sub-optimality, while the truncated estimator is also minimax-optimal. Our minimax analysis significantly extends the existing results presented in Yang et al. (2017), offering a unified perspective for realistic scenarios where the true target is assumed to reside in the RKHS.

(iii) Various numerical experiments are conducted in the context of various regression and classification problems to demonstrate the advantages of the truncated estimator, to support the established theory substantially. More interestingly, we also empirically verify the existence of a trade-off arising from the model complexity of the RKHS and target-kernel alignment.

### Related Work

Kernel-based methods have been widely studied for the past few decades, and are known as the time-proven efficient tools for statistical analysis. The theoretical behaviors of the kernel-based estimator have been established in Caponnetto & De Vito (2007); Li et al. (2007); Smale & Zhou (2007); Patle & Chouhan (2013). The concept of target-kernel alignment has been introduced in Cristianini et al. (2001), where a classification algorithm is developed adapting to the target-kernel alignment with a significant improvement in classification accuracy. Follow-up works have expanded the concept of target-kernel alignment to some other learning tasks, including regression (Kandola et al., 2002) and multi-class classification (Guermeur et al., 2004). The implications of target-kernel alignment have also inspired some applications to spectral kernel learning (Hoi et al., 2006), and feature selection (Wong & Burkowski, 2011).

Most recently, many works have attempted to provide a theoretical understanding of the kernel-based method by considering the target-kernel alignment. Specifically, Canatar et al. (2021) investigates the generalization error of KRR and derives an analytical framework for the generalization error that captures the effect of the target-kernel alignment. Amini (2021) considers a spectrally truncated KRR and demonstrates that with a carefully chosen truncation, the truncated KRR outperforms the standard KRR. Li et al. (2022) verifies the saturation effect observed behind the KRR estimator by establishing a lower bound that \((n^{-})\) whatever the smoothness degree of the target function is. Motivated by these works, Amini et al. (2022) further demonstrates the non-monotonicity of the regularization curve for the bandlimited alignment setting and further reveals that the learning rate of the truncated KRR can be consistently enhanced as the degree of target-kernel alignment increases.

## 2 Preliminaries

**Background on RKHS.** Let \(_{K}\) denote the reproducing kernel Hilbert space (RKHS) induced by a positive semi-definite kernel function \(K(,):^{+}\), where \(^{p}\). The inner product equipped with \(_{K}\) is denoted as \(,_{K}\) with the endowed norm \(\|\|_{K}^{2}=,_{K}\). For each \(\), it is well-known that \(K_{}:=K(,)_{K}\) and the reproducing property holds that \( f,K_{}_{K}=f()\) for all \(f_{K}\). We assume that \(_{,^{}}K(,^{ })^{2}\) for some positive constant \(\). This condition is commonly assumed in literature and various popularly used kernel functions satisfy this condition, including the Gaussian kernel and Laplacian kernel.

**Problem setup.** We consider a collection of pairs \(\{(_{i},Y_{i})\}_{i=1}^{n}\) where \(\{_{i}\}_{i=1}^{n}\) is a collection of covariates and the response \(Y_{i}\) is independently drawn from a conditional distribution \(_{Y|_{i}}\) on \(\). Throughout this paper, we focus on the fixed design setting, where \(\{_{i}\}_{i=1}^{n}\) are fixed, otherwise we treat all the random quantities as conditioning on \(\{_{i}\}_{i=1}^{n}\). A similar treatment also appears in Yang et al. (2017); Wei et al. (2017); Amini et al. (2022).

In the literature of machine learning, the learning task is often defined with some pre-specified loss function. Specifically, we consider a loss function \(L(,):^{+}\), where \(L(y,f())\) quantifies the inaccuracy for predictor \(f()\) when \(y\) is the true response. Then, the population risk function can be defined as

\[(f):=_{Y^{n}}_{i=1}^{n}LY_{ i},f(_{i}),\]

where \(_{Y^{n}}\) denotes the expectation taken over \(Y_{1},...,Y_{n}\). In literature, the target function of interest in the learning task is typically defined as the minimizer of the population risk \(f^{*}=*{argmin}_{f}(f)\). In this paper, we assume \(f^{*}_{K}\) and consider a family of loss functions that \(L\) is assumed to be convex and locally Lipschitz continuous in the second argument (Wainwright, 2019; Dasgupta et al., 2019). Here, locally Lipschitz continuity is specified as that for any \(b>0\), there exists some positive constant \(M_{L,b}\)2 such that for any \(y\) and \(\), and for any \(f,f^{}_{K}\) satisfying \(\{\|f\|_{K},\|f^{}\|_{K}\} b\), the following inequality holds:

\[|L(y,f())-L(y,f^{}())| M_{L,b}|f()-f^{ }()|.\]

It is worth pointing out that the local Lipschitz continuity is satisfied for a variety of commonly used loss functions, and some of them are listed in Table 1.

Note that the choice of the loss function is task-specific based on the problem of interest and prior knowledge on the data. For instance, under the regression setting, the squared loss can be specified for mean regression and the check loss can be specified for quantile regression. Under the classification setting, the hinge loss can be specified for margin-based classification and the logistic loss can be specified for estimating the conditional probability.

## 3 Standard Kernel-based Method

In the rest of this paper, we use lowercase letters \(\{y_{i}\}_{i=1}^{n}\) to denote the observations of \(\{Y_{i}\}_{i=1}^{n}\), and denote the empirical measure of \(\{_{1},...,_{n}\}\) by \(_{n}\). Given an estimator \(\), its accuracy can be evaluated by the \((_{n})\)-error which is defined as \(\|-f^{*}\|_{n}^{2}=_{i=1}^{n}((_{i})-f^{*}(_{i}))^{2}\). We also use the excess risk that \(()-(f^{*})\) as an evaluation measure. To estimate the underlying target function \(f^{*}\), we consider the following penalized empirical risk minimization problem that

\[_{}=*{argmin}_{f_{K}} }(f)+\|f\|_{K}^{2}},\] (1)

where \(}(f)=_{i=1}^{n}L(y_{i},f(_{i}))\) and \(\) is regularization parameter. We define a sample operator \(S_{}:_{K}^{n}\) as \(S_{}(f):=}(f(_{1}),...,f(_{n}) )^{}\), and its adjoint operator \(S_{}^{}:^{n}_{K}\) is defined as

\[S_{}^{}():=}_{j=1}^{n} _{j}K(,_{j}),\ \ \ =(_{1},...,_{n})^{}^{n}.\]

Then, by the representer theorem (Kimeldorf and Wahba, 1971), the minimizer of the learning task (1) must have a finite form that \(_{}=S_{}^{}(})\) where \(}^{n}\) is the solution to the following optimization task

\[}=*{argmin}_{ ^{n}}\ }(S_{}^{}())+ ^{}}.\]

Let \(=K(_{i},_{j})}_{i,j=1}^ {n}\) be the empirical kernel matrix where the scaling is for analytical simplicity. In the subsequent analysis, we further assume that \(\) is positive which is also required in literature (Liang and Rakhlin, 2020; Amini et al., 2022). Then, the kernel matrix \(\) admits an eigen-decomposition that \(=\,\,^{}\), where \(=(_{1},...,_{n})^{n n}\) is an orthonormal matrix and \(^{n n}\) is a diagonal matrix with positive elements \(_{1},...,_{n}\) arranging in a descending ordering. Without of loss generality, we further require that \(_{j} 0\) as \(j\).

Let \(^{*}=^{}\,S_{}(f^{*})\). The elements of the vector \(^{*}\) are referred to as target alignment (TA) scores (Amini et al., 2022), which quantify the agreement level between \(f^{*}\) and \(\). Intuitively, a more

  
**Loss** & Squared & Exponential & Check & Hinge & Huber & Logistic \\  \(_{L,b}\) & \(2U+2b^{3}\) & \(1\) & \(1\) & \(1\) & \(^{4}\) & \(1\) \\   

Table 1: Different losses with corresponding Lipschitz constant \(M_{L,b}\)favorable learning rate can be achieved if the target and kernel are strongly aligned corresponding to fast decay TA scores. For example, the scenario that \(S_{}(f^{*})\) is predominantly situated in the space generated by the eigenvectors corresponding to the first several eigenvalues of \(\) indicates a strong alignment. In other words, \(_{j}^{*}\) is expected to be as large as possible for small \(j\) and as small as possible for large \(j\). An ideal scenario is that \(S_{}(f^{*})\) exactly matches the eigenvector \(_{1}\), leading to \(^{*}=(1,0,...,0)^{}\) with proper scaling such that \(\|f^{*}\|_{n}^{2}=1\). In this paper, we are devoted to providing an analytic framework for the impact of alignment on the performance of the kernel-based methods based on the kernel complexity of \(\).

### Technical Assumptions

The following necessary assumption is needed in our theoretical analysis.

**Assumption 3.1**.: There exist two constants \(0<c_{0} c_{0}^{}\) such that

\[c_{0}\|f-f^{*}\|_{n}^{2}(f)-(f^{*}) c_{0}^{ }\|f-f^{*}\|_{n}^{2},\]

for any \(f_{K}\) satisfying \(\|f-f^{*}\|_{n}^{2} b\) with some sufficiently small constant \(b>0\).

The first inequality in Assumption 3.1 is a \(c_{0}\)-locally strong convexity condition, and the second inequality is a \(c_{0}^{}\)-local smooth condition of the loss function with respect to \(\|\|_{n}\). Assumption 3.1 is commonly assumed in literature (Steinwart and Christmann, 2008; Wei et al., 2017; Li et al., 2019; Farrell et al., 2021). Due to space limit, some brief discussions are provided below. For the squared loss, Assumption 3.1 is satisfied with \(c_{0}=c_{0}^{}=1\). For the check loss, the \(c_{0}\)-locally strong convexity condition is slightly more relaxing than the similar assumption in the literature (Lian, 2022) that requires the conditional density function of the noise term to be uniformly bounded away from zero. And, the \(c_{0}^{}\)-local smoothness condition holds if the conditional density function is uniformly bounded. Other widely used loss functions including Huber loss, Logistic loss, Hinge loss, and exponential loss also satisfy Assumption 3.1 under some mild conditions as discussed in Appendix F.

**Assumption 3.2**.: There exist some constants \(\) and \(u 2\) such that \(_{j=1}^{n}{_{j}^{*}}^{2}{_{j}}^{-2} u^{2}\) for any \(n\).

Assumption 3.2 imposes the regularization on the TA scores \(^{*}\) concerning \(\). Note that the parameter \(\) reflects the degree of target-kernel alignment, where a larger \(\) indicates stronger alignment between \(\) and \(f^{*}\). Moreover, the parameter \(\) in Assumption 3.2 can be considered analogous to the source condition parameter under the random design setting (Caponnetto and De Vito, 2007; Cui et al., 2021; Li et al., 2023). Further discussions on the extension to the random setting are deferred to Appendix B. In our subsequent analysis, we consider the following three cases that

* Just-aligned regime: \(=\) where we only assume \(f^{*}_{K}\).
* Weakly-aligned regime: \(< 1\) where \(f^{*}_{K}\) and is more aligned with \(\).
* Over-aligned regime: \(>1\) where \(f^{*}_{K}\) and has a strong alignment with \(\).

### The Upper Bound for Standard Kernel-based Method

In the rest of this paper, we use \(c,C\) to denote some constants independent of \(n,,\), which may hide the constants such as \(u,c_{0},c_{0}^{}\) and whose values may vary from line to line. In literature, the empirical kernel complexity function is defined as \(R():=_{j=1}^{n}\{^{2},_{j}\}}\)(Bartlett et al., 2005). \(R()\) serves as a measure of complexity of \(_{K}\) and is closely connected to local Rademacher complexity (Bartlett et al., 2005; Steinwart et al., 2009). It plays a crucial role in establishing our theoretical results via the critical radius \(_{n}\), defined as the smallest positive value \(\) such that

\[C\,^{-1}R()}{2}^{2+1},\] (2)

where \(=\{,1\}\) and \(\) is specified in the subsequent theorems and corollaries. The learning rate of the kernel-based estimator defined in (1) highly depends on \(_{n}\), and the existence and uniqueness of \(_{n}\) are verified in Appendix B.1. As pointed out in Yang et al. (2017), the statistical dimension is defined as the first index for which the associated eigenvalue \(_{j}\) drops below \(^{2}\) that \(d():=\{j[n]:_{j}^{2}\}\), where \([n]=\{1,2,..,n\}\) and \(d():=n\) if \(\{j[n]:_{j}^{2}\}=\).

Note that the statistical dimension serves as a measure of the intrinsic dimension of the kernel matrix \(\). Moreover, a kernel is regular if the tail sum of its eigenvalue sequence can be well bounded as the form \(_{d()+1}^{n}_{j} d()^{2}\)(Yang et al., 2017). Note that kernels in the kernel class with the polynomial or exponential decay in their eigenvalues are regular. Then, the kernel complexity can be well approximated by \(/n}\). The close connection between \(d()\) and \(R()\) enables us to find the explicit formulation of \(_{n}\) in our theoretical analysis.

The following theorem provides theoretical guarantees of \(_{}\) defined in (1) in terms of \((_{n})\)-error and the excess risk which hold with high probability.

**Theorem 3.3**.: _Suppose that Assumptions 3.1 and 3.2 are satisfied and \(_{n}^{2} 1\)5. Let \(=\{,1\}\). Then, for any \((0,1)\), with probability at least \(1-\), there holds_

\[\|_{}-f^{*}\|_{n}^{2},\ (_{ })-(f^{*})} C(_{n}^{4}+^{2}).\]

The proof of Theorem 3.3 is provided in Appendix C. To complete the proof of Theorem 3.3, we only need to require the first inequality in Assumption 3.1. Note that the established bound for \(_{}\) consists of two terms that are related to the critical radius \(_{n}\) and the parameter \(\). Compared to the existing works (Yang et al., 2017; Amini et al., 2022) under the fixed setting where only the squared loss is considered, Theorem 3.3 provides a comprehensive theoretical analysis on various kernel-based estimators by considering a general loss function with the help of kernel complexity and also considers the effect of the target-kernel alignment on the estimation performance under different aligned regimes. Moreover, we notice that with the choice of \(\) satisfying \(_{n}^{2}\), an optimal rate can be achieved that

\[(_{})-(f^{*})\|_{ }-f^{*}\|_{n}^{2}_{n}^{4}.\]

Note that Amini et al. (2022) provides some valuable insights into the learning rate of the kernel-based estimator under the squared loss in terms of the expected \((_{n})\)-error where the following polynomial decay condition is required.

**Assumption 3.4**.: There exist some constants \(>1\) and \(\) such that the eigenvalues of \(\) and the TA scores exhibit polynomial decay rate that

\[_{j} j^{-}_{j}^{*2} j^{-2 -1}.\]

In Assumption 3.4, the parameter \(\) controls the complexity of \(_{K}\) in the sense that a decreasing \(\) results in the increasing capacity of the RKHS \(_{K}\)(Cui et al., 2021; Amini et al., 2022). Various widely used kernels, including the Sobolev kernel and the Laplacian kernel, belong to this class. Note that with slight modification by setting \(_{j}^{*2} j^{-2-1}( j)^{-2}\), it can be verified that Assumption 3.4 directly leads to Assumption 3.2 if we ignore the logarithmic term.

By Assumption 3.4, it is clear that \(d()^{-2/}\), and consequently \(_{n}^{2})^{2}}{n}^{}\). To better understand the established results in Theorem 3.3, the following corollary is also provided.

**Corollary 3.5**.: _Suppose that Assumptions 3.1, 3.2 and 3.4 are satisfied. Let \(=\{,1\}\). For any \((0,1)\), if we choose \()^{2}}{n}^{}\), then with probability at least \(1-\), there holds_

\[(_{})-(f^{*})\|_{ }-f^{*}\|_{n}^{2} C)^{2}}{n}^{ }.\]

Under the just-aligned regime that \(=\), the learning rate turns to be \((^{-1})^{2}/n^{}\), which is consistent with that in literature (Wei et al., 2017) where merely assumes \(f^{*}_{K}\). Yet, under the weakly-aligned regime that \(< 1\), the learning rate exceeds \((^{-1})^{2}/n^{}\) due to the stronger target-kernel alignment. More interestingly, under the over-aligned regime that \(>1\), the learning rate plateaus with no improvement as \(\) increases, which indicates a saturation effect for the standard kernel-based method. It is also interesting to notice that no matter how the choice of \(\), the learning rate is always lower bounded by \((n^{-})\) for \( 1\)(Li et al., 2022). In the next section, we will show that a careful choice of truncation allows us to construct an estimator based on a reduced RKHS that achieves the best rate and mitigates the saturation effect for \(>1\).

## 4 Truncated Kernel-based Method

To construct the reduced RKHS, we introduce a collection of functions \(\{_{k}\}_{k[n]}_{K}\), defined as \(_{k}:=*{argmin}\|\|_{K}:_{K},S_{ }()=_{k}}\). It can be verified that \(\{_{k}\}_{k[n]}\) is unique and by the orthogonality of \(_{1},...,_{n}\), we also have \(_{i},_{j}_{n}=1\) for \(i=j\) and \(0\) otherwise (Amini et al., 2022). Then, for a given \(r\), we define a function space as

\[_{K_{r}}:=_{k=1}^{r}_{k}_{k}:=(_{1},...,_{r})^{}^{r}}.\]

Let \(_{K_{r}}\) be equipped with the norm \(\|f\|_{K_{r}}^{2}= f,f_{K_{r}}\), where the inner product is defined as \( f,g_{K_{r}}:=_{k=1}^{r}_{k}_{k}/_{k}\) for \(f=_{i=1}^{r}_{k}_{k}\) and \(g=_{i=1}^{r}_{k}_{k}\). The following lemma from Amini et al. (2022) indicates that \(_{K_{r}}\) is also an RKHS associated with a different kernel function.

**Lemma 4.1**.: \(_{K_{r}}_{K}\) _is an \(r\)-dimensional RKHS with reproducing kernel \(K_{r}(,^{})=_{k=1}^{r}_{k}_{k}( )_{k}(^{})\)._

Clearly, \(_{K_{r}}\) can be treated as a relatively smaller function space compared to the full RKHS \(_{K}\). Based on \(_{K_{r}}\), we can find a truncated kernel-based estimator by solving

\[_{,r}=*{argmin}_{f_{K_{r}}} }(f)+\|f\|_{K_{r}}^{2}}.\]

For the truncated RKHS \(_{K_{r}}\), we also define the operator \(S_{,r}^{}:^{n}_{K_{r}}\) as

\[S_{,r}^{}():=}_{j=1}^{n }_{j}K_{r}(,_{j}),\ \ =(_{1},...,_{n})^{}^{n}.\]

Then, by the representer theorem (Kimeldorf & Wahba, 1971) again, \(_{,r}\) also has a finite solution that \(_{,r}=S_{,r}^{}(}_ {r})\) and \(}_{r}\) can be obtained by solving the following optimization task

\[}_{r}=*{argmin}_{ ^{n}}}(S_{,r}^{}( ))+^{}_{r} }.\]

where \(_{r}=K_{r}(_{i},_{j})} _{i,j=1}^{n}\) is the empirical kernel matrix w.r.t. \(K_{r}\). Note that the truncated method does not impose an additional computational cost compared to the standard kernel method, and its detailed discussion will be provided in Appendix B.5. By the construction of \(\{_{i}\}_{i[n]}\), it is easy to verify that \(_{r}=\,_{r}^{}\), where \(_{r}\) is diagonal matrix with elements \(_{1},...,_{r},0,...,0\), detailed proof can be seen in Appendix B.2. This further implies that \(_{r}=\) when \(r=n\), and thus leads to \(_{}(_{i})=_{,n}(_{i})\) for each \(i[n]\).

### The Upper Bound for Truncated Kernel-based Method

Given the truncated RKHS \(_{K_{r}}\), our theoretical results below rely on the truncated kernel complexity function, defined as \(R_{r}():=_{j=1}^{r}\{^{2},_{j}\}}\). Moreover, the critical radius \(_{n,r}\) can be defined as the smallest positive value \(\) such that

\[C\,^{-1}R_{r}()}{2}^{2+1}.\] (3)

The existence and uniqueness of \(_{n,r}\) are verified in Appendix B.1. It can be verified that \(R_{r}() R()\) and thus leads to \(_{n,r}_{n}\). This observation indicates a potential improvement of the truncated estimator \(_{,r}\) and is the core of our theoretical analysis. The following theorem shows that \(_{,r}\) converges to the underlying target in terms of the \((_{n})\)-error and the excess risk with high probability.

**Theorem 4.2**.: _Suppose that Assumptions 3.1 and 3.2 are satisfied and \(\{_{n,r}^{2},_{j=r+1}^{n}{_{j}}^{2}\} 1\). Let \(=\{,1\}\). Then, for any \((0,1)\), with probability at least \(1-\), there holds_

\[\|_{,r}-f^{*}\|_{n}^{2},\ \ (_{ ,r})-(f^{*})} C_{n,r}^{4}+ ^{2}+_{j=r+1}^{n}{_{j}}^{2}.\]The proof of Theorem 4.2 is provided in Appendix D. The established upper bound of \(_{,r}\) first decomposes the total error into two components: estimation error (the first two terms), which is controlled by complexity of the reduced RKHS \(_{K_{r}}\), and approximation bias (the last term), which results from the dissimilarity between the truncated RKHS \(_{K_{r}}\) and the full RKHS \(_{K}\) where \(f^{*}\) belongs to. Specifically, a smaller value of \(r\) reduces the complexity of \(_{K_{r}}\), possibly leading to a more favorable estimation error. Yet, it amplifies the gap between \(_{K_{r}}\) and \(_{K}\) and may lead to an extra approximation bias which may be significantly large. Clearly, \(r\) can be regarded as a trade-off parameter that balances the approximation bias \(_{j=r+1}^{n}_{j}^{*2}\) and the estimation error \(_{n,r}^{4}+^{2}\). It is clear that if \(r=n\), the approximation bias is zero and the upper bound of \(_{,n}\) recovers that of \(_{}\). This implies that with careful choice of \(r\), the truncated method at least performs as well as the standard estimator. If Assumption 3.4 also holds, we can conclude that \(_{j=r+1}^{n}_{j}^{*2} r^{-2}_{\{r<n\}}\). Then, the best choice of \(r\) and \(\) to achieve an optimal rate is given by \(r_{n,r}^{-2/()}\) if \(>1\); \(r=n\) if \( 1\), and \(_{n,r}^{2}\). Accordingly, there holds

\[(_{,r})-(f^{*})\|_{ ,r}-f^{*}\|_{n}^{2}_{n,r}^{4}.\]

For the comparison of Corollary 3.5, we also establish the following corollary for \(_{,r}\).

**Corollary 4.3**.: _Suppose that Assumptions 3.1, 3.2 and 3.4 are satisfied. For any \((0,1)\), if we choose \()^{2}}{n}^{}\) and \(r()^{2}})^{}I_{\{ >1\}}+nI_{\{ 1\}}\), then with probability at least \(1-\), there holds_

\[(_{,r})-(f^{*})\|_{ ,r}-f^{*}\|_{n}^{2} C)^{2}}{n}^{ }.\]

Clearly, under the over-aligned regime that \(>1\), the truncated estimator \(_{,r}\) can achieve a faster learning rate compared to \(_{}\); for \( 1\), the trivial choice of \(r=n\) is optimal and \(_{,r}\) shares the same learning rate as \(_{}\). More impressively, the learning rate of \(_{,r}\) can be continuously increased with the enhancement of the target-kernel alignment, thus eliminating the saturation effect. Note that as \(\), the learning rate of \(_{,r}\) approaches \(\), meaning that the truncated estimator can successfully capture the strong alignment to attain a comparable rate to the parametric rate.

**The connection between \(r\) and \(d()\).** Recall that for the regular kernel class, we have \(R()d()^{2}}\). It can also be shown by simple algebra that \(R_{r}()\{r,d()\}^{2}}\) (See Appendix D.3 for details). Particularly, for the kernel class with polynomial decay, we have \(d()^{-2/}\). Once the critical radius \(_{n,r}\) is determined for specified kernel matrix, we denote \(d_{n}=d(_{n,r})_{n,r}^{-2/}\) and take \(r_{n,r}^{-2/()}\) to balance \(_{n,r}^{4}\) and \(r^{-2}\). Consequently, we obtain \(r d_{n}^{/}\). Such a relation between \(r\) and \(d_{n}\) is very reflective and provides theoretical insight into why the truncated estimator is more efficient under a more aligned situation. Specifically, for the case \(>1\), it is clear that \(r d_{n}^{1/}<d_{n}\), and we have

\[R_{r}(_{n,r})r_{n,r}^{2}}<d_{n} _{n,r}^{2}} R(_{n,r}).\]

As a result, the truncated kernel complexity is substantially reduced compared to \(R(_{n,r})\), leading to an improved learning rate. On the contrary, for the case that \( 1\), we have \(r d_{n}\) and \(R_{r}(_{n,r})d_{n}_{n,r}^{2}} R(_{n,r})\), which indicates the truncated kernel complexity remains invariant as \(r\) decreases. To avoid introducing additional approximation bias, the best choice of truncation level turns out to be \(r=n\).

### Minimax Lower Bound

In this section, we establish the minimax lower bound under squared loss based on the Fano method (see Chapter 15 in Wainwright (2019) for more details). For \(\), we consider the space within a ball as \(_{K}^{b}=f_{K}:_{j=1}^{n}_{j}{}^{2}_ {j}{}^{-2} u^{2}}\), where \(_{j}\)'s are the TA scores associated with \(f\).

**Theorem 4.4**.: _Suppose that the RKHS is induced by the regular kernel, and \(\) is any estimator based on the data \(\{(_{i},y_{i})\}_{i=1}^{n}\). If \( 1\), we have_

\[_{}_{f^{*}_{K}^{1}}\, \|-f^{*}\|_{n}^{2} c_{n}^{4} .\]

_If \(>1\), with the choice of \(r\) satisfying \(r d(_{n,r}^{1/}) d(_{n,r})\), we have_

\[_{}_{f^{*}_{K}^{1}} \|-f^{*}\|_{n}^{2} c_{n,r}^{4}.\]

The proof of Theorem 4.4 is provided in Appendix E. For \(>1\), the condition \(d(_{n,r}^{1/}) d(_{n,r})\) can be easily verified for the most popular polynomial decay case that \(_{j} j^{-}\). Specifically, for the kernel class with polynomial decay, we have \(d()^{-2/}\), which leads to

\[d(_{n,r}^{1/})_{n,r}^{-2/}_{n,r}^{-2 /} d(_{n,r}).\]

Moreover, it can be seen that \(r_{n,r}^{-2/}\) is the optimal choice, aligning with the optimal choice in the upper bound. Note that it is common to establish the upper bound for the other loss function and compare it to the lower bound established under the squared loss to check the optimality (Wei et al., 2017; Lv et al., 2018; Li et al., 2019). By comparing the lower bounds in Theorem 4.4 with the achievable rates from Theorems 3.3 and 4.2, we can conclude that under the case that \( 1\), both the standard kernel-based estimator \(_{}\) and the truncated estimator \(_{,r}\) is minimax-optimal. More importantly, under the more challenging case that \(>1\), \(_{}\) can only achieve a sub-optimal rate, whereas \(_{,r}\) can attain the minimax rate as long as \(r d(_{n,r}^{1/}) d(_{n,r})\), suggesting that the truncated kernel-based method can be treated as optimal tackling. It is also worthy pointing out that under the just-aligned regime that \(=\), Yang et al. (2017) derives the minimax lower bound by considering the regular kernel class, and Theorem 4.4 extends it to the more general setting by allowing \(\).

## 5 Numerical Verification

Our established results indicate that a larger \(\) corresponding to a lower model complexity of the RKHS leads to a better rate. As opposed, a smaller model with lower complexity simultaneously may result in a potential mismatch between the model space and the target. This may weaken the target-kernel alignment which undermines the learning efficiency. Consequently, a trade-off exists between model capacity \(\) and target-kernel alignment \(\), with a preference for relatively lower model complexity and stronger target-kernel alignment.

To illustrate this, we conduct some numerical experiments to study how the RKHS with varying model complexities affect the numerical performance of KM and TKM. Specifically, we use the spline kernel with order \(\) that \(K_{}(,^{})=_{k=-}^{}e^{2 ik }e^{-2 ik^{}}|k|^{-}\)(Wahba, 1990), where \(\) controls the model complexity of the induced RKHS at the population level. Moreover, we consider the nonparametric quantile regression that

\[Y_{i}=f^{*}(_{i})+(_{i}-^{-1}()),\ i=1,...,n,\]

where \(f^{*}()=K_{3.5}(,0)()\), \(=2\), \(_{i} N(0,1)\), \(\{_{i}\}_{i=1}^{n}\) are independently sampled from the uniform distribution on \((0,1)\) and \(\) denotes CDF function of standard normal distribution. We conduct the numerical experiments by varying \(\{0.3,0.5,0.7\}\) and \(\{2,4,6,8,10\}\) with fixed \(n=300\). The data generating scheme is repeated for \(50\) times and all the tuning parameters are tuned to the best for both methods. The obtained results are presented in Figure 1.

From Figure 1, we can conclude that the smaller \(\), corresponding to richer RKHS and potentially stronger alignment, results in significant improvement in TKM over KM. Conversely, the larger \(\), corresponding to a smaller RKHS and potentially weaker alignment, results in a comparable performance for these two methods. This observation precisely aligns with our theoretical results. Clearly, based on our theoretical findings, the experiment results verify the existence of a trade-off between the model complexity and target-kernel alignment, indicating that a carefully data-drivenchoice of the kernel may be necessary to achieve better learning efficiency. We defer the deeper exploration of data-driven selection of an appropriate kernel to future research endeavors.

The real data analysis is deferred to Appendix A. Furthermore, a variety of additional experiments are presented in Appendix H. The obtained results are discussed in detail, which further supports our theoretical findings.

## 6 Discussions and Conclusion

### Comparison and Discussions

Amini et al. (2022) studied how the target-kernel alignment affects both the standard KRR and the truncated KRR. Although our work is motivated by Amini et al. (2022), especially for the methodological aspect, there exist significant differences between our established results and those in Amini et al. (2022), and some are summarized as follows: (a) Amini et al. (2022) only focused on the upper bounds in terms of expected mean squared error, while our results provide more precise high-probability upper bounds. (b) Beyond the polynomial decay condition considered in Amini et al. (2022), we introduce a more general condition as stated in Assumption 3.2. This condition involves \(\), reflecting the degree of target-kernel alignment. (c) In Amini et al. (2022), both the standard KRR and the truncated KRR have explicit solutions. This allows leveraging analytic solutions to establish critical results, without requiring more advanced techniques. In contrast, no explicit solutions exist in our case and our theoretical analysis adopts an alternative analytic treatment by utilizing kernel complexity and empirical process techniques. (d) Last but not least, we rigorously confirm the conjecture in Amini et al. (2022) asserting that the truncated KRR can achieve the minimax optimality for all \(\).

### Conclusion and Future Work

This paper provides a comprehensive theoretical understanding of the properties of the truncated kernel-based method for a broad family of loss functions. By using kernel complexity and empirical process techniques, the established results reveal some significant benefits from the truncated RKHS and indicate that a carefully chosen truncation allows for an optimal trade-off between the model complexity and approximation bias. Extensive numerical studies further justify our theoretical findings, demonstrating a consistent improvement of the truncated estimator over the standard kernel-based estimator. We also derived an algorithm-free minimax lower bound that matches the upper bound on the truncated estimator and therefore confirmed its optimality. To some extent, our results shed light on future research in statistical learning theory and real-world applications. This paper also leaves several interesting open questions for future investigation, including the theoretical explorations under the misspecified setting that \(0<<\) and how to develop a data-driven algorithm for selecting a strongly-aligned kernel with lower model complexity.

Figure 1: Averaged log MSE and log empirical excess risk for KM and TKM versus \(\) for different \(\).