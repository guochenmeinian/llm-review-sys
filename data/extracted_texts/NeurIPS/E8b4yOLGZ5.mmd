# Meta-Learning Universal Priors

Using Non-Injective Change of Variables

 Yilang Zhang

Department of ECE

University of Minnesota

Minneapolis, MN 55414

zhan7453@umn.edu

&Alireza Sadeghi

Department of ECE

University of Minnesota

Minneapolis, MN 55414

sadeg012@umn.edu

&Georgios B. Giannakis

Department of ECE

University of Minnesota

Minneapolis, MN 55414

georgios@umn.edu

###### Abstract

Meta-learning empowers data-hungry deep neural networks to rapidly learn from merely a few samples, which is especially appealing to tasks with small datasets. Critical in this context is the _prior knowledge_ accumulated from related tasks. Existing meta-learning approaches typically rely on preselected priors, such as a Gaussian probability density function (pdf). The limited expressiveness of such priors however, hinders the enhanced performance of the trained model when dealing with tasks having exceedingly scarce data. Targeting improved expressiveness, this contribution introduces a _data-driven_ prior that optimally fits the provided tasks using a novel non-injective change-of-variable (NCoV) model. Unlike preselected prior pdfs with fixed shapes, the advocated NCoV model can effectively approximate a considerably wide range of pdfs. Moreover, compared to conventional change-of-variable models, the introduced NCoV exhibits augmented expressiveness for pdf modeling, especially in high-dimensional spaces. Theoretical analysis underscores the appealing universal approximation capacity of the NCoV model. Numerical experiments conducted on three few-shot learning datasets validate the superiority of data-driven priors over the prespecified ones, showcasing its pronounced effectiveness when dealing with extremely limited data resources.

## 1 Introduction

Advances in deep learning (DL) have boosted the notion of "learning from data" with field-changing performance improvements reported across a wide range of applications . Large-scale DL models with high fitting capacity have documented ability to cope with the "curse of dimensionality" by providing compact low-dimensional representations of high-dimensional data. Nonetheless, these high-capacity models typically require protracted training using massive data records. Humans on the contrary, can perform exceptionally well on tasks such as object recognition or concept comprehension with merely a few samples. How to acquire the learning ability of humans in the DL training processes is thus appealing and imperative for a number of application domains, especially when data are scarce or costly to annotate. Examples of such applications include machine translation , medical imaging , and robot manipulations .

Meta-learning, also referred to as "learning to learn," seeks to gather the _prior knowledge_ shared across a set of inter-related tasks, to enable quickly solving an unseen yet related learning task using minimal training samples . This form of higher-level learning effectively extracts domain-generic inductive biases from prior tasks, which can be subsequently transferred to learn a new task even with limited data. This mirrors the capability that humans excel at -- leveraging past experiences to rapidly acquire new skills. Meta-learning holds the promise of yielding powerful priors with whichDL models can generalize better, require fewer data for training, and adapt more effectively to new tasks in dynamically changing environments.

Conventional approaches to meta-learning have relied on hand-crafted techniques to extract prior knowledge [47; 46]. With the advent of DL and growing volume of data, there has been a paradigm shift from such cumbersome procedures towards more efficient data-driven strategies. In particular, the prior information is encoded in hyperparameters, which are shared across tasks and can be fine-tuned using the validation data of all tasks. Utilizing these informative hyperparameters, task-specific learning can be performed even with limited data. Early attempts adopted a neural network (NN) with its weights serving as the shared hyperparameters [55; 45; 33]. The _task-invariant_ NN leverages the shared hyperparameters, and training data per task, to output the _task-specific_ model. However, the selection of an appropriate NN architecture is tailored to the choice of the task-specific models. In addition, NNs inherently lack interpretability and robustness due to their "black box" nature.

Unlike NN-based meta-learning, model-agnostic meta-learning (MAML) does not rely on any presumptions about task-specific models . Instead, it relies on an iterative optimizer to learn the task-specific model. The task-invariant prior information is embodied in the initialization of the optimizer, which is shared across tasks. By learning an informative initialization, task-specific learning can rapidly converge to a local minimum within a few iterations. Interestingly, the initialization generated by MAML can be viewed as a learnable mean of an implicit Gaussian prior probability density function (pdf) over the task-specific model parameters . Building on MAML, several optimization-based meta-learning algorithms have been advocated to learn different prior pdfs [29; 37; 25; 1; 58]. In addition, theoretical studies have been carried out to further offer insights into these approaches [13; 39; 8; 9; 61]. Nevertheless, the prior models of most existing meta-learning methods are confined to preselected pdfs, such as the Gaussian one, and thus have limited expressiveness, meaning fitting ability. Consequently, generalizing meta-learning to domains that deal with scarce datasets, and need sophisticated priors, remains a challenging and largely uncharted territory.

To improve the prior expressiveness in meta-learning, this contribution puts forth what we term non-injective change-of-variable (NCoV) model, which enables learning a universal data-driven prior from related tasks. The contribution of the resultant method named MetaNCoV is threefold:

1. Our novel NCoV model is proven capable of mapping a known source pdf to an _arbitrary_ target pdf. This markedly enhances the model expressiveness, especially in high-dimensional spaces.
2. Theoretical analysis is provided to demonstrate that a parametric NCoV can approximate a broad spectrum of pdfs, that in turn enables versatile plug-in priors for meta-learning. Moreover, this parametric NCoV inherently provides a task-invariant initialization, rather nicely eliminating the need for its explicit learning.
3. Numerical tests on three benchmark few-shot learning datasets corroborate our theoretical analysis, and underscore the superior prior expressiveness of the proposed MetaNCoV method compared to meta-learning approaches with prespecified pdfs.

## 2 Problem setup

Meta-learning relies on task-invariant prior information from a collection of \(T\) given tasks (indexed by \(t=1,,T\)), to deal with data-limited settings. For each \(t\), there is a dataset \(_{t}:=\{(_{t}^{n},y_{t}^{n})\}_{n=1}^{N_{t}}\) consisting of \(N_{t}\) (data, label) pairs. The dataset is divided into a training subset \(_{t}^{}_{t}\), and a validation subset \(_{t}^{}:=_{t}_{t}^{ }\). In addition, a new task indexed by \(\) is also provided, with its training set \(_{}^{}\), and an unannotated test set \(_{}^{}:=\{_{}^{n}\}_{n=1}^{N_{ }}\) for which the corresponding labels \(\{y_{}^{n}\}_{n=1}^{N_{}^{}}\) are to be inferred. The major premise of meta-learning is that the aforementioned tasks are related through their underlying data distributions or problem structures. This relationship makes it feasible to employ a unified large-scale model such as a deep NN to fit all tasks, with each task tailored by its specific model parameter \(_{t}^{d}\). However, as the cardinality \(|_{t}^{}|\) can be much smaller than \(d\), directly optimizing \(_{t}\) over \(_{t}^{}\) could readily lead to overfitting.

Meta-learning addresses this issue by capitalizing on the relationships among tasks. Specifically, since \(T\) is considerably large in meta-learning, a _task-invariant_ prior can be extracted to capture knowledge across tasks, thereby facilitating the data-limited per-task training. This nested structure of prior extraction and per-task training lends itself to a _bilevel optimization_ problem. The inner-level (task-level) optimizes the per-task parameter \(_{t}\) using \(_{t}^{}\), and the prior provided by outer-level,while the outer-level (meta-level) evaluates the trained \(\{_{t}\}_{t=1}^{T}\) using \(\{_{t}^{}\}_{t=1}^{T}\), and refines the prior parameterized by \(^{D}\).

The bilevel optimization objective of meta-learning can be expressed as

\[_{}_{t=1}^{T}(_{t}^{*}( {});_{t}^{}) \] \[_{t}^{*}()=*{argmin}_{ _{t}}(_{t};_{t}^{})+ (_{t};),\;t=1,,T \]

where the loss function \(\) assesses the fit of a task-specific model to a designated dataset, and the regularizer \(\) quantifies the impact of task-invariant prior. From the Bayesian viewpoint, \((_{t};_{t}^{})=- p( _{t}^{}|_{t};_{t}^{})\) can be interpreted as the negative log-likelihood (nll), and \((_{t};)=- p(_{t};)\) is the negative log-prior (nlp), where \(_{t}^{}\) denotes the matrix collecting all the data vectors in \(_{t}^{}\), and \(_{t}^{}\) is the corresponding label vector. Using Bayes' rule, it follows that \(_{t}^{*}=*{argmax}_{_{t}}p(_{t}| _{t}^{};_{t}^{},)\) is the maximum a posteriori (MAP) estimator.

Unfortunately, the global optimum \(_{t}^{*}\) in (1b) is generally unreachable when the postulated model is a nonlinear function of \(_{t}\). Hence, a feasible alternative is to rely on an approximate solver \(}_{t}_{t}^{*}\) obtained by a tractable optimizer. Depending on how the alternative solver is acquired, meta-learning algorithms can be categorized as either NN- or optimization-based ones. The former harnesses an NN optimizer \(}_{t}=*{NN}(_{t}^{};)\) to model the training process that maps \(_{t}^{}\) to \(}_{t}\), with the sought prior encoded in the NN's learnable weights \(\). Despite the effectiveness of NN optimizers in fitting complex mappings, it is hard to decipher the learned prior due to their black-box nature. To improve the interpretability and robustness of the approximate solver, optimization-based meta-learning decodes the "tractable optimizer" as a cascade of a few optimization iterations. The prior is captured by the shared hyperparameters of the optimizer. The first effort towards this direction is termed MAML , which relies on a \(K\)-step gradient descent (GD) optimizer

\[_{t}^{k+1}()=_{t}^{k}()- (_{t}^{k}();_{t}^{}),\;k =0,,K-1 \]

where task-invariant initialization \(_{t}^{0}=^{0}=\) parameterizes the prior information, and \(}_{t}=_{t}^{K}\) gives the desired approximate solver. Interestingly, despite the absence of an explicit regularization term (that is, \((_{t};)=0\)), it has been shown that, under second-order Taylor approximation, MAML's GD solver (2) satisfies 

\[}_{t}()_{t}^{*}()= *{argmin}_{_{t}}(_{t};_ {t}^{})+\|_{t}-^{0}\|_{ _{t}}^{2}\]

where the precision matrix \(_{t}\) is determined by \(\), \(K\), and \(^{2}(^{0};_{t}^{})\). This observation indicates MAML's optimizer approximately amounts to an implicit Gaussian prior \(p(_{t};)(_{t};^{0}, _{t}^{-1})\), with the shared initialization \(^{0}=\) serving as its mean vector.

Building upon MAML, various methods have been investigated to learn different prior pdfs in both implicit and explicit forms. For example, recent advances further render the (per-step) precision matrix learnable by replacing it with a \(\) that is common across tasks. Letting \(_{}\) denote the parameter of \(\), the prior parameter is thus augmented as \(:=[^{0},_{}^{}]\), where \({}^{}\) denotes transposition. However, a complete parametrization of \(\) would result in \(\) having prohibitively high dimensionality, that is, \(D=(d^{2})\). To ensure scalability with respect to \(D\), \(\) should have a sufficiently simple structure such as isotropic , diagonal , and or block diagonal  matrices. Inspired by transfer learning, one can instead split the model into an embedding "body" and a classifier/regressor "head," and learn their priors independently; that is, with \(_{t}^{}\) and \(_{t}^{}\) denoting the corresponding partitions of \(_{t}\), the prior is presumed factorable as \(p(_{t};)=p(_{t}^{};)p( {}_{t}^{};)\). On the one hand, the head typically has a nontrivial prior such as the Gaussian one . On the other hand, the body's prior is intentionally restricted to a degenerate pdf \(p(_{t}^{};):=(_{t}^{}-^{})\), where \(^{}\) is a subvector of \(\), and \(()\) is the Dirac delta function. This eliminates the need for optimizing \(_{t}^{}\) in (1b), thus markedly lowering the overall complexity for solving (1). Although freezing the body in (1b) allows for escalating the dimension of \(_{t}^{}\), it often leads to degraded empirical performance  compared to the full update (2). In addition to Gaussian and degenerate pdfs, sparse priors (Laplace distributions) have been investigated in the context of network pruning .

Meta-Learning using non-injective change of variables

Existing meta-learning algorithms rely on a _preselected_ pdf to parameterize the prior. However, the chosen pdf can have limited expressiveness; that is, it may have insufficient ability to offer an accurate fit due to its prefixed shape. Consider for instance a Gaussian prior pdf, which is inherently unimodal, symmetric, log-concave, and infinitely differentiable by definition. Such a prior may not be well-suited for tasks with multimodal or asymmetric parametric pdfs.

In this work, we propose to learn a _data-driven_ prior pdf that optimally fits the given tasks using a novel non-injective change-of-variable (NCoV) model. We thus term the proposed method as Meta-learning with NCoV (MetaNCoV). In contrast to preselected prior pdfs with fixed shapes, the advocated prior model can dynamically adjust its form to approximate a considerably wide range of pdfs, as will be demonstrated both theoretically and numerically. Furthermore, compared to conventional change-of-variable models such as generative adversarial networks (GANs)  and normalizing flows (NFs) , the introduced NCoV exhibits enhanced capacity for pdf estimation, especially in high-dimensional spaces. Change-of-variable models and their applications in pdf estimations will be first elaborated. All the proofs are delegated to the Appendix.

### Pdf estimation via change of variables

The key idea of change-of-variable model is to identify a transformation \(f\), through which a known pdf \(p_{}\) can be altered to approximate a target pdf \(q\). For instance, GANs  and variational autoencoders (VAEs)  seek a generator/encoder such that high-dimensional pdfs can be acquired from a low-dimensional latent Gaussian pdf \(p_{}=(,)\). Due to the dimensional discrepancy between signal and latent spaces, these models are typically utilized to estimate signals living on a low-dimensional manifold; e.g., images.

To enhance the model capacity as well as pdf tractability, NFs were introduced in  as a surrogate variational model for posterior inference. Recently, they have been shown also effective in estimating prior pdfs from a set of unannotated samples [6; 14; 7]. The formulation of NFs relies on the well-known change-of-variable formula. Given a continuous random vector \(^{d}\) with prior pdf \(p_{}\), and a bijection \(f:^{d}^{d}\), then \(^{}:=f()\) is also a continuous random vector with analytical pdf

\[p_{^{}}(^{})=p_{}(f^{-1}(^ {}))| J_{f^{-1}}(^{})|=}(f^{-1}(^{}))}{| J_{f}(^{})|}\ ( ) \]

where \(J_{f}(^{})\) denotes the Jacobian of \(f\) at \(^{}^{d}\), \(\) is the determinant, and \( J_{f} 0\) almost everywhere (a.e.) for bijective \(f\). To ensure the invertibility of \(f\), a prudent choice is to model it as a composition of a sequence of bijective functions \(f=f_{1} f_{2} f_{n}\).

In Bayesian inference , \(q\) is an intractable posterior, and \(f\) is optimized to minimize the KL-divergence between \(p_{^{}}\) and \(q\), or equivalently, maximize the so-termed evidence lower bound (ELBO). For density estimation , the wanted \(q\) is an unknown prior pdf, while \(f\) is acquired via maximum likelihood training. The obtained \(f\) can be leveraged in two important applications: i) probability estimation \(p_{^{}}() q()\) for a given sample \( q\) using (3), and ii) generation of a sample \(^{}=f(), p_{}\) for which \(p_{^{}} q\).

When \(d=1\), the probability integral transform (PIT) suggests that, the optimal \(f^{*}=Q^{-1} P_{}\) leads to precisely \(P_{^{}}=Q\) a.e., where \(Q\), \(P_{}\) and \(P_{^{}}\) are the cumulative distribution functions (cdfs) corresponding to \(q\), \(p_{}\) and \(p_{^{}}\), and \(q>0\) a.e. ensures \(Q\) is bijective. The resultant cdf \(P_{^{}}=P_{} f^{*-1}\) is a pushforward measure, also notated as \(P_{^{}}=f^{*}{}_{\#}P_{}\). In high-dimensional spaces (\(d>1\)) however, the existence of such an \(f^{*}\) may not hold due to the invertibility assumption of \(f^{*}\); see examples in e.g., [22, Section 4]. In fact, it has been shown that NFs are capable of modeling pdfs with a full support; i.e., when \(q>0\) on \(^{d}\).

### Improved pdf estimation via non-injective change of variable

To improve the fitting capacity of change-of-variable models for generic \(q\), especially those in high-dimensional spaces or without full support, the fresh idea of this work is to waive the injectivity assumption on \(f\). In doing so, we can generalize the PIT to an arbitrary \(q\), as illustrated in the following theorem.

**Theorem 3.1** (Multivariate PIT).: _Let \(P_{}:^{d}\) be the cdf of continuous random vector \(:=[Z_{1},,Z_{d}]^{}\) with \(\{Z_{i}\}_{i=1}^{d}\) mutually independent. For any differentiable a.e. cdf \(Q:^{d}\), there exists a non-decreasing function \(f^{*}:^{d}^{d}\) that the random vector \(^{}:=f^{*}()\) has cdf_

\[P_{^{}}=Q\ (). \]

_Remark 3.2_ (Choice of source distribution).: In the theorem, the prior distribution for the source random vector \(\) can be chosen arbitrarily, as if it is continuous and has mutually independent entries. Popular choices include standard Gaussian \((_{d},_{d})\) and uniform \((^{d})\).

_Remark 3.3_ (Comparison with injective NFs).: While conventional NFs (3) require \(J_{f}\) a.e. to ensure the injectivity of \(f\), Theorem 3.1 relaxes this assumption to allows \(f\) being non-injective and thus enables \(^{}=f()\) to match an arbitrary target distribution (even discrete one) in a high-dimensional space. It is worth mentioning that the mild assumption on the differentiability of \(Q\) is merely used to guarantee the existence of \(q\), which can be easily satisfied. However, one limitation of the advocated NCoV is that it generally has no analytical solution for the resultant surrogate pdf

\[p_{^{}}(^{})=_{^{d}}p_{}()[^{}-f()]d. \]

As a remedy, efficient numerical integration can be performed to estimate \(p_{^{}}\) when \(d\) is small. Additional comparisons with NFs and optimal transport are deferred to Appendix E.

While Theorem 3.1 suggests the existence of the optimal \(f^{*}\) that incurs the exact match \(p_{^{}}=q\), the expression for such an \(f^{*}\) relies on the sought \(q\), which is typically intractable or unknown. Therefore, a feasible alternative is to resort to a tractable parametric \(f(;_{f})\), which approximates \(f^{*}\) by learning \(_{f}\) from the provided data. To further compare NCoVs with NFs, we will focus exclusively on Sylvester NF  in the following sections, but our analysis can be readily generalized to other transformations; see Remark 3.7. Sylvester NF was introduced in  to improve the expressiveness of planar NF  by increasing its "width". In particular, Sylvester NF adopts the form

\[f(;_{f}):=+( +),\ ^{d} \]

where \(^{d m},^{m d},^{m}\) are learnable weights with \(m\) being the number of hidden neurons (a.k.a. width), \(\) is an entry-wise nonlinear operator, and \(_{f}:=[()^{}, ()^{},^{}]^{}\). It can be easily verified that the Sylvester NF boils down to the planar one when \(m=1\). Akin to other NFs, one can also increase the "depth" of the flows by stacking multiple Sylvester NF layers into a chain \(f_{1} f_{2} f_{n}\). The next theorem states that, the optimal \(f^{*}\) can be approximated to arbitrary precision using a sufficiently wide one-layer Sylvester NCoV.

**Definition 3.4**.: A random vector on \(^{d}\) is said to be tail-convergent if i) it has a pdf \(p:^{d}^{+}\{0\}\), and ii) for \(>0\) there exists a bounded \(E^{d}\) for which

\[_{^{d} E}p<. \]

**Theorem 3.5** (Universal approximation via Sylvester NCoVs).: _Let \(P_{}\) denote the cdf of tail-convergent continuous random vector \(^{d}\) with mutually independent entries, and \(Q\) a Lipschitz cdf of a tail-convergent random vector. For any \(>0\), there exists cdfs \(,\) for which the corresponding pdfs \(,\) vanishes outside compact sets \(E_{p},E_{q}\), and_

\[|P_{}()-()|<,\ |Q_{}( )-()|<,\ ^{d}. \]

_Moreover, let \(E E_{p}\) be any set on which the optimal \(f^{*}\) matching \(_{}\) to \(\) is injective and right-continuous. There exists a Sylvester NCoV \(f\) and a zero-measure set \(E_{0}\), such that_

\[|f()-f^{*}()| <,\  E_{p} E_{0}, \] \[|P_{}()-Q f()| <,\  E E_{0}. \]

We have shown that when \(f^{*}\) is injective, the cdf of the optimally transformed \(^{}=f^{*}()\) can be written as a pushforward \(Q=P_{^{}}=P_{} f^{*-1}\). Likewise, this relationship remains valid when restricting \(f^{*}\) to a set \(E\) on which \(f^{*}\) is injective. However, since the Sylvester NCoV \(f\) may not be injective on \(E\), one cannot directly compare \(Q\) with \(P_{} f^{-1}\). Fortunately, this pushforward can be equivalently written as \(P_{}()=Q f^{*}(),\  E\); see Lemma B.1 in the Appendix. Utilizing this alternative relationship, Theorem 3.5 states that the Sylvester NCoV \(f\) not only approximates \(f^{*}\) a.e. on \(E_{p}\), but also results in pushforward approximation \(P_{} Q f\) a.e. on \(E\).

_Remark 3.6_ (Mild assumptions).: The assumptions in Theorem 3.5 are mild and common. In particular, tail-convergence only requires the probability of large deviation diminishing to \(0\) as the norm of the random vector goes to \(+\), while imposing no constraint on the decaying rate. This assumption can be easily satisfied by a wide family of distributions, even including the heavily-tailed ones. Under this benign assumption, (8) suggests \(P_{}\) and \(Q\) can be approximated by alternatives \(,\) with pdfs \(,\) having truncated tails. This is crucial to universal approximation, which typically requires \(f^{*}\) to be bounded or Lebesgue integrable . Moreover, the Lipschitzness of \(Q\) is solely utilized to ensure the boundness of its gradient, namely the pdf \(q\). This can be also readily met by most practical cdfs.

_Remark 3.7_ (Generalization to other NFs).: Although Theorem 3.5 primarily focuses on one-layer Sylvester NCoVs, similar analysis for other transformation \(f\) can be acquired by employing different universal approximation models. For instance, results for multi-layer planar NCoVs and multi-layer Sylvester NCoVs can be respectively established leveraging  and .

_Remark 3.8_ (Influence of \(\)).: It is worth noting that the width \(m\) of the Sylvester NCoV depends on \(\) as well as the optimal \(f^{*}\). Smaller \(\) typically leads to larger \(m\). Additionally, the nonlinearity \(\) must be sigmoidal; see Definition B.3 in the Appendix for details.

Figure 1: Transforming a standard Gaussian pdf into multi-modal target pdfs using Sylvester NCoVs.

### Meta-learning universal priors via MetaNCoV

Next, we elucidate how universal priors can be learned in meta-learning by harnessing the proposed NCoV model. Different from existing works that rely on prespecified prior forms such as Gaussian pdfs, the novel concept of this work is to learn a data-driven prior that optimally conforms with the given tasks. This is achieved by transforming the random vector \(^{d}\) with a known prior \(p_{}\) to \(^{}=f(;_{f})\), whose pdf is given by (5). This \(p_{^{}}\) acts as a surrogate model for the unknown \(p(_{t};)\), and learning the prior parameter \(\) thus boils down to optimization of the transformation parameter \(_{f}\). Nevertheless, as discussed in Remark 3.3, \(p_{^{}}\) typically has no close-form expression when \(f\) is non-injective. Therefore, instead of directly optimizing \(_{t}\), we propose to optimize the latent vector \(_{t}\) corresponding to \(_{t}=f(_{t};_{f})\), which yields

\[_{_{f}} _{t=1}^{T}(f(_{t}^{*}(_{f}); _{f});_{t}^{}) \] \[_{t}^{*}(_{f})=*{ argmin}_{_{t}}(f(_{t};_{f}); _{t}^{})+_{}(_{t}),\;  t \]

where \(_{}(_{t}):=- p_{}(_{t})\) is the nlp regularizer, and \(_{t}^{*}\) is thus the MAP estimator for \(_{t}\).

Similar to (2), the global task-level minimizer \(_{t}^{*}\) is generally infeasible to attain. Hence, a tractable alternative is to rely on an approximate GD solver. Interestingly, our formulation (10) naturally offers a convenient initialization using the _maximum a priori estimator_

\[_{t}^{0}=*{argmax}_{_{t}}p_{}( _{t})=*{argmin}_{_{t}}_{}(_{t}),\; t \]

As an example, choosing \(p_{}=(_{d},_{d})\) automatically gives \(_{t}^{0}=_{d}\) and the corresponding \(_{t}^{0}=f(_{d};_{f})\). This elegantly removes the need for separately learning the task-invariant initialization \(^{0}\), which is exactly the maximum a priori estimator of the preselected Gaussian prior pdf \(p(_{t};)=(^{0},_{t})\). In fact, the task-invariant initialization reflects our optimal guess of \(_{t}\) before accessing any task-specific data, and can be naturally derived by maximizing the prior pdf. It also worth noting that while the idea of optimizing the latent (instead of primal) variables shares similarities with , the latent space in  is designed from a different perspective, which is low-dimensional to the end, and requires an initialization.

To this end, (10) can be solved using a standard alternating optimizer. The resultant MetaNCoV algorithm is listed step-by-step in Algorithm 1, where the inner-level (10b) and outer-level (10a) are respectively optimized using \(K\)-step GD and mini-batch stochastic GD.

While our idea has the potential to be broadened beyond meta-learning, we must emphasize that our current setup is specifically tailored to meta-learning, which does not require a tractable pdf, but rather demands enhanced prior expressiveness. We should also highlight that the _intractability_ of (5) prohibits learning NCoV via conventional approaches such as maximum likelihood training and evidence lower-bound maximization - this thus necessitates careful attention and extra certain designs when applying NCoV to other domains.

## 4 Numerical tests

In this section, we test and showcase the empirical superiority of MetaNCoV on both synthetic and real datasets. All datasets descriptions and hyperparameter setups are deferred to the Appendix C. Codes for reproducing the results are available at [https://github.com/zhangyilang/MetaNCoV](https://github.com/zhangyilang/MetaNCoV).

### Tests with toy data

Here, we investigate an intricate yet interesting scenario to demonstrate the efficacy of NCoVs to approximate complex multi-modal pdfs in two-dimensional (2D) settings. The primary objective is to transform a standard Gaussian random vector \((_{2 1},_{2 2})\) into multi-modal complex pdfs. The outcomes of this experiment are presented in Figure 1. The lower row displays the ground-truth pdfs \(q\) of interest, while the upper row showcases the numerically estimated pdfs of the transformed random vector \(^{}=f()\), where \(f\) is a Sylvester NCoV, and the pdf of \(^{}\) is estimated via (5). As clearly evidenced in these results, the advocated NCoVs exhibit their capability to effectively convert

[MISSING_PAGE_FAIL:8]

prior. Moreover, a remarkable performance gain can be observed on the \(1\)-shot dataset. This justifies the claim that prior can be particularly informative when the training data are extremely scarce. For an apples-to-apples comparison, methods that use pre-trained feature extractors or more complicated models (e.g., residual networks) are not included in this table. The compatibility of MetaNCoV to these models will be demonstrated in the subsequent tests.

The second test evaluates MetaNCoV on miniImageNet and tieredImageNet feature embeddings extracted using a pre-trained Wide ResNet(WRN)-28-10 backbone . Compared to the \(4\)-block CNN, this model has a greater number of parameters and thus enhanced expressiveness. The results are summarized in Table 2, where MetaNCoV is implemented with MetaSGD  and MC . In all tests, MetaNCoV brings about notable performance improvement compared to the corresponding baselines. This validates MetaNCoV's effectiveness and flexibility as a plug-in prior module.

The last test assesses the performance of MetaNCoV on the CUB-200-2011 dataset . In contrast to the previous two datasets that contain nature images of distinct objects, this dataset specifically focuses on birds of various species. While the classification of nature objects primarily relies on low-level features such as shapes and colors, classifying various birds requires further recognition of high-level features including textures and segmentations. To learn these complicated features, the model needs to be either trained with sufficient data, or equipped with a powerful prior. Table 3 showcases the performances of different meta- and metric-learning methods on such a dataset. Again, our MetaNCoV method is markedly effective on the \(1\)-shot dataset where data are exceptionally limited. This highlights the significance of an expressive prior. For the \(5\)-shot dataset where data are relatively abundant, its performance is also comparable to the state-of-the-art ones.

### Ablation study

Next, ablation tests are conducted to analyze the performance gain of MetaNCoV. The test is carried out on the miniImageNet dataset, with results gathered in Table 4. The first ablation investigates the impact of the advocated NCoVs over the injective ones. To ensure the injectivity of the Sylvester NF \(f\), we follow the QR parameterization recommended in . One can see the improved performance of NCoV due to its enhanced expressiveness, which numerically verifies Theorem 3.1 and Remark 3.3. The second ablation examines the influence of nonlinear function \(\) in the Sylvester NCoVs. By changing the \(\) from sigmoid to the popular ReLU activation, a degradation of empirical performance can be observed. This observation corroborates with Remark 3.8. Additional experiments and visualizations can be found in Appendix D.

### Cross-domain generalization

This subsection showcases the generalization capacity of MetaNCoV in cross-domain few-shot learning. This test is more challenging compared to previous ones due to the domain gap between

    &  \\  & \(1\)-shot (\(\%\)) & \(5\)-shot (\(\%\)) \\  NCoV (baseline) & \(}\) & \(}\) \\ Injective NF & \(56.72_{ 1.46}\) & \(69.41_{ 0.68}\) \\ ReLU \(\) & \(56.54_{ 1.46}\) & \(69.84_{ 0.68}\) \\   

Table 4: Ablation tests for MetaNCoV.

    &  &  \\  & & \(1\)-shot (\(\%\)) & \(5\)-shot (\(\%\)) \\  MatchingNet  & metric-learning & \(45.30_{ 1.03}\) & \(59.50_{ 1.01}\) \\ MAML  & meta-learning & \(58.13_{ 0.36}\) & \(71.51_{ 0.30}\) \\ ProtoNet  & metric-learning & \(37.36_{ 1.00}\) & \(45.28_{ 1.03}\) \\ RelationNet  & metric-learning & \(58.99_{ 0.52}\) & \(71.20_{ 0.40}\) \\ DN4  & metric-learning & \(53.15_{ 0.84}\) & \(}\) \\ MattML  & meta-learning & \(66.29_{ 0.56}\) & \(80.34_{ 0.30}\) \\  MAML + MetaNCoV & meta-learning & \(_{ 1.36}\) & \(80.41_{ 0.60}\) \\ MetaSGD + MetaNCoV & & \(}\) & \(80.54_{ 0.59}\) \\   

Table 3: Performance comparison of MetaNCoV against meta-learning and metric-learning methods on the CUB-20-2011 dataset. For fairness, the backbone model is a \(4\)-block CNN.

the meta-training and meta-testing phases. By shifting the task domain, this test aims to assess the overfitting of the learned prior to a specific domain. Our test setup follows from , where the prior model is meta-trained on the miniImageNet  dataset, and meta-tested on tieredImageNet , Cars , and CUB  datasets. Our MetaNCoV is implemented with MetaSGD  in this test. As shown in Table 5, our method consistently outperforms popular meta-learning approaches in such a setup, especially in the 1-shot case. This not only confirms the cross-domain generalization of MetaNCoV, but again justifies the importance of expressive prior when data are exceedingly limited.

## 5 Conclusions and outlook

An informative prior plays a crucial role in training a large-scale model with limited small-scale data. This work introduced a novel NCoV model for learning an expressive task-invariant prior. By transforming a known pdf of a continuous random vector, the NCoV model enables a large family of target pdfs. As a flexible plug-in prior model, our MetaNCoV method offers enhanced prior expressiveness compared to existing meta-learning methods that rely on preselected prior pdfs. Numerical studies validate our theoretical analysis, and highlight the superior performance of the proposed method, especially when datasets are scarce. Our future research agenda includes i) investigation of more generic universal approximation theorems; ii) bilevel convergence analysis for the MetaNCoV method; and, iii) implementation of MetaNCoV with alternative transformations, backbone models, and meta-learning methods.