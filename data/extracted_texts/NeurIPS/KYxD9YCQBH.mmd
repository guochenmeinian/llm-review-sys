# ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction

Jia Guo\({}^{1}\) Shuai Lu\({}^{2}\) Lize Jia\({}^{2}\) Weihang Zhang\({}^{2}\) Huiqi Li\({}^{1,2}\)

\({}^{1}\)School of Information and Electronics, Beijing Institute of Technology, China

\({}^{2}\)School of Medical Technology, Beijing Institute of Technology, China

guojia.jeremy@gmail.com, lushuaie@163.com

{3220212096, zhangweihang, huiqili}@bit.edu.cn

###### Abstract

Most advanced unsupervised anomaly detection (UAD) methods rely on modeling feature representations of frozen encoder networks pre-trained on large-scale datasets, e.g. ImageNet. However, the features extracted from the encoders that are borrowed from natural image domains coincide little with the features required in the target UAD domain, such as industrial inspection and medical imaging. In this paper, we propose a novel epistemic UAD method, namely ReContrast, which optimizes the entire network to reduce biases towards the pre-trained image domain and orients the network in the target domain. We start with a feature reconstruction approach that detects anomalies from errors. Essentially, the elements of contrastive learning are elegantly embedded in feature reconstruction to prevent the network from training instability, pattern collapse, and identical shortcut, while simultaneously optimizing both the encoder and decoder on the target domain. To demonstrate our transfer ability on various image domains, we conduct extensive experiments across two popular industrial defect detection benchmarks and three medical image UAD tasks, which shows our superiority over current state-of-the-art methods. Code is available at: https://github.com/guojiajeremy/ReContrast

## 1 Introduction

Unsupervised anomaly detection (UAD) aims to recognize and localize anomalies based on the training set that contains only normal images. UAD has a wide range of applications, e.g., industrial defect detection and medical disease screening, addressing the difficulty of collection and labeling of all possible anomalies.

Efforts on UAD attempt to learn the distribution of available normal samples. Most state-of-the-arts utilize networks pre-trained on large-scale datasets, e.g. ImageNet , for extracting discriminative and informative feature representations. **Feature reconstruction**[2; 3; 4] and **feature distillation** methods [5; 6] are proposed to reconstruct features of pre-trained encoders as alternatives to **pixel reconstruction**[7; 8], because learned features provide more informative representations than raw pixels [2; 3; 9]. The above approaches can be categorized as epistemic methods, under the hypothesis that the networks trained on normal images can only construct normal regions, but fail for unseen anomalous regions. **Feature memory & modeling** methods [10; 11; 12; 13; 14] memorize and model all anomaly-free features extracted from pre-trained networks in training, and compare them with test features during inference. In feature reconstruction, the parameters of encoders should be frozen to prevent the networks from converging to a trivial solution, i.e., pattern collapse [2; 3]. In feature distillation and feature memory & modeling methods, there are no gradients for optimizingfeature encoders. Unfortunately, poor transfer ability is caused by the semantic gap between natural images on which the frozen networks were pre-trained and various UAD image modalities.

Most recently, sparse studies have addressed the transfer problem in UAD settings. CFA  and SimpleNet unanimously utilize a learnable linear layer to adapt the output features of the encoder to task-oriented features, while still keeping the encoder frozen. However, the layer after a frozen encoder may be insufficient for adaptation as it can hardly recover the domain-specific information already lost during extraction, especially when the target domain is far from natural images, e.g. medical images. More related works are presented in Appendix A.

In this work, we focus on orienting the whole UAD network in target domains. We propose **R**econstructive **Contrastive** Learning (ReContrast) for domain-specific anomaly detection by optimizing all parameters end-to-end. We start with epistemic UAD methods that detect anomalies by reconstruction errors between the encoder and the reconstruction network (decoder) [2; 9]. By inspecting the behavior of encoder, we find that direct optimization of the encoder causes diversity degradation of encoder features and thus harms the performance. Accordingly, we introduce **three** key elements of contrastive learning (Figure 1(a)) into feature reconstruction UAD (Figure 1(b)), building a 2-D feature map contrast paradigm to train the encoder and decoder jointly on target images without obstacles. First, to mimic the role of global average pooling (GAP), a new optimization objective, namely global cosine distance, is proposed to make the contrast in a global manner to stabilize the optimization. Second, we bring in the essential operation, i.e., stop-gradient, which prevents positive-pair contrastive learning [16; 17] from pattern collapse. Third, as image augmentation can introduce potential anomalies, we propose to utilize two encoders in different domains to generate representations in two views from the same input image. In addition, we propose a hard-mining strategy for optimizing normal regions that are hard to reconstruct, to magnify the difference between intrinsic reconstruction error and epistemic reconstruction error. It is noted that ReContrast is **NOT** a self-supervised learning method (SSL) for pre-text pre-training like SimSiam  and SimCLR , but an end-to-end anomaly detection approach.

To validate our transfer ability on various domains, we evaluate ReContrast with extensive experiments on two widely used industrial defect detection benchmarks, i.e., MVTec AD  and VisA , and three medical image benchmarks, including optical coherence tomography (OCT) , color fundus image , and skin lesion image . Without elaborately designing memory banks or handcrafted pseudo anomalies, our proposed method achieves superior performance compared with previous state-of-the-arts. Notably, we present an excellent image-level AUROC of 99.5% on MVTec AD, which reduces the error of the previous best by nearly half. The contributions of this study can be summarized as:

* We propose a simple yet effective UAD method to handle the poor transfer ability of the frozen ImageNet encoder when applied to UAD images with a large domain gap. Targeting the similarity and difference between contrastive learning and feature reconstruction, **three** key elements are elegantly introduced into feature reconstruction UAD for optimizing the entire network without obstacles.

Figure 1: Comparison of architectures. (a) Contrastive learning [16; 17]. (b) Feature reconstruction UAD [2; 9]. (c) Proposed ReContrast. Two decoders share the same weights. z is the output of encoder. q is the output of predictor or decoder. z’ and q’ is another view of z and q, respectively.

* Inspired by the GAP in contrastive learning, we design a new objective function, introducing globality into point-by-point cosine distance for improving training stability.
* We utilize the key operation of positive-pair contrastive learning, i.e., stop-gradient, to prevent the encoder from pattern collapse.
* We propose a novel way of generating contrastive pairs to avoid potential anomalies introduced by image augmentation.

## 2 Method: From Reconstruction to ReContrast

In the following subsections, we present an exploration journey starting from a conventional feature reconstruction baseline to our ReContrast. We propose three cohesive components inspired by the elements of contrastive representation learning and present them in a "motivation, connection, mechanism" manner. We believe such progressive exploration would illustrate the motivation of our method better.

### Preliminary: Reverse Distillation

First, we revisit a simple yet powerful epistemic UAD method based on feature reconstruction, i.e., Reverse Distillation (RD4AD) . The method is illustrated in _Config.A_ of Figure 2. RD4AD consists of an encoder (teacher), a bottleneck, and a reconstruction decoder (student). Without loss of generality, the first three layers of a standard 4-layer network are used as the encoder. The encoder extracts informative feature maps with different spatial sizes. The bottleneck is similar to the fourth layer of the encoder but takes the feature maps of the first three layers as the input. The decoder is the reverse of the encoder, i.e., the down-sampling operation at the beginning of each layer is replaced by up-sampling. During training, the decoder learns to reconstruct the output of the encoder layers by maximizing the similarity between feature maps. During inference, the decoder is expected to reconstruct normal regions of feature maps but fails for anomalous regions as it has never seen such samples.

Let \(f_{E}^{k},f_{D}^{k}^{C^{k} H^{k} W^{k}}\) denote the output feature maps from the \(k^{th}\) layer of the encoder and decoder respectively, where \(C^{k}\), \(H^{k}\), and \(W^{k}\) denote the number of channels, height, and width of the \(k^{th}\) layer, respectively. Regional cosine distance [2; 6; 24] is used to measure the difference between \(f_{E}^{k}\) and \(f_{D}^{k}\) point by point, obtaining a 2-D distance map \(^{k}^{H^{k} W^{k}}\):

Figure 2: From RD4AD to ReContrast. (a) Training configurations. (b) Calculation of anomaly map. The calculation and optimization of regional and global cosine distance is presented in Figure 3.

\[^{k}(h,w)=1-^{k}(h,w)^{T} f_{D}^{ k}(h,w)}{\|f_{E}^{k}(h,w)\|\ \|f_{D}^{k}(h,w)\|},\] (1)

where \(\|\|\) is \(_{2}\) norm, and a set of \((h,w)\) locates a spatial point in the feature map. The calculation is depicted in Figure 3(a). A large value in \(^{k}\) indicates anomaly at the corresponding position. Final anomaly score map \(^{map}\) is obtained by up-sampling all \(^{k}\) to image size and a sum operation. The maximum value of \(^{map}\) is taken as the anomaly score for the image, denoted as \(^{img}\). During training, the anomaly maps are minimized on normal images by the regional cosine distance loss, given as:

\[_{region}=_{k=1}^{3}W^{k}}_{h=1}^{H^{k}} _{w=1}^{W^{k}}1-^{k}(h,w)^{T} f_{D}^{k}(h,w )}{\|f_{E}^{k}(h,w)\|\ \|f_{D}^{k}(h,w)\|}.\] (2)

The optimization of \(_{region}\) is illustrated in the upper-left of Figure 3(b). Notably, though \(_{region}\) was proposed clearly as the loss function of RD4AD , the official code1 implemented a **different** function that makes an extra dimension flatten operation (maybe by coding bug). However, we observe extreme instability during the training with \(_{region}\), producing degenerated I-AUROC of 95.3% on MVTec AD compared with 98.5% reported in the paper. The I-AUROC of 95.3% is more consistent with other methods using \(_{region}\)[6; 24]. In the next subsection, we will show that the seemingly unreasonable "bug" in the code improves the performance by explicitly introducing globality into optimization.

### Global Cosine Similarity

It is a convention to use global average pooling (GAP) to pool the 2-D feature map of the last network layer to a 1-D representation, in both supervised classification and self-supervised contrastive learning. In UAD, GAP will simply mess up all feature points together, losing the ability to distinguish normal and anomalous regions, as shown in the upper-right of Figure 3(b). Therefore, we try to directly bring globality into the loss function while maintaining the point-to-point correspondence between \(f_{E}^{k}\) and \(f_{D}^{k}\). Let \(\) denote a flatten operation that casts a 2-D feature map \(f^{C H W}\) to a vector \(v^{CHW}\). Our proposed global cosine distance loss is given as:

\[_{global}=_{k=1}^{3}1-(f_{E}^{k})^{ T}(f_{D}^{k})}{\|(f_{E}^{k} )\|\ \|(f_{D}^{k})\|},\] (3)

Figure 3: Cosine distances. (a) Calculation of regional and global cosine distance. (b) Optimization of distance losses. The black arrows denote optimization direction. The black and red dotted circles indicate the range of whether the feature point pair is close enough to be selected as easy-normal.

where the number of cosine dimension is \(C H W\) instead of \(C\) in \(_{region}\), as depicted in Figure 3(a). Intuitively, \(^{k}\) is also minimized along with the minimization of \(_{global}\). It is easy to prove that minimizing \(_{global}\) to 0 equals to minimizing \(^{k}\) to 0. Though losses are not completely optimized to 0 in neural network training, the regional \(^{map}\) still works as the anomaly map. The model trained with \(_{global}\) is named as _Config.B_.

As shown in Figure 5(a), the performance of _Config.B_ (blue) is much more favorable and stable than _Config.A_ (purple) during training. Here, we briefly analyze the underlying mechanism by plotting the landscape of \(^{map}\) (average of all samples) against model parameters near optimized minima in Figure 4. We observe two differences. First, the overall landscape of the model optimized by \(_{global}\) is flatter than that by \(_{region}\), which implies \(^{map}\) is more stable during training with \(_{global}\). The instability of \(_{region}\) can be caused by the point-by-point distance, where the fitting of one region may cause under-fitting of the others. \(_{global}\) can be regarded as the distance between manifolds of feature points, as depicted in the lower-left of Figure 3(b), which measures the consistency globally instead of excessively focusing on individual regions. Second, the landscape around the final minima of the model trained by \(_{global}\) is sharper than that by \(_{region}\). Previous explorations on the geometry of loss landscapes suggest that flat minima generalize better on unseen samples [25; 26], which is unwanted in UAD settings. The real mechanism can be more complicated and is still under further investigation.

### Feature Degradation of Optimized Encoder

It seems easy to adapt the network in the target domain by jointly optimizing the encoder and decoder, as illustrated in Figure 2_Config.C_. However, previous attempts  suggest that training encoders would lead encoders to extract indiscriminative features that are easy to reconstruct, which is also described as pattern collapsing or converging to trivial solutions . We observe a similar phenomenon that the training loss quickly decreases to nearly zero, as shown by the orange dotted line in Figure 5(a).

To inspect the behavior of the encoder, we probe the diversity of encoder feature maps by the standard deviation (_std_) of \(_{2}\) normalized feature points \(f_{E}^{k}(h,w)/\|f_{E}^{k}(h,w)\|_{2}\). The larger the _std_, the more discriminative the feature, and the more distinctive between different regions. We plot the feature diversity of outputs of the \(2^{nd}\) encoder layer during training in Figure 5(b). The _std_ of _Config.C_ drops quickly during training, while the _std_ of _Config.B_ keeps constant as a comparison. In addition, the _std_ of _Config.C_ does not completely collapse to zero, which explains the passable performance. The outputs of other middle layers follow the same trend. The observation indicates that the degeneration of performance is attributed to the degradation of encoder features. It is noted that the _std_ starts from different values because of the different modes of batch normalization (BN)  during training.2

Figure 4: \(^{map}\) landscape in parameter space (reduced to 2-dimension), on APTOS , visualized using .

### Stop Gradient

Contrastive learning for SSL maximizes the similarity between the representations of two augmentations (views) of one image while trying to avoid collapsing solutions in which all inputs are encoded to a constant value. SimSiam  indicated that the stop-gradient operation plays an essential role in preventing collapsing for SSL methods with positive pairs only [16; 17]. Intriguingly, we found that contrastive SSL can be explained from the perspective of feature reconstruction. The predictor (a multi-layer perceptron, MLP) can be regarded as a reconstruction network that constructs the representation of one view \(x\) to match the representation of another view \(x^{}\), as in Figure 1. Vice versa, feature reconstruction UAD can be transformed into contrastive learning as well, regarding the reconstruction network as a predictor.

To this end, we introduce the stop-gradient operation into feature reconstruction, transforming it into a contrastive-like paradigm. The network is optimized by contrasting the feature maps of the encoder and decoder, as shown in _Config.D_ of Figure 2. The gradients do not propagate directly into the encoder, but back into the encoder through the decoder. It is implemented by modifying (3) as:

\[_{global}=_{k=1}^{3}1-(f_{E}^{k }))^{T}(f_{D}^{k})}{sg(\| (f_{E}^{k})\|)\ \|(f_{D}^{k})\|}\] (4)

where \(sg\) denotes the stop-gradient operation. The training of this configuration can be intuitively explained as a "mutual reinforcement". The optimization of \(f_{D}^{k}\) drives the encoder to be more specific and informative in the target domain by the gradients from the decoder, while the more domain-specific \(f_{E}^{k}\) of encoder requires further optimization of \(f_{D}^{k}\). There are previous works trying to explain the deeper mechanism of stop gradient in contrastive learning. In , the authors hypothesize this configuration as an implementation of an Expectation-Maximization (EM) algorithm that implicitly solves two underlying sub-problems with two sets of variables. In , the authors argue that there are flaws in the hypothesis of  and suggest that the decomposed gradient vector (center gradient) helps prevent collapse via the de-centering effect.

As shown in Figure 5(a) and (b), _Config.D_ can constantly extract more discriminative features without degeneration, boosting it to perform better than _Config.C_ at the beginning of training. Previous works [9; 3] suggested that the reconstruction decoder may learn an "identical shortcut" that well reconstructs both seen and unseen samples, which explains the performance drop of _Config.D_ after reaching the peak in Figure 5(a). To handle this issue, contrastive pairs are introduced.

### Contrastive Pairs

Contrastive learning methods make use of (positive) contrastive image pairs, based on the intuition that the semantic information of different augmented views of the same image should be the same. Without image augmentations (\(x\)=\(x^{}\)), contrastive learning degrades into a self-reconstruction paradigm that the predictor predicts the input of itself, which may also collapse into an "identical shortcut" just like _Config.D_. In epistemic UAD methods, it is impossible to construct contrastive pairs by image augmentation, because 1) any augmentation (flip, cutout, color jitter, etc.) could be potential

Figure 5: (a) I-AUROC and loss during training on APTOS. (b) Feature diversity during training on APTOS. (c) Example \(^{map}\) of a _Hazelnut_, optimized by \(_{global}\) or \(_{global-hm}\).

anomalies; and 2) the feature maps of two augmented views lost the regional correspondence after spatial augmentations (shift, rotate, scale, etc.). Therefore, we propose an augmentation-free method to construct two views of one image.

In previous explorations, the encoder is adapted to the target image domain and generates feature representations in a domain-specific view. We additionally introduce a frozen encoder network without any adaptation throughout the training, which perceives images in the view of pre-train image domain. As shown in _Config.E_ of Figure 2, the decoder and bottleneck take the features of the domain-specific encoder to reconstruct the frozen encoder; vice versa, they simultaneously take the features of the frozen encoder to reconstruct the domain-specific encoder, building a complete two-view contrastive paradigm. This configuration can also be interpreted as cross-reconstruction, as the decoder reconstructs the information of the frozen encoder given the input of the adapted encoder; and reconstructs the information of the adapted encoder given the input of the frozen encoder. With six pairs of feature maps, the \(^{map}\) is obtained by adding up the six up-sampled cosine distance maps. As shown in Figure 5(b), the encoder feature diversity of _Config.E_ not only holds but slightly increases during training.

### Magnify Epistemic Error by Hard-Normal Mining

Due to network information capacity, intrinsic reconstruction errors exist in available normal regions. The errors of normal details and edges (hard-normal) are generally higher than other plain regions (easy-normal), leading to confusion between the intrinsic error of hard-normal regions and the epistemic error of unseen anomalies, as demonstrated in the lower-left of Figure 5(c). It was proposed to hard-mine the hard-normal regions by simply discarding the "easy" pairs of \(f_{E}^{k}(h,w)\), \(f_{D}^{k}(h,w)\) with small cosine distance in \(_{region}\). However, arbitrarily discarding feature points in \(f_{D}^{k}\) breaks the global feature point manifolds in \(_{global}\). Therefore, we propose a hard mining strategy, namely \(_{global-hm}\), focusing on the optimization of hard-normal regions to widen the gap between the epistemic errors caused by unseen anomalies and the errors of all normal regions. The gradients of easy \(f_{D}^{k}(h,w)\) are discarded instead of eliminating \(f_{D}^{k}(h,w)\) itself, by modifying \(f_{D}^{k}\) as:

\[f_{D}^{k}(h,w)=\{sg(f_{D}^{k}(h,w)), \ if\ ^{k}(h,w)<(^{k}(h,w)) \ +\ \ *(^{k}(h,w)),\\ f_{D}^{k}(h,w),\ else.\] (5)

where \((^{k}(h,w))\) is the average regional distance on the dataset (approximated on mini-batch), \((^{k}(h,w))\) is the standard deviation of the regional distance, and \(\) is a hyper-parameter to control the discarding rate. The optimization of \(_{global-hm}\) is depicted in the lower-right of Figure 3(b). As shown in Figure 5(c), the model trained by \(_{global-hm}\) activates fewer normal regions than the model trained by \(_{global}\). The usage of hard mining strategy can also mitigate the over-fitting of easy training samples.

## 3 Experiments

In this section, we evaluate the proposed method on two popular UAD applications, i.e., industrial defect inspection and medical disease screening. We refer to our complete approach (ReContrast+\(_{global-hm}\)) as ReContrast and Ours in the Experiments section for simplicity.

### Experimental Settings

**Datasets**. **MVTec AD** is the most widely used industrial defect detection dataset, containing 15 categories of sub-datasets (5 textures and 10 objects). **VisA** is a challenging industrial defect detection dataset, containing 12 categories of sub-datasets. **OCT2017** is an optical coherence tomography dataset . **APTOS** is a color fundus image dataset, available as the training set of the 2019 APTOS blindness detection challenge . **ISIC2018** is a skin disease dataset, available as task 3 of ISIC2018 challenge . Detailed information is presented in Appendix B.

**Implementation**. WideResNet50  pre-trained on ImageNet  is utilized as the encoder by default. AdamW optimizer  is utilized with \(\)=(0.9,0.999) and weight decay=1e-5. The learning rates of new (decoder and bottleneck) and pre-trained (encoder) parameters are 2e-3 and 1e-5, respectively. The network is trained for 3,000 iterations on VisA, 2,000 on MVTec AD and ISIC2018, and 1,000 on APTOS and OCT2017. The \(\) in equation (5) linearly rises from -3 to 1 in the first one-tenth iterations and keeps 1 for the rest training. More experimental details and environments are presented in Appendix C.

**Metrics**. Image-level anomaly detection performance is measured by the Area Under the Receiver Operator Curve (I-AUROC). F1-score (F1) and accuracy (ACC) are adopted for medical datasets following . The operating threshold of F1 and ACC is determined by the optimal value of F1. For anomaly segmentation, pixel-level AUROC (P-AUROC) and Area Under the Per-Region-Overlap (AUPRO)  are used as the evaluation metrics. AUPRO is more meaningful than P-AUROC because of the unbalanced amount of normal and anomalous pixels .

### Anomaly Detection and Segmentation on Industrial Images

Anomaly detection results on MVTec AD are shown in Table 1. Our approach achieves a superior average I-AUROC of **99.5%**. In the aspect of error (100%\(-\)I-AUROC), we achieve only 0.5%, reducing the previous SOTA error of PatchCore (0.9%) by relatively **44%**. Anomaly segmentation results are shown in Table 2 (average of 15 categories). Our ReContrast achieves SOTA performance measured by both P-AUROC and AUPRO. In AUPRO, we produce **95.2%**, exceeding the previous SOTA RD4AD by 1.3%. We also present SOTA performances on texture (_Text. Avg._) and object (_Obj. Avg._) categories, respectively.

    & ADTR & RD4AD & CFlowAD & CutPaste & DRAEM & PaDiM & PatchCore & \\  &  &  &  &  &  &  &  & Ours \\  Carpet & 98.7 & 98.9 & 98.7 & 93.9 & 97.0 & **99.8** & 98.7 & **99.8** \\ Grid & 95.0 & **100** & 99.6 & 100 & 99.9 & 96.7 & 98.2 & **100** \\ Leather & 98.1 & **100** & **100** & **100** & **100** & **100** & **100** & **100** \\ Tile & 93.8 & 99.3 & 99.9 & 94.6 & 99.6 & 98.1 & 98.7 & **99.8** \\ Wood & 91.2 & **99.2** & 99.1 & 99.1 & 99.1 & **99.2** & **99.2** & 99.0 \\  _Text. Avg._ & 95.4 & 99.5 & 99.5 & 97.5 & 99.1 & 98.9 & 99.0 & **99.7** \\  Bottle & 98.0 & **100** & **100** & 98.2 & 99.2 & 99.9 & **100** & **100** \\ Cable & 96.8 & 95 & 97.6 & 81.2 & 91.8 & 92.7 & 99.5 & **99.8** \\ Capsule & **99.1** & 96.3 & 97.7 & 98.2 & 98.5 & 91.3 & 98.1 & 97.7 \\ Hazelnut & 98.6 & 99.9 & **100** & 98.3 & **100** & 92.0 & **100** & **100** \\ MetalNut & 97.0 & **100** & 99.3 & 99.9 & 98.7 & 98.7 & **100** & **100** \\ Pill & 98.3 & 96.6 & 96.8 & 94.9 & **98.9** & 93.3 & 96.6 & 98.6 \\ Screw & **99.3** & 97.0 & 91.9 & 88.7 & 93.9 & 85.8 & 98.1 & 98.0 \\ Toothbrush & 98.5 & 99.5 & **100** & 99.4 & **100** & 96.1 & **100** & **100** \\ Transistor & 97.9 & 96.7 & 95.2 & 96.1 & 93.1 & 97.4 & **100** & 99.7 \\ Zipper & 97.2 & 98.5 & 98.5 & 99.9 & **100** & 90.3 & 99.4 & 99.5 \\  _Obj. Avg._ & 98.1 & 98.0 & 97.7 & 95.5 & 97.4 & 93.8 & 99.2 & **99.3** \\  _All Avg._ & 97.2 & 98.5 & 98.3 & 96.2 & 98.0 & 95.4 & 99.1 & **99.5** \\   

Table 1: Anomaly detection performance on MVTec AD, measured in I-AUROC (%).

    & RD4AD  & DRAEM  & SPADE  & PaDiM  & PatchCore  & Ours \\  I-AUROC _Avg._ & 96.0 & 88.7 & 92.1 & 89.1 & 95.1 & **97.5** \\ P-AUROC _Avg._ & 90.1 & 93.5 & 85.6 & 98.1 & **98.8** & 98.2 \\ AUPRO _Avg._ & 70.9 & 72.4 & 65.9 & 85.9 & 91.2 & **92.6** \\   

Table 3: Anomaly detection and segmentation performance on VisA (%).

Anomaly detection and segmentation results on VisA are shown in Table 3 (average of 12 categories). Our approach achieves a superior average I-AUROC of **97.5%**, exceeding previous SOTA RD4AD by 1.5%. In AUPRO, we produce **92.6%**, outperforming previous SOTA PatchCore by 1.4%.

### Multi-Class Anomaly Detection with A Unified Model

Conventional UAD settings train separate models for different object categories. In UniAD , the authors proposed to train a unified model for multiple classes. We evaluate our ReContrast under the unified setting on MVTec AD and VisA. Our model is trained for 5,000 iterations on all images containing all 15 MVTec AD categories (or 12 categories in VisA) and evaluated on each category following . The results are presented in Table 4. On MVTec AD, our method produces an I-AUROC of **98.2%**, exceeding the previous SOTA 96.5% of UniAD by a large **1.7%**. On VisA, both our ReContrast (**95.1%**) and baseline RD4AD (92.7%) outperform the previous multi-class SOTA UniAD (91.5%), while the proposed method exceeds RD4AD by 2.4%.

### Anomaly Detection on Medical Images

The experimental results on three medical image datasets are presented in Table 5. Apart from industrial UAD methods, we also compare our ReContrast with approaches designed for medical images (f-AnoGan , GANomaly , AE-flow ), and contemporaneous methods proposed for better domain adaption ability (CFA , SimpleNet ). We reproduce the methods that were not evaluated on the corresponding dataset if they are open-sourced. Our method achieves the best results in all metrics on APTOS and ISIC2018, and comparable best results on the relatively easy OCT2017, demonstrating that ReContrast can directly adapt to various medical image modalities.

### Ablation Study

To verify the effect of the proposed elements, we conduct thorough ablation studies on MVTec AD and APTOS. Because some configurations are unstable during training on some categories, we report the last iteration's performances and those of the best checkpoint during training (best by I-AUROC, evaluate every 250 iterations). The results are reported in Table 6. The use of \(_{global}\) (_Config.B_) boosts the performance of _Config.A_ trained by \(_{region}\). Directly optimizing the encoder and decoder together (_Config.C_) causes a great degeneration because of pattern collapse, while either stop-gradient (_Config.D_) or contrastive pairs (_Config.C\(+cp\)_) recovers some of it. By introducing both (_Config.E_), the performance is further improved to exceed the frozen encoder baseline. Finally, training with \(_{global-hm}\) yields SOTA performances (Ours). Meanwhile, \(_{global-hm}\) can also improve the baseline reconstruction method alone (_Config.B\(+hm\)_). More ablation studies, experimental results with error bars, limitations, and qualitative visualization are presented in Appendix.

   _MVTec AD_ & RD4AD  & UniAD  & Ours & _VisA_ & RD4AD  & UniAD  & Ours \\  Carpet & 98.3 & **99.8** & 98.3 & candle & 93.6 & 94.6 & **96.3** \\ Grid & **99.0** & 98.2 & 98.9 & capsules & 67.8 & 74.3 & **77.7** \\ Leather & **100** & **100** & **100** & cashew & **94.6** & 92.6 & 94.5 \\ Tile & 98.1 & 99.3 & **99.5** & chewingum & 95.3 & 98.7 & **98.6** \\ Wood & 99.3 & 98.6 & **99.7** & fryum & 94.4 & 90.2 & **97.3** \\ Bottle & 79.9 & 99.7 & **100** & macaron1 & 97.2 & 91.5 & **97.6** \\ Cable & 86.8 & 95.2 & **95.6** & macaron2 & 86.2 & 83.6 & **89.5** \\ Capsule & 96.3 & 86.9 & **97.3** & pcb1 & **96.5** & 94.3 & **96.5** \\ Hazelnut & **100** & 99.8 & **100** & pcb2 & 93.0 & 92.5 & **96.8** \\ MetalNut & 99.8 & 99.2 & **100** & pcb3 & 94.5 & 89.8 & **96.8** \\ Pill & 92.8 & 93.7 & **96.3** & pcb4 & **100** & 99.3 & 99.9 \\ Screw & 96.5 & 87.5 & **97.2** & pip\_fryum & **99.7** & 97.3 & 99.3 \\ Toothbrush & **97.5** & 94.2 & 96.7 & & & & \\ Transistor & 93.3 & **99.8** & 94.5 & & & & \\ Zipper & 98.6 & 95.8 & **99.4** & & & & \\  _All Avg._ & 95.8 & 96.5 & **98.2** & _All Avg._ & 92.7 & 91.5 & **95.1** \\   

Table 4: Anomaly detection performances under unified multi-class setting on MVTec AD and VisA, measured in I-AUROC(%).

## 4 Conclusion

In this study, we propose a novel contrastive learning paradigm, namely ReContrast, for domain-specific unsupervised anomaly detection. It addresses the transfer ability of the pre-trained encoder by jointly optimizing all parameters end-to-end. The key elements of contrastive learning are elegantly embedded in epistemic UAD method to avoid pattern collapse, training instability, and identical shortcut. Extensive experiments on MVTec AD, VisA, and three medical image datasets demonstrate our superiority. The idea of optimizing encoders can further boost the application of UAD methods on more image modalities that are far from natural image domain.

## 5 Limitations

**Scope of Application** In this work, we mainly focus on UAD that detects regional defects (most common in practical applications like industrial inspection and medical disease screening), which is distinguished from one-class classification (OCC, or Semantic AD). In our UAD, normal and anomalous samples are semantically the same objects except local detects, e.g. good cable v.s. spoiled cable. In OCC, normal samples and anomalous samples are semantically different, e.g. cat v.s. other animals. More details are discussed in Appendix E.

**Training Instability** Our method still suffers some extent of training instability. Because of the absence of validation sets in UAD settings, whether the last epoch (for reporting results) is in the middle of a loss spike and performance dip is related to random seeds. We found that a number of UAD methods (RD4AD, CFA, SimpleNet) are also subject to training instability when running their code. We discuss the reason and solutions to mitigate this effect in Appendix E.

    & _{global}\)} & optim. & stop & contrast & hard &  &  \\   & & encoder & grad. & pairs & mining & & I-AUROC & AUPRO & I-AUROC \\  _Config.A_ & & & & & & & 95.31/97.55 & 93.34/94.05 & 90.12/90.50 \\ _Config.B_ & ✓ & & & & & 98.86/99.07 & 94.51/94.59 & 92.49/93.62 \\ _Config.B_\(+hm\) & ✓ & & & & ✓ & 99.00/99.12 & 94.86/94.91 & 93.87/94.36 \\ _Config.C_ & ✓ & ✓ & & & & 91.54/95.96 & 88.24/92.14 & 90.71/91.06 \\ _Config.D_ & ✓ & ✓ & ✓ & & & 94.64/97.07 & 84.11/87.83 & 93.06/95.66 \\ _Config.C_\(+cp\) & ✓ & ✓ & & ✓ & & 97.59/97.88 & 93.76/93.92 & 92.68/92.68 \\ _Config.E\(-gl\)_ & & ✓ & ✓ & ✓ & & 98.93/99.26 & 94.65/94.71 & 96.39/96.39 \\ _Config.E_ & ✓ & ✓ & ✓ & ✓ & & 99.13/99.34 & 94.59/94.60 & 97.32/97.43 \\ Ours & ✓ & ✓ & ✓ & ✓ & ✓ & **99.45/99.52** & **95.20/95.29** & **97.51/97.51** \\   

Table 6: Ablation study on MVTec AD and APTOS. Reported in last/best