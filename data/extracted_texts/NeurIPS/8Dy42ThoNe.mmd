# Large Language Model Unlearning

Yuanshun Yao

Meta GenAI

kevinyao@meta.com

&Xiaojun Xu

ByteDance Research

xiaojun.xu@bytedance.com

&YangLiu\({}^{*}\)

UC Santa Cruz

yangliu@ucsc.edu

Work done while at ByteDance Research.

###### Abstract

We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in the standard alignment process. (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.

## 1 Introduction

Making sure large language models (LLMs) generate safe outputs that align with human values and policy regulation is currently a major task for LLM practitioners. The common tasks include: (1) removing harmful responses , (2) erasing copyrighted contents , (3) reducing hallucinations, (4) removing a user's data from the trained LLMs after they stop giving consents, (5) quickly re-enforcing compliance  after policy updates.

Though those tasks seem different, the central technical question is identical: How to quickly remove the impact of certain training samples on LLMs? To this end, we study how to perform large language model unlearning. If an LLM learns unwanted (mis)behaviors in its pretraining stage, we aim to unlearn them with samples that represent those problematic behaviors, i.e. _with only negative samples_.

The benefits of LLM unlearning include: (1) It only requires negative examples that we want the LLM to forget, which are cheaper and easier to collect through user reporting or red teaming than positive examples (that are required in the standard RLHF). In addition, discovering negative examples is highly automatable given the pretrained (unaligned) LLM. (2) It is computationally efficient; the cost is similar to finetuning LLMs. (3) Unlearning is particularly efficient in removing unwanted behaviors if practitioners already know which training samples cause them. Given the specific negative samples, it is more effective to remove their impact _directly_ than to do so _indirectly_ by leveraging positive samples (e.g. in RLHF) - if the goal is to _not_ generate undesirable outputs, e.g. generating _non-harmful_ outputs (e.g. nonsensical strings or responses unrelated to prompts) rather than helpful outputs. If we only have limited resources, unlearning provides a promising alternative to RLHF to align LLMs when the first priority is to stop LLMs from generating undesirable outputs since undesirable outputs often cause far more damage than what can be offset by the benefits of desirable outputs.

In this work, we show three successful examples of LLM unlearning: (1) After the LLM learns harmful behaviors from its training data, we want it to stop generating harmful responses. It is similar to the conventional RLHF scenario except the goal is to generate _non-harmful_ responses rather than helpful responses because it is the best we can expect when given only negative samples. (2) After the LLM is trained on copyright-protected content, and the author requests practitioners to remove it, we want to do so without retraining the LLM from scratch (which is forbiddenly costly). (3) If the LLM learns wrong facts in its training data, i.e. "hallucination," we want the LLM to forget them.

Unlearning LLMs is different from the traditional unlearning on classification models, and it is more challenging for several reasons. (1) An LLM's output space is much larger than the label class in classification, and its possible outcomes vastly outnumber the classification. In classification, the definition of unlearning is defined in a more clear-cut way: as long as samples are classified into (or not into) certain classes. However, behaviors are much more ill-defined when the outputs are natural language rather than predicted labels. (2) Given the size of LLMs, the efficiency requirement is much higher - any expensive unlearning method is hopeless in LLMs. (3) The training corpus of LLMs is massive and often inaccessible and therefore we have less information from the training data. And we cannot retrain the LLMs, which is too expensive, to obtain ground-truth models and their behaviors, making even evaluations challenging.

To the best of our knowledge, our work is among the first ones to investigate how to perform unlearning on LLMs, as well as to formulate the settings, goals, and evaluations in LLM unlearning. Our results suggest this is a promising direction for aligning LLMs with limited resources. We show that despite only having negative samples, our unlearning algorithm can still achieve better alignment performance than RLHF with only 2% of its computational time.

We hope our work can bring more attention to using unlearning as an alternative to RLHF as the alignment technique, especially when given limited resources and only negative samples, and the first priority is to put an immediate stop to generating undesirable outputs.

### Related Work

LLM unlearning is a largely under-explored topic but machine unlearning has arisen as a promising solution to teach a classification model to forget specific training data [3; 2; 46]. Due to the high computational cost, most of the existing works have focused on developing approximate unlearning algorithms for classification models, including data-reversed training [39; 24; 8], optimization-based unlearning [14; 31] and influence function based approaches [19; 45; 17]. For example, a typical optimization-based techinque  is gradient ascent (GA). Given a dataset \(D=\{(x_{i},y_{i})\}_{i=1}^{N}\) and a loss function \((h_{}(x),y)\) where the model is parametrized by \(\), the GA algorithm iteratively updates the model:

\[_{t+1}_{t}+_{_{t}}(h_{}(x),y),(x,y) D\] (1)

where \(\) is the (un)learning rate. It reverts the change of the gradient descent during the training with its opposite operation.

Due to the size of the parameters and training data, a large portion of existing unlearning methods would not fit to unlearn an LLM, including those use efficient retraining [2; 24] (which is now likely to be insufficient for LLMs) and the ones that involve the computation of influence functions (which requires the computation of the inverse Hessian matrix defined on the model parameter space).

The relevant work is aligning the LLMs with human values. The current mainstream approach is RLHF (reinforcement learning from human feedback, and its variants) [32; 1; 7; 47]. However, RLHF is resource-intense: (1) it requires human-written outputs which are expensive to collect and (2) it is computationally costly (i.e. the standard three-stage aligning procedure). In this work, we propose unlearning as an alternative aligning method. Collecting negative (i.e. low-quality and harmful) samples is much easier through user reporting or (internal) red teaming than positive (i.e. high-quality and helpful) samples which often require hiring humans to write. Therefore, aligning LLMs _with only negative examples_ is appealing.

Several concurrent works to our work also study unlearning in LLMs.  unlearn answers related to Harry Potter by finetuning based on the difference between the model trained on Harry Potter data and the counterfactual outputs as if the Harry Potter data were not used. However, this approach might lead to incorrect (i.e. hallucinated) answers, e.g. when being asked who Harry Potter is, the 

[MISSING_PAGE_EMPTY:3]

on normal prompts should remain as close as possible to the original LLM \(^{o}\). (4) **Low cost**: We aim for a low-computational-cost approach that does not require a procedure with similar costs to retraining.

**Remark.** In our setting, unlike, for example, RLHF, we assume we do not have access to positive samples (helpful, high-quality, and often human-written outputs). In other words, given an undesirable (e.g. harmful) prompt \(x^{}\), we do not know its corresponding desirable (e.g. helpful) output. Nor do we assume we have any external models to generate desirable outputs. Under this assumption, we have no information about what a desirable output would look like. Therefore, the best we can achieve is to make LLMs stop outputting undesirable answers. For example, when unlearning harmfulness, our goal is to output non-harmful answers (e.g. answers unrelated to the harmful prompts or nonsensical strings) rather than helpful answers (e.g. declining to answer the question or outputting correct answers). Similarly, when unlearning copyrighted content, our goal is to output what is unrelated to copyrighted data, which could be non-readable strings, rather than providing more polite responses.

## 3 Method

We mainly follow the approach of gradient ascent (GA). We include the discussion of this design in Appendix A. At each training step \(t\), we use \(_{t}\) to denote the current LLM we obtained through the unlearning. The update in our unlearning approach is summarized by:

\[_{t+1}_{t}-_{ _{t}}_{}}_{}- _{_{t}}_{}}_{}- _{_{t}}_{}}_{ }\] (2)

where \(_{i} 0\) are hyperparameters to weigh different losses. \(_{},_{},_{}\) are three loss functions we introduce below.

Let \(h_{}(x,y_{<i}):=(y_{i}|(x,y_{<i});)\) be the predicted probability of the token \(y_{i}\) by an LLM \(\) conditioned on the prompt \(x\) and the already generated tokens \(y_{<i}:=[y_{1},...,y_{i-1}]\).3 For a prompt-output pair \((x,y)\) and LLM \(\), the loss on \(y\) is:

\[L(x,y;):=_{i=1}^{|y|}(h_{}(x,y_{<i}),y_{i})\] (3)

where \((.)\) is the cross-entropy loss.

Denote by \(^{}\) a set of random (e.g. non-harmful) responses that have no connection to the unlearned prompts \(x^{}\) - it can be constructed by collecting the irrelevant responses from the normal dataset. We then have the three losses in Eqn(2) defined as:

\[_{}:=-_{(x^{},y^{}) D^{ }}L(x^{},y^{};_{t})\] (4)

\[_{}:=_{(x^{},) D^{}} ^{}|}_{y^{}^{ {rdh}}}L(x^{},y^{};_{t})\] (5)

\[_{}:=_{(x^{},y^{}) D^{}}_{i=1}^{|y^{}|}h_{^{o}}(x^{},y^{ }_{<i})||h_{_{t}}(x^{},y^{}_{<i})\] (6)

where \((.)\) is the KL divergence term.

We explain each loss. Eqn(4) is the gradient ascent (GA) loss to forget the unlearned samples. Note we compute it on \(y^{}\) only, as indicated in Eqn(3). Eqn(5) forces the LLM to predict a random output \(y^{}\) on the unlearned \(x^{}\). This term reinforces the forgetting of prompt \(x^{}\) by adding irrelevance into the predicted outcome, with the similar insight of label smoothing  in classification. Eqn(6) is to preserve the normal utility by comparing it with the original LLM (Key Difference 2). Note that we use _forward KL_ (which is typically used in supervised learning) instead of reverse KL (whichis typically used in sampling, e.g. RLHF) because it forces the distribution of the unlearned model to cover all the areas of space of the original LLM .

We highlight two designs in our method. (1) We find that performing gradient ascent or decent on the output (i.e. \(y\)) part only is much more effective than on both prompt and output (i.e. \((x,y)\)). In other words, the loss should be only computed on the tokens in \(y\) conditioned on \(x\), excluding the tokens in \(x\), i.e. Eqn(3). (2) Adding \(_{}\) has two advantages. _First_, it helps the LLM forget the learned undesirable outputs on \(x^{}\) by forcing it to predict random outputs. _Second_, it can stabilize the unlearning performance when the gradient on \((x,y)\) is small. We include the detailed explanation in Appendix B.

We perform a series of empirical studies that highlight the difference between unlearning on traditional models and LLMs in Appendix C. We incorporate three key lessons. (1) We continue to unlearn after we have observed the loss on forgetting samples raises to an abnormally high level, for 3x-5x more batches. (2) To preserve normal utility, we minimize the KL divergence on predicted distribution on \(x^{}\) between the original and the unlearned LLM, i.e. Eqn(6). (3) We choose \(D^{}\) to be the same format as \(D^{}\), e.g. to unlearn the harmful data from PKU-SafeRLHF which is in the format of Q&A, we use TruthfulQA as the normal data.

## 4 Applications

We include three applications of unlearning: (1) Unlearning the harmfulness of outputs responding to harmful prompts, (2) Unlearning copyright-protected contents requested by creators after they have been trained into LLMs, and (3) Reducing hallucinated outputs. In addition, we also compare our method to RLHF.

### Evaluation Design

Broadly speaking, our evaluation metrics fall into two categories: (1) performance on the unlearned samples and (2) utility on the remaining samples.

**Unlearning Performance:** Since we want the effectiveness of unlearning to generalize to unseen samples rather than just unlearned samples, we need to test both unlearned and unseen prompts that would cause misbehavior. We measure the following metrics on the outputs generated given both unlearned prompts that cause unwanted misbehaviors on LLMs as well as unseen prompts that are similar to the exactly unlearned prompts.45

* **Unlearning Efficacy**: It measures the effectiveness of the unlearning algorithm. It is context-dependent. For example, in terms of unlearning harmfulness, it means, after unlearning, the decrease in the harmfulness of the outputs responding to harmful prompts. In terms of unlearning copyrighted data, it means a decrease in leaked copyrighted information when prompting maliciously to extract it.
* **Diversity**: It measures the diversity of outputs, i.e. the percentage of the unique tokens in the text. A high diversity score indicates the unlearned LLM generates non-trivial, informative, and helpful outputs.
* **Fluency**: Following the prior work , we use fluency (the perplexity of generated text tested on a reference LLM) to measure the quality of outputs. A low perplexity score indicates the unlearned LLM generates reasonable outputs. Note that it is only meaningful when the diversity is not extremely low. We find the unlearned LLMs frequently output a sequence of repeated single characters, i.e. with unreasonably low diversity. In this case, fluency has no meaning. Later,when we find more than 80% of the generated text is merely a repetition of a single character, we simply label its Fluency as "NM" (Not Meaningful).

**Utility Preservation:** In terms of evaluating outputs on normal prompts, unfortunately, retraining LLMs is prohibitively expensive, and therefore the conventional metrics in the literature based on the retrained model are not applicable. We assume unlearning the samples that we hope to forget would not impact the outputs on the normal samples, and use the original LLM rather than retrained LLM as ground-truth.

We measure the utility on normal prompts, i.e. prompts come from a different distribution compared to unlearned prompts. For example, in terms of unlearning harmfulness, the normal prompts are normal questions (e.g. factual questions) rather than harmful questions. In terms of unlearning copyrighted data, normal prompts are to seek information about non-copyrighted content.

* **Reward Model**: We use reward models to measure the quality of the generated outputs on the normal prompts. The goal is to make the reward of the unlearned LLM's outputs on the normal prompts remain similar to the original LLM.
* **Output Similarity**: We measure the similarity of the outputs on the normal prompts between the original and the unlearned LLM. We use BLEURT  as the metric.

### Application: Unlearning Harmfulness

The setting is similar to RLHF, except we are only given negative samples. In addition, unlike traditional unlearning, the unlearned samples do not have to belong to the LLM's training set.

**Dataset and Model.** We use harmful Q&A pairs in PKU-SafeRLHF  dataset as \(D^{}\) and TruthfulQA  dataset as \(D^{}\). We further split \(D^{}\), according to the PKU original dataset's train/test split, into the harmful samples we unlearn and the unseen harmful samples for evaluation. We use three models: OPT-1.3B, OPT-2.7B  and Llama2-7B  as the original LLM to perform the unlearning algorithm.

**Setting.** We use the baseline that finetunes LLM on the remaining data, which we choose BookCorpus , one of the OPT model's training data. In our method, we test plain GA, i.e. \(_{2}=0\) in Eqn(3), and GA with random mismatch. We use harmful rate flagged by the PKU moderation model 6 as the unlearning efficacy. We evaluate the utility rewards by _deberta-v3-large-v2_ reward model7_ on answers to TruthfulQA questions. We include detailed experimental settings in Appendix D.1 and generated samples in Appendix E.1.

In terms of the test set, we sample 200 prompts for unlearned harmful prompts, unseen harmful prompts, and normal prompts. For Fluency, we use the original LLM as the reference model. To compute Output Similarity on a given normal prompt, we sample 3 outputs from the test LLM and 3 outputs from the original LLM, and we report the maximum pairwise BLEURT score between them.

**Results.** Table 1 shows our results. We summarize the findings. (1) Both GA and GA+Mismatch can significantly reduce the harmful rate, achieving near-zero harmful rates. The outputs are mostly just whitespaces or nonsensical strings (see Appendix E.1 for examples). We stress again that given no helpful responses, generating nonsensical but non-harmful answers is what we expect; it is the best we can do given the absence of how helpful text looks like. (2) Both GA and GA+Mismatch generalize well to unseen harmful prompts, showing the unlearned LLMs indeed forget the concept of harmful behaviors, not merely individual unlearned samples. (3) Both GA and GA+Mismatch's outputs on the normal prompts remain at a similar level of utility compared to the original model8 and are close to the original model's outputs.

### Application: Unlearning Copyrighted Contents

Unlike unlearning harmfulness in Section 4.2, in this scenario, the unlearned samples belong exactly to the LLM's training set. The scenario is once an LLM is trained on a copyright-protected corpus, and the author requests the practitioners to remove it, we study how we can do so without retraining the LLM from scratch.

Dataset and Model.We use _Harry Potter and the Sorecer's Stone_ as the copyright corpus,9 HP data in short. We first finetune the pretrained LLMs on the HP data to make sure the fact that they are actually trained on the copyrighted HP data. They then serve as our original LLMs. We then split the HP data into the unlearned set and the test set. We use BookCorpus  as the normal dataset \(D^{}\) since it is also book text which is in the same format as \(D^{}\) (Key Difference 3 in Section 3.2). We test the same three LLMs in Section 4.2.

Setting.The LLM task in this application is text completion. We largely follow the setting from . Each prompt starts with the beginning of a sentence in the HP corpus, continuing for the next 200 characters as the prompt text (therefore an attempt to extract the copyrighted text). Given a prompt, we can test how much copyrighted information is leaked by comparing the LLM's completion (with greedy sampling, i.e. setting temperature to 0) to the ground-truth HP text. We set the comparison length to 200 characters and use BLEU score  as the text similarity metric.

For a prompt, i.e. an extraction attempt, we judge the copyright information is leaked if its completion's BLEU score is above a threshold.10 We choose the threshold by randomly sampling 100K sentences in the HP corpus, computing their average BLEU score, and using 10% of it as the threshold. We report the leak rate, i.e. the percentage of extraction prompts that lead to the leakage as the unlearning effectiveness measure. We use BookCorpus as the data for the baseline of fine-tuning. We sample 100 prompts from the unlearned samples, unseen HP samples (HP text trained into the LLM but not unlearned), and normal samples (BookCorpus as the normal completion test set) respectively. We include the hyperparameter setting in Appendix D.2 and generated samples in Appendix E.2.

**Results.** Table 2 reports the results. We summarize the findings. (1) Both GA and GA+Mismatch can reduce the leak rate on the unlearned extraction attempts to nearly zero, showing the effectiveness of our unlearning algorithm in removing copyrighted content.11 The completed text is mostly a repetition of a single character; such nonsensical output is expected in our setting given we have no

   &  &  &  \\   & &  &  &  &  &  \\   & &  &  &  &  &  \\   & &  &  &  &  &  &  \\  & &  &  &  &  &  &  &  &  &  &  \\   & Original & 47\% & 0.787 & 2.655 & 53\% & 0.804 & 2.723 & 3.599 & -0.778 \\  & Finetuning & 34.5\% & 0.582 & 2.687 & 34.5\% & 0.584 & 2.753 & -5.260 & -1.136 \\  & GA & **1\%** & 0.118 & NM & **3\%** & 0.101 & NM & -3.538 & -1.034 \\  & GA+Mismatch & **6\%** & **0.832** & **1.509** & **7\%** & **0.818** & **1.564** & **-2.982** & **-0.943** \\   & Original & 52.5\% & 0.823 & 2.720 & 52.5\% & 0.809 & 2.742 & -3.610 & -0.825 \\  & Finetuning & 15\% & **0.572** & **3.799** & 16\% & **0.570** & **3.792** & -5.408 & -1.466 \\  & GA & **1.5\%** & 0.206 & NM & **4\%** & 0.271 & NM & -3.281 & **-1.004** \\  & GA+Mismatch & 3\% & 0.275 & NM & **4\%** & 0.218 & NM & **-2.959** & -1.164 \\   & Original & 54\% & 0.355 & 0.799 & 51.5\% & 0.358 & 0.796 & -3.338 & -0.421 \\  & Finetuning & 51\% & 0.394 & **0.801** & 52.5\% & 0.397 & **0.820** & **-2.936** & **-0.436** \\   & GA & 2\% & **0.953** & 1.288 & **1\%** & **0.955** & 1.303 & -4.252 & -0.689 \\   & GA+Mismatch & **1\%** & 0.240 & NM & 3\% & 0.167 & NM & -3.438 & -1.319 \\  

Table 1: Experimental results on **unlearning harmfulness**. NM = “Not Meaningful”. GA and GA+Mismatch can achieve near zero harmful rates and generalize to unseen harmful prompts.

positive examples that show what a good completion should be. (2) Both GA and GA+Mismatch can generalize to unseen extraction attempts, showing unlearned LLM can distinguish copyright-related prompts from other prompts. (3) Both GA and GA+Mismatch achieve a similar utility on the normal completion task compared to the original LLM.

### Application: Reducing Hallucination

If an LLM outputs factually wrong answers (i.e. hallucinations) given fact-related questions, the goal is to make the LLM unlearn wrong answers. Similar to unlearning harmfulness in Section 4.2, we do not assume the unlearned (i.e. hallucinated) Q&A samples (which are wrong answers given the questions) exist in the LLM's training set.

It is easy to imagine LLM can forget the wrong answer to the exact unlearned prompts. But it seems hard to generalize to unseen prompts since each individual factual question is different and highly specific and unlearning wrong answers to a specific question seems unlikely to impact answers to other questions. However, we do not aim to give factually correct answers but rather not give factually wrong answers. Therefore, all the LLM needs to do is to learn to classify which questions to respond (i.e. normal questions) and which do not (i.e. similar questions to the unlearned ones) by learning the distribution difference between questions.

**Dataset and Model.** We select the hallucinated Q&A pairs (i.e. negative samples) in the HaluEval  dataset as \(D^{}\) and TruthfulQA  dataset as \(D^{}\). We split \(D^{}\) into 70% for training, 10% for validation, and 20% for testing. Note that there exists a distribution shift between HaluEval data and TruthfulQA data. The questions in HaluEval are intentionally misleading; the questions in TruthfulQA are benignly straightforward. Therefore, this difference allows the unlearned LLM to distinguish between those two types of questions and therefore give different answers accordingly. In other words, the test (not unlearned) questions from HaluEval are in-distributional in terms of unlearning while the questions from the normal TruthfulQA data are out-of-distributional. Regarding models, we use the same three LLMs in Section 4.2.

**Setting.** To evaluate the effectiveness of reducing hallucination, we define the hallucination rate. Given the LLM's output, we compute its text similarity to the hallucinated answer and the correct answer. We choose BERTscore  as the text similarity because it is insensitive to text length and there is a significant length difference between hallucinated and correct answers. We decide an answer is hallucinated if its similarity to the hallucinated answer is 10% higher than the similarity to the correct answer. The hallucination rate is the percentage of test samples with hallucinated answers given by the LLM. The rest of the setting is similar to Section 4.2. We include the hyperparameter setting in Appendix D.3 and generated samples in Appendix E.3.

**Results.** Table 3 shows the results. The observations are largely similar to the previous applications. (1) Both GA and GA+Mismatch can significantly reduce the hallucination rate on the unlearned questions. (2) Both GA and GA+Mismatch can generalize de-hallucinating to the in-distributional questions from the same dataset used in unlearning. (3) Both GA and GA+Mismatch can distinguish

between in-distributional and out-of-distributional questions. They remove hallucinations when responding to in-distributional questions w.r.t unlearned questions and maintain similar answers as the original LLM when responding to out-of-distributional questions. (4) Compared to the previous two applications, the hallucination rate is not at a similarly low level (\( 10\%\)), which shows unlearning hallucination is a harder task. We think the goal is to _reduce_ in-distributional hallucination rather than completely unlearning general hallucination.

### Ablation Studies

**Comparing to RLHF.** We compare our unlearning algorithm to the standard RLHF. However, note that in this case we already assume RLHF has access to the expensively collected positive samples (as well as negative samples) while our algorithm only has negative samples. Therefore, the comparison has already put our method in a disadvantaged position. Nevertheless, we still show that our method can achieve better alignment performance with only a fraction of computational cost despite that we only have negative samples.

Using unlearning harmfulness as an example, we perform RLHF on PKU-SafeRLHF data. The LLM is OPT-1.3B and the hyperparameters in RLHF are mostly default. We run both SFT (supervised fine-tuning) and full RLHF pipeline (SFT + reward model training + Proximal Policy Optimization ). We report the run time on a single NVIDIA A100 SXM4 80 GB GPU in Figure 1. Our unlearning algorithm only needs about 2% of the time required for the full RLHF pipeline, with a comparable cost to mere finetuning.

Table 4 shows the evaluation results compared to RLHF. Unlearning can achieve a lower harmful rate compared to the full RLHF, and a far lower harmful rate than SFT. This result is worth highlighting given we do not even use positive samples and with only 2% of the computational time. It shows that only using negative samples with unlearning can achieve a surprisingly promising _non-harmful_ rate, which is the goal in our setting. Therefore, if the goal is to stop outputing undesirable responses rather than to output helpful responses, our results show unlearning might be a more appealing approach than RLHF.

   &  &  &  &  \\   & &  &  &  &  &  &  &  \\   & &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  \\   & Original & 47\% & 0.787 & 2.655 & 53\% & 0.804 & 2.723 & -3.599 & -0.778 \\  & Finetuning & 34.5\% & 0.582 & 2.687 & 34.5\% & 0.584 & 2.753 & -5.260 & -1.136 \\  & SFT & 34\% & 0.801 & 2.938 & 38\% & 0.807 & 3.009 & **-2.916** & **-0.639** \\  & Full RLHF & 4\% & **0.868** & 3.414 & 7.5\% & **0.876** & 3.502 & -3.212 & -0.834 \\  & GA & **1\%** & 0.118 & NM & **3\%** & 0.101 & NM & -3.838 & -1.034 \\  & GA+Mismatch & 6\% & 0.832 & **1.509** & 7\% & 0.818 & **1.564** & -2.982 & -0.943 \\  

Table 4: Comparison to RLHF in the application of unlearning harmfulness on OPT-1.3B with PKU-SafeRLHF data. NM = “Not Meaningful”. Despite that we only have negative samples without the expensively collected and human-written positive samples, our unlearning algorithm can still achieve better alignment performance with only 2% of the computational time.

   &  &  &  &  &  \\  & &  &  &  \\   & &  & Diversity &  &  &  &  &  &  \\  & &  &  &  &  &  &  &  \\   & Original & 58.5\% & 0.852 & 3.020 & 60\% & 0.836 & 3.052 & -3.604 & -0.806 \\  & Finetuning & 48\% & **0.559** & **3.123** & 46\% & **0.569** & **3.148** & -5.697 & -1.386 \\  & GA & **11\%** & 0.015 & **NM** & 9\%** & 0.012 & NM & **-3.917** & -1.333 \\  & GA+Mismatch & 15\% & 0.033 & NM & 10.5\% & 0.132 & NM & -3.958 & **-0.940** \\   & Original & 60\% & 0.846 & 3.120 & 55\% & 0.838 & 3.088 & 3.630 & -0.855 \\  & Fineetuning & 48\% & **0.604** & **3.198** & 43.5\% & **0.587** & **3.136** & -5.700 & -1.354 \\  & GA & **10.5\%** & 0.001 & NM & **9\%** & 0.014 & NM & **-3.324** & -1.050 \\  & GA+Mismatch & 12.5\% & 0.058 & NM & 12.5\% & 0.059 & NM & -3.473 & **-0.830** \\   & Original & 49.5\% & 0.435 & 1.046 & 45.5\% & 0.473 & 1.128 & -3.467 & -0.430 \\  & Finetuning & 48\% & **0.466** & **1.040** & 43.5\% & **0.475** & **1.045** & -3.144 & -0.731 \\  & GA & 13\% & 0.035 & NM & **8.5\%** & 0.012 & NM & -2.579 & **-0.505** \\   & GA+Mismatch & **11.5\%** & 0.009 & NM & **8.5\%** & 0.005 & NM & **-2.100** & -0.620 \\  

Table 3: Experimental results on **reducing hallucinations**. NM = “Not Meaningful”. Both GA and GA+Mismatch can significantly reduce the hallucination rate and distinguish between in-distributional and out-of-distributional questions.

**Templated Outputs.** If practitioners do not want the unlearned LLM to generate nonsensical outputs (e.g. whitespace) on harmful prompts, we can replace the random output \(y^{}\) in Eqn(5) with templated outputs, e.g. "I can't assist it." In other words, we force the LLM to generate the templated answers on the unlearned prompt.

We follow the setting of unlearning harmfulness in Section 4.2. We replace the random output \(y^{}\) in Eqn(5) with the templated answer "I can't assist it." and keep other settings the same except we re-tune loss weight \(_{1}\), \(_{2}\), and \(_{3}\) in Eqn(2). Table 5 shows the comparison with the previous results on OPT-1.3B. GA+Template achieves a similar level of unlearning performance compared to GA and GA+Mismatch. Overall, using templated answers instead of random answers does not show significant differences.

Table 38 in Appendix E.4 shows generated examples compared to GA and GA+Mismatch. We observe that if the unlearned LLM has learned to respond differently to the harmful prompts, we can easily make it output templated responses instead of nonsensical strings. In addition, it is easy to enable templated answers without changing unlearning optimization: we can check if the outputted text is a nonsensical string and replace it with templated strings as a post-processing heuristic.

Finally, an even simpler heuristic for generating templated outputs is to check if the outputted text is a nonsensical string and replace it with templated strings.

## 5 Conclusion

We explore unlearning in LLMs, as well as its formal setups, goals, and evaluations. Our results show that unlearning is a promising approach to aligning LLMs to stop generating undesirable outputs, especially when practitioners do not have enough resources to apply other alignment techniques such as RLHF. We present three scenarios in which unlearning can successfully remove harmful responses, erase copyrighted content, and reduce hallucinations. Our ablation study shows that despite only having negative samples, unlearning can still achieve better alignment performance than RLHF with only a fraction of its computational time. One limitation of our approach is it might induce refusal responses on normal prompts.