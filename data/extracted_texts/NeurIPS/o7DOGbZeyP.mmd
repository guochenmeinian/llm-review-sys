# LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate

Anthony Fuller, Daniel G. Kyrollos, Yousef Yassin, James R. Green

Department of Systems and Computer Engineering

Carleton University

Ottawa, Ontario, Canada

anthony.fuller@carleton.ca

AF, DGK, and YY made significant technical contributions. AF and DGK initiated the project. AF and JRG led the project. Code and data are available at: https://github.com/GreenCUBIC/lookhere

###### Abstract

High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning -- ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating.

We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using \(2\)D attention masks. Our novel method, called LookHere, provides translation-equivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg.\( 1.6\%\)), against adversarial attack (avg.\( 5.4\%\)), and decreases calibration error (avg.\( 1.5\%\)) -- on ImageNet _without_ extrapolation. _With_ extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by \(21.7\%\) on ImageNet when trained at \(224^{2}\) px and tested at \(1024^{2}\) px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR.

## 1 Introduction

There is a decades-long trend in computer vision towards higher-resolution imagery, which contains more detailed scene information. Increasing resolution is a reliable way to improve model accuracy , but this comes at a cost; training models for hundreds of epochs on large-scale datasets is expensive, especially at high-resolutions. There are two ways to reduce this cost and still see accuracy benefits from high-resolutions: \(\) high-resolution finetuning, which pretrains models at a lower resolution, like \(224^{2}\) px, then finetunes them at a higher resolution, like \(384^{2}\) px; and \(\) extrapolating, which deploys models at a higher resolution, without further training. Of these two options, we should aim for models that can _effectively extrapolate_, as it presents a zero-cost solution that does not require finetuning at every target resolution. Finetuning costs aside, improvements to extrapolation should benefit high-resolution finetuning since models that are better at extrapolating can adapt to higher resolutions more easily. Although extrapolation is a significant and exciting challenge, state-of-the-art (SoTA) model architectures extrapolate poorly.

Vision transformers (ViTs ) offer SoTA performance on many computer vision tasks. ViTs are simple; they split images into non-overlapping patches, linearly project pixels to form patch embeddings, and process these "tokens" with a stack of architecturally identical transformer layers -- maintaining a constant feature map size throughout. This non-hierarchical design enables learning _patch_ representations, which are useful for dense prediction tasks  and are fundamentalfor vision-language models [24; 25; 26]. The design enables efficient processing of only a subset of patches, known as token dropping [27; 28]. Lastly, it enables model scaling by increasing the embedding size and the layer count [29; 30].

Image-size extrapolation with ViTs can be achieved in three ways: }}}}}} increasing the patch size, which packs more pixels into each patch embedding; }}}}} increasing the "patchification" stride, which skips-over pixels; and }}}}} increasing the number of patches. Of these three options, we should aim for models that can effectively ingest more patches -- called "sequence length extrapolation" in the natural language processing (NLP) community  -- as a greater number of patches presents models with more (uncompressed) information that we hope to leverage into higher accuracy. Furthermore, methods that improve sequence length extrapolation, like our proposed method, can be fused with methods that adjust patch sizes, like FlexiViT . We strongly believe that patch _position encoding_ is a primary cause of the poor sequence length extrapolation ability of ViTs -- like it is in NLP, where significant advancements have been made by improving position encoding [31; 33; 34; 35].

Adding learnable or fixed sinusoidal position embeddings to patch embeddings before the first layer is the most common way ViTs encode positions. Recently, the rotary position embeddings (RoPE ) used in SoTA language models [37; 38] were extended to ViTs, as 2D-RoPE , showing exciting results. RoPE is a different approach to position encoding that injects positional information in each self-attention layer by rotating queries and keys with fixed sinusoidal embeddings. But for these methods to ingest more patches at test time, they must either introduce new position embeddings or modify existing embeddings -- both options create a significant distribution shift. Motivated by these observations and more, we make the following contributions:

}}}}} LookHere -- We introduce a novel position encoding method for plain ViTs that restricts attention heads to fixed fields of view (FOV) and points them in different directions via 2D masks. This design provides: }}}}} translation-equivariance, }}}}} attention head diversity, }}}}} improved interpretability, and }}}}} limits the distribution shift that attention heads face when extrapolating.

}}}}} Controlled Experiments -- We perform an apples-to-apples comparison between _seven_ position encoding methods for plain ViTs alongside our three LookHere variants. We demonstrate that LookHere: }}}}} improves classification, segmentation, adversarial robustness, and model calibration

Figure 1: ViT-B/\(16\) models trained for \(150\) epochs on ImageNet at \(224^{2}\) px and tested up to \(1024^{2}\) px. Model architectures are consistent between runs other than _position encoding_ methods. We perform an \(8\)-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at \(1024^{2}\).

when tested _at_ the training resolution; \(\) significantly improves performance when tested _beyond_ the training resolution; and \(\) increases its performance advantage after high-resolution finetuning.

\(\)Extrapolation Insights -- We show that extrapolation: \(\) benefits images with small objects the most, as they occupy more patches at test time; \(\) produces class-level and dataset-level effects; and \(\) creates distribution shifts that can be visualized via attention maps.

\(\)ImageNet-HR -- We introduce the first natively high-resolution ImageNet test set (\(1024^{2}\) px) aimed to benchmark classifiers on images that were not upsampled to achieve the target image size.

## 2 Background and Related Work

A ViT splits an image into a grid of non-overlapping patches, flattens the grid into a sequence, and flattens the patches into vectors; i.e., \(^{Y X C}^{N_{y} N_{x} P ^{2} C}^{(N_{y} N_{x})(P^{2} C)}\), where \(Y\) is the image-height, \(X\) is the image-width, \(C\) is the number of channels, \(N_{y}\) is the grid-height, \(N_{x}\) is the grid-width, \(P\) is the patch height and width. A linear layer maps each vector of pixels to a patch embedding; i.e., \(^{P^{2} C} E_{i}^{patch}^{D}\), where \(D\) is the embedding dimension also known as the transformer width. We define \(i\) and \((i_{y},i_{x})\) as the sequence position and the \(2\)D position of the \(i\)th patch, respectively, where \(N\) is the total number of patches, equal to \(N_{y} N_{x}\), \(i\{1,2,,N\}\), \(i_{y}\{1,2,,N_{y}\}\), and \(i_{x}\{1,2,,N_{x}\}\). Finally, sequence length extrapolation occurs when \(N_{test}>N_{train}\).

A patch embedding represents the _content_ of a patch, and contains no information representing its original location within the image. Thus, we must encode patch positions to enable spatial reasoning; otherwise, a ViT will operate on a bag of patches.

We define a "plain ViT" as attention-only and non-hierarchical. Our primary goal is to improve the extrapolation ability -- i.e., generalize to more patches at test time -- of plain ViTs. Our work is motivationally aligned with FlexiViT  and NaViT , improving the flexibility of plain ViTs. Next, we briefly describe seven position encoding methods and refer the reader to the cited studies for further details; we include them _all_ in our controlled experiments. Another method, iRPE , is also compatible with plain ViTs. However, we exclude it because it is more than twice as slow as other methods; nonetheless, we benchmark iRPE with our best training recipe in Appendix A.2.1.

**Input Embeddings.** This group leverages learned or fixed position embeddings, \(E_{i}^{pos}^{D}\), that are added to patch embeddings at the transformer input; i.e., \(z_{i}=E_{i}^{patch}+E_{i}^{pos}\), where \(z\) is the input to the first transformer layer. Position embeddings represent the absolute positions of patches in an image.

\(\) 1D position embeddings  (**1D-learn** for short) map \(i\) to learnable embeddings. \(\) 2D sinusoidal embeddings  (**2D-sincos** for short) individually map \(i_{y}\) and \(i_{x}\) to fixed 1D-sinusoidal embeddings (\(E_{i}^{y},E_{i}^{x}^{}\)), then concatenate them along the embedding dimension. \(\) Factorized position embeddings  (**Factorized** for short) individually map \(i_{y}\) and \(i_{x}\) to learnable embeddings (\(E_{i}^{y},E_{i}^{x}^{D}\)), then add them. \(\) Learnable Fourier features  (**Fourier** for short) map \((i_{y},i_{x})\) to Fourier features [40; 41], then to embeddings with a multi-layer perceptron (MLP).

**Attention Biases.** This group leverages learned or fixed operations that encode positions by modifying the pairwise interactions between patches in self-attention _without_ adding position embeddings to patch embeddings. Recall that self-attention first applies three separate linear transformations to project internal patch representations and splits the resultant vectors into \(H\) smaller vectors of length \(D_{H}\); i.e., \(^{N D}^{3 N H D_{H}}\) -- creating queries, keys, and values for each attention head. We denote a specific head by \(h\). Next, attention scores (\(A^{H N N}\)) are calculated by measuring the similarity between all pairs of queries (\(q_{hi}^{D_{H}}\)) and keys (\(k_{hj}^{D_{H}}\)), separately, for each head; i.e., \(a_{hij}=q_{hi} k_{hj}/}\), where \(i\) and \(j\) are query and key sequence positions, and we define \((i_{y},i_{x})\) and \((j_{y},j_{x})\) as their \(2\)D positions. Attention scores (\(a_{hij}\)) represent the _amount_ of information moving from patch position \(j\) to \(i\) -- whereas values (\(v_{hj}^{D_{H}}\)) represent the _content_ of the moving information.

\(\) Learnable relative position encoding  (**RPE-learn** for short) biases attention scores by mapping all possible relative positions between queries and keys to learnable embeddings (\(B_{ij}^{H}\)); i.e., biases are a function of \(i_{y}-j_{y}\), \(i_{x}-j_{x}\), and \(h\). \(\) A \(2\)D extension of Attention with Linear Biases(ALBi ), **2D-ALBiBi** penalizes attention scores as a function of the Euclidean distance between \((i_{y},i_{x})\) and \((j_{y},j_{x})\), and a head-specific scalar, called a slope. Slopes bias attention heads at different rates. A A 2D extension of rotary position embeddings (RoPE ), **2D-RoPE** rotates queries and keys as a function of their positions. Each query is rotated by the sinusoidal embedding of \(i_{y}\) for half its dimensions and the sinusoidal embedding of \(i_{x}\) for the other half of its dimensions; likewise, keys are rotated as a function of \(j_{y}\) and \(j_{x}\).

**Non-plain ViTs.** Many hybrid or hierarchical architectures have been invented that often encode positions differently [42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53]. Although these architectures may be favored in some circumstances, the plain ViT is the most common single architecture due to its simplicity, flexibility, and scalability. We benchmark many non-plain ViTs and large SoTA ViTs on extrapolation in Appendix A.2.1.

**ViT Extrapolation.** Some ViTs have been tested at higher resolutions than they were trained [54; 43; 12; 55]. NaViT  benchmarked input embedding methods on extrapolation, none see the gains at higher resolutions that we observe.

## 3 LookHere

**Design Motivation.** We introduce 2D attention masks that assign each attention head a direction and a FOV, preventing attention outside the head's FOV. Within a head's FOV, attention scores are penalized based on relative patch distances. Three ideas motivate this design. A Attention head diversity: heads often learn redundant algorithms that can be pruned with little accuracy penalty [56; 57; 58]. Head redundancy has also been observed in NLP [59; 60; 61], where diversity-encouraging loss functions have been leveraged to improve generalization [62; 63; 64; 65]. From a mechanistic point of view, we can think of attention heads as an ensemble of sub-networks that "operate completely in parallel, and each add their output back into the residual stream,"  and the residual stream is mapped to logits. Diversity has long been a desirable property of ensembles [67; 68], and constraining attention heads to focus in different directions ensures it. A Attention head consistency: heads often learn interpretable spatial algorithms, like "attend to the area above the query," which reliably retrieves information from the internal representations above the query; however, we believe these types of spatial algorithms might fail when new or modified position embeddings are introduced to encode _new_ patch positions during extrapolation -- misleading the model about the information above the query, for example. We believe hard-coding both directions and distances (via attention masks and biases) will reduce the need for models to learn their own spatial algorithms. A Translation-equivariance has long been a desirable property of vision models, contributing to the success of convolutional networks [69; 70; 71]. ViTs are critiqued for weak inductive biases, leading to poor sample efficiency when trained from scratch [72; 73; 74]. We believe that LookHere's stronger inductive biases, achieved via directional masking and distance penalties, can improve ViT sample efficiency.

**Design Specifics.** Let \(H\) be the number of heads, \(L\) be the number of layers, and \(N\) be the number of patches (plus one for the CLS token). We denote the LookHere matrices by \(_{}^{L H(N+1)(N+1)}\). We encode positions by subtracting the LookHere matrix for a layer \(l\), \(_{}^{l}\), from the learned attention matrix, \(_{}^{l}=QK^{T}/}\), before the softmax that normalizes the attention matrix prior to multiplying it by values , i.e., \(^{l}=(_{}^{l}-_{ }^{l})\). We do not add position embeddings to patch embeddings.

Let \(i\) and \(j\) be query and key sequence positions, respectively, with \(2\)D-coordinates \((i_{y},i_{x})\) and \((j_{y},j_{x})\). Crucially, \(j\) is visible to \(i\) if \(j\) lies within \(i\)'s FOV. This attention masking technique is inspired by the \(1\)D causal masks used in autoregressive transformer decoders used in NLP . When \(j\) is visible, we bias the attention score based on the Euclidean distance between \(i\) and \(j\) to encode the relative distance between patches. We scale distances via a slope function \(m:_{L}_{H}\), \(m(l,h)=s_{l}(l) s_{h}(h) s_{g}\) that strengthens or weakens the distance penalty as a function of the head (\(s_{h}:_{H}\)) and layer (\(s_{l}:_{L}\)), scaled by a global slope \(s_{g}\). Finally, the CLS token is visible to all positions.

\[(l,h,i,j) =m(l,h)(i,j)&\\ &\] (1) \[(i,j) =-j_{y})^{2}+(i_{x}-j_{x})^{2}}\] (2)For example, Figure 2 displays attention matrices of a head that "looks right" with a \(90^{}\) FOV. We create three LookHere variants, the first two have FOVs of \(180^{}\) and \(90^{}\) (\(\)-\(180\) and \(\)-\(90\)). We direct attention heads eight different ways, selecting the four cardinal directions (\(,,,\)) and the four intercardinal directions (\(,,,\)). ViT-B models have twelve attention heads; we leave the last four attention heads undirected to allow them unrestricted attention over the full image. We create a final variant that cuts the first four \(\)-\(90\) masks in two, creating eight \(45^{}\) views that cover the full image without overlapping (\(\)-\(45\)). Visualizations of the bias matrices are in Appendix A.3.

**Design Ablations.** We offer four takeaways through extensive ablations (Appendix A.6): \(\) LookHere is robust to the choice of slope function. We set our default \(s_{l}\) to linearly decrease from \(1.5\) to \(0.5\) with increasing depth (inspired by depth-wise attention distance findings ). This helped in preliminary experiments, but the benefits disappear in our ablations. We arbitrarily set our default \(s_{h}\) to \((,,,)\) for the four undirected heads, but distance penalties on undirected heads can be removed entirely. We set \(s_{g}=1\); LookHere is also robust to the choice of the global slope. We believe precisely tuning slopes is unnecessary because models can learn to scale attention logit magnitudes. \(\) Increasing penalties with the square or square root of the distance harms extrapolation. \(\) Removing all distance penalties harms extrapolation. \(\) Our main contribution, 2D directional masks, are crucial to retain performance, but our method is robust to _many_ directional configurations.

**Compute.**\(_{}\) is precomputed and fixed, subtracting it element-wise from the learned attention matrices \(_{}\) only costs \(H(N+1)(N+1)\) floating point operations (FLOPs) per layer. For a ViT-B/\(16\) model, these subtractions account for \(0.016\%\) of the total FLOPs. LookHere reduces FLOPs by _not_ adding position embeddings to patch embeddings, but this amount is also negligible. Additionally, LookHere matrices offer structured sparsity (up to \(7/8\) for a \(45^{}\) FOV) that can speedup attention -- although exciting, this speedup requires custom kernels that we leave for future work.

## 4 Experiments

Deep neural networks -- including ViTs -- can be sensitive to seemingly minor hyperparameter changes when trained from scratch. Dosovitskiy et al.  finetuned the original ViT at a higher resolution, reaching \(77.9\%\) top-1 accuracy on ImageNet (we refer to ILSVRC\(2012\) or ImageNet-1k as ImageNet). Steiner et al.  searched \(28\) hyperparameter configurations, achieving best and average runs of \(80.0\%\) and \(76.9\%\), respectively (average calculation omits runs without data augmentation, as they were poor). Touvron et al.  ablated repeat augmentation , dropping accuracy by \(4.8\%\). Touvron et al.  replaced cross-entropy loss with binary cross-entropy loss, raising accuracy by \(1.3\%\). Importantly, these are all ViT-B/\(16\) models trained from scratch for \(300\) epochs on ImageNet. Informed by these observations and more, we design a controlled experiment: We search \(8\) hyperparameter configurations for _each_ position encoding method using a single codebase; this offers an apples-to-apples comparison between our three LookHere variants and seven baselines.

Figure 2: LookHere masks and biases (center) the learned attention matrix (left, where colors are random). Masked cells are **black**, encoding directions (\(\) with a \(90^{}\) FOV); biased cells are shaded bluish-green, encoding relative patch distances. (Right) An example of the FOV of the center query patch. The final attention matrix is computed as \(^{l}=(^{l}_{}-^{l }_{})\), at each layer \(l\).

### Setup

Our \(80\) training runs result from the following Cartesian product:

**Position encoding:** 1D-learn, 2D-sincos, **Augmentations:** RandAugment\((2,15)\), \(3\)-Augment  Factorized, Fourier, RPE-learn, 2D-ALBiBi, **Learning rate:**\(1.5 10^{-3}\), \(3.0 10^{-3}\)

2D-RoPE, LH-\(180\), LH-\(90\), LH-\(45\)

**Weight decay:**\(0.02,0.05\)

For each configuration, we train a ViT-B\(/16\) on \(99\%\) of the ImageNet training set, holding the last \(1\%\) as a validation set called "minival", following  (see Appendix A.4.1 for other hyperparameters). We train all models from scratch for \(150\) epochs on \(224^{2}\) px images. Our results are competitive and sometimes surpass ViTs trained for much longer, which validates our setup. The best models (according to minival accuracy), among our \(8\)-run hyperparameter sweep per method, are always trained using \(3\)-Augment , a \(3.0 10^{-3}\) learning rate, and a \(0.05\) weight decay.

**Test sets.** We test all \(80\) models on six ImageNet test sets. This includes \(\) the original "validation" set used as a test set (Val for short ), \(\) the reassessed labels of the original validation set (Real for short ), \(\) the independently collected and in-distribution test set (v2 for short ), \(\) the natural adversarial test set (-A for short ), \(\) the ImageNet rendition test set (-R for short ), and \(\) the high-resolution test set that we introduce (-HR for short).

**ImageNet-HR.** Since there are no natively high-resolution ImageNet test sets, there are two options to test the extrapolation ability of models trained on ImageNet: \(\) upsample existing test sets to higher resolutions, and \(\) collect a high-resolution test set ourselves. However, upsampling low-resolution images introduces another distribution shift (i.e., interpolated pixels) that we may not want to test. Thus, we collect a high-resolution test set to remove this confounding variable from our analysis. We manually collect \(5\) images for each ImageNet class, resulting in \(5\)k total images, and manually crop them to \(1024^{2}\) px. This is smaller than other test sets (v2 is \(30\)k images, -A is \(7.5\)k images). However, we invest considerable resources to ensure its quality with two priorities: annotation accuracy and image diversity. See Appendix A.1 for details. ImageNet-HR can be accessed: https://huggingface.co/datasets/antofuller/ImageNet-HR

**Adversarial Attacks.** We perform Fast Gradient Sign Method (FGSM ) adversarial attacks with two strengths (\(,\)) on all models using Val images.

**Calibration Estimates.** We calculate the Expected Calibration Error (ECE ) with \(15\) bins of all models using Val images.

**Higher-Resolution Finetuning.** With the best model per method, we continue training on ImageNet for \(5\) epochs at \(384^{2}\) px. We test at \(384^{2}\) px without extrapolating.

Figure 3: Images of three classes from ImageNet-HR. (Bottom left is Anthony’s niece Addison.)

**Segmentation.** With the best model per method, we finetune following the Segmenter protocol with a linear decoder . Additionally, we probe the patches by only training a linear layer to produce a low-resolution logit map which is upsampled to obtain a full resolution segmentation map, following . We run these experiments on ADE\(20\)k  at \(512^{2}\) px and Cityscapes  at \(768^{2}\) px.

**Patch Logit-lens.** Inspired by interpretability research , we evaluate the quality of the learned patch representations for models leveraging LookHere compared with other methods. Following prior work , we project frozen patch representations onto the learned class embedding space using the MLP classifier head that was learned for the CLS token. We leverage the ImageNet-S dataset , which contains partial segmentation maps for \(12\)k images from Val, covering \(919\) ImageNet classes.

**Extrapolating.** With the best model per method, we test on images larger than \(224^{2}\) px, increasing the number of patches and we test on images smaller than \(224^{2}\) px, decreasing the number of patches; for both experiments, no further training is performed -- the models are tested on their resolution generalization ability. For \(1\)D-learn and \(2\)D-sincos, we bilinearly interpolate the position embeddings used during training. For Factorized, we linearly interpolate the position embeddings for each axis. Fourier does not require adjustment since fractional positions along each axis are used as input. For RPE-learn, we interpolate the learned relative biases using the official BEiT implementation . \(2\)D-ALBi does not require adjustment either. However, we tune a parameter on minival that scales the distance penalty at each test resolution. For \(2\)D-RoPE, we tune its base frequency on minival -- this is a SoTA method to extrapolate RoPE used in NLP . Lastly for LookHere, we tune the global slope on minival. The benefits of tuning slopes are minimal, see Appendix A.4.4.

### Results and Analysis

LookHere improves ViT sample efficiency (Table 1). Our three variants outperform the best baseline, 2D-RoPE, under almost all test conditions (the single exception being the best 2D-RoPE model on -R). LookHere further improves gains when considering averaged results -- i.e., when accuracy values are averaged over 8 hyperparameter configurations (please see the Appendix A.5 for individual results). For instance, LH-\(180\) outperforms \(2\)D-RoPE by \(0.93\%\) / \(1.36\%\) on on Val / v2 on our best runs and by \(1.64\%\) / \(1.96\%\) on Val / v2 on our averaged runs -- indicating that LookHere decreases hyperparameter sensitivity. Surprisingly, LH-\(180\)_averages_\(80.01\%\) on Val, which matches the _best_ run trained for twice as long by Steiner et al. .

LookHere improves ViT adversarial robustness and model calibration (Tables 2 3); both have been linked to ensemble diversity , which we offer as a potential explanation. This is an interesting finding because adversarial robustness and calibration can be at odds with accuracy . We show that LookHere learns more diverse attention heads by measuring the generalized Jensen-Shannon divergence  between heads (Figure 4). In the Appendix A.8, we measure more properties of models leveraging different position encoding methods. LookHere significantly outperforms other methods on segmentation linear probing, demonstrating its ability to learn spatially

    &  &  &  &  &  &  \\  Method & Best & Avg. & Best & Avg. & Best & Avg. & Best & Avg. & Best & Avg. & Best & Avg. \\  \(1\)D-learn & 79.45 & 77.35 & 84.97 & 82.87 & 68.49 & 65.17 & 10.97 & 7.58 & 29.64 & 25.73 & 88.28 & 85.22 \\ \(2\)D-sincos & 79.05 & 77.44 & 84.62 & 82.96 & 67.86 & 65.31 & 10.45 & 7.76 & 29.11 & 26.07 & 87.58 & 85.36 \\ Factorized & 79.86 & 77.29 & 85.30 & 82.99 & 69.11 & 65.34 & 11.00 & 7.16 & 29.99 & 26.18 & 87.86 & 85.37 \\ Fourier & 79.69 & 77.37 & 85.13 & 82.89 & 68.30 & 65.33 & 11.36 & 7.79 & 29.73 & 24.62 & 88.14 & 85.39 \\ RPE-learn & 79.86 & 77.26 & 85.46 & 82.88 & 68.57 & 65.19 & 9.85 & 7.18 & 29.10 & 24.62 & 88.22 & 85.17 \\ \(2\)D-ALBi & 79.54 & 77.29 & 85.15 & 82.92 & 68.47 & 65.15 & 10.45 & 7.27 & 28.26 & 24.41 & 87.70 & 85.13 \\ \(2\)D-RoPE & 80.38 & 78.37 & 85.64 & 83.78 & 69.34 & 66.56 & 13.03 & 8.84 & 32.45 & 28.55 & 88.78 & 86.35 \\ LH-\(180\) & 81.31 & 80.01 & 86.53 & 85.30 & 70.70 & 68.52 & 13.53 & 10.45 & 32.10 & 28.94 & 89.86 & 87.80 \\ LH-\(90\) & 81.02 & 79.89 & 86.44 & 85.28 & 70.28 & 68.54 & 13.15 & 10.80 & 31.77 & 29.47 & 89.90 & 87.86 \\ LH-\(45\) & 81.06 & 79.74 & 86.23 & 85.07 & 69.65 & 68.18 & 13.41 & 10.21 & 32.12 & 29.51 & 89.46 & 87.43 \\   

Table 1: Top-\(1\) acc. (\(\%\)) for ViT-B models trained on ImageNet for \(150\) epochs; trained and tested at \(224^{2}\). We report the best and average results across our 8-run hyper-parameter sweep.

Figure 4: LookHere learns more diverse attention heads and prevents attention collapse. Legend follows Figures 1 7.

aware patch representations. LookHere also performs well with segmentation finetuning, achieving comparable performance to 2D-RoPE (Table 4).

High-resolution finetuning increases the performance advantage of all three LookHere variants over \(2\)D-RoPE (Table 5). This aligns with our intuition that improving extrapolation methods can improve high-resolution finetuning. Lower initial finetuning loss has been linked to better retaining the general representations learned during pretraining , and better extrapolating models have lower initial loss at a higher-resolution, by definition.

Using a "logit lens"  approach, we project patch representations onto the class embedding space . We observe that LookHere encodes semantic information in its patches faithful to the original patch location; these patch-level predictions act as a segmentation map that can be generated without additional training. The officer in Figure 5 is not a one-off example; using ImageNet-S , we see that LookHere outperforms \(2\)D-RoPE by at least \(22\%\) mIoU using this patch-projection method (Figure 5). Our best explanation is that, by restricting attention, LookHere

    &  &  \\   & Best & Avg. & Best & Avg. & \\ 
1D-learn & 58.87 & 54.36 & 44.23 & 41.37 & 10.13 & 12.21 \\
2D-sincos & 60.38 & 55.16 & 45.37 & 41.61 & 10.14 & 11.85 \\ Factorized & 60.86 & 56.19 & 46.34 & 42.32 & 10.01 & 11.37 \\ Fourier & 59.91 & 54.74 & 44.99 & 41.90 & 9.65 & 12.13 \\ RPE-learn & 59.81 & 53.36 & 45.04 & 40.19 & 8.66 & 11.42 \\
2D-ALBi & 58.07 & 53.68 & 43.32 & 40.30 & 2.96 & 11.24 \\
2D-RoPE & 60.59 & 57.16 & 47.11 & 43.77 & 9.60 & 11.48 \\ LH-\(180\) & **65.06** & **62.59** & **51.81** & **49.06** & **8.28** & **9.76** \\ LH-\(90\) & 63.89 & 61.88 & 50.87 & 48.07 & 8.68 & 9.91 \\ LH-\(45\) & **64.71** & **61.71** & **50.21** & **47.86** & 8.87 & 9.99 \\   

Table 2: Fast Gradient Sign Method attack  (\(\%\) top-\(5\) acc. on Val), best and average runs.

   Method & Val & Real. & -v2 & -A & -R & -HR \\ 
1D-learn & 81.46 & 86.46 & 70.69 & 18.80 & 29.80 & 89.82 \\
2D-sincos & 81.33 & 86.50 & 70.53 & 17.73 & 29.26 & 89.62 \\ Factorized & 81.50 & 86.62 & 70.95 & 18.05 & 29.98 & 89.50 \\ Fourier & 81.71 & 86.73 & 71.01 & 19.73 & 29.68 & 89.90 \\ RPE-learn & 82.01 & 87.17 & 71.66 & 18.13 & 29.53 & 90.20 \\
2D-ALBi & 81.41 & 86.73 & 70.50 & 18.01 & 28.60 & 89.46 \\
2D-RoPE & 82.31 & 87.21 & 71.82 & 21.68 & 33.38 & 89.92 \\ LH-\(180\) & **83.28** & **88.05** & **73.12** & **22.85** & **32.95** & **91.38** \\ LH-\(90\) & **83.08** & **87.99** & **72.99** & **23.51** & **32.63** & **91.24** \\ LH-\(45\) & **83.10** & **87.83** & **72.43** & **22.39** & **33.10** & **90.92** \\   

Table 5: Top-1 acc. (\(\%\)) for models trained at \(224^{2}\) px, finetuned and tested at \(384^{2}\) px.

Figure 5: We apply frozen MLP classifying heads (learned on the CLS token) on frozen patch representations. We visualize ImageNet class predictions: assault rifle (**red**), bulletproof vest (green), crash helmet (**blue**), and holster (**blue**). In parentheses, we show mIoU results (@224px) on ImageNet-S , where we apply this technique to segment images _without_ training.

presents the attention collapse at deeper layers observed in Figure 4 that divorces patch representations from their original patch locations; this collapse has been observed in other ViTs . We also expect that preventing attention collapse will benefit vision-language models, where frozen patch representations are used as "image tokens" that _should_ represent their original patch locations . More examples and detailed analysis are in Appendix A.7

LookHere significantly improves extrapolation ability (Figure 1). Our smallest FOV variant (LH-\(45\)) sees improving relative performance as resolution increases. LH-\(45\) outperforms \(2\)D-ALBi, which is equivalent to LookHere without our \(2\)D directional masks, by \(9.5\%\) on Val at \(1024^{2}\) px. These two results demonstrate the extrapolation benefits of restricting attention to fixed FOVs. LH-\(45\) gains \(1.3\%\) on Val when extrapolating from \(224^{2}\) to \(384^{2}\) px; this is the largest gain we find in the literature, including our extensive benchmarking of SoTA models in Appendix A.2. LookHere also outperforms other methods when tested on _smaller_ images, but the advantage narrows (Figure 6).

Interestingly, smaller _objects_ benefit most from extrapolation (Figure 7), which are distributed over more patches at test time. We believe this effect also explains the \(6-8\%\) that LookHere models gain when extrapolating on ImageNet-A from \(224^{2}\) to \(448^{2}\) px; by inspection, ImageNet-A seems to have small objects, and other work found zooming-in on center-cropped ImageNet-A images improves

Figure 6: ViT-B/\(16\) models trained for \(150\) epochs on ImageNet at \(224^{2}\) px and tested down to \(64^{2}\) px. Model architectures are consistent between runs other than _position encoding_ methods.

Figure 7: The effect of object size on accuracy gains or losses due to extrapolation. Object size is measured using annotations from Kaggle’s ImageNet Object Localization Challenge .

performance . Finally, all LookHere variants outperform other methods on ImageNet-HR, indicating better handling of interpolated pixels generated when upsampling lower-resolution imagery is _not_ the reason why LookHere extrapolates better.

Reducing the distribution shift faced by attention heads during extrapolation is our best explanation for LookHere's large relative improvement. Figure 8 shows attention maps that are "unflattened" to visualize the image regions to which heads attend, averaged over the same 5k images. We show one head per model that exhibits similar behavior at a \(224^{2}\) resolution. Models leveraging RPE-learn and 2D-ALBi learn variants of an algorithm that retrieve information from above the query; however, both models retrieve information elsewhere in the image when extrapolating. LookHere hard-codes this type of algorithm, which it continues to execute when extrapolating. In Appendix A.8 we find more examples of interesting attention head behaviour.

Extrapolation affects different datasets differently; it also affects different classes differently. For example, when extrapolating, all models underpredict certain classes (bakery, church, and tights) and overpredict other classes (mobile home, threshing machine, and sports car). This investigation is inspired by the class-level effects of data augmentation . In Appendix A.9 we find more class-level effects of extrapolation.

## 5 Closing

**Limitations**. The primary limitation of LookHere is it requires hand-designed directional masks and distance penalties. However, our extensive ablations demonstrate that LookHere is robust to the choice of directional masks and distance penalties. The primary limitation of our experiments is we do not scale ViTs to giant sizes. Instead, we select the most common size, the ViT-B/\(16\), and focus our computational resources on a controlled experiment -- that extensively and fairly tunes the appropriate baselines for plain ViTs; this allows us to make confident conclusions based on our thorough experiments.

**Conclusion.** LookHere position encoding significantly improves the ability of plain ViTs to make inferences when provided a greater number of patches than seen during training. We thoroughly demonstrate that LookHere outperforms other methods with and without extrapolation on standard image benchmarks and our high-resolution ImageNet test set called ImageNet-HR. We provide new insights into ViT extrapolation by showing object-size, class-level, and dataset-level effects. We believe LookHere will help the vision community transform higher-resolution into higher accuracy.

**Future Work.** We are excited to realize the computational gains that LookHere makes available via sparse attention kernels, as well as bring LookHere to video and 3D point-cloud applications.

**Acknowledgments.** Anthony thanks NSERC's Postgraduate Scholarships Doctoral program for funding his PhD.

Figure 8: Attention maps of three attention heads across four resolutions, where the query is in the center. We use the colormap: