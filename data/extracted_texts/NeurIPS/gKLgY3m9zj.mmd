# An Information Theoretic Perspective on

Conformal Prediction

 Alvaro H.C. Correia  Fabio Valerio Massoli  Christos Louizos  Arash Behboodi

Qualcomm AI Research

Amsterdam, The Netherlands

{acorreia, fmassoli, clouizos, behboodi}@qti.qualcomm.com

Qualcomm AI Research

Qualcomm AI Research, Qualcomm Technologies Netherlands B.V (Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.). 'Equal contribution.

###### Abstract

Conformal Prediction (CP) is a distribution-free uncertainty estimation framework that constructs prediction sets guaranteed to contain the true answer with a user-specified probability. Intuitively, the size of the prediction set encodes a general notion of uncertainty, with larger sets associated with higher degrees of uncertainty. In this work, we leverage information theory to connect conformal prediction to other notions of uncertainty. More precisely, we prove three different ways to upper bound the intrinsic uncertainty, as described by the conditional entropy of the target variable given the inputs, by combining CP with information theoretical inequalities. Moreover, we demonstrate two direct and useful applications of such connection between conformal prediction and information theory: (i) more principled and effective _conformal training_ objectives that generalize previous approaches and enable end-to-end training of machine learning models from scratch, and (ii) a natural mechanism to incorporate side information into conformal prediction. We empirically validate both applications in centralized and federated learning settings, showing our theoretical results translate to lower _inefficiency_ (average prediction set size) for popular CP methods.

## 1 Introduction

Machine learning (ML) models have rapidly grown in popularity and reach, having now found use in many safety-critical domains like healthcare  and autonomous driving . In these areas, predictions must be accompanied by reliable measures of uncertainty to ensure safe decision-making. However, most ML models are designed and trained to produce only point estimates, which capture only crude notions of uncertainty with no statistical guarantees. Conformal prediction (CP) , in particular its split variant (SCP) , has recently gained in popularity as a principled and scalable solution to equip _any_, potentially black-box, model with proper uncertainty estimates in the form of prediction sets; in loose terms, larger sets are associated with higher degrees of uncertainty.

In this work, we take a closer look at conformal prediction through the lens of information theory (IT), establishing a connection between conformal prediction and the underlying intrinsic uncertainty of the data-generating process, as captured by the conditional entropy \(H(Y|X)=-E_{P_{XY}}[ P_{Y|X}]\) of the target variable \(Y\) given the inputs \(X\). We prove conformal prediction can be used to bound \(H(Y|X)\) from above in three different ways: one derived from the data processing inequality, which we dub _DPI bound_, and two coming from a variation of Fano's inequality , a model agnostic one, which we call (simple) _Fano bound_, and another informed by the predictive model itself, to which we refer as _model-based Fano bound_. To the best of our knowledge, these bounds represent the first bridge connecting information theory and conformal prediction, which we hope will bring new tools to both fields. We already present two such tools in this paper: (i) we show our upper bounds serve asprincipled training objectives to learn classifiers that are more amenable to SCP, and (ii) we advance a systematic way to incorporate side information into the construction of prediction sets. In a number of classification tasks, we empirically validate that both these applications of our theoretical results lead to better predictive efficiency, i.e., narrower and, consequently, more informative prediction sets.

The rest of the paper is organized as follows. In Section 2, we first introduce the necessary background to guide the reader through our main theoretical results. We introduce our three new upper bounds to the intrinsic uncertainty in Section 3, and their applications to conformal training and side information in Sections 4 and 5, respectively. Thereafter, we explore the related work in CP and IT in Section 6, present and analyze our experimental results in Section 7, and finally conclude in Section 8.

## 2 Background

In this section, we present the needed background on conformal prediction and list decoding [18; 22; 68], an area of information theory that, as we show, is closely related to CP and especially useful in deriving our main results. We start with the necessary notation. As usual, we denote random variables in uppercase letters and their realization in lowercase, e.g., \(X=x\). We reserve calligraphic letters, e.g. \(\), for sets and use \(P_{X},Q_{X},\) to denote probability measures on the space \(\). To simplify the notation, we use \(P,Q,\) when the underlying space is clear. For example, given a probability measure \(Q_{XY}\), the probability of the event \(\{(X,Y):Y(X)\}\) is denoted as \(Q(Y(X))\).

### Conformal Prediction

Conformal prediction (CP) is a theoretically grounded framework that provides _prediction sets_ with finite-sample guarantees under minimal distribution-free assumptions. Concretely, given a set of \(n\) data points \((X_{i},Y_{i}),i=1,,n\) drawn from some (unknown) joint distribution \(P_{XY}\), CP allows us to construct sets \((X)\), such that for a new data point from the same distribution \((X_{test},Y_{test})\) we have the following guarantee for a target error rate \((0,1)\)

\[(Y_{test}(X_{test})) 1-,\] (1)

where the probability is over the randomness in the sample \(\{(X_{i},Y_{i})\}_{i=1}^{n}\{(X_{test},Y_{test})\}\). To make this more tangible, the reader can picture \((X)\) as a subset of the possible labels in a classification problem, or as a confidence interval around the point estimate of a regressor in a regression setting.

In this work, we focus on a variant called split conformal prediction (SCP)  that gained popularity in the ML community, since it can leverage _any_ pre-trained model \(f:\) in the construction of prediction sets. In this setting, the aforesaid \(n\) data points constitute a _calibration data set_\(_{cal}\), which must be disjoint from the training data set used to fit the predictive model \(f\). This separation between training and calibration data is what gives the name _split_ to the method.

The first step in SCP is to define a _nonconformity score function_\(s_{f}:\), which is itself a function of model \(f\) and captures the magnitude of the prediction error at a given data point; the higher the score \(s_{f}(x,y)\), the higher the disagreement between input \(x\) and prediction \(y\). At calibration time, we evaluate the score function at every \((X_{i},Y_{i})_{cal}\) to get a collection of scores \(\{S_{i}=s_{f}(X_{i},Y_{i})\}_{i=1}^{n}\), and at test time, we construct prediction set \((X_{test})\) as

\[(X_{test})=\{y:s(X_{test},y)(1- ;\{S_{i}\}_{i=1}^{n}\{\})\},\] (2)

where \((1-;\{S_{i}\}_{i=1}^{n})\) is the level \(1-\) quantile of the empirical distribution defined by \(\{S_{i}\}_{i=1}^{n}\). The central result in conformal prediction, which we restate below for completeness, proves that prediction sets thus constructed achieve _marginal valid coverage_, i.e., satisfy (1).

**Theorem 2.1** ([31; 64]).: _If \(\{(X_{i},Y_{i})\}_{i}^{n}\) are i.i.d. (or only exchangeable), then for a new i.i.d. draw \((X_{test},Y_{test})\), and for any \((0,1)\) and for any score function \(s\) such that \(\{S_{i}\}_{i=1}^{n}\) are almost surely distinct, then \((X_{test})\) as defined above satisfies_

\[1-(Y_{test}(X_{test})) 1-_{n}, { where }_{n}=-}{{n+1}}.\]

See Appendix B for the proof and a more thorough introduction to CP. It is worth noting that valid coverage is not sufficient; the uninformative set predictor that always outputs \((X_{test})=\) trivially satisfies (1). We would also like our prediction sets to be as narrow as possible, and that is why CP methods are often compared in terms of their (empirical) _inefficiency_, i.e., the average prediction set size \(}{{|D_{test}|}}_{x_{test}}|(x)|\) for some test data set \(_{test}\). This is, in fact, not the only type of inefficiency criterion, but we use it as our main performance metric since it is the most common .

We depict the split conformal prediction procedure in Figure 1, where we include two extra variables that will be useful later in the text: the model prediction \(=f(X)\), and the event of valid coverage \(E=\{Y(X)\}\), i.e., the event of the prediction set containing the correct class.

### Conformal Prediction as List Decoding

In a nutshell, we can see conformal prediction as defining a map from an input \(x\) to a set of candidates in the target set \(\). It turns out that the conformal prediction framework is equivalent to (variable-size) list decoding, an error-recovery model going back to the works of Elias  and Wozencraft  in communication theory--we review some of these results in Appendix C.2. In particular, consider the mapping from the true label \(y\) to the input \(x\) as a noisy communication channel \(p(x|y)\). The goal of an error-correcting code is then to decode the input \(x\) and recover the one true label \(y\). List decoding generalizes this idea, allowing the decoder to return a set of outcomes (a list) instead of a pointwise prediction. If the correct _solution_ is not part of the set output by the decoder, an error is declared. Although conformal prediction and list decoding were developed for different purposes, namely uncertainty quantification and error correction, it is easy to see that, if we allow for variable-size lists, the list decoding problem for the channel \(p(x|y)\) as described above is equivalent to the conformal prediction problem.

To our knowledge, this link between conformal prediction and information theory (and list decoding in particular) has gone unnoticed in the literature, and in this paper we leverage it in two directions. First, we apply information-theoretic inequalities for list decoding to upper bound the conditional entropy \(H(Y|X)\) of the data-generating process. This leads to new objectives for conformal training (see Section 4) and new bounds on the inefficiency of a given model (see Appendices E and G.3). Second, the information-theoretic interpretation of CP gives us an effective and theoretically grounded way of incorporating side-information into CP to improve predictive efficiency (see Section 5).

## 3 Information Theory Applied to Conformal Prediction

In this section, we develop our main results, which link information theory and conformal prediction. Concretely, we provide three novel upper bounds on the conditional entropy \(H(Y|X)\): one coming from the data processing inequality and two derived in a similar way to Fano's inequality. We defer the proofs to Appendix D for the sake of conciseness, but in broad strokes, our results come from relating the bounds on the error probability provided by these information-theoretic inequalities (typically in the context of error-correcting codes) to the guarantees provided by CP in Theorem 2.1.

### Data Processing Inequality for Conformal Prediction

We start by using the classical data processing inequality (DPI) in the context of conformal prediction. Specifically, we focus on the DPI for \(f\)-divergences, which we discuss thoroughly in Appendix C.1. In brief, for a convex function \(f\) with \(f(1)=0\), the \(f\)-divergence between two probability measures \(P\) and \(Q\) is defined as (see )

\[D_{f}(P||Q):=_{Q}[f()].\]

In particular, with \(f(x)=x x\) we recover the familiar notion of KL-divergence . The DPI for \(f\)-divergences states that for any two probability measures \(P_{X}\) and \(Q_{X}\) defined on a space \(\), and any map \(W_{Y|X}\), which maps \((P_{X},Q_{X})\) to \((P_{Y},Q_{Y})\), we have

\[D_{f}(P_{X}||Q_{X}) D_{f}(P_{Y}||Q_{Y}).\]

We can apply the DPI for \(f\)-divergence above in the context of conformal prediction by considering the probability of the event of valid coverage \(\{Y(X)\}\) under two different probability measures \(P\) and \(Q\). Taking \(P\) as the data-generating distribution \(P:=P_{X}P_{Y|X}\) and constructing \(Q:=P_{X}Q_{Y|X}\) for an arbitrary \(Q_{Y|X}\) (e.g., a machine learning model), we get the following proposition.

Figure 1: Graphical model of SCP. \(_{cal}\) is a calibration set, \((X)\) the prediction set, \(=f(X)\) the model prediction, and \(E\) the event \(\{Y(X)\}\). Square and round nodes are, respectively, deterministic and stochastic functions of their parents.

**Proposition 3.1** (DPI Bound).: Consider any conformal prediction method satisfying the upper and lower bounds of Theorem 2.1 for \((0,0.5)\). For any arbitrary conditional distribution \(Q_{Y|X}\), the true conditional distribution \(P_{Y|X}\) and the input measure \(P_{X}\), define the following two measures \(Q:=P_{X}Q_{Y|X}\) and \(P:=P_{X}P_{Y|X}\). Then, we have

\[H(Y|X) h_{b}()+(1-) Q(Y(X))+ _{n} Q(Y(X))-_{P}[ Q_{Y|X}],\] (3)

with \(_{n}=-}{{n+1}}\) and \(h_{b}()\) the binary entropy function \(h_{b}()=-()-(1-)(1-)\).

Note that the entropy term \(H(Y|X)\) is computed using the measure \(P\). We relegate the proof to Appendix D.1. In the bound in (3), the term \(Q(Y(X))\) appears inside a log, so an empirical estimate \((Y(X))\) would result in a lower bound and would be biased. We can provide an upper confidence bound on this estimate using the empirical Bernstein inequality  and use that instead. Based on the empirical Bernstein inequality, with probability \(1-\), we have

\[_{}(,n) :=()(2/)}{n}}+\] \[Q(Y(X)) (Y(X))+_{}(,n):= (Y(X)),\] \[Q(Y(X)) (Y(X))+_{}(,n) :=(Y(X)),\]

with \(V_{n}()\) the empirical variance of \(=(Z_{1},,Z_{n})\), \(Z_{i}=Q(y_{i}(x_{i}))\). Using these bounds, we get the following inequality with probability \(1-\):

\[H(Y|X) h_{b}()+(1-)(Y(X))+_{n }(Y(X))-_{P}[ Q_{Y|X}].\] (4)

This upper bound is one of our main results, which we dub the _DPI bound_. Note that for the last expectation, we can use the empirical estimate, as it is an unbiased approximation.

### Model-Based Fano's Inequality and Variations

Next, we present an inequality which is a variation of Fano's inequality , a classical result that, among other things, is used to prove Shannon's classical theorem on channel capacity. See Appendix C.4 for more details. In our context, we can use Fano's inequality to relate the conditional entropy \(H(Y|X)\) to the probability of error, i.e., \((Y(X))\). From that insight, we obtain Proposition 3.2 by modifying the classical proof of Fano's inequality, which can be found in , and applying the conformal guarantees from Theorem 2.1 to bound the probability of error.

**Proposition 3.2** (Model-Based Fano Bound).: Consider any conformal prediction method satisfying the upper and lower bounds of Theorem 2.1 for \((0,0.5)\). Then, for the true distribution \(P\), and for any probability distribution \(Q\), we have

\[H(Y|X) h_{b}()+_{P_{Y,X,_{ calI}Y(X)}}[- Q_{Y|X,(X),Y (X)}]\\ +(1-_{n})\,_{P_{Y,X,_{cal}|Y (X)}}[- Q_{Y|X,(X),Y(X)}].\] (5)

Note that we have one term conditioned on the event of valid coverage, \(\{Y(X)\}\), and another conditioned on \(\{Y(X)\}\). We provide the proof in Appendix D.2. A good choice for \(Q\) is the predictive model itself, and that is why we refer to the bound above as _Model-Based (MB) Fano bound_. Another natural choice for \(Q\) is the uniform distribution, which gives us the following result.

**Corollary 3.1** (Simple Fano Bound).: Consider any conformal prediction method satisfying the upper and lower bounds of Theorem 2.1 for \((0,0.5)\). Then, for the true distribution \(P\) we have

\[H(Y|X) h_{b}()+_{P_{Y,X,_{ calI}Y(X)}}[(||-|C(X)|)]\\ +(1-_{n})\,_{P_{Y,X,_{cal}|Y(X)}}[|C(X)|].\] (6)

The proof follows directly from Proposition 3.2 by replacing \(Q\) with the uniform distribution. We refer to the bound in (6) as (simple) _Fano bound_, since it is model agnostic and can be approximated directly with only empirical estimates of the prediction set size. This last bound explicitly relates the central notion of uncertainty in conformal prediction, the prediction set size, to an information-theoretic concept of uncertainty in \(H(Y|X)\). This reinterpretation of conformal prediction as a form of list decoding introduces various information-theoretic tools, potentially useful for various applications. In Appendix E we derive some new inequalities for conformal prediction, in particular offering new lower bounds on the inefficiency of the conformal prediction. In the next section, we discuss how these inequalities can be used as conformal training schemes.

Conformal Training

Although split conformal prediction is applicable to any pretrained ML model as a post-processing step, the overall performance of any CP method (commonly measured by its inefficiency) is highly dependent on the underlying model itself. Therefore, previous works have proposed to take CP into account already during model training and directly optimize for low predictive inefficiency . We use the term _conformal training_ to refer to such approaches in general (see Appendix F for an overview of the topic). In particular, we focus on ConfTr  since it generalizes and outperforms . In ConfTr  each training batch \(\) is split into calibration \(_{cal}\) and test \(_{test}\) halves to simulate the SCP process (see Section 2) for each gradient update of model \(f\) and minimize the following _size loss_

\[[|_{f}(X)|](}{{| _{test}|}}_{x_{test}}|_{f}(x)|),\] (7)

where \(_{f}(x)\) is constructed using the statistics of the nonconformity scores computed on \(_{cal}\). We use the notation \(_{f}(x)\) to emphasize the dependence of the prediction set on model \(f\). Still, SCP involves step functions and Stutz et al.  introduce two relaxations to recover a differentiable objective: (i) the computation of quantiles is relaxed via differentiable sorting operators ; (ii) the thresholding operation in the construction of prediction sets in (2) is replaced by smooth assignments via the logistic sigmoid. The latter relaxation gives "soft" prediction sets \(}_{f}(x)\), which contain each of the labels \(y\) with a certain probability. See Algorithm 1 for a depiction of conformal training.

Our upper bounds on \(H(Y|X)\), namely DPI, MB Fano and simple Fano, presented in the previous section can be made differentiable in the same way, and thus can also serve as proper loss functions for conformal training. The motivation for doing so is twofold. First, the conditional entropy \(H(Y|X)\) captures the underlying uncertainty of the task, or equivalently, the uncertainty under the true labelling distribution \(P_{Y|X}\). Thus, by minimizing these upper bounds, we can hope to push the model \(f\) closer to the true distribution, which is known to achieve minimal inefficiency . Interestingly, the cross-entropy loss also bounds \(H(Y|X)\) from above and thus can be motivated as a conformal training objective from the same angle. In that regard, the DPI bound from Proposition 3.1 is particularly advantageous as it is provably tighter than the cross-entropy (see Appendix D.1).

Second, we can connect the simple Fano bound from Corollary 3.1 to the size loss (7) from . In Appendix F.1, we show that via Jensen's inequality and \((|Y|-|C(X)|)|Y|\) the bound in (6) can be further upper bounded as

\[_{}:=h_{b}()+||-(1- _{n})(1-),\] \[H(Y|X)_{}+(1-_{n})[|C( X)|],\] (8)

Since \(_{}\) and \((1-_{n})\) are constants, they do not affect optimization, and minimizing the right hand side in (8) is equivalent to minimizing the size loss in (7). Therefore, we ground ConfTr as minimizing an upper bound to the true conditional entropy that is looser than the simple Fano bound and likely also looser than the model-based Fano bound for an appropriate choice for \(Q\).

``` input:A batch of labeled samples \(\), a model \(f\), a loss function \(\) that can be any of our upper bounds (4), (5) or (6) or a version of (7)--see variants (25) and (26) in Appendix F. for each training batch \(\)do  split \(\) into \(_{cal}\) and \(_{test}\) for each \((x_{i},y_{i})_{cal}\)do  compute score \(s_{i}=s_{f}(x_{i},y_{i})\) sort scores (in a differentiable manner) obtaining \(s_{(1)}<s_{(2)}<<s_{(|_{cal}|)}\) set \(=s_{((|_{cal}|+1)(1-))}\)// get \(1-\) quantile estimate using \(_{cal}\) for each \((x_{j},y_{j})_{test}\)do for each \(y\)do // Construct soft prediction set // Bounds (4) and (5) also require class probabilities under \(Q\), which in this case are given by the model \(f(x_{j})\) compute loss according to \(\) on \(_{test}\) using \(y_{j},}_{f}(x_{j})\) and if needed \(f(x_{j})\) update \(f\) via gradient descent to minimize loss ```

**Algorithm 1**Conformal training algorithm.

Side Information

With the information theoretical interpretation of conformal prediction, we can translate various intuitions from information theory, for example, about different types of channels or network information theory, to conformal prediction. In this section, we consider the notion of side information.

Let \(X\) be the input covariates, \(Y\) be the target variable, \(Z\) be some side information about the task, and let \(Q_{Y|X}\) be the model which we use to perform conformal prediction. As we can relate CP with \(Q_{Y|X}\) to an upper bound on the conditional entropy \(H(Y|X)\), we would like to do the same for the case of the conditional entropy when side information is available, i.e., \(H(Y|X,Z)\). Since we know that the conditional entropy directly affects the expected set size, i.e., the inefficiency of CP, and given that \(H(Y|X) H(Y|X,Z)\) we can expect that with the additional side information the inefficiency of CP will decrease. We can take side information into account by defining conformity scores as a function of \(Q_{Y|X,Z}\) instead of \(Q_{Y|X}\). A simple way to do that would be via the Bayes rule

\[Q_{Y|X,Z}=Q_{Z|X,Y}}{_{Y}Q_{Y|X}Q_{Z|X,Y}},\] (9)

where \(Q_{Z|X,Y}\) is an auxiliary model that predicts the side information given the input and target variables. Such a model could be learned separately from the main model \(Q_{Y|X}\) given access to a dataset \(_{side}=\{(x_{i},y_{i},z_{i})\}\). Therefore, we can now calibrate with CP by taking into account the side information and then, at test time, given access to the input and side information, we can use the appropriate probabilities \(Q_{Y|X,Z}\) to construct the prediction sets. Intuitively, the prediction sets from such a procedure should be smaller than the prediction sets obtained from using \(Q_{Y|X}\) directly. It should be noted that, in the case of side information not being available, we can marginalize \(Q_{Y,Z|X}\) over \(Z\), which, by construction, falls back to the original model \(Q_{Y|X}\). If the availability pattern of \(Z\) is consistent between the calibration and test sets, the conformal prediction guarantees still hold by defining the model as

\[f=Q_{Y|X,Z}&\\ Q_{Y|X}&\] (10)

With this addition to the split conformal prediction tool set, if new (side) information is made available at test time, we can properly incorporate it into the CP pipeline without having to retrain the main classifier. One needs only train an auxiliary classifier \(Q_{Z|X,Y}\), which might be much simpler than \(Q_{Y|X}\) (in our experiments, \(Q_{Z|X,Y}\) is given by a single linear layer) and require much less data. One notable example of side information arises naturally in the distributed setting, which we discuss next.

### The Distributed Learning Setting

Consider the case where we have a dataset that is distributed among a set of \(m\) devices and want to run conformal training to get a global model \(Q_{Y|X}\) trained on all the data. Further, assume it is hard to gather the data at a central location (e.g., due to privacy reasons), and thus we have to work with the data staying locally on each device. An example of this would be federated learning (or FL) . In this case, if \(Z\{1,,m\}\) identifies the device, the entropy \(H(Y|X)\) can be expressed as

\[H(Y|X)=H(Y|X,Z)+I(Y;Z|X)=_{P_{Z}}[H(Y|X,Z=z)]+I(Y;Z|X),\]

which decomposes into a weighted average of local entropy functions \(H(Y|X,Z=z)\). We can now use any of our proposed bounds for each of the conditional entropies \(H(Y|X,Z=z)\) by calibrating with CP independently on each device, ending up with

\[H(Y|X)_{P_{Z}}[H_{ub}(Y|X,Z=z)]+I(Y;Z|X),\]

where \(H_{ub}(Y|X,Z=z)\) corresponds to an upper bound of the conditional entropy \(H(Y|X,Z=z)\). Furthermore, for the mutual information term we have that

\[I(Y;Z|X)=_{P_{Z,X,Y}}[}{P_{Z|X}}] _{P_{Z,X}}[- P_{Z|X}]_{P_{Z,X}}[ - Q_{Z|X}]\]

where the first inequality is due to \(Z\) being discrete and having non-negative entropy and the second is due to Gibbs inequality with \(Q_{Z|X}\) being an auxiliary model trained to predict the user id \(Z=z\)given input \(X=x\). A similar upper bound has been considered before in a federated setting . With this upper bound we get

\[H(Y|X)_{P_{Z}}H_{ub}(Y|X,Z=z)-_{P_{X|Z=z}}[  Q_{Z=z|X}].\] (11)

This gives us an upper bound on the entropy of the entire population that decomposes into a sum of local functions, one for each client, only requiring local information. Thus, we can easily carry out conformal training for \(Q_{Y|X}\) by minimizing this upper bound in the federated setting with, e.g., federated averaging . At test time, we can take the device ID \(z\) as side information. To this end, we can either train a model \(Q_{Z|X,Y}\) in parallel and use (9) with the global model \(Q_{Y|X}\) at test time to get \(Q_{Y|X,Z}\), or we can obtain \(Q_{Y|X,Z}\) by fine-tuning the global model \(Q_{Y|X}\) with local data.

## 6 Related Work

Conformal prediction, a powerful framework for uncertainty quantification developed by Vovk and collaborators [60; 64], has recently witnessed a wide adoption in many fields, e.g., healthcare [3; 38; 39; 48] and finance [6; 67]. The marriage of conformal prediction and machine learning has been especially fruitful. Since the seminal work by Vovk et al. , many extensions and applications have been proposed, covering topics such as survival analysis , treatment effect evaluation , classification [4; 21] and regression  settings, risk control [5; 7], and covariate shift .

To our knowledge, our work represents the first attempt to bridge conformal prediction and information theory. Among other things, this allows us to build on the conformal training ideas of Bellotti  and Stutz et al. , deriving principled learning objectives that generalize their approaches, dispense with some of their hyperparameters and result in more efficient prediction sets. Further, we empirically show that our conformal training objectives provide a strong enough learning signal to train complex architectures from scratch, with strong results on ResNet-34 and ResNet-50  fitted on CIFAR10 and CIFAR100, respectively. In contrast, the previous state-of-the-art approach, ConfTr, struggles in those settings (see experiments in Section 7) and required pretrained models for consistent results . Further, our information-theoretic interpretation of CP provides a new simple and effective mechanism to leverage side information in split conformal prediction. We are unaware of any other approaches to treat side information within the conformal prediction framework in the literature.

On the information theory side, the notion of \(f\)-divergence and related inequalities have appeared in many different works. The use of \(f\)-divergence goes back to works of Ali and Silvey, Csiszar, and Morimoto in the 60s, as in, for instance, [2; 15; 44]. A key \(f\)-divergence inequality is the data processing inequality--see [57; 58] for an extensive survey. It provides a unified way of obtaining many classical and new results, including Fano's inequality . The tightness of the data processing inequalities is discussed in terms of Bregman's divergence in [12; 34] and in terms of \(^{2}\)-divergence in . List decoding, which is closely connected to CP, was introduced in the context of communication design by Elias  and Wozencraft . A generalization of Fano's inequality to list decoding was given in  in the context of multi-user information theory, see also the general Fano inequality for list decoding presented in . Variable-size list decoding was discussed in  using ideas first introduced in  and . A selection of relevant inequalities for list decoding can be found in .

## 7 Experiments

In this section, we empirically study two applications of our theoretical results, namely conformal prediction with side information and conformal training with our upper bounds on the conditional entropy as optimization objectives. We focus our experiments on classification tasks since this is the most common setting in previous works in conformal training [8; 13; 61].

### Conformal Training

We test the effectiveness of our upper bounds as objectives for conformal training in five data sets: MNIST , Fashion-MNIST , EMNIST , CIFAR10 and CIFAR100 . In addition to our three upper bounds, we also evaluate the cross-entropy loss (CE, also another upper bound to the entropy), and the two main variants proposed in , namely ConfTr, which minimizes (7) and ConfTr\({}_{}\) that optimizes an additional classification loss term (see Appendix F). We follow a similar optimization procedure and experimental setup to that of , but with the key differences that we learn the classifiers from scratch in all cases (without the need of pretrained CIFAR models), and that we use the larger "by class" split of EMNIST. For each data set, we use the default train and test splits but transfer 10% of the training data to the test data set. We train the classifiers only on the remaining 90% of the training data and, at test time, run SCP with 10 different calibration/test splits by randomly splitting the enlarged test data set. See Appendix G for a complete description of the experimental setup, with extra results and details on model architectures and hyperparameter search.

In Table 1, we report the empirical inefficiency on test data considering two SCP methods, threshold CP with probabilities (or THR)  and APS --see Appendix G.1.1 for results with RAPS . In all cases, our upper bounds proved effective loss functions to train efficient classifiers end-to-end and from scratch. For the simpler data sets (MNIST, Fashion-MNIST and EMNIST), all conformal training methods achieved similar results, but both ConfTr methods proved less consistent. This is noticeable in the oftentimes sharp difference in performance between THR and APS, since even after fine-tuning the hyperparameters (see Appendix G) some of the models failed to converge properly. For the remaining and more challenging data sets, both ConfTr variants lagged behind, probably because they do not provide a strong enough signal to train ResNets from scratch (on CIFAR data sets, Stutz et al.  only used ConfTr to fine tune pretrained models). A similar observation applies to the simple Fano bound (6), where the relaxed prediction set size is the only learning signal.

In all experiments, we run conformal training with a target coverage rate of 99%, i.e., \(=0.01\). It is then important to assess whether the performance of the resulting models deteriorates at different coverage rates, "overfitting" to the value of \(\) used for training. In Table 2, we see how inefficiency varies with \(\) at test time for models trained via conformal training with \(=0.01\). In particular, we can contrast their performance against that of models trained via the CE loss, which is agnostic to the desired coverage rate. In all cases, our model-based Fano and DPI bound performs best with the THR and APS methods, respectively, proving conformal training is worthwhile even if the desired coverage rate might vary at test time. Still, as noticed in , there is a drop in performance in comparison to the CE loss for higher values of \(\) at test time. This could be due to some degree of overfitting, but it could also be attributed to the conformal prediction problem becoming easier for lower coverage rates, thus reducing the gap between our bounds and the CE loss.

   Method &  &  &  \\   & THR & APS & THR & APS & THR & APS & THR & APS \\  CE & \(_{ 0.25}\) & \(26.02_{ 1.31}\) & \(6.11_{ 0.34}\) & \(9.19_{ 0.34}\) & \(3.02_{ 0.10}\) & \(4.52_{ 0.12}\) \\  ConfTr & \(32.80_{ 2.75}\) & \(40.58_{ 1.23}\) & \(12.25_{ 0.47}\) & \(21.60_{ 0.78}\) & \(7.13_{ 0.23}\) & \(14.58_{ 0.47}\) \\ ConfTr\({}_{}\) & \(66.48_{ 3.67}\) & \(32.91_{ 1.53}\) & \(14.18_{ 0.60}\) & \(16.80_{ 0.60}\) & \(8.90_{ 0.40}\) & \(11.29_{ 0.42}\) \\  Fano & \(40.30_{ 1.10}\) & \(33.80_{ 0.93}\) & \(19.43_{ 0.80}\) & \(16.17_{ 0.49}\) & \(11.46_{ 0.58}\) & \(9.72_{ 0.25}\) \\ MB Fano & \(_{}\) & \(21.68_{ 1.44}\) & \(_{}\) & \(9.25_{ 0.30}\) & \(_{}\) & \(5.51_{ 0.14}\) \\ DPI & \(17.55_{ 1.31}\) & \(_{}\) & \(6.26_{ 0.20}\) & \(_{}\) & \(3.33_{ 0.11}\) & \(_{}\) \\   

Table 2: **Infefficiency results with varying \(\) at test time**. Average prediction set size on CIFAR100 for different \(\) targets at test time, averaged across 10 random calib./test splits. All methods were only optimized for \(0.01\). The models used for THR and APS might not be the same according to the best hyperparameters found in Table 11. Lower is better.

   Method &  &  &  \\   & THR & APS & THR & APS & THR & APS \\  CE & \(19.70_{ 0.25}\) & \(26.02_{ 1.31}\) & \(6.11_{ 0.34}\) & \(9.19_{ 0.34}\) & \(3.02_{ 0.10}\) & \(4.52_{ 0.12}\) \\  ConfTr & \(32.80_{ 2.75}\) & \(40.58_{ 1.23}\) & \(12.25_{ 0.47}\) & \(21.60_{ 0.78}\) & \(7.13_{ 0.23}\) & \(14.58_{ 0.47}\) \\ ConfTr\({}_{}\) & \(66.48_{ 3.67}\) & \(32.91_{ 1.53}\) & \(14.18_{ 0.60}\) & \(16.80_{ 0.60}\) & \(8.90_{ 0.40}\) & \(11.29_{ 0.42}\) \\  Fano & \(40.30_{ 1.10}\) & \(33.80_{ 0.93}\) & \(19.43_{ 0.80}\) & \(16.17_{ 0.49}\) & \(11.46_{ 0.58}\) & \(9.72_{ 0.25}\) \\ MB Fano & \(_{}\) & \(21.68_{ 1.44}\) & \(_{}\) & \(9.25_{ 0.30}\) & \(_{}\) & \(5.51_{ 0.14}\) \\ DPI & \(17.55_{ 1.31}\) & \(_{}\) & \(6.26_{ 0.20}\) & \(_{}\) & \(3.33_{ 0.11}\) & \(_{}\) \\   

Table 1: **Inficiency results for conformal training in the centralized setting. We report the mean prediction set size (\(\) standard deviation) across 10 different calib./test splits for \(=0.01\), showing in bold all values within one std. of the best result. Results for THR and APS correspond to different models trained with different hyperparameters (see Appendix G). Lower is better.**

### Side Information

As a first experiment, we consider datasets for which a natural grouping of the labels exists and use the group assignment as side information \(Z\). In CIFAR100, there is a disjoint partition of the 100 classes into 20 superclasses, so we define \(z\) as the superclass to which example \((x,y)\) belongs. In EMNIST, \(z\) indicates whether the example is a digit, uppercase or lowercase letter. We train an auxiliary model \(R_{Z|X,Y}\) and, at test time, assume access to side information \(z\) to recompute class probabilities \(Q_{Y|X,Z=z}\) from the original classifier \(Q_{Y|X}\) as in equation (9).

We report results for two different scenarios in Table 3. The first is the standard SCP setting, where we assess the inefficiency of THR and APS methods, with side information \(Z\) observed for 10, 30 and 100% of the instances. We redefine the classifier \(f\) as in (10) to account for when \(Z\) is missing, but otherwise, the SCP process remains unchanged. The second scenario is Mondrian or group-balanced CP , where one splits \(_{cal}\) into groups and runs CP for each of them individually. In this setting, we group the calibration data points according to \(Z\) and base the score function on \(Q_{Y|X,Z}\). In all cases, taking the side information into account reduced the inefficiency considerably.

### Federated Learning (FL)

A practically relevant application of side information arises in FL, where we can take the device ID as side information \(Z\). In the federated setting, we train two extra heads on top of the main classifier, one computing \(Q_{Z|X}\) so that we can optimize the proper upper bound in (11), and another computing \(Q_{Z|X,Y}\) (while detaching gradients to the main classifier so as to not affect the upper bound optimization) that we use to integrate side information into CP using (9). Besides being a practically relevant application of side information to CP, FL also serves as a more challenging test bed for our conformal training methods, which has not been explored in previous work. We ran federated averaging  with CE, ConfTr, ConfTr\({}_{}\), and our upper bounds as local loss functions. In this setting, we consider CIFAR10, CIFAR100, and EMNIST with 100, 500, and 1K devices, resp. We assign data points to devices imposing a _distribution-based label imbalance_, i.e., we sample a marginal label distribution for each device from a Dirichlet distribution Dir\((1.0)\). See Appendix G for results with Dir\((0.5)\) and Dir\((0.1)\). As hyperparameter search in FL is notably challenging and costly , we keep the same hyperparameters found for the centralized case in Section 7.1.

After convergence, we ran SCP with the final global model assuming calibration and test data sets at the server, or equivalently that the clients share their scores with the server. This reflects the best inefficiency results we can hope for with the global model, as in practice we might need to resort to privacy-preserving methods that are likely to hurt performance. See Appendix G for a discussion and extra results on other possible settings. We report inefficiency results for the global model with THR in Table 4 (see Table 5 in the appendix for APS results), where we observe similar trends to the centralized experiments in Table 1. ConfTr methods still perform well on EMNIST but struggle on CIFAR data (with the notable exception on CIFAR100, where ConfTr excelled) while our methods delivered consistent results across all data sets. This probably reflects the sensitivity of both ConfTr objectives to hyperparameters, which makes them hard to use in practice, especially in FL where hyperparameter optimization is difficult. Conversely, our bounds seem more robust to such variations, as the hyperparameters found in the centralized setting seem to translate well to the federated case.

   Method &  &  \\   & THR & APS & Acc.(\%) & THR & APS & Acc.(\%) \\  CP & \(19.70_{ 0.25}\) & \(26.02_{ 1.31}\) & \(72.22\) & \(2.06_{ 0.11}\) & \(3.37_{ 0.15}\) & \(85.74\) \\ CP w/ 10\% SI & \(18.13_{ 2.63}\) & \(23.59_{ 2.08}\) & \(72.84\) & \(1.91_{ 0.07}\) & \(2.18_{ 0.09}\) & \(86.93\) \\ CP w/ 30\% SI & \(15.74_{ 1.11}\) & \(21.63_{ 1.45}\) & \(74.83\) & \(1.69_{ 0.05}\) & \(1.88_{ 0.07}\) & \(89.43\) \\ CP w/ 100\% SI & \(10.28_{ 0.86}\) & \(15.65_{ 1.17}\) & \(78.72\) & \(1.06_{ 0.02}\) & \(1.07_{ 0.02}\) & \(97.65\) \\  Group CP & \(17.59_{ 1.89}\) & \(21.92_{ 1.80}\) & \(72.22\) & \(2.32_{ 0.14}\) & \(2.68_{ 0.11}\) & \(85.74\) \\ Group CP w/ 100\% SI & \(9.07_{ 0.60}\) & \(13.16_{ 0.68}\) & \(78.72\) & \(1.14_{ 0.03}\) & \(1.16_{ 0.04}\) & \(97.65\) \\   

Table 3: **Inefficiency results with side information.** We report the mean prediction set size (\(\) std.) across 10 different calib./test splits for \(=0.01\). The side information is the superclass assignment for CIFAR100 and whether the class is a digit / uppercase letter / lowercase letter for EMNIST.

One marked difference between Tables 1 and 4 is that the simple Fano bound (6), which lagged behind the DPI bound and its model-based counterpart in the centralized setting, achieved the best results on the federated setting. We hypothesize this could be due to overfitting of the local optimization procedures to the individual data distribution of each device, which hurts the convergence of the global model. This effect is exacerbated on CIFAR100, where we have 500 devices, each of which with very few data points. The simple Fano bound is less vulnerable to such overfitting since it relies on the main classifier to a much lesser degree. Finally, in almost all cases, our bounds outperformed the CE loss, reassuring the potential of conformal training. Moreover, the inclusion of side information reduced inefficiency in all settings, and markedly so in a few instances. This confirms the effectiveness of our side information approach in a complex and practically relevant scenario, like federated learning.

## 8 Conclusion

In this work, we established a link between notions of uncertainty coming from conformal prediction and information theory (or more precisely variable-size list decoding). We proved that one can use split conformal prediction methods to upper bound the conditional entropy of the target variable given the inputs, and that these upper bounds form principled objectives for conformal training. We empirically validated our approach to conformal training, with strong results in both centralized and federated settings. Furthermore, the information-theoretic perspective also offers a simple yet rigorous approach to incorporate side information into conformal prediction, which we experimentally show leads to better predictive efficiency. To the best of our knowledge, this is the first attempt at connecting information theory and conformal prediction. Given the limited communication between these two research communities thus far, we expect our work to incite a fruitful exchange of not only ideas but also theory and algorithms between these two research domains. In this paper, we concentrated our exposition and experiments in classification tasks, but we see an extension of our methods to the regression setting, as a particularly promising avenue for future work.

   Method &  &  &  \\   & THR & THR\({}_{+}\) & THR & THR\({}_{+}\) & THR & THR\({}_{+}\) \\  CE & \(2.91_{ 0.02}\) & \(2.46_{ 0.02}\) & \(2.73_{ 0.04}\) & \(2.30_{ 0.06}\) & \(55.41_{ 1.09}\) & \(52.31_{ 1.03}\) \\  ConfTr & \(4.60_{ 0.05}\) & \(3.30_{ 0.02}\) & \(10.00_{ 0.00}\) & \(10.00_{ 0.00}\) & \(}\) & \(}\) \\ ConfTr\({}_{}\) & \(2.88_{ 0.02}\) & \(}\) & \(3.53_{ 0.09}\) & \(3.39_{ 0.08}\) & \(58.53_{ 1.40}\) & \(56.03_{ 1.29}\) \\  Fano & \(}\) & \(2.37_{ 0.02}\) & \(}\) & \(}\) & \(}\) & \(}\) \\ MB Fano & \(2.84_{ 0.04}\) & \(2.25_{ 0.03}\) & \(}\) & \(}\) & \(52.94_{ 1.40}\) & \(46.97_{ 1.30}\) \\ DPI & \(}\) & \(2.23_{ 0.01}\) & \(2.76_{ 0.07}\) & \(2.28_{ 0.03}\) & \(52.36_{ 0.95}\) & \(48.64_{ 0.70}\) \\   

Table 4: **Inefficiency results for conformal training in the federated setting with THR. We report the mean prediction set size (\(\) standard deviation) of the global federated model across 10 different calib./test splits for \(=0.01\) and using THR. We use \({}_{+}\) to indicate the inclusion of side information. We show in bold all values within one standard deviation of the best result. Lower is better.**

   Method &  &  &  \\   & APS & APS\({}_{+}\) & APS & APS\({}_{+}\) & APS & APS\({}_{+}\) \\  CE & \(3.69_{ 0.03}\) & \(3.14_{ 0.04}\) & \(}\) & \(2.43_{ 0.06}\) & \(64.73_{ 0.34}\) & \(62.67_{ 3.68}\) \\  ConfTr & \(6.14_{ 0.04}\) & \(5.25_{ 0.04}\) & \(10.00_{ 0.00}\) & \(10.00_{ 0.00}\) & \(55.18_{ 2.10}\) & \(47.58_{ 1.48}\) \\ ConfTr\({}_{}\) & \(}\) & \(}\) & \(10.00_{ 0.00}\) & \(10.00_{ 0.00}\) & \(99.92_{ 0.02}\) & \(99.91_{ 0.01}\) \\  Fano & \(3.12_{ 0.04}\) & \(2.72_{ 0.03}\) & \(}\) & \(}\) & \(}\) & \(}\) \\ MB Fano & \(4.75_{ 0.03}\) & \(}\) & \(}\) & \(}\) & \(50.72_{ 1.17}\) & \(45.72_{ 1.38}\) \\ DPI & \(2.98_{ 0.03}\) & \(2.58_{ 0.02}\) & \(}\) & \(}\) & \(51.29_{ 1.07}\) & \(47.18_{ 1.27}\) \\   

Table 5: **Inefficiency results for conformal training in the federated setting with APS. We report the mean prediction set size (\(\) standard deviation) of the global federated model across 10 different calib./test splits for \(=0.01\) and using APS. We use \({}_{+}\) to indicate the inclusion of side information. We show in bold all values within one standard deviation of the best result. Lower is better.**