# Weak-to-Strong Search:

Align Large Language Models via

Searching over Small Language Models

 Zhanhui Zhou\({}^{*}\), Zhixuan Liu\({}^{*}\), Jie Liu, Zhichen Dong, Chao Yang\({}^{}\), Yu Qiao

Shanghai Artificial Intelligence Laboratory

\({}^{*}\)Core Contribution, \({}^{}\)Corresponding Author

asap.zzhou@gmail.com, yangchao@pjlab.org.cn

Code: https://github.com/ZHZisZZ/weak-to-strong-search

###### Abstract

Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce _weak-to-strong search_, framing the alignment of a large language model as a test-time greedy search to maximize the log-probability difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (1) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (2) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned gpt2s to improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., zephyr-7b-beta and its untuned version) can improve the length-controlled win rates of both white-box and black-box large models against gpt-4-turbo (e.g., \(34.4\% 37.9\%\) for Llama-3-70B-Instruct and \(16.0\% 20.1\%\) for gpt-3.5-turbo-instruct), despite the small models' low win rates \( 10.0\%\).

## 1 Introduction

Learning-based algorithms  have become the standard approach for aligning large language models (LLMs) with human preferences . However, fine-tuning large language models is resource-intensive and difficult to implement . These challenges have motivated recent studies on search-based algorithms that keep the large language models frozen and steer their decoding with test-time guidance . Typical examples of search-based algorithms include rejection sampling  and Monte Carlo Tree Search . These search-based algorithms are promising as they can reuse the same guiding signal to steer the decoding of any large language model without additional training. However, existing search-based methods either simplify the search over tokens as a bandit problem , which limits their steerability, or require a value function learned from scratch to address preference reward sparsity and prune search space , which can be as difficult as fine-tuning a large language model.

To make search-based algorithms better suited for aligning large language models, we introduce _weak-to-strong search_, a simple algorithm that frames the alignment of a large model as a test-time search over the log-probabilities of small language models. This algorithm makes two contributions: **(1)** First, it builds on the theoretical foundation of the token-level MDP for alignment , using thelog-probability difference between small tuned and untuned language models as both reward and value [4; 20] to guide the decoding of a large model (Section 4.1). Theoretically, this formulation is suitable for search as it converts the otherwise sequence-level sparse preference reward function to a per-token dense reward function, which can be summed up as a value function . Practically, this formulation allows the reuse of off-the-shelf small tuned and untuned language model pairs as steering forces, avoiding the need to train a reward or value model from scratch. **(2)** Second, it introduces a beam search variant, Chunk-level Beam Search (CBS), tailored for optimizing the proposed search objective. CBS guides the large language model towards high-reward regions by alternating between sampling from the frozen large model and expanding promising states as evaluated by the small tuned and untuned models (Section 4.2). Especially, when the small models are weaker than the large model, our method can be viewed as an instance of weak-to-strong generalization  that makes the strong model stronger with weak test-time guidance (Figure 1).

Empirically, we verify weak-to-strong search's flexibility in various tasks (Section 5). First, in controlled-sentiment generation  and summarization , our method uses small language models of 124M parameters (i.e., gpt2) to effectively steer much larger language models from the GPT-2 (e.g., gpt2-xl) , Llama-2  and Llama-3  families, at least as effective as existing methods. Then, in a more difficult instruction-following benchmark, AlpacEval 2.0 , we show reusing off-the-shelf small models (e.g., zephyr-7b-beta and its untuned version) as test-time guidance can significantly improve the length-controlled win rates of both white-box and black-box large models against gpt-4-turbo (e.g, \(34.4\% 37.9\%\) for Llama-3-7OB-Instruct, and \(16.0\% 20.1\%\) for gpt-3.5-turbo-instruct), despite the small models' low win rates \( 10.0\%\) (Figure 1).

## 2 Related Work

Large unsupervised language models trained on internet-scale corpus acquire broad knowledge and abilities [26; 27; 28]. However, these large pre-trained language models may not always align with human values. To instill the desired behaviors into language models, most existing methods fine-tune these pre-trained language models on human comparisons of model-generated responses [1; 2; 3; 4; 7; 24; 6; 29]. Despite these successes, fine-tuning a large language model requires substantial computational resources and engineering effort. These problems are compounded by the reality that different humans have different values [3; 30; 13; 31; 32; 33], as it is nearly impossible to train a new large language model from scratch for individual preference. In light of these issues, our work takes a search-based approach, folding as much of the complexity of alignment as possible into the decoding phase. This allows us to keep the large pre-trained language models frozen, steering their outputs at test time with only small models that are easier to obtain.

Framing alignment as a test-time search to maximize a reward function is not a novel formulation. However, most existing works either simplify autoregressive decoding as a bandit problem [16; 17], which limits their steerability, or require a value function learned from scratch to handle sparse

Figure 1: Weak-to-strong search enhances the alignment of large models through test-time guidance from small models (dashed lines). This method is applicable to white-box models that use the same or **different** vocabularies as the small models, as well as to **black-box** models. We present the results for the instruction-tuned models from each family (e.g., Llama2-7B denotes Llama-2-7b-chat).

preference rewards and prune search space [13; 18], which can be as difficult as training a large language model from scratch. Our work avoids these issues by parametrizing the sparse preference reward function with the log-probability difference between small tuned and untuned language models . This parametrization not only simplifies the search objective, allowing a simple greedy search algorithm to generate good results, but also reuses off-the-shelf models as steering forces, eliminating the need to train a reward or critic model from scratch.

Concurrently with our work, Rafailov et al.  proposes a token-level MDP interpretation for language model alignment, demonstrating that a greedy probability search over a trained language model can achieve improvements over regular decoding. Our work builds on their theoretical foundations and proposes a practical greedy search algorithm designed for weak-to-strong guidance.

The idea of using small language models to align large language models has arisen in many recent works. The most related is proxy or emulated fine-tuning [34; 12; 11; 35], which uses the distributional difference of a small tuned and untuned model pair to modify the output distribution of a large model, approximating the output of the directly tuned large model. However, these methods require that both small and large models share the same vocabulary, limiting their practical applications. In contrast, our approach does not modify the sampling distribution of the large model at the token level. Instead, we perform a tree search that periodically prioritizes the most promising states for further expansion (as evaluated by the small models) while sampling from the frozen large model's distribution. Thus our approach does not require shared vocabulary and is applicable to black-box language models.

## 3 Preliminaries

In this section, we introduce the mathematical formulation of alignment (Section 3.1) and describe the duality between language models and reward functions (Section 3.2).

### Aligning Language Models with Human Preferences

The alignment of language models is typically cast as a KL-constrained optimization problem :

\[*{arg\,max}_{} _{ p(),( |)}[r(,)]\] (1a) s.t. \[_{ p()}[_{}(()_{}( ))],\] (1b)

where \(p()\) is a distribution of prompts, \(\) is the complete language model response, \(r\) is a preference reward function that encourages human-preferred responses, and \(_{}\) limits how far the optimized language model \(\) can deviate from the reference (untuned) model \(_{}\). There are two main categories of alignment algorithms: (1) _search-based_ algorithms that optimize Eq. 1 with graph-based search during inference [16; 13; 18; 19; 11; 12], and (2) _learning-based_ algorithms that optimize Eq. 1 through gradient descent, aiming for a parametrized optimal language model [1; 36; 4; 37]. Our work falls in the first category, proposing a search-based algorithm capable of using small language models to guide the decoding of a large language model to align with human preferences.

### Duality between Language Models and Reward Functions

The analytical solution to Eq. 1 can be obtained through the following Lagrangian [38; 39]:

\[(,)=_{ p(), (|)}[r(,)+( -_{}(() _{}()))],\] (2)

which has a well-known closed-form solution that expresses a duality between the reward function \(r(,)\) and the optimal language model \(^{*}()\)[40; 41]:

\[r(,)=()}{ _{}()}+ Z(),\] (3)

where \(Z()=_{}_{}() (r(,))\) denotes the partition function. One takeaway from this duality is that we can always express a reward function using tuned and untuned language models: (1) If a reward function is given [1; 2; 3], we can first obtain the optimally tuned language model under this reward function with any learning-based algorithms, and then use the tuned and untuned models \((^{*},_{})\) to reparametrize the reward function ; (2) If a dataset is given from which the reward function can be derived, we can then directly parametrize the reward function with the tuned and untuned language models \((^{*},_{})\) during reward modeling .

Weak-to-Strong Search

In this section, we introduce weak-to-strong search, a search-based algorithm that aligns a large language model by searching over the log-probability difference between small tuned and untuned language models. First, we discuss how using language models to parametrize the preference reward function (Eq. 1) makes the reward-maximization problem solvable by a simple greedy search algorithm (e.g., beam search) (Section 4.1). Then, we introduce a practical beam search method, Chunk-level Beam Search (CBS) (Section 4.2), that balances reward maximization and KL minimization, which is applicable to steering both white-box and black-box large language models.

### Language Models as Both Reward and Value Functions

One practical challenge for search-based alignment algorithms is the sparsity of the preference reward signal. The preference reward function \(r(,)\), based on the Bradley-Terry model , only emits a terminal reward when the model response is complete. Search-based algorithms often struggle without any intermediate rewards or a value function providing intermediate guidance . However, if we parameterize this sparse reward function with language models (Section 3.2), we can obtain both a dense reward function and a value function simultaneously.

Language models as a dense reward function.To obtain a dense reward function, we leverage the duality between the sparse preference reward function and the dense language model probability (Eq. 3). By explicitly factorizing the log-probability of a complete response \(\) under the language models, we obtain a sum-of-rewards style formulation for Eq. 3:

\[r(,)=(_{t=1}^{||}(_{t},_{<t})}{_{}(_{t},_{<t})})+ Z(),\] (4)

where \(_{<t}\) denotes the response tokens from \(1\) to \(t-1\), and the last response token \(_{||}\) is the EOS token. Combining Eq. 1 and 4, we rewrite the original objective with a per-token reward function:

\[*{arg\,max}_{} _{ p(),^{} (|)}[_{t=1}^{||}( _{t},_{<t})}{_{}(_{t },_{<t})}]\] (5a) s.t. \[_{ p()}[_{}(()_{}( ))],\] (5b)

where \(\) and \(Z()\) are omitted as they do not influence the optimal solution. It is important to note that the reference model that parametrizes the reward function (\(_{}\)) (Eq. 5a) and the reference model that constrains the test-time search space (\(_{}\)) (Eq. 5b) can be different. **Practically, decoupling the reference models is useful as it allows using a tuned and untuned language model pair - namely (\(^{*},_{}\)) - to steer the decoding of any base language model \(_{}\) without retraining**.

Setting aside the KL constraint (Eq. 5b) for now, we can apply existing search algorithms like beam search  to optimize Eq. 5a. Beam search is often criticized for leading to myopic solutions , as it tends to greedily prioritize states \((,^{})\) (\(^{}\) is incomplete)1 with high cumulative reward \(^{*}(^{})-_{}(^{})\) midway through generation, which is generally viewed as poorly correlated with the overall return we care about. While this criticism is valid for most MDPs, we argue that in the token-level MDP  of our case, the cumulative reward mid-generation is actually a reliable indicator of the long-term value, making beam search less myopic.

Cumulative reward under language models as a value function .Appendix A shows that:

\[(^{})}{_{}( ^{})}-V^{*}()+V^{* }(,^{})&^{}\\ -V^{*}()+r(,^{})&^{ },\] (6)

where \(V^{*}(,^{})\) denotes the value function, predicting the expected terminal reward under the optimal \(^{*}\) in the original KL-constrained sparse reward problem. Although \(V^{*}(,^{})\) is not necessarily achievable by the searched policy, it approximates how good the state \((,^{})\) is in the long run. In other words, continuing from the state \((,^{})\) of high cumulative reward \(^{*}(^{})-_{}(^{})\) is likely to generate a complete response \(\) with high overall return \(^{*}()-_{}( )\).

### Chunk-level Beam Search (CBS)

After analyzing the feasibility of optimizing Eq. 5a with greedy search algorithms (e.g., beam search), we introduce a practical beam search variant that optimizes the dense reward objective (Eq. 5a) while ensuring the KL-constraint from \(_{}\) (Eq. 5b).

The core algorithm providing the foundation of our method, Chunk-level Beam Search (CBS), is detailed in Algorithm 1 and illustrated in Figure 2. The key insight is that our beam search operates at the level of chunk. The search starts at the prompt and always maintains a hypothesis set \(=\{(,^{})_{i}\}_{i=1}^{W}\) of \(W\) states. For each state \((,^{})\) in \(\), CBS samples \(K\) continuation chunks \(_{L}\) of length \(L\) from \(_{}\). This results in \(WK\) successor states. Among these successors, only the top-\(W\) successors with the highest partial return \(^{*}(^{}_{L})-_{ }(^{}_{L})\) are stored in \(\) and expanded further. Finally, the terminal state \((,)\) with the highest intermediate return \(^{*}()-_{}( )\) is selected, from which the complete response \(\) is extracted.

```
1:Input: prompt \(\), beam width \(W\), successors per state \(K\), chunk length \(L\),
2: model to steer \(_{}\), tuned model \(^{*}\), and untuned model \(_{}\).
3:Output: optimal terminal state \((,)\)
4:Initialize \(=\{(,^{}=)_{i}\}_{i=1}^{W}\)
5:while\((,^{})\) such that \(^{}\) is incomplete do
6: Initialize \(=\{\}\)
7:for each \((,^{})\)do
8:\(\{(_{L})_{i}\}_{i=1}^{K} }{}_{}(,^{})\)// \(_{L}=\) if \(^{}\) is complete
9:\(\{(,^{} _{L})_{L}\}\)
10:endfor
11:\(W_{(,^{} _{L})}(^{*}(^{} _{L})-_{}(^{}_ {L}))\)
12:endwhile
13:return\(_{(,)}(^{*}( )-_{}())\) ```

**Algorithm 1** Chunk-level Beam Search (CBS)

**CBS is a unified framework that encompasses several search-based algorithms**: (1) CBS with \(W=1\), \(K=N\), \(L=\) (i.e., infinite chunk length) is equivalent to BoN sampling with \(^{*}()-_{}( )\) as the scoring function, and (2) CBS with \(K=\), \(L=1\) (i.e., exploring all possible next tokens from the vocabulary) is equivalent to vanilla token-level beam search. _However, we always ensure finite chunk length and limited successor exploration via sampling to achieve the best of both worlds_: (1) Using a finite chunk length allows CBS to prune bad states during generation, enhancing steerability more efficiently compared to BoN. (2) Sampling from

Figure 2: Illustration of Chunk-level Beam Search with \(W,K=2,2\).

\(_{}\) with limited successor exploration implicitly enforces the KL-constraint from \(_{}\) (Eq. 5b); otherwise, integrating the KL-constraint into the objective (Eq. 5a) would be necessary for token-level search, but this can be challenging, especially when vocabularies of models differ or with black-box language base models \(_{}\) whose log-probabilities are inaccessible.

**Computation costs.** In practice, CBS samples \(WK\) continuation chunks in parallel from the frozen base model \(_{}\) and prune states by calling tuned and untuned model pair \((^{*},_{})\) every \(L\) tokens. Larger \(WK\) and smaller \(L\) enhance steerability at the cost of increased computations. Note that high steerability, while beneficial, is not always ideal as it may lead to large KL deviation and over-optimization .

### Application: Model Up-Scaling and Weak-to-Strong Generalization

The most practical use of CBS occurs when the tuned and untuned models, \((^{*},_{})\), are smaller than the model to steer, \(_{}\). (1) First, this instance serves as a model up-scaling strategy, directly tuning a small model \(_{}^{*}\), by which the large model decoding can then be guided, to achieve similar outcomes as directly tuning the large model. (2) Second, since the small models \((^{*},_{})\) are usually weaker than the large model to steer \(_{}\), this instance also exemplifies weak-to-strong generalization , enhancing the strong model with only weak test-time guidance. We refer to this instance of CBS as weak-to-strong search, which is the main focus of our study.

## 5 Experiments

In this section, we empirically evaluate weak-to-strong search's ability to align large language models using only test-time guidance from small language models. First, in **controlled-sentiment generation** and **sumarization**, we tune gpt2 to model the desired behaviors in each task and then use tuned and untuned gpt2 to steer larger models of various scales (Section 5.1). Next, in a more difficult **instruction-following** benchmark, AlpacaEval 2.0 , instead of tunning small models, we reuse off-the-shelf open-source 7B models and their untuned versions to steer a series of large models, including open-source 70B models and a black-box model (Section 5.2).

Baselines.In addition to weak-to-strong search, we evaluate several existing test-time approaches that steer a large language model \(_{}\) using small tuned and untuned language models \((^{*},_{})\): (1) **Base**: we explore regular decoding from the frozen large language model with n-shot prompting (see Appendix B.1.6 for prompt details). (2) **Best-of-N Sampling (BoN)**[16; 17]: BoN uses \(r=^{*}()-_{}( )\) to select the highest-scoring responses among the \(N\) independent responses from the frozen large language model. Since weak-to-strong search (CBS) samples \(WK\) response chunks in parallel, for fair computational comparisons, we always ensure \(N=WK\). (3) **Emulated Fine-Tuning (EFT)**[34; 12; 11; 35]: EFT approximates the results of directly fine-tuning the large language model by sampling from \(_{}(_{t},_{<t}) _{}(_{t},_{<t})+^{-1}( ^{*}(_{t},_{<t})-_{ }(_{t},_{<t}))\), where \(\) is the hyperparameter from Eq. 2. Note that EFT is only applicable when all models share the same vocabulary (which is necessary for composing output distributions from different models). Whenever possible, we also compare test-time methods against **directly fine-tuning** the large models in the same way small models are tuned.

### Controlled-Sentiment Generation & Summarization

Setup.For these two tasks, we follow the synthetic setups from [16; 46; 4], assuming access to a gold reward model \(r_{}\). For controlled-sentiment generation, \(r_{}\) encourages positive continuations of movie reviews, while for summarization, it encourages high-quality summaries of Reddit posts (details in Appendix B.1.4). We generate synthetic preference datasets \(=\{(,_{w},_{l})_{i}\}_{i=1}^{N}\) from \(r_{}\) with \(p(_{1}_{2})=(r_{}( ,_{1})-r_{}(,_{2}))\) to mimic human feedback .

To obtain the small language models, we optimize gpt2 (124M parameters) using the standard DPO pipeline : (1) we first obtain the reference model \(_{}\) through supervised fine-tuning on both chosen and rejected responses from the synthetic preference dataset, then (2) we apply DPO on the synthetic preference dataset with \(_{}\) as the reference policy to obtain the optimal language model \(^{*}\). Note that the first stage primarily informs the language model of the desired response format, with most of the tuning occurring in the second DPO stage.

Given the tuned and untuned (un-DPO-tuned) gpt2 pair \((^{*},_{})\), we use them to steer the large pre-trained language models without additional training. The large pre-trained language models we study fall into two categories based on whether they share the same vocabulary as the small models: (1) **same vocabulary**: gpt2-large (774M), gpt2-xl (1.5B) and (2) **cross vocabulary**: Llama-2-7b, Llama-3-8B. Eventually, since we have access to the gold reward model, language model responses can be fairly evaluated on the test split of prompts using this gold reward model.

Results.Figure 3 demonstrates weak-to-strong search's great flexibility and steerability in both tasks. For summarization, weak-to-strong search consistently outperforms other test-time methods by large margins. For controlled-sentiment generation, weak-to-strong search is second only to EFT with a carefully selected hyperparameter (\(^{*}=1/4\)) when EFT is applicable. We hypothesize that token-level adjustments from EFT are sufficient for controlled-sentiment generation, which primarily requires minor stylistic changes at the token level (e.g., "hate" \(\) "love"). However, in the more complex task of summarization, where broader subsequence-level manipulations are essential, weak-to-strong search excels. Please refer to Appendix D for quantitative comparisons of samples from different methods. We need to mention that we do not meaningfully tune weak-to-strong search (CBS)'s hyperparameters to obtain the results in Figure 3 (we use a fixed set of hyperparameters of \((4,4,5)\) for \(W,K,L\) across all models), which may underestimate the performance of our method. In addition, our method enables **consistent weak-to-strong generalization in the harder task of summarization**: most large pre-trained models (except for gpt2-large) are stronger than the tuned gpt2 in summarizing long text, but the weak models are still able to improve the strong models through test-time guidance, **nearly matching the results of direct fine-tuning**. The phenomenon of weak-to-strong generalization will be further studied in Section 5.2.

Chunk-level Beam Search ablations.We perform additional ablations to understand how CBS hyperparameters (beam width \(W\), successors per state \(K\), and chunk length \(L\)) influence performance. Figure 4 displays the ablation results for \(W\) and \(K\). With the same computation budget (i.e., \(WK\)), the optimal trade-off between \(W\) and \(K\) varies by tasks: for controlled-sentiment generation, the best results come from retaining the most promising state and concentrating computational efforts on expanding from it (\(W,K=1,16\)); in contrast, for summarization, maintaining multiple hypotheses (\(W,K=8,2\)) yields the best results probably because it helps avoid local optima. Figure 5 displays the ablation results for \(L\) where smaller \(L\) benefits controlled-sentiment generation, while an intermediate \(L\) is optimal for summarization. These results are consistent with our findings from Figure 3, suggesting that the simple nature of controlled-sentiment generation makes token-level manipulation sufficient and cumulative reward mid-generation a more reliable indicator of overall return. See Appendix C.1 for extended ablations on more models.

Figure 3: **The gold reward achieved for different large pre-trained models under the gpt2 guidance. We show the mean reward (\(\) standard deviations) across three random seeds. EFT (\(^{*}\)) denotes the best EFT results among \(\{1/4,1/2,1,2,4\}\); Weak-to-strong search (\(4,4,5\)) denotes CBS with \(W,K,L=4,4,5\); BoN (\(16\)) denotes BoN with \(N=16\).**

### Instruction Following

Setup.Next, we evaluate weak-to-strong search on a standard single-turn instruction-following benchmark, AlpacaEval 2.0 , which consists of \(805\) prompts from various open-source datasets. Unlike the previous section where we steer large _pre-trained_ language models (e.g., Llama-2-7b), we now steer large _instruction-tuned_ language models (e.g., Llama-2-7b-chat). This is because (1) instruction-tuned models often require further alignment to match human preferences , and (2) to study weak-to-strong generalization in instruction-following, the models must be proficient at following instructions before steering.

For small language models, we reuse two high-ranking 7B model pairs from the AlpacaEval 2.0 leaderboard as guidance: (1) **Zephyr guidance**: zephyr-7b-beta and its untuned version mistral-7b-sft-beta; (2) **Tulu guidance**: tulu-2-dpo-7b and its untuned version tulu-2-7b. All four models use the Llama-2 tokenizer. The large instruction-tuned language models we aim to further align fall into three categories: (1) **same vocabulary**: Llama-2-7b-chat, Llama-2-70b-chat; (2) **cross vocabulary**: Llama-3-8B-Instruct, Llama-3-70B-Instruct; and (3) **black box**: gpt-3.5-turbo-instruct. As it is nearly impossible to reproduce the exact training pipeline for these small models (\(_{}^{}\)), we do not test the baseline results of directly fine-tuning the large models as in Figure 3. Language model responses are evaluated by their length-controlled win rates (LC WR) against gpt-4-turbo, with gpt-4-turbo serving as the judge.

Results.Experimental results with Zephyr and Tulu guidance are shown in Figure 6 (detailed hyperparameters in Appendix B.2.2). Weak-to-strong search consistently outperforms other test-time baselines with great margins. There are two crucial takeaways worth mentioning: **(1) Weak-to-strong search makes strong models stronger with only weak test-time guidance.** Take Zephyr guidance for an example (Figure 6, left), even if most large instruction-tuned models \(_{}\) are stronger than zephyr-7b-beta before steering, weak-to-strong search is still able to enhance their performances

Figure 4: **W, K ablations for CBS (\(\)). We show the mean rewards across three random seeds. With the same computation budget (i.e., same \(WK\)), the optimal hyperparameters differ by tasks.**

Figure 5: \(\) ablations for CBS (\(\)). We show the mean rewards (\(\) standard deviations) across three random seeds.

using weak models as guidance. Conversely, EFT and BoN mainly interpolate between weak and strong models, resulting in limited, if any, improvements over the strong models. We also tested beam search over the strong models without external guidance  but we found no obvious improvements compared with regular decoding (Table 2), probably because the latent reward functions behind these language models are not well aligned with the human preference that gpt-4-turbo approximates. The same observations apply to Tulu guidance, even though the tuned tulu-2-dpo-7b is weaker than all the large instruction-tuned language models by significant margins (Figure 6, right). **(2) Weak-to-strong search applies to black-box language models.** Our method, requiring only sampling from large language models, is also effective for black-box models like gpt-3.5-turbo-instruct. For weak-to-strong search with gpt-3.5-turbo-instruct, we use a relatively long chunk length of 100, as the black-box language model APIs are stateless and do not retain activation caches, making repeated context embedding costly. Despite the long chunk length, our method still effectively improves the alignment of black-box models, significantly outperforming BoN, a special case of weak-to-strong search (CBS) with infinite chunk length.

## 6 Discussion

We have presented weak-to-strong search, an alignment method that keeps the large language model frozen while steering its decoding through a test-time greedy search over small language models. This method builds on the insight that the log-probability difference between small tuned and untuned language models can serve both as a dense reward function and a value function, and then introduces a novel beam search algorithm designed for balancing reward maximization and KL minimization. This method offers a compute-efficient model up-scaling strategy that eliminates the complexity of directly fine-tuning the large models, and exemplifies weak-to-strong generalization  that makes strong models stronger with only weak test-time guidance. Empirically, this approach is effective in controlled-sentiment generation, summarization, and instruction following.

Limitations & Future Work.While our work focuses on aligning with human preferences, weak-to-strong search could also apply to tasks like reasoning [49; 50] and coding , where ground truth answers exist. This is because any pair of tuned and untuned language models can act as test-time steering forces, without necessarily being trained on preferences. This then raises several questions beyond the scope of our current study: (1) In our study, we consistently use SFTed policy as the untuned model \(_{}\) due to the two-stage nature of preference learning; however, in single-stage fine-tuning tasks, does weak-to-strong search still work with a pre-trained model serving as the untuned model \(_{}\)? (2) Although our method shows consistent weak-to-strong generalization across diverse alignment tasks, it is also critical to probe its potential failure modes . Can weak-to-strong search enhance language models in tasks where ground truth answers exist, beyond merely tailoring their knowledge and skills to human preferences? (3) Additionally, while our work mainly focuses on how dense language model reward function (Eq. 4) benefits language model decoding at test time, it's

Figure 6: **The length-controlled win rates against gpt-4-turbo for various instruction-tuned models under Zephyr (left) or Tulu (right) guidance.** Hyperparameters are in Appendix B.2.2.

also worth exploring the potential benefits of this reward parametrization for RL tuning. Although Appendix C.3 presents some promising preliminary results, we leave further analysis for future work.

## Author Contributions

**Zhanhui Zhou** led the project, proposed the research idea, wrote the codebase, designed the experiments, conducted most of the initial experiments, and wrote the paper. **Zhixuan Liu** assisted with running many of the ablation studies throughout the experiments presented in the paper. All other authors provided feedback throughout the project.