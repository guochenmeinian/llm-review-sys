# Why the Metric Backbone

Preserves Community Structure

 Maximilien Dreveton

EPFL

maximilien.dreveton@epfl.ch

&Charbel Chucri

EPFL

charbel.chucri@epfl.ch

&Matthias Grossglauser

EPFL

matthias.grossglauser@epfl.ch

&Patrick Thiran

EPFL

patrick.thiran@epfl.ch

###### Abstract

The _metric backbone_ of a weighted graph is the union of all-pairs shortest paths. It is obtained by removing all edges \((u,v)\) that are not on the shortest path between \(u\) and \(v\). In networks with well-separated communities, the metric backbone tends to preserve many inter-community edges, because these edges serve as bridges connecting two communities, but tends to delete many intra-community edges because the communities are dense. This suggests that the metric backbone would dilute or destroy the community structure of the network. However, this is not borne out by prior empirical work, which instead showed that the metric backbone of real networks preserves the community structure of the original network well. In this work, we analyze the metric backbone of a broad class of weighted random graphs with communities, and we formally prove the robustness of the community structure with respect to the deletion of all the edges that are not in the metric backbone. An empirical comparison of several graph sparsification techniques confirms our theoretical finding and shows that the metric backbone is an efficient sparsifier in the presence of communities.

## 1 Introduction

_Graph clustering_ partitions the vertex set of a graph into non-overlapping groups, so that the vertices of each group share some typical pattern or property. For example, each group might be composed of vertices that interact closely with each other. Graph clustering is one of the main tasks in the statistical analysis of networks .

In many scenarios, the observed pairwise interactions are weighted. In a _proximity graph_, these weights measure the degree of similarity between edge endpoints (e.g., frequency of interaction in a social network), whereas in a _distance graph_, they measure dissimilarity instead (for example, the length of segments in a road network or travel times in a flight network). To avoid confusion, we refer to the weights in a distance graph as _costs_, so that the cost of a path will be naturally defined as the sum of the edge costs on this path.1

Distance graphs obtained from real-world data typically violate the triangle inequality. More precisely, the shortest distance between two vertices in the graph is not always equal to the cost of the direct edge, but rather equal to the cost of an indirect path via other vertices. For example, the least expensiveflight between two cities is often a flight including one or more layovers. An edge whose endpoints are connected by an indirect shorter path is called _semi-metric_; otherwise, it is called _metric_.

We obtain the _metric backbone_ of a distance graph by removing all its semi-metric edges. It has been experimentally observed that the metric backbone retains only a small fraction of the original edges, typically between 10% and 30% in social networks . Properties of the original graph that depend only on the shortest paths (such as connectivity, diameter, and betweenness centrality) are preserved by its metric backbone. Moreover, experiments on contact networks indicate that other properties (such as spreading processes and community structures) are also empirically well approximated by computing them on the metric backbone, rather than on the original graph .

The preservation of these properties by the backbone highlights a well-known empirical feature of complex networks: _redundancy_. This is the basis for _graph sparsification_: the task of building a sparser graph from a given graph so that important structural properties are approximately preserved while reducing storage and computational costs. Many existing network sparsifiers identify the statistically significant edges of a graph by comparing them to a null-model . These methods typically require hyperparameters and can leave some vertices isolated (by removing all edges from a given vertex). _Spectral sparsification_ aims at preserving the spectral structure of a graph but also relies on a hyperparameter, and the edges in the sparsified graph might have different weights than in the original graph . In contrast, the metric backbone is parameter-free and automatically preserves all properties linked to the shortest path structure. Moreover, all-pairs shortest paths can be efficiently computed . This makes the metric backbone an appealing mechanism for sparsification.

Among all the properties empirically shown to be well preserved by the metric backbone, the community structure is perhaps the most surprising. Indeed, if a network contains dense communities that are only loosely connected by a small number of inter-community edges, then all shortest paths between vertices in different communities must go through one of these edges. This suggests that these "bridge" edges are less likely to be semi-metric than intra-community edges, where the higher density provides shorter alternative paths.2 This in turn implies that metric sparsification should thin out intra-community edges more than inter-community edges, thereby diluting the community structure. The central contribution of this paper is to show that this intuition is wrong.

To do so, we formally characterize the metric backbone of a weighted stochastic block model (wSBM). We assume that the vertices are separated into \(k\) blocks (also referred to as clusters or communities). An edge between two vertices belonging to blocks \(a\) and \(b\) is present with probability \(p_{ab}\) and is independent of the presence or absence of other edges. This edge, if present, has a weight sampled from a distribution with cdf \(F_{ab}\), and this weight represents the cost of traveling through this edge. We denote by \(p_{ab}^{}\) the probability that an edge between a vertex in block \(a\) and a vertex in block \(b\) is present in the backbone.3 Under loose assumptions on the costs distributions \(F_{ab}\) and on the probabilities \(p_{ab}\), we show that \(p_{ab}^{}/p_{cd}^{}=(1+o(1))p_{ab}/p_{cd}\) for every \(a,b,c,d[k]\). This shows that the metric backbone thins the edge set approximately uniformly and, therefore, preserves the community structure of the original graph. Moreover, we also prove that a spectral algorithm recovers almost exactly the communities.

We also conduct numerical experiments with several graph clustering algorithms and datasets to back up these theoretical results. We show that all clustering algorithms achieve similar accuracy on the metric backbone as on the original network. These simulations, performed on different types of networks and with different clustering algorithms, generalize the experiments of , which are restricted to contact networks and the Louvain algorithm. Another type of graph we consider are graphs constructed from data points in a Euclidean space, typically by a kernel similarity measure between \(q\)-nearest neighbors (\(q\)-NN) . This is an important technique for graph construction, with applications in non-linear embedding and clustering. Although \(q\) controls the sparsity of this embedding graph, this procedure is not guaranteed to produce only metric edges, and varying \(q\) often impacts the clustering performance. By investigating the metric backbone of \(q\)-NN graphs, we notice that it makes clustering results more robust against the value of \(q\). Consequently, leveraging graph sparsification alongside \(q\)-NN facilitates graph construction.

The paper is structured as follows. We introduce the main definitions and theoretical results on the metric backbone of wSBMs in Section 2. We discuss these results in Section 3 and compare them with the existing literature. Sections 4 and 5 are devoted to numerical experiments and applications. Finally, we conclude in Section 6.

Code availabilityWe provide the code used for the experiments: https://github.com/Charbel-11/Why-the-Metric-Backbone-Preserves-Community-Structure

NotationsThe notation \(1_{n}\) denotes the vector of size \(n 1\) whose entries are all equal to one. For any vector \(^{n}\) and any matrix \(B^{n m}\), we denote by \(_{}=_{a[n]}_{a}\) and \(B_{}=_{a,b}B_{ab}\), and similarly for \(_{}\) and \(B_{}\). Given two matrices \(A\) and \(B\) of the same size, we denote by \(A B\) the entry-wise matrix product (_i.e.,_ their Hadamard product). \(A^{T}\) is the transpose of a matrix \(A\). For a vector \(\), we denote \(()\) the diagonal matrix whose diagonal element \((a,a)\) is \(_{a}\).

The indicator of an event \(A\) is denoted \(1\!\{A\}\). Binomial and exponential random variables are denoted by \((n,p)\) and \(()\), respectively. The uniform distribution over an interval \([a,b]\) is denoted \((a,b)\). Finally, given a cumulative distribution function \(F\), we write \(X F\) for a random variable \(X\) sampled from \(F\), _i.e.,_\((X x)=F(x)\), and we denote by \(f\) the pdf of this distribution. We write whp (with high probability) for events with probability tending to 1 as \(n\).

## 2 The Metric Backbone of Weighted Stochastic Block Models

### Definitions and Main Notations

Let \(G=(V,E,c)\) be an undirected weighted graph, where \(V=[n]\) is the set of vertices, \(E V V\) is the set of undirected edges, and \(c E_{+}\) is the cost function.The _cost_ of a path \((u_{1},,u_{p})\) is naturally defined as \(_{q=1}^{p-1}c(u_{q},u_{q+1})\), and a _shortest path_ between \(u\) and \(v\) is a path of minimal cost starting from vertex \(u\) and finishing at vertex \(v\). We define the _metric backbone_ as the union of all shortest paths of \(G\).

**Definition 1**.: The _metric backbone_ of a weighted graph \(G=(V,E,c)\) where \(c\) represents the edge cost is the subgraph \(G^{}=(V,E^{},c^{})\) of \(G\), where \(E^{} E\) is such that \(e E^{}\) if and only if \(e\) belongs to a shortest path from two vertices in \(G\), and \(c^{} E^{}_{+};e c(e)\) is the restriction of \(c\) to \(E^{}\).

We investigate the structure of the metric backbone of weighted random graphs with community structure. We generate these graphs as follows. Each vertex \(u[n]\) is randomly assigned to the cluster \(a[k]\) with probability \(_{a}\). We denote by \(z_{u}[k]\) the cluster of vertex \(u\). Conditioned on \(z_{u}\) and \(z_{v}\), an edge \((u,v)\) is present with probability \(p_{z_{u}z_{v}}\), independently of the presence or absence of other edges. If an edge \((u,v)\) is present, it is assigned a cost \(c(u,v)\). The cost \(c(u,v)\) is sampled from \(F_{z_{u}z_{v}}\) where \(F=(F_{ab})_{1 a,b k}\) denotes a collection of cumulative distribution functions such that \(F_{ab}=F_{ba}\). This defines the _weighted stochastic block model_, and we denote \((z,G)(n,,p,F)\) with \(G=([n],E,c)\), \(z[k]^{n}\) and

\[(z) = _{u=1}^{n}_{z_{u}},\] \[(E\,|\,z) = _{1 u<v n}p_{z_{u}z_{v}}^{1\{(u,v) E\}}(1-p_{ z_{u}z_{v}})^{1\{(u,v) E\}},\] \[(c\,|\,E,z) = _{\{u,v\} E}(c(u,v)\,|\,z_{u},z_{v}),\]

and \(c(u,v)\,|\,z_{u}=a,z_{v}=b\) is sampled from \(F_{ab}\).

The wSBM is a direct extension of the standard (non-weighted) SBM into the weighted setting. SBM is the predominant benchmark for studying community detection and establishing theoretical guarantees of recovery by clustering algorithms . Despite its shortcomings, such as a low clustering coefficient, the SBM is analytically tractable and is a useful model for inference purposes .

Throughout this paper, we will make the following assumptions.

**Assumption 1** (Asymptotic scaling).: The edge probabilities \(p_{ab}\) between blocks \(a\) and \(b\) can depend on \(n\), such that \(p_{ab}=B_{ab}_{n}\) with \(_{n}=(n^{-1} n)\) and \(B_{ab}\) is independent of \(n\). Furthermore, the number of communities \(k\), the matrix \(B\), the probabilities \(_{a}\) and the cdf \(F_{ab}\) are all fixed (independent of \(n\)). We also assume \(_{}>0\) and \(B_{}>0\).

To rule out some issues, such as edges with a zero cost,4 we also assume that the probability distributions of the costs have no mass at \(0\). More precisely, we require that the cumulative distribution functions \(F_{ab}\) verify \(F_{ab}(0)=0\) and \(F^{}_{ab}(0)=_{ab}>0\), where \(F^{}\) denotes the derivative of \(F\) (_i.e.,_ the pdf). The first condition ensures that the distribution has support \(_{+}\) and no mass at \(0\), and the second ensures that, around a neighborhood of \(0\), \(F_{ab}\) behaves as the exponential distribution \((_{ab})\) (or as the uniform distribution \(([0,_{ab}^{-1}])\)).

**Assumption 2** (Condition on the cost distributions).: The costs are sampled from continuous distributions, and there exists \(=(_{ab})_{a,b}\) with \(_{ab}>0\) such that \(F_{ab}(0)=0\) and \(F^{}_{ab}(0)=_{ab}\).

We define the following matrix

\[T\;=\;[ B](),\] (2.1)

where \(B\) and \(\) are defined in Assumptions 1 and 2. Finally, we denote by \(_{}\) and \(_{}\) the minimum and maximum entries of the vector \(=T1_{k}\).

**Remark 1**.: Assume that \(= 1_{k}1_{k}^{T}\) with \(>0\). We notice that \(_{a}=_{b}_{b}B_{ab}\). Denote by \(=(_{1},,_{k})\) the vector whose \(a\)-entry \(_{a}\) is the expected degree of a vertex in community \(a\). We have \(_{a}=n_{b}_{b}p_{ab}\). Then \(_{}= d_{}(n_{n})^{-1}\) and \(_{}=_{}(n_{n})^{-1}\), where \(_{}\) and \(_{}\) are the minimum and maximum entries of \(\).

### Cost of Shortest Paths in wSBMs

Given a path \((u_{1},,u_{p})\), recall that its cost is \(_{q=1}^{p-1}c(u_{q},u_{q+1})\), and the _hop count_ is the number of edges composing the path (that is, the hop count of \((u_{1},,u_{p})\) is \(p-1\)).

For two vertices \(u,v V\), we denote by \(C(u,v)\) the cost of the shortest path from \(u\) to \(v\).5 The following proposition provides asymptotics for the cost of shortest paths in wSBMs.

**Proposition 1**.: _Let \((z,G)(n,,p,F)\). Suppose that Assumptions 1 and 2 hold and let \(_{}\) and \(_{}\) be defined following Equation (2.1). Then, for two vertices \(u\) and \(v\) chosen uniformly at random in blocks \(a\) and \(b\), respectively. We have whp_

\[(_{})^{-1}\;\;}{ n}C(u,v)\;\;(_{}) ^{-1}\,.\]

To prove Proposition 1, in the first stage, we simplify the problem by assuming exponentially distributed weights. We then analyze two first passage percolation (FPP) processes, originating from vertices \(u\) and \(v\), respectively. Using the memoryless property of the exponential distribution, we analyze two first passage percolation (FPP) processes, originating from vertex \(u\) and \(v\), respectively. Each FPP explores the nearest neighbors of its starting vertex until it reaches \(q\) neighbors. As long as \(q=o()\), the two FPP processes remain disjoint (with high probability). Thus, the cost \(C(u,v)\) is lower-bounded by the sum of (i) the cost of the shortest path from \(u\) to its \(q\)-nearest neighbor and of (ii) the cost of the shortest path from \(v\) to its \(q\)-nearest neighbor. On the contrary, when \(q=()\), the two FPPs intersect, revealing a path from \(u\) to \(v\), and the cost of this path upper-bounds the cost \(C(u,v)\) of the shortest path from \(u\) to \(v\). In the second stage, we extend the result to general weight distributions by noticing that the edges belonging to the shortest paths have very small costs. Moreover, Assumption 2 yields that the weight distributions behave as an exponential distribution in a neighborhood of \(0\). We can thus adapt the coupling argument of  to show that the edge weights distributions do not need to be exponential, as long as Assumption 2 is verified. The proof of Proposition 1 is provided in Section A.

### Metric Backbone of wSBMs

Let \((z,G)(n,,p,F)\) and denote by \(G^{}\) the metric backbone of \(G\). Choose two vertices \(u\) and \(v\) uniformly at random, and notice that the probability that the edge \((u,v)\) is present in \(G^{}\) depends only on \(z_{u}\) and \(z_{v}\), and not on \(z_{w}\) for \(w\{u,v\}\). Denote by \(p^{}_{ab}\) the probability that an edge between a vertex in community \(a\) and a vertex in community \(b\) appears in the metric backbone, _i.e.,_

\[p^{}_{ab}\ =\ ((u,v) G^{}\,|\,z_{u}=a,z_ {v}=b).\]

The following theorem shows that the ratio \(}_{ab}}{p^{}_{ab}}\) scales as \((})\).

**Theorem 1**.: _Let \((z,G)\ \ (n,,p,F)\) and suppose that Assumptions 1 and 2 hold. Let \(_{}\) and \(_{}\) be defined after Equation (2.1). Then_

\[(1+o(1))}{_{}}\ \ }{ n }}_{ab}}{p_{ab}}\ \ (1+o(1))}{_{}}.\]

We prove Theorem 1 in Appendix B.1. Theorem 1 shows that the metric backbone maintains the same proportion of intra- and inter-community edges as in the original graph. We illustrate the theorem with two important examples.

**Example 1**.: Consider a weighted version of the planted partition model, where for all \(a,b[k]\) we have \(_{a}=1/k\) and

\[B_{ab}\ =\ p_{0}&a=b,\\ q_{0}&\]

where \(p_{0}\) and \(q_{0}\) are constant. Assume that \(= 1_{k}1_{k}^{T}\) with \(>0\). Using Remark 1, we have \(_{}=_{}= k^{-1}\,(p_{0}+(k-1)q_{0})\), and Theorem 1 states that

\[p^{}\ =\ (1+o(1))}{p_{0}+(k-1)q_{0}}  q^{}\ =\ (1+o(1))}{p_{0}+(k-1)q_{0}}.\]

In particular, \(}}{q^{}}\ =\ (1+o(1))}{q_{0}}\).

**Example 2**.: Consider a stochastic block model with edge probabilities \(p_{ab}=B_{ab}_{n}\) such that the vertices of different communities have the same expected degree \(\). If \(= 1_{k}1_{k}^{T}\), then

\[p^{}_{ab}\ =\ (1+o(1))}{}.\]

### Recovering Communities from the Metric Backbone

In this section, we prove that a spectral algorithm on the (weighted) adjacency matrix of the metric backbone of a wSBM asymptotically recovers the clusters whp. Given an estimate \([k]^{n}\) of the clusters \(z[k]^{n}\), we define the loss as

\[(z,)\ =\ _{(k)}(z,),\]

where \(\) denotes the Hamming distance and \((k)\) is the set of all permutations of \([k]\).

``` Input: Graph \(G\), number of clusters \(k\) Output: Predicted community memberships \([k]^{n}\)
1 Denote \(W^{}^{n n}_{+}\) the weighted adjacency matrix of the metric backbone \(G^{}\) of \(G\)
2 Let \(W^{}=_{i=1}^{n}_{i}u_{i}u_{i}^{T}\) be an eigen-decomposition of \(W^{}\), with eigenvalues ordered in decreasing absolute value (\(|_{1}||_{n}|\)) and eigenvectors \(u_{1},,u_{n}^{n}\)
3 Denote \(U=[u_{1},,u_{k}]^{n k}\) and \(=(_{1},,_{k})\)
4 Let \([k]^{n}\) be a \((1+)\)-approximate solution of \(k\)-means performed on the rows of \(U^{n k}\) ```

**Algorithm 1**Spectral Clustering on the weighted adjacency matrix of the metric backbone

The following theorem states that, as long as the matrix \(T\) defined in (2.1) is invertible, the loss of spectral clustering applied on the metric backbone asymptotically vanishes whp.

**Theorem 2**.: _Let \((z,G)\;\;(n,,p,F)\) and suppose that Assumptions 1 and 2 hold. Let \(\) be the minimal absolute eigenvalue of the matrix \(T\) defined in (2.1). Moreover, assume that \(_{}=_{}\) and \( 0\). Then, the output \(\) of Algorithm 1 on \(G\) verifies whp_

\[(z,)\;=\;O(\; n}).\]

We prove Theorem 2 in Appendix B.2. We saw in Example 1 and 2 that the condition \(_{}=_{}\) is verified in several important settings. The additional assumption \( 0\) (equivalent to \(T\) being invertible) also often holds: in the planted partition model of Example 1, \(T\) is invertible if \(p_{0} q_{0}\).

## 3 Comparison with Previous Work

The metric backbone has been introduced under different names, such as the _essential subgraph_, the _transport overlay network_, or simply the _union of shortest path trees_. In this section, we discuss our contribution with respect to closely related earlier works.

### Computing the Metric Backbone

Computing the metric backbone requires solving the All Pairs Shortest Path (APSP) problem, a classic and fundamental problem in computer science. Simply running Dijkstra's algorithm on each vertex of the graph solves the APSP in \(O(nm+n^{2} n)\) worst-case time, where \(m=|E|\) is the number of edges in the original graph , whereas  proposed an algorithm running in \(O(nm^{}+n^{2} n)\) worst-case time, where \(m^{}\) is the number of edges in the metric backbone. APSP has also been studied in weighted random graphs in which the weights are independent and identically distributed [20; 16]. In particular, the APSP can be solved in \(O(n^{2})\) time with high probability on complete weighted graphs whose weights are drawn independently and uniformly at random from \(\).

However, practical implementations of APSP can achieve faster results. For example, in , an empirical observation regarding the low hop count of shortest paths is leveraged to compute the metric backbone efficiently. Although exact time complexity is not provided, the implementation scales well for massive graphs, such as a Facebook graph with 190 million nodes and 49.9 billion edges, and the empirical running time appears to be linear with the number of edges [24, Table 1 and Figure 5]. Additionally, our simulations reveal that some popular clustering algorithms such as spectral and subspace clustering have higher running times than computing the metric backbone.

### First-Passage Percolation in Random Graphs

To study the metric backbone theoretically, we need to understand the structure of the shortest path between vertices in a random graph. This classical and fundamental topic of probability theory is known as first-passage percolation (FPP) . The paper  originally studied the weights and hop counts of shortest paths on the complete graph with iid weights. This was later generalized to Erdos-Renyi graphs and configuration models (see, for example, [28; 12] and references therein).

Closer to our setting,  studied the FPP on inhomogeneous random graphs. Indeed, SBMs are a particular case of inhomogeneous random graphs, for which the set of vertex types is discrete (we refer to  for general statements on inhomogeneous random graphs). Assuming that the edge weights are independent and \(()\)-distributed,  established a central limit theorem of the weight and hop count of the shortest path between two vertices chosen uniformly at random among all vertices. Using the notation of Section 2, this result implies that \(on}}{ n}C(u,v)\) converges in probability to \(^{-1}\), where \(\) is the Perron-Frobenius eigenvalue of \( B()\).

The novelty of our work is two-fold. First, we allow different cost distributions for each pair of communities, whereas all previous works in FPP on random graphs assume that the costs are sampled from a single distribution. Furthermore, we examine the cost of a path between two vertices, \(u\) and \(v\), chosen _uniformly at random among vertices in block \(a\) and in block \(b\)_, respectively. This differs from previous work (and, in particular, ) in which vertices \(u\) and \(v\) are _selected uniformly at random among all vertices_. As a result, even for a single cost distribution, Proposition 1 cannot be obtained directly from [25, Theorem 1.2]. This difference is key, as this proposition is required to establish Theorem 1.

The closest result to Theorem 1 appearing in the literature is [39, Corollary 1]; it establishes a formula for the probability \(p_{uv}^{}\) that an edge between two vertices \(u\) and \(v\) exists in the metric backbone of a random graph whose edge costs are iid. This previous work does not focus on community structure, so the costs are sampled from a single distribution. More importantly, the expression of \(p_{uv}^{}\) given by [39, Theorem 2 and Corollary 1] is mainly of theoretical interest (and we use it in the proof of Theorem 1). Indeed, understanding the asymptotic behavior of \(p_{uv}^{}\) requires a complete analysis of the cost \(C(u,v)\) of the shortest path between \(u\) and \(v\).  propose such an analysis only in one simple scenario (namely, a complete graph with iid exponentially distributed costs).

## 4 Experimental Results

In this section, we test whether the metric backbone preserves a graph community structure in various real networks for which a ground truth community structure is known (see Table 2 in Appendix C.1 for an overview).

As in many weighted networks, such as social networks, the edge weights represent a measure (e.g., frequency) of interaction between two entities over time, we need to preprocess these proximity graphs into distance graphs. More precisely, given the original (weighted or unweighted) graph \(G=(V,E,s)\), where the weights measure the similarities between pairs of vertices, we define the proximity \(p(u,v)\) of vertices \(u\) and \(v\) as the _weighted Jaccard similarity_ between the neighborhoods of \(u\) and \(v\), _i.e.,_

\[p(u,v)\;=\;(u)(v)}\{s(u,w),s( v,w)\}}{_{u(u)(v)}\{s(u,w),s(v,w)\}},\]

where \((u)=\{w V(u,w) E\}\) denotes the neighborhood of \(u\), _i.e.,_ the vertices connected to \(u\) by an edge. If \(G\) is unweighted (\(s(e)=1\) for all \(e E\)), we simply recover the Jaccard index \((u)(v)|}{|(u)( v)|}\). We note that other choices for normalization could have been made, such as the Adamic-Adar index . We refer to  for an in-depth discussion of similarity and distance indices.

Once the proximity graph \(G=(V,E,p)\) has been computed, we construct the distance graph \(D=(V,E,c)\) where \(c E_{+}\) is such that

\[ e E\,:\;c(e)\;=\;-1.\]

This is the simplest and most commonly used method for converting a similarity to a distance .

We then compute the set \(E^{}\) of metric edges of the distance graph \(D\), and let \(G^{}=(V,E^{},p^{})\) where \(p^{} E^{};e p(e)\) is the restriction of \(p\) to \(E^{}\).

We will also consider the two following sparsifications (with the corresponding restrictions of \(c\) to the sparsified edge sets) to compare the resulting community structure:

* the _threshold graph_\(G^{}=(V,E^{},p^{})\), where an edge \(e E\) is kept in \(E^{}\) iff \(p(e)\);
* the graph \(G^{}=(V,E^{},p^{})\) obtained by _spectral sparsification_ on \(G\). We use the Spielman-Srivastava sparsification , implemented in the PyGSP package .

For both threshold and spectral sparsification, we tune the hyperparameters so that the number of edges kept is the same as in the metric backbone: \(|E^{}|=|^{}|=|E^{}|\). We provide in Table 3 (in Appendix) some statistics on the percentage of edges remaining in the sparsified graphs.

For each proximity graph \(G\) and its three sparsifications \(G^{}\), \(G^{}\), and \(G^{}\), we run a graph clustering algorithm to obtain the respective predicted clusters \(\), \(^{}\), \(^{}\) and \(^{}\). We show, in Figure 1, the _adjusted Rand index_6 (ARI) obtained between the ground truth communities and the predicted clusters for three widely used graph clustering algorithms: _Bayesian_ algorithm , _Leiden_ algorithm  and _spectral clustering_. We use the _graph-tool_ implementation for the _Bayesian_ algorithm, with an exponential prior for the weight distributions. The _Leiden_ algorithm is implemented at https://github.com/vtraag/leidenalg. For _spectral clustering_, we assume that the algorithm knows the correct number of clusters in advance, and we use the implementation from _scikit-learn_.7

We highlight the difference between the metric backbone and the threshold subgraph of the _Primary school_ data set in Figure 2. We observe, in Figure 1(a), that the edges in red (which are present in the backbone but not in the threshold graph) are mostly inter-community edges. On the contrary, in Figure 1(b), the blue edges (which are present in the threshold graph but not in the backbone) are mostly intra-community edges. Despite this difference, the metric backbone retains the information about the community structure, as shown in Figure 1.

## 5 Application to Graph Construction Using \(q\)-NN

In a large number of machine-learning tasks, data does not originally come from a graph structure, but instead from a cloud of points \(x_{1},,x_{n}\) where each \(x_{u}\) belongs to a metric space (say \(^{d}\) for simplicity). The goal of _graph construction_ is to discover a proximity graph \(G=([n],E,p)\) from the original data points. Graph construction is commonly done through the \(q\)-nearest neighbors (\(q\)-NN). Given a similarity function \(:^{d}^{d}_{+}\) that quantifies the resemblance between two data points, we define the set \((u,q)\) of \(q\)-nearest neighbors of \(u[n]\). More precisely, \((u,q)\) is the subset of \([n]\{u\}\) with cardinality \(q\) such that for all \(v(u,q)\) and for all \(w(u,q)\) we have

\[(x_{u},x_{v})\ \ (x_{u},x_{w}).\]

Figure 1: Effect of sparsification on the performance of clustering algorithms on various data sets. We observe that the metric backbone and the spectral sparsification retain equally well the community structure across all data sets and for all clustering algorithms tested. Thresholding often yields several disconnected components of small sizes, impacting the performance of clustering algorithms on \(G^{}\).

Figure 2: Graphs obtained from _Primary school_ data set, after taking the metric backbone (Figure 1(a)) and after thresholding (Figure 1(b)), are drawn using the same layout. Vertex colors represent the true clusters. Edges present in the metric backbone but not in the threshold graph are highlighted in red. Edges present in the threshold graph, but not in the metric backbone, are highlighted in blue.

The edge set \(E\) is composed of all pairs of vertices \((u,v)\) such that \(u(v,q)\) or \(v(u,q)\),8 and the proximity \(p_{uv}\) associated with the edge \((u,v)\) is \((s_{uv}+s_{vu})/2\), where

\[s_{uv} =(x_{u},x_{v})&v (u,q),\\ 0&\]

In the following, we use the Gaussian kernel similarity \((x_{u},x_{v})=(--x_{v}\|^{2}}{d_{K}^{ 2}(x_{u})})\), where \(d_{K}(x_{i})\) is the Euclidean distance between \(x_{u}\) and its \(q\)-NN. In Appendix C.2, we provide results using another similarity measure.

We investigate the effect of graph sparsification on graphs built by \(q\)-NN. We sample \(n=10000\) images from MNIST  and FashionMNIST , and use the full HAR  dataset (\(n=10299\)). From the sampled data points, we build the \(q\)-NN graph \(G_{q}\), as well as its _metric backbone_\(G_{q}^{}\) and its _spectral sparsification_\(G_{q}^{}\). We then compare the performance of two clustering algorithms, _spectral clustering_ and _Poisson learning_. _Poisson learning_ is a semi-supervised graph clustering algorithm and was recently shown to outperform other graph-based semi-supervised algorithms . Results of spectral clustering using another similarity measure are presented in the Appendix.

We compare the ARI of clustering algorithms on \(q\)-NN graphs and its sparsifications (metric backbone and spectral sparsification) for various choices of the number of nearest neighbors \(q\). The results are shown in Figure 3 (for spectral clustering) and Figure 4 (for Poisson-learning). Unlike the spectral sparsifier, the metric backbone retains a high ARI across all choices of \(q\). Interestingly, the performance on the original graph often decreases with \(q\), which is a hyperparameter of the graph construction step. Applying a clustering algorithm on the metric backbone comes with the two advantages of significantly reducing the number of edges in the graph and of making its performance robust against the choice of the hyperparameter \(q\). Indeed, a larger value of \(q\) creates more edges but with a higher distance (cost), which are therefore more likely to be non-metric.

Finally, we compare in Table 1 the ARI obtained on the \(q\)-nearest neighbor graph using \(q=10\) (as it is a common default choice) with the metric backbone graph of a \(q\)-nearest neighbor graph with

Figure 4: Performance of _Poisson learning_ on subsets of MNIST, FashionMNIST datasets, and the HAR dataset. The ARI is averaged over 100 trials, and error bars show the standard error of the mean.

Figure 3: Performance of _spectral clustering_ on subsets of MNIST, FashionMNIST datasets, and on the HAR dataset. The ARI is averaged over 10 trials; error bars show the standard error of the mean.

\(q=\). Moreover, we compute an approximation of the metric backbone by sampling uniformly at random \(2 n\) vertices and taking the union of the \(2 n\) shortest-path trees rooted at each one of them instead of the union of all \(n\) shortest-path trees. This produces a graph \(_{/2}^{}\) whose edge set is a subset of the edges of the true metric backbone \(G_{/2}^{}\). We observe that \(_{/2}^{}\) retains the community structure albeit being typically twice sparser than the \(10\)-nearest neighbor graph \(G_{10}\).

Additional discussionThe performance of clustering algorithms on the \(q\)-nearest neighbor graph \(G_{q}\) tends to decrease when \(q\) increases. Indeed, a larger \(q\) introduces many low-similarity edges, which can act as noise. Spectral sparsification preserves the spectral properties of the graph, but this becomes ineffective if the spectral properties of \(G_{q}\) are insufficient to recover the communities (as attested by the poor performance of spectral clustering for large values of \(q\) in Figures 2(a) and 7(a)). However, when the performance of spectral clustering on \(G_{q}\) remains stable as \(q\) is varied, so does the performance of spectral clustering on the spectral sparsified graph \(G_{q}^{}\) (as seen in Figures 2(b) and 7(b)). In contrast, the metric backbone preserves the shortest paths rather than spectral properties. Because the shortest paths are robust to the addition of numerous low-similarity edges,9 the performance of clustering algorithms on the metric backbone \(G_{q}^{}\) remains stable when \(q\) increases. This holds regardless of whether the performance on the original graph \(G_{q}\) is stable or decreases with increasing \(q\). Finally, sparsified graphs obtained by metric sparsification are more consistent across different values of \(q\) than those obtained via spectral sparsification. For instance, on the MNIST dataset with Gaussian kernel similarity, the metric backbone \(G_{30}^{}\) and \(G_{40}^{}\) have both approximately 70k edges, with 64k edges in common. In contrast, the spectrally sparsified graphs \(G_{30}^{}\) and \(G_{40}^{}\), also with around 70k edges each, share only 22k edges in common.

## 6 Conclusion

The metric backbone plays a crucial role in preserving several essential properties of a network. Notably, the metric backbone effectively preserves the network community structure, although many inter-community edges belong to shortest paths. In this paper, we have specifically proven that the metric backbone preserves the community structure in weighted stochastic block models. Moreover, our numerical experiments emphasize the performance of the metric backbone as a powerful graph sparsification tool. Furthermore, the metric backbone can serve as a preprocessing step for graph construction employing \(q\)-nearest neighbors, alleviating the sensitivity associated with selecting the hyperparameter \(q\) and producing sparser graphs.

   Algorithm & Data set & & \(G_{10}\) & \(_{}^{}\) \\   &  & ARI & \(0.533\) & \( 0.011\) \\  & & Edges & \(550,653\) & \(373,379 3,018\) \\  & & ARI & \(0.411\) & \( 0.006\) \\  & & Edges & \(578,547\) & \(272,063 857\) \\  & & ARI & \(\) & \(0.492 0.001\) \\  & & Edges & \(77,526\) & \(37,535 977\) \\   &  & ARI & \(0.814 0.015\) & \( 0.021\) \\  & & Edges & \(550,653\) & \(373,379 3,018\) \\   & & ARI & \(0.526 0.010\) & \( 0.014\) \\   & & Edges & \(578,547\) & \(272,063 857\) \\   & & ARI & \(0.618 0.039\) & \( 0.037\) \\   & & Edges & \(77,526\) & \(37,535 977\) \\   

Table 1: Comparison of clustering on the _full_ data sets. \(G_{10}\) denotes the \(10\)-nearest neighbors graph, and \(_{/2}^{}\) denotes the approximate metric backbone of the \(/2\)-nearest neighbor graph. We approximate the metric backbone by sampling only \(2 n\) shortest-path trees.