# When Do Transformers Shine in RL?

Decoupling Memory from Credit Assignment

Tianwei Ni

Mila, Universite de Montreal

tianwei.ni@mila.quebec

Michel Ma

Mila, Universite de Montreal

michel.ma@mila.quebec

Benjamin Eysenbach

Princeton University

eysenbach@princeton.edu

Pierre-Luc Bacon

Mila, Universite de Montreal

pierre-luc.bacon@mila.quebec

###### Abstract

Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The Transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capability of RL algorithms, scaling up to tasks that require memorizing observations \(1500\) steps ago. However, Transformers do not improve long-term credit assignment. In summary, our results provide an explanation for the success of Transformers in RL, while also highlighting an important area for future research and benchmark design. Our code is open-sourced1.

## 1 Introduction

In recent years, Transformers (Vaswani et al., 2017; Radford et al., 2019) have achieved remarkable success in domains ranging from language modeling to computer vision. Within the RL community, there has been excitement around the idea that large models with attention architectures, such as Transformers, might enable rapid progress on challenging RL tasks. Indeed, prior works have shown that Transformer-based methods can achieve excellent results in offline RL (Chen et al., 2021; Janner et al., 2021; Lee et al., 2022), online RL (Parisotto et al., 2020; Lampinen et al., 2021; Zheng et al., 2022; Melo, 2022; Micheli et al., 2022; Robine et al., 2023), and real-world tasks (Ouyang et al., 2022; Ichter et al., 2022) (see Li et al. (2023); Agarwal et al. (2023) for the recent surveys).

However, the underlying reasons why Transformers achieve excellent results in the RL setting remain a mystery. Is it because they learn better representations of sequences, akin to their success in computer vision and NLP tasks? Alternatively, might they be internally implementing a learned algorithm, one that performs better credit assignment than previously known RL algorithms? At the core of these questions is the fact that RL, especially in partially observable tasks, requires two distinct forms of temporal reasoning: (working) **memory** and (temporal) **credit assignment**. Memory refers to the ability to recall a distant past event at the current time (Blankenship, 1938; Dempster, 1981),while credit assignment is the ability to determine _when_ the actions that deserve current credit occurred (Sutton, 1984). These two concepts are also loosely intertwined - learning what bits of history to remember depends on future rewards, and learning to assign current rewards to previous actions necessitates some form of (episodic) memory (Zilli and Hasselmo, 2008; Gershman and Daw, 2017).

Inspired by the distinct nature of memory and credit assignment in temporal dependencies, we aim to understand which of these two concepts Transformers address in the context of RL. Despite empirical success in prior works, directly answering the question is challenging due to two issues. First, many of the benchmarks used in prior works, such as Atari (Bellemare et al., 2013) and MuJoCo (Todorov et al., 2012), require minimal memory and short-term credit assignment, because they closely resemble MDPs and their rewards are mostly immediate. Other benchmarks, such as Key-to-Door (Hung et al., 2018; Mesnard et al., 2021), entangle medium-term memory and credit assignment. Second, there is a lack of rigorous definition of memory and credit assignment in RL (Osband et al., 2020). The absence of quantifiable measures creates ambiguity in the concept of "long-term" often found in prior works.

In this paper, we address both issues concerning memory and credit assignment to better understand Transformers in RL. First, we provide a mathematical definition of memory lengths and credit assignment lengths in RL, grounded in common understanding. We distinguish the memory lengths in reward, transition, policy, and value, which relate to the minimal lengths of recent histories needed to preserve the corresponding quantities. Crucially, our main theoretical result reveals that the memory length of an optimal policy can be upper bounded by the reward and transition memory lengths. For credit assignment, we adopt the forward view and define its length as the minimal number of future steps required for a greedy action to start outperforming any non-greedy counterpart.

The definitions provided are not only quantitative in theory but also actionable for practitioners, allowing them to analyze the memory and credit assignment requirements of many existing tasks. Equipped with these tools, we find that many tasks designed for evaluating memory also evaluate credit assignment, and vice versa, as demonstrated in Table 1. As a solution, we introduce toy examples called Passive and Active T-Mazes that decouple memory and credit assignment. These toy examples are configurable, enabling us to perform a scalable unit-test of a sole capability, in line with best practice (Osband et al., 2020).

Lastly, we evaluate memory-based RL algorithms with LSTMs or Transformers on our configurable toy examples and other challenging tasks to address our research question. We find that Transformers can indeed boost long-term memory in RL, scaling up to memory lengths of \(1500\). However, Transformers do not improve long-term credit assignment, and struggle with data complexity in even short-term dependency tasks. While these results suggest that practitioners might benefit from using Transformer-based architectures in RL, they also highlight the importance of continued research on core RL algorithms. Transformers have yet to replace RL algorithm designers.

## 2 Measuring Temporal Dependencies in RL

MDPs and POMDPs.In a partially observable Markov decision process (POMDP) \(_{O}=(,,P,R,,T)\)2, an agent receives an observation \(o_{t}\) at step \(t\{1,,T\}\), takes an action \(a_{t}\) based on the observed history \(h_{1:t}:=(o_{1:t},a_{1:t-1})_{t}\)3, and receives a reward \(r_{t} R_{t}(h_{1:t},a_{t})\) and the next observation \(o_{t+1} P( h_{1:t},a_{t})\). The initial observation \(h_{1:1}:=o_{1}\) follows \(P(o_{1})\). The total horizon is \(T^{+}\{+\}\) and the discount factor is \(\) (less than \(1\) for infinite horizon). In a Markov decision process (MDP) \(_{S}=(,,P,R,,T)\), the observation \(o_{t}\) and history \(h_{1:t}\) are replaced by the state \(s_{t}\). In the most generic case, the agent is composed of a policy \((a_{t} h_{1:t})\) and a value function \(Q^{}(h_{1:t},a_{t})\). The optimal value function \(Q^{*}(h_{1:t},a_{t})\) satisfies \(Q^{*}(h_{1:t},a_{t})=[r_{t} h_{1:t},a_{t}]+_{o_{ t+1} P( h_{1:t},a_{t})}_{a_{t+1}}Q^{*}(h_{1:t+1},a_{t+1}) \) and induces a deterministic optimal policy \(^{*}(h_{1:t})=*{argmax}_{a_{t}}Q^{*}(h_{1:t},a_{t})\). Let \(^{*}_{}\) be the space of all optimal policies for a POMDP \(\).

Below, we provide definitions for context-based policy and value function, as well as the sum of \(n\)-step rewards given a policy. These concepts will be used in defining memory and credit assignment lengths.

Context-based policy and value function.A context-based policy \(\) takes the recent \(l_{}()\) observations and actions as inputs, expressed as \((a_{t} h_{t-l_{}()+1:t})\). Here, \(l_{}()\) represents the **policy context length**. For simplicity, we refer to context-based policies as "policies". Let \(_{k}\) be the space of all policies with a context length of \(k\). Markovian policies form \(_{1}\), while \(_{T}\), the largest policy space, contains an optimal policy. Recurrent policies, recursively taking one input at a time, belong to \(_{}\); while Transformer-based policies have a limited context length.

**Context-based value function**\(Q_{n}^{}\) of a POMDP \(\) is the expected return conditioned on the past \(n\) observations and actions, denoted as \(Q_{n}^{}(h_{t-n+1:t},a_{t})=_{,}_{i=t} ^{i-t}r_{i} h_{t-n+1:t},a_{t}\). By definition, \(Q^{}(h_{1:t},a_{t})=Q_{T}^{}(h_{1:t},a_{t}), h_{1:t},a_{t}\).

The sum of \(n\)-step rewards.Given a POMDP \(\) and a policy \(\), the expectation of the discounted sum of \(n\)-step rewards is denoted as \(G_{n}^{}(h_{1:t},a_{t})=_{,}_{i=t}^{t+n- 1}^{i-t}r_{i} h_{1:t},a_{t}\). By definition, \(G_{1}^{}(h_{1:t},a_{t})=[r_{t} h_{1:t},a_{t}]\) (immediate reward), and \(G_{T-t+1}^{}(h_{1:t},a_{t})=Q^{}(h_{1:t},a_{t})\) (value).

### Memory Lengths

In this subsection, we introduce the memory lengths in the common components of RL, including the reward, transition, policy, and value functions. Then, we will show the theoretical result of the relation between these lengths.

The reward (or transition) memory length of a POMDP quantifies the temporal dependency distance between _current expected reward_ (or _next observation distribution_) and the most distant observation.

**Definition 1.A** (**Reward memory length \(m_{}^{}\)**).: _For a POMDP \(\), \(m_{}^{}\) is the smallest \(n\) such that the expected reward conditioned on recent \(n\) observations is the same as the one conditioned on full history, i.e., \([r_{t} h_{1:t},a_{t}]=[r_{t} h_{t-n+1:t},a_{t}],  t,h_{1:t},a_{t}\)._

**Definition 1.B** (**Transition memory length \(m_{}^{}\)**).: _For a POMDP \(\), \(m_{}^{}\) is the smallest \(n\) such that next observations are conditionally independent of the rest history given the past \(n\) observations and actions, i.e., \(o_{t+1}\!\!\! h_{1:t},a_{t} h_{t-n+1:t},a_{t}\)._

Next, policy memory length is defined based on the intuition that while a policy may take a long history of observations as its context, its action distribution only depends on recent observations. Therefore, policy memory length represents the temporal dependency distance between current _action_ and the most distant observation.

**Definition 1.C** (**Policy memory length \(l_{}()\)**).: _The policy memory length \(l_{}()\) of a policy \(\) is the minimum horizon \(n\{0,,l_{}()\}\) such that its actions are conditionally independent of the rest history given the past \(n\) observations and actions, i.e., \(a_{t}\!\! h_{t-l_{}()+1:t} h_{t-n+1:t}\)._

Lastly, we define the value memory length of a policy, which measures the temporal dependency distance between the current _value_ and the most distant observation.

**Definition 1.D** (**Value memory length of a policy \(l_{}^{}()\)**).: _A policy \(\) has its value memory length \(l_{}^{}()\) as the smallest \(n\) such that the context-based value function is equal to the value function, i.e., \(Q_{n}^{}(h_{t-n+1:t},a_{t})=Q^{}(h_{1:t},a_{t})\), \( t,h_{1:t},a_{t}\)._

Now we prove that the policy and value memory lengths of optimal policies can be upper bounded by the reward and transition memory lengths (Proof in Appendix A).

**Theorem 1** (**Upper bounds of memory lengths for optimal policies)**.: _For optimal policies with the shortest policy memory length, their policy memory lengths are upper bounded by their value memory lengths, which are further bounded by the maximum of reward and transition memory lengths._

\[l_{}(^{*}) l_{}^{}(^{*})( m_{}^{},m_{}^{}):=m^{ },^{*}*{argmin}_{_{ }^{}}\{l_{}()\} \]Thm. 1 suggests that the constant of \(m^{}\) can serve as a proxy to analyze the policy and value memory lengths required in a task, which are often challenging to directly compute, as we will demonstrate in Sec. 3.4

### Credit Assignment Lengths

Classically, temporal credit assignment is defined with a backward view (Minsky, 1961; Sutton, 1984) - how previous actions contributed to current reward. The backward view is specifically studied through gradient back-propagation in supervised learning (Lee et al., 2015; Ke et al., 2018). In this paper, we consider the forward view in reinforcement learning - how current action will influence future rewards. Specifically, we hypothesize that the credit assignment length \(n\) of a policy is how long the greedy action at the current time-step _starts to_ make a difference to the sum of future \(n\)-step rewards.

Before introducing the formal definition, let us recall the common wisdom on dense-reward versus sparse-reward tasks. Dense reward is generally easier to solve than sparse reward if all other aspects of the tasks are the same. One major reason is that an agent can get immediate reward feedback in dense-reward tasks, while the feedback is delayed or even absent in sparse-reward tasks. In this view, solving sparse reward requires long-term credit assignment. Our definition will reflect this intuition.

**Definition 2.1** (**Credit assignment length of a history given a policy \(c(h_{1:t};)\))**.: _Given a policy \(\) in a task, the credit assignment length of a history \(h_{1:t}\) quantifies the minimal temporal distance \(n^{5}\), such that a greedy action \(a^{*}_{t}\) is strictly better than any non-greedy action \(a^{}_{t}\) in terms of their \(n\)-step rewards \(G^{}_{n}\). Formally, let \(A^{*}_{t}:=*{argmax}_{a_{t}}Q^{}(h_{1:t},a_{t})\)6, define_

\[c(h_{1:t};):=_{1 n T-t+1}\{n a^{*}_{t} A^{*} _{t},\,G^{}_{n}(h_{1:t},a^{*}_{t})>G^{}_{n}(h_{1:t},a^{ }_{t}), a^{}_{t} A^{*}_{t}\} \]

**Definition 2.2** (**Credit assignment length of a policy \(c()\)** and of a task \(c^{}\))**.: _Credit assignment length of a policy \(\) represents the worst case of credit assignment lengths of all visited histories. Formally, let \(d_{}\) be the history occupancy distribution of \(\), define \(c():=_{h_{1:t}*{supp}(d_{})}c(h_{1:t};)\)._

_Further, we define the credit assignment length of a task \(\), denoted as \(c^{}\), as the minimal credit assignment lengths over all optimal policies7, i.e., \(c^{}:=_{^{*}^{}_{}}c(^{*})\)._

As an illustrative example, consider the extreme case of a sparse-reward MDP where the reward is only provided at the terminal time step \(T\). In this environment, the credit assignment length of any policy \(\) is \(c()=T\). This is because the credit assignment length of the initial state \(s_{1}\) is always \(c(s_{1};)=T\), as the performance gain of any greedy action at \(s_{1}\) is only noticeable after \(T\) steps.

Our definition of credit assignment lengths has ties with the temporal difference (TD) learning algorithm (Sutton, 1988). With the tabular TD(0) algorithm, the value update is given by \(V(s_{t}) V(s_{t})+(r_{t}+ V(s_{t+1})-V(s_{t}))\). This formula back-propagates the information of current reward \(r_{t}\) and next value \(V(s_{t+1})\) to current value \(V(s_{t})\). When we apply the TD(0) update backwards in time for \(n\) steps along a trajectory, we essentially back-propagate the information of \(n\)-step rewards to the current value. Thus, the credit assignment length of a state measures the number of TD(0) updates we need to take a greedy action for that state.

## 3 Environment Analysis of Memory and Credit Assignment Lengths

By employing the upper bound \(m^{}\) on memory lengths (Thm. 1) and credit assignment length \(c^{}\) (Def. 2.2), we can perform a quantitative analysis of various environments. These environments include both abstract problems and concrete benchmarks. In this section, we aim to address two questions: (1) Do prior environments actually require long-term memory or credit assignment? (2) Can we disentangle memory from credit assignment?

### How Long Do Prior Tasks Require Memory and Credit Assignment?

To study memory or credit assignment in RL, prior works propose abstract problems and evaluate their algorithms in benchmarks. A prerequisite for evaluating agent capabilities in memory or credit assignment is understanding the requirements for them in the tested tasks. Therefore, we collect representative problems and benchmarks with a similar analysis in Sec. 3.2, to figure out the quantities \(m^{}\) related to memory lengths and \(c^{}\) related to credit assignment length.

Regarding abstract problems, we summarize the analysis in Appendix B. For example, decomposable episodic reward assumes the reward is only given at the terminal step and can be decomposed into the sum of Markovian rewards. This applies to episodic MuJoCo (Ren et al., 2022) and Catch with delayed rewards (Raposo et al., 2021). We show that it has policy memory length \(l_{}(^{*})\) at most \(1\) while requiring \(m^{}_{}\) and \(c^{}\) of \(T\), indicating that it is really challenging in both memory and credit assignment. Delayed environments are known as difficult to assign credit (Sutton, 1984). We show that delayed reward (Arjona-Medina et al., 2019) or execution (Derman et al., 2021) or observation (Katsikopoulos and Engelbrecht, 2003) with \(n\) steps all require \(m^{}_{}\) of \(n\), while only delayed rewards and execution may be challenging in credit assignment.

As for concrete benchmarks, Table 1 summarizes our analysis, with detailed descriptions of these environments in Appendix B. First, some tasks designed for long-term memory (Beck et al., 2019; Osband et al., 2020; Hung et al., 2018; Yang and Nguyen, 2021; Esslinger et al., 2022) or credit

   & **Task**\(\) & \(T\) & \(l_{}(^{*})\) & \(m^{}\) & \(c^{}\) \\   & Reacher-pomdp (Yang and Nguyen, 2021) & 50 & long & long & short \\  & Memory Cards (Esslinger et al., 2022) & 50 & long & long & \(1\) \\  & TMaze Long (Noise) (Beck et al., 2019) & 100 & \(T\) & \(T\) & \(1\) \\  & Memory Length (Osband et al., 2020) & 100 & \(T\) & \(T\) & \(1\) \\  & Mortar Mayhem (Pleines et al., 2023) & 135 & long & long & \( 25\) \\  & Autoencode (Morad et al., 2023) & 311 & \(T\) & \(T\) & \(1\) \\  & Numpad (Parisotto et al., 2020) & 500 & long & long & short \\  & PsychLab (Fortunato et al., 2019) & 600 & \(T\) & \(T\) & short \\  & Passive Visual Match (Hung et al., 2018) & 600 & \(T\) & \(T\) & \( 18\) \\  & Repeat First (Morad et al., 2023) & 831 & \(2\) & \(T\) & \(1\) \\  & Ballet (Lampinen et al., 2021) & 1024 & \( 464\) & \(T\) & short \\  & Passive Visual Match (Sec. 5.1; Our experiment) & 1030 & \(T\) & \(T\) & \( 18\) \\  & \(17^{2}\) MiniGrid-Memory (Chevalier-Boisvert et al., 2018) & 1445 & \( 51\) & \(T\) & \( 51\) \\  & Passive T-Maze (Eg. 1; Ours) & 1500 & \(T\) & \(T\) & \(1\) \\  & \(15^{2}\) Memory Maze (Pasukonis et al., 2022) & **4000** & long & long & \( 225\) \\  & HeavenHell (Esslinger et al., 2022) & 20 & \(T\) & \(T\) & \(T\) \\  & T-Maze (Bakker, 2001) & 70 & \(T\) & \(T\) & \(T\) \\  & Goal Navigation (Fortunato et al., 2019) & 120 & \(T\) & \(T\) & \(T\) \\  & T-Maze (Lampenechts et al., 2021) & 200 & \(T\) & \(T\) & \(T\) \\  & Spot the Difference (Fortunato et al., 2019) & 240 & \(T\) & \(T\) & \(T\) \\  & PyBullet-P benchmark (Ni et al., 2022) & 1000 & \(2\) & \(2\) & short \\  & PyBullet-V benchmark (Ni et al., 2022) & 1000 & short & short & short \\   & **Umbrella Length**(Osband et al., 2020) & 100 & \(1\) & \(1\) & \(T\) \\  & Push-r-bump (Yang and Nguyen, 2021) & 50 & long & long & long \\  & Key-to-Door (Raposo et al., 2021) & 90 & short & \(T\) & \(T\) \\  & Delayed Catch (Raposo et al., 2021) & 280 & \(1\) & \(T\) & \(T\) \\  & Active T-Maze (Eg. 2; Ours) & 500 & \(T\) & \(T\) & \(T\) \\  & Key-to-Door (Sec. 5.2; Our experiment) & 530 & short & \(T\) & \(T\) \\  & Active Visual Match (Hung et al., 2018) & 600 & \(T\) & \(T\) & \(T\) \\  & Episodic MuJoCo (Ren et al., 2022) & **1000** & \(1\) & \(T\) & \(T\) \\  

Table 1: _Estimated memory and credit assignment lengths required in prior tasks,_ as defined in Sec. 2. The top block shows tasks designed for memory, while the bottom one shows tasks designed for credit assignment. All tasks have a horizon of \(T\) and a discount factor of \((0,1)\). The \(T\) column shows the largest value used in prior work. The terms “long” and “short” are relative to horizon \(T\). We _manually_ estimate these lengths from task definition and mark tasks _purely_ evaluating long-term memory or credit assignment in black.

assignment (Osband et al., 2020) indeed _solely_ evaluate the respective capability. Although some memory tasks such as Numpad (Parisotto et al., 2020) and Memory Maze (Pasukonis et al., 2022) do require long-term memory with short-term credit assignment, the exact memory and credit assignment lengths are hard to quantify due to the complexity. In contrast, our Passive T-Maze introduced in Sec. 3.2, provides a clear understanding of the memory and credit assignment lengths, allowing us to successfully scale the memory length up to \(1500\).

Conversely, other works (Bakker, 2001; Lambrechts et al., 2021; Esslinger et al., 2022; Hung et al., 2018; Raposo et al., 2021; Ren et al., 2022) actually entangle memory and credit assignment together, despite their intention to evaluate one of the two. Lastly, some POMDPs (Ni et al., 2022) only require short-term memory and credit assignment, which cannot be used for any capability evaluation.

### Disentangling Memory from Credit Assignment

We can demonstrate that memory and credit assignment are independent concepts by providing two examples, all with a finite horizon of \(T\) and no discount. The examples have distinct pairs of \(((r_{},m^{}_{}),c^{})\): \((T,1),(T,T)\), respectively.

We adapt T-Maze (Fig. 1), a POMDP environment rooted in neuroscience (O'Keefe and Dostrovsky, 1971) and AI (Bakker, 2001; Osband et al., 2020), to allow different credit assignment lengths. Passive and active concepts are borrowed from Hung et al. (2018).

**Example 1** (**Passive T-Maze**).: _T-Maze has a 4-direction action space \(\{,,,\}\). It features a long corridor of length \(L\), starting from (oracle) state \(O\) and ending with (junction) state \(J\), connecting two potential goal states \(G_{1}\) and \(G_{2}\). \(O\) has a horizontal position of \(0\), and \(J\) has \(L\). The agent observes null \(N\) except at states \(J,O,G_{1},G_{2}\). The observation space \(\{N,J,O,G_{1},G_{2}\}\) is two-dimensional. At state \(O\), the agent can observe the goal position \(G\{G_{1},G_{2}\}\), uniformly sampled at the beginning of an episode. The initial state is \(S\). The transition is deterministic, and the agent remains in place if it hits the wall. Thus, the horizontal position of the agent \(x_{t}\) is determined by its previous action sequence \(a_{1:t-1}\)._

_In a Passive T-Maze \(_{}\), \(O=S\), allowing the agent to observe \(G\) initially, and \(L=T-1\). Rewards are given by \(R_{t}(h_{1:t},a_{t})=(x_{t+1} t)-1}{T-1}\) for \(t T-1\), and \(R_{T}(h_{1:T},a_{T})=(o_{T+1}=G)\). The unique optimal policy \(^{*}\) moves right for \(T-1\) steps, then towards \(G\), yielding an expected return of \(1.0\). The optimal Markovian policy can only guess the goal position, ending with an expected return of \(0.5\). The worst policy has an expected return of \(-1.0\)._

**Example 2** (**Active T-Maze**).: _In an Active T-Maze \(_{}\), \(O\) is one step left of \(S\) and \(L=T-2\). The unique optimal policy moves left to reach \(O\), then right to \(J\). The rewards \(R_{1}=0\), \(R_{t}(h_{1:t},a_{t})=(x_{t+1} t-1)-1}{T-2}\) for \(2 t T-1\), and \(R_{T}(h_{1:T},a_{T})=(o_{T+1}=G)\). The optimal Markovian and worst policies have the same expected returns as in the passive setting._

\(_{}\) is specially designed for testing memory only. To make the final decision, the optimal policy must recall \(G\), observed in the first step. All the memory lengths are \(T\) as the terminal reward depends on initial observation and observation at \(J\) depends on the previous action sequence. The credit assignment length is \(1\) as immediate penalties occur for suboptimal actions. In contrast, \(_{}\) also requires a credit assignment length of \(T\), as the initial optimal action \(\) only affects \(n\)-step rewards when \(n=T\). Passive and Active T-Mazes are suitable for evaluating the ability of pure memory, and both memory and credit assignment in RL, as they require little exploration due to the dense rewards before reaching the goal.

## 4 Related Work

Memory and credit assignment have been extensively studied in RL, with representative works shown in Table 1. The work most closely related to ours is bsuite(Osband et al., 2020), which

Figure 1: **T-Maze task. In the passive version, the agent starts at the oracle state (“S” and “O” are the same state), while the active version requires that the agent navigate left from the initial state to collect information from the oracle state.**

also disentangles memory and credit assignment in their benchmark. bsuite is also configurable with regard to the memory and credit assignment lengths to evaluate an agent's capability. Our work extends Osband et al. (2020) by providing formal definitions of memory and credit assignment lengths in RL for task design and evaluation. Our definitions enable us to analyze existing POMDP benchmarks.

Several prior works have assessed memory (Parisotto et al., 2020; Esslinger et al., 2022; Chen et al., 2022; Pleines et al., 2023; Morad et al., 2023) in the context of Transformers in RL. However, these prior methods use tasks that require (relatively short) context lengths between \(50\) and \(831\). In addition, there are two separate lines of work using Transformers for better credit assignment. One line of work (Liu et al., 2019; Ferret et al., 2020) has developed algorithms that train Transformers to predict rewards for return redistribution. The other line of work (Chen et al., 2021; Zheng et al., 2022) proposes return-conditioned agents trained on offline RL datasets. Both lines are beyond the scope of canonical online RL algorithms that purely rely on TD learning, which we focus on.

## 5 Evaluating Memory-Based RL Algorithms

In this section, we evaluate the memory and credit assignment capabilities of memory-based RL algorithms. Our first experiment aims to evaluate the memory abilities by disentangling memory from credit assignment, using our Passive T-Maze and the Passive Visual Match (Hung et al., 2018) tasks in Sec. 5.1. Next, we test the credit assignment abilities of memory-based RL in our Active T-Maze and the Key-to-Door (Raposo et al., 2021) tasks in Sec. 5.2. Passive Visual Match and Key-to-Door tasks (demonstrated in Fig. 2) can be viewed as pixel-based versions of Passive and Active T-Mazes, respectively. Lastly, we evaluate memory-based RL on standard POMDP benchmarks that only require short-term dependencies in Sec. 5.3, with a focus on their sample efficiency.

We focus on model-free RL agents that take observation and action _sequences_ as input, which serves as a simple baseline that can achieve excellent results in many tasks (Ni et al., 2022). The agent architecture is based on the codebase from Ni et al. (2022), consisting of observation and action embedders, a sequence model, and actor-critic heads. We compare agents with LSTM and Transformer (GPT-2 (Radford et al., 2019)) architectures. For a fair comparison, we use the same hidden state size (\(128\)) and tune the number of layers (varying from \(1,2,4\)) for both LSTM and Transformer architectures. Since our tasks all require value memory lengths to be the full horizon, we set the _context lengths_ to the full horizon \(T\).

For the T-Maze tasks, we use DDQN (van Hasselt et al., 2016) with epsilon-greedy exploration as the RL algorithm to better control the exploration strategy. In pixel-based tasks, we use CNNs as observation embedders and SAC-Discrete (Christodoulou, 2019) as RL algorithm following Ni et al. (2022). We train all agents to convergence or at most 10M steps on these tasks. We provide training details and learning curves in Appendix C.

### Transformers Shine in Pure Long-Term Memory Tasks

First, we provide evidence that Transformers can indeed enhance long-term memory in tasks that purely test memory, Passive T-Mazes. In Fig. 3 (left), Transformer-based agents consistently solve the task requiring memory lengths from \(50\) up to \(1500\). To the best of our knowledge, this achievement

Figure 2: **Pixel-based tasks, Passive Visual Match (left) and Key-to-Door (right), evaluated in our experiments. Each task has three phases, where we adjust the length of Phase 2 and keep the lengths of the others constant. The agent (in beige) only observes nearby grids (within white borders) and cannot pass through the walls (in black). In Passive Visual Match, the agent observes a randomized target color (blue in this case) in Phase 1, collects apples (in green) in Phase 2, and reaches the grid with the matching color in Phase 3. In Key-to-Door, the agent reaches the key (in brown) in Phase 1, collects apples in Phase 2, and reaches the door (in cyan) in Phase 3.**

may well set a new record for the longest memory length of a task that an RL agent can solve. In contrast, LSTM-based agents start to falter at a memory length of \(250\), underperforming the optimal Markovian policies with a return of \(0.5\). Both architectures use a single layer, yielding the best performance. We also provide detailed learning curves for tasks requiring long memory lengths in Fig. 4. Transformer-based agent also shows much greater stability and sample efficiency across seeds when compared to their LSTM counterparts.

On the other hand, the pixel-based Passive Visual Match (Hung et al., 2018) has a more complex reward function: zero rewards for observing the color in Phase 1, immediate rewards for apple picking in Phase 2 requiring short-term dependency, and a final bonus for reaching the door of the matching color, requiring long-term memory. The success rate solely depends on the final bonus. The ratio of the final bonus w.r.t. the total rewards is inversely proportional to the episode length, as the number of apples increases over time in Phase 2. Thus, the low ratio makes learning long-term memory challenging for this task. As shown in Fig. 3 (right), Transformer-based agents slightly outperform LSTM-based ones8 in success rates when the tasks require a memory length over \(120\), yet their success rates are close to that of the optimal Markovian policy (around \(1/3\)). Interestingly, Transformer-based agents achieve much higher returns with more apples collected, shown in Fig. 5. This indicates Transformers can help short-term memory (credit assignment) in some tasks.

### Transformers Enhance Temporal Credit Assignment in RL, but not Long-Term

Next, we present evidence that Transformers do not enhance long-term credit assignment for memory-based RL. As illustrated in Fig. 6 (left), in Active T-Mazes, by merely shifting the Oracle one step to the left from the starting position as compared to Passive T-Mazes, both Transformer-based and LSTM-based agents fail to solve the task when the credit assignment (and memory) length extends to just \(250\). The failure of Transformer-based agents on Active T-Mazes suggests the bottleneck of credit assignment. This is because they possess sufficient memory capabilities to solve this task (as evidenced by their success in Passive T-Mazes), and they can reach the junction state without exploration issues (as evidenced by their return of more than \(0.0\)). A similar trend is observed in pixel-based Key-to-Door tasks (Raposo et al., 2021), where both Transformer-based and LSTM

Figure 4: **Transformer-based agent can solve long-term memory low-dimensional tasks with good sample efficiency. In Passive T-Maze with long memory length (from left to right: 500, 750, 1000, 1500), Transformer-based agent is significantly better than LSTM-based agent.**

Figure 3: **Transformer-based RL outperforms LSTM-based RL in tasks (purely) requiring long-term memory. Left: results in Passive T-Mazes with varying memory lengths from \(50\) to \(1500\); Right: results in Passive Visual Match with varying memory lengths from \(60\) to \(1000\). We also show the performance of the optimal Markovian policies. Each data point in the figure represents the final performance of an agent with the memory length indicated on the x-axis. All the figures in this paper show the mean and its 95% confidence interval over 10 seeds.**

based agents struggle to solve the task with a credit assignment length of \(250\), as indicated by the low success rates in Fig. 6 (right). Nevertheless, we observe that Transformers can improve credit assignment, especially when the length is medium around \(100\) in both tasks, compared to LSTMs.

On the other hand, we investigate the performance of multi-layer Transformers on credit assignment tasks to explore the potential scaling law in RL, as discovered in supervised learning (Kaplan et al., 2020). Fig. 7 reveals that multi-layer Transformers greatly enhance performance in tasks with credit assignment lengths up to \(100\) in Active T-Maze and \(120\) in Key-to-Door. Yet they fail to improve long-term credit assignment. Interestingly, 4-layer Transformers have similar or worse performance compared to 2-layer Transformers, suggesting that the scaling law might not be seamlessly applied to credit assignment in RL.

### Transformers in RL Raise Sample Complexity in Certain Short-Term Memory Tasks

Lastly, we revisit the standard POMDP benchmark used in prior works (Ni et al., 2022; Han et al., 2020; Meng et al., 2021). This PyBullet benchmark consists of tasks where only the positions (referred to as "-P" tasks) or velocities (referred to as "-V" tasks) of the agents are revealed, with dense rewards. As a result, they necessitate short-term credit assignment. Further, the missing velocities can be readily inferred from two successive positions, and the missing positions can be

Figure 5: **Transformer-based agent is more sample-efficient in long-term memory high-dimensional tasks. In Passive Visual Match with long memory length (750 on the left; 1000 on the right), Transformer-based agents show a slight advantage over LSTM-based agents in the memory capability (as measured by success rate), while being much higher in total rewards (as measured by return).**

Figure 6: **Transformer-based RL improves temporal credit assignment compared to LSTM-based RL, but its advantage diminishes in long-term scenarios. Left: results in Active T-Mazes with varying credit assignment lengths from \(20\) to \(500\); Right: results in Key-to-Door with varying credit assignment lengths from \(60\) to \(500\). We also show the performance of the optimal policies that lack (long-term) credit assignment – taking greedy actions to maximize _immediate_ rewards. Each data point in the figure represents the final performance of an agent in a task with the credit assignment length indicated on the x-axis, averaged over 10 seeds.**

Figure 7: **Increasing the number of layers (and heads) in Transformers aids in temporal, but not long-term, credit assignment. The left two figures show the final results from Active T-Maze and Key-to-Door, varying the number of layers and heads in Transformers from 1, 2 to 4. The right two figures show the associated learning curves in Active T-Maze with different credit assignment lengths.**

approximately deduced by recent velocities, thereby requiring short-term memory. Adhering to the implementation of recurrent TD3 (Ni et al., 2022) and substituting LSTMs with Transformers, we investigate their sample efficiency over 1.5M steps. We find that in most tasks, Transformer-based agents show worse sample efficiency than LSTM-based agents, as displayed in Fig. 8. This aligns with the findings in POPGym paper (Morad et al., 2023) where GRU-based PPO is more sample-efficient than Transformer-based PPO across POPGym benchmark that requires relatively short memory lengths. These observations are also consistent with the known tendency of Transformers to thrive in situations with large datasets in supervised learning.

## 6 Limitations

We provide quantitative definitions of memory and credit assignment lengths, but developing programs capable of calculating them for any given task remains an open challenge. This can often be desirable since sequence models can be expensive to train in RL agents. The worst compute complexity of \(c^{}\) can be \(O(||||T^{2})\) for a finite MDP, while the complexity of \(m^{}\) can be \(O((||+||)^{T}||||T)\) for a finite POMDP. In addition, \(c^{}\) does not take the variance of intermediate rewards into account. These reward noises have been identified to complicate the learning process for credit assignment (Mesnard et al., 2021). On the other hand, \(m^{}\) does not take into account the memory capacity (_i.e._, the number of bits) and robustness to observation noise (Beck et al., 2019).

On the empirical side, we evaluate Transformers on a specific architecture (GPT-2) with a relatively small size in the context of online model-free RL. As a future research direction, it would be interesting to explore how various sequence architectures and history abstraction methods may impact and potentially improve long-term memory and/or credit assignment.

## 7 Conclusion

In this study, we evaluate the memory and credit assignment capabilities of memory-based RL agents, with a focus on Transformer-based RL. While Transformer-based agents excel in tasks requiring long-term memory, they do not improve long-term credit assignment and generally have poor sample efficiency. Furthermore, we highlighted that many existing RL tasks, even those designed to evaluate (long-term) memory or credit assignment, often intermingle both capabilities or require only short-term dependencies. While Transformers are powerful tools in RL, especially for long-term memory tasks, they are not a universal solution to all RL challenges. Our results underscore the ongoing need for careful task design and the continued advancement of core RL algorithms.

Figure 8: **Transformer-based RL is sample-inefficient compared to LSTM-based RL in most PyBullet occlusion tasks. The top row shows tasks with occluded velocities, while the bottom row shows tasks with occluded positions. Results are averaged over 10 seeds.**