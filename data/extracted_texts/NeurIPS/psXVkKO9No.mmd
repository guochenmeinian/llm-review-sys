# Self-Predictive Universal AI

Elliot Catt, Jordi Grau Moya, Marcus Hutter, Matthew Aitchison

Tim Genewein, Gregoire Deletang, Kevin Li Wenliang, Joel Veness

Google DeepMind

ecatt@google.com

###### Abstract

Reinforcement Learning (RL) algorithms typically utilize learning and/or planning techniques to derive effective policies. Integrating both approaches has proven to be highly successful in addressing complex sequential decision-making challenges, as evidenced by algorithms such as AlphaZero and MuZero, which consolidate the planning process into a parametric search-policy. AIXI, the universal Bayes-optimal agent, leverages planning through comprehensive search as its primary means to find an optimal policy. Here we define an alternative universal Bayesian agent, which we call Self-AIXI, that on the contrary to AIXI, maximally exploits learning to obtain good policies. It does so by _self-predicting_ its own stream of action data, which is generated, similarly to other TD(0) agents, by taking an action maximization step over the current on-policy (universal mixture-policy) Q-value estimates. We prove that Self-AIXI converges to AIXI, and inherits a series of properties like maximal Legg-Hutter intelligence and the self-optimizing property.

## 1 Introduction

Reinforcement Learning (RL)  algorithms exploit learning, planning 1, or their combination, to obtain good policies from experience. Pure learning consists of using real experience for improving a policy via a (parametric) model, possibly representing an explicit policy-model and/or the Q-values [2; 3; 4; 5; 6]. In a sense, learning stores the computational effort of policy-improvement into the parameters, which makes it a computationally efficient approach when needing to reuse the policy later on. In contrast, pure planning finds good policies via simulated experience using an environment model and a randomized (or exhaustive) search policy [1; 7; 8]. In the case of unknown or stochastic environments, one must re-plan after receiving a new observation, thus wasting all computational-effort from the previous step. This makes pure planning a wasteful approach. Using both, planning and learning, is a good way to improve performance and efficiency as demonstrated by modern high-performant RL algorithms such as MuZero [9; 10; 11; 12]. These algorithms distill the planning effort back into the parametric search-policy by training it to predict the good actions obtained from planning. In a way, these agents are _self-predicting_ their own policy-improvements. Although empirically successful and widely used, this distillation  or self-prediction 2 process is motivated in a purely heuristic way without much theoretical understanding on its optimality condition.

The AIXI agent [14; 15] is a theoretical universal Bayes-optimal agent obtained through pure planning without relying on distilling the search effort as described above. AIXI learns an environment model via a Solomonoff predictor [16; 17] and uses it for exhaustive (computationally intractable) planning. Thus, although it uses learning for the environment model, we say AIXI adopts a pure planning approach in the context of policy generation. Two desirable properties of Solomonoff prediction are universality--obtained by considering a huge hypothesis class containing all computableenvironments--and fast convergence to the true environment statistics [18; 19]. Unfortunately, universality makes Solomonoff prediction incomputable, a property that AIXI also inherits. Nevertheless, the AIXI agent is considered as the most powerful agent and the gold-standard in decision-making under unknown general environments. An existing gap in the literature, however, is the lack of an alternative universal agent that maximally exploits learning and distillation, instead of the more wasteful planning approach used by AIXI. Understanding this distillation process from a theoretical standpoint can inspire taking new directions in approximating the AIXI agent as done in , which is based on a pure planning approach.

The central aim of this paper is to define an agent, which we name Self-AIXI, that maximally exploits _self-prediction_ (or distillation) instead of planning, and to prove that this agent is optimal in the sense that converges to the gold-standard AIXI agent. We define self-prediction as the process of predicting the stream of action-data generated by the agent itself. Self-AIXI generates action-data in a similar fashion as other Temporal Difference TD(0)  algorithms in which there is only a single "planning" step when choosing an action i.e. \(*{arg\,max}_{a_{t}}(h_{t},a_{t})\) for history \(h_{t}\), action \(a_{t}\) and current Q-value estimates \(\). The difference lies in that Self-AIXI does exact Bayesian inference on the policy space by holding a universal mixture \(\) over policies (in addition to \(\), the universal mixture over environments), and consumes action-data generated by maximizing the Self-AIXI policy i.e. \(*{arg\,max}_{a}Q_{}^{}(h_{t},a)\). In summary our contributions are:

* We define Self-AIXI, a novel universal agent based on self-prediction.
* We prove that Self-AIXI's Q-values converge to AIXI's Q-values asymptotically.
* We show how Self-AIXI leads to maximum Legg-Hutter  intelligence and inherits the self-optimization  property.

These results provide compelling evidence that self-prediction can effectively serve as a robust alternative to traditional planning methods.

## 2 Background

### General Reinforcement Learning

Let \(,,\) denote the (finite) action, observation and reward set respectively. Let the set of percepts be defined as \(:=\). \(\) denotes a distribution over \(\). Define the set of histories \(:=()^{*}\). A policy \(:\) is a distribution over actions given a history. An environment \(:\) is a distribution over percepts given history and action. We use \(h_{<t}:=a_{1}e_{1} a_{t-1}e_{t-1}=a_{1}o_{1}r_{1} a_{t-1}o_{t-1} r_{t-1}\) to denote the history of interactions up to time \(t-1\). Given an environment \(\) and policy \(\) we use \(^{}\) to denote the induced probability measure on histories, i.e. \(h_{<t}\) is assumed to be sampled from \(^{}\). We are interested in how well agents (policies) perform. We measure this performance by the future expected sum of discounted rewards, called value function.

**Definition 1** (Value function and optimal policy).: _The value \(V_{}^{}:\) of a policy \(\) in an environment \(\), and discount factor \(\) given a history \(h_{<t}\) is defined as_

\[V_{}^{}(h_{<t}):=}_{}^{}[_ {k=t}^{}^{k-t}r_{k}h_{<t}]\]

_Where \(_{t}=_{k=t}^{}^{k-t}=\) is the normalization factor of the discount. The optimal value is defined as \(V_{}^{*}(h_{<t}):=*{sup}_{}V_{}^{}(h_{<t})\), the set of optimal policies with respect to that value is defined as \(*{arg\,max}_{}V_{}^{}(h_{<t})\), and an optimal policy with respect to the value is defined as \(_{}^{*}(|h_{<t})*{arg\,max}_{}V_{}^{}(h_ {<t})\)._

We can similarly define the action value or Q-value function as \(Q_{}^{}(h_{<t},a_{t}):=V_{}^{}(h_{<t}a_{t})\), which allows to write the pseudo-recursive Bellman forms

\[V_{}^{}(h_{<t})=_{a_{t}}(a_{t}|h_{<t})Q_{}^{ }(h_{<t},a_{t}),\;Q_{}^{}(h_{<t},a_{t})=_{e_{t}}(e _{t}|h_{<t}a_{t})(r_{t}+ V_{}^{}(h_{1:t}))\] (1)We use throughout the paper the Total-Variation distance as a convenient way to compare two policies.

**Definition 2** (TV distance).: _We define the Total Variation (TV) distance of two measures \(_{1}^{_{1}},_{2}^{_{2}}\) between timesteps \(t\) and \(t m\) as_

\[D_{m}(_{1}^{_{1}},_{2}^{_{2}}|h_{<t}):=_{H()^{m}}|_{1}^{_{1}}(H|h_{<t})-_{2}^{_{2}}(H|h_{<t})|  1\]

The following lemma, useful for our later proofs, shows that TV distance is an upper bound for value function difference in general reinforcement learning.

**Lemma 3** ([23, Lemma 4.17]).: _For any two policies \(_{1},_{2}\) and two environments \(_{1},_{2}\)_

\[|V_{_{1}}^{_{1}}(h_{<t})-V_{_{2}}^{_{2}}(h_{<t})| D_{}( _{1}^{_{1}},_{2}^{_{2}}|h_{<t})\]

### Universal Artificial Intelligence

We achieve universality by considering a Bayesian mixture over a class of potential environments \(\) which is large enough so as to include the true environment the agent is interacting with. A typical choice is the class of all computable functions.

**Definition 4** (Bayesian mixture environment).: _The Bayesian mixture over the class of environments \(\) is defined as_

\[(e_{t}|h_{<t}a_{t}):=_{}w(|h_{<t})(e_{t}|h_{<t} a_{t}) w(|h_{1:t}):=w(|h_{<t})|h_{<t}a_{t})}{(e_{t}|h_{<t} a_{t})}\]

_and \(w(|):=w()\) is the prior probability of \(\)._

Given this environment mixture \(\), we define AI\(\) (AIXI) as acting optimally with respect to \(\).

**Definition 5** (Al\(\)).: _AIXI, the optimal Bayesian agent is defined as_

\[_{}^{}(h_{<t}):=*{arg\,max}_{a}Q_{}^{}(h_{<t},a)\]

The following results prove that the AIXI agent achieves maximal intelligence in the Legg-Hutter sense and is self-optimizing. These are important to have in mind throughout the paper since we will prove the same type of results for our Self-AIXI agent.

Legg-Hutter Intelligence measures the performance of an agent \(\) in a wide range of environments \(\), weighted by their p(oste)prior plausibility \(w()\):

**Definition 6** (LH Intelligence ).: _The Legg-Hutter Intelligence measure \(\) of a policy \(\) given a history \(h_{<t}\) is defined as_

\[(|h_{<t}):=_{}w(|h_{<t})V_{}^{}(h_{< t})=V_{}^{}(h_{<t})\]

It has been shown that if \(\) is such that there exists a policy (sequence) which is able to achieve strong asymptotic optimality then AIXI can achieve this as well. This property is known as self-optimizing.

**Theorem 7** (Alxi is Self-optimizing ).: _Let \(\) be some environment. If there is a policy \(\) and a sequence of policies \(},}\) such that for all \(\)_

\[V_{}^{}(h_{<t})-V_{}^{}}(h_{<t}) 0\ \ \ \ \ t\ \ ^{}\] (2)

_then_

\[V_{}^{}(h_{<t})-V_{}^{_{}^{}}(h_{<t}) 0\ \ \ \ \ t\ \ ^{}\]

_If \(=_{}^{}\) and Equation 2 holds for all \(\), then \(_{}^{}\) is strongly asymptotically optimal in the class \(\)._

For a taxonomy of classes that do (not) allow for self-optimizing policies, see [21, Figure 3.1]. A challenge that is difficult to overcome is that for geometric discounting, there may not yet be widely-recognized classes of environments that allow self-optimizing policies.

Self-prediction in General Reinforcement Learning

We are ready to define our Self-AIXI agent that is based on _self-prediction_ instead of pure planning. We start with our definition of the mixture policy \(\) and discuss the choices of model class and prior.

**Definition 8** (Bayesian mixture policy).: _The Bayesian mixture over the class of policies \(\) is defined as_

\[(a_{t}|h_{<t}):=_{}(|h_{<t})(a_{t}|h_{<t} )(|h_{1:t}):=(|h_{<t})|h_{<t})}{(a_{t}|h_{<t})}\]

_and \((|):=()\) is the prior probability of \(\). Note \(()\) is a prior for the mixture policy, while \(w()\) was a prior for the mixture environment._

**Lemma 9** (Linearity of Q-values).: _For all \(\), \(\), \(h_{<t}\) and \(a_{t}\) we have_

\[Q^{}_{}(h_{<t},a_{t})=_{}w(|h_{<t})Q^{}_{} (h_{<t},a_{t}), Q^{}_{}(h_{<t},a_{t})=_{} (|h_{<t})Q^{}_{}(h_{<t},a_{t})\]

From the linearity of \(Q\)-values we can define the following \(Q\)-value over the Bayesian mixture environment and Bayesian mixture policy.

\[Q^{}_{}(h_{<t},a_{t}):=_{}(|h_{<t}) _{}w(|h_{<t})Q^{}_{}(h_{<t},a_{t})\] (3)

**Definition 10** (Self-Aixi).: _The agent Self-AIXI is defined as taking the (one step) optimal action with respect to \(Q^{}_{}\). Formally,_

\[_{S}(h_{<t}):=*{arg\,max}_{a_{t}}Q^{}_{}(h_{<t},a_{t})\]

**Remark 11** (Self-Aixi does not optimize the future.).: _Importantly, Self-AIXI maximizes the Q-values from the mixture policy instead of the optimal policy, which means that there is no need to optimize the future but it needs to know the Q-values of the current \(\)-mixture. Pragmatically, these Q-values are on-policy, typically easier to estimate than the optimal off-policy values._

Note how action selection and history interact with the policy-mixture. The action selected by the Self-AIXI agent necessarily improves, by definition, over the current value estimates \(_{a_{t}}Q^{}_{}(h_{<t},a_{t}) V^{}_{}(h_{<t})\). Then, this action is added to the next history \(h_{<t+1}\) which is consumed by the policy-mixture at the next time step i.e., \((a_{t+1}|h_{<t+1})\). This leads to the following remark.

**Remark 12** (Self-predicting minimal improvements.).: _The policy-mixture does Bayesian inference over the incoming self-generated action-data and makes better predictions over time. Thus, Self-AIXI is self-predicting its own small improvements made by the \(*{arg\,max}\) operation in Definition 10. In the case of using the largest class of policies (all computable functions), the mixture is a Solomonoff predictor with enough power to represent, within itself, the policy evaluation and improvement operation._

Showing that the self-prediction process employed by Self-AIXI converges to AIXI is the aim of our theoretical results in Section 4.

### Choice of class and prior

When choosing an environment class \(\) we need to pick one large enough so as to contain the true environment \(\); the larger the class the weaker the assumption that \(\). However, the larger the class we choose the harder it will be to compute the Bayesian mixture of that class, to the point that \(\) may become incomputable (or at least more incomputable than any \(\)). With all of this in mind there are two directions that this can be taken: The first is to choose the largest class such that computing \(\) is still tractable and the second is to choose the largest possible class such that it contains all possible environments our universe could contain (or be). In the first case we can choose something like the class of variable-order MDPs with efficient Context Tree Weighting(CTW) algorithm  for \(\), as is done in . In the second case we can pick the set of all (cumulatively lower-) semicomputable semimeatures . Without going to much into the choice of prior, a simplicity-based prior is an ideal choice. There is an interesting poly-time computable class of all measures of logarithmic complexity  for which \(\) (\((,)\)-simple measures). One easy way of achieving \(\) is to add \(\) to \(\) with some weight \(w_{0}^{}>0\) and renormalize \(w_{i} w_{i}^{}=(1-w_{0}^{})w_{i}\), then \(^{}^{}:=\{\}\). We can apply the same logic to the policy class \(\) and policy prior \(()\) as we just did for \(\) and \(w()\), similarly we need to have that \(_{S}\). We leave the full investigation of policy classes to future work. Full discussion on the choice of class \(\) and prior \(w()\) are beyond the scope of this work [14; 27].

### Experimental Results

While our work is mainly theoretical, we also conducted experiments (see Appendix B) comparing self-prediction, using a Self-AIXI approximation, against the pure planning approach, using an AIXI approximation, using Context Tree Weighting as predictor and Monte-Carlo Tree Search for the Q-value estimates. In short, the Self-AIXI approximation outperforms the AIXI approximation in three environments and performs equally in the remaining two.

## 4 Theoretical Results

In this section we will demonstrate the intelligence of Self-AIXI by showing that it asymptotically converges to AIXI in expectation. We specifically chose this criterion over other types of optimality, since Cesaro and Almost-Sure asymptotic optimality (converging to the optimal policy) are too strong [28; 29; 30], and are not satisfied by AIXI. In fact, agents which satisfy these also die with certainty .

The goal is to show that Self-AIXI eventually performs as well as the most intelligent agent, and AIXI is the most intelligent agent . We will do this by proving an asymptotic convergence result to the Bayes optimal policy in all environments.

To summarize the results of this section:

* Theorem 18: \(\). \(V_{}^{_{S}} V_{}^{s}\) in \(^{_{S}}\)-expectation.
* Theorem 21: \(\). \(V_{}^{_{S}} V_{}^{_{}^{s}}\) in \(^{_{S}}\)-expectation.
* Theorem 22: \(V_{}^{_{S}} V_{}^{_{S}}\)-almost surely, under similar conditions to the AIXI Self-optimizing result from Theorem 7.

The initial three results successively broaden in scope, with the second being a more general form of the first, and the third further generalizing the second, holding for all \(\) instead of just \(\). All proofs not found in this section can be found in the appendix.

### Expected AIXI-like behavior

To start with, we will present a useful Lemma about the environment mixture \(\):

**Lemma 13** (Convergence of \(\) to \(\) in TV).: _For any environment \(\) and policy \(\) we have_

\[D_{}(^{},^{}|h_{<t}) 0\ \ \ \ \ t\ \ ^{}\]

Proof.: Dominance \(^{}() w()^{}()\) implies that \(^{}\) is absolutely continuous with respect to \(^{}\) and therefore \(^{}\) merges strongly with \(^{}\). For full details and definitions of strong merging and absolute continuity see . 

We can derive a dual to Lemma 13 and restate it in terms of asymptotic convergence in expectation:

**Lemma 14** (Convergence of \(\) to \(\) in expectation).: _If \(\) then for all \(\) we have_

\[_{}^{}[D_{}(^{},^{}|h_{<t})] 0\ \ \ \ \ t\]

Proof.: The proof is dual to the proof of Lemma 13 with policies and environments exchanged. Finally, almost sure convergence implies convergence in expectation for bounded random variables.

Now we can show that if a policy is close to one-step optimal, then it is also close to optimal (this result is independent of \(\) and will work for any environment, but we are only interested in the \(\) case).

**Lemma 15** (A one-step good policy is close to optimal).: _Let \((h):=|V_{}^{*}(h)-V_{}^{}(h)|\) with \(h()^{t}\) for \(t t_{0}\)._

\[&\ \ _{}^{}|_{a}Q_{}^{ }(h,a)-V_{}^{}(h)|<\ \  t t_{0}\\ &\ \ _{}^{}[_{a}_{e}(e|ha)(hae)] (1+)_{}^{}(hae)\ \  t t_{0}\\ &\ \ _{}^{}(h)<\ \  t t_{0}\ \ \ \ 1+<1/\]

Using the previous Lemma we can show our main result about the behavior of Self-AIXI. This result shows that the performance of Self-AIXI converges to that of AIXI on (expected) histories generated by Self-AIXI. We call a policy that satisfied the second condition for some \(1+<1/\) for some (large) \(t_{0}\), _sensible off-policy_. We conjecture this plausible condition holds for \(_{S}\), bar some exotic counter-example classes \(\).

**Theorem 16** (Self-AIXI converges to AIXI in \(\)-expectation).: _Assuming \(_{S}\) is sensible off-policy,_

\[_{}^{_{S}}[V_{}^{*}(h_{<t})-V_{}^{_{S}}(h_{<t}) ] 0\ \ \ \ t.\]

Proof.: \[0\ }{{}}\ \ _{a}Q_{}^{}(h,a)-V_{}^{}(h)\ }{{=}}\ V_{}^{_{S}}(h)-V_{}^{}(h) \ }{{}}\ D_{}(^{_{S}},^{}|h) \]

where (a) and (b) follow from

\[V_{}^{_{S}}(h)\ \ _{a}Q_{}^{}(h,a)\ \ _{a}(a|h)Q_{}^{}(h,a)\ =\ V_{}^{}(h)\]

and (c) follows from Lemma 3. Let \(>0\) and \(h()^{t}\). Now Lemma 14 and the above implies that there exists a \(t_{0}\) such that for all \(t t_{0}\),

\[0\ \ _{}^{_{S}}[_{a}Q_{}^{}(h,a)-V_{}^{ }(h)]\ \ _{}^{_{S}}D_{}(^{_{S}},^{}|h)\ <\ \]

Taking the expectation of \(0 V_{}^{*}-V_{}^{_{S}} V_{}^{*}-V_{}^{}\) (which follows from \(V_{}^{*} V_{}^{_{S}} V_{}^{}\)), Lemma 15 implies

\[0\ \ _{}^{_{S}}[V_{}^{*}(h)-V_{}^{_{S}}(h)]\ \ _{}^{_{S}}[V_{}^{*}-V_{}^{}]\ \ /(1-(1+)).\]

Since \(>0\) was arbitrary, we get the \(_{}^{_{S}}[V_{}^{*}(h)-V_{}^{_{S}}(h)] 0\). 

The previous theorem also holds in expectation over the true environment \(\), instead of in expectation w.r.t. \(\), but first we need a small lemma:

**Lemma 17** (\(_{}^{} 0\) implies \(_{}^{} 0\)).: _If \(\) is such that_

\[_{}^{}[V_{}^{*}(h_{<t})-V_{}^{}(h_{<t})]  0\ \ \ \ t.\]

_then for all \(\) we have_

\[_{}^{}[V_{}^{*}(h_{<t})-V_{}^{}(h_{<t})]  0\ \ \ \ t.\]

We can now present the main Theorem of the paper, this theorem states that as Self-AIXI interacts with the true environment \(\) Self-AIXI will learn to act like the optimal Bayesian agent AIXI.

**Theorem 18** (Main Theorem: Self-AIXI converges to AIXI in \(\)-expectation).: _For all \(\) we have_

\[_{}^{_{S}}[V_{}^{*}(h_{<t})-V_{}^{_{S}}(h_{<t}) ] 0\ \ \ \ t.\]

Proof.: Theorem 16 and Lemma 17. 

We can rewrite the above theorem in the form of the Universal Intelligence measure \(\),

**Corollary 19** (LH Form of Theorem 18).: _For all \(\)_

\[_{}^{_{S}}[_{}(|h_{<t})-(_{S}| h_{<t})] 0\ \ \ \ t\]

Finally we can generalise our previous result to work for the value function of any \(\). This is important because we are most interested in the performance of our agent on the true environment. We first need to show that convergence of the value function with \(\) implies convergence of the value function with \(\) (in \(\)-expectation).

**Lemma 20** (\(V_{}^{^{}} V_{}^{}\) implies \(V_{}^{^{}} V_{}^{}\) in \(\)-expectation).: _If \(\) is such that for all \(\)_

\[_{}^{}[V_{}^{^{}}(h_{<t})-V_{}^{}(h_{< t})] 0\ \ \ \ t.\]

_and \(D_{}(^{^{}},^{^{}}|h_{<t}) 0\ ^{}\)-almost surely then we have_

\[_{}^{}[V_{}^{^{}}(h_{<t})-V_{}^{}(h_{< t})] 0\ \ \ \ t.\]

Now we can present a convergence in value functions with the true environment \(\).

**Theorem 21** (Self-AIXI converges to the \(\)-optimal agent in \(\)-expectation).: _For all \(\) if \(D_{}(^{_{}^{*}},^{_{}^{*}}|h_{<t}) 0\ ^{_{S}}\)-almost surely then we have_

\[_{}^{_{S}}[|V_{}^{_{}^{*}}(h_{<t})-V_{}^{ _{S}}(h_{<t})|] 0\ \ \ \ t.\]

Proof.: Immediate result form Theorem 18 and Lemma 20 

This result states that as Self-AIXI interacts with the true environment \(\), the expected future performance difference between Self-AIXI and AIXI in \(\) will go to zero, that is to say, they will (asymptotically) have equal performance in \(\).

### Self-optimization

We have previously shown the performance of Self-AIXI using convergence of value functions \(\)-expectation. We can also demonstrate the performance of Self-AIXI in a more direct way, using self-optimization. That is, showing that if it is possible for a policy to learn to be optimal in all environments in a given class, then Self-AIXI will learn to be optimal in all environments in this class.

Self-AIXI can self-optimize in the same way as AIXI, with an additional assumption on the self model. The additional assumption is that for all \(t,h_{<t}\) we have \(V_{}^{}(h_{<t}) V_{}^{}}(h_{<t})-_{t}\).

**Theorem 22** (Self-AIXI is Self-optimizing).: _Let \(\) be some environment. If there is a policy \(\) and a sequence of policies \(},}\) all contained within \(\) such that for all \(t,h_{<t}\) we have \(V_{}^{}(h_{<t}) V_{}^{}}(h_{<t})-_{t}\) with \(_{t} 0\), and for all \(\)_

\[V_{}^{*}(h_{<t})-V_{}^{}}(h_{<t}) 0\ \ \ \ t \ \ ^{}\] (4)

_then_

\[V_{}^{*}(h_{<t})-V_{}^{_{S}}(h_{<t}) 0\ \ \ \ t \ \ ^{}\]

_If \(=_{S}\) and Equation 4 holds for all \(\), then \(_{S}\) is strongly asymptotically optimal in the class \(\)._

There are known important cases of model classes which admit self-optimising policies such as Ergodic MDPs  and Ergodic k-MDPs . However, we have not yet shown there exist (interesting) classes \(\) and \(\) where the assumption that for all \(t,h_{<t}\) we have \(V_{}^{}(h_{<t}) V_{}^{}}(h_{<t})-_{t}\) with \(_{t} 0\) and the other self-optimizing assumptions hold. We leave this to future work.

## 5 Related work

### Universal Artificial Intelligence

There has been much work studying the Bayesian optimal agent AIXI and extending it in various ways [14; 21; 23]. Some of these extensions include: Knowledge seeking agent (KSA), an extension of AIXI which seeks information leading to a form of maximal exploration [35; 36]. BayesExp, which combines the exploiting of AIXI and exploring of KSA in a way that allows the agent to be weakly asymptotically optimal [37; 38]. An AIXI variant that uses Thompson sampling  for additional exploration is asymptotically optimal in expectation . Inquisitive Agent (Inq) uses an information gaining strategy based on expeditions to achieve strong asymptotic optimality .

AIXI achieves (by definition) Bayesian optimality, and although there are problems with this notion of optimality, it is widely agreed to be the (current) best choice for an optimality criterion. Every notion of optimality possesses inherit difficulties; these difficulties have been extensively studied Leike and Hutter , Lattimore and Hutter , Cohen et al. .

### Comparison with other methods in the literature

To overcome the computational burden of the AIXI's planning phase, approximation techniques usually relax the planning problem into an estimation problem [41; 42]. The main approach for off-the-shelf estimation methods such as Monte Carlo Tree Search (MCTS) is an (optimistic) estimation of the Q-values. Though seemingly different, planning and learning, in the general case, can be seen as following very similar computational processes as shown by . Other practical methods which reduce or remove the planning problem are distributional reinforcement learning , Compress and Control (CnC)  and feature reinforcement learning (FRL) [46; 47]. In distributional reinforcement learning and CnC powerful sequence prediction methods are utilized to estimate the distribution of returns directly. While it does not remove the planning entirely, FRL adaptively compresses the general RL problem to a more simple (often MDP) problem and solves that problem by traditional methods. The planning required in the simple problem is often far less than in the general problem.

In Table 1 we compare some exemplary agents where each column denotes whether they use some form of learning (or policy distillation), whether they use explicit search/planning, whether they learn an explicit model of the environment, if they are a Bayes-optimal agent, and if there are practical in the sense that there is an actual implementation of the agent. As can be seen, Self-AIXI is the only Bayes-optimal agent that is universal and exploits learning instead of planning.

### Agents modeling themselves

Self-prediction can be thought as an agent modeling itself. Within the universal AI literature, there are other works that go further and drop the dualism of traditional reinforcement learning for the (more realistic) physicalism where agents are contained within their own environment. In this case many complications arise which previous work tries to address. For example, in  the behavior of several agents able to modify their own source code is analyzed. In , Orseau et. al. presents a space-time embedded definition of the value function and intelligence, and in  Leike et. al. aims to solve the grain of truth (a self-referential) problem of Bayesian agents in GRL, showing that there

   & Policy Learning & & Environment & Bayes & \\  & (or distillation) & Planning & Model & Optimal & Practical \\   AIXI & No & Yes & Yes & Yes & No \\  Self-AIXI & Yes & No & Yes & Yes & No \\  MC-AIXI-CTW & No & Yes & Yes & Yes & Yes \\   \(\)-Zero & Yes & Yes & Yes & No & Yes \\  DQN & Yes & No & No & No & Yes \\  CnC & No & No & No & Yes & Yes \\  PhiMDP & No & No & No & No & No \\  

Table 1: Comparison between different methodsis a non-trivial class of environments for which the optimal Bayesian agent over that class is within the class.

### Self Prediction in Traditional RL

Recently traditional reinforcement learning has begun to embrace self-prediction. In  Predictions of Bootstrapped Latents (PBL) is presented as a method which can use predictive representations on multitask environments.  used Self-Predictive Representations (SPR) to train agents to anticipate future latent states, which showed improvements on Atari games benchmarks. In , exploration in visually complex environments was streamlined with BYOL-Explore using a bootstrapped prediction. Additionally,  underscores the challenges of representation collapse in self-predictive learning and introduces bidirectional learning to address this. Together, these studies underscore the evolving landscape of self-predictive strategies in traditional RL.

## 6 Discussion, Future Work and Limitations

Practical considerations for computing the mixture component in each step.Computing the Self-AIXI policy requires estimating Q-values using the Bayesian mixture policy \((a_{t}|h_{<t})\). Naively, this means evaluating each policy in the mixture and computing its respective posterior weight. However, more efficient schemes can be derived, e.g., similar to how the CTW algorithm computes the Bayesian mixture without explicitly evaluating each hypothesis and computing the (marginalizing) sum over hypotheses in each step.

Leveraging large sequence models.Powerful sequence predictors like large language models (LLMs) are the cornerstone of modern general AI due to their impressive zero- and few-shot performance [55; 56; 57]. An open question in AGI research is how to leverage the strength of sequence prediction not only for environment modeling, but also to derive goal-directed policies; how to build agents from predictors. We have shown how to shift computational effort for planning (in AIXI) can be transferred into effort devoted to prediction in Self-AIXI, making self-prediction theoretically very well suited to exploit strong predictors such as LLMs. In contrast to similar ad-hoc approaches , Self-AIXI is derived from sound theoretical design principles and comes with theoretical guarantees.

Safety implications.Self-AIXI constitutes a theoretical blueprint for building powerful general AI agents that may be very suitable to leverage strong predictors, such as LLMs, in practice. The implications of such agents have recently been discussed in the philosophical and technical literature, and concerns regarding their safety have been raised [59; 60]. Self-AIXI comes with some interesting favorable safety properties. First and foremost, Self-AIXI is derived from sound reasoning and decision-making principles: Bayesian posterior inference and rational optimization of rewards. Self-AIXI comes with a well-understood theoretical model to study abstract properties, guarantees and bounds. An advantage over AIXI, by design, is the self-model of the agent, which can be analyzed on its own. The explicit separation of beliefs over the environment and posterior over the own policy leads to an agent that is more interpretable and can be verified more easily compared to a monolithic agent where components are mixed. If the self-predictor used is not a Bayesian one, but instead some black box predictor, then both the guarantees of Self-AIXI and the guarantees on interpretability are no longer maintained. We strongly encourage an open and differentiated wider discussion on the safety of (universal) RL agents, and believe that theoretical models, like Self-AIXI, can facilitate this discussion by providing a formal and mathematically concrete formulation.

Limitations.Our theoretical results require that the Self-AIXI policy is part of the policy class under consideration, i.e., \(_{S}\) (meaning that the mixture policy is itself part of the policy class). Generally, if there is a policy in the policy class that is "close" to the optimal policy (in terms of minimal KL divergence), Self-AIXI will still perform well. This result can be stated formally using known formulations and bounds for Bayesian mixtures. We leave a detailed mathematical analysis for future work, since the aim of this work is to introduce the theory of Self-AIXI. Another issue is that Self-AIXI, like the original AIXI, is incomputable for environment classes required for (strict) universality. To tackle this, many practical approximations of AIXI have been proposed in the literature, which provide straightforward starting points for similar approximations for Self-AIXI.

Designing and studying non-trivial practical approximations of Self-AIXI is beyond the scope of this paper.

Future work.On the theoretical side there are several promising directions of future work: exploring which non-trivial policy classes \(\) satisfy \(_{S}\) (or replacing this assumption); extending this work with a sampling-first approach (e.g. Thompson Sampling). On the practical side the first step is to empirically demonstrate the performance of the self-prediction approach proposed in this paper. We believe that especially when coupled with powerful modern predictors, e.g., transformer-based sequence predictors, high-fidelity approximations of \(\) and \(\) are possible, which can lead to strong, general agents.

## 7 Conclusion

In this paper we presented Self-AIXI, a theoretical framework for universal AI that extends AIXI by self-prediction (the agent performs inference over its own policy). The result is an agent that matches AIXIs performance guarantees, while shifting the computational focus on prediction instead of planning. This makes Self-AIXI interesting for building general agents by leveraging recent breakthroughs in sequence predictors, and combining them with RL in a sound fashion. Current approaches to do this, such as RL from human/artificial feedback (RLHF/RLAF), are quite limited (in terms of generality of the RL formulation) and do not come with optimality or universality guarantees. We believe our results to be an important cornerstone for building agents that go beyond zero- or few-shot transfer, towards rapid and data efficient RL.

#### Acknowledgments

We thank Tor Lattimore, Laurent Orseau and Anian Ruoss for their helpful feedback and insightful discussions.