# Reasoning and Tools for Human-Level Forecasting

Elvis Hsieh

Equal contribution

Preston Fu1

Jonathan Chen1

UC Berkeley

{htelvis92,prestonfu,jonchen25}@berkeley.edu

###### Abstract

Language models (LMs) trained on web-scale datasets are largely successful due to their ability to memorize large amounts of training data, even if only present in a few examples. These capabilities are often desirable in evaluation on tasks such as question answering but raise questions about whether these models can exhibit genuine reasoning or succeed only at mimicking patterns from the training data. This distinction is particularly salient in forecasting tasks, where the answer is not present in the training data, and the model must reason to make logical deductions. We present Reasoning and Tools for Forecasting (RTF), a framework of reasoning-and-acting (ReAct) agents that can dynamically retrieve updated information and run numerical simulation with equipped tools. We evaluate our model with questions from competitive forecasting platforms and demonstrate that our method is competitive with and can outperform human predictions. This suggests that LMs, with the right tools, can indeed think and adapt like humans, offering valuable insights for real-world decision-making.

## 1 Introduction

Forecasting is an essential tool today, playing a critical role in government, corporate, and personal decision-making. Weather forecasting provides essential information for agriculture, natural disaster preparedness for governments, and travel plans for individuals. During the COVID-19 pandemic, lockdown policies were largely determined by forecasts, which were required to be sufficiently accurate due to their global impact . Forecasting methodologies fall into two main categories : statistical and judgmental. Statistical forecasting leverages time-series modeling and excels with abundant data under stable conditions. Conversely, judgmental forecasting, which we refer to simply as "forecasting," typically relies on human expertise, integrating historical data, domain knowledge, and intuition to make predictions, and is particularly useful when data are sparse or conditions are volatile. By nature, forecasting requires not only accuracy but also the ability to continuously adapt to dynamic data streams. This is where traditional LMs often struggle: timely data updates may cause predictions to change considerably and past data to be irrelevant.

## 2 Related Work

Information retrievalReliable and accurate predictions are largely dependent on the information available to the predictor. This is especially the case of LMs, which are trained on data preceding a knowledge cutoff and have been shown to perform better with information retrieval .

Language models model the likelihood \(p_{}(y_{i}|x,y_{<i})\) for input sequences \(x\) and target sequences \(y\). Retrieval-augmented generation (RAG)  proposes augmenting this approach with non-parametric memory, i.e. retrieving the top-\(k\) text documents \(z\) via \(p_{}(z|x)\) and conditioning the generator on

[MISSING_PAGE_FAIL:2]

Reasoning and Tools for Forecasting

Forecasting is a complex task solving environment, for which we would like to where we leverage a frozen LM \(p_{}\) as reasoning. Successful forecasting agents rely on the most up-to-date information, and accordingly operate as agents that collect observations \(_{t}\) and take actions \(_{t}\). The observation space \(\) is natural language, as collected from the prompt itself or information on the internet. The agent's actions are distributed according to \(_{t}(_{t}|_{t})\), where \(_{t}=(_{1},_{1},,_{t-1},_{t-1})\) is the context to the agents.

Our proposed approach \(\) satisfies the following criteria:

* It is **simple, scalable, and time-invariant**. As we consider different datasets of forecasting questions or language models at least as capable as the current state-of-the-art, we would like our approach to work at least as well.
* It can produce comprehensive responses through zero-shot prompting from factual information, which can be used to **reliably support decision-making** in downstream scenarios.
* These responses should be **consistent**, i.e. they should correctly synthesize the up-to-date information the model collects.

It's shown that CoT prompting, even with in-context examples, can iteratively hallucinate to produce incorrect responses on complex tasks . CoT satisfies (i) but neither (ii) nor (iii). We find that CoT's lack of interaction with the environment (i.e. sole reliance on its training data) limits its reasoning abilities and over-emphasizes irrelevant information. Yao et al.  proposes ReAct for this setting: \(=\{,,\}\), and observations \(_{t}\) from search and lookup are collected from \(\) Wikipedia web API. The context is then augmented a thought \(}_{t} p_{}(}_{t}|_{t})\) that composes information about the existing context. This method has shown to significantly enhance the model's ability to refine its responses continuously, reducing the likelihood of erroneous outputs due to lacking critical context information. Vanilla ReAct satisfies (i); as part of our framework, we show that it can additionally satisfy (ii) and (iii).

Hierarchical planningWe define \(\) by an aggregate of a collection of hierarchical ReAct agents with tools for real-time data retrieval and simulation, expanding \(\)'s observations \(_{t}\) collected from \(\) Google Search API and Python interpreter.

We propose hierarchical ReAct planning, where a LM agent acts as a high-level planner for handling abstract logic and forecasting principles based on the outputs collected from the low-level agents (Figure 1). When LLMs handle API directly with individual agents, it can consume a large portion of the context window. We delegate the reasoning and API calling to specialized agents to enhances efficiency, conserves tokens, and allows for more complex operations. The high-level agent interacts with the low-level agent by invoking it as a tool. We wrap API tools with another ReAct agent to form the low-level agent, which significantly increases API call success rates due to its self-correction mechanism . Both classes of agents are implemented with GPT-4o backbones.

## 4 Experiments

### Setup

Models and dataJin et al. , Zou et al.  have proposed forecasting benchmarks to assess models' forecasting abilities, simulating forecasting by leveraging that models are only trained up to a cutoff date. However, these benchmarks, consisting of questions that resolved in 2022, are now outdated for evaluating the performance of models such as GPT-4o due to answer leakage in training data (knowledge cutoff October 2023; see Appendix A.1).

We curated the dataset on April 15, 2024, when we scraped the platform for questions resolving within the next two weeks and corresponding human crowd predictions. We then filtered out vague questions, and ran every prediction method on these questions, enabling a fair comparison between each method and the human crowd. To prevent answer leakage from the Google API, we set the search range to prior to this date. Our final dataset consisted of 201 questions spanning across 9 diverse categories (see Appendix B).

None of our baselines have direct access to prediction market data, and empirically we found that this information was never scraped via Google search. That is, the prediction given by the ensemble of agents relies on only the agents themselves, with no human crowd influence. (By contrast, if deployed in the real world, this approach could benefit from incorporating the current human crowd performance as an input to the prediction due to the wisdom-of-crowds effect. Indeed, we observe in our experiments that human crowds are fairly well-calibrated.)

Performance metricsOur \(n\) forecasting questions have true outcomes \(o_{i}\{0,1\}\) and probabilistic forecasts \(f_{i}\). We evaluate our forecasts using Brier scores , i.e. \(_{i=1}^{n}(f_{i}-o_{i})^{2}\), and accuracy, i.e. \(_{i=1}^{n}\{\{f_{i}>0.5\}=o_{i}\}\).23 In case LMs decline to give numerical answers, the question is dropped over all methods when evaluating scores.

BaselinesIn Table 1, we compare RTF ensemble to multiple baselines: (a) crowd scores given by the current traded values on Manifold Markets (see Appendix A.2), (b) scratchpad prompting, ensemble, and fine-tuning , and (c) base models from different providers.

### Results and Observations

Table 1 demonstrates that RTF significantly improves over CoT and scratchpad with fine-tuning. We also achieve comparable Brier score (0.169 vs. 0.172) and superior accuracy (73.9% vs. 73.8%) compared to human predictors using the median and mean of our ensemble, respectively.

We also demonstrate that ensembles for RTF yield better performance than individual agents (Brier 0.169 vs. 0.180). However, this is not the case for base LMs (Brier 0.218 vs. 0.210 for GPT-4o). Base LMs tend to produce higher-variance outputs (standard deviation in ensemble size 4 of 0.150) compared to our better-calibrated ReAct agents (standard deviation in ensemble size 3 of 0.092), which satisfied (iii) as defined in Section 3.

Ensembles only contribute to the final performance if each ensemble member is already sufficiently calibrated. Indeed, Brier scores given by randomly sampling our ReAct ensemble outputs, "React Sampled" in the table, achieved a score of 0.180, far better than was achieved by any of the base methods (which, aside from GPT-4o, perform worse than guessing 0.5 every time by Brier score).

Ablation studyTo demonstrate the effectiveness of our introduced components, we conduct the ablation study. We showed each component is necessary for the fully functioning RTF framework.

* **ReAct:** RTF itself without adequate guidance from ReAct struggles to properly use the tools provided by our low-level agents, which leads to misguided lines of reasoning that cascade downstream. This is consistent with the observation (B) in , where groundedness and trustworthiness come at the cost of higher reasoning error rates.
* **Hierarchical Planning:** Empirically, without the cooperation of high- and low-level agents, a single agent fails to call APIs and perform necessary reasoning, as it exhausted available tokens on API schemas. In our experiments, the single-agent approach frequently encountered time-out errors or exceeded rate limits when handling complex queries.

Qualitative analysisWhile the baselines systematically evaluate multiple considerations, they do not consider interactions between these considerations. Empirically, we find in our samples that the prompting style we present is useful in generating a wide variety of arguments and providing reasonable estimates for how to weight each of those arguments. On the other hand, we see that this same prompt GPT-4o directly does this calibration in a sequential manner to update its final estimate, which may result in over- or under-estimate based on the recency of its considerations. In general, we find that RTF yield human-like reasoning trajectories, showing the robustness of interactive decision making, supporting goal (ii) from Section 3 (see Appendix D).

Calibration indexIn Table 2, we evaluate our methods by calibration index, which compares binned forecast probabilities to observed outcomes. A well-calibrated model means that if a forecast predicts an event with a certain probability, the event should occur approximately that fraction of the time over many predictions.

We calculate the calibration index as

\[CI=_{k=1}^{K}N_{k}(f_{k}-o_{k})^{2},\]

where \(N\) is the total number of forecasts, \(N_{k}\) is the number of forecasts in bin \(k\), \(f_{k}\) is the mean forecast probability in bin \(k\), and \(o_{k}\) is the observed probability with which events occur in bin \(k\). We select bins as the \(K\)-quantiles of the forecasts.

Comparing GPT-4o and React Mean, we see a significant decrease in calibration index (0.0194 vs. 0.0129), which shows that ensembling with ReAct not only increases forecasting accuracy, but also more accurately measures the specific magnitudes with which events occur.

## 5 Conclusion

We present Reasoning and Tools for Forecasting, a framework to leverage LMs' reasoning capabilities by interacting with the latest information. It is competitive with the predictive capabilities of human forecasters on forecasting platforms. The RTF synthesizes information through a structured decision-making process, ensuring that the predictions are both current and relevant. Additionally, while previous work has shown that ensembling can improve prediction accuracy, a carefully calibrated smaller set of models is often more cost-effective than larger ensembles. By advancing LMs' abilities to reason and dynamically interact with new data, RTF offers a robust tool for real-world decision-making for tasks like forecasting.

LimitationsThe evaluation dataset is based on prediction market data and popular questions rather than domain-specific questions. This facilitates a comparison with crowd prediction performance, but may not fully capture the nuances of more specialized domains. In addition, the Google Search API retrieves a list of URL links, titles, and text snippets, but lacks nuanced, context-specific understanding. As we can observe qualitatively (see Appendix D.2), interpreting incoherent observations from the API can be challenging.

AcknowledgmentsWe appreciate the inspiration from Prof. Jacob Steinhardt's amazing forecasting class at UC Berkeley. We thank OpenAI for granting API credits.