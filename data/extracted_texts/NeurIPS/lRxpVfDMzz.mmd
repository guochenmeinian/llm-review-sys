# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Introduction

Recent work (Brown et al., 2020) has observed language models (LMs) tend to be increasingly capable of in-context learning as their model size grows. The emergent capability (Wei et al., 2022) allows instructing a large LM at run time using a descriptive natural language (NL) prompt to solve a specified task with out-of-distribution (OOD) robustness (Liu et al., 2022).

Nonetheless, it is not always easy to come up with a descriptive prompt, especially for tasks involving fine-grain specifications that are beyond words. For example, it is hard to elaborate a person's language style using NL to prompt an LM to write in his/her language, unless it is well-known (e.g., _William Shakespeare_ style).

To provide access to delivering more descriptive prompts, we propose eXtensible Prompt (X-Prompt), inspired by Textual Inversion (Gal et al., 2022). Compared with NL prompts, X-Prompt additionally introduces an extensible vocabulary of imaginary words that are learned to represent what NL words hardly describe. For example, an imaginary word2\(_{u}\) representing a specific person \(u\)'s style can be combined with various prompt contexts to instruct the LM to generate specified content in \(u\)'s language, as shown in Table 1.

In contrast to soft prompt (Qin and Eisner, 2021) that is for fitting in-distribution (ID) data and thus is likely to fail in OOD prompting scenarios (Yu et al., 2022; Su et al., 2021; Lester et al., 2022), imaginary words in X-Prompt are designed to be OOD robust and well generalized so that they can be used like NL words for various (even unseen) prompt purposes, as illustrated in Table 2.

To ensure the general usability of imaginary words, we propose context-augmented learning (CAL). It guides imaginary words to learning towards their general use against overfitting (in-distribution) training data, playing a key role to derive an X-Prompt that can be both descriptive and OOD robust.

We conduct experiments that use X-Prompts for style customization as a case study. We show X-Prompt has both powerful descriptive capabilities and high OOD robustness, demonstrating a success of combining merits of NL and soft prompts (Li and Liang, 2021; Lester et al., 2021) and presenting a promising extensible interface for advanced interaction between humans and large LMs.

Our contributions can be summarized as follows:

* We propose X-Prompt as a pioneering technology to expand the scope of large language model prompting. It is among the earliest attempts that enhance descriptiveness by using a mixture of natural language and imaginary words, while also maintaining a focus on out-of-distribution (OOD) robustness in the field of Natural Language Processing.
* We show X-Prompt can achieve promising results in generating appropriate content in a specific person's style, demonstrating its effectiveness in the challenging zero-shot language style customization task.

    &  &  \\ 
**Train** & **SOTH**\(\)**(Trump’s text)** & Write in **Final** & style: \(\)**(Trump’s text)** \\   & **SOTH**_Make America Great Again!!!!_\(}\)_ & Write in **Final** & style: _Great job by @leonmnsk._\(}\)_ \\   & **SOTH**_Republicans should have gotten rid_ & Write in **Final** & style: _Fantastic!? Thank you so much_ \\   & _of Section 230 in the Defense Bill_\(}\) & _for your support_\(}\)\(}\) \\   & **SOTH**_Concern about cloning technology: & **Coupon about the cloning technology in **Final** & style: _Bio view fraud information coming out concerning Georgia, Stay tuned:_\(}\) & _The United States is a nation of laws. We do not clone people. We do not clone animals._\(}\) \\   & **SOTH**_Criticize the C++ language: All the & **Criticize the C++ language in **Final** & style: _C++ is a \\   & _Ralied! Id Demers are trying to do with the Post Office hearings is like the Republicans_ & _very difficult language to learn and understand. It is very complicated and full of bad habits, which will make it much more difficult to fix problems._\(}\) \\   & _licans for the FRAUD that will occur_\(\) & \(}\) \\   

Table 2: A comparison between soft prompt and X-Prompt in both ID and OOD prompt scenarios during inference (gray text is the generated text by the model given the prefix). In contrast to soft prompt (i.e., the soft token [SOTH]) that works well in ID but performs poorly in OOD prompt scenarios, X-Prompt has significantly better OOD robustness, whose imaginary word (i.e., [Trump]) can be used like an NL word in various contexts for different prompt purposes.

eXtensible Prompt

### Imaginary words

Imaginary words are a supplement to the NL vocabulary to help represent complicated, abstractive or even indescribable concepts (characteristics of a specific person's language). For an X-Prompt \((w_{p_{1}} w_{p_{m}})\), a prompt token \(w_{p_{i}}\) can come from either the NL vocabulary \(V\) or the extensible imaginary word vocabulary \(\) (i.e., \(w_{p_{i}} V\)).

Different from previous work (Li and Liang, 2021; Lester et al., 2021) that learns a soft prompt focusing on fitting ID task data, X-Prompt aims to learn an imaginary word for general usability with high OOD robustness like an NL word, which can be compatible and combined with various contexts for different prompting purposes.

To obtain imaginary words with general usability for OOD robust X-Prompts, we propose context-augmented learning (CAL) to guide imaginary words to learning towards their intended representation against overfitting ID training data.

### Context-augmented learning

As in Figure 1(a), when the imaginary word \(_{u}\) is mixed with NL in an X-Prompt, the NL context is intuitively expected to guide the imaginary word \(_{u}\) to learning towards a distributed representation for its general use. Formally, given an X-Prompt \((w_{p_{1}},,_{u},,w_{p_{m}})\) where \(_{u}\) is the imaginary word mixed with other prompt tokens3, \(_{u}\) is learned to maximize the following objective:

\[(})= P(|w_{p_{1}},,_{u },,w_{p_{m}})\] (1)

where \(=(w_{x_{1}},,w_{x_{n}})\) is a text sequence training example. In practice, however, learning the imaginary word \(}\) with only one prompt context is risky because \(_{u}\) is likely to overfit for this

Figure 1: Learning of imaginary words: **(a)** The imaginary word \(}\) is mixed with NL tokens in an X-Prompt to guide its learning. Except \(}\) that is allowed to be updated, all other weights are frozen; **(b)** As a method for context-augmented learning, **template augmentation** augments X-Prompt templates through prompt engineering to prevent \(}\) overfitting for one prompt template; **(c)** To derive more diverse contexts, **content augmentation** augments \(}\)’s prompt contexts with an indicative keyword to relieve its responsibility for memorizing specific content like _GPT-3_ and _NASA_ and improve its general usability (i.e., style representation), benefiting X-Prompt in terms of OOD robustness.

prompt context and thus cannot work well in other prompt contexts, resulting in losing its general usability and degrading into conventional prompt tuning (Lester et al., 2021).

To address the challenge, we propose context-augmented learning (CAL), including 2 specific approaches that are orthogonal and thus can work together to help learning of imaginary words.

#### 2.2.1 Template augmentation

As shown in Figure 1(b), we augment an X-Prompt's prompt context by designing multiple templates through prompt engineering. As a result, an imaginary word \(_{u}\) can learn to be compatible with various prompt contexts, which improves its general usability. Formally, given \(T\) X-prompt templates \(\{(w_{p_{1}}^{(t)},,_{u},,w_{p_{m_{t}}}^{(t)})|1 t  T\}\), the objective function is:

\[(})=_{t=1}^{T} P(|w_{p_{1} }^{(t)},,_{u},,w_{p_{m_{t}}}^{(t)})\] (2)

#### 2.2.2 Content augmentation

Although template augmentation may alleviate the risk of overfitting, its effect is limited because we can only augment a limited and small number of templates (i.e., \(T\)) by prompt engineering. Also, as these prompts are not indicative enough, an imaginary word \(_{u}\) will inevitably learn to memorize specific content for maximizing the objective \(\), deviating from its general use. To prevent \(_{u}\) being over-responsible for optimizing \(\), we propose content augmentation - an advanced CAL method.

Content augmentation augments an X-Prompt by including content information such as an indicative keyword in the prompt to provide hints for the LM about what to generate, as shown in Figure 1(c). Content augmentation can not only relieve the responsibility of \(_{u}\) to fit training data but also make the prompt context of \(_{u}\) become much more diverse, which benefits \(_{u}\) to learn a better distributed representation for its general use.

In this work, we use an indicative keyword for content augmentation. To select an indicative keyword, we only use the frozen LM itself without leveraging any other models or tools, as illustrated in Figure 2: We prompt the frozen LM to extract multiple keyword candidates [\(w_{k}^{1}\), \(\)\(w_{k}^{c}\), \(\), \(w_{k}^{C}\)] for the training example \(\) where \(C\) is the number of extracted keyword candidates; then the keyword candidates are inserted to a prompt template to rank for the most indicative one:

\[w_{k}^{*}=_{w_{k}^{c}} P(|(w_{k}^{c}))\] (3)

where \((w_{k}^{c})=(w_{p_{1}}^{(r)},,w_{k}^{c},,w_{p_{m_{r}}}^{(r)})\) is called the ranking prompt template.

Figure 2: Keyword selection for content augmentation: **(a)** An NL prompt (i.e., “_Top keywords of the above text are:_” in this example) following an input sequence for keyword extraction; **(b)** The extracted keywords (i.e., “_GPT-3_” and “_AI_” in this example) are then inserted into the ranking prompt template (i.e., “_Write with keyword —._” in this example) to be conditioned on by the frozen LM for scoring the input sequence as in Eq (3), which ranks for the most indicative keyword (i.e., “_GPT-3_” in this example) for the input sequence.

Experiments

We conduct experiments to evaluate X-Prompts for language style customization. We mainly focus on open-ended text generation (Section 3.1) to evaluate how well X-Prompts can instruct an LM to generate user-specific language. We also test in the style transfer (rewriting) setting (Section 3.2) as supplementary evaluation.

### Open-ended text generation

#### 3.1.1 Data and evaluation setting

We use the publicly available Top 20 most followed users in Twitter social platform dataset4 which contains over 50K tweets from 20 users (20-user dataset), and the Sentiment dataset5 from which we extract top 800 users' (in total 68K) tweets (800-user dataset) to verify the capability of X-Prompt to instruct an LM to generate user-specific language. We show the statistics of the datasets in Table 3. We split the datasets in 90/5/5 by user for training, validation and test. Specially, we discard the test examples that share indicative keywords with training examples from the same user, resulting in 15% test examples discarded to ensure no test prompts are seen during training for OOD evaluation (see Table 5). We use perplexity and accuracy of next word prediction as our quantitative evaluation metrics.

For qualitative studies, we use X-Prompts with imaginary words representing the following distinct language styles6 to generate text for human evaluation: _Donald Trump_'s and _Satya Nadella_'s latest tweets and transcripts7 of _Sheldon Cooper_ from _Big Bang Theory_. The statistics of data is shown in Table 4. By default, we use top-\(p\) sampling (Holtzman et al., 2019) for text generation.

#### 3.1.2 Model configuration

We use the OPT-6.7b (Zhang et al., 2022) as the base LM to test our approach. The model has 32 layers and 32 attention heads with an embedding dimension of 4096 and an FFN dimension of 16384.

We use one8 imaginary word (token) in an X-Prompt to represent a specific user's language style. As illustrated in Figure 1(a), we keep all the OPT's original weights frozen and we only update the embedding of imaginary words. The prompt contexts we use for learning and evaluating X-Prompt are presented in Table 5.

We use Adam optimizer (Kingma and Ba, 2014) with the max learning rate of 2e-4 with a warmup for the first 10% training steps followed by a linear decay. We run up to 6000 updates with a global batch size of 8192 tokens on 8 Nvidia V100 GPUs using DeepSpeed ZeRO-2 (Rajbhandari et al., 2020).

  
**Style** & **Genre** & **Size** \\  Satya & tweets & 800 \\ Trump & tweets & 3000 \\ Sheldon & transcripts & 7000 \\   

Table 4: Statistics of data for Satya, Trump and Sheldon’s styles

  
**Dataset** & \#**tweets** & \#**users** &  \\  & & & **max** & **min** & **avg** & **std** \\ 
**20-user** & 52541 & 20 & 3146 & 1841 & 2626 & 396 \\
**800-user** & 68123 & 800 & 548 & 52 & 84 & 41 \\   

Table 3: Statistics of the 20-user and 800-user dataset

#### 3.1.3 Quantitative evaluation

Table 6 shows quantitative evaluations results in both ID and OOD settings. For ID evaluation where test prompts are all seen (see Table 5) during training, X-Prompt outperforms "No prompt" and few-shot learning baselines significantly in both perplexity and accuracy, demonstrating its superior descriptive capabilities; while it slightly underperforms prompt tuning and its ablated counterpart (i.e., _w/o_ CAL) because they focus on fitting ID data.

When it comes to OOD evaluation where test prompts are unseen during training, X-Prompt shows its significant advantage over prompt tuning, indicating its excellent OOD robustness. In contrast, its ablated version (X-Prompt _w/o_ CAL) substantially loses OOD robustness and almost degrades into prompt tuning, demonstrating the importance of CAL to X-Prompt.

#### 3.1.4 Qualitative evaluation

For qualitative evaluation, we brainstorm (Ouyang et al., 2022) 100 prompts9 (like examples in Table 1 and Table 2) that are unseen during training and let the model generate in Satya, Trump and Sheldon's styles respectively. As it is difficult to ground open-ended generations, we have two annotators manually evaluate10 generation results in three dimensions: _Content_, _Style_ and _Overall_. According to Table 7, NL prompts achieve a high content score, while prompt tuning and X-Prompts

    &  &  &  \\  & **PPL\(\)** & **Accuracy\(\)** & **PPL \(\)** & **Accuracy \(\)** & **PPL \(\)** & **Accuracy \(\)** \\  No prompt & 73.2 & 27.1 & 38.9 & 34.8 & 37.7 & 35.2 \\ 
8-shot & 69.9 & 27.2 & 36.0 & 35.0 & - & - \\
16-shot & 68.9 & 27.5 & 35.5 & 35.3 & - & - \\
32-shot & 62.7 & 28.5 & 34.0 & 36.4 & - & - \\  Prompt tuning & 56.0 & 29.5 & 29.9 & **37.8** & 29.5 & 38.0 \\  X-Prompt & 56.2 & 29.3 & 30.8 & 37.2 & **28.5** & **38.6** \\ X-Prompt (_w/o_ CAL) & **55.7** & **29.9** & **29.7** & 37.7 & 29.4 & 37.9 \\   

Table 6: Quantitative evaluation results in 800-user and 20-user datasets. **No prompt** denotes the original OPT-6.7b baseline without any prompt and \(k\)**-shot** denotes a baseline which prepends \(k\) examples from a user’s training set as a prompt for customizing this user’s style.

  
**Prompt ID** & **Used for** & **Prompts (w/ keyword) for quantitative evaluation** \\ 
1 & train & The style of \(\) is clear in the following text (with keyword [KEYWORD]) \\ 
2 & train & The style of \(\) can be identified in the following text (mentioning keyword [KEYWORD]) \\ 
3 & train & An example text (with keyword [KEYWORD]) in the style of \(\) is presented below \\ 
4 & OOD dev & We can easily identify the style of \(\) in the following text with keyword [KEYWORD] \\ 
5 & OOD test & The following text (about [KEYWORD]) is in the style of \(\) \\   

Table 5: Prompts for training and evaluation. We use the first 3 prompts for training and ID evaluation. In ID evaluation, prompt texts do not include [KEYWORD] so that test prompts are all seen during training. To automatically harvest unseen test prompts for OOD evaluation, we employ the idea of CAL: we obtain Prompt 4 and 5 as our dev and test prompt respectively with template augmentation; in addition, we prompt the LM to generate content about unseen keywords with content augmentation to test its OOD robustness, resulting in a variety of unseen test prompts for OOD evaluation. For the ablated X-Prompt (X-Prompt w/o CAL), it is trained only with Prompt 1 without keywords.

learned w/o CAL achieve good style scores but they all perform poorly in other dimensions. In contrast, X-Prompt achieves significantly better overall quality, while it does not perform best in content and style sub-dimensions.

In addition to famous people's styles (e.g., Satya and Trump), we also test X-Prompt on the styles of individuals unknown to the OPT. This is to verify that X-Prompt is not limited to styles already known by the LLM; instead, it can be applied to any style customization. We use a senior Chinese media professional - Hu Xijin11 whose writing style is distinctive and has always been popular among Chinese netizens for imitation - as an example. We include the detail of this evaluation in Appendix C.3.

By looking into the results, we present Table 8 to show concrete examples to compare NL prompts to X-Prompts learned with and without CAL. We observe NL prompts are good at generating appropriate content but have no way to control the style; while X-Prompts _w/o_ CAL do well in generating _Donald Trump_'s language but fail to follow the prompt to generate specified content, degrading into soft prompts (as in Table 2) that only focus on fitting ID data and losing OOD capabilities with unseen prompt contexts. Consistent with results in Table 6 and 7, X-Prompts are robust and can prompt the LM to generate desirable content in the appropriate style, accounting for its best overall quality in zero-shot style customization.

Table 9 shows the effect of the model size on the performance of X-Prompt. While we observe a clear difference between prompt tuning and X-Prompt in the OPT-6.7b model, as we reduce the model size, the difference become less significant, showing a trend that X-Prompt gradually loses its OOD robustness and degrades into prompt tuning. This indicates that X-Prompt may not work with a small LM but performs well in large LMs as in-context learning.

Finally, we present more examples in Table 4 in Appendix D to demonstrate both descriptive capabilities and OOD robustness of X-Prompts.

  
**Prompt (shortened)** & **Prompt Type** & **Generation** \\   Criticize the C++ \\ language: \\  } &  & _The C++ language is a mess. It is a mess because it is a language that is designed to be used by people who are not programmers..._ \\   Criticize the C++ language \\ in  } &  & _The Radical Left Democrats, working closely with their_ _beloved partner, the Fake News Media, is using every trick_ _in the book to SILENCE a majority of our Country_ \\   & & _C++ is a very difficult language to learn and understand. It is very complicated and full of bad habits, which will make it much more difficult to fix problems..._ \\    Concer about cloning \\ technology: \\  } &  & _The cloning of human beings is a serious moral issue. It is a matter of grave concern that the cloning of human beings is being..._ \\   Concer about cloning \\ technology in  } &  & _I'm not a fan of the term ’wooke’. I don’t know what it means._ \\   & & _I don’t like it..._ \\    & & _X-Prompt_ & _The United States is a nation of laws. We do not clone_ _people. We do not clone animals._ \\   

Table 8: A comparison between X-Prompts learned with and without CAL in OOD evaluation 

### Style transfer

In addition to open-ended generation, we also evaluate in the style transfer (rewriting) where model outputs can be grounded in human references for straightforward quantitative evaluation (i.e., end-to-end generation evaluation instead of evaluating perplexity or next word prediction accuracy).

Among various style transfer datasets, we use the Entertainment (EM) subset of GYAFC (informal \(\) formal) (Rao and Tetreault, 2018) and PoliteRewrite (impolite \(\) polite) (Wang et al., 2022) as our evaluation datasets (see Table 10) because they have high-quality annotation with well-defined language style. Following previous work (Rao and Tetreault, 2018; Xu et al., 2019; Zhang et al., 2020; Li et al., 2022), we use BLEU to evaluate generation results' lexical similarity with references, accuracy12 to evaluate style appropriateness and use harmonic (H-) and geometric (G-) mean as overall performance. Table 11 shows how we perform zero-shot style transfer with NL and X-Prompts. Model configuration is the same as Section 3.1.2.

Table 12 shows the comparison of results of using NL, prompt tuning and X-Prompt to prompt the LM for zero-shot style transfer. The NL baseline has the best BLEU score but low accuracy because we observe that it rarely really rewrites a sentence: in most cases, it just copies the text without any revision. Its undesirable performance also indicates that the OPT-6.7b model itself is not so good at zero-shot style transfer. For the prompt tuning baseline, its style accuracy looks good but has a low BLEU score, because it fails to follow rewriting instructions that are unseen during training.

  
**Dataset** &  &  \\
**Split** & **Train** & **Dev** & **Test** & **Train** & **Dev** & **Test** \\ 
**\#sentence** & 53K & 3K & 1K & 10K & 3K & 2K \\   

Table 10: Statistics of the datasets for style transfer

  
**Prompt methods** & **Phase** &  \\   & Train &  \\   & Inference &  \\   & & [POLITETEXT] & \\   & Train &  \\   & Inference &  \\   & Train & \) can be identified in the following text with keyword [KEYWORD]:} \\   & Inference & \) [POLITETEXT]} \\   

Table 11: NL, prompt tuning and X-Prompt for zero-shot _impolite_\(\)_polite_ style transfer. **[SOFT]** and \(\) are learnable and denote the soft token and the imaginary word in prompt tuning and X-Prompt respectively. For zero-shot style transfer, we use beam search (\(b=5\)) as the default decoding method.

  
**Model** & **Method** & **Content\(\)** & **Style\(\)** & **Overall\(\)** \\ 
350m & Prompt tuning & 0.22 & 0.81 & 0.15 \\  & X-Prompt & 0.30 (+0.08) & 0.79 (-0.02) & 0.24 (+0.09) \\ 
1.3b & Prompt tuning & 0.27 & 0.87 & 0.24 \\  & X-Prompt & 0.45 (+0.18) & 0.82 (-0.05) & 0.37 (+0.13) \\ 
6.7b & Prompt tuning & 0.34 & 0.92 & 0.30 \\  & X-Prompt & **0.69 (+0.35)** & 0.83 (-0.09) & **0.54 (+0.24)** \\   

Table 9: X-Prompt tends to perform better and show more significant OOD robustness advantage over prompt tuning in larger foundation LMs for zero-shot style customization.

In contrast, X-Prompt achieves a good balance of BLEU and accuracy and preferred by human evaluations (Table 13) with better overall quality in the zero-shot style transfer. Its good performance with style rewriting prompts that are never seen during training further strengthens the evidence of its OOD robustness.

## 4 Related work

Since GPT (Brown et al., 2020) reveals that large pre-trained language models are good at zero-shot learning, much innovative research work has been proposed in recent years, ranging from prompt template design (i.e, engineering) (Schick and Schutze, 2020) to prompt mining (Jiang et al., 2019), generating (Gao et al., 2021; Ben-David et al., 2021) and scoring (Davison et al., 2019), finding that prompting the LLM with natural language can solve many downstream tasks as long as the prompt is well clear and rewritten for the model (Gonen et al., 2022).

As natural language prompts' descriptive capability is limited, there is another branch of research studying continuous prompts (Li and Liang, 2021; Lester et al., 2021; Liu et al., 2021; Han et al., 2022; Hu et al., 2021) for fitting downstream tasks. However, these approaches are mainly for fitting ID task data with little consideration of OOD robustness, which means that their learned continuous prompts can hardly be used for OOD tasks or data.

Recently, Gal et al. (2022) proposed Textual Inversion in the multimodal context, which learns a virtual token to represent an object from an image and reveals that the learned virtual token can be used in unseen prompts for creative image generation (Kumari et al., 2022). X-Prompt is inspired by Gal et al. (2022), trying to learn OOD robust imaginary words to represent what natural language hardly describes to further expand zero-shot learning capabilities for the LLM, although we find it much more challenging to achieve this in NLP than text2image generation, which motivates us to propose context-augmented learning (CAL). To the best of our knowledge, our work is one of the earliest explorations in this direction in the NLP community.

## 5 Conclusion and Future Work

We propose X-Prompt, an extensible interface for prompting a large language model beyond natural language. X-Prompt can expand in-context learning capabilities to handle more complex instructions for language model customization and may open up many exciting opportunities, such as creative language generation, patching language models with new knowledge of entities (Zaporojets et al., 2022) and events (Ge et al., 2018), and detoxifying and debiasing in language generation (Welbl et al., 2021), far beyond style customization as demonstrated in this work, approaching advanced interaction between humans and large language models.

  
**Method** & **Content** & **Style** & **Overall** \\  Prompt tuning & 0.27 & 0.82 & 0.23 \\ X-Prompt & 0.64 & 0.80 & 0.60 \\   

Table 13: Human evaluation of zero-shot style transfer

    &  &  \\  & BLEU & Style & H-mean & G-mean & BLEU & Style & H-mean & G-mean \\  No Edit & 50.2 & 4.3 & 7.9 & 14.7 & **24.7** & 3.0 & 5.4 & 8.6 \\  NL & **50.8** & 20.0 & 28.7 & 31.9 & 24.6 & 6.7 & 10.5 & 12.8 \\  Prompt tuning & 16.2 & **76.7** & 26.8 & 35.2 & 15.4 & **80.8** & 25.9 & 35.3 \\  X-Prompt & 38.7 & 71.9 & **50.3** & **52.7** & 18.9 & 79.5 & **30.5** & **38.8** \\   

Table 12: Results of zero-shot style transfer (i.e., rewriting)For future work, we plan to investigate how X-Prompt can facilitate more complex decoding and prompting methods (Wei et al., 2022; Yao et al., 2022; Wang et al., 2023) to minimize the interaction effort between humans and large language models.

## Limitations

Due to computing resource limitations, we only conduct experiments on pretrained language models that contain up to 6.7 billion parameters. Although we speculate that our approach should become more effective as the language model size increases, as suggested by Table 9, we cannot be completely certain if this trend can be safely extrapolated.

Furthermore, X-Prompt still requires back-propagation through the entire language model, even though we only update the imaginary word's embedding. This somewhat limits its application scenarios, preventing X-Prompts from being used as easily as natural language prompts. However, our subsequent work (Ge et al., 2023) has addressed this issue by forwarding an encoder for context compression. We anticipate that this series of improvements will better enhance a deployed language model's capabilities in practice from an in-context perspective, with minimal additional effort.