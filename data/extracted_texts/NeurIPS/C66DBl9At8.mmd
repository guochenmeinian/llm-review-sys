# Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the _New York Times v. OpenAI_ 2023 lawsuit

Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the _New York Times v. OpenAI_ 2023 lawsuit

Joshua Freeman

ETH Zurich

Chloe Rippe

Duke University

Edoardo Debenedetti

ETH Zurich

Maksym Andriushchenko

EPFL

###### Abstract

Copyright infringement in frontier large language models ("LLMs") has received much attention recently due to The New York Times Company v. Microsoft Corp., filed in December 2023. The New York Times claims that GPT-4 has infringed its copyrights through reproducing articles for use in LLM training and by memorizing the inputs and thereby publicly displaying them in LLM outputs. This research attempts to measure the propensity of OpenAI's LLM to exhibit verbatim memorization in its outputs relative to other LLMs, specifically focusing on news articles. LLMs operate on statistical patterns, indirectly "storing" information by learning the statistical distribution of text over a training corpus. We show that OpenAI models are currently less prone to the elicitation of memorization than either Meta, Anthropic, or Mistral. We also find that the bigger the model, the more memorization we can elicit, particularly for models with more than 100 billion parameters. Our findings have practical implications for training: more attention must be put on preventing verbatim memorization for bigger models. Our findings also have legal significance: in assessing the relative memorization capacity of OpenAI's LLM, we probe the strength of The New York Times' copyright infringement claims and OpenAI's legal defenses while underscoring issues at the intersection of generative artificial intelligence ("AI") and law and policy more broadly.

## 1 Introduction

The generative AI market has grown quickly since OpenAI released its LLM ChatGPT in 2022 . Competitors have emerged in the LLM space, and they follow a similar approach to training: feeding multi-billion-parameter LLMs massive corpora of data scraped from the internet. Text generation LLMs are thus trained on scraped text data to develop a capability to generate plausible text responses to user queries. A key phenomenon observed in LLMs is verbatim memorization. When a model has memorized a text, it is possible to extract that exact text (or a close variant) by prompting it. This often happens with state-of-the-art models, like in Carlini et al. . LLM training sets often contain copyrighted material. For instance, EleutherAI's PILE dataset  is an 875 GB diverse and open-source dataset designed for training language models. The PILE's Books3 section contains copyrighted books . Although the reproduction and public display of this copyrighted content may infringe the rights of copyright owners, public-facing LLMs may be accessed by millions of users. Therefore, legal risk is associated with "how memorized" given copyrighted material is within an LLM: If it is reasonably easy to extract memorized text from the LLM, the LLM could effectively reproduce or publicly display text ingested during training, in violation of the copyright owner's rights.

As suggested above, the emergence of LLMs, and their alleged propensity to memorize, has spurred numerous lawsuits. The ongoing The New York Times Company v. Microsoft Corp. lawsuit was the first in a series of cases testing the boundaries of copyright law in the face of generative AI. Other relevant cases include: Chabon v. OpenAI, Inc., a class action lawsuit waged by authors alleging that OpenAI's use of their works to train its models constituted copyright infringement; and Doe v. GitHub, Inc., a class action alleging that Github Codex and Github Copilot, automated software development tools, produce verbatim copies of plaintiffs' source code without abiding by terms of the code's applicable licenses, in violation of the Digital Millenium Copyright Act ("DMCA").

Of central importance to this report, in The New York Times Company v. Microsoft Corp., the New York Times claims in its complaint ("Complaint") that OpenAI has "A Business Model Based on Mass Copyright Infringement", allegedly using copyrighted materials to train its models and "disseminating" such materials through its public-facing LLM (Complaint, The New York Times Company v. Microsoft Corp., No. 23-CV-11195 [S.D.N.Y. filed Dec. 27, 2023]). Exhibit J of the Complaint highlights instances where AI systems provided near-verbatim excerpts from its articles, potentially reducing web traffic and revenue. Exhibit J includes one hundred examples of articles allegedly memorized by OpenAI's ChatGPT.

OpenAI's case filings and public statement (see OpenAI ) suggest that the New York Times misrepresents ChatGPT's propensity for verbatim memorization in its outputs. Here, we endeavor to quantify ChatGPT's memorization of news articles, guided by the following questions: How easily can we extract New York Times articles from the GPT models? Can we reproduce these findings on other model architectures?

We show that it is possible to extract some of the articles of Exhibit J of the Complaint, testing 3 different prompt injection/context manipulation attacks. These attacks, seen in figures 8 and 2, are manipulations of the LLM's input to extract an article. They are numbered in increasing order of effectiveness. With \(5\) different metrics, we measure what makes a good attack. While finding that the Lawsuit Articles are far more memorized than the baseline sample of news articles, we reproduce known results that model size increases memorization ability. We also find that a bigger prefix size does not necessarily imply longer regurgitation. We characterize defenses against copyright infringement used by different LLM providers. Pieces of highly duplicated text in the training set are more straightforward to retrieve. Outside of highly duplicated articles, copyrighted articles are highly nontrivial to extract. In summary, this paper has three main contributions:

* Curation of three sets of approximately 100 articles from the New York Times for our experiments with memorization. However, we cannot provide them publicly due to copyright concerns.
* Quantification of the claims made in the New York Times v. OpenAI lawsuit across five metrics, three experiments in increasing difficulty level, and two different parameter changes.
* Commentary on the legal implications of our findings on the New York Times v. OpenAI.

Figure 1: **Model size vs. longest common contiguous subsequence (in characters).** The amount of verbatim memorization increases significantly for larger models, especially those with more than 100 billion parameters. The error bars represent the range of \( 1\) standard deviation taken across all samples. Note that we excluded the samples that were defended by the model or by an output filter on top of it that GPT and Claude use.

Background

GPT-2 (Radford et al., 2019) was proposed in \(2019\) to have natural language processing (NLP) capabilities across multiple tasks. From the study of LLMs for their NLP capabilities emerged the study of their capacity for memorization (Carlini et al., 2020). Since then, there has been increased concern about memorization. When non-generative models are trained on copyrighted content, legal scholars, such as Lemley and Casey (2020) and Levendowski (2018), have argued that fair use doctrine should protect the use of such content. This analysis changes with the introduction of generative models capable of learning the underlying distribution of the training data, as they may regurgitate copyrighted content.

Memorization causes and metrics.Studies like Carlini et al. (2021) revealed that simple attacks could lead to retrieving verbatim training data, such as personally identifiable information. Lee et al. (2022) has shown the leading known cause for memorization to be the presence of duplicates in the training set. Chen et al. (2024) had corroborating findings. Lastly, Carlini et al. (2023) demonstrated that an attack with more context tokens is a third factor for eliciting memorization. We find Carlini et al. (2023)'s finding on the size of the context to be invalid.

Different metrics and experiments have been used to quantify memorization. Verbatim restitution of an entire article is sound proof of memorization, but there are many ways that a text can be close to complete verbatim restitution: qualitatively or quantitatively, and there are many ways to qualify or quantify said closeness. Since the 1960s, it has been necessary to quantify how close two texts are automatically for tasks like information retrieval, plagiarism detection, and computational linguistics. Different metrics existed for different reasons, some relevant to us (see section 3). The Levenshtein distance (Levenshtein, 1966) is the most basic way to quantify string proximity, measuring the minimal modifications needed to go from one string to another. BLEU (Papineni et al., 2002) was developed to evaluate machine translation in its early days automatically. It primarily focuses on how many \(n\)-grams in the generated text appear in the reference texts. ROUGE (Lin, 2004) was intended to assess the number of matching elements, including word sequences and word pairs, between a machine-generated summary under evaluation and reference summaries produced by humans. Although neither BLEU nor ROUGE was initially intended for detecting verbatim copying, they have been repurposed for this use in practice (Wei et al., 2024). BLEU is preferred for measuring how closely a generated model's output matches copyrighted text verbatim because it directly assesses the precision of \(n\)-gram matches, making it more sensitive to exact matches. This means it measures how many \(n\)-grams in the generated text are also in the reference text. ROUGE, however, is more suitable for evaluating content coverage rather than exact matches. More recently, the longest verbatim match has been used to compare how close sequences are in the context of LLM memorization (Sonkar and Baraniuk, 2024). The latter quantified differences in memorization for a specific attack setting by looking at the distributional differences in the longest continuous common substring, with approaches like Kolmogorov-Smirnov testing (An, 1933). We proceed by taking inspiration from all of these previous approaches.

Memorization mitigations.After training and instruction tuning, LLMs are further aligned with their purpose (e.g., general-purpose, task-specific, etc.) using Reinforcement Learning From Human Feedback (RLHF) (Christiano et al., 2023). This consists of humans ranking model answers via a list of criteria. This can mitigate memorization.

Some attempts to mitigate memorization include separating copyrighted or sensitive data from the rest to process it differently. One approach would be to remove this data from the training set completely, but because copyrighted or sensitive data often contain valuable, nonsensitive training data, this could lead to the overexclusion of training data. Another approach is teaching the model to forget the sensitive data (Goldatkar et al., 2020; Rafailov et al., 2023). This approach, called model unlearning, has successfully been shown to work on LLMs (Yao et al., 2024). Notwithstanding this, unlearning is still computationally expensive and could again risk overexcluding non-copyrighted data contained within copyrighted works2.

Differential Privacy (DP) is a mathematical framework in which models can be trained in a way that respects the need to keep some subset of the training dataset private. Such methods have been shown to work (Behnia et al., 2022) for finetuning LLMs while mitigating verbatim memorization. As with other methods, there is a trade off between the utility of the model and how differentially private it is. After all, the best case for privacy is not to reveal any information about sensitive parts of the dataset. But those parts of the dataset may contain non-sensitive facts. This leaves the quest for a perfect in-training memorization mitigation technique as an open research question.

To fill the need for mitigation, other, post-generation methods have been developed to fight the generation of memorized sensitive data, such as OpenAI's or Anthropic's content filters, which stop generation in real-time when sensitive data generation is detected.

Legal aspects of memorization.A legal claim of copyright infringement generally requires showing that a defendant made an unauthorized copy (a "substantially similar" reproduction) or other unlawful use of a work subject to a valid copyright. See 17 U.S.C. SS 106. The New York Times alleges that infringement of its news articles has occurred at multiple stages in the training and use of ChatGPT, including when the copyrighted articles were allegedly reproduced as training data and subsequently, when certain articles were "memorized" and regurgitated in ChatGPT outputs. Memorization in LLMs can be thought of as an application of the idea/expression doctrine in copyright law. This doctrine underscores the dichotomy between abstract ideas (which are generally not copyrightable) and the original expression of such ideas (which may be copyrightable). Though LLMs are trained to identify abstract features and relationships in training data (where the data itself might be copyrightable, but not the LLM's mathematical inferences about such data), when memorization occurs, the LLM has not adequately "abstracted" its inferences about the data, thereby increasing the risk that its outputs will infringe.

OpenAI's argument that memorization is a bug rather than a feature of ChatGPT, as supported by the research herein, could influence the court's analysis with respect to the defendants' defenses to these infringement claims. In particular, if heeded, OpenAI's stance could influence the court's analysis of fair use as a defense to direct infringement or of the New York Time's claim for contributory infringement in the context of a publicly accessible LLM. For context, fair use is a copyright law doctrine that negates a finding of copyright infringement if the court, upon balancing four statutory factors, determines the use is "fair". 17 U.S.C. SS 107. Case law, the use at issue and even policy concerns may inform this balance, making the analysis highly context specific and at times unpredictable. Furthermore, contributory infringement is a doctrine through which a product provider can be found liable for copyright infringement performed by a product user [Sony Corp. of Am. v. Universal City Studios, Inc., 464 U.S. 417, 104 S. Ct. 774, 78 L. Ed. 2d 574, 1984]. The implications of this research on these doctrines as applied to this case will be discussed in greater depth below.

This paper will not focus on certain legal issues raised in the Complaint, including the copyright implications of "synthetic search" associated with certain Microsoft Bing products. This feature enables the relevant Bing products to scrape the internet for data in real time. The disparity in ability to reconstitute pieces of the New Articles (out of the training set of the LLMs) versus articles from before the model publication ostensibly shows that none of the LLMs tested have the ability to access NYT articles in real time. We will further not focus on potential DMCA violations and the allegation that the LLM itself infringes The New York Times' copyrights. Instead, this paper will focus on copyright infringement with respect to LLM training inputs and LLM outputs that do not rely on contemporaneous internet searches.

## 3 Methodology

Data collection.First, we curate three sets of articles from the New York Times by hand. One set of articles corresponds to 99 articles from exhibit J. The other one is a set of 100 Arbitrary Articles published no later than December 27th, 2022. The intention is to make it likely that they would have been in the model's training set. Lastly, 90 articles published after the end of training of any models experimented on were included, published no earlier than July 5th, 2024. To refine the data, mentions of images or other pieces of text that were not part of the article text were removed. In addition, the title was removed, leaving only the article's text and a line mentioning the author's name(s) (i.e., By Jane Doe, John Smith, and Ada Lovelace).

Attacks.In The New York Times Company , the New York Times alleges that it submitted short subtexts of articles to GPT, thus extracting 2000 character-long near-exact matches to the articles. We were not able to reproduce those strong claims. There are two possible reasons for this.

Either the models analyzed differ from when the New York Times sued or because an element is omitted, such as a system prompt or other parts of the extraction attack. If the temperature, a setting for the stochasticity of the generation, with 0 being fully deterministic, were not 0, that would also explain the difficulty of reproducing the attacks. We took a multi-staged approach to get as close as possible to the New York Times's claims. We tried three context manipulation attacks, each building on one another, as seen in Figure 8. The first attack only contains a system prompt. The second one builds on the previous one but adds an acknowledgement message from the assistant to the system. It then adds a message from the user, prompting the assistant to start. The last attack, the closest to New York Times's Exhibit J, builds upon the previous attack but appends one more message to the history from the assistant, containing a prefix of tokens of the article. All models' original tokenizers were used except for Anthropic, for which an approximation of 4 characters for one token was made because of the lack of efficient open-source tokenizers. Attacks have been conducted using the publicly available APIs.

**Grid search study (Prefix and Model Size).** We looked for the best way to elicit memorization by varying the prefix and model sizes. We chose these based on the claims of Carlini et al. (2023). They claim that there is such a thing as a "discoverability phenomenon", in which prompting a model with a more extended prompt would imply more elicitation of memorization. They also suggest that memorization scales superlinearly with the size of a model. In attack 3, we thus looked at three different models for all four major LLM providers and five different prefix sizes (in tokens): 25, 50, 100, 200, and 400.

**Memorization metrics.** For a fine-grained quantitative analysis of memorization, we measured the similarity between two strings in \(5\) different ways. The Levenshtein distance, first introduced by Levenshtein (1966), is the oldest way the authors of this paper could find of quantifying how distant two texts are to each other. The Levenshtein distance of words \(w_{1}\) and \(w_{2}\) consists of the minimal single-letter insertion/deletion/modification needed to go from one to the other. It can be used as a way to compare two words. The apparent limit in using a naive Levenshtein distance to measure memorization is the confounding variable of expected output length, with which a naive Levenshtein distance correlates highly. We chose to normalize the distance between an LLM output and an expected output by dividing it by the length of the expected output.

Second, we measured similarity using the longest common subsequence between two strings. We chose this metric as it is commonly used to quantify verbatim memorization, particularly in exhibit J of the OpenAI lawsuit (see The New York Times Company (2023)). Although insightful when many long bouts of text are restituted, it can sometimes be uninterpretable, like in the following example. The first sequence could be "The quick brown fox jumps over the lazy dog", and the second sequence could be any superset of "a crown on a dog." The longest common subsequence between them could be "crown o a dog", which makes no intuitive sense. For this reason, we use a further metric, as illustrated in Table 1.

The longest (common) continuous substring (LCCS) length will be much smaller than the longest common subsequence. In the example above, it could be, at most, " own ". Dynamic and rolling hash

Figure 2: Our prompt-based attack #3 that involves a role play and fictional future scenario. A fake history is inserted, giving the impression to the model that it has already started outputting the article.

programming techniques can calculate this and previous metrics. The pro of this metric is that it is robust to a difference in our processing method and what the models saw. Ideally, it should probably be normalized to account for the probability of a randomly long, longest, common continuous substring for a fixed length output, growing as the expected output length grows. However, it would not make sense to normalize it by the length of the expected text: the correlation values between expected output length and LCCS are low, indicating that the random probability of a long continuous common substring grows, at most, sublinearly with the expected output length, as shown in Table 2.

We also use a semantic metric: the cosine distance between the expected and actual text embeddings. The goal behind using this metric is to try to understand how well the model's output shows its knowledge of the general idea of the article. This has been used since the early days of machine learning-based NLP, like in Mikolov et al. (2013). The distance between vector embeddings is a known marker of semantic similarity. The last metric, BLEU, was introduced to assess machine translation quality by measuring the overlap between machine-generated and human reference translations. We prioritize using word-level BLEU over token-level BLEU to ensure consistency across different models, thereby avoiding unnecessary metric variations caused by differing tokenization schemes.

## 4 Experiments

Contrary to previous findings, the amount of context tokens provided in an attack does not correlate with memorization. We also find high variance between LLM providers. OpenAI is the least memorization-prone in absolute numbers, and Anthropic (the provider marketing itself as being most focused on safety) is the most vulnerable to elicitation of memorization.

Impact of metric.As expected, different metrics show substantially different pictures. Although they are all highly correlated pairwise (or anti-correlated in the case of the Lewishtein Distance), each one gives a different insight into the data.

Using the cosine similarity of a Sentence Bert embedding, we can see that Arbitrary Articles that were likely in the training set are restituted with an overall **worse** fidelity to the original meaning of the article than a baseline of New Articles that the model has never seen. Although this is somewhat perplexing, it should be nuanced by the fact that all three results (arbitrary, new, and Lawsuit Articles) are within the margin of error. Although the longest common subsequence correlates with the longest continuous common substring, the extent of how much more the Lawsuit Articles are memorized than the baseline is exacerbated the most by this metric. We chose LCCS as the primary metric to measure memorization because it is used the most in the literature, but BLEU-4 would have also been a senseful choice. The high correlation between LCCS and BLEU-4 is shown in Figure 5. However, the absolute values of BLEU-4 in which differences are measured are extremely

    & &  &  &  \\  Attack & Dataset & mean & std & mean & std & mean & std \\  \#1 & 1 & 33.1 & 23.6 & 0.63 & 0.19 & 1350 & 352 \\  & 2 & 65.3 & 44.1 & 0.76 & 0.11 & 1729 & 384 \\  & 3 & 29.0 & 13.8 & 0.65 & 0.18 & 1286 & 379 \\  \#2 & 1 & 33.0 & 24.4 & 0.63 & 0.20 & 1336 & 364 \\  & 2 & 56.2 & 32.7 & 0.75 & 0.11 & 1764 & 595 \\  & 3 & 27.7 & 12.3 & 0.64 & 0.17 & 1318 & 338 \\  \#3 & 1 & 30.9 & 19.5 & 0.61 & 0.16 & 859 & 783 \\  & 2 & 69.5 & 40.5 & 0.67 & 0.13 & 491 & 565 \\  & 3 & 26.9 & 10.4 & 0.66 & 0.14 & 1045 & 593 \\   

Table 1: Values of three different attack metrics for GPT-4. Dataset 1: Arbitrary Articles, Dataset 2: Lawsuit Articles, Dataset 3: New Articles. LCS values can uncover extremely close restitutions. LCCS measures the maximum measurable verbatim regurgitation, while LCS and cosine similarity measure how close we are to verbatim restitution.

    &  \\  Model & 1 & 2 & 3 \\  Opus & 0.04 & -0.12 & 0.14 \\ GPT-4 & 0.02 & -0.08 & 0.13 \\ Llama-3.1-405B & 0.11 & -0.01 & 0.21 \\ Mixtrl-8x22B & 0.07 & 0.23 & 0.29 \\   

Table 2: LCCS and expected completion length (both in characters) correlation values for attack 3, where completion length varies. Dataset 1: Arbitrary Articles, Dataset 2: Lawsuit Articles, Dataset 3: New Articles. The correlation is negligible, indicating that the longer Lawsuit Articles do not confound the observed high memorization.

small, potentially making calculations unstable. In Figure 6, the link between decreasing Levenshtein distance and increasing LCCS is demonstrated. Interestingly, the correlation between LCCS and Normalized Levenshtein Distance is ostensibly much stronger for the Lawsuit Articles than for any other class. We see groups of "arbitrary" articles with a correlation of 0.

Impact of context size.Contrary to our expectations (as well as the findings of Carlini et al. (2023)), it is not the case that a bigger prefix is always better. All models seem to have a particular optimum, after which metrics of similarity decrease. It does, however, seem to be the case that a more sophisticated, more complicated prompt elicits more memorization, as shown in the increasing effectiveness of attacks 1, 2, and 3 (see Table 3). There is, notwithstanding, a limit to the amount of an article that can be fed and still elicit memorization.

Another surprising finding brought to bear by these experiments is how much more other LLM providers are susceptible to verbatim memorization than OpenAI. The baseline arbitrary article memorization in Claude's Opus is higher than any of the highest memorization observed in GPT-4. This can be explained by the fact that OpenAI is being sued, and is filtering their model answers more aggressively than any other provider.

Impact of model size.We can test how memorization changes as the number of parameters grows by looking at different model checkpoints. This can be done on the OpenAI API side by looking at versions of GPT-3 vs GPT-4. The only model that seems to have any severe memorization power is GPT-4. As suggested by Table 4 and measured, the performance of attack 1 on GPT-3.5-16k is weak. Smaller models don't always even seem to understand what they're being asked. Many smaller models answer with inappropriately pithy responses (e.g., saying Assistant: Yes, I?m ready. instead of starting to restitute an article). Looking at the results in Figure 1, it is evident that model size causes memorization, especially in highly duplicated articles such as the ones from exhibit J of The New York Times Company (2023). The growth in memorization for the latter is superlinear. At the same time, it is linear for the arbitrary baseline and constant (non-existent) for the new baseline, i.e., the articles the models are guaranteed to have never seen.

Defenses and mitigations.The models have very likely been fine-tuned defensively. This involves training the model to answer "I'm sorry, as an AI, I can not do this..." when it detects it is being tricked. We consider this when running our experiments: we count those as refusals. As we see Table 3 (and its continuation, Table 4 in the appendix), Anthropic and OpenAI use content filters to defend against verbatim regurgitation attacks.

Figure 3: **Prefix Size vs. LCCS (characters).** The Lawsuit Articles are more memorized than the arbitrary articles, which are more memorized than the New Articles, which models have never seen. There is no clear upward trend for memorization as prefix size increases. Some models seem to follow a normal distribution with maxima around a prefix of 150 tokens.

   Model & Attack & Articles & Excluded \\   & Arbitrary & 3\% \\  & Lawsuit & 53\% \\  & New & 1\% \\   & Arbitrary & 3\% \\  & Lawsuit & 52\% \\  & New & 1\% \\   & Arbitrary & 4\% \\  & Lawsuit & 69\% \\  & New & 0\% \\   & Arbitrary & 83\% \\  & Lawsuit & 73\% \\  & New & 66\% \\   & Arbitrary & 79\% \\  & Lawsuit & 53\% \\  & New & 48\% \\   & Arbitrary & 15\% \\  & Lawsuit & 36\% \\   & New & 7\% \\   

Table 3: Rates of different responses excluded from analysis when quantifying memorization. Decreasing rates of refusals and increasing rates of Content Filtering denote better attacks. More models are shown in Table 4.

The increasing amount of content filter hits with increasing attack number shows the increasing effectiveness of attacks 1, 2, and 3 (signifying a word-for-word match of considerable length, by their admission). It is another way to show that the Lawsuit Articles were selected selectively. The ease with which their memorization can be elicited does not represent an average arbitrary article. It seems that Anthropic has gone through rounds of IT with its Opus models, teaching it to refuse to generate copyrighted content politely. We see that the latter's refusal rates decrease as the attack number increases, indicating yet again the increasing effectiveness of the attacks.

OpenAI seems to have implemented some equivalent to memorization-free decoding (Ippolito et al., 2023), blocking the model's output from being returned to the user. Filtered output includes simple repetitions of specific articles that OpenAI has deemed its models likely to distribute. Not all New York Times articles in a model's training set are filtered within a given period. For example, the article titled _How Israel Became a World Leader in Vaccinating Against COVID-19_ is not filtered. It is also the case that not only New York Times articles are filtered out. Parts of the Evgeny Afineevsky documentary "Francesco" that were not quoted verbatim in any New York Times article are also filtered. Looking at Wei et al. (2024), it seems to be that OpenAI is cleaning the output and passing it through a combination of suffix arrays and a Bloom filter (Bloom, 1970). We can ask the model to repeat a particular text to test the filter. The response time will be prolonged if the content filter is triggered. This is confirmed by the stop_reason flag provided in the API response, but it can also be measured (e.g., by using a 95% confidence interval), as seen in Figure 8. The filtering method is effective at taking down the copyrighted content.

A content filter is, however, an implicit admission that the model is generating a verbatim output that violates the copyright. Similarly to Debenedetti et al. (2024), to show this, the reader may ask a GPT model to "repeat after me:" providing a long enough copyrighted text from Exhibit J. The reader will then find the output coming in much slower, if at all, with exact matches of the copyrighted article inputted. In other words, the filter will hit when the model outputs copyrighted content, word for word.

We consider the time and length of a response in tokens to detect the filter via the API. We benchmark GPT's response lengths using a list of 100 non-adversarial prompts generated by GPT-4. We then use a confidence interval to determine whether the response is filtered.

## 5 Conclusions

While some degree of memorization in LLMs may be unavoidable, the New York Times' Complaint presents a skewed perspective of the frequency of verbatim memorization in ChatGPT. Our findings suggest that on average, ChatGPT and other competing LLMs exhibit verbatim memorization of news articles less frequently than how presented by the New York Times, although more-duplicated articles (like the New York Times') may tend to be more memorized. Our findings show that OpenAI is currently the provider with the _leas_ amount of memorization in absolute numbers among the four LLM families we tested. We also reproduced the previous finding that memorization risk grows with the model size and how duplicated a piece of content is. Of course, this is by assuming that an average-case scenario is what is relevant for copyright: in a worst case scenario (as, for example for privacy), the model verbatim repeating a sensitive portion of the training data even once would be considered a failure.

This work opens many doors, for example, questions on ways to minimize verbatim restitution of copyrighted text. It could be interesting to look at whether training models on maximizing high semantic similarity and minimizing LCCS similarity is a way to minimize verbatim restitution of copyrighted text. It would be interesting to reproduce these findings on models with better-documented datasets, like OLMo-7B (Groeneveld et al., 2024).

Future work could explore the many potential ways to defend against memorization. This ongoing exploration is crucial in the rapidly evolving field of AI. Machine unlearning, or differential privacy, could be a solution to eliminating memorization. Within the framework of this paper, one could also quantify the impact of GCG, or defensive IT. It would be interesting to see if document-level deduplication is insufficient, or that it is too harmful to the utility of model output to be of practical use. One would expect the same trends measured to hold still true; namely, the bigger the model, the more inevitable the memorization.