# Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack

Tiansheng Huang, Sihao Hu, Ling Liu

School of Computer Science

Georgia Institute of Technology, Atlanta, USA

{thuang374, shu335}@gatech.edu,ling.liu@cc.gatech.edu

###### Abstract

The new paradigm of fine-tuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the fine-tuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a _harmful embedding drift_ phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users fine-tuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the fine-tuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at https://github.com/git-disl/Vaccine.

Disclaimer: This document contains content that some may find disturbing or offensive, including content that is hateful or violent in nature.

## 1 Introduction

Despite the success of Large language models (LLMs) in Question-Answering tasks, it has been challenging to ensure their answers are _harmless and helpful_. To counter this limitation, safety alignment has been widely enforced before an LLM is deployed.

The alignment techniques usually include supervised fine-tuning (SFT) on a safe demonstration dataset. Via this channel, an LLM learns how to react to human instruction in a harmless and helpful way, as demonstrated in the alignment dataset. However, user fine-tuning service poses serious challenges for service providers to sustain truthful and responsible service, because in the most common business models1, users can upload arbitrary demonstration data with a particular format to the service provider for fine-tuning. Supervised fine-tuning on these data may break the alignment with a small amount of harmful data that is mixed into the benign fine-tuning data (Qi et al., 2023; Zhan et al., 2023; Yang et al., 2023; Yi et al., 2024; Lermen et al., 2023; Chen et al., 2024). Unfortunately, it is almost impossible to either manually use a filter to detect

Figure 1: Attack surface of harmful fine-tuning attack. Before fine-tuning, the model is aligned with alignment data with supervised fine-tuning (SFT). Fine-tuning on the aligned model breaks the alignment.

and remove all the harmful data during fine-tuning, or heal the model simply by restraining the model update in fine-tuning stage into a subspace (Wei et al., 2024). This vulnerability poses a serious threat to the service provider, who is liable for the potentially harmful output of the customized model after fine-tuning on the user data. **Figure 1** shows the attack surface during fine-tuning with users' data.

To mitigate such a security risk in the fine-tuning stage, one approach is to apply two categories of general solutions originally proposed to counter "catastrophic forgetting" in the field of continual learning (Wang et al., 2023). The first category is represented by (Kirkpatrick et al., 2017; Serra et al., 2018; Hayes et al., 2020; Li & Hoiem, 2017; Zhao et al., 2023; Zong et al., 2024), and can be applied in fine-tuning stage to better preserve the alignment knowledge. However, considerable extra computation is needed for each fine-tuning request, which is impractical for fine-tuning as a service scenarios. The second category is meta-learning (Finn et al., 2017; Javed & White, 2019) and Ripple (Kurita et al., 2020), which only modifies the alignment stage to counter the perturbation of user fine-tuning. As alignment only need to be done once for all user fine-tuning requests, it only incurs small extra overhead. However, these solutions require the service provider to have the user data used for fine-tuning in the alignment stage, which is unrealistic because user data are unavailable before a fine-tuning request arrives. To this end, we aim to answer the question:

_Assume no knowledge of fine-tuning data, can we design an alignment-stage solution that will withstand harmful user data during fine-tuning?_

In this paper, we first share two observations made form our empirical study: (i) a few harmful data uploaded by users can easily trick the fine-tuning of different LLMs to produce broken alignments; and (ii) the fundamental reason of alignment-broken effect in an aligned LLM is due to the drift of hidden embedding (of alignment data) induced by fine-tuning on user data. We call this phenomenon the _Harmful Embedding Drift_, and our experiment further shows that the drift will be aggravated when fine-tuning data contains more harmful data. To counter the embedding drift, we develop Vaccine, a perturbation-aware alignment method that _only modifies the alignment stage_. Vaccine finds the optimal bounded perturbation on embedding that maximizes the alignment loss with the first forward/backward pass of the model, and then we add the perturbation in the second forward/backward pass to produce gradient that optimizes the model such that it can be robust to the perturbation. Through invariant hidden embeddings, Vaccine enables the embeddings to withstand harmful perturbation from raw user data used in fine-tuning. Experiments show that Vaccine can significantly reduce the harmful score (by up-to 9.8%) compared to standard alignment technique, while maintaining good performance with negligible loss (up to 1.8%) for downstream tasks when the user data used in fine-tuning contain harmful instructions.

The paper makes three original contributions:

* We discover a _harmful embedding drift_ phenomenon -the embedding of original alignment data would largely change after fine-tuning on partially harmful data. We identify harmful embedding drift as the cause of broken alignment after fine-tuning.
* Assume no knowledge of the user data, we develop a robust LLM alignment solution (Vaccine) to strengthen the resilience of the aligned model for fine-tuning on partially harmful user data.
* Finally, we conduct evaluations on the efficacy, the hyper-parameters analysis and ablation study of Vaccine. Results show that Vaccine consistently outperforms baseline alignment solutions in diversified settings (e.g., harmful ratio, sample number in the user fine-tuning data).

## 2 Related Work

LLM alignment.Supervised fine-tuning of human preference dataset plays a vital role in LLM alignment. On top of supervised fine-tuning, more complicated techniques are utilized. Reinforcement learning-based RLHF techniques (Ouyang et al., 2022; Griffith et al., 2013; Dai et al., 2023; Bai et al., 2022; Wu et al., 2023; Dong et al., 2023; Rafailov et al., 2023; Yuan et al., 2023; Song et al., 2023) are the most prominent ones. In this paper, we focus our evaluation on the supervised fine-tuning-based alignment, but we insist that our proposed solution can potentially be extended to RLHF. Other alignment techniques include Chain of Hindsight (Liu et al., 2023), which utilizes pairs of good/bad answer for SFT, ITI (Li et al., 2023), which utilizes testing-time perturbation to elicit trustful answer, Stable Alignment (Liu et al., 2023) and selfee (Ye et al., 2023), which both utilize the predict/re-evaluation idea to augment the alignment data, and LM+Prompt (Askell et al., 2021), which promotes alignment by injecting harmless textual prompts.

**Catastrophic forgetting.** Existing LLM alignment techniques do not account for the risk of fine-tuning, which may force the LLM to forget the knowledge previously learned. Similar issues known as catastrophic forgetting (French, 1999; Kemker et al., 2018; Goodfellow et al., 2013; Robins, 1995) are studied in the area of continual learning. The first category of existing solutions can be applied in the finetuing stage. For example, (Kirkpatrick et al., 2017) use Fisher-information, (Serra et al., 2018) use attention mechanism, (Hayes et al., 2020) use replay buffer, ((Li and Hoiem, 2017)) use knowledge distillation, (Zhao et al., 2023) filter unsafe data, and (Zong et al., 2024) mix helpfulness data. The second category keeps the fine-tuning stage unchanged but modifies the alignment stage. For example, (Kurita et al., 2020) use restricted inner product, and (Finn et al., 2017; Javed and White, 2019) use meta learning to minimize the gap between the gradients of alignment/fine-tuned tasks. Recent study (Lucki et al., 2024) show that the safety alignment done by unlearning also exhibits catastrophic forgetting phenomenon.

**Harmful fine-tuning attack.** Harmful fine-tuning attack are concurrently proposed by Yang et al. (2023); Qi et al. (2023); Lermen et al. (2023); Zhan et al. (2023); Chen et al. (2024); Yi et al. (2024), and later a few more advanced attack are proposed and evaluated by (He et al., 2024; Halawi et al., 2024; Rosati et al., 2024). After pre-printing the first version of this work, we observe a surge of related papers trying to mitigate the harmful fine-tuning issue, which are categorized by the timing the defense takes place, as follows. i) Alignment-stage defenses. Representative solutions include RepNoise(Rosati et al., 2024), CTRL(Liu et al., 2024), TAR(Tamirisa et al., 2024), Booster(Huang et al., 2024), RSN-Tune(Anonymous, 2024). ii) Fine-tuning stage solutions, which includes LDIFS (Mukhoti et al., 2023), SafeInstr Bianchi et al. (2023), VLGuard (Zong et al., 2024), Freeze (Wei et al., 2024), BEA(Wang et al., 2024), PTST (Lyu et al., 2024), Lisa (Huang et al., 2024), Constrain-SFT (Qi et al., 2024), Paraphrase (Eiras et al., 2024), ML-LR (Du et al., 2024), Freeze+ (Anonymous, 2024), Seal (Shen et al., 2024), SaLoRA (Anonymous, 2024), and SAFT (Choi et al., 2024). iii) Post-fine-tuning stage solution, which includes LAT(Casper et al., 2024), SOMF (Yi et al., 2024), Safe Lora (Hsu et al., 2024), Antidote (Huang et al., 2024), SafetyLock (Zhu et al., 2024). Recent research also study the mechanism of harmful fine-tuning, including (Leong et al., 2024), (Peng et al., 2024), (Anonymous, 2024), (Qi et al., 2024). Particularly, (Peng et al., 2024) proposes a concept of safety basin, which might be useful to analyze/visualize the landscape of models aligned by Vaccine and other alignment stage solutions. Harmful fine-tuning might also be extended to federated fine-tuning (Ye et al., 2024; Li et al., 2024), and some insights from data poisoning defense for FL (e.g., (Huang et al., 2024; Ozdayi et al., 2021) ) can be utilized. For future study on harmful fine-tuning, we **advocate a thorough citation** of all the related research, which are continuously updated in our survey (Huang et al., 2024).

To our best knowledge, this is the first attempt to address security risk in LLM fine-tuning (with a few concurrent study (Wang et al., 2024; Lyu et al., 2024; Zong et al., 2024)). Our proposed solution only modifies the alignment stage with dual benefits: (i) small computation overhead (compared to solutions that require more computation for each fine-tuning request) and (ii) no assumption on accessing user data used for fine-tuning, supporting a more realistic LLM serving scenario. Follow-up research T-Vaccine (Liu et al., 2024) introduce layer-wise training mechanism to Vaccine to enable the algorithm to be able to trained on consumer GPUs with limited memory, e.g., RTX4090. Layer-wise consideration for safety research is also availble in (Peng et al., 2023; Hsu et al., 2024).

## 3 Preliminaries

### Setting

**Two stage fine-tuning solution.** We consider a two-stage solution (Figure 1) i.e., alignment - user fine-tuning for personalizing a pre-train model. At the first stage, the pre-trained model is first fine-tuned to learn alignment knowledge and is subsequently fine-tuned on the user data to customize the user's need. The data used in the alignment stage is collected by the service provider and the data in user fine-tuning stage is uploaded by the users. After the two stages, the model will be deployed in the server and is used to serve personalized outputs to the prompts provided by the user.

**Threat model.** Following (Qi et al., 2023), in the user fine-tuning stage, we assume the user uploads a set of data points \(\{}_{i},}_{i}\}_{}\), and asks the service provider supervised fine-tuning (SFT) on them. The fine-tuning data is sampled from a mixed of distribution \(}=}_{B}+(1-)}_{H}\) where \(}_{B}\) is the benign distribution for user fine-tuning task and \(}_{H}\) is the distribution contains harmful data.

Among \(\) pieces of data, \(p\) pieces of them are sampled from \(}_{H}\), and the remains are sampled from \(}_{B}\).

**Defense setting.** Following the OpenAI's RLHF paper (Ouyang et al., 2022), we assume the server hosts a human-aligned QA dataset \(\{_{i},_{i}\}_{N}\) for safety alignment. This dataset contains malicious prompt and safe answer pair (i.e., safe demonstration data).

### Risk Analysis

Existing studies, e.g., (Qi et al., 2023; Zhan et al., 2023; Yang et al., 2023; Yi et al., 2024) show that the aligned model could be jail-broken if it is fine-tuned on potentially harmful data.

To validate the threat, we first derive two observations showing how the SFT aligned/non-aligned model performs after fine-tuning on partially harmful data. All the experiments in this section use the default setting in Section 5.1. The Non-Aligned model is a pre-trained Llama2-7B _after supervised fine-tuning on the user data_, and the SFT model is the same Llama2-7B after _sequentially supervised fine-tuning_ on the alignment data and the user data.

* **SFT alignment increases resilience towards harmful user fine-tuning.** We show in Figure 1(a) that alignment with supervised-fine-tuning (SFT) can significantly reduce the harmful score compared to the unaligned version (\(>30\%\)) when the user data contains harmful data. Another interesting observation is that when there is no harmful data within the fine-tuning data, the harmful score of the Non-Aligned model is lower. We postpone a justification for this counter-intuitive phenomenon in Appendix B.3 to avoid deviation from our main logistics.
* **Larger poison ratio compromises SFT alignment.** Figure 1(a) shows that user fine-tuning can significantly downgrade the alignment performance even with a small ratio of harmful data mixed in the user fine-tuning data, and it becomes more severe when the harmful ratio is higher.
* **Fine-tune accuracy is mostly unaffected when the model is becoming harmful.** Another observation is that the harmful ratio would not significantly affect the fine-tune accuracy, which makes the attack even more stealthly to be detected, i.e., it cannot be detected solely by looking at its performance on the fine-tune task.

In summary, our empirical studies show that a few harmful data in the user fine-tuning stage can potentially compromise the alignment2. To uncover the hidden reason of the corruption when harmful data is present, we further derive the statistics of the model to assist our analysis. 
* **Training loss over the alignment data.** We record the model's loss over the alignment dataset (the one used for alignment). As shown in the left of Figure 1(b), for the model produced by SFT, the alignment loss is increased when the harmful ratio becomes larger. This partially explains that the model is less aligned to the alignment data after fine-tuning on more harmful user data, i.e., it starts to forget the alignment knowledge. For the non-aligned model, we see that the alignment loss starts in a high value and then becomes stable at the same level even fine-tuning on more harmful data. We resort to Appendix B.3 for an explanation of this phenomenon.
* **Hidden embedding drift.** To further explain the change of alignment loss, we measure the drift of hidden embedding after user fine-tuning in the right of Figure1(b). More precisely, _embedding drift_ is measured as the L2 norm of the difference between the hidden embedding of the aligned model (or pre-trained model for Non-aligned) and that of the fine-tuned model over the same alignment data. Here hidden embedding refers to the output of each attention layer in an LLM. We see that the embedding drift of the SFT model is significantly higher when the harmful ratio is higher. The same phenomenon is observed for non-aligned model, though the drift is less severe.

Figure 2: Statistic of SFT/non-aligned fine-tuned on SST2 mixed with different ratio of harmful data.

We refer to the embedding drift phenomenon as "_Harmful Embedding Drift_" (HED). As the drift follows the same trend with the harmful score, we conjecture that it is the reason responsible for the corruption of model alignment. Our justification is that with such a significant drift, the perturbed embedding may no longer encode the right information of the input, thereby breaking the alignment.

**The fundamental reason of harmful embedding drift.** We now explain why fine-tuning on user data in essence changes the hidden embedding of alignment data. Formally, denote \(f()=_{l}\) as the original output embedding of an attention module given the alignment input \(\), where \(_{l}\) is the projection matrix, and \(\) is an input embedding. If a perturbation \(_{l}^{}\) is added to the original projection matrix (by fine-tuning on user data), the new output of this attention module will become \(()=_{l}+_{l}^{}=f()+_{ft}\) where \(_{ft}}_{l}\) is the resulted harmful embedding drift.

## 4 Methodology

To mitigate the impact of embedding drift in the user fine-tuning stage, our idea is to add artificial perturbation to the embedding at the model alignment stage to lower its sensitivity to the drift introduced in the fine-tuning stage, i.e., to achieve perturbation-aware alignment.

### Perturbation-aware Alignment

We first formulate the optimization problem we need to solve at the alignment stage. Formally, given the alignment dataset \(\{_{i},_{i}\}_{N}\), we aim to optimize this mini-max problem:

\[_{}_{\|\|}_{i= 1}^{N}((}_{_{L},_{L}} }_{_{1},_{1}})(_{i }),_{i})\] \[}_{_{l},_{l}}( _{l-1})=_{w_{l}}(_{l-1})+_{l} l[L]\] \[=(_{1},,_{L})\] (1)

where \(}_{_{l},_{l}}(_{l-1})\) is the \(l\)-th layer in a LLM that maps the input to a perturbed embedding and \((_{i})\) is the tokenizer function that produces embedding \(_{i,0}\). In the inner maximization function, we aim to find the perturbation \(^{d}\) over each layer's hidden embedding that maximizes the loss over alignment data. To formulate a meaningful perturbation, we constrain the perturbation to be L2-norm bounded by intensity \(\). In the outer minimization, we optimize the model weights that can withstand such an adversarial perturbation, such that the model is robust to the real harmful perturbation that might be introduced in the later user fine-tuning stage.

To solve the mini-max optimization problem, we first approximate the alignment loss with Taylor expansion on \(_{L}\), where \(_{L}=_{w_{L}}(_{L-1})\) is the hidden embedding of the \(L\)-th layer, as follows.

\[((}_{w_{L},_{L }}}_{w_{1},_{1}})( _{i}),_{i})((_{w_{L}} }_{w_{1},_{1}})(_{i},_{i })+_{L}^{T}}{d_{L}}\\ ((_{w_{L}}_{w_{1}} )(_{i}),_{i})+_{l=1}^{L}_{l}^{T} }{d_{l}}\] (2)

where the second approximation holds by applying Taylor expansion for all layers of embedding sequentially. Denote \(_{_{l}}_{}(_{l})=}{d_{l}}\) the backward gradient w.r.t the hidden embedding, and plug the approximation into the inner maximization problem. The optimal perturbation for \(l\)-th layer, i.e., \(_{l}^{*}(_{l})\) is as follows (See Appendix C for a proof).

\[_{l}^{*}(_{l})=_{l}}_{ }(_{l})}{\|_{}(_{1},,_{L })\|}\] (3)

where \(_{}(_{1},,_{L})=(_{_{l}} _{}(_{1}),,_{_{L}}_{ }(_{L}))\) denotes the concatenated gradient over all the hidden embedding (note that the norm constraint of the perturbation is imposed over all layers).

With an optimal perturbation, we then can apply iterative gradient method to solve the outer problem to find the robust model weights that can be resistant to the given perturbation.

In summary, we first find the optimal perturbation that leads the model to forget the alignment data. Then we update the model such that it can withstand such a "detrimental" perturbation. For finding the optimal perturbation, we need the gradient information of the model in the current iteration. Therefore, we need two forward-backward passes for each step of model optimization. See Algorithm 1 for details. The proposed algorithm to solve the min-max problem has a similar form with FGSMGoodfellow et al. (2014) and SAM Foret et al. (2020); Sun et al. (2023); Mi et al. (2022); Sun et al. (2024); Mi et al. (2023).

### Implementation on LoRA-based Fine-tuning

LoRA Hu et al. (2021) or similar techniques Li et al. (2023c); Dettmers et al. (2023); Zhang et al. (2023) are extensively used in fine-tuning/alignment task for LLM due to their efficient training nature. It is natural to extend Vaccine to LoRA-based fine-tuning/alignment. Our implementation is as follows.

At alignment stage, we fix the pre-trained model and load a LoRA adaptor on the attention modules. The LoRA adaptor is then trained on the alignment data with gradient-based perturbation to learn how to provide helpful but harmless answers. At fine-tuning stage, we first merge the LoRA adaptor trained for alignment into the pre-trained model. Then we load and train another adaptor for the user fine-tuning task using the general supervised fine-tuning.

We name our implementation as _Double-LoRA_ as we separately train two adaptors respectively for alignment and user fine-tuning. Other potential implementations include _Single-LoRA_, in which we utilize the same LoRA adaptor for both the alignment stage and the user fine-tuning stage. We discuss and compare this alternative implementation in Section 5.5.

## 5 Experiment

### Setup

**Datasets and models**. For the alignment task, we use the safe samples from the alignment dataset of BeaverTails Ji et al. (2023). For fine-tuning task, we consider SST2Socher et al. (2013), AGNEWSZhang et al. (2015), GSM8KCobbe et al. (2021) and AlpacaEval Li et al. (2023b) as the user fine-tuning task. Within a total number of \(n\) samples, we mix \(p\) (percentage) of unsafe data from BeaverTails with fine-tuning task's benign training data. In our experiment, the default setting is \(p=0.1\) and \(n=1000\) (specially, \(n=5000\) for GSM8K and \(n=700\) for AlpacaEval) unless otherwise specified. We use Llama2-7B Touvron et al. (2023), Opt-3.7B Zhang et al. (2022) and Vicuna-7B Anil et al. (2023) for evaluation. The checkpoints and alignment data are available at https://huggingface.co/anonymous4486. All the experiments are done with an A100-80G.

**Metrics**. We consider two main metrics for evaluation of model's performance.

* **Fine-tune Accuracy (FA).** We measure the Top-1 accuracy of the testing dataset from the corresponding fine-tune task.
* **Harmful Score (HS).** We use the moderation model from Ji et al. (2023) to classify the model output given unseen malicious instructions. Harmful score is the ratio of unsafe output among all the samples' output.

To calculate harmful score, we sample 500 testing instruction from BeaverTails Ji et al. (2023). To obtain fine-tune accuracy, we sample 500 (specially, 200 for AlpacaEval) instruction-label pairs from the corresponding fine-tuning testing dataset. We use the template in Appendix B.1 to obtain the LLM's answer and compare with the ground-truth label.

**Baselines**. We compare the performance of the fine-tuned model with a base model without alignment (Non-Aligned), a base model aligned by SFT (SFT), and EWC Kirkpatrick et al. (2017) (a solution originally proposed to counter catastrophic forgetting), Vlguard (Zong et al., 2024), and KL (a potential defense based on KL regularization). See Appendix B.2 for details.

**Training details and hyper-parameters**. Due to resource constraints, we utilize LoRA (Hu et al., 2021) for efficient LLM training. The rank of the adaptor is set to 8. For alignment, we use AdamW as optimizer (Loshchilov and Hutter, 2017) with a learning rate 1e-3 and a weight decay factor of 0.1. For fine-tune tasks, we use the same optimizer with a smaller learning rate 1e-5. We train 50 epochs for alignment. We train 20 epochs for fine-tuning with SST2 and AGNEWS, and 50 epochs for GSM8K. We need longer fine-tuning epochs for GSM8K because it needs a longer time to converge. Both alignment and fine-tuning use the same batch size of 5. See appendix B.1 for details.

### Main Evaluation

We in this sub-section provide main evaluation results to showcase the efficacy of Vaccine.

**Robustness to harmful ratio.** Fixing sample number \(n=1000\), we compare Vaccine with other baselines under different harmful ratios in Table 1. As shown, Vaccine significantly reduces the harmful score of the model (by up-to 9.8% reduction compared to SFT and by 38.2% compared to Non-Aligned). We also observe that the harmful score reduction compared to SFT is diminished when the harmful ratio becomes higher. However, we insist that a high harmful ratio of fine-tuning data is less common, as it can be more easily identified by conventional screening of the service provider. EWC maintains the same harmful score for all the harmful ratios, but we see that its fine-tune accuracy decreases when the harmful ratio is higher, and its number is lower than Vaccine in all the settings. Another observation is that the fine-tune accuracy of Vaccine becomes higher when the poison ratio is higher. This may indicate that adding perturbation in the alignment stage will incur some minor negative impact on the fine-tune task, but it may be erased by training on partially harmful data (though at the cost of reducing alignment performance).

**Robustness to fine-tune sample number.** Fixing harmful ratio \(p=0.05\), we tune the number of fine-tune samples in Table 2. Our observation is that Vaccine is able to outperform all the baselines in \(n=100,500\) and \(1000\) in terms of harmful score. When there are more fine-tune sample, e.g., 1500, 2000, we show that EWC can outperform Vaccine in this case. However, it also achieves a significantly lower fine-tune accuracy (over 6% loss). We in Section 5.5 will show the possibility of combining EWC fine-tuning with Vaccine, which can achieve a lower harmful score, but also at the cost of losing fine-tuning accuracy.

**Generalization to models.** We show how different methods perform in diversified model/fine-tuning task in Table 3 (in next page). As shown, Vaccine achieves consistently good performance in terms of reducing HS while maintaining FA. Particularly, we observe that Vaccine has better alignment performance in reducing HS when the model is larger (e.g., compared to SFT, for AGNEWS, Vaccine respectively achieves 2% and 11% respectively for Opt-2.7B and Llama2-7B).

   Methods &  &  \\  (p=0.05) & rel=0.01 & p=0.05 & p=0.1 & p=0.2 & Average & rel=0.01 & p=0.05 & p=0.1 & p=0.2 & Average \\  Non-Aligned & 34.20 & 65.60 & 81.00 & 77.60 & 79.20 & 67.52 & 95.60 & 94.00 & 94.00 & 94.00 & 94.00 & 94.40 & 94.64 \\ SFT & 48.60 & 49.80 & 52.60 & 55.20 & 60.00 & 53.24 & **94.20** & **94.80** & **94.00** & 94.20 & 94.40 \\ FWC & 50.60 & 50.60 & 50.60 & 50.60 & **50.60** & 50.60 & 58.60 & 88.80 & 87.40 & 86.80 & 80.60 & 86.32 \\ Vizard & 49.40 & 50.00 & 54.00 & 54.00 & 53.60 & 62.00 & 94.80 & 94.80 & 94.60 & 94.60 & 94.60 & 94.68 \\ KL & 54.40 & 53.60 & 55.20 & 54.00 & 56.60 & 54.76 & 85.80 & 85.80 & 55.00 & 53.50 & 84.60 & 59.08 \\ Vaccine & **42.40** & **42.20** & **42.80** & **42.80** & **42.50** & **56.00** & **46.44** & 92.60 & 92.60 & 93.00 & 93.80 & **95.00** & 93.4 \\   

Table 1: Performance under different harmful ratio.

   Methods &  &  &  \\  (SST2) & HS \(\) & FA \(\) & HS \(\) & FA \(\) & HS \(\) & FA \(\) & HS \(\) & FA \(\) \\  Non-Aligned & 81.20 & 95.40 & 82.40 & 94.00 & 78.60 & 94.20 \\ SFT & 50.20 & **92.00** & 52.80 & **94.80** & 49.80 & **94.20** \\ EWC & 49.40 & 47.20 & 50.60 & 87.40 & 48.80 & 88.00 \\ Vaccine & **44.60** & 91.00 & **42.80** & 93.00 & **43.40** & 93.40 \\   

Table 3: Performance on different models under default setting.

Generalization to datasets.We show how different methods perform in diversified fine-tuning tasks in Table 4. As shown, Vaccine is able to reduce harmful score for all the downstream tasks. Particularly, we observe that Vaccine achieves even better fine-tune accuracy while simultaneously reduce harmful score by 13.8% compared to SFT in the AlpacaEval task.

### Statistical/System Analysis

We further show more statistics for a more comprehensive evaluation.

**Alignment loss**. The left of Figure 3 shows that SFT boosts the alignment loss after 1500 fine-tuning steps, which potentially is the step that the model starts to learn the harmful pattern and forget the alignment knowledge. However, Vaccine is able to withstand the harmful fine-tuning and still maintains a comparably low alignment loss even after sufficient rounds of fine-tuning, which explains the improved performance of Vaccine against harmful fine-tuning.

**Embedding drift**. The right of Figure 3 shows that the embedding drift of SFT start to escalate at the 1000-th step, which roughly coincides with the point that alignment loss starts to rise. This corroborates our conjecture that embedding drift is the main reason for the increase in alignment loss (which further induces the alignment-broken effect). For Vaccine, we observe a smaller embedding drift compared to SFT, due to perturbation-aware training enforced in the alignment stage.

**System performance.** We further compare the system performance between Vaccine and SFT in terms of training clock time and GPU memory consumption. Our results show that Vaccine is 2x slower than the conventional SFT solution and it also incurs slightly larger GPU memory consumption (approximately 0.11 GB). The extra training time is because Vaccine needs to do two forward-backward passes for each optimizer step. The extra memory consumption comes from the artificial perturbation that we need to track in the first forward/backward pass. We insist that such an overhead incurred during the alignment stage is tolerable because alignment only needs to be done once for all incoming user fine-tuning. For more comparison results, we refer to Appendix B.5. Future system-level optimization includes sparsification/quantization/factorization for the perturbation (or gradient) in the first (or second) forward-backward pass.

### Ablation Study and Hyper-parameter Analysis

**Impact of noise intensity \(\).** We show in Table 6 how the perturbation intensity \(\) of Vaccine affects its practical performance. As shown, with a larger \(\), i.e., when the perturbation is larger, the harmful score of the model will be lowered (\(10.2\%\) decrease comparing \(=0.01\) and \(=10\)), but at the same time, the fine-tune accuracy will also decrease. Another observation is that alignment loss in the first round is increased, while the alignment loss in the last round is decreased when \(\) is larger (0.0348 decrease comparing \(=0.01\) and \(=10\)). This phenomenon is understandable because i) the model aligned with larger perturbation is more difficult to converge (i.e., reach the point whose alignment loss is zero) but it is more capable of resisting the perturbation in fine-tuning, therefore the alignment loss after fine-tuning is smaller when \(\) is larger.

**Random perturbation vs. gradient-based perturbation.** By our design, we optimize the bounded perturbation using the gradient obtained by the first forward/backward pass. Another simpler design is to add random Gaussian perturbation to the model in each step similar to (Neelakantan et al., 2015). Results in Table 7 show that gradient-based perturbation is performing better in balancing harmful scores and fine-tune accuracy. Specifically, when \(=0.2\), we show that gradient perturbation simultaneously achieves \(7\%\)_lower harmful score_ and \(19.8\%\)_higher fine-tune accuracy_, compared to random perturbation when \(^{}=5 10^{-3}\). The same superiority is also observed over \(^{}=10^{-2}\). However, in all the settings, there is not an experiment group that random perturbation can outperform gradient perturbation _in both the two metrics simultaneously_, which further validates the effectiveness of gradient-based perturbation.

### Alternative Design

**Single/Double LoRA Adaptor.** As mentioned in Section 4.2, we adopt a Double-LoRA implementation for Vaccine (also for SFT for fair comparison). We compare in table 8 how _Single-Lora_ performs. For Single-LoRA, we use the same LoRA adaptor for alignment and user fine-tuning. Our results show that Double-LoRA implementation decreases harmful scores (3% reduction for SFT and 6.4% reduction for Vaccine) in SST2. For fine-tune accuracy, we observe a minor reduction, i.e., \(0.6\%\) decrease for SFT and \(1.4\%\) reduction for Vaccine in SST2. A similar conclusion is made for AgNews, but single-LoRA seems to benefit Vaccine for GSM8K per our results, as it simultaneously achieves lower HS and higher FA. Overall, we see that no matter we adopt single/double LoRA implementation, Vaccine generally reduces harmful scores compared to the corresponding implementation for SFT.

**Vaccine + EWC fine-tuning.** Vanilla Vaccine only modifies the alignment stage but uses the original SFT for the user fine-tuning stage. We show how Vaccine performs when combining EWC into the user fine-tuning stage in Table 9. Our results show that with EWC, we can further reduce the harmful score by up to 5.2% for SST2, for 4.6% for AGNEWS and over 26.6% for GSM8K. However, we also observe that the fine-tune accuracy will suffer (up to 39.2% loss for SST2). The performance degradation is particularly pronounced when the regularization intensity \(\) is larger.

   Methods & \(=0.01\) & \(=0.1\) & \(=1\) & \(=2\) & \(=5\) & \(=10\) \\  HS & 54.40 & 56.80 & 54.40 & 49.00 & 46.20 & **44.20** \\ FA & 94.40 & **95.00** & 94.40 & 93.60 & 92.80 & 89.00 \\ Alignment loss(FS) & **0.0040** & 0.0041 & 0.0047 & 0.0051 & 0.0059 & 0.0077 \\ Alignment loss(LS) & 0.0437 & 0.0300 & 0.0075 & **0.0065** & 0.0071 & 0.0089 \\   

Table 6: Performance on different perturbation intensity \(\). Alignment loss (FS) and Alignment loss (LS) respectively means alignment loss in the First Step and Last Step of fine-tuning.

   Methods &  &  &  \\   & HS \(\) & FA \(\) & HS \(\) & FA \(\) & HS \(\) & FA \(\) \\  SFT-SL & 58.20 & **95.00** & 59.00 & 88.80 & 70.60 & 18.60 \\ SFT-DL & 55.70 & 94.40 & 53.40 & **89.40** & 77.60 & **23.20** \\ Vaccine-SL & 55.20 & **95.00** & 53.40 & **89.40** & **68.40** & 21.60 \\ Vaccine-DL & **48.90** & 93.60 & **47.80** & 89.20 & 69.40 & 20.20 \\   

Table 7: Performance between random perturbation and gradient-based perturbation. \(^{}\) is the variance of the Gaussian perturbation with mean equals to 0.

   Methods &  &  &  \\   & HS \(\) & FA \(\) & HS \(\) & FA \(\) & HS \(\) & FA \(\) \\  Vaccine(pretrain) & 43.80 & 17.60 & 43.80 & 28.80 & 43.80 & 2.20 \\ Vaccine (wamilla) & 48.20 & **93.80** & 47.80 & **89.20** & 69.40 & 20.20 \\ Vaccine+EWC(\(\)=1e6) & 48.80 & 93.60 & 48.00 & 89.00 & 69.80 & **21.20** \\ Vaccine+EWC(\(\)=1e9) & 44.60 & 92.20 & 44.80 & 88.00 & 70.00 & 19.40 \\ Vaccine+EWC(\(\)=1e12) & **43.00** & 54.40 & **43.20** & 58.60 & **42.80** & 6.20 \\   

Table 9: Performance combining EWC in user fine-tuning. \(\) is the regularization intensity.

### Visualization

**Visualization of embedding drift over alignment data.** We visualize the embedding drift of SFT and Vaccine under different harmful ratios in Figure 4. As shown, When the harmful ratio is high, it is intuitive to see that the embedding of both SFT's model and Vaccine's model is drifting toward a specific direction. However, the embedding drift of Vaccine is slighter, making the drifted embedding still able to preserve the alignment knowledge. This may better explain how Vaccine really works.

**Real Prompt Examples.** To give examples of how Vaccine provides a better alignment performance, we show in the following box how baselines and Vaccine react to the given harmful prompt. As shown, alignment with Vaccine enables the model to give a harmless but also helpful answer even though it is fine-tuned on a poisoned user dataset - it does not forget to answer the questions in a harmless and helpful way as demonstrated in the alignment dataset. Due to space limitation, we show more examples in the Appendix B.4.

## 6 Conclusion

Fine-tuning as a service offers users an opportunity to customize a pre-trained LLM by fine-tuning it over their own data. However, it also opens door to new attack surface, compromising LLM safety. Guided by the immunization conditions proposed by (Rosati et al., 2024), we have presented Vaccine, a robust perturbation-aware alignment solution, for safeguarding LLM fine-tuning against the potentially harmful user data. Evaluation on several popular open source LLMs with diversified settings shows that the fine-tuned model produced by Vaccine can significantly reduce the harmful score of the model under attack, while maintaining good performance on-par to the benign scenarios with a minor and negligible performance loss. As this is the very first defense to the harmful fine-tuning, we recognize several limitations of Vaccine, e.g., extra overhead and weak extension to RLHF, which we postpone our discussion to Appendix E.

Figure 4: T-SNE visualization of harmful embedding drift under different harmful ratios \(p\). Each point represents the embedding of each alignment data.

Acknowledgment

This research is partially sponsored by the NSF CISE grants 2302720, 2312758, 2038029, an IBM faculty award, a grant from CISCO Edge AI program. This research is supported in part through research cyberinfrastructure resources and services provided by the Partnership for an Advanced Computing Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia, USA. Tiansheng would like to thank Domenic Rosati from Dalhousie University and ShengYun Peng from Georgia Tech for the insightful discussions on the future of harmful fine-tuning attacks/defenses. Tiansheng also wants to thank Divyesh Jadav from IBM research for a discussion on the initial idea of Vaccine. All the authors truly appreciate the constructive review comments from the anonymous reviewers/ACs during our submissions to ICML2024 and NeurIPS2024.