# Online Ad Procurement in Non-stationary Autobidding Worlds

Jason Cheuk Nam Liang

MIT

jcnliang@mit.edu

&Haihao Lu

University of Chicago

haihao.lu@chicagobooth.edu

&Baoyu Zhou

University of Michigan

zbaoyu@umich.edu

###### Abstract

Today's online advertisers procure digital ad impressions through interacting with autobidding platforms: advertisers convey high level procurement goals via setting levers such as budget, target return-on-investment, max cost per click, etc. Then ads platforms subsequently procure impressions on advertisers' behalf, and report final procurement conversions (e.g. click) to advertisers. In practice, advertisers may receive minimal information on platforms' procurement details, and procurement outcomes are subject to non-stationary factors like seasonal patterns, occasional system corruptions, and market trends which make it difficult for advertisers to optimize lever decisions effectively. Motivated by this, we present an online learning framework that helps advertisers dynamically optimize ad platform lever decisions while subject to general long-term constraints in a realistic bandit feedback environment with non-stationary procurement outcomes. In particular, we introduce a primal-dual algorithm for online decision making with multi-dimensional decision variables, bandit feedback and long-term uncertain constraints. We show that our algorithm achieves low regret in many worlds when procurement outcomes are generated through procedures that are stochastic, adversarial, adversarially corrupted, periodic, and ergodic, respectively, without having to know which procedure is the ground truth. Finally, we emphasize that our proposed algorithm and theoretical results extend beyond the applications of online advertising.

## 1 Introduction

Automated bidding (or autobidding for short) has become the dominant mode for advertisers to procure digital ad inventories and impressions, contributing to more than \(\$120\) billion dollar ad spend in 2022 and more than \(90\%\) of total online ad transaction volumes . In an autobidding platform, advertisers only need to convey their high-level procurement goals for an ad campaign to the platform, which then takes charge of procuring ads on advertisers' behalf. Such procurement goals are communicated to a platform through platform _levers_, which are adjustable parameters that advertisers can control to influence the bidding process and campaign performance. To exemplify, Figure 1 displays certain several levers presented on the Google Ads interface, where an advertiser can set per-campaign budgets, target cost-per-actions, campaign duration, campaign schedules, targeting, etc; similar examples are also shown in related literature .

As the primary avenue for advertisers to run ad campaigns on autobidding platforms and influence ad conversion outcomes (e.g., clicks), making efficient lever decisions is vital to advertisers to achieve their procurement objectives. However, advertisers face many challenges in practice when optimizing for lever decisions, namely high-dimensional decision making under long-term constraints, non-stationary autobidding environments, and limited procurement feedback.

High-dimensional decision making and long-term constraint satisfaction.Making multiple lever decisions involves evaluating numerous possible combinations of lever configurations, which is computationally intensive and time-consuming in real-time decision making setups. Also, advertisers need to understand the potential interactions and dependencies between levers, as adjusting one lever may have unintended consequences or interactions with other levers, making it challenging topredict the overall impact of adjustments accurately. Furthermore, advertisers may need to satisfy certain long-term constraints over time, e.g., limiting total spend under a budget, or achieving certain return-on-investment targets. Hence in addition to analyzing the interactions between different levers, advertisers also need to concern long-term consequences of making certain lever decisions.

**Limited feedback on procurement outcome and constraints.** Despite the fact that autobidding via lever decisions greatly simplifies advertisers' ad procurement process as they no longer have to handle the intricacies of procuring individual ad impressions, the procurement procedure becomes a black box for advertisers, as advertisers only have limited visibility into the specific details of how the platform executes ad placement processes. This lack of transparency can make it challenging for advertisers to control the nuances of their campaign execution through lever decisions, and it amplifies the complexity to conduct counterfactual analyses on outcomes for past lever decisions.

**Many non-stationary autobidding worlds.** Autobidding procurement environments are highly non-stationary, as a wide spectrum of latent factors in online ad marketplaces may greatly vary procurement outcomes of the same lever decisions in different time periods. These latent factors include but are not limited to changing user preferences, seasonality effects, shifts in market trends, occasional malfunctions in autobidding platforms, etc. These dynamics may influence how users interact with different types of ads, which necessitates continuous adjustment of lever decision strategies to adapt to current and future market conditions.

To address these challenges, in this work we answer the following questions: _How should an advertiser dynamically set multiple levers to optimize conversion outcomes subject to long-term constraints under limited information? And can she run a unified algorithm that can perform well while being agnostic to many types of non-stationary autobidding procurement environments?_

Motivated by these questions, in this work we study an advertiser's online high-dimensional lever decision problem with long-term constraints under limited bandit feedback in many non-stationary worlds. We summarize our main contributions as followed:

**1. Modelling online lever decisions in many worlds using online constrained optimization with bandit function-valued feedback and uncertain constraints (Section 2)**. We cast the advertiser lever decision problem as an online constrained optimization with bandit function-valued feedback and uncertain constraints, where functions at which (lever) decisions are evaluated correspond to the conversion and constraint functions in our autobidding setup. Further, we model real-world non-stationarity in autobidding environments as possibly time-varying distributions from which conversion and constraint functions are sampled, and then further applied to lever decisions. Under this model, we discuss five different input procedures from which the sequence of functional distributions are sampled to model stochastic, adversarial, corrupted, periodic, and ergodic environments; see Section 2.2 for more details. To the best of our knowledge, from a modelling perspective this is the first work to model a high-dimensional lever decision problem in practical non-stationary autobidding worlds.

**2. Proposing a constrained bandit optimization algorithm universally applicable to many worlds (Section 3).** We develop a unified bandit optimization algorithm with robust performance guarantees across different worlds. Our algorithm incorporates four key designs to handle bandit function-valued feedback and unknown non-stationary environments. 1. we utilize dual descent to update dual variables associated with long-term constraints and decouple decisions over time; 2. we employ a primal-ascent-based bandit online convex optimization (BOCO) approach to make (primal) lever decisions to cope with bandit function valued feedback; 3. we implement an exponential weights

Figure 1: Example levers on the Google Ads interface to create digital advertising campaigns.

expert algorithm on top of primal-ascent BCOO, where each expert corresponds to a different primal ascent step size. This enables our algorithm to adapt to the optimal primal ascent step size for each world without prior knowledge on which world we are in; 4. our algorithm dynamically checks for realized constraint violations and applies "safe lever decisions" to ensure long-term constraint satisfaction. For further details, we refer the readers to Algorithm 1 in Section 3.

**3. Analyzing the performance of proposed algorithms in many worlds (Section 4).** We present theoretical analysis (Theorem 4.2) on the regret bound of the proposed algorithm, and we show that our unified algorithm can achieve reasonable regret bounds with all five input procedures. These regret bounds are summarized in the following Table 1.

The parameters \((,,(_{1:T}),q,)\) are formally defined in Section 2.2 and Theorem 4.2. Finally, in Section 5 we remark all results are applicable to other problems.

### Related works.

**Autobidding.** In an autobidding framework, there is a considerable body of works that study the price of anarchy, which aims to improve worst case individual or total advertiser welfare guarantees w.r.t. the optimal welfare via mechanism design frameworks; see e.g. . We remark that this line of work does not concern developing online learning algorithms. On the other hand, numerous works have concentrated on developing online bidding algorithms for repeated ad auctions , as well as designing repeated selling mechanisms for ad impression allocation; see  and references therein. However, as discussed in the introduction, autobidding platforms conduct bidding on behalf of advertisers while keeping bidding and selling mechanism details undisclosed. In this study, we treat the bidding procedure and selling mechanism as a black box and directly model bidding and auction outcomes using conversion functions; see Section 2. To the best of our knowledge, the most pertinent work to this paper is , which explores a similar ad procurement problem by optimizing levers within a bandit environment. However, this work solely focuses on the stochastic world and optimizing a single lever (i.e., a 1-dimensional decision space). In contrast, our paper develops a unified algorithm capable of making high-dimensional lever decisions in many worlds.

**Online convex optimization.** In Section 2, we cast the advertiser's online learning problem of interest to a high-dimensional bandit online convex optimization problem with uncertain long-term constraints, and develop an algorithm that yields good performance guarantees under different procedures from which the objective and constraint functions are generated. There has been a rich line of works that study bandit convex optimization with no long-term constraints in stochastic and adversarial worlds , as well as works that study (full-information feedback) online convex optimization with long-term constraints . Further, works that study both bandit feedback and long-term constraints either consider single-dimensional (such as multi-arm bandits with constraints) , or consider static regret (i.e., benchmarking performance to that of a single optimal action) . This paper distinguishes itself from these two streams of works by considering high-dimensional decisions as well as dynamic regret, i.e., comparing to the best sequence of actions instead of a single action; see Eq. (1). Finally, all aforementioned works only study algorithms in stochastic or adversarial setups, whereas in this work we go beyond these two worlds and address more complex learning environment such as periodic, corrupted, ergodic, and finite switching. To the best of our knowledge, the only related work that develops a universal algorithm under "many world" setups is . However,  considers a full-information scenario where the online decisions in period can be made after observing realized objective and constraint functions during that period. In this work, we present an efficient algorithm to handle bandit feedback.

**Notation.** For any vector \(^{d}\), let \(\|\|\) be its Euclidean norm. Denote \(=\{^{d}:\|\| 1\}\) as the \(d\)-dimensional unit ball centered at \(\), and let \(=\{^{d}:\|\|=1\}\) be the unit sphere. For any set \(\), let \(U()\) be the uniform distribution over \(\). Let \(\) denote the vector whose components

   Stochastic & \(\)-corrupted & Adversarial & Periodic & Ergodic \\  \((T^{})\) & \((T^{}+)\) & \(1-(_{1:T})\\ &+}(A T^{})\) & \((T^{}+q)\) & \((T^{}+)\) \\   

Table 1: \(T\)-period regret bounds for Algorithm 1 under different input models. Here \(A>0\) is some algorithm dependent factor which we will later specify in Section 4.

are all 1's, and \(_{j}\) be the unit vector whose \(j\)'th position is 1. We use \(\) notation to represent the asymptotic order of a term when the period \(T\) and ignore the \( T\) terms.

## 2 Autobidding with bandit feedback in many worlds (input models)

### Autobidding as a bandit online optimization with long-term uncertain constraints

Consider an advertiser repeatedly interacting with an ad platform over \(T\) periods, where each period can be interpreted as a single ad campaign that is run on the platform. During each period \(t[T]\), the advertiser makes \(d 1\) lever decisions denoted as \(_{t}^{d}\), e.g., setting the per-campaign budget, campaign duration, per-campaign target return-on-investment, and max spend per conversion; see Figure 1 for example lever decisions in practice. Here, \(\) is some compact and convex decision set whose diameter is \(D=_{(,^{})}-^{ }\). For simplicity, assume \(\) so \( D\) for any \(\). After the lever decisions \(_{t}\) are made the campaign is fully executed via autobidding, the advertiser observes bandit feedback for her campaign outcomes: she only observes her realized conversion \(f_{t}(_{t})_{ 0}\) (e.g., number of clicks on her ads), as well as some constraint balance \(_{t}(_{t})^{K}\). The conversion and constraint functions \((f_{t},_{t})\) are sampled from some (possibly infinite) support \(\) according to distribution \(_{t}()\) (we will discuss how \(_{t}\)'s are generated by nature in Section 2.2). Using the notation \(_{1:T}=(_{t})_{t[T]}\) the advertiser's hindsight optimization problem is

\[(_{1:T})=_{^{T}}~{}_{t[T] }_{_{t}}[f_{t}(_{t})]~{}~{}~{}_{t[ T]}_{_{t}}[g_{k,t}(_{t})] 0 k=1 K\,.\] (1)

Here, we use a constraint function \(g_{k,t}:\) to characterize general performance metrics of the advertiser for her ad campaigns. In the following, we present several examples for constraint functions that are widely used in practice or studied in literature. For illustrative purposes, we assume \(x_{t,1}=_{1}^{}_{t}\) (i.e., the first lever) represents the per-campaign budget for the \(t\)'th campaign.

* _Long-term budget constraint._ The advertiser has a total budget \(BT>0\). Then by letting constraint function \(g_{k,t}()=B-x_{t,1}\), we have \(_{t[T]}g_{k,t}(_{t}) 0_{t[T]}x_{t,1}  BT\), which means total campaign spent over \(T\) periods (assuming each campaign fully depletes the campaign budget) must be less than the advertiser's total budget \(BT\).
* _Long-term return-on-investment constraint._ The advertiser intends to safeguard a long-term return-on-investment \(>0\), i.e., she attains a long-term average of at least \(\) conversions per dollar spent. Then by considering \(g_{k,t}()=f_{t}()- x_{t,1}\), we have \(_{t[T]}g_{k,t}(_{t}) 0_{t[T]}f_{t}(_ {t})_{t[T]}x_{t,1}\), which means total conversion over \(T\) periods is at least \(\) times total spend.

Finally, we make the following mild assumptions on conversion and constraint functions.1

**Assumption 2.1** (Mild assumptions on conversion and constraint functions).: _For any \((f,)\), \(f\), \(g_{1} g_{K}\) are all bounded concave functions, i.e., we assume \(_{}()_{} G\) and \(_{} f()\) for some \(\), \(<\). Moreover, \(f\) and \(\) are \(L\)-Lipschitz, i.e., for any \(\{,^{}\}\), we have \(|f()-f(^{})| L-^{}\) and \(()-(^{}) L-^{ }\). Furthermore, there exists \((f,)\) such that \(_{}_{k[K]}g_{k}()<0\)._

### Five input models characterizing many autobidding worlds

In this subsection, we describe structural properties of the input distribution sequence \(_{1:T}\), and shed light on how we utilize various properties to model a wide spectrum of autobidding environments (called worlds) such as time-varying user preferences, seasonality, shifts in market trends, etc., that may potentially lead to different procurement outcomes for the same ad campaign lever decisions.

**Stochastic**: There exists some probability distribution \(()\) such that \(_{1}==_{T}=\). This stochastic world represents a stationary autobidding environment where the underlying latent factors influencing user behaviors (and correspondingly conversion results) remain constant over time; see [31; 9; 21].

\(\)**-corrupted.** There exists \(()\) as well as \(\) periods \(=\{_{1}_{}\}[T]\) such that \(_{t}=\) for all \(t\). This \(\)-corrupted input sequence represents occasional anomalies in the autobidding environment that may be caused by systematic malfunctions in the autobidding platform, or deliberate attempts by malicious competitors to exploit the system for their own benefit; e.g., some competitors may engage in click fraud to inflate the number of clicks on our ads to exhaust our budget or generate false data to manipulate autobidding algorithms (for example, see ).

**Adversarial.**\(_{1:T}\) is adversarially chosen by nature before the process starts, and the distributions over time can possibly be non-identical and/or dependent. This adversarial world can be viewed a hypothetical extreme case for the \(\)-corrupted world where each period the procurement outcomes can potentially be corrupted by an adversary. Adversarial input sequences have been widely studied in the literature to assess algorithmic performances in worst-case scenarios; see .

**Periodic.** There exists period length \(q\) such that \(T=cq\) for some integer \(c 2\) with \(_{1:T}\) satisfying \(_{1:q}=_{q+1:2q}==_{(c-1)q+1:T}\). This periodic world captures regular cyclic patterns or fluctuations in the user behavior over specific time intervals; e.g., seasonality, day-of-week patterns, and time of a day.

**Ergodic.**\(_{1:T}\) is an ergodic process (e.g., an irreducible and aperiodic Markov chain or stationary autoregressive processes), where there exists some \(>0\) and a stationary distribution such that the distance between \(\)-step transition probabilities and this stationary distribution decreases exponentially fast in \(\). Mathematically, given the input distribution sequence \(_{1:T}\), denote \(_{(t+)|[t-1]}\) as the conditional distribution of \((f_{t+},_{t+})\) conditioned on the realizations \(\{(f_{r},_{r})\}_{[t]}\). Then, for the ergodic input model there exists a stationary distribution \(}()\) and absolute constant \(R>0\) such that

\[_{\{(f_{t},_{t})\}_{t[T]}^{T}}_{t[T-]} \|_{(t+)|[t-1]}-}\|_{TV}:=R (-)\;;\] (2)

We remark that exponentially decaying ergodic models have been studied widely in the context of online optimization; see e.g. [10, Section 5.2] for further discussions. To make the problem of interest tractable, we assume \(>(T)\). Intuitively, an ergodic input sequence signifies that the procurement outcomes in close time proximity are correlated, which is commonly observed in real-world autobidding systems, as they often involve iterative processes that enable procurement algorithms (operated on behalf of advertisers) to converge to a stable state; see details in .

### Minimizing regret subject to long-term constraints with a universal algorithm

In this work, we take the perspective of an advertiser making repeated lever decisions as described in Section 2.1. We focus on designing a single online algorithm that determines a lever decision \(_{t}\) in each period \(t\) with the goal to minimize regret \(_{T}\) (defined as follows) under any input sequence \(_{1:T}\) while satisfying long-term constraints

\[_{T}=(_{1:T})-_{t[T]}[f _{t}(_{t})]\ _{t[T]}[_{t}(_{t})].\] (3)

Here, the expectation is taken w.r.t. randomness from the input sequence as well as any randomness in our algorithm. We highlight that our desired policy should be agnostic to the input model as well as the input sequence \(_{1:T}\). We remark that our work distinguishes itself from related work that study solely stochastic and/or adversarial input models, as we aim to design a universal algorithm that achieves salient performance over any input model described in Section 2.2.

## 3 A universal constrained BOCO framework for many worlds

In this section, we focus on designing an algorithm that achieves low regret (per Eq. (3)) while maintaining long-term constraint satisfaction in any world described in Section 2.2, without having to know which world we are in. We first highlight three main challenges for our problem of interest.

**Dynamic benchmark lever sequence and unknown input sequence \(_{1:T}\).** Recall the hindsight optimal problem in Eq. (1), with which we are comparing our algorithm's performance to the optimal sequence of lever decisions over time given any input sequence \(_{1:T}\), instead of comparing to a single optimal lever decision as in many related works; e.g., see  and references therein. This dynamic optimal sequence presents a very strong benchmark and makes learning very difficult as we (ideally) need to account for variations in the underlying non-stationary ground truth input sequence \(_{1:T}\). Nevertheless, we do not know the input sequence \(_{1:T}\) in advance, nor do we know in which world the sequence belongs (Section 2.2).

Bandit function-value (zeroth-order) feedback.Our setup concerns a bandit function-value feedback model, where the realized conversion and constraint functions in each period, namley \(f_{t}:_{ 0}\) and \(_{t}:^{K}\) are never revealed to us, and we can only access their function values at our single lever decisions during the period. We also contrast our setup with a "two-point feedback" setup in online convex optimization literature, where in each period one can access function values twice (see  and references therein). Using the context of our setup, in a "two-point feedback" after nature samples \(f_{t}:_{ 0}\) and \(_{t}:^{K}\) from \(_{t}\), we can observe the values for two lever decisions \(\{_{t}^{(1)},_{t}^{(2)}\}\), namely \((f_{t}(_{t}^{(1)}),_{t}(_{t}^{(1)}))\) and \((f_{t}(_{t}^{(2)}),_{t}(_{t}^{(2)}))\).

Universal algorithm for many worlds.Instead of developing separate algorithms that cope with each individual world described in Section 2.1, we aim to develop a single universal algorithm that performs well in every world without exploiting any knowledge on the realized input sequence \(_{1:T}\) or its structural properties.

The key component of our algorithm to handle the aforementioned challenges is the following Lagrangian function: for any \(f_{t}:_{ 0}\) and \(_{t}:^{K}\), let's define \(_{t}:_{ 0}^{K}\) such that

\[_{t}(,)=f_{t}()+^{}_ {t}()\;.\] (4)

Surrounding this Lagrangian function, our proposed algorithm is formally stated in Algorithm 1 with four key components described as follows.

1. Dual descent to decouple decisions across time.As our lever decisions over time should be somewhat intertwined with one another due to the presence of long term constraints, we decouple this cross-period dependency by lagrangifying the hindsight problem in Eq. (1) w.r.t. some benchmark dual variables. In each period \(t\), we maintain an estimate of dual variables \(_{t}_{ 0}^{K}\) corresponding to each of our \(K\) constraints, and adopt a standard dual descent approach to dynamically adjust these estimates; see Eq. (9). In particular, after observing \((f_{t}(_{t}),_{t}(_{t}))\), we update our dual variable by projecting each coordinate of \(_{t}-_{}_{t}(_{t},_{t})=_{t}-_{t}(_{t})\) onto some interval \([0,}{}]\) where \(>0\) is the dual descent step size, \(\) is the maximum achievable conversion, and \(>0\) is some "safety buffer" to be defined later.

2. Primal ascent to handle bandit function-valued feedback in many worlds.Given a dual variable \(_{t}\) during period \(t\), standard optimization frameworks suggest optimizing primal decisions (i.e., lever decisions) by setting \(_{t}=_{}_{_{t}}[ _{t}(,_{t})]\). However, in our setting this is not possible due to the bandit function-valued feedback structure: we do not know \(f_{t}()\), \(_{t}()\) nor \(_{t}\), and hence cannot optimize for \(_{_{t}}[_{t}(,_{t})]\). To handle this, we view the primal decision problem as a _bandit online convex optimization problem (BOCO)_ where the adversarial objective functions are \(_{t}(,_{t})\), and run a BOCO algorithm to make primal lever decisions in each period.

Mathematically, we define the BOCO objective function \(h_{t}:\) as

\[h_{t}()=_{t}(,_{t})=f_{t}()+_{t}^{}_{t}()\;.\] (5)

Then, the BOCO algorithm constructs an estimate \(_{t}\) of \(h_{t}()\) based on a random perturbation approach with perturbation parameter \(>0\) (see Eq. (7) and  for more details in random perturbations methods to estimate gradients). Next, the BOCO algorithm computes primal decision \(_{t}\) by an ascent type update, i.e., projecting \(_{t-1}+_{t}\) back onto \(\), where \(>0\) is the primal ascent step size; see Eq. (9). The loss we incur by running BOCO primal update instead of setting \(_{}_{_{t}}[h_{t}(_{t})]\) in the full information setting depends on the specific type of world we are in, i.e., the input sequence \(_{1:T}\). We later show in Lemma 4.3 that input sequences in each world induce some comparator sequence \(_{1}_{T}\), so the conversion loss due to BOCO primal ascent updates can be characterized as some BOCO "dynamic" regret \(_{t[T]}h_{t}(_{t})-h_{t}(_{t}^{BOCO})\). Later in Lemma 4.6, we show that our BOCO algorithm achieves low dynamic regret against any comparator sequence \(_{1}_{T}\).

3. Choosing primal ascent step sizes from expert advice for different worlds.Different worlds (and correspondingly different BOCO comparator sequences) may require different "optimal" BOCO primal ascent step sizes in order to achieve optimal regret guarantees. Since we are unaware of the world from which the input sequence \(_{1:T}\) is generated, on top of our primal ascent procedure we run a meta "expert" algorithm that allows us to adaptively approximate optimal step sizes in each world. In particular, we consider \(N\) experts, each corresponding to a primal ascent step size \(_{i}>0\)and produce independent lever decision based on primal ascent with her own step size; see Eq. (9). Then, we dynamically maintain exponential weights \(_{t}_{N}\) in the \(N\)-dimensional simplex over \(N\) experts, based on their past performance measured by some surrogate loss function defined in Eq. (8). Finally we set the weighted average primal decision over all experts as our ultimate primal decision; see Eq. (6). We later show in Lemma A.3 that such exponentially weighted average BOCO primal ascent decisions achieve low conversion loss compared to the "optimal" BOCO primal ascent expert (i.e., step size) in every world, without having to know which world we are in.

**4. Constraint violation check to ensure constraint satisfaction.** To ensure long-term constraint satisfaction, we maintain a constraint balance \(B_{k,t}=_{[t-1]}g_{k,t}(_{})\) for each constraint \(k[K]\) that keeps track of our deviation from satisfying the constraint. To make our problem tractable, we make the assumption that there is always a "safe action" with which we are always guaranteed to attain some small positive constraint balance.2

**Assumption 3.1** (Safe action).: _Assume there exists some \(>0\) and an action \(}_{}\) such that for any \((f,)\), we have \(g_{k}(}_{})>\) for all \(k[K]\)._

With the constraint balance, in each period before making a lever decision, we check the following: if by playing the safe action in all future rounds nearly violates constraints (step 2 in Algorithm 1), then we hard stop our algorithm and play the safe action in all subsequent rounds. We remark that we do not necessarily know the constraint function value lower bound \(>0\) in Assumption 3.1. However, we show later in Theorem 4.1 that by considering any safety buffer \(<\) (e.g., taking \(=1/(T)\) for large enough \(T\)), we are hard stopping more conservatively, and thus maintain constraint satisfaction. Note that our final regret scales with \(}\); see Lemma 4.6.

```
0: Initial dual variable \(_{1}=\), primal expert solutions \(}_{1}^{i}=\) for any \(i[N]\); perturbation parameter \(>0\) and \((0,1)\); primal ascent step sizes for experts \(\{_{1},,_{N}\}>0\); dual descent step size \(\); learning rate of the meta-algorithm \(\); initial expert weights \(w_{i,1}=1\) for all \(i[N]\); safety buffer \(>0\).
1: Initialize constraint balance \(B_{k,1}=0\) for all \(k[K]\)
2:while {for all \(k[K]\), \(B_{k,t}-+(T-t-1) 0\)}do
3: Compute exponentially weighted average forecaster: \[}_{t}=w_{i}^{i}}_{i[N]}w_{i}^{i }}_{t}^{i}\] (6)
4: Sample \(_{t} U()\) uniformly at random from the unit sphere.
5: Set \(_{t}=}_{t}+_{t}\) and observe \(f_{t}(_{t})\) and \(_{t}(_{t})\). Update \(B_{k,t+1}=B_{k,t}+g_{k,}(_{t})\).
6: Construct gradient estimate for \(_{}(_{t},_{t})\) \[_{t}=(f_{t}(_{t})+_{t}^{} g_{t}(_{t}))_{t}\] (7)
7: Update exponential weights for experts: Let \(_{t}:\) be a surrogate loss function to measure the performance of each expert. Then we update expert weights by \[w_{i,t+1}=w_{i,t}(-_{t}(}_{t}^{i})) _{t}()=_{t}^{}(}_{t}-)\] (8)
8: Primal ascent per expert and dual descent: \[}_{t+1}^{i}=_{(1-)}(}_{t }^{i}+_{i}_{t})\;\;\;\;_{t+1}=_{[0, }{}]}(_{t}-_{}}_{t}(_{t},_{t}))_{+}\] (9)
9:endwhile
10: Set stopping time \(_{A}=t\). For \(t=_{A} T\) set safety option \(_{t}=}_{}\). ```

**Algorithm 1**

## 4 Performance analysis of our constrained BOCO algorithm

In this section, we analyze the performance of Algorithm 1 under input sequence \(_{1:T}\) which may be generated from any world described in Section 2.2. Our first result Lemma 4.1 shows that our algorithm maintains long-term constraint satisfaction almost surely for any input sequence.

**Lemma 4.1** (Strict constraint satisfaction).: _Suppose Assumption 3.1 hold, and \(T\) is large enough so that the safety buffer \(=<\), where \(\) is defined in Assumption 3.1. Then, for any \(k[K]\), we have \(_{t[T]}g_{k,t}(_{t})>0\)._

The main result of this paper is the following Theorem 4.2, where we bound the regret of our proposed Algorithm 1 in all worlds specified in Section 2.2.

**Theorem 4.2** (Bounding regret in many worlds).: _Let the safety buffer \(=\), and the number of experts \(N=(1,-_{2}(K^{-}(1+DT)^{}T^{- })+1)\), where \(K\) is the total number of constraints and \(D\) is diameter of the decision set \(\). Choose the corresponding primal ascent step sizes for the \(N\) experts as \(\{_{1}_{N}\}=\{2^{-i}K^{-}(1+DT)^{}T^ {-}:i=0 N-1\}\). Then by taking dual descent step size \(=}\), random perturbation parameter \(=K^{}T^{-}\) and \(=(,K^{}T^{-})\), and exponential weighted expert learning rate \(=T^{-}\), we obtain the following bounds on \(_{T}\) in each input setting of interest: (1) **Stochastic:**\(_{T}(T^{3/4})\); (2) **Adversarial:**\(_{T}(1-)(_{1: T})+}(}_{1:T})} T^{})\), where \(=1-_{(f,)}_{k[K], }g_{k}()>1\) under Assumption 2.1, \(>0\) is defined in Assumption 3.1, \(}_{t}=_{}f_{t}(_{t})+_{t}^{}_{t}(_{t})\), and \(P(}_{1:T})=_{t[T-1]}\|}_{t}- }_{t+1}\|\); (3) \(\)-corrupted: \(_{T}(T^{3/4}+)\); (4) **Periodic**: \(_{T}(T^{3/4}+q)\); (5) **Ergodic**: \(_{T}(T^{3/4}+)\)._

We also summarize our regret bounds in Table 1. Although it is always challenging to identify which input procedure the data currently comes from, Theorem 4.2 shows that the proposed Algorithm 1 achieves reasonable regret bound in five input settings without knowing which setting we are in. We remark that in the adversarial setting, our algorithm is \((1-)\)-competitive, namely, it can achieve at least a \(\)-portion of the reward compared to OPT; later in Section 5 we comment that no sublinear regret is achievable. For the \(\)-corrupted world, we corrupt \(\) instances in the input sequence over time, so if \(=0\), we recover the stochastic world and achieve diminishing \((T^{3/4})\) regret; in the case we corrupt all samples, then we recover the adversarial case, and the regret becomes \((T)\) as expected. In the periodic world, if period length \(q=1\), we recover the stochastic case.

**Remark 4.1**.: _We remark that our regret upper bound in the adversarial case depends on the dual variables \(\{_{t}\}_{t[T]}\) generated throughout the algorithm, which contrasts adversarial regret bounds in  that only depend on the total variance of the optimal sequence of \(1\). This is due to that fact that  handles a non-constrained online optimization problem, whereas we tackle a more difficult problem with long-term adversarial constraints that leads to more severe regret degradation as we ensure satisfaction of long-term adversarial constraints. We acknowledge that this bound should ideally be independent of dual variables, and we leave this for future work._

In the rest of this section, we present a proof scratch of Theorem 4.2, and the formal proof of the results can be found in Appendix A. The high-level idea of the proof is that we decompose total regret into three components as in Lemma 4.3 bellow, namely the conversion losses due to hard stopping, dual descent, and primal ascent, respectively. Then, we bound each component's regret.

**Lemma 4.3** (Regret decomposition).: _Let \((_{t}_{ 0}^{K})_{t[T]}\) be the sequence of dual variables generated from Algorithm 1, and define \(h_{t}()=_{t}(,_{t})\) where there Lagrangian function \(_{t}(,)\) is defined in Eq. (4). Then we have_

\[(_{1:T})-_{t[T]}[f_{t}(_{ t})](T-_{A})+_{t[_{A}]}_{t}^{}_{t} (_{t})+_{}(_{A})\] (10)

_where \(_{}(_{A})\) admits the following bounds in each input setting: (1) **Stochastic:**\(_{}_{t[_{A}]}h_{t}()-h_{t}(_{t})\); (2) **Adversarial:**\(_{t[_{A}]}h_{t}(}_{t})-h_{t}(_{t})+(1- )(_{1:T})\) where \(}_{t}=_{}_{ _{t}}[f_{t}()+_{t}^{}_{t}()]\); (3) \(\)**-corrupted:**\(_{}_{t[_{A}]}h_{t}()-h_{t}(_{t})+ ( qT)\); (5) **Ergodic**: \(_{}_{t[_{A}]}h_{t}()-h_{t}(_{t})+ ( T)\)._

Here, we remark that the first term \((T-_{A})\) is the loss due to hard stopping to maintain long-term constraint satisfaction; the second term \(_{t[_{A}]}_{t}^{}_{t}(_{t})\) is the loss due to dual descent; and thefinal term \(_{}(_{A})\) is the loss due to primal ascent. The first two components turn out to be identical in different settings, and the input model only affects the last term \(_{}(_{A})\). We next present our key results with which we use to bound first two components (dual descent and hard stop loss), and the third component (primal ascent loss), respectively.

Bounding dual descent and hard stop loss.Our strategy to bound the loss due to dual descent and hard stopping relies on bounding some cumulative "complementary slackness" induced from dual descent in the following Lemma 4.4. We note that the bound in Lemma 4.4 holds for any \([,}{}]\), and thus by choosing appropriate \(\), we can "internalize" the losses due to dual descent and hard stopping, as demonstrted in Lemma 4.5.

**Lemma 4.4** (Bounding complementary slackness).: _For any \([,}{}]\) and \(t[T]\), we have \(_{[t]}(_{}-)^{}_{}(_{})=_{[t]}(_{}-)^{}_{ }_{}(_{},_{})\;\; tK^{2}+\|\|_{2}^{2}\)_

**Lemma 4.5** (Internalizing dual descent and hard stop losses.).: _Given the stopping time \(_{A}[T]\), w.p. 1 we have \((T-_{A})+_{t[_{A}]}_{t}^{}_{t}( {x}_{t})\;\;TK^{2}+}{}^{2}++}{}\)._

Bounding primal ascent loss (dynamic BOCO regret).We present our main result Lemma 4.6, which bounds the loss due to primal ascent for given any comparator sequence \(_{1}_{T}\).

**Lemma 4.6** (Bounding primal ascent (dynamic BOCO regret)).: _For any \(i[N],\,t[T]\) and any sequence \(_{1:t}^{t}\) we have \(_{[t]}h_{}(_{})-h_{}(_{})\;\; +_{1:})}{ _{i}}+KT}{^{2}^{2}}+T+\), where \(P(_{1:T})=_{t[T-1]}\|_{t}-_{t+1}\|\), and parameters \((,_{i},,)\) are specified in Algorithm 1._

Proof sketch.: We define the smoothed-versions of BOCO rewards \(h_{t}:\) as \(_{t}()=_{ U()}[_{t}( {x}+,_{t})]\). Then, we decompose the primal ascent loss (BOCO dynamic regret) \(_{[t]}(h_{}(_{})-h_{}(_{}))\) into 3 terms:

\[_{[t]}(h_{}(_{})-_{}((1-)_{ }))+_{[t]}(_{}((1-)_{})-_{ }(}_{}))+_{[t]}(_{}(}_{})-h_{}(_{})),\]

where we recall the \(}_{}\) is the un-perturbed version of our primal lever decision in Algorithm 1. The first and third summands can be directly be bounded via exploiting Lipschitz continuity properties for \(h_{t}\) (see Lemma A.1). The second summand can further be upper bounded (using Lemma A.2) by the surrogate loss difference between the primal decision weighted over all expert decisions and the comparator decision, namely \(_{}(}_{})-_{}((1-) _{})\). Then, we choose any expert \(i[N]\) corresponding to primal ascent step size \(_{i}>0\) as an "intermediary" and decompose cumulative surrogate loss as \(_{t[T]}(_{t}(}_{t})-_{t}((1- )_{t}))=_{t[T]}(_{t}(}_{t} )-_{t}(}_{t}^{i}))+_{t[T]}(_{t }(}_{t}^{i})-_{t}((1-)_{t} ))\). The first summand corresponds to the cumulative loss of our unperturbed primal decisions against any expert, and the second summand represents the cumulative loss of any expert w.r.t. the comparator sequence. Both summands are bounded in Lemma A.3. 

We note that choosing different experts as the intermediary may yield different bounds. But since Lemma A.3 holds for any expert \(i[N]\), we can consider the expert with the "optimal" primal ascent step size in each world, respectively.

## 5 Additional discussions

Lower bounds.A natural question to ask is whether regret bounds in Theorem 4.2 are optimal. Here, we look at lower bounds in "relaxed" problem settings, e.g., bandit online convex optimization with no-constraints [11; 44], or online constrained optimization under full-information feedback [8; 10]:

   Stochastic & \(\)-corrupted & Adversarial & Periodic & Ergodic \\  \(()\) & \((+)\) & \(1-T\) & \(()\) & NA \\    These relaxed lower bounds are applicable to our setting (though they may not be the tightest bounds), and we can see that it may be possible to employ more complex machinery to attain better regret performances for our bandit convex online optimization setup with uncertain constraints. Yet, to identify an algorithm that can achieve the optimal regret in many worlds is an extremely challengingopen problem. For instance, even in the relaxed setting of bandit convex optimization with no-constraints, to the best of our knowledge there is no universal algorithm that achieves optimal regret in just the stochastic and adversarial worlds.

**Other applications.** Our proposed Algorithm 1 addresses a more general problem of online decision making with multi-dimensional decisions, bandit feedback and long-time uncertain constraints. That being said, our algorithm can also be applicable to other problem settings, for example:

_Personalized recommendation and assortment in online retailing._ Online retail platforms aim to optimize sales or revenue by recommending a limited assortment of products to customers. However, the likelihood of customer purchases given an assortment as well as their preferences are unknown, and different customer types may change over time due to evolving market trends. Platforms only observe bandit binary purchase decisions. Hence in our context, the decision set \(\) represents a probability simplex over \(d\) items. The online decisions \(_{t}\) correspond to recommendation probabilities for each item. The function \(f_{t}(_{t})\) reflects the revenue generated from customer purchases, while \(_{t}(_{t})\) captures assortment capacity constraints, product inventory limits, or fairness considerations to ensure equal visibility for each product .

_Real time posted pricing in E-commerce and cloud computing._ In online E-commerce platforms such as Amazon and eBay, sellers set prices to sell various items to sequentially arriving customers ; and in cloud computing, operators of cloud services such as Alibaba Cloud (Alicloud) or Amazon Web Services (AWS) set prices for renting out different computing capacities (virtual machines, VMs etc.) upon customer requests . The arrival of customer types as well as their demand may differ significantly over time (think about demand for sanitary products during pandemics outbursts, or computing resource demands during periodic business hours). Further, decision makers only get to observe (bandit) demand at the realized pricing decisions. Thereby, we can view \(_{t}\) as the vector of prices for various sold products, \(f_{t}(_{t})\) as the generated revenue over a given period, \(_{t}(_{t})\) as real time product/resource capacity constraints, operating cost constraints, etc.