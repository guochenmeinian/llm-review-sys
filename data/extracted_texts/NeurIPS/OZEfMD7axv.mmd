# SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions

Yuseung Lee Kunho Kim Hyunjin Kim Minhyuk Sung KAIST

{phillip0701,kaist984,rlaguswls98,mhsung}@kaist.ac.kr

###### Abstract

The remarkable capabilities of pretrained image diffusion models have been utilized not only for generating fixed-size images but also for creating panoramas. However, naive stitching of multiple images often results in visible seams. Recent techniques have attempted to address this issue by performing joint diffusions in multiple windows and averaging latent features in overlapping regions. However, these approaches, which focus on seamless montage generation, often yield incoherent outputs by blending different scenes within a single image. To overcome this limitation, we propose SyncDiffusion, a plug-and-play module that synchronizes multiple diffusions through gradient descent from a perceptual similarity loss. Specifically, we compute the gradient of the perceptual loss using the predicted denoised images at each denoising step, providing meaningful guidance for achieving coherent montages. Our experimental results demonstrate that our method produces significantly more coherent outputs for text-guided panorama generation compared to previous methods (66.35% vs. 33.65% in our user study) while still maintaining fidelity (as assessed by GIQA) and compatibility with the input prompt (as measured by CLIP score). We further demonstrate the versatility of our method across three plug-and-play applications: layout-guided image generation, conditional image generation and 360-degree panorama generation. Our project page is at https://syncdiffusion.github.io.

Figure 1: Comparison of panoramas generated with prompt _“A photo of a rock concert”_ by Blended Latent Diffusion  (top), MultiDiffusion  (middle), and our SyncDiffusion (bottom). Blended Latent Diffusion, when applied on image extrapolation, often generates visible seams and repetitive patterns. MultiDiffusion creates seamless panoramas but fails to achieve global coherence across the image. In contrast, our SyncDiffusion synchronizes windows across the panorama by increasing the perceptual similarity of the denoised output predictions. This results in significantly more coherent panorama outputs.

Introduction

Diffusion models have recently emerged as the forefront of generative models. Recent breakthroughs in text-to-image generation such as DALL-E 2  and Stable Diffusion  are diffusion models trained with billions of images. Compared with GANs , diffusion models offer advantages not only in producing high-quality, realistic images but also in being utilized for conditional generation setups such as inpainting [30; 34; 41], editing [31; 18; 7; 1; 2; 34], and deblurring [48; 9], with few-shot [25; 12; 40] or even zero-shot [37; 36; 28; 46; 49] adaptation. The iterative reverse diffusion process can easily incorporate diverse conditions and regularizations at each step while guiding the entire process to produce realistic images. Hence, the diffusion model, once trained on a large-scale dataset, becomes a versatile and multi-purpose pretrained model that can be applied to various tasks and applications.

Recent work has extended the versatility of pretrained diffusion models to generate arbitrary-sized images or panoramas using either sequential  or joint  diffusion processes. Since typical image diffusion models are trained to generate fixed-sized images, creating panoramic images requires stitching multiple fixed-size images together, which can be impossible to do seamlessly without considering integration in the generation process. Two approaches have been proposed to tackle this issue. The first approach  involves generating the final output as a sequence of inpainting starting from an initial image, where each consecutive view image is produced while fixing the overlapped region (which is referred to as _image extrapolation_ in their work ). However, this approach often struggles to seamlessly extend the given image and also tends to repeat similar patterns, resulting in unrealistic panoramas as shown in the first row of Fig. 1. The other approach is joint diffusion , which operates the reverse generative process simultaneously across multiple views while averaging the intermediate noisy images (or the noisy latent features) in the overlapped regions at each reverse process step. The blending of noisy latent features among the views at each denoising step can effectively generate a seamless montage of images. However, it is important to note that the content and styles of the images may vary across the views, resulting in a mixture of colorful and black-and-white images in a single panorama, as shown in the second row of Fig. 1. The lack of consistency occurs because the latent features of the overlapped regions are simply averaged without considering the coherence between them.

To address the limitation of previous work that produces unrealistic or incoherent montages, we present a novel synchronization module for joint diffusion, dubbed SyncDiffusion. This module guides the reverse diffusion processes to achieve global coherence across different areas of the panorama image. Similar to previous guided diffusion methods [36; 30; 34; 41], our SyncDiffusion guides the reverse diffusion process while adjusting the intermediate noisy images at each step. Our guidance is specifically provided as a gradient descent from a perceptual similarity loss calculated across multiple windows. Various off-the-shelf perceptual similarity losses such as LPIPS  or Style Loss  can be utilized in our framework. However, perceptual similarity losses computed with noisy images cannot effectively guide the denoising process. Thus, we draw inspiration from the non-Markovian formulation of DDIM  leveraging the prediction of the _denoised output_ from the current noisy image at each denoising step. We compute the perceptual similarity loss using the _foreseen_ denoised images at each step and then backpropagate the gradient through the noisy images. By leveraging the synergy with a prior seamless stitching technique  based on averaging latent features at each denoising step, our joint diffusion framework demonstrates the capability to generate montages that exhibit both local seamlessness and global coherence, as shown in the last row of Fig. 1. This is achieved in a zero-shot manner, without the need for retraining or fine-tuning of existing diffusion models.

In our experiments on text-guided panorama generation using Stable Diffusion 2.0  model, the results demonstrate that our method achieves significantly higher coherence compared to previous methods. Quantitatively, as we increase the weight of the gradient descent, we observe improved coherence, measured by LPIPS  and Style Loss , while maintaining fidelity (measured by GIQA ) and compatibility with the input prompt (assessed by the CLIP score ). Diversity, measured by FID and KID, exhibits a trade-off with coherence, but our method still achieves much better scores compared to the baseline method. Our user studies confirm a significant preference for our method (66.35%) over the previous method (33.65%) in terms of coherence, while also suggesting superior image quality and higher prompt compatibility. Moreover, we further demonstrate the versabtility of SyncDiffusion across three plug-and-play applications: layout-guided image generation, conditional image generation and 360-degree panorama generation.

## 2 Related Work

### Diffusion Models

Diffusion probabilistic models [44; 11; 21; 33] are a group of generative models that generate data by sampling from an initial Gaussian distribution and iteratively applying a denoising process (referred to as the reverse process). These methods have achieved remarkable strides in image generation [39; 11; 42; 36], surpassing previous image generation models including GANs [24; 6]. DDPM  was among the pioneering models that showcased the impressive capability of image generation through Markovian forward and reverse processes, albeit with longer computation times in generation. This limitation was addressed by DDIM , which significantly reduced the sampling time in the reverse process using a non-Markovian transition formulation conditioned on the predicted denoised data. Furthermore, LDM  proposed incorporating the diffusion process into the latent space, achieving state-of-the-art realism in generated images and gaining attention in the text-to-image generation. Diffusion models have also demonstrated their applicability to diverse data modalities such as audio [50; 23; 29; 15], videos [22; 5], and 3D objects [35; 28; 46; 49].

### Few-Shot or Zero-Shot Adaptation of Diffusion Models

Building upon the remarkable generation capabilities of pretrained public text-to-image diffusion models such as Stable Diffusion , recent research has introduced various methodologies for leveraging the pretrained models in diverse tasks including conditional generation, image editing, and manipulation, without the need to retrain the models from scratch. ControlNet  is an example of a method that enables the incorporation of additional conditions into existing text-to-image diffusion models through few-shot finetuning, wherein the image encoder is duplicated to handle the additional conditional image, and only a carefully selected subset of parameters is modified during the finetuning process. Custom Diffusion  also introduces a similar idea of enabling few-shot tuning while keeping the majority of parameters in the neural network frozen, but with applications of finetuning the model for a particular class or concept of images. Other previous work has also demonstrated that diffusion models can even be applied to novel tasks in a zero-shot manner. SDEdit  was the first to show zero-shot conditional image generation using a pretrained diffusion model by dispersing noise over the conditional image and denoising it back to a real image. RePaint  introduced an image inpainting idea by combining a generated foreground image and a noised background image at each time step. Similar _guided_ diffusion ideas have also been explored for various tasks, such as image super-resolution [27; 13; 9; 43], colorization [41; 10], deblurring [48; 9], and style transfer [26; 25; 12; 40]. We propose a novel guided diffusion framework for image montage generation via joint diffusion.

### Montage Generation via Diffusion Models

Panorama generation is one of the zero-shot applications of diffusion models. Since diffusion models are trained to generate images of a specific size and on a 2D plane, stitching is required to generate panoramas or textures. Most previous methods [38; 8; 2; 1] have employed inpainting-based approaches for seamless stitching. These methods extrapolate the accumulated image and fill only the missing regions to generate the panorama or texture. In contrast, MultiDiffusion  and DiffCollage  conduct diffusion in multiple views jointly while combining noisy latent features or scores at each reverse diffusion step. While both approaches have successfully produced continuous images, they have limitations in enforcing global coherence across the panorama or texture. MVDiffusion , a concurrent work, extends multi-view diffusion to produce non-square panorama images such as 360 panorama images by leveraging pixel-wise correspondence and attention modules However, it focuses on achieving smooth stitching, without addressing global coherence. In this work, we propose a simple yet effective synchronization module that can be integrated into any joint diffusion process to achieve global semantic coherence.

## 3 Backgrounds

### Diffusion Models

In this section, we provide a brief overview of the Denoising Diffusion Probabilistic Models (DDPM)  and Denoising Diffusion Implicit Models (DDIM) , which are the foundations of recent pretrained image diffusion models. The aim of DDPM is to approximate the data distribution \(q(_{0})\) with a tractable model distribution \(p_{}(_{0})\), which takes the form of a _Markov_ chain withlearned Gaussian transitions \(p_{}(_{t-1}|_{t})\) from \(p(_{T})=(_{T};,)\):

\[p_{}(_{0})= p_{}(_{0:T})d_{1:T},  p_{}(_{0:T})=p(_{T})_{t=1 }^{T}p_{}(_{t-1}|_{t}).\] (1)

The parameters of the joint distribution (known as the _reverse process_) \(\) are learned by minimizing the negative evidence lower bound (ELBO):

\[_{}_{q(_{0})}[- p_{}(_ {0})]_{}_{q(_{0},_{1},,_{T})}[- p_{}(_{0:T})+ q(_{ 1:T}|_{0})],\] (2)

where \(q_{}(_{1:T}|_{0})\) is the _forward process_ adding a sequence of Gaussian noise to the data while increasing the noise scale. Among the variations of the forward processes, DDPM uses the _variance-preserving_ diffusion that parameterizes the Gaussian transitions as follows with a decreasing sequence \(_{1:T}(0,1]^{T}\):

\[q(_{1:T}|_{0}):=_{t=1}^{T}q(_{t}|_{t-1}),q(_{t}|_{t-1}):=(}{ _{t-1}}}_{t-1},(1-}{_{t-1}}) ).\] (3)

The definition of the Gaussian transitions in the forward process derives the following property:

\[q(_{t}|_{0}):=(_{t};}_{0},(1-_{t})),\] (4)

and thus matches the choice of the starting distribution in the reverse process (a unit Gaussian) since \(q(_{T}|_{0})\) converges to a unit Gaussian when \(_{T}\) is set close to 0. It also allows expressing \(_{t}\) with \(_{0}\) and a unit Gaussian noise variable \(\):

\[_{t}=}_{0}+}, (,).\] (5)

In DDPM , the Gaussian transition \(p_{}(_{t-1}|_{t})\) for each \(_{t}\) in the reverse process is modeled as follows 1:

\[p_{}(_{t-1}|_{t}):=(}{_{t}}}(_{t}-}} (1-}{_{t-1}})_{}(_{t },t)),_{t}^{2}),\] (6)

where \(_{t}^{2}=}{1-_{t}}(1-}{_{t-1}})\), and \(_{}(_{t},t)\) is a learned function that optimizes the objective in Eq. 2 when it maps each \(_{t}\) at time \(t\) to a unit Gaussian noise, thus resulting in the following simplified loss:

\[L(_{}):=_{t=1}^{T}_{_{0} q(_{0}),_{t}(,)}[\|_{ }(}_{0}+}_{t}, t)-_{t}\|_{2}^{2}].\] (7)

DDIM  provides a different perspective of seeing the same forward process as a _non-Markovian_ process while taking the input data \(_{0}\) into consideration in _reversed_ transitions:

\[q(_{1:T}|_{0}):=q(_{T}|_ {0})_{t=2}^{T}q(_{t-1}|_{t},_{0}),  q(_{T}|_{0})=(}_{0},(1-_{T}))\] (8) \[ s<t, q(_{s}|_{t}, _{0})=(}_{0}+-_{t}^{2}}_{t}-}_{0}}{}},_{t}^{2}).\]

Then, each transition in the reverse process is also redefined as first predicting the _denoised observation_\(_{0}\) given each \(_{t}\) and then sampling \(_{t-1}\) via the conditional distribution \(q(_{t-1}|_{t},_{0})\):

\[p_{}(_{t-1}|_{t}):=q(_{t-1}| _{t},_{}(_{t},t))&t 2\\ (_{}(_{t},t),_{t}^{2})&t=1, \] (9)where

\[_{}(_{t},t)=}}(_{t}-}_{}(_{t},t))\] (10)

is the predicted denoised observation. The key observations of DDIM are twofold. First, the same simplified objective (Eq. 7) can be used to find the best models \(_{}(_{t},t)\) in Eq. 10 that minimize the negative ELBO (Eq. 2). This means that the DDIM reverse process can be used with a pretrained DDPM without retraining. Second, a subset of the time sequence \([1,,T]\) can be used in the reverse process of DDIM since \(_{s}\) for any \(s<t\) can be sampled from \(_{t}\) via the \(_{0}\) prediction, enabling a significant boost in the reverse process computation.

In the rest of the paper, the operation sampling the next denoised data in the reverse process with the learned distribution \(p_{}(_{t-1}|_{t})\) defined in either Eq. 6 (DDPM) or Eq. 9 (DDIM) is denoted as:

\[(_{t},t,),\] (11)

which takes a noisy data \(_{t}\) at timestep \(t\) and a unit Gaussian noise \((,)\) as input.

### Joint Diffusion

In an image diffusion model, each sample from the data distribution is either a 2D grid of per-pixel colors, or a 2D grid of latent features (as in Latent Diffusion ) that can be encoded from or decoded to a real image through a pretrained encoder \(\) and decoder \(\). In the rest of the paper, the term _image_ will thus be used to refer to either a _color_ image or a _latent feature_ image, unless explicitly stated otherwise. Image diffusion models pretrained on fixed-size images cannot be used directly to produce arbitrary-size images. MultiDiffusion  has addressed this limitation by using a multi-window joint diffusion approach. The framework integrates images generated from multiple windows seamlessly by _averaging_ colors or features across the windows at _every_ reverse diffusion step. For instance, consider the case of generating a panorama image \(^{H_{x} W_{x} D}\). The image at each window \(^{(i)}^{H_{x} W_{x} D}\) is a subarea of the panorama image whose union across all the windows covers the entire panorama image. Let \(^{(i)}^{H_{x} H_{x}}\) denote a binary mask for the subregion in the panorama image corresponding to the \(i\)-th window. The function \(_{ i}:^{H_{x} W_{x} D} ^{H_{x} W_{x} D}\) maps (crops) the panorama image \(z\) to the \(i\)-th window image, while \(_{i}:^{H_{x} W_{x} D} ^{H_{z} W_{x} D}\) is its inverse function that fills the region outside of the mask \(_{i}\) with zeros. During the joint diffusion process running the reverse process simultaneously for each window, the noisy images from the windows \(_{t}^{(i)}\) are first averaged in the panorama space:

\[_{t}=_{i}( _{t}^{(i)})}{_{i}^{(i)}},\] (12)

and then, the resulting combined noisy image \(_{t}\) is cropped again for each window \(_{t}^{(i)}=_{ i}(_{t})\), modifying the noisy image at each window with a inter-window regularization.

## 4 SyncDiffusion

While MultiDiffusion  can generate seamless panorama images from joint diffusion, it often fails to produce coherent and realistic montages. The left image in Fig. 2 demonstrates that the resulting image often oddly combines various scenes, such as mountains with trees and snow. Also, the blending occasionally fails to merge them in a realistic manner, as shown in the figure where distant objects are connected to closer objects. This incoherence issue in MultiDiffusion arises due to two main reasons. Firstly, the averaging operation only aligns the colors or features in the overlapped regions but does not match the _content_ or _style_ of the images. Secondly, it only enforces _adjacent_ views to influence each other, and thus global coherence between distant windows cannot be achieved.

Figure 2: Panoramas generated by MultiDiffusion  (left) and our SyncDiffusion (right), with a prompt _“A photo of a mountain range at twilight”_. MultiDiffusion often combines various scenes, such as mountains with trees and snow, and even awkwardly blends them. In contrast, SyncDiffusion generates panoramas that are significantly more coherent.

To address this problem, we introduce a module called SyncDiffusion which enables the generation of coherent montages, as shown on the right in Fig. 2. This module can be easily integrated into an existing joint diffusion framework. Similar to MultiDiffusion, our SyncDiffusion module updates the noisy image at every step of the reverse diffusion process. In contrast to averaging the colors or latent features in the overlapped regions, however, SyncDiffusion employs the backpropagation of gradients from a perceptual similarity loss computed across the windows to perform the update. The perceptual similarity loss, denoted as \(\), can utilize any off-the-shelf loss function for perceptual similarity, such as LPIPS  and Style Loss . To facilitate efficient computation, we designate an _anchor_ window with an index of \(0\). For each view's noisy color image \((_{t}^{(i)})\) and the anchor window's noisy color image \((_{t}^{(0)})\) (where the decoder \(\) can be treated as an identity function if the given diffusion model operates in image space rather than latent space), one can measure the coherence using the images and conduct gradient descent through \(_{t}^{(i)}\):

\[}_{t}^{(i)}=_{t}^{(i)}-w_{_{t}^{(i)}} ((_{t}^{(i)}),(_{t }^{(0)})),\] (13)

where \(w\) is the weight of the gradient descent. However, the coherence measured with the _noisy_ images cannot provide meaningful guidance. Fig. 3 shows examples where the left three images are the intermediate noisy images \((_{t})\) at timestep \(t=45\) out of a total of \(50\) timesteps in the DDIM

Figure 3: LPIPS  scores computed across the noisy images \((_{t})\) at the intermediate step (\(t=45\) out of \(50\)) of the reverse process (left), the _predicted_ denoised images \(((_{t},t))\) at the same timestep \(t\) (middle), and the final generated images \((_{0})\) at timestep \(t=0\) (right). The indistinguishable noisy images yield similar LPIPS scores among them, whereas the predicted denoised images, which closely resemble the final outputs even at the beginning of the denoising process, exhibit LPIPS scores that align with those of the final generated images. This indicates that the predicted denoised images can provide meaningful guidance for producing coherent panoramas in the diffusion process.

reverse process. Note that the LPIPS scores among the noisy images are indistinguishable. Hence, similar to the DDIM reverse process, we utilize the _foreseen denoised_ observation of each noisy data \(_{}(_{t}^{(i)},t)\) in Eq. 10. We measure the coherence not with the current noisy color images \(\{(_{t}^{(i)})\}\) but with the predicted denoised color images \(\{(_{}(_{t}^{(i)},t))\}\) with the timestep \(t\) and perform the backpropagation of the gradient through \(_{t}^{(i)}\), resulting in the following updated formulation:

\[}_{t}^{(i)}=_{t}^{(i)}-w_{_{t}^{(i)} }((_{}(_{t}^{(i)},t)),(_{}(_{t}^{(0)},t))).\] (14)

In Fig. 3, the middle three images depict the predicted denoised images \((_{}(_{t},t))\) at timestep \(t=45\), which closely resemble the final generated images \((_{0})\) at timestep \(t=0\), even during the initial stages of the reverse diffusion process. Therefore, the LPIPS scores among the predicted denoised images also match those of the generated images, providing meaningful guidance for maintaining coherence. During each denoising step in the joint reverse process, we apply this update to the noisy images for all windows \(\{_{t}^{(i)}\}\), and sample the one-step denoised images. MultiDiffusion is also applied to average the sampled images at the end. Refer to Alg. 1 for detailed pseudocode.

## 5 Results

### Text-Guided Panorama Generation

In our experiments, we generate panorama images using our SyncDiffusion method and the pretrained Stable Diffusion 2.0  model. Stable Diffusion model operates in a latent space of \(^{64 64 4}\) and generates images of \(^{512 512 3}\). We generate panorama images of resolution \(512 3072\) (\(64 384\) in the latent space), where the width is six times the width of the output of Stable Diffusion. Each window \(^{(i)}\) has an image resolution of \(512 512\), with a stride of \(128\) pixels along the width in the image space which is equivalent to stride 16 in the latent space, resulting in a total 21 windows to operate diffusion processes jointly. We use six text prompts from MultiDiffusion  (see S.8 of the **supplementary**) and generate 500 panoramas per prompt. For the gradient descent weight \(w\) (Eq. 14), we experiment with various initial values while applying a weight decay with a rate of \(0.95\). We also set the center window as the anchor window with an index of \(0\).

BaselinesWe compare our SyncDiffusion with previous methods that generate panoramas using a pretrained diffusion model. Blended Latent Diffusion  is an inpainting-based method that

Figure 4: Qualitative comparisons. Blended Latent Diffusion  (BLD, the first row of each case) tends to exhibit visible seams and repetitive patterns. MultiDiffusion  (MD, the second row of each case) generates seamless results but lacks coherence, such as blending a sunset sky with a blue sky (top right), and displaying a combination of purple, pink, and blue backgrounds (bottom). In contrast, our SyncDiffusion (the last in each case) produces seamless panoramas with significantly improved coherence. Best view in zoom and color.

extrapolates a single window image. MultiDiffusion , served as the base of our framework, is a special case of our method when the weight of gradient descent \(w\) is \(0\). The same Stable Diffusion 2.0 model is used for all the methods for a fair comparison.

Evaluation MetricsWe utilize a range of metrics to assess the coherence, fidelity, diversity, and compatibility of the output panoramas with the input prompt.

* (_Coherence_) **Intra-LPIPS** and **Intra-Style-L**: To assess the coherence of the generated panoramas, we introduce two metrics. Intra-LPIPS and Intra-Style-L, which are computed as the averages of LPIPS  and Style Loss , respectively, between a pair of non-overlapping window images from the same panorama. Specifically, we divide the panorama into 6 windows, each with dimensions of \(512 512\) and then compute the average of LPIPS and Style Loss across the 15 combinations of these cropped views. To provide a reference for the scale of these values, we generate 500 single-window-size images using the same Stable Diffusion model and compute the LPIPS and Style Loss for randomly selected 1,000 pairs of these reference images.
* (Fidelity) **Mean-GIQA**: GIQA quantifies the fidelity of _individual_ images by calculating the inverse of the distance between a query image and a reference set in a feature space. Mean-GIQA is computed by taking a _single_ random crop of each panorama in \(512 512\) size and computing the average GIQA score from each cropped image to the reference set of images mentioned above.
* (Fidelity & Diversity) **FID** and **KID**: FID  and KID  are used to measure both fidelity and diversity. Both of them are measured with the aforementioned randomly cropped images and the set of reference images.
* (_Compatibility with the Input Prompt_) **Mean-CLIP-S**: The compatibility with the input prompt is assessed using the mean of CLIP scores , denoted as Mean-CLIP-S. This metric is calculated using the same set of cropped images and the input prompt.

Qualitative ComparisonsFig. 4 showcases qualitative comparisons between our method and the baseline methods. Here, we show the results of our method generated with a weight parameter of \(w=20\). Blended Latent Diffusion  often exhibits visible seams due to the sequential inpainting scheme and produces repetitive patterns in the extrapolation, as illustrated by the mountains in the second case of the top row and the flowers and trees in both cases of the bottom row. MultiDiffusion  achieves seamless outputs, although it often produces incoherent outputs, such as mixing a sunset sky with a blue sky, as shown in the second cases of the first and second rows, and pink and purple backgrounds with blue backgrounds in the two cases of the bottom row. Our SyncDiffusion generates visually and semantically more coherent panoramas with all the prompts. More qualitative comparisons are provided in the **supplementary (S.1, S.8)**.

Quantitative ResultsFig. 5.1 presents a quantitative comparison among the methods. Our method's results are displayed using two different gradient descent weights, \(w=10\) and \(20\). MultiDiffusion  is also the case when \(w=0\) in our framework. For results obtained with different weights, please refer to the **supplementary (S.2)**. The color bars in the plots indicate the average scores across the six prompts, while the black lines depict the standard deviation. Note that as the gradient weight increases, both Intra-LPIPS and Intra-Style-L decrease. When \(w=20\), the Intra-LPIPS and Intra-Style-L of our method are approximately 3/4 and 1/6 of those computed with the reference set images (referred to as SD, Stable Diffusion), respectively, indicating significantly higher coherence. Moreover, the Mean-CLIP-S and Mean-GIQA scores are comparable to those computed with the reference set, meaning that the compatibility with the input prompt and fidelity are not compromised by our diffusion synchronization. The results of FID and KID demonstrate the trade-off between coherence and diversity. As the gradient descent weight \(w\) increases, FID and KID also increase slightly, although they are still much lower compared to Blended Latent Diffusion . This implies that for certain images, it is more difficult to find coherent images. In the **supplementary (S.3)**, we substantiate this claim with the results of shorter generated panoramas. Blended Latent Diffusion results in low Intra-LPIPS due to its tendency to repeat similar patterns, but it leads to low Mean-GIQA and very high FID and KID, indicating a significant degradation in fidelity.

User StudyWe conducted three user studies to further evaluate the coherence, image quality and prompt compatibility of the generated panoramas, respectively. Following Ritchie , participants were presented with panorama images generated by both MultiDiffusion  and our SyncDiffusion methods (with \(w=20\)). They were then asked to choose one of them by answering the question: Which one appears a more coherent panorama image to you? **(Coherence)**, Which one is of higher quality? **(Image Quality)**, or Which one best matches the shared caption? **(Prompt Compatibility)**. We collected 25 responses each, including 5 vigilance tasks, from 100 participants for each user study. The results in Tab. 1 affirm that human evaluators perceive SyncDiffusion as producing more coherent results compared to MultiDiffusion, while also demonstrating superior image quality and higher prompt compatibility. Refer to the **supplementary (S.7)** for detailed setups for the user study.

### Additional Applications of SyncDiffusion

We further demonstrate the versatility of SyncDiffusion through three additional plug-and-play applications: layout-guided image generation, conditional image generation and 360-degree panorama generation.

Layout-Guided Image GenerationPlugging SyncDiffusion into the layout-to-image pipeline in MultiDiffusion  leads to a notable enhancement in the global coherence as displayed in Fig. 6-(A). While MultiDiffusion (middle row) generates an unnatural image with incoherent background around the house and the bear, our method produces a natural image with a globally coherent background.

Conditional Image GenerationWhen integrated with ControlNet , SyncDiffusion extends the conditional image generation to arbitrary resolutions. Let \(c^{H_{x} W_{x} 3}\) denote an input condition and \(_{c i}:^{H_{x} W_{x} 3}^{H_{x}  W_{x} 3}\) be a mapping from \(c\) to the \(i\)-th cropped condition corresponding to the window \(_{t}^{(i)}\). We define conditional SyncDiffusion by substituting \(_{}(_{t},t)\) in Alg. 1 with \(_{}(_{t},t,c^{(i)})\), where \(c^{(i)}:=_{c i}(c)\). Fig. 6-(B) illustrates that the combination of ControlNet and SyncDiffusion generates coherent panoramas while reflecting the given condition Canny edge map (top row).

360-degree Panorama GenerationWe further plug SyncDiffusion into MVDiffusion , a concurrent work that generates 360-degree panoramas from text prompts via multi-view diffusion. As shown Fig. 6-(C), our SyncDiffusion distinctly improves the global coherence of the generated panorama. The increase in coherence becomes more apparent when comparing perspective views

    & Coherence (\(\%\)) & Image Quality (\(\%\)) & Prompt Compatibility (\(\%\)) \\  MultiDiffusion  & 33.65 & 42.81 & 40.50 \\ SyncDiffusion & **66.35** & **57.19** & **59.50** \\   

Table 1: User study results.

Figure 5: Quantitative results. MultiDiffusion  (MD) can be considered as a special case of our method when the gradient descent weight \(w\) is set to 0. As \(w\) increases, coherence (Intra-LPIPS and Intra-Style-L) improves while maintaining the compatibility with the input prompt (Mean-CLIP-S) and fidelity (Mean-GIQA). There is a trade-off between coherency and diversity, as indicated by the FID and KID results. Note that the FID and KID of our method are still significantly lower than those of Blended Latent Diffusion  (BLD). SD (Stable Diffusion) is the score with the reference set images. Refer to the text for the details.

from different angles. While View 1 and View 2 from the vanilla MVDiffusion (top row) seem to be from two different rooms, with our method the generated images better depict two views from the same room (bottom row).

LimitationsWhile our SyncDiffusion module can significantly enhance the coherence of generated panoramas, it relies on appropriate input prompts to achieve realistic results, as illustrated in Fig. 7. Also, the SyncDiffusion module that includes a forward pass through the neural network and gradient descent computation introduces additional computational overhead.

SupplementaryDue to space constraints, we present the following additional results in the Supplementary: more qualitative comparisons with various prompts (S.1, S.8), details about the quantitative evaluation (S.2), evaluation on different resolutions (S.3), results with Style Loss  as the perceptual loss (S.4), an ablation study using Eq. 13 instead of Eq. 14 (S.5), an analysis of computation time (S.6), and details on the user study (S.7).

## 6 Conclusion

We presented SyncDiffusion, a diffusion synchronization module designed to generate coherent montages through joint diffusions. Using a pretrained diffusion model, we propose guiding the reverse process by updating the noisy images at each intermediate step using gradient descent. This update is based on a perceptual similarity loss calculated with the predictions of the denoised images. Moreover, the idea of SyncDiffusion can be applied to generating textures for 3D models. We plan to investigate such possibilities in future work.

Potential Negative Societal ImpactsImage generative models can potentially generate deepfakes, images resembling copyrighted material, biased or discriminatory images, and harmful outputs. Future research is needed to advance the detection of manipulated content and establish societal barriers to protect intellectual property.

Figure 6: Plug-and-play applications of SyncDiffusion.

Figure 7: Our failure cases. A suitable input prompt is required to generate realistic panoramas.