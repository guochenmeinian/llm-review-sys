# Accelerated Zeroth-order Method for Non-Smooth Stochastic Convex Optimization Problem with Infinite Variance

Accelerated Zeroth-order Method for Non-Smooth Stochastic Convex Optimization Problem with Infinite Variance

 Nikita Kornilov

MIPT, SkoITech

kornilov.nm@phystech.edu &Ohad Shamir

Weizmann Institute of Science

ohad.shamir@weizmann.ac.il &Aleksandr Lobanov

MIPT, ISP RAS

lobanov.av@mipt.ru &Darina Dvinskikh

HSE University,

ISP RAS

dmdvinskikh@hse.ru &Alexander Gasnikov

MIPT,

ISP RAS, SkoITech

gasnikov@yandex.ru &Innokentiy Shibaev

MIPT,

IITP RAS

innokentiy.shibayev@phystech.edu &Eduard Gorbunov

MBZUAI

eduard.gorbunov@mbzuai.ac.ae &Samuel Horvath

MBZUAI

samuel.horvath@mbzuai.ac.ae

###### Abstract

In this paper, we consider non-smooth stochastic convex optimization with two function evaluations per round under infinite noise variance. In the classical setting when noise has finite variance, an optimal algorithm, built upon the batched accelerated gradient method, was proposed in . This optimality is defined in terms of iteration and oracle complexity, as well as the maximal admissible level of adversarial noise. However, the assumption of finite variance is burdensome and it might not hold in many practical scenarios. To address this, we demonstrate how to adapt a refined clipped version of the accelerated gradient (Stochastic Similar Triangles) method from  for a two-point zero-order oracle. This adaptation entails extending the batching technique to accommodate infinite variance -- a non-trivial task that stands as a distinct contribution of this paper.

## 1 Introduction

In this paper, we consider stochastic non-smooth convex optimization problem

\[_{x^{d}}\{f(x)}}{{=}} _{}[f(x,)]\},\] (1)

where \(f(x,)\) is \(M_{2}()\)-Lipschitz continuous in \(x\) w.r.t. the Euclidean norm, and the expectation \(_{}[f(x,)]\) is with respect to a random variable \(\) with unknown distribution \(\). The optimization is performed only by accessing two function evaluations per round rather than sub-gradients, i.e., for any pair of points \(x,y^{d}\), an oracle returns \(f(x,)\) and \(f(y,)\) with the same \(\). The primary motivation for this gradient-free oracle arises from different applications where calculating gradients is computationally infeasible or even impossible. For instance, in medicine, biology, and physics, the objective function can only be computed through numerical simulation or as the result of a real experiment, i.e., automatic differentiation cannot be employed to calculate function derivatives. Usually, a black-box function we are optimizing is affected by stochastic or computational noise. This noise can arise naturally from modeling randomness within a simulation or by computer discretization.

In classical setting, this noise has light tails. However, usually in black-box optimization, we know nothing about the function, only its values at the requested points are available/computable, so light tails assumption may be violated. In this case, gradient-free algorithms may diverge. We aim to develop an algorithm that is robust even to heavy-tailed noise, i.e., noise with infinite variance. In theory, one can consider heavy-tailed noise to simulate situations where noticeable outliers may occur (even if the nature of these outliers is non-stochastic). Therefore we relax classical finite variance assumption and consider less burdensome assumption of finite \(\)-th moment, where \((1,2]\).

In machine learning, interest in gradient-free methods is mainly driven by the bandit optimization problem , where a learner engages in a game with an adversary: the learner selects a point \(x\), and the adversary chooses a point \(\). The learner's goal is to minimize the average regret based solely on observations of function values (losses) \(f(x,)\). As feedback, at each round, the learner receives losses at two points. This corresponds to a zero-order oracle in stochastic convex optimization with two function evaluations per round. The vast majority of researches assume sub-Gaussian distribution of rewards. However, in some practical cases (e.g., in finance ) reward distribution has heavy-tails or can be adversarial. For the heavy-tailed bandit optimization, we refer to .

Two-point gradient-free optimization for non-smooth (strongly) convex objective is a well-researched area. Numerous algorithms have been proposed which are optimal with respect to two criteria: oracle and iteration complexity. For a detailed overview, see the recent survey  and the references therein. Optimal algorithms, in terms of oracle call complexity, are presented in . The distinction between the number of successive iterations (which cannot be executed in parallel) and the number of oracle calls was initiated with the lower bound obtained in . It culminated with the optimal results from , which provides an algorithm which is optimal in both criteria. Specifically, the algorithm produces \(\), an \(\)-solution of (1), such that we can guarantee \([f()]-_{x^{d}}f(x)\) after

\[ d^{}^{-1}\] successive iterations, \[ d^{-2}\] oracle calls (number of \[f(x,)\] calculations).

The convergence guarantee for this optimal algorithm from  was established in the classical setting of _light-tailed_ noise, i.e., when noise has finite variance: \(_{}[M_{2}()^{2}]<\). However, in many modern learning problems the variance might not be finite, leading the aforementioned algorithms to potentially diverge. Indeed, _heavy-tailed_ noise is prevalent in contemporary applications of statistics and deep learning. For example, heavy-tailed behavior can be observed in training attention models  and convolutional neural networks . Consequently, our goal is to develop an optimal algorithm whose convergence is not hampered by this restrictive assumption. To the best of our knowledge, no existing literature on gradient-free optimization allows for \(_{}[M_{2}()^{2}]\) to be infinite. Furthermore, convergence results for all these gradient-free methods were provided in expectation, that is, without (non-trivial) high-probability bounds. Although the authors of  mentioned (without proof) that their results can be formulated in high probability using , this aspect notably affects the oracle calls complexity bound and complicates the analysis.

A common technique to relax finite variance assumption is gradient clipping . Starting from the work of  (see also ), there has been increased interest in algorithms employing gradient clipping to achieve high-probability convergence guarantees for stochastic optimization problems with _heavy-tailed_ noise. Particularly, in just the last two years there have been proposed

* an optimal algorithm with a general proximal setup for non-smooth stochastic convex optimization problems with infinite variance  that converges in expectation (also referenced in ),
* an optimal adaptive algorithm with a general proximal setup for non-smooth online stochastic convex optimization problems with infinite variance  that converges with high probability,
* optimal algorithms using the Euclidean proximal setup for both smooth and non-smooth stochastic convex optimization problems and variational inequalities with infinite variance  that converge with high probability,
* an optimal variance-adaptive algorithm with the Euclidean proximal setup for non-smooth stochastic (strongly) convex optimization problems with infinite variance  that converges with high probability.

None of these papers discuss a gradient-free oracle. Moreover, they do not incorporate acceleration (given the non-smooth nature of the problems) with the exception of . Acceleration is a crucial component to achieving optimal bounds on the number of successive iterations. However, the approach in  presumes smoothness and does not utilize batching. To apply the convergence results from  to our problem, we need to adjust our problem formulation to be smooth. This is achieved by using the Smoothing technique [27; 36; 17]. In the work  authors proposed an algorithm based on Smoothing technique and non-accelerated Stochastic Mirror Descent with clipping. However, this work also does not support acceleration, minimization on the whole space and batching. Adapting the technique from  to incorporate batching necessitates a substantial generalization. We regard this aspect of our work as being of primary interest.

Heavy-tailed noise can also be handled without explicit gradient clipping, e.g., by using Stochastic Mirror Descent algorithm with a particular class of uniformly convex mirror maps . However, the convergence guarantee for this algorithm is given in expectation. Moreover, applying batching and acceleration is a non-trivial task. Without this, we are not able to get the optimal method in terms of the number of iterations and not only in terms of oracle complexity. There are also some studies on the alternatives to gradient clipping  but the results for these alternatives are given in expectation and are weaker than the state-of-the-art results for the methods with clipping. This is another reason why we have chosen gradient clipping to handle the heavy-tailed noise.

### Contributions

We generalize the optimal result from  to accommodate a weaker assumption that allows the noise to exhibit _heavy tails_. Instead of the classical assumption of finite variance, we require the boundedness of the \(\)-moment: there exists \((1,2]\) such that \(_{}[M_{2}()^{}]<\). Notably, when \(<2\), this assumption is less restrictive than the assumption of a finite variance and thus it has garnered considerable interest recently, see [41; 35; 29] and the references therein. Under this assumption we prove that for convex \(f\), an \(\)-solution can be found with _high probability_ after

\[ d^{}^{-1}\] successive iterations, \[(/)^{} ,\]

and for \(\)-strongly convex \(f\), the \(\)-solution can be found with _high probability_ after

\[ d^{}()^{-} ,\] \[(d/())^{} .\]

In both instances, the number of oracle calls is optimal in terms of \(\)-dependency within the non-smooth setting [27; 39]. For first-order optimization under _heavy-tailed_ noise, the optimal \(\)-dependency remains consistent, as shown in [35, Table 1].

In what follows, we highlight the following several important aspects of our results

* **High-probability guarantees.** We provide upper-bounds on the number of iterations/oracle calls needed to find a point \(\) such that \(f()-_{x^{d}}f(x)\) with probability at least \(1-\). The derived bounds have a poly-logarithmic dependence on \(}{{}}\). To the best of our knowledge, there are no analogous high-probability results, even for noise with bounded variance.
* **Generality of the setup.** Our results are derived under the assumption that gradient-free oracle returns values of stochastic realizations \(f(x,)\) subject to (potentially adversarial) bounded noise. We further provide upper bounds for the magnitude of this noise, contingent upon the target accuracy \(\) and confidence level \(\). Notably, our assumptions about the objective and noise are confined to a compact subset of \(^{d}\). This approach, which differs from standard ones in derivative-free optimization, allows us to encompass a wide range of problems. This approach differs from standard ones in derivative-free optimization
* **Batching without bounded variance.** To establish the aforementioned upper bounds, we obtain the following: given \(X_{1},,X_{B}\) as independent random vectors in \(^{d}\) where \([X_{i}]=x^{d}\) and \(\|X_{i}-x\|_{2}^{}^{}\) for some \( 0\) and \((1,2]\), then \[[\|_{i=1}^{B}X_{i}-x\|_{2}^{} ]}{B^{-1}}.\] (2)When \(=2\), this result aligns with the conventional case of bounded variance (accounting for a numerical factor of 2). Unlike existing findings, such as [40, Lemma 7] where \(<2\), the relation (2) does not exhibit a dependency on the dimension \(d\). Moreover, (2) offers a theoretical basis to highlight the benefits of mini-batching, applicable to methods highlighted in this paper as well as first-order methods presented in [35; 29].
* **Dependency on \(d\).** As far as we are aware, an open question remains: is the bound \((/)^{}\) optimal regarding its dependence on \(d\)? For smooth stochastic convex optimization problems using a \((d+1)\)-points stochastic zeroth-order oracle, the answer is negative. The optimal bound is proportional to \(d^{-}\). Consequently, for \((1,2)\), our results are intriguing because the dependence on \(d\) in our bound differs from known results in the classical case where \(=2\).

### Paper organization

The paper is organized as follows. In Section 2, we give some preliminaries, such as smoothing technique and gradient estimation, that are workhorse of our algorithms. Section 3 is the main section presenting two novel gradient-free algorithms along with their convergence results in high probability. These algorithms solve non-smooth stochastic optimization under _heavy-tailed_ noise, and they will be referred to as ZO-clipped-SSTM and R-ZO-clipped-SSTM (see Algorithms 1 and 2 respectively). In Section 4, we extend our results to gradient-free oracle corrupted by additive deterministic adversarial noise. In Section 5, we describe the main ideas behind the proof and emphasize key lemmas. In Section 6, we provide numerical experiments on the synthetic task that demonstrate the robustness of the proposed algorithms towards heavy-tailed noise.

## 2 Preliminaries

Assumptions on a subset.Although we consider an unconstrained optimization problem, our analysis does not require any assumptions to hold on the entire space. For our purposes, it is sufficient to introduce all assumptions only on some convex set \(Q^{d}\) since we prove that the considered methods do not leave some ball around the solution or some level-set of the objective function with high probability. This allows us to consider fairly large classes of problems.

**Assumption 1** (Strong convexity): _There exist a convex set \(Q^{d}\) and \( 0\) such that function \(f(x,)\) is \(\)-strongly convex on \(Q\) for any fixed \(\), i.e._

\[f( x_{1}+(1-)x_{2}) f(x_{1})+(1-)f(x_{2})- (1-)\|x_{1}-x_{2}\|_{2}^{2},\]

_for all \(x_{1},x_{2} Q,\)._

This assumption implies that \(f(x)\) is \(\)-strongly convex as well.

For a small constant \(>0\), let us define an expansion of set \(Q\) namely \(Q_{}=Q+B_{2}^{d}\), where \(+\) stands for Minkowski addition. Using this expansion we make the following assumption.

**Assumption 2** (Lipschitz continuity and boundedness of \(\)-moment): _There exist a convex set \(Q^{d}\) and \(>0\) such that function \(f(x,)\) is \(M_{2}()\)-Lipschitz continuous w.r.t. the Euclidean norm on \(Q_{}\), i.e., for all \(x_{1},x_{2} Q_{}\)_

\[|f(x_{1},)-f(x_{2},)| M_{2}()\|x_{1}-x_{2}\|_{2}.\]

_Moreover, there exist \((1,2]\) and \(M_{2}>0\) such that \(_{}[M_{2}()^{}] M_{2}^{}\)._

If \(<2\), we say that noise is _heavy-tailed_. When \(=2\), the above assumption recovers the standard uniformly bounded variance assumption.

**Lemma 1**: _Assumption 2 implies that \(f(x)\) is \(M_{2}\)-Lipschitz on \(Q\)._

Randomized smoothing.The main scheme that allows us to develop batch-parallel gradient-free methods for non-smooth convex problems is randomized smoothing [13; 17; 27; 28; 38] of a non-smooth function \(f(x,)\). The smooth approximation to a non-smooth function \(f(x,)\) is defined as \[_{}(x)}}{{=}}_{, }[f(x+,)],\] (3)

where \( U(B_{2}^{d})\) is a random vector uniformly distributed on the Euclidean unit ball \(B_{2}^{d}}}{{=}}\{x^{d}:\|x\|_ {2} 1\}\). In this approximation, a new type of randomization appears in addition to stochastic variable \(\).

The next lemma gives estimates for the quality of this approximation. In contrast to \(f(x)\), function \(_{}(x)\) is smooth and has several useful properties.

**Lemma 2**: _[_17_, Theorem 2.1.]_ _Let there exist a subset \(Q^{d}\) and \(>0\) such that Assumptions 1 and 2 hold on \(Q_{}\). Then,_

1. _Function_ \(_{}(x)\) _is convex, Lipschitz with constant_ \(M_{2}\) _on_ \(Q\)_, and satisfies_ \[_{x Q}|_{}(x)-f(x)| M_{2}.\]
2. _Function_ \(_{}(x)\) _is differentiable on_ \(Q\) _with the following gradient_ \[_{}(x)=_{}[f(x+ )],\] _where_ \( U(S_{2}^{d})\) _is a random vector uniformly distributed on unit Eucledian sphere_ \(S_{2}^{d}}}{{=}}\{x^{d}:\|x\|_ {2}=1\}\)_._
3. _Function_ \(_{}(x)\) _is_ \(L\)_-smooth with_ \(L=[d]{dM_{2}}/\) _on_ \(Q\)_._

Our algorithms will aim at minimizing the smooth approximation \(_{}(x)\). Given Lemma 2, the output of the algorithm will also be a good approximate minimizer of \(f(x)\) when \(\) is sufficiently small.

Gradient estimation.Our algorithms will based on randomized gradient estimate, which will then be used in a first order algorithm. Following , the gradient can be estimated by the following vector:

\[g(x,,)=(f(x+,)-f(x-,)),\] (4)

where \(>0\) and \( U(S_{2}^{d})\). We also use batching technique in order to allow parallel calculation of gradient estimation and acceleration. Let \(B\) be a batch size, we sample \(\{_{i}\}_{i=1}^{B}\) and \(\{_{i}\}_{i=1}^{B}\) independently, then

\[g^{B}(x,\{\},\{\})=_{i=1}^{B}(f(x+ _{i},_{i})-f(x-_{i},_{i}))_{i}.\] (5)

The next lemma states that \(g^{B}(x,\{\},\{\})\) from (5) is an unbiased estimate of the gradient of \(_{}(x)\) (3). Moreover, under heavy-tailed noise (Assumption 2) with bounded \(\)-moment \(g^{B}(x,\{\},\{\})\) has also bounded \(\)-moment.

**Lemma 3**: _Under Assumptions 1 and 2, it holds_

\[_{,}[g(x,,)]=_{\{\},\{ \}}[g^{B}(x,\{\},\{\})]=_{}(x).\]

_and_

\[_{,}[\|g(x,,)-_{ ,}[g(x,,)]\|_{2}^{}]^{} }}{{=}}(M_{2}}{2^{}})^{},\] \[_{\{\},\{\}}[\|g^{B}(x,\{\},\{\})-_{\{\},\{\}}[g^{B}(x,\{\},\{\})]\|_{ 2}^{}]}{B^{-1}}.\] (6)Main Results

In this section, we present two our new zero-order algorithms, which we will refer to as ZO-clipped-SSTM and R-ZO-clipped-SSTM, to solve problem (1) under _heavy-tailed_ noise assumption. To deal with heavy-tailed noise, we use clipping technique which clips heavy tails. Let \(>0\) be clipping constant and \(g^{d}\), then clipping operator clip is defined as

\[(g,)=} (\|g\|_{2},),&g 0,\\ 0,&g=0.\] (7)

We apply clipping operator for batched gradient estimate \(g^{B}(x,\{\},\{\})\) from (5) and then feed it into first-order Clipped Stochastic Similar Triangles Method (clipped-SSTM) from . We will refer to our zero-order versions of clipped-SSTM as ZO-clipped-SSTM and R-ZO-clipped-SSTM for the convex case and strongly convex case respectively.

### Convex case

Let us suppose that Assumption 1 is satisfied with \(=0\).

```
0: starting point \(x^{0}\), number of iterations \(K\), batch size \(B\), stepsize \(a>0\), smoothing parameter \(\), clipping levels \(\{_{k}\}_{k=0}^{K-1}\).
1: Set \(L=M_{2}/\), \(A_{0}=_{0}=0\), \(y^{0}=z^{0}=x^{0}\)
2:for\(k=0,,K-1\)do
3: Set \(_{k+1}=}{{2a_{2}L}}\), \(A_{k+1}=A_{k}+_{k+1}\).
4:\(x^{k+1}=y^{k}+_{k+1}z^{k}}{A_{k+1}}\).
5: Sample \(\{_{i}^{k}\}_{i=1}^{B}\) and \(\{_{i}^{k}\}_{i=1}^{B} S_{2}^{d}\) independently.
6: Compute gradient estimation \(g^{B}(x^{k+1},\{^{k}\},\{^{k}\})\) as defined in (5).
7: Compute clipped \(_{k+1}=(g^{B}(x^{k+1},\{^{k}\},\{^{k }\}),_{k})\) as defined in (7).
8:\(z^{k+1}=z^{k}-_{k+1}_{k+1}\).
9:\(y^{k+1}=y^{k}+_{k+1}z^{k+1}}{A_{k+1}}\).
10:endfor
10:\(y^{K}\) ```

**Algorithm 1** ZO-clipped-SSTM \((x^{0},K,B,a,,\{_{k}\}_{k=0}^{K-1})\)

**Theorem 1** (Convergence of ZO-clipped-SSTM): _Let Assumptions 1 and 2 hold with \(=0\) and \(Q=^{d}\). Let \(\|x^{0}-x^{*}\|^{2} R^{2}\), where \(x^{0}\) is a starting point and \(x^{*}\) is an optimal solution to (1). Then for the output \(y^{K}\) of ZO-clipped-SSTM, run with batchsize \(B\), \(A=}{{}} 1\), \(a=(\{A^{2},M_{2}K^{}A^{}/\!\!\!\!/_{LRB}\}),= }{{4M_{2}}}\) and \(_{k}=(R/A)}}{{A}})\), we can guarantee \(f(y^{K})-f(x^{*})\) with probability at least \(1-\) ( for any \((0,1]\)) after_

\[K=}(\{R}{ },(M_{2}R}{})^{}\})\] (8)

_successive iterations and \(K B\) oracle calls. Moreover, with probability at least \(1-\) the iterates of ZO-clipped-SSTM remain in the ball \(B_{2R}(x^{*})\), i.e., \(\{x^{k}\}_{k=0}^{K+1},\{y^{k}\}_{k=0}^{K},\{z^{k}\}_{k=0}^{K} B_{2R}( x^{*})\)._

Here and below notation \(}\) means an upper bound on the growth rate hiding logarithms. The first term in bound (8) is optimal for the deterministic case for non-smooth problem (see ) and the second term in bound (8) is optimal in \(\) for \((1,2]\) and zero-point oracle (see ).

We notice that increasing the batch size \(B\) to reduce the number of successive iterations makes sense only as long as the first term of (8) lower than the second one, i.e. there exists optimal value of batchsize

\[B(M_{2}R}{})^{}.\]

### Strongly-convex case

Now we suppose that Assumption 1 is satisfied with \(>0\). For this case we employ ZO-clipped-SSTM with restarts technique. We will call this algorithm as R-ZO-clipped-SSTM (see Algorithm 2). At each round R-ZO-clipped-SSTM call ZO-clipped-SSTM with starting point \(^{t}\), which is the output from the previous round, and for \(K_{t}\) iterations.

```
0: starting point \(x^{0}\), number of restarts \(N\), number of steps \(\{K_{t}\}_{t=1}^{N}\), batchsizes \(\{B_{t}\}_{t=1}^{N}\), stepsizes \(\{a_{t}\}_{t=1}^{N}\), smoothing parameters \(\{_{t}\}_{t=1}^{N}\), clipping levels \(\{_{k}^{1}\}_{k=0}^{K_{1}-1},...,\{_{k}^{N}\}_{k=0}^{K_{N}-1}\)
1:\(^{0}=x^{0}\).
2:for\(t=1,,N\)do
3:\(^{t}=\) ZO-clipped-SSTM \((^{t-1},K_{t},B_{t},a_{t},_{t},\{_{k}^{t}\}_{k=0}^{K_{ 1}-1})\).
4:endfor
5:\(^{N}\) ```

**Algorithm 2** R-ZO-clipped-SSTM

**Theorem 2** (Convergence of R-ZO-clipped-SSTM): _Let Assumptions 1, 2 hold with \(>0\) and \(Q=^{d}\). Let \(\|x^{0}-x^{*}\|^{2} R^{2}\), where \(x^{0}\) is a starting point and \(x^{*}\) is the optimal solution to (1). Let also \(N=_{2}(}}{{2}})\) be the number of restarts. Let at each stage \(t=1,...,N\) of R-ZO-clipped-SSTM, ZO-clipped-SSTM is run with batchsize \(B_{t}\), \(_{t}=}}{{4M_{2}}},L_{t}=}}{{ _{t}}},K_{t}=(\{R_{t-1}^{2}-1}/ _{t},(}}{{_{t}}})^{(-1 )}/B_{t}\})\), \(a_{t}=(\{1,^{}}}{{L_{t}R_{t}}}\})\) and \(_{k}^{t}=(^{_{k+1}}}}{{ _{k+1}^{2}}})\), where \(R_{t-1}=2^{-}R\), \(_{t}=^{2}-1}}{{4}}\), \( 4NK_{t/} 1\), \((0,1]\). Then to guarantee \(f(^{N})-f(x^{*})\) with probability at least \(1-\), R-ZO-clipped-SSTM requires_

\[}(\{^{2}}{ }},(^{2}}{})^{}\})\] (9)

_oracle calls. Moreover, with probability at least \(1-\) the iterates of_ R-ZO-clipped-SSTM _at stage \(t=1,,N\) stay in the ball \(B_{2R_{t-1}}(x^{*})\)._

The obtained complexity bound (see the proof in Appendix C.2) is the first optimal (up to logarithms) high-probability complexity bound under Assumption 2 for the smooth strongly convex problems. Indeed, the first term cannot be improved in view of the deterministic lower bound , and the second term is optimal .

## 4 Setting with Adversarial Noise

Often, black-box access of \(f(x,)\) are affected by some deterministic noise \((x)\). Thus, now we suppose that a zero-order oracle instead of objective values \(f(x,)\) returns its noisy approximation

\[f_{}(x,)}}{{=}}f(x,)+(x).\] (10)

This noise \((x)\) can be interpreted, e.g., as a computer discretization when calculating \(f(x,)\). For our analysis, we need this noise to be uniformly bounded. Recently, noisy <<black-box>> optimization with bounded noise has been actively studied [11; 25]. The authors of  consider deterministic adversarial noise, while in  stochastic adversarial noise was explored.

**Assumption 3** (Boundedness of noise): _There exists a constant \(>0\) such that \(|(x)|\) for all \(x Q\)._

This is a standard assumption often used in the literature (e.g., ). Moreover, in some applications  the bigger the noise the cheaper the zero-order oracle. Thus, it is important to understand the maximum allowable level of adversarial noise at which the convergence of the gradient-free algorithm is unaffected.

### Non-smooth setting

In noisy setup, gradient estimate from (4) is replaced by

\[g(x,,)=(f_{}(x+,)-f_{ }(x-,)).\] (11)

Then (6) from Lemma 3 will have an extra factor driven by noise (see Lemma \(2.3\) from )

\[_{\{\},\{\}}[\|g^{B}(x,\{\},\{ \})-_{\{\},\{\}}[g^{B}(x,\{\},\{\})]\| _{2}^{}]}(M_{2}}{2^{ }}+)^{}.\]

To guarantee the same convergence of the algorithm as in Theorem 1 (see (8)) for the adversarial deterministic noise case, the variance term must dominate the noise term, i.e. \(d^{-1}M_{2}\). Note that if the term with noise dominates the term with variance, it does not mean that the gradient-free algorithm will not converge. In contrast, algorithm will still converge, only slower (oracle complexity will be \(^{-2}\) times higher). Thus, if we were considering the zero-order oracle concept with adversarial stochastic noise, it would be enough to express the noise level \(\), and substitute the value of the smoothing parameter \(\) to obtain the maximum allowable noise level. However, since we are considering the concept of adversarial noise in a deterministic setting, following previous work [11; 1] we can say that adversarial noise accumulates not only in the variance, but also in the bias:

\[_{,}[g(x,,)]-_{}( x),r\|r\|_{2}^{-1},r^{d}.\]

This bias can be controlled by the noise level \(\), i.e., in order to achieve the \(\)-accuracy algorithm considered in this paper, the noise condition must be satisfied:

\[}.\] (12)

As we can see, we have a more restrictive condition on the noise level in the bias (12) than in the variance (\(}}{{}}\)). Thus, the maximum allowable level of adversarial deterministic noise, which guarantees the same convergence of \(\)-clipped-SSTM, as in Theorem 1 (see (8)) is as follows

\[}{RM_{2}},\]

where \(=}{{2M_{2}}}\) the smoothing parameter from Lemma 2.

**Remark 1** (\(\)**-strongly convex case**): _Let us assume that \(f(x)\) is also \(\)-strongly convex (see Assumption 1). Then, following the works [11; 22], we can conclude that the \(\)-\(\)-clipped-SSTM has the same oracle and iteration complexity as in Theorem 2 at the following maximum allowable level of adversarial noise: \(^{3/2}}}{{}}/}{{d}}M_{2}\)._

### Smooth setting

Now we examine the maximum allowable level of noise at which we can solve optimization problem (1) with \(\)-precision under the following additional assumption

**Assumption 4** (Smoothness): _The function \(f\) is \(L\)-smooth, i.e., it is differentiable on \(Q\) and for all \(x,y Q\) with \(L>0\):_

\[\| f(y)- f(x)\|_{2} L\|y-x\|_{2}.\]

If Assumption 4 holds, then Lemma 2 can be rewritten as

\[_{x Q}|_{}(x)-f(x)|}{2}.\]

Thus, we can now present the convergence results of the gradient-free algorithm in the smooth setting. Specifically, if the Assumptions 2-4 are satisfied, then \(\)-clipped-SSTM converges to \(\)-accuracy after \(K=}(^{-1}})\) iterations with probability at least \(1-\). It is easy to see that the iteration complexity improves in the smooth setting (since the Lipschitz gradient constant \(L\) already exists,i.e., no smoothing is needed), but oracle complexity remained unchanged (since we are still using the gradient approximation via \(l_{2}\) randomization (11) instead of the true gradient \( f(x)\)), consistent with the already optimal estimate on oracle calls: \(}((M_{2}R^{-1})^{})\). And to obtain the maximum allowable level of adversarial noise \(\) in the smooth setting, which guarantees such convergence, it is sufficient to substitute the smoothing parameter \(=\) in the inequality (12):

\[}{R}.\]

Thus, we can conclude that smooth setting improves iteration complexity and the maximum allowable noise level for the gradient-free algorithm, but the oracle complexity remains unchanged.

**Remark 2** (\(\)-strongly convex case): _Suppose that \(f(x)\) is also \(\)-strongly convex (see Assumption 1). Then we can conclude that \(\)-\(\)-\(\)-\(\) has the oracle and iteration complexity just mentioned above at the following maximum allowable level of adversarial noise: \(}}{{}}}{{ }}\)._

**Remark 3** (Upper bounds optimality): _The upper bounds on maximum allowable level of adversarial noise obtained in this section in both non-smooth and smooth settings are optimal in terms of dependencies on \(\) and \(d\) according to the works ._

**Remark 4** (Better oracle complexity): _In the aforementioned approach in the case when \(f(x,)\) has Lipschitz gradient in \(x\) (for all \(\)) one can improve oracle complexity from \(}((M_{2}R^{-1})^{ })\) to \(}(d(M_{2}R^{-1})^{})\). This can be done by using component-wise finite-difference stochastic gradient approximation . Iteration complexity remains \(}(^{-1}})\). The same can be done for \(\)-strongly convex case: from \(}((dM_{2}^{2}()^{-1})^{ })\) to \(}(d(M_{2}^{2}()^{-1})^{ })\)._

## 5 Details of the proof

The proof is built upon a combination of two techniques. The first one is the Smoothing technique from  that is used to develop a gradient-free method for convex non-smooth problems based on full-gradient methods. The second technique is the Accelerated Clipping technique that has been recently developed for smooth problems with the noise having infinite variance and first-order oracle . The authors of  propose \(\)-\(\) method that we develop in our paper. We modify \(\)-\(\) by introducing batching into it. Note that due to the infinite variance, such a modification is interesting in itself. Then we run batched \(\)-\(\) with gradient estimations of function \(f\) obtained via Smoothing technique and two-point zeroth-order oracle. To do this, we need to estimate the variance of the clipped version of the batched gradient estimation.

In more detail, we replace the initial problem of minimizing \(f\) by minimizing its smoothed approximation \(_{}\), see Lemma 2 In order to use estimated gradient of \(_{}\) defined in (4) or (5), we make sure that it has bounded \(\)-th moment. For these purposes we proof Lemma 3. First part shows boundness of unbatched estimated gradient \(g\) defined in (4). It follows from measure concentration phenomenon for the Euclidean sphere for \(}{}\)-Lipschitz function \(f(x+)\). According to this phenomenon probability of the functions deviation from its math expectation becomes exponentially small and \(\)-th moment of this deviation becomes bounded. Furthermore, the second part of Lemma 3 shows that batching helps to bound \(\)-th moment of batched gradient \(g^{B}\) defined in (5) even more. Also batching allows parallel calculations reducing number of necessary iteration with the same number of oracle calls. All this is possible thanks to the result, interesting in itself, presented in the following Lemma.

**Lemma 4**: _Let \(X_{1},,X_{B}\) be \(d\)-dimensional martingale difference sequence (i.e. \([X_{i}|X_{i-1},,X_{1}]=0\) for \(1<i B\)) satisfying for \(1 2\)_

\[[\|X_{i}\|^{}|X_{i-1},,X_{1}]^{ }.\]

_Then we have_

\[[\|_{i=1}^{B}X_{i}\|_{2}^{} ]}{B^{-1}}.\]Next, we use the clipped-SSTM for function \(_{}\) with heavy-tailed gradient estimates. This algorithm was initially proposed for smooth functions in the work . The scheme for proving convergence with high probability is also taken from it, the only difference is additional randomization caused by smoothing scheme.

## 6 Numerical experiments

We tested ZO-clipped-SSTM on the following problem

\[_{x^{d}}\|Ax-b\|_{2}+,x,\]

where \(\) is a random vector with independent components sampled from the symmetric Levy \(\)-stable distribution with \(=}{{2}}\), \(A^{m d}\), \(b^{m}\) (we used \(d=16\) and \(m=500\)). For this problem, Assumption 1 holds with \(=0\) and Assumption 2 holds with \(=}{{2}}\) and \(M_{2}()=\|A\|_{2}+\|\|_{2}\).

We compared ZO-clipped-SSTM, proposed in this paper, with ZO-SGD and ZO-SSTM. For these algorithms, we gridsearched batchsize \(B:5,10,50,100,500\) and stepsize \(:1e-3,1e-4,1e-5,1e-6\). The best convergence was achieved with the following parameters:

* ZO-clipped-SSTM: \(=1e-3\), \(B=10\), \(=0.01\),
* ZO-SSTM: \(=1e-5,B=500\),
* ZO-SGD: \(=1e-4,B=100\), \(=0.9\), where \(\) is a heavy-ball momentum parameter.

The code is written on Python and is publicly available at https://github.com/ClippedStochasticMethods/ZO-clipped-SSTM. Figure 1 presents the comparison of convergences averaged over \(15\) launches with different noise. In contrast to ZO-clipped-SSTM, the last two methods are unclipped and therefore failed to converge under hairy-tailed noise.

## 7 Conclusion and future directions

In this paper, we propose a first gradient-free algorithm ZO-clipped-SSTM and to solve problem (1) under _heavy-tailed_ noise assumption. By using the restart technique we extend this algorithm for strongly convex objective, we refer to this algorithm as R-ZO-clipped-SSTM. The proposed algorithms are optimal with respect to oracle complexity (in terms of the dependence on \(\)), iteration complexity and the maximal level of noise (possibly adversarial). The algorithms can be adapted to composite and distributed minimization problems, saddle-point problems, and variational inequalities. Despite the fact that the algorithms utilize the two-point feedback, they can be modified to the one-point feedback. We leave it for future work.

Moreover, we provide theoretical basis to demonstrate benefits of batching technique in case of heavy-tailed stochastic noise and apply it to methods from this paper. Thanks to this basis, it is possible to use batching in other methods with heavy-tiled noise, e.g. first-order methods presented in .

## 8 Acknowledgment

The work of Alexander Gasnikov, Aleksandr Lobanov, and Darina Dvinskikh was supported by a grant for research centers in the field of artificial intelligence, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000D730321P5Q0002) and the agreement with the Ivannikov Institute for System Programming of dated November 2, 2021 No. 70-2021-00142.

Figure 1: Convergence of ZO-clipped-SSTM, ZO-SGD and ZO-clipped-SSTM in terms of a gap function w.r.t. the number of consumed samples.