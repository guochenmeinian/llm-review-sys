# D-LLM: A Token Adaptive Computing Resource Allocation Strategy for Large Language Models

Yikun Jiang\({}^{1}\), Huanyu Wang\({}^{1}\), Lei Xie\({}^{1}\), Hanbin Zhao\({}^{2}\),

**Chao Zhang\({}^{2}\), Hui Qian\({}^{2}\), John C.S. Lui\({}^{3}\)**

\({}^{1}\)FSI Lab, Huawei Technologies Co., Ltd.

\({}^{2}\)Zhejiang University \({}^{3}\)Chinese University of Hong Kong

{jiangyikun5,xiele170}@huawei.com

{huanyuhello,zhaohanbin,zczju,qianhui}@zju.edu.cn

cslui@cse.cuhk.edu.hk

Equal contribution. Correspondence to Hanbin Zhao. Our code is available at https://github.com/Jyk-122/D-LLM.

###### Abstract

Large language models have shown an impressive societal impact owing to their excellent understanding and logical reasoning skills. However, such strong ability relies on a huge amount of computing resources, which makes it difficult to deploy LLMs on computing resource-constrained platforms. Currently, LLMs process each token equivalently, but we argue that not every word is equally important. Some words should not be allocated excessive computing resources, particularly for dispensable terms in simple questions. In this paper, we propose a novel dynamic inference paradigm for LLMs, namely D-LLMs, which adaptively allocate computing resources in token processing. We design a dynamic decision module for each transformer layer that decides whether a network unit should be executed or skipped. Moreover, we tackle the issue of adapting D-LLMs to real-world applications, specifically concerning the missing KV-cache when layers are skipped. To overcome this, we propose a simple yet effective eviction policy to exclude the skipped layers from subsequent attention calculations. The eviction policy not only enables D-LLMs to be compatible with prevalent applications but also reduces considerable storage resources. Experimentally, D-LLMs show superior performance, in terms of computational cost and KV storage utilization. It can reduce up to 45% computational cost and KV storage on Q&A, summarization, and math solving tasks, 50% on commonsense reasoning tasks.

## 1 Introduction

Large language models(LLMs), have demonstrated amazing capability on generation tasks. Benefiting from existing parameter-efficient finetuning strategies , LLMs can also be adapted to specific domains with extra training costs. However, such models containing billions of parameters require expensive computation costs and memory overhead at the inference stage. Thus, deploying LLMs on resource-constrained platforms becomes challenging especially considering the ever-growing requirements for offline using, model customizing, and data privacy . To alleviate this problem, various inference acceleration techniques have been proposed, _e.g._, quantization , distillation , and pruning . Among these methods, pruning is a typical structural compression method to reduce the participating network layers. These methods usually remove redundant neurons based on hand-crafted metrics , when optimizing or finetuning large language models.

Although previous methods can decrease computational cost, many essential characteristics of natural languages are ignored. _First, tasks of different difficulties should not be allocated with the same computing resource._ Obviously, a response to "how are you" is much easier than "explaining a math theorem". _More importantly, not every word in a sentence is equally important._ Thus, it is not necessary to allocate too much computing resources to non-critical tokens, _i.e._, articles and punctuation marks. In this paper, we introduce a novel _dynamic inference mechanism, namely D-LLMs,_ which can boost inference speeds by dynamically skipping redundant network components. Specifically, we assign an execution decision module for each transformer layer. Before going through a transformer layer, tokens need to pass through the decision module to obtain an execution decision. Based on the execution decision, D-LLMs would adaptively decide whether the following layer should be executed or skipped. Thus, dispensable tokens and simple tasks would utilize much fewer layers than relevant tokens and difficult tasks. Another critical issue that should be emphasized here is how to apply such dynamic inference mechanism on LLMs, _e.g._, LLaMA , GPT [55; 58], _etc._ Note that it is unacceptable to retrain or finetune entire models due to the high computational overhead and massive training data. Therefore, we combine our strategy with finetuning methods by inserting low-rank adaptors, _LoRA_, for well pre-trained large language models.

With the aforementioned method, each token execution by a specific network topology can successfully reduce the computational cost at inference time. However, to deploy such dynamic inference networks for realistic scenarios, making this dynamic inference compatible with KV-cache strategies [13; 21; 29] is a crucial challenge. Compared with static inference networks which cache all KV embeddings of previous tokens, dynamic inference networks selectively skip layers would lead to KV-cache misses. To tackle this issue, we design a simple yet effective eviction strategy on KV-cache in our proposed D-LLMs framework. Specifically, we evict the KV embeddings of uncalculated transformer layers in previous tokens and simplify the regular causal masks to sparse ones. For example, if a given token skips a transformer layer at the inference time, it would not be included in self-attention calculation of subsequent tokens. This way, the attention mask in each layer is also dynamically changed according to the execution decisions of preceding tokens.

To summarize, the main contributions of our paper are concluded as follows:

* We propose a novel inference framework for LLMs, termed D-LLMs, which can significantly reduce computational resource requirements with an adaptive allocation scheme. We design decision modules in LLMs, which dynamically decide to execute or skip transformer layers at the inference time.
* We propose a simple yet effective eviction policy by transforming the causal self-attention masks. As a result, it not only makes the proposed D-LLMs compatible with KV-cache methods but also saves the storage overhead at inference time, which is particularly important for resource-constrained platforms.
* We conduct extensive experiments on main-stream LLMs. The results demonstrate that D-LLMs reduce up to 45% computational cost and KV-cache storage on Q&A and math solving tasks, and 50% on commonsense reasoning tasks without performance degradation.

## 2 Related Works.

**LLMs Finetuning Methods.** Due to the large amount of computational cost of training LLMs, various parameter efficient finetuning methods have been explored, including prompt-based learning methods [7; 19; 25; 41; 80], adapter-based learning methods[27; 34; 67; 74] and reparametrization-based learning methods [11; 15; 30; 45]. Prompt-based learning methods propose to insert and infer a collection of trainable embeddings to existing tokens. For example, P-tuning  adds a prefix to input, and P-tuning v2  applies prompts to intermediate layers. Adapter-based learning methods propose to insert specially designed adaptors to pre-trained LLMs. How to insert adaptors has also been explored, _i.e._, Adamix  and Compacter  serially connect adaptors and [27; 67] parallelly connect adaptors with transformer. Reparametrization-based learning methods exploit finetuning models with low intrinsic dimension . Specifically, LoRA  decomposes parameters into two low-rank trainable matrices before computation.

Different from these methods which focus on parameter-efficient finetuning, D-LLMs pay more attention to achieving high-speed inference via adaptive computing resource allocation.

**LLMs Acceleration Methods.** Extensive research works have been proposed to compress LLMs and reduce the network parameters. Among these methods, quantization [4; 36; 42; 57; 62], distillation [24; 39; 40; 64; 65] and pruning [38; 43; 48; 51; 82] are mainstream approaches. Quantization methods propose to reduce the bits of each parameter by converting floating-point (FP32 / FP16) parameters to integers (INTS / INT4) or other discrete forms. This line of methods preserves the structural characteristics of the network and compresses models in exchange for lowering accuracy. Distillation methods [24; 39; 40] attempt to transfer knowledge from LLMs to a lightweight student model. However, such methods usually require retraining models in an end-to-end manner and may take up large computing resources. Model pruning methods propose to reduce the complexity of models by removing redundant parameters. These methods have two forms, static pruning [3; 18; 23] and dynamic pruning [2; 12; 21; 49; 81]. Static pruning methods usually remove redundant layers based on various relevant metrics. For example, Shortened-LLaMA  measures the importance of layers and prunes the unnecessary ones. However, with the increase of pruned parameters, static pruning poses inevitable performance degradation. Dynamic pruning methods tend to prune unimportance layers based on inputs. For example, Ada-Infer  introduces an early-exit mechanism to stop the inference at intermediate layers. Mixture-of-Depths  measures the importance of each token and only calculates the top-k tokens at different layers.

In essence, the proposed D-LLM belongs to dynamic pruning methods, which dynamically decide which layers should be executed for each input token.

**Dynamic Inference in Computer Vision.** Dynamic inference is a promising technique to skip layers at inference time to achieve acceleration [70; 71; 72; 73], which was initially proposed in computer visions. Research on dynamic inference mechanisms has two different approaches. The first one is layer skipping. This line of methods mainly focus on how to design a specific dynamic decision module or metric to reduce computational cost. Specifically, ConvNet-AIG  uses a convolutional module to define the inference graph conditioned on inputs. It proposes a router to make the execution decision for each model layer. SkipNet  utilizes LSTM as the decision module to determine whether a layer should be executed or not. Besides, CoDiNet  attempts to model the relationship between the inputs and their executing decisions to enhance the optimization. The second line of methods is early prediction. While a dynamic inference network has only one exit, an early prediction network is characterized by multiple exits. In early prediction networks, the execution would stop once a criterion for a given sample is satisfied. MSDN  is a typical example, which introduces multiple early-exits, according to the allowed time budget. Instead of bypassing all units, DCP  generated decisions to reduce the computational cost on channels.

Inspired by dynamic inference in the CV task, we rethink the characteristics of NLP and raise the following question. _Should models allocate the same computing resource on tasks of different difficulties or tokens of different importance?_ Motivated by this, we introduce the D-LLMs framework, which will dynamically allocate fewer computing resources for unimportant tokens.

## 3 Methods

### Preliminary: LLMs Architecture

As a natural language processing task, a sequence of input words is first embedded into tokens \(\{x^{1},x^{2},,x^{N}\}\) by a tokenizer. Each token would then go through a pre-defined large language model sequentially in an autoregressive manner. That is to say the target of \(\{x^{1},x^{2},,x^{n}\}\) is \(x^{n+1}\). In this way, the inference paradigm of a large language model is formulated as

\[x^{n+1}=f_{} f_{L} f_{L-1} f_{1} f _{}(x^{1},x^{2},,x^{n}),\] (1)

where \(L\) is the total number of transformer layers, \(f_{}\) is the word embedding and \(f_{}\) is the word classifier. For convenience, we use the superscript to represent the token index and the subscript to represent the layer index of token embeddings in the following sections.

**Transformer Layer.** Dominant architectures of LLMs are decoder-only transformer networks, which consist of \(L\) transformer layers. Each transformer layer contains a multi-head self-attention (MHA) and a feed-forward network (FFN). The inference of the \(l\)-th transformer layer can be defined as,

\[h_{l}^{n}=(x_{l}^{n})+x_{l}^{n},\] (2) \[x_{l+1}^{n}=(h_{l}^{n})+h_{l}^{n},\] (3)

where \(x_{l}^{n}\) is the \(n\)-th input token embedding of the \(l\)-th transformer layer and \(h_{l}^{n}\) is the output of MHA. Specifically, a MHA divides token embeddings into multiple heads and perform self-attention operations in parallel.

**Self Attention.** Since large language models mainly process inputs sequentially, the calculation of former tokens should not be affected by latter ones. To tackle this problem, the self-attention module (SA) uses a causal attention mask \(\) on attention scores as

\[_{i,j}=0,&j i,\\ -,&j>i.\] (4)

With the causal attention mask \(\), the features of the \(n\)-th token are processed without subsequent tokens. Under such framework, the formulation of \(()\) is defined as

\[(x_{l}^{n}):=((x_{l,1}^{n}),(x_{l,2}^ {n}),,(x_{l,h}^{n})) W_{O},\] (5)

where \(W_{O}\) is a learnable parameter and \(\) is matrix multiplication. \(()\) is essentially concatenation of self-attention operated features divided into \(h\) heads \(\{x_{l,i}^{n}\}_{i=1}^{h}\). Based on learnable weights \(W_{Q},W_{K},W_{V}\), a self attention is formulated as

\[(x_{l,i}^{n}):=(^{n} W_{Q}(X  W_{K})^{T}}{}+) X W_{V},\] (6)

where \(X=\{x_{l,i}^{1},x_{l,i}^{2},,x_{l,i}^{N}\}\) is input features of the sentence, \(d\) is the dimension of each head and \(()^{T}\) is transpose. In addition, FFN consists of several linear layers with activation functions following the MHA.

**KV-Cache.** The KV-Cache method  has been a standard acceleration technique for large language model inference. As shown in Eq. 6, the query embedding of a given token only calculates the self-attention with previous keys and values. Since decoder-only LLMs process input tokens in an autoregressive way, the key and value embeddings can be reused to boost inference speed by storing calculated key and value embeddings. The KV-cache method reduces the computation complexity from \(O(n^{2})\) to \(O(n)\), but brings extra memory overheads to store the KV-cache when generating long texts [13; 21; 29; 75].

### D-LLMs: Dynamic Inference LLMs

In this section, we illustrate the proposed dynamic inference mechanism in large language models. We assign a dynamic decision module before each single transformer layer. At the inference time, the dynamic decision module makes an execution decision whether the related transformer layer should be executed or not as shown in Fig. 0(a).

Given a token \(x\) in an input sequence, it would go through a dynamic decision module \(g_{l}\), prior to the \(l\)-th transformer layer \(f_{l}\) in large language models. As shown in Fig. 0(b), a dynamic decision module is composed of two linear layers separated by an activation function. At the inference time, the dynamic decision module outputs \(g_{l}(x_{l})\), the parameters of a categorical distribution that represent the probability of skipping and executing based on the input feature \(x_{l}\) to the \(l\)-th transformer layer. The execution decision for the \(l\)-th layer is defined as

\[b_{l}:=((g_{l}(x_{l}))),\] (7)

where \(\) is one-hot operation and \(b_{l}\) is a two-dimensional vector. This way, the result of the execution decision tuple for each token is either \(\) or \(\). However, \(\) operation deterministically outputs the max argument without exploration and is not differentiable during training. To solve this problem, we utilize the Gumbel-Softmax reparametrization technique  and straight-through estimator  to ensure the transformer networks can be optimized in an end-to-end fashion during training. Thus, the execution decisions are organized as hard forms when forward computing:

\[_{l}:=(((g_{l}(x_{l}))+)), G(0,1),\] (8)where \(G(0,1)\)is a Gumbel distribution. With sampled noise \(\), the decisions \(_{l}\) satisfy the categorical distribution \(g_{l}(x_{l})\). The decisions are calculated in soft forms when back-propagating:

\[_{l,i}=(x_{l}))_{i}+_{i})/)}{_{i}( ((g_{l}(x_{l}))_{i}+_{i})/)},i\{0,1\}.\] (9)

The temperature \(\) controls the sharpness of softmax. As \( 0\), the softmax result converges to the discrete form from the categorical distribution. Finally, the output of layer \(l\) is defined as

\[x_{l+1}=_{l,0} x_{l}+_{l,1} f_{l}(x_{l}),\] (10)

where \(_{l,0},_{l,1}\{0,1\}\) are the decisions of skipping or executing the \(l\)-th transformer layer.

### Customizable Acceleration Rate

According to Eq. 10, when \(_{l,1}\) is zero, \(f_{l}(x_{l})\) would not be involved in calculation. Thus, it achieves reducing the computational cost of \(f_{l}\). The average acceleration rate \(^{n}\) for the token \(x^{n}\) can be defined as

\[^{n}=1-_{l=1}^{L}(_{l,1}^{n}),\] (11)

where \(L\) is the total number of layers, and \(_{l,1}\) is the decision of executing the \(n\)-th token at the \(l\)-th layer. To make the acceleration rate adaptive to platforms of different computing capabilities, we design an acceleration ratio loss to customize the acceleration rate as

\[_{}=_{n=1}^{N}(|^{n}-|_ {1}),\] (12)

where \(||_{1}\) is \(l_{1}\)-norm and the \(\) is the user-defined target acceleration rate.

Based on the aforementioned method, we achieve the optimization and inference for D-LLMs in end-to-end manners. Note that in addition to training the LLM model from scratch and obtaining generalized D-LLMs, our method can easily work on pre-trained LLMs by employing _LoRA_.

Figure 1: The framework the proposed D-LLMs. The inference paradigm of dynamic decisions for transformer layers is shown in Fig. 0(a). The design of dynamic execution decision modules is shown in Fig. 0(b). The mask in multi-head self-attention with eviction strategy is shown in Fig. 0(c).

### KV-Cache Eviction Strategy

In this section, we elaborate how our proposed dynamic inference mechanism can cooperate with the KV-cache strategy. Considering the inference of the \(n\)-th token of the \(l\)-th transformer layer in eq. (6), all previous tokens are involved in the calculation of \(x_{l}^{n}\). It means that even the execution decision \(_{l,1}^{i},i\{1,,n\}\) on token embedding \(x_{l}^{i}\) is zero, we still need to calculate its keys and values for self-attention calculation of \(x_{l}^{n}\). We conduct an aggressive strategy of self-attention, which bypasses the features of skipped tokens. We propose a KV-cache eviction policy by designing a mask on the attention matrix to ignore the uncalculated features. Thus, the corresponding KV-cache is unnecessary to keep, which reduces storage overhead during inference.

As shown in Fig. 0(c), we design an eviction attention mask E at each layer together with causal mask in Eq. 6, which supports training in batch. In addition, recent research on attention maps and KV-cache [26; 75] shows that tokens at the beginning of a sentence are crucial to subsequent tokens. We also find that evicting the KV-cache of initial tokens at skipped layers would either leads to performance decreasing or causes unstable optimization. Therefore, we preserve the keys and values of the first \(m\) tokens at the beginning of texts. Hence, the eviction attention mask for the \(l\)-th layer is encoded as

\[_{i,j}=0,& j mb_{l,1}^{j}=1\\ -,&,\] (13)

where \(b_{l,1}^{j}=1\) represents the \(j\)-th token executing the \(l\)-th transformer layer, and \(m\) is the number of preserved tokens at beginning. Thus, the KV-cache of \(m\) initial tokens are kept in the prefilling phase. In our method, we simply set the decisions of initial tokens to be executed in a deterministic way, which demonstrates equivalent performance and is easier to implement in practice.

Finally, we introduce the overall loss functions in D-LLMs. The objective function is composed of two parts: cross-entropy loss and acceleration ratio loss.

\[=_{}+_{},\] (14)

where \(_{}\) is the cross-entropy loss and \(\) is a hyper-parameter of the acceleration ratio loss \(_{}\).

## 4 Experiments

### Experimental Setup

**Models and Benchmarks.** We conduct experiments of the proposed D-LLMs on widely used LLMs, LLaMA2-7B and LLaMA3-8B. LLaMA2-7B and LLaMA3-8B both contain 32 transformer layers. We perform few-shot finetuning on nine different benchmarks to evaluate the effectiveness. The benchmarks are divided into three different tasks. The first task is Q&A and summarization, including Alpaca  and SAMSum , and perplexity (PPL) is used to measure their performance.

Figure 2: The performance against computational cost of D-LLMs on three datasets. The figures show that reducing around 40% to 60% computational cost achieves the best trade-off.

Second, GSM8K , MaWPS  are about math problem solving. The answers are marked as correct only if the numerical difference is less than \(10^{-5}\). Finally, BoolQ , PIQA , SIQA , OBQA , MMLU  are datasets about common sense reasoning. The performance on these benchmarks is evaluated by accuracy.

**Implementation Details.** To avoid finetuning the entire parameters in D-LLMs, we adopt the LoRA finetuning method as a baseline. We apply dynamic decision modules to transformer layers except the first two for training stability. The maximal context length of tokens is set to 1024. In the designing of the dynamic decision module, we use two linear layers with a hidden dimension of 512. For the KV-cache eviction policy, we reserve the first two tokens, _i.e._, \(m=2\). Finally, \(\) in the loss function is set to 5 for Q&A, summarization task, 1.0 for math problems tasks, and 0.1 for commonsense reasoning tasks. We finetune 20 epochs for GSM8K and 10 epochs for other datasets.

### Performance Comparison

**Inference Acceleration.** We show the acceleration of our proposed D-LLMs. As defined in Eq. 11, D-LLMs can customize the acceleration rate by adjusting the target rate \(\). In Fig. 2, we show the performance under different FLOPs on three datasets. The performance on MaWPS and OBQA is evaluated by accuracy, while that on SAMSum is evaluated by PPL. With the increasing FLOPs on MaWPS and OBQA, the accuracy shows improvement. When only using 40% FLOPs and 30% FLOPs on MaWPS and OBQA, D-LLM exceeds the baseline with 100% FLOPs. In terms of SAM-Sum, which is a Q&A task, the performance gets better first and then gets worse. The reason that performance gets worse when the cost is over 60%, might be due to overfitting.

**Comparison with state-of-the-art.** We compare the proposed D-LLMs with state-of-the-art methods on nine benchmarks. These benchmarks are concluded in three tasks, _i.e._, Q&A with summarization, Math Solving, and Common Sense Reasoning. The performance of D-LLMs exceed MoD , Shortened-LLaMA , and Ada-Infer  in a large margin. In terms of computational cost, we denoted the FLOPs of LLaMA2-7B with LoRA as 1.00, the cost of others are percentages based on it as shown in Tab. 1. It is worth noting that FLOPs are calculated on average cost over tokens. Specifically, with the same baseline, _i.e._, LLaMA2-7B with LoRA finetuning, D-LLMs achieve better performance with only using about 50% computational cost. Shortened-LLaMA reports two different results in their paper, which utilize Taylor and PPL as metrics, respectively. D-LLMs surpass Shortened-LLaMA on all datasets, though adopting fewer computing resources. Besides, we also reproduce the MoD and Ada-Infer and show that D-LLMs perform better as well.

   Dataset &  &  &  &  &  \\  Q\&A & PPL(\(\)) & FLOPs(\(\)) & PPL(\(\)) & FLOPs(\(\)) & PPL(\(\)) & FLOPs(\(\)) & PPL(\(\)) & FLOPs(\(\)) & PPL(\(\)) & FLOPs(\(\)) \\  Alpaca & 10.32 & 0.56 & 7.09 & 0.66 & 7.65 & 0.66 & 319 & 0.65 & 6.01 & 0.59 \\ SAMSum & 4.47 & 0.56 & 4.39 & 0.66 & 4.66 & 0.66 & 874 & 0.56 & 3.18 & 0.55 \\  Math & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) \\  GSM8K & 0.08 & 0.56 & 0.10 & 0.66 & 0.18 & 0.66 & 0.00 & 0.83 & 0.29 & 0.59 \\ MaWPS & 0.33 & 0.56 & 0.52 & 0.66 & 0.39 & 0.66 & 0.00 & 0.90 & 0.74 & 0.56 \\  Com. Sen. & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) \\  BoolQ & 0.64 & 0.56 & 0.67 & 0.66 & 0.73 & 0.66 & 0.71 & 0.61 & 0.73 & 0.52 \\ PIQA & 0.49 & 0.56 & 0.76 & 0.66 & 0.83 & 0.66 & 0.55 & 0.63 & 0.84 & 0.52 \\ SIQA & 0.58 & 0.56 & 0.75 & 0.66 & 0.81 & 0.66 & 0.80 & 0.64 & 0.82 & 0.54 \\ OBQA & 0.42 & 0.56 & 0.63 & 0.66 & 0.81 & 0.66 & 0.78 & 0.76 & 0.80 & 0.53 \\ MMLU & 0.28 & 0.56 & 0.47 & 0.66 & 0.53 & 0.66 & 0.41 & 0.60 & 0.53 & 0.55 \\   

Table 1: Performance Comparison on different tasks under Few-shot Settings based on LLaMA2-7B. _Sh. Lla. PPL_ refers to Shortened-LLaMA applying PPL metric. _Sh. Lla. Tay._ refers to Shortened-LLaMA applying Taylor metric. _Ada-Inf._ refers to Ada-Infer. For convenience, we mark the best performance in red and the lowest computational cost in blue.

### Quantitative Analysis

**Ablation Study.** We conduct ablation studies of our proposed method based on LLaMA2-7B and LLaMA3-8B and then perform the results in three different tasks including Q&A and Summary, Math Problem, and Common Sense Reasoning. As shown in Tab. 2, we illustrate the performance of dynamic inference large language models over the baseline. Based on LLaMA3-8B, with less than 55% computational cost, our proposed D-LLM achieves better performance than that of the baseline on five datasets. In comparison, obtaining a comparable performance, D-LLMs on LLaMA2-7B only take about 55% computational cost. On Q&A and Summary tasks, D-LLMs sacrifice some accuracy for lower computational cost. The performance of baseline methods, _i.e._, LLaMA with LoRA finetuning, outperforms D-LLMs on Alpaca using 40% extra cost. Considering math-solving problems, D-LLMs spend more computing resources than common sense reasoning tasks, which also demonstrates that math-solving problems are more difficult.

**KV-Cache Overhead.** In this part, we discuss the KV-cache storage benefits of D-LLMs. With the KV-cache eviction strategy, some keys and values are not necessary, because these features are useless to subsequent tokens. So the memory overhead on KV-cache is reduced with the layers are skipped. The benefits of reducing KV-cache storage are actually the skipped ratios of transformer layers in D-LLMs. As a result, we achieve almost 45% storage overhead reduction than the baselines on both LLaMA3-8B and LLaMA2-7B.

**Reserved Tokens (\(m\)).** We conduct experiments to analyze the number \(m\) of initial tokens reserved in our proposed eviction policy. For a fair comparison, we set the computational cost of all experiments at the same level with \(=0.5\). We compare reserved tokens of 0, 1, 2, 4, and 8 on SAMSum and SIQA datasets. As shown in Tab. 3, using about 55% computational cost of entire LLMs, the perplexity decreases at first and then increases. The result demonstrates that keeping the first two

   Datasets & Alpaca & SAMSum & GSM8K & MaWPS & BoolQ & PIQA & SIQA & OBQA & MMLU \\   & \(()\\ ()\) & \(()\\ ()\) & \(()\\ ()\) & \(()\\ ()\) & \(()\\ ()\) & \(()\\ ()\) & \(()\\ ()\) & \(()\\ ()\) & \(()\\ ()\) & \(()\\ ()\) \\  _Backbone_ &  &  \\  _LoRA_ & 4.69 & 3.61 & 0.27 & 0.72 & 0.73 & 0.84 & 0.81 & 0.79 & 0.54 \\ _finetune_ & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\  _D-LLM w/o._ & 6.08 & 3.63 & 0.29 & 0.72 & 0.72 & 0.83 & 0.81 & 0.82 & 0.53 \\ _Evi. Str._ & 0.64 & 0.62 & 0.60 & 0.61 & 0.60 & 0.59 & 0.60 & 0.59 & 0.60 \\  _D-LLM_ & 6.01 & 3.18 & 0.29 & 0.74 & 0.73 & 0.84 & 0.82 & 0.80 & 0.53 \\ _D-LLM_ & 0.59 & 0.55 & 0.59 & 0.56 & 0.52 & 0.52 & 0.54 & 0.53 & 0.55 \\  _Backbone_ &  &  \\  _LoRA_ & 4.84 & 3.93 & 0.52 & 0.88 & 0.75 & 0.87 & 0.81 & 0.85 & 0.56 \\ _finetune_ & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\  _D-LLM_ & 8.43 & 3.63 & 0.50 & 0.91 & 0.75 & 0.88 & 0.81 & 0.82 & 0.57 \\ _D-LLM_ & 0.59 & 0.58 & 0.60 & 0.55 & 0.51 & 0.52 & 0.55 & 0.53 & 0.55 \\   

Table 2: Ablation on different tasks under Few-shot Settings based on LLaMA2-7B and LLaMA3-8B. _LoRA_ refers to using LLaMA3-8B and LLaMA2-7B as pretrained model and finetuning with LoRA. _D-LLM w/o Evi. Str._ refers to the proposed method without eviction strategy.

   Res. Tok. &  &  &  &  &  \\  Metrics & \(()\) & FLOPs(\()\) & \(()\) & FLOPs(\()\) & \(()\) & FLOPs(\()\) & \(()\) & FLOPs(\()\) & \(()\) & FLOPs(\()\) \\  SAMSum & 3.33 & 0.55 & 3.32 & 0.55 & 3.18 & 0.55 & 3.29 & 0.55 & 3.35 & 0.55 \\  Metrics & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) & Acc.(\(\)) & FLOPs(\(\)) \\  SIQA & 0.80 & 0.53 & 0.81 & 0.55 & 0.82 & 0.53 & 0.81 & 0.55 & 0.80 & 0.55 \\   

Table 3: The parameter analysis on numbers of reserved tokens not participate in dynamic inference.

tokens is the best setting. Similarly, it shows the same tendency on SIQA, and the inflection point appears at the same time. As a result, we keep the first two tokens without skipping over layers.

### Qualitative Analysis.

**Execution Layers of Grammatical Terms.** Here, we present a visualization of the execution ratios of different grammatical terms as shown in Fig. 3. From this figure, we observe that executing ratios of number and math symbols are much higher than others on layer #4 and layer #26. Meanwhile, layer #21 and layer #23 are utilized more by modal verbs and subject terms respectively. All terms show high execution ratios on the layer #18. Since different grammatical terms usually play the same role in sentences, thus, their execution ratios are similar. More importantly, different layers in D-LLMs are of different abilities, which are used by different grammatical terms.

**Execution Decisions of Similar Tasks.** We count up and visualize the executing ratios of different layers to illustrate the difference on Benchmarks and Questions. As shown in Fig. 3(a), the first few layers are utilized by most Benchmarks. Q&A and summarization tasks and math solving show larger diversity than common sense reasoning. Common sense reasoning tasks utilize more shallow layers and show smaller variances. In addition, we visualize several questions about high school history, science, and engineering knowledge from MMLU. As shown in Fig. 3(b), the execution ratios of historical knowledge are similar, while that of science and engineering knowledge are similar. In conclusion, the execution behavior of utilizing similar knowledge tends to adopt similar ratios in D-LLM.

## 5 Conclusion

In this paper, we take the inference of large language models from a new perspective, that tasks of different difficulties should not require the same amount of computing resources and not every word is equally important in a sentence. Motivated by this, we propose a novel dynamic inference framework for LLMs, called D-LLMs, which achieves adaptive computing resource allocation for different tokens. We first propose a dynamic decision module to decide whether a layer is necessary

Figure 4: The execution ratios of different layers visualizations on Benchmarks and Questions. Fig. 3(a) shows the execution ratios of different benchmarks on respective layers. A deeper color refers to a higher execution ratios. Fig. 3(b) shows six standard questions’ execution ratios over layers from MMLU. The Y-axis is the execution ratios and X-axis is the layer index.

Figure 3: Execution ratios on different layers of three grammatical terms. Blue dots refer to number and math symbols, _e.g._, ‘1, +, \(\)’. Red dots refer to subject terms, _e.g._, ‘She, He, They’. Green dots refer to modal verbs, _e.g._, ‘may, should, might’. The distance of a dot to the center represents executing ratio. The red circle is probability of 100% and the blue circle is the average ratios.

for the given token. Then, we propose a KV-cache eviction strategy to enable D-LLMs' deployment in real-world applications. Finally, we conduct experiments on nine benchmarks and show D-LLMs performing comparable accuracy with only about 50% computational cost over baselines. More importantly, the D-LLMs can be deployed on almost all LLMs with negligible training cost.

**Limitation.** Our proposed D-LLMs make the first attempt to adaptively allocate computing resources at inference time for LLMs. Although experiments show surprising performance with lower cost, there are still some interesting issues worthy of future research. First, the generalizability of D-LLMs for broader tasks still needs verification. Second, it is worth exploring the dynamic mechanism on different granularities, _e.g._, sentences and tasks.