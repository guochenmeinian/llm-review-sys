# DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given Sparse Views

Paul Yoo &Jiaxian Guo &Yutaka Matsuo &Shixiang Shane Gu

The University of Tokyo

{paulyoo, jiaxian.guo}@weblab.t.u-tokyo.ac.jp

Correspondence to Jiaxian Guo <jiaxian.guo@weblab.t.u-tokyo.ac.jp>

###### Abstract

Synthesizing novel view images from a few views is a challenging but practical problem. Existing methods often struggle with producing high-quality results or necessitate per-object optimization in such few-view settings due to the insufficient information provided. In this work, we explore leveraging the strong 2D priors in pre-trained diffusion models for synthesizing novel view images. 2D diffusion models, nevertheless, lack 3D awareness, leading to distorted image synthesis and compromising the identity. To address these problems, we propose _DreamSparse_, a framework that enables the frozen pre-trained diffusion model to generate geometry and identity-consistent novel view images. Specifically, DreamSparse incorporates a geometry module designed to capture features about spatial information from sparse views as a 3D prior. Subsequently, a spatial guidance model is introduced to convert rendered feature maps as spatial information for the generative process. This information is then used to guide the pre-trained diffusion model to encourage the synthesis of geometrically consistent images without further tuning. Leveraging the strong image priors in the pre-trained diffusion models, DreamSparse is capable of synthesizing high-quality novel views for both object and object-centric scene-level images and generalising to open-set images. Experimental results demonstrate that our framework can effectively synthesize novel view images from sparse views and outperforms baselines in both trained and open-set category images. More results can be found on our project page: https://sites.google.com/view/dreamsparse-webpage.

Figure 1: Qualitative results on novel view synthesis of real-world objects from the CO3D dataset.

Introduction

"How could they see anything but the shadows if they were never allowed to move their heads?"

- Plato's Allegory of the Cave

Plato's Allegory of the Cave raises a thought-provoking question about our perception of reality. Human perception of 3D objects is often limited to the projection of the world as 2D observations. We rely on our prior experiences and imagination abilities to infer the unseen views of objects from these 2D observations. As such, perception is to some degree a creative process retrieving from imagination. Recently, Neural Radiance Fields (NeRF)  exhibited impressive results on novel view synthesis by utilizing implicit functions to represent volumetric density and color data. However, NeRF requires a large amount of images from different camera poses and additional optimizations to model the underlying 3D structure and synthesize an object from a novel view, limiting its use in real-world applications such as AR/VR and autonomous driving. In most practical applications, typically only a few views are available for each object, in which case leads NeRF to output degenerate solutions with distorted geometry .

Recent works  started to explore sparse-view novel view synthesis, specifically focusing on generating novel views from a limited number of input images (typically 2-3) with known camera poses. Some of them  introduce additional priors into NeRF, _e.g._ depth information, to enhance the understanding of 3D structures in sparse-view scenarios. However, due to the limited information available in few-view settings, these methods struggle to generate clear novel images for unobserved regions. To address this issue, SparseFusion  and GeNVS propose learning a diffusion model as an image synthesizer for inferring high-quality novel-view images and leveraging prior information from other images within the same category. Nevertheless, since the diffusion model is only trained within a single category, it faces difficulties in generating objects in unseen categories and needs further distillation for each object, rendering it still impractical.

In this paper, we investigate the utilization of 2D image priors from pre-trained diffusion models, such as Stable Diffusion , for generalizable novel view synthesis **without** further per-object training based on sparse views. However, since pre-trained diffusion models are not designed for 3D structures, directly applying them can result in geometrically and textually inconsistent images, compromising the object's identity in Figure 6. To address this issue, we introduce _DreamSparse_, a framework designed to leverage the 2D image prior from pre-trained diffusion models for novel view image synthesis using a few (2) views. In order to inject 3D information into the pre-trained diffusion model and enable it to synthesize images with consistent geometry and texture, we initially employ a geometry module  as a 3D geometry prior inspired by previous geometry-based works , which is capable of aggregating feature maps across multi-view context images and learning to infer the spatial features, _i.e._ features with spatial information, for the novel view image synthesis. This 3D prior allows us to render an estimate from a previously unseen viewpoint while maintaining accurate geometry.

However, due to the modality gap, the extracted spatial features cannot be directly used as the input to the pre-trained diffusion model for synthesizing geometrically consistent novel view images. Alternatively, we propose a spatial guidance module which is able to convert rendered feature maps into meaningful guidance to change the spatial features  in the pre-trained diffusion model, thus promoting the pre-trained diffusion model to generate geometry-consistent novel view image without altering its parameters. Nevertheless, the spatial guidance from feature maps alone cannot completely overcome the hallucination problem of the pre-trained models, as the spatial information encoded in spatial features is limited. This means it cannot guarantee identity consistency in synthesized novel view images. To overcome the limitation, we further propose a noise perturbation method, where we denoise a blurry target view color estimate modulated with noise instead of a randomly initialized noise with the pre-trained diffusion model, so that we can further utilize the coarse shape and appearance information present in 3D geometry model's estimate. In this way, the frozen pre-trained diffusion model is able to effectively synthesize high-quality novel view images with improved consistency in both geometry and identity.

With the strong image synthesis capacity of the frozen pre-trained diffusion model, our approach offers several benefits: **1.** The ability to infer unseen regions of objects without additional training, as pre-trained diffusion models already possess strong image priors learned from large-scale image-text

[MISSING_PAGE_FAIL:3]

, have utilized image diffusion models. On the other hand, we propose a framework to guide a foundational 2D diffusion model while keeping it frozen and simultaneously enforce 3D structure through a geometry-based module.

Diffusion Model for Novel View SynthesizeIn order to achieve high-quality novel view synthesis, recent works [16; 56; 41; 45; 26; 2; 71; 77; 7; 21; 22; 28; 68; 36; 32; 30; 8] introduce diffusion models conditioned on text or images to generate or reconstruct 3D representations. In the context of novel view synthesis, 3DiM performs view synthesis only conditioned on input images and poses without 3D geometry, leaving vulnerabilities in generating 3D-consistent images. SparseFusion  and GeNVS  proposed to integrate additional geometric structure as training conditions for the diffusion model, thereby enabling the generation of 3D-consistent images. However, due to the absence of strong 2D prior in the diffusion models they employ, these approaches are challenging to generalize to objects in open-set categories. In contrast, our approach utilizes a frozen diffusion model pre-trained on a large-scale dataset , enhancing its generalization ability for objects in open-set categories.

## 3 Method

Given a few context images \(\{C_{i}^{inputs}\}_{i=1}^{N}\) and their poses \(_{i}\), we aim to leverage the 2D image prior from pre-trained diffusion models to synthesize a novel view image at a target pose \(_{target}\). Because pre-trained diffusion models are not 3D-aware, we first employ a geometry-aware module as a 3D prior to extract features in a 3D volume encoded from given context images. In order to leverage feature map renderings for the pre-trained diffusion model, we further propose a spatial guidance model to convert geometry-grounded features into spatial features [63; 3] with consistent shape in diffusion models, guiding it to synthesize novel view images with correct geometry. However, we discover that relying solely on the diffusion model to sample an accurate reconstruction from random noise is inadequate at maintaining the object's identity due to the hallucination problem [19; 36] as shown in 6. Therefore, we propose a noise perturbation method to alleviate it, guiding the diffusion model to synthesize a novel view image with correct geometry and identity. The overall pipeline is illustrated in Fig. 2.

### 3D Geometry Module

In order to infuse 3D awareness into 2D diffusion models, we propose a geometry module to extract features with geometric information for 2D diffusion models. In order to obtain geometry grounding, the process begins by casting a query ray from \(^{target}\) and sampling uniformly spaced points along the ray. For each 3D point, we aim to learn density weighting for computing a weighted linear combination of features along the query ray. Subsequently, this per-ray feature is aggregated across multiple context images, yielding a unified perspective on the 3D structure we aim to reconstruct. Lastly, we render a feature map at \(^{target}\) by raycasting from the target view. Next, we will present the details of this model.

Figure 2: The illustration of the method. The first stage involves utilizing a 3D geometry module to estimate 3D structure and aggregate features from context views. In the next stage, a pre-trained 2D diffusion model conditioned on the aggregate features is leveraged to learn a spatial guidance model that guides the diffusion process for accurate synthesis of the underlying object.

**Point-wise density weighting for each context image.** For each input context image \(C_{i}^{inputs}\), our geometry module first extracts semantic features using a ResNet50  backbone and then reshapes the encoded feature into a 4 dimensional volumetric representation \(V_{i}^{c d h w}\), where \(h\) and \(w\) are the height and width of the feature volume, respectively, \(d\) is the depth resolution, and \(c\) is the feature dimension. We pixel-align the spatial dimensions of the volume to that of the original input image via bilinear upsampling. To derive benefit from multi-scale feature representation, we draw feature maps from the first three blocks of the backbone and reshape them into volumetric representations capturing the same underlying 3D space. Given a 3D query point \(_{j}\) along a query ray \(^{i}\), we sample feature vectors from all three scales of feature volumes using trilinear interpolation concatenating them together. To calculate the point-wise density weighting, we employ a transformer  with a linear projection layer at last followed by a softmax operation to determine a weighted linear combination of point features, resulting in a per-ray feature vector. Limited by the page limit, we leave further implementation details in the Appendix A.

**Aggregate features from different context images.** To understand the unified structure of the 3D object, we consolidate information from all given context images. More specifically, we employ an extra transformer, enabling us to dynamically consolidate ray features from a varying number of context images that correlate with each query ray. The final feature map rendering at a query view is constructed by rvacasting from the query view and computing per-ray feature vector for each ray. We render the feature map \(\) at a resolution of \(^{64 64}\), compositing features sampled from a 3D volume with geometry awareness with respect to the target view. We denote \(g\) as the feature map rendering function and \(\) as the resulting aggregate feature map.

\[=g_{}(^{inputs},^{inputs},^{target})\] (1)

where \(^{d 64 64}\) with \(d=256\), and \(\) is trainable parameters.

**Color Estimation** To enforce geometric consistency, we directly obtain aggregation weights from the transformer outputs and linearly combine RGB color values drawn from the context images to render a coarse color estimate \(E\) at the query view.

\[E=g_{,color}(^{inputs},^{inputs},^{target})\] (2)

We impose a color reconstruction loss on the coarse image against the ground-truth image.

\[_{recom}=_{^{target}}\|g_{,color}(^{ inputs},^{inputs},^{target})-C^{target}\|^{2}\] (3)

### Spatial Guidance Module

Because of the modality gap between the feature map rendering \(\) and the input of the pre-trained diffusion model, feature maps \(\) cannot be directly used as the input of the pre-trained diffusion model. To leverage the 3D information in the feature maps, we propose the spatial guidance module to convert the feature maps into guidance to rectify the spatial features [63; 74; 3] that have a role in forming fine-grained spatial information in the diffusion process (normally the feature maps after the 4-th layer). To derive this guidance from spatial information in the features \(\), we construct our spatial guidance module following ControlNet  which trains a separate copy of all the encoder blocks as well as the middle block from Stable Diffusion's U-Net with 1x1 convolution layers initialized with zeros between each block. Let \(T_{}\) be the spatial guidance module, and intermediate outputs from each block \(j\) of \(T_{}\) as \(T_{,j}()\) with weight \(\). In order to change the spatial features in the pre-trained diffusion model, we directly add \(T_{,j}()\) into the corresponding decoder block of the pre-trained diffusion model's U-Net. By optimizing \(T_{}\) with gradients backpropagated from the pre-trained diffusion model's noise prediction objective.

\[_{diffusion}=_{x_{0},t,,(0, 1)}[\|-_{}(x_{t+1},t,T_{}())\| ^{2}]\] (4)

\(T_{}\) will be optimized to learn how to convert the feature map from the geometry module into semantically meaningful guidance to rectify spatial features in the diffusion process, enabling it to generate geometry-consistent images. In Section 4.5, we visualize the spatial features after adding the spatial guidance to show the effects of the spatial guidance model. During training, we jointly optimize \(g_{}\) and \(T_{}\) using the overall loss.

\[_{,}_{recom}(g_{})+_{diffusion}(T_{ })\] (5)While in training time, we use a ground-truth image as \(x_{0}\) to optimize \(_{diffusion}\), in inference time, we initialize \(x_{0}\) with an image rendered from \(g_{,color}\).

Noise PerturbationWhile spatial guidance module by itself is able to guide the pre-trained diffusion model to synthesize novel view images with consistent geometry. It still cannot always synthesize images with the same identity as context views because of the hallucinate problem [19; 36] in the pre-trained models. To alleviate this problem, we propose adding noise perturbation to the novel view estimate \(E\) from the geometry module and denoising the result with the pre-trained diffusion model, _e.g._ Stable Diffusion , so that it can leverage the identity information from the estimate. As shown by , applying the denoising procedure can project the sample to a manifold of natural images. We use the formulations from denoising diffusion models  to perturb an initial image \(x_{0}=E\) with Gaussian noise to get a noisy image \(x_{t}\) as follows:

\[x_{t}=_{t}}x_{0}+_{t}}\] (6)

where \(_{t}\) depends on scheduling hyperparameters and \((0,\,1)\). During the training time, the noise is still randomly initialized, and we use the Noise Perturbation method in the inference time to improve the identity consistency. We show its ablation study in Section 4.5.

## 4 Experiments

In this section, we first validate the efficacy of our DreamSparse framework on zero-shot novel view synthesis by comparing it with other baselines. Then, we perform ablation studies on important design choices, such as noise perturbation and visualization of spatial features, to understand their effects. We also present qualitative examples of our textual control ability and include a discussion on observations.

### Dataset and Training Details

Following SparseFusion , we perform experiments on real-world scenes from the Common Objects in 3D (CO3Dv2) , a dataset with real-world objects annotated with camera poses. We train and evaluate our framework on the CO3Dv2  dataset's fewview_train and fewview_dev sequence sets respectively. We use Stable Diffusion v1.5  as the frozen pre-trained diffusion model and DDIM  to synthesize novel views with 20 denoising steps. The resolutions of the feature map for the spatial guidance module and latent noise are set as 64 \(\) 64 with spatial guidance weight \(=2\). The three transformers used in the geometry module all contain 4 layers, and the output feature map is rendered at a resolution of 64 \(\) 64 to match the latent noise dimensions. We jointly train the geometry and the spatial modules on 8 A100-40GB GPUs for 3 days with a batch size of 15. To demonstrate our framework's generalization capability at object-level novel view synthesis, we trained our framework on a subset of 10 categories as specified in . During each training iteration, a query view and one to four context views of an object were randomly sampled as inputs to the pipeline. To further evaluate scene-level novel view synthesis capability, we trained our framework on the hydrant category, incorporating the full background, using the same training methodology as above.

### Competing Methods

We compare against previous state-of-the-art (SoTA) methods for which open-source code is available. We have included PixelNeRF , a feature re-projection method, in our comparison. Additionally, we compare our methods against SparseFusion , the most recently published SoTA method that utilizes a diffusion model for NVS. We train our framework and SparseFusion on 10 categories of training sets. The PixelNeRF training was conducted per category due to its category-specific hyperparameters. For a fair comparison, all methods perform NVS **without** per-object optimization during the inference time. Because we do not replace the textual embedding in the pre-trained diffusion model, we use the prompt 'a picture of <class_name>' as the default prompt for both training and inference.

### Main Results Analysis

Given 2 context views, we evaluate novel view synthesis quality using the following metrics: FID , LPIPS , and PSNR 2. We believe that the combination of FID, LPIPS, and PSNR provides a comprehensive evaluation of novel view synthesis quality. FID and LPIPS measure the perceptual quality of the images, while PSNR measures the per-pixel accuracy. We note that PSNR has some drawbacks as a metric for evaluating generative models. Specifically, PSNR tends to favor blurry images that lack detail. This is because PSNR only measures the per-pixel accuracy of an image, and does not take into account the overall perceptual quality of the image. By using all three metrics, we can get a more complete picture of the quality of the images generated by our model.

#### 4.3.1 Object Level Novel View Synthesis

In-Domain EvaluationWe evaluate the performance of unseen objects NVS in training 10 categories. The quantitative results are presented in Table 5, which clearly demonstrates that our method surpasses the baseline methods in terms of both FID and LPIPS metrics. More specifically, DreamSparse outperforms SparseFusion by a substantial margin of 53% in the FID score and 28% in LPIPS. This significant improvement can be attributed to DreamSparse's capacity to generate sharper, semantically richer images, as depicted in Figure 1. This indicates the benefits of utilizing the potent image synthesis capabilities of pre-trained diffusion models.

Open-Set EvaluationWe also evaluate the performance of objects NVS in open-set 10 categories, because PixelNerf is per-category trained, we do not report its open-set generalization results. According to Table 6, it is evident that our method surpasses the baseline in both evaluation metrics in all categories, surpassing the second-best method by 28% in LPIPS and 43% in FID. Moreover, the results derived from our method are not just competitive, but can even be compared favourably to the training category evaluations of the baseline in Table 5 (122.2 vs 172.2 in FID and 0.24 vs 0.29 in LPIPS). This clearly illustrates the benefits of utilizing 2D priors from a large-scale, pre-trained 2D diffusion model for open-set generalization. We also show the qualitative results in Figure 3, and it shows that the novel view image synthesised by our method can still achieve sharp and meaningful results on objects in open-set categories.

    &  &  &  &  &  &  &  &  &  &  &  \\  FID & LPIPS & FID & LPIPS & FID & LPIPS & FID & LPIPS & FID & LPIPS & FID & LPIPS & FID & LPIPS & FID & LPIPS & FID & LPIPS & FID & LPIPS & FID & LPIPS & LPIPS \\  PN & 247.1 & 0.57 & 319.2 & 0.56 & 344.0 & 0.53 & 380.8 & 0.58 & 340.8 & 0.63 & 318.7 & 0.48 & 335.0 & 0.52 & 333.9 & 0.45 & 352.1 & 0.56 & 288.9 & 0.47 & 326.1 & 0.54 \\ SF & 110.9 & 0.28 & 143.3 & 0.30 & 258.5 & 0.34 & 185.7 & 0.33 & 126.6 & 0.29 & 165.7 & 0.23 & 168.5 & 0.31 & 202.6 & 0.28 & 199.3 & 0.34 & 167.8 & 0.23 & 172.6 & 0.29 \\ Ours **42.8** & **0.19** & **45.8** & **0.19** & **122.5** & **0.25** & **106.2** & **0.23** & **67.2** & **0.2** & **0.21** & **87.8** & **0.16** & **86.2** & **0.23** & **100.2** & **0.20** & **91.2** & **0.23** & **69.1** & **0.16** & **81.8** & **0.21** \\   

Table 2: Quantitative evaluation metrics on 10 subset of training categories from CO3D, where PN denotes PixelNerf  and SF denotes SparseFusion . Limited by the width, we only show FID and LPIPS score here.

Figure 3: Novel view synthesizing results on **open-set** category objects with the same context image inputs, where SF denotes SparseFusion  and GT denotes Ground-Truth image. More results are given at our project webpage and appendix F.

[MISSING_PAGE_FAIL:8]

### Ablation Studies

Number of Input ViewsWe investigate the performance by varying the number of context view inputs. The results are depicted in Table 6, which clearly illustrates the enhancement in three evaluation metrics as the number of input views increases. Moreover, the performance disparity between the **single view** and multiple input views is less pronounced in our method than in other baselines - a 6% difference vs 14% difference in Sparse Fusion vs a 17% difference in PixelNerf. This observation leads to two key conclusions: 1) DreamSparse exhibits greater robustness in response to variations in the number of context view inputs. 2) Despite the decrease in performance, DreamSparse can efficiently synthesise novel views from a single input view.

Spatial Feature VisualizationTo investigate the impact of the spatial guidance model, we employ Principal Component Analysis (PCA)  to visualize the spatial features post-integration of spatial guidance following . As shown in Figure 7, the visualized feature maps from the 2nd, 3rd, and 4th blocks of the UNet decoder indicate that despite the contextual view's geometry varying from that of the novel view, the feature maps steered by our spatial guidance model maintain alignment with the geometry of the ground truth image. This consistency enables the pre-trained diffusion model to generate images that accurately mirror the original geometry.

Spatial Guidance WeightWe investigate the effects of spatial guidance weight on the quality and consistency of synthesized novel view images. Our study varies the spatial guidance weight \(\), and the results in Fig 6 showed that when \(=0\) (indicating no spatial guidance), the pre-trained diffusion model failed to synthesize a novel view image that was consistent in terms of geometry and identity. However, as the weight increased, the synthesized images exhibited greater consistency with the ground truth. It is important to note, though, that an excessively high weight could diminish the influence of features in the pre-trained diffusion model, potentially leading to blurry output. Given the trade-off between quality and consistency, we set \(=2\) as the default hyperparameter.

Effect of Noise Perturbing Color EstimationThe impact of the Noise Perturbation method is showcased in Figure 8. It is evident that when the diffusion process begins from random noise, the spatial guidance model can successfully guide the pre-trained diffusion model to synthesize images

Figure 5: Qualitative results of novel view synthesis with textual control style transfer

Figure 6: Effect of the spatial guidance weighting of spatial guidance signals at the residual connections in the middle and decoder blocks of the diffusion model.

Figure 7: The spatial feature visualization with spatial guidance model, where context denotes the input context image, 2nd block denotes the visualization of feature maps from the 2nd block of the decoder and output denotes the synthesised novel view image.

with consistent geometry. However, the color or illumination density information is partially lost, leading to distortions in the synthesized novel view. In contrast, synthesizing the image from the noise that is added to the color estimation in the geometry model yields better results. As depicted in '+20 Noise' in Figure 8, the pre-trained diffusion model can effectively utilize the color information in the estimates, resulting in a more consistent image synthesis. We also experimented with varying the noise level added to the estimate. Our observations suggest that if the noise added to the blurry estimation is insufficient, the pre-trained diffusion model struggles to denoise the image because of the distribution mismatch between the blurry color estimate and Gaussian distribution, thereby failing to produce a sharp and consistent output.

## 5 Conclusion

In this paper, we present the _DreamSparse_, a framework that leverages the strong 2D priors of a frozen pre-trained text-to-image diffusion model for novel view synthesis from sparse views. Our method outperforms baselines on existing benchmarks in both training and open-set object-level novel view synthesis. Further results corroborate the benefits of utilizing a pre-trained diffusion model in object-centric scene NVS as well as in the generation of text-controlled scenes style transfer, clearly outperforming existing models and demonstrating the potential of leveraging the 2D pre-trained diffusion models for novel view synthesis.

Limitations and Negative Social ImpactDespite its capabilities, we discovered that our 3D Geometry Module struggles with generating complex scenes, especially ones with non-standard geometry or intricate details. This is due to the limited capacity of the geometry module and limited data, and we will introduce a stronger geometry backbone and train it on larger datasets. On the social impact front, our technology could potentially lead to job displacement in certain sectors. For instance, professionals in fields such as graphic design or 3D modelling might find their skills becoming less in-demand as AI-based techniques become more prevalent and advanced. It's important to note that these negative implications are not exclusive to this study, and should be widely considered and addressed within the realm of AI research.

## 6 Acknowledgement

Computational resources of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) were used for the experiments.