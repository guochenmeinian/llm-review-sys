# decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points

decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Quantization emerges as one of the most promising compression technologies for deploying efficient large models in recent years. However, existing quantization schemes suffer from significant accuracy degradation at very low bits, or require some additional computational overhead when deployed, making it difficult to be applied to large-scale applications in industry. In this paper, we propose decoupleQ, achieving a substantial increase in model accuracy, especially at very low bits.

decoupleQ abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into a mathematical constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods. decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels, etc., and focuses only on the basic optimization objective to achieve high model accuracy on extreme low bit quantization. Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness.

decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantization of large speech models in our company. The code (including the W2 CUDA kernels) is attached and will be made public.

## 1 Introduction

Serving large models [(1; 2; 37; 38)] in industry is budget-consuming because of the huge computational, IO and storage cost. Model compression [(10; 11; 16)] has therefore become a necessity to alleviate this pain. Among which, Post-Training Quantization (PTQ) [(9; 26)] has gained more and more popularity among researchers and engineers because it does not require heavy GPU-hours training with labeled datasets.

However, previous quantization schemes remain confined within the traditional heuristic quantization paradigm, e.g., how to deal with outliers [(33; 35)], how to deal with sensitive channels [(6)], how to determine the clipping range [(29)], and so on. These methods have achieved some success, but the quantization at extreme low bit often suffers from significant accuracy degradation, thus failing to meet the launching requirements of industrial practice. There are also some other options to mitigate the accuracy loss. QuIP [(4)] pushes the accuracy limits of 2-bit quantization and can achieve performance close to fp16/bf16. However, compared to traditional quantization schemes, its inference imposes an additional burden due to the need to multiply two random orthogonal matrices to de-quant the weights. N2UQ [(20)] fit the real-value distribution with non-uniform grids then quantize them into equidistant output levels. But it need to train to get the input thresholds. SpQR [(7)]and SqueezeLLM (14) use mixed-precision quantization or non-uniform scheme to safeguard the important channels, but they need customized hardware support.

In order to alleviate the above pains in industry, we proposed decoupleQ, which completely abandons the traditional heuristic quantization paradigm and instead decouples the model parameters into integer and floating point parts, then transforming the quantization problem into a mathematical constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods. The integer part contains the main weights of the model, and the floating-point part contains scales and zero points induced via quantization. decoulpeQ starts from an abstract objective function and thus does not need any tricks to deal with the minutiae of traditional quantization paradigm, such as outlier, salient weights (19), and so on. Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness.

decoupleQ contains two stages: 1. layer-wise minimization, defined in Eq. 1, is used to optimize the integer part and the floating-point part; 2. block-wise minimization, defined in Eq. 2, is used to further optimize the floating-point part while freezing the integer part1.

Layer-wise minimization is to minimize the \(^{2}\) loss of the outputs between pre- and post-quantization for a linear layer:

\[_{}\|X-XW_{0}\|_{2}^{2} \]

where \(X^{batch d_{in}}\) is the input of this layer, \(W_{0}^{d_{in} d_{out}}\) is the pre-trained full precision weight, \(d_{in}\) and \(d_{out}\) are the input and output dimensions respectively. The objective is to find a matrix \(\) with quantized-then-dequantized elements to minimize Eq. 1.

Some works (4; 8; 9; 13; 25) started from Eq. 1 and achieved some success, but they still haven't thought outside the box of traditional quantization. GPTQ series (8; 9) fake-quantize the first element of \(W_{0}\) and then update the the remaining elements so as to keep Eq. 1 minimized. This process is then continued element by element until all elements are fake-quantized. However, on the one hand, they do not give any indication of how scale and zero point should be calculated, and on the other hand, the optimization problem formulated for updating the remaining elements is unconstrained (explained in detail later). decoupleQ models Eq. 1 as a constrained optimization problem, as shown in Eq. 6. It no longer needs to pay attention to some of the minutiae unique to quantization, such as outliers, clipping threshold, etc., but abstracts the essence of the problem from a higher level.

In the second stage, block-wise minimization is used to further improve the model accuracy:

\[(X)}-(X)\|_{2}^{2} \]

where \(()}\) is a common transformer block (32) with quantized weights. In this stage, we freeze the integer part of the weights, and train the scales, zero points and norm layers.

decoupleQ implements 2-bit uniform quantization and achieves state-of-the-art accuracy in Llama-1/2 (30; 31). Like traditional uniform quantization, decoupleQ does not incur additional inference burden and only requires a linear transformation to convert the quantized weights into floating point ones.

Our main highlights are summarized as follows:

* **New insight:** We abandoned the traditional quantization paradigm, and no longer need to focus on some of the minutiae unique to quantization, but abstracts the essence of the problem from a higher level and transforms it into a constrained optimization problem.
* **Extreme low-bit:** decoupleQ achieves 2-bit uniform quantization with performance matching fp16/bf16 for industrial applications in the ASR model in our company, and we will also release the W2A16 CUDA kernel as one of our core contribution.
* **Extensibility:** As a bonus, if labeled datasets are available, the idea of decoupleQ can be easily extended to supervised fune-tuning (sft) to further improve model accuracy, or the adaptation to the downstream sub-tasks.

Related Works

Quantization can be roughly divided into Quantization Aware Training (QAT) [(21; 33)] and Post-Training Quantization (PTQ) [(4; 35)]. In this paper, we focus on weight-only quantization in PTQ, and we will only summarize a few works that are closely related to our work.

PTQ is commonly used for LLM quantization because it does not require a lot of GPU hours of training with labeled datasets. AdaRound [(25)] and BRECQ [(18)] start from the rounding operation and explore whether to round up or down is better. SqQR [(7)] and OWQ [(17)] use mixed-precision quantization strategy to protect sensitive parameters, while AWQ [(19)] opts for scaling up the weights of sensitive channels to reduce the loss of quantization of sensitive channels. OmniQuant [(29)] use gradient decent to optimize for the weight clipping threshold and the rescale factors. In decoupleQ, we abandon patchwork solutions and transform the quantization into a principled traditional optimization problem by decoupling the model parameters into integer and floating-point parts.

GPTQ [(9)] is an influential work, and it quantizes the current weights and then updates the remaining weights to minimize the \(^{2}\) loss of the output of the layer between pre- and post-quantization. As we will see later, this update actually approximates much, and GPTQ does not optimize for the scale and zero point reduced by quantization.

QALora [(36)] also decouples model parameters at a certain level and uses labeled datasets to fine-tune the zero points. decoupleQ takes this idea a step further, optimizing the scales, zero points and norm layers with supervised fine-tuning, while freezing the integer weights.

## 3 Methods

### Preliminaries

For a linear layer with input dimension \(d_{in}\) and output dimension \(d_{out}\), quantization maps the weights with high-precision into discrete level, and the previous scheme can be described as follows:

\[=(-z}{s},,) \] \[=*s+z \]

where \(W_{0}^{d_{in} d_{out}}\) is the pre-trained full precision weights, \(s\) and \(z\) are the scale and zero point (what we call floating-point part above), \(\) is the round-to-nearest function, \(^{d_{in} d_{out}}\) is the quantized integer-point matrix (what we call integer part above), \(\) is the de-quantized floating-point matrix, \(\) and \(\) are the lower and upper bounds of the range of integer representations, respectively. For example, in 2-bit weight only linear quantization scheme, the value of each entry of \(\) is limited to one of \(\{-2,-1,0,1\}\), and \(=-2\), \(=1\) in this case. To get the values of \(\), previous methods [(8; 9)] show that layer-wise \(^{2}\) loss between the outputs pre- and post-quantization is well related to the model accuracy, i.e., to optimize the following objective function,

\[_{} X-XW_{0}_{2}^{2}= \{(-W_{0})^{T}H(-W_{0})\} \]

where \(X^{batch d_{in}}\) is the input of this linear layer, generated by a small set of calibration dataset, and \(H=X^{T}X\).

In the very low-bit quantization regime, the model accuracy can be further improved via finer-grained grouping. This would impose additional overhead on inference. For example, when \(=64\), it imposes an average overhead of 0.5 bit per element (FP16/BF16 for scale \(s\) and zero point \(z\)). The extra overhead is acceptable compared to the model accuracy gain.

### decoupleQ

When a model is quantized, only the integer part \(\) and the floating-point part \((s,z)\) in Eq. 4 are delivered to the downstream inference engine, and the inference process does not need to know how \(\) and \((s,z)\) are obtained at all. That is, if we can find the values of \(\) and \((s,z)\) to minimize Eq. 5 by other methods, then we don't need to use Eq. 3. So, we can decouple the model parameters into integer part \(\) and floating point part \((s,z)\), which are then optimized alternatively via off-the-shelfsolution methods. decoupleQ views the process of solving for \(\) and \((s,z)\) in Eq. 4 as a constrained optimization problem independent of the previous quantization paradigm! We only need to regard Eq. 4 as an ordinary affine transformation, in which the value of \(s\) can be 0 or even negative.

In per-channel quantization, each column of the weight matrix is optimized independently of each other. For simplicity of notation, we only focus on one column in \(\) later and re-define the notations. Based on Eq. 5, the optimization problem of decoupleQ in the first stage, layer-wise minimization, can then be formulated as:

\[ g(w;s,z)\] (6) s.t. \[ i=1,2,...,d_{in}\] \[w_{i}- 0\] \[-w_{i}+ 0\] \[w_{i}\]

where the objective function is:

\[g(w;s,z)=(w*s+z-b)^{T}H(w*s+z-b) \]

\(w^{d_{in}}\) is one column of \(\), \(b^{d_{in}}\) is the corresponding column of \(W_{0}\), \(s^{ng}\) is the scale and \(z^{ng}\) is the zero point, \(ng\) is the number of groups when grouping-quantization. The operations w.r.t \((s,z)\), i.e., \(*s\) and \(+z\), need to be broadcasted to each group. In this paradigm, we have completely abandoned the traditional framework of quantization and instead transformed quantization into a constrained optimization problem 6, which is then solved to achieve the purpose of quantization. \((s,z)\) in problem 6 have lost the traditional meaning of scale and zero point, and are just two optimization variables.

Transforming the traditional quantization problem into problem 6 is the soul of decoupleQ! Problem 6 is a quadratic programming problem with an additional non-convex constraints \(w_{i}\). Quadratic programming has been studied for many years and there are now many well-established solution (24; 34). We provide one solution in the next subsection, which may not be efficient or optimal.

The core idea of decoupleQ is to decouple the model weights into the integer part \(w\) and the floating-point part \((s,z)\), with the integer part occupying most of the model's expressive power. The extensibility of the idea of decoupleQ is that we can freeze the integer part of the entire model, and use labeled data to train the \((s,z)\) as well as other floating point parameters. The advantage of this is that on the one hand, it can further improve the accuracy of the model, on the other hand, it can fit specific downstream sub-tasks while maintaining the generalization ability of the model.

### Optimization via Alternative Iteration

The problem 6 is not easy to solve because of the non-convex constraint \(w_{i}\). After obtaining a good initialization (explained in detail later), we solve for \(w\) and \((s,z)\) alternately and iteratively. In each round of alternation, the objective function 7 w.r.t \((s,z)\) is an unconstrained quadratic function, thus \((s,z)\) can be readily determined _analytically_: by differentiating the objective function and equating the derivative to zero, followed by solving the resultant linear system of equations. While for \(w\), the problem become problem 8:

\[ g(w;s,z)\] (8) s.t. \[ i=1,2,...,d_{in}\] \[w_{i}- 0\] \[-w_{i}+ 0\] \[w_{i}\]

For problem 8, one solution is to round-and-clip one element of \(w\) to be integer in \([,]\) and then update the remaining. And then this process is then performed sequentially for all elements. After the \(j\)-th element has been rounded-and-clipped, the objective for the updating then becomes problem 9. problem 9 is also intractable, and we can make two levels of approximation:\[_{w_{i};i>j} g(w;s,z)\] s.t. \[ i=j+1,...,d_{in} \] \[w_{i}- 0\] \[-w_{i}+ 0\]

In the first-level approximation 10, only the non-convex constraint \(w_{i}\) is discarded, while in the second-level approximation 11, both the non-convex constraint \(w_{i}\) and the convex constraint \(w_{i}[,]\) are discarded. Intuitively, problem 11 is much simpler to solve than problem 10, but solving problem 10 will lead to a better convergence of the primary objective( 6) than solving problem 11. GPTQ (9) provides an efficient analytical solution for problem 11, which we will directly utilize in our experiments. ( GPTQ updates the remaining elements by considering only the second-level approximation 11 and ignoring the constrain \(w_{i}[,]\) in the first ( 10), which is what we mentioned in the introduction, that the update of GPTQ is unconstrained.) As for problem 10, there are many mature solutions in the field of convex optimization, such as active-set method, projected gradient descent (PGD), projected coordinate descent and so on (3). We choose PGD because its parallelization is much better than the other two methods. In the experimental part, we will compare the final accuracy of the model via between solving the first level (10) and the second level 11 approximation on small models, while on large models (e.g. lager than 7 billion parameters), we have to choose the second level 11 approximation because the intolerable runtime of solving the first (10). The algorithm is shown in Alg. 1 and Alg. 2.

```
Input: predefined iteration number \(N\). Result:\(w^{*}\), \(s^{*}\), \(z^{*}\)
1 Initialize \(t=1,w_{0},s_{0},z_{0}\);
2while\(t N\)do
3 Freeze \((s_{t-1},z_{t-1})\), and optimize \(g(w;s_{t-1},z_{t-1})\) to obtain an approximate solution \(w_{t}\) via solving 8 via 2;
4 Freeze \(w_{t}\), and solve the unconstraint quadratic equation \(g(w_{t};s,z)\) to obtain an analytic solution for \((s_{t},z_{t})\);
5\(t=t+1\)
6 end while
7\(w^{*}=w_{N}\); \(s^{*}=s_{N}\); \(z^{*}=z_{N}\)
```

**Algorithm 1**Alternative Iteration to solve problem 6.

### Initialization of \(w\) and \((s,z)\)

Since the values of \(w\) are discrete, a good initialization is very important in order to obtain a more accurate solution to the original problem 6 with a faster convergence. Intuitively, the function \(g(w;s,z)\) contains the term \(w*s\), which means that the scales of the initial values of \(w\) and \(s\) have to be reasonably distributed. For example, in the extreme case when the initial value of \((s,z)\) have a very large scale, the first iteration will make most of the entries of \(w\) strictly 0, which will make the iteration crash. We start by initializing \((s,z)\). We can use grid search to solve the Eq. 12 for the initial value of \((s,z)\). In Eq. 12, \(p\) is a single number, may be different for different columns of \(W_{0}\), \(b_{min}\) and \(b_{max}\) are the minimum and maximum value of \(b\) respectively. This step is the same as the previous post-training quantization (19) process. Once the grid search is finished, we no longer need to concern ourselves with the \((s,z)\) inside the \(\) function. The point of this step is simply to find an initial value for \((s,z)\) for the optimization problem 6.

When solving problem 8 via the first-level approximation ( 10), before entering the for-loop in Alg. 2, we ignore the constraint \(w_{i}\) in problem 8 and optimize it via projected gradient decent with \(M\) iterations. The purpose of this is to allow the first-level approximation to converge in a small number of iterations, i.e., a small \(K\).

### Block-wise minimization

After solving problem 6, we obtain a solution for the layer-wise minimization stage and a reasonable model accuracy. But minimizing the \(^{2}\) loss at the layer level does not necessarily lead to the minimizing the \(^{2}\) loss at the block level. We found that the model accuracy can be further improved via optimization 2. BRECQ (18) also shows that block-reconstruction results in a better model accuracy than layer-reconstruction. In this stage, we freeze the integer part \(\) in the whole block and fine-tuning \((s,z)\) and the parameters in norm layer with \(J\) epochs.

## 4 Experiments

In this section, we describe in detail the experimental results of our method in comparison with other methods. Unless otherwise stated, all the experiments are conducted on a single A100-SXM-80GB, and the default experimental setting is as follows:

**ResNet:** 10240 images in the training dateloader are used as calibration data, with the standard augmentation in Pytorch official code (27), and the pretrained full precision checkpoints are from Torchvision (22). \(N=4,M=50\) (\(N\) and \(M\) is defined in refalg1 and refalg2). All the convolution layers and fully-connected layers are quantized into W2 without groups.

**Llama-1/2:** 128 2048-token segments from C4 (28) are used as calibration data. We choose C4 as calibration dataset instead of WikiText2 (23) to be consistent with GPTQ. If the block-wise minimization is used, we use Adam optimizer (15) to finetune the \((s,z)\) and the parameters in norm layer with \(J=4\) epochs. The learning rate is \(1e\)-\(5\), weight decay is \(1e\)-\(6\).

### Private Experiments

We applied decoupleQ to our company's two Automatic Speech Recognition models(ASR) (corresponding to task A and task B). Each of the models contain an encoder and an LLM decoder. The input of the models is a speech sequence and some prompt, and the output is the corresponding text. We quantize the LLM decoder to W2A16g64. The decoders of the two models contain 40 transformer blocks with 13 billion parameters and 32 transformer blocks with 7 billion parameters, respectively. Word Error Rate (WER) is used as metric to measure the accuracy of the models (less is better). In this experiments, we use about 8 millions of speech tokens as calibration dataset, and train 3 epoch in each block-wise minimization process. When an input batch contains sequences of varying lengths, we use a mask to make sure that the padding part is not involved in the computation of \(H\) and the loss of Eq. 2. In task B, once the whole model is quantized, we also fine-tune all the \((s,z)\) and layer norm in the LLM with labeled dataset, while freezing all the integer part \(\), with 8 A100-SXM-80GB GPUs. The accuracy is shown in Tab. 1, and the CUDA kernel latency is shown in Fig. 1. The W2A16 CUDA kernel is attached and will be merged into the NVIDIA repo as one of our core contribution.

Figure 1: The latency (in \(1e\)-6 seconds) of the four GEMMs in transformer block on L4 GPU, (The three GEMMs for query, key and value are concatenated into GEMM 1), with \(hidden\_dim=5120,batch\_size=4\).

### Public Comparison

As a first comparison, we compare decoupleQ with other methods on ImageNet (5) with ResNet (12), which are standard benchmarks and are efficient to implement. Most importantly, its Top-1 is a strong indicator of model accuracy. Tab. 2 shows the results of decoupleQ and others. The results other than decoupleQ are copied from GPTQ (9) and OBQ (8).

Tab. 3 shows the results on Llama. In this experiment, we have to choose the second level approximation(11) because the intolerable runtime of solving the first(10). For a fair comparison, the calibration dataset contains 128 samples, although a larger calibration dataset will result in stronger results. we can see that decoupleQ outperforms others almost in all settings, although we use a weaker approximation(11) to save time. As for the hype-parameters, we choose \(\{N=4,J=4\}\).

### Ablation studies

#### 4.3.1 the two approximations

The soul of decoupleQ is problem 6, but when solving problem 6, we have to take some approximations(10 or 11). Obviously, solving approximation 10 will be much more time consuming than solving approximation 11. But if solving approximation 10 yields better results, the time cost may be worth it. We first evaluate these two approximations from the perspective of model accuracy. In practice, we don't have to wait for approximation 10 to fully converge when we solve it via projected gradient decent, and only need to iterate some steps to get a sub-optimal solution. In Alg. 2, the for-loop takes up the majority of the runtime. So, we first study the influence of the number of iterations \(K\) (defined in the for-loop) on the final accuracy of the model. Fig. 2 shows the Top-1 accuracy of ResNet-18 on ImageNet w.r.t the number of iterations \(K\). First of all, in the blue line, we use only the layer-wise minimization of decooupleQ to quantize the model. After the quantization is finished, in the red line, we use the labeled dataset with the common 1.2 millions images to fine-tune all the \((s,z)\) and norm layers for one epoch, with the integer part being frozen. In this step, we use SGD optimizer with learning rate \(1e\)-\(6\), weight decaying rate \(1e\)-\(4\) to train for only one epoch. Fig. 2 clearly indicates the following conclusions: 1. As the number of iterations \(K\) increases, the model accuracy increases almost monotonically; 2. When \(K>4\), model accuracy via the first approximation(10) is better than via the second(11). This is to be expected, since the second approximation(11) drops the constraint \( w_{i}\), leading to a looser approximation; 3. By the supervised fine-tuning (sft), the model accuracy is further improved. The same experimental phenomenon also occurs on the ResNet-50 model, which we do not show here.

    &  &  \\   & BF16 & decoupleQ & BF16 & decoupleQ & decoupleQ+sft \\  WER & 6.68 & 6.70 & (5.86, 11.43) & (5.87, 11.56) & (5.77, 11.43) \\  runtime & - & 25 & - & 32 & 32+5 \\   

Table 1: The results of our two ASR models. The models are quantized into W2A16g64. runtime for the quantization process is measured in hours. There are two sub-domains in task B, and we report the WER of both.

    &  &  \\  & 2bit & 3bit & 4bit & 2bit & 3bit & 4bit \\  GPTQ & - & 67.88 & 69.37 & - & 74.87 & 75.71 \\ OBQ & 64.04 & 68.69 & 69.56 & 70.71 & 75.24 & 75.72 \\ BRECQ & 64.70 & 68.47 & 69.37 & 72.41 & 75.32 & 75.88 \\ decoupleQ & 64.15 & 68.65 & 69.58 & 71.34 & 75.24 & 76.00 \\ decoupleQ+sft & 65.45 & 68.94 & 69.71 & 72.65 & 75.61 & 75.97 \\   

Table 2: Comparison of decoupleQ with other methods. In decoupleQ, we only use the first stage, layer-wise minimization. All the models are quantized into W2A16 without groups. In decoupleQ+sft, we train the \((s,z)\) and norm layers for one epoch, using the regular labeled dataset containing 1.2 million images.

In the experiment shown in 3, we randomly select 512 2048-token segments from C4 (28). We chose 512 segments here instead of the common 128 in order to reduce the effect of overfitting and thus compare the two approximations more objectively. In this experiment, we take \(N=2\), and quantize Llama-7B into W2A16 without groups, and only the layer-wise minimization is used to exclude the interference of other factors. The PPL decrease almost monotonically as the number of iterations \(K\) increases. It shows that, when \(K>1\), solving approximation 10 yields better model accuracy than approximation 11.

However, when block-wise minimization is introduced in addition to the experiment in 3, the situation becomes a little more elusive. The results are shown in 4. The model's best PPL is where \(K=1\), and then fluctuates within a range as \(K\) continues to increase. But all PPLs are inferior to when the second-level approximation (11) is used. We also plot the loss, defined in 2, of the first block between pre-and post quantization on the right vertical axis. As \(K\) increases, the loss decreases strictly monotonically, and when \(K>2\), the loss falls below the case when the approximation 11 is used. This suggests that the correlation between PPL and loss is perhaps weak, and we will investigate this in the future.

   Llama & 1-7B & 1-13B & 1-30B & 1-65B & 2-7B & 2-13B & 2-70B \\  FP16 & & 5.68 & 5.09 & 4.10 & 3.53 & 5.47 & 4.88 & 3.31 \\   & GPTQ & 2.1e3 & 5.5e3 & 499.75 & 55.91 & 7.7e3 & 2.1e3 & 77.95 \\  & OmniQuant & 15.47 & 13.21 & 8.71 & 7.58 & 37.37 & 17.21 & 7.81 \\   & **decoupleQ** & **9.49** & **7.86** & **6.37** & **5.59** & **9.74** & **13.03** & **5.23** \\   & runtime & 2.5 & 4.8 & 12.7 & 27.6 & 2.5 & 4.5 & 33.4 \\   & GPTQ & 44.01 & 15.60 & 10.92 & 9.51 & 36.77 & 28.14 & - \\  & OmniQuant & 8.90 & 7.34 & 6.59 & 5.65 & 9.62 & 7.56 & 6.11 \\  & **decoupleQ** & **8.65** & **7.25** & **6.04** & **5.19** & **8.79** & **7.44** & **4.96** \\   & runtime & 3.7 & 7.7 & 24.3 & 55.0 & 3.7 & 7.9 & 70.6 \\   & GPTQ & 22.10 & 10.06 & 8.54 & 8.31 & 20.85 & 22.44 & - \\  & OmniQuant & 8.90 & 7.34 & 6.59 & 5.65 & 9.62 & 7.56 & 6.11 \\  & **decoupleQ** & **8.18** & **6.96** & **5.81** & **5.07** & **8.41** & **6.98** & **5.34** \\   & runtime & 4.3 & 8.9 & 27.9 & 64.5 & 4.4 & 9.0 & 98.2 \\   & GPTQ & 8.06 & 6.76 & 5.84 & 5.06 & 8.37 & 6.44 & 4.82 \\  & AWQ & 11.88 & 7.45 & 10.07 & 5.21 & 24.00 & 10.45 & - \\  & OmniQuant & 6.49 & 5.68 & 4.74 & **4.04** & 6.58 & **5.58** & 3.92 \\  & **decoupleQ** & **6.38** & **5.60** & **4.67** & 6.05 & **6.22** & **5.72** & **3.84** \\   & GPTQ & 6.13 & 5.40 & 4.48 & 3.83 & 5.83 & 5.13 & 3.58 \\  & AWQ & 6.08 & 5.34 & 4.39 & 3.76 & 6.15 & 5.12 & - \\  & OmniQuant & 5.86 & 5.21 & 4.25 & 3.71 & 5.74 & **5.02** & 3.47 \\   & **decoupleQ** & **5.85** & **5.21** & **4.24** & **3.67** & **5.70** & **5.06** & **3.45** \\   

Table 3: The results of PPL of wikitext-2 on Llama-1/2. We also report the runtime (measured in hours) for the W2 quantization via decoupleQ in the gray background row. The results other than decoupleQ are copied from OmniQuant (29). All the results of decoupleQ use the approximation 11.

#### 4.3.2 the size of calibration dataset

The solution of problem 6 is dependent on \(H\) and thus on the the calibration dataset, as does Eq. 2. Fig. 5 shows the relationship between dataset size and PPL. In this experiment, Llama-7B is quantized into W2A16g64. We use the second-level approximation (11) to save time, and \(\{N=4,J=4\}\). For runtime reference, when the number of segments is 128/2048, the experiment took 4.3/19.5 hours.

#### 4.3.3 the necessity of block-wise minimization

Tab. 4 shows that block-wise minimization(2) can further improve the model accuracy. In this experiment, we choose \(N=4\) and the approximation 11 for the layer-wise minimization, and \(J=4\) if block-wise minimization is used.

## 5 Conclusion and Discussion

decoupleQ decouples the model parameters into the integer part and a floating point part, and then optimizes them alternately. This optimization process contains two stages. In the layer-wise minimization, we transform the quantization problem into the purely mathematical constrained optimization problem refdecoupleQ; while in the block-wise minimization, we freeze the integer part and then finetune the floating point part.

The risks of decoupleQ include the following: 1. How much the minimization of the \(^{2}\) loss of the layer's or block's output correlates with the accuracy of the model; 2. decoupleQ is prone to overfitting the calibration dataset; 3. The runtime of the quantization process is longer than others.

For the first risk, we find experimentally that the correlation between Top-1 and the loss is strong in the Imagenet classification task; however, the correlation between PPL and the loss is slightly weaker in LLM. This could be mainly because of an inherent bias between the loss and the accuracy of the model, or because PPL is not a good indicator of the accuracy of LLM, or for other reasons. For the second risk, when \(H\) in Eq. 7 is an underdetermined matrix, the risk of overfitting rises sharply. In this case, the possibility of \(H\) being underdetermined can be reduced either by enhancing the diagonal element values of \(H\) or by increasing the amount of calibration data. In our practice, we found that the accuracy of quantization models can rise monotonically with the increase of the size of the calibration dataset, especially in W2 quantization, but the runtime of quantization rise as well. In addition, due to time constraints, we do not provide a wealth of public comparisons. However, we believe that the novelty of a method may outweigh the number of experiments.

The idea of decoupleQ is helpful for the adaptation of large model to downstream sub-task. We can quantize a large foundation model via decoupleQ, then freeze the integer part of the model, and finetune the floating-point part with labeled dataset from downstream sub-task. Tab. 1 and Tab. 2 show that the model accuracy can be further improved by end-to-end supervised learning.

   Llama & 1-7B & 1-13B & 1-30B & 2-7B & 2-13B \\  w/o & 13.66 & 9.68 & 7.35 & 14.66 & 12.93 \\ w & 9.49 & 7.86 & 6.37 & 9.74 & 13.03 \\   

Table 4: The perplexity of Llama on WikiText2 with and without the block-wise minimization. All the models are quantized into W2A16.

Figure 4: The PPL of Llama-7B on WikiText2 and the loss of the first block between pre-and post-quantization. Solid and dashed lines are for approximation 10 and 11 respectively.

Figure 5: The perplexity of Llama-7B on WikiText2 and C4 dataset w.r.t the number of segments as calibration datasets. The model is quantized into W2A16g64.