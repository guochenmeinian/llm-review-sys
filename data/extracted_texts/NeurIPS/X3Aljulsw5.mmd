# Zeroth-Order Sampling Methods for

Non-Log-Concave Distributions: Alleviating

Metastability by Denoising Diffusion

 Ye He

Georgia Institute of Technology

yhe367@gatech.edu &Kevin Rojas

Georgia Institute of Technology

kevin.rojas@gatech.edu &Molei Tao

Georgia Institute of Technology

mtao@gatech.edu

###### Abstract

This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Denoising Diffusion Monte Carlo (DDMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DDMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DDMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZODMC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DDMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality. Consequently, for low dimensional distributions, ZOD-MC is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based RDMC and RSDMC. Last, we experimentally demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between modes or discontinuity in non-convex potential.

## 1 Introduction

The problem of drawing samples from a distribution based on unnormalized density \((-V)\) (described by the potential \(V\)) is a fundamental statistical and algorithmic problem. This classical problem nevertheless remains as a research frontier, providing pivotal tools to applications such as decision making, statistical inference / estimation, uncertainty quantification, data assimilation, and molecular dynamics. Worth mentioning is that machine learning could benefit vastly from progress in sampling as well, not only because of its connection to inference, optimization and approximation, but also through modern domains such as diffusion generative modeling & differential privacy.

Recent years have seen rapid developments of sampling algorithms with quantitative and non-asymptotic theoretical guarantees. Many of the results are either based on discretizations of diffusion processes [12; 13; 50; 16; 34; 33] or gradient flows [38; 10; 22]. In order to develop such guarantees, it is necessary to make assumptions about the target distributions, for instance, that it satisfies an isoperimetric property, where standard requirements are log-concavity or functional inequalities[12; 56; 21; 8; 48]. However, there is empirical evidence that the corresponding algorithms struggle to sample from targets that have high barriers between modes that create metastability. Overcoming such issues is highly nontrivial and researchers have continued to develop new methods to tackle these problems.

Diffusion models have lately shown remarkable ability in the generative modeling setting, with applications including image, video, audio, and macromolecule generations. This created a wave of theoretical work that showed the ability of diffusion models to sample from distributions under minimal assumptions [14; 57; 5; 32; 29; 4; 11; 3]. However, these works all started with the assumption that there is access to an approximation of the score function with some accuracy. This is a reasonable assumption for the task of generative modeling when one spends enough efforts on the training of the score, but the task of sampling is different. A natural question is: can we leverage the insensitivity of diffusion models to multimodality to efficiently sample from unnormalized, non-log-concave density? This would require approximating the score, which is then used as an inner loop inside an outer loop that integrates reverse diffusion process to transport, e.g., Gaussian initial condition, to nearly the target distribution.

The seminal works by [26; 27; 20] try to answer this question using Monte Carlo estimators of the score function and to provide theoretical guarantees. We also mention earlier work by  which learns parameterized scores and more work along the same line by [58; 44; 54; 55], whose theoretical guarantees are less clear but are based other interesting ideas.  proposed Reverse Diffusion Monte Carlo (RDMC), which estimates the score via LMC algorithm and relaxes the isoperimetric assumptions in the analysis of traditional sampling algorithms.  proposed a similar method, stochastic localization via iterative posterior sampling (SLIPS), which approximate the score via Metropolis-adjusted Langevin algorithm (MALA). However, both methods rely on the usage of a small time window where isoperimetric properties hold. This leaves the problem of finding a good initialization for the diffusion process. To alleviate this issue,  developed an acceleration of RDMC, the Recursive Score Diffusion-based Monte Carlo (RSDMC), which improves the non-asymptotic complexity to be quasi-polynomial in both dimension and inverse accuracy and gets rid of any isoperimetric assumption. Such work provides strong theoretical guarantees, however it requires a lot of computational power to get a high accuracy sampler. Additionally, RDMC, SLIPS and RSDMC are all based on first-order queries (i.e. gradients of \(V\)), which brings extra computational and memory costs, in addition to requiring a continuous differentiable \(V\). Motivated by these two observations, we create a sampler that only makes use of **zeroth-order** queries without assuming any isoperimetric conditions on the target distribution. Our contributions can be summarized as follows.

* We introduce an oracle-based meta-algorithm **DDMC (Denoising Diffusion Monte Carlo)** and provide a non-asymptotic guarantee in KL-divergence in Theorem 1. Our result provides theoretical insight on the choice of optimal step-size in DDMC as well as in denoising diffusion models (Sec. 3.2).
* We develop a novel algorithm **ZOD-MC (Zeroth Order Diffusion-Monte Carlo)** that uses zeroth-order queries and the global minimal value of the potential function to generate samples approximating the target distribution. In Corollary 3.1, we establish a zeroth-order query complexity upper bound for general target distributions satisfying mild smoothness and moment conditions. Our result is summarized and compared to other sampling algorithms in Table 5.
* The advantages of our algorithm are experimentally verified for non-log-concave target distributions. We demonstrate the insensitivity of our algorithm to various high barriers between modes, and the ability of correctly account for discontinuities in the potential.

## 2 Preliminaries

### Diffusion Model

Diffusion model generates samples that are similar to training data, by requiring the generated data to follow the same latent distribution \(p\) as the training data. To do so, it considers a forward noisingprocess that transforms a random variable into Gaussian noise. One most commonly used forward process is (a time reparameterization of) the Ornstein-Uhlenbeck (OU) process, given by the SDE:

\[X_{t}=-X_{t}t+B_{t}, X_{0} p,\] (1)

where \(\{B_{t}\}_{t 0}\) is the standard Brownian motion in \(^{d}\). The OU process that solves (1) is in distribution equivalent to a sum of two independent random vectors: \(X_{t}=e^{-t}X_{0}+}Z\) where \((X_{0},Z) p^{d}\) and \(^{d}\) is the standard Gaussian distribution in \(^{d}\). Denote \(p_{t}=(X_{t})\) for all \(t 0\). If we consider a large, fixed terminal time \(T\) of (1), then \(p_{T}\) is close to \(^{d}\). Then, the denoising or backwards diffusion process, \(\{_{t}\}_{0 0 T}\), can be constructed by reversing the OU process from time \(T\), meaning that \((_{t})(X_{T-t})\) for all \(t[0,T]\). By doing so we obtain the denoising diffusion process which solves the following SDE:

\[_{t}=(_{t}+2 p_{T-t}(_{t})) t+_{t},_{0} p_{T},\ 0 t T,\] (2)

where \(\{_{t}\}_{0 t T}\) is a Brownian motion in \(^{d}\), independent of \(\{B_{t}\}_{0 t T}\) and \( p_{t}\) is usually referred as the score function for \(p_{t}\). Although the denoising process initializes at \(p_{T}\), we can't generate exact samples from \(p_{T}\). In practice, people consider the standard Gaussian initialization \(^{d}\) due to the fact that \(p_{T}\) is close to \(^{d}\) when \(T\) is large. The denosing process with the standard Gaussian initialization is given by

\[_{t}=(_{t}+2 p_{T-t}(_{t})) t+_{t},_{0}^{d},\ 0 t T.\] (3)

By simulating this denoising process (3), we can achieve the goal of generating new samples. However, the denoising process (3) can't be simulated directly due to the fact that the score function is not explicitly known. A widely applied method to solve this issue is to learn the score function through denoising score matching . Given a learned score, denoted as \(s(t,x)\), one can simulate the denoising diffusion process using discretizations like the Euler Maruyama or some exponential integrator. From a theoretical perspective, assuming the learned score satisfies

\[_{x p_{t}}\|s(t,x)- p_{t}(x)\|^{2} _{}^{2},\ 0 t T,\] (4)

non-asymptotic convergence guarantees for diffusion models are obtained in . For instance, in , polynomial iteration complexities were proved without assuming any isoperimetric property of the data distribution and only assuming the data distribution has a finite second moment and a score estimator satisfying (4) is available.

In this work, we consider instead the sampling setting, in which no existing samples from the target distribution is available. Our sampling algorithm and theoretical analysis are motivated from the denoising diffusion process given by (2) and its corresponding discretization through the exponential integrator in Algorithm 1. In particular, we first introduce an oracle-based meta-algorithm, DDMC, which integrates Algorithm 1 and Algorithm 2, where the exponential integrator scheme of (2) is applied to generate samples and the score function is approximated by a Monte Carlo estimator assuming independent samples from a conditional distribution are available.

### Rejection Sampling and Restricted Gaussian Oracle

Rejection sampling is a popular Monte Carlo method for sampling a target distribution, \(p\), based on the zeroth-order queries of the potential \(V\). It requires that we have access to the potential function

  Algorithms & Queries & Assumptions & Criterion & Oracle Complexity \\  LMC & first-order & LSI & KL & \((d^{-1})\) \\  RDMC & first order & soft log-concave1  & TV & \((((d))(^{-1}))\) \\  RSDMC & first-order & None & KL & \(((^{3}(d^{-1})))\) \\  Proximal Sampler & zeroth-order & log-concave & KL & \((d^{-1})\) \\  ZOD-MC & zeroth-order & None & \(+W_{2}\)2  & \((}(d)((^{-1})))\) \\  

Table 1: Comparison of ZOD-MC to LMC, RDMC, RSDMC and the Proximal Sampler: Summary of isoperimetric assumptions and oracle complexities to generate a \(\)-accurate sample under different criterion. \(\) hides \((d^{-1})\) factors. The zeroth-order oracle complexity of ZOD-MC is from Corollary 3.1 for achieving both \(\) KL and \(\)\(W_{2}\) errors. These theoretical results suggest that in the absense of isoperimetric assumptions, ZOD-MC excels in low-dimensions.

\(V_{}\) of some other distribution \(\), such that \(\) is easy to sample from and \((-V)(-V_{})\) globally. Such a distribution \(\) is typically called an envelope for the distribution \(p\). With an envelope \(\), rejection sampling generates samples from \(p\) by running the following algorithm till acceptance:

1. Sample \(X\),
2. Accept \(X\) with probability \((-V(X)+V_{}(X))\).

The rejection sampling is considered as a high-accuracy algorithm as it outputs a unbiased sample from the target distribution. However, despite such a remarkable property, it has drawbacks. First, it is a nontrivial task to find an envelope for a general target distribution. Second, rejection sampling usually suffers from "curse of dimensionality". Even for strongly logconcave target distributions, the complexity of the rejection sampling increases exponentially fast with the dimension: in expectation it requires \(^{d/2}\) many rejections before one acceptance, where \(\) is the condition number for the potential \(V\), see .

The Restricted Gaussian Oracle (RGO), which was first introduced in , assumes that an accurate sample from distribution \((|y)-V()-\|-y\|^ {2}\) can be generated for any \(y^{d}\), \(>0\) and any potential \(V\). Implementing the RGO is challenging. It is usually done by rejection sampling. However, most proposed methods [36; 18], are only suitable for small \(\).

Our proposed sampling algorithm, ZOD-MC applies the rejection sampling (Algorithm 3) to implement the RGO with a large value of \(\). Details on ZOD-MC are introduced in Section 3.1.

``` Input :\(N_{+}\), \(0=t_{0}<<t_{N}=T-\), score estimator \(\{s(T-t_{k},)\}_{k=0}^{N-1}\). Output :\(x_{N}\).  generate a sample \(x_{0}^{d}\); for\(k=0,1,,N-1\)do  generate \(_{k}^{d}\) such that \(_{k}\) is independent to \(_{0},,_{k-1}\); \(x_{k+1} e^{t_{k+1}-t_{k}}x_{k}+2(e^{t_{k+1}-t_{k}}-1)s(T-t_{k},x_{k} )+-t_{k})}-1}_{k}\).  end for ```

**Algorithm 1**Denoising Diffusion Sampling via Exponential Integrator

## 3 Denoising Diffusion Monte Carlo Sampling

In this section, we first introduce DDMC and ZOD-MC in Section 3.1. Then we provide a convergence guarantee for DDMC in Section 3.2. Last, in Section 3.3, we establish the zeroth-order query complexity of ZOD-MC. Note DDMC is a meta-algorithm that still requires an implementation of its oracle, and ZOD-MC is an actual algorithm that contains such an implementation. The theoretical guarantee of ZOD-MC (Sec. 3.3), therefore, is based on the analysis framework of DDMC (Sec. 3.2).

### Denoising Diffusion Monte Carlo and Zeroth-Order Diffusion Monte Carlo

_Denoising Diffusion Monte Carlo (DDMC)._ Let's start with a known but helpful lemma on score representation, derivable from Tweedie's formula .

**Lemma 1**.: _Let \(\{X_{t}\}_{t 0}\) be the solution to the OU process (1) and \(p_{t}=(X_{t})\). Then for all \(t>0\),_

\[ p_{t}(x)=_{x_{0} p_{0|t}(|x)} x_{0}-x}{1-e^{-2t}},\] (5)

_where \(p_{0|t}(|x)-V()-x\|^{2}}{e^{2t}-1}\) is the distribution of \(X_{0}\) conditioned on \(\{X_{t}=x\}\)._

This lemma was for example applied in  to do sampling based on the denoising diffusion process in (2). For the sake of completeness, we include its proof in Appendix C.5.

Due to (5), to approximate the score function \( p_{t}(x)\), it suffices to generate samples that approximate \(p_{0|t}(|x)\). [26; 27] proposed to use Langevin-based algorithms to sample from \(p_{0|t}(|x)\). The first step of our work is to generalize this, with refined and more general theoretical analysis later on, by considering an oracle algorithm, DDMC, which assumes independent samples \(\{z_{t,i}\}_{i=1}^{n(t)}\) that approximate \(p_{0|t}(|x)\) are available. The Monte Carlo score estimator in Algorithm 2 is given by

\[s(t,x)=_{i=1}^{n(t)}z_{t,i}-x}{1-e^{-2t}},\] (6)

where \(n(t)\) is the number of samples and \((t)\) is such that \(W_{2}((z_{t,i}),p_{0|t}(|x))(t)\) for all \(i\). In Section 3.2, we will discuss how the performance of sampling depends on \(n(t),(t)\).

_Zeroth-Order Diffusion Monte Carlo (ZOD-MC)._ Noticing that in Lemma 1, the conditional distribution has a structured potential function: a summation of the target potential and a quadratic function. Therefore, implementing the oracle in DDMC is equivalent to implementing RGO with \(y=e^{t}x\) and \(=e^{2t}-1\). Based on this, we propose ZOD-MC, a novel methodology based on rejection sampling and DDMC. Rejection sampling (Algorithm 3) can generate i.i.d. Monte Carlo samples required in Algorithm 2. Therefore, ZOD-MC, as a combination of rejection sampling and DDMC, can efficiently sample from non-logconcave distributions. See Appendix B for more details.

```
0:\(x^{d}\), zeroth-order queries of \(V\).
0:\(z\). whileTRUEdo  Generate \((,u)^{d} U\); \(z e^{t}x+-1}\); return\(z\) if\(u(-V(z)+V^{*})\);  end while ```

**Algorithm 3**Rejection Sampling: generating \(\{z_{t,i}\}_{i=1}^{n(t)}\) in Algorithm 2

**Remark 1**.: _(Remark on the optimization step) In theory, we assume an oracle access to the minimum value of \(V\). However, in practice we use Newton's method to find a local minimum. Throughout the sampling process we update the local minimum as we explore the search space._

**Remark 2**.: _(Parallelization) Notice that Algorithm 3 can be run in parallel to generate all the \(n(t)\) samples required to compute the score. Contrary to methods like LMC that have a sequential nature, this allows our method to be more computationally efficient and reduce the running time. This is a feature that RDMC or RSDMC doesn't benefit as much from._

### Convergence of DDMC

Our oracle-based meta-algorithm, DDMC, provides a framework for designing and analyzing sampling algorithms that integrate the denoising diffusion model and the Monte Carlo score estimation. In this section, we first present an error analysis to the Monte Carlo score estimation in Proposition 3.1, whose proof is in Appendix C.3. After that, we leverage our result in Proposition 3.1 and provide a non-asymptotic convergence result for DDMC in Theorem 1, whose proof is in Appendix C.4.

**Proposition 3.1**.: _Let \(\{X_{t}\}_{t 0}\) be the solution of the OU process (1) and \(p_{t}=(X_{t})\) for all \(t>0\). If we define \(s(t,x)=_{i=1}^{n(t)}z_{t,i}-x}{1-e^{-2t}}\) with \(\{z_{t,i}\}_{i=1}^{n(t)}\) being a sequence of independent random vectors such that \(W_{2}((z_{t,1}),p_{0|t}(|x))(t)\) for all \(t>0\) and \(x^{d}\), then we have_

\[\| p_{t}(X_{t})-s(t,X_{t})\|^{2} }{(1-e^{-2t})^{2}}(t)^{2}+} {(1-e^{-2t})^{2}}_{p}(x).\] (7)

_Choice of \((t)\) and \(n(t)\)._ The error bound in (7) helps choose the accuracy threshold \((t)\) and the number of samples \(n(t)\) to control the score estimation error over different time. In fact, when \(t\) increases, it requires less samples and allows larger sample errors to get a good Monte Carlo score estimator. If we assume \(_{p}(x)=(d)\) for simplicity, then when \(t\) is small, the factor \(}{(1-e^{-2t})^{2}}=(t^{-2})\) and the choice of \((t)=(t)\) and \(n(t)=(dt^{-2}^{-2})\) will lead to the \(L^{2}\)-error of order \((^{2})\). When \(t\) is large, the factor \(}{(1-e^{-2t})^{2}}=(e^{-2t})\) and it only requires \((t)=(e^{t})\) and \(n(t)=(de^{-2t}^{-2})\) to ensure the \(L^{2}\)-error is of order \((^{2})\). In the latter case, the \((t)\) is of a larger order and \(n(t)\) is of a smaller order than the first case.

We now analyze the convergence of DDMC. Recall that Algorithm 1 is an exponential integrator discretization scheme of (2) with the time schedule \(0=t_{0}<t_{1}<<t_{N}=T-\) for some \(>0\). In each iteration, \(x_{k+1}=e^{t_{k+1}-t_{k}}x_{k}+2(e^{t_{k+1}-t_{k}}-1)s(T-t_{k},x_{k})+-t_{k})}-1}_{k},\) where \(_{k}^{d}\) and \(s(T-t_{k},)\) is the Monte Carlo score estimator generated by Algorithm 2. The trajectory of Algorithm 1 can be piece-wisely characterized by the following SDEs: for all \(t[t_{k},t_{k+1})\),

\[_{t}=(_{t}+2s(T-t_{k},_{t_{k}})t+_{t},_{0}^{d},\ _{t_{k}}=x_{k}.\] (8)

Therefore, the convergence of DDMC is equivalent to the convergence of the process \(\{_{t}\}_{0 t t_{N}}\), which could be quantified under mild assumptions on the target distribution. Next, we present the moment assumption on the target distribution and our non-asymptotic convergence theorem.

**Assumption 3.1**.: _The distribution \(p\) has a finite second moment: \(_{x p}[\|x\|^{2}]=_{2}^{2}<\)._

**Theorem 1**.: _Assume that the target distribution satisfies Assumption 3.1. Let \(\{X_{t}\}_{t 0}\) be the solution of (1) with \(p_{t}(X_{t})\) and \(\{_{t}\}_{t 0}\) be the solution of (8) with \(q_{t}(_{t})\). For any \((0,1)\) and \(T>1\), let \(0=t_{0}<t_{1}<<t_{N}=T-\) be a time schedule such that \(_{k}=(_{k-1})\) for all \(k=0,1,,N-1\), where \(_{k} t_{k+1}-t_{k}\). Then_

\[(p_{}|q_{t_{N}}) _{2}^{2})e^{-2T}}_{I}+ {_{k=0}^{N-1}e^{-2(T-t_{k})}}{(1-e^{-2(T-t_{k})})^{2}} (T-t_{k})^{2}+_{2}^{2}}{n(T-t_{k})}}_{I\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!

### Complexity of ZOD-MC

With the convergence result for DDMC in Theorem 1, we introduce the query complexity bound of ZOD-MC. Our analysis assumes a relaxation of the commonly used gradient-Lipschitz condition on the potential. The formal statement is presented in Corollary 3.1, whose proof is provided in Appendix C.6.

**Assumption 3.2**.: _There exists a constant \(L>0\) such that for any \(x^{*}*{arg\,min}_{y^{d}}V(y)\) and \(x^{d}\), \(V\) satisfies \(V(x)-V(x^{*})\|x-x^{*}\|^{2}\)._

**Corollary 3.1**.: _Under the assumptions in Theorem 1 and Assumption 3.2, if we set \(T=(_{2}^{2}}{_{}})\), \(_{k}=(1,T-t_{k})\), \(=(_{2}}^{2}}{d},_{2}}}{*{m}_{2}})\), \(=)}{N}\), then to obtain an output (with distribution \(q_{t_{N}}\)) in ZOD-MC such that \(W_{2}(p,p_{})_{_{2}}\) and KL\((p_{},q_{t_{N}})_{}\), the zeroth-order query complexity is of order_

\[}_{2}^{2}}{ _{}},}{_{}^{2}} _{}^{-}(d+_{2}^{2})L^{}d^{-1}_{0 k N-1}L\|x^{*} \|^{2}+\|x_{k}\|^{2},\] (10)

_where the \(}\) hides \((_{2}^{2}}{_{_{2}}})\) factors._

**Remark 4**.: _If we assume WLOG that the minimizer of the potential is at the origin, i.e., \(x^{*}=0\), and further make reasonable assumptions that \(_{2}^{2},L\) and \(\{\|x_{k}\|^{2}\}\) are all of order \((d)\), where \(\{x_{k}\}\) are the iterates in Algorithm 1, then the query complexity of ZOD-MC is of order \(}(d)(_{}^{-1})\). Even though this complexity bound has an exponential dimension dependence, it only depends polynomially on the inverse accuracy. Since it applies to any target distribution satisfying Assumptions 3.1 and 3.2, this complexity bound suggests that with the same overall complexity, ZOD-MC can generate samples more accurate than other algorithms in Table 5, for a large class of **low-dimensional non-logconcave** target distributions._

**Comparison to LMC, RDMC and RSDMC.** When no isoperimetric condition is assumed, we compare convergence for ZOD-MC to convergence for LMC, RDMC and RSDMC.

In the absence of the isoperimetric condition,  demonstrated that LMC is capable of producing samples that are close to the target in FI assuming the target potential is smooth. However, FI is a weaker divergence than KL divergence/ Wasserstein-2 distance. It has been observed that, in certain instances, the KL divergence/Wasserstein-2 distance may still be significantly different from zero, despite a minimal FI value. This observation implies that the convergence criteria based on FI may not be as stringent as our result which is based on KL divergence/Wasserstein-2 distance.  proved that RDMC produces samples that are \(\)-close to the target in KL divergence with high probability. Assuming the potential is smooth and a tail-growth condition, the first order oracle complexity is shown to be of order \((^{-1} d)\).  introduced RSDMC as an acceleration of RDMC. They were able to show that if the potential is smooth, RSDMC produces a sample that is \(\)-close to the target in KL divergence with high probability. The first order oracle complexity is shown to be of order \((^{3}(d/))\). Compared to RDMC and RSDMC, our result on ZOD-MC doesn't require the potential to be smooth as our Assumption 3.2 is only a growth condition of the potential. This indicates that our convergence result applies to targets with non-smooth, or even discontinuous potentials. Our result in Corollary 3.1 shows the zeroth-order oracle complexity for ZOD-MC is of order \((d(^{-1}))\), which achieves a better \(\)-dependence compared to RDMC and RSDMC, at the price of a worse dimension dependence. This suggests that, for any low-dimensional target, ZOD-MC produces a more accurate sample than RDMC/RSDMC when the overall oracle complexity are the same. Last, zeroth-order queries cost less computationally than first-order queries in practice, which also makes ZOD-MC a more suitable sampling algorithm when the gradients of the potential are hard to compute.

## 4 Experiments

We will demonstrate ZOD-MC on three examples, namely Gaussian mixtures, Gaussian mixtures plus discontinuities, and Muller-Brown which is a highly-nonlinear, nonconvex test problem popular in computational chemistry and material sciences. Multiple Gaussian mixtures will be considered, for showcasing the robustness of our method under worsening isoperimetric properties. The baselines we consider include RDMC , RSDMC , SLIPS , the proximal sampler , annealedimportance sample , sequential Monte Carlo , a parallel tempering approach with MALA proposals  and naive unadjusted Langevin Monte Carlo. All the experiments are conducted using a NVIDIA GeForce RTX \(4070\) Laptop GPU with \(8\)GB of VRAM and Pytorch.

### Results for Gaussian Mixtures

**Matched Oracle Complexity.** We modify a 2D Gaussian mixture example frequently considered in the literature to make it more challenging, by making its modes unbalanced with non-isotropic variances, resulting in a highly asymmetrical, multi-modal problem. We include the full details of the parameters in Appendix D. We fix the same oracle complexity (total number of \(0^{th}\) and \(1^{st}\) order \(V\) queries) for different methods, and show the generated samples in Figure 2. Note matching oracle complexity puts our method at a disadvantage, since other techniques require querying the gradient, which results in more function evaluations. Despite this, we see in Figure 0(a) that our method achieves both the lowest MMD and \(W_{2}\) using the least number of oracle complexity.

**Robustness Against Mode Separation.** Now let's further separate the modes in the mixture to investigate the robustness of our method to increasing nonconvexity/metastability. More precisely, we scale the means of each mode by a constant factor to have a mode located at \((0,R)\); doing so increases the barriers between the modes and exponentially worsens the isoperimetric properties of the target distribution . Figure 3(a) shows our method is the most insensitive to mode separation. Being the only one that can successfully sample from all modes, as observed in Figure 3, ZOD-MC suffers less from metastability. Note there is still some dependence on mode separation due to the \(x_{k}\) dependence in the complexity bound in Corollary 3.1.

**Dimension Dependence Against Other Diffusion Based Methods.** One drawback of our method, is its bad dimension dependence when compared to diffusion based methods. For instance, RDMC and RSDMC have a dependence of \((((d))}(^{-1}))\) and \(((^{3}(d^{-1})))\) respectively, in comparison to our \((}(d)(((^{-1}))))\). Despite this theoretical disadvantage, we find empirically that these methods don't scale well with dimension either. To demonstrate this we sample \(5\) points on the positive quadrant and use them as means for a GMM. We then evaluate statistics on the generated samples and \(W_{2}\) as a function of dimension. We observe in Figure 0(b) that under a fixed number of function evaluations our method results in the lowest \(W_{2}\). More details are in Appendix D.

**Discontinuous Potentials.** The use of zeroth-order queries allows ZOD-MC to solve problems that would be completely infeasible to first order methods. To demonstrate this, we modify the

Figure 1: Accuracies of different methods for sampling Gaussian Mixture

Figure 2: _Sampling from asymmetric, unbalanced Gaussian Mixture._ All diffusion-based methods (ZOD-MC, RDMC, RSDMC) use \(2200\) oracles per score evaluation. Langevin and the proximal sampler are set to use the same total amount of oracles as diffusion based methods. While other methods suffer from metastability, ZOD-MC correctly samples all modes.

potential in Figure 2. We consider \(V(x)+U(x)\) where \(U\) is a discontinuous function given by \(U(x)=8[\|x\|]\,_{\{5<|x| 11\}}\) This creates an annulus of much lower probability and a strong potential barrier. In the original problem, the mode centered at the origin was chosen to have the smallest weight (\(0.1\)), but adding this discontinuity significantly changes the problem. As observed in Figure 5, our method is still able to correctly sample from the target distribution, while other methods not only continue to suffer from metastability but also fail to see the discontinuities. We quantitatively evaluate the sampling accuracy by using rejection sampling (slow but unbiased) to obtain ground truth samples, and then compute MMD and \(W_{2}\). See Appendix D.2 for details.

**Score Approximation of Diffusion Based Methods.** One explanation of our method's great success in comparison with RDMC and RSDMC is the ability to approximate the score correctly. We select an unbalanced assymetrical \(5\)d GMM and evaluate the average \(L^{2}\) score error between methods. On Figure 3(b) we show that the best approximations of the score are found by using ZODMC as an estimator as opposed to other methods. Even as \(t\) increases and the approximation gets harder we are able to retain accuracy and therefore generate high quality samples.

### Results of Muller Brown Potential

The Muller Brown potential is a toy model for molecular dynamics. Its highly nonlinear potential has 3 modes despite of being the sum of 4 exponentials. The original version has 2 of its modes corresponding to negligible probabilities when compared to the 3rd, which is not good to visualization and comparison across different methods. Thus we consider a balanced version  and further translate and dilate \(x\) and \(y\) so that one of the modes is centered near the origin. The details of the potential can be found in Appendix D.5. Our method is the only one that can correctly sample from all \(3\) modes as observed in Figure 6 (note they are leveled).

Figure 4: _Accuracies of generated samples against dimension and Score Error_. On the right, the result for SLIPS is not directly comparable as it has a different forward process.

Figure 5: _Generated samples for discontinuous Gaussian Mixture_. Our method can recover the target distribution even under the presence of discontinuities. The same oracle complexity is again used in each method, \(3200\) per score evaluation in diffusion-based approaches.

Figure 3: _Gaussian Mixture with further separated modes (\(R=26\)). ZOD-MC can overcome strengthened metastability and sample from every mode, while other methods are stuck at the mode at the origin, where every method is initialized._