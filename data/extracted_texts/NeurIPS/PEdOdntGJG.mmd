# Looped Transformers for Length Generalization

Ying Fan\({}^{1}\), Yilun Du\({}^{2}\), Kannan Ramchandran\({}^{3}\), Kangwook Lee\({}^{1}\)

\({}^{1}\)University of Wisconsin-Madison \({}^{2}\)Massachusetts Institute of Technology \({}^{3}\)UC Berkeley

###### Abstract

Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e., handling inputs of unseen lengths. In this work, we demonstrate that looped Transformers with an _adaptive number of steps_ significantly improve length generalization. We focus on tasks with a known iterative solution, involving multiple iterations of a RASP-L operation--a length-generalizable operation that can be expressed by a finite-sized Transformer. We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.

## 1 Introduction

Most algorithmic tasks such as coding, writing mathematical proofs, and reasoning are defined with inputs of variable _length_. The length of an input often correlates with the difficulty of the problem instance. For example, the longer the input, the more difficult the problem tends to be. We say a model perfectly _length-generalizes_ if it can solve an algorithmic task on inputs of any length, even if it was only trained on data with inputs up to a finite length . Generally, it is hard to expect models to be trained on inputs with all possible lengths, and we need to rely on length generalization. Also, if a model can length-generalize, it means the model has truly learned the correct algorithmic solution to the task, not just a spurious solution that works only for a certain range of input lengths.

Recently, many works on Large Language Models (LLMs) have shown that we can get more powerful AI models by scaling both compute and data at training time. This scaling approach has indeed succeeded in improving accuracies on various benchmarks. However, even the largest and latest LLMs like  which have been trained on much of the existing text on the Internet, still struggle with length generalization . One possible cause is the particular computing model. LLMs are built based mostly on the Transformer architecture . While Transformers can accept a variable length of inputs (that can be processed in parallel), they usually have a fixed depth. This might be sufficient for certain tasks, but not always.

To learn a model that can effectively generalize to longer problems, it is important to consider architectures that can adaptively adjust the computational budget to the difficulty of the tasks . One approach to achieve this is to explicitly generate intermediate output tokens, similar to writing down a scratchpad, which improves LLMs' capability for solving harder problems. In theory, LLMs may generate more scratchpad tokens representing intermediate computation when solving a more difficult task, indicating that they can allocate elastic computation according to the length and difficulty of the given instance. This approach can be learned by explicitly training a model on data with intermediate computation steps . Alternatively, it can be achieved via Chain-of-Thought (CoT) reasoning with few-shot examples  or even in a zero-shot manner . Notice that these approaches still use fixed-depth models. While these approaches help solve more complex reasoning tasks, they are still far from achieving near-perfect length generalization for simple algorithmic tasks. For instance, Lee et al. applied CoT for arithmetic tasks but observed that Transformers cannot length generalize even for simple addition tasks .

Recently, there has been growing interest in using recurrent architectures for reasoning [10; 3; 5; 36]. Unlike standard RNN-type architectures that process different parts of the input sequence incrementally, one can consider a recurrent architecture that processes the entire input sequence multiple times. This architecture passes the intermediate processing output to the next iteration's input, possibly along with the original input. In particular, if the base model used in each iteration is a Transformer, this model is called a Looped Transformer .

Looped Transformer can naturally break the limitation of the fixed depth in the standard Transformer architecture: _One can adjust the number of looped steps based on the computational complexity of the underlying algorithmic solution_. Consider a problem set with the following properties: 1) The problems can be solved by a loop of one RASP-L  program1, i.e., each step in the loop can be performed by a decoder-only Transformer with a fixed depth; 2) The number of steps needed in the loop depends on the problem's complexity, i.e., more difficult problems could potentially require more steps to solve. Under the length generalization scheme, we consider the number of steps depending on the problem length, and define this problem set as \(n\)-RASP-L problems. For \(n\)-RASP-L problems, if we can learn these length-independent steps, we can utilize an adaptive number of steps to achieve length generalization.

Inspired by this observation, we study training Looped Transformers models for length generalization. Specifically, we consider a training setup where we do not require any intermediate supervision data (such as reasoning steps or scratchpad). We only assume access to end-to-end supervision (input and output) and the number of steps needed. Depending on the number of steps, we iteratively apply the same decoder block and then decode the final answer; See Figure 1 for illustration. At inference time, the model could either decide when to stop with predefined stopping criteria or stop when reaching the ground-truth number of steps. Empirically, we show that looped Transformers with an adaptive number of steps can successfully length-generalize to longer lengths simply by appropriately adapting the number of loops at inference time, indicating that our approach encourages the model to implicitly learn the necessary steps to solve a task.

Our contributions can be summarized as follows: **(1)** We first formally define \(n\)-RASP-L problems, and provide examples of \(n\)-RASP-L solutions to the Copy, Parity, and Addition tasks (Section 2); **(2)** We propose to learn \(n\)-RASP-L problems with Looped Transformers where we supervise the final answer in a step-dependent way, enabling us to use an adaptive number of steps depending on the problem complexity (Section 3); **(3)** Empirically, we show that our proposed method outperforms the baseline approaches in terms of length generalization performance (Section 5). Due to lack of space, we present the background introduction of RASP-L, next-token prediction and full-answer prediction in Section A, related work in Section 4, and full experimental results in Section C in the appendix.

## 2 \(n\)-Rasp-L

RASP-L programs  do not allow loops. If we consider the next-token prediction (NTP) scheme, it means that we need to find the same RASP-L program (which can be represented with a fixed-depth decoder-only Transformer) to predict the next token given any possible prefix in the answer sequence. Such solutions might not always exist for all problems: there is no known RASP-L program for addition, parity, and copy under the NTP scheme . On the other hand, architectures such as the Looped Transformer have external loops embedded in the architecture which naturally provides adaptive depth. Thus, a natural question is: what kind of algorithmic tasks can we represent with a decoder-only Transformer in a loop? Specifically, what if we also allow the number of iterations to

Figure 1: **Method Overview. During training, we supervise the output of the model to match the target data only after a certain number of steps of applying the same decoder block, helping the model learn intermediate steps that can be reused and can handle input of arbitrary lengths. All grey blocks share the same parameters. Examples are from the Copy task with \(n\) symbols. “#” indicates EOS, “*” indicates ignored output, and “\(>\)” indicates the end of the query (EOQ).**

explicitly depend on the input length, say \(n\)? Moreover, what if we are not constrained by the NTP scheme, but a more general full-answer prediction (FAP) scheme?

Inspired by these questions, we define the following class of algorithmic tasks:

**Definition 2.1** (\(n\)-Rasp-L).: _A program \(P\) is called an \(n\)-RASP-L program if (1) there exist \(T(n):\), and (2) \(P\) can be decomposed to a sequential application of \(P^{}\) for \(T(n)\) steps._

We show that \(n\)-digit addition, \(n\)-bit parity, copying \(n\) symbols indeed have \(n\)-RASP-L solutions. For the parity task, \(P^{}_{}\) is to shift the input sequence to the right by 1 and calculate XOR of the answer sequence and the input sequence; For the copy task, \(P^{}_{}\) is to shift the input sequence to the right by 1; For the addition task \(P^{}_{}\) is to calculate the XOR of two sequences and shift the results to the right by 1 position as the partial answer, and calculate the AND of two sequences as the carry-on sequence2. See Figure 4, Proposition B.1, B.2, B.3 and Listings 1, 2, 3 for details.

## 3 Learning \(n\)-RASP-L problems with looped Transformers

We present a novel framework for length generalization: In the absence of ground truth CoT data/intermediate output, we propose to leverage the inherent structure of the problem with the help of "knowing when to stop". We present the setup for training data in Section 3.1, the model architecture and training algorithm in Section 3.2, and the inference algorithm in Section 3.3.

### End-to-end supervised data without intermediate step supervision

We consider the following settings: (1) There exists an \(n\)-RASP-L program that solves the given task. (2) Training data consists only of \((x,y)\) pairs, but not intermediate steps. That is, we do not have access to \(P^{}(x),P^{}(P^{}(x)),\). (3) \(T(n)\), i.e., the ground truth number of iterations to solve the problem (with some \(P^{}\)) is available in the training data3. (4) The length \(n\) is diversely distributed in the dataset, e.g., \(n\{1,,n_{}\}\) where \(n_{}\) is the maximum number of lengths in the dataset; The ground truth number of steps needed \(T(n)\) is also diversely distributed in the dataset, e.g., \(T(n)\{1,,T(n_{})\}\) where \(T(n_{})\) is the maximum number of steps in the dataset4.

### Looped training with step supervision

#### 3.2.1 Architecture of the looped Transformers

We present the model architecture in Figure 1 with following key characteristics. (1) Recurrence: The Looped Transformer is recurrent (like  but with decoder-only structure): We reuse the same decoder block for a number of steps, where each block consists of a certain number of layers. We can adjust the number of looped steps at will. (2) Input injection: For each step, the input embeddings are added to the output embeddings of the previous step as the input of the current step, preventing information loss with improved performance . (3) Positional embedding: There is no positional encoding in the RASP-L operations . To follow our \(n\)-RASP-L assumption and test the effect of the looped training, we use NoPE  to avoid the impact from different positional embeddings5.

#### 3.2.2 Training

Given a dataset \(D=\{(\{(x_{l})_{l=1}^{L_{i}}\}_{i},\{(y_{l})_{l=1}^{L_{i}}\}_{i},T_{i},L_{i}) \}_{i=1}^{N}\), where \(\{(\{(x_{l})_{l=1}^{L_{i}}\}_{i}\) is the input with \(L_{i}\) tokens, \(\{(y_{l})_{l=1}^{L_{i}}\}_{i}\) is the output with \(L_{i}\) tokens, and \(T_{i}\) is ground truth number of steps of sample \(i\). We aim to learn the transformer model \(M_{}\)6 by minimizing the following loss:

\[_{D}[(f_{T_{i}}(M_{},\{(\{(x_{l})_{l=1}^{L_{i }}\}_{i}),(\{y_{l})_{l=1}^{L_{i}}\}_{i}\}_{i})],\] (1)

where \(\) is the cross entropy loss and \(f_{T_{i}}(M_{},\{(\{(x_{l})_{l=1}^{L_{i}}\}_{i})_{i}=(M_{}( M_{}(\{(x_{l})_{l=1}^{L_{i}}\}_{i}))}_{})\).

### Adaptive inference

Looped Transformers can use adaptive depth at inference time, so we need certain rules to decide when to stop. We consider two rules: 1) _Oracle_: We can assume that the number of steps needed is given; 2) _Maximum confidence:_ We can use confidence base rules to decide when to stop, i.e., stop when we are confident about the output at the current step. More specifically, for 2), given a test sequence \(\{(\{(x_{l})_{l=1}^{L}\}\) and a trained model \(M_{}\), we can get the number of steps \(T\) from Equation (2):

\[T=_{t[1,T_{}]}(f_{t}(M_{},\{(\{(x_{ l})_{l=1}^{L}\}),\{(})_{l=1}^{L}\}^{t}\}),\] (2)

where \(\{(})_{l=1}^{L}\}^{t}\) is the decoded sequence from \(f_{t}(M_{},\{(\{(x_{l})_{l=1}^{L}\})\) at step \(t\), \(T_{}\) is a threshold for the maximum number of steps.

## 4 Related work

**Positional embedding for length generalization.** Positional embeddings have been shown to greatly affect Transformers' ability to generalize to longer lengths [27; 19; 29; 31; 22; 7; 30; 24; 16]. By designing positional embedding schemes that better capture relative positional information with techniques such as randomization and functional representations, researchers have made significant progress in improving length generalization. Especially,  and  use tailor-made positional embeddings for some arithmetic problems without potential generality. 7 This direction is orthogonal to our work since there is no positional encoding in RASP-L operations. We choose no positional embedding in our experiments, but other positional embeddings could further be synergistically applied with our approach. However, they might not be expressed as RASP-L operations. We leave further investigation with different positional embeddings to future work.

**RNNs and Chomsky Hierarchy.** conduct an extensive empirical study to investigate the limits of the generalization performance of different neural network structures, demonstrate that grouping tasks according to the Chomsky hierarchy allows forecasting whether certain architectures will be able to generalize to out-of-distribution inputs. Their results show that RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on some context-free and context-sensitive tasks. In our paper, the Looped Transformer

Figure 2: **Length Generalization Performance. Our looped Transformer model with adaptive depth generalized better than NTP methods across studied tasks, including the variants with pause tokens and weight-tied layers. The vertical dashed line indicates the maximum training length. NTP indicates vanilla next-token prediction; NTP-Pause indicates next-token prediction with pause tokens; NTP-Loop indicates next-token prediction with a fixed number of weight-tied layers.**architecture also has augmented memory and the recurrent structure but is potentially more powerful since each iteration contains an operation of the whole sequence.

Universal Transformers and other looped models.Our method is highly inspired by Universal Transformers (UT) , but we introduce several novel modifications to design looped Transformers that are compatible with our \(n\)-RASP-L assumption. One major architectural innovation is the use of FAP, while all the other prior works are based on NTP. We also only use decoder-only Transformers, which is different from UT and the follow-up work PonderNet , which use both encoder and decoder Transformers. In addition to these two critical differences, we do not use any positional encoding, and use a simpler halting mechanism. Moreover, we find input injection useful to further improve the performance (see details in Section C.3). Table 1 summarizes the differences between ours and the previous approaches. Besides architectural differences, we are also the first to show the benefit of using step-dependent supervision for training looped Transformers. Apart from Transformers,  study learning recurrent networks to generalize to harder maze problems than seen during training, but with a focus on CNNs.

Input representation.Recall that adding two numbers of length \(n\) could not be solved by a RASP-L program where the difficulty mainly comes from indexing operations . It could be solved by reformatting the input so that each digit is presented to the model with "index hints" in . Such reformatting enables a simple RASP-L program for addition. Similarly, representing the answer in reversed order also helps because the corresponding RASP-L program gets much shorter, providing a concrete justification of the empirical observation made in . However, such input representations are highly dependent on the specific problems and might not necessarily exist in general.

COT.Scratchpad or CoT reasoning [25; 23; 9; 33; 18] is also useful for length generalization as it could simplify the next-token prediction task with intermediate results presented to the input layer. There are also potential drawbacks and limitations to CoT reasoning. First, CoT training data could be hard to collect. Training and inference with pause tokens  has been proposed to learn implicit CoT steps without CoT data, but pause tokens only increase horizontal compute, not sequential compute. Second, not all CoT steps are helpful. If CoT steps introduce additional complexity or require operations not easily expressible in RASP-L, then CoT may hinder length generalization, as shown in . Moreover, CoT steps that could convert the next token prediction task to RASP-L programs might not always exist. Besides, CoT is normally constrained to fixed-depth models, while we study a more general and powerful way to use adaptive compute at inference time.

## 5 Experiments

In this section, we evaluate the efficacy of looped Transformers. We consider tasks with \(n\)-RASP-L solutions presented in Section 2: Parity, Copy, and Addition, together with more tasks like calculating the sum, multiplication, and calculating the unique set. Due to lack of space, we introduce the detailed experimental setup in Section C.1, present length generalization results in Section C.2, ablation studies in Section C.3, and visualize the stopping criterion in Section C.4 in the appendix.

Looped Transformers help with length generalization.As shown in Figure 2, our looped model significantly improves the length generalization performance. For example, for Parity, it can generalize to more than 50 digits near perfectly8 when only trained with up to 20 digits. Moreover, for tasks like addition and copy, where the next token prediction failed when tested on maximum training length \(+10\), our looped model can still perform nearly perfectly. All of the models are only trained with a relatively small number of lengths, and the looped model generalizes surprisingly well.

Variants of NTP could improve generalization but not as effectively as our adaptive-depth model.Compared with vanilla NTP, we observe that NTP-Loop could lead to improved generalization in tasks like Addition, Copy and Multiplication. Similarly, NTP-pause could introduce slight improvement in Parity and Unique Set. However, they all fall behind compared with our method. Besides, NTP-Loop suffers from lower in-distribution accuracy in Parity, possibly because using a fixed-depth model with weight-tied layers for NTP with all lengths might be too constrained for the task.

   Method & Encoder/Decoder & Prediction Type & PE & Input Injection & Halting Mechanism \\  UT & Both & NTP & Yes & No & ACT  \\ PonderNet & Both & NTP & Yes & No & Halting node \\ Ours & Decoder-only & FAP & No & Yes & Confidence based or predefined \\   

Table 1: Comparison between UT, PonderNet, and ours. PE is short for “Positional Embeddings”.