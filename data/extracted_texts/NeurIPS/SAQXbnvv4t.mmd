# AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data

Zifan Song\({}^{1,2}\)\({}^{*}\) Yudong Wang\({}^{2}\)\({}^{*}\) Wenwei Zhang\({}^{2}\)\({}^{*}\) Kuikun Liu\({}^{2}\)

Chengqi Lyu\({}^{2}\) Demin Song\({}^{2}\) Qipeng Guo\({}^{2}\) Hang Yan\({}^{2}\)

Dahua Lin\({}^{2,3,4}\) Kai Chen\({}^{2}\)\({}^{}\) Cairong Zhao\({}^{1}\)\({}^{}\)

\({}^{1}\)Tongji University \({}^{2}\)Shanghai AI Laboratory

\({}^{3}\)MMLab, The Chinese University of Hong Kong

\({}^{4}\)HKGAI under InnoHK

###### Abstract

Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained LLMs. In this paper, we present _AlchemistCoder_, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed _AlchemistPrompts_, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that _AlchemistCoder_ holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence. Source code and models are available at https://github.com/InternlM/AlchemistCoder.

+
Footnote †: Corresponding author

+
Footnote †: Corresponding author

## 1 Introduction

Closed-source Large Language Models (LLMs) like ChatGPT and GPT-4  exhibit impressive code intelligence by learning on large-scale and diverse code corpus, which also benefits many other applications, such as math reasoning , embodied control , and agent . Since open-source LLMs  still lag behind closed-source LLMs  in this field, there has been growing interest in investigating the acquisition of code capabilities by developing specialized Code LLMs .

The training of Code LLMs mainly goes through pre-training and fine-tuning stages . Pioneer

Figure 1: Performance scatter plot (_top right_ is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our _AlchemistCoder_ series demonstrates astonishing performance across all open-source Code LLMs.

works [5; 32; 22; 36; 3] have amassed extensive code data for pre-training, while recent open-source models [30; 44] highlight the effectiveness of high-quality or targeted code fine-tuning datasets. Despite these advancements, current fine-tuning methods mainly rely on a specific type of code-related question-answering dataset, unlike the pre-training stage that integrates code-related corpus from various sources . Such a discrepancy indicates that the fine-tuning data may lack the necessary diversity to fully stimulate the capabilities of base models, resulting in limited performance, generalization, and robustness.

To overcome the limitations in quality and diversity within single-source data, we pioneer to explore integrating multi-source data for Code LLM fine-tuning. However, this is a non-trivial paradigm and blindly integrating multi-source data can potentially lead to inferior performance (_e.g._, the DirectlyMix-L-7B model in Fig. 1). To track this, we unveil inherent conflicts in multi-source code corpora, including conflicting code language requirements and response styles. Inspired by hindsight relabeling [1; 48], we propose to design data-specific prompts to harmonize the inherent conflicts for multi-source data integration, better eliciting the performance of base models. We term this form of prompts as _AlchemistPrompts_, inspired by the power and definition of _Alchemists_:

_"Alchemist: Someone Who Transforms Things for the Better."_ ---- Merriam Webster

Specifically, we first integrate several open-source code datasets and conduct instruction evolution  based on some of them (Fig. 2(a, b)). As shown in Fig. 2(c), for instruction-response pairs of different sources, we adopt one LLM to generate _AlchemistPrompts_ that accurately and explicitly describe the characteristics as requirements of the response to enrich the instructions. In-depth, the efficacy of _AlchemistPrompts_ is twofold: 1) Harmonization between different data sources: _AlchemistPrompts_ generated from the same LLM have similar styles and can bridge the style differences between sources, while the introduction of _AlchemistPrompt_-customized data, accounting for only 5%, achieves a balance between data diversity and domain gaps; 2) Harmonization within instruction-response pairs: As fine-grained and data-specific prompts, _AlchemistPrompts_ are designed to augment instructions with specific programming languages, algorithm concepts, and other code-related information involved in responses, which can refine the alignment within instruction-response pairs and enhance the instruction-following abilities of fine-tuned models.

Apart from the conventional problem-solution data, we argue that the progression of code data (_e.g._, data evolution, cleaning, and quality evaluation) reflects higher-level capabilities and offers valuable insights for the enhancement of Code LLMs. Consequently, we delineate the construction of data into three integral tasks for training: instruction evolution, data filtering, and code review (see Fig. 2 (d)), facilitating enhanced code comprehension capabilities.

We conduct extensive experiments with various base models [40; 35; 14] and develop the instruction-tuned _AlchemistCoder_ series. As shown in Fig. 1, on two mainstream code benchmarks, HumanEval

Figure 2: Overview for developing _AlchemistCoder_ series. We first integrate high-quality open-source data (a) and conduct data evolution based on them (b). Then, we adopt _AlchemistPrompt_ to harmonize their inherent conflicts (c) and construct code comprehension data (d). We use a mix of these data to fine-tune various pre-trained LLMs to obtain our _AlchemistCoder_ models.

and MBPP, _AlchemistCoder_ holds a clear lead among all models of equivalent size (6.7B/7B), and rivals or even surpasses larger models (15B/33B/70B), demonstrating harmonized and formidable code capabilities. Furthermore, we delve into the effectiveness of _AlchemistPrompts_ and discern that they alleviate the misalignment between instructions and responses within the data. Remarkably, _AlchemistPrompts_ allow the code corpus to also significantly improve the general capability of Code LLMs, as demonstrated by the improvements on MMLU, BBH, and GSM8K. Our main contributions are summarized as follows:

* Our work pioneers to integrate multi-source data for Code LLM fine-tuning to overcome the limitations of quality and diversity inherent in single-source data.
* We unveil inherent conflicts within multi-source code corpora and introduce _AlchemistPrompts_, revealing the power of hindsight tuning for code generation, aiming to harmonize the conflicts among sources and bridge the alignment within instruction-response pairs.
* We propose to incorporate data construction process into the fine-tuning data and design code comprehension tasks, including instruction evolution, data filtering, and code review, endowing comprehensive code capabilities.
* Extensive ablation and analytical studies confirm the efficacy of our key concepts for enhancing the diversity, quality, and cost-effectiveness of Code LLM fine-tuning data. Through instruction tuning on various base models, we develop the _AlchemistCoder_ series, surpassing all Code LLMs of the same size on a wide spectrum of code benchmarks.

## 2 Method

To more comprehensively elicit the capability of the base LLMs, we first construct multi-source data for fine-tuning (SS 2.1), which is harmonized by _AlchemistPrompts_ to take effect(SS 2.2). Code comprehension tasks are also constructed to further improve the performance(SS 2.3). We also discuss the details and statistics of the filtered and harmonized multi-source data in SS 2.4.

### Multi-source data construction

To fully elicit the capability of code LLMs, we first collect the fine-tuning data from multiple sources (Fig. 2(a)) and adopt the instruction evolution  to improve the complexity of the instructions (Fig. 2(b)). However, integrating multi-source data for instruction tuning poses challenges. Naturally,

Figure 3: Examples of inherent conflicts (_e.g._, various styles and quality) within multi-source code corpora. By applying _AlchemistPrompt_-customized instructions that are more consistent with the responses, the diversity from multiple sources can be effectively managed and utilized, thereby improving the quality of our fine-tuning data and the instruction-following capabilities of the fine-tuned models.

one code-related question can be solved by different coding languages with various algorithms or response styles (_e.g._, with or without reasoning). When naively combing data curated by different developers with different LLMs, the model tends to learn to answer similar questions with different coding languages and response styles, as depicted in Fig. 3. On the one hand, learning diverse responses may elicit different capability aspects of the base models. On the other hand, since the learned responses to similar instructions often diverge due to implicit human intentions, the LLMs tend to be unaligned (to our expectation) after the fine-tuning on the directly mixed data (_e.g._, we cannot expect which coding language the LLMs will use in real-world applications), resulting in inferior performance. Therefore, directly mixing multi-source data is not a promising solution and can be detrimental.

### AlchemistPrompt

To harmonize the inherent conflicts within multi-source data, we propose to customize data-specific prompts called _AlchemistPrompts_, (Fig. 2(c)), inspired by the concept of hindsight [1; 48]. Specifically, we employ GPT-4  to play the role of an _Alchemist_ and design the prompt as illustrated in Fig. 4 to obtain _AlchemistPrompts_. For instance, for an instruction of 'Write code to find the shortest path from one vertex to all other vertices in a graph', if the response involves Python code of a Bellman-Ford algorithm with dynamic programming, we would expect to customize the instruction with an _AlchemistPrompt_ of 'Please generate Python code for the following task and attempt to use the concept of Dynamic Programming'.

For the selection of data customized by _AlchemistPrompts_, we calculate the differences in perplexities of generating responses with/without given instructions, called Conditional Perplexity Discrepancy (CPD). Then, we selectively chose data with higher CPD values for _AlchemistPrompt_ harmonizations. We treat CPD as an indicator of how data affects the complexity of model-generated responses under given conditions (_i.e._, instructions), and its calculation formula is \(=(+)-()\). The level of CPD reflects the impact of the conditional instruction on the complexity of the generated response. Specifically, a high CPD indicates that the perplexity of the generated response significantly increases under the presence of a conditional instruction, which usually reflects a poor alignment between the instruction and the response, the instruction may be unclear or not specific enough, or insufficient contextual information, thereby increasing the difficulty of model response generation. By analyzing high CPD values, we can identify cases where instructions and responses are poorly aligned and more effectively optimize data quality.

The adjustments to data made by _AlchemistPrompts_ are relatively minor and well-calibrated. Our ablation study indicates that the optimal performance can be achieved by incorporating _AlchemistPrompts_ into only 5% samples, striking a balance between the diversity and domain gap resulting from the fusion of multi-source data. Crucially, by retrospectively analyzing responses and reinterpreting them as alternate goals, the _AlchemistPrompts_ serve to elevate the condition/goal of data. This hindsight integration [1; 48; 26] allows for a more nuanced and adaptive learning process, enhancing not only the models' comprehension of data but also refining instruction-following capabilities.

### Code comprehension task

The existing training datasets for Code LLMs [24; 4; 39; 30; 44] primarily focus on the code generation task consisting of programming problems and their corresponding code solutions. However, we

Figure 4: Detailed prompt designed for generating data-specific _AlchemistPrompts_.

contend that beyond this, the process of constructing code data demonstrates higher-level abilities. Consequently, we devise three code comprehension tasks relevant to data construction, including instruction evolution, data filtering, and code review (Fig. 2(d)).

**Instruction evolution.** Inspired by the concept of instruction evolution [45; 30], we employ GPT-3.5  to construct instruction evolution task data, which entails augmenting the requirements for instructions and providing detailed explanations for programming tasks. Integrating the instruction evolution task aids the model in discerning the disparities before and after evolutions, thereby deepening the comprehension of programming requirements, code complexity, task decomposition, and other code-related concepts.

**Data filtering.** We identify four categories of low-quality data from multiple sources: (a) responses that are excessively short and lack code, (b) code that fails to compile, (c) code with poor clarity, and (d) code that does not adhere to the requirement in the instruction regarding its organization in function form. Each instruction in the data filtering task presents the model with a low-quality sample and prompts the model to classify it into one of the four categories. The data filtering task entails recycling the filtered-out data by offering counterexamples, thereby assisting the model in generating fewer low-quality responses.

**Code review.** In this task, we require the model to review a piece of code and assign scores between 0 and 10 for correctness and clarity separately. Additionally, the model is expected to provide suggestions for code improvement and present the refined code. To obtain higher-quality data, we utilize GPT-4  to generate code reviews and select cases that are more representative, particularly those with average correctness and clarity scores exceeding 8 or falling below 6. Simultaneously, we focus on instances where one aspect exhibits severe deficiencies, _i.e._, the score of correctness or clarity is equal to or below 4.

### Data cleaning and decontamination

In practice, we have established a set of filtering rules to enhance our data cleaning and purification procedures. These rules involve excluding samples based on various criteria, such as response length (either too short or too long), absence of code or insufficient code content, non-compilable code, code failing test cases (pertinent to certain samples), responses structured in notebook form, and instances with excessive textual descriptions preceding the code. After conducting an extensive series of validation experiments, we conclusively decide to eliminate low-quality data meeting either of the following conditions: (a) responses that are excessively brief and lack code. Such responses typically offer direct answers to the instructions, neglecting both the code solution and explanatory annotations. Additionally, these samples frequently present overly simplistic questions in the instructions; (b) code solutions that are non-compilable or fail test cases (relevant to specific samples).

Concurrently, following , we employ N-gram similarity, cosine distance of code embeddings, and edit distance of code syntax trees to calculate the similarity between training data and samples in HumanEval and MBPP. We subsequently discard samples through this process of data filtering and deduplication, resulting in the removal of approximately 6% of the dataset.

### Harmonized AlchemistCoder dataset

Our _AlchemistCoder_ dataset (\(\)200M tokens) comprises four types of multi-source data, encompassing open-source datasets and three types of data constructed by us. Specifically, (a) open-source datasets including Evol-Instruct-Code-80k-v1 , CodeExercise-Python-27k , and evol-codealpaca-v1 , (b) EvolCode data generated from gpt-3.5-turbo following , (c) data customized by _AlchemistPrompts_, and (d) data of the code comprehension tasks (_i.e._, instruction evolution, data filtering, and code review).

We visualize the distributions of data sources and programming languages using two circular graphs in Fig. 5. Concurrently, Fig. 6 reports a distribution of text description lengths and code lines. Compared to CodeAlpaca  and OOS-INSTRUCT , our _AlchemistCoder_ dataset presents a notably diverse distribution and maintains moderate overall text description and code lengths, benefiting significantly from the integration of multi-source data along with _AlchemistPrompts_ and code comprehension tasks. This is instrumental in contributing to a comprehensive evolution of code capability.

## 3 Experiments

In this section, we report results on various benchmarks of code generation and conduct ablation experiments. Furthermore, we present analytical studies to provide a more in-depth demonstration of the efficacy of our _AlchemistCoder_.

### Benchmarks and implementation details

**Benchmarks.** We adopt six code benchmarks: HumanEval , HumanEval+ , HumanEval-X , MBPP , MBPP+ , and DS-1000 . In addition, we access three mainstream benchmarks (MMLU , BBH , and GSM8K ) to evaluate generalization abilities. All evaluation and benchmark details can be found in Appendix SSD.

**Baselines.** We compare with the following competitive baselines. Closed-Source Models: GPT-3.5-Turbo  and GPT-4-Turbo . Open-Source Models: Llama 2 , CodeLlama , StarCoder , WizardCoder , DeepSeek-Coder , and Magicoder .

**Supervised fine-tuning.** We adopt Llama-2-7B, CodeLlama-Python-7B, and DeepSeek-Coder-Base-6.7B as the base models and fine-tune all the base models for 2 epochs using 32 NVIDIA A100-80GB GPUs. We set the initial learning rate, minimum learning rate, and optimizer warmup steps and at 1e-4, 6e-6, and 15, respectively. We use Adam optimizer  and choose a batch size of 2 with a sequence length of 8192.

### Evaluation on code generation task

**Results on python code generation.** We first access HumanEval and MBPP to evaluate the capability of the _AlchemistCoder_ series for Python code generation. These benchmarks necessitate models to generate code based on the function definitions and subsequently pass the test cases. Models are evaluated in zero-shot on HumanEval and 3-shot on MBPP. The comprehensive comparisons in Tab. 1 and Fig. 1 demonstrate the impressive capabilities of _AlchemistCoder_ models. From the results, _AlchemistCoder-L_ attains a remarkable performance boost of 42.7% and 28.4% pass@1 scores on HumanEval and MBPP respectively, compared to Llama 2-7B. Notably, _AlchemistCoder-DS_ elevates the pass@1 scores to 79.9% and 77.0% on these benchmarks, holding an overall improvement of 33.3%. Moreover, our _AlchemistCoder_ series with 7B parameters outperforms larger models (_e.g._,WizardCoder-CL-34B and CodeLlama-Instruct-70B) and rivals with GPT-3.5-Turbo, significantly bridging the performance gap between closed-source and open-source models.

**Results on multilingual code generation.** We compare the pass@1 accuracy of the base models and the corresponding fine-tuned _AlchemistCoder_ models on Humaneval-X . The results presented in Tab. 2 demonstrate that the _AlchemistCoder_ series exhibits great improvements (exceeding 50%) for multilingual code generation, delivering comprehensive code capabilities.

**Results on code generation for data science.** We further conduct the evaluation of data science code completion on DS-1000 . According to Tab. 3, _AlchemistCoder_ models exhibit a notable improvement of up to 19.2% in overall performance compared to the base models. Particularly,

  
**Model** & **Python** & **C++** & **Go** & **Java** & **JS** & **Avg** \\    & 14.0 & 11.0 & 6.1 & 11.0 & 14.0 & 11.2 \\  & 31.7 & 27.4 & 12.8 & 25.6 & 32.9 & 26.1 \\  & **4chemistCoder-L** & **56.7** & **31.1** & **25.6** & **45.1** & **41.5** & **37.1** \\   & 37.8 & 33.5 & 30.5 & 36.4 & 35.4 & 35.4 \\  & 68.3 & 47.6 & 36.9 & 34.8 & **57.9** & 49.6 \\  & **74.4** & **53.1** & **42.7** & **64.0** & 52.4 & **57.3** \\   & 47.6 & 45.1 & 38.4 & 56.1 & 43.9 & 46.2 \\  & 72.6 & **63.4** & 51.8 & 70.7 & 66.5 & 65.0 \\   & **79.9** & 62.2 & **59.8** & **72.0** & **68.9** & **68.6** \\   

Table 2: Results of pass@1 on HumanEval-X. Table 3: Pass@1 results of models with 6.7B/7B We present the multilingual code capabilities of our _AlchemistCoder_ with the respective base models and competitors (6.7B/7B).

  
**Model** & **Params** & **Base Model** & **HumanEval (+)** & **MBPP (+)** & **Average (+)** \\    \\  GPT-3.5-Turbo  & - & - & 72.6 (65.9) & 81.7 (69.4) & 77.2 (67.7) \\ GPT-4:Turbo  & - & - & **85.4 (81.7)** & **83.0 (70.7)** & **84.2 (76.2)** \\   \\  Llama 2-Chat  & 70B & Llama 2 & 31.7 (26.2) & 52.1 (38.6) & 41.9 (32.4) \\ CodeLlama-Python  & 70B & Llama 2 & 57.9 (50.0) & 72.4 (52.4) & 65.2 (51.2) \\ CodeLlama-Instruct  & 70B & CodeLlama & **65.2 (58.5)** & **73.5 (55.1)** & **69.4 (56.8)** \\   & 34B & Llama 2 & 51.8 (43.9) & 67.2 (50.4) & 59.5 (47.2) \\ WizardCoder-CL  & 34B & CodeLlama-Python & 73.2 (56.7) & 73.2 (51.9) & 73.2 (54.3) \\ DeepSeek-Coder-Instr  & 33B & DeepSeek-Coder-Base & **78.7 (67.7)** & **78.7 (59.7)** & **78.7 (63.7)** \\   & 15B & - & 34.1 (33.5) & 55.1 (43.4) & 44.6 (38.5) \\ CodeLlama-Python  & 13B & Llama 2 & 42.7 (36.6) & 61.2 (**45.6)** & 52.0 (41.1) \\ WizardCoder-SC  & 15B & StarCoder & **51.9 (45.7)** & **61.9 (44.9)** & **56.9 (45.3)** \\  Llama 2  & 7B & - & 14.0 (10.4) & 26.1 (17.5) & 20.1 (14.0) \\ StarCoder  & 7B & - & 24.4 (21.3) & 33.1 (29.2) & 28.8 (25.3) \\ CodeLlama-Python  & 7B & Llama 2 & 37.8 (33.5) & 57.6 (42.4) & 47.7 (38.0) \\ WizardCoder-CL  & 7B & CodeLlama-Python & 48.2 (42.1) & 56.6 (42.4) & 52.4 (42.3) \\ DeepSeek-Coder-Base  & 6.7B & - & 47.6 (41.5) & 70.2 (53.6) & 58.9 (47.6) \\ Magicoder-CL  & 7B & CodeLlama-Python & 60.4 (49.4) & 64.2 (46.1) & 62.3 (47.8) \\ Magicoder-CL  & 7B & CodeLlama-Python & 70.7 (60.4) & 68.4 (49.1) & 69.6 (54.8) \\ Magicoder-DS  & 6.7B & DeepSeek-Coder-Base & 66.5 (55.5) & 74.4 (55.6) & 71.0 (55.6) \\ DeepSeek-Coder-Instruct  & 6.7B & DeepSeek-Coder-Base & 73.8 (69.5) & 72.7 (55.6) & 73.3 (62.6) \\ MagicoderS-DS  & 6.7B & DeepSeek-Coder-Base & 76.8 (65.2) & 75.7 (56.1) & 76.3 (60.7) \\   & 7B & Llama 2 & 56.7 (52.4) & 54.5 (49.6) & 55.6 (51.0) \\  & 7B & CodeLlama-Python & 74.4 (68.3) & 68.5 (55.1) & 71.5 (61.7) \\  & 6.7B & DeepSeek-Coder-Base & **79.9 (75.6)** & **77.0 (60.2)** & **78.5 (67.9)** \\   

Table 1: Results of pass@1 on HumanEval (HumanEval+) and MBPP (MBPP+) benchmarks. We report the results of HumanEval and MBPP consistently from the EvalPlus  and the **bold** scores denote the best performance among models of the same size.

_AlchemistCoder-CL_ achieves an astonishing overall accuracy of 40.3% with relatively better performance in all libraries, demonstrating powerful capabilities in data science workflows.

### Ablation study

**The Recipe of _AlchemistPrompts_.** As illustrated in Sec. 2.2, _AlchemistPrompts_ can further align the instructions and responses of data samples and harmonize the domain gap between multiple sources. Code data from different sources may vary significantly in language style and content, including question types, code style, presence of comments, test cases, etc. Therefore, multi-source data mixing is a double-edged sword: it provides necessary diversity but can also bring large domain gaps. Adding concise corpus generated from the same Alchemist model (_i.e._, _AlchemistPrompts_ with similar language styles) to a small amount of data can effectively bridge this gap while maintaining diversity. To find the appropriate recipe of _AlchemistPrompts_ that maintains a balance between data diversity and domain gap, we conduct ablation experiments on the proportion (0% to 20%) of data customized by _AlchemistPrompts_. We adopt two settings: (a) augment the original data with its customized variant and report the results of fine-tuning for 2 epochs on CodeLlama-Python-7B; (b) replace the original data and report the results of fine-tuning for the same steps (_i.e._, keeping the number of tokens used consistent). As shown in Fig. 7, _AlchemistCoder_ is particularly enhanced when the proportion of customized data increases from 1% to 5%, and nearly peaks in performance at 5%. Thus, we introduce _AlchemistPrompts_ into 5% of the training set to balance the performance gain and the generation cost. Additionally, both two strategies effectively enhance the performance and validate the efficacy of our approach. To push the limit of _AlchemistCoder_, we employ the augmentation strategy in our performance experiments. In addition, we present detailed experimental results from the multi-source integration and harmonization process in our Appendix SSC to offer a more in-depth demonstration of the _AlchemistPrompts_ efficacy as data complexity scales.

**Efficacy of the code comprehension tasks.** We conduct an ablation study on the key components of the code comprehension tasks to ascertain their individual contributions to the overall performance. As reported in Tab. 4, compared to the baselines (the first and second rows), the model demonstrates

  
**Multi-source** & **Data** & _AlchemistPrompt_ & **Instruction** & **Data** & **Code** & **HumanEval** & **MBPP** \\
**Integration** & **Decontamination** & **Harmonization** & **Evolution Task** & **Filtering Task** & **Review Task** & **(Pass@1)** & **(Pass@1)** \\  - & - & - & - & - & - & - & 37.8 & 57.6 \\ ✓ & - & - & - & - & - & 54.6 (16.8\(\)) & 57.9 (0.3\(\)) \\ ✓ & ✓ & - & - & - & - & 59.8 (5.2\(\)) & 58.2 (0.3\(\)) \\ ✓ & ✓ & ✓ & - & - & - & 72.0 (12.2\(\)) & 63.4 (5.2\(\)) \\ ✓ & ✓ & ✓ & ✓ & - & - & 71.3 (0.7\(\)) & 65.8 (2.4\(\)) \\ ✓ & ✓ & ✓ & ✓ & ✓ & - & 73.8 (2.5\(\)) & 67.7 (1.9\(\)) \\ ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & **74.4** (**0.6\(\)) & **68.5** (0.8\(\)) \\   

Table 4: Ablation study on the effectiveness of multi-source harmonization (_i.e._, Multi-source Integration, Data Decontamination, and _AlchemistPrompt_ Harmonization) and code understanding tasks (_i.e._, Instruction Evolution Task, Data Filtering Task, and Code Review Task) for the _AlchemistCoder-CL-7B_ model, evaluated on the HumanEval and MBPP benchmarks.

Figure 7: Ablation study on the proportion of _AlchemistPrompt_-customized data conducted on _AlchemistCoder-CL-7B_. Left: Augment the original data. Right: Replace the original data.

enhanced performance on both benchmarks following the incremental incorporation of code comprehension task data. Notably, the improvement (5.1% regard to the pass@1 metric) is particularly remarkable on MBPP. This indicates the significant contribution of all code comprehension tasks to furthering programming capabilities.

### Analytical study

_AlchemistPrompts_** harmonize the discrepancy between instructions and responses.** To in-depth verify the efficacy of _AlchemistPrompts_, we calculate the Conditional Perplexity Discrepancy (CPD, refer to Sec. 2.2) values of our fine-tuning data harmonized by _AlchemistPrompts_, _i.e._, the difference between \((+)\) and \(()\). The CPD value quantifies the difficulty change in generating responses before and after adding specific inputs (_e.g._, instructions) to the model (the smaller the value, the easier it becomes). Specifically, we adopt the instructions before and after customization by _AlchemistPrompts_ for comparison, and provide the Kernel Density Estimation of CPD in Fig. 8. Clearly, the latter (green) gains smaller overall CPD values, indicating that _AlchemistPrompts_ are beneficial for prediction and can provide effective contextual information. Furthermore, we randomly select 10 groups of these samples and use UMAP  to map their feature representations into a 2-D space in the right of Fig. 8. From the fact that the solid lines are generally shorter than the dashed lines, our _AlchemistPrompts_ can harmonize the discrepancy between instructions and responses, leading to higher-quality data for attaining improved instruction-following ability.

_AlchemistCoder_** models are better generalists.** To further analyze the comprehensive capabilities of our _AlchemistCoder_, we conduct evaluations on more diversified benchmarks, including MMLU  for multitask language understanding, BBH  for comprehensive reasoning, and GSM8K  for mathematical ability. The results are presented in Tab. 5 and illustrate that the _AlchemistCoder_ models exhibit an overall performance increase of 6.4%, 13.6%, and 14.5% over the base models Llama 2, CodeLlama-Python, and DeepSeek-Coder-Base, respectively. Notably, CodeLlama-Python presents inferior performance on these benchmarks relative to Llama 2, indicating the discrepancy between natural language processing and code capabilities of open-source models. Such divergence can be attributed to "catastrophic forgetting" [11; 29; 20], often occurring when fine-tuning is exclusively concentrated on domain-specific data. By leveraging harmonized multi-source data, our _AlchemistCoder_ series models achieve enhanced reasoning abilities, better instruction-following abilities, and improved context understanding, which contribute to develop better generalists.

**Error case analysis.** To meticulously dissect the improvements brought by our method, we provide an analysis of error cases on HumanEval and MBPP. We compare models before and after the introduction of _AlchemistPrompts_ and code understanding task data. The bar chart shown in Fig. 9 (top) indicates that these two types of key data help to better handle compilation errors (_i.e._, SyntaxError, NameError, and ValueError), and eliminate the occurrence of no code written in the responses. On the other hand, the results of Fig. 9 (bottom) on MBPP suggest that the _AlchemistCoder_ series incorporated with these two types of data attains stronger programming logic, as evidenced by the clear reduction in the 'Wrong Answer' error cases.

Figure 8: In-depth analysis of the efficacy from _AlchemistPrompts_. Left: Kernel Density Estimation of Conditional Perplexity Discrepancy. Right: UMAP visualization of 10 instruction/response groups.

## 4 Related Work

**Code large language models.** Early researches [5; 32; 22] focus on collecting massive amounts of code data to develop pretrained Code LLMs. Recent efforts [30; 47; 44] are dedicated to fine-tuning these pretrained models with specific instructional data to further the coding abilities. For instance, WizardCoder  and Magicoder  construct their instruction tuning datasets based on CodeAlpaca  and the stack  dataset, respectively. In this work, we develop the _AlchemistCoder_ series by instruction tuning on optimized multi-source data instead of single-category data as in previous methods, endowing astonishing and harmonized code capability.

**Instruction tuning.** Instruction tuning aims to enhance LLMs via fine-tuning pre-trained LLMs using samples of instruction/response pairs. Obtaining high-quality data for instruction tuning is typically challenging and extensive works have been dedicated to this endeavor. For instance, Alpaca  employs self-instruct  to generate instruction-following demonstrations. WizardLM  introduces Evol-Instruct and transforms the instruction data into more complex variants. In addition to Evol-Instruct, we also incorporate the data construction process itself as a form of data into the training. Moreover, although previous works [16; 43; 23; 16] utilize multiple fine-tuning datasets, we harmonize multi-source data at a fine-grained level.

**Learning from hindsight.** The concept of learning from hindsight  has been explored in goal-conditioned learning [17; 12]. Hindsight Experience Replay (HER)  is designed to re-label rewards and facilitate learning from sparse feedback. Korbak _et al._ study the influence of human preferences during pre-training, showing improved performance when models are aligned with human preferences. Previous work primarily serves as an alternative to RLFT, utilizing HER to leverage (suboptimal) historical data for model learning. We focus on harmonizing the inherent conflicts within multi-source data through hindsight, to fully tap into the potential of base models.

## 5 Conclusion

In this paper, we propose an effective framework for integrating multi-source data to fine-tune Code LLMs, addressing the limitations in quality and diversity inherent within a single-source dataset. This is a non-trivial paradigm and we pioneer to unveil inherent conflicts in multi-source code corpora. To resolve this challenge, we innovatively design data-specific _AlchemistPrompts_, inspired by hindsight relabeling. Additionally, we make the first effort of integrating the data construction process as code comprehension tasks into the training process. These key concepts enhance the diversity, quality, and cost-effectiveness of code fine-tuning data, facilitating the development of the _AlchemistCoder_ series models with significantly improved and comprehensive coding capabilities.

## 6 Acknowledgments

This work is supported by National Natural Science Fund of China (62076184, 62473286) in part by Shanghai Natural Science Foundation (22ZR1466700). Besides, this project is funded in part by the Hong Kong Generative AI Research and Development Center (HKGAI) under the Innovation and Technology Commission (ITC)'s InnoHK. Dahua Lin is a PI of HKGAI under the InnoHK.

  
**Model** & **MMLU** & **BBH** & **GSM8K** & **Avg** \\  Llama 2 & 41.1 & 34.6 & 16.8 & 30.8 \\ Code.lama & 31.5 & **42.7** & 14.4 & 29.5 \\ _AlchemistCoder-L_ & **43.9** & **42.7** & **25.0** & **37.2** \\  Cold.lama-Python & 26.1 & 26.7 & 6.6 & 19.8 \\ Magicoder-CL & 33.0 & **41.5** & 18.8 & 31.1 \\ _AlchemistCoder-CL_ & **42.1** & 39.3 & **20.2** & **33.9** \\  DeepSeek-Coder-Base & 34.0 & 12.8 & 22.0 & 22.9 \\ Magicoders-DS & 34.4 & 43.8 & 14.3 & 30.8 \\ _AlchemistCoder-DS_ & **38.5** & **45.6** & **28.0** & **37.4** \\   

Table 5: Results of models (6.7B/7B) on various benchmarks, including MMLU for multitask language understanding, BBH for comprehensive reasoning, and GSM8K for mathematical ability.

Figure 9: Analysis of error case proportions on HumanEval (top) and MBPP (bottom). \({}^{}\) represents the models fine-tuned without _AlchemistPrompts_ and the code comprehension task data.