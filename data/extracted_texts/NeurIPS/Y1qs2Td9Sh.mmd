# Waste Not, Want Not;

Recycled Gumbel Noise Improves Consistency in

Natural Language Generation

 Damien de Mijolla, Hannan Saddiq, Kim Moore

Faculty Science Ltd

damien.de-mijolla@faculty.ai

###### Abstract

Consistency in the output of language models is critical for their reliability and practical utility. Due to their training objective, language models learn to model the full space of possible continuations, leading to outputs that can vary significantly in style, content, and tone, even for similar inputs. To address this, we propose a novel decoding algorithm that enhances response consistency across different prompts with no degradation in response quality. By incorporating a latent variable into the next-token sampling process based on the Gumbel reparametrisation trick, our method outperforms standard sampling by up to 10% across semantic and stylistic consistency benchmarks. Additionally, our approach integrates seamlessly with existing sampling methods with negligible computational overhead, providing a practical solution for improving the reliability of language model outputs.

## 1 Introduction

In recent years, state-of-the-art language models (LMs) have demonstrated remarkable performance across a wide range of benchmarks, often rivaling human capabilities in tasks such as translation, summarization, and question-answering [1; 2]. However, these advancements have not always translated into practical usefulness for real-world applications, where reliability and consistency are crucial .

One of the primary challenges is the inconsistency of these models' responses, which can vary significantly in style, factual accuracy, and tone . This inconsistency, a byproduct of the probabilistic nature of language model training, can lead to a range of issues, including reduced trust in outputs, exposure to more diverse failure modes and less reliable behaviour .

Although traditional methods (e.g the use of random seeds) can be applied to introduce determinism in natural language generation, ensuring identical responses for identical inputs, they do not help ensure similar responses when inputs are similar. In practice, due to the richness of language, input queries can often be reworded in many ways while retaining their meaning. To achieve greater consistency, it is desirable for the model to generate similar responses across all these variations.

In this paper, we investigate whether next-token sampling procedures can be modified to enhance consistency across different prompts. Our main contributions include:

1. We propose a simple, computationally inexpensive sampling procedure that (i) can be applied to any model, (ii) does not require any additional training, and (iii) has negligible impact on inference costs. We also ensure that the probability of any individual response is unchanged and so does not compromise response quality.

2. We also leverage an auxiliary approach to further improve consistency between model responses using distributional ensembling, which can be applied in conjunction with our aforementioned sampling procedure.
3. We investigate the performance of our approach against standard sampling across a number of benchmarks covering semantic and stylistic similarity, across a number of different models.

In particular, we highlight that our combined sampler outperforms standard sampling across all benchmark suites and models tested, by up to 10% in some cases.

## 2 Related works

Decoding approachesLanguage model decoding strategies can be broadly classified into two categories: optimization and sampling-based approaches . Optimization-based approaches, such as greedy decoding and beam search [7; 8], frame text generation as an optimization problem, searching for sequences that maximize a specific metric such as probability, whereas sampling-based approach incorporate stochasticity into the next-token selection process. Optimization-based approaches are typically perceived as yielding less engaging but more accurate responses and so are often favoured for closed-ended tasks expecting a fixed answer . However, recent work has put into question the greater accuracy of their responses .

In contrast, sampling-based approaches are usually preferred for open-ended tasks, as they typically yield more engaging answers [11; 6]. Our proposed method falls within this category. Many existing methods in the literature, such as nucleus sampling and mirostat [9; 11; 12], aim to improve text generation quality by directly modifying the probability distribution from which tokens are sampled. We consider these methods, which directly alter the next-token distribution, as complementary to our approach, which maintains the next-token distribution and instead modifies the joint distribution over responses.

Our approach is methodologically most closely related to methods [13; 14] which also adjust the joint distribution of sampled responses. However, while these methods aim to maximize response diversity--an advantage when ensembling multiple responses as done in self-consistency voting --our approach is distinct in its focus on minimizing response diversity to achieve more consistent outputs.

Self-ConsistencyLanguage models lack robustness to prompt variations [16; 17] and give contradictory responses in such cases, motivating the need for enhanced self-consistency. Self-consistency in language models has been studied from many different angles, but usually with a focus on factual rather than stylistic consistency. Prior work has proposed a number of fine-tuning approaches for increasing self-consistency, including fine-tuning approaches for increasing the ability of language models to respond consistently to paraphrases of questions [17; 18], and approaches for correcting model contradictions using a factor graph over beliefs .

Our approach is methodologically orthogonal to previous approaches for enhancing self-consistency. Previous work has relied on fine-tuning which not only is more cumbersome to implement but also modifies the raw next-token probabilities, potentially affecting responses in unforeseen ways or contributing to catastrophic forgetting.

Since our approach only modifies the joint distribution over responses without modifying the next-token probability distribution, it does not suffer from the same issues, and comes with principled guarantees around maintaining the model's original response style and quality. Additionally, it enhances _all_ aspects of self-consistency, not just factual consistency of responses.

## 3 Problem statement

Let \(X\) be a language model prompt composed of a sequence of tokens drawn from a vocabulary of size \(N_{v}\), and let \(_{}\) be a language model trained on the task of next-token-prediction. For the remainder of the paper we denote a forward pass through the language model by \(h_{t}=_{}(X,Y_{1:t-1})\) where \(h_{t}^{N_{v}-1}\) is a probability distribution over the token vocabulary and \(Y_{1:T}\) is the full response obtained by auto-regressively applying the language model with the next token at each step sampledfrom the categorical distribution parameterized by the model, \(Y_{t}(h_{t})\). In what follows, we use a subscript to represent position in a sequence, and a superscript to represent the token index. So for example, \(h_{t}^{i}\) represents the probability of sampling token \(i\) at position \(t\).

Suppose that \(U\) is a different prompt that is semantically similar to \(X\) for which we generate a response \(V=V_{1:M}\). Motivated by the inconsistency of LM responses, our goal is to modify the LM sampling procedure in a way that increases the similarity between responses \(Y\) and \(V\) according to some yet-to-be-specified notion of similarity. Furthermore, we focus on sampling approaches that modify the joint probability of responses \(p(Y,V)\) without affecting the marginal probability of individual responses, \(p(Y)\) and \(p(V)\), to guarantee that quality of the original responses is maintained.

## 4 Approach

Our proposed sampling approach, motivated in Figure 1, modifies the joint probability distribution over responses by introducing a latent variable \(g\) to the sampling process. Conditioning the generation of distinct responses on a common realisation of this latent variable introduces a statistical dependency between them. Generating responses with greater similarity can then be straightforwardly done by conditioning the generation of all responses on a common realisation of the latent variable, that is to say to sample \(Y p(Y|X,g)\) and \(V p(V|U,g)\).

To ensure efficacity of the approach, we design the latent variable in such a way that conditioning responses on a common value of the latent variable makes responses as similar as possible. To ensure the preservation of the probability distribution parameterized by the language model, we sample the latent variable from a probability distribution \(g p(g)\) such that marginalising over the latent variable recovers the original distribution over responses, \(_{g}[p(Y|g)]=p(Y)\).

To construct a latent variable with the above properties, we employ the reparametrization trick for categorical distributions. Introduced for normal distributions in  and extended to categorical distributions in [21; 22; 23], the reparametrization trick is a procedure that refactors the sampling from a distribution into a deterministic function of the parameters and a draw from some independent noise with a fixed distribution. For a categorical distribution with parameters \(p^{1},...,p^{N_{v}}\), this can be cast as first drawing random noise \(g=(g^{1},...,g^{N_{v}})\) where each \(g^{i} G(0,1)\) is independently drawn from the Gumbel distribution  and selecting a category \(k\) according to \(k=_{i}( p^{i}+g^{i})\)

Figure 1: Motivating toy example highlighting the aim of our approach. Even when language models yield similar probability distributions over responses, responses sampled independently can be inconsistent or contradictory due to the inherent stochasticity of sampling. By generating responses in a correlated manner it is possible to alleviate inconsistencies across responses while still respecting the marginal probabilities of each response. In this paper we propose, Gumbel Consistent Sampling, an approach for increasing response consistency through drawing correlated responses, by conditioning all responses on a shared latent variable, that is robust to differences between probability distributions over responses.

**Theorem 4.1**.: _Suppose we have two different categorical distributions parametrized by \(p^{1},...,p^{N_{v}}\) and \(q^{1},...,q^{N_{v}}\). Define a joint distribution over pairs of categories \((Y,V)\) by defining_

\[Y=*{arg\,max}_{i}( p^{i}+g^{i}),\,V=*{arg\,max}_ {i}( q^{i}+g^{i}),\] (1)

_where \(g^{1},...,g^{N_{v}} G(0,1)\) are independent. We have that_

\[P(Y=k,V=k)=q^{k}}{p^{k}q^{k}+_{i k}\{p^{i}q^{k},q^{i}p^{ k}\}}.\]

Theorem 4.1 (proved in Appendix A) shows that interpreting the Gumbel noise as a latent variable and conditioning sampling events on the same realisation of this latent variable increases the probability of selecting the same category with both distributions compared to sampling from each categorical distribution independently, with identical sampling outcomes in the limit where \(p\) and \(q\) become identical.

Since generating a response using a LM consists of successive draws from categorical distributions, the above idea can be applied to language modelling in order to increase the token overlap across distinct responses. Indeed, we can generate ahead of time a sequence of independent Gumbel latent vectors, \(g_{1:t}\), one for each position in the sequence up to the maximum sequence length, and sample each token using the Gumbel latent vector assigned to that position in the sequence when generating a response. That is to say, drawing \(Y_{t} p(Y_{t}|h_{y,t},g_{t})\) and \(V_{t} p(V_{t}|h_{v,t},g_{t})\), where here we denote by \(h_{y,t}\) and \(h_{v,t}\), the next-token probabilities obtained by running the language model on the context-up-to-now (i.e. \(h_{y,t}=_{}(X,Y_{1:t-1})\), \(h_{v,t}=_{}(U,V_{1:t-1})\) ). We refer to the above approach as **Gumbel Consistency Sampling, GCS**.

This sequential Gumbel sampling approach increases similarity of responses by increasing the rate at which identical tokens are generated at fixed positions in the sequence \(p(Y_{i}=k,V_{i}=k)\) but has the limitation of not increasing the co-occurrence across sequence positions \(p(Y_{j}=k,V_{i}=k)\). We expect that two similar responses are likely to contain some of the same tokens, but likely in different positions, so it would be advantageous for our final sampling approach to reflect this.

Introducing such an inter-position correlation in sampling outcomes across sequences is made challenging by the requirement of conditional independence between sampling steps. Indeed, to respect the LM's probability distribution, it is necessary for sequential sampling steps to be independent of each other, i.e. for \(p(Y_{t+1}|X,Y_{1:t})=p(Y_{t+1}|h_{t+1})=(Y_{t+1};h_{t+1})\) which prevents the direct reuse of Gumbel samples across sequence positions.

The procedure denoted in Algorithm 1, which we henceforth refer to as **Gumbel Consistency Sampling with Recycling, (GCSwR)**, respects this property and thus recovers the correct marginal distribution over responses, which we prove in Appendix B.1.

```
0: Context \(X\), sequence length \(T\), language model parameters \(\)
0: Generated token sequence \(Y_{1:T}\)
1: Initialize \(g G(0,1)^{N_{} T}\) and \(c=[0,0,,0]^{N_{}}\)
2:for\(t=1\) to \(T\)do
3:\(h_{t}_{}(X,Y_{1:t-1})\)
4:\(k*{arg\,max}_{j}(g^{j}_{c_{j}}+ h^{j}_{t})\)
5:\(Y_{t} k\)
6:\(c_{k} c_{k}+1\)
7:for each \(i k\)do
8:\(g^{i}_{c_{i}} Q((g^{i}_{c_{i}})}{Q^{-1}(g^{b}_ {c_{k}}+ h^{k}_{t}- h^{i}_{t})})\)\(\{Q()\): Gumbel quantile function\(\}\)
9:endfor
10:endfor
11:return\(Y_{1:T}\) ```

**Algorithm 1** Gumbel Consistency Sampling with Recycling (GCSwR)

This approach relies on the observation that awareness of the value of a token \(Y_{t}\) only reveals the value of \(*{arg\,max}_{j}( h^{j}_{t}+g^{j}_{t})\) rather than fully revealing \(g_{t}\). From one sequence position to thenext, the recycling procedure involves resampling a new Gumbel noise value for this position in the Gumbel latent vector, but recycling (a rescaled version of) the existing Gumbel values for every other position in the vector. In practice, we generate all Gumbel samples for the resampling ahead of time so that we may use the same sequence of Gumbel vectors independent of each other for each response.

The standard procedure for autoregressive token sampling, which is equivalent to independent sampling of a new Gumbel latent vector for every sequence position and every sequence, acts as a baseline for subsequent experiments, and is denoted as **Independent Sampling, (IS)**.

## 5 Ensembling semantically similar responses

A complementary approach to enhance consistency between responses given semantically similar prompts is to reduce the impact of semantically irrelevant prompt attributes on the next-token probability distributions, which can be achieved by increasing the similarity between the sampling distributions.

In our experiments, we explore sampling tokens from an ensembled probability distribution over semantically equivalent prompts as a means of minimising impact of semantically irrelevant prompt variations on responses. Specifically, we generate semantically equivalent variations of the user prompt by asking a separate LM (gpt-40 mini) to rephrase the prompt. We then run the target LM separately on all of the prompts, producing a set \(\{P_{i}\}\) of next-token probability distributions. We then sample from an ensembled distribution, ensembled using the following formula:

\[Q^{j}=_{i=1}^{n}(P_{i}^{j})^{}\] (2)

where Z is the normalisation constant that ensures \(Q\) defines a valid probability distribution function:

\[Z=_{j}_{i=1}^{n}(P_{i}^{j})^{}\]

This formula corresponds to selecting the categorical distribution that minimizes the average forward-KL divergence over all next-token probability distributions (see Appendix D). We found that direct averaging (which can equivalently be shown to minimize the reverse-KL distribution) tended to generate worse-quality responses due to at times sampling tokens that were only high-probability for a subset of question rewordings.

Note that, contrary to our proposed Gumbel sampling approach, ensembling comes at a cost of additional inference-time compute and also modifies the language model probability distributions. We highlight that ensembling can be applied in conjunction with any of the three samplers discussed in section 4, and we investigate the performance of each sampler with and without ensembling in our experiments.

## 6 Experiments

In our experiments, we empirically demonstrate the utility and limitations of GCS and GCSwR. We begin by quantifying the utility of the procedure for enhancing semantic similarity of responses, and highlight a number of stylistic dimensions of text along which Gumbel sampling improves consistency. Details for reproducing experiments are shown in Appendix E.

### Semantic similarity

We start by quantifying the improvement in the semantic similarity between responses for semantically equivalent queries by using our Gumbel sampling variants (GCS and GCSwR). To measure semantic similarity, we use E5\({}_{}\), a specialised state-of-the-art model trained specifically on the task of semantic similarity .

We create semantically equivalent pairs of questions for evaluation by randomly sampling 300 questions from the Alpaca dataset  -- a popular human-preference dataset - and rephrasingthem using gpt-4o mini. We then generate responses to the original and rephrased version of each question using Meta-Llama-3-8B-Instruct, Meta-Llama-3-8B, Mistral-7B-v0.1, Llama-2-7b-chat-hf[27; 28; 29]. In all cases we sample from the raw unmodified next-token probabilities predicted by the language models (i.e. temperature of 1) and for Gumbel sampling, we resample the Gumbel latent vector for each pair of questions such that responses are correlated within but not between pairs.

The aggregated results, shown in Table 1, demonstrate that the most performant sampling scheme tested (GCSwR with ensembling) significantly increases response similarity to semantically equivalent questions across all models considered, by more than 10% when compared to the baseline in some cases. We note more pronounced enhancements from Gumbel sampling for unaligned models like Mistral and Llama3 Base, which we hypothesise is caused by their lower base semantic similarity compared to their instruction fine-tuned counterparts.

### Semantic similarity as a function of temperature

Next, we investigate how the effectiveness of GCSwR varies with sampling temperature. We compare the semantic similarity metric on the Alpaca dataset as a function of temperature in Figure 2 with IS as a baseline, without using ensembling in both cases. GCSwR improves the semantic consistency of responses across all temperatures, except temperature 0, where the model probabilities with and without GWSwR become identical due to the fully deterministic nature of model outputs at this temperature1 Example responses for Llama3 models at temperature 0.8 can be found in Appendix G.

It is also interesting to note that although GCSwR improves self-consistency at all non-zero temperatures, the highest self-consistency achieved is with greedy decoding (i.e. temperature 0) which is where both approaches behave identically. However, we caution that this result does not imply that greedy decoding will always be preferable to higher-temperature Gumbel sampling. Using greedy decoding is widely considered to decrease the quality of responses across a number of important dimensions and so model providers typically use non-zero default temperatures [11; 6; 30]. Gumbel sampling offers a way of increasing the consistency of responses without the negative side-effects associated with excessively lowering the sampling temperature. We also note that using Gumbel sampling is much more effective at increasing self-consistency of responses than decreasing temperature, with temperatures needing to be roughly halved in order to match the benefits of using Gumbel consistency sampling.

  
**Model** & **Sampler** & **Without Ensembling** & **With Ensembling** \\   & IS & 86.34\(\)0.07 & 87.56\(\)0.29 \\  & GCS & 88.28\(\)0.10 & 90.26\(\)0.27 \\  & GCSwR & **88.61\(\)0.15** & **90.38\(\)0.25** \\   & IS & 72.00\(\)0.27 & 72.34\(\)0.93 \\  & GCS & 78.55\(\)0.22 & 81.17\(\)0.77 \\  & GCSwR & **80.94\(\)1.05** & **82.74\(\)0.81** \\   & IS & 85.61\(\)0.18 & 86.90\(\)0.16 \\  & GCS & 86.81\(\)0.46 & 89.01\(\)0.35 \\  & GCSwR & **87.37\(\)0.27** & **89.68\(\)0.08** \\   & IS & 71.23\(\)0.41 & 71.46\(\)0.70 \\  & GCS & 76.68\(\)0.80 & 78.71\(\)0.82 \\   & GCSwR & **80.10\(\)0.80** & **82.04\(\)0.81** \\   

Table 1: Model results by sampler type. Scores shown as mean\(\)std.err with std.err obtained from 3 independent runs. Bold indicates highest scores for each model in both ensembling categories.

### Stylistic similarity

In this section, we study Gumbel consistency sampling's ability to enhance stylistic consistency across several distinct stylistic dimensions, evaluating GCSwR without ensembling using Mistral-7B-v0.1.

We conduct our experiments on two datasets: Code-Alpaca and Aleatoric-List. The Code-Alpaca dataset  consists of coding-related questions, from which we select a subset of 20 random questions that are agnostic to programming languages. For this dataset, we assess stylistic consistency based on several factors: whether the response contains a code snippet, whether the response starts directly with the code snippet or begins with freeform text, whether the code snippet includes comments, and the programming language used in the response (such as Python, JavaScript, or C++).

The second dataset, Aleatoric-List, is a synthetic dataset we created containing 20 questions that ask for five different items fitting a specific category. An illustrative example question is "Give me the names of five capital cities in Europe." For this dataset, we evaluate stylistic consistency based on whether the answer is terse, whether it contains bullet points, and whether these bullet points are numerical.

To evaluate stylistic consistency along each dimension, we begin by generating 100 Gumbel latent vectors. Then, for each Gumbel vector, we generate a response to all questions in the dataset which we classify along each of the stylistic dimensions through prompting gpt-40 mini (with prompts shown in Appendix F). For each factor, we then define the stylistic consistency as the probability that responses to two randomly selected questions share the same label, denoted as \(p_{repeat}\). We then compare this probability with the equivalent probability when the responses are generated with our independent sampling baseline (IS).

Let \(Z\) be a Bernoulli random variable that denotes whether a randomly sampled response is labelled with a given stylistic dimension, \(p(Z=1)=p\). For IS, \(p_{repeat}=p^{2}+(1-p)^{2}\). However, for GCS and GCSwR, \(p_{repeat}=_{g}[p_{q}^{2}+(1-p_{g})^{2}]\) where \(p_{g}\) denotes the probability of a randomly sampled response generated using Gumbel latent vector \(g\) taking value \(Z=1\). These expressions follow directly from the conditional independence of responses generated with a common initial Gumbel latent vector \(g\) and generated independently, and additionally from marginalisation over initial Gumbel latent vectors \(g\).

Figure 2: Mean semantic consistency between responses to paraphrased questions as a function of temperature, comparing independent sampling (IS) against GCSwR.

Although the estimator \(=_{i=1}^{n}Z_{i}\) is an unbiased estimator of \(p\), \(^{2}+(1-)^{2}\) yields a biased estimator of \(p^{2}+(1-p)^{2}\). To correct for this bias, we use the following estimator \((^{2}+(1-)^{2})-\) which we show in Appendix C to be unbiased.

We show, in Table 2, the results of this experiment, using Mistral-7B to generate responses. Across all stylistic dimensions considered, using GCSwR increases the frequency with which generated responses follow a common style. For many factors, the increase is significant (>10%), showing that Gumbel consistency sampling can have an appreciable impact on style consistency.

## 7 Conclusion

We have introduced Gumbel consistency sampling, a straightforward and computationally inexpensive sampling approach for increasing consistency amongst model responses. The method requires no additional fine-tuning, additional language model calls or apriori knowledge of what prompts will be used, and guarantees responses indistinguishable to those obtained using standard sampling at the level of individual responses. The approach enhances consistency by sampling responses in a correlated manner through the introduction of a latent variable, in a way that increases the token overlap across responses. In our experiments, we find that this approach is not only able to enhance semantic similarity between responses but also stylistic similarity. These results showcase how Gumbel consistency sampling offers a principled quick and easy way of enhancing language model consistency.

Future work could extend the Gumbel consistency sampling to imposing local rather than global correlation to responses. Currently, all responses are globally coupled due to dependence on the same global latent variable, which makes localised adjustments to model behaviour impossible. However, the framework could easily enable for latent variables to be varied locally depending on question specifics, which would enable finer-grain control of model behaviour. Another, promising direction for extending the work could be to treat the Gumbel noise as a learnable task-specific parameter. Such an approach may be especially useful for building stronger model safeguards while preserving general utility.