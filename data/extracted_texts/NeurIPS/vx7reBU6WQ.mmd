# Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models

Neel Jain\({}^{1}\), Aditya Shrivastava\({}^{2}\), Chenyang Zhu\({}^{2}\), Daben Liu\({}^{2}\), Alfy Samuel\({}^{2}\),

**Ashwinee Panda\({}^{1}\), Anoop Kumar\({}^{2}\), Micah Goldblum\({}^{3}\), Tom Goldstein\({}^{1}\)**

\({}^{1}\) University of Maryland, \({}^{2}\) Capital One, \({}^{3}\) New York University

Work completed during Capital One Internship; Correspondence to <njain17@umd.edu>

###### Abstract

A key component of building safe and reliable language models is enabling the models to appropriately refuse to follow certain instructions or answer certain questions. We may want models to output refusal messages for various categories of user queries, for example, ill-posed questions, instructions for committing illegal acts, or queries which require information past the model's knowledge horizon. Engineering models that refuse to answer such questions is complicated by the fact that an individual may want their model to exhibit varying levels of sensitivity for refusing queries of various categories, and different users may want different refusal rates. The current default approach involves training multiple models with varying proportions of refusal messages from each category to achieve the desired refusal rates, which is computationally expensive and may require training a new model to accommodate each user's desired preference over refusal rates. To address these challenges, we propose refusal tokens, one such token for each refusal category or a single refusal token, which are prepended to the model's responses during training. We then show how to increase or decrease the probability of generating the refusal token for each category during inference to steer the model's refusal behavior. Refusal tokens enable controlling a single model's refusal rates without the need of any further fine-tuning, but only by selectively intervening during generation.

## 1 Introduction

An essential property of a useful language model is the ability to produce _refusal messages_ at appropriate times. Refuses messages not only enhance the safety of LLMs, but also their utility and trustworthiness, as refusal messages can prevent LLMs from hallucinating or answering invalid requests. For example, an LLM that lacks the ability to browse the web should refuse when asked to access and summarize the content behind a URL. Likewise, a model should provide an informative refusal when asked to answer a question that is too under-specified or poorly formed to be answerable. To minimize hallucinations and unsafe behavior, instruction models like GPT-4 (Achiam et al., 2023) and Llama-3 (Dubey et al., 2024) have been processed with alignment pipelines that imbue them with extensive refusal capabilities.

Despite advancements in model finetuning and alignment, controlling refusal messages in these models remains a challenging task. For instance, Llama-2-Chat (Touvron et al., 2023) experienced issues with over-refusal, where the model would refuse too many queries, negatively impacting usability, mostly likely due to a post-training set with too many refusal messages. Simple approaches, such as training multiple models with varying levels of refusal data until the desired rates are achieved (Dubey et al., 2024) are resource-intensive and still lack the precision to carefully adjust different categories of refusals. Moreover, the criteria for refusal are constantly evolving. What is considered an acceptable refusal for one use case or time may not align with the ethical, legal, or technical standards in a different setting.

To address these weaknesses, we introduce a simple strategy that makes refusal behavior controllable at test-time without retraining: the refusal token. During alignment, we prepend a special[refuse] token to responses that contain a refusal. The model quickly learns to generate this token before refusing, and then to refuse when this token is present. At test-time, the softmax probability of the refusal token can be used as a metric for how likely it is that a refusal is necessary. By thresholding on this probability, one can turn a knob to control the refusal sensitivity after the model is trained. By employing different refusal tokens for different refusal types, one can impose fine-grained control over refusal behavior along different axes of behavior, and carefully optimize refusal rates in this multi-dimensional space.

Our main contributions are the following:

* We introduce a refusal token strategy. By thresholding the probability of this refusal token, we give model developers calibrated control over refusal rates without retraining. This development opens the door for sophisticated post-training calibration of refusal rates. For example, with minimal computation, one could sweep over refusal thresholds and select a value that achieves a specified rate of false refusals, or a value that maximizes an F1 score.
* We show that multiple refusal tokens can manage different refusal message sets, enabling independent control over each refusal distribution. Additionally, we explore various strategies for manipulating these category-specific refusal tokens to meet test-time requirements.

## 2 Related Work

**Refusal messages.** The ability of generative models to refuse certain messages is particularly crucial for mitigating toxicity and reducing hallucinations. In the context of toxicity, several studies explore how language models respond to toxic prompts or instructions. Arditi et al. (2024) find a one-dimensional subspace such that erasing this specific direction from the model's residual stream activations causes the model to consistently answer harmful queries. Bianchi et al. (2024) demonstrate that incorporating refusals into training data does not diminish a model's helpfulness but can lead to over-refusals, where the model declines to respond even on innocuous requests. Similarly, Cui et al. (2024); An et al. (2024) investigate over-refusal behavior across various language models, developing an evaluation framework to assess over-refusals in response to harmful prompts. Regarding hallucinations, Zhang et al. (2024) introduce an algorithm called R-Tuning, which prompts the model to state "I am unsure" or "I am sure" after a question and answer session, framing the problem as a discrimination task. Additionally, Kang et al. (2024) and Kapoor et al. (2024) propose alternative algorithms for alleviating the hallucination problem, focusing on instances where it is unclear whether the model possesses the required knowledge. Feng et al. (2024) uses multiple agents to determine when to abstain from queries. For predetermined queries the model is designed to refuse, Brahman et al. (2024) presents a comprehensive taxonomy of such questions, highlighting scenarios where the model should appropriately refuse to respond. This work also releases instructional data designed to train models in this regard. Evaluative studies by Liu et al. (2023), Yin et al. (2023), and Amayuelas et al. (2024) further explore the types of questions that warrant refusal.

**Tagging, control codes, and meta-tokens.** The concept of tagging or using control codes was introduced by Sennrich et al. (2016) in machine translation and later for more general-case by Keskar et al. (2019). A control code is a piece of text, \(c\), used in a conditional language model that always conditions on a control code \(c\) and learns the distribution \(p(x|c)\). Specifically, Keskar et al. (2019) pretrain a model using control codes to regulate style, content, and task-specific behavior. Tagging and control codes can also be viewed as form of prefix-tuning (Li and Liang, 2021). Lu et al. (2022) combines tagging with Reinforcement learning for model unlearning; while, Chan et al. (2021) introduces a new architecture to improve the behavior of the meta-tokens. Dong et al. (2023) extend this idea by adding controls to different distributions during supervised fine-tuning (SFT) that users might want to control, including seven categories which are collected by training another classifier to first categorize and score the responses based on the selected seven attributes. These tags or tokens can also be predicted by the model to help the model generate its response to a query. The general use of these "meta-tokens", or tokens that the model predicts to help itself generate its response to the query, has seen a recent increase with the introduction of tool calling in LLMs, or function calling (Nakano et al., 2021; Schick et al., 2024). However, others propose using meta-tokens for various purposes, such as enhancing reasoning capabilities (Yao et al., 2023), thinking capabilities (Goyal et al., 2024), or a variety of others (Teknium et al., 2024). In Table 1, we highlight the differences between these methods and our own.

## 3 Learning to Refuse with Tokens

Instruction models are trained on instruction-response pairs, \((x,y)\), sampled from instruction dataset \(D\). The user provides the model with a question or an instruction, \(x\), and the model then outputs a response \(y\). Each datapoint is usually given an additional chat template, \(C\). Here, \(y\) consists only of natural language without any meta-information contained in the messages. We introduce a new token, [refuse], at the beginning of the response if it is a refusal message, or [respond] otherwise during training. This modifies \(y\) to \(y^{}=+y\) or \(y^{}=+y\), depending on whether \(y\) is a refusal message or a normal response. This can also be written as an application of the token to the end of the chat template, or \(C(x)+\).

We will see that including the [refuse] and [respond] tokens during training will influence the model at test-time. The model builds stronger associations during fine-tuning the more it encounters response tokens together with non-refusal messages and refusal tokens together with refusal messages. After fine tuning, the presence of the refusal token at the beginning of the response results in a high likelihood of a refusal message, and vis-versa. Note, however, that the association of refusal tokens with refusal messages is not guaranteed. In our studies below, we used LLM-as-a-judge (Zheng et al., 2024) for measuring refusal rates.

**Test-time control.** The primary reason to include this refusal token is the test-time capabilities that the token introduces. The model predicts this token, and there is a softmax probability associated with it that can be used as a confidence measure for determining whether the question should be refused or not. This confidence can manipulated in many ways such as thresholding the token or

   Potential Approach &  Test-Time \\ Control \\  &  Differentiates between \\ refusal types/reasons \\  &  Refusal accompanied \\ by notification \\  &  Quantifes probability \\ that refusal is needed \\  & 
 Calibrate refusal rates \\ without retraining \\  \\  System Prompt & ✓ & ✓ & ✗ & ✗ & ✗ \\ TaggingControl Codes & ✓ & ✓ & ✗ & ✗ & ✗ \\ Model Reflection & ✗ & ✗ & ✓ & ✓ & ✗ \\  Refusal Tokens & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: A list of capability differences between approaches for controlling refusal behavior. Refusal tokens provide more capabilities than other solutions. Tagging or Control Codes apply “tags” to the prompt to encourage safe outputs. In Model Reflection, the model outputs a response and then is asked to reflect on the safety of its response. See Section 2. Our proposed approach yields the most control over refusals: It (i) enables test-time control of the kinds of refusals that are enabled. It also (ii) produces an interpretable score (the refusal token “probability”) that quantifies the risk of answering without a refusal, and (iii) these scores can be thresholded/calibrated at inference time to optimize refusal rates. (iv) It also enables different refusal types/reasons to be adjusted separately. (v) It notifies the user with a special token when a refusal takes place, allowing developers to see the type query.

Figure 1: The refusal token is only produced when its score rises above a threshold chosen by the user. A higher threshold yields a response from the model; whereas, a low threshold yields a refusal message. In this example, the question assumes that George Orwell wrote “The Invisible Men”, which is not true.

adding a logit bias. We focus our studies on the thresholding method, and emit the [refuse] token if its softmax score is \(>T\), for some \(T\) chosen by the user.

**Controlling different types of queries.** We consider applying categorical refusal tokens for different refusal reasons. Our experimental setting includes five refusal tokens corresponding to the refusal categories defined in Brahman et al. (2024), and one respond token. Details of our multi-category thresholding schemes and logit bias mechanisms are described in greater detail in Section 5.1.

## 4 Experimental Set-up

We use the hyperparameters and codebase from Tunstall et al. (2023) for supervised finetuning. Our initial results with DPO (Rafailov et al., 2023) show that the SFT stage is required for the desired refusal behavior (See Appendix Table 5). The importance of the SFT stage before DPO was also seen in Sharma et al. (2024). We adopt Ilama-3 SB (Dubey et al., 2024) as the base model. Additionally, we mix the instruction pairs that contain refusal messages with UltraChat (Tunstall et al., 2023) or Alpaca (Taori et al., 2023). We experimented with Alpaca as it is largely free of any refusal messages, and its low training time facilitates more ablations in Section A.1.

_Coconot_ Experimental Setting. For the main experimental setting, we utilize a diverse and comprehensive dataset--extending beyond just toxicity--for both training and evaluation to ensure robust performance in refusal prediction. Specifically, we adopt Brahman et al. (2024)'s _coconot_ dataset and evaluation due to the breadth of the categories and subcategories that are considered. The _coconot_ dataset contains five refusal categories-Humanizing, Indeterminate, Incomplete, Safety, and Unsupported-and \(26\) subcategories. Additionally, the dataset contains contrast data, or examples that the model should answer but are close to questions that the model should refuse. We consider two main training settings on UltraChat with refusal data and training on UltraChat with refusal and contrast data. For these two settings, we either train with no refusal token, one refusal token, or multiple category refusal tokens. The _coconot_ dataset contains \( 10\)k refusals SFT data, \( 1\)k of contrast preference data (which we use as SFT data), and \( 1.4\)k, or \(1379\), for the evaluation. The evaluation contains 1k queries that should refuse to answer and \(379\) queries that the model should respond to the query-referred to as the contrast category. We refer to this evaluation and experimental set-up as _coconot_.

**Temporal Experimental Setting.** We considered a second more controlled experimental setting. We created temporal refusal and contrast training data to address _coconot_'s low contrast-to-refusal ratio, at one to ten. For this setting, we consider a refusal message, where the query is temporally ambiguous or relates to events beyond the model's cutoff dates. Additionally, we considered contrast data, or examples close to a refusal query but answerable, as temporal questions that contain dates about an event within its training period. Using Ilama-3 70B, we prompted the model to generate questions from news articles beyond its cutoff date for refusal data, and before the cutoff data of the model for contrast data, with modified prompts. More details can be found in Appendix A.3. We generated \(2\)k examples each for refusal and contrast datasets, focusing on temporal questions, resulting in \(4\)k instruction-response pairs. We consider two main training settings on UltraChat with refusal data and contrast data used throughout the sections, and Alpaca (Taori et al., 2023) with refusal data and contrast data used in Section A.1. For these two settings, we either train with no refusal token or one refusal token. We consider this setting to understand the effect of balanced contrast data on the refusal token. In this setting, we developed \(200\) temporal questions evaluation, which humans verified manually. The evaluation also included refusal instructions from _coconot_'s refusal categories (excluding the temporal subcategory) and TriviaQA questions for model-appropriate responses. The inclusion of _coconot_'s refusal questions was to determine how models may "generalize" to other refusal categories when trained only on a single question type, see Section A.1. The total question count was \(1400\) for this evaluation, matching _coconot_'s evaluation set. We refer to this evaluation and experimental set-up as _Temp_.

**Evaluation.** For both experimental settings, we use the Brahman et al. (2024)'s prompts and evaluation framework with Ilama-3.1 70B as the LLM judge (Zheng et al., 2024). Brahman et al. (2024) originally found no evaluation quality difference between GPT-4 (Achiani et al., 2023) and GPT-3.5 (Brown, 2020). Furthermore, with Ilama-3.1 70B showing similar performance as GPT-3.5, we decided that an open-source model would be easier to reproduce as API models change and appreciate constantly. Additionally, we manually verified the effectiveness of Ilama-3.1 70B as the evaluator.

## 5 Test-Time Control Using [Refuse] and [Respond] Tokens

The refusal token introduces test-time capabilities. By training with the refusal token, the refusal rate can be altered at test-time. This ability cannot occur when training without the token. The model predicts this token, providing a softmax probability associated with the refusal token. This token probability can be interpreted as the confidence with which the model "thinks" the question should respond with a refusal message. Conversely, the response token is interpreted as the probability that the model should respond. We use this confidence measure and generate the token if \(p(|C(x))>T\), where \(T\) is a threshold set by the user. By adjusting the threshold, \(T\), we demonstrate that the refusal rates can be effectively controlled.

**Refusal tokens provides control of the refusal rate.** We sweep the thresholds of the refusal token across the two settings-training with and without contrast training examples-to observe the change in the true positive and false positive rates. In Figure 2, the threshold provides control over the true positive and false positive rates. Figures 1(a) and 1(b) show that adding contrast data (SFT data that lies close the boundary between the two classes but are non-refusal) results in a better Pareto frontier than training without the token.

### Controlling Individual Types of Instructions with Category Refusal Tokens

We now experiment with having five distinct refusal tokens that differentiate between refusal types for _coconot_. Additionally, we consider the temporal setting with one temporal refusal token. For all experiments in this section, we add refusals and/or contrast data to UltraChat.

**Thresholding schemes and logit bias.** We explore two types of thresholding strategies: (1) category thresholding, refusing with that category token if a token from selected category tokens is the highest probability among the refusal tokens and (2) sum thresholding, refusing only if the sum of all category token scores exceeds a threshold. For category thresholding, we emit the refusal token that is the highest probability among the refusal tokens and is in the selected category tokens; otherwise, we emit the token with the highest probability. For sum thresholding, we emit the category refusal token highest probability when the condition described earlier is met; otherwise, we emit a response token. Algorithmic versions of these schemes can be found in Appendix A.6. For logit bias, we manipulate the sensitivity of different refusal types by adding a constant bias to the (unnormalized) logits for the refusal tokens.

**Independent control of sensitivity for different refusal types.** To test whether categories can be independently controlled, we completely suppress each token one-at-a-time, and observe the

Figure 2: **Manipulating the refusal token provides different refusal rates at test-time without retraining. The left and right figures show that both true positive and false positive rates on _coconot_ eval change as we vary the threshold of the refusal token. The models are trained with ultrachat and refusal messages from the _coconot_ training data. Left is trained without any contrast data, and the right is trained with contrast data, which is one-tenth of the refusal data. All refusal and training are from the _coconot_ training data.**impact of this suppression on other (non-suppressed) refusal types. In Figure 3, we observe that the sensitivity of each refusal category can be adjusted with little impact on other categories of refusals. There is an exception though: _Humanizing Requests_ proved particularly difficult to suppress and did not respond to their token as other categories did. After inspecting the questions and responses of the _Humanizing Requests_ category, we found that many of the questions contained questions or instructions similar to other categories.

Thus, many of the _Humanizing_ questions or instructions are classified as one of the other refusal categories, i.e. the model emitted the incorrect refusal token. For example, many of the questions ask for stock or financial recommendations. These types of requests could easily be refused due to temporal issues (no access to real-time information), input modality issues (needing access to current portfolios), or safety (not wanting to provide financial information). Nevertheless, Figure 3 highlights that one can use individual category tokens to control individual distributions.

We first consider our temporal setting. Particularly, we sweep the thresholds of a model trained with UltraChat, \( 2\)k temporal refusal messages, and \( 2\)k temporal contrast training examples. We experiment with values of \(T\) from \(0\) to \(1\) in increments of \(0.1\), where we only sweep one token. In Figure 4, we observe that F1 scores improve when properly calibrating the thresholds, finding that \(T=0.1\) performs the best.

Figure 4: **Thresholding the refusal tokens increase F1 scores and controls the true positive and false positive rates for a single instruction type (temporal setting).** For our temporal experimental setting, we train UltraChat with \(2\)k refusals and \(2\)k contrast examples. The left shows thresholding achieves a better F1 Score, and the right shows thresholding controls the true positive false positive rates.

Figure 3: **Individual category refusal tokens enable precise control over query types.** Refusal rates for different categories on _coconot_ when category-specific tokens are suppressed or not generated by the model. The blue dashed bars compare this with the suppression of a single refusal token. By suppressing tokens from specific categories during inference, we demonstrate control over the types of refusals. The two dashed bars per group reflect the effect of suppressing a category’s token, either through category-specific suppression or a single refusal token. We also observe category overlap with both these experiments and a manual inspection; for instance, Humanizing Requests may fall into multiple categories.

[MISSING_PAGE_FAIL:7]

## 6 Discussion

An issue with refusal messages in LLMs is that generation sampling can cause the model's response to vary across multiple iterations of the same query (Huang et al., 2024). However, the use of a refusal token can help mitigate this issue. For example, we compared two models--one with the refusal token and one without--over five generations. We recorded the entropy of each set of responses. We found that the model with the token had a slightly lower entropy (\(0.07\) compared to \(0.10\)), where the entropy would be \(0.69\) if the probability of generating a refusal message (or any refusal message) is \(0.50\). Additionally, we found that in \(81\%\) of cases, the responses had zero entropy, meaning all generations are identical, compared to \(87\%\) with the refusal token. Providing an explanation, Table 3 shows that a refusal or response token does not guarantee that the generation is a response or refusal. Nevertheless, the refusal token improves consistency in model generations. Another aspect of refusal to consider is adversarial attacks. Although we assume that the user in these settings is not acting maliciously, an individual may optimize the refusal tokens directly optimize on short strings like "Sure here's,.." such as Shin et al. (2020); Wen et al. (2023); Zou et al. (2023); Zhu et al. (2024). However, these attacks are well-studied in the community (Alon and Kamfonas, 2023; Jain et al., 2023; Zhou et al., 2024).

The ability of a model to refuse queries-whether due to toxicity, limitations, or other reasons-is crucial for developing safer and more trustworthy LLMs. To advance this, we need to understand how and why models generalize across different contexts, which requires the appropriate data. While some datasets, such as Brahman et al. (2024), provide broad coverage, there remains a gap in preference data and multi-turn evaluations, complicating the task of generalizing single-turn results to multi-turn interactions. Thus, we need additional data to better understand this property of LLMs.

Nevertheless, adding a refusal token during fine-tuning offers several benefits. When the model generates the token, it associates a softmax probability of refusal with the query. At test-time, the refusal token allows for adjusting the refusal rate. Moreover, by applying the refusal token to specific categories, the distribution can be controlled, and thresholding techniques can further improve the F1 scores of refusal rates. Additionally, these tokens can be modified in various ways during testing, such as using logit bias, category-specific thresholding, or sum thresholding, highlighting their flexibility. Therefore, without retraining language models, refusal tokens offer the advantage of test-time control, benefiting both users and API providers.

## 7 Acknowledgements

Neel Jain and Ashwinee Panda were supported by Capital One Bank during the majority of work. Additinoally, this work was made possible by the ONR MURI program, DAPRA TIAMAT, the National Science Foundation (IIS-2212182), and the NSF TRAILS Institute (2229885). Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy.

   Response & Refusal Token & Response Token \\ Label & Generated & Generated \\   Refused & 1019 & 46 \\ Responded & 29 & 277 \\    
   Response & Refuse Cat. & Respond Token \\ Label & Generated & Generated \\   Refused & 945 & 68 \\ Responded & 43 & 315 \\   

Table 3: The counts of response tokens or refusal tokens generated and what the model generation was labeled. **Left** shows the counts for a single refusal token under default sampling parameters. **Right** shows the counts for category refusal tokens under default sampling parameters.