# AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across Any Scenario

Yuhan Li\({}^{1}\)*, Hao Zhou\({}^{2}\), Wenxiang Shang\({}^{2}\), Ran Lin\({}^{2}\), Xuanhong Chen\({}^{1}\), BingBing Ni\({}^{1}\)\({}^{}\)

\({}^{1}\)Shanghai Jiao Tong University, Shanghai 200240, China

\({}^{2}\)Alibaba

{meldious, nibingbing}@sjtu.edu.cn

###### Abstract

While image-based virtual try-on has made significant strides, emerging approaches still fall short of delivering high-fidelity and robust fitting images across various scenarios, as their models suffer from issues of ill-fitted garment styles and quality degrading during the training process, not to mention the lack of support for various combinations of attire. Therefore, we first propose a lightweight, scalable, operator known as Hydra Block for attire combinations. This is achieved through a parallel attention mechanism that facilitates the feature injection of multiple garments from conditionally encoded branches into the main network. Secondly, to significantly enhance the model's robustness and expressiveness in real-world scenarios, we evolve its potential across diverse settings by synthesizing the residuals of multiple models, as well as implementing a mask region boost strategy to overcome the instability caused by information leakage in existing models. Equipped with the above design, AnyFit surpasses all baselines on high-resolution benchmarks and real-world data by a large gap, excelling in producing well-fitting garments replete with photorealistic and rich details. Furthermore, AnyFit's impressive performance on high-fidelity virtual try-ons in any scenario from any image, paves a new path for future research within the fashion community.

Figure 1: AnyFit shows superior try-ons for any combination of attire across any scenario.

Introduction

The dramatic success of e-commerce is steadily demanding a more convenient and personalized customer shopping experience. Among them, image-based virtual try-on (VTON) [(23; 47; 22)] has emerged as a promising topic in the research community and witnesses rapid advancements [(15; 24; 18)], whose task is to fit the target garment to the human body with various gestures. However, current methodologies do not meet the high-fidelity and robustness required in real-world applications, often resulting in artifacts or the mismatch of clothing details. Furthermore, the support for a variety of try-on combinations of attire [(10)] remains an area of ongoing research.

Most prior methodologies [(6; 14)] utilize a separate warping module to align garments on the human body, subsequently employing a Generative Adversarial Network (GAN) [(19)] for their integration. This explicit warping process typically yields overly smooth garment transformations and struggles to cope with complex poses and occlusions. [(50)]. While some diffusion-based methods [(58; 50)] leverage pre-trained diffusion models [(43)], with structures akin to ReferenceNet [(26)] to preserve fine-grained garment information; However, these methods encounter difficulties in producing vivid fabric textures and photorealistic lighting and shadows. They also show artifacts in cross-category try-ons, diminishing their inherent text-image capacity when applied to specialized tasks as depicted in Fig. 4. In summary, existing methods still fall short in producing images of high fidelity that exhibit clothing styles rendered with exceptional detail and true-to-life accuracy across scenes. Moreover, these methods are designed solely for trying on individual items of clothing and do not support multi-conditions, thereby failing to facilitate the free combination of tops and bottoms.

As discussed above, we believe that an ideal VTON workflow should exhibit the following properties:

* **Scalability.** The ultimate goal of the VTON model is to enable any free and customizable virtual outfit combination of multiple garments [(57)]. It should support multi-condition injection, allowing for easy expansion to more applications, such as mixing and matching tops and bottoms, layering inner and outer garments, _etc_.
* **Robustness.** Given the diverse scenarios encountered in e-commerce settings [(13)], the VTON model should generate authentic fabric textures and natural lighting, reproducing the details of the target clothing (_e.g._, logos, patterns, texts and strips) stably and accurately.

We present the following critical contributions to establish AnyFit as a novel VTON paradigm, which adeptly addresses the challenge of any combination of attire across any conceivable scenario, in Fig. 1. AnyFit mainly consists of two isomorphic U-Nets, namely HydraNet and MainNet. The former is tasked with extracting fine-grained clothing features, while the latter is responsible for generating try-ons. [(1)] **Scalability**: A hallmark of AnyFit is its innovative introduction of the **Hydra Encoding Block** that only parallelizes attention matrices within a sharing HydraNet, enabling effortless expansion to any quantity of conditions with only 8% increase in parameters for each additional branch. The proposal of parallelizing these blocks is built on the insight that only the self-attention layers are crucial for implicit warping [(50)], while the remaining components primarily serve as generic feature extractors. We further invent **Hydra Fusion Block** to seamlessly integrate the features of Hydra Encoding into MainNet, with positional embeddings to distinguish encodings from different sources. It is important to note that ReferenceNet [(26; 9)] or GarmentNet [(50)] could be seen as specific instances of HydraNet when limited to a single condition. [(2)] **Robustness**: Observations indicate a noticeable reduction in the robustness and quality of images generated by existing Virtual Try-On (VTON) works, in comparison to the original stable diffusion performances. Inspired by discussions in the community [(2)], we present the **Prior Model Evolution** strategy. This innovative approach involves merging parameter variations within a model family (_e.g._, a collection of fine-tuned versions of SDXL [(41)]), enabling the independent evolution of multiple capabilities of the base model. This strategy emerges as an intuitively logical and highly effective method for amplifying the model's innate potential prior to training. This is particularly relevant when contending with the significant escalation in training costs associated with dual U-Nets--an aspect that is overlooked in previous research. Furthermore, we introduce the **Adaptive Mask Boost** to further enhance the fit of the attire as a bonus. It requires length augmentation of _parsing-free_ mask regions during the training phase, allowing the model to autonomously understand the overall shape of the clothing, which emancipates the model from previous reliance on hints of masks derived from garments. During inference, we adapt the shape of the mask area based on the aspect ratio of the target garment, thereby markedly encouraging the generation of well-fitted try-ons, particularly for long garments (_e.g._, windbreaker).

To the best of our knowledge, AnyFit stands as a pioneering VTON method to fulfill scalability and robustness requirements. Our innovative HydraNet and Prior Model Evolution strategies have the potential to transform not just the domain of VTON, but to catalyze advancements across a broader spectrum of conditional generation applications. Finally, we have carried out comprehensive experiments on try-on benchmarks [(12; 16)] and engaged in challenging validation using in-the-wild sets. These experiments demonstrate that our model shows exceptional performance that eclipses current methods by a substantial margin, in terms of garment fidelity and robustness when addressing street-captured scenarios. In addition, our method has realized a formidable capability for multi-garment try-ons, culminating in results that exhibit strikingly harmonized upper and lower styles.

## 2 Related works

**GAN-based virtual try-on.** The virtual try-on task is concerned with synthesizing images of a person donning the designated garment with appropriate fit [(23)], while retaining salient characteristics of the original garment and person, given a pair of images depicting a person and a target garment. To execute this task, numerous works [(17; 30; 34; 6; 14; 22)] have utilized Generative Adversarial Networks (GANs) [(19)] with two-stage strategy [(35; 18; 52)]: (1) warping the clothing to the desired shape [(7; 36)] and (2) fusing the deformed clothing via try-on generator based on GAN. HR-VITON [(35)] conducts both warping and segmentation concurrently to address issues related to body occlusion and misfit of garments. GP-VTON [(49)] introduces localized warping along with global parsing to independently simulate the deformation of different regions of clothing, aiming to achieve a more form-fitting result. However, these existing approaches that rely on an explicit warping module are incapable of supplementing the sides of the clothing and the natural lighting and shadows [(50)].

**Diffusion-based virtual try-on.** As significant progress in Text-to-Image diffusion models [(25; 39; 27; 11)] is witnessed in recent years, some works [(10)] have been motivated to incorporate pre-trained diffusion models [(43; 41)] as priors into virtual try-on task. LADI-VTON [(38)] and DCI-VTON [(20)] explicitly deform the clothing to achieve pixel-level alignment with the human body, followed by a diffusion model to blend the clothing with the human body as refinement. StableVITON [(31)] introduces an end-to-end approach that injects intermediate feature maps from a spatial encoder into the U-Net decoder via a zero cross-attention block, akin to the ControlNet [(55)] structure. Most recently, OOTDiffusion [(50)] and IDM [(13)] achieve garment feature extraction with a parallel U-Net and feed them through self-attention for enhanced integration. Unfortunately, these methods intrinsically lack support for try-ons that involve multiple garments. Moreover, they exhibit artifacts and unstable garment fits for arbitrary images, which leads to a degradation of performance on out-of-distribution images in complex backgrounds and poses.

## 3 Method

### Model overview

An overview of the AnyFit is presented in Fig. 2. The backbone of AnyFit employs the SDXL [(43)], with the preliminary detailed in Appendix C.2. Given a human image \(x_{h}^{H W 3}\) and a target garment image \(x_{g}^{H W 3}\), AnyFit is aimed to generate an authentic try-on image \(x_{tr}\). We employ OpenPose [(8; 53)] to obtain clothing-agnostic mask \(x_{m}\) and masked person image \(x_{ag}\)

Figure 2: Overall framework of our method.

adjusting for the size of different garments, as detailed in Sec. 3.3. We treat VTON as a specific case of image inpainting (51), endeavoring to fill the masked person \(x_{ag}\) with the cloth \(x_{g}\). The main inpainting U-Net (MainNet) inputs \(3\) concatenated components with \(9\) channels: the noisy image \(z_{t}\), the latent agnostic image \(E(x_{ag})\) and the resized agnostic mask \(x_{m}\), where \(E()\) represents VAE (32) encoding. A Pose Guider (26) with \(4\) convolution layers (\(4 4\) kernels, \(2 2\) strides, \(16,32,64,128\) channels) is incorporated to align the pose image \(E(x_{p})\) with noise \(z_{t}\).

**Scalability:** To preserve the fine details of the clothing, as well as to support both single and multiple garment VTONs, we employ a HydraNet that mirrors the MainNet in encoding clothing information. It shares the same weight initialization as the MainNet and innovatively parallelizes attention metrics based on the number of conditions to create Hydra Encoding Blocks for different conditional encodings. **Robustness:** During training, issues such as mask information leakage and quality degradation were observed. To address these issues, we adopt Adaptive Mask Boost and Prior Model Evolution, respectively, which significantly bolster the model's robustness across different scenarios cost-effectively and straightforwardly.

### HydraNet for multi-condition VTON

**HydraNet.** Inspired by successful practice in human editing (26; 9), we introduce a garment encoding network isomorphic to the main generative network (MainNet), that precisely preserves the details of clothing. When dealing with multi-garment VTON, a direct method might involve replicating multiple garment encoding nets to manage different conditions. This approach, however, would lead to a significant increase in the number of parameters, rendering it computationally prohibitive. Experimentally we discover that for conditions with similar content (such as different types of clothing), the self-attention module plays a vital role in the latent warping of the garments, aligning them with the locations requiring inpainting. Conversely, other network architectures, which typically tasked with general feature extraction, can be shared across different condition encoding branches without compromising the model's performance. In view of this, we innovatively propose HydraNet for multi-condition encoding. It operates based on a shared Unet structure, while parallelizing the attention modules according to the number of input conditions, thereby constructing Hydra Encoding Blocks. Specifically, we parallelize the self-attention matrices with identical initial weights, and feed the multi-condition _key_ and _value_ features \(\{z_{hk}^{i},z_{hv}^{i}\}\) into MainNet, which encode the fine-grained details of the clothing. It's notable that ReferenceNet (26; 9) or GarmentNet (50) could be seen as specific instances of HydraNet limited to a single condition. HydraNet requires only one forward pass (timestep \(t=0\)) to encode clothing before the multiple denoising steps in MainNet, with the additional temporal and parameter overheads being minimal for each added condition, in Tab. 2.

**Hydra Fusion.** We propose a highly efficient and easily scalable Hydra Fusion Block to replace the self-attention layers in MainNet, accomplishing the feature injection from HydraNet to MainNet for any length via concatenation. Specifically, given the _key_ and _value_ features \(\{z_{hk}^{i},z_{hv}^{i}\}^{b l c}\) from the HydraNet, we introduce learnable position embeddings to distinguish features from different source conditions. The superscript \(i\) denotes different input conditions. Subsequently, we concatenate the _key_ and _value_ along the \(l\) dimension to obtain the final \(\{z_{hk}^{all},z_{hv}^{all}\}^{b Nl c}\) as:

\[z_{hk}^{all}=(z_{hk}^{1}+PE^{1}(z_{hk}^{1}))(z_{hk}^{2}+PE^{2}(z_{hk}^{ 2}))(z_{hk}^{N}+PE^{N}(z_{hk}^{N})),\] (1)

where \(N\) denotes the total number of input conditions, \(PE\) represents positional encoding, and \(\) signifies concatenation. \(z_{hv}^{all}\) follows a similar formulation. Facing the _key_ and _value_ features \(\{z_{mk},z_{mv}\}^{b l c}\) from the MainNet and concatenated HydraNet features \(\{z_{hk}^{all},z_{hv}^{all}\}\), we once again concatenate the corresponding features along the \(l\) dimension into \(\{z_{ck},z_{cv}\}^{b(N+1)l c}\), which are then used in subsequent attention calculations with \(z_{mq}\). It is noteworthy that, with the lightweight design leveraging parallelization and concatenation, HydraNet can be effortlessly extended to perform injections with any number of conditions, thereby possessing a more expansive application potential within the domain of generative models.

### Model evolution and mask boost for robust VTON

**Prior Model Evolution.** A diminution in image generation performance with the SDXL-inpainting model compared to the SDXL base model is noted (1). We attribute this degradation to the disruption of previously well-aligned correspondences between text and images during the inpainting pre-training phase. Drawing inspiration from the open-source community (2), we develop a Prior Modelvolution strategy, which enhances the model's strength and adaptability in generating outfit images at a very low cost, even without training. Specifically, we meticulously amalgamate the weights from three distinct, powerful models to evolve the initial weights for our model. These models comprise: SDXL-base-1.0 (41), SDXL-inpainting-0.1 (4) with inpainting capabilities, and DreamshaperXL alpha2 (3), which demonstrates superior performance in generating clothing and human figures. Their weights are denoted by \(_{base},_{inp},_{ds}\). The evolution formula is as follows:

\[_{new}=_{base}+_{inp}- _{base})}_{}+_{ds}-_{base})}_{},\] (2)

where \(\) and \(\) are the balancing coefficients that account for the capability enhancements from SDXL-inpainting and DreamshaperXL. It is important to note that we directly copy the extra \(5\) channels in the _conv in_ layer of the SDXL-inpainting into the merged model, multiplying them by \(\).

However, the optimal values of \(\) and \(\) are not apparent. We hope to determine the optimal \(\) and \(\) to ensure that the initial weight \(_{new}\) achieve the best evaluation performance, i.e.

\[*{arg\,min}_{(,)^{2}}f(,)= (_{base}+(_{inp}-_{base})+ (_{ds}-_{base})),\] (3)

where \(\) is a non-differentiable evaluation function. Empirically, we assume that \(f\) exhibits monotonic or convex properties with respect to the balancing coefficients \((,)\) in most regions. Therefore, we discretize the continuous domain \(^{2}\) into a grid with \(=0.1\) as the step size and design the discrete greedy algorithm 1, to search for the optimal \((,)\). In our algorithm, we selecte the CLIP score (42) on \(20\) fixed inpainting image-text pairs as the evaluation function \(\). The optimal solution obtained is \((,)=(1.0,1.1)\). Please refer to the Appendix C.1 for more explanations.

```
1:Evaluation function \(f\), step size \(\).
2:Initialize \((,)(0.5,0.5)\)
3:while True do
4:\(f_{} f(,)\)
5:\(N\{(+,),(-,),(,+ ),(,-)\}\)
6:\(N\{(^{},^{}) N 0^{}  20^{} 2\}\)
7:\(F_{N} I(^{},^{})(^{},^{ }) N\)
8:if\((F_{N}) f_{}\)then
9:break
10:else
11:\((,)*{arg\,min}_{(^{},^{ }) N}f(^{},^{})\)
12:endif
13:endwhile
14:return\((,)\) ```

**Algorithm 1** Discrete greedy algorithm

**Adaptive Mask Boost.** Previous works generally exhibit limited robustness in cross-category try-on scenarios, resulting in inaccurately rendered clothing styles as shown in Fig. 6 and 10. This is largely due to a dependence on agnostic masks derived from clothing parsing, which tends to leak the edges of the clothing shape during training. This leakage may cause the generated garments to almost entirely cover the agnostic mask region. In response to these limitations, we employ an intuitive and effective approach that significantly enhances the model's robustness about cross-category try-on, _i.e._, the Adaptive Mask Boost strategy, which primarily comprises mask augmentation during training and

Figure 3: Visual comparisons on VITON-HD. AnyFit displays superior details and outfit styling.

adaptive elongation during inference. Specifically, during training, the agnostic mask is extracted solely using OpenPose body joint detections _without leveraging human parsing_. We perform random elongation of the mask by a factor \(f(1.2,1.5)\) with a probability of \(P=0.5\). This training setting forces the model to autonomously determine the optimal cloth length. During inference, we assess the aspect ratio \(\) of the bounding box of the laid-out garment. If \(>1.2\), we proportionally extend the agnostic area to match \(\), creating an adaptive agnostic mask that conforms to the garment's style. Experiments have validated that AnyFit with Adaptive Mask Boost autonomously determines the appropriate garment length, yielding robust try-on results across different clothing categories.

## 4 Experiments

### Experimental setup

**Datasets.** Our experiments are carried out on two publicly available datasets, VITON-HD (12) (\(11647\) training pairs) and DressCode (16) (\(48392\) training pairs) using the official splits for training and testing, as well as an additional proprietary e-commerce dataset. This proprietary dataset contains \(50602\) training pairs and \(2500\) testing pairs of mainly upper-body person and upper garment images, featuring complex patterns, backgrounds, and postures, alongside a rich variety of styles including layered garment ensembles, which present a more challenging scenario. As for multi-garment try-on, we utilized a HumanParsing model (28; 29) to extract clothing items from DressCode and constructed triplets consisting of (upper-body garment, lower-body garment, model image). In these triplets, one garment is an original laid-out image, while the other is a cropped image from the person's image. Finally, we construct \(24314\) publicly available upper-lower triplets by garment crop from the \(15363\) upper-body pairs and \(8951\) lower-body pairs in DressCode, called DressCode-multiple. A subset of \(1800\) triplets is reserved as the test set. Please refer to the Appendix B for more details.

Figure 4: Visual results on proprietary and in-the-wild data. Best viewed when zoomed in.

**Implementation details.** We initialize the AnyFit with Prior Model Evolution strategy described in Sec. 3.3, and fine-tune it using an AdamW optimizer (37) with a constant learning rate of \(5e-5\). We train three variants of models on the VITON-HD, DressCode and proprietary dataset at a resolution of \(1024 768\), independently. Subsequently, we extend the model trained on DressCode to enable multi-garment try-ons by training on the DressCode-multiple dataset. All the models are trained for \(150\) epochs on \(8\) NVIDIA A100 GPUs with DeepSpeed (5) ZeRO-2 to reduce memory usage, at a batch size of \(15\). At inference time, we run AnyFit on a single NVIDIA RTX 3090 GPU for \(30\) sampling steps with the DDIM sampler (44). Outfitting dropout (50) is used with a guidance scale \(s_{g}=1.3\). The data augmentation follows the same protocol as in StableVTON (31). We also employ pretrained IP-Adapter (54) for SDXL. Please refer to the Appendix D for more details.

Figure 5: Visual comparisons on the DressCode-multiple. AnyFit exhibits an elegant integration between upper and lower garments, accurate length control, and appropriate overall styling.

Figure 6: Visual validation about model evolution and mask boost in (a), (c), (d). We also provide visual results about mask reliance in (b) found in previous work.

**Evaluation protocols.** We measure reconstruction accuracy by LPIPS (56) and SSIM (48) in a paired setting provided ground truth images, and authenticity of unpaired synthesized images by FID (40) and KID (45) without ground truth. All evaluations are conducted at a resolution of \(512 384\).

**Baselines.** We compare our model on single try-on tasks on VITON-HD, DressCode and our proprietary dataset with previous baselines including HR-VTON (35), LADI-VTON (38), DCI-VTON (20), StableVITON (31), OOTDiffusion (50), and the state of the art IDM (13). We directly utilize their released pre-trained models. As for multi-garment try-on, we developed a two-stage IDM model as a strong baseline, referred to as IDM-2Stage, which dresses the upper and lower garments sequentially. Inspired by (10), we concatenate the upper and lower garments on width spatially and feed them to a single-conditional HydraNet for training as another baseline, termed VTON-concat. Finally, we compared AnyFit with Paint by Example (51), IDM-2Stage, and VTON-concat.

   Dataset &  &  \\  Method & LPIPS \(\) & SSIM \(\) & FID \(\) & KID \(\) & LPIPS \(\) & SSIM \(\) & FID \(\) & KID \(\) \\  HR-VTON (35) & 0.097 & 0.878 & 12.31 & 3.86 & - & - & - & - \\ DCI-VTON (20) & **0.072** & 0.892 & 8.76 & 0.92 & - & - & - & - \\ StableVITON (31) & 0.076 & 0.891 & 9.35 & 1.51 & - & - & - & - \\ OOTDiffusion (50) & 0.093 & 0.856 & 9.16 & 0.68 & - & - & - & - \\ GP-VTON* (49) & 0.083 & 0.892 & 9.17 & 0.93 & 0.051 & 0.921 & 5.88 & 1.28 \\ LADI-VTON (38) & 0.091 & 0.875 & 9.42 & 1.63 & 0.072 & 0.902 & 6.94 & 2.33 \\ IDM (13) & 0.078 & 0.881 & 9.12 & 1.03 & 0.046 & **0.923** & 5.32 & 1.24 \\ 
**AnyFit(ours)** & 0.075 & **0.893** & **8.60** & **0.55** & **0.044** & \(0.904\) & **4.51** & **0.48** \\   

Table 1: Quantitative comparisons on the VITON-HD (12) and DressCode (16).

Figure 7: We separately cut off the self-attention injections between different blocks of HydraNet and MainNet, as well as the image features from IP-Adapter in cross-attention layers. The results show that the self-attention layers between the up blocks are the decisive factor affecting the performance.

### Qualitative results

**Single-garment try-on.** Fig.3 and 4 provide a qualitative comparison between AnyFit and the baselines on VITON-HD, the more challenging proprietary and in-the-wild data, covering open-garment and layering rendering scenarios. For a fair comparison with the baselines, we include results of AnyFit trained on VITON-HD. AnyFit excels in retaining intricate pattern details, owing to the effective collaboration between HydraNet and the IP-Adapter. It also maintains the correct silhouette of the clothing at a semantic level. This suggests that, through Mask Boost, AnyFit enhances the recollection of the original shape of the clothing, while other models, influenced by the mask, tend to generate incorrect appearances. The Prior Model Evolution further strengthens the texture representation of the apparel. Notably, when trained on the proprietary dataset, AnyFit automatically fills in inner garments or unzips clothing based on posture, a capability absent in the version trained on VITON-HD due to the lack of such training data.

**Multi-garment try-on.** Fig. 5 offers a qualitative comparison for multi-garment try-ons using the compiled DressCode-multiple dataset. Firstly, AnyFit demonstrates high-fidelity cloth preservation. Importantly, thanks to the distinct and individual Hydra-Blocks situated in different condition branches, AnyFit accurately depicts the demarcation between the upper and lower garments, showcasing a reasonable transition at the interconnection. In contrast, VITON-concat mishandles the relative clothing sizes after concatenation, leading to garment distortion and blurring. Meanwhile, IDM-2Stage faces artifacts at the juncture of the upper and lower garments, because it obscures parts of one garment while trying on another. Remarkably, despite training with one garment presented as a flat lay image and the other as a warped cloth cropped from a person image, AnyFit remains strikingly robust when faced with both garments presented as flat lays during inference.

### Quantitative results

As indicated in Tab. 123, extensive experiments conducted on VITON-HD (12), DressCode (16), the proprietary dataset, and DressCode-multiple consistently prove that AnyFit significantly surpasses all baselines. This confirms AnyFit's capability to deliver superior try-on quality in both single-garment and multi-garment tasks across various scenes. Moreover, we note that AnyFit shows considerable improvement in unpaired settings in terms of the FID and KID metrics, demonstrating our model's robustness for cross-category try-ons. For more results, please refer to the Appendix E.

### Ablation study

**Hydra Blocks.** To validate our proposed Hydra Blocks, we directly employ a singular conditioned HydraNet (which degenerates to ReferenceNet (26) actually) as the baseline "w/o Hydra Block" to encode both the top and bottom garment conditions concurrently, and then concatenate them into MainNet. As illustrated in Tab. 4, Fig. 8 and 11, a model lacking the Hydra Block tends to produce artifacts at the junction of the top and bottom garments. Such models also frequently allow the features of one garment to influence the other, leading to incorrect clothing styles. However, with the introduction of the Hydra Block, AnyFit consistently exhibits more stable results.

**Prior Model Evolution.** We qualitatively demonstrate the effects of the Prior Model Evolution in Fig. 13 and 6 (a). The SDXL-evolved model reduces artifacts and enhances robustness significantly, while outputs without Prior Model Evolution typically feature oversaturated colors as well as lighting and shadows that do not harmonize with the background. The gradual enhancement of model capabilities is visualized in Fig. 6(c). We also empirically and quantitatively validate the effectiveness

   Method & FID \(\) & KID \(\) & Time \(\) \\   Pain-by-Example (51) \\ IDM-2Stage (13) \\ VTON-concat (10) \\  } & 35.17 & 13.12 & 95\% \\  & 21.47 & 7.85 & 93\% \\   & 21.11 & 7.30 & **8\%** \\   **AnyFit (ours)** \\  } & **20.43** & **7.10** & 9\% \\   

Table 2: Quantitative comparisons on the DressCode-multiple. The ”Time” represents the inference time increase compared to its single-garment try-on.

   Method & FID \(\) & KID \(\) \\  LADI-VTON (38) & 52.24 & 6.51 \\ DCI-VTON (20) & 57.96 & 12.35 \\ StableVTON (31) & 53.80 & 8.13 \\ IDM (13) & 48.76 & 4.35 \\   **AnyFit (VITON-HD)** \\ **AnyFit (proprietary)** \\  } & 46.95 & 2.73 \\
**43.97** & **0.69** \\   

Table 3: Comparisons on proprietary dataset. _AnyFit (xxx)_ is trained only on _xxx_ dataset.

of the Prior Model Evolution strategy after training on the Virtual Try-On (VTON) task in Fig. 8 and Tab. 4. The Prior Model Evolution, by improving the model's initial capabilities, lessens the difficulty of learning and fosters a dramatic improvement in outfitting capacity and logo fidelity.

**Adaptive Mask Boost.** We illustratively showcase the issues of information leakage and mask reliance found in previous methods in Fig. 6 (b) and Fig. 10. Additionally, we empirically and quantitatively validate the effectiveness of the Adaptive Mask Boost strategy in Table 4 and Fig. 10. This strategy significantly heightens the model's robustness towards different categories of clothing, enabling the autonomous determination of appropriate garment length rather than relying on masks. Furthermore, we manually adjust the aspect ratios \(\) in Fig. 6 (d), which demonstrates the positive impact of adaptive elongation during inference. More ablation studies are detailed in the Appendix A.

## 5 Conclusion

We introduce AnyFit, a novel and robust VTON pipeline suitable for any combination of attire across any imaginable scenario, offering a revolutionary leap in realistic try-on effects. To support multi-garment try-ons, AnyFit constructs HydraNet with lightweight and scalable parallelized attention that facilitates the feature injection of multiple garments. Observing artifacts in real-world scenarios, we evolve its potential by synthesizing the residuals of multiple models, as well as implementing a mask region boost strategy. Comprehensive experiments on high-resolution benchmarks and real-world data have demonstrated that AnyFit significantly surpasses all baselines by a large gap.

**Broader impacts.** With the ability to synthesize images, arises the risk that AnyFit might be used for inappropriate purposes such as producing media that breaches intellectual property rights or privacy norms. Because of these risks, we strongly advocate for the conscientious use of this technology.

**Limitation and future work.** Our approach exhibits excellent performance in single-garment and multi-garment virtual try-on applications. However, it still faces some limitations. Firstly, it shares the shortcomings of large text-image models, sometimes showing instability in generating hands with complex structures. Secondly, our model offers initial but not yet fully mature text control capabilities (for details, please refer to the Appendix A), providing opportunities for future enhancements.

   Dataset & Method & LPIPS \(\) & SSIM \(\) & FID \(\) & KID \(\) \\   & - w/o Hydra Blocks & - & - & 22.48 & 8.02 \\  & - w/o Prior Model Evolution & - & - & 21.35 & 7.58 \\  & **Full AnyFit** & - & - & **20.43** & **7.10** \\   & - w/o Adaptive Mask Boost & 0.183 & **0.748** & 44.75 & 1.44 \\  & - w/o Prior Model Evolution & 0.192 & 0.740 & 45.01 & 1.52 \\   & **Full AnyFit** & **0.181** & 0.743 & **43.97** & **0.69** \\   

Table 4: Quantitative ablation study.

Figure 8: Visual ablation study. Without Prior Model Evolution, AnyFit suffers reduced fabric detail and less realistic textures. While Hydra Blocks improve intersections of upper and lower garments.