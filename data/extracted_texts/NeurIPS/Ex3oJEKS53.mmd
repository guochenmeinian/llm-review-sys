# Kronecker-Factored Approximate Curvature for

Modern Neural Network Architectures

Runa Eschenhagen\({}^{*,}\)

University of Cambridge

Alexander Immer\({}^{2,3}\)

Richard E. Turner\({}^{1}\)

Frank Schneider\({}^{4,5}\)

Philipp Hennig\({}^{4,5}\)

Correspondence to: re393@cam.ac.uk. Department of Computer Science, ETH Zurich

\({}^{3}\)Max Planck Institute for Intelligent Systems

\({}^{4}\)University of Tubingen

\({}^{5}\)Tubingen AI Center

###### Abstract

The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with _weight-sharing_. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC - _expand_ and _reduce_. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer. However, both variations are able to reach a fixed validation metric target in \(50\)-\(75\)% of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. This highlights the potential of applying K-FAC to modern neural network architectures.

## 1 Introduction

One of the key driving forces behind the success of deep learning is arguably the development and scaling of novel deep neural network (DNN) architectures like transformers (Vaswani et al., 2017) or graph neural networks (Battaglia et al., 2018; Scarselli et al., 2009). While the landscape of neural network architectures seems vast, their core building blocks like attention, graph, recurrent, and convolutional layers can be expressed as simple linear layers with _weight-sharing_. For example, the weight-sharing can happen over a sequence of tokens, e.g. in language modelling, over image patches, or by tying weights in the forward pass. The perspective of expressing the core neural network operations as linear operations has been formalised in a different context with the Tensor Programs framework (Yang, 2019), which develops a language to express arbitrary neural network computations as a composition of simple matrix multiplications and coordinate-wise nonlinearities. One downside of relying on increasing the scale of the models to achieve better performance is the increased compute cost. To decrease these costs, we can try to improve the efficiency of the optimisation algorithms, which motivates exploring second-order methods like Newton's method and natural gradient descent (Amari, 1998), which uses the Fisher information matrix, or short, the Fisher.

Kronecker-Factored Approximate Curvature (Heskes, 2000; Martens and Grosse, 2015, K-FAC) is an approximation to the Fisher, or for common loss functions equivalently, the generalised Gauss-Newton matrix (GGN). It has first been derived in the context of optimisation for linear layers and later for convolutional (Grosse and Martens, 2016) and recurrent (Martens et al., 2018) layers. In deep learning, using the Fisher/GGN instead of the Hessian has been the de-facto standard; in turn, K-FAC has arguably been one of the most popular approximations of the Fisher/GGN, probably due to its relative efficiency as it approximates each layer's Fisher/GGN independently with a Kronecker product. While K-FAC has also been used for transformers (Grosse et al., 2023; Osawa et al., 2022; Pauloski et al., 2021; Zhang et al., 2019) and a graph neural network (Izadi et al., 2020), there is no theoretical framework for these cases and for applying K-FAC to new architectures.

**Contributions.** We propose such a framework by leveraging the idea of linear layers with weight-sharing. This concept reveals the additional structure in the Fisher/GGN due to the weight-sharing that has to be explicitly considered when applying K-FAC. We identify two settings of such layers that motivate two different flavours of the K-FAC approximation: _K-FAC-expand_ and _K-FAC-reduce_. The former corresponds to the Kronecker Factors for Convolution (KFC) approximation in Grosse and Martens (2016), to the simplest approximation for recurrent layers proposed in Martens et al. (2018), and has been used for transformers (Grosse et al., 2023; Osawa et al., 2022; Pauloski et al., 2021; Zhang et al., 2019) - though without discussion or motivation. Notably, K-FAC-reduce is exact for certain settings, like deep linear networks with convolutions and average pooling, under the same conditions that K-FAC is exact for deep linear networks without weight-sharing, whereas the currently used K-FAC-expand is not. In practice, both approximations can be used in each setting and K-FAC-reduce generally has a lower computational and memory complexity than K-FAC-expand. We empirically verify this speed difference with a Wide ResNet on CIFAR-10. Moreover, we show that the two K-FAC variations applied to a graph neural network and vision transformer can reach a fixed validation metric target in \(50\)-\(75\)% of the steps of a first-order reference method, which translates into an almost equivalent decrease in wall-clock time. Although this does not demonstrate that K-FAC is a superior training algorithm, it indicates the potential of extending K-FAC to modern deep learning architectures. In general, K-FAC can be used as a drop-in Hessian approximation, e.g., in Laplace approximations for Bayesian deep learning (Daxberger et al., 2021; MacKay, 1992; Ritter et al., 2018) or in natural gradient variational inference (Khan et al., 2018; Zhang et al., 2018). To demonstrate this, we show that K-FAC-reduce can speed up automatic weight decay selection via approximate marginal likelihood optimisation, compared to the previously used K-FAC-expand.

## 2 Background

We consider the supervised learning setup with a dataset \(\) of \(N\) i.i.d. samples \(\{_{n},_{n}\}_{n=1}^{N}\), with \(_{n}^{D}\) and \(_{n}^{C}\), a neural net \(f_{}:^{D}^{C}\), parameterised with \(^{P}\), and a loss function \(\!:^{C}^{}\), which is often equivalent to a negative log likelihood, i.e. \((,f_{}())\!=\!- p(|f_{}())\). The DNN is usually trained with a (stochastic) gradient-based iterative update rule, in its simplest form \(_{t+1}=_{t}-_{_{t}}(,f_ {_{t}}())\), where \(t\) indicates the iteration and \(\) is the learning rate. If we assume a local quadratic approximation of the loss around the current parameter iterate \(_{t}\), we get a _preconditioned_ gradient update step, i.e.

\[_{t+1}=_{t}-\,_{t}^{-1}_{_ {t}}(,f_{_{t}}()),\] (1)

where \(_{t}\!\!^{P P}\) is the symmetric positive definite (p.d.) preconditioner. Setting \(_{t}\) to the Hessian of the loss w.r.t. the model parameters \(_{t}:=_{_{t}}^{2}(,f_{_{t}}())\) yields a simplified version of the classic Newton's method; however, since \(_{t}\) is expensive to compute, store, and not guaranteed to be p.d., it is not commonly used in deep learning.

### Second-order optimisation in deep learning

The term "second-order" is used ambiguously in the literature: it can refer to using second-order derivatives like the Hessian of the loss w.r.t. the parameters of the model, or approximations thereof, like the GGN, which however only contains second-order derivatives of the loss w.r.t. the model outputs and not the parameters. It can also refer to using some variation of the second moment of the (average) mini-batch gradient, which blurs the line between commonly used methods that are usually considered "first-order" methods like Adam (Kingma and Ba, 2015).

There have been numerous works on "second-order" methods in deep learning. For example, Martens (2010) proposes to use Hessian-free optimisation, Gupta et al. (2018) introduce an architecture-agnostic structured version of AdaGrad (Duchi et al., 2011) called Shampoo, Goldfarb et al. (2020) develop K-BFGS, a Kronecker-factored stochastic quasi-Newton method, and Ren & Goldfarb (2021) introduce Tensor Normal Training, which is similar to Shampoo, but approximates the Fisher; just to name a few among many more methods (Yang et al., 2022, 2023; Yao et al., 2021). While these methods have been shown to require fewer steps than commonly used first-order methods to reach similar performance in many settings, the mechanism behind their benefits is not fully understood. It could be orthogonal to unique properties of the true GGN or Fisher but might be more related to being able to adapt to the stochastic gradient noise (Kunstner et al., 2019) or to the fact that K-FAC with a specific but commonly used form of damping can be viewed as an approximation to gradient descent on neurons instead of weights (Benzing, 2022). In any case, generalising K-FAC to modern architectures is a promising direction because it is used in many training algorithms that can potentially speed up training and train a wider range of architectures. Moreover, it can be used to study the mechanisms behind the benefits of second-order methods in deep learning.

### Linear weight-sharing layers

Here, we present three examples of how the core building blocks of modern DNN architectures can be expressed as linear layers with weight-sharing. DNNs have a layered structure, i.e. they can be written as \(f_{}=f_{_{L}} f_{_{L}}  f_{_{1}}\), with \(=(_{1},,_{ },,_{L})\) and \(L\) the number of layers; \((,,)\) concatenates vector inputs to a larger vector. For a linear layer, we have \(f_{_{}}()=(_{}+_{})\), where \(\!\!^{P_{,}}\), \(_{}\!\!^{P_{,} P_{,}}\), \(\!\!^{P_{,}}\), \(_{}=\!((_{ }),_{})\!\!^{P_{}}\), and \(P_{}=P_{,}P_{,}\!+\!P_{,}\); \(()\) vectorises a matrix by concatenating its column vectors and \(\) is an element-wise nonlinearity. For simplicity, we subsume the bias into the weights. We are interested in linear layers with weights shared across an additional input dimension of size \(R\), i.e. with inputs \(\!\!^{R D}\).1 The linear layer is applied to this input as \(^{}\) and the weight matrix \(\) is shared across the additional first dimension, i.e. every element in \(\) contributes to each row of the output.

**Example 1: Attention.** The transformer architecture is popular in vision (Dosovitskiy et al., 2021), language (Brown et al., 2020), and spatio-temporal modelling (Bi et al., 2022). The inputs and outputs of the attention operation (Bahdanau et al., 2015) in transformers are exactly shaped as described above, where \(R\) is the sequence length, e.g. the number of tokens in language data or the number of image patches in vision. For the \(i\)-th row of the output matrix \(\) and the \(r\)-th row of the inputs \(\), the attention operation is generally defined as

\[_{i}=_{r=1}^{R}A_{i,r}_{r},\] (2)

where \(\!\!^{R R}\) is the _attention matrix_, which is normalised such that \(_{r=1}^{R}A_{i,r}=1\); alternatively, we can write it using matrix operations as \(=\). Intuitively, the outputs are a weighted sum of the inputs, where each weighting factor indicates the importance of the \(r\)-th sequence element to the element with index \(i\). The most common choice for \(\) is a variation of so-called self-attention, defined as \(=_{}(_{Q}^{}_{K}^{})\), where \(_{Q}\!\!^{P_{} D}\) and \(_{K}\!\!^{P_{} D}\) are weight matrices and \(_{}()\) applies the softmax function \(()_{i}:=(x_{i})/_{j=1}^{J}(x_{j})\) for an input \(\!\!^{J}\) to each row of the input matrix. This corresponds to a dot-product similarity measure of two independent linear projections of the input, allowing for asymmetric relationships between sequence elements. The two weight matrices \(_{Q},_{K}\), and any linear projection of the attention output are applied to all \(R\) sequence elements - they are linear weight-sharing layers.

**Example 2: Convolution.** A typical input to a \(2\)d convolution is an image \(^{I}\!\!^{C_{} H W}\), where \(C_{}\) is the number of input channels, \(H\) is the height, and \(W\) the width of the image. We choose a kernel of size \(K K\), such that we have a rank-\(4\) weight tensor of shape \(C_{} C_{} K K\), where \(C_{}\) is the number of output channels. For simplicity, following Grosse & Martens (2016), we assume that the convolution is performed with a stride of \(1\) and that the padding is chosen such that the height and width of the inputs are maintained, i.e. the output image will be \(^{I}\!\!^{C_{} H W}\).

The elements of the output tensor of the convolution are then defined as

\[o^{I}_{d,i,j}=_{c=1}^{C_{in}}_{k=1}^{K}_{k^{}=1}^{K}X^{I}_{c,i+k- 1,j+k^{}-1}W_{d,c,k,k^{}};\] (3)

the input elements \(X_{c,i+k-1,j+k^{}-1}\) are defined as \(0\) outside of the image's boundaries. Now, we can _unfold_ the input tensor to a matrix \(\!\!^{HW C_{in}K^{2}}\), where we have extracted and vectorised the \(K\!\!K\) sized patches for each of the \(H\!\!W\) spatial locations. The weight tensor can be reshaped to the matrix \(\!\!^{C_{} C_{}K^{2}}\). Using these quantities, we can write the convolution as \(\!=\!^{}\!\!^{HW C_{}}\). Again, we can recognise the canonical form of a linear weight-sharing layer - here, with \(R=HW\), i.e. the weights are shared across all spatial locations.

**Example 3: Graph neural network layer.** We follow Battaglia et al. (2018) since their formulation encompasses many graph neural network (GNN) layers. For the full example and notation, please refer to Appendix A. Once again, the core operations in these kinds of models can be expressed as linear weight-sharing layers. Here, the weights are shared across all \(R=N_{V}\) nodes or \(R=N_{E}\) edges of a graph, depending on whether node or edge features are being updated.

### The generalised Gauss-Newton and the Fisher matrix

To avoid computing the Hessian, approximations like the generalised Gauss-Newton matrix (GGN) are usually used in deep learning (Botev et al., 2017; Martens, 2010; Schraudolph, 2002). Defining \((f_{}(_{n}))\!:=\!_{f_{}}^{2} (_{n},f_{}(_{n}))\!\!^{C C}\), we have

\[()=_{n=1}^{N}}f_{}( _{n})^{}(f_{}(_{n}))}f_{ }(_{n}),\] (4)

which is equivalent to the Hessian when \(f_{}\) is a linear model or the residual \(g^{-1}(f_{}(_{n}))-_{n}\) is zero for all data points, where \(g^{-1}\) is the inverse link function, e.g. the softmax function for classification. For all likelihoods of the exponential family with natural parameterisation the GGN coincides with the Fisher information matrix (Martens, 2014; Wang, 2010). This is, for example, the case for the common cross-entropy loss corresponding to a categorical likelihood and the mean-square error loss corresponding to a Gaussian likelihood. Setting \(_{t}\) in Equation (1) to the Fisher leads to natural gradient descent (Amari, 1998). We will only state our derivations for the GGN explicitly, but they follow analogously for the Fisher.

### Kronecker-Factored Approximate Curvature

The GGN is a semi-p.d. approximation to the Hessian, of size \(P P\), which is still prohibitively large for DNNs. K-FAC was proposed as an efficient approximation to a neural network's Fisher (Heskes, 2000; Martens and Grosse, 2015) and GGN (Botev et al., 2017) matrix.2 First, we only approximate the GGN for each layer, ignoring cross-layer interactions, which results in a block-diagonal matrix. We define \(}}(_{n})\!:=\!}}f_{}( _{n})\!=\!}}f_{}(_{n})}}s_{,n}\!\!^{C P_{}}\) as the Jacobian of the model outputs w.r.t. the parameters of the \(\)-th layer. Now we can write \(_{,n}\!=\!_{}_{,n}\!=\!(_{,n}^{} _{_{,}})\;(_{})\) and with this we have \(}}s_{,n}\!=\!_{,n}^{} _{_{,}}\). Additionally, by defining \(_{,n}\!:=\!}}f_{}(_{n})^{} \!\!^{P_{,} C}\) as the transposed Jacobian of the model outputs w.r.t. the pre-activations of the \(\)-th layer, we note that \(}}(_{n})^{}\!=\!(_{,n}^{} _{_{,}})^{}_{,n} \!=\!_{,n}_{,n}\). K-FAC approximates the GGN of the \(\)-th layer as

\[(_{})_{n=1} ^{N}_{,n}_{,n}^{}]}_{=:_{}} ^{N}_{,n}(f_{}(_{n}))_{,n}^{}]}_{=:_{}},\] (5)

where we have replaced the (transposed) Jacobians in the definition of the GGN in Equation (4) by \(_{,n}_{,n}\) and approximated the sum of Kronecker products with a Kronecker product of sums.3

## 3 K-FAC for linear weight-sharing layers

In this section, we propose to distinguish between two different linear weight-sharing settings when applying K-FAC. The two settings motivate two corresponding approximations, _K-FAC-expand_ and _K-FAC-reduce_. However, _both approximations can be applied in each setting_, see Listing 1 for an illustration with code. More details on the derivations and the proofs for Proposition 1 and Proposition 2 can be found in Appendix B.

### The expand and reduce settings

Within a network, linear layers with weight-sharing can be classified based on the point where the weight-sharing dimension is aggregated. Specifically, there are three possible aggregation points, which will define the setting for the \(\)-th layer:

1. _Before_ the \(\)-th layer, i.e. \(_{,n}\!\!^{R P_{,}}\) is reduced to \(}_{,n}\!\!^{P_{,}}\) before being multiplied with the weight matrix of the layer \(_{}\). \(\) We are in the setting of a regular linear layer and no additional considerations are necessary when using K-FAC.
2. _After the per-example loss_, i.e. there will be \(N\!\!R\) labels and outputs of the model. The per-example loss is applied to each of the \(N\!\!R\) output-label pairs. \(\) This is the _expand_ setting, as the loss for each of the \(N\) data points is expanded with \(R\) terms.
3. _In between_ the pre-activation of the \(\)-th layer \(_{,n}\!\!^{R P_{,}}\) and the model output \(f_{}(_{n})\), i.e. before the final aggregation over the per-example losses. \(\) This is the _reduce_ setting, as all weight-sharing dimensions have been reduced during the forward pass.

If we assume a single weight-sharing dimension which will be reduced once, the number of loss terms determines which setting applies to all linear weight-sharing layers within the model. So for our purposes, _we can identify the setting we are in simply by looking at the form of the loss_.

### K-FAC-expand

The expand setting can be characterised by a loss with \(N\!\!R\) terms, similar to \(N\!\!R\) i.i.d. examples, \(_{}(f_{},)\!:=\!-_{n=1}^{ N}_{r=1}^{R} p(_{n,r}|f_{}(_{n})_{r})\), where \(f_{}(_{n})_{r}\) is the \(r\)-th row of the model output \(f_{}(_{n})\!\!^{R C}\) and \(_{n,r}\) is the \(r\)-th row of the label \(_{n}\!\!^{R C}\). A typical example of this type of loss function is language translation, where \(N\) is the dataset size and \(R\) is the sequence length.

We can express the Jacobian of the \(r\)-th row of the model output \(f_{}(_{n})^{R C}\) w.r.t. \(_{}\) as \(_{_{}}(_{n})_{r}\!=\!_{m=1}^{R}_{_{,n,m}}f_{}(_{n})_{r}_{_{}} _{,n,m}\). Since the weights \(_{}\) are shared across the weight-sharing dimension of size \(R\), we can write the \(r\)-th row of \(_{,n}\) as \(_{,n,r}\!=\!_{}_{,n,r}\) and we have \(_{_{}}_{,n,r}=_{,n,r}^{}\! \!_{P_{,}}\), as for a regular linear layer. We denote \(_{,n,r,m}:=_{_{,n,m}}f_{}(_{n})_{r}^ {}\).

Figure 1: **Visualisation of \(\) and K-FAC-reduce in the expand setting. The shown quantities are for a single layer within a deep linear network. We have \(N\!=\!4,R\!=\!2,P_{,}\!=\!8,P_{,}\!=\!8,\) and \(P_{}\!=\!P_{,}\!\!P_{,}\!=\!64\). As we have seen in Section 3.2, K-FAC-expand is exact for the expand case in this setting and K-FAC-reduce is not. For better visibility, the color scale is not the same for all quantities, except for the approximation error (_right_) where black represents zero.**Hence, we have \(_{_{}}(_{n})_{r}^{}=_{m=1}^{R}_{,n,m}_{,n,r,m}\). Deriving the approximation the same way as we would with \(N\) examples is not possible, since we cannot directly write each of the \(N\!\!R\) loss terms as a Kronecker product without any approximation. The Jacobians \(_{_{}}(_{n})_{r}\) could be approximated with a Kronecker product of sums, but this requires access to \(_{m=1}^{R}_{,n,r,m}\) and would not be exact in the simple settings we consider later. In contrast, what can be implemented in practice without additional backward passes and what has been used for convolutional neural networks (Grosse and Martens, 2016) and language transformers (Grosse et al., 2023; Osawa et al., 2022; Pauloski et al., 2021; Zhang et al., 2019) is

\[(_{})&=_ {n=1}^{N}_{r=1}^{R}(_{m=1}^{R}_{,n,m}_{ ,n,r,m})(f_{}(_{n})_{r})(_{m^ {}=1}^{R}_{,n,m^{}}^{}_{,n,r, m^{}}^{})\\ &_{n=1}^{N}_{r=1}^{R}_{m=1}^{R}(_{ ,n,m}_{,n,r,m})(f_{}(_ {n})_{r})(_{,n,m}^{}_{,n,r,m}^{ }),\] (6)

where all terms with \(m\!\!m^{}\) are ignored.4 Consequently, we can apply the regular K-FAC approximation over \(N\!\!R\) terms instead of the usual \(N\) terms, resulting in what we call the _K-FAC-expand_ approximation:

\[_{_{}}^{}: =_{n=1}^{N}_{m=1}^{R}_{,n,m} _{,n,m}^{}]}_{=_{}}^{N}_{m=1}^{R}_{r=1}^{R}_{,n,r,m}(f_{}(_{n})_{r})_{,n,r,m}^{}]}_{= _{}}\] (7)

In the context of convolutions, this approximation has been derived by explicitly stating the assumptions on the activations and pre-activation derivatives (Grosse and Martens, 2016).

For deep linear networks without any weight-sharing layers, K-FAC is known to be exact assuming a Gaussian likelihood (Bernacchia et al., 2018). While this holds for the full GGN/Fisher, we only focus on the block-diagonal case here. To motivate K-FAC-expand, we want to show that the same also holds for deep linear networks with weight-sharing in the expand setting. However, it does not even hold for simplistic transformer models since the dot-product attention mechanism in transformers directly correlates elements across the weight-sharing dimension; we discuss this in more detail in Appendix B. A deep linear network is defined as a model of the form

\[f_{}()=_{L}_{}_{1}= ,\] (8)

where \(\!\!^{D}\) and \(_{L}\!\!^{C P_{}}\), \(_{}\!\!^{P_{$}}} P_{ $}}}}\) (with \(P_{,}\!=\!P_{-1,}\)), and \(_{1}\!\!^{P_{$}}} D}\). Decomposing a single weight matrix \(\) into \(L\) separate ones is a common framework for theoretical analysis since it creates nonlinear training dynamics for gradient-based training algorithms, while still having analytical solutions (Bernacchia et al., 2018; Saxe et al., 2014). For an input \(\!\!^{R D}\) to this type of network, we can show:

**Proposition 1** (**Exactness of K-FAC-expand for deep linear network in the expand setting)**.: _For layer \(\) of a deep linear network defined as in Equation (8) and a Gaussian likelihood with p.d. covariance matrix \(\!\!^{C C}\), K-FAC-expand is exact in the expand setting._

### K-FAC-reduce

The reduce setting is characterised by a loss with just \(N\) loss terms, i.e. \(_{}(f_{},):=-_{n=1}^{N}  p(_{n}|f_{}(_{n}))\), and thus the weight-sharing dimension must have been reduced somewhere in the forward pass of the neural network \(f_{}\). A typical instance where this type of loss is used in a model with linear weight-sharing layers is image classification with a convolutional neural network or vision transformer.

Since \(_{,n}\!\!^{R P_{,}}\) is now a matrix, we have \(_{,n}\!=\!_{,n}_{}^{}\!\!^{R  P_{,}}\). Hence, \(_{_{}}_{,n}\) and \(_{_{,n}}f_{}(_{n})\) are now both multi-dimensional arrays. Luckily, we can avoid dealing with this directly by writing \(_{_{}}f_{}(_{n})=_{r=1}^{R}_{ _{,n},r}f_{}(_{n})_{_{}} _{,n,r}\), where \(_{,n,r}\!\!^{P_{,}}\) is the \(r\)-th row of \(_{,n}\) and \(_{,n,r}\!=\!_{}_{,n,r}\). Using this equivalence, we can approximate the (transposed) Jacobians in the GGN for layer \(\) as

\[_{_{}}f_{}(_{n})^{}=_{r=1}^ {R}_{,n,r}_{,n,r}_{r=1}^{R} _{,n,r}_{r=1}^{R}_{,n,r}.\] (9)

Here, we have approximated the sum of Kronecker products with a Kronecker product of sums over \(R\) terms of each of the \(N\) per-input Jacobians. This approximation has been proposed in Tang et al. (2021) to improve the efficiency of their proposed K-FAC variation for convolutions and in a different context for invariance learning with deep neural networks via differentiable Laplace approximations (Immer et al., 2022). We can apply the same approximation as usual to the sum over the \(N\) data points and call the resulting final approximation _K-FAC-reduce_ and can write it explicitly as

\[&}_{_{ }}^{}:=\\ &}\!_{n=1}^{N}\!(_{ r=1}^{R}_{,n,r})\!(_{r=1}^{R}_{,n,r}^{ })]}_{=}_{}}^{N}\!(_{r=1}^{R}_{,n,r})\!(f_ {}(_{n}))\!(_{r=1}^{R}_{,n,r}^{ })]}_{=}_{}}.\] (10)

To ensure that K-FAC-reduce is exact in the simple setting of Proposition 1, we have to choose an appropriate aggregation function \(z:^{R P_{,}}^{P_{,}}\). A simple and relevant case for which this holds is a _scaled sum_, i.e. \(z(_{,n})\!=\!c_{r=1}^{R}_{,n,r}\) with \(c\!\!\). Both vision transformers and convolutional neural networks with average pooling use scaled sums as the aggregation function (with \(c\!=\!1/R\)). With this, we can state the corresponding statement to Proposition 1.

**Proposition 2** (**Exactness of K-FAC-reduce for deep linear network in the reduce setting)**.: _For layer \(\) of a deep linear network (Equation (8)), a Gaussian likelihood with p.d. covariance matrix \(\!\!^{C C}\), and a scaled sum aggregation function, K-FAC-reduce is exact in the reduce setting._

Notably, for a deep linear model with convolution layers and average pooling, K-FAC reduce will be exact under the assumptions of Proposition 2, whereas the approximation proposed in the literature (Grosse and Martens, 2016) (which corresponds to K-FAC-expand) is not. Moreover, the calculation of \(_{}\) costs \((NRP_{,}^{2})\) for K-FAC-expand and only \((NP_{,}(P_{,}+R))\) for \(}_{}\) and K-FAC-reduce. The same holds for \(_{}\) and \(}_{}\), with complexities of \((NRCP_{,}(C\!+\!P_{,}))\) and \((NCP_{,}(C\!+\!P_{,}\!+\!R))\). The space complexity of K-FAC's overhead also differs by a factor of \(R\), with \((NR(P_{,}+P_{,}))\) for K-FAC-expand and \((N(P_{,}+P_{,}))\) for K-FAC-reduce.

Figure 2: **Visualisation of \(\) and K-FAC-reduce in the reduce setting. This is similar to Figure 1, but for the reduce setting, where K-FAC-reduce is exact and K-FAC-expand is not (Section 3.3).**

Experiments

In Section 4.1, we empirically validate the speed up of K-FAC-reduce compared to K-FAC-expand in line with their computational complexities with a Wide ResNet on CIFAR-10 and show how this scales with the batch size. Additionally, in Sections 4.2 and 4.3, we compare the two approximations based on their downstream optimisation performance. We validate that both K-FAC variations can reduce the number of required steps to achieve a fixed validation target compared to a well-tuned first-order reference optimiser. For this, we use a GNN and a vision transformer (ViT), two of the examples in Section 2.2. We leverage the codebase and validation metric targets of the MLCommons algorithmic-efficiency (AlgoPerf) repository5(Dahl et al., 2023), and use their target-setting runs as reference runs. Both K-FAC variations use the same learning rate schedule as those target-setting runs but the warmup and expected number of steps are multiplied by \(0.75\) to account for the fact that second-order methods tend to require fewer steps. Additionally, we tune the learning rate and damping for K-FAC via random search and set all other hyperparameters to the same values as the reference target-setting run. To exclude the adjusted learning rate schedule and the additional tuning of the learning rate as confounding factors, we additionally applied the same tuning to the reference runs; however, none of these runs hit the target validation performance. Despite these considerations, the experiments in Sections 4.2 and 4.3 are _not_ meant to demonstrate that K-FAC is a superior training algorithm (see Appendix C.1). However, the reference runs allow us to put the training performances of both K-FAC flavours into perspective and also indicate the potential of extending second-order methods like K-FAC to modern neural network architectures Lastly, to demonstrate a use case of K-FAC other than as a preconditioner, we automatically learn the weight decay hyperparameter by optimising a Laplace approximation of the marginal likelihood in Section 4.4. Our K-FAC implementation leverages the ASDL package (Osawa et al., 2023). See Appendix C for more details on the experiments and additional results.

### Update step speed with K-FAC-expand and K-FAC-reduce

To empirically validate the smaller computational complexity of K-FAC-reduce compared to K-FAC-expand, we time a single preconditioned gradient update step for a Wide ResNet on CIFAR-10 with both approximations and five different batch sizes on an NVIDIA V100 GPU (Table 1). As expected based on the complexities of the two approximations, we see increasing gains in speed with increasing batch size. For the largest tested batch size, \(2048\), K-FAC-reduce is faster than K-FAC-expand is for half the batch size. Moreover, K-FAC-expand runs out-of-memory (OOM) at a batch size of \(2048\). Notably, we implement both approximations with the naive unfold operation (c.f. Section 2.2), but K-FAC-reduce could be implemented even more efficiently by never explicitly allocating memory for the full unfolded layer input, which has been shown to lead to a further speed-up of about \(4.5\)(Dangel, 2023). To highlight the speed difference of the two K-FAC variations for language models, we compute a K-FAC GGN approximation for nanoGPT (Karpathy, 2023), a popular implementation of GPT-2 (Radford et al., 2019), on the full DART dataset (Nan et al., 2021). K-FAC-reduce is significantly faster than K-FAC-expand, which is currently used for transformers, only taking \(70\%\) of the time of K-FAC-expand. We expect this discrepancy to become even more pronounced for larger batch sizes and sequence lengths.

### Graph neural network on ogbg-molpcba

We use a basic GraphNetwork (Battaglia et al., 2018) with multi-layer perceptrons as the update functions (c.f. Appendix A) and train it on the ogbg-molpcba dataset (Hu et al., 2020) (Figure 3). The target validation metric is a mean average precision (mAP) of \(0.28098\). The reference algorithm is SGD with Nesterov momentum, with the tuned hyperparameters from the AlgoPerf target-setting runs. For both K-FAC variations, the statistics and preconditioner are updated every \(10\) iterations. We report the mean and standard error for five runs with different random seeds. The reference run reaches the target in \(31{,}887 2{,}070\) steps, whereas K-FAC-expand takes \(16{,}106 863\) and K-FAC

 K-FAC &  \\  & \(128\) & \(256\) & \(512\) & \(1024\) & \(2048\) \\  expand & \(0.24\) & \(0.38\) & \(0.75\) & \(1.36\) & OOM \\ reduce & \(0.17\) & \(0.24\) & \(0.43\) & \(0.63\) & \(1.17\) \\  

Table 1: Timing [s] of an update step with the two K-FAC variants for a Wide ResNet on CIFAR-10.

reduce \(15{,}566 429\) steps, about \(51\,\%\) and \(49\,\%\) of the reference run, respectively. In wall-clock time, the reference run takes about \(2.33 0.22\) hours, whereas K-FAC-expand takes \(1.32 0.11\) and K-FAC-reduce \(1.23 0.03\) hours, about \(57\,\%\) and \(53\,\%\) of the reference run, respectively. The reduced number of steps almost directly translates into reduced wall-clock time since the update step time is dominated by the data pipeline. K-FAC-reduce appears to be slightly faster in steps and wall-clock time than K-FAC-expand, but not significantly. Moreover, the runs with K-FAC-reduce have lower variance.

### Vision transformer on ImageNet

We train a ViT (Dosovitskiy et al., 2021) on LSVRC-2012 ImageNet (Russakovsky et al., 2015) and use NAdamW (Loshchilov and Hutter, 2019) with the hyperparameters from the AlgoPerf target-setting runs as the reference run (Figure 4); each run is repeated with three different random seeds. For K-FAC-expand and K-FAC-reduce, we update the K-FAC statistics and preconditioner every \(50\) iterations. The reference run reaches the target validation accuracy of \(0.77309\) in \(122{,}139 1119\) steps or about \(26.10 0.24\) hours. K-FAC-expand only takes \(87{,}464 245\) steps or \(19.57 0.13\) hours, and K-FAC-reduce \(92{,}550 731\) steps or \(20.28 0.00\) hours. This corresponds to about \(72\,\%\) of the steps and \(75\,\%\) of the time of the reference run for K-FAC-expand and \(76\,\%\) of the steps and \(78\,\%\) of the reference run's time for K-FAC-reduce. While the difference in speed between both K-FAC variations does become less relevant due to the infrequent K-FAC updates, one update step with K-FAC-expand takes about \(1.36\) as much as with K-FAC-reduce, which has a significant impact on the runtime when the K-FAC statistics are computed more frequently; see Figure 5 for a run where the update is performed every step. While K-FAC-expand requires slightly fewer steps and wall-clock time than K-FAC-reduce here, the difference does not appear significant.

### K-FAC for automatic hyperparameter selection via marginal likelihood optimisation

K-FAC has also been used for marginal likelihood maximisation in Bayesian neural networks using Laplace approximations (Daxberger et al., 2021; Immer et al., 2021). In particular, K-FAC can be used to obtain a scalable Laplace approximation and optimise its marginal likelihood with respect to regularisation parameters, e.g., weight decay, during training.

Figure 3: **Training results for a graph neural network on ogbg-molpcba. Both K-FAC variations require \(\!50\,\%\) of the steps which almost directly translates into reduced wall-clock time. The K-FAC statistics are updated every \(10\) steps. We show runs with five different random seeds for each method.**

Therefore, we compare K-FAC-reduce and K-FAC-expand in this setting following the setup of Daxberger et al. (2021) on CIFAR-10 with a Wide ResNet (Zagoruyko and Komodakis, 2016) and optimise a weight decay parameter per layer every five epochs during \(100\) epochs of training. Table 2 shows that K-FAC-reduce is significantly faster than K-FAC-expand in this setting, but K-FAC-expand provides better test negative log likelihoods (NLL) in both settings.

## 5 Discussion and conclusion

We have leveraged the idea of linear weight-sharing layers to generalise K-FAC to a broader range of architectures. This resulted in two distinct flavours of the approximation, which are exact for simple cases with deep linear networks in their respective setting. Maybe surprisingly, we see little difference between the two K-FAC variations in terms of their downstream optimisation performance. Moreover, in some settings, the overhead of K-FAC can be amortised or is negligible. This is, for example, the case when 1) the data loading dominates the algorithm's update step time (c.f. the GNN experiment), 2) when the K-FAC update frequency can be reduced sufficiently (c.f. the ViT experiment), or 3) when underutilised resources can be leveraged to compute K-FAC updates, for example when using pipeline parallelism for language models (Osawa et al., 2022). In these settings, the improvement in wall-clock time of K-FAC-reduce compared to K-FAC-expand is rendered insignificant. However, in applications where the K-FAC overhead still significantly impacts the runtime, e.g. in the online weight decay selection experiment in Section 4.4, or when there are memory constraints (c.f. Section 4.1), K-FAC-reduce can greatly reduce the runtime compared to the previously used K-FAC-expand. Some challenges remain when applying K-FAC in modern deep learning settings. For example, the memory overhead can be prohibitive, even for K-FAC-reduce, which could be addressed by sparse structured Kronecker factors (Grosse et al., 2023; Zhang et al., 2018). Also, numerical instabilities might arise in low-precision settings due to the need for matrix inversions or decompositions; this could be addressed by inverse-free methods relying on the K-FAC approximation (Lin et al., 2023).

   K-FAC & DA & NLL \(\) & Acc [\%] \(\) & Time [\%] \(\) \\  expand & ✗ & \(0.42\) & \(88.9\) & \(100\) \\  & ✓ & \(0.24\) & \(92.5\) & \(100\) \\  & ✗ & \(0.70\) & \(86.7\) & \(50.5\) \\ reduce & ✓ & \(0.35\) & \(93.5\) & \(50.5\) \\   

Table 2: Performance of K-FAC variants for marginal likelihood maximisation on CIFAR-10 with a Wide ResNet with and without data augmentation (DA).

Figure 4: **Training results for a vision transformer on ImageNet. The K-FAC statistics are updated every \(50\) steps. Due to amortising the costs, the reduced number of steps to the target translates into reduced wall-clock time. We show runs with three different random seeds for each method.**