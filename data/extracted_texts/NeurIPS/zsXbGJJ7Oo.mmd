# G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training

Che Liu1,2, Cheng Ouyang3,8,9 Sibo Cheng10

Anand Shah6,7 Wenjia Bai2,3,4  Rossella Arcucci1,2

###### Abstract

Medical imaging tasks require an understanding of subtle and localized visual features due to the inherently detailed and area-specific nature of pathological patterns, which are crucial for clinical diagnosis. Although recent advances in medical vision-language pre-training (VLP) enable models to learn clinically relevant visual features by leveraging both medical images and their associated radiology reports, current medical VLP methods primarily focus on aligning images with entire reports. This focus hinders the learning of dense (pixel-level) visual features and is suboptimal for dense prediction tasks (e.g., medical image segmentation). To address this challenge, we propose a novel medical VLP framework, named **G**lobal to **D**ense level representation learning (**G2D**), which aims to learn global and dense visual features simultaneously using only image-text pairs without extra annotations. In particular, G2D designs a **P**seudo Segmentation (**PS**) task, which enables the model to learn dense visual features during VLP. Notably, generating PS masks can be performed on the fly during VLP, which does not incur extra trainable parameters. With this simple yet effective idea, G2D achieves superior performance across 5 medical imaging tasks and 25 diseases. Particularly, in the segmentation task which requires dense visual features, G2D surpasses existing models even with just 1% of the training data for finetuning, compared to 100% used by other models. The code can be found in https://github.com/cheliu-computation/G2D-NeurIPS24/tree/main.

## 1 Introduction

In medical image analysis, learning global and dense visual representations typically requires labor-intensive and costly image and pixel-level annotations [1; 2]. Vision-language pre-training (VLP) attempts addressing this by aligning vision and language content using paired datasets [3; 4; 5; 6]. Although existing medical VLP methods excel at learning global visual features , they face challenges with dense visual features because the level of detail in text reports does not offer sufficientpixel-level supervision for learning these more detailed aspects. Existing medical VLP methods are categorized into two main types, as shown in Fig. 1:

* **Alignment-based Approaches**, which focus on aligning images with reports . Although methods like  align images with entire reports and text tokens, they struggle to learn dense, clinically relevant visual features. This is due to the ambiguous supervision targets provided by text tokens, which lack explicit relational pairing with image regions, as discussed in .
* **Reconstruction-based Approaches**, which learn representations by reconstructing masked images or reports using masked modeling techniques . However, they also lack success in capturing dense, clinically relevant visual features, as the reconstruction task primarily focuses on low-level patterns (texture, shape) rather than high-level semantics .

Despite advancements in medical VLP, limitations still exist. Current alignment approaches align image patches with text tokens in a brute-force manner and possibly cause misalignments when some word tokens (_e.g.,_ 'compatible' or 'unremarkable') lack direct visual counterparts, leading to ambiguous local alignments. Meanwhile, reconstruction-based approaches may ignore high-level image semantics. They are designed to recover low-level visual information such as intensity and texture, without accounting for high-level semantics . As a result, both approaches perform suboptimally for downstream tasks, such as semantic segmentation and visual grounding, which require learning of granular visual features that are aligned with high-level semantics.

While numerous VLP methods are designed to capture dense visual features for natural image datasets (e.g., ImageNet) they often struggle to transfer directly to medical images because they depend on a well-trained object detection model  or a well-aligned VLP model . In the medical domain, obtaining such pre-trained models is difficult as objects can be defined in various ways within a single medical image (e.g., based on organs, anatomical structures, or abnormal regions). Additionally, in medical domain, there is a lack of foundational VLP models that are both publicly accessible and are trained on sufficiently large image-text pairs that cover diverse medical imaging applications.

In response to the aforementioned challenges, we introduce a novel medical VLP approach termed G2D. This approach is designed to extract global and dense visual representations from radiography along with their associated radiology reports, with improved **feature granularity** and enriched **semantic information**. Central to our approach is a pretext task, **P**seudo **S**egmentation (PS), which is guided by a pseudo mask (segmentation target) derived from a carefully refined and filtered attention map. PS encourages the model to learn dense representations through a pixel-level pretext task that incorporates high-level semantics. This approach, in contrast to traditional methods that align image patches with text tokens, inherently mitigates the misalignment bias and allows learning of more representative features. Notably, the PS pretext task can be implemented to run concurrently with vision-language alignment, ensuring that the model can be trained end-to-end, contrasting with the two-stage training methods .

To evaluate the effectiveness of G2D relative to other state-of-the-art (SOTA) VLP approaches, we deploy the pre-trained model across a diverse range of downstream tasks, including medical image classification, semantic segmentation, object detection, as well as zero-shot image classification and

Figure 1: Comparing existing medical VLP methods with G2D: **a)** Alignment-based approaches lack dense (pixel-level) feature learning. **b)** Reconstruction-based approaches do not align with text, resulting in a deficiency in discriminative and clinically relevant visual features. **c)** The framework of **G2D (proposed)** learns dense, clinically relevant, text-aligned visual features through derived pseudo masks and image-text alignment. We use red text to highlight the deficiencies of existing methods and blue text to emphasize our advantages.

visual grounding, on six public large-scale CXR datasets. The experimental results demonstrate the superior performance of G2D over existing VLP approaches on these medical applications. Overall, our contribution is three-fold:

1. We introduce G2D, the first end-to-end encoder-decoder medical VLP approach designed to learn visual representations from the global level down to the dense level, supervised by paired radiology reports and a pixel-wise pretext task.
2. We carefully design a pretext task tailored for medical VLP, pseudo segmentation. It formulates a pseudo mask as segmentation target, allowing the model to learn dense visual representations in the pretext task which can benefit downstream dense visual tasks in medicine. The pseudo mask can be generated using a parameter-free processor that leverages the attention map derived from the visual representation associated with radiology reports.
3. We conduct comprehensive experiments to validate the efficacy of the proposed G2D approach, which outperforms peer approaches across five uni-modal and cross-modal downstream tasks.

## 2 Related Works

**Alignment-based Medical VLP.** Drawing inspiration from , aligning images with their corresponding textual descriptions in the latent space has led to notable advancements in VLP. Within the CXR domain, while ConVIRT  made an early attempt at employing bidirectional contrastive learning to globally align entire images with their paired reports, there remained room for refinement. GLoRIA  and MGCA  represent advancements in image-report alignment, introducing sophisticated global-local methodologies to the field [8; 9]. These approaches endeavor to establish correspondences between distinct image and text tokens. However, it is crucial to recognize that the granularity of token-level alignment could inadvertently introduce distortions to the medical context, potentially leading to misalignments, as illustrated by [20; 2]. Med-UniC  utilizes augmented text in VLP training to cultivate language invariance, with the goal of mitigating linguistic biases from VLP. Meanwhile, MedKLIP  and KAD  harness domain-specific knowledge from external annotated datasets to enhance textual information extraction. Notably, these approaches [20; 5; 21] are contingent upon external resources or extra data to optimize cross-modal representation learning, which could potentially constrain their generalizability.

**Reconstruction-based Medical VLP.** Several studies, including [12; 11; 22], have employed reconstruction of image and text tokens as a pretext task within VLP. Specifically, MRM  endeavors to reconstruct the original image from a masked version and simultaneously aims to regenerate the original text using both the masked image and text as inputs. Conversely, PRIOR  adopts a strategy that focuses on cross-modal representation by reconstructing images and sentences based on complete image and report inputs. An enhancement to the MRM  approach is proposed by , where token weights are adjusted during the reconstruction phase.

While these methods have demonstrated promising outcomes, the ability of the reconstruction pretext task to capture high-level semantic representations is limited, as shown in [14; 15; 13], and is further challenged by the absence of explicit semantic-related constraints in dense visual representation learning.

## 3 Methodology

The central aim of G2D is to learn global and dense visual representations from medical images under the supervision of their corresponding radiology reports. As illustrated in Fig 2 Left, G2D integrates two alignment strategies: vision-language alignment (VLA) that learns global representations, and pixel alignment (PA) that focuses on granular representation via a pixel-level pretext task, **P**seudo **S**egmentation (**PS**). The pseudo mask for PS is constructed through a parameter-free mechanism, which is operated alongside VLA. The PS pretext task enables G2D to derive dense representations at both encoder and decoder levels during pre-training. Moreover, the task head of the pretext task facilitates a smoother transfer for the pre-trained encoder to be applied to downstream segmentation tasks, reducing the gap between the dense visual representation learned from VLP and the needs of downstream dense visual tasks after VLP. This contrasts with previous methods [4; 8; 9; 21; 5; 6] that typically transfer only the pre-trained encoder, potentially leading to an information gap between the pre-training and downstream tasks.

### Vision-Language Contrastive Learning

We utilise a dual-encoder image-text contrastive approach following [4; 8; 9; 5]. Given a training set \(S\) consisting of \(N\) pairs of image-text \((v_{i},l_{i})\), where \(v_{i}\) denotes an image and \(l_{i}\) denotes a text report, \(i=1,2,3,...,N\), G2D employs an image encoder \(_{e}:^{D_{v}}\) to encode the image into an embedding of dimension \(D_{v}\), and a text encoder \(_{l}:^{D_{l}}\) to encode the text report into an embedding of dimension \(D_{l}\). The embedded image and text features can be denoted as \(=\{(_{1},_{1}),(_{2},_{2}),,(_{N},_{N})\}\), where \(_{i}=_{e}(v_{i})\) and \(_{i}=_{l}(l_{i})\).

As depicted in Fig. 2, G2D incorporates two alignment strategies: VLA and PA. For VLA, the model aims to learn global visual and text representations by pulling the embeddings of paired image-report samples closer, while distancing embeddings of unpaired samples, using a contrastive loss \(_{}\). The objective of contrastive learning is to predict \(N\) positive matched pairs \((v_{i},l_{i})\) and \(N^{2}-N\) negative pairs among \(N N\) possible image-text pair combinations . Subsequently, two non-linear vision and language projectors \(_{v}\) and \(_{l}\) transform \(_{i}\) and \(_{i}\) into the same dimension \(d\), where \(_{i}=_{v}(_{i})\), \(}_{i}=_{l}(_{i})\), and \(}_{i},}_{i}^{d}\). After obtaining image feature vectors \([}_{i}]_{i=1}^{N}\) and text feature vectors \([}_{i}]_{i=1}^{N}\) with the same dimension \(d\), the contrastive loss \(_{}\) can be formulated as:

\[_{}=-_{i=1}^{N}(}_{i}^{}}_{i}/)}{_{j=1}^{K} (}_{i}^{}}_{j}/)})\] (1)

\(\) denotes the temperature hyper-parameter empirically set to 0.07 following , and \(K N\) is the batch size.

### Pseudo Segmentation Mask Construction

Notably, although MedSAM  claims to build image-mask pairs, it requires box prompt inputs not available in the MIMIC-CXR  dataset. Designing a box prompt for each image is labor-intensive and unfeasible for this work, so we construct the pseudo mask based on attention maps.

**Attention Aggregation.** Inspired by CLIP , we incorporate an attention pooling mechanism in conjunction with the non-linear projector \(_{v}\) to derive a pixel-wise attention map. A dense feature map \(_{i}\) is extracted from the final convolutional layer before the pooling operation in the image encoder \(_{e}\), with the dimension \(C H W\). Here, \(C\) denotes the number of channels, while \(H\) and \(W\) represent the height and width of the feature maps. Subsequently, we reshape \(_{i}\) into a dimension of \(HW C\). In this way, \(_{i}\) can be interpreted as a sequence of pixel embeddings, where each token in this sequence represents the embedding of an individual pixel. The length of this sequence is defined by the number of channels, \(C\). A special token, \([]\), is introduced to aggregate all pixel embeddings through multi-head self-attention (MHSA) [25; 3]. This process offers an attention score matrix \(W_{i}^{h}\) for each pixel, with dimensions \(h H W\). Here, \(h\) signifies the

Figure 2: **Left:** Framework of G2D. **Right:** Pipeline for pseudo mask construction. We visualize the constructed pseudo mask and corresponding sentence in the radiology report in Sec A.7.

attention head number, and \(h\), with \(\) being the total number of attention heads. This attention score matrix characterizes the information exchange between pixels and semantics provided by the text [3; 18], and therefore it carries semantic information and is an ideal candidate for constructing the pretext pseudo mask. To derive the pseudo mask, we aggregate \(W_{i}^{h}\) across all attention heads to produce \(_{i}\), as described by:

\[_{i}=^{}W_{i}^{h}}{h}\] (2)

**Mask Filtering and Edge Smoothing.** After obtaining the aggregated attention map \(_{i}\), we upsample it to match the original image dimensions \(H^{{}^{}} W^{{}^{}}\). To remove pseudo mask regions in the background, we construct a body mask for each CXR image using a histogram-based thresholding approach, following common practice [26; 27]. Subsequently, all attention scores outside the body mask are set to zero. A threshold is applied to filter out low attention scores within the body mask, transforming \(_{i}\) into a binary mask. The threshold is determined at the \(85\%\) percentile of attention scores from \(_{i}\). The binary pseudo mask \(M_{i}\) is formulated as:

\[M_{i}^{j,k}=1&W_{i}^{j,k}\\ 0&,\]

\[j=1,2,3,...,H^{{}^{}},k=1,2,3,...,W^{{}^{}}\] (3)

To smooth the square-like boundary in the mask caused by upsampling, we apply bilateral filtering (BF)  to \(M_{i}\), resulting in a refined pseudo mask \(_{i}\), as shown in Fig. 2 Right. A comprehensive ablation study discussing the threshold and smoothing operation is presented in Sec. 4.5.

### Dense Visual Representation Learning through Pseudo Segmentation in VLP

While the global visual representation can be learned via VLA, dense representation often lacks direct alignment. To tackle this limitation, we introduce an image decoder, denoted as \(_{d}\), as shown in Fig. 2 Left. This decoder takes visual feature \(_{i}\) as input and utilises the pseudo mask \(_{i}\) as the supervisory signal for the pretext task. We employ the commonly used soft Dice loss and binary cross-entropy loss  to optimise this task. The training loss function for \(_{}\) is formulated as:

\[_{} =(_{}+_{}),\] \[_{} =_{i=1}^{K}_{j=1}^{H^{{}^{}}}_{k=1}^{W^{{}^{ }}}(1-_{i,j,k}^{{}^{}}_{ i,j,k})}{_{i,j,k}^{{}^{}}+_{i,j,k}}),\] \[_{} =-_{i=1}^{K}_{j=1}^{H^{{}^{}}}_{k=1}^{W^{{}^{ }}}[_{i,j,k}(_{i,j,k}^{{}^{}})+(1- _{i,j,k})(1-_{i,j,k}^{{}^{}})],\] \[_{i}^{{}^{}}=_{d}( _{i})\] (4)

The total loss for G2D is the sum of the VLA loss (Eq. 1) and the PA loss (Eq. 4):

\[_{total}=_{}+_{}\] (5)

It is worth noting that the pseudo mask is designed as a pixel-wise pretext supervisory signal. Although there is no manual annotation involved, the pseudo mask is constructed from the visual feature of the image encoder, which is pre-trained to align with radiology reports and thus contains clinical knowledge such as anatomical regions mentioned by the reports. In this sense, it can be a good surrogate target for learning pixel-wise semantic information. To demonstrate that the pseudo mask serves as a meaningful target for dense visual pre-training, we conduct an ablation study to use a perturbed pseudo mask with corrupt semantics for pre-training, and compare it to the proposed pseudo mask, as detailed in Table 8 and Sec A.6.

Experiments and Analysis

In this section, we compare our approach with SOTA medical VLP techniques. The implementation details and dataset training/test splits are reported in Sec A.3, A.4.

**Pretraining Dataset and Configuration** We utilise the MIMIC-CXR dataset [29; 24]. After preprocessing based on established protocols [9; 5], it provides 213,384 image-text pairs for pre-training. For the VLP part, we employ a standard ResNet-50 as the vision encoder \(_{e}\) and adopt the decoder part of a U-Net as the vision decoder \(_{d}\). We adopt ClinicalBERT  as the text encoder using configurations described in [5; 21]. In line with [9; 8], G2D is pre-trained for 50 epochs across 16 A100 GPUs, each accommodating a batch size of 128. The AdamW optimizer is employed with a learning rate set to \(2 10^{-4}\) and a weight decay of \(1 10^{-8}\). Additionally, a linear warm-up and a cosine annealing scheduler are incorporated in the training process.

### Downstream Task Datasets and Configurations

For downstream tasks, our focus is to evaluate the efficacy of G2D in learning granular visual features that can be used for localisation, vision-language understanding, and visual recognition tasks. We examine the capability and transferability of the learned cross-modal representations by using them for five distinct medical imaging tasks, covering a spectrum of 25 different diseases.

**Medical Image Segmentation.** This task utilises the RSNA  and SIIM  datasets, following preprocessing guidelines established in [9; 8]. We adopt U-Net  fine-tuning configurations following [8; 9]. The pre-trained vision encoder is frozen, while only the decoder parameters are updated during fine-tuning. Performance is assessed using the Dice score, following the evaluation protocol in [8; 9]. It is noteworthy that the original MedKLIP  uses a different configuration (_updating the vision encoder_) compared to other methods (_freezing the vision encoder_) [4; 8; 9; 6]. Therefore, in these experiments, we reference the results reported in , which reimplemented MedKLIP under a setting consistent with all other methods. For a fair comparison specifically with MedKLIP, we also reimplement G2D under MedKLIP's original setting, as reported in the Sec A.5.

**Medical Object Detection.** This task is conducted using the RSNA dataset  for Pneumonia Detection and the Object-CXR dataset  for Foreign Objects Detection, adhering to preprocessing methods from . We employ YOLOv3  for detection, using the pre-trained vision encoder and updating an additional detection head during fine-tuning. We report the mean Average Precision (mAP) with IoU thresholds between 0.4\(\)0.75. The setup for this task is in accordance with in .

**Zero-shot Medical Image Visual Grounding.** In accordance with , this task is conducted on the RSNA  and SIIM  datasets, using the same official data split and evaluation metrics. We employ CXR images as input and utilise the corresponding ground truth label maps for assessing the grounding performance, in terms of recall, IoU, and Dice score.

**Zero-shot Medical Image Classification.** In compliance with the guidelines set forth in [5; 21], we conduct this task on the RSNA , SIIM , CheXpert , and CXR14  datasets. For the RSNA and SIIM datasets, we employ the test set splits provided by MedKLIP , given that KAD  did not conduct experiments on these two datasets. For the CheXpert and CXR14 datasets [35; 36], we use the official test set splits to ensure a fair comparison with KAD . It is important to note that MedKLIP  creates its own test split rather than using the official test split. Hence, we do not use MedKLIP's splits in our experiments. We report the results using the macro average of AUC, F1, and ACC scores across all diseases.

**Medical Image Fine-tuned Classification.** In alignment with [5; 21], we use the CXR14 dataset , comprising 112,120 frontal-view X-rays from 30,805 patients, annotated for 14 diseases. We adhere to the official split for consistent evaluation, following KAD . It is worth noting that MedKLIP does not use the official data split. Hence, we refer to the results reported in KAD  rather than those from the original MedKLIP . To ensure a fair comparison with MedKLIP, we reimplemented G2D for this experiment under the MedKLIP configuration, as detailed in Sec A.5. CXR images are resized to \(256 256\). During fine-tuning, all model parameters are updated, including the pre-trained vision encoder and linear classifier. The AdamW optimizer is used with a learning rate of \(1 10^{-4}\) and a batch size of 64 for 50 epochs. Evaluation is based on the AUC score, adhering to the protocol outlined in [8; 9; 12].

**Medical Image Linear Classification.** In strict accordance with the configuration in [8; 4; 9], this task is conducted on the CheXpert , RSNA , and COVIDx  datasets. We only update a randomly initialized linear classification layer, while the pre-trained vision encoder remains frozen. For fair evaluation, we employ AUC scores on CheXpert and RSNA, along with accuracy metrics on COVIDx, as mentioned in [8; 9]. Apart from zero-shot image classification and visual grounding, we fine-tune using \(1\%,10\%,100\%\) of the training data for all downstream tasks. Detailed settings, including implementation and data splits, are outlined in Sec A.4.

### Performance on Visual Localisation Tasks

In Tab 1, following [16; 38], we evaluate G2D alongside other SOTA approaches on two pivotal visual localisation tasks: semantic segmentation and object detection. The aim is to assess the efficacy of the dense visual features learned.

Initially, we transfer only the encoder weights from the pre-trained G2D for the segmentation task, adhering to the protocols of [9; 8; 4; 6]. In this setup, our approach consistently achieves the highest performance across all data fractions for both SIIM  and RSNA datasets . To assess the impact of the visual decoder pre-trained with the PS pretext task, we transfer the weights of both the encoder and decoder from G2D for the segmentation task, resulting in striking outcomes. Remarkably, with just 1% of training data, G2D surpasses the performance of all peer methods, even those trained with a full 100% of training data. This observation underlines the fact that the pixel-level pretext task, PS, significantly improves the quality of dense visual features derived from VLP, which provide advantages for the downstream segmentation task.

In object detection, our method consistently outperforms existing methods across all data fractions for both RSNA and Object-CXR datasets [31; 33]. Notably, G2D achieves a 3.8% mAP on the Object-CXR dataset with just 1% of the data for fine-tuning, a significant leap from other methods that scarcely reach a 1% mAP.

These results highlight the efficacy of our proposed model, G2D, and the pretext task, PS, especially in semantic segmentation tasks that rely on dense visual features. PS not only enables G2D to learn visual representations in the encoder-decoder structure but also reduces the gap between pre-training and downstream tasks. By enhancing the encoder's ability to capture global and dense features simultaneously, PS surpasses existing approaches, proving particularly advantageous for object detection tasks that heavily rely on dense features .

### Performance on Vision-Language Understanding

In Tab 2, we evaluate the efficacy of G2D on vision-language understanding tasks, zero-shot visual grounding and zero-shot image classification. For the zero-shot visual grounding task, our proposed method outperforms peer approaches. Specifically, on the SIIM dataset , it achieves a leading Dice score of 5.1. This dominance persists in the RSNA dataset , where our method reaches a

  
**Tasks** &  &  \\ 
**Datasets** &  &  &  &  \\
**Methods** & 1\% & 10\% & 100\% & 1\% & 10\% & 100\% & 1\% & 10\% & 1\% & 100\% \\  Random Init & 9.0 & 28.6 & 54.3 & 6.9 & 10.6 & 18.5 & 1.0 & 4.0 & 8.9 & - & 0.5 & 4.4 \\ ImageNet Init & 10.2 & 35.5 & 63.5 & 34.8 & 39.9 & 64.0 & 3.6 & 8.0 & 15.7 & - & 2.9 & 8.3 \\  ConvVIRT  & 25.0 & 43.2 & 59.9 & 55.0 & 67.4 & 67.5 & 8.2 & 15.6 & 17.9 & - & 8.6 & 15.9 \\ GLoRA  & 35.8 & 46.9 & 63.4 & 59.3 & 67.5 & 67.8 & 9.8 & 14.8 & 18.8 & - & 10.6 & 15.6 \\ GLoRA-MIMIC  & 37.4 & 57.1 & 64.0 & 60.3 & 68.7 & 68.3 & 11.6 & 16.1 & 24.8 & - & 8.90 & 16.6 \\ MGCA  & 49.7 & 59.3 & 64.2 & 63.0 & 68.3 & 69.8 & 12.9 & 16.8 & 24.9 & - & 12.1 & 19.2 \\ M-FL-KDA  & 52.5 & 61.2 & 64.8 & 64.6 & 69.7 & 70.5 & 13.7 & 17.5 & 25.4 & - & 12.4 & 19.3 \\ MedKLIP  & 50.2 & 60.8 & 63.9 & 66.2 & 69.4 & 71.9 & 8.9 & 16.3 & 24.5 & - & 7.1 & 11.6 \\ 
**Ours (encoder)** & **62.6** & **63.1** & **66.8** & **70.9** & **72.6** & **75.1** & **15.9** & **21.7** & **27.2** & **3.8** & **13.1** & **20.4** \\
**Ours (encoder-decoder)** & **65.6** & **66.9** & **68.4** & **72.8** & **73.4** & **76.9** & & / & & \\   

Table 1: Results of semantic segmentation and object detection. Best results are highlighted in bold, with ‘-’ denoting mAP values \(<\) 1%. Methods with + use disease-level annotations. ‘\(\)’ indicates object detection not deployable with encoder-decoder architecture. The MedKLIP results in this table differ from the original work  because MedKLIP fine-tuned the encoder in their original study, whereas other methods froze the encoder. To ensure fairness, we reimplemented MedKLIP with the frozen encoder for comparison in this table. Additionally, for a fair comparison specifically with MedKLIP, we compare G2D with MedKLIP under its original configuration in Tab 7 and Sec A.5.

[MISSING_PAGE_FAIL:8]

### Ablation Studies

**Pseudo Segmentation vs. Reconstruction.** In Tab 4(a), we evaluate the impact of the proposed PS pretext task in comparison to pixel reconstruction and models without a decoder-level constraint. The model pre-trained with PS outperforms the other two approaches across all three downstream tasks, particularly in semantic segmentation. While the model pre-trained with a pixel reconstruction constraint exhibit improved performance compared to unconstrained variants, such models still underperform the model with the PS constraint. These results underscore the effectiveness of decoder-level pretext tasks and suggest that an emphasis on high-level semantics, derived from PS, is more beneficial than focusing on the low-level semantics from pixel reconstruction. The PS potentially reduces the disparity between features learned through VLP and those required by downstream semantic segmentation tasks. It also enables the model to acquire more representative features that are beneficial for various tasks.

**Threshold of Pseudo Mask Construction.** As shown in Tab 4(b), performance varies with different thresholds, with the 85% percentile threshold proving most effective across all three downstream tasks. Despite employing the Gaussian Mixture Model (GMM) for pseudo mask creation, as suggested by , its performance is still surpassed by the 85% percentile approach. This indicates that the original attention map might contain noise, and a higher threshold is beneficial for generating more effective pseudo masks.

Furthermore, Tab 4(d) highlights the importance of aggregating multi-head attention maps for mask construction. Given the absence of explicit semantic supervision in the PS pretext task, not aggregating these maps leads to the creation of multiple pseudo masks. This excess of masks introduce ambiguous training objectives for VLP.

**Impact of Mask Refinement.** Refinement of the pseudo masks affects the model's efficacy, as shown in Tab 4(f). Performance tends to decrease when either the body mask is omitted or edge smoothing is not applied. However, integrating both these strategies, as we implement in G2D, yields optimal results. This underscores the vital role of pseudo mask refinement in enhancing model performance.

**Ablation on Hyperparameters.** We further ablate the number of attention heads and projector dimensionality. Performance improves with more attention heads, peaking at 3 before slightly declin

  
**Datasets (Metric)** &  &  &  \\
**Methods** & 1\% & 10\% & 100\% & 1\% & 10\% & 100\% & 1\% & 10\% & 100\% \\  Random Init & 56.1 & 62.6 & 65.7 & 58.9 & 69.4 & 74.1 & 50.5 & 60.3 & 70.0 \\ ImageNet Init & 74.4 & 79.7 & 81.4 & 74.9 & 74.5 & 76.3 & 64.8 & 78.8 & 86.3 \\  ConVIRT  & 85.9 & 86.8 & 87.3 & 77.4 & 80.1 & 81.3 & 72.5 & 82.5 & 92.0 \\ GLoRIA  & 86.6 & 87.8 & 88.1 & 86.1 & 88.0 & 88.6 & 67.3 & 77.8 & 89.0 \\ GLoRIA-MIMIC  & 87.1 & 88.7 & 88.0 & 87.0 & 89.4 & 90.2 & 66.5 & 80.5 & 88.8 \\ MCCA  & 87.6 & 88.0 & 88.2 & 88.6 & 89.1 & 89.9 & 72.0 & 83.5 & 90.5 \\ MRM  & 88.5 & 88.5 & 88.7 & 91.3 & 92.7 & 93.3 & 66.9 & 79.3 & 90.8 \\ MedKILP\({}^{*}\) & 86.2 & 86.5 & 87.7 & 87.3 & 88.0 & 89.3 & 74.5 & 85.2 & 90.3 \\ 
**Ours** & **89.7** & **90.4** & **91.1** & **92.2** & **92.9** & **93.6** & **76.6** & **88.2** & **93.4** \\   

Table 4: Linear classification results for CheXpert, RSNA, and COVIDx datasets with 1%, 10%, and 100% training data. The best results are highlighted in bold. Methods with \(\) leverage disease-level annotations for pre-training. The evaluation metric follows .

     } & SIM & RSNA & CXR14 \\  & Dice & mAP & AUC \\  
**85\% percentile** & **66\(\)1.7** & **15\(\)0.8** & **79\(\)1.2** \\ 
75\% percentile & 63.0\(\)1.4** & 14.2\(\) & 73.3\(\)2.0 \\ 
**median** & 58.8\(\)1.6 & 12.5\(\)2.3 & 75.6\(\)1.1 \\  GMM  & 59.24.1 & 12.9\(\)1.4 & 75.2\(\)1.9 \\      &    & 
  \\  
**85\% percentile** & **66\(\)1.7** & **15\(\)0.8** & **79\(\)1.2** \\ 
75\% percentile & 63.0\(\)1.4** & 14.2\(\) & 73.3\(\)2.0 \\ 
**median** & 58.8\(\)1.6 & 12.5\(\)2.3 & 75.6\(\)1.1 \\  GMM  & 59.24.1 & 12.9\(\)1.4 & 75.2\(\)1.9 \\      &   \\    
 

Table 5: Results of various ablation experiments. The best results are bolded.

ing at 4 (Tab 5e). Optimal segmentation and classification results are achieved with 128-dimensional projectors. While 256 dimensions provide slight benefits for object detection, they reduce performance in other tasks (Tab 5c). Projectors of 512 dimensions do not yield further gains. Thus, we select 3 attention heads and 128-dimensional projectors for an optimal balance of complexity and effectiveness.

## 5 Conclusion

In this study, we introduce G2D, a novel medical VLP framework for learning global and dense-level representations. Our proposed pixel-level pretext task, pseudo segmentation, leverages a refined attention map to predict a pseudo mask, capturing dense visual features during VLP without requiring additional trainable parameters for its construction. Our model pretrained with this pretext task achieves superior performance across five diverse medical imaging tasks and outperforms methods pretrained with annotated data [5; 21], especially in semantic segmentation. Specifically, on the SIIM  dataset, G2D, when fine-tuned with only 1% of the training data, outperforms other medical VLP approaches that utilize the full 100% training set. We anticipate that G2D will inspire further exploration of novel and clinically-guided pretext tasks for medical VLP.