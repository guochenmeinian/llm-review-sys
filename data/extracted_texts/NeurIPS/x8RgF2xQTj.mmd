# Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks

Balint Mucsanyi

University of Tubingen

b.h.mucsanyi@gmail.com &Michael Kirchhoff

University of Tubingen

&Seong Joon Oh

University of Tubingen

Tubingen AI Center

###### Abstract

Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one source of uncertainty. This paper presents the first benchmark of uncertainty disentanglement. We reimplement and evaluate a comprehensive range of uncertainty estimators, from Bayesian over evidential to deterministic ones, across a diverse range of uncertainty tasks on ImageNet. We find that, despite recent theoretical endeavors, no existing approach provides pairs of disentangled uncertainty estimators in practice. We further find that specialized uncertainty tasks are harder than predictive uncertainty tasks, where we observe saturating performance. Our results provide both practical advice for which uncertainty estimators to use for which specific task, and reveal opportunities for future research toward task-centric and disentangled uncertainties. All our reimplementations and Weights & Biases logs are available at https://github.com/bmucsanyi/untangle.

## 1 Introduction

When uncertainty quantification methods were first pioneered for deep learning [12; 29], their task was simple: giving one total uncertainty estimate. The recent demand for trustworthy machine learning  created new requirements, mostly centering around disentangling the above predictive uncertainty into aleatoric (data-inherent and irreducible) and epistemic (model-centric and reducible) components [11; 52; 47]. Such disentangled estimators are needed for multiple modern applications: Out-of-distribution detection needs to filter unseen samples with high epistemic uncertainty without being confounded with seen samples with high aleatoric uncertainty , and active learning uses individual aleatoric and epistemic estimates to select the most efficient samples to learn from [28; 35].

However, recent advances towards such disentangled uncertainties are primarily theoretical and supported by only small-scale experiments [47; 53; 37]. Conversely, larger-scale benchmarks evaluate methods w.r.t. only one uncertainty component and do not test for undesirable side effects on other components [13; 40]. There is currently no study that evaluates which component(s) each method captures in practice and which it does not - which is often contrary to their original intuition.

Our work provides a comprehensive benchmark of the vast recent landscape of uncertainty methods and tasks. We reimplement nineteen uncertainty quantification methods in up to fourteen ways and evaluate each on thirteen practically defined tasks on ImageNet-1k  and CIFAR-10 . This includes recent information-theoretical and Bregman decomposition formulas that intend to disentangle total uncertainties into aleatoric and epistemic components [57; 42; 11]. We reveal that none of the existing approaches achieve disentanglement in practice. Most proposed pairs of estimators are highly internally correlated (rank corr. \( 0.78\)) and fail to unmix aleatoric and epistemic uncertainty (Section 3.1). We also find that specialized tasks (Sections 3.2 and 3.3) are harder to solvethan previous predictive uncertainty tasks, on which we observe saturating performance (Section 3.4). Based on these insights, we uncover a promising path for future disentangled uncertainty estimates: combining individual estimators that strongly reflect one type of uncertainty while being (almost) unrelated to the other.

These findings emphasize the importance of clearly specifying the task one wants to solve with an uncertainty estimator and tailoring the estimator to it. We anticipate that our quantitative insights will drive the field toward developing more disentangled and specialized uncertainty estimators.

## 2 Benchmarked Methods

This section provides an overview of the benchmarked uncertainty estimators and disentanglement formulas. We reimplement all nineteen methods and explain implementation details in Appendix A.

### Uncertainty Quantification Methods

We consider a classification setting with a discrete label space of \(C\) classes. On top of the eight supervised uncertainty quantification methods from Kirchhoff et al. , we reimplement another eleven methods to encourage diversity and general applicability of our findings. Below, we categorize the benchmarked approaches into distributional and deterministic methods.

#### 2.1.1 Distributional methods

Distributional methods model a second-order predictive distribution \(q()\) over class probability vectors \(^{C-1}\) for an input \(\). For example, \(q()\) can correspond to a Bayesian posterior on the simplex, \(p(,)\), induced by a weight-space posterior \(p() p()\,p()\) when training on dataset \(\).

**Spectral-Normalized Gaussian Processes (SNGP)** represent the \(q()\) distributions by approximating a Gaussian process (GP) over the classifier _output_, aided by spectral normalization. We also benchmark the last-layer GP without spectral normalization. The last-layer **Laplace Approximation** and **Stochastic Weight Averaging - Gaussian (SWAG)** both model a Gaussian parameter distribution in a post-hoc fashion that induces the \(q()\) distributions. The Laplace approximation does so by fitting the parameter-space Gaussian w.r.t. the local curvature around the MAP estimate, whereas SWAG samples model weights via checkpointing and fits an empirical distribution. Similarly, **Heteroscedastic Classifiers (HET)** and **Latent Heteroscedastic Classifiers (HET-XL)** predict a heteroscedastic Gaussian distribution over the _logits_ and pre-logit _embeddings_, respectively. Evidential deep learning methods for classification  directly learn a

Figure 1: Decomposition formulas like in Eq. (1) decompose second-order distributions into individual estimates for epistemic and aleatoric uncertainties. However, we find that the estimates are internally highly correlated. The density plot on the right shows this for the epistemic and aleatoric uncertainty estimates obtained from decomposing deep ensemble uncertainties on ImageNet-1k. This means that they capture the same notion of uncertainty in practice as opposed to two disentangled ones.

Dirichlet distribution over the output probability vectors. Following Ulmer et al. , we refer to the method of Sensoy et al.  as **Evidential Deep Learning (EDL)** and that of Charpentier et al.  as the **Posterior Network (PostNet)**.

**MC Dropout** and **Deep Ensemble** do not construct second-order predictive distributions \(q()\) explicitly. Instead, they sample from them by \(M\) repeated forward passes with randomly switched off activations or by training \(M\) models, respectively. The **Heteroscedastic Classification Neural Network (HetClassNN)** uses the uncertainties from MC Dropout for epistemic uncertainty and models an input-conditional heteroscedastic logit variance for aleatoric uncertainty. The **Shallow Ensemble** is a lightweight approximation of the Deep Ensemble with a shared backbone and \(M\) output heads.

Practical tasks like threshold-based rejection often need a scalar uncertainty value \(u()\) instead of a second-order predictive distribution \(q()\). To this end, **uncertainty aggregators** compile the above distributions into scalar uncertainty estimates. Several methods exist for this aggregation, such as calculating the Bayesian Model Average (BMA) \(}():=_{q( )}[]\) and using its entropy as the uncertainty estimate \(u()\) or quantifying the variance of \(q()\), as often seen in ensembles. While many distributional methods are proposed with a specific aggregator, we show in Appendix D.5 that they do not always behave as expected and limit performance. To remove this confounder, we consider fourteen aggregators (Appendix D) for distributional methods and use the best-performing one.

#### 2.1.2 Deterministic Methods

Deterministic methods  directly output scalar uncertainty estimates \(u()\) instead of modeling a second-order predictive distribution \(q()\) over class probability vectors.

**Loss Prediction** employs an additional MLP head for \(u()\) that estimates the loss of the network's prediction \(()^{C-1}\), reflecting a notion of (in-)correctness. **Correctness Prediction** is a special variant for classification where \(u()\) predicts how likely the predicted class \(:=_{c\{1,,C\}}_{c}()\) is to be the correct class \(y\), i.e., \(p(=y)\).

**Deterministic Uncertainty Quantification (DUQ)** learns a latent mixture-of-RBF density on the training dataset and outputs as \(u()\) how close an input's embedding is to the mixture means. The **Mahalanobis** method  builds a similar latent mixture of Gaussians in a post-hoc fashion. It also perturbs the inputs adversarially to separate in-distribution (ID) and out-of-distribution (OOD) samples. The **Deep Deterministic Uncertainty (DDU)** method  combines the spectral normalization of SNGPs with the latent density of the Mahalanobis method. **Temperature Scaling** post-hoc calibrates the predicted probability vectors with a temperature scalar.1 As a **Baseline**, we use a deterministic single-point network trained with the cross-entropy loss.

### Uncertainty Decomposition Formulas

So far, we only considered uncertainty estimators that (sometimes after aggregating) output a single estimate \(u()\). A second strain of literature outputs not only one estimate but decomposes the \(q()\) of distributional methods into multiple estimators that each intend to quantify one source of uncertainty, such as epistemic and aleatoric uncertainty . We benchmark two prominent approaches to obtain such pairs of estimators: the **information-theoretical (IT)** and the **Bregman** decomposition . In the main paper, we focus on the IT decomposition due to its widespread use. The definition and results of the Bregman decomposition are shown in Appendix B.

The IT decomposition decomposes the entropy of the predictive distribution \(p(y)= p(y,)\ q( )\) into an aleatoric and an epistemic component:

\[_{p(y)}(y)}_{}=_{q()} [_{p(y,)}(y) ]}_{}+_{p(y, )}(y;)}_{},\] (1)

where \(p(y,)=(y;)=_{y}\), \(p(y,)=p(y,)q()\), \(_{p(y)}(y)\) is the entropy, and \(_{p(y,)}(y;)\) is the mutual information. Intuitively, the aleatoric component represents the spread of the labels that the plausible predictions in the posterior have on average. In contrast, the epistemic component only captures the disagreement of the predictions \(p(y,)\) in the second-order predictive distribution \(q()\). For evidential deep learning methods with Dirichlet \(q()\) distributions, closed-form expressions exist for each term of Eq.1, whereas other approaches require Monte Carlo approximations .

The key goal behind these decompositions is **uncertainty disentanglement**: The aleatoric component should capture aleatoric and only aleatoric uncertainty, and the epistemic estimator should reflect epistemic and only epistemic uncertainty. In particular, this entails that both components need to be sufficiently uncorrelated. See AppendixE for more details and a formal definition.

## 3 Experiments

We now investigate our main research question: Does any approach give disentangled uncertainty estimators (Section3.1)? Then, we go into each individual type of uncertainty and investigate which estimator practically performs the best on epistemic (Section3.2), aleatoric (Section3.3), and predictive uncertainty tasks (Section3.4). Lastly, we draw conclusions across all tasks (Section3.5) and benchmark the robustness of current uncertainty estimators (Section3.6).

To provide even grounds, we reimplement each method and provide it as an easy-to-use uncertainty wrapper that can be added to arbitrary timm models2. In this paper, we use pretrained ResNet-50 backbones and train each approach for \(50\) ImageNet-1k  epochs with a training pipeline following Tran et al.. The CE baseline converges to an accuracy of \(0.785\) with this strategy. Since the DUQ method has memory and stability issues on ImageNet, in Section3.7, we repeat all experiments on CIFAR-10  with the WideResNet 28-10 architecture, following Liu et al.. We only report the other 18 methods on ImageNet. We search for ideal hyperparameters and an early stopping checkpoint for each method by tracking the validation performance. We then run the best hyperparameters across five seeds and report mean, minimum, and maximum test performance. This overall takes 1.5 GPU years on RTX 2080 Ti GPUs. We report the main results in the paper and go into more detail for, e.g., different uncertainty aggregators in the appendix. We also publish all of these metrics and their logs.3

### Decomposition Formulas Fail to Disentangle Aleatoric and Epistemic Uncertainty

We first study if decomposition formulas, IT or Bregman, yield disentangled estimators. Since they decompose second-order predictive distributions \(q()\), we analyze distributional methods and no deterministic methods in this section.

Fig.1 reveals a simple failure: The decomposed aleatoric and epistemic uncertainty estimates are strongly correlated, being high or low iff the other component is high or low. These severe internal correlations prohibit the estimators from capturing semantically different sources of uncertainty and hinder applications that require unconfounded uncertainty estimates, such as active learning.

Figure 2: Rank correlation between the aleatoric and epistemic estimates obtained by the IT decomposition on ImageNet (left) and CIFAR-10 (right). The two uncertainty components are strongly correlated for most methods, violating a necessary condition of their disentanglement.

The behavior of deep ensembles is just one example. Figure 1(a) shows that aleatoric and epistemic estimates obtained via the IT decomposition are highly rank correlated (rank corr. \([0.78,0.99]\)) for all distributional methods that we benchmark. This holds similarly on CIFAR-10 (Figure 1(b)), as well as for the Bregman decomposition (Appendix B) and also does not considerably lower when we artificially add more epistemic uncertainty into the dataset, see Appendix C.2. Often, the components are even linearly correlated; see the Pearson correlation results in Appendix C.4.

A part of these correlations is inevitable: On ImageNet, regions with aleatorically uncertain images are undersampled compared to regions without aleatoric uncertainty and thus also more epistemically uncertain (see Fig. 1). This means that ImageNet has a level of inevitable correlation between epistemic and aleatoric uncertainty estimates. We quantify this inevitable correlation via the rank correlation between the GT aleatoric uncertainty (i.e., the entropy of the GT label distribution) and the models' epistemic uncertainty given by the Bregman decomposition in Appendix B.4. This gives levels of inevitable correlation for the Bregman decomposition that are at most \(0.45\). Further, we show in Section 3.3 that there are pairs of uncertainty estimators where one performs well on aleatoric and the other on epistemic uncertainty, with a notably low rank correlation of \(0.15 0.01\). Thus, the severe correlations exceeding \(0.78\) are shortcomings of the decomposition formulas and not inherent properties of the ImageNet dataset.

In conclusion, decomposition formulas of various forms applied to various second-order distributions produce uncertainty estimators that are so highly correlated that they hardly capture the different individual notions of aleatoric and epistemic uncertainty that they are intended to capture.

### Epistemic Uncertainty: Specialized Uncertainty Estimators Detect OOD Inputs the Best

If decomposition formulas cannot yield epistemic and aleatoric uncertainty estimates, which methods can? For the rest of the paper, we widen the scope and include not only the aleatoric and epistemic estimators defined by the decomposition formulas but also arbitrary aggregators of second-order distributions, as well as deterministic methods. This section tests which of these estimators represents epistemic uncertainty, measured by an out-of-distribution (OOD) detection task [17; 37]. We create a 50/50 dataset of in-distribution (ID) and OOD samples, with ID samples getting class 0 and OOD samples getting class 1. We quantify via a binary classification AUROC if uncertainty estimates are higher on OOD samples than on ID samples. We use ImageNet-C  with all of its corruptions of severity level two as OOD data. The severity level two is far enough out-of-distribution to deteriorate the ImageNet accuracy by 27% (Section 3.6).

Fig. 2(a) shows that the methods differ greatly in their ability to detect OOD samples and, thus, in their alignment with epistemic uncertainty. The Mahalanobis method performs best. This is likely because it is the only method trained specifically for OOD detection with ImageNet-C corruptions of severity level two. We find its advantage already vanishes when changing the task to severity level three (Appendix H.3). A method with a similar latent density intuition and distance-awareness induced by spectral normalization, DDU, is the worst at telling ID and OOD samples apart (\(=0.675\)). Additionally, the best-performing aggregators for the second-order distributions, the performance of which Fig. 2(a) shows, are often not the disagreement-based aggregators that decomposition formulas propose for epistemic uncertainty tasks (Appendix D.5). These insights highlight that the practical

Figure 3: Performance of uncertainty quantification methods on epistemic (left) and aleatoric (right) uncertainty tasks on the ImageNet validation dataset.

tailoring of uncertainties to a specific uncertainty task of interest, as done by the Mahalanobis method, weighs more than high-level intuitions, which, e.g., decomposition formulas are based on.

### Aleatoric Uncertainty: No Method With Outstanding Performance

The previous experiment isolated the epistemic capabilities of uncertainty estimates. We now evaluate how well the benchmarked models predict aleatoric uncertainty. We follow Tran et al.  and Kirchhoff et al. [24; 25] and use the disagreement of human annotators as ground truths for aleatoric uncertainty. ImageNet-ReaL  (and CIFAR-10H ) queries multiple annotators for labels on each image. We showcase some examples in Appendix J. We use the entropy of the soft-label distributions per image as GT aleatoric uncertainties. We then calculate the rank correlation between the methods' uncertainty estimates and the GT label entropies across all images. We do not use an AUROC here because the GT values are continuous, but provide binarized AUROCs for direct comparability of aleatoric and epistemic uncertainty performance results in Appendix H.4.

Fig. 2(b) shows that almost all methods lie within a correlation of \([0,37,0.46]\). Note that the best achievable rank correlation is not one since the GT aleatoric data contains ties. While it is unknown how high the best achievable rank correlation is, the fact that there are consistent improvements across the methods hints at the fact that further performance gains are far from saturated. The method that sticks out on the low end of the spectrum is Mahalanobis, which is uncorrelated with the GT aleatoric uncertainty. This is, in fact, a strength: Mahalanobis estimates reflect epistemic uncertainty while being non-informative of aleatoric uncertainty. Combining this with a second estimator for aleatoric uncertainty can pave the way for a pair of disentangled uncertainty estimators. As a simple start, combining it with the CE baseline achieves a low rank correlation of \(0.15 0.01\) between the two. We see this as a promising pathway to disentangled uncertainty estimators in the future.

With the aleatoric and epistemic tasks introduced, we can take a final look at the epistemic and aleatoric estimators proposed by the IT decomposition. In Appendix C.3, using them instead of the best ones reduces Shallow Ensemble's performance, which was the best distributional method on OOD detection, to the CE baseline level. It again shows that the theoretically intuitive estimators underperform in practice.

### Predictive Uncertainty: The Best Method Depends on the Precise Task

Let us now broaden the view beyond disentanglement to benchmark how well uncertainty estimators solve other practically relevant tasks. We start with correctness prediction, where the AUROC quantifies whether wrong predictions generally have higher uncertainties than correct predictions.

Fig. 3(a) shows that most uncertainty estimators perform within \( 0.014\) of the cross-entropy baseline when predicting correctness. Modern methods like HET-XL do not outperform older methods like the Deep Ensemble or MC Dropout. Evidential deep learning methods, like EDL and PostNet, are an exception to this. They are considerably better at predicting correctness. This also holds when mixing the datasets with OOD data in Appendix H.1. However, their better performance comes at a cost. Evidential methods have a trade-off between the quality of their uncertainty estimates and the

Figure 4: ID predictive uncertainty evaluation on the ImageNet validation dataset. The Mahalanobis method is a specialized OOD detector that cannot differentiate between ID samples.

classification accuracy. When demanding similar classification accuracies, we find that they lose their advantage.

A related task to correctness prediction is abstained prediction. It involves refusing to predict on the \(x\%\) most uncertain examples and calculating the model's accuracy on the remaining samples. We use the Accuracy-Coverage (AC) curve  that plots increasing fractions of abstained samples from \(0\%\) to \(100\%\) on the \(x\)-axis against the accuracy on the non-abstained portion. Following the conventions of Galil et al. , we denote this metric as the area under the accuracy coverage curve (AUAC). In Appendix H.5, we also evaluate methods on the rAULC and E-AURC metrics that normalize the AUAC by the accuracy of the underlying model [43; 14].

Fig. 3(b) shows that this predictive uncertainty task is saturated. All uncertainty methods apart from Mahalanobis obtain an AUAC score greater than \(0.91\), and only a few outperform the CE baseline. Since the AUAC depends on accuracy, EDL and PostNet perform worse in this metric, although both AUROC and AUAC target predictive uncertainty. This demonstrates that practitioners need to carefully specify an uncertainty estimator's overall goal. Designing a system that can detect errors is not the same as designing a system that reduces errors.

The same holds for calibration. While this is also a predictive uncertainty task, the goal is slightly different. It is not to provide a good _ranking_ of uncertain images but to give correctness _probabilities_ that are close to the true frequentist probabilities.

Fig. 5 shows that different methods excel at this task than at the AUROC correctness task (Fig. 3(a)). In particular, Laplace and temperature scaling, which were only as good as the CE baseline in terms of AUROC, show drastically improved performance in terms of the ECE. Note that these two use a validation dataset to become better calibrated, similar to how Mahalanobis specialized and outperformed on the epistemic uncertainty task.

In conclusion, the predictive uncertainty results show that the exact definition of the task one intends to solve with uncertainty estimators matters because different estimators specialize in different notions of uncertainty.

### Different Tasks Require Different Methods

The previous sections suggest that uncertainty tasks are not all solved by the same best method. In this section, we investigate how the performance of methods on different tasks is correlated. In particular, we use the previous practical tasks along with further popular metrics and measure the between-task Pearson correlations of the performance of all benchmarked methods. The correlations of the rankings of the methods are similar; see Appendix H.7.

Fig. 6 shows barely any recognizable clusters, except that AUAC is confounded by accuracy. While all other metrics, except OOD AUROC, correlate to some extent, their correlation is not perfect, once again demonstrating that the performances of the uncertainty estimators depend on the exact task. These findings further corroborate that there is no one-fits-all uncertainty estimator, but there are multiple tasks with nuanced differences to which an uncertainty estimator can be tailored.

Figure 5: Expected calibration error on ImageNet.

Figure 6: The Pearson correlation of metric pairs across methods and aggregators on the ImageNet validation dataset is only medium. Most capture different aspects of uncertainty methods.

### Uncertainties are Robust to Distribution Shifts

As uncertainty estimates are often intended to increase the reliability of systems, one necessity is that they remain robust when a system faces unforeseen inputs. We test this by checking if their previous abstinence and correctness performances are preserved even when the model's accuracy drops with increasing ImageNet-C perturbation levels. Only then can we trust them and, e.g., base the abstinence from prediction on these uncertainty estimates.

Fig. 6(a) shows the correctness prediction AUROC, AUAC, and model accuracy as we increasingly perturb the ImageNet validation samples and go OOD. The correctness prediction performance in terms of the AUROC remains almost constant, whereas the accuracy degrades to less than \(25\%\) at severity level five. The AUAC performance degrades together with accuracy, which is a fundamental property of the metric itself since the area under the accuracy is lower-bounded by the baseline accuracy. The AUAC gain (i.e., AUAC \(-\) Accuracy) increases with the perturbation severity, showing that the uncertainty estimators even become relatively _better_ on the abstinence task as the severity increases. The tendencies are maintained when we normalize the metrics (solid lines) according to their random predictive performance (see Appendix H.2 for details). This observation holds for all methods except Mahalanobis, see Appendix H.2. Figure 7(a) shows that the methods' ECE also remains robust to perturbations on ImageNet. These results underline the trustworthiness of existing uncertainty quantification methods as we go OOD on ImageNet.

Figure 8: ECE results on ImageNet (left) and CIFAR-10 (right). Methods display drastically different behavior on ImageNet and CIFAR-10 regarding the robustness of their calibration. OOD samples are perturbed with ImageNet-C (left) and CIFAR-10C (right) corruptions of severity level one.

Figure 7: Degradation of correctness prediction, abstained prediction, and accuracy metrics with increasingly severe ImageNet-C (left) and CIFAR-10C (right) corruptions. The shown MC Dropout results are typical for all methods (except Mahalanobis). Solid lines: metrics normalized to the \(\) range w.r.t. corresponding random and oracle predictors. Dashed lines: unnormalized values.

### CIFAR-10 Results Do Not Always Transfer to ImageNet

We conclude our experiments with a word of caution. Appendix G repeats all experiments on CIFAR-10, which is widely used in the uncertainty quantification literature [53; 37; 17]. While some conclusions from CIFAR-10 experiments replicate on ImageNet, like the correlated aleatoric and epistemic estimators, the larger-scale ImageNet often shows different behavior.

Robustness.Uncertainty estimates are far less robust on CIFAR-10 than on the ImageNet scale, even though the drop in classification accuracy is very similar. Unlike on ImageNet, where the uncertainty estimators maintain a close to constant performance in predicting correctness as we go OOD (Fig. 6(a)), on CIFAR-10, correctness estimators deteriorate together with the model's accuracy (Fig. 6(b)). The same holds for the ECE (Fig. 7(a) vs. Fig. 7(b)). So, while robustness appears to be a striking problem on CIFAR-10, it gets resolved by scaling to a larger dataset.

Method rankings.Nine out of thirteen tasks exhibit substantially different rankings (rank corr. \(<0.5\)) between CIFAR-10 and ImageNet. See Table H.1 for details. This indicates that performance on CIFAR-10 should not be taken as an estimate for ImageNet performance.

These experiments underline that methods might show substantially different behaviors on large-scale datasets. As best practice, we encourage to first scale the approaches to the final deployment domain (and define a precise task) instead of making fundamental design choices on toy datasets.

## 4 Connections Between Our Findings and Related Works

Uncertainty Disentanglement.The decomposition of aleatoric and epistemic uncertainties [42; 11] has recently been shown to have failure cases. The disentanglement is usually analyzed theoretically [57; 1; 16] or with qualitative plots (Kirchhof et al. , Fig. 6-9; Mukhoti et al. , Fig. 2; Valdenegro-Toro and Mori , Fig. 8-10). Our results support this discussion with a practical and quantitative perspective. To the best of our knowledge, we are the first to quantify the uncertainty disentanglement. We find that no tested decomposition formula works for any tested second-order distribution, neither on ImageNet-1k nor CIFAR-10. Our findings encourage combining separate methods instead, such as the CE baseline's predictive entropy and the Mahalanobis values, where each method handles a specific type of uncertainty. This is similar to the recent work of Mukhoti et al. . We expect that our quantitative benchmarking methods help develop this field further.

Robustness.Recent benchmarks on OOD detection and robustness [38; 40; 43; 13] have first highlighted robustness issues of uncertainty estimates. Our benchmark supports these findings on CIFAR-10, especially in the region that is slightly OOD yet already causes degradation of both the main task and the uncertainty estimator. The latter implies that uncertainty estimators either need to become more robust to distribution shifts  or be better able to detect subtle epistemic uncertainties. However, our experiments on ImageNet do not show robustness issues. It is possible that the vast space of natural images that the ImageNet training dataset covers resolves this issue. We encourage repeating our experiments and testing the uncertainty estimation not just on test data but also on perturbed test data for future large-scale uncertainty estimators.

Aleatoric uncertainty.While epistemic uncertainty is widely evaluated on the OOD detection proxy task [37; 49; 17], aleatoric uncertainty still lacks a standardized testing protocol. The current approaches seem to converge to soft labels, but nuances in how they are collected still need discussion (compare, for example, CIFAR-10H  to CIFAR-10S  and CIFAR-10N ). An increasing number of uncertainty quantification approaches compare to such human GT notions of aleatoric uncertainty [49; 24; 25; 26], indicating the interest in the field. Our benchmark shows that no method can give highly accurate aleatoric uncertainty estimates yet, stressing the need for benchmarks, methods, and training resources to develop along.

Predictive uncertainty and calibration.Contrary to aleatoric uncertainty alignment, calibration and predictive uncertainty benchmarks are starting to become saturated and, according to our experiments, the top performers are ready for deployment. This corroborates recent findings by Galil et al. . In comparison to this benchmark that compared model architectures, we compared nineteen different approaches on the same backbone with a wide range of aggregator functions.

Conclusions, Limitations, and Outlooks

We study how a diverse spectrum of uncertainty estimators and decomposition formulas perform on a comprehensive set of uncertainty quantification tasks. Our quantitative findings bring an empirical foundation to recent discussions in the field, namely that 1) the aleatoric and epistemic uncertainty components of decomposition formulas are highly correlated and not disentangled, 2) epistemic and aleatoric tasks are best solved by practically tailored methods, whereas methods relying on intuitions often underperform, and 3) there is no one-fits-all uncertainty estimate. On a brighter side, our experiments also reveal the important fact for practitioners that 4) predictive uncertainty estimation achieve a high, saturating performance across almost all methods, and 5) uncertainty estimates, when trained on large amounts of data, stay robust to perturbations longer than the classifiers whose uncertainties they predict, hence enabling to safeguard the classifiers to some extent.

A limitation of our disentanglement benchmark is that we tested on two datasets, which are both classification tasks. This is because we require ground truths for aleatoric uncertainty. Currently, the only larger-scale datasets with such ground truths, in the form of multiple annotations per input, are the two classification datasets we base our analysis on . Further aleatoric uncertainty ground truths are an ongoing effort . We encourage the expansion of the set of datasets, both within classification and to fields like regression  or unsupervised learning , to expand our uncertainty disentanglement investigations. A second limitation is that we focus on models that have converged after training on the large-scale ImageNet dataset. A different interesting setup is models trained on small amounts of data, where epistemic uncertainty may be further from convergence. For example, there is a follow-up investigation of our work by de Jong et al.  that undersamples the train data. We replicate parts of their main experiment results on CIFAR-10 in Appendix H.9. We encourage future works to evaluate uncertainties on an as broad array of tasks as possible to refine the understanding of which specific uncertainty tasks individual uncertainty estimators excel at.

This last suggestion is a corollary of how our findings changed our perspective on uncertainty quantification. There is no general uncertainty; instead, uncertainty quantification covers a spectrum of tasks where the definition of the exact task heavily influences the optimal method and performance. Such a precise definition of tasks per estimator would help construct disentangled uncertainties and could lead to the alignment of theoretical developments and intuitive descriptions about what particular types of uncertainty methods aim to capture. This pragmatic reassessment of the field could overcome the traditional one-fits-all view of uncertainty and even the more recent epistemic vs. aleatoric dichotomy and uncover the full variety of uncertainty estimates that are tailored to nuanced, practical tasks.