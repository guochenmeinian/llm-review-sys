# Vocabulary-free Image Classification

Alessandro Conti\({}^{1}\)  Enrico Fini\({}^{1}\)  Massimiliano Mancini\({}^{1}\)

Paolo Rota\({}^{1}\)  Yiming Wang\({}^{2}\)  Elisa Ricci\({}^{1,2}\)

\({}^{1}\)University of Trento \({}^{2}\)Fondazione Bruno Kessler (FBK)

###### Abstract

Recent advances in large vision-language models have revolutionized the image classification paradigm. Despite showing impressive zero-shot capabilities, a pre-defined set of categories, _a.k.a._ the vocabulary, is assumed at test time for composing the textual prompts. However, such assumption can be impractical when the semantic context is unknown and evolving. We thus formalize a novel task, termed as Vocabulary-free Image Classification (VIC), where we aim to assign to an input image a class that resides in an unconstrained language-induced semantic space, without the prerequisite of a known vocabulary. VIC is a challenging task as the semantic space is extremely large, containing millions of concepts, with hard-to-discriminate fine-grained categories. In this work, we first empirically verify that representing this semantic space by means of an external vision-language database is the most effective way to obtain semantically relevant content for classifying the image. We then propose Category Search from External Databases (CaSED), a method that exploits a pre-trained vision-language model and an external vision-language database to address VIC in a training-free manner. CaSED first extracts a set of candidate categories from captions retrieved from the database based on their semantic similarity to the image, and then assigns to the image the best matching candidate category according to the same vision-language model. Experiments on benchmark datasets validate that CaSED outperforms other complex vision-language frameworks, while being efficient with much fewer parameters, paving the way for future research in this direction1.

## 1 Introduction

Large-scale Vision-Language Models (VLMs)  enabled astonishing progress in computer vision by aligning multimodal semantics in a shared embedding space. This paper focuses on their use for image classification, where models such as CLIP  demonstrated strength in zero-shot transfer. While we witnessed advances in VLM-based classification in many directions, _e.g._ prompt learning , scaling up to larger models and datasets , or by jointly considering captioning task , they all assume a finite set of target categories, _i.e._ the _vocabulary_, to be pre-defined and static (as shown in Fig. 0(a)). However, this assumption is fragile, as it is often violated in practical applications (_e.g._ robotics, autonomous driving) where semantic categories can either differ from the development/training to the deployment/testing or evolve dynamically over time.

In this work, we remove this assumption and study the new task of Vocabulary-free Image Classification (VIC). The objective of VIC is to assign an image to a class that belongs to an unconstrained language-induced semantic space at test time, _without a vocabulary_, _i.e._ without a pre-defined set of categories (as shown in Fig. 0(b)). The unconstrained nature of the semantic space makes VIC a challenging problem. First, the search space is extremely large, with a cardinality on the orderof millions of semantic concepts2, way larger than any existing image classification benchmark (_e.g_. ImageNet-21k ). Second, it also includes very fine-grained concepts that might be hard to discriminate by the model. Third, as the categories encountered at test time are undefined beforehand, VIC calls for classification methods that do not rely on any vocabulary-aware supervision.

Recent web-scale Vision-Language Databases (VLDs) , offer a unique opportunity to address VIC, as they cover a wide set of semantic concepts, ranging from general to highly specific ones. We empirically show that such external databases allow identifying semantic content that is more relevant to the target category than the captioning-enhanced VLMs supporting visual queries (_e.g_. BLIP-2 ). Motivated by this observation, we propose a training-free method for VIC, Category Search from External Databases (CaSED), which jointly leverages the discriminative multimodal representations derived from CLIP  and the information provided by recent VLDs (_e.g_. PMD ). CaSED operates in two steps: it first coarsely estimates a set of candidate categories for the test image, then it predicts the final category via multimodal matching. Specifically, we first retrieve the captions from the database that are semantically closer to the input image, from which we extract candidate categories via text parsing and filtering. We then estimate the similarity score between the input image and each candidate category via CLIP, using both visual and textual information, predicting as output the best matching candidate. CaSED exploits the pre-trained CLIP without further training, thus being flexible and computationally efficient.

We experiment on several datasets, considering both coarse- (_e.g_. Caltech-101 , UCF101 ) and fine-grained (_e.g_. FGVC-Aircraft , Flowers-102 ) classification tasks. To quantitatively assess the performance of methods addressing VIC, we also propose a set of metrics to measure how well the predicted class matches the semantics of the ground-truth label. Across all tasks and metrics, CaSED consistently outperforms all baselines, including VLMs as complex as BLIP-2 . We believe that thanks to its simplicity and effectiveness, our training-free CaSED can serve as a competitive baseline for future works aiming to address the challenging VIC task.

To summarize, this work provides the following contributions:

* We explore the task of Vocabulary-free Image Classification where the goal is to assign a class to an image over an unconstrained set of semantic concepts, overcoming the fundamental assumption of existing VLM-based methods for image classification. We formalize this task and suggest specific evaluation metrics that can be used as a reference for future works.
* We propose CaSED, the first method to address VIC thanks to the adoption of large captioning databases. Notably, CaSED is training-free, not requiring any additional parameter nor finetuning of the network's textual and visual encoders.
* Our large-scale evaluation demonstrates that CaSED consistently outperforms a more complex VLM such as BLIP-2  on VIC, while requiring much fewer parameters.

Figure 1: Vision-Language Model (VLM)-based classification (a) assumes a pre-defined set of target categories, _i.e_. the vocabulary, while our novel task (b) lifts this assumption by directly operating on the unconstrained language-induced semantic space, without a known vocabulary. \(f^{v}_{}\) and \(f^{t}_{}\) denote the pre-trained vision and text models of a VLM, respectively.

Related work

**Vision-Language Models.** Leveraging large-scale datasets with image-text pairs , recent works train models by mapping the two modalities into a shared representation space . A notable example is CLIP  which, using modality-specific encoders and a contrastive objective to align their output representations, showed remarkable performance on zero-shot classification. Subsequent works improved CLIP by _e.g_. connecting the two modalities via cross-modal attention , multi-object representation alignment , learning from weak-supervision  or unaligned data .

Another line of works improved vision-language pre-training for complex vision-language tasks, such as image captioning and visual question answering (VQA) . In this context, BLIP  exploits web data and generated captions to supervise pre-training of a multimodal architecture, outperforming existing VLMs on both captioning and VQA. The current state-of-the-art method BLIP-2  trains a module connecting the two modalities on top of a frozen visual encoder and a frozen large-language model, enabling instructed zero-shot image-to-text generation.

In this work, we challenge a fundamental assumption of zero-shot classification with VLMs: the set of target classes is known a priori. We propose a new task, VIC, which sidesteps this assumption, performing classification in a language-induced open-ended space of semantic categories. We show that even BLIP-2 struggles in this scenario while external multimodal databases provide valuable priors for inferring the semantic category of an image. As a final note, VIC differs from open-vocabulary recognition (e.g. ) since the latter assumes that the list of target classes is known and available to the model during inference.

**Retrieval augmented models.** In natural language processing, multiple works showed the benefit of retrieving information from external databases, improving the performance of large language models . In computer vision, such a paradigm has been used mostly to deal with the imbalanced distribution of classes. Examples are , addressing long-tail recognition by learning to retrieve training samples  or image-text pairs from an external database . Similarly,  retrieves images from a given dataset to learn fine-grained visual representations. More recently, retrieval-augmentation has been extended to various types of sources for visual question answering , as well as to condition the generative process in diffusion models , or image captioning . Our work is close in spirit to , as we exploit an external database. However,  assumes a pre-defined set of classes (and data) available for training a retrieval module, something we cannot have for the extremely large semantic space of VIC. In CaSED, retrieval is leveraged to first create a set of candidate classes, and to then perform the final class prediction. Moreover, we assume the database to contain only captions, and not necessarily paired image-text data, thus being less memory-demanding.

Finally, the performance of retrieval models is largely affected by the employed database. In the context of VLMs, researchers collected multiple open datasets to study vision-language pre-training. Two notable examples are LAION-5B , collected by filtering Common Crawl  via CLIP, and the Public Multimodal Datasets (PMD) , collecting image-text pairs from different public datasets, such as Conceptual Captions , YFCC100M , Wikipedia Image Text , and Redcaps . In our experiments, we use a subset of PMD as database and investigate how classification performance varies based on the size and quality of the database.

## 3 Vocabulary-free Image Classification

**Preliminaries.** Given the image space \(\) and a set of class labels \(C\), a classification model \(f: C\) is a function mapping an image \(\) to its corresponding class \( C\). While \(C\) may be represented by indices that refer to semantic classes, here we focus on cases where \(C\) consists of a set of concept names that correspond to real entities. Note that \(C\) is a finite set whose elements belong to the semantic space \(\), _i.e_. \(C\). In standard image classification, \(C\) is given a priori, and \(f\) is learned on a dataset of image-label pairs.

Recently, the introduction of contrastive-based VLMs , which learn a cross-modal aligned feature space, revised this paradigm by defining a function \(f_{}\) that infers the similarity between an image and a textual description \(\), \(\)\(f_{}:\), with \(\) being the language space. Given this function, classification can be performed with:

\[f()=_{ C}f_{}(,()) \]

where \(()\) is a string concatenation operation, combining a fixed text template, \(\) a _prompt_, with a class name. This definition allows for zero-shot transfer, \(\) performing arbitrary classification tasks by re-defining the set \(C\) at test time, without re-training the model. However, such zero-shot transfer setup still assumes that the set \(C\) is provided. In this work, Vocabulary-free Image Classification (VIC) is formulated to surpass this assumption.

**Task definition.** VIC aims to assign a class \(\) to an image \(\)_without_ prior knowledge on \(C\), thus operating on the semantic class space \(\) that contains all the possible concepts. Formally, we want to produce a function \(f\) mapping an image to a semantic label in \(\), \(:\). Our task definition implies that at test time, the function \(f\) has only access to an input image \(\) and a large source of semantic concepts that approximates \(\). VIC is a challenging classification task by definition due to the extremely large cardinality of the semantic classes in \(\). As an example, ImageNet-21k , one of the largest classification benchmarks, is \(200\) times smaller than the semantic classes in BabelNet . This large search space poses a prime challenge for distinguishing fine-grained concepts across multiple domains as well as ones that naturally follow a long-tailed distribution.

**Semantic space representation.** As the main challenge of VIC, how to represent the large semantic space plays a fundamental role in the method design. We can either model the multimodal semantic space directly with a VLM equipped with an autoregressive language decoder  or via image-text retrieval from VLDs. Consequently, we can approach VIC either via VQA-enabled VLMs by querying for the candidate class given the input image, or by retrieving and processing data from an external VLD to obtain the candidate class.

To investigate the two potential strategies, we perform a preliminary experimental analysis to understand how well the output of a method semantically captures the image category, or in other words, to assess the alignment of class boundaries in the visual and textual representations. Specifically, we compare the semantic accuracy of querying VQA VLMs and of retrieving from VLDs w.r.t. the ground-truth class labels. We consider the output of a method as correct if its closest textual embedding among the target classes of the dataset corresponds to the ground-truth class of the test sample3. We exploit the text encoder of CLIP (ViT-L)  to obtain textual embeddings.

Figure 2: Results of our preliminary study, showing the top-1 accuracy when matching semantic descriptions to ground-truth class names in ten different datasets. We compare **BLIP-2 (VQA)** and **BLIP-2 (Captioning)** with **Closest Caption** and **Captions Centroid**, \(\)the average representation of the retrieved captions. We additionally highlight the **Upper bound** for zero-shot CLIP. Representing the large semantic space as VLDs and retrieving captions from it produces semantically more similar outputs to ground-truth labels w.r.t. querying outputs from VQA-enabled VLMs, while requiring 10 times fewer parameters compared to the latter.

Regarding experimented methods, we select BLIP-2  to represent VQA-enabled VLMs for its state-of-the-art performance in VQA benchmarks, while we use a subset of PMD  as the VLD. In particular, we compare the following methods: i) _BLIP-2 VQA_, which directly queries BLIP-2 for the image category; ii) _BLIP-2 Captioning_, which queries BLIP-2 for the image caption; iii) _Closest Caption_, which is the closest caption to the image, as retrieved from the database; iv) _Caption Centroid_, which averages the textual embeddings of the 10 most similar captions to the input image. As we use CLIP embeddings, if visual and textual representations perfectly align, the performance would be the same as zero-shot CLIP with given target classes. We thus report zero-shot CLIP to serve as the upper bound for retrieval accuracy.

We experiment on a variety of test datasets for both coarse- and fine-grained classification (see details in Sec. 5), and report the results in Fig. 2. The average textual embedding of the retrieved captions (_i.e_. Caption Centroid) achieves the best semantic accuracy for 9 datasets out of 10, consistently surpassing methods based on BLIP-2. On average, the accuracy achieved by Caption Centroid is \(60.47\%\), which is \(+17.36\%\) higher than the one achieved by BLIP-2 Captioning (\(43.11\%\)). Moreover, Captions Centroid achieves results much closer to the CLIP upper bound (\(67.17\%\)) than the other approaches. Notably, such VLD-based retrieval is also computationally more efficient, faster (~ 4 second for a batch size of 64 on a single A6000 GPU), and requires fewer parameters (approximately 10 times less) than BLIP-2 (see Tab. 7 in Appendix B).

The results of this preliminary study clearly suggest that representing the large semantic space with VLDs can produce (via retrieval) semantically more relevant content to the input image, in comparison to querying VQA-enabled VLMs, while being computationally efficient. Based on this conclusion, we develop an approach, Category Search from External Databases (CaSED), that searches for the semantic class from the large semantic space represented in the captions of VLDs.

## 4 CaSED: Category Search from External Databases

Our proposed method CaSED finds the best matching category within the unconstrained semantic space by multimodal data from large VLDs. Fig. 3 provides an overview of our proposed method. We first retrieve the semantically most similar captions from a database, from which we extract a set of candidate categories by applying text parsing and filtering techniques. We further score the candidates using the multimodal aligned representation of the large pre-trained VLM, _i.e_. CLIP , to obtain the best-matching category. We describe in detail each process in the following.

### Generating candidate categories

We first restrict the extremely large classification space to a few most probable candidate classes. Let \(f_{}\) be the pre-trained VLM and \(D\) be the external database of image captions. Given an input \(\), we retrieve the set \(D_{} D\) of \(K\) closest captions to the input image via

\[D_{}=*{top-k}_{ D}\,f_{}(,)= *{top-k}_{ D}\, f_{}^{v}(),f_{ }^{t}(), \]

where \(f_{}^{v}:\) is the visual encoder of the VLM, \(f_{}^{t}:\) is the textual encoder, and \(\) is their shared embedding space. The operation \(,\) indicates the computation of the cosine similarity. Note that our approach is agnostic to the particular form of \(D\), and that it can accommodate a flexible database size by including captions from additional resources.

From the set \(D_{}\), we then extract a finite set of candidate classes \(C_{}\) by performing simple text parsing and filtering techniques, _e.g_. stop-words removal, POS tagging. Details on the filtering procedure can be found in Appendix A.

### Multimodal candidate scoring

Among the small set \(C_{}\), we score each candidate by accounting for both visual and textual semantic similarities using the VLM encoders, in order to select the best-matching class for the input image.

**Image-to-text score.** As the image is the main driver for the semantic class we aim to recognize, we use the visual information to score the candidate categories. We denote \(s_{}^{v}\) as the visual score of each candidate category \(\) and compute it as the similarity between the visual representation of the input image and the textual representation of the candidate name:

\[s^{v}_{}= f^{v}_{}(),f^{t}_{}(). \]

The higher value of \(s^{v}_{}\) indicates a closer alignment between the target image and the candidate class.

**Text-to-text score.** While the image-to-text score \(s^{v}_{}\) is effective, there exists a well-known modality gap in the space \(\), harming the performance of zero-shot models . As suggested by Fig. 2 in Sec. 3, the semantic relevance of the retrieved captions, and their centroid in particular, is high w.r.t. the underlying ground-truth label. We are therefore motivated to exploit such attributes and introduce a unimodal text-to-text scoring to mitigate the modality gap of cross-modal scoring.

Formally, we define the centroid \(_{}}\) of the retrieved captions as:

\[_{}}=_{ D_{}}f^{t}_{}(), \]

where \(K\) is the number of retrieved captions. We then define the text-based matching score \(s^{t}_{}\) as the similarity between the centroid and the candidate category:

\[s^{t}_{}=_{}},f^{t}_{}(). \]

A higher value of \(s^{t}_{}\) means a higher alignment between the caption centroid and the candidate embedding. Note that the semantic relevance of the caption centroid is an inherent property of the text encoder of VLMs (CLIP). Since CLIP is trained with an image-text alignment loss, its text encoder focuses on the visual elements of the caption, discarding parts that are either non-visual or non-relevant to the visual content. This improves the model's robustness to noisy candidates.

**Final predicted candidate.** To predict the final candidate, we merge the two scores, obtaining the final score \(s_{}\) for each candidate \(\) as:

\[s_{}=~{}(s^{v}_{})~{}+~{}(1-)~{}(s^{t}_{ }) \]

where \(()\) is the softmax operation on the two scores of each candidate class, and \(\) is a hyperparameter regulating the contribution of the two modalities. Finally we obtain the output category as \(f(x)=_{ C_{}}s_{}\). Notably, CaSED respects the VIC task definition, performing classification without known class priors, while being _training-free_ with the use of a pre-trained and frozen VLM. This makes the approach flexible and applicable to a variety of architectures and databases.

Figure 3: Overview of CaSED. Given an input image, CaSED retrieves the most relevant captions from an external database filtering them to extract candidate categories. We classify image-to-text and text-to-text, using the retrieved captions centroid as the textual counterpart of the input image.

Experiments

We evaluate CaSED in comparison to other VLM-based methods on the novel task Vocabulary-free Image Classification with extensive benchmark datasets covering both coarse-grained and fine-grained classification. We first describe the experimental protocol in terms of the datasets, the proposed evaluation metrics, and the baselines applicable to this task (Sec. 5.1). We then discuss the quantitative results regarding the comparison between our method and baselines (Sec. 5.2). Finally, we present a thorough ablation to justify the design choices of CaSED (Sec. 5.3). In addition, we provide a cost comparison between the baseline methods and CaSED in Appendix B. We also offer further ablation of our method in Appendix C regarding architecture and database, and showcase qualitative results of predicted categories from multiple datasets in Appendix D.

### Experimental protocol

**Datasets.** We follow existing works [53; 66] and use ten datasets that feature both coarse-grained and fine-grained classification in different domains: Caltech-101 (C101) , DTD , EuroSAT (ESAT) , FGVC-Aircraft (Airc.) , Flowers-102 (Flwr) , Food-101 (Food) , Oxford Pets (Pets), Stanford Cars (Cars) , SUN397 (SUN) , and UCF101 (UCF) . Additionally, we used ImageNet  for hyperparameters tuning.

**Evaluation metrics.** Due to the unconstrained nature of the semantic space, evaluating the effectiveness of methods for VIC is not trivial. In this paper, we propose to use two main criteria, namely _semantic relevance_, _i.e_. the similarity of the predicted class w.r.t. the ground-truth label, and _image grouping_, _i.e_. the quality of the predicted classes for organizing images into clusters. For semantic, we consider i) _Semantic Similarity_, _i.e_. the similarity of predicted/ground-truth labels in a semantic space, and ii) _Semantic IoU_, _i.e_. the overlap of words between the prediction and the true label. More formally, given an input \(\) with ground-truth label \(\) and prediction \(}=f()\), we compute the _Semantic Similarity_ as \( g(}),g()\), where \(g:\) is a function mapping text to an embedding space \(\). Since we want to model free-form text, we use Sentence-BERT  as \(g\). For _Semantic IoU_, given a predicted label \(\), and assuming \(\) being a set of words, we compute the Semantic IoU as \(||/||\), where \(\) is the set of words in the ground-truth label. To assess grouping, we measure the classic _Cluster Accuracy_ by first clustering images according to their predicted label, and then assigning each cluster to a ground-truth label with Hungarian matching. Sometimes, this mapping is resolved with a many-to-one match, where a predicted cluster is assigned to the most present ground-truth label. This evaluation draws inspiration from the protocols used for deep visual clustering [59; 24; 20].

**Baselines.** We consider three main groups of baselines for our comparisons. The most straightforward baselines consist of using CLIP with large vocabularies, such as WordNet  (117k names) or the English Words (234k names ). As an upper bound, we also consider CLIP with the perfect vocabulary, _i.e_. the ground-truth names of the target dataset (CLIP upper bound). Due to lack of space, we only report results for CLIP with ViT-L , while results with other architectures are reported in Appendix C. The second group of baselines consists of captioning methods, as captions can well describe the semantic content of images. We consider two options: captions retrieved from a database and captions generated by a pre-trained image captioning model. For the former we exploit a large collection of textual descriptions, retrieving the most fitting caption for the given image. For the latter, we exploit BLIP-2  -- a VLM with remarkable performance on a variety of tasks, including image captioning -- to provide a description for the image. The last group of baselines consists of using a VQA model to directly predict the class name associated with the image. Again, we consider BLIP-2 , since being highly effective also in VQA. We evaluate BLIP-2 over both ViT-L and ViT-g throughout the experiments 4.

**Implementation details.** Our experiments were conducted using NVIDIA A6000 GPUs with mixed-bit precision. As database, we use a subset of PMD , containing five of its largest datasets: Conceptual Captions (CC3M) , Conceptual Captions 12M (CC12M) , Wikipedia Image Text (WIT) , Redcaps , and a subset of  used for PMD (YFCC100M*). Further details on the selection are left for Appendix C. We speed up the retrieval process by embedding the database via the text encoder \(f_{}^{t}\) and using fast indexing technique, _i.e_. FAISS . We tuned the \(\) hyperparameter of Eq. (6) and the number of retrieved captions \(K\) of our method on the ImageNet dataset, finding that \(=0.7\) and \(K=10\) led to the best results. We use these values across all experiments.

### Quantitative results

The results of CaSED and the baselines are presented in Table 1, Table 2, and Table 3 for Cluster Accuracy (\(\%\)), Semantic Similarity, and Semantic IoU (\(\%\)), respectively. CaSED consistently outperforms all baselines in all metrics on average and in most of the datasets. Notably, CaSED surpasses BLIP-\(2\) (VQA) over ViT-g by \(+4.4\%\) in Cluster Accuracy and \(+1.7\%\) on Semantic IoU, while using much fewer parameters (_i.e_. 102M vs 4.1B). The gap is even larger over the same visual backbone, with CaSED outperforming BLIP-2 on ViT-L (VQA) by \(+14.3\%\) on Cluster Accuracy, \(+5.7\) in Semantic Similarity, and \(+6.8\%\) on Semantic IoU. These results highlight the effectiveness of CaSED, achieving the best performance both in terms of semantic relevance and image grouping.

An interesting observation from the tables is that simply applying CLIP on top of a pre-defined, large vocabulary, is not effective in VIC. This is due to the challenge of classifying over a huge search space, where class boundaries are hard to model. This is confirmed by the results of CLIP with English Words having a larger search space but performing consistently worse than CLIP with WordNet across all metrics (_e.g_. \(-7.7\) on Semantic Similarity, -3.2% on Semantic IoU).

A final observation relates to the captioning models. Despite their ability to capture the image semantic even in challenging settings (_e.g_. 39.3 Semantic Similarity of BLIP-2 ViT-L on ESAT), captions exhibit high variability across images of the same category. This causes the worst performance on Clustering and Semantic IoU across all approaches (_e.g_. almost \(-20\%\) less than CaSED on average in terms of Cluster Accuracy), thus demonstrating that while captions can effectively describe the content of an image, they do not necessarily imply better categorization for VIC.

    &  \\  & C101 & DTD & ESAT & Airc. & Flwr & Food & Pets & SUN & Cars & UCF & **Avg.** \\   & WordNet & 34.0 & 20.1 & 16.7 & 16.7 & 58.3 & 40.9 & 52.0 & 29.4 & 18.6 & 39.5 & 32.6 \\  & English Words & 29.1 & 19.6 & 22.1 & 15.9 & 64.0 & 30.9 & 44.4 & 24.2 & 19.3 & 34.5 & 30.4 \\   & Closest Caption & 12.8 & 8.9 & 16.7 & 13.3 & 28.5 & 13.1 & 15.0 & 8.6 & 20.0 & 17.8 & 15.5 \\  & BLIP-2 (ViT-L) & 26.5 & 11.7 & 23.3 & 5.4 & 23.6 & 12.4 & 11.6 & 19.5 & 14.8 & 25.7 & 17.4 \\  & BLIP-2 (ViT-g) & 37.4 & 13.0 & **25.2** & 10.0 & 29.5 & 19.9 & 15.5 & 21.5 & 27.9 & 32.7 & 23.3 \\   & BLIP-2 (ViT-L) & 60.4 & 20.4 & 21.4 & 8.1 & 36.7 & 21.3 & 14.0 & 32.6 & 28.8 & 44.3 & 28.8 \\  & BLIP-2 (ViT-g) & **62.2** & 23.8 & 22.0 & 15.9 & 57.8 & 33.4 & 23.4 & 36.4 & **57.2** & **55.4** & 38.7 \\   & 51.5 & **29.1** & 23.8 & **22.8** & **68.7** & **58.8** & **60.4** & **37.4** & 31.3 & 47.7 & **43.1** \\   & 87.6 & 52.9 & 47.4 & 31.8 & 78.0 & 89.9 & 88.0 & 65.3 & 76.5 & 72.5 & 69.0 \\   

Table 1: Cluster Accuracy on the ten datasets. \(\) given method, \(\) given shows the upper bound.

    &  \\  & C101 & DTD & ESAT & Airc. & Flwr & Food & Pets & SUN & Cars & UCF & **Avg.** \\   & WordNet & 48.6 & 32.7 & 24.4 & 18.9 & 55.9 & 49.6 & 53.7 & 44.9 & 28.8 & 44.2 & 40.2 \\  & English Words & 39.3 & 31.6 & 19.1 & 18.6 & 43.4 & 38.0 & 44.2 & 36.0 & 19.9 & 34.7 & 32.5 \\   & Closest Caption & 42.1 & 23.9 & 23.4 & 29.2 & 40.0 & 46.9 & 40.2 & 39.8 & 49.2 & 40.3 & 37.5 \\  & BLIP-2 (ViT-L) & 57.8 & 31.4 & **39.9** & 24.4 & 36.1 & 44.6 & 29.0 & 45.3 & 46.4 & 38.0 & 39.3 \\  & BLIP-2 (ViT-g) & 63.0 & 33.1 & 36.2 & 24.3 & 45.2 & 51.6 & 31.6 & 48.3 & 61.0 & 44.6 & 43.9 \\   & BLIP-2 (ViT-L) & 70.5 & 34.9 & 29.7 & 29.1 & 48.8 & 42.0 & 40.0 & 50.6 & 52.4 & 48.6 & 44.7 \\  & BLIP-2 (ViT-g) & **73.5** & 36.5 & 31.4 & **30.8** & **59.9** & 52.1 & 43.9 & **53.3** & **65.1** & **55.1** & 50.1 \\   & 65.7 & **40.0** & 32.0 & 30.3 & 55.5 & **64.5** & **62.5** & 52.5 & 47.4 & 54.1 & **50.4** \\   & 90.8 & 69.8 & 67.7 & 66.7 & 83.4 & 93.7 & 91.8 & 80.5 & 92.3 & 83.3 & 82.0 \\   

Table 2: Semantic Similarity on the ten datasets. Values are multiplied by x100 for readability. \(\) given highlights our method and \(\) indicates the upper bound.

### Ablation studies

In this section, we present the ablation study associated with different components of our approach. We first analyze the impact of our retrieval-based candidate selection and multimodal scoring. We then show the results of CaSED for different databases, and how the number of retrieved captions impacts the performance.

**Candidates generation.** We consider two options to generate the set of candidate classes. The first uses BLIP-2 (ViT-g), asking for multiple candidate labels for the input image. The second is our caption retrieval and filtering strategy. Table 3(a) shows the results where, for fairness, we score both sets with the same VLM (\(i.e.\) CLIP ViT-L). Our approach consistently outperforms BLIP-2 across all metrics (\(e.g.\) 41.7 vs 23.3 for Cluster Accuracy). This confirms the preliminary results of our study in Fig. 2, with retrieved captions providing better semantic priors than directly using a powerful VLM.

**Multimodal scoring.** The second ablation studies the effect of our proposed multimodal scoring vs its unimodal counterparts. As Table 3(a) shows, multimodal candidate scoring provides the best results across all metrics, with clear improvements over the visual modality alone (\(i.e.\) +1.4% of Cluster Accuracy). Notably, scoring via language only partially fills the gap between visual and multimodal scoring (\(e.g.\) +1% on Cluster Accuracy and +1 on Semantic Similarity), confirming that caption centroids contain discriminative semantic information.

**Retrieval database.** We analyze the impact of retrieving captions from different databases, using five public ones, \(i.e.\) CC3M, WIT, Redcaps, CC12M, and a subset of YFCC100M. The databases have different sizes (\(e.g.\) from 2.8M captions of CC3M to 29.9M captions of the YFCC100M subset), and different levels of noise. As shown in Table 3(b), the results tend to improve as the size of the database increases (\(e.g.\) +8.9% on Cluster Accuracy over CC3M). However, the quality of the captions influences the performance, with CC12M and Redcaps alone achieving either comparable or slightly better results than the full database. These results suggest that while performance improves with the size of the database, the quality of the captions has a higher impact than the mere quantity.

Table 4: Ablation studies. Metrics are averaged across the ten datasets. **Green** highlights our setting. **Bold** represents best, _underline_ indicates second best.

Table 3: Semantic IoU on the ten datasets. **Green** is our method, **gray** shows the upper bound.

**Number of captions.** Finally, we check how performance varies w.r.t. the number of retrieved captions. As shown in Fig. 4, all metrics consistently improve as the number of captions increases from 1 to 10. After that, performance tends to saturate, with a slight decrease in terms of Semantic Similarity for \(K=20\). These results suggest that while a minimum number of captions is needed to fully capture the semantics of the image, the possible interference of less-related (or even noisy) captions may impact the final performance. Future research may further improve the performance on VIC by focusing on how to deal with noisy retrieval results.

## 6 Discussion and conclusions

In this work, we proposed a new task, VIC, which operates on an unconstrained semantic space, without assuming a pre-defined set of classes, a brittle assumption of VLM-based classification. We experimentally verified that multimodal databases provide good semantic priors to restrict the large search space, and developed CaSED, an efficient training-free approach that retrieves the closest captions to the input image to extract candidate categories and scores them in a multimodal fashion. On different benchmarks, CaSED consistently achieved better results than more complex VLMs.

**Limitations and future works.** The performance of CaSED strongly depends on the choice of the retrieval database, with potential issues in retrieving concepts that are not well represented in the latter. Moreover, if the domain of application contains fine-grained concepts, a generic database might not be suitable. Without any prior information on the test labels, it is hard to predict the performance of a database a priori. On the other hand, CaSED can flexibly mitigate this issue by incrementally including new concepts in the database (even domain-specific ones) from textual corpora, without retraining. Future research may explore strategies to automatically select/extend a database based on test samples and/or pre-computing the best database to use in the absence of test label information.

Additionally, as CaSED lacks control over the classes contained in the database, its predictions might reflect potential biases. Improvements in mitigating biases and data quality control would reduce this issue. Another limitation is that CaSED does not keep track of its output history. This may lead to inconsistent predictions, _i.e_. assigning slightly different labels to images of the same semantic concept (_e.g_. _cassowary_ vs _Casuarius_). Equipping CaSED with a memory storing the predicted labels may address this issue. Finally, CaSED does not deal with different class granularities: _e.g_. an image of a _cassowary_ can be as well predicted as a _bird_. Future works may disambiguate such cases by explicitly integrating the user needs within VIC models.

**Broader impact.** CaSED addresses VIC in a scalable and training-free manner. We believe that our new problem formulation, metrics, and the effectiveness of CaSED will encourage future research in this topic, overcoming the limiting assumption of VLM-based classification and allowing the power of VLMs to benefit dynamic and unconstrained scenarios.