# EgoSim:

An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity

 Dominik Hollidt, Paul Streli, Jiaxi Jiang, Yasaman Haghighi,

Changlin Qian, Xintong Liu, and Christian Holz

Department of Computer Science

ETH Zurich, Switzerland

firstname.lastname@inf.ethz.ch

###### Abstract

Research on egocentric tasks in computer vision has mostly focused on head-mounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets. We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations. This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition--particularly for the lower body, which is typically occluded.

In this paper, we introduce _EgoSim_, a novel simulator of body-worn cameras that generates realistic egocentric renderings from multiple perspectives across a wearer's body. A key feature of EgoSim is its use of real motion capture data to render motion artifacts, which are especially noticeable with arm- or leg-worn cameras. In addition, we introduce _MultiEgoView_, a dataset of egocentric footage from six body-worn cameras and ground-truth full-body 3D poses during several activities: 119 hours of data are derived from AMASS motion sequences in four high-fidelity virtual environments, which we augment with 5 hours of real-world motion data from 13 participants using six GoPro cameras and 3D body pose references from an Xsens motion capture suit.

We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D pose estimation network. Analyzing its domain gap, we show that our dataset and simulator substantially aid training for inference on real-world data.

EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim

## 1 Introduction

The newest generation of AI-based personal devices evidently requires an understanding of the world from a user's perspective to provide meaningful context. For example, Meta's Ray-Ban glasses , Hu.ma.ne AI pin , or the glasses demoed at Google I/O 2024 all share the wearer's perspective to analyze their surroundings. Such emerging devices in addition to existing immersive Mixed Reality platforms have further spurred research efforts on egocentric perception tasks .

While head-worn cameras have primarily been used for localization , they are ideally positioned to simultaneously capture the wearer's arm motions, for example, to estimate upper body poses  or detect user input from hand poses and actions . For egocentric pose estimation, previous work has commonly used head-mounted fisheye cameras pointing down , which can capture much of the upper body. This promise has spurred interest in egocentric pose estimation, for whichseveral real  and synthetic  datasets have been collected. The advantage of synthetic data has been demonstrated for simultaneous localization and mapping (SLAM ), 3D reconstruction , and human mesh recovery (HMR ).

For more comprehensive capture of body motion, prior work has used motion-capture suits  or individual body-worn motion sensors , where learned methods predict 3D body poses from up to a set of inertial sensors as input. These sensor ensembles provide rich information about the various limb motions and enable fine-grained pose estimation. However, estimates from motion sensors alone suffer from drift and struggle with tracking global positions, for which previous work has added head-worn cameras  to complement inertial motion cues.

Considering the ongoing miniaturization of camera technology, there is promise in further augmenting on-body tracking methods with camera sensors, for example, to remove the occlusion of lower body parts and extend the coverage of the environment . Indeed, Shiratori et al.'s pioneering effort to track 3D body poses in the wild from multiple body-worn cameras in 2011 predates many learning-based methods  and demonstrated the potential of the richer modality that is videos for human motion tasks. In addition, body-worn cameras, such as those on the wrists  or legs  benefit from their proximity to the point of interest during human activity or hand-object interaction. The use of multiple cameras mitigates the effect of occlusion and provides multiple vantage points of the ego-body, surrounding people, and the environment. Extensive research on integrating multi-view data (e.g., ), albeit typically from static third-person perspectives, has shown benefits for navigation , 3D reconstruction , and pose estimation .

In this paper, we introduce _EgoSim_, a multi-view body-worn camera simulator designed for human motion tasks. We also present _MultiEgoView_, a dataset that comprises rendered footage simulated from existing human motion data and novel complementary real-world recordings (Figure 1). We demonstrate the benefit of body-worn cameras and our simulator with the example of ego-body pose estimation using an end-to-end trained vision-only model. Our contributions in this dataset paper are:

1. EgoSim, an easy-to-use, adaptable, and highly realistic simulator for multiple body-worn cameras that uses real human motion as input. Camera positions on the body and their intrinsics can be configured flexibly, and EgoSim renders a range of useful modalities. EgoSim also simulates the attachment of body-worn cameras realistically via a spring arm to include motion artifacts.
2. MultiEgoView, a 119-hour video dataset of one or more avatars that perform natural motions and activities based on AMASS  in four virtual environments with reference 3D body poses. We contribute a novel 5-hour real-world dataset with 13 participants who wore 6 GoPro cameras with 3D body reference poses (from Xsens ) and dense human activity classes (BABEL ).
3. A learning-based multi-view method for end-to-end 3D pose estimation tasks from video. We analyze the sim2real gap based on our dataset and show the benefits of simulated data.

Taken together, we believe that EgoSim--alongside other emerging simulators (e.g., for faces  and scene interactions of human bodies , and hands )--will contribute to advancing open research on egocentric perception tasks.

Figure 1: (Left) Our dataset _MultiEgoView_ contains 5 hours of egocentric real-world footage from 6 body-worn GoPro cameras and ground-truth 3D body poses from an Xsens motion capture suit as well as 119 hours of simulated footage in high-fidelity virtual environments on the basis of real motion capture data and associated 3D body poses. (Right) Our method estimates ego poses from video data alone, here visualized inside the scanned 3D scene.

Related Work

Synthetic datasets and simulators.The advancement of deep learning in recent years has necessitated larger and more varied datasets that can be acquired using simulated data. Visual synthetic data proved its benefits in many fields such as human mesh recovery , visual-inertial odometry , visual SLAM , and human pose estimation . Microsoft AirSim  stands out as one of the most effective simulators. It has facilitated the creation of photo-realistic datasets such as TartanAir , optimized for Visual SLAM tasks, and Mid-air , designed for low-altitude drone flights. So far, AirSim  and other simulators  fall short in tasks centered on human dynamics, such as 3D human pose estimation or multi-actor interactions. Only recently, the Habitat 3  simulator targets human-robot interaction tasks and progresses in this area but offers limited configuration for sensor placement and environmental diversity. EgoGen as a novel human-centered simulator demonstrates promise by focusing on human motion synthesis . Traditionally, datasets simulate cameras either statically or with smooth movements. Such datasets fail to generalize to egocentric scenarios where the camera's position dynamically changes in relation to the wearer's movements. EgoSim advances this field by being specifically designed for human-centric research with wearable cameras that follow the natural non-smooth movements within the human body. It uniquely supports complex multi-character interactions in varied environments, both indoor and outdoor, enabling more comprehensive and diverse studies in this field.

Human motion datasets.In controlled settings, multiple third-person view cameras and motion capture equipment offer accurate ground-truth data . Fitting 3D body models  to point cloud marker sets  or using RGBD camera data can provide ground-truth poses. However, the complexity of these setups mostly limits their scalability to indoor environments . Pseudo-ground truth pose annotations can overcome these limitations for outdoor environments. Several methods use 2D keypoints , which are easy to label at a large scale, but provide 2D constraints only on the human pose. Alternatively, fitting 3D body models such as SMPL  to images provides pseudo-ground truth parameters . You2Me  and EgoBody  capture human pose data for interacting individuals using head-mounted cameras in indoor settings. Recently, Egohumans  has expanded the scope to include up to four interacting individuals in both indoor and outdoor settings. Meanwhile, larger datasets like Ego4D  offer extensive data from head-mounted cameras for tasks such as social interaction and hand-object interaction, but they lack data from additional body-worn cameras. The recently published Nymeria dataset  addresses this gap partially and includes real-world videos from wrist-mounted cameras. Our real-world MultiEgoView dataset further extends to a setup with six body-worn cameras with additional sensors at the knees and pelvis. To overcome limitations in real-world datasets, realistic synthetic datasets offer an alternative that offers diversity and quality ground truth annotations . Our work expands on this approach by introducing a configurable simulator tailored to body-worn sensors, with adjustable parameters for lighting, scene, and camera placement. EgoSim complements real-world datasets like Nymeria by enabling the rendering of synthetic images from adjustable body-worn cameras based on their captured motion sequences.

Egocentric perception.Wearable cameras serve as the primary input for research on egocentric perception tasks. Currently, real and synthetic egocentric datasets mainly feature head-mounted sensors. Some systems  use a single head-mounted, body-facing, fisheye camera to estimate 3D ego-body pose, while others rely on a stereo configuration . Head-mounted, body-facing cameras benefit from capturing visible joints in image space to aid ego-body pose estimation. Other methods recover the 3D pose from non-body-facing cameras. HPS  integrates multiple body-worn IMUs with camera-based localization using structure from motion. Kinpoly  recovers the whole body pose from a front-facing camera using physics simulation with reinforcement learning, while EgoEgo  combines SLAM with a diffusion model to recover the ego-body pose. AvatarPoser  and its subsequent work  predict full-body poses based on head and hand poses tracked by commercial mixed reality devices. HOOV  extends hand tracking beyond the field of view of head-mounted cameras using inertial signals captured at the wrist.

So far, egocentric datasets have mainly focused on head-mounted cameras that either point down toward the body  or forward , often designed for specific devices . Our work extends egocentric datasets to multiple body-worn cameras by providing an adaptable simulation platform and a real-world dataset of six body-worn cameras.

## 3 EgoSim Simulation Platform

EgoSim is designed for body-worn camera simulation. We extend Microsoft's AirSim simulator  integrated within the Unreal Engine  to leverage its flexibility and realistic output renders (e.g., ). Specifically, we augment the platform with the capability of simulating _body-worn_ cameras during realistic human motion, generating dynamic changes in camera motion that correspond to a person's movements, including potentially irregular, rough, and non-smooth moments.

Simulating images.EgoSim renders footage through Unreal Engine's cinematic camera  for realistic images. The camera model and noise parameters are adjustable. EgoSim supports simultaneously rendering multiple modalities (Figure 2), including RGB, depth, normal maps, and semantic segmentation masks. These modalities are complementary and can serve as input to various computer vision tasks in the future.

Simulating physical attachment and motion artifacts.A key feature of EgoSim is the consideration of camera attachment to account for motion artifacts during simulation. Since body-worn cameras are non-rigidly mounted, often coupled to clothing or strapped to the limbs like a smartwatch, the loose attachments can lead to slip and drag in the camera's position and orientation. EgoSim simulates these using spring arm mounts that connect the avatar's body and the virtual cameras. We demonstrate that spring-damper systems as a camera mounting model help to realistically capture the effects of loose camera attachment as found in the real world (Section 6.3).

Simulating diverse environments.EgoSim benefits from the vast selection of indoor and outdoor environments available in Unreal and previous work, e.g., . As shown in Figure 3, it can render both, large, realistic hand-modeled scenes and scanned scenes that closely resemble their real-world counterparts. The used scenes are in wide open spaces where motion capture is traditionally hard to perform. Additional details about EgoSim's features are provided in the appendix Table 3.

Synchronizing multi-sensor and multi-person recordings.Synchronizing multiple cameras poses challenges in real-world recordings, yet it is straightforward to generate synchronized multi-modal data in EgoSim while obtaining ground-truth characteristics of the environment or avatars. In addition EgoSim is capable of simulating and rendering data from _across_ multiple avatars and to obtain corresponding ground-truth poses and camera positions. EgoSim supports a flexible number of sensors, sensor characteristics, and attachment locations--independently for each avatar.

 Dataset & Mo2Cap2 & xR-EgoPose & EgoCap & EgoSim & UnrealEgo & EgoBody & ARES & MultipleView (ours) \\  Head cameras type & fishege & fishegev & wide stereo & stereo & fishegev stereo & front facing & fishegev stereo & front facing \\ sees body & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ & partly \\ Hand camera & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & 2\(\) \\ Leg camera & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Pelsk camera & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Image generation & composite & Mugas & compile & real & Unreal Engine & real & Replica & Unreal Engine \\ Image quality & low & high & real & open screen & high & real & high & high \\ Environment & \(_{}\) & Outdoor & Mostly Indoor & Indoor & Indoor & \(_{}\) & Outdoor & Indoor & Indoor & Indoor \\ Dataset Size & \(5.3\)kk & \(38\)k\(2 4\)k & \(2 170\)k & \(2 4\)k & \(220k\) & \(1.2\)M & \(6 1.2\)M \\ Real data & ✗ & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ Motion Diversity & mid & low & low & high & high & high & high & high \\ 

Table 1: Comparison of previous datasets for egocentric 3D human pose estimation.

Figure 2: EgoSim renders multiple modalities: (a) RGB, (b) depth, (c) normals, (d) semantic labels.

## 4 MultiEgoView Dataset

MultiEgoView contributes a sizeable and synchronized dataset of RGB data from six body-worn cameras, along with ground-truth body poses and activity annotations. Our dataset includes real and synthetic data, providing a challenging and interesting testbed for training and benchmarking body-pose estimation, activity classification, dynamic camera localization, and mapping algorithms.

Synthetic data generation.Using EgoSim, we rendered a dataset of 77.4 M RGB images corresponding to 119.4 hours captured by six virtual cameras on a virtual avatar. Images were rendered with a 118\({}^{}\) field of view (FOV) at a resolution of \(640 360\) and a framerate of 30 fps. Cameras were attached to the head, pelvis, wrists, and knees, facing outwards to capture both the environment and parts of the wearer's body. This considerably extends the focus of prior work on head-mounted cameras  and better resembles emerging wearable platforms devices . To ensure realistic motions, we animated avatars using motion capture sequences from AMASS , converted to FBX format for EgoSim support . We randomly varied avatar appearances in terms of skin color and clothing texture using BEDLAM's assets . Our dataset features 24 locations across 4 scenes: (1) a hand-built virtual outdoor environment of a city, (2) the front courtyard of a university building that we scanned using Polycam, with an accurate public point cloud scan and structure-from-motion model available , (3) Downtown city with skyscrapers and (4) a park with sport courts, lawn, vegetation and water. Each scene includes up to four simultaneously animated avatars to increase diversity and support multi-view multi-human pose estimation . In addition to the RGB data, we provide ground-truth camera and 3D avatar poses, as well as simulated accelerometer and gyroscope readings from all six cameras.

Real-world data collection.We captured a dataset of \(\)5 hours in the real world using six GoPro cameras (5\(\)HERO 10, 1\(\)HERO 9) , worn at the same body locations as in our simulation. We recruited 13 participants from places around our institution for this collection, who consented to participation and data recording. The study considers ETH ethics guidelines and Participants received a small gratuity for their time. Data was recorded in the same university front courtyard that was scanned for the synthetic environment (2), using GoPros set to a resolution of 1080p at 30 fps and a horizontal FOV of 118\({}^{}\). The 13 participants (4 female, 9 male, ages 21-30, mean = 26.4) were recruited from our institution, with heights ranging from 160-190 cm (mean = 176.1, SD = 9.5) and

Figure 3: Example RGB renders produced by EgoSim and included in our MultiEgoView dataset. Qualitatively, the simulated scan (d, e, f) and real data (g, h, i) look similar. Both simulated scenes (Scene 1: a, b, c; Scene 2: d, e, f) offer high-fidelity environments. The pelvis provides a stable view of the environment, whereas wrist and knee cameras typically move quickly and capture artifacts.

weights from 50-94 kg (mean = 69.6, SD = 13.2). After providing consent (see Appendix for details), participants were equipped with a full-body Xsens  motion capture suit for ground-truth pose capture. Following an initial calibration, participants performed a block of 35 different activities featuring the most common motions from AMASS according to the BABEL annotation . For a full list of activities, see our appendix Table 7. Each block lasted about 10 minutes, with participants repeating the block 1-3 times. To synchronize the GoPro camera with Xsens, participants clap at the beginning of each recording and match the camera with extracted SMPL poses. We compute shape parameters from the body measurement of participants with Virtual Caliper .

All sequences across real and synthetic data are labeled with activity classes from BABEL .

## 5 Baseline Method: Wearable Multi-Camera Body Pose Estimation

To demonstrate the benefits of MultiEgoView, we trained a neural network to estimate 3D ego body poses using multiple body-worn cameras. The input to the network consists of the aligned video sequences \(^{C F 3 H W}\), with \(F\) frames from \(C\) body-attached cameras. Based on these inputs, the network predicts a pose \(}_{i}\) for each input frame \(i\).

### Network architecture

Our network is a Vision Transformer Model based on Sparse Video Tube ViTs . We extract feature vectors from each input video using a sparse view tokenizer SVT with a shared interpolated kernel. The extracted feature vectors from the sparse tube tokenizers are then added to their fixed spatio-temporal position encoding \(_{p}\) and their learnable view encoding \(_{v,c}\) per camera \(c\).

\[_{c}=(_{c},)+_{p}+_{v,c}, \] (1)

The resulting feature vectors for the different cameras \(^{c}\) are concatenated with the pose token \(_{j}=(j)+,j[0,F-1]\), where \(\) is a trainable pose token and \(\) is a sinusoidal positional encoding. The resulting token sequence is then processed using a Vision Transformer Encoder.

\[\{_{0},,_{F-1}\}=((_{0},...,_{F-1},_{0},...,_{c-1}))\] (2)

Based on each embedded pose token \(\), we obtain the 6D representation  of the SMPL pose parameters \(\), the 6D relative rotation \(_{r}\), and 3D relative translation of the root \(r\) with respect the previous frame.

\[}=W_{},}_{r}=W_{R}, }_{r}=W_{t}\] (3)

To improve generalization, the network is trained to predict the pose difference, i.e., the relative root pose with respect to the previous pose, instead of directly predicting global root poses.

Using Forward Kinematics, we obtain the global body pose \(\) with respect to the starting pose.

\[\{}_{0},,}_{F-1}\}=_{}(,}_{g},}_{g},),}_{g},}_{g}=_{g}(}_{r},}_{r})\] (4)

Where \(\) are the shape parameters of the SMPL-X model  for a given person.

We use 4 tubes with the following configurations: \(16 16 16\) with stride \((12,48,48)\) and offset \((0,0,0)\), \(24 6 6\) with stride \((12,32,32)\) and offset \((8,12,12)\), \(12 24 24\) with stride \((24,48,48)\) and offset \((0,28,28)\), and \(1 32 32\) with stride \((12,64,64)\) and offset \((0,0,0)\). The pose embedding parameter is initialized using the Kaiming uniform distribution , and the pose token is initialized using the Normal distribution.

### Loss function

We supervise the network with the following loss function:

\[=_{}_{}+_{p}_{p}+ _{v}_{v}+_{t_{r}}_{t_{r}}+_{R_{r}} _{R_{r}}+_{t_{g}}_{t_{g}}+_{R_{g}} _{R_{g}}+_{z}_{z}\] (5)

The angle loss \(_{}\) encourages the model to learn the SMPL angles \(\), while the joint position loss \(_{p}\) forces the predicted joint positions through forward kinematics to be close to the ground-truth joint positions. This way, both the local and the accumulated errors are considered.

\[_{}=|_{}-}_{}|_ {1}_{p}=|-}|_{1},\] (6)where _6D_ indicates the six-dimensional representation of the rotation matrices . For the root pose, we penalize both the relative and absolute translation and orientation error accumulated through the kinematic chain,

\[_{R_{r}}=|_{r,6}-} _{r,6}|_{1}_{t_{r}}=|_{r}-}_{r}|_{1}\\ _{R_{g}}=\|}_{g}_{g}^{-1}-I\|_{2} _{t_{g}}=|_{g}-}_{g}|_{1}\] (7)

To encourage the model to estimate more expressive motions accurately, we add a velocity loss \(_{v}\). We also regularize the embedded pose token \(\) using an \(l_{2}\)-regularization term \(_{z}\).

\[_{v}=|(_{i}-_{i-1})-(}_{i}-}_{i- 1})|_{1}_{z}=\|\|_{2}\] (8)

We set \(_{}=10\), \(_{p}=25\), \(_{v}=40\), \(_{t_{r}}=25\), \(_{R_{r}}=15\), \(_{t_{g}}=1\), \(_{R_{g}}=0.025\), and \(_{z}=0.0005\).

## 6 Experiments

We empirically study the effectiveness of MultiEgoView for egocentric body pose estimation. Following the BABEL-60 split  (60%/20%/20%), sequences of synthetic data are divided into segments of up to 5 seconds. The baseline model directly takes inputs from all six cameras, normalizes the images to the ImageNet mean, and downsamples them to \(224 224\) pixels at \(10\,\). We accelerate the training process with a pre-trained sparse tube tokenizer on UCF101 . The model is trained using the Adam optimizer with a learning rate of \(1 10^{-4}\) on an Nvidia GeForce RTX 4090 with a batch size of 12 for 135k steps, taking around 3 days.

For real data, we use a random 80%/20% split with the same 5-second chunking and training parameters. We also conduct a cross-participant evaluation, using 10 participants for training and 3 for testing, to demonstrate the model's generalization ability.

### Quantitative metrics

We evaluate our model on a series of metrics using the body joints of the SMPLX model as follows:

**Global MPJPE (\(m\))**: Evaluates the mean \(l_{2}\)-norm between predicted and ground truth joint positions, punishing both pose and global position errors.
**PA-MPJPE (\(m\))**: Assesses pose estimation accuracy after aligning joint positions up to a similarity transform, isolating pure pose errors.
**MTE (\(m\))**: Mean Translation Error measures the mean \(l_{2}\)-norm of global root translation errors, indicating global translation accuracy.
**MRE**: Mean Rotation Error reports global orientation error using \(\|R^{-1}-I\|\).
**MJAE (\({}^{}\))**: Mean Joint Angle Error compares predicted joint angle errors in degrees without considering forward kinematic chain errors.
**Jerk (\(m/s^{3}\))**: Measures the smoothness of the predicted movement, indicating temporal continuity and naturalness of motion.

### Evaluation results

Table 2 shows the results of our multi-view pose transformer when trained on MultiEgoView. Training on synthetic data shows a low PA-MPJPE, implying a very good pose estimation. The slightly higher global MPJPE error arises due to a worse estimation of the root translation and rotation. The combination of synthetic and real data in MultiEgoView is crucial, as direct sim-2-real and training solely on real data fails to achieve accurate pose estimation. Pretraining on synthetic data followed by fine-tuning on real data improves the global MPJPE by 3.1-4 times and also lowers the PA-MPJPE by at least 2.7 cm, indicating a knowledge transfer of pose understanding from the large synthetic dataset to the real-world data. Even with a reduced fine-tuning train split of 20%, the network predicts accurate poses, though with a 8.8% increase in translation error. This showcases the benefit of synthetic data in improving pose estimation on scarce real training data. The results of the cross-participant evaluation lag behind the others. Indicating that more diversity could be required to obtain stable cross-participant results.

The visualization of the predicted poses in Figure 4 confirms the quantitative metrics. Generally, the model estimates the pose accurately. The biggest errors typically occur in the fast-moving limbs, as seen in the right column of Figure 4 where the model does mistakenly detect an arm movement. The pose outputs of the model are spatial-temporally smooth, which is also reflected in low jerk values (Table 2). Generally, the evaluations yielding lower Jerk indicate less active predictions that do not fully capture the full range and speed of the gt-motions (gt-jerk on eval set is 29.3) but still look natural. The model's weak point is the higher error global root position estimates. We attribute this weakness in global transformation prediction to two factors: 1) The model predicts the relative transformation between each frame, simplifying training by focusing on neighboring frames' transformations. However, small errors in relative prediction quickly accumulate in the forward kinematics. 2) The model lacks a method-based grounding of global position (e.g., through SLAM or SfM), making its transformation prediction reliant on learned environmental understanding.

Overall, MultiEgoView, with its synthetic and real-world data, shows its utility by enabling sim-to-real transfer learning. Our results also show that this would not be possible with just synthetic data or the amount of real data captured, validating the benefit of our simulator.

### Spring Damper

Body-worn cameras experience motion artifacts, especially when mounted on limbs that move quickly, due to non-rigid mounting points. EgoSim models these motion artifacts via a Spring Arm. We demonstrate that a spring-damper system approximates real camera motion better than a rigid mount. For that, we used an OptiTrack motion capture system to track both the attached body and the GoPro that we loosely attached to the body. Using an OptiTrack motion capture system, we tracked both the body and a loosely attached GoPro. Results show the spring-damper model yields a lower mean position error (1.98 cm) compared to the rigid model (2.35 cm), highlighting its effectiveness.

## 7 Discussion

While egocentric pose estimation has been well explored, in prior implementations head-mounted cameras faces challenges such as self-occlusion, reduced resolution for lower body reconstruction,

Figure 4: Visualization of our results obtained from our multi-view egocentric pose estimator on real-world data. The change of color denotes different timestamps.

 Method &  &  &  \(\) &  \(\) &  \\ trained on & evaluated on & MPJPE \(\) & MPJPE \(\) & MTE \(\) & MRE \(\) & MJE \(\) & Jerk \\  Synthetic & Synthetic & 0.16 & 0.040 & 0.13 & 0.272 & 9.1 & 21.9 \\ Synthetic & Real & 0.77 & 0.119 & 0.71 & 0.947 & 29.0 & 20.9 \\ Real & Real & 1.23 & 0.087 & 0.79 & 1.030 & 16.4 & 1.5 \\ _with fine-tuning:_ & & & & & & & \\ Synthetic + 20\% Real & Real & 0.40 & 0.056 & 0.37 & 0.504 & 12.8 & 15.4 \\ Synthetic + 80\% real & Real & 0.33 & 0.044 & 0.31 & 0.415 & 10.2 & 16.7 \\ Synthetic + 10 real participants & Real (cross-participant) & 0.35 & 0.060 & 0.32 & 0.557 & 16.6 & 18.3 \\ 

Table 2: Results of our method on MultiEgoView, showing the benefit of our simulator.

and lack of environmental information. Here, multiple body-worn cameras can mitigate these by providing dynamic, multi-view perspectives that simultaneously capture the environment and the body and, more importantly, the interaction between our hands and legs with the surroundings.

EgoSim, together with MultiEgoView, is a first stepping stone to deepen our understanding of human activity from body-worn cameras at various locations. We showcase the usefulness of MultiEgoView for ego pose estimation with our learned video-based end-to-end multi-view model. Our findings show that ego pose can effectively be estimated from several body-mounted cameras and EgoSim's rendered data helps obtain better pose estimation in sim-to-real scenarios.

**Limitations of EgoSim.** Our current simulator has some limitations that will be addressed in future iterations. First, although our data includes multi-human scenarios, individual avatar animations are sampled independently from AMASS. These animations, while physically plausible, do not account for interactions with other humans or objects, limiting the study of such interactions. Additionally, our system currently features only four scenes, which can be extended to improve the generalization. Lastly, while our simulator supports high-fidelity rendering, improvements in graphics and neural rendering methods  are expected to reduce the simulation-to-real gap further.

**Future research on MultiEgoView.** While the pose estimation capabilities of our multi-view transformer trained using EgoMultiView are convincing on real-world data (PA-MPJPE < 5 cm), there is still room for improvement in the global position and orientation estimation of the root, especially for long sequences, where cumulative errors in root position become more pronounced. Future research directions could consider integrating low-drift camera localization methods, such as SLAM , or image-based localization via structure from motion , to achieve more stable global translation and orientation. Moreover, our current experiments only utilize RGB data. Future research could leverage MultiEgoView to enhance inertial-based pose estimation , depth estimation using monocular or multiple cameras , and semantic scene classification , all of which are supported by the ground-truth annotations provided by our simulator.

## 8 Conclusion

We have proposed EgoSim, an egocentric multi-view simulator for body-worn cameras that generates multiple data modalities to support emerging wearable motion capture and method development. Using EgoSim, we partially generated MultiEgoView, the first dataset that complements existing head-focused egocentric datasets with synchronized footage from six cameras worn at other locations on the body, simulated from accurate and real human motion and artifacts. We complement MultiEgoView's 119 hours of synthetic data with 5 hours of actual recordings from 6 body-worn GoPro cameras and 13 participants during a wide range of motions and activities in the wild with annotated 3D body poses and classification labels to bridge the gap between simulation and real-world data.

In the wake of the emerging area of vision-based method development from one or more body-worn sensors, we believe that our release of EgoSim and MultiEgoView will be a useful resource for future work to increase our understanding of human activities and interactions in the real world.