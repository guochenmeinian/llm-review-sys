# Equivariant Light Field Convolution and Transformer

in Ray Space

 Yinshuang Xu

University of Pennsylvania

xuyin@seas.upenn.edu

&Jiahui Lei

University of Pennsylvania

leijh@seas.upenn.edu

&Kostas Daniilidis

University of Pennsylvania

and Archimedes, Athena RC

kostas@cis.upenn.edu

###### Abstract

3D reconstruction and novel view rendering can greatly benefit from geometric priors when the input views are not sufficient in terms of coverage and inter-view baselines. Deep learning of geometric priors from 2D images requires each image to be represented in a \(2D\) canonical frame and the prior to be learned in a given or learned \(3D\) canonical frame. In this paper, given only the relative poses of the cameras, we show how to learn priors from multiple views equivariant to coordinate frame transformations by proposing an \(SE(3)\)-equivariant convolution and transformer in the space of rays in 3D. We model the ray space as a homogeneous space of \(SE(3)\) and introduce the \(SE(3)\)-equivariant convolution in ray space. Depending on the output domain of the convolution, we present convolution-based \(SE(3)\)-equivariant maps from ray space to ray space and to \(^{3}\). Our mathematical framework allows us to go beyond convolution to \(SE(3)\)-equivariant attention in the ray space. We showcase how to tailor and adapt the equivariant convolution and transformer in the tasks of equivariant \(3D\) reconstruction and equivariant neural rendering from multiple views. We demonstrate \(SE(3)\)-equivariance by obtaining robust results in roto-translated datasets without performing transformation augmentation.

## 1 Introduction

Recent years have seen significant advances in learning-based techniques  harnessing the power of deep learning for extraction of geometric priors from multiple images and associated ground-truth shapes. Such approaches extract features from each view and aggregate these features into a geometric prior. However, these approaches are not \(SE(3)\)-equivariant to transformations of the frame where the priors and images are defined. While view pooling or calculating variance  can be used to aggregate features and tackle equivariance, view pooling discards the rich geometric information contained in a multiple view setup.

In this paper, we address the problem of learning geometric priors that are \(SE(3)\)-equivariant with respect to transformations of the reference coordinate frame. We argue that all information needed for tasks like novel view rendering or 3D reconstruction is contained in the light field . Our input is a light field, a function defined on oriented rays in 3D whose values can be the radiance or features extracted from pixel values. We will use the term light field, and we will be specific when it is a radiance field or a feature field. Images are discrete samples of this field: the camera position determines which rays are sampled, while the camera orientation leaves the sample of the light field unchanged up to pixel discretization. We model the light field as a field over a homogeneous space of \(SE(3)\), the ray space \(\) parameterized by the Plucker coordinates. The ray space \(\) is the space of oriented light rays, for any ray \(x\), the Plucker coordinate is \(x=(,)\), where \(^{2}\) is the direction of the ray, and \(=\) where \(\) is any point on the ray.

We define a convolution in the continuous ray space as an equivariant convolution on a homogeneous space . Since our features are not limited to scalar values, we will draw upon the tools of tensor field networks and representation theory, discussed in detail in the Appendix. In Sec. 3.1 we study the group action of \(SE(3)\) on \(\), the stabilizer group for \(\), and how \(SE(3)\) transforms the feature field over \(\). In Sec. 3.4, we focus on developing the equivariant convolution in \(\), providing analytical solutions for the kernels with the derived constraints in convolution from \(\) to \(\) and from \(\) to \(^{3}\), respectively. Meanwhile, we make the kernel locally supported without breaking the equivariance. By varying the output domain of the convolution, we introduce equivariant convolutions from the ray space to the ray space and from the ray space to the \(3D\) Euclidean space.

The constraint of the kernel limits the expressiveness of equivariant convolution when used without a deep structure. In Sec 3.5, we introduce an equivariant transformer in \(\). The equivariant transformer generates the equivariant key, query, and value by leveraging the kernel derived in the convolution, resulting, thus, in invariant attention weights and, hence, equivariant outputs. We provide a detailed derivation of two cases of cross-attention: the equivariant transformer from \(\) to \(\) and the equivariant transformer from \(\) to \(^{3}\). In the first case, the features that generate the key and value are attached to source rays, while the feature generating the query is attached to the target ray. In the second case, the feature generating the query is attached to the target point.

We demonstrate the composition of equivariant convolution and transformer modules in the tasks of \(3D\) reconstruction from multi-views and novel view synthesis given the multi-view features. The inputs consist of finite sampled radiance fields or finite feature fields, while our proposed equivariant convolution and transformer are designed for continuous light fields. If an object or a scene undergoes a rigid transformation and is resampled by the same multiple cameras, the \(SE(3)\) group action is not transitive in the light field sample. This lack of transitivity can significantly impact the computation of equivariant features, mainly because the views are sparse, unlike densely sampled point clouds. Object motion introduces new content, resulting in previously non-existing rays in the light field sampling. Hence, our equivariance is an exact equivariance with respect to the choice of coordinate frame. In the 3D reconstruction task, we experimentally show that equivariance is effective for small camera motions or arbitrary object rotations and generally provides more expressive representations. In the \(3D\) object reconstruction application, we first apply an equivariant convolutional network in ray space to obtain the equivariant features attached to rays. We then apply equivariant convolution and equivariant transformer from \(\) to \(^{3}\) to obtain equivariant features attached to the query point, which are used to calculate the signed distance function (SDF) values and ultimately reconstruct the object. In the generalized rendering task, our model queries a target ray and obtains neighboring rays from source views. Our composition of equivariant modules is based on IBRNet , which consists of view feature aggregation and ray transformer. We replace the view feature aggregation in  with the equivariant convolution and transformer over rays and the ray transformer part with the equivariant transformer the points along the ray to get the density and color of the point, see Sec. 3.3. We summarize here our main contributions:

(1) We model the ray space as a homogeneous space with \(SE(3)\) as the acting group, and we propose the \(SE(3)\)-equivariant generalized convolution as the fundamental operation on a light field whose values may be radiance or features. We derive two \(SE(3)\)-equivariant convolutions, both taking input ray features and producing output ray features and point features, respectively.

(2) To enhance the feature expressiveness, we extend the equivariant convolution to an equivariant transformer in \(\), in particular, a transformer from \(\) to \(\) and a transformer from \(\) to \(^{3}\).

(3) We adapt and compose the equivariant convolution and transformer module for \(3D\) reconstruction from multiple views and generalized rendering from multi-view features. The experiments demonstrate the equivariance of our models.

## 2 Related Work

Equivariant NetworksGroup equivariant networks [15; 65; 62; 55; 63; 12; 19; 17; 22; 24; 23] provide deep learning pipelines that are equivariant by design with respect to group transformations of the input. While inputs like point clouds, 2D and 3D images, and spherical images have been studied extensively, our work is the first, as far as we know, to study equivariant convolution and cross-attention on light fields. The convolutional structure on homogeneous spaces or groups is sufficient and necessary for equivariance with respect to compact group actions as proved in [18; 1; 36]. Recently, Cesa et al. , Xu et al.  provided a uniform way to design the steerable kernel in an equivariant convolutional neural network on a homogeneous space using Fourier analysis of the stabilizer group and the acting group, respectively, while Finzi et al.  proposed a numerical algorithm to compute a kernel by solving the linear equivariant map constraint. For arbitrary Lie groups, Finzi et al. , MacDonald et al. , Bekkers  designed the uniform group convolutional neural network. The fact that any \(O(n)\) equivariant function can be expressed in terms of a collection of scalars is shown in . For general manifolds, Cohen et al. , Weiler et al.  derived the general steerable kernel from a differential geometry perspective, where the group convolution on homogeneous space is a special case. The equivalent derivation for the light field is in the Appendix. Recently, equivariant transformers drew increasing attention, in particular for 3D point cloud analysis and reconstruction [27; 48; 10; 7]. A general equivariant self-attention mechanism for arbitrary groups was proposed in [44; 43], while an equivariant transformer model for Lie groups was introduced in Hutchinson et al. . We are the first to propose an equivariant attention model in the \(3D\) ray space.

Light Field and Neural Rendering from Multiple ViewsThe plenoptic function introduced in perception  and later in graphics  brought a new light into the scene representation problem and was directly applicable to the rendering problem. Instead of reconstructing and then rendering, light fields enabled rendering just by sampling the right rays. Recently, learning-based light field reconstruction [41; 34; 5; 66; 51; 2] became increasingly popular for novel view synthesis, while [50; 54; 53] proposed non-equivariant networks in the ray space. Due to the smaller dimension of the ray space, the networks in the ray space are more efficient compared to neural radiance fields , which leverages volumetric rendering. Several studies [73; 61; 53; 50; 11; 38; 13; 31; 58; 20] concentrate on generalizable rendering. These works are similar to ours in that they obtain the \(3D\) prior from the \(2D\) images, but they are not equivariant since they explicitly use the coordinates of the points or the rays in the network.

The most related equivariant rendering approaches to us are [21; 47; 45]. The equivariance in the paper  is not the equivariance we mean in this work:  enforces the geometric consistency via a loss function.  is not strictly equivariant and it depends on the assumption of upright views and the camera ID embeddings, which results in its non-equivariance to camera permutation. It achieves data-driven equivariance by randomly choosing the first camera frame as the canonical frame. While  addresses the frame problem through relative pose, it is not theoretically equivariant in cases of individual camera rotations around their axes or minor individual rotations accompanied by small content changes. We want to emphasize that our central contribution is to propose an equivariant convolution and transformer on ray space, which can be integrated into a wide range of 3D learning models.

Reconstruction from Multiple ViewsDense reconstruction from multiple views is a well-established field of computer vision with advanced results even before the introduction of deep learning . Such approaches cannot take advantage of shape priors and need a lot of views to provide a dense reconstruction. Deep learning enabled semantic reconstruction, i.e., the reconstruction from single or multiple views by providing the ground-truth 3D shape during training [14; 67; 68; 40]. These approaches decode the object from a global code without using absolute or relative camera poses. Regression of absolute or relative poses applied in [35; 71; 56; 69; 72; 57; 3; 46; 33; 20] is non-equivariant.

## 3 Method

In this section, we will first introduce the (feature) field on the ray space and \(3D\) Euclidean space, respectively, and the corresponding \(SE(3)\) group actions on the values of the fields in Sec. 3.1. To offer readers a holistic grasp--from a broad overview down to the intricate specifics and from foundational concepts to advanced techniques-- we will present the reconstruction and the generalized rendering with the neural components of their architectures (convolutional and attentional) and their inputs and outputs in Sec. 3.2 and Sec. 3.3. Following that, we expose our central contribution: equivariant convolution and attention in ray space in Sec. 3.4 and Sec. 3.5.

### Feature field on Ray Space and \(3d\) Euclidean Space

#### 3.1.1 Ray Space

The ray space is the space of oriented light rays. As introduced in the introduction and App. Ex. 1, we use Plucker coordinates to parameterize the ray space \(\): for any ray \(x\), \(x\) can be denoted as \((,)\), where \(^{2}\) is the direction of the ray, and \(=\) is the moment of the ray with being a point on the ray. Then any \(g=(R,) SE(3)\) acts on the the ray space as:

\[gx=g(,)=(R,R+(R)).\] (1)

The ray space \(\) is a homogeneous space with a transitive group action by \(SE(3)\). Given the origin in the homogeneous space as \(=(^{T},^{T})\) (the line representing \(z\)-axis), the stabilizer group \(H\) that leaves \(\) unchanged is \(SO(2)\) (the rotation around and translation along the ray). The ray space is, thus, isomorphic to the quotient space \( SE(3)/(SO(2))\). We parameterize the stabilizer group \(H\) as \(H=\{(,t)|[0,2),t\}\).

We follow the generalized convolution derivation for other homogeneous spaces in , which requires the use of principal bundles, section maps, and twists  explained in the appendix section A.2 and onwards. \(SE(3)\) can be viewed as the principal \(SO(2)\)-bundle, where we have the projection \(p:SE(3)\), for any \(g SE(3)\), \(p(g)=g\); a section map \(s: SE(3)\) can be defined such that \(p s=id_{}\). In App. Example 6, we elaborate on how we define the section map from the ray space to \(SE(3)\) in our model. Generally, the action of \(SE(3)\) induces a twist as \(gs(x) s(gx)\). The twist can be characterized by the twist function \(:SE(3) SO(2)\), \(gs(x)=s(gx)(g,x)\), we provide the twist function in our model and its visualization in App. Example 6.

#### 3.1.2 Light Field

The light field can be modeled as a function from the ray space to a vector space, \(f: V\). We also need to define the \(SE(3)\) group action on the values of that field. Since the group action will be on a vector space \(V\), we will use the corresponding group representation of the stabilizer group \(:SO(2) GL(V)\), see details in App. A.3. For example, a light field can be a radiance field \(f\) that maps the ray space of oriented rays to their observed radiance (RGB) \(f:^{3}\) which is a concatenation of three scalar fields over \(\). The group representation \(\) in this case is the identity and \(g SE(3)\) acts on the radiance field \(f\) as \((_{g}f)(x)=f(g^{-1}x)\), shown as the scalar features in Fig. 1. Given that the stabilizer \(H=SO(2)\) is a product group, the stabilizer representation can be written as the product \((,t)=_{1}()_{2}(t)\), where \(_{1}\) is the group representation of \(SO(2)\) and \(_{2}\) is the group representation of \(\). If the light field is a feature field (Fig. 1) with \(_{2}\) being the identity representation and \(_{1}\) corresponding to a type-1 field, \(_{1}()=e^{i}\), then type-1 features change position and orientation when \(g SE(3)\) acts on it. Having explained the examples of scalar (type-0) and type-1 fields, we introduce the action on any feature field \(f\) as :

\[(_{g}f)(x)=((g^{-1},x)^{-1})f(g^{-1}x),\] (2)

where \(\) is the group representation of \(SO(2)\) corresponding to the space \(V\), determined by the field type of \(f\), and h is the twist function introduced by \(SE(3)\) as shown in App. Example 6.

#### 3.1.3 Feature Field on \(^{3}\)

\(^{3}\) is also a homogeneous space of \(SE(3)\) like the ray space \(\), with the stabilizer group as \(SO(3)\), as stated in App. Example 2. For any \(g=(R,) SE(3)\), it acts on the field \(f\) over \(^{3}\) also follows

Figure 1: Feature attached to rays: we show the scalar feature and type-1 feature. When \(_{2}\) is the trivial representation, tensor features can be viewed in the plane orthogonal to the ray (the blue plane). When rotations act on the feature field, the scalar feature only changes position as attached to the rays: \((_{g}f)(x)=f(g^{-1}x)\); while the type-1 feature changes position and is itself rotated: \((_{g}f)(x)=((g^{-1},x)^{-1})f(g^{-1}x)\), where \((,t)=e^{i}\).

Figure 2: Features attached to points: we show scalars and vectors (type-1 features). The black dot in the figure is the point, and the square and the vectors are the scalar and type-1 features attached to the point. When \(g SE(3)\) acts on the feature field, we will see that the scalars are kept the same while the attached position is rotated, and the vector features change their position and alter their direction.

:

\[(_{g}f)(x)=(R)f(R^{-1}(x-))\]

,where \(\) is the group representation of \(SO(3)\), since the twist function can be independent of the \(3D\) position due to the fact that \(SE(3)=^{3} SO(3)\) is a semidirect product group as stated in App. Example 4. The feature field over \(^{3}\) and the corresponding group action is also used in . Fig. 2 visualizes the scalar feature (\(l_{out}=0\)) and vector feature (\(l_{out}=1\)) attached to one point, offering an intuitive understanding of the feature field over \(^{3}\).

Given the feature field on the ray space and \(3D\) Euclidean space and the corresponding group actions of \(SE(3)\). We will show two 3D multi-view applications of the equivariant convolution and transformer: \(3D\) reconstruction and generalized neural rendering. In each application, we start with the specific definition of equivariance and then outline the corresponding pipeline.

### Equivariant 3D Reconstruction

The radiance field serves as the input for the \(3D\) reconstruction, which ultimately generates a signed distance field (SDF) denoted by the function \(e:^{3}\). As aforementioned, the radiance field is the multi-channel scalar field over \(\), while SDF is the scalar field over \(^{3}\). A \(3D\) reconstruction \(:\), where \(\) denotes the space of radiance fields and \(\) denotes the space of signed distance fields, is equivariant when for any \(g SE(3)\), any \(x^{3}\), and any \(f\), \(_{g}f)(x)}=_{g}^{}(\), where \(_{g}\) and \(_{g}^{}\) are group actions on the light field and the SDF, respectively. Specifically, as \(f\) and \(e\) are scalar fields, \((_{g}f)(x)=f(g^{-1}x)\) for any \(x\), and \((_{g}^{}e)(x)=e(g^{-1}x)\) for any \(x^{3}\).

In practice, we have a finite sampling of the radiance field corresponding to the pixels of multiple views \(V=\{f(x)|x L_{V}\}\), where \(L_{V}\) denotes the ray set of multi-views and \(f\) is the radiance field induced by multi views sample from. The \(3D\) reconstruction \(\) is equivariant when for any \(g SE(3)\) and any \(x^{3}\): \(=x)}\). If we denote \(V\) as \((L_{V},f)\), \(g V=(g L_{V},_{g}f)\), where \(g L_{V}\) is \(g\) acting on the rays defined Eq. 1.

We achieve equivariance using three steps as illustrated in Fig. 3: (1) the transition from pixel colors to a feature-valued light field (equi-CNN over rays), (2) the computation of features in \(^{3}\) from features on the ray space by equivariant convolution from \(\) to \(^{3}\), and (3) the equivariant transformer with the query generated by the feature on the point we want to compute SDF and key/value generated by features on rays. Note that we need (3) following (2) because the output feature of a single convolution layer is not expressive enough due to the constrained kernel.For the detailed practical adaption of the convolution and transformer in \(3D\) reconstruction, please see the App. B, where we approximate the intra-view with \(SE(2)\) equivariant convolution.

### Generalized Neural Rendering

The light feature field \(f_{in}: V\) serves as the input for neural rendering, which ultimately generates the light field \(f:^{3}\), a multi-channel scalar field over \(\). A neural rendering \(:\), where \(\) denotes the space of the light feature fields and \(\) denotes the space of the light field, is equivariant when for any \(g SE(3)\), any \(x\), and any \(f_{in}\), \(_{g}f_{in})(x)}=)(g^{-1}x)}\),where \(_{g}\) is the group operator on the light feature field \(f_{in}\), as shown in Eq. 2 depending on the feature type. In the experiment of this paper, the input light feature field is

Figure 3: The pipeline of equivariant \(3D\) reconstruction: Firstly, we obtain the feature field over the ray space. Secondly, we perform an equivariant convolution from ray space to point space. Thirdly, we apply a \(SE(3)\) equivariant cross-attention module to obtain an equivariant feature for a query.

scalar, i.e., \(_{g}f_{in}(x)=f_{in}(g^{-1}x)\). Similar to reconstruction, in practice, the neural rendering \(\) is equivariant when for any \(g SE(3)\) and any \(x\): \(x)}\), where \(V=\{f_{in}(x)|x L_{V}\}\), and if we denote \(V\) as \((L_{V},f_{in})\), then \(g V=(g L_{V},_{g}f_{in})\),

By restricting the field type of the output field over rays to have a group representation of \(SO(2)\) as \((,t)=_{1}()_{2}(t)\), where \(_{2}\) is the regular representation, we can obtain the feature of points along the ray by convolution or transformer from \(\) to \(\). See App. Example 9 for more explanation of the regular representation. Alternatively, we can obtain the desired feature by applying convolution or transformer from \(\) to \(\), with output features attached to the target ray corresponding to different irreducible representations of the stabilizer group. These features can be interpreted as Fourier coefficients of the function of the points along the ray. The Inverse Fourier Transform yields features for the points along the ray. More details are in the App. I.1.

The feature of the points along the ray can be used to generate density and color for volumetric rendering , or fed into attention and pooling for the final ray feature . In this paper, we opt to generate the density and color and utilize volumetric rendering, which can be viewed as a specialized equivariant convolution from points to the ray. Method details are in App. I.

We achieve the equivariant rendering through three steps as shown in Fig. 4: (1) we apply equivariant convolution from rays to rays to get the equivariant feature for points along the rays, which is a specific field type over \(\); 2) to enhance the feature expressivity, we apply an equivariant transformer from rays to rays to get the color for each point; (3) we apply the equivariant self-attention over the points along the ray to reason over the points on the same ray; the output feature of the points will be fed to multiple perceptron layers to get the density of the points.

### Convolution in Ray Space

#### 3.4.1 Convolution from Rays to Rays

The convolution, as stated in App. A.4 and  is then defined as

\[f^{l_{out}}(x)=_{}(s(x)^{-1}y)_{in}((s(x)^ {-1}s(y)))f^{l_{in}}(y)dy,\] (3)

where \((g)\) is the simplified form of the twist \((g,)\). Eq.3 is equivariant to \(SE(3)\) if and only if the convolution kernel \(\) satisfies that \((hx)=_{out}(h)(x)_{in}(^{-1}(h,x))\), where \(_{in}\) and \(_{out}\) are the group representations of \(SO(2)\) corresponding to the input feature type \(l_{in}\) and output feature type \(l_{out}\), respectively. We derive the solutions of the kernel in the App. Example 9.

Local kernel supportThe equivariance stands even if we constrain the kernel to be local. When \(x=(_{x},_{x})\) meets the condition that \((_{x},^{T})_{0}\) and \(d(x,) d_{0}\), \((x) 0\), this local support will not violate the constraint that \((hx)=_{out}(h)(x)_{in}(^{-1}(h,x))\).

Figure 4: The pipeline of equivariant neural rendering. Firstly, we obtain the features of the points along the target ray through convolution over rays. Secondly, we apply the equivariant cross-attention module to obtain features for generating the color of the points. Finally, we use equivariant self-attention over the points along the ray to obtain features for generating the density of points.

Then, convolution in Eq. 3 is accomplished over the neighbors only as visualized in Fig. 5. In Fig. 5, any ray \(y=(_{y},_{y})\) (denoted in blue) in the neighborhood of a ray \(x=(_{x},_{x})\) will go through the cylinder with \(x\) as the axis and \(d_{0}\) as the radius since \(d(x,y) d_{0}\).

Moreover, for any \(y\), \((_{y},_{x})_{0}\). Any ray \(y(x)\) is on one tangent plane of a cylinder with \(x\) as the axis and \(d(x,y)\) as the radius when \(d(x,y)>0\).

#### 3.4.2 Convolution from Rays to Points

In applications such as \(3D\) reconstruction, key point detection, and \(3D\) segmentation, we expect the output to be the field over \(^{3}\). Using a convolution, we will define an equivariant map from light fields (fields on \(\)) to fields on \(^{3}\). We denote with \(H_{1}\) and \(H_{2}\) the stabilizer groups for the input and output homogeneous spaces, respectively, i.e., \(SO(2)\) and \(SO(3)\) in this case. As shown in the App. Example 4, we can choose the section map \(s_{2}:^{3} SE(3)\): \(s_{2}()=(I,)\) for any \(^{3}\) and I is the identity matrix. Following , the convolution from rays to points becomes:

\[f_{2}^{l_{out}}(x)=_{}(s_{2}(x)^{-1}y)_{in}(_{1}(s_{2}(x)^{-1}s_{1}(y)))f_{1}^{l_{in}}(y)dy,\]

where \(_{1}\) is the twist function corresponding to section \(s_{1}: SE(3)\) defined aforementioned, \(_{in}\) is the group representation of \(H_{1}\) (\(SO(2)\)) corresponding to the feature type \(l_{in}\). The subscripts \(1\) and \(2\) denote the homogeneous spaces the features are defined on. The convolution is equivariant if and only if the kernel \(\) satisfies that \((h_{2}x)=_{out}(h_{2})(x)_{in}(_{1}^{-1}(h_{2},x))\) for any \(h_{2} H_{2}\), where \(_{out}\) is the group representation of \(H_{2}\) (\(SO(3)\)) corresponding to the feature type \(l_{out}\).

In 3D reconstruction, \(f^{l_{in}}\) is the scalar field over \(\), i.e., \(_{in}=1\). The convolution is simplified to \(f_{2}^{l_{out}}(x)=_{G/H_{1}}(s_{2}(x)^{-1}y)f_{1}^{l_{in}}(y)dy\) and the corresponding constraint becomes \((h_{2}x)=_{out}(h_{2})(x)\). App. Example 10 provides analytical kernel solutions.

### Equivariant Transformer over Rays

We can extend the equivariant convolution to the equivariant transformer model. In general, the equivariant transformer can be formulated as:

\[f_{2}^{out}(x)=_{y(x)}(x,f_{2}^{in}),f_{k}(x,y,f_{1}^{in}))}{_{y(x)}exp( f_{q}(x, f_{2}^{in})f_{k}(x,y,f_{1}^{in}))}f_{v}(x,y,f_{1}^{in}),\] (4)

where the subscript \(1\) denotes the homogeneous space \(M_{1} G/H_{1}\) of the feature field \(f_{1}^{in}\) that generates the key and value in the transformer; the subscript \(2\) denotes the homogeneous space \(M_{2} G/H_{2}\) of the feature field \(f_{2}^{in}\) that generates query in the transformer, which is also the homogeneous space of the output feature \(f_{2}^{out}\); \(x\) and \(y\) represent elements in the homogeneous spaces \(M_{2}\) and \(M_{1}\), respectively, where \(y(x)\) indicates that the attention model is applied over \(y\), the neighbor of \(x\) based on a defined metric. \(f_{k}\), \(f_{q}\), and \(f_{v}\) are constructed equivariant keys, queries, and values in the transformer. \(f_{k}\) and \(f_{v}\) are constructed by equivariant kernel \(_{k}\) and \(_{v}\) while \(f_{q}\) is constructed through an equivariant linear map, see App. F for detailed construction.

When the transformer is a self-attention model, homogeneous space \(M_{1}\) and \(M_{2}\) are the same since \(f_{2}^{in}=f_{1}^{in}\). The above equivariant transformer could be applied to the other homogeneous space other than \(\), \(^{3}\), and acting group other than \(SE(3)\). This paper presents the equivariant cross-attention model over rays, i.e., \(M_{1}\) is \(\). When the transformer is the cross-attention from rays to rays, \(M_{2}\) is also \(\), the equivariant kernel \(_{k}\) and \(_{v}\) is the convolution kernel we derived in convolution from rays to rays in Sec. 3.4.1. When the transformer is the cross-attention from rays to points, \(M_{2}\) is \(^{3}\), the equivariant kernel \(_{k}\) and \(_{v}\) is the convolution kernel we derived in convolution from rays to points in Sec. 3.4.2. With the construction in App. F, we claim that the transformer from rays to rays or from rays to points, as shown in the equation 4, is equivariant. The proof is in App. G.

Figure 5: Neighborhood of a ray \(x\) in the convolution.

[MISSING_PAGE_EMPTY:8]

\(SO(3)/SO(3)\) settings while it is slightly inferior to  in the airplane category. Notably, our model only requires relative camera poses, while  and  utilize camera poses relative to the object frame, leveraging explicit positional encoding of the query point in the object frame, which is concatenated to the point feature. In addition, our model performs better in several experiments in \(I/I\) and \(Y/Y\) settings. This superiority can be attributed to the \(SE(3)\) equivariant attention model, which considers scalar features and ray directions. For a detailed discussion of the results, please see appendix Sec. J.3

We provide an ablation study of the effectiveness of \(SE(2)\) CNNs, equivariant convolution, transformer, and type-1 feature (vector feature) in our model. Meanwhile, we compare our method with the model that explicitly encodes the direction of rays. Please see the App. J.5 for the details of the ablation study.

### Neural Rendering

Datasets and ImplementationWe use the same training and test dataset as in , which consists of both synthetic and real data. Two experiment settings illustrate our model's equivariance: \(I/I\) and \(I/SO(3)\). \(I/I\) is the canonical setting, where we train and test the model in the same canonical frame defined in the dataset. In the \(I/SO(3)\) setting, we test the model trained in the conical frame under arbitrarily rotated coordinate frames while preserving relative camera poses and the relative poses between the camera and the scene, thereby preserving the content of the multiple views. Each individual view itself is not transformed. Note that this experiment's \(SO(3)\) setup differs from the \(R\) and \(SO(3)\) setups used in the reconstruction. Further details and discussions on this difference can be found in App. K.1.

  Method &  \\   & I/I & I/Z & I/R & R/R & Y/Y & Y/SO(3) & SO(3)/SO(3) \\   Fvor w/ gt pose & 0.691/0.099 & 0.409/0.253 & 0.398/0.257 & 0.669/0.113 & 0.687/0.103 & 0.518/0.194 & 0.664/0.114 \\ DISN w/ gt pose & 0.725/0.094 & 0.335/0.396 & 0.322/0.405 & 0.500/0.201 & 0.659/0.120 & 0.419/0.303 & 0.549/0.174 \\ Ours & **0.731/0.090** & **0.631/0.130** & **0.592/0.137** & **0.689/0.105** & **0.698/0.102** & **0.589/0.142** & **0.674/0.113** \\  Method &  \\   & I/I & I/Z & I/R & R/R & Y/Y & Y/SO(3) & SO(3)/SO(3) \\   Fvor w/ gt pose & 0.770/0.051 & 0.534/0.168 & 0.533/0.174 & **0.766/0.053** & **0.760/0.052** & 0.579/0.147 & **0.746/0.056** \\ DISN w/ gt pose & 0.752/0.053 & 0.465/0.173 & 0.462/0.171 & 0.611/0.104 & 0.760/0.069 & 0.530/0.151 & 0.631/0.103 \\ Ours & **0.73/0.050** & **0.600/0.092** & **0.579/0.100** & **0.759/0.051** & **0.734/0.052** & **0.597/0.101** & **0.722/0.056** \\  Method &  \\   & I/I & I/Z & I/R & R/R & Y/Y & Y/SO(3) & SO(3)/SO(3) \\   Fvor w/ gt pose & 0.837/0.090 & 0.466/0.254 & 0.484/0.258 & 0.816/0.107 & **0.830**/0.094 & 0.496/0.240 & 0.798/0.111 \\ DISN w/ gt pose & 0.822/0.089 & 0.610/0.232 & 0.567/0.236 & 0.772/0.135 & 0.802/0.098 & 0.614/0.205 & 0.769/0.123 \\ Ours & **0.844/0.081** & **0.739/0.142** & **0.741/0.150** & **0.836/0.089** & **0.830**/**0.089** & **0.744/0.137** & **0.813/0.097** \\  

Table 1: The results for the seven experiments of 8-view \(3D\) reconstruction for the ShapeNet dataset. The metrics in the cell are _IoU_\(\) and _Chamfer-L1 Distance_\(\). We implement  and  ourselves on our equivariant dataset. For the performance of , we follow their work to conduct the multi-view reconstruction by pooling over the feature of every view. The value of _Chamfer-L1 Distance_ is \( 10\).

Figure 8: Qualitative results for equivariant reconstruction. Left: input views; Right: reconstruction meshes of different models and ground truth meshes show how the model is trained and tested, explained in the text.

Our model architecture is based on IBRNet, with view feature aggregation and ray transformer components modifications. Specifically, we replace the view feature aggregation in  with the equivariant convolution and transformer over rays and the ray transformer part with the equivariant self-attention over the points along the ray. For more information on the implementation details, please refer to App. K.2.

ResultsWe compare with IBRNet on \(I/I\) and \(I/SO(3)\) settings to show that our proposed models can be embedded in the existing rendering framework and achieve equivariance. Following previous works on novel view synthesis, our evaluation metrics are PSNR, SSIM, and LPIPS . In the \(I/SO(3)\) test period, we randomly rotate each data six times and report the average metrics. Meanwhile, we record the max pixel variance and report the average value. We show a qualitative result in Fig. 9, where IBRNet presents several blurred transverse lines in the \(I/SO(3)\) setting while ours are robust to the rotation. In table 2, our model performs comparably with IBRNet  in \(I/I\) setting without performance drop in \(I/SO(3)\) setting. The slight decrease in PSNR/SSIM/LPIPS for IBRNet from \(I/I\) to \(I/SO(3)\) can be attributed to the training process involving multiple datasets with different canonical frames, which includes transformation augmentation and makes the model more robust to coordinate frame changes. Additionally, conventional metrics like PSNR/SSIM may not directly capture image variations. Therefore, we introduce an additional metric, pixel variance, to illustrate the changes better. We observe that IBRNet  exhibits pixel variance for different rotations, whereas our approach remains robust to rotation. Our method performs comparably with IBRNet in the \(I/SO(3)\) setting in DeepVoxels  because the synthetic data consists of Lambertian objects with simple geometry, where the ray directions do not significantly affect the radiance. For more qualitative results, see App. K.3.

## 5 Conclusion and Broader Impacts

To learn equivariant geometric priors from multiple views, we modeled the convolution on the light field as a generalized convolution on the homogeneous space of rays with \(SE(3)\) as the acting group. To obtain expressive point features, we extended convolution to equivariant attention over rays. The main limitation of the approach is the finite sampling of the light field. The sampling of the light field by sparse views cannot account for large object motions with drastic aspect change, leading to a breakdown of equivariance. This novel general equivariant representation framework for light fields can inspire further work on 3D vision and graphics tasks. We do not see any direct negative impact of our work, but it could have negative societal consequences if misused without authorization, for example, when using images violating privacy.

  Dataset & Method &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & Pix- Var\(\) \\  Realistic Synthetic & IBRNet & **26.91** & 0.928 & **0.084** & 26.77 & 0.923 & 0.091 & 66.58 \\ \(360^{}\) & Ours & 26.90 & **0.929** & 0.086 & **26.90** & **0.929** & **0.086** & **0.00** \\  Real & IBRNet & **25.13** & **0.817** & **0.205** & 24.60 & 0.797 & 0.223 & 52.66 \\ Forward-Facing  & Ours & 24.93 & 0.808 & 0.212 & **24.93** & **0.808** & **0.212** & **0.00** \\  Diffuse Synthetic & IBRNet  & **37.21** & **0.989** & **0.019** & 37.07 & **0.988** & **0.019** & 34.51 \\ \(360^{}\) & Ours & 37.11 & 0.987 & **0.019** & **37.11** & 0.987 & **0.019** & **0.00** \\  

Table 2: The results for the experiments of generalized rendering without per-scene tuning. The metrics in the cell are PSNR\(\), SSIM\(\), and Pixel Variance\(\) (denoted as Pix-Var). The evaluation of IBRNet is performed by testing the released model on both canonical and rotated test datasets.

Figure 9: Qualitative results for Generalized Rendering. We observe a performance drop for IBRNet from \(I\) to \(SO(3)\), while ours are robust to rotations in the testset.

Acknowledgement

The authors gratefully acknowledge support by the support by the following grants: NSF FRR 2220868, NSF IIS-RI 2212433, NSF TRIPODS 1934960, NSF CPS 2038873.