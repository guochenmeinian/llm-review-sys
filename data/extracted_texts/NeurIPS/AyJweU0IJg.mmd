# Mode Collapse in Variational Deep Gaussian Processes

Francisco Javier Saez-Maldonado

Universidad de Granada

fjaviersaezm@ugr.es

Juan Maronas

CUNEF Universidad

Universidad Autonoma de Madrid

juan.maronas@cunef.edu

Daniel Hernandez-Lobato

Universidad Autonoma de Madrid

daniel.hernandez@uam.es

###### Abstract

Gaussian Processes (gps) are flexible non-parametric models that have shown promising results across many applications, such as molecule optimization  or uncertainty estimation in dnns. They are used as prior distributions over some target function \(f\), _i.e._, \(f((),K_{}(,))\), where \(()\) and \(K_{}(,)\) are, respectively, the mean and covariance functions of the gp. Deep Gaussian Processes (dgps)  concatenate several gps across a layered network, defining a hierarchical structure that leads to a more flexible probabilistic model. This concatenation enlarges the class of functions that can be modeled, allowing to capture complex patterns within the data.

Bayesian inference in dgps is intractable. Thus, approximations using variational inducing points such as Double Stochastic Variational Inference (dsvi)  must be employed. To alleviate optimization difficulties, two main tools are often used: (i) a whitened representation of the process values at the inducing points  to enhance the numerical stability of the model , and (ii) an identity mean function in the inner layers of the model . In , it is claimed that the identity mean function is needed to avoid the pathologies outlined in . However, the reality is that, in practice, this mean function is used even in \(2\)-layer dgps, which are far from being similar models to those in .

Here, we show, via experiments in a toy dataset and a dataset from the UCI repository, that the need for the identity mean function in dgps is linked to the initial variational parameters and the usage of the whitened representation. Specifically, we found that a zero mean dgp, with the variational initialization used in , can lead to mode collapse during the optimization process, which is undesirable. Namely, the algorithm sets the variational mean and covariance to those of the gp prior (a standard Gaussian in the whitened case). Then, a huge noise variance is set in the observation model to explain the observed data, which is considered pure noise. This is not a desirable behavior.

Our main contributions are: 1) we explain why the whitened representation, beyond being designed to enhance mixing in MCMC algorithms , provides numerical stability in the Variational's Inference optimization process; 2) we highlight why the identity mean function in dgps avoids the mode collapse effect that occurs when the zero mean function is used; 3) we provide a theoretical explanation of the effect of whitening in the optimization process; and, 4) we propose a new initialization of the variational parameters of the zero mean dgp that alleviates the mode collapse problem.

## 2 Mode collapse in variational dgps

Consider the observed data \(=\{(_{i},y_{i})\;\;_{i}^{D},\; y_{i},\;i=1,,N\}\) which is grouped into a matrix \(=(_{1},,_{N})\) and a vector \(=(y_{1},,y_{N})^{}\). Consider a single gp. The process values at the training points are denoted by \(=(f(_{1}),,f(_{N}))^{}\). The set of \(M\)inducing locations \(=(_{1},,_{M})\), with \(_{i}^{D}\), have their corresponding inducing values \(=f()=(^{1},,^{M})^{}\). The posterior distribution is approximated using a variational distribution \(q(,)=p()q()\), with \(q()=(,)\) where \(\) and \(\) are the _variational parameters_. The prior over \(\) is \((_{},K_{})\). In the whitened representation, we rewrite \(=_{}+_{}\) where \(K_{}=_{}_{}^{}\) and \(p()=(,)\). Then, we make inference about \(\) instead of \(\) and learn \(q()=(_{},_{})\). Variational parameters are initialized as \(=_{}=0\), \(=_{}=10^{-5}\).

DGPs concatenate layers of GPs as observed in Fig. 1. Adding the superscript \(l\) to denote the \(l\)-th layer, the training objective for the dsvi algorithm, described in , is the elbo:

\[=_{n=1}^{N}_{q(^{L}_{n})}[ p(y_{n} ^{L}_{n})]-_{l=1}^{L}(q(^{l})  p(^{l};^{l}))\,. \]

Non-whitened GPs:In classic svgps, the mean of the marginal variational distribution \(q(^{l=1})\) is1:

\[_{qf}=K_{}K_{}^{-1}+ _{}-K_{}K_{}^{-1}_{ }\,. \]

The kl between prior and posterior is given by:

\[(q() p())=[ (|K_{}|||^{-1})-M+(K_{}^{-1}(+(-_{})(-_{ })^{T}))]. \]

Whitened GPs:In the whitened representation we have:

\[_{qf}^{w} =K_{}[L_{}^{}]^{ -1}_{}+_{}\,, \] \[(q() p()) =[-|_{}|-M+_{ }^{T}_{}+(_{}) ]\,. \]

KL at initialization and mode collapse:In the whitened case, at initialization, the marginal distribution's variational mean for a zero mean dgp is zero and the kl is very close to zero. Thus, the minimization of the objective will likely force \(_{}=0\) and \(_{}=\) (since it is the kl minimizer), and will minimize the ell, _i.e._, the data-dependent term in (1), by learning a huge observation noise. However, when using an identity mean function it is simpler to optimize the ell, since the posterior mean at initialization is \(_{i}\), the optimization forces \(_{}\) to move away from its initial value \(0\) to learn a map from \(_{i}\) to the outputs \(\). This often avoids mode collapse. For the non-whitened case, at initialization, the variational mean is also \(0\) in the zero mean dgp. However, we should expect the model not to be so prone to mode collapse, since at initialization the kl depends on \(K_{}\) via the prior, which can differ from the identity matrix (depending on the length-scale value). The contrary may also be true, _i.e._, one may also expect mode collapse, since now we can freely adapt both the non-whitened gp prior kernel hyper-parameters and \(\), and the variational parameters \(\) and \(\) to minimize kl. Therefore, in the non-whitened case, mode collapse depends not only on the initialization of \(q\), but on the initial kernel's hyper-parameter as well. In any case, this model usually suffers from optimization difficulties, so even though mode collapse can be controlled, the model is not useful in practice as it is complicated to optimize.

Optimization difficulties:There is a different impact on the kl divergence depending on whether the whitened representation is used or not. In the whitened representation one often observes that: 1) the kl at initialization is close to zero, providing a small learning signal to the objective; and

Figure 1: Example of a three layer dgp, with three units per layer.

[MISSING_PAGE_FAIL:3]

layer, and that we initialize \(^{{}^{}}=\). This is the typical setup considered in the literature for DGPs . The output of the inner layers of this model model at \(\), is \(\), _i.e._, the identity, as specified by the prior mean \(_{}\). See Eq. (6). However, since the identity mean function is not used in the last layer , the predictive mean at the inducing points \(\) will be zero, unlike in the proposed method, described in the previous paragraph. Something similar will happen in the case of the whitened representation, when \(_{}=\). See Eq. (4). That is, the initial DGP predictive mean, without training the model, will be zero for every input.

**Inducing points:** The initial inducing points may impact the initial solution. Selecting them at random from the training set may lead to regions of the input space being unrepresented, leading to a poor initial solution in those areas. See Fig. 2. To ensure a good initialization, we use a two-step algorithm:

1. We compute \(M\) centroids \(=\{_{1},,_{M}\}\) of \(_{}\) using _k-means_.
2. We select the inducing locations \(_{j}\) from \(_{}\) using the cosine distance: \[_{j}=_{_{j}_{}}d( _{j},_{j})=_{_{j}_{}}_{j}_{j}/(\|_{j}\|\|_{j}\|), _{j}.\] (8)

Fig. 2 compares the predictive distribution (without training) of the standard initialization of a dgp with that of the proposed initialization with a random selection of the inducing points among the training points, and when selecting the inducing locations as described above. We observe that the proposed initialization already explains quite well the observed data with no training of the objective.

**Optimization difficulties:** As mentioned, with the standard initialization, the kl in the whitened case provides a more stable learning signal (Sec. 2). In the proposed initialization, in the whitened case we have \(p()=(0,)\) and \(q()=(_{}^{-1},10^{ -5})\). In the non-whitened (reparameterized version) we have \(p()=(0,K_{})\) and \(q()=(,10^{-5})\). Therefore, the whitened parameterization is expected to lead to a more stable optimization process since, although both kl are different from \(0\) at initialization, the gradient of the kl for the non-whitened case depends on both the model parameters (_e.g._, length-scales and inducing points) and the variational parameters.

Figure 3: **Left and middle**: Ell and kl, respectively, during train of the \(5\) layer, zero mean function dgp in the _Yacht_ dataset. The mean and standard deviation across 20 splits are plotted. **Right**: Log likelihood results (right is better) of the \(L=2\) and \(L=5\) layer dgps in _Yacht_. The model with \(5\) layers and zero mean suffers from mode collapse, which is solved by the proposed initialization.

Figure 2: Initialization predictions of whitened 2 layer dgps. Yellow dots indicate the inducing locations \(\) and red dots indicate predictive mean \(_{}\) at the inducing locations.

## 4 Experiments

To validate the proposed initialization, we compare it with the standard initialization. We perform an extensive qualitative evaluation on the toy dataset presented in Fig. 2 and on the _Yatch_ dataset. The results show that the proposed initialization has two benefits: a) it leads to a faster convergence than the standard initialized DGP; and b) the proposed initialization avoids mode collapse when using a 5-layer DGP. Fig. 5 in Appendix B.1 supports point a). The next paragraph supports point b).

We perform an extensive analysis of 8 different UCI datasets. See Appendix B.2. The most interesting case is the _Yacht_ dataset analyzed below. The data-dependent term of the objective (ELL), as a function of the training epochs, is shown in Fig. 3 (left). The kl is also reported (center). We observe a clear example of the mode collapse in the \(5\)-layer dgp when using the zero mean function with the standard initialization. Specifically, the test log-likelihood results displayed in Fig. 3 (right) are significantly worse for the zero mean function with the standard initialization dgps. We observe the ell and the kl (left and center of the figure). In the standard initialization, when using the zero mean function, the kl quickly falls to zero when whitening is not used. Besides this, we also observe that the kl has a high variance when the whitened representation is used. This indicates that there are some data splits where mode collapse happens, even when using whitening. By contrast, the proposed initialization solves the mode collapse problem, preventing the kl from falling to zero by choosing a good initial solution. It also leads to an overall higher final kl. The ell chart (left) shows that, in the whitened case, our proposal exhibits a fast convergence and the final results pair up with the dgp with identity mean function. The non-whitened case has a slower convergence because the initial solution has a low ell, caused by the initial value of the kernel's length-scale4. We observe this behavior in other datasets. Appendix B.2 has a figure with all the results of ll and rmse.

### Kernel invertibility dependency

As a limitation of the proposed initialization, we remark that the correct inversion of the kernel evaluated at the inducing points highly influences the initial solution. The parameters of the kernel (in our case, the length-scale \(\) of the rbf kernel) must be properly selected so that the covariance matrix is not ill-conditioned, making the proposed initialization properly predict the corresponding inducing value at the selected inducing locations. An example of this can be observed in Fig. 4. However, this limitation can be surpassed by performing an initial evaluation of the model using different values of the kernel length-scale and selecting the one with higher ell or lower rmse.

## 5 Conclusions

DGPs may suffer from mode collapse (_i.e._, convergence to the prior) as the number of layers grows. Here, we have presented an initialization of svgps and dgps that uses the training data. This initialization fixes the inducing points and the variational parameters to predict an initial good solution of the targets. The results obtained, when using this initialization, show that it has benefits both in the convergence speed dgps and in avoiding the mode collapse problem. A limitation is that it requires solving a linear system that depends on the initial kernel parameters, which may be problem-dependent.

Figure 4: Predictive distribution of the proposed initialization when the length-scale varies. Proper kernel initialization is key for the success of this initialization.