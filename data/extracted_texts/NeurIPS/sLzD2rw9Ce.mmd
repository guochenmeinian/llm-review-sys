# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

## 1 Introduction

World models [17; 20; 33; 21] have gained increasing attention due to their ability to model complex real-world physical dynamics. They also hold potential as general-purpose simulators, capable of predicting future states in response to diverse action instructions. Facilitated by advancements in video generation techniques [53; 24; 3; 2], models like Sora have achieved remarkable success in producing high-quality videos, thereby opening up a new avenue that treats video generation as real-world dynamics modeling problem [47; 19; 56]. Generative world models, in particular, hold significant promise as real-world simulators and have garnered extensive research in the field of autonomous driving [28; 48; 30; 49; 54; 60; 13].

However, existing driving world models fall short of meeting the requirements of model-based planning in autonomous driving, which aims to improve driving safety in scenarios with diverse ego maneuvers and intricate interaction between the ego vehicle and other road users. These models perform well for non-interactive in-lane maneuvers but have shown limited capability in following more challenging action instructions like lane change. One significant roadblock to building next-generation driving world models lies in the datasets. Autonomous driving datasets commonly used in current world model literature like nuScenes , Waymo , and ONCE , are primarily designed and curated in a perception-oriented manner. As a result, it contains limited driving patterns and multi-agent interactions, which may not fully capture the complexities of real-world driving scenarios. The scarcity of interaction data limits the ability of models to accurately simulate and predict the complex dynamics of real-world driving environments.

In this paper, we propose **DrivingDojo**, a large-scale driving video dataset designed to simulate real-world visual interaction. As illustrated in Figure 1, DrivingDojo features action completeness, multi-agent interplay, and open-world driving knowledge. Our dataset aims to unleash the full potential of world models in action instruction following by including rich longitudinal maneuvers like acceleration, emergency braking and stop-and-go as well as lateral ones like U-turn, overtaking, and lane change. Besides, we explicitly curate the dataset to include a large volume of trajectories

Figure 2: **Enhancing interactive and knowledge-enriched learning of world models. Data plays a crucial role in modeling the world. DrivingDojo is a large-scale video dataset curated from millions of daily collected videos, designed to investigate real-world visual interactions. DrivingDojo features comprehensive actions, multi-agent interplay, and rich open-world driving knowledge, serving as a superb platform for studying driving world models.**containing multi-agent interplays like cut-in, cut-off, and head-to-head merging. Finally, DrivingDojo taps into the open-world driving knowledge by including videos containing rare events sampled from tens of millions of driving video clips, including crossing animals, falling bottles and debris. As shown in Figure 2, we hope that DrivingDojo could serve as a solid stepping stone for developing next-generation driving world models.

To measure the progress of driving scene modeling, we propose a new action instruction following (AIF) benchmark to assess the ability of world models to perform plausible future rollouts. The AIF benchmark measures the visual and structural fidelity of videos generated by world models in an action-conditioned manner. We propose the AIF errors calculated on the withheld validation data to evaluate the long-term motion controllability for generated videos. The error is defined as the mean error between the actions estimated from the generated video and the given action instructions. Then the baseline world model is evaluated on our DrivingDojo AIF benchmark, for in-domain data and out-of-domain images or action conditions.

Our major contributions are as follows. (1) We design a large-scale driving video dataset to facilitate research in world model for autonomous driving. Compared to previous datasets in Table 1, our dataset features complete driving actions, diverse multi-agent interplay, and rich open-world driving knowledge. (2) We design an action instruction following task for driving world model and provide corresponding video world model baseline methods. (3) Benchmark results on both driving video generation and action instruction following show that there are plenty of new opportunities for future driving world model development on our new dataset.

## 2 Related Works

### Autonomous Driving Datasets

Datasets for perception.The driving dataset has played a crucial role in advancing computer vision in recent years, aiming to achieve comprehensive perception and understanding surrounding the ego vehicle. Initially, perception in autonomous driving relied on 2D image-based perception. Datasets like Cityscapes , Mapillary Vistas , and BDD100k  provided instance-level masks for learning tasks. With the integration of LiDAR sensors and advancements in 3D perception, datasets like KITTI , nuScenes , and Waymo  have emerged as standard benchmarks for various 3D perception tasks. Additionally, datasets like ONCE , Argoverse [8; 50], and others [29; 15; 1] are also utilized for studying various perception tasks.

Datasets for prediction and planning.In recent years, there's been increasing attention on prediction and planning in autonomous driving. Prediction involves anticipating the behavior of other agents, while planning relates to the behavior of the ego vehicle. Prediction methods typically rely on semantic maps and dynamic traffic light statuses to anticipate future vehicle motions. Notable datasets in this area include Argoverse Motion Forecasting , Waymo Open Motion Dataset , Lyft Level 5 Prediction Dataset , and nuScenes Prediction  challenge. Additionally, the Interaction dataset  provides interactive driving scenarios with semantic maps derived from drones and traffic cameras, enriching the understanding of complex driving interactions. Transitioning to planning, CARLA  stands out as an open-source simulator designed to simulate real-world traffic scenarios, providing a platform for testing and validating planning algorithms. Complementing this, nuPlan  introduces the first closed-loop planning benchmark for autonomous vehicles, closely mirroring real-world scenarios.

   Dataset & Videos & Duration (hours) & Ego & Complete & Multi-agent & Open-world \\  & & (hours) & Trajectory & Actions & Interplay & Knowledge \\  nuScenes  & 1k & 5.5 & ✓ & & & \\ Waymo  & 1k & 11 & ✓ & & & \\ OpenDV-2k  & 2k & 2059 & & ✓ & & \\ nuPlan  & - & 1500 & ✓ & ✓ & ✓ & \\  DrivingDojo (Ours) & 18k & 150\({}^{*}\) & ✓ & ✓ & ✓ & ✓ \\   

Table 1: **A comparison of driving datasets for world model**. This comparison emphasizes the diversity of the video content, placing less focus on annotations or sensor data. \({}^{*}\) denotes that the videos are curated from our data pool of around 7500 hours.

### World Model

Learning world models.World models [17; 33] enable next-frame prediction based on action inputs, aiming to build general simulators of the physical world. However, learning dynamic modeling in pixel space is challenging, leading previous image-based world models to focus on simplistic gaming environments or simulations [18; 20; 9; 52; 44; 43; 21]. With advances in video generation, models like Sora can now produce high-definition videos up to one minute long with natural, coherent dynamics. This progress has encouraged researchers to explore world models in real-world scenarios. DayDreamer  applies the Dreamer algorithm to four robots, allowing them to learn online and directly in the real world without simulators, demonstrating that world models can facilitate faster learning on physical robots. Genie  demonstrates interactive generation capabilities using vast internet gaming videos and shows potential for robotics applications. UniSim  aims to create a universal simulator for real-world interactions using generative modeling, with applications extending to real-robot executions.

World model for autonomous driving.World models serving as real-world simulators have garnered widespread attention [16; 61] and can be categorized into two main branches. The first branch explores agent policies in virtual simulators. MILE  employed imitation learning to jointly learn the dynamics model and driving behavior in CARLA . Think2Drive  proposed a model-based RL method in CARLA v2, using a world model to learn environment transitions and acting as a neural simulator to train the planner. The second branch focuses on simulating and generating real-world driving scenarios. GAIA-1  introduced a generative world model for autonomous driving, capable of simulating realistic driving videos from inputs like images, texts, and actions. DriveDreamer  emphasized scenario generation, leveraging HD maps and 3D boxes to enhance video quality. Drive-WM  was the first to propose a multiview world model for generating high-quality, controllable multiview videos, exploring applications in end-to-end planning. ADriver-I  constructed a general world model based on MLLM and diffusion models, using vision-action pairs to auto-regressively predict current frame control signals. DriveDreamer2  leveraged LLMs and text prompts to generate diverse driving videos in a user-friendly manner. Unlike previous methods that focused on model design, OpenDV-2K  addressed the issue of training data by collecting over 2000 hours of driving videos from the internet. Previous research has predominantly addressed static scene generation, with limited emphasis on multi-agent interplays. Our dataset enables the exploration of world model predictions within dynamic, interactive driving scenarios.

## 3 The DrivingDojo Dataset

Our goal is to provide a large and diverse action-instructed driving video dataset DrivingDojo to support the development of driving world models. To accomplish this, we extract highly informative clips from a video pool collected through fleet data, spanning several years and comprising more than 500 operating vehicles across multiple major Chinese cities. As a result, our DrivingDojo features diverse ego actions, rich interactions with road users, and rare driving knowledge which are crucial for high-quality future forecasting as shown in Table 2.

Figure 3: **The strengths of the DrivingDojo dataset. (a) illustrates a comparison of action distributions among nuScenes, ONCE, and our DrivingDojo. We compare the average hourly event counts of driving actions. (b) presents the distribution of text descriptions for the video clips in DrivingDojo.**We begin with the design principles of DrivingDojo and its uniqueness compared with existing datasets in Section 3.1- 3.3. We then describe the data curation procedure and statistics in Section 3.4. Here, we only describe the design principles. More detailed information refer to the Appendix.

### Action Completeness

Using the driving world model as a real-world simulator requires it to follow action prompts accurately. Existing autonomous driving datasets, such as ONCE  and nuScenes , are generally curated for developing perception algorithms and thus lack diverse driving maneuvering.

To enable the world model to generate an infinite number of high-fidelity, action-controllable virtual driving environments, we create a subset called DrivingDojo-Action that features a balanced distribution of driving maneuvers. This subset includes a diverse range of both longitudinal maneuvers, such as acceleration, deceleration, emergency braking, and stop-and-go driving, as well as lateral maneuvers, including lane-changing and lane-keeping. As demonstrated in Figure 2(a), our DrivingDojo-Action subset offers a significantly more balanced and complete set of ego actions compared to existing autonomous driving datasets.

### Multi-agent Interplay

Besides navigating in a static road network environment, modeling the dynamics of multi-agent interplay like merge and yield is also a crucial task for world models. However, current datasets are either built without considering multi-agent interplays, such as nuScenes  and Waymo , or are constructed from large-scale internet videos that lack proper curation and balancing, like OpenDV-2K .

To address this issue, we design the DrivingDojo-Interplay subset focusing on interactions with dynamic agents as a core component of the dataset. As shown in Figure 0(b), we curate this subset to include at least one of the following driving scenarios: cutting in/off, meeting, blocked, overtaking, and being overtaken. These scenarios encompass a variety of realistic situations, such as vehicles cutting into lanes, encounters with oncoming traffic, and the necessity for emergency braking. By incorporating these diverse scenarios, our dataset enables world models to better understand and anticipate complex interactions with dynamic agents, thereby improving their performance in real-world driving conditions.

### Rich Open-world Knowledge

In contrast to perception and prediction models, which compress high-dimensional sensor input into low-dimensional vector representations, world models exhibit a superior modeling capacity by operating in the pixel space. This increased capacity enables world models to effectively capture the intricate dynamics of open-world driving scenarios, such as animals unexpectedly crossing the road or parcels falling off the trunks of vehicles.

However, existing datasets, either perception-oriented ONCE  or planning-oriented ones like nuPlan , do not have adequate data for developing and assessing the long-tail knowledge modeling ability of world models. Therefore, we place a unique emphasis on including rich open-world knowledge video clips and construct the DrivingDojo-Open subset. As shown in Figure 0(c), describing open-world driving knowledge like this is challenging due to its complexity and variability, but these scenarios are crucial for ensuring safe driving.

  
**Dataset** & **Videos** & **Type** & **Camera** & **Ego Trajectory** & **Text Description** \\  DrivingDojo & 17.8k & total & ✓ & ✓ & ✓ \\  DrivingDojo-Action & 7.9k & rich ego-actions & ✓ & ✓ & \\ DrivingDojo-Interplay & 6.2k & multi-agent interplay & ✓ & ✓ & \\ DrivingDojo-Open & 3.7k & open-world knowledge & ✓ & ✓ & ✓ \\   

Table 2: **DrivingDojo dataset constitution.** The dataset is organized into three subsets: DrivingDojo-Action, DrivingDojo-Interplay, and DrivingDojo-Open, to support research on specific tasks.

The DrivingDojo-Open subset consists of 3.7k video clips about the open-world knowledge in driving scenarios. This subset is curated from fleet data that includes unusual weather, foreign objects on the road surface, floating obstacles, falling objects, taking over cases, and interactions with traffic lights and boom barriers. A word cloud of video descriptions for DrivingDojo-Open are shown in Figure 2(b). DrivingDojo-Open serves as an invaluable supplementary for driving world modeling by including driving knowledge beyond simply interacting with structured road networks and other regular road users.

### Data Curation and Statistics

Dataset statistics.The DrivingDojo dataset contains around 18k videos with resolution of 1920\(\)1080 and frame rate at 5 fps. Our video clips are collected from major Chinese cities including Beijing, Shenzhen, Xuzhou, etc., as shown in Figure 4. Furthermore, these videos are recorded in diverse weather conditions at different daylight conditions. All videos are paired with synced camera poses derived from the HD-Map powered high precision localization stack onboard. Videos in the DrivingDojo-Open subset are paired with text descriptions about the rare event happening in each video. More details are in the Appendix.

**Data collection.** We collected multi-modal fleet data using the platform of Meituan's autonomous delivery vehicles. Our dataset consists of video clips recorded by the front-view camera with a horizontal field of view of 120\({}^{}\) to capture comprehensive visual information. The raw data is collected from multiple Chinese cities between May 2022 and May 2024, amassing a total of 900,000 videos and approximately 7,500 hours of driving footage pre-filtered before recording.

**Data curation.** In order to ensure both the data diversity as well as balanced ego action and multi-agent interplay distribution, we include fleet data with different criteria. The data sources of DrivingDojo include 1) intervention data from safety inspectors during vehicle operation, 2) emergency brake data from automatic emergency braking, 3) randomly sampled 30-second general videos from collected videos, 4) selected distinct scenarios such as traffic light changes, barrier opening, left and right turns, straight crossings, vehicle encounters, lane changes, and pedestrian interactions, 5) manually sorted rare data containing moving and static foreign objects on the road, floating obstacles, falling and rolling objects. The curation details are in the Appendix.

**Personal Identification Information (PII) removal.** To avoid privacy infringement and obey the regulation laws, we employ a high precision license plate and face detectors  to detect and blur these PII for each frame of all videos. An in-house annotation team and the authors have manually double-checked that the PII removal procedure is correctly carried out for all the videos.

## 4 DrivingDojo for World Model

To facilitate the study of world models in autonomous driving, we define a novel action instruction following (AIF) task. We provide baseline methods (Section 4.2) and evaluation metrics (Section 4.3), enabling further investigations. More details are described in the Appendix.

Figure 4: **Descriptive statistics of the DrivingDojo dataset. The dataset was collected from various regions across China, including nighttime and rainy/snowy conditions.**

### Action Instruction Following

Action-controllable video forecasting is the core ability of world models . Instead of solely focusing on predicting high-quality video frames, action instruction following requires world models to take both the initial video frame and ego action prompts into consideration for predicting corresponding world responses. Given the initial image \(I_{t}\) and a sequence of actions \(\{A_{t},...,A_{t+k}\}\), the model \(f_{}\) predicts future states \(\{I_{t+1},...,I_{t+k}\}\) as:

\[\{I_{t},...,I_{t+k}\}=f_{}(I_{t},\{A_{t},...,A_{t+k}\}).\] (1)

Here, \(\{A_{t},...,A_{t+k}\}\) refers to the action prompts for each frame, with trajectories \(A_{t}=( x_{t}, y_{t})\) in our experiment. \(f_{}\) represents the world model, and \(\{I_{t+1},...,I_{t+k}\}\) signifies the visual prediction for subsequent \(k\) frames.

### Model Architecture

We propose DrivingDojo baseline, a video generation model based on Stable Video Diffusion (SVD) . While SVD is a latent diffusion model for image-to-video generation, we extend its capability to generate videos conditioned on action. For the AIF task, we encode the value of each action sequence into a 1024-dimensional vector using a Multilayer Perceptron (MLP). Subsequently, the action feature is concatenated with the first-frame image feature and passed into the U-Net .

### Evaluation Metrics

**Visual quality.** To evaluate the quality of the generated video, we utilize FID (Frechet Inception Distance)  and FVD (Frechet Video Distance)  as the main metrics.

**Action instruction following.** We propose the action instruction following (AIF) errors \(E_{x}^{}\) and \(E_{y}^{}\) to measure the consistency between the generated video and the input action conditions. Given the generated video sequences \(\{I_{t},...,I_{t+k}\}\), we estimate vehicle trajectories in the generated videos with the offline visual structure-from-motion (SfM) implementation like COLMAP [41; 42]: \(\{_{t},...,_{t+k}\}=(\{I_{t},...,I_{t+k}\})\), where \(\{_{t},...,_{t+k}\}\) are estimated trajectories of unknown scale. We estimated the scale factor \(\) for the predicted trajectory by minimizing the error between estimated and input ego-motion in the first \(N\) frames. We compare the estimated actions with the ground-truth action instructions \(\{A_{t},...,A_{t+k}\}\) and report the mean absolute error for both lateral (\(E_{y}^{}\)) and longitudinal (\(E_{x}^{}\)) actions:

\[(E_{x}^{},E_{y}^{})=^{k}|A_{t+i}- _{t+i}*|}{k+1},\] (2)

where the scale factor \(=_{i=0}^{N}|A_{t+i}-_{t+ i}*S|\).

## 5 Experiments

### Results of Visual Prediction

To illustrate the richness of behaviors and dynamics within our dataset, we compare video fine-tuning quality across various datasets. In Table 3, we random selected 256 video segments from the OpenDV

  
**Method** & **Fine-tuning** & **Evaluation** & **FID** & **FVD** \\  SVD & OpenDV-2K & OpenDV-2K & 18.27 & 321.05 \\  SVD & - & OpenDV-2K & 24.17 & 580.94 \\ SVD & nuScenes\(\) & OpenDV-2K & 21.05 & 395.04 \\ SVD & DrivingDojo & OpenDV-2K & **19.20** & **343.91** \\   

Table 3: **Comparison of visual prediction fine-tuning across different datasets., \(\) indicates using camera sweeps data. The performance is zero-shot evaluated on the OpenDV-2K dataset.**2K dataset  as our test set and evaluated fine-tuning performance of SVD  model across various datasets. The results indicate that models trained on our dataset exhibit better visual quality.

### Results of Action Instruction Following

**Diverse driving behaviors.** Based on different sequences of actions, our model is able to generate multiple possible futures. As shown in Figure 5, we showcase the model's capability to execute forward, left turn, and right turn maneuvers at intersections, as well as lane-changing to the left or right, and maintaining on straight roads.

**Action instruction following.** Although qualitative evaluations demonstrate the powerful generative ability of our model, we also endeavor to measure the accuracy of action instruction following quantitatively. We seek to evaluate whether the video trajectories generated by the model closely adhere to our expected route paths. This serves as a fundamental assurance for the future application of world model. As shown in Table 4, with the in-domain actions (original action sequences of the test video) as conditions, videos generated by the baseline world model trained on DrivingDojo exhibit strong loyalty towards the action instructions. The mean action error in each video frame is limited to only 10 cm in the lateral or longitudinal directions. In row 3, feeding the model with the same initial images and randomly sampled action instructions slightly increases the mean action errors. When the model is applied zero-shot to initial images from OpenDV-2K  and fed with randomly sampled action instructions, its generated videos still demonstrate considerable consistency to the action instructions. Note that the proposed action instruction following errors can sensitively reflect the impact of out-of-domain inputs on the performance of the model.

   Action Type & Test Dataset & FID & FVD & \(E_{x}^{}()\) & \(E_{y}^{}()\) \\   In-Domain & DrivingDojo(GT) & - & - & 0.036m & 0.019m \\  In-Domain & DrivingDojo & 37.07 & 658.72 & 0.100m & 0.062m \\ Out-of-Domain & DrivingDojo & 38.30 & 716.44 & 0.173m & 0.110m \\  Out-of-Domain & OpenDV-2K\({}^{*}\) & 24.27 & 442.67 & 0.238m & 0.136m \\   

Table 4: **Action instruction following on the DrivingDojo dataset.** GT refers to using real images to test the accuracy of the reconstructed trajectory. \({}^{*}\) denotes the model is applied zero-shot to this dataset without fine-tuning.

Figure 5: **Predicting multiple futures based on different actions.** Left: going straight, turning left, and turning right at a crossing; Right: changing to the left lane, staying in the current lane, and changing to the right lane.

   Training set & Test set & FID & FVD & \(E_{x}^{}()\) & \(E_{y}^{}()\) \\  DrivingDojo & OpenDV-2K\({}^{*}\) & **24.27** & **442.67** & **0.238m** & **0.136m** \\ ONCE & OpenDV-2K\({}^{*}\) & 28.37 & 473.59 & 0.255m & 0.23d9m \\ nuScenes & OpenDV-2K\({}^{*}\) & 37.90 & 794.36 & 0.387m & 0.254m \\   

Table 5: **Action instruction following under zero-shot evaluation.**\({}^{*}\) denotes the model is applied zero-shot to this dataset without fine-tuning.

Zero-shot evaluation.As shown in Table 5, we compared the performance of models trained on different datasets and their zero-shot generalization performance on new datasets. The results indicate that models trained on our dataset exhibit higher generation quality and significantly improved action-following ability. Especially, we noticed that richer driving actions in the autonomous driving datasets lead to significantly better AIF performance of models trained on them. According to Figure 2(a), videos in DrivingDojo averagely contain far richer driving actions compared to ONCE or nuScenes. This leads to the far better AIF performance of model trained on DrivingDojo compared to those trained on ONCE or nuScenes. we observed that the model trained on the ONCE dataset will always generate videos in which the vehicle moves in a straight line, even with action instructions to turn left/right or change lanes. This leads to its especially poor AIF performance in the lateral direction (\(E_{y}^{}\)). We speculate that this is because the driving action of making turns or changing lanes is very rare in the ONCE dataset, as shown in Figure 2(a), which results in the lack of ability of the model trained on the ONCE dataset to follow the lateral motion instructions. Moreover, the even more lacking driving actions in the nuScenes dataset lead to a worse AIF performance of the world model.

AIF visualization.We showcase examples of estimated trajectories from generated videos in Figure 6. In each frame, the red dot represents the current estimated camera pose and the black dots represent the camera poses in past frames.

### Real-world Simulation

**Action generalization.** Our model demonstrates robust generalization capabilities in two key aspects. As illustrated in Figure 6(a), firstly, it effectively generalizes to out-of-domain (OOD) actions, such as forcefully driving on pedestrian walkways, showcasing its adaptability to some unreasonable actions. Secondly, it successfully extends its capabilities to other datasets, executing tasks such as lane changes on the OpenDV-2K  dataset and backing-the-car maneuvers on the nuScenes  dataset without requiring further fine-tuning. This underscores the model's potential as a real-world simulator, capable of adapting to diverse driving scenarios.

Figure 6: **Examples of ego trajectories estimated based on the generated videos.**

Figure 7: **Qualitative examples of our model’s capability.**

**Dynamic agents.** We showcase our model's ability to simulate interactions with dynamic agents in Figure (b)b. The results indicate that the model can provide reasonable responses based on our actions. The first scenario depicts a pedestrian opting to yield as our vehicle continues forward, resulting in a change in trajectory. In the second scenario, a delivery person opts to stop and wait at a narrow road.

**Open-world dynamics.** In Figure (b)b, our model showcases the simulations of rare scenarios encountered on the road, including interactions with moving birds and parking lot barriers.

### Limitations and Future Work

This dataset currently comprises only single-camera videos. Our primary focus is to maximize video diversity, which has led us to reduce the number of sensors used, enabling us to capture a wider range of scenes. Additionally, this paper primarily explores the value of the dataset, treating the model aspect as a baseline without any specialized design. Although the DrivingDojo dataset significantly improves model capabilities, there are still several limitations that require further investigation in future studies.

**Hallucination.** As shown in Figure 8, we observed that the model exhibits some hallucinations, such as the sudden disappearance of objects, and when an action is unrealistic given the scene, such as forcefully turning right, the model sometimes imagines a new road.

**Long-horizon visual prediction.** Our baseline model is only capable of generating short videos, which can be used to simulate short-term interaction events. Longer predictions [4; 57; 22] and faster generation [38; 36] are left for future research.

**Driving policy.** The long-tail cases in our dataset are valuable for driving policy research. While this work focuses on visual prediction in world models, future studies can investigate how this data improves driving policy.

## 6 Conclusion

In this work, we present DrivingDojo, a large-scale video dataset aimed at advancing the study of driving world models. DrivingDojo offers a testbed for studying diverse real-world interactions. Our findings indicate that simulating interactions and rare dynamics observed in open-world environments remains an unsolved challenge, highlighting significant opportunities for future research.

Societal impacts.By providing a comprehensive dataset covering diverse driving scenarios and behaviors, researchers can develop and refine algorithms that increase the safety, reliability, and efficiency of autonomous vehicles. However, the development of driving world model requires large and diverse driving videos, introducing privacy issues.