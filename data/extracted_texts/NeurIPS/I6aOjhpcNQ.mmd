# Robust Concept Erasure via Kernelized

Rate-Distortion Maximization

 Sommath Basu Roy Chowdhury

UNC Chapel Hill

&Nicholas Monath

Google DeepMind

Avinava Dubey

Google Research

&Amr Ahmed

Google Research

&Sniegha Chaturvedi

UNC Chapel Hill

{sommath, snigdha}@cs.unc.edu

{nmonath, avinavadubey, amra}@google.com

###### Abstract

Distributed representations provide a vector space that captures meaningful relationships between data instances. The distributed nature of these representations, however, entangles together multiple attributes or _concepts_ of data instances (e.g., the topic or sentiment of a text, characteristics of the author (age, gender, etc), etc). Recent work has proposed the task of _concept erasure_, in which rather than making a concept predictable, the goal is to remove an attribute from distributed representations while retaining other information from the original representation space as much as possible. In this paper, we propose a new distance metric learning-based objective, the **K**erlized **R**ate-Distortion **M**aximizer (KRaM), for performing concept erasure. KRaM fits a transformation of representations to match a specified distance measure (defined by a labeled concept to erase) using a modified rate-distortion function. Specifically, KRaM's objective function aims to make instances with similar concept labels dissimilar in the learned representation space while retaining other information. We find that optimizing KRaM effectively erases various types of concepts--categorical, continuous, and vector-valued variables--from data representations across diverse domains. We also provide a theoretical analysis of several properties of KRaM's objective. To assess the quality of the learned representations, we propose an alignment score to evaluate their similarity with the original representation space. Additionally, we conduct experiments to showcase KRaM's efficacy in various settings, from erasing binary gender variables in word embeddings to vector-valued variables in GPT-3 representations.

## 1 Introduction

Learned representations, particularly distributed representations , are at the core of machine learning with applications in natural language , images , biology , physics , and several other domains . These vector-based representations of data instances create an inner product space where similarities and nearest-neighbor relationships are "meaningful". However, due to the distributed nature of these representations, the definition of "meaningful" is often not easily discernible. In other words, shared properties of data instances nearby in the vector space are not always evident. These shared properties are often referred to as _concepts_. For instance, concepts in representations of images include objects in the image, whether it is indoor or outdoor, etc. Similarly, concepts in text representations include the topics, and characteristics of the author (e.g., geographic location, gender, etc.). For applications that necessitate conditioning on specific attributes to make or explain predictions, these distributed representations can pose challenges. Consequently,a significant body of work has focused on jointly learning representations and disentangling their underlying concepts [28; 29].

However, state-of-the-art representations for tasks across various domains often come from pre-trained models (e.g., ViTs  for images, GPT  for text, among others). In many cases, these pre-trained representations are utilized directly without fine-tuning the original model, due to factors such as computational burden or limited model API access . This presents a challenge for fitting disentangled representations of data instances and their concepts since the model parameters are frozen. Rather, it presents an opportunity for learning a transformation (or similarly learning a distance metric) for the pre-trained representations.

While learning such a transformation of pre-trained representations can be used for many applications (e.g., classification, regression, etc.) that involve disentangling specific concepts from representations, we focus on the recently proposed task of _concept erasure_. The objective of concept erasure is twofold: (1) to learn representations that minimize the classification accuracy (or mean squared error) for a specific concept variable and (2) to retain as much other information from the original representations as possible. This becomes possible when the representations are not correlated with the concept variable. We should note that there are indeed some trivial methods to reduce the correlation - such as generating random representations or making all representations identical. However, these solutions fail to retain any information from the original representation space. This highlights one of the key challenges in concept erasure - retaining information from the original space while removing a given concept. This challenge is accentuated by the lack of a _pre-specified_ downstream task for which the representations could be optimized. Instead, objectives for concept erasure use no supervision (apart from the labels of the concept to erase). This makes it difficult to use adversarial learning  or mutual information estimation [17; 43] methods for concept erasure.

The independence of concept erasure from down-stream tasks makes it amenable to a variety of applications that use the modified representations. For example, when developing a toxicity classifier for online comments, an organization may seek to ensure that content from different religious backgrounds is treated equitably. This can be achieved using concept erasure to remove information about religion from the text representations . Concept erasure has also shown promise in interpreting the decision-making of large models by studying counterfactual scenarios where certain properties of the input are erased from intermediate layers .

In this paper, we present a new distance metric learning-based objective for concept erasure. We refer to our objective, **K**ernalized **R**ate-Distortion **M**aximizer (**K**RaM**). The objective fits a transformation of representations to match a specified distance measure using a kernelized rate-distortion function, where the kernel is constructed using concept labels. Specifically, KRaM's objective function tries to make instances with similar concept labels dissimilar in the learned representation space (Figure 1). Empirically, we find that optimizing KRaM results in representations that are uncorrelated with the concept variable, effectively leading to its erasure. To evaluate the quality of the representations, we propose a \(k\)-nearest neighbour based measure to capture the alignment of the learned representations with the original representation space. We conduct extensive experiments to demonstrate that KRaM is capable of erasing various types of concepts--categorical, continuous, and vector-valued variables--from data representations across a wide range of domains. We also theoretically analyze several properties of the proposed objective function. Our primary contributions are:

* KRaM, which uses a kernelized formulation of the rate-distortion function that is able to delete a range of concepts
- categorical, continuous, or vector-valued variables from representations (Section 3).
* We propose a computationally efficient alignment measure, to evaluate how informative the learned representations are about the original representation space (Section 4).
* We perform a theoretical analysis of KRaM's objective and alignment measure. We conduct extensive experiments to showcase the efficacy of KRaM in a range of settings, from erasing binary gender variables in word embeddings to vector-valued variables in GPT-3 representations (Section 5).

## 2 Preliminaries & Background

In this section, we first formally describe the concept erasure setup, then discuss some prior concept erasure techniques, and finally introduce the fundamentals of rate-distortion theory.

**Problem Setup**. In _concept erasure_, we consider the input representations \(x\) and the concept \(a\) as random variables. We assume access to samples \([(x_{1},a_{1}),(x_{2},a_{2}),]\) drawn from the joint distribution \(P(,)\). The goal of concept erasure is to learn a function \(f()\) that generates representations \([f(x_{1}),f(x_{2}),]\), such that it is infeasible to predict the concept labels \(a\) from \(\). In addition to erasing the concept variable, \(f(x)\) should retain as much information about \(x\) as possible. We do not impose any constraints on the nature of the concept. It can be: categorical (\(a[k]\)), continuous (\(a\)), or vector-valued (\(a^{d}\)) random variable.

Concept Erasure is also closely related to learning invariant representations with respect to an attribute through adversarial learning . However, concept erasure differs from adversarial learning in two key aspects: (a) the input representations \(x\) remain frozen during the concept erasure process (only erasure function \(f\) is updated), and (b) it does not rely on a specific downstream task. This setup is beneficial in situations where we can access representations but lack the necessary resources or infrastructure to train or fine-tune the model that generated them. In the following section, we describe the details of the proposed concept erasure framework, KRaM.

**Prior Work**. Concept erasure was initially introduced by  in the context of removing binary gender labels from GloVe embeddings . Initial works on this problem  performed concept erasure by projecting representations onto the null space of the optimal separating linear subspace for the categorical concept. Recent work  has introduced a generalized objective for this solution, presenting it as a minimax game between concept identification and nullspace projection, and further provided a closed-form solution for its relaxed convex version. Nonetheless, these techniques are limited by two main assumptions: (a) the erasure function \(f()\) is linear, and (b) the concept variable is categorical. A linear erasure function ensures that a linear subspace, which could identify the concept label for instances, does not exist in the learned representation space, \(\). Consequently, it prevents any linear network from extracting the concept labels from the learned representations. However, it can still be possible for a non-linear network to predict the concept labels by identifying a non-linear concept subspace. Given that most modern ML architectures rely on non-linear networks, it is crucial to ensure that the concept is inaccessible to non-linear networks. More recent works have made progress towards non-linear concept erasure. This has been done either by using a linear concept erasure after projecting the input into a non-linear feature space , or directly utilizing a non-linear erasure function \(f\) using a rate-distortion objective . Despite their potential, these concept erasure techniques require access to categorical concept labels for all instances. Hence, it is not possible to erase other forms of concept variables (continuous or vector-valued) using these techniques without discretization of the concept labels, which often leads to information loss. In contrast to these techniques, our erasure framework KRaM is able to handle a variety of concept variables (categorical, continuous, or vector-valued) while performing non-linear concept erasure. This becomes possible as KRaM presumes access to a kernel matrix that is defined by the concept labels, and does not impose any additional constraints on the nature of the concept variable. Next, we discuss the fundamentals of the rate-distortion function that forms a building block of our framework.

**Rate Distortion**. In information theory , the compactness of a distribution is measured by their _coding length_ - the number of binary bits required to encode it. In lossy data compression, a set of vectors \(=\{z_{1},,z_{n}\}^{n d}\), sampled from a distribution \(P()\), is encoded using a coding scheme, such that the transmitted vectors \(\{_{i}\}_{i=1}^{n}\) can be recovered up to a distortion \(\). The minimal number of bits required per vector to encode the sequence \(\) is defined by the _rate-distortion_ function \(R(,)\). The optimal \(R(,)\) for vectors \(\) sampled from a multivariate Gaussian \((0,)\) is:

\[R(,)=_{2}(I+ }^{T}),\] (1)

where \(n\) is the number of vectors and \(d\) is the dimension of individual vectors. Equation 1 provides a tight bound even in cases where the underlying distribution \(P()\) is degenerate . The rate-distortion function is also closely tied to the sphere packing problem  and represents the volume (or intrinsic dimension) of a representation set. Recent works like MCR\({}^{2}\) have built on the rate-distortion function to learn discriminative representations for classification tasks. Concept erasure techniques , have also used the rate-distortion function to erase categorical variables. Even though KRaM uses the rate-distortion function similar to these techniques, it is more versatile and capable of erasing different types of concept variables. KRaM also imposes additional constraints on the feature space to ensure robust concept erasure, which we discuss in the following section.

## 3 Kernelized Rate-Distortion Maximizer (KRaM)

We proceed by discussing how a concept variable can be erased from a representation set. A concept cannot be extracted by a predictive network (which is equivalent to erasure) if there is minimal correlation between the representation set, \(\), and the concepts, \(\). Note that distances in the representation space can be indicative of the concept variable. For example, in Figure 1 (a) we observe that instances in the original representation space with similar concept labels (shown by their color \(\) and \(\)) appear close to each other. Here, the distance between instances is correlated with the concept label, thereby making it feasible to identify the concept labels via a linear or non-linear boundary. Ideally, we want a representation space where distances are not reflective of concept labels, where instances with different concept labels appear together (e.g., Figure 1 (c) & (d)).

The intuition behind our approach, KRaM, is to make the distances in the learned representation space, \(\), uncorrelated with the concept variable, \(\). Specifically, we try to make instance pairs similar in the concept space to be distant (or dissimilar) in the learned representation space, \(\). We achieve this by learning an erasure function \(f\) (parameterized by a neural network) to transform the input \(x\) into \(\). We propose a kernelized formulation of the rate-distortion function to train \(f\):

\[R(|)=_{2}(I+}^{T}),\] (2)

where \(=f()^{n d}\) and the kernel matrix, \(^{n n}\), captures similarities between concept labels. The entries of the kernel matrix are inversely proportional to the distance between concept labels \(_{ij} 1/(a_{i},a_{j})\), where \((,)\) can be an arbitrary symmetric distance function, such that \((x,x)=0\). We observe that \(R(|)\) is sensitive to the scale of the representations \(f(x)\). Therefore, we fix the Frobenius norm of the representations using a layer normalization layer  to make \(f(x)^{d}\), ensuring that individual instances have an equal impact on the loss.

Next, we discuss how maximizing \(R(|)\) (Equation 2) helps in concept erasure. We proceed by noting that maximizing the standard version of the rate-distortion function (Equation 1) is equivalent to increasing the covariance of the representations, \(^{T}\). In the kernelized rate-distortion function (Equation 2), we observe that the kernel matrix \(\) assigns higher weights to instance pairs that have similar concept labels (\(_{ij} 1/(a_{i},a_{j})\)). Intuitively, this means that maximizing \(R(|)\) results in an increased dissimilarity between instance pairs with similar concept labels, as illustrated by the arrows in the center of Figure 1(a). This gradually leads to the distances in the learned representation space \(\) being unrelated to the concept labels.

However, simply maximizing \(R(|)\) may not guarantee robust concept erasure without imposing additional constraints on the overall feature space. We explain why this happens by considering a few scenarios. First, consider the scenario where we do not impose any constraints on the feature space, shown in Figure 1(b). In this scenario, the volume (or intrinsic dimension) of the feature space (analogous to \(R()\) term) also expands as \(R(|)\) is maximized (Lemma 1). Here, we observe that even though the intra-group distances have increased it is still possible to separate the two groups

Figure 1: An illustration of concept erasure using KRaM. Input representations are retrieved from a large language model. The original representations (a) encodes a binary concept variable (the two classes are shown in \(\) and \(\)), which we aim to erase. \(R(|)\) term forces instances from the same class to move apart. However, for robust concept erasure the size of the representation space \(R(Z)\) matters, which we illustrate visually in (b), (c), and (d). KRaM enforces the constraint \(R(Z)=b\) to erase the concept while retaining information from original representations.

using a non-linear decision boundary. Second, we consider the scenario where we try to minimize the volume of the feature space, which is equivalent to minimizing \(R()\). This is illustrated in Figure 1(c), where all instances are pushed together making it hard to predict the concept labels. However, it also results in a significant loss of information from the original representations (as the volume or intrinsic dimension collapses). As different instances become almost similar it destroys the unique features present in Figure 1(a), potentially rendering them ineffective for downstream tasks. Thus, it appears that the optimal approach is to maintain a constant size of the feature space as illustrated in Figure 1(d). We verify these scenarios empirically in Section 5.2. With this consideration, we present the following objective:

\[_{f}R(|),R()=b,\] (3)

where \(b=R()\) is the initial number of bits required to encode the data and \(=f()\). In practice, we found that satisfying the equality \(R()=b\) using a Lagrangian function hinders the maximization of \(R(|)\). For concept erasure, we only want the feature space volume to be constant and do not require it to exactly be \(b\). Therefore, we optimize a relaxed version of the objective (Equation 3):

\[_{f}R(|)-|R()-b|,\] (4)

where \(\) is a hyperparameter. The second term in Equation 4 penalizes the network, \(f\), if the overall volume \(R()\) deviates too far from \(b\). Depending on the nature of the attribute (categorical, continuous, or vector-valued), the user can define the kernel matrix \(\) between concept labels. For categorical concept variables, in our experiments, we use the kernel matrix whose values are \(_{ij}\{0,1\}\), where \(_{ij}=1\) if \(a_{i}=a_{j}\) otherwise \(_{ij}=0\). For continuous and vector-valued variables, the kernel matrix can be derived from the concept labels by using a suitable kernel function (e.g., Gaussian, Laplacian, or Cauchy kernels) if it is not specified by the user. In our experiments, we use a Gaussian (RBF) kernel function for continuous and vector-valued concepts.

**Lemma 1** (General Bounds for \(R(|)\)).: _For any set of representations \(^{n d}\), a kernel matrix \(^{n n}\) using a kernel function satisfying \(k(x,x)=1\) and \(>0\), it holds that:_

\[R() R(|)_{2}(1+d /n^{2}),\] (5)

_where the first equality is satisfied when \(=^{T}\) and the second equality when \(^{T}=I\)._

The detailed proof is provided in Appendix A.1. This result shows that \(R(|)\) has a lower bound equal to the rate-distortion function of the representations with the upper bound being independent of the kernel matrix. We empirically show that maximizing \(R(|)\) also results in an increase in \(R()\) in Section 5.2. Using the results of the above lemma, we can show that the proposed objective (Equation 4) is bounded in the following corollary.

**Corollary 1**.: _Using assumptions in Lemma 1, for \(\) the objective function (Equation 4) is bounded between \([- b,\{(1+)U- b,(1-)U+ b\}]\), where \(U=_{2}(1+d/n^{2})\)._

## 4 Measuring Alignment

A limitation of prior works  is the lack of evaluation beyond proxy tasks of how information preserved by the learned representations \(\) about the original representations \(\), which we refer to as _alignment_. An erasure framework that generates random representations is able to erase concept \(\) perfectly but does not retain any information from \(\). Therefore, it is important to measure the alignment as well while optimizing the erasure function \(f\). To this end, we propose a computationally efficient measure by computing the overlap between \(k\)-nearest neighbour sets of \(\) and \(\). The average nearest neighbour overlap across all representations is the alignment score (\(A_{k}\)) for a given concept erasure function \(f\):

\[A_{k}(f)=}_{x}[|(x) (f(x))|]/k,\] (6)

where \(()\) function computes the \(k\)-nearest neighbour set of a representation. Alignment scores lie between \(A_{k}(f)[k/n,1]\) (Lemma 2). Notice that this measure is quite similar to non-parametric approaches for mutual information (MI) estimation . These methods, while leveraging nearest neighbour information, compute data statistics within a hypercube. However, these MIestimates are often biased for high-dimensional data . In contrast to these methods, we utilize the bijective mapping (erasure function \(f\)) between \(\) and \(\) (which is not available in the general case of MI estimation) to compute the overlap between nearest neighbour sets. The computation of \(A_{k}\) can be made faster using an efficient nearest neighbour data structure like \(k\)d-tree . Note that a similar measure was used to determine the stability of word embeddings .

Empirically, we find that \(A_{k}\) is well correlated with the downstream performance of \(\). Specifically, we perform a synthetic experiment to simulate concept erasure and assess the efficacy of \(A_{k}\) in capturing the alignment of \(\), where we sample a representation set (\(x^{100}\)) and their corresponding labels. We remove information from these representations by projecting them onto nullspaces of its dominant eigenvectors (details provided in Algorithm 1). During this process, we measure the prediction accuracy for the original labels and \(A_{k}\) scores (shown in Figure 2). We find the \(A_{k}\) is highly correlated with the prediction accuracy achieving Pearson correlation scores \( 0.99\) (the average correlation over multiple runs). We also compare \(A_{k}\) with a few different alignment measures (including MI estimates) and find that our method outperforms others (more details in Appendix B).

**Lemma 2** (Alignment for random representations).: _Expected alignment score achieved by a concept erasure framework \(f\) that generates random representations is \([A_{k}(f)]=k/n\)._

The proof is provided in Appendix A.2. This result shows the importance of choosing \(k\). If \(k\) is too small, the \(A_{k}(f)\) scores may be low for many concept erasure functions. Conversely, if \(k n\), then the \(A_{k}(f)\) scores will almost always be close to 1. In our experiments (Figure 2), we find that \(A_{k}\)'s correlation is maximized when \(k=0.5n\).

## 5 Evaluation

In this section, we provide the specifics of the experimental setup and evaluation results for concept erasure using KRaM across various datasets. The implementation of KRaM is publicly available at https://github.com/brcosmmath/KRaM.

**Setup**. In all settings, we follow the same routine for concept erasure: (a) we obtain representations either directly from the dataset or an encoder (e.g., BERT, GPT-3), which are kept frozen; (b) we perform concept erasure in a post-hoc manner to obtain representations \(f(x)\), where \(f\) is a non-linear neural network; and (c) we use \(f(x)\) on downstream tasks and report the metrics.

**Datasets**. We assess the effectiveness of KRaM in erasing 3 types of concept variables: (a) _categorical concepts_ - we apply KRaM to erase binary gender variables from GloVe embeddings and race from BERT embeddings for tweets in the Dial dataset ; (b) _continuous concepts_ - we evaluate KRaM on a synthetic dataset, generated using a continuous latent variable, and UCI Crimes . For these datasets, we treat one of the latent continuous variables and African American (AAE) population ratio as the concepts to be erased, respectively; (c) _vector-valued concepts_ - we evaluate on Jigsaw toxicity detection dataset , where we consider religion and gender (which are vector-valued variables) as the concepts to be erased from GPT-3.5  embeddings. We present additional details in Appendix C.1.

**Baselines**. We compare KRaM with the following baselines: (a) INLP  (linear) iteratively projects representations onto the nullspace of optimal separating linear subspaces; (b) RLACE  (linear) is an extension of INLP that performs concept erasure by solving minimax game; (c) KCE  (non-linear) presents a kernelized version of the minimax game introduced in RLACE; (d) FaRM  (non-linear) employs rate-distortion maximization for erasing categorical concepts ; (e) KRaM\({}_{}\) uses KRaM's with a linear erasure function \(f\). To the best of our knowledge, there are no existing methods for continuous or vector-valued concept erasure. For continuous concepts, we normalize the labels and quantize them into \(n_{b}\) bins (a hyperparameter). We denote a concept erasure method as Method\({}_{}\) whenever quantization is used. For vector-valued concepts, we extend nullspace projection-based

Figure 2: Correlation of alignment score, \(A_{k}(f)\), with accuracy for a synthetic dataset. For different \(k\) values, \(A_{k}(f)\) achieve high Pearson correlation ~\(0.99\).

erasure techniques by quantizing each dimension and projecting onto a series of nullspaces. It is unclear how to utilize non-projection-based methods for vector-valued concept erasure.

**Metrics**. Following previous work [18; 50], we assess concept erasure quality using the following:

_Probing representations_. We use a scikit-learn MLP classifier (non-linear)  to probe \(f(x)\), report classification accuracy for categorical attributes, MSE for continuous and vector-valued attributes (in a dimension-wise manner). A better concept erasure method would have low accuracy and a high MSE for the deleted concept.

_Downstream metrics_. For datasets with a downstream task, we evaluate if the deleted concept still affects the task by measuring statistical parity. We report DP for categorical concepts, \(\)GDP  (see Appendix C.2) for continuous concepts, and \(\)GDP values for each dimension of vector-valued concepts. Lower statistical parity scores are expected after concept erasure. However, we note that concept erasure does not necessarily guarantee fairness . Applying these methods for fairness would require a more application-specific analysis of the risks and biases.

_Alignment_. We report the alignment scores (\(A_{k}\) for \(k=0.5n\), therefore \(A_{k}[0.5,1]\)) wherever a downstream task is absent. Higher \(A_{k}\) scores are expected. If a downstream task is available, we probe \(f(x)\) for that task, where we expect high accuracy or low MSE scores.

### Main Results

In this section, we report the performance of KRaM in erasing different types of concepts: categorical, continuous, and vector-valued variables. Note that the primary objective of concept erasure is to robustly erase the concept variable (by achieving lower probing and fairness metrics) even if that reduces the utility of the representations to some extent. This is different from adversarial learning where we try to achieve a balance between fairness and utility. In our experiments, we show KRaM is able to robustly erase concepts while retaining a significant amount of original information.

**Vector-valued Concept Erasure.** Erasure of vector-valued concepts is useful when the attribute to be deleted is not available in the form of a categorical or normalized continuous variable. We evaluate KRaM on erasing information about religion, a vector-valued concept, in the Jigsaw toxicity dataset , where the downstream task involves detecting whether an online comment is toxic. Each text is annotated with a vector-valued concept: religion with scores over the categories ({'buddhist', 'christian', 'hindu', 'jewish','muslim', 'others'}). We obtain text representations from GPT-3.5 API and use a RBF kernel with a cosine distance function. In Figure 3, we report the MSE and \(\)GDP scores for KRaM along with other baselines. We observe that KRaM performs the best achieving low \(\)GDP with high MSE scores across most concept labels. We also observe that the change in toxicity classification accuracy (93.2% \(\) 92.1%) is minimal during concept erasure. This showcases the efficacy of KRaM in erasing vector-valued attributes as it achieves up to 76% MSE gains over the best baselines. We also report the results for vector-valued gender erasure in Appendix D.

**Continuous Concept Erasure**. We evaluate the efficacy of KRaM in erasing continuous concepts on a synthetic dataset and UCI Crimes. We compare with baseline approaches that use quantized concept labels. In Table 1 (left), we report the results on the synthetic dataset and observe that KRaM performs the best in preventing leakage of \(a\) (high MSE scores) while achieving considerable alignment score, \(A_{k}\). While both FaRM and KRaM utilize non-linear erasure functions, a necessity for robust concept erasure, they tend to achieve relatively lower \(A_{k}\) scores. Despite this, it is important to note that significant information can still be preserved through non-linear warping, which we show

Figure 3: Vector-valued concept (religion) erasure performance using KRaM on Jigsaw toxicity dataset. KRaM achieves better performance (\(\) MSE & \(\)\(\)GDP) than baseline approaches in most settings.

in the categorical experiments. Note that linear erasure functions are able to retain nearest neighbour structures better, thereby achieving higher \(A_{k}\) scores. In Figure 5, we visualize the UMAP projection of synthetic data, where the representations' position is indicative of the latent continuous concept attribute prior to concept erasure (left). Post concept erasure (right), we observe no such discernible correlation. For UCI Crimes, we perform erasure for the African-American (AAE) population ratio, and use the generated representations to predict the normalized number of crimes per capita (\(y\)). Table 1 (right) shows that KRaM generates representations with minimal information about AAE ratio (high MSE (\(a\))) and low \(\)GDP scores (\(\)GDP \( 0\)). These experiments showcase KRaM's efficacy in erasing continuous attributes and minimizing their impact on downstream tasks (low \(\)GDP scores).

**Categorical Concept Erasure**. In Table 2, we evaluate categorical concept erasure on Dial tweet classification (race) and GloVe (gender) datasets. For Dial dataset, we obtain BERT  representations of tweets, perform concept erasure for race (binary) attribute, and use the generated representations \(f(x)\) for sentiment classification. We report the accuracy of predicting sentiment (\(y\)), race (\(a\)), and demographic parity (DP) of the predictions in Table 2 (left). We observe that KRaM performs the best in erasing race attribute (evident from high Acc. (\(a\))) and demographic parity while attaining comparable accuracy on sentiment classification (Acc. (\(y\))). For GloVe embeddings (Table 2 (right)), we observe that KRaM achieves the state-of-the-art result in suppressing the gender leakage (probing accuracy for predicting binary gender attribute). We observe that linear techniques INLP and RLACE, obtain high alignment scores, \(A_{k}\), but their representations still contain significant gender information (indicated by high Acc. (\(a\))). This shows a trade-off between information alignment and concept erasure, where robustly erasing a concept may also result in the removal of other information. Categorical concept erasure has been extensively studied, and the fact that a general erasure framework, KRaM, can perform on par with state-of-the-art methods underscores its efficacy.

### Analysis

In this section, we perform several analysis experiments to understand the functioning of KRaM.

**Comparison with MI estimation techniques**. In this experiment, we compare our concept erasure framework with a few state-of-the-art mutual information (MI) estimation approaches. Essentially,

    &  &  \\  Method & MSE (\(a\)) \(\) & \(A_{k}\) \(\) & Rank \(\) & MSE (\(y\)) \(\) & MSE (\(a\)) \(\) & \(\)GDP \(\) & \(A_{k}\) \(\) \\  Original & \(0.006\) & \(1.0\) & \(100\) & \(0.046\) & \(0.030\) & \(0.058\) & \(1.0\) \\ Random & \(0.174\) & \(0.50\) & \(100\) & \(0.211\) & \(0.251\) & \(0.006\) & \(0.50\) \\ INLP\({}_{9}\) & \(0.084\)\(\,\)\% & \(0.85\)\(\,\)\% & \(100\) & \(0.055\)\(\,\)\% & \(0.056\) & \(0.0\)\(\,\)\% & \(0.90\)\(\,\)\% \\
**RLACE\({}_{9}\)** & \(0.021\) & \(0.87\)\(\,\)\% & \(100\) & \(0.038\)\(\,\)\% & \(0.022\) & \(0.051\) & \(0.81\) \\
**FaRM\({}_{6}\)** & \(0.068\) & \(0.74\) & \(100\) & \(0.050\)\(\,\)\% & \(0.064\)\(\,\)\% & \(0.013\)\(\,\)\% & \(0.62\)\(\,\)\% \\  KRaM & \(0.109\)\(\,\)\% & \(0.67\) & \(100\) & \(0.069\) & \(0.104\)\(\,\)\% & \(0.001\)\(\,\)\% & \(0.59\) \\ KRaM\({}_{}\) & \(0.083\)\(\,\)\% & \(0.75\)\(\,\)\% & \(100\) & \(0.067\) & \(0.082\)\(\,\)\% & \(0.022\) & \(0.69\)\(\,\)\% \\   

Table 1: Continuous concept erasure: We evaluate on the synthetic and UCI Crimes. Post concept erasure using KRaM, we observe a significant increase in MSE (\(a\)) combined with a drop in \(\)GDP.

    &  &  \\  Method & Acc. (\(y\)) \(\) & Acc. (\(a\)) \(\) & DP \(\) & Acc. (\(a\)) \(\) & \(A_{k}\) \(\) & Rank \(\) \\  Original & \(75.5\) & \(87.7\) & \(0.26\) & \(100.0\) & \(1.0\) & \(300\) \\ Random & \(50.8\) & \(50.5\) & \(0.01\) & \(50.2\) & \(0.50\) & \(300\) \\ INLP  & \(75.1\)\(\,\)\% & \(69.5\) & \(0.16\) & \(86.3\) & \(0.85\)\(\,\)\% & \(210\) \\ RLACE  & \(75.5\)\(\,\)\% & \(82.1\)\(\,\)\% & \(0.18\) & \(95.5\) & \(0.93\)\(\,\)\% & \(300\)\(\,\)\% \\ KCE  & \(75.0\) & \(80.1\)\(\,\)\% & \(0.12\)\(\,\)\% & \(63.5\)\(\,\)\% & \(0.62\) & \(100\) \\ FaRM  & \(74.8\) & \(54.2\)\(\,\)\% & \(0.09\)\(\,\)\% & \(53.9\)\(\,\)\% & \(0.65\) & \(247\)\(\,\)\% \\  KRaM & \(72.4\) & \(54.0\)\(\,\)\% & \(0.08\)\(\,\)\% & \(52.6\)\(\,\)\% & \(0.65\) & \(246\)\(\,\)\% \\ KRaM\({}_{}\) & \(75.4\)\(\,\)\% & \(67.5\)\(\,\)\% & \(0.18\) & \(67.0\) & \(0.73\)\(\,\)\% & \(130\) \\   

Table 2: Categorical concept erasure: We assess binary gender and race erasure from GloVe and BERT representations (from Dial) respectively. We denote the top 3 results for any metric using \(\,\)\%, and \(\,\)\% respectively. Desired trends for all metrics are shown using \(\) or \(\).

the task of concept erasure can be formalized as maximizing the objective: \(I(,)-I(,)\), where \(I(,)\) denotes the mutual information between two sets. We optimize this objective function with the following MI estimates: InfoNCE , MINE , CLUB , and KNIFE . In Table 4, we report the gender accuracy (Acc. (\(a\))), \(A_{k}\), and performance on WordSim-353 benchmark , which is also reflective of the alignment, on GloVe dataset. We observe that MI techniques lose significant information from the original representations achieving near-random \(A_{k}\) and WS-353 scores. We believe this happens because MI approaches optimize their estimation parameters along with the erasure function \(f\), which is difficult.

We observe a unique scenario for the CLUB method, where the Acc. (\(a\)) is high and \(A_{k}\) is low. This implies that the clusters (related to different genders) may be retained but the nearest neighbour structure within the clusters is perturbed significantly. Probing for the downstream task (Acc. (\(a\))) may give you the impression that information from the original space is retained, while \(A_{k}\) provides a more fine-grained view contradicting such an incorrect conclusion. In general, we find that \(A_{k}\) values are relatively well correlated with WS-353 scores, which is a good measure of the information retained in the representation space after erasure. However, computing WS-353 requires additional annotation that may not be feasible for representation sets other than word embeddings.

We also compare KRaM with several other mutual information estimation based baselines that use a unique objective function for controlling information in representations. Specifically, we compare with MIFR , CCL , and ICVAE  and report the results on DIAL dataset in Table 3. Note that all of these methods work for categorical concepts only, and ICVAE requires access to concept labels for test instances as well, a limitation compared to KRaM and the other methods. In Table 3, we observe MIFR is unable to erase concepts robustly (based on Acc. (\(a\)) and DP scores). ICVAE and CCL are able to delete concepts but at the significant cost of deleting a lot of original information. Loss of information from the original space (low Acc. (\(y\))) using ICVAE and CCL is quite similar to other MI methods reported in Figure 4. Compared to these methods, KRaM is able to get similar concept erasure performance while achieving much higher Acc. (\(y\)) scores. We report additional results using these baselines on the GloVe dataset in Appendix D.

**Image-based datasets**. We perform experiments on image-based datasets to evaluate the efficacy of KRaM on different domains. We consider two different setups using CelebA  and Colored MNIST  datasets. In CelebA, we consider the binary variable _attractiveness_ as the target attribute and whether the face has makeup applied (binary variable) as the protected attribute. With concept erasure using KRaM, the accuracy of predicting the makeup attribute dropped from 85.7% to 69.6%. The demographic parity of the predictions (for attractiveness) also improved from 0.94 to 0.54. This shows the effectiveness of KRaM in removing the makeup concept.

For Colored MNIST, we follow the setup of  to create a biased version of the MNIST dataset . The background color of the digits is made to be well correlated with the digit in the training set. However, no such correlation exists in the test set. We use a 0.8 correlation between the background

Figure 4: Comparison of KRaM with state-of-the-art mutual information (MI) estimation methods. KRaM achieves a balance between good concept erasure nearly with high \(A_{k}\).

Figure 5: Visualization of UMAP projection of representations obtained from the synthetic dataset before and after concept erasure. Concept erasure makes the positions of representations uncorrelated with their continuous concept labels (denoted by their color).

   Method & Acc (\(y\)) \(\) & Acc (\(a\)) \(\) & DP \(\) \\  Original & 75.5 & 87.7 & 0.26 \\ MIFR  & 75.4 & 68.7 & 0.21 \\ CCL  & 50.7 & 52.6 & 0.01 \\ ICVAE  & 66.5 & 53.3 & 0.10 \\   

Table 3: Comparison of KRaM with mutual information based baseline approaches on DIAL dataset. We observe that KRaM achieves a fine balance between concept erasure and retaining task performance.

color and the digit label. We treat background color as the concept to be erased. Naively training a classifier on the images will perform poorly on the test set as the classifier can easily overfit the background color. The digit information could be predicted with an accuracy of 79.9%. Post-concept erasure we observe that the classifier's performance improves from 79.9% to 87.1%. This shows the effectiveness of KRaM to remove background information and help the classifier generalize better.

**Evolution of loss functions**. In this experiment, we investigate the evolution of loss terms \(R()\) and \(R(|)\) under various settings of the objective function (described in Figure 1) on the synthetic dataset. In Figure 6 (a), we observe that indeed maximizing the kernelized rate-distortion function also leads to an increase in \(R()\), which can result in the concept variable not being completely erased. In Figure 6 (b), we examine the scenario where \(R(|)-R()\) is maximized, where we observe indicate a substantial drop in \(R()\). This reduction often implies a low intrinsic dimension and a substantial information loss. In Figure 6 (c), we present the evolution of loss terms for KRaM's objective. We notice that \(R()\) aligns closely with the initial number of bits, denoted as \(b\). Meanwhile, the \(R(|)\) is maximized to its full extent (similar to Figure 6 (a)). Through these experiments, we provide empirical verification for the insights discussed in relation to Figure 1.

We conduct additional ablation experiments that involve varying hyperparameters and kernel functions. The results of these experiments are reported in Appendix D.

## 6 Conclusion

In this paper, we proposed KRaM, a novel framework to robustly perform concept erasure from a representation set. KRaM uses a kernelized formulation of the rate-distortion function, where the kernel is created using concept labels. This approach ensures that instances with similar concept labels become dissimilar in the representation space, which ultimately results in the erasure of the concept variable. KRaM is a versatile method capable of erasing a wide range of concepts, including categorical, continuous, and vector-valued variables. We theoretically analyze several properties of the proposed KRaM objective. Empirical evaluation shows the efficacy of KRaM on a wide range of setups ranging from gender erasure from GloVe embeddings to vector-valued concept erasure from GPT-3.5 embeddings. We also propose a heuristic-based measure to capture the information alignment of the erasure function \(f\) by analyzing the \(k\)-nearest neighbours of the representations. While KRaM effectively erases concepts, it does result in the loss of some information from the original space, as evidenced by the alignment scores. Determining the minimum amount of information that must be distorted to fully erase a concept remains an open question. Future research could concentrate on gaining a deeper understanding of this issue and developing techniques that can erase concepts from representations while having minimal impact on alignment with original representations.

#### Acknowledgement

The authors are thankful to Alex Beutel (OpenAI previously Google Research) and Kelsey Allen (Google DeepMind) for helpful feedback on an earlier version of this paper. The work of Somnath Basu Roy Chowdhury and Snigdha Chaturvedi was supported in part by Amazon Research Awards.