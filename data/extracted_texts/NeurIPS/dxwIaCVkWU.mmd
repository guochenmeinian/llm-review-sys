# Divide-and-Conquer Predictive Coding: a Structured Bayesian Inference Algorithm

Eli Sennesh\({}^{1}\), Hao Wu\({}^{2}\), Tommaso Salvatori\({}^{2,3}\)

\({}^{1}\)Department of Psychology, Vanderbilt University, Nashville, TN, USA

\({}^{2}\)VERSES AI Research Lab, Los Angeles, USA

\({}^{3}\)Vienna University of Technology, Vienna, Austria

eli.sennesh@vanderbilt.edu, uuhaomxby@gmail.com, tommaso.salvatori@verses.ai

###### Abstract

Unexpected stimuli induce "error" or "surprise" signals in the brain. The theory of predictive coding promises to explain these observations in terms of Bayesian inference by suggesting that the cortex implements variational inference in a probabilistic graphical model. However, when applied to machine learning tasks, this family of algorithms has yet to perform on par with other variational approaches in high-dimensional, structured inference problems. To address this, we introduce a novel predictive coding algorithm for structured generative models, that we call divide-and-conquer predictive coding (DCPC); it differs from other formulations of predictive coding, as it respects the correlation structure of the generative model and provably performs maximum-likelihood updates of model parameters, all without sacrificing biological plausibility. Empirically, DCPC achieves better numerical performance than competing algorithms and provides accurate inference in a number of problems not previously addressed with predictive coding. We provide an open implementation of DCPC in Pyro on Github.

## 1 Introduction

In recent decades, the fields of cognitive science, machine learning, and theoretical neuroscience have borne witness to a flowering of successes in modeling intelligent behavior via statistical learning. Each of these fields has taken a different approach: cognitive science has studied probabilistic _inverse inference_ in models of each task and environment, machine learning has employed the backpropagation of errors , and neuroscience has hypothesized that _predictive coding_ (PC)  may explain neural activity in perceptual tasks. These approaches share in common a commitment to "deep" models, in which task processing emerges from the composition of elementary units.

In machine learning, PC-based algorithms have recently gained popularity for their theoretical potential to provide a more biologically plausible alternative to backpropagation for training neural networks . However, PC does not perform comparably in these tasks to backpropagation due to limitations in current formulations. First, predictive coding for gradient calculation typically models every node in the computation graph with a Gaussian, and hence fails to express many common generative models. Recent work on PC has addressed this by allowing approximating non-Gaussian energy functions with samples . Second, the Laplace approximation to the posterior infers only a maximum-a-posteriori (MAP) estimate and Gaussian covariance for each latent variable, keeping PC from capturing multimodal or correlated distributions. Third, this loose approximation to the posterior distribution results in inaccurate, high-variance updates to the parameters of the generative model.

In this work we propose a new algorithm, _divide-and-conquer predictive coding_ (DCPC), for approximating structured target distributions with populations of Monte Carlo samples. DCPC goes beyond Gaussian assumptions, and decomposes the problem of sampling from structured targets into local coordinate updates to individual random variables. These local updates are informed by unadjusted Langevin proposals parameterized in terms of biologically plausible prediction errors. Nesting the local updates within divide-and-conquer Sequential Monte Carlo (Lindsten et al., 2017; Kuntz et al., 2024) ensures that DCPC can target any statically structured graphical model, while Theorem 2 provides a locally factorized way to learn model parameters by maximum marginal likelihood.

DCPC also provides a computational perspective on the canonical cortical microcircuit (Bastos et al., 2012, 2020; Campagnola et al., 2022) hypothesis in neuroscience. Experiments have suggested that deep laminar layers in the cortical microcircuit represent sensory imagery, while superficial laminar represent raw stimulus information (Bergmann et al., 2024); experiments in a predictive coding paradigm specifically suggested that the deep layers represent "predictions" while the shallow layers represent "prediction errors". This circuitry could provide the brain with its fast, scalable, generic Bayesian inference capabilities. Figure 1 compares the computational structure of DCPC with that of previous PC models. The following sections detail the contributions of this work:

* Section 3 defines the divide-and-conquer predictive coding algorithm and shows how to use it as a variational inference algorithm;
* Section 4 examines under what assumptions the cortex could plausibly implement DCPC, proving two theorems that contribute to biological plausibility;
* Section 5 demonstrates DCPC experimentally in head-to-head comparisons against recent generative models and inference algorithms from the predictive coding literature.

Section 2 will review the background for Section 3's algorithm: the problem predictive coding aims to solve and a line of recent work adressing that problem from which this paper draws.

## 2 Background

This section reviews the background necessary to construct the divide-and-conquer predictive coding algorithm in Section 3. Let us assume we have a directed, acyclic graphical model with a joint density split into observations \(x\) and latents \(z\), parameterized by some \(\) at each conditional density

\[p_{}(,):=_{x}p_{}(x (x))_{z}p_{}(z(z)),\] (1)

where \((z)\) denotes the parents of the random variable \(z\) and \((z)\) denotes its children.

Empirical Bayes_Empirical Bayes_ consists of jointly estimating, in light of the data, both the parameters \(^{*}\) and the Bayesian posterior over the latent variables \(\), that is:

\[^{*}=_{}p_{}()=_{}_{ }\ p_{}(,)\,d,  28.452756ptp_{^{*}}():=}(,)}{p_{^{*}}()}.\]

Figure 1: **Left**: Classical PC learns a mean-field approximate posterior with prediction error layers. **Right**: Divide-and-conquer PC approximates the joint posterior with bottom-up and recurrent errors. Where classical predictive coding has layers communicate through shared error units, divide-and-conquer predictive coding separates recurrent from “bottom-up” error pathways to target complete conditional distributions rather than posterior marginal distributions.

Typically the marginal and posterior densities have no closed form, so learning and inference algorithms treat the joint distribution as a closed-form _un_normalized density over the latent variables; its integral then gives the normalizing constant for approximation

\[_{}():=p_{}(,), Z_{ }:=_{}\;_{}()\;d =p_{}(),_{}():= {_{}()}{Z_{}}.\]

Neal and Hinton (1998) reduced empirical Bayes to minimization of the _variational free energy_:

\[(,q):=_{ q()}[- ()}{q()}]- Z().\] (2)

The ratio of densities in Equation 2 is an example of a _weight_ used to approximate a distribution known only up to its normalizing constant. The _proposal_ distribution \(q()\) admits tractable sampling, while the unnormalized _target_ density \(_{}()\) admits tractable, pointwise density evaluation.

Predictive CodingComputational neuroscientists now often hypothesize that _predictive coding_ (PC) can optimize the above family of objective functionals in a local, neuronally plausible way (Millidge et al., 2021, 2023). More in detail, it is possible to define this class of algorithms as follows:

**Definition 1** (Predictive Coding Algorithm).: _Consider approximate inference in a model \(p_{}(,)\) using an algorithm \(\). Salvatori et al. (2023) calls \(\) a predictive coding algorithm if and only if:_

1. _It maximizes the model evidence_ \( p_{}()\) _by minimizing a variational free energy;_
2. _The proposal_ \(q()=_{z}q(z)\) _factorizes via a mean-field approximation; and_
3. _Each proposal factor is a Laplace approximation (i.e._ \(q_{}(z):=(,())\)_)._

Particle AlgorithmsIn contrast to predictive coding, particle algorithms approach empirical Bayes problems by setting the proposal to a collection of weighted particles \((w^{k},^{k})\) drawn from a sampling algorithm meeting certain conditions (see Definition 4 in Appendix B). Any proposal meeting these conditions (see Proposition 1 in Appendix B and Naesseth et al. (2015), Stites et al. (2021)) defines a free energy functional, analogous to Equation 2 in upper-bounding the model surprisal:

\[(,q):=_{w, q(w,)}[-  w](,q)- Z().\]

This paper builds on the particle gradient descent (PGD) algorithm of Kuntz et al. (2023), that works as follows: At each iteration \(t\), PGD diffuses the particle cloud \(q_{K}()=_{k=1}^{K}_{^{k}}()\) across the target log-density with a learning rate \(\) and independent Gaussian noise; it then updates the parameters \(\) by ascending the gradient of the log-likelihood, estimated by averaging over the particles. The update rules are then the following:

\[^{t+1,k} :=^{t,k}+_{}_{^{t} }(^{t,k})+^{k},\] (3) \[^{t+1} :=^{t}+(_{k=1}^{K}_{} _{^{t}}(^{t+1,k})).\] (4)

The above equations target the joint density of an entire graphical model1. When the prior \(p_{}()\) factorizes into many separate conditional densities, achieving high inference performance often requires factorizing the inference network or algorithm into conditionals as well (Webb et al., 2018). Estimating the gradient of the entire log-joint, as in PGD and amortized inference (Dasgupta et al., 2020; Peters et al., 2024), also requires nonlocal backpropagation. To provide a generic inference algorithm for high-dimensional, structured models using only local computations, Section 3 will apply Equation 3 to sample individual random variables in a joint density, combine the coordinate updates via sequential Monte Carlo, and locally estimate gradients for model parameters via Equation 4.

## 3 Divide-and-Conquer Predictive Coding

The previous section provided a mathematical toolbox for constructing Monte Carlo algorithms based on gradient updates and a working definition of predictive coding. This section will combine those tools to generalize the above notion of predictive coding, yielding the novel _divide-and-conquer predictive coding_ (DCPC) algorithm. Given a causal graphical model, DCPC will approximate the posterior with a population \(q()\) of \(K\) samples, while also learning \(\) explaining the data. This will require deriving local coordinate updates and then parameterizing them in terms of prediction errors.

Let us assume we again have a causal graphical model \(p_{}(,)\) locally parameterized by \(\) and factorized (as in Equation 1) into conditional densities for each \(x\) and \(z\). DCPC then requires two hyperparameters: a learning rate \(^{+}\), and particle count \(K^{+}\), and is initialized (at \(t=0\)) via a population of predictions by ancestor sampling defined as \(^{0}_{z}p_{}(z^{0}(z^{0}))\).

DCPC aims to minimize the variational free energy (Equation 2). The optimal proposal \(q_{*}\) for each random variable would equal, if it had closed form, the _complete conditional_ density for that variable, containing all information from other random variables

\[q_{*}(^{t}^{t-1})_{}(z;_ { z})=p_{}(z(z))_{v(z)}p_{ }(v(v)).\] (5)

We observe that the prediction errors \(_{z}\) in classical predictive coding, usually defined as the precision weighted difference between predicted and actual value of a variable, can be seen as the _score function_ of a Gaussian, where the score is the gradient with respect to the parameter \(z\) of the log-likelihood:

\[_{z}:=_{z}(z,)=(x-z);\]

When given the ground-truth parameter \(z\), the _expected_ score function \(_{x p(x|z)}[_{z} p(x z)]\) under the likelihood becomes zero, making score functions a good candidate for implementing predictive coding. We therefore define \(_{z}\) in DCPC as the complete conditional's score function

\[_{z}:=_{z}_{}(z;_{ z})= _{z} p_{}(z(z))+_{v(z)} _{z} p_{}(v(v)).\] (6)

This gradient consists of a sum of local prediction-error terms: one for the local "prior" on \(z\) and one for each local "likelihood" of a child variable. By defining the prediction error as a sum of local score functions, we write Equation 3 in terms of \(_{z}\) (Equation 6) and the preconditioner of Definition 3:

\[q_{}(z^{t}_{z}^{t},z^{t-1}):=(z^{t-1}+ _{}_{z}^{t},2_{} ).\]

The resulting proposal now targets the complete conditional density (Equation 5), simultaneously meeting the informal requirement of Definition 1 for purely local proposal computations while also "dividing and conquering" the sampling problem into lower-dimensional coordinate updates.

Since the proposal from which we can sample by predictive coding is not the optimal coordinate update, we importance weight for the true complete conditional distribution that is optimal

\[z^{t} q_{}(z^{t} z^{t-1},_{z}^{t}) u_{z}^{t}=}(z^{t};_{ z})}{q_ {}(z^{t} z^{t-1},_{z}^{t})};\] (7)

resampling with respect to these weights corrects for discretization error, yields particles distributed according to the true complete conditional, and estimates the complete conditional's normalizer

\[(z^{t},u_{z}^{t})_{^{t-1}}(z^{t} _{ z}), _{^{t-1}}(_{ z})^{t}:= _{k=1}^{K}u_{z}^{t,k}.\]

The recursive step of "Divide and Conquer" Sequential Monte Carlo (Lindsten et al., 2017; Kuntz et al., 2024) exploits the estimates \(_{^{t-1}}(_{ z})^{t}\) to weigh the samples for the complete target density

\[w_{^{t-1}}^{t}=}(,^{t})}{ _{z}_{}(z^{t};_{ z})}_{z }_{^{t-1}}(_{ z})^{t}.\] (8)

   & PC & LPC & MCPC & DCPC (ours) \\  Generative density & Gaussian & Differentiable & Gaussian & Differentiable \\ Inference approximation & Laplace & Gaussian & Empirical & Empirical \\ Posterior conditional structure & ✗ & ✗ & ✗ & ✓ \\  

Table 1: Comparison of divide-and-conquer predictive coding (DCPC) against other predictive coding algorithms. DCPC provides the greatest flexibility: arbitrary differentiable generative models, an empirical approximation to the posterior, and sampling according to the target’s conditional structure.

By Proposition 1, log-transforming these weights estimates the free energy (Equation 2):

\[^{t}(^{t-1},^{t-1}):=_{q_{*}(^{t}| ^{t-1})}[-}(,^{t})} {q_{*}(^{t}^{t-1})}]_{q}[ - w_{^{t-1}}^{t}].\]

Theorem 3 in Appendix B shows that the gradient \(_{^{t-1}}^{t}=_{q}[-_{^{t-1 }} p_{^{t-1}}(,^{t})]\) of the above estimator equals the expected gradient of the log-joint distribution. Descending this gradient \(^{t}:=^{t-1}-_{^{t-1}}^{t}\) enables DCPC to learn model parameters \(\).

The above steps describe a single pass of divide-and-conquer predictive coding over a causal graphical model. Algorithm 1 shows the complete algorithm, consisting of nested iterations over latent variables \(z\) (inner loop) and iterations \(t T\) (outer loop). DCPC satisfies criteria (1) and (2) of Definition 1, and relaxes criterion (3) to allow gradient-based proposals beyond the Laplace assumption. As with Pinchetti et al. (2022) and Oliviers et al. (2024), relaxing the Laplace assumption enables much greater flexibility in approximating the model's true posterior distribution.

```
0: learning rate \(^{+}\), particle count \(K\), number of sweeps \(S\)
0: initial particle vector \(^{0}\), initial parameters \(^{0}\), observations \(\)
1:for\(t[1 T]\)do\(\) Loop through predictive coding steps
2:for\(s[1 S]\)do\(\) Loop through Gibbs sweeps over graphical model
3:for\(z\)do\(\) Loop through latent variables in graphical model
4:\(_{z}_{} p_{^{t-1}}(z (z))\)\(\) Local prediction error
5:\(_{z}_{z}+_{v(z)}_{ } p_{^{t-1}}(v(v))\)\(\) Children's prediction errors
6:\(_{}_{K}(_{z}^{1:K })^{-1}}{[_{K}(_{z}^{1:K})^{-1}]}\)\(\) Estimate precision of prediction errors
7:\(z^{t} q_{}(z^{t}_{z},z^{t-1})\)\(\) Sample coordinate update
8:\(u_{z}^{t}}(z^{t},_{z})}{_{ }(z^{t},z^{t-1})}\)\(\) Correct coordinate update by weighing
9:\(z^{t}(z^{t},u_{z}^{t})\)\(\) Resample from true coordinate update
10:\(_{^{t-1}}(_{ z})^{t} _{k=1}^{K}u_{z}^{t,k}\)\(\) Estimate coordinate update's normalizer
11:\(^{t}-_{k=1}^{K}(}(,^{t})}{_{i}_{^{t-1} }(z^{t,k};_{ z}^{t,k})}_{z}_{ ^{t-1}}(_{ z})^{t})\)\(\) Update free energy
12:\(^{t}^{t-1}+_{k=1}^{K}_{^{t-1}} p_{ ^{t-1}}(,^{t,k})\)\(\) Update parameters
13:return\(^{T}\), \(^{T},^{T}\)\(\) Output: updated particles, parameters, free energy ```

**Algorithm 1** Divide-and-Conquer Predictive Coding for empirical Bayes

## 4 Biological plausibility

Different works in the literature consider different criteria for biological plausibility. This paper follows the non-spiking predictive coding literature and considers an algorithm biologically plausible if it performs only spatially local computations in a probabilistic graphical model (Whittington and Bogacz, 2017), without requiring a global control of computation. However, while in the standard literature locality is either directly defined in the objective function (Rao and Ballard, 1999), or derived from a mean-field approximation to the joint density (Friston, 2005), showing that the updates of the parameters of DCPC require only local information is not as trivial. To this end, in this section we first formally show that DCPC achieves decentralized inference of latent variables \(\) (Theorem 1), and then that also the parameters \(\) are updated via local information (Theorem 2).

Gibbs sampling provides the most widely-used algorithm for sampling from a high-dimensional probability distribution by local signaling. It consists of successively sampling coordinate updates to individual nodes in the graphical model by targeting their complete conditional densities \(_{}(z,_{ z})\). Theorem 1 demonstrates that DCPC's coordinate updates approximate Gibbs sampling.

**Theorem 1** (DCPC coordinate updates sample from the true complete conditionals).: _Each DCPC coordinate update (Equation 7) for a latent \(z\) samples from \(z\)'s complete conditional (the normalization of Equation 5). Formally, for every measurable \(h:\), resampled expectations with respect to the DCPC coordinate update equal those with respect to the complete conditional_

\[_{z q_{}(z z^{t-1},_{z}^{t})}[_{u(u),z^{t}(z,u_{z})}[h(z) ]]=_{z}\ h(z)\ _{}(z_{ z})\ dz.\]Proof.: See Corollary 4.1 in Appendix B. 

We follow the canonical cortical microcircuit hypothesis of predictive coding (Bastos et al., 2012; Gillon et al., 2023) or predictive routing (Bastos et al., 2020). Consider a cortical column representing \(z\); the \(\), \(/\), and \(\) frequency bands of neuronal oscillations (Buzsaki and Draguhn, 2004) could synchronize parallelizations (known to exist for simple Gibbs sampling in a causal graphical model (Gonzalez et al., 2011)) of the loops in Algorithm 1. From the innermost to the outermost and following the neurophysiological findings of Bastos et al. (2015), Fries (2015), \(\)-band oscillations could synchronize the bottom-up conveyance of prediction errors (lines 4-6) from L2/3 of lower cortical columns to L4 of higher columns, \(\)-band oscillations could synchronize the top-down conveyance of fresh predictions (implied in passing from \(s\) to \(s+1\) in the loop of lines 2-9) from L5/6 of higher columns to L1+L6 of lower columns, and \(\)-band oscillations could synchronize complete attention-directed sampling of stimulus representations (lines 1-11). Figure 5 in Appendix A visualizes these hypotheses for how neuronal areas and connections could implement DCPC.

Biological neurons often spike to represent _changes_ in their membrane voltage (Mainen and Sejnowski, 1995; Lundstrom et al., 2008; Forkosh, 2022), and some have even been tested and found to signal the temporal derivative of the logarithm of an underlying signal (Adler and Alon, 2018; Borba et al., 2021). Theorists have also proposed models (Chavlis and Poirazi, 2021; Moldwin et al., 2021) under which single neurons could calculate gradients internally. In short, if neurons can represent probability densities, as many theoretical proposals and experiments suggest they can, then they can likely also calculate the prediction errors used in DCPC. Theorem 2 will demonstrate that given the "factorization" above, DCPC's model learning requires only local prediction errors.

**Theorem 2** (DCPC parameter learning requires only local gradients in a factorized generative model).: _Consider a graphical model factorized according to Equation 1, with the additional assumption that the model parameters \(=_{x}_{x}_{z} _{z}\) factorize disjointly. Then the gradient \(_{}(,q)\) of DCPC's free energy similarly factorizes into a sum of local particle averages_

\[_{}=_{q}[-_{} p_{ }(,)]-_{v(,)} _{k=1}^{K}_{_{v}} p_{_{v}}(v^{k} (v)^{k}).\] (9)

Proof.: See Proposition 5 in Appendix B. 

Our practical implementation of DCPC, evaluated in the experiments above, takes advantage of Theorem 2 to save memory by detaching samples from the automatic differentiation graph in the forward ancestor-sampling pass through the generative model.

Finally, DCPC passes from local coordinate updates to the joint target density via an importance resampling operation, requiring that implementations synchronously transmit numerical densities or log-densities for the freshly proposed particle population. While phase-locking to a cortical oscillation may make this biologically feasible, resampling then requires normalizing the weights. Thankfully, divisive normalization appears ubiquitously throughout the brain (Carandini and Heeger, 2012), as well as just the type of "winner-take-all" circuit that implements a softmax function (e.g. for normalizing and resampling importance weights) being ubiquitous in crosstalk between superficial and deep layers of the cortical column (Liu, 1999; Douglas and Martin, 2004).

## 5 Experiments

Divide-and-conquer predictive coding is not the first predictive coding algorithm to incorporate sampling into the inference process, and certainly not the first variational inference algorithm for structured graphical models. This section therefore evaluates DCPC's performance against both models from the predictive coding literature and against a standard deep generative model. Each experiment holds the generative model, dataset, and hyperparameters constant except where noted.

We have implemented DCPC as a variational proposal or "guide" program in the deep probabilistic programming language Pyro (Bingham et al., 2019); doing so enables us to compute free energy and prediction errors efficiently in graphical models involving neural networks. Since the experiments below involve minibatched subsampling of observations \(\) from a dataset \( p()\) of unknown distribution, we replace Equation 9 with a subsampled form (see Welling and Teh (2011)for derivation) of the variational Sequential Monte Carlo gradient estimator (Naesseth et al., 2018)

\[_{}||_{ p( )}[|}_{^{b}} _{(,w)^{1:K} q}[(_{k=1}^ {K}w^{k})^{b}]].\] (10)

We optimized the free energy in all experiments using Adam (Kingma and Ba, 2014), making sure to call detach() after every Pyro sample() operation to implement the purely local gradient calculations of Theorem 2 and Equation 10. The first experiment below considers a hierarchical Gaussian model on three simple datasets. The model consists of two latent codes above an observation.

Deep latent Gaussian models with predictive codingOliviers et al. (2024) brought together predictive coding with neural sampling hypotheses in a single model: Monte Carlo predictive coding (MCPC). Their inference algorithm functionally backpropagated the score function of a log-likelihood, applying Langevin proposals to sample latent variables from the posterior joint density along the way. They evaluated MCPC's performance on MNIST with a deep latent Gaussian model (Rezende et al., 2014) (DLGM). Their model's conditional densities consisted of nonlinearities followed by linear transformations to parameterize the mean of each Gaussian conditional, with learned covariances. Figure 2 shows that the DLGM structure already requires DCPC to respect hierarchical dependencies.

We tested DCPC's performance on elementary reconstruction and generation tasks by using it to train this exact generative model, changing only the likelihood from a discrete Bernoulli to a continuous Bernoulli (Loaiza-Ganem and Cunningham, 2019). After training we evaluated with a discrete Bernoulli likelihood. Table 2 shows that in terms of both surprise (negative log evidence, with the discrete Bernoulli likelihood) and mean squared reconstruction error, DCPC enjoys better average performance with a lower standard deviation of performance, the latter by an order of magnitude. All experiments used a learning rate \(=0.1\) and \(K=4\) particles.

Figure 3 shows an extension of this experiment to EMNIST (Cohen et al., 2017) and Fashion MNIST (Xiao et al., 2017) as well as the original MNIST, with ground-truth images in the top row and their reconstructions from DCPC-inferred latent codes below. The ground-truth images come from a 10% validation split of each data-set, on which DCPC only infers particles \(q_{K=4}()\). The above datasets do not typically challenge a new inference algorithm. The next experiment will thus attempt to learn representations of color images, as in the widely-used variational autoencoder (Kingma and Welling, 2013) framework, without an encoder network or amortized inference.

Image generation with representation learningZahid et al. (2024) have also recently designed and evaluated Langevin predictive coding (LPC), with differences from both MCPC and DCPC.

  Inference algorithm & Dataset & NLL \(\) & Mean Squared Error \(\) \\  MCPC & MNIST & \(144.6 0.7\) & \((8.29 0.05) 10^{-2}\) \\ DCPC & MNIST & \(\) & \(}\) \\  DCPC & EMNIST & \(160.8 0.05\) & \(3.3 10^{-6} 3.5 10^{-9}\) \\ DCPC & Fashion MNIST & \(284.1 0.05\) & \(0.03 2.7 10^{-5}\) \\  

Table 2: Negative log-likelihood and mean squared error for MCPC against DCPC on held-out images from the MNISTs. Means and standard deviations are taken across five random seeds.

Figure 3: **Top**: images from validation sets of MNIST (left), EMNIST (middle), and Fashion MNIST (right). **Bottom**: reconstructions by deep latent Gaussian models trained with DCPC for MNIST (left), EMNIST (middle), and Fashion MNIST (right), averaging over \(K=4\) particles. DCPC achieves quality reconstructions by inference over \(\) without training an inference network.

Figure 2: Hierarchical graphical model for DLGM’s.

While MCPC sends prediction errors up through a hierarchical model, LPC computed as its prediction error the log-joint gradient for all latent variables in the generative model. This meant that biological plausibility, and their goal of amortizing predictive coding inference, restricted them to single-level decoder adapted from Higgins et al. (2017). We evaluated with their discretized Gaussian likelihood, taken from Cheng et al. (2020), Ho et al. (2020), learning the variance as in Rybkin et al. (2021).

We compare DCPC to LPC using the Frechet Inception Distance (FID) (Seitzer, 2020) featured in Zahid et al. (2024), holding constant the prior, neural network architecture, learning rate on \(\), and number of gradient evaluations used to train the parameters \(\) and latents \(\). Zahid et al. (2024) evaluated a variety of scenarios and reported that their training could converge quickly when counted in epochs, but they accumulated gradients of \(\) over inference steps. We compare to the results they report after \(15\) epochs with \(300\) inference steps applied to latents initialized from the prior, equivalent to \(15 300=4500\) gradient steps on \(\) per batch, replicating their batch size of \(64\). Since Algorithm 1 updates \(\) only in its outer loop, we set \(S=30\) and ran DCPC for \(150\) epochs, with \(=0.001\) and \(K=16\). Table 3 shows that DCPC outperforms LPC in apples-to-apples generative quality, though not to the point of matching other model architectures2 by inference quality alone.

Figure 4 shows reconstructed images from the validation set (left) and samples from the posterior predictive generative model (right). There is blurriness in the reconstructions, as often occurs with variational autoencoders, but DCPC training allows the network to capture background color, hair color, direction in which a face is looking, and other visual properties. Figure 3(a) shows reconstructions over the validation set, while Figure 3(b) shows samples from the predictive distribution.

Kuntz et al. (2023) also reported an experiment on CelebA in terms of FID score, at the lower \(32 32\) resolution. Since they provided both source code and an exact mathematical description, we were able to run an exact, head-to-head comparison with PGD. The line in Table 3 evaluating DCPC with PGD's example neural architecture at the \(32 32\) resolution (with similar particle count and learning rate) demonstrates a significant improvement in FID for DCPC, alongside reduced FID variance.

Figure 4: **Left**: reconstructions from the CelebA validation set. **Right**: samples from the generative model. DCPC achieves quality reconstructions by inference over \(\) with \(K=16\) particles and no inference network, while the learned generative model captures variation in the data.

 Algorithm & Likelihood & Resolution \(\) & \(S\) Epochs \(\) & FID \(\) \\  PGD & \(\) & \(32 32\) & \(1 100\) & \(100 2.7\) \\ DCPC (ours) & \(\) & \(32 32\) & \(1 100\) & \(\) \\  LPC & \(\) & \(64 64\) & \(300 15=4500\) & \(120\) (approximate) \\ VAE & \(\) & \(64 64\) & \(1 4500=4500\) & \(86.3 0.03\) \\ DCPC (ours) & \(\) & \(64 64\) & \(30 150=4500\) & \(\) \\ 

Table 3: FID score comparisons on the CelebA dataset (Liu et al., 2015). The score for LPC comes from Figure 2 in Zahid et al. (2024), where they ablated warm-starts and initialized from the prior.

Necessary Compute ResourcesThe initial DLGM experiments on the MNIST-alike datasets were performed on a desktop workstation with 128GB of RAM and an NVIDIA Quadro P4000 with 8GB of VRAM. Experiments on CelebA were conducted on an NVIDIA DGX equipped with eight (8) NVIDIA A100's, each with 80GB of VRAM. The latter compute infrastructure was also used for unpublished experiments, on several different datasets, in structured time-series modeling.

## 6 Related Work

Pinchetti et al. (2022) expanded predictive coding beyond Gaussian generative models for the first time, applying the resulting algorithm to train variational autoencoders by variational inference and transformer architectures by maximum likelihood. DCPC, in turn, broadens predictive coding to target arbitrary probabilistic graphical models, following the broadening in Salvatori et al. (2022) to arbitrary deterministic computation graphs. DCPC follows on incremental predictive coding Salvatori et al. (2024) in quickly alternating between updates to random variables and model parameters, giving an incremental EM algorithm Neal and Hinton (1998). Finally, Zahid et al. (2024) and Oliviers et al. (2024) also recognized the analogy between predictive coding's prediction errors and the score functions used in Langevin dynamics for continuous random variables.

There exists a large body of work on how neurobiologically plausible circuits could implement probabilistic inference. Classic work by Shi and Griffiths (2009) provided a biologically plausible implementation of hierarchical inference via importance sampling; DCPC proceeds from importance sampling as a foundation, while parameterizing the proposal distribution via prediction errors. Recent work by Fang et al. (2022) studied neurally plausible algorithms for sampling-based inference with Langevin dynamics, though only for a Gaussian generative model of sparse coding. Golkar et al. (2022) imposed a whitening constraint on a Gaussian generative model for biological plausibility. Finally, Dong and Wu (2023) and Zahid et al. (2024) both suggest mechanisms for employing momentum to reduce gradient noise in a biologically plausible sampling algorithm; the former intriguingly analogize their momentum term to neuronal adaptation. To conclude, other works have already implemented predictive coding models for image generation tasks, a notable example being the neural generative coding framework Ororbia and Kifer (2022), Ororbia and Mali (2022).

## 7 Conclusion

This paper proposed divide-and-conquer predictive coding (DCPC), an algorithm that efficiently and scalably approximates Gibbs samplers by importance sampling; DCPC parameterizes efficient proposals for a model's complete conditional densities using local prediction errors. Section 4 showed how Monte Carlo sampling can implement a form of "prospective configuration" Song et al. (2024), first inferring a sample from the joint posterior density (Theorem 1) and then updating the generative model without a global backpropagation pass ( Theorem 2). Experiments in Section 5 showed that DCPC outperforms the state of the art Monte Carlo Predictive Coding from computational neuroscience, head-to-head, on the simple generative models typically considered in theoretical neuroscience; DCPC also outperforms the particle gradient descent algorithm of Kuntz et al. (2023) while under the constraint of purely local computation. DCPC's Langevin proposals admit the same extension to constrained sample spaces as applied in Hamiltonian Monte Carlo Brubaker et al. (2012); our Pyro implementation includes this extension via Pyro's preexisting support for HMC.

DCPC offers a number of ways forward. Particularly, this paper employed naive Langevin proposals, while Dong and Wu (2023), Zahid et al. (2024) applied momentum-based preconditioning to take advantage of the target's geometry. Yin and Ao (2006) demonstrated that gradient flows of this general kind can also provide more efficient samplers by breaking the detailed-balance condition necessary for the Metropolis-Hastings algorithm, motivating the choice of SMC over MCMC to correct proposal bias. Appendix C derives a mathematical background for an extension of DCPC to discrete random variables. Future work could follow Marino et al. (2018), Taniguchi et al. (2022) in using a neural network to iteratively map from particles and prediction errors to proposal parameters.

### Limitations

DCPC's main limitations are its longer training time, and greater sensitivity to learning rates, than state-of-the-art amortized variational inference trained end-to-end. Such limitations occur frequently in the literature on neuroscience-inspired learning algorithms, as well as in the literature on particle-based algorithms with no parametric form. This work has no singular ethical concerns specific only to DCPC, rather than the broader implications and responsibilities accompanying advancements in biologically plausible learning and Bayesian inference.