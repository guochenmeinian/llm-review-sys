# Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities

Dongrui Liu\({}^{a}\)   Huiqi Deng\({}^{a}\)   Xu Cheng\({}^{a}\)   Qihan Ren\({}^{a}\)   Kangrui Wang\({}^{b}\)

**Quanshi Zhang\({}^{a}\)**

Shanghai Jiao Tong University  \({}^{b}\)University of Chicago

Equal contributionThis research is done under the supervision of Dr. Quanshi Zhang. He is with the Department of Computer Science and Engineering, the John Hopcroft Center at the Shanghai Jiao Tong University, China. Correspondence to: Quanshi Zhang <zqs1022@sjtu.edu.cn>.

###### Abstract

This paper theoretically explains the intuition that simple concepts are more likely to be learned by deep neural networks (DNNs) than complex concepts. In fact, recent studies have observed  and proved  the emergence of interactive concepts in a DNN, _i.e._, it is proven that a DNN usually only encodes a small number of interactive concepts, and can be considered to use their interaction effects to compute the inference score. Each interactive concept is encoded by the DNN to represent the collaboration between a set of input variables. Therefore, in this study, we aim to theoretically explain that interactive concepts involving more input variables (_i.e._, more complex concepts) are more difficult to learn. Our finding clarifies the exact conceptual complexity that boosts the learning difficulty.

## 1 Introduction

Deep neural networks (DNNs) have exhibited superior performance in various tasks, but the reason for their superior performance remains an open problem. To this end, many attempts have been made to explain the representation capacity of DNNs from different perspectives. For example, Montufar et al.  used the number of linear response regions in a DNN to evaluate the expressive power of the DNN. Dinh et al.  and Petzka et al.  used the flatness of loss functions at minima to explain the generalization power.

In this paper, we explore a fundamental yet not well-formulated problem in terms of the representation capacity of DNNs, _i.e._, what types of concepts are easier to be learned by DNNs. However, the core challenge of this problem is that researchers have not reached a consensus on how to define a concept encoded by DNNs. Therefore, previous findings  that _DNNs easily learn simple concepts_ remain intuition or empirical observations, without a clear theoretical formulation and explanation.

Fortunately, Ren et al. , Li and Zhang  have observed and Ren et al.  have, for the first time, mathematically proven that the emergence of interactive concepts is a common phenomenon shared by different DNNs. _I.e._, **it is proved that under some common conditions, a well-trained DNN will encode just a small number of interactive concepts for inference.** Specifically, the interactive concept encoded by a DNN is defined as a Harsanyi interaction  in game theory, which represents an AND relationship between input variables in a specific set \(S\). For example, as Figure 1(a) shows, the DNN encodes the co-appearance relationship between input variables (image patches) \(x_{1}\), \(x_{2}\), and \(x_{3}\) to form the _mouth concept_\(S=\{x_{1},x_{2},x_{3}\}\). Only when all three patches are all present, the mouth concept is triggered, and adds a numerical effect \(I(S)\) to the inference score \(y\). The masking of any patches (_e.g._, \(x_{1}\)) will break this AND/co-appearance relationship of the mouthconcept, and removes the numerical effect \(I(S)\), _i.e._, making \(I(S)=0\). More crucially, it proved in  that **the network output \(y\) on each input sample can be represented as the sum of numerical effects of such a few interactive concepts**, \(y=_{S}I(S)\).

The mathematical proof in  makes the interactive concept the **first concept whose emergence in DNNs is certificated**. Furthermore, Li and Zhang  have discovered that the interactive concepts **have considerable transferability over different samples and strong discrimination power for classification.** In this way, we can roughly consider such interactive concepts a rigorous definition of concepts encoded by DNNs.

**Explaining the difficulty of learning complex interactive concepts.** Therefore, in this study, given the above interactive concepts, we aim to further explore the learning difficulty of these concepts, and explain that a DNN is more likely to encode simple interactive concepts. Here, we use the number of variables in an interactive concept \(S\) to represent the complexity of the concept, which is also termed _the order_ of the concept. Thus, low-order interactive concepts usually represent simple AND interactions among a few input variables.

Specifically, we find that low-order interactive concepts encoded by DNNs are generally more stable to inevitable noises in data. In this way, low-order interactive concepts usually exhibit consistently positive (or negative) effects on the inference score of different samples in the same category. In comparison, a high-order interactive concept is more likely to have significantly diverse effects on inference scores of different samples. This explains the reason why DNNs easily learn simple interactive concepts.

In addition, we discover that our research may provide a new perspective to explain previous findings/understandings of the conceptual complexity [5; 34; 63; 28]. Besides, the conceptual complexity can directly explain the adversarial robustness of a DNN. Thus, our study is of considerable value in explaining the representation capacity of a DNN.

**Interactive concepts vs. cognitive concepts.** Although the Harsanyi interactive concepts visualized by  seem partially aligned with cognitive concepts to some extent, we do not think such interactive concepts fit humans' cognition [24; 53]. For example, the recognition of a _big ball concept_ and that of a _small ball concept_ have almost the same cognitive complexity for humans. In comparison, it is more difficult for the DNN to recognize a large ball, because it has to use a high-order interaction that contains a lot of pixels to examine whether all pixels within the ball have the same color. Thus, this study clarifies the exact formulation for the conceptual complexity on interactive concepts that boosts the learning difficulty, which is different from the cognitive difficulty.

## 2 Explainable AI (XAI) theories based on game-theoretic interactions

Explaining DNNs based on a system of game-theoretic interactions becomes an emerging direction in recent years. Specifically, such a system aims to tackle the following challenges in XAI.

Figure 1: (a) Interactive concepts encoded by a DNN. Each interactive concept \(S\) represents an AND relationship between input variables (image patches) in \(S\). Masking any input variables in \(S\) will deactivate the concept \(S\). (b) Extensive experiments illustrate the common concept-emergence phenomenon. Various DNNs all encode very sparse interactive concepts for inference. Only a small number of interactions have salient effects \(I(S)\) and are termed interactive concepts. Other interactions _w.r.t._ other \(S N\) all have almost zero effect \(I(S) 0\), thus being termed noisy patterns. For clarity, we sort the strength of interaction effects in a descending order.

\(\)_Defining, extracting, and quantifying interactive concepts encoded by DNNs._ Quantifying the interactions between different input variables [57; 58] is a new perspective to formulate the knowledge encoded by a DNN. Based on game theory, Zhang et al. [66; 68; 67] introduced multi-variate interaction and multi-order interaction to quantify the interactions encoded by the DNN. Ren et al.  empirically observed and Ren et al.  further proved that a DNN usually just encoded a small number of salient interactions. Ren et al.  defined the optimal baseline value for computing the Shapley value based on these interactions. Li and Zhang  found that salient interactions usually have high transferability over different samples and DNNs. These findings showed that these salient interactions could be viewed as interactive concepts encoded by a DNN.

\(\)_Using interactive concepts to explain/measure the representation power of DNNs._ The representation power of DNNs has been explained by game-theoretic multi-order interaction, including generalization power [67; 70], adversarial robustness [45; 70], and adversarial transferability . Besides, Cheng et al.  used interactions to investigate how shapes and textures were encoded in DNNs. Cheng et al.  found that salient interactions often represented prototypical patterns encoded by DNNs. Deng et al.  derived that the DNN was less likely to encode mid-order interactions. In comparison, Ren et al.  proved that the Bayesian neural network was less likely to encode high-order interactions, thereby obtaining good adversarial robustness.

\(\)_Unifying the common underlying mechanism shared by previous empirical findings._ Interactions have superior representation power, and we find that many previous studies can be re-explained from the perspective of interactions. Deng et al.  proved that fourteen attribution methods could be reformulated and considered as a certain allocation of interaction effects. Besides, Zhang et al.  proved that reducing the interactions was the common underlying mechanism shared by twelve previous studies for improving adversarial transferability.

## 3 Explaining simple interactive concepts are easy to learn

### Preliminaries: sparse interactive concepts emerge in DNNs

It is generally believed that the learning of DNNs is a fitting problem between the ground-truth label and the model output, instead of explicitly learning specific concepts like graphical models. However, Ren et al. , Li and Zhang  have empirically observed and Ren et al.  have mathematically proven the counter-intuitive emergence of concepts in DNNs, _i.e._, **it is proven that under a set of common conditions, a DNN encodes just**_a small number of_ **interactive concepts for inference.**

Specifically, given a pre-trained DNN \(v\) and an input sample \(=[x_{1},,x_{n}]\) with \(n\) variables indexed by \(N=\{1,,n\}\), \(v()\) denotes a scalar output of the DNN3. Each interaction in [45; 27; 47] is defined as Harsanyi dividend (or Harsanyi interaction)  in game theory, which represents a collaboration (AND relationship) between input variables in a specific set \(S\) (\(S N\)). For example, as Figure 1(a) shows, the co-appearance of image patches forms a mouth interaction \(S=\{x_{1},x_{2},x_{3}\}\). Only when these three patches are all present, the mouth interaction will be triggered, and make a certain interaction effect \(I(S)\) on the network output. The absence (masking) of any patches of \(x_{1}\), \(x_{2}\), and \(x_{3}\) will deactivate the mouth interaction and remove the interaction effect, _i.e.,_\(I(S)=0\). The interaction effect \(I(S|)\) on the input sample \(\) is computed as follows.

\[I(S|)=_{ S}(-1)^{|S|-|T|} v(_{T}),\] (1)

where \(_{T}\) denotes the masked input sample, when we mask input variables in \(N T\) and keep variables in \(T\) unchanged. Please see more properties of the Harsanyi dividend in the supplementary material.

Theoretically, there are \(2^{n}\) potential subsets \(S\) of input variables (\(S 2^{N}=\{S|S N\}\)). The proven concept-emergence phenomenon refers to that **only a small number of subsets**\(S_{} 2^{N}\) **make salient interaction effects**\(I(S|)\) **on the network output, and can be considered as**_interactive concepts._ Interaction effects of all other subsets are close to zero (\(I(S|) 0\)), which can be considered as ignorable _noisy patterns_. Therefore, **the network output**\(v()\) **can be well approximated by interaction effects of a small number of interactive concepts,**_i.e._,

\[v()=_{S N}I(S|)_{S _{}}I(S|)\] (2)We also conduct experiments to illustrate the concept-emergence phenomenon on various DNNs, including multi-layer perceptrons (MLPs), long short-term memory (LSTM), AlexNet , ResNet , convolutional neural networks (CNNs), and PointNet  trained on different types of data, including tabular data (UCI dataset ), natural language data (CoLA  and SST-2 ), image data (MNIST  and CelebA ), and point cloud data (ShapeNet ). We follow experimental settings in [45; 27], and Figure 1(b) shows that interactive concepts encoded by a DNN are usually indeed sparse.

**Theorem 1**.: _Given an input sample \(^{n}\) with \(n\) variables, there are \(2^{n}\) different masked samples \(_{T}\) w.r.t. all potential subsets \(T N\). Then,  has proven that_

\[\,T N,\;v(_{T})=_{S T}I(S| )_{S_{}k S T}I(S|).\] (3)

Theorem 1 means that we can use a small number of interactive concepts in \(_{}\) to well approximate network outputs on anyone \(_{T}\) of the \(2^{n}\) masked samples.

**Trustworthiness of interactive concepts.** The mathematical proof in  makes \(I(S)\) become the first concepts whose emergence in DNNs is certificated. Equations (2) and (3) further guarantee that interactive concepts can faithfully explain the output of DNNs. In addition, Li and Zhang  have discovered that (i) interactive concepts are transferable across different samples; (ii) interactive concepts are discriminative, _i.e._, if a concept \(S\) has salient interaction effects \(I(S)\) on a set of samples, then the concept tends to push these samples to be classified towards the same category. In this way, we can roughly consider such interactive concepts a relatively rigorous definition of concepts.

**Complexity (order) of interactive concepts.** The complexity of an interactive concept \(S\) is defined as the number of input variables involved in \(S\), which is also termed _the order_ of the interactive concept, _i.e._, \((S)=(S)=|S|\). Thus, low-order interactive concepts usually represent simple AND relationships among a few input variables. In comparison, high-order interactive concepts often refer to as relatively complex AND relationships among a large number of input variables.

### Simple interactive concepts in data are more stable and easier to learn

Unlike other interaction metrics [16; 32; 57], sparse interactive concepts are certificated to emerge in DNN, and can faithfully represent the inference score of DNNs. Therefore, in this study, we aim to further explore the learning difficulty of these certificated concepts. Specifically, we theoretically explain that a DNN is more likely to encode simple (_i.e._, low-order) interactive concepts.

In this subsection, we show that low-order interactive concepts encoded by DNNs are generally more stable to inevitable noises in data, compared to high-order interactive concepts. Specifically, we derive an approximate analytical solution to the variance (instability) of interactive concepts' effects _w.r.t._ data noise, and show that the variance (instability) increases along the order of concepts in an exponential manner.

**Theorem 2** (proven in the supplementary material).: _Given a neural network \(v\) and an arbitrary input sample \(^{}^{n}\), the network output can be decomposed by using the Taylor expansion i.e., \(v(^{})=_{S N}_{_{S}}U_{S, {}} J(S,|^{})\). In this way, according to Equation (1), the Harsanyi interaction effect \(I(S|^{})\) on the sample \(^{}\) can be reformulated as follows._

\[I(S|^{})=_{ Q_{S}}U_{S,} J(S,|^{}).\] (4)

_Here, \(J(S,|^{})\!\!=\!_{i S}((x ^{}_{i}-b_{i})_{i}-b_{i}}{})^{_{i}}\) denotes a Taylor expansion term of the degree \(\), where the degree \( Q_{S}=\{[_{1},,_{n}]| i S,_{i}^{+}; i S,_{i}=0\}\) and \(b_{i}\) is the baseline value to mask the input variable \(x_{i}\). In addition, \(U_{S,}\!\!=\!\!}{_{i=1}^{m}_{i}^{i}}v(^{}_{0})}{ x^{_{1}}_{1} x^{ _{n}}_{n}}_{i S}[(x^{}_{i}-b_{i})]^{ _{i}}\), where \(^{}_{0}\) denotes the sample whose input variables are all masked. \(m=_{i=1}^{m}_{i}\)._

Theorem 2 rewrites the Harsanyi interaction effect \(I(S|)\) of each concept from a new perspective, to facilitate the analysis of the instability of interactive concepts. The derivation of Theorem 2 is inspired by [12; 70; 46], in which the Harsanyi interaction effect is re-written as the sum of the Taylor interaction effects. Therefore, in this study, we define a new baseline value \(b_{i}\) to further extend the finding in  and obtain Theorem 2, which significantly simplifies the further proof. Specifically, the new baseline value \(b_{i}\) is set as follows to represent the masked state of the \(i\)-th input variable. _i.e._,\(x_{i} b_{i}\). Previous studies usually set \(b_{i}=_{i}=_{}[x_{i}]\) as the average value over different samples to represent the masked state . However, we consider that pushing the input variable \(x_{i}\) to move a large distance \(\) towards \(_{i}\) has been significant enough to remove its information. To this end, we set \(b_{i}=x_{i}-^{4}\), if \(x_{i}>_{i}\); else, \(b_{i}=x_{i}+^{4}\). The above new setting ensures comparable perturbation magnitudes over different dimensions.

Next, we analyze the variance (instability) of the interaction effect \(I(S|^{}=+)\) when we add a Gaussian perturbation \((,^{2})\) to the input sample \(\). Here, the Gaussian perturbation \(^{n}\) is considered as a rough representation of inevitable variations in data, _e.g._, shape deformation and object rotation variations in image classification. However, such variations are quite difficult to formulate, so we use a Gaussian perturbation as a rough representation. Fortunately, we find that our reformulation of interactions in Theorem 2 enables us to directly apply the finding in  to prove the variance (instability) of the interaction effect \(I(S|^{}=+)\).

**Theorem 3**.: _Let us add a Gaussian perturbation \((,^{2})\) to the input sample \(\). Let us first consider the case with the lowest degree \(}=[_{1},,_{n}] Q_{S}\), satisfying that \( i S,_{i}=1; i S,_{i}=0\). The mean and variance of \(J(S,}|+)\) over the Gaussian perturbation \(\) are given as_

\[_{}[J(S,}|+)]=1, _{}[J(S,}|+)]=( 1+()^{2})^{|S|}-1.\] (5)

_Furthermore, for the more general case with an arbitrary degree \( Q_{S}=\{[_{1},,_{n}]| i S,_{i}^{+}; i S,_{i}=0\}\), the mean and variance of \(J(S,|+)\) are computed as_

\[_{}[J(S,|+)]=_{ }[_{i S}(1+}{})^{_{i}} ],\ _{}[J(S,|+)]=_{}[_{i S}(1+}{})^{_{i}}].\] (6)

Theorem 3 shows the (variance) instability of interactive concepts of different complexities (orders) _w.r.t._ data variations. It indicates that the variance of \(J(S,|+)\) roughly increases along with the order\((S)=|S|\) in an exponential manner. Similar conclusions are also introduced in . Furthermore, according to Equation (4), the interaction effect \(I(S|+)\) of the concept \(S\) can be represented as the weighted sum of \(J(S,|+)\), and coefficients \(U_{S,}\)_w.r.t._ different orders \(s\) and degrees \(\) are usually chaotic. Hence, _we can roughly consider that the variance (instability) of the interaction effect \(I(S|+)\) increases along with the order \(|S|\) exponentially, as well._ In other words, **compared to high-order concepts, low-order concepts are much more stable to slight input perturbations.**

**Experimental verification.** Here, we conduct experiments to verify whether the variance (instability) of interaction effects indeed increased along with the order in an approximately exponential manner. Specifically, to mimic variations in the data, we add Gaussian perturbations \((,0.02^{2})\) to each training sample. Then, we compute the average mean \(E^{(s)}\)=\(_{}[_{S_{i}|S|=s}[ _{}[I(S|+)]]]\) and the average variance \(V^{(s)}\)=\(_{}[_{S_{i}|S|=s}[ _{}[I(S|+)]]]\) of interaction effects of the \(s\)-th order concepts _w.r.t._ Gaussian perturbations. For verification, we calculate such metrics on DNNs trained for image classification and DNNs trained on tabular data. We train AlexNet , VGG-11 , ResNet-18/20  on the CIFAR-10 dataset  and the Tiny-ImageNet dataset , respectively. We also train a five-layer MLP  on the UCI census dataset (namely _census dataset_) and the UCI TV news dataset (namely _TV news dataset_) , respectively. In addition, considering the computational cost of \(I(S|+)\) in Equation (1) is intolerable if each pixel is considered as an input variable, we divide images into 8\(\)8 patches  to calculate \(I(S|+)\). Please see the supplementary material for more implementation details.

Figure 2 shows that the variance (instability) \(V^{(s)}\) of interaction effects increases in an exponential manner along with the order \(s\) of the interactive concept. In addition, the relative stability \(E^{(s)}/}\) of interaction effects decreases significantly along with the order \(s\). Such phenomenon successfully verifies findings in Theorem 3.

#### 3.2.1 Formulating the conceptual learning as a linear regression problem

Note that, Li and Zhang  have discovered that interactive concepts have high transferability over samples in the same category, _i.e._, most interactive concepts extracted from different samples may all belong to the same subset \(_{c}_{}\). Therefore, we can consider the interactive concepts as common knowledge learned by the DNN, and we aim to analyze the difficulty of learning each interactive concept with different complexity.

To facilitate the analysis, we first simplify the conceptual learning as a linear regression problem. Specifically, we first rewrite the interaction effect of an interactive concept \(S\). Given an input sample \(\), according to Equation (4), the interaction effect of the concept \(S\) on the sample \(^{}\) (obtained by applying some transformations on \(\)), \(I(S|^{})\), can be rewritten as

\[I(S|^{})=U_{S} C_{S}(^{}),\] (7)

where the constant \(U_{S}=I(S|)\) denotes the interaction effect on the sample \(\), and the function for the triggering state on the transformed sample \(^{}\) is given as \(C_{S}(^{})=_{ Q_{S}}U_{S,}J(S,|^{})/U_{S}\).

**Theorem 4**.: _Given an arbitrarily masked sample \(_{T}(T N)\), the function \(C_{S}(_{T})\) defined above can be computed as the binary triggering state of the concept \(S\) in the sample \(_{T}\)._

\[\;T N,\;C_{S}(_{T})=_{i S}A_{i}(_{T})= (S T),\] (8)

_where \(A_{i}(_{T})\{0,1\}\) denotes whether the variable \(x_{i}\) is present \(A_{i}(_{T})=1\) or being masked \(A_{i}(_{T})=0\) in the sample \(_{T}\)._

The function \(C_{S}(_{T})\) represents the triggering state of the concept \(S\) under an arbitrarily masking condition \(\;T N\). Only when all variables in \(S\) are present under the masking condition \(T\), the concept \(S\) is triggered \(C_{S}(_{T})=1\). If any of variables in \(S\) is masked, then the concept \(S\) will not be triggered \(C_{S}(_{T})=0\), yielding zero interaction effect \(I(S|_{T})=0\). Theorem 4 shows that given a masked sample \(_{T}\), the concept \(S\) is only triggered when the sample \(_{T}\) contains all variables in \(S\), _i.e._, \(T S\).

Thus, inspired by , we can extend the conclusion to a continuous version that explains the output of the DNN as a **linear regression of very few salient interactive concepts** based on Equation (3).

\[v(^{})=_{S N}U_{S} C_{S}(^{ })_{S_{}}U_{S} C_{S}( ^{}),\] (9)

where the triggering state \(C_{S}(^{})\) of each interactive concept \(S\) can be considered as an input dimension of the linear function, which reflects whether the input sample \(^{}\) contains the concept \(S\).

Therefore, the absolute value of the weight coefficient \(U_{S}\) in Equation (9) can be viewed as _the strength of the DNN encoding the interactive concept \(S\)_. Considering the sparsity of interactive concepts discussed in Section 3.1, most interactive concepts have ignorable coefficients \(U_{S} 0\), and not so many concepts \(S\) have large absolute value of \(|U_{S}|\). Thus, we can consider that the DNN only learns a small number of salient interactive concepts.

#### 3.2.2 Explaining the learning difficulty of concepts

Equation (9) in the previous subsection enables us to understand a DNN for the classification task as a pseudo-linear function. Then, if an input feature dimension has a stable value (_i.e._, the triggering state \(C_{S}\) of an interactive concept \(S\) stably being present/absent) across all samples in the same category, then we consider this feature dimension (_i.e.,_ the concept) is easy to learn. In comparison, when we extract the same concept from different samples in the same category, if this concept exhibits inconsistent interaction effects (_e.g._, such a concept does not consistently present or absent over different samples), then this feature dimension (_i.e.,_ the concept) is hard to learn.

Figure 2: The logarithm of the variance of interaction effects \([V^{(s)}]\) and the stability of interaction effects (measured by \(E^{(s)}/}\)). The variance \(V^{(s)}\) of interaction effects increases along with the order of interactive concepts exponentially. The stability of effects decreases along with the order.

To analyze the consistency/stability of the concept _w.r.t_ data variations, we use small perturbations \(\) to represent various inevitable variations in the data, such as shape deformation and object rotation variations. Theorem 3 shows that high-order interactive concepts are much more unstable to inevitable variations in the data than low-order interactive concepts. This makes high-order concepts more likely to be influenced by data variations and less likely to be consistently present or absent in samples of the same category, which boosts the learning difficulty.

**Experimental verification.** We conduct experiments to verify the claim that high-order interactive concepts are less stably extracted under data variations than low-order interaction. We use the following two metrics to evaluate the each interactive concept \(S\) (_i.e.,_ each single input dimension of the above linear function) of a certain order. Specifically, we use the first metric \((S)=_{c}[[_{_{}}[I(S|)] ]/[_{_{}}[I(S|)]]\) to measure the relative consistency of the interactive concept appearing over different input samples in a certain category \(c\). Here, \(_{c}\) denotes a set of training samples belonging to the category \(c\), and \(_{_{}}[I(S|)]\) indicates the standard deviation of \(I(S|)\) over different input samples. A large \((S)\) value means that the interactive concept \(S\) in all samples of the category \(c\) has similar/consistent effects \(I(S|)\). It is easier for a DNN to learn such a consistent concept \(S\).

In addition, we use another metric \((S)\) to verify whether the interaction effect of the high-order interactive concept is usually less stably extracted than those of the low-order interactive concept. To this end, the metric \((S)=\)\(_{}[_{}[|I(S|+) -I(S|)|]]/_{}[|I(S|)|]=_{}[_{}[|C_{S}(+)-C_{S}( )|]]/_{}[|C_{S}()|]\) measures the relative instability of the interactive concept \(S\) to the inevitable slight variations \(\) in the data. Here, \(C_{S}(+)=I(S|+)/U_{S}\) computed in Equation (7) denotes the trigger state of the interactive concept \(S\) on the sample \(^{}=+\).

To this end, we use DNNs and experimental settings in the _experimental verification_ paragraph of Section 3.2 for evaluation. Figure 3 and Figure 4 show the change of the average consistency \((S)\) and the average instability \((S)\) of \(s\)-order interactive concepts through the learning process, respectively. At each training epoch, low-order concepts usually obtain higher consistency \((S)\) and

Figure 4: Instability \((S)\) of \(s\)-order interactive concepts to data variations. The curve shows the average instability \(_{S N,|S|=s}[(S)]\) over different interactive concepts of the \(s\)-th order, and the shade represents the standard deviation \(_{S N,|S|=s}[(S)]\). Effects of high-order concepts are more sensitive to data variations than those of low-order concepts.

Figure 3: Consistency \((S)\) of \(s\)-order interactive concepts. The curve shows the mean consistency \(_{S N,|S|=s}[(S)]\) over different interactive concepts of the \(s\)-th order, and the shade indicates the standard deviation \(_{S N,|S|=s}[(S)]\). Effects of low-order concepts are more consistent than those of high-order concepts.

lower instability \((S)\) than high-order concepts. It means that low-order concepts usually are more consistent and more stable. Thus, this experiment explains the reason why low-order interactive concepts are easier to learn.

#### 3.2.3 Fast learning of low-order concepts

In this subsection, we illustrate the phenomenon that low-order interactive concepts are usually learned faster than high-order concepts, as a support for the claim that low-order interactive concepts are easier to be learned. We have theoretically proven in Section 3.2 and experimentally verified that low-order interactive concepts are more consistently present or consistently absent in different samples of the same category, which makes low-order interactive concepts easier to be learned. Then, this experiment is conducted to check whether low-order concepts are really learned faster than high-order concepts.

Specifically, we examine whether interactive concepts encoded by the finally-learned DNN \(v_{}()\) have already been encoded by the DNN that is \(v_{t}()\) trained after \(t\) epochs. If so, we consider such interactive concepts are learned fast. Specifically, let \(_{t}^{(s)}=[I_{t}(S_{1}|),,I_{t}(S_{d}|)]^{} ^{d}\) denote a vector of interaction effects for all \(d=\) interactive concepts of \(s\)-order. Then, we compute the Jaccard similarity between \(s\)-order interactive concepts encoded by the DNN \(v_{}(x)\) and those encoded by the DNN \(v_{t}(x)\), _i.e._, \((_{t}^{(s)},_{}^{(s)})\!=\!(}_{t}^{(s)},}_{}^{(s)})\|_{1}/\|(}_{t}^{(s)},}_{}^{(s)})\|_{1}\) and \(\|\|_{1}\) denotes L1-norm. We extend the \(d\)-dimensional vector \(_{t}^{(s)}\) into a \(2d\)-dimensional vector with non-negative elements \(}_{t}^{(s)}=[(_{t}^{(s),+})^{},(_{t}^{(s),-})^{ }]^{}=[(_{t}^{(s)},0)^{},-(_{t}^{(s)},0)^{ }]^{}^{2d}\). Similarly, we compute \(}_{}^{(s)}\) with non-negative elements based on \(_{}^{(s)}\). In this way, a large similarity \((_{t}^{(s)},_{}^{(s)})\) at an earlier epoch \(t\) indicates that interactive concepts are easier to be learned.

To this end, let us use DNNs and experimental settings introduced in the _experimental verification_ paragraph of Section 3.2 for evaluation. Figure 5 shows that DNNs first learn low-order interactive concepts, and then learn high-order interactive concepts. Such a phenomenon verifies the conclusion that a DNN easily learns low-order (simple) interactive concepts.

## 4 Explaining findings in previous studies

### Explaining adversarial robustness

Previous study  has discovered that low-order interactions are more robust to adversarial attacks than high-order interactions. Notice that their high-order (low-order) interactions are different from, but highly related to our high-order (low-order) interactive concepts. Specifically, our interactive concepts can be explained as elementary components for such multi-order interactions in . Please see the supplementary material for the proof. _Therefore, from this perspective, our conclusion that "low-order interactive concepts are easy to learn" can also explain how a DNN encodes concepts of different adversarial robustness_.

Figure 5: The weighted Jaccard similarity \((_{t}^{(s)},_{}^{(s)})\) between \(s\)-order interactive concepts learned after the intermediate epoch \(_{t}^{(s)}\) and those learned after the final epoch \(_{}^{(s)}\). Low-order concepts usually have higher Jaccard similarity during the learning process, which indicates that DNNs first learn low-order concepts and then gradually learn more about high-order concepts.

**Can we use interactive concepts in this paper to verify the heuristic findings of adversarial robustness in ?** According to Equation (2), we can represent the inference logic of a DNN by a set of interactive concepts, _i.e.,_\(v()_{S_{}}I(S|)\). In this way, we conduct experiments to evaluate the sensitivity of each interactive concept \(S\) to adversarial perturbations. The symbolic conceptual representation of interactive concepts allows us to measure the adversarial robustness in a more direct way. That is, if the effect of an interactive concept does not significantly change under adversarial attacks, we consider this interactive concept is robust to adversarial attacks. In contrast, if the effect of an interactive concept changes significantly under adversarial attacks, we consider this interactive concept is sensitive to adversarial attacks.

To this end, we use the metric \((S)=_{}}[|I(S|+)-I(S| )|]\)\(/_{}|I(S|)|]\) to evaluate the sensitivity of the interactive concept \(S\) to adversarial perturbations, where \(\) denotes the adversarial perturbation generated by the \(_{}\) attack . In this way, a small \((S)\) value indicates that the interactive concept \(S\) is robust to the adversarial attack.

We follow experimental settings in the _experimental verification_ paragraph of Section 3.2 to evaluate different DNNs. Figure 6 shows that compared to high-order interactive concepts, low-order concepts usually obtain smaller \(A^{(s)}=_{S N,|S|=s}[(S)]\) values. Such phenomena demonstrate that low-order interactive concepts are more robust to adversarial attacks.

### Connections to existing findings on what are learned first by a DNN

In this subsection, we discuss some related studies on which kind of knowledge is usually first learned by a DNN. Most previous studies conducted experiments to explore the knowledge that was easier to be learned by a DNN, without providing much theoretical support. However, we find that our theorems can partially explain mechanisms behind some previous findings.

\(\) Arpit et al.  trained DNNs to classify both normal samples and white-noise samples to different object categories. In this way, they considered that the DNN encoded simple concepts to classify normal samples, but the DNN had to learn complex concepts to classify white noises to randomly-assigned labels. They observed that the DNN usually learned normal samples first, because the classification accuracy of normal samples increased before that of white noises. To this end, our research provides more insights into such an observation. Specifically, Cheng et al.  have proven that the classification of noisy data usually depends on high-order concepts, _i.e._, the classification of noisy data forces the DNN to memorize each specific white-noise sample as a specific high-order interactive concept. Let us combine this conclusion with our finding that high-order interactive concepts are hard to learn. Then, we can easily owe the slow learning of white-noise samples observed in  to the difficulty of learning high-order concepts.

\(\) Mangalam and Prabhu  considered/defined easy samples as training samples that could be correctly classified by shallow machine learning models, such as support vector machine (SVM) and random forests (RF). They discovered that DNNs first learned easy samples, and then gradually learned more about hard samples. To this end, our research verifies such observation. Specifically, we claim that easy samples mainly contain low-order interactive concepts. Thus, hard samples mainly contain high-order interactive concepts corresponding to complex interactions between numerous

Figure 6: Average adversarial sensitivity \(A^{(s)}\) of \(s\)-order interactive concepts to adversarial perturbations. Low-order interactive concepts are usually much less sensitive to adversarial attacks than high-order interactive concepts.

input variables, and thus are difficult to be classified by shallow models (_e.g._, the SVM and the RF). In this way, the fast learning of low-order concepts is another understanding of the finding in .

\(\) Xu et al.  discovered that during the training process, DNNs usually first learned samples of low frequencies (_e.g.,_ robust to noises), and then encoded samples of high frequencies (_e.g.,_ sensitive to noises). However, the original design of DNNs is not towards learning specific spectrums, and techniques of deep learning are not developed by assuming a periodic loss landscape of training samples. Therefore, we believe there should be a more direct explanation for the spectrum-learning phenomenon discovered by . To this end, our research explains this phenomenon as the difficulty of learning high-order concepts. According to Section 4.1, low-order concepts usually are less sensitive to input perturbations, thereby corresponding to low-frequency components defined in the loss landscape in . Accordingly, high-order concepts correspond to high-frequency components.

\(\) Liu et al.  discovered that during adversarial training, the training loss of the DNN trained on easy samples decreased faster than that of the DNN trained on hard samples. To this end, we consider easy samples in adversarial training mentioned by  may mainly contain low-order interactive concepts. It is because, as discussed in Section 4.1,  discovered that low-order interactions were robust to adversarial perturbations. Thus, the fast learning of easy samples in adversarial training can be roughly owing to the learning of low-order concepts.

## 5 Conclusion and discussion

In this paper, we theoretically explain the trend of DNNs learning simple concepts. Since the emergence of interactive concepts of DNNs has been observed [45; 27] and proved , we aim to theoretically explain the explicit theoretical connection between the conceptual complexity and the difficulty of learning concepts. In this way, we prove that low-order interactive concepts in the data are much more stable than high-order interactive concepts, which makes low-order interactive concepts more likely to be encoded. Besides, our research can also provide new insights into several previous empirical understandings [5; 34; 63; 28]_w.r.t._ the conceptual representation of DNNs.

There are very few ways to define and examine what is a "concept." In this paper, we only use the following three properties to support the faithfulness of using sparse salient interactions as concepts encoded by the DNN.

\(\) Ren et al.  have proved that a well-trained DNN can encode just a small number of salient interactions for inference under some common conditions.

\(\) Ren et al.  have proved that, such a small number of salient interactions extracted from a sample can well mimic DNN's outputs on numerous masked samples.

\(\) Li and Zhang  have discovered that salient interactions have considerable transferability and strong discrimination power.

However, the above properties cannot guarantee a clear correspondence between an interactive concept and a concept in human cognition. Up to now, we cannot mathematically formulate what is a concept in cognitive science. Thus, there is still a long way to unify the learning difficulty of a DNN and the cognitive difficulty of human beings.