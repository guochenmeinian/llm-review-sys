# DP-SGD Without Clipping:

The Lipschitz Neural Network Way

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) faces difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, our theoretical analysis of Lipschitz constrained networks reveals an unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters we guarantee DP training of these networks. This analysis not only allows the computation of the aforementioned sensitivities at scale but also provides leads on to how maximize the gradient-to-noise ratio for fixed privacy guarantees. To facilitate the application of Lipschitz networks and foster robust and certifiable learning under privacy guarantees, we provide a Python package that implements building blocks allowing the construction and private training of such networks.

## 1 Introduction

Machine learning relies more than ever on foundational models, and such practices raise questions about privacy. Differential privacy allows to develop methods for training models that preserve the privacy of individual data points in the training set. The field seeks to enable deep learning on sensitive data, while ensuring that models do not inadvertently memorize or reveal specific details about individual samples in their weights. This involves incorporating privacy-preserving mechanisms into the design of deep learning architectures and training algorithms, whose most popular example is Differentially Private Stochastic Gradient Descent (DP-SGD) . One main drawback of classical DP-SGD methods is that they require costly per-sample backward processing and gradient clipping. In this paper, we offer a new method that unlocks fast differentially private training through the use of Lipschitz constrained neural networks. Additionally, this method offers new opportunities for practitioners that wish to easily "DP-fy"  the training procedure of a deep neural network.

**Differential privacy fundamentals.** Informally, differential privacy is a _definition_ that quantifies how much the change of a single sample in a dataset affects the range of a stochastic function (here the DP training), called _mechanism_ in this context. This quantity can be bounded in an inequality involving two parameters \(\) and \(\). A mechanism fulfilling such inequality is said \((,)\)-DP (see Definition 1). This definition is universally accepted as a strong guarantee against privacy leakages under various scenarii, including data aggregation or post-processing . A popular rule of thumb suggests using \( 10\) and \(<\) with \(N\) the number of records  for mild guarantees. In practice, most classic algorithmic procedures (called _queries_ in this context) do not readily fulfill the definition for useful values of \((,)\), in particular the deterministic ones: randomization is mandatory. This randomizationcomes at the expense of "utility", i.e the usefulness of the output for downstream tasks . The goal is then to strike a balance between privacy and utility, ensuring that the released information remains useful and informative for the intended purpose while minimizing the risk of privacy breaches. The privacy/utility trade-off yields a Pareto front, materialized by plotting \(\) against a measurement of utility, such as validation accuracy for a classification task.

**Private gradient descent.** The SGD algorithm consists of a sequence of queries that (i) take the dataset in input, sample a minibatch from it, and return the gradient of the loss evaluated on the minibatch, before (ii) performing a descent step following the gradient direction. The sensitivity (see Definition 2) of SGD queries is proportional to the norm of the per-sample gradients. DP-SGD turns each query into a Gaussian mechanism by perturbing the gradients with a noise \(\). The upper bound on gradient norms is generally unknown in advance, which leads practitioners to clip it to \(C>0\), in order to bound the sensitivity manually. This is problematic for several reasons: **1.** Hyper-parameter search on the broad-range clipping value \(C\) is required to train models with good privacy/utility tradeoffs , **2.** The computation of per-sample gradients is expensive: DP-SGD is usually slower and consumes more memory than vanilla SGD, in particular for the large batch sizes often used in private training , **3.** Clipping the per-sample gradients biases their average . This is problematic as the average direction is mainly driven by misclassified examples, that carry the most useful information for future progress.

**An unexplored approach: Lipschitz constrained networks.** We propose to train neural networks for which the parameter-wise gradients are provably and analytically bounded during the whole training procedure, in order to get rid of the clipping process. This allows for rapid training of models without a need for tedious hyper-parameter optimization.

The main reason why this approach has not been experimented much in the past is that upper bounding the gradient of neural networks is often intractable. However, by leveraging the literature of Lipschitz constrained networks , we show that these networks allows to estimate their gradient bound. This yields tight bounds on the sensitivity of SGD steps, making their transformation into Gaussian mechanisms inexpensive - hence the name **Clipless DP-SGD**.

Informally, the Lipschitz constant quantifies the rate at which the function's output varies with respect to changes in its input. A Lipschitz constrained network is one in which its weights and activations are constrained such that it can only represent \(l\)-Lipschitz functions. In this work, we will focus our attention on feed-forward networks (refer to Definition 3). Note that the most common architectures, such as Convolutional Neural Networks (CNNs), Fully Connected Networks (FCNs), Residual Networks (ResNets), or patch-based classifiers (like MLP-Mixers), all fall under the category of feed-forward networks. We will also tackle the particular case of Gradient Norm Preserving (GNP) networks, a subset of Lipschitz networks that enjoy tighter bounds (see appendix).

Figure 1: **An example of usage of our framework, illustrating how to create a small Lipschitz VGG and how to train it under \((,)\)-DP guarantees while reporting \((,)\) values.**

Contributions

While the properties of Lipschitz constrained networks regarding their inputs are well explored, the properties with respect to its parameters remain non-trivial. This work provides a first step to fill this gap: our analysis shows that under appropriate architectural constraints, a \(l\)-Lipschitz network has a tractable, finite Lipschitz constant with respect to its parameters. We prove that this Lipschitz constant allows for easy estimation of the sensitivity of the gradient computation queries. The prerequisite and details of the method to compute the sensitivities are explained in Section 2.

Our contributions are the following:

1. We extend the field of applications of Lipschitz constrained neural networks. So far the literature focused on Lipschitzness with respect to the _inputs_: we extend the framework to **compute the Lipschitzness with respect to the parameters**. This is exposed in Section 2.
2. We propose a **general framework to handle layer gradient steps as Gaussian mechanisms** that depends on the loss and the model structure. Our framework covers widely used architectures, including VGG and ResNets.
3. We show that SGD training of deep neural networks can be achieved **without gradient clipping** using Lipschitz layers. This allows the use of larger networks and larger batch sizes, as illustrated by our experiments in Section 4.
4. We establish connections between **Gradient Norm Preserving** (GNP) networks and **improved privacy/utility trade-offs** (Section 3.1).
5. Finally, a **Python package1** companions the project, with pre-computed Lipschitz constant and noise for each layer type, ready to be forked on any problem of interest (Section 3.2).

### Differential Privacy and Lipschitz Networks

The definition of DP relies on the notion of neighboring datasets, i.e datasets that vary by at most one example. We highlight below the central tools related to the field, inspired from .

**Definition 1** (\((,)\)-Differential Privacy).: _A labeled dataset \(\) is a finite collection of input/label pairs \(=\{(x_{1},y_{1}),(x_{2},y_{2}),....(x_{N},y_{N})\}\). Two datasets \(\) and \(^{}\) are said to be neighboring for the "replace-one" relation if they differ by at most one sample: \(^{}=\{(x_{i}^{},y_{i}^{})\} \{(x_{i},y_{i})\}\). Let \(\) and \(\) be two non-negative scalars. A mechanism \(\) is \((,)\)-DP if for any two neighboring datasets \(\) and \(^{}\), and for any \(S()\):_

\[[() S] e^{}[ (^{}) S]+.\] (1)

A cookbook to create a \((,)\)-DP mechanism from a query is to compute its _sensitivity_\(\) (see Definition 2), and to perturb its output by adding a Gaussian noise of predefined variance \(^{2}=^{2}^{2}\), where the \((,)\)-DP guarantees depends on \(\). This yields what is called a _Gaussian mechanism_.

**Definition 2** (\(l_{2}\)-sensitivity).: _Let \(\) be a query mapping from the space of the datasets to \(^{p}\). Let \(\) be the set of all possible pairs of neighboring datasets \(,^{}\). The \(l_{2}\) sensitivity of \(\) is defined by:_

\[()=_{,^{}} (D)-(D^{})_{2}.\] (2)

**Differentially Private SGD.** The classical algorithm keeps track of \((,)\)-DP values with a _moments accountant_ which allows to keep track of privacy guarantees at each epoch, by composing different sub-mechanisms. For a dataset with \(N\) records and a batch size \(b\), it relies on two parameters: the sampling ratio \(p=\) and the "noise multiplier" \(\) defined as the ratio between effective noise strength \(\) and sensitivity \(\). Bounds on gradient norm can be turned into bounds on sensitivity of SGD queries. In "replace-one" policy for \((,)\)-DP accounting, if the gradients are bounded by \(K>0\), the sensitivity of the gradients averaged on a minibatch of size \(b\) is \(=2K/b\)..

Crucially, the algorithm requires a bound on \(_{}(,y)_{2} K\). The whole difficulty lies in bounding tightly this value in advance for neural networks. Currently, gradient clipping serves as a patch to circumvent the issue . Unfortunately, clipping individual gradients in the batch is costly and will bias the direction of their average, which may induce underfitting .

Lipschitz constrained networks.Our proposed solution comes from the observation that the norm of the gradient and the Lipschitz constant are two sides of the same coin. The function \(f:^{m}^{n}\) is said \(l\)-Lipschitz for \(l_{2}\) norm if for every \(x,y^{m}\) we have \(\|f(x)-f(y)\|_{2} l\|x-y\|_{2}\). Per Rademacher's theorem , its gradient is bounded: \(\|_{x}f\| l\). Reciprocally, continuous functions gradient bounded by \(l\) are \(l\)-Lipschitz.

In Lipschitz networks, the literature has predominantly concentrated on investigating the control of Lipschitzness with respect to the inputs (i.e bounding \(_{x}f\)), primarily motivated by concerns of robustness . However, in this work, we will demonstrate that it is also possible to control Lipschitzness with respect to parameters (i.e bounding \(_{}f\)), which is essential for ensuring privacy. Our first contribution will point out the tight link that exists between those two quantities.

**Definition 3** (Lipschitz feed-forward neural network).: _A feedforward neural network of depth \(D\), with input space \(^{n}\), output space \(^{K}\) (e.g logits), and parameter space \(^{p}\), is a parameterized function \(f:\) defined by the sequential composition of layers \(f_{d}\):_

\[f(,x):=(f_{D}(_{d}) f_{2}(_{2}) f _{1}(_{1}))(x).\] (3)

_The parameters of the layers are denoted by \(=(_{d})_{1 d D}\). For affine layers, it corresponds to bias and weight matrix \(_{d}=(W_{d},b_{d})\). For activation functions, there is no parameters: \(_{d}=\). Lipschitz networks are feed-forward networks, with the additionnal constraint that each layer \(x_{d} f_{d}(_{d},x_{d}):=y_{d}\) is \(l_{d}\)-Lipschitz for all \(_{d}\). Consequently, the function \(x f(,x)\) is \(l\)-Lipschitz with \(l=l_{1} l_{d}\) for all \(\)._

In practice, this is enforced by using activations with Lipschitz constant \(l_{d}\), and by applying a constraint \(:^{p}\) on the weights of affine layers. This corresponds to spectrally normalized matrices [12; 13], since for affine layers we have \(l_{d}=\|W_{d}\|_{2}:=_{\|x\| 1}\|W_{d}x\|_{2}\) hence \(=\{\|W_{d}\| l_{q}\}\).

The seminal work of  proved that universal approximation in the set of \(l\)-Lipschitz functions was achievable by this family of architectures. Concurrent approaches are based on regularization (like in [14; 15; 16]) but they fail to produce formal guarantees. While they have primarily been studied in the context of adversarial robustness [11; 17], recent works have revealed additional properties of these networks, such as improved generalization [13; 18]. However, the properties of their parameter gradient \(_{}f(,x)\) remain largely unexplored.

## 2 Clipless DP-SGD with \(l\)-Lipschitz networks

Our framework consists of **1.** a method that computes the maximum gradient norm of a network with respect to its parameters to obtain a _per-layer_ sensitivity \(_{d}\), **2.** a moments accountant that relies on the per-layer sensitivities to compute \((,)\)-DP guarantees. The method 1. is based on the recursive formulation of the chain rule involved in backpropagation, while 2. keeps track of \((,)\)-DP values with RDP accounting. It requires some natural assumptions that we highlight below.

**Requirement 1** (Lipschitz loss.).: _The loss function \((,y)\) must be \(L\)-Lipschitz with respect to the logits \(\) for all ground truths \(y\). This is notably the case of Categorical Softmax-Crossentropy._

The Lipschitz constants of common classification losses can be found in the appendix.

**Requirement 2** (Bounded input).: _There exists \(X_{0}>0\) such that for all \(x\) we have \(\|x\| X_{0}\)._

While there exist numerous approaches for the parametrization of Lipschitz networks (e.g differentiable re-parametrization [19; 8], optimization over matrix manifolds  or projections ), our framework only provides sensitivity bounds for projection-based algorithms (see appendix).

**Requirement 3** (Lipschitz projection).: _The Lipschitz constraints must be enforced with a projection operator \(:^{p}\). This corresponds to Tensorflow  constraints and Pytorch  hooks. Projection is a post-processing of private gradients: it induces no privacy leakage ._

To compute the per-layer sensitivities, our framework mimics the backpropagation algorithm, where _Vector-Jacobian_ products (VJP) are replaced by _Scalar-Scalar_ products of element-wise bounds. For an arbitrary layer \(x_{d} f_{d}(_{d},x_{d}):=y_{d}\) the operation is sketched below:

\[_{x_{d}}:=(_{y_{d}})}{  x_{d}}}\|_{2} \|_{y_{d}}\|_{2}}{  x_{d}}}_{}.\] (4)The notation \(\|\|_{2}\) must be understood as the spectral norm for Jacobian matrices, and the Euclidean norm for gradient vectors. The scalar-scalar product is inexpensive. For Lipschitz layers the spectral norm of the Jacobian \(\|\|\) is kept constant during training with projection operator \(\). The bound of the gradient with respect to the parameters then takes a simple form:

\[\|_{_{d}}\|_{2}=\|_{y_{d}}\|_{2} \|}{_{d}}\|_{2}.\] (5)

Once again the operation is inexpensive. The upper bound \(\|\|_{2}\) typically depends on the supremum of \(\|x_{d}\|_{2}\), that can also be analytically bounded, as exposed in the following section.

### Backpropagation for bounds

The pseudo-code of **Clipless DP-SGD** is sketched in Algorithm 2. The algorithm avoids clipping by computing a _per-layer_ bound on the element-wise gradient norm. The computation of this _per-layer_ bound is described by Algorithm 1 (graphically explained in Figure 2). Crucially, it requires to compute the spectral norm of the Jacobian of each layer with respect to input and parameters.

**Input bound propagation (line 2).** We compute \(X_{d}=_{\|x\| X_{d-1}}\|f_{d}(x)\|_{2}\). For activation functions it depends on their range. For linear layers, it depends on the spectral norm of the operator itself. This quantity can be computed with SVD or Power Iteration , and constrained during training using projection operator \(\). In particular, it covers the case of convolutions, for which tight bounds are known . For affine layers, it additionally depends on the amplitude of the bias \(\|b_{d}\|\).

**Remark 1** (Tighter bounds in literature.).: _Although libraries such as Decomon  or auto-LiRPA  provide tighter bounds \(X_{d}\) via linear relaxations , our approach is capable of delivering practically tighter bounds than worst-case scenarios thanks to the projection operator \(\), while also being significantly less computationally expensive. Moreover, hybridizing our method with scalable certification methods can be a path for future extensions._

**Computing maximum gradient norm (line 6).** We bound the Jacobian \((_{d},x)}{_{d}}\). In neural networks, the parameterized layers \(f(,x)\) (fully connected, convolutions) are bilinear operators. Hence we typically obtain bounds of the form:

\[\|(_{d},x)}{_{d}}\|_{2}  K(f_{d},_{d})\|x\|_{2} K(f_{d},_{d})X_{d-1},\] (6)

where \(K(f_{d},_{d})\) is a constant that depends on the nature of the operator. \(X_{d-1}\) is obtained in line 2 with input bound propagation. Values of \(K(f_{d},_{d})\) for popular layers are pre-computed in the library.

Backpropagate octangeant vector bounds (line 7).We bound the Jacobian \((_{d},x)}{ x}\). For activation functions this value can be hard-coded, while for affine layers it is the spectral norm of the linear operator. Like before, this value is constrained with projection operator \(\).

### Privacy accounting for Clipless DP-SGD

Two strategies are available to keep track of \((,)\) values as the training progresses, based on accounting either a per-layer "local" sensitivity, either by aggregating them into a "global" sensitivity.

Figure 2: **Backpropagation for bounds, Algorithm 1. Compute the per-layer sensitivity \(_{d}\).**

**The "global" strategy.** Illustrated in the appendix,this strategy simply aggregates the individual sensitivities \(_{d}\) of each layer to obtain the global sensitivity of the whole gradient vector \(=_{d}^{2}}\). The origin of the clipping-based version of this strategy can be traced back to . With noise variance \(^{2}^{2}\) we recover the accountant that comes with DP-SGD. It tends to overestimate the true sensitivity (in particular for deep networks), but its implementation is straightforward with existing tools.

**The "local" strategy.** Recall that we are able to characterize the sensitivity \(_{d}\) of every layer of the network. Hence, we can apply a different noise to each of the gradients. We dissect the whole training procedure in Figure 3. At same noise multiplier \(\), it tends to produce a higher value of \(\) per epoch than "global" strategy, but has the advantage over the latter to add smaller effective noise \(\) to each weight.

We rely on the autodp2 library  as it uses the Renyi Differential Privacy (RDP) adaptive composition theorem , that ensures tighter bounds than naive DP composition.

## 3 From theory to practice

Beyond the application of Algorithms 1 and 2, our framework provides numerous opportunities to enhance our understanding of prevalent techniques identified in the literature. An in-depth exploration of these is beyond the scope of this work, so we focus on giving insights on promising tracks based on our theoretical analysis. In particular, we discuss how the tightness of the bound provided by Algorithm 1 can be influenced by working on the architecture, the input pre-processing and the loss post-processing.

### Gradient Norm Preserving networks

We can manually derive the bounds obtained from Algorithm 2 across diverse configurations. Below, we conduct a sensitivity analysis on \(l\)-Lipschitz networks.

**Theorem (informal) 1. Gradient Norm of Lipschitz Networks.**_Assume that every layer \(f_{d}\) is \(K\)-Lipschitz, i.e \(l_{1}==l_{D}=K\). Assume that every bias is bounded by \(B\). We further assume that each activation is centered in zero (e.g ReLU, tanh, GroupSort). We recall that \(=[_{1},_{2},_{D}]\). Then the global upper bound of Algorithm 2 can be expanded analytically._

**1. If \(K<1\) we have:**\(\|_{}(f(,x),y)\|_{2}=(L(K^{D}(X_ {0}+B)+1)).\)

Due to the \(K^{D} 1\) term this corresponds to a vanishing gradient phenomenon . The output of the network is essentially independent of its input, and the training is nearly impossible.

**2. If \(K>1\) we have:**\(\|_{}(f(,x),y)\|_{2}=(LK^{D} (X_{0}+B)).\)

Due to the \(K^{D} 1\) term this corresponds to an exploding gradient phenomenon . The upper bound becomes vacuous for deep networks: the added noise \(\) is at risk of being too high.

**3. If \(K=1\) we have:**\(\|_{}(f(,x),y)\|_{2}=(L(X_ {0}++}D+BD^{3/2})),\)

which for linear layers without biases further simplify to \((L(X_{0}+))\).

The formal statement can be found in appendix. From Theorem 1 we see that most favorable bounds are achieved by 1-Lipschitz neural networks with 1-Lipschitz layers. In classification tasks, they are not less expressive than conventional networks . Hence, this choice of architecture is not at the expense of utility. Moreover an accuracy/robustness trade-off exists, determined by the choice of loss function . However, setting \(K=1\) merely ensures that \(\|_{x}f\| 1\), and in the worst-case scenario we have \(\|_{x}f\|<1\) almost everywhere. This could result in a situation where the bound of case 3 in Theorem 1 is not tight, leading to an underfitting regime as in case \(K<1\). With Gradient Norm Preserving (GNP) networks , we expect to mitigate this issue.

Controlling \(K\) with Gradient Norm Preserving (GNP) networks.GNP networks are 1-Lipschitz neural networks with the additional constraint that the Jacobian of layers consists of orthogonal matrices. They fulfill the Eikonal equation \(\|(_{d},x_{d})}{ x_{d}}\|_{2}=1\) for any intermediate activation \(f_{d}(_{d},x_{d})\). Without biases these networks are also norm preserving: \(\|f(,x)\|=\|x\|\).

As a consequence, the gradient of the loss with respect to the parameters is easily bounded by

\[\|_{_{d}}\|=\|_{y_{D}}\|\| (_{d},x_{d})}{_{d}}\|,\] (7)

which for weight matrices \(W_{d}\) further simplifies to \(\|_{W_{d}}\|\|_{y_{D}}\|\|f_{d-1}( _{d-1},x_{d-1})\|\). We see that this upper bound crucially depends on two terms than can be analyzed separately. On one hand, \(\|f_{d-1}(_{d-1},x_{d-1})\|\) depends on the scale of the input. On the other, \(\|_{y_{D}}\|\) depends on the loss, the predictions and the training stage. We show below how to intervene on these two quantities.

Figure 3: **Account for locally enforced differential privacy.****(i)** The gradient query for each layer is turned into a Gaussian mechanism , **(ii)** their composition at the scale of the whole network is a non isotropic Gaussian mechanism, **(iii)** that benefits from amplification via sub-sampling , **(iv)** the train steps are composed over the course of training.

**Remark 2** (Implementation of GNP Networks).: _In practice, GNP are parametrized with GroupSort activation [8; 39], Householder activation , and orthogonal weight matrices [17; 41]. Strict orthogonality is challenging to enforce, especially for convolutions for which it is still an active research area (see [42; 43; 44; 45; 46] and references therein). Our line of work traces an additional motivation for the development of GNP and the bounds will strengthen as the field progresses._

**Controlling \(X_{0}\) with input pre-processing.** The weight gradient norm \(\|_{_{}}\|\) indirectly depends on the norm of the inputs. This observation implies that the pre-processing of input data significantly influences the bounding of sensitivity. Multiple strategies are available to keep the input's norm under control: projection onto the ball ("norm clipping"), or projection onto the sphere ("normalization"). In the domain of natural images for instance, this result sheds light on the importance of color space such as RGB, HSV, YIQ, YUV or Grayscale. These strategies are natively handled by our library.

**Controlling \(L\) with the hybrid approach, loss gradient clipping.** As training progresses, the magnitude of \(\|_{f}\|\) tends to diminish when approaching a local minima, quickly falling below the upper bound and diminishing the gradient norm to noise ratio. To circumvent the issue, the gradient clipping strategy is still available in our framework. Crucially, instead of clipping the parameter gradient \(_{}\), any intermediate gradient \(_{f_{d}}\) can be clipped during backpropagation. This can be achieved with a special "_clipping layer_" that behaves like the identity function at the forward pass, and clips the gradient during the backward pass. The resulting cotangent vector is not a true gradient anymore, but rather a descent direction . In vanilla DP-SGD the clipping is applied on the batched gradient \(_{W_{d}}\) of size \(b h^{2}\) for matrix weight \(W_{d}^{h h}\) and clipping this vector can cause memory issues or slowdowns . In our case, \(_{y_{D}}\) is of size \(b h\) which reduces overhead.

### Lip-dp library

To foster and spread accessibility, we provide an opensource tensorflow library for Clipless DP-SGD training, named lip-dp. It provides an exposed Keras API for seamless usability. It is implemented as a wrapper over the Lipschitz layers of deel-lip3 library . Its usage is illustrated in Figure 1.

## 4 Experimental results

We validate our implementation with a speed benchmark against competing approaches, and we present the privacy/utility Pareto front that can be obtained with GNP networks.

Speed and memory consumption.We benchmarked the median runtime per epoch of vanilla DP-SGD against the one of Clipless DP-SGD, on a CNN architecture and its Lipschitz equivalent respectively. The experiment was run on a GPU with 48GB video memory. We compare against the implementation of tf_privacy, opacus and optax. In order to allow a fair comparison, when evaluating Opacus, we reported the runtime with respect to the logical batch size, while capping the physical batch size to avoid Out Of Memory error (OOM). Although our library does not implement logical batching yet, it is fully compatible with this feature.

An advantage of projection \(\) over per-sample gradient clipping is that the projection cost is independent of the batch size. Fig 4 validates that our method scales much better than vanilla DP-SGD, and is compatible with large batch sizes. It offers several advantages: firstly, a larger batch size contributes to a decrease of the sensitivity \( 1/b\), which diminishes the ratio between noise and gradient norm. Secondly, as the batch size \(b\) increases, the variance decreases at the parametric rate \(()\) (as demonstrated in appendix), aligning with expectations. This observation does not apply to DP-SGD: gradient clipping biases the direction of the average gradient, as noticed by .

Figure 4: **Our approach outperforms concurrent frameworks in terms of runtime and memory:** we trained CNNs (ranging from 130K to 2M parameters) on CIFAR-10, and report the median batch processing time (including noise, and constraints application \(\) or gradient clipping).

Pareto front of privacy/utility trade-off.We performed a search over a broad range of hyperparameters values to cover the Pareto front between utility and privacy. Results are reported in Figure 5. We emphasize that our experiments did not use the elements behind the success of most recent papers (pre-training, data preparation, or handcrafted feature are examples). Hence our results are more representative of the typical performance that can be obtained in an "out of the box" setting. Future endeavors or domain-specific engineering can enhance the performance even further, but such improvements currently lie beyond the scope of our work. We also benchmarked architectures inspired from VGG , Resnet  and MLP_Mixers  see appendix for more details. Following standard practices of the community , we used _sampling without replacement_ at each epoch (by shuffling examples), but we reported \(\) assuming _Poisson sampling_ to benefit from privacy amplification . We also ignore the privacy loss that may be induced by hyper-parameter search, which is a limitation per recent studies , but is common practice.

## 5 Limitations and future work

Although this framework offers a novel approach to address differentially private training, it introduces new challenges. We primary rely on GNP networks, where high performing architectures are quite different from the usual CNN architectures. As emphasized in Remark 2, we anticipate that progress in these areas would greatly enhance the effectiveness of our approach. Additionally, to meet requirement 3, we rely on projections, necessitating additional efforts to incorporate recent advancements associated with differentiable reparametrizations . It is worth noting that our methodology is applicable to most layers. Another limitation of our approach is the accurate computation of sensitivity \(\), which is challenging due to the non-associativity of floating-point arithmetic and its impact on numerical stability . This challenge is exacerbated on GPUs, where operations are inherently non-deterministic . Finally, as mentioned in Remark 1, our propagation bound method can be refined.

## 6 Concluding remarks and broader impact

Besides its main focus on differential privacy, our work provides **(1) a motivation to further develop Gradient Norm Preserving architectures**. Furthermore, the development of networks with known Lipschitz constant with respect to parameters is a question of independent interest, **(2) a useful tool for the study of the optimization dynamics** in neural networks. Finally, Lipschitz networks are known to enjoy certificates against adversarial attacks , and from generalization guarantees , without cost in accuracy . We advocate for the spreading of their use in the context of robust and certifiable learning.

Figure 5: **Our framework paints a clearer picture of the privacy/utility trade-off. We trained models in an ”out of the box setting” (no pre-training, no data augmentation and no handcrafted features) on multiple tasks. While our results align with the baselines presented in other frameworks, we recognize the importance of domain-specific engineering. In this regard, we find the innovations introduced in  and references therein highly relevant. These advancements demonstrate compatibility with our framework and hold potential for future integration.**