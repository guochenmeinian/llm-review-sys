# Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making

Manling Li\({}^{1,\,2}\), Shiyu Zhao\({}^{1}\), Qineng Wang\({}^{1,\,2}\), Kangrui Wang\({}^{1,\,2}\), Yu Zhou\({}^{1}\),

**Sanjana Srivastava\({}^{1}\), Cem Gokmen\({}^{1}\), Tony Lee\({}^{1}\), Li Erran Li\({}^{3}\), Ruohan Zhang\({}^{1}\), Weiyu Liu\({}^{1}\), Percy Liang\({}^{1}\), Li Fei-Fei\({}^{1}\), Jiayuan Mao\({}^{4}\), Jiajun Wu\({}^{1}\)**

\({}^{1}\)Stanford University \({}^{2}\)Northwestern University \({}^{3}\)Amazon \({}^{4}\)MIT

embodied-agent-interface.github.io

\({}^{}\)Data \({}^{}\)CodePyPI \({}^{}\)Docker \({}^{}\)Video \({}^{}\)Docs

Equal contribution.

###### Abstract

We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics that break down evaluation into error types, such as hallucination errors, affordance errors, and various types of planning errors. Overall, our benchmark offers a comprehensive assessment of LLMs' performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights into the effective and selective use of LLMs in embodied decision making.

## 1 Introduction

Large Language Models (LLMs) have emerged as powerful tools for building embodied decision-making agents capable of following human instructions (such as "_cleaning the refrigerator_", "_polishing furniture_") and achieving the specified goals through a sequence of actions in various digital and physical environments . Despite many reports of their success, our understanding of LLMs' full capabilities and limitations in embodied decision-making remains limited. Existing evaluation methods fall short of providing comprehensive insights due to three key limitations: the lack of standardization in 1) embodied decision-making tasks, 2) modules that an LLM can interface with or be implemented for, and 3) fine-grained evaluation metrics beyond a single success rate. In this paper, we propose the Embodied Agent Interface to address these challenges.

(1) **Standardization of goal specifications**: We want embodied agents to achieve goals. However, the specification of goals and the criteria for agents' success evaluation vary significantly across different domains, even for similar tasks. For example, BEHAVIOR  focuses on achieving a state that satisfies certain _state goals_ (e.g., "_not_stained_(fridge)" in Figure 1), while VirtualHome  usestemporally extended goals by imposing temporal order constraints on actions. We include an extended discussion in Appendix C.1. Our Embodied Agent Interface implements a general object-centric state and action representation, where object states, relations, and actions are represented in abstract language terms (see Figure 1). Our innovation is to describe goals as linear temporal logic (LTL) formulas, which define task-success criteria over trajectories. LTL affords the specification of both state-based and temporally extended goals and allows for alternative goal interpretations.

(2) **Standardization of modules and interfaces**: Existing LLM-based embodied agent frameworks often make different assumptions based on the availability of additional knowledge and external modules. For instance, Code as Policies  and SayCan  utilize LLMs for action sequencing given a given set of primitive skills, while LLM+P  uses LLMs for goal interpretation and generates plans using PDDL planners with given domain definitions; Ada  leverages LLMs to generate high-level planning domain definitions in PDDL and uses a low-level planner to generate control commands. Consequently, they have defined different input-output specifications for the LLM module, making comparisons and evaluations challenging. In Embodied Agent Interface, built on top of our object-centric and LTL-based task specification, we formalize four critical _ability modules_ in LLM-based embodied decision making, as illustrated in Figure 1: _Goal Interpretation_, _Subgoal Decomposition_, _Action Sequencing_, and _Transition Modeling_. We formalize the input-output

Figure 1: Embodied Agent Interface unifies a broad set of tasks involving both state and temporally extended goals and four LLM-based modules for decision-making.

Figure 2: The input and output formulation of four ability modules.

specifications that LLMs can use to interface with other modules in the environment. This modular interface automatically enables the integration of different LLM-based and external modules. Figure 2 shows the input and output formulation of four ability modules. Taking _Subgoal Decomposition_ as an example, this module takes initial states (e.g., a fridge is stained initially) and a task goal (e.g., clean fridge), and asks LLMs to generate a subgoal trajectory (e.g., first the cloth is soaked, then it is held by the agent, then the agent is next to the fridge, and in the end, the fridge is clean). Formal definitions and notations can be found in Table 1.

(3) **Standardization of fine-grained evaluation metrics with broad coverage**: Current evaluations of LLMs for embodied decision-making have been overly simplified, usually focusing on the success rate of a single task. The recent work LOTA-Bench  aims to break down the evaluation but is limited to generating action sequences and does not support analysis of fine-grained planning errors. Our Embodied Agent Interface, leveraging object-centric and factorized representations of states and actions, implements a collection of fine-grained evaluation metrics, designed to automatically locate different types of errors such as hallucination errors, different types of planning errors (e.g., object affordance errors, wrong action orders). Figure 3 illustrates different types of errors made by GPT-4o on four different ability modules across two simulators. Specifically, we evaluate two aspects of each module: trajectory evaluation, which checks if the generated plan can be executed in simulators, and goal evaluation, which ensures the plan achieves correct outcomes. Goal evaluation applies to goal interpretation, action sequencing, and subgoal decomposition, while trajectory evaluation applies to action sequencing, subgoal decomposition, and transition modeling.

We implement Embodied Agent Interface on two embodied decision-making benchmarks: BEHAVIOR  and VirtualHome , and evaluated 18 different LLMs. Figure 3 visualizes the performance of 5 representative LLMs on different tasks in Behavior. Our key findings include:

* Most LLMs struggle to faithfully translate natural language instructions into grounded states (objects, object states, and relations) in the environment. They sometimes predict intermediate subgoals as part of the final goals, e.g., predicting the state _open_(freezer) for task "_drinking water_".
* Reasoning ability is a crucial aspect that LLMs should improve. Trajectory feasibility errors are common (45.2%), with a large portion of missing step (19.5%) and additional step (14.2%) errors, often due to overlooking preconditions. For instance, LLMs may ignore the agent's _sitting_ or _lying_ state and fail to include a _standup_ action before executing other actions. They sometimes also fail to understand the need to _open_ a _closed_ object before _fetching_ items from inside. Additional step errors frequently occur when LLMs output actions for previously achieved goals.
* Trajectory evaluation performance decreases as the trajectory sequence length increases; goal evaluation performance, which refers to evaluating if a plan can achieve task goals when executed, decreases when the environment becomes more complex, involving a larger variety of object and state features.
* LLM errors include not only hallucinations of nonexistent objects and actions but also a heavy reporting bias. They often ignore commonsense preconditions that are elided in the language. For example, "put the turkey on the table" should be interpreted as "put the turkey on a plate, and place the plate on the table."
* Subgoal decomposition, though designed to simplify planning, is as complex as action sequencing in abstract spaces, as LLMs must declaratively break goals into feasible steps.

Figure 3: Embodied Agent Interface supports a collection of fine-grained metrics and provides automatic toolkits for error analysis and benchmarking different LLMs on various embodied decision-making tasks.

* We further provide quantitative analysis for the robustness of the modules through sensitivity analysis, pipeline-based versus modularized comparison, and replanning. These analyses aim to identify potential ways to integrate LLM-based and external modules.
* o1-preview significantly outperforms others, especially on the BEHAVIOR simulator (74.9% vs. 64.2%). It excels in goal interpretation on VirtualHome, as well as action sequencing, transition modeling, and subgoal decomposition on both BEHAVIOR and VirtualHome. Claude-3.5 Sonnet is strong in goal interpretation on BEHAVIOR and transition modeling on VirtualHome, while Mistral Large performs well in action sequencing on VirtualHome.

## 2 Embodied Agent Interface Based on LTL

Table 1 summarizes our Embodied Agent Interface. First, we define an **embodied decision-making problem representation**\(,,,g,,\), which is a language-based, object-centric abstraction for embodied agent environments with _objects_ (\(o\)), _states_ (\(s\)), _actions_ (\(a\)), _goal_\(g\), _subgoal_\(\), and trajectories \(\). Second, we formally define four **ability modules**\(,,,\), including their standardized input-output specifications. They are fundamental and commonly-used modules that LLMs can be implemented for and interface with the _goal interpretation_ module \(\), the _action sequencing_ module \(\), the _subgoal decomposition_ module \(\), and the _transition modeling_ module \(\). In this paper, we focus on object-centric modeling: states are described as relational features among entities in the environment, actions are defined functions that take entity names as inputs and can be executed in the environment, goals and subgoals are defined as linear-temporal logic (LTL)  formulas on states and actions. We define each component in detail as follows.

### Representation for Objects, States and Actions

In Embodied Agent Interface, a state is represented as a tuple \(s=,\), where \(\) is the universe of objects, assumed to be a fixed finite set. \(\) is a set of relational Boolean features. Each \(f\) is a table where each entry is associated with a tuple of objects \((o_{1},,o_{k})\). Each entry has the value of the feature in the state, and \(k\) is the arity of the feature. Actions can be viewed as primitive functions that take objects as inputs, denoted as \(,\). Throughout the paper, we focus on tasks where states and actions are described in abstract language forms, including object states (e.g., _is-open_ (cabinet1)), relations (e.g., _is-on_ (rag0, window3)), and actions (e.g., _soak_(rag0)).

### Representation for Goals, Subgoals, Action Sequences, and State-Action Trajectories

In Embodied Agent Interface, goals \(g\), subgoals \(\), and action sequences \(\) are modeled as linear temporal logic (LTL) formulas. This is motivated by two critical desiderata. First, we need an expressive and compact language to describe both state-based and temporally extended goals. Second, we need a unified interface between different LLM-based modules. LTL addresses both challenges. At a high level, an LTL formula can describe state constraints (e.g., a subgoal should be achieved),

   &  & **Description** \\   & Object & \(u\) & An object, which has relational features \(f\) \\  & State & \(s=,\) & A tuple of the universe of objects and relational features \\   & Action & \(a=,\) & A tuple of the action name and arguments \\  & Operator & \(o=,\) & An action schema: a tuple of the name and a list of parameters. \\  & Transition Model & \(:\) & The deterministic transition function of the environment \\   & Natural Language Goal & \(l_{g}\) & A sentence in English \\  & LTL Goal & \(g\) & An LTL formula. Here, we only consider formulas containing a sequence of action items and a conjunction of propositions (for the final state): \(g=a_{1}\)**then**\(\)**then**\(a_{k}\)**then**\((p_{1} p_{t})\). \\   & Action Trajectory & \(=\{a_{i}\}_{i=1}^{n}\) & A sequence of \(n\) actions \\  & Subgoal Trajectory & \(=\{_{i}\}_{i=1}^{n}\) & A sequence of LTL subgoals \(_{i}\) connected by “**then**” \\  & State-action Trajectory & \(=\{s_{i}\}_{i=0}^{n},\{a_{i}\}_{i=1}^{n}\) & A sequence of state-action pairs. \( t.s_{t+1}=(s_{t},a_{t})\) \\   & Task & \( s_{0},g,l_{g}\) & A tuple of the initial state and the LTL/Natural Language goals \\   & Goal Interpretation & \(: s_{0},l_{g} g\) & Initial State \& Natural Language Goal \(\) LTL Goal \\  & Subgoal Decomposition & \(: s_{0},g\) & Initial State \& Goal \(\) Subgoal Trajectory \\   & Action Sequencing & \(: s_{0},g,\) & Initial State \& Goal \& Transition Model \(\) Action Trajectory \\   & Transition Modeling & \(: s_{0},g,o,\) & Initial State \& Goal \& Operator \& Preconditions \& Effects \\  

Table 1: Summary of notations used in Embodied Agent Interface.

action constraints (e.g., a particular action should be executed), and possible temporal orders among them (e.g., all dishes should be cleaned before we cook). By combining temporal connectives (such as "eventually") and propositional logic connectives (such as "or"), we can also flexibly describe alternative goals or trajectories. As a byproduct, using a single description language for all inputs and outputs enables us to design a unified metric to measure accuracy, which we detail in Appendix C.1.

In Embodied Agent Interface, we use a fragment of the full linear temporal logic (LTL) formalism on finite trajectories. We allow two types of atomic propositions: state propositions (object properties and relations) and action propositions. Our LTL language contains Boolean conjunction \(\), disjunction \(\), negation \(\), implication \(\), first-order logic quantifiers \(\), \(\), \(^{=n}\) (the equal quantifier: there are exactly \(n\) objects satisfying a condition), and the temporal connective **then**.

An LTL formula is a trajectory classifier semantically: the function \((,)\) evaluates an LTL formula \(\) on a state-action sequence \(\). We say that the state-action sequence satisfies \(\) if \((,)=\) (i.e., the goal \(\) is satisfied). For state formulas \(\) (formulas without **then**), we define \((,)= t.(s_{t})\) ("eventually" the goal is satisfied). For formulas connected by **then**, \((_{1}_{2},)= k._{1}(t _{ k})_{2}(_{>k})\) (\(_{2}\) is achieved after \(_{1}\)), where \(_{ k}\) and \(_{>k}\) denote prefixes and suffixes. Currently, we have not implemented other temporal connectives such as "globally" and "until" but our overall framework can be extended to them. An LTL formula example of a subgoal plan for task _browse Internet_ is: "_ontop_(character, chair) **then**_holds_rh_(character, mouse) \(\) holds_lh_(character, keyboard) **then**_facing_(character, computer)". We include LTL details in Appendix C.3.

### Ability Module 1: Goal Interpretation \(: s_{0},l_{g} g\)

**Input-Output Specification.** The _goal interpretation_ module takes the state \(s_{0}\) and a natural language instruction \(l_{g}\) as input, and generates an LTL goal \(\), as a formal goal specification which a symbolic planner can conceeivably take as input. In this paper, we only generate simple LTL goals formed by an ordered action sequence and a conjunction of propositions to be satisfied in the final state.

**Evaluation Metric.** An LTL goal can be evaluated by directly comparing it with the ground truth goal \(g\). While we have restricted generated \(\) to be simple LTL goals, we do not require the ground truth goal \(g\) to be simple. Therefore, we additionally define \(\) that takes the object universe \(\) as input to translate \(g\) to a set of simple LTL goals \(g_{0},g_{1},,g_{k}\) where all \(g_{i}\)'s entail \(g\). We describe our implementation in the Appendix. Given two simple LTL goals \(g_{i}\) and \(\), the accuracy of \(\) can be computed as an F\({}_{1}\) set-matching score between them. Let \(g=a_{1}\)\({}^{}a_{k}\)**then**\((p_{1} p_{})\). We define \((g)=\{\{a_{i}\}_{i=1}^{k}\}\{p_{i}\}_{i=1}^{}\) (i.e., the action sequence \(\{a_{i}\}\) is treated as a single element). The F\({}_{1}\) score between \(g\) and \(\) is defined as: \(_{1}(g,)=_{g_{i}(g,)}_{1}( (g_{i}),()).\)

### Ability Module 2: Subgoal Decomposition \(: s_{0},g\)

**Input-Output Specification.** The _subgoal decomposition_ module takes the task \( s_{0},g\) as input and generates a sequence of subgoals \(=\{_{i}\}_{i=1}^{k}\), where each \(_{i}\) is an LTL formula. The entire

Figure 4: The overview of evaluation pipeline for four abilities. For each ability module, to provide a comprehensive evaluation for it, we isolate this single module to be handled by the LLMs while using existing data or tools for the other modules. Note that the pipeline consists of goal interpretation, action sequencing to achieve the goal, and transition modeling that predicts how each action operate the environment’s state. Evaluating subgoal decomposition presents a challenge since it cannot be evaluated directly with no unified annotation strategy. To address this, we employ breadth-first search (BFS) to identify potential action sequences that accomplish each subgoal, allowing us to convert state trajectories into action sequences that can be executed in the simulator (Figure 21 in Appendix). Transition modeling evaluation poses another challenge, we first annotate transition models in PDDL for \(F_{1}\) evaluation followed with a PDDL planner to validate the feasibility of supporting potential plans. We also conduct a pipeline-based vs modularized analysis, detailed in the Appendix G.

sequence \(\) can also be represented as a single LTL formula. One may refer to Appendix D.3 for decomposition choice-making.

**Evaluation Metric.** To evaluate the subgoal decomposition module, we use a customized planner to refine it into an action sequence \(\). This subgoal-action mapping function \((,s_{0})\) takes the LTL representation of \(\) and \(s_{0}\) and generates a state-action sequence \(\). We implement this with a breadth-first search. Then, we use the same metrics in _action sequencing_ for evaluation: trajectory feasibility and goal satisfaction. Since each \(\) can be grounded into different action sequences, we restrict the number of actions per subgoal to generate a finite set of possible action sequences \(_{i}\) satisfying \(\). Then, we compute the metrics for each \(_{i}\) and report the maximum score across all \(_{i}\)'s as the trajectory feasibility and the goal satisfaction scores for \(\).

Ability Module 3: Action Sequencing \(: s_{0},g,\)

**Input-Output Specification.** The _action sequencing_ module takes the task \( s_{0},g\) as input, and the transition model \(\), and generates an action sequence \(=\{a_{i}\}_{i=1}^{n}\).

**Evaluation Metric.** We use two evaluation metrics for the action sequencing module. First, the _trajectory feasibility evaluation_ focuses on evaluating whether the trajectory is executable (i.e., all actions are feasible). We will execute the trajectory \(\) from \(s_{0}\) in the simulator. When infeasible action presents, the execution may stop at an early step and we categorize the execution failure into missing steps, additional steps, wrong temporal order, and affordance errors.

Second, the _goal satisfaction evaluation_ evaluates if the goal is satisfied after executing \(\). Specifically, we obtain \(T=\{s_{i}\}_{i=0}^{m},\{a_{i}\}_{i=1}^{m}\) by executing \(\), and directly use the _eval\((g,T)\)_ function to check for goal satisfaction. We also evaluate the _partial goal satisfaction evaluation_, which is the percentage of "subgoals" in \(g\) that are satisfied in \(\). To compute this partial success rate, we again consider all simple LTL goals \(g_{i}\) derived from \(g_{i}=a_{1}}}}{{}}a_{k}\)**then**\((p_{1} p_{})\). If there is a subsequence in \(\) that is the same as \(\{a_{j}\}_{j=1}^{k}\), we consider the action sequence successfully executed. Next, we evaluate all final state propositions \(p_{j}\) and give models partial credits based on the number of propositions satisfied in \(s_{m}\). Finally, _PartialSucc\((,g)=_{g_{i}(g,)}( ,g_{i})\)_.

Ability Module 4: Transition Modeling \(: s_{0},g,o pre,\)

**Input-Output Specification.** The _transition modeling_ module takes the task \( s_{0},g\) and a set of operator definitions \(\{o_{i}\}\) as input, and generates a PDDL operator definition  for each \(o_{i}\). In this module, we aim to create a formal definition of actions in order to generate plans to solve the task. During evaluation, we first extract relevant operator definitions, \(\{o_{i}\}\), based on the ground truth action trajectory \(\) associated with each task, with details provided in Appendix C.3. Then, the LLM generates the preconditions and effects \(\{ pre_{i},_{i}\}\) for all operators \(\{o_{i}\}\).

**Evaluation Metric.** The _transition modeling_ module can be evaluated in two ways. First, the _logic matching score_ for an operator \(o_{i}\) compares the generated \(_{i}\) and \(_{i}\) against the ground truth operator definition annotated by human experts. This comparison uses a surface form matching score to produce an F\({}_{1}\)-based score between two logic formulas. Intuitively, when both the LLM-generated \(_{i}\) and ground truth \(_{i}^{gt}\) are conjunctions of propositions, the F\({}_{1}\) score is computed as the set matching score between the sets of propositions. More complex logic formulas (e.g., \( x.(x)\)) are evaluated recursively, as detailed in Appendix C.3. The evaluation of effects is performed similarly.

Furthermore, the _planning success rate_ assesses whether the preconditions and effects of different operators enable a viable plan. This is computed by running an external PDDL planner  based on generated operator definitions to achieve \(g\) from the initial state \(s_{0}\). For simplicity, we only state goals in \(g\) (and ignore action subgoals). The planning success rate is 1 if the planner finds a plan.

## 3 Dataset Annotations and Benchmark Implementations

**Annotations.** Focusing on complex long-horizon tasks, we select BEHAVIOR (B) and VirtualHome (V) as our evaluation simulators based on their task length and scene complexity. We include a comparison of different simulators and detailed selection considerations in Appendix M.1. Table 2 shows our annotations. Apart from the goal and trajectory annotations, we introduce the Goal Action annotation to reflect necessary actions that do not have post effects, such as the goal action _touch_ in the task _"pet the cat"_, as detailed in Appendix M.3. In the subset of VirtualHome tasks we work on, 80.7% task categories include instructions with action steps longer than 10, and 33% of the instructions have step lengths of more than 10.

We select BEHAVIOR as another simulator for our evaluation due to its task complexity. BEHAVIOR BDDL goals may contain quantifiers, such as (forpairs (?jar?apple) (inside?apple?jar)), which need to be translated into grounded goals of only atomic propositions, e.g., and ((inside apple_1 jar_1) (inside apple_2 jar_2)). There can be different grounded goals that satisfy the same BDDL goal, such as ((inside apple_2 jar_1) (inside apple_1 jar_2)). We call them goal options. In general, one BDDL goal corresponds to a number of goal options. The average number of grounded goals for each task is \(6.7\), and there are \(4,164.4\) goal options for each task on average. We show data distributions of goal options and other statistics in Appendix M.2.

**Implementation on simulators.** As BEHAVIOR does not have an action transition model layer, we implemented a symbolic simulator with an action transition model layer. Our implementation, EvalGibson, offers 30 actions that agents can use to change the states of objects. Implementation details are in Appendix N.1. We also revise the VirtualHome simulator to support accurate evaluation, as detailed in Appendix N.2. Evaluation settings for each large model are detailed in Appendix O.

## 4 Results

We evaluate 18 open-weight and proprietary LLMs on four embodied agent ability modules across two benchmark simulators: BEHAVIOR and VirtualHome. Table 3 gives an overview. Table 4, Table 5, Table 6, and Table 7 break down the analysis of four representative LLMs on four ability modules. Figure 5 shows examples of different types of error. We start with the overall analysis.

**Model Comparison.** Shown in Figure 3, the top performing models overall are o1-preview, Claude-3.5 Sonnet and Gemini 1.5 Pro, with o1-preview leading in all aspects except **object states** and Gemini 1.5 Pro leading in its **object state reasoning** ability. Among all open-weight models, the best performing models are Llama-3-70B and Mistral-Large-2402, while there is still a performance gap with commercial models.

**Ability Comparison.** o1-preview shows a clear advantage over other models, particularly on the BEHAVIOR simulator, where it achieves 74.9% compared to 64.2%. It leads in several areas, including goal interpretation on VirtualHome and both action sequencing and transition modeling on BEHAVIOR. Moreover, it outperforms in subgoal decomposition across both BEHAVIOR and VirtualHome simulators. In contrast, Claude-3.5 Sonnet shines in goal interpretation on BEHAVIOR and transition modeling on VirtualHome, while Mistral Large stands out in action sequencing on VirtualHome. Mistral-8x22B shines in transition modeling among open-weight LLMs, and Llama-3-70B Instruct in goal interpretation.

We also observe a performance gap between different simulators. Models achieve significantly lower trajectory feasibility scores on BEHAVIOR compared to VirtualHome, but achieve higher scores on goal interpretation. This is because BEHAVIOR tasks have a much longer horizon (avg 14.6 steps) while VirtualHome goals have a larger state space to search (such as "_work_"), as detailed in Appendix L.2. It shows the inverse correlation between trajectory evaluation performance and sequence length, as well as between goal evaluation performance and environment complexity. We further perform a systematic analysis to discover the cofactors for the goal success rate, including the number of task goals, particularly node goals, the ground truth action length, and the task object length, with details in Appendix E.5.

**Object States vs Relationship.** Relational goals are generally harder to reason about compared to object-state goals. Spatial relations have a significantly lower recall in the goal interpretation task (Table 4) and a lower goal satisfaction rate (Table 5). Some non-spatial relations (e.g., _hold_) are even more difficult for LLM to predict than spatial relations, as shown in the transition modeling accuracy (Table 6): for example _holding_(toothbrush) should be a precondition for brushing teeth.

   & **BEHAVIOR** \\  \#task name & 26 & 100 \\ \#task instruction & 338 & 100 \\  \#goal & 801 & 673 \\ - \#state & 340 & 153 \\ - \#relation & 299 & 520 \\ - \#action & 162 & - \\ \#trajectory & 338 & 100 \\ - \#step & 2960 & 1460 \\ - avg. step & 8.76 & 14.6 \\ \#transition model & **33** & 30 \\ - \#precondition & 99 & 84 \\ - \#effect & 57 & 51 \\  

Table 2: Simulator dataset statistics. New annotations collected in this paper are highlighted in color.

[MISSING_PAGE_FAIL:8]

### Ability Module Analysis

**Goal Interpretation.** LLMs generally have difficulties distinguishing intermediate subgoals and final goals. For example, in the VirtualHome task _Drink_, GPT-4o predicts some intermediate states as part of the final goal (e.g., _open_(freezer) and _inside_(water, glass)). Overall, we observe that LLMs tend to translate NL goals word-by-word into their symbolic correspondence, rather than grounding them in the environment state. More analyses are in Appendix E.1.

**Subgoal Decomposition and Action Sequencing on Trajectory Feasibility.** Most errors are runtime errors (rather than syntax errors). We illustrate examples in Figure 5. Overall, LLMs are more likely to make missing-step and additional-step errors than wrong-order or affordance errors. Missing-step errors occur when a precondition is not satisfied before the execution of an action (e.g., fetching an object without opening the box containing it). Additional steps form the most frequent errors, even for the most powerful models--it occurs when a goal has already been achieved but the model still predicts to execute an additional action to achieve it (e.g., opening a box twice). More analysis is in Appendix E.3.

**Subgoal Decomposition and Action Sequencing on Goal Satisfaction Rates.** Shown in Table 5, object goals (such as _toggled_on_) are generally easier to achieve than relational goals (such as _ontop_(agent, chair)). More analysis is provided in Appendix E.2.

**Transition Modeling.** Table 7 shows the overall performance of the logic form accuracy. For a systematic evaluation, we further categorize the tasks into five distinct ability categories requiring the transition modeling for different types of object states and relations (see Appendix F.3). Overall, we reveal significant variations in performance across different models; relational preconditions and effects are generally harder to predict than object-state ones. For instance, the Claude-3 Opus model excelled in object states (63% on VirtualHome), but its performance in spatial relations is weak. Additionally, in tasks that focus on object properties, models generally perform poorly in reasoning about object orientation (e.g., the agent should be facing the TV to watch it). We also provide a sensitivity analysis tool to visualize how different transition modeling errors result in downstream

    &  &  &  \\   &  &  &  &  &  &  &  &  &  \\   & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) \\   \\  Claude-3 Somet & **76.7** & 60.0 & **81.3** & 69.0 & **0.0 & **0.0 & 0.0** & 0.3 & 0.0 & 0.3 & 5.0 & 14.4 & 25.0 & 16.0 & **1.3** & 2.0 \\ Claude-3 Opus & 64.9 & 51.0 & 69.5 & 59.0 & **0.0 & **0.0 & 17.0 & **0.0** & **0.0** & 0.0 & 3.0 & 12.8 & 35.0 & 0.3 & 3.0 & 2.3 \\ Gemini 1.5 Pro & **76.7** & 42.0 & 83.6 & 54.0 & **0.0 & **0.1 & **1.3** & 0.7 & **0.0** & 0.3 & 6.0 & 14.1 & 39.0 & **0.0** & **1.0** & 3.0 & 2.0 \\ GPT-4 & 71.5 & 47.0 & **81.3** & 53.0 & 10.0 & 0.2 & 1.0 & 0.7 & **0.0** & 0.3 & 9.0 & 15.1 & 36.0 & **0.0** & **1.0** & 2.3 & **0.0** \\ Llama 3.70B & 59.0 & 34.0 & 66.6 & 42.0 & **0.0 & **0.1 & 1.2 & 0.8 & **2.0** & **0.0** & 21.0 & 15.0 & **9.2** & **3.0** & **0.0** & 3.0 & 6.2 & 6.0 \\ ol-mini & 71.5 & 56.0 & 76.4 & 65.0 & 49.0 & **0.0** & 2.0 & **0.0** & **0.0** & **0.0** & 1.0 & 7.0 & 17.7 & 17.0 & 0.3 & 6.0 & 2.6 & 5.0 \\ ol-preview & 65.2 & **81.0** & 72.5 & **91.0** & 6.0 & **0.0** & **1.5** & **0.0** & **0.0** & **0.0** & 0.0** & 12.1 & **6.0** & 0.3 & 2.0 & 2.0 & 3.0 \\   \\   \\  Claude-3 Somet & 89.1 & 39.0 & 92.0 & 44.0 & **0.0 & **0.0 & 1.8 & 1.0 & **0.0** & **0.0** & 1.5 & 11.0 & 2.7 & 44.0 & 2.1 & **0.0** & 24.6 & 4.0 \\ Claude-3 Opus & 87.0 & 39.0 & 89.7 & 47.0 & **0.0 & **0.0 & 3.3 & 3.0 & **0.0** & **0.0** & 1.2 & 5.0 & 3.0 & 45.0 & 2.4 & **0.0** & 16.0 & 5.0 \\ Gemini 1.5 Pro & 87.0 & 31.0 & 91.1 & 37.0 & **0.0** & 1.5 & **0.0** & 1.8 & 1.0 & **0.0** & **3.0** & 5.6 & 9.0 & **0.0** & **0.0** & 16.0 & 2.0 \\ GPT-4o & 88.8 & 48.0 & 90.2 & 55.0 & **0.0 & **0.0** & 6.2 & 3.0 & **0.0** & **0.0** & 1.2 & 5.0 & **2.4** & 37.0 & **0.0** & **0.0** & 15.7 & 5.0 \\ Llama 3.70B & 78.4 & 20.0 & 87.3 & 30.0 & **0.0** & 1.0 & 2.4 & 5.0 & 9.1 & 1.0 & 2.4 & 8.0 & 5.3 & 51.0 & 1.8 & 4.0 & 20.4 & 4.0 \\ ol-mini & 79.3 & 31.0 & 84.6 & 39.0 & **0.0 & **0.0 & 1.5 & 3.0 & 0.6 & 3.0 & 0.3 & 7.0 & 8.9 & 46.0 & 4.1 & 2.0 & 21.9 & **1.0** \\ ol-preview & **89.4** & **57.0** & **93.2** & **62.0** & **0.0** & 2.0 & **1.5** & 3.0 & **0.0** & **0.0** & 0.3 & 5.0 & 2.7 & **25.0** & 2.4 & 3.0 & **12.1** & 7.0 \\   

Table 6: Trajectory evaluation results (%) for _action sequencing_ and _subgoal decomposition_. Full results in Appendix E.3.

    &  &  \\   &  &  &  &  &  &  &  &  \\   & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) & \(V\) & \(B\) \\  Claude-3.5 Sonnet & 87.8 & 63.0 & **83.3** & 62.4 & 60.8 & - & **79.9** & 62.6 & 92.9 & 41.0 & **88.6** & 39.5 & 87.0 & - & 90.1 & 39.9 \\ Claude-3 Opus & 57.2 & 45.0 & 77.8 & 53.0 & 54.7 & - & 62.7 & 50.8 & 92.4 & 43.0 & **88.6** & 41.6 & 83.3 & - & 89.1 & 42.0 \\ Gemini 1.5 Pro & 85.6 & 4planning errors (see Appendix F and E.4). We found that LLMs tend to overstate object states in effects while understating them in preconditions. Conversely, they overstate spatial relationships in preconditions and understate them in effects. As a result, in many cases, even if the downstream planner successfully generates a plan, it may not be feasible in the actual environment.

**Implications in Embodied Agent System Design.** We further investigate the potential integration of LLM-based ability modules and their robustness through **sensitive analysis** (Appendix F), **modularized vs pipeline-based** experiments (Appendix G), and **replanning** (Appendix H). We observe that trajectory feasibilities are similar, although with error accumulation from different module compositions, showing the potential of module composition. We have also compared different **prompting strategies** for embodied decision-making tasks, and summarize the best practices in Appendix 1.

## 5 Related Work

Recent work in embodied decision making has been using LLMs to perform various tasks, and we include a comprehensive summary in Appendix P, see also Table 8 for a quick summary. LLMs can also be used to combine multiple of the above modules at once via chain-of-thought prompting or pipelined queries, such as goal interpretation with action sequencing , goal interpretation with subgoal decomposition , action sequencing with subgoal decomposition , action sequencing with transition modeling . Our work aims to standardize the interface between LLMs and various decision-making modules to support the seamless integration, modular evaluation, and fine-grained metrics, aiming to provide implications on using LLMs in embodied decision making more effectively and selectively. We provide additional related work on agent interfaces  and simulation benchmarks in Appendix P.

## 6 Conclusions and Future Work

We propose a systematic evaluation framework Embodied Agent Interface to benchmark LLMs for embodied decision-making. It focuses on 1) standardizing goal specifications using LTL formulas, 2) unifying decision-making tasks through a standard interface and four fundamental ability modules, and 3) providing comprehensive fine-grained evaluation metrics and automatic error identification. We highlight the limitations of current LLMs in interpreting complex goals and different errors in reasoning, further attributing errors to various cofactors, including trajectory length, goal complexity, spatial relation goals, etc.

**Limitations and future work:** Our current evaluation is limited to states, actions, and goals that can be described in abstract language terms, with the input environment abstracted by relational graphs of objects. Future work should extend this to include sensory inputs and actuation outputs, possibly by extending the studied model class to include Vision-Language Models (VLMs), which we discuss further in Appendix K. Other aspects of extension include the integration of memory systems (episodic memory and state memory), geometric reasoning, and navigation.