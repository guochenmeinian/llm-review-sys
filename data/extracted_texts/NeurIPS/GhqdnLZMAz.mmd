# Improving Decision Sparsity

Yiyang Sun

Duke University &Tong Wang

Yale University &Cynthia Rudin

Duke University

###### Abstract

Sparsity is a central aspect of interpretability in machine learning. Typically, sparsity is measured in terms of the size of a model globally, such as the number of variables it uses. However, this notion of sparsity is not particularly relevant for decision making; someone subjected to a decision does not care about variables that do not contribute to the decision. In this work, we dramatically expand a notion of _decision sparsity_ called the _Sparse Explanation Value_ (SEV) so that its explanations are more meaningful. SEV considers movement along a hypercube towards a reference point. By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes. We present cluster-based SEV and its variant tree-based SEV, introduce a method that improves credibility of explanations, and propose algorithms that optimize decision sparsity in machine learning models.

## 1 Introduction

The notion of _sparsity_ is a major focus of interpretability in machine learning and statistical modeling (Tibshirani, 1996; Rudin et al., 2022). Typically, sparsity is measured _globally_, such as the number of variables in a model, or as the number of leaves in a decision tree (Murdoch et al., 2019). Global sparsity is relevant in many situations, but it is less relevant for individuals subject to the model's decisions. Individuals care less about, and often do not even have access to, the global model. For them, _local_ sparsity, or **decision sparsity**, meaning the amount of information critical to _their own_ decision, is more consequential.

An important notion of decision sparsity has been established in the work of Sun et al. (2024), which defined the Sparse Explanation Value (SEV), in the context of binary classification, as the number of factors that need to be changed to a reference feature value in order to change the decision. In contrast to SEV, counterfactual explanations tend not to be _sparse_ since they require small changes to many variables in order to reach the decision boundary (Sun et al., 2024). Instead, SEV provides sparse explanations: consider a loan application that is denied because the applicant has many delinquent payments. In that case, the decision sparsity (that is, the SEV) would be 1 because only a single factor was required to change the decision, overwhelming all possible mitigating factors. The framework of SEV thus allows us to see sparsity of models in a new light.

Prior to this work, SEV had one basic definition: it is the minimal number of features we need to set to their reference values to flip the sign of the prediction. The reference values are typically defined as the mean of the instances in the opposite class. This calculation is easy to understand, but somewhat limiting because the reference could be far in feature space from the point being explained and the explanation could land in a low density area where explanations are not credible. As an example, for the loan decision for a 21 year old applicant, SEV could create a counterfactual such as "Changing the applicant's 3-year credit history to 15 years would change the decision." While this counterfactual is valid, faithful, and sparse, it is _not close_ because the distance between the query point and the counterfactual is so large (3 years to 15 years). In addition, this explanation is not _credible_ because the proposed changes to the features lead to an unrealistic circumstance - 6-year-olds do not typically have credit. That is, the counterfactual does not represent a typical member of the opposite class.

Lack of credibility is a common problem for many counterfactual explanations (Mothil et al., 2020; Wachter et al., 2017; Laugel et al., 2017; Joshi et al., 2019). Therefore, in this work, we propose to augment the SEV framework by adding two practical considerations, _closeness_ of the reference point to the query, and _credibility_ of the explanation, while also optimizing _decision sparsity_.

We propose three ways to create close, sparse and credible explanations. The first way is to create multiple possibilities for the reference, one at the center of each cluster of points (Section 4.1). Having a finite set of references keeps the references _auditable_, meaning that a domain expert can manually check the references prior to generating any explanations. By creating references spread throughout the opposite class, queries can be assigned to closer references than before. Second, we allow the references to be flexible, where their position can be shifted slightly from a central location in order to reduce the SEV (Section 4.4). The third way pertains to decision tree classifiers, where a reference point is placed on each opposite-class leaf, and an efficient shortest-path algorithm is used to find the nearest reference (Section 4.2). Table 1 shows a query at the top, and some SEV calculations from our methods below, showing feature values that were changed within the explanation.

In addition to developing methods for calculating SEV, we propose two algorithms to optimize a machine learning model to reduce the number of points that have high SEV without sacrificing predictive performance in Section 5, one based on gradient optimization, and the other based on search. The search algorithm is exact. It uses an exhaustive enumeration of the set of accurate models to find one with (provably) optimal SEV.

Our notions of decision sparsity are general and can be used for any model type, including neural networks and boosted decision trees. Decision sparsity can benefit any application where individuals are subject to decisions made from predictive models - these are cases where decision sparsity is more important than global sparsity from the individual perspectives.

## 2 Related Work

The concept of SEV revolves around finding models that are simple, in that the explanations for their predictions are sparse, while recognizing that different predictions can be simple in different ways (i.e., involving different features). In this way, it relates to (i) instance-wise explanations (iii) local sparsity optimization Models, which seek to explain and provide predictions of complex models. We further comment on these below.

**Instance-wise Explanations.** Prior work has developed methods to explain predictions of black boxes (e.g., Guidotti et al., 2018; Ribeiro et al., 2016, 2018; Lundberg and Lee, 2017; Baehrens et al., 2010) for individual instances. These explanations are designed to estimate importance of features, are not necessarily faithful to the model, and are not associated with sparsity in decisions, so they are fairly distant from the purpose of the present work. Our work is on tabular data; there is a multitude of unrelated work on explanations for images (e.g., Apicella et al., 2019, 2020) and text (e.g., Lei et al., 2016; Li et al., 2016; Treviso and Martins, 2020; Bastings et al., 2019; Yu et al., 2019, 2021). More closely related are _counterfactual explanations_, also called inverse classification (e.g., Mothil et al., 2020; Wachter et al., 2017; Lash et al., 2017; Sharma et al., 2024; Virgolin and Fracaros, 2023; Guidotti et al., 2019; Poyiadzi et al., 2020; Russell, 2019; Boreiko et al., 2022; Laugel et al., 2017; Pawelczyk et al., 2020). Counterfactual explanations are typically designed to find the closest instance to a query point with the opposite prediction, without considering sparsity of

   & External & Numsatis- & NetFraction & PercentGrades & NUMFeature \\  & RiskEstimate & factoryTrades & RevolvingBurden & NeverDelq & CHANGED \\ 
**Query** & 69.00 & 10.00 & 117.01 & 90 & \\ 
**SEV\({}^{1}\)** & **72.65** & **21.47** & **22.39** & 90 & 3 \\
**SEV\({}^{F}\)** & **78.00** & 10.00 & **9.00** & 90 & 2 \\
**SEV\({}^{[rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@gray@stroke{0}@color@gray@fill{0}}\)** & **81.00** & **26.00** & **12.00** & 90 & 3 \\
**SEV\({}^{T}\)** & 69.00 & 10.00 & 117.01 & **100** & 1 \\  

Table 1: An example for a query in the FICO Dataset with different kinds of explanations, SEV\({}^{1}\) represents the SEV calculation with one single reference using population mean, SEV\({}^{[rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@gray@stroke{0}@color@gray@fill{0}}\) represents the cluster-based SEV, SEV\({}^{F}\) represents the flexible-based SEV. SEV\({}^{T}\) represents the tree-based SEV The columns are four features.

the explanation. However, extensive experiments (Delaney et al., 2023) indicate that these "closest counterfactuals" tend to be unnatural for humans because the decision boundary is typically in a region where humans have no intuition for why a point belongs to one class or the other. For SEV, on the other hand, reference values represent the population commons, so they are intuitive. Thus, SEV has two advantages over standard counterfactuals: its references are meaningful because they represent population commons, and its explanations are _sparse_.

Local Sparsity Optimization ModelsWhile there are numerous prior works on developing post-hoc explanations, limited attention has been paid to developing models that provide sparse explanations. We are aware of only one work on this, namely the Explanation-based Optimization (ExpO) algorithm of Plumb et al. (2020) that used a neighborhood-fidelity regularizer to optimize the model to provide sparser post-hoc LIME explanations. Experiment in Appendix K in our paper shows that ExpO is both slower and provides less sparse predictions than our algorithms.

## 3 Preliminaries and Motivation

The Sparse Explanation Value (SEV) is defined to measure the sparsity of individual predictions of binary classifiers. The point we are creating an explanation for is called the _query_. The SEV is the smallest set of feature changes from the query to a reference that can flip the prediction of the model. When we make a change to the query's feature, we _align_ it to be equal to that of the reference point. The reference point is a "commons," i.e., a prototypical point of the opposite class as the query. In this section, we will focus on the basic definition of SEV, the selection criteria for the references, as well as three reference selection methods.

### Recap of Sparse Explanation Values

We define SEV following Sun et al. (2024). For a specific binary classification dataset \(\{_{i},y_{i}\}_{i=1}^{n}\), with each \(_{i}^{p}\), and the outcome of interest is \(y_{i}\{0,1\}\). (This can be extended to multi-class classification by providing counterfactuals for every other class than the query's class.) We predict the outcome using a classifier \(f:\{0,1\}\).

Without loss of generality, in this paper, we are only interested in queries predicted as positive (class 1). We focus on providing a sparse explanation from the query to a _reference_ that serves as a population commons, denoted \(\). Human studies (Delaney et al., 2023) have shown that contrasting an instance with prototypical instances from another class provides more intuitive explanations than comparing it with instances from the same class. Thus, we define our references in the opposite class (negative class in this paper). To calculate SEV, we will align (i.e., equate) features from query \(_{i}\) and reference \(}\) one at a time, checking at each time whether the prediction flipped. Thinking of these alignment steps as binary moves, it is convenient to represent the \(2^{p}\) possible different alignment combinations as vertices on the boolean hypercube. The hypercube is defined below:

**Definition 3.1** (SEV hypercube).: A SEV hypercube \(_{f,i,}\) for a model \(f\), an instance \(_{i}\) with label \(f(_{i})=1\), and a reference \(\), is a graph with \(2^{p}\) vertices. Here \(p\) is the number of features in \(_{i}\) and \(_{v}\{0,1\}^{p}\) is a Boolean vector that represents each vertex. Vertices \(u\) and \(v\) are adjacent when their Boolean vectors differ in one bit, \(\|_{u}-_{v}\|_{0}=1\). \(0\)'s in \(_{v}\) indicate the corresponding features are aligned, i.e., set to the feature values of the reference \(\), while 1's indicate the true feature value of instance \(i\). Thus, the actual feature values represented by the vertex \(v\) is \(_{i}^{,v}:=_{v}_{i}+(-_{v})\), where \(\) is the Hadamard product. The score of vertex \(v\) is \(f(_{i}^{,v})\), also denoted as \(_{f,i,}(_{v})\).

The SEV hypercube definition can also be extended from a hypercube to a Boolean lattice as they have the same geometric structure. There are two variants of the Sparse Explanation Value: one gradually aligns the query to the reference (SEV\({}^{-}\)), and the other gradually aligns the reference to the query (SEV\({}^{+}\)). In this paper, we focus on SEV\({}^{-}\):

   & Type & Housing & Loan & Education & \(Y\)(Risk) \\   & query & Rent & \(>\)10K High & School & High \\  & SEV\({}^{-}\) &  &  &  &  \\  & Explanation & & & & \\  & reference & & & & \\ 

Table 2: Calculation process for SEV\({}^{-}\) = 1

Figure 1: SEV Hypercube

**Definition 3.2** (\(^{-}\)).: For a positively-predicted query \(_{i}\) (i.e., \(f(_{i})=1\)), the Sparse Explanation Value Minus (\(^{-}\)) is the minimum number of features in the query that must be aligned to reference \(\) to elicit a negative prediction from \(f\). It is the length of the shortest path along the hypercube to obtain a negative prediction,

\[^{-}(f,_{i},):=_{\{0,1\}^{p}}\|-\|_{0}_{f,i,}()=0.\]

Figure 1 and Table 2 shows an example of \(^{-}\)=1 in a credit risk evaluation setting. Since \(p=3\), we construct a SEV hypercube with \(2^{3}=8\) vertices. The red vertex \((1,1,1)\) corresponds to the query. The dark blue vertex at \((0,0,0)\) represents the negatively-predicted reference value. The orange vertices are predicted to be positive, and the light blue vertices are predicted to be negative. To compute \(^{-}\), we start at \((1,1,1)\) and find the shortest path to a negatively-predicted vertex. On this hypercube, \((0,1,1)\) is closest. Translating this to feature space, this means that if the query's housing situation changes from renting to the reference value "owning," it would be predicted as negative. This means that **\(^{-}\) is equal to 1** in this case. The feature vector corresponding to this closest vertex \((0,1,1)\), is called the **\(^{-}\) explanation** for the query, denoted by \(_{i}^{,}\) for reference \(\).

### Motivation of Our Work: Sensitivity to Reference Points

Since \(^{-}\) is determined by the path on a SEV hypercube and each hypercube is determined by the reference point, the \(^{-}\) is therefore sensitive to the selection of reference points. Adjusting the reference point trades off between _sparsity_ (according to \(^{-}\)) and _closeness_ (measured by \(_{2}\), \(_{}\)(see Section 6.1) or \(_{0}\) (see Section 6.4) distance between the query and its assigned reference point). Note that this trade-off exists because \(^{-}\) tends to be small when the reference is far from the query. More detailed explanations, visualizations, and experiments are shown in Appendix B.

Selecting References.The reference must represent the commons, meaning the negative population, and the generated explanations should represents the negative populations as well. Moreover, the negative population may have subpopulations; e.g., Diabetes patients may have higher blood glucose levels, while hypertension patients have higher blood pressure. To have meaningful coverage of the negative population, in this work, we consider _multiple_ references, placed _within the various subpopulations_. This allows each point in the positive population to be closer to a reference. Let \(\) denote possible placements of references. For query \(_{i}\), an individual-specific reference \(_{i}\) for \(_{i}\) is chosen based on three criteria: it should be nearby (i.e., close), and should provide a sparse and reasonable explanation. That is, we are looking to minimize the following three objectives over placement of the reference \(_{i}\):

\[\|_{i}-_{i}\|,_{i}\] (1)

\[^{-}(f,_{i},_{i}),_{i} \] (2)

\[-P(_{i}^{,_{i}}|X^{-}),\] (3)

with the constraint that the references obey auditability, meaning that domain experts are able to check the references manually, or construct them manually. The function \(^{-}(f,_{i},_{i})\) in (2) represents the \(^{-}\) computed with the given function \(f\), query \(_{i}\), and the individual-specific reference \(_{i}\) for generating the hypercube. \(_{i}^{,_{i}}\) is the sparse explanation for the sample \(_{i}\), and \(P(|X^{-})\) in the definition of credibility represents the probability density distribution of the negative population and \(P(_{i}^{,_{i}}|X^{-})\) is the density of the negative distribution at \(_{i}^{,_{i}}\). If \(P(_{i}^{,_{i}}|X^{-})\) is large, \(_{i}^{,_{i}}\) is in a high-density region.

## 4 Meaningful and Credible SEV

We now describe cluster-based SEV, which improves closeness at the expense of SEV, and its variant, tree-based SEV, which improves all three objectives and computational efficiency. We also present methods to improve the credibility and sparsity of the explanations.
This approach creates multiple references for the negative population. A clustering algorithm is used to group negative samples, and the resulting cluster centroids are assigned as references. A query is assigned to its closest cluster center:

\[}_{i}_{}\|_{i}-\|_{2}\]

where \(\) is the collection of centroids obtained by clustering the negative samples. We refer to the SEV\({}^{-}\) produced by the grouped samples as cluster-based SEV, denoted SEV\({}^{\]

Figure 2: Cluster-based SEV

Figure 3: SEV\({}^{T}\) Preprocessing

preprocessing. We traverse each of these, and the minimum distance among these is the \(^{-}\). This is described in Algorithm 3 in Appendix E and illustrated in Figure 4. Note that we actually would traverse to each negative node because some internal decisions might not need to be changed along the path. In the example in Figure 4, we change the split at node 3, and use the value that the query already has for the split at node 6, landing in node 10, so \(^{-}\) is 1 not 2.

Table 3 walks through the calculation again, using the names of the features (hypertension, diabetes, etc.). On the first action line, the decision path to the query is 3 - 6 - 10. That means we check 1 and 3 for negative paths, yielding path LL. We flip node 3 (change Hyperlipidemia to 'yes') and follow the LL path. We do not change Obesity to get to the negative node, so we record the \(^{T}\) as 1 in that row. In our implementation, we simply stop when we reach an \(^{T}\)=1 solution, but we will continue in order to illustrate how the calculation works. We go up to node 1 and repeat the process for the LR and LLR paths. Those both have \(^{T}\)=2.

Note that the reference can be any point \(x\) within the leaf; if the leaf is defined by thresholds such as \(3<x_{1}<5\) and \(x_{2}>7\), then any point satisfying those conditions is a viable reference. Given a query, the algorithm flips some of its feature values to satisfy conditions of a leaf with the opposite prediction. Since any point in the leaf is a viable reference, we could choose the median/mean values of points in the leaf as the references, or a more meaningful value. That choice will not influence the fast calculation of SEV-T.

### Improving Credibility for All SEV Calculations

As we mentioned in Section 3.2, the credibility objective encourages explanations to be located in high-density region of the negative population. Previous \(^{-}\) definitions focus on sparsity and closeness objectives, but did not consider credibility. It is possible to increase credibility easily while constructing an explanation: if the explanation veers out of the high-density region, we continue walking along the SEV hypercube during SEV calculations. Specifically, we continue moving towards the reference until the vertex is in a high-density region. Since the reference is in a high-density region, walking towards it will eventually lead to a high-density point. The tree-based SEV explanations automatically satisfy high credibility:

**Theorem 4.3**.: _With a single sparse decision tree classifier \(DT\) with support at least \(S\) in each negative leaf, the \(^{T}\) explanation for query \(_{i}\) always satisfies credibility at least \(}\), where \(N^{-}\) is the total number of negative samples._

This theorem can be easily proved because \(^{-}\) explanations generated by \(^{T}\) are always the negative leaf nodes (which are the references), and the references are located in regions with support at least \(S\) by assumption.

### Flexible Reference SEV: Improving Sparsity

From Section 3.2, we know that queries further from the decision boundary tend to have lower \(^{-}\). Based on this, we introduce Flexible Reference \(\) (denoted \(^{F}\)), which moves the

   & Action &  Hyper- \\ tension \\  & Diabetes &  Hyper- \ \\  & Obesity &  Have \\ Stroke \\  &  \# of changed \\ condition \\ (SEV) \\  \\   **Instance** \\ **1**\(\)**3**\(\)**7** \\  & **node 1000** & 1000** & 3 & No & Yes & No & Yes & Yes 10 & \\   **Flip at** \\ **node 3** \\  & **Check LL** & No & Yes & Yes & No 1000 & 1 \\  **Flip at** \\ **node 11** \\  & **Check LR** & Yes & No & & No 5 & 2 \\  **Flip at** \\ **node 12** \\  & **Check LR** & Yes & Yes & No & No 5 & 2 \\  **2**\(\)**5** \\ **Check LLR** \\  & **Yes** & Yes & No & No 5 & 2 \\ 
 **2**\(\)**4** \\  & **Flip at 10** & (Unchanged) & Flip at 4 & & \\  

Table 3: Illustration of \(^{T}\) calculation.

reference value slightly in order to achieve a lower value of the model output \(f(})\) given a reference \(}\), and the decision function for classification \(f()\), which, in turn, is likely to lead to lower \(^{-}\). The optimization for finding the optimal reference is: \(^{*}*{arg\,min}_{}f()\|- }\|_{}_{F}\) where the \(*{arg\,min}\) is over reference candidates that are near the original reference value \(}\). The flexibility threshold \(_{F}\) represents the flexibility allowed for moving the reference within a ball. We limit flexibility so the explanation stays meaningful. Since it is impractical to explore all potential combinations of feature-value candidates, we address this problem by marginalizing. Specifically, we optimize the reference over each feature independently. The detailed algorithm for calculating Flexible Reference SEV, denoted \(^{F}\), is shown in Algorithm 1 in Appendix D. In Section 6.2, we show that moving the reference slightly can sometimes reduce the SEV, improving sparsity.

## 5 Optimizing Models for \(^{-}\)

Above, we showed how to calculate \(^{-}\) for a fixed model. In this section, we describe how to train classifiers that optimize the average \(^{-}\) without loss in predictive performance. We propose two methods: minimizing an easy-to-optimize surrogate objective (Section 5.1) and searching for models with the smallest SEV from a "Rashomon set" of equally-good models (Section 5.2). In what follows, we assume that \(^{-}\) was calculated prior to optimization, that reference points were assigned to each query, and that these assignments do not change throughout the calculation.

### Gradient-based SEV Optimization

Since we want to minimize expected test \(^{-}\), the most obvious approach would be to choose our model \(f\) to minimize average training \(^{-}\). However, since SEV calculations are not differentiable and they are combinatorial in the number of features and data points, this would be intractable. Following Sun et al. (2024), we instead design the optimization objective to penalize each sample where \(^{-}\) is more than 1. Thus, we propose the loss term:

\[_{-}(f):=}_{i=1}^{n^{+}}( _{j=1,,p}f((-_{j})_{i}+_{j}}_{i}),\;0.5),\]

where \(_{j}\) is the vector with a 1 in the \(j^{th}\) coordinate and 0's elsewhere, \(n^{+}\) is the number of queries, and the reference point \(}_{i}\) is specific to query \(_{i}\) and chosen beforehand. Intuitively, \(((-_{j})_{i}+_{j}}_{i})\) is the function value of query \(_{i}\) where its feature \(j\) has been replaced with the reference's feature \(j\). \(_{j=1,,p}f((-_{j})_{i}+_{j}}_{i})\) chooses the variable to replace that most reduces the function value. If the \(^{-}\) is 1, then when this replacement is made, the point now is on the negative side of the decision boundary and \(f\) is less than 0.5, in which case the \(\) chooses 0.5. If \(^{-}\) is more than 1, then after replacement, \(f\) will still predict positive and be more than 0.5, in which case, its value will contribute to the loss. This loss is differentiable with respect to model parameters except at the "corners" and not difficult to optimize.

To put these into an algorithm, we optimize a linear combination of different loss terms,

\[_{f}_{}(f)+C_{1}_{-}(f)\] (4)

where \(_{}\) is the Binary Cross Entropy Loss to control the accuracy of the training model and \(\) is a class of classification models that estimate the probability of belonging to the positive class. \(_{-}\) is the loss term that we have just introduced above. \(C_{1}\) can be chosen using cross-validation. We define **All-Opt\({}^{-}\)** as the method that optimizes (4). Our experiments show that this method is not only effective in shrinking the average \(^{-}\) but often attains the minimum possible \(^{-}\) value of 1 for most or all queries.

### Search-based SEV Optimization

As defined in Section 4.2, our goal is to find a model with the lowest average \(^{-}\) among classification models with the best performance.

The Rashomon set (Semenova et al., 2022; Fisher et al., 2019) is defined as the set of all models from a given class with performance approximately that of the best-performing model. The first method that stores the entire Rashomon set of any nontrivial function class is called TreeFARMS (Xin et al., 2022), which stores all good sparse decision trees in a data structure. TreeFARMS allows us to optimize multiple objectives over the space of sparse trees easily by enumeration of the Rashomon set to find all accurate models, and a loop through the Rashomon set to optimize secondary objectives. We use TreeFARMS and search through the Rashomon set for a model with the lowest average \(^{-}\):

\[_{f_{}}}_{i=1}^{n^{+}} ^{T}(f,_{i}),\]

where the Rashomon set is \(_{}\), and where we use \(^{T}\) as the \(^{-}\) for each sparse tree in the Rashomon set. Recall that Algorithms 2 and 3 show how to calculate \(^{T}\). We call this search-based optimization as **TOpt**.

## 6 Experiments

Training DatasetsTo evaluate whether our proposed methods would achieve sparser, more credible and closer explanations, we present experiments on seven datasets: (i) UCI Adult Income dataset for predicting income levels (Dua and Graff, 2017), (ii) FICO Home Equity Line of Credit Dataset for assessing credit risk, used for the Explainable Machine Learning Challenge (FICO, 2018), (iii) UCI German Credit dataset for determining creditworthiness (Dua and Graff, 2017), (iv) MIMIC-III dataset for predicting patient outcomes in intensive care units (Johnson et al., 2016, 2016), (v) COMPAS dataset (Jeff Larson and Angwin, 2016; Wang et al., 2022) for predicting recidivism, (vi) Diabetes dataset (Strack et al., 2014) for predicting whether patients will be re-admitted within two years, and (vii) Headline dataset for predicting whether the headline is likely to be shared by readers (Chen et al., 2023). Additional details on data and preprocessing are in Appendix A.

Training ModelsFor \(^{}\), we trained four baseline binary classifiers: (i, ii) logistic regression classifiers with \(_{1}\) (L1LR) and \(_{2}\) (L2LR) penalties, (iii) a gradient boosting decision tree classifier (GBDT), and (iv) a 2-layer multi-layer perceptron (MLP), and tested its performance with \(^{F}\) added, and the credibility rules added. In addition, we trained All-Opt\({}^{-}\) variants of these models in which the SEV penalties described in the previous sections are implemented. For \(^{T}\) methods, we compared tree-based models from CART, C4.5, and GOSDT (Lin et al., 2020; McTavish et al., 2022) with the TOpt method proposed in Section 5.2. Details on training the methods is in Appendix F.

Evaluation MetricsTo evaluate whether good references are selected for the queries, we evaluate sparsity and closeness (i.e., similarity of query to reference). For **sparsity**, we use the average number of feature changes (which is the same as \(_{0}\) norm) between the query and the explanation; for **closeness**, we use the median \(_{}\) norm between the generated explanation and the original query as the metric for \(^{}\). For tree-based models, we use only \(^{T}\) as the metric since \(^{T}\) and \(_{0}\) norm are equivalent; for **credibility**, we need some way of estimating \(P(|X)\) since we cannot observe it directly, so we trained a Gaussian mixture model on the negative samples of each dataset, and used the mean log-likelihood of the generated explanations as the metric for \(^{}\) and \(^{F}\), for TOpt, since it has already been a sparse decision tree, then we don't need to calculate the credibility.

### Cluster-based SEV shows improvement in credibility and closeness

Let us show that \(^{}\) provides improved explanations. Here, we calculated the metric for different \(^{}\) variants, \(^{}\) and \(^{+F}(^{}\) with flexible reference), and compared to the original \(^{1}\), where \(^{1}\) is defined as the \(^{-}\) calculation with single reference generated by the mean value of each numerical feature and mode value of each categorical feature of the negative population, as done in the original SEV paper (Sun et al., 2024) under various datasets and models.

Figure 4(a) shows the relationship between sparsity and variants, the scatter plot between mean \(^{-}\) and mean \(_{}\) for each explanation generated by different variants. We find that \(^{}\) **improves closeness**, which was expected since the references were designed to be closer to the queries. Interestingly, \(^{}\) sometimes has lower decision sparsity than \(^{1}\). \(^{}\) was designed to trade off \(^{-}\) for closeness, so it is surprising that it sometimes performs strictly better on both metrics, particularly for the COMPAS, Diabetes, and German Credit datasets.

Interestingly, we also find that even though we do not optimize credibility for our model, Figure 4(b) shows that \(^{}\) improves credibility, particularly for the Adult, German, and Diabetes datasets by plotting the relationship between mean \(^{-}\) and mean log-likelihood of the generated explanations. It is reasonable since the references are the cluster centroids for the negative samples, so the explanations are more likely to be located in the same high-density area. More detailed values for those methods and metrics are shown in Appendix H.

### Flexible Reference SEV can improve sparsity without losing credibility

In Section 4.4, we proposed the flexible reference method for sparsifying \(^{-}\) explanations, which moves the reference slightly away from the decision boundary. The blue points in Figure 4(a) and 4(b) have already shown that with small modification of the reference, the credibility of the explanations is not affected. Figure 5(a) shows how \(^{-}\) and credibility change as we increase flexibility; \(^{-}\) sometimes substantially decreases while credibility is maintained.

### \(^{-}\) provides the sparsest explanation compared to other counterfactual explanations

Recall that \(^{-}\) flips features of the query to values of the population commons. This can be viewed as a type of counterfactual explanation, though typically, counterfactual explanations aim to find the minimal distance from one class to another. In this experiment, we compare the sparsity of \(^{-}\) calculations to that of baseline methods from the literature on counterfactual explanations, namely Watcher (Wachter et al., 2017), REVISE (Joshi et al., 2019), Growing Sphere (Laugel et al., 2017), and DiCE (Mothil et al., 2020).

Figure 5(b) shows sparsity and credibility performance of all counterfactual explanation methods on different datasets under \(_{2}\) logistic regression (other information, including \(_{}\) norms for counterfactual explanation methods, is in Appendix G). All \(\) variants are in warm colors, while competitors are in cool colors. \(^{-}\) methods have the sparsest explanations, followed by DiCE. (A comparison of \(^{-}\) to DiCE is provided by Sun et al. (2024).) We point out that this comparison was made on

Figure 5: Explanation performance under different models and metrics. We desire lower \(^{-}\) for sparsity, lower \(_{}\) for closeness and higher log likelihood for credibility (shaded regions)

Figure 6: (a) Sparsity and Credibility as a function of the change of flexibility level (0 to 5%/10%/20%) under different models and datasets (b) The median log-likelihood and number of features within different counterfactual explanations. Points at the upper left corner are desired.

[MISSING_PAGE_EMPTY:10]