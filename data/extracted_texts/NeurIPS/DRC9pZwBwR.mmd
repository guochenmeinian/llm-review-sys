# Recursive Introspection: Teaching Language Model Agents _How_ to _Self-Improve_

Yuxiao Qu\({}^{1}\), Tianjun Zhang\({}^{2}\), Naman Garg\({}^{3}\), Aviral Kumar\({}^{1}\)

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)UC Berkeley, \({}^{3}\)MultiOn

Website: https://cohenqu.github.io/rise.github.io/, Email: yuxiaoq@andrew.cmu.edu

###### Abstract

A central piece in enabling intelligent agentive behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially. In this paper, we develop **RISE:**_Recursive IntroSpEction_, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation and offline reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models, without disrupting one-turn abilities as a result of expressing more complex distributions.

## 1 Introduction

A promising approach to utilizing foundation models, and in particular, large language models (LLMs), is to treat them as general-purpose decision-making machines, or "agents". To be successful, an LLM agent must not just provide plausible completions for input text, but must exhibit interactive, goal-directed behavior to accomplish a given task. Put in abstract terms, this requires mastering two qualities: **(a)** producing responses that explicitly seek information about the task, followed by **(b)** making decisions and improving them by "thinking" and verifying them at inference time. For instance, to succeed in using a new coding library, an effective LLM agent should first synthesize programs, then try the most promising subset against a compiler, use the resulting feedback to improve the program, and repeat the process for multiple turns. Having the ability to successfully improve a response in sequential attempts is equivalent to a form of "self-improvement", at test time.

To enable test-time self-improvement, recent approaches attempt to repurpose the knowledge stored in pre-trained models via few-shot prompting . Although prompt tuning in conjunction with feedback is effective in eliciting improved responses from capable models, it fails to produce models that can succeed in complex tasks by correcting their own mistakes, such as those that require logical reasoning . In many of these problems, models contain the "knowledge" needed to answer a challenging prompt, but fail to elicit that knowledge even when asked to sequentially correct their mistakes. Fine-tuning the LLM on domain-specific question-answering data  can help,but it still does not teach the agent a test-time improvement strategy (see Section 5). A strategy for improving responses over sequential attempts at test time is crucial for tackling challenging prompts, where directly attempting the problem in one shot may largely be futile.

**Can we train models to be capable of improving their _own_ responses?** If done correctly on a diverse set of problems and scenarios, this could introduce in an LLM, a general procedure for _"how"_ it can tackle a hard prompt by improving itself as opposed to supervising it with "what" to respond with, which may not generalize as the test prompt becomes out of distribution. Although one approach to inducing this capability into a model would be to generate data that showcase improvements over multiple sequential turns (potentially from highly capable models), we find that simply imitating these data is not sufficient to enable this capability (Section 5.3). This is due to two reasons: First, multi-turn data from a different model would not show improvements in the kinds of errors the learner would make, thereby being irrelevant to the learner . Second, often sequential multi-turn data collected from proprietary models is also not of high quality since these models are typically not good at proposing meaningful improvements to their own errors  even though they can still provide useful responses to the problem at hand. Therefore, we need a different strategy to endow models with a self-improvement capability. Our key insight is to supervise improvements to the learner's own responses in an iterative fashion, taking inspiration from methods in online imitation learning  and reinforcement learning (RL) . This supervision can be in the form of oracle responses to the prompt sampled i.i.d. from more capable models, or be generated from the learner itself.

Our contribution, **RISE: Recursive Introspection** (Figure 1), utilizes these insights to improve the self-improvement capability of an LLM over multiple attempts at a given prompt. In each iteration, our approach bootstraps on-policy rollouts from the learner with better responses at the next turn obtained by running best-of-N (using a success indicator on the task) on multiple revision candidates obtained by sampling from the learner itself or using responses from a more capable model, whichever is more convenient. In this way, we construct rollouts that demonstrate the learner how it can improve its responses under its own distribution. Then, we fine-tune the learner on these data using a reward-weighted regression (RWR [36; 37]) objective, that learns from both high- and low-quality parts of such rollouts. By iteratively repeating this procedure, we are able to instill a general self-improvement capability into an LLM. Our results show that LLMs trained via RISE can produce correct responses on more prompts, improving over turns for more challenging prompts.

Even though strong base and instruction-tuned LLMs [25; 63] often fail to improve their responses over multiple sequential attempts (even when explicitly told about their mistakes previously), **RISE** successfully endows similarly-sized LLMs with self-improvement capabilities, resulting in monotonically increasing task performance after each turn. Specifically, on the GSM8K  dataset, RISE demonstrates significant improvement over various models. RISE improves the performance of LLaMa3-8B by 8.2% and Mistral-7B by 6.6%, entirely using their own data. RISE attains a 17.7% improvement for LLaMa2-7B over the course of 5-turn introspection (outperforming parallel sampling from the first turn), and a 23.9% improvement for Mistral-7B. In contrast, GPT-3.5 itself only improves by 4.6% over five turns. We see similar trends on the MATH dataset , where RISE improves LLaMa2-7B by 4.6% and Mistral-7B by 11.1% over five turns. These results consistently demonstrate RISE's effectiveness in enhancing reasoning capabilities for different models.

## 2 Problem Setup and Preliminaries

The goal of our work is to improve LLM performance over sequential attempts / turns at a given problem. Concretely, given a dataset \(=\{(_{i},_{i}^{*})\}_{i=1}^{N}\) of problems \(_{i}\) and oracle responses \(_{i}^{*}\)

Figure 1: _Recursive Introspection (RISE)._ Using multi-round training on on-policy rollouts and supervision from a reward function, RISE trains models that are capable of improving themselves over multiple turns. At inference, we run majority voting on candidate outputs from different turns to obtain the final response.

our goal is to obtain an LLM \(_{}(|[,}_{1:t},p_{1:t}])\) that, given the problem \(\), previous model attempts \(}_{1:t}\) at the problem, and auxiliary instructions \(p_{1:t}\) (e.g., instruction to find a mistake and improve the response; or additional compiler feedback from the environment) solves a given problem as correctly as possible. To this end, we encode this goal into the following learning objective:

\[_{_{}}\ _{i=1}^{L}_{,^{*},}_{i}_{}(|[,}_{1:i-1},p_{1:t-1} ])}[(}_{i}==^{*})].\] (2.1)

Unlike standard supervised fine-tuning that trains the model \(\) to produce a single response \(}\) given \(\), Equation 2.1 trains \(\) to also appropriately react to a given history of responses from its own previous attempts \(}_{1:i-1}\). Equation 2.1 most closely resembles an RL objective, and we will indeed develop our approach by converting a single-turn problem into a multi-turn MDP. Finally, note that prompting-based methods such as Self-Refine  can still be viewed as training \(\) to optimize \((^{*}|)\) but only when only allowed to modulate the prompt \(p_{i}\) to optimize Equation 2.1. Naturally, since the parameters \(\) are unchanged, this would not be effective in optimizing the objective fully.

## 3 RISE: Recursive Introspection for Self-Improvement

Since even strong off-the-shelf models do not exhibit an effective ability to improve themselves when provided with sequential attempts at a given problem , a natural next step is to ask how to train models to induce this capability. In this section, we will develop our approach, **RISE**, for fine-tuning foundation models towards improving their own predictions over multiple turns. Our approach will first convert a problem into a multi-turn MDP, then collect data, and finally run offline reward-weighted supervised learning in this multi-turn MDP to induce this capability.

### Converting Single-Turn Problems into a Multi-Turn Markov Decision Process (MDP)

The first step in building our approach is to procedurally construct a multi-turn MDP out of a single-turn dataset of prompts and oracle responses (Figure 2, Left). Given a dataset, \(=\{(_{i},_{i}^{*})\}\), consisting of prompts \(_{i}\) and corresponding oracle responses \(_{i}^{*}\) (e.g., math questions and natural language responses to those questions), we will construct an _induced_ MDP \(\) from \(\), and then learn policies in this MDP. An initial state in this MDP is a possible prompt \(_{i}\). We denote the output response from the foundation model as action \(\). Given a state \(\), the next state can be obtained by concatenating the tokens representing \(\) with the action \(\) proposed by the model, and an additional fixed prompt \(\) that asks the model to introspect, e.g., _"this response is not correct, please introspect and correct your answer."_ (the exact prompt is shown in Appendix F.4). The reward function is a sparse binary indicator of answer correctness at a given state \(\), \(r([_{i},],)=1\) if and only if \(=_{i}^{*}\) and is obtained from an answer checking function. This construction from dataset \(\) to MDP \(\) is shown below:

\[=\{(_{i},_{i}^{*})\}\ \ \ : (_{0})=(_{1},_{2},, _{N})\] (3.2) \[P(^{}|,)=(^{}= [,,])\] (3.3) \[r(,)=(=_{i}^{*}_{i} ).\]

Figure 2: _Left: Problem formulation._ We convert single-turn problems into multi-turn MDPs as discussed in Section 3.1. The state is given by the prompt, history of prior attempts, and optional feedback from the environment. An action is a response generated from the LLM given the state of multi-turn interaction so far _Right: Data collection._ We collect data by unrolling the current model \(k-1\) times followed by an improved version of the response, which is obtained by either (1) **self-distillation**: sample multiple responses from the current model, and use the best response, or (2) **distillation**: obtain oracle responses by querying a more capable model. In either case, RISE then trains on the generated data.

### Learning in the Multi-Turn MDP

With the MDP construction in place, the next step involves training a model to improve itself over the course of a rollout. We subscribe to an offline approach to learning that we describe in the following.

**Step 1: Data collection for self-improvement.** To ensure that rollout data from this multi-turn MDP is useful for teaching the model how to self-improve, it must satisfy a few desiderata: **(1)** it must illustrate the mistakes that the learner is likely to make and showcase how to improve upon them in the next attempt, **(2)** the data must illustrate responses that are relevant to the model given the problem and previous attempts in context, and **(3)** it must not contain any rollout that degrades in a subsequent turn. Our data collection strategy (Figure 2, Right) satisfies these desiderata.

In a given round \(k\), for a given problem \(_{i}\), we unroll the _current_ model \(_{_{k}}(|)\) to produce multiple sequential attempts, denoted by \(_{i}^{i}_{_{k}}(|_{i}^{i})\). In problems, where external input (e.g., compiler feedback) is available, we also observe a variable-length, natural language external input, \(f_{i}^{i}\) (e.g., in math problems we ask the model to correct itself). We also observe a scalar reward value \(r(_{i}^{i},_{i}^{i})\), denoted as \(r_{i}^{i}\) in short. Let us denote this dataset of "on-policy" model rollouts as \(_{}:=\{(_{t}^{i},_{t}^{i},f_{t}^{i},r_{ t}^{i})_{t=1}^{T}\}\).

For each time-step, we construct an improved version of the response \(_{t}^{i}\) that we will denote by \(}_{t}^{i}\). We also record the reward score associated with this improved response as \(r(_{t}^{i},}_{t}^{i})\), or \(_{t}^{i}\) in short. To obtain an improved version of a response \(_{t}^{i}\), we can employ several strategies. Perhaps the most straightforward approach is to query an off-the-shelf more capable model to provide a correct response given the prompt \(_{i}\), the previous response \(_{t}^{i}\), and an optional external feedback \(f_{t}^{i}\). We refer to this as the **distillation** variant of our approach, since it uses a strong "teacher" model to guide self-improvement (note that this is different from the classic notion of knowledge distillation, and we will in fact show results in Section 5.1 that will help understand the differences).

\[}_{}:=\{\{(_{t}^{i}, }_{t}^{i},f_{t}^{i},_{t}^{i})\}_{t=1}^{| |}..\] (3.4)

The second variant of our approach, which alleviates the need for a teacher model, involves constructing an improved response by sampling multiple times from the learner itself. We refer to this approach as the **self-distillation** variant. Concretely, for each state in the dataset, \(_{t}^{i}_{}\), we sample \(N\) responses \(}_{t}^{i},}_{t}^{i},,}_{t}^ {i}[N]_{}(|_{t}^{i})\), and use the best response from these \(N\) candidates (as measured by the associated reward values \(_{t}^{i},,_{t}^{i}[N]\)) to relabel the model response at the next step \(t+1\) in an improvement trajectory. Formally, say \(}_{t}^{i}[m]=*{arg\,max}_{j[N]}r(_{i}, }_{t}^{i}[j])\), then we label the responses in the dataset \(_{}\) at step \(t+1\) with the improved response and its associated reward value \(_{t}^{i}[m]\):

\[}_{}:=\{\{(_{t +1}^{i},}_{t}^{i}[m],f_{t+1}^{i},_{t}^{i}[m]) \}_{t=0}^{T-1}\}_{i=1}^{||}.\] (3.5)

**Step 2: Policy improvement.** With the aforementioned data construction schemes, we can now train a model on these datasets. While in general, any offline RL approach can be used to train on these data, in our experiments we adopt an approach based on weighted supervised learning  due to ease of experimentation and its simplicity. In particular, we perform a weighted supervised regression, where the weights are given by the exponential transformation of the reward values in \(}\).

\[_{}\ \ _{_{i} }}[_{t=1}^{T}_{}(}_{t}^{i} |_{t}^{i})(r_{i}^{t}/)],\] (3.6)

where \(\) is a temperature parameter to further expand or narrow the difference between good and bad actions. In our preliminary experiments, we found that Equation 3.6 can often induce a bias towards increasing log likelihoods of responses where rewards are high, prioritizing updates on easy problems where rewards are already high. To address this issue, we apply a slight modification to Equation 3.6 and center the exponentiated rewards around the mean value averaged across all attempts on a given prompt, akin to advantage-weighted regression . We find that the use of advantages in place of rewards helps us avoid the "rich-gets-richer" phenomenon with easy problems.

### Inference at Deployment Time

RISE can be run in two modes at inference time. The most straightforward way to run the policy \(_{}(|)\) trained by RISE is within a multi-turn rollout, where the model samples a new responseconditioned on the past context (i.e., state in the multi-turn MDP). This past context consists of the external feedback \(p_{i}^{}\) concerning the response \(_{i}^{}\) and the rollout terminates as soon as the current response is judged to be correct according to the environment's answer verification function. In other words, we terminate the rollout as soon as the reward equals to the reward for the oracle response: \(r(,_{i}^{})=r(,^{*})\). This protocol queries the reward function after each turn in the rollout. Since several reward function queries are performed, we refer to this approach as **"with oracle"**.

RISE can also be run in a mode that avoids the need to query the answer checker or the reward function within a rollout. In this case, we run full-length rollouts by forcing the model to retry, ignoring the correctness of the response. We then utilize a self-consistency mechanism  based on majority voting to decide the candidate response at the end of each turn. Concretely, at the end of each turn \(j\), we identify the response by running a majority vote over all response candidates from the previous turns (\((_{1}^{},_{2}^{},, _{j}^{})\)), including turn \(j\). We call this **"without oracle"**. A schematic illustration of these approach is shown in Figure 3. Most of our evaluations use no oracle.

At iteration \(k\), since the agent is able to improve its own response from \(j\) to \(j+1\) when \(j k\), to avoid test time distribution shift, in both modes, we use a size \(k\) shift window to store the most recent conversation history when the turn number \(j\) is larger than the iteration number \(k\).

### Practical Algorithm and Implementation Details

A complete algorithmic pseudocode for each approach is shown in Appendix D. We trained 7B models via RISE and found that these models often could not adhere to response style and instructions for improving their responses when generating on-policy data. As a result, before running on-policy data collection, we find it often useful to run an initial phase of supervised fine-tuning on in-domain, multi-turn rollouts generated from a capable model to provide style and instruction-following information to the learner. We call this the "knowledge boosting" stage. We then run on-policy rollouts starting from a boosted model. In each iteration, we generate 1 trajectory for each unique problem. We then run fine-tuning, with hyperparameters and details in Appendix F. For iterative fine-tuning, we find that starting from the _base_ model but training on data from all iterations thus far is more beneficial than continued fine-tuning from the checkpoint obtained in the previous iteration.

## 4 When and Why is Self-Improvement Over Turns Possible?

A natural question to ask is why self-improvement with **RISE** even possible. One might surmise that the model may simply not have enough knowledge to correct its _own_ mistakes if it is unable to correctly answer the problem in the first turn. Then, why is it possible to teach the model to correct its own mistakes? In this section, we provide the reason why this kind of self-improvement is possible, supported with empirical evidence to justify our hypotheses.

Iteratively teaching a model how to make updates on a given response can be crucial when representing the target distribution \(p^{*}(|)\) requires more capacity than what the model \(_{}\) affords by conditioning on only the input prompt tokens. When the target distribution requires greater capacity, learning a sequence of conditionals, \(_{}(_{i+1}|,_{0:i})\) followed by marginalization is expected to induce a more flexible marginal distribution over \(_{T}\) given \(\). This hypothesis is akin to the difference between

Figure 3: _RISE Inference._ There are two ways to query the model trained via RISE upon inference: **(1) with oracle (_Left_): each time the model improves its response, it is allowed to check its answer against an environment and terminate early as soon as a correct answer is found; or **(2) without oracle (_Right_): we ask the model to sequentially revise its own responses j times, and perform majority voting on all candidate outputs from different turns to obtain the final response. If the turn number \(j\) is larger than the iteration number \(k\), the agent only keeps the most recent history with \(k\) interactions to avoid test-time distribution shift.

diffusion models  and variational autoencoders (VAEs)  in image generation: iteratively fitting a sequence of generative distributions over intermediate noisy inputs in a diffusion model gives rise to a more flexible distribution  than monolithic variational auto-encoding, even though diffusion models still utilize an evidence lower-bound objective(ELBO). While the diffusion process utilizes hand-designed noise schedules, RISE utilizes the base model itself to induce iterative improvements.

To verify this hypothesis, we tracked the training un-weighted, negative log-likelihood loss (NLL) values for the oracle response \(^{*}\) given the input prompt \(\) marginalized over intermediate steps in a multi-turn rollout, and compared it against the NLL values \(- p_{}(^{*}|)\) attained by directly attempting to predict the final response in Figure 4 (Left, labeled as "Classic"). Concretely, we sampled 256 prompts \(\) and their oracle responses \(^{*}\) and computed the average \(- p_{}(^{*}|)\) across all \(\), along with a 95% confidence interval for different checkpoints during training. We find that for any number of epochs (including fractional number of epochs on the x-axis), the NLL value is lower when conditioning on multi-turn data that RISE generates compared to oracle responses to the prompts obtained from an expert. This suggests that RISE is able to utilize the computation of tokens from previous turns to model the target distribution. We also measure the average NLL loss on all samples through training, sampled i.i.d. from the training dataset for RISE and classic fine-tuning and observe a similar trend: RISE reduces loss more than the standard approach, attaining lower perplexity values (Figure 4 Right).

For knowledge-based question answering, it is not possible for the model to produce any meaningful improvements because learning \(p^{*}(|)\) is not bounded by insufficient capacity of \(_{}(|)\), but is rather unable to match \(p^{*}\) due to the absence of features that are critical to learn the correct mapping from \(\) to \(\). Training with RISE would likely incentivize hallucinations , since more input tokens appearing from previous attempts would only provide easier ways to pick up on spurious correlations. However, this is not the failure mode on reasoning problems , where maj@K rates at turn 1 tend to be higher than pass@1 as we find in our experiments (indicating that performance can be improved by sampling the model itself). Figure 5 demonstrates that RISE's sequential procedure solves many problems unsolved by pass@B for larger \(B\) in the first turn, indicating it learns to access the model's pre-trained knowledge differently, rather than merely converting pass@K to pass@1 performance like most single-turn approaches.

## 5 Experimental Evaluation

The goal of our experiments is to demonstrate the efficacy of RISE in instilling language models with the ability to self-improve their responses over turns. Our experiments answer the following questions: **(1)** How effectively can RISE improve performance over multiple sequential attempts (i.e., turns) at a given prompt?; **(2)** Does the performance of RISE improve with more rounds of iterative training?; **(3)** Does the self-improvement strategy induced by RISE generalize to novel problems that are out of the training domain? and finally; **(4)** What is the best data composition for training RISE? To this end, we compare RISE to other prior and baseline approaches, and perform ablations on GSM8K , MATH . We defer RISE's performance on code generation tasks to Appendix B.3. Additional analyses are presented in Appendix C, where we examine the out-of-distribution generalization capabilities of RISE and explore the model's behavior in correct-to-correct scenarios. We also present a detailed study of weak-to-strong generalization, demonstrating how RISE-generated data from weaker models can enhance the performance of stronger models.

**Baselines, comparisons, and evaluation.** We compare RISE to several prior methods that attempt to induce similar self-improvement capabilities: **(a) self-refine** that prompts a base model to critique and revise its mistakes; **(b) GloRE**, which trains a separate reward model to locate

Figure 4: _Left: The probability of the true answer given the prompt._ Observe that model trained with RISE has higher probability for the true answer. **Right: The training perplexity (loss) of fitting only the oracle answer or a sequence of answers.** Note that fitting a sequence of answers (RISE) reduces the loss more than fitting the oracle answer (Classic).

Figure 5: _Fraction of problems unsolved by pass@B at first turn that sequential 5-turn RISE sampling solves,_ where \(B=5 k\) (\(k\) is the x-axis). RISE can solve several challenging problems that sampling at the first turn with much larger budgets cannot solve.

errors and a refinement model to improve responses of a base LLM; and**(c) self-consistency**, which runs majority voting on multiple responses from the first turn as a baseline to compare to our sequential strategy. We tried to construct fair comparisons between RISE and these methods using a similar-sized model , but differences in base model, training data, and evaluation setups still prohibits us from performing an apples-to-apples comparison in some cases. Nonetheless, we can still understand the ballpark of improvement by contextualizing our results with these prior works. We also compare to V-STaR , but since this is not an fair comparison, we defer it to Appendix B.1.

We evaluate RISE in both modes at inference time: with and without an oracle (Section 3.3) at the end of five turns (the performance for each turn is in Appendix B.2). Concretely, these metrics are defined as follows: **(1) with oracle, "p1@t5"**: this run terminates the rollout as soon as the response is correct. In other words, this metric allows queries to the final answer verifier at the end of each turn, and **(2) without oracle, "m1@t5"**: this run sequentially sample five responses, and we compute the maj@1 performance on the candidates produced in each turn as detailed in Section 3.3. We also compare maj@K performance at the first turn for all the models we train.

### Does RISE improve performance over multiple turns compared to other approaches?

**Main results.** We present the comparisons in Table 1. First, note that RISE ("Iteration 1" and "Iteration 2") boosts up the LLama2 base model's five-turn performance by 15.1% and 17.7% respectively with each iteration on GSM8K and 3.4% and 4.6% on MATH, w/o any oracle. Interestingly, we found using prompting-only self-refine  largely degrades performance across the board, even with a strong proprietary model, GPT-3.5. The strongest 7B base models, Mistral-7B and Eurus-7B-SFT , when coupled with standard prompting, are only able to improve their performance, but only by 5.3% / 11.6% and 0.9% / 4.0% respectively on GSM8K and MATH, which is significantly lower than our approach. The performance of GLoRE improves only by 3.4% on GSM8K (over two turns), but this is still lower than our approach, which improves by 6.3% in two turns and 13.4% in three turns (see Appendix B.1). This indicates that RISE is effective in teaching models how

    &  &  \\  & **w/o oracle** &  &  &  &  \\  & m1@t1 & m\(\)s@t1 & m\(\)s@t1 & m\(\)s@t5 & p1@t5 & m1@t1 & m\(\)s@t1 & m1@t5 & p1@t5 \\ 
**RISE (Ours)** & & & & & & & & & \\ Llam2 Base & 10.5 & 22.8 (+12.3) & 11.1 (+0.6) & 13.9 (+3.4) & 1.9 & 5.1 (+3.2) & 1.4 (+0.5) & 2.3 (+0.4) \\ +Boost & 32.9 & 45.4 (+12.5) & 39.2 (+6.5) & 55.5 (+22.6) & 5.5 & 6.8 (+1.3) & 5.5 (0.6) & 14.6 (+9.1) \\ +Iteration 1 & 35.6 & 49.7 (+14.1) & 50.7 (+15.1) & 63.9 (+28.3) & 6.3 & 8.8 (+2.5) & 9.7 (+3.4) & 19.4 (+13.1) \\ +Iteration 2 & 37.3 & 51.0 (+13.7) & 55.0 (+17.7) & 68.4 (+3.1) & 5.8 & 10.4 (+4.6) & 10.4 (+6.6) & 19.8 (+14.9) \\ 
**SFT on oracle data** & & & & & & & & \\ Only correct data & 27.4 & 42.2 (+14.9) & 34.0 (+6.6) & 43.6 (+16.2) & 5.8 & 7.9 (+2.1) & 5.5 (+0.3) & 12.1 (+6.2) \\ Correct and incorrect & 25.7 & 41.8 (+16.1) & 31.2 (+5.5) & 41.5 (+15.8) & 5.0 & 5.2 (+0.2) & 5.0 (+0.0) & 13.1 (+8.1) \\ 
**RISE (Ours)** & & & & & & & & \\ Mistral-7B & 33.7 & 49.4 (+15.7) & 39.0 (+5.3) & 46.9 (+13.2) & 7.5 & 13.0 (+5.5) & 8.4 (+0.9) & 13.0 (+5.5) \\ + Iteration 1 & 35.3 & 50.6 (+15.3) & 59.2 (+23.9) & 68.6 (+33.3) & 6.7 & 9.5 (+2.8) & 18.4 (+11.3) & 29.7 (+22.4) \\ 
**7B SoTA** & & & & & & & & \\ Eurus-7B-SFT & 36.3 & 66.3 (+30.0) & 47.9 (+11.6) & 53.1 (+6.8) & 12.3 & 19.8 (+7.5) & 16.3 (+4.0) & 22.9 (+10.6) \\ 
**Self-Refine** & & & & & & & & \\ Base & 10.5 & 22.4 (+11.9) & 7.1 (+3.4) & 1.3 (+2.5) & 1.9 & 5.1 (+3.2) & 1.9 (+0.0) & 3.1 (+1.2) \\ +Iteration 2 & 37.3 & 50.5 (+13.2) & 33.3 (+4.0) & 44.5 (+5.2) & 5.8 & 9.4 (+3.6) & 5.7 (+0.1) & 9.5 (+3.7) \\ GPT-3.5 & 66.4 & 80.2 (+13.8) & 61.0 (+5.4) & 71.6 (+5.2) & 39.7 & 46.5 (+6.8) & 36.5 (+3.2) & 46.7 (+7.0) \\ Mistral-7B & 33.7 & 48.5 (+14.8) & 21.2 (+12.5) & 79.4 (+2.4) & 7.5 & 12.3 (+4.9) & 7.1 (+4.0) & 11.4 (+13.9) \\ Eurus-7B-SFT & 36.3 & 65.9 (+29.6) & 26.2 (+10.1) & 42.8 (+6.5) & 12.3 & 19.4 (+7.1) & 9.0 (+3.3) & 15.1 (+2.8) \\ 
**GoRE** & & & & \(\)m1@t3 & \(\)p1@t3 & & & & \\ +ORM & 48.2 & 49.5 (+13.1) & 57.1 (+8.9) & & & & & \\ +SORM & 48.2 & 51.6 (+34.4) & 59.7 (+11.5) & & & & & \\ +Direct & 48.2 & 47.4 (+0.8) & 59.2 (+11.0) & & & & & \\   

Table 1: **RISE vs. other approaches (Self-Refine, GLoRE) and baselines.** Observe that RISE attains the biggest performance improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH. This performance gap grows larger when oracle early termination is allowed (p1@t5 w/ oracle). Self-Refine  degrades performance across the board when used without an oracle, and attains minor performance improvements with an oracle. GLoRE trains a separate refinement model, but performs worse than RISE; more details about it are in Appendix B.1. Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT . Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that algorithmic design choices in RISE are crucial for performance. Color coding indicates numbers that can be compared.

to improve their own errors. **To summarize,** training with RISE gives the largest performance improvement gains compared to other approaches both with and without the use of an oracle, and these gains are transferred to other base models.

One might also hypothesize that the performance gains with RISE here are largely a result of utilizing queries to an off-the-shelf more capable model for providing supervision and not the algorithmic approach for data collection and training. To address this hypothesis, we store all the data generated by RISE from more capable models and train on this data via standard single-turn SFT (**"SFT on oracle data**). Since not all of this data are guaranteed to be correct, we also run this experiment on only the correct responses in these oracle data. Observe in Table 1 that this procedure does not still instill self-improvement capabilities, largely preserving or degrading sequential (**"maj@1@turn5"**) performance compared to simply sampling one response in the first turn. This means that the algorithmic design of RISE is critical, as opposed to the use of expert supervision.

#### 5.1.1 Can RISE Effectively Make Use of Mistakes and Correct Them?

One concern that arises from prior results on self-refinement or self-correction is whether the model can truly correct itself over turns or whether the improvement comes from the effect of sampling more answers and picking the best one. In Table 1, we see that sequentially improving responses via RISE (**"maj@1@turn8"**) outperforms sampling 5 responses in parallel at the first turn and applying a majority vote on them (**"maj@5@turn1"**). Please note that this comparison utilizes an equal number of samples, with the only difference being that these samples are drawn in parallel at the first turn in one case and sequentially at the end of five turns in the other. Comparing maj@5 performance at the end of 1 turn and 5 turns, we observe a consistent 4% to 8% improvement on GSM8K and an 6.5% improvement on MATH (with Mistral-7B model). This means that RISE can imbue models with a self-improvement ability, while running parallel sampling alone on any model cannot endow the same ability. Even the **maj@5@turn1** performance of standard single-turn SFT on the data used by RISE is substantially worse than the sequential **maj@1@turn5** performance of RISE, implying that the algorithmic protocol of RISE plays a critical underlying role. Finally, we also remark that in Figure 5, we showed that the sequential procedure learned by RISE over five turns could solve a significant fraction of problems that were unsolved by pass@B for much larger values of \(B 5\) in the first turn, implying that sequential RISE can actually tackle prompts that were not solvable by simply sampling more responses in the first turn. One might speculate if these improvements in sequential improvement ability largely come at a cost of reduced improvements in first turn performance. We also observe that running multiple iterations of RISE still preserves the first turn performance while improving the 5-turn performance.

#### 5.1.2 How Does the Base Model Affect RISE?

The performance of RISE with Llama2-7B on an absolute scale is lower than the best models specifically fine-tuned on math data (e.g., Eurus-7B-SFT or Mistral-7B). However, we find that RISE is still effective on top of Mistral-7B base model. In fact, _our performance at the end of five turns outperforms one of the best 7B SFT models, customized to math reasoning_. Compare the \(\) performance of Eurus-7B-SFT and Mistral-7B in RISE (ours), to find that Mistral-7B + RISE outperforms Eurus-7B-SFT.

#### 5.1.3 Self-Distillation Version of RISE

We compare the performance of RISE with self-generated data and supervision (Equation 3.4, \(N=16\)) after one iteration directly on top of more capable models: Mistral-7B and Llama-3-8B on GSM8K in Table 2, without any knowledge boosting phase. We find that this variant also improves the 5-turn performance of the base model compared to the first turn: compare "m1@t5" vs "m1@t1" for both the models Llama-3-8B and Mistral-7B, where RISE boosts the sequential self-improvement performance by more than 1% compared to turn 1 performance w/o any oracle.

Of course, this version of RISE does not outperform the "m5@t1" performance of the fine-tuned model. We expect this to be largely a function of one single iteration of training. Since the self-distillation version of RISE utilizes best-of-N sampling against the same model to produce supervision

    &  & **w/ oracle** \\   & \(\) & \(\) & \(\) & \(\) & \(\) \\   & 33.7 & 49.4 (+15.7) & 39.0 (+5.3) & 46.9 (+13.2) \\ + Iteration 1 & 36.8 & 44.4 (+7.6) & 39.5 (+6.6) & 48.7 (+15.9) \\  Llama-3-8B & 45.3 & 69.7 (+4.4) & 52.5 (+7.2) & 61.0 (+10.5) \\ + Iteration 1 & 65.6 & 80.7 (+15.1) & 73.8 (+8.2) & 81.2 (+15.6) \\   

Table 2: _RISE with self-distillation on GSM8K._ RISE is able to improve 5-turn maj@1 performance of the model with entirely self-generated data and supervision, despite the fact that the base Mistral-7B model does not produce correct answers for several problems.

for self-improvement, RISE would first have to match the performance of best-of-N sampling before it can start to improve over it via reward maximization. Due to the significant gap between the base model's m5@t1 and m1@t5 performance, we expect that this will take quite a few iterations or a fully online RL algorithm. We did not have computational resources and infrastructure to run multiple iterations, but this is an interesting avenue for future work. In this self-distillation setting, we could also divide the computation between sequential and parallel sampling strategies to get the best results at the end of five turns. Nonetheless, this result shows that even by training on self-generated samples, RISE can actually amplify the sequential sampling performance of the base model.

### Does the Performance of RISE Improve with Iterative Training?

Next, we attempt to understand if RISE improves with multiple rounds of training on on-policy data. As shown in Tables 1 and 2, the performance of RISE improves from iteration to iteration constantly. The 5-turn performance of RISE, both with and without an oracle, exhibits a clear improvement with more rounds. This implies that iterative self-training procedures of the form of STAR  can also be combined with RISE to train models for self-improvement. This also perhaps serves as a strong hint towards the potential utility of full online reinforcement learning (RL) techniques.

### What Data Compositions and Data Quantity are Crucial for RISE?

We now study how different data compositions affect the performance of RISE with the goal of answering questions such as _should we collect on-policy error correction data like DAgger  or should we bias towards high-quality off-policy data?_. To understand the utility of different data compositions, we enlist the three aspects RISE: **(a)** the use of multi-turn rollout data for fine-tuning, **(b)** the use of unsuccessful / suboptimal rollouts via weighted supervised fine-tuning compared to naive supervised learning, which only utilizes successful rollouts for fine-tuning; and **(c)** the use of on-policy rollouts and self-generated or oracle data. We will now perform controlled experiments to understand the effect of each of these factors on the overall performance of RISE.

**(a) Data composition for fine-tuning.** We first study the necessity of using the interaction of error correction history in RISE in Figure 6 (Left). We compare two approaches: model trained with oracle answers shown right after the query ("1-turn") and oracle answers shown after intermediate failed attempts ("Multi-turn") in Figure 6 (Left). Even though the latter trains on intermediate responses that may not always be correct, it attains a higher performance than training on the correct response for a given prompt. This highlights the importance of training on contexts with a multi-turn interaction history depicting mistakes from the learner to improve self-improvement capabilities.

**(b) Weighted supervised learning vs unweighted supervised learning.** We examine reward-weighted RL's impact on multi-turn data in RISE as opposed to simply imitating filtered successful data. We find that using all the data leads to improved performance over simply filtering good datain Figure 6 (Right), which reduces sample size. In Figure 6 (Left), we find that reward-weighted training improves performance on later turns, allowing us to better leverage all the sub-optimal data.

**(c) On-policy vs off-policy data; self-generated vs. expert data.** RISE runs on-policy rollouts and seeks improvements on responses that the learner produces. As shown in Figure 9 (Left), a "DAgger "-style approach that seeks improvements on responses appearing in on-policy rollouts improves performance (green/orange) compared to using expert data alone (blue/pink). Conceptually, this addresses the train-test mismatch between the distribution of context tokens, enabling imitation learning methods to now target the correct distribution. In addition, recent work  has shown that LLMs often memorize "unfamiliar" examples generated by oracle models; by training on on-policy rollouts, we should be able to eliminate any such potential issues. Thus, while the model trained via offline imitation is able to reduce loss, these improvements do not generalize to new problems.

Figure 6: _Left: The importance of multi-turn interaction history and weighted objectives for training RISE._ Note that training with multi-turn data leads to better self-improvement performance at the end of 5 turns, than one-turn data from the original dataset with oracle answers from another model; also observe that using a weighted objective performs better. _Right: The importance of using all rollouts for learning_, instead of only successful rollouts or only successful responses in the data. Using all data performs best in our results.

### Error Analysis of RISE over Turns

Following the protocol of Huang et al. , in this section, we perform an error analysis of the improvement performed by RISE (without any oracle feedback) to understand how the fraction of incorrect and correct responses changes over turns, when **no oracle** is used for early termination. We demonstrate this in the form of Venn diagrams in Figure 7. First note that there is a consistent increase in the portion of problems that stay correct and a consistent decrease in the portion of problems that stay incorrect, which means that the model is able to answer more and more problems as we increase the number of turns. Second, there is a consistent decrease in the number of problems that change from being correct to incorrect, which is often also not the case for strong proprietary LLMs such as GPT in Huang et al. . We also note that there is a decrease in the total number of incorrect problems that become correct in the subsequent turn, but this is a direct consequence of a shrinkage in the size of the incorrect response set as more problems become correct over turns. This indicates that one can induce "intrinsic" self-improvement (per the terminology of Huang et al. ) via fine-tuning with RISE, even though no external environment input is provided during evaluation.

**Qualitative examples.** We also inspect several examples from the GSM8K test set to qualitatively understand the behavior of RISE over turns and observe different behavior patterns, that we show in Appendix E. For instance, the trained model may choose to completely rewrite its previous response if it is totally incorrect in order to get to the correct answer or make small edits if the previous response is mostly correct. Another interesting pattern we note is that the model implicitly has the ability to locate errors in previous responses and only refine the erroneous steps. Additionally, the model is tolerant of noisy environmental feedback when there is no oracle-assisted early termination.

## 6 Discussion, Future Directions, and Limitations

We presented RISE, an approach for fine-tuning LLMs to be able to improve their own responses over multiple turns sequentially. RISE prescribes an iterative RL recipe on top of on-policy rollout data, with expert or self-generated supervision to steer self-improvement. RISE significantly improves the self-improvement abilities of 7B models on reasoning tasks (GSM8K and MATH), attaining an improvement over turns that previous work  has not observed in strong proprietary models. In addition, RISE outperforms prior approaches that attempt to tackle similar problems of refinement and correction, while being simpler in that it does not require running multiple models and works well with just one model.

Despite these good results, there are still many open questions and limitations. Due to computational constraints, we were not able to perform more than two iterations of training with RISE, and no more than one iteration when the supervision comes from the learner itself. Improving with self-generated supervision will likely require more computation and more iterations, since it will be slower than when using an off-the-shelf expert model. RISE requires running manual iterations and hence, a more "online" variant of RISE is likely the solution in the long run, especially when we wish to scale on-policy learning in a data-efficient manner. Additionally, while our work fine-tunes models on one task at a time, it will be certainly interesting to include data from the protocols specified by RISE into general instruction tuning and post-training pipelines. Given the results that fine-tuning on data prescribed by RISE does not hurt the first-turn performance of any model we trained, we hypothesize that adding this sort of data in general instruction-tuning pipelines should not hurt either, while enabling the sequential self-improvement capability that is largely absent from models today.

Figure 7: _Change in the fraction of responses that transition their correctness values over the course of multi-turn rollouts from RISE, w/o oracle._ Observe that in general, the fraction of Correct \(\) Correct responses increases; Incorrect \(\) Incorrect responses decreases; and the fraction of Correct \(\) Incorrect responses also decreases, indicating that RISE (w/o any oracle) is able to iteratively improve its responses.