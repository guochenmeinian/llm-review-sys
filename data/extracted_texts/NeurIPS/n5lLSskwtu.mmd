# Evidential Mixture Machines: Deciphering Multi-Label Correlations for Active Learning Sensitivity

Dayou Yu\({}^{1}\) Minghao Li\({}^{2}\) Weishi Shi\({}^{2}\) Qi Yu\({}^{1}\)

Rochester Institute of Technology, Rochester, NY 14623\({}^{1}\)

University of North Texas, Denton, TX 76203\({}^{2}\)

{dy2507,qi.yu}@rit.edu {minghaoli@my.,weishi.shi@}unt.edu\({}^{2}\)

Corresponding author.

###### Abstract

Multi-label active learning is a crucial yet challenging area in contemporary machine learning, often complicated by a large and sparse label space. This challenge is further exacerbated in active learning scenarios where labeling resources are constrained. Drawing inspiration from existing mixture of Bernoulli models, which efficiently compress the label space into a more manageable weight coefficient space by learning correlated Bernoulli components, we propose a novel model called Evidential Mixture Machines (EMM). Our model leverages mixture components derived from unsupervised learning in the label space and improves prediction accuracy by predicting weight coefficients following the evidential learning paradigm. These coefficients are aggregated as proxy pseudo counts to enhance component offset predictions. The evidential learning approach provides an uncertainty-aware connection between input features and the predicted coefficients and components. Additionally, our method combines evidential uncertainty with predicted label embedding covariances for active sample selection, creating a richer, multi-source uncertainty metric beyond traditional uncertainty scores. Experiments on synthetic datasets show the effectiveness of evidential uncertainty prediction and EMM's capability to capture label correlations through predicted components. Further testing on real-world datasets demonstrates improved performance compared to existing multi-label active learning methods.

## 1 Introduction

Active learning (AL) is a paradigm where we have access to abundant unlabeled data instances with a limited labeling budget . Most AL methods focus on the selection strategy that helps the machine learner achieve better performances with an informed selection of labeled data instances. However, while AL for standard classification tasks has been studied extensively, an important task that is multi-label classification has been largely overlooked. In real-world problems, each data instance may be associated with more than one labels . For machine learning models, the difference between the multi-class problem where only one ground truth label is associated with a data instance and the multi-label classification (MLC) problem is fundamental. Classic solutions either transforming the MLC problem into multiple binary problems or directly build a joint learning problem for all labels. With the binary approach, we lose the common underlying correlations between input features and labels. The number of classifiers may be large due to the large label space, thus the separated training process can be costly. Also, many labels are relatively rare and may depend on other labels, and we can not learn such dependencies when we isolate these labels. With the joint learning approach, the biggest challenge is to combine common labels with rare labels in a balancedway. For the rare labels, there might be few positive data instances. A typical model is likely to optimize the feature embedding according to popular labels or adopt the so-called "shortcut" learning, making a direct connection from the input features to the rare labels more difficult. Although we now have access to common and rare labels at the same time, it might not suffice to promote the learning of their correlations if we directly model the labels from the input features. These unique challenges of the MLC problem become more pronounced in AL settings, where the rare labels become even more scarce. They also increase the difficulty of obtaining high-quality uncertainty estimation, which is often crucial in many AL selection strategies as it allows us to know when the model "does not know" in order to make an educated decision on which data instances to label.

To address these challenges, we draw inspiration from the mixture of Bernoulli model, which can model a large label space with a small number of mixture components. There are existing methods that try to capture the label correlation using such a model [17; 30]. However, to connect with the input features, they either resort to a purely conditional case, namely conditional Bernoulli mixtures (CBM)  or use a conjugate classification head, which is a Gaussian Process model (GP-B\({}^{2}\)M) . For the former, a distinct set of label clusters is predicted for each data instance, meaning that the label correlations are completely separated from the learning tasks. This shortcoming makes the CBM model unsuitable for AL. The latter relies on the Gaussian Process (GP) which outputs the weights of the mixture components. For the intended task, a complete GP is too expensive [20; 16], while a sparse GP has limited predictive ability. Unlike the CBM model, GP-B\({}^{2}\)M creates a set of global label clusters. However, the label prediction for rare labels remains challenging because they can only ever be as good as the best label cluster available. Furthermore, the uncertainty quantification is superficial because it only captures the approximate covariance of the label prediction and the variance of the GP predictions. This point estimate does not fully capture the unknown and is not sufficiently effective in AL.

In this work, we propose to combine the mixture of Bernoulli with a deep evidential model , both incorporating deep learning and enabling more fine-grained uncertainty analysis. The deep learning model needs only a forward pass during prediction time, which is much more efficient than estimating the predictive distribution in a random process model. The uncertainty analysis can now include each evidential prediction. This flexibility is crucial in improving the informativeness evaluation of multi-label data samples as mentioned above. Structurally, the proposed model is composed of a shared encoder that provides embeddings and two decoders that predict the weight coefficients and the proxy pseudo counts correction for the Posterior Beta which models the global label cluster, respectively, as shown in Figure 1. The weight coefficient predictor is trained as a deep evidential regression model, which makes uncertainty-aware predictions of optimal assignments to the mixture of label clusters for each data instance. By using a deep evidential model as the classification head, we achieve the aforementioned improvement of efficiency during inference time. By sharing the encoder, we maintain a close connection beginning from the input features, through the weight coefficients, and to the label clusters. Compared to the CBM model, the proposed Evidential Mixture Machines (EMM) maintains a global label cluster. Compared to the GP-B\({}^{2}\)M model, the EMM model allows

Figure 1: Overview of evidential mixture machines

adjustment to the label clusters based on each data instance. Additionally, the uncertainty information can be captured by both the fine-grained uncertainty decomposition from the evidential posterior parameters and the predicted variability in final labels. The evidential uncertainty decomposition is implemented through a conjugate interpretation of the Normal Inverse Gamma parameters, which gives these parameters a more impactful pseudo count meaning. The label variability is assessed using the covariance approximation in predictions and the discrepancy between the global label cluster and the proxy pseudo counts. Apart from the novel sampling strategy, the model is trained using alternating loops of the evidential training step and a joint label-based training step, which strengthens the ability to accommodate individual samples.

With both the demonstration on synthetic datasets and the AL evaluation on real-world datasets, we show the effectiveness of the EMM model. Our main contribution is threefold:

* integration of evidential learning with multi-label classification through EMM,
* principled uncertainty quantification for active sampling using EMM,
* intensive evaluation showing performance gain by EMM for large label spaces, especially on sparse and rare labels.

## 2 Related Works

**Multi-label classification.** In the realm of multi-label classification (MLC) , Binary Relevance Models (BRMs) have gained widespread use, leading to the development of various active learning (AL) models based on BRMs. Notable examples include employing the estimated reduction of a BRM loss function as an uncertainty criterion for data sampling, as demonstrated by . A large portion of the multi-label active learning work do not require annotators to label all possible labels for a given data instance [25; 37; 7]. The main consideration behind these approaches is the significant reduction in annotator labeling costs. However, this strategy inevitably breaks the inherent connections between labels, making it impossible to comprehensively measure the informativeness of a data instance using label correlations. Additionally, models designed to handle partially labeled data are required, limiting the applicability of such methods. Therefore, in this paper, we will not compare our approach to these methods. Other approaches integrate the properties of the support vectors of individual support vector machines (SVM) within BRMs, using label correlation more as a means to simplify the querying process than to enhance active sampling [29; 32; 8; 9; 10]. While these methods incorporate label correlation to some extent, such as through label inconsistency , label ranking , or learning reularization, they do not systematically capture label correlations, potentially leading to imprecise uncertainty measures in ML-AL contexts.

**Label correlations and multi-label AL.** Some existing models attempt to explicitly capture label correlations or use latent embeddings to facilitate active multi-label sampling. For instance, the CBM model uses the approximate entropy of predicted labels for data sampling , but its dependency on an external multi-class classifier for predicting component coefficients complicates AL due to the challenges in model selection and parameter tuning. Furthermore, CBM, designed primarily for MLC rather than AL, predicts distinct label clusters for each data sample without discovering global label clusters, thus limiting its effectiveness in multi-label AL . Other approaches like correlation-aware method for transfer learning  struggle with large and sparse label spaces due to their reliance on kernel functions for measuring label similarity. Compressed sensing (CS) techniques [34; 31] innovative in learning latent embeddings to capture label correlations but assume labels are drawn from a Gaussian distribution and are not efficient in AL, especially in early stages with limited training data. In , a Bayesian mixture of Bernoulli model is proposed. A set of global label clusters are captured in a Bayesian manner. However, the inference process of the model is complicated and the fixed label clusters limit the predictive ability in the final label space.

**Evidential learning.** Evidential models have been developed to enable fine-grained uncertainty quantification in deep learning (DL) models . These models introduce a higher order conjugate prior distribution over the likelihood distribution, and train the DL model to output the parameters of the higher order distribution . The higher order distribution enables the model to express the fine-grained uncertainty information. Evidential models have been successfully extended to classification [12; 19], regression [1; 21], action recognition , OOD detection , and meta-learning problems . We extend the evidential deep learning framework to our setting that leads to novel fine-grained uncertainty guided active-learning for multi-label classification.

## 3 Methodology

**Problem setting.** In our multi-label active learning problem, the essential task is to predict a multivariate 0-1 label vector \(=(y_{1},...,y_{L})^{}\{0,1\}^{L}\) from the input features \(^{M}\). For AL, we start with a small initial labeled set \(_{L}\), and a large unlabeled pool \(_{U}\) (\(N_{L}=|_{L}| N_{U}=|_{U}|\)). The AL strategy \(():^{M}\) is a function that grades the instance \(^{*}_{U}\) according to its informativeness. The top graded samples will be labeled as a batch \(b_{t}\) and added to \(_{L}\). The building blocks of our proposed method include the Mixture of Bernoulli label clusters and the coefficient predictor which connects the input features to the label clusters.

### Preliminaries

**Mixture of Bernoulli.** The set of label clusters contains \(K\) mixture components. Each mixture component is a \(L\)-variate Bernoulli distribution \(_{k}=_{l=1}^{L}(y_{l};_{kl})\) that captures a local'stereotype' of the complete label distribution, where \((y_{l};_{kl})=_{kl}^{y_{l}}(1-_{kl})^{(1-y_{l})}\) and \(L\) is the total number of labels. The Bernoulli parameter \(_{kl}\) has conjugate prior \((_{kl};a_{kl},b_{kl})\). Initially, the mixture of Bernoulli can be found through label-only learning. Using an EM algorithm, we can learn the initial components \(_{K L}^{(0)}\) from the labeled samples \(_{L}\). These components can model the set of labels \(p(|)=_{k=1}^{K}_{k}_{l=1}^{L}(y_ {l};_{kl})\), where \(_{k}(0,1]\) is the normalized weight coefficient of the component \(k\). However, since this is a label-only process, we are missing the connection to the input features \(\).

**Connecting to input features.** For a conditional model such as CBM, the connection is through \(_{k}=p(_{k}|)\) and \(=()\). The model is still trained in an EM manner so that the predictions \(}\) can be made for each \(\). For a conjugate model such as GP-B\({}^{2}\)M, the connection is through \(_{k}=p(_{k}|_{k}())\), and the variational training process of the conjugate model impacting the posterior \(\). As mentioned before, one issue with CBM is the disconnected prediction models of \(\) and \(\), while the inference process of GP-B\({}^{2}\)M is too expensive. In our proposed model, we also predict both \(}\) and \(}\) from \(\) using two decoder networks \(g_{}()\) and \(g_{}()\). However, we maintain the connection by using a shared encoder network \(e()\), as shown in the structure in Figure 1. Specifically, \(g_{}(e())\) is an evidential model that predicts the distribution of \(\) using the output evidence parameters, which function in the following way.

**Evidential weight coefficient predictor.** We model the connection from input features to label clusters with fine-grained uncertainty information using the evidential regression model. The target for regression is to learn a combination of \(_{k}\) values that best reconstruct the final label predictions. To this end, we place a higher-order Normal Inverse Gamma (NIG) prior \((,^{2}|)=(|,}{})^{-1}(^{2}|,)\) over the regression model's Gaussian output \((|,)\). The evidential model is trained to output the NIG parameters \(=(,,,)\) similar to . In this evidential model, the Gaussian likelihood interacts with the NIG prior, leading to a Student-t predictive distribution:

\[p(|x,)=_{}_{^{2}}p(|x,,^{2})(,^{2}|)^{2}= ;,,2\] (1)

Here, the evidential model predicts coefficients for input \(x\) as: \(=_{p(|x,)}[]=\).

The NIG parameters \(,,,\) are outputs from the predictor branch \(g_{}(e())\). The evidential model, through its higher order NIG prior, can quantify the aleatoric (ALE) and epistemic (EP) uncertainty  as \(=[^{2}]=\),\(=[]=\). In this evidential framework, due to the conjugacy of the NIG prior with the Gaussian likelihood, the posterior is also the NIG distribution. Moreover, in this model, after interacting with \(N\) i.i.d. data points (\(_{1},...,_{N}\)), the posterior NIG parameters update as the observations increase . Through the pseudo-count interpretation, the total evidence is quantified as \(=v++\).

We train the model to maximize The likelihood under the predictive Student-t distribution, which gives the NLL loss:

\[_{} =-(p(_{k}|x,)\] \[=-+ (+)(_{k}-)^{2}+ +()})\]where \(=2(1+)\). Additionally, we want the model's confidence/evidence for the prediction to be low when the prediction is incorrect by introducing a evidence-based regularization \(_{REG}=(_{k}-)^{2}\). The regularization penalizes the highly confident wrong predictions, and ensures model's confidence is rightly placed. The overall loss is

\[_{}=_{}+_{reg}_ {}\] (2)

where \(\) controls the impact of the regularization.

### Bringing Evidential Learning into Multi-Label Active Learning

The integration of an evidential model in our multi-label classification approach presents several distinct advantages, particularly in addressing the inherent complexities of active learning (AL) environments. Firstly, evidential models provide a more nuanced and sophisticated mechanism for uncertainty quantification. This is crucial in AL settings, especially when dealing with sparse and rare labels, where traditional models often struggle. By effectively capturing and quantifying uncertainty, our approach leads to more informed and strategic decisions selecting data instances for labeling, optimizing the use of limited labeling resources. Furthermore, the evidential model facilitates a deeper understanding of the underlying label correlations, enabling the model to make more accurate predictions across a broad spectrum of labels, including the rare ones. This leads to a significant improvement in the overall classification performance, especially in scenarios where conventional methods might overlook subtle but crucial label dependencies. Additionally, the evidential approach inherently enhances the interpretability of the model's predictions, offering insights into the confidence and reliability of these predictions. This aspect is particularly valuable in knowledge-rich domains where understanding the model's decision-making process is as important as the predictions accuracy. Thus, incorporating an evidential model into our multi-label classification framework marks a substantial advancement, offering a robust, efficient, and insightful solution to the challenges posed by large and complex label spaces in active learning scenarios.

### Evidential Mixture Machines

Building upon the tools above, we propose a novel EMM model. Compared to existing methods, our novel contribution to model learning lies in how we connect label clusters to input features and how we learns the final labels in a joint manner. The integration enables evidential uncertainty analysis through both weight coefficient predictions and final label predictions. The evidential learning of \(\) is already explained above. Here, we introduce the joint learning using actual labels \(\).

**Joint multi-label training with label clusters.** Once we have the coefficient predictor branch, we can train the full model to make the final label predictions. We first freeze \(e()\) and \(g_{_{k}}()\) to train \(g_{}()\). Instead of directly letting the network predict \(\) from \(g_{}()\), we make \(g_{}()\) output proxy pseudo counts \(_{kl}\) and \(_{kl}\) to be combined with the initial \((_{K L}^{(0)};a_{kl}^{(0)},b_{kl}^{(0)})\). This ensures that we maintain the correlations encoded in \(_{K L}^{(0)}\). The network outputs of dimension \(2 N K L\) are split into \(\) and \(\) and added to \(a^{(0)}\) and \(b^{(0)}\) in a weighted fashion: \(a_{kl}()=a_{kl}^{(0)}+w_{}_{kl}(),b_{kl}()=b_{kl}^{(0)}+w_{}_{kl}()\). The new Bernoulli parameter for each instance is then computed by \(_{kl}()=a_{kl}()/(a_{kl}()+b_{kl}())\). The label prediction is \(_{l}()=_{k}_{k}()_{kl}()\). The model is trained using a soft margin multi-label loss:

\[_{}=-_{l=1}^{L}y_{l}( _{l})})+(1-y_{l})(_{l})}{1+(\,-_{l})})\] (3)

We then adopt the evidential pseudo-count style update of the label clusters, we would have \(a_{kl}^{(1)}=a_{kl}^{(0)}+_{k=1}^{K}}(_{n})y_{nl}\) and \(b_{kl}^{(1)}=b_{kl}^{(0)}+_{k=1}^{K}}(_{n})(1-y_{nl})\). However, this update only makes the correction based on the predicted weights/\(}\). If we keep updating in this way, the biases build up and the popular labels will be dominant in future components. Thus, we also include a weighted update based on the predicted proxy pseudo-counts, similar to when we make label predictions: \(a_{kl}()=a_{kl}^{(0)}+w_{up}_{kl}(),b_{kl}()=b_{kl}^{(0)}+w_{up}_{kl}()\). This step ensures that our model mutually benefits from the coefficient predictor and the proxy pseudo-count predictor. The initial components are learned unsupervised and do not make up for the training of the predictors. By introducing the joint update, we connect the two predictors more closely.

The joint training step of EMM addresses an important problem with the mixture model formulation, where the model prediction is restricted by the mixture components \(\). Because the weight coefficients\(0_{k} 1\), the label prediction for \(y_{l}\) can only be as great as \(_{k}_{kl}\). If we only make updates to the mixture components using \(a_{kl}^{(1)}=a_{kl}^{(0)}+_{k=1}^{K}}(_{n})y_{nl}\) and \(b_{kl}^{(1)}=b_{kl}^{(0)}+_{k=1}^{K}}(_{n})(1-y_{nl})\), the rare labels will still suffer from the label imbalance which will be reflected in \(_{k}_{kl}=_{k}}{a_{kl}+b_{kl}}\). By incorporating the joint training, we make the prediction based on \(a_{kl}()=a_{kl}^{(0)}+w_{}_{kl}(),b_{kl}()=b_{kl}^{(0)}+w_{}_{kl}()\), allowing the model to better fit the labels using instance-wise predictions \(_{kl}(),_{kl}()\). The soft margin multi-label loss effectively brings the benefits of binary relevance machines into model training because it promotes positive predictions through the multi-label one-versus-rest formulation.

**Complete EMM learning process.** Having established the weight coefficient training step and the joint multi-label training step, we integrate them in a complete learning process. To start with, we have the EM-learned initial clusters \(_{K L}^{(0)}\) and the optimal \(^{(0)}\) which reconstructs the labels when combined with \(_{K L}^{(0)}\). In one learning round, the model first goes through a pre-training stage of \(g_{}(e())\) to fit the set of \(^{(0)}\) optimized for the initial \(_{K L}^{(0)}\). Then, we move on to the joint training stage where we alternate between training the coefficient predictor and jointly training the full model to fit the labels \(\). Each coefficient predictor training step is the same as the pre-training stage, while the joint training step is described above. The model will continually improve the quality of label clusters to maximize the ability to recreate the labels in the joint training step, each time followed by the evidential learning of optimal \(\) to keep up with the new label clusters. The complete training process is presented in Figure 1. At this stage, we have combined the advantages of Bayesian mixture models, deep evidential models, and a bi-level multi-label problem formulation to obtain a powerful multi-label classification model. Next, we introduce how the evidential flavor can provide fine-grained uncertainty information and benefit active learning.

### Active Learning Strategy

In order to select the most informative samples given the small initial budget, we adopt an uncertainty-oriented selection strategy. From the proposed EMM model, we can obtain uncertainty information from three sources: weight coefficient branch, proxy pseudo count predictor, and the final label predictions. The first part of the uncertainty information is from the evidential model that predicts the weight coefficients. The evidential model naturally decomposes into the aleatoric uncertainty \([_{_{k}}^{2}]\) and the epistemic uncertainty \(\). For AL purposes, we should target the samples that give us the most epistemic uncertainty, which can potentially improve the model's knowledge of the unknown. From the weight coefficient perspective, this criterion searches for the least confident samples based on our current model. Selecting these samples will help us quickly gain knowledge of the connection between input features and the weight coefficients, which is the determining factor for predictive performance. Thus, the first part of the selection function is

\[_{_{k}}()=()}{_{k}( )(_{k}()-1)}\] (4)

The second part of the uncertainty information comes from the proxy pseudo counts. We can compare the current components and the updated components when the proxy pseudo counts for \(\) are added, and select the samples that would introduce more difference to the current model: \(_{}()=-(, ^{}())=^{}()}{\|\|\|^{} ()\|}\). Because the label clusters play the most important role in recreating the label space, we should try to capture as much latent label correlation as possible. This requires sufficient exploration in the label space. Selection criterion \(_{}()\) does exactly this by searching for data samples that are potentially the most different from the currently captured label space. The last part of the uncertainty information is computed over the final label prediction. Since the full model is a mixture of Bernoulli, we can compute the expected covariance of the predicted label distribution. Here, we adopt the point estimate as in existing methods:

\[[}|]= _{k}_{k}(((1-))+_{k}_{k}^{})-(}|)\:(}|)^{}\] (5)

Note that ideally we would like to use the conditional entropy \(H[y|x]\) to measure the uncertainty raised due to observing input \(x\). However, the entropy of a mixture random variable is hard (or intractable) to compute because of the sum in the expression. But under mild conditions, we can show that \(y\), as an average of the combination of weights and components, converge to a normal distribution so that the \(cov(y|x)\), which is easy to compute, can be used to quantify the uncertainties in the mixture random variables.

Let \(_{1},.._{K}\) be the i.i.d samples that encapsulates the coefficient-component pairs where \(_{i}=_{i}_{1i}\\...\\ _{i}_{Li}\) with mean \(=m_{1}\\...\\ m_{l}=[_{i}_{1i}]\\...\\ [_{i}_{Li}]\) and covariance \(\). Let \(y_{|x}=_{1}\\...\\ _{L}\) where \(_{L}=_{k=1}^{K}_{k}_{kl}\). Then, according to the Multivariate Central Limit Theorem, we have \(y(}{K},)\). So we can leverage the dominant term in the entropy of multivariant Gaussian, \((|(y|x)|)\) as a surrogate measurement of \(H[y|x]\), since the number of components is relatively large.

The selection score is \(_{}}()=|[}| ]|\). This criterion captures the expected information gain evaluated on the final label predictions when including the unlabeled samples. It aggregates the predictions from the weight coefficient predictor and the proxy pseudo-count predictor, and shapes the fine-grained uncertainty in a global view. By focusing on epistemic uncertainty from the evidential model for weight coefficients, differences in label clusters indicated by proxy pseudo counts, and the covariance in label predictions, we devise a comprehensive multi-source uncertainty-based selection score (MSU) \(()=_{_{k}}()+_ {}()+_{}}()\). Compared to a single uncertainty score, the integration of these uncertainty measures facilitates a targeted exploration of the data space and enables the identification of the most informative samples within a constrained budget.

## 4 Experiments

We conduct experiments on both synthetic and real data to demonstrate the effectiveness of EMM. We use the synthetic data experiment to show how label clusters of the mixture model can capture various label compositions and correlations. We then verify the AUC performance of EMM on real-world multi-label datasets, along with rare label analysis and ablation studies.

### Synthetic Data Experiments

To demonstrate the effectiveness of the EMM model and the MSU sampling strategy, we design a synthetic dataset that can verify each of the proposed functionalities. The synthetic dataset contains mostly geometric feature related labels, along with a few carefully designed labels. The input features of the data points consist of 16 clusters distributed as \(m\)-dimensional Gaussian's. In other words, each point is sampled from one Gaussian cluster. The clusters have different means and universal variances such that they have slight overlaps. The geometric feature related labels indicate which Gaussian the point is sampled from. We denote all the geometric-related labels as \(_{geo}\)'s (all 16 of them) for simplicity. While \(_{geo}\)'s mainly indicate the location of the data samples in feature space and do not carry correlations in themselves, we construct the _labels of interest_ using certain groups of them to ensure complex label correlations that are also feature-rooted. These _labels of interest_ include a rare label **L1**, which has a frequency as low as 5% of a regular label; a couple of highly-correlated labels **L2** and **L3** which share similar features; and **L4**, which only depends on **L2** and **L3** and is dependent on other labels instead of input features. Specifically, **L1** is assigned to data samples randomly with a low probability of 5%. **L2** and **L3** are randomly sampled from the same quarter of the feature space. Then, **L4** is generated following the rule **L4\(=\) L2**\(\) -**L3**. Besides these labels, we also append a set of non-geometry information-based labels \(_{non-geo}\), which prevent the problem from being purely geometry-based. The label composition is shown in Figure 2.

**Capturing label correlations.** In one example of our experiments, we train the EMM with 6 label clusters. Among these clusters, one has a particularly high weight \(_{\{1,L2\}}\) for **L2**. Simultaneously, the weight \(_{\{1,L3\}}\) for **L3** is low while the weight \(_{\{1,L4\}}\) for **L4** is high. Such behavior will ensure that the co-appearing **L2** and **L4** are captured during the prediction process. In the 6-cluster setting,

Figure 2: Visualization of label composition in the synthetic dataset. For visualization we show the geometric clusters in 2D, while they are high dimensional Gaussians in practice.

only \(_{\{1,\}}\) and \(_{\{3,\}}\) have higher weights for **L2**, and in both clusters \(_{\{,L4\}}\) is similarly high as \(_{\{,L2\}}\). Meanwhile, in \(_{\{6,\}}\) we have a high weight \(_{\{3,L3\}}\) for **L3**, in which case \(_{\{3,L4\}}\) for **L4** is low (\(\) 0.01). Furthermore, in cluster 4 \(_{\{4,\}}\), both \(_{\{4,L2\}}\) and \(_{\{4,L3\}}\) have slightly higher values but \(_{\{4,L4\}}\) is low (\(\) 0.01) so **L4** does not falsely appear.

We further test various numbers of components and quantify the label correlations described in those label clusters. The results show that the average \((_{\{,L2\}},_{\{,L4\}})\) is 0.91 meaning that the positive correlation between **L2** and **L4** is always captured. Meanwhile, the average \((_{\{,L3\}},_{\{,L4\}})\) is as low as 0.12 showing the lack of correlation between **L3** and **L4**. More specifically, when neither \(_{\{,L2\}}\) or \(_{\{,L3\}}\) is insignificant, \(R(_{\{,L2\}},_{\{,L4\}})\) drops to 0.31, indicating that the fine relationship of "if and only if" is well-captured by the mixture model.

**Prediction enhancement by proxy pseudo-count combination.** Although the mixture model is great at capturing the label correlations with label clusters, it is not always good at predicting rare labels. For example, in the 6-cluster setting, the largest weight for **L1** is \(_{\{2,L1\}}=0.016\) because of the extremely imbalanced label distribution. In this case, even if the model predicts solely \(_{\{2,\}}\) for a sample \(\) (\(_{2}=1,_{k}=0,k 2\)), the prediction of **L1** is 0.016. This small value creates difficulty in converting the predicted score to a positive label prediction.

**Rare label and correlation discovery by uncertainty quantification.** As for actively selecting data samples, we study the correlations between \(_{_{k}}()\), \(_{}()\), and the true labels of \(\). We show a set of statistics in Table 1 to analyze these behaviors. For \(_{_{k}}()\), we compare the sampling score with the estimated unknown information of the corresponding pool samples. The unknown information is evaluated from both the feature and label perspective, using the feature similarity and the label cardinality. The correlation between the feature similarity and the uncertainty score is -0.73, and the correlation between the label similarity and the uncertainty score is -0.41. From the label similarity we can also conclude the negative correlation between the similarity and uncertainty. For \(_{}()\), we specifically focus on the rare labels. On average, the samples containing less than 3 labels have an uncertainty score 28.8% higher than the other samples and the samples containing rare labels have an uncertainty score 65.8% higher than the regular samples.

### Real Data Experiments

**Datasets and experiment settings.** We conduct AL experiments on representative real-world datasets including Delicious, Bibtex, Corel5k, Enron, and NUS-WIDE, covering multiple application domains . The number of labels ranges from 53 to 156, most of which are relatively rare in the entire dataset. We summarize the datasets and data preprocessing in Appendix D.

**Performance comparison.** We compare the AL performance with competitive AL baselines:

* **GP-B\({}^{2}\)M** uses a Bayesian mixture model and conducts active sampling using the combined predicted variance of multi-output GP and the label clusters .
* **MMC** is model-adaptive (implemented with label ranking model) and involves a predictive process for the number of labels. It samples instances based on the expected loss .
* **Adaptive** Adaptive uses SVM margin and label cardinality inconsistency for data sampling. .

    & \(||3\) & \(|| 3\) & \(_{L1}=1\) & \(_{L1}=0\) \\  Average \(_{_{k}}()\) & 46.5 & 36.1 & 52.9 & 31.9 \\  Average \(_{}()\) & 0.079 & 0.070 & 0.138 & 0.069 \\   

Table 1: The relationship between average uncertainty scores, label cardinality, and rare labels

Figure 3: (a) A visualization of the labels clusters concerning **L2**, **L3**, and **L4**; (b) A visualization of the labels clusters concerning **L1** with and without updating with proxy pseudo-counts. “fixed” is the original unsupervisedly trained \(_{1,L1}\), “updated” is an updated \(_{1,L1}(_{1})\) where \(_{1}(_{1})=0.83\) meaning \(_{\{1,\}}\) is dominant, and “irrelevant” is an updated \(_{1,L1}(_{2})\) where \(_{1}(_{2})=0.18\) meaning this cluster \(_{\{1,\}}\) does not contribute much to the prediction of \(_{2}\).

* **CVIRS** combines the margin based margin ranking and label inconsistency for data sampling .

For AL comparison, we use the area under the ROC curve as the main criterion. We start with \(2\%\) initially labeled samples for datasets Delicious, Bibtex, Core15K, and \(0.03\%\) for NUS-WIDE. The initial labeled set contains a minimum of one positive instance per label to ensure that binary solutions can be trained. For EMM and methods that can perform batch active learning, we sample 5 rounds with 100 samples selected in each round. For single-batch baseline methods, we sample 500 rounds to obtain the same number of labels in the end. The base performance of classification models varies as some baseline methods use binary-SVMs (Adaptive, CVIRS), some use strategy-specific models such as the label-ranking model (MMC) and the GP-B\({}^{2}\)M model.

We also include a configuration EMM-entropy that uses the proposed EMM model and a simple entropy-based selection strategy as an additional baseline, showcasing the performance gain from the proposed sampling (MSU selection) on its own. From Figure 4, we can see that the EMM model makes better predictions using the same amount of initial labels compared to the SVM model, which explains the advantage at the starting point. Although the label ranking model or the GP-B\({}^{2}\)M model may also have good performance at the starting point, they are restricted by specific sampling methods. To separately verify the advantage brought by the uncertainty quantification, we show that our MSU selection always has an advantage in selecting better AL samples compared to a simple uncertainty-based selection (EMM-Entropy).

For a more fine-grained analysis of the model performance, we also compute the average precision improvement . This shows how the rare-label predictions have improved using the EMM model compared to a fully Bayesian mixture model where the label clusters are completely global.

\[_{l}(\%)=_{l}()-_{l}(^{2})}{_{l}(^{2})} 100\%\] (6)

In Figure 5, we show the API metric for the 50 rarest labels on each dataset. The improved API on a label is shown by a blue bar above the \(API=0\) axis, while the worse API performances are shown

Figure 4: Performances on real-world datasets (AU-ROC increases as we sample 5 rounds with 100 samples in each round)

Figure 5: Average precision improvement (API) for rare labels

by the orange bars. The \(x\) axis shows the number of times each label has appeared in the testing set. We can see that EMM has a significant advantage on rarer labels.

**Ablation study.** We conduct an ablation study on model components (weight coefficient and proxy pseudo-count predictors) and the AL sampling method (balancing parameters \(\) and \(\)). Since the proposed EMM model combines the evidential weight coefficient learning and the proxy pseudo-counts learning, we compare the complete model with two weakened configurations:

* **EMM\({}^{-rr}\)** reduces the weight coefficient leaner to a simple ridge regression model, and only combines the prediction with fixed Bernoulli mixtures as the label clusters.
* **EMM\({}^{-fixed}\)** uses the evidential learning of weight coefficients with only the fixed Bernoulli mixtures as the label clusters.

From Figure 6, we can see that the evidential regression model predicts the weight coefficients better than a simple regression model such as Ridge Regression, which shows the effectiveness of the first branch of EMM (\(e()\) and \(g_{}()\)). We can also see that without the proxy pseudo-count updates, the performance is not as good, which shows the effectiveness of the second branch of EMM (\(g_{}()\)) and the joint training of the entire model.

From Figure 4, we can already see that the proposed evidential uncertainty-based sampling outperforms a simple metric such as Entropy. From Figure 7, we can see that our MSU sampling strategy effectively benefited from the multi-source uncertainty information compared to the single-source uncertainty (\(==0\)). However, the epistemic uncertainty from the evidential model is still the most important source of uncertainty as the sampling performance decreases when we increase the balancing parameters for \(_{}()\) and \(_{}}()\) too much.

## 5 Conclusion

In this work, we introduced a novel Evidential Mixture Machines (EMM) model, which integrates deep evidential learning with a multi-label classification framework of AL. This approach effectively tackle a large and sparse label space, particularly addressing the challenges posed by sparse and rare labels. Our model's sophisticated uncertainty quantification and improved prediction accuracy set it apart from traditional Binary Relevance Models and other existing methodologies. The effectiveness of the EMM model is demonstrated through rigorous evaluations on both synthetic and real-world datasets, showcasing its superiority in diverse labeling scenarios. This advancement not only contributes to the development of more efficient MLC methods but also paves the way for future research in this domain. The potential for scaling this approach to larger datasets and adapting it to various domains offers exciting opportunities for further exploration and refinement.

Figure 6: Ablation study on model components

Figure 7: Ablation study on balancing parameters