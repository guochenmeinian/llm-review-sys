# Distribution Learnability and Robustness+

Footnote â€ : Authors are listed in alphabetical order.

Shai Ben-David

University of Waterloo, Vector Institute

shai@uwaterloo.ca

&Alex Bie

University of Waterloo

yabie@uwaterloo.ca

Gautam Kamath

University of Waterloo, Vector Institute

g@csail.mit.edu

&Tosca Lechner

University of Waterloo

tlechner@uwaterloo.ca

###### Abstract

We examine the relationship between learnability and robust (or agnostic) learnability for the problem of distribution learning. We show that learnability of a distribution class implies robust learnability with only additive corruption, but not if there may be subtractive corruption. Thus, contrary to other learning settings (e.g., PAC learning of function classes), realizable learnability does not imply agnostic learnability. We also explore related implications in the context of compression schemes and differentially private learnability.

## 1 Introduction

Distribution learning (sometimes called _density estimation_) refers to the following statistical task: given i.i.d. samples from some (unknown) distribution \(p\), produce an estimate of \(p\). This is one of the most fundamental and well-studied questions in both statistics  and computer science , often equivalent to classic problems of parameter estimation (e.g., mean estimation) in parametric settings. It is easy to see that no learner can meaningfully approximate any given \(p\) without having some prior knowledge. The problem then becomes: assuming the sample generating distribution \(p\) belongs to a given class of distributions \(\), and given parameters \(,(0,1)\), output some distribution \(\) such that with probability at least \(1-\), the statistical distance between \(p\) and \(\) is at most \(\). Specifically, we employ _total variation distance_, the most studied metric in density estimation , using \(d_{}(p,q)\) to denote the distance between distributions \(p\) and \(q\). This case, when \(p\), is often called the _realizable_ setting. If, for some particular class \(\), this is doable with a finite number of samples \(n(,)\), then we say the distribution class is _\((,)\)-learnable_.1 A class is _learnable_ if it is _\((,)\)-learnable_ for every \((,)(0,1)^{2}\). A significant amount of work has focused on proving bounds on \(n(,)\) for a number of classes \(\) of interest - for example, one can consider the class \(\) of all Gaussian distributions \((,)\) in some Euclidean space \(^{d}\).

However, this framework is restrictive in the sense that it requires the unknown distribution to be _exactly_ a member of the class \(\) of interest. This may not be the case for a variety of possible reasons, including some innocuous and some malicious. As one example, while it is a common modelling assumption to posit that data comes from a Gaussian distribution, Nature rarely samples exactly from Gaussians, we consider this only to be a convenient _approximation_. More generally, the class \(\) that the learner assumes can be thought of as reflecting some prior knowledge about the task at hand. Such prior knowledge is almost always only an approximation of reality. Alternatively, we may be in an adversarial setting, where a malicious actor has the ability to modify an otherwise well-behaveddistribution, say by injecting datapoints of their own (known in the machine learning literature as data poisoning attacks , \(^{+}19\), \(^{+}20\), \(^{+}21\), \(23\)).

More formally, the classic problem of _agnostic_ learnability is generally described as follows: given a (known) class of distributions \(\), and a (finite) set of samples drawn i.i.d. from some (unknown) distribution \(p\), find a distribution \(\) whose statistical distance from \(p\) is not much more than that of the closest member of \(\). It is not hard to see that this is equivalent to a notion of _robust_ learnability, where the distribution \(p\) is not viewed as arbitrary, but instead an adversarial corruption of some distribution within \(\).2 Given their equivalence, throughout this work, we will use agnostic and robust learnability interchangeably.

The difference between a robust setting and the previous realizable one is that now, instead of assuming \(p\) and asking for an arbitrarily good approximation of \(p\), we make no prior assumption about the data-generating distribution and only ask to approximate as well as (or close to) what the best member of some "benchmark" class \(\) can do.

We address the following question: Assuming a class of distributions is learnable, under which notions of robustness is it guaranteed to be robustly learnable? We focus entirely on information-theoretic learnability, and eschew concerns of computational efficiency. Indeed, our question of interest is so broad that computationally efficient algorithms may be too much to hope for.

We shall consider a few variants of robust learnability. Specifically, we will impose requirements on the nature of the difference between the data-generating distribution \(p\) and members of the class \(\). Obviously, such requirements can only make the task of robust learning easier.

One such model considers _additive robustness_. The underlying distribution is restricted to be a mixture of a distribution \(p\) from \(\), and some 'contaminating' distribution \(q\). In this Statistics community, this celebrated model is known as _Huber's contamination model_. Analogously, one can consider _subtractive robustness_. It includes the case where the starting point is a distribution in the class \(\), but a fraction of the probability mass is removed and samples are drawn from the resulting distribution (after rescaling). These two models are related to adversaries who can add or remove points from a sampled dataset, see discussion at the end of Section 1.2.

A significant line of work focuses on understanding the sample complexity of agnostic distribution learning (see examples and discussion in Section 1.3). Most study restricted classes of distributions, with analyses that are only applicable in certain classes. Some works have found quantitative separations between the different robustness models. For instance, in the specific case of Gaussian mean estimation,  give strong evidence that efficient algorithms can achieve better error if they must only be additively robust, rather than robust in general. However, such findings are again restricted to specific cases, and say little about the overall relationship between learnability in general and these various robust learning models.

Current results leave open a more comprehensive treatment of robustness in distribution learning. Specifically, what is the relative power of these different robustness models, and what is their impact on which types of distributions are learnable? Are there more generic ways to design robust learning algorithms?

Our two main contributions are the following:

* We give a generic algorithm which converts a learner for a distribution class into a learner for that class which is robust to additive corruptions.
* We show that there exist distribution classes which are learnable, but are no longer learnable after subtractive corruption.

Stated succinctly: we show that learnability implies robust learnability when an adversary can make additive corruptions, but not subtractive corruptions. Other results explore implications related to compression schemes and differentially private learnability.

### Definitions of Learnability

In order to more precisely describe our results, we define various notions of learnability. We start with the standard notion of PAC learnability for a distribution class. We get samples from a distribution \(p\) belonging to a distribution class \(\), and the goal is to output a distribution similar to \(p\).

**Definition 1.1** (Learnability).: _We say that a class \(\) of probability distributions is learnable (or, realizably learnable) if there exists a learner \(A\) and a function \(n_{}:(0,1)^{2}\), such that for every probability distribution \(p\), and every \((,)(0,1)^{2}\), for \(n n_{}(,)\) the probability over samples \(S\) of that size drawn i.i.d. from the distribution \(p\) that \(d_{}(p,A(S))\) is at least \(1-\)._

We next introduce the more challenging setting of robust, or agnostic, learning. In this setting, the sampled distribution is within bounded distance to the distribution class \(\), rather than being in \(\) itself. For technical reasons, we introduce two closely-related definitions. Roughly speaking, the latter definition assumes the distance from the sampling distribution to \(\) is fixed, whereas the former (more commonly considered in the agnostic learning literature) doesn't. Note that in many cases, robust algorithms designed with knowledge of the distance \(\) to \(\) can be modified to do without .

**Definition 1.2** (Robust learnability).:
1. _For_ \(>0\)_, we say that a class_ \(\) _of probability distributions is_ \(\)-robustly learnable _(also referred to as_ \(\)-agnostically learnable_) if there exists a learner_ \(A\) _and a function_ \(n_{}:(0,1)^{2}\)_, such that for every probability distribution_ \(p\)_, and_ \((,)(0,1)^{2}\)_, for_ \(n n_{}(,)\) _the probability over samples_ \(S\) _of that size drawn i.i.d. from the distribution_ \(p\) _that_ \(d_{}(p,A(S))\{d_{}(p,p^{}):p^{} \}+\) _is at least_ \(1-\)_._ _When_ \(=1\) _we omit it and say that the class is robustly (or agnostically) learnable._
2. _For_ \(0\) _and_ \(>0\)_, we say that a class_ \(\) _of probability distributions is_ \(\)-\(\)-robustly learnable _if there exists a learner_ \(A\) _and a function_ \(n_{}:(0,1)^{2}\)_, such that for every probability distribution_ \(p\) _such that_ \(\{d_{}(p,p^{}):p^{}\}\) _and_ \((,)(0,1)^{2}\)_, for_ \(n n_{}(,)\) _the probability over samples_ \(S\) _of that size drawn i.i.d. from the distribution_ \(p\) _that_ \(d_{}(p,A(S))+\) _is at least_ \(1-\)_._

Finally, we introduce notions of robust learnability which correspond to only additive or subtractive deviations from the distribution class \(\). These more stringent requirements than standard (realizable) learnability, but more lenient than \(\)-\(\)-robust learnability: the adversary in that setting can deviate from the distribution class \(\) with both additive and subtractive modifications simultaneously.

**Definition 1.3** (Additive robust learnability).: _Given parameters \(0 1\) and \(>0\), we say that a class \(\) of probability distributions is \(\)-additive\(\)-robustly learnable if there exists a learner \(A\) and a function \(n_{}:(0,1)^{2}\), such that for every probability distribution \(q\), every \(p\), and \((,)(0,1)^{2}\), for \(n n_{}(,)\) the probability over samples \(S\) of that size drawn i.i.d. from the distribution \( q+(1-)p\), that \(d_{}(A(S),p)+\) is at least \(1-\)._

**Definition 1.4** (Subtractive robust learnability).: _Given parameters \(0 1\) and \(>0\), we say that a class \(\) of probability distributions is \(\)-subtractive\(\)-robustly learnable if there exists a learner \(A\) and a function \(n_{}:(0,1)^{2}\), such that for every probability distribution \(p\) for which there exists a probability distribution \(q\) such that \( q+(1-)p\), and for every \((,)(0,1)^{2}\), for \(n n_{}(,)\) the probability over samples \(S\) of that size drawn i.i.d. from the distribution \(p\), that \(d_{}(A(S),p)+\) is at least \(1-\)._

### Results and Techniques

We explore how different robustness models affect learnability of distributions, showing strong separations between them. Our first main result shows that learnability implies additively robust learnability.

**Theorem 1.5**.: _Any class of probability distributions \(\) which is realizably learnable, is also \(\)-additively \(2\)-robustly learnable for every \((0,1/4)\)._

Note that, since additively robust learnability trivially implies learnability, this shows an _equivalence_ between learnability and additively robust learnability.

Our algorithm enumerates over all subsets of the dataset of an appropriate size, such that at least one subset contains no samples from the contaminating distribution. A realizable learner is applied toeach subset, and techniques from hypothesis selection  are used to pick the best of the learned distributions. Further details appear in Section 2.

We also note that since our robust learning algorithm enumerates all large subsets of the training dataset, it is _not_ computationally efficient. Indeed, for such a broad characterization, this would be too much to ask. Efficient algorithms for robust learnability are an exciting and active field of study, but outside the scope of this work. For further discussion see Section 1.3.

Our other main result shows that a distribution class being learnable does _not_ imply that it is subtractive robustly learnable.

**Theorem 1.6**.: _For every \(>0\), there exists a class that is learnable, but not \(\)-subtractively \(\)-robustly learnable for any \(0\)._

An immediate corollary is that learnability does _not_ imply robust (or agnostic) learnability, since this is a more demanding notion than subtractive robust learnability.

Our proof of this theorem proceeds by constructing a class of distributions that is learnable, but classes obtained by subtracting light-weight parts of these distributions are not \(\)-robustly learnable with respect to the original learnable class. More concretely, our construction works as follows. We start by considering a distribution class that, by itself, is not learnable with any finite number of samples. We map each distribution in that class to a new distribution, which additionally features a point with non-trivial mass that "encodes" the identity of the distribution, thus creating a new class of distributions which _is_ learnable. Subtractive contamination is then able to "erase" this point mass, leaving a learner with sample access only to the original (unlearnable) class. Our construction is inspired by the recent construction of Lechner and Ben-David , showing that the learnability of classes of probability distributions cannot be characterized by any notion of combinatorial dimension. For more details, see Section 3.

Thus far, we have only considered additive and subtractive robustness separately. General robustness, where probability mass can be both added _and_ removed, is more powerful than either model individually. However, if a class is additive robustly learnable _and_ subtractive robustly learnable, is it robustly learnable? Though this is intuitively true, we are not aware of an immediate proof. Using a similar argument as Theorem 1.5, we derive a stronger statement: that subtractively robust learnability implies robust learnability.

**Theorem 1.7**.: _If a class \(\) is \(\)-subtractive \(\)-robustly learnable, then it is also \(\)-\((2+4)\)-robustly learnable._

Adjacent to distribution learning is the notion of _sample compression schemes_. Recent work by Ashtiani, Ben-David, Harvey, Liaw, Mehrabian, and Plan  expanded notions of sample compression schemes to apply to the task of learning probability distributions. They showed that the existence of such sample compression schemes for a class of distributions imply the learnability of that class. While the existence of sample compression schemes for classification tasks imply the existence of such schemes for robust leaning, the question if similar implication hold for distribution learning schemes was not answered. We strongly refute this statement. We use a construction similar to that of Theorem 1.6, see Section 3.1 for more details.

**Theorem 1.8**.: _The existence of compression schemes for a class of probability distributions does not imply the existence of robust compression schemes for that class._

Finally, a natural question is whether other forms of learnability imply robust learnability. We investigate when _differentially private3_ (DP) learnability does or does not imply robust learnability. We find that the same results and separations as before hold when the distribution class is learnable under _approximate_ differential privacy (i.e., \((,)\)-DP), but, perhaps surprisingly, under _pure_ differential privacy (i.e., \((,0)\)-DP), private learnability implies robust learnability for all considered adversaries.4

**Theorem 1.9** (Informal).: \((,0)\)_-DP learnability implies robust \((,0)\)-DP learnability. For any \(>0\), \((,)\)-DP learnability implies additively robust learnability, but not subtractively robust learnability._

[MISSING_PAGE_FAIL:5]

 for a survey. A number of these works have focused on connections between robustness and privacy . Again, these results either focus on specific classes of distributions, or give implications that require additional technical conditions, whereas we aim to give characterizations of robust learnability under minimal assumptions.

The question whether learnability under realizability assumptions extends to non-realizable setting has a long history. For binary classification tasks, both notions are characterized by the finiteness of the VC-dimension, and are therefore equivalent .  show a similar result for online learning. Namely, that agnostic (non-realizable) learnability is characterized by the finiteness of the Littlestone dimension, and is therefore equivalent to realizable learnability.

Going beyond binary classification, recent work  shows that the equivalence of realizable and agnostic learnability extends across a wide variety of settings. These include models with no known characterization of learnability such as learning with arbitrary distributional assumptions and more general loss functions, as well as a host of other popular settings such as robust learning, partial learning, fair learning, and the statistical query model. This stands in contrast to our results for the distribution learning setting. We show that realizable learnability of a class of probability distributions does _not_ imply its agnostic learnability. It is interesting and natural to explore the relationship between various notions of distribution learnability, which we have scratched the surface of in this work.

## 2 Learnability Implies Additive Robust Learnability

We recall Theorem 1.5, which shows that any class that is realizably learnable is also additive robustly learnable.

**Theorem 1.5**.: _Any class of probability distributions \(\) which is realizably learnable, is also \(\)-additively \(2\)-robustly learnable for every \((0,1/4)\)._

We prove this theorem by providing an algorithm based on classical tools for hypothesis selection . These methods take as input a set of samples from an unknown distribution and a collection of hypotheses distributions. If the unknown distribution is close to one of the hypotheses, then, given enough samples, the algorithm will output a close hypothesis. Roughly speaking, our algorithm looks at all large subsets of the dataset, such that at least one will correspond to an uncontaminated set of samples. A learner for the realizable setting (whose existence we assumed) is applied to each to generate a set of hypotheses, and we then use hypothesis selection to pick one with sufficient accuracy. The proof of Theorem 1.7 (showing that subtractively robust learnability implies robust learnability) follows almost the exact same recipe, except the realizable learner is replaced with a learner robust to subtractive contaminations. We recall some preliminaries in Section A. We then prove Theorem 1.5 in Section B, and we formalize and prove a version of Theorem 1.7 in Section 2.1.

We note that \(=2\) and \(=3\) are often the optimal factors to expect in distribution learning settings, even for the case of finite distribution classes. For example, for proper agnostic learning the factor \(=3\) is known to be optimal for finite collections of distributions, which holds for classes with only 2 distributions . Similarly the factor of \(=2\) is optimal if the notion of learning is relaxed to improper learners . While we are not aware of lower bounds for the additive setting, a small constant factor such as \(2\) is within expectations for these problems.

For the proof of Theorem 1.5, we refer the reader to Section B in the appendix.

### Subtractive Robust Learnability Implies Robust Learnability

Similarly, we can show that robustness with respect to a subtractive adversary implies robustness with respect to a general adversary. We note that this theorem requires a change in constants from \(\) to \((2+4)\).

**Theorem 1.7**.: _If a class \(\) is \(\)-subtractive \(\)-robustly learnable, then it is also \(\)-\((2+4)\)-robustly learnable._

The proof follows a similar argument as the proof of Theorem 1.5 and can be found in Section C in the appendix.

Learnability Does Not Imply Robust (Agnostic) Learnablity

In this section we show that there are classes of distributions which are realizably learnable, but not robustly learnable.

**Theorem 3.1**.: _There are classes of distributions \(\), such that \(\) is realizably learnable, but for every \(\), \(\) is not \(\)-robustly learnable. Moreover, the sample complexity of learning \(\) can be arbitrarily close to (but larger than) linear. Namely, for any super-linear function \(g\), there is a class \(_{g}\), with_

* \(_{g}\) _is realizable learnable with sample complexity_ \(n^{re}_{_{g}}(,)(1/)g(1/)\)_;_
* _for every_ \(\)_,_ \(_{g}\) _is_ not \(\)_-robustly learnable._

Note that this statement appears _slightly_ weaker than Theorem 1.6, in that it holds for \(\)-robust learnability rather than \(\)-subtractive \(\)-robust learnability. In fact, the two statements are incomparable, due to the order of quantifiers in the construction. Here we provide a single class which is not \(\)-robustly learnable for every \(\), whereas in the proof of Theorem 1.6 we give a different class for each \(\) (though the two constructions are similar). For simplicity we focus here on Theorem 3.1, whereas the proofs of Theorem 1.6 and other claims appear in Section D.

The key idea to the proof is to construct a class which is easy to learn in the realizable case, by having each distribution of the class have a unique support element that is not shared by any other distributions in the class. Distributions on which this "indicator element" has sufficient mass will be easily identified, independent of how rich the class is on other domain elements. That richness makes the class hard to learn from samples that miss those indicators. Furthermore, we construct the class in a way that its members are close in total variation distance to distributions that place no weight on those indicator elements.

This is done by making the mass on these indicator elements small, so that the members of a class of distributions that results from deleting these indicator bits are close to the initially constructed class, \(_{g}\). In order to make this work for every target accuracy and sample complexity, we need to have a union of such classes with decreasingly small mass on the indicator bits. In order for this to not interfere with the realizable learnability, we let the distributions with small mass on the indicator bits have most of their mass on one point \((0,0)\) that is the same for all distributions in the class. This ensures that distributions for which the indicator bit will likely not be observed because their mass is smaller than some \(\) are still easily \(\)-approximated by a constant distribution (\(_{(0,0)}\)). Lastly we ensure the impossibility of agnostic learnability, by controlling the rate at which \(\) approaches zero to be faster than the rate at which \(\) approaches zero. With this intuition in mind, we will now describe the construction and proof of this theorem.

Proof.: We first define the distributions in \(_{g}\). Let \(\{A_{i}:i\}\) be an enumeration of all finite subsets of \(\). Define distributions over \(\) as follows:

\[q_{i,j,k}=(1-)_{(0,0)}+(- {k})U_{A_{i}\{2j+1\}}+_{(i,2j+2)}, \]

where, for every finite set \(W\), \(U_{W}\) denotes the uniform distribution over \(W\). For a monotone, super-linear function \(g:\), we now let \(_{g}=\{q_{i,j,g(j)}:i,j\}\). The first bullet point of the theorem (the class is learnable) follows from Claim 3.2 and the second bullet point (the class is not robustly learnable) follows from Claim 3.3. 

**Claim 3.2**.: _For a monotone function \(g:\), let \(_{g}=\{q_{i,j,g(j)}:i,j\}\). Then, the sample complexity of \(_{g}\) in the realizable case is upper bounded by_

\[n^{re}_{_{g}}(,)(1/)g(1/).\]

This claim can be proved by showing that the following learner defined by

\[(S)=q_{i,j,g(j)}&(i,2j+2) S\\ _{(0,0)}&\]is a successful learner in the realizable case. Intuitively, this learner is successful for distributions \(q_{i,j,g(j)}\) for which \(j\) is large (i.e., \(j>\)), since this will mean that \(d_{}(q_{i,j,g(j)},_{(0,0)})\) is small. Furthermore, it is successful for distributions \(q_{i,j,g(j)}\) for which \(j\) is small (i.e., upper bounded by some constant dependent on \(\)), because this will lower bound the probability \(1/g(j)\) of observing the indicator bit on \((i,2j+2)\). Once the indicator bit is observed the distribution will be uniquely identified.

**Claim 3.3**.: _For every function \(g(n)\) the class \(_{g}\) is not \(\)-robustly learnable for any \(>0\)._

This claim can be proven by showing that for every \(\), there is \(\), such that the class of distributions \(Q^{}\) such that for every \(q^{} Q^{}\) there is \(q_{g}\) with \(d_{}(q,q^{})<\) which is not \(\)-weakly learnable.7 In particular, those for every \(q^{} Q^{}\) there is \(q_{g}\) and \(p\) such that \(q=(1-)q^{}+ q\). We construct this class and show that it is not learnable by using the construction and Lemma 3 from .

### Existence of sample compression schemes

Sample compression schemes are combinatorial properties of classes that imply their learnability. For a variety of learning tasks, such as binary classification or expectation maximization a class has a sample compression scheme if and only if it is learnable . For classification tasks, sample compression for realizable samples implies agnostic sample compression.  used compression schemes to show learnability of classes of distributions in the realizable case, but left open the question if for learning probability distributions, the existence of realizable sample compression schemes implies the existence of similar schemes for the non-realizable (agnostic, or robust) settings. We provide a negative answer to this question.

More concretely, let \(\) be a class of distributions over some domain \(X\). A compression scheme for \(\) involves two agents: an encoder and a decoder.

* The encoder knows a distribution \(q\) and receives a sample \(S\) generated by this distribution. The encoder picks a bounded size sub-sample and sends it, possibly with a few additional bits to the decoder.
* The decoder receives the message and uses an agreed upon decoding rule (that may depend on \(\) but not on \(q\) or \(S\)) to constructs a distribution \(p\) that is close to \(q\).

Of course, there is some probability that the samples are not representative of the distribution \(q\), in which case the compression scheme will fail. Thus, we only require that the decoding succeed with constant probability.

We say that a class \(\) has a sample compression scheme (realizable or robust) if for every accuracy parameter \(>0\), the minimal required size of the sample \(S\), and upper bounds on the size of the sub-sample and number of additional bits in the encoder's message depend only of \(\) and \(\) (and are independent of the sample generating \(q\) and on the sample \(S\)).

A realizable compression scheme is required to handle only \(q\)'s in \(\) and output \(p\) such that \(d_{}(p,q)\), while a robust compression scheme should handle any \(q\) but the decoder's output \(p\) is only required to be \(_{q}\{d_{}(p,q)\}+\) close to \(q\).

**Theorem 3.4** (Formal version of Theorem 1.8).: _For every \(\), the existence of a realizable compression scheme, does not imply the existence of an \(\)-robust compression scheme. That is, there is a class \(\) that has a realizable compression scheme, but for every \(\), \(\) does not have an \(\)-robust compression scheme._

Proof.: Consider the class \(=_{g}\) from Section 3. We note that this class has a compression scheme of size 1. However, from , we know that having a \(\)-robust compression scheme implies \(\)-agnostic learnability. We showed in Theorem 1.6 that for every \(\) and for every superlinear function \(g\), the class \(_{g}\) is not \(\)-agnostically learnable. It follows that \(_{g}\) does not have an \(\)-robust compression scheme.

In Section E, we present a precise quantitative definition of sample compression schemes, as well as the proof that the class \(_{g}\) has a sample compression scheme of size 1.

## 4 Implications of Private Learnability

Qualitatively speaking, differentially private algorithms offer a form of "robustness" - the output distribution of a differentially private algorithm is insensitive to the change of a single point in its input sample. The relationship between privacy and notions of "robustness" has been studied under various settings, where it has been shown that robust algorithms can be made private and vice versa .

For distribution learning, we find that: (1) the requirement of approximate differentially private learnability also does not imply (general) robust learnability; and (2) the stronger requirement of _pure_ differentially private learnability does imply robust learnability.

**Definition 4.1** (Differential Privacy ).: _Let \(X\) be an input domain and \(Y\) to be an output domain. A randomized algorithm \(A:X^{m} Y\) is \((,)\)-differentially private (DP) if for every \(x,x^{} X^{n}\) that differ in one entry,_

\[[A(x) B] e^{}[A(x^{}) B]+  28.452756pt.\]

_If \(A\) is \((,)\)-DP for \(>0\), we say it satisfies approximate DP. If it satisfies \((,0)\)-DP, we say it satisfies pure DP._

**Definition 4.2** (DP learnable class).: _We say that a class \(\) of probability distributions is (approximate) DP learnable if there exists a randomized learner \(A\) and a function \(n_{}:(0,1)^{4}\), such that for every probability distribution \(p\), and every \((,,,)(0,1)^{4}\), for \(n n_{}(,,,)\)_

1. \(A\) _is_ \((,)\)_-DP; and_
2. _The probability over samples_ \(S\) _of size_ \(n\) _drawn i.i.d. from the distribution_ \(p\)_, as well as over the randomness of_ \(A\) _that_ \[d_{}(p,A(S))\] _is at least_ \(1-\)_._

_We say \(\) is pure DP learnable if a learner \(A\) can be found that satisfies \((,0)\)-DP, in which case the sample complexity function \(n_{}:(0,1)^{3}\) does not take \(\) as a parameter._

**Theorem 4.3** (Approximate DP learnability vs. robust learnability).:
1. _If a class_ \(\) _is approximate DP learnable, then_ \(\) _is_ \(\)_-additive_ \(2\)_-robustly learnable for any_ \((0,1/4)\)_._
2. _There exists an approximate DP learnable class_ \(\) _that is not_ \(\)_-robustly learnable for any_ \( 1\)_._

Note that the first claim is immediate from Theorem 1.5, since approximate DP learnability implies learnability. To prove the second claim, we show that the learner for the class \(\) described in Theorem 3.1 can be made differentially private by employing stability-based histograms . The proof appears in Section F.

**Theorem 4.4** (Pure DP learnable vs. robustly learnable).: _If a class \(\) is pure DP learnable, then \(\) is \(3\)-robustly learnable._

The proof relies on the finite cover characterization of pure DP learnability.

**Proposition 4.5** (Packing lower bound, Lemma 5.1 from ).: _Let \(\) be a class of distributions, and let \(,>0\). Suppose \(_{}\) is a \(\)-packing of \(\), that is, \(_{}\) such that for any \(p q_{},d_{}(p,q)>\)._

_Any \(\)-DP algorithm \(A\) that takes \(n\) i.i.d. samples \(S\) from any \(p\) and has \(d_{}(p,A(S))/2\) with probability \( 9/10\) requires_

\[n_{}|-}{}.\]Proof of Theorem 4.4.: Let \(,>0\). Pure DP learnability of \(\) implies that there exists a \(1\)-DP algorithm \(A_{DP}\) and \(n=n_{}(/12,1/10,1)\) such that for any \(p\), with probability \( 9/10\) over the sampling of \(n\) i.i.d. samples \(S\) from \(p\), as well as over the randomness of the algorithm \(A_{DP}\), we have \(d_{}(p,A_{DP}(S))/12\). By Proposition 4.5, any \(/6\)-packing \(_{/6}\) of \(\) has

\[|_{/6}|(10/9).\]

Let \(}\) be such a maximal \(/6\)-packing. By maximality, \(}\) is also an \(/6\)-cover of \(\). Hence, running Yatracos' 3-robust finite class learner (Theorem A.1) \(A\) over \(}\) with

\[n_{}}(/2,)=O(}|+(1/)}{(/2)^{2}})\]

samples drawn i.i.d. from \(p\) yields, with probability \( 1-\)

\[d_{}(p,A(S))  3\{d_{}(p,p^{}):p^{}}\}+/2\] \[ 3(\{d_{}(p,p^{}):p^{}\}+/6)+/2\] \[=3\{d_{}(p,p^{}):p^{}\}+.\]

Note that Yatracos' algorithm for hypothesis selection can be replaced with a pure DP algorithm for hypothesis selection (Theorem 27 of ) in order to achieve the following stronger implication.

**Theorem 4.6** (Pure DP learnable vs. robustly learnable).: _If a class \(\) is pure DP learnable, then \(\) is pure DP \(3\)-robustly learnable._

## 5 Conclusions

We examine the connection between learnability and robust learnability for general classes of probability distributions. Our main findings are somewhat surprising in that, in contrast to most known leaning scenarios, learnability does _not_ imply robust learnability. We also show that learnability _does_ imply additively robust learnability. We use our proof techniques to draw new insights related to compression schemes and differentially private distribution learning.