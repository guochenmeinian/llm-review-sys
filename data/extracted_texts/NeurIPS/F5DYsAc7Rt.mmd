# Grand-SLAM' Interpretable Additive Modeling with Structural Constraints

Shibal Ibrahim

MIT

Cambridge, MA

shibal@mit.edu

&Gabriel Isaac Afriat

MIT

Cambridge, MA

afriatg@mit.edu

&Kayhan Behdin

MIT

Cambridge, MA

behdink@mit.edu

&Rahul Mazumder

MIT

Cambridge, MA

rahulmaz@mit.edu

###### Abstract

Generalized Additive Models (GAMs) are a family of flexible and interpretable models with old roots in statistics. GAMs are often used with pairwise interactions to improve model accuracy while still retaining flexibility and interpretability but lead to computational challenges as we are dealing with order of \(p^{2}\) terms. It is desirable to restrict the number of components (i.e., encourage sparsity) for easier interpretability, and better computational and statistical properties. Earlier approaches, considering sparse pairwise interactions, have limited scalability, especially when imposing additional structural interpretability constraints. We propose a flexible GRAND-SLAM framework that can learn GAMs with interactions under sparsity and additional structural constraints in a differentiable end-to-end fashion. We customize first-order gradient-based optimization to perform sparse backpropagation to exploit sparsity in additive effects for any differentiable loss function in a GPU-compatible manner. Additionally, we establish novel non-asymptotic prediction bounds for our estimators with tree-based shape functions. Numerical experiments on real-world datasets show that our toolkit performs favorably in terms of performance, variable selection and scalability when compared with popular toolkits to fit GAMs with interactions. Our work expands the landscape of interpretable modeling while maintaining prediction accuracy competitive with non-interpretable black-box models. Our code is available at https://github.com/mazumder-lab/grandslamin.

## 1 Introduction

Many state-of-the-art learners e.g., tree ensembles, neural networks, kernel support vector machines, can be difficult to interpret. There have been various efforts to derive some post-training explainability from these models --see  for a survey. Post-hoc explainability attempts to explain black-box prediction with interpretable instance-specific approximations e.g, LIME  and SHAP . However, such approximations are known to be unstable [12; 28], expensive  and inaccurate . Hence, it is desirable to consider modeling approaches that are inherently interpretable.

Amongst classical approaches, there are some models that have inherent interpretability e.g., Linear models, CART  and Generalized Additive Models (GAMs). GAMs  which have old roots in statistics are considered a front-runner in the context of interpretable modeling. They consider an additive model of the main effects of the form: \(g([y])=_{j[p]}f_{j}(x_{j})\), where \(x_{j}\) denotes the \(j\)th feature in input \(^{p}\), \(f_{j}\) is a univariate shape function and \(g\) denotes the link function that adapts the model to various settings such as regression or classification. GAMs are considered easy to interpret as the impact of each feature can be understood via visualizing the corresponding shape function e.g., plotting \(f_{j}(x_{j})\) vs \(x_{j}\). However, such models often suffer in prediction performance when compared to black-box methods e.g., deep neural networks (DNNs). This can be attributed in part to the fact that GAMs do not consider interactions among covariates.

There has been some exciting work that aims to reduce this performance gap by considering GAMs with pairwise interactions [see, for example, 34, 60, 6, 46, 9, 10, and the references therein]. GAMs with pairwise interactions consider a model of the following form:

\[g([y])=_{j[p]}f_{j}(x_{j})+_{(j,k) }f_{j,k}(x_{j},x_{k})\] (1)

where \(f_{j,k}\) is a bivariate shape function and \(\{(1,2),(1,3),...,(p-1,p)\}\) denotes the set of all pairwise interactions. Under this model, \(f_{j}(x_{j})\) is the \(j\)-main effect and \(f_{j,k}(x_{j},x_{k})\) is the \((j,k)\)-th interaction effect. Pairwise interactions are considered interpretable as each of the bivariate shape function \(f_{j,k}\) can be visualized as a heatmap on an \(x_{j},x_{k}\)-plane. Despite their appeal, GAMs with pairwise interactions pose several challenges: (i) Learning all pairwise interaction effects of the order of \(p^{2}\) lead to computational and statistical challenges. (ii) Performing component selection such that only a few of the components \(\{f_{j}\}\) and \(\{f_{j,k}\}\) are nonzero in an end-to-end fashion (while training) is a hard combinatorial optimization problem. We remind the reader that component selection is needed to aid interpretability. (iii) Imposing structural constraints on the interaction effects, e.g., hierarchy  makes the associated optimization task more complex.

In this paper, we introduce a novel GRAND-SLAMIN framework that allows for a flexible way to do component selection in GAMs with interactions under additional structural constraints in an end-to-end fashion. In particular, we introduce an alternative formulation of GAMs with interactions with additional binary variables. Next, we _smooth_ these binary variables so that we can effectively learn these components via first-order methods in smooth optimization (e.g, SGD). Our formulation appears to have an edge over existing methods in terms of (i) model flexibility for enhanced structural interpretability and (ii) computational efficiency. First, the binary variables allow us to impose in the model (a) component selection constraints and (b) additional structural constraints (e.g, hierarchy) via a unified optimization formulation. Both of constraints (a), (b) can aid interpretability, model compression, and result in faster inference and better statistical properties. Second, the our smoothing procedure for the binary variables allows us to have customized algorithms that exploit sparsity in the forward and backward pass of the backpropagation algorithm.

For structural interpretability, we study two notions: weak and strong hierarchy .

\[: f_{j,k} 0 f_{j} 0 f_{k} 0 (j,k),\ j[p],\ k[p].\] (2) \[: f_{j,k} 0 f_{j} 0 f_{k} 0 (j,k),\ j[p],\ k[p].\] (3)

Weak hierarchy allows an interaction effect \(f_{j,k}\) to be selected if either main effect \(f_{j}\) or \(f_{k}\) is selected. Strong hierarchy allows for an interaction effect \(f_{j,k}\) to be selected only if both main effects \(f_{j}\) and \(f_{k}\) are selected. Such hierarchy constraints are popular in high-dimensional statistics: (i) They lead to more interpretable models , (ii) They promote practical sparsity, i.e., reduce the number of features that need to be measured when making new predictions (see Sec. 6.2) -- this can reduce future data collection costs , (iii) Additional constraints can also help regularize a model, sometimes resulting in improved AUC (see Sec. 6.1). (iv) They can also reduce variance in estimation of main effects in the presence of interaction effects (see Sec. 6.4), allowing the user to have more "trust" on model interpretability explanations.

**Contributions.** To summarise, while it's well acknowledged that GAMs with sparse interactions are a useful flexible family of explainable models, learning them pose significant computational challenges due to the combinatorial nature of the associated optimization problem. Our technical contributions in this paper be summarized as:

1. We propose a novel optimization formulation that makes use of indicator (binary) variables. The indicator variables allow us to impose both (a) component selection and (b) structural constraints in an end-to-end fashion. We consider a smooth and continuous parameterization of the binary variables so that the optimization objective is differentiable (for a smooth training loss) and hence amenable to first order methods such as SGD.
2. We show the flexibility of our framework by considering two different notions of hierarchy. While these constraints improve interpretability, they make the combinatorial problem more challenging . We propose end-to-end algorithms to train these models, making our approach quite different from existing neural-based toolkits .
3. We exploit sparsity in the indicator variables during the course of the training for sparse forward and backward passes in a customized backpropagation algorithm in a GPU-compatible manner. This provides speedups on training times up to a factor of \(10\) over standard backpropagation.

4. We study novel statistical properties of our model, and present non-asymptotic prediction error bounds. Different from earlier work, our results apply to learning with tree-based shape functions (for both main and interaction effects).
5. We introduce a new open-source toolkit GRAND-SLAMIN and perform experiments on a collection of 16 real-world datasets to demonstrate the effectiveness of our toolkit in terms of prediction performance, variable selection and scalability.

## 2 Related Work

**GAMs.** GAMs have a long history in statistics  and have been extensively studied. They're often studied with smooth spline shape functions [see, e.g, 37, 47, 20, 63, 62, and references therein]. Some works have studied tree-based shape functions  and neural basis functions .

**GAMs with all pairwise interactions.** In this thread,  study low-rank decomposition with neural network shape functions;  fit all pairwise interactions using shared neural bases.

**Sparse GAMs with interactions.** introduced COSSO, which penalizes the sum of the Sobolev norms of the functional components, producing sparse models.  propose ELAAN, which is an \(_{0}\)-regularized formulation with smooth cubic splines.  demonstrate the usefulness of their approach in a regression setting in terms of compact component selection and efficiency on a large-scale Census survey response prediction.  explore tree-based shape functions for fitting additive models with sparse interactions.  consider _all_ main effects and a subset of pairwise interactions; the subset of interactions are selected via greedy stage-wise interaction detection heuristics.  provide an efficient implementation of the above approach as Explainable Boosting Machines (EBMs).  propose NODE-GA\({}^{2}\)M: an end-to-end learning approach with differentiable neural oblivious decision (NODE) trees . Component selection in NODE-GAM is achieved by constraining the number of trees, and each tree learns to use one or two features via entmax transformation .

**Structural Constraints.** Structural interpretability constraints such as hierarchical interactions, have been studied for both linear settings  and nonparametric settings . We briefly review prior work on nonparametric hierarchical interactions as it relates to this paper.  proposed GAMI-Net, which is a multi-stage neural-based approach that fits _all_ main effects and a subset of Top-_k_ interaction effects, selected via a fast interaction screening method . Amongst this screened set, interactions that satisfy the weak hierarchy are later used to fit interaction effects. They also prune some main and interaction effects after training based on a variation-based ranking measure.  proposed SIAN, which uses Archipelago  to measure the strength of each main and interaction effect from a _trained_ DNN, and then screens (i.e., selects a subset of candidate main and interaction effects) using Archipelago scores to identify main effects and interaction effects that obey strong hierarchy. Then, it fits a GAM model with the screened main and interaction effects.  and  only support screening of interactions obeying hierarchy _before_ the training for interaction effects is done. None of these approaches impose hierarchy _while_ training with interactions.  with their ELAAN framework also consider strong hierarchy in the presence of \(_{0}\)-regularized formulation with splines for the least squares loss (regression). ELAAN has a two-stage approach: It selects a candidate set of interactions and then applies commercial mixed integer programming solvers to learn sparse interactions under a hierarchy constraint. This approach would require customized algorithms to adapt to different loss objectives e.g., multiclass classification. To our knowledge, current techniques for sparse hierarchical (nonparametric) interactions are not based on end-to-end differentiable training: they can be limited in flexibility and scalability--a gap we intend to fill in this work.

   &  &  & } &  &  &  \\    & Main & Interactions & Weak &  &  &  &  &  &  &  &  &  &  \\  EBM  & None & Greedy & ✗ & ✗ & ✓ & ✓ & trees & ✗ & ✓ \\ NODE-GA\({}^{2}\)M  & Entmax+Anneal & ✗ & ✗ & ✓ & ✓ & ✓ & trees & ✗ & ✓ \\  GAMI-Net  & Prune & Screening & ✗ & ✓ & ✓ & ✗ & neural & ✗ & ✗ \\ SIAN  & None & Screening & ✗ & Screening & ✓ & ✓ & ✗ & neural & ✓ & ✗ \\ ELAAN  & Group L0 & ✗ & Screening+Convex Relax. & ✓ & ✗ & splines & ✓ & ✓ \\  GRAND-SLAMIN & Binary Variables & End-to-end & End-to-end & ✓ & ✓ & ✓ & trees & ✓ & ✓ \\   
 

Table 1: Relevant work on sparse GAMs with Interactions. Models in rows 1-2 have some variable selection but no hierarchy; models in row 3-5 have screening-based approaches for hierarchy.

Note  and  also consider higher-order interactions (beyond two-way ones), which can be hard to interpret. For convenience, Table 1 summarizes some relevant work on Sparse GAMs with interactions and possible structural constraints.

## 3 Problem Formulation

We first present in Sec. 3.1 an alternative formulation of GAMs with interactions using binary variables for imposing sparsity and structural constraints. Next, in Sec. 3.2, we present a smooth reformulation of the objective that can be solved with first-order gradient-based methods.

### An optimization formulation with binary variables

We first present an alternative formulation of GAMs with interactions under sparsity with/without additional structured hierarchy constraints. Let us consider the parameterization:

\[f=_{j[p]}f_{j}(x_{j})z_{j}+_{(j,k)}f_{j,k}(x_{j},x_{k })q(z_{j},z_{k},z_{j,k}),\] (4)

with main effects \(f_{j}()\), interaction effects \(f_{j,k}()\) and binary gates \(z_{j}\) and \(q(z_{j},z_{k},z_{j,k})\). We consider three different parameterizations for \(q()\), satisfying the following different constraints:

\[ q(z_{j},z_{k},z_{j,k})}}{{=}}z_{j,k},\] (5) \[ q(z_{j},z_{k},z_{j,k})}}{{=}}(z_{j}+z_{j}-z_{j}z_{k})z_{j,k},\] (6) \[ q(z_{j},z_{k},z_{j,k})}}{{=}}z_{j}z_{k}z_{j,k}.\] (7)

The binary gates \(z_{j}\) and \(q(z_{j},z_{k},z_{j,k})\) play the role of selection. In particular, when \(z_{j}=0\), the corresponding \(j\)-th main effect \(f_{j}()\) is excluded from our additive model (4). Similarly, when \(q(z_{j},z_{k},z_{j,k})=0\), the corresponding \((j,k)\)-th interaction effect \(f_{j,k}()\) is excluded. Then, we can formulate the regularized objective as:

\[_{\{f_{j}\},\{f_{j,k}\},\\ \{z_{j}\}\{0,1\}^{p},\{(z_{j,k})\}\{0,1\}^{2}}}[(y,f)]+_{j[p]}z_{j}+_{(j,k) }z_{j,k},\] (8)

where the first term denotes empirical loss over the training data, the penalty term controls model sparsity: \( 0\) is the selection penalty, \([1,)\) controls the relative selection strength of main and interaction effects. We refer to the framework in (8) under the different constraints (5)-(7) as GRAND-SLAMI1. We discuss extension of this framework to third-order interactions in Supplement Sec. D. However, we do not consider third-order interactions in our experiments as third-order interactions are hard to interpret.

The formulation in (8) with binary variables \(z_{j}\), \(z_{j,k}\) and functions \(f_{j}\), \(f_{j,k}\) with any of the constraint sets (5)-(7) is a challenging discrete optimization problem (with binary variables) and is not amenable to differentiable training via SGD (for example). Sec. 3.2 explores approximate solutions to (8) using a smooth reformulation of the binary variables. Intuitively, we rely on continuous relaxations of the binary variables \(z\)'s and parameterize \(f\)'s with smooth tree-based shape functions. The reformulation allows us to use first-order methods.

### A Smooth Reformulation of Problem (8)

We discuss a smooth reformulation of the objective in (8). We describe an approach to parameterize the continuous relaxation of the binary variables \(z_{j}\), \(z_{j,k}\) with a Smooth-Step function  and use smooth tree-based shape functions to model \(\{f_{i}\}\), \(\{f_{j,k}\}\).

#### 3.2.1 Relaxing Binary Variables with Smooth Gates

We present an approach to smooth the binary gates \(z\)'s in (8) using a smooth-step function , which we define next.

**Smooth-Step Function.** Smooth-step function is a continuously differentiable function, similar in shape to the logistic function. However, unlike the logistic function, the smooth-step function can output 0 and 1 exactly for sufficiently large magnitudes of the input (see Appendix B for details). This function has been used for smoothing binary representations for conditional computation [18; 19; 21].

We parameterize each of the \(z\)'s in (8) as \(S()\), where \(\) is a learnable parameter and \(S()\) denotes the Smooth-step function. We parameterize the additive function as: \(f=_{j[p]}f_{j}(x_{j})S(_{j})+_{(j,k)}f_{j,k}(x_{j },x_{k})q(S(_{j}),S(_{k}),S(_{j,k})))\) and optimize the following objective:

\[_{\{f_{j}\},\{f_{j,k}\},\\ \{_{j}\}^{p},\{_{j,k}\}||} }[(y,f)]+(_{j[p]}S(_{j})+_{(j,k) }S(_{j,k})).\] (9)

Note that \(S(_{j})\) and \(S(_{j,k})\) are continuously differentiable, so the formulation in (9) is amenable to first-order gradient-based methods (e.g, SGD).

**Achieving binary gates.** To encourage each of the \(S(_{j})\)'s and \(S(_{j,k})\)'s to achieve binary state (and, not fractional) by the end of training, we add an entropy regularizer \((_{j[p]}(S(_{j}))+_{(j,k)}(S(_{ j,k})))\) where \((S())=-(S() S()+(1-S())(1-S()))\) and \( 0\) controls how quickly each of the gates \(S()\) converges to a binary \(z\).

#### 3.2.2 Soft trees

We use soft trees [25; 24; 53; 11]--based on hyperplane splits (univariate or bivariate) and constant leaf nodes -- as shape functions to parameterize the main effects \(f_{j}()\) and the pairwise interaction effects \(f_{j,k}()\). See Figure 1 for an illustration. Soft trees were introduced as hierarchical mixture of experts by  and further developed by [24; 53; 11]. They allow for end-to-end learning [27; 18; 22]. They also have efficient implementations when learning tree ensembles . A detailed definition of soft tress is given in Appendix A.

## 4 Efficient Implementation

We discuss a fast implementation of our approach. The key elements are: (i) Tensor parameterization of trees, (ii) Sparse backpropagation, and (iii) Complementary screening heuristics.

**Tensor Parameterization of Additive Effects.** Typically, in neural-based additive model implementations [1; 60], a separate network module is constructed for each shape function. The outputs from each model are sequentially computed and combined additively. This can create bottleneck in scaling these models. Some recent approaches e.g.,  try to work around this approach by constructing a large block-wise network to compute representations of all shape functions simultaneously. However, this comes at a cost of large memory footprint. For tree-based shape functions, drawing inspiration from , we can implement a tensor-based formulation of all shape functions, leveraging the fact that each tree has the same depth. This parameterization can exploit GPU-friendly parallelization in computing all shape functions simultaneously without increasing memory footprints.

**Sparse Backpropagation.** We use first-order optimization methods (e.g., SGD and its variants) to optimize GRAND-SLAMIN. Typically, a main computational bottleneck in optimizing GAMs with all pairwise interactions via standard gradient-based backpropagation methods is the computation

Figure 1: Modeling main and interaction effects with soft trees. \(\) denotes the sigmoid activation function. We omit biases in split nodes for brevity. For interaction effect, \(W_{i,1}^{j,k}\) and \(W_{i,2}^{j,k}\) denote the weights in \(i\)-th node of the \((j,k)\)-th tree.

of forward pass and gradient computations with respect to all additive components (both main and interaction effects). This can hinder training large GAMs with interactions. We exploit the sparsity in GRAND-SLAMN via the sparsity in the smooth-step function and its gradient during training.

Recall that \(S(_{j})\)'s and \(S(_{jk})\)'s play a role of selection in a smoothed fashion. In the early stages of training, \(S(_{j})\)'s and \(S(_{jk})\)'s are all in the range \((0,1)\). As the optimization proceeds, due to the entropic regularization and selection regularization, \(S(_{j})\)'s and \(S(_{jk})\)'s progressively achieve binary state \(\{0,1\}\) -- the gradient with respect to \(_{j}\) and \(_{jk}\) also reaches \(0\) because of the nature of smooth-step function \(S()\). All the additive components corresponding to the selection variables that reached \(0\) can be removed from both the forward and the backward computational graph. This sparse backpropagation approach can provide large speedups on training times up to a factor of \(10\) over standard backpropagation. Additionally, there is progressively a reduced memory footprint during the course of training in comparison to standard backpropagation.

The approach outlined above (use of Smooth-Step function for selection) when specialized to additive models allows us to implement GPU-friendly sparse backpropagation -- this makes our work different from , which does not support GPU training.

**Screening.** We describe how screening approaches prior to training, can complement our sparse backpropagation approach when the number of all pairwise interactions is large e.g., of the order \(100,000\). Fast screening methods based on shallow-tree like models are proposed in  for identifying prominent pairwise interactions. These are used by various toolkits e.g., EBM , GANMI-Net . These screening approaches are complementary to our selection approach with indicator variables. We used CART  for each pairwise interaction and sorted the interaction effects based on AUC performance to select an initial screened set of interaction effects. In particular, we can consider \(\) to be a screened subset (e.g., \(10,000\)) of all pairwise interaction effects of the order \(100,000\). Then, we run our end-to-end learning framework under the component selection constraints on the main and screened interaction effects. We observe that such screening can be beneficial for multiple reasons: (i) The training time can be reduced further by \(3-5\). (ii) The memory footprint of the model reduces by \(10\). (iii) There is no loss in accuracy--the accuracy can sometimes improve with screening -- see ablation study in Supplement Sec. F.6. Note that even with screening, our approach is different from GANMI-Net as they directly screen to a much smaller set of interactions e.g., \(500\) and there is no component selection when training with these interactions.

## 5 Statistical Theory

In this section, we explore the statistical properties of our additive model with soft tree shape functions. We assume that observations are noisy versions of some (unknown) underlying noiseless data. We do not assume the noiseless data comes from a tree-based model. We show that if our model can approximate the true noiseless data well, the prediction error resulting from the noise converges to zero as the number of observations grows. In particular, for \(n\) data observations \((_{i},y_{i})_{i=1}^{n}\) we consider a sparsity-constrained version of Problem (8) with the least-squares loss as given by

\[f_{(t)}()*{argmin} _{i=1}^{n}[y_{i}-_{j[p]}f_{j}(x_{i,j})z_{j}-(t-1) _{(j,k)}f_{j,k}(x_{i,j},x_{i,k})q(z_{j},z_{k},z_{j,k})]^{2},\] (10) s.t. \[\{z_{j}\}\{0,1\}^{p},\{z_{j,k}\}\{0,1\}^{||}, _{j[p]}z_{j}+_{(j,k)}q(z_{j},z_{k},z_{j,k}) s,\]

for \(t\{1,2\}\), where \(f_{j},f_{j,k}\) are depth-\(d\) soft trees (cf. Section 3.2.2). For \(t=1\), Problem (10) simplifies to a main effects model, while \(t=2\) corresponds to the model with pairwise interactions. We study the case with no hierarchy constraints here, \(q(z_{j},z_{k},z_{j,k})=z_{j,k}\) as in (5). We expect our approach to extend to more general cases, but we do not pursue that here.

**Model Setup.** We assume for \(i[n]\), the data is bounded, \(\|_{i}\|_{2} 1\). The noisy observations are given as \(y_{i}=h^{*}(_{i})+_{i}=y_{i}^{*}+_{i}\) where \(h^{*}\) is the unknown underlying generative model (need not be a tree ensemble), and \(_{i}}}{{}}(0, ^{2})\) are noise values. Suppose \(f\) is a feasible solution for Problem (10). Let \(,\) denote the set of internal nodes and leaves for a depth \(d\) tree, and let \((^{j},^{j})\) be the set of weights corresponding to internal nodes and leaves of tree \(j\) i.e. \(f_{j}(x_{j})\) in this solution, where \(^{j}^{||},^{j}^{||}\). We also let \(W^{j}_{i},o^{j}_{l}\) be the weights corresponding to node \(i\) and leaf \(l\) in tree \(j\), respectively. We define \(^{j,k}_{i},o^{j,k}_{l}\) for \(f_{j,k}(x_{j},x_{k})\) and \(i,l\)in a similar fashion, where \(^{j,k}^{|| 2},^{j,k}^{| |}\). See Figure 1 for an illustration. Define

\[(f)=_{j[p],i\\ z_{j}=1}|W^{j}_{i}|_{j[p],l \\ z_{j}=1}|o^{j}_{i}|_{j,k[p],i \\ z_{j,k}=1}\|^{j,k}_{i}\|_{2}_{j,k [p],l\\ z_{j,k}=1}|o^{j,k}_{l}|\]

where \(\) denotes maximum. Conceptually, \((f)\) is the largest weight (in absolute value) that appears in all main effect and interaction soft trees. Let \(=f_{(t)}()\) be the estimator resulting from the noisy data, and \(f^{*}=f_{(t)}(^{*})\) be the oracle estimator that is the best approximation to the noiseless data among feasible solutions of (10) (i.e., among sparse additive tree models). We assume the following:

1. The activation function \(:\) for soft trees is \(L\)-Lipschitz for some \(L>0\).
2. There exists \(B>0\) such that \(()(f^{*}) B\).

Assumption (A2) ensures the trees resulting from the data are uniformly bounded. This is a mild assumption as in practice, the data is bounded and the resulting trees are also bounded as a result.

**Main Results.** Our first result is for a general setup, where \(p\) can be much larger than \(n\).

**Theorem 1**.: _Let \(,f^{*}\) be as defined above and take \(A=2 2^{d+2}B B^{2}dL2^{d+3}\) and \(a=(-1)\). Under the model setup, assume \(s 1\) and \( 1\).2 Then, **(1)** For \(t=1\), if \(n^{2}(s p+s^{2}A^{2})\), then with high probability_

\[_{i=1}^{n}((_{i})-y_{i}^{*})^{2}_{i=1}^{n}(f^{*}(_{i})-y_{i}^{*})^{2}+}{n^{2/3}} ((s p)^{2/3}+(sA)^{4/3}).\]

_(2) For \(t=2\), if \(n^{2+2a}(s p+s^{3}A^{3})\), then with high probability_

\[_{i=1}^{n}((_{i})-y_{i}^{*})^{2}_{i=1}^{n}(f^{*}(_{i})-y_{i}^{*})^{2}+}{ n^{1/(2+a)}}(+(sA)^{3/2})^{2/(2+a)}.\]

Theorem 1 presents non-asymptotic bounds for the performance of our method. Particularly, this theorem shows that under a well-specified model where \(y_{i}^{*}=f^{*}(_{i})\), prediction error rates of \(n^{-2/3}\) and \(n^{-1/(2+a)} n^{-0.42}\) are achievable for main effects and interaction models, respectively. Particularly, this implies that the prediction error of our estimator (resulting from the noise in observations) converges to zero as we increase the total number of samples, \(n\). We note that although the rate from the main effects model is sharper, such models might be too simple to capture the noiseless data, leading to larger oracle error. Therefore, in practice, the interactions model can lead to better performance. Next, we show that under the asymptotic assumption that \(n\), the error rate for the interaction models can be further improved.

**Theorem 2**.: _Under the model setup with \(t=2\), assume all parameters are fixed except \(n\). For any positive sequence \(a_{n}\) with \(_{n}a_{n}=0\), there exists a positive sequence \(b_{n}\) with \(_{n}b_{n}=0\) such that if \(n b_{n}^{-1/2(2+a_{n})}\) then with high probability_

\[_{i=1}^{n}((_{i})-y_{i})^{2} _{i=1}^{n}(f^{*}(_{i})-y_{i}^{*})^{2}+)}}.\]

Theorem 2 shows that essentially when \(n\) and other parameters in the problem stay constant, an error rate of \(n^{-0.5}\) is achievable for the interactions model which improves upon Theorem 1.

**Discussion of previous work:** As stated, our results are the first to present prediction bounds specialized to learning sparse additive models with soft trees. Moreover, as we discuss in Appendix C.5, our upper bounds (rates) generally align with the best known rates available in the literature for learning main effects and interactions, such as , or hold under milder assumptions.

## 6 Experiments

We study the performance of GRAND-SLAMIN on 16 real-world datasets and compare against relevant baselines for different cases. We make the following comparisons:

1. Performance comparison of GRAND-SLAMIN without/with structural constraints against existing toolkits for sparse GAMs with interactions.

1. Toolkits that support sparse interactions: EB\({}^{2}\)M  and NODE-GA\({}^{2}\)M  2. Toolkits that support hierarchy constraints: GAMI-Net  and SIAN 
2. Variable selection comparison against the competing toolkits.
3. Computational scalability of GRAND-SLAMIN toolkit with sparse backpropagation.
4. Variance reduction with structural constraints

Additional results are included in Supplement Sec. F that study (i) comparison with full complexity models in F.1, (ii) comparison with GAMs with all pairwise interactions in F.2, (iii) comparison with Group Lasso selection approach in F.3, (iv) choice of shape functions in F.4, (v) effect of entropy on performance and component selection in F.5, and (vi) effect of screening on training times, memory and performance in F.6.

**Datasets.** We use a collection of 16 open-source classification datasets (8 binary, 6 multiclass and 2 regression) from various domains. We consider datasets with a wide range of number of all pairwise interactions \(10-200000\). A summary of the datasets is in Table E.1 in the Appendix.

### Prediction Performance

**Comparison with EB\({}^{2}\)M and NODE-GA\({}^{2}\)M.** We first study the performance of our model in comparison to two tree-based state-of-the-art toolkits which support sparse GAMs with interactions without any structural constraints e.g., EB\({}^{2}\)M and NODE-GA\({}^{2}\)M. We report the ROC AUC performance in Table 2. Our model outperforms EB\({}^{2}\)M in 10 out of 14 datasets. Our model is also competitive with NODE-GA\({}^{2}\)M as it can outperform in \(50\%\) of the datasets. In summary, our results in Table 2 show that we are at par with state-of-the-art methods for unstructured component selection. Our key advantage is to do hierarchical interactions, which NODE-GA\({}^{2}\)M and EB\({}^{2}\)M can not support. Additionally, we can achieve faster training times (Sec. 6.3) and improve on variable selection (Sec. 6.2) than NODE-GA\({}^{2}\)M and EB\({}^{2}\)M.

**Structural constraints: Weak and Strong Hierarchy.** Next, we study our method with structural constraints i.e., (6) for weak hierarchy or (7) for strong hierarchy. We compare against two competing neural-based state-of-the-art methods for sparse GAMs with hierarchical interactions: (i) GAMI-Net with support for weak hierarchy, and (ii) SIAN with support for strong hierarchy. We omit multiclass datasets as both GAMI-Net and SIAN do not support them. We report the ROC AUC performance in Table 3. Our models outperform GAMI-Net and SIAN in 7/8 datasets.

Additionally, our models are much more compact in terms of overall number of parameters -- our tree-based shape functions have \(100\) and \(10\) smaller number of parameters than the neural-based

  Dataset & EB\({}^{2}\)M & NODE-GA\({}^{2}\)M & GRAND-SLAMIN \\  Magic & \(93.12 0.001\) & \( 0.13\) & \(93.86 0.30\) \\ Adult & \(91.41 0.0004\) & \( 0.14\) & \(91.54 0.14\) \\ Churn & \(91.97 0.005\) & \(98.62 5.61\) & \( 0.41\) (SH) \\ Satimage & \(97.65 0.0007\) & \(98.70 0.07\) & \( 0.04\) \\ Texture & \(99.81 0.0004\) & \( 0.00\) & \( 0.00\) \\ MiniBooNE & \(97.86 0.0001\) & \( 0.02\) & \(97.77 0.05\) (WH) \\ Covertype & \(90.08 0.0003\) & \(95.39 0.12\) & \( 0.08\) \\ Spambase & \( 0.01\) & \(98.78 0.06\) & \(98.55 0.07\) (SH) \\ News & \(73.03 0.002\) & \( 0.06\) & \(73.24 0.04\) (SH) \\ Optdigits & \(99.79 0.0003\) & \(99.93 0.02\) & \( 0.0\) \\ Bankruptcy & \( 0.01\) & \(92.02 1.03\) & \(92.51 0.54\) (WH) \\ Madelon & \(88.04 0.02\) & \(60.07 0.82\) & \( 1.03\) (WH) \\ Activity & \(74.96 8.77\) & \( 0.04\) & \(99.24 1.45\) \\ Multiple & \( 0.0002\) & \(99.94 0.02\) & \(99.95 0.02\) \\  

Table 2: Test ROC AUC of GRAND-SLAMIN, EB\({}^{2}\)M and NODE-GA\({}^{2}\)M. We report median along with mean absolute deviation across 10 runs.

   &  &  &  \\  DatasetModel & WH & SH & WH & SH \\  Magic & \(91.72 0.05\) & \(93.02 0.06\) & \(93.16 0.55\) & \( 0.16\) \\ Adult & \(91.01 0.04\) & \(90.67 0.05\) & \(93.14 0.32\) & \( 0.15\) \\ Churn & \(90.05 0.07\) & \( 0.02\) & \(92.38 0.75\) & \(92.40 0.41\) \\ Spambase & \( 0.04\) & \(98.28 0.04\) & \(98.45 0.15\) & \(98.55 0.07\) \\ MiniBooNE & \(96.11 0.41\) & \(95.90\) & \( 0.05\) & \(97.62 0.30\) \\ News & \(72.54 0.05\) & \(72.28\) & \(73.15 0.08\) & \( 0.04\) \\ Bankruptcy & \(92.46 0.12\) & \(90.71\) & \( 0.54\) & \(90.45 1.87\) \\ Madelon & \(88.14 0.94\) & \(83.18\) & \( 1.03\) & \(86.23 1.89\) \\  

Table 3: Test ROC AUC for GRAND-SLAMIN with structural constraints i.e., (6) or (7), GAMI-Net and SIAN. We report median across 10 runs along with mean absolute deviation.

shape functions used by GAMI-Net and SIAN respectively. Moreover, our toolkit is significantly faster than SIAN and GAMI-Net on larger datasets -- see Sec. 6.3.

Additionally, we compare interpretable modeling toolkits with full complexity models e.g., deep neural network (DNN), in Supplement Sec. F.1. We observed interpretable models to outperform full complexity models on these datasets. We also compare our toolkit that fits sparse components with toolkits that fit all pairwise interactions e.g., NA\({}^{2}\)M, NB\({}^{2}\)M and SPAM in Supplement Sec. F.2. GRAND-SLAM generally outperform these methods with enhanced interpretability due to sparsity and structural constraints. We also study how our toolkit performs when we replace soft tree shape functions with MLP shape functions in Supplement Sec. F.4. Interestingly, we observe that soft trees seem to have an edge when the parameters are matched with MLP.

### Variable Selection

We evaluate the performance of our models in terms of feature selection. We report the number of features selected in Table 4 by each toolkit for sparse GAMs with interactions. We see that GRAND-SLAMIN with structural constraints, in particular strong hierarchy, can significantly reduce the number of features selected by the GAMs with interactions model. For example, on Bankruptcy datasets, GRAND-SLAMIN achieves feature compression up to a factor of \(8\) over state-of-the-art GAM toolkits. Having fewer features reinforces the usefulness of additive models as being interpretable.

### Computational Scalability

Next, we discuss the scalability of GRAND-SLAMIN.

**Sparse backpropagation.** We highlight the usefulness of our efficient approach with sparse backpropagation in Figure 2 on Activity dataset. We show in Figure 2 that during the course of training, the number of selected components (selected via binary variables \(z\)'s and \(q()\)'s) becomes progressively smaller. This leads to much faster computations at each epoch in Figure 2 due to

   & EB\({}^{}\) & NODE & GAMI & SIAN &  \\  Dataset(Model & & GA\({}^{2}\)M & Net & & & None & WH & SH \\  Magic & \(10 0\) & \(10 0\) & \(10 0\) & \(10 0\) & \(10 0\) & \(9 1\) & \( 0\) \\ Adult & \(14 0\) & \(14 0\) & \(14 1\) & \(14 0\) & \(13 1\) & \( 1\) & \( 1\) \\ Churn & \(19 0\) & \(19 0\) & \(18 2\) & \(19 0\) & \(19 0\) & \(11 1\) & \( 1\) \\ Satimage & \(36 0\) & \(36 0\) & \(-\) & \(-\) & \(36 0\) & \(36 0\) & \( 2\) \\ Texture & \(40 0\) & \(40 0\) & \(-\) & \(-\) & \(40 0\) & \(37 2\) & \( 2\) \\ MiniBooNE & \(50 0\) & \(50 12\) & \(34\) & \(50 0\) & \(50 0\) & \(28 3\) \\ Covertype & \(54 0\) & \(54 0\) & \(-\) & \(-\) & \( 1\) & \(54 1\) & \(54 0\) \\ Spambase & \(57 0\) & \(57 0\) & \( 2\) & \(55 1\) & \(57 0\) & \(56 3\) & \(54 2\) \\ News & \(58 0\) & \(58 0\) & \( 1\) & \(52\) & \(58 0\) & \(58 0\) & \(58 0\) \\ Optdigits & \(64 0\) & \(64 0\) & \(-\) & \(-\) & \(64 0\) & \(64 0\) & \( 1\) \\ Bankruptcy & \(95 0\) & \(95 0\) & \(60 15\) & \(69\) & \(95 0\) & \(60 26\) & \( 16\) \\ Madelon & \(500 0\) & \(500 0\) & \(61 56\) & \(490\) & \(26 19\) & \( 15\) & \(24 9\) \\ Activity & \(533 0\) & \(346 0\) & \(-\) & \(182 15\) & \(440 22\) & \( 21\) \\ Multiple & \(649 0\) & \(649 0\) & \(-\) & \(-\) & \(648 1\) & \( 9\) & \(649 0\) \\   WH-Weak Hierarchy, SH=Strong Hierarchy.} \\  &  \\ 

Table 4: Number of features used by GRAND-SLAMIN without/with additional structural constraints and competing approaches. Hyphen (-) indicates multiclass classification is not supported by GAMI-Net and SIAN.

Figure 2: GRAND-SLAMIN with standard (dense) backpropagation vs sparse backpropagation on Activity dataset. (a) shows the number of nonzero effects: \(_{j}z_{j}+_{(j,k)}q(z_{j},z_{k},z_{j,k})\) and (b) shows the time for each epoch during the course of training.

sparse forward and backward passes. By exploiting sparsity during training, we can get \(10\) faster training times than with standard backpropagation.

**Comparison with other toolkits.** Our toolkit is highly competitive in terms of training times with all existing tree-based and neural-based toolkits for sparse GAMs with interactions. For example, on Madelon dataset, we are \(15\) faster than NODE-GA\({}^{2}\)M, \(20\) faster than EBM, \(1300\) faster than SIAN and \(25\) faster than GAMI-Net. See Supplement Sec. F.7 for more detailed timing comparisons across multiple datasets. Note that, in addition, we can also handle the case of structured interactions -- extending the flexibility of existing end-to-end training methods.

### Variance Reduction with Structural Constraints

We provide a visualization study to further highlight an important contribution of our work. In particular, our framework can support models with structural constraints. Hence, we study the effect of these constraints on the stability of learning main effects (in the presence of interactions) when these structural constraints are imposed or not. For this exercise, we consider bikesharing dataset. We visualize some of the main effects in the presence/absence of hierarchy in Figure 3. Note that for visualization, we used the purification strategy [29; 6] post-training that pushes interaction effects into main effects if possible. We can observe in Figure 3 that when additional hierarchy constraints are imposed, the error bars are much more compact across different runs. This can potentially increase the trust you can have on the model for deriving interpretability insights. We show additional visualizations on another dataset (American Community Survey from US Census Planning Database 2022 ) to show the same behavior in Supplement Section G.

## 7 Conclusion

We introduce GRAND-SLAMIN: a novel and flexible framework for learning sparse GAMs with interactions with additional structural constraints e.g., hierarchy. This is the first approach to do end-to-end training of nonparameteric additive models with hierarchically structured sparse interactions. Our formulation uses binary variables to encode combinatorial constraints. For computational reasons, we employ smoothing of the indicator variables for end-to-end optimization with first-order methods (e.g., SGD). We propose sparse backpropagation, which exploits sparsity in the nature of the smoothing function in a GPU-compatible manner and results in \(10\) speedups over standard backpropagation. We present non-asymptotic prediction bounds for our estimators with tree-based shape functions. Numerical experiments on a collection of 16 real-world datasets demonstrate the effectiveness of our toolkit in terms of prediction performance, variable selection and scalability.

Figure 3: Estimated main effects in the presence of interaction effects on bikesharing dataset [Left] without hierarchy, [Middle] weak hierarchy and [Right] strong hierarchy. Strong hierarchy has the smallest error bars.