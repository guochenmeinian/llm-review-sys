# BERT Lost Patience

Won't Be Robust to Adversarial Slowdown

 Zachary Coalson, Gabriel Ritter, Rakesh Bobba, and Sanghyun Hong

Oregon State University

{coalsonz, ritterg, bobbar, hongsa}@oregonstate.edu

###### Abstract

In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To _audit_ their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting Wafle attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can remove perturbations effectively. This result suggests that future work is needed for developing efficient yet robust multi-exit models.

Our code is available at: https://github.com/zcoalson/WAFFLE

## 1 Introduction

A key factor behind the recent advances in natural language processing is the _scale_ of language models pre-trained on a large corpus of data. Compared to BERT  with 110 million parameters that achieves the GLUE benchmark score  of 81% from three years ago, T5  improves the score to 90% with 100\(\) more parameters. However, pre-trained language models with this scale typically require large memory and high computational costs to run inferences, making them challenging in scenarios where latency and computations are limited.

To address this issue, _input-adaptive_ multi-exit mechanisms  have been proposed. By attaching internal classifiers (or early exits) to each intermediate layer of a pre-trained language model, the resulting multi-exit language model utilizes these exits to stop its forward pass preemptively, when the model is confident about its prediction at any exit point. This prevents models from spending excessive computation for "easy" inputs, where shallow models are sufficient for correct predictions, and therefore reduces the post-training workloads while preserving accuracy.

In this work, we study the robustness of multi-exit language models to _adversarial slowdown_. Recent work  showed that, against multi-exit models developed for computer vision tasks, an adversary can craft human-imperceptible input perturbations to offset their computational savings. However, it has not yet been shown that the input-adaptive methods proposed in language domains are susceptible to such input perturbations. It is also unknown why these perturbations cause slowdown and how similar they are to those generated by standard adversarial attacks. Moreover, it is unclear if existing defenses, e.g., adversarial training , proposed in the community can mitigate slowdown attacks.

**Our contributions.** To bridge this gap, we _first_ develop Wafle, a slowdown attack that generates natural adversarial text that bypasses early-exits. We illustrate how our attack works in Figure 1.

Based on our finding that existing adversarial text attacks [16; 43] fail to cause significant slowdown, we design a novel objective function that pushes a multi-exit model's predictions at its early-exits toward the uniform distribution. Waffle integrates this objective into existing attack algorithms.

_Second_, we systematically evaluate the robustness of three early-exit mechanisms [41; 45; 21] on the GLUE benchmark against adversarial slowdown. We find that Waffle significantly offsets the computational savings provided by the mechanisms when each text input is individually subject to perturbations. We also show that methods offering more aggressive computational savings are more vulnerable to our attack.

_Third_, we show that Waffle can be effective in black-box scenarios. We demonstrate that our attack transfers, i.e., the adversarial texts crafted with limited knowledge about the victim cause slowdown across different models and multi-exit mechanisms. We are also able to find universal slowdown triggers, i.e., input-agnostic sequences of words that reduces the computational savings of multi-exit language models when attached to any text input from a dataset.

_Fourth_, we conduct a linguistic analysis of the adversarial texts Waffle crafts. We find that the effectiveness of the attack is not due to the amount of perturbations made on the input text, but rather how perturbations are made. Specifically, we find two critical characteristics present in a vast majority of successful samples: (1) subject-predicate disagreement, meaning that a subject and corresponding verb within a sentence do not match, and (2) the changing of named entities. These characteristics are highlighted in , where it was shown that BERT takes both into account when making predictions.

_Fifth_, we test the effectiveness of potential countermeasures against adversarial slowdown. We find that adversarial training [12; 43] is ineffective against Waffle. The defended multi-exit models lose efficacy, or they lose significant amounts of accuracy in exchange for aggressive computational savings. In contrast, we show that input sanitization can be an effective countermeasure. This result suggests that future work is needed to develop robust yet effective multi-exit language models.

## 2 Related Work

**Adversarial text attacks on language models.** Szegedy et al.  showed that neural network predictions can be _fool_-ed by human-imperceptible input perturbations and called such perturbed inputs _adversarial examples_. While earlier work in this direction studied these adversarial attacks on computer vision models [2; 25], there has been a growing body of work on searching for adversarial examples in language domains as a result of language models gaining more traction. However, adversarial texts are much harder to craft due to the discrete nature of natural language. Attacks on images leverage perturbations derived from computing _gradients_, as they are composed of pixel values forming a near-continuous space, but applying them to texts where each word is in a discrete space is not straightforward. As a result, diverse mechanisms for crafting natural adversarial texts [7; 30; 20; 16; 8; 9; 19] have been proposed. In this work, we show that an adversary can exploit the language model's sensitivity to input text perturbations to achieve a completely different attack objective, i.e., adversarial slowdown. A standard countermeasure against the adversarial input perturbation is _adversarial training_ that augments the training data with natural adversarial texts [13; 47; 15; 23; 43]. We also show that adversarial training and its adaptation to our slowdown attacks are ineffective in mitigating the vulnerability to adversarial slowdown.

**Input-adaptive efficient neural network inference.** Neural networks are, while accurate, computationally demanding in their post-training operations. Kaya et al.  showed that _overthinking_ is one problem--these models use all their internal layers for making predictions on every single input even for the "easy" samples where a few initial layers would be sufficient. Prior work [35; 17; 11] proposed _multi-exit architectures_ to mitigate the wasteful effect of overthinking. They introduce multiple internal classifiers (i.e., early exits) to a pre-trained model and fine-tune them on the training data to

Figure 1: **Illustrating adversarial slowdown. Replacing the word “He” with “Her” makes the resulting text input bypass all 11 ICs (internal classifiers) and leads to misclassification. The text is chosen from the Corpus of Linguistic Acceptability.**

make correct predictions. During the inference on an input, these early-exits enable _input-adaptive_ inference, i.e., a model stops running forward if the prediction confidence is sufficient at an exit.

Recent work [32; 41; 45; 40; 42; 48; 21; 44] adapted the multi-exit architectures to language models, e.g., BERT , to save their computations at inference. DeeBERT  and FastBERT  have been proposed, both straightforward adaptations of multi-exit architectures to language models. Zhou et al.  proposed patience-based early exits (PABEE) and showed that one could achieve efficiency, accuracy, and robustness against natural adversarial texts. Liao et al.  presented PastFuture that makes predictions from a global perspective, considering past and future predictions from all the exits. However, no prior work studied the robustness of the computational savings that these mechanisms offer to adversarial slowdown . We design a slowdown attack to _audit_ their robustness. More comparisons of our work to the work done in computer vision domains are in Appendix A.

## 3 Our Auditing Method: Waffle Attack

### Threat Model

We consider an adversary who aims to reduce the computational savings provided by a _victim_ multi-exit language model. To achieve this goal, the attacker performs perturbations to a natural test-time text input \(x\). The resulting adversarial text \(x^{}\) needs more layers to process for making a prediction. This attack potentially violates the computational guarantees made by real-time systems harnessing multi-exit language models. For example, the attacker can increase the operational costs of the victim model or push the response time of such systems outside the expected range.

Just like language models deployed in the real-world that accept any user input, we assume that the attacker has the ability to query the victim model with perturbed inputs. We focus on the word-level perturbations as they are well studied and efficient to perform with word embeddings . But it is straightforward to extend our attack to character-level or sentence-level attacks by incorporating the slowdown objective we design in Sec 3.2 into their adversarial example-crafting algorithms.

To assess the slowdown vulnerability, we comprehensively consider both _white-box_ and _black-box_ settings. The white-box adversary knows all the details of the victim model, such as the training data and the model parameters, while the black-box attacker has limited knowledge of the victim model.

### The Slowdown Objective

Most adversarial text-crafting algorithms iteratively apply perturbations to a text input until the resulting text \(x^{}\) achieves a pre-defined goal. The goal here for the standard adversarial attacks is the untargeted misclassification of \(x^{}\), i.e., \(f_{}(x^{}) y\). Existing adversarial text attacks design an objective (or a score) function that quantifies how the perturbation of a word (e.g., substitution or removal) helps the perturbed sample achieve the goal. At each iteration \(t\), the attacker considers all feasible perturbations and chooses one that minimizes the objective the most.

We design our score function to quantify how close a perturbed sample \(x^{}\) is to causing the worst-case slowdown. The worst-case we consider here is that \(x^{}\) bypasses all the early exits, and the victim model classifies \(x^{}\) at the final layer. We formulate our score function \(s(x^{},f_{})\) as follows:

\[s(x^{},f_{})=_{0<i K}1- F_{i}(x^{}),\]

Here, the score function takes \(x^{}\) as the perturbed text and \(f_{}\) as the victim model. It computes the loss \(\) between the softmax output of an \(i\)-th internal classifier \(F_{i}\) and the uniform probability distribution \(\) over classes. We use \(_{1}\) loss. \(K\) is the number of early-exits, and \(N\) is the number of classes.

The score function \(s\) returns a value in . It becomes one if all \(F_{i}\) is close to the uniform distribution \(\); otherwise, it will be zero. Unlike conventional adversarial attacks, our score function over iterations pushes all \(F_{i}\) to \(\) (_i.e._, the lowest confidence case). Most early-exit mechanisms stop forward pass if \(F_{i}\)'s prediction confidence is higher than a pre-defined threshold \(T\); thus, \(x^{}\) that achieves the lowest confidence bypasses all the exit points.

### The Waffle Attack

We finally implement Waffle by incorporating the slowdown objective we design into existing adversarial text attacks. In this work, we adapt two existing attacks: TextFooler  and A2T . TextFooler is by far the strongest black-box attack , and A2T is a gradient-based white-box attack. In particular, A2T can craft natural adversarial texts much faster than black-box attacks; thus, it facilitates adversarial training of language models. We discuss the effectiveness of this in Sec 7.

We describe how we adapt TextFooler for auditing the slowdown risk in Algorithm 1 (see Appendix for our adaptation of A2T).

```
0: a text input \(x=\{w_{1},w_{2},...,w_{n}\}\), its label \(y\), the victim model \(f_{}\), its early exits \(F_{i}\), sentence similarity function \(Sim()\), its threshold \(\), word embeddings \(E\) over the vocabulary \(V\), and the attack success threshold \(\).
0: a natural adversarial text \(x^{}\)
1:\(x^{} x\)
2:for each word \(w_{i}\) in \(x\)do
3: Compute the importance \(I_{w_{i}}\)
4:endfor
5: Compose a set \(W\) of all words \(w_{i} x\) sorted by the descending order of their importance
6: Remove the stop words from the set \(W\)
7:for each word \(w_{i}\) in \(W\)do
8: Initiate the set of substitute candidates \(C\) by computing the top \(N\) synonyms; we compute the cosine similarity between \(E_{w_{i}}\) and \(E_{w^{}}\), where \(w^{} V\)
9:\(C(C)\)
10:\(C_{final}\{\}\)
11:for\(c_{k}\) in \(C\)do
12:\(x^{temp}w_{j}\) with \(c_{k}\) in \(x^{}\)
13:if\((x^{temp},x^{})>\)then
14: Add \(c_{k}\) to \(C_{final}\)
15:\(s_{k} f_{}(x^{temp})\)
16:endif
17:endfor
18:if\( c_{k}\) whose score is \(s_{k}\)then
19: Keep the candidates \(c_{k} C_{final}\)
20:\(c^{*}*{argmax}_{c C_{final}}(x,x_{w_{j } c}^{temp})\)
21:\(x^{}w_{j}\) with \(c^{*}\) in \(x^{}\)
22:return\(x^{}\)
23:elseif\(s_{k}(x^{})>min\ s_{k}\)then
24:\(c^{*}*{argmax}_{c_{k} C_{final}}s_{k}\)
25:\(x^{}w_{j}\) with \(c^{*}\) in \(x^{}\)
26:endif
27:endfor
28:return\(x^{}\) ```

**Algorithm 1** Waffle (based on TextFooler)

We first compute the importance of each word \(w_{i}\) in a text input \(x\). TextFooler removes each word from \(x\) and computes their influence on the final prediction result. It then ranks the words based on their influence. In contrast, we rank the words based on the _increase_ in the slowdown objective after each removal. By perturbing only a few words, we can minimize the alterations to \(x\) and keep the semantic similarity between \(x^{}\) and \(x\). Following the original study, we filter out stop words, _e.g._, 'the' or 'when', to minimize sentence structure destruction.

**(line 7-9) Choose the set of candidate words for substitution.** The attack then works by replacing a set of words in \(x\) with the candidates carefully chosen from \(V\). For each word \(w_{i} x\), the attack collects the set of \(C\) candidates by computing the top \(N\) synonyms from \(V\) (line 8). It computes the cosine similarity between the embeddings of the word \(w_{i}\) and the word \(w^{} V\). We use the same embeddings  that the original study uses. TextFooler only keeps the candidate words with the same part-of-speech (POS) as \(w_{i}\) to minimize grammar destruction (line 9).

**(line 10-28) Craft a natural slowdown text.** We then iterate over the remaining candidates and substitute \(w_{i}\) with \(c_{k}\). If the text after this substitution \(x^{temp}\) is sufficiently similar to the text before it, \(x^{}\), we store the candidate \(c_{k}\) into \(C_{final}\) and compute the slowdown score \(s_{k}\). In the end, we have a perturbed text input \(x^{}\) that is similar to the original input within the \(\) similarity and the slowdown score \(s_{k}\) (line 10-17). To compute the semantic similarity, we use Universal Sentence Encoder that the original study uses .

In line 20-26, if there exists any candidate \(c_{k}\) that already increases the slowdown score \(s_{k}\) over the threshold \(\) we choose the word with the highest semantic similarity among these winning candidates. However, when there is no such candidate, we pick the candidate with the highest slowdown score, substitute the candidate with \(w_{i}\), and repeat the same procedure with the next word \(w_{i+1}\). At the end (line 28), TextFooler does not return any adversarial example if it fails to flip the prediction. However, as our goal is causing slowdown, we use this adversarial text even when the score is \(s_{k}\).

## 4 Auditing the Robustness to Adversarial Slowdown

We now utilize our Waffle attack as a vehicle to evaluate the robustness of the computational savings provided by multi-exit language models. Our adaptations of two adversarial text attacks, TextFooler and A2T, represent the black-box and white-box settings, respectively.

**Tasks.** We evaluate the multi-exit language models trained on seven classification tasks chosen from the GLUE benchmark : RTE, MRPC, MNLI, QNLI, QQP, SST-2, and CoLA.

**Multi-exit mechanisms.** We consider three early-exit mechanisms recently proposed for language models: DeeBERT , PABEE , and Past-Future . In DeeBERT, we take the pre-trained BERT and fine-tune it on the GLUE tasks. We use the pre-trained ALBERT  for PABEE and Past-Future. To implement these mechanisms, we use the source code from the original studies. We describe all the implementation details, e.g., the hyper-parameter choices, in Appendix.

**Metrics.** We employ two metrics: _classification accuracy_ and _efficacy_ proposed by Hong et al. . We compute both the metrics on the test-set \(S\) or the adversarial texts crafted on \(S\). Efficacy is a standardized metric that quantifies a model's ability to use its early exits. It is close to one when a higher percentage of inputs exit at an early layer; otherwise, it is 0. To quantify the robustness, we report the changes in accuracy and efficacy of a clean test-set \(S\) and \(S\) perturbed using Waffle.

### Multi-exit Language Models Are Not Robust to Adversarial Slowdown

Table 1 shows our evaluation results. Following the original studies, we set the early-exit threshold, i.e., entropy or patience, so that multi-exit language models have 0.33-0.5 efficacy on the clean test set (see Appendix for more details). We use four adversarial attacks: two standard adversarial attacks, TextFooler (TF) and A2T, and their adaptations: Waffle (TF) and Waffle (A2T). We perform

    &  &  \\   & & **RTE** & **MNLI** & **MRPC** & **QNLI** & **QQP** & **SST-2** & **CoLA** \\    \\   & **Acc.** & \(63 48\) & - & \(82 75\) & \(88 78\) & \(92 67\) & - & \(79 57\) \\  & **Ef.** & \(0.34 0.32\) & - & \(0.35 0.32\) & \(0.35 0.33\) & \(0.36 0.40\) & - & \(0.34 0.20\) \\   & **Acc.** & \(63 52\) & - & \(82 75\) & \(88 81\) & \(92 74\) & - & \(79 66\) \\  & **Ef.** & \(0.34 0.32\) & - & \(0.35 0.33\) & \(0.35 0.35\) & \(0.36 0.41\) & - & \(0.34 0.29\) \\   & **Acc.** & \(63 51\) & - & \(82 61\) & \(88 62\) & \(92 69\) & - & \(79 70\) \\  & **Ef.** & \(0.34 0.11\) & - & \(0.35 0.09\) & \(0.35 0.10\) & \(0.36 0.22\) & - & \(0.34 0.13\) \\   & **Acc.** & \(63 57\) & - & \(82 75\) & \(88 78\) & \(92 83\) & - & \(79 73\) \\  & **Ef.** & \(0.34 0.19\) & - & \(0.35 0.17\) & \(0.35 0.19\) & \(0.36 0.30\) & - & \(0.34 0.24\) \\    \\   & **Acc.** & \(79 34\) & \(83 25\) & \(87 37\) & \(91 33\) & \(92 31\) & \(93 22\) & \(82 5\) \\  & **Ef.** & \(0.24 0.22\) & \(0.28 0.17\) & \(0.32 0.21\) & \(0.31 0.18\) & \(0.37 0.27\) & \(0.37 0.26\) & \(0.32 0.23\) \\   & **Acc.** & \(79 57\) & \(83 52\) & \(87 63\) & \(91 71\) & \(92 61\) & \(93 76\) & \(82 38\) \\  & **Ef.** & \(0.24 0.22\) & \(0.28 0.21\) & \(0.32 0.26\) & \(0.31 0.27\) & \(0.37 0.31\) & \(0.37 0.32\) & \(0.32 0.23\) \\   & **Acc.** & \(79 57\) & \(83 38\) & \(87 47\) & \(91 51\) & \(92 67\) & \(93 50\) & \(82 48\) \\  & **Ef.** & \(0.24 0.09\) & \(0.28 0.05\) & \(0.32 0.08\) & \(0.31 0.06\) & \(0.37 0.17\) & \(0.37 0.08\) & \(0.32 0.08\) \\   & **Acc.** & \(79 72\) & \(83 69\) & \(87 73\) & \(91 82\) & \(92 79\) & \(93 85\) & \(82 60\) \\  & **Ef.** & \(0.24 0.17\) & \(0.28 0.18\) & \(0.32 0.21\) & \(0.32 0.23\) & \(0.37 0.27\) & \(0.37 0.29\) & \(0.32 0.19 attacks on the entire test-set and report the changes in accuracy and efficacy. In each cell, we include their flat changes and the values computed on the clean and adversarial data in parenthesis.

**Standard adversarial attacks are ineffective in auditing slowdown.** We observe that the standard attacks (TF and A2T) are ineffective in causing a significant slowdown. In DeeBERT, those attacks cause negligible changes in efficacy (-0.05-0.14), while they inflict a large accuracy drop (7%-25%). Against PABEE and PastFuture, we find that the changes are slightly higher than those observed from DeeBERT (_i.e._, 0.02-0.13 and 0.03-0.31). We can observe slowdowns in PastFuture, but this is not because the standard attacks are effective in causing slowdowns. This indicates the mechanism is more sensitive to input changes, which may lead to greater vulnerability to adversarial slowdown.

**Waffle is an effective auditing tool for assessing the slowdown risk.** We show that our slowdown attack can inflict significant changes in efficacy. In DeeBERT and PABEE, the attacks reduce the efficacy 0.06-0.26 and 0.07-0.29, respectively. In PastFuture, we observe more reduction in efficacy 0.13-0.45. These multi-exit language models are designed to achieve an efficacy of 0.33-0.5; thus its reduction up to 0.29-0.45 means a complete offset of their computational savings.

**The more complex a mechanism is, the more vulnerable it is to adversarial slowdown.** Waffle causes the most significant slowdown on PastFuture, followed by PABEE and DeeBERT. PastFuture stops forwarding based on the predictions from past exits and the estimated predictions from future exits. PABEE also uses patience, i.e., how often we observe the same decision over early-exits. They enable more _aggressive_ efficacy compared to DeeBERT, which only uses entropy. However, this aggressiveness can be exploited by our attacks, e.g., introducing inconsistencies over exit points; thus, PABEE needs more layers to make a prediction.

### Sensitivity to Attack Hyperparameter

The key hyperparameter of our attack, the attack success threshold (\(\)), determines the magnitude of the scores pursued by Waffle while crafting adversarials. The score measures how uniform all output distributions of \(F_{i}\) are. A higher \(\) pushes Waffle to have a higher slowdown score before returning a perturbed sample. Figure 2 shows the accuracy and efficacy of all three mechanisms on QNLI against \(\) in [0.1, 1]. We show that as \(\) increases, the slowdown (represented as a decrease in efficacy) increases, and the accuracy decreases.

In addition, as \(\) increases, the rate of decrease in accuracy and efficacy decreases. Note that in PastFuture, when \( 0.4\) the rate at which efficacy decreases drops by a large margin. The same applies to accuracy, and when \( 0.8\), accuracy surprisingly increases, a potentially undesirable outcome. Moreover, when \( 0.8\) efficacy does not decrease any further, which potentially wastes computational resources as the time required to craft samples increases greatly as \(\) is increased.

## 5 Practical Exploitation of Waffle in Black-box Settings

In Sec 4, we show in the worst-case scenarios, multi-exit language models are not robust to adversarial slowdown. We now turn our attention to black-box settings where an adversary does not have full knowledge of the victim's system. We consider two attack scenarios: (1) _Transfer-based attacks_ where an adversary who has the knowledge of the training data trains _surrogate_ models to craft adversarial texts and use them to attack the victim models. (2) _Universal attacks_ where an attacker finds a set of _trigger_ words that can inflict slowdown when attached to any test-time inputs. We run these experiments across various GLUE tasks and show the results from the RTE and QQP datasets. We include all our results on different datasets and victim-surrogate combinations in the Appendix.

**Transferability of Waffle.** We first test if our attack is transferable in three different scenarios: (1) Cross-seed; (2) Cross-architecture; and (3) Cross-mechanism. Table 2 summarizes our results.

Figure 2: **The impact of \(\) on accuracy and efficacy.** Taking each model’s results on QNLI, as \(\) is increased, the accuracy and efficacy decrease.

_Cross-seed._ Both the model architecture and early-exit mechanism are identical for the surrogate and the victim models. In RTE and QQP, our transfer attack (S\(\)V) demonstrates a significant slowdown on the victim model, resulting in a reduction in efficacy of 0.25 and 0.18, respectively. In comparison to the white-box scenarios (S\(\)S), these attacks achieve approximately 50% effectiveness.

_Cross-architecture._ We vary the model architecture, using either BERT or ALBERT, while keeping the early-exit mechanism (PABEE) the same. Across the board, we achieve the lowest transferability among the three attacking scenarios, with a reduction in efficacy of 0.01 in RTE and 0.02 in QQP, respectively. This indicates that when conducting transfer-based attacks, the matching of the victim and surrogate models' architectures has a greater impact than the early-exit mechanism.

_Cross-mechanism._ We now vary the early-exit mechanism used by the victim and surrogate models while the architecture (BERT) remains consistent. In QQP and RTE, we cause significant slowdown to the victim model (a reduction in efficacy of 0.21 and 0.13, respectively), even when considering the relational speed-up offered by different mechanisms (e.g., PastFuture offers more computational savings than PABEE and DeeBERT). The slowdown is comparable to the white-box cases (S\(\)S).

**Universal slowdown triggers.** If the attacker is unable to train surrogate models, they can find a few words (i.e., a trigger) that causes slowdown to any test-time inputs when attached. Searching for such a trigger does not require the knowledge of the training data. To demonstrate the practicality of this attack, we select 1000 random words from BERT's vocabulary and compute the total slowdown across 10% of the SST-2 test dataset by appending each vocab word to the beginning of every sentence. We then choose the word that induces the highest slowdown and evaluate it against the entire test dataset. We find that the most effective word, "unable", reduces efficacy by 9% and accuracy by 14% when appended to the beginning of all sentences once. When appended three times successively (i.e. "unable unable unable..."), the trigger reduces efficacy by 18% and accuracy by 3%.

## 6 Linguistic Analysis of Our Adversarial Texts

To qualitatively analyze the text generated by Waffle, we first consider how the number of perturbations applied to an input text affects the slowdown it induces. We choose samples crafted against PastFuture , due to it being the most vulnerable to our attack. We select the datasets that induce the most and least slowdown, MNLI and QQP, respectively, in order to conduct a well-rounded analysis. Using 100 samples randomly drawn from both datasets, we record the percentage of words perturbed by Waffle and the consequent increase in exit layer. In Figure 3, we find that there is no relationship between the percentage of words perturbed and the increase in exit layer. It is not the number of perturbations made that affects slowdown, but rather how perturbations are made.

In an effort to find another explanation for why samples crafted by Waffle induce slowdown on multi-exit models, we look to analyze the inner workings of BERT. Through the qualitative analysis performed by Rogers et al. , we find particular interest in two characteristics deemed of high importance to BERT when it makes predictions: (1) subject-predicate agreement and (2) the changing of named entities. In our experiment, these characteristics are incredibly prevalent in successful attacks. Particularly, we find that the score assigned by Waffle is much higher when the subject and predicate of a sentence do not match, or a named entity is changed. In addition, Waffle

   &  &  &  &  &  &  \\  & & & & & **Acc.** & **Eff.** & **Acc.** & **Eff.** \\  
**S** & **BERT** & **PastFuture** &  & S\(\)S & 66 \(\) 52 & 0.47 \(\) 0.11 & 91 \(\) 72 & 0.50 \(\) 0.26 \\
**V** & **BERT** & **PastFuture** & & S\(\)V & 66 \(\) 55 & 0.50 \(\) 0.25 & 91 \(\) 72 & 0.51 \(\) 0.33 \\ 
**S** & **BERT** & **PABEE** &  & S\(\)S & 66 \(\) 49 & 0.22 \(\) 0.08 & 91 \(\) 69 & 0.35 \(\) 0.16 \\
**V** & **ALBERT** & **PABEE** & & S\(\)V & 77 \(\) 55 & 0.22 \(\) 0.21 & 91 \(\) 74 & 0.36 \(\) 0.34 \\ 
**S** & **BERT** & **PABEE** &  & S\(\)S & 66 \(\) 49 & 0.22 \(\) 0.08 & 91 \(\) 69 & 0.35 \(\) 0.16 \\
**V** & **BERT** & **PastFuture** & & S\(\)V & 66 \(\) 55 & 0.50 \(\) 0.29 & 91 \(\) 74 & 0.51 \(\) 0.38 \\  

* S = Surrogate model; V = Victim model

Table 2: **Transfer-based attack results.** Results from the cross-seed, cross-mechanism, and cross-architecture experiments on RTE and QQP. In all experiments, we craft adversarial texts on the surrogate model (S) and then evaluated on both the surrogate (S\(\)S) and victim (S\(\)V) models.

[MISSING_PAGE_FAIL:8]

run AT with two different natural adversarial texts, those crafted by A2T and by Waffle (adapted from A2T). During training, we attack 20% of the total samples in each batch. We run our experiments with PABEE trained on RTE and SST-2. We first set the patience to 6 (consistent with the rest of our experiments), but we set it to 2 for the models trained with Waffle (A2T). Once trained, we examine the defended models with attacks stronger than A2T: TextFooler (TF) and Waffle (Ours).

**AT is ineffective against our slowdown attacks.** Table 4 shows that AT significantly reduces the efficacy of a model. Compared to the undefended models, the defended models achieve \(\)0 efficacy. As these models do not utilize early exits, they seem robust to our attacks. But certainly, it is not desirable. It is noticeable that the defended models still suffer from a large accuracy drop. We then decided to set the patience to two, i.e., the multi-exit language models use early-exits more aggressively. The defended models have either very low efficacy or accuracy, and our attacks can reduce both. This result highlights a trade-off between being robust against adversarial slowdown and being efficient. We leave further explorations as future work.

**Input sanitization can be a defense against adversarial slowdown.** Our linguistic analysis in Sec 6 shows that the subject-predicate discrepancy is one of the root causes of the slowdown. Building on this insight, we test if sanitizing the perturbed input before feeding it into the models can be a countermeasure against our slowdown attacks. We evaluate this hypothesis with two approaches.

We first use OpenAI's ChatGPT1, a conversational model where we can ask questions and get answers. We manually query the model with natural adversarial texts generated by Waffle (TF) and collect the revised texts. Our query starts with the prompt "Can you fix all of these?" followed by perturbed texts in the subsequent lines. We evaluate with the MNLI and QQP datasets, on 50 perturbed test-set samples randomly chosen from each. We compute the changes in accuracy and average exit number on the perturbed samples and their sanitized versions. We compute them on the PastFuture models trained on the datasets. Surprisingly, we find that the inputs sanitized by ChatGPT greatly recovers both accuracy and efficacy. In MNLI, we recover the accuracy by 12 percentage points (54%\(\)66%) and reduce the average exit number by 4 (11.5\(\)7.5). In QQP, the accuracy is increased by 24 percentage points (56%\(\)80%), and the average exit number is reduced by 2.5 (9.6\(\)7.1).

We also test the effectiveness of additional grammar-checking tools, such as Grammarly2 and language_tool_python3, in defeating our slowdown attacks. We run this evaluation using the same settings as mentioned above. We feed the adversarial texts generated by our attack into Grammarly and have it correct them. Note that we only take the Grammarly recommendations for the correctness and disregard any other recommendations, such as for clarity. We find that the inputs sanitized by Grammarly still suffer from a significant accuracy loss and slowdown. In MNLI, both the accuracy and the average exit number stay the same 56%\(\)58% and 9.6\(\)9.5, respectively. In QQP, we observe that there is almost no change in accuracy (54%\(\)58%) or the average exit number (11.5\(\)11.5).

## 8 Conclusion

This work shows that the computational savings that input-adaptive multi-exit language models offer are _not_ robust against adversarial slowdown. To evaluate, we propose Waffle, an adversarial text-crafting algorithm with the objective of bypassing early-exit points. Waffle significantly reduces the computational savings offered by those models. More sophisticated input-adaptive mechanisms suited for higher efficacy become more vulnerable to slowdown attacks. Our linguistic analysis

   &  &  &  &  \\   & & & **Acc.** & **Eff.** & **Acc.** & **Eff.** \\    & **6** & **TF** & \(81 8\) & \(0.04 0.04\) & \(92 5\) & \(0.04 0.04\) \\  & **Ours** & \(81 60\) & \(0.04 0.04\) & \(92 59\) & \(0.04 0.04\) \\   & **2** & **TF** & \(72 24\) & \(0.13 0.13\) & \(89 10\) & \(0.08 0.07\) \\   & **Ours** & \(72 59\) & \(0.13 0.14\) & \(89 56\) & \(0.08 0.07\) \\    & **6** & **TF** & \(78 7\) & \(0.04 0.04\) & \(92 6\) & \(0.04 0.04\) \\  & **Ours** & \(78 56\) & \(0.04 0.04\) & \(92 61\) & \(0.04 0.04\) \\   & **Ours** & **TF** & \(53 53\) & \(0.65 0.65\) & \(90 7\) & \(0.05 0.04\) \\   & **2** & **Ours** & \(53 53\) & \(0.65 0.65\) & \(90 57\) & \(0.05 0.04\) \\  

Table 4: **Effectiveness of AT.** AT is ineffective against Waffle. The defended models completely lose the computational efficiency (_i.e._, they have \(\)0 efficacy), even with the aggressive setting with the patience of 2. **P** is the patience.

exposes that it is not about the magnitude of perturbations but because pushing an input outside the distribution on which a model is trained is easy. We also show the limits of adversarial training in defeating our attacks and the effectiveness of input sanitization as a defense. Our results suggest that future research is required to develop efficient yet robust input-adaptive multi-exit inference.

## 9 Limitations, Societal Impacts, and Future Work

As shown in our work, word-level perturbations carefully calibrated by Waffle make the resulting natural adversarial texts offset the computational savings multi-exit language models provide. However, there have been other types of text perturbations, e.g., character-level  or sentence-level perturbations . We have not tested whether an adversary can adapt them to cause slowdowns. If these new attacks are successful, we can hypothesize that some other attributes of language models contribute to lowering the confidence of the predictions made by internal classifiers (early-exit points). It may also render potential countermeasures, such as input sanitization, ineffective. Future work is needed to investigate attacks exploiting different perturbations to cause adversarial slowdown.

To foster future research, we developed Waffle in an open-source adversarial attack framework, TextAttack . This will make our attacks more accessible to the community. A concern is that a potential adversary can use those attacks to push the behaviors of systems that harness multi-exit mechanisms outside the expectations. But we believe that our offensive measures will be adopted broadly by practitioners and have them audit such systems before they are publicly available.

We have also shown that using state-of-the-art conversational models, such as ChatGPT, to sanitize perturbed inputs can be an effective defense against adversarial slowdown. But it is unclear what attributes of those models were able to remove the artifacts (i.e., perturbations) our attack induces. Moreover, the fact that this defense heavily relies on the referencing model's capability that the victim cannot control may give room for an adversary to develop stronger attacks in the future.

It is also possible that when using conversational models online as a potential countermeasure, there will be risks of data leakage. However, our proposal does not mean to use ChatGPT as-is. Instead, since other input sanitation (Grammarly) failed, we used it as an accessible tool for input sanitization via a conversational model as a proof-of-concept that it may have effectiveness as a defense. Alternatively, a defender can compose input sanitization as a completely in-house solution by leveraging off-the-shelf models like Vicuna-13B . We leave this exploration as future work.

An interesting question is to what extent models like ChatGPT offer robustness to the _conventional_ adversarial attacks that aim to reduce a model's utility in the inference time. But this is not the scope of our work. While the conversational models we use offer some robustness to our slowdown attacks with fewer side-effects, it does not mean that this direction is bulletproof against all adversarial attacks and/or adaptive adversaries in the future. Recent work shows two opposite views about the robustness of conversational models . We envision more future work on this topic.

We find that the runtime of the potential countermeasures we explore in Sec 7 is higher than the average inference time of _undefended_ multi-exit language models. This would make them useless from a pure runtime standpoint. However, we reason that the purpose of using these defenses was primarily exploratory, aiming to understand further why specific text causes more slowdown and how modifying such text can revert this slowdown. Moreover, input sanitization is already used in commercial models. Claude-24, a conversational model similar to ChatGPT, already employs input-filtering techniques, which we believe, when combined together, is a promising future work direction. Defenses with less computational overheads must be an important area for future work.

Overall, this work raises an open-question to the community about the feasibility of _input-adaptive_ efficient inference on large language models. We believe future work is necessary to evaluate this feasibility and develop a mechanism that kills two birds (efficacy and robustness) with one method.