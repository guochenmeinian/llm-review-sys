# GmGM: a fast Gaussian graphical model for multi-modal data

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper introduces the Gaussian multi-Graphical Model, a model to construct sparse graph representations of matrix- and tensor-variate data. We generalize prior work in this area by simultaneously learning this representation across several tensors that share axes, which is necessary to allow the analysis of multimodal datasets such as those encountered in multi-omics. Our algorithm uses only a single eigendecomposition per axis, achieving an order of magnitude speedup over prior work in the ungeneralized case. This allows the use of our methodology on large multi-modal datasets such as single-cell multi-omics data, which was challenging with previous approaches. We validate our model on synthetic data and five real-world datasets.

## 1 Introduction

A number of modern applications require the estimation of networks (graphs) exploring the dependency structures underlying the data. In this paper, we propose a new approach for estimating conditional dependency graphs. Two datapoints \(x,y\) are _conditionally independent_ (with respect to a dataset \(\)) if knowing one provides no information about the other that is not already contained in the rest of the dataset: \((x|y,_{ xy})=(x|_{ xy})\). For normally distributed data, conditional dependencies are encoded in the inverse of the covariance matrix (the 'precision' matrix). Two datapoints are conditionally dependent on each other if and only if their corresponding element in the precision matrix is not zero. If our dataset were in the form of a vector \(\), we could then model it as \((,^{-1})\) for precision matrix \(\). This is a Gaussian Graphical Model (GGM); \(\) encodes the graph.

However, datasets are often more structured than vectors. For example, single-cell RNA sequencing datasets (scRNA-seq) come in the form of a matrix of gene expression counts whose rows are cells and columns are genes. Video data naturally requires a third-order tensor of pixels to represent it - rows, columns, and frames. Furthermore multi-omics datasets such as those including both scRNA-seq and scATAC-seq may require two or more matrices to be properly represented; one for each modality.

We could assume that each row of our matrix is an i.i.d. sample drawn from our model. However, independence is a strong and often incorrect assumption. If we wanted to make no independence assumptions, we could vectorize the dataset \(\) and estimate \(\) in \([](,^{-1})\). However, this produces intractably large \(\), whose number of elements is quadratic in the product of the lengths of our dataset's axes.

Thankfully, tensors are highly structured, and we are often interested in the dependency structure of each axis individually - i.e. the dependencies between samples or the dependencies between features - rather than the dependencies between the elements of the tensor themselves. To model this, we can represent \(\) as some deterministic combination of the axis-wise dependencies:\((,(_{},_{})^{-1})\), for some function \(\). The strategy is to estimate \(_{},_{}\) directly, without computing the intractable \((_{},_{})^{-1}\). While there are multiple choices for \(\), this paper considers only the Kronecker sum.

### Prior work

The Kronecker sum BiGraphical Lasso (BiGLasso) model was first considered by Kalaitzis et al. . BiGLasso is the multi-axis analog to graphical lasso methods , which are used to estimate covariance matrices of data drawn from a multivariate Gaussian distribution. The Kronecker sum of two matrices, \(\), can be expressed in terms of Kronecker products: \(+\). When the matrices \(,\) are adjacency matrices of graphs, the Kronecker sum has the interpretation as the Cartesian product of those graphs. This sum is one choice \(\) to combine the per-axis precision matrices into the precision matrix of the vectorized dataset, \([](,(_{}_{})^{-1})\).

Other choices for \(\) have been considered, such as using the Kronecker product [23; 8], or the square of the Kronecker sum [24; 25]. Each method has its strengths; the benefits of a Kronecker sum structure are its interpretability as a graph product, stronger sparsity, and its allowance of inter-task transfer .

The original BiGLasso model was very slow to converge to a solution, in large part due to its non-optimal space complexity of \(O(n^{2}p^{2})\). This prohibited its use on large datasets (measuring in a couple hundred samples and/or features). Numerous modifications have been made to the algorithm to improve its speed and achieve an optimal space complexity of \(O(n^{2}+p^{2})\), such as scBiGLasso , TeraLasso , and BiGLasso . Of these, TeraLasso is notable in that it generalizes to an arbitrary number of axes, i.e. \((_{1},...,_{k})=_{1}... _{k}\). TeraLasso and BiGLasso, the fastest prior work, both rely on computing an eigendecomposition every iteration.

All of these algorithms and models, including our own, rely on a normality assumption. We are most interested in the case of omics data, in which case a log-transform renders our dataset sufficiently Gaussian-like for our algorithm to achieve good performance. An overview of the use of GGMs in omics data is given by Altenbuchinger et al. .

### Unmet need

Many datasets, especially those in multi-omics, are representable as a collection of matrices or tensors. As a case study, we consider (a subset of) the Lifelines-DEEP dataset from Tigchelaar et al. , which is summarized graphically in Figure 1.

In this dataset, two different modalities of data were gathered from the same people: counts of microbial species found in their stools (metagenomics) and counts of metabolites found in their blood plasma (metabolomics). While different matrices, each modality shares an axis. If we were to estimate a graph of people on each modality independently, they would likely yield different graphs. This is not ideal; if our aim is to estimate the true graph of conditional dependencies, there should be only one resultant graph. To estimate it, we should be considering both modalities simultaneously.

Figure 1: The two matrices of the LifeLines-DEEP dataset. As both matrices include data for the same people, the learned graph between people should be the same.

One way to do this would be to concatenate the modalities, producing a matrix of people by "species+metabolites". This could yield interesting results, if one is interested in connections between individual species and a metabolite. However, it would increase the size of the output graph, which grows quadratically in the length of the axis. Furthermore, it is not always possible or feasible; some datasets may not be concatenatable. We visually demonstrate some cases where concatenation fails in Figure 2.

### Our contributions

We introduce a novel method to extend the use of Gaussian Graphical Models to multi-tensor datasets. This extension is essential to model conditional dependencies in multimodal datasets such as those frequently occurring in multi-omics. We present an efficient algorithm to estimate these conditional dependencies. When restricted to the single-tensor case, our algorithm is much faster than previous algorithms that estimated conditional dependency graphs for each axis, such as TeraLasso and EiGLasso.

## 2 Methods

### Notation

In prior work, a single-tensor dataset \(\) is modelled as \([](, (_{}_{})^{-1})\), also written as \(_{KS}(\{_{}\} _{})\).

Our model considers multiple tensors, each with their own (potentially shared) axes. We aim to estimate the precision matrices \(_{}\) for each axis \(\) of each tensor \(^{}\), indexed by \(\). To describe that an axis \(\) is one of the axes of a tensor \(^{}\), we will write \(\). Some values will be indexed by both an axis and a tensor; for consistency we will use subscripts to denote axes (typically \(\)) and superscripts to denote tensors (typically \(\)). \(d_{}^{}\) will represent the number of elements in \(^{}\), and \(d_{}=_{}d_{}^{}\).

Figure 2: (A) A hypothetical dataset whose structure cannot be reduced to a single tensor by concatenation. Concatenating would lead to a block of missing values for a hypothetical (and nonsensical) species by elements matrix. (B) A hypothetical single-cell RNA-sequencing dataset procured from multiple patients. Concatenation is possible, but would lead to a very large output graph for a modest number of patients.

An important concept is the Gram matrix \(_{}^{}\). In the single-tensor case, this is a sufficient statistic; all prior work first computes these matrices as the first step in their algorithm. Let \(_{}[^{}]\) represent the "matricization" of \(^{}\) along axis \(\), then \(_{}^{}=_{}[^{} ]_{}[^{}]^{T}\). The matricization of a tensor picks one axis, \(\), to index the rows, and flattens the rest out into columns. Note that for a matrix \(\), \(_{}[]=^{T}\). Rather than \(_{}^{}\), we consider the "effective Gram matrices" \(_{}=_{}_{}^{}\), as these fulfill the role of the Gram matrices in the multi-tensor case.

### The model

To properly handle sets of tensors, we propose modelling each tensor as being drawn independently from a Kronecker-sum normal distribution. If the tensors share an axis \(\), then they will still be drawn independently - but their distributions will be parameterized by the same \(_{}\). For an arbitrary set of tensors, the model is:

\[^{} _{KS}(\{_{}\}_{ })\] \[^{}\{^{} \}_{}\]

We call this model the "Gaussian multi-Graphical Model" (GmGM) as it extends Gaussian Graphical Models to estimate multiple graphs from a set of tensors. In this paper, we will make the assumption that no tensor in our set contains the same axis twice - notably, covariance matrices would violate this assumption. Any tensor with a repeated axis would naturally be interpretable as a graph - such datasets are rare, and if one already has a graph the need for an algorithm such as this is diminished.

As an example, we model the LifeLines-DEEP dataset \(^{}\) and \(^{}\) independently as:

\[^{} _{KS}(_{},_{})\] \[^{} _{KS}(_{},_{})\]

### The algorithm

Here, we present an algorithm to compute the maximum likelihood estimate (MLE) jointly for all parameters \(_{}\) of the GmGM. The general idea is to produce an analytic estimate for the eigenvectors of \(_{}\), and then iterate to solve for the eigenvalues; this is summed up graphically in Figure 3.

In the supplementary material, we derive the following:

\[p(\{^{}\}) =_{}|}}{(2)^{}{2}}}e^{ _{}[_{}_{}]}\] (pdf of GmGM) \[[\{^{}\}] _{}[_{}_ {}]-_{}|_{}_{ }|\] (negative log likelihood)

From this, we can observe that the effective Gram matrices \(_{}\) form a set of sufficient statistics for our distribution. Furthermore, the log-likelihood is the sum of log-likelihoods in the single-axis case, thus preserving convexity of the loss function.

**Theorem 1**.: _Let \(_{}_{}_{}^{T}\) be the eigendecomposition of \(_{}\) (where \(_{}^{d_{} d_{}}\) and \(_{}^{d_{} d_{}}\) is a diagonal matrix). Then \(_{}\) are the eigenvectors of the maximum likelihood estimate of \(_{}\)._

Theorem 1 is critical to allowing efficient estimation of \(_{}\), as it not only allows us to extract the computationally intensive eigendecomposition operation from the iterative portion of the algorithm, but also reduces the number of parameters to be linear in the length of an axis.

To find the eigenvalues \(_{}\) of \(_{}\), we produce the second theorem:

**Theorem 2**.: _Let \(\{_{}^{}\}\) be matrices such that the expression \(_{}_{}^{}\) is the best Frobenius-norm approximation of \((_{}_{}^{})^{-1}\). Then, for a learning rate \(_{t}\), gradient descent can be performed with the update equation \(_{}^{t+1}=_{}^{t}-_{t}[_{}- _{|}_{}^{}]\). As \(_{}\) is positive definite, \(_{t}\) must be chosen to prevent \(_{}^{t}\) from becoming negative._

While the definition of \(_{}^{}\) is technical, it is analogous to the notion of the blockwise-trace from Kalaitzis et al.  and \(_{}\) from Greenewald, Zhou, and Hero III . Proofs of Theorems 1 and 2, along with a method to compute \(_{}^{}\), are given in the supplementary material. Overall, our algorithm is described in the pseudocode at the top of the next page.

For regularization, one can choose to either keep the top \(p\)% of edges, or keep the top \(k\) edges per vertex (for parameters \(p,k\)). The incorporation of more advanced regularizers, such as Lasso, would require an eigen-recomposition on each iteration, which would be much slower. As we demonstrate empirically in Section 3, it is not necessary to use advanced regularizers to recover the graph structure to the same precision as prior work.

Figure 3: A graphical overview of how the GmGM algorithm works. We use \(\) to represent an arbitrary modality, and \(\) to represent an arbitrary axis. Proofs are given in the supplementary material.

## 3 Results

We tested our algorithm on synthetic data and five real-world datasets. Explanations of data generation, collection, preprocessing, and regularization are given in the supplementary material.

### Synthetic Data

We verified that our algorithm was indeed faster on matrix-variate data compared to prior work (Figure 4) on our computer (Ubuntu 20.04 with Intel Core i7 Processor and 8GB RAM). Our results on matrix data are encouraging - extrapolating the runtimes, datasets up to size 16,000 by 16,000 could have their graphs estimated in less than an hour. Larger datasets would require more than 6GB of memory for our algorithm to run, pushing the limits of RAM. Our algorithm was not significantly faster on higher-order tensor data (see the supplementary material). This is due to the complexity of computing the Gram matrices, which grows exponentially with the number of axes.

In addition to these speed improvements, we show that we perform equivalently to state-of-the-art on matrix data (Figure 5a). On higher-order tensor data, we are outperformed by TeraLasso, which is able to achieve near-perfect recovery of the graphs. We believe this is due to our algorithm's use of thresholding rather than a more advanced regularization technique. Since our speed gains are not significant relative to TeraLasso, on higher-order tensor data without shared axes one should prefer TeraLasso to GmGM. Finally, we demonstrate that taking into account shared axes does indeed improve performance (see blue line, Figure 5b). Prior work could not take this into account.

### Real Data

We tested our method on various real datasets. These include two video datasets (COIL-20  and EchoNet-Dynamic ), a transcriptomics dataset (E-MTAB-2805 ), and two multi-omics datasets (LifeLines-DEEP  and a 10x Genomics dataset ).

Figure 4: A comparison of the runtimes of our algorithm against (a) bi-graphical and (b) tensor-graphical prior work. Runtimes were averaged over 5 runs.

Figure 5: (a) Precision-recall curves comparing various algorithms on synthetic 50x50 matrix data. (b) Precision-recall curves comparing our algorithm on two 50x50 matrices with one shared axis. We considered both modalities simultaneously (blue) and an individual modality (red, orange). In both subfigures, each edge of the true graphs was generated independently with probability \(\).

Figure 6: The estimated precision matrices on the E-MTAB-2805 dataset (a) and the EchoNet-Dynamic dataset (b). Yellow represents an edge and purple represents the lack of an edge. The E-MTAB-2805 cells have been grouped together by cell cycle stage, in the order G, S, and G2/M.

The E-MTAB-2805 dataset consists of transcriptomics data for individual cells split into three groups by their stage in the cell cycle (G, S, and G2/M). If our estimated precision matrices had a 3x3 block-diagonal structure, this would indicate that it had recreated this grouping. This is not what we see, but we do see a 3x3 block matrix structure (Figure 5(a)). We found that cells in the DNA synthesis stage (S) had few connections between them, and that there were many connections between the G1 and G2/M stages. This result is biologically plausible, as cells in the synthesis stage are the most variable.

The results on EchoNet-Dynamic (Figure 5(b)) are much more encouraging, as we would expect a periodic structure due to the beating of the heart. A precision matrix with repeating diagonals is what we would expect to see in this case, which is what our algorithm produces. In the supplementary material, we further verify that this corresponds to a heartbeat by using the repetition to accurately predict the opening of the mitral valve in the video.

The duck video in the COIL-20 dataset was considered in the original BiGLasso paper , in which they showed that their algorithm could recover the ordering of the frames of the video. To do this they had to heavily downsample the image (to a 9x9 image with half the frames), and flatten the rows and frames into a single axis. Due to the speed improvements of our algorithm, and its ability to handle tensor-variate data, we were able to run our algorithm on the raw, unprocessed data and achieve a similar result in negligible time. Specifically, the reconstruction of the frames had an accuracy of 99%.

Prior work by Prost, Gazut, and Bruls  used assortativity to assess their validity of the species graph estimated by their model on the LifeLines-DEEP metagenomics dataset. Assortativity represents the tendency of related species to cluster together in the graph. A random graph would have an assortativity of zero, but we would expect moderate assortativity in the true network as similar species may fulfill similar roles in the gut microbiome. Our assortativity is comparable to prior work (Figure 7). We also found that our graphs were more robust to noise than prior work; we analyze this in the supplementary material.

Finally, we tested our approach on a 10x Genomics single-cell (RNA+ATAC) dataset taken from a B Cell lymphoma tumour. We demonstrate that the clusters we find (using Louvain clustering) on the graph remain visually cohesive when projected into lower-dimensional space by UMAP (Figure 8). In particular, the disconnected "islands" in UMAP correspond to their own cluster on the graph as well. As these island-clusters were arrived at independently through two methods, UMAP and our algorithm, it increases our confidence in the validity of the clustering. In the supplementary

Figure 7: Assortativity with increasing regulatization in the LifeLines-DEEP dataset, comparing our method with the Zero-inflated Log-Normal (ZiLN) model. In one case we show the performance of our algorithm restricted to the metagenomics dataset (a) and when augmented with the metabolomics dataset (b). In both cases, ZiLN is only trained on the metagenomics dataset, as it is a single-axis model.

material, we verify that these clusters do represent distinct groups via a GO term enrichment analysis. Our overall approach has been implemented in Python. All of the code to run the algorithm and recreate the experiments has been made publicly available on GitHub; https://github.com/NeurIPS-GmGM-Paper/GmGM.

## 4 Limitations

Our method uses thresholding rather than more sophisticated regularizers. However, there is no fundamental barrier preventing our algorithm from allowing regularizers at the cost of an eigen-recognition per iteration. This would increase the asymptotic complexity of the iterative portion of our algorithm, making it questionable whether any gains in precision would be worth the loss in efficiency.

Our method assumes that no tensor has a repeated axis (i.e. a matrix of people by people rather than people by species). If there is a repeated axis, one can no longer analytically find the eigenvectors of the MLE, at least by our methods. This is not a substantial issue, as such datasets are uncommon and already represent graphs. Rather than extending the algorithm to work with repeated-axis tensors, it would be more fruitful to extend it to work with priors.

When considering multi-tensor datasets, it may be the case that two axes only partially overlap. For example, the full LifeLines-DEEP dataset contains a second (follow-up) metagenomics dataset for a third of the study participants; two thirds of the patients are missing from this dataset. We do not make an attempt to handle this type of missing data, even though missing data shows up in many applications. The lack of ability to handle missing data is a major limitation of our algorithm. It is nontrivial to extend the algorithm to handle this case, as it renders Theorem 1 ineffective and hence removes the speed advantage we attained. Prior work has not addressed this problem, as it only exists in multi-tensor datasets and we are the first to consider this case.

## 5 Conclusion

We have created a novel model, GmGM, which successfully generalizes Gaussian graphical models to the common scenario of multi-tensor datasets. Furthermore, we demonstrated that our algorithm is significantly faster than prior work focusing on Gaussian tensor-graphical models such as EiGLasso and TeraLasso while still preserving state-of-the-art performance. These speed improvements allow tensor-graphical models to be applied to datasets with axes of length in the thousands. Finally, we demonstrated the application of our algorithm on five real-world datasets to prove its efficacy.

Figure 8: Two plots of the same cells from the 10x Genomics dataset, displayed via UMAP  (a) and the Fruchterman-Reingold layout algorithm (b). Colors are based on Louvain clustering of the graph, and represent the same clustering in both figures.